Project Path: arc_gmh5225_zyemu_k8py1c8n

Source Tree:

```txt
arc_gmh5225_zyemu_k8py1c8n
├── CMakeLists.txt
├── LICENSE
├── README.md
├── cmake.toml
├── cmkr.cmake
├── include
│   └── zyemu
│       ├── types.hpp
│       └── zyemu.hpp
├── src
│   ├── playground
│   │   └── main.cpp
│   └── zyemu
│       ├── assembler.cpp
│       ├── assembler.hpp
│       ├── codegen.cpp
│       ├── codegen.hpp
│       ├── cpu.cpp
│       ├── internal.hpp
│       ├── registers.cpp
│       ├── registers.hpp
│       └── unordered_dense.h
└── thirdparty
    ├── CMakeLists.txt
    └── cmake.toml

```

`CMakeLists.txt`:

```txt
# This file is automatically generated from cmake.toml - DO NOT EDIT
# See https://github.com/build-cpp/cmkr for more information

cmake_minimum_required(VERSION 3.25)

if(CMAKE_SOURCE_DIR STREQUAL CMAKE_BINARY_DIR)
	message(FATAL_ERROR "In-tree builds are not supported. Run CMake from a separate directory: cmake -B build")
endif()

set(CMKR_ROOT_PROJECT OFF)
if(CMAKE_CURRENT_SOURCE_DIR STREQUAL CMAKE_SOURCE_DIR)
	set(CMKR_ROOT_PROJECT ON)

	# Bootstrap cmkr and automatically regenerate CMakeLists.txt
	include(cmkr.cmake OPTIONAL RESULT_VARIABLE CMKR_INCLUDE_RESULT)
	if(CMKR_INCLUDE_RESULT)
		cmkr()
	endif()

	# Enable folder support
	set_property(GLOBAL PROPERTY USE_FOLDERS ON)

	# Create a configure-time dependency on cmake.toml to improve IDE support
	set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS cmake.toml)
endif()

project(zyemu)

# Subdirectory: thirdparty
set(CMKR_CMAKE_FOLDER ${CMAKE_FOLDER})
if(CMAKE_FOLDER)
	set(CMAKE_FOLDER "${CMAKE_FOLDER}/thirdparty")
else()
	set(CMAKE_FOLDER thirdparty)
endif()
add_subdirectory(thirdparty)
set(CMAKE_FOLDER ${CMKR_CMAKE_FOLDER})

# Target: zyemu
set(zyemu_SOURCES
	cmake.toml
	"include/zyemu/types.hpp"
	"include/zyemu/zyemu.hpp"
	"src/zyemu/assembler.cpp"
	"src/zyemu/assembler.hpp"
	"src/zyemu/codegen.cpp"
	"src/zyemu/codegen.hpp"
	"src/zyemu/cpu.cpp"
	"src/zyemu/internal.hpp"
	"src/zyemu/registers.cpp"
	"src/zyemu/registers.hpp"
)

add_library(zyemu STATIC)

target_sources(zyemu PRIVATE ${zyemu_SOURCES})
source_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${zyemu_SOURCES})

add_library(zyemu::zyemu ALIAS zyemu)
target_compile_features(zyemu PUBLIC
	cxx_std_23
)

target_include_directories(zyemu PUBLIC
	include
)

target_link_libraries(zyemu PUBLIC
	Zydis
	sfl::sfl
)

# Target: zyemu-playground
set(zyemu-playground_SOURCES
	cmake.toml
	"src/playground/main.cpp"
)

add_executable(zyemu-playground)

target_sources(zyemu-playground PRIVATE ${zyemu-playground_SOURCES})
source_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${zyemu-playground_SOURCES})

target_compile_features(zyemu-playground PRIVATE
	cxx_std_23
)

target_link_libraries(zyemu-playground PRIVATE
	zyemu::zyemu
)

get_directory_property(CMKR_VS_STARTUP_PROJECT DIRECTORY ${PROJECT_SOURCE_DIR} DEFINITION VS_STARTUP_PROJECT)
if(NOT CMKR_VS_STARTUP_PROJECT)
	set_property(DIRECTORY ${PROJECT_SOURCE_DIR} PROPERTY VS_STARTUP_PROJECT zyemu-playground)
endif()

```

`LICENSE`:

```
MIT License

Copyright (c) 2021 ζeh Matt

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

`README.md`:

```md
## x86-64 emulation using JIT 

WIP x86-64 user mode emulation using Zydis. Instead of writing code for each instruction zyemu
will generate a specific function for the emulator to execute. 

For an instruction such as `mov r12, rax` it would generate following function to emulate it:
```asm
0000021CF4CB0000  mov         r11, qword ptr [rcx+18h]  ; Load value of rax from virtual context.
0000021CF4CB0004  mov         r10, r11                  ; r10 is used for r12
0000021CF4CB0007  mov         qword ptr [rcx+78h], r10  ; Store value of r12 in virtual context.
0000021CF4CB000B  add         qword ptr [rcx+8], 3      ; Update IP, length of this instruction is 3 bytes.
0000021CF4CB0010  mov         rax, 0                    ; Status code.
0000021CF4CB0017  ret  
```


```

`cmake.toml`:

```toml
# Reference: https://build-cpp.github.io/cmkr/cmake-toml
[cmake]
version = "3.25"

[project]
name = "zyemu"

[subdir.thirdparty]

[target.zyemu]
type = "static"
alias = "zyemu::zyemu"
sources = [
    "src/zyemu/cpu.cpp",
    "src/zyemu/codegen.cpp",
    "src/zyemu/assembler.cpp",
    "src/zyemu/registers.cpp",
]
headers = [
    "src/zyemu/assembler.hpp",
    "src/zyemu/codegen.hpp",
    "src/zyemu/internal.hpp",
    "src/zyemu/registers.hpp",
    "include/zyemu/zyemu.hpp",
    "include/zyemu/types.hpp",
]
include-directories = ["include"]
compile-features = ["cxx_std_23"]
link-libraries = ["Zydis", "sfl::sfl"]

[target.zyemu-playground]
type = "executable"
sources = [
    "src/playground/main.cpp"
]
compile-features = ["cxx_std_23"]
link-libraries = ["zyemu::zyemu"]
```

`cmkr.cmake`:

```cmake
include_guard()

# Change these defaults to point to your infrastructure if desired
set(CMKR_REPO "https://github.com/build-cpp/cmkr" CACHE STRING "cmkr git repository" FORCE)
set(CMKR_TAG "v0.2.43" CACHE STRING "cmkr git tag (this needs to be available forever)" FORCE)
set(CMKR_COMMIT_HASH "" CACHE STRING "cmkr git commit hash (optional)" FORCE)

# To bootstrap/generate a cmkr project: cmake -P cmkr.cmake
if(CMAKE_SCRIPT_MODE_FILE)
    set(CMAKE_BINARY_DIR "${CMAKE_BINARY_DIR}/build")
    set(CMAKE_CURRENT_BINARY_DIR "${CMAKE_BINARY_DIR}")
    file(MAKE_DIRECTORY "${CMAKE_BINARY_DIR}")
endif()

# Set these from the command line to customize for development/debugging purposes
set(CMKR_EXECUTABLE "" CACHE FILEPATH "cmkr executable")
set(CMKR_SKIP_GENERATION OFF CACHE BOOL "skip automatic cmkr generation")
set(CMKR_BUILD_TYPE "Debug" CACHE STRING "cmkr build configuration")
mark_as_advanced(CMKR_REPO CMKR_TAG CMKR_COMMIT_HASH CMKR_EXECUTABLE CMKR_SKIP_GENERATION CMKR_BUILD_TYPE)

# Disable cmkr if generation is disabled
if(DEFINED ENV{CI} OR CMKR_SKIP_GENERATION OR CMKR_BUILD_SKIP_GENERATION)
    message(STATUS "[cmkr] Skipping automatic cmkr generation")
    unset(CMKR_BUILD_SKIP_GENERATION CACHE)
    macro(cmkr)
    endmacro()
    return()
endif()

# Disable cmkr if no cmake.toml file is found
if(NOT CMAKE_SCRIPT_MODE_FILE AND NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/cmake.toml")
    message(AUTHOR_WARNING "[cmkr] Not found: ${CMAKE_CURRENT_SOURCE_DIR}/cmake.toml")
    macro(cmkr)
    endmacro()
    return()
endif()

# Convert a Windows native path to CMake path
if(CMKR_EXECUTABLE MATCHES "\\\\")
    string(REPLACE "\\" "/" CMKR_EXECUTABLE_CMAKE "${CMKR_EXECUTABLE}")
    set(CMKR_EXECUTABLE "${CMKR_EXECUTABLE_CMAKE}" CACHE FILEPATH "" FORCE)
    unset(CMKR_EXECUTABLE_CMAKE)
endif()

# Helper macro to execute a process (COMMAND_ERROR_IS_FATAL ANY is 3.19 and higher)
function(cmkr_exec)
    execute_process(COMMAND ${ARGV} RESULT_VARIABLE CMKR_EXEC_RESULT)
    if(NOT CMKR_EXEC_RESULT EQUAL 0)
        message(FATAL_ERROR "cmkr_exec(${ARGV}) failed (exit code ${CMKR_EXEC_RESULT})")
    endif()
endfunction()

# Windows-specific hack (CMAKE_EXECUTABLE_PREFIX is not set at the moment)
if(WIN32)
    set(CMKR_EXECUTABLE_NAME "cmkr.exe")
else()
    set(CMKR_EXECUTABLE_NAME "cmkr")
endif()

# Use cached cmkr if found
if(DEFINED ENV{CMKR_CACHE})
    set(CMKR_DIRECTORY_PREFIX "$ENV{CMKR_CACHE}")
    string(REPLACE "\\" "/" CMKR_DIRECTORY_PREFIX "${CMKR_DIRECTORY_PREFIX}")
    if(NOT CMKR_DIRECTORY_PREFIX MATCHES "\\/$")
        set(CMKR_DIRECTORY_PREFIX "${CMKR_DIRECTORY_PREFIX}/")
    endif()
    # Build in release mode for the cache
    set(CMKR_BUILD_TYPE "Release")
else()
    set(CMKR_DIRECTORY_PREFIX "${CMAKE_CURRENT_BINARY_DIR}/_cmkr_")
endif()
set(CMKR_DIRECTORY "${CMKR_DIRECTORY_PREFIX}${CMKR_TAG}")
set(CMKR_CACHED_EXECUTABLE "${CMKR_DIRECTORY}/bin/${CMKR_EXECUTABLE_NAME}")

# Helper function to check if a string starts with a prefix
# Cannot use MATCHES, see: https://github.com/build-cpp/cmkr/issues/61
function(cmkr_startswith str prefix result)
    string(LENGTH "${prefix}" prefix_length)
    string(LENGTH "${str}" str_length)
    if(prefix_length LESS_EQUAL str_length)
        string(SUBSTRING "${str}" 0 ${prefix_length} str_prefix)
        if(prefix STREQUAL str_prefix)
            set("${result}" ON PARENT_SCOPE)
            return()
        endif()
    endif()
    set("${result}" OFF PARENT_SCOPE)
endfunction()

# Handle upgrading logic
if(CMKR_EXECUTABLE AND NOT CMKR_CACHED_EXECUTABLE STREQUAL CMKR_EXECUTABLE)
    cmkr_startswith("${CMKR_EXECUTABLE}" "${CMAKE_CURRENT_BINARY_DIR}/_cmkr" CMKR_STARTSWITH_BUILD)
    cmkr_startswith("${CMKR_EXECUTABLE}" "${CMKR_DIRECTORY_PREFIX}" CMKR_STARTSWITH_CACHE)
    if(CMKR_STARTSWITH_BUILD)
        if(DEFINED ENV{CMKR_CACHE})
            message(AUTHOR_WARNING "[cmkr] Switching to cached cmkr: '${CMKR_CACHED_EXECUTABLE}'")
            if(EXISTS "${CMKR_CACHED_EXECUTABLE}")
                set(CMKR_EXECUTABLE "${CMKR_CACHED_EXECUTABLE}" CACHE FILEPATH "Full path to cmkr executable" FORCE)
            else()
                unset(CMKR_EXECUTABLE CACHE)
            endif()
        else()
            message(AUTHOR_WARNING "[cmkr] Upgrading '${CMKR_EXECUTABLE}' to '${CMKR_CACHED_EXECUTABLE}'")
            unset(CMKR_EXECUTABLE CACHE)
        endif()
    elseif(DEFINED ENV{CMKR_CACHE} AND CMKR_STARTSWITH_CACHE)
        message(AUTHOR_WARNING "[cmkr] Upgrading cached '${CMKR_EXECUTABLE}' to '${CMKR_CACHED_EXECUTABLE}'")
        unset(CMKR_EXECUTABLE CACHE)
    endif()
endif()

if(CMKR_EXECUTABLE AND EXISTS "${CMKR_EXECUTABLE}")
    message(VERBOSE "[cmkr] Found cmkr: '${CMKR_EXECUTABLE}'")
elseif(CMKR_EXECUTABLE AND NOT CMKR_EXECUTABLE STREQUAL CMKR_CACHED_EXECUTABLE)
    message(FATAL_ERROR "[cmkr] '${CMKR_EXECUTABLE}' not found")
elseif(NOT CMKR_EXECUTABLE AND EXISTS "${CMKR_CACHED_EXECUTABLE}")
    set(CMKR_EXECUTABLE "${CMKR_CACHED_EXECUTABLE}" CACHE FILEPATH "Full path to cmkr executable" FORCE)
    message(STATUS "[cmkr] Found cached cmkr: '${CMKR_EXECUTABLE}'")
else()
    set(CMKR_EXECUTABLE "${CMKR_CACHED_EXECUTABLE}" CACHE FILEPATH "Full path to cmkr executable" FORCE)
    message(VERBOSE "[cmkr] Bootstrapping '${CMKR_EXECUTABLE}'")

    message(STATUS "[cmkr] Fetching cmkr...")
    if(EXISTS "${CMKR_DIRECTORY}")
        cmkr_exec("${CMAKE_COMMAND}" -E rm -rf "${CMKR_DIRECTORY}")
    endif()
    find_package(Git QUIET REQUIRED)
    cmkr_exec("${GIT_EXECUTABLE}"
        clone
        --config advice.detachedHead=false
        --branch ${CMKR_TAG}
        --depth 1
        ${CMKR_REPO}
        "${CMKR_DIRECTORY}"
    )
    if(CMKR_COMMIT_HASH)
        execute_process(
            COMMAND "${GIT_EXECUTABLE}" checkout -q "${CMKR_COMMIT_HASH}"
            RESULT_VARIABLE CMKR_EXEC_RESULT
            WORKING_DIRECTORY "${CMKR_DIRECTORY}"
        )
        if(NOT CMKR_EXEC_RESULT EQUAL 0)
            message(FATAL_ERROR "Tag '${CMKR_TAG}' hash is not '${CMKR_COMMIT_HASH}'")
        endif()
    endif()
    message(STATUS "[cmkr] Building cmkr (using system compiler)...")
    cmkr_exec("${CMAKE_COMMAND}"
        --no-warn-unused-cli
        "${CMKR_DIRECTORY}"
        "-B${CMKR_DIRECTORY}/build"
        "-DCMAKE_BUILD_TYPE=${CMKR_BUILD_TYPE}"
        "-DCMAKE_UNITY_BUILD=ON"
        "-DCMAKE_INSTALL_PREFIX=${CMKR_DIRECTORY}"
        "-DCMKR_GENERATE_DOCUMENTATION=OFF"
    )
    cmkr_exec("${CMAKE_COMMAND}"
        --build "${CMKR_DIRECTORY}/build"
        --config "${CMKR_BUILD_TYPE}"
        --parallel
    )
    cmkr_exec("${CMAKE_COMMAND}"
        --install "${CMKR_DIRECTORY}/build"
        --config "${CMKR_BUILD_TYPE}"
        --prefix "${CMKR_DIRECTORY}"
        --component cmkr
    )
    if(NOT EXISTS ${CMKR_EXECUTABLE})
        message(FATAL_ERROR "[cmkr] Failed to bootstrap '${CMKR_EXECUTABLE}'")
    endif()
    cmkr_exec("${CMKR_EXECUTABLE}" version)
    message(STATUS "[cmkr] Bootstrapped ${CMKR_EXECUTABLE}")
endif()
execute_process(COMMAND "${CMKR_EXECUTABLE}" version
    RESULT_VARIABLE CMKR_EXEC_RESULT
)
if(NOT CMKR_EXEC_RESULT EQUAL 0)
    message(FATAL_ERROR "[cmkr] Failed to get version, try clearing the cache and rebuilding")
endif()

# Use cmkr.cmake as a script
if(CMAKE_SCRIPT_MODE_FILE)
    if(NOT EXISTS "${CMAKE_SOURCE_DIR}/cmake.toml")
        execute_process(COMMAND "${CMKR_EXECUTABLE}" init
            RESULT_VARIABLE CMKR_EXEC_RESULT
        )
        if(NOT CMKR_EXEC_RESULT EQUAL 0)
            message(FATAL_ERROR "[cmkr] Failed to bootstrap cmkr project. Please report an issue: https://github.com/build-cpp/cmkr/issues/new")
        else()
            message(STATUS "[cmkr] Modify cmake.toml and then configure using: cmake -B build")
        endif()
    else()
        execute_process(COMMAND "${CMKR_EXECUTABLE}" gen
            RESULT_VARIABLE CMKR_EXEC_RESULT
        )
        if(NOT CMKR_EXEC_RESULT EQUAL 0)
            message(FATAL_ERROR "[cmkr] Failed to generate project.")
        else()
            message(STATUS "[cmkr] Configure using: cmake -B build")
        endif()
    endif()
endif()

# This is the macro that contains black magic
macro(cmkr)
    # When this macro is called from the generated file, fake some internal CMake variables
    get_source_file_property(CMKR_CURRENT_LIST_FILE "${CMAKE_CURRENT_LIST_FILE}" CMKR_CURRENT_LIST_FILE)
    if(CMKR_CURRENT_LIST_FILE)
        set(CMAKE_CURRENT_LIST_FILE "${CMKR_CURRENT_LIST_FILE}")
        get_filename_component(CMAKE_CURRENT_LIST_DIR "${CMAKE_CURRENT_LIST_FILE}" DIRECTORY)
    endif()

    # File-based include guard (include_guard is not documented to work)
    get_source_file_property(CMKR_INCLUDE_GUARD "${CMAKE_CURRENT_LIST_FILE}" CMKR_INCLUDE_GUARD)
    if(NOT CMKR_INCLUDE_GUARD)
        set_source_files_properties("${CMAKE_CURRENT_LIST_FILE}" PROPERTIES CMKR_INCLUDE_GUARD TRUE)

        file(SHA256 "${CMAKE_CURRENT_LIST_FILE}" CMKR_LIST_FILE_SHA256_PRE)

        # Generate CMakeLists.txt
        cmkr_exec("${CMKR_EXECUTABLE}" gen
            WORKING_DIRECTORY "${CMAKE_CURRENT_SOURCE_DIR}"
        )

        file(SHA256 "${CMAKE_CURRENT_LIST_FILE}" CMKR_LIST_FILE_SHA256_POST)

        # Delete the temporary file if it was left for some reason
        set(CMKR_TEMP_FILE "${CMAKE_CURRENT_SOURCE_DIR}/CMakerLists.txt")
        if(EXISTS "${CMKR_TEMP_FILE}")
            file(REMOVE "${CMKR_TEMP_FILE}")
        endif()

        if(NOT CMKR_LIST_FILE_SHA256_PRE STREQUAL CMKR_LIST_FILE_SHA256_POST)
            # Copy the now-generated CMakeLists.txt to CMakerLists.txt
            # This is done because you cannot include() a file you are currently in
            configure_file(CMakeLists.txt "${CMKR_TEMP_FILE}" COPYONLY)

            # Add the macro required for the hack at the start of the cmkr macro
            set_source_files_properties("${CMKR_TEMP_FILE}" PROPERTIES
                CMKR_CURRENT_LIST_FILE "${CMAKE_CURRENT_LIST_FILE}"
            )

            # 'Execute' the newly-generated CMakeLists.txt
            include("${CMKR_TEMP_FILE}")

            # Delete the generated file
            file(REMOVE "${CMKR_TEMP_FILE}")

            # Do not execute the rest of the original CMakeLists.txt
            return()
        endif()
        # Resume executing the unmodified CMakeLists.txt
    endif()
endmacro()

```

`include/zyemu/types.hpp`:

```hpp
#pragma once

#include <cstddef>
#include <cstdint>
#include <variant>

namespace zyemu
{
    enum class StatusCode : std::uint32_t
    {
        success,
        invalidOperation,
        invalidArgument,
        invalidState,
        invalidInstruction,
        invalidRegister,
        invalidMemory,
        invalidThread,
        invalidCallback,
        invalidMode,
        invalidAddress,
        invalidSize,
        invalidAccess,
        invalidAlignment,
        invalidLength,
        invalidBuffer,
        invalidUserData,
        invalidInstructionPointer,
        invalidStackPointer,
        invalidFramePointer,
        invalidBasePointer,
        invalidSegment,
        invalidFlags,
        invalidRounding,
        invalidMasking,
        invalidBroadcast,
        labelAlreadyBound,
        bufferTooSmall,
        outOfMemory,
    };

    template<typename TResult> struct Result
    {
        using TResultReal = std::conditional_t<std::is_void_v<TResult>, std::monostate, TResult>;

        std::variant<TResultReal, StatusCode> value{};

        constexpr Result() = default;

        constexpr Result(const TResult& value)
            : value(value)
        {
        }

        constexpr Result(TResult&& value)
            : value(std::move(value))
        {
        }

        constexpr Result(StatusCode error)
            : value{ error }
        {
        }

        constexpr bool hasValue() const
        {
            return !hasError();
        }

        constexpr bool hasError() const
        {
            return std::holds_alternative<StatusCode>(value);
        }

        constexpr TResult& getValue()
        {
            assert(hasValue());
            return std::get<TResult>(value);
        }

        constexpr const TResult& getValue() const
        {
            return std::get<TResult>(value);
        }

        constexpr StatusCode& getError()
        {
            return std::get<StatusCode>(value);
        }

        constexpr const StatusCode& getError() const
        {
            return std::get<StatusCode>(value);
        }

        constexpr operator bool() const
        {
            return hasValue();
        }

        constexpr TResult& operator*()
        {
            return getValue();
        }

        constexpr const TResult& operator*() const
        {
            return getValue();
        }

        constexpr TResult* operator->()
        {
            return &getValue();
        }

        constexpr const TResult* operator->() const
        {
            return &getValue();
        }
    };

    enum class ThreadId : std::uint32_t
    {
        invalid = 0xFFFFFFFFU,
    };

    // TODO: Move this into platform.hpp
#ifdef _WIN32
#    ifdef _MSC_VER
#        define ZYEMU_FASTCALL __fastcall
#    else
#        define ZYEMU_FASTCALL __attribute__((fastcall))
#    endif
#else
#    define ZYEMU_FASTCALL
    static_assert(false, "Unsupported platform");
#endif

    using MemoryReadHandler = StatusCode(ZYEMU_FASTCALL*)(
        ThreadId tid, uint64_t address, void* buffer, size_t length, void* userData);

    using MemoryWriteHandler = StatusCode(ZYEMU_FASTCALL*)(
        ThreadId tid, uint64_t address, const void* buffer, size_t length, void* userData);

} // namespace zyemu
```

`include/zyemu/zyemu.hpp`:

```hpp
#pragma once

#include <Zydis/Register.h>
#include <Zydis/SharedTypes.h>
#include <cstddef>
#include <cstdint>
#include <span>
#include <zyemu/types.hpp>

namespace zyemu
{
    namespace detail
    {
        struct CPUState;
    }

    class CPU
    {
        detail::CPUState* state{};

    public:
        CPU();
        ~CPU();

        StatusCode setMode(ZydisMachineMode mode);

        void setMemReadHandler(MemoryReadHandler callback, void* userData);

        void setMemWriteHandler(MemoryWriteHandler callback, void* userData);

        void clearCodeCache();

        ThreadId createThread();

        void destroyThread(ThreadId tid);

        StatusCode setRegData(ThreadId tid, ZydisRegister reg, std::span<const std::uint8_t> data);

        template<typename T> StatusCode setRegValue(ThreadId tid, ZydisRegister reg, T value)
        {
            return setRegData(tid, reg, { reinterpret_cast<const std::uint8_t*>(&value), sizeof(T) });
        }

        StatusCode getRegData(ThreadId tid, ZydisRegister reg, std::span<std::uint8_t> buffer);

        template<typename T> StatusCode getRegValue(ThreadId tid, ZydisRegister reg, T& value)
        {
            return getRegData(tid, reg, { reinterpret_cast<std::uint8_t*>(&value), sizeof(T) });
        }

        StatusCode step(ThreadId tid);
    };

} // namespace zyemu

```

`src/playground/main.cpp`:

```cpp
#include <array>
#include <chrono>
#include <print>
#include <zyemu/zyemu.hpp>

static constexpr std::uint64_t kShellCodeAddress = 0x0000000140007314;
static std::uint8_t kShellCode[0x1000] = {};

static constexpr std::uint64_t kStackAddress = 0x00007FFB67A49000;
static constexpr std::uint64_t kStackBaseOffset = 0x500;
static constexpr std::uint64_t kStackBase = kStackAddress + kStackBaseOffset;
static std::uint8_t kStackSpace[0x1000] = {};

zyemu::StatusCode memReadHandler(zyemu::ThreadId tid, std::uint64_t readAddress, void* dst, std::size_t size, void* userData)
{
    if (readAddress >= kShellCodeAddress && (readAddress + size) <= (kShellCodeAddress + std::size(kShellCode)))
    {
        const auto offset = readAddress - kShellCodeAddress;
        std::memcpy(dst, kShellCode + offset, size);
        return zyemu::StatusCode::success;
    }
    else if (readAddress >= kStackAddress && (readAddress + size) <= (kStackAddress + std::size(kStackSpace)))
    {
        const auto offset = readAddress - kStackAddress;
        std::memcpy(dst, kStackSpace + offset, size);
        return zyemu::StatusCode::success;
    }

    return zyemu::StatusCode::invalidMemory;
}

zyemu::StatusCode memWriteHandler(
    zyemu::ThreadId tid, std::uint64_t writeAddress, const void* src, std::size_t size, void* userData)
{
    if (writeAddress >= kShellCodeAddress && (writeAddress + size) <= (kShellCodeAddress + std::size(kShellCode)))
    {
        const auto offset = writeAddress - kShellCodeAddress;
        std::memcpy(kShellCode + offset, src, size);
        return zyemu::StatusCode::success;
    }
    else if (writeAddress >= kStackAddress && (writeAddress + size) <= (kStackAddress + std::size(kStackSpace)))
    {
        const auto offset = writeAddress - kStackAddress;
        std::memcpy(kStackSpace + offset, src, size);
        return zyemu::StatusCode::success;
    }

    return zyemu::StatusCode::invalidMemory;
}

static void testBasicMov()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0x49, 0x89, 0xC4, // mov r12, rax
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    std::uint64_t testValue{ 0x1AF20384ECAB27F };

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();

    ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RAX, testValue);

    auto status = ctx.step(th1);
    if (status != zyemu::StatusCode::success)
    {
        assert(false);
    }

    std::uint64_t rip{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);

    assert(rip == kShellCodeAddress + sizeof(kTestShellCode));

    std::uint64_t r12{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_R12, r12);

    assert(r12 == testValue);
}

static void testMemoryRead()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0x48, 0x8B, 0x04, 0x24, // mov rax, qword ptr ss:[rsp]
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    std::uint64_t testValue{ 0x1AF20384ECAB27F };
    std::memcpy(kStackSpace + kStackBaseOffset, &testValue, sizeof(testValue));

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();

    ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);

    auto status = ctx.step(th1);
    if (status != zyemu::StatusCode::success)
    {
        assert(false);
    }

    std::uint64_t rip{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);

    assert(rip == kShellCodeAddress + sizeof(kTestShellCode));

    std::uint64_t rax{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RAX, rax);

    assert(rax == testValue);
}

static void testMemoryWrite()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0x48, 0x89, 0x04, 0x24, // mov qword ptr ss:[rsp], rax
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    std::memset(kStackSpace, 0xCC, sizeof(kStackSpace));

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();

    ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);

    std::uint64_t testValue{ 0x1AF20384ECAB27F };
    ctx.setRegValue(th1, ZYDIS_REGISTER_RAX, testValue);

    auto status = ctx.step(th1);
    if (status != zyemu::StatusCode::success)
    {
        assert(false);
    }

    std::uint64_t rip{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);

    assert(rip == kShellCodeAddress + sizeof(kTestShellCode));

    std::uint64_t rax{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RAX, rax);

    assert(rax == testValue);

    std::uint64_t stackValue{};
    std::memcpy(&stackValue, kStackSpace + kStackBaseOffset, sizeof(stackValue));

    assert(stackValue == testValue);
}

static void testBranchJnz()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0x0F, 0x85, 0x7E, 0xFE, 0xFF, 0xFF, // jnz 0x0000000140007198
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();

    // Test with ZF == 0.
    {
        std::uint32_t flags = 0;

        ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
        ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);
        ctx.setRegValue(th1, ZYDIS_REGISTER_EFLAGS, flags);

        auto status = ctx.step(th1);
        if (status != zyemu::StatusCode::success)
        {
            assert(false);
        }

        std::uint64_t rip{};
        ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);

        assert(rip == 0x0000000140007198);
    }

    // Test with ZF == 1.
    {
        std::uint32_t flags = (1U << 6);

        ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
        ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);
        ctx.setRegValue(th1, ZYDIS_REGISTER_EFLAGS, flags);

        auto status = ctx.step(th1);
        if (status != zyemu::StatusCode::success)
        {
            assert(false);
        }

        std::uint64_t rip{};
        ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);

        assert(rip == kShellCodeAddress + sizeof(kTestShellCode));
    }
}

static void testPushReg64()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0x50, // push rax
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    std::uint64_t testValue{ 0x1AF20384ECAB27F };

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();
    ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RAX, testValue);

    auto status = ctx.step(th1);
    if (status != zyemu::StatusCode::success)
    {
        assert(false);
    }

    std::uint64_t rip{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);
    assert(rip == kShellCodeAddress + sizeof(kTestShellCode));

    std::uint64_t rsp{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RSP, rsp);
    assert(rsp == kStackBase - 8);

    std::uint64_t stackValue{};
    memReadHandler(th1, rsp, &stackValue, sizeof(stackValue), nullptr);
    assert(stackValue == testValue);
}

static void testPopReg64()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0x58, // pop rax
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    std::uint64_t testValue{ 0x1AF20384ECAB27F };

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();
    ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);

    memWriteHandler(th1, kStackBase, &testValue, sizeof(testValue), nullptr);
    auto status = ctx.step(th1);
    if (status != zyemu::StatusCode::success)
    {
        assert(false);
    }

    std::uint64_t rip{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);
    assert(rip == kShellCodeAddress + sizeof(kTestShellCode));

    std::uint64_t rsp{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RSP, rsp);
    assert(rsp == kStackBase + 8);

    std::uint64_t rax{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RAX, rax);
    assert(rax == testValue);
}

static void testCall()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0xE8, 0x00, 0x00, 0x00, 0x00, // call 0x0000000140007198
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();
    ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);

    auto status = ctx.step(th1);
    if (status != zyemu::StatusCode::success)
    {
        assert(false);
    }

    std::uint64_t rip{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);
    assert(rip == kShellCodeAddress + sizeof(kTestShellCode));

    std::uint64_t rsp{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RSP, rsp);
    assert(rsp == kStackBase - 8);

    std::uint64_t stackValue{};
    memReadHandler(th1, rsp, &stackValue, sizeof(stackValue), nullptr);
    assert(stackValue == kShellCodeAddress + sizeof(kTestShellCode));
}

static void testRet()
{
    constexpr std::uint8_t kTestShellCode[] = {
        0xC3, // ret
    };

    std::memcpy(kShellCode, kTestShellCode, sizeof(kTestShellCode));

    zyemu::CPU ctx{};
    ctx.setMode(ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64);
    ctx.setMemReadHandler(memReadHandler, nullptr);
    ctx.setMemWriteHandler(memWriteHandler, nullptr);

    auto th1 = ctx.createThread();
    ctx.setRegValue(th1, ZYDIS_REGISTER_RSP, kStackBase);
    ctx.setRegValue(th1, ZYDIS_REGISTER_RIP, kShellCodeAddress);

    // Push return address.
    const std::uint64_t testRetAddr = 0x0000000150007198;
    memWriteHandler(th1, kStackBase, &testRetAddr, sizeof(testRetAddr), nullptr);

    auto status = ctx.step(th1);
    if (status != zyemu::StatusCode::success)
    {
        assert(false);
    }

    std::uint64_t rip{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RIP, rip);
    assert(rip == testRetAddr);

    std::uint64_t rsp{};
    ctx.getRegValue(th1, ZYDIS_REGISTER_RSP, rsp);
    assert(rsp == kStackBase + 8);
}

int main()
{
    testRet();
    testCall();
    testPopReg64();
    testPushReg64();
    testMemoryRead();
    testBranchJnz();
    testBasicMov();
    testMemoryWrite();

    return 0;
}
```

`src/zyemu/assembler.cpp`:

```cpp
#include "assembler.hpp"

#include <Zydis/Encoder.h>
#include <map>

namespace zyemu::x86
{
    struct NodeData
    {
        std::uint64_t address{};
        std::uint8_t size{};
    };

    struct EncodeState
    {
        ZydisMachineMode mode{};
        std::map<std::int32_t, std::size_t> labelMap{};
        std::vector<std::uint32_t> nodeSize{};
        std::uint64_t baseAddress{};
        std::uint64_t currentAddress{};
        std::vector<std::uint8_t> buffer;
    };

    struct EncodeInfo
    {
        std::uint32_t size{};
        bool needsPass{};
    };

    static Result<EncodeInfo> handleNode(EncodeState& state, const Label& label)
    {
        assert(label.isValid());

        state.labelMap[label.id] = state.currentAddress;
        return EncodeInfo{ 0, false };
    }

    static inline bool isBranchingInstr(ZydisMnemonic mnemonic)
    {
        switch (mnemonic)
        {
            case ZYDIS_MNEMONIC_CALL:
            case ZYDIS_MNEMONIC_JB:
            case ZYDIS_MNEMONIC_JBE:
            case ZYDIS_MNEMONIC_JCXZ:
            case ZYDIS_MNEMONIC_JECXZ:
            case ZYDIS_MNEMONIC_JKNZD:
            case ZYDIS_MNEMONIC_JKZD:
            case ZYDIS_MNEMONIC_JL:
            case ZYDIS_MNEMONIC_JLE:
            case ZYDIS_MNEMONIC_JMP:
            case ZYDIS_MNEMONIC_JNB:
            case ZYDIS_MNEMONIC_JNBE:
            case ZYDIS_MNEMONIC_JNL:
            case ZYDIS_MNEMONIC_JNLE:
            case ZYDIS_MNEMONIC_JNO:
            case ZYDIS_MNEMONIC_JNP:
            case ZYDIS_MNEMONIC_JNS:
            case ZYDIS_MNEMONIC_JNZ:
            case ZYDIS_MNEMONIC_JO:
            case ZYDIS_MNEMONIC_JP:
            case ZYDIS_MNEMONIC_JRCXZ:
            case ZYDIS_MNEMONIC_JS:
            case ZYDIS_MNEMONIC_JZ:
                return true;
        }
        return false;
    }

    static Result<EncodeInfo> handleNode(EncodeState& state, const Instruction& instr)
    {
        ZydisEncoderRequest request{};
        request.machine_mode = state.mode;
        request.mnemonic = instr.mnemonic;
        request.operand_count = instr.operands.size();
        request.branch_type = ZYDIS_BRANCH_TYPE_NONE;

        bool needsPass = false;
        for (std::size_t i = 0; i < instr.operands.size(); i++)
        {
            const auto op = instr.operands[i];
            auto& dstOp = request.operands[i];
            if (const auto* opReg = std::get_if<x86::Reg>(&op); opReg != nullptr)
            {
                dstOp.type = ZYDIS_OPERAND_TYPE_REGISTER;
                dstOp.reg.value = opReg->value;

                assert(opReg->value != ZYDIS_REGISTER_NONE);
            }
            else if (const auto* opMem = std::get_if<x86::Mem>(&op); opMem != nullptr)
            {
                dstOp.type = ZYDIS_OPERAND_TYPE_MEMORY;
                dstOp.mem.size = opMem->bitSize / 8;
                dstOp.mem.base = opMem->base.value;
                dstOp.mem.index = opMem->index.value;
                dstOp.mem.scale = opMem->scale;
                dstOp.mem.displacement = opMem->disp;
                if (opMem->label.id != -1)
                {
                    if (auto it = state.labelMap.find(opMem->label.id); it == state.labelMap.end())
                    {
                        dstOp.mem.displacement += state.currentAddress + 0x12345;
                        needsPass = true;
                    }
                    else
                    {
                        dstOp.mem.displacement += it->second;
                    }
                }
            }
            else if (const auto* opImm = std::get_if<x86::Imm>(&op); opImm != nullptr)
            {
                dstOp.type = ZYDIS_OPERAND_TYPE_IMMEDIATE;
                dstOp.imm.s = opImm->value;
            }
            else if (const auto* opLabel = std::get_if<x86::Label>(&op); opLabel != nullptr)
            {
                dstOp.type = ZYDIS_OPERAND_TYPE_IMMEDIATE;
                assert(opLabel->isValid());

                if (auto it = state.labelMap.find(opLabel->id); it == state.labelMap.end())
                {
                    needsPass = true;
                    dstOp.imm.s = state.currentAddress + 0x12345;
                }
                else
                {
                    dstOp.imm.s = it->second;
                }
            }
        }

        std::uint8_t buf[16]{};
        std::size_t bufSize = sizeof(buf);

        auto res = ZydisEncoderEncodeInstructionAbsolute(&request, buf, &bufSize, state.currentAddress);
        if (res != ZYAN_STATUS_SUCCESS)
        {
            return StatusCode::invalidInstruction;
        }

        state.buffer.insert(state.buffer.end(), buf, buf + bufSize);

        return EncodeInfo{ static_cast<std::uint32_t>(bufSize), needsPass };
    }

    Result<std::size_t> Assembler::finalize(
        ZydisMachineMode mode, std::uint64_t baseAddress, std::uint8_t* buffer, std::size_t bufSize)
    {
        EncodeState state{};
        state.mode = mode;
        state.baseAddress = baseAddress;
        state.nodeSize.resize(_nodes.size());

        bool needsPass = true;
        while (needsPass)
        {
            needsPass = false;

            state.currentAddress = baseAddress;
            state.buffer.clear();

            for (std::size_t i = 0; i < _nodes.size(); i++)
            {
                const auto& node = this->_nodes[i];

                auto encodeInfo = std::visit([&](const auto& data) { return handleNode(state, data); }, node);
                if (!encodeInfo)
                {
                    return encodeInfo.getError();
                }

                needsPass |= encodeInfo->needsPass;

                const auto oldNodeSize = state.nodeSize[i];
                if (oldNodeSize != 0 && oldNodeSize != encodeInfo->size)
                {
                    needsPass = true;
                }

                state.nodeSize[i] = encodeInfo->size;
                state.currentAddress += encodeInfo->size;
            }
        }

        if (state.buffer.size() > bufSize)
        {
            return StatusCode::bufferTooSmall;
        }

        std::memcpy(buffer, state.buffer.data(), state.buffer.size());

        return state.buffer.size();
    }

} // namespace zyemu::x86
```

`src/zyemu/assembler.hpp`:

```hpp
#pragma once

#include <Zydis/Encoder.h>
#include <Zydis/Mnemonic.h>
#include <Zydis/Register.h>
#include <array>
#include <cstddef>
#include <cstdint>
#include <sfl/static_vector.hpp>
#include <variant>
#include <vector>
#include <zyemu/types.hpp>

namespace zyemu::x86
{
    struct Reg
    {
        ZydisRegister value{ ZYDIS_REGISTER_NONE };

        constexpr bool isGp() const
        {
            return value >= ZYDIS_REGISTER_AL && value <= ZYDIS_REGISTER_R15;
        }

        constexpr bool isGp8() const
        {
            return value >= ZYDIS_REGISTER_AL && value <= ZYDIS_REGISTER_R15B;
        }

        constexpr bool isGp16() const
        {
            return value >= ZYDIS_REGISTER_AX && value <= ZYDIS_REGISTER_R15W;
        }

        constexpr bool isGp32() const
        {
            return value >= ZYDIS_REGISTER_EAX && value <= ZYDIS_REGISTER_R15D;
        }

        constexpr bool isGp64() const
        {
            return value >= ZYDIS_REGISTER_RAX && value <= ZYDIS_REGISTER_R15;
        }

        constexpr auto operator<=>(const Reg&) const = default;
    };

    struct Gp : public Reg
    {
    };

    struct Gp8 : public Gp
    {
    };

    struct Gp16 : public Gp
    {
    };

    struct Gp32 : public Gp
    {
    };

    struct Gp64 : public Gp
    {
    };

    struct Seg : public Reg
    {
    };

    struct Label
    {
        std::int32_t id{ -1 };

        constexpr bool isValid() const
        {
            return id != -1;
        }
    };

    struct Mem
    {
        std::uint16_t bitSize{};
        Seg seg{};
        Reg base{};
        Reg index{};
        std::int64_t disp{};
        std::uint8_t scale{};
        Label label{};
    };

    struct Imm
    {
        std::int64_t value{};

        constexpr Imm() = default;

        template<typename T>
        constexpr Imm(T value)
            : value{ static_cast<std::int64_t>(value) }
        {
        }
    };

    using Operand = std::variant<Reg, Mem, Label, Imm>;

    struct Instruction
    {
        ZydisMnemonic mnemonic{};
        sfl::static_vector<Operand, ZYDIS_ENCODER_MAX_OPERANDS> operands{};
    };

    // Segment regs.
    static constexpr Seg es{ ZYDIS_REGISTER_ES };
    static constexpr Seg cs{ ZYDIS_REGISTER_CS };
    static constexpr Seg ss{ ZYDIS_REGISTER_SS };
    static constexpr Seg ds{ ZYDIS_REGISTER_DS };
    static constexpr Seg fs{ ZYDIS_REGISTER_FS };
    static constexpr Seg gs{ ZYDIS_REGISTER_GS };

    // Gp32
    static constexpr Gp32 eax{ ZYDIS_REGISTER_EAX };
    static constexpr Gp32 ecx{ ZYDIS_REGISTER_ECX };
    static constexpr Gp32 edx{ ZYDIS_REGISTER_EDX };
    static constexpr Gp32 ebx{ ZYDIS_REGISTER_EBX };
    static constexpr Gp32 esp{ ZYDIS_REGISTER_ESP };
    static constexpr Gp32 ebp{ ZYDIS_REGISTER_EBP };
    static constexpr Gp32 esi{ ZYDIS_REGISTER_ESI };
    static constexpr Gp32 edi{ ZYDIS_REGISTER_EDI };

    // Gp64
    static constexpr Gp64 rax{ ZYDIS_REGISTER_RAX };
    static constexpr Gp64 rcx{ ZYDIS_REGISTER_RCX };
    static constexpr Gp64 rdx{ ZYDIS_REGISTER_RDX };
    static constexpr Gp64 rbx{ ZYDIS_REGISTER_RBX };
    static constexpr Gp64 rsp{ ZYDIS_REGISTER_RSP };
    static constexpr Gp64 rbp{ ZYDIS_REGISTER_RBP };
    static constexpr Gp64 rsi{ ZYDIS_REGISTER_RSI };
    static constexpr Gp64 rdi{ ZYDIS_REGISTER_RDI };
    static constexpr Gp64 r8{ ZYDIS_REGISTER_R8 };
    static constexpr Gp64 r9{ ZYDIS_REGISTER_R9 };
    static constexpr Gp64 r10{ ZYDIS_REGISTER_R10 };
    static constexpr Gp64 r11{ ZYDIS_REGISTER_R11 };
    static constexpr Gp64 r12{ ZYDIS_REGISTER_R12 };
    static constexpr Gp64 r13{ ZYDIS_REGISTER_R13 };
    static constexpr Gp64 r14{ ZYDIS_REGISTER_R14 };
    static constexpr Gp64 r15{ ZYDIS_REGISTER_R15 };
    static constexpr Gp64 rip{ ZYDIS_REGISTER_RIP };

    // Xmm regs.
    static constexpr Reg xmm0{ ZYDIS_REGISTER_XMM0 };
    static constexpr Reg xmm1{ ZYDIS_REGISTER_XMM1 };
    static constexpr Reg xmm2{ ZYDIS_REGISTER_XMM2 };
    static constexpr Reg xmm3{ ZYDIS_REGISTER_XMM3 };
    static constexpr Reg xmm4{ ZYDIS_REGISTER_XMM4 };
    static constexpr Reg xmm5{ ZYDIS_REGISTER_XMM5 };
    static constexpr Reg xmm6{ ZYDIS_REGISTER_XMM6 };
    static constexpr Reg xmm7{ ZYDIS_REGISTER_XMM7 };
    static constexpr Reg xmm8{ ZYDIS_REGISTER_XMM8 };
    static constexpr Reg xmm9{ ZYDIS_REGISTER_XMM9 };
    static constexpr Reg xmm10{ ZYDIS_REGISTER_XMM10 };
    static constexpr Reg xmm11{ ZYDIS_REGISTER_XMM11 };
    static constexpr Reg xmm12{ ZYDIS_REGISTER_XMM12 };
    static constexpr Reg xmm13{ ZYDIS_REGISTER_XMM13 };
    static constexpr Reg xmm14{ ZYDIS_REGISTER_XMM14 };
    static constexpr Reg xmm15{ ZYDIS_REGISTER_XMM15 };
    static constexpr Reg xmm16{ ZYDIS_REGISTER_XMM16 };
    static constexpr Reg xmm17{ ZYDIS_REGISTER_XMM17 };
    static constexpr Reg xmm18{ ZYDIS_REGISTER_XMM18 };
    static constexpr Reg xmm19{ ZYDIS_REGISTER_XMM19 };
    static constexpr Reg xmm20{ ZYDIS_REGISTER_XMM20 };
    static constexpr Reg xmm21{ ZYDIS_REGISTER_XMM21 };
    static constexpr Reg xmm22{ ZYDIS_REGISTER_XMM22 };
    static constexpr Reg xmm23{ ZYDIS_REGISTER_XMM23 };
    static constexpr Reg xmm24{ ZYDIS_REGISTER_XMM24 };
    static constexpr Reg xmm25{ ZYDIS_REGISTER_XMM25 };
    static constexpr Reg xmm26{ ZYDIS_REGISTER_XMM26 };
    static constexpr Reg xmm27{ ZYDIS_REGISTER_XMM27 };
    static constexpr Reg xmm28{ ZYDIS_REGISTER_XMM28 };
    static constexpr Reg xmm29{ ZYDIS_REGISTER_XMM29 };
    static constexpr Reg xmm30{ ZYDIS_REGISTER_XMM30 };
    static constexpr Reg xmm31{ ZYDIS_REGISTER_XMM31 };

    // Memory helpers.
    inline Mem qword_ptr(const Reg& base, std::int64_t disp = 0)
    {
        return Mem{ 64, ds, base, {}, disp, 0, {} };
    }

    inline Mem dword_ptr(const Reg& base, std::int64_t disp = 0)
    {
        return Mem{ 32, ds, base, {}, disp, 0, {} };
    }

    inline Mem ptr(std::uint16_t size, const Reg& base, std::int64_t disp = 0)
    {
        return Mem{ size, ds, base, {}, disp, 0, {} };
    }

    class Assembler
    {
        using Node = std::variant<Instruction, Label>;

        std::vector<Node> _nodes;
        std::int32_t labelId{};

    public:
        Label createLabel()
        {
            return Label{ labelId++ };
        }

        Assembler& bind(Label label)
        {
            _nodes.push_back(label);
            return *this;
        }

        template<typename... TOperands> Assembler& emit(const Instruction& instr)
        {
            _nodes.push_back(instr);
            return *this;
        }

        template<typename... TOperands> Assembler& emit(ZydisMnemonic mnemonic, TOperands&&... operands)
        {
            _nodes.push_back(Instruction{ mnemonic, { std::forward<TOperands>(operands)... } });
            return *this;
        }

        template<typename Op0, typename Op1> Assembler& mov(const Op0& dst, const Op1& src)
        {
            return emit(ZYDIS_MNEMONIC_MOV, dst, src);
        }

        template<typename Op0, typename Op1> Assembler& sub(const Op0& dst, const Op1& src)
        {
            return emit(ZYDIS_MNEMONIC_SUB, dst, src);
        }

        template<typename Op0, typename Op1> Assembler& add(const Op0& dst, const Op1& src)
        {
            return emit(ZYDIS_MNEMONIC_ADD, dst, src);
        }

        template<typename Op0, typename Op1> Assembler& xor_(const Op0& dst, const Op1& src)
        {
            return emit(ZYDIS_MNEMONIC_XOR, dst, src);
        }

        template<typename Op0> Assembler& lea(const Op0& dst, const Mem& src)
        {
            return emit(ZYDIS_MNEMONIC_LEA, dst, src);
        }

        template<typename Op0, typename Op1> Assembler& test(const Op0& dst, const Op1& src)
        {
            return emit(ZYDIS_MNEMONIC_TEST, dst, src);
        }

        Assembler& jnz(const Label& label)
        {
            return emit(ZYDIS_MNEMONIC_JNZ, label);
        }

        Assembler& jz(const Label& label)
        {
            return emit(ZYDIS_MNEMONIC_JNZ, label);
        }

        Assembler& call(Imm imm)
        {
            return emit(ZYDIS_MNEMONIC_CALL, imm);
        }

        Assembler& call(Reg reg)
        {
            return emit(ZYDIS_MNEMONIC_CALL, reg);
        }

        Assembler& ret()
        {
            return emit(ZYDIS_MNEMONIC_RET);
        }

        Assembler& ret(Imm imm)
        {
            return emit(ZYDIS_MNEMONIC_RET, imm);
        }

        template<typename Op0> Assembler& push(const Op0& src)
        {
            return emit(ZYDIS_MNEMONIC_PUSH, src);
        }

        template<typename Op0> Assembler& pop(const Op0& dst)
        {
            return emit(ZYDIS_MNEMONIC_POP, dst);
        }

        Assembler& pushfq()
        {
            return emit(ZYDIS_MNEMONIC_PUSHFQ);
        }

        Assembler& popfq()
        {
            return emit(ZYDIS_MNEMONIC_POPFQ);
        }

        Result<std::size_t> finalize(
            ZydisMachineMode mode, std::uint64_t baseAddress, std::uint8_t* buffer, std::size_t bufSize);
    };

} // namespace zyemu::x86
```

`src/zyemu/codegen.cpp`:

```cpp
#include "codegen.hpp"

#include "assembler.hpp"
#include "internal.hpp"
#include "registers.hpp"

#include <Zydis/Decoder.h>
#include <cassert>
#include <sfl/static_flat_set.hpp>
#include <sfl/static_vector.hpp>

#ifndef WIN32_LEAN_AND_MEAN
#    define WIN32_LEAN_AND_MEAN
#endif
#include <Windows.h>

namespace zyemu::codecache
{
    using RegSet = sfl::static_flat_set<ZydisRegister, 8>;

    struct DecodedInstruction
    {
        std::uint64_t address{};
        ZydisDecodedInstruction data{};
        ZydisDecodedOperand operands[ZYDIS_MAX_OPERAND_COUNT]{};
        RegSet regsRead{};
        RegSet regsModified{};
        RegSet regsUsed{};
    };

    static const std::array kVolatileGpRegs = {
        x86::rdx, x86::r8, x86::r9, x86::r10, x86::r11,
    };

    static const std::array kNonVolatileGpRegs = {
        x86::rsi, x86::rdi, x86::rbp, x86::rbx, x86::r12, x86::r13, x86::r14, x86::r15,
    };

    static const std::array kVolatileXmmRegs = {
        x86::xmm0, x86::xmm1, x86::xmm2, x86::xmm3, x86::xmm4, x86::xmm5,
    };

    static constexpr std::int32_t kHomeAreaSize = 64;
    static constexpr std::int32_t kMemoryStackSize = 128;
    static constexpr std::int32_t kSpillAreaSize = 64;
    static constexpr std::int32_t kSpillAreaOffset = kHomeAreaSize + kMemoryStackSize;

    struct FastCallInfoWin64
    {
        static constexpr x86::Reg gpArg0 = x86::rcx;
        static constexpr x86::Reg gpArg1 = x86::rdx;
        static constexpr x86::Reg gpArg2 = x86::r8;
        static constexpr x86::Reg gpArg3 = x86::r9;
        static constexpr x86::Reg xmmArg0 = x86::xmm0;
        static constexpr x86::Reg xmmArg1 = x86::xmm1;
        static constexpr x86::Reg xmmArg2 = x86::xmm2;
        static constexpr x86::Reg xmmArg3 = x86::xmm3;
    };

    struct FastCallInfoWin32
    {
        static constexpr x86::Reg gpArg0 = x86::ecx;
        static constexpr x86::Reg gpArg1 = x86::edx;
    };

    // FIXME: This must match the host platform.
    using FastCallInfo = FastCallInfoWin64;

    struct GeneratorState
    {
        ZydisMachineMode mode{};
        x86::Assembler assembler;
        x86::Label lblExit;
        x86::Reg regCtx{};
        sfl::static_vector<x86::Reg, 16> freeVolatileGpRegs{ kVolatileGpRegs.begin(), kVolatileGpRegs.end() };
        sfl::static_vector<x86::Reg, 16> freeNonVolatileGpRegs{ kNonVolatileGpRegs.begin(), kNonVolatileGpRegs.end() };
        sfl::static_vector<x86::Reg, 16> freeXmmRegs{ kVolatileXmmRegs.begin(), kVolatileXmmRegs.end() };
        bool requiresExternalCalls{};
        std::int32_t usedStackSpaceSize{};

        std::map<ZydisRegister, x86::Reg> regMap{};

        x86::Reg allocGpReg(bool nonVolatile)
        {
            if (!nonVolatile)
            {
                assert(!freeVolatileGpRegs.empty());
                auto reg = freeVolatileGpRegs.back();
                freeVolatileGpRegs.pop_back();
                return reg;
            }
            else
            {
                assert(!freeNonVolatileGpRegs.empty());
                auto reg = freeNonVolatileGpRegs.back();
                freeNonVolatileGpRegs.pop_back();
                return reg;
            }
        }

        x86::Reg allocXmmReg()
        {
            assert(!freeXmmRegs.empty());
            auto reg = freeXmmRegs.back();
            freeXmmRegs.pop_back();
            return reg;
        }
    };

    static Result<detail::CacheRegion*> getCacheRegion(detail::CPUState* cpuState, std::size_t estimatedSize)
    {
        auto& cacheRegions = cpuState->cacheRegions;

        const auto allocRegion = [&]() {
            // Allocate a new region.
            auto* codeCacheMem = VirtualAlloc(nullptr, 0x1000, MEM_COMMIT | MEM_RESERVE, PAGE_EXECUTE_READWRITE);
            if (!codeCacheMem)
            {
                return StatusCode::outOfMemory;
            }

            detail::CacheRegion entry{};
            entry.base = reinterpret_cast<std::uint64_t>(codeCacheMem);
            entry.data = static_cast<std::uint8_t*>(codeCacheMem);
            entry.capacity = 0x1000;

#ifdef _DEBUG
            std::memset(codeCacheMem, 0xCC, entry.capacity);
#endif

            cacheRegions.push_back(entry);

            return StatusCode::success;
        };

        if (cacheRegions.empty())
        {
            if (auto status = allocRegion(); status != StatusCode::success)
            {
                return status;
            }
        }

        // Check if we have enough space otherwise allocate a new region.
        {
            const auto& lastRegion = cacheRegions.back();
            const auto remaining = lastRegion.capacity - lastRegion.size;
            if (remaining < estimatedSize)
            {
                if (auto status = allocRegion(); status != StatusCode::success)
                {
                    return status;
                }
            }
        }

        auto& lastRegion = cacheRegions.back();
        return &lastRegion;
    }

    static inline bool isGpReg(ZydisRegister reg)
    {
        // Special case for RIP.
        if (reg == ZYDIS_REGISTER_RIP || reg == ZYDIS_REGISTER_EIP)
        {
            return true;
        }
        return reg >= ZYDIS_REGISTER_AL && reg <= ZYDIS_REGISTER_R15;
    }

    static inline bool instructionModifiesFlags(const DecodedInstruction& instr)
    {
        return (instr.data.cpu_flags->modified | instr.data.cpu_flags->set_0 | instr.data.cpu_flags->set_1
                | instr.data.cpu_flags->undefined)
            != 0;
    }

    static inline bool instructionTestsFlags(const DecodedInstruction& instr)
    {
        return instr.data.cpu_flags->tested != 0;
    }

    static bool isAddressableReg(ZydisRegister reg)
    {
        switch (reg)
        {
            case ZYDIS_REGISTER_FLAGS:
            case ZYDIS_REGISTER_EFLAGS:
            case ZYDIS_REGISTER_RFLAGS:
                return false;
            default:
                break;
        }
        return true;
    }

    // Returns the registers read by the instruction, it will always use the largest size of the register.
    static inline RegSet getRegsRead(const DecodedInstruction& instr)
    {
        RegSet regs{};
        for (std::size_t i = 0; i < instr.data.operand_count; ++i)
        {
            const auto& op = instr.operands[i];
            if (op.type == ZYDIS_OPERAND_TYPE_REGISTER && (op.actions & ZYDIS_OPERAND_ACTION_MASK_READ) != 0)
            {
                regs.insert(ZydisRegisterGetLargestEnclosing(instr.data.machine_mode, op.reg.value));
            }
            else if (op.type == ZYDIS_OPERAND_TYPE_MEMORY)
            {
                if (op.mem.base != ZYDIS_REGISTER_NONE)
                {
                    regs.insert(ZydisRegisterGetLargestEnclosing(instr.data.machine_mode, op.mem.base));
                }
                if (op.mem.index != ZYDIS_REGISTER_NONE)
                {
                    regs.insert(ZydisRegisterGetLargestEnclosing(instr.data.machine_mode, op.mem.index));
                }
            }
        }
        return regs;
    }

    // Returns the registers modified by the instruction, it will always use the largest size of the register.
    static inline RegSet getRegsModified(const DecodedInstruction& instr)
    {
        RegSet regs{};
        for (std::size_t i = 0; i < instr.data.operand_count; ++i)
        {
            const auto& op = instr.operands[i];
            if (op.type == ZYDIS_OPERAND_TYPE_REGISTER && (op.actions & ZYDIS_OPERAND_ACTION_MASK_WRITE) != 0)
            {
                regs.insert(ZydisRegisterGetLargestEnclosing(instr.data.machine_mode, op.reg.value));
            }
        }
        return regs;
    }

    static inline x86::Reg getRemappedReg(GeneratorState& state, ZydisRegister reg)
    {
        if (reg == ZYDIS_REGISTER_NONE)
        {
            return x86::Reg{};
        }

        const auto largeReg = ZydisRegisterGetLargestEnclosing(state.mode, reg);
        if (!state.regMap.contains(largeReg))
        {
            assert(false);
            return x86::Reg{};
        }

        // TODO: This probably doesn't work for gp8 as as it may be hi/lo so the id doesn't align.
        const auto remappedReg = state.regMap[largeReg];
        if (ZydisRegisterGetWidth(state.mode, reg) == ZydisRegisterGetWidth(state.mode, remappedReg.value))
        {
            return remappedReg;
        }

        // Convert it to the size expected.
        const auto cls = ZydisRegisterGetClass(reg);
        const auto newReg = ZydisRegisterEncode(cls, ZydisRegisterGetId(remappedReg.value));

        return x86::Reg{ newReg };
    }

    static inline bool requiresExternalCall(const DecodedInstruction& instr)
    {
        switch (instr.data.mnemonic)
        {
            case ZYDIS_MNEMONIC_SYSCALL:
                return true;
        }

        // If the insntructions reads or writes memory we need to call the memory
        // read/write handlers.
        for (std::size_t i = 0; i < instr.data.operand_count; ++i)
        {
            const auto& op = instr.operands[i];
            if (op.type == ZYDIS_OPERAND_TYPE_MEMORY)
            {
                return true;
            }
        }

        return false;
    }

    static inline StatusCode memoryRead(
        GeneratorState& state, detail::CPUState* cpuState, const auto& srcAddr, const auto& dstBuf, std::int32_t readSize)
    {
        auto& a = state.assembler;

        // Call memory read handler.
        a.mov(x86::ecx, x86::dword_ptr(state.regCtx, offsetof(detail::ThreadContext, tid)));
        if constexpr (std::convertible_to<std::decay_t<decltype(srcAddr)>, x86::Reg>)
        {
            if (srcAddr != x86::rdx)
            {
                a.mov(x86::rdx, srcAddr);
            }
        }
        else
        {
            a.mov(x86::rdx, srcAddr);
        }
        a.mov(x86::rdx, srcAddr);
        a.lea(x86::r8, dstBuf);
        a.mov(x86::r9, x86::Imm(readSize));
        a.push(x86::Imm(0)); // FIXME: userData

        a.mov(x86::rax, x86::Imm(reinterpret_cast<std::intptr_t>(cpuState->memReadHandler)));
        a.call(x86::rax);
        a.lea(x86::rsp, x86::qword_ptr(x86::rsp, 8));
        a.test(x86::rax, x86::rax);
        a.jnz(state.lblExit);

        return StatusCode::success;
    }

    static inline StatusCode memoryWrite(
        GeneratorState& state, detail::CPUState* cpuState, const auto& dstAddr, const auto& srcBuf, std::int32_t writeSize)
    {
        auto& a = state.assembler;

        // Call memory write handler.
        a.mov(x86::ecx, x86::dword_ptr(state.regCtx, offsetof(detail::ThreadContext, tid)));
        a.mov(x86::rdx, dstAddr);
        a.lea(x86::r8, srcBuf);
        a.mov(x86::r9, x86::Imm(writeSize));
        a.push(x86::Imm(0)); // FIXME: userData

        a.mov(x86::rax, x86::Imm(reinterpret_cast<std::intptr_t>(cpuState->memWriteHandler)));
        a.call(x86::rax);
        a.lea(x86::rsp, x86::qword_ptr(x86::rsp, 8));

        a.test(x86::rax, x86::rax);
        a.jnz(state.lblExit);

        return StatusCode::success;
    }

    static inline StatusCode handleInstrRet(GeneratorState& state, detail::CPUState* cpuState, const DecodedInstruction& instr)
    {
        auto& a = state.assembler;

        const auto userRspReg = getRemappedReg(state, ZYDIS_REGISTER_RSP);
        const auto regRip = getRemappedReg(state, ZYDIS_REGISTER_RIP);

        // Call memory read handler.
        if (auto status = memoryRead(
                state, cpuState, userRspReg, x86::qword_ptr(x86::rsp, kHomeAreaSize), instr.data.stack_width / 8);
            status != StatusCode::success)
        {
            return status;
        }

        // Update SP.
        if (instr.operands[0].type == ZYDIS_OPERAND_TYPE_IMMEDIATE)
        {
            a.add(userRspReg, x86::Imm(instr.data.stack_width + instr.operands[0].imm.value.s));
        }
        else
        {
            a.add(userRspReg, x86::Imm(8));
        }

        // Assign RIP
        a.mov(regRip, x86::qword_ptr(x86::rsp, kHomeAreaSize));

        // Update RIP.
        const auto regInfoRip = getContextRegInfo(cpuState, ZYDIS_REGISTER_RIP);
        a.mov(x86::qword_ptr(state.regCtx, regInfoRip.offset), regRip);

        // Update RSP in context.
        const auto regInfoRsp = getContextRegInfo(cpuState, ZYDIS_REGISTER_RSP);
        a.mov(x86::qword_ptr(state.regCtx, regInfoRsp.offset), userRspReg);

        return StatusCode::success;
    }

    static inline StatusCode handleInstrCall(GeneratorState& state, detail::CPUState* cpuState, const DecodedInstruction& instr)
    {
        auto& a = state.assembler;

        const auto srcOp = instr.operands[0];

        // Push return address.
        const auto userRspReg = getRemappedReg(state, ZYDIS_REGISTER_RSP);
        const auto regRip = getRemappedReg(state, ZYDIS_REGISTER_RIP);

        // Compute return address into stack buffer.
        {
            a.mov(x86::rax, regRip);
            a.add(x86::rax, x86::Imm(instr.data.length));
            a.mov(x86::qword_ptr(x86::rsp, kHomeAreaSize), x86::rax);
        }

        // Push return address
        {
            // Update RSP.
            a.sub(userRspReg, x86::Imm(instr.data.stack_width / 8));

            // Memory write.
            if (auto status = memoryWrite(
                    state, cpuState, userRspReg, x86::qword_ptr(x86::rsp, kHomeAreaSize), instr.data.stack_width / 8);
                status != StatusCode::success)
            {
                return status;
            }
        }

        // Update RIP to call target.
        {
            if (srcOp.type == ZYDIS_OPERAND_TYPE_IMMEDIATE)
            {
                if (srcOp.imm.is_relative)
                {
                    a.add(regRip, x86::Imm(srcOp.imm.value.s + instr.data.length));
                }
                else
                {
                    a.mov(regRip, x86::Imm(srcOp.imm.value.s));
                }
            }
            else if (srcOp.type == ZYDIS_OPERAND_TYPE_REGISTER)
            {
                const auto dstReg = getRemappedReg(state, srcOp.reg.value);
                a.mov(regRip, dstReg);
            }
            else
            {
                // TODO: Implement memory read.
                assert(false);
            }
        }

        // Update RIP.
        const auto regInfoRip = getContextRegInfo(cpuState, ZYDIS_REGISTER_RIP);
        a.mov(x86::qword_ptr(state.regCtx, regInfoRip.offset), regRip);

        // Update RSP in context.
        const auto regInfoRsp = getContextRegInfo(cpuState, ZYDIS_REGISTER_RSP);
        a.mov(x86::qword_ptr(state.regCtx, regInfoRsp.offset), userRspReg);

        return StatusCode::success;
    }

    static inline StatusCode handleInstrPush(GeneratorState& state, detail::CPUState* cpuState, const DecodedInstruction& instr)
    {
        auto& a = state.assembler;

        const auto srcOp = instr.operands[0];

        // Write to operand to stack.
        if (srcOp.type == ZYDIS_OPERAND_TYPE_REGISTER)
        {
            const auto srcReg = getRemappedReg(state, srcOp.reg.value);
            a.mov(x86::ptr(srcOp.size, x86::rsp, kHomeAreaSize), srcReg);
        }
        else if (srcOp.type == ZYDIS_OPERAND_TYPE_IMMEDIATE)
        {
            a.mov(x86::ptr(srcOp.size, x86::rsp, kHomeAreaSize), x86::Imm(srcOp.imm.value.s));
        }
        else
        {
            // TODO: Implement me.
            assert(false);
        }

        const auto userRspReg = getRemappedReg(state, ZYDIS_REGISTER_RSP);

        // Update RSP.
        a.sub(userRspReg, x86::Imm(srcOp.size / 8));

        // Memory write.
        if (auto status = memoryWrite(state, cpuState, userRspReg, x86::qword_ptr(x86::rsp, kHomeAreaSize), srcOp.size / 8);
            status != StatusCode::success)
        {
            return status;
        }

        // Update RIP.
        const auto regInfoRip = getContextRegInfo(cpuState, ZYDIS_REGISTER_RIP);
        a.add(x86::qword_ptr(state.regCtx, regInfoRip.offset), x86::Imm(instr.data.length));

        // Update RSP in context.
        const auto regInfoRsp = getContextRegInfo(cpuState, ZYDIS_REGISTER_RSP);
        a.mov(x86::qword_ptr(state.regCtx, regInfoRsp.offset), userRspReg);

        // Status code.
        a.mov(x86::rax, x86::Imm(StatusCode::success));

        return StatusCode::success;
    }

    static inline StatusCode handleInstrPop(GeneratorState& state, detail::CPUState* cpuState, const DecodedInstruction& instr)
    {
        auto& a = state.assembler;

        const auto dstOp = instr.operands[0];
        const auto userRspReg = getRemappedReg(state, ZYDIS_REGISTER_RSP);

        // Memory read.
        if (auto status = memoryRead(state, cpuState, userRspReg, x86::qword_ptr(x86::rsp, kHomeAreaSize), dstOp.size / 8);
            status != StatusCode::success)
        {
            return status;
        }

        // Write from buffer to destination operand.
        if (dstOp.type == ZYDIS_OPERAND_TYPE_REGISTER)
        {
            const auto destReg = getRemappedReg(state, dstOp.reg.value);
            a.mov(destReg, x86::ptr(dstOp.size, x86::rsp, kHomeAreaSize));
        }
        else if (dstOp.type == ZYDIS_OPERAND_TYPE_IMMEDIATE)
        {
            assert(false);
        }
        else
        {
            // TODO: Implement me.
            assert(false);
        }

        // Update RSP.
        a.add(userRspReg, x86::Imm(dstOp.size / 8));

        for (auto& regW : instr.regsModified)
        {
            if (!isAddressableReg(regW))
            {
                continue;
            }

            const auto remappedReg = state.regMap[regW];

            const auto regInfo = getContextRegInfo(cpuState, regW);

            if (isGpReg(regW))
            {
                a.mov(x86::qword_ptr(state.regCtx, regInfo.offset), state.regMap[regW]);
            }
            else
            {
                assert(false);
            }
        }

        // Update RIP.
        const auto regInfoRip = getContextRegInfo(cpuState, ZYDIS_REGISTER_RIP);
        a.add(x86::qword_ptr(state.regCtx, regInfoRip.offset), x86::Imm(instr.data.length));

        // Status code.
        a.mov(x86::rax, x86::Imm(StatusCode::success));

        return StatusCode::success;
    }

    static inline StatusCode handleInstrJcc(GeneratorState& state, detail::CPUState* cpuState, const DecodedInstruction& instr)
    {
        auto& a = state.assembler;

        // Restore flags.
        auto regInfoFlags = getContextRegInfo(cpuState, ZYDIS_REGISTER_FLAGS);
        a.mov(x86::rax, x86::qword_ptr(state.regCtx, regInfoFlags.offset));
        a.push(x86::rax);
        a.popfq();

        assert(instr.operands[0].type == ZYDIS_OPERAND_TYPE_IMMEDIATE);

        a.mov(x86::rax, x86::Imm(instr.data.length));
        a.mov(x86::r8, x86::Imm(instr.operands[0].imm.value.s + instr.data.length));

        // If the jcc cond is true mov r8 to rax, otherwise rax is just this instruction.
        // Cmovcc based on mnemonic.
        switch (instr.data.mnemonic)
        {
            case ZYDIS_MNEMONIC_JB:
                a.emit(ZYDIS_MNEMONIC_CMOVNB, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JBE:
                a.emit(ZYDIS_MNEMONIC_CMOVNBE, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JL:
                a.emit(ZYDIS_MNEMONIC_CMOVNL, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JLE:
                a.emit(ZYDIS_MNEMONIC_CMOVNLE, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNB:
                a.emit(ZYDIS_MNEMONIC_CMOVB, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNBE:
                a.emit(ZYDIS_MNEMONIC_CMOVBE, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNL:
                a.emit(ZYDIS_MNEMONIC_CMOVL, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNLE:
                a.emit(ZYDIS_MNEMONIC_CMOVLE, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNO:
                a.emit(ZYDIS_MNEMONIC_CMOVNO, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNP:
                a.emit(ZYDIS_MNEMONIC_CMOVNP, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNS:
                a.emit(ZYDIS_MNEMONIC_CMOVNS, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JNZ:
                a.emit(ZYDIS_MNEMONIC_CMOVNZ, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JO:
                a.emit(ZYDIS_MNEMONIC_CMOVO, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JP:
                a.emit(ZYDIS_MNEMONIC_CMOVP, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JS:
                a.emit(ZYDIS_MNEMONIC_CMOVS, x86::rax, x86::r8);
                break;
            case ZYDIS_MNEMONIC_JZ:
                a.emit(ZYDIS_MNEMONIC_CMOVZ, x86::rax, x86::r8);
                break;
            default:
                assert(false);
                return StatusCode::invalidOperation;
        }

        // Add rax to rip.
        const auto regInfoIp = getContextRegInfo(cpuState, ZYDIS_REGISTER_RIP);

        a.add(x86::qword_ptr(state.regCtx, regInfoIp.offset), x86::rax);

        a.mov(x86::rax, x86::Imm(StatusCode::success));

        return StatusCode::success;
    }

    static inline StatusCode handleInstrGeneric(
        GeneratorState& state, detail::CPUState* cpuState, const DecodedInstruction& instr)
    {
        auto& a = state.assembler;

        // First is source, second is destination.
        struct MemWriteData
        {
            x86::Reg memAddrReg;
            x86::Reg dstReg;
            std::int32_t srcSaveOffset;
            std::int32_t dstSaveOffset;
            std::int32_t bitSize;
        };

        sfl::static_vector<MemWriteData, 16> memWrites;

        std::int32_t regSaveOffset = 0;
        std::int32_t memBufferOffset = 0;

        x86::Instruction newInstr{};
        newInstr.mnemonic = instr.data.mnemonic;
        for (std::size_t i = 0; i < instr.data.operand_count_visible; ++i)
        {
            const auto& opSrc = instr.operands[i];
            if (opSrc.type == ZYDIS_OPERAND_TYPE_REGISTER)
            {
                const auto remappedReg = getRemappedReg(state, opSrc.reg.value);
                newInstr.operands.push_back(remappedReg);
            }
            else if (opSrc.type == ZYDIS_OPERAND_TYPE_IMMEDIATE)
            {
                newInstr.operands.push_back(x86::Imm(opSrc.imm.value.s));
            }
            else if (opSrc.type == ZYDIS_OPERAND_TYPE_MEMORY)
            {
                // Generate a memory handler.
                if (opSrc.actions & ZYDIS_OPERAND_ACTION_MASK_READ)
                {
                    // Compute address.
                    x86::Mem leaMem{};
                    leaMem.base = getRemappedReg(state, opSrc.mem.base);
                    leaMem.index = getRemappedReg(state, opSrc.mem.index);
                    leaMem.scale = opSrc.mem.scale;
                    leaMem.disp = opSrc.mem.disp.value;
                    leaMem.bitSize = opSrc.size;

                    // Memory read.
                    a.lea(x86::rdx, leaMem);
                    if (auto status = memoryRead(
                            state, cpuState, x86::rdx, x86::qword_ptr(x86::rsp, kHomeAreaSize + memBufferOffset),
                            opSrc.size / 8);
                        status != StatusCode::success)
                    {
                        return status;
                    }

                    x86::Mem mem{};
                    mem.bitSize = opSrc.size;
                    mem.base = x86::rsp;
                    mem.disp = kHomeAreaSize + memBufferOffset;

                    memBufferOffset += opSrc.size / 8;

                    newInstr.operands.push_back(mem);
                }
                else if (opSrc.actions & ZYDIS_OPERAND_ACTION_MASK_WRITE)
                {
                    // We need to set a register as the destination so we can write that into our buffer
                    // and then call the memory write handler.
                    const auto destReg = state.allocGpReg(true);
                    const auto memAddrReg = state.allocGpReg(true);

                    // Compute address.
                    x86::Mem leaMem{};
                    leaMem.base = getRemappedReg(state, opSrc.mem.base);
                    leaMem.index = getRemappedReg(state, opSrc.mem.index);
                    leaMem.scale = opSrc.mem.scale;
                    leaMem.disp = opSrc.mem.disp.value;
                    leaMem.bitSize = opSrc.size;

                    MemWriteData memWriteData{};
                    memWriteData.dstReg = destReg;
                    memWriteData.memAddrReg = memAddrReg;
                    memWriteData.bitSize = opSrc.size;

                    // Save registers, non-volatile need to be saved.
                    a.mov(x86::qword_ptr(x86::rsp, kSpillAreaOffset + regSaveOffset), memAddrReg);
                    memWriteData.dstSaveOffset = regSaveOffset;
                    regSaveOffset += 8;

                    a.mov(x86::qword_ptr(x86::rsp, kSpillAreaOffset + regSaveOffset), destReg);
                    memWriteData.srcSaveOffset = regSaveOffset;
                    regSaveOffset += 8;

                    // Store memory write address.
                    a.lea(memAddrReg, leaMem);

                    newInstr.operands.push_back(destReg);

                    memWrites.emplace_back(memWriteData);
                }
                else
                {
                    assert(false);
                }
            }
            else
            {
                assert(false);
            }
        }
        a.emit(newInstr);

        // Capture flags before we do any other operations.
        if (instructionModifiesFlags(instr))
        {
            auto regInfo = getContextRegInfo(cpuState, ZYDIS_REGISTER_FLAGS);
            a.pushfq();
            a.pop(x86::rax);
            a.mov(x86::qword_ptr(state.regCtx, regInfo.offset), x86::rax);
        }

        // Process memory writes.
        for (const auto& memWrite : memWrites)
        {
            // Write value to stack.
            a.mov(x86::ptr(memWrite.bitSize, x86::rsp, kHomeAreaSize), memWrite.dstReg);

            // Call memory write handler.
            a.mov(x86::ecx, x86::dword_ptr(state.regCtx, offsetof(detail::ThreadContext, tid)));
            a.mov(x86::rdx, memWrite.memAddrReg);
            a.lea(x86::r8, x86::qword_ptr(x86::rsp, kHomeAreaSize));
            a.mov(x86::r9, x86::Imm(memWrite.bitSize / 8));
            a.push(x86::Imm(0)); // FIXME: userData

            a.mov(x86::rax, x86::Imm(reinterpret_cast<std::intptr_t>(cpuState->memWriteHandler)));
            a.call(x86::rax);
            a.lea(x86::rsp, x86::qword_ptr(x86::rsp, 8));

            // Restore registers.
            a.mov(memWrite.memAddrReg, x86::qword_ptr(x86::rsp, kSpillAreaOffset + memWrite.dstSaveOffset));
            a.mov(memWrite.dstReg, x86::qword_ptr(x86::rsp, kSpillAreaOffset + memWrite.srcSaveOffset));

            a.test(x86::rax, x86::rax);
            a.jnz(state.lblExit);
        }

        // Write from re-mapped registers to context.
        for (auto& regW : instr.regsModified)
        {
            if (!isAddressableReg(regW))
            {
                continue;
            }

            const auto remappedReg = state.regMap[regW];

            const auto regInfo = getContextRegInfo(cpuState, regW);

            if (isGpReg(regW))
            {
                a.mov(x86::qword_ptr(state.regCtx, regInfo.offset), state.regMap[regW]);
            }
            else
            {
                assert(false);
            }
        }

        // Update RIP.
        {
            const auto regInfo = getContextRegInfo(cpuState, ZYDIS_REGISTER_RIP);

            a.add(x86::qword_ptr(state.regCtx, regInfo.offset), x86::Imm(instr.data.length));
        }

        a.mov(x86::rax, x86::Imm(StatusCode::success));

        return StatusCode::success;
    }

    static constexpr auto kHandlerTable = []() {
        using Handler = StatusCode (*)(GeneratorState&, detail::CPUState*, const DecodedInstruction&);

        std::array<Handler, ZYDIS_MNEMONIC_MAX_VALUE> table{};
        std::fill(table.begin(), table.end(), handleInstrGeneric);

        table[ZYDIS_MNEMONIC_CALL] = handleInstrCall;
        table[ZYDIS_MNEMONIC_JB] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JBE] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JL] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JLE] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNB] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNBE] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNL] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNLE] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNO] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNP] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNS] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JNZ] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JO] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JP] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JS] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_JZ] = handleInstrJcc;
        table[ZYDIS_MNEMONIC_POP] = handleInstrPop;
        table[ZYDIS_MNEMONIC_PUSH] = handleInstrPush;
        table[ZYDIS_MNEMONIC_RET] = handleInstrRet;

        return table;
    }();

    static inline StatusCode assembleInstrHandler(
        GeneratorState& state, detail::CPUState* cpuState, const DecodedInstruction& instr)
    {
        auto& a = state.assembler;

        state.lblExit = a.createLabel();

        // To simplify things a bit we map ZydisRegisterNone to our reg type.
        state.regMap[ZYDIS_REGISTER_NONE] = x86::Reg{};

        state.requiresExternalCalls = requiresExternalCall(instr);

        if (state.requiresExternalCalls)
        {
            // Use a different register for the context, we need rcx to pass arguments.
            // We do this first before allocating registers for remapping the context.
            state.regCtx = state.allocGpReg(true);
        }

        // Remap registers and sync context.
        for (auto& reg : instr.regsUsed)
        {
            if (!isAddressableReg(reg))
            {
                continue;
            }

            if (isGpReg(reg))
            {
                auto newReg = state.allocGpReg(state.requiresExternalCalls);
                state.regMap[reg] = newReg;
            }
            else
            {
                // TODO: Implement me.
                assert(false);
            }
        }

        if (state.requiresExternalCalls)
        {
            std::int64_t savedRegsSize = 0;

            // Save all used registers.
            a.push(state.regCtx);

            for (auto& [reg, remappedReg] : state.regMap)
            {
                if (remappedReg.value == ZYDIS_REGISTER_NONE)
                {
                    continue;
                }
                if (remappedReg.isGp64())
                {
                    a.push(remappedReg);
                    savedRegsSize += 8;
                }
                else
                {
                    assert(false);
                }
            }

            a.mov(state.regCtx, x86::rcx);

            // Allocate space for temporary buffer and align to 16.
            state.usedStackSpaceSize = (savedRegsSize + kHomeAreaSize + kMemoryStackSize + kSpillAreaSize + 0xF) & ~0xF;
            a.sub(x86::rsp, x86::Imm(state.usedStackSpaceSize));
        }

        // Sync virtual context.
        if (instructionTestsFlags(instr))
        {
            auto regInfo = getContextRegInfo(cpuState, ZYDIS_REGISTER_FLAGS);
            a.mov(x86::rax, x86::qword_ptr(state.regCtx, regInfo.offset));
            a.push(x86::rax);
            a.popfq();
        }

        // Write from context to re-mapped registers.
        for (auto& regR : instr.regsRead)
        {
            if (!isAddressableReg(regR))
            {
                continue;
            }

            const auto remappedReg = state.regMap[regR];

            const auto regInfo = getContextRegInfo(cpuState, regR);

            if (isGpReg(regR))
            {
                a.mov(remappedReg, x86::qword_ptr(state.regCtx, regInfo.offset));
            }
            else
            {
                assert(false);
            }
        }

        auto handlerFunc = kHandlerTable[instr.data.mnemonic];
        assert(handlerFunc != nullptr);

        if (auto status = handlerFunc(state, cpuState, instr); status != StatusCode::success)
        {
            return status;
        }

        a.bind(state.lblExit);

        if (state.requiresExternalCalls)
        {
            // Restore stack.
            a.add(x86::rsp, x86::Imm(state.usedStackSpaceSize));

            // Restore all used registers.
            for (auto it = state.regMap.rbegin(); it != state.regMap.rend(); ++it)
            {
                const auto& [reg, remappedReg] = *it;
                if (remappedReg.value == ZYDIS_REGISTER_NONE)
                {
                    continue;
                }
                if (remappedReg.isGp())
                {
                    a.pop(remappedReg);
                }
                else
                {
                    assert(false);
                }
            }

            a.pop(state.regCtx);
        }

        a.ret();

        return StatusCode::success;
    }

    Result<detail::CodeCacheFunc> generate(
        detail::CPUState* cpuState, std::uint64_t rip, const detail::InstructionData& instrData)
    {
        DecodedInstruction instruction{};

        // TODO: Avoid decoding again.
        if (auto status = ZydisDecoderDecodeFull(
                &cpuState->decoder, instrData.buffer(), instrData.length(), &instruction.data, instruction.operands);
            status != ZYAN_STATUS_SUCCESS)
        {
            return StatusCode::invalidInstruction;
        }

        instruction.address = rip;
        instruction.regsRead = getRegsRead(instruction);
        instruction.regsModified = getRegsModified(instruction);
        instruction.regsUsed = instruction.regsRead;
        instruction.regsUsed.insert(instruction.regsModified.begin(), instruction.regsModified.end());

        GeneratorState state{};
        state.mode = cpuState->mode;
        state.regCtx = x86::rcx;

        auto status = assembleInstrHandler(state, cpuState, instruction);
        if (status != StatusCode::success)
        {
            return status;
        }

        // FIXME: Proper estimate of the size.
        auto cacheRegionRes = getCacheRegion(cpuState, 64);
        if (!cacheRegionRes)
        {
            return cacheRegionRes.getError();
        }

        auto* cacheRegion = *cacheRegionRes;
        const auto baseAddress = cacheRegion->base + cacheRegion->size;
        const auto cacheData = cacheRegion->data + cacheRegion->size;
        const auto remainingSize = cacheRegion->capacity - cacheRegion->size;

        auto encodeRes = state.assembler.finalize(cpuState->mode, baseAddress, cacheData, remainingSize);
        if (!encodeRes)
        {
            return StatusCode::invalidInstruction;
        }

        // Commit the cache.
        cacheRegion->size += *encodeRes;

        detail::CacheEntry entry{};
        entry.address = rip;
        entry.cacheAddress = baseAddress;
        entry.size = *encodeRes;
        entry.func = reinterpret_cast<detail::CodeCacheFunc>(baseAddress);

        cpuState->cacheEntries[instrData] = entry;

        return entry.func;
    }

} // namespace zyemu::codecache
```

`src/zyemu/codegen.hpp`:

```hpp
#pragma once

#include "internal.hpp"

#include <zyemu/types.hpp>

namespace zyemu::codecache
{

    Result<detail::CodeCacheFunc> generate(
        detail::CPUState* state, std::uint64_t rip, const detail::InstructionData& instrData);

} // namespace zyemu::codecache
```

`src/zyemu/cpu.cpp`:

```cpp
#include "codegen.hpp"
#include "internal.hpp"
#include "registers.hpp"

#include <zyemu/zyemu.hpp>

namespace zyemu
{
    using namespace detail;

    CPU::CPU()
    {
        state = new detail::CPUState{};
    }

    CPU::~CPU()
    {
        delete state;
    }

    StatusCode CPU::setMode(ZydisMachineMode mode)
    {
        ZydisStackWidth stackWidth{};
        switch (mode)
        {
            case ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_64:
                stackWidth = ZydisStackWidth::ZYDIS_STACK_WIDTH_64;
                break;
            case ZydisMachineMode::ZYDIS_MACHINE_MODE_LEGACY_32:
                [[fallthrough]];
            case ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_COMPAT_32:
                stackWidth = ZydisStackWidth::ZYDIS_STACK_WIDTH_32;
                break;
            case ZydisMachineMode::ZYDIS_MACHINE_MODE_LEGACY_16:
                [[fallthrough]];
            case ZydisMachineMode::ZYDIS_MACHINE_MODE_LONG_COMPAT_16:
                stackWidth = ZydisStackWidth::ZYDIS_STACK_WIDTH_16;
                break;
            default:
                return StatusCode::invalidMode;
        }
        if (auto status = ZydisDecoderInit(&state->decoder, mode, stackWidth); status != ZYAN_STATUS_SUCCESS)
        {
            return StatusCode::invalidMode;
        }

        if (auto status = ZydisDecoderInit(&state->ldeDecoder, mode, stackWidth); status != ZYAN_STATUS_SUCCESS)
        {
            return StatusCode::invalidMode;
        }
        ZydisDecoderEnableMode(&state->ldeDecoder, ZYDIS_DECODER_MODE_MINIMAL, true);

        state->mode = mode;
        return StatusCode::success;
    }

    void CPU::setMemReadHandler(MemoryReadHandler callback, void* userData)
    {
        state->memReadHandler = callback;
        state->memReadUserData = userData;
    }

    void CPU::setMemWriteHandler(MemoryWriteHandler callback, void* userData)
    {
        state->memWriteHandler = callback;
        state->memWriteUserData = userData;
    }

    void CPU::clearCodeCache()
    {
        state->cacheEntries.clear();
        for (auto& region : state->cacheRegions)
        {
            region.size = 0;
#ifdef _DEBUG
            std::memset(region.data, 0xCC, region.capacity);
#endif
        }
    }

    ThreadId CPU::createThread()
    {
        const auto initThread = [&](ThreadData& th, ThreadId tid) {
            th.state = ThreadState::idle;
            th.context = {};
            th.context.cpuState = state;
            th.context.tid = tid;
        };

        for (std::size_t i = 0; i < this->state->threads.size(); ++i)
        {
            ThreadData& th = state->threads[i];
            if (th.state != detail::ThreadState::dead)
            {
                continue;
            }

            const auto tid = static_cast<ThreadId>(i);
            initThread(th, tid);

            return tid;
        }

        const auto tid = static_cast<ThreadId>(state->threads.size());
        auto& th = state->threads.emplace_back();

        initThread(th, tid);

        return tid;
    }

    static ThreadData* getThread(detail::CPUState* state, ThreadId tid)
    {
        const auto idx = static_cast<std::size_t>(tid);
        if (idx >= state->threads.size())
        {
            return nullptr;
        }
        return &state->threads[idx];
    }

    void CPU::destroyThread(ThreadId tid)
    {
        const auto idx = static_cast<std::size_t>(tid);
        if (idx >= state->threads.size())
        {
            return;
        }

        ThreadData& th = state->threads[idx];
        th.state = ThreadState::dead;
    }

    StatusCode CPU::setRegData(ThreadId tid, ZydisRegister reg, std::span<const std::uint8_t> data)
    {
        auto* th = getThread(state, tid);
        if (!th)
        {
            return StatusCode::invalidThread;
        }

        const auto regInfo = getContextRegInfo(state, reg);
        if (regInfo.offset == 0)
        {
            return StatusCode::invalidRegister;
        }

        if (data.size() != regInfo.bitSize / 8)
        {
            return StatusCode::invalidRegister;
        }

        std::memcpy(reinterpret_cast<std::uint8_t*>(&th->context) + regInfo.offset, data.data(), data.size());

        return StatusCode::success;
    }

    StatusCode CPU::getRegData(ThreadId tid, ZydisRegister reg, std::span<std::uint8_t> buffer)
    {
        auto* th = getThread(state, tid);
        if (!th)
        {
            return StatusCode::invalidThread;
        }

        const auto regInfo = getContextRegInfo(state, reg);
        if (regInfo.offset == 0)
        {
            return StatusCode::invalidRegister;
        }

        if (buffer.size() != regInfo.bitSize / 8)
        {
            return StatusCode::invalidRegister;
        }

        std::memcpy(buffer.data(), reinterpret_cast<std::uint8_t*>(&th->context) + regInfo.offset, buffer.size());

        return StatusCode::success;
    }

    static Result<InstructionData> getInstructionData(CPUState* state, ThreadId tid, std::uint64_t address)
    {
        InstructionData instrBuf{};

        ZydisDecodedInstruction instr;
        for (std::uint8_t i = 0U; i < 15U; i++)
        {
            // Try to read i bytes.
            if (auto status = state->memReadHandler(tid, address + i, instrBuf.buffer() + i, 1, state->memReadUserData);
                status != StatusCode::success)
            {
                return status;
            }

            auto status = ZydisDecoderDecodeInstruction(&state->ldeDecoder, nullptr, instrBuf.buffer(), i + 1, &instr);
            if (status == ZYAN_STATUS_SUCCESS)
            {
                instrBuf.data[0] = i + 1;
                return instrBuf;
            }
            else if (status == ZYDIS_STATUS_NO_MORE_DATA)
            {
                continue;
            }
            else
            {
                return StatusCode::invalidInstruction;
            }
        }

        return StatusCode::invalidInstruction;
    }

    StatusCode CPU::step(ThreadId tid)
    {
        auto* th = getThread(state, tid);
        if (!th)
        {
            return StatusCode::invalidThread;
        }

        auto& ctx = th->context;

        const auto rip = ctx.rip;

        auto instrDataRes = getInstructionData(state, tid, rip);
        if (!instrDataRes)
        {
            return instrDataRes.getError();
        }

        auto cacheIt = state->cacheEntries.find(*instrDataRes);
        if (cacheIt != state->cacheEntries.end())
        {
            const auto& func = cacheIt->second.func;
            return func(&ctx);
        }

        // Generate code cache entry
        auto result = codecache::generate(state, rip, *instrDataRes);
        if (!result)
        {
            return result.getError();
        }

        const auto execRes = result.getValue()(&ctx);
        if (execRes != StatusCode::success)
        {
            return execRes;
        }

        return StatusCode::success;
    }

} // namespace zyemu

```

`src/zyemu/internal.hpp`:

```hpp
#pragma once

#include "unordered_dense.h"

#include <Zydis/Decoder.h>
#include <array>
#include <map>
#include <sfl/map.hpp>
#include <sfl/segmented_vector.hpp>
#include <sfl/small_vector.hpp>
#include <unordered_map>
#include <zyemu/types.hpp>

namespace zyemu
{
    namespace detail
    {
        struct CPUState;

        enum class ThreadState
        {
            dead = 0,
            idle,
            running,
        };

        struct ThreadContext
        {
            // We use the largest possible size as its backwards compatible with smaller registers.
            using GpReg = std::array<std::uint8_t, 8>;
            using MmxReg = std::array<std::uint8_t, 8>;
            using X87Reg = std::array<std::uint8_t, 10>;
            using TmmReg = std::array<std::uint8_t, 16>;
            using ZmmReg = std::array<std::uint8_t, 64>;
            using KReg = std::array<std::uint8_t, 64>;

            // Not really part of the context but can be useful for callbacks.
            CPUState* cpuState{};
            ThreadId tid{ ThreadId::invalid };

            std::uint64_t rip{};
            std::uint64_t flags{};
            std::array<GpReg, 16> gpRegs{};
            std::array<MmxReg, 8> mmxRegs{};
            std::array<X87Reg, 8> x87Regs{};
            std::array<ZmmReg, 32> zmmRegs{};
            std::array<TmmReg, 8> tmmRegs{};
            std::array<KReg, 8> kRegs{};
        };

        struct ThreadData
        {
            ThreadState state{};
            ThreadContext context{};
        };

        struct CacheRegion
        {
            std::uint64_t base{};
            std::size_t size{};
            std::size_t capacity{};
            std::uint8_t* data{};
        };

        using CodeCacheFunc = StatusCode (*)(ThreadContext* th);

        struct CacheEntry
        {
            std::uint64_t address{};
            std::uint64_t cacheAddress{};
            std::uint64_t size{};
            CodeCacheFunc func{};
        };

        // First byte is the length of the instruction followed by the instruction data.
        union InstructionData
        {
            struct
            {
                std::uint64_t low;
                std::uint64_t high;
            };

            std::array<std::uint8_t, 16> data{};

            constexpr bool operator==(const InstructionData& other) const
            {
                return low == other.low && high == other.high;
            }

            constexpr bool operator<(const InstructionData& other) const
            {
                return std::tie(low, high) < std::tie(other.low, other.high);
            }

            constexpr std::uint8_t length() const
            {
                return data[0];
            }

            constexpr std::uint8_t* buffer()
            {
                return data.data() + 1;
            }

            constexpr const std::uint8_t* buffer() const
            {
                return data.data() + 1;
            }
        };

        struct InstructionDataHash
        {
            using is_transparent = void; // enable heterogeneous overloads
            using is_avalanching = void; // mark class as high quality avalanching hash

            constexpr std::size_t operator()(const InstructionData& data) const noexcept
            {
                std::size_t hash = data.low >> 32;
                hash ^= data.low & 0xFFFFFFF;
                hash ^= data.high >> 32;
                hash ^= data.high & 0xFFFFFFF;
                return hash;
            }
        };

        struct CPUState
        {
            ZydisMachineMode mode{};
            ZydisDecoder decoder{};
            ZydisDecoder ldeDecoder{};

            sfl::segmented_vector<ThreadData, 16> threads{};

            MemoryReadHandler memReadHandler{};
            void* memReadUserData{};
            MemoryWriteHandler memWriteHandler{};
            void* memWriteUserData{};

            sfl::small_vector<CacheRegion, 64> cacheRegions{};
            sfl::map<InstructionData, CacheEntry> cacheEntries{};
        };

    } // namespace detail

} // namespace zyemu
```

`src/zyemu/registers.hpp`:

```hpp
#pragma once

#include "internal.hpp"

#include <cassert>
#include <cstddef>

namespace zyemu
{
    struct RegInfo
    {
        std::uint16_t offset;
        std::uint16_t bitSize;
        std::uint16_t base;
        std::uint16_t largeBitSize;
    };

    inline RegInfo getContextRegInfo(detail::CPUState* state, ZydisRegister reg)
    {
        const std::uint16_t regSize = ZydisRegisterGetWidth(state->mode, reg);
        const ZydisRegister largeReg = ZydisRegisterGetLargestEnclosing(state->mode, reg);
        const std::uint16_t largeBitSize = ZydisRegisterGetWidth(state->mode, largeReg);
        const std::uint16_t largeByteSize = largeBitSize / 8U;

        const auto getLocalOffset = [](ZydisRegister reg) {
            switch (reg)
            {
                case ZYDIS_REGISTER_AH:
                case ZYDIS_REGISTER_CH:
                case ZYDIS_REGISTER_DH:
                case ZYDIS_REGISTER_BH:
                    return 1;
                default:
                    break;
            }
            return 0;
        };

        switch (ZydisRegisterGetClass(largeReg))
        {
            case ZydisRegisterClass::ZYDIS_REGCLASS_GPR8:
            case ZydisRegisterClass::ZYDIS_REGCLASS_GPR16:
            case ZydisRegisterClass::ZYDIS_REGCLASS_GPR32:
            case ZydisRegisterClass::ZYDIS_REGCLASS_GPR64:
            {
                const std::uint16_t baseOffset = offsetof(detail::ThreadContext, gpRegs);
                const std::uint16_t regId = ZydisRegisterGetId(reg);
                const std::uint16_t regOffset = baseOffset + (regId * largeByteSize);
                const std::uint16_t regOffsetWithLocal = regOffset + getLocalOffset(reg);

                assert(regOffset - baseOffset < sizeof(detail::ThreadContext::gpRegs));

                return RegInfo{
                    .offset = regOffsetWithLocal,
                    .bitSize = regSize,
                    .base = regOffset,
                    .largeBitSize = largeBitSize,
                };
            }
            case ZydisRegisterClass::ZYDIS_REGCLASS_IP:
            {
                return RegInfo{
                    .offset = offsetof(detail::ThreadContext, rip),
                    .bitSize = regSize,
                    .base = offsetof(detail::ThreadContext, rip),
                    .largeBitSize = largeBitSize,
                };
            }
            case ZydisRegisterClass::ZYDIS_REGCLASS_FLAGS:
            {
                return RegInfo{
                    .offset = offsetof(detail::ThreadContext, flags),
                    .bitSize = regSize,
                    .base = offsetof(detail::ThreadContext, flags),
                    .largeBitSize = largeBitSize,
                };
            }
            case ZydisRegisterClass::ZYDIS_REGCLASS_MMX:
            {
                const std::uint16_t baseOffset = offsetof(detail::ThreadContext, mmxRegs);
                const std::uint16_t regId = ZydisRegisterGetId(reg);
                const std::uint16_t regOffset = baseOffset + (regId * largeByteSize);
                const std::uint16_t regOffsetWithLocal = regOffset;

                assert(regOffset - baseOffset < sizeof(detail::ThreadContext::mmxRegs));

                return RegInfo{
                    .offset = regOffsetWithLocal,
                    .bitSize = regSize,
                    .base = regOffset,
                    .largeBitSize = largeByteSize,
                };
            }
            case ZydisRegisterClass::ZYDIS_REGCLASS_X87:
            {
                const std::uint16_t baseOffset = offsetof(detail::ThreadContext, x87Regs);
                const std::uint16_t regId = ZydisRegisterGetId(reg);
                const std::uint16_t regOffset = baseOffset + (regId * largeByteSize);
                const std::uint16_t regOffsetWithLocal = regOffset;

                assert(regOffset - baseOffset < sizeof(detail::ThreadContext::x87Regs));

                return RegInfo{
                    .offset = regOffsetWithLocal,
                    .bitSize = regSize,
                    .base = regOffset,
                    .largeBitSize = largeBitSize,
                };
            }
            case ZydisRegisterClass::ZYDIS_REGCLASS_XMM:
            case ZydisRegisterClass::ZYDIS_REGCLASS_YMM:
            case ZydisRegisterClass::ZYDIS_REGCLASS_ZMM:
            {
                const std::uint16_t baseOffset = offsetof(detail::ThreadContext, zmmRegs);
                const std::uint16_t regId = ZydisRegisterGetId(reg);
                const std::uint16_t regOffset = baseOffset + (regId * largeByteSize);
                const std::uint16_t regOffsetWithLocal = regOffset;

                assert(regOffset - baseOffset < sizeof(detail::ThreadContext::zmmRegs));

                return RegInfo{
                    .offset = regOffsetWithLocal,
                    .bitSize = regSize,
                    .base = regOffset,
                    .largeBitSize = largeBitSize,
                };
            }
        }

        assert(false);
        return {};
    }

} // namespace zyemu
```

`src/zyemu/unordered_dense.h`:

```h
///////////////////////// ankerl::unordered_dense::{map, set} /////////////////////////

// A fast & densely stored hashmap and hashset based on robin-hood backward shift deletion.
// Version 4.5.0
// https://github.com/martinus/unordered_dense
//
// Licensed under the MIT License <http://opensource.org/licenses/MIT>.
// SPDX-License-Identifier: MIT
// Copyright (c) 2022-2024 Martin Leitner-Ankerl <martin.ankerl@gmail.com>
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.

#ifndef ANKERL_UNORDERED_DENSE_H
#define ANKERL_UNORDERED_DENSE_H

// see https://semver.org/spec/v2.0.0.html
#define ANKERL_UNORDERED_DENSE_VERSION_MAJOR 4 // NOLINT(cppcoreguidelines-macro-usage) incompatible API changes
#define ANKERL_UNORDERED_DENSE_VERSION_MINOR 5 // NOLINT(cppcoreguidelines-macro-usage) backwards compatible functionality
#define ANKERL_UNORDERED_DENSE_VERSION_PATCH 0 // NOLINT(cppcoreguidelines-macro-usage) backwards compatible bug fixes

// API versioning with inline namespace, see https://www.foonathan.net/2018/11/inline-namespaces/

// NOLINTNEXTLINE(cppcoreguidelines-macro-usage)
#define ANKERL_UNORDERED_DENSE_VERSION_CONCAT1(major, minor, patch) v##major##_##minor##_##patch
// NOLINTNEXTLINE(cppcoreguidelines-macro-usage)
#define ANKERL_UNORDERED_DENSE_VERSION_CONCAT(major, minor, patch) ANKERL_UNORDERED_DENSE_VERSION_CONCAT1(major, minor, patch)
#define ANKERL_UNORDERED_DENSE_NAMESPACE                                                                                       \
    ANKERL_UNORDERED_DENSE_VERSION_CONCAT(                                                                                     \
        ANKERL_UNORDERED_DENSE_VERSION_MAJOR, ANKERL_UNORDERED_DENSE_VERSION_MINOR, ANKERL_UNORDERED_DENSE_VERSION_PATCH)

#if defined(_MSVC_LANG)
#    define ANKERL_UNORDERED_DENSE_CPP_VERSION _MSVC_LANG
#else
#    define ANKERL_UNORDERED_DENSE_CPP_VERSION __cplusplus
#endif

#if defined(__GNUC__)
// NOLINTNEXTLINE(cppcoreguidelines-macro-usage)
#    define ANKERL_UNORDERED_DENSE_PACK(decl) decl __attribute__((__packed__))
#elif defined(_MSC_VER)
// NOLINTNEXTLINE(cppcoreguidelines-macro-usage)
#    define ANKERL_UNORDERED_DENSE_PACK(decl) __pragma(pack(push, 1)) decl __pragma(pack(pop))
#endif

// exceptions
#if defined(__cpp_exceptions) || defined(__EXCEPTIONS) || defined(_CPPUNWIND)
#    define ANKERL_UNORDERED_DENSE_HAS_EXCEPTIONS() 1 // NOLINT(cppcoreguidelines-macro-usage)
#else
#    define ANKERL_UNORDERED_DENSE_HAS_EXCEPTIONS() 0 // NOLINT(cppcoreguidelines-macro-usage)
#endif
#ifdef _MSC_VER
#    define ANKERL_UNORDERED_DENSE_NOINLINE __declspec(noinline)
#else
#    define ANKERL_UNORDERED_DENSE_NOINLINE __attribute__((noinline))
#endif

// defined in unordered_dense.cpp
#if !defined(ANKERL_UNORDERED_DENSE_EXPORT)
#    define ANKERL_UNORDERED_DENSE_EXPORT
#endif

#if ANKERL_UNORDERED_DENSE_CPP_VERSION < 201703L
#    error ankerl::unordered_dense requires C++17 or higher
#else
#    include <array>            // for array
#    include <cstdint>          // for uint64_t, uint32_t, uint8_t, UINT64_C
#    include <cstring>          // for size_t, memcpy, memset
#    include <functional>       // for equal_to, hash
#    include <initializer_list> // for initializer_list
#    include <iterator>         // for pair, distance
#    include <limits>           // for numeric_limits
#    include <memory>           // for allocator, allocator_traits, shared_ptr
#    include <optional>         // for optional
#    include <stdexcept>        // for out_of_range
#    include <string>           // for basic_string
#    include <string_view>      // for basic_string_view, hash
#    include <tuple>            // for forward_as_tuple
#    include <type_traits>      // for enable_if_t, declval, conditional_t, ena...
#    include <utility>          // for forward, exchange, pair, as_const, piece...
#    include <vector>           // for vector
#    if ANKERL_UNORDERED_DENSE_HAS_EXCEPTIONS() == 0
#        include <cstdlib> // for abort
#    endif

#    if defined(__has_include) && !defined(ANKERL_UNORDERED_DENSE_DISABLE_PMR)
#        if __has_include(<memory_resource>)
#            define ANKERL_UNORDERED_DENSE_PMR std::pmr // NOLINT(cppcoreguidelines-macro-usage)
#            include <memory_resource>                  // for polymorphic_allocator
#        elif __has_include(<experimental/memory_resource>)
#            define ANKERL_UNORDERED_DENSE_PMR std::experimental::pmr // NOLINT(cppcoreguidelines-macro-usage)
#            include <experimental/memory_resource>                   // for polymorphic_allocator
#        endif
#    endif

#    if defined(_MSC_VER) && defined(_M_X64)
#        include <intrin.h>
#        pragma intrinsic(_umul128)
#    endif

#    if defined(__GNUC__) || defined(__INTEL_COMPILER) || defined(__clang__)
#        define ANKERL_UNORDERED_DENSE_LIKELY(x) __builtin_expect(x, 1)   // NOLINT(cppcoreguidelines-macro-usage)
#        define ANKERL_UNORDERED_DENSE_UNLIKELY(x) __builtin_expect(x, 0) // NOLINT(cppcoreguidelines-macro-usage)
#    else
#        define ANKERL_UNORDERED_DENSE_LIKELY(x) (x)   // NOLINT(cppcoreguidelines-macro-usage)
#        define ANKERL_UNORDERED_DENSE_UNLIKELY(x) (x) // NOLINT(cppcoreguidelines-macro-usage)
#    endif

namespace ankerl::unordered_dense
{
    inline namespace ANKERL_UNORDERED_DENSE_NAMESPACE
    {

        namespace detail
        {

#    if ANKERL_UNORDERED_DENSE_HAS_EXCEPTIONS()

            // make sure this is not inlined as it is slow and dramatically enlarges code, thus making other
            // inlinings more difficult. Throws are also generally the slow path.
            [[noreturn]] inline ANKERL_UNORDERED_DENSE_NOINLINE void on_error_key_not_found()
            {
                throw std::out_of_range("ankerl::unordered_dense::map::at(): key not found");
            }
            [[noreturn]] inline ANKERL_UNORDERED_DENSE_NOINLINE void on_error_bucket_overflow()
            {
                throw std::overflow_error("ankerl::unordered_dense: reached max bucket size, cannot increase size");
            }
            [[noreturn]] inline ANKERL_UNORDERED_DENSE_NOINLINE void on_error_too_many_elements()
            {
                throw std::out_of_range("ankerl::unordered_dense::map::replace(): too many elements");
            }

#    else

            [[noreturn]] inline void on_error_key_not_found()
            {
                abort();
            }
            [[noreturn]] inline void on_error_bucket_overflow()
            {
                abort();
            }
            [[noreturn]] inline void on_error_too_many_elements()
            {
                abort();
            }

#    endif

        } // namespace detail

        // hash ///////////////////////////////////////////////////////////////////////

        // This is a stripped-down implementation of wyhash: https://github.com/wangyi-fudan/wyhash
        // No big-endian support (because different values on different machines don't matter),
        // hardcodes seed and the secret, reformats the code, and clang-tidy fixes.
        namespace detail::wyhash
        {

            inline void mum(uint64_t* a, uint64_t* b)
            {
#    if defined(__SIZEOF_INT128__)
                __uint128_t r = *a;
                r *= *b;
                *a = static_cast<uint64_t>(r);
                *b = static_cast<uint64_t>(r >> 64U);
#    elif defined(_MSC_VER) && defined(_M_X64)
                *a = _umul128(*a, *b, b);
#    else
                uint64_t ha = *a >> 32U;
                uint64_t hb = *b >> 32U;
                uint64_t la = static_cast<uint32_t>(*a);
                uint64_t lb = static_cast<uint32_t>(*b);
                uint64_t hi{};
                uint64_t lo{};
                uint64_t rh = ha * hb;
                uint64_t rm0 = ha * lb;
                uint64_t rm1 = hb * la;
                uint64_t rl = la * lb;
                uint64_t t = rl + (rm0 << 32U);
                auto c = static_cast<uint64_t>(t < rl);
                lo = t + (rm1 << 32U);
                c += static_cast<uint64_t>(lo < t);
                hi = rh + (rm0 >> 32U) + (rm1 >> 32U) + c;
                *a = lo;
                *b = hi;
#    endif
            }

            // multiply and xor mix function, aka MUM
            [[nodiscard]] inline auto mix(uint64_t a, uint64_t b) -> uint64_t
            {
                mum(&a, &b);
                return a ^ b;
            }

            // read functions. WARNING: we don't care about endianness, so results are different on big endian!
            [[nodiscard]] inline auto r8(const uint8_t* p) -> uint64_t
            {
                uint64_t v{};
                std::memcpy(&v, p, 8U);
                return v;
            }

            [[nodiscard]] inline auto r4(const uint8_t* p) -> uint64_t
            {
                uint32_t v{};
                std::memcpy(&v, p, 4);
                return v;
            }

            // reads 1, 2, or 3 bytes
            [[nodiscard]] inline auto r3(const uint8_t* p, size_t k) -> uint64_t
            {
                return (static_cast<uint64_t>(p[0]) << 16U) | (static_cast<uint64_t>(p[k >> 1U]) << 8U) | p[k - 1];
            }

            [[maybe_unused]] [[nodiscard]] inline auto hash(void const* key, size_t len) -> uint64_t
            {
                static constexpr auto secret = std::array{ UINT64_C(0xa0761d6478bd642f), UINT64_C(0xe7037ed1a0b428db),
                                                           UINT64_C(0x8ebc6af09c88c6e3), UINT64_C(0x589965cc75374cc3) };

                auto const* p = static_cast<uint8_t const*>(key);
                uint64_t seed = secret[0];
                uint64_t a{};
                uint64_t b{};
                if (ANKERL_UNORDERED_DENSE_LIKELY(len <= 16))
                {
                    if (ANKERL_UNORDERED_DENSE_LIKELY(len >= 4))
                    {
                        a = (r4(p) << 32U) | r4(p + ((len >> 3U) << 2U));
                        b = (r4(p + len - 4) << 32U) | r4(p + len - 4 - ((len >> 3U) << 2U));
                    }
                    else if (ANKERL_UNORDERED_DENSE_LIKELY(len > 0))
                    {
                        a = r3(p, len);
                        b = 0;
                    }
                    else
                    {
                        a = 0;
                        b = 0;
                    }
                }
                else
                {
                    size_t i = len;
                    if (ANKERL_UNORDERED_DENSE_UNLIKELY(i > 48))
                    {
                        uint64_t see1 = seed;
                        uint64_t see2 = seed;
                        do
                        {
                            seed = mix(r8(p) ^ secret[1], r8(p + 8) ^ seed);
                            see1 = mix(r8(p + 16) ^ secret[2], r8(p + 24) ^ see1);
                            see2 = mix(r8(p + 32) ^ secret[3], r8(p + 40) ^ see2);
                            p += 48;
                            i -= 48;
                        } while (ANKERL_UNORDERED_DENSE_LIKELY(i > 48));
                        seed ^= see1 ^ see2;
                    }
                    while (ANKERL_UNORDERED_DENSE_UNLIKELY(i > 16))
                    {
                        seed = mix(r8(p) ^ secret[1], r8(p + 8) ^ seed);
                        i -= 16;
                        p += 16;
                    }
                    a = r8(p + i - 16);
                    b = r8(p + i - 8);
                }

                return mix(secret[1] ^ len, mix(a ^ secret[1], b ^ seed));
            }

            [[nodiscard]] inline auto hash(uint64_t x) -> uint64_t
            {
                return detail::wyhash::mix(x, UINT64_C(0x9E3779B97F4A7C15));
            }

        } // namespace detail::wyhash

        ANKERL_UNORDERED_DENSE_EXPORT template<typename T, typename Enable = void> struct hash
        {
            auto operator()(T const& obj) const
                noexcept(noexcept(std::declval<std::hash<T>>().operator()(std::declval<T const&>()))) -> uint64_t
            {
                return std::hash<T>{}(obj);
            }
        };

        template<typename T> struct hash<T, typename std::hash<T>::is_avalanching>
        {
            using is_avalanching = void;
            auto operator()(T const& obj) const
                noexcept(noexcept(std::declval<std::hash<T>>().operator()(std::declval<T const&>()))) -> uint64_t
            {
                return std::hash<T>{}(obj);
            }
        };

        template<typename CharT> struct hash<std::basic_string<CharT>>
        {
            using is_avalanching = void;
            auto operator()(std::basic_string<CharT> const& str) const noexcept -> uint64_t
            {
                return detail::wyhash::hash(str.data(), sizeof(CharT) * str.size());
            }
        };

        template<typename CharT> struct hash<std::basic_string_view<CharT>>
        {
            using is_avalanching = void;
            auto operator()(std::basic_string_view<CharT> const& sv) const noexcept -> uint64_t
            {
                return detail::wyhash::hash(sv.data(), sizeof(CharT) * sv.size());
            }
        };

        template<class T> struct hash<T*>
        {
            using is_avalanching = void;
            auto operator()(T* ptr) const noexcept -> uint64_t
            {
                // NOLINTNEXTLINE(cppcoreguidelines-pro-type-reinterpret-cast)
                return detail::wyhash::hash(reinterpret_cast<uintptr_t>(ptr));
            }
        };

        template<class T> struct hash<std::unique_ptr<T>>
        {
            using is_avalanching = void;
            auto operator()(std::unique_ptr<T> const& ptr) const noexcept -> uint64_t
            {
                // NOLINTNEXTLINE(cppcoreguidelines-pro-type-reinterpret-cast)
                return detail::wyhash::hash(reinterpret_cast<uintptr_t>(ptr.get()));
            }
        };

        template<class T> struct hash<std::shared_ptr<T>>
        {
            using is_avalanching = void;
            auto operator()(std::shared_ptr<T> const& ptr) const noexcept -> uint64_t
            {
                // NOLINTNEXTLINE(cppcoreguidelines-pro-type-reinterpret-cast)
                return detail::wyhash::hash(reinterpret_cast<uintptr_t>(ptr.get()));
            }
        };

        template<typename Enum> struct hash<Enum, typename std::enable_if<std::is_enum<Enum>::value>::type>
        {
            using is_avalanching = void;
            auto operator()(Enum e) const noexcept -> uint64_t
            {
                using underlying = typename std::underlying_type_t<Enum>;
                return detail::wyhash::hash(static_cast<underlying>(e));
            }
        };

        template<typename... Args> struct tuple_hash_helper
        {
            // Converts the value into 64bit. If it is an integral type, just cast it. Mixing is doing the rest.
            // If it isn't an integral we need to hash it.
            template<typename Arg> [[nodiscard]] constexpr static auto to64(Arg const& arg) -> uint64_t
            {
                if constexpr (std::is_integral_v<Arg> || std::is_enum_v<Arg>)
                {
                    return static_cast<uint64_t>(arg);
                }
                else
                {
                    return hash<Arg>{}(arg);
                }
            }

            [[nodiscard]] static auto mix64(uint64_t state, uint64_t v) -> uint64_t
            {
                return detail::wyhash::mix(state + v, uint64_t{ 0x9ddfea08eb382d69 });
            }

            // Creates a buffer that holds all the data from each element of the tuple. If possible we memcpy the data directly.
            // If not, we hash the object and use this for the array. Size of the array is known at compile time, and memcpy is
            // optimized away, so filling the buffer is highly efficient. Finally, call wyhash with this buffer.
            template<typename T, std::size_t... Idx>
            [[nodiscard]] static auto calc_hash(T const& t, std::index_sequence<Idx...>) noexcept -> uint64_t
            {
                auto h = uint64_t{};
                ((h = mix64(h, to64(std::get<Idx>(t)))), ...);
                return h;
            }
        };

        template<typename... Args> struct hash<std::tuple<Args...>> : tuple_hash_helper<Args...>
        {
            using is_avalanching = void;
            auto operator()(std::tuple<Args...> const& t) const noexcept -> uint64_t
            {
                return tuple_hash_helper<Args...>::calc_hash(t, std::index_sequence_for<Args...>{});
            }
        };

        template<typename A, typename B> struct hash<std::pair<A, B>> : tuple_hash_helper<A, B>
        {
            using is_avalanching = void;
            auto operator()(std::pair<A, B> const& t) const noexcept -> uint64_t
            {
                return tuple_hash_helper<A, B>::calc_hash(t, std::index_sequence_for<A, B>{});
            }
        };

// NOLINTNEXTLINE(cppcoreguidelines-macro-usage)
#    define ANKERL_UNORDERED_DENSE_HASH_STATICCAST(T)                                                                          \
        template<> struct hash<T>                                                                                              \
        {                                                                                                                      \
            using is_avalanching = void;                                                                                       \
            auto operator()(T const& obj) const noexcept -> uint64_t                                                           \
            {                                                                                                                  \
                return detail::wyhash::hash(static_cast<uint64_t>(obj));                                                       \
            }                                                                                                                  \
        }

#    if defined(__GNUC__) && !defined(__clang__)
#        pragma GCC diagnostic push
#        pragma GCC diagnostic ignored "-Wuseless-cast"
#    endif
        // see https://en.cppreference.com/w/cpp/utility/hash
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(bool);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(char);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(signed char);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(unsigned char);
#    if ANKERL_UNORDERED_DENSE_CPP_VERSION >= 202002L && defined(__cpp_char8_t)
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(char8_t);
#    endif
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(char16_t);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(char32_t);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(wchar_t);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(short);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(unsigned short);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(int);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(unsigned int);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(long);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(long long);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(unsigned long);
        ANKERL_UNORDERED_DENSE_HASH_STATICCAST(unsigned long long);

#    if defined(__GNUC__) && !defined(__clang__)
#        pragma GCC diagnostic pop
#    endif

        // bucket_type //////////////////////////////////////////////////////////

        namespace bucket_type
        {

            struct standard
            {
                static constexpr uint32_t dist_inc = 1U << 8U;             // skip 1 byte fingerprint
                static constexpr uint32_t fingerprint_mask = dist_inc - 1; // mask for 1 byte of fingerprint

                uint32_t m_dist_and_fingerprint; // upper 3 byte: distance to original bucket. lower byte: fingerprint from hash
                uint32_t m_value_idx;            // index into the m_values vector.
            };

            ANKERL_UNORDERED_DENSE_PACK(struct big {
                static constexpr uint32_t dist_inc = 1U << 8U;             // skip 1 byte fingerprint
                static constexpr uint32_t fingerprint_mask = dist_inc - 1; // mask for 1 byte of fingerprint

                uint32_t m_dist_and_fingerprint; // upper 3 byte: distance to original bucket. lower byte: fingerprint from hash
                size_t m_value_idx;              // index into the m_values vector.
            });

        } // namespace bucket_type

        namespace detail
        {

            struct nonesuch
            {
            };
            struct default_container_t
            {
            };

            template<class Default, class AlwaysVoid, template<class...> class Op, class... Args> struct detector
            {
                using value_t = std::false_type;
                using type = Default;
            };

            template<class Default, template<class...> class Op, class... Args>
            struct detector<Default, std::void_t<Op<Args...>>, Op, Args...>
            {
                using value_t = std::true_type;
                using type = Op<Args...>;
            };

            template<template<class...> class Op, class... Args>
            using is_detected = typename detail::detector<detail::nonesuch, void, Op, Args...>::value_t;

            template<template<class...> class Op, class... Args> constexpr bool is_detected_v = is_detected<Op, Args...>::value;

            template<typename T> using detect_avalanching = typename T::is_avalanching;

            template<typename T> using detect_is_transparent = typename T::is_transparent;

            template<typename T> using detect_iterator = typename T::iterator;

            template<typename T> using detect_reserve = decltype(std::declval<T&>().reserve(size_t{}));

            // enable_if helpers

            template<typename Mapped> constexpr bool is_map_v = !std::is_void_v<Mapped>;

            // clang-format off
template <typename Hash, typename KeyEqual>
constexpr bool is_transparent_v = is_detected_v<detect_is_transparent, Hash> && is_detected_v<detect_is_transparent, KeyEqual>;
            // clang-format on

            template<typename From, typename To1, typename To2>
            constexpr bool is_neither_convertible_v = !std::is_convertible_v<From, To1> && !std::is_convertible_v<From, To2>;

            template<typename T> constexpr bool has_reserve = is_detected_v<detect_reserve, T>;

            // base type for map has mapped_type
            template<class T> struct base_table_type_map
            {
                using mapped_type = T;
            };

            // base type for set doesn't have mapped_type
            struct base_table_type_set
            {
            };

        } // namespace detail

        // Very much like std::deque, but faster for indexing (in most cases). As of now this doesn't implement the full
        // std::vector API, but merely what's necessary to work as an underlying container for ankerl::unordered_dense::{map,
        // set}. It allocates blocks of equal size and puts them into the m_blocks vector. That means it can grow simply by
        // adding a new block to the back of m_blocks, and doesn't double its size like an std::vector. The disadvantage is that
        // memory is not linear and thus there is one more indirection necessary for indexing.
        template<typename T, typename Allocator = std::allocator<T>, size_t MaxSegmentSizeBytes = 4096> class segmented_vector
        {
            template<bool IsConst> class iter_t;

        public:
            using allocator_type = Allocator;
            using pointer = typename std::allocator_traits<allocator_type>::pointer;
            using const_pointer = typename std::allocator_traits<allocator_type>::const_pointer;
            using difference_type = typename std::allocator_traits<allocator_type>::difference_type;
            using value_type = T;
            using size_type = std::size_t;
            using reference = T&;
            using const_reference = T const&;
            using iterator = iter_t<false>;
            using const_iterator = iter_t<true>;

        private:
            using vec_alloc = typename std::allocator_traits<Allocator>::template rebind_alloc<pointer>;
            std::vector<pointer, vec_alloc> m_blocks{};
            size_t m_size{};

            // Calculates the maximum number for x in  (s << x) <= max_val
            static constexpr auto num_bits_closest(size_t max_val, size_t s) -> size_t
            {
                auto f = size_t{ 0 };
                while (s << (f + 1) <= max_val)
                {
                    ++f;
                }
                return f;
            }

            using self_t = segmented_vector<T, Allocator, MaxSegmentSizeBytes>;
            static constexpr auto num_bits = num_bits_closest(MaxSegmentSizeBytes, sizeof(T));
            static constexpr auto num_elements_in_block = 1U << num_bits;
            static constexpr auto mask = num_elements_in_block - 1U;

            /**
             * Iterator class doubles as const_iterator and iterator
             */
            template<bool IsConst> class iter_t
            {
                using ptr_t = typename std::conditional_t<
                    IsConst, segmented_vector::const_pointer const*, segmented_vector::pointer*>;
                ptr_t m_data{};
                size_t m_idx{};

                template<bool B> friend class iter_t;

            public:
                using difference_type = segmented_vector::difference_type;
                using value_type = T;
                using reference = typename std::conditional_t<IsConst, value_type const&, value_type&>;
                using pointer = typename std::conditional_t<
                    IsConst, segmented_vector::const_pointer, segmented_vector::pointer>;
                using iterator_category = std::forward_iterator_tag;

                iter_t() noexcept = default;

                template<bool OtherIsConst, typename = typename std::enable_if<IsConst && !OtherIsConst>::type>
                // NOLINTNEXTLINE(google-explicit-constructor,hicpp-explicit-conversions)
                constexpr iter_t(iter_t<OtherIsConst> const& other) noexcept
                    : m_data(other.m_data)
                    , m_idx(other.m_idx)
                {
                }

                constexpr iter_t(ptr_t data, size_t idx) noexcept
                    : m_data(data)
                    , m_idx(idx)
                {
                }

                template<bool OtherIsConst, typename = typename std::enable_if<IsConst && !OtherIsConst>::type>
                constexpr auto operator=(iter_t<OtherIsConst> const& other) noexcept -> iter_t&
                {
                    m_data = other.m_data;
                    m_idx = other.m_idx;
                    return *this;
                }

                constexpr auto operator++() noexcept -> iter_t&
                {
                    ++m_idx;
                    return *this;
                }

                constexpr auto operator++(int) noexcept -> iter_t
                {
                    iter_t prev(*this);
                    this->operator++();
                    return prev;
                }

                constexpr auto operator+(difference_type diff) noexcept -> iter_t
                {
                    return { m_data, static_cast<size_t>(static_cast<difference_type>(m_idx) + diff) };
                }

                template<bool OtherIsConst>
                constexpr auto operator-(iter_t<OtherIsConst> const& other) noexcept -> difference_type
                {
                    return static_cast<difference_type>(m_idx) - static_cast<difference_type>(other.m_idx);
                }

                constexpr auto operator*() const noexcept -> reference
                {
                    return m_data[m_idx >> num_bits][m_idx & mask];
                }

                constexpr auto operator->() const noexcept -> pointer
                {
                    return &m_data[m_idx >> num_bits][m_idx & mask];
                }

                template<bool O> constexpr auto operator==(iter_t<O> const& o) const noexcept -> bool
                {
                    return m_idx == o.m_idx;
                }

                template<bool O> constexpr auto operator!=(iter_t<O> const& o) const noexcept -> bool
                {
                    return !(*this == o);
                }
            };

            // slow path: need to allocate a new segment every once in a while
            void increase_capacity()
            {
                auto ba = Allocator(m_blocks.get_allocator());
                pointer block = std::allocator_traits<Allocator>::allocate(ba, num_elements_in_block);
                m_blocks.push_back(block);
            }

            // Moves everything from other
            void append_everything_from(segmented_vector&& other)
            {
                reserve(size() + other.size());
                for (auto&& o : other)
                {
                    emplace_back(std::move(o));
                }
            }

            // Copies everything from other
            void append_everything_from(segmented_vector const& other)
            {
                reserve(size() + other.size());
                for (auto const& o : other)
                {
                    emplace_back(o);
                }
            }

            void dealloc()
            {
                auto ba = Allocator(m_blocks.get_allocator());
                for (auto ptr : m_blocks)
                {
                    std::allocator_traits<Allocator>::deallocate(ba, ptr, num_elements_in_block);
                }
            }

            [[nodiscard]] static constexpr auto calc_num_blocks_for_capacity(size_t capacity)
            {
                return (capacity + num_elements_in_block - 1U) / num_elements_in_block;
            }

        public:
            segmented_vector() = default;

            // NOLINTNEXTLINE(google-explicit-constructor,hicpp-explicit-conversions)
            segmented_vector(Allocator alloc)
                : m_blocks(vec_alloc(alloc))
            {
            }

            segmented_vector(segmented_vector&& other, Allocator alloc)
                : segmented_vector(alloc)
            {
                *this = std::move(other);
            }

            segmented_vector(segmented_vector const& other, Allocator alloc)
                : m_blocks(vec_alloc(alloc))
            {
                append_everything_from(other);
            }

            segmented_vector(segmented_vector&& other) noexcept
                : segmented_vector(std::move(other), get_allocator())
            {
            }

            segmented_vector(segmented_vector const& other)
            {
                append_everything_from(other);
            }

            auto operator=(segmented_vector const& other) -> segmented_vector&
            {
                if (this == &other)
                {
                    return *this;
                }
                clear();
                append_everything_from(other);
                return *this;
            }

            auto operator=(segmented_vector&& other) noexcept -> segmented_vector&
            {
                clear();
                dealloc();
                if (other.get_allocator() == get_allocator())
                {
                    m_blocks = std::move(other.m_blocks);
                    m_size = std::exchange(other.m_size, {});
                }
                else
                {
                    // make sure to construct with other's allocator!
                    m_blocks = std::vector<pointer, vec_alloc>(vec_alloc(other.get_allocator()));
                    append_everything_from(std::move(other));
                }
                return *this;
            }

            ~segmented_vector()
            {
                clear();
                dealloc();
            }

            [[nodiscard]] constexpr auto size() const -> size_t
            {
                return m_size;
            }

            [[nodiscard]] constexpr auto capacity() const -> size_t
            {
                return m_blocks.size() * num_elements_in_block;
            }

            // Indexing is highly performance critical
            [[nodiscard]] constexpr auto operator[](size_t i) const noexcept -> T const&
            {
                return m_blocks[i >> num_bits][i & mask];
            }

            [[nodiscard]] constexpr auto operator[](size_t i) noexcept -> T&
            {
                return m_blocks[i >> num_bits][i & mask];
            }

            [[nodiscard]] constexpr auto begin() -> iterator
            {
                return { m_blocks.data(), 0U };
            }
            [[nodiscard]] constexpr auto begin() const -> const_iterator
            {
                return { m_blocks.data(), 0U };
            }
            [[nodiscard]] constexpr auto cbegin() const -> const_iterator
            {
                return { m_blocks.data(), 0U };
            }

            [[nodiscard]] constexpr auto end() -> iterator
            {
                return { m_blocks.data(), m_size };
            }
            [[nodiscard]] constexpr auto end() const -> const_iterator
            {
                return { m_blocks.data(), m_size };
            }
            [[nodiscard]] constexpr auto cend() const -> const_iterator
            {
                return { m_blocks.data(), m_size };
            }

            [[nodiscard]] constexpr auto back() -> reference
            {
                return operator[](m_size - 1);
            }
            [[nodiscard]] constexpr auto back() const -> const_reference
            {
                return operator[](m_size - 1);
            }

            void pop_back()
            {
                back().~T();
                --m_size;
            }

            [[nodiscard]] auto empty() const
            {
                return 0 == m_size;
            }

            void reserve(size_t new_capacity)
            {
                m_blocks.reserve(calc_num_blocks_for_capacity(new_capacity));
                while (new_capacity > capacity())
                {
                    increase_capacity();
                }
            }

            [[nodiscard]] auto get_allocator() const -> allocator_type
            {
                return allocator_type{ m_blocks.get_allocator() };
            }

            template<class... Args> auto emplace_back(Args&&... args) -> reference
            {
                if (m_size == capacity())
                {
                    increase_capacity();
                }
                auto* ptr = static_cast<void*>(&operator[](m_size));
                auto& ref = *new (ptr) T(std::forward<Args>(args)...);
                ++m_size;
                return ref;
            }

            void clear()
            {
                if constexpr (!std::is_trivially_destructible_v<T>)
                {
                    for (size_t i = 0, s = size(); i < s; ++i)
                    {
                        operator[](i).~T();
                    }
                }
                m_size = 0;
            }

            void shrink_to_fit()
            {
                auto ba = Allocator(m_blocks.get_allocator());
                auto num_blocks_required = calc_num_blocks_for_capacity(m_size);
                while (m_blocks.size() > num_blocks_required)
                {
                    std::allocator_traits<Allocator>::deallocate(ba, m_blocks.back(), num_elements_in_block);
                    m_blocks.pop_back();
                }
                m_blocks.shrink_to_fit();
            }
        };

        namespace detail
        {

            // This is it, the table. Doubles as map and set, and uses `void` for T when its used as a set.
            template<
                class Key,
                class T, // when void, treat it as a set.
                class Hash, class KeyEqual, class AllocatorOrContainer, class Bucket, class BucketContainer, bool IsSegmented>
            class table : public std::conditional_t<is_map_v<T>, base_table_type_map<T>, base_table_type_set>
            {
                using underlying_value_type = typename std::conditional_t<is_map_v<T>, std::pair<Key, T>, Key>;
                using underlying_container_type = std::conditional_t<
                    IsSegmented, segmented_vector<underlying_value_type, AllocatorOrContainer>,
                    std::vector<underlying_value_type, AllocatorOrContainer>>;

            public:
                using value_container_type = std::conditional_t<
                    is_detected_v<detect_iterator, AllocatorOrContainer>, AllocatorOrContainer, underlying_container_type>;

            private:
                using bucket_alloc = typename std::allocator_traits<
                    typename value_container_type::allocator_type>::template rebind_alloc<Bucket>;
                using default_bucket_container_type = std::conditional_t<
                    IsSegmented, segmented_vector<Bucket, bucket_alloc>, std::vector<Bucket, bucket_alloc>>;

                using bucket_container_type = std::conditional_t<
                    std::is_same_v<BucketContainer, detail::default_container_t>, default_bucket_container_type,
                    BucketContainer>;

                static constexpr uint8_t initial_shifts = 64 - 2; // 2^(64-m_shift) number of buckets
                static constexpr float default_max_load_factor = 0.8F;

            public:
                using key_type = Key;
                using value_type = typename value_container_type::value_type;
                using size_type = typename value_container_type::size_type;
                using difference_type = typename value_container_type::difference_type;
                using hasher = Hash;
                using key_equal = KeyEqual;
                using allocator_type = typename value_container_type::allocator_type;
                using reference = typename value_container_type::reference;
                using const_reference = typename value_container_type::const_reference;
                using pointer = typename value_container_type::pointer;
                using const_pointer = typename value_container_type::const_pointer;
                using const_iterator = typename value_container_type::const_iterator;
                using iterator = std::conditional_t<is_map_v<T>, typename value_container_type::iterator, const_iterator>;
                using bucket_type = Bucket;

            private:
                using value_idx_type = decltype(Bucket::m_value_idx);
                using dist_and_fingerprint_type = decltype(Bucket::m_dist_and_fingerprint);

                static_assert(
                    std::is_trivially_destructible_v<Bucket>, "assert there's no need to call destructor / std::destroy");
                static_assert(std::is_trivially_copyable_v<Bucket>, "assert we can just memset / memcpy");

                value_container_type m_values{}; // Contains all the key-value pairs in one densely stored container. No holes.
                bucket_container_type m_buckets{};
                size_t m_max_bucket_capacity = 0;
                float m_max_load_factor = default_max_load_factor;
                Hash m_hash{};
                KeyEqual m_equal{};
                uint8_t m_shifts = initial_shifts;

                [[nodiscard]] auto next(value_idx_type bucket_idx) const -> value_idx_type
                {
                    return ANKERL_UNORDERED_DENSE_UNLIKELY(bucket_idx + 1U == bucket_count())
                        ? 0
                        : static_cast<value_idx_type>(bucket_idx + 1U);
                }

                // Helper to access bucket through pointer types
                [[nodiscard]] static constexpr auto at(bucket_container_type& bucket, size_t offset) -> Bucket&
                {
                    return bucket[offset];
                }

                [[nodiscard]] static constexpr auto at(const bucket_container_type& bucket, size_t offset) -> const Bucket&
                {
                    return bucket[offset];
                }

                // use the dist_inc and dist_dec functions so that uint16_t types work without warning
                [[nodiscard]] static constexpr auto dist_inc(dist_and_fingerprint_type x) -> dist_and_fingerprint_type
                {
                    return static_cast<dist_and_fingerprint_type>(x + Bucket::dist_inc);
                }

                [[nodiscard]] static constexpr auto dist_dec(dist_and_fingerprint_type x) -> dist_and_fingerprint_type
                {
                    return static_cast<dist_and_fingerprint_type>(x - Bucket::dist_inc);
                }

                // The goal of mixed_hash is to always produce a high quality 64bit hash.
                template<typename K> [[nodiscard]] constexpr auto mixed_hash(K const& key) const -> uint64_t
                {
                    if constexpr (is_detected_v<detect_avalanching, Hash>)
                    {
                        // we know that the hash is good because is_avalanching.
                        if constexpr (sizeof(decltype(m_hash(key))) < sizeof(uint64_t))
                        {
                            // 32bit hash and is_avalanching => multiply with a constant to avalanche bits upwards
                            return m_hash(key) * UINT64_C(0x9ddfea08eb382d69);
                        }
                        else
                        {
                            // 64bit and is_avalanching => only use the hash itself.
                            return m_hash(key);
                        }
                    }
                    else
                    {
                        // not is_avalanching => apply wyhash
                        return wyhash::hash(m_hash(key));
                    }
                }

                [[nodiscard]] constexpr auto dist_and_fingerprint_from_hash(uint64_t hash) const -> dist_and_fingerprint_type
                {
                    return Bucket::dist_inc | (static_cast<dist_and_fingerprint_type>(hash) & Bucket::fingerprint_mask);
                }

                [[nodiscard]] constexpr auto bucket_idx_from_hash(uint64_t hash) const -> value_idx_type
                {
                    return static_cast<value_idx_type>(hash >> m_shifts);
                }

                [[nodiscard]] static constexpr auto get_key(value_type const& vt) -> key_type const&
                {
                    if constexpr (is_map_v<T>)
                    {
                        return vt.first;
                    }
                    else
                    {
                        return vt;
                    }
                }

                template<typename K> [[nodiscard]] auto next_while_less(K const& key) const -> Bucket
                {
                    auto hash = mixed_hash(key);
                    auto dist_and_fingerprint = dist_and_fingerprint_from_hash(hash);
                    auto bucket_idx = bucket_idx_from_hash(hash);

                    while (dist_and_fingerprint < at(m_buckets, bucket_idx).m_dist_and_fingerprint)
                    {
                        dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                        bucket_idx = next(bucket_idx);
                    }
                    return { dist_and_fingerprint, bucket_idx };
                }

                void place_and_shift_up(Bucket bucket, value_idx_type place)
                {
                    while (0 != at(m_buckets, place).m_dist_and_fingerprint)
                    {
                        bucket = std::exchange(at(m_buckets, place), bucket);
                        bucket.m_dist_and_fingerprint = dist_inc(bucket.m_dist_and_fingerprint);
                        place = next(place);
                    }
                    at(m_buckets, place) = bucket;
                }

                [[nodiscard]] static constexpr auto calc_num_buckets(uint8_t shifts) -> size_t
                {
                    return (std::min)(max_bucket_count(), size_t{ 1 } << (64U - shifts));
                }

                [[nodiscard]] constexpr auto calc_shifts_for_size(size_t s) const -> uint8_t
                {
                    auto shifts = initial_shifts;
                    while (shifts > 0
                           && static_cast<size_t>(static_cast<float>(calc_num_buckets(shifts)) * max_load_factor()) < s)
                    {
                        --shifts;
                    }
                    return shifts;
                }

                // assumes m_values has data, m_buckets=m_buckets_end=nullptr, m_shifts is INITIAL_SHIFTS
                void copy_buckets(table const& other)
                {
                    // assumes m_values has already the correct data copied over.
                    if (empty())
                    {
                        // when empty, at least allocate an initial buckets and clear them.
                        allocate_buckets_from_shift();
                        clear_buckets();
                    }
                    else
                    {
                        m_shifts = other.m_shifts;
                        allocate_buckets_from_shift();
                        if constexpr (IsSegmented || !std::is_same_v<BucketContainer, default_container_t>)
                        {
                            for (auto i = 0UL; i < bucket_count(); ++i)
                            {
                                at(m_buckets, i) = at(other.m_buckets, i);
                            }
                        }
                        else
                        {
                            std::memcpy(m_buckets.data(), other.m_buckets.data(), sizeof(Bucket) * bucket_count());
                        }
                    }
                }

                /**
                 * True when no element can be added any more without increasing the size
                 */
                [[nodiscard]] auto is_full() const -> bool
                {
                    return size() > m_max_bucket_capacity;
                }

                void deallocate_buckets()
                {
                    m_buckets.clear();
                    m_buckets.shrink_to_fit();
                    m_max_bucket_capacity = 0;
                }

                void allocate_buckets_from_shift()
                {
                    auto num_buckets = calc_num_buckets(m_shifts);
                    if constexpr (IsSegmented || !std::is_same_v<BucketContainer, default_container_t>)
                    {
                        if constexpr (has_reserve<bucket_container_type>)
                        {
                            m_buckets.reserve(num_buckets);
                        }
                        for (size_t i = m_buckets.size(); i < num_buckets; ++i)
                        {
                            m_buckets.emplace_back();
                        }
                    }
                    else
                    {
                        m_buckets.resize(num_buckets);
                    }
                    if (num_buckets == max_bucket_count())
                    {
                        // reached the maximum, make sure we can use each bucket
                        m_max_bucket_capacity = max_bucket_count();
                    }
                    else
                    {
                        m_max_bucket_capacity = static_cast<value_idx_type>(
                            static_cast<float>(num_buckets) * max_load_factor());
                    }
                }

                void clear_buckets()
                {
                    if constexpr (IsSegmented || !std::is_same_v<BucketContainer, default_container_t>)
                    {
                        for (auto&& e : m_buckets)
                        {
                            std::memset(&e, 0, sizeof(e));
                        }
                    }
                    else
                    {
                        std::memset(m_buckets.data(), 0, sizeof(Bucket) * bucket_count());
                    }
                }

                void clear_and_fill_buckets_from_values()
                {
                    clear_buckets();
                    for (value_idx_type value_idx = 0, end_idx = static_cast<value_idx_type>(m_values.size());
                         value_idx < end_idx; ++value_idx)
                    {
                        auto const& key = get_key(m_values[value_idx]);
                        auto [dist_and_fingerprint, bucket] = next_while_less(key);

                        // we know for certain that key has not yet been inserted, so no need to check it.
                        place_and_shift_up({ dist_and_fingerprint, value_idx }, bucket);
                    }
                }

                void increase_size()
                {
                    if (m_max_bucket_capacity == max_bucket_count())
                    {
                        // remove the value again, we can't add it!
                        m_values.pop_back();
                        on_error_bucket_overflow();
                    }
                    --m_shifts;
                    if constexpr (!IsSegmented || std::is_same_v<BucketContainer, default_container_t>)
                    {
                        deallocate_buckets();
                    }
                    allocate_buckets_from_shift();
                    clear_and_fill_buckets_from_values();
                }

                template<typename Op> void do_erase(value_idx_type bucket_idx, Op handle_erased_value)
                {
                    auto const value_idx_to_remove = at(m_buckets, bucket_idx).m_value_idx;

                    // shift down until either empty or an element with correct spot is found
                    auto next_bucket_idx = next(bucket_idx);
                    while (at(m_buckets, next_bucket_idx).m_dist_and_fingerprint >= Bucket::dist_inc * 2)
                    {
                        at(m_buckets, bucket_idx) = { dist_dec(at(m_buckets, next_bucket_idx).m_dist_and_fingerprint),
                                                      at(m_buckets, next_bucket_idx).m_value_idx };
                        bucket_idx = std::exchange(next_bucket_idx, next(next_bucket_idx));
                    }
                    at(m_buckets, bucket_idx) = {};
                    handle_erased_value(std::move(m_values[value_idx_to_remove]));

                    // update m_values
                    if (value_idx_to_remove != m_values.size() - 1)
                    {
                        // no luck, we'll have to replace the value with the last one and update the index accordingly
                        auto& val = m_values[value_idx_to_remove];
                        val = std::move(m_values.back());

                        // update the values_idx of the moved entry. No need to play the info game, just look until we find the
                        // values_idx
                        auto mh = mixed_hash(get_key(val));
                        bucket_idx = bucket_idx_from_hash(mh);

                        auto const values_idx_back = static_cast<value_idx_type>(m_values.size() - 1);
                        while (values_idx_back != at(m_buckets, bucket_idx).m_value_idx)
                        {
                            bucket_idx = next(bucket_idx);
                        }
                        at(m_buckets, bucket_idx).m_value_idx = value_idx_to_remove;
                    }
                    m_values.pop_back();
                }

                template<typename K, typename Op> auto do_erase_key(K&& key, Op handle_erased_value) -> size_t
                {
                    if (empty())
                    {
                        return 0;
                    }

                    auto [dist_and_fingerprint, bucket_idx] = next_while_less(key);

                    while (dist_and_fingerprint == at(m_buckets, bucket_idx).m_dist_and_fingerprint
                           && !m_equal(key, get_key(m_values[at(m_buckets, bucket_idx).m_value_idx])))
                    {
                        dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                        bucket_idx = next(bucket_idx);
                    }

                    if (dist_and_fingerprint != at(m_buckets, bucket_idx).m_dist_and_fingerprint)
                    {
                        return 0;
                    }
                    do_erase(bucket_idx, handle_erased_value);
                    return 1;
                }

                template<class K, class M> auto do_insert_or_assign(K&& key, M&& mapped) -> std::pair<iterator, bool>
                {
                    auto it_isinserted = try_emplace(std::forward<K>(key), std::forward<M>(mapped));
                    if (!it_isinserted.second)
                    {
                        it_isinserted.first->second = std::forward<M>(mapped);
                    }
                    return it_isinserted;
                }

                template<typename... Args>
                auto do_place_element(dist_and_fingerprint_type dist_and_fingerprint, value_idx_type bucket_idx, Args&&... args)
                    -> std::pair<iterator, bool>
                {
                    // emplace the new value. If that throws an exception, no harm done; index is still in a valid state
                    m_values.emplace_back(std::forward<Args>(args)...);

                    auto value_idx = static_cast<value_idx_type>(m_values.size() - 1);
                    if (ANKERL_UNORDERED_DENSE_UNLIKELY(is_full()))
                    {
                        increase_size();
                    }
                    else
                    {
                        place_and_shift_up({ dist_and_fingerprint, value_idx }, bucket_idx);
                    }

                    // place element and shift up until we find an empty spot
                    return { begin() + static_cast<difference_type>(value_idx), true };
                }

                template<typename K, typename... Args> auto do_try_emplace(K&& key, Args&&... args) -> std::pair<iterator, bool>
                {
                    auto hash = mixed_hash(key);
                    auto dist_and_fingerprint = dist_and_fingerprint_from_hash(hash);
                    auto bucket_idx = bucket_idx_from_hash(hash);

                    while (true)
                    {
                        auto* bucket = &at(m_buckets, bucket_idx);
                        if (dist_and_fingerprint == bucket->m_dist_and_fingerprint)
                        {
                            if (m_equal(key, get_key(m_values[bucket->m_value_idx])))
                            {
                                return { begin() + static_cast<difference_type>(bucket->m_value_idx), false };
                            }
                        }
                        else if (dist_and_fingerprint > bucket->m_dist_and_fingerprint)
                        {
                            return do_place_element(
                                dist_and_fingerprint, bucket_idx, std::piecewise_construct,
                                std::forward_as_tuple(std::forward<K>(key)),
                                std::forward_as_tuple(std::forward<Args>(args)...));
                        }
                        dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                        bucket_idx = next(bucket_idx);
                    }
                }

                template<typename K> auto do_find(K const& key) -> iterator
                {
                    if (ANKERL_UNORDERED_DENSE_UNLIKELY(empty()))
                    {
                        return end();
                    }

                    auto mh = mixed_hash(key);
                    auto dist_and_fingerprint = dist_and_fingerprint_from_hash(mh);
                    auto bucket_idx = bucket_idx_from_hash(mh);
                    auto* bucket = &at(m_buckets, bucket_idx);

                    // unrolled loop. *Always* check a few directly, then enter the loop. This is faster.
                    if (dist_and_fingerprint == bucket->m_dist_and_fingerprint
                        && m_equal(key, get_key(m_values[bucket->m_value_idx])))
                    {
                        return begin() + static_cast<difference_type>(bucket->m_value_idx);
                    }
                    dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                    bucket_idx = next(bucket_idx);
                    bucket = &at(m_buckets, bucket_idx);

                    if (dist_and_fingerprint == bucket->m_dist_and_fingerprint
                        && m_equal(key, get_key(m_values[bucket->m_value_idx])))
                    {
                        return begin() + static_cast<difference_type>(bucket->m_value_idx);
                    }
                    dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                    bucket_idx = next(bucket_idx);
                    bucket = &at(m_buckets, bucket_idx);

                    while (true)
                    {
                        if (dist_and_fingerprint == bucket->m_dist_and_fingerprint)
                        {
                            if (m_equal(key, get_key(m_values[bucket->m_value_idx])))
                            {
                                return begin() + static_cast<difference_type>(bucket->m_value_idx);
                            }
                        }
                        else if (dist_and_fingerprint > bucket->m_dist_and_fingerprint)
                        {
                            return end();
                        }
                        dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                        bucket_idx = next(bucket_idx);
                        bucket = &at(m_buckets, bucket_idx);
                    }
                }

                template<typename K> auto do_find(K const& key) const -> const_iterator
                {
                    return const_cast<table*>(this)->do_find(key); // NOLINT(cppcoreguidelines-pro-type-const-cast)
                }

                template<typename K, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true> auto do_at(K const& key) -> Q&
                {
                    if (auto it = find(key); ANKERL_UNORDERED_DENSE_LIKELY(end() != it))
                    {
                        return it->second;
                    }
                    on_error_key_not_found();
                }

                template<typename K, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto do_at(K const& key) const -> Q const&
                {
                    return const_cast<table*>(this)->at(key); // NOLINT(cppcoreguidelines-pro-type-const-cast)
                }

            public:
                explicit table(
                    size_t bucket_count, Hash const& hash = Hash(), KeyEqual const& equal = KeyEqual(),
                    allocator_type const& alloc_or_container = allocator_type())
                    : m_values(alloc_or_container)
                    , m_buckets(alloc_or_container)
                    , m_hash(hash)
                    , m_equal(equal)
                {
                    if (0 != bucket_count)
                    {
                        reserve(bucket_count);
                    }
                    else
                    {
                        allocate_buckets_from_shift();
                        clear_buckets();
                    }
                }

                table()
                    : table(0)
                {
                }

                table(size_t bucket_count, allocator_type const& alloc)
                    : table(bucket_count, Hash(), KeyEqual(), alloc)
                {
                }

                table(size_t bucket_count, Hash const& hash, allocator_type const& alloc)
                    : table(bucket_count, hash, KeyEqual(), alloc)
                {
                }

                explicit table(allocator_type const& alloc)
                    : table(0, Hash(), KeyEqual(), alloc)
                {
                }

                template<class InputIt>
                table(
                    InputIt first, InputIt last, size_type bucket_count = 0, Hash const& hash = Hash(),
                    KeyEqual const& equal = KeyEqual(), allocator_type const& alloc = allocator_type())
                    : table(bucket_count, hash, equal, alloc)
                {
                    insert(first, last);
                }

                template<class InputIt>
                table(InputIt first, InputIt last, size_type bucket_count, allocator_type const& alloc)
                    : table(first, last, bucket_count, Hash(), KeyEqual(), alloc)
                {
                }

                template<class InputIt>
                table(InputIt first, InputIt last, size_type bucket_count, Hash const& hash, allocator_type const& alloc)
                    : table(first, last, bucket_count, hash, KeyEqual(), alloc)
                {
                }

                table(table const& other)
                    : table(other, other.m_values.get_allocator())
                {
                }

                table(table const& other, allocator_type const& alloc)
                    : m_values(other.m_values, alloc)
                    , m_max_load_factor(other.m_max_load_factor)
                    , m_hash(other.m_hash)
                    , m_equal(other.m_equal)
                {
                    copy_buckets(other);
                }

                table(table&& other) noexcept
                    : table(std::move(other), other.m_values.get_allocator())
                {
                }

                table(table&& other, allocator_type const& alloc) noexcept
                    : m_values(alloc)
                {
                    *this = std::move(other);
                }

                table(
                    std::initializer_list<value_type> ilist, size_t bucket_count = 0, Hash const& hash = Hash(),
                    KeyEqual const& equal = KeyEqual(), allocator_type const& alloc = allocator_type())
                    : table(bucket_count, hash, equal, alloc)
                {
                    insert(ilist);
                }

                table(std::initializer_list<value_type> ilist, size_type bucket_count, allocator_type const& alloc)
                    : table(ilist, bucket_count, Hash(), KeyEqual(), alloc)
                {
                }

                table(
                    std::initializer_list<value_type> init, size_type bucket_count, Hash const& hash,
                    allocator_type const& alloc)
                    : table(init, bucket_count, hash, KeyEqual(), alloc)
                {
                }

                ~table()
                {
                }

                auto operator=(table const& other) -> table&
                {
                    if (&other != this)
                    {
                        deallocate_buckets(); // deallocate before m_values is set (might have another allocator)
                        m_values = other.m_values;
                        m_max_load_factor = other.m_max_load_factor;
                        m_hash = other.m_hash;
                        m_equal = other.m_equal;
                        m_shifts = initial_shifts;
                        copy_buckets(other);
                    }
                    return *this;
                }

                auto operator=(table&& other) noexcept(noexcept(
                    std::is_nothrow_move_assignable_v<value_container_type> && std::is_nothrow_move_assignable_v<Hash>
                    && std::is_nothrow_move_assignable_v<KeyEqual>)) -> table&
                {
                    if (&other != this)
                    {
                        deallocate_buckets(); // deallocate before m_values is set (might have another allocator)
                        m_values = std::move(other.m_values);
                        other.m_values.clear();

                        // we can only reuse m_buckets when both maps have the same allocator!
                        if (get_allocator() == other.get_allocator())
                        {
                            m_buckets = std::move(other.m_buckets);
                            other.m_buckets.clear();
                            m_max_bucket_capacity = std::exchange(other.m_max_bucket_capacity, 0);
                            m_shifts = std::exchange(other.m_shifts, initial_shifts);
                            m_max_load_factor = std::exchange(other.m_max_load_factor, default_max_load_factor);
                            m_hash = std::exchange(other.m_hash, {});
                            m_equal = std::exchange(other.m_equal, {});
                            other.allocate_buckets_from_shift();
                            other.clear_buckets();
                        }
                        else
                        {
                            // set max_load_factor *before* copying the other's buckets, so we have the same
                            // behavior
                            m_max_load_factor = other.m_max_load_factor;

                            // copy_buckets sets m_buckets, m_num_buckets, m_max_bucket_capacity, m_shifts
                            copy_buckets(other);
                            // clear's the other's buckets so other is now already usable.
                            other.clear_buckets();
                            m_hash = other.m_hash;
                            m_equal = other.m_equal;
                        }
                        // map "other" is now already usable, it's empty.
                    }
                    return *this;
                }

                auto operator=(std::initializer_list<value_type> ilist) -> table&
                {
                    clear();
                    insert(ilist);
                    return *this;
                }

                auto get_allocator() const noexcept -> allocator_type
                {
                    return m_values.get_allocator();
                }

                // iterators //////////////////////////////////////////////////////////////

                auto begin() noexcept -> iterator
                {
                    return m_values.begin();
                }

                auto begin() const noexcept -> const_iterator
                {
                    return m_values.begin();
                }

                auto cbegin() const noexcept -> const_iterator
                {
                    return m_values.cbegin();
                }

                auto end() noexcept -> iterator
                {
                    return m_values.end();
                }

                auto cend() const noexcept -> const_iterator
                {
                    return m_values.cend();
                }

                auto end() const noexcept -> const_iterator
                {
                    return m_values.end();
                }

                // capacity ///////////////////////////////////////////////////////////////

                [[nodiscard]] auto empty() const noexcept -> bool
                {
                    return m_values.empty();
                }

                [[nodiscard]] auto size() const noexcept -> size_t
                {
                    return m_values.size();
                }

                [[nodiscard]] static constexpr auto max_size() noexcept -> size_t
                {
                    if constexpr ((std::numeric_limits<value_idx_type>::max)() == (std::numeric_limits<size_t>::max)())
                    {
                        return size_t{ 1 } << (sizeof(value_idx_type) * 8 - 1);
                    }
                    else
                    {
                        return size_t{ 1 } << (sizeof(value_idx_type) * 8);
                    }
                }

                // modifiers //////////////////////////////////////////////////////////////

                void clear()
                {
                    m_values.clear();
                    clear_buckets();
                }

                auto insert(value_type const& value) -> std::pair<iterator, bool>
                {
                    return emplace(value);
                }

                auto insert(value_type&& value) -> std::pair<iterator, bool>
                {
                    return emplace(std::move(value));
                }

                template<class P, std::enable_if_t<std::is_constructible_v<value_type, P&&>, bool> = true>
                auto insert(P&& value) -> std::pair<iterator, bool>
                {
                    return emplace(std::forward<P>(value));
                }

                auto insert(const_iterator /*hint*/, value_type const& value) -> iterator
                {
                    return insert(value).first;
                }

                auto insert(const_iterator /*hint*/, value_type&& value) -> iterator
                {
                    return insert(std::move(value)).first;
                }

                template<class P, std::enable_if_t<std::is_constructible_v<value_type, P&&>, bool> = true>
                auto insert(const_iterator /*hint*/, P&& value) -> iterator
                {
                    return insert(std::forward<P>(value)).first;
                }

                template<class InputIt> void insert(InputIt first, InputIt last)
                {
                    while (first != last)
                    {
                        insert(*first);
                        ++first;
                    }
                }

                void insert(std::initializer_list<value_type> ilist)
                {
                    insert(ilist.begin(), ilist.end());
                }

                // nonstandard API: *this is emptied.
                // Also see "A Standard flat_map" https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p0429r9.pdf
                auto extract() && -> value_container_type
                {
                    return std::move(m_values);
                }

                // nonstandard API:
                // Discards the internally held container and replaces it with the one passed. Erases non-unique elements.
                auto replace(value_container_type&& container)
                {
                    if (ANKERL_UNORDERED_DENSE_UNLIKELY(container.size() > max_size()))
                    {
                        on_error_too_many_elements();
                    }
                    auto shifts = calc_shifts_for_size(container.size());
                    if (0 == bucket_count() || shifts < m_shifts || container.get_allocator() != m_values.get_allocator())
                    {
                        m_shifts = shifts;
                        deallocate_buckets();
                        allocate_buckets_from_shift();
                    }
                    clear_buckets();

                    m_values = std::move(container);

                    // can't use clear_and_fill_buckets_from_values() because container elements might not be unique
                    auto value_idx = value_idx_type{};

                    // loop until we reach the end of the container. duplicated entries will be replaced with back().
                    while (value_idx != static_cast<value_idx_type>(m_values.size()))
                    {
                        auto const& key = get_key(m_values[value_idx]);

                        auto hash = mixed_hash(key);
                        auto dist_and_fingerprint = dist_and_fingerprint_from_hash(hash);
                        auto bucket_idx = bucket_idx_from_hash(hash);

                        bool key_found = false;
                        while (true)
                        {
                            auto const& bucket = at(m_buckets, bucket_idx);
                            if (dist_and_fingerprint > bucket.m_dist_and_fingerprint)
                            {
                                break;
                            }
                            if (dist_and_fingerprint == bucket.m_dist_and_fingerprint
                                && m_equal(key, get_key(m_values[bucket.m_value_idx])))
                            {
                                key_found = true;
                                break;
                            }
                            dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                            bucket_idx = next(bucket_idx);
                        }

                        if (key_found)
                        {
                            if (value_idx != static_cast<value_idx_type>(m_values.size() - 1))
                            {
                                m_values[value_idx] = std::move(m_values.back());
                            }
                            m_values.pop_back();
                        }
                        else
                        {
                            place_and_shift_up({ dist_and_fingerprint, value_idx }, bucket_idx);
                            ++value_idx;
                        }
                    }
                }

                template<class M, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto insert_or_assign(Key const& key, M&& mapped) -> std::pair<iterator, bool>
                {
                    return do_insert_or_assign(key, std::forward<M>(mapped));
                }

                template<class M, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto insert_or_assign(Key&& key, M&& mapped) -> std::pair<iterator, bool>
                {
                    return do_insert_or_assign(std::move(key), std::forward<M>(mapped));
                }

                template<
                    typename K, typename M, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<is_map_v<Q> && is_transparent_v<H, KE>, bool> = true>
                auto insert_or_assign(K&& key, M&& mapped) -> std::pair<iterator, bool>
                {
                    return do_insert_or_assign(std::forward<K>(key), std::forward<M>(mapped));
                }

                template<class M, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto insert_or_assign(const_iterator /*hint*/, Key const& key, M&& mapped) -> iterator
                {
                    return do_insert_or_assign(key, std::forward<M>(mapped)).first;
                }

                template<class M, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto insert_or_assign(const_iterator /*hint*/, Key&& key, M&& mapped) -> iterator
                {
                    return do_insert_or_assign(std::move(key), std::forward<M>(mapped)).first;
                }

                template<
                    typename K, typename M, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<is_map_v<Q> && is_transparent_v<H, KE>, bool> = true>
                auto insert_or_assign(const_iterator /*hint*/, K&& key, M&& mapped) -> iterator
                {
                    return do_insert_or_assign(std::forward<K>(key), std::forward<M>(mapped)).first;
                }

                // Single arguments for unordered_set can be used without having to construct the value_type
                template<
                    class K, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<!is_map_v<Q> && is_transparent_v<H, KE>, bool> = true>
                auto emplace(K&& key) -> std::pair<iterator, bool>
                {
                    auto hash = mixed_hash(key);
                    auto dist_and_fingerprint = dist_and_fingerprint_from_hash(hash);
                    auto bucket_idx = bucket_idx_from_hash(hash);

                    while (dist_and_fingerprint <= at(m_buckets, bucket_idx).m_dist_and_fingerprint)
                    {
                        if (dist_and_fingerprint == at(m_buckets, bucket_idx).m_dist_and_fingerprint
                            && m_equal(key, m_values[at(m_buckets, bucket_idx).m_value_idx]))
                        {
                            // found it, return without ever actually creating anything
                            return { begin() + static_cast<difference_type>(at(m_buckets, bucket_idx).m_value_idx), false };
                        }
                        dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                        bucket_idx = next(bucket_idx);
                    }

                    // value is new, insert element first, so when exception happens we are in a valid state
                    return do_place_element(dist_and_fingerprint, bucket_idx, std::forward<K>(key));
                }

                template<class... Args> auto emplace(Args&&... args) -> std::pair<iterator, bool>
                {
                    // we have to instantiate the value_type to be able to access the key.
                    // 1. emplace_back the object so it is constructed. 2. If the key is already there, pop it later in the
                    // loop.
                    auto& key = get_key(m_values.emplace_back(std::forward<Args>(args)...));
                    auto hash = mixed_hash(key);
                    auto dist_and_fingerprint = dist_and_fingerprint_from_hash(hash);
                    auto bucket_idx = bucket_idx_from_hash(hash);

                    while (dist_and_fingerprint <= at(m_buckets, bucket_idx).m_dist_and_fingerprint)
                    {
                        if (dist_and_fingerprint == at(m_buckets, bucket_idx).m_dist_and_fingerprint
                            && m_equal(key, get_key(m_values[at(m_buckets, bucket_idx).m_value_idx])))
                        {
                            m_values.pop_back(); // value was already there, so get rid of it
                            return { begin() + static_cast<difference_type>(at(m_buckets, bucket_idx).m_value_idx), false };
                        }
                        dist_and_fingerprint = dist_inc(dist_and_fingerprint);
                        bucket_idx = next(bucket_idx);
                    }

                    // value is new, place the bucket and shift up until we find an empty spot
                    auto value_idx = static_cast<value_idx_type>(m_values.size() - 1);
                    if (ANKERL_UNORDERED_DENSE_UNLIKELY(is_full()))
                    {
                        // increase_size just rehashes all the data we have in m_values
                        increase_size();
                    }
                    else
                    {
                        // place element and shift up until we find an empty spot
                        place_and_shift_up({ dist_and_fingerprint, value_idx }, bucket_idx);
                    }
                    return { begin() + static_cast<difference_type>(value_idx), true };
                }

                template<class... Args> auto emplace_hint(const_iterator /*hint*/, Args&&... args) -> iterator
                {
                    return emplace(std::forward<Args>(args)...).first;
                }

                template<class... Args, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto try_emplace(Key const& key, Args&&... args) -> std::pair<iterator, bool>
                {
                    return do_try_emplace(key, std::forward<Args>(args)...);
                }

                template<class... Args, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto try_emplace(Key&& key, Args&&... args) -> std::pair<iterator, bool>
                {
                    return do_try_emplace(std::move(key), std::forward<Args>(args)...);
                }

                template<class... Args, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto try_emplace(const_iterator /*hint*/, Key const& key, Args&&... args) -> iterator
                {
                    return do_try_emplace(key, std::forward<Args>(args)...).first;
                }

                template<class... Args, typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto try_emplace(const_iterator /*hint*/, Key&& key, Args&&... args) -> iterator
                {
                    return do_try_emplace(std::move(key), std::forward<Args>(args)...).first;
                }

                template<
                    typename K, typename... Args, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<
                        is_map_v<Q> && is_transparent_v<H, KE> && is_neither_convertible_v<K&&, iterator, const_iterator>, bool>
                    = true>
                auto try_emplace(K&& key, Args&&... args) -> std::pair<iterator, bool>
                {
                    return do_try_emplace(std::forward<K>(key), std::forward<Args>(args)...);
                }

                template<
                    typename K, typename... Args, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<
                        is_map_v<Q> && is_transparent_v<H, KE> && is_neither_convertible_v<K&&, iterator, const_iterator>, bool>
                    = true>
                auto try_emplace(const_iterator /*hint*/, K&& key, Args&&... args) -> iterator
                {
                    return do_try_emplace(std::forward<K>(key), std::forward<Args>(args)...).first;
                }

                auto erase(iterator it) -> iterator
                {
                    auto hash = mixed_hash(get_key(*it));
                    auto bucket_idx = bucket_idx_from_hash(hash);

                    auto const value_idx_to_remove = static_cast<value_idx_type>(it - cbegin());
                    while (at(m_buckets, bucket_idx).m_value_idx != value_idx_to_remove)
                    {
                        bucket_idx = next(bucket_idx);
                    }

                    do_erase(bucket_idx, [](value_type&& /*unused*/) {});
                    return begin() + static_cast<difference_type>(value_idx_to_remove);
                }

                auto extract(iterator it) -> value_type
                {
                    auto hash = mixed_hash(get_key(*it));
                    auto bucket_idx = bucket_idx_from_hash(hash);

                    auto const value_idx_to_remove = static_cast<value_idx_type>(it - cbegin());
                    while (at(m_buckets, bucket_idx).m_value_idx != value_idx_to_remove)
                    {
                        bucket_idx = next(bucket_idx);
                    }

                    auto tmp = std::optional<value_type>{};
                    do_erase(bucket_idx, [&tmp](value_type&& val) { tmp = std::move(val); });
                    return std::move(tmp).value();
                }

                template<typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true> auto erase(const_iterator it) -> iterator
                {
                    return erase(begin() + (it - cbegin()));
                }

                template<typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto extract(const_iterator it) -> value_type
                {
                    return extract(begin() + (it - cbegin()));
                }

                auto erase(const_iterator first, const_iterator last) -> iterator
                {
                    auto const idx_first = first - cbegin();
                    auto const idx_last = last - cbegin();
                    auto const first_to_last = std::distance(first, last);
                    auto const last_to_end = std::distance(last, cend());

                    // remove elements from left to right which moves elements from the end back
                    auto const mid = idx_first + (std::min)(first_to_last, last_to_end);
                    auto idx = idx_first;
                    while (idx != mid)
                    {
                        erase(begin() + idx);
                        ++idx;
                    }

                    // all elements from the right are moved, now remove the last element until all done
                    idx = idx_last;
                    while (idx != mid)
                    {
                        --idx;
                        erase(begin() + idx);
                    }

                    return begin() + idx_first;
                }

                auto erase(Key const& key) -> size_t
                {
                    return do_erase_key(key, [](value_type&& /*unused*/) {});
                }

                auto extract(Key const& key) -> std::optional<value_type>
                {
                    auto tmp = std::optional<value_type>{};
                    do_erase_key(key, [&tmp](value_type&& val) { tmp = std::move(val); });
                    return tmp;
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto erase(K&& key) -> size_t
                {
                    return do_erase_key(std::forward<K>(key), [](value_type&& /*unused*/) {});
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto extract(K&& key) -> std::optional<value_type>
                {
                    auto tmp = std::optional<value_type>{};
                    do_erase_key(std::forward<K>(key), [&tmp](value_type&& val) { tmp = std::move(val); });
                    return tmp;
                }

                void swap(table& other) noexcept(noexcept(
                    std::is_nothrow_swappable_v<value_container_type> && std::is_nothrow_swappable_v<Hash>
                    && std::is_nothrow_swappable_v<KeyEqual>))
                {
                    using std::swap;
                    swap(other, *this);
                }

                // lookup /////////////////////////////////////////////////////////////////

                template<typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true> auto at(key_type const& key) -> Q&
                {
                    return do_at(key);
                }

                template<
                    typename K, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<is_map_v<Q> && is_transparent_v<H, KE>, bool> = true>
                auto at(K const& key) -> Q&
                {
                    return do_at(key);
                }

                template<typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true>
                auto at(key_type const& key) const -> Q const&
                {
                    return do_at(key);
                }

                template<
                    typename K, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<is_map_v<Q> && is_transparent_v<H, KE>, bool> = true>
                auto at(K const& key) const -> Q const&
                {
                    return do_at(key);
                }

                template<typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true> auto operator[](Key const& key) -> Q&
                {
                    return try_emplace(key).first->second;
                }

                template<typename Q = T, std::enable_if_t<is_map_v<Q>, bool> = true> auto operator[](Key&& key) -> Q&
                {
                    return try_emplace(std::move(key)).first->second;
                }

                template<
                    typename K, typename Q = T, typename H = Hash, typename KE = KeyEqual,
                    std::enable_if_t<is_map_v<Q> && is_transparent_v<H, KE>, bool> = true>
                auto operator[](K&& key) -> Q&
                {
                    return try_emplace(std::forward<K>(key)).first->second;
                }

                auto count(Key const& key) const -> size_t
                {
                    return find(key) == end() ? 0 : 1;
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto count(K const& key) const -> size_t
                {
                    return find(key) == end() ? 0 : 1;
                }

                auto find(Key const& key) -> iterator
                {
                    return do_find(key);
                }

                auto find(Key const& key) const -> const_iterator
                {
                    return do_find(key);
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto find(K const& key) -> iterator
                {
                    return do_find(key);
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto find(K const& key) const -> const_iterator
                {
                    return do_find(key);
                }

                auto contains(Key const& key) const -> bool
                {
                    return find(key) != end();
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto contains(K const& key) const -> bool
                {
                    return find(key) != end();
                }

                auto equal_range(Key const& key) -> std::pair<iterator, iterator>
                {
                    auto it = do_find(key);
                    return { it, it == end() ? end() : it + 1 };
                }

                auto equal_range(const Key& key) const -> std::pair<const_iterator, const_iterator>
                {
                    auto it = do_find(key);
                    return { it, it == end() ? end() : it + 1 };
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto equal_range(K const& key) -> std::pair<iterator, iterator>
                {
                    auto it = do_find(key);
                    return { it, it == end() ? end() : it + 1 };
                }

                template<class K, class H = Hash, class KE = KeyEqual, std::enable_if_t<is_transparent_v<H, KE>, bool> = true>
                auto equal_range(K const& key) const -> std::pair<const_iterator, const_iterator>
                {
                    auto it = do_find(key);
                    return { it, it == end() ? end() : it + 1 };
                }

                // bucket interface ///////////////////////////////////////////////////////

                auto bucket_count() const noexcept -> size_t
                { // NOLINT(modernize-use-nodiscard)
                    return m_buckets.size();
                }

                static constexpr auto max_bucket_count() noexcept -> size_t
                { // NOLINT(modernize-use-nodiscard)
                    return max_size();
                }

                // hash policy ////////////////////////////////////////////////////////////

                [[nodiscard]] auto load_factor() const -> float
                {
                    return bucket_count() ? static_cast<float>(size()) / static_cast<float>(bucket_count()) : 0.0F;
                }

                [[nodiscard]] auto max_load_factor() const -> float
                {
                    return m_max_load_factor;
                }

                void max_load_factor(float ml)
                {
                    m_max_load_factor = ml;
                    if (bucket_count() != max_bucket_count())
                    {
                        m_max_bucket_capacity = static_cast<value_idx_type>(
                            static_cast<float>(bucket_count()) * max_load_factor());
                    }
                }

                void rehash(size_t count)
                {
                    count = (std::min)(count, max_size());
                    auto shifts = calc_shifts_for_size((std::max)(count, size()));
                    if (shifts != m_shifts)
                    {
                        m_shifts = shifts;
                        deallocate_buckets();
                        m_values.shrink_to_fit();
                        allocate_buckets_from_shift();
                        clear_and_fill_buckets_from_values();
                    }
                }

                void reserve(size_t capa)
                {
                    capa = (std::min)(capa, max_size());
                    if constexpr (has_reserve<value_container_type>)
                    {
                        // std::deque doesn't have reserve(). Make sure we only call when available
                        m_values.reserve(capa);
                    }
                    auto shifts = calc_shifts_for_size((std::max)(capa, size()));
                    if (0 == bucket_count() || shifts < m_shifts)
                    {
                        m_shifts = shifts;
                        deallocate_buckets();
                        allocate_buckets_from_shift();
                        clear_and_fill_buckets_from_values();
                    }
                }

                // observers //////////////////////////////////////////////////////////////

                auto hash_function() const -> hasher
                {
                    return m_hash;
                }

                auto key_eq() const -> key_equal
                {
                    return m_equal;
                }

                // nonstandard API: expose the underlying values container
                [[nodiscard]] auto values() const noexcept -> value_container_type const&
                {
                    return m_values;
                }

                // non-member functions ///////////////////////////////////////////////////

                friend auto operator==(table const& a, table const& b) -> bool
                {
                    if (&a == &b)
                    {
                        return true;
                    }
                    if (a.size() != b.size())
                    {
                        return false;
                    }
                    for (auto const& b_entry : b)
                    {
                        auto it = a.find(get_key(b_entry));
                        if constexpr (is_map_v<T>)
                        {
                            // map: check that key is here, then also check that value is the same
                            if (a.end() == it || !(b_entry.second == it->second))
                            {
                                return false;
                            }
                        }
                        else
                        {
                            // set: only check that the key is here
                            if (a.end() == it)
                            {
                                return false;
                            }
                        }
                    }
                    return true;
                }

                friend auto operator!=(table const& a, table const& b) -> bool
                {
                    return !(a == b);
                }
            };

        } // namespace detail

        ANKERL_UNORDERED_DENSE_EXPORT template<
            class Key, class T, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>,
            class AllocatorOrContainer = std::allocator<std::pair<Key, T>>, class Bucket = bucket_type::standard,
            class BucketContainer = detail::default_container_t>
        using map = detail::table<Key, T, Hash, KeyEqual, AllocatorOrContainer, Bucket, BucketContainer, false>;

        ANKERL_UNORDERED_DENSE_EXPORT template<
            class Key, class T, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>,
            class AllocatorOrContainer = std::allocator<std::pair<Key, T>>, class Bucket = bucket_type::standard,
            class BucketContainer = detail::default_container_t>
        using segmented_map = detail::table<Key, T, Hash, KeyEqual, AllocatorOrContainer, Bucket, BucketContainer, true>;

        ANKERL_UNORDERED_DENSE_EXPORT template<
            class Key, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>,
            class AllocatorOrContainer = std::allocator<Key>, class Bucket = bucket_type::standard,
            class BucketContainer = detail::default_container_t>
        using set = detail::table<Key, void, Hash, KeyEqual, AllocatorOrContainer, Bucket, BucketContainer, false>;

        ANKERL_UNORDERED_DENSE_EXPORT template<
            class Key, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>,
            class AllocatorOrContainer = std::allocator<Key>, class Bucket = bucket_type::standard,
            class BucketContainer = detail::default_container_t>
        using segmented_set = detail::table<Key, void, Hash, KeyEqual, AllocatorOrContainer, Bucket, BucketContainer, true>;

#    if defined(ANKERL_UNORDERED_DENSE_PMR)

        namespace pmr
        {

            ANKERL_UNORDERED_DENSE_EXPORT template<
                class Key, class T, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>,
                class Bucket = bucket_type::standard>
            using map = detail::table<
                Key, T, Hash, KeyEqual, ANKERL_UNORDERED_DENSE_PMR::polymorphic_allocator<std::pair<Key, T>>, Bucket,
                detail::default_container_t, false>;

            ANKERL_UNORDERED_DENSE_EXPORT template<
                class Key, class T, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>,
                class Bucket = bucket_type::standard>
            using segmented_map = detail::table<
                Key, T, Hash, KeyEqual, ANKERL_UNORDERED_DENSE_PMR::polymorphic_allocator<std::pair<Key, T>>, Bucket,
                detail::default_container_t, true>;

            ANKERL_UNORDERED_DENSE_EXPORT template<
                class Key, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>, class Bucket = bucket_type::standard>
            using set = detail::table<
                Key, void, Hash, KeyEqual, ANKERL_UNORDERED_DENSE_PMR::polymorphic_allocator<Key>, Bucket,
                detail::default_container_t, false>;

            ANKERL_UNORDERED_DENSE_EXPORT template<
                class Key, class Hash = hash<Key>, class KeyEqual = std::equal_to<Key>, class Bucket = bucket_type::standard>
            using segmented_set = detail::table<
                Key, void, Hash, KeyEqual, ANKERL_UNORDERED_DENSE_PMR::polymorphic_allocator<Key>, Bucket,
                detail::default_container_t, true>;

        } // namespace pmr

#    endif

        // deduction guides ///////////////////////////////////////////////////////////

        // deduction guides for alias templates are only possible since C++20
        // see https://en.cppreference.com/w/cpp/language/class_template_argument_deduction

    } // namespace ANKERL_UNORDERED_DENSE_NAMESPACE
} // namespace ankerl::unordered_dense

// std extensions /////////////////////////////////////////////////////////////

namespace std
{ // NOLINT(cert-dcl58-cpp)

    ANKERL_UNORDERED_DENSE_EXPORT template<
        class Key, class T, class Hash, class KeyEqual, class AllocatorOrContainer, class Bucket, class Pred,
        class BucketContainer, bool IsSegmented>
    // NOLINTNEXTLINE(cert-dcl58-cpp)
    auto erase_if(
        ankerl::unordered_dense::detail::table<
            Key, T, Hash, KeyEqual, AllocatorOrContainer, Bucket, BucketContainer, IsSegmented>& map,
        Pred pred) -> size_t
    {
        using map_t = ankerl::unordered_dense::detail::table<
            Key, T, Hash, KeyEqual, AllocatorOrContainer, Bucket, BucketContainer, IsSegmented>;

        // going back to front because erase() invalidates the end iterator
        auto const old_size = map.size();
        auto idx = old_size;
        while (idx)
        {
            --idx;
            auto it = map.begin() + static_cast<typename map_t::difference_type>(idx);
            if (pred(*it))
            {
                map.erase(it);
            }
        }

        return old_size - map.size();
    }

} // namespace std

#endif
#endif
```

`thirdparty/CMakeLists.txt`:

```txt
# This file is automatically generated from cmake.toml - DO NOT EDIT
# See https://github.com/build-cpp/cmkr for more information

# Create a configure-time dependency on cmake.toml to improve IDE support
if(CMKR_ROOT_PROJECT)
	set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS cmake.toml)
endif()

# Options
option(INSTALL_GTEST "" OFF)
option(BUILD_GMOCK "" OFF)
option(BENCHMARK_ENABLE_TESTING "" OFF)
option(BENCHMARK_ENABLE_INSTALL "" OFF)
option(BENCHMARK_USE_BUNDLED_GTEST "" OFF)

include(FetchContent)

# Fix warnings about DOWNLOAD_EXTRACT_TIMESTAMP
if(POLICY CMP0135)
	cmake_policy(SET CMP0135 NEW)
endif()
message(STATUS "Fetching Zydis (v4.1.0)...")
FetchContent_Declare(Zydis SYSTEM
	GIT_REPOSITORY
		"https://github.com/zyantific/zydis"
	GIT_TAG
		v4.1.0
)
FetchContent_MakeAvailable(Zydis)

message(STATUS "Fetching GTest (release-1.11.0)...")
FetchContent_Declare(GTest SYSTEM
	GIT_REPOSITORY
		"https://github.com/google/googletest"
	GIT_TAG
		release-1.11.0
)
FetchContent_MakeAvailable(GTest)

message(STATUS "Fetching GBenchmark (v1.6.1)...")
FetchContent_Declare(GBenchmark SYSTEM
	GIT_REPOSITORY
		"https://github.com/google/benchmark"
	GIT_TAG
		v1.6.1
)
FetchContent_MakeAvailable(GBenchmark)

message(STATUS "Fetching sfl (1.9.0)...")
FetchContent_Declare(sfl
	GIT_REPOSITORY
		"https://github.com/slavenf/sfl-library.git"
	GIT_TAG
		1.9.0
)
FetchContent_MakeAvailable(sfl)

```

`thirdparty/cmake.toml`:

```toml
# Reference: https://build-cpp.github.io/cmkr/cmake-toml

[options]
INSTALL_GTEST = false
BUILD_GMOCK = false
BENCHMARK_ENABLE_TESTING = false
BENCHMARK_ENABLE_INSTALL = false
BENCHMARK_USE_BUNDLED_GTEST = false

[fetch-content.Zydis]
git = "https://github.com/zyantific/zydis"
tag = "v4.1.0"
system = true

[fetch-content.GTest]
git = "https://github.com/google/googletest"
tag = "release-1.11.0"
system = true

[fetch-content.GBenchmark]
git = "https://github.com/google/benchmark"
tag = "v1.6.1"
system = true

[fetch-content.sfl]
git = "https://github.com/slavenf/sfl-library.git"
tag = "1.9.0"
```