Project Path: arc_gmh5225_binaryninja-openai_2a2nic__

Source Tree:

```txt
arc_gmh5225_binaryninja-openai_2a2nic__
├── LICENSE
├── README.md
├── __init__.py
├── plugin.json
├── requirements.txt
├── resources
│   ├── output.png
│   ├── rename-after.png
│   ├── rename-before.png
│   └── settings.png
└── src
    ├── agent.py
    ├── c.py
    ├── entry.py
    ├── exceptions.py
    ├── query.py
    └── settings.py

```

`LICENSE`:

```
MIT License

Copyright (c) 2022 Sean Deaton (@WhatTheFuzz)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`README.md`:

```md
[![CodeQL](https://github.com/WhatTheFuzz/binaryninja-openai/actions/workflows/codeql.yml/badge.svg)](https://github.com/WhatTheFuzz/binaryninja-openai/actions/workflows/codeql.yml)

# BinaryNinja-OpenAI

Integrates OpenAI's GPT3 with Binary Ninja via a plugin and currently supports
two actions:

- Queries OpenAI to determine what a given function does (in Pseudo-C and HLIL).
  - The results are logged to Binary Ninja's log to assist with RE.
- Allows users to rename variables in HLIL using OpenAI.
  - Variable are renamed immediately and the decompiler is reloaded.

## Installation

If you're installing this as a standalone plugin, you can place (or sym-link)
this in Binary Ninja's plugin path. Default paths are detailed on
[Vector 35's documentation][default-plugin-dir].

This plugin has been tested on macOS and Linux. It probably works on Windows;
please submit a pull request if you've tested it.

### Dependencies

- Python 3.10+
- `openai` installed with `pip3 install --user openai`

## API Key

This requires an [API token from OpenAI][token]. The plugin checks for the API
key in three ways (in this order).

First, it tries to read the key from Binary Ninja's preferences. You can
access the entry in Binary Ninja via `Edit > Preferences > Settings > OpenAI`.
Or, use the hotkey ⌘+, and search for `OpenAI`. You should see customizable
settings like so.

![Settings](https://github.com/WhatTheFuzz/binaryninja-openai/blob/main/resources/settings.png?raw=true)

Second, it checks the environment variable `OPENAI_API_KEY`, which you can set
inside of Binary Ninja's Python console like so:

```python
import os
os.environ["OPENAI_API_KEY"] = "INSERT KEY HERE"
```

Or you can write it to a file. The file is set in [entry.py][entry] and is a
parameter to the Agent class. By default it checks for the file
`~/.openai/api_key.txt`. You can add your API token like so:

```shell
mkdir ~/.openai
echo -n "INSERT KEY HERE" > ~/.openai/api_key.txt
```

Note that if you have all three set, the plugin defaults to one set in Binary
Ninja. If your API token is invalid, you'll receive the following error:

```python
openai.error.AuthenticationError: Incorrect API key provided: <BAD KEY HERE>.
You can find your API key at https://beta.openai.com.
```

## Usage

### What Does this Function Do?

After installation, you can right-click on any function in Binary Ninja and
select `Plugins > OpenAI > What Does this Function Do (HLIL/Pseudo-C)?`.
Alternatively, select a function in Binary Ninja (by clicking on any instruction
in the function) and use the menu bar options `Plugins > OpenAI > ...`. If your
cursor has anything else selected other than an instruction inside a function,
`OpenAI` will not appear as a selection inside the `Plugins` menu. This can
happen if you've selected data or instructions that Binary Ninja determined did
not belong inside of the function. Additionally, the HLIL options are context
sensitive; if you're looking at the decompiled results in LLIL, you will not see
the HLIL options; this is easily fixed by changing the user view to HLIL
(Pseudo-C should always be visible).

The output will appear in Binary Ninja's Log like so:

![The output of running the plugin.](https://github.com/WhatTheFuzz/binaryninja-openai/blob/main/resources/output.png?raw=true)

### Renaming Variables

I feel like half of reverse engineering is figuring out variable names (which
in-turn assist with program understanding). This plugin is an experimental look
to see if OpenAI can assist with that. Right click on an instruction where a
variable is initialized and select `OpenAI > Rename Variable (HLIL)`. Watch the
magic happen. Here's a quick before-and-after.

![Before renaming](https://github.com/WhatTheFuzz/binaryninja-openai/blob/main/resources/rename-before.png?raw=true)

![After renaming](https://github.com/WhatTheFuzz/binaryninja-openai/blob/main/resources/rename-after.png?raw=true)

Renaming variables only works on HLIL instructions that are initializations (ie.
`HighLevelILVarInit`). You might also want this to support assignments
(`HighLevelILAssign`), but I did not get great results with this. Most of the
responses were just `result`. If your experience is different, please submit a
pull request.

## OpenAI Model

By default, the plugin uses the `text-davinci-003` model, you can tweak this
inside Binary Ninja's preferences. You can access these settings as described in
the [API Key](#api-key) section. It uses the maximum available number of tokens
for each model, as described in [OpenAI's documentation][tokens].

## Known Issues

Please submit an issue if you find something that isn't working properly.

## License

This project is licensed under the [MIT license][license].

[default-plugin-dir]:https://docs.binary.ninja/guide/plugins.html
[token]:https://beta.openai.com/account/api-keys
[tokens]:https://beta.openai.com/docs/models/gpt-3
[entry]:./src/entry.py
[license]:./LICENSE

```

`__init__.py`:

```py
from binaryninja import PluginCommand
from . src.settings import OpenAISettings
from . src.entry import check_function, rename_variable

# Register the settings group in Binary Ninja to store the API key and model.
OpenAISettings()

PluginCommand.register_for_high_level_il_function(r"OpenAI\What Does this Function Do (HLIL)?",
                            "Checks OpenAI to see what this HLIL function does." \
                            "Requires an internet connection and an API key "
                            "saved under the environment variable "
                            "OPENAI_API_KEY or modify the path in entry.py.",
                            check_function)

PluginCommand.register_for_function(r"OpenAI\What Does this Function Do (Pseudo-C)?",
                            "Checks OpenAI to see what this pseudo-C function does." \
                            "Requires an internet connection and an API key "
                            "saved under the environment variable "
                            "OPENAI_API_KEY or modify the path in entry.py.",
                            check_function)

PluginCommand.register_for_high_level_il_instruction(r"OpenAI\Rename Variable (HLIL)",
                            "If the current expression is a HLIL Initialization " \
                            "(HighLevelILVarInit), then query OpenAI to rename the " \
                            "variable to what it believes is correct. If the expression" \
                            "is not an HighLevelILVarInit, then do nothing. Requires " \
                            "an internet connection and an API key. ",
                            rename_variable)

```

`plugin.json`:

```json
{
   "pluginmetadataversion": 2,
   "name": "OpenAI GPT3",
   "author": "Sean Deaton (@WhatTheFuzz)",
   "type": [
      "helper"
   ],
   "api": [
      "python3"
   ],
   "description": "Queries OpenAI's GPT3 to determine what a given function does.",
   "longdescription": "",
   "license": {
      "name": "MIT",
      "text": "Copyright 2022 Sean Deaton (@WhatTheFuzz)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   },
   "platforms": [
      "Darwin",
      "Linux"
   ],
   "installinstructions": {
      "Darwin": "`pip3 install --user openai`.\n Add your OpenAI API key to the OpenAI preferences in Binary Ninja.",
      "Linux": "`pip3 install --user openai`.\n Add your OpenAI API key to the OpenAI preferences in Binary Ninja."
   },
   "dependencies": {
      "pip": [
         "openai"
      ]
	},
   "version": "2.0.1",
   "minimumbinaryninjaversion": 3200
}
```

`requirements.txt`:

```txt
openai
```

`src/agent.py`:

```py
from __future__ import annotations
from collections.abc import Callable
import os
from typing import Optional, Union
from pathlib import Path

import openai
from openai.api_resources.model import Model
from openai.error import APIError

from binaryninja.function import Function
from binaryninja.lowlevelil import LowLevelILFunction
from binaryninja.mediumlevelil import MediumLevelILFunction
from binaryninja.highlevelil import HighLevelILFunction, HighLevelILInstruction, \
                                    HighLevelILVarInit
from binaryninja.settings import Settings
from binaryninja import log, BinaryView

from . query import Query
from . c import Pseudo_C


class Agent:

    function_question: str = '''
    This is a function that was decompiled with Binary Ninja.
    It is in IL_FORM. What does this function do?
    '''

    rename_variable_question: str = "In one word, what should the variable " \
        "be for the variable that is assigned to the result of the C " \
        "expression:\n"


    # A mapping of IL forms to their names.
    il_name: dict[type, str] = {
        LowLevelILFunction: 'Low Level Intermediate Language',
        MediumLevelILFunction: 'Medium Level Intermediate Language',
        HighLevelILFunction: 'High Level Intermediate Language',
        Function: 'decompiled C code'
    }

    def __init__(self,
                bv: BinaryView,
                path_to_api_key: Optional[Path]=None) -> None:

        # Read the API key from the environment variable.
        openai.api_key = self.read_api_key(path_to_api_key)

        assert bv is not None, 'BinaryView is None. Check how you called this function.'
        # Set instance attributes.
        self.bv = bv
        self.model = self.get_model()
        # Used for the callback function.
        self.instruction = None

    def read_api_key(self, filename: Optional[Path]=None) -> str:
        '''Checks for the API key in three locations.

        First, it checks the openai.api_key key:value in Binary Ninja
        preferences. This is accessed in Binary Ninja by going to Edit >
        Preferences > Settings > OpenAI.
        Second, it checks the OPENAI_API_KEY environment variable.
        Finally, it checks the file specified by the filename argument.
        Defaults to ~/.openai/api_key.txt.
        '''

        # First, check the Binary Ninja settings.
        settings: Settings = Settings()
        if settings.contains('openai.api_key'):
            if key := settings.get_string('openai.api_key'):
                return str(key)

        # If the settings don't exist, contain the key, or the key is empty,
        # check the environment variable.
        if key := os.getenv('OPENAI_API_KEY'):
            return key

        # Finally, if the environment variable doesn't exist, check the default
        # file.
        if filename:
            log.log_info(f'No API key detected under the environment variable '
                         f'OPENAI_API_KEY. Reading API key from {filename}')
            try:
                with open(filename, mode='r', encoding='ascii') as api_key_file:
                    return api_key_file.read()
            except FileNotFoundError:
                log.log_error(f'Could not find API key file at {filename}.')

        raise APIError('No API key found. Refer to the documentation to add the '
                       'API key.')

    def is_valid_model(self, model: str) -> bool:
        '''Checks if the model is valid by querying the OpenAI API.'''
        models: list[Model] = openai.Model.list().data
        return model in [m.id for m in models]

    def get_model(self) -> str:
        '''Returns the model that the user has selected from Binary Ninja's
        preferences. The default value is set by the OpenAISettings class. If
        for some reason the user selected a model that doesn't exist, this
        function defaults to 'text-davinci-003'.
        '''
        settings: Settings = Settings()
        # Check that the key exists.
        if settings.contains('openai.model'):
            # Check that the key is not empty and get the user's selection.
            if model := settings.get_string('openai.model'):
                # Check that is a valid model by querying the OpenAI API.
                if self.is_valid_model(model):
                    return str(model)
        # Return a valid, default model.
        assert self.is_valid_model('text-davinci-003')
        return 'text-davinci-003'

    def get_token_count(self) -> int:
        '''Returns the maximum token count specified by the user. If no value is
        set, for whatever reason, returns 1,024.'''
        settings: Settings = Settings()
        # Check that the key exists.
        if settings.contains('openai.max_tokens'):
            # Check that the value is not None.
            if (max_tokens := settings.get_integer('openai.max_tokens')) is not None:
                return int(max_tokens)
        return 1_024

    def instruction_list(self, function: Union[LowLevelILFunction,
                                         MediumLevelILFunction,
                                         HighLevelILFunction]) -> list[str]:
        '''Generates a list of instructions in string representation given a
        BNIL function.
        '''

        # Ensure that a function type was passed in.
        if not isinstance(function, (Function, LowLevelILFunction,
                            MediumLevelILFunction, HighLevelILFunction)):
            raise TypeError(f'Expected a BNIL function of type '
                            f'Function, LowLevelILFunction, '
                            f'MediumLevelILFunction, or HighLevelILFunction, '
                            f'got {type(function)}.')

        if isinstance(function, Function):
            return Pseudo_C(self.bv, function).get_c_source()
        instructions: list[str] = []
        for instruction in function.instructions:
            instructions.append(str(instruction))
        return instructions

    def generate_query(self, function: Union[Function,
                                            LowLevelILFunction,
                                            MediumLevelILFunction,
                                            HighLevelILFunction]) -> str:
        '''Generates a query string given a BNIL function. Returns the query as
        a string.
        '''
        prompt: str = self.function_question
        # Read the prompt from the text file.
        prompt = prompt.replace('IL_FORM', self.il_name[type(function)])
        # Add some new lines. Maybe not necessary.
        prompt += '\n\n'
        # Add the instructions to the prompt.
        prompt += '\n'.join(self.instruction_list(function))
        return prompt

    def generate_rename_variable_query(self,
                                    instruction: HighLevelILInstruction) -> str:
        '''Generates a query string given a BNIL instruction. Returns the query
        as a string.
        '''
        if not isinstance(instruction, HighLevelILVarInit):
            raise TypeError(f'Expected a BNIL instruction of type '
                            f'HighLevelILVarInit got {type(instruction)}.')
        # Assign the instruction to the Agent instance. This is used for the
        # callback function so we don't need to pass in the instruction to the
        # Query instance. This is kind of janky and should be examined in future
        # versions.
        self.instruction = instruction

        prompt: str = self.rename_variable_question
        # Get the disassembly lines and add them to the prompt.
        for line in instruction.instruction_operands:
            prompt += str(line)

        return prompt

    def rename_variable(self, response: str) -> None:
        '''Renames the variable of the instruction saved in the Agent instance
        to the response passed in as an argument.
        '''
        if self.instruction is None:
            raise TypeError('No instruction was saved in the Agent instance.')
        if response is None or response == '':
            raise TypeError(f'No response was returned from OpenAI; got type {type(response)}.')
        # Get just one word from the response. Remove spaces and quotes.
        try:
            response = response.split()[0]
            response = response.replace(' ', '')
            response = response.replace('"', '')
            response = response.replace('\'', '')
        except IndexError as error:
            raise IndexError(f'Could not split the response: `{response}`.') from error
        # Assign the variable name to the response.
        log.log_debug(f'Renaming variable in expression {self.instruction} to {response}.')
        self.instruction.dest.name = response


    def send_query(self, query: str, callback: Optional[Callable]=None) -> None:
        '''Sends a query to the engine and prints the response.'''
        query = Query(query_string=query,
                      model=self.model,
                      max_token_count=self.get_token_count(),
                      callback_function=callback)
        query.start()

```

`src/c.py`:

```py
from binaryninja.function import Function, DisassemblySettings
from binaryninja.enums import DisassemblyOption
from binaryninja.lineardisassembly import LinearViewObject, LinearViewCursor, \
                                          LinearDisassemblyLine
from binaryninja import BinaryView


class Pseudo_C:

    def __init__(self, bv: BinaryView, func: Function) -> None:
        self.bv = bv
        self.func = func

    def get_c_source(self) -> list[str]:
        '''Returns a list of strings representing the C source code for the
        function.'''
        lines: list[str] = []
        settings: DisassemblySettings = DisassemblySettings()
        settings.set_option(DisassemblyOption.ShowAddress, False)

        linear_view: LinearViewObject = LinearViewObject.language_representation(
            self.bv, settings)
        cursor_end: LinearViewCursor = LinearViewCursor(linear_view)
        cursor_end.seek_to_address(self.func.highest_address)

        body: list[
            LinearDisassemblyLine] = self.bv.get_next_linear_disassembly_lines(
                cursor_end)
        cursor_end.seek_to_address(self.func.highest_address)

        header: list[
            LinearDisassemblyLine] = self.bv.get_previous_linear_disassembly_lines(
                cursor_end)

        for line in header:
            lines.append(str(line))
        for line in body:
            lines.append(str(line))
        return lines

```

`src/entry.py`:

```py
from pathlib import Path
from binaryninja import BinaryView, Function
from binaryninja.highlevelil import HighLevelILInstruction, HighLevelILVarInit
from binaryninja.log import log_error
from . agent import Agent

API_KEY_PATH = Path.home() / Path('.openai/api_key.txt')

def check_function(bv: BinaryView, func: Function) -> None:
    agent: Agent = Agent(
        bv=bv,
        path_to_api_key=API_KEY_PATH
    )
    query: str = agent.generate_query(func)
    agent.send_query(query)

def rename_variable(bv: BinaryView, instruction: HighLevelILInstruction) -> None:

    if not isinstance(instruction, HighLevelILVarInit):
        log_error(f'Instruction must be of type HighLevelILVarInit, got type: ' \
                  f'{type(instruction)}')
        return

    agent: Agent = Agent(
        bv=bv,
        path_to_api_key=API_KEY_PATH
    )
    query: str = agent.generate_rename_variable_query(instruction)
    agent.send_query(query=query, callback=agent.rename_variable)

# Difficult to test without a payment method added, given that the rate limits
# are so low. This should also probably take place in a background task of its
# own.
# def rename_all_variables_in_function(bv: BinaryView, func: HighLevelILFunction) -> None:
#     # Get each instruction in the High Level IL Function.
#     for instruction in func.instructions:
#         match instruction:
#             # Rename the variable if it is a HighLevelILVarInit.
#             case HighLevelILVarInit():
#                 rename_variable(bv, instruction)
#             # Explicit pass for all other cases.
#             case _ :
#                 pass


```

`src/exceptions.py`:

```py
class RegisterSettingsGroupException(Exception):
    pass

class RegisterSettingsKeyException(Exception):
    pass

```

`src/query.py`:

```py
from __future__ import annotations
from collections.abc import Callable
from typing import Optional
import openai
from binaryninja.plugin import BackgroundTaskThread
from binaryninja.log import log_debug, log_info

class Query(BackgroundTaskThread):

    def __init__(self, query_string: str, model: str,
                 max_token_count: int, callback_function: Optional[Callable]=None) -> None:
        BackgroundTaskThread.__init__(self,
                                      initial_progress_text="",
                                      can_cancel=False)
        self.query_string: str = query_string
        self.model: str = model
        self.max_token_count: int = max_token_count
        self.callback = callback_function

    def run(self) -> None:
        self.progress = "Submitting query to OpenAI."

        log_debug(f'Sending query: {self.query_string}')

        response = openai.Completion.create(
            model=self.model,
            prompt=self.query_string,
            max_tokens=self.max_token_count,
        )
        # Get the response text.
        result: str = response.choices[0].text
        # If there is a callback, do something with it.
        if self.callback:
            self.callback(result)
        # Otherwise, assume we just want to log it.
        else:
            log_info(result)
```

`src/settings.py`:

```py
import json
from binaryninja.settings import Settings
from . exceptions import RegisterSettingsGroupException, \
                         RegisterSettingsKeyException

class OpenAISettings(Settings):

    def __init__(self) -> None:
        # Initialize the settings with the default instance ID.
        super().__init__(instance_id='default')
        # Register the OpenAI group.
        if not self.register_group('openai', 'OpenAI'):
            raise RegisterSettingsGroupException('Failed to register OpenAI '
                                                 'settings group.')
        # Register the setting for the API key.
        if not self.register_api_key_settings():
            raise RegisterSettingsKeyException('Failed to register OpenAI API '
                                               'key settings.')

        # Register the setting for the model used to query.
        if not self.register_model_settings():
            raise RegisterSettingsKeyException('Failed to register OpenAI '
                                               'model settings.')

        # Register the setting for the max tokens used for both the prompt and
        # completion.
        if not self.register_max_tokens():
            raise RegisterSettingsKeyException('Failed to register OpenAI '
                                               'max tokens settings.')

    def register_api_key_settings(self) -> bool:
        '''Register the OpenAI API key settings in Binary Ninja.'''
        # Set the attributes of the settings. Refer to:
        # https://api.binary.ninja/binaryninja.settings-module.html
        properties = {
            'title': 'OpenAI API Key',
            'type': 'string',
            'description': 'The user\'s OpenAI API key used to make requests '
            'the server.'
        }
        return self.register_setting('openai.api_key', json.dumps(properties))

    def register_model_settings(self) -> bool:
        '''Register the OpenAI model settings in Binary Ninja.
        Defaults to text-davinci-003.
        '''
        # Set the attributes of the settings. Refer to:
        # https://api.binary.ninja/binaryninja.settings-module.html
        properties = {
            'title': 'OpenAI Model',
            'type': 'string',
            'description': 'The OpenAI model used to generate the response.',
            # https://beta.openai.com/docs/models
            'enum': [
                'text-davinci-003',
                'text-curie-001',
                'text-babbage-001',
                'text-babbage-002',
                'code-davinci-002',
                'code-cushman-001'
            ],
            'enumDescriptions': [
                'Most capable GPT-3 model. Can do any task the other models can do, often with higher quality, longer output and better instruction-following. Also supports inserting completions within text.',
                'Very capable, but faster and lower cost than Davinci.',
                'Capable of straightforward tasks, very fast, and lower cost.',
                'Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.',
                'Most capable Codex model. Particularly good at translating natural language to code. In addition to completing code, also supports inserting completions within code.',
                'Almost as capable as Davinci Codex, but slightly faster. This speed advantage may make it preferable for real-time applications.'
            ],
            'default': 'text-davinci-003'
        }
        return self.register_setting('openai.model', json.dumps(properties))

    def register_max_tokens(self) -> bool:
        '''Register the OpenAI max tokens used for both the prompt and
        completion. Defaults to 2,048. The Davinci model can use 4,000 or 8,000
        tokens for GPT and Codex respectively. Check out the documentation here:
        https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
        '''

        properties = {
            'title': 'OpenAI Max Completion Tokens',
            'type': 'number',
            'description': 'The maximum number of tokens used for completion. Tokens do not necessarily align with word or instruction count. Typically, each token is four characters. If your function is very large, you may need to decrease this value, as the number of tokens in your prompt counts against the total number of tokens supported by the model. Not all models support the same number of maximum tokens; most support 2,048 tokens. For larger functions, check out text-davinci-003 and code-davinci-002 which support 4,000 and 8,000 respectively.',
            'default': 1_024,
            'minValue': 1,
            'maxValue': 8_000,
            'message': "Min: 1, Max: 8,000"
        }
        return self.register_setting('openai.max_tokens',
                                     json.dumps(properties))

```