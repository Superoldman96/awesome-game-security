Project Path: arc_gmh5225_river_evwirtpq

Source Tree:

```txt
arc_gmh5225_river_evwirtpq
├── CHANGELOG.md
├── LICENSE
├── Makefile
├── client.go
├── client_test.go
├── cmd
│   └── river
│       ├── go.mod
│       ├── go.sum
│       ├── main.go
│       ├── riverbench
│       │   └── river_bench.go
│       └── rivercli
│           ├── command.go
│           ├── river_cli.go
│           └── river_cli_test.go
├── common_test.go
├── context.go
├── context_test.go
├── doc.go
├── docs
│   ├── README.md
│   ├── development.md
│   └── state_machine.md
├── driver_test.go
├── error.go
├── error_handler.go
├── error_handler_test.go
├── error_test.go
├── event.go
├── event_test.go
├── example_batch_insert_test.go
├── example_client_from_context_dbsql_test.go
├── example_client_from_context_test.go
├── example_complete_job_within_tx_test.go
├── example_cron_job_test.go
├── example_custom_insert_opts_test.go
├── example_error_handler_test.go
├── example_global_hooks_test.go
├── example_global_middleware_test.go
├── example_graceful_shutdown_test.go
├── example_insert_and_work_test.go
├── example_job_args_hooks_test.go
├── example_job_cancel_from_client_test.go
├── example_job_cancel_test.go
├── example_job_snooze_test.go
├── example_periodic_job_test.go
├── example_queue_pause_test.go
├── example_scheduled_job_test.go
├── example_subscription_test.go
├── example_unique_job_test.go
├── example_work_func_test.go
├── go.mod
├── go.sum
├── go.work
├── hook_defaults_funcs.go
├── hook_defaults_funcs_test.go
├── insert_opts.go
├── insert_opts_test.go
├── internal
│   ├── cmd
│   │   └── testdbman
│   │       ├── main.go
│   │       └── main_test.go
│   ├── dblist
│   │   ├── db_list.go
│   │   └── db_list_test.go
│   ├── dbunique
│   │   ├── db_unique.go
│   │   ├── db_unique_test.go
│   │   ├── main_test.go
│   │   └── unique_fields.go
│   ├── execution
│   │   └── execution.go
│   ├── hooklookup
│   │   ├── hook_lookup.go
│   │   └── hook_lookup_test.go
│   ├── jobcompleter
│   │   ├── job_completer.go
│   │   ├── job_completer_test.go
│   │   └── main_test.go
│   ├── jobexecutor
│   │   ├── job_executor.go
│   │   └── job_executor_test.go
│   ├── jobstats
│   │   └── job_statistics.go
│   ├── leadership
│   │   ├── elector.go
│   │   ├── elector_test.go
│   │   └── main_test.go
│   ├── maintenance
│   │   ├── job_cleaner.go
│   │   ├── job_cleaner_test.go
│   │   ├── job_rescuer.go
│   │   ├── job_rescuer_test.go
│   │   ├── job_scheduler.go
│   │   ├── job_scheduler_test.go
│   │   ├── main_test.go
│   │   ├── periodic_job_enqueuer.go
│   │   ├── periodic_job_enqueuer_test.go
│   │   ├── queue_cleaner.go
│   │   ├── queue_cleaner_test.go
│   │   ├── queue_maintainer.go
│   │   ├── queue_maintainer_test.go
│   │   ├── reindexer.go
│   │   └── reindexer_test.go
│   ├── middlewarelookup
│   │   ├── middleware_lookup.go
│   │   └── middleware_lookup_test.go
│   ├── notifier
│   │   ├── main_test.go
│   │   ├── notifier.go
│   │   └── notifier_test.go
│   ├── notifylimiter
│   │   ├── limiter.go
│   │   └── limiter_test.go
│   ├── rivercommon
│   │   └── river_common.go
│   ├── riverinternaltest
│   │   ├── retrypolicytest
│   │   │   └── retrypolicytest.go
│   │   ├── riverdrivertest
│   │   │   └── riverdrivertest.go
│   │   ├── riverinternaltest.go
│   │   ├── riverinternaltest_test.go
│   │   └── sharedtx
│   │       ├── shared_tx.go
│   │       └── shared_tx_test.go
│   ├── testdb
│   │   ├── db_with_pool.go
│   │   ├── manager.go
│   │   └── manager_test.go
│   ├── util
│   │   ├── chanutil
│   │   │   ├── debounced_chan.go
│   │   │   ├── debounced_chan_test.go
│   │   │   └── main_test.go
│   │   ├── dbutil
│   │   │   ├── db_util.go
│   │   │   ├── db_util_test.go
│   │   │   └── main_test.go
│   │   └── hashutil
│   │       ├── hash_util.go
│   │       └── hash_util_test.go
│   └── workunit
│       └── work_unit.go
├── job.go
├── job_args_reflect_kind_test.go
├── job_complete_tx.go
├── job_complete_tx_test.go
├── job_list_params.go
├── job_list_params_test.go
├── job_test.go
├── main_test.go
├── middleware_defaults.go
├── middleware_defaults_test.go
├── middleware_test.go
├── periodic_job.go
├── periodic_job_test.go
├── plugin.go
├── plugin_test.go
├── producer.go
├── producer_test.go
├── queue_list_params.go
├── queue_pause_opts.go
├── recorded_output.go
├── recorded_output_test.go
├── retry_policy.go
├── retry_policy_test.go
├── riverdriver
│   ├── go.mod
│   ├── go.sum
│   ├── river_driver_interface.go
│   ├── river_driver_interface_test.go
│   ├── riverdatabasesql
│   │   ├── go.mod
│   │   ├── go.sum
│   │   ├── internal
│   │   │   ├── dbsqlc
│   │   │   │   ├── db.go
│   │   │   │   ├── models.go
│   │   │   │   ├── pg_misc.sql.go
│   │   │   │   ├── river_client.sql.go
│   │   │   │   ├── river_client_queue.sql.go
│   │   │   │   ├── river_job.sql.go
│   │   │   │   ├── river_leader.sql.go
│   │   │   │   ├── river_migration.sql.go
│   │   │   │   ├── river_queue.sql.go
│   │   │   │   └── sqlc.yaml
│   │   │   └── pgtypealias
│   │   │       ├── null_bytea.go
│   │   │       └── pgtype_alias.go
│   │   ├── migration
│   │   │   └── main
│   │   │       ├── 001_create_river_migration.down.sql
│   │   │       ├── 001_create_river_migration.up.sql
│   │   │       ├── 002_initial_schema.down.sql
│   │   │       ├── 002_initial_schema.up.sql
│   │   │       ├── 003_river_job_tags_non_null.down.sql
│   │   │       ├── 003_river_job_tags_non_null.up.sql
│   │   │       ├── 004_pending_and_more.down.sql
│   │   │       ├── 004_pending_and_more.up.sql
│   │   │       ├── 005_migration_unique_client.down.sql
│   │   │       ├── 005_migration_unique_client.up.sql
│   │   │       ├── 006_bulk_unique.down.sql
│   │   │       └── 006_bulk_unique.up.sql
│   │   ├── river_database_sql_driver.go
│   │   └── river_database_sql_driver_test.go
│   └── riverpgxv5
│       ├── go.mod
│       ├── go.sum
│       ├── internal
│       │   └── dbsqlc
│       │       ├── copyfrom.go
│       │       ├── db.go
│       │       ├── models.go
│       │       ├── pg_misc.sql
│       │       ├── pg_misc.sql.go
│       │       ├── river_client.sql
│       │       ├── river_client.sql.go
│       │       ├── river_client_queue.sql
│       │       ├── river_client_queue.sql.go
│       │       ├── river_job.sql
│       │       ├── river_job.sql.go
│       │       ├── river_job_copyfrom.sql
│       │       ├── river_job_copyfrom.sql.go
│       │       ├── river_leader.sql
│       │       ├── river_leader.sql.go
│       │       ├── river_migration.sql
│       │       ├── river_migration.sql.go
│       │       ├── river_queue.sql
│       │       ├── river_queue.sql.go
│       │       └── sqlc.yaml
│       ├── migration
│       │   └── main
│       │       ├── 001_create_river_migration.down.sql
│       │       ├── 001_create_river_migration.up.sql
│       │       ├── 002_initial_schema.down.sql
│       │       ├── 002_initial_schema.up.sql
│       │       ├── 003_river_job_tags_non_null.down.sql
│       │       ├── 003_river_job_tags_non_null.up.sql
│       │       ├── 004_pending_and_more.down.sql
│       │       ├── 004_pending_and_more.up.sql
│       │       ├── 005_migration_unique_client.down.sql
│       │       ├── 005_migration_unique_client.up.sql
│       │       ├── 006_bulk_unique.down.sql
│       │       └── 006_bulk_unique.up.sql
│       ├── river_pgx_v5_driver.go
│       └── river_pgx_v5_driver_test.go
├── rivermigrate
│   ├── example_migrate_database_sql_test.go
│   ├── example_migrate_test.go
│   ├── main_test.go
│   ├── migration
│   │   ├── alternate
│   │   │   ├── 001_premier.down.sql
│   │   │   ├── 001_premier.up.sql
│   │   │   ├── 002_deuxieme.down.sql
│   │   │   ├── 002_deuxieme.up.sql
│   │   │   ├── 003_troisieme.down.sql
│   │   │   ├── 003_troisieme.up.sql
│   │   │   ├── 004_quatrieme.down.sql
│   │   │   ├── 004_quatrieme.up.sql
│   │   │   ├── 005_cinquieme.down.sql
│   │   │   ├── 005_cinquieme.up.sql
│   │   │   ├── 006_sixieme.down.sql
│   │   │   └── 006_sixieme.up.sql
│   │   ├── commit_required
│   │   │   ├── 001_first.down.sql
│   │   │   ├── 001_first.up.sql
│   │   │   ├── 002_second.down.sql
│   │   │   ├── 002_second.up.sql
│   │   │   ├── 003_third.down.sql
│   │   │   └── 003_third.up.sql
│   │   └── main
│   │       ├── 001_first.down.sql
│   │       ├── 001_first.up.sql
│   │       ├── 002_second.down.sql
│   │       └── 002_second.up.sql
│   ├── river_migrate.go
│   └── river_migrate_test.go
├── rivershared
│   ├── README.md
│   ├── baseservice
│   │   ├── base_service.go
│   │   └── base_service_test.go
│   ├── cmd
│   │   ├── update-mod-go
│   │   │   ├── main.go
│   │   │   └── main_test.go
│   │   └── update-mod-version
│   │       ├── main.go
│   │       └── main_test.go
│   ├── go.mod
│   ├── go.sum
│   ├── levenshtein
│   │   ├── License.txt
│   │   ├── levenshtein.go
│   │   └── levenshtein_test.go
│   ├── riverpilot
│   │   ├── pilot.go
│   │   └── standard.go
│   ├── riversharedtest
│   │   ├── riversharedtest.go
│   │   └── riversharedtest_test.go
│   ├── slogtest
│   │   ├── slog_test_handler.go
│   │   └── slog_test_handler_test.go
│   ├── sqlctemplate
│   │   ├── sqlc_template.go
│   │   └── sqlc_template_test.go
│   ├── startstop
│   │   ├── main_test.go
│   │   ├── start_stop.go
│   │   └── start_stop_test.go
│   ├── startstoptest
│   │   ├── startstoptest.go
│   │   └── startstoptest_test.go
│   ├── testfactory
│   │   └── test_factory.go
│   ├── testsignal
│   │   ├── test_signal.go
│   │   └── test_signal_test.go
│   └── util
│       ├── maputil
│       │   ├── map_util.go
│       │   └── map_util_test.go
│       ├── ptrutil
│       │   ├── ptr_util.go
│       │   └── ptr_util_test.go
│       ├── randutil
│       │   ├── rand_util.go
│       │   └── rand_util_test.go
│       ├── serviceutil
│       │   ├── service_util.go
│       │   └── service_util_test.go
│       ├── sliceutil
│       │   ├── slice_util.go
│       │   └── slice_util_test.go
│       ├── slogutil
│       │   └── slog_util.go
│       ├── timeutil
│       │   ├── main_test.go
│       │   ├── time_util.go
│       │   └── time_util_test.go
│       └── valutil
│           ├── val_util.go
│           └── val_util_test.go
├── rivertest
│   ├── example_require_inserted_test.go
│   ├── example_require_many_inserted_test.go
│   ├── main_test.go
│   ├── rivertest.go
│   ├── rivertest_test.go
│   ├── time_stub.go
│   ├── time_stub_test.go
│   ├── worker.go
│   └── worker_test.go
├── rivertype
│   ├── execution_error.go
│   ├── go.mod
│   ├── go.sum
│   ├── river_type.go
│   ├── river_type_test.go
│   └── time_generator.go
├── subscription_manager.go
├── subscription_manager_test.go
├── work_unit_wrapper.go
├── worker.go
└── worker_test.go

```

`CHANGELOG.md`:

```md
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Changed

- Set minimum Go version to Go 1.23. [PR #811](https://github.com/riverqueue/river/pull/811).
- Deprecate `river.JobInsertMiddlewareDefaults` and `river.WorkerMiddlewareDefaults` in favor of the more general `river.MiddlewareDefaults` embeddable struct. The two former structs will be removed in a future version. [PR #815](https://github.com/riverqueue/river/pull/815).

## [0.19.0] - 2025-03-16

⚠️ Version 0.19.0 has minor breaking changes for the `Worker.Middleware`, introduced fairly recently in 0.17.0 that has a worker's `Middleware` function now taking a non-generic `JobRow` parameter instead of a generic `Job[T]`. We tried not to make this change, but found the existing middleware interface insufficient to provide the necessary range of functionality we wanted, and this is a secondary middleware facility that won't be in use for many users, so it seemed worthwhile.

### Added

- Added a new "hooks" API for tying into River functionality at various points like job inserts or working. Differs from middleware in that it doesn't go on the stack and can't modify context, but in some cases is able to run at a more granular level (e.g. for each job insert rather than each _batch_ of inserts). [PR #789](https://github.com/riverqueue/river/pull/789).
- `river.Config` has a generic `Middleware` setting that can be used as a convenient way to configure middlewares that implement multiple middleware interfaces (e.g. `JobInsertMiddleware` _and_ `WorkerMiddleware`). Use of this setting is preferred over `Config.JobInsertMiddleware` and `Config.WorkerMiddleware`, which have been deprecated. [PR #804](https://github.com/riverqueue/river/pull/804).

### Changed

- The `river.RecordOutput` function now returns an error if the output is too large. The output is limited to 32MB in size. [PR #782](https://github.com/riverqueue/river/pull/782).
- **Breaking change:** The `Worker` interface's `Middleware` function now takes a `JobRow` parameter instead of a generic `Job[T]`. This was necessary to expand the potential of what middleware can do: by letting the executor extract a middleware stack from a worker before a job is fully unmarshaled, the middleware can also participate in the unmarshaling process. [PR #783](https://github.com/riverqueue/river/pull/783).
- `JobList` has been reimplemented to use sqlc. [PR #795](https://github.com/riverqueue/river/pull/795).

## [0.18.0] - 2025-02-20

⚠️ Version 0.18.0 has breaking changes for the `rivertest.Worker` type that was just introduced. While attempting to round out some edge cases with its design, we realized some of them simply couldn't be solved adequately without changing the overall design such that all tested jobs are inserted into the database. Given the short duration since it was released (over a weekend) it's unlikely many users have adopted it and it seemed best to rip off the bandaid to fix it before it gets widely used.

### Added

- Jobs can now store a recorded "output" value, a JSON-encoded payload set by the job during execution and stored in the job's metadata. The `river.RecordOutput` function makes it easy to use the job row to store transient/temporary values that are needed for introspection or for other downstream jobs. The output can be accessed using the `JobRow.Output()` helper method.

  This output is stored at the same time as the job is completed following execution, so it does not require additional database calls or overhead. Output can be anything that can be stored in a Postgres JSONB field, though for performance reasons it should be limited in size. [PR #758](https://github.com/riverqueue/river/pull/758).

### Changed

- **Breaking change:** The `rivertest.Worker` type now requires all jobs to be inserted into the database. The original design allowed workers to be tested without hitting the database at all. Ultimately this design made it hard to correctly simulate features like `JobCompleteTx` and the other potential solutions seemed undesirable.

  As part of this change, the `Work` and `WorkJob` methods now take a transaction argument. The expectation is that a transaction will be opened by the caller and rolled back after test completion. Additionally, the return signature was changed to return a `WorkResult` struct alongside the error. The struct includes the post-execution job row as well as the event kind that occurred, making it easy to inspect the job's state after execution.

  Finally, the implementation was refactored so that it uses the _real_ `river.Client` insert path, and also uses the same job execution path as real execution. This minimizes the potential for differences in behavior between testing and real execution.
  [PR #766](https://github.com/riverqueue/river/pull/766).

- Adjusted panic stack traces to filter out irrelevant frames like the ones generated by the runtime package that constructed the trace, or River's internal rescuing code. This makes the first panic frame reflect the actual panic origin for easier debugging. [PR #774](https://github.com/riverqueue/river/pull/774).

### Fixed

- Fix error message on unsuccessful client subscribe that erroneously referred to "Workers" not configured. [PR #771](https://github.com/riverqueue/river/pull/771).
- Fix an issue with encoding unique keys in riverdatabasesql driver. [PR #777](https://github.com/riverqueue/river/pull/777).

## [0.17.0] - 2025-02-16

### Added

- Exposed `TestConfig` struct on `Config` under the `Test` field for configuration that is specific to test environments. For now, the only field on this type is `Time`, which can be used to set a synthetic `TimeGenerator` for tests. A stubbable time generator was added as `rivertest.TimeStub` to allow time to be easily stubbed in tests. [PR #754](https://github.com/riverqueue/river/pull/754).
- New `rivertest.Worker` type to make it significantly easier to test River workers. Either real or synthetic jobs can be worked using this interface, generally without requiring any database interactions. The `Worker` type provides a realistic execution environment with access to the full range of River features, including `river.ClientFromContext`, middleware (both global and per-worker), and timeouts. [PR #753](https://github.com/riverqueue/river/pull/753).

### Changed

- Errors returned from retryable jobs are now logged with warning logs instead of error logs. Error logs are still used for jobs that error after reaching `max_attempts`. [PR #743](https://github.com/riverqueue/river/pull/743).
- Remove range variable capture in `for` loops and use simplified `range` syntax. Each of these requires Go 1.22 or later, which was already our minimum required version since Go 1.23 was released. [PR #755](https://github.com/riverqueue/river/pull/755).

### Fixed

- `riverdatabasesql` driver: properly handle `nil` values in `bytea[]` inputs. This fixes the driver's handling of empty unique keys on insert for non-unique jobs with the newer unique jobs implementation. [PR #739](https://github.com/riverqueue/river/pull/739).
- `JobCompleteTx` now returns `rivertype.ErrNotFound` if the job doesn't exist instead of panicking. [PR #753](https://github.com/riverqueue/river/pull/753).
- - `NeverSchedule.Next` now returns the correct maximum time value, ensuring that the periodic job truly never runs. This fixes an issue where an incorrect maximum timestamp was previously used. Thanks Hubert Krauze ([@krhubert](https://github.com/krhubert))! [PR #760](https://github.com/riverqueue/river/pull/760)

## [0.16.0] - 2024-01-27

### Added

- `NeverSchedule` returns a `PeriodicSchedule` that never runs. This can be used to effectively disable the reindexer or any other maintenance service. [PR #718](https://github.com/riverqueue/river/pull/718).
- Add `SkipUnknownJobCheck` client config option to skip job arg worker validation. [PR #731](https://github.com/riverqueue/river/pull/731).

### Changed

- The reindexer maintenance process has been enabled. As of now, it will reindex only the `river_job_args_index` and `river_jobs_metadata_index` `GIN` indexes, which are more prone to bloat than b-tree indexes. By default it runs daily at midnight UTC, but can be customized on the `river.Config` type via `ReindexerSchedule`. Most installations will benefit from this process, but it can be disabled altogether using `NeverSchedule`. [PR #718](https://github.com/riverqueue/river/pull/718).
- Periodic jobs now have a `"periodic": true` attribute set in their metadata to make them more easily distinguishable from other types of jobs. [PR #728](https://github.com/riverqueue/river/pull/728).
- Snoozing a job now causes its `attempt` to be _decremented_, whereas previously the `max_attempts` would be incremented. In either case, this avoids allowing a snooze to exhaust a job's retries; however the new behavior also avoids potential issues with wrapping the `max_attempts` value, and makes it simpler to implement a `RetryPolicy` based on either `attempt` or `max_attempts`. The number of snoozes is also tracked in the job's metadata as `snoozes` for debugging purposes.

  The implementation of the builtin `RetryPolicy` implementations is not changed, so this change should not cause any user-facing breakage unless you're relying on `attempt - len(errors)` for some reason. [PR #730](https://github.com/riverqueue/river/pull/730).

- `ByPeriod` uniqueness is now based off a job's `ScheduledAt` instead of the current time if it has a value. [PR #734](https://github.com/riverqueue/river/pull/734).

## [0.15.0] - 2024-12-26

### Added

- The River CLI will now respect the standard set of `PG*` environment variables like `PGHOST`, `PGPORT`, `PGDATABASE`, `PGUSER`, `PGPASSWORD`, and `PGSSLMODE` to configure a target database when the `--database-url` parameter is omitted. [PR #702](https://github.com/riverqueue/river/pull/702).
- Add missing doc for `JobRow.UniqueStates` + reveal `rivertype.UniqueOptsByStateDefault()` to provide access to the default set of unique job states. [PR #707](https://github.com/riverqueue/river/pull/707).

### Changed

- Sleep durations are now logged as Go-like duration strings (e.g. "10s") in either text or JSON instead of duration strings in text and nanoseconds in JSON. [PR #699](https://github.com/riverqueue/river/pull/699).
- Altered the migration comments from `river migrate-get` to include the "line" of the migration being run (`main`, or for River Pro `workflow` and `sequence`) to make them more distinguishable. [PR #703](https://github.com/riverqueue/river/pull/703).
- Fewer slice allocations during unique insertions. [PR #705](https://github.com/riverqueue/river/pull/705).

### Fixed

- Exponential backoffs at degenerately high job attempts (>= 310) no longer risk overflowing `time.Duration`. [PR #698](https://github.com/riverqueue/river/pull/698).

## [0.14.3] - 2024-12-14

### Changed

- Dropped internal random generators in favor of `math/rand/v2`, which will have the effect of making code fully incompatible with Go 1.21 (`go.mod` has specified a minimum of 1.22 for some time already though). [PR #691](https://github.com/riverqueue/river/pull/691).

### Fixed

- 006 migration now tolerates previous existence of a `unique_states` column in case it was added separately so that the new index could be raised with `CONCURRENTLY`. [PR #690](https://github.com/riverqueue/river/pull/690).

## [0.14.2] - 2024-11-16

### Fixed

- Cancellation of running jobs relied on a channel that was only being received when in the job fetch routine, meaning that jobs which were cancelled would not be cancelled until the next scheduled fetch. This was fixed by also receiving from the job cancellation channel when in the main producer loop, even if no fetches are happening. [PR #678](https://github.com/riverqueue/river/pull/678).
- Job insert middleware were not being utilized for periodic jobs. This insertion path has been refactored to rely on the unified insertion path from the client. Fixes #675. [PR #679](https://github.com/riverqueue/river/pull/679).

## [0.14.1] - 2024-11-04

### Fixed

- In [PR #663](https://github.com/riverqueue/river/pull/663) the client was changed to be more aggressive about re-fetching when it had previously fetched a full batch. Unfortunately a clause was missed, which resulted in the client being more aggressive any time even a single job was fetched on the previous attempt. This was corrected with a conditional to ensure it only happens when the last fetch was full. [PR #668](https://github.com/riverqueue/river/pull/668).

## [0.14.0] - 2024-11-03

### Added

- Expose `JobCancelError` and `JobSnoozeError` types to more easily facilitate testing. [PR #665](https://github.com/riverqueue/river/pull/665).

### Changed

- Tune the client to be more aggressive about fetching when it just fetched a full batch of jobs, or when it skipped its previous triggered fetch because it was already full. This should bring more consistent throughput to poll-only mode and in cases where there is a backlog of existing jobs but new ones aren't being actively inserted. This will result in increased fetch load on many installations, with the benefit of increased throughput. As before, `FetchCooldown` still limits how frequently these fetches can occur on each client and can be increased to reduce the amount of fetch querying. Thanks Chris Gaffney ([@gaffneyc](https://github.com/gaffneyc)) for the idea, initial implementation, and benchmarks. [PR #663](https://github.com/riverqueue/river/pull/663).

### Fixed

- `riverpgxv5` driver: `Hijack()` the underlying listener connection as soon as it is acquired from the `pgxpool.Pool` in order to prevent the pool from automatically closing it after it reaches its max age. A max lifetime makes sense in the context of a pool with many conns, but a long-lived listener does not need a max lifetime as long as it can ensure the conn remains healthy. [PR #661](https://github.com/riverqueue/river/pull/661).

## [0.13.0] - 2024-10-07

⚠️ Version 0.13.0 removes the original advisory lock based unique jobs implementation that was deprecated in v0.12.0. See details in the note below or the v0.12.0 release notes.

### Added

- A middleware system was added for job insertion and execution, providing the ability to extract shared functionality across workers. Both `JobInsertMiddleware` and `WorkerMiddleware` can be configured globally on the `Client`, and `WorkerMiddleware` can also be added on a per-worker basis using the new `Middleware` method on `Worker[T]`. Middleware can be useful for logging, telemetry, or for building higher level abstractions on top of base River functionality.

  Despite the interface expansion, users should not encounter any breakage if they're embedding the `WorkerDefaults` type in their workers as recommended. [PR #632](https://github.com/riverqueue/river/pull/632).

### Changed

- **Breaking change:** The advisory lock unique jobs implementation which was deprecated in v0.12.0 has been removed. Users of that feature should first upgrade to v0.12.1 to ensure they don't see any warning logs about using the deprecated advisory lock uniqueness. The new, faster unique implementation will be used automatically as long as the `UniqueOpts.ByState` list hasn't been customized to remove [required states](https://riverqueue.com/docs/unique-jobs#unique-by-state) (`pending`, `scheduled`, `available`, and `running`). As of this release, customizing `ByState` without these required states returns an error. [PR #614](https://github.com/riverqueue/river/pull/614).
- Single job inserts are now unified under the hood to use the `InsertMany` bulk insert query. This should not be noticeable to users, and the unified code path will make it easier to build new features going forward. [PR #614](https://github.com/riverqueue/river/pull/614).

### Fixed

- Allow `river.JobCancel` to accept a `nil` error as input without panicking. [PR #634](https://github.com/riverqueue/river/pull/634).

## [0.12.1] - 2024-09-26

### Changed

- The `BatchCompleter` that marks jobs as completed can now batch database updates for _all_ states of jobs that have finished execution. Prior to this change, only `completed` jobs were batched into a single `UPDATE` call, while jobs moving to any other state used a single `UPDATE` per job. This change should significantly reduce database and pool contention on high volume system when jobs get retried, snoozed, cancelled, or discarded following execution. [PR #617](https://github.com/riverqueue/river/pull/617).

### Fixed

- Unique job changes from v0.12.0 / [PR #590](https://github.com/riverqueue/river/pull/590) introduced a bug with scheduled or retryable unique jobs where they could be considered in conflict with themselves and moved to `discarded` by mistake. There was also a possibility of a broken job scheduler if duplicate `retryable` unique jobs were attempted to be scheduled at the same time. The job scheduling query was corrected to address these issues along with missing test coverage. [PR #619](https://github.com/riverqueue/river/pull/619).

## [0.12.0] - 2024-09-23

⚠️ Version 0.12.0 contains a new database migration, version 6. See [documentation on running River migrations](https://riverqueue.com/docs/migrations). If migrating with the CLI, make sure to update it to its latest version:

```shell
go install github.com/riverqueue/river/cmd/river@latest
river migrate-up --database-url "$DATABASE_URL"
```

If not using River's internal migration system, the raw SQL can alternatively be dumped with:

```shell
go install github.com/riverqueue/river/cmd/river@latest
river migrate-get --version 6 --up > river6.up.sql
river migrate-get --version 6 --down > river6.down.sql
```

The migration **includes a new index**. Users with a very large job table may want to consider raising the index separately using `CONCURRENTLY` (which must be run outside of a transaction), then run `river migrate-up` to finalize the process (it will tolerate an index that already exists):

```sql
ALTER TABLE river_job ADD COLUMN unique_states BIT(8);

CREATE UNIQUE INDEX CONCURRENTLY river_job_unique_idx ON river_job (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state);
```

```shell
go install github.com/riverqueue/river/cmd/river@latest
river migrate-up --database-url "$DATABASE_URL"
```

## Added

- `rivertest.WorkContext`, a test function that can be used to initialize a context to test a `JobArgs.Work` implementation that will have a client set to context for use with `river.ClientFromContext`. [PR #526](https://github.com/riverqueue/river/pull/526).
- A new `river migrate-list` command is available which lists available migrations and which version a target database is migrated to. [PR #534](https://github.com/riverqueue/river/pull/534).
- `river version` or `river --version` now prints River version information. [PR #537](https://github.com/riverqueue/river/pull/537).
- `Config.JobCleanerTimeout` was added to allow configuration of the job cleaner query timeout. In some deployments with millions of stale jobs, the cleaner may not be able to complete its query within the default 30 seconds. [PR #576](https://github.com/riverqueue/river/pull/576).

### Changed

⚠️ Version 0.12.0 has two small breaking changes, one for `InsertMany` and one in `rivermigrate`. As before, we try never to make breaking changes, but these ones were deemed worth it because of minimal impact and to help avoid panics.

- **Breaking change:** `Client.InsertMany` / `InsertManyTx` now return the inserted rows rather than merely returning a count of the inserted rows. The new implementations no longer use Postgres' `COPY FROM` protocol in order to facilitate return values.

  Users who relied on the return count can merely wrap the returned rows in a `len()` to return to that behavior, or you can continue using the old APIs using their new names `InsertManyFast` and `InsertManyFastTx`. [PR #589](https://github.com/riverqueue/river/pull/589).

- **Breaking change:** `rivermigrate.New` now returns a possible error along with a migrator. An error may be returned, for example, when a migration line is configured that doesn't exist. [PR #558](https://github.com/riverqueue/river/pull/558).

  ```go
  # before
  migrator := rivermigrate.New(riverpgxv5.New(dbPool), nil)

  # after
  migrator, err := rivermigrate.New(riverpgxv5.New(dbPool), nil)
  if err != nil {
      // handle error
  }
  ```

- Unique jobs have been improved to allow bulk insertion of unique jobs via `InsertMany` / `InsertManyTx`, and to allow customizing the `ByState` list to add or remove certain states. This enables users to expand the set of unique states to also include `cancelled` and `discarded` jobs, or to remove `retryable` from uniqueness consideration. This updated implementation maintains the speed advantage of the newer index-backed uniqueness system, while allowing some flexibility in which job states.

  Unique jobs utilizing `ByArgs` can now also opt to have a subset of the job's arguments considered for uniqueness. For example, you could choose to consider only the `customer_id` field while ignoring the `trace_id` field:

  ```go
  type MyJobArgs {
    CustomerID string `json:"customer_id" river:"unique`
    TraceID string `json:"trace_id"`
  }
  ```

  Any fields considered in uniqueness are also sorted alphabetically in order to guarantee a consistent result, even if the encoded JSON isn't sorted consistently. For example `encoding/json` encodes struct fields in their defined order, so merely reordering struct fields would previously have been enough to cause a new job to not be considered identical to a pre-existing one with different JSON order.

  The `UniqueOpts` type also gains an `ExcludeKind` option for cases where uniqueness needs to be guaranteed across multiple job types.

  In-flight unique jobs using the previous designs will continue to be executed successfully with these changes, so there should be no need for downtime as part of the migration. However the v6 migration adds a new unique job index while also removing the old one, so users with in-flight unique jobs may also wish to avoid removing the old index until the new River release has been deployed in order to guarantee that jobs aren't duplicated by old River code once that index is removed.

  **Deprecated**: The original unique jobs implementation which relied on advisory locks has been deprecated, but not yet removed. The only way to trigger this old code path is with a single insert (`Insert`/`InsertTx`) and using `UniqueOpts.ByState` with a custom list of states that omits some of the now-required states for unique jobs. Specifically, `pending`, `scheduled`, `available`, and `running` can not be removed from the `ByState` list with the new implementation. These are included in the default list so only the places which customize this attribute need to be updated to opt into the new (much faster) unique jobs. The advisory lock unique implementation will be removed in an upcoming release, and until then emits warning level logs when it's used.

  [PR #590](https://github.com/riverqueue/river/pull/590).

- **Deprecated**: The `MigrateTx` method of `rivermigrate` has been deprecated. It turns out there are certain combinations of schema changes which cannot be run within a single transaction, and the migrator now prefers to run each migration in its own transaction, one-at-a-time. `MigrateTx` will be removed in future version.

- The migrator now produces a better error in case of a non-existent migration line including suggestions for known migration lines that are similar in name to the invalid one. [PR #558](https://github.com/riverqueue/river/pull/558).

## Fixed

- Fixed a panic that'd occur if `StopAndCancel` was invoked before a client was started. [PR #557](https://github.com/riverqueue/river/pull/557).
- A `PeriodicJobConstructor` should be able to return `nil` `JobArgs` if it wishes to not have any job inserted. However, this was either never working or was broken at some point. It's now fixed. Thanks [@semanser](https://github.com/semanser)! [PR #572](https://github.com/riverqueue/river/pull/572).
- Fixed a nil pointer exception if `Client.Subscribe` was called when the client had no configured workers (it still, panics with a more instructive error message now). [PR #599](https://github.com/riverqueue/river/pull/599).

## [0.11.4] - 2024-08-20

### Fixed

- Fixed release script that caused CLI to become uninstallable because its reference to `rivershared` wasn't updated. [PR #541](https://github.com/riverqueue/river/pull/541).

## [0.11.3] - 2024-08-19

### Changed

- Producer's logs are quieter unless jobs are actively being worked. [PR #529](https://github.com/riverqueue/river/pull/529).

### Fixed

- River CLI now accepts `postgresql://` URL schemes in addition to `postgres://`. [PR #532](https://github.com/riverqueue/river/pull/532).

## [0.11.2] - 2024-08-08

### Fixed

- Derive all internal contexts from user-provided `Client` context. This includes the job fetch context, notifier unlisten, and completer. [PR #514](https://github.com/riverqueue/river/pull/514).
- Lowered the `go` directives in `go.mod` to Go 1.21, which River aims to support. A more modern version of Go is specified with the `toolchain` directive. This should provide more flexibility on the minimum required Go version for programs importing River. [PR #522](https://github.com/riverqueue/river/pull/522).

## [0.11.1] - 2024-08-05

### Fixed

- `database/sql` driver: fix default value of `scheduled_at` for `InsertManyTx` when it is not specified in `InsertOpts`. [PR #504](https://github.com/riverqueue/river/pull/504).
- Change `ColumnExists` query to respect `search_path`, thereby allowing migrations to be runnable outside of default schema. [PR #505](https://github.com/riverqueue/river/pull/505).

## [0.11.0] - 2024-08-02

### Added

- Expose `Driver` on `Client` for additional River Pro integrations. This is not a stable API and should generally not be used by others. [PR #497](https://github.com/riverqueue/river/pull/497).

## [0.10.2] - 2024-07-31

### Fixed

- Include `pending` state in `JobListParams` by default so pending jobs are included in `JobList` / `JobListTx` results. [PR #477](https://github.com/riverqueue/river/pull/477).
- Quote strings when using `Client.JobList` functions with the `database/sql` driver. [PR #481](https://github.com/riverqueue/river/pull/481).
- Remove use of `filepath` for interacting with embedded migration files, fixing the migration CLI for Windows. [PR #485](https://github.com/riverqueue/river/pull/485).
- Respect `ScheduledAt` if set to a non-zero value by `JobArgsWithInsertOpts`. This allows for job arg definitions to utilize custom logic at the args level for determining when the job should be scheduled. [PR #487](https://github.com/riverqueue/river/pull/487).

## [0.10.1] - 2024-07-23

### Fixed

- Migration version 005 has been altered so that it can run even if the `river_migration` table isn't present, making it more friendly for projects that aren't using River's internal migration system. [PR #465](https://github.com/riverqueue/river/pull/465).

## [0.10.0] - 2024-07-19

⚠️ Version 0.10.0 contains a new database migration, version 5. See [documentation on running River migrations](https://riverqueue.com/docs/migrations). If migrating with the CLI, make sure to update it to its latest version:

```shell
go install github.com/riverqueue/river/cmd/river@latest
river migrate-up --database-url "$DATABASE_URL"
```

If not using River's internal migration system, the raw SQL can alternatively be dumped with:

```shell
go install github.com/riverqueue/river/cmd/river@latest
river migrate-get --version 5 --up > river5.up.sql
river migrate-get --version 5 --down > river5.down.sql
```

The migration **includes a new index**. Users with a very large job table may want to consider raising the index separately using `CONCURRENTLY` (which must be run outside of a transaction), then run `river migrate-up` to finalize the process (it will tolerate an index that already exists):

```sql
ALTER TABLE river_job
    ADD COLUMN unique_key bytea;

CREATE UNIQUE INDEX CONCURRENTLY river_job_kind_unique_key_idx ON river_job (kind, unique_key) WHERE unique_key IS NOT NULL;
```

```shell
go install github.com/riverqueue/river/cmd/river@latest
river migrate-up --database-url "$DATABASE_URL"
```

### Added

- Fully functional driver for `database/sql` for use with packages like Bun and GORM. [PR #351](https://github.com/riverqueue/river/pull/351).
- Queues can be added after a client is initialized using `client.Queues().Add(queueName string, queueConfig QueueConfig)`. [PR #410](https://github.com/riverqueue/river/pull/410).
- Migration that adds a `line` column to the `river_migration` table so that it can support multiple migration lines. [PR #435](https://github.com/riverqueue/river/pull/435).
- `--line` flag added to the River CLI. [PR #454](https://github.com/riverqueue/river/pull/454).

### Changed

- Tags are now limited to 255 characters in length, and should match the regex `\A[\w][\w\-]+[\w]\z` (importantly, they can't contain commas). [PR #351](https://github.com/riverqueue/river/pull/351).
- Many info logging statements have been demoted to debug level. [PR #452](https://github.com/riverqueue/river/pull/452).
- `pending` is now part of the default set of unique job states. [PR #461](https://github.com/riverqueue/river/pull/461).

## [0.9.0] - 2024-07-04

### Added

- `Config.TestOnly` has been added. It disables various features in the River client like staggered maintenance service start that are useful in production, but may be somewhat harmful in tests because they make start/stop slower. [PR #414](https://github.com/riverqueue/river/pull/414).

### Changed

⚠️ Version 0.9.0 has a small breaking change in `ErrorHandler`. As before, we try never to make breaking changes, but this one was deemed quite important because `ErrorHandler` was fundamentally lacking important functionality.

- **Breaking change:** Add stack trace to `ErrorHandler.HandlePanicFunc`. Fixing code only requires adding a new `trace string` argument to `HandlePanicFunc`. [PR #423](https://github.com/riverqueue/river/pull/423).

  ```go
  # before
  HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any) *ErrorHandlerResult

  # after
  HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult
  ```

### Fixed

- Pausing or resuming a queue that was already paused or not paused respectively no longer returns `rivertype.ErrNotFound`. The same goes for pausing or resuming using the all queues string (`*`) when no queues are in the database (previously that also returned `rivertype.ErrNotFound`). [PR #408](https://github.com/riverqueue/river/pull/408).
- Fix a bug where periodic job constructors were only called once when adding the periodic job rather than being invoked every time the periodic job is scheduled. [PR #420](https://github.com/riverqueue/river/pull/420).

## [0.8.0] - 2024-06-25

### Added

- Add transaction variants for queue-related client functions: `QueueGetTx`, `QueueListTx`, `QueuePauseTx`, and `QueueResumeTx`. [PR #402](https://github.com/riverqueue/river/pull/402).

### Fixed

- Fix possible Client shutdown panics if the user-provided context is cancelled while jobs are still running. [PR #401](https://github.com/riverqueue/river/pull/401).

## [0.7.0] - 2024-06-13

### Added

- The default max attempts of 25 can now be customized on a per-client basis using `Config.MaxAttempts`. This is in addition to the ability to customize at the job type level with `JobArgs`, or on a per-job basis using `InsertOpts`. [PR #383](https://github.com/riverqueue/river/pull/383).
- Add `JobDelete` / `JobDeleteTx` APIs on `Client` to allow permanently deleting any job that's not currently running. [PR #390](https://github.com/riverqueue/river/pull/390).

### Fixed

- Fix `StopAndCancel` to not hang if called in parallel to an ongoing `Stop` call. [PR #376](https://github.com/riverqueue/river/pull/376).

## [0.6.1] - 2024-05-21

### Fixed

- River now considers per-worker timeout overrides when rescuing jobs so that jobs with a long custom timeout won't be rescued prematurely. [PR #350](https://github.com/riverqueue/river/pull/350).
- River CLI now exits with status 1 in the case of a problem with commands or flags, like an unknown command or missing required flag. [PR #363](https://github.com/riverqueue/river/pull/363).
- Fix migration version 4 (from 0.5.0) so that the up migration can be re-run after it was originally rolled back. [PR #364](https://github.com/riverqueue/river/pull/364).

## [0.6.0] - 2024-05-08

### Added

- `RequireNotInserted` test helper (in addition to the existing `RequireInserted`) that verifies that a job with matching conditions was _not_ inserted. [PR #237](https://github.com/riverqueue/river/pull/237).

### Changed

- The periodic job enqueuer now sets `scheduled_at` of inserted jobs to the more precise time of when they were scheduled to run, as opposed to when they were inserted. [PR #341](https://github.com/riverqueue/river/pull/341).

### Fixed

- Remove use of `github.com/lib/pq`, making it once again a test-only dependency. [PR #337](https://github.com/riverqueue/river/pull/337).

## [0.5.0] - 2024-05-03

⚠️ Version 0.5.0 contains a new database migration, version 4. This migration is backward compatible with any River installation running the v3 migration. Be sure to run the v4 migration prior to deploying the code from this release.

### Added

- Add `pending` job state. This is currently unused, but will be used to build higher level functionality for staging jobs that are not yet ready to run (for some reason other than their scheduled time being in the future). Pending jobs will never be run or deleted and must first be moved to another state by external code. [PR #301](https://github.com/riverqueue/river/pull/301).
- Queue status tracking, pause and resume. [PR #301](https://github.com/riverqueue/river/pull/301).

  A useful operational lever is the ability to pause and resume a queue without shutting down clients. In addition to pause/resume being a feature request from [#54](https://github.com/riverqueue/river/pull/54), as part of the work on River's UI it's been useful to list out the active queues so that they can be displayed and manipulated.

  A new `river_queue` table is introduced in the v4 migration for this purpose. Upon startup, every producer in each River `Client` will make an `UPSERT` query to the database to either register the queue as being active, or if it already exists it will instead bump the timestamp to keep it active. This query will be run periodically in each producer as long as the `Client` is alive, even if the queue is paused. A separate query will delete/purge any queues which have not been active in awhile (currently fixed to 24 hours).

  `QueuePause` and `QueueResume` APIs have been introduced to `Client` pause and resume a single queue by name, or _all_ queues using the special `*` value. Each producer will watch for notifications on the relevant `LISTEN/NOTIFY` topic unless operating in poll-only mode, in which case they will periodically poll for changes to their queue record in the database.

### Changed

- Job insert notifications are now handled within application code rather than within the database using triggers. [PR #301](https://github.com/riverqueue/river/pull/301).

  The initial design for River utilized a trigger on job insert that issued notifications (`NOTIFY`) so that listening clients could quickly pick up the work if they were idle. While this is good for lowering latency, it does have the side effect of emitting a large amount of notifications any time there are lots of jobs being inserted. This adds overhead, particularly to high-throughput installations.

  To improve this situation and reduce overhead in high-throughput installations, the notifications have been refactored to be emitted at the application level. A client-level debouncer ensures that these notifications are not emitted more often than they could be useful. If a queue is due for an insert notification (on a particular Postgres schema), the notification is piggy-backed onto the insert query within the transaction. While this has the impact of increasing insert latency for a certain percentage of cases, the effect should be small.

  Additionally, initial releases of River did not properly scope notification topics within the global `LISTEN/NOTIFY` namespace. If two River installations were operating on the same Postgres database but within different schemas (search paths), their notifications would be emitted on a shared topic name. This is no longer the case and all notifications are prefixed with a `{schema_name}.` string.

- Add `NOT NULL` constraints to the database for `river_job.args` and `river_job.metadata`. Normal code paths should never have allowed for null values any way, but this constraint further strengthens the guarantee. [PR #301](https://github.com/riverqueue/river/pull/301).
- Stricter constraint on `river_job.finalized_at` to ensure it is only set when paired with a finalized state (completed, discarded, cancelled). Normal code paths should never have allowed for invalid values any way, but this constraint further strengthens the guarantee. [PR #301](https://github.com/riverqueue/river/pull/301).

## [0.4.1] - 2024-04-22

### Fixed

- Update job state references in `./cmd/river` and some documentation to `rivertype`. Thanks Danny Hermes (@dhermes)! 🙏🏻 [PR #315](https://github.com/riverqueue/river/pull/315).

## [0.4.0] - 2024-04-20

### Changed

⚠️ Version 0.4.0 has a number of small breaking changes which we've decided to release all as part of a single version. More breaking changes in one release is inconvenient, but we've tried to coordinate them in hopes that any future breaking changes will be non-existent or very rare. All changes will get picked up by the Go compiler, and each one should be quite easy to fix. The changes don't apply to any of the most common core APIs, and likely many projects won't have to change any code.

- **Breaking change:** There are a number of small breaking changes in the job list API using `JobList`/`JobListTx`:
  - Now support querying jobs by a list of Job Kinds and States. Also allows for filtering by specific timestamp values. Thank you Jos Kraaijeveld (@thatjos)! 🙏🏻 [PR #236](https://github.com/riverqueue/river/pull/236).
  - Job listing now defaults to ordering by job ID (`JobListOrderByID`) instead of a job timestamp dependent on requested job state. The previous ordering behavior is still available with `NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc)`. [PR #307](https://github.com/riverqueue/river/pull/307).
  - The function `JobListCursorFromJob` no longer needs a sort order parameter. Instead, sort order is determined based on the job list parameters that the cursor is subsequently used with. [PR #307](https://github.com/riverqueue/river/pull/307).
- **Breaking change:** Client `Insert` and `InsertTx` functions now return a `JobInsertResult` struct instead of a `JobRow`. This allows the result to include metadata like the new `UniqueSkippedAsDuplicate` property, so callers can tell whether an inserted job was skipped due to unique constraint. [PR #292](https://github.com/riverqueue/river/pull/292).
- **Breaking change:** Client `InsertMany` and `InsertManyTx` now return number of jobs inserted as `int` instead of `int64`. This change was made to make the type in use a little more idiomatic. [PR #293](https://github.com/riverqueue/river/pull/293).
- **Breaking change:** `river.JobState*` type aliases have been removed. All job state constants should be accessed through `rivertype.JobState*` instead. [PR #300](https://github.com/riverqueue/river/pull/300).

See also the [0.4.0 release blog post](https://riverqueue.com/blog/a-few-breaking-changes) with code samples and rationale behind various changes.

## [0.3.0] - 2024-04-15

### Added

- The River client now supports "poll only" mode with `Config.PollOnly` which makes it avoid issuing `LISTEN` statements to wait for new events like a leadership resignation or new job available. The program instead polls periodically to look for changes. A leader resigning or a new job being available will be noticed less quickly, but `PollOnly` potentially makes River operable on systems without listen/notify support, like PgBouncer operating in transaction pooling mode. [PR #281](https://github.com/riverqueue/river/pull/281).
- Added `rivertype.JobStates()` that returns the full list of possible job states. [PR #297](https://github.com/riverqueue/river/pull/297).

## [0.2.0] - 2024-03-28

### Added

- New periodic jobs can now be added after a client's already started using `Client.PeriodicJobs().Add()` and removed with `Remove()`. [PR #288](https://github.com/riverqueue/river/pull/288).

### Changed

- The level of some of River's common log statements has changed, most often demoting `info` statements to `debug` so that `info`-level logging is overall less verbose. [PR #275](https://github.com/riverqueue/river/pull/275).

### Fixed

- Fixed a bug in the (log-only for now) reindexer service in which it might repeat its work loop multiple times unexpectedly while stopping. [PR #280](https://github.com/riverqueue/river/pull/280).
- Periodic job enqueuer now bases next run times on each periodic job's last target run time, instead of the time at which the enqueuer is currently running. This is a small difference that will be unnoticeable for most purposes, but makes scheduling of jobs with short cron frequencies a little more accurate. [PR #284](https://github.com/riverqueue/river/pull/284).
- Fixed a bug in the elector in which it was possible for a resigning, but not completely stopped, elector to reelect despite having just resigned. [PR #286](https://github.com/riverqueue/river/pull/286).

## [0.1.0] - 2024-03-17

Although it comes with a number of improvements, there's nothing particularly notable about version 0.1.0. Until now we've only been incrementing the patch version given the project's nascent nature, but from here on we'll try to adhere more closely to semantic versioning, using the patch version for bug fixes, and incrementing the minor version when new functionality is added.

### Added

- The River CLI now supports `river bench` to benchmark River's job throughput against a database. [PR #254](https://github.com/riverqueue/river/pull/254).
- The River CLI now has a `river migrate-get` command to dump SQL for River migrations for use in alternative migration frameworks. Use it like `river migrate-get --up --version 3 > version3.up.sql`. [PR #273](https://github.com/riverqueue/river/pull/273).
- The River CLI's `migrate-down` and `migrate-up` options get two new options for `--dry-run` and `--show-sql`. They can be combined to easily run a preflight check on a River upgrade to see which migration commands would be run on a database, but without actually running them. [PR #273](https://github.com/riverqueue/river/pull/273).
- The River client gets a new `Client.SubscribeConfig` function that lets a subscriber specify the maximum size of their subscription channel. [PR #258](https://github.com/riverqueue/river/pull/258).

### Changed

- River uses a new job completer that batches up completion work so that large numbers of them can be performed more efficiently. In a purely synthetic (i.e. mostly unrealistic) benchmark, River's job throughput increases ~4.5x. [PR #258](https://github.com/riverqueue/river/pull/258).
- Changed default client IDs to be a combination of hostname and the time which the client started. This can still be changed by specifying `Config.ID`. [PR #255](https://github.com/riverqueue/river/pull/255).
- Notifier refactored for better robustness and testability. [PR #253](https://github.com/riverqueue/river/pull/253).

## [0.0.25] - 2024-03-01

### Fixed

- Fixed a problem in `riverpgxv5`'s `Listener` where it wouldn't unset an internal connection if `Close` returned an error, making the listener not reusable. Thanks @mfrister for pointing this one out! [PR #246](https://github.com/riverqueue/river/pull/246).

## [0.0.24] - 2024-02-29

### Fixed

- Fixed a memory leak caused by not always cancelling the context used to enable jobs to be cancelled remotely. [PR #243](https://github.com/riverqueue/river/pull/243).

## [0.0.23] - 2024-02-29

### Added

- `JobListParams.Kinds()` has been added so that jobs can now be listed by kind. [PR #212](https://github.com/riverqueue/river/pull/212).

### Changed

- The underlying driver system's been entirely revamped so that River's non-test code is now decoupled from `pgx/v5`. This will allow additional drivers to be implemented, although there are no additional ones for now. [PR #212](https://github.com/riverqueue/river/pull/212).

### Fixed

- Fixed a memory leak caused by allocating a new random source on every job execution. Thank you @shawnstephens for reporting ❤️ [PR #240](https://github.com/riverqueue/river/pull/240).
- Fix a problem where `JobListParams.Queues()` didn't filter correctly based on its arguments. [PR #212](https://github.com/riverqueue/river/pull/212).
- Fix a problem in `DebouncedChan` where it would fire on its "out" channel too often when it was being signaled continuously on its "in" channel. This would have caused work to be fetched more often than intended in busy systems. [PR #222](https://github.com/riverqueue/river/pull/222).

## [0.0.22] - 2024-02-19

### Fixed

- Brings in another leadership election fix similar to #217 in which a TTL equal to the elector's run interval plus a configured TTL padding is also used for the initial attempt to gain leadership (#217 brought it in for reelection only). [PR #219](https://github.com/riverqueue/river/pull/219).

## [0.0.21] - 2024-02-19

### Changed

- Tweaked behavior of `JobRetry` so that it does actually update the `ScheduledAt` time of the job in all cases where the job is actually being rescheduled. As before, jobs which are already available with a past `ScheduledAt` will not be touched by this query so that they retain their place in line. [PR #211](https://github.com/riverqueue/river/pull/211).

### Fixed

- Fixed a leadership re-election issue that was exposed by the fix in #199. Because we were internally using the same TTL for both an internal timer/ticker and the database update to set the new leader expiration time, a leader wasn't guaranteed to successfully re-elect itself even under normal operation. [PR #217](https://github.com/riverqueue/river/pull/217).

## [0.0.20] - 2024-02-14

### Added

- Added an `ID` setting to the `Client` `Config` type to allow users to override client IDs with their own naming convention. Expose the client ID programmatically (in case it's generated) in a new `Client.ID()` method. [PR #206](https://github.com/riverqueue/river/pull/206).

### Fixed

- Fix a leadership re-election query bug that would cause past leaders to think they were continuing to win elections. [PR #199](https://github.com/riverqueue/river/pull/199).

## [0.0.19] - 2024-02-10

### Added

- Added `JobGet` and `JobGetTx` to the `Client` to enable easily fetching a single job row from code for introspection. [PR #186].
- Added `JobRetry` and `JobRetryTx` to the `Client` to enable a job to be retried immediately, even if it has already completed, been cancelled, or been discarded. [PR #190].

### Changed

- Validate queue name on job insertion. Allow queue names with hyphen separators in addition to underscore. [PR #184](https://github.com/riverqueue/river/pull/184).

## [0.0.18] - 2024-01-25

### Fixed

- Remove a debug statement from periodic job enqueuer that was accidentally left in. [PR #176](https://github.com/riverqueue/river/pull/176).

## [0.0.17] - 2024-01-22

### Added

- Added `JobCancel` and `JobCancelTx` to the `Client` to enable cancellation of jobs. [PR #141](https://github.com/riverqueue/river/pull/141) and [PR #152](https://github.com/riverqueue/river/pull/152).
- Added `ClientFromContext` and `ClientFromContextSafely` helpers to extract the `Client` from the worker's context where it is now available to workers. This simplifies making the River client available within your workers for i.e. enqueueing additional jobs. [PR #145](https://github.com/riverqueue/river/pull/145).
- Add `JobList` API for listing jobs. [PR #117](https://github.com/riverqueue/river/pull/117).
- Added `river validate` command which fails with a non-zero exit code unless all migrations are applied. [PR #170](https://github.com/riverqueue/river/pull/170).

### Changed

- For short `JobSnooze` times (smaller than the scheduler's run interval) put the job straight into an `available` state with the specified `scheduled_at` time. This avoids an artificially long delay waiting for the next scheduler run. [PR #162](https://github.com/riverqueue/river/pull/162).

### Fixed

- Fixed incorrect default value handling for `ScheduledAt` option with `InsertMany` / `InsertManyTx`. [PR #149](https://github.com/riverqueue/river/pull/149).
- Add missing `t.Helper()` calls in `rivertest` internal functions that caused it to report itself as the site of a test failure. [PR #151](https://github.com/riverqueue/river/pull/151).
- Fixed problem where job uniqueness wasn't being respected when used in conjunction with periodic jobs. [PR #168](https://github.com/riverqueue/river/pull/168).

## [0.0.16] - 2024-01-06

### Changed

- Calls to `Stop` error if the client hasn't been started yet. [PR #138](https://github.com/riverqueue/river/pull/138).

### Fixed

- Fix typo in leadership resignation query to ensure faster new leader takeover. [PR #134](https://github.com/riverqueue/river/pull/134).
- Elector now uses the same `log/slog` instance configured by its parent client. [PR #137](https://github.com/riverqueue/river/pull/137).
- Notifier now uses the same `log/slog` instance configured by its parent client. [PR #140](https://github.com/riverqueue/river/pull/140).

## [0.0.15] - 2023-12-21

### Fixed

- Ensure `ScheduledAt` is respected on `InsertManyTx`. [PR #121](https://github.com/riverqueue/river/pull/121).

## [0.0.14] - 2023-12-13

### Fixed

- River CLI `go.sum` entries fixed for 0.0.13 release.

## [0.0.13] - 2023-12-12

### Added

- Added `riverdriver/riverdatabasesql` driver to enable River Go migrations through Go's built in `database/sql` package. [PR #98](https://github.com/riverqueue/river/pull/98).

### Changed

- Errored jobs that have a very short duration before their next retry (<5 seconds) are set to `available` immediately instead of being made `scheduled` and having to wait for the scheduler to make a pass to make them workable. [PR #105](https://github.com/riverqueue/river/pull/105).
- `riverdriver` becomes its own submodule. It contains types that `riverdriver/riverdatabasesql` and `riverdriver/riverpgxv5` need to reference. [PR #98](https://github.com/riverqueue/river/pull/98).
- The `river/cmd/river` CLI has been made its own Go module. This is possible now that it uses the exported `river/rivermigrate` API, and will help with project maintainability. [PR #107](https://github.com/riverqueue/river/pull/107).

## [0.0.12] - 2023-12-02

### Added

- Added `river/rivermigrate` package to enable migrations from Go code as an alternative to using the CLI. PR #67.

## [0.0.11] - 2023-12-02

### Added

- `Stop` and `StopAndCancel` have been changed to respect the provided context argument. When that context is cancelled or times out, those methods will now immediately return with the context's error, even if the Client's shutdown has not yet completed. Apps may need to adjust their graceful shutdown logic to account for this. PR #79.

### Changed

- `NewClient` no longer errors if it was provided a workers bundle with zero workers. Instead, that check's been moved to `Client.Start` instead. This allows adding workers to a bundle that'd like to reference a River client by letting `AddWorker` be invoked after a client reference is available from `NewClient`. PR #87.

## [0.0.10] - 2023-11-26

### Added

- Added `Example_scheduledJob`, demonstrating how to schedule a job to be run in the future.
- Added `Stopped` method to `Client` to make it easier to wait for graceful shutdown to complete.

### Fixed

- Fixed a panic in the periodic job enqueuer caused by sometimes trying to reset a `time.Ticker` with a negative or zero duration. Fixed in PR #73.

### Changed

- `DefaultClientRetryPolicy`: calculate the next attempt based on the current time instead of the time the prior attempt began.

## [0.0.9] - 2023-11-23

### Fixed

- **DATABASE MIGRATION**: Database schema v3 was introduced in v0.0.8 and contained an obvious flaw preventing it from running against existing tables. This migration was altered to execute the migration in multiple steps.

## [0.0.8] - 2023-11-21

### Changed

- License changed from LGPLv3 to MPL-2.0.
- **DATABASE MIGRATION**: Database schema v3, alter river_job tags column to set a default of `[]` and add not null constraint.

## [0.0.7] - 2023-11-20

### Changed

- Constants renamed so that adjectives like `Default` and `Min` become suffixes instead of prefixes. So for example, `DefaultFetchCooldown` becomes `FetchCooldownDefault`.
- Rename `AttemptError.Num` to `AttemptError.Attempt` to better fit with the name of `JobRow.Attempt`.
- Document `JobState`, `AttemptError`, and all fields its fields.
- A `NULL` tags value read from a database job is left as `[]string(nil)` on `JobRow.Tags` rather than a zero-element slice of `[]string{}`. `append` and `len` both work on a `nil` slice, so this should be functionally identical.

## [0.0.6] - 2023-11-19

### Changed

- `JobRow`, `JobState`, and other related types move into `river/rivertype` so they can more easily be shared amongst packages. Most of the River API doesn't change because `JobRow` is embedded on `river.Job`, which doesn't move.

## [0.0.5] - 2023-11-19

### Changed

- Remove `replace` directive from the project's `go.mod` so that it's possible to install River CLI with `@latest`.

## [0.0.4] - 2023-11-17

### Changed

- Allow River clients to be created with a driver with `nil` database pool for use in testing.
- Update River test helpers API to use River drivers like `riverdriver/riverpgxv5` to make them agnostic to the third party database package in use.
- Document `Config.JobTimeout`'s default value.
- Functionally disable the `Reindexer` queue maintenance service. It'd previously only operated on currently unused indexes anyway, indexes probably do _not_ need to be rebuilt except under fairly rare circumstances, and it needs more work to make sure it's shored up against edge cases like indexes that fail to rebuild before a client restart.

## [0.0.3] - 2023-11-13

### Changed

- Fix license detection issues with `riverdriver/riverpgxv5` submodule.
- Ensure that river requires the `riverpgxv5` module with the same version.

## [0.0.2] - 2023-11-13

### Changed

- Pin own `riverpgxv5` dependency to v0.0.1 and make it a direct locally-replaced dependency. This should allow projects to import versioned deps of both river and `riverpgxv5`.

## [0.0.1] - 2023-11-12

### Added

- This is the initial prerelease of River.

```

`LICENSE`:

```
Mozilla Public License Version 2.0
==================================

1. Definitions
--------------

1.1. "Contributor"
    means each individual or legal entity that creates, contributes to
    the creation of, or owns Covered Software.

1.2. "Contributor Version"
    means the combination of the Contributions of others (if any) used
    by a Contributor and that particular Contributor's Contribution.

1.3. "Contribution"
    means Covered Software of a particular Contributor.

1.4. "Covered Software"
    means Source Code Form to which the initial Contributor has attached
    the notice in Exhibit A, the Executable Form of such Source Code
    Form, and Modifications of such Source Code Form, in each case
    including portions thereof.

1.5. "Incompatible With Secondary Licenses"
    means

    (a) that the initial Contributor has attached the notice described
        in Exhibit B to the Covered Software; or

    (b) that the Covered Software was made available under the terms of
        version 1.1 or earlier of the License, but not also under the
        terms of a Secondary License.

1.6. "Executable Form"
    means any form of the work other than Source Code Form.

1.7. "Larger Work"
    means a work that combines Covered Software with other material, in
    a separate file or files, that is not Covered Software.

1.8. "License"
    means this document.

1.9. "Licensable"
    means having the right to grant, to the maximum extent possible,
    whether at the time of the initial grant or subsequently, any and
    all of the rights conveyed by this License.

1.10. "Modifications"
    means any of the following:

    (a) any file in Source Code Form that results from an addition to,
        deletion from, or modification of the contents of Covered
        Software; or

    (b) any new file in Source Code Form that contains any Covered
        Software.

1.11. "Patent Claims" of a Contributor
    means any patent claim(s), including without limitation, method,
    process, and apparatus claims, in any patent Licensable by such
    Contributor that would be infringed, but for the grant of the
    License, by the making, using, selling, offering for sale, having
    made, import, or transfer of either its Contributions or its
    Contributor Version.

1.12. "Secondary License"
    means either the GNU General Public License, Version 2.0, the GNU
    Lesser General Public License, Version 2.1, the GNU Affero General
    Public License, Version 3.0, or any later versions of those
    licenses.

1.13. "Source Code Form"
    means the form of the work preferred for making modifications.

1.14. "You" (or "Your")
    means an individual or a legal entity exercising rights under this
    License. For legal entities, "You" includes any entity that
    controls, is controlled by, or is under common control with You. For
    purposes of this definition, "control" means (a) the power, direct
    or indirect, to cause the direction or management of such entity,
    whether by contract or otherwise, or (b) ownership of more than
    fifty percent (50%) of the outstanding shares or beneficial
    ownership of such entity.

2. License Grants and Conditions
--------------------------------

2.1. Grants

Each Contributor hereby grants You a world-wide, royalty-free,
non-exclusive license:

(a) under intellectual property rights (other than patent or trademark)
    Licensable by such Contributor to use, reproduce, make available,
    modify, display, perform, distribute, and otherwise exploit its
    Contributions, either on an unmodified basis, with Modifications, or
    as part of a Larger Work; and

(b) under Patent Claims of such Contributor to make, use, sell, offer
    for sale, have made, import, and otherwise transfer either its
    Contributions or its Contributor Version.

2.2. Effective Date

The licenses granted in Section 2.1 with respect to any Contribution
become effective for each Contribution on the date the Contributor first
distributes such Contribution.

2.3. Limitations on Grant Scope

The licenses granted in this Section 2 are the only rights granted under
this License. No additional rights or licenses will be implied from the
distribution or licensing of Covered Software under this License.
Notwithstanding Section 2.1(b) above, no patent license is granted by a
Contributor:

(a) for any code that a Contributor has removed from Covered Software;
    or

(b) for infringements caused by: (i) Your and any other third party's
    modifications of Covered Software, or (ii) the combination of its
    Contributions with other software (except as part of its Contributor
    Version); or

(c) under Patent Claims infringed by Covered Software in the absence of
    its Contributions.

This License does not grant any rights in the trademarks, service marks,
or logos of any Contributor (except as may be necessary to comply with
the notice requirements in Section 3.4).

2.4. Subsequent Licenses

No Contributor makes additional grants as a result of Your choice to
distribute the Covered Software under a subsequent version of this
License (see Section 10.2) or under the terms of a Secondary License (if
permitted under the terms of Section 3.3).

2.5. Representation

Each Contributor represents that the Contributor believes its
Contributions are its original creation(s) or it has sufficient rights
to grant the rights to its Contributions conveyed by this License.

2.6. Fair Use

This License is not intended to limit any rights You have under
applicable copyright doctrines of fair use, fair dealing, or other
equivalents.

2.7. Conditions

Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
in Section 2.1.

3. Responsibilities
-------------------

3.1. Distribution of Source Form

All distribution of Covered Software in Source Code Form, including any
Modifications that You create or to which You contribute, must be under
the terms of this License. You must inform recipients that the Source
Code Form of the Covered Software is governed by the terms of this
License, and how they can obtain a copy of this License. You may not
attempt to alter or restrict the recipients' rights in the Source Code
Form.

3.2. Distribution of Executable Form

If You distribute Covered Software in Executable Form then:

(a) such Covered Software must also be made available in Source Code
    Form, as described in Section 3.1, and You must inform recipients of
    the Executable Form how they can obtain a copy of such Source Code
    Form by reasonable means in a timely manner, at a charge no more
    than the cost of distribution to the recipient; and

(b) You may distribute such Executable Form under the terms of this
    License, or sublicense it under different terms, provided that the
    license for the Executable Form does not attempt to limit or alter
    the recipients' rights in the Source Code Form under this License.

3.3. Distribution of a Larger Work

You may create and distribute a Larger Work under terms of Your choice,
provided that You also comply with the requirements of this License for
the Covered Software. If the Larger Work is a combination of Covered
Software with a work governed by one or more Secondary Licenses, and the
Covered Software is not Incompatible With Secondary Licenses, this
License permits You to additionally distribute such Covered Software
under the terms of such Secondary License(s), so that the recipient of
the Larger Work may, at their option, further distribute the Covered
Software under the terms of either this License or such Secondary
License(s).

3.4. Notices

You may not remove or alter the substance of any license notices
(including copyright notices, patent notices, disclaimers of warranty,
or limitations of liability) contained within the Source Code Form of
the Covered Software, except that You may alter any license notices to
the extent required to remedy known factual inaccuracies.

3.5. Application of Additional Terms

You may choose to offer, and to charge a fee for, warranty, support,
indemnity or liability obligations to one or more recipients of Covered
Software. However, You may do so only on Your own behalf, and not on
behalf of any Contributor. You must make it absolutely clear that any
such warranty, support, indemnity, or liability obligation is offered by
You alone, and You hereby agree to indemnify every Contributor for any
liability incurred by such Contributor as a result of warranty, support,
indemnity or liability terms You offer. You may include additional
disclaimers of warranty and limitations of liability specific to any
jurisdiction.

4. Inability to Comply Due to Statute or Regulation
---------------------------------------------------

If it is impossible for You to comply with any of the terms of this
License with respect to some or all of the Covered Software due to
statute, judicial order, or regulation then You must: (a) comply with
the terms of this License to the maximum extent possible; and (b)
describe the limitations and the code they affect. Such description must
be placed in a text file included with all distributions of the Covered
Software under this License. Except to the extent prohibited by statute
or regulation, such description must be sufficiently detailed for a
recipient of ordinary skill to be able to understand it.

5. Termination
--------------

5.1. The rights granted under this License will terminate automatically
if You fail to comply with any of its terms. However, if You become
compliant, then the rights granted under this License from a particular
Contributor are reinstated (a) provisionally, unless and until such
Contributor explicitly and finally terminates Your grants, and (b) on an
ongoing basis, if such Contributor fails to notify You of the
non-compliance by some reasonable means prior to 60 days after You have
come back into compliance. Moreover, Your grants from a particular
Contributor are reinstated on an ongoing basis if such Contributor
notifies You of the non-compliance by some reasonable means, this is the
first time You have received notice of non-compliance with this License
from such Contributor, and You become compliant prior to 30 days after
Your receipt of the notice.

5.2. If You initiate litigation against any entity by asserting a patent
infringement claim (excluding declaratory judgment actions,
counter-claims, and cross-claims) alleging that a Contributor Version
directly or indirectly infringes any patent, then the rights granted to
You by any and all Contributors for the Covered Software under Section
2.1 of this License shall terminate.

5.3. In the event of termination under Sections 5.1 or 5.2 above, all
end user license agreements (excluding distributors and resellers) which
have been validly granted by You or Your distributors under this License
prior to termination shall survive termination.

************************************************************************
*                                                                      *
*  6. Disclaimer of Warranty                                           *
*  -------------------------                                           *
*                                                                      *
*  Covered Software is provided under this License on an "as is"       *
*  basis, without warranty of any kind, either expressed, implied, or  *
*  statutory, including, without limitation, warranties that the       *
*  Covered Software is free of defects, merchantable, fit for a        *
*  particular purpose or non-infringing. The entire risk as to the     *
*  quality and performance of the Covered Software is with You.        *
*  Should any Covered Software prove defective in any respect, You     *
*  (not any Contributor) assume the cost of any necessary servicing,   *
*  repair, or correction. This disclaimer of warranty constitutes an   *
*  essential part of this License. No use of any Covered Software is   *
*  authorized under this License except under this disclaimer.         *
*                                                                      *
************************************************************************

************************************************************************
*                                                                      *
*  7. Limitation of Liability                                          *
*  --------------------------                                          *
*                                                                      *
*  Under no circumstances and under no legal theory, whether tort      *
*  (including negligence), contract, or otherwise, shall any           *
*  Contributor, or anyone who distributes Covered Software as          *
*  permitted above, be liable to You for any direct, indirect,         *
*  special, incidental, or consequential damages of any character      *
*  including, without limitation, damages for lost profits, loss of    *
*  goodwill, work stoppage, computer failure or malfunction, or any    *
*  and all other commercial damages or losses, even if such party      *
*  shall have been informed of the possibility of such damages. This   *
*  limitation of liability shall not apply to liability for death or   *
*  personal injury resulting from such party's negligence to the       *
*  extent applicable law prohibits such limitation. Some               *
*  jurisdictions do not allow the exclusion or limitation of           *
*  incidental or consequential damages, so this exclusion and          *
*  limitation may not apply to You.                                    *
*                                                                      *
************************************************************************

8. Litigation
-------------

Any litigation relating to this License may be brought only in the
courts of a jurisdiction where the defendant maintains its principal
place of business and such litigation shall be governed by laws of that
jurisdiction, without reference to its conflict-of-law provisions.
Nothing in this Section shall prevent a party's ability to bring
cross-claims or counter-claims.

9. Miscellaneous
----------------

This License represents the complete agreement concerning the subject
matter hereof. If any provision of this License is held to be
unenforceable, such provision shall be reformed only to the extent
necessary to make it enforceable. Any law or regulation which provides
that the language of a contract shall be construed against the drafter
shall not be used to construe this License against a Contributor.

10. Versions of the License
---------------------------

10.1. New Versions

Mozilla Foundation is the license steward. Except as provided in Section
10.3, no one other than the license steward has the right to modify or
publish new versions of this License. Each version will be given a
distinguishing version number.

10.2. Effect of New Versions

You may distribute the Covered Software under the terms of the version
of the License under which You originally received the Covered Software,
or under the terms of any subsequent version published by the license
steward.

10.3. Modified Versions

If you create software not governed by this License, and you want to
create a new license for such software, you may create and use a
modified version of this License if you rename the license and remove
any references to the name of the license steward (except to note that
such modified license differs from this License).

10.4. Distributing Source Code Form that is Incompatible With Secondary
Licenses

If You choose to distribute Source Code Form that is Incompatible With
Secondary Licenses under the terms of this version of the License, the
notice described in Exhibit B of this License must be attached.

Exhibit A - Source Code Form License Notice
-------------------------------------------

  This Source Code Form is subject to the terms of the Mozilla Public
  License, v. 2.0. If a copy of the MPL was not distributed with this
  file, You can obtain one at http://mozilla.org/MPL/2.0/.

If it is not possible or desirable to put the notice in a particular
file, then You may include the notice in a location (such as a LICENSE
file in a relevant directory) where a recipient would be likely to look
for such a notice.

You may add additional accurate notices of copyright ownership.

Exhibit B - "Incompatible With Secondary Licenses" Notice
---------------------------------------------------------

  This Source Code Form is "Incompatible With Secondary Licenses", as
  defined by the Mozilla Public License, v. 2.0.


```

`Makefile`:

```
.DEFAULT_GOAL := help

.PHONY: db/reset
db/reset: ## Drop, create, and migrate dev and test databases
db/reset: db/reset/dev
db/reset: db/reset/test

.PHONY: db/reset/dev
db/reset/dev: ## Drop, create, and migrate dev database
	dropdb river_dev --force --if-exists
	createdb river_dev
	cd cmd/river && go run . migrate-up --database-url "postgres://localhost/river_dev"

.PHONY: db/reset/test
db/reset/test: ## Drop, create, and migrate test databases
	go run ./internal/cmd/testdbman reset

.PHONY: generate
generate: ## Generate generated artifacts
generate: generate/migrations
generate: generate/sqlc

.PHONY: generate/migrations
generate/migrations: ## Sync changes of pgxv5 migrations to database/sql
	rsync -au --delete "riverdriver/riverpgxv5/migration/" "riverdriver/riverdatabasesql/migration/"

.PHONY: generate/sqlc
generate/sqlc: ## Generate sqlc
	cd riverdriver/riverdatabasesql/internal/dbsqlc && sqlc generate
	cd riverdriver/riverpgxv5/internal/dbsqlc && sqlc generate

# Looks at comments using ## on targets and uses them to produce a help output.
.PHONY: help
help: ALIGN=22
help: ## Print this message
	@awk -F '::? .*## ' -- "/^[^':]+::? .*## /"' { printf "'$$(tput bold)'%-$(ALIGN)s'$$(tput sgr0)' %s\n", $$1, $$2 }' $(MAKEFILE_LIST)

# Each directory of a submodule in the Go workspace. Go commands provide no
# built-in way to run for all workspace submodules. Add a new submodule to the
# workspace with `go work use ./driver/new`.
submodules := $(shell go list -f '{{.Dir}}' -m)

# Definitions of following tasks look ugly, but they're done this way because to
# produce the best/most comprehensible output by far (e.g. compared to a shell
# loop).
.PHONY: lint
lint:: ## Run linter (golangci-lint) for all submodules
define lint-target
    lint:: ; cd $1 && golangci-lint run --fix
endef
$(foreach mod,$(submodules),$(eval $(call lint-target,$(mod))))

.PHONY: test
test:: ## Run test suite for all submodules
define test-target
    test:: ; cd $1 && go test ./... -p 1
endef
$(foreach mod,$(submodules),$(eval $(call test-target,$(mod))))

.PHONY: test/race
test/race:: ## Run test suite for all submodules with race detector
define test-race-target
    test/race:: ; cd $1 && go test ./... -p 1 -race
endef
$(foreach mod,$(submodules),$(eval $(call test-race-target,$(mod))))

.PHONY: tidy
tidy:: ## Run `go mod tidy` for all submodules
define tidy-target
    tidy:: ; cd $1 && go mod tidy
endef
$(foreach mod,$(submodules),$(eval $(call tidy-target,$(mod))))

.PHONY: update-mod-go
update-mod-go: ## Update `go`/`toolchain` directives in all submodules to match `go.work`
	go run ./rivershared/cmd/update-mod-go ./go.work

.PHONY: update-mod-version
update-mod-version: ## Update River packages in all submodules to $VERSION
	PACKAGE_PREFIX="github.com/riverqueue/river" go run ./rivershared/cmd/update-mod-version ./go.work

.PHONY: verify
verify: ## Verify generated artifacts
verify: verify/migrations
verify: verify/sqlc

.PHONY: verify/migrations
verify/migrations: ## Verify synced migrations
	diff -qr riverdriver/riverpgxv5/migration riverdriver/riverdatabasesql/migration

.PHONY: verify/sqlc
verify/sqlc: ## Verify generated sqlc
	cd riverdriver/riverdatabasesql/internal/dbsqlc && sqlc diff
	cd riverdriver/riverpgxv5/internal/dbsqlc && sqlc diff

```

`client.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"log/slog"
	"os"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/riverqueue/river/internal/dblist"
	"github.com/riverqueue/river/internal/dbunique"
	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/leadership"
	"github.com/riverqueue/river/internal/maintenance"
	"github.com/riverqueue/river/internal/middlewarelookup"
	"github.com/riverqueue/river/internal/notifier"
	"github.com/riverqueue/river/internal/notifylimiter"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riverpilot"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/maputil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
	"github.com/riverqueue/river/rivertype"
)

const (
	FetchCooldownDefault = 100 * time.Millisecond
	FetchCooldownMin     = 1 * time.Millisecond

	FetchPollIntervalDefault = 1 * time.Second
	FetchPollIntervalMin     = 1 * time.Millisecond

	JobTimeoutDefault  = 1 * time.Minute
	MaxAttemptsDefault = rivercommon.MaxAttemptsDefault
	PriorityDefault    = rivercommon.PriorityDefault
	QueueDefault       = rivercommon.QueueDefault
	QueueNumWorkersMax = 10_000
)

// TestConfig contains configuration specific to test environments.
type TestConfig struct {
	// DisableUniqueEnforcement disables the application of unique job
	// constraints. This is useful for testing scenarios when testing a worker
	// that typically uses uniqueness, but where enforcing uniqueness would cause
	// conflicts with parallel test execution.
	//
	// The [rivertest.Worker] type automatically disables uniqueness enforcement
	// when creating jobs.
	DisableUniqueEnforcement bool

	// Time is a time generator to make time stubbable in tests.
	Time rivertype.TimeGenerator
}

// Config is the configuration for a Client.
//
// Both Queues and Workers are required for a client to work jobs, but an
// insert-only client can be initialized by omitting Queues, and not calling
// Start for the client. Workers can also be omitted, but it's better to include
// it so River can check that inserted job kinds have a worker that can run
// them.
type Config struct {
	// AdvisoryLockPrefix is a configurable 32-bit prefix that River will use
	// when generating any key to acquire a Postgres advisory lock. All advisory
	// locks share the same 64-bit number space, so this allows a calling
	// application to guarantee that a River advisory lock will never conflict
	// with one of its own by cordoning each type to its own prefix.
	//
	// If this value isn't set, River defaults to generating key hashes across
	// the entire 64-bit advisory lock number space, which is large enough that
	// conflicts are exceedingly unlikely. If callers don't strictly need this
	// option then it's recommended to leave it unset because the prefix leaves
	// only 32 bits of number space for advisory lock hashes, so it makes
	// internally conflicting River-generated keys more likely.
	//
	// Advisory locks are currently only used for the deprecated fallback/slow
	// path of unique job insertion when pending, scheduled, available, or running
	// are omitted from a customized ByState configuration.
	AdvisoryLockPrefix int32

	// CancelledJobRetentionPeriod is the amount of time to keep cancelled jobs
	// around before they're removed permanently.
	//
	// Defaults to 24 hours.
	CancelledJobRetentionPeriod time.Duration

	// CompletedJobRetentionPeriod is the amount of time to keep completed jobs
	// around before they're removed permanently.
	//
	// Defaults to 24 hours.
	CompletedJobRetentionPeriod time.Duration

	// DiscardedJobRetentionPeriod is the amount of time to keep discarded jobs
	// around before they're removed permanently.
	//
	// Defaults to 7 days.
	DiscardedJobRetentionPeriod time.Duration

	// ErrorHandler can be configured to be invoked in case of an error or panic
	// occurring in a job. This is often useful for logging and exception
	// tracking, but can also be used to customize retry behavior.
	ErrorHandler ErrorHandler

	// FetchCooldown is the minimum amount of time to wait between fetches of new
	// jobs. Jobs will only be fetched *at most* this often, but if no new jobs
	// are coming in via LISTEN/NOTIFY then fetches may be delayed as long as
	// FetchPollInterval.
	//
	// Throughput is limited by this value.
	//
	// Defaults to 100 ms.
	FetchCooldown time.Duration

	// FetchPollInterval is the amount of time between periodic fetches for new
	// jobs. Typically new jobs will be picked up ~immediately after insert via
	// LISTEN/NOTIFY, but this provides a fallback.
	//
	// Defaults to 1 second.
	FetchPollInterval time.Duration

	// ID is the unique identifier for this client. If not set, a random
	// identifier will be generated.
	//
	// This is used to identify the client in job attempts and for leader election.
	// This value must be unique across all clients in the same database and
	// schema and there must not be more than one process running with the same
	// ID at the same time.
	//
	// A client ID should differ between different programs and must be unique
	// across all clients in the same database and schema. There must not be
	// more than one process running with the same ID at the same time.
	// Duplicate IDs between processes will lead to facilities like leader
	// election or client statistics to fail in novel ways. However, the client
	// ID is shared by all executors within any given client. (i.e.  different
	// Go processes have different IDs, but IDs are shared within any given
	// process.)
	//
	// If in doubt, leave this property empty.
	ID string

	// JobCleanerTimeout is the timeout of the individual queries within the job
	// cleaner.
	//
	// Defaults to 30 seconds, which should be more than enough time for most
	// deployments.
	JobCleanerTimeout time.Duration

	// JobInsertMiddleware are optional functions that can be called around job
	// insertion.
	//
	// Deprecated: Prefer the use of Middleware instead (which may contain
	// instances of rivertype.JobInsertMiddleware).
	JobInsertMiddleware []rivertype.JobInsertMiddleware

	// JobTimeout is the maximum amount of time a job is allowed to run before its
	// context is cancelled. A timeout of zero means JobTimeoutDefault will be
	// used, whereas a value of -1 means the job's context will not be cancelled
	// unless the Client is shutting down.
	//
	// Defaults to 1 minute.
	JobTimeout time.Duration

	// Hooks are functions that may activate at certain points during a job's
	// lifecycle (see rivertype.Hook), installed globally.
	//
	// The effect of hooks in this list will depend on the specific hook
	// interfaces they implement, so for example implementing
	// rivertype.HookInsertBegin will cause the hook to be invoked before a job
	// is inserted, or implementing rivertype.HookWorkBegin will cause it to be
	// invoked before a job is worked. Hook structs may implement multiple hook
	// interfaces.
	//
	// Order in this list is significant. A hook that appears first will be
	// entered before a hook that appears later. For any particular phase, order
	// is relevant only for hooks that will run for that phase. For example, if
	// two rivertype.HookInsertBegin are separated by a rivertype.HookWorkBegin,
	// during job insertion those two outer hooks will run one after another,
	// and the work hook between them will not run. When a job is worked, the
	// work hook runs and the insertion hooks on either side of it are skipped.
	//
	// Jobs may have their own specific hooks by implementing JobArgsWithHooks.
	Hooks []rivertype.Hook

	// Logger is the structured logger to use for logging purposes. If none is
	// specified, logs will be emitted to STDOUT with messages at warn level
	// or higher.
	Logger *slog.Logger

	// MaxAttempts is the default number of times a job will be retried before
	// being discarded. This value is applied to all jobs by default, and can be
	// overridden on individual job types on the JobArgs or on a per-job basis at
	// insertion time.
	//
	// If not specified, defaults to 25 (MaxAttemptsDefault).
	MaxAttempts int

	// Middleware contains middleware that may activate at certain points during
	// a job's lifecycle (see rivertype.Middleware), installed globally.
	//
	// The effect of middleware in this list will depend on the specific
	// middleware interfaces they implement, so for example implementing
	// rivertype.JobInsertMiddleware will cause the middleware to be invoked
	// when jobs are inserted, and implementing rivertype.WorkerMiddleware will
	// cause it to be invoked when a job is worked. Middleware structs may
	// implement multiple middleware interfaces.
	//
	// Order in this list is significant. Middleware that appears first will be
	// entered before middleware that appears later. For any particular phase,
	// order is relevant only for middlewares that will run for that phase. For
	// example, if two rivertype.JobInsertMiddleware are separated by a
	// rivertype.WorkerMiddleware, during job insertion those two outer
	// middlewares will run one after another, and the work middleware between
	// them will not run. When a job is worked, the work middleware runs and the
	// insertion middlewares on either side of it are skipped.
	Middleware []rivertype.Middleware

	// PeriodicJobs are a set of periodic jobs to run at the specified intervals
	// in the client.
	PeriodicJobs []*PeriodicJob

	// PollOnly starts the client in "poll only" mode, which avoids issuing
	// `LISTEN` statements to wait for events like a leadership resignation or
	// new job available. The program instead polls periodically to look for
	// changes (checking for new jobs on the period in FetchPollInterval).
	//
	// The downside of this mode of operation is that events will usually be
	// noticed less quickly. A new job in the queue may have to wait up to
	// FetchPollInterval to be locked for work. When a leader resigns, it will
	// be up to five seconds before a new one elects itself.
	//
	// The upside is that it makes River compatible with systems where
	// listen/notify isn't available. For example, PgBouncer in transaction
	// pooling mode.
	PollOnly bool

	// Queues is a list of queue names for this client to operate on along with
	// configuration for the queue like the maximum number of workers to run for
	// each queue.
	//
	// This field may be omitted for a program that's only queueing jobs rather
	// than working them. If it's specified, then Workers must also be given.
	Queues map[string]QueueConfig

	// ReindexerSchedule is the schedule for running the reindexer. If nil, the
	// reindexer will run at midnight UTC every day.
	ReindexerSchedule PeriodicSchedule

	// RescueStuckJobsAfter is the amount of time a job can be running before it
	// is considered stuck. A stuck job which has not yet reached its max attempts
	// will be scheduled for a retry, while one which has exhausted its attempts
	// will be discarded.  This prevents jobs from being stuck forever if a worker
	// crashes or is killed.
	//
	// Note that this can result in repeat or duplicate execution of a job that is
	// not actually stuck but is still working. The value should be set higher
	// than the maximum duration you expect your jobs to run. Setting a value too
	// low will result in more duplicate executions, whereas too high of a value
	// will result in jobs being stuck for longer than necessary before they are
	// retried.
	//
	// RescueStuckJobsAfter must be greater than JobTimeout. Otherwise, jobs
	// would become eligible for rescue while they're still running.
	//
	// Defaults to 1 hour, or in cases where JobTimeout has been configured and
	// is greater than 1 hour, JobTimeout + 1 hour.
	RescueStuckJobsAfter time.Duration

	// RetryPolicy is a configurable retry policy for the client.
	//
	// Defaults to DefaultRetryPolicy.
	RetryPolicy ClientRetryPolicy

	// SkipUnknownJobCheck is a flag to control whether the client should skip
	// checking to see if a registered worker exists in the client's worker bundle
	// for a job arg prior to insertion.
	//
	// This can be set to true to allow a client to insert jobs which are
	// intended to be worked by a different client which effectively makes
	// the client's insertion behavior mimic that of an insert-only client.
	//
	// Defaults to false.
	SkipUnknownJobCheck bool

	// Test holds configuration specific to test environments.
	Test TestConfig

	// TestOnly can be set to true to disable certain features that are useful
	// in production, but which may be harmful to tests, in ways like having the
	// effect of making them slower. It should not be used outside of test
	// suites.
	//
	// For example, queue maintenance services normally stagger their startup
	// with a random jittered sleep so they don't all try to work at the same
	// time. This is nice in production, but makes starting and stopping the
	// client in a test case slower.
	TestOnly bool

	// Workers is a bundle of registered job workers.
	//
	// This field may be omitted for a program that's only enqueueing jobs
	// rather than working them, but if it is configured the client can validate
	// ahead of time that a worker is properly registered for an inserted job.
	// (i.e.  That it wasn't forgotten by accident.)
	Workers *Workers

	// WorkerMiddleware are optional functions that can be called around
	// all job executions.
	//
	// Deprecated: Prefer the use of Middleware instead (which may contain
	// instances of rivertype.WorkerMiddleware).
	WorkerMiddleware []rivertype.WorkerMiddleware

	// Scheduler run interval. Shared between the scheduler and producer/job
	// executors, but not currently exposed for configuration.
	schedulerInterval time.Duration
}

// WithDefaults returns a copy of the Config with all default values applied.
func (c *Config) WithDefaults() *Config {
	if c == nil {
		c = &Config{}
	}

	// Use the existing logger if set, otherwise create a default one.
	logger := c.Logger
	if logger == nil {
		logger = slog.New(slog.NewTextHandler(os.Stdout, &slog.HandlerOptions{
			Level: slog.LevelWarn,
		}))
	}

	// Compute the default rescue value. For convenience, if JobTimeout is specified
	// but RescueStuckJobsAfter is not set (or less than 1) and JobTimeout is large, use
	// JobTimeout + maintenance.JobRescuerRescueAfterDefault as the default.
	rescueAfter := maintenance.JobRescuerRescueAfterDefault
	if c.JobTimeout > 0 && c.RescueStuckJobsAfter < 1 && c.JobTimeout > c.RescueStuckJobsAfter {
		rescueAfter = c.JobTimeout + maintenance.JobRescuerRescueAfterDefault
	}

	// Set default retry policy if none is provided.
	retryPolicy := c.RetryPolicy
	if retryPolicy == nil {
		retryPolicy = &DefaultClientRetryPolicy{}
	}

	return &Config{
		AdvisoryLockPrefix:          c.AdvisoryLockPrefix,
		CancelledJobRetentionPeriod: valutil.ValOrDefault(c.CancelledJobRetentionPeriod, maintenance.CancelledJobRetentionPeriodDefault),
		CompletedJobRetentionPeriod: valutil.ValOrDefault(c.CompletedJobRetentionPeriod, maintenance.CompletedJobRetentionPeriodDefault),
		DiscardedJobRetentionPeriod: valutil.ValOrDefault(c.DiscardedJobRetentionPeriod, maintenance.DiscardedJobRetentionPeriodDefault),
		ErrorHandler:                c.ErrorHandler,
		FetchCooldown:               valutil.ValOrDefault(c.FetchCooldown, FetchCooldownDefault),
		FetchPollInterval:           valutil.ValOrDefault(c.FetchPollInterval, FetchPollIntervalDefault),
		ID:                          valutil.ValOrDefaultFunc(c.ID, func() string { return defaultClientID(time.Now().UTC()) }),
		Hooks:                       c.Hooks,
		JobInsertMiddleware:         c.JobInsertMiddleware,
		JobTimeout:                  valutil.ValOrDefault(c.JobTimeout, JobTimeoutDefault),
		Logger:                      logger,
		MaxAttempts:                 valutil.ValOrDefault(c.MaxAttempts, MaxAttemptsDefault),
		Middleware:                  c.Middleware,
		PeriodicJobs:                c.PeriodicJobs,
		PollOnly:                    c.PollOnly,
		Queues:                      c.Queues,
		ReindexerSchedule:           c.ReindexerSchedule,
		RescueStuckJobsAfter:        valutil.ValOrDefault(c.RescueStuckJobsAfter, rescueAfter),
		RetryPolicy:                 retryPolicy,
		SkipUnknownJobCheck:         c.SkipUnknownJobCheck,
		Test:                        c.Test,
		TestOnly:                    c.TestOnly,
		WorkerMiddleware:            c.WorkerMiddleware,
		Workers:                     c.Workers,
		schedulerInterval:           valutil.ValOrDefault(c.schedulerInterval, maintenance.JobSchedulerIntervalDefault),
	}
}

func (c *Config) validate() error {
	if c.CancelledJobRetentionPeriod < 0 {
		return errors.New("CancelledJobRetentionPeriod time cannot be less than zero")
	}
	if c.CompletedJobRetentionPeriod < 0 {
		return errors.New("CompletedJobRetentionPeriod cannot be less than zero")
	}
	if c.DiscardedJobRetentionPeriod < 0 {
		return errors.New("DiscardedJobRetentionPeriod cannot be less than zero")
	}
	if c.FetchCooldown < FetchCooldownMin {
		return fmt.Errorf("FetchCooldown must be at least %s", FetchCooldownMin)
	}
	if c.FetchPollInterval < FetchPollIntervalMin {
		return fmt.Errorf("FetchPollInterval must be at least %s", FetchPollIntervalMin)
	}
	if c.FetchPollInterval < c.FetchCooldown {
		return fmt.Errorf("FetchPollInterval cannot be shorter than FetchCooldown (%s)", c.FetchCooldown)
	}
	if len(c.ID) > 100 {
		return errors.New("ID cannot be longer than 100 characters")
	}
	if c.JobTimeout < -1 {
		return errors.New("JobTimeout cannot be negative, except for -1 (infinite)")
	}
	if c.MaxAttempts < 0 {
		return errors.New("MaxAttempts cannot be less than zero")
	}
	if len(c.Middleware) > 0 && (len(c.JobInsertMiddleware) > 0 || len(c.WorkerMiddleware) > 0) {
		return errors.New("only one of the pair JobInsertMiddleware/WorkerMiddleware or Middleware may be provided (Middleware is recommended, and may contain both job insert and worker middleware)")
	}
	if c.RescueStuckJobsAfter < 0 {
		return errors.New("RescueStuckJobsAfter cannot be less than zero")
	}
	if c.RescueStuckJobsAfter < c.JobTimeout {
		return errors.New("RescueStuckJobsAfter cannot be less than JobTimeout")
	}

	for queue, queueConfig := range c.Queues {
		if err := queueConfig.validate(queue); err != nil {
			return err
		}
	}

	if c.Workers == nil && c.Queues != nil {
		return errors.New("Workers must be set if Queues is set")
	}

	return nil
}

// Indicates whether with the given configuration, this client will be expected
// to execute jobs (rather than just being used to enqueue them). Executing jobs
// requires a set of configured queues.
func (c *Config) willExecuteJobs() bool {
	return len(c.Queues) > 0
}

// QueueConfig contains queue-specific configuration.
type QueueConfig struct {
	// MaxWorkers is the maximum number of workers to run for the queue, or put
	// otherwise, the maximum parallelism to run.
	//
	// This is the maximum number of workers within this particular client
	// instance, but note that it doesn't control the total number of workers
	// across parallel processes. Installations will want to calculate their
	// total number by multiplying this number by the number of parallel nodes
	// running River clients configured to the same database and queue.
	//
	// Requires a minimum of 1, and a maximum of 10,000.
	MaxWorkers int
}

func (c QueueConfig) validate(queueName string) error {
	if c.MaxWorkers < 1 || c.MaxWorkers > QueueNumWorkersMax {
		return fmt.Errorf("invalid number of workers for queue %q: %d", queueName, c.MaxWorkers)
	}
	if err := validateQueueName(queueName); err != nil {
		return err
	}

	return nil
}

// Client is a single isolated instance of River. Your application may use
// multiple instances operating on different databases or Postgres schemas
// within a single database.
type Client[TTx any] struct {
	// BaseService and BaseStartStop can't be embedded like on other services
	// because their properties would leak to the external API.
	baseService   baseservice.BaseService
	baseStartStop startstop.BaseStartStop

	completer              jobcompleter.JobCompleter
	config                 *Config
	driver                 riverdriver.Driver[TTx]
	elector                *leadership.Elector
	hookLookupByJob        *hooklookup.JobHookLookup
	hookLookupGlobal       hooklookup.HookLookupInterface
	insertNotifyLimiter    *notifylimiter.Limiter
	middlewareLookupGlobal middlewarelookup.MiddlewareLookupInterface
	notifier               *notifier.Notifier // may be nil in poll-only mode
	periodicJobs           *PeriodicJobBundle
	pilot                  riverpilot.Pilot
	producersByQueueName   map[string]*producer
	queueMaintainer        *maintenance.QueueMaintainer
	queues                 *QueueBundle
	services               []startstop.Service
	stopped                <-chan struct{}
	subscriptionManager    *subscriptionManager
	testSignals            clientTestSignals

	// workCancel cancels the context used for all work goroutines. Normal Stop
	// does not cancel that context.
	workCancel context.CancelCauseFunc
}

// Test-only signals.
type clientTestSignals struct {
	electedLeader testsignal.TestSignal[struct{}] // notifies when elected leader

	jobCleaner          *maintenance.JobCleanerTestSignals
	jobRescuer          *maintenance.JobRescuerTestSignals
	jobScheduler        *maintenance.JobSchedulerTestSignals
	periodicJobEnqueuer *maintenance.PeriodicJobEnqueuerTestSignals
	queueCleaner        *maintenance.QueueCleanerTestSignals
	reindexer           *maintenance.ReindexerTestSignals
}

func (ts *clientTestSignals) Init() {
	ts.electedLeader.Init()

	if ts.jobCleaner != nil {
		ts.jobCleaner.Init()
	}
	if ts.jobRescuer != nil {
		ts.jobRescuer.Init()
	}
	if ts.jobScheduler != nil {
		ts.jobScheduler.Init()
	}
	if ts.periodicJobEnqueuer != nil {
		ts.periodicJobEnqueuer.Init()
	}
	if ts.queueCleaner != nil {
		ts.queueCleaner.Init()
	}
	if ts.reindexer != nil {
		ts.reindexer.Init()
	}
}

var (
	// ErrNotFound is returned when a query by ID does not match any existing
	// rows. For example, attempting to cancel a job that doesn't exist will
	// return this error.
	ErrNotFound = rivertype.ErrNotFound

	errMissingConfig                 = errors.New("missing config")
	errMissingDatabasePoolWithQueues = errors.New("must have a non-nil database pool to execute jobs (either use a driver with database pool or don't configure Queues)")
	errMissingDriver                 = errors.New("missing database driver (try wrapping a Pgx pool with river/riverdriver/riverpgxv5.New)")
)

// NewClient creates a new Client with the given database driver and
// configuration.
//
// Currently only one driver is supported, which is Pgx v5. See package
// riverpgxv5.
//
// The function takes a generic parameter TTx representing a transaction type,
// but it can be omitted because it'll generally always be inferred from the
// driver. For example:
//
//	import "github.com/riverqueue/river"
//	import "github.com/riverqueue/river/riverdriver/riverpgxv5"
//
//	...
//
//	dbPool, err := pgxpool.New(ctx, os.Getenv("DATABASE_URL"))
//	if err != nil {
//		// handle error
//	}
//	defer dbPool.Close()
//
//	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
//		...
//	})
//	if err != nil {
//		// handle error
//	}
func NewClient[TTx any](driver riverdriver.Driver[TTx], config *Config) (*Client[TTx], error) {
	if driver == nil {
		return nil, errMissingDriver
	}
	if config == nil {
		return nil, errMissingConfig
	}

	config = config.WithDefaults()

	if err := config.validate(); err != nil {
		return nil, err
	}

	archetype := baseservice.NewArchetype(config.Logger)
	if config.Test.Time != nil {
		if withStub, ok := config.Test.Time.(baseservice.TimeGeneratorWithStub); ok {
			archetype.Time = withStub
		} else {
			archetype.Time = &baseservice.TimeGeneratorWithStubWrapper{TimeGenerator: config.Test.Time}
		}
	}

	client := &Client[TTx]{
		config:               config,
		driver:               driver,
		hookLookupByJob:      hooklookup.NewJobHookLookup(),
		hookLookupGlobal:     hooklookup.NewHookLookup(config.Hooks),
		producersByQueueName: make(map[string]*producer),
		testSignals:          clientTestSignals{},
		workCancel:           func(cause error) {}, // replaced on start, but here in case StopAndCancel is called before start up
	}
	client.queues = &QueueBundle{addProducer: client.addProducer}

	baseservice.Init(archetype, &client.baseService)
	client.baseService.Name = "Client" // Have to correct the name because base service isn't embedded like it usually is
	client.insertNotifyLimiter = notifylimiter.NewLimiter(archetype, config.FetchCooldown)

	// Validation ensures that config.JobInsertMiddleware/WorkerMiddleware or
	// the more abstract config.Middleware for middleware are set, but not both,
	// so in practice we never append all three of these to each other.
	{
		middleware := config.Middleware
		for _, jobInsertMiddleware := range config.JobInsertMiddleware {
			middleware = append(middleware, jobInsertMiddleware)
		}
	outerLoop:
		for _, workerMiddleware := range config.WorkerMiddleware {
			// Don't add the middleware if it also implements JobInsertMiddleware
			// and the instance has been added to config.JobInsertMiddleware. This
			// is a hedge to make sure we don't accidentally double add middleware
			// as we've converted over to the unified config.Middleware setting.
			if workerMiddlewareAsJobInsertMiddleware, ok := workerMiddleware.(rivertype.JobInsertMiddleware); ok {
				for _, jobInsertMiddleware := range config.JobInsertMiddleware {
					if workerMiddlewareAsJobInsertMiddleware == jobInsertMiddleware {
						continue outerLoop
					}
				}
			}

			middleware = append(middleware, workerMiddleware)
		}
		client.middlewareLookupGlobal = middlewarelookup.NewMiddlewareLookup(middleware)
	}

	pluginDriver, _ := driver.(driverPlugin[TTx])
	if pluginDriver != nil {
		pluginDriver.PluginInit(archetype)
		client.pilot = pluginDriver.PluginPilot()
	}
	if client.pilot == nil {
		client.pilot = &riverpilot.StandardPilot{}
	}
	client.pilot.PilotInit(archetype)
	pluginPilot, _ := client.pilot.(pilotPlugin)

	// There are a number of internal components that are only needed/desired if
	// we're actually going to be working jobs (as opposed to just enqueueing
	// them):
	if config.willExecuteJobs() {
		if !driver.HasPool() {
			return nil, errMissingDatabasePoolWithQueues
		}

		client.completer = jobcompleter.NewBatchCompleter(archetype, driver.GetExecutor(), client.pilot, nil)
		client.subscriptionManager = newSubscriptionManager(archetype, nil)
		client.services = append(client.services, client.completer, client.subscriptionManager)

		if driver.SupportsListener() {
			// In poll only mode, we don't try to initialize a notifier that
			// uses listen/notify. Instead, each service polls for changes it's
			// interested in. e.g. Elector polls to see if leader has expired.
			if !config.PollOnly {
				client.notifier = notifier.New(archetype, driver.GetListener())
				client.services = append(client.services, client.notifier)
			}
		} else {
			config.Logger.Info("Driver does not support listener; entering poll only mode")
		}

		client.elector = leadership.NewElector(archetype, driver.GetExecutor(), client.notifier, &leadership.Config{
			ClientID: config.ID,
		})
		client.services = append(client.services, client.elector)

		for queue, queueConfig := range config.Queues {
			client.addProducer(queue, queueConfig)
		}

		client.services = append(client.services,
			startstop.StartStopFunc(client.logStatsLoop))

		client.services = append(client.services,
			startstop.StartStopFunc(client.handleLeadershipChangeLoop))

		if pluginPilot != nil {
			client.services = append(client.services, pluginPilot.PluginServices()...)
		}

		//
		// Maintenance services
		//

		maintenanceServices := []startstop.Service{}

		{
			jobCleaner := maintenance.NewJobCleaner(archetype, &maintenance.JobCleanerConfig{
				CancelledJobRetentionPeriod: config.CancelledJobRetentionPeriod,
				CompletedJobRetentionPeriod: config.CompletedJobRetentionPeriod,
				DiscardedJobRetentionPeriod: config.DiscardedJobRetentionPeriod,
				Timeout:                     config.JobCleanerTimeout,
			}, driver.GetExecutor())
			maintenanceServices = append(maintenanceServices, jobCleaner)
			client.testSignals.jobCleaner = &jobCleaner.TestSignals
		}

		{
			jobRescuer := maintenance.NewRescuer(archetype, &maintenance.JobRescuerConfig{
				ClientRetryPolicy: config.RetryPolicy,
				RescueAfter:       config.RescueStuckJobsAfter,
				WorkUnitFactoryFunc: func(kind string) workunit.WorkUnitFactory {
					if workerInfo, ok := config.Workers.workersMap[kind]; ok {
						return workerInfo.workUnitFactory
					}
					return nil
				},
			}, driver.GetExecutor())
			maintenanceServices = append(maintenanceServices, jobRescuer)
			client.testSignals.jobRescuer = &jobRescuer.TestSignals
		}

		{
			jobScheduler := maintenance.NewJobScheduler(archetype, &maintenance.JobSchedulerConfig{
				Interval:     config.schedulerInterval,
				NotifyInsert: client.maybeNotifyInsertForQueues,
			}, driver.GetExecutor())
			maintenanceServices = append(maintenanceServices, jobScheduler)
			client.testSignals.jobScheduler = &jobScheduler.TestSignals
		}

		{
			periodicJobEnqueuer := maintenance.NewPeriodicJobEnqueuer(archetype, &maintenance.PeriodicJobEnqueuerConfig{
				AdvisoryLockPrefix: config.AdvisoryLockPrefix,
				Insert:             client.insertMany,
			}, driver.GetExecutor())
			maintenanceServices = append(maintenanceServices, periodicJobEnqueuer)
			client.testSignals.periodicJobEnqueuer = &periodicJobEnqueuer.TestSignals

			client.periodicJobs = newPeriodicJobBundle(client.config, periodicJobEnqueuer)
			client.periodicJobs.AddMany(config.PeriodicJobs)
		}

		{
			queueCleaner := maintenance.NewQueueCleaner(archetype, &maintenance.QueueCleanerConfig{
				RetentionPeriod: maintenance.QueueRetentionPeriodDefault,
			}, driver.GetExecutor())
			maintenanceServices = append(maintenanceServices, queueCleaner)
			client.testSignals.queueCleaner = &queueCleaner.TestSignals
		}

		{
			var scheduleFunc func(time.Time) time.Time
			if config.ReindexerSchedule != nil {
				scheduleFunc = config.ReindexerSchedule.Next
			}

			reindexer := maintenance.NewReindexer(archetype, &maintenance.ReindexerConfig{ScheduleFunc: scheduleFunc}, driver.GetExecutor())
			maintenanceServices = append(maintenanceServices, reindexer)
			client.testSignals.reindexer = &reindexer.TestSignals
		}

		if pluginPilot != nil {
			maintenanceServices = append(maintenanceServices, pluginPilot.PluginMaintenanceServices()...)
		}

		// Not added to the main services list because the queue maintainer is
		// started conditionally based on whether the client is the leader.
		client.queueMaintainer = maintenance.NewQueueMaintainer(archetype, maintenanceServices)

		if config.TestOnly {
			client.queueMaintainer.StaggerStartupDisable(true)
		}
	}

	return client, nil
}

// Start starts the client's job fetching and working loops. Once this is called,
// the client will run in a background goroutine until stopped. All jobs are
// run with a context inheriting from the provided context, but with a timeout
// deadline applied based on the job's settings.
//
// A graceful shutdown stops fetching new jobs but allows any previously fetched
// jobs to complete. This can be initiated with the Stop method.
//
// A more abrupt shutdown can be achieved by either cancelling the provided
// context or by calling StopAndCancel. This will not only stop fetching new
// jobs, but will also cancel the context for any currently-running jobs. If
// using StopAndCancel, there's no need to also call Stop.
func (c *Client[TTx]) Start(ctx context.Context) error {
	fetchCtx, shouldStart, started, stopped := c.baseStartStop.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	c.queues.startStopMu.Lock()
	defer c.queues.startStopMu.Unlock()

	// BaseStartStop will set its stopped channel to nil after it stops, so make
	// sure to take a channel reference before finishing stopped.
	c.stopped = c.baseStartStop.StoppedUnsafe()

	producersAsServices := func() []startstop.Service {
		return sliceutil.Map(
			maputil.Values(c.producersByQueueName),
			func(p *producer) startstop.Service { return p },
		)
	}

	// Startup code. Wrapped in a closure so it doesn't have to remember to
	// close the stopped channel if returning with an error.
	if err := func() error {
		if !c.config.willExecuteJobs() {
			return errors.New("client Queues and Workers must be configured for a client to start working")
		}
		if c.config.Workers != nil && len(c.config.Workers.workersMap) < 1 {
			return errors.New("at least one Worker must be added to the Workers bundle")
		}

		// Before doing anything else, make an initial connection to the database to
		// verify that it appears healthy. Many of the subcomponents below start up
		// in a goroutine and in case of initial failure, only produce a log line,
		// so even in the case of a fundamental failure like the database not being
		// available, the client appears to have started even though it's completely
		// non-functional. Here we try to make an initial assessment of health and
		// return quickly in case of an apparent problem.
		_, err := c.driver.GetExecutor().Exec(fetchCtx, "SELECT 1")
		if err != nil {
			return fmt.Errorf("error making initial connection to database: %w", err)
		}

		// Each time we start, we need a fresh completer subscribe channel to
		// send job completion events on, because the completer will close it
		// each time it shuts down.
		completerSubscribeCh := make(chan []jobcompleter.CompleterJobUpdated, 10)
		c.completer.ResetSubscribeChan(completerSubscribeCh)
		c.subscriptionManager.ResetSubscribeChan(completerSubscribeCh)

		// In case of error, stop any services that might have started. This
		// is safe because even services that were never started will still
		// tolerate being stopped.
		stopServicesOnError := func() {
			startstop.StopAllParallel(c.services...)
		}

		// The completer is part of the services list below, but although it can
		// stop gracefully along with all the other services, it needs to be
		// started with a context that's _not_ cancelled if the user-provided
		// context is cancelled.  This ensures that even when fetch is cancelled on
		// shutdown, the completer is still given a separate opportunity to start
		// stopping only after the producers have finished up and returned.
		if err := c.completer.Start(context.WithoutCancel(ctx)); err != nil {
			stopServicesOnError()
			return err
		}

		// We use separate contexts for fetching and working to allow for a graceful
		// stop. Both inherit from the provided context, so if it's cancelled, a
		// more aggressive stop will be initiated.
		workCtx, workCancel := context.WithCancelCause(withClient[TTx](ctx, c))

		if err := startstop.StartAll(fetchCtx, c.services...); err != nil {
			workCancel(err)
			stopServicesOnError()
			return err
		}

		for _, producer := range c.producersByQueueName {
			if err := producer.StartWorkContext(fetchCtx, workCtx); err != nil {
				startstop.StopAllParallel(producersAsServices()...)
				workCancel(err)
				stopServicesOnError()
				return err
			}
		}

		c.queues.fetchCtx = fetchCtx
		c.queues.workCtx = workCtx
		c.workCancel = workCancel

		return nil
	}(); err != nil {
		defer stopped()
		if errors.Is(context.Cause(fetchCtx), startstop.ErrStop) {
			return rivercommon.ErrShutdown
		}
		return err
	}

	// Generate producer services while c.queues.startStopMu.Lock() is still
	// held. This is used for WaitAllStarted below, but don't use it elsewhere
	// because new producers may have been added while the client is running.
	producerServices := producersAsServices()

	go func() {
		// Wait for all subservices to start up before signaling our own start.
		// This isn't strictly needed, but gives tests a way to fully confirm
		// that all goroutines for subservices are spun up before continuing.
		//
		// Stop also cancels the "started" channel, so in case of a context
		// cancellation, this statement will fall through. The client will
		// briefly start, but then immediately stop again.
		startstop.WaitAllStarted(append(
			c.services,
			producerServices..., // see comment on this variable
		)...)

		started()
		defer stopped()

		c.baseService.Logger.InfoContext(ctx, "River client started", slog.String("client_id", c.ID()))
		defer c.baseService.Logger.InfoContext(ctx, "River client stopped", slog.String("client_id", c.ID()))

		// The call to Stop cancels this context. Block here until shutdown.
		<-fetchCtx.Done()

		c.queues.startStopMu.Lock()
		defer c.queues.startStopMu.Unlock()

		// On stop, have the producers stop fetching first of all.
		c.baseService.Logger.DebugContext(ctx, c.baseService.Name+": Stopping producers")
		startstop.StopAllParallel(producersAsServices()...)
		c.baseService.Logger.DebugContext(ctx, c.baseService.Name+": All producers stopped")

		c.workCancel(rivercommon.ErrShutdown)

		// Stop all mainline services where stop order isn't important.
		startstop.StopAllParallel(append(
			// This list of services contains the completer, which should always
			// stop after the producers so that any remaining work that was enqueued
			// will have a chance to have its state completed as it finishes.
			//
			// TODO: there's a risk here that the completer is stuck on a job that
			// won't complete. We probably need a timeout or way to move on in those
			// cases.
			c.services,

			// Will only be started if this client was leader, but can tolerate a
			// stop without having been started.
			c.queueMaintainer,
		)...)
	}()

	return nil
}

// Stop performs a graceful shutdown of the Client. It signals all producers
// to stop fetching new jobs and waits for any fetched or in-progress jobs to
// complete before exiting. If the provided context is done before shutdown has
// completed, Stop will return immediately with the context's error.
//
// There's no need to call this method if a hard stop has already been initiated
// by cancelling the context passed to Start or by calling StopAndCancel.
func (c *Client[TTx]) Stop(ctx context.Context) error {
	shouldStop, stopped, finalizeStop := c.baseStartStop.StopInit()
	if !shouldStop {
		return nil
	}

	select {
	case <-ctx.Done(): // stop context cancelled
		finalizeStop(false) // not stopped; allow Stop to be called again
		return ctx.Err()
	case <-stopped:
		finalizeStop(true)
		return nil
	}
}

// StopAndCancel shuts down the client and cancels all work in progress. It is a
// more aggressive stop than Stop because the contexts for any in-progress jobs
// are cancelled. However, it still waits for jobs to complete before returning,
// even though their contexts are cancelled. If the provided context is done
// before shutdown has completed, Stop will return immediately with the
// context's error.
//
// This can also be initiated by cancelling the context passed to Run. There is
// no need to call this method if the context passed to Run is cancelled
// instead.
func (c *Client[TTx]) StopAndCancel(ctx context.Context) error {
	c.baseService.Logger.InfoContext(ctx, c.baseService.Name+": Hard stop started; cancelling all work")
	c.workCancel(rivercommon.ErrShutdown)

	shouldStop, stopped, finalizeStop := c.baseStartStop.StopInit()
	if !shouldStop {
		return nil
	}

	select {
	case <-ctx.Done(): // stop context cancelled
		finalizeStop(false) // not stopped; allow Stop to be called again
		return ctx.Err()
	case <-stopped:
		finalizeStop(true)
		return nil
	}
}

// Stopped returns a channel that will be closed when the Client has stopped.
// It can be used to wait for a graceful shutdown to complete.
//
// It is not affected by any contexts passed to Stop or StopAndCancel.
func (c *Client[TTx]) Stopped() <-chan struct{} {
	return c.stopped
}

// Subscribe subscribes to the provided kinds of events that occur within the
// client, like EventKindJobCompleted for when a job completes.
//
// Returns a channel over which to receive events along with a cancel function
// that can be used to cancel and tear down resources associated with the
// subscription. It's recommended but not necessary to invoke the cancel
// function. Resources will be freed when the client stops in case it's not.
//
// The event channel is buffered and sends on it are non-blocking. Consumers
// must process events in a timely manner or it's possible for events to be
// dropped. Any slow operations performed in a response to a receipt (e.g.
// persisting to a database) should be made asynchronous to avoid event loss.
//
// Callers must specify the kinds of events they're interested in. This allows
// for forward compatibility in case new kinds of events are added in future
// versions. If new event kinds are added, callers will have to explicitly add
// them to their requested list and ensure they can be handled correctly.
func (c *Client[TTx]) Subscribe(kinds ...EventKind) (<-chan *Event, func()) {
	return c.SubscribeConfig(&SubscribeConfig{Kinds: kinds})
}

// The default maximum size of the subscribe channel. Events that would overflow
// it will be dropped.
const subscribeChanSizeDefault = 1_000

// SubscribeConfig is more thorough subscription configuration used for
// Client.SubscribeConfig.
type SubscribeConfig struct {
	// ChanSize is the size of the buffered channel that will be created for the
	// subscription. Incoming events that overall this number because a listener
	// isn't reading from the channel in a timely manner will be dropped.
	//
	// Defaults to 1000.
	ChanSize int

	// Kinds are the kinds of events that the subscription will receive.
	// Requiring that kinds are specified explicitly allows for forward
	// compatibility in case new kinds of events are added in future versions.
	// If new event kinds are added, callers will have to explicitly add them to
	// their requested list and ensure they can be handled correctly.
	Kinds []EventKind
}

// Special internal variant that lets us inject an overridden size.
func (c *Client[TTx]) SubscribeConfig(config *SubscribeConfig) (<-chan *Event, func()) {
	if c.subscriptionManager == nil {
		panic("created a subscription on a client that will never work jobs (Queues not configured)")
	}

	return c.subscriptionManager.SubscribeConfig(config)
}

// Dump aggregate stats from job completions to logs periodically.  These
// numbers don't mean much in themselves, but can give a rough idea of the
// proportions of each compared to each other, and may help flag outlying values
// indicative of a problem.
func (c *Client[TTx]) logStatsLoop(ctx context.Context, shouldStart bool, started, stopped func()) error {
	if !shouldStart {
		return nil
	}

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		ticker := time.NewTicker(5 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ctx.Done():
				return

			case <-ticker.C:
				c.subscriptionManager.logStats(ctx, c.baseService.Name)
			}
		}
	}()

	return nil
}

func (c *Client[TTx]) handleLeadershipChangeLoop(ctx context.Context, shouldStart bool, started, stopped func()) error {
	handleLeadershipChange := func(ctx context.Context, notification *leadership.Notification) {
		c.baseService.Logger.DebugContext(ctx, c.baseService.Name+": Election change received",
			slog.String("client_id", c.config.ID), slog.Bool("is_leader", notification.IsLeader))

		switch {
		case notification.IsLeader:
			// Starting the queue maintainer can take a little time so send to
			// this test signal _first_ so tests waiting on it can finish,
			// cancel the queue maintainer start, and overall run much faster.
			c.testSignals.electedLeader.Signal(struct{}{})

			if err := c.queueMaintainer.Start(ctx); err != nil {
				c.baseService.Logger.ErrorContext(ctx, "Error starting queue maintainer", slog.String("err", err.Error()))
			}

		default:
			c.queueMaintainer.Stop()
		}
	}

	if !shouldStart {
		return nil
	}

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		sub := c.elector.Listen()
		defer sub.Unlisten()

		for {
			select {
			case <-ctx.Done():
				return

			case notification := <-sub.C():
				handleLeadershipChange(ctx, notification)
			}
		}
	}()

	return nil
}

// Driver exposes the underlying driver used by the client.
//
// API is not stable. DO NOT USE.
func (c *Client[TTx]) Driver() riverdriver.Driver[TTx] {
	return c.driver
}

// JobCancel cancels the job with the given ID. If possible, the job is
// cancelled immediately and will not be retried. The provided context is used
// for the underlying Postgres update and can be used to cancel the operation or
// apply a timeout.
//
// If the job is still in the queue (available, scheduled, or retryable), it is
// immediately marked as cancelled and will not be retried.
//
// If the job is already finalized (cancelled, completed, or discarded), no
// changes are made.
//
// If the job is currently running, it is not immediately cancelled, but is
// instead marked for cancellation. The client running the job will also be
// notified (via LISTEN/NOTIFY) to cancel the running job's context. Although
// the job's context will be cancelled, since Go does not provide a mechanism to
// interrupt a running goroutine the job will continue running until it returns.
// As always, it is important for workers to respect context cancellation and
// return promptly when the job context is done.
//
// Once the cancellation signal is received by the client running the job, any
// error returned by that job will result in it being cancelled permanently and
// not retried. However if the job returns no error, it will be completed as
// usual.
//
// In the event the running job finishes executing _before_ the cancellation
// signal is received but _after_ this update was made, the behavior depends on
// which state the job is being transitioned into (based on its return error):
//
//   - If the job completed successfully, was cancelled from within, or was
//     discarded due to exceeding its max attempts, the job will be updated as
//     usual.
//   - If the job was snoozed to run again later or encountered a retryable error,
//     the job will be marked as cancelled and will not be attempted again.
//
// Returns the up-to-date JobRow for the specified jobID if it exists. Returns
// ErrNotFound if the job doesn't exist.
func (c *Client[TTx]) JobCancel(ctx context.Context, jobID int64) (*rivertype.JobRow, error) {
	return c.jobCancel(ctx, c.driver.GetExecutor(), jobID)
}

// JobCancelTx cancels the job with the given ID within the specified
// transaction. This variant lets a caller cancel a job atomically alongside
// other database changes. A cancelled job doesn't take effect until the
// transaction commits, and if the transaction rolls back, so too is the
// cancelled job.
//
// If possible, the job is cancelled immediately and will not be retried. The
// provided context is used for the underlying Postgres update and can be used
// to cancel the operation or apply a timeout.
//
// If the job is still in the queue (available, scheduled, or retryable), it is
// immediately marked as cancelled and will not be retried.
//
// If the job is already finalized (cancelled, completed, or discarded), no
// changes are made.
//
// If the job is currently running, it is not immediately cancelled, but is
// instead marked for cancellation. The client running the job will also be
// notified (via LISTEN/NOTIFY) to cancel the running job's context. Although
// the job's context will be cancelled, since Go does not provide a mechanism to
// interrupt a running goroutine the job will continue running until it returns.
// As always, it is important for workers to respect context cancellation and
// return promptly when the job context is done.
//
// Once the cancellation signal is received by the client running the job, any
// error returned by that job will result in it being cancelled permanently and
// not retried. However if the job returns no error, it will be completed as
// usual.
//
// In the event the running job finishes executing _before_ the cancellation
// signal is received but _after_ this update was made, the behavior depends on
// which state the job is being transitioned into (based on its return error):
//
//   - If the job completed successfully, was cancelled from within, or was
//     discarded due to exceeding its max attempts, the job will be updated as
//     usual.
//   - If the job was snoozed to run again later or encountered a retryable error,
//     the job will be marked as cancelled and will not be attempted again.
//
// Returns the up-to-date JobRow for the specified jobID if it exists. Returns
// ErrNotFound if the job doesn't exist.
func (c *Client[TTx]) JobCancelTx(ctx context.Context, tx TTx, jobID int64) (*rivertype.JobRow, error) {
	return c.jobCancel(ctx, c.driver.UnwrapExecutor(tx), jobID)
}

func (c *Client[TTx]) jobCancel(ctx context.Context, exec riverdriver.Executor, jobID int64) (*rivertype.JobRow, error) {
	return exec.JobCancel(ctx, &riverdriver.JobCancelParams{
		ID:                jobID,
		CancelAttemptedAt: c.baseService.Time.NowUTC(),
		ControlTopic:      string(notifier.NotificationTopicControl),
	})
}

// JobDelete deletes the job with the given ID from the database, returning the
// deleted row if it was deleted. Jobs in the running state are not deleted,
// instead returning rivertype.ErrJobRunning.
func (c *Client[TTx]) JobDelete(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	return c.driver.GetExecutor().JobDelete(ctx, id)
}

// JobDelete deletes the job with the given ID from the database, returning the
// deleted row if it was deleted. Jobs in the running state are not deleted,
// instead returning rivertype.ErrJobRunning. This variant lets a caller retry a
// job atomically alongside other database changes. A deleted job isn't deleted
// until the transaction commits, and if the transaction rolls back, so too is
// the deleted job.
func (c *Client[TTx]) JobDeleteTx(ctx context.Context, tx TTx, id int64) (*rivertype.JobRow, error) {
	return c.driver.UnwrapExecutor(tx).JobDelete(ctx, id)
}

// JobGet fetches a single job by its ID. Returns the up-to-date JobRow for the
// specified jobID if it exists. Returns ErrNotFound if the job doesn't exist.
func (c *Client[TTx]) JobGet(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	return c.driver.GetExecutor().JobGetByID(ctx, id)
}

// JobGetTx fetches a single job by its ID, within a transaction. Returns the
// up-to-date JobRow for the specified jobID if it exists. Returns ErrNotFound
// if the job doesn't exist.
func (c *Client[TTx]) JobGetTx(ctx context.Context, tx TTx, id int64) (*rivertype.JobRow, error) {
	return c.driver.UnwrapExecutor(tx).JobGetByID(ctx, id)
}

// JobRetry updates the job with the given ID to make it immediately available
// to be retried. Jobs in the running state are not touched, while jobs in any
// other state are made available. To prevent jobs already waiting in the queue
// from being set back in line, the job's scheduled_at field is set to the
// current time only if it's not already in the past.
//
// MaxAttempts is also incremented by one if the job has already exhausted its
// max attempts.
func (c *Client[TTx]) JobRetry(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	return c.driver.GetExecutor().JobRetry(ctx, id)
}

// JobRetryTx updates the job with the given ID to make it immediately available
// to be retried, within the specified transaction. This variant lets a caller
// retry a job atomically alongside other database changes. A retried job isn't
// visible to be worked until the transaction commits, and if the transaction
// rolls back, so too is the retried job.
//
// Jobs in the running state are not touched, while jobs in any other state are
// made available. To prevent jobs already waiting in the queue from being set
// back in line, the job's scheduled_at field is set to the current time only if
// it's not already in the past.
//
// MaxAttempts is also incremented by one if the job has already exhausted its
// max attempts.
func (c *Client[TTx]) JobRetryTx(ctx context.Context, tx TTx, id int64) (*rivertype.JobRow, error) {
	return c.driver.UnwrapExecutor(tx).JobRetry(ctx, id)
}

// ID returns the unique ID of this client as set in its config or
// auto-generated if not specified.
func (c *Client[TTx]) ID() string {
	return c.config.ID
}

func insertParamsFromConfigArgsAndOptions(archetype *baseservice.Archetype, config *Config, args JobArgs, insertOpts *InsertOpts) (*rivertype.JobInsertParams, error) {
	encodedArgs, err := json.Marshal(args)
	if err != nil {
		return nil, fmt.Errorf("error marshaling args to JSON: %w", err)
	}

	if insertOpts == nil {
		insertOpts = &InsertOpts{}
	}

	var jobInsertOpts InsertOpts
	if argsWithOpts, ok := args.(JobArgsWithInsertOpts); ok {
		jobInsertOpts = argsWithOpts.InsertOpts()
	}

	// If the time is stubbed (in a test), use that for `created_at`. Otherwise,
	// leave an empty value which will either use the database's `now()` or be defaulted
	// by drivers as necessary.
	createdAt := archetype.Time.NowUTCOrNil()

	maxAttempts := valutil.FirstNonZero(insertOpts.MaxAttempts, jobInsertOpts.MaxAttempts, config.MaxAttempts)
	priority := valutil.FirstNonZero(insertOpts.Priority, jobInsertOpts.Priority, rivercommon.PriorityDefault)
	queue := valutil.FirstNonZero(insertOpts.Queue, jobInsertOpts.Queue, rivercommon.QueueDefault)

	if err := validateQueueName(queue); err != nil {
		return nil, err
	}

	tags := insertOpts.Tags
	if insertOpts.Tags == nil {
		tags = jobInsertOpts.Tags
	}
	if tags == nil {
		tags = []string{}
	} else {
		for _, tag := range tags {
			if len(tag) > 255 {
				return nil, errors.New("tags should be a maximum of 255 characters long")
			}
			if !tagRE.MatchString(tag) {
				return nil, errors.New("tags should match regex " + tagRE.String())
			}
		}
	}

	if priority > 4 {
		return nil, errors.New("priority must be between 1 and 4")
	}

	var uniqueOpts UniqueOpts
	if !config.Test.DisableUniqueEnforcement {
		uniqueOpts = insertOpts.UniqueOpts
		if uniqueOpts.isEmpty() {
			uniqueOpts = jobInsertOpts.UniqueOpts
		}
	}
	if err := uniqueOpts.validate(); err != nil {
		return nil, err
	}

	metadata := insertOpts.Metadata
	if len(metadata) == 0 {
		metadata = []byte("{}")
	}

	insertParams := &rivertype.JobInsertParams{
		Args:        args,
		CreatedAt:   createdAt,
		EncodedArgs: encodedArgs,
		Kind:        args.Kind(),
		MaxAttempts: maxAttempts,
		Metadata:    metadata,
		Priority:    priority,
		Queue:       queue,
		State:       rivertype.JobStateAvailable,
		Tags:        tags,
	}
	if !uniqueOpts.isEmpty() {
		internalUniqueOpts := (*dbunique.UniqueOpts)(&uniqueOpts)
		insertParams.UniqueKey, err = dbunique.UniqueKey(archetype.Time, internalUniqueOpts, insertParams)
		if err != nil {
			return nil, err
		}
		insertParams.UniqueStates = internalUniqueOpts.StateBitmask()
	}

	switch {
	case !insertOpts.ScheduledAt.IsZero():
		insertParams.ScheduledAt = &insertOpts.ScheduledAt
		insertParams.State = rivertype.JobStateScheduled
	case !jobInsertOpts.ScheduledAt.IsZero():
		insertParams.ScheduledAt = &jobInsertOpts.ScheduledAt
		insertParams.State = rivertype.JobStateScheduled
	default:
		// Use a stubbed time if there was one, but otherwise prefer the value
		// generated by the database. createdAt is nil unless time is stubbed.
		insertParams.ScheduledAt = createdAt
	}

	if insertOpts.Pending {
		insertParams.State = rivertype.JobStatePending
	}

	return insertParams, nil
}

var errNoDriverDBPool = errors.New("driver must have non-nil database pool to use non-transactional methods like Insert and InsertMany (try InsertTx or InsertManyTx instead")

// Insert inserts a new job with the provided args. Job opts can be used to
// override any defaults that may have been provided by an implementation of
// JobArgsWithInsertOpts.InsertOpts, as well as any global defaults. The
// provided context is used for the underlying Postgres insert and can be used
// to cancel the operation or apply a timeout.
//
//	jobRow, err := client.Insert(insertCtx, MyArgs{}, nil)
//	if err != nil {
//		// handle error
//	}
func (c *Client[TTx]) Insert(ctx context.Context, args JobArgs, opts *InsertOpts) (*rivertype.JobInsertResult, error) {
	if !c.driver.HasPool() {
		return nil, errNoDriverDBPool
	}

	tx, err := c.driver.GetExecutor().Begin(ctx)
	if err != nil {
		return nil, err
	}
	defer tx.Rollback(ctx)

	inserted, err := c.insert(ctx, tx, args, opts)
	if err != nil {
		return nil, err
	}

	if err := tx.Commit(ctx); err != nil {
		return nil, err
	}
	return inserted, nil
}

// InsertTx inserts a new job with the provided args on the given transaction.
// Job opts can be used to override any defaults that may have been provided by
// an implementation of JobArgsWithInsertOpts.InsertOpts, as well as any global
// defaults. The provided context is used for the underlying Postgres insert and
// can be used to cancel the operation or apply a timeout.
//
//	jobRow, err := client.InsertTx(insertCtx, tx, MyArgs{}, nil)
//	if err != nil {
//		// handle error
//	}
//
// This variant lets a caller insert jobs atomically alongside other database
// changes. It's also possible to insert a job outside a transaction, but this
// usage is recommended to ensure that all data a job needs to run is available
// by the time it starts. Because of snapshot visibility guarantees across
// transactions, the job will not be worked until the transaction has committed,
// and if the transaction rolls back, so too is the inserted job.
func (c *Client[TTx]) InsertTx(ctx context.Context, tx TTx, args JobArgs, opts *InsertOpts) (*rivertype.JobInsertResult, error) {
	return c.insert(ctx, c.driver.UnwrapExecutor(tx), args, opts)
}

func (c *Client[TTx]) insert(ctx context.Context, tx riverdriver.ExecutorTx, args JobArgs, opts *InsertOpts) (*rivertype.JobInsertResult, error) {
	params := []InsertManyParams{{Args: args, InsertOpts: opts}}
	results, err := c.validateParamsAndInsertMany(ctx, tx, params)
	if err != nil {
		return nil, err
	}

	return results[0], nil
}

// InsertManyParams encapsulates a single job combined with insert options for
// use with batch insertion.
type InsertManyParams struct {
	// Args are the arguments of the job to insert.
	Args JobArgs

	// InsertOpts are insertion options for this job.
	InsertOpts *InsertOpts
}

// InsertMany inserts many jobs at once. Each job is inserted as an
// InsertManyParams tuple, which takes job args along with an optional set of
// insert options, which override insert options provided by an
// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.
// The provided context is used for the underlying Postgres inserts and can be
// used to cancel the operation or apply a timeout.
//
//	count, err := client.InsertMany(ctx, []river.InsertManyParams{
//		{Args: BatchInsertArgs{}},
//		{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},
//	})
//	if err != nil {
//		// handle error
//	}
//
// Job uniqueness is not respected when using InsertMany due to unique inserts
// using an internal transaction and advisory lock that might lead to
// significant lock contention. Insert unique jobs using Insert instead.
//
// Job uniqueness is not respected when using InsertMany due to unique inserts
// using an internal transaction and advisory lock that might lead to
// significant lock contention. Insert unique jobs using Insert instead.
func (c *Client[TTx]) InsertMany(ctx context.Context, params []InsertManyParams) ([]*rivertype.JobInsertResult, error) {
	if !c.driver.HasPool() {
		return nil, errNoDriverDBPool
	}

	tx, err := c.driver.GetExecutor().Begin(ctx)
	if err != nil {
		return nil, err
	}
	defer tx.Rollback(ctx)

	inserted, err := c.validateParamsAndInsertMany(ctx, tx, params)
	if err != nil {
		return nil, err
	}

	if err := tx.Commit(ctx); err != nil {
		return nil, err
	}
	return inserted, nil
}

// InsertManyTx inserts many jobs at once. Each job is inserted as an
// InsertManyParams tuple, which takes job args along with an optional set of
// insert options, which override insert options provided by an
// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.
// The provided context is used for the underlying Postgres inserts and can be
// used to cancel the operation or apply a timeout.
//
//	count, err := client.InsertManyTx(ctx, tx, []river.InsertManyParams{
//		{Args: BatchInsertArgs{}},
//		{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},
//	})
//	if err != nil {
//		// handle error
//	}
//
// Job uniqueness is not respected when using InsertMany due to unique inserts
// using an internal transaction and advisory lock that might lead to
// significant lock contention. Insert unique jobs using Insert instead.
//
// This variant lets a caller insert jobs atomically alongside other database
// changes. An inserted job isn't visible to be worked until the transaction
// commits, and if the transaction rolls back, so too is the inserted job.
func (c *Client[TTx]) InsertManyTx(ctx context.Context, tx TTx, params []InsertManyParams) ([]*rivertype.JobInsertResult, error) {
	exec := c.driver.UnwrapExecutor(tx)
	return c.validateParamsAndInsertMany(ctx, exec, params)
}

// validateParamsAndInsertMany is a helper method that wraps the insertMany
// method to provide param validation and conversion prior to calling the actual
// insertMany method. This allows insertMany to be reused by the
// PeriodicJobEnqueuer which cannot reference top-level river package types.
func (c *Client[TTx]) validateParamsAndInsertMany(ctx context.Context, tx riverdriver.ExecutorTx, params []InsertManyParams) ([]*rivertype.JobInsertResult, error) {
	insertParams, err := c.insertManyParams(params)
	if err != nil {
		return nil, err
	}

	return c.insertMany(ctx, tx, insertParams)
}

// insertMany is a shared code path for InsertMany and InsertManyTx, also used
// by the PeriodicJobEnqueuer.
func (c *Client[TTx]) insertMany(ctx context.Context, tx riverdriver.ExecutorTx, insertParams []*rivertype.JobInsertParams) ([]*rivertype.JobInsertResult, error) {
	return c.insertManyShared(ctx, tx, insertParams, func(ctx context.Context, insertParams []*riverdriver.JobInsertFastParams) ([]*rivertype.JobInsertResult, error) {
		results, err := c.pilot.JobInsertMany(ctx, tx, insertParams)
		if err != nil {
			return nil, err
		}

		return sliceutil.Map(results,
			func(result *riverdriver.JobInsertFastResult) *rivertype.JobInsertResult {
				return (*rivertype.JobInsertResult)(result)
			},
		), nil
	})
}

// The shared code path for all Insert and InsertMany methods. It takes a
// function that executes the actual insert operation and allows for different
// implementations of the insert query to be passed in, each mapping their
// results back to a common result type.
func (c *Client[TTx]) insertManyShared(
	ctx context.Context,
	tx riverdriver.ExecutorTx,
	insertParams []*rivertype.JobInsertParams,
	execute func(context.Context, []*riverdriver.JobInsertFastParams) ([]*rivertype.JobInsertResult, error),
) ([]*rivertype.JobInsertResult, error) {
	doInner := func(ctx context.Context) ([]*rivertype.JobInsertResult, error) {
		for _, params := range insertParams {
			for _, hook := range append(
				c.hookLookupGlobal.ByHookKind(hooklookup.HookKindInsertBegin),
				c.hookLookupByJob.ByJobArgs(params.Args).ByHookKind(hooklookup.HookKindInsertBegin)...,
			) {
				if err := hook.(rivertype.HookInsertBegin).InsertBegin(ctx, params); err != nil { //nolint:forcetypeassert
					return nil, err
				}
			}
		}

		finalInsertParams := sliceutil.Map(insertParams, func(params *rivertype.JobInsertParams) *riverdriver.JobInsertFastParams {
			return (*riverdriver.JobInsertFastParams)(params)
		})
		results, err := execute(ctx, finalInsertParams)
		if err != nil {
			return results, err
		}

		queues := make([]string, 0, 10)
		for _, params := range insertParams {
			if params.State == rivertype.JobStateAvailable {
				queues = append(queues, params.Queue)
			}
		}
		if err := c.maybeNotifyInsertForQueues(ctx, tx, queues); err != nil {
			return nil, err
		}
		return results, nil
	}

	jobInsertMiddleware := c.middlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindJobInsert)
	if len(jobInsertMiddleware) > 0 {
		// Wrap middlewares in reverse order so the one defined first is wrapped
		// as the outermost function and is first to receive the operation.
		for i := len(jobInsertMiddleware) - 1; i >= 0; i-- {
			middlewareItem := jobInsertMiddleware[i].(rivertype.JobInsertMiddleware) //nolint:forcetypeassert // capture the current middleware item
			previousDoInner := doInner                                               // Capture the current doInner function
			doInner = func(ctx context.Context) ([]*rivertype.JobInsertResult, error) {
				return middlewareItem.InsertMany(ctx, insertParams, previousDoInner)
			}
		}
	}

	return doInner(ctx)
}

// Validates input parameters for a batch insert operation and generates a set
// of batch insert parameters.
func (c *Client[TTx]) insertManyParams(params []InsertManyParams) ([]*rivertype.JobInsertParams, error) {
	if len(params) < 1 {
		return nil, errors.New("no jobs to insert")
	}

	insertParams := make([]*rivertype.JobInsertParams, len(params))
	for i, param := range params {
		if err := c.validateJobArgs(param.Args); err != nil {
			return nil, err
		}

		insertParamsItem, err := insertParamsFromConfigArgsAndOptions(&c.baseService.Archetype, c.config, param.Args, param.InsertOpts)
		if err != nil {
			return nil, err
		}

		insertParams[i] = insertParamsItem
	}

	return insertParams, nil
}

// InsertManyFast inserts many jobs at once using Postgres' `COPY FROM` mechanism,
// making the operation quite fast and memory efficient. Each job is inserted as
// an InsertManyParams tuple, which takes job args along with an optional set of
// insert options, which override insert options provided by an
// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.
// The provided context is used for the underlying Postgres inserts and can be
// used to cancel the operation or apply a timeout.
//
//	count, err := client.InsertMany(ctx, []river.InsertManyParams{
//		{Args: BatchInsertArgs{}},
//		{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},
//	})
//	if err != nil {
//		// handle error
//	}
//
// Job uniqueness is supported using this path, but unlike with `InsertMany`
// unique conflicts cannot be handled gracefully. If a unique constraint is
// violated, the operation will fail and no jobs will be inserted.
func (c *Client[TTx]) InsertManyFast(ctx context.Context, params []InsertManyParams) (int, error) {
	if !c.driver.HasPool() {
		return 0, errNoDriverDBPool
	}

	// Wrap in a transaction in case we need to notify about inserts.
	tx, err := c.driver.GetExecutor().Begin(ctx)
	if err != nil {
		return 0, err
	}
	defer tx.Rollback(ctx)

	inserted, err := c.insertManyFast(ctx, tx, params)
	if err != nil {
		return 0, err
	}
	if err := tx.Commit(ctx); err != nil {
		return 0, err
	}
	return inserted, nil
}

// InsertManyTx inserts many jobs at once using Postgres' `COPY FROM` mechanism,
// making the operation quite fast and memory efficient. Each job is inserted as
// an InsertManyParams tuple, which takes job args along with an optional set of
// insert options, which override insert options provided by an
// JobArgsWithInsertOpts.InsertOpts implementation or any client-level defaults.
// The provided context is used for the underlying Postgres inserts and can be
// used to cancel the operation or apply a timeout.
//
//	count, err := client.InsertManyTx(ctx, tx, []river.InsertManyParams{
//		{Args: BatchInsertArgs{}},
//		{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},
//	})
//	if err != nil {
//		// handle error
//	}
//
// This variant lets a caller insert jobs atomically alongside other database
// changes. An inserted job isn't visible to be worked until the transaction
// commits, and if the transaction rolls back, so too is the inserted job.
//
// Job uniqueness is supported using this path, but unlike with `InsertManyTx`
// unique conflicts cannot be handled gracefully. If a unique constraint is
// violated, the operation will fail and no jobs will be inserted.
func (c *Client[TTx]) InsertManyFastTx(ctx context.Context, tx TTx, params []InsertManyParams) (int, error) {
	exec := c.driver.UnwrapExecutor(tx)
	return c.insertManyFast(ctx, exec, params)
}

func (c *Client[TTx]) insertManyFast(ctx context.Context, tx riverdriver.ExecutorTx, params []InsertManyParams) (int, error) {
	insertParams, err := c.insertManyParams(params)
	if err != nil {
		return 0, err
	}

	results, err := c.insertManyShared(ctx, tx, insertParams, func(ctx context.Context, insertParams []*riverdriver.JobInsertFastParams) ([]*rivertype.JobInsertResult, error) {
		count, err := tx.JobInsertFastManyNoReturning(ctx, insertParams)
		if err != nil {
			return nil, err
		}
		return make([]*rivertype.JobInsertResult, count), nil
	})
	if err != nil {
		return 0, err
	}

	return len(results), nil
}

// Notify the given queues that new jobs are available. The queues list will be
// deduplicated and each will be checked to see if it is due for an insert
// notification from this client.
func (c *Client[TTx]) maybeNotifyInsertForQueues(ctx context.Context, tx riverdriver.ExecutorTx, queues []string) error {
	if len(queues) < 1 {
		return nil
	}

	queueMap := make(map[string]struct{})
	queuesDeduped := make([]string, 0, len(queues))
	payloads := make([]string, 0, len(queues))

	for _, queue := range queues {
		if _, ok := queueMap[queue]; ok {
			continue
		}

		queueMap[queue] = struct{}{}
		if c.insertNotifyLimiter.ShouldTrigger(queue) {
			payloads = append(payloads, fmt.Sprintf("{\"queue\": %q}", queue))
			queuesDeduped = append(queuesDeduped, queue)
		}
	}

	if len(payloads) < 1 {
		return nil
	}

	err := tx.NotifyMany(ctx, &riverdriver.NotifyManyParams{
		Topic:   string(notifier.NotificationTopicInsert),
		Payload: payloads,
	})
	if err != nil {
		c.baseService.Logger.ErrorContext(
			ctx,
			c.baseService.Name+": Failed to send job insert notification",
			slog.String("queues", strings.Join(queuesDeduped, ",")),
			slog.String("err", err.Error()),
		)
		return err
	}
	return nil
}

// emit a notification about a queue being paused or resumed.
func (c *Client[TTx]) notifyQueuePauseOrResume(ctx context.Context, tx riverdriver.ExecutorTx, action controlAction, queue string, opts *QueuePauseOpts) error {
	c.baseService.Logger.DebugContext(ctx,
		c.baseService.Name+": Notifying about queue state change",
		slog.String("action", string(action)),
		slog.String("queue", queue),
		slog.String("opts", fmt.Sprintf("%+v", opts)),
	)

	payload, err := json.Marshal(jobControlPayload{Action: action, Queue: queue})
	if err != nil {
		return err
	}

	err = tx.NotifyMany(ctx, &riverdriver.NotifyManyParams{
		Payload: []string{string(payload)},
		Topic:   string(notifier.NotificationTopicControl),
	})
	if err != nil {
		c.baseService.Logger.ErrorContext(
			ctx,
			c.baseService.Name+": Failed to send queue state change notification",
			slog.String("err", err.Error()),
		)
		return err
	}
	return nil
}

// Validates job args prior to insertion. Currently, verifies that a worker to
// handle the kind is registered in the configured workers bundle.
// This validation is skipped if the client is configured as an insert-only (with no workers)
// or if the client is configured to skip unknown job kinds.
func (c *Client[TTx]) validateJobArgs(args JobArgs) error {
	if c.config.Workers == nil || c.config.SkipUnknownJobCheck {
		return nil
	}

	if _, ok := c.config.Workers.workersMap[args.Kind()]; !ok {
		return &UnknownJobKindError{Kind: args.Kind()}
	}

	return nil
}

func (c *Client[TTx]) addProducer(queueName string, queueConfig QueueConfig) *producer {
	producer := newProducer(&c.baseService.Archetype, c.driver.GetExecutor(), &producerConfig{
		ClientID:               c.config.ID,
		Completer:              c.completer,
		ErrorHandler:           c.config.ErrorHandler,
		FetchCooldown:          c.config.FetchCooldown,
		FetchPollInterval:      c.config.FetchPollInterval,
		HookLookupByJob:        c.hookLookupByJob,
		HookLookupGlobal:       c.hookLookupGlobal,
		JobTimeout:             c.config.JobTimeout,
		MaxWorkers:             queueConfig.MaxWorkers,
		MiddlewareLookupGlobal: c.middlewareLookupGlobal,
		Notifier:               c.notifier,
		Queue:                  queueName,
		QueueEventCallback:     c.subscriptionManager.distributeQueueEvent,
		RetryPolicy:            c.config.RetryPolicy,
		SchedulerInterval:      c.config.schedulerInterval,
		Workers:                c.config.Workers,
	})
	c.producersByQueueName[queueName] = producer
	return producer
}

var nameRegex = regexp.MustCompile(`^(?:[a-z0-9])+(?:[_|\-]?[a-z0-9]+)*$`)

func validateQueueName(queueName string) error {
	if queueName == "" {
		return errors.New("queue name cannot be empty")
	}
	if len(queueName) > 64 {
		return errors.New("queue name cannot be longer than 64 characters")
	}
	if !nameRegex.MatchString(queueName) {
		return fmt.Errorf("queue name is invalid, expected letters and numbers separated by underscores or hyphens: %q", queueName)
	}
	return nil
}

// JobListResult is the result of a job list operation. It contains a list of
// jobs and a cursor for fetching the next page of results.
type JobListResult struct {
	// Jobs is a slice of job returned as part of the list operation.
	Jobs []*rivertype.JobRow

	// LastCursor is a cursor that can be used to list the next page of jobs.
	LastCursor *JobListCursor
}

// JobList returns a paginated list of jobs matching the provided filters. The
// provided context is used for the underlying Postgres query and can be used to
// cancel the operation or apply a timeout.
//
//	params := river.NewJobListParams().First(10).State(rivertype.JobStateCompleted)
//	jobRows, err := client.JobList(ctx, params)
//	if err != nil {
//		// handle error
//	}
func (c *Client[TTx]) JobList(ctx context.Context, params *JobListParams) (*JobListResult, error) {
	if !c.driver.HasPool() {
		return nil, errNoDriverDBPool
	}

	if params == nil {
		params = NewJobListParams()
	}
	dbParams, err := params.toDBParams()
	if err != nil {
		return nil, err
	}

	jobs, err := dblist.JobList(ctx, c.driver.GetExecutor(), dbParams)
	if err != nil {
		return nil, err
	}
	res := &JobListResult{Jobs: jobs}
	if len(jobs) > 0 {
		res.LastCursor = jobListCursorFromJobAndParams(jobs[len(jobs)-1], params)
	}
	return res, nil
}

// JobListTx returns a paginated list of jobs matching the provided filters. The
// provided context is used for the underlying Postgres query and can be used to
// cancel the operation or apply a timeout.
//
//	params := river.NewJobListParams().First(10).States(river.JobStateCompleted)
//	jobRows, err := client.JobListTx(ctx, tx, params)
//	if err != nil {
//		// handle error
//	}
func (c *Client[TTx]) JobListTx(ctx context.Context, tx TTx, params *JobListParams) (*JobListResult, error) {
	if params == nil {
		params = NewJobListParams()
	}

	dbParams, err := params.toDBParams()
	if err != nil {
		return nil, err
	}

	jobs, err := dblist.JobList(ctx, c.driver.UnwrapExecutor(tx), dbParams)
	if err != nil {
		return nil, err
	}
	res := &JobListResult{Jobs: jobs}
	if len(jobs) > 0 {
		res.LastCursor = jobListCursorFromJobAndParams(jobs[len(jobs)-1], params)
	}
	return res, nil
}

// PeriodicJobs returns the currently configured set of periodic jobs for the
// client, and can be used to add new ones or remove existing ones.
func (c *Client[TTx]) PeriodicJobs() *PeriodicJobBundle { return c.periodicJobs }

// Driver exposes the underlying pilot used by the client.
//
// API is not stable. DO NOT USE.
func (c *Client[TTx]) Pilot() riverpilot.Pilot {
	return c.pilot
}

// Queues returns the currently configured set of queues for the client, and can
// be used to add new ones.
func (c *Client[TTx]) Queues() *QueueBundle { return c.queues }

// QueueGet returns the queue with the given name. If the queue has not recently
// been active or does not exist, returns ErrNotFound.
//
// The provided context is used for the underlying Postgres query and can be
// used to cancel the operation or apply a timeout.
func (c *Client[TTx]) QueueGet(ctx context.Context, name string) (*rivertype.Queue, error) {
	return c.driver.GetExecutor().QueueGet(ctx, name)
}

// QueueGetTx returns the queue with the given name. If the queue has not recently
// been active or does not exist, returns ErrNotFound.
//
// The provided context is used for the underlying Postgres query and can be
// used to cancel the operation or apply a timeout.
func (c *Client[TTx]) QueueGetTx(ctx context.Context, tx TTx, name string) (*rivertype.Queue, error) {
	return c.driver.UnwrapExecutor(tx).QueueGet(ctx, name)
}

// QueueListResult is the result of a job list operation. It contains a list of
// jobs and leaves room for future cursor functionality.
type QueueListResult struct {
	// Queues is a slice of queues returned as part of the list operation.
	Queues []*rivertype.Queue
}

// QueueList returns a list of all queues that are currently active or were
// recently active. Limit and offset can be used to paginate the results.
//
// The provided context is used for the underlying Postgres query and can be
// used to cancel the operation or apply a timeout.
//
//	params := river.NewQueueListParams().First(10)
//	queueRows, err := client.QueueListTx(ctx, tx, params)
//	if err != nil {
//		// handle error
//	}
func (c *Client[TTx]) QueueList(ctx context.Context, params *QueueListParams) (*QueueListResult, error) {
	if params == nil {
		params = NewQueueListParams()
	}

	queues, err := c.driver.GetExecutor().QueueList(ctx, int(params.paginationCount))
	if err != nil {
		return nil, err
	}

	return &QueueListResult{Queues: queues}, nil
}

// QueueListTx returns a list of all queues that are currently active or were
// recently active. Limit and offset can be used to paginate the results.
//
// The provided context is used for the underlying Postgres query and can be
// used to cancel the operation or apply a timeout.
//
//	params := river.NewQueueListParams().First(10)
//	queueRows, err := client.QueueListTx(ctx, tx, params)
//	if err != nil {
//		// handle error
//	}
func (c *Client[TTx]) QueueListTx(ctx context.Context, tx TTx, params *QueueListParams) (*QueueListResult, error) {
	if params == nil {
		params = NewQueueListParams()
	}

	queues, err := c.driver.UnwrapExecutor(tx).QueueList(ctx, int(params.paginationCount))
	if err != nil {
		return nil, err
	}

	return &QueueListResult{Queues: queues}, nil
}

// QueuePause pauses the queue with the given name. When a queue is paused,
// clients will not fetch any more jobs for that particular queue. To pause all
// queues at once, use the special queue name "*".
//
// Clients with a configured notifier should receive a notification about the
// paused queue(s) within a few milliseconds of the transaction commit. Clients
// in poll-only mode will pause after their next poll for queue configuration.
//
// The provided context is used for the underlying Postgres update and can be
// used to cancel the operation or apply a timeout. The opts are reserved for
// future functionality.
func (c *Client[TTx]) QueuePause(ctx context.Context, name string, opts *QueuePauseOpts) error {
	tx, err := c.driver.GetExecutor().Begin(ctx)
	if err != nil {
		return err
	}
	defer tx.Rollback(ctx)

	if err := tx.QueuePause(ctx, name); err != nil {
		return err
	}

	if err := c.notifyQueuePauseOrResume(ctx, tx, controlActionPause, name, opts); err != nil {
		return err
	}

	return tx.Commit(ctx)
}

// QueuePauseTx pauses the queue with the given name. When a queue is paused,
// clients will not fetch any more jobs for that particular queue. To pause all
// queues at once, use the special queue name "*".
//
// Clients with a configured notifier should receive a notification about the
// paused queue(s) within a few milliseconds of the transaction commit. Clients
// in poll-only mode will pause after their next poll for queue configuration.
//
// The provided context is used for the underlying Postgres update and can be
// used to cancel the operation or apply a timeout. The opts are reserved for
// future functionality.
func (c *Client[TTx]) QueuePauseTx(ctx context.Context, tx TTx, name string, opts *QueuePauseOpts) error {
	executorTx := c.driver.UnwrapExecutor(tx)

	if err := executorTx.QueuePause(ctx, name); err != nil {
		return err
	}

	if err := c.notifyQueuePauseOrResume(ctx, executorTx, controlActionPause, name, opts); err != nil {
		return err
	}

	return nil
}

// QueueResume resumes the queue with the given name. If the queue was
// previously paused, any clients configured to work that queue will resume
// fetching additional jobs. To resume all queues at once, use the special queue
// name "*".
//
// Clients with a configured notifier should receive a notification about the
// resumed queue(s) within a few milliseconds of the transaction commit. Clients
// in poll-only mode will resume after their next poll for queue configuration.
//
// The provided context is used for the underlying Postgres update and can be
// used to cancel the operation or apply a timeout. The opts are reserved for
// future functionality.
func (c *Client[TTx]) QueueResume(ctx context.Context, name string, opts *QueuePauseOpts) error {
	tx, err := c.driver.GetExecutor().Begin(ctx)
	if err != nil {
		return err
	}
	defer tx.Rollback(ctx)

	if err := tx.QueueResume(ctx, name); err != nil {
		return err
	}

	if err := c.notifyQueuePauseOrResume(ctx, tx, controlActionResume, name, opts); err != nil {
		return err
	}

	return tx.Commit(ctx)
}

// QueueResume resumes the queue with the given name. If the queue was
// previously paused, any clients configured to work that queue will resume
// fetching additional jobs. To resume all queues at once, use the special queue
// name "*".
//
// Clients with a configured notifier should receive a notification about the
// resumed queue(s) within a few milliseconds of the transaction commit. Clients
// in poll-only mode will resume after their next poll for queue configuration.
//
// The provided context is used for the underlying Postgres update and can be
// used to cancel the operation or apply a timeout. The opts are reserved for
// future functionality.
func (c *Client[TTx]) QueueResumeTx(ctx context.Context, tx TTx, name string, opts *QueuePauseOpts) error {
	executorTx := c.driver.UnwrapExecutor(tx)

	if err := executorTx.QueueResume(ctx, name); err != nil {
		return err
	}

	if err := c.notifyQueuePauseOrResume(ctx, executorTx, controlActionResume, name, opts); err != nil {
		return err
	}

	return nil
}

// QueueBundle is a bundle for adding additional queues. It's made accessible
// through Client.Queues.
type QueueBundle struct {
	// Function that adds a producer to the associated client.
	addProducer func(queueName string, queueConfig QueueConfig) *producer

	fetchCtx context.Context //nolint:containedctx

	// Mutex that's acquired when client is starting and stopping and when a
	// queue is being added so that we can be sure that a client is fully
	// stopped or fully started when adding a new queue.
	startStopMu sync.Mutex

	workCtx context.Context //nolint:containedctx
}

// Add adds a new queue to the client. If the client is already started, a
// producer for the queue is started. Context is inherited from the one given to
// Client.Start.
func (b *QueueBundle) Add(queueName string, queueConfig QueueConfig) error {
	if err := queueConfig.validate(queueName); err != nil {
		return err
	}

	b.startStopMu.Lock()
	defer b.startStopMu.Unlock()

	producer := b.addProducer(queueName, queueConfig)

	// Start the queue if the client is already started.
	if b.fetchCtx != nil && b.fetchCtx.Err() == nil {
		if err := producer.StartWorkContext(b.fetchCtx, b.workCtx); err != nil {
			return err
		}
	}

	return nil
}

// Generates a default client ID using the current hostname and time.
func defaultClientID(startedAt time.Time) string {
	host, _ := os.Hostname()
	if host == "" {
		host = "unknown_host"
	}

	return defaultClientIDWithHost(startedAt, host)
}

// Same as the above, but allows host injection for testability.
func defaultClientIDWithHost(startedAt time.Time, host string) string {
	const maxHostLength = 60

	// Truncate degenerately long host names.
	host = strings.ReplaceAll(host, ".", "_")
	if len(host) > maxHostLength {
		host = host[0:maxHostLength]
	}

	// Dots, hyphens, and colons aren't particularly friendly for double click
	// to select (depends on application and configuration), so avoid them all
	// in favor of underscores.
	//
	// Go's time package is really dumb and can't format subseconds without
	// using a dot. So use the dot, then replace it with an underscore below.
	const rfc3339Compact = "2006_01_02T15_04_05.000000"

	return host + "_" + strings.Replace(startedAt.Format(rfc3339Compact), ".", "_", 1)
}

```

`client_test.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"log/slog"
	"os"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/jackc/pgerrcode"
	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgconn"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/jackc/pgx/v5/stdlib"
	"github.com/robfig/cron/v3"
	"github.com/stretchr/testify/require"
	"github.com/tidwall/sjson"

	"github.com/riverqueue/river/internal/dbunique"
	"github.com/riverqueue/river/internal/jobexecutor"
	"github.com/riverqueue/river/internal/maintenance"
	"github.com/riverqueue/river/internal/middlewarelookup"
	"github.com/riverqueue/river/internal/notifier"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/riverinternaltest/retrypolicytest"
	"github.com/riverqueue/river/internal/util/dbutil"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

type noOpArgs struct {
	Name string `json:"name"`
}

func (noOpArgs) Kind() string { return "noOp" }

type noOpWorker struct {
	WorkerDefaults[noOpArgs]
}

func (w *noOpWorker) Work(ctx context.Context, job *Job[noOpArgs]) error { return nil }

type periodicJobArgs struct{}

func (periodicJobArgs) Kind() string { return "periodic_job" }

type periodicJobWorker struct {
	WorkerDefaults[periodicJobArgs]
}

func (w *periodicJobWorker) Work(ctx context.Context, job *Job[periodicJobArgs]) error {
	return nil
}

type callbackFunc func(context.Context, *Job[callbackArgs]) error

func makeAwaitCallback(startedCh chan<- int64, doneCh chan struct{}) callbackFunc {
	return func(ctx context.Context, job *Job[callbackArgs]) error {
		client := ClientFromContext[pgx.Tx](ctx)
		client.config.Logger.InfoContext(ctx, "callback job started with id="+strconv.FormatInt(job.ID, 10))

		select {
		case <-ctx.Done():
			return ctx.Err()
		case startedCh <- job.ID:
		}

		// await done signal, or context cancellation:
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-doneCh:
			return nil
		}
	}
}

type callbackArgs struct {
	Name string `json:"name"`
}

func (callbackArgs) Kind() string { return "callback" }

type callbackWorker struct {
	WorkerDefaults[callbackArgs]
	fn callbackFunc
}

func (w *callbackWorker) Work(ctx context.Context, job *Job[callbackArgs]) error {
	return w.fn(ctx, job)
}

// A small wrapper around Client that gives us a struct that corrects the
// client's Stop function so that it can implement startstop.Service.
type clientWithSimpleStop[TTx any] struct {
	*Client[TTx]
}

func (c *clientWithSimpleStop[TTx]) Started() <-chan struct{} {
	return c.baseStartStop.Started()
}

func (c *clientWithSimpleStop[TTx]) Stop() {
	_ = c.Client.Stop(context.Background())
}

func newTestConfig(t *testing.T, callback callbackFunc) *Config {
	t.Helper()
	workers := NewWorkers()
	if callback != nil {
		AddWorker(workers, &callbackWorker{fn: callback})
	}
	AddWorker(workers, &noOpWorker{})

	return &Config{
		FetchCooldown:     20 * time.Millisecond,
		FetchPollInterval: 50 * time.Millisecond,
		Logger:            riversharedtest.Logger(t),
		MaxAttempts:       MaxAttemptsDefault,
		Queues:            map[string]QueueConfig{QueueDefault: {MaxWorkers: 50}},
		Test: TestConfig{
			Time: &riversharedtest.TimeStub{},
		},
		TestOnly:          true, // disables staggered start in maintenance services
		Workers:           workers,
		schedulerInterval: riverinternaltest.SchedulerShortInterval,
	}
}

func newTestClient(t *testing.T, dbPool *pgxpool.Pool, config *Config) *Client[pgx.Tx] {
	t.Helper()

	client, err := NewClient(riverpgxv5.New(dbPool), config)
	require.NoError(t, err)

	return client
}

func startClient[TTx any](ctx context.Context, t *testing.T, client *Client[TTx]) {
	t.Helper()

	if err := client.Start(ctx); err != nil {
		require.NoError(t, err)
	}

	t.Cleanup(func() {
		ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
		defer cancel()
		require.NoError(t, client.Stop(ctx))
	})
}

func runNewTestClient(ctx context.Context, t *testing.T, config *Config) *Client[pgx.Tx] {
	t.Helper()

	dbPool := riverinternaltest.TestDB(ctx, t)
	client := newTestClient(t, dbPool, config)
	startClient(ctx, t, client)
	return client
}

func subscribe[TTx any](t *testing.T, client *Client[TTx]) <-chan *Event {
	t.Helper()

	subscribeChan, cancel := client.Subscribe(
		EventKindJobCancelled,
		EventKindJobCompleted,
		EventKindJobFailed,
		EventKindJobSnoozed,
		EventKindQueuePaused,
		EventKindQueueResumed,
	)
	t.Cleanup(cancel)
	return subscribeChan
}

func Test_Client(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		config *Config
		dbPool *pgxpool.Pool
	}

	// Alternate setup returning only client Config rather than a full Client.
	setupConfig := func(t *testing.T) (*Config, *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)

		return config, &testBundle{
			config: config,
			dbPool: dbPool,
		}
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		config, bundle := setupConfig(t)
		return newTestClient(t, bundle.dbPool, config), bundle
	}

	t.Run("StartInsertAndWork", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		workedChan := make(chan struct{})

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			workedChan <- struct{}{}
			return nil
		}))

		startClient(ctx, t, client)

		_, err := client.Insert(ctx, &JobArgs{}, nil)
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, workedChan)
	})

	t.Run("Queues_Add_BeforeStart", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		workedChan := make(chan struct{})

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			workedChan <- struct{}{}
			return nil
		}))

		queueName := "new_queue"
		err := client.Queues().Add(queueName, QueueConfig{
			MaxWorkers: 2,
		})
		require.NoError(t, err)

		startClient(ctx, t, client)

		_, err = client.Insert(ctx, &JobArgs{}, &InsertOpts{
			Queue: queueName,
		})
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, workedChan)
	})

	t.Run("Queues_Add_AfterStart", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		workedChan := make(chan struct{})

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			workedChan <- struct{}{}
			return nil
		}))

		startClient(ctx, t, client)
		riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

		queueName := "new_queue"
		err := client.Queues().Add(queueName, QueueConfig{
			MaxWorkers: 2,
		})
		require.NoError(t, err)

		_, err = client.Insert(ctx, &JobArgs{}, &InsertOpts{
			Queue: queueName,
		})
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, workedChan)
	})

	t.Run("Queues_Add_Stress", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		var wg sync.WaitGroup

		// Uses a smaller number of workers and iterations than most stress
		// tests because there's quite a lot of mutex contention so that too
		// many can make the test run long.
		for i := range 3 {
			wg.Add(1)
			workerNum := i
			go func() {
				defer wg.Done()

				for j := range 5 {
					err := client.Queues().Add(fmt.Sprintf("new_queue_%d_%d_before", workerNum, j), QueueConfig{MaxWorkers: 1})
					require.NoError(t, err)

					err = client.Start(ctx)
					if !errors.Is(err, rivercommon.ErrShutdown) {
						require.NoError(t, err)
					}

					err = client.Queues().Add(fmt.Sprintf("new_queue_%d_%d_after", workerNum, j), QueueConfig{MaxWorkers: 1})
					require.NoError(t, err)

					stopped := make(chan struct{})

					go func() {
						defer close(stopped)
						require.NoError(t, client.Stop(ctx))
					}()

					select {
					case <-stopped:
					case <-time.After(5 * time.Second):
						require.FailNow(t, "Timed out waiting for service to stop")
					}
				}
			}()
		}

		wg.Wait()
	})

	t.Run("JobCancelErrorReturned", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return JobCancel(errors.New("a persisted internal error"))
		}))

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, &JobArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCancelled, event.Kind)
		require.Equal(t, rivertype.JobStateCancelled, event.Job.State)
		require.WithinDuration(t, time.Now(), *event.Job.FinalizedAt, 2*time.Second)

		updatedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCancelled, updatedJob.State)
		require.WithinDuration(t, time.Now(), *updatedJob.FinalizedAt, 2*time.Second)
	})

	t.Run("JobCancelErrorReturnedWithNilErr", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return JobCancel(nil)
		}))

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, &JobArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCancelled, event.Kind)
		require.Equal(t, rivertype.JobStateCancelled, event.Job.State)
		require.WithinDuration(t, time.Now(), *event.Job.FinalizedAt, 2*time.Second)

		updatedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCancelled, updatedJob.State)
		require.WithinDuration(t, time.Now(), *updatedJob.FinalizedAt, 2*time.Second)
	})

	t.Run("JobSnoozeErrorReturned", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return JobSnooze(15 * time.Minute)
		}))

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, &JobArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobSnoozed, event.Kind)
		require.Equal(t, rivertype.JobStateScheduled, event.Job.State)
		require.WithinDuration(t, time.Now().Add(15*time.Minute), event.Job.ScheduledAt, 2*time.Second)

		updatedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.Equal(t, 0, updatedJob.Attempt)
		require.Equal(t, rivertype.JobStateScheduled, updatedJob.State)
		require.WithinDuration(t, time.Now().Add(15*time.Minute), updatedJob.ScheduledAt, 2*time.Second)
	})

	// This helper is used to test cancelling a job both _in_ a transaction and
	// _outside of_ a transaction. The exact same test logic applies to each case,
	// the only difference is a different cancelFunc provided by the specific
	// subtest.
	cancelRunningJobTestHelper := func(t *testing.T, config *Config, cancelFunc func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error)) { //nolint:thelper
		defaultConfig, bundle := setupConfig(t)
		if config == nil {
			config = defaultConfig
		}
		client := newTestClient(t, bundle.dbPool, config)

		jobStartedChan := make(chan int64)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			jobStartedChan <- job.ID
			<-ctx.Done()
			return ctx.Err()
		}))

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)
		riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

		insertRes, err := client.Insert(ctx, &JobArgs{}, nil)
		require.NoError(t, err)

		startedJobID := riversharedtest.WaitOrTimeout(t, jobStartedChan)
		require.Equal(t, insertRes.Job.ID, startedJobID)

		// Cancel the job:
		updatedJob, err := cancelFunc(ctx, bundle.dbPool, client, insertRes.Job.ID)
		require.NoError(t, err)
		require.NotNil(t, updatedJob)
		// Job is still actively running at this point because the query wouldn't
		// modify that column for a running job:
		require.Equal(t, rivertype.JobStateRunning, updatedJob.State)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCancelled, event.Kind)
		require.Equal(t, rivertype.JobStateCancelled, event.Job.State)
		require.WithinDuration(t, time.Now(), *event.Job.FinalizedAt, 2*time.Second)

		jobAfterCancel, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCancelled, jobAfterCancel.State)
		require.WithinDuration(t, time.Now(), *jobAfterCancel.FinalizedAt, 2*time.Second)
	}

	t.Run("CancelRunningJob", func(t *testing.T) {
		t.Parallel()

		cancelRunningJobTestHelper(t, nil, func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error) {
			return client.JobCancel(ctx, jobID)
		})
	})

	t.Run("CancelRunningJobWithLongPollInterval", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)
		config.FetchPollInterval = 60 * time.Second
		cancelRunningJobTestHelper(t, config, func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error) {
			return client.JobCancel(ctx, jobID)
		})
	})

	t.Run("CancelRunningJobInTx", func(t *testing.T) {
		t.Parallel()

		cancelRunningJobTestHelper(t, nil, func(ctx context.Context, dbPool *pgxpool.Pool, client *Client[pgx.Tx], jobID int64) (*rivertype.JobRow, error) {
			var (
				job *rivertype.JobRow
				err error
			)
			txErr := pgx.BeginFunc(ctx, dbPool, func(tx pgx.Tx) error {
				job, err = client.JobCancelTx(ctx, tx, jobID)
				return err
			})
			require.NoError(t, txErr)
			return job, err
		})
	})

	t.Run("CancelScheduledJob", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return nil
		}))

		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, &JobArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(5 * time.Minute)})
		require.NoError(t, err)

		// Cancel the job:
		updatedJob, err := client.JobCancel(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.NotNil(t, updatedJob)
		require.Equal(t, rivertype.JobStateCancelled, updatedJob.State)
		require.WithinDuration(t, time.Now(), *updatedJob.FinalizedAt, 2*time.Second)
	})

	t.Run("CancelNonExistentJob", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)
		startClient(ctx, t, client)

		// Cancel an unknown job ID:
		jobAfter, err := client.JobCancel(ctx, 0)
		require.ErrorIs(t, err, ErrNotFound)
		require.Nil(t, jobAfter)

		// Cancel an unknown job ID, within a transaction:
		err = dbutil.WithTx(ctx, client.driver.GetExecutor(), func(ctx context.Context, exec riverdriver.ExecutorTx) error {
			jobAfter, err := exec.JobCancel(ctx, &riverdriver.JobCancelParams{ID: 0})
			require.ErrorIs(t, err, ErrNotFound)
			require.Nil(t, jobAfter)
			return nil
		})
		require.NoError(t, err)
	})

	t.Run("AlternateSchema", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		// Reconfigure the pool with an alternate schema, initialize a new pool
		dbPoolConfig := bundle.dbPool.Config() // a copy of the original config
		dbPoolConfig.ConnConfig.RuntimeParams["search_path"] = "alternate_schema"

		dbPool, err := pgxpool.NewWithConfig(ctx, dbPoolConfig)
		require.NoError(t, err)
		t.Cleanup(dbPool.Close)

		client, err := NewClient(riverpgxv5.New(dbPool), bundle.config)
		require.NoError(t, err)

		// We don't actually verify that River's functional on another schema so
		// that we don't have to raise and migrate it. We cheat a little by
		// configuring a different schema and then verifying that we can't find
		// a `river_job` to confirm we're point there.
		_, err = client.Insert(ctx, &noOpArgs{}, nil)
		var pgErr *pgconn.PgError
		require.ErrorAs(t, err, &pgErr)
		require.Equal(t, pgerrcode.UndefinedTable, pgErr.Code)
		// PgError has SchemaName and TableName properties, but unfortunately
		// neither contain a useful value in this case.
		require.Equal(t, `relation "river_job" does not exist`, pgErr.Message)
	})

	t.Run("WithGlobalInsertBeginHook", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		insertBeginHookCalled := false

		bundle.config.Hooks = []rivertype.Hook{
			HookInsertBeginFunc(func(ctx context.Context, params *rivertype.JobInsertParams) error {
				insertBeginHookCalled = true
				return nil
			}),
		}

		AddWorker(bundle.config.Workers, WorkFunc(func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		}))

		client, err := NewClient(riverpgxv5.New(bundle.dbPool), bundle.config)
		require.NoError(t, err)

		_, err = client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(t, err)

		require.True(t, insertBeginHookCalled)
	})

	t.Run("WithGlobalWorkBeginHook", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		workBeginHookCalled := false

		bundle.config.Hooks = []rivertype.Hook{
			HookWorkBeginFunc(func(ctx context.Context, job *rivertype.JobRow) error {
				workBeginHookCalled = true
				return nil
			}),
		}

		AddWorker(bundle.config.Workers, WorkFunc(func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		}))

		client, err := NewClient(riverpgxv5.New(bundle.dbPool), bundle.config)
		require.NoError(t, err)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes.Job.ID, event.Job.ID)

		require.True(t, workBeginHookCalled)
	})

	t.Run("WithInsertBeginHookOnJobArgs", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		AddWorker(bundle.config.Workers, WorkFunc(func(ctx context.Context, job *Job[jobArgsWithCustomHook]) error {
			return nil
		}))

		client, err := NewClient(riverpgxv5.New(bundle.dbPool), bundle.config)
		require.NoError(t, err)

		insertRes, err := client.Insert(ctx, jobArgsWithCustomHook{}, nil)
		require.NoError(t, err)

		var metadataMap map[string]any
		err = json.Unmarshal(insertRes.Job.Metadata, &metadataMap)
		require.NoError(t, err)
		require.Equal(t, "called", metadataMap["insert_begin_hook"])
	})

	t.Run("WithWorkBeginHookOnJobArgs", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		AddWorker(bundle.config.Workers, WorkFunc(func(ctx context.Context, job *Job[jobArgsWithCustomHook]) error {
			return nil
		}))

		client, err := NewClient(riverpgxv5.New(bundle.dbPool), bundle.config)
		require.NoError(t, err)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, jobArgsWithCustomHook{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes.Job.ID, event.Job.ID)

		var metadataMap map[string]any
		err = json.Unmarshal(event.Job.Metadata, &metadataMap)
		require.NoError(t, err)
		require.Equal(t, "called", metadataMap["work_begin_hook"])
	})

	t.Run("WithGlobalWorkerMiddleware", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)
		middlewareCalled := false

		type privateKey string

		middleware := &overridableJobMiddleware{
			workFunc: func(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {
				ctx = context.WithValue(ctx, privateKey("middleware"), "called")
				middlewareCalled = true
				return doInner(ctx)
			},
		}
		bundle.config.Middleware = []rivertype.Middleware{middleware}

		AddWorker(bundle.config.Workers, WorkFunc(func(ctx context.Context, job *Job[callbackArgs]) error {
			require.Equal(t, "called", ctx.Value(privateKey("middleware")))
			return nil
		}))

		driver := riverpgxv5.New(bundle.dbPool)
		client, err := NewClient(driver, bundle.config)
		require.NoError(t, err)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		result, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, result.Job.ID, event.Job.ID)
		require.True(t, middlewareCalled)
	})

	t.Run("WithWorkerMiddlewareOnWorker", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)
		middlewareCalled := false

		type privateKey string

		worker := &workerWithMiddleware[callbackArgs]{
			workFunc: func(ctx context.Context, job *Job[callbackArgs]) error {
				require.Equal(t, "called", ctx.Value(privateKey("middleware")))
				return nil
			},
			middlewareFunc: func(job *rivertype.JobRow) []rivertype.WorkerMiddleware {
				require.Equal(t, "callback", job.Kind)

				return []rivertype.WorkerMiddleware{
					&overridableJobMiddleware{
						workFunc: func(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {
							ctx = context.WithValue(ctx, privateKey("middleware"), "called")
							middlewareCalled = true
							return doInner(ctx)
						},
					},
				}
			},
		}

		AddWorker(bundle.config.Workers, worker)

		driver := riverpgxv5.New(bundle.dbPool)
		client, err := NewClient(driver, bundle.config)
		require.NoError(t, err)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		result, err := client.Insert(ctx, callbackArgs{Name: "middleware_test"}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, result.Job.ID, event.Job.ID)
		require.True(t, middlewareCalled)
	})

	t.Run("MiddlewareModifiesEncodedArgs", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)
		middlewareCalled := false

		worker := &workerWithMiddleware[callbackArgs]{
			workFunc: func(ctx context.Context, job *Job[callbackArgs]) error {
				require.Equal(t, "middleware name", job.Args.Name)
				return nil
			},
			middlewareFunc: func(job *rivertype.JobRow) []rivertype.WorkerMiddleware {
				return []rivertype.WorkerMiddleware{
					&overridableJobMiddleware{
						workFunc: func(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {
							middlewareCalled = true
							require.JSONEq(t, `{"name": "inserted name"}`, string(job.EncodedArgs))
							job.EncodedArgs = []byte(`{"name": "middleware name"}`)
							return doInner(ctx)
						},
					},
				}
			},
		}

		AddWorker(bundle.config.Workers, worker)

		driver := riverpgxv5.New(bundle.dbPool)
		client, err := NewClient(driver, bundle.config)
		require.NoError(t, err)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		result, err := client.Insert(ctx, callbackArgs{Name: "inserted name"}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, result.Job.ID, event.Job.ID)
		require.True(t, middlewareCalled)
	})

	t.Run("PauseAndResumeSingleQueue", func(t *testing.T) {
		t.Parallel()

		config, bundle := setupConfig(t)
		client := newTestClient(t, bundle.dbPool, config)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes1, err := client.Insert(ctx, &noOpArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes1.Job.ID, event.Job.ID)

		require.NoError(t, client.QueuePause(ctx, QueueDefault, nil))
		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, &Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: QueueDefault}}, event)

		insertRes2, err := client.Insert(ctx, &noOpArgs{}, nil)
		require.NoError(t, err)

		select {
		case <-subscribeChan:
			t.Fatal("expected job 2 to not start on paused queue")
		case <-time.After(500 * time.Millisecond):
		}

		require.NoError(t, client.QueueResume(ctx, QueueDefault, nil))
		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, &Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: QueueDefault}}, event)

		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes2.Job.ID, event.Job.ID)
	})

	t.Run("PauseAndResumeMultipleQueues", func(t *testing.T) {
		t.Parallel()

		config, bundle := setupConfig(t)
		config.Queues["alternate"] = QueueConfig{MaxWorkers: 10}
		client := newTestClient(t, bundle.dbPool, config)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes1, err := client.Insert(ctx, &noOpArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes1.Job.ID, event.Job.ID)

		// Pause only the default queue:
		require.NoError(t, client.QueuePause(ctx, QueueDefault, nil))
		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, &Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: QueueDefault}}, event)

		insertRes2, err := client.Insert(ctx, &noOpArgs{}, nil)
		require.NoError(t, err)

		select {
		case <-subscribeChan:
			t.Fatal("expected job 2 to not start on paused queue")
		case <-time.After(500 * time.Millisecond):
		}

		// alternate queue should still be running:
		insertResAlternate1, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{Queue: "alternate"})
		require.NoError(t, err)

		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertResAlternate1.Job.ID, event.Job.ID)

		// Pause all queues:
		require.NoError(t, client.QueuePause(ctx, rivercommon.AllQueuesString, nil))
		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, &Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: "alternate"}}, event)

		insertResAlternate2, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{Queue: "alternate"})
		require.NoError(t, err)

		select {
		case <-subscribeChan:
			t.Fatal("expected alternate job 2 to not start on paused queue")
		case <-time.After(500 * time.Millisecond):
		}

		// Resume only the alternate queue:
		require.NoError(t, client.QueueResume(ctx, "alternate", nil))
		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, &Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: "alternate"}}, event)

		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertResAlternate2.Job.ID, event.Job.ID)

		// Resume all queues:
		require.NoError(t, client.QueueResume(ctx, rivercommon.AllQueuesString, nil))
		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, &Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: QueueDefault}}, event)

		event = riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes2.Job.ID, event.Job.ID)
	})

	t.Run("PauseAndResumeSingleQueueTx", func(t *testing.T) {
		t.Parallel()

		config, bundle := setupConfig(t)
		client := newTestClient(t, bundle.dbPool, config)

		queue := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)

		tx, err := bundle.dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { tx.Rollback(ctx) })

		require.NoError(t, client.QueuePauseTx(ctx, tx, queue.Name, nil))

		queueRes, err := client.QueueGetTx(ctx, tx, queue.Name)
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), *queueRes.PausedAt, 2*time.Second)

		// Not paused outside transaction.
		queueRes, err = client.QueueGet(ctx, queue.Name)
		require.NoError(t, err)
		require.Nil(t, queueRes.PausedAt)

		require.NoError(t, client.QueueResumeTx(ctx, tx, queue.Name, nil))

		queueRes, err = client.QueueGetTx(ctx, tx, queue.Name)
		require.NoError(t, err)
		require.Nil(t, queueRes.PausedAt)
	})

	t.Run("PausedBeforeStart", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		jobStartedChan := make(chan int64)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			jobStartedChan <- job.ID
			return nil
		}))

		// Ensure queue record exists:
		queue := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)

		// Pause only the default queue:
		require.NoError(t, client.QueuePause(ctx, queue.Name, nil))

		startClient(ctx, t, client)

		_, err := client.Insert(ctx, &JobArgs{}, &InsertOpts{Queue: queue.Name})
		require.NoError(t, err)

		select {
		case <-jobStartedChan:
			t.Fatal("expected job to not start on paused queue")
		case <-time.After(500 * time.Millisecond):
		}
	})

	t.Run("PollOnlyDriver", func(t *testing.T) {
		t.Parallel()

		config, bundle := setupConfig(t)
		bundle.config.PollOnly = true

		stdPool := stdlib.OpenDBFromPool(bundle.dbPool)
		t.Cleanup(func() { require.NoError(t, stdPool.Close()) })

		client, err := NewClient(riverdatabasesql.New(stdPool), config)
		require.NoError(t, err)

		client.testSignals.Init()

		// Notifier should not have been initialized at all.
		require.Nil(t, client.notifier)

		insertRes, err := client.Insert(ctx, &noOpArgs{}, nil)
		require.NoError(t, err)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		// Despite no notifier, the client should still be able to elect itself
		// leader.
		client.testSignals.electedLeader.WaitOrTimeout()

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes.Job.ID, event.Job.ID)
		require.Equal(t, rivertype.JobStateCompleted, event.Job.State)
	})

	t.Run("PollOnlyOption", func(t *testing.T) {
		t.Parallel()

		config, bundle := setupConfig(t)
		bundle.config.PollOnly = true

		client := newTestClient(t, bundle.dbPool, config)
		client.testSignals.Init()

		// Notifier should not have been initialized at all.
		require.Nil(t, client.notifier)

		insertRes, err := client.Insert(ctx, &noOpArgs{}, nil)
		require.NoError(t, err)

		subscribeChan := subscribe(t, client)
		startClient(ctx, t, client)

		// Despite no notifier, the client should still be able to elect itself
		// leader.
		client.testSignals.electedLeader.WaitOrTimeout()

		event := riversharedtest.WaitOrTimeout(t, subscribeChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.Equal(t, insertRes.Job.ID, event.Job.ID)
		require.Equal(t, rivertype.JobStateCompleted, event.Job.State)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		clientWithStop := &clientWithSimpleStop[pgx.Tx]{Client: client}

		startstoptest.StressErr(ctx, t, clientWithStop, rivercommon.ErrShutdown)
	})
}

type jobArgsWithCustomHook struct{}

func (jobArgsWithCustomHook) Kind() string { return "with_custom_hook" }

func (jobArgsWithCustomHook) Hooks() []rivertype.Hook {
	return []rivertype.Hook{
		&testHookInsertAndWorkBegin{},
	}
}

var (
	_ rivertype.HookInsertBegin = &testHookInsertAndWorkBegin{}
	_ rivertype.HookWorkBegin   = &testHookInsertAndWorkBegin{}
)

type testHookInsertAndWorkBegin struct{ HookDefaults }

func (t *testHookInsertAndWorkBegin) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	var metadataMap map[string]any
	if err := json.Unmarshal(params.Metadata, &metadataMap); err != nil {
		return err
	}

	metadataMap["insert_begin_hook"] = "called"

	var err error
	params.Metadata, err = json.Marshal(metadataMap)
	if err != nil {
		return err
	}

	return nil
}

func (t *testHookInsertAndWorkBegin) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	metadataUpdates, hasMetadataUpdates := jobexecutor.MetadataUpdatesFromWorkContext(ctx)
	if !hasMetadataUpdates {
		panic("expected to be called from within job executor")
	}

	metadataUpdates["work_begin_hook"] = "called"

	return nil
}

type workerWithMiddleware[T JobArgs] struct {
	WorkerDefaults[T]
	workFunc       func(context.Context, *Job[T]) error
	middlewareFunc func(*rivertype.JobRow) []rivertype.WorkerMiddleware
}

func (w *workerWithMiddleware[T]) Middleware(job *rivertype.JobRow) []rivertype.WorkerMiddleware {
	return w.middlewareFunc(job)
}

func (w *workerWithMiddleware[T]) Work(ctx context.Context, job *Job[T]) error {
	return w.workFunc(ctx, job)
}

func Test_Client_Stop(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	// Performs continual job insertion on a number of background goroutines.
	// Returns a `finish` function that should be deferred to stop insertion and
	// safely stop goroutines.
	doParallelContinualInsertion := func(ctx context.Context, t *testing.T, client *Client[pgx.Tx]) func() {
		t.Helper()

		ctx, cancel := context.WithCancel(ctx)

		var wg sync.WaitGroup
		for range 10 {
			wg.Add(1)
			go func() {
				defer wg.Done()
				for {
					select {
					case <-ctx.Done():
						return
					default:
					}

					_, err := client.Insert(ctx, callbackArgs{}, nil)
					// A cancelled context may produce a variety of underlying
					// errors in pgx, so rather than comparing the return error,
					// first check if context is cancelled, and ignore an error
					// return if it is.
					if ctx.Err() != nil {
						return
					}
					require.NoError(t, err)

					// Sleep a brief time between inserts.
					serviceutil.CancellableSleep(ctx, randutil.DurationBetween(1*time.Microsecond, 10*time.Millisecond))
				}
			}()
		}

		return func() {
			cancel()
			wg.Wait()
		}
	}

	t.Run("no jobs in progress", func(t *testing.T) {
		t.Parallel()
		client := runNewTestClient(ctx, t, newTestConfig(t, nil))

		// Should shut down quickly:
		ctx, cancel := context.WithTimeout(ctx, time.Second)
		defer cancel()

		require.NoError(t, client.Stop(ctx))
	})

	t.Run("jobs in progress, completing promptly", func(t *testing.T) {
		t.Parallel()
		require := require.New(t)
		doneCh := make(chan struct{})
		startedCh := make(chan int64)

		client := runNewTestClient(ctx, t, newTestConfig(t, makeAwaitCallback(startedCh, doneCh)))

		ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
		defer cancel()

		// enqueue job:
		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(err)

		var startedJobID int64
		select {
		case startedJobID = <-startedCh:
		case <-time.After(500 * time.Millisecond):
			t.Fatal("timed out waiting for job to start")
		}
		require.Equal(insertRes.Job.ID, startedJobID)

		// Should not shut down immediately, not until jobs are given the signal to
		// complete:
		go func() {
			<-time.After(50 * time.Millisecond)
			close(doneCh)
		}()

		require.NoError(client.Stop(ctx))
	})

	t.Run("jobs in progress, failing to complete before stop context", func(t *testing.T) {
		t.Parallel()

		jobDoneChan := make(chan struct{})
		jobStartedChan := make(chan int64)

		callbackFunc := func(ctx context.Context, job *Job[callbackArgs]) error {
			select {
			case <-ctx.Done():
				return ctx.Err()
			case jobStartedChan <- job.ID:
			}

			select {
			case <-ctx.Done():
				require.FailNow(t, "Did not expect job to be cancelled")
			case <-jobDoneChan:
			}

			return nil
		}

		client := runNewTestClient(ctx, t, newTestConfig(t, callbackFunc))

		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(t, err)

		startedJobID := riversharedtest.WaitOrTimeout(t, jobStartedChan)
		require.Equal(t, insertRes.Job.ID, startedJobID)

		go func() {
			<-time.After(100 * time.Millisecond)
			close(jobDoneChan)
		}()

		t.Logf("Shutting down client with timeout, but while jobs are still in progress")

		// Context should expire while jobs are still in progress:
		stopCtx, stopCancel := context.WithTimeout(ctx, 50*time.Millisecond)
		t.Cleanup(stopCancel)

		err = client.Stop(stopCtx)
		require.Equal(t, context.DeadlineExceeded, err)

		select {
		case <-jobDoneChan:
			require.FailNow(t, "Expected Stop to return before job was done")
		default:
		}
	})

	t.Run("with continual insertion, no jobs are left running", func(t *testing.T) {
		t.Parallel()

		startedCh := make(chan int64)
		callbackFunc := func(ctx context.Context, job *Job[callbackArgs]) error {
			select {
			case startedCh <- job.ID:
			default:
			}
			return nil
		}

		config := newTestConfig(t, callbackFunc)
		client := runNewTestClient(ctx, t, config)

		finish := doParallelContinualInsertion(ctx, t, client)
		t.Cleanup(finish)

		// Wait for at least one job to start
		riversharedtest.WaitOrTimeout(t, startedCh)

		require.NoError(t, client.Stop(ctx))

		listRes, err := client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning))
		require.NoError(t, err)
		require.Empty(t, listRes.Jobs, "expected no jobs to be left running")
	})

	t.Run("WithSubscriber", func(t *testing.T) {
		t.Parallel()

		callbackFunc := func(ctx context.Context, job *Job[callbackArgs]) error { return nil }

		client := runNewTestClient(ctx, t, newTestConfig(t, callbackFunc))

		subscribeChan, cancel := client.Subscribe(EventKindJobCompleted)
		defer cancel()

		finish := doParallelContinualInsertion(ctx, t, client)
		defer finish()

		// Arbitrarily wait for 100 jobs to come through.
		for range 100 {
			riversharedtest.WaitOrTimeout(t, subscribeChan)
		}

		require.NoError(t, client.Stop(ctx))
	})
}

func Test_Client_Stop_AfterContextCancelled(t *testing.T) {
	t.Parallel()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// doneCh will never close, job will exit due to context cancellation:
	doneCh := make(chan struct{})
	startedCh := make(chan int64)

	dbPool := riverinternaltest.TestDB(ctx, t)
	client := newTestClient(t, dbPool, newTestConfig(t, makeAwaitCallback(startedCh, doneCh)))
	require.NoError(t, client.Start(ctx))
	t.Cleanup(func() { require.NoError(t, client.Stop(context.Background())) })

	insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
	require.NoError(t, err)
	startedJobID := riversharedtest.WaitOrTimeout(t, startedCh)
	require.Equal(t, insertRes.Job.ID, startedJobID)

	cancel()

	require.ErrorIs(t, client.Stop(ctx), context.Canceled)
}

func Test_Client_StopAndCancel(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		jobDoneChan    chan struct{}
		jobStartedChan chan int64
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		jobStartedChan := make(chan int64)
		jobDoneChan := make(chan struct{})

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			jobStartedChan <- job.ID
			t.Logf("Job waiting for context cancellation")
			defer t.Logf("Job finished")
			<-ctx.Done()
			require.ErrorIs(t, context.Cause(ctx), rivercommon.ErrShutdown)
			t.Logf("Job context done, closing chan and returning")
			close(jobDoneChan)
			return nil
		})

		client := runNewTestClient(ctx, t, config)

		return client, &testBundle{
			jobDoneChan:    jobDoneChan,
			jobStartedChan: jobStartedChan,
		}
	}

	t.Run("OnItsOwn", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		startClient(ctx, t, client)

		_, err := client.Insert(ctx, &callbackArgs{}, nil)
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, bundle.jobStartedChan)

		require.NoError(t, client.StopAndCancel(ctx))
		riversharedtest.WaitOrTimeout(t, client.Stopped())
	})

	t.Run("BeforeStart", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		require.NoError(t, client.StopAndCancel(ctx))
		riversharedtest.WaitOrTimeout(t, client.Stopped()) // this works because Stopped is nil
	})

	t.Run("AfterStop", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		startClient(ctx, t, client)

		_, err := client.Insert(ctx, &callbackArgs{}, nil)
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, bundle.jobStartedChan)

		go func() {
			require.NoError(t, client.Stop(ctx))
		}()

		select {
		case <-client.Stopped():
			t.Fatal("expected client to not be stopped yet")
		case <-time.After(500 * time.Millisecond):
		}

		require.NoError(t, client.StopAndCancel(ctx))
		riversharedtest.WaitOrTimeout(t, client.Stopped())

		select {
		case <-bundle.jobDoneChan:
		default:
			t.Fatal("expected job to have exited")
		}
	})
}

type callbackWithCustomTimeoutArgs struct {
	TimeoutValue time.Duration `json:"timeout"`
}

func (callbackWithCustomTimeoutArgs) Kind() string { return "callbackWithCustomTimeout" }

type callbackWorkerWithCustomTimeout struct {
	WorkerDefaults[callbackWithCustomTimeoutArgs]
	fn func(context.Context, *Job[callbackWithCustomTimeoutArgs]) error
}

func (w *callbackWorkerWithCustomTimeout) Work(ctx context.Context, job *Job[callbackWithCustomTimeoutArgs]) error {
	return w.fn(ctx, job)
}

func (w *callbackWorkerWithCustomTimeout) Timeout(job *Job[callbackWithCustomTimeoutArgs]) time.Duration {
	return job.Args.TimeoutValue
}

func Test_Client_JobContextInheritsFromProvidedContext(t *testing.T) {
	t.Parallel()

	deadline := time.Now().Add(2 * time.Minute)

	require := require.New(t)
	jobCtxCh := make(chan context.Context)
	doneCh := make(chan struct{})
	close(doneCh)

	callbackFunc := func(ctx context.Context, job *Job[callbackWithCustomTimeoutArgs]) error {
		// indicate the job has started, unless context is already done:
		select {
		case <-ctx.Done():
			return ctx.Err()
		case jobCtxCh <- ctx:
		}
		return nil
	}
	config := newTestConfig(t, nil)
	AddWorker(config.Workers, &callbackWorkerWithCustomTimeout{fn: callbackFunc})

	// Set a deadline and a value on the context for the client so we can verify
	// it's propagated through to the job:
	ctx, cancel := context.WithDeadline(context.Background(), deadline)
	t.Cleanup(cancel)

	type customContextKey string
	ctx = context.WithValue(ctx, customContextKey("BestGoPostgresQueue"), "River")
	client := runNewTestClient(ctx, t, config)

	insertCtx, insertCancel := context.WithTimeout(ctx, time.Second)
	t.Cleanup(insertCancel)

	// enqueue job:
	_, err := client.Insert(insertCtx, callbackWithCustomTimeoutArgs{TimeoutValue: 5 * time.Minute}, nil)
	require.NoError(err)

	var jobCtx context.Context
	select {
	case jobCtx = <-jobCtxCh:
	case <-time.After(5 * time.Second):
		t.Fatal("timed out waiting for job to start")
	}

	require.Equal("River", jobCtx.Value(customContextKey("BestGoPostgresQueue")), "job should persist the context value from the client context")
	jobDeadline, ok := jobCtx.Deadline()
	require.True(ok, "job should have a deadline")
	require.Equal(deadline, jobDeadline, "job should have the same deadline as the client context (shorter than the job's timeout)")
}

func Test_Client_ClientFromContext(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	var clientResult *Client[pgx.Tx]
	jobDoneChan := make(chan struct{})
	config := newTestConfig(t, func(ctx context.Context, j *Job[callbackArgs]) error {
		clientResult = ClientFromContext[pgx.Tx](ctx)
		close(jobDoneChan)
		return nil
	})
	client := runNewTestClient(ctx, t, config)

	_, err := client.Insert(ctx, callbackArgs{}, nil)
	require.NoError(t, err)

	riversharedtest.WaitOrTimeout(t, jobDoneChan)

	require.NotNil(t, clientResult)
	require.Equal(t, client, clientResult)
}

func Test_Client_JobDelete(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{dbPool: dbPool}
	}

	t.Run("DeletesANonRunningJob", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		insertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)

		jobAfter, err := client.JobDelete(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.NotNil(t, jobAfter)
		require.Equal(t, rivertype.JobStateScheduled, jobAfter.State)

		_, err = client.JobGet(ctx, insertRes.Job.ID)
		require.ErrorIs(t, err, ErrNotFound)
	})

	t.Run("DoesNotDeleteARunningJob", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		doneCh := make(chan struct{})
		startedCh := make(chan int64)

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[callbackArgs]) error {
			close(startedCh)
			<-doneCh
			return nil
		}))

		require.NoError(t, client.Start(ctx))
		t.Cleanup(func() { require.NoError(t, client.Stop(ctx)) })
		t.Cleanup(func() { close(doneCh) }) // must close before stopping client

		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(t, err)

		// Wait for the job to start:
		riversharedtest.WaitOrTimeout(t, startedCh)

		jobAfter, err := client.JobDelete(ctx, insertRes.Job.ID)
		require.ErrorIs(t, err, rivertype.ErrJobRunning)
		require.Nil(t, jobAfter)

		jobFromGet, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateRunning, jobFromGet.State)
	})

	t.Run("TxVariantAlsoDeletesANonRunningJob", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		insertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)

		var jobAfter *rivertype.JobRow

		err = pgx.BeginFunc(ctx, bundle.dbPool, func(tx pgx.Tx) error {
			var err error
			jobAfter, err = client.JobDeleteTx(ctx, tx, insertRes.Job.ID)
			return err
		})
		require.NoError(t, err)
		require.NotNil(t, jobAfter)
		require.Equal(t, insertRes.Job.ID, jobAfter.ID)
		require.Equal(t, rivertype.JobStateScheduled, jobAfter.State)

		jobFromGet, err := client.JobGet(ctx, insertRes.Job.ID)
		require.ErrorIs(t, ErrNotFound, err)
		require.Nil(t, jobFromGet)
	})

	t.Run("ReturnsErrNotFoundIfJobDoesNotExist", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		jobAfter, err := client.JobDelete(ctx, 0)
		require.Error(t, err)
		require.ErrorIs(t, err, ErrNotFound)
		require.Nil(t, jobAfter)
	})
}

func Test_Client_Insert(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{dbPool: dbPool}
	}

	t.Run("Succeeds", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		insertRes, err := client.Insert(ctx, &noOpArgs{}, nil)
		require.NoError(t, err)
		jobRow := insertRes.Job
		require.Equal(t, 0, jobRow.Attempt)
		require.Equal(t, rivercommon.MaxAttemptsDefault, jobRow.MaxAttempts)
		require.JSONEq(t, "{}", string(jobRow.Metadata))
		require.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)
		require.Equal(t, PriorityDefault, jobRow.Priority)
		require.Equal(t, QueueDefault, jobRow.Queue)
		require.Equal(t, []string{}, jobRow.Tags)
	})

	t.Run("WithInsertOpts", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		insertRes, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{
			MaxAttempts: 17,
			Metadata:    []byte(`{"foo": "bar"}`),
			Priority:    3,
			Queue:       "custom",
			Tags:        []string{"custom"},
		})
		jobRow := insertRes.Job
		require.NoError(t, err)
		require.Equal(t, 0, jobRow.Attempt)
		require.Equal(t, 17, jobRow.MaxAttempts)
		require.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)
		require.JSONEq(t, `{"foo": "bar"}`, string(jobRow.Metadata))
		require.WithinDuration(t, time.Now(), jobRow.ScheduledAt, 2*time.Second)
		require.Equal(t, 3, jobRow.Priority)
		require.Equal(t, "custom", jobRow.Queue)
		require.Equal(t, []string{"custom"}, jobRow.Tags)
	})

	t.Run("WithInsertOptsScheduledAtZeroTime", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		insertRes, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{
			ScheduledAt: time.Time{},
		})
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), insertRes.Job.ScheduledAt, 2*time.Second)
	})

	t.Run("OnlyTriggersInsertNotificationForAvailableJobs", func(t *testing.T) {
		t.Parallel()

		ctx := context.Background()

		_, bundle := setup(t)

		config := newTestConfig(t, nil)
		config.FetchCooldown = 5 * time.Second
		config.FetchPollInterval = 5 * time.Second
		client := newTestClient(t, bundle.dbPool, config)

		startClient(ctx, t, client)
		riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

		_, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{Queue: "a", ScheduledAt: time.Now().Add(1 * time.Hour)})
		require.NoError(t, err)

		// Queue `a` should be "due" to be triggered because it wasn't triggered above.
		require.True(t, client.insertNotifyLimiter.ShouldTrigger("a"))

		_, err = client.Insert(ctx, noOpArgs{}, &InsertOpts{Queue: "b"})
		require.NoError(t, err)

		// Queue `b` should *not* be "due" to be triggered because it was triggered above.
		require.False(t, client.insertNotifyLimiter.ShouldTrigger("b"))

		require.NoError(t, client.Stop(ctx))
	})

	t.Run("WithUniqueOpts", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		job1, err := client.Insert(ctx, noOpArgs{Name: "foo"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})
		require.NoError(t, err)
		require.NotNil(t, job1)

		// Dupe, same args:
		job2, err := client.Insert(ctx, noOpArgs{Name: "foo"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})
		require.NoError(t, err)
		require.Equal(t, job1.Job.ID, job2.Job.ID)

		// Not a dupe, different args
		job3, err := client.Insert(ctx, noOpArgs{Name: "bar"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})
		require.NoError(t, err)
		require.NotEqual(t, job1.Job.ID, job3.Job.ID)
	})

	t.Run("ErrorsOnInvalidQueueName", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		_, err := client.Insert(ctx, &noOpArgs{}, &InsertOpts{Queue: "invalid*queue"})
		require.ErrorContains(t, err, "queue name is invalid")
	})

	t.Run("ErrorsOnDriverWithoutPool", func(t *testing.T) {
		t.Parallel()

		_, _ = setup(t)

		client, err := NewClient(riverpgxv5.New(nil), &Config{
			Logger: riversharedtest.Logger(t),
		})
		require.NoError(t, err)

		_, err = client.Insert(ctx, &noOpArgs{}, nil)
		require.ErrorIs(t, err, errNoDriverDBPool)
	})

	t.Run("ErrorsOnUnknownJobKindWithWorkers", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		_, err := client.Insert(ctx, &unregisteredJobArgs{}, nil)
		var unknownJobKindErr *UnknownJobKindError
		require.ErrorAs(t, err, &unknownJobKindErr)
		require.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)
	})

	t.Run("AllowsUnknownJobKindWithoutWorkers", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		client.config.Workers = nil

		_, err := client.Insert(ctx, &unregisteredJobArgs{}, nil)
		require.NoError(t, err)
	})

	t.Run("AllowsUnknownJobKindWithSkipUnknownJobCheck", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		client.config.SkipUnknownJobCheck = true

		_, err := client.Insert(ctx, &unregisteredJobArgs{}, nil)
		require.NoError(t, err)
	})
}

func Test_Client_InsertTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		tx pgx.Tx
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		tx, err := dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { tx.Rollback(ctx) })

		return client, &testBundle{
			tx: tx,
		}
	}

	t.Run("Succeeds", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		insertRes, err := client.InsertTx(ctx, bundle.tx, &noOpArgs{}, nil)
		require.NoError(t, err)
		jobRow := insertRes.Job
		require.Equal(t, 0, jobRow.Attempt)
		require.Equal(t, rivercommon.MaxAttemptsDefault, jobRow.MaxAttempts)
		require.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)
		require.Equal(t, PriorityDefault, jobRow.Priority)
		require.Equal(t, QueueDefault, jobRow.Queue)
		require.Equal(t, []string{}, jobRow.Tags)

		// Job is not visible outside of the transaction.
		_, err = client.JobGet(ctx, jobRow.ID)
		require.ErrorIs(t, err, ErrNotFound)
	})

	t.Run("WithInsertOpts", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		insertRes, err := client.InsertTx(ctx, bundle.tx, &noOpArgs{}, &InsertOpts{
			MaxAttempts: 17,
			Priority:    3,
			Queue:       "custom",
			Tags:        []string{"custom"},
		})
		jobRow := insertRes.Job
		require.NoError(t, err)
		require.Equal(t, 0, jobRow.Attempt)
		require.Equal(t, 17, jobRow.MaxAttempts)
		require.Equal(t, (&noOpArgs{}).Kind(), jobRow.Kind)
		require.Equal(t, 3, jobRow.Priority)
		require.Equal(t, "custom", jobRow.Queue)
		require.Equal(t, []string{"custom"}, jobRow.Tags)
	})

	// A client's allowed to send nil to their driver so they can, for example,
	// easily use test transactions in their test suite.
	t.Run("WithDriverWithoutPool", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		client, err := NewClient(riverpgxv5.New(nil), &Config{
			Logger: riversharedtest.Logger(t),
		})
		require.NoError(t, err)

		_, err = client.InsertTx(ctx, bundle.tx, &noOpArgs{}, nil)
		require.NoError(t, err)
	})

	t.Run("WithUniqueOpts", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		job1, err := client.InsertTx(ctx, bundle.tx, noOpArgs{Name: "foo"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})
		require.NoError(t, err)
		require.NotNil(t, job1)

		// Dupe, same args:
		job2, err := client.InsertTx(ctx, bundle.tx, noOpArgs{Name: "foo"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})
		require.NoError(t, err)
		require.Equal(t, job1.Job.ID, job2.Job.ID)

		// Not a dupe, different args
		job3, err := client.InsertTx(ctx, bundle.tx, noOpArgs{Name: "bar"}, &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}})
		require.NoError(t, err)
		require.NotEqual(t, job1.Job.ID, job3.Job.ID)
	})

	t.Run("ErrorsOnUnknownJobKindWithWorkers", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		_, err := client.InsertTx(ctx, bundle.tx, &unregisteredJobArgs{}, nil)
		var unknownJobKindErr *UnknownJobKindError
		require.ErrorAs(t, err, &unknownJobKindErr)
		require.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)
	})

	t.Run("AllowsUnknownJobKindWithoutWorkers", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		client.config.Workers = nil

		_, err := client.InsertTx(ctx, bundle.tx, &unregisteredJobArgs{}, nil)
		require.NoError(t, err)
	})

	t.Run("AllowsUnknownJobKindWithSkipUnknownJobCheck", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		client.config.SkipUnknownJobCheck = true

		_, err := client.InsertTx(ctx, bundle.tx, &unregisteredJobArgs{}, nil)
		require.NoError(t, err)
	})
}

func Test_Client_InsertManyFast(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{dbPool: dbPool}
	}

	t.Run("SucceedsWithMultipleJobs", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: "foo", Priority: 2}},
			{Args: noOpArgs{}},
		})
		require.NoError(t, err)
		require.Equal(t, 2, count)

		jobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 2, "Expected to find exactly two jobs of kind: "+(noOpArgs{}).Kind())
	})

	t.Run("TriggersImmediateWork", func(t *testing.T) {
		t.Parallel()

		ctx := context.Background()
		_, bundle := setup(t)

		ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
		t.Cleanup(cancel)

		doneCh := make(chan struct{})
		close(doneCh) // don't need to block any jobs from completing
		startedCh := make(chan int64)

		config := newTestConfig(t, makeAwaitCallback(startedCh, doneCh))
		config.FetchCooldown = 20 * time.Millisecond
		config.FetchPollInterval = 20 * time.Second // essentially disable polling
		config.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 2}, "another_queue": {MaxWorkers: 1}}

		client := newTestClient(t, bundle.dbPool, config)

		startClient(ctx, t, client)
		riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: callbackArgs{}},
			{Args: callbackArgs{}},
		})
		require.NoError(t, err)
		require.Equal(t, 2, count)

		// Wait for the client to be ready by waiting for a job to be executed:
		riversharedtest.WaitOrTimeoutN(t, startedCh, 2)

		// Now that we've run one job, we shouldn't take longer than the cooldown to
		// fetch another after insertion. LISTEN/NOTIFY should ensure we find out
		// about the inserted job much faster than the poll interval.
		//
		// Note: we specifically use a different queue to ensure that the notify
		// limiter is immediately to fire on this queue.
		count, err = client.InsertManyFast(ctx, []InsertManyParams{
			{Args: callbackArgs{}, InsertOpts: &InsertOpts{Queue: "another_queue"}},
		})
		require.NoError(t, err)
		require.Equal(t, 1, count)

		select {
		case <-startedCh:
		// As long as this is meaningfully shorter than the poll interval, we can be
		// sure the re-fetch came from listen/notify.
		case <-time.After(5 * time.Second):
			t.Fatal("timed out waiting for another_queue job to start")
		}

		require.NoError(t, client.Stop(ctx))
	})

	t.Run("DoesNotTriggerInsertNotificationForNonAvailableJob", func(t *testing.T) {
		t.Parallel()

		ctx := context.Background()

		_, bundle := setup(t)

		config := newTestConfig(t, nil)
		config.FetchCooldown = 5 * time.Second
		config.FetchPollInterval = 5 * time.Second
		client := newTestClient(t, bundle.dbPool, config)

		startClient(ctx, t, client)
		riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: "a", ScheduledAt: time.Now().Add(1 * time.Hour)}},
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: "b"}},
		})
		require.NoError(t, err)
		require.Equal(t, 2, count)

		// Queue `a` should be "due" to be triggered because it wasn't triggered above.
		require.True(t, client.insertNotifyLimiter.ShouldTrigger("a"))
		// Queue `b` should *not* be "due" to be triggered because it was triggered above.
		require.False(t, client.insertNotifyLimiter.ShouldTrigger("b"))

		require.NoError(t, client.Stop(ctx))
	})

	t.Run("WithInsertOptsScheduledAtZeroTime", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: &noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: time.Time{}}},
		})
		require.NoError(t, err)
		require.Equal(t, 1, count)

		jobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 1, "Expected to find exactly one job of kind: "+(noOpArgs{}).Kind())
		jobRow := jobs[0]
		require.WithinDuration(t, time.Now(), jobRow.ScheduledAt, 2*time.Second)
	})

	t.Run("ErrorsOnInvalidQueueName", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: &noOpArgs{}, InsertOpts: &InsertOpts{Queue: "invalid*queue"}},
		})
		require.ErrorContains(t, err, "queue name is invalid")
		require.Equal(t, 0, count)
	})

	t.Run("ErrorsOnDriverWithoutPool", func(t *testing.T) {
		t.Parallel()

		_, _ = setup(t)

		client, err := NewClient(riverpgxv5.New(nil), &Config{
			Logger: riversharedtest.Logger(t),
		})
		require.NoError(t, err)

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: noOpArgs{}},
		})
		require.ErrorIs(t, err, errNoDriverDBPool)
		require.Equal(t, 0, count)
	})

	t.Run("ErrorsWithZeroJobs", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		count, err := client.InsertManyFast(ctx, []InsertManyParams{})
		require.EqualError(t, err, "no jobs to insert")
		require.Equal(t, 0, count)
	})

	t.Run("ErrorsOnUnknownJobKindWithWorkers", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		var unknownJobKindErr *UnknownJobKindError
		require.ErrorAs(t, err, &unknownJobKindErr)
		require.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)
		require.Equal(t, 0, count)
	})

	t.Run("AllowsUnknownJobKindWithoutWorkers", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		client.config.Workers = nil

		_, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
	})

	t.Run("AllowsUnknownJobKindWithSkipUnknownJobCheck", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		client.config.SkipUnknownJobCheck = true

		_, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
	})

	t.Run("ErrorsOnInsertOptsWithoutRequiredUniqueStates", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		count, err := client.InsertManyFast(ctx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{
				ByArgs: true,
				// force the v1 unique path with a custom state list that isn't supported in v3:
				ByState: []rivertype.JobState{rivertype.JobStateAvailable},
			}}},
		})
		require.EqualError(t, err, "UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled")
		require.Equal(t, 0, count)
	})
}

func Test_Client_InsertManyFastTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		tx pgx.Tx
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		tx, err := dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { tx.Rollback(ctx) })

		return client, &testBundle{
			tx: tx,
		}
	}

	t.Run("SucceedsWithMultipleJobs", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		count, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: "foo", Priority: 2}},
			{Args: noOpArgs{}},
		})
		require.NoError(t, err)
		require.Equal(t, 2, count)

		jobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 2, "Expected to find exactly two jobs of kind: "+(noOpArgs{}).Kind())

		require.NoError(t, bundle.tx.Commit(ctx))

		// Ensure the jobs are visible outside the transaction:
		jobs, err = client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 2, "Expected to find exactly two jobs of kind: "+(noOpArgs{}).Kind())
	})

	t.Run("SetsScheduledAtToNowByDefault", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		_, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, nil}})
		require.NoError(t, err)

		insertedJobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, insertedJobs, 1)
		require.Equal(t, rivertype.JobStateAvailable, insertedJobs[0].State)
		require.WithinDuration(t, time.Now(), insertedJobs[0].ScheduledAt, 2*time.Second)
	})

	t.Run("SupportsScheduledJobs", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		startClient(ctx, t, client)

		count, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Minute)}}})
		require.NoError(t, err)
		require.Equal(t, 1, count)

		insertedJobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, insertedJobs, 1)
		require.Equal(t, rivertype.JobStateScheduled, insertedJobs[0].State)
		require.WithinDuration(t, time.Now().Add(time.Minute), insertedJobs[0].ScheduledAt, 2*time.Second)
	})

	// A client's allowed to send nil to their driver so they can, for example,
	// easily use test transactions in their test suite.
	t.Run("WithDriverWithoutPool", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		client, err := NewClient(riverpgxv5.New(nil), &Config{
			Logger: riversharedtest.Logger(t),
		})
		require.NoError(t, err)

		count, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{
			{Args: noOpArgs{}},
		})
		require.NoError(t, err)
		require.Equal(t, 1, count)
	})

	t.Run("ErrorsWithZeroJobs", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		count, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{})
		require.EqualError(t, err, "no jobs to insert")
		require.Equal(t, 0, count)
	})

	t.Run("ErrorsOnUnknownJobKindWithWorkers", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		count, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		var unknownJobKindErr *UnknownJobKindError
		require.ErrorAs(t, err, &unknownJobKindErr)
		require.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)
		require.Equal(t, 0, count)
	})

	t.Run("AllowsUnknownJobKindWithoutWorkers", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		client.config.Workers = nil

		_, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
	})

	t.Run("AllowsUnknownJobKindWithSkipUnknownJobCheck", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		client.config.SkipUnknownJobCheck = true

		_, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
	})

	t.Run("ErrorsOnInsertOptsWithV1UniqueOpts", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		count, err := client.InsertManyFastTx(ctx, bundle.tx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{
				ByArgs: true,
				// force the v1 unique path with a custom state list that isn't supported in v3:
				ByState: []rivertype.JobState{rivertype.JobStateAvailable},
			}}},
		})
		require.EqualError(t, err, "UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled")
		require.Equal(t, 0, count)
	})
}

func Test_Client_InsertMany(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{dbPool: dbPool}
	}

	t.Run("SucceedsWithMultipleJobs", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		now := time.Now().UTC()

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: noOpArgs{Name: "Foo"}, InsertOpts: &InsertOpts{Metadata: []byte(`{"a": "b"}`), Queue: "foo", Priority: 2}},
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: now.Add(time.Minute)}},
		})
		require.NoError(t, err)
		require.Len(t, results, 2)

		require.False(t, results[0].UniqueSkippedAsDuplicate)
		require.Equal(t, 0, results[0].Job.Attempt)
		require.Nil(t, results[0].Job.AttemptedAt)
		require.WithinDuration(t, now, results[0].Job.CreatedAt, 2*time.Second)
		require.Empty(t, results[0].Job.AttemptedBy)
		require.Positive(t, results[0].Job.ID)
		require.JSONEq(t, `{"name": "Foo"}`, string(results[0].Job.EncodedArgs))
		require.Empty(t, results[0].Job.Errors)
		require.Nil(t, results[0].Job.FinalizedAt)
		require.Equal(t, "noOp", results[0].Job.Kind)
		require.Equal(t, 25, results[0].Job.MaxAttempts)
		require.JSONEq(t, `{"a": "b"}`, string(results[0].Job.Metadata))
		require.Equal(t, 2, results[0].Job.Priority)
		require.Equal(t, "foo", results[0].Job.Queue)
		require.WithinDuration(t, now, results[0].Job.ScheduledAt, 2*time.Second)
		require.Equal(t, rivertype.JobStateAvailable, results[0].Job.State)
		require.Empty(t, results[0].Job.Tags)
		require.Empty(t, results[0].Job.UniqueKey)

		require.False(t, results[1].UniqueSkippedAsDuplicate)
		require.Equal(t, 0, results[1].Job.Attempt)
		require.Nil(t, results[1].Job.AttemptedAt)
		require.WithinDuration(t, now, results[1].Job.CreatedAt, 2*time.Second)
		require.Empty(t, results[1].Job.AttemptedBy)
		require.Positive(t, results[1].Job.ID)
		require.JSONEq(t, `{"name": ""}`, string(results[1].Job.EncodedArgs))
		require.Empty(t, results[1].Job.Errors)
		require.Nil(t, results[1].Job.FinalizedAt)
		require.Equal(t, "noOp", results[1].Job.Kind)
		require.Equal(t, 25, results[1].Job.MaxAttempts)
		require.JSONEq(t, `{}`, string(results[1].Job.Metadata))
		require.Equal(t, 1, results[1].Job.Priority)
		require.Equal(t, "default", results[1].Job.Queue)
		require.WithinDuration(t, now.Add(time.Minute), results[1].Job.ScheduledAt, time.Millisecond)
		require.Equal(t, rivertype.JobStateScheduled, results[1].Job.State)
		require.Empty(t, results[1].Job.Tags)
		require.Empty(t, results[1].Job.UniqueKey)

		require.NotEqual(t, results[0].Job.ID, results[1].Job.ID)

		jobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 2, "Expected to find exactly two jobs of kind: "+(noOpArgs{}).Kind())
	})

	t.Run("TriggersImmediateWork", func(t *testing.T) {
		t.Parallel()

		ctx := context.Background()
		_, bundle := setup(t)

		ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
		t.Cleanup(cancel)

		doneCh := make(chan struct{})
		close(doneCh) // don't need to block any jobs from completing
		startedCh := make(chan int64)

		config := newTestConfig(t, makeAwaitCallback(startedCh, doneCh))
		config.FetchCooldown = 20 * time.Millisecond
		config.FetchPollInterval = 20 * time.Second // essentially disable polling
		config.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 2}, "another_queue": {MaxWorkers: 1}}

		client := newTestClient(t, bundle.dbPool, config)

		startClient(ctx, t, client)
		riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: callbackArgs{}},
			{Args: callbackArgs{}},
		})
		require.NoError(t, err)
		require.Len(t, results, 2)

		// Wait for the client to be ready by waiting for a job to be executed:
		riversharedtest.WaitOrTimeoutN(t, startedCh, 2)

		// Now that we've run one job, we shouldn't take longer than the cooldown to
		// fetch another after insertion. LISTEN/NOTIFY should ensure we find out
		// about the inserted job much faster than the poll interval.
		//
		// Note: we specifically use a different queue to ensure that the notify
		// limiter is immediately to fire on this queue.
		results, err = client.InsertMany(ctx, []InsertManyParams{
			{Args: callbackArgs{}, InsertOpts: &InsertOpts{Queue: "another_queue"}},
		})
		require.NoError(t, err)
		require.Len(t, results, 1)

		select {
		case <-startedCh:
		// As long as this is meaningfully shorter than the poll interval, we can be
		// sure the re-fetch came from listen/notify.
		case <-time.After(5 * time.Second):
			t.Fatal("timed out waiting for another_queue job to start")
		}

		require.NoError(t, client.Stop(ctx))
	})

	t.Run("DoesNotTriggerInsertNotificationForNonAvailableJob", func(t *testing.T) {
		t.Parallel()

		ctx := context.Background()

		_, bundle := setup(t)

		config := newTestConfig(t, nil)
		config.FetchCooldown = 5 * time.Second
		config.FetchPollInterval = 5 * time.Second
		client := newTestClient(t, bundle.dbPool, config)

		startClient(ctx, t, client)
		riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: "a", ScheduledAt: time.Now().Add(1 * time.Hour)}},
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{Queue: "b"}},
		})
		require.NoError(t, err)
		require.Len(t, results, 2)

		// Queue `a` should be "due" to be triggered because it wasn't triggered above.
		require.True(t, client.insertNotifyLimiter.ShouldTrigger("a"))
		// Queue `b` should *not* be "due" to be triggered because it was triggered above.
		require.False(t, client.insertNotifyLimiter.ShouldTrigger("b"))

		require.NoError(t, client.Stop(ctx))
	})

	t.Run("WithInsertOptsScheduledAtZeroTime", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: &noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: time.Time{}}},
		})
		require.NoError(t, err)
		require.Len(t, results, 1)

		jobs, err := client.driver.GetExecutor().JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 1, "Expected to find exactly one job of kind: "+(noOpArgs{}).Kind())
		jobRow := jobs[0]
		require.WithinDuration(t, time.Now(), jobRow.ScheduledAt, 2*time.Second)
	})

	t.Run("ErrorsOnInvalidQueueName", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: &noOpArgs{}, InsertOpts: &InsertOpts{Queue: "invalid*queue"}},
		})
		require.ErrorContains(t, err, "queue name is invalid")
		require.Nil(t, results)
	})

	t.Run("ErrorsOnDriverWithoutPool", func(t *testing.T) {
		t.Parallel()

		_, _ = setup(t)

		client, err := NewClient(riverpgxv5.New(nil), &Config{
			Logger: riversharedtest.Logger(t),
		})
		require.NoError(t, err)

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: noOpArgs{}},
		})
		require.ErrorIs(t, err, errNoDriverDBPool)
		require.Nil(t, results)
	})

	t.Run("ErrorsWithZeroJobs", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		results, err := client.InsertMany(ctx, []InsertManyParams{})
		require.EqualError(t, err, "no jobs to insert")
		require.Nil(t, results)
	})

	t.Run("ErrorsOnUnknownJobKindWithWorkers", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		var unknownJobKindErr *UnknownJobKindError
		require.ErrorAs(t, err, &unknownJobKindErr)
		require.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)
		require.Nil(t, results)
	})

	t.Run("AllowsUnknownJobKindWithoutWorkers", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		client.config.Workers = nil

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
		require.Len(t, results, 1)
	})

	t.Run("AllowsUnknownJobKindWithSkipUnknownJobCheck", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		client.config.SkipUnknownJobCheck = true

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
		require.Len(t, results, 1)
	})

	t.Run("ErrorsOnInsertOptsWithV1UniqueOpts", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		results, err := client.InsertMany(ctx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{
				ByArgs: true,
				// force the v1 unique path with a custom state list that isn't supported in v3:
				ByState: []rivertype.JobState{rivertype.JobStateAvailable},
			}}},
		})
		require.EqualError(t, err, "UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled")
		require.Empty(t, results)
	})
}

func Test_Client_InsertManyTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		tx pgx.Tx
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		tx, err := dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { tx.Rollback(ctx) })

		return client, &testBundle{
			tx: tx,
		}
	}

	t.Run("SucceedsWithMultipleJobs", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		now := time.Now().UTC()

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{
			{Args: noOpArgs{Name: "Foo"}, InsertOpts: &InsertOpts{Metadata: []byte(`{"a": "b"}`), Queue: "foo", Priority: 2}},
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{ScheduledAt: now.Add(time.Minute)}},
		})
		require.NoError(t, err)
		require.Len(t, results, 2)

		require.False(t, results[0].UniqueSkippedAsDuplicate)
		require.Equal(t, 0, results[0].Job.Attempt)
		require.Nil(t, results[0].Job.AttemptedAt)
		require.WithinDuration(t, now, results[0].Job.CreatedAt, 2*time.Second)
		require.Empty(t, results[0].Job.AttemptedBy)
		require.Positive(t, results[0].Job.ID)
		require.JSONEq(t, `{"name": "Foo"}`, string(results[0].Job.EncodedArgs))
		require.Empty(t, results[0].Job.Errors)
		require.Nil(t, results[0].Job.FinalizedAt)
		require.Equal(t, "noOp", results[0].Job.Kind)
		require.Equal(t, 25, results[0].Job.MaxAttempts)
		require.JSONEq(t, `{"a": "b"}`, string(results[0].Job.Metadata))
		require.Equal(t, 2, results[0].Job.Priority)
		require.Equal(t, "foo", results[0].Job.Queue)
		require.WithinDuration(t, now, results[0].Job.ScheduledAt, 2*time.Second)
		require.Equal(t, rivertype.JobStateAvailable, results[0].Job.State)
		require.Empty(t, results[0].Job.Tags)
		require.Empty(t, results[0].Job.UniqueKey)

		require.False(t, results[1].UniqueSkippedAsDuplicate)
		require.Equal(t, 0, results[1].Job.Attempt)
		require.Nil(t, results[1].Job.AttemptedAt)
		require.WithinDuration(t, now, results[1].Job.CreatedAt, 2*time.Second)
		require.Empty(t, results[1].Job.AttemptedBy)
		require.Positive(t, results[1].Job.ID)
		require.JSONEq(t, `{"name": ""}`, string(results[1].Job.EncodedArgs))
		require.Empty(t, results[1].Job.Errors)
		require.Nil(t, results[1].Job.FinalizedAt)
		require.Equal(t, "noOp", results[1].Job.Kind)
		require.Equal(t, 25, results[1].Job.MaxAttempts)
		require.JSONEq(t, `{}`, string(results[1].Job.Metadata))
		require.Equal(t, 1, results[1].Job.Priority)
		require.Equal(t, "default", results[1].Job.Queue)
		require.WithinDuration(t, now.Add(time.Minute), results[1].Job.ScheduledAt, time.Millisecond)
		require.Equal(t, rivertype.JobStateScheduled, results[1].Job.State)
		require.Empty(t, results[1].Job.Tags)
		require.Empty(t, results[1].Job.UniqueKey)

		require.NotEqual(t, results[0].Job.ID, results[1].Job.ID)

		jobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 2, "Expected to find exactly two jobs of kind: "+(noOpArgs{}).Kind())
	})

	t.Run("SetsScheduledAtToNowByDefault", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, nil}})
		require.NoError(t, err)
		require.Len(t, results, 1)

		require.Equal(t, rivertype.JobStateAvailable, results[0].Job.State)
		require.WithinDuration(t, time.Now(), results[0].Job.ScheduledAt, 2*time.Second)

		insertedJobs, err := client.driver.UnwrapExecutor(bundle.tx).JobGetByKindMany(ctx, []string{(noOpArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, insertedJobs, 1)
		require.Equal(t, rivertype.JobStateAvailable, insertedJobs[0].State)
		require.WithinDuration(t, time.Now(), insertedJobs[0].ScheduledAt, 2*time.Second)
	})

	t.Run("SupportsScheduledJobs", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		startClient(ctx, t, client)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Minute)}}})
		require.NoError(t, err)
		require.Len(t, results, 1)

		require.Equal(t, rivertype.JobStateScheduled, results[0].Job.State)
		require.WithinDuration(t, time.Now().Add(time.Minute), results[0].Job.ScheduledAt, 2*time.Second)
	})

	// A client's allowed to send nil to their driver so they can, for example,
	// easily use test transactions in their test suite.
	t.Run("WithDriverWithoutPool", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		client, err := NewClient(riverpgxv5.New(nil), &Config{
			Logger: riversharedtest.Logger(t),
		})
		require.NoError(t, err)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{
			{Args: noOpArgs{}},
		})
		require.NoError(t, err)
		require.Len(t, results, 1)
	})

	t.Run("WithJobInsertMiddleware", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)
		config := newTestConfig(t, nil)
		config.Queues = nil

		insertCalled := false
		var innerResults []*rivertype.JobInsertResult

		middleware := &overridableJobMiddleware{
			insertManyFunc: func(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
				insertCalled = true
				var err error
				for _, params := range manyParams {
					params.Metadata, err = sjson.SetBytes(params.Metadata, "middleware", "called")
					require.NoError(t, err)
				}

				results, err := doInner(ctx)
				require.NoError(t, err)
				innerResults = results
				return results, nil
			},
		}

		config.JobInsertMiddleware = []rivertype.JobInsertMiddleware{middleware}
		driver := riverpgxv5.New(nil)
		client, err := NewClient(driver, config)
		require.NoError(t, err)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{}}})
		require.NoError(t, err)
		require.Len(t, results, 1)

		require.True(t, insertCalled)
		require.Len(t, innerResults, 1)
		require.Len(t, results, 1)
		require.Equal(t, innerResults[0].Job.ID, results[0].Job.ID)
		require.JSONEq(t, `{"middleware": "called"}`, string(results[0].Job.Metadata))
	})

	t.Run("WithUniqueOpts", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{Name: "foo"}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}}}})
		require.NoError(t, err)
		require.Len(t, results, 1)
		job1 := results[0]

		// Dupe, same args:
		results, err = client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{Name: "foo"}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}}}})
		require.NoError(t, err)
		job2 := results[0]
		require.Equal(t, job1.Job.ID, job2.Job.ID)

		// Not a dupe, different args
		results, err = client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{{Args: noOpArgs{Name: "bar"}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{ByArgs: true}}}})
		require.NoError(t, err)
		job3 := results[0]
		require.NotEqual(t, job1.Job.ID, job3.Job.ID)
	})

	t.Run("ErrorsWithZeroJobs", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{})
		require.EqualError(t, err, "no jobs to insert")
		require.Nil(t, results)
	})

	t.Run("ErrorsOnUnknownJobKindWithWorkers", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		var unknownJobKindErr *UnknownJobKindError
		require.ErrorAs(t, err, &unknownJobKindErr)
		require.Equal(t, (&unregisteredJobArgs{}).Kind(), unknownJobKindErr.Kind)
		require.Nil(t, results)
	})

	t.Run("AllowsUnknownJobKindWithoutWorkers", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		client.config.Workers = nil

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
		require.Len(t, results, 1)
	})

	t.Run("AllowsUnknownJobKindWithSkipUnknownJobCheck", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		client.config.SkipUnknownJobCheck = true

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{
			{Args: unregisteredJobArgs{}},
		})
		require.NoError(t, err)
		require.Len(t, results, 1)
	})

	t.Run("ErrorsOnInsertOptsWithV1UniqueOpts", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		results, err := client.InsertManyTx(ctx, bundle.tx, []InsertManyParams{
			{Args: noOpArgs{}, InsertOpts: &InsertOpts{UniqueOpts: UniqueOpts{
				ByArgs: true,
				// force the v1 unique path with a custom state list that isn't supported in v3:
				ByState: []rivertype.JobState{rivertype.JobStateAvailable},
			}}},
		})
		require.EqualError(t, err, "UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled")
		require.Empty(t, results)
	})
}

func Test_Client_JobGet(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{}
	}

	t.Run("FetchesAnExistingJob", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		insertRes, err := client.Insert(ctx, noOpArgs{}, nil)
		require.NoError(t, err)

		job, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)

		require.Equal(t, insertRes.Job.ID, job.ID)
		require.Equal(t, insertRes.Job.State, job.State)
	})

	t.Run("ReturnsErrNotFoundIfJobDoesNotExist", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		job, err := client.JobGet(ctx, 0)
		require.Error(t, err)
		require.ErrorIs(t, err, ErrNotFound)
		require.Nil(t, job)
	})
}

func Test_Client_JobList(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		exec riverdriver.Executor
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{
			exec: client.driver.GetExecutor(),
		}
	}

	t.Run("FiltersByKind", func(t *testing.T) { //nolint:dupl
		t.Parallel()

		client, bundle := setup(t)

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("test_kind_1")})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("test_kind_1")})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("test_kind_2")})

		listRes, err := client.JobList(ctx, NewJobListParams().Kinds("test_kind_1"))
		require.NoError(t, err)
		// jobs ordered by ScheduledAt ASC by default
		require.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

		listRes, err = client.JobList(ctx, NewJobListParams().Kinds("test_kind_2"))
		require.NoError(t, err)
		require.Equal(t, []int64{job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
	})

	t.Run("FiltersByQueue", func(t *testing.T) { //nolint:dupl
		t.Parallel()

		client, bundle := setup(t)

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Queue: ptrutil.Ptr("queue_1")})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Queue: ptrutil.Ptr("queue_1")})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Queue: ptrutil.Ptr("queue_2")})

		listRes, err := client.JobList(ctx, NewJobListParams().Queues("queue_1"))
		require.NoError(t, err)
		// jobs ordered by ScheduledAt ASC by default
		require.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

		listRes, err = client.JobList(ctx, NewJobListParams().Queues("queue_2"))
		require.NoError(t, err)
		require.Equal(t, []int64{job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
	})

	t.Run("FiltersByState", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		job4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStatePending)})

		listRes, err := client.JobList(ctx, NewJobListParams().States(rivertype.JobStateAvailable))
		require.NoError(t, err)
		// jobs ordered by ScheduledAt ASC by default
		require.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

		listRes, err = client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning))
		require.NoError(t, err)
		require.Equal(t, []int64{job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

		// All by default:
		listRes, err = client.JobList(ctx, NewJobListParams())
		require.NoError(t, err)
		require.Equal(t, []int64{job1.ID, job2.ID, job3.ID, job4.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
	})

	t.Run("DefaultsToOrderingByID", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})

		listRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))
		require.NoError(t, err)
		require.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

		listRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderDesc))
		require.NoError(t, err)
		require.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
	})

	t.Run("OrderByTimeSortsAvailableRetryableAndScheduledJobsByScheduledAt", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		now := time.Now().UTC()

		states := []rivertype.JobState{
			rivertype.JobStateAvailable,
			rivertype.JobStateRetryable,
			rivertype.JobStateScheduled,
		}
		for _, state := range states {
			job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), ScheduledAt: &now})
			job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})

			listRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(state))
			require.NoError(t, err)
			require.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

			listRes, err = client.JobList(ctx, NewJobListParams().States(state).OrderBy(JobListOrderByTime, SortOrderDesc))
			require.NoError(t, err)
			require.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		}
	})

	t.Run("OrderByTimeSortsCancelledCompletedAndDiscardedJobsByFinalizedAt", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		now := time.Now().UTC()

		states := []rivertype.JobState{
			rivertype.JobStateCancelled,
			rivertype.JobStateCompleted,
			rivertype.JobStateDiscarded,
		}
		for _, state := range states {
			job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), FinalizedAt: ptrutil.Ptr(now.Add(-10 * time.Second))})
			job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(state), FinalizedAt: ptrutil.Ptr(now.Add(-15 * time.Second))})

			listRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(state))
			require.NoError(t, err)
			require.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

			listRes, err = client.JobList(ctx, NewJobListParams().States(state).OrderBy(JobListOrderByTime, SortOrderDesc))
			require.NoError(t, err)
			require.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		}
	})

	t.Run("OrderByTimeSortsRunningJobsByAttemptedAt", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		now := time.Now().UTC()
		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: &now})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second))})

		listRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateRunning))
		require.NoError(t, err)
		require.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

		listRes, err = client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning).OrderBy(JobListOrderByTime, SortOrderDesc))
		require.NoError(t, err)
		// Sort order was explicitly reversed:
		require.Equal(t, []int64{job1.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
	})

	t.Run("WithNilParamsFiltersToAllStatesByDefault", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		now := time.Now().UTC()
		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: &now})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), ScheduledAt: ptrutil.Ptr(now.Add(-2 * time.Second))})

		listRes, err := client.JobList(ctx, nil)
		require.NoError(t, err)
		// sort order defaults to ID
		require.Equal(t, []int64{job1.ID, job2.ID, job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
	})

	t.Run("PaginatesWithAfter_JobListOrderByID", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})

		listRes, err := client.JobList(ctx, NewJobListParams().After(JobListCursorFromJob(job1)))
		require.NoError(t, err)
		require.Equal(t, []int64{job2.ID, job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Equal(t, JobListOrderByID, listRes.LastCursor.sortField)
		require.Equal(t, job3.ID, listRes.LastCursor.id)

		// No more results
		listRes, err = client.JobList(ctx, NewJobListParams().After(JobListCursorFromJob(job3)))
		require.NoError(t, err)
		require.Equal(t, []int64{}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Nil(t, listRes.LastCursor)

		// Descending
		listRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByID, SortOrderDesc).After(JobListCursorFromJob(job3)))
		require.NoError(t, err)
		require.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Equal(t, JobListOrderByID, listRes.LastCursor.sortField)
		require.Equal(t, job1.ID, listRes.LastCursor.id)
	})

	t.Run("PaginatesWithAfter_JobListOrderByScheduledAt", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		now := time.Now().UTC()
		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{ScheduledAt: &now})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{ScheduledAt: ptrutil.Ptr(now.Add(1 * time.Second))})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{ScheduledAt: ptrutil.Ptr(now.Add(2 * time.Second))})

		listRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByScheduledAt, SortOrderAsc).After(JobListCursorFromJob(job1)))
		require.NoError(t, err)
		require.Equal(t, []int64{job2.ID, job3.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Equal(t, JobListOrderByScheduledAt, listRes.LastCursor.sortField)
		require.Equal(t, job3.ID, listRes.LastCursor.id)

		// No more results
		listRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByScheduledAt, SortOrderAsc).After(JobListCursorFromJob(job3)))
		require.NoError(t, err)
		require.Equal(t, []int64{}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Nil(t, listRes.LastCursor)

		// Descending
		listRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByScheduledAt, SortOrderDesc).After(JobListCursorFromJob(job3)))
		require.NoError(t, err)
		require.Equal(t, []int64{job2.ID, job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Equal(t, JobListOrderByScheduledAt, listRes.LastCursor.sortField)
		require.Equal(t, job1.ID, listRes.LastCursor.id)
	})

	t.Run("PaginatesWithAfter_JobListOrderByTime", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		now := time.Now().UTC()
		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable), ScheduledAt: &now})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second)), AttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second))})
		job4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning), ScheduledAt: ptrutil.Ptr(now.Add(-6 * time.Second)), AttemptedAt: &now})
		job5 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), ScheduledAt: ptrutil.Ptr(now.Add(-7 * time.Second)), FinalizedAt: ptrutil.Ptr(now.Add(-5 * time.Second))})
		job6 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), ScheduledAt: ptrutil.Ptr(now.Add(-7 * time.Second)), FinalizedAt: &now})

		listRes, err := client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateAvailable).After(JobListCursorFromJob(job1)))
		require.NoError(t, err)
		require.Equal(t, []int64{job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Equal(t, JobListOrderByTime, listRes.LastCursor.sortField)
		require.Equal(t, job2.ID, listRes.LastCursor.id)

		listRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateRunning).After(JobListCursorFromJob(job3)))
		require.NoError(t, err)
		require.Equal(t, []int64{job4.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Equal(t, JobListOrderByTime, listRes.LastCursor.sortField)
		require.Equal(t, job4.ID, listRes.LastCursor.id)

		listRes, err = client.JobList(ctx, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).States(rivertype.JobStateCompleted).After(JobListCursorFromJob(job5)))
		require.NoError(t, err)
		require.Equal(t, []int64{job6.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
		require.Equal(t, JobListOrderByTime, listRes.LastCursor.sortField)
		require.Equal(t, job6.ID, listRes.LastCursor.id)
	})

	t.Run("MetadataOnly", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Metadata: []byte(`{"foo": "bar"}`)})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Metadata: []byte(`{"baz": "value"}`)})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Metadata: []byte(`{"baz": "value"}`)})

		listRes, err := client.JobList(ctx, NewJobListParams().Metadata(`{"foo": "bar"}`))
		require.NoError(t, err)
		require.Equal(t, []int64{job1.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))

		listRes, err = client.JobList(ctx, NewJobListParams().Metadata(`{"baz": "value"}`).OrderBy(JobListOrderByTime, SortOrderDesc))
		require.NoError(t, err)
		// Sort order was explicitly reversed:
		require.Equal(t, []int64{job3.ID, job2.ID}, sliceutil.Map(listRes.Jobs, func(job *rivertype.JobRow) int64 { return job.ID }))
	})

	t.Run("WithCancelledContext", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		ctx, cancel := context.WithCancel(ctx)
		cancel() // cancel immediately

		listRes, err := client.JobList(ctx, NewJobListParams().States(rivertype.JobStateRunning))
		require.ErrorIs(t, context.Canceled, err)
		require.Nil(t, listRes)
	})
}

func Test_Client_JobRetry(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{dbPool: dbPool}
	}

	t.Run("UpdatesAJobScheduledInTheFutureToBeImmediatelyAvailable", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		insertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)

		job, err := client.JobRetry(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.NotNil(t, job)

		require.Equal(t, rivertype.JobStateAvailable, job.State)
		require.WithinDuration(t, time.Now().UTC(), job.ScheduledAt, 5*time.Second)
	})

	t.Run("TxVariantAlsoUpdatesJobToAvailable", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		insertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{ScheduledAt: time.Now().Add(time.Hour)})
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateScheduled, insertRes.Job.State)

		var jobAfter *rivertype.JobRow

		err = pgx.BeginFunc(ctx, bundle.dbPool, func(tx pgx.Tx) error {
			var err error
			jobAfter, err = client.JobRetryTx(ctx, tx, insertRes.Job.ID)
			return err
		})
		require.NoError(t, err)
		require.NotNil(t, jobAfter)

		require.Equal(t, rivertype.JobStateAvailable, jobAfter.State)
		require.WithinDuration(t, time.Now().UTC(), jobAfter.ScheduledAt, 5*time.Second)
	})

	t.Run("ReturnsErrNotFoundIfJobDoesNotExist", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		job, err := client.JobRetry(ctx, 0)
		require.Error(t, err)
		require.ErrorIs(t, err, ErrNotFound)
		require.Nil(t, job)
	})
}

func Test_Client_ErrorHandler(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		SubscribeChan <-chan *Event
	}

	setup := func(t *testing.T, config *Config) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		client := runNewTestClient(ctx, t, config)

		subscribeChan, cancel := client.Subscribe(EventKindJobCompleted, EventKindJobFailed)
		t.Cleanup(cancel)

		return client, &testBundle{SubscribeChan: subscribeChan}
	}

	requireInsert := func(ctx context.Context, client *Client[pgx.Tx]) *rivertype.JobRow {
		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(t, err)
		return insertRes.Job
	}

	t.Run("ErrorHandler", func(t *testing.T) {
		t.Parallel()

		handlerErr := errors.New("job error")
		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return handlerErr
		})

		var errorHandlerCalled bool
		config.ErrorHandler = &testErrorHandler{
			HandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {
				require.Equal(t, handlerErr, err)
				errorHandlerCalled = true
				return &ErrorHandlerResult{}
			},
		}

		client, bundle := setup(t, config)

		requireInsert(ctx, client)
		riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)

		require.True(t, errorHandlerCalled)
	})

	t.Run("ErrorHandler_UnknownJobKind", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)

		var errorHandlerCalled bool
		config.ErrorHandler = &testErrorHandler{
			HandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {
				var unknownJobKindErr *UnknownJobKindError
				require.ErrorAs(t, err, &unknownJobKindErr)
				require.Equal(t, UnknownJobKindError{Kind: "RandomWorkerNameThatIsNeverRegistered"}, *unknownJobKindErr)
				errorHandlerCalled = true
				return &ErrorHandlerResult{}
			},
		}

		client, bundle := setup(t, config)

		// Bypass the normal Insert function because that will error on an
		// unknown job.
		insertParams, err := insertParamsFromConfigArgsAndOptions(&client.baseService.Archetype, config, unregisteredJobArgs{}, nil)
		require.NoError(t, err)
		_, err = client.driver.GetExecutor().JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{(*riverdriver.JobInsertFastParams)(insertParams)})
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)

		require.True(t, errorHandlerCalled)
	})

	t.Run("PanicHandler", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			panic("panic val")
		})

		var panicHandlerCalled bool
		config.ErrorHandler = &testErrorHandler{
			HandlePanicFunc: func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
				require.Equal(t, "panic val", panicVal)
				panicHandlerCalled = true
				return &ErrorHandlerResult{}
			},
		}

		client, bundle := setup(t, config)

		requireInsert(ctx, client)
		riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)

		require.True(t, panicHandlerCalled)
	})
}

func Test_Client_Maintenance(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		exec riverdriver.Executor
	}

	setup := func(t *testing.T, config *Config) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		var (
			dbPool = riverinternaltest.TestDB(ctx, t)
			client = newTestClient(t, dbPool, config)
		)

		client.testSignals.Init()

		return client, &testBundle{exec: client.driver.GetExecutor()}
	}

	// Starts the client, then waits for it to be elected leader and for the
	// queue maintainer to start.
	startAndWaitForQueueMaintainer := func(ctx context.Context, t *testing.T, client *Client[pgx.Tx]) {
		t.Helper()

		startClient(ctx, t, client)
		client.testSignals.electedLeader.WaitOrTimeout()
		riversharedtest.WaitOrTimeout(t, client.queueMaintainer.Started())
	}

	t.Run("JobCleaner", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)
		config.CancelledJobRetentionPeriod = 1 * time.Hour
		config.CompletedJobRetentionPeriod = 1 * time.Hour
		config.DiscardedJobRetentionPeriod = 1 * time.Hour

		client, bundle := setup(t, config)

		deleteHorizon := time.Now().Add(-config.CompletedJobRetentionPeriod)

		// Take care to insert jobs before starting the client because otherwise
		// there's a race condition where the cleaner could run its initial
		// pass before our insertion is complete.
		ineligibleJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		ineligibleJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		ineligibleJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled)})

		jobBeyondHorizon1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})
		jobBeyondHorizon2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})
		jobBeyondHorizon3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})

		// Will not be deleted.
		jobWithinHorizon1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})
		jobWithinHorizon2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})
		jobWithinHorizon3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})

		startAndWaitForQueueMaintainer(ctx, t, client)

		jc := maintenance.GetService[*maintenance.JobCleaner](client.queueMaintainer)
		jc.TestSignals.DeletedBatch.WaitOrTimeout()

		var err error
		_, err = client.JobGet(ctx, ineligibleJob1.ID)
		require.NotErrorIs(t, err, ErrNotFound) // still there
		_, err = client.JobGet(ctx, ineligibleJob2.ID)
		require.NotErrorIs(t, err, ErrNotFound) // still there
		_, err = client.JobGet(ctx, ineligibleJob3.ID)
		require.NotErrorIs(t, err, ErrNotFound) // still there

		_, err = client.JobGet(ctx, jobBeyondHorizon1.ID)
		require.ErrorIs(t, err, ErrNotFound)
		_, err = client.JobGet(ctx, jobBeyondHorizon2.ID)
		require.ErrorIs(t, err, ErrNotFound)
		_, err = client.JobGet(ctx, jobBeyondHorizon3.ID)
		require.ErrorIs(t, err, ErrNotFound)

		_, err = client.JobGet(ctx, jobWithinHorizon1.ID)
		require.NotErrorIs(t, err, ErrNotFound) // still there
		_, err = client.JobGet(ctx, jobWithinHorizon2.ID)
		require.NotErrorIs(t, err, ErrNotFound) // still there
		_, err = client.JobGet(ctx, jobWithinHorizon3.ID)
		require.NotErrorIs(t, err, ErrNotFound) // still there
	})

	t.Run("JobRescuer", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)
		config.RescueStuckJobsAfter = 5 * time.Minute

		client, bundle := setup(t, config)

		now := time.Now()

		// Take care to insert jobs before starting the client because otherwise
		// there's a race condition where the rescuer could run its initial
		// pass before our insertion is complete.
		ineligibleJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(time.Minute))})
		ineligibleJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(time.Minute))})
		ineligibleJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(now.Add(-time.Minute))})

		// large attempt number ensures these don't immediately start executing again:
		jobStuckToRetry1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(20), AttemptedAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})
		jobStuckToRetry2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(20), AttemptedAt: ptrutil.Ptr(now.Add(-30 * time.Minute))})
		jobStuckToDiscard := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{
			State:       ptrutil.Ptr(rivertype.JobStateRunning),
			Attempt:     ptrutil.Ptr(20),
			AttemptedAt: ptrutil.Ptr(now.Add(-5*time.Minute - time.Second)),
			MaxAttempts: ptrutil.Ptr(1),
		})

		// Will not be rescued.
		jobNotYetStuck1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-4 * time.Minute))})
		jobNotYetStuck2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-1 * time.Minute))})
		jobNotYetStuck3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("noOp"), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(now.Add(-10 * time.Second))})

		startAndWaitForQueueMaintainer(ctx, t, client)

		svc := maintenance.GetService[*maintenance.JobRescuer](client.queueMaintainer)
		svc.TestSignals.FetchedBatch.WaitOrTimeout()
		svc.TestSignals.UpdatedBatch.WaitOrTimeout()

		requireJobHasState := func(jobID int64, state rivertype.JobState) {
			t.Helper()
			job, err := bundle.exec.JobGetByID(ctx, jobID)
			require.NoError(t, err)
			require.Equal(t, state, job.State)
		}

		// unchanged
		requireJobHasState(ineligibleJob1.ID, ineligibleJob1.State)
		requireJobHasState(ineligibleJob2.ID, ineligibleJob2.State)
		requireJobHasState(ineligibleJob3.ID, ineligibleJob3.State)

		// Jobs to retry should be retryable:
		requireJobHasState(jobStuckToRetry1.ID, rivertype.JobStateRetryable)
		requireJobHasState(jobStuckToRetry2.ID, rivertype.JobStateRetryable)

		// This one should be discarded because it's already at MaxAttempts:
		requireJobHasState(jobStuckToDiscard.ID, rivertype.JobStateDiscarded)

		// not eligible for rescue, not stuck long enough yet:
		requireJobHasState(jobNotYetStuck1.ID, jobNotYetStuck1.State)
		requireJobHasState(jobNotYetStuck2.ID, jobNotYetStuck2.State)
		requireJobHasState(jobNotYetStuck3.ID, jobNotYetStuck3.State)
	})

	t.Run("JobScheduler", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)
		config.Queues = map[string]QueueConfig{"another_queue": {MaxWorkers: 1}} // don't work jobs on the default queue we're using in this test

		client, bundle := setup(t, config)

		now := time.Now()

		// Take care to insert jobs before starting the client because otherwise
		// there's a race condition where the scheduler could run its initial
		// pass before our insertion is complete.
		ineligibleJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		ineligibleJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		ineligibleJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})

		jobInPast1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})
		jobInPast2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Minute))})
		jobInPast3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})

		// Will not be scheduled.
		jobInFuture1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(now.Add(1 * time.Hour))})
		jobInFuture2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(now.Add(1 * time.Minute))})
		jobInFuture3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(now.Add(10 * time.Second))})

		startAndWaitForQueueMaintainer(ctx, t, client)

		scheduler := maintenance.GetService[*maintenance.JobScheduler](client.queueMaintainer)
		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()

		requireJobHasState := func(jobID int64, state rivertype.JobState) {
			t.Helper()
			job, err := client.JobGet(ctx, jobID)
			require.NoError(t, err)
			require.Equal(t, state, job.State)
		}

		// unchanged
		requireJobHasState(ineligibleJob1.ID, ineligibleJob1.State)
		requireJobHasState(ineligibleJob2.ID, ineligibleJob2.State)
		requireJobHasState(ineligibleJob3.ID, ineligibleJob3.State)

		// Jobs with past timestamps should be now be made available:
		requireJobHasState(jobInPast1.ID, rivertype.JobStateAvailable)
		requireJobHasState(jobInPast2.ID, rivertype.JobStateAvailable)
		requireJobHasState(jobInPast3.ID, rivertype.JobStateAvailable)

		// not scheduled, still in future
		requireJobHasState(jobInFuture1.ID, jobInFuture1.State)
		requireJobHasState(jobInFuture2.ID, jobInFuture2.State)
		requireJobHasState(jobInFuture3.ID, jobInFuture3.State)
	})

	t.Run("PeriodicJobEnqueuerWithInsertOpts", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)

		worker := &periodicJobWorker{}
		AddWorker(config.Workers, worker)
		config.PeriodicJobs = []*PeriodicJob{
			NewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {
				return periodicJobArgs{}, nil
			}, &PeriodicJobOpts{RunOnStart: true}),
		}

		client, bundle := setup(t, config)

		startAndWaitForQueueMaintainer(ctx, t, client)

		svc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
		svc.TestSignals.InsertedJobs.WaitOrTimeout()

		jobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 1, "Expected to find exactly one job of kind: "+(periodicJobArgs{}).Kind())
	})

	t.Run("PeriodicJobEnqueuerNoInsertOpts", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)

		worker := &periodicJobWorker{}
		AddWorker(config.Workers, worker)
		config.PeriodicJobs = []*PeriodicJob{
			NewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {
				return periodicJobArgs{}, nil
			}, nil),
		}

		client, bundle := setup(t, config)

		startAndWaitForQueueMaintainer(ctx, t, client)

		svc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
		svc.TestSignals.EnteredLoop.WaitOrTimeout()

		// No jobs yet because the RunOnStart option was not specified.
		jobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})
		require.NoError(t, err)
		require.Empty(t, jobs)
	})

	t.Run("PeriodicJobConstructorReturningNil", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)

		worker := &periodicJobWorker{}
		AddWorker(config.Workers, worker)
		config.PeriodicJobs = []*PeriodicJob{
			NewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {
				// Returning nil from the constructor function should not insert a new
				// job and should be handled cleanly
				return nil, nil
			}, &PeriodicJobOpts{RunOnStart: true}),
		}

		client, bundle := setup(t, config)

		startAndWaitForQueueMaintainer(ctx, t, client)

		svc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
		svc.TestSignals.SkippedJob.WaitOrTimeout()

		jobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})
		require.NoError(t, err)
		require.Empty(t, jobs, "Expected to find zero jobs of kind: "+(periodicJobArgs{}).Kind())
	})

	t.Run("PeriodicJobEnqueuerAddDynamically", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)

		worker := &periodicJobWorker{}
		AddWorker(config.Workers, worker)

		dbPool := riverinternaltest.TestDB(ctx, t)

		client := newTestClient(t, dbPool, config)
		client.testSignals.Init()
		startClient(ctx, t, client)

		exec := client.driver.GetExecutor()

		client.testSignals.electedLeader.WaitOrTimeout()

		client.PeriodicJobs().Add(
			NewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {
				return periodicJobArgs{}, nil
			}, &PeriodicJobOpts{RunOnStart: true}),
		)

		svc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
		svc.TestSignals.EnteredLoop.WaitOrTimeout()
		svc.TestSignals.InsertedJobs.WaitOrTimeout()

		// We get a queued job because RunOnStart was specified.
		jobs, err := exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 1)
	})

	t.Run("PeriodicJobEnqueuerRemoveDynamically", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)

		worker := &periodicJobWorker{}
		AddWorker(config.Workers, worker)

		client := newTestClient(t, riverinternaltest.TestDB(ctx, t), config)
		client.testSignals.Init()
		exec := client.driver.GetExecutor()

		handle := client.PeriodicJobs().Add(
			NewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {
				return periodicJobArgs{}, nil
			}, &PeriodicJobOpts{RunOnStart: true}),
		)

		startClient(ctx, t, client)

		client.testSignals.electedLeader.WaitOrTimeout()

		svc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
		svc.TestSignals.EnteredLoop.WaitOrTimeout()
		svc.TestSignals.InsertedJobs.WaitOrTimeout()

		client.PeriodicJobs().Remove(handle)

		type OtherPeriodicArgs struct {
			JobArgsReflectKind[OtherPeriodicArgs]
		}

		client.PeriodicJobs().Add(
			NewPeriodicJob(cron.Every(15*time.Minute), func() (JobArgs, *InsertOpts) {
				return OtherPeriodicArgs{}, nil
			}, &PeriodicJobOpts{RunOnStart: true}),
		)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()

		// One of each because the first periodic job was inserted on the first
		// go around due to RunOnStart, but then subsequently removed. The next
		// periodic job was inserted also due to RunOnStart, but only after the
		// first was removed.
		{
			jobs, err := exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})
			require.NoError(t, err)
			require.Len(t, jobs, 1)
		}
		{
			jobs, err := exec.JobGetByKindMany(ctx, []string{(OtherPeriodicArgs{}).Kind()})
			require.NoError(t, err)
			require.Len(t, jobs, 1)
		}
	})

	t.Run("PeriodicJobEnqueuerUsesMiddleware", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)

		worker := &periodicJobWorker{}
		AddWorker(config.Workers, worker)
		config.PeriodicJobs = []*PeriodicJob{
			NewPeriodicJob(cron.Every(time.Minute), func() (JobArgs, *InsertOpts) {
				return periodicJobArgs{}, nil
			}, &PeriodicJobOpts{RunOnStart: true}),
		}
		config.JobInsertMiddleware = []rivertype.JobInsertMiddleware{&overridableJobMiddleware{
			insertManyFunc: func(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
				for _, job := range manyParams {
					job.EncodedArgs = []byte(`{"from": "middleware"}`)
				}
				return doInner(ctx)
			},
		}}

		client, bundle := setup(t, config)

		startAndWaitForQueueMaintainer(ctx, t, client)

		svc := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
		svc.TestSignals.InsertedJobs.WaitOrTimeout()

		jobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(periodicJobArgs{}).Kind()})
		require.NoError(t, err)
		require.Len(t, jobs, 1, "Expected to find exactly one job of kind: "+(periodicJobArgs{}).Kind())
	})

	t.Run("QueueCleaner", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)
		client.testSignals.Init()
		exec := client.driver.GetExecutor()

		deleteHorizon := time.Now().Add(-maintenance.QueueRetentionPeriodDefault)

		// Take care to insert queues before starting the client because otherwise
		// there's a race condition where the cleaner could run its initial
		// pass before our insertion is complete.
		queueBeyondHorizon1 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})
		queueBeyondHorizon2 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})
		queueBeyondHorizon3 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(-1 * time.Hour))})

		// Will not be deleted.
		queueWithinHorizon1 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})
		queueWithinHorizon2 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})
		queueWithinHorizon3 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(deleteHorizon.Add(1 * time.Hour))})

		startClient(ctx, t, client)

		client.testSignals.electedLeader.WaitOrTimeout()
		qc := maintenance.GetService[*maintenance.QueueCleaner](client.queueMaintainer)
		qc.TestSignals.DeletedBatch.WaitOrTimeout()

		var err error
		_, err = client.QueueGet(ctx, queueBeyondHorizon1.Name)
		require.ErrorIs(t, err, ErrNotFound)
		_, err = client.QueueGet(ctx, queueBeyondHorizon2.Name)
		require.ErrorIs(t, err, ErrNotFound)
		_, err = client.QueueGet(ctx, queueBeyondHorizon3.Name)
		require.ErrorIs(t, err, ErrNotFound)

		_, err = client.QueueGet(ctx, queueWithinHorizon1.Name)
		require.NotErrorIs(t, err, ErrNotFound) // still there
		_, err = client.QueueGet(ctx, queueWithinHorizon2.Name)
		require.NotErrorIs(t, err, ErrNotFound) // still there
		_, err = client.QueueGet(ctx, queueWithinHorizon3.Name)
		require.NotErrorIs(t, err, ErrNotFound) // still there
	})

	t.Run("Reindexer", func(t *testing.T) {
		t.Parallel()

		config := newTestConfig(t, nil)
		config.ReindexerSchedule = &runOnceSchedule{}

		client, _ := setup(t, config)

		startAndWaitForQueueMaintainer(ctx, t, client)

		svc := maintenance.GetService[*maintenance.Reindexer](client.queueMaintainer)
		// There are two indexes to reindex by default:
		svc.TestSignals.Reindexed.WaitOrTimeout()
		svc.TestSignals.Reindexed.WaitOrTimeout()
	})
}

type runOnceSchedule struct {
	ran atomic.Bool
}

func (s *runOnceSchedule) Next(time.Time) time.Time {
	if !s.ran.Swap(true) {
		return time.Now()
	}
	// Return the maximum future time so that the schedule doesn't run again.
	return time.Unix(1<<63-1, 0)
}

func Test_Client_QueueGet(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{}
	}

	t.Run("FetchesAnExistingQueue", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		queue := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)

		queueRes, err := client.QueueGet(ctx, queue.Name)
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), queueRes.CreatedAt, 2*time.Second)
		require.WithinDuration(t, queue.CreatedAt, queueRes.CreatedAt, time.Millisecond)
		require.Equal(t, []byte("{}"), queueRes.Metadata)
		require.Equal(t, queue.Name, queueRes.Name)
		require.Nil(t, queueRes.PausedAt)
	})

	t.Run("ReturnsErrNotFoundIfQueueDoesNotExist", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		queueRes, err := client.QueueGet(ctx, "a_queue_that_does_not_exist")
		require.Error(t, err)
		require.ErrorIs(t, err, ErrNotFound)
		require.Nil(t, queueRes)
	})
}

func Test_Client_QueueGetTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		executorTx riverdriver.ExecutorTx
		tx         pgx.Tx
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		tx, err := dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { tx.Rollback(ctx) })

		return client, &testBundle{executorTx: client.driver.UnwrapExecutor(tx), tx: tx}
	}

	t.Run("FetchesAnExistingQueue", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		queue := testfactory.Queue(ctx, t, bundle.executorTx, nil)

		queueRes, err := client.QueueGetTx(ctx, bundle.tx, queue.Name)
		require.NoError(t, err)
		require.Equal(t, queue.Name, queueRes.Name)

		// Not visible outside of transaction.
		_, err = client.QueueGet(ctx, queue.Name)
		require.Error(t, err)
		require.ErrorIs(t, err, ErrNotFound)
	})

	t.Run("ReturnsErrNotFoundIfQueueDoesNotExist", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		queueRes, err := client.QueueGet(ctx, "a_queue_that_does_not_exist")
		require.Error(t, err)
		require.ErrorIs(t, err, ErrNotFound)
		require.Nil(t, queueRes)
	})
}

func Test_Client_QueueList(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		return client, &testBundle{}
	}

	t.Run("ListsAndPaginatesQueues", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		requireQueuesEqual := func(t *testing.T, target, actual *rivertype.Queue) {
			t.Helper()
			require.WithinDuration(t, target.CreatedAt, actual.CreatedAt, time.Millisecond)
			require.Equal(t, target.Metadata, actual.Metadata)
			require.Equal(t, target.Name, actual.Name)
			if target.PausedAt == nil {
				require.Nil(t, actual.PausedAt)
			} else {
				require.NotNil(t, actual.PausedAt)
				require.WithinDuration(t, *target.PausedAt, *actual.PausedAt, time.Millisecond)
			}
		}

		listRes, err := client.QueueList(ctx, NewQueueListParams().First(2))
		require.NoError(t, err)
		require.Empty(t, listRes.Queues)

		// Make queue1, pause it, refetch:
		queue1 := testfactory.Queue(ctx, t, client.driver.GetExecutor(), &testfactory.QueueOpts{Metadata: []byte(`{"foo": "bar"}`)})
		require.NoError(t, client.QueuePause(ctx, queue1.Name, nil))
		queue1, err = client.QueueGet(ctx, queue1.Name)
		require.NoError(t, err)

		queue2 := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)
		queue3 := testfactory.Queue(ctx, t, client.driver.GetExecutor(), nil)

		listRes, err = client.QueueList(ctx, NewQueueListParams().First(2))
		require.NoError(t, err)
		require.Len(t, listRes.Queues, 2)
		requireQueuesEqual(t, queue1, listRes.Queues[0])
		requireQueuesEqual(t, queue2, listRes.Queues[1])

		listRes, err = client.QueueList(ctx, NewQueueListParams().First(3))
		require.NoError(t, err)
		require.Len(t, listRes.Queues, 3)
		requireQueuesEqual(t, queue3, listRes.Queues[2])

		listRes, err = client.QueueList(ctx, NewQueueListParams().First(10))
		require.NoError(t, err)
		require.Len(t, listRes.Queues, 3)
	})
}

func Test_Client_QueueListTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		executorTx riverdriver.ExecutorTx
		tx         pgx.Tx
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)

		tx, err := dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { tx.Rollback(ctx) })

		return client, &testBundle{executorTx: client.driver.UnwrapExecutor(tx), tx: tx}
	}

	t.Run("ListsQueues", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		listRes, err := client.QueueListTx(ctx, bundle.tx, NewQueueListParams())
		require.NoError(t, err)
		require.Empty(t, listRes.Queues)

		queue := testfactory.Queue(ctx, t, bundle.executorTx, nil)

		listRes, err = client.QueueListTx(ctx, bundle.tx, NewQueueListParams())
		require.NoError(t, err)
		require.Len(t, listRes.Queues, 1)
		require.Equal(t, queue.Name, listRes.Queues[0].Name)

		// Not visible outside of transaction.
		listRes, err = client.QueueList(ctx, NewQueueListParams())
		require.NoError(t, err)
		require.Empty(t, listRes.Queues)
	})
}

func Test_Client_RetryPolicy(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	requireInsert := func(ctx context.Context, client *Client[pgx.Tx]) *rivertype.JobRow {
		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(t, err)
		return insertRes.Job
	}

	t.Run("RetryUntilDiscarded", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return errors.New("job error")
		})

		// The default policy would work too, but this takes some variability
		// out of it to make comparisons easier.
		config.RetryPolicy = &retrypolicytest.RetryPolicyNoJitter{}

		client := newTestClient(t, dbPool, config)

		subscribeChan, cancel := client.Subscribe(EventKindJobCompleted, EventKindJobFailed)
		t.Cleanup(cancel)

		originalJobs := make([]*rivertype.JobRow, rivercommon.MaxAttemptsDefault)
		for i := 0; i < len(originalJobs); i++ {
			job := requireInsert(ctx, client)
			// regression protection to ensure we're testing the right number of jobs:
			require.Equal(t, rivercommon.MaxAttemptsDefault, job.MaxAttempts)

			updatedJob, err := client.driver.GetExecutor().JobUpdate(ctx, &riverdriver.JobUpdateParams{
				ID:                  job.ID,
				AttemptedAtDoUpdate: true,
				AttemptedAt:         ptrutil.Ptr(time.Now().UTC()),
				AttemptDoUpdate:     true,
				Attempt:             i, // starts at i, but will be i + 1 by the time it's being worked

				// Need to find a cleaner way around this, but state is required
				// because sqlc can't encode an empty string to the
				// corresponding enum. This value is not actually used because
				// StateDoUpdate was not supplied.
				State: rivertype.JobStateAvailable,
			})
			require.NoError(t, err)

			originalJobs[i] = updatedJob
		}

		startClient(ctx, t, client)

		// Wait for the expected number of jobs to be finished.
		for i := range originalJobs {
			t.Logf("Waiting on job %d", i)
			_ = riversharedtest.WaitOrTimeout(t, subscribeChan)
		}

		finishedJobs, err := client.driver.GetExecutor().JobGetByIDMany(ctx,
			sliceutil.Map(originalJobs, func(m *rivertype.JobRow) int64 { return m.ID }))
		require.NoError(t, err)

		// Jobs aren't guaranteed to come back out of the queue in the same
		// order that we inserted them, so make sure to compare using a lookup
		// map.
		finishedJobsByID := sliceutil.KeyBy(finishedJobs,
			func(m *rivertype.JobRow) (int64, *rivertype.JobRow) { return m.ID, m })

		for i, originalJob := range originalJobs {
			// This loop will check all jobs that were to be rescheduled, but
			// not the final job which is discarded.
			if i >= len(originalJobs)-1 {
				continue
			}

			finishedJob := finishedJobsByID[originalJob.ID]

			// We need to advance the original job's attempt number to represent
			// how it would've looked after being run through the queue.
			originalJob.Attempt += 1

			expectedNextScheduledAt := client.config.RetryPolicy.NextRetry(originalJob)

			t.Logf("Attempt number %d scheduled %v from original `attempted_at`",
				originalJob.Attempt, finishedJob.ScheduledAt.Sub(*originalJob.AttemptedAt))
			t.Logf("    Original attempt at:   %v", originalJob.AttemptedAt)
			t.Logf("    New scheduled at:      %v", finishedJob.ScheduledAt)
			t.Logf("    Expected scheduled at: %v", expectedNextScheduledAt)

			// TODO(brandur): This tolerance could be reduced if we could inject
			// time.Now into adapter which may happen with baseservice
			require.WithinDuration(t, expectedNextScheduledAt, finishedJob.ScheduledAt, 2*time.Second)

			require.Equal(t, rivertype.JobStateRetryable, finishedJob.State)
		}

		// One last discarded job.
		{
			originalJob := originalJobs[len(originalJobs)-1]
			finishedJob := finishedJobsByID[originalJob.ID]

			originalJob.Attempt += 1

			t.Logf("Attempt number %d discarded", originalJob.Attempt)

			// TODO(brandur): See note on tolerance above.
			require.WithinDuration(t, time.Now(), *finishedJob.FinalizedAt, 2*time.Second)
			require.Equal(t, rivertype.JobStateDiscarded, finishedJob.State)
		}
	})
}

func Test_Client_Subscribe(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	keyEventsByName := func(events []*Event) map[string]*Event {
		return sliceutil.KeyBy(events, func(event *Event) (string, *Event) {
			var args callbackArgs
			require.NoError(t, json.Unmarshal(event.Job.EncodedArgs, &args))
			return args.Name, event
		})
	}

	requireInsert := func(ctx context.Context, client *Client[pgx.Tx], jobName string) *rivertype.JobRow {
		insertRes, err := client.Insert(ctx, callbackArgs{Name: jobName}, nil)
		require.NoError(t, err)
		return insertRes.Job
	}

	t.Run("Success", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		// Fail/succeed jobs based on their name so we can get a mix of both to
		// verify.
		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			if strings.HasPrefix(job.Args.Name, "failed") {
				return errors.New("job error")
			}
			return nil
		})

		client := newTestClient(t, dbPool, config)

		subscribeChan, cancel := client.Subscribe(EventKindJobCompleted, EventKindJobFailed)
		t.Cleanup(cancel)

		jobCompleted1 := requireInsert(ctx, client, "completed1")
		jobCompleted2 := requireInsert(ctx, client, "completed2")
		jobFailed1 := requireInsert(ctx, client, "failed1")
		jobFailed2 := requireInsert(ctx, client, "failed2")

		expectedJobs := []*rivertype.JobRow{
			jobCompleted1,
			jobCompleted2,
			jobFailed1,
			jobFailed2,
		}

		startClient(ctx, t, client)

		events := make([]*Event, len(expectedJobs))

		for i := range expectedJobs {
			events[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)
		}

		eventsByName := keyEventsByName(events)

		{
			eventCompleted1 := eventsByName["completed1"]
			require.Equal(t, EventKindJobCompleted, eventCompleted1.Kind)
			require.Equal(t, jobCompleted1.ID, eventCompleted1.Job.ID)
			require.Equal(t, rivertype.JobStateCompleted, eventCompleted1.Job.State)
		}

		{
			eventCompleted2 := eventsByName["completed2"]
			require.Equal(t, EventKindJobCompleted, eventCompleted2.Kind)
			require.Equal(t, jobCompleted2.ID, eventCompleted2.Job.ID)
			require.Equal(t, rivertype.JobStateCompleted, eventCompleted2.Job.State)
		}

		{
			eventFailed1 := eventsByName["failed1"]
			require.Equal(t, EventKindJobFailed, eventFailed1.Kind)
			require.Equal(t, jobFailed1.ID, eventFailed1.Job.ID)
			require.Equal(t, rivertype.JobStateRetryable, eventFailed1.Job.State)
		}

		{
			eventFailed2 := eventsByName["failed2"]
			require.Equal(t, EventKindJobFailed, eventFailed2.Kind)
			require.Equal(t, jobFailed2.ID, eventFailed2.Job.ID)
			require.Equal(t, rivertype.JobStateRetryable, eventFailed2.Job.State)
		}
	})

	t.Run("CompletedOnly", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			if strings.HasPrefix(job.Args.Name, "failed") {
				return errors.New("job error")
			}
			return nil
		})

		client := newTestClient(t, dbPool, config)

		subscribeChan, cancel := client.Subscribe(EventKindJobCompleted)
		t.Cleanup(cancel)

		jobCompleted := requireInsert(ctx, client, "completed1")
		requireInsert(ctx, client, "failed1")

		expectedJobs := []*rivertype.JobRow{
			jobCompleted,
		}

		startClient(ctx, t, client)

		events := make([]*Event, len(expectedJobs))

		for i := range expectedJobs {
			events[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)
		}

		eventsByName := keyEventsByName(events)

		eventCompleted := eventsByName["completed1"]
		require.Equal(t, EventKindJobCompleted, eventCompleted.Kind)
		require.Equal(t, jobCompleted.ID, eventCompleted.Job.ID)
		require.Equal(t, rivertype.JobStateCompleted, eventCompleted.Job.State)

		_, ok := eventsByName["failed1"] // filtered out
		require.False(t, ok)
	})

	t.Run("FailedOnly", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			if strings.HasPrefix(job.Args.Name, "failed") {
				return errors.New("job error")
			}
			return nil
		})

		client := newTestClient(t, dbPool, config)

		subscribeChan, cancel := client.Subscribe(EventKindJobFailed)
		t.Cleanup(cancel)

		requireInsert(ctx, client, "completed1")
		jobFailed := requireInsert(ctx, client, "failed1")

		expectedJobs := []*rivertype.JobRow{
			jobFailed,
		}

		startClient(ctx, t, client)

		events := make([]*Event, len(expectedJobs))

		for i := range expectedJobs {
			events[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)
		}

		eventsByName := keyEventsByName(events)

		_, ok := eventsByName["completed1"] // filtered out
		require.False(t, ok)

		eventFailed := eventsByName["failed1"]
		require.Equal(t, EventKindJobFailed, eventFailed.Kind)
		require.Equal(t, jobFailed.ID, eventFailed.Job.ID)
		require.Equal(t, rivertype.JobStateRetryable, eventFailed.Job.State)
	})

	t.Run("PanicOnUnknownKind", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		})

		client := newTestClient(t, dbPool, config)

		require.PanicsWithError(t, "unknown event kind: does_not_exist", func() {
			_, _ = client.Subscribe(EventKind("does_not_exist"))
		})
	})

	t.Run("SubscriptionCancellation", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		})

		client := newTestClient(t, dbPool, config)

		subscribeChan, cancel := client.Subscribe(EventKindJobCompleted)
		cancel()

		// Drops through immediately because the channel is closed.
		riversharedtest.WaitOrTimeout(t, subscribeChan)

		require.Empty(t, client.subscriptionManager.subscriptions)
	})

	// Just make sure this doesn't fail on a nil pointer exception.
	t.Run("SubscribeOnClientWithoutWorkers", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		client := newTestClient(t, dbPool, &Config{})

		require.PanicsWithValue(t, "created a subscription on a client that will never work jobs (Queues not configured)", func() {
			_, _ = client.Subscribe(EventKindJobCompleted)
		})
	})
}

// SubscribeConfig uses all the same code as Subscribe, so these are just a
// minimal set of new tests to make sure that the function also works when used
// independently.
func Test_Client_SubscribeConfig(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	keyEventsByName := func(events []*Event) map[string]*Event {
		return sliceutil.KeyBy(events, func(event *Event) (string, *Event) {
			var args callbackArgs
			require.NoError(t, json.Unmarshal(event.Job.EncodedArgs, &args))
			return args.Name, event
		})
	}

	requireInsert := func(ctx context.Context, client *Client[pgx.Tx], jobName string) *rivertype.JobRow {
		insertRes, err := client.Insert(ctx, callbackArgs{Name: jobName}, nil)
		require.NoError(t, err)
		return insertRes.Job
	}

	t.Run("Success", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		// Fail/succeed jobs based on their name so we can get a mix of both to
		// verify.
		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			if strings.HasPrefix(job.Args.Name, "failed") {
				return errors.New("job error")
			}
			return nil
		})

		client := newTestClient(t, dbPool, config)

		subscribeChan, cancel := client.SubscribeConfig(&SubscribeConfig{
			Kinds: []EventKind{EventKindJobCompleted, EventKindJobFailed},
		})
		t.Cleanup(cancel)

		jobCompleted1 := requireInsert(ctx, client, "completed1")
		jobCompleted2 := requireInsert(ctx, client, "completed2")
		jobFailed1 := requireInsert(ctx, client, "failed1")
		jobFailed2 := requireInsert(ctx, client, "failed2")

		expectedJobs := []*rivertype.JobRow{
			jobCompleted1,
			jobCompleted2,
			jobFailed1,
			jobFailed2,
		}

		startClient(ctx, t, client)

		events := make([]*Event, len(expectedJobs))

		for i := range expectedJobs {
			events[i] = riversharedtest.WaitOrTimeout(t, subscribeChan)
		}

		eventsByName := keyEventsByName(events)

		{
			eventCompleted1 := eventsByName["completed1"]
			require.Equal(t, EventKindJobCompleted, eventCompleted1.Kind)
			require.Equal(t, jobCompleted1.ID, eventCompleted1.Job.ID)
			require.Equal(t, rivertype.JobStateCompleted, eventCompleted1.Job.State)
		}

		{
			eventCompleted2 := eventsByName["completed2"]
			require.Equal(t, EventKindJobCompleted, eventCompleted2.Kind)
			require.Equal(t, jobCompleted2.ID, eventCompleted2.Job.ID)
			require.Equal(t, rivertype.JobStateCompleted, eventCompleted2.Job.State)
		}

		{
			eventFailed1 := eventsByName["failed1"]
			require.Equal(t, EventKindJobFailed, eventFailed1.Kind)
			require.Equal(t, jobFailed1.ID, eventFailed1.Job.ID)
			require.Equal(t, rivertype.JobStateRetryable, eventFailed1.Job.State)
		}

		{
			eventFailed2 := eventsByName["failed2"]
			require.Equal(t, EventKindJobFailed, eventFailed2.Kind)
			require.Equal(t, jobFailed2.ID, eventFailed2.Job.ID)
			require.Equal(t, rivertype.JobStateRetryable, eventFailed2.Job.State)
		}
	})

	t.Run("EventsDropWithNoListeners", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		})

		client := newTestClient(t, dbPool, config)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return nil
		}))

		// A first channel that we'll use to make sure all the expected jobs are
		// finished.
		subscribeChan, cancel := client.Subscribe(EventKindJobCompleted)
		t.Cleanup(cancel)

		// Artificially lowered subscribe channel size so we don't have to try
		// and process thousands of jobs.
		const (
			subscribeChanSize = 100
			numJobsToInsert   = subscribeChanSize + 1
		)

		// Another channel with no listeners. Despite no listeners, it shouldn't
		// block or gum up the client's progress in any way.
		subscribeChan2, cancel := client.SubscribeConfig(&SubscribeConfig{
			ChanSize: subscribeChanSize,
			Kinds:    []EventKind{EventKindJobCompleted},
		})
		t.Cleanup(cancel)

		var (
			insertParams = make([]*riverdriver.JobInsertFastParams, numJobsToInsert)
			kind         = (&JobArgs{}).Kind()
		)
		for i := range numJobsToInsert {
			insertParams[i] = &riverdriver.JobInsertFastParams{
				Args:        &JobArgs{},
				EncodedArgs: []byte(`{}`),
				Kind:        kind,
				MaxAttempts: rivercommon.MaxAttemptsDefault,
				Priority:    rivercommon.PriorityDefault,
				Queue:       rivercommon.QueueDefault,
				State:       rivertype.JobStateAvailable,
			}
		}

		_, err := client.driver.GetExecutor().JobInsertFastMany(ctx, insertParams)
		require.NoError(t, err)

		// Need to start waiting on events before running the client or the
		// channel could overflow before we start listening.
		var wg sync.WaitGroup
		wg.Add(1)
		go func() {
			defer wg.Done()
			_ = riversharedtest.WaitOrTimeoutN(t, subscribeChan, numJobsToInsert)
		}()

		startClient(ctx, t, client)

		wg.Wait()

		// Filled to maximum.
		require.Len(t, subscribeChan2, subscribeChanSize)
	})

	t.Run("PanicOnChanSizeLessThanZero", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		})

		client := newTestClient(t, dbPool, config)

		require.PanicsWithValue(t, "SubscribeConfig.ChanSize must be greater or equal to 1", func() {
			_, _ = client.SubscribeConfig(&SubscribeConfig{
				ChanSize: -1,
			})
		})
	})

	t.Run("PanicOnUnknownKind", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		})

		client := newTestClient(t, dbPool, config)

		require.PanicsWithError(t, "unknown event kind: does_not_exist", func() {
			_, _ = client.SubscribeConfig(&SubscribeConfig{
				Kinds: []EventKind{EventKind("does_not_exist")},
			})
		})
	})
}

func Test_Client_InsertTriggersImmediateWork(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	require := require.New(t)

	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	t.Cleanup(cancel)

	doneCh := make(chan struct{})
	close(doneCh) // don't need to block any jobs from completing
	startedCh := make(chan int64)

	dbPool := riverinternaltest.TestDB(ctx, t)

	config := newTestConfig(t, makeAwaitCallback(startedCh, doneCh))
	config.FetchCooldown = 20 * time.Millisecond
	config.FetchPollInterval = 20 * time.Second // essentially disable polling
	config.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}, "another_queue": {MaxWorkers: 1}}

	client := newTestClient(t, dbPool, config)

	startClient(ctx, t, client)
	riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

	insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
	require.NoError(err)

	// Wait for the client to be ready by waiting for a job to be executed:
	select {
	case jobID := <-startedCh:
		require.Equal(insertRes.Job.ID, jobID)
	case <-ctx.Done():
		t.Fatal("timed out waiting for warmup job to start")
	}

	// Now that we've run one job, we shouldn't take longer than the cooldown to
	// fetch another after insertion. LISTEN/NOTIFY should ensure we find out
	// about the inserted job much faster than the poll interval.
	//
	// Note: we specifically use a different queue to ensure that the notify
	// limiter is immediately to fire on this queue.
	insertRes2, err := client.Insert(ctx, callbackArgs{}, &InsertOpts{Queue: "another_queue"})
	require.NoError(err)

	select {
	case jobID := <-startedCh:
		require.Equal(insertRes2.Job.ID, jobID)
	// As long as this is meaningfully shorter than the poll interval, we can be
	// sure the re-fetch came from listen/notify.
	case <-time.After(5 * time.Second):
		t.Fatal("timed out waiting for 2nd job to start")
	}

	require.NoError(client.Stop(ctx))
}

func Test_Client_InsertNotificationsAreDeduplicatedAndDebounced(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	dbPool := riverinternaltest.TestDB(ctx, t)
	config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
		return nil
	})
	config.FetchPollInterval = 20 * time.Second // essentially disable polling
	config.FetchCooldown = time.Second
	config.schedulerInterval = 20 * time.Second // quiet scheduler
	config.Queues = map[string]QueueConfig{"queue1": {MaxWorkers: 1}, "queue2": {MaxWorkers: 1}, "queue3": {MaxWorkers: 1}}
	client := newTestClient(t, dbPool, config)

	startClient(ctx, t, client)
	riversharedtest.WaitOrTimeout(t, client.baseStartStop.Started())

	type insertPayload struct {
		Queue string `json:"queue"`
	}
	type notification struct {
		topic   notifier.NotificationTopic
		payload insertPayload
	}
	notifyCh := make(chan notification, 10)
	handleNotification := func(topic notifier.NotificationTopic, payload string) {
		config.Logger.Info("received notification", slog.String("topic", string(topic)), slog.String("payload", payload))
		notif := notification{topic: topic}
		require.NoError(t, json.Unmarshal([]byte(payload), &notif.payload))
		notifyCh <- notif
	}
	sub, err := client.notifier.Listen(ctx, notifier.NotificationTopicInsert, handleNotification)
	require.NoError(t, err)
	t.Cleanup(func() { sub.Unlisten(ctx) })

	expectImmediateNotification := func(t *testing.T, queue string) {
		t.Helper()
		config.Logger.Info("inserting " + queue + " job")
		_, err = client.Insert(ctx, callbackArgs{}, &InsertOpts{Queue: queue})
		require.NoError(t, err)
		notif := riversharedtest.WaitOrTimeout(t, notifyCh)
		require.Equal(t, notifier.NotificationTopicInsert, notif.topic)
		require.Equal(t, queue, notif.payload.Queue)
	}

	// Immediate first fire on queue1:
	expectImmediateNotification(t, "queue1")
	tNotif1 := time.Now()

	for range 5 {
		config.Logger.Info("inserting queue1 job")
		_, err = client.Insert(ctx, callbackArgs{}, &InsertOpts{Queue: "queue1"})
		require.NoError(t, err)
	}
	// None of these should fire an insert notification due to debouncing:
	select {
	case notification := <-notifyCh:
		t.Fatalf("received insert notification when it should have been debounced %+v", notification)
	case <-time.After(100 * time.Millisecond):
	}

	expectImmediateNotification(t, "queue2") // Immediate first fire on queue2
	expectImmediateNotification(t, "queue3") // Immediate first fire on queue3

	// Wait until the queue1 cooldown period has passed:
	<-time.After(time.Until(tNotif1.Add(config.FetchCooldown)))

	// Now we should receive an immediate notification again:
	expectImmediateNotification(t, "queue1")
}

func Test_Client_JobCompletion(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		DBPool        *pgxpool.Pool
		SubscribeChan <-chan *Event
	}

	setup := func(t *testing.T, config *Config) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		client := newTestClient(t, dbPool, config)
		startClient(ctx, t, client)

		subscribeChan, cancel := client.Subscribe(EventKindJobCancelled, EventKindJobCompleted, EventKindJobFailed)
		t.Cleanup(cancel)

		return client, &testBundle{
			DBPool:        dbPool,
			SubscribeChan: subscribeChan,
		}
	}

	t.Run("JobThatReturnsNilIsCompleted", func(t *testing.T) {
		t.Parallel()

		require := require.New(t)
		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return nil
		})

		client, bundle := setup(t, config)

		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(err)

		event := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)
		require.Equal(insertRes.Job.ID, event.Job.ID)
		require.Equal(rivertype.JobStateCompleted, event.Job.State)

		reloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(err)

		require.Equal(rivertype.JobStateCompleted, reloadedJob.State)
		require.WithinDuration(time.Now(), *reloadedJob.FinalizedAt, 2*time.Second)
	})

	t.Run("JobThatIsAlreadyCompletedIsNotAlteredByCompleter", func(t *testing.T) {
		t.Parallel()

		require := require.New(t)
		var exec riverdriver.Executor
		now := time.Now().UTC()
		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			_, err := exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{
				ID:                  job.ID,
				FinalizedAtDoUpdate: true,
				FinalizedAt:         &now,
				StateDoUpdate:       true,
				State:               rivertype.JobStateCompleted,
			})
			require.NoError(err)
			return nil
		})

		client, bundle := setup(t, config)
		exec = client.driver.GetExecutor()

		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(err)

		event := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)
		require.Equal(insertRes.Job.ID, event.Job.ID)
		require.Equal(rivertype.JobStateCompleted, event.Job.State)

		reloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(err)

		require.Equal(rivertype.JobStateCompleted, reloadedJob.State)
		require.WithinDuration(now, *reloadedJob.FinalizedAt, time.Microsecond)
	})

	t.Run("JobThatReturnsErrIsRetryable", func(t *testing.T) {
		t.Parallel()

		require := require.New(t)
		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return errors.New("oops")
		})

		client, bundle := setup(t, config)

		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(err)

		event := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)
		require.Equal(insertRes.Job.ID, event.Job.ID)
		require.Equal(rivertype.JobStateRetryable, event.Job.State)

		reloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(err)

		require.Equal(rivertype.JobStateRetryable, reloadedJob.State)
		require.WithinDuration(time.Now(), reloadedJob.ScheduledAt, 2*time.Second)
		require.Nil(reloadedJob.FinalizedAt)
	})

	t.Run("JobThatReturnsJobCancelErrorIsImmediatelyCancelled", func(t *testing.T) {
		t.Parallel()

		require := require.New(t)
		config := newTestConfig(t, func(ctx context.Context, job *Job[callbackArgs]) error {
			return JobCancel(errors.New("oops"))
		})

		client, bundle := setup(t, config)

		insertRes, err := client.Insert(ctx, callbackArgs{}, nil)
		require.NoError(err)

		event := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)
		require.Equal(insertRes.Job.ID, event.Job.ID)
		require.Equal(rivertype.JobStateCancelled, event.Job.State)

		reloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(err)

		require.Equal(rivertype.JobStateCancelled, reloadedJob.State)
		require.NotNil(reloadedJob.FinalizedAt)
		require.WithinDuration(time.Now(), *reloadedJob.FinalizedAt, 2*time.Second)
	})

	t.Run("JobThatIsAlreadyDiscardedIsNotAlteredByCompleter", func(t *testing.T) {
		t.Parallel()

		require := require.New(t)
		now := time.Now().UTC()

		client, bundle := setup(t, newTestConfig(t, nil))

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			_, err := client.driver.GetExecutor().JobUpdate(ctx, &riverdriver.JobUpdateParams{
				ID:                  job.ID,
				ErrorsDoUpdate:      true,
				Errors:              [][]byte{[]byte("{\"error\": \"oops\"}")},
				FinalizedAtDoUpdate: true,
				FinalizedAt:         &now,
				StateDoUpdate:       true,
				State:               rivertype.JobStateDiscarded,
			})
			require.NoError(err)
			return errors.New("oops")
		}))

		insertRes, err := client.Insert(ctx, JobArgs{}, nil)
		require.NoError(err)

		event := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)
		require.Equal(insertRes.Job.ID, event.Job.ID)
		require.Equal(rivertype.JobStateDiscarded, event.Job.State)

		reloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(err)

		require.Equal(rivertype.JobStateDiscarded, reloadedJob.State)
		require.NotNil(reloadedJob.FinalizedAt)
	})

	t.Run("JobThatIsCompletedManuallyIsNotTouchedByCompleter", func(t *testing.T) {
		t.Parallel()

		require := require.New(t)
		now := time.Now().UTC()

		client, bundle := setup(t, newTestConfig(t, nil))

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		var updatedJob *Job[JobArgs]
		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			tx, err := bundle.DBPool.Begin(ctx)
			require.NoError(err)

			updatedJob, err = JobCompleteTx[*riverpgxv5.Driver](ctx, tx, job)
			require.NoError(err)

			return tx.Commit(ctx)
		}))

		insertRes, err := client.Insert(ctx, JobArgs{}, nil)
		require.NoError(err)

		event := riversharedtest.WaitOrTimeout(t, bundle.SubscribeChan)
		require.Equal(insertRes.Job.ID, event.Job.ID)
		require.Equal(rivertype.JobStateCompleted, event.Job.State)
		require.Equal(rivertype.JobStateCompleted, updatedJob.State)
		require.NotNil(updatedJob)
		require.NotNil(event.Job.FinalizedAt)
		require.NotNil(updatedJob.FinalizedAt)

		// Make sure the FinalizedAt is approximately ~now:
		require.WithinDuration(now, *updatedJob.FinalizedAt, 2*time.Second)

		// Make sure we're getting the same timestamp back from the event and the
		// updated job inside the txn:
		require.WithinDuration(*updatedJob.FinalizedAt, *event.Job.FinalizedAt, time.Microsecond)

		reloadedJob, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(err)

		require.Equal(rivertype.JobStateCompleted, reloadedJob.State)
		require.Equal(updatedJob.FinalizedAt, reloadedJob.FinalizedAt)
	})
}

type unregisteredJobArgs struct{}

func (unregisteredJobArgs) Kind() string { return "RandomWorkerNameThatIsNeverRegistered" }

func Test_Client_UnknownJobKindErrorsTheJob(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	require := require.New(t)

	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	t.Cleanup(cancel)

	doneCh := make(chan struct{})
	close(doneCh) // don't need to block any jobs from completing

	config := newTestConfig(t, nil)
	client := runNewTestClient(ctx, t, config)

	subscribeChan, cancel := client.Subscribe(EventKindJobFailed)
	t.Cleanup(cancel)

	insertParams, err := insertParamsFromConfigArgsAndOptions(&client.baseService.Archetype, config, unregisteredJobArgs{}, nil)
	require.NoError(err)
	insertedResults, err := client.driver.GetExecutor().JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{(*riverdriver.JobInsertFastParams)(insertParams)})
	require.NoError(err)

	insertedResult := insertedResults[0]

	event := riversharedtest.WaitOrTimeout(t, subscribeChan)
	require.Equal(insertedResult.Job.ID, event.Job.ID)
	require.Equal("RandomWorkerNameThatIsNeverRegistered", insertedResult.Job.Kind)
	require.Len(event.Job.Errors, 1)
	require.Equal((&UnknownJobKindError{Kind: "RandomWorkerNameThatIsNeverRegistered"}).Error(), event.Job.Errors[0].Error)
	require.Equal(rivertype.JobStateRetryable, event.Job.State)
	// Ensure that ScheduledAt was updated with next run time:
	require.True(event.Job.ScheduledAt.After(insertedResult.Job.ScheduledAt))
	// It's the 1st attempt that failed. Attempt won't be incremented again until
	// the job gets fetched a 2nd time.
	require.Equal(1, event.Job.Attempt)

	require.NoError(client.Stop(ctx))
}

func Test_Client_Start_Error(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("NoQueueConfiguration", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, nil)
		config.Queues = nil
		config.Workers = nil

		client := newTestClient(t, dbPool, config)
		err := client.Start(ctx)
		require.EqualError(t, err, "client Queues and Workers must be configured for a client to start working")
	})

	t.Run("NoRegisteredWorkers", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)

		config := newTestConfig(t, nil)
		config.Workers = NewWorkers() // initialized, but empty

		client := newTestClient(t, dbPool, config)
		err := client.Start(ctx)
		require.EqualError(t, err, "at least one Worker must be added to the Workers bundle")
	})

	t.Run("DatabaseError", func(t *testing.T) {
		t.Parallel()

		dbConfig := riverinternaltest.DatabaseConfig("does-not-exist-and-dont-create-it")

		dbPool, err := pgxpool.NewWithConfig(ctx, dbConfig)
		require.NoError(t, err)

		config := newTestConfig(t, nil)

		client := newTestClient(t, dbPool, config)

		err = client.Start(ctx)
		require.Error(t, err)

		var pgErr *pgconn.PgError
		require.ErrorAs(t, err, &pgErr)
		require.Equal(t, pgerrcode.InvalidCatalogName, pgErr.Code)
	})
}

func Test_NewClient_BaseServiceName(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	dbPool := riverinternaltest.TestDB(ctx, t)
	client := newTestClient(t, dbPool, newTestConfig(t, nil))
	// Ensure we get the clean name "Client" instead of the fully qualified name
	// with generic type param:
	require.Equal(t, "Client", client.baseService.Name)
}

func Test_NewClient_ClientIDWrittenToJobAttemptedByWhenFetched(t *testing.T) {
	t.Parallel()
	require := require.New(t)
	ctx := context.Background()
	doneCh := make(chan struct{})
	startedCh := make(chan *Job[callbackArgs])

	callback := func(ctx context.Context, job *Job[callbackArgs]) error {
		startedCh <- job
		<-doneCh
		return nil
	}

	client := runNewTestClient(ctx, t, newTestConfig(t, callback))
	t.Cleanup(func() { close(doneCh) })

	// enqueue job:
	insertCtx, insertCancel := context.WithTimeout(ctx, 5*time.Second)
	t.Cleanup(insertCancel)
	insertRes, err := client.Insert(insertCtx, callbackArgs{}, nil)
	require.NoError(err)
	require.Nil(insertRes.Job.AttemptedAt)
	require.Empty(insertRes.Job.AttemptedBy)

	var startedJob *Job[callbackArgs]
	select {
	case startedJob = <-startedCh:
		require.Equal([]string{client.ID()}, startedJob.AttemptedBy)
		require.NotNil(startedJob.AttemptedAt)
		require.WithinDuration(time.Now().UTC(), *startedJob.AttemptedAt, 2*time.Second)
	case <-time.After(5 * time.Second):
		t.Fatal("timed out waiting for job to start")
	}
}

func Test_NewClient_Defaults(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	dbPool := riverinternaltest.TestDB(ctx, t)

	workers := NewWorkers()
	AddWorker(workers, &noOpWorker{})

	client, err := NewClient(riverpgxv5.New(dbPool), &Config{
		Queues:  map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},
		Workers: workers,
	})
	require.NoError(t, err)

	require.Zero(t, client.config.AdvisoryLockPrefix)

	jobCleaner := maintenance.GetService[*maintenance.JobCleaner](client.queueMaintainer)
	require.Equal(t, maintenance.CancelledJobRetentionPeriodDefault, jobCleaner.Config.CancelledJobRetentionPeriod)
	require.Equal(t, maintenance.CompletedJobRetentionPeriodDefault, jobCleaner.Config.CompletedJobRetentionPeriod)
	require.Equal(t, maintenance.DiscardedJobRetentionPeriodDefault, jobCleaner.Config.DiscardedJobRetentionPeriod)
	require.Equal(t, maintenance.JobCleanerTimeoutDefault, jobCleaner.Config.Timeout)
	require.False(t, jobCleaner.StaggerStartupIsDisabled())

	enqueuer := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
	require.Zero(t, enqueuer.Config.AdvisoryLockPrefix)
	require.False(t, enqueuer.StaggerStartupIsDisabled())

	reindexer := maintenance.GetService[*maintenance.Reindexer](client.queueMaintainer)
	require.Equal(t, []string{"river_job_args_index", "river_job_metadata_index"}, reindexer.Config.IndexNames)
	now := time.Now().UTC()
	nextMidnight := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, time.UTC).AddDate(0, 0, 1)
	require.Equal(t, nextMidnight, reindexer.Config.ScheduleFunc(now))

	require.Nil(t, client.config.ErrorHandler)
	require.Equal(t, FetchCooldownDefault, client.config.FetchCooldown)
	require.Equal(t, FetchPollIntervalDefault, client.config.FetchPollInterval)
	require.Equal(t, JobTimeoutDefault, client.config.JobTimeout)
	require.Nil(t, client.config.Hooks)
	require.NotZero(t, client.baseService.Logger)
	require.Equal(t, MaxAttemptsDefault, client.config.MaxAttempts)
	require.IsType(t, &DefaultClientRetryPolicy{}, client.config.RetryPolicy)
	require.False(t, client.config.SkipUnknownJobCheck)
	require.IsType(t, nil, client.config.Test.Time)
	require.IsType(t, &baseservice.UnStubbableTimeGenerator{}, client.baseService.Time)
}

func Test_NewClient_Overrides(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	dbPool := riverinternaltest.TestDB(ctx, t)
	errorHandler := &testErrorHandler{}
	logger := slog.New(slog.NewTextHandler(os.Stderr, nil))

	workers := NewWorkers()
	AddWorker(workers, &noOpWorker{})

	retryPolicy := &DefaultClientRetryPolicy{}

	type noOpHook struct {
		HookDefaults
	}

	type noOpInsertMiddleware struct {
		JobInsertMiddlewareDefaults
	}

	type noOpWorkerMiddleware struct {
		WorkerMiddlewareDefaults
	}

	client, err := NewClient(riverpgxv5.New(dbPool), &Config{
		AdvisoryLockPrefix:          123_456,
		CancelledJobRetentionPeriod: 1 * time.Hour,
		CompletedJobRetentionPeriod: 2 * time.Hour,
		DiscardedJobRetentionPeriod: 3 * time.Hour,
		ErrorHandler:                errorHandler,
		FetchCooldown:               123 * time.Millisecond,
		FetchPollInterval:           124 * time.Millisecond,
		Hooks:                       []rivertype.Hook{&noOpHook{}},
		JobInsertMiddleware:         []rivertype.JobInsertMiddleware{&noOpInsertMiddleware{}},
		JobTimeout:                  125 * time.Millisecond,
		Logger:                      logger,
		MaxAttempts:                 5,
		Queues:                      map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},
		ReindexerSchedule:           &periodicIntervalSchedule{interval: time.Hour},
		RetryPolicy:                 retryPolicy,
		SkipUnknownJobCheck:         true,
		TestOnly:                    true, // disables staggered start in maintenance services
		Workers:                     workers,
		WorkerMiddleware:            []rivertype.WorkerMiddleware{&noOpWorkerMiddleware{}},
	})
	require.NoError(t, err)

	require.Equal(t, int32(123_456), client.config.AdvisoryLockPrefix)

	jobCleaner := maintenance.GetService[*maintenance.JobCleaner](client.queueMaintainer)
	require.Equal(t, 1*time.Hour, jobCleaner.Config.CancelledJobRetentionPeriod)
	require.Equal(t, 2*time.Hour, jobCleaner.Config.CompletedJobRetentionPeriod)
	require.Equal(t, 3*time.Hour, jobCleaner.Config.DiscardedJobRetentionPeriod)
	require.True(t, jobCleaner.StaggerStartupIsDisabled())

	enqueuer := maintenance.GetService[*maintenance.PeriodicJobEnqueuer](client.queueMaintainer)
	require.Equal(t, int32(123_456), enqueuer.Config.AdvisoryLockPrefix)
	require.True(t, enqueuer.StaggerStartupIsDisabled())

	reindexer := maintenance.GetService[*maintenance.Reindexer](client.queueMaintainer)
	now := time.Now().UTC()
	require.Equal(t, now.Add(time.Hour), reindexer.Config.ScheduleFunc(now))

	require.Equal(t, errorHandler, client.config.ErrorHandler)
	require.Equal(t, 123*time.Millisecond, client.config.FetchCooldown)
	require.Equal(t, 124*time.Millisecond, client.config.FetchPollInterval)
	require.Len(t, client.config.JobInsertMiddleware, 1)
	require.Equal(t, 125*time.Millisecond, client.config.JobTimeout)
	require.Equal(t, []rivertype.Hook{&noOpHook{}}, client.config.Hooks)
	require.Equal(t, logger, client.baseService.Logger)
	require.Equal(t, 5, client.config.MaxAttempts)
	require.Equal(t, retryPolicy, client.config.RetryPolicy)
	require.True(t, client.config.SkipUnknownJobCheck)
	require.Len(t, client.config.WorkerMiddleware, 1)
}

func Test_NewClient_MissingParameters(t *testing.T) {
	t.Parallel()

	t.Run("ErrorOnNilDriver", func(t *testing.T) {
		t.Parallel()

		_, err := NewClient[pgx.Tx](nil, &Config{})
		require.ErrorIs(t, err, errMissingDriver)
	})

	t.Run("ErrorOnNilConfig", func(t *testing.T) {
		t.Parallel()

		_, err := NewClient[pgx.Tx](riverpgxv5.New(nil), nil)
		require.ErrorIs(t, err, errMissingConfig)
	})

	t.Run("ErrorOnDriverWithNoDatabasePoolAndQueues", func(t *testing.T) {
		t.Parallel()

		_, err := NewClient[pgx.Tx](riverpgxv5.New(nil), newTestConfig(t, nil))
		require.ErrorIs(t, err, errMissingDatabasePoolWithQueues)
	})
}

func Test_NewClient_Validations(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name           string
		configFunc     func(*Config)
		wantErr        error
		validateResult func(*testing.T, *Client[pgx.Tx])
	}{
		{
			name:       "CompletedJobRetentionPeriod cannot be less than zero",
			configFunc: func(config *Config) { config.CompletedJobRetentionPeriod = -1 * time.Second },
			wantErr:    errors.New("CompletedJobRetentionPeriod cannot be less than zero"),
		},
		{
			name:       "FetchCooldown cannot be less than FetchCooldownMin",
			configFunc: func(config *Config) { config.FetchCooldown = time.Millisecond - 1 },
			wantErr:    errors.New("FetchCooldown must be at least 1ms"),
		},
		{
			name:       "FetchCooldown cannot be negative",
			configFunc: func(config *Config) { config.FetchCooldown = -1 },
			wantErr:    errors.New("FetchCooldown must be at least 1ms"),
		},
		{
			name:       "FetchCooldown defaults to FetchCooldownDefault",
			configFunc: func(config *Config) { config.FetchCooldown = 0 },
			wantErr:    nil,
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Equal(t, FetchCooldownDefault, client.config.FetchCooldown)
			},
		},
		{
			name: "FetchCooldown cannot be less than FetchPollInterval",
			configFunc: func(config *Config) {
				config.FetchCooldown = 20 * time.Millisecond
				config.FetchPollInterval = 19 * time.Millisecond
			},
			wantErr: fmt.Errorf("FetchPollInterval cannot be shorter than FetchCooldown (%s)", 20*time.Millisecond),
		},
		{
			name:       "FetchPollInterval cannot be less than MinFetchPollInterval",
			configFunc: func(config *Config) { config.FetchPollInterval = time.Millisecond - 1 },
			wantErr:    errors.New("FetchPollInterval must be at least 1ms"),
		},
		{
			name:       "FetchPollInterval cannot be negative",
			configFunc: func(config *Config) { config.FetchPollInterval = -1 },
			wantErr:    errors.New("FetchPollInterval must be at least 1ms"),
		},
		{
			name:       "FetchPollInterval defaults to DefaultFetchPollInterval",
			configFunc: func(config *Config) { config.FetchPollInterval = 0 },
			wantErr:    nil,
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Equal(t, FetchPollIntervalDefault, client.config.FetchPollInterval)
			},
		},
		{
			name: "ID cannot be longer than 100 characters",
			// configFunc: func(config *Config) { config.ID = strings.Repeat("a", 101) },
			configFunc: func(config *Config) {
				config.ID = strings.Repeat("a", 101)
			},
			wantErr: errors.New("ID cannot be longer than 100 characters"),
		},
		{
			name: "JobTimeout can be -1 (infinite)",
			configFunc: func(config *Config) {
				config.JobTimeout = -1
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Equal(t, time.Duration(-1), client.config.JobTimeout)
			},
		},
		{
			name: "JobTimeout cannot be less than -1",
			configFunc: func(config *Config) {
				config.JobTimeout = -2
			},
			wantErr: errors.New("JobTimeout cannot be negative, except for -1 (infinite)"),
		},
		{
			name: "JobTimeout of zero applies DefaultJobTimeout",
			configFunc: func(config *Config) {
				config.JobTimeout = 0
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				// A client config value of zero gets interpreted as the default timeout:
				require.Equal(t, JobTimeoutDefault, client.config.JobTimeout)
			},
		},
		{
			name: "JobTimeout can be a large positive value",
			configFunc: func(config *Config) {
				config.JobTimeout = 7 * 24 * time.Hour
			},
		},
		{
			name: "MaxAttempts cannot be less than zero",
			configFunc: func(config *Config) {
				config.MaxAttempts = -1
			},
			wantErr: errors.New("MaxAttempts cannot be less than zero"),
		},
		{
			name: "MaxAttempts of zero applies DefaultMaxAttempts",
			configFunc: func(config *Config) {
				config.MaxAttempts = 0
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				// A client config value of zero gets interpreted as the default max attempts:
				require.Equal(t, MaxAttemptsDefault, client.config.MaxAttempts)
			},
		},
		{
			name: "Middleware can be configured independently",
			configFunc: func(config *Config) {
				config.Middleware = []rivertype.Middleware{&overridableJobMiddleware{}}
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Len(t, client.middlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindJobInsert), 1)
				require.Len(t, client.middlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindWorker), 1)
			},
		},
		{
			name: "JobInsertMiddleware and WorkMiddleware may be configured together with separate middlewares",
			configFunc: func(config *Config) {
				config.JobInsertMiddleware = []rivertype.JobInsertMiddleware{&overridableJobMiddleware{}}
				config.WorkerMiddleware = []rivertype.WorkerMiddleware{&overridableJobMiddleware{}}
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Len(t, client.middlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindJobInsert), 2)
				require.Len(t, client.middlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindWorker), 2)
			},
		},
		{
			name: "JobInsertMiddleware and WorkMiddleware may be configured together with same middleware",
			configFunc: func(config *Config) {
				middleware := &overridableJobMiddleware{}
				config.JobInsertMiddleware = []rivertype.JobInsertMiddleware{middleware}
				config.WorkerMiddleware = []rivertype.WorkerMiddleware{middleware}
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Len(t, client.middlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindJobInsert), 1)
				require.Len(t, client.middlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindWorker), 1)
			},
		},
		{
			name: "Middleware not allowed with JobInsertMiddleware",
			configFunc: func(config *Config) {
				config.JobInsertMiddleware = []rivertype.JobInsertMiddleware{&overridableJobMiddleware{}}
				config.Middleware = []rivertype.Middleware{&overridableJobMiddleware{}}
			},
			wantErr: errors.New("only one of the pair JobInsertMiddleware/WorkerMiddleware or Middleware may be provided (Middleware is recommended, and may contain both job insert and worker middleware)"),
		},
		{
			name: "Middleware not allowed with WorkerMiddleware",
			configFunc: func(config *Config) {
				config.Middleware = []rivertype.Middleware{&overridableJobMiddleware{}}
				config.WorkerMiddleware = []rivertype.WorkerMiddleware{&overridableJobMiddleware{}}
			},
			wantErr: errors.New("only one of the pair JobInsertMiddleware/WorkerMiddleware or Middleware may be provided (Middleware is recommended, and may contain both job insert and worker middleware)"),
		},
		{
			name: "RescueStuckJobsAfter may be overridden",
			configFunc: func(config *Config) {
				config.RescueStuckJobsAfter = 23 * time.Hour
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Equal(t, 23*time.Hour, client.config.RescueStuckJobsAfter)
			},
		},
		{
			name: "RescueStuckJobsAfter must be larger than JobTimeout",
			configFunc: func(config *Config) {
				config.JobTimeout = 7 * time.Hour
				config.RescueStuckJobsAfter = 6 * time.Hour
			},
			wantErr: errors.New("RescueStuckJobsAfter cannot be less than JobTimeout"),
		},
		{
			name: "RescueStuckJobsAfter increased automatically on a high JobTimeout when not set explicitly",
			configFunc: func(config *Config) {
				config.JobTimeout = 23 * time.Hour
			},
			validateResult: func(t *testing.T, client *Client[pgx.Tx]) { //nolint:thelper
				require.Equal(t, 23*time.Hour+maintenance.JobRescuerRescueAfterDefault, client.config.RescueStuckJobsAfter)
			},
		},
		{
			name: "Queues can be nil when Workers is also nil",
			configFunc: func(config *Config) {
				config.Queues = nil
				config.Workers = nil
			},
		},
		{
			name: "Queues can be nil when Workers is not nil",
			configFunc: func(config *Config) {
				config.Queues = nil
			},
		},
		{
			name:       "Queues can be empty",
			configFunc: func(config *Config) { config.Queues = make(map[string]QueueConfig) },
		},
		{
			name: "Queues MaxWorkers can't be negative",
			configFunc: func(config *Config) {
				config.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: -1}}
			},
			wantErr: errors.New("invalid number of workers for queue \"default\": -1"),
		},
		{
			name: "Queues can't have limits larger than MaxQueueNumWorkers",
			configFunc: func(config *Config) {
				config.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: QueueNumWorkersMax + 1}}
			},
			wantErr: fmt.Errorf("invalid number of workers for queue \"default\": %d", QueueNumWorkersMax+1),
		},
		{
			name: "Queues queue names can't be empty",
			configFunc: func(config *Config) {
				config.Queues = map[string]QueueConfig{"": {MaxWorkers: 1}}
			},
			wantErr: errors.New("queue name cannot be empty"),
		},
		{
			name: "Queues queue names can't be too long",
			configFunc: func(config *Config) {
				config.Queues = map[string]QueueConfig{strings.Repeat("a", 65): {MaxWorkers: 1}}
			},
			wantErr: errors.New("queue name cannot be longer than 64 characters"),
		},
		{
			name: "Queues queue names can't have asterisks",
			configFunc: func(config *Config) {
				config.Queues = map[string]QueueConfig{"no*hyphens": {MaxWorkers: 1}}
			},
			wantErr: errors.New("queue name is invalid, expected letters and numbers separated by underscores or hyphens: \"no*hyphens\""),
		},
		{
			name: "Queues queue names can be letters and numbers joined by underscores",
			configFunc: func(config *Config) {
				config.Queues = map[string]QueueConfig{"some_awesome_3rd_queue_namezzz": {MaxWorkers: 1}}
			},
		},
		{
			name: "Queues queue names can be letters and numbers joined by hyphens",
			configFunc: func(config *Config) {
				config.Queues = map[string]QueueConfig{"some-awesome-3rd-queue-namezzz": {MaxWorkers: 1}}
			},
		},
		{
			name: "Workers can be nil",
			configFunc: func(config *Config) {
				config.Queues = nil
				config.Workers = nil
			},
		},
		{
			name: "Workers can be empty", // but notably, not allowed to be empty if started
			configFunc: func(config *Config) {
				config.Workers = NewWorkers()
			},
		},
		{
			name: "Workers cannot be empty if Queues is set",
			configFunc: func(config *Config) {
				config.Workers = nil
			},
			wantErr: errors.New("Workers must be set if Queues is set"),
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			ctx := context.Background()
			require := require.New(t)
			dbPool := riverinternaltest.TestDB(ctx, t)

			workers := NewWorkers()
			AddWorker(workers, &noOpWorker{})

			config := &Config{
				Queues:  map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},
				Workers: workers,
			}
			tt.configFunc(config)

			client, err := NewClient(riverpgxv5.New(dbPool), config)
			if tt.wantErr != nil {
				require.Error(err)
				require.ErrorContains(err, tt.wantErr.Error())
				return
			}
			require.NoError(err)

			if tt.validateResult != nil {
				tt.validateResult(t, client)
			}
		})
	}
}

type timeoutTestArgs struct {
	TimeoutValue time.Duration `json:"timeout_value"`
}

func (timeoutTestArgs) Kind() string { return "timeoutTest" }

type testWorkerDeadline struct {
	deadline time.Time
	ok       bool
}

type timeoutTestWorker struct {
	WorkerDefaults[timeoutTestArgs]
	doneCh chan testWorkerDeadline
}

func (w *timeoutTestWorker) Timeout(job *Job[timeoutTestArgs]) time.Duration {
	return job.Args.TimeoutValue
}

func (w *timeoutTestWorker) Work(ctx context.Context, job *Job[timeoutTestArgs]) error {
	deadline, ok := ctx.Deadline()
	w.doneCh <- testWorkerDeadline{deadline: deadline, ok: ok}
	return nil
}

func TestClient_JobTimeout(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name             string
		jobArgTimeout    time.Duration
		clientJobTimeout time.Duration
		wantDuration     time.Duration
	}{
		{
			name:             "ClientJobTimeoutIsUsedIfJobArgTimeoutIsZero",
			jobArgTimeout:    0,
			clientJobTimeout: time.Hour,
			wantDuration:     time.Hour,
		},
		{
			name:             "JobArgTimeoutTakesPrecedenceIfBothAreSet",
			jobArgTimeout:    2 * time.Hour,
			clientJobTimeout: time.Hour,
			wantDuration:     2 * time.Hour,
		},
		{
			name:             "DefaultJobTimeoutIsUsedIfBothAreZero",
			jobArgTimeout:    0,
			clientJobTimeout: 0,
			wantDuration:     JobTimeoutDefault,
		},
		{
			name:             "NoJobTimeoutIfClientIsNegativeOneAndJobArgIsZero",
			jobArgTimeout:    0,
			clientJobTimeout: -1,
			wantDuration:     0, // infinite
		},
		{
			name:             "NoJobTimeoutIfJobArgIsNegativeOne",
			jobArgTimeout:    -1,
			clientJobTimeout: time.Hour,
			wantDuration:     0, // infinite
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			require := require.New(t)
			ctx := context.Background()

			testWorker := &timeoutTestWorker{doneCh: make(chan testWorkerDeadline)}

			workers := NewWorkers()
			AddWorker(workers, testWorker)

			config := newTestConfig(t, nil)
			config.JobTimeout = tt.clientJobTimeout
			config.Queues = map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}}
			config.Workers = workers

			client := runNewTestClient(ctx, t, config)
			_, err := client.Insert(ctx, timeoutTestArgs{TimeoutValue: tt.jobArgTimeout}, nil)
			require.NoError(err)

			result := riversharedtest.WaitOrTimeout(t, testWorker.doneCh)
			if tt.wantDuration == 0 {
				require.False(result.ok, "expected no deadline")
				return
			}
			require.True(result.ok, "expected a deadline, but none was set")
			require.WithinDuration(time.Now().Add(tt.wantDuration), result.deadline, 2*time.Second)
		})
	}
}

type JobArgsStaticKind struct {
	kind string
}

func (a JobArgsStaticKind) Kind() string {
	return a.kind
}

func TestInsertParamsFromJobArgsAndOptions(t *testing.T) {
	t.Parallel()

	archetype := riversharedtest.BaseServiceArchetype(t)
	config := newTestConfig(t, nil)

	t.Run("Defaults", func(t *testing.T) {
		t.Parallel()

		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, nil)
		require.NoError(t, err)
		require.JSONEq(t, `{"name":""}`, string(insertParams.EncodedArgs))
		require.Equal(t, (noOpArgs{}).Kind(), insertParams.Kind)
		require.Equal(t, config.MaxAttempts, insertParams.MaxAttempts)
		require.Equal(t, rivercommon.PriorityDefault, insertParams.Priority)
		require.Equal(t, QueueDefault, insertParams.Queue)
		require.Nil(t, insertParams.ScheduledAt)
		require.Equal(t, []string{}, insertParams.Tags)
		require.Empty(t, insertParams.UniqueKey)
		require.Zero(t, insertParams.UniqueStates)
	})

	t.Run("ConfigOverrides", func(t *testing.T) {
		t.Parallel()

		overrideConfig := &Config{
			MaxAttempts: 34,
		}

		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, overrideConfig, noOpArgs{}, nil)
		require.NoError(t, err)
		require.Equal(t, overrideConfig.MaxAttempts, insertParams.MaxAttempts)
	})

	t.Run("InsertOptsOverrides", func(t *testing.T) {
		t.Parallel()

		opts := &InsertOpts{
			MaxAttempts: 42,
			Priority:    2,
			Queue:       "other",
			ScheduledAt: time.Now().Add(time.Hour),
			Tags:        []string{"tag1", "tag2"},
		}
		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, opts)
		require.NoError(t, err)
		require.Equal(t, 42, insertParams.MaxAttempts)
		require.Equal(t, 2, insertParams.Priority)
		require.Equal(t, "other", insertParams.Queue)
		require.Equal(t, opts.ScheduledAt, *insertParams.ScheduledAt)
		require.Equal(t, []string{"tag1", "tag2"}, insertParams.Tags)
	})

	t.Run("WorkerInsertOptsOverrides", func(t *testing.T) {
		t.Parallel()

		nearFuture := time.Now().Add(5 * time.Minute)

		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{
			ScheduledAt: nearFuture,
		}, nil)
		require.NoError(t, err)
		// All these come from overrides in customInsertOptsJobArgs's definition:
		require.Equal(t, 42, insertParams.MaxAttempts)
		require.Equal(t, 2, insertParams.Priority)
		require.Equal(t, "other", insertParams.Queue)
		require.NotNil(t, insertParams.ScheduledAt)
		require.Equal(t, nearFuture, *insertParams.ScheduledAt)
		require.Equal(t, []string{"tag1", "tag2"}, insertParams.Tags)
	})

	t.Run("WorkerInsertOptsScheduledAtNotRespectedIfZero", func(t *testing.T) {
		t.Parallel()

		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{
			ScheduledAt: time.Time{},
		}, nil)
		require.NoError(t, err)
		require.Nil(t, insertParams.ScheduledAt)
	})

	t.Run("TagFormatValidated", func(t *testing.T) {
		t.Parallel()

		{
			_, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{}, &InsertOpts{
				Tags: []string{strings.Repeat("h", 256)},
			})
			require.EqualError(t, err, "tags should be a maximum of 255 characters long")
		}

		{
			_, err := insertParamsFromConfigArgsAndOptions(archetype, config, &customInsertOptsJobArgs{}, &InsertOpts{
				Tags: []string{"tag,with,comma"},
			})
			require.EqualError(t, err, "tags should match regex "+tagRE.String())
		}
	})

	t.Run("UniqueOptsDefaultStates", func(t *testing.T) {
		t.Parallel()

		archetype := riversharedtest.BaseServiceArchetype(t)
		archetype.Time.StubNowUTC(time.Now().UTC())

		uniqueOpts := UniqueOpts{
			ByArgs:      true,
			ByPeriod:    10 * time.Second,
			ByQueue:     true,
			ExcludeKind: true,
		}

		params, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, &InsertOpts{UniqueOpts: uniqueOpts})
		require.NoError(t, err)
		internalUniqueOpts := &dbunique.UniqueOpts{
			ByArgs:      true,
			ByPeriod:    10 * time.Second,
			ByQueue:     true,
			ByState:     []rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted, rivertype.JobStatePending, rivertype.JobStateRunning, rivertype.JobStateRetryable, rivertype.JobStateScheduled},
			ExcludeKind: true,
		}

		expectedKey, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts, params)
		require.NoError(t, err)

		require.Equal(t, expectedKey, params.UniqueKey)
		require.Equal(t, internalUniqueOpts.StateBitmask(), params.UniqueStates)
	})

	t.Run("UniqueOptsCustomStates", func(t *testing.T) {
		t.Parallel()

		archetype := riversharedtest.BaseServiceArchetype(t)
		archetype.Time.StubNowUTC(time.Now().UTC())

		states := []rivertype.JobState{
			rivertype.JobStateAvailable,
			rivertype.JobStatePending,
			rivertype.JobStateRetryable,
			rivertype.JobStateRunning,
			rivertype.JobStateScheduled,
		}

		uniqueOpts := UniqueOpts{
			ByPeriod: 10 * time.Second,
			ByQueue:  true,
			ByState:  states,
		}

		params, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, &InsertOpts{UniqueOpts: uniqueOpts})
		require.NoError(t, err)
		internalUniqueOpts := &dbunique.UniqueOpts{
			ByPeriod: 10 * time.Second,
			ByQueue:  true,
			ByState:  states,
		}

		expectedKey, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts, params)
		require.NoError(t, err)

		require.Equal(t, expectedKey, params.UniqueKey)
		require.Equal(t, internalUniqueOpts.StateBitmask(), params.UniqueStates)
	})

	t.Run("UniqueOptsWithPartialArgs", func(t *testing.T) {
		t.Parallel()

		uniqueOpts := UniqueOpts{ByArgs: true}

		type PartialArgs struct {
			JobArgsStaticKind
			Included bool `json:"included" river:"unique"`
			Excluded bool `json:"excluded"`
		}

		args := PartialArgs{
			JobArgsStaticKind: JobArgsStaticKind{kind: "partialArgs"},
			Included:          true,
			Excluded:          true,
		}

		params, err := insertParamsFromConfigArgsAndOptions(archetype, config, args, &InsertOpts{UniqueOpts: uniqueOpts})
		require.NoError(t, err)
		internalUniqueOpts := &dbunique.UniqueOpts{ByArgs: true}

		expectedKey, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts, params)
		require.NoError(t, err)
		require.Equal(t, expectedKey, params.UniqueKey)
		require.Equal(t, internalUniqueOpts.StateBitmask(), params.UniqueStates)

		argsWithExcludedFalse := PartialArgs{
			JobArgsStaticKind: JobArgsStaticKind{kind: "partialArgs"},
			Included:          true,
			Excluded:          false,
		}

		params2, err := insertParamsFromConfigArgsAndOptions(archetype, config, argsWithExcludedFalse, &InsertOpts{UniqueOpts: uniqueOpts})
		require.NoError(t, err)
		internalUniqueOpts2 := &dbunique.UniqueOpts{ByArgs: true}

		expectedKey2, err := dbunique.UniqueKey(archetype.Time, internalUniqueOpts2, params2)
		require.NoError(t, err)
		require.Equal(t, expectedKey2, params2.UniqueKey)
		require.Equal(t, internalUniqueOpts2.StateBitmask(), params.UniqueStates)
		require.Equal(t, params.UniqueKey, params2.UniqueKey, "unique keys should be identical because included args are the same, even though others differ")
	})

	t.Run("PriorityIsLimitedTo4", func(t *testing.T) {
		t.Parallel()

		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, noOpArgs{}, &InsertOpts{Priority: 5})
		require.ErrorContains(t, err, "priority must be between 1 and 4")
		require.Nil(t, insertParams)
	})

	t.Run("NonEmptyArgs", func(t *testing.T) {
		t.Parallel()

		args := timeoutTestArgs{TimeoutValue: time.Hour}
		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, args, nil)
		require.NoError(t, err)
		require.Equal(t, `{"timeout_value":3600000000000}`, string(insertParams.EncodedArgs))
	})

	t.Run("UniqueOptsAreValidated", func(t *testing.T) {
		t.Parallel()

		// Ensure that unique opts are validated. No need to be exhaustive here
		// since we already have tests elsewhere for that. Just make sure validation
		// is running.
		insertParams, err := insertParamsFromConfigArgsAndOptions(
			archetype,
			config,
			noOpArgs{},
			&InsertOpts{UniqueOpts: UniqueOpts{ByPeriod: 1 * time.Millisecond}},
		)
		require.EqualError(t, err, "UniqueOpts.ByPeriod should not be less than 1 second")
		require.Nil(t, insertParams)
	})
}

func TestID(t *testing.T) {
	t.Parallel()
	ctx := context.Background()

	t.Run("IsGeneratedWhenNotSpecifiedInConfig", func(t *testing.T) {
		t.Parallel()
		dbPool := riverinternaltest.TestDB(ctx, t)
		client := newTestClient(t, dbPool, newTestConfig(t, nil))
		require.NotEmpty(t, client.ID())
	})

	t.Run("IsGeneratedWhenNotSpecifiedInConfig", func(t *testing.T) {
		t.Parallel()
		config := newTestConfig(t, nil)
		config.ID = "my-client-id"
		dbPool := riverinternaltest.TestDB(ctx, t)
		client := newTestClient(t, dbPool, config)
		require.Equal(t, "my-client-id", client.ID())
	})
}

type customInsertOptsJobArgs struct {
	ScheduledAt time.Time `json:"scheduled_at"`
}

func (w *customInsertOptsJobArgs) Kind() string { return "customInsertOpts" }

func (w *customInsertOptsJobArgs) InsertOpts() InsertOpts {
	return InsertOpts{
		MaxAttempts: 42,
		Priority:    2,
		Queue:       "other",
		ScheduledAt: w.ScheduledAt,
		Tags:        []string{"tag1", "tag2"},
	}
}

func (w *customInsertOptsJobArgs) Work(context.Context, *Job[noOpArgs]) error { return nil }

func TestInsert(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	dbPool := riverinternaltest.TestDB(ctx, t)
	workers := NewWorkers()
	AddWorker(workers, &noOpWorker{})

	config := &Config{
		Queues:  map[string]QueueConfig{QueueDefault: {MaxWorkers: 1}},
		Workers: workers,
	}

	client, err := NewClient(riverpgxv5.New(dbPool), config)
	if err != nil {
		t.Fatal(err)
	}

	now := time.Now()
	requireEqualArgs := func(t *testing.T, expectedArgs *noOpArgs, actualPayload []byte) {
		t.Helper()
		var actualArgs noOpArgs
		if err := json.Unmarshal(actualPayload, &actualArgs); err != nil {
			t.Fatal(err)
		}
		require.Equal(t, *expectedArgs, actualArgs)
	}

	tests := []struct {
		name   string
		args   noOpArgs
		opts   *InsertOpts
		assert func(t *testing.T, args *noOpArgs, opts *InsertOpts, insertedJob *rivertype.JobRow)
	}{
		{
			name: "all options specified",
			args: noOpArgs{Name: "testJob"},
			opts: &InsertOpts{
				Queue:    "other",
				Priority: 2, // TODO: enforce a range on priority
				// TODO: comprehensive timezone testing
				ScheduledAt: now.Add(time.Hour).In(time.FixedZone("UTC-5", -5*60*60)),
				Tags:        []string{"tag1", "tag2"},
			},
			assert: func(t *testing.T, args *noOpArgs, opts *InsertOpts, insertedJob *rivertype.JobRow) {
				t.Helper()

				require := require.New(t)
				// specified by inputs:
				requireEqualArgs(t, args, insertedJob.EncodedArgs)
				require.Equal("other", insertedJob.Queue)
				require.Equal(2, insertedJob.Priority)
				// Postgres timestamptz only stores microsecond precision so we need to
				// assert approximate equality:
				require.WithinDuration(opts.ScheduledAt.UTC(), insertedJob.ScheduledAt, time.Microsecond)
				require.Equal([]string{"tag1", "tag2"}, insertedJob.Tags)
				// derived state:
				require.Equal(rivertype.JobStateScheduled, insertedJob.State)
				require.Equal("noOp", insertedJob.Kind)
				// default state:
				// require.Equal([]byte("{}"), insertedJob.metadata)
			},
		},
		{
			name: "all defaults",
			args: noOpArgs{Name: "testJob"},
			opts: nil,
			assert: func(t *testing.T, args *noOpArgs, opts *InsertOpts, insertedJob *rivertype.JobRow) {
				t.Helper()

				require := require.New(t)
				// specified by inputs:
				requireEqualArgs(t, args, insertedJob.EncodedArgs)
				// derived state:
				require.Equal(rivertype.JobStateAvailable, insertedJob.State)
				require.Equal("noOp", insertedJob.Kind)
				// default state:
				require.Equal(QueueDefault, insertedJob.Queue)
				require.Equal(1, insertedJob.Priority)
				// Default comes from database now(), and we can't know the exact value:
				require.WithinDuration(time.Now(), insertedJob.ScheduledAt, 2*time.Second)
				require.Equal([]string{}, insertedJob.Tags)
				// require.Equal([]byte("{}"), insertedJob.metadata)
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			require := require.New(t)
			if tt.assert == nil {
				t.Fatalf("test %q did not specify an assert function", tt.name)
			}

			ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
			defer cancel()

			insertRes, err := client.Insert(ctx, tt.args, tt.opts)
			require.NoError(err)
			tt.assert(t, &tt.args, tt.opts, insertRes.Job)

			// Also test InsertTx:
			tx, err := dbPool.Begin(ctx)
			require.NoError(err)
			defer tx.Rollback(ctx)

			insertedJob2, err := client.InsertTx(ctx, tx, tt.args, tt.opts)
			require.NoError(err)
			tt.assert(t, &tt.args, tt.opts, insertedJob2.Job)
		})
	}
}

func TestUniqueOpts(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		workers := NewWorkers()
		AddWorker(workers, &noOpWorker{})

		dbPool := riverinternaltest.TestDB(ctx, t)

		client := newTestClient(t, dbPool, newTestConfig(t, nil))

		// Tests that use ByPeriod below can be sensitive to intermittency if
		// the tests run at say 23:59:59.998, then it's possible to accidentally
		// cross a period threshold, even if very unlikely. So here, seed mostly
		// the current time, but make sure it's nicened up a little to be
		// roughly in the middle of the hour and well clear of any period
		// boundaries.
		client.baseService.Time.StubNowUTC(
			time.Now().Truncate(1 * time.Hour).Add(37*time.Minute + 23*time.Second + 123*time.Millisecond).UTC(),
		)

		return client, &testBundle{}
	}

	t.Run("DeduplicatesJobs", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		uniqueOpts := UniqueOpts{
			ByPeriod: 24 * time.Hour,
		}

		insertRes0, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{
			UniqueOpts: uniqueOpts,
		})
		require.NoError(t, err)
		require.False(t, insertRes0.UniqueSkippedAsDuplicate)

		insertRes1, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{
			UniqueOpts: uniqueOpts,
		})
		require.NoError(t, err)
		require.True(t, insertRes1.UniqueSkippedAsDuplicate)

		// Expect the same job to come back.
		require.Equal(t, insertRes0.Job.ID, insertRes1.Job.ID)
	})

	t.Run("UniqueByCustomStates", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		uniqueOpts := UniqueOpts{
			ByPeriod: 24 * time.Hour,
			ByState:  rivertype.JobStates(),
			ByQueue:  true,
		}

		insertRes0, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{
			UniqueOpts: uniqueOpts,
		})
		require.NoError(t, err)

		insertRes1, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{
			UniqueOpts: uniqueOpts,
		})
		require.NoError(t, err)

		// Expect the same job to come back because we deduplicate from the original.
		require.Equal(t, insertRes0.Job.ID, insertRes1.Job.ID)

		insertRes2, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{
			// Use another queue so the job can be inserted:
			Queue:      "other",
			UniqueOpts: uniqueOpts,
		})
		require.NoError(t, err)

		// This job however is _not_ the same because it's inserted as
		// `scheduled` which is outside the unique constraints.
		require.NotEqual(t, insertRes0.Job.ID, insertRes2.Job.ID)
	})

	t.Run("ErrorsWithUniqueV1CustomStates", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		uniqueOpts := UniqueOpts{
			ByPeriod: 24 * time.Hour,
			ByState:  []rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted},
		}

		insertRes, err := client.Insert(ctx, noOpArgs{}, &InsertOpts{
			UniqueOpts: uniqueOpts,
		})
		require.EqualError(t, err, "UniqueOpts.ByState must contain all required states, missing: pending, running, scheduled")
		require.Nil(t, insertRes)
	})
}

func TestDefaultClientID(t *testing.T) {
	t.Parallel()

	host, _ := os.Hostname()
	require.NotEmpty(t, host)

	startedAt := time.Date(2024, time.March, 7, 4, 39, 12, 123456789, time.UTC)

	require.Equal(t, strings.ReplaceAll(host, ".", "_")+"_2024_03_07T04_39_12_123456", defaultClientID(startedAt))
}

func TestDefaultClientIDWithHost(t *testing.T) {
	t.Parallel()

	host, _ := os.Hostname()
	require.NotEmpty(t, host)

	startedAt := time.Date(2024, time.March, 7, 4, 39, 12, 123456789, time.UTC)

	require.Equal(t, "example_com_2024_03_07T04_39_12_123456", defaultClientIDWithHost(startedAt,
		"example.com"))
	require.Equal(t, "this_is_a_degenerately_long_host_name_that_will_be_truncated_2024_03_07T04_39_12_123456", defaultClientIDWithHost(startedAt,
		"this.is.a.degenerately.long.host.name.that.will.be.truncated.so.were.not.storing.massive.strings.to.the.database.com"))

	// Test strings right around the boundary to make sure we don't have some off-by-one slice error.
	require.Equal(t, strings.Repeat("a", 59)+"_2024_03_07T04_39_12_123456", defaultClientIDWithHost(startedAt, strings.Repeat("a", 59)))
	require.Equal(t, strings.Repeat("a", 60)+"_2024_03_07T04_39_12_123456", defaultClientIDWithHost(startedAt, strings.Repeat("a", 60)))
	require.Equal(t, strings.Repeat("a", 60)+"_2024_03_07T04_39_12_123456", defaultClientIDWithHost(startedAt, strings.Repeat("a", 61)))
}

```

`cmd/river/go.mod`:

```mod
module github.com/riverqueue/river/cmd/river

go 1.23.0

toolchain go1.24.1

require (
	github.com/jackc/pgx/v5 v5.7.3
	github.com/lmittmann/tint v1.0.7
	github.com/riverqueue/river v0.19.0
	github.com/riverqueue/river/riverdriver v0.19.0
	github.com/riverqueue/river/riverdriver/riverpgxv5 v0.19.0
	github.com/riverqueue/river/rivershared v0.19.0
	github.com/riverqueue/river/rivertype v0.19.0
	github.com/spf13/cobra v1.9.1
	github.com/stretchr/testify v1.10.0
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/jackc/pgpassfile v1.0.0 // indirect
	github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect
	github.com/jackc/puddle/v2 v2.2.2 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/spf13/pflag v1.0.6 // indirect
	github.com/tidwall/gjson v1.18.0 // indirect
	github.com/tidwall/match v1.1.1 // indirect
	github.com/tidwall/pretty v1.2.1 // indirect
	github.com/tidwall/sjson v1.2.5 // indirect
	go.uber.org/goleak v1.3.0 // indirect
	golang.org/x/crypto v0.31.0 // indirect
	golang.org/x/sync v0.12.0 // indirect
	golang.org/x/text v0.23.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

```

`cmd/river/go.sum`:

```sum
github.com/cpuguy83/go-md2man/v2 v2.0.6/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa h1:s+4MhCQ6YrzisK6hFJUX53drDT4UsSW3DEhKn0ifuHw=
github.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa/go.mod h1:a/s9Lp5W7n/DD0VrVoyJ00FbP2ytTPDVOivvn2bMlds=
github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
github.com/jackc/pgx/v5 v5.7.3 h1:PO1wNKj/bTAwxSJnO1Z4Ai8j4magtqg2SLNjEDzcXQo=
github.com/jackc/pgx/v5 v5.7.3/go.mod h1:ncY89UGWxg82EykZUwSpUKEfccBGGYq1xjrOpsbsfGQ=
github.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo=
github.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=
github.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=
github.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
github.com/lmittmann/tint v1.0.7 h1:D/0OqWZ0YOGZ6AyC+5Y2kD8PBEzBk6rFHVSfOqCkF9Y=
github.com/lmittmann/tint v1.0.7/go.mod h1:HIS3gSy7qNwGCj+5oRjAutErFBl4BzdQP6cJZ0NfMwE=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/riverqueue/river v0.19.0 h1:WRh/NXhp+WEEY0HpCYgr4wSRllugYBt30HtyQ3jlz08=
github.com/riverqueue/river v0.19.0/go.mod h1:YJ7LA2uBdqFHQJzKyYc+X6S04KJeiwsS1yU5a1rynlk=
github.com/riverqueue/river/riverdriver v0.19.0 h1:NyHz5DfB13paT2lvaO0CKmwy4SFLbA7n6MFRGRtwii4=
github.com/riverqueue/river/riverdriver v0.19.0/go.mod h1:Soxi08hHkEvopExAp6ADG2437r4coSiB4QpuIL5E28k=
github.com/riverqueue/river/riverdriver/riverdatabasesql v0.19.0 h1:ytdPnueiv7ANxJcntBtYenrYZZLY5P0mXoDV0l4WsLk=
github.com/riverqueue/river/riverdriver/riverdatabasesql v0.19.0/go.mod h1:5Fahb3n+m1V0RAb0JlOIpzimoTlkOgudMfxSSCTcmFk=
github.com/riverqueue/river/riverdriver/riverpgxv5 v0.19.0 h1:QWg7VTDDXbtTF6srr7Y1C888PiNzqv379yQuNSnH2hg=
github.com/riverqueue/river/riverdriver/riverpgxv5 v0.19.0/go.mod h1:uvF1YS+iSQavCIHtaB/Y6O8A6Dnn38ctVQCpCpmHDZE=
github.com/riverqueue/river/rivershared v0.19.0 h1:TZvFM6CC+QgwQQUMQ5Ueuhx25ptgqcKqZQGsdLJnFeE=
github.com/riverqueue/river/rivershared v0.19.0/go.mod h1:JAvmohuC5lounVk8e3zXZIs07Da3klzEeJo1qDQIbjw=
github.com/riverqueue/river/rivertype v0.19.0 h1:5rwgdh21pVcU9WjrHIIO9qC2dOMdRrrZ/HZZOE0JRyY=
github.com/riverqueue/river/rivertype v0.19.0/go.mod h1:DETcejveWlq6bAb8tHkbgJqmXWVLiFhTiEm8j7co1bE=
github.com/robfig/cron/v3 v3.0.1 h1:WdRxkvbJztn8LMz/QEvLN5sBU+xKpSqwwUO1Pjr4qDs=
github.com/robfig/cron/v3 v3.0.1/go.mod h1:eQICP3HwyT7UooqI/z+Ov+PtYAWygg1TEWWzGIFLtro=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/spf13/cobra v1.9.1 h1:CXSaggrXdbHK9CF+8ywj8Amf7PBRmPCOJugH954Nnlo=
github.com/spf13/cobra v1.9.1/go.mod h1:nDyEzZ8ogv936Cinf6g1RU9MRY64Ir93oCnqb9wxYW0=
github.com/spf13/pflag v1.0.6 h1:jFzHGLGAlb3ruxLB8MhbI6A8+AQX/2eW4qeyNZXNp2o=
github.com/spf13/pflag v1.0.6/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/tidwall/gjson v1.14.2/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/gjson v1.18.0 h1:FIDeeyB800efLX89e5a8Y0BNH+LOngJyGrIWxG2FKQY=
github.com/tidwall/gjson v1.18.0/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/match v1.1.1 h1:+Ho715JplO36QYgwN9PGYNhgZvoUSc9X2c80KVTi+GA=
github.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=
github.com/tidwall/pretty v1.2.0/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/pretty v1.2.1 h1:qjsOFOWWQl+N3RsoF5/ssm1pHmJJwhjlSbZ51I6wMl4=
github.com/tidwall/pretty v1.2.1/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/sjson v1.2.5 h1:kLy8mja+1c9jlljvWTlSazM7cKDRfJuR/bOJhcY5NcY=
github.com/tidwall/sjson v1.2.5/go.mod h1:Fvgq9kS/6ociJEDnK0Fk1cpYF4FIW6ZF7LAe+6jwd28=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
golang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/sync v0.12.0 h1:MHc5BpPuC30uJk597Ri8TV3CNZcTLu6B6z4lJy+g6Jw=
golang.org/x/sync v0.12.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/text v0.23.0 h1:D71I7dUrlY+VX0gQShAThNGHFxZ13dGLBHQLVl1mJlY=
golang.org/x/text v0.23.0/go.mod h1:/BLNzu4aZCJ1+kcD0DNRotWKage4q2rGVAg4o22unh4=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

```

`cmd/river/main.go`:

```go
package main

import (
	"os"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river/cmd/river/rivercli"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
)

type DriverProcurer struct{}

func (p *DriverProcurer) ProcurePgxV5(pool *pgxpool.Pool) riverdriver.Driver[pgx.Tx] {
	return riverpgxv5.New(pool)
}

func main() {
	cli := rivercli.NewCLI(&rivercli.Config{
		DriverProcurer: &DriverProcurer{},
		Name:           "River",
	})

	if err := cli.BaseCommandSet().Execute(); err != nil {
		// Cobra will already print an error on problems like an unknown command
		// or missing required flag. Set an exit status of 1 on error, but don't
		// print it again.
		os.Exit(1)
	}
}

```

`cmd/river/riverbench/river_bench.go`:

```go
package riverbench

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"os"
	"os/signal"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivertype"
)

type Benchmarker[TTx any] struct {
	driver riverdriver.Driver[TTx] // database pool wrapped in River driver
	logger *slog.Logger            // logger, also injected to client
	name   string                  // name of the service for logging purposes
}

func NewBenchmarker[TTx any](driver riverdriver.Driver[TTx], logger *slog.Logger) *Benchmarker[TTx] {
	return &Benchmarker[TTx]{
		driver: driver,
		logger: logger,
		name:   "Benchmarker",
	}
}

// Run starts the benchmarking loop. Stops upon receiving SIGINT/SIGTERM, or
// when reaching maximum configured run duration.
func (b *Benchmarker[TTx]) Run(ctx context.Context, duration time.Duration, numTotalJobs int) error {
	var (
		lastJobWorkedAt time.Time
		numJobsInserted atomic.Int64
		numJobsLeft     atomic.Int64
		numJobsWorked   atomic.Int64
		shutdown        = make(chan struct{})
		shutdownClosed  bool
	)

	// Prevents double-close on shutdown channel.
	closeShutdown := func() {
		if !shutdownClosed {
			b.logger.DebugContext(ctx, "Closing shutdown channel")
			close(shutdown)
		}
		shutdownClosed = true
	}

	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Installing signals allows us to try and stop the client cleanly, and also
	// to produce a final summary log line for th whole bench run (by default,
	// Go will terminate programs abruptly and not even defers will run).
	go func() {
		signalChan := make(chan os.Signal, 1)
		signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM)

		select {
		case <-ctx.Done():
		case <-signalChan:
			closeShutdown()

			// Wait again since the client may take an absurd amount of time to
			// shut down. If we receive another signal in the intervening
			// period, cancel context, thereby forcing a hard shut down.
			select {
			case <-ctx.Done():
			case <-signalChan:
				fmt.Printf("second signal received; canceling context\n")
				cancel()
			}
		}
	}()

	if err := b.resetJobsTable(ctx); err != nil {
		return err
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &BenchmarkWorker{})

	client, err := river.NewClient(b.driver, &river.Config{
		// When benchmarking to maximize job throughput these numbers have an
		// outsized effect on results. The ones chosen here could possibly be
		// optimized further, but based on my tests of throwing a lot of random
		// values against the wall, they perform quite well. Much better than
		// the client's default values at any rate.
		FetchCooldown:     2 * time.Millisecond,
		FetchPollInterval: 20 * time.Millisecond,

		Logger: b.logger,
		Queues: map[string]river.QueueConfig{
			// This could probably use more refinement, but in my quick and
			// dirty tests I found that roughly 1k workers was most optimal. 500
			// and 2,000 performed a little more poorly, and jumping up to the
			// maximum of 10k performed quite badly (scheduler contention?).
			// There may be a more optimal number than 1,000, but it seems close
			// enough to target for now.
			river.QueueDefault: {MaxWorkers: 2_000},
		},
		Workers: workers,
	})
	if err != nil {
		return err
	}

	// Notably, we use a subscribe channel to track how many jobs have been
	// worked instead of using telemetry from the worker itself because the
	// subscribe channel accounts for the job moving through the completer while
	// the worker does not.
	subscribeChan, subscribeCancel := client.SubscribeConfig(&river.SubscribeConfig{
		// The benchmark may be processing a huge quantity of jobs far in excess
		// of what River under normal conditions might see, so pick a much
		// larger than normal subscribe channel size to make sure we don't
		// accidentally drop any events.
		//
		// The subscribe channel is used to determine when jobs finish, so
		// dropping jobs is very detrimental because it confuses the benchmark's
		// bookkeeping of how many jobs there are left to work.
		ChanSize: minJobs,

		Kinds: []river.EventKind{
			river.EventKindJobCancelled,
			river.EventKindJobCompleted,
			river.EventKindJobFailed,
		},
	})
	defer subscribeCancel()

	go func() {
		for {
			select {
			case <-ctx.Done():
				return

			case <-shutdown:
				return

			case event := <-subscribeChan:
				if event == nil { // Closed channel.
					b.logger.InfoContext(ctx, "Subscription channel closed")
					return
				}

				switch {
				case event.Kind == river.EventKindJobCancelled:
					b.logger.ErrorContext(ctx, "Job unexpectedly cancelled", "job_id", event.Job.ID)

				case event.Kind == river.EventKindJobCompleted:

				// Only count a job as complete if it failed for the last time.
				// We don't expect benchmark jobs to ever fail, so this extra
				// attention to detail is here, but shouldn't be needed.
				case event.Kind == river.EventKindJobFailed && event.Job.State == rivertype.JobStateDiscarded:
					b.logger.ErrorContext(ctx, "Job unexpectedly failed and discarded", "job_id", event.Job.ID)

				default:
					b.logger.ErrorContext(ctx, "Unhandled subscription event kind", "kind", event.Kind)
				}

				lastJobWorkedAt = time.Now()
				numJobsLeft.Add(-1)
				numJobsWorked := numJobsWorked.Add(1)

				const logBatchSize = 10_000
				if numJobsWorked%logBatchSize == 0 {
					b.logger.DebugContext(ctx, b.name+": Worked batch of job(s)", "num_worked", logBatchSize)
				}
			}
		}
	}()

	// Goroutine that ticks periodically to show how many available jobs there are.
	go func() {
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ctx.Done():
				return

			case <-shutdown:
				return

			case <-ticker.C:
				numJobs, err := b.driver.GetExecutor().JobCountByState(ctx, rivertype.JobStateAvailable)
				if err != nil {
					b.logger.ErrorContext(ctx, "Error counting jobs", "err", err)
					continue
				}
				b.logger.InfoContext(ctx, "Available job count", "num_jobs", numJobs)
			}
		}
	}()

	minJobsReady := make(chan struct{})

	if numTotalJobs != 0 {
		b.insertJobs(ctx, client, minJobsReady, &numJobsInserted, &numJobsLeft, numTotalJobs, shutdown)
	} else {
		insertJobsFinished := make(chan struct{})
		defer func() { <-insertJobsFinished }()

		go func() {
			defer close(insertJobsFinished)
			b.insertJobsContinuously(ctx, client, minJobsReady, &numJobsInserted, &numJobsLeft, shutdown)
		}()
	}

	// Must appear after we wait for insert jobs to finish before so that the
	// defers run in the right order.
	defer closeShutdown()

	// Don't start measuring until the first batch of jobs is confirmed ready.
	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-minJobsReady:
		// okay
	case <-shutdown:
		return nil
	case <-time.After(5 * time.Second):
		return errors.New("timed out waiting for starting jobs to be inserted")
	}

	b.logger.InfoContext(ctx, b.name+": Minimum jobs inserted; starting iteration")

	if err := client.Start(ctx); err != nil {
		return err
	}

	defer func() {
		if err := client.Stop(ctx); err != nil {
			b.logger.ErrorContext(ctx, b.name+": Error stopping client", "err", err)
		}
	}()

	// Prints one last log line before exit summarizing all operations.
	start := time.Now()
	defer func() {
		// Use timing since the last job worked since even in burn down mode,
		// the benchmark waits for one last interval before ending.
		runPeriod := lastJobWorkedAt.Sub(start)
		jobsPerSecond := float64(numJobsWorked.Load()) / runPeriod.Seconds()

		fmt.Printf("bench: total jobs worked [ %10d ], total jobs inserted [ %10d ], overall job/sec [ %10.1f ], running %s\n",
			numJobsWorked.Load(), numJobsInserted.Load(), jobsPerSecond, runPeriod)
	}()

	const iterationPeriod = 2 * time.Second

	var (
		firstRun            = true
		numJobsInsertedLast int64
		numJobsWorkedLast   int64
		ticker              = time.NewTicker(iterationPeriod)
	)
	defer ticker.Stop()

	for numIterations := 0; ; numIterations++ {
		// Use iterations multiplied by period time instead of actual elapsed
		// time to allow a precise, predictable run duration to be specified.
		if duration != 0 && time.Duration(numIterations)*iterationPeriod >= duration {
			return nil
		}

		var (
			numJobsInsertedSinceLast = numJobsInserted.Load() - numJobsInsertedLast
			numJobsWorkedSinceLast   = numJobsWorked.Load() - numJobsWorkedLast
		)

		jobsPerSecond := float64(numJobsWorkedSinceLast) / iterationPeriod.Seconds()

		// On first run, show iteration period as 0s because no time was given
		// for jobs to be worked.
		period := iterationPeriod
		if firstRun {
			period = 0 * time.Second
		}

		fmt.Printf("bench: jobs worked [ %10d ], inserted [ %10d ], job/sec [ %10.1f ] [%s]\n",
			numJobsWorkedSinceLast, numJobsInsertedSinceLast, jobsPerSecond, period)

		firstRun = false
		numJobsInsertedLast = numJobsInserted.Load()
		numJobsWorkedLast = numJobsWorked.Load()

		// If working in the mode where we're burning jobs down and there are no
		// jobs left, end.
		if numTotalJobs != 0 && numJobsLeft.Load() < 1 {
			return nil
		}

		select {
		case <-ctx.Done():
			return nil

		case <-shutdown:
			return nil

		case <-ticker.C:
		}
	}
}

const (
	insertBatchSize = 5_000
	minJobs         = 75_000 // max per/sec I've seen it work + 50% head room
)

// Inserts `b.numTotalJobs` in batches. This variant inserts a bulk of initial
// jobs and ends, and is used in cases the `-n`/`--num-total-jobs` flag is
// specified.
func (b *Benchmarker[TTx]) insertJobs(
	ctx context.Context,
	client *river.Client[TTx],
	minJobsReady chan struct{},
	numJobsInserted *atomic.Int64,
	numJobsLeft *atomic.Int64,
	numTotalJobs int,
	shutdown chan struct{},
) {
	defer close(minJobsReady)

	var (
		// We'll be reusing the same batch for all inserts because (1) we can
		// get away with it, and (2) to avoid needless allocations.
		insertParamsBatch = make([]river.InsertManyParams, insertBatchSize)
		jobArgsBatch      = make([]BenchmarkArgs, insertBatchSize)

		jobNum int
	)

	var numInsertedThisRound int

	for {
		for _, jobArgs := range jobArgsBatch {
			jobNum++
			jobArgs.Num = jobNum
		}

		for i := range insertParamsBatch {
			insertParamsBatch[i].Args = jobArgsBatch[i]
		}

		numLeft := numTotalJobs - numInsertedThisRound
		if numLeft < insertBatchSize {
			insertParamsBatch = insertParamsBatch[0:numLeft]
		}

		start := time.Now()
		if _, err := client.InsertMany(ctx, insertParamsBatch); err != nil {
			b.logger.ErrorContext(ctx, b.name+": Error inserting jobs", "err", err)
		}

		numJobsInserted.Add(int64(len(insertParamsBatch)))
		numJobsLeft.Add(int64(len(insertParamsBatch)))
		numInsertedThisRound += len(insertParamsBatch)

		if numJobsLeft.Load() >= int64(numTotalJobs) {
			b.logger.InfoContext(ctx, b.name+": Finished inserting jobs",
				"duration", time.Since(start), "num_inserted", numInsertedThisRound)
			return
		}

		// Will be very unusual, but break early if done between batches.
		select {
		case <-ctx.Done():
			return
		case <-shutdown:
			return
		default:
		}
	}
}

// Inserts jobs continuously, but only if it notices that the number of jobs
// left is below a minimum threshold. This has the effect of keeping enough job
// slack in the pool to be worked, but keeping the total number of jobs being
// inserted roughly matched with the rate at which the benchmark can work them.
func (b *Benchmarker[TTx]) insertJobsContinuously(
	ctx context.Context,
	client *river.Client[TTx],
	minJobsReady chan struct{},
	numJobsInserted *atomic.Int64,
	numJobsLeft *atomic.Int64,
	shutdown chan struct{},
) {
	var (
		// We'll be reusing the same batch for all inserts because (1) we can
		// get away with it, and (2) to avoid needless allocations.
		insertParamsBatch = make([]river.InsertManyParams, insertBatchSize)
		jobArgsBatch      = make([]BenchmarkArgs, insertBatchSize)

		jobNum int
	)

	for {
		select {
		case <-ctx.Done():
			return

		case <-shutdown:
			return

		case <-time.After(250 * time.Millisecond):
		}

		var numInsertedThisRound int

		for {
			for _, jobArgs := range jobArgsBatch {
				jobNum++
				jobArgs.Num = jobNum
			}

			for i := range insertParamsBatch {
				insertParamsBatch[i].Args = jobArgsBatch[i]
			}

			if _, err := client.InsertMany(ctx, insertParamsBatch); err != nil {
				b.logger.ErrorContext(ctx, b.name+": Error inserting jobs", "err", err)
			}

			numJobsInserted.Add(int64(len(insertParamsBatch)))
			numJobsLeft.Add(int64(len(insertParamsBatch)))
			numInsertedThisRound += len(insertParamsBatch)

			if numJobsLeft.Load() >= minJobs {
				b.logger.InfoContext(ctx, b.name+": Finished inserting batch of jobs",
					"num_inserted", numInsertedThisRound)
				break // break inner loop to go back to sleep
			}

			// Will be very unusual, but break early if done between batches.
			select {
			case <-ctx.Done():
				return
			case <-shutdown:
				return
			default:
			}
		}

		// Close the first time we insert a full batch to tell the main loop it
		// can start benchmarking.
		if minJobsReady != nil {
			close(minJobsReady)
			minJobsReady = nil
		}
	}
}

// Truncates and `VACUUM FULL`s the jobs table to guarantee as little state
// related job variance as possible.
func (b *Benchmarker[TTx]) resetJobsTable(ctx context.Context) error {
	b.logger.InfoContext(ctx, b.name+": Truncating and vacuuming jobs table")

	_, err := b.driver.GetExecutor().Exec(ctx, "TRUNCATE river_job")
	if err != nil {
		return err
	}
	_, err = b.driver.GetExecutor().Exec(ctx, "VACUUM FULL river_job")
	if err != nil {
		return err
	}

	return nil
}

type BenchmarkArgs struct {
	Num int `json:"num"`
}

func (BenchmarkArgs) Kind() string { return "benchmark" }

// BenchmarkWorker is a job worker for counting the number of worked jobs.
type BenchmarkWorker struct {
	river.WorkerDefaults[BenchmarkArgs]
}

func (w *BenchmarkWorker) Work(ctx context.Context, j *river.Job[BenchmarkArgs]) error {
	return nil
}

```

`cmd/river/rivercli/command.go`:

```go
package rivercli

import (
	"context"
	"fmt"
	"io"
	"log/slog"
	"os"
	"strconv"
	"strings"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river/cmd/river/riverbench"
	"github.com/riverqueue/river/rivermigrate"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
)

const (
	uriScheme      = "postgresql://"
	uriSchemeAlias = "postgres://"
)

// BenchmarkerInterface is an interface to a Benchmarker. Its reason for
// existence is to wrap a benchmarker to strip it of its generic parameter,
// letting us pass it around without having to know the transaction type.
type BenchmarkerInterface interface {
	Run(ctx context.Context, duration time.Duration, numTotalJobs int) error
}

// MigratorInterface is an interface to a Migrator. Its reason for existence is
// to wrap a migrator to strip it of its generic parameter, letting us pass it
// around without having to know the transaction type.
type MigratorInterface interface {
	AllVersions() []rivermigrate.Migration
	ExistingVersions(ctx context.Context) ([]rivermigrate.Migration, error)
	GetVersion(version int) (rivermigrate.Migration, error)
	Migrate(ctx context.Context, direction rivermigrate.Direction, opts *rivermigrate.MigrateOpts) (*rivermigrate.MigrateResult, error)
	Validate(ctx context.Context) (*rivermigrate.ValidateResult, error)
}

// Command is an interface to a River CLI subcommand. Commands generally only
// implement a Run function, and get the rest of the implementation by embedding
// CommandBase.
type Command[TOpts CommandOpts] interface {
	Run(ctx context.Context, opts TOpts) (bool, error)
	GetCommandBase() *CommandBase
	SetCommandBase(b *CommandBase)
}

// CommandBase provides common facilities for a River CLI command. It's
// generally embedded on the struct of a command.
type CommandBase struct {
	DriverProcurer DriverProcurer
	Logger         *slog.Logger
	Out            io.Writer

	GetBenchmarker func() BenchmarkerInterface
	GetMigrator    func(config *rivermigrate.Config) (MigratorInterface, error)
}

func (b *CommandBase) GetCommandBase() *CommandBase     { return b }
func (b *CommandBase) SetCommandBase(base *CommandBase) { *b = *base }

// CommandOpts are options for a command options. It makes sure that options
// provide a way of validating themselves.
type CommandOpts interface {
	Validate() error
}

// RunCommandBundle is a bundle of utilities for RunCommand.
type RunCommandBundle struct {
	DatabaseURL    *string
	DriverProcurer DriverProcurer
	Logger         *slog.Logger
	OutStd         io.Writer
}

// RunCommand bootstraps and runs a River CLI subcommand.
func RunCommand[TOpts CommandOpts](ctx context.Context, bundle *RunCommandBundle, command Command[TOpts], opts TOpts) error {
	procureAndRun := func() (bool, error) {
		if err := opts.Validate(); err != nil {
			return false, err
		}

		commandBase := &CommandBase{
			DriverProcurer: bundle.DriverProcurer,
			Logger:         bundle.Logger,
			Out:            bundle.OutStd,
		}

		var databaseURL *string

		switch {
		case pgEnvConfigured():
			databaseURL = ptrutil.Ptr("")

		case bundle.DatabaseURL != nil:
			if !strings.HasPrefix(*bundle.DatabaseURL, uriScheme) &&
				!strings.HasPrefix(*bundle.DatabaseURL, uriSchemeAlias) {
				return false, fmt.Errorf(
					"unsupported database URL (`%s`); try one with a `%s` or `%s` scheme/prefix",
					*bundle.DatabaseURL,
					uriSchemeAlias,
					uriScheme,
				)
			}

			databaseURL = bundle.DatabaseURL
		}

		if databaseURL == nil {
			commandBase.GetBenchmarker = func() BenchmarkerInterface { panic("neither PG* env nor databaseURL was not set") }
			commandBase.GetMigrator = func(config *rivermigrate.Config) (MigratorInterface, error) {
				panic("neither PG* env nor databaseURL was not set")
			}
		} else {
			dbPool, err := openPgxV5DBPool(ctx, *databaseURL)
			if err != nil {
				return false, err
			}
			defer dbPool.Close()

			driver := bundle.DriverProcurer.ProcurePgxV5(dbPool)

			commandBase.GetBenchmarker = func() BenchmarkerInterface { return riverbench.NewBenchmarker(driver, commandBase.Logger) }
			commandBase.GetMigrator = func(config *rivermigrate.Config) (MigratorInterface, error) { return rivermigrate.New(driver, config) }
		}

		command.SetCommandBase(commandBase)

		return command.Run(ctx, opts)
	}

	ok, err := procureAndRun()
	if err != nil {
		return err
	}
	if !ok {
		os.Exit(1)
	}
	return nil
}

func openPgxV5DBPool(ctx context.Context, databaseURL string) (*pgxpool.Pool, error) {
	const (
		defaultIdleInTransactionSessionTimeout = 11 * time.Second // should be greater than statement timeout because statements count towards idle-in-transaction
		defaultStatementTimeout                = 10 * time.Second
	)

	pgxConfig, err := pgxpool.ParseConfig(databaseURL)
	if err != nil {
		return nil, fmt.Errorf("error parsing database URL: %w", err)
	}

	// Sets a parameter in a parameter map (aimed at a Postgres connection
	// configuration map), but only if that parameter wasn't already set.
	setParamIfUnset := func(runtimeParams map[string]string, name, val string) {
		if currentVal := runtimeParams[name]; currentVal != "" {
			return
		}

		runtimeParams[name] = val
	}

	setParamIfUnset(pgxConfig.ConnConfig.RuntimeParams, "application_name", "river CLI")
	setParamIfUnset(pgxConfig.ConnConfig.RuntimeParams, "idle_in_transaction_session_timeout", strconv.Itoa(int(defaultIdleInTransactionSessionTimeout.Milliseconds())))
	setParamIfUnset(pgxConfig.ConnConfig.RuntimeParams, "statement_timeout", strconv.Itoa(int(defaultStatementTimeout.Milliseconds())))

	dbPool, err := pgxpool.NewWithConfig(ctx, pgxConfig)
	if err != nil {
		return nil, fmt.Errorf("error connecting to database: %w", err)
	}

	return dbPool, nil
}

// Determines if there's a minimum number of `PG*` env vars configured to
// consider that configurable path viable. A `--database-url` parameter will
// take precedence.
func pgEnvConfigured() bool {
	return os.Getenv("PGDATABASE") != ""
}

```

`cmd/river/rivercli/river_cli.go`:

```go
// Package rivercli provides an implementation for the River CLI.
//
// This package is largely for internal use and doesn't provide the same API
// guarantees as the main River modules. Breaking API changes will be made
// without warning.
package rivercli

import (
	"context"
	"errors"
	"fmt"
	"io"
	"log/slog"
	"os"
	"runtime/debug"
	"slices"
	"strings"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/lmittmann/tint"
	"github.com/spf13/cobra"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivermigrate"
	"github.com/riverqueue/river/rivershared/util/valutil"
)

type Config struct {
	// DriverProcurer provides a way of procuring drivers for various supported
	// databases.
	DriverProcurer DriverProcurer

	// Name is the human-friendly named of the executable, used while showing
	// version output. Usually this is just "River", but it could be "River
	// Pro".
	Name string
}

// DriverProcurer is an interface that provides a way of procuring drivers for
// various supported databases.
type DriverProcurer interface {
	ProcurePgxV5(pool *pgxpool.Pool) riverdriver.Driver[pgx.Tx]
}

// CLI provides a common base of commands for the River CLI.
type CLI struct {
	driverProcurer DriverProcurer
	name           string
	out            io.Writer
}

func NewCLI(config *Config) *CLI {
	return &CLI{
		driverProcurer: config.DriverProcurer,
		name:           config.Name,
		out:            os.Stdout,
	}
}

// BaseCommandSet provides a base River CLI command set which may be further
// augmented with additional commands.
func (c *CLI) BaseCommandSet() *cobra.Command {
	ctx := context.Background()

	var globalOpts struct {
		Debug   bool
		Verbose bool
	}

	makeLogger := func() *slog.Logger {
		switch {
		case globalOpts.Debug:
			return slog.New(tint.NewHandler(os.Stdout, &tint.Options{Level: slog.LevelDebug}))
		case globalOpts.Verbose:
			return slog.New(tint.NewHandler(os.Stdout, nil))
		default:
			return slog.New(tint.NewHandler(os.Stdout, &tint.Options{Level: slog.LevelWarn}))
		}
	}

	// Make a bundle for RunCommand. Takes a database URL pointer because not every command is required to take a database URL.
	makeCommandBundle := func(databaseURL *string) *RunCommandBundle {
		return &RunCommandBundle{
			DatabaseURL:    databaseURL,
			DriverProcurer: c.driverProcurer,
			Logger:         makeLogger(),
			OutStd:         c.out,
		}
	}

	var rootCmd *cobra.Command
	{
		var rootOpts struct {
			Version bool
		}

		rootCmd = &cobra.Command{
			Use:   "river",
			Short: "Provides command line facilities for the River job queue",
			Long: strings.TrimSpace(`
Provides command line facilities for the River job queue.

Commands that need database access will take a --database-url argument, but can
also accept Postgres configuration through the standard set of libpq environment
variables like PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD, and PGSSLMODE,
with a minimum of PGDATABASE required. --database-url will take precedence of
PG* vars if it's been specified.
		`),
			RunE: func(cmd *cobra.Command, args []string) error {
				if rootOpts.Version {
					return RunCommand(ctx, makeCommandBundle(nil), &version{}, &versionOpts{Name: c.name})
				}

				_ = cmd.Usage()
				return nil
			},
		}
		rootCmd.SetOut(c.out)

		rootCmd.PersistentFlags().BoolVar(&globalOpts.Debug, "debug", false, "output maximum logging verbosity (debug level)")
		rootCmd.PersistentFlags().BoolVarP(&globalOpts.Verbose, "verbose", "v", false, "output additional logging verbosity (info level)")
		rootCmd.MarkFlagsMutuallyExclusive("debug", "verbose")

		rootCmd.Flags().BoolVar(&rootOpts.Version, "version", false, "print version information")
	}

	addDatabaseURLFlag := func(cmd *cobra.Command, databaseURL *string) {
		cmd.Flags().StringVar(databaseURL, "database-url", "", "URL of the database (should look like `postgres://...)`")
	}
	addLineFlag := func(cmd *cobra.Command, line *string) {
		cmd.Flags().StringVar(line, "line", "", "migration line to operate on (default: main)")
	}

	// bench
	{
		var opts benchOpts

		cmd := &cobra.Command{
			Use:   "bench",
			Short: "Run River benchmark",
			Long: strings.TrimSpace(`
Run a River benchmark which inserts and works jobs continually, giving a rough
idea of jobs per second and time to work a single job.

By default, the benchmark will continuously insert and work jobs in perpetuity
until interrupted by SIGINT (Ctrl^C). It can alternatively take a maximum run
duration with --duration, which takes a Go-style duration string like 1m.
Lastly, it can take --num-total-jobs, which inserts the given number of jobs
before starting the client, and works until all jobs are finished.

The database in --database-url will have its jobs table truncated, so make sure
to use a development database only.
	`),
			RunE: func(cmd *cobra.Command, args []string) error {
				return RunCommand(ctx, makeCommandBundle(&opts.DatabaseURL), &bench{}, &opts)
			},
		}
		addDatabaseURLFlag(cmd, &opts.DatabaseURL)
		cmd.Flags().DurationVar(&opts.Duration, "duration", 0, "duration after which to stop benchmark, accepting Go-style durations like 1m, 5m30s")
		cmd.Flags().IntVarP(&opts.NumTotalJobs, "num-total-jobs", "n", 0, "number of jobs to insert before starting and which are worked down until finish")
		rootCmd.AddCommand(cmd)
	}

	// migrate-down and migrate-up share a set of options, so this is a way of
	// plugging in all the right flags to both so options and docstrings stay
	// consistent.
	addMigrateFlags := func(cmd *cobra.Command, opts *migrateOpts) {
		addDatabaseURLFlag(cmd, &opts.DatabaseURL)
		cmd.Flags().BoolVar(&opts.DryRun, "dry-run", false, "print information on migrations, but don't apply them")
		cmd.Flags().StringVar(&opts.Line, "line", "", "migration line to operate on (default: main)")
		cmd.Flags().IntVar(&opts.MaxSteps, "max-steps", 0, "maximum number of steps to migrate")
		cmd.Flags().BoolVar(&opts.ShowSQL, "show-sql", false, "show SQL of each migration")
		cmd.Flags().IntVar(&opts.TargetVersion, "target-version", 0, "target version to migrate to (final state includes this version, but none after it)")
	}

	// migrate-down
	{
		var opts migrateOpts

		cmd := &cobra.Command{
			Use:   "migrate-down",
			Short: "Run River schema down migrations",
			Long: strings.TrimSpace(`
Run down migrations to reverse the River database schema changes.

Defaults to running a single down migration. This behavior can be changed with
--max-steps or --target-version.

SQL being run can be output using --show-sql, and executing real database
operations can be prevented with --dry-run. Combine --show-sql and --dry-run to
dump prospective migrations that would be applied to stdout.
	`),
			RunE: func(cmd *cobra.Command, args []string) error {
				return RunCommand(ctx, makeCommandBundle(&opts.DatabaseURL), &migrateDown{}, &opts)
			},
		}
		addMigrateFlags(cmd, &opts)
		rootCmd.AddCommand(cmd)
	}

	// migrate-get
	{
		var opts migrateGetOpts

		cmd := &cobra.Command{
			Use:   "migrate-get",
			Short: "Get SQL for specific River migration",
			Long: strings.TrimSpace(`
Retrieve SQL for a single migration version. This command is aimed at cases
where using River's internal migration framework isn't desirable by allowing
migration SQL to be dumped for use elsewhere.

Specify a version with --version, and one of --down or --up:

    river migrate-get --version 3 --up > river3.up.sql
    river migrate-get --version 3 --down > river3.down.sql

Can also take multiple versions by separating them with commas or passing
--version multiple times:

    river migrate-get --version 1,2,3 --up > river.up.sql
    river migrate-get --version 3,2,1 --down > river.down.sql

Or use --all to print all known migrations in either direction. Often used in
conjunction with --exclude-version 1 to exclude the tables for River's migration
framework, which aren't necessary if using an external framework:

    river migrate-get --all --exclude-version 1 --up > river_all.up.sql
    river migrate-get --all --exclude-version 1 --down > river_all.down.sql
	`),
			RunE: func(cmd *cobra.Command, args []string) error {
				return RunCommand(ctx, makeCommandBundle(nil), &migrateGet{}, &opts)
			},
		}
		cmd.Flags().BoolVar(&opts.All, "all", false, "print all migrations; down migrations are printed in descending order")
		cmd.Flags().BoolVar(&opts.Down, "down", false, "print down migration")
		cmd.Flags().IntSliceVar(&opts.ExcludeVersion, "exclude-version", nil, "exclude version(s), usually version 1, containing River's migration tables")
		addLineFlag(cmd, &opts.Line)
		cmd.Flags().BoolVar(&opts.Up, "up", false, "print up migration")
		cmd.Flags().IntSliceVar(&opts.Version, "version", nil, "version(s) to print (can be multiple versions)")
		cmd.MarkFlagsMutuallyExclusive("all", "version")
		cmd.MarkFlagsOneRequired("all", "version")
		cmd.MarkFlagsMutuallyExclusive("down", "up")
		cmd.MarkFlagsOneRequired("down", "up")
		rootCmd.AddCommand(cmd)
	}

	// migrate-list
	{
		var opts migrateListOpts

		cmd := &cobra.Command{
			Use:   "migrate-list",
			Short: "List River schema migrations",
			Long: strings.TrimSpace(`
TODO
	`),
			RunE: func(cmd *cobra.Command, args []string) error {
				return RunCommand(ctx, makeCommandBundle(&opts.DatabaseURL), &migrateList{}, &opts)
			},
		}
		addDatabaseURLFlag(cmd, &opts.DatabaseURL)
		cmd.Flags().StringVar(&opts.Line, "line", "", "migration line to operate on (default: main)")
		rootCmd.AddCommand(cmd)
	}

	// migrate-up
	{
		var opts migrateOpts

		cmd := &cobra.Command{
			Use:   "migrate-up",
			Short: "Run River schema up migrations",
			Long: strings.TrimSpace(`
Run up migrations to raise the database schema necessary to run River.

Defaults to running all up migrations that aren't yet run. This behavior can be
restricted with --max-steps or --target-version.

SQL being run can be output using --show-sql, and executing real database
operations can be prevented with --dry-run. Combine --show-sql and --dry-run to
dump prospective migrations that would be applied to stdout.
	`),
			RunE: func(cmd *cobra.Command, args []string) error {
				return RunCommand(ctx, makeCommandBundle(&opts.DatabaseURL), &migrateUp{}, &opts)
			},
		}
		addMigrateFlags(cmd, &opts)
		rootCmd.AddCommand(cmd)
	}

	// validate
	{
		var opts validateOpts

		cmd := &cobra.Command{
			Use:   "validate",
			Short: "Validate River schema",
			Long: strings.TrimSpace(`
Validates the current River schema, exiting with a non-zero status in case there
are outstanding migrations that still need to be run.

Can be paired with river migrate-up --dry-run --show-sql to dump information on
migrations that need to be run, but without running them.
	`),
			RunE: func(cmd *cobra.Command, args []string) error {
				return RunCommand(ctx, makeCommandBundle(&opts.DatabaseURL), &validate{}, &opts)
			},
		}
		addDatabaseURLFlag(cmd, &opts.DatabaseURL)
		cmd.Flags().StringVar(&opts.Line, "line", "", "migration line to operate on (default: main)")
		rootCmd.AddCommand(cmd)
	}

	// version
	{
		cmd := &cobra.Command{
			Use:   "version",
			Short: "Print version information",
			Long: strings.TrimSpace(`
Print River and Go version information.
	`),
			RunE: func(cmd *cobra.Command, args []string) error {
				return RunCommand(ctx, makeCommandBundle(nil), &version{}, &versionOpts{Name: c.name})
			},
		}
		rootCmd.AddCommand(cmd)
	}

	return rootCmd
}

// SetOut sets standard output. Should be called before BaseCommandSet.
func (c *CLI) SetOut(out io.Writer) { c.out = out }

type benchOpts struct {
	DatabaseURL  string
	Debug        bool
	Duration     time.Duration
	NumTotalJobs int
	Verbose      bool
}

func (o *benchOpts) Validate() error {
	if o.DatabaseURL == "" && !pgEnvConfigured() {
		return errors.New("either PG* env vars or --database-url must be set")
	}

	return nil
}

type bench struct {
	CommandBase
}

func (c *bench) Run(ctx context.Context, opts *benchOpts) (bool, error) {
	if err := c.GetBenchmarker().Run(ctx, opts.Duration, opts.NumTotalJobs); err != nil {
		return false, err
	}
	return true, nil
}

type migrateOpts struct {
	DatabaseURL   string
	DryRun        bool
	Line          string
	ShowSQL       bool
	MaxSteps      int
	TargetVersion int
}

func (o *migrateOpts) Validate() error {
	if o.DatabaseURL == "" && !pgEnvConfigured() {
		return errors.New("either PG* env vars or --database-url must be set")
	}

	return nil
}

type migrateDown struct {
	CommandBase
}

func (c *migrateDown) Run(ctx context.Context, opts *migrateOpts) (bool, error) {
	migrator, err := c.GetMigrator(&rivermigrate.Config{Line: opts.Line, Logger: c.Logger})
	if err != nil {
		return false, err
	}

	res, err := migrator.Migrate(ctx, rivermigrate.DirectionDown, &rivermigrate.MigrateOpts{
		DryRun:        opts.DryRun,
		MaxSteps:      opts.MaxSteps,
		TargetVersion: opts.TargetVersion,
	})
	if err != nil {
		return false, err
	}

	migratePrintResult(c.Out, opts, res, rivermigrate.DirectionDown)

	return true, nil
}

// Rounds a duration so that it doesn't show so much cluttered and not useful
// precision in printf output.
func roundDuration(duration time.Duration) time.Duration {
	switch {
	case duration > 1*time.Second:
		return duration.Truncate(10 * time.Millisecond)
	case duration < 1*time.Millisecond:
		return duration.Truncate(10 * time.Nanosecond)
	default:
		return duration.Truncate(10 * time.Microsecond)
	}
}

func migratePrintResult(out io.Writer, opts *migrateOpts, res *rivermigrate.MigrateResult, direction rivermigrate.Direction) {
	if len(res.Versions) < 1 {
		fmt.Fprintf(out, "no migrations to apply\n")
		return
	}

	versionWithLongestName := slices.MaxFunc(res.Versions,
		func(v1, v2 rivermigrate.MigrateVersion) int { return len(v1.Name) - len(v2.Name) })

	for _, migrateVersion := range res.Versions {
		if opts.DryRun {
			fmt.Fprintf(out, "migration %03d [%s] [DRY RUN]\n", migrateVersion.Version, direction)
		} else {
			fmt.Fprintf(out, "applied migration %03d [%s] %-*s [%s]\n", migrateVersion.Version, direction, len(versionWithLongestName.Name), migrateVersion.Name, roundDuration(migrateVersion.Duration))
		}

		if opts.ShowSQL {
			fmt.Fprintf(out, "%s\n", strings.Repeat("-", 80))
			fmt.Fprintf(out, "%s\n", migrationComment(opts.Line, migrateVersion.Version, direction))
			fmt.Fprintf(out, "%s\n\n", strings.TrimSpace(migrateVersion.SQL))
		}
	}

	// Only prints if more steps than available were requested.
	if opts.MaxSteps > 0 && len(res.Versions) < opts.MaxSteps {
		fmt.Fprintf(out, "no more migrations to apply\n")
	}
}

// An informational comment that's tagged on top of any migration's SQL to help
// attribute what it is for when it's copied elsewhere like other migration
// frameworks.
func migrationComment(line string, version int, direction rivermigrate.Direction) string {
	return fmt.Sprintf("-- River %s migration %03d [%s]", line, version, direction)
}

type migrateGetOpts struct {
	All            bool
	Down           bool
	ExcludeVersion []int
	Line           string
	Up             bool
	Version        []int
}

func (o *migrateGetOpts) Validate() error { return nil }

type migrateGet struct {
	CommandBase
}

func (c *migrateGet) Run(_ context.Context, opts *migrateGetOpts) (bool, error) {
	// We'll need to have a way of using an alternate driver if support for
	// other databases is added in the future. Unlike other migrate commands,
	// this one doesn't take a `--database-url`, so we'd need a way of
	// detecting the database type.
	migrator, err := rivermigrate.New(c.DriverProcurer.ProcurePgxV5(nil), &rivermigrate.Config{Line: opts.Line, Logger: c.Logger})
	if err != nil {
		return false, err
	}

	var migrations []rivermigrate.Migration
	if opts.All {
		migrations = migrator.AllVersions()
		if opts.Down {
			slices.Reverse(migrations)
		}
	} else {
		for _, version := range opts.Version {
			migration, err := migrator.GetVersion(version)
			if err != nil {
				return false, err
			}

			migrations = append(migrations, migration)
		}
	}

	var printedOne bool

	for _, migration := range migrations {
		if slices.Contains(opts.ExcludeVersion, migration.Version) {
			continue
		}

		// print newlines between multiple versions
		if printedOne {
			fmt.Fprintf(c.Out, "\n")
		}

		var (
			direction rivermigrate.Direction
			sql       string
		)
		switch {
		case opts.Down:
			direction = rivermigrate.DirectionDown
			sql = migration.SQLDown
		case opts.Up:
			direction = rivermigrate.DirectionUp
			sql = migration.SQLUp
		}

		printedOne = true
		fmt.Fprintf(c.Out, "%s\n", migrationComment(opts.Line, migration.Version, direction))
		fmt.Fprintf(c.Out, "%s\n", strings.TrimSpace(sql))
	}

	return true, nil
}

type migrateListOpts struct {
	DatabaseURL string
	Line        string
}

func (o *migrateListOpts) Validate() error { return nil }

type migrateList struct {
	CommandBase
}

func (c *migrateList) Run(ctx context.Context, opts *migrateListOpts) (bool, error) {
	migrator, err := c.GetMigrator(&rivermigrate.Config{Line: opts.Line, Logger: c.Logger})
	if err != nil {
		return false, err
	}

	allMigrations := migrator.AllVersions()

	existingMigrations, err := migrator.ExistingVersions(ctx)
	if err != nil {
		return false, err
	}

	var maxExistingVersion int
	if len(existingMigrations) > 0 {
		maxExistingVersion = existingMigrations[len(existingMigrations)-1].Version
	}

	for _, migration := range allMigrations {
		var currentVersionPrefix string
		switch {
		case migration.Version == maxExistingVersion:
			currentVersionPrefix = "* "
		case maxExistingVersion > 0:
			currentVersionPrefix = "  "
		}

		fmt.Fprintf(c.Out, "%s%03d %s\n", currentVersionPrefix, migration.Version, migration.Name)
	}

	return true, nil
}

type migrateUp struct {
	CommandBase
}

func (c *migrateUp) Run(ctx context.Context, opts *migrateOpts) (bool, error) {
	migrator, err := c.GetMigrator(&rivermigrate.Config{Line: opts.Line, Logger: c.Logger})
	if err != nil {
		return false, err
	}

	res, err := migrator.Migrate(ctx, rivermigrate.DirectionUp, &rivermigrate.MigrateOpts{
		DryRun:        opts.DryRun,
		MaxSteps:      opts.MaxSteps,
		TargetVersion: opts.TargetVersion,
	})
	if err != nil {
		return false, err
	}

	migratePrintResult(c.Out, opts, res, rivermigrate.DirectionUp)

	return true, nil
}

type validateOpts struct {
	DatabaseURL string
	Line        string
}

func (o *validateOpts) Validate() error {
	if o.DatabaseURL == "" && !pgEnvConfigured() {
		return errors.New("either PG* env vars or --database-url must be set")
	}

	return nil
}

type validate struct {
	CommandBase
}

func (c *validate) Run(ctx context.Context, opts *validateOpts) (bool, error) {
	migrator, err := c.GetMigrator(&rivermigrate.Config{Line: opts.Line, Logger: c.Logger})
	if err != nil {
		return false, err
	}

	res, err := migrator.Validate(ctx)
	if err != nil {
		return false, err
	}

	return res.OK, nil
}

type versionOpts struct {
	Name string
}

func (o *versionOpts) Validate() error {
	if o.Name == "" {
		return errors.New("name should be set")
	}

	return nil
}

type version struct {
	CommandBase
}

func (c *version) Run(ctx context.Context, opts *versionOpts) (bool, error) {
	buildInfo, _ := debug.ReadBuildInfo()

	// Go 1.24 appears to have changed the build version to "(devel)" even when
	// using a release version of Go. This is a workaround to print "(unknown)"
	// in that case to match previous versions. We could relax the test instead
	// but it's unclear if this is a permanent change or not.
	buildVersion := buildInfo.Main.Version
	if strings.HasPrefix(buildInfo.GoVersion, "go1.24.") && buildVersion == "(devel)" {
		buildVersion = ""
	}

	fmt.Fprintf(c.Out, "%s version %s\n", opts.Name, valutil.ValOrDefault(buildVersion, "(unknown)"))
	fmt.Fprintf(c.Out, "Built with %s\n", buildInfo.GoVersion)

	return true, nil
}

```

`cmd/river/rivercli/river_cli_test.go`:

```go
package rivercli

import (
	"bytes"
	"cmp"
	"context"
	"fmt"
	"net/url"
	"os"
	"runtime/debug"
	"strings"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/spf13/cobra"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivermigrate"
	"github.com/riverqueue/river/rivershared/riversharedtest"
)

type MigratorStub struct {
	allVersionsStub      func() []rivermigrate.Migration
	existingVersionsStub func(ctx context.Context) ([]rivermigrate.Migration, error)
	getVersionStub       func(version int) (rivermigrate.Migration, error)
	migrateStub          func(ctx context.Context, direction rivermigrate.Direction, opts *rivermigrate.MigrateOpts) (*rivermigrate.MigrateResult, error)
	validateStub         func(ctx context.Context) (*rivermigrate.ValidateResult, error)
}

func (m *MigratorStub) AllVersions() []rivermigrate.Migration {
	if m.allVersionsStub == nil {
		panic("AllVersions is not stubbed")
	}

	return m.allVersionsStub()
}

func (m *MigratorStub) ExistingVersions(ctx context.Context) ([]rivermigrate.Migration, error) {
	if m.allVersionsStub == nil {
		panic("ExistingVersions is not stubbed")
	}

	return m.existingVersionsStub(ctx)
}

func (m *MigratorStub) GetVersion(version int) (rivermigrate.Migration, error) {
	if m.allVersionsStub == nil {
		panic("GetVersion is not stubbed")
	}

	return m.getVersionStub(version)
}

func (m *MigratorStub) Migrate(ctx context.Context, direction rivermigrate.Direction, opts *rivermigrate.MigrateOpts) (*rivermigrate.MigrateResult, error) {
	if m.allVersionsStub == nil {
		panic("Migrate is not stubbed")
	}

	return m.migrateStub(ctx, direction, opts)
}

func (m *MigratorStub) Validate(ctx context.Context) (*rivermigrate.ValidateResult, error) {
	if m.allVersionsStub == nil {
		panic("Validate is not stubbed")
	}

	return m.validateStub(ctx)
}

var (
	testMigration01 = rivermigrate.Migration{Name: "1st migration", SQLDown: "SELECT 1", SQLUp: "SELECT 1", Version: 1} //nolint:gochecknoglobals
	testMigration02 = rivermigrate.Migration{Name: "2nd migration", SQLDown: "SELECT 1", SQLUp: "SELECT 1", Version: 2} //nolint:gochecknoglobals
	testMigration03 = rivermigrate.Migration{Name: "3rd migration", SQLDown: "SELECT 1", SQLUp: "SELECT 1", Version: 3} //nolint:gochecknoglobals

	testMigrationAll = []rivermigrate.Migration{testMigration01, testMigration02, testMigration03} //nolint:gochecknoglobals
)

type TestDriverProcurer struct{}

func (p *TestDriverProcurer) ProcurePgxV5(pool *pgxpool.Pool) riverdriver.Driver[pgx.Tx] {
	return riverpgxv5.New(pool)
}

// High level integration tests that operate on the Cobra command directly. This
// isn't always appropriate because there's no way to inject a test transaction.
func TestBaseCommandSetIntegration(t *testing.T) {
	t.Parallel()

	type testBundle struct {
		out *bytes.Buffer
	}

	setup := func(t *testing.T) (*cobra.Command, *testBundle) {
		t.Helper()

		cli := NewCLI(&Config{
			DriverProcurer: &TestDriverProcurer{},
			Name:           "River",
		})

		var out bytes.Buffer
		cli.SetOut(&out)

		return cli.BaseCommandSet(), &testBundle{
			out: &out,
		}
	}

	t.Run("DebugVerboseMutuallyExclusive", func(t *testing.T) {
		t.Parallel()

		cmd, _ := setup(t)

		cmd.SetArgs([]string{"--debug", "--verbose"})
		require.EqualError(t, cmd.Execute(), "if any flags in the group [debug verbose] are set none of the others can be; [debug verbose] were all set")
	})

	t.Run("DatabaseURLWithInvalidPrefix", func(t *testing.T) {
		t.Parallel()

		cmd, _ := setup(t)

		cmd.SetArgs([]string{"migrate-down", "--database-url", "post://"})
		require.EqualError(t, cmd.Execute(), "unsupported database URL (`post://`); try one with a `postgres://` or `postgresql://` scheme/prefix")
	})

	t.Run("MissingDatabaseURLAndPGEnv", func(t *testing.T) {
		t.Parallel()

		cmd, _ := setup(t)

		cmd.SetArgs([]string{"migrate-down"})
		require.EqualError(t, cmd.Execute(), "either PG* env vars or --database-url must be set")
	})

	t.Run("VersionFlag", func(t *testing.T) {
		t.Parallel()

		cmd, bundle := setup(t)

		cmd.SetArgs([]string{"--version"})
		require.NoError(t, cmd.Execute())

		buildInfo, _ := debug.ReadBuildInfo()

		require.Equal(t, strings.TrimSpace(fmt.Sprintf(`
River version (unknown)
Built with %s
		`, buildInfo.GoVersion)), strings.TrimSpace(bundle.out.String()))
	})

	t.Run("VersionSubcommand", func(t *testing.T) {
		t.Parallel()

		cmd, bundle := setup(t)

		cmd.SetArgs([]string{"version"})
		require.NoError(t, cmd.Execute())

		buildInfo, _ := debug.ReadBuildInfo()

		require.Equal(t, strings.TrimSpace(fmt.Sprintf(`
River version (unknown)
Built with %s
		`, buildInfo.GoVersion)), strings.TrimSpace(bundle.out.String()))
	})
}

// Same as the above, but non-parallel so tests can use `t.Setenv`. Separated
// out into its own test block so that we don't have to mark the entire block
// above as non-parallel because a few tests can't be made parallel.
func TestBaseCommandSetNonParallel(t *testing.T) {
	type testBundle struct {
		out *bytes.Buffer
	}

	setup := func(t *testing.T) (*cobra.Command, *testBundle) {
		t.Helper()

		cli := NewCLI(&Config{
			DriverProcurer: &TestDriverProcurer{},
			Name:           "River",
		})

		var out bytes.Buffer
		cli.SetOut(&out)

		return cli.BaseCommandSet(), &testBundle{
			out: &out,
		}
	}

	t.Run("PGEnvWithoutDatabaseURL", func(t *testing.T) {
		cmd, _ := setup(t)

		// Deconstruct a database URL into its PG* components. This path is the
		// one that gets taken in CI, but could work locally as well.
		if databaseURL := os.Getenv("TEST_DATABASE_URL"); databaseURL != "" {
			parsedURL, err := url.Parse(databaseURL)
			require.NoError(t, err)

			t.Setenv("PGDATABASE", parsedURL.Path[1:])
			t.Setenv("PGHOST", parsedURL.Hostname())
			pass, _ := parsedURL.User.Password()
			t.Setenv("PGPASSWORD", pass)
			t.Setenv("PGPORT", cmp.Or(parsedURL.Port(), "5432"))
			t.Setenv("PGSSLMODE", parsedURL.Query().Get("sslmode"))
			t.Setenv("PGUSER", parsedURL.User.Username())
		} else {
			// With no `TEST_DATABASE_URL` available, try a simpler alternative
			// configuration. Requires a database on localhost that doesn't
			// require authentication, which should exist from testdbman.
			t.Setenv("PGDATABASE", "river_test")
			t.Setenv("PGHOST", "localhost")
		}

		cmd.SetArgs([]string{"validate"})
		require.NoError(t, cmd.Execute())
	})
}

func TestMigrateList(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		migratorStub *MigratorStub
		out          *bytes.Buffer
	}

	setup := func(t *testing.T) (*migrateList, *testBundle) {
		t.Helper()

		cmd, out := withCommandBase(t, &migrateList{})

		migratorStub := &MigratorStub{}
		migratorStub.allVersionsStub = func() []rivermigrate.Migration { return testMigrationAll }
		migratorStub.existingVersionsStub = func(ctx context.Context) ([]rivermigrate.Migration, error) { return nil, nil }

		cmd.GetCommandBase().GetMigrator = func(config *rivermigrate.Config) (MigratorInterface, error) { return migratorStub, nil }

		return cmd, &testBundle{
			out:          out,
			migratorStub: migratorStub,
		}
	}

	t.Run("NoExistingMigrations", func(t *testing.T) {
		t.Parallel()

		cmd, bundle := setup(t)

		_, err := runCommand(ctx, t, cmd, &migrateListOpts{})
		require.NoError(t, err)

		require.Equal(t, strings.TrimSpace(`
001 1st migration
002 2nd migration
003 3rd migration
		`), strings.TrimSpace(bundle.out.String()))
	})

	t.Run("WithExistingMigrations", func(t *testing.T) {
		t.Parallel()

		cmd, bundle := setup(t)

		bundle.migratorStub.existingVersionsStub = func(ctx context.Context) ([]rivermigrate.Migration, error) {
			return []rivermigrate.Migration{testMigration01, testMigration02}, nil
		}

		_, err := runCommand(ctx, t, cmd, &migrateListOpts{})
		require.NoError(t, err)

		require.Equal(t, strings.TrimSpace(`
  001 1st migration
* 002 2nd migration
  003 3rd migration
		`), strings.TrimSpace(bundle.out.String()))
	})
}

func TestVersion(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		buf *bytes.Buffer
	}

	setup := func(t *testing.T) (*version, *testBundle) {
		t.Helper()

		cmd, buf := withCommandBase(t, &version{})

		return cmd, &testBundle{
			buf: buf,
		}
	}

	t.Run("PrintsVersion", func(t *testing.T) {
		t.Parallel()

		cmd, bundle := setup(t)

		_, err := runCommand(ctx, t, cmd, &versionOpts{Name: "River"})
		require.NoError(t, err)

		buildInfo, _ := debug.ReadBuildInfo()

		require.Equal(t, strings.TrimSpace(fmt.Sprintf(`
River version (unknown)
Built with %s
		`, buildInfo.GoVersion)), strings.TrimSpace(bundle.buf.String()))
	})
}

// runCommand runs a CLI command while doing some additional niceties like
// validating options.
func runCommand[TCommand Command[TOpts], TOpts CommandOpts](ctx context.Context, t *testing.T, cmd TCommand, opts TOpts) (bool, error) {
	t.Helper()

	require.NoError(t, opts.Validate())

	return cmd.Run(ctx, opts)
}

func withCommandBase[TCommand Command[TOpts], TOpts CommandOpts](t *testing.T, cmd TCommand) (TCommand, *bytes.Buffer) {
	t.Helper()

	var out bytes.Buffer
	cmd.SetCommandBase(&CommandBase{
		Logger: riversharedtest.Logger(t),
		Out:    &out,

		GetMigrator: func(config *rivermigrate.Config) (MigratorInterface, error) { return &MigratorStub{}, nil },
	})
	return cmd, &out
}

func TestMigrationComment(t *testing.T) {
	t.Parallel()

	require.Equal(t, "-- River main migration 001 [down]", migrationComment("main", 1, rivermigrate.DirectionDown))
	require.Equal(t, "-- River main migration 002 [up]", migrationComment("main", 2, rivermigrate.DirectionUp))
}

func TestRoundDuration(t *testing.T) {
	t.Parallel()

	mustParseDuration := func(s string) time.Duration {
		d, err := time.ParseDuration(s)
		require.NoError(t, err)
		return d
	}

	require.Equal(t, "1.33µs", roundDuration(mustParseDuration("1.332µs")).String())
	require.Equal(t, "765.62µs", roundDuration(mustParseDuration("765.625µs")).String())
	require.Equal(t, "4.42ms", roundDuration(mustParseDuration("4.422125ms")).String())
	require.Equal(t, "13.28ms", roundDuration(mustParseDuration("13.280834ms")).String())
	require.Equal(t, "234.91ms", roundDuration(mustParseDuration("234.91075ms")).String())
	require.Equal(t, "3.93s", roundDuration(mustParseDuration("3.937042s")).String())
	require.Equal(t, "34.04s", roundDuration(mustParseDuration("34.042234s")).String())
	require.Equal(t, "2m34.04s", roundDuration(mustParseDuration("2m34.042234s")).String())
}

```

`common_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"time"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/rivershared/riversharedtest"
)

//
// This file used as a holding place for test helpers for examples so that the
// helpers aren't included in Godoc and keep each example more succinct.
//

type NoOpArgs struct{}

func (NoOpArgs) Kind() string { return "no_op" }

type NoOpWorker struct {
	river.WorkerDefaults[NoOpArgs]
}

func (w *NoOpWorker) Work(ctx context.Context, job *river.Job[NoOpArgs]) error {
	fmt.Printf("NoOpWorker.Work ran\n")
	return nil
}

// Wait on the given subscription channel for numJobs. Times out with a panic if
// jobs take too long to be received.
func waitForNJobs(subscribeChan <-chan *river.Event, numJobs int) {
	var (
		timeout  = riversharedtest.WaitTimeout()
		deadline = time.Now().Add(timeout)
		events   = make([]*river.Event, 0, numJobs)
	)

	for {
		select {
		case event := <-subscribeChan:
			events = append(events, event)

			if len(events) >= numJobs {
				return
			}

		case <-time.After(time.Until(deadline)):
			panic(fmt.Sprintf("waitForNJobs timed out after waiting %s (received %d job(s), wanted %d)",
				timeout, len(events), numJobs))
		}
	}
}

```

`context.go`:

```go
package river

import (
	"context"
	"errors"

	"github.com/riverqueue/river/internal/rivercommon"
)

var errClientNotInContext = errors.New("river: client not found in context, can only be used in a Worker")

func withClient[TTx any](ctx context.Context, client *Client[TTx]) context.Context {
	return context.WithValue(ctx, rivercommon.ContextKeyClient{}, client)
}

// ClientFromContext returns the Client from the context. This function can
// only be used within a Worker's Work() method because that is the only place
// River sets the Client on the context.
//
// It panics if the context does not contain a Client, which will never happen
// from the context provided to a Worker's Work() method.
//
// When testing JobArgs.Work implementations, it might be useful to use
// rivertest.WorkContext to initialize a context that has an available client.
//
// The type parameter TTx is the transaction type used by the [Client],
// pgx.Tx for the pgx driver, and *sql.Tx for the [database/sql] driver.
func ClientFromContext[TTx any](ctx context.Context) *Client[TTx] {
	client, err := ClientFromContextSafely[TTx](ctx)
	if err != nil {
		panic(err)
	}
	return client
}

// ClientFromContext returns the Client from the context. This function can
// only be used within a Worker's Work() method because that is the only place
// River sets the Client on the context.
//
// It returns an error if the context does not contain a Client, which will
// never happen from the context provided to a Worker's Work() method.
//
// When testing JobArgs.Work implementations, it might be useful to use
// rivertest.WorkContext to initialize a context that has an available client.
//
// See the examples for [ClientFromContext] to understand how to use this
// function.
func ClientFromContextSafely[TTx any](ctx context.Context) (*Client[TTx], error) {
	client, exists := ctx.Value(rivercommon.ContextKeyClient{}).(*Client[TTx])
	if !exists || client == nil {
		return nil, errClientNotInContext
	}
	return client, nil
}

```

`context_test.go`:

```go
package river

import (
	"context"
	"testing"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"
)

func TestClientFromContext(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	client := &Client[pgx.Tx]{}
	ctx = withClient(ctx, client)

	require.Equal(t, client, ClientFromContext[pgx.Tx](ctx))

	result, err := ClientFromContextSafely[pgx.Tx](ctx)
	require.NoError(t, err)
	require.Equal(t, client, result)

	require.PanicsWithError(t, errClientNotInContext.Error(), func() {
		ClientFromContext[pgx.Tx](context.Background())
	})

	result, err = ClientFromContextSafely[pgx.Tx](context.Background())
	require.ErrorIs(t, err, errClientNotInContext)
	require.Nil(t, result)
}

```

`doc.go`:

```go
/*
Package river is a robust high-performance job processing system for Go and
Postgres.

See [homepage], [docs], and [godoc], as well as the [River UI].

Being built for Postgres, River encourages the use of the same database for
application data and job queue. By enqueueing jobs transactionally along with
other database changes, whole classes of distributed systems problems are
avoided. Jobs are guaranteed to be enqueued if their transaction commits, are
removed if their transaction rolls back, and aren't visible for work _until_
commit. See [transactional enqueueing] for more background on this philosophy.

# Job args and workers

Jobs are defined in struct pairs, with an implementation of [`JobArgs`] and one
of [`Worker`].

Job args contain `json` annotations and define how jobs are serialized to and
from the database, along with a "kind", a stable string that uniquely identifies
the job.

	type SortArgs struct {
		// Strings is a slice of strings to sort.
		Strings []string `json:"strings"`
	}

	func (SortArgs) Kind() string { return "sort" }

Workers expose a `Work` function that dictates how jobs run.

	type SortWorker struct {
	    // An embedded WorkerDefaults sets up default methods to fulfill the rest of
	    // the Worker interface:
	    river.WorkerDefaults[SortArgs]
	}

	func (w *SortWorker) Work(ctx context.Context, job *river.Job[SortArgs]) error {
	    sort.Strings(job.Args.Strings)
	    fmt.Printf("Sorted strings: %+v\n", job.Args.Strings)
	    return nil
	}

# Registering workers

Jobs are uniquely identified by their "kind" string. Workers are registered on
start up so that River knows how to assign jobs to workers:

	workers := river.NewWorkers()
	// AddWorker panics if the worker is already registered or invalid:
	river.AddWorker(workers, &SortWorker{})

# Starting a client

A River [`Client`] provides an interface for job insertion and manages job
processing and [maintenance services]. A client's created with a database pool,
[driver], and config struct containing a `Workers` bundle and other settings.
Here's a client `Client` working one queue (`"default"`) with up to 100 worker
goroutines at a time:

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
	    Queues: map[string]river.QueueConfig{
	        river.QueueDefault: {MaxWorkers: 100},
	    },
	    Workers: workers,
	})
	if err != nil {
	    panic(err)
	}

	// Run the client inline. All executed jobs will inherit from ctx:
	if err := riverClient.Start(ctx); err != nil {
	    panic(err)
	}

## Insert-only clients

It's often desirable to have a client that'll be used for inserting jobs, but
not working them. This is possible by omitting the `Queues` configuration, and
skipping the call to `Start`:

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
	    Workers: workers,
	})
	if err != nil {
	    panic(err)
	}

`Workers` can also be omitted, but it's better to include it so River can check
that inserted job kinds have a worker that can run them.

## Stopping

The client should also be stopped on program shutdown:

	// Stop fetching new work and wait for active jobs to finish.
	if err := riverClient.Stop(ctx); err != nil {
	    panic(err)
	}

There are some complexities around ensuring clients stop cleanly, but also in a
timely manner. See [graceful shutdown] for more details on River's stop modes.

# Inserting jobs

[`Client.InsertTx`] is used in conjunction with an instance of job args to
insert a job to work on a transaction:

	_, err = riverClient.InsertTx(ctx, tx, SortArgs{
	    Strings: []string{
	        "whale", "tiger", "bear",
	    },
	}, nil)

	if err != nil {
	    panic(err)
	}

See the [`InsertAndWork` example] for complete code.

# Other features

  - [Batch job insertion] for efficiently inserting many jobs at once using
    Postgres `COPY FROM`.

  - [Cancelling jobs] from inside a work function.

  - [Error and panic handling].

  - [Multiple queues] to better guarantee job throughput, worker availability,
    and isolation between components.

  - [Periodic and cron jobs].

  - [Scheduled jobs] that run automatically at their scheduled time in the
    future.

  - [Snoozing jobs] from inside a work function.

  - [Subscriptions] to queue activity and statistics, providing easy hooks for
    telemetry like logging and metrics.

  - [Test helpers] to verify that jobs are inserted as expected.

  - [Transactional job completion] to guarantee job completion commits with
    other changes in a transaction.

  - [Unique jobs] by args, period, queue, and state.

  - [Web UI] for inspecting and interacting with jobs and queues.

  - [Work functions] for simplified worker implementation.

## Cross language enqueueing

River supports inserting jobs in some non-Go languages which are then worked by Go implementations. This may be desirable in performance sensitive cases so that jobs can take advantage of Go's fast runtime.

  - [Inserting jobs from Python].
  - [Inserting jobs from Ruby].

# Development

See [developing River].

[`Client`]: https://pkg.go.dev/github.com/riverqueue/river#Client
[`Client.InsertTx`]: https://pkg.go.dev/github.com/riverqueue/river#Client.InsertTx
[`InsertAndWork` example]: https://pkg.go.dev/github.com/riverqueue/river#example-package-InsertAndWork
[`JobArgs`]: https://pkg.go.dev/github.com/riverqueue/river#JobArgs
[`Worker`]: https://pkg.go.dev/github.com/riverqueue/river#Worker
[Batch job insertion]: https://riverqueue.com/docs/batch-job-insertion
[Cancelling jobs]: https://riverqueue.com/docs/cancelling-jobs
[Error and panic handling]: https://riverqueue.com/docs/error-handling
[Inserting jobs from Python]: https://riverqueue.com/docs/python
[Inserting jobs from Ruby]: https://riverqueue.com/docs/ruby
[Multiple queues]: https://riverqueue.com/docs/multiple-queues
[Periodic and cron jobs]: https://riverqueue.com/docs/periodic-jobs
[River UI]: https://github.com/riverqueue/riverui
[Scheduled jobs]: https://riverqueue.com/docs/scheduled-jobs
[Snoozing jobs]: https://riverqueue.com/docs/snoozing-jobs
[Subscriptions]: https://riverqueue.com/docs/subscriptions
[Test helpers]: https://riverqueue.com/docs/testing
[Transactional job completion]: https://riverqueue.com/docs/transactional-job-completion
[Unique jobs]: https://riverqueue.com/docs/unique-jobs
[Web UI]: https://github.com/riverqueue/riverui
[Work functions]: https://riverqueue.com/docs/work-functions
[docs]: https://riverqueue.com/docs
[developing River]: https://github.com/riverqueue/river/blob/master/docs/development.md
[driver]: https://riverqueue.com/docs/database-drivers
[godoc]: https://pkg.go.dev/github.com/riverqueue/river
[graceful shutdown]: https://riverqueue.com/docs/graceful-shutdown
[homepage]: https://riverqueue.com
[maintenance services]: https://riverqueue.com/docs/maintenance-services
[transactional enqueueing]: https://riverqueue.com/docs/transactional-enqueueing
*/
package river

```

`docs/README.md`:

```md
# River [![Build Status](https://github.com/riverqueue/river/actions/workflows/ci.yaml/badge.svg?branch=master)](https://github.com/riverqueue/river/actions) [![Go Reference](https://pkg.go.dev/badge/github.com/riverqueue/river.svg)](https://pkg.go.dev/github.com/riverqueue/river)

River is a robust high-performance job processing system for Go and Postgres.

See [homepage], [docs], and [godoc], as well as the [River UI].

Being built for Postgres, River encourages the use of the same database for
application data and job queue. By enqueueing jobs transactionally along with
other database changes, whole classes of distributed systems problems are
avoided. Jobs are guaranteed to be enqueued if their transaction commits, are
removed if their transaction rolls back, and aren't visible for work _until_
commit. See [transactional enqueueing] for more background on this philosophy.

## Job args and workers

Jobs are defined in struct pairs, with an implementation of [`JobArgs`] and one
of [`Worker`].

Job args contain `json` annotations and define how jobs are serialized to and
from the database, along with a "kind", a stable string that uniquely identifies
the job.

```go
type SortArgs struct {
    // Strings is a slice of strings to sort.
    Strings []string `json:"strings"`
}

func (SortArgs) Kind() string { return "sort" }
```

Workers expose a `Work` function that dictates how jobs run.

```go
type SortWorker struct {
    // An embedded WorkerDefaults sets up default methods to fulfill the rest of
    // the Worker interface:
    river.WorkerDefaults[SortArgs]
}

func (w *SortWorker) Work(ctx context.Context, job *river.Job[SortArgs]) error {
    sort.Strings(job.Args.Strings)
    fmt.Printf("Sorted strings: %+v\n", job.Args.Strings)
    return nil
}
```

## Registering workers

Jobs are uniquely identified by their "kind" string. Workers are registered on
start up so that River knows how to assign jobs to workers:

```go
workers := river.NewWorkers()
// AddWorker panics if the worker is already registered or invalid:
river.AddWorker(workers, &SortWorker{})
```

## Starting a client

A River [`Client`] provides an interface for job insertion and manages job
processing and [maintenance services]. A client's created with a database pool,
[driver], and config struct containing a `Workers` bundle and other settings.
Here's a client `Client` working one queue (`"default"`) with up to 100 worker
goroutines at a time:

```go
riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
    Queues: map[string]river.QueueConfig{
        river.QueueDefault: {MaxWorkers: 100},
    },
    Workers: workers,
})
if err != nil {
    panic(err)
}

// Run the client inline. All executed jobs will inherit from ctx:
if err := riverClient.Start(ctx); err != nil {
    panic(err)
}
```

## Insert-only clients

It's often desirable to have a client that'll be used for inserting jobs, but
not working them. This is possible by omitting the `Queues` configuration, and
skipping the call to `Start`:

```go
riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
    Workers: workers,
})
if err != nil {
    panic(err)
}
```

`Workers` can also be omitted, but it's better to include it so River can check
that inserted job kinds have a worker that can run them.

### Stopping

The client should also be stopped on program shutdown:

```go
// Stop fetching new work and wait for active jobs to finish.
if err := riverClient.Stop(ctx); err != nil {
    panic(err)
}
```

There are some complexities around ensuring clients stop cleanly, but also in a
timely manner. See [graceful shutdown] for more details on River's stop modes.

## Inserting jobs

[`Client.InsertTx`] is used in conjunction with an instance of job args to
insert a job to work on a transaction:

```go
_, err = riverClient.InsertTx(ctx, tx, SortArgs{
    Strings: []string{
        "whale", "tiger", "bear",
    },
}, nil)

if err != nil {
    panic(err)
}
```

See the [`InsertAndWork` example] for complete code.

## Other features

  - [Batch job insertion] for efficiently inserting many jobs at once using
    Postgres `COPY FROM`.

  - [Cancelling jobs] from inside a work function.

  - [Error and panic handling].

  - [Multiple queues] to better guarantee job throughput, worker availability,
    and isolation between components.

  - [Periodic and cron jobs].

  - [Scheduled jobs] that run automatically at their scheduled time in the
    future.

  - [Snoozing jobs] from inside a work function.

  - [Subscriptions] to queue activity and statistics, providing easy hooks for
    telemetry like logging and metrics.

  - [Test helpers] to verify that jobs are inserted as expected.

  - [Transactional job completion] to guarantee job completion commits with
    other changes in a transaction.

  - [Unique jobs] by args, period, queue, and state.

  - [Web UI] for inspecting and interacting with jobs and queues.

  - [Work functions] for simplified worker implementation.

## Cross language enqueueing

River supports inserting jobs in some non-Go languages which are then worked by Go implementations. This may be desirable in performance sensitive cases so that jobs can take advantage of Go's fast runtime.

  - [Inserting jobs from Python](https://riverqueue.com/docs/python).
  - [Inserting jobs from Ruby](https://riverqueue.com/docs/ruby).

## Development

See [developing River].

## Thank you

River was in large part inspired by our experiences with other background job libraries over the years, most notably:

- [Oban](https://github.com/sorentwo/oban) in Elixir.
- [Que](https://github.com/que-rb/que), [Sidekiq](https://github.com/sidekiq/sidekiq), [Delayed::Job](https://github.com/collectiveidea/delayed_job), and [GoodJob](https://github.com/bensheldon/good_job) in Ruby.
- [Hangfire](https://www.hangfire.io/) in .NET.

Thank you for driving the software ecosystem forward.

[`Client`]: https://pkg.go.dev/github.com/riverqueue/river#Client
[`Client.InsertTx`]: https://pkg.go.dev/github.com/riverqueue/river#Client.InsertTx
[`InsertAndWork` example]: https://pkg.go.dev/github.com/riverqueue/river#example-package-InsertAndWork
[`JobArgs`]: https://pkg.go.dev/github.com/riverqueue/river#JobArgs
[`Worker`]: https://pkg.go.dev/github.com/riverqueue/river#Worker
[Batch job insertion]: https://riverqueue.com/docs/batch-job-insertion
[Cancelling jobs]: https://riverqueue.com/docs/cancelling-jobs
[Error and panic handling]: https://riverqueue.com/docs/error-handling
[Multiple queues]: https://riverqueue.com/docs/multiple-queues
[Periodic and cron jobs]: https://riverqueue.com/docs/periodic-jobs
[River UI]: https://github.com/riverqueue/riverui
[Scheduled jobs]: https://riverqueue.com/docs/scheduled-jobs
[Snoozing jobs]: https://riverqueue.com/docs/snoozing-jobs
[Subscriptions]: https://riverqueue.com/docs/subscriptions
[Test helpers]: https://riverqueue.com/docs/testing
[Transactional job completion]: https://riverqueue.com/docs/transactional-job-completion
[Unique jobs]: https://riverqueue.com/docs/unique-jobs
[Web UI]: https://github.com/riverqueue/riverui
[Work functions]: https://riverqueue.com/docs/work-functions
[developing River]: https://github.com/riverqueue/river/blob/master/docs/development.md
[docs]: https://riverqueue.com/docs
[driver]: https://riverqueue.com/docs/database-drivers
[godoc]: https://pkg.go.dev/github.com/riverqueue/river
[graceful shutdown]: https://riverqueue.com/docs/graceful-shutdown
[homepage]: https://riverqueue.com
[maintenance services]: https://riverqueue.com/docs/maintenance-services
[riverui]: https://github.com/riverqueue/riverui
[transactional enqueueing]: https://riverqueue.com/docs/transactional-enqueueing

```

`docs/development.md`:

```md
# River development

## Run tests

Raise test databases:

    go run ./internal/cmd/testdbman create

Run tests:

    go test ./... -p 1

## Run lint

Run the linter and try to autofix:

    golangci-lint run --fix

## Generate sqlc

The project uses sqlc (`brew install sqlc`) to generate Go targets for Postgres
queries. After changing an sqlc `.sql` file, generate Go with:

    make generate

## Releasing a new version

1. Fetch changes to the repo and any new tags. Export `VERSION` by incrementing the last tag. Execute `update-mod-version` to add it the project's `go.mod` files:

    ```shell
    git checkout master && git pull --rebase
    export VERSION=v0.x.y
    make update-mod-version
    git checkout -b $USER-$VERSION
    ```

2. Prepare a PR with the changes, updating `CHANGELOG.md` with any necessary additions at the same time. Include **`[skip ci]`** in the commit description so that CI doesn't run and pollute the Go Module cache by trying to fetch a version that's not available yet (and it would fail anyway). Have it reviewed and merged.

3. Upon merge, pull down the changes, tag each module with the new version, and push the new tags:

    ```shell
    git checkout master && git pull --rebase
    git tag cmd/river/$VERSION -m "release cmd/river/$VERSION"
    git tag riverdriver/$VERSION -m "release riverdriver/$VERSION"
    git tag riverdriver/riverpgxv5/$VERSION -m "release riverdriver/riverpgxv5/$VERSION"
    git tag riverdriver/riverdatabasesql/$VERSION -m "release riverdriver/riverdatabasesql/$VERSION"
    git tag rivershared/$VERSION -m "release rivershared/$VERSION"
    git tag rivertype/$VERSION -m "release rivertype/$VERSION"
    git tag $VERSION
    ```

4. Push new tags to GitHub:

    ```shell
    git push --tags
    ```

5. Cut a new GitHub release by visiting [new release](https://github.com/riverqueue/river/releases/new), selecting the new tag, and copying in the version's `CHANGELOG.md` content as the release body.

### Updating Go or toolchain versions in all `go.mod` files

Modify `go.work` so it contains the new desired version in `go` and/or `toolchain` directives, then run `make update-mod-go` to have it reflect the new version(s) into all the workspace's `go.mod` files:

```shell
make update-mod-go
```
```

`docs/state_machine.md`:

```md
```mermaid
flowchart LR
    %% Define Styles
    classDef waiting fill:#f9f,stroke:#333,stroke-width:2px;
    classDef available fill:#9fc,stroke:#333,stroke-width:2px;
    classDef running fill:#ff9,stroke:#333,stroke-width:2px;
    classDef retryable fill:#ffcc00,stroke:#333,stroke-width:2px;
    classDef final fill:#9f9,stroke:#333,stroke-width:2px,color:black;
    classDef finalFailed fill:#f99,stroke:#333,stroke-width:2px,color:black;
    classDef retryLine stroke-dasharray: 5 5

    %% Apply Styles
    A:::available
    S:::waiting
    P:::waiting
    R:::running
    Re:::retryable
    C:::final
    Ca:::finalFailed
    D:::finalFailed

    %% Define Initial States
    subgraph Initial_States
        A["Available"]
        S["Scheduled"]
        P["Pending"]
    end

    %% Define Intermediate States
    R["Running"]
    Re["Retryable"]

    %% Define Final States
    subgraph Finalized
        C["Completed"]
        Ca["Cancelled"]
        D["Discarded"]
    end


    %% Main Flow
    A -- fetched --> R
    R -- success --> C
    R -- error --> Re
    R -- too many errors --> D

    R -- cancel --> Ca
    R -- discard --> D
    R -- snooze --> S

    Re -- schedule --> A

    S -- schedule --> A

    P -- preconditions met, future schedule --> S
    P -- preconditions met --> A

    %% Rescuer
    R -- rescued --> Re
    R -- rescued --> D

    %% Retry Transitions
    C -- manual retry --> A
    D -- manual retry --> A
    Ca -- manual retry  --> A
    Re -- manual retry --> A
    S -- manual retry --> A
    P -- manual retry --> A

    %% Cancellation Transitions
    A -- manual cancel --> Ca
    R -- manual cancel --> Ca
    S -- manual cancel --> Ca
    P -- manual cancel --> Ca
    Re -- manual cancel  --> Ca

```

```

`driver_test.go`:

```go
package river_test

import (
	"context"
	"database/sql"
	"runtime"
	"strconv"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/stdlib"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/riverinternaltest/riverdrivertest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivertype"
)

func TestDriverDatabaseSQL(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	dbPool := riverinternaltest.TestDB(ctx, t)

	stdPool := stdlib.OpenDBFromPool(dbPool)
	t.Cleanup(func() { require.NoError(t, stdPool.Close()) })

	riverdrivertest.Exercise(ctx, t,
		func(ctx context.Context, t *testing.T) riverdriver.Driver[*sql.Tx] {
			t.Helper()

			return riverdatabasesql.New(stdPool)
		},
		func(ctx context.Context, t *testing.T) riverdriver.Executor {
			t.Helper()

			tx, err := stdPool.BeginTx(ctx, nil)
			require.NoError(t, err)
			t.Cleanup(func() { _ = tx.Rollback() })

			return riverdatabasesql.New(nil).UnwrapExecutor(tx)
		})
}

func TestDriverRiverPgxV5(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	riverdrivertest.Exercise(ctx, t,
		func(ctx context.Context, t *testing.T) riverdriver.Driver[pgx.Tx] {
			t.Helper()

			dbPool := riverinternaltest.TestDB(ctx, t)
			return riverpgxv5.New(dbPool)
		},
		func(ctx context.Context, t *testing.T) riverdriver.Executor {
			t.Helper()

			tx := riverinternaltest.TestTx(ctx, t)
			return riverpgxv5.New(nil).UnwrapExecutor(tx)
		})
}

func BenchmarkDriverRiverPgxV5_Executor(b *testing.B) {
	const (
		clientID = "test-client-id"
		timeout  = 5 * time.Minute
	)

	ctx := context.Background()

	type testBundle struct{}

	setupPool := func(b *testing.B) (riverdriver.Executor, *testBundle) {
		b.Helper()

		driver := riverpgxv5.New(riverinternaltest.TestDB(ctx, b))

		b.ResetTimer()

		return driver.GetExecutor(), &testBundle{}
	}

	setupTx := func(b *testing.B) (riverdriver.Executor, *testBundle) {
		b.Helper()

		driver := riverpgxv5.New(nil)
		tx := riverinternaltest.TestTx(ctx, b)

		b.ResetTimer()

		return driver.UnwrapExecutor(tx), &testBundle{}
	}

	makeInsertParams := func() []*riverdriver.JobInsertFastParams {
		return []*riverdriver.JobInsertFastParams{{
			EncodedArgs: []byte(`{}`),
			Kind:        "fake_job",
			MaxAttempts: rivercommon.MaxAttemptsDefault,
			Metadata:    []byte(`{}`),
			Priority:    rivercommon.PriorityDefault,
			Queue:       rivercommon.QueueDefault,
			ScheduledAt: nil,
			State:       rivertype.JobStateAvailable,
		}}
	}

	b.Run("JobInsert_Sequential", func(b *testing.B) {
		ctx, cancel := context.WithTimeout(ctx, timeout)
		defer cancel()

		exec, _ := setupTx(b)

		for range b.N {
			if _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {
				b.Fatal(err)
			}
		}
	})

	b.Run("JobInsert_Parallel", func(b *testing.B) {
		ctx, cancel := context.WithTimeout(ctx, timeout)
		defer cancel()

		exec, _ := setupPool(b)

		b.RunParallel(func(pb *testing.PB) {
			i := 0
			for pb.Next() {
				if _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {
					b.Fatal(err)
				}
				i++
			}
		})
	})

	b.Run("JobGetAvailable_100_Sequential", func(b *testing.B) {
		ctx, cancel := context.WithTimeout(ctx, timeout)
		defer cancel()

		exec, _ := setupTx(b)

		for range b.N * 100 {
			if _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {
				b.Fatal(err)
			}
		}

		b.ResetTimer()

		for range b.N {
			if _, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         100,
				Queue:       rivercommon.QueueDefault,
			}); err != nil {
				b.Fatal(err)
			}
		}
	})

	b.Run("JobGetAvailable_100_Parallel", func(b *testing.B) {
		ctx, cancel := context.WithTimeout(ctx, timeout)
		defer cancel()

		exec, _ := setupPool(b)

		for range b.N * 100 * runtime.NumCPU() {
			if _, err := exec.JobInsertFastMany(ctx, makeInsertParams()); err != nil {
				b.Fatal(err)
			}
		}

		b.ResetTimer()

		b.RunParallel(func(pb *testing.PB) {
			for pb.Next() {
				if _, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
					AttemptedBy: clientID,
					Max:         100,
					Queue:       rivercommon.QueueDefault,
				}); err != nil {
					b.Fatal(err)
				}
			}
		})
	})
}

func BenchmarkDriverRiverPgxV5Insert(b *testing.B) {
	ctx := context.Background()

	type testBundle struct {
		exec riverdriver.Executor
		tx   pgx.Tx
	}

	setup := func(b *testing.B) (*riverpgxv5.Driver, *testBundle) {
		b.Helper()

		var (
			driver = riverpgxv5.New(nil)
			tx     = riverinternaltest.TestTx(ctx, b)
		)

		bundle := &testBundle{
			exec: driver.UnwrapExecutor(tx),
			tx:   tx,
		}

		return driver, bundle
	}

	b.Run("InsertFastMany", func(b *testing.B) {
		_, bundle := setup(b)

		for range b.N {
			_, err := bundle.exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{{
				EncodedArgs: []byte(`{"encoded": "args"}`),
				Kind:        "test_kind",
				MaxAttempts: rivercommon.MaxAttemptsDefault,
				Priority:    rivercommon.PriorityDefault,
				Queue:       rivercommon.QueueDefault,
				State:       rivertype.JobStateAvailable,
			}})
			require.NoError(b, err)
		}
	})

	b.Run("InsertFastMany_WithUnique", func(b *testing.B) {
		_, bundle := setup(b)

		for i := range b.N {
			_, err := bundle.exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{{
				EncodedArgs:  []byte(`{"encoded": "args"}`),
				Kind:         "test_kind",
				MaxAttempts:  rivercommon.MaxAttemptsDefault,
				Priority:     rivercommon.PriorityDefault,
				Queue:        rivercommon.QueueDefault,
				State:        rivertype.JobStateAvailable,
				UniqueKey:    []byte("test_unique_key_" + strconv.Itoa(i)),
				UniqueStates: 0xFB,
			}})
			require.NoError(b, err)
		}
	})
}

```

`error.go`:

```go
package river

import (
	"time"

	"github.com/riverqueue/river/rivertype"
)

// ErrJobCancelledRemotely is a sentinel error indicating that the job was cancelled remotely.
var ErrJobCancelledRemotely = rivertype.ErrJobCancelledRemotely

// JobCancelError is the error type returned by JobCancel. It should not be
// initialized directly, but is returned from the [JobCancel] function and can
// be used for test assertions.
type JobCancelError = rivertype.JobCancelError

// JobCancel wraps err and can be returned from a Worker's Work method to cancel
// the job at the end of execution. Regardless of whether or not the job has any
// remaining attempts, this will ensure the job does not execute again.
func JobCancel(err error) error {
	return rivertype.JobCancel(err)
}

// JobSnoozeError is the error type returned by JobSnooze. It should not be
// initialized directly, but is returned from the [JobSnooze] function and can
// be used for test assertions.
type JobSnoozeError = rivertype.JobSnoozeError

// JobSnooze can be returned from a Worker's Work method to cause the job to be
// tried again after the specified duration. This also has the effect of
// incrementing the job's MaxAttempts by 1, meaning that jobs can be repeatedly
// snoozed without ever being discarded.
//
// Panics if duration is < 0.
func JobSnooze(duration time.Duration) error {
	return &rivertype.JobSnoozeError{Duration: duration}
}

// UnknownJobKindError is returned when a Client fetches and attempts to
// work a job that has not been registered on the Client's Workers bundle (using AddWorker).
type UnknownJobKindError = rivertype.UnknownJobKindError

```

`error_handler.go`:

```go
package river

import (
	"context"

	"github.com/riverqueue/river/rivertype"
)

// ErrorHandler provides an interface that will be invoked in case of an error
// or panic occurring in the job. This is often useful for logging and exception
// tracking, but can also be used to customize retry behavior.
type ErrorHandler interface {
	// HandleError is invoked in case of an error occurring in a job.
	//
	// Context is descended from the one used to start the River client that
	// worked the job.
	HandleError(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult

	// HandlePanic is invoked in case of a panic occurring in a job.
	//
	// Context is descended from the one used to start the River client that
	// worked the job.
	HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult
}

type ErrorHandlerResult struct {
	// SetCancelled can be set to true to fail the job immediately and
	// permanently. By default it'll continue to follow the configured retry
	// schedule.
	SetCancelled bool
}

```

`error_handler_test.go`:

```go
package river

import (
	"context"

	"github.com/riverqueue/river/rivertype"
)

type testErrorHandler struct {
	HandleErrorCalled bool
	HandleErrorFunc   func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult

	HandlePanicCalled bool
	HandlePanicFunc   func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult
}

// Test handler with no-ops for both error handling functions.
func newTestErrorHandler() *testErrorHandler {
	return &testErrorHandler{
		HandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult { return nil },
		HandlePanicFunc: func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
			return nil
		},
	}
}

func (h *testErrorHandler) HandleError(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {
	h.HandleErrorCalled = true
	return h.HandleErrorFunc(ctx, job, err)
}

func (h *testErrorHandler) HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
	h.HandlePanicCalled = true
	return h.HandlePanicFunc(ctx, job, panicVal, trace)
}

```

`error_test.go`:

```go
// Package river_test is an external test package for river. It is separated
// from the internal package to ensure that no internal packages are relied on.
package river_test

import (
	"errors"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river"
)

func TestUnknownJobKindError_As(t *testing.T) {
	// This test isn't really necessary because we didn't have to write any code
	// to make it pass, but it does demonstrate that we can successfully use
	// errors.As with this custom error type.
	t.Parallel()

	t.Run("ReturnsTrueForAnotherUnregisteredKindError", func(t *testing.T) {
		t.Parallel()

		err1 := &river.UnknownJobKindError{Kind: "MyJobArgs"}
		var err2 *river.UnknownJobKindError
		require.ErrorAs(t, err1, &err2)
		require.Equal(t, err1, err2)
		require.Equal(t, err1.Kind, err2.Kind)
	})

	t.Run("ReturnsFalseForADifferentError", func(t *testing.T) {
		t.Parallel()

		var err *river.UnknownJobKindError
		require.False(t, errors.As(errors.New("some other error"), &err))
	})
}

func TestUnknownJobKindError_Is(t *testing.T) {
	t.Parallel()

	t.Run("ReturnsTrueForAnotherUnregisteredKindError", func(t *testing.T) {
		t.Parallel()

		err1 := &river.UnknownJobKindError{Kind: "MyJobArgs"}
		require.ErrorIs(t, err1, &river.UnknownJobKindError{})
	})

	t.Run("ReturnsFalseForADifferentError", func(t *testing.T) {
		t.Parallel()

		err1 := &river.UnknownJobKindError{Kind: "MyJobArgs"}
		require.NotErrorIs(t, err1, errors.New("some other error"))
	})
}

func TestJobCancel(t *testing.T) {
	t.Parallel()

	t.Run("ErrorsIsReturnsTrueForAnotherErrorOfSameType", func(t *testing.T) {
		t.Parallel()
		err1 := river.JobCancel(errors.New("some message"))
		require.ErrorIs(t, err1, river.JobCancel(errors.New("another message")))
	})

	t.Run("ErrorsIsReturnsFalseForADifferentErrorType", func(t *testing.T) {
		t.Parallel()
		err1 := river.JobCancel(errors.New("some message"))
		require.NotErrorIs(t, err1, &river.UnknownJobKindError{Kind: "MyJobArgs"})
	})
}

```

`event.go`:

```go
package river

import (
	"time"

	"github.com/riverqueue/river/internal/jobstats"
	"github.com/riverqueue/river/rivertype"
)

// EventKind is a kind of event to subscribe to from a client.
type EventKind string

const (
	// EventKindJobCancelled occurs when a job is cancelled.
	EventKindJobCancelled EventKind = "job_cancelled"

	// EventKindJobCompleted occurs when a job is completed.
	EventKindJobCompleted EventKind = "job_completed"

	// EventKindJobFailed occurs when a job fails. Occurs both when a job fails
	// and will be retried and when a job fails for the last time and will be
	// discarded. Callers can use job fields like `Attempt` and `State` to
	// differentiate each type of occurrence.
	EventKindJobFailed EventKind = "job_failed"

	// EventKindJobSnoozed occurs when a job is snoozed.
	EventKindJobSnoozed EventKind = "job_snoozed"

	// EventKindQueuePaused occurs when a queue is paused.
	EventKindQueuePaused EventKind = "queue_paused"

	// EventKindQueueResumed occurs when a queue is resumed.
	EventKindQueueResumed EventKind = "queue_resumed"
)

// All known event kinds, used to validate incoming kinds. This is purposely not
// exported because end users should have no way of subscribing to all known
// kinds for forward compatibility reasons.
var allKinds = map[EventKind]struct{}{ //nolint:gochecknoglobals
	EventKindJobCancelled: {},
	EventKindJobCompleted: {},
	EventKindJobFailed:    {},
	EventKindJobSnoozed:   {},
	EventKindQueuePaused:  {},
	EventKindQueueResumed: {},
}

// Event wraps an event that occurred within a River client, like a job being
// completed.
type Event struct {
	// Kind is the kind of event. Receivers should read this field and respond
	// accordingly. Subscriptions will only receive event kinds that they
	// requested when creating a subscription with Subscribe.
	Kind EventKind

	// Job contains job-related information.
	Job *rivertype.JobRow

	// JobStats are statistics about the run of a job.
	JobStats *JobStatistics

	// Queue contains queue-related information.
	Queue *rivertype.Queue
}

// JobStatistics contains information about a single execution of a job.
type JobStatistics struct {
	CompleteDuration  time.Duration // Time it took to set the job completed, discarded, or errored.
	QueueWaitDuration time.Duration // Time the job spent waiting in available state before starting execution.
	RunDuration       time.Duration // Time job spent running (measured around job worker.)
}

func jobStatisticsFromInternal(stats *jobstats.JobStatistics) *JobStatistics {
	return &JobStatistics{
		CompleteDuration:  stats.CompleteDuration,
		QueueWaitDuration: stats.QueueWaitDuration,
		RunDuration:       stats.RunDuration,
	}
}

// eventSubscription is an active subscription for events being produced by a
// client, created with Client.Subscribe.
type eventSubscription struct {
	Chan  chan *Event
	Kinds map[EventKind]struct{}
}

func (s *eventSubscription) ListensFor(kind EventKind) bool {
	_, ok := s.Kinds[kind]
	return ok
}

```

`event_test.go`:

```go
package river

import (
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/jobstats"
)

func TestJobStatisticsFromInternal(t *testing.T) {
	t.Parallel()

	require.Equal(t, &JobStatistics{
		CompleteDuration:  1 * time.Second,
		QueueWaitDuration: 2 * time.Second,
		RunDuration:       3 * time.Second,
	}, jobStatisticsFromInternal(&jobstats.JobStatistics{
		CompleteDuration:  1 * time.Second,
		QueueWaitDuration: 2 * time.Second,
		RunDuration:       3 * time.Second,
	}))
}

```

`example_batch_insert_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type BatchInsertArgs struct{}

func (BatchInsertArgs) Kind() string { return "batch_insert" }

// BatchInsertWorker is a job worker demonstrating use of custom
// job-specific insertion options.
type BatchInsertWorker struct {
	river.WorkerDefaults[BatchInsertArgs]
}

func (w *BatchInsertWorker) Work(ctx context.Context, job *river.Job[BatchInsertArgs]) error {
	fmt.Printf("Worked a job\n")
	return nil
}

// Example_batchInsert demonstrates how many jobs can be inserted for work as
// part of a single operation.
func Example_batchInsert() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &BatchInsertWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	results, err := riverClient.InsertMany(ctx, []river.InsertManyParams{
		{Args: BatchInsertArgs{}},
		{Args: BatchInsertArgs{}},
		{Args: BatchInsertArgs{}},
		{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 3}},
		{Args: BatchInsertArgs{}, InsertOpts: &river.InsertOpts{Priority: 4}},
	})
	if err != nil {
		panic(err)
	}
	fmt.Printf("Inserted %d jobs\n", len(results))

	waitForNJobs(subscribeChan, 5)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Inserted 5 jobs
	// Worked a job
	// Worked a job
	// Worked a job
	// Worked a job
	// Worked a job
}

```

`example_client_from_context_dbsql_test.go`:

```go
package river_test

import (
	"context"
	"database/sql"
	"errors"
	"fmt"
	"log/slog"
	"time"

	_ "github.com/jackc/pgx/v5/stdlib"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type ContextClientSQLArgs struct{}

func (args ContextClientSQLArgs) Kind() string { return "ContextClientSQLWorker" }

type ContextClientSQLWorker struct {
	river.WorkerDefaults[ContextClientSQLArgs]
}

func (w *ContextClientSQLWorker) Work(ctx context.Context, job *river.Job[ContextClientSQLArgs]) error {
	client := river.ClientFromContext[*sql.Tx](ctx)
	if client == nil {
		fmt.Println("client not found in context")
		return errors.New("client not found in context")
	}

	fmt.Printf("client found in context, id=%s\n", client.ID())
	return nil
}

// ExampleClientFromContext_databaseSQL demonstrates how to extract the River
// client from the worker context when using the [database/sql] driver.
// ([github.com/riverqueue/river/riverdriver/riverdatabasesql]).
func ExampleClientFromContext_databaseSQL() {
	ctx := context.Background()

	config := riverinternaltest.DatabaseConfig("river_test_example")
	db, err := sql.Open("pgx", config.ConnString())
	if err != nil {
		panic(err)
	}
	defer db.Close()

	workers := river.NewWorkers()
	river.AddWorker(workers, &ContextClientSQLWorker{})

	riverClient, err := river.NewClient(riverdatabasesql.New(db), &river.Config{
		ID:     "ClientFromContextClientSQL",
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 10},
		},
		FetchCooldown:     10 * time.Millisecond,
		FetchPollInterval: 10 * time.Millisecond,
		TestOnly:          true, // suitable only for use in tests; remove for live environments
		Workers:           workers,
	})
	if err != nil {
		panic(err)
	}

	// Not strictly needed, but used to help this test wait until job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}
	if _, err := riverClient.Insert(ctx, ContextClientSQLArgs{}, nil); err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// client found in context, id=ClientFromContextClientSQL
}

```

`example_client_from_context_test.go`:

```go
package river_test

import (
	"context"
	"errors"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type ContextClientArgs struct{}

func (args ContextClientArgs) Kind() string { return "ContextClientWorker" }

type ContextClientWorker struct {
	river.WorkerDefaults[ContextClientArgs]
}

func (w *ContextClientWorker) Work(ctx context.Context, job *river.Job[ContextClientArgs]) error {
	client := river.ClientFromContext[pgx.Tx](ctx)
	if client == nil {
		fmt.Println("client not found in context")
		return errors.New("client not found in context")
	}

	fmt.Printf("client found in context, id=%s\n", client.ID())
	return nil
}

// ExampleClientFromContext_pgx demonstrates how to extract the River client
// from the worker context when using the pgx/v5 driver.
// ([github.com/riverqueue/river/riverdriver/riverpgxv5]).
func ExampleClientFromContext_pgx() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &ContextClientWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		ID:     "ClientFromContextClient",
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 10},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Not strictly needed, but used to help this test wait until job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}
	if _, err = riverClient.Insert(ctx, ContextClientArgs{}, nil); err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// client found in context, id=ClientFromContextClient
}

```

`example_complete_job_within_tx_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type TransactionalArgs struct{}

func (TransactionalArgs) Kind() string { return "transactional_worker" }

// TransactionalWorker is a job worker which runs an operation on the database
// and transactionally completes the current job.
//
// While this example is simplified, any operations could be performed within
// the transaction such as inserting additional jobs or manipulating other data.
type TransactionalWorker struct {
	river.WorkerDefaults[TransactionalArgs]
	dbPool *pgxpool.Pool
}

func (w *TransactionalWorker) Work(ctx context.Context, job *river.Job[TransactionalArgs]) error {
	tx, err := w.dbPool.Begin(ctx)
	if err != nil {
		return err
	}
	defer tx.Rollback(ctx)

	var result int
	if err := tx.QueryRow(ctx, "SELECT 1").Scan(&result); err != nil {
		return err
	}

	// The function needs to know the type of the database driver in use by the
	// Client, but the other generic parameters can be inferred.
	jobAfter, err := river.JobCompleteTx[*riverpgxv5.Driver](ctx, tx, job)
	if err != nil {
		return err
	}
	fmt.Printf("Transitioned TransactionalWorker job from %q to %q\n", job.State, jobAfter.State)

	if err = tx.Commit(ctx); err != nil {
		return err
	}
	return nil
}

// Example_completeJobWithinTx demonstrates how to transactionally complete
// a job alongside other database changes being made.
func Example_completeJobWithinTx() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &TransactionalWorker{dbPool: dbPool})
	river.AddWorker(workers, &SortWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Not strictly needed, but used to help this test wait until job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	if _, err = riverClient.Insert(ctx, TransactionalArgs{}, nil); err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Transitioned TransactionalWorker job from "running" to "completed"
}

```

`example_cron_job_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/robfig/cron/v3"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type CronJobArgs struct{}

// Kind is the unique string name for this job.
func (CronJobArgs) Kind() string { return "cron" }

// CronJobWorker is a job worker for sorting strings.
type CronJobWorker struct {
	river.WorkerDefaults[CronJobArgs]
}

func (w *CronJobWorker) Work(ctx context.Context, job *river.Job[CronJobArgs]) error {
	fmt.Printf("This job will run once immediately then every hour on the half hour\n")
	return nil
}

// Example_cronJob demonstrates how to create a cron job with a more complex
// schedule using a third party cron package to parse more elaborate crontab
// syntax.
func Example_cronJob() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &CronJobWorker{})

	schedule, err := cron.ParseStandard("30 * * * *") // every hour on the half hour
	if err != nil {
		panic(err)
	}

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		PeriodicJobs: []*river.PeriodicJob{
			river.NewPeriodicJob(
				schedule,
				func() (river.JobArgs, *river.InsertOpts) {
					return CronJobArgs{}, nil
				},
				&river.PeriodicJobOpts{RunOnStart: true},
			),
		},
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	// There's no need to explicitly insert a periodic job. One will be inserted
	// (and worked soon after) as the client starts up.
	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// This job will run once immediately then every hour on the half hour
}

```

`example_custom_insert_opts_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type AlwaysHighPriorityArgs struct{}

func (AlwaysHighPriorityArgs) Kind() string { return "always_high_priority" }

// InsertOpts returns custom insert options that every job of this type will
// inherit by default.
func (AlwaysHighPriorityArgs) InsertOpts() river.InsertOpts {
	return river.InsertOpts{
		Queue: "high_priority",
	}
}

// AlwaysHighPriorityWorker is a job worker demonstrating use of custom
// job-specific insertion options.
type AlwaysHighPriorityWorker struct {
	river.WorkerDefaults[AlwaysHighPriorityArgs]
}

func (w *AlwaysHighPriorityWorker) Work(ctx context.Context, job *river.Job[AlwaysHighPriorityArgs]) error {
	fmt.Printf("Ran in queue: %s\n", job.Queue)
	return nil
}

type SometimesHighPriorityArgs struct{}

func (SometimesHighPriorityArgs) Kind() string { return "sometimes_high_priority" }

// SometimesHighPriorityWorker is a job worker that's made high-priority
// sometimes through the use of options at insertion time.
type SometimesHighPriorityWorker struct {
	river.WorkerDefaults[SometimesHighPriorityArgs]
}

func (w *SometimesHighPriorityWorker) Work(ctx context.Context, job *river.Job[SometimesHighPriorityArgs]) error {
	fmt.Printf("Ran in queue: %s\n", job.Queue)
	return nil
}

// Example_customInsertOpts demonstrates the use of a job with custom
// job-specific insertion options.
func Example_customInsertOpts() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &AlwaysHighPriorityWorker{})
	river.AddWorker(workers, &SometimesHighPriorityWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
			"high_priority":    {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	// This job always runs in the high-priority queue because its job-specific
	// options on the struct above dictate that it will.
	_, err = riverClient.Insert(ctx, AlwaysHighPriorityArgs{}, nil)
	if err != nil {
		panic(err)
	}

	// This job will run in the high-priority queue because of the options given
	// at insertion time.
	_, err = riverClient.Insert(ctx, SometimesHighPriorityArgs{}, &river.InsertOpts{
		Queue: "high_priority",
	})
	if err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 2)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Ran in queue: high_priority
	// Ran in queue: high_priority
}

```

`example_error_handler_test.go`:

```go
package river_test

import (
	"context"
	"errors"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
	"github.com/riverqueue/river/rivertype"
)

type CustomErrorHandler struct{}

func (*CustomErrorHandler) HandleError(ctx context.Context, job *rivertype.JobRow, err error) *river.ErrorHandlerResult {
	fmt.Printf("Job errored with: %s\n", err)
	return nil
}

func (*CustomErrorHandler) HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *river.ErrorHandlerResult {
	fmt.Printf("Job panicked with: %v\n", panicVal)

	// Either function can also set the job to be immediately cancelled.
	return &river.ErrorHandlerResult{SetCancelled: true}
}

type ErroringArgs struct {
	ShouldError bool
	ShouldPanic bool
}

func (ErroringArgs) Kind() string { return "erroring" }

// Here to make sure our jobs are never accidentally retried which would add
// additional output and fail the example.
func (ErroringArgs) InsertOpts() river.InsertOpts {
	return river.InsertOpts{MaxAttempts: 1}
}

type ErroringWorker struct {
	river.WorkerDefaults[ErroringArgs]
}

func (w *ErroringWorker) Work(ctx context.Context, job *river.Job[ErroringArgs]) error {
	switch {
	case job.Args.ShouldError:
		return errors.New("this job errored")
	case job.Args.ShouldPanic:
		panic("this job panicked")
	}
	return nil
}

// Example_errorHandler demonstrates how to use the ErrorHandler interface for
// custom application telemetry.
func Example_errorHandler() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &ErroringWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		ErrorHandler: &CustomErrorHandler{},
		Logger:       slog.New(&slogutil.SlogMessageOnlyHandler{Level: 9}), // Suppress logging so example output is cleaner (9 > slog.LevelError).
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 10},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Not strictly needed, but used to help this test wait until job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled, river.EventKindJobFailed)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	if _, err = riverClient.Insert(ctx, ErroringArgs{ShouldError: true}, nil); err != nil {
		panic(err)
	}

	// Wait for the first job before inserting another to guarantee test output
	// is ordered correctly.
	waitForNJobs(subscribeChan, 1)

	if _, err = riverClient.Insert(ctx, ErroringArgs{ShouldPanic: true}, nil); err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Job errored with: this job errored
	// Job panicked with: this job panicked
}

```

`example_global_hooks_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
	"github.com/riverqueue/river/rivertype"
)

type BothInsertAndWorkBeginHook struct{ river.HookDefaults }

func (BothInsertAndWorkBeginHook) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	fmt.Printf("BothInsertAndWorkBeginHook.InsertBegin ran\n")
	return nil
}

func (BothInsertAndWorkBeginHook) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	fmt.Printf("BothInsertAndWorkBeginHook.WorkBegin ran\n")
	return nil
}

type InsertBeginHook struct{ river.HookDefaults }

func (InsertBeginHook) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	fmt.Printf("InsertBeginHook.InsertBegin ran\n")
	return nil
}

type WorkBeginHook struct{ river.HookDefaults }

func (WorkBeginHook) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	fmt.Printf("WorkBeginHook.WorkBegin ran\n")
	return nil
}

// Verify interface compliance. It's recommended that these are included in your
// test suite to make sure that your hooks are complying to the specific
// interface hooks that you expected them to be.
var (
	_ rivertype.HookInsertBegin = &BothInsertAndWorkBeginHook{}
	_ rivertype.HookWorkBegin   = &BothInsertAndWorkBeginHook{}
	_ rivertype.HookInsertBegin = &InsertBeginHook{}
	_ rivertype.HookWorkBegin   = &WorkBeginHook{}
)

// Example_globalHooks demonstrates the use of hooks to modify River behavior
// which are global to a River client.
func Example_globalHooks() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &NoOpWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		// Order is significant. See output below.
		Hooks: []rivertype.Hook{
			&BothInsertAndWorkBeginHook{},
			&InsertBeginHook{},
			&WorkBeginHook{},
		},
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	_, err = riverClient.Insert(ctx, NoOpArgs{}, nil)
	if err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// BothInsertAndWorkBeginHook.InsertBegin ran
	// InsertBeginHook.InsertBegin ran
	// BothInsertAndWorkBeginHook.WorkBegin ran
	// WorkBeginHook.WorkBegin ran
	// NoOpWorker.Work ran
}

```

`example_global_middleware_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
	"github.com/riverqueue/river/rivertype"
)

type JobBothInsertAndWorkMiddleware struct{ river.MiddlewareDefaults }

func (JobBothInsertAndWorkMiddleware) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
	fmt.Printf("JobBothInsertAndWorkMiddleware.InsertMany ran\n")
	return doInner(ctx)
}

func (JobBothInsertAndWorkMiddleware) Work(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {
	fmt.Printf("JobBothInsertAndWorkMiddleware.Work ran\n")
	return doInner(ctx)
}

type JobInsertMiddleware struct{ river.MiddlewareDefaults }

func (JobInsertMiddleware) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
	fmt.Printf("JobInsertMiddleware.InsertMany ran\n")
	return doInner(ctx)
}

type WorkerMiddleware struct{ river.MiddlewareDefaults }

func (WorkerMiddleware) Work(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {
	fmt.Printf("WorkerMiddleware.Work ran\n")
	return doInner(ctx)
}

// Verify interface compliance. It's recommended that these are included in your
// test suite to make sure that your middlewares are complying to the specific
// interface middlewares that you expected them to be.
var (
	_ rivertype.JobInsertMiddleware = &JobBothInsertAndWorkMiddleware{}
	_ rivertype.WorkerMiddleware    = &JobBothInsertAndWorkMiddleware{}
	_ rivertype.JobInsertMiddleware = &JobInsertMiddleware{}
	_ rivertype.WorkerMiddleware    = &WorkerMiddleware{}
)

// Example_globalMiddleware demonstrates the use of middleware to modify River
// behavior which are global to a River client.
func Example_globalMiddleware() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &NoOpWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		// Order is significant. See output below.
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Middleware: []rivertype.Middleware{
			&JobBothInsertAndWorkMiddleware{},
			&JobInsertMiddleware{},
			&WorkerMiddleware{},
		},
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	_, err = riverClient.Insert(ctx, NoOpArgs{}, nil)
	if err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// JobBothInsertAndWorkMiddleware.InsertMany ran
	// JobInsertMiddleware.InsertMany ran
	// JobBothInsertAndWorkMiddleware.Work ran
	// WorkerMiddleware.Work ran
	// NoOpWorker.Work ran
}

```

`example_graceful_shutdown_test.go`:

```go
package river_test

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type WaitsForCancelOnlyArgs struct{}

func (WaitsForCancelOnlyArgs) Kind() string { return "waits_for_cancel_only" }

// WaitsForCancelOnlyWorker is a worker that will never finish jobs until its
// context is cancelled.
type WaitsForCancelOnlyWorker struct {
	river.WorkerDefaults[WaitsForCancelOnlyArgs]
	jobStarted chan struct{}
}

func (w *WaitsForCancelOnlyWorker) Work(ctx context.Context, job *river.Job[WaitsForCancelOnlyArgs]) error {
	fmt.Printf("Working job that doesn't finish until cancelled\n")
	close(w.jobStarted)

	<-ctx.Done()
	fmt.Printf("Job cancelled\n")

	// In the event of cancellation, an error should be returned so that the job
	// goes back in the retry queue.
	return ctx.Err()
}

// Example_gracefulShutdown demonstrates a realistic-looking stop loop for
// River. It listens for SIGINT/SIGTERM (like might be received by a Ctrl+C
// locally or on a platform like Heroku to stop a process) and when received,
// tries a soft stop that waits for work to finish. If it doesn't finish in
// time, a second SIGINT/SIGTERM will initiate a hard stop that cancels all jobs
// using context cancellation. A third will give up on the stop procedure and
// exit uncleanly.
func Example_gracefulShutdown() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	jobStarted := make(chan struct{})

	workers := river.NewWorkers()
	river.AddWorker(workers, &WaitsForCancelOnlyWorker{jobStarted: jobStarted})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	_, err = riverClient.Insert(ctx, WaitsForCancelOnlyArgs{}, nil)
	if err != nil {
		panic(err)
	}

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	sigintOrTerm := make(chan os.Signal, 1)
	signal.Notify(sigintOrTerm, syscall.SIGINT, syscall.SIGTERM)

	// This is meant to be a realistic-looking stop goroutine that might go in a
	// real program. It waits for SIGINT/SIGTERM and when received, tries to stop
	// gracefully by allowing a chance for jobs to finish. But if that isn't
	// working, a second SIGINT/SIGTERM will tell it to terminate with prejudice and
	// it'll issue a hard stop that cancels the context of all active jobs. In
	// case that doesn't work, a third SIGINT/SIGTERM ignores River's stop procedure
	// completely and exits uncleanly.
	go func() {
		<-sigintOrTerm
		fmt.Printf("Received SIGINT/SIGTERM; initiating soft stop (try to wait for jobs to finish)\n")

		softStopCtx, softStopCtxCancel := context.WithTimeout(ctx, 10*time.Second)
		defer softStopCtxCancel()

		go func() {
			select {
			case <-sigintOrTerm:
				fmt.Printf("Received SIGINT/SIGTERM again; initiating hard stop (cancel everything)\n")
				softStopCtxCancel()
			case <-softStopCtx.Done():
				fmt.Printf("Soft stop timeout; initiating hard stop (cancel everything)\n")
			}
		}()

		err := riverClient.Stop(softStopCtx)
		if err != nil && !errors.Is(err, context.DeadlineExceeded) && !errors.Is(err, context.Canceled) {
			panic(err)
		}
		if err == nil {
			fmt.Printf("Soft stop succeeded\n")
			return
		}

		hardStopCtx, hardStopCtxCancel := context.WithTimeout(ctx, 10*time.Second)
		defer hardStopCtxCancel()

		// As long as all jobs respect context cancellation, StopAndCancel will
		// always work. However, in the case of a bug where a job blocks despite
		// being cancelled, it may be necessary to either ignore River's stop
		// result (what's shown here) or have a supervisor kill the process.
		err = riverClient.StopAndCancel(hardStopCtx)
		if err != nil && errors.Is(err, context.DeadlineExceeded) {
			fmt.Printf("Hard stop timeout; ignoring stop procedure and exiting unsafely\n")
		} else if err != nil {
			panic(err)
		}

		// hard stop succeeded
	}()

	// Make sure our job starts being worked before doing anything else.
	<-jobStarted

	// Cheat a little by sending a SIGTERM manually for the purpose of this
	// example (normally this will be sent by user or supervisory process). The
	// first SIGTERM tries a soft stop in which jobs are given a chance to
	// finish up.
	sigintOrTerm <- syscall.SIGTERM

	// The soft stop will never work in this example because our job only
	// respects context cancellation, but wait a short amount of time to give it
	// a chance. After it elapses, send another SIGTERM to initiate a hard stop.
	select {
	case <-riverClient.Stopped():
		// Will never be reached in this example because our job will only ever
		// finish on context cancellation.
		fmt.Printf("Soft stop succeeded\n")

	case <-time.After(100 * time.Millisecond):
		sigintOrTerm <- syscall.SIGTERM
		<-riverClient.Stopped()
	}

	// Output:
	// Working job that doesn't finish until cancelled
	// Received SIGINT/SIGTERM; initiating soft stop (try to wait for jobs to finish)
	// Received SIGINT/SIGTERM again; initiating hard stop (cancel everything)
	// Job cancelled
	// JobExecutor: Job errored; retrying
}

```

`example_insert_and_work_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"
	"sort"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type SortArgs struct {
	// Strings is a slice of strings to sort.
	Strings []string `json:"strings"`
}

func (SortArgs) Kind() string { return "sort" }

type SortWorker struct {
	river.WorkerDefaults[SortArgs]
}

func (w *SortWorker) Work(ctx context.Context, job *river.Job[SortArgs]) error {
	sort.Strings(job.Args.Strings)
	fmt.Printf("Sorted strings: %+v\n", job.Args.Strings)
	return nil
}

// Example_insertAndWork demonstrates how to register job workers, start a
// client, and insert a job on it to be worked.
func Example_insertAndWork() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &SortWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	// Start a transaction to insert a job. It's also possible to insert a job
	// outside a transaction, but this usage is recommended to ensure that all
	// data a job needs to run is available by the time it starts. Because of
	// snapshot visibility guarantees across transactions, the job will not be
	// worked until the transaction has committed.
	tx, err := dbPool.Begin(ctx)
	if err != nil {
		panic(err)
	}
	defer tx.Rollback(ctx)

	_, err = riverClient.InsertTx(ctx, tx, SortArgs{
		Strings: []string{
			"whale", "tiger", "bear",
		},
	}, nil)
	if err != nil {
		panic(err)
	}

	if err := tx.Commit(ctx); err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Sorted strings: [bear tiger whale]
}

```

`example_job_args_hooks_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
	"github.com/riverqueue/river/rivertype"
)

type JobWithHooksArgs struct{}

func (JobWithHooksArgs) Kind() string { return "job_with_hooks" }

// Warning: Hooks is only called once per job insert or work and its return
// value is memoized. It should not vary based on the contents of any particular
// args because changes will be ignored.
func (JobWithHooksArgs) Hooks() []rivertype.Hook {
	// Order is significant. See output below.
	return []rivertype.Hook{
		&JobWithHooksBothInsertAndWorkBeginHook{},
		&JobWithHooksInsertBeginHook{},
		&JobWithHooksWorkBeginHook{},
	}
}

type JobWithHooksWorker struct {
	river.WorkerDefaults[JobWithHooksArgs]
}

func (w *JobWithHooksWorker) Work(ctx context.Context, job *river.Job[JobWithHooksArgs]) error {
	fmt.Printf("JobWithHooksWorker.Work ran\n")
	return nil
}

type JobWithHooksBothInsertAndWorkBeginHook struct{ river.HookDefaults }

func (JobWithHooksBothInsertAndWorkBeginHook) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	fmt.Printf("JobWithHooksInsertAndWorkBeginHook.InsertBegin ran\n")
	return nil
}

func (JobWithHooksBothInsertAndWorkBeginHook) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	fmt.Printf("JobWithHooksInsertAndWorkBeginHook.WorkBegin ran\n")
	return nil
}

type JobWithHooksInsertBeginHook struct{ river.HookDefaults }

func (JobWithHooksInsertBeginHook) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	fmt.Printf("JobWithHooksInsertBeginHook.InsertBegin ran\n")
	return nil
}

type JobWithHooksWorkBeginHook struct{ river.HookDefaults }

func (JobWithHooksWorkBeginHook) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	fmt.Printf("JobWithHooksWorkBeginHook.WorkBegin ran\n")
	return nil
}

// Verify interface compliance. It's recommended that these are included in your
// test suite to make sure that your hooks are complying to the specific
// interface hooks that you expected them to be.
var (
	_ rivertype.HookInsertBegin = &BothInsertAndWorkBeginHook{}
	_ rivertype.HookWorkBegin   = &BothInsertAndWorkBeginHook{}
	_ rivertype.HookInsertBegin = &InsertBeginHook{}
	_ rivertype.HookWorkBegin   = &WorkBeginHook{}
)

// Example_jobArgsHooks demonstrates the use of hooks to modify River behavior.
func Example_jobArgsHooks() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &JobWithHooksWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	_, err = riverClient.Insert(ctx, JobWithHooksArgs{}, nil)
	if err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// JobWithHooksInsertAndWorkBeginHook.InsertBegin ran
	// JobWithHooksInsertBeginHook.InsertBegin ran
	// JobWithHooksInsertAndWorkBeginHook.WorkBegin ran
	// JobWithHooksWorkBeginHook.WorkBegin ran
	// JobWithHooksWorker.Work ran
}

```

`example_job_cancel_from_client_test.go`:

```go
package river_test

import (
	"context"
	"errors"
	"log/slog"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type SleepingArgs struct{}

func (args SleepingArgs) Kind() string { return "SleepingWorker" }

type SleepingWorker struct {
	river.WorkerDefaults[CancellingArgs]
	jobChan chan int64
}

func (w *SleepingWorker) Work(ctx context.Context, job *river.Job[CancellingArgs]) error {
	w.jobChan <- job.ID
	select {
	case <-ctx.Done():
	case <-time.After(5 * time.Second):
		return errors.New("sleeping worker timed out")
	}
	return ctx.Err()
}

// Example_jobCancelFromClient demonstrates how to permanently cancel a job from
// any Client using JobCancel.
func Example_jobCancelFromClient() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	jobChan := make(chan int64)

	workers := river.NewWorkers()
	river.AddWorker(workers, &SleepingWorker{jobChan: jobChan})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 10},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Not strictly needed, but used to help this test wait until job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}
	insertRes, err := riverClient.Insert(ctx, CancellingArgs{ShouldCancel: true}, nil)
	if err != nil {
		panic(err)
	}
	select {
	case <-jobChan:
	case <-time.After(2 * time.Second):
		panic("no jobChan signal received")
	}

	// There is presently no way to wait for the client to be 100% ready, so we
	// sleep for a bit to give it time to start up. This is only needed in this
	// example because we need the notifier to be ready for it to receive the
	// cancellation signal.
	time.Sleep(500 * time.Millisecond)

	if _, err = riverClient.JobCancel(ctx, insertRes.Job.ID); err != nil {
		panic(err)
	}
	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// JobExecutor: job cancelled remotely
}

```

`example_job_cancel_test.go`:

```go
package river_test

import (
	"context"
	"errors"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type CancellingArgs struct {
	ShouldCancel bool
}

func (args CancellingArgs) Kind() string { return "Cancelling" }

type CancellingWorker struct {
	river.WorkerDefaults[CancellingArgs]
}

func (w *CancellingWorker) Work(ctx context.Context, job *river.Job[CancellingArgs]) error {
	if job.Args.ShouldCancel {
		fmt.Println("cancelling job")
		return river.JobCancel(errors.New("this wrapped error message will be persisted to DB"))
	}
	return nil
}

// Example_jobCancel demonstrates how to permanently cancel a job from within
// Work using JobCancel.
func Example_jobCancel() { //nolint:dupl
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &CancellingWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 10},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Not strictly needed, but used to help this test wait until job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}
	if _, err = riverClient.Insert(ctx, CancellingArgs{ShouldCancel: true}, nil); err != nil {
		panic(err)
	}
	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// cancelling job
}

```

`example_job_snooze_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type SnoozingArgs struct {
	ShouldSnooze bool
}

func (args SnoozingArgs) Kind() string { return "Snoozing" }

type SnoozingWorker struct {
	river.WorkerDefaults[SnoozingArgs]
}

func (w *SnoozingWorker) Work(ctx context.Context, job *river.Job[SnoozingArgs]) error {
	if job.Args.ShouldSnooze {
		fmt.Println("snoozing job for 5 minutes")
		return river.JobSnooze(5 * time.Minute)
	}
	return nil
}

// Example_jobSnooze demonstrates how to snooze a job from within Work using
// JobSnooze. The job will be run again after 5 minutes and the snooze attempt
// will decrement the job's attempt count, ensuring that one can snooze as many
// times as desired without being impacted by the max attempts.
func Example_jobSnooze() { //nolint:dupl
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &SnoozingWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 10},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// The subscription bits are not needed in real usage, but are used to make
	// sure the test waits until the job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobSnoozed)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}
	if _, err = riverClient.Insert(ctx, SnoozingArgs{ShouldSnooze: true}, nil); err != nil {
		panic(err)
	}
	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// snoozing job for 5 minutes
}

```

`example_periodic_job_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type PeriodicJobArgs struct{}

// Kind is the unique string name for this job.
func (PeriodicJobArgs) Kind() string { return "periodic" }

// PeriodicJobWorker is a job worker for sorting strings.
type PeriodicJobWorker struct {
	river.WorkerDefaults[PeriodicJobArgs]
}

func (w *PeriodicJobWorker) Work(ctx context.Context, job *river.Job[PeriodicJobArgs]) error {
	fmt.Printf("This job will run once immediately then approximately once every 15 minutes\n")
	return nil
}

// Example_periodicJob demonstrates the use of a periodic job.
func Example_periodicJob() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &PeriodicJobWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		PeriodicJobs: []*river.PeriodicJob{
			river.NewPeriodicJob(
				river.PeriodicInterval(15*time.Minute),
				func() (river.JobArgs, *river.InsertOpts) {
					return PeriodicJobArgs{}, nil
				},
				&river.PeriodicJobOpts{RunOnStart: true},
			),
		},
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	// There's no need to explicitly insert a periodic job. One will be inserted
	// (and worked soon after) as the client starts up.
	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	// Periodic jobs can also be configured dynamically after a client has
	// already started. Added jobs are scheduled for run immediately.
	riverClient.PeriodicJobs().Clear()
	riverClient.PeriodicJobs().Add(
		river.NewPeriodicJob(
			river.PeriodicInterval(15*time.Minute),
			func() (river.JobArgs, *river.InsertOpts) {
				return PeriodicJobArgs{}, nil
			},
			nil,
		),
	)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// This job will run once immediately then approximately once every 15 minutes
}

```

`example_queue_pause_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type ReportingArgs struct{}

func (args ReportingArgs) Kind() string { return "Reporting" }

type ReportingWorker struct {
	river.WorkerDefaults[ReportingArgs]
	jobWorkedCh chan<- string
}

func (w *ReportingWorker) Work(ctx context.Context, job *river.Job[ReportingArgs]) error {
	select {
	case <-ctx.Done():
		return ctx.Err()
	case w.jobWorkedCh <- job.Queue:
		return nil
	}
}

// Example_queuePause demonstrates how to pause queues to prevent them from
// working new jobs, and later resume them.
func Example_queuePause() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	const (
		unreliableQueue = "unreliable_external_service"
		reliableQueue   = "reliable_jobs"
	)

	workers := river.NewWorkers()
	jobWorkedCh := make(chan string)
	river.AddWorker(workers, &ReportingWorker{jobWorkedCh: jobWorkedCh})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			unreliableQueue: {MaxWorkers: 10},
			reliableQueue:   {MaxWorkers: 10},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a queue is paused or unpaused.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindQueuePaused, river.EventKindQueueResumed)
	defer subscribeCancel()

	fmt.Printf("Pausing %s queue\n", unreliableQueue)
	if err := riverClient.QueuePause(ctx, unreliableQueue, nil); err != nil {
		panic(err)
	}

	// Wait for queue to be paused:
	waitOrTimeout(subscribeChan)

	fmt.Println("Inserting one job each into unreliable and reliable queues")
	if _, err = riverClient.Insert(ctx, ReportingArgs{}, &river.InsertOpts{Queue: unreliableQueue}); err != nil {
		panic(err)
	}
	if _, err = riverClient.Insert(ctx, ReportingArgs{}, &river.InsertOpts{Queue: reliableQueue}); err != nil {
		panic(err)
	}
	// The unreliable queue is paused so its job should get worked yet, while
	// reliable queue is not paused so its job should get worked immediately:
	receivedQueue := waitOrTimeout(jobWorkedCh)
	fmt.Printf("Job worked on %s queue\n", receivedQueue)

	// Resume the unreliable queue so it can work the job:
	fmt.Printf("Resuming %s queue\n", unreliableQueue)
	if err := riverClient.QueueResume(ctx, unreliableQueue, nil); err != nil {
		panic(err)
	}
	receivedQueue = waitOrTimeout(jobWorkedCh)
	fmt.Printf("Job worked on %s queue\n", receivedQueue)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Pausing unreliable_external_service queue
	// Inserting one job each into unreliable and reliable queues
	// Job worked on reliable_jobs queue
	// Resuming unreliable_external_service queue
	// Job worked on unreliable_external_service queue
}

func waitOrTimeout[T any](ch <-chan T) T {
	select {
	case item := <-ch:
		return item
	case <-time.After(5 * time.Second):
		panic("WaitOrTimeout timed out after waiting 5s")
	}
}

```

`example_scheduled_job_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type ScheduledArgs struct {
	Message string `json:"message"`
}

func (ScheduledArgs) Kind() string { return "scheduled" }

type ScheduledWorker struct {
	river.WorkerDefaults[ScheduledArgs]
}

func (w *ScheduledWorker) Work(ctx context.Context, job *river.Job[ScheduledArgs]) error {
	fmt.Printf("Message: %s\n", job.Args.Message)
	return nil
}

// Example_scheduledJob demonstrates how to schedule a job to be worked in the
// future.
func Example_scheduledJob() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &ScheduledWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	_, err = riverClient.Insert(ctx,
		ScheduledArgs{
			Message: "hello from the future",
		},
		&river.InsertOpts{
			// Schedule the job to be worked in three hours.
			ScheduledAt: time.Now().Add(3 * time.Hour),
		})
	if err != nil {
		panic(err)
	}

	// Unlike most other examples, we don't wait for the job to be worked since
	// doing so would require making the job's scheduled time contrived, and the
	// example therefore less realistic/useful.

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
}

```

`example_subscription_test.go`:

```go
package river_test

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type SubscriptionArgs struct {
	Cancel bool `json:"cancel"`
	Fail   bool `json:"fail"`
}

func (SubscriptionArgs) Kind() string { return "subscription" }

type SubscriptionWorker struct {
	river.WorkerDefaults[SubscriptionArgs]
}

func (w *SubscriptionWorker) Work(ctx context.Context, job *river.Job[SubscriptionArgs]) error {
	switch {
	case job.Args.Cancel:
		return river.JobCancel(errors.New("cancelling job"))
	case job.Args.Fail:
		return errors.New("failing job")
	}
	return nil
}

// Example_subscription demonstrates the use of client subscriptions to receive
// events containing information about worked jobs.
func Example_subscription() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &SubscriptionWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: 9}), // Suppress logging so example output is cleaner (9 > slog.LevelError).
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Subscribers tell the River client the kinds of events they'd like to receive.
	completedChan, completedSubscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer completedSubscribeCancel()

	// Multiple simultaneous subscriptions are allowed.
	failedChan, failedSubscribeCancel := riverClient.Subscribe(river.EventKindJobFailed)
	defer failedSubscribeCancel()

	otherChan, otherSubscribeCancel := riverClient.Subscribe(river.EventKindJobCancelled, river.EventKindJobSnoozed)
	defer otherSubscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	// Insert one job for each subscription above: one to succeed, one to fail,
	// and one that's cancelled that'll arrive on the "other" channel.
	_, err = riverClient.Insert(ctx, SubscriptionArgs{}, nil)
	if err != nil {
		panic(err)
	}
	_, err = riverClient.Insert(ctx, SubscriptionArgs{Fail: true}, nil)
	if err != nil {
		panic(err)
	}
	_, err = riverClient.Insert(ctx, SubscriptionArgs{Cancel: true}, nil)
	if err != nil {
		panic(err)
	}

	waitForJob := func(subscribeChan <-chan *river.Event) {
		select {
		case event := <-subscribeChan:
			if event == nil {
				fmt.Printf("Channel is closed\n")
				return
			}

			fmt.Printf("Got job with state: %s\n", event.Job.State)
		case <-time.After(riversharedtest.WaitTimeout()):
			panic("timed out waiting for job")
		}
	}

	waitForJob(completedChan)
	waitForJob(failedChan)
	waitForJob(otherChan)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	fmt.Printf("Client stopped\n")

	// Try waiting again, but none of these work because stopping the client
	// closed all subscription channels automatically.
	waitForJob(completedChan)
	waitForJob(failedChan)
	waitForJob(otherChan)

	// Output:
	// Got job with state: completed
	// Got job with state: available
	// Got job with state: cancelled
	// Client stopped
	// Channel is closed
	// Channel is closed
	// Channel is closed
}

```

`example_unique_job_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

// Account represents a minimal account including recent expenditures and a
// remaining total.
type Account struct {
	RecentExpenditures int
	AccountTotal       int
}

// Map of account ID -> account.
var allAccounts = map[int]Account{ //nolint:gochecknoglobals
	1: {RecentExpenditures: 100, AccountTotal: 1_000},
	2: {RecentExpenditures: 999, AccountTotal: 1_000},
}

type ReconcileAccountArgs struct {
	AccountID int `json:"account_id"`
}

func (ReconcileAccountArgs) Kind() string { return "reconcile_account" }

// InsertOpts returns custom insert options that every job of this type will
// inherit, including unique options.
func (ReconcileAccountArgs) InsertOpts() river.InsertOpts {
	return river.InsertOpts{
		UniqueOpts: river.UniqueOpts{
			ByArgs:   true,
			ByPeriod: 24 * time.Hour,
		},
	}
}

type ReconcileAccountWorker struct {
	river.WorkerDefaults[ReconcileAccountArgs]
}

func (w *ReconcileAccountWorker) Work(ctx context.Context, job *river.Job[ReconcileAccountArgs]) error {
	account := allAccounts[job.Args.AccountID]

	account.AccountTotal -= account.RecentExpenditures
	account.RecentExpenditures = 0

	fmt.Printf("Reconciled account %d; new total: %d\n", job.Args.AccountID, account.AccountTotal)

	return nil
}

// Example_uniqueJob demonstrates the use of a job with custom
// job-specific insertion options.
func Example_uniqueJob() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &ReconcileAccountWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	// First job insertion for account 1.
	_, err = riverClient.Insert(ctx, ReconcileAccountArgs{AccountID: 1}, nil)
	if err != nil {
		panic(err)
	}

	// Job is inserted a second time, but it doesn't matter because its unique
	// args cause the insertion to be skipped because it's meant to only run
	// once per account per 24 hour period.
	_, err = riverClient.Insert(ctx, ReconcileAccountArgs{AccountID: 1}, nil)
	if err != nil {
		panic(err)
	}

	// Cheat a little by waiting for the first job to come back so we can
	// guarantee that this example's output comes out in order.
	waitForNJobs(subscribeChan, 1)

	// Because the job is unique ByArgs, another job for account 2 is allowed.
	_, err = riverClient.Insert(ctx, ReconcileAccountArgs{AccountID: 2}, nil)
	if err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Reconciled account 1; new total: 900
	// Reconciled account 2; new total: 1
}

```

`example_work_func_test.go`:

```go
package river_test

import (
	"context"
	"fmt"
	"log/slog"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
)

type WorkFuncArgs struct {
	Message string `json:"message"`
}

func (WorkFuncArgs) Kind() string { return "work_func" }

// Example_workFunc demonstrates the use of river.WorkFunc, which can be used to
// easily add a worker with only a function instead of having to implement a
// full worker struct.
func Example_workFunc() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, river.WorkFunc(func(ctx context.Context, job *river.Job[WorkFuncArgs]) error {
		fmt.Printf("Message: %s", job.Args.Message)
		return nil
	}))

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger: slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Queues: map[string]river.QueueConfig{
			river.QueueDefault: {MaxWorkers: 100},
		},
		TestOnly: true, // suitable only for use in tests; remove for live environments
		Workers:  workers,
	})
	if err != nil {
		panic(err)
	}

	// Out of example scope, but used to wait until a job is worked.
	subscribeChan, subscribeCancel := riverClient.Subscribe(river.EventKindJobCompleted)
	defer subscribeCancel()

	if err := riverClient.Start(ctx); err != nil {
		panic(err)
	}

	_, err = riverClient.Insert(ctx, WorkFuncArgs{
		Message: "hello from a function!",
	}, nil)
	if err != nil {
		panic(err)
	}

	waitForNJobs(subscribeChan, 1)

	if err := riverClient.Stop(ctx); err != nil {
		panic(err)
	}

	// Output:
	// Message: hello from a function!
}

```

`go.mod`:

```mod
module github.com/riverqueue/river

go 1.23.0

toolchain go1.24.1

require (
	github.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa
	github.com/jackc/pgx/v5 v5.7.3
	github.com/jackc/puddle/v2 v2.2.2
	github.com/riverqueue/river/riverdriver v0.19.0
	github.com/riverqueue/river/riverdriver/riverdatabasesql v0.19.0
	github.com/riverqueue/river/riverdriver/riverpgxv5 v0.19.0
	github.com/riverqueue/river/rivershared v0.19.0
	github.com/riverqueue/river/rivertype v0.19.0
	github.com/robfig/cron/v3 v3.0.1
	github.com/stretchr/testify v1.10.0
	github.com/tidwall/gjson v1.18.0
	github.com/tidwall/sjson v1.2.5
	go.uber.org/goleak v1.3.0
	golang.org/x/sync v0.12.0
	golang.org/x/text v0.23.0
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/jackc/pgpassfile v1.0.0 // indirect
	github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect
	github.com/lib/pq v1.10.9 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/tidwall/match v1.1.1 // indirect
	github.com/tidwall/pretty v1.2.1 // indirect
	golang.org/x/crypto v0.31.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

replace github.com/riverqueue/river/rivershared => ./rivershared

```

`go.sum`:

```sum
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa h1:s+4MhCQ6YrzisK6hFJUX53drDT4UsSW3DEhKn0ifuHw=
github.com/jackc/pgerrcode v0.0.0-20220416144525-469b46aa5efa/go.mod h1:a/s9Lp5W7n/DD0VrVoyJ00FbP2ytTPDVOivvn2bMlds=
github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
github.com/jackc/pgx/v5 v5.7.3 h1:PO1wNKj/bTAwxSJnO1Z4Ai8j4magtqg2SLNjEDzcXQo=
github.com/jackc/pgx/v5 v5.7.3/go.mod h1:ncY89UGWxg82EykZUwSpUKEfccBGGYq1xjrOpsbsfGQ=
github.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo=
github.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=
github.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=
github.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/riverqueue/river/riverdriver v0.19.0 h1:NyHz5DfB13paT2lvaO0CKmwy4SFLbA7n6MFRGRtwii4=
github.com/riverqueue/river/riverdriver v0.19.0/go.mod h1:Soxi08hHkEvopExAp6ADG2437r4coSiB4QpuIL5E28k=
github.com/riverqueue/river/riverdriver/riverdatabasesql v0.19.0 h1:ytdPnueiv7ANxJcntBtYenrYZZLY5P0mXoDV0l4WsLk=
github.com/riverqueue/river/riverdriver/riverdatabasesql v0.19.0/go.mod h1:5Fahb3n+m1V0RAb0JlOIpzimoTlkOgudMfxSSCTcmFk=
github.com/riverqueue/river/riverdriver/riverpgxv5 v0.19.0 h1:QWg7VTDDXbtTF6srr7Y1C888PiNzqv379yQuNSnH2hg=
github.com/riverqueue/river/riverdriver/riverpgxv5 v0.19.0/go.mod h1:uvF1YS+iSQavCIHtaB/Y6O8A6Dnn38ctVQCpCpmHDZE=
github.com/riverqueue/river/rivertype v0.19.0 h1:5rwgdh21pVcU9WjrHIIO9qC2dOMdRrrZ/HZZOE0JRyY=
github.com/riverqueue/river/rivertype v0.19.0/go.mod h1:DETcejveWlq6bAb8tHkbgJqmXWVLiFhTiEm8j7co1bE=
github.com/robfig/cron/v3 v3.0.1 h1:WdRxkvbJztn8LMz/QEvLN5sBU+xKpSqwwUO1Pjr4qDs=
github.com/robfig/cron/v3 v3.0.1/go.mod h1:eQICP3HwyT7UooqI/z+Ov+PtYAWygg1TEWWzGIFLtro=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/tidwall/gjson v1.14.2/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/gjson v1.18.0 h1:FIDeeyB800efLX89e5a8Y0BNH+LOngJyGrIWxG2FKQY=
github.com/tidwall/gjson v1.18.0/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/match v1.1.1 h1:+Ho715JplO36QYgwN9PGYNhgZvoUSc9X2c80KVTi+GA=
github.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=
github.com/tidwall/pretty v1.2.0/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/pretty v1.2.1 h1:qjsOFOWWQl+N3RsoF5/ssm1pHmJJwhjlSbZ51I6wMl4=
github.com/tidwall/pretty v1.2.1/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/sjson v1.2.5 h1:kLy8mja+1c9jlljvWTlSazM7cKDRfJuR/bOJhcY5NcY=
github.com/tidwall/sjson v1.2.5/go.mod h1:Fvgq9kS/6ociJEDnK0Fk1cpYF4FIW6ZF7LAe+6jwd28=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
golang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/sync v0.12.0 h1:MHc5BpPuC30uJk597Ri8TV3CNZcTLu6B6z4lJy+g6Jw=
golang.org/x/sync v0.12.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/text v0.23.0 h1:D71I7dUrlY+VX0gQShAThNGHFxZ13dGLBHQLVl1mJlY=
golang.org/x/text v0.23.0/go.mod h1:/BLNzu4aZCJ1+kcD0DNRotWKage4q2rGVAg4o22unh4=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

```

`go.work`:

```work
go 1.23.0

toolchain go1.24.1

use (
	.
	./cmd/river
	./riverdriver
	./riverdriver/riverdatabasesql
	./riverdriver/riverpgxv5
	./rivershared
	./rivertype
)

```

`hook_defaults_funcs.go`:

```go
package river

import (
	"context"

	"github.com/riverqueue/river/rivertype"
)

// HookDefaults should be embedded on any hooks implementation. It helps
// identify a struct as hooks, and guarantee forward compatibility in case
// additions are necessary to the rivertype.Hook interface.
type HookDefaults struct{}

func (d *HookDefaults) IsHook() bool { return true }

// HookInsertBeginFunc is a convenience helper for implementing HookInsertBegin
// using a simple function instead of a struct.
type HookInsertBeginFunc func(ctx context.Context, params *rivertype.JobInsertParams) error

func (f HookInsertBeginFunc) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	return f(ctx, params)
}

func (f HookInsertBeginFunc) IsHook() bool { return true }

// HookWorkBeginFunc is a convenience helper for implementing HookworkBegin
// using a simple function instead of a struct.
type HookWorkBeginFunc func(ctx context.Context, job *rivertype.JobRow) error

func (f HookWorkBeginFunc) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	return f(ctx, job)
}

func (f HookWorkBeginFunc) IsHook() bool { return true }

```

`hook_defaults_funcs_test.go`:

```go
package river

import (
	"context"

	"github.com/riverqueue/river/rivertype"
)

// Verify interface compliance.
var (
	_ rivertype.Hook            = HookInsertBeginFunc(func(ctx context.Context, params *rivertype.JobInsertParams) error { return nil })
	_ rivertype.HookInsertBegin = HookInsertBeginFunc(func(ctx context.Context, params *rivertype.JobInsertParams) error { return nil })

	_ rivertype.Hook          = HookWorkBeginFunc(func(ctx context.Context, job *rivertype.JobRow) error { return nil })
	_ rivertype.HookWorkBegin = HookWorkBeginFunc(func(ctx context.Context, job *rivertype.JobRow) error { return nil })
)

```

`insert_opts.go`:

```go
package river

import (
	"errors"
	"fmt"
	"regexp"
	"slices"
	"strings"
	"time"

	"github.com/riverqueue/river/rivertype"
)

// Regular expression to which the format of tags must comply. Mainly, no
// special characters, and with hyphens in the middle.
//
// A key property here (in case this is relaxed in the future) is that commas
// must never be allowed because they're used as a delimiter during batch job
// insertion for the `riverdatabasesql` driver.
var tagRE = regexp.MustCompile(`\A[\w][\w\-]+[\w]\z`)

// InsertOpts are optional settings for a new job which can be provided at job
// insertion time. These will override any default InsertOpts settings provided
// by JobArgsWithInsertOpts, as well as any global defaults.
type InsertOpts struct {
	// MaxAttempts is the maximum number of total attempts (including both the
	// original run and all retries) before a job is abandoned and set as
	// discarded.
	MaxAttempts int

	// Metadata is a JSON object blob of arbitrary data that will be stored with
	// the job. Users should not overwrite or remove anything stored in this
	// field by River.
	Metadata []byte

	// Pending indicates that the job should be inserted in the `pending` state.
	// Pending jobs are not immediately available to be worked and are never
	// deleted, but they can be used to indicate work which should be performed in
	// the future once they are made available (or scheduled) by some external
	// update.
	Pending bool

	// Priority is the priority of the job, with 1 being the highest priority and
	// 4 being the lowest. When fetching available jobs to work, the highest
	// priority jobs will always be fetched before any lower priority jobs are
	// fetched. Note that if your workers are swamped with more high-priority jobs
	// then they can handle, lower priority jobs may not be fetched.
	//
	// Defaults to PriorityDefault.
	Priority int

	// Queue is the name of the job queue in which to insert the job.
	//
	// Defaults to QueueDefault.
	Queue string

	// ScheduledAt is a time in future at which to schedule the job (i.e. in
	// cases where it shouldn't be run immediately). The job is guaranteed not
	// to run before this time, but may run slightly after depending on the
	// number of other scheduled jobs and how busy the queue is.
	//
	// Use of this option generally only makes sense when passing options into
	// Insert rather than when a job args struct is implementing
	// JobArgsWithInsertOpts, however, it will work in both cases.
	ScheduledAt time.Time

	// Tags are an arbitrary list of keywords to add to the job. They have no
	// functional behavior and are meant entirely as a user-specified construct
	// to help group and categorize jobs.
	//
	// Tags should conform to the regex `\A[\w][\w\-]+[\w]\z` and be a maximum
	// of 255 characters long. No special characters are allowed.
	//
	// If tags are specified from both a job args override and from options on
	// Insert, the latter takes precedence. Tags are not merged.
	Tags []string

	// UniqueOpts returns options relating to job uniqueness. An empty struct
	// avoids setting any worker-level unique options.
	UniqueOpts UniqueOpts
}

// UniqueOpts contains parameters for uniqueness for a job.
//
// When the options struct is uninitialized (its zero value) no uniqueness at is
// enforced. As each property is initialized, it's added as a dimension on the
// uniqueness matrix. When any property has a non-zero value specified, the
// job's kind automatically counts toward uniqueness, but can be excluded by
// setting ExcludeKind to true.
//
// So for example, if only ByQueue is on, then for the given job kind, only a
// single instance is allowed in any given queue, regardless of other properties
// on the job. If both ByArgs and ByQueue are on, then for the given job kind, a
// single instance is allowed for each combination of args and queues. If either
// args or queue is changed on a new job, it's allowed to be inserted as a new
// job.
//
// Uniqueness relies on a hash of the job kind and any unique properties along
// with a database unique constraint. See the note on ByState for more details
// including about the fallback to a deprecated advisory lock method.
type UniqueOpts struct {
	// ByArgs indicates that uniqueness should be enforced for any specific
	// instance of encoded args for a job.
	//
	// Default is false, meaning that as long as any other unique property is
	// enabled, uniqueness will be enforced for a kind regardless of input args.
	//
	// When set to true, the entire encoded args field will be included in the
	// uniqueness hash, which requires care to ensure that no irrelevant args are
	// factored into the uniqueness check. It is also possible to use a subset of
	// the args by indicating on the `JobArgs` struct which fields should be
	// included in the uniqueness check using struct tags:
	//
	// 	type MyJobArgs struct {
	// 		CustomerID string `json:"customer_id" river:"unique"`
	// 		TraceID string `json:"trace_id"
	// 	}
	//
	// In this example, only the encoded `customer_id` key will be included in the
	// uniqueness check and the `trace_id` key will be ignored.
	//
	// All keys are sorted alphabetically before hashing to ensure consistent
	// results.
	ByArgs bool

	// ByPeriod defines uniqueness within a given period. On an insert time is
	// rounded down to the nearest multiple of the given period, and a job is
	// only inserted if there isn't an existing job that will run between then
	// and the next multiple of the period.
	//
	// Default is no unique period, meaning that as long as any other unique
	// property is enabled, uniqueness will be enforced across all jobs of the
	// kind in the database, regardless of when they were scheduled.
	ByPeriod time.Duration

	// ByQueue indicates that uniqueness should be enforced within each queue.
	//
	// Default is false, meaning that as long as any other unique property is
	// enabled, uniqueness will be enforced for a kind across all queues.
	ByQueue bool

	// ByState indicates that uniqueness should be enforced across any of the
	// states in the given set. Unlike other unique options, ByState gets a
	// default when it's not set for user convenience. The default is equivalent
	// to:
	//
	// 	ByState: []rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted, rivertype.JobStatePending, rivertype.JobStateRunning, rivertype.JobStateRetryable, rivertype.JobStateScheduled}
	//
	// Or more succinctly:
	//
	// 	ByState: rivertype.UniqueOptsByStateDefault()
	//
	// With this setting, any jobs of the same kind that have been completed or
	// discarded, but not yet cleaned out by the system, will still prevent a
	// duplicate unique job from being inserted. For example, with the default
	// states, if a unique job is actively `running`, a duplicate cannot be
	// inserted. Likewise, if a unique job has `completed`, you still can't
	// insert a duplicate, at least not until the job cleaner maintenance process
	// eventually removes the completed job from the `river_job` table.
	//
	// The list may be safely customized to _add_ additional states (`cancelled`
	// or `discarded`), though only `retryable` may be safely _removed_ from the
	// list.
	//
	// Warning: Removing any states from the default list (other than `retryable`)
	// forces a fallback to a slower insertion path that takes an advisory lock
	// and performs a look up before insertion. This path is deprecated and should
	// be avoided if possible.
	ByState []rivertype.JobState

	// ExcludeKind indicates that the job kind should not be included in the
	// uniqueness check. This is useful when you want to enforce uniqueness
	// across all jobs regardless of kind.
	ExcludeKind bool
}

// isEmpty returns true for an empty, uninitialized options struct.
//
// This is required because we can't check against `UniqueOpts{}` because slices
// aren't comparable. Unfortunately it makes things a little more brittle
// comparatively because any new options must also be considered here for things
// to work.
func (o *UniqueOpts) isEmpty() bool {
	return !o.ByArgs &&
		o.ByPeriod == time.Duration(0) &&
		!o.ByQueue &&
		o.ByState == nil
}

var jobStateAll = rivertype.JobStates() //nolint:gochecknoglobals

var requiredV3states = []rivertype.JobState{ //nolint:gochecknoglobals
	rivertype.JobStateAvailable,
	rivertype.JobStatePending,
	rivertype.JobStateRunning,
	rivertype.JobStateScheduled,
}

func (o *UniqueOpts) validate() error {
	if o.isEmpty() {
		return nil
	}

	if o.ByPeriod != time.Duration(0) && o.ByPeriod < 1*time.Second {
		return errors.New("UniqueOpts.ByPeriod should not be less than 1 second")
	}

	// Job states are typed, but since the underlying type is a string, users
	// can put anything they want in there.
	for _, state := range o.ByState {
		// This could be turned to a map lookup, but last I checked the speed
		// difference for tiny slice sizes is negligible, and map lookup might
		// even be slower.
		if !slices.Contains(jobStateAll, state) {
			return fmt.Errorf("UniqueOpts.ByState contains invalid state %q", state)
		}
	}

	// Skip required states validation if no custom states were provided.
	if len(o.ByState) == 0 {
		return nil
	}

	var missingStates []string
	for _, state := range requiredV3states {
		if !slices.Contains(o.ByState, state) {
			missingStates = append(missingStates, string(state))
		}
	}
	if len(missingStates) > 0 {
		return fmt.Errorf("UniqueOpts.ByState must contain all required states, missing: %s", strings.Join(missingStates, ", "))
	}

	return nil
}

```

`insert_opts_test.go`:

```go
package river

import (
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivertype"
)

func TestTagRE(t *testing.T) {
	t.Parallel()

	require.Regexp(t, tagRE, "aaa")
	require.Regexp(t, tagRE, "_aaa")
	require.Regexp(t, tagRE, "aaa_")
	require.Regexp(t, tagRE, "777")
	require.Regexp(t, tagRE, "my-tag")
	require.Regexp(t, tagRE, "my_tag")
	require.Regexp(t, tagRE, "my-longer-tag")
	require.Regexp(t, tagRE, "my_longer_tag")
	require.Regexp(t, tagRE, "My_Capitalized_Tag")
	require.Regexp(t, tagRE, "ALL_CAPS")
	require.Regexp(t, tagRE, "1_2_3")

	require.NotRegexp(t, tagRE, "a")
	require.NotRegexp(t, tagRE, "aa")
	require.NotRegexp(t, tagRE, "-aaa")
	require.NotRegexp(t, tagRE, "aaa-")
	require.NotRegexp(t, tagRE, "special@characters$banned")
	require.NotRegexp(t, tagRE, "commas,never,allowed")
}

func TestUniqueOpts_validate(t *testing.T) {
	t.Parallel()

	require.NoError(t, (&UniqueOpts{}).validate())
	require.NoError(t, (&UniqueOpts{
		ByArgs:   true,
		ByPeriod: 1 * time.Second,
		ByQueue:  true,
	}).validate())

	require.EqualError(t, (&UniqueOpts{ByPeriod: 1 * time.Millisecond}).validate(), "UniqueOpts.ByPeriod should not be less than 1 second")
	require.EqualError(t, (&UniqueOpts{ByState: []rivertype.JobState{rivertype.JobState("invalid")}}).validate(), `UniqueOpts.ByState contains invalid state "invalid"`)

	requiredStates := []rivertype.JobState{
		rivertype.JobStateAvailable,
		rivertype.JobStatePending,
		rivertype.JobStateRunning,
		rivertype.JobStateScheduled,
	}

	for _, state := range requiredStates {
		// Test with each state individually removed from requiredStates to ensure
		// it's validated.

		// Create a copy of requiredStates without the current state
		var testStates []rivertype.JobState
		for _, s := range requiredStates {
			if s != state {
				testStates = append(testStates, s)
			}
		}

		// Test validation
		require.EqualError(t, (&UniqueOpts{ByState: testStates}).validate(), "UniqueOpts.ByState must contain all required states, missing: "+string(state))
	}

	// test with more than one required state missing:
	require.EqualError(t, (&UniqueOpts{ByState: []rivertype.JobState{
		rivertype.JobStateAvailable,
		rivertype.JobStateScheduled,
	}}).validate(), "UniqueOpts.ByState must contain all required states, missing: pending, running")

	require.NoError(t, (&UniqueOpts{ByState: rivertype.JobStates()}).validate())
}

```

`internal/cmd/testdbman/main.go`:

```go
// testdbman is a command-line tool for managing the test databases used by
// parallel tests and the sample applications.
package main

import (
	"context"
	"errors"
	"flag"
	"fmt"
	"io"
	"os"
	"runtime"
	"slices"
	"strings"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivermigrate"
	"github.com/riverqueue/river/rivershared/util/maputil"
)

func main() {
	commandBundle := NewCommandBundle(
		"testdbman",
		"testdbman manages test databases",
		`
A small program to create and manage test databases. River currently requires a
number of different of test databases loaded with its schema for it to be able
to run the full test suite in parallel.

Run "testdbman create" to raise all required test databases and prepare for a
test run.
		`,
	)

	// create
	{
		commandBundle.AddCommand(
			"create",
			"Create test databases",
			`
Creates the test databases used by parallel tests and the sample applications.
Each is migrated with River's current schema.

The sample application DB is named river_test, while the DBs for parallel
tests are named river_test_0, river_test_1, etc. up to the larger of 4 or
runtime.NumCPU() (a choice that comes from pgx's default connection pool size).
`,
			createTestDatabases,
		)
	}

	// drop
	{
		commandBundle.AddCommand(
			"drop",
			"Drop test databases",
			`
Drops all test databases. Any test database matching the base name
(river_test) or the base name with an underscore followed by any other token
(river_test_example, river_test_0, river_test_1, etc.) will be dropped.
`,
			dropTestDatabases,
		)
	}

	// reset
	{
		commandBundle.AddCommand(
			"reset",
			"Drop and recreate test databases",
			`
Reset the test databases, dropping the existing database(s) if they exist, and
recreating them with the most up to date schema. Equivalent to running "drop"
followed by "create".
`,
			resetTestDatabases,
		)
	}

	ctx := context.Background()

	if err := commandBundle.Exec(ctx, os.Args); err != nil {
		fmt.Fprintf(os.Stderr, "failed: %s\n", err)
		os.Exit(1)
	}
}

//
// Commands
//

const managementDatabaseURL = "postgres:///postgres"

//
// Helpers
//

func createTestDatabases(ctx context.Context, out io.Writer) error {
	mgmtConn, err := pgx.Connect(ctx, managementDatabaseURL)
	if err != nil {
		return fmt.Errorf("error opening management connection: %w", err)
	}
	defer mgmtConn.Close(ctx)

	createDBAndMigrate := func(dbName string) error {
		if _, err := mgmtConn.Exec(ctx, "CREATE DATABASE "+dbName); err != nil {
			return fmt.Errorf("error crating database %q: %w", dbName, err)
		}
		fmt.Fprintf(out, "created: %-20s", dbName)

		// Defer printing a newline, which will be either added to the end of a
		// successful invocation of this command (after the string "[and
		// migrated]" has been printed to the current line), or printed before
		// returning an error so that in either case output looks right.
		defer fmt.Fprintf(out, "\n")

		dbURL := "postgres:///" + dbName

		dbPool, err := pgxpool.New(ctx, dbURL)
		if err != nil {
			return fmt.Errorf("error creating connection pool to %q: %w", dbURL, err)
		}
		defer dbPool.Close()

		migrator, err := rivermigrate.New(riverpgxv5.New(dbPool), nil)
		if err != nil {
			return err
		}

		if _, err = migrator.Migrate(ctx, rivermigrate.DirectionUp, &rivermigrate.MigrateOpts{}); err != nil {
			return err
		}
		fmt.Fprintf(out, " [and migrated]")

		return nil
	}

	// Allow up to one database per concurrent test, plus two for overhead:
	maxTestDBs := runtime.GOMAXPROCS(0) + 2
	dbNames := generateTestDBNames(maxTestDBs)
	for _, dbName := range dbNames {
		if err := createDBAndMigrate(dbName); err != nil {
			return err
		}
	}

	return nil
}

func generateTestDBNames(numDBs int) []string {
	dbNames := []string{
		"river_test",
		"river_test_example",
	}

	for i := range numDBs {
		dbNames = append(dbNames, fmt.Sprintf("river_test_%d", i))
	}

	return dbNames
}

func dropTestDatabases(ctx context.Context, out io.Writer) error {
	mgmtConn, err := pgx.Connect(ctx, managementDatabaseURL)
	if err != nil {
		return fmt.Errorf("error opening management connection: %w", err)
	}
	defer mgmtConn.Close(ctx)

	rows, err := mgmtConn.Query(ctx, "SELECT datname FROM pg_database")
	if err != nil {
		return fmt.Errorf("error listing databases: %w", err)
	}
	defer rows.Close()

	allDBNames := make([]string, 0)
	for rows.Next() {
		var dbName string
		err := rows.Scan(&dbName)
		if err != nil {
			return fmt.Errorf("error scanning database name: %w", err)
		}
		allDBNames = append(allDBNames, dbName)
	}
	rows.Close()

	for _, dbName := range allDBNames {
		if strings.HasPrefix(dbName, "river_test") {
			if _, err := mgmtConn.Exec(ctx, "DROP DATABASE "+dbName); err != nil {
				return fmt.Errorf("error dropping database %q: %w", dbName, err)
			}
			fmt.Fprintf(out, "dropped: %s\n", dbName)
		}
	}

	return nil
}

func resetTestDatabases(ctx context.Context, out io.Writer) error {
	if err := dropTestDatabases(ctx, out); err != nil {
		return err
	}

	if err := createTestDatabases(ctx, out); err != nil {
		return err
	}

	return nil
}

//
// Command bundle framework
//

// CommandBundle is a basic CLI command framework similar to Cobra, but with far
// reduced capabilities. I know it seems crazy to write one when Cobra is
// available, but the test manager's interface is quite simple, and not using
// Cobra lets us drop its dependency in the main River package.
type CommandBundle struct {
	commands map[string]*commandBundleCommand
	long     string
	out      io.Writer
	short    string
	use      string
}

func NewCommandBundle(use, short, long string) *CommandBundle {
	if use == "" {
		panic("use is required")
	}
	if short == "" {
		panic("short is required")
	}
	if long == "" {
		panic("long is required")
	}

	return &CommandBundle{
		commands: make(map[string]*commandBundleCommand),
		long:     long,
		out:      os.Stdout,
		short:    short,
		use:      use,
	}
}

func (b *CommandBundle) AddCommand(use, short, long string, execFunc func(ctx context.Context, out io.Writer) error) {
	if use == "" {
		panic("use is required")
	}
	if short == "" {
		panic("short is required")
	}
	if long == "" {
		panic("long is required")
	}
	if execFunc == nil {
		panic("execFunc is required")
	}

	if _, ok := b.commands[use]; ok {
		panic("command already registered: " + use)
	}

	b.commands[use] = &commandBundleCommand{
		execFunc: execFunc,
		long:     long,
		short:    short,
		use:      use,
	}
}

const helpUse = "help"

func (b *CommandBundle) Exec(ctx context.Context, args []string) error {
	ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	var (
		flagSet flag.FlagSet
		help    bool
	)
	flagSet.BoolVar(&help, "help", false, "help for program or command")

	args = args[1:] // drop program name

	var commandUse string
	if len(args) > 0 && args[0][0] != '-' {
		commandUse = args[0]
		args = args[1:]
	}

	if err := flagSet.Parse(args); err != nil {
		return err
	}

	args = flagSet.Args()

	// Try extracting a command again after flags are parsed and we didn't get
	// one on the first pass.
	if commandUse == "" && len(args) > 0 {
		commandUse = args[0]
		args = args[1:]
	}

	if commandUse != "" && commandUse != helpUse && len(args) > 0 || len(args) > 1 {
		return errors.New("expected exactly one command")
	}

	if commandUse == "" || commandUse == helpUse && len(args) < 1 {
		fmt.Fprintf(b.out, "%s\n", b.usage(&flagSet))
		return nil
	}

	if commandUse == "help" {
		commandUse = args[0]
		help = true
	}

	command, ok := b.commands[commandUse]
	if !ok {
		return errors.New("unknown command: " + commandUse)
	}

	if help {
		fmt.Fprintf(b.out, "%s\n", command.printUsage(b.use, &flagSet))
		return nil
	}

	return command.execFunc(ctx, b.out)
}

func (b *CommandBundle) usage(flagSet *flag.FlagSet) string {
	var sb strings.Builder

	sb.WriteString(fmt.Sprintf(`
%s

Usage:
  %s [command] [flags]

Available Commands:
`, strings.TrimSpace(b.long), b.use))

	var longestUse int
	for use := range b.commands {
		if len(use) > longestUse {
			longestUse = len(use)
		}
	}

	// Go's maps are unordered of course. Alphabetize.
	sortedCommandUses := maputil.Keys(b.commands)
	slices.Sort(sortedCommandUses)

	for _, use := range sortedCommandUses {
		command := b.commands[use]
		sb.WriteString(fmt.Sprintf("  %-*s  %s\n", longestUse, use, command.short))
	}

	sb.WriteString("\nFlags:\n")
	flagSet.SetOutput(&sb)
	flagSet.PrintDefaults()

	sb.WriteString(fmt.Sprintf(`
Use "%s [command] -help" for more information about a command.
  `, b.use))

	// Go's flag module loves tabs of course. Kill them in favor of spaces,
	// which are easier to test against.
	return strings.TrimSpace(strings.ReplaceAll(sb.String(), "\t", "    "))
}

type commandBundleCommand struct {
	execFunc func(ctx context.Context, out io.Writer) error
	long     string
	short    string
	use      string
}

func (b *commandBundleCommand) printUsage(bundleUse string, flagSet *flag.FlagSet) string {
	var sb strings.Builder

	sb.WriteString(fmt.Sprintf(`
%s

Usage:
  %s %s [flags]
`, strings.TrimSpace(b.long), bundleUse, b.use))

	sb.WriteString("\nFlags:\n")
	flagSet.SetOutput(&sb)
	flagSet.PrintDefaults()

	// Go's flag module loves tabs of course. Kill them in favor of spaces,
	// which are easier to test against.
	return strings.TrimSpace(strings.ReplaceAll(sb.String(), "\t", "    "))
}

```

`internal/cmd/testdbman/main_test.go`:

```go
package main

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"io"
	"strings"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestGenerateTestDBNames(t *testing.T) {
	t.Parallel()

	require.Equal(t, []string{
		"river_test",
		"river_test_example",
		"river_test_0",
		"river_test_1",
		"river_test_2",
		"river_test_3",
	}, generateTestDBNames(4))
}

//
// Command bundle framework
//

func TestCommandBundle(t *testing.T) {
	t.Parallel()

	var (
		baseArgs = []string{"fake-program-name"} // os args always include a program name
		ctx      = context.Background()
	)

	type testBundle struct {
		buf             *bytes.Buffer
		command1Invoked *bool
		command2Invoked *bool
	}

	setup := func() (*CommandBundle, *testBundle) {
		commandBundle := NewCommandBundle(
			"testcom",
			"testcom is a test bundle for use tests",
			`
A test only program for testing the command bundle framework, especially some
complexities around how it emits output. This is the long description and is
meant to be wrapped at 80 characters in your editor.

It may be multiple paragraphs. This is a second paragraph with additional
information and context.
		`,
		)

		var command1Invoked bool
		{
			commandBundle.AddCommand(
				"command1",
				"The program's first command",
				`
This is a long description for the program's first command. It acts somewhat
like a mock, setting a boolean to true that we can easily check in tests in case
the program makes the decision to invoke it.
`,
				func(ctx context.Context, out io.Writer) error {
					fmt.Fprintf(out, "command1 executed\n")
					command1Invoked = true
					return nil
				},
			)
		}

		var command2Invoked bool
		{
			commandBundle.AddCommand(
				"command2",
				"The program's second command",
				`
This is a long description for the program's second command. It's the same as
the first command, and acts somewhat like a mock, setting a boolean to true that
we can easily check in tests in case the program makes the decision to invoke
it.
`,
				func(ctx context.Context, out io.Writer) error {
					fmt.Fprintf(out, "command2 executed\n")
					command2Invoked = true
					return nil
				},
			)
		}
		{
			commandBundle.AddCommand(
				"makeerror",
				"A command that returns an error",
				`
The long description for a command that returns an error that we can check
against in tests to make sure that piece of the puzzle works as expected.
`,
				func(ctx context.Context, out io.Writer) error {
					fmt.Fprintf(out, "makeerror executed\n")
					return errors.New("command error!")
				},
			)
		}

		var buf bytes.Buffer
		commandBundle.out = &buf

		return commandBundle, &testBundle{
			buf:             &buf,
			command1Invoked: &command1Invoked,
			command2Invoked: &command2Invoked,
		}
	}

	expectedCommandBundleUsage := strings.TrimSpace(`
A test only program for testing the command bundle framework, especially some
complexities around how it emits output. This is the long description and is
meant to be wrapped at 80 characters in your editor.

It may be multiple paragraphs. This is a second paragraph with additional
information and context.

Usage:
  testcom [command] [flags]

Available Commands:
  command1   The program's first command
  command2   The program's second command
  makeerror  A command that returns an error

Flags:
  -help
        help for program or command

Use "testcom [command] -help" for more information about a command.
	`) + "\n"

	t.Run("ShowsUsageWithHelpArgument", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, append(baseArgs, "-help")))

		require.Equal(t, expectedCommandBundleUsage, bundle.buf.String())
	})

	t.Run("ShowsUsageWithHelpCommand", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, append(baseArgs, "help")))

		require.Equal(t, expectedCommandBundleUsage, bundle.buf.String())
	})

	t.Run("ShowsUsageWithNoArguments", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, baseArgs))

		require.Equal(t, expectedCommandBundleUsage, bundle.buf.String())
	})

	expectedCommand1Usage := strings.TrimSpace(`
This is a long description for the program's first command. It acts somewhat
like a mock, setting a boolean to true that we can easily check in tests in case
the program makes the decision to invoke it.

Usage:
  testcom command1 [flags]

Flags:
  -help
        help for program or command
	`) + "\n"

	t.Run("ShowsCommandUsageWithHelpArgument", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, append(baseArgs, "command1", "-help")))

		require.Equal(t, expectedCommand1Usage, bundle.buf.String())
	})

	t.Run("ShowsCommandUsageWithHelpCommand", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, append(baseArgs, "help", "command1")))

		require.Equal(t, expectedCommand1Usage, bundle.buf.String())
	})

	t.Run("ShowsCommandUsageWithMisorderedHelpArgument", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, append(baseArgs, "-help", "command1")))

		require.Equal(t, expectedCommand1Usage, bundle.buf.String())
	})

	t.Run("ErrorsOnTooManyArguments", func(t *testing.T) {
		t.Parallel()

		commandBundle, _ := setup()

		require.EqualError(t, commandBundle.Exec(ctx, append(baseArgs, "command1", "command2")),
			"expected exactly one command")
	})

	t.Run("ErrorsOnUnknownCommand", func(t *testing.T) {
		t.Parallel()

		commandBundle, _ := setup()

		require.EqualError(t, commandBundle.Exec(ctx, append(baseArgs, "command3")),
			"unknown command: command3")
	})

	t.Run("RunsCommand", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, append(baseArgs, "command1")))
		require.True(t, *bundle.command1Invoked)
		require.False(t, *bundle.command2Invoked)

		require.Equal(t, "command1 executed\n", bundle.buf.String())
	})

	t.Run("DisambiguatesCommands", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.NoError(t, commandBundle.Exec(ctx, append(baseArgs, "command2")))
		require.False(t, *bundle.command1Invoked)
		require.True(t, *bundle.command2Invoked)

		require.Equal(t, "command2 executed\n", bundle.buf.String())
	})

	t.Run("ReturnsErrorFromCommand", func(t *testing.T) {
		t.Parallel()

		commandBundle, bundle := setup()

		require.EqualError(t, commandBundle.Exec(ctx, append(baseArgs, "makeerror")), "command error!")

		require.Equal(t, "makeerror executed\n", bundle.buf.String())
	})
}

```

`internal/dblist/db_list.go`:

```go
package dblist

import (
	"context"
	"errors"
	"strings"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

type SortOrder int

const (
	SortOrderUnspecified SortOrder = iota
	SortOrderAsc
	SortOrderDesc
)

type JobListOrderBy struct {
	Expr  string
	Order SortOrder
}

type JobListParams struct {
	Conditions string
	Kinds      []string
	LimitCount int32
	NamedArgs  map[string]any
	OrderBy    []JobListOrderBy
	Priorities []int16
	Queues     []string
	States     []rivertype.JobState
}

func JobList(ctx context.Context, exec riverdriver.Executor, params *JobListParams) ([]*rivertype.JobRow, error) {
	var whereBuilder strings.Builder

	orderBy := make([]JobListOrderBy, len(params.OrderBy))
	for i, o := range params.OrderBy {
		orderBy[i] = JobListOrderBy{
			Expr:  o.Expr,
			Order: o.Order,
		}
	}

	namedArgs := params.NamedArgs
	if namedArgs == nil {
		namedArgs = make(map[string]any)
	}

	// Writes an `AND` to connect SQL predicates as long as this isn't the first
	// predicate.
	writeAndAfterFirst := func() {
		if whereBuilder.Len() != 0 {
			whereBuilder.WriteString("\n  AND ")
		}
	}

	if len(params.Kinds) > 0 {
		writeAndAfterFirst()
		whereBuilder.WriteString("kind = any(@kinds::text[])")
		namedArgs["kinds"] = params.Kinds
	}

	if len(params.Queues) > 0 {
		writeAndAfterFirst()
		whereBuilder.WriteString("queue = any(@queues::text[])")
		namedArgs["queues"] = params.Queues
	}

	if len(params.States) > 0 {
		writeAndAfterFirst()
		whereBuilder.WriteString("state = any(@states::river_job_state[])")
		namedArgs["states"] = sliceutil.Map(params.States, func(s rivertype.JobState) string { return string(s) })
	}

	if params.Conditions != "" {
		writeAndAfterFirst()
		whereBuilder.WriteString(params.Conditions)
	}

	// A condition of some kind is needed, so given no others write one that'll
	// always return true.
	if whereBuilder.Len() < 1 {
		whereBuilder.WriteString("1")
	}

	if params.LimitCount < 1 {
		return nil, errors.New("required parameter 'Count' in JobList must be greater than zero")
	}

	if len(params.OrderBy) == 0 {
		return nil, errors.New("sort order is required")
	}

	var orderByBuilder strings.Builder

	for i, orderBy := range params.OrderBy {
		orderByBuilder.WriteString(orderBy.Expr)
		if orderBy.Order == SortOrderAsc {
			orderByBuilder.WriteString(" ASC")
		} else if orderBy.Order == SortOrderDesc {
			orderByBuilder.WriteString(" DESC")
		}
		if i < len(params.OrderBy)-1 {
			orderByBuilder.WriteString(", ")
		}
	}

	return exec.JobList(ctx, &riverdriver.JobListParams{
		Max:           params.LimitCount,
		NamedArgs:     namedArgs,
		OrderByClause: orderByBuilder.String(),
		WhereClause:   whereBuilder.String(),
	})
}

```

`internal/dblist/db_list_test.go`:

```go
package dblist

import (
	"context"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

func TestJobListNoJobs(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		exec riverdriver.Executor
	}

	setup := func() *testBundle {
		driver := riverpgxv5.New(nil)

		return &testBundle{
			exec: driver.UnwrapExecutor(riverinternaltest.TestTx(ctx, t)),
		}
	}

	t.Run("Minimal", func(t *testing.T) {
		t.Parallel()

		bundle := setup()

		_, err := JobList(ctx, bundle.exec, &JobListParams{
			States:     []rivertype.JobState{rivertype.JobStateCompleted},
			LimitCount: 1,
			OrderBy:    []JobListOrderBy{{Expr: "id", Order: SortOrderAsc}},
		})
		require.NoError(t, err)
	})

	t.Run("WithConditionsAndSortOrders", func(t *testing.T) {
		t.Parallel()

		bundle := setup()

		_, err := JobList(ctx, bundle.exec, &JobListParams{
			Conditions: "queue = 'test' AND priority = 1 AND args->>'foo' = @foo",
			NamedArgs:  pgx.NamedArgs{"foo": "bar"},
			States:     []rivertype.JobState{rivertype.JobStateCompleted},
			LimitCount: 1,
			OrderBy:    []JobListOrderBy{{Expr: "id", Order: SortOrderAsc}},
		})
		require.NoError(t, err)
	})
}

func TestJobListWithJobs(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		baselineTime time.Time // baseline time frozen at now when setup is called
		driver       riverdriver.Driver[pgx.Tx]
		exec         riverdriver.Executor
		jobs         []*rivertype.JobRow
	}

	setup := func(t *testing.T) *testBundle {
		t.Helper()

		var (
			driver = riverpgxv5.New(nil)
			tx     = riverinternaltest.TestTx(ctx, t)
			exec   = driver.UnwrapExecutor(tx)
		)

		job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Queue: ptrutil.Ptr("priority")})
		job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{EncodedArgs: []byte(`{"job_num": 2}`)})
		job3 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Metadata: []byte(`{"some_key": "some_value"}`)})
		job4 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		job5 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("alternate_kind")})

		return &testBundle{
			baselineTime: time.Now(),
			driver:       driver,
			exec:         exec,
			jobs:         []*rivertype.JobRow{job1, job2, job3, job4, job5},
		}
	}

	type testListFunc func(jobs []*rivertype.JobRow, err error)

	execTest := func(ctx context.Context, t *testing.T, bundle *testBundle, params *JobListParams, testFunc testListFunc) {
		t.Helper()
		t.Logf("testing JobList in Executor")
		jobs, err := JobList(ctx, bundle.exec, params)
		testFunc(jobs, err)

		t.Logf("testing JobListTx")
		// use a sub-transaction in case it's rolled back or errors:
		tx, err := bundle.exec.Begin(ctx)
		require.NoError(t, err)
		defer tx.Rollback(ctx)
		jobs, err = JobList(ctx, tx, params)
		testFunc(jobs, err)
	}

	t.Run("Minimal", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		params := &JobListParams{
			LimitCount: 3,
			OrderBy:    []JobListOrderBy{{Expr: "id", Order: SortOrderDesc}},
			States:     []rivertype.JobState{rivertype.JobStateAvailable},
		}

		execTest(ctx, t, bundle, params, func(jobs []*rivertype.JobRow, err error) {
			require.NoError(t, err)

			// job 1 is excluded due to pagination limit of 2, while job 4 is excluded
			// due to its state:
			job2 := bundle.jobs[1]
			job3 := bundle.jobs[2]
			job5 := bundle.jobs[4]

			returnedIDs := sliceutil.Map(jobs, func(j *rivertype.JobRow) int64 { return j.ID })
			require.Equal(t, []int64{job5.ID, job3.ID, job2.ID}, returnedIDs)
		})
	})

	t.Run("ComplexConditionsWithNamedArgs", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		params := &JobListParams{
			Conditions: "jsonb_extract_path(args, VARIADIC @paths1::text[]) = @value1::jsonb",
			LimitCount: 2,
			NamedArgs:  map[string]any{"paths1": []string{"job_num"}, "value1": 2},
			OrderBy:    []JobListOrderBy{{Expr: "id", Order: SortOrderDesc}},
			States:     []rivertype.JobState{rivertype.JobStateAvailable},
		}

		execTest(ctx, t, bundle, params, func(jobs []*rivertype.JobRow, err error) {
			require.NoError(t, err)

			job2 := bundle.jobs[1]
			returnedIDs := sliceutil.Map(jobs, func(j *rivertype.JobRow) int64 { return j.ID })
			require.Equal(t, []int64{job2.ID}, returnedIDs)
		})
	})

	t.Run("ConditionsWithKinds", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		params := &JobListParams{
			Conditions: "finalized_at IS NULL",
			LimitCount: 2,
			OrderBy:    []JobListOrderBy{{Expr: "id", Order: SortOrderDesc}},
			Kinds:      []string{"alternate_kind"},
			States:     []rivertype.JobState{rivertype.JobStateAvailable},
		}

		execTest(ctx, t, bundle, params, func(jobs []*rivertype.JobRow, err error) {
			require.NoError(t, err)

			job1 := bundle.jobs[4]
			returnedIDs := sliceutil.Map(jobs, func(j *rivertype.JobRow) int64 { return j.ID })
			require.Equal(t, []int64{job1.ID}, returnedIDs)
		})
	})

	t.Run("ConditionsWithQueues", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		params := &JobListParams{
			Conditions: "finalized_at IS NULL",
			LimitCount: 2,
			OrderBy:    []JobListOrderBy{{Expr: "id", Order: SortOrderDesc}},
			Queues:     []string{"priority"},
			States:     []rivertype.JobState{rivertype.JobStateAvailable},
		}

		execTest(ctx, t, bundle, params, func(jobs []*rivertype.JobRow, err error) {
			require.NoError(t, err)

			job1 := bundle.jobs[0]
			returnedIDs := sliceutil.Map(jobs, func(j *rivertype.JobRow) int64 { return j.ID })
			require.Equal(t, []int64{job1.ID}, returnedIDs)
		})
	})

	t.Run("WithMetadataAndNoStateFilter", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		params := &JobListParams{
			Conditions: "metadata @> @metadata_filter::jsonb",
			LimitCount: 2,
			NamedArgs:  map[string]any{"metadata_filter": `{"some_key": "some_value"}`},
			OrderBy:    []JobListOrderBy{{Expr: "id", Order: SortOrderDesc}},
		}

		execTest(ctx, t, bundle, params, func(jobs []*rivertype.JobRow, err error) {
			require.NoError(t, err)

			job3 := bundle.jobs[2]
			returnedIDs := sliceutil.Map(jobs, func(j *rivertype.JobRow) int64 { return j.ID })
			require.Equal(t, []int64{job3.ID}, returnedIDs)
		})
	})
}

```

`internal/dbunique/db_unique.go`:

```go
package dbunique

import (
	"crypto/sha256"
	"slices"
	"strings"
	"time"

	"github.com/tidwall/gjson"
	"github.com/tidwall/sjson"

	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

// Default job states for UniqueOpts.ByState. Stored here to a variable so we
// don't have to reallocate a slice over and over again.
var uniqueOptsByStateDefault = rivertype.UniqueOptsByStateDefault() //nolint:gochecknoglobals

var jobStateBitPositions = map[rivertype.JobState]uint{ //nolint:gochecknoglobals
	rivertype.JobStateAvailable: 7,
	rivertype.JobStateCancelled: 6,
	rivertype.JobStateCompleted: 5,
	rivertype.JobStateDiscarded: 4,
	rivertype.JobStatePending:   3,
	rivertype.JobStateRetryable: 2,
	rivertype.JobStateRunning:   1,
	rivertype.JobStateScheduled: 0,
}

type UniqueOpts struct {
	ByArgs      bool
	ByPeriod    time.Duration
	ByQueue     bool
	ByState     []rivertype.JobState
	ExcludeKind bool
}

func (o *UniqueOpts) IsEmpty() bool {
	return !o.ByArgs &&
		o.ByPeriod == time.Duration(0) &&
		!o.ByQueue &&
		o.ByState == nil &&
		!o.ExcludeKind
}

func (o *UniqueOpts) StateBitmask() byte {
	states := uniqueOptsByStateDefault
	if len(o.ByState) > 0 {
		states = o.ByState
	}
	return UniqueStatesToBitmask(states)
}

func UniqueKey(timeGen rivertype.TimeGenerator, uniqueOpts *UniqueOpts, params *rivertype.JobInsertParams) ([]byte, error) {
	uniqueKeyString, err := buildUniqueKeyString(timeGen, uniqueOpts, params)
	if err != nil {
		return nil, err
	}
	uniqueKeyHash := sha256.Sum256([]byte(uniqueKeyString))
	return uniqueKeyHash[:], nil
}

// Builds a unique key made up of the unique options in place. The key is hashed
// to become a value for `unique_key`.
func buildUniqueKeyString(timeGen rivertype.TimeGenerator, uniqueOpts *UniqueOpts, params *rivertype.JobInsertParams) (string, error) {
	var sb strings.Builder

	if !uniqueOpts.ExcludeKind {
		sb.WriteString("&kind=" + params.Kind)
	}

	if uniqueOpts.ByArgs {
		var encodedArgsForUnique []byte
		// Get unique JSON keys from the JobArgs struct:
		uniqueFields, err := getSortedUniqueFieldsCached(params.Args)
		if err != nil {
			return "", err
		}

		if len(uniqueFields) > 0 {
			// Extract unique values from the EncodedArgs JSON
			uniqueValues := extractUniqueValues(params.EncodedArgs, uniqueFields)

			// Assemble the JSON object using bytes.Buffer
			// Better to overallocate a bit than to allocate multiple times, so just
			// assume we'll cap out at the length of the full encoded args.
			sortedJSONWithOnlyUniqueValues := make([]byte, 0, len(params.EncodedArgs))

			sjsonOpts := &sjson.Options{ReplaceInPlace: true}
			for i, key := range uniqueFields {
				if uniqueValues[i] == "undefined" {
					continue
				}
				sortedJSONWithOnlyUniqueValues, err = sjson.SetRawBytesOptions(sortedJSONWithOnlyUniqueValues, key, []byte(uniqueValues[i]), sjsonOpts)
				if err != nil {
					// Should not happen unless key was invalid
					return "", err
				}
			}
			encodedArgsForUnique = sortedJSONWithOnlyUniqueValues
		} else {
			// Use all keys from EncodedArgs sorted alphabetically
			keys := sliceutil.Map(gjson.GetBytes(params.EncodedArgs, "@keys").Array(), func(v gjson.Result) string { return v.String() })
			slices.Sort(keys)

			sortedJSON := make([]byte, 0, len(params.EncodedArgs))
			sortedJSON = append(sortedJSON, "{}"...)
			sjsonOpts := &sjson.Options{ReplaceInPlace: true}
			for _, key := range keys {
				sortedJSON, err = sjson.SetRawBytesOptions(sortedJSON, key, []byte(gjson.GetBytes(params.EncodedArgs, key).Raw), sjsonOpts)
				if err != nil {
					// Should not happen unless key was invalid
					return "", err
				}
			}
			encodedArgsForUnique = sortedJSON
		}

		sb.WriteString("&args=")
		sb.Write(encodedArgsForUnique)
	}

	if uniqueOpts.ByPeriod != time.Duration(0) {
		lowerPeriodBound := ptrutil.ValOrDefaultFunc(params.ScheduledAt, timeGen.NowUTC).Truncate(uniqueOpts.ByPeriod)
		sb.WriteString("&period=" + lowerPeriodBound.Format(time.RFC3339))
	}

	if uniqueOpts.ByQueue {
		sb.WriteString("&queue=" + params.Queue)
	}

	return sb.String(), nil
}

func UniqueStatesToBitmask(states []rivertype.JobState) byte {
	var val byte

	for _, state := range states {
		bitIndex, exists := jobStateBitPositions[state]
		if !exists {
			continue // Ignore unknown states
		}
		bitPosition := 7 - (bitIndex % 8)
		val |= 1 << bitPosition
	}

	return val
}

func UniqueBitmaskToStates(mask byte) []rivertype.JobState {
	var states []rivertype.JobState

	for state, bitIndex := range jobStateBitPositions {
		bitPosition := 7 - (bitIndex % 8)
		if mask&(1<<bitPosition) != 0 {
			states = append(states, state)
		}
	}

	slices.Sort(states)
	return states
}

```

`internal/dbunique/db_unique_test.go`:

```go
package dbunique

import (
	"crypto/sha256"
	"encoding/json"
	"slices"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

type JobArgsStaticKind struct {
	kind string
}

func (a JobArgsStaticKind) Kind() string {
	return a.kind
}

func TestUniqueKey(t *testing.T) {
	t.Parallel()

	// Fixed timestamp for consistency across tests:
	now := time.Now().UTC()
	stubSvc := &riversharedtest.TimeStub{}
	stubSvc.StubNowUTC(now)

	tests := []struct {
		name                   string
		argsFunc               func() rivertype.JobArgs
		modifyInsertParamsFunc func(insertParams *rivertype.JobInsertParams)
		uniqueOpts             UniqueOpts
		expectedJSON           string
	}{
		{
			name: "ByArgsWithMultipleUniqueStructTagsAndDefaultStates",
			argsFunc: func() rivertype.JobArgs {
				type EmailJobArgs struct {
					JobArgsStaticKind
					Recipient   string `json:"recipient"    river:"unique"`
					Subject     string `json:"subject"      river:"unique"`
					Body        string `json:"body"`
					TemplateID  int    `json:"template_id"`
					ScheduledAt string `json:"scheduled_at"`
				}
				return EmailJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_1"},
					Recipient:         "user@example.com",
					Subject:           "Test Email",
					Body:              "This is a test email.",
					TemplateID:        101,
					ScheduledAt:       "2024-09-15T10:00:00Z",
				}
			},
			uniqueOpts:   UniqueOpts{ByArgs: true},
			expectedJSON: `&kind=worker_1&args={"recipient":"user@example.com","subject":"Test Email"}`,
		},
		{
			name: "ByArgsWithUniqueFieldsSomeEmpty",
			argsFunc: func() rivertype.JobArgs {
				type SMSJobArgs struct {
					JobArgsStaticKind
					PhoneNumber string `json:"phone_number"      river:"unique"`
					Message     string `json:"message,omitempty" river:"unique"`
					TemplateID  int    `json:"template_id"`
				}
				return SMSJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_2"},
					PhoneNumber:       "555-5678",
					Message:           "", // Empty unique field, omitted from key
					TemplateID:        202,
				}
			},
			uniqueOpts:   UniqueOpts{ByArgs: true},
			expectedJSON: `&kind=worker_2&args={"phone_number":"555-5678"}`,
		},
		{
			name: "ByArgsUniqueWithNoJSONTagsUsesFieldName",
			argsFunc: func() rivertype.JobArgs {
				type EmailJobArgs struct {
					JobArgsStaticKind
					Recipient  string `river:"unique"`
					Subject    string `river:"unique"`
					TemplateID int
				}
				return EmailJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_1"},
					Recipient:         "john@example.com",
					Subject:           "Another Test Email",
					TemplateID:        102,
				}
			},
			uniqueOpts:   UniqueOpts{ByArgs: true},
			expectedJSON: `&kind=worker_1&args={"Recipient":"john@example.com","Subject":"Another Test Email"}`,
		},
		{
			name: "ByArgsWithPointerToStruct",
			argsFunc: func() rivertype.JobArgs {
				type EmailJobArgs struct {
					JobArgsStaticKind
					Recipient string `json:"recipient" river:"unique"`
					Subject   string `json:"subject"   river:"unique"`
					Body      string `json:"body"`
				}
				return &EmailJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_1"},
					Recipient:         "john@example.com",
					Subject:           "Another Test Email",
					Body:              "This is another test email.",
				}
			},
			uniqueOpts:   UniqueOpts{ByArgs: true},
			expectedJSON: `&kind=worker_1&args={"recipient":"john@example.com","subject":"Another Test Email"}`,
		},
		{
			name: "ByArgsWithNoUniqueFields",
			argsFunc: func() rivertype.JobArgs {
				type GenericJobArgs struct {
					JobArgsStaticKind
					Description string `json:"description"`
					Count       int    `json:"count"`
					foo         string // won't be marshaled in JSON
				}
				return GenericJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_3"},
					Description:       "A generic job without unique fields.",
					Count:             10,
					foo:               "bar",
				}
			},
			uniqueOpts: UniqueOpts{ByArgs: true},
			// args JSON should be sorted alphabetically:
			expectedJSON: `&kind=worker_3&args={"count":10,"description":"A generic job without unique fields."}`,
		},
		{
			name: "ByArgsWithEmptyEncodedArgs",
			argsFunc: func() rivertype.JobArgs {
				type EmailJobArgs struct {
					JobArgsStaticKind
				}

				return EmailJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_1"},
				}
			},
			uniqueOpts: UniqueOpts{ByArgs: true},
			// args JSON should be sorted alphabetically:
			expectedJSON: `&kind=worker_1&args={}`,
		},
		{
			name: "CustomByStateWithPeriod",
			argsFunc: func() rivertype.JobArgs {
				type TaskJobArgs struct {
					JobArgsStaticKind
					TaskID string
				}
				return TaskJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_4"},
					TaskID:            "task_123",
				}
			},
			uniqueOpts:   UniqueOpts{ByPeriod: time.Hour, ByState: []rivertype.JobState{rivertype.JobStateCompleted}},
			expectedJSON: "&kind=worker_4&period=" + now.Truncate(time.Hour).Format(time.RFC3339),
		},
		{
			name: "PeriodFromScheduledAt",
			argsFunc: func() rivertype.JobArgs {
				type TaskJobArgs struct {
					JobArgsStaticKind
				}
				return TaskJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_4"},
				}
			},
			modifyInsertParamsFunc: func(insertParams *rivertype.JobInsertParams) {
				insertParams.ScheduledAt = ptrutil.Ptr(now.Add(time.Hour))
			},
			uniqueOpts:   UniqueOpts{ByPeriod: time.Hour},
			expectedJSON: "&kind=worker_4&period=" + now.Add(time.Hour).Truncate(time.Hour).Format(time.RFC3339),
		},
		{
			name: "ExcludeKindByArgs",
			argsFunc: func() rivertype.JobArgs {
				type TaskJobArgs struct {
					JobArgsStaticKind
					TaskID string `json:"task_id"`
				}
				return TaskJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_5"},
					TaskID:            "task_123",
				}
			},
			uniqueOpts:   UniqueOpts{ByArgs: true, ExcludeKind: true},
			expectedJSON: `&args={"task_id":"task_123"}`,
		},
		{
			name: "ByQueue",
			argsFunc: func() rivertype.JobArgs {
				type TaskJobArgs struct {
					JobArgsStaticKind
					TaskID string `json:"task_id"`
				}
				return TaskJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_6"},
					TaskID:            "task_123",
				}
			},
			uniqueOpts:   UniqueOpts{ByQueue: true},
			expectedJSON: `&kind=worker_6&queue=email_queue`,
		},
		{
			name: "EmptyUniqueOpts",
			argsFunc: func() rivertype.JobArgs {
				type TaskJobArgs struct {
					JobArgsStaticKind
					TaskID string `json:"task_id"`
				}
				return TaskJobArgs{
					JobArgsStaticKind: JobArgsStaticKind{kind: "worker_7"},
					TaskID:            "task_123",
				}
			},
			uniqueOpts:   UniqueOpts{},
			expectedJSON: `&kind=worker_7`,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			args := tt.argsFunc()

			encodedArgs, err := json.Marshal(args)
			require.NoError(t, err)

			states := uniqueOptsByStateDefault
			if len(tt.uniqueOpts.ByState) > 0 {
				states = tt.uniqueOpts.ByState
			}

			insertParams := &rivertype.JobInsertParams{
				Args:         args,
				CreatedAt:    &now,
				EncodedArgs:  encodedArgs,
				Kind:         args.Kind(),
				Metadata:     []byte(`{"source":"api"}`),
				Queue:        "email_queue",
				ScheduledAt:  &now,
				State:        "Pending",
				Tags:         []string{"notification", "email"},
				UniqueStates: UniqueStatesToBitmask(states),
			}

			if tt.modifyInsertParamsFunc != nil {
				tt.modifyInsertParamsFunc(insertParams)
			}

			uniqueKeyPreHash, err := buildUniqueKeyString(stubSvc, &tt.uniqueOpts, insertParams)
			require.NoError(t, err)
			require.Equal(t, tt.expectedJSON, uniqueKeyPreHash)
			expectedHash := sha256.Sum256([]byte(tt.expectedJSON))

			uniqueKey, err := UniqueKey(stubSvc, &tt.uniqueOpts, insertParams)
			require.NoError(t, err)
			require.NotNil(t, uniqueKey)

			require.Equal(t, expectedHash[:], uniqueKey, "UniqueKey hash does not match expected value")
		})
	}
}

func TestDefaultUniqueStatesSorted(t *testing.T) {
	t.Parallel()

	states := slices.Clone(uniqueOptsByStateDefault)
	slices.Sort(states)
	require.Equal(t, states, uniqueOptsByStateDefault, "Default unique states should be sorted")
}

func TestUniqueOptsIsEmpty(t *testing.T) {
	t.Parallel()

	emptyOpts := &UniqueOpts{}
	require.True(t, emptyOpts.IsEmpty(), "Empty unique options should be empty")

	require.False(t, (&UniqueOpts{ByArgs: true}).IsEmpty(), "Unique options with ByArgs should not be empty")
	require.False(t, (&UniqueOpts{ByPeriod: time.Minute}).IsEmpty(), "Unique options with ByPeriod should not be empty")
	require.False(t, (&UniqueOpts{ByQueue: true}).IsEmpty(), "Unique options with ByQueue should not be empty")
	require.False(t, (&UniqueOpts{ByState: []rivertype.JobState{rivertype.JobStateAvailable}}).IsEmpty(), "Unique options with ByState should not be empty")
	require.False(t, (&UniqueOpts{ExcludeKind: true}).IsEmpty(), "Unique options with ExcludeKind should not be empty")

	nonEmptyOpts := &UniqueOpts{
		ByArgs:      true,
		ByPeriod:    time.Minute,
		ByQueue:     true,
		ByState:     []rivertype.JobState{rivertype.JobStateAvailable},
		ExcludeKind: true,
	}
	require.False(t, nonEmptyOpts.IsEmpty(), "Non-empty unique options should not be empty")
}

func TestUniqueOptsStateBitmask(t *testing.T) {
	t.Parallel()

	emptyOpts := &UniqueOpts{}
	require.Equal(t, UniqueStatesToBitmask(uniqueOptsByStateDefault), emptyOpts.StateBitmask(), "Empty unique options should have default bitmask")

	otherStates := []rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted}
	nonEmptyOpts := &UniqueOpts{
		ByState: otherStates,
	}
	require.Equal(t, UniqueStatesToBitmask([]rivertype.JobState{rivertype.JobStateAvailable, rivertype.JobStateCompleted}), nonEmptyOpts.StateBitmask(), "Non-empty unique options should have correct bitmask")
}

func TestUniqueStatesToBitmask(t *testing.T) {
	t.Parallel()

	bitmask := UniqueStatesToBitmask(uniqueOptsByStateDefault)
	require.Equal(t, byte(0b11110101), bitmask, "Default unique states should be all set except cancelled and discarded")

	for state, position := range jobStateBitPositions {
		bitmask = UniqueStatesToBitmask([]rivertype.JobState{state})
		// Bit shifting uses postgres bit numbering with MSB on the right, so we
		// need to flip the position when shifting manually:
		require.Equal(t, byte(1<<(7-position)), bitmask, "Bitmask should be set for single state %s", state)
	}
}

```

`internal/dbunique/main_test.go`:

```go
package dbunique

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`internal/dbunique/unique_fields.go`:

```go
package dbunique

import (
	"fmt"
	"reflect"
	"sort"
	"strings"
	"sync"

	"github.com/tidwall/gjson"

	"github.com/riverqueue/river/rivertype"
)

var (
	// uniqueFieldsCache caches the unique fields for each JobArgs type. These are
	// global to ensure that each struct type's tags are only extracted once.
	uniqueFieldsCache = make(map[reflect.Type][]string) //nolint:gochecknoglobals
	cacheMutex        sync.RWMutex                      //nolint:gochecknoglobals
)

// extractUniqueValues extracts the raw JSON values of the specified keys from the JSON-encoded args.
func extractUniqueValues(encodedArgs []byte, uniqueKeys []string) []string {
	// Use GetManyBytes to retrieve multiple values at once
	results := gjson.GetManyBytes(encodedArgs, uniqueKeys...)

	uniqueValues := make([]string, len(results))
	for i, res := range results {
		if res.Exists() {
			uniqueValues[i] = res.Raw // Use Raw to get the JSON-encoded value
		} else {
			// Handle missing keys as "undefined" (they'll be skipped when building
			// the unique key). We don't want to use "null" here because the JSON may
			// actually contain "null" as a value.
			uniqueValues[i] = "undefined"
		}
	}

	return uniqueValues
}

// getSortedUniqueFields uses reflection to retrieve the JSON keys of fields
// marked with `river:"unique"` among potentially other comma-separated values.
// The return values are the JSON keys using the same logic as the `json` struct tag.
func getSortedUniqueFields(args rivertype.JobArgs) ([]string, error) {
	typ := reflect.TypeOf(args)

	// Handle pointer to struct
	if typ != nil && typ.Kind() == reflect.Ptr {
		typ = typ.Elem()
	}

	// Ensure we're dealing with a struct
	if typ == nil || typ.Kind() != reflect.Struct {
		return nil, fmt.Errorf("expected struct, got %T", args)
	}

	var uniqueFields []string

	// Iterate over all fields
	for i := range typ.NumField() {
		field := typ.Field(i)

		// Check for `river:"unique"` tag, possibly among other comma-separated values
		if riverTag, ok := field.Tag.Lookup("river"); ok {
			// Split riverTag by comma
			tags := strings.Split(riverTag, ",")
			for _, tag := range tags {
				if strings.TrimSpace(tag) == "unique" {
					// Get the corresponding JSON key
					jsonTag := field.Tag.Get("json")
					if jsonTag == "" {
						// If no JSON tag, use the field name as-is
						uniqueFields = append(uniqueFields, field.Name)
					} else {
						// Handle cases like `json:"recipient,omitempty"`
						jsonKey := parseJSONTag(jsonTag)
						uniqueFields = append(uniqueFields, jsonKey)
					}
					break // No need to check other tags once "unique" is found
				}
			}
		}
	}

	// Sort the uniqueFields alphabetically for consistent ordering
	sort.Strings(uniqueFields)

	return uniqueFields, nil
}

// getSortedUniqueFieldsCached retrieves unique fields with caching to avoid
// extracting fields from the same struct type repeatedly.
func getSortedUniqueFieldsCached(args rivertype.JobArgs) ([]string, error) {
	typ := reflect.TypeOf(args)

	// Check cache first
	cacheMutex.RLock()
	if fields, ok := uniqueFieldsCache[typ]; ok {
		cacheMutex.RUnlock()
		return fields, nil
	}
	cacheMutex.RUnlock()

	// Not in cache; retrieve using reflection
	fields, err := getSortedUniqueFields(args)
	if err != nil {
		return nil, err
	}

	// Store in cache
	cacheMutex.Lock()
	uniqueFieldsCache[typ] = fields
	cacheMutex.Unlock()

	return fields, nil
}

// parseJSONTag extracts the JSON key from the struct tag.
// It handles tags with options, e.g., `json:"recipient,omitempty"`.
func parseJSONTag(tag string) string {
	// Tags can be like "recipient,omitempty", so split by comma
	if commaIdx := strings.Index(tag, ","); commaIdx != -1 {
		return tag[:commaIdx]
	}
	return tag
}

```

`internal/execution/execution.go`:

```go
package execution

import (
	"context"
	"time"

	"github.com/riverqueue/river/rivertype"
)

// ContextKeyInsideTestWorker is an internal context key that indicates whether
// the worker is running inside a [rivertest.Worker].
type ContextKeyInsideTestWorker struct{}

type Func func(ctx context.Context) error

// MaybeApplyTimeout returns a context that will be cancelled after the given
// timeout. If the timeout is <= 0, the context will not be timed out, but it
// will still have a cancel function returned. In either case the cancel
// function should be called after execution (in a defer).
func MaybeApplyTimeout(ctx context.Context, timeout time.Duration) (context.Context, context.CancelFunc) {
	// No timeout if a -1 was specified.
	if timeout > 0 {
		return context.WithTimeout(ctx, timeout)
	}

	return context.WithCancel(ctx)
}

// MiddlewareChain chains together the given middleware functions, returning a
// single function that applies them all in reverse order.
func MiddlewareChain(globalMiddleware []rivertype.Middleware, workerMiddleware []rivertype.WorkerMiddleware, doInner Func, jobRow *rivertype.JobRow) Func {
	// Quick return for no middleware, which will often be the case.
	if len(globalMiddleware) < 1 && len(workerMiddleware) < 1 {
		return doInner
	}

	// Wrap middlewares in reverse order so the one defined first is wrapped
	// as the outermost function and is first to receive the operation.
	for i := len(globalMiddleware) - 1; i >= 0; i-- {
		middlewareItem := globalMiddleware[i].(rivertype.WorkerMiddleware) //nolint:forcetypeassert // capture the current middleware item
		previousDoInner := doInner                                         // capture the current doInner function
		doInner = func(ctx context.Context) error {
			return middlewareItem.Work(ctx, jobRow, previousDoInner)
		}
	}

	for i := len(workerMiddleware) - 1; i >= 0; i-- {
		middlewareItem := workerMiddleware[i] // capture the current middleware item
		previousDoInner := doInner            // capture the current doInner function
		doInner = func(ctx context.Context) error {
			return middlewareItem.Work(ctx, jobRow, previousDoInner)
		}
	}

	return doInner
}

```

`internal/hooklookup/hook_lookup.go`:

```go
package hooklookup

import (
	"sync"

	"github.com/riverqueue/river/rivertype"
)

//
// HookKind
//

type HookKind string

const (
	HookKindInsertBegin HookKind = "insert_begin"
	HookKindWorkBegin   HookKind = "work_begin"
)

//
// HookLookupInterface
//

// HookLookupInterface is an interface to look up hooks by hook kind. It's
// commonly implemented by HookLookup, but may also be EmptyHookLookup as a
// memory allocation optimization for bundles where no hooks are present.
type HookLookupInterface interface {
	ByHookKind(kind HookKind) []rivertype.Hook
}

// NewHookLookup returns a new hook lookup interface based on the given hooks
// that satisfies HookLookupInterface. This is often hookLookup, but may be
// emptyHookLookup as an optimization for the common case of an empty hook
// bundle.
func NewHookLookup(hooks []rivertype.Hook) HookLookupInterface {
	if len(hooks) < 1 {
		return &emptyHookLookup{}
	}

	return &hookLookup{
		hooks:       hooks,
		hooksByKind: make(map[HookKind][]rivertype.Hook),
		mu:          &sync.RWMutex{},
	}
}

//
// hookLookup
//

// hookLookup looks up and caches hooks based on a HookKind, saving work when
// looking up hooks for specific operations, a common operation that gets
// repeated over and over again. This struct may be used as a lookup for
// globally installed hooks or hooks for specific job kinds through the use of
// JobHookLookup.
type hookLookup struct {
	hooks       []rivertype.Hook
	hooksByKind map[HookKind][]rivertype.Hook
	mu          *sync.RWMutex
}

func (c *hookLookup) ByHookKind(kind HookKind) []rivertype.Hook {
	c.mu.RLock()
	cache, ok := c.hooksByKind[kind]
	c.mu.RUnlock()
	if ok {
		return cache
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	// Even if this ends up being empty, make sure there's an entry for the next
	// time the cache gets invoked for this kind.
	c.hooksByKind[kind] = nil

	// Rely on exhaustlint to find any missing hook kinds here.
	switch kind {
	case HookKindInsertBegin:
		for _, hook := range c.hooks {
			if typedHook, ok := hook.(rivertype.HookInsertBegin); ok {
				c.hooksByKind[kind] = append(c.hooksByKind[kind], typedHook)
			}
		}
	case HookKindWorkBegin:
		for _, hook := range c.hooks {
			if typedHook, ok := hook.(rivertype.HookWorkBegin); ok {
				c.hooksByKind[kind] = append(c.hooksByKind[kind], typedHook)
			}
		}
	}

	return c.hooksByKind[kind]
}

//
// emptyHookLookup
//

// emptyHookLookup is an empty version of HookLookup that's zero allocation. For
// most applications, most job args won't have hooks, so this prevents us from
// allocating dozens/hundreds of small HookLookup objects that go unused.
type emptyHookLookup struct{}

func (c *emptyHookLookup) ByHookKind(kind HookKind) []rivertype.Hook { return nil }

//
// JobHookLookup
//

type JobHookLookup struct {
	hookLookupByKind map[string]HookLookupInterface
	mu               sync.RWMutex
}

func NewJobHookLookup() *JobHookLookup {
	return &JobHookLookup{
		hookLookupByKind: make(map[string]HookLookupInterface),
	}
}

// ByJobArgs returns a HookLookupInterface by job args, which is a HookLookup if
// the job args had specific hooks (i.e. implements JobArgsWithHooks and returns
// a non-empty set of hooks), or an EmptyHashLookup otherwise.
func (c *JobHookLookup) ByJobArgs(args rivertype.JobArgs) HookLookupInterface {
	kind := args.Kind()

	c.mu.RLock()
	lookup, ok := c.hookLookupByKind[kind]
	c.mu.RUnlock()
	if ok {
		return lookup
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	var hooks []rivertype.Hook
	if argsWithHooks, ok := args.(jobArgsWithHooks); ok {
		hooks = argsWithHooks.Hooks()
	}

	lookup = NewHookLookup(hooks)
	c.hookLookupByKind[kind] = lookup
	return lookup
}

// Same as river.JobArgsWithHooks, but duplicated here so that can still live in
// the top level package.
type jobArgsWithHooks interface {
	Hooks() []rivertype.Hook
}

```

`internal/hooklookup/hook_lookup_test.go`:

```go
package hooklookup

import (
	"context"
	"sync"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivertype"
)

func TestHookLookup(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (*hookLookup, *testBundle) { //nolint:unparam
		t.Helper()

		return NewHookLookup([]rivertype.Hook{ //nolint:forcetypeassert
			&testHookInsertAndWorkBegin{},
			&testHookInsertBegin{},
			&testHookWorkBegin{},
		}).(*hookLookup), &testBundle{}
	}

	t.Run("LooksUpHooks", func(t *testing.T) {
		t.Parallel()

		hookLookup, _ := setup(t)

		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookInsertBegin{},
		}, hookLookup.ByHookKind(HookKindInsertBegin))
		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookWorkBegin{},
		}, hookLookup.ByHookKind(HookKindWorkBegin))

		require.Len(t, hookLookup.hooksByKind, 2)

		// Repeat lookups to make sure we get the same result.
		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookInsertBegin{},
		}, hookLookup.ByHookKind(HookKindInsertBegin))
		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookWorkBegin{},
		}, hookLookup.ByHookKind(HookKindWorkBegin))
	})

	t.Run("Stress", func(t *testing.T) {
		t.Parallel()

		hookLookup, _ := setup(t)

		var wg sync.WaitGroup

		parallelLookupLoop := func(kind HookKind) {
			wg.Add(1)
			go func() {
				defer wg.Done()

				for range 50 {
					hookLookup.ByHookKind(kind)
				}
			}()
		}

		parallelLookupLoop(HookKindInsertBegin)
		parallelLookupLoop(HookKindWorkBegin)
		parallelLookupLoop(HookKindInsertBegin)
		parallelLookupLoop(HookKindWorkBegin)

		wg.Wait()
	})
}

func TestEmptyHookLookup(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (*emptyHookLookup, *testBundle) {
		t.Helper()

		return NewHookLookup(nil).(*emptyHookLookup), &testBundle{} //nolint:forcetypeassert
	}

	t.Run("AlwaysReturnsNil", func(t *testing.T) {
		t.Parallel()

		hookLookup, _ := setup(t)

		require.Nil(t, hookLookup.ByHookKind(HookKindInsertBegin))
		require.Nil(t, hookLookup.ByHookKind(HookKindWorkBegin))
	})
}

func TestJobHookLookup(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (*JobHookLookup, *testBundle) { //nolint:unparam
		t.Helper()

		return NewJobHookLookup(), &testBundle{}
	}

	t.Run("LooksUpHooks", func(t *testing.T) {
		t.Parallel()

		jobHookLookup, _ := setup(t)

		require.Nil(t, jobHookLookup.ByJobArgs(&jobArgsNoHooks{}).ByHookKind(HookKindInsertBegin))
		require.Nil(t, jobHookLookup.ByJobArgs(&jobArgsNoHooks{}).ByHookKind(HookKindWorkBegin))
		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookInsertBegin{},
		}, jobHookLookup.ByJobArgs(&jobArgsWithCustomHooks{}).ByHookKind(HookKindInsertBegin))
		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookWorkBegin{},
		}, jobHookLookup.ByJobArgs(&jobArgsWithCustomHooks{}).ByHookKind(HookKindWorkBegin))

		require.Len(t, jobHookLookup.hookLookupByKind, 2)

		// Repeat lookups to make sure we get the same result.
		require.Nil(t, jobHookLookup.ByJobArgs(&jobArgsNoHooks{}).ByHookKind(HookKindInsertBegin))
		require.Nil(t, jobHookLookup.ByJobArgs(&jobArgsNoHooks{}).ByHookKind(HookKindWorkBegin))
		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookInsertBegin{},
		}, jobHookLookup.ByJobArgs(&jobArgsWithCustomHooks{}).ByHookKind(HookKindInsertBegin))
		require.Equal(t, []rivertype.Hook{
			&testHookInsertAndWorkBegin{},
			&testHookWorkBegin{},
		}, jobHookLookup.ByJobArgs(&jobArgsWithCustomHooks{}).ByHookKind(HookKindWorkBegin))
	})

	t.Run("Stress", func(t *testing.T) {
		t.Parallel()

		jobHookLookup, _ := setup(t)

		var wg sync.WaitGroup

		parallelLookupLoop := func(args rivertype.JobArgs) {
			wg.Add(1)
			go func() {
				defer wg.Done()

				for range 50 {
					jobHookLookup.ByJobArgs(args)
				}
			}()
		}

		parallelLookupLoop(&jobArgsNoHooks{})
		parallelLookupLoop(&jobArgsWithCustomHooks{})
		parallelLookupLoop(&jobArgsNoHooks{})
		parallelLookupLoop(&jobArgsWithCustomHooks{})

		wg.Wait()
	})
}

//
// jobArgsNoHooks
//

var _ rivertype.JobArgs = &jobArgsNoHooks{}

type jobArgsNoHooks struct{}

func (jobArgsNoHooks) Kind() string { return "no_hooks" }

//
// jobArgsWithHooks
//

var (
	_ rivertype.JobArgs = &jobArgsWithCustomHooks{}
	_ jobArgsWithHooks  = &jobArgsWithCustomHooks{}
)

type jobArgsWithCustomHooks struct{}

func (jobArgsWithCustomHooks) Hooks() []rivertype.Hook {
	return []rivertype.Hook{
		&testHookInsertAndWorkBegin{},
		&testHookInsertBegin{},
		&testHookWorkBegin{},
	}
}

func (jobArgsWithCustomHooks) Kind() string { return "with_custom_hooks" }

//
// testHookInsertAndWorkBegin
//

var (
	_ rivertype.HookInsertBegin = &testHookInsertAndWorkBegin{}
	_ rivertype.HookWorkBegin   = &testHookInsertAndWorkBegin{}
)

type testHookInsertAndWorkBegin struct{ rivertype.Hook }

func (t *testHookInsertAndWorkBegin) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	return nil
}

func (t *testHookInsertAndWorkBegin) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	return nil
}

//
// testHookInsertBegin
//

var _ rivertype.HookInsertBegin = &testHookInsertBegin{}

type testHookInsertBegin struct{ rivertype.Hook }

func (t *testHookInsertBegin) InsertBegin(ctx context.Context, params *rivertype.JobInsertParams) error {
	return nil
}

//
// testHookWorkBegin
//

var _ rivertype.HookWorkBegin = &testHookWorkBegin{}

type testHookWorkBegin struct{ rivertype.Hook }

func (t *testHookWorkBegin) WorkBegin(ctx context.Context, job *rivertype.JobRow) error {
	return nil
}

```

`internal/jobcompleter/job_completer.go`:

```go
package jobcompleter

import (
	"context"
	"errors"
	"log/slog"
	"sync"
	"time"

	"golang.org/x/sync/errgroup"

	"github.com/riverqueue/river/internal/jobstats"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riverpilot"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

// JobCompleter is an interface to a service that "completes" jobs by marking
// them with an appropriate state and any other necessary metadata in the
// database. It's a generic interface to let us experiment with the speed of a
// number of implementations, although River will likely always prefer our most
// optimized one.
type JobCompleter interface {
	startstop.Service

	// JobSetState sets a new state for the given job, as long as it's
	// still running (i.e. its state has not changed to something else already).
	JobSetStateIfRunning(ctx context.Context, stats *jobstats.JobStatistics, params *riverdriver.JobSetStateIfRunningParams) error

	// ResetSubscribeChan resets the subscription channel for the completer. It
	// must only be called when the completer is stopped.
	ResetSubscribeChan(subscribeCh SubscribeChan)
}

type SubscribeChan chan<- []CompleterJobUpdated

// SubscribeFunc will be invoked whenever a job is updated.
type SubscribeFunc func(update CompleterJobUpdated)

type CompleterJobUpdated struct {
	Job      *rivertype.JobRow
	JobStats *jobstats.JobStatistics
}

type InlineCompleter struct {
	baseservice.BaseService
	startstop.BaseStartStop

	disableSleep bool // disable sleep in testing
	exec         riverdriver.Executor
	pilot        riverpilot.Pilot
	subscribeCh  SubscribeChan

	// A waitgroup is not actually needed for the inline completer because as
	// long as the caller is waiting on each function call, completion is
	// guaranteed to be done by the time Wait is called. However, we use a
	// generic test helper for all completers that starts goroutines, so this
	// left in for now for the benefit of the test suite.
	wg sync.WaitGroup
}

func NewInlineCompleter(archetype *baseservice.Archetype, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh SubscribeChan) *InlineCompleter {
	return baseservice.Init(archetype, &InlineCompleter{
		exec:        exec,
		pilot:       pilot,
		subscribeCh: subscribeCh,
	})
}

func (c *InlineCompleter) JobSetStateIfRunning(ctx context.Context, stats *jobstats.JobStatistics, params *riverdriver.JobSetStateIfRunningParams) error {
	c.wg.Add(1)
	defer c.wg.Done()

	start := c.Time.NowUTC()

	jobs, err := withRetries(ctx, &c.BaseService, c.disableSleep, func(ctx context.Context) ([]*rivertype.JobRow, error) {
		execTx, err := c.exec.Begin(ctx)
		if err != nil {
			return nil, err
		}
		defer execTx.Rollback(ctx)

		jobs, err := c.pilot.JobSetStateIfRunningMany(ctx, execTx, setStateParamsToMany(params))
		if err != nil {
			return nil, err
		}

		if err := execTx.Commit(ctx); err != nil {
			return nil, err
		}
		return jobs, nil
	})
	if err != nil {
		return err
	}

	stats.CompleteDuration = c.Time.NowUTC().Sub(start)
	c.subscribeCh <- []CompleterJobUpdated{{Job: jobs[0], JobStats: stats}}

	return nil
}

func (c *InlineCompleter) ResetSubscribeChan(subscribeCh SubscribeChan) {
	c.subscribeCh = subscribeCh
}

func (c *InlineCompleter) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := c.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	if c.subscribeCh == nil {
		panic("subscribeCh must be non-nil")
	}

	go func() {
		started()
		defer stopped()
		defer close(c.subscribeCh)

		<-ctx.Done()

		c.wg.Wait()
	}()

	return nil
}

func setStateParamsToMany(params *riverdriver.JobSetStateIfRunningParams) *riverdriver.JobSetStateIfRunningManyParams {
	return &riverdriver.JobSetStateIfRunningManyParams{
		Attempt:         []*int{params.Attempt},
		ErrData:         [][]byte{params.ErrData},
		FinalizedAt:     []*time.Time{params.FinalizedAt},
		ID:              []int64{params.ID},
		MetadataDoMerge: []bool{params.MetadataDoMerge},
		MetadataUpdates: [][]byte{params.MetadataUpdates},
		ScheduledAt:     []*time.Time{params.ScheduledAt},
		State:           []rivertype.JobState{params.State},
	}
}

// A default concurrency of 100 seems to perform better a much smaller number
// like 10, but it's quite dependent on environment (10 and 100 bench almost
// identically on MBA when it's on battery power). This number should represent
// our best known default for most use cases, but don't consider its choice to
// be particularly well informed at this point.
const asyncCompleterDefaultConcurrency = 100

type AsyncCompleter struct {
	baseservice.BaseService
	startstop.BaseStartStop

	concurrency  int
	disableSleep bool // disable sleep in testing
	errGroup     *errgroup.Group
	exec         riverdriver.Executor
	pilot        riverpilot.Pilot
	subscribeCh  SubscribeChan
}

func NewAsyncCompleter(archetype *baseservice.Archetype, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh SubscribeChan) *AsyncCompleter {
	return newAsyncCompleterWithConcurrency(archetype, exec, pilot, asyncCompleterDefaultConcurrency, subscribeCh)
}

func newAsyncCompleterWithConcurrency(archetype *baseservice.Archetype, exec riverdriver.Executor, pilot riverpilot.Pilot, concurrency int, subscribeCh SubscribeChan) *AsyncCompleter {
	errGroup := &errgroup.Group{}
	errGroup.SetLimit(concurrency)

	return baseservice.Init(archetype, &AsyncCompleter{
		concurrency: concurrency,
		errGroup:    errGroup,
		exec:        exec,
		pilot:       pilot,
		subscribeCh: subscribeCh,
	})
}

func (c *AsyncCompleter) JobSetStateIfRunning(ctx context.Context, stats *jobstats.JobStatistics, params *riverdriver.JobSetStateIfRunningParams) error {
	// Start clock outside of goroutine so that the time spent blocking waiting
	// for an errgroup slot is accurately measured.
	start := c.Time.NowUTC()

	c.errGroup.Go(func() error {
		jobs, err := withRetries(ctx, &c.BaseService, c.disableSleep, func(ctx context.Context) ([]*rivertype.JobRow, error) {
			execTx, err := c.exec.Begin(ctx)
			if err != nil {
				return nil, err
			}
			defer execTx.Rollback(ctx)

			rows, err := c.pilot.JobSetStateIfRunningMany(ctx, execTx, setStateParamsToMany(params))
			if err != nil {
				return nil, err
			}

			if err := execTx.Commit(ctx); err != nil {
				return nil, err
			}
			return rows, nil
		})
		if err != nil {
			return err
		}

		stats.CompleteDuration = c.Time.NowUTC().Sub(start)
		c.subscribeCh <- []CompleterJobUpdated{{Job: jobs[0], JobStats: stats}}

		return nil
	})
	return nil
}

func (c *AsyncCompleter) ResetSubscribeChan(subscribeCh SubscribeChan) {
	c.subscribeCh = subscribeCh
}

func (c *AsyncCompleter) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := c.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	if c.subscribeCh == nil {
		panic("subscribeCh must be non-nil")
	}

	go func() {
		started()
		defer stopped() // this defer should come first so it's first out
		defer close(c.subscribeCh)

		<-ctx.Done()

		if err := c.errGroup.Wait(); err != nil {
			c.Logger.Error("Error waiting on async completer", "err", err)
		}
	}()

	return nil
}

type batchCompleterSetState struct {
	Params *riverdriver.JobSetStateIfRunningParams
	Stats  *jobstats.JobStatistics
}

// BatchCompleter accumulates incoming completions, and instead of completing
// them immediately, every so often complete many of them as a single efficient
// batch. To minimize the amount of driver surface area we need, the batching is
// only performed for jobs being changed to a `completed` state, which we expect
// to the vast common case under normal operation. The completer embeds an
// AsyncCompleter to perform other non-`completed` state completions.
type BatchCompleter struct {
	baseservice.BaseService
	startstop.BaseStartStop

	completionMaxSize    int  // configurable for testing purposes; max jobs to complete in single database operation
	disableSleep         bool // disable sleep in testing
	maxBacklog           int  // configurable for testing purposes; max backlog allowed before no more completions accepted
	exec                 riverdriver.Executor
	pilot                riverpilot.Pilot
	setStateParams       map[int64]*batchCompleterSetState
	setStateParamsMu     sync.RWMutex
	setStateStartTimes   map[int64]time.Time
	subscribeCh          SubscribeChan
	waitOnBacklogChan    chan struct{}
	waitOnBacklogWaiting bool
}

func NewBatchCompleter(archetype *baseservice.Archetype, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh SubscribeChan) *BatchCompleter {
	const (
		completionMaxSize = 5_000
		maxBacklog        = 20_000
	)

	return baseservice.Init(archetype, &BatchCompleter{
		completionMaxSize:  completionMaxSize,
		exec:               exec,
		maxBacklog:         maxBacklog,
		pilot:              pilot,
		setStateParams:     make(map[int64]*batchCompleterSetState),
		setStateStartTimes: make(map[int64]time.Time),
		subscribeCh:        subscribeCh,
	})
}

func (c *BatchCompleter) ResetSubscribeChan(subscribeCh SubscribeChan) {
	c.subscribeCh = subscribeCh
}

func (c *BatchCompleter) Start(ctx context.Context) error {
	stopCtx, shouldStart, started, stopped := c.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	if c.subscribeCh == nil {
		panic("subscribeCh must be non-nil")
	}

	go func() {
		started()
		defer stopped() // this defer should come first so it's first out
		defer close(c.subscribeCh)

		c.Logger.DebugContext(ctx, c.Name+": Run loop started")
		defer c.Logger.DebugContext(ctx, c.Name+": Run loop stopped")

		ticker := time.NewTicker(50 * time.Millisecond)
		defer ticker.Stop()

		backlogSize := func() int {
			c.setStateParamsMu.RLock()
			defer c.setStateParamsMu.RUnlock()
			return len(c.setStateParams)
		}

		for numTicks := 0; ; numTicks++ {
			select {
			case <-stopCtx.Done():
				// Try to insert last batch before leaving. Note we use the
				// original context so operations aren't immediately cancelled.
				if err := c.handleBatch(ctx); err != nil {
					c.Logger.Error(c.Name+": Error completing batch", "err", err)
				}
				return

			case <-ticker.C:
			}

			// The ticker fires quite often to make sure that given a huge glut
			// of jobs, we don't accidentally build up too much of a backlog by
			// waiting too long. However, don't start a complete operation until
			// we reach a minimum threshold unless we're on a tick that's a
			// multiple of 5. So, jobs will be completed every 250ms even if the
			// threshold hasn't been met.
			const batchCompleterStartThreshold = 100
			if backlogSize() < min(c.maxBacklog, batchCompleterStartThreshold) && numTicks != 0 && numTicks%5 != 0 {
				continue
			}

			for {
				if err := c.handleBatch(ctx); err != nil {
					c.Logger.Error(c.Name+": Error completing batch", "err", err)
				}

				// New jobs to complete may have come in while working the batch
				// above. If enough have to bring us above the minimum complete
				// threshold, loop again and do another batch. Otherwise, break
				// and listen for a new tick.
				if backlogSize() < batchCompleterStartThreshold {
					break
				}
			}
		}
	}()

	return nil
}

func (c *BatchCompleter) handleBatch(ctx context.Context) error {
	var (
		setStateBatch      map[int64]*batchCompleterSetState
		setStateStartTimes map[int64]time.Time
	)
	func() {
		c.setStateParamsMu.Lock()
		defer c.setStateParamsMu.Unlock()

		setStateBatch = c.setStateParams
		setStateStartTimes = c.setStateStartTimes

		// Don't bother resetting the map if there's nothing to process,
		// allowing the completer to idle efficiently.
		if len(setStateBatch) > 0 {
			c.setStateParams = make(map[int64]*batchCompleterSetState)
			c.setStateStartTimes = make(map[int64]time.Time)
		} else {
			// Set nil to avoid a data race below in case the map is set as a
			// new job comes in.
			setStateBatch = nil
		}
	}()

	if len(setStateBatch) < 1 {
		return nil
	}

	// Complete a sub-batch with retries. Also helps reduce visual noise and
	// increase readability of loop below.
	completeSubBatch := func(batchParams *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
		start := time.Now()
		defer func() {
			c.Logger.DebugContext(ctx, c.Name+": Completed sub-batch of job(s)", "duration", time.Since(start), "num_jobs", len(batchParams.ID))
		}()

		return withRetries(ctx, &c.BaseService, c.disableSleep, func(ctx context.Context) ([]*rivertype.JobRow, error) {
			tx, err := c.exec.Begin(ctx)
			if err != nil {
				return nil, err
			}
			defer tx.Rollback(ctx)

			rows, err := c.pilot.JobSetStateIfRunningMany(ctx, tx, batchParams)
			if err != nil {
				return nil, err
			}
			if err := tx.Commit(ctx); err != nil {
				return nil, err
			}

			return rows, nil
		})
	}

	// This could be written more simply using multiple `sliceutil.Map`s, but
	// it's done this way to allocate as few new slices as necessary.
	mapBatch := func(setStateBatch map[int64]*batchCompleterSetState) *riverdriver.JobSetStateIfRunningManyParams {
		params := &riverdriver.JobSetStateIfRunningManyParams{
			ID:              make([]int64, len(setStateBatch)),
			Attempt:         make([]*int, len(setStateBatch)),
			ErrData:         make([][]byte, len(setStateBatch)),
			FinalizedAt:     make([]*time.Time, len(setStateBatch)),
			MetadataDoMerge: make([]bool, len(setStateBatch)),
			MetadataUpdates: make([][]byte, len(setStateBatch)),
			ScheduledAt:     make([]*time.Time, len(setStateBatch)),
			State:           make([]rivertype.JobState, len(setStateBatch)),
		}
		var i int
		for _, setState := range setStateBatch {
			params.ID[i] = setState.Params.ID
			params.Attempt[i] = setState.Params.Attempt
			params.ErrData[i] = setState.Params.ErrData
			params.FinalizedAt[i] = setState.Params.FinalizedAt
			params.MetadataDoMerge[i] = setState.Params.MetadataDoMerge
			params.MetadataUpdates[i] = setState.Params.MetadataUpdates
			params.ScheduledAt[i] = setState.Params.ScheduledAt
			params.State[i] = setState.Params.State
			i++
		}
		return params
	}

	// Tease apart enormous batches into sub-batches.
	//
	// All the code below is concerned with doing that, with a fast loop that
	// doesn't allocate any additional memory in case the entire batch is
	// smaller than the sub-batch maximum size (which will be the common case).
	var (
		params  = mapBatch(setStateBatch)
		jobRows []*rivertype.JobRow
	)
	c.Logger.DebugContext(ctx, c.Name+": Completing batch of job(s)", "num_jobs", len(setStateBatch))
	if len(setStateBatch) > c.completionMaxSize {
		jobRows = make([]*rivertype.JobRow, 0, len(setStateBatch))
		for i := 0; i < len(setStateBatch); i += c.completionMaxSize {
			endIndex := min(i+c.completionMaxSize, len(params.ID)) // beginning of next sub-batch or end of slice
			subBatch := &riverdriver.JobSetStateIfRunningManyParams{
				ID:              params.ID[i:endIndex],
				Attempt:         params.Attempt[i:endIndex],
				ErrData:         params.ErrData[i:endIndex],
				FinalizedAt:     params.FinalizedAt[i:endIndex],
				MetadataDoMerge: params.MetadataDoMerge[i:endIndex],
				MetadataUpdates: params.MetadataUpdates[i:endIndex],
				ScheduledAt:     params.ScheduledAt[i:endIndex],
				State:           params.State[i:endIndex],
			}
			jobRowsSubBatch, err := completeSubBatch(subBatch)
			if err != nil {
				return err
			}
			jobRows = append(jobRows, jobRowsSubBatch...)
		}
	} else {
		var err error
		jobRows, err = completeSubBatch(params)
		if err != nil {
			return err
		}
	}

	events := sliceutil.Map(jobRows, func(jobRow *rivertype.JobRow) CompleterJobUpdated {
		setState := setStateBatch[jobRow.ID]
		startTime := setStateStartTimes[jobRow.ID]
		setState.Stats.CompleteDuration = c.Time.NowUTC().Sub(startTime)
		return CompleterJobUpdated{Job: jobRow, JobStats: setState.Stats}
	})

	c.subscribeCh <- events

	func() {
		c.setStateParamsMu.Lock()
		defer c.setStateParamsMu.Unlock()

		if c.waitOnBacklogWaiting && len(c.setStateParams) < c.maxBacklog {
			c.Logger.DebugContext(ctx, c.Name+": Disabling waitOnBacklog; ready to complete more jobs")
			close(c.waitOnBacklogChan)
			c.waitOnBacklogWaiting = false
		}
	}()

	return nil
}

func (c *BatchCompleter) JobSetStateIfRunning(ctx context.Context, stats *jobstats.JobStatistics, params *riverdriver.JobSetStateIfRunningParams) error {
	now := c.Time.NowUTC()
	// If we've built up too much of a backlog because the completer's fallen
	// behind, block completions until the complete loop's had a chance to catch
	// up.
	c.waitOrInitBacklogChannel(ctx)

	c.setStateParamsMu.Lock()
	defer c.setStateParamsMu.Unlock()

	c.setStateParams[params.ID] = &batchCompleterSetState{params, stats}
	c.setStateStartTimes[params.ID] = now

	return nil
}

func (c *BatchCompleter) waitOrInitBacklogChannel(ctx context.Context) {
	c.setStateParamsMu.RLock()
	var (
		backlogSize = len(c.setStateParams)
		waitChan    = c.waitOnBacklogChan
		waiting     = c.waitOnBacklogWaiting
	)
	c.setStateParamsMu.RUnlock()

	if waiting {
		<-waitChan
		return
	}

	// Not at max backlog. A little raciness is allowed here: multiple
	// goroutines may have acquired the read lock above and seen a size under
	// limit, but with all allowed to continue it could put the backlog over its
	// maximum. The backlog will only be nominally over because generally max
	// backlog >> max workers, so consider this okay.
	if backlogSize < c.maxBacklog {
		return
	}

	c.setStateParamsMu.Lock()
	defer c.setStateParamsMu.Unlock()

	// Check once more if another process has already started waiting (it's
	// possible for multiple to race between the acquiring the lock above). If
	// so, we fall through and allow this insertion to happen, even though it
	// might bring the batch slightly over limit, because arranging the locks
	// otherwise would get complicated.
	if c.waitOnBacklogWaiting {
		return
	}

	// Tell all future insertions to start waiting. This one is allowed to fall
	// through and succeed even though it may bring the batch a little over
	// limit.
	c.waitOnBacklogChan = make(chan struct{})
	c.waitOnBacklogWaiting = true
	c.Logger.WarnContext(ctx, c.Name+": Hit maximum backlog; completions will wait until below threshold", "max_backlog", c.maxBacklog)
}

// As configured, total time asleep from initial attempt is ~7 seconds (1 + 2 +
// 4) (not including jitter). However, if each attempt times out, that's up to
// ~37 seconds (7 seconds + 3 * 10 seconds).
const numRetries = 3

func withRetries[T any](logCtx context.Context, baseService *baseservice.BaseService, disableSleep bool, retryFunc func(ctx context.Context) (T, error)) (T, error) {
	uncancelledCtx := context.WithoutCancel(logCtx)

	var (
		defaultVal T
		lastErr    error
	)

	for attempt := 1; attempt <= numRetries; attempt++ {
		const timeout = 10 * time.Second

		// I've found that we want at least ten seconds for a large batch,
		// although it usually doesn't need that long.
		ctx, cancel := context.WithTimeout(uncancelledCtx, timeout)
		defer cancel()

		retVal, err := retryFunc(ctx)
		if err != nil {
			// A cancelled context will never succeed, return immediately.
			if errors.Is(err, context.Canceled) {
				return defaultVal, err
			}

			// A closed pool will never succeed, return immediately.
			if errors.Is(err, riverdriver.ErrClosedPool) {
				return defaultVal, err
			}

			lastErr = err
			sleepDuration := serviceutil.ExponentialBackoff(attempt, serviceutil.MaxAttemptsBeforeResetDefault)
			baseService.Logger.ErrorContext(logCtx, baseService.Name+": Completer error (will retry after sleep)",
				slog.Int("attempt", attempt),
				slog.String("err", err.Error()),
				slog.String("sleep_duration", sleepDuration.String()),
				slog.String("timeout", timeout.String()),
			)
			if !disableSleep {
				serviceutil.CancellableSleep(logCtx, sleepDuration)
			}
			continue
		}

		return retVal, nil
	}

	baseService.Logger.ErrorContext(logCtx, baseService.Name+": Too many errors; giving up")

	return defaultVal, lastErr
}

```

`internal/jobcompleter/job_completer_test.go`:

```go
package jobcompleter

import (
	"context"
	"errors"
	"fmt"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/jackc/puddle/v2"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/jobstats"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riverpilot"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

type partialExecutorMock struct {
	riverdriver.Executor
	JobSetStateIfRunningManyCalled bool
	JobSetStateIfRunningManyFunc   func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error)
	mu                             sync.Mutex
}

// NewPartialExecutorMock returns a new mock with all mock functions set to call
// down into the given real executor.
func NewPartialExecutorMock(exec riverdriver.Executor) *partialExecutorMock {
	return &partialExecutorMock{
		Executor:                     exec,
		JobSetStateIfRunningManyFunc: exec.JobSetStateIfRunningMany,
	}
}

func (m *partialExecutorMock) Begin(ctx context.Context) (riverdriver.ExecutorTx, error) {
	tx, err := m.Executor.Begin(ctx)
	if err != nil {
		return nil, err
	}
	return &partialExecutorTxMock{ExecutorTx: tx, partial: m}, nil
}

func (m *partialExecutorMock) JobSetStateIfRunningMany(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
	m.setCalled(func() { m.JobSetStateIfRunningManyCalled = true })
	return m.JobSetStateIfRunningManyFunc(ctx, params)
}

func (m *partialExecutorMock) setCalled(setCalledFunc func()) {
	m.mu.Lock()
	defer m.mu.Unlock()
	setCalledFunc()
}

type partialExecutorTxMock struct {
	riverdriver.ExecutorTx
	partial *partialExecutorMock
}

func (m *partialExecutorTxMock) JobSetStateIfRunningMany(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
	return m.partial.JobSetStateIfRunningMany(ctx, params)
}

func TestInlineJobCompleter_Complete(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	var (
		tx       = riverinternaltest.TestTx(ctx, t)
		driver   = riverpgxv5.New(nil)
		exec     = driver.UnwrapExecutor(tx)
		execMock = NewPartialExecutorMock(exec)
	)

	var attempt int
	expectedErr := errors.New("an error from the completer")

	execMock.JobSetStateIfRunningManyFunc = func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
		require.Len(t, params.ID, 1)
		require.Equal(t, int64(1), params.ID[0])
		attempt++
		return nil, expectedErr
	}

	subscribeCh := make(chan []CompleterJobUpdated, 10)
	t.Cleanup(riverinternaltest.DiscardContinuously(subscribeCh))

	completer := NewInlineCompleter(riversharedtest.BaseServiceArchetype(t), execMock, &riverpilot.StandardPilot{}, subscribeCh)
	t.Cleanup(completer.Stop)
	completer.disableSleep = true

	err := completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(1, time.Now(), nil))
	if !errors.Is(err, expectedErr) {
		t.Errorf("expected %v, got %v", expectedErr, err)
	}

	require.True(t, execMock.JobSetStateIfRunningManyCalled)
	require.Equal(t, numRetries, attempt)
}

func TestInlineJobCompleter_Subscribe(t *testing.T) {
	t.Parallel()

	testCompleterSubscribe(t, func(exec riverdriver.Executor, subscribeCh SubscribeChan) JobCompleter {
		return NewInlineCompleter(riversharedtest.BaseServiceArchetype(t), exec, &riverpilot.StandardPilot{}, subscribeCh)
	})
}

func TestInlineJobCompleter_Wait(t *testing.T) {
	t.Parallel()

	testCompleterWait(t, func(exec riverdriver.Executor, subscribeChan SubscribeChan) JobCompleter {
		return NewInlineCompleter(riversharedtest.BaseServiceArchetype(t), exec, &riverpilot.StandardPilot{}, subscribeChan)
	})
}

func TestAsyncJobCompleter_Complete(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type jobInput struct {
		// TODO: Try to get rid of containing the context in struct. It'd be
		// better to pass it forward instead.
		ctx   context.Context //nolint:containedctx
		jobID int64
	}
	inputCh := make(chan jobInput)
	resultCh := make(chan error)

	expectedErr := errors.New("an error from the completer")

	go func() {
		riversharedtest.WaitOrTimeout(t, inputCh)
		resultCh <- expectedErr
	}()

	var (
		db       = riverinternaltest.TestDB(ctx, t)
		driver   = riverpgxv5.New(db)
		execMock = NewPartialExecutorMock(driver.GetExecutor())
	)

	execMock.JobSetStateIfRunningManyFunc = func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
		require.Len(t, params.ID, 1)
		inputCh <- jobInput{ctx: ctx, jobID: params.ID[0]}
		err := <-resultCh
		if err != nil {
			return nil, err
		}
		return []*rivertype.JobRow{{ID: params.ID[0], State: params.State[0]}}, nil
	}
	subscribeChan := make(chan []CompleterJobUpdated, 10)
	completer := newAsyncCompleterWithConcurrency(riversharedtest.BaseServiceArchetype(t), execMock, &riverpilot.StandardPilot{}, 2, subscribeChan)
	completer.disableSleep = true
	require.NoError(t, completer.Start(ctx))
	t.Cleanup(completer.Stop)

	// launch 4 completions, only 2 can be inline due to the concurrency limit:
	for i := range int64(2) {
		if err := completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(i, time.Now(), nil)); err != nil {
			t.Errorf("expected nil err, got %v", err)
		}
	}
	bgCompletionsStarted := make(chan struct{})
	go func() {
		for i := int64(2); i < 4; i++ {
			if err := completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(i, time.Now(), nil)); err != nil {
				t.Errorf("expected nil err, got %v", err)
			}
		}
		close(bgCompletionsStarted)
	}()

	expectCompletionInFlight := func() {
		select {
		case input := <-inputCh:
			t.Logf("completion for %d in-flight", input.jobID)
		case <-time.After(time.Second):
			t.Fatalf("expected a completion to be in-flight")
		}
	}
	expectNoCompletionInFlight := func() {
		select {
		case input := <-inputCh:
			t.Fatalf("unexpected completion for %d in-flight", input.jobID)
		case <-time.After(500 * time.Millisecond):
		}
	}

	// two completions should be in-flight:
	expectCompletionInFlight()
	expectCompletionInFlight()

	// A 3rd one shouldn't be in-flight due to the concurrency limit:
	expectNoCompletionInFlight()

	// Finish the first two completions:
	resultCh <- nil
	resultCh <- nil

	// The final two completions should now be in-flight:
	<-bgCompletionsStarted
	expectCompletionInFlight()
	expectCompletionInFlight()

	// A 5th one shouldn't be in-flight because we only started 4:
	expectNoCompletionInFlight()

	// Finish the final two completions:
	resultCh <- nil
	resultCh <- nil
}

func TestAsyncJobCompleter_Subscribe(t *testing.T) {
	t.Parallel()

	testCompleterSubscribe(t, func(exec riverdriver.Executor, subscribeCh SubscribeChan) JobCompleter {
		return newAsyncCompleterWithConcurrency(riversharedtest.BaseServiceArchetype(t), exec, &riverpilot.StandardPilot{}, 4, subscribeCh)
	})
}

func TestAsyncJobCompleter_Wait(t *testing.T) {
	t.Parallel()

	testCompleterWait(t, func(exec riverdriver.Executor, subscribeCh SubscribeChan) JobCompleter {
		return newAsyncCompleterWithConcurrency(riversharedtest.BaseServiceArchetype(t), exec, &riverpilot.StandardPilot{}, 4, subscribeCh)
	})
}

func testCompleterSubscribe(t *testing.T, constructor func(riverdriver.Executor, SubscribeChan) JobCompleter) {
	t.Helper()

	ctx := context.Background()

	var (
		db       = riverinternaltest.TestDB(ctx, t)
		driver   = riverpgxv5.New(db)
		execMock = NewPartialExecutorMock(driver.GetExecutor())
	)
	execMock.JobSetStateIfRunningManyFunc = func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
		return []*rivertype.JobRow{{ID: params.ID[0], State: rivertype.JobStateCompleted}}, nil
	}

	subscribeChan := make(chan []CompleterJobUpdated, 10)
	completer := constructor(execMock, subscribeChan)
	require.NoError(t, completer.Start(ctx))

	// Flatten the slice results from subscribeChan into jobUpdateChan:
	jobUpdateChan := make(chan CompleterJobUpdated, 10)
	go func() {
		defer close(jobUpdateChan)
		for update := range subscribeChan {
			for _, u := range update {
				jobUpdateChan <- u
			}
		}
	}()

	for i := range 4 {
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(int64(i), time.Now(), nil)))
	}

	completer.Stop() // closes subscribeChan

	updates := riversharedtest.WaitOrTimeoutN(t, jobUpdateChan, 4)
	for range 4 {
		require.Equal(t, rivertype.JobStateCompleted, updates[0].Job.State)
	}
	go completer.Stop()
	// drain all remaining jobs
	for range jobUpdateChan {
	}
}

func testCompleterWait(t *testing.T, constructor func(riverdriver.Executor, SubscribeChan) JobCompleter) {
	t.Helper()

	ctx := context.Background()

	var (
		db       = riverinternaltest.TestDB(ctx, t)
		driver   = riverpgxv5.New(db)
		execMock = NewPartialExecutorMock(driver.GetExecutor())
	)

	resultCh := make(chan struct{})
	completeStartedCh := make(chan struct{})
	execMock.JobSetStateIfRunningManyFunc = func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
		completeStartedCh <- struct{}{}
		<-resultCh
		results := make([]*rivertype.JobRow, len(params.ID))
		for i := range params.ID {
			results[i] = &rivertype.JobRow{ID: params.ID[i], State: rivertype.JobStateCompleted}
		}
		return results, nil
	}
	subscribeCh := make(chan []CompleterJobUpdated, 100)

	completer := constructor(execMock, subscribeCh)
	require.NoError(t, completer.Start(ctx))

	// launch 4 completions:
	for i := range 4 {
		go func() {
			require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(int64(i), time.Now(), nil)))
		}()
		<-completeStartedCh // wait for func to actually start
	}

	// Give one completion a signal to finish, there should be 3 remaining in-flight:
	resultCh <- struct{}{}

	waitDone := make(chan struct{})
	go func() {
		completer.Stop()
		close(waitDone)
	}()

	select {
	case <-waitDone:
		t.Fatalf("expected Wait to block until all jobs are complete, but it returned when there should be three remaining")
	case <-time.After(100 * time.Millisecond):
	}

	// Get us down to one in-flight completion:
	resultCh <- struct{}{}
	resultCh <- struct{}{}

	select {
	case <-waitDone:
		t.Fatalf("expected Wait to block until all jobs are complete, but it returned when there should be one remaining")
	case <-time.After(100 * time.Millisecond):
	}

	// Finish the last one:
	resultCh <- struct{}{}

	select {
	case <-waitDone:
	case <-time.After(100 * time.Millisecond):
		t.Errorf("expected Wait to return after all jobs are complete")
	}
}

func TestAsyncCompleter(t *testing.T) {
	t.Parallel()

	testCompleter(t, func(t *testing.T, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) *AsyncCompleter {
		t.Helper()
		return NewAsyncCompleter(riversharedtest.BaseServiceArchetype(t), exec, pilot, subscribeCh)
	},
		func(completer *AsyncCompleter) { completer.disableSleep = true },
		func(completer *AsyncCompleter, exec riverdriver.Executor) { completer.exec = exec },
	)
}

func TestBatchCompleter(t *testing.T) {
	t.Parallel()

	testCompleter(t, func(t *testing.T, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) *BatchCompleter {
		t.Helper()
		return NewBatchCompleter(riversharedtest.BaseServiceArchetype(t), exec, pilot, subscribeCh)
	},
		func(completer *BatchCompleter) { completer.disableSleep = true },
		func(completer *BatchCompleter, exec riverdriver.Executor) { completer.exec = exec },
	)

	ctx := context.Background()

	type testBundle struct {
		exec        riverdriver.Executor
		subscribeCh <-chan []CompleterJobUpdated
	}

	setup := func(t *testing.T) (*BatchCompleter, *testBundle) {
		t.Helper()

		var (
			driver      = riverpgxv5.New(riverinternaltest.TestDB(ctx, t))
			exec        = driver.GetExecutor()
			pilot       = &riverpilot.StandardPilot{}
			subscribeCh = make(chan []CompleterJobUpdated, 10)
			completer   = NewBatchCompleter(riversharedtest.BaseServiceArchetype(t), exec, pilot, subscribeCh)
		)

		require.NoError(t, completer.Start(ctx))
		t.Cleanup(completer.Stop)

		riversharedtest.WaitOrTimeout(t, completer.Started())

		return completer, &testBundle{
			exec:        exec,
			subscribeCh: subscribeCh,
		}
	}

	t.Run("CompletionsCompletedInSubBatches", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)
		completer.completionMaxSize = 10 // set to something artificially low

		jobUpdateChan := make(chan CompleterJobUpdated, 100)
		go func() {
			defer close(jobUpdateChan)
			for update := range bundle.subscribeCh {
				for _, u := range update {
					jobUpdateChan <- u
				}
			}
		}()

		stopInsertion := doContinuousInsertion(ctx, t, completer, bundle.exec)

		// Wait for some jobs to come through, giving lots of opportunity for
		// the completer to have pooled some completions and being forced to
		// work them in sub-batches with our diminished sub-batch size.
		riversharedtest.WaitOrTimeoutN(t, jobUpdateChan, 100)

		stopInsertion()
		go completer.Stop()
		// drain all remaining jobs
		for range jobUpdateChan {
		}
	})

	t.Run("BacklogWaitAndContinue", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)
		completer.maxBacklog = 10 // set to something artificially low

		jobUpdateChan := make(chan CompleterJobUpdated, 100)
		go func() {
			defer close(jobUpdateChan)
			for update := range bundle.subscribeCh {
				for _, u := range update {
					jobUpdateChan <- u
				}
			}
		}()

		stopInsertion := doContinuousInsertion(ctx, t, completer, bundle.exec)

		// Wait for some jobs to come through. Waiting for these jobs to come
		// through will provide plenty of opportunity for the completer to back
		// up with our small configured backlog.
		riversharedtest.WaitOrTimeoutN(t, jobUpdateChan, 100)

		stopInsertion()
		go completer.Stop()
		// drain all remaining jobs
		for range jobUpdateChan {
		}
	})
}

func TestInlineCompleter(t *testing.T) {
	t.Parallel()

	testCompleter(t, func(t *testing.T, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) *InlineCompleter {
		t.Helper()
		return NewInlineCompleter(riversharedtest.BaseServiceArchetype(t), exec, pilot, subscribeCh)
	},
		func(completer *InlineCompleter) { completer.disableSleep = true },
		func(completer *InlineCompleter, exec riverdriver.Executor) { completer.exec = exec })
}

func testCompleter[TCompleter JobCompleter](
	t *testing.T,
	newCompleter func(t *testing.T, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) TCompleter,

	// These functions are here to help us inject test behavior that's not part
	// of the JobCompleter interface. We could alternatively define a second
	// interface like jobCompleterWithTestFacilities to expose the additional
	// functionality, although that's not particularly beautiful either.
	disableSleep func(completer TCompleter),
	setExec func(completer TCompleter, exec riverdriver.Executor),
) {
	t.Helper()

	ctx := context.Background()

	type testBundle struct {
		exec        riverdriver.Executor
		subscribeCh <-chan []CompleterJobUpdated
	}

	setup := func(t *testing.T) (TCompleter, *testBundle) {
		t.Helper()

		var (
			driver      = riverpgxv5.New(riverinternaltest.TestDB(ctx, t))
			exec        = driver.GetExecutor()
			pilot       = &riverpilot.StandardPilot{}
			subscribeCh = make(chan []CompleterJobUpdated, 10)
			completer   = newCompleter(t, exec, pilot, subscribeCh)
		)

		require.NoError(t, completer.Start(ctx))
		t.Cleanup(completer.Stop)

		return completer, &testBundle{
			exec:        exec,
			subscribeCh: subscribeCh,
		}
	}

	requireJob := func(t *testing.T, exec riverdriver.Executor, jobID int64) *rivertype.JobRow {
		t.Helper()

		job, err := exec.JobGetByID(ctx, jobID)
		require.NoError(t, err)
		return job
	}

	requireState := func(t *testing.T, exec riverdriver.Executor, jobID int64, state rivertype.JobState) *rivertype.JobRow {
		t.Helper()

		job := requireJob(t, exec, jobID)
		require.Equal(t, state, job.State)
		return job
	}

	t.Run("CompletesJobs", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		var (
			finalizedAt1 = time.Now().UTC().Add(-1 * time.Minute)
			finalizedAt2 = time.Now().UTC().Add(-2 * time.Minute)
			finalizedAt3 = time.Now().UTC().Add(-3 * time.Minute)

			job1 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job2 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job3 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		)

		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job1.ID, finalizedAt1, nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job2.ID, finalizedAt2, nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job3.ID, finalizedAt3, nil)))

		completer.Stop()

		job1Updated := requireState(t, bundle.exec, job1.ID, rivertype.JobStateCompleted)
		job2Updated := requireState(t, bundle.exec, job2.ID, rivertype.JobStateCompleted)
		job3Updated := requireState(t, bundle.exec, job3.ID, rivertype.JobStateCompleted)

		require.WithinDuration(t, finalizedAt1, *job1Updated.FinalizedAt, time.Microsecond)
		require.WithinDuration(t, finalizedAt2, *job2Updated.FinalizedAt, time.Microsecond)
		require.WithinDuration(t, finalizedAt3, *job3Updated.FinalizedAt, time.Microsecond)
	})

	// Some completers like BatchCompleter have special logic for when they're
	// handling enormous numbers of jobs, so make sure we're covered for cases
	// like that.
	t.Run("CompletesManyJobs", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		const (
			kind    = "many_jobs_kind"
			numJobs = 4_400
		)

		var (
			insertParams = make([]*riverdriver.JobInsertFastParams, numJobs)
			stats        = make([]jobstats.JobStatistics, numJobs)
		)
		for i := range numJobs {
			insertParams[i] = &riverdriver.JobInsertFastParams{
				EncodedArgs: []byte(`{}`),
				Kind:        kind,
				MaxAttempts: rivercommon.MaxAttemptsDefault,
				Priority:    rivercommon.PriorityDefault,
				Queue:       rivercommon.QueueDefault,
				State:       rivertype.JobStateRunning,
			}
		}

		_, err := bundle.exec.JobInsertFastMany(ctx, insertParams)
		require.NoError(t, err)

		jobs, err := bundle.exec.JobGetByKindMany(ctx, []string{kind})
		require.NoError(t, err)

		t.Cleanup(riverinternaltest.DiscardContinuously(bundle.subscribeCh))

		for i := range jobs {
			require.NoError(t, completer.JobSetStateIfRunning(ctx, &stats[i], riverdriver.JobSetStateCompleted(jobs[i].ID, time.Now(), nil)))
		}

		completer.Stop()

		updatedJobs, err := bundle.exec.JobGetByKindMany(ctx, []string{kind})
		require.NoError(t, err)
		for i := range updatedJobs {
			require.Equal(t, rivertype.JobStateCompleted, updatedJobs[i].State)
		}
	})

	// The minimum time to wait go guarantee a batch of completions from the
	// batch completer. Unless jobs are above a threshold it'll wait a number of
	// ticks before starting completions. 5 ticks @ 50 milliseconds.
	const minBatchCompleterPassDuration = 5 * 50 * time.Millisecond

	t.Run("FastContinuousCompletion", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		t.Cleanup(riverinternaltest.DiscardContinuously(bundle.subscribeCh))
		stopInsertion := doContinuousInsertion(ctx, t, completer, bundle.exec)

		// Give some time for some jobs to be inserted, and a guaranteed pass by
		// the batch completer.
		time.Sleep(minBatchCompleterPassDuration)

		// Signal to stop insertion and wait for the goroutine to return.
		numInserted := stopInsertion()

		require.Positive(t, numInserted)

		numCompleted, err := bundle.exec.JobCountByState(ctx, rivertype.JobStateCompleted)
		require.NoError(t, err)
		t.Logf("Counted %d jobs as completed", numCompleted)
		require.Positive(t, numCompleted)
	})

	t.Run("SlowerContinuousCompletion", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		// Number here is chosen to be a little higher than the batch
		// completer's tick interval so we can make sure that the right thing
		// happens even on an empty tick.
		stopInsertion := doContinuousInsertionInterval(ctx, t, completer, bundle.exec, 30*time.Millisecond)

		// Give some time for some jobs to be inserted, and a guaranteed pass by
		// the batch completer.
		time.Sleep(minBatchCompleterPassDuration)

		// Signal to stop insertion and wait for the goroutine to return.
		numInserted := stopInsertion()

		require.Positive(t, numInserted)

		numCompleted, err := bundle.exec.JobCountByState(ctx, rivertype.JobStateCompleted)
		require.NoError(t, err)
		t.Logf("Counted %d jobs as completed", numCompleted)
		require.Positive(t, numCompleted)
	})

	t.Run("AllJobStates", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		var (
			job1 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job2 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job3 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job4 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job5 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job6 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			job7 = testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		)

		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCancelled(job1.ID, time.Now(), []byte("{}"), nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job2.ID, time.Now(), nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateDiscarded(job3.ID, time.Now(), []byte("{}"), nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateErrorAvailable(job4.ID, time.Now(), []byte("{}"), nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateErrorRetryable(job5.ID, time.Now(), []byte("{}"), nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateSnoozed(job6.ID, time.Now(), 10, nil)))
		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateSnoozedAvailable(job7.ID, time.Now(), 10, nil)))

		completer.Stop()

		requireState(t, bundle.exec, job1.ID, rivertype.JobStateCancelled)
		requireState(t, bundle.exec, job2.ID, rivertype.JobStateCompleted)
		requireState(t, bundle.exec, job3.ID, rivertype.JobStateDiscarded)
		requireState(t, bundle.exec, job4.ID, rivertype.JobStateAvailable)
		requireState(t, bundle.exec, job5.ID, rivertype.JobStateRetryable)
		requireState(t, bundle.exec, job6.ID, rivertype.JobStateScheduled)
		requireState(t, bundle.exec, job7.ID, rivertype.JobStateAvailable)
	})

	t.Run("Subscription", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil)))

		completer.Stop()

		jobUpdate := riversharedtest.WaitOrTimeout(t, bundle.subscribeCh)
		require.Len(t, jobUpdate, 1)
		require.Equal(t, rivertype.JobStateCompleted, jobUpdate[0].Job.State)
	})

	t.Run("MultipleCycles", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		{
			job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

			require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil)))

			completer.Stop()

			requireState(t, bundle.exec, job.ID, rivertype.JobStateCompleted)
		}

		// Completer closes the subscribe channel on stop, so we need to reset it between runs.
		completer.ResetSubscribeChan(make(SubscribeChan, 10))

		{
			require.NoError(t, completer.Start(ctx))

			job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

			require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil)))

			completer.Stop()

			requireState(t, bundle.exec, job.ID, rivertype.JobStateCompleted)
		}
	})

	t.Run("CompletionFailure", func(t *testing.T) {
		t.Parallel()

		completer, bundle := setup(t)

		// The completers will do an exponential backoff sleep while retrying.
		// Make sure to disable it for this test case so the tests stay fast.
		disableSleep(completer)

		var numCalls int
		maybeError := func() error {
			numCalls++
			switch numCalls {
			case 1:
				fallthrough
			case 2:
				return fmt.Errorf("error from executor %d", numCalls)
			}
			return nil
		}

		execMock := NewPartialExecutorMock(bundle.exec)
		execMock.JobSetStateIfRunningManyFunc = func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
			if err := maybeError(); err != nil {
				return nil, err
			}
			return bundle.exec.JobSetStateIfRunningMany(ctx, params)
		}
		setExec(completer, execMock)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

		require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil)))

		completer.Stop()

		// Make sure our mocks were really called.
		require.True(t, execMock.JobSetStateIfRunningManyCalled)

		// Job still managed to complete despite the errors.
		requireState(t, bundle.exec, job.ID, rivertype.JobStateCompleted)
	})

	t.Run("CompletionImmediateFailureOnContextCanceled", func(t *testing.T) { //nolint:dupl
		t.Parallel()

		completer, bundle := setup(t)

		// The completers will do an exponential backoff sleep while retrying.
		// Make sure to disable it for this test case so the tests stay fast.
		disableSleep(completer)

		execMock := NewPartialExecutorMock(bundle.exec)
		execMock.JobSetStateIfRunningManyFunc = func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
			return nil, context.Canceled
		}
		setExec(completer, execMock)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

		err := completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil))

		// The error returned will be nil for asynchronous completers, but
		// returned immediately for synchronous ones.
		require.True(t, err == nil || errors.Is(err, context.Canceled))

		completer.Stop()

		// Make sure our mocks were really called.
		require.True(t, execMock.JobSetStateIfRunningManyCalled)

		// Job is still running because the completer is forced to give up
		// immediately on certain types of errors like where a pool is closed.
		requireState(t, bundle.exec, job.ID, rivertype.JobStateRunning)
	})

	t.Run("CompletionImmediateFailureOnErrClosedPool", func(t *testing.T) { //nolint:dupl
		t.Parallel()

		completer, bundle := setup(t)

		// The completers will do an exponential backoff sleep while retrying.
		// Make sure to disable it for this test case so the tests stay fast.
		disableSleep(completer)

		execMock := NewPartialExecutorMock(bundle.exec)
		execMock.JobSetStateIfRunningManyFunc = func(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
			return nil, puddle.ErrClosedPool
		}
		setExec(completer, execMock)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

		err := completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil))

		// The error returned will be nil for asynchronous completers, but
		// returned immediately for synchronous ones.
		require.True(t, err == nil || errors.Is(err, puddle.ErrClosedPool))

		completer.Stop()

		// Make sure our mocks were really called.
		require.True(t, execMock.JobSetStateIfRunningManyCalled)

		// Job is still running because the completer is forced to give up
		// immediately on certain types of errors like where a pool is closed.
		requireState(t, bundle.exec, job.ID, rivertype.JobStateRunning)
	})

	// The batch completer supports an interface that lets caller wait for it to
	// start. Make sure this works as expected.
	t.Run("WithStartedWaitsForStarted", func(t *testing.T) {
		t.Parallel()

		completer, _ := setup(t)

		var completerInterface JobCompleter = completer
		if withWait, ok := completerInterface.(startstop.Service); ok {
			riversharedtest.WaitOrTimeout(t, withWait.Started())
		}
	})
}

func BenchmarkAsyncCompleter_Concurrency10(b *testing.B) {
	benchmarkCompleter(b, func(b *testing.B, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) JobCompleter {
		b.Helper()
		return newAsyncCompleterWithConcurrency(riversharedtest.BaseServiceArchetype(b), exec, pilot, 10, subscribeCh)
	})
}

func BenchmarkAsyncCompleter_Concurrency100(b *testing.B) {
	benchmarkCompleter(b, func(b *testing.B, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) JobCompleter {
		b.Helper()
		return newAsyncCompleterWithConcurrency(riversharedtest.BaseServiceArchetype(b), exec, pilot, 100, subscribeCh)
	})
}

func BenchmarkBatchCompleter(b *testing.B) {
	benchmarkCompleter(b, func(b *testing.B, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) JobCompleter {
		b.Helper()
		return NewBatchCompleter(riversharedtest.BaseServiceArchetype(b), exec, pilot, subscribeCh)
	})
}

func BenchmarkInlineCompleter(b *testing.B) {
	benchmarkCompleter(b, func(b *testing.B, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) JobCompleter {
		b.Helper()
		return NewInlineCompleter(riversharedtest.BaseServiceArchetype(b), exec, pilot, subscribeCh)
	})
}

func benchmarkCompleter(
	b *testing.B,
	newCompleter func(b *testing.B, exec riverdriver.Executor, pilot riverpilot.Pilot, subscribeCh chan<- []CompleterJobUpdated) JobCompleter,
) {
	b.Helper()

	ctx := context.Background()

	type testBundle struct {
		exec  riverdriver.Executor
		jobs  []*rivertype.JobRow
		pilot riverpilot.Pilot
		stats []jobstats.JobStatistics
	}

	setup := func(b *testing.B) (JobCompleter, *testBundle) {
		b.Helper()

		var (
			driver      = riverpgxv5.New(riverinternaltest.TestDB(ctx, b))
			exec        = driver.GetExecutor()
			pilot       = &riverpilot.StandardPilot{}
			subscribeCh = make(chan []CompleterJobUpdated, 100)
			completer   = newCompleter(b, exec, pilot, subscribeCh)
		)

		b.Cleanup(riverinternaltest.DiscardContinuously(subscribeCh))

		require.NoError(b, completer.Start(ctx))
		b.Cleanup(completer.Stop)

		if withWait, ok := completer.(startstop.Service); ok {
			riversharedtest.WaitOrTimeout(b, withWait.Started())
		}

		insertParams := make([]*riverdriver.JobInsertFastParams, b.N)
		for i := range b.N {
			insertParams[i] = &riverdriver.JobInsertFastParams{
				EncodedArgs: []byte(`{}`),
				Kind:        "benchmark_kind",
				MaxAttempts: rivercommon.MaxAttemptsDefault,
				Priority:    rivercommon.PriorityDefault,
				Queue:       rivercommon.QueueDefault,
				State:       rivertype.JobStateRunning,
			}
		}

		_, err := exec.JobInsertFastMany(ctx, insertParams)
		require.NoError(b, err)

		jobs, err := exec.JobGetByKindMany(ctx, []string{"benchmark_kind"})
		require.NoError(b, err)

		return completer, &testBundle{
			exec:  exec,
			jobs:  jobs,
			pilot: pilot,
			stats: make([]jobstats.JobStatistics, b.N),
		}
	}

	b.Run("Completion", func(b *testing.B) {
		completer, bundle := setup(b)

		b.ResetTimer()

		for i := range b.N {
			err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateCompleted(bundle.jobs[i].ID, time.Now(), nil))
			require.NoError(b, err)
		}

		completer.Stop()
	})

	b.Run("RotatingStates", func(b *testing.B) {
		completer, bundle := setup(b)

		b.ResetTimer()

		for i := range b.N {
			switch i % 7 {
			case 0:
				err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateCancelled(bundle.jobs[i].ID, time.Now(), []byte("{}"), nil))
				require.NoError(b, err)

			case 1:
				err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateCompleted(bundle.jobs[i].ID, time.Now(), nil))
				require.NoError(b, err)

			case 2:
				err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateDiscarded(bundle.jobs[i].ID, time.Now(), []byte("{}"), nil))
				require.NoError(b, err)

			case 3:
				err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateErrorAvailable(bundle.jobs[i].ID, time.Now(), []byte("{}"), nil))
				require.NoError(b, err)

			case 4:
				err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateErrorRetryable(bundle.jobs[i].ID, time.Now(), []byte("{}"), nil))
				require.NoError(b, err)

			case 5:
				err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateSnoozed(bundle.jobs[i].ID, time.Now(), 10, nil))
				require.NoError(b, err)

			case 6:
				err := completer.JobSetStateIfRunning(ctx, &bundle.stats[i], riverdriver.JobSetStateSnoozedAvailable(bundle.jobs[i].ID, time.Now(), 10, nil))
				require.NoError(b, err)

			default:
				panic("unexpected modulo result (did you update cases without changing the modulo divider or vice versa?")
			}
		}

		completer.Stop()
	})
}

// Performs continuous job insertion from a background goroutine. Returns a
// function that should be invoked to stop insertion, which will block until
// insertion stops, then return the total number of jobs that were inserted.
func doContinuousInsertion(ctx context.Context, t *testing.T, completer JobCompleter, exec riverdriver.Executor) func() int {
	t.Helper()

	return doContinuousInsertionInterval(ctx, t, completer, exec, 1*time.Millisecond)
}

func doContinuousInsertionInterval(ctx context.Context, t *testing.T, completer JobCompleter, exec riverdriver.Executor, insertInterval time.Duration) func() int {
	t.Helper()

	var (
		insertionStopped = make(chan struct{})
		numInserted      atomic.Int64
		stopInsertion    = make(chan struct{})
		ticker           = time.NewTicker(insertInterval)
	)
	go func() {
		defer close(insertionStopped)

		defer ticker.Stop()

		defer func() {
			t.Logf("Inserted %d jobs", numInserted.Load())
		}()

		for {
			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
			require.NoError(t, completer.JobSetStateIfRunning(ctx, &jobstats.JobStatistics{}, riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil)))
			numInserted.Add(1)

			select {
			case <-stopInsertion:
				return
			case <-ticker.C:
			}
		}
	}()

	return func() int {
		close(stopInsertion)
		<-insertionStopped
		return int(numInserted.Load())
	}
}

```

`internal/jobcompleter/main_test.go`:

```go
package jobcompleter

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`internal/jobexecutor/job_executor.go`:

```go
package jobexecutor

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"log/slog"
	"runtime"
	"time"

	"github.com/tidwall/gjson"
	"github.com/tidwall/sjson"

	"github.com/riverqueue/river/internal/execution"
	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/jobstats"
	"github.com/riverqueue/river/internal/middlewarelookup"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/util/valutil"
	"github.com/riverqueue/river/rivertype"
)

type ClientRetryPolicy interface {
	NextRetry(job *rivertype.JobRow) time.Time
}

// ErrorHandler provides an interface that will be invoked in case of an error
// or panic occurring in the job. This is often useful for logging and exception
// tracking, but can also be used to customize retry behavior.
type ErrorHandler interface {
	// HandleError is invoked in case of an error occurring in a job.
	//
	// Context is descended from the one used to start the River client that
	// worked the job.
	HandleError(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult

	// HandlePanic is invoked in case of a panic occurring in a job.
	//
	// Context is descended from the one used to start the River client that
	// worked the job.
	HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult
}

type ErrorHandlerResult struct {
	// SetCancelled can be set to true to fail the job immediately and
	// permanently. By default it'll continue to follow the configured retry
	// schedule.
	SetCancelled bool
}

// Error used in CancelFunc in cases where the job was not cancelled for
// purposes of resource cleanup. Should never be user visible.
var errExecutorDefaultCancel = errors.New("context cancelled as executor finished")

type contextKey string

// ContextKeyMetadataUpdates is the context key for the metadata updates map
// stored in the context. It's exposed from this internal package solely so
// that it can be used in tests for JobCompleteTx.
const ContextKeyMetadataUpdates contextKey = "river_metadata_updates"

// MetadataUpdatesFromWorkContext returns the metadata updates stored in the
// work context, if any.
//
// When run on a non-work context, it returns nil, false.
func MetadataUpdatesFromWorkContext(ctx context.Context) (map[string]any, bool) {
	metadataUpdates := ctx.Value(ContextKeyMetadataUpdates)
	if metadataUpdates == nil {
		return nil, false
	}
	typedMetadataUpdates, ok := metadataUpdates.(map[string]any)
	if !ok {
		return nil, false
	}
	return typedMetadataUpdates, true
}

type jobExecutorResult struct {
	Err             error
	MetadataUpdates map[string]any
	NextRetry       time.Time
	PanicTrace      string
	PanicVal        any
}

// ErrorStr returns an appropriate string to persist to the database based on
// the type of internal failure (i.e. error or panic). Panics if called on a
// non-errored result.
func (r *jobExecutorResult) ErrorStr() string {
	switch {
	case r.Err != nil:
		return r.Err.Error()
	case r.PanicVal != nil:
		return fmt.Sprintf("%v", r.PanicVal)
	}

	panic("ErrorStr should not be called on non-errored result")
}

type JobExecutor struct {
	baseservice.BaseService

	CancelFunc               context.CancelCauseFunc
	ClientJobTimeout         time.Duration
	Completer                jobcompleter.JobCompleter
	ClientRetryPolicy        ClientRetryPolicy
	DefaultClientRetryPolicy ClientRetryPolicy
	ErrorHandler             ErrorHandler
	HookLookupByJob          *hooklookup.JobHookLookup
	HookLookupGlobal         hooklookup.HookLookupInterface
	InformProducerDoneFunc   func(jobRow *rivertype.JobRow)
	JobRow                   *rivertype.JobRow
	MiddlewareLookupGlobal   middlewarelookup.MiddlewareLookupInterface
	SchedulerInterval        time.Duration
	WorkUnit                 workunit.WorkUnit

	// Meant to be used from within the job executor only.
	start time.Time
	stats *jobstats.JobStatistics // initialized by the executor, and handed off to completer
}

func (e *JobExecutor) Cancel() {
	e.Logger.Warn(e.Name+": job cancelled remotely", slog.Int64("job_id", e.JobRow.ID))
	e.CancelFunc(rivertype.ErrJobCancelledRemotely)
}

func (e *JobExecutor) Execute(ctx context.Context) {
	// Ensure that the context is cancelled no matter what, or it will leak:
	defer e.CancelFunc(errExecutorDefaultCancel)

	e.start = e.Time.NowUTC()
	e.stats = &jobstats.JobStatistics{
		QueueWaitDuration: e.start.Sub(e.JobRow.ScheduledAt),
	}

	res := e.execute(ctx)
	if res.Err != nil && errors.Is(context.Cause(ctx), rivertype.ErrJobCancelledRemotely) {
		res.Err = context.Cause(ctx)
	}

	e.reportResult(ctx, res)

	e.InformProducerDoneFunc(e.JobRow)
}

// Executes the job, handling a panic if necessary (and various other error
// conditions). The named return value is so that we can still return a value in
// case of a panic.
//
//nolint:nonamedreturns
func (e *JobExecutor) execute(ctx context.Context) (res *jobExecutorResult) {
	metadataUpdates := make(map[string]any)
	ctx = context.WithValue(ctx, ContextKeyMetadataUpdates, metadataUpdates)

	defer func() {
		if recovery := recover(); recovery != nil {
			e.Logger.ErrorContext(ctx, e.Name+": panic recovery; possible bug with Worker",
				slog.Int64("job_id", e.JobRow.ID),
				slog.String("kind", e.JobRow.Kind),
				slog.String("panic_val", fmt.Sprintf("%v", recovery)),
			)

			res = &jobExecutorResult{
				MetadataUpdates: metadataUpdates,
				// Skip the first 4 frames which are:
				//
				// 1. The `runtime.Callers` function.
				// 2. The `captureStackTraceSkipFrames` function.
				// 3. The current recovery defer function.
				// 4. The `JobExecutor.execute` method working the job.
				PanicTrace: captureStackTraceSkipFrames(4),
				PanicVal:   recovery,
			}
		}
		e.stats.RunDuration = e.Time.NowUTC().Sub(e.start)
	}()

	if e.WorkUnit == nil {
		e.Logger.ErrorContext(ctx, e.Name+": Unhandled job kind",
			slog.String("kind", e.JobRow.Kind),
			slog.Int64("job_id", e.JobRow.ID),
		)
		return &jobExecutorResult{Err: &rivertype.UnknownJobKindError{Kind: e.JobRow.Kind}, MetadataUpdates: metadataUpdates}
	}

	doInner := execution.Func(func(ctx context.Context) error {
		{
			for _, hook := range append(
				e.HookLookupGlobal.ByHookKind(hooklookup.HookKindWorkBegin),
				e.WorkUnit.HookLookup(e.HookLookupByJob).ByHookKind(hooklookup.HookKindWorkBegin)...,
			) {
				if err := hook.(rivertype.HookWorkBegin).WorkBegin(ctx, e.JobRow); err != nil { //nolint:forcetypeassert
					return err
				}
			}
		}

		if err := e.WorkUnit.UnmarshalJob(); err != nil {
			return err
		}

		jobTimeout := valutil.FirstNonZero(e.WorkUnit.Timeout(), e.ClientJobTimeout)
		ctx, cancel := execution.MaybeApplyTimeout(ctx, jobTimeout)
		defer cancel()

		return e.WorkUnit.Work(ctx)
	})

	executeFunc := execution.MiddlewareChain(
		e.MiddlewareLookupGlobal.ByMiddlewareKind(middlewarelookup.MiddlewareKindWorker),
		e.WorkUnit.Middleware(),
		doInner,
		e.JobRow,
	)

	return &jobExecutorResult{Err: executeFunc(ctx), MetadataUpdates: metadataUpdates}
}

func (e *JobExecutor) invokeErrorHandler(ctx context.Context, res *jobExecutorResult) bool {
	invokeAndHandlePanic := func(funcName string, errorHandler func() *ErrorHandlerResult) *ErrorHandlerResult {
		defer func() {
			if panicVal := recover(); panicVal != nil {
				e.Logger.ErrorContext(ctx, e.Name+": ErrorHandler invocation panicked",
					slog.String("function_name", funcName),
					slog.String("panic_val", fmt.Sprintf("%v", panicVal)),
				)
			}
		}()

		return errorHandler()
	}

	var errorHandlerRes *ErrorHandlerResult
	switch {
	case res.Err != nil:
		errorHandlerRes = invokeAndHandlePanic("HandleError", func() *ErrorHandlerResult {
			return e.ErrorHandler.HandleError(ctx, e.JobRow, res.Err)
		})

	case res.PanicVal != nil:
		errorHandlerRes = invokeAndHandlePanic("HandlePanic", func() *ErrorHandlerResult {
			return e.ErrorHandler.HandlePanic(ctx, e.JobRow, res.PanicVal, res.PanicTrace)
		})
	}

	return errorHandlerRes != nil && errorHandlerRes.SetCancelled
}

func (e *JobExecutor) reportResult(ctx context.Context, res *jobExecutorResult) {
	var snoozeErr *rivertype.JobSnoozeError

	var (
		metadataUpdatesBytes []byte
		err                  error
	)
	if len(res.MetadataUpdates) > 0 {
		metadataUpdatesBytes, err = json.Marshal(res.MetadataUpdates)
		if err != nil {
			e.Logger.ErrorContext(ctx, e.Name+": Failed to marshal metadata updates", slog.String("error", err.Error()))
			return
		}
	}

	if res.Err != nil && errors.As(res.Err, &snoozeErr) {
		e.Logger.DebugContext(ctx, e.Name+": Job snoozed",
			slog.Int64("job_id", e.JobRow.ID),
			slog.String("job_kind", e.JobRow.Kind),
			slog.Duration("duration", snoozeErr.Duration),
		)
		nextAttemptScheduledAt := time.Now().Add(snoozeErr.Duration)

		snoozesValue := gjson.GetBytes(e.JobRow.Metadata, "snoozes").Int()
		metadataUpdatesBytes, err = sjson.SetBytes(metadataUpdatesBytes, "snoozes", snoozesValue+1)
		if err != nil {
			e.Logger.ErrorContext(ctx, e.Name+": Failed to set snoozes", slog.String("error", err.Error()))
			return
		}

		// Normally, snoozed jobs are set `scheduled` for the future and it's the
		// scheduler's job to set them back to `available` so they can be reworked.
		// Just as with retryable jobs, this isn't friendly for short snooze times
		// so we instead make the job immediately `available` if the snooze time is
		// smaller than the scheduler's run interval.
		var params *riverdriver.JobSetStateIfRunningParams
		if nextAttemptScheduledAt.Sub(e.Time.NowUTC()) <= e.SchedulerInterval {
			params = riverdriver.JobSetStateSnoozedAvailable(e.JobRow.ID, nextAttemptScheduledAt, e.JobRow.Attempt-1, metadataUpdatesBytes)
		} else {
			params = riverdriver.JobSetStateSnoozed(e.JobRow.ID, nextAttemptScheduledAt, e.JobRow.Attempt-1, metadataUpdatesBytes)
		}
		if err := e.Completer.JobSetStateIfRunning(ctx, e.stats, params); err != nil {
			e.Logger.ErrorContext(ctx, e.Name+": Error snoozing job",
				slog.Int64("job_id", e.JobRow.ID),
			)
		}
		return
	}

	if res.Err != nil || res.PanicVal != nil {
		e.reportError(ctx, res, metadataUpdatesBytes)
		return
	}

	if err := e.Completer.JobSetStateIfRunning(ctx, e.stats, riverdriver.JobSetStateCompleted(e.JobRow.ID, e.Time.NowUTC(), metadataUpdatesBytes)); err != nil {
		e.Logger.ErrorContext(ctx, e.Name+": Error completing job",
			slog.String("err", err.Error()),
			slog.Int64("job_id", e.JobRow.ID),
		)
		return
	}
}

func (e *JobExecutor) reportError(ctx context.Context, res *jobExecutorResult, metadataUpdates []byte) {
	var (
		cancelJob bool
		cancelErr *rivertype.JobCancelError
	)

	logAttrs := []any{
		slog.String("error", res.ErrorStr()),
		slog.Int64("job_id", e.JobRow.ID),
		slog.String("job_kind", e.JobRow.Kind),
	}

	switch {
	case errors.As(res.Err, &cancelErr):
		cancelJob = true
		e.Logger.DebugContext(ctx, e.Name+": Job cancelled explicitly", logAttrs...)
	case res.Err != nil:
		if e.JobRow.Attempt >= e.JobRow.MaxAttempts {
			e.Logger.ErrorContext(ctx, e.Name+": Job errored", logAttrs...)
		} else {
			e.Logger.WarnContext(ctx, e.Name+": Job errored; retrying", logAttrs...)
		}
	case res.PanicVal != nil:
		e.Logger.ErrorContext(ctx, e.Name+": Job panicked", logAttrs...)
	}

	if e.ErrorHandler != nil && !cancelJob {
		// Error handlers also have an opportunity to cancel the job.
		cancelJob = e.invokeErrorHandler(ctx, res)
	}

	attemptErr := rivertype.AttemptError{
		At:      e.start,
		Attempt: e.JobRow.Attempt,
		Error:   res.ErrorStr(),
		Trace:   res.PanicTrace,
	}

	errData, err := json.Marshal(attemptErr)
	if err != nil {
		e.Logger.ErrorContext(ctx, e.Name+": Failed to marshal attempt error", logAttrs...)
		return
	}

	now := time.Now()

	if cancelJob {
		if err := e.Completer.JobSetStateIfRunning(ctx, e.stats, riverdriver.JobSetStateCancelled(e.JobRow.ID, now, errData, metadataUpdates)); err != nil {
			e.Logger.ErrorContext(ctx, e.Name+": Failed to cancel job and report error", logAttrs...)
		}
		return
	}

	if e.JobRow.Attempt >= e.JobRow.MaxAttempts {
		if err := e.Completer.JobSetStateIfRunning(ctx, e.stats, riverdriver.JobSetStateDiscarded(e.JobRow.ID, now, errData, metadataUpdates)); err != nil {
			e.Logger.ErrorContext(ctx, e.Name+": Failed to discard job and report error", logAttrs...)
		}
		return
	}

	var nextRetryScheduledAt time.Time
	if e.WorkUnit != nil {
		nextRetryScheduledAt = e.WorkUnit.NextRetry()
	}
	if nextRetryScheduledAt.IsZero() {
		nextRetryScheduledAt = e.ClientRetryPolicy.NextRetry(e.JobRow)
	}
	if nextRetryScheduledAt.Before(now) {
		e.Logger.WarnContext(ctx,
			e.Name+": Retry policy returned invalid next retry before current time; using default retry policy instead",
			slog.Int("error_count", len(e.JobRow.Errors)+1),
			slog.Time("next_retry_scheduled_at", nextRetryScheduledAt),
			slog.Time("now", now),
		)
		nextRetryScheduledAt = e.DefaultClientRetryPolicy.NextRetry(e.JobRow)
	}

	// Normally, errored jobs are set `retryable` for the future and it's the
	// scheduler's job to set them back to `available` so they can be reworked.
	// This isn't friendly for smaller retry times though because it means that
	// effectively no retry time smaller than the scheduler's run interval is
	// respected. Here, we offset that with a branch that makes jobs immediately
	// `available` if their retry was smaller than the scheduler's run interval.
	var params *riverdriver.JobSetStateIfRunningParams
	if nextRetryScheduledAt.Sub(e.Time.NowUTC()) <= e.SchedulerInterval {
		params = riverdriver.JobSetStateErrorAvailable(e.JobRow.ID, nextRetryScheduledAt, errData, metadataUpdates)
	} else {
		params = riverdriver.JobSetStateErrorRetryable(e.JobRow.ID, nextRetryScheduledAt, errData, metadataUpdates)
	}
	if err := e.Completer.JobSetStateIfRunning(ctx, e.stats, params); err != nil {
		e.Logger.ErrorContext(ctx, e.Name+": Failed to report error for job", logAttrs...)
	}
}

// captureStackTrace returns a formatted stack trace string starting after
// skipping the specified number of frames. The skip parameter should be
// adjusted so that frames you want to hide (like the ones generated by the
// tracing functions themselves) are excluded.
func captureStackTraceSkipFrames(skip int) string {
	// Allocate room for up to 100 callers; adjust as needed.
	pcs := make([]uintptr, 100)
	// Skip the specified number of frames.
	n := runtime.Callers(skip, pcs)
	frames := runtime.CallersFrames(pcs[:n])

	var stackTrace string
	for {
		frame, more := frames.Next()
		stackTrace += fmt.Sprintf("%s\n\t%s:%d\n", frame.Function, frame.File, frame.Line)
		if !more {
			break
		}
	}
	return stackTrace
}

```

`internal/jobexecutor/job_executor_test.go`:

```go
package jobexecutor

import (
	"context"
	"errors"
	"strings"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/middlewarelookup"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/riverinternaltest/retrypolicytest"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riverpilot"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

// customizableWorkUnit is a wrapper around a workUnit that allows for customization
// of the workUnit.  Unlike in other packages, this one does not make use of any
// types from the top level river package (like `river.Job[T]`).
type customizableWorkUnit struct {
	middleware []rivertype.WorkerMiddleware
	nextRetry  func() time.Time
	timeout    time.Duration
	work       func() error
}

func (w *customizableWorkUnit) HookLookup(lookup *hooklookup.JobHookLookup) hooklookup.HookLookupInterface {
	return hooklookup.NewHookLookup(nil)
}

func (w *customizableWorkUnit) Middleware() []rivertype.WorkerMiddleware {
	return w.middleware
}

func (w *customizableWorkUnit) NextRetry() time.Time {
	if w.nextRetry != nil {
		return w.nextRetry()
	}
	return time.Time{}
}

func (w *customizableWorkUnit) Timeout() time.Duration {
	return w.timeout
}

func (w *customizableWorkUnit) UnmarshalJob() error {
	return nil
}

func (w *customizableWorkUnit) Work(ctx context.Context) error {
	return w.work()
}

type workUnitFactory struct {
	workUnit *customizableWorkUnit
}

func (w *workUnitFactory) MakeUnit(jobRow *rivertype.JobRow) workunit.WorkUnit {
	return w.workUnit
}

// Makes a workerInfo using the real workerWrapper with a job that uses a
// callback Work func and allows for customizable maxAttempts and nextRetry.
func newWorkUnitFactoryWithCustomRetry(f func() error, nextRetry func() time.Time) workunit.WorkUnitFactory {
	return &workUnitFactory{
		workUnit: &customizableWorkUnit{
			work:      f,
			nextRetry: nextRetry,
		},
	}
}

type testErrorHandler struct {
	HandleErrorCalled bool
	HandleErrorFunc   func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult

	HandlePanicCalled bool
	HandlePanicFunc   func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult
}

// Test handler with no-ops for both error handling functions.
func newTestErrorHandler() *testErrorHandler {
	return &testErrorHandler{
		HandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult { return nil },
		HandlePanicFunc: func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
			return nil
		},
	}
}

func (h *testErrorHandler) HandleError(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {
	h.HandleErrorCalled = true
	return h.HandleErrorFunc(ctx, job, err)
}

func (h *testErrorHandler) HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
	h.HandlePanicCalled = true
	return h.HandlePanicFunc(ctx, job, panicVal, trace)
}

func TestJobExecutor_Execute(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		completer    *jobcompleter.InlineCompleter
		exec         riverdriver.Executor
		errorHandler *testErrorHandler
		jobRow       *rivertype.JobRow
		updateCh     <-chan []jobcompleter.CompleterJobUpdated
	}

	setup := func(t *testing.T) (*JobExecutor, *testBundle) {
		t.Helper()

		var (
			tx        = riverinternaltest.TestTx(ctx, t)
			archetype = riversharedtest.BaseServiceArchetype(t)
			exec      = riverpgxv5.New(nil).UnwrapExecutor(tx)
			updateCh  = make(chan []jobcompleter.CompleterJobUpdated, 10)
			completer = jobcompleter.NewInlineCompleter(archetype, exec, &riverpilot.StandardPilot{}, updateCh)
		)

		t.Cleanup(completer.Stop)

		workUnitFactory := newWorkUnitFactoryWithCustomRetry(func() error { return nil }, nil)

		now := time.Now().UTC()
		results, err := exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{{
			EncodedArgs: []byte("{}"),
			Kind:        "jobexecutor_test",
			MaxAttempts: rivercommon.MaxAttemptsDefault,
			Priority:    rivercommon.PriorityDefault,
			Queue:       rivercommon.QueueDefault,
			// Needs to be explicitly set to a "now" horizon that's aligned with the
			// JobGetAvailable call. InsertMany applies a default scheduled_at in Go
			// so it can't pick up the Postgres-level `now()` default.
			ScheduledAt: ptrutil.Ptr(now),
			State:       rivertype.JobStateAvailable,
		}})
		require.NoError(t, err)

		// Fetch the job to make sure it's marked as running:
		jobs, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
			Max:   1,
			Now:   ptrutil.Ptr(now),
			Queue: rivercommon.QueueDefault,
		})
		require.NoError(t, err)

		require.Len(t, jobs, 1)
		require.Equal(t, results[0].Job.ID, jobs[0].ID)
		job := jobs[0]

		bundle := &testBundle{
			completer:    completer,
			exec:         exec,
			errorHandler: newTestErrorHandler(),
			jobRow:       job,
			updateCh:     updateCh,
		}

		// allocate this context just so we can set the CancelFunc:
		_, cancel := context.WithCancelCause(ctx)
		t.Cleanup(func() { cancel(nil) })

		executor := baseservice.Init(archetype, &JobExecutor{
			CancelFunc:               cancel,
			ClientRetryPolicy:        &retrypolicytest.RetryPolicyNoJitter{},
			Completer:                bundle.completer,
			DefaultClientRetryPolicy: &retrypolicytest.RetryPolicyNoJitter{},
			ErrorHandler:             bundle.errorHandler,
			HookLookupByJob:          hooklookup.NewJobHookLookup(),
			HookLookupGlobal:         hooklookup.NewHookLookup(nil),
			InformProducerDoneFunc:   func(job *rivertype.JobRow) {},
			JobRow:                   bundle.jobRow,
			MiddlewareLookupGlobal:   middlewarelookup.NewMiddlewareLookup(nil),
			SchedulerInterval:        riverinternaltest.SchedulerShortInterval,
			WorkUnit:                 workUnitFactory.MakeUnit(bundle.jobRow),
		})

		return executor, bundle
	}

	t.Run("Success", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		// A simple `return nil` occasionally clocks in at exactly 0s of run
		// duration and fails the assertion on non-zero below. To avoid that,
		// make sure we sleep a tiny amount of time.
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error {
			time.Sleep(1 * time.Microsecond)
			return nil
		}, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		jobUpdates := riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCompleted, job.State)

		require.Len(t, jobUpdates, 1)
		jobUpdate := jobUpdates[0]
		t.Logf("Job statistics: %+v", jobUpdate.JobStats)
		require.NotZero(t, jobUpdate.JobStats.CompleteDuration)
		require.NotZero(t, jobUpdate.JobStats.QueueWaitDuration)
		require.NotZero(t, jobUpdate.JobStats.RunDuration)

		select {
		case <-bundle.updateCh:
			t.Fatalf("unexpected job update: %+v", jobUpdate)
		default:
		}
	})

	t.Run("FirstError", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		now := executor.Archetype.Time.StubNowUTC(time.Now().UTC())

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateRetryable, job.State)
		require.Len(t, job.Errors, 1)
		require.Equal(t, now.Truncate(1*time.Microsecond), job.Errors[0].At.Truncate(1*time.Microsecond))
		require.Equal(t, 1, job.Errors[0].Attempt)
		require.Equal(t, "job error", job.Errors[0].Error)
		require.Equal(t, "", job.Errors[0].Trace)
	})

	t.Run("ErrorAgainAfterRetry", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		bundle.jobRow.Attempt = 2

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateRetryable, job.State)
	})

	t.Run("ErrorSetsJobAvailableBelowSchedulerIntervalThreshold", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		executor.SchedulerInterval = 3 * time.Second

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)

		{
			executor.Execute(ctx)
			riversharedtest.WaitOrTimeout(t, bundle.updateCh)

			job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
			require.NoError(t, err)
			require.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)
			require.Equal(t, rivertype.JobStateAvailable, job.State)
		}

		_, err := bundle.exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{
			ID:            bundle.jobRow.ID,
			StateDoUpdate: true,
			State:         rivertype.JobStateRunning,
		})
		require.NoError(t, err)

		bundle.jobRow.Attempt = 2

		{
			executor.Execute(ctx)
			riversharedtest.WaitOrTimeout(t, bundle.updateCh)

			job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
			require.NoError(t, err)
			require.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 16*time.Second)
			require.Equal(t, rivertype.JobStateRetryable, job.State)
		}
	})

	t.Run("ErrorDiscardsJobAfterTooManyAttempts", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		bundle.jobRow.Attempt = bundle.jobRow.MaxAttempts

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), *job.FinalizedAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateDiscarded, job.State)
	})

	t.Run("JobCancelErrorCancelsJobEvenWithRemainingAttempts", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		// ensure we still have remaining attempts:
		require.Greater(t, bundle.jobRow.MaxAttempts, bundle.jobRow.Attempt)

		// add a unique key so we can verify it's cleared
		var err error
		bundle.jobRow, err = bundle.exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{
			ID:    bundle.jobRow.ID,
			State: rivertype.JobStateAvailable, // required for encoding but ignored
		})
		require.NoError(t, err)

		cancelErr := rivertype.JobCancel(errors.New("throw away this job"))
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return cancelErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), *job.FinalizedAt, 2*time.Second)
		require.Equal(t, rivertype.JobStateCancelled, job.State)
		require.Nil(t, job.UniqueKey)
		require.Len(t, job.Errors, 1)
		require.WithinDuration(t, time.Now(), job.Errors[0].At, 2*time.Second)
		require.Equal(t, 1, job.Errors[0].Attempt)
		require.Equal(t, "JobCancelError: throw away this job", job.Errors[0].Error)
		require.Equal(t, "", job.Errors[0].Trace)
	})

	t.Run("JobSnoozeErrorReschedulesJobAndDecrementsAttempt", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)
		attemptBefore := bundle.jobRow.Attempt

		cancelErr := &rivertype.JobSnoozeError{Duration: 30 * time.Minute}
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return cancelErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateScheduled, job.State)
		require.WithinDuration(t, time.Now().Add(30*time.Minute), job.ScheduledAt, 2*time.Second)
		require.Equal(t, attemptBefore-1, job.Attempt)
		require.Empty(t, job.Errors)
	})

	t.Run("JobSnoozeErrorInNearFutureMakesJobAvailableAndDecrementsAttempt", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)
		attemptBefore := bundle.jobRow.Attempt

		cancelErr := &rivertype.JobSnoozeError{Duration: time.Millisecond}
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return cancelErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateAvailable, job.State)
		require.WithinDuration(t, time.Now(), job.ScheduledAt, 2*time.Second)
		require.Equal(t, attemptBefore-1, job.Attempt)
		require.Empty(t, job.Errors)
	})

	t.Run("ErrorWithCustomRetryPolicy", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)
		executor.ClientRetryPolicy = &retrypolicytest.RetryPolicyCustom{}

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateRetryable, job.State)
	})

	t.Run("ErrorWithCustomNextRetryReturnedFromWorker", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		workerErr := errors.New("job error")
		nextRetryAt := time.Now().Add(1 * time.Hour).UTC()
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, func() time.Time {
			return nextRetryAt
		}).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateRetryable, job.State)
		require.WithinDuration(t, nextRetryAt, job.ScheduledAt, time.Microsecond)
	})

	t.Run("InvalidNextRetryAt", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)
		executor.ClientRetryPolicy = &retrypolicytest.RetryPolicyInvalid{}

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, executor.DefaultClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateRetryable, job.State)
	})

	t.Run("ErrorWithErrorHandler", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)
		bundle.errorHandler.HandleErrorFunc = func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {
			require.Equal(t, workerErr, err)
			return nil
		}

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateRetryable, job.State)

		require.True(t, bundle.errorHandler.HandleErrorCalled)
	})

	t.Run("ErrorWithErrorHandlerSetCancelled", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)
		bundle.errorHandler.HandleErrorFunc = func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {
			return &ErrorHandlerResult{SetCancelled: true}
		}

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCancelled, job.State)

		require.True(t, bundle.errorHandler.HandleErrorCalled)
	})

	t.Run("ErrorWithErrorHandlerPanic", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		workerErr := errors.New("job error")
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return workerErr }, nil).MakeUnit(bundle.jobRow)
		bundle.errorHandler.HandleErrorFunc = func(ctx context.Context, job *rivertype.JobRow, err error) *ErrorHandlerResult {
			panic("error handled panicked!")
		}

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateRetryable, job.State)

		require.True(t, bundle.errorHandler.HandleErrorCalled)
	})

	t.Run("Panic", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic("panic val") }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateRetryable, job.State)
		require.Len(t, job.Errors, 1)
		// Sufficient enough to ensure that the stack trace is included:
		require.Contains(t, job.Errors[0].Trace, "river/internal/jobexecutor/job_executor.go")
	})

	t.Run("PanicAgainAfterRetry", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		bundle.jobRow.Attempt = 2

		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic("panic val") }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, executor.ClientRetryPolicy.NextRetry(bundle.jobRow), job.ScheduledAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateRetryable, job.State)
	})

	t.Run("PanicDiscardsJobAfterTooManyAttempts", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		bundle.jobRow.Attempt = bundle.jobRow.MaxAttempts

		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic("panic val") }, nil).MakeUnit(bundle.jobRow)

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), *job.FinalizedAt, 1*time.Second)
		require.Equal(t, rivertype.JobStateDiscarded, job.State)
	})

	t.Run("PanicWithPanicHandler", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		// Add a middleware so we can verify it's in the trace too:
		executor.MiddlewareLookupGlobal = middlewarelookup.NewMiddlewareLookup([]rivertype.Middleware{
			&testMiddleware{
				work: func(ctx context.Context, job *rivertype.JobRow, next func(context.Context) error) error {
					return next(ctx)
				},
			},
		})

		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error {
			panic("panic val")
		}, nil).MakeUnit(bundle.jobRow)
		bundle.errorHandler.HandlePanicFunc = func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
			require.Equal(t, "panic val", panicVal)
			require.NotContains(t, trace, "runtime/debug.Stack()\n")
			require.Contains(t, trace, "(*testMiddleware).Work")
			// Ensure that the first frame (i.e. the file and line info) corresponds
			// to the code that raised the panic. This ensures we've stripped out
			// irrelevant frames like the ones from the runtime package which
			// generated the trace, or the panic rescuing code.
			lines := strings.Split(trace, "\n")
			require.GreaterOrEqual(t, len(lines), 2, "expected at least one frame in the stack trace")
			firstFrame := lines[1] // this line contains the file and line of the panic origin
			require.Contains(t, firstFrame, "job_executor_test.go")

			return nil
		}

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateRetryable, job.State)

		require.True(t, bundle.errorHandler.HandlePanicCalled)
	})

	t.Run("PanicWithPanicHandlerSetCancelled", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic("panic val") }, nil).MakeUnit(bundle.jobRow)
		bundle.errorHandler.HandlePanicFunc = func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
			return &ErrorHandlerResult{SetCancelled: true}
		}

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCancelled, job.State)

		require.True(t, bundle.errorHandler.HandlePanicCalled)
	})

	t.Run("PanicWithPanicHandlerPanic", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { panic("panic val") }, nil).MakeUnit(bundle.jobRow)
		bundle.errorHandler.HandlePanicFunc = func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *ErrorHandlerResult {
			panic("panic handler panicked!")
		}

		executor.Execute(ctx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		job, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateRetryable, job.State)

		require.True(t, bundle.errorHandler.HandlePanicCalled)
	})

	t.Run("CancelFuncCleanedUpEvenWithoutCancel", func(t *testing.T) {
		t.Parallel()

		executor, bundle := setup(t)

		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error { return nil }, nil).MakeUnit(bundle.jobRow)

		workCtx, cancelFunc := context.WithCancelCause(ctx)
		executor.CancelFunc = cancelFunc

		executor.Execute(workCtx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		require.ErrorIs(t, context.Cause(workCtx), errExecutorDefaultCancel)
	})

	runCancelTest := func(t *testing.T, returnErr error) *rivertype.JobRow { //nolint:thelper
		executor, bundle := setup(t)

		// ensure we still have remaining attempts:
		require.Greater(t, bundle.jobRow.MaxAttempts, bundle.jobRow.Attempt)

		jobStarted := make(chan struct{})
		haveCancelled := make(chan struct{})
		executor.WorkUnit = newWorkUnitFactoryWithCustomRetry(func() error {
			close(jobStarted)
			<-haveCancelled
			return returnErr
		}, nil).MakeUnit(bundle.jobRow)

		go func() {
			<-jobStarted
			executor.Cancel()
			close(haveCancelled)
		}()

		workCtx, cancelFunc := context.WithCancelCause(ctx)
		executor.CancelFunc = cancelFunc
		t.Cleanup(func() { cancelFunc(nil) })

		executor.Execute(workCtx)
		riversharedtest.WaitOrTimeout(t, bundle.updateCh)

		jobRow, err := bundle.exec.JobGetByID(ctx, bundle.jobRow.ID)
		require.NoError(t, err)
		return jobRow
	}

	t.Run("RemoteCancellationViaCancel", func(t *testing.T) {
		t.Parallel()

		job := runCancelTest(t, errors.New("a non-nil error"))

		require.WithinDuration(t, time.Now(), *job.FinalizedAt, 2*time.Second)
		require.Equal(t, rivertype.JobStateCancelled, job.State)
		require.Len(t, job.Errors, 1)
		require.WithinDuration(t, time.Now(), job.Errors[0].At, 2*time.Second)
		require.Equal(t, 1, job.Errors[0].Attempt)
		require.Equal(t, "JobCancelError: job cancelled remotely", job.Errors[0].Error)
		require.Equal(t, rivertype.ErrJobCancelledRemotely.Error(), job.Errors[0].Error)
		require.Equal(t, "", job.Errors[0].Trace)
	})

	t.Run("RemoteCancellationJobNotCancelledIfNoErrorReturned", func(t *testing.T) {
		t.Parallel()

		job := runCancelTest(t, nil)

		require.WithinDuration(t, time.Now(), *job.FinalizedAt, 2*time.Second)
		require.Equal(t, rivertype.JobStateCompleted, job.State)
		require.Empty(t, job.Errors)
	})
}

type testMiddleware struct {
	work func(ctx context.Context, job *rivertype.JobRow, next func(context.Context) error) error
}

func (m *testMiddleware) IsMiddleware() bool { return true }

func (m *testMiddleware) Work(ctx context.Context, job *rivertype.JobRow, next func(context.Context) error) error {
	return m.work(ctx, job, next)
}

```

`internal/jobstats/job_statistics.go`:

```go
package jobstats

import "time"

// JobStatistics contains information about a single execution of a job.
//
// This type has an identical one in the top-level package. The reason for that
// is so that we can use statistics from subpackages, but can reveal all
// public-facing River types in a single public-facing package.
type JobStatistics struct {
	CompleteDuration  time.Duration // Time it took to set the job completed, discarded, or errored.
	QueueWaitDuration time.Duration // Time the job spent waiting in available state before starting execution.
	RunDuration       time.Duration // Time job spent running (measured around job worker.)
}

```

`internal/leadership/elector.go`:

```go
package leadership

import (
	"context"
	"encoding/json"
	"errors"
	"log/slog"
	"strings"
	"sync"
	"time"

	"github.com/riverqueue/river/internal/notifier"
	"github.com/riverqueue/river/internal/util/dbutil"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
)

const (
	electIntervalDefault           = 5 * time.Second
	electIntervalJitterDefault     = 1 * time.Second
	electIntervalTTLPaddingDefault = 10 * time.Second
)

type dbLeadershipNotification struct {
	Action   string `json:"action"`
	LeaderID string `json:"leader_id"`
}

type Notification struct {
	IsLeader  bool
	Timestamp time.Time
}

type Subscription struct {
	creationTime time.Time
	ch           chan *Notification

	unlistenOnce *sync.Once
	e            *Elector
}

func (s *Subscription) C() <-chan *Notification {
	return s.ch
}

func (s *Subscription) Unlisten() {
	s.unlistenOnce.Do(func() {
		s.e.unlisten(s)
	})
}

// Test-only properties.
type electorTestSignals struct {
	DeniedLeadership     testsignal.TestSignal[struct{}] // notifies when elector fails to gain leadership
	GainedLeadership     testsignal.TestSignal[struct{}] // notifies when elector gains leadership
	LostLeadership       testsignal.TestSignal[struct{}] // notifies when an elected leader loses leadership
	MaintainedLeadership testsignal.TestSignal[struct{}] // notifies when elector maintains leadership
	ResignedLeadership   testsignal.TestSignal[struct{}] // notifies when elector resigns leadership
}

func (ts *electorTestSignals) Init() {
	ts.DeniedLeadership.Init()
	ts.GainedLeadership.Init()
	ts.LostLeadership.Init()
	ts.MaintainedLeadership.Init()
	ts.ResignedLeadership.Init()
}

type Config struct {
	ClientID            string
	ElectInterval       time.Duration // period on which each elector attempts elect even without having received a resignation notification
	ElectIntervalJitter time.Duration
}

func (c *Config) mustValidate() *Config {
	if c.ClientID == "" {
		panic("Config.ClientID must be non-empty")
	}
	if c.ElectInterval <= 0 {
		panic("Config.ElectInterval must be above zero")
	}

	return c
}

type Elector struct {
	baseservice.BaseService
	startstop.BaseStartStop

	config                     *Config
	exec                       riverdriver.Executor
	leadershipNotificationChan chan struct{}
	notifier                   *notifier.Notifier
	testSignals                electorTestSignals

	mu            sync.Mutex
	isLeader      bool
	subscriptions []*Subscription
}

// NewElector returns an Elector using the given adapter. The name should correspond
// to the name of the database + schema combo and should be shared across all Clients
// running with that combination. The id should be unique to the Client.
func NewElector(archetype *baseservice.Archetype, exec riverdriver.Executor, notifier *notifier.Notifier, config *Config) *Elector {
	return baseservice.Init(archetype, &Elector{
		config: (&Config{
			ClientID:            config.ClientID,
			ElectInterval:       valutil.ValOrDefault(config.ElectInterval, electIntervalDefault),
			ElectIntervalJitter: valutil.ValOrDefault(config.ElectIntervalJitter, electIntervalJitterDefault),
		}).mustValidate(),
		exec:     exec,
		notifier: notifier,
	})
}

func (e *Elector) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := e.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	// We'll send to this channel anytime a leader resigns on the key with `name`
	e.leadershipNotificationChan = make(chan struct{})

	var sub *notifier.Subscription
	if e.notifier == nil {
		e.Logger.DebugContext(ctx, e.Name+": No notifier configured; starting in poll mode", "client_id", e.config.ClientID)
	} else {
		e.Logger.DebugContext(ctx, e.Name+": Listening for leadership changes", "client_id", e.config.ClientID, "topic", notifier.NotificationTopicLeadership)
		var err error
		sub, err = e.notifier.Listen(ctx, notifier.NotificationTopicLeadership, func(topic notifier.NotificationTopic, payload string) {
			e.handleLeadershipNotification(ctx, topic, payload)
		})
		if err != nil {
			stopped()
			if strings.HasSuffix(err.Error(), "conn closed") || errors.Is(err, context.Canceled) {
				return nil
			}
			return err
		}
	}

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		e.Logger.DebugContext(ctx, e.Name+": Run loop started")
		defer e.Logger.DebugContext(ctx, e.Name+": Run loop stopped")

		if sub != nil {
			defer sub.Unlisten(ctx)
		}

		for {
			if err := e.attemptGainLeadershipLoop(ctx); err != nil {
				// Function above only returns an error if context was cancelled
				// or overall context is done.
				if !errors.Is(err, context.Canceled) && ctx.Err() == nil {
					panic(err)
				}
				return
			}

			e.Logger.DebugContext(ctx, e.Name+": Gained leadership", "client_id", e.config.ClientID)
			e.testSignals.GainedLeadership.Signal(struct{}{})

			err := e.keepLeadershipLoop(ctx)
			if err != nil {
				if errors.Is(err, context.Canceled) {
					return
				}

				if errors.Is(err, errLostLeadershipReelection) {
					continue // lost leadership reelection; unusual but not a problem; don't log
				}

				e.Logger.ErrorContext(ctx, e.Name+": Error keeping leadership", "client_id", e.config.ClientID, "err", err)
			}
		}
	}()

	return nil
}

func (e *Elector) attemptGainLeadershipLoop(ctx context.Context) error {
	var attempt int
	for {
		attempt++
		e.Logger.DebugContext(ctx, e.Name+": Attempting to gain leadership", "client_id", e.config.ClientID)

		elected, err := attemptElectOrReelect(ctx, e.exec, false, &riverdriver.LeaderElectParams{
			LeaderID: e.config.ClientID,
			TTL:      e.leaderTTL(),
		})
		if err != nil {
			if errors.Is(err, context.Canceled) || ctx.Err() != nil {
				return err
			}

			sleepDuration := serviceutil.ExponentialBackoff(attempt, serviceutil.MaxAttemptsBeforeResetDefault)
			e.Logger.ErrorContext(ctx, e.Name+": Error attempting to elect", e.errorSlogArgs(err, attempt, sleepDuration)...)
			serviceutil.CancellableSleep(ctx, sleepDuration)
			continue
		}
		if elected {
			return nil
		}

		attempt = 0

		e.Logger.DebugContext(ctx, e.Name+": Leadership bid was unsuccessful (not an error)", "client_id", e.config.ClientID)
		e.testSignals.DeniedLeadership.Signal(struct{}{})

		select {
		// TODO: This could potentially leak memory / timers if we're seeing a ton
		// of resignations. May want to make this reusable & cancel it when retrying?
		// We may also want to consider a specialized ticker utility that can tick
		// within a random range.
		case <-serviceutil.CancellableSleepC(ctx, randutil.DurationBetween(e.config.ElectInterval, e.config.ElectInterval+e.config.ElectIntervalJitter)):
			if ctx.Err() != nil { // context done
				return ctx.Err()
			}

		case <-e.leadershipNotificationChan:
			// Somebody just resigned, try to win the next election after a very
			// short random interval (to prevent all clients from bidding at once).
			serviceutil.CancellableSleep(ctx, randutil.DurationBetween(0, 50*time.Millisecond))
		}
	}
}

// Handles a leadership notification from the notifier.
func (e *Elector) handleLeadershipNotification(ctx context.Context, topic notifier.NotificationTopic, payload string) {
	if topic != notifier.NotificationTopicLeadership {
		// This should not happen unless the notifier is broken.
		e.Logger.ErrorContext(ctx, e.Name+": Received unexpected notification", "client_id", e.config.ClientID, "topic", topic, "payload", payload)
		return
	}

	notification := dbLeadershipNotification{}
	if err := json.Unmarshal([]byte(payload), &notification); err != nil {
		e.Logger.ErrorContext(ctx, e.Name+": Unable to unmarshal leadership notification", "client_id", e.config.ClientID, "err", err)
		return
	}

	e.Logger.DebugContext(ctx, e.Name+": Received notification from notifier", "action", notification.Action, "client_id", e.config.ClientID)

	if notification.Action != "resigned" {
		// We only care about resignations because we use them to preempt the
		// election attempt backoff.
		return
	}

	// If this a resignation from _this_ client, ignore the change.
	if notification.LeaderID == e.config.ClientID {
		return
	}

	// Do an initial context check so in case context is done, it always takes
	// precedence over sending a leadership notification.
	if ctx.Err() != nil {
		return
	}

	select {
	case <-ctx.Done():
	case e.leadershipNotificationChan <- struct{}{}:
	}
}

var errLostLeadershipReelection = errors.New("lost leadership with no error")

func (e *Elector) keepLeadershipLoop(ctx context.Context) error {
	// notify all subscribers that we're the leader
	e.notifySubscribers(true)

	// Defer is LIFO. This will run after the resign below.
	defer e.notifySubscribers(false)

	var lostLeadership bool

	// Before the elector returns, run a delete with NOTIFY to give up any
	// leadership that we have. If we do that here, we guarantee that any locks
	// we have will be released (even if they were acquired in
	// attemptGainLeadership but we didn't wait for the response)
	//
	// This doesn't use ctx because it runs *after* the ctx is done.
	defer func() {
		if !lostLeadership {
			e.attemptResignLoop(ctx) // will resign using WithoutCancel context, but ctx sent for logging
		}
	}()

	const maxNumErrors = 5

	var (
		numErrors = 0
		timer     = time.NewTimer(0) // reset immediately below
	)
	<-timer.C

	for {
		timer.Reset(e.config.ElectInterval)

		select {
		case <-ctx.Done():
			if !timer.Stop() {
				<-timer.C
			}

			return ctx.Err()

		case <-timer.C:
			// Reelect timer expired; attempt reelection below.

		case <-e.leadershipNotificationChan:
			// Used only in tests for force an immediately reelect attempt.

			if !timer.Stop() {
				<-timer.C
			}
		}

		e.Logger.DebugContext(ctx, e.Name+": Current leader attempting reelect", "client_id", e.config.ClientID)

		reelected, err := attemptElectOrReelect(ctx, e.exec, true, &riverdriver.LeaderElectParams{
			LeaderID: e.config.ClientID,
			TTL:      e.leaderTTL(),
		})
		if err != nil {
			if errors.Is(err, context.Canceled) {
				return err
			}

			numErrors++
			if numErrors >= maxNumErrors {
				return err
			}

			sleepDuration := serviceutil.ExponentialBackoff(numErrors, serviceutil.MaxAttemptsBeforeResetDefault)
			e.Logger.Error(e.Name+": Error attempting reelection", e.errorSlogArgs(err, numErrors, sleepDuration)...)
			serviceutil.CancellableSleep(ctx, sleepDuration)
			continue
		}
		if !reelected {
			lostLeadership = true
			e.testSignals.LostLeadership.Signal(struct{}{})
			return errLostLeadershipReelection
		}

		numErrors = 0
		e.testSignals.MaintainedLeadership.Signal(struct{}{})
	}
}

// Try up to 3 times to give up any currently held leadership.
//
// The context received is used for logging purposes, but the function actually
// makes use of a background context to try and guarantee that leadership is
// always surrendered in a timely manner so it can be picked up quickly by
// another client, even in the event of a cancellation.
func (e *Elector) attemptResignLoop(ctx context.Context) {
	e.Logger.DebugContext(ctx, e.Name+": Attempting to resign leadership", "client_id", e.config.ClientID)

	// Make a good faith attempt to resign, even in the presence of errors, but
	// don't keep hammering if it doesn't work. In case a resignation failure,
	// leader TTLs will act as an additional hedge to ensure a new leader can
	// still be elected.
	const maxNumErrors = 3

	// This does not inherit the parent context's cancellation because we want to
	// give up leadership even during a shutdown. There is no way to short-circuit
	// this, though there are timeouts per call within attemptResign.
	ctx = context.WithoutCancel(ctx)

	for attempt := 1; attempt <= maxNumErrors; attempt++ {
		if err := e.attemptResign(ctx, attempt); err != nil {
			sleepDuration := serviceutil.ExponentialBackoff(attempt, serviceutil.MaxAttemptsBeforeResetDefault)
			e.Logger.ErrorContext(ctx, e.Name+": Error attempting to resign", e.errorSlogArgs(err, attempt, sleepDuration)...)
			serviceutil.CancellableSleep(ctx, sleepDuration)

			continue
		}

		return
	}
}

// attemptResign attempts to resign any currently held leaderships for the
// elector's name and leader ID.
func (e *Elector) attemptResign(ctx context.Context, attempt int) error {
	// Wait one second longer each time we try to resign:
	timeout := time.Duration(attempt) * time.Second

	ctx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()

	resigned, err := e.exec.LeaderResign(ctx, &riverdriver.LeaderResignParams{
		LeaderID:        e.config.ClientID,
		LeadershipTopic: string(notifier.NotificationTopicLeadership),
	})
	if err != nil {
		return err
	}

	if resigned {
		e.Logger.DebugContext(ctx, e.Name+": Resigned leadership successfully", "client_id", e.config.ClientID)
		e.testSignals.ResignedLeadership.Signal(struct{}{})
	}

	return nil
}

// Produces a common set of key/value pairs for logging when an error occurs.
//
// Refactored out because we had three repeats of identical information in this
// file, but if it causes things to get messy, may want to refactor again.
func (e *Elector) errorSlogArgs(err error, attempt int, sleepDuration time.Duration) []any {
	return []any{
		slog.Int("attempt", attempt),
		slog.String("client_id", e.config.ClientID),
		slog.String("err", err.Error()),
		slog.String("sleep_duration", sleepDuration.String()),
	}
}

func (e *Elector) Listen() *Subscription {
	sub := &Subscription{
		creationTime: time.Now().UTC(),
		ch:           make(chan *Notification, 1),
		e:            e,
		unlistenOnce: &sync.Once{},
	}

	e.mu.Lock()
	defer e.mu.Unlock()

	initialNotification := &Notification{
		IsLeader:  e.isLeader,
		Timestamp: sub.creationTime,
	}
	sub.ch <- initialNotification

	e.subscriptions = append(e.subscriptions, sub)
	return sub
}

func (e *Elector) unlisten(sub *Subscription) {
	success := e.tryUnlisten(sub)
	if !success {
		panic("BUG: tried to unlisten for subscription not in list")
	}
}

// needs to be in a separate method so the defer will cleanly unlock the mutex,
// even if we panic.
func (e *Elector) tryUnlisten(sub *Subscription) bool {
	e.mu.Lock()
	defer e.mu.Unlock()

	for i, s := range e.subscriptions {
		if s.creationTime.Equal(sub.creationTime) {
			e.subscriptions = append(e.subscriptions[:i], e.subscriptions[i+1:]...)
			return true
		}
	}
	return false
}

// leaderTTL is at least the reelect run interval used by clients to try and gain
// leadership or reelect themselves as leader, plus a little padding to account
// to give the leader a little breathing room in its reelection loop.
func (e *Elector) leaderTTL() time.Duration {
	return e.config.ElectInterval + electIntervalTTLPaddingDefault
}

func (e *Elector) notifySubscribers(isLeader bool) {
	notifyTime := time.Now().UTC()
	e.mu.Lock()
	defer e.mu.Unlock()

	e.isLeader = isLeader

	for _, s := range e.subscriptions {
		s.ch <- &Notification{
			IsLeader:  isLeader,
			Timestamp: notifyTime,
		}
	}
}

const deadlineTimeout = 5 * time.Second

// attemptElectOrReelect attempts to elect a leader for the given name. The
// bool alreadyElected indicates whether this is a potential reelection of
// an already-elected leader. If the election is successful because there is
// no leader or the previous leader expired, the provided leaderID will be
// set as the new leader with a TTL of ttl.
//
// Returns whether this leader was successfully elected or an error if one
// occurred.
func attemptElectOrReelect(ctx context.Context, exec riverdriver.Executor, alreadyElected bool, params *riverdriver.LeaderElectParams) (bool, error) {
	ctx, cancel := context.WithTimeout(ctx, deadlineTimeout)
	defer cancel()

	return dbutil.WithTxV(ctx, exec, func(ctx context.Context, exec riverdriver.ExecutorTx) (bool, error) {
		if _, err := exec.LeaderDeleteExpired(ctx); err != nil {
			return false, err
		}

		var (
			elected bool
			err     error
		)
		if alreadyElected {
			elected, err = exec.LeaderAttemptReelect(ctx, params)
		} else {
			elected, err = exec.LeaderAttemptElect(ctx, params)
		}
		if err != nil {
			return false, err
		}

		return elected, nil
	})
}

```

`internal/leadership/elector_test.go`:

```go
package leadership

import (
	"context"
	"encoding/json"
	"log/slog"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/notifier"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/riverinternaltest/sharedtx"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

func TestElector_PollOnly(t *testing.T) {
	t.Parallel()

	var (
		ctx    = context.Background()
		driver = riverpgxv5.New(nil)
	)

	type electorBundle struct {
		tx pgx.Tx
	}

	testElector(ctx, t,
		func(t *testing.T) *electorBundle {
			t.Helper()

			tx := riverinternaltest.TestTx(ctx, t)

			// We'll put multiple electors on one transaction. Make sure they can
			// live with each other in relative harmony.
			tx = sharedtx.NewSharedTx(tx)

			return &electorBundle{
				tx: tx,
			}
		},
		func(t *testing.T, electorBundle *electorBundle) *Elector {
			t.Helper()

			return NewElector(
				riversharedtest.BaseServiceArchetype(t),
				driver.UnwrapExecutor(electorBundle.tx),
				nil,
				&Config{ClientID: "test_client_id"},
			)
		})
}

func TestElector_WithNotifier(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type electorBundle struct {
		archetype *baseservice.Archetype
		exec      riverdriver.Executor
		notifier  *notifier.Notifier
	}

	testElector(ctx, t,
		func(t *testing.T) *electorBundle {
			t.Helper()

			var (
				archetype = riversharedtest.BaseServiceArchetype(t)
				dbPool    = riverinternaltest.TestDB(ctx, t)
				driver    = riverpgxv5.New(dbPool)
			)

			notifier := notifier.New(archetype, driver.GetListener())
			{
				require.NoError(t, notifier.Start(ctx))
				t.Cleanup(notifier.Stop)
			}

			return &electorBundle{
				archetype: archetype,
				exec:      driver.GetExecutor(),
				notifier:  notifier,
			}
		},
		func(t *testing.T, electorBundle *electorBundle) *Elector {
			t.Helper()

			return NewElector(
				electorBundle.archetype,
				electorBundle.exec,
				electorBundle.notifier,
				&Config{ClientID: "test_client_id"},
			)
		})
}

// This system of "elector bundles" may appear to be a little convoluted, but
// it's built so that we can initialize multiple electors against a single
// database or transaction.
func testElector[TElectorBundle any](
	ctx context.Context,
	t *testing.T,
	makeElectorBundle func(t *testing.T) TElectorBundle,
	makeElector func(t *testing.T, bundle TElectorBundle) *Elector,
) {
	t.Helper()

	type testBundle struct {
		electorBundle TElectorBundle
		exec          riverdriver.Executor
	}

	setup := func(t *testing.T) (*Elector, *testBundle) {
		t.Helper()

		electorBundle := makeElectorBundle(t)

		elector := makeElector(t, electorBundle)
		elector.testSignals.Init()

		return elector, &testBundle{
			electorBundle: electorBundle,
			exec:          elector.exec,
		}
	}

	startElector := func(ctx context.Context, t *testing.T, elector *Elector) {
		t.Helper()
		t.Logf("Starting %s", elector.config.ClientID)
		require.NoError(t, elector.Start(ctx))
		t.Cleanup(elector.Stop)
	}

	t.Run("StartsGainsLeadershipAndStops", func(t *testing.T) {
		t.Parallel()

		elector, bundle := setup(t)

		startElector(ctx, t, elector)

		elector.testSignals.GainedLeadership.WaitOrTimeout()

		leader, err := bundle.exec.LeaderGetElectedLeader(ctx)
		require.NoError(t, err)
		require.Equal(t, elector.config.ClientID, leader.LeaderID)

		elector.Stop()

		elector.testSignals.ResignedLeadership.WaitOrTimeout()

		_, err = bundle.exec.LeaderGetElectedLeader(ctx)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
	})

	t.Run("NotifiesSubscribers", func(t *testing.T) {
		t.Parallel()

		elector, _ := setup(t)

		sub := elector.Listen()
		t.Cleanup(func() { elector.unlisten(sub) })

		// Drain an initial notification that occurs on Listen.
		notification := riversharedtest.WaitOrTimeout(t, sub.ch)
		require.False(t, notification.IsLeader)

		startElector(ctx, t, elector)

		elector.testSignals.GainedLeadership.WaitOrTimeout()

		notification = riversharedtest.WaitOrTimeout(t, sub.ch)
		require.True(t, notification.IsLeader)

		elector.Stop()

		elector.testSignals.ResignedLeadership.WaitOrTimeout()

		notification = riversharedtest.WaitOrTimeout(t, sub.ch)
		require.False(t, notification.IsLeader)
	})

	t.Run("SustainsLeadership", func(t *testing.T) {
		t.Parallel()

		elector, _ := setup(t)

		startElector(ctx, t, elector)

		elector.testSignals.GainedLeadership.WaitOrTimeout()

		// The leadership maintenance loop also listens on the leadership
		// notification channel. Take advantage of that to cause an
		// immediate reelect attempt with no sleep.
		elector.leadershipNotificationChan <- struct{}{}
		elector.testSignals.MaintainedLeadership.WaitOrTimeout()

		elector.leadershipNotificationChan <- struct{}{}
		elector.testSignals.MaintainedLeadership.WaitOrTimeout()

		elector.leadershipNotificationChan <- struct{}{}
		elector.testSignals.MaintainedLeadership.WaitOrTimeout()

		elector.Stop()

		elector.testSignals.ResignedLeadership.WaitOrTimeout()
	})

	t.Run("LosesLeadership", func(t *testing.T) {
		t.Parallel()

		elector, bundle := setup(t)

		startElector(ctx, t, elector)

		elector.testSignals.GainedLeadership.WaitOrTimeout()

		t.Logf("Force resigning %s", elector.config.ClientID)

		// Artificially force resign the elector and add a new leader record
		// so that it can't be elected again.
		_, err := bundle.exec.LeaderResign(ctx, &riverdriver.LeaderResignParams{
			LeaderID:        elector.config.ClientID,
			LeadershipTopic: string(notifier.NotificationTopicLeadership),
		})
		require.NoError(t, err)

		_ = testfactory.Leader(ctx, t, bundle.exec, &testfactory.LeaderOpts{
			LeaderID: ptrutil.Ptr("other-client-id"),
		})

		elector.leadershipNotificationChan <- struct{}{}
		elector.testSignals.LostLeadership.WaitOrTimeout()

		// Wait for the elector to try and fail to gain leadership so we
		// don't finish the test while it's still operating.
		elector.testSignals.DeniedLeadership.WaitOrTimeout()

		elector.Stop()
	})

	t.Run("CompetingElectors", func(t *testing.T) {
		t.Parallel()

		elector1, bundle := setup(t)
		elector1.config.ClientID = "elector1"

		{
			startElector(ctx, t, elector1)

			// next to avoid any raciness.
			t.Logf("Waiting for %s to gain leadership", elector1.config.ClientID)
			elector1.testSignals.GainedLeadership.WaitOrTimeout()

			leader, err := bundle.exec.LeaderGetElectedLeader(ctx)
			require.NoError(t, err)
			require.Equal(t, elector1.config.ClientID, leader.LeaderID)
		}

		// Make another elector and make sure it's using the same executor.
		elector2 := makeElector(t, bundle.electorBundle)
		elector2.config.ClientID = "elector2"
		elector2.exec = elector1.exec
		elector2.testSignals.Init()

		{
			startElector(ctx, t, elector2)

			elector2.testSignals.DeniedLeadership.WaitOrTimeout()

			t.Logf("Stopping %s", elector1.config.ClientID)
			elector1.Stop()
			elector1.testSignals.ResignedLeadership.WaitOrTimeout()

			// Cheat if we're in poll only by notifying leadership channel to
			// wake the elector from sleep.
			if elector2.notifier == nil {
				elector2.leadershipNotificationChan <- struct{}{}
			}

			t.Logf("Waiting for %s to gain leadership", elector2.config.ClientID)
			elector2.testSignals.GainedLeadership.WaitOrTimeout()

			t.Logf("Stopping %s", elector2.config.ClientID)
			elector2.Stop()
			elector2.testSignals.ResignedLeadership.WaitOrTimeout()
		}

		_, err := bundle.exec.LeaderGetElectedLeader(ctx)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		elector, _ := setup(t)
		elector.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress
		elector.testSignals = electorTestSignals{}     // deinit so channels don't fill

		startstoptest.Stress(ctx, t, elector)
	})
}

func TestAttemptElectOrReelect(t *testing.T) {
	t.Parallel()

	const (
		clientID           = "client-id"
		leaderInstanceName = "default"
		leaderTTL          = 10 * time.Second
	)

	ctx := context.Background()

	type testBundle struct {
		exec   riverdriver.Executor
		logger *slog.Logger
	}

	setup := func(t *testing.T) *testBundle {
		t.Helper()

		driver := riverpgxv5.New(nil)

		return &testBundle{
			exec:   driver.UnwrapExecutor(riverinternaltest.TestTx(ctx, t)),
			logger: riversharedtest.Logger(t),
		}
	}

	t.Run("ElectsLeader", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		elected, err := attemptElectOrReelect(ctx, bundle.exec, false, &riverdriver.LeaderElectParams{
			LeaderID: clientID,
			TTL:      leaderTTL,
		})
		require.NoError(t, err)
		require.True(t, elected) // won election

		leader, err := bundle.exec.LeaderGetElectedLeader(ctx)
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), leader.ElectedAt, 100*time.Millisecond)
		require.WithinDuration(t, time.Now().Add(leaderTTL), leader.ExpiresAt, 100*time.Millisecond)
	})

	t.Run("ReelectsSameLeader", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		leader := testfactory.Leader(ctx, t, bundle.exec, &testfactory.LeaderOpts{
			LeaderID: ptrutil.Ptr(clientID),
		})

		// Re-elect the same leader. Use a larger TTL to see if time is updated,
		// because we are in a test transaction and the time is frozen at the start of
		// the transaction.
		elected, err := attemptElectOrReelect(ctx, bundle.exec, true, &riverdriver.LeaderElectParams{
			LeaderID: clientID,
			TTL:      30 * time.Second,
		})
		require.NoError(t, err)
		require.True(t, elected) // won re-election

		// expires_at should be incremented because this is the same leader that won
		// previously and we specified that we're already elected:
		updatedLeader, err := bundle.exec.LeaderGetElectedLeader(ctx)
		require.NoError(t, err)
		require.Greater(t, updatedLeader.ExpiresAt, leader.ExpiresAt)
	})

	t.Run("CannotElectDifferentLeader", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		leader := testfactory.Leader(ctx, t, bundle.exec, &testfactory.LeaderOpts{
			LeaderID: ptrutil.Ptr(clientID),
		})

		elected, err := attemptElectOrReelect(ctx, bundle.exec, true, &riverdriver.LeaderElectParams{
			LeaderID: "different-client-id",
			TTL:      leaderTTL,
		})
		require.NoError(t, err)
		require.False(t, elected) // lost election

		// The time should not have changed because we specified that we were not
		// already elected, and the elect query is a no-op if there's already a
		// updatedLeader:
		updatedLeader, err := bundle.exec.LeaderGetElectedLeader(ctx)
		require.NoError(t, err)
		require.Equal(t, leader.ExpiresAt, updatedLeader.ExpiresAt)
	})
}

func TestElectorHandleLeadershipNotification(t *testing.T) {
	t.Parallel()

	var (
		ctx    = context.Background()
		driver = riverpgxv5.New(nil)
	)

	type testBundle struct{}

	setup := func(t *testing.T) (*Elector, *testBundle) {
		t.Helper()

		tx := riverinternaltest.TestTx(ctx, t)

		elector := NewElector(
			riversharedtest.BaseServiceArchetype(t),
			driver.UnwrapExecutor(tx),
			nil,
			&Config{ClientID: "test_client_id"},
		)

		// This channel is normally only initialized on start, so we need to
		// create it manually here.
		elector.leadershipNotificationChan = make(chan struct{}, 1)

		return elector, &testBundle{}
	}

	mustMarshalJSON := func(t *testing.T, val any) []byte {
		t.Helper()

		data, err := json.Marshal(val)
		require.NoError(t, err)
		return data
	}

	validLeadershipChange := func() *dbLeadershipNotification {
		t.Helper()

		return &dbLeadershipNotification{
			Action:   "resigned",
			LeaderID: "other-client-id",
		}
	}

	t.Run("SignalsLeadershipChange", func(t *testing.T) {
		t.Parallel()

		elector, _ := setup(t)

		elector.handleLeadershipNotification(ctx, notifier.NotificationTopicLeadership, string(mustMarshalJSON(t, validLeadershipChange())))

		riversharedtest.WaitOrTimeout(t, elector.leadershipNotificationChan)
	})

	t.Run("StopsOnContextDone", func(t *testing.T) {
		t.Parallel()

		elector, _ := setup(t)

		ctx, cancel := context.WithCancel(ctx)
		cancel() // cancel immediately

		elector.handleLeadershipNotification(ctx, notifier.NotificationTopicLeadership, string(mustMarshalJSON(t, validLeadershipChange())))

		require.Empty(t, elector.leadershipNotificationChan)
	})

	t.Run("IgnoresNonResignedAction", func(t *testing.T) {
		t.Parallel()

		elector, _ := setup(t)

		change := validLeadershipChange()
		change.Action = "not_resigned"

		elector.handleLeadershipNotification(ctx, notifier.NotificationTopicLeadership, string(mustMarshalJSON(t, change)))

		require.Empty(t, elector.leadershipNotificationChan)
	})

	t.Run("IgnoresSameClientID", func(t *testing.T) {
		t.Parallel()

		elector, _ := setup(t)

		change := validLeadershipChange()
		change.LeaderID = elector.config.ClientID

		elector.handleLeadershipNotification(ctx, notifier.NotificationTopicLeadership, string(mustMarshalJSON(t, change)))

		require.Empty(t, elector.leadershipNotificationChan)
	})
}

```

`internal/leadership/main_test.go`:

```go
package leadership

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`internal/maintenance/job_cleaner.go`:

```go
package maintenance

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"time"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
)

const (
	CancelledJobRetentionPeriodDefault = 24 * time.Hour
	CompletedJobRetentionPeriodDefault = 24 * time.Hour
	DiscardedJobRetentionPeriodDefault = 7 * 24 * time.Hour
	JobCleanerIntervalDefault          = 30 * time.Second
	JobCleanerTimeoutDefault           = 30 * time.Second
)

// Test-only properties.
type JobCleanerTestSignals struct {
	DeletedBatch testsignal.TestSignal[struct{}] // notifies when runOnce finishes a pass
}

func (ts *JobCleanerTestSignals) Init() {
	ts.DeletedBatch.Init()
}

type JobCleanerConfig struct {
	// CancelledJobRetentionPeriod is the amount of time to keep cancelled jobs
	// around before they're removed permanently.
	CancelledJobRetentionPeriod time.Duration

	// CompletedJobRetentionPeriod is the amount of time to keep completed jobs
	// around before they're removed permanently.
	CompletedJobRetentionPeriod time.Duration

	// DiscardedJobRetentionPeriod is the amount of time to keep cancelled jobs
	// around before they're removed permanently.
	DiscardedJobRetentionPeriod time.Duration

	// Interval is the amount of time to wait between runs of the cleaner.
	Interval time.Duration

	// Timeout of the individual queries in the job cleaner.
	Timeout time.Duration
}

func (c *JobCleanerConfig) mustValidate() *JobCleanerConfig {
	if c.CancelledJobRetentionPeriod <= 0 {
		panic("JobCleanerConfig.CancelledJobRetentionPeriod must be above zero")
	}
	if c.CompletedJobRetentionPeriod <= 0 {
		panic("JobCleanerConfig.CompletedJobRetentionPeriod must be above zero")
	}
	if c.DiscardedJobRetentionPeriod <= 0 {
		panic("JobCleanerConfig.DiscardedJobRetentionPeriod must be above zero")
	}
	if c.Interval <= 0 {
		panic("JobCleanerConfig.Interval must be above zero")
	}
	if c.Timeout <= 0 {
		panic("JobCleanerConfig.Timeout must be above zero")
	}

	return c
}

// JobCleaner periodically removes finalized jobs that are cancelled, completed,
// or discarded. Each state's retention time can be configured individually.
type JobCleaner struct {
	queueMaintainerServiceBase
	startstop.BaseStartStop

	// exported for test purposes
	Config      *JobCleanerConfig
	TestSignals JobCleanerTestSignals

	batchSize int // configurable for test purposes
	exec      riverdriver.Executor
}

func NewJobCleaner(archetype *baseservice.Archetype, config *JobCleanerConfig, exec riverdriver.Executor) *JobCleaner {
	return baseservice.Init(archetype, &JobCleaner{
		Config: (&JobCleanerConfig{
			CancelledJobRetentionPeriod: valutil.ValOrDefault(config.CancelledJobRetentionPeriod, CancelledJobRetentionPeriodDefault),
			CompletedJobRetentionPeriod: valutil.ValOrDefault(config.CompletedJobRetentionPeriod, CompletedJobRetentionPeriodDefault),
			DiscardedJobRetentionPeriod: valutil.ValOrDefault(config.DiscardedJobRetentionPeriod, DiscardedJobRetentionPeriodDefault),
			Interval:                    valutil.ValOrDefault(config.Interval, JobCleanerIntervalDefault),
			Timeout:                     valutil.ValOrDefault(config.Timeout, JobCleanerTimeoutDefault),
		}).mustValidate(),

		batchSize: BatchSizeDefault,
		exec:      exec,
	})
}

func (s *JobCleaner) Start(ctx context.Context) error { //nolint:dupl
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	s.StaggerStart(ctx)

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStarted)
		defer s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStopped)

		ticker := timeutil.NewTickerWithInitialTick(ctx, s.Config.Interval)
		for {
			select {
			case <-ctx.Done():
				return
			case <-ticker.C:
			}

			res, err := s.runOnce(ctx)
			if err != nil {
				if !errors.Is(err, context.Canceled) {
					s.Logger.ErrorContext(ctx, s.Name+": Error cleaning jobs", slog.String("error", err.Error()))
				}
				continue
			}

			if res.NumJobsDeleted > 0 {
				s.Logger.InfoContext(ctx, s.Name+logPrefixRanSuccessfully,
					slog.Int("num_jobs_deleted", res.NumJobsDeleted),
				)
			}
		}
	}()

	return nil
}

type jobCleanerRunOnceResult struct {
	NumJobsDeleted int
}

func (s *JobCleaner) runOnce(ctx context.Context) (*jobCleanerRunOnceResult, error) {
	res := &jobCleanerRunOnceResult{}

	for {
		// Wrapped in a function so that defers run as expected.
		numDeleted, err := func() (int, error) {
			ctx, cancelFunc := context.WithTimeout(ctx, s.Config.Timeout)
			defer cancelFunc()

			numDeleted, err := s.exec.JobDeleteBefore(ctx, &riverdriver.JobDeleteBeforeParams{
				CancelledFinalizedAtHorizon: time.Now().Add(-s.Config.CancelledJobRetentionPeriod),
				CompletedFinalizedAtHorizon: time.Now().Add(-s.Config.CompletedJobRetentionPeriod),
				DiscardedFinalizedAtHorizon: time.Now().Add(-s.Config.DiscardedJobRetentionPeriod),
				Max:                         s.batchSize,
			})
			if err != nil {
				return 0, fmt.Errorf("error deleting completed jobs: %w", err)
			}

			return numDeleted, nil
		}()
		if err != nil {
			return nil, err
		}

		s.TestSignals.DeletedBatch.Signal(struct{}{})

		res.NumJobsDeleted += numDeleted
		// Deleted was less than query `LIMIT` which means work is done.
		if numDeleted < s.batchSize {
			break
		}

		s.Logger.DebugContext(ctx, s.Name+": Deleted batch of jobs",
			slog.Int("num_jobs_deleted", numDeleted),
		)

		serviceutil.CancellableSleep(ctx, randutil.DurationBetween(BatchBackoffMin, BatchBackoffMax))
	}

	return res, nil
}

```

`internal/maintenance/job_cleaner_test.go`:

```go
package maintenance

import (
	"context"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

func TestJobCleaner(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		cancelledDeleteHorizon time.Time
		completedDeleteHorizon time.Time
		exec                   riverdriver.Executor
		discardedDeleteHorizon time.Time
	}

	setup := func(t *testing.T) (*JobCleaner, *testBundle) {
		t.Helper()

		tx := riverinternaltest.TestTx(ctx, t)
		bundle := &testBundle{
			cancelledDeleteHorizon: time.Now().Add(-CancelledJobRetentionPeriodDefault),
			completedDeleteHorizon: time.Now().Add(-CompletedJobRetentionPeriodDefault),
			exec:                   riverpgxv5.New(nil).UnwrapExecutor(tx),
			discardedDeleteHorizon: time.Now().Add(-DiscardedJobRetentionPeriodDefault),
		}

		cleaner := NewJobCleaner(
			riversharedtest.BaseServiceArchetype(t),
			&JobCleanerConfig{
				CancelledJobRetentionPeriod: CancelledJobRetentionPeriodDefault,
				CompletedJobRetentionPeriod: CompletedJobRetentionPeriodDefault,
				DiscardedJobRetentionPeriod: DiscardedJobRetentionPeriodDefault,
				Interval:                    JobCleanerIntervalDefault,
			},
			bundle.exec)
		cleaner.StaggerStartupDisable(true)
		cleaner.TestSignals.Init()
		t.Cleanup(cleaner.Stop)

		return cleaner, bundle
	}

	t.Run("Defaults", func(t *testing.T) {
		t.Parallel()

		cleaner := NewJobCleaner(riversharedtest.BaseServiceArchetype(t), &JobCleanerConfig{}, nil)

		require.Equal(t, CancelledJobRetentionPeriodDefault, cleaner.Config.CancelledJobRetentionPeriod)
		require.Equal(t, CompletedJobRetentionPeriodDefault, cleaner.Config.CompletedJobRetentionPeriod)
		require.Equal(t, DiscardedJobRetentionPeriodDefault, cleaner.Config.DiscardedJobRetentionPeriod)
		require.Equal(t, JobCleanerIntervalDefault, cleaner.Config.Interval)
		require.Equal(t, JobCleanerTimeoutDefault, cleaner.Config.Timeout)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress
		cleaner.TestSignals = JobCleanerTestSignals{}  // deinit so channels don't fill

		startstoptest.Stress(ctx, t, cleaner)
	})

	t.Run("DeletesCompletedJobs", func(t *testing.T) {
		t.Parallel()

		cleaner, bundle := setup(t)

		// none of these get removed
		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled)})

		cancelledJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(bundle.cancelledDeleteHorizon.Add(-1 * time.Hour))})
		cancelledJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(bundle.cancelledDeleteHorizon.Add(-1 * time.Minute))})
		cancelledJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(bundle.cancelledDeleteHorizon.Add(1 * time.Minute))}) // won't be deleted

		completedJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(bundle.completedDeleteHorizon.Add(-1 * time.Hour))})
		completedJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(bundle.completedDeleteHorizon.Add(-1 * time.Minute))})
		completedJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(bundle.completedDeleteHorizon.Add(1 * time.Minute))}) // won't be deleted

		discardedJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(bundle.discardedDeleteHorizon.Add(-1 * time.Hour))})
		discardedJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(bundle.discardedDeleteHorizon.Add(-1 * time.Minute))})
		discardedJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateDiscarded), FinalizedAt: ptrutil.Ptr(bundle.discardedDeleteHorizon.Add(1 * time.Minute))}) // won't be deleted

		require.NoError(t, cleaner.Start(ctx))

		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		var err error
		_, err = bundle.exec.JobGetByID(ctx, job1.ID)
		require.NotErrorIs(t, err, rivertype.ErrNotFound) // still there
		_, err = bundle.exec.JobGetByID(ctx, job2.ID)
		require.NotErrorIs(t, err, rivertype.ErrNotFound) // still there
		_, err = bundle.exec.JobGetByID(ctx, job3.ID)
		require.NotErrorIs(t, err, rivertype.ErrNotFound) // still there

		_, err = bundle.exec.JobGetByID(ctx, cancelledJob1.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.JobGetByID(ctx, cancelledJob2.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.JobGetByID(ctx, cancelledJob3.ID)
		require.NotErrorIs(t, err, rivertype.ErrNotFound) // still there

		_, err = bundle.exec.JobGetByID(ctx, completedJob1.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.JobGetByID(ctx, completedJob2.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.JobGetByID(ctx, completedJob3.ID)
		require.NotErrorIs(t, err, rivertype.ErrNotFound) // still there

		_, err = bundle.exec.JobGetByID(ctx, discardedJob1.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.JobGetByID(ctx, discardedJob2.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.JobGetByID(ctx, discardedJob3.ID)
		require.NotErrorIs(t, err, rivertype.ErrNotFound) // still there
	})

	t.Run("DeletesInBatches", func(t *testing.T) {
		t.Parallel()

		cleaner, bundle := setup(t)
		cleaner.batchSize = 10 // reduced size for test speed

		// Add one to our chosen batch size to get one extra job and therefore
		// one extra batch, ensuring that we've tested working multiple.
		numJobs := cleaner.batchSize + 1

		jobs := make([]*rivertype.JobRow, numJobs)

		for i := range numJobs {
			job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(bundle.completedDeleteHorizon.Add(-1 * time.Hour))})
			jobs[i] = job
		}

		require.NoError(t, cleaner.Start(ctx))

		// See comment above. Exactly two batches are expected.
		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()
		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		for _, job := range jobs {
			_, err := bundle.exec.JobGetByID(ctx, job.ID)
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		}
	})

	t.Run("CustomizableInterval", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = 1 * time.Microsecond

		require.NoError(t, cleaner.Start(ctx))

		// This should trigger ~immediately every time:
		for i := range 5 {
			t.Logf("Iteration %d", i)
			cleaner.TestSignals.DeletedBatch.WaitOrTimeout()
		}
	})

	t.Run("StopsImmediately", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		require.NoError(t, cleaner.Start(ctx))
		cleaner.Stop()
	})

	t.Run("RespectsContextCancellation", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		ctx, cancelFunc := context.WithCancel(ctx)

		require.NoError(t, cleaner.Start(ctx))

		// To avoid a potential race, make sure to get a reference to the
		// service's stopped channel _before_ cancellation as it's technically
		// possible for the cancel to "win" and remove the stopped channel
		// before we can start waiting on it.
		stopped := cleaner.Stopped()
		cancelFunc()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("CanRunMultipleTimes", func(t *testing.T) {
		t.Parallel()

		cleaner, bundle := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(bundle.completedDeleteHorizon.Add(-1 * time.Hour))})

		require.NoError(t, cleaner.Start(ctx))

		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		cleaner.Stop()

		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(bundle.completedDeleteHorizon.Add(-1 * time.Minute))})

		require.NoError(t, cleaner.Start(ctx))

		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		var err error
		_, err = bundle.exec.JobGetByID(ctx, job1.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.JobGetByID(ctx, job2.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
	})
}

```

`internal/maintenance/job_rescuer.go`:

```go
package maintenance

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"log/slog"
	"time"

	"github.com/riverqueue/river/internal/jobexecutor"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
	"github.com/riverqueue/river/rivertype"
)

const (
	JobRescuerRescueAfterDefault = time.Hour
	JobRescuerIntervalDefault    = 30 * time.Second
)

// Test-only properties.
type JobRescuerTestSignals struct {
	FetchedBatch testsignal.TestSignal[struct{}] // notifies when runOnce has fetched a batch of jobs
	UpdatedBatch testsignal.TestSignal[struct{}] // notifies when runOnce has updated rescued jobs from a batch
}

func (ts *JobRescuerTestSignals) Init() {
	ts.FetchedBatch.Init()
	ts.UpdatedBatch.Init()
}

type JobRescuerConfig struct {
	// ClientRetryPolicy is the default retry policy to use for workers that don't
	// override NextRetry.
	ClientRetryPolicy jobexecutor.ClientRetryPolicy

	// Interval is the amount of time to wait between runs of the rescuer.
	Interval time.Duration

	// RescueAfter is the amount of time for a job to be active before it is
	// considered stuck and should be rescued.
	RescueAfter time.Duration

	WorkUnitFactoryFunc func(kind string) workunit.WorkUnitFactory
}

func (c *JobRescuerConfig) mustValidate() *JobRescuerConfig {
	if c.ClientRetryPolicy == nil {
		panic("RescuerConfig.ClientRetryPolicy must be set")
	}
	if c.Interval <= 0 {
		panic("RescuerConfig.Interval must be above zero")
	}
	if c.RescueAfter <= 0 {
		panic("RescuerConfig.JobDuration must be above zero")
	}
	if c.WorkUnitFactoryFunc == nil {
		panic("RescuerConfig.WorkUnitFactoryFunc must be set")
	}

	return c
}

// JobRescuer periodically rescues jobs that have been executing for too long
// and are considered to be "stuck".
type JobRescuer struct {
	queueMaintainerServiceBase
	startstop.BaseStartStop

	// exported for test purposes
	Config      *JobRescuerConfig
	TestSignals JobRescuerTestSignals

	batchSize int // configurable for test purposes
	exec      riverdriver.Executor
}

func NewRescuer(archetype *baseservice.Archetype, config *JobRescuerConfig, exec riverdriver.Executor) *JobRescuer {
	return baseservice.Init(archetype, &JobRescuer{
		Config: (&JobRescuerConfig{
			ClientRetryPolicy:   config.ClientRetryPolicy,
			Interval:            valutil.ValOrDefault(config.Interval, JobRescuerIntervalDefault),
			RescueAfter:         valutil.ValOrDefault(config.RescueAfter, JobRescuerRescueAfterDefault),
			WorkUnitFactoryFunc: config.WorkUnitFactoryFunc,
		}).mustValidate(),

		batchSize: BatchSizeDefault,
		exec:      exec,
	})
}

func (s *JobRescuer) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	s.StaggerStart(ctx)

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStarted)
		defer s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStopped)

		ticker := timeutil.NewTickerWithInitialTick(ctx, s.Config.Interval)
		for {
			select {
			case <-ctx.Done():
				return
			case <-ticker.C:
			}

			res, err := s.runOnce(ctx)
			if err != nil {
				if !errors.Is(err, context.Canceled) {
					s.Logger.ErrorContext(ctx, s.Name+": Error rescuing jobs", slog.String("error", err.Error()))
				}
				continue
			}

			if res.NumJobsDiscarded > 0 || res.NumJobsRetried > 0 {
				s.Logger.InfoContext(ctx, s.Name+logPrefixRanSuccessfully,
					slog.Int64("num_jobs_discarded", res.NumJobsDiscarded),
					slog.Int64("num_jobs_retry_scheduled", res.NumJobsRetried),
				)
			}
		}
	}()

	return nil
}

type rescuerRunOnceResult struct {
	NumJobsCancelled int64
	NumJobsDiscarded int64
	NumJobsRetried   int64
}

type metadataWithCancelAttemptedAt struct {
	CancelAttemptedAt time.Time `json:"cancel_attempted_at"`
}

func (s *JobRescuer) runOnce(ctx context.Context) (*rescuerRunOnceResult, error) {
	res := &rescuerRunOnceResult{}

	for {
		stuckJobs, err := s.getStuckJobs(ctx)
		if err != nil {
			return nil, fmt.Errorf("error fetching stuck jobs: %w", err)
		}

		s.TestSignals.FetchedBatch.Signal(struct{}{})

		now := time.Now().UTC()

		rescueManyParams := riverdriver.JobRescueManyParams{
			ID:          make([]int64, 0, len(stuckJobs)),
			Error:       make([][]byte, 0, len(stuckJobs)),
			FinalizedAt: make([]time.Time, 0, len(stuckJobs)),
			ScheduledAt: make([]time.Time, 0, len(stuckJobs)),
			State:       make([]string, 0, len(stuckJobs)),
		}

		for _, job := range stuckJobs {
			var metadata metadataWithCancelAttemptedAt
			if err := json.Unmarshal(job.Metadata, &metadata); err != nil {
				return nil, fmt.Errorf("error unmarshaling job metadata: %w", err)
			}

			errorData, err := json.Marshal(rivertype.AttemptError{
				At:      now,
				Attempt: max(job.Attempt, 0),
				Error:   "Stuck job rescued by Rescuer",
				Trace:   "TODO",
			})
			if err != nil {
				return nil, fmt.Errorf("error marshaling error JSON: %w", err)
			}

			addRescueParam := func(state rivertype.JobState, finalizedAt *time.Time, scheduledAt time.Time) {
				rescueManyParams.ID = append(rescueManyParams.ID, job.ID)
				rescueManyParams.Error = append(rescueManyParams.Error, errorData)
				rescueManyParams.FinalizedAt = append(rescueManyParams.FinalizedAt, ptrutil.ValOrDefault(finalizedAt, time.Time{}))
				rescueManyParams.ScheduledAt = append(rescueManyParams.ScheduledAt, scheduledAt)
				rescueManyParams.State = append(rescueManyParams.State, string(state))
			}

			if !metadata.CancelAttemptedAt.IsZero() {
				res.NumJobsCancelled++
				addRescueParam(rivertype.JobStateCancelled, &now, job.ScheduledAt) // reused previous scheduled value
				continue
			}

			retryDecision, retryAt := s.makeRetryDecision(ctx, job, now)

			switch retryDecision {
			case jobRetryDecisionDiscard:
				res.NumJobsDiscarded++
				addRescueParam(rivertype.JobStateDiscarded, &now, job.ScheduledAt) // reused previous scheduled value

			case jobRetryDecisionIgnore:
				// job not timed out yet due to kind-specific timeout value; ignore

			case jobRetryDecisionRetry:
				res.NumJobsRetried++
				addRescueParam(rivertype.JobStateRetryable, nil, retryAt)
			}
		}

		if len(rescueManyParams.ID) > 0 {
			_, err = s.exec.JobRescueMany(ctx, &rescueManyParams)
			if err != nil {
				return nil, fmt.Errorf("error rescuing stuck jobs: %w", err)
			}
		}

		s.TestSignals.UpdatedBatch.Signal(struct{}{})

		// Number of rows fetched was less than query `LIMIT` which means work is
		// done for this round:
		if len(stuckJobs) < s.batchSize {
			break
		}

		serviceutil.CancellableSleep(ctx, randutil.DurationBetween(BatchBackoffMin, BatchBackoffMax))
	}

	return res, nil
}

func (s *JobRescuer) getStuckJobs(ctx context.Context) ([]*rivertype.JobRow, error) {
	ctx, cancelFunc := context.WithTimeout(ctx, 30*time.Second)
	defer cancelFunc()

	stuckHorizon := time.Now().Add(-s.Config.RescueAfter)

	return s.exec.JobGetStuck(ctx, &riverdriver.JobGetStuckParams{
		Max:          s.batchSize,
		StuckHorizon: stuckHorizon,
	})
}

// jobRetryDecision is a signal from makeRetryDecision as to what to do with a
// particular job that appears to be eligible for rescue.
type jobRetryDecision int

const (
	jobRetryDecisionDiscard jobRetryDecision = iota // discard the job
	jobRetryDecisionIgnore                          // don't retry or discard the job
	jobRetryDecisionRetry                           // retry the job
)

// makeRetryDecision decides whether or not a rescued job should be retried, and if so,
// when.
func (s *JobRescuer) makeRetryDecision(ctx context.Context, job *rivertype.JobRow, now time.Time) (jobRetryDecision, time.Time) {
	workUnitFactory := s.Config.WorkUnitFactoryFunc(job.Kind)
	if workUnitFactory == nil {
		s.Logger.ErrorContext(ctx, s.Name+": Attempted to rescue unhandled job kind, discarding",
			slog.String("job_kind", job.Kind), slog.Int64("job_id", job.ID))
		return jobRetryDecisionDiscard, time.Time{}
	}

	workUnit := workUnitFactory.MakeUnit(job)
	if err := workUnit.UnmarshalJob(); err != nil {
		s.Logger.ErrorContext(ctx, s.Name+": Error unmarshaling job args: %s"+err.Error(),
			slog.String("job_kind", job.Kind), slog.Int64("job_id", job.ID))
	}

	if workUnit.Timeout() != 0 && now.Sub(*job.AttemptedAt) < workUnit.Timeout() {
		return jobRetryDecisionIgnore, time.Time{}
	}

	nextRetry := workUnit.NextRetry()
	if nextRetry.IsZero() {
		nextRetry = s.Config.ClientRetryPolicy.NextRetry(job)
	}

	if job.Attempt < max(job.MaxAttempts, 0) {
		return jobRetryDecisionRetry, nextRetry
	}

	return jobRetryDecisionDiscard, time.Time{}
}

```

`internal/maintenance/job_rescuer_test.go`:

```go
package maintenance

import (
	"context"
	"fmt"
	"math"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivertype"
)

// callbackWorkUnitFactory wraps a Worker to implement workUnitFactory.
type callbackWorkUnitFactory struct {
	Callback func(ctx context.Context, jobRow *rivertype.JobRow) error
	timeout  time.Duration // defaults to 0, which signals default timeout
}

func (w *callbackWorkUnitFactory) MakeUnit(jobRow *rivertype.JobRow) workunit.WorkUnit {
	return &callbackWorkUnit{callback: w.Callback, jobRow: jobRow, timeout: w.timeout}
}

// callbackWorkUnit implements workUnit for a job and Worker.
type callbackWorkUnit struct {
	callback func(ctx context.Context, jobRow *rivertype.JobRow) error
	jobRow   *rivertype.JobRow
	timeout  time.Duration // defaults to 0, which signals default timeout
}

func (w *callbackWorkUnit) HookLookup(cache *hooklookup.JobHookLookup) hooklookup.HookLookupInterface {
	return nil
}
func (w *callbackWorkUnit) Middleware() []rivertype.WorkerMiddleware { return nil }
func (w *callbackWorkUnit) NextRetry() time.Time                     { return time.Now().Add(30 * time.Second) }
func (w *callbackWorkUnit) Timeout() time.Duration                   { return w.timeout }
func (w *callbackWorkUnit) Work(ctx context.Context) error           { return w.callback(ctx, w.jobRow) }
func (w *callbackWorkUnit) UnmarshalJob() error                      { return nil }

type SimpleClientRetryPolicy struct{}

func (p *SimpleClientRetryPolicy) NextRetry(job *rivertype.JobRow) time.Time {
	errorCount := len(job.Errors) + 1
	retrySeconds := math.Pow(float64(errorCount), 4)
	return job.AttemptedAt.Add(timeutil.SecondsAsDuration(retrySeconds))
}

func TestJobRescuer(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	const (
		rescuerJobKind            = "rescuer"
		rescuerJobKindLongTimeout = "rescuer_long_timeout"
	)

	type testBundle struct {
		exec          riverdriver.Executor
		rescueHorizon time.Time
	}

	setup := func(t *testing.T) (*JobRescuer, *testBundle) {
		t.Helper()

		tx := riverinternaltest.TestTx(ctx, t)
		bundle := &testBundle{
			exec:          riverpgxv5.New(nil).UnwrapExecutor(tx),
			rescueHorizon: time.Now().Add(-JobRescuerRescueAfterDefault),
		}

		rescuer := NewRescuer(
			riversharedtest.BaseServiceArchetype(t),
			&JobRescuerConfig{
				ClientRetryPolicy: &SimpleClientRetryPolicy{},
				Interval:          JobRescuerIntervalDefault,
				RescueAfter:       JobRescuerRescueAfterDefault,
				WorkUnitFactoryFunc: func(kind string) workunit.WorkUnitFactory {
					emptyCallback := func(ctx context.Context, jobRow *rivertype.JobRow) error { return nil }

					switch kind {
					case rescuerJobKind:
						return &callbackWorkUnitFactory{Callback: emptyCallback}
					case rescuerJobKindLongTimeout:
						return &callbackWorkUnitFactory{Callback: emptyCallback, timeout: JobRescuerRescueAfterDefault + 5*time.Minute}
					}
					panic("unhandled kind: " + kind)
				},
			},
			bundle.exec)
		rescuer.StaggerStartupDisable(true)
		rescuer.TestSignals.Init()
		t.Cleanup(rescuer.Stop)

		return rescuer, bundle
	}

	t.Run("Defaults", func(t *testing.T) {
		t.Parallel()

		cleaner := NewRescuer(
			riversharedtest.BaseServiceArchetype(t),
			&JobRescuerConfig{
				ClientRetryPolicy:   &SimpleClientRetryPolicy{},
				WorkUnitFactoryFunc: func(kind string) workunit.WorkUnitFactory { return nil },
			},
			nil,
		)

		require.Equal(t, JobRescuerRescueAfterDefault, cleaner.Config.RescueAfter)
		require.Equal(t, JobRescuerIntervalDefault, cleaner.Config.Interval)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		rescuer, _ := setup(t)
		rescuer.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress
		rescuer.TestSignals = JobRescuerTestSignals{}  // deinit so channels don't fill

		startstoptest.Stress(ctx, t, rescuer)
	})

	t.Run("RescuesStuckJobs", func(t *testing.T) {
		t.Parallel()
		require := require.New(t)

		cleaner, bundle := setup(t)

		stuckToRetryJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), MaxAttempts: ptrutil.Ptr(5)})
		stuckToRetryJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Minute)), MaxAttempts: ptrutil.Ptr(5)})
		stuckToRetryJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(1 * time.Minute)), MaxAttempts: ptrutil.Ptr(5)}) // won't be rescued

		// Already at max attempts:
		stuckToDiscardJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(5), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), MaxAttempts: ptrutil.Ptr(5)})
		stuckToDiscardJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(5), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(1 * time.Minute)), MaxAttempts: ptrutil.Ptr(5)}) // won't be rescued

		// Marked as cancelled by query:
		cancelTime := time.Now().UTC().Format(time.RFC3339Nano)
		stuckToCancelJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), Metadata: []byte(fmt.Sprintf(`{"cancel_attempted_at": %q}`, cancelTime)), MaxAttempts: ptrutil.Ptr(5)})
		stuckToCancelJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(1 * time.Minute)), Metadata: []byte(fmt.Sprintf(`{"cancel_attempted_at": %q}`, cancelTime)), MaxAttempts: ptrutil.Ptr(5)}) // won't be rescued

		// these aren't touched because they're in ineligible states
		notRunningJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), FinalizedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), State: ptrutil.Ptr(rivertype.JobStateCompleted), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), MaxAttempts: ptrutil.Ptr(5)})
		notRunningJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), FinalizedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), State: ptrutil.Ptr(rivertype.JobStateDiscarded), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), MaxAttempts: ptrutil.Ptr(5)})
		notRunningJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), FinalizedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), State: ptrutil.Ptr(rivertype.JobStateCancelled), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), MaxAttempts: ptrutil.Ptr(5)})

		// Jobs with worker-specific long timeouts. The first isn't rescued
		// because the difference between its `attempted_at` and now is still
		// within the timeout threshold. The second _is_ rescued because it
		// started earlier and even with the longer timeout, has still timed out.
		longTimeOutJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKindLongTimeout), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Minute)), MaxAttempts: ptrutil.Ptr(5)})
		longTimeOutJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKindLongTimeout), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-6 * time.Minute)), MaxAttempts: ptrutil.Ptr(5)})

		require.NoError(cleaner.Start(ctx))

		cleaner.TestSignals.FetchedBatch.WaitOrTimeout()
		cleaner.TestSignals.UpdatedBatch.WaitOrTimeout()

		confirmRetried := func(jobBefore *rivertype.JobRow) {
			jobAfter, err := bundle.exec.JobGetByID(ctx, jobBefore.ID)
			require.NoError(err)
			require.Equal(rivertype.JobStateRetryable, jobAfter.State)
		}

		var err error
		confirmRetried(stuckToRetryJob1)
		confirmRetried(stuckToRetryJob2)
		job3After, err := bundle.exec.JobGetByID(ctx, stuckToRetryJob3.ID)
		require.NoError(err)
		require.Equal(stuckToRetryJob3.State, job3After.State) // not rescued

		discardJob1After, err := bundle.exec.JobGetByID(ctx, stuckToDiscardJob1.ID)
		require.NoError(err)
		require.Equal(rivertype.JobStateDiscarded, discardJob1After.State)
		require.WithinDuration(time.Now(), *discardJob1After.FinalizedAt, 5*time.Second)
		require.Len(discardJob1After.Errors, 1)

		discardJob2After, err := bundle.exec.JobGetByID(ctx, stuckToDiscardJob2.ID)
		require.NoError(err)
		require.Equal(rivertype.JobStateRunning, discardJob2After.State)
		require.Nil(discardJob2After.FinalizedAt)

		cancelJob1After, err := bundle.exec.JobGetByID(ctx, stuckToCancelJob1.ID)
		require.NoError(err)
		require.Equal(rivertype.JobStateCancelled, cancelJob1After.State)
		require.WithinDuration(time.Now(), *cancelJob1After.FinalizedAt, 5*time.Second)
		require.Len(cancelJob1After.Errors, 1)

		cancelJob2After, err := bundle.exec.JobGetByID(ctx, stuckToCancelJob2.ID)
		require.NoError(err)
		require.Equal(rivertype.JobStateRunning, cancelJob2After.State)
		require.Nil(cancelJob2After.FinalizedAt)

		notRunningJob1After, err := bundle.exec.JobGetByID(ctx, notRunningJob1.ID)
		require.NoError(err)
		require.Equal(notRunningJob1.State, notRunningJob1After.State)
		notRunningJob2After, err := bundle.exec.JobGetByID(ctx, notRunningJob2.ID)
		require.NoError(err)
		require.Equal(notRunningJob2.State, notRunningJob2After.State)
		notRunningJob3After, err := bundle.exec.JobGetByID(ctx, notRunningJob3.ID)
		require.NoError(err)
		require.Equal(notRunningJob3.State, notRunningJob3After.State)

		notTimedOutJob1After, err := bundle.exec.JobGetByID(ctx, longTimeOutJob1.ID)
		require.NoError(err)
		require.Equal(rivertype.JobStateRunning, notTimedOutJob1After.State)
		notTimedOutJob2After, err := bundle.exec.JobGetByID(ctx, longTimeOutJob2.ID)
		require.NoError(err)
		require.Equal(rivertype.JobStateRetryable, notTimedOutJob2After.State)
	})

	t.Run("RescuesInBatches", func(t *testing.T) {
		t.Parallel()

		cleaner, bundle := setup(t)
		cleaner.batchSize = 10 // reduced size for test speed

		// Add one to our chosen batch size to get one extra job and therefore
		// one extra batch, ensuring that we've tested working multiple.
		numJobs := cleaner.batchSize + 1

		jobs := make([]*rivertype.JobRow, numJobs)

		for i := range numJobs {
			job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), MaxAttempts: ptrutil.Ptr(5)})
			jobs[i] = job
		}

		require.NoError(t, cleaner.Start(ctx))

		// See comment above. Exactly two batches are expected.
		cleaner.TestSignals.FetchedBatch.WaitOrTimeout()
		cleaner.TestSignals.UpdatedBatch.WaitOrTimeout()
		cleaner.TestSignals.FetchedBatch.WaitOrTimeout()
		cleaner.TestSignals.UpdatedBatch.WaitOrTimeout() // need to wait until after this for the conn to be free

		for _, job := range jobs {
			jobUpdated, err := bundle.exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRetryable, jobUpdated.State)
		}
	})

	t.Run("CustomizableInterval", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = 1 * time.Microsecond

		require.NoError(t, cleaner.Start(ctx))

		// This should trigger ~immediately every time:
		for i := range 5 {
			t.Logf("Iteration %d", i)
			cleaner.TestSignals.FetchedBatch.WaitOrTimeout()
		}
	})

	t.Run("StopsImmediately", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		require.NoError(t, cleaner.Start(ctx))
		cleaner.Stop()
	})

	t.Run("RespectsContextCancellation", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		ctx, cancelFunc := context.WithCancel(ctx)

		require.NoError(t, cleaner.Start(ctx))

		// To avoid a potential race, make sure to get a reference to the
		// service's stopped channel _before_ cancellation as it's technically
		// possible for the cancel to "win" and remove the stopped channel
		// before we can start waiting on it.
		stopped := cleaner.Stopped()
		cancelFunc()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("CanRunMultipleTimes", func(t *testing.T) {
		t.Parallel()

		rescuer, bundle := setup(t)
		rescuer.Config.Interval = time.Minute // should only trigger once for the initial run

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(5), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Hour)), MaxAttempts: ptrutil.Ptr(5)})

		require.NoError(t, rescuer.Start(ctx))

		rescuer.TestSignals.FetchedBatch.WaitOrTimeout()
		rescuer.TestSignals.UpdatedBatch.WaitOrTimeout()

		rescuer.Stop()

		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(rescuerJobKind), State: ptrutil.Ptr(rivertype.JobStateRunning), Attempt: ptrutil.Ptr(5), AttemptedAt: ptrutil.Ptr(bundle.rescueHorizon.Add(-1 * time.Minute)), MaxAttempts: ptrutil.Ptr(5)})

		require.NoError(t, rescuer.Start(ctx))

		rescuer.TestSignals.FetchedBatch.WaitOrTimeout()
		rescuer.TestSignals.UpdatedBatch.WaitOrTimeout()

		job1After, err := bundle.exec.JobGetByID(ctx, job1.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateDiscarded, job1After.State)
		job2After, err := bundle.exec.JobGetByID(ctx, job2.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateDiscarded, job2After.State)
	})
}

```

`internal/maintenance/job_scheduler.go`:

```go
package maintenance

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"time"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
	"github.com/riverqueue/river/rivertype"
)

const (
	JobSchedulerIntervalDefault = 5 * time.Second
	JobSchedulerLimitDefault    = 10_000
)

// Test-only properties.
type JobSchedulerTestSignals struct {
	NotifiedQueues testsignal.TestSignal[[]string] // notifies when queues are sent an insert notification
	ScheduledBatch testsignal.TestSignal[struct{}] // notifies when runOnce finishes a pass
}

func (ts *JobSchedulerTestSignals) Init() {
	ts.NotifiedQueues.Init()
	ts.ScheduledBatch.Init()
}

type InsertFunc func(ctx context.Context, tx riverdriver.ExecutorTx, insertParams []*rivertype.JobInsertParams) ([]*rivertype.JobInsertResult, error)

// NotifyInsert is a function to call to emit notifications for queues where
// jobs were scheduled.
type NotifyInsertFunc func(ctx context.Context, tx riverdriver.ExecutorTx, queues []string) error

type JobSchedulerConfig struct {
	// Interval is the amount of time between periodic checks for jobs to
	// be moved from "scheduled" to "available".
	Interval time.Duration

	// Limit is the maximum number of jobs to transition at once from
	// "scheduled" to "available" during periodic scheduling checks.
	Limit int

	// NotifyInsert is a function to call to emit notifications for queues
	// where jobs were scheduled.
	NotifyInsert NotifyInsertFunc
}

func (c *JobSchedulerConfig) mustValidate() *JobSchedulerConfig {
	if c.Interval <= 0 {
		panic("SchedulerConfig.Interval must be above zero")
	}
	if c.Limit <= 0 {
		panic("SchedulerConfig.Limit must be above zero")
	}

	return c
}

// JobScheduler periodically moves jobs in `scheduled` or `retryable` state and
// which are ready to run over to `available` so that they're eligible to be
// worked.
type JobScheduler struct {
	queueMaintainerServiceBase
	startstop.BaseStartStop

	// exported for test purposes
	TestSignals JobSchedulerTestSignals

	config *JobSchedulerConfig
	exec   riverdriver.Executor
}

func NewJobScheduler(archetype *baseservice.Archetype, config *JobSchedulerConfig, exec riverdriver.Executor) *JobScheduler {
	return baseservice.Init(archetype, &JobScheduler{
		config: (&JobSchedulerConfig{
			Interval:     valutil.ValOrDefault(config.Interval, JobSchedulerIntervalDefault),
			Limit:        valutil.ValOrDefault(config.Limit, JobSchedulerLimitDefault),
			NotifyInsert: config.NotifyInsert,
		}).mustValidate(),
		exec: exec,
	})
}

func (s *JobScheduler) Start(ctx context.Context) error { //nolint:dupl
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	s.StaggerStart(ctx)

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStarted)
		defer s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStopped)

		ticker := timeutil.NewTickerWithInitialTick(ctx, s.config.Interval)
		for {
			select {
			case <-ctx.Done():
				return
			case <-ticker.C:
			}

			res, err := s.runOnce(ctx)
			if err != nil {
				if !errors.Is(err, context.Canceled) {
					s.Logger.ErrorContext(ctx, s.Name+": Error scheduling jobs", slog.String("error", err.Error()))
				}
				continue
			}

			if res.NumCompletedJobsScheduled > 0 {
				s.Logger.InfoContext(ctx, s.Name+logPrefixRanSuccessfully,
					slog.Int("num_jobs_scheduled", res.NumCompletedJobsScheduled),
				)
			}
		}
	}()

	return nil
}

type schedulerRunOnceResult struct {
	NumCompletedJobsScheduled int
}

func (s *JobScheduler) runOnce(ctx context.Context) (*schedulerRunOnceResult, error) {
	res := &schedulerRunOnceResult{}

	for {
		// Wrapped in a function so that defers run as expected.
		numScheduled, err := func() (int, error) {
			ctx, cancelFunc := context.WithTimeout(ctx, 30*time.Second)
			defer cancelFunc()

			tx, err := s.exec.Begin(ctx)
			if err != nil {
				return 0, fmt.Errorf("error starting transaction: %w", err)
			}
			defer tx.Rollback(ctx)

			now := s.Time.NowUTC()
			nowWithLookAhead := now.Add(s.config.Interval)

			scheduledJobResults, err := tx.JobSchedule(ctx, &riverdriver.JobScheduleParams{
				Max: s.config.Limit,
				Now: nowWithLookAhead,
			})
			if err != nil {
				return 0, fmt.Errorf("error scheduling jobs: %w", err)
			}

			queues := make([]string, 0, len(scheduledJobResults))

			// Notify about scheduled jobs with a scheduled_at in the past, or just
			// slightly in the future (this loop, the notify, and tx commit will take
			// a small amount of time). This isn't going to be perfect, but the goal
			// is to roughly try to guess when the clients will attempt to fetch jobs.
			notificationHorizon := s.Time.NowUTC().Add(5 * time.Millisecond)

			for _, result := range scheduledJobResults {
				if result.Job.ScheduledAt.After(notificationHorizon) {
					continue
				}

				queues = append(queues, result.Job.Queue)
			}

			if len(queues) > 0 {
				if err := s.config.NotifyInsert(ctx, tx, queues); err != nil {
					return 0, fmt.Errorf("error notifying insert: %w", err)
				}
				s.TestSignals.NotifiedQueues.Signal(queues)
			}

			return len(scheduledJobResults), tx.Commit(ctx)
		}()
		if err != nil {
			return nil, err
		}

		s.TestSignals.ScheduledBatch.Signal(struct{}{})

		res.NumCompletedJobsScheduled += numScheduled
		// Scheduled was less than query `LIMIT` which means work is done.
		if numScheduled < s.config.Limit {
			break
		}

		serviceutil.CancellableSleep(ctx, randutil.DurationBetween(BatchBackoffMin, BatchBackoffMax))
	}

	return res, nil
}

```

`internal/maintenance/job_scheduler_test.go`:

```go
package maintenance

import (
	"context"
	"sort"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
	"github.com/tidwall/gjson"

	"github.com/riverqueue/river/internal/dbunique"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

func TestJobScheduler(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		exec                 riverdriver.Executor
		notificationsByQueue map[string]int
	}

	setup := func(t *testing.T, exec riverdriver.Executor) (*JobScheduler, *testBundle) {
		t.Helper()

		archetype := riversharedtest.BaseServiceArchetype(t)

		bundle := &testBundle{
			exec:                 exec,
			notificationsByQueue: make(map[string]int),
		}

		scheduler := NewJobScheduler(
			archetype,
			&JobSchedulerConfig{
				Interval: JobSchedulerIntervalDefault,
				Limit:    10,
				NotifyInsert: func(ctx context.Context, tx riverdriver.ExecutorTx, queues []string) error {
					for _, queue := range queues {
						bundle.notificationsByQueue[queue]++
					}
					return nil
				},
			},
			bundle.exec)
		scheduler.TestSignals.Init()
		t.Cleanup(scheduler.Stop)

		return scheduler, bundle
	}

	setupTx := func(t *testing.T) (*JobScheduler, *testBundle) {
		t.Helper()
		tx := riverinternaltest.TestTx(ctx, t)
		return setup(t, riverpgxv5.New(nil).UnwrapExecutor(tx))
	}

	requireJobStateUnchanged := func(t *testing.T, exec riverdriver.Executor, job *rivertype.JobRow) *rivertype.JobRow {
		t.Helper()
		newJob, err := exec.JobGetByID(ctx, job.ID)
		require.NoError(t, err)
		require.Equal(t, job.State, newJob.State)
		return newJob
	}
	requireJobStateAvailable := func(t *testing.T, exec riverdriver.Executor, job *rivertype.JobRow) *rivertype.JobRow {
		t.Helper()
		newJob, err := exec.JobGetByID(ctx, job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateAvailable, newJob.State)
		return newJob
	}
	requireJobStateDiscardedWithMeta := func(t *testing.T, exec riverdriver.Executor, job *rivertype.JobRow) *rivertype.JobRow {
		t.Helper()
		newJob, err := exec.JobGetByID(ctx, job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateDiscarded, newJob.State)
		require.NotNil(t, newJob.FinalizedAt)
		require.Equal(t, "scheduler_discarded", gjson.GetBytes(newJob.Metadata, "unique_key_conflict").String())
		return newJob
	}

	t.Run("Defaults", func(t *testing.T) {
		t.Parallel()

		scheduler := NewJobScheduler(riversharedtest.BaseServiceArchetype(t), &JobSchedulerConfig{}, nil)

		require.Equal(t, JobSchedulerIntervalDefault, scheduler.config.Interval)
		require.Equal(t, JobSchedulerLimitDefault, scheduler.config.Limit)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		scheduler, _ := setupTx(t)
		scheduler.Logger = riversharedtest.LoggerWarn(t)  // loop started/stop log is very noisy; suppress
		scheduler.TestSignals = JobSchedulerTestSignals{} // deinit so channels don't fill

		startstoptest.Stress(ctx, t, scheduler)
	})

	t.Run("SchedulesScheduledAndRetryableJobs", func(t *testing.T) {
		t.Parallel()

		scheduler, bundle := setupTx(t)
		now := time.Now().UTC()

		// none of these should get updated
		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{FinalizedAt: ptrutil.Ptr(now), State: ptrutil.Ptr(rivertype.JobStateCompleted)})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{FinalizedAt: ptrutil.Ptr(now), State: ptrutil.Ptr(rivertype.JobStateCancelled)})
		job4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{FinalizedAt: ptrutil.Ptr(now), State: ptrutil.Ptr(rivertype.JobStateDiscarded)})
		job5 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})

		scheduledJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})
		scheduledJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})
		scheduledJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(scheduler.config.Interval - time.Millisecond))}) // won't be scheduled
		scheduledJob4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(30 * time.Second))})                             // won't be scheduled

		retryableJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})
		retryableJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})
		retryableJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(30 * time.Second))}) // won't be scheduled

		require.NoError(t, scheduler.Start(ctx))

		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()

		requireJobStateUnchanged(t, bundle.exec, job1)
		requireJobStateUnchanged(t, bundle.exec, job2)
		requireJobStateUnchanged(t, bundle.exec, job3)
		requireJobStateUnchanged(t, bundle.exec, job4)
		requireJobStateUnchanged(t, bundle.exec, job5)

		requireJobStateAvailable(t, bundle.exec, scheduledJob1)
		requireJobStateAvailable(t, bundle.exec, scheduledJob2)
		requireJobStateAvailable(t, bundle.exec, scheduledJob3)
		requireJobStateUnchanged(t, bundle.exec, scheduledJob4) // still scheduled

		requireJobStateAvailable(t, bundle.exec, retryableJob1)
		requireJobStateAvailable(t, bundle.exec, retryableJob2)
		requireJobStateUnchanged(t, bundle.exec, retryableJob3) // still retryable
	})

	t.Run("MovesUniqueKeyConflictingJobsToDiscarded", func(t *testing.T) {
		t.Parallel()

		scheduler, bundle := setupTx(t)
		now := time.Now().UTC()

		// The list of default states, but without retryable to allow for dupes in that state:
		uniqueStates := []rivertype.JobState{
			rivertype.JobStateAvailable,
			rivertype.JobStateCompleted,
			rivertype.JobStatePending,
			rivertype.JobStateRunning,
			rivertype.JobStateScheduled,
		}
		uniqueMap := dbunique.UniqueStatesToBitmask(uniqueStates)

		retryableJob1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("1"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})
		retryableJob2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("2"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))})
		retryableJob3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("3"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))}) // dupe
		retryableJob4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("4"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))}) // dupe
		retryableJob5 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("5"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))}) // dupe
		retryableJob6 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("6"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))}) // dupe
		retryableJob7 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("7"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-5 * time.Second))}) // dupe

		// Will cause conflicts with above jobs when retried:
		testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("3"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("4"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateCompleted)})
		testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("5"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStatePending)})
		testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("6"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateRunning)})
		testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{UniqueKey: []byte("7"), UniqueStates: uniqueMap, State: ptrutil.Ptr(rivertype.JobStateScheduled)})

		require.NoError(t, scheduler.Start(ctx))

		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()

		requireJobStateAvailable(t, bundle.exec, retryableJob1)
		requireJobStateAvailable(t, bundle.exec, retryableJob2)
		requireJobStateDiscardedWithMeta(t, bundle.exec, retryableJob3)
		requireJobStateDiscardedWithMeta(t, bundle.exec, retryableJob4)
		requireJobStateDiscardedWithMeta(t, bundle.exec, retryableJob5)
		requireJobStateDiscardedWithMeta(t, bundle.exec, retryableJob6)
		requireJobStateDiscardedWithMeta(t, bundle.exec, retryableJob7)
	})

	t.Run("SchedulesInBatches", func(t *testing.T) {
		t.Parallel()

		scheduler, bundle := setupTx(t)
		scheduler.config.Limit = 10 // reduced size for test speed

		now := time.Now().UTC()

		// Add one to our chosen batch size to get one extra job and therefore
		// one extra batch, ensuring that we've tested working multiple.
		numJobs := scheduler.config.Limit + 1

		jobs := make([]*rivertype.JobRow, numJobs)

		for i := range numJobs {
			jobState := rivertype.JobStateScheduled
			if i%2 == 0 {
				jobState = rivertype.JobStateRetryable
			}
			job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{
				Queue:       ptrutil.Ptr("scheduler_test"),
				State:       &jobState,
				ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour)),
			})
			jobs[i] = job
		}

		require.NoError(t, scheduler.Start(ctx))

		// See comment above. Exactly two batches are expected.
		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()
		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()

		for _, job := range jobs {
			requireJobStateAvailable(t, bundle.exec, job)
		}
	})

	t.Run("CustomizableInterval", func(t *testing.T) {
		t.Parallel()

		scheduler, _ := setupTx(t)
		scheduler.config.Interval = 1 * time.Microsecond

		require.NoError(t, scheduler.Start(ctx))

		// This should trigger ~immediately every time:
		for i := range 5 {
			t.Logf("Iteration %d", i)
			scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()
		}
	})

	t.Run("StopsImmediately", func(t *testing.T) {
		t.Parallel()

		scheduler, _ := setupTx(t)
		scheduler.config.Interval = time.Minute // should only trigger once for the initial run

		require.NoError(t, scheduler.Start(ctx))
		scheduler.Stop()
	})

	t.Run("RespectsContextCancellation", func(t *testing.T) {
		t.Parallel()

		scheduler, _ := setupTx(t)
		scheduler.config.Interval = time.Minute // should only trigger once for the initial run

		ctx, cancelFunc := context.WithCancel(ctx)

		require.NoError(t, scheduler.Start(ctx))

		// To avoid a potential race, make sure to get a reference to the
		// service's stopped channel _before_ cancellation as it's technically
		// possible for the cancel to "win" and remove the stopped channel
		// before we can start waiting on it.
		stopped := scheduler.Stopped()
		cancelFunc()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("CanRunMultipleTimes", func(t *testing.T) {
		t.Parallel()

		scheduler, bundle := setupTx(t)
		scheduler.config.Interval = time.Minute // should only trigger once for the initial run
		now := time.Now().UTC()

		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour))})

		require.NoError(t, scheduler.Start(ctx))

		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()

		scheduler.Stop()

		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRetryable), ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Minute))})

		require.NoError(t, scheduler.Start(ctx))

		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()

		requireJobStateAvailable(t, bundle.exec, job1)
		requireJobStateAvailable(t, bundle.exec, job2)
	})

	t.Run("TriggersNotificationsOnEachQueueWithNewlyAvailableJobs", func(t *testing.T) {
		t.Parallel()

		dbPool := riverinternaltest.TestDB(ctx, t)
		driver := riverpgxv5.New(dbPool)
		exec := driver.GetExecutor()
		notifyCh := make(chan []string, 10)

		scheduler, _ := setup(t, exec)
		scheduler.config.Interval = time.Minute // should only trigger once for the initial run
		scheduler.config.NotifyInsert = func(ctx context.Context, tx riverdriver.ExecutorTx, queues []string) error {
			notifyCh <- queues
			return nil
		}
		now := time.Now().UTC()

		addJob := func(queue string, fromNow time.Duration, state rivertype.JobState) {
			t.Helper()
			var finalizedAt *time.Time
			switch state { //nolint:exhaustive
			case rivertype.JobStateCompleted, rivertype.JobStateCancelled, rivertype.JobStateDiscarded:
				finalizedAt = ptrutil.Ptr(now.Add(fromNow))
			}
			testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				FinalizedAt: finalizedAt,
				Queue:       &queue,
				State:       &state,
				ScheduledAt: ptrutil.Ptr(now.Add(fromNow)),
			})
		}

		addJob("queue1", -1*time.Hour, rivertype.JobStateScheduled)
		addJob("queue2", -1*time.Minute, rivertype.JobStateScheduled)
		// deduplication is handled in client, so this dupe should appear:
		addJob("queue2", -5*time.Second, rivertype.JobStateScheduled)
		addJob("queue3", -30*time.Second, rivertype.JobStateRetryable)
		// This one is scheduled only a millisecond in the future, so it should
		// trigger a notification:
		addJob("queue4", time.Millisecond, rivertype.JobStateRetryable)

		// these shouldn't cause notifications:
		addJob("future_queue", 2*time.Minute, rivertype.JobStateScheduled)     // it's in the future
		addJob("other_status_queue", time.Minute, rivertype.JobStateCancelled) // it's cancelled
		// This one is scheduled in the future, just barely before the next run, so it should
		// be scheduled but shouldn't trigger a notification:
		addJob("queue5", scheduler.config.Interval-time.Millisecond, rivertype.JobStateRetryable)

		// Run the scheduler and wait for it to execute once:
		require.NoError(t, scheduler.Start(ctx))
		scheduler.TestSignals.ScheduledBatch.WaitOrTimeout()

		expectedQueues := []string{"queue1", "queue2", "queue2", "queue3", "queue4"}

		notifiedQueues := riversharedtest.WaitOrTimeout(t, notifyCh)
		sort.Strings(notifiedQueues)
		require.Equal(t, expectedQueues, notifiedQueues)
	})
}

```

`internal/maintenance/main_test.go`:

```go
package maintenance

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`internal/maintenance/periodic_job_enqueuer.go`:

```go
package maintenance

import (
	"context"
	"errors"
	"slices"
	"sync"
	"time"

	"github.com/tidwall/sjson"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/maputil"
	"github.com/riverqueue/river/rivertype"
)

// ErrNoJobToInsert can be returned by a PeriodicJob's JobToInsertFunc to
// signal that there's no job to insert at this time.
var ErrNoJobToInsert = errors.New("a nil job was returned, nothing to insert")

// Test-only properties.
type PeriodicJobEnqueuerTestSignals struct {
	EnteredLoop  testsignal.TestSignal[struct{}] // notifies when the enqueuer finishes start up and enters its initial run loop
	InsertedJobs testsignal.TestSignal[struct{}] // notifies when a batch of jobs is inserted
	SkippedJob   testsignal.TestSignal[struct{}] // notifies when a job is skipped because of nil JobInsertParams
}

func (ts *PeriodicJobEnqueuerTestSignals) Init() {
	ts.EnteredLoop.Init()
	ts.InsertedJobs.Init()
	ts.SkippedJob.Init()
}

// PeriodicJob is a periodic job to be run. It's similar to the top-level
// river.PeriodicJobArgs, but needs a separate type because the enqueuer is in a
// subpackage.
type PeriodicJob struct {
	ConstructorFunc func() (*rivertype.JobInsertParams, error)
	RunOnStart      bool
	ScheduleFunc    func(time.Time) time.Time

	nextRunAt time.Time // set on service start
}

func (j *PeriodicJob) mustValidate() *PeriodicJob {
	if j.ScheduleFunc == nil {
		panic("PeriodicJob.ScheduleFunc must be set")
	}
	if j.ConstructorFunc == nil {
		panic("PeriodicJob.ConstructorFunc must be set")
	}

	return j
}

type PeriodicJobEnqueuerConfig struct {
	AdvisoryLockPrefix int32

	// Insert is the function to call to insert jobs into the database.
	Insert InsertFunc

	// PeriodicJobs are the periodic jobs with which to configure the enqueuer.
	PeriodicJobs []*PeriodicJob
}

func (c *PeriodicJobEnqueuerConfig) mustValidate() *PeriodicJobEnqueuerConfig {
	return c
}

// PeriodicJobEnqueuer inserts jobs configured to run periodically as unique
// jobs to make sure they'll run as frequently as their period dictates.
type PeriodicJobEnqueuer struct {
	queueMaintainerServiceBase
	startstop.BaseStartStop

	// exported for test purposes
	Config      *PeriodicJobEnqueuerConfig
	TestSignals PeriodicJobEnqueuerTestSignals

	exec               riverdriver.Executor
	mu                 sync.RWMutex
	nextHandle         rivertype.PeriodicJobHandle
	periodicJobs       map[rivertype.PeriodicJobHandle]*PeriodicJob
	recalculateNextRun chan struct{}
}

func NewPeriodicJobEnqueuer(archetype *baseservice.Archetype, config *PeriodicJobEnqueuerConfig, exec riverdriver.Executor) *PeriodicJobEnqueuer {
	var (
		nextHandle   rivertype.PeriodicJobHandle
		periodicJobs = make(map[rivertype.PeriodicJobHandle]*PeriodicJob, len(config.PeriodicJobs))
	)

	for _, periodicJob := range config.PeriodicJobs {
		periodicJob.mustValidate()

		periodicJobs[nextHandle] = periodicJob
		nextHandle++
	}

	svc := baseservice.Init(archetype, &PeriodicJobEnqueuer{
		Config: (&PeriodicJobEnqueuerConfig{
			AdvisoryLockPrefix: config.AdvisoryLockPrefix,
			Insert:             config.Insert,
			PeriodicJobs:       config.PeriodicJobs,
		}).mustValidate(),

		exec:               exec,
		nextHandle:         nextHandle,
		periodicJobs:       periodicJobs,
		recalculateNextRun: make(chan struct{}, 1),
	})

	return svc
}

// Add adds a new periodic job to the enqueuer. The service's run loop is woken
// immediately so that the job is scheduled appropriately, and inserted if its
// RunOnStart flag is set to true.
func (s *PeriodicJobEnqueuer) Add(periodicJob *PeriodicJob) rivertype.PeriodicJobHandle {
	s.mu.Lock()
	defer s.mu.Unlock()

	periodicJob.mustValidate()

	handle := s.nextHandle
	s.periodicJobs[handle] = periodicJob
	s.nextHandle++

	select {
	case s.recalculateNextRun <- struct{}{}:
	default:
	}

	return handle
}

// AddMany adds many new periodic job to the enqueuer. The service's run loop is
// woken immediately so that the job is scheduled appropriately, and inserted if
// any RunOnStart flags are set to true.
func (s *PeriodicJobEnqueuer) AddMany(periodicJobs []*PeriodicJob) []rivertype.PeriodicJobHandle {
	s.mu.Lock()
	defer s.mu.Unlock()

	handles := make([]rivertype.PeriodicJobHandle, len(periodicJobs))

	for i, periodicJob := range periodicJobs {
		periodicJob.mustValidate()

		handles[i] = s.nextHandle
		s.periodicJobs[handles[i]] = periodicJob
		s.nextHandle++
	}

	select {
	case s.recalculateNextRun <- struct{}{}:
	default:
	}

	return handles
}

// Clear clears all periodic jobs from the enqueuer.
func (s *PeriodicJobEnqueuer) Clear() {
	s.mu.Lock()
	defer s.mu.Unlock()

	// `nextHandle` is _not_ reset so that even across multiple generations of
	// jobs, handles aren't reused.
	s.periodicJobs = make(map[rivertype.PeriodicJobHandle]*PeriodicJob)
}

// Remove removes a periodic job from the enqueuer. Its current target run time
// and all future runs are cancelled.
func (s *PeriodicJobEnqueuer) Remove(periodicJobHandle rivertype.PeriodicJobHandle) {
	s.mu.Lock()
	defer s.mu.Unlock()

	delete(s.periodicJobs, periodicJobHandle)
}

// RemoveMany removes many periodic jobs from the enqueuer. Their current target
// run time and all future runs are cancelled.
func (s *PeriodicJobEnqueuer) RemoveMany(periodicJobHandles []rivertype.PeriodicJobHandle) {
	s.mu.Lock()
	defer s.mu.Unlock()

	for _, handle := range periodicJobHandles {
		delete(s.periodicJobs, handle)
	}
}

func (s *PeriodicJobEnqueuer) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	s.StaggerStart(ctx)

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStarted)
		defer s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStopped)

		// Drain the signal to recalculate next run if it's been sent (i.e. Add
		// or AddMany called before Start). We're about to schedule jobs from
		// scratch, and therefore don't need to immediately do so again.
		select {
		case <-s.recalculateNextRun:
		default:
		}

		var lastHandleSeen rivertype.PeriodicJobHandle = -1 // so handle 0 is considered

		validateInsertRunOnStartAndScheduleNewlyAdded := func() {
			s.mu.RLock()
			defer s.mu.RUnlock()

			var (
				insertParamsMany []*rivertype.JobInsertParams
				now              = s.Time.NowUTC()
			)

			// Handle periodic jobs in sorted order so we can correctly account
			// for the most recently added one that we've seen.
			sortedPeriodicJobHandles := maputil.Keys(s.periodicJobs)
			slices.Sort(sortedPeriodicJobHandles)

			for _, handle := range sortedPeriodicJobHandles {
				if handle <= lastHandleSeen {
					continue
				}

				lastHandleSeen = handle

				periodicJob := s.periodicJobs[handle].mustValidate()

				periodicJob.nextRunAt = periodicJob.ScheduleFunc(now)

				if !periodicJob.RunOnStart {
					continue
				}

				if insertParams, ok := s.insertParamsFromConstructor(ctx, periodicJob.ConstructorFunc, now); ok {
					insertParamsMany = append(insertParamsMany, insertParams)
				}
			}

			s.insertBatch(ctx, insertParamsMany)

			if len(insertParamsMany) > 0 {
				s.Logger.DebugContext(ctx, s.Name+": Inserted RunOnStart jobs", "num_jobs", len(insertParamsMany))
			}
		}

		// Run any jobs that need to run on start and calculate initial runs.
		validateInsertRunOnStartAndScheduleNewlyAdded()

		s.TestSignals.EnteredLoop.Signal(struct{}{})

		timerUntilNextRun := time.NewTimer(s.timeUntilNextRun())

		for {
			select {
			case <-timerUntilNextRun.C:
				var insertParamsMany []*rivertype.JobInsertParams

				now := s.Time.NowUTC()

				// Add a small margin to the current time so we're not only
				// running jobs that are already ready, but also ones ready at
				// this exact moment or ready in the very near future.
				nowWithMargin := now.Add(100 * time.Millisecond)

				func() {
					s.mu.RLock()
					defer s.mu.RUnlock()

					for _, periodicJob := range s.periodicJobs {
						if periodicJob.nextRunAt.IsZero() || !periodicJob.nextRunAt.Before(nowWithMargin) {
							continue
						}

						if insertParams, ok := s.insertParamsFromConstructor(ctx, periodicJob.ConstructorFunc, periodicJob.nextRunAt); ok {
							insertParamsMany = append(insertParamsMany, insertParams)
						}

						// Although we may have inserted a new job a little
						// preemptively due to the margin applied above, try to stay
						// as true as possible to the original schedule by using the
						// original run time when calculating the next one.
						periodicJob.nextRunAt = periodicJob.ScheduleFunc(periodicJob.nextRunAt)
					}
				}()

				s.insertBatch(ctx, insertParamsMany)

			case <-s.recalculateNextRun:
				if !timerUntilNextRun.Stop() {
					<-timerUntilNextRun.C
				}

			case <-ctx.Done():
				// Clean up timer resources. We know it has _not_ received from the
				// timer since its last reset because that would have led us to the case
				// above instead of here.
				if !timerUntilNextRun.Stop() {
					<-timerUntilNextRun.C
				}
				return
			}

			// Insert any RunOnStart initial runs for new jobs that've been
			// added since the last run loop.
			validateInsertRunOnStartAndScheduleNewlyAdded()

			// Reset the timer after the insert loop has finished so it's
			// paused during work. Makes its firing more deterministic.
			timerUntilNextRun.Reset(s.timeUntilNextRun())
		}
	}()

	return nil
}

func (s *PeriodicJobEnqueuer) insertBatch(ctx context.Context, insertParamsMany []*rivertype.JobInsertParams) {
	if len(insertParamsMany) == 0 {
		return
	}

	tx, err := s.exec.Begin(ctx)
	if err != nil {
		s.Logger.ErrorContext(ctx, s.Name+": Error starting transaction", "error", err.Error())
		return
	}
	defer tx.Rollback(ctx)

	if len(insertParamsMany) > 0 {
		_, err := s.Config.Insert(ctx, tx, insertParamsMany)
		if err != nil {
			s.Logger.ErrorContext(ctx, s.Name+": Error inserting periodic jobs",
				"error", err.Error(), "num_jobs", len(insertParamsMany))
			return
		}
	}

	if err := tx.Commit(ctx); err != nil {
		s.Logger.ErrorContext(ctx, s.Name+": Error committing transaction", "error", err.Error())
		return
	}

	s.TestSignals.InsertedJobs.Signal(struct{}{})
}

func (s *PeriodicJobEnqueuer) insertParamsFromConstructor(ctx context.Context, constructorFunc func() (*rivertype.JobInsertParams, error), scheduledAt time.Time) (*rivertype.JobInsertParams, bool) {
	insertParams, err := constructorFunc()
	if err != nil {
		if errors.Is(err, ErrNoJobToInsert) {
			s.Logger.InfoContext(ctx, s.Name+": nil returned from periodic job constructor, skipping")
			s.TestSignals.SkippedJob.Signal(struct{}{})
			return nil, false
		}
		s.Logger.ErrorContext(ctx, s.Name+": Internal error generating periodic job", "error", err.Error())
		return nil, false
	}

	if insertParams.ScheduledAt == nil {
		insertParams.ScheduledAt = &scheduledAt
	}

	if insertParams.Metadata, err = sjson.SetBytes(insertParams.Metadata, "periodic", true); err != nil {
		s.Logger.ErrorContext(ctx, s.Name+": Error setting periodic metadata", "error", err.Error())
		return nil, false
	}

	return insertParams, true
}

const periodicJobEnqueuerVeryLongDuration = 24 * time.Hour

func (s *PeriodicJobEnqueuer) timeUntilNextRun() time.Duration {
	s.mu.RLock()
	defer s.mu.RUnlock()

	// With no configured jobs, just return a big duration for the loop to block
	// on.
	if len(s.periodicJobs) < 1 {
		return periodicJobEnqueuerVeryLongDuration
	}

	var (
		firstNextRunAt time.Time
		now            = s.Time.NowUTC()
	)

	for _, periodicJob := range s.periodicJobs {
		// Jobs may have been added after service start, but before this
		// function runs for the first time. They're not scheduled properly yet,
		// but they will be soon, at which point this function will run again.
		// Skip them for now.
		if periodicJob.nextRunAt.IsZero() {
			continue
		}

		// In case we detect a job that should've run before now, immediately short
		// circuit with a 0 duration. This avoids needlessly iterating through the
		// rest of the loop when we already know we're overdue for the next job.
		if periodicJob.nextRunAt.Before(now) {
			return 0
		}

		if firstNextRunAt.IsZero() || periodicJob.nextRunAt.Before(firstNextRunAt) {
			firstNextRunAt = periodicJob.nextRunAt
		}
	}

	// Only encountered unscheduled jobs (see comment above). Don't schedule
	// anything for now.
	if firstNextRunAt.IsZero() {
		return periodicJobEnqueuerVeryLongDuration
	}

	return firstNextRunAt.Sub(now)
}

```

`internal/maintenance/periodic_job_enqueuer_test.go`:

```go
package maintenance

import (
	"context"
	"fmt"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/dbunique"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

type noOpArgs struct{}

func (noOpArgs) Kind() string { return "no_op" }

func TestPeriodicJobEnqueuer(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		exec                 riverdriver.Executor
		notificationsByQueue map[string]int
		waitChan             chan (struct{})
	}

	stubSvc := &riversharedtest.TimeStub{}
	stubSvc.StubNowUTC(time.Now().UTC())

	jobConstructorWithQueueFunc := func(name string, unique bool, queue string) func() (*rivertype.JobInsertParams, error) {
		return func() (*rivertype.JobInsertParams, error) {
			params := &rivertype.JobInsertParams{
				Args:        noOpArgs{},
				EncodedArgs: []byte("{}"),
				Kind:        name,
				MaxAttempts: rivercommon.MaxAttemptsDefault,
				Priority:    rivercommon.PriorityDefault,
				Queue:       queue,
				State:       rivertype.JobStateAvailable,
			}
			if unique {
				uniqueOpts := &dbunique.UniqueOpts{ByArgs: true}
				var err error
				params.UniqueKey, err = dbunique.UniqueKey(stubSvc, uniqueOpts, params)
				if err != nil {
					return nil, err
				}

				params.UniqueStates = uniqueOpts.StateBitmask()
			}

			return params, nil
		}
	}

	jobConstructorFunc := func(name string, unique bool) func() (*rivertype.JobInsertParams, error) {
		return jobConstructorWithQueueFunc(name, unique, rivercommon.QueueDefault)
	}

	periodicIntervalSchedule := func(d time.Duration) func(time.Time) time.Time {
		return func(t time.Time) time.Time {
			return t.Add(d)
		}
	}

	// A simplified version of `Client.insertMany` that only inserts jobs directly
	// via the driver instead of using the pilot.
	insertFunc := func(ctx context.Context, tx riverdriver.ExecutorTx, insertParams []*rivertype.JobInsertParams) ([]*rivertype.JobInsertResult, error) {
		finalInsertParams := sliceutil.Map(insertParams, func(params *rivertype.JobInsertParams) *riverdriver.JobInsertFastParams {
			return (*riverdriver.JobInsertFastParams)(params)
		})
		results, err := tx.JobInsertFastMany(ctx, finalInsertParams)
		if err != nil {
			return nil, err
		}
		return sliceutil.Map(results,
			func(result *riverdriver.JobInsertFastResult) *rivertype.JobInsertResult {
				return (*rivertype.JobInsertResult)(result)
			},
		), nil
	}

	setup := func(t *testing.T) (*PeriodicJobEnqueuer, *testBundle) {
		t.Helper()

		bundle := &testBundle{
			exec:                 riverpgxv5.New(riverinternaltest.TestDB(ctx, t)).GetExecutor(),
			notificationsByQueue: make(map[string]int),
			waitChan:             make(chan struct{}),
		}

		svc := NewPeriodicJobEnqueuer(riversharedtest.BaseServiceArchetype(t), &PeriodicJobEnqueuerConfig{Insert: insertFunc}, bundle.exec)
		svc.StaggerStartupDisable(true)
		svc.TestSignals.Init()

		return svc, bundle
	}

	requireNJobs := func(t *testing.T, exec riverdriver.Executor, kind string, n int) []*rivertype.JobRow {
		t.Helper()

		jobs, err := exec.JobGetByKindMany(ctx, []string{kind})
		require.NoError(t, err)
		require.Len(t, jobs, n, "Expected to find exactly %d job(s) of kind: %s, but found %d", n, kind, len(jobs))

		return jobs
	}

	startService := func(t *testing.T, svc *PeriodicJobEnqueuer) {
		t.Helper()

		require.NoError(t, svc.Start(ctx))
		t.Cleanup(svc.Stop)

		riversharedtest.WaitOrTimeout(t, startstop.WaitAllStartedC(svc))
	}

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)
		svc.Logger = riversharedtest.LoggerWarn(t)         // loop started/stop log is very noisy; suppress
		svc.TestSignals = PeriodicJobEnqueuerTestSignals{} // deinit so channels don't fill

		startstoptest.Stress(ctx, t, svc)
	})

	// This test run is somewhat susceptible to the "ready margin" applied on
	// enqueuer loops to find jobs that aren't quite ready yet, but close
	// enough. The 500 ms/1500 ms job types can have their ready times diverge
	// slightly as they're enqueued separately. Usually they're ~identical, but
	// a large enough divergence which can occur with `-race` and a hundred test
	// iterations can cause the test to fail as an expected job wasn't enqueued
	// on the expected loop. The ready margin is currently high enough (100 ms)
	// that this problem won't occur, but in case it's ever substantially
	// lowered, this test will need to be rewritten.
	t.Run("EnqueuesPeriodicJobs", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false)},
			{ScheduleFunc: periodicIntervalSchedule(1500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_1500ms", false)},
		})

		startService(t, svc)

		// Should be no jobs to start.
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 0)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 1)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 2)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 3)
		requireNJobs(t, bundle.exec, "periodic_job_1500ms", 1)
	})

	t.Run("SetsPeriodicMetadataAttribute", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		jobConstructorWithMetadata := func(name string, metadata []byte) func() (*rivertype.JobInsertParams, error) {
			return func() (*rivertype.JobInsertParams, error) {
				params, err := jobConstructorFunc(name, false)()
				if err != nil {
					return nil, err
				}
				params.Metadata = metadata
				return params, nil
			}
		}

		svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorWithMetadata("p_md_nil", nil)},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorWithMetadata("p_md_empty_string", []byte(""))},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorWithMetadata("p_md_empty_obj", []byte("{}"))},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorWithMetadata("p_md_existing", []byte(`{"key": "value"}`))},
		})

		startService(t, svc)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()

		assertMetadata := func(name string, expected string) {
			job := requireNJobs(t, bundle.exec, name, 1)[0]
			require.JSONEq(t, expected, string(job.Metadata))
		}

		assertMetadata("p_md_nil", `{"periodic": true}`)
		assertMetadata("p_md_empty_string", `{"periodic": true}`)
		assertMetadata("p_md_empty_obj", `{"periodic": true}`)
		assertMetadata("p_md_existing", `{"key": "value", "periodic": true}`)
	})

	t.Run("SetsScheduledAtAccordingToExpectedNextRunAt", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false), RunOnStart: true},
		})

		startService(t, svc)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		job1 := requireNJobs(t, bundle.exec, "periodic_job_500ms", 1)[0]
		require.Equal(t, rivertype.JobStateAvailable, job1.State)
		require.WithinDuration(t, time.Now(), job1.ScheduledAt, 1*time.Second)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		job2 := requireNJobs(t, bundle.exec, "periodic_job_500ms", 2)[1] // ordered by ID

		// The new `scheduled_at` is *exactly* the original `scheduled_at` plus
		// 500 milliseconds because the enqueuer used the target next run time
		// to calculate the new `scheduled_at`.
		require.Equal(t, job1.ScheduledAt.Add(500*time.Millisecond), job2.ScheduledAt)

		require.Equal(t, rivertype.JobStateAvailable, job2.State)
	})

	t.Run("RespectsJobUniqueness", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("unique_periodic_job_500ms", true)},
		})

		startService(t, svc)

		// Should be no jobs to start.
		requireNJobs(t, bundle.exec, "unique_periodic_job_500ms", 0)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "unique_periodic_job_500ms", 1)

		// Another insert was attempted, but there's still only one job due to
		// uniqueness conditions.
		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "unique_periodic_job_500ms", 1)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "unique_periodic_job_500ms", 1)
	})

	t.Run("RunOnStart", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(5 * time.Second), ConstructorFunc: jobConstructorFunc("periodic_job_5s", false), RunOnStart: true},
			{ScheduleFunc: periodicIntervalSchedule(5 * time.Second), ConstructorFunc: jobConstructorFunc("unique_periodic_job_5s", true), RunOnStart: true},
		})

		start := time.Now()
		startService(t, svc)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_5s", 1)
		requireNJobs(t, bundle.exec, "unique_periodic_job_5s", 1)

		// Should've happened quite quickly.
		require.WithinDuration(t, time.Now(), start, 1*time.Second)
	})

	t.Run("ErrNoJobToInsert", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		svc.AddMany([]*PeriodicJob{
			// skip this insert when it returns nil:
			{ScheduleFunc: periodicIntervalSchedule(time.Second), ConstructorFunc: func() (*rivertype.JobInsertParams, error) {
				return nil, ErrNoJobToInsert
			}, RunOnStart: true},
		})

		startService(t, svc)

		svc.TestSignals.SkippedJob.WaitOrTimeout()
	})

	t.Run("InitialScheduling", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		now := svc.Time.StubNowUTC(time.Now())

		svc.periodicJobs = make(map[rivertype.PeriodicJobHandle]*PeriodicJob)
		periodicJobHandles := svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false)},
			{ScheduleFunc: periodicIntervalSchedule(1500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_1500ms", false)},
			{ScheduleFunc: periodicIntervalSchedule(5 * time.Second), ConstructorFunc: jobConstructorFunc("periodic_job_5s", false)},
			{ScheduleFunc: periodicIntervalSchedule(15 * time.Minute), ConstructorFunc: jobConstructorFunc("periodic_job_15m", false)},
			{ScheduleFunc: periodicIntervalSchedule(3 * time.Hour), ConstructorFunc: jobConstructorFunc("periodic_job_3h", false)},
			{ScheduleFunc: periodicIntervalSchedule(7 * 24 * time.Hour), ConstructorFunc: jobConstructorFunc("periodic_job_7d", false)},
		})

		startService(t, svc)

		svc.TestSignals.EnteredLoop.WaitOrTimeout()

		require.Equal(t, now.Add(500*time.Millisecond), svc.periodicJobs[periodicJobHandles[0]].nextRunAt)
		require.Equal(t, now.Add(1500*time.Millisecond), svc.periodicJobs[periodicJobHandles[1]].nextRunAt)
		require.Equal(t, now.Add(5*time.Second), svc.periodicJobs[periodicJobHandles[2]].nextRunAt)
		require.Equal(t, now.Add(15*time.Minute), svc.periodicJobs[periodicJobHandles[3]].nextRunAt)
		require.Equal(t, now.Add(3*time.Hour), svc.periodicJobs[periodicJobHandles[4]].nextRunAt)
		require.Equal(t, now.Add(7*24*time.Hour), svc.periodicJobs[periodicJobHandles[5]].nextRunAt)

		// Schedules a job for the distant future. This is so that we can remove
		// jobs from the running and verify that waiting until each successive
		// periodic job really works.
		scheduleDistantFuture := func(periodicJob *PeriodicJob) {
			// It may feel a little heavy-handed to stop and start the service
			// for each job we check, but the alternative is an internal locking
			// system needed for the tests only because modifying a job while
			// the service is running will be detected by `-race`.
			svc.Stop()

			periodicJob.ScheduleFunc = periodicIntervalSchedule(365 * 24 * time.Hour)

			require.NoError(t, svc.Start(ctx))
			svc.TestSignals.EnteredLoop.WaitOrTimeout()
		}

		require.Equal(t, 500*time.Millisecond, svc.timeUntilNextRun())

		scheduleDistantFuture(svc.periodicJobs[periodicJobHandles[0]])
		require.Equal(t, 1500*time.Millisecond, svc.timeUntilNextRun())

		scheduleDistantFuture(svc.periodicJobs[periodicJobHandles[1]])
		require.Equal(t, 5*time.Second, svc.timeUntilNextRun())

		scheduleDistantFuture(svc.periodicJobs[periodicJobHandles[2]])
		require.Equal(t, 15*time.Minute, svc.timeUntilNextRun())

		scheduleDistantFuture(svc.periodicJobs[periodicJobHandles[3]])
		require.Equal(t, 3*time.Hour, svc.timeUntilNextRun())

		scheduleDistantFuture(svc.periodicJobs[periodicJobHandles[4]])
		require.Equal(t, 7*24*time.Hour, svc.timeUntilNextRun())
	})

	// To ensure we are protected against runs that are supposed to have already happened,
	// this test uses a totally-not-safe schedule to enqueue every 0.5ms.
	t.Run("RapidScheduling", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		svc.Add(&PeriodicJob{ScheduleFunc: periodicIntervalSchedule(time.Microsecond), ConstructorFunc: jobConstructorFunc("periodic_job_1us", false)})
		// make a longer list of jobs so the loop has to run for longer
		for i := 1; i < 100; i++ {
			svc.Add(&PeriodicJob{
				ScheduleFunc:    periodicIntervalSchedule(time.Duration(i) * time.Hour),
				ConstructorFunc: jobConstructorFunc(fmt.Sprintf("periodic_job_%dh", i), false),
			})
		}

		startService(t, svc)

		svc.TestSignals.EnteredLoop.WaitOrTimeout()

		for range 100 {
			svc.TestSignals.InsertedJobs.WaitOrTimeout()
		}
	})

	t.Run("ConfigurableViaConstructor", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		svc := NewPeriodicJobEnqueuer(
			riversharedtest.BaseServiceArchetype(t),
			&PeriodicJobEnqueuerConfig{
				Insert: insertFunc,
				PeriodicJobs: []*PeriodicJob{
					{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false), RunOnStart: true},
					{ScheduleFunc: periodicIntervalSchedule(1500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_1500ms", false), RunOnStart: true},
				},
			}, bundle.exec)
		svc.StaggerStartupDisable(true)
		svc.TestSignals.Init()

		startService(t, svc)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 1)
		requireNJobs(t, bundle.exec, "periodic_job_1500ms", 1)
	})

	t.Run("AddAfterStart", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		startService(t, svc)

		svc.Add(
			&PeriodicJob{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false)},
		)
		svc.Add(
			&PeriodicJob{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms_start", false), RunOnStart: true},
		)

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 0)
		requireNJobs(t, bundle.exec, "periodic_job_500ms_start", 1)
	})

	t.Run("AddManyAfterStart", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		startService(t, svc)

		svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false)},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms_start", false), RunOnStart: true},
		})

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 0)
		requireNJobs(t, bundle.exec, "periodic_job_500ms_start", 1)
	})

	t.Run("ClearAfterStart", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		startService(t, svc)

		handles := svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false)},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms_start", false), RunOnStart: true},
		})

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 0)
		requireNJobs(t, bundle.exec, "periodic_job_500ms_start", 1)

		svc.Clear()

		require.Empty(t, svc.periodicJobs)

		handleAfterClear := svc.Add(
			&PeriodicJob{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms_new", false)},
		)

		// Handles are not reused.
		require.NotEqual(t, handles[0], handleAfterClear)
		require.NotEqual(t, handles[1], handleAfterClear)
	})

	t.Run("RemoveAfterStart", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		startService(t, svc)

		handles := svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false)},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms_start", false), RunOnStart: true},
		})

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 0)
		requireNJobs(t, bundle.exec, "periodic_job_500ms_start", 1)

		svc.Remove(handles[1])

		require.Len(t, svc.periodicJobs, 1)
	})

	t.Run("RemoveManyAfterStart", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		startService(t, svc)

		handles := svc.AddMany([]*PeriodicJob{
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms", false)},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms_other", false)},
			{ScheduleFunc: periodicIntervalSchedule(500 * time.Millisecond), ConstructorFunc: jobConstructorFunc("periodic_job_500ms_start", false), RunOnStart: true},
		})

		svc.TestSignals.InsertedJobs.WaitOrTimeout()
		requireNJobs(t, bundle.exec, "periodic_job_500ms", 0)
		requireNJobs(t, bundle.exec, "periodic_job_500ms_other", 0)
		requireNJobs(t, bundle.exec, "periodic_job_500ms_start", 1)

		svc.RemoveMany([]rivertype.PeriodicJobHandle{handles[1], handles[2]})

		require.Len(t, svc.periodicJobs, 1)
	})

	// To suss out any race conditions in the add/remove/clear/run loop code,
	// and interactions between them.
	t.Run("AddRemoveStress", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		var wg sync.WaitGroup

		randomSleep := func() {
			time.Sleep(time.Duration(randutil.IntBetween(1, 5)) * time.Millisecond)
		}

		for i := range 10 {
			wg.Add(1)

			jobBaseName := fmt.Sprintf("periodic_job_1ms_%02d", i)

			go func() {
				defer wg.Done()

				for range 50 {
					handle := svc.Add(&PeriodicJob{ScheduleFunc: periodicIntervalSchedule(time.Millisecond), ConstructorFunc: jobConstructorFunc(jobBaseName, false)})
					randomSleep()

					svc.Add(&PeriodicJob{ScheduleFunc: periodicIntervalSchedule(time.Millisecond), ConstructorFunc: jobConstructorFunc(jobBaseName+"_second", false)})
					randomSleep()

					svc.Remove(handle)
					randomSleep()

					svc.Clear()
					randomSleep()
				}
			}()
		}

		wg.Wait()
	})

	t.Run("NoJobsConfigured", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		startService(t, svc)

		svc.TestSignals.EnteredLoop.WaitOrTimeout()

		require.LessOrEqual(t, svc.timeUntilNextRun(), 24*time.Hour)
	})

	t.Run("StopsImmediately", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		startService(t, svc)
		svc.Stop()
	})

	t.Run("RespectsContextCancellation", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		ctx, cancelFunc := context.WithCancel(ctx)
		require.NoError(t, svc.Start(ctx))

		// To avoid a potential race, make sure to get a reference to the
		// service's stopped channel _before_ cancellation as it's technically
		// possible for the cancel to "win" and remove the stopped channel
		// before we can start waiting on it.
		stopped := svc.Stopped()
		cancelFunc()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("TimeUntilNextRun", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		now := svc.Time.StubNowUTC(time.Now())

		// no jobs
		require.Equal(t, periodicJobEnqueuerVeryLongDuration, svc.timeUntilNextRun())

		svc.periodicJobs = map[rivertype.PeriodicJobHandle]*PeriodicJob{
			1: {nextRunAt: now.Add(2 * time.Hour)},
			2: {nextRunAt: now.Add(1 * time.Hour)},
			3: {nextRunAt: now.Add(3 * time.Hour)},
		}

		// pick job with soonest next run
		require.Equal(t, 1*time.Hour, svc.timeUntilNextRun())

		svc.periodicJobs = map[rivertype.PeriodicJobHandle]*PeriodicJob{
			1: {nextRunAt: now.Add(2 * time.Hour)},
			2: {nextRunAt: now.Add(-1 * time.Hour)},
			3: {nextRunAt: now.Add(3 * time.Hour)},
		}

		// job is already behind so time until next run is 0
		require.Equal(t, time.Duration(0), svc.timeUntilNextRun())

		svc.periodicJobs = map[rivertype.PeriodicJobHandle]*PeriodicJob{
			1: {},
			2: {},
		}

		// jobs not scheduled yet
		require.Equal(t, periodicJobEnqueuerVeryLongDuration, svc.timeUntilNextRun())

		svc.periodicJobs = map[rivertype.PeriodicJobHandle]*PeriodicJob{
			1: {},
			2: {nextRunAt: now.Add(1 * time.Hour)},
			3: {},
		}

		// pick job with soonest next run amongst some not scheduled yet
		require.Equal(t, 1*time.Hour, svc.timeUntilNextRun())
	})
}

```

`internal/maintenance/queue_cleaner.go`:

```go
package maintenance

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"strings"
	"time"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
)

const (
	queueCleanerIntervalDefault = time.Hour
	QueueRetentionPeriodDefault = 24 * time.Hour
)

// Test-only properties.
type QueueCleanerTestSignals struct {
	DeletedBatch testsignal.TestSignal[struct{}] // notifies when runOnce finishes a pass
}

func (ts *QueueCleanerTestSignals) Init() {
	ts.DeletedBatch.Init()
}

type QueueCleanerConfig struct {
	// Interval is the amount of time to wait between runs of the cleaner.
	Interval time.Duration
	// RetentionPeriod is the amount of time to keep queues around before they're
	// removed.
	RetentionPeriod time.Duration
}

func (c *QueueCleanerConfig) mustValidate() *QueueCleanerConfig {
	if c.Interval <= 0 {
		panic("QueueCleanerConfig.Interval must be above zero")
	}
	if c.RetentionPeriod <= 0 {
		panic("QueueCleanerConfig.RetentionPeriod must be above zero")
	}

	return c
}

// QueueCleaner periodically removes queues from the river_queue table that have
// not been updated in a while, indicating that they are no longer active.
type QueueCleaner struct {
	queueMaintainerServiceBase
	startstop.BaseStartStop

	// exported for test purposes
	Config      *QueueCleanerConfig
	TestSignals QueueCleanerTestSignals

	batchSize int // configurable for test purposes
	exec      riverdriver.Executor
}

func NewQueueCleaner(archetype *baseservice.Archetype, config *QueueCleanerConfig, exec riverdriver.Executor) *QueueCleaner {
	return baseservice.Init(archetype, &QueueCleaner{
		Config: (&QueueCleanerConfig{
			Interval:        valutil.ValOrDefault(config.Interval, queueCleanerIntervalDefault),
			RetentionPeriod: valutil.ValOrDefault(config.RetentionPeriod, QueueRetentionPeriodDefault),
		}).mustValidate(),

		batchSize: BatchSizeDefault,
		exec:      exec,
	})
}

func (s *QueueCleaner) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	s.StaggerStart(ctx)

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStarted)
		defer s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStopped)

		ticker := timeutil.NewTickerWithInitialTick(ctx, s.Config.Interval)
		for {
			select {
			case <-ctx.Done():
				return
			case <-ticker.C:
			}

			res, err := s.runOnce(ctx)
			if err != nil {
				if !errors.Is(err, context.Canceled) {
					s.Logger.ErrorContext(ctx, s.Name+": Error cleaning queues", slog.String("error", err.Error()))
				}
				continue
			}

			if len(res.QueuesDeleted) > 0 {
				s.Logger.InfoContext(ctx, s.Name+logPrefixRanSuccessfully,
					slog.String("queues_deleted", strings.Join(res.QueuesDeleted, ",")),
				)
			}
		}
	}()

	return nil
}

type queueCleanerRunOnceResult struct {
	QueuesDeleted []string
}

func (s *QueueCleaner) runOnce(ctx context.Context) (*queueCleanerRunOnceResult, error) {
	res := &queueCleanerRunOnceResult{QueuesDeleted: make([]string, 0, 10)}

	for {
		// Wrapped in a function so that defers run as expected.
		queuesDeleted, err := func() ([]string, error) {
			ctx, cancelFunc := context.WithTimeout(ctx, 30*time.Second)
			defer cancelFunc()

			queuesDeleted, err := s.exec.QueueDeleteExpired(ctx, &riverdriver.QueueDeleteExpiredParams{
				Max:              s.batchSize,
				UpdatedAtHorizon: time.Now().Add(-s.Config.RetentionPeriod),
			})
			if err != nil {
				return nil, fmt.Errorf("error deleting expired queues: %w", err)
			}

			return queuesDeleted, nil
		}()
		if err != nil {
			return nil, err
		}

		s.TestSignals.DeletedBatch.Signal(struct{}{})

		res.QueuesDeleted = append(res.QueuesDeleted, queuesDeleted...)
		// Deleted was less than query `LIMIT` which means work is done.
		if len(queuesDeleted) < s.batchSize {
			break
		}

		serviceutil.CancellableSleep(ctx, randutil.DurationBetween(BatchBackoffMin, BatchBackoffMax))
	}

	return res, nil
}

```

`internal/maintenance/queue_cleaner_test.go`:

```go
package maintenance

import (
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

func TestQueueCleaner(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		deleteHorizon time.Time
		exec          riverdriver.Executor
	}

	setup := func(t *testing.T) (*QueueCleaner, *testBundle) {
		t.Helper()

		tx := riverinternaltest.TestTx(ctx, t)
		bundle := &testBundle{
			deleteHorizon: time.Now().Add(-QueueRetentionPeriodDefault),
			exec:          riverpgxv5.New(nil).UnwrapExecutor(tx),
		}

		cleaner := NewQueueCleaner(
			riversharedtest.BaseServiceArchetype(t),
			&QueueCleanerConfig{
				Interval:        queueCleanerIntervalDefault,
				RetentionPeriod: QueueRetentionPeriodDefault,
			},
			bundle.exec)
		cleaner.StaggerStartupDisable(true)
		cleaner.TestSignals.Init()
		t.Cleanup(cleaner.Stop)

		return cleaner, bundle
	}

	t.Run("Defaults", func(t *testing.T) {
		t.Parallel()

		cleaner := NewQueueCleaner(riversharedtest.BaseServiceArchetype(t), &QueueCleanerConfig{}, nil)

		require.Equal(t, QueueRetentionPeriodDefault, cleaner.Config.RetentionPeriod)
		require.Equal(t, queueCleanerIntervalDefault, cleaner.Config.Interval)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Logger = riversharedtest.LoggerWarn(t)  // loop started/stop log is very noisy; suppress
		cleaner.TestSignals = QueueCleanerTestSignals{} // deinit so channels don't fill

		startstoptest.Stress(ctx, t, cleaner)
	})

	t.Run("DeletesExpiredQueues", func(t *testing.T) {
		t.Parallel()

		cleaner, bundle := setup(t)

		now := time.Now()
		// None of these should get removed:
		queue1 := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{Name: ptrutil.Ptr("queue1"), UpdatedAt: ptrutil.Ptr(now)})
		queue2 := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{Name: ptrutil.Ptr("queue2"), UpdatedAt: ptrutil.Ptr(now.Add(-23 * time.Hour))})

		// These get deleted:
		queue3 := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{Name: ptrutil.Ptr("queue3"), UpdatedAt: ptrutil.Ptr(now.Add(-25 * time.Hour))})
		queue4 := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{Name: ptrutil.Ptr("queue4"), UpdatedAt: ptrutil.Ptr(now.Add(-26 * time.Hour))})
		queue5 := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{Name: ptrutil.Ptr("queue5"), UpdatedAt: ptrutil.Ptr(now.Add(-48 * time.Hour))})

		require.NoError(t, cleaner.Start(ctx))

		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		var err error
		_, err = bundle.exec.QueueGet(ctx, queue1.Name)
		require.NoError(t, err) // still there
		_, err = bundle.exec.QueueGet(ctx, queue2.Name)
		require.NoError(t, err) // still there

		_, err = bundle.exec.QueueGet(ctx, queue3.Name)
		require.ErrorIs(t, err, rivertype.ErrNotFound) // still there
		_, err = bundle.exec.QueueGet(ctx, queue4.Name)
		require.ErrorIs(t, err, rivertype.ErrNotFound) // still there
		_, err = bundle.exec.QueueGet(ctx, queue5.Name)
		require.ErrorIs(t, err, rivertype.ErrNotFound) // still there
	})

	t.Run("DeletesInBatches", func(t *testing.T) {
		t.Parallel()

		cleaner, bundle := setup(t)
		cleaner.batchSize = 10 // reduced size for test speed

		// Add one to our chosen batch size to get one extra job and therefore
		// one extra batch, ensuring that we've tested working multiple.
		numQueues := cleaner.batchSize + 1

		queues := make([]*rivertype.Queue, numQueues)

		for i := range numQueues {
			queue := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{
				Name:      ptrutil.Ptr(fmt.Sprintf("queue%d", i)),
				UpdatedAt: ptrutil.Ptr(bundle.deleteHorizon.Add(-25 * time.Hour)),
			})
			queues[i] = queue
		}

		require.NoError(t, cleaner.Start(ctx))

		// See comment above. Exactly two batches are expected.
		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()
		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		for _, queue := range queues {
			_, err := bundle.exec.QueueGet(ctx, queue.Name)
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		}
	})

	t.Run("CustomizableInterval", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = 1 * time.Microsecond

		require.NoError(t, cleaner.Start(ctx))

		// This should trigger ~immediately every time:
		for i := range 5 {
			t.Logf("Iteration %d", i)
			cleaner.TestSignals.DeletedBatch.WaitOrTimeout()
		}
	})

	t.Run("StopsImmediately", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		require.NoError(t, cleaner.Start(ctx))
		cleaner.Stop()
	})

	t.Run("RespectsContextCancellation", func(t *testing.T) {
		t.Parallel()

		cleaner, _ := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		ctx, cancelFunc := context.WithCancel(ctx)

		require.NoError(t, cleaner.Start(ctx))

		// To avoid a potential race, make sure to get a reference to the
		// service's stopped channel _before_ cancellation as it's technically
		// possible for the cancel to "win" and remove the stopped channel
		// before we can start waiting on it.
		stopped := cleaner.Stopped()
		cancelFunc()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("CanRunMultipleTimes", func(t *testing.T) {
		t.Parallel()

		cleaner, bundle := setup(t)
		cleaner.Config.Interval = time.Minute // should only trigger once for the initial run

		queue1 := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{Name: ptrutil.Ptr("queue1"), UpdatedAt: ptrutil.Ptr(bundle.deleteHorizon.Add(-1 * time.Hour))})

		require.NoError(t, cleaner.Start(ctx))

		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		cleaner.Stop()

		queue2 := testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{Name: ptrutil.Ptr("queue2"), UpdatedAt: ptrutil.Ptr(bundle.deleteHorizon.Add(-1 * time.Minute))})

		require.NoError(t, cleaner.Start(ctx))

		cleaner.TestSignals.DeletedBatch.WaitOrTimeout()

		var err error
		_, err = bundle.exec.QueueGet(ctx, queue1.Name)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = bundle.exec.QueueGet(ctx, queue2.Name)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
	})
}

```

`internal/maintenance/queue_maintainer.go`:

```go
package maintenance

import (
	"context"
	"reflect"
	"time"

	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/util/maputil"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
)

const (
	// Maintainers will sleep a brief period of time between batches to give the
	// database some breathing room.
	BatchBackoffMax = 1 * time.Second
	BatchBackoffMin = 50 * time.Millisecond

	// Bulk maintenance tasks like job removal operate in batches so that even
	// in the event of an enormous backlog of work to do, transactions stay
	// relatively short and aren't at risk of cancellation. This number is the
	// batch size, or the number of rows that are handled at a time.
	//
	// The specific value is somewhat arbitrary as large enough to make good
	// progress, but not so large as to make the operation overstay its welcome.
	// For now it's not configurable because we can likely pick a number that's
	// suitable for almost everyone.
	BatchSizeDefault = 1_000

	logPrefixRanSuccessfully = ": Ran successfully"
	logPrefixRunLoopStarted  = ": Run loop started"
	logPrefixRunLoopStopped  = ": Run loop stopped"
)

// QueueMaintainer runs regular maintenance operations against job queues, like
// pruning completed jobs. It runs only on the client which has been elected
// leader at any given time.
//
// Its methods are not safe for concurrent usage.
type QueueMaintainer struct {
	baseservice.BaseService
	startstop.BaseStartStop

	servicesByName map[string]startstop.Service
}

func NewQueueMaintainer(archetype *baseservice.Archetype, services []startstop.Service) *QueueMaintainer {
	servicesByName := make(map[string]startstop.Service, len(services))
	for _, service := range services {
		servicesByName[serviceName(service)] = service
	}
	return baseservice.Init(archetype, &QueueMaintainer{
		servicesByName: servicesByName,
	})
}

// StaggerStartupDisable sets whether the short staggered sleep on start up
// is disabled. This is useful in tests where the extra sleep involved in a
// staggered start up is not helpful for test run time.
func (m *QueueMaintainer) StaggerStartupDisable(disabled bool) {
	for _, svc := range m.servicesByName {
		if svcWithDisable, ok := svc.(withStaggerStartupDisable); ok {
			svcWithDisable.StaggerStartupDisable(disabled)
		}
	}
}

func (m *QueueMaintainer) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := m.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	for _, service := range m.servicesByName {
		if err := service.Start(ctx); err != nil {
			return err
		}
	}

	go func() {
		// Wait for all subservices to start up before signaling our own start.
		startstop.WaitAllStarted(maputil.Values(m.servicesByName)...)

		started()
		defer stopped() // this defer should come first so it's last out

		<-ctx.Done()

		startstop.StopAllParallel(maputil.Values(m.servicesByName)...)
	}()

	return nil
}

// GetService is a convenience method for getting a service by name and casting
// it to the desired type. It should only be used in tests due to its use of
// reflection and potential for panics.
func GetService[T startstop.Service](maintainer *QueueMaintainer) T {
	var kindPtr T
	return maintainer.servicesByName[serviceName(kindPtr)].(T) //nolint:forcetypeassert
}

func serviceName(service startstop.Service) string {
	elem := reflect.TypeOf(service).Elem()
	return elem.PkgPath() + "." + elem.Name()
}

// queueMaintainerServiceBase is a struct that should be embedded on all queue
// maintainer services. Its main use is to provide a StaggerStart function that
// should be called on service start to avoid thundering herd problems.
type queueMaintainerServiceBase struct {
	baseservice.BaseService
	staggerStartupDisabled bool
}

// StaggerStart is called when queue maintainer services start. It jitters by
// sleeping for a short random period so services don't all perform their first
// run at exactly the same time.
func (s *queueMaintainerServiceBase) StaggerStart(ctx context.Context) {
	if s.staggerStartupDisabled {
		return
	}

	serviceutil.CancellableSleep(ctx, randutil.DurationBetween(0*time.Second, 1*time.Second))
}

// StaggerStartupDisable sets whether the short staggered sleep on start up
// is disabled. This is useful in tests where the extra sleep involved in a
// staggered start up is not helpful for test run time.
func (s *queueMaintainerServiceBase) StaggerStartupDisable(disabled bool) {
	s.staggerStartupDisabled = disabled
}

func (s *queueMaintainerServiceBase) StaggerStartupIsDisabled() bool {
	return s.staggerStartupDisabled
}

// withStaggerStartupDisable is an interface to a service whose stagger startup
// sleep can be disabled.
type withStaggerStartupDisable interface {
	// StaggerStartupDisable sets whether the short staggered sleep on start up
	// is disabled. This is useful in tests where the extra sleep involved in a
	// staggered start up is not helpful for test run time.
	StaggerStartupDisable(disabled bool)
}

```

`internal/maintenance/queue_maintainer_test.go`:

```go
package maintenance

import (
	"context"
	"testing"
	"time"

	"github.com/robfig/cron/v3"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/riverinternaltest/sharedtx"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivertype"
)

type testService struct {
	queueMaintainerServiceBase
	startstop.BaseStartStop

	testSignals testServiceTestSignals
}

func newTestService(tb testing.TB) *testService {
	tb.Helper()

	testSvc := baseservice.Init(riversharedtest.BaseServiceArchetype(tb), &testService{})
	testSvc.testSignals.Init()

	return testSvc
}

func (s *testService) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	go func() {
		started()
		defer stopped()

		s.testSignals.started.Signal(struct{}{})
		<-ctx.Done()
		s.testSignals.returning.Signal(struct{}{})
	}()

	return nil
}

type testServiceTestSignals struct {
	returning testsignal.TestSignal[struct{}]
	started   testsignal.TestSignal[struct{}]
}

func (ts *testServiceTestSignals) Init() {
	ts.returning.Init()
	ts.started.Init()
}

func TestQueueMaintainer(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	setup := func(t *testing.T, services []startstop.Service) *QueueMaintainer {
		t.Helper()

		maintainer := NewQueueMaintainer(riversharedtest.BaseServiceArchetype(t), services)
		maintainer.StaggerStartupDisable(true)

		return maintainer
	}

	t.Run("StartStop", func(t *testing.T) {
		t.Parallel()

		testSvc := newTestService(t)
		maintainer := setup(t, []startstop.Service{testSvc})

		require.NoError(t, maintainer.Start(ctx))
		testSvc.testSignals.started.WaitOrTimeout()
		maintainer.Stop()
		testSvc.testSignals.returning.WaitOrTimeout()
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		tx := riverinternaltest.TestTx(ctx, t)
		sharedTx := sharedtx.NewSharedTx(tx)

		archetype := riversharedtest.BaseServiceArchetype(t)
		archetype.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress

		driver := riverpgxv5.New(nil).UnwrapExecutor(sharedTx)

		// Use realistic services in this one so we can verify stress not only
		// on the queue maintainer, but it and all its subservices together.
		maintainer := setup(t, []startstop.Service{
			NewJobCleaner(archetype, &JobCleanerConfig{}, driver),
			NewPeriodicJobEnqueuer(archetype, &PeriodicJobEnqueuerConfig{
				PeriodicJobs: []*PeriodicJob{
					{
						ConstructorFunc: func() (*rivertype.JobInsertParams, error) {
							return nil, ErrNoJobToInsert
						},
						ScheduleFunc: cron.Every(15 * time.Minute).Next,
					},
				},
			}, driver),
			NewQueueCleaner(archetype, &QueueCleanerConfig{}, driver),
			NewJobScheduler(archetype, &JobSchedulerConfig{}, driver),
		})
		maintainer.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress
		startstoptest.Stress(ctx, t, maintainer)
	})

	t.Run("StopWithoutHavingBeenStarted", func(t *testing.T) {
		t.Parallel()

		testSvc := newTestService(t)
		maintainer := setup(t, []startstop.Service{testSvc})

		// Tolerate being stopped without having been started, without blocking:
		maintainer.Stop()

		require.NoError(t, maintainer.Start(ctx))
		testSvc.testSignals.started.WaitOrTimeout()
		maintainer.Stop()
	})

	t.Run("MultipleStartStop", func(t *testing.T) {
		t.Parallel()

		testSvc := newTestService(t)
		maintainer := setup(t, []startstop.Service{testSvc})

		runOnce := func() {
			require.NoError(t, maintainer.Start(ctx))
			testSvc.testSignals.started.WaitOrTimeout()
			maintainer.Stop()
			testSvc.testSignals.returning.WaitOrTimeout()
		}

		for range 3 {
			runOnce()
		}
	})

	t.Run("RespectsContextCancellation", func(t *testing.T) {
		t.Parallel()

		testSvc := newTestService(t)
		maintainer := setup(t, []startstop.Service{testSvc})

		ctx, cancelFunc := context.WithCancel(ctx)

		require.NoError(t, maintainer.Start(ctx))

		// To avoid a potential race, make sure to get a reference to the
		// service's stopped channel _before_ cancellation as it's technically
		// possible for the cancel to "win" and remove the stopped channel
		// before we can start waiting on it.
		stopped := maintainer.Stopped()
		cancelFunc()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("GetService", func(t *testing.T) {
		t.Parallel()

		testSvc := newTestService(t)

		maintainer := setup(t, []startstop.Service{testSvc})

		require.NoError(t, maintainer.Start(ctx))
		testSvc.testSignals.started.WaitOrTimeout()

		svc := GetService[*testService](maintainer)
		if svc == nil {
			t.Fatal("GetService returned nil")
		}

		maintainer.Stop()
		testSvc.testSignals.returning.WaitOrTimeout()
	})
}

```

`internal/maintenance/reindexer.go`:

```go
package maintenance

import (
	"context"
	"errors"
	"log/slog"
	"time"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/valutil"
)

const (
	ReindexerIntervalDefault = 24 * time.Hour
	ReindexerTimeoutDefault  = 15 * time.Second
)

var defaultIndexNames = []string{"river_job_args_index", "river_job_metadata_index"} //nolint:gochecknoglobals

// Test-only properties.
type ReindexerTestSignals struct {
	Reindexed testsignal.TestSignal[struct{}] // notifies when a run finishes executing reindexes for all indexes
}

func (ts *ReindexerTestSignals) Init() {
	ts.Reindexed.Init()
}

type ReindexerConfig struct {
	// IndexNames is a list of indexes to reindex on each run.
	IndexNames []string

	// ScheduleFunc returns the next scheduled run time for the reindexer given the
	// current time.
	ScheduleFunc func(time.Time) time.Time

	// Timeout is the amount of time to wait for a single reindex query to return.
	Timeout time.Duration
}

func (c *ReindexerConfig) mustValidate() *ReindexerConfig {
	if c.ScheduleFunc == nil {
		panic("ReindexerConfig.ScheduleFunc must be set")
	}
	if c.Timeout <= 0 {
		panic("ReindexerConfig.Timeout must be above zero")
	}

	return c
}

// Reindexer periodically executes a REINDEX command on the important job
// indexes to rebuild them and fix bloat issues.
type Reindexer struct {
	queueMaintainerServiceBase
	startstop.BaseStartStop

	// exported for test purposes
	Config      *ReindexerConfig
	TestSignals ReindexerTestSignals

	batchSize int64 // configurable for test purposes
	exec      riverdriver.Executor
}

func NewReindexer(archetype *baseservice.Archetype, config *ReindexerConfig, exec riverdriver.Executor) *Reindexer {
	indexNames := defaultIndexNames
	if config.IndexNames != nil {
		indexNames = config.IndexNames
	}

	scheduleFunc := config.ScheduleFunc
	if scheduleFunc == nil {
		scheduleFunc = (&DefaultReindexerSchedule{}).Next
	}

	return baseservice.Init(archetype, &Reindexer{
		Config: (&ReindexerConfig{
			IndexNames:   indexNames,
			ScheduleFunc: scheduleFunc,
			Timeout:      valutil.ValOrDefault(config.Timeout, ReindexerTimeoutDefault),
		}).mustValidate(),

		batchSize: BatchSizeDefault,
		exec:      exec,
	})
}

func (s *Reindexer) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	s.StaggerStart(ctx)

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStarted)
		defer s.Logger.DebugContext(ctx, s.Name+logPrefixRunLoopStopped)

		nextRunAt := s.Config.ScheduleFunc(time.Now().UTC())

		s.Logger.DebugContext(ctx, s.Name+": Scheduling first run", slog.Time("next_run_at", nextRunAt))

		timerUntilNextRun := time.NewTimer(time.Until(nextRunAt))

		for {
			select {
			case <-timerUntilNextRun.C:
				for _, indexName := range s.Config.IndexNames {
					if err := s.reindexOne(ctx, indexName); err != nil {
						if !errors.Is(err, context.Canceled) {
							s.Logger.ErrorContext(ctx, s.Name+": Error reindexing", slog.String("error", err.Error()), slog.String("index_name", indexName))
						}
						continue
					}
				}

				s.TestSignals.Reindexed.Signal(struct{}{})

				// On each run, we calculate the new schedule based on the
				// previous run's start time. This ensures that we don't
				// accidentally skip a run as time elapses during the run.
				nextRunAt = s.Config.ScheduleFunc(nextRunAt)

				// TODO: maybe we should log differently if some of these fail?
				s.Logger.DebugContext(ctx, s.Name+logPrefixRanSuccessfully,
					slog.Time("next_run_at", nextRunAt), slog.Int("num_reindexes_initiated", len(s.Config.IndexNames)))

				// Reset the timer after the insert loop has finished so it's
				// paused during work. Makes its firing more deterministic.
				timerUntilNextRun.Reset(time.Until(nextRunAt))

			case <-ctx.Done():
				// Clean up timer resources. We know it has _not_ received from
				// the timer since its last reset because that would have led us
				// to the case above instead of here.
				if !timerUntilNextRun.Stop() {
					<-timerUntilNextRun.C
				}
				return
			}
		}
	}()

	return nil
}

func (s *Reindexer) reindexOne(ctx context.Context, indexName string) error {
	ctx, cancel := context.WithTimeout(ctx, s.Config.Timeout)
	defer cancel()

	_, err := s.exec.Exec(ctx, "REINDEX INDEX CONCURRENTLY "+indexName)
	if err != nil {
		return err
	}

	s.Logger.InfoContext(ctx, s.Name+": Initiated reindex", slog.String("index_name", indexName))
	return nil
}

// DefaultReindexerSchedule is a default schedule for the reindexer job which
// runs at midnight UTC daily.
type DefaultReindexerSchedule struct{}

// Next returns the next scheduled time for the reindexer job.
func (s *DefaultReindexerSchedule) Next(t time.Time) time.Time {
	return t.Add(24 * time.Hour).Truncate(24 * time.Hour)
}

```

`internal/maintenance/reindexer_test.go`:

```go
package maintenance

import (
	"context"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
)

func TestReindexer(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		exec riverdriver.Executor
		now  time.Time
	}

	setup := func(t *testing.T) (*Reindexer, *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		bundle := &testBundle{
			exec: riverpgxv5.New(dbPool).GetExecutor(),
		}

		archetype := riversharedtest.BaseServiceArchetype(t)
		bundle.now = archetype.Time.StubNowUTC(time.Now())

		fromNow := func(d time.Duration) func(time.Time) time.Time {
			return func(t time.Time) time.Time {
				return t.Add(d)
			}
		}

		svc := NewReindexer(archetype, &ReindexerConfig{
			ScheduleFunc: fromNow(500 * time.Millisecond),
		}, bundle.exec)
		svc.StaggerStartupDisable(true)
		svc.TestSignals.Init()
		t.Cleanup(svc.Stop)

		return svc, bundle
	}

	runImmediatelyThenOnceAnHour := func() func(time.Time) time.Time {
		alreadyRan := false
		return func(t time.Time) time.Time {
			if alreadyRan {
				return t.Add(time.Hour)
			}
			alreadyRan = true
			return t.Add(time.Millisecond)
		}
	}

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)
		svc.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress
		svc.TestSignals = ReindexerTestSignals{}   // deinit so channels don't fill

		startstoptest.Stress(ctx, t, svc)
	})

	t.Run("ReindexesEachIndex", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		svc.Config.IndexNames = []string{
			"river_job_kind",
			"river_job_prioritized_fetching_index",
			"river_job_state_and_finalized_at_index",
		}
		svc.Config.ScheduleFunc = runImmediatelyThenOnceAnHour()

		require.NoError(t, svc.Start(ctx))
		svc.TestSignals.Reindexed.WaitOrTimeout()

		select {
		case <-svc.TestSignals.Reindexed.WaitC():
			require.FailNow(t, "Didn't expect reindexing to occur again")
		case <-time.After(100 * time.Millisecond):
		}
	})

	t.Run("StopsImmediately", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		require.NoError(t, svc.Start(ctx))
		svc.Stop()
	})

	t.Run("RespectsContextCancellation", func(t *testing.T) {
		t.Parallel()

		svc, _ := setup(t)

		ctx, cancelFunc := context.WithCancel(ctx)
		require.NoError(t, svc.Start(ctx))

		// To avoid a potential race, make sure to get a reference to the
		// service's stopped channel _before_ cancellation as it's technically
		// possible for the cancel to "win" and remove the stopped channel
		// before we can start waiting on it.
		stopped := svc.Stopped()
		cancelFunc()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("DefaultConfigs", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)
		svc = NewReindexer(&svc.Archetype, &ReindexerConfig{}, bundle.exec)

		require.Equal(t, defaultIndexNames, svc.Config.IndexNames)
		require.Equal(t, ReindexerTimeoutDefault, svc.Config.Timeout)
		require.Equal(t, svc.Config.ScheduleFunc(bundle.now), (&DefaultReindexerSchedule{}).Next(bundle.now))
	})
}

func TestDefaultReindexerSchedule(t *testing.T) {
	t.Parallel()

	t.Run("WithMidnightInputReturnsMidnight24HoursLater", func(t *testing.T) {
		t.Parallel()

		schedule := &DefaultReindexerSchedule{}
		result := schedule.Next(time.Date(2023, 8, 31, 0, 0, 0, 0, time.UTC))
		require.Equal(t, time.Date(2023, 9, 1, 0, 0, 0, 0, time.UTC), result)
	})

	t.Run("WithMidnightInputReturnsMidnight24HoursLater", func(t *testing.T) {
		t.Parallel()

		schedule := &DefaultReindexerSchedule{}
		result := schedule.Next(time.Date(2023, 8, 31, 0, 0, 0, 0, time.UTC))
		require.Equal(t, time.Date(2023, 9, 1, 0, 0, 0, 0, time.UTC), result)
	})

	t.Run("With1NanosecondBeforeMidnightItReturnsUpcomingMidnight", func(t *testing.T) {
		t.Parallel()

		schedule := &DefaultReindexerSchedule{}
		result := schedule.Next(time.Date(2023, 8, 31, 23, 59, 59, 999999999, time.UTC))
		require.Equal(t, time.Date(2023, 9, 1, 0, 0, 0, 0, time.UTC), result)
	})
}

```

`internal/middlewarelookup/middleware_lookup.go`:

```go
package middlewarelookup

import (
	"sync"

	"github.com/riverqueue/river/rivertype"
)

//
// MiddlewareKind
//

type MiddlewareKind string

const (
	MiddlewareKindJobInsert MiddlewareKind = "job_insert"
	MiddlewareKindWorker    MiddlewareKind = "worker"
)

//
// MiddlewareLookupInterface
//

// MiddlewareLookupInterface is an interface to look up middlewares by
// middleware kind. It's commonly implemented by MiddlewareLookup, but may also
// be EmptyMiddlewareLookup as a memory allocation optimization for bundles
// where no middlewares are present.
type MiddlewareLookupInterface interface {
	ByMiddlewareKind(kind MiddlewareKind) []rivertype.Middleware
}

// NewMiddlewareLookup returns a new middleware lookup interface based on the given middlewares
// that satisfies MiddlewareLookupInterface. This is often middlewareLookup, but may be
// emptyMiddlewareLookup as an optimization for the common case of an empty middleware
// bundle.
func NewMiddlewareLookup(middlewares []rivertype.Middleware) MiddlewareLookupInterface {
	if len(middlewares) < 1 {
		return &emptyMiddlewareLookup{}
	}

	return &middlewareLookup{
		middlewares:       middlewares,
		middlewaresByKind: make(map[MiddlewareKind][]rivertype.Middleware),
		mu:                &sync.RWMutex{},
	}
}

//
// middlewareLookup
//

// middlewareLookup looks up and caches middlewares based on a MiddlewareKind, saving work when
// looking up middlewares for specific operations, a common operation that gets
// repeated over and over again. This struct may be used as a lookup for
// globally installed middlewares or middlewares for specific job kinds through the use of
// JobMiddlewareLookup.
type middlewareLookup struct {
	middlewares       []rivertype.Middleware
	middlewaresByKind map[MiddlewareKind][]rivertype.Middleware
	mu                *sync.RWMutex
}

func (c *middlewareLookup) ByMiddlewareKind(kind MiddlewareKind) []rivertype.Middleware {
	c.mu.RLock()
	cache, ok := c.middlewaresByKind[kind]
	c.mu.RUnlock()
	if ok {
		return cache
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	// Even if this ends up being empty, make sure there's an entry for the next
	// time the cache gets invoked for this kind.
	c.middlewaresByKind[kind] = nil

	// Rely on exhaustlint to find any missing middleware kinds here.
	switch kind {
	case MiddlewareKindJobInsert:
		for _, middleware := range c.middlewares {
			if typedMiddleware, ok := middleware.(rivertype.JobInsertMiddleware); ok {
				c.middlewaresByKind[kind] = append(c.middlewaresByKind[kind], typedMiddleware)
			}
		}
	case MiddlewareKindWorker:
		for _, middleware := range c.middlewares {
			if typedMiddleware, ok := middleware.(rivertype.WorkerMiddleware); ok {
				c.middlewaresByKind[kind] = append(c.middlewaresByKind[kind], typedMiddleware)
			}
		}
	}

	return c.middlewaresByKind[kind]
}

//
// emptyMiddlewareLookup
//

// emptyMiddlewareLookup is an empty version of MiddlewareLookup that's zero allocation. For
// most applications, most job args won't have middlewares, so this prevents us from
// allocating dozens/hundreds of small MiddlewareLookup objects that go unused.
type emptyMiddlewareLookup struct{}

func (c *emptyMiddlewareLookup) ByMiddlewareKind(kind MiddlewareKind) []rivertype.Middleware {
	return nil
}

```

`internal/middlewarelookup/middleware_lookup_test.go`:

```go
package middlewarelookup

import (
	"context"
	"sync"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivertype"
)

func TestMiddlewareLookup(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (*middlewareLookup, *testBundle) { //nolint:unparam
		t.Helper()

		return NewMiddlewareLookup([]rivertype.Middleware{ //nolint:forcetypeassert
			&testMiddlewareJobInsertAndWorker{},
			&testMiddlewareJobInsert{},
			&testMiddlewareWorker{},
		}).(*middlewareLookup), &testBundle{}
	}

	t.Run("LooksUpMiddleware", func(t *testing.T) {
		t.Parallel()

		middlewareLookup, _ := setup(t)

		require.Equal(t, []rivertype.Middleware{
			&testMiddlewareJobInsertAndWorker{},
			&testMiddlewareJobInsert{},
		}, middlewareLookup.ByMiddlewareKind(MiddlewareKindJobInsert))
		require.Equal(t, []rivertype.Middleware{
			&testMiddlewareJobInsertAndWorker{},
			&testMiddlewareWorker{},
		}, middlewareLookup.ByMiddlewareKind(MiddlewareKindWorker))

		require.Len(t, middlewareLookup.middlewaresByKind, 2)

		// Repeat lookups to make sure we get the same result.
		require.Equal(t, []rivertype.Middleware{
			&testMiddlewareJobInsertAndWorker{},
			&testMiddlewareJobInsert{},
		}, middlewareLookup.ByMiddlewareKind(MiddlewareKindJobInsert))
		require.Equal(t, []rivertype.Middleware{
			&testMiddlewareJobInsertAndWorker{},
			&testMiddlewareWorker{},
		}, middlewareLookup.ByMiddlewareKind(MiddlewareKindWorker))
	})

	t.Run("Stress", func(t *testing.T) {
		t.Parallel()

		middlewareLookup, _ := setup(t)

		var wg sync.WaitGroup

		parallelLookupLoop := func(kind MiddlewareKind) {
			wg.Add(1)
			go func() {
				defer wg.Done()

				for range 50 {
					middlewareLookup.ByMiddlewareKind(kind)
				}
			}()
		}

		parallelLookupLoop(MiddlewareKindJobInsert)
		parallelLookupLoop(MiddlewareKindWorker)
		parallelLookupLoop(MiddlewareKindJobInsert)
		parallelLookupLoop(MiddlewareKindWorker)

		wg.Wait()
	})
}

func TestEmptyMiddlewareLookup(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (*emptyMiddlewareLookup, *testBundle) {
		t.Helper()

		return NewMiddlewareLookup(nil).(*emptyMiddlewareLookup), &testBundle{} //nolint:forcetypeassert
	}

	t.Run("AlwaysReturnsNil", func(t *testing.T) {
		t.Parallel()

		middlewareLookup, _ := setup(t)

		require.Nil(t, middlewareLookup.ByMiddlewareKind(MiddlewareKindJobInsert))
		require.Nil(t, middlewareLookup.ByMiddlewareKind(MiddlewareKindWorker))
	})
}

//
// testMiddlewareInsertAndWorkBegin
//

var (
	_ rivertype.JobInsertMiddleware = &testMiddlewareJobInsertAndWorker{}
	_ rivertype.WorkerMiddleware    = &testMiddlewareJobInsertAndWorker{}
)

type testMiddlewareJobInsertAndWorker struct{ rivertype.Middleware }

func (t *testMiddlewareJobInsertAndWorker) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
	return doInner(ctx)
}

func (t *testMiddlewareJobInsertAndWorker) Work(ctx context.Context, job *rivertype.JobRow, doInner func(context.Context) error) error {
	return doInner(ctx)
}

//
// testMiddlewareJobInsert
//

var _ rivertype.JobInsertMiddleware = &testMiddlewareJobInsert{}

type testMiddlewareJobInsert struct{ rivertype.Middleware }

func (t *testMiddlewareJobInsert) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
	return doInner(ctx)
}

//
// testMiddlewareWorker
//

var _ rivertype.WorkerMiddleware = &testMiddlewareWorker{}

type testMiddlewareWorker struct{ rivertype.Middleware }

func (t *testMiddlewareWorker) Work(ctx context.Context, job *rivertype.JobRow, doInner func(context.Context) error) error {
	return doInner(ctx)
}

```

`internal/notifier/main_test.go`:

```go
package notifier

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`internal/notifier/notifier.go`:

```go
package notifier

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"slices"
	"sync"
	"time"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/maputil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
)

type NotificationTopic string

const (
	NotificationTopicControl    NotificationTopic = "river_control"
	NotificationTopicInsert     NotificationTopic = "river_insert"
	NotificationTopicLeadership NotificationTopic = "river_leadership"
)

type NotifyFunc func(topic NotificationTopic, payload string)

type Subscription struct {
	notifyFunc   NotifyFunc
	notifier     *Notifier
	topic        NotificationTopic
	unlistenOnce sync.Once
}

func (s *Subscription) Unlisten(ctx context.Context) {
	s.unlistenOnce.Do(func() {
		// Unlisten strips cancellation from the parent context to ensure it runs:
		if err := s.notifier.unlisten(context.WithoutCancel(ctx), s); err != nil {
			s.notifier.Logger.ErrorContext(ctx, s.notifier.Name+": Error unlistening on topic", "err", err, "topic", s.topic)
		}
	})
}

// Test-only properties.
type notifierTestSignals struct {
	BackoffError   testsignal.TestSignal[error]    // non-cancellation error received by main run loop
	ListeningBegin testsignal.TestSignal[struct{}] // notifier has entered a listen loop
	ListeningEnd   testsignal.TestSignal[struct{}] // notifier has left a listen loop
}

func (ts *notifierTestSignals) Init() {
	ts.BackoffError.Init()
	ts.ListeningBegin.Init()
	ts.ListeningEnd.Init()
}

type Notifier struct {
	baseservice.BaseService
	startstop.BaseStartStop

	disableSleep      bool // for tests only; disable sleep on exponential backoff
	listener          riverdriver.Listener
	notificationBuf   chan *riverdriver.Notification
	testSignals       notifierTestSignals
	waitInterruptChan chan func()

	mu            sync.RWMutex
	isConnected   bool
	isStarted     bool
	isWaiting     bool
	subscriptions map[NotificationTopic][]*Subscription
	waitCancel    context.CancelFunc
}

func New(archetype *baseservice.Archetype, listener riverdriver.Listener) *Notifier {
	notifier := baseservice.Init(archetype, &Notifier{
		listener:          listener,
		notificationBuf:   make(chan *riverdriver.Notification, 1000),
		waitInterruptChan: make(chan func(), 10),

		subscriptions: make(map[NotificationTopic][]*Subscription),
	})
	return notifier
}

func (n *Notifier) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := n.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	// The loop below will connect/close on every iteration, but do one initial
	// connect so the notifier fails fast in case of an obvious problem.
	if err := n.listenerConnect(ctx, false); err != nil {
		stopped()
		if errors.Is(err, context.Canceled) {
			return nil
		}
		return err
	}

	go func() {
		started()
		defer stopped()

		n.Logger.DebugContext(ctx, n.Name+": Run loop started")
		defer n.Logger.DebugContext(ctx, n.Name+": Run loop stopped")

		n.withLock(func() { n.isStarted = true })
		defer n.withLock(func() { n.isStarted = false })

		defer n.listenerClose(ctx, false)

		var wg sync.WaitGroup

		wg.Add(1)
		go func() {
			defer wg.Done()
			n.deliverNotifications(ctx)
		}()

		for attempt := 0; ; attempt++ {
			if err := n.listenAndWait(ctx); err != nil {
				if errors.Is(err, context.Canceled) {
					break
				}

				sleepDuration := serviceutil.ExponentialBackoff(attempt, serviceutil.MaxAttemptsBeforeResetDefault)
				n.Logger.ErrorContext(ctx, n.Name+": Error running listener (will attempt reconnect after backoff)",
					slog.Int("attempt", attempt),
					slog.String("err", err.Error()),
					slog.String("sleep_duration", sleepDuration.String()),
				)
				n.testSignals.BackoffError.Signal(err)
				if !n.disableSleep {
					serviceutil.CancellableSleep(ctx, sleepDuration)
				}
			}
		}

		wg.Wait()
	}()

	return nil
}

func (n *Notifier) deliverNotifications(ctx context.Context) {
	for {
		select {
		case <-ctx.Done():
			return

		case notification := <-n.notificationBuf:
			notifyFuncs := func() []NotifyFunc {
				n.mu.RLock()
				defer n.mu.RUnlock()

				return sliceutil.Map(n.subscriptions[NotificationTopic(notification.Topic)], func(s *Subscription) NotifyFunc { return s.notifyFunc })
			}()

			for _, notifyFunc := range notifyFuncs {
				// TODO: panic recovery on delivery attempts
				notifyFunc(NotificationTopic(notification.Topic), notification.Payload)
			}
		}
	}
}

func (n *Notifier) listenAndWait(ctx context.Context) error {
	if err := n.listenerConnect(ctx, false); err != nil {
		return err
	}
	defer n.listenerClose(ctx, false)

	topics := func() []NotificationTopic {
		n.mu.RLock()
		defer n.mu.RUnlock()

		return maputil.Keys(n.subscriptions)
	}()

	for _, topic := range topics {
		if err := n.listenerListen(ctx, topic); err != nil {
			return err
		}
	}

	n.Logger.DebugContext(ctx, n.Name+": Notifier healthy")

	n.testSignals.ListeningBegin.Signal(struct{}{})
	defer n.testSignals.ListeningEnd.Signal(struct{}{})

	drainInterrupts := func() {
		for {
			select {
			case interruptOperation := <-n.waitInterruptChan:
				interruptOperation()
			default:
				return
			}
		}
	}

	// Drain interrupts one last time before leaving to make sure we're not
	// leaving any goroutines hanging anywhere.
	defer drainInterrupts()

	for {
		// Top level context is done, meaning we're shutting down.
		if ctx.Err() != nil {
			return ctx.Err()
		}

		// Drain any and all interrupt operations before continuing back into a
		// new wait to give any new subscribers a chance to listen/unlisten.
		drainInterrupts()

		err := n.waitOnce(ctx)
		if err != nil {
			// On cancellation, reenter loop, but the check at the top on
			// `ctx.Err()` will end it if the service is shutting down.
			if errors.Is(err, context.Canceled) {
				continue
			}

			n.Logger.InfoContext(ctx, n.Name+": Notifier unhealthy")

			return err
		}
	}
}

func (n *Notifier) listenerClose(ctx context.Context, skipLock bool) {
	if !skipLock {
		n.mu.Lock()
		defer n.mu.Unlock()
	}

	if !n.isConnected {
		return
	}

	n.Logger.DebugContext(ctx, n.Name+": Listener closing")
	if err := n.listener.Close(ctx); err != nil {
		if !errors.Is(err, context.Canceled) {
			n.Logger.ErrorContext(ctx, n.Name+": Error closing listener", "err", err)
		}
	}

	n.isConnected = false
}

const listenerTimeout = 10 * time.Second

func (n *Notifier) listenerConnect(ctx context.Context, skipLock bool) error {
	if !skipLock {
		n.mu.Lock()
		defer n.mu.Unlock()
	}

	if n.isConnected {
		return nil
	}

	ctx, cancel := context.WithTimeout(ctx, listenerTimeout)
	defer cancel()

	n.Logger.DebugContext(ctx, n.Name+": Listener connecting")
	if err := n.listener.Connect(ctx); err != nil {
		if !errors.Is(err, context.Canceled) {
			n.Logger.ErrorContext(ctx, n.Name+": Error connecting listener", "err", err)
		}

		return err
	}

	n.isConnected = true
	return nil
}

// Listens on a topic with an appropriate logging statement. Should be preferred
// to `listener.Listen` for improved logging/telemetry.
//
// Not protected by mutex because it doesn't modify any notifier state and the
// underlying listener has a mutex around its operations.
func (n *Notifier) listenerListen(ctx context.Context, topic NotificationTopic) error {
	ctx, cancel := context.WithTimeout(ctx, listenerTimeout)
	defer cancel()

	n.Logger.DebugContext(ctx, n.Name+": Listening on topic", "topic", topic)
	if err := n.listener.Listen(ctx, string(topic)); err != nil {
		return fmt.Errorf("error listening on topic %q: %w", topic, err)
	}

	return nil
}

// Unlistens on a topic with an appropriate logging statement. Should be
// preferred to `listener.Unlisten` for improved logging/telemetry.
//
// Not protected by mutex because it doesn't modify any notifier state and the
// underlying listener has a mutex around its operations.
func (n *Notifier) listenerUnlisten(ctx context.Context, topic NotificationTopic) error {
	ctx, cancel := context.WithTimeout(ctx, listenerTimeout)
	defer cancel()

	n.Logger.DebugContext(ctx, n.Name+": Unlistening on topic", "topic", topic)
	if err := n.listener.Unlisten(ctx, string(topic)); err != nil {
		return fmt.Errorf("error unlistening on topic %q: %w", topic, err)
	}

	return nil
}

// Enters a single blocking wait for notifications on the underlying listener.
// Waiting for a notification locks an underlying connection, so infrastructure
// elsewhere in the notifier must preempt it by sending to `n.waitInterruptChan`
// and invoking `n.waitCancel()`. Cancelling the input context (as occurs during
// shutdown) also unblocks the wait.
func (n *Notifier) waitOnce(ctx context.Context) error {
	n.withLock(func() {
		n.isWaiting = true
		ctx, n.waitCancel = context.WithCancel(ctx) //nolint:fatcontext
	})
	defer n.withLock(func() {
		n.isWaiting = false
		n.waitCancel()
	})

	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	errChan := make(chan error)

	go func() {
		for {
			notification, err := n.listener.WaitForNotification(ctx)
			if err != nil {
				errChan <- err
				return
			}

			select {
			case n.notificationBuf <- notification:
			default:
				n.Logger.WarnContext(ctx, n.Name+": Dropping notification due to full buffer", "payload", notification.Payload)
			}
		}
	}()

	drainErrChan := func() error {
		cancel()

		// There's a chance we encounter some other error before the context.Canceled comes in:
		err := <-errChan
		if err != nil && !errors.Is(err, context.Canceled) {
			// A non-cancel error means something went wrong with the conn, so we should bail.
			n.Logger.ErrorContext(ctx, n.Name+": Error on draining notification wait", "err", err)
			return err
		}
		// If we got a context cancellation error, it means we successfully
		// interrupted the WaitForNotification so that we could make the
		// subscription change.
		return nil
	}

	needPingCtx, needPingCancel := context.WithTimeout(ctx, 5*time.Second)
	defer needPingCancel()

	// * Wait for notifications
	// * Ping conn if 5 seconds have elapsed between notifications to keep it alive
	// * Manage listens/unlistens on conn (waitInterruptChan)
	// * If any errors are encountered, return them so we can kill the conn and start over
	select {
	case <-ctx.Done():
		return <-errChan

	case <-needPingCtx.Done():
		if err := drainErrChan(); err != nil {
			return err
		}
		// Ping the conn to see if it's still alive
		if err := n.listener.Ping(ctx); err != nil {
			return err
		}

	case err := <-errChan:
		if errors.Is(err, context.Canceled) {
			return nil
		}
		if err != nil {
			n.Logger.ErrorContext(ctx, n.Name+": Error from notification wait", "err", err)
			return err
		}
	}

	return nil
}

// Sends an interrupt operation to the main loop, waits on the result, and
// returns an error if there was one.
//
// MUST be called with the `n.mu` mutex already locked.
func (n *Notifier) sendInterruptAndReceiveResult(operation func() error) error {
	errChan := make(chan error)
	n.waitInterruptChan <- func() {
		errChan <- operation()
	}

	n.waitCancel()

	// Notably, these unlock then lock again, the reverse of what you'd normally
	// expect in a mutex pattern. This is because this function is only expected
	// to be called with the mutex already locked, but we need to unlock it to
	// give the main loop a chance to run interrupt operations.
	n.mu.Unlock()
	defer n.mu.Lock()

	select {
	case err := <-errChan:
		return err
	case <-time.After(5 * time.Second):
		return errors.New("timed out waiting for interrupt operation")
	}
}

func (n *Notifier) Listen(ctx context.Context, topic NotificationTopic, notifyFunc NotifyFunc) (*Subscription, error) {
	n.mu.Lock()
	defer n.mu.Unlock()

	sub := &Subscription{
		notifyFunc: notifyFunc,
		topic:      topic,
		notifier:   n,
	}

	existingSubs, existingTopic := n.subscriptions[topic]
	if !existingTopic {
		existingSubs = make([]*Subscription, 0, 10)
	}
	n.subscriptions[topic] = append(existingSubs, sub)

	n.Logger.DebugContext(ctx, n.Name+": Added subscription", "new_num_subscriptions", len(n.subscriptions[topic]), "topic", topic)

	// We add the new subscription to the subscription list optimistically, and
	// it needs to be done this way in case of a restart after an interrupt
	// below has been run, but after a return to this function (say we were to
	// add the new sub at the end of this function, it would not be picked
	// during the restart). But in case of an error subscribing, remove the sub.
	//
	// By the time this function is run (i.e. after an interrupt), a lock on
	// `n.mu` has been reacquired, and modifying subscription state is safe.
	removeSub := func() { n.removeSubscription(ctx, sub) }

	if !existingTopic {
		// If already waiting, send an interrupt to the wait function to run a
		// listen operation. If not, connect and listen directly, returning any
		// errors as feedback to the caller.
		if n.isWaiting {
			if err := n.sendInterruptAndReceiveResult(func() error { return n.listenerListen(ctx, topic) }); err != nil {
				removeSub()
				return nil, err
			}
		} else {
			var justConnected bool

			if !n.isConnected {
				if err := n.listenerConnect(ctx, true); err != nil {
					removeSub()
					return nil, err
				}
				justConnected = true
			}

			if err := n.listenerListen(ctx, topic); err != nil {
				removeSub()

				// If we just connected above and the notifier hasn't started in
				// the interim, also close the connection so we don't leave any
				// resources hanging.
				if justConnected && !n.isStarted {
					n.listenerClose(ctx, true)
				}

				return nil, err
			}
		}
	}

	return sub, nil
}

func (n *Notifier) unlisten(ctx context.Context, sub *Subscription) error {
	n.mu.Lock()
	defer n.mu.Unlock()

	subs := n.subscriptions[sub.topic]

	// If this is the last subscription on the topic, unlisten if we're connected.
	if len(subs) <= 1 {
		// If already waiting, send an interrupt to the wait function to run an
		// unlisten operation. If not, if connected, unlisten directly.
		if n.isWaiting {
			if err := n.sendInterruptAndReceiveResult(func() error { return n.listenerUnlisten(ctx, sub.topic) }); err != nil {
				return err
			}
		} else {
			if n.isConnected {
				if err := n.listenerUnlisten(ctx, sub.topic); err != nil {
					return err
				}

				// If this was the last subscription, we weren't in a wait loop,
				// and the notifier never started, also clean up by closing the
				// listener.
				if !n.isStarted && len(n.subscriptions) <= 1 {
					n.listenerClose(ctx, true)
				}
			}
		}
	}

	n.removeSubscription(ctx, sub)

	return nil
}

// This function requires that the caller already has a lock on `n.mu`.
func (n *Notifier) removeSubscription(ctx context.Context, sub *Subscription) {
	n.subscriptions[sub.topic] = slices.DeleteFunc(n.subscriptions[sub.topic], func(s *Subscription) bool {
		return s == sub
	})

	if len(n.subscriptions[sub.topic]) < 1 {
		delete(n.subscriptions, sub.topic)
	}

	n.Logger.DebugContext(ctx, n.Name+": Removed subscription", "new_num_subscriptions", len(n.subscriptions[sub.topic]), "topic", sub.topic)
}

func (n *Notifier) withLock(lockedFunc func()) {
	n.mu.Lock()
	defer n.mu.Unlock()
	lockedFunc()
}

```

`internal/notifier/notifier_test.go`:

```go
package notifier

import (
	"context"
	"errors"
	"fmt"
	"strconv"
	"sync"
	"testing"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/util/maputil"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
)

func TestNotifier(t *testing.T) {
	t.Parallel()

	const (
		testTopic1 = "test_topic1"
		testTopic2 = "test_topic2"
	)

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
		exec   riverdriver.Executor
	}

	setup := func(t *testing.T) (*Notifier, *testBundle) {
		t.Helper()

		var (
			dbPool   = riverinternaltest.TestDB(ctx, t)
			driver   = riverpgxv5.New(dbPool)
			listener = driver.GetListener()
		)

		notifier := New(riversharedtest.BaseServiceArchetype(t), listener)
		notifier.testSignals.Init()

		return notifier, &testBundle{
			dbPool: dbPool,
			exec:   driver.GetExecutor(),
		}
	}

	start := func(t *testing.T, notifier *Notifier) {
		t.Helper()

		require.NoError(t, notifier.Start(ctx))
		t.Cleanup(notifier.Stop)
	}

	t.Run("StartsAndStops", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)
		start(t, notifier)

		notifier.testSignals.ListeningBegin.WaitOrTimeout()

		notifier.Stop()

		notifier.testSignals.ListeningEnd.WaitOrTimeout()
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)
		notifier.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress
		notifier.testSignals = notifierTestSignals{}    // deinit so channels don't fill

		startstoptest.Stress(ctx, t, notifier)
	})

	t.Run("StartErrorsOnImmediateProblem", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)

		t.Log("Closing database pool")
		bundle.dbPool.Close()

		require.EqualError(t, notifier.Start(ctx), "closed pool")
	})

	t.Run("ListenErrorsOnImmediateProblem", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)

		// Use a mock to simulate an error for this one because it's really hard
		// to get the timing right otherwise, and hard to avoid races.
		listenerMock := NewListenerMock(notifier.listener)
		listenerMock.listenFunc = func(ctx context.Context, topic string) error {
			return errors.New("error from listener")
		}
		notifier.listener = listenerMock

		start(t, notifier)

		notifier.testSignals.ListeningBegin.WaitOrTimeout()

		_, err := notifier.Listen(ctx, testTopic1, nil)
		require.EqualError(t, err, fmt.Sprintf("error listening on topic %q: error from listener", testTopic1))

		require.Empty(t, notifier.subscriptions)
	})

	// A reasonable amount of time to wait for a notification that we don't
	// expect to come through before timing out and assuming that it won't.
	const notificationWaitLeeway = 50 * time.Millisecond

	requireNoNotification := func(t *testing.T, notifyChan chan TopicAndPayload) {
		t.Helper()

		if len(notifyChan) > 0 {
			notification := <-notifyChan
			require.FailNow(t, "Expected no more notifications", "Expected no more notifications, but got: %+v", notification)
		}
	}

	t.Run("ListensAndUnlistens", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)
		start(t, notifier)

		notifyChan := make(chan TopicAndPayload, 10)

		sub, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan))
		require.NoError(t, err)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg1")

		require.Equal(t, TopicAndPayload{testTopic1, "msg1"}, riversharedtest.WaitOrTimeout(t, notifyChan))

		sub.Unlisten(ctx)

		require.Empty(t, notifier.subscriptions)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg2")

		time.Sleep(notificationWaitLeeway)

		requireNoNotification(t, notifyChan)
	})

	t.Run("ListenWithoutStart", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)

		notifyChan := make(chan TopicAndPayload, 10)

		sub, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan))
		require.NoError(t, err)
		t.Cleanup(func() { sub.Unlisten(ctx) })

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg1")

		time.Sleep(notificationWaitLeeway)

		// Not received because the notifier was never started and therefore
		// never started processing messages.
		requireNoNotification(t, notifyChan)
	})

	// This next set of tests are largely here to make sure in case of listen
	// problems, the internal subscriptions map is correctly reset back to its
	// expected state by removing the problematic subscription.

	t.Run("ListenWithoutStartConnectError", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)

		listenerMock := NewListenerMock(notifier.listener)
		listenerMock.connectFunc = func(ctx context.Context) error {
			return errors.New("error on connect")
		}
		notifier.listener = listenerMock

		_, err := notifier.Listen(ctx, testTopic1, nil)
		require.EqualError(t, err, "error on connect")

		require.Empty(t, notifier.subscriptions)
	})

	t.Run("ListenWithoutStartListenError", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)

		listenerMock := NewListenerMock(notifier.listener)
		listenerMock.listenFunc = func(ctx context.Context, topic string) error {
			return errors.New("error on listen")
		}
		notifier.listener = listenerMock

		_, err := notifier.Listen(ctx, testTopic1, nil)
		require.EqualError(t, err, fmt.Sprintf("error listening on topic %q: error on listen", testTopic1))

		require.Empty(t, notifier.subscriptions)
	})

	t.Run("ListenWithoutStartMultipleSubscriptionsError", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)

		listenerMock := NewListenerMock(notifier.listener)
		listenerMock.listenFunc = func(ctx context.Context, topic string) error {
			// First is allowed to succeed. Others fail.
			switch topic {
			case testTopic2:
				return errors.New("error on listen")
			default:
				return listenerMock.Listener.Listen(ctx, topic)
			}
		}
		notifier.listener = listenerMock

		sub, err := notifier.Listen(ctx, testTopic1, nil)
		require.NoError(t, err)
		t.Cleanup(func() { sub.Unlisten(ctx) })

		_, err = notifier.Listen(ctx, testTopic2, nil)
		require.EqualError(t, err, fmt.Sprintf("error listening on topic %q: error on listen", testTopic2))

		// Only the successful subscription is left.
		require.Equal(t, []NotificationTopic{testTopic1}, maputil.Keys(notifier.subscriptions))
	})

	t.Run("ListenBeforeStart", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)

		notifyChan := make(chan TopicAndPayload, 10)

		sub, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan))
		require.NoError(t, err)
		t.Cleanup(func() { sub.Unlisten(ctx) })

		start(t, notifier)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg1")

		require.Equal(t, TopicAndPayload{testTopic1, "msg1"}, riversharedtest.WaitOrTimeout(t, notifyChan))
	})

	t.Run("SingleTopicMultipleSubscribers", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)
		start(t, notifier)

		notifyChan1 := make(chan TopicAndPayload, 10)
		notifyChan2 := make(chan TopicAndPayload, 10)

		sub1, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan1))
		require.NoError(t, err)
		sub2, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan2))
		require.NoError(t, err)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg1")

		require.Equal(t, TopicAndPayload{testTopic1, "msg1"}, riversharedtest.WaitOrTimeout(t, notifyChan1))
		require.Equal(t, TopicAndPayload{testTopic1, "msg1"}, riversharedtest.WaitOrTimeout(t, notifyChan2))

		sub1.Unlisten(ctx)
		sub2.Unlisten(ctx)

		require.Empty(t, notifier.subscriptions)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg2")

		time.Sleep(notificationWaitLeeway)

		requireNoNotification(t, notifyChan1)
		requireNoNotification(t, notifyChan2)
	})

	t.Run("MultipleTopicsLockStep", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)
		start(t, notifier)

		notifyChan1 := make(chan TopicAndPayload, 10)
		notifyChan2 := make(chan TopicAndPayload, 10)

		sub1, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan1))
		require.NoError(t, err)
		sub2, err := notifier.Listen(ctx, testTopic2, topicAndPayloadNotifyFunc(notifyChan2))
		require.NoError(t, err)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg1_1")
		sendNotification(ctx, t, bundle.exec, testTopic2, "msg1_2")

		require.Equal(t, TopicAndPayload{testTopic1, "msg1_1"}, riversharedtest.WaitOrTimeout(t, notifyChan1))
		require.Equal(t, TopicAndPayload{testTopic2, "msg1_2"}, riversharedtest.WaitOrTimeout(t, notifyChan2))

		sub1.Unlisten(ctx)
		sub2.Unlisten(ctx)

		require.Empty(t, notifier.subscriptions)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg2_1")
		sendNotification(ctx, t, bundle.exec, testTopic2, "msg2_2")

		time.Sleep(notificationWaitLeeway)

		requireNoNotification(t, notifyChan1)
		requireNoNotification(t, notifyChan2)
	})

	t.Run("MultipleTopicsStaggered", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)
		start(t, notifier)

		notifyChan1 := make(chan TopicAndPayload, 10)
		notifyChan2 := make(chan TopicAndPayload, 10)

		sub1, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan1))
		require.NoError(t, err)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg1_1")
		sendNotification(ctx, t, bundle.exec, testTopic2, "msg1_2")

		time.Sleep(notificationWaitLeeway)

		// Only the first channel is subscribed.
		require.Equal(t, TopicAndPayload{testTopic1, "msg1_1"}, riversharedtest.WaitOrTimeout(t, notifyChan1))
		requireNoNotification(t, notifyChan2)

		sub2, err := notifier.Listen(ctx, testTopic2, topicAndPayloadNotifyFunc(notifyChan2))
		require.NoError(t, err)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg2_1")
		sendNotification(ctx, t, bundle.exec, testTopic2, "msg2_2")

		// Now both subscriptions are active.
		require.Equal(t, TopicAndPayload{testTopic1, "msg2_1"}, riversharedtest.WaitOrTimeout(t, notifyChan1))
		require.Equal(t, TopicAndPayload{testTopic2, "msg2_2"}, riversharedtest.WaitOrTimeout(t, notifyChan2))

		sub1.Unlisten(ctx)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg3_1")
		sendNotification(ctx, t, bundle.exec, testTopic2, "msg3_2")

		time.Sleep(notificationWaitLeeway)

		// First channel unsubscribed, but the second remains.
		requireNoNotification(t, notifyChan1)
		require.Equal(t, TopicAndPayload{testTopic2, "msg3_2"}, riversharedtest.WaitOrTimeout(t, notifyChan2))

		sub2.Unlisten(ctx)

		require.Empty(t, notifier.subscriptions)

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg4_1")
		sendNotification(ctx, t, bundle.exec, testTopic2, "msg4_2")

		time.Sleep(notificationWaitLeeway)

		requireNoNotification(t, notifyChan1)
		requireNoNotification(t, notifyChan2)
	})

	// Stress test meant to suss out any races that there might be in the
	// subscribe or interrupt loop code.
	t.Run("MultipleSubscribersStress", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)
		start(t, notifier)

		const (
			numSubscribers         = 10
			numSubscribeIterations = 5
		)

		notifyChans := make([]chan TopicAndPayload, numSubscribers)
		for i := range notifyChans {
			notifyChans[i] = make(chan TopicAndPayload, 1000)
		}

		// Start a goroutine to send messages constantly.
		var (
			sendNotificationsDone     = make(chan struct{})
			sendNotificationsShutdown = make(chan struct{})
		)
		go func() {
			defer close(sendNotificationsDone)

			ticker := time.NewTicker(10 * time.Millisecond)
			defer ticker.Stop()

			for messageNum := 0; ; messageNum++ {
				sendNotification(ctx, t, bundle.exec, testTopic1, "msg"+strconv.Itoa(messageNum))

				select {
				case <-ctx.Done():
					return
				case <-sendNotificationsShutdown:
					return
				case <-ticker.C:
					// loop again
				}
			}
		}()

		var wg sync.WaitGroup
		wg.Add(len(notifyChans))
		for i := range notifyChans {
			notifyChan := notifyChans[i]

			go func() {
				defer wg.Done()

				for range numSubscribeIterations {
					sub, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan))
					require.NoError(t, err)

					// Pause a random brief amount of time.
					serviceutil.CancellableSleep(ctx, randutil.DurationBetween(15*time.Millisecond, 50*time.Millisecond))

					sub.Unlisten(ctx)
				}
			}()
		}

		wg.Wait()                        // wait for subscribe loops to finish all their work
		close(sendNotificationsShutdown) // stop sending notifications
		<-sendNotificationsDone          // wait for notifications goroutine to finish

		for i := range notifyChans {
			t.Logf("Channel %2d contains %3d message(s)", i, len(notifyChans[i]))

			// Don't require a specific number of messages to have been received
			// since it's non-deterministic, but every channel should've gotten
			// at least one message. It my test runs, they receive ~15 each.
			require.NotEmpty(t, notifyChans[i])
		}
	})

	t.Run("WaitErrorAndBackoff", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)

		notifier.disableSleep = true

		var errorNum int

		// Use a mock to simulate an error for this one because it's really hard
		// to get the timing right otherwise, and hard to avoid races.
		listenerMock := NewListenerMock(notifier.listener)
		listenerMock.waitForNotificationFunc = func(ctx context.Context) (*riverdriver.Notification, error) {
			errorNum++
			return nil, fmt.Errorf("error during wait %d", errorNum)
		}
		notifier.listener = listenerMock

		start(t, notifier)

		// The service normally sleeps with an exponential backoff after an
		// error, but we've disabled sleep above, so we can pull errors out of
		// the test signal as quickly as we want.
		require.EqualError(t, notifier.testSignals.BackoffError.WaitOrTimeout(), "error during wait 1")
		require.EqualError(t, notifier.testSignals.BackoffError.WaitOrTimeout(), "error during wait 2")
		require.EqualError(t, notifier.testSignals.BackoffError.WaitOrTimeout(), "error during wait 3")
	})

	t.Run("BackoffSleepCancelledOnStop", func(t *testing.T) {
		t.Parallel()

		notifier, _ := setup(t)

		listenerMock := NewListenerMock(notifier.listener)
		listenerMock.waitForNotificationFunc = func(ctx context.Context) (*riverdriver.Notification, error) {
			return nil, errors.New("error during wait")
		}
		notifier.listener = listenerMock

		start(t, notifier)

		// The loop goes to sleep as soon as it fires this test signal, but it's
		// cancelled immediately as the test cleanup look issues a Stop.
		require.EqualError(t, notifier.testSignals.BackoffError.WaitOrTimeout(), "error during wait")
	})

	t.Run("StillFunctionalAfterMainLoopFailure", func(t *testing.T) {
		t.Parallel()

		notifier, bundle := setup(t)

		// Disable the backoff sleep that would occur after the first retry.
		notifier.disableSleep = true

		var errorNum int

		listenerMock := NewListenerMock(notifier.listener)
		listenerMock.waitForNotificationFunc = func(ctx context.Context) (*riverdriver.Notification, error) {
			// Returns an error the first time, but then works after.
			errorNum++
			switch errorNum {
			case 1:
				return nil, errors.New("error during wait")
			default:
				return listenerMock.Listener.WaitForNotification(ctx)
			}
		}
		notifier.listener = listenerMock

		notifyChan := make(chan TopicAndPayload, 10)

		start(t, notifier)

		notifier.testSignals.ListeningBegin.WaitOrTimeout()

		sub, err := notifier.Listen(ctx, testTopic1, topicAndPayloadNotifyFunc(notifyChan))
		require.NoError(t, err)
		t.Cleanup(func() { sub.Unlisten(ctx) })

		// First failure, after which the loop will reenter and start producing again.
		require.EqualError(t, notifier.testSignals.BackoffError.WaitOrTimeout(), "error during wait")

		// It is possible for notifications to be missed while the loop is
		// restarting, so make sure we're back in the listening loop before
		// sending the notification below.
		notifier.testSignals.ListeningBegin.WaitOrTimeout()

		sendNotification(ctx, t, bundle.exec, testTopic1, "msg1")

		// Subscription should still work.
		require.Equal(t, TopicAndPayload{testTopic1, "msg1"}, riversharedtest.WaitOrTimeout(t, notifyChan))
	})
}

type ListenerMock struct {
	riverdriver.Listener

	connectFunc             func(ctx context.Context) error
	listenFunc              func(ctx context.Context, topic string) error
	waitForNotificationFunc func(ctx context.Context) (*riverdriver.Notification, error)
}

func NewListenerMock(listener riverdriver.Listener) *ListenerMock {
	return &ListenerMock{
		Listener: listener,

		connectFunc:             listener.Connect,
		listenFunc:              listener.Listen,
		waitForNotificationFunc: listener.WaitForNotification,
	}
}

func (l *ListenerMock) Connect(ctx context.Context) error {
	return l.connectFunc(ctx)
}

func (l *ListenerMock) Listen(ctx context.Context, topic string) error {
	return l.listenFunc(ctx, topic)
}

func (l *ListenerMock) WaitForNotification(ctx context.Context) (*riverdriver.Notification, error) {
	return l.waitForNotificationFunc(ctx)
}

type TopicAndPayload struct {
	topic   NotificationTopic
	payload string
}

func topicAndPayloadNotifyFunc(notifyChan chan TopicAndPayload) NotifyFunc {
	return func(topic NotificationTopic, payload string) {
		notifyChan <- TopicAndPayload{topic, payload}
	}
}

func sendNotification(ctx context.Context, t *testing.T, exec riverdriver.Executor, topic string, payload string) {
	t.Helper()

	t.Logf("Sending notification on %q: %s", topic, payload)
	require.NoError(t, exec.NotifyMany(ctx, &riverdriver.NotifyManyParams{Payload: []string{payload}, Topic: topic}))
}

```

`internal/notifylimiter/limiter.go`:

```go
package notifylimiter

import (
	"sync"
	"time"

	"github.com/riverqueue/river/rivershared/baseservice"
)

type NotifyFunc func(name string)

type Limiter struct {
	baseservice.BaseService

	waitDuration time.Duration

	mu              sync.Mutex // protects lastSentByTopic
	lastSentByTopic map[string]time.Time
}

// NewLimiter creates a new Limiter, calling the NotifyFunc no more than once per waitDuration.
// The function must be fast as it is called within a mutex.
func NewLimiter(archetype *baseservice.Archetype, waitDuration time.Duration) *Limiter {
	return baseservice.Init(archetype, &Limiter{
		lastSentByTopic: make(map[string]time.Time),
		waitDuration:    waitDuration,
	})
}

func (l *Limiter) ShouldTrigger(topic string) bool {
	// Calculate this beforehand to reduce mutex duration.
	now := l.Time.NowUTC()
	lastSentHorizon := now.Add(-l.waitDuration)

	l.mu.Lock()
	defer l.mu.Unlock()

	if l.lastSentByTopic[topic].Before(lastSentHorizon) {
		l.lastSentByTopic[topic] = now
		return true
	}

	return false
}

```

`internal/notifylimiter/limiter_test.go`:

```go
package notifylimiter

import (
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/riversharedtest"
)

func TestLimiter(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func() (*Limiter, *testBundle) {
		bundle := &testBundle{}

		archetype := riversharedtest.BaseServiceArchetype(t)
		limiter := NewLimiter(archetype, 10*time.Millisecond)

		return limiter, bundle
	}

	t.Run("OnlySendsOncePerWaitDuration", func(t *testing.T) {
		t.Parallel()

		limiter, _ := setup()
		now := time.Now()
		limiter.Time.StubNowUTC(now)

		require.True(t, limiter.ShouldTrigger("a"))
		for range 10 {
			require.False(t, limiter.ShouldTrigger("a"))
		}
		// Move the time forward, by just less than waitDuration:
		limiter.Time.StubNowUTC(now.Add(9 * time.Millisecond))
		require.False(t, limiter.ShouldTrigger("a"))

		require.True(t, limiter.ShouldTrigger("b")) // First time being triggered on "b"

		// Move the time forward to just past the waitDuration:
		limiter.Time.StubNowUTC(now.Add(11 * time.Millisecond))
		require.True(t, limiter.ShouldTrigger("a"))
		for range 10 {
			require.False(t, limiter.ShouldTrigger("a"))
		}

		require.False(t, limiter.ShouldTrigger("b")) // has only been 2ms since last trigger of "b"

		// Move forward by another waitDuration (plus padding):
		limiter.Time.StubNowUTC(now.Add(22 * time.Millisecond))
		require.True(t, limiter.ShouldTrigger("a"))
		require.True(t, limiter.ShouldTrigger("b"))
		require.False(t, limiter.ShouldTrigger("b"))
	})

	t.Run("ConcurrentAccessStressTest", func(t *testing.T) {
		t.Parallel()

		doneCh := make(chan struct{})
		t.Cleanup(func() { close(doneCh) })

		limiter, _ := setup()
		now := time.Now()
		limiter.Time.StubNowUTC(now)

		counters := make(map[string]*atomic.Int64)
		for _, topic := range []string{"a", "b", "c"} {
			counters[topic] = &atomic.Int64{}
		}

		signalContinuously := func(topic string) {
			for {
				select {
				case <-doneCh:
					return
				default:
					shouldTrigger := limiter.ShouldTrigger(topic)
					if shouldTrigger {
						counters[topic].Add(1)
					}
				}
			}
		}
		go signalContinuously("a")
		go signalContinuously("b")
		go signalContinuously("c")

		// Duration doesn't really matter here, just need time for these all to fire
		// a bit:
		<-time.After(100 * time.Millisecond)

		require.Equal(t, int64(1), counters["a"].Load())
		require.Equal(t, int64(1), counters["b"].Load())
		require.Equal(t, int64(1), counters["c"].Load())

		limiter.Time.StubNowUTC(now.Add(11 * time.Millisecond))

		<-time.After(100 * time.Millisecond)

		require.Equal(t, int64(2), counters["a"].Load())
		require.Equal(t, int64(2), counters["b"].Load())
		require.Equal(t, int64(2), counters["c"].Load())
	})
}

```

`internal/rivercommon/river_common.go`:

```go
package rivercommon

import (
	"errors"
)

// These constants are made available in rivercommon so that they're accessible
// by internal packages, but the top-level river package re-exports them, and
// all user code must use that set instead.
const (
	// AllQueuesString is a special string that can be used to indicate all
	// queues in some operations, particularly pause and resume.
	AllQueuesString    = "*"
	MaxAttemptsDefault = 25
	PriorityDefault    = 1
	QueueDefault       = "default"
)

type ContextKeyClient struct{}

// ErrShutdown is a special error injected by the client into its fetch and work
// CancelCauseFuncs when it's stopping. It may be used by components for such
// cases like avoiding logging an error during a normal shutdown procedure. This
// is internal for the time being, but we could also consider exposing it.
var ErrShutdown = errors.New("shutdown initiated")

```

`internal/riverinternaltest/retrypolicytest/retrypolicytest.go`:

```go
package retrypolicytest

import (
	"fmt"
	"math"
	"time"

	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivertype"
)

// RetryPolicyCustom is a retry policy demonstrating trivial customization.
type RetryPolicyCustom struct{}

func (p *RetryPolicyCustom) NextRetry(job *rivertype.JobRow) time.Time {
	var backoffDuration time.Duration
	switch job.Attempt {
	case 1:
		backoffDuration = 10 * time.Second
	case 2:
		backoffDuration = 20 * time.Second
	case 3:
		backoffDuration = 30 * time.Second
	default:
		panic(fmt.Sprintf("next retry should not have been called for attempt %d", job.Attempt))
	}

	return job.AttemptedAt.Add(backoffDuration)
}

// RetryPolicyInvalid is a retry policy that returns invalid timestamps.
type RetryPolicyInvalid struct{}

func (p *RetryPolicyInvalid) NextRetry(job *rivertype.JobRow) time.Time { return time.Time{} }

// The maximum value of a duration before it overflows. About 292 years.
const maxDuration time.Duration = 1<<63 - 1

// Same as the above, but changed to a float represented in seconds.
var maxDurationSeconds = maxDuration.Seconds() //nolint:gochecknoglobals

// RetryPolicyNoJitter is identical to default retry policy except that it
// leaves off the jitter to make checking against it more convenient.
type RetryPolicyNoJitter struct{}

func (p *RetryPolicyNoJitter) NextRetry(job *rivertype.JobRow) time.Time {
	return job.AttemptedAt.Add(timeutil.SecondsAsDuration(p.retrySecondsWithoutJitter(job.Attempt)))
}

// Gets a base number of retry seconds for the given attempt, jitter excluded.
// If the number of seconds returned would overflow time.Duration if it were to
// be made one, returns the maximum number of seconds that can fit in a
// time.Duration instead, approximately 292 years.
func (p *RetryPolicyNoJitter) retrySecondsWithoutJitter(attempt int) float64 {
	retrySeconds := math.Pow(float64(attempt), 4)
	return min(retrySeconds, maxDurationSeconds)
}

```

`internal/riverinternaltest/riverdrivertest/riverdrivertest.go`:

```go
package riverdrivertest

import (
	"context"
	"encoding/json"
	"fmt"
	"slices"
	"sort"
	"strconv"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
	"github.com/tidwall/gjson"
	"golang.org/x/text/cases"
	"golang.org/x/text/language"

	"github.com/riverqueue/river/internal/dbunique"
	"github.com/riverqueue/river/internal/notifier"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

// Exercise fully exercises a driver. The driver's listener is exercised if
// supported.
func Exercise[TTx any](ctx context.Context, t *testing.T,
	driverWithPool func(ctx context.Context, t *testing.T) riverdriver.Driver[TTx],
	executorWithTx func(ctx context.Context, t *testing.T) riverdriver.Executor,
) {
	t.Helper()

	if driverWithPool(ctx, t).SupportsListener() {
		exerciseListener(ctx, t, driverWithPool)
	} else {
		t.Logf("Driver does not support listener; skipping listener tests")
	}

	t.Run("GetMigrationFS", func(t *testing.T) {
		driver := driverWithPool(ctx, t)

		for _, line := range driver.GetMigrationLines() {
			migrationFS := driver.GetMigrationFS(line)

			// Directory for the advertised migration line should exist.
			_, err := migrationFS.Open("migration/" + line)
			require.NoError(t, err)
		}
	})

	t.Run("GetMigrationLines", func(t *testing.T) {
		driver := driverWithPool(ctx, t)

		// Should contain at minimum a main migration line.
		require.Contains(t, driver.GetMigrationLines(), riverdriver.MigrationLineMain)
	})

	type testBundle struct{}

	setup := func(ctx context.Context, t *testing.T) (riverdriver.Executor, *testBundle) {
		t.Helper()
		return executorWithTx(ctx, t), &testBundle{}
	}

	const clientID = "test-client-id"

	t.Run("Begin", func(t *testing.T) {
		t.Parallel()

		t.Run("BasicVisibility", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			tx, err := exec.Begin(ctx)
			require.NoError(t, err)
			t.Cleanup(func() { _ = tx.Rollback(ctx) })

			// Job visible in subtransaction, but not parent.
			{
				job := testfactory.Job(ctx, t, tx, &testfactory.JobOpts{})

				_, err = tx.JobGetByID(ctx, job.ID)
				require.NoError(t, err)

				require.NoError(t, tx.Rollback(ctx))

				_, err = exec.JobGetByID(ctx, job.ID)
				require.ErrorIs(t, err, rivertype.ErrNotFound)
			}
		})

		t.Run("NestedTransactions", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			tx1, err := exec.Begin(ctx)
			require.NoError(t, err)
			t.Cleanup(func() { _ = tx1.Rollback(ctx) })

			// Job visible in tx1, but not top level executor.
			{
				job1 := testfactory.Job(ctx, t, tx1, &testfactory.JobOpts{})

				{
					tx2, err := tx1.Begin(ctx)
					require.NoError(t, err)
					t.Cleanup(func() { _ = tx2.Rollback(ctx) })

					// Job visible in tx2, but not top level executor.
					{
						job2 := testfactory.Job(ctx, t, tx2, &testfactory.JobOpts{})

						_, err = tx2.JobGetByID(ctx, job2.ID)
						require.NoError(t, err)

						require.NoError(t, tx2.Rollback(ctx))

						_, err = tx1.JobGetByID(ctx, job2.ID)
						require.ErrorIs(t, err, rivertype.ErrNotFound)
					}

					_, err = tx1.JobGetByID(ctx, job1.ID)
					require.NoError(t, err)
				}

				// Repeat the same subtransaction again.
				{
					tx2, err := tx1.Begin(ctx)
					require.NoError(t, err)
					t.Cleanup(func() { _ = tx2.Rollback(ctx) })

					// Job visible in tx2, but not top level executor.
					{
						job2 := testfactory.Job(ctx, t, tx2, &testfactory.JobOpts{})

						_, err = tx2.JobGetByID(ctx, job2.ID)
						require.NoError(t, err)

						require.NoError(t, tx2.Rollback(ctx))

						_, err = tx1.JobGetByID(ctx, job2.ID)
						require.ErrorIs(t, err, rivertype.ErrNotFound)
					}

					_, err = tx1.JobGetByID(ctx, job1.ID)
					require.NoError(t, err)
				}

				require.NoError(t, tx1.Rollback(ctx))

				_, err = exec.JobGetByID(ctx, job1.ID)
				require.ErrorIs(t, err, rivertype.ErrNotFound)
			}
		})

		t.Run("RollbackAfterCommit", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			tx1, err := exec.Begin(ctx)
			require.NoError(t, err)
			t.Cleanup(func() { _ = tx1.Rollback(ctx) })

			tx2, err := tx1.Begin(ctx)
			require.NoError(t, err)
			t.Cleanup(func() { _ = tx2.Rollback(ctx) })

			job := testfactory.Job(ctx, t, tx2, &testfactory.JobOpts{})

			require.NoError(t, tx2.Commit(ctx))
			_ = tx2.Rollback(ctx) // "tx is closed" error generally returned, but don't require this

			// Despite rollback being called after commit, the job is still
			// visible from the outer transaction.
			_, err = tx1.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
		})
	})

	t.Run("ColumnExists", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		exists, err := exec.ColumnExists(ctx, "river_job", "id")
		require.NoError(t, err)
		require.True(t, exists)

		exists, err = exec.ColumnExists(ctx, "river_job", "does_not_exist")
		require.NoError(t, err)
		require.False(t, exists)

		exists, err = exec.ColumnExists(ctx, "does_not_exist", "id")
		require.NoError(t, err)
		require.False(t, exists)

		// Will be rolled back by the test transaction.
		_, err = exec.Exec(ctx, "CREATE SCHEMA another_schema_123")
		require.NoError(t, err)

		_, err = exec.Exec(ctx, "SET search_path = another_schema_123")
		require.NoError(t, err)

		// Table with the same name as the main schema, but without the same
		// columns.
		_, err = exec.Exec(ctx, "CREATE TABLE river_job (another_id bigint)")
		require.NoError(t, err)

		exists, err = exec.ColumnExists(ctx, "river_job", "id")
		require.NoError(t, err)
		require.False(t, exists)
	})

	t.Run("Exec", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		_, err := exec.Exec(ctx, "SELECT 1 + 2")
		require.NoError(t, err)
	})

	t.Run("JobCancel", func(t *testing.T) {
		t.Parallel()

		for _, startingState := range []rivertype.JobState{
			rivertype.JobStateAvailable,
			rivertype.JobStateRetryable,
			rivertype.JobStateScheduled,
		} {
			t.Run(fmt.Sprintf("CancelsJobIn%sState", startingState), func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				now := time.Now().UTC()
				nowStr := now.Format(time.RFC3339Nano)

				job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
					State:     &startingState,
					UniqueKey: []byte("unique-key"),
				})
				require.Equal(t, startingState, job.State)

				jobAfter, err := exec.JobCancel(ctx, &riverdriver.JobCancelParams{
					ID:                job.ID,
					CancelAttemptedAt: now,
					ControlTopic:      string(notifier.NotificationTopicControl),
				})
				require.NoError(t, err)
				require.NotNil(t, jobAfter)

				require.Equal(t, rivertype.JobStateCancelled, jobAfter.State)
				require.WithinDuration(t, time.Now(), *jobAfter.FinalizedAt, 2*time.Second)
				require.JSONEq(t, fmt.Sprintf(`{"cancel_attempted_at":%q}`, nowStr), string(jobAfter.Metadata))
			})
		}

		t.Run("RunningJobIsNotImmediatelyCancelled", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()
			nowStr := now.Format(time.RFC3339Nano)

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State:     ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey: []byte("unique-key"),
			})
			require.Equal(t, rivertype.JobStateRunning, job.State)

			jobAfter, err := exec.JobCancel(ctx, &riverdriver.JobCancelParams{
				ID:                job.ID,
				CancelAttemptedAt: now,
				ControlTopic:      string(notifier.NotificationTopicControl),
			})
			require.NoError(t, err)
			require.NotNil(t, jobAfter)
			require.Equal(t, rivertype.JobStateRunning, jobAfter.State)
			require.Nil(t, jobAfter.FinalizedAt)
			require.JSONEq(t, fmt.Sprintf(`{"cancel_attempted_at":%q}`, nowStr), string(jobAfter.Metadata))
			require.Equal(t, "unique-key", string(jobAfter.UniqueKey))
		})

		for _, startingState := range []rivertype.JobState{
			rivertype.JobStateCancelled,
			rivertype.JobStateCompleted,
			rivertype.JobStateDiscarded,
		} {
			t.Run(fmt.Sprintf("DoesNotAlterFinalizedJobIn%sState", startingState), func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
					FinalizedAt: ptrutil.Ptr(time.Now()),
					State:       &startingState,
				})

				jobAfter, err := exec.JobCancel(ctx, &riverdriver.JobCancelParams{
					ID:                job.ID,
					CancelAttemptedAt: time.Now(),
					ControlTopic:      string(notifier.NotificationTopicControl),
				})
				require.NoError(t, err)
				require.Equal(t, startingState, jobAfter.State)
				require.WithinDuration(t, *job.FinalizedAt, *jobAfter.FinalizedAt, time.Microsecond)
				require.JSONEq(t, `{}`, string(jobAfter.Metadata))
			})
		}

		t.Run("ReturnsErrNotFoundIfJobDoesNotExist", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			jobAfter, err := exec.JobCancel(ctx, &riverdriver.JobCancelParams{
				ID:                1234567890,
				CancelAttemptedAt: time.Now(),
				ControlTopic:      string(notifier.NotificationTopicControl),
			})
			require.ErrorIs(t, err, rivertype.ErrNotFound)
			require.Nil(t, jobAfter)
		})
	})

	t.Run("JobCountByState", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		// Included because they're the queried state.
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})

		// Excluded because they're not.
		finalizedAt := ptrutil.Ptr(time.Now())
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: finalizedAt, State: ptrutil.Ptr(rivertype.JobStateCancelled)})
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: finalizedAt, State: ptrutil.Ptr(rivertype.JobStateCompleted)})
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: finalizedAt, State: ptrutil.Ptr(rivertype.JobStateDiscarded)})

		numJobs, err := exec.JobCountByState(ctx, rivertype.JobStateAvailable)
		require.NoError(t, err)
		require.Equal(t, 2, numJobs)
	})

	t.Run("JobDelete", func(t *testing.T) {
		t.Parallel()

		t.Run("DoesNotDeleteARunningJob", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State: ptrutil.Ptr(rivertype.JobStateRunning),
			})

			jobAfter, err := exec.JobDelete(ctx, job.ID)
			require.ErrorIs(t, err, rivertype.ErrJobRunning)
			require.Nil(t, jobAfter)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRunning, jobUpdated.State)
		})

		for _, state := range []rivertype.JobState{
			rivertype.JobStateAvailable,
			rivertype.JobStateCancelled,
			rivertype.JobStateCompleted,
			rivertype.JobStateDiscarded,
			rivertype.JobStatePending,
			rivertype.JobStateRetryable,
			rivertype.JobStateScheduled,
		} {
			t.Run(fmt.Sprintf("DeletesA_%s_Job", state), func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				now := time.Now().UTC()

				setFinalized := slices.Contains([]rivertype.JobState{
					rivertype.JobStateCancelled,
					rivertype.JobStateCompleted,
					rivertype.JobStateDiscarded,
				}, state)

				var finalizedAt *time.Time
				if setFinalized {
					finalizedAt = &now
				}

				job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
					FinalizedAt: finalizedAt,
					ScheduledAt: ptrutil.Ptr(now.Add(1 * time.Hour)),
					State:       &state,
				})

				jobAfter, err := exec.JobDelete(ctx, job.ID)
				require.NoError(t, err)
				require.NotNil(t, jobAfter)
				require.Equal(t, job.ID, jobAfter.ID)
				require.Equal(t, state, jobAfter.State)

				_, err = exec.JobGetByID(ctx, job.ID)
				require.ErrorIs(t, err, rivertype.ErrNotFound)
			})
		}

		t.Run("ReturnsErrNotFoundIfJobDoesNotExist", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			jobAfter, err := exec.JobDelete(ctx, 1234567890)
			require.ErrorIs(t, err, rivertype.ErrNotFound)
			require.Nil(t, jobAfter)
		})
	})

	t.Run("JobDeleteBefore", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		var (
			horizon       = time.Now()
			beforeHorizon = horizon.Add(-1 * time.Minute)
			afterHorizon  = horizon.Add(1 * time.Minute)
		)

		deletedJob1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateCancelled)})
		deletedJob2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateCompleted)})
		deletedJob3 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateDiscarded)})

		// Not deleted because not appropriate state.
		notDeletedJob1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		notDeletedJob2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

		// Not deleted because after the delete horizon.
		notDeletedJob3 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: &afterHorizon, State: ptrutil.Ptr(rivertype.JobStateCancelled)})

		// Max two deleted on the first pass.
		numDeleted, err := exec.JobDeleteBefore(ctx, &riverdriver.JobDeleteBeforeParams{
			CancelledFinalizedAtHorizon: horizon,
			CompletedFinalizedAtHorizon: horizon,
			DiscardedFinalizedAtHorizon: horizon,
			Max:                         2,
		})
		require.NoError(t, err)
		require.Equal(t, 2, numDeleted)

		// And one more pass gets the last one.
		numDeleted, err = exec.JobDeleteBefore(ctx, &riverdriver.JobDeleteBeforeParams{
			CancelledFinalizedAtHorizon: horizon,
			CompletedFinalizedAtHorizon: horizon,
			DiscardedFinalizedAtHorizon: horizon,
			Max:                         2,
		})
		require.NoError(t, err)
		require.Equal(t, 1, numDeleted)

		// All deleted.
		_, err = exec.JobGetByID(ctx, deletedJob1.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = exec.JobGetByID(ctx, deletedJob2.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)
		_, err = exec.JobGetByID(ctx, deletedJob3.ID)
		require.ErrorIs(t, err, rivertype.ErrNotFound)

		// Not deleted
		_, err = exec.JobGetByID(ctx, notDeletedJob1.ID)
		require.NoError(t, err)
		_, err = exec.JobGetByID(ctx, notDeletedJob2.ID)
		require.NoError(t, err)
		_, err = exec.JobGetByID(ctx, notDeletedJob3.ID)
		require.NoError(t, err)
	})

	t.Run("JobGetAvailable", func(t *testing.T) {
		t.Parallel()

		t.Run("Success", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})

			jobRows, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         100,
				Queue:       rivercommon.QueueDefault,
			})
			require.NoError(t, err)
			require.Len(t, jobRows, 1)

			jobRow := jobRows[0]
			require.Equal(t, []string{clientID}, jobRow.AttemptedBy)
		})

		t.Run("ConstrainedToLimit", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})

			// Two rows inserted but only one found because of the added limit.
			jobRows, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         1,
				Queue:       rivercommon.QueueDefault,
			})
			require.NoError(t, err)
			require.Len(t, jobRows, 1)
		})

		t.Run("ConstrainedToQueue", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				Queue: ptrutil.Ptr("other-queue"),
			})

			// Job is in a non-default queue so it's not found.
			jobRows, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         100,
				Queue:       rivercommon.QueueDefault,
			})
			require.NoError(t, err)
			require.Empty(t, jobRows)
		})

		t.Run("ConstrainedToScheduledAtBeforeNow", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt: ptrutil.Ptr(time.Now().Add(1 * time.Minute)),
			})

			// Job is scheduled a while from now so it's not found.
			jobRows, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         100,
				Queue:       rivercommon.QueueDefault,
			})
			require.NoError(t, err)
			require.Empty(t, jobRows)
		})

		t.Run("ConstrainedToScheduledAtBeforeCustomNowTime", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().Add(1 * time.Minute)
			// Job 1 is scheduled after now so it's not found:
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt: ptrutil.Ptr(now.Add(1 * time.Minute)),
			})
			// Job 2 is scheduled just before now so it's found:
			job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Microsecond)),
			})

			jobRows, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         100,
				Now:         ptrutil.Ptr(now),
				Queue:       rivercommon.QueueDefault,
			})
			require.NoError(t, err)
			require.Len(t, jobRows, 1)
			require.Equal(t, job2.ID, jobRows[0].ID)
		})

		t.Run("Prioritized", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			// Insert jobs with decreasing priority numbers (3, 2, 1) which means increasing priority.
			for i := 3; i > 0; i-- {
				_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
					Priority: &i,
				})
			}

			jobRows, err := exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         2,
				Queue:       rivercommon.QueueDefault,
			})
			require.NoError(t, err)
			require.Len(t, jobRows, 2, "expected to fetch exactly 2 jobs")

			// Because the jobs are ordered within the fetch query's CTE but *not* within
			// the final query, the final result list may not actually be sorted. This is
			// fine, because we've already ensured that we've fetched the jobs we wanted
			// to fetch via that ORDER BY. For testing we'll need to sort the list after
			// fetch to easily assert that the expected jobs are in it.
			sort.Slice(jobRows, func(i, j int) bool { return jobRows[i].Priority < jobRows[j].Priority })

			require.Equal(t, 1, jobRows[0].Priority, "expected first job to have priority 1")
			require.Equal(t, 2, jobRows[1].Priority, "expected second job to have priority 2")

			// Should fetch the one remaining job on the next attempt:
			jobRows, err = exec.JobGetAvailable(ctx, &riverdriver.JobGetAvailableParams{
				AttemptedBy: clientID,
				Max:         1,
				Queue:       rivercommon.QueueDefault,
			})
			require.NoError(t, err)
			require.NoError(t, err)
			require.Len(t, jobRows, 1, "expected to fetch exactly 1 job")
			require.Equal(t, 3, jobRows[0].Priority, "expected final job to have priority 3")
		})
	})

	t.Run("JobGetByID", func(t *testing.T) {
		t.Parallel()

		t.Run("FetchesAnExistingJob", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})

			fetchedJob, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.NotNil(t, fetchedJob)

			require.Equal(t, job.ID, fetchedJob.ID)
			require.Equal(t, rivertype.JobStateAvailable, fetchedJob.State)
			require.WithinDuration(t, now, fetchedJob.CreatedAt, 100*time.Millisecond)
			require.WithinDuration(t, now, fetchedJob.ScheduledAt, 100*time.Millisecond)
		})

		t.Run("ReturnsErrNotFoundIfJobDoesNotExist", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			job, err := exec.JobGetByID(ctx, 0)
			require.Error(t, err)
			require.ErrorIs(t, err, rivertype.ErrNotFound)
			require.Nil(t, job)
		})
	})

	t.Run("JobGetByIDMany", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})
		job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})

		// Not returned.
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})

		jobs, err := exec.JobGetByIDMany(ctx, []int64{job1.ID, job2.ID})
		require.NoError(t, err)
		require.Equal(t, []int64{job1.ID, job2.ID},
			sliceutil.Map(jobs, func(j *rivertype.JobRow) int64 { return j.ID }))
	})

	t.Run("JobGetByKindAndUniqueProperties", func(t *testing.T) {
		t.Parallel()

		const uniqueJobKind = "unique_job_kind"

		t.Run("NoOptions", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind)})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("other_kind")})

			fetchedJob, err := exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind: uniqueJobKind,
			})
			require.NoError(t, err)
			require.Equal(t, job.ID, fetchedJob.ID)

			_, err = exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind: "does_not_exist",
			})
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		})

		t.Run("ByArgs", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			args := []byte(`{"unique": "args"}`)

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), EncodedArgs: args})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), EncodedArgs: []byte(`{"other": "args"}`)})

			fetchedJob, err := exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:   uniqueJobKind,
				ByArgs: true,
				Args:   args,
			})
			require.NoError(t, err)
			require.Equal(t, job.ID, fetchedJob.ID)

			_, err = exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:   uniqueJobKind,
				ByArgs: true,
				Args:   []byte(`{"does_not_exist": "args"}`),
			})
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		})

		t.Run("ByCreatedAt", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			createdAt := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), CreatedAt: &createdAt})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), CreatedAt: ptrutil.Ptr(createdAt.Add(10 * time.Minute))})

			fetchedJob, err := exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:           uniqueJobKind,
				ByCreatedAt:    true,
				CreatedAtBegin: createdAt.Add(-5 * time.Minute),
				CreatedAtEnd:   createdAt.Add(5 * time.Minute),
			})
			require.NoError(t, err)
			require.Equal(t, job.ID, fetchedJob.ID)

			_, err = exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:           uniqueJobKind,
				ByCreatedAt:    true,
				CreatedAtBegin: createdAt.Add(-15 * time.Minute),
				CreatedAtEnd:   createdAt.Add(-5 * time.Minute),
			})
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		})

		t.Run("ByQueue", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			const queue = "unique_queue"

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), Queue: ptrutil.Ptr(queue)})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), Queue: ptrutil.Ptr("other_queue")})

			fetchedJob, err := exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:    uniqueJobKind,
				ByQueue: true,
				Queue:   queue,
			})
			require.NoError(t, err)
			require.Equal(t, job.ID, fetchedJob.ID)

			_, err = exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:    uniqueJobKind,
				ByQueue: true,
				Queue:   "does_not_exist",
			})
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		})

		t.Run("ByState", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			const state = rivertype.JobStateCompleted

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), FinalizedAt: ptrutil.Ptr(time.Now()), State: ptrutil.Ptr(state)})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr(uniqueJobKind), State: ptrutil.Ptr(rivertype.JobStateRetryable)})

			fetchedJob, err := exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:    uniqueJobKind,
				ByState: true,
				State:   []string{string(state)},
			})
			require.NoError(t, err)
			require.Equal(t, job.ID, fetchedJob.ID)

			_, err = exec.JobGetByKindAndUniqueProperties(ctx, &riverdriver.JobGetByKindAndUniquePropertiesParams{
				Kind:    uniqueJobKind,
				ByState: true,
				State:   []string{string(rivertype.JobStateScheduled)},
			})
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		})
	})

	t.Run("JobGetByKindMany", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("kind1")})
		job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("kind2")})

		// Not returned.
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("kind3")})

		jobs, err := exec.JobGetByKindMany(ctx, []string{job1.Kind, job2.Kind})
		require.NoError(t, err)
		require.Equal(t, []int64{job1.ID, job2.ID},
			sliceutil.Map(jobs, func(j *rivertype.JobRow) int64 { return j.ID }))
	})

	t.Run("JobGetStuck", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		var (
			horizon       = time.Now()
			beforeHorizon = horizon.Add(-1 * time.Minute)
			afterHorizon  = horizon.Add(1 * time.Minute)
		)

		stuckJob1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{AttemptedAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateRunning)})
		stuckJob2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{AttemptedAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateRunning)})

		// Not returned because we put a maximum of two.
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{AttemptedAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateRunning)})

		// Not stuck because not in running state.
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})

		// Not stuck because after queried horizon.
		_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{AttemptedAt: &afterHorizon, State: ptrutil.Ptr(rivertype.JobStateRunning)})

		// Max two stuck
		stuckJobs, err := exec.JobGetStuck(ctx, &riverdriver.JobGetStuckParams{
			StuckHorizon: horizon,
			Max:          2,
		})
		require.NoError(t, err)
		require.Equal(t, []int64{stuckJob1.ID, stuckJob2.ID},
			sliceutil.Map(stuckJobs, func(j *rivertype.JobRow) int64 { return j.ID }))
	})

	t.Run("JobInsertFastMany", func(t *testing.T) {
		t.Parallel()

		t.Run("AllArgs", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			insertParams := make([]*riverdriver.JobInsertFastParams, 10)
			for i := 0; i < len(insertParams); i++ {
				insertParams[i] = &riverdriver.JobInsertFastParams{
					CreatedAt:    ptrutil.Ptr(now.Add(time.Duration(i) * 5 * time.Second)),
					EncodedArgs:  []byte(`{"encoded": "args"}`),
					Kind:         "test_kind",
					MaxAttempts:  rivercommon.MaxAttemptsDefault,
					Metadata:     []byte(`{"meta": "data"}`),
					Priority:     rivercommon.PriorityDefault,
					Queue:        rivercommon.QueueDefault,
					ScheduledAt:  ptrutil.Ptr(now.Add(time.Duration(i) * time.Minute)),
					State:        rivertype.JobStateAvailable,
					Tags:         []string{"tag"},
					UniqueKey:    []byte("unique-key-" + strconv.Itoa(i)),
					UniqueStates: 0xff,
				}
			}

			resultRows, err := exec.JobInsertFastMany(ctx, insertParams)
			require.NoError(t, err)
			require.Len(t, resultRows, len(insertParams))

			for i, result := range resultRows {
				require.False(t, result.UniqueSkippedAsDuplicate)
				job := result.Job
				require.Equal(t, 0, job.Attempt)
				require.Nil(t, job.AttemptedAt)
				require.Empty(t, job.AttemptedBy)
				require.WithinDuration(t, now.Add(time.Duration(i)*5*time.Second), job.CreatedAt, time.Millisecond)
				require.JSONEq(t, `{"encoded": "args"}`, string(job.EncodedArgs))
				require.Empty(t, job.Errors)
				require.Nil(t, job.FinalizedAt)
				require.Equal(t, "test_kind", job.Kind)
				require.Equal(t, rivercommon.MaxAttemptsDefault, job.MaxAttempts)
				require.JSONEq(t, `{"meta": "data"}`, string(job.Metadata))
				require.Equal(t, rivercommon.PriorityDefault, job.Priority)
				require.Equal(t, rivercommon.QueueDefault, job.Queue)
				requireEqualTime(t, now.Add(time.Duration(i)*time.Minute), job.ScheduledAt)
				require.Equal(t, rivertype.JobStateAvailable, job.State)
				require.Equal(t, []string{"tag"}, job.Tags)
				require.Equal(t, []byte("unique-key-"+strconv.Itoa(i)), job.UniqueKey)
				require.Equal(t, rivertype.JobStates(), job.UniqueStates)
			}
		})

		t.Run("MissingValuesDefaultAsExpected", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			insertParams := make([]*riverdriver.JobInsertFastParams, 10)
			for i := 0; i < len(insertParams); i++ {
				insertParams[i] = &riverdriver.JobInsertFastParams{
					EncodedArgs:  []byte(`{"encoded": "args"}`),
					Kind:         "test_kind",
					MaxAttempts:  rivercommon.MaxAttemptsDefault,
					Metadata:     []byte(`{"meta": "data"}`),
					Priority:     rivercommon.PriorityDefault,
					Queue:        rivercommon.QueueDefault,
					ScheduledAt:  nil, // explicit nil
					State:        rivertype.JobStateAvailable,
					Tags:         []string{"tag"},
					UniqueKey:    nil,  // explicit nil
					UniqueStates: 0x00, // explicit 0
				}
			}

			results, err := exec.JobInsertFastMany(ctx, insertParams)
			require.NoError(t, err)
			require.Len(t, results, len(insertParams))

			jobsAfter, err := exec.JobGetByKindMany(ctx, []string{"test_kind"})
			require.NoError(t, err)
			require.Len(t, jobsAfter, len(insertParams))
			for _, job := range jobsAfter {
				require.WithinDuration(t, time.Now().UTC(), job.CreatedAt, 2*time.Second)
				require.WithinDuration(t, time.Now().UTC(), job.ScheduledAt, 2*time.Second)

				// UniqueKey and UniqueStates are not set in the insert params, so they should
				// be nil and an empty slice respectively.
				require.Nil(t, job.UniqueKey)
				var emptyJobStates []rivertype.JobState
				require.Equal(t, emptyJobStates, job.UniqueStates)
			}
		})

		t.Run("BinaryNonUTF8UniqueKey", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			uniqueKey := []byte{0x00, 0x01, 0x02}
			results, err := exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{{
				EncodedArgs:  []byte(`{"encoded": "args"}`),
				Kind:         "test_kind",
				MaxAttempts:  rivercommon.MaxAttemptsDefault,
				Metadata:     []byte(`{"meta": "data"}`),
				Priority:     rivercommon.PriorityDefault,
				Queue:        rivercommon.QueueDefault,
				ScheduledAt:  nil, // explicit nil
				State:        rivertype.JobStateAvailable,
				Tags:         []string{"tag"},
				UniqueKey:    uniqueKey,
				UniqueStates: 0xff,
			}})
			require.NoError(t, err)
			require.Len(t, results, 1)
			require.Equal(t, uniqueKey, results[0].Job.UniqueKey)

			jobs, err := exec.JobGetByKindMany(ctx, []string{"test_kind"})
			require.NoError(t, err)
			require.Equal(t, uniqueKey, jobs[0].UniqueKey)
		})
	})

	t.Run("JobInsertFastManyNoReturning", func(t *testing.T) {
		t.Parallel()

		t.Run("AllArgs", func(t *testing.T) {
			exec, _ := setup(ctx, t)

			// This test needs to use a time from before the transaction begins, otherwise
			// the newly-scheduled jobs won't yet show as available because their
			// scheduled_at (which gets a default value from time.Now() in code) will be
			// after the start of the transaction.
			now := time.Now().UTC().Add(-1 * time.Minute)

			insertParams := make([]*riverdriver.JobInsertFastParams, 10)
			for i := 0; i < len(insertParams); i++ {
				insertParams[i] = &riverdriver.JobInsertFastParams{
					CreatedAt:    ptrutil.Ptr(now.Add(time.Duration(i) * 5 * time.Second)),
					EncodedArgs:  []byte(`{"encoded": "args"}`),
					Kind:         "test_kind",
					MaxAttempts:  rivercommon.MaxAttemptsDefault,
					Metadata:     []byte(`{"meta": "data"}`),
					Priority:     rivercommon.PriorityDefault,
					Queue:        rivercommon.QueueDefault,
					ScheduledAt:  &now,
					State:        rivertype.JobStateAvailable,
					Tags:         []string{"tag"},
					UniqueKey:    []byte("unique-key-" + strconv.Itoa(i)),
					UniqueStates: 0xff,
				}
			}

			count, err := exec.JobInsertFastManyNoReturning(ctx, insertParams)
			require.NoError(t, err)
			require.Len(t, insertParams, count)

			jobsAfter, err := exec.JobGetByKindMany(ctx, []string{"test_kind"})
			require.NoError(t, err)
			require.Len(t, jobsAfter, len(insertParams))
			for i, job := range jobsAfter {
				require.Equal(t, 0, job.Attempt)
				require.Nil(t, job.AttemptedAt)
				require.WithinDuration(t, now.Add(time.Duration(i)*5*time.Second), job.CreatedAt, time.Millisecond)
				require.JSONEq(t, `{"encoded": "args"}`, string(job.EncodedArgs))
				require.Empty(t, job.Errors)
				require.Nil(t, job.FinalizedAt)
				require.Equal(t, "test_kind", job.Kind)
				require.Equal(t, rivercommon.MaxAttemptsDefault, job.MaxAttempts)
				require.JSONEq(t, `{"meta": "data"}`, string(job.Metadata))
				require.Equal(t, rivercommon.PriorityDefault, job.Priority)
				require.Equal(t, rivercommon.QueueDefault, job.Queue)
				requireEqualTime(t, now, job.ScheduledAt)
				require.Equal(t, rivertype.JobStateAvailable, job.State)
				require.Equal(t, []string{"tag"}, job.Tags)
			}
		})

		t.Run("MissingCreatedAtDefaultsToNow", func(t *testing.T) {
			exec, _ := setup(ctx, t)

			insertParams := make([]*riverdriver.JobInsertFastParams, 10)
			for i := 0; i < len(insertParams); i++ {
				insertParams[i] = &riverdriver.JobInsertFastParams{
					CreatedAt:   nil, // explicit nil
					EncodedArgs: []byte(`{"encoded": "args"}`),
					Kind:        "test_kind",
					MaxAttempts: rivercommon.MaxAttemptsDefault,
					Metadata:    []byte(`{"meta": "data"}`),
					Priority:    rivercommon.PriorityDefault,
					Queue:       rivercommon.QueueDefault,
					ScheduledAt: ptrutil.Ptr(time.Now().UTC()),
					State:       rivertype.JobStateAvailable,
					Tags:        []string{"tag"},
				}
			}

			count, err := exec.JobInsertFastManyNoReturning(ctx, insertParams)
			require.NoError(t, err)
			require.Len(t, insertParams, count)

			jobsAfter, err := exec.JobGetByKindMany(ctx, []string{"test_kind"})
			require.NoError(t, err)
			require.Len(t, jobsAfter, len(insertParams))
			for _, job := range jobsAfter {
				require.WithinDuration(t, time.Now().UTC(), job.CreatedAt, 2*time.Second)
			}
		})

		t.Run("MissingScheduledAtDefaultsToNow", func(t *testing.T) {
			exec, _ := setup(ctx, t)

			insertParams := make([]*riverdriver.JobInsertFastParams, 10)
			for i := 0; i < len(insertParams); i++ {
				insertParams[i] = &riverdriver.JobInsertFastParams{
					EncodedArgs: []byte(`{"encoded": "args"}`),
					Kind:        "test_kind",
					MaxAttempts: rivercommon.MaxAttemptsDefault,
					Metadata:    []byte(`{"meta": "data"}`),
					Priority:    rivercommon.PriorityDefault,
					Queue:       rivercommon.QueueDefault,
					ScheduledAt: nil, // explicit nil
					State:       rivertype.JobStateAvailable,
					Tags:        []string{"tag"},
				}
			}

			count, err := exec.JobInsertFastManyNoReturning(ctx, insertParams)
			require.NoError(t, err)
			require.Len(t, insertParams, count)

			jobsAfter, err := exec.JobGetByKindMany(ctx, []string{"test_kind"})
			require.NoError(t, err)
			require.Len(t, jobsAfter, len(insertParams))
			for _, job := range jobsAfter {
				require.WithinDuration(t, time.Now().UTC(), job.ScheduledAt, 2*time.Second)
			}
		})
	})

	t.Run("JobInsertFull", func(t *testing.T) {
		t.Parallel()

		t.Run("MinimalArgsWithDefaults", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			job, err := exec.JobInsertFull(ctx, &riverdriver.JobInsertFullParams{
				EncodedArgs: []byte(`{"encoded": "args"}`),
				Kind:        "test_kind",
				MaxAttempts: rivercommon.MaxAttemptsDefault,
				Priority:    rivercommon.PriorityDefault,
				Queue:       rivercommon.QueueDefault,
				State:       rivertype.JobStateAvailable,
			})
			require.NoError(t, err)
			require.Equal(t, 0, job.Attempt)
			require.Nil(t, job.AttemptedAt)
			require.WithinDuration(t, time.Now().UTC(), job.CreatedAt, 2*time.Second)
			require.JSONEq(t, `{"encoded": "args"}`, string(job.EncodedArgs))
			require.Empty(t, job.Errors)
			require.Nil(t, job.FinalizedAt)
			require.Equal(t, "test_kind", job.Kind)
			require.Equal(t, rivercommon.MaxAttemptsDefault, job.MaxAttempts)
			require.Equal(t, rivercommon.QueueDefault, job.Queue)
			require.Equal(t, rivertype.JobStateAvailable, job.State)
		})

		t.Run("AllArgs", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job, err := exec.JobInsertFull(ctx, &riverdriver.JobInsertFullParams{
				Attempt:     3,
				AttemptedAt: &now,
				AttemptedBy: []string{"worker1", "worker2"},
				CreatedAt:   &now,
				EncodedArgs: []byte(`{"encoded": "args"}`),
				Errors:      [][]byte{[]byte(`{"error": "message"}`)},
				FinalizedAt: &now,
				Kind:        "test_kind",
				MaxAttempts: 6,
				Metadata:    []byte(`{"meta": "data"}`),
				Priority:    2,
				Queue:       "queue_name",
				ScheduledAt: &now,
				State:       rivertype.JobStateCompleted,
				Tags:        []string{"tag"},
				UniqueKey:   []byte("unique-key"),
			})
			require.NoError(t, err)
			require.Equal(t, 3, job.Attempt)
			requireEqualTime(t, now, *job.AttemptedAt)
			require.Equal(t, []string{"worker1", "worker2"}, job.AttemptedBy)
			requireEqualTime(t, now, job.CreatedAt)
			require.JSONEq(t, `{"encoded": "args"}`, string(job.EncodedArgs))
			require.Equal(t, "message", job.Errors[0].Error)
			requireEqualTime(t, now, *job.FinalizedAt)
			require.Equal(t, "test_kind", job.Kind)
			require.Equal(t, 6, job.MaxAttempts)
			require.JSONEq(t, `{"meta": "data"}`, string(job.Metadata))
			require.Equal(t, 2, job.Priority)
			require.Equal(t, "queue_name", job.Queue)
			requireEqualTime(t, now, job.ScheduledAt)
			require.Equal(t, rivertype.JobStateCompleted, job.State)
			require.Equal(t, []string{"tag"}, job.Tags)
			require.Equal(t, []byte("unique-key"), job.UniqueKey)
		})

		t.Run("JobFinalizedAtConstraint", func(t *testing.T) {
			t.Parallel()

			capitalizeJobState := func(state rivertype.JobState) string {
				return cases.Title(language.English, cases.NoLower).String(string(state))
			}

			for _, state := range []rivertype.JobState{
				rivertype.JobStateCancelled,
				rivertype.JobStateCompleted,
				rivertype.JobStateDiscarded,
			} {
				t.Run(fmt.Sprintf("CannotSetState%sWithoutFinalizedAt", capitalizeJobState(state)), func(t *testing.T) {
					t.Parallel()

					exec, _ := setup(ctx, t)
					// Create a job with the target state but without a finalized_at,
					// expect an error:
					params := testfactory.Job_Build(t, &testfactory.JobOpts{
						State: &state,
					})
					params.FinalizedAt = nil
					_, err := exec.JobInsertFull(ctx, params)
					require.ErrorContains(t, err, "violates check constraint \"finalized_or_finalized_at_null\"")
				})

				t.Run(fmt.Sprintf("CanSetState%sWithFinalizedAt", capitalizeJobState(state)), func(t *testing.T) {
					t.Parallel()

					exec, _ := setup(ctx, t)

					// Create a job with the target state but with a finalized_at, expect
					// no error:
					_, err := exec.JobInsertFull(ctx, testfactory.Job_Build(t, &testfactory.JobOpts{
						FinalizedAt: ptrutil.Ptr(time.Now()),
						State:       &state,
					}))
					require.NoError(t, err)
				})
			}

			for _, state := range []rivertype.JobState{
				rivertype.JobStateAvailable,
				rivertype.JobStateRetryable,
				rivertype.JobStateRunning,
				rivertype.JobStateScheduled,
			} {
				t.Run(fmt.Sprintf("CanSetState%sWithoutFinalizedAt", capitalizeJobState(state)), func(t *testing.T) {
					t.Parallel()

					exec, _ := setup(ctx, t)

					// Create a job with the target state but without a finalized_at,
					// expect no error:
					_, err := exec.JobInsertFull(ctx, testfactory.Job_Build(t, &testfactory.JobOpts{
						State: &state,
					}))
					require.NoError(t, err)
				})

				t.Run(fmt.Sprintf("CannotSetState%sWithFinalizedAt", capitalizeJobState(state)), func(t *testing.T) {
					t.Parallel()

					exec, _ := setup(ctx, t)

					// Create a job with the target state but with a finalized_at, expect
					// an error:
					_, err := exec.JobInsertFull(ctx, testfactory.Job_Build(t, &testfactory.JobOpts{
						FinalizedAt: ptrutil.Ptr(time.Now()),
						State:       &state,
					}))
					require.ErrorContains(t, err, "violates check constraint \"finalized_or_finalized_at_null\"")
				})
			}
		})
	})

	t.Run("JobList", func(t *testing.T) {
		t.Parallel()

		t.Run("ListsJobs", func(t *testing.T) {
			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				Attempt:      ptrutil.Ptr(3),
				AttemptedAt:  &now,
				CreatedAt:    &now,
				EncodedArgs:  []byte(`{"encoded": "args"}`),
				Errors:       [][]byte{[]byte(`{"error": "message1"}`), []byte(`{"error": "message2"}`)},
				FinalizedAt:  &now,
				Metadata:     []byte(`{"meta": "data"}`),
				ScheduledAt:  &now,
				State:        ptrutil.Ptr(rivertype.JobStateCompleted),
				Tags:         []string{"tag"},
				UniqueKey:    []byte("unique-key"),
				UniqueStates: 0xFF,
			})

			// Does not match predicate (makes sure where clause is working).
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})

			fetchedJobs, err := exec.JobList(ctx, &riverdriver.JobListParams{
				Max:           100,
				NamedArgs:     map[string]any{"job_id_123": job.ID},
				OrderByClause: "id",
				WhereClause:   "id = @job_id_123",
			})
			require.NoError(t, err)
			require.Len(t, fetchedJobs, 1)

			fetchedJob := fetchedJobs[0]
			require.Equal(t, job.Attempt, fetchedJob.Attempt)
			require.Equal(t, job.AttemptedAt, fetchedJob.AttemptedAt)
			require.Equal(t, job.CreatedAt, fetchedJob.CreatedAt)
			require.Equal(t, job.EncodedArgs, fetchedJob.EncodedArgs)
			require.Equal(t, "message1", fetchedJob.Errors[0].Error)
			require.Equal(t, "message2", fetchedJob.Errors[1].Error)
			require.Equal(t, job.FinalizedAt, fetchedJob.FinalizedAt)
			require.Equal(t, job.Kind, fetchedJob.Kind)
			require.Equal(t, job.MaxAttempts, fetchedJob.MaxAttempts)
			require.Equal(t, job.Metadata, fetchedJob.Metadata)
			require.Equal(t, job.Priority, fetchedJob.Priority)
			require.Equal(t, job.Queue, fetchedJob.Queue)
			require.Equal(t, job.ScheduledAt, fetchedJob.ScheduledAt)
			require.Equal(t, job.State, fetchedJob.State)
			require.Equal(t, job.Tags, fetchedJob.Tags)
			require.Equal(t, []byte("unique-key"), fetchedJob.UniqueKey)
			require.Equal(t, rivertype.JobStates(), fetchedJob.UniqueStates)
		})

		t.Run("HandlesRequiredArgumentTypes", func(t *testing.T) {
			exec, _ := setup(ctx, t)

			job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("test_kind1")})
			job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{Kind: ptrutil.Ptr("test_kind2")})

			{
				fetchedJobs, err := exec.JobList(ctx, &riverdriver.JobListParams{
					Max:           100,
					NamedArgs:     map[string]any{"kind": job1.Kind},
					OrderByClause: "id",
					WhereClause:   "kind = @kind",
				})
				require.NoError(t, err)
				require.Len(t, fetchedJobs, 1)
			}

			{
				fetchedJobs, err := exec.JobList(ctx, &riverdriver.JobListParams{
					Max:           100,
					NamedArgs:     map[string]any{"kind": []string{job1.Kind, job2.Kind}},
					OrderByClause: "id",
					WhereClause:   "kind = any(@kind::text[])",
				})
				require.NoError(t, err)
				require.Len(t, fetchedJobs, 2)
			}
		})
	})

	t.Run("JobRescueMany", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		now := time.Now().UTC()

		job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

		_, err := exec.JobRescueMany(ctx, &riverdriver.JobRescueManyParams{
			ID: []int64{
				job1.ID,
				job2.ID,
			},
			Error: [][]byte{
				[]byte(`{"error": "message1"}`),
				[]byte(`{"error": "message2"}`),
			},
			FinalizedAt: []time.Time{
				{},
				now,
			},
			ScheduledAt: []time.Time{
				now,
				now,
			},
			State: []string{
				string(rivertype.JobStateAvailable),
				string(rivertype.JobStateDiscarded),
			},
		})
		require.NoError(t, err)

		updatedJob1, err := exec.JobGetByID(ctx, job1.ID)
		require.NoError(t, err)
		require.Equal(t, "message1", updatedJob1.Errors[0].Error)
		require.Nil(t, updatedJob1.FinalizedAt)
		requireEqualTime(t, now, updatedJob1.ScheduledAt)
		require.Equal(t, rivertype.JobStateAvailable, updatedJob1.State)

		updatedJob2, err := exec.JobGetByID(ctx, job2.ID)
		require.NoError(t, err)
		require.Equal(t, "message2", updatedJob2.Errors[0].Error)
		requireEqualTime(t, now, *updatedJob2.FinalizedAt)
		requireEqualTime(t, now, updatedJob2.ScheduledAt)
		require.Equal(t, rivertype.JobStateDiscarded, updatedJob2.State)
	})

	t.Run("JobRetry", func(t *testing.T) {
		t.Parallel()

		t.Run("DoesNotUpdateARunningJob", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State: ptrutil.Ptr(rivertype.JobStateRunning),
			})

			jobAfter, err := exec.JobRetry(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRunning, jobAfter.State)
			require.WithinDuration(t, job.ScheduledAt, jobAfter.ScheduledAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRunning, jobUpdated.State)
		})

		for _, state := range []rivertype.JobState{
			rivertype.JobStateAvailable,
			rivertype.JobStateCancelled,
			rivertype.JobStateCompleted,
			rivertype.JobStateDiscarded,
			rivertype.JobStatePending,
			rivertype.JobStateRetryable,
			rivertype.JobStateScheduled,
		} {
			t.Run(fmt.Sprintf("UpdatesA_%s_JobToBeScheduledImmediately", state), func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				now := time.Now().UTC()

				setFinalized := slices.Contains([]rivertype.JobState{
					rivertype.JobStateCancelled,
					rivertype.JobStateCompleted,
					rivertype.JobStateDiscarded,
				}, state)

				var finalizedAt *time.Time
				if setFinalized {
					finalizedAt = &now
				}

				job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
					FinalizedAt: finalizedAt,
					ScheduledAt: ptrutil.Ptr(now.Add(1 * time.Hour)),
					State:       &state,
				})

				jobAfter, err := exec.JobRetry(ctx, job.ID)
				require.NoError(t, err)
				require.Equal(t, rivertype.JobStateAvailable, jobAfter.State)
				require.WithinDuration(t, time.Now().UTC(), jobAfter.ScheduledAt, 100*time.Millisecond)

				jobUpdated, err := exec.JobGetByID(ctx, job.ID)
				require.NoError(t, err)
				require.Equal(t, rivertype.JobStateAvailable, jobUpdated.State)
				require.Nil(t, jobUpdated.FinalizedAt)
			})
		}

		t.Run("AltersScheduledAtForAlreadyCompletedJob", func(t *testing.T) {
			// A job which has already completed will have a ScheduledAt that could be
			// long in the past. Now that we're re-scheduling it, we should update that
			// to the current time to slot it in alongside other recently-scheduled jobs
			// and not skip the line; also, its wait duration can't be calculated
			// accurately if we don't reset the scheduled_at.
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				FinalizedAt: &now,
				ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour)),
				State:       ptrutil.Ptr(rivertype.JobStateCompleted),
			})

			jobAfter, err := exec.JobRetry(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, jobAfter.State)
			require.WithinDuration(t, now, jobAfter.ScheduledAt, 5*time.Second)
		})

		t.Run("DoesNotAlterScheduledAtIfInThePastAndJobAlreadyAvailable", func(t *testing.T) {
			// We don't want to update ScheduledAt if the job was already available
			// because doing so can make it lose its place in line.
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt: ptrutil.Ptr(now.Add(-1 * time.Hour)),
			})

			jobAfter, err := exec.JobRetry(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, jobAfter.State)
			require.WithinDuration(t, job.ScheduledAt, jobAfter.ScheduledAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, jobUpdated.State)
		})

		t.Run("ReturnsErrNotFoundIfJobNotFound", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			_, err := exec.JobRetry(ctx, 0)
			require.Error(t, err)
			require.ErrorIs(t, err, rivertype.ErrNotFound)
		})
	})

	t.Run("JobSchedule", func(t *testing.T) {
		t.Parallel()

		t.Run("BasicScheduling", func(t *testing.T) {
			exec, _ := setup(ctx, t)

			var (
				horizon       = time.Now()
				beforeHorizon = horizon.Add(-1 * time.Minute)
				afterHorizon  = horizon.Add(1 * time.Minute)
			)

			job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{ScheduledAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateRetryable)})
			job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{ScheduledAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateScheduled)})
			job3 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{ScheduledAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateScheduled)})

			// States that aren't scheduled.
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{ScheduledAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateAvailable)})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: &beforeHorizon, ScheduledAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateCompleted)})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{FinalizedAt: &beforeHorizon, ScheduledAt: &beforeHorizon, State: ptrutil.Ptr(rivertype.JobStateDiscarded)})

			// Right state, but after horizon.
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{ScheduledAt: &afterHorizon, State: ptrutil.Ptr(rivertype.JobStateRetryable)})
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{ScheduledAt: &afterHorizon, State: ptrutil.Ptr(rivertype.JobStateScheduled)})

			// First two scheduled because of limit.
			result, err := exec.JobSchedule(ctx, &riverdriver.JobScheduleParams{
				Max: 2,
				Now: horizon,
			})
			require.NoError(t, err)
			require.Len(t, result, 2)

			// And then job3 scheduled.
			result, err = exec.JobSchedule(ctx, &riverdriver.JobScheduleParams{
				Max: 2,
				Now: horizon,
			})
			require.NoError(t, err)
			require.Len(t, result, 1)

			updatedJob1, err := exec.JobGetByID(ctx, job1.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, updatedJob1.State)

			updatedJob2, err := exec.JobGetByID(ctx, job2.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, updatedJob2.State)

			updatedJob3, err := exec.JobGetByID(ctx, job3.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, updatedJob3.State)
		})

		t.Run("HandlesUniqueConflicts", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			var (
				horizon       = time.Now()
				beforeHorizon = horizon.Add(-1 * time.Minute)
			)

			defaultUniqueStates := []rivertype.JobState{
				rivertype.JobStateAvailable,
				rivertype.JobStatePending,
				rivertype.JobStateRetryable,
				rivertype.JobStateRunning,
				rivertype.JobStateScheduled,
			}
			// The default unique state list, minus retryable to allow for these conflicts:
			nonRetryableUniqueStates := []rivertype.JobState{
				rivertype.JobStateAvailable,
				rivertype.JobStatePending,
				rivertype.JobStateRunning,
				rivertype.JobStateScheduled,
			}

			job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt:  &beforeHorizon,
				State:        ptrutil.Ptr(rivertype.JobStateRetryable),
				UniqueKey:    []byte("unique-key-1"),
				UniqueStates: dbunique.UniqueStatesToBitmask(nonRetryableUniqueStates),
			})
			job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt:  &beforeHorizon,
				State:        ptrutil.Ptr(rivertype.JobStateRetryable),
				UniqueKey:    []byte("unique-key-2"),
				UniqueStates: dbunique.UniqueStatesToBitmask(nonRetryableUniqueStates),
			})
			// job3 has no conflict (it's the only one with this key), so it should be
			// scheduled.
			job3 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt:  &beforeHorizon,
				State:        ptrutil.Ptr(rivertype.JobStateRetryable),
				UniqueKey:    []byte("unique-key-3"),
				UniqueStates: dbunique.UniqueStatesToBitmask(defaultUniqueStates),
			})

			// This one is a conflict with job1 because it's already running and has
			// the same unique properties:
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt:  &beforeHorizon,
				State:        ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey:    []byte("unique-key-1"),
				UniqueStates: dbunique.UniqueStatesToBitmask(nonRetryableUniqueStates),
			})
			// This one is *not* a conflict with job2 because it's completed, which
			// isn't in the unique states:
			_ = testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt:  &beforeHorizon,
				State:        ptrutil.Ptr(rivertype.JobStateCompleted),
				UniqueKey:    []byte("unique-key-2"),
				UniqueStates: dbunique.UniqueStatesToBitmask(nonRetryableUniqueStates),
			})

			result, err := exec.JobSchedule(ctx, &riverdriver.JobScheduleParams{
				Max: 100,
				Now: horizon,
			})
			require.NoError(t, err)
			require.Len(t, result, 3)

			updatedJob1, err := exec.JobGetByID(ctx, job1.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateDiscarded, updatedJob1.State)
			require.Equal(t, "scheduler_discarded", gjson.GetBytes(updatedJob1.Metadata, "unique_key_conflict").String())

			updatedJob2, err := exec.JobGetByID(ctx, job2.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, updatedJob2.State)
			require.False(t, gjson.GetBytes(updatedJob2.Metadata, "unique_key_conflict").Exists())

			updatedJob3, err := exec.JobGetByID(ctx, job3.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, updatedJob3.State)
			require.False(t, gjson.GetBytes(updatedJob3.Metadata, "unique_key_conflict").Exists())
		})

		t.Run("SchedulingTwoRetryableJobsThatWillConflictWithEachOther", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			var (
				horizon       = time.Now()
				beforeHorizon = horizon.Add(-1 * time.Minute)
			)

			// The default unique state list, minus retryable to allow for these conflicts:
			nonRetryableUniqueStates := []rivertype.JobState{
				rivertype.JobStateAvailable,
				rivertype.JobStatePending,
				rivertype.JobStateRunning,
				rivertype.JobStateScheduled,
			}

			job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt:  &beforeHorizon,
				State:        ptrutil.Ptr(rivertype.JobStateRetryable),
				UniqueKey:    []byte("unique-key-1"),
				UniqueStates: dbunique.UniqueStatesToBitmask(nonRetryableUniqueStates),
			})
			job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				ScheduledAt:  &beforeHorizon,
				State:        ptrutil.Ptr(rivertype.JobStateRetryable),
				UniqueKey:    []byte("unique-key-1"),
				UniqueStates: dbunique.UniqueStatesToBitmask(nonRetryableUniqueStates),
			})

			result, err := exec.JobSchedule(ctx, &riverdriver.JobScheduleParams{
				Max: 100,
				Now: horizon,
			})
			require.NoError(t, err)
			require.Len(t, result, 2)

			updatedJob1, err := exec.JobGetByID(ctx, job1.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateAvailable, updatedJob1.State)
			require.False(t, gjson.GetBytes(updatedJob1.Metadata, "unique_key_conflict").Exists())

			updatedJob2, err := exec.JobGetByID(ctx, job2.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateDiscarded, updatedJob2.State)
			require.Equal(t, "scheduler_discarded", gjson.GetBytes(updatedJob2.Metadata, "unique_key_conflict").String())
		})
	})

	makeErrPayload := func(t *testing.T, now time.Time) []byte {
		t.Helper()

		errPayload, err := json.Marshal(rivertype.AttemptError{
			Attempt: 1, At: now, Error: "fake error", Trace: "foo.go:123\nbar.go:456",
		})
		require.NoError(t, err)
		return errPayload
	}

	setStateManyParams := func(params ...*riverdriver.JobSetStateIfRunningParams) *riverdriver.JobSetStateIfRunningManyParams {
		batchParams := &riverdriver.JobSetStateIfRunningManyParams{}
		for _, param := range params {
			var (
				attempt     *int
				errData     []byte
				finalizedAt *time.Time
				scheduledAt *time.Time
			)
			if param.Attempt != nil {
				attempt = param.Attempt
			}
			if param.ErrData != nil {
				errData = param.ErrData
			}
			if param.FinalizedAt != nil {
				finalizedAt = param.FinalizedAt
			}
			if param.ScheduledAt != nil {
				scheduledAt = param.ScheduledAt
			}

			batchParams.ID = append(batchParams.ID, param.ID)
			batchParams.Attempt = append(batchParams.Attempt, attempt)
			batchParams.ErrData = append(batchParams.ErrData, errData)
			batchParams.FinalizedAt = append(batchParams.FinalizedAt, finalizedAt)
			batchParams.MetadataDoMerge = append(batchParams.MetadataDoMerge, param.MetadataDoMerge)
			batchParams.MetadataUpdates = append(batchParams.MetadataUpdates, param.MetadataUpdates)
			batchParams.ScheduledAt = append(batchParams.ScheduledAt, scheduledAt)
			batchParams.State = append(batchParams.State, param.State)
		}

		return batchParams
	}

	t.Run("JobSetStateIfRunningMany_JobSetStateCompleted", func(t *testing.T) {
		t.Parallel()

		t.Run("CompletesARunningJob", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State:     ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey: []byte("unique-key"),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateCompleted(job.ID, now, nil)))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateCompleted, jobAfter.State)
			require.WithinDuration(t, now, *jobAfter.FinalizedAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateCompleted, jobUpdated.State)
			require.Equal(t, "unique-key", string(jobUpdated.UniqueKey))
		})

		t.Run("DoesNotCompleteARetryableJob", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State:     ptrutil.Ptr(rivertype.JobStateRetryable),
				UniqueKey: []byte("unique-key"),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateCompleted(job.ID, now, nil)))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateRetryable, jobAfter.State)
			require.Nil(t, jobAfter.FinalizedAt)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRetryable, jobUpdated.State)
			require.Equal(t, "unique-key", string(jobUpdated.UniqueKey))
		})

		t.Run("StoresMetadataUpdates", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				Metadata:  []byte(`{"foo":"baz", "something":"else"}`),
				State:     ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey: []byte("unique-key"),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateCompleted(job.ID, now, []byte(`{"a":"b", "foo":"bar"}`))))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateCompleted, jobAfter.State)
			require.JSONEq(t, `{"a":"b", "foo":"bar", "something":"else"}`, string(jobAfter.Metadata))
		})
	})

	t.Run("JobSetStateIfRunningMany_JobSetStateErrored", func(t *testing.T) {
		t.Parallel()

		t.Run("SetsARunningJobToRetryable", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State:     ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey: []byte("unique-key"),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateErrorRetryable(job.ID, now, makeErrPayload(t, now), nil)))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateRetryable, jobAfter.State)
			require.WithinDuration(t, now, jobAfter.ScheduledAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRetryable, jobUpdated.State)
			require.Equal(t, "unique-key", string(jobUpdated.UniqueKey))

			// validate error payload:
			require.Len(t, jobAfter.Errors, 1)
			require.Equal(t, now, jobAfter.Errors[0].At)
			require.Equal(t, 1, jobAfter.Errors[0].Attempt)
			require.Equal(t, "fake error", jobAfter.Errors[0].Error)
			require.Equal(t, "foo.go:123\nbar.go:456", jobAfter.Errors[0].Trace)
		})

		t.Run("DoesNotTouchAlreadyRetryableJobWithNoMetadataUpdates", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State:       ptrutil.Ptr(rivertype.JobStateRetryable),
				ScheduledAt: ptrutil.Ptr(now.Add(10 * time.Second)),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateErrorRetryable(job.ID, now, makeErrPayload(t, now), nil)))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateRetryable, jobAfter.State)
			require.WithinDuration(t, job.ScheduledAt, jobAfter.ScheduledAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRetryable, jobUpdated.State)
			require.WithinDuration(t, job.ScheduledAt, jobAfter.ScheduledAt, time.Microsecond)
		})

		t.Run("UpdatesOnlyMetadataForAlreadyRetryableJobs", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				Metadata:    []byte(`{"baz":"qux", "foo":"bar"}`),
				State:       ptrutil.Ptr(rivertype.JobStateRetryable),
				ScheduledAt: ptrutil.Ptr(now.Add(10 * time.Second)),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(
				riverdriver.JobSetStateErrorRetryable(job1.ID, now, makeErrPayload(t, now), []byte(`{"foo":"1", "output":{"a":"b"}}`)),
			))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateRetryable, jobAfter.State)
			require.JSONEq(t, `{"baz":"qux", "foo":"1", "output":{"a":"b"}}`, string(jobAfter.Metadata))
			require.Empty(t, jobAfter.Errors)
			require.Equal(t, job1.ScheduledAt, jobAfter.ScheduledAt)

			jobUpdated, err := exec.JobGetByID(ctx, job1.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRetryable, jobUpdated.State)
			require.JSONEq(t, `{"baz":"qux", "foo":"1", "output":{"a":"b"}}`, string(jobUpdated.Metadata))
			require.Empty(t, jobUpdated.Errors)
			require.Equal(t, job1.ScheduledAt, jobUpdated.ScheduledAt)
		})

		t.Run("SetsAJobWithCancelAttemptedAtToCancelled", func(t *testing.T) {
			// If a job has cancel_attempted_at in its metadata, it means that the user
			// tried to cancel the job with the Cancel API but that the job
			// finished/errored before the producer received the cancel notification.
			//
			// In this case, we want to move the job to cancelled instead of retryable
			// so that the job is not retried.
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				Metadata:    []byte(fmt.Sprintf(`{"cancel_attempted_at":"%s"}`, time.Now().UTC().Format(time.RFC3339))),
				State:       ptrutil.Ptr(rivertype.JobStateRunning),
				ScheduledAt: ptrutil.Ptr(now.Add(-10 * time.Second)),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateErrorRetryable(job.ID, now, makeErrPayload(t, now), nil)))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateCancelled, jobAfter.State)
			require.NotNil(t, jobAfter.FinalizedAt)
			// Loose assertion against FinalizedAt just to make sure it was set (it uses
			// the database's now() instead of a passed-in time):
			require.WithinDuration(t, time.Now().UTC(), *jobAfter.FinalizedAt, 2*time.Second)
			// ScheduledAt should not be touched:
			require.WithinDuration(t, job.ScheduledAt, jobAfter.ScheduledAt, time.Microsecond)

			// Errors should still be appended to:
			require.Len(t, jobAfter.Errors, 1)
			require.Contains(t, jobAfter.Errors[0].Error, "fake error")

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateCancelled, jobUpdated.State)
			require.WithinDuration(t, job.ScheduledAt, jobAfter.ScheduledAt, time.Microsecond)
		})
	})

	t.Run("JobSetStateIfRunningMany_JobSetStateCancelled", func(t *testing.T) {
		t.Parallel()

		t.Run("CancelsARunningJob", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State:        ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey:    []byte("unique-key"),
				UniqueStates: 0xFF,
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateCancelled(job.ID, now, makeErrPayload(t, now), nil)))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateCancelled, jobAfter.State)
			require.WithinDuration(t, now, *jobAfter.FinalizedAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateCancelled, jobUpdated.State)
			require.Equal(t, "unique-key", string(jobUpdated.UniqueKey))
		})
	})

	t.Run("JobSetStateIfRunningMany_JobSetStateDiscarded", func(t *testing.T) {
		t.Parallel()

		t.Run("DiscardsARunningJob", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				State:        ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey:    []byte("unique-key"),
				UniqueStates: 0xFF,
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateDiscarded(job.ID, now, makeErrPayload(t, now), nil)))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, rivertype.JobStateDiscarded, jobAfter.State)
			require.WithinDuration(t, now, *jobAfter.FinalizedAt, time.Microsecond)
			require.Equal(t, "unique-key", string(jobAfter.UniqueKey))
			require.Equal(t, rivertype.JobStates(), jobAfter.UniqueStates)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateDiscarded, jobUpdated.State)
		})
	})

	t.Run("JobSetStateIfRunningMany_JobSetStateSnoozed", func(t *testing.T) {
		t.Parallel()

		t.Run("SnoozesARunningJob_WithNoPreexistingMetadata", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()
			snoozeUntil := now.Add(1 * time.Minute)

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				Attempt:   ptrutil.Ptr(5),
				State:     ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey: []byte("unique-key"),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateSnoozed(job.ID, snoozeUntil, 4, []byte(`{"snoozes": 1}`))))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, 4, jobAfter.Attempt)
			require.Equal(t, job.MaxAttempts, jobAfter.MaxAttempts)
			require.JSONEq(t, `{"snoozes": 1}`, string(jobAfter.Metadata))
			require.Equal(t, rivertype.JobStateScheduled, jobAfter.State)
			require.WithinDuration(t, snoozeUntil, jobAfter.ScheduledAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, 4, jobUpdated.Attempt)
			require.Equal(t, job.MaxAttempts, jobUpdated.MaxAttempts)
			require.JSONEq(t, `{"snoozes": 1}`, string(jobUpdated.Metadata))
			require.Equal(t, rivertype.JobStateScheduled, jobUpdated.State)
			require.Equal(t, "unique-key", string(jobUpdated.UniqueKey))
		})

		t.Run("SnoozesARunningJob_WithPreexistingMetadata", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().UTC()
			snoozeUntil := now.Add(1 * time.Minute)

			job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{
				Attempt:   ptrutil.Ptr(5),
				State:     ptrutil.Ptr(rivertype.JobStateRunning),
				UniqueKey: []byte("unique-key"),
				Metadata:  []byte(`{"foo": "bar", "snoozes": 5}`),
			})

			jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(riverdriver.JobSetStateSnoozed(job.ID, snoozeUntil, 4, []byte(`{"snoozes": 6}`))))
			require.NoError(t, err)
			jobAfter := jobsAfter[0]
			require.Equal(t, 4, jobAfter.Attempt)
			require.Equal(t, job.MaxAttempts, jobAfter.MaxAttempts)
			require.JSONEq(t, `{"foo": "bar", "snoozes": 6}`, string(jobAfter.Metadata))
			require.Equal(t, rivertype.JobStateScheduled, jobAfter.State)
			require.WithinDuration(t, snoozeUntil, jobAfter.ScheduledAt, time.Microsecond)

			jobUpdated, err := exec.JobGetByID(ctx, job.ID)
			require.NoError(t, err)
			require.Equal(t, 4, jobUpdated.Attempt)
			require.Equal(t, job.MaxAttempts, jobUpdated.MaxAttempts)
			require.JSONEq(t, `{"foo": "bar", "snoozes": 6}`, string(jobUpdated.Metadata))
			require.Equal(t, rivertype.JobStateScheduled, jobUpdated.State)
			require.Equal(t, "unique-key", string(jobUpdated.UniqueKey))
		})
	})

	t.Run("JobSetStateIfRunningMany_MultipleJobsAtOnce", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		now := time.Now().UTC()
		future := now.Add(10 * time.Second)

		job1 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		job2 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})
		job3 := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRunning)})

		jobsAfter, err := exec.JobSetStateIfRunningMany(ctx, setStateManyParams(
			riverdriver.JobSetStateCompleted(job1.ID, now, []byte(`{"a":"b"}`)),
			riverdriver.JobSetStateErrorRetryable(job2.ID, future, makeErrPayload(t, now), nil),
			riverdriver.JobSetStateCancelled(job3.ID, now, makeErrPayload(t, now), nil),
		))
		require.NoError(t, err)
		completedJob := jobsAfter[0]
		require.Equal(t, rivertype.JobStateCompleted, completedJob.State)
		require.WithinDuration(t, now, *completedJob.FinalizedAt, time.Microsecond)
		require.JSONEq(t, `{"a":"b"}`, string(completedJob.Metadata))

		retryableJob := jobsAfter[1]
		require.Equal(t, rivertype.JobStateRetryable, retryableJob.State)
		require.WithinDuration(t, future, retryableJob.ScheduledAt, time.Microsecond)
		// validate error payload:
		require.Len(t, retryableJob.Errors, 1)
		require.Equal(t, now, retryableJob.Errors[0].At)
		require.Equal(t, 1, retryableJob.Errors[0].Attempt)
		require.Equal(t, "fake error", retryableJob.Errors[0].Error)
		require.Equal(t, "foo.go:123\nbar.go:456", retryableJob.Errors[0].Trace)

		cancelledJob := jobsAfter[2]
		require.Equal(t, rivertype.JobStateCancelled, cancelledJob.State)
		require.WithinDuration(t, now, *cancelledJob.FinalizedAt, time.Microsecond)
	})

	t.Run("JobUpdate", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		job := testfactory.Job(ctx, t, exec, &testfactory.JobOpts{})

		now := time.Now().UTC()

		updatedJob, err := exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{
			ID:                  job.ID,
			AttemptDoUpdate:     true,
			Attempt:             7,
			AttemptedAtDoUpdate: true,
			AttemptedAt:         &now,
			AttemptedByDoUpdate: true,
			AttemptedBy:         []string{"worker1"},
			ErrorsDoUpdate:      true,
			Errors:              [][]byte{[]byte(`{"error":"message"}`)},
			FinalizedAtDoUpdate: true,
			FinalizedAt:         &now,
			StateDoUpdate:       true,
			State:               rivertype.JobStateDiscarded,
		})
		require.NoError(t, err)
		require.Equal(t, 7, updatedJob.Attempt)
		requireEqualTime(t, now, *updatedJob.AttemptedAt)
		require.Equal(t, []string{"worker1"}, updatedJob.AttemptedBy)
		require.Equal(t, "message", updatedJob.Errors[0].Error)
		requireEqualTime(t, now, *updatedJob.FinalizedAt)
		require.Equal(t, rivertype.JobStateDiscarded, updatedJob.State)
	})

	const leaderTTL = 10 * time.Second

	t.Run("LeaderDeleteExpired", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		now := time.Now().UTC()

		{
			numDeleted, err := exec.LeaderDeleteExpired(ctx)
			require.NoError(t, err)
			require.Zero(t, numDeleted)
		}

		_ = testfactory.Leader(ctx, t, exec, &testfactory.LeaderOpts{
			ElectedAt: ptrutil.Ptr(now.Add(-2 * time.Hour)),
			ExpiresAt: ptrutil.Ptr(now.Add(-1 * time.Hour)),
			LeaderID:  ptrutil.Ptr(clientID),
		})

		{
			numDeleted, err := exec.LeaderDeleteExpired(ctx)
			require.NoError(t, err)
			require.Equal(t, 1, numDeleted)
		}
	})

	t.Run("LeaderAttemptElect", func(t *testing.T) {
		t.Parallel()

		t.Run("ElectsLeader", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			elected, err := exec.LeaderAttemptElect(ctx, &riverdriver.LeaderElectParams{
				LeaderID: clientID,
				TTL:      leaderTTL,
			})
			require.NoError(t, err)
			require.True(t, elected) // won election

			leader, err := exec.LeaderGetElectedLeader(ctx)
			require.NoError(t, err)
			require.WithinDuration(t, time.Now(), leader.ElectedAt, 100*time.Millisecond)
			require.WithinDuration(t, time.Now().Add(leaderTTL), leader.ExpiresAt, 100*time.Millisecond)
		})

		t.Run("CannotElectTwiceInARow", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			leader := testfactory.Leader(ctx, t, exec, &testfactory.LeaderOpts{
				LeaderID: ptrutil.Ptr(clientID),
			})

			elected, err := exec.LeaderAttemptElect(ctx, &riverdriver.LeaderElectParams{
				LeaderID: "different-client-id",
				TTL:      leaderTTL,
			})
			require.NoError(t, err)
			require.False(t, elected) // lost election

			// The time should not have changed because we specified that we were not
			// already elected, and the elect query is a no-op if there's already a
			// updatedLeader:
			updatedLeader, err := exec.LeaderGetElectedLeader(ctx)
			require.NoError(t, err)
			require.Equal(t, leader.ExpiresAt, updatedLeader.ExpiresAt)
		})
	})

	t.Run("LeaderAttemptReelect", func(t *testing.T) {
		t.Parallel()

		t.Run("ElectsLeader", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			elected, err := exec.LeaderAttemptReelect(ctx, &riverdriver.LeaderElectParams{
				LeaderID: clientID,
				TTL:      leaderTTL,
			})
			require.NoError(t, err)
			require.True(t, elected) // won election

			leader, err := exec.LeaderGetElectedLeader(ctx)
			require.NoError(t, err)
			require.WithinDuration(t, time.Now(), leader.ElectedAt, 100*time.Millisecond)
			require.WithinDuration(t, time.Now().Add(leaderTTL), leader.ExpiresAt, 100*time.Millisecond)
		})

		t.Run("ReelectsSameLeader", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			leader := testfactory.Leader(ctx, t, exec, &testfactory.LeaderOpts{
				LeaderID: ptrutil.Ptr(clientID),
			})

			// Re-elect the same leader. Use a larger TTL to see if time is updated,
			// because we are in a test transaction and the time is frozen at the start of
			// the transaction.
			elected, err := exec.LeaderAttemptReelect(ctx, &riverdriver.LeaderElectParams{
				LeaderID: clientID,
				TTL:      30 * time.Second,
			})
			require.NoError(t, err)
			require.True(t, elected) // won re-election

			// expires_at should be incremented because this is the same leader that won
			// previously and we specified that we're already elected:
			updatedLeader, err := exec.LeaderGetElectedLeader(ctx)
			require.NoError(t, err)
			require.Greater(t, updatedLeader.ExpiresAt, leader.ExpiresAt)
		})
	})

	t.Run("LeaderInsert", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		leader, err := exec.LeaderInsert(ctx, &riverdriver.LeaderInsertParams{
			LeaderID: clientID,
			TTL:      leaderTTL,
		})
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), leader.ElectedAt, 500*time.Millisecond)
		require.WithinDuration(t, time.Now().Add(leaderTTL), leader.ExpiresAt, 500*time.Millisecond)
		require.Equal(t, clientID, leader.LeaderID)
	})

	t.Run("LeaderGetElectedLeader", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		_ = testfactory.Leader(ctx, t, exec, &testfactory.LeaderOpts{
			LeaderID: ptrutil.Ptr(clientID),
		})

		leader, err := exec.LeaderGetElectedLeader(ctx)
		require.NoError(t, err)
		require.WithinDuration(t, time.Now(), leader.ElectedAt, 500*time.Millisecond)
		require.WithinDuration(t, time.Now().Add(leaderTTL), leader.ExpiresAt, 500*time.Millisecond)
		require.Equal(t, clientID, leader.LeaderID)
	})

	t.Run("LeaderResign", func(t *testing.T) {
		t.Parallel()

		t.Run("Success", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			{
				resigned, err := exec.LeaderResign(ctx, &riverdriver.LeaderResignParams{
					LeaderID:        clientID,
					LeadershipTopic: string(notifier.NotificationTopicLeadership),
				})
				require.NoError(t, err)
				require.False(t, resigned)
			}

			_ = testfactory.Leader(ctx, t, exec, &testfactory.LeaderOpts{
				LeaderID: ptrutil.Ptr(clientID),
			})

			{
				resigned, err := exec.LeaderResign(ctx, &riverdriver.LeaderResignParams{
					LeaderID:        clientID,
					LeadershipTopic: string(notifier.NotificationTopicLeadership),
				})
				require.NoError(t, err)
				require.True(t, resigned)
			}
		})

		t.Run("DoesNotResignWithoutLeadership", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			_ = testfactory.Leader(ctx, t, exec, &testfactory.LeaderOpts{
				LeaderID: ptrutil.Ptr("other-client-id"),
			})

			resigned, err := exec.LeaderResign(ctx, &riverdriver.LeaderResignParams{
				LeaderID:        clientID,
				LeadershipTopic: string(notifier.NotificationTopicLeadership),
			})
			require.NoError(t, err)
			require.False(t, resigned)
		})
	})

	// Truncates the migration table so we only have to work with test
	// migration data.
	truncateMigrations := func(ctx context.Context, t *testing.T, exec riverdriver.Executor) {
		t.Helper()

		_, err := exec.Exec(ctx, "TRUNCATE TABLE river_migration")
		require.NoError(t, err)
	}

	t.Run("MigrationDeleteAssumingMainMany", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		truncateMigrations(ctx, t, exec)

		migration1 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{})
		migration2 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{})

		// This query is designed to work before the `line` column was added to
		// the `river_migration` table. These tests will be operating on a fully
		// migrated database, so drop the column in this transaction to make
		// sure we are really checking that this operation works as expected.
		_, err := exec.Exec(ctx, "ALTER TABLE river_migration DROP COLUMN line")
		require.NoError(t, err)

		migrations, err := exec.MigrationDeleteAssumingMainMany(ctx, []int{
			migration1.Version,
			migration2.Version,
		})
		require.NoError(t, err)
		require.Len(t, migrations, 2)
		slices.SortFunc(migrations, func(a, b *riverdriver.Migration) int { return a.Version - b.Version })
		require.Equal(t, riverdriver.MigrationLineMain, migrations[0].Line)
		require.Equal(t, migration1.Version, migrations[0].Version)
		require.Equal(t, riverdriver.MigrationLineMain, migrations[1].Line)
		require.Equal(t, migration2.Version, migrations[1].Version)
	})

	t.Run("MigrationDeleteByLineAndVersionMany", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		truncateMigrations(ctx, t, exec)

		// not touched
		_ = testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{})

		migration1 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{Line: ptrutil.Ptr("alternate")})
		migration2 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{Line: ptrutil.Ptr("alternate")})

		migrations, err := exec.MigrationDeleteByLineAndVersionMany(ctx, "alternate", []int{
			migration1.Version,
			migration2.Version,
		})
		require.NoError(t, err)
		require.Len(t, migrations, 2)
		slices.SortFunc(migrations, func(a, b *riverdriver.Migration) int { return a.Version - b.Version })
		require.Equal(t, "alternate", migrations[0].Line)
		require.Equal(t, migration1.Version, migrations[0].Version)
		require.Equal(t, "alternate", migrations[1].Line)
		require.Equal(t, migration2.Version, migrations[1].Version)
	})

	t.Run("MigrationGetAllAssumingMain", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		truncateMigrations(ctx, t, exec)

		migration1 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{})
		migration2 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{})

		// This query is designed to work before the `line` column was added to
		// the `river_migration` table. These tests will be operating on a fully
		// migrated database, so drop the column in this transaction to make
		// sure we are really checking that this operation works as expected.
		_, err := exec.Exec(ctx, "ALTER TABLE river_migration DROP COLUMN line")
		require.NoError(t, err)

		migrations, err := exec.MigrationGetAllAssumingMain(ctx)
		require.NoError(t, err)
		require.Len(t, migrations, 2)
		require.Equal(t, migration1.Version, migrations[0].Version)
		require.Equal(t, migration2.Version, migrations[1].Version)

		// Check the full properties of one of the migrations.
		migration1Fetched := migrations[0]
		requireEqualTime(t, migration1.CreatedAt, migration1Fetched.CreatedAt)
		require.Equal(t, riverdriver.MigrationLineMain, migration1Fetched.Line)
		require.Equal(t, migration1.Version, migration1Fetched.Version)
	})

	t.Run("MigrationGetByLine", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		truncateMigrations(ctx, t, exec)

		// not returned
		_ = testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{})

		migration1 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{Line: ptrutil.Ptr("alternate")})
		migration2 := testfactory.Migration(ctx, t, exec, &testfactory.MigrationOpts{Line: ptrutil.Ptr("alternate")})

		migrations, err := exec.MigrationGetByLine(ctx, "alternate")
		require.NoError(t, err)
		require.Len(t, migrations, 2)
		require.Equal(t, migration1.Version, migrations[0].Version)
		require.Equal(t, migration2.Version, migrations[1].Version)

		// Check the full properties of one of the migrations.
		migration1Fetched := migrations[0]
		requireEqualTime(t, migration1.CreatedAt, migration1Fetched.CreatedAt)
		require.Equal(t, "alternate", migration1Fetched.Line)
		require.Equal(t, migration1.Version, migration1Fetched.Version)
	})

	t.Run("MigrationInsertMany", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		truncateMigrations(ctx, t, exec)

		migrations, err := exec.MigrationInsertMany(ctx, "alternate", []int{1, 2})
		require.NoError(t, err)
		require.Len(t, migrations, 2)
		require.Equal(t, "alternate", migrations[0].Line)
		require.Equal(t, 1, migrations[0].Version)
		require.Equal(t, "alternate", migrations[1].Line)
		require.Equal(t, 2, migrations[1].Version)
	})

	t.Run("MigrationInsertManyAssumingMain", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		truncateMigrations(ctx, t, exec)

		// This query is designed to work before the `line` column was added to
		// the `river_migration` table. These tests will be operating on a fully
		// migrated database, so drop the column in this transaction to make
		// sure we are really checking that this operation works as expected.
		_, err := exec.Exec(ctx, "ALTER TABLE river_migration DROP COLUMN line")
		require.NoError(t, err)

		migrations, err := exec.MigrationInsertManyAssumingMain(ctx, []int{1, 2})
		require.NoError(t, err)
		require.Len(t, migrations, 2)
		require.Equal(t, riverdriver.MigrationLineMain, migrations[0].Line)
		require.Equal(t, 1, migrations[0].Version)
		require.Equal(t, riverdriver.MigrationLineMain, migrations[1].Line)
		require.Equal(t, 2, migrations[1].Version)
	})

	t.Run("TableExists", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		exists, err := exec.TableExists(ctx, "river_job")
		require.NoError(t, err)
		require.True(t, exists)

		exists, err = exec.TableExists(ctx, "does_not_exist")
		require.NoError(t, err)
		require.False(t, exists)

		// Will be rolled back by the test transaction.
		_, err = exec.Exec(ctx, "CREATE SCHEMA another_schema_123")
		require.NoError(t, err)

		_, err = exec.Exec(ctx, "SET search_path = another_schema_123")
		require.NoError(t, err)

		exists, err = exec.TableExists(ctx, "river_job")
		require.NoError(t, err)
		require.False(t, exists)
	})

	t.Run("PGAdvisoryXactLock", func(t *testing.T) {
		t.Parallel()

		exec, _ := setup(ctx, t)

		// Acquire the advisory lock.
		_, err := exec.PGAdvisoryXactLock(ctx, 123456)
		require.NoError(t, err)

		// Open a new transaction and try to acquire the same lock, which should
		// block because the lock can't be acquired. Verify some amount of wait,
		// cancel the lock acquisition attempt, then verify return.
		{
			otherExec := executorWithTx(ctx, t)

			goroutineDone := make(chan struct{})

			ctx, cancel := context.WithCancel(ctx)
			t.Cleanup(cancel)

			go func() {
				defer close(goroutineDone)

				_, err := otherExec.PGAdvisoryXactLock(ctx, 123456)
				require.ErrorIs(t, err, context.Canceled)
			}()

			select {
			case <-goroutineDone:
				require.FailNow(t, "Unexpectedly acquired lock that should've held by other transaction")
			case <-time.After(50 * time.Millisecond):
			}

			cancel()

			select {
			case <-goroutineDone:
			case <-time.After(50 * time.Millisecond):
				require.FailNow(t, "Goroutine didn't finish in a timely manner")
			}
		}
	})

	t.Run("QueueCreateOrSetUpdatedAt", func(t *testing.T) {
		t.Run("InsertsANewQueueWithDefaultUpdatedAt", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			metadata := []byte(`{"foo": "bar"}`)
			queue, err := exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{
				Metadata: metadata,
				Name:     "new-queue",
			})
			require.NoError(t, err)
			require.WithinDuration(t, time.Now(), queue.CreatedAt, 500*time.Millisecond)
			require.Equal(t, metadata, queue.Metadata)
			require.Equal(t, "new-queue", queue.Name)
			require.Nil(t, queue.PausedAt)
			require.WithinDuration(t, time.Now(), queue.UpdatedAt, 500*time.Millisecond)
		})

		t.Run("InsertsANewQueueWithCustomPausedAt", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now().Add(-5 * time.Minute)
			queue, err := exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{
				Name:     "new-queue",
				PausedAt: ptrutil.Ptr(now),
			})
			require.NoError(t, err)
			require.Equal(t, "new-queue", queue.Name)
			require.WithinDuration(t, now, *queue.PausedAt, time.Millisecond)
		})

		t.Run("UpdatesTheUpdatedAtOfExistingQueue", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			metadata := []byte(`{"foo": "bar"}`)
			tBefore := time.Now().UTC()
			queueBefore, err := exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{
				Metadata:  metadata,
				Name:      "updatable-queue",
				UpdatedAt: &tBefore,
			})
			require.NoError(t, err)
			require.WithinDuration(t, tBefore, queueBefore.UpdatedAt, time.Millisecond)

			tAfter := tBefore.Add(2 * time.Second)
			queueAfter, err := exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{
				Metadata:  []byte(`{"other": "metadata"}`),
				Name:      "updatable-queue",
				UpdatedAt: &tAfter,
			})
			require.NoError(t, err)

			// unchanged:
			require.Equal(t, queueBefore.CreatedAt, queueAfter.CreatedAt)
			require.Equal(t, metadata, queueAfter.Metadata)
			require.Equal(t, "updatable-queue", queueAfter.Name)
			require.Nil(t, queueAfter.PausedAt)

			// Timestamp is bumped:
			require.WithinDuration(t, tAfter, queueAfter.UpdatedAt, time.Millisecond)
		})

		t.Run("QueueDeleteExpired", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			now := time.Now()
			_ = testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(now)})
			queue2 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(now.Add(-25 * time.Hour))})
			queue3 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(now.Add(-26 * time.Hour))})
			queue4 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(now.Add(-48 * time.Hour))})
			_ = testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{UpdatedAt: ptrutil.Ptr(now.Add(-23 * time.Hour))})

			horizon := now.Add(-24 * time.Hour)
			deletedQueueNames, err := exec.QueueDeleteExpired(ctx, &riverdriver.QueueDeleteExpiredParams{Max: 2, UpdatedAtHorizon: horizon})
			require.NoError(t, err)

			// queue2 and queue3 should be deleted, with queue4 being skipped due to max of 2:
			require.Equal(t, []string{queue2.Name, queue3.Name}, deletedQueueNames)

			// Try again, make sure queue4 gets deleted this time:
			deletedQueueNames, err = exec.QueueDeleteExpired(ctx, &riverdriver.QueueDeleteExpiredParams{Max: 2, UpdatedAtHorizon: horizon})
			require.NoError(t, err)

			require.Equal(t, []string{queue4.Name}, deletedQueueNames)
		})

		t.Run("QueueGet", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			queue := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{Metadata: []byte(`{"foo": "bar"}`)})

			queueFetched, err := exec.QueueGet(ctx, queue.Name)
			require.NoError(t, err)

			require.WithinDuration(t, queue.CreatedAt, queueFetched.CreatedAt, time.Millisecond)
			require.Equal(t, queue.Metadata, queueFetched.Metadata)
			require.Equal(t, queue.Name, queueFetched.Name)
			require.Nil(t, queueFetched.PausedAt)
			require.WithinDuration(t, queue.UpdatedAt, queueFetched.UpdatedAt, time.Millisecond)

			queueFetched, err = exec.QueueGet(ctx, "nonexistent-queue")
			require.ErrorIs(t, err, rivertype.ErrNotFound)
			require.Nil(t, queueFetched)
		})

		t.Run("QueueList", func(t *testing.T) {
			t.Parallel()

			exec, _ := setup(ctx, t)

			requireQueuesEqual := func(t *testing.T, target, actual *rivertype.Queue) {
				t.Helper()
				require.WithinDuration(t, target.CreatedAt, actual.CreatedAt, time.Millisecond)
				require.Equal(t, target.Metadata, actual.Metadata)
				require.Equal(t, target.Name, actual.Name)
				if target.PausedAt == nil {
					require.Nil(t, actual.PausedAt)
				} else {
					require.NotNil(t, actual.PausedAt)
					require.WithinDuration(t, *target.PausedAt, *actual.PausedAt, time.Millisecond)
				}
			}

			queues, err := exec.QueueList(ctx, 10)
			require.NoError(t, err)
			require.Empty(t, queues)

			// Make queue1, already paused:
			queue1 := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{Metadata: []byte(`{"foo": "bar"}`), PausedAt: ptrutil.Ptr(time.Now())})
			require.NoError(t, err)

			queue2 := testfactory.Queue(ctx, t, exec, nil)
			queue3 := testfactory.Queue(ctx, t, exec, nil)

			queues, err = exec.QueueList(ctx, 2)
			require.NoError(t, err)

			require.Len(t, queues, 2)
			requireQueuesEqual(t, queue1, queues[0])
			requireQueuesEqual(t, queue2, queues[1])

			queues, err = exec.QueueList(ctx, 3)
			require.NoError(t, err)

			require.Len(t, queues, 3)
			requireQueuesEqual(t, queue3, queues[2])
		})

		t.Run("QueuePause", func(t *testing.T) {
			t.Parallel()

			t.Run("ExistingPausedQueue", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				queue := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{
					PausedAt: ptrutil.Ptr(time.Now()),
				})

				require.NoError(t, exec.QueuePause(ctx, queue.Name))

				queueFetched, err := exec.QueueGet(ctx, queue.Name)
				require.NoError(t, err)
				require.NotNil(t, queueFetched.PausedAt)
				requireEqualTime(t, *queue.PausedAt, *queueFetched.PausedAt) // paused_at stays unchanged
				requireEqualTime(t, queue.UpdatedAt, queueFetched.UpdatedAt) // updated_at stays unchanged
			})

			t.Run("ExistingUnpausedQueue", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				queue := testfactory.Queue(ctx, t, exec, nil)
				require.Nil(t, queue.PausedAt)

				require.NoError(t, exec.QueuePause(ctx, queue.Name))

				queueFetched, err := exec.QueueGet(ctx, queue.Name)
				require.NoError(t, err)
				require.NotNil(t, queueFetched.PausedAt)
				require.WithinDuration(t, time.Now(), *(queueFetched.PausedAt), 500*time.Millisecond)
			})

			t.Run("NonExistentQueue", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				err := exec.QueuePause(ctx, "queue1")
				require.ErrorIs(t, err, rivertype.ErrNotFound)
			})

			t.Run("AllQueuesExistingQueues", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				queue1 := testfactory.Queue(ctx, t, exec, nil)
				require.Nil(t, queue1.PausedAt)
				queue2 := testfactory.Queue(ctx, t, exec, nil)
				require.Nil(t, queue2.PausedAt)

				require.NoError(t, exec.QueuePause(ctx, rivercommon.AllQueuesString))

				now := time.Now()

				queue1Fetched, err := exec.QueueGet(ctx, queue1.Name)
				require.NoError(t, err)
				require.NotNil(t, queue1Fetched.PausedAt)
				require.WithinDuration(t, now, *(queue1Fetched.PausedAt), 500*time.Millisecond)

				queue2Fetched, err := exec.QueueGet(ctx, queue2.Name)
				require.NoError(t, err)
				require.NotNil(t, queue2Fetched.PausedAt)
				require.WithinDuration(t, now, *(queue2Fetched.PausedAt), 500*time.Millisecond)
			})

			t.Run("AllQueuesNoQueues", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				require.NoError(t, exec.QueuePause(ctx, rivercommon.AllQueuesString))
			})
		})

		t.Run("QueueResume", func(t *testing.T) {
			t.Parallel()

			t.Run("ExistingPausedQueue", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				queue := testfactory.Queue(ctx, t, exec, &testfactory.QueueOpts{
					PausedAt: ptrutil.Ptr(time.Now()),
				})

				require.NoError(t, exec.QueueResume(ctx, queue.Name))

				queueFetched, err := exec.QueueGet(ctx, queue.Name)
				require.NoError(t, err)
				require.Nil(t, queueFetched.PausedAt)
			})

			t.Run("ExistingUnpausedQueue", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				queue := testfactory.Queue(ctx, t, exec, nil)

				require.NoError(t, exec.QueueResume(ctx, queue.Name))

				queueFetched, err := exec.QueueGet(ctx, queue.Name)
				require.NoError(t, err)
				require.Nil(t, queueFetched.PausedAt)
				requireEqualTime(t, queue.UpdatedAt, queueFetched.UpdatedAt) // updated_at stays unchanged
			})

			t.Run("NonExistentQueue", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				err := exec.QueueResume(ctx, "queue1")
				require.ErrorIs(t, err, rivertype.ErrNotFound)
			})

			t.Run("AllQueuesExistingQueues", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				queue1 := testfactory.Queue(ctx, t, exec, nil)
				require.Nil(t, queue1.PausedAt)
				queue2 := testfactory.Queue(ctx, t, exec, nil)
				require.Nil(t, queue2.PausedAt)

				require.NoError(t, exec.QueuePause(ctx, rivercommon.AllQueuesString))
				require.NoError(t, exec.QueueResume(ctx, rivercommon.AllQueuesString))

				queue1Fetched, err := exec.QueueGet(ctx, queue1.Name)
				require.NoError(t, err)
				require.Nil(t, queue1Fetched.PausedAt)

				queue2Fetched, err := exec.QueueGet(ctx, queue2.Name)
				require.NoError(t, err)
				require.Nil(t, queue2Fetched.PausedAt)
			})

			t.Run("AllQueuesNoQueues", func(t *testing.T) {
				t.Parallel()

				exec, _ := setup(ctx, t)

				require.NoError(t, exec.QueueResume(ctx, rivercommon.AllQueuesString))
			})
		})
	})
}

type testListenerBundle[TTx any] struct {
	driver riverdriver.Driver[TTx]
	exec   riverdriver.Executor
}

func setupListener[TTx any](ctx context.Context, t *testing.T, getDriverWithPool func(ctx context.Context, t *testing.T) riverdriver.Driver[TTx]) (riverdriver.Listener, *testListenerBundle[TTx]) {
	t.Helper()

	driver := getDriverWithPool(ctx, t)

	listener := driver.GetListener()

	return listener, &testListenerBundle[TTx]{
		driver: driver,
		exec:   driver.GetExecutor(),
	}
}

func exerciseListener[TTx any](ctx context.Context, t *testing.T, driverWithPool func(ctx context.Context, t *testing.T) riverdriver.Driver[TTx]) {
	t.Helper()

	connectListener := func(ctx context.Context, t *testing.T, listener riverdriver.Listener) {
		t.Helper()

		require.NoError(t, listener.Connect(ctx))
		t.Cleanup(func() { require.NoError(t, listener.Close(ctx)) })
	}

	requireNoNotification := func(ctx context.Context, t *testing.T, listener riverdriver.Listener) {
		t.Helper()

		// Ugh, this is a little sketchy, but hard to test in another way.
		ctx, cancel := context.WithTimeout(ctx, 50*time.Millisecond)
		defer cancel()

		notification, err := listener.WaitForNotification(ctx)
		require.ErrorIs(t, err, context.DeadlineExceeded, "Expected no notification, but got: %+v", notification)
	}

	waitForNotification := func(ctx context.Context, t *testing.T, listener riverdriver.Listener) *riverdriver.Notification {
		t.Helper()

		ctx, cancel := context.WithTimeout(ctx, 3*time.Second)
		defer cancel()

		notification, err := listener.WaitForNotification(ctx)
		require.NoError(t, err)

		return notification
	}

	t.Run("Close_NoOpIfNotConnected", func(t *testing.T) {
		t.Parallel()

		listener, _ := setupListener(ctx, t, driverWithPool)
		require.NoError(t, listener.Close(ctx))
	})

	t.Run("RoundTrip", func(t *testing.T) {
		t.Parallel()

		listener, bundle := setupListener(ctx, t, driverWithPool)

		connectListener(ctx, t, listener)

		require.NoError(t, listener.Listen(ctx, "topic1"))
		require.NoError(t, listener.Listen(ctx, "topic2"))

		require.NoError(t, listener.Ping(ctx)) // still alive

		{
			require.NoError(t, bundle.exec.NotifyMany(ctx, &riverdriver.NotifyManyParams{Topic: "topic1", Payload: []string{"payload1_1"}}))
			require.NoError(t, bundle.exec.NotifyMany(ctx, &riverdriver.NotifyManyParams{Topic: "topic2", Payload: []string{"payload2_1"}}))

			notification := waitForNotification(ctx, t, listener)
			require.Equal(t, &riverdriver.Notification{Topic: "topic1", Payload: "payload1_1"}, notification)
			notification = waitForNotification(ctx, t, listener)
			require.Equal(t, &riverdriver.Notification{Topic: "topic2", Payload: "payload2_1"}, notification)
		}

		require.NoError(t, listener.Unlisten(ctx, "topic2"))

		{
			require.NoError(t, bundle.exec.NotifyMany(ctx, &riverdriver.NotifyManyParams{Topic: "topic1", Payload: []string{"payload1_2"}}))
			require.NoError(t, bundle.exec.NotifyMany(ctx, &riverdriver.NotifyManyParams{Topic: "topic2", Payload: []string{"payload2_2"}}))

			notification := waitForNotification(ctx, t, listener)
			require.Equal(t, &riverdriver.Notification{Topic: "topic1", Payload: "payload1_2"}, notification)

			requireNoNotification(ctx, t, listener)
		}

		require.NoError(t, listener.Unlisten(ctx, "topic1"))

		require.NoError(t, listener.Close(ctx))
	})

	t.Run("TransactionGated", func(t *testing.T) {
		t.Parallel()

		listener, bundle := setupListener(ctx, t, driverWithPool)

		connectListener(ctx, t, listener)

		require.NoError(t, listener.Listen(ctx, "topic1"))

		tx, err := bundle.exec.Begin(ctx)
		require.NoError(t, err)

		require.NoError(t, tx.NotifyMany(ctx, &riverdriver.NotifyManyParams{Topic: "topic1", Payload: []string{"payload1"}}))

		// No notification because the transaction hasn't committed yet.
		requireNoNotification(ctx, t, listener)

		require.NoError(t, tx.Commit(ctx))

		// Notification received now that transaction has committed.
		notification := waitForNotification(ctx, t, listener)
		require.Equal(t, &riverdriver.Notification{Topic: "topic1", Payload: "payload1"}, notification)
	})

	t.Run("MultipleReuse", func(t *testing.T) {
		t.Parallel()

		listener, _ := setupListener(ctx, t, driverWithPool)

		connectListener(ctx, t, listener)

		require.NoError(t, listener.Listen(ctx, "topic1"))
		require.NoError(t, listener.Unlisten(ctx, "topic1"))

		require.NoError(t, listener.Close(ctx))
		require.NoError(t, listener.Connect(ctx))

		require.NoError(t, listener.Listen(ctx, "topic1"))
		require.NoError(t, listener.Unlisten(ctx, "topic1"))
	})
}

// requireEqualTime compares to timestamps down the microsecond only. This is
// appropriate for comparing times that might've roundtripped from Postgres,
// which only stores to microsecond precision.
func requireEqualTime(t *testing.T, expected, actual time.Time) {
	t.Helper()

	// Leaving off the nanosecond portion has the effect of truncating it rather
	// than rounding to the nearest microsecond, which functionally matches
	// pgx's behavior while persisting.
	const rfc3339Micro = "2006-01-02T15:04:05.999999Z07:00"

	require.Equal(t,
		expected.Format(rfc3339Micro),
		actual.Format(rfc3339Micro),
	)
}

```

`internal/riverinternaltest/riverinternaltest.go`:

```go
// Package riverinternaltest contains shared testing utilities for tests
// throughout the rest of the project.
package riverinternaltest

import (
	"context"
	"fmt"
	"log"
	"net/url"
	"os"
	"runtime"
	"sync"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/stretchr/testify/require"
	"go.uber.org/goleak"

	"github.com/riverqueue/river/internal/testdb"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/util/valutil"
)

// SchedulerShortInterval is an artificially short interval for the scheduler
// that's used in the tests of various components to make sure that errored jobs
// always end up in a `retryable` state rather than `available`. Normally, the
// job executor sets `available` if the retry delay is smaller than the
// scheduler's interval. To simplify things so errors are always `retryable`,
// this time is picked to be smaller than any retry delay that the default
// retry policy will ever produce. It's shared so we can document/explain it all
// in one place.
const SchedulerShortInterval = 500 * time.Millisecond

var (
	dbManager *testdb.Manager //nolint:gochecknoglobals

	// Maximum number of connections for the connection pool. This is the same
	// default that pgxpool uses (the larger of 4 or number of CPUs), but made a
	// variable here so that we can reference it from the test suite and not
	// rely on implicit knowledge of pgxpool implementation details that could
	// change in the future. If changing this value, also change the number of
	// databases to create in `testdbman`.
	dbPoolMaxConns = int32(max(4, runtime.NumCPU())) //nolint:gochecknoglobals
)

func DatabaseConfig(databaseName string) *pgxpool.Config {
	config, err := pgxpool.ParseConfig(DatabaseURL(databaseName))
	if err != nil {
		panic(fmt.Sprintf("error parsing database URL: %v", err))
	}
	config.MaxConns = dbPoolMaxConns
	// Use a short conn timeout here to attempt to quickly cancel attempts that
	// are unlikely to succeed even with more time:
	config.ConnConfig.ConnectTimeout = 2 * time.Second
	config.ConnConfig.RuntimeParams["timezone"] = "UTC"
	return config
}

// DatabaseURL gets a test database URL from TEST_DATABASE_URL or falls back on
// a default pointing to `river_test`. If databaseName is set, it replaces the
// database in the URL, although the host and other parameters are preserved.
//
// Most of the time DatabaseConfig should be used instead of this function, but
// it may be useful in non-pgx situations like for examples showing the use of
// `database/sql`.
func DatabaseURL(databaseName string) string {
	parsedURL, err := url.Parse(valutil.ValOrDefault(
		os.Getenv("TEST_DATABASE_URL"),
		"postgres://localhost/river_test?sslmode=disable"),
	)
	if err != nil {
		panic(err)
	}

	if databaseName != "" {
		parsedURL.Path = databaseName
	}

	return parsedURL.String()
}

// DiscardContinuously drains continuously out of the given channel and discards
// anything that comes out of it. Returns a stop function that should be invoked
// to stop draining. Stop must be invoked before tests finish to stop an
// internal goroutine.
func DiscardContinuously[T any](drainChan <-chan T) func() {
	var (
		stop     = make(chan struct{})
		stopped  = make(chan struct{})
		stopOnce sync.Once
	)

	go func() {
		defer close(stopped)

		for {
			select {
			case <-drainChan:
			case <-stop:
				return
			}
		}
	}()

	return func() {
		stopOnce.Do(func() {
			close(stop)
			<-stopped
		})
	}
}

// DrainContinuously drains continuously out of the given channel and
// accumulates items that are received from it. Returns a get function that can
// be called to retrieve the current set of received items, and which will also
// cause the function to shut down and stop draining. This function must be
// invoked before tests finish to stop an internal goroutine. It's safe to call
// it multiple times.
func DrainContinuously[T any](drainChan <-chan T) func() []T {
	var (
		items    []T
		stop     = make(chan struct{})
		stopped  = make(chan struct{})
		stopOnce sync.Once
	)

	go func() {
		defer close(stopped)

		for {
			select {
			case item := <-drainChan:
				items = append(items, item)
			case <-stop:
				// Drain until empty
				for {
					select {
					case item := <-drainChan:
						items = append(items, item)
					default:
						return
					}
				}
			}
		}
	}()

	return func() []T {
		stopOnce.Do(func() {
			close(stop)
			<-stopped
		})

		return items
	}
}

// TestDB acquires a dedicated test database for the duration of the test. If an
// error occurs, the test fails. The test database will be automatically
// returned to the pool at the end of the test. If the pool was closed, it will
// be recreated.
func TestDB(ctx context.Context, tb testing.TB) *pgxpool.Pool {
	tb.Helper()

	ctx, cancel := context.WithTimeout(ctx, riversharedtest.WaitTimeout())
	defer cancel()

	testPool, err := dbManager.Acquire(ctx)
	if err != nil {
		tb.Fatalf("Failed to acquire pool for test DB: %v", err)
	}
	tb.Cleanup(testPool.Release)

	return testPool.Pool()
}

// A pool and mutex to protect it, lazily initialized by TestTx. Once open, this
// pool is never explicitly closed, instead closing implicitly as the package
// tests finish.
var (
	dbPool   *pgxpool.Pool //nolint:gochecknoglobals
	dbPoolMu sync.RWMutex  //nolint:gochecknoglobals
)

// TestTx starts a test transaction that's rolled back automatically as the test
// case is cleaning itself up. This can be used as a lighter weight alternative
// to `testdb.Manager` in components where it's not necessary to have many
// connections open simultaneously.
func TestTx(ctx context.Context, tb testing.TB) pgx.Tx {
	tb.Helper()

	tryPool := func() *pgxpool.Pool {
		dbPoolMu.RLock()
		defer dbPoolMu.RUnlock()
		return dbPool
	}

	getPool := func() *pgxpool.Pool {
		if dbPool := tryPool(); dbPool != nil {
			return dbPool
		}

		dbPoolMu.Lock()
		defer dbPoolMu.Unlock()

		// Multiple goroutines may have passed the initial `nil` check on start
		// up, so check once more to make sure pool hasn't been set yet.
		if dbPool != nil {
			return dbPool
		}

		var err error
		dbPool, err = pgxpool.NewWithConfig(ctx, DatabaseConfig("river_test"))
		require.NoError(tb, err)

		return dbPool
	}

	return riversharedtest.TestTxPool(ctx, tb, getPool())
}

// TruncateRiverTables truncates River tables in the target database. This is
// for test cleanup and should obviously only be used in tests.
func TruncateRiverTables(ctx context.Context, pool *pgxpool.Pool) error {
	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	tables := []string{"river_job", "river_leader", "river_queue"}

	for _, table := range tables {
		if _, err := pool.Exec(ctx, fmt.Sprintf("TRUNCATE TABLE %s;", table)); err != nil {
			return fmt.Errorf("error truncating %q: %w", table, err)
		}
	}

	return nil
}

// WrapTestMain performs some common setup and teardown that should be shared
// amongst all packages. e.g. Configures a manager for test databases on setup,
// and checks for no goroutine leaks on teardown.
func WrapTestMain(m *testing.M) {
	poolConfig := DatabaseConfig("river_test")
	// Use a smaller number of conns per pool, because otherwise we could have
	// NUM_CPU pools, each with NUM_CPU connections, and that's a lot of
	// connections if there are many CPUs.
	poolConfig.MaxConns = 4
	// Pre-initialize 1 connection per pool.
	poolConfig.MinConns = 1

	var err error
	// Allow up to one database per concurrent test, plus two for overhead:
	maxTestDBs := int32(runtime.GOMAXPROCS(0)) + 2 //nolint:gosec
	dbManager, err = testdb.NewManager(poolConfig, maxTestDBs, nil, TruncateRiverTables)
	if err != nil {
		log.Fatal(err)
	}

	status := m.Run()

	dbManager.Close()

	if status == 0 {
		if err := goleak.Find(riversharedtest.IgnoredKnownGoroutineLeaks...); err != nil {
			fmt.Fprintf(os.Stderr, "goleak: Errors on successful test run: %v\n", err)
			status = 1
		}
	}

	os.Exit(status)
}

```

`internal/riverinternaltest/riverinternaltest_test.go`:

```go
package riverinternaltest

import (
	"context"
	"sync"
	"testing"

	"github.com/jackc/pgerrcode"
	"github.com/jackc/pgx/v5/pgconn"
	"github.com/stretchr/testify/require"
)

// Implemented by `pgx.Tx` or `pgxpool.Pool`. Normally we'd use a similar type
// from `dbsqlc` or `dbutil`, but riverinternaltest is extremely low level and
// that would introduce a cyclic dependency. We could package as
// `riverinternaltest_test`, except that the test below uses internal variables
// like `dbPool`.
type Executor interface {
	Exec(ctx context.Context, query string, args ...interface{}) (pgconn.CommandTag, error)
}

func TestTestTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	checkTestTable := func(executor Executor) error {
		_, err := executor.Exec(ctx, "SELECT * FROM test_tx_table")
		return err
	}

	// Test cleanups are invoked in the order of last added, first called. When
	// TestTx is called below it adds a cleanup, so we want to make sure that
	// this cleanup, which checks that the database remains pristine, is invoked
	// after the TestTx cleanup, so we add it first.
	t.Cleanup(func() {
		err := checkTestTable(dbPool)
		require.Error(t, err)

		var pgErr *pgconn.PgError
		require.ErrorAs(t, err, &pgErr)
		require.Equal(t, pgerrcode.UndefinedTable, pgErr.Code)
	})

	tx := TestTx(ctx, t)

	_, err := tx.Exec(ctx, "CREATE TABLE test_tx_table (id bigint)")
	require.NoError(t, err)

	err = checkTestTable(tx)
	require.NoError(t, err)
}

// Simulates a bunch of parallel processes starting a `TestTx` simultaneously.
// With the help of `go test -race`, should identify mutex/locking/parallel
// access problems if there are any.
//
// This test does NOT run in parallel on purpose because we want to be able to
// check access and set up on the `dbPool` global package variable which may be
// tainted if another test calls `TestTx` at the same time.
func TestTestTx_ConcurrentAccess(t *testing.T) { //nolint:paralleltest
	var (
		ctx = context.Background()
		wg  sync.WaitGroup
	)

	wg.Add(int(dbPoolMaxConns))

	// Before doing anything, zero out the pool because another test may have
	// initialized it already.
	dbPool = nil

	// Don't open more than maximum pool size transactions at once because that
	// would deadlock.
	for i := range dbPoolMaxConns {
		workerNum := i
		go func() {
			_ = TestTx(ctx, t)
			t.Logf("Opened transaction: %d", workerNum)
			wg.Done()
		}()
	}

	wg.Wait()
}

```

`internal/riverinternaltest/sharedtx/shared_tx.go`:

```go
package sharedtx

import (
	"context"
	"sync"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgconn"
)

// SharedTx can be used to wrap a test transaction in cases where multiple
// callers may want to access it concurrently during the course of a single test
// case. Normally this is not allowed and an access will error with "conn busy"
// if another caller is already using it.
//
// This is a test-only construct because in non-test environments an executor is
// a full connection pool which can support concurrent access without trouble.
// Many test cases use single test transactions, and that's where code written
// to use a connection pool can become problematic.
//
// SharedTx uses a channel for synchronization and does *not* guarantee FIFO
// ordering for callers.
//
// Avoid using SharedTx if possible because while it works, problems
// encountered by use of concurrent accesses will be more difficult to debug
// than otherwise, so it's better to not go there at all if it can be avoided.
type SharedTx struct {
	inner pgx.Tx
	wait  chan struct{}
}

func NewSharedTx(tx pgx.Tx) *SharedTx {
	wait := make(chan struct{}, 1)

	// Send one initial signal in so that the first caller is able to acquire a
	// lock.
	wait <- struct{}{}

	return &SharedTx{
		inner: tx,
		wait:  wait,
	}
}

func (e *SharedTx) Begin(ctx context.Context) (pgx.Tx, error) {
	e.lock()
	// no unlock until transaction commit/rollback

	tx, err := e.inner.Begin(ctx)
	if err != nil {
		e.unlock()
		return nil, err
	}

	// shared tx is unlocked when the transaction is committed or rolled back
	return &SharedSubTx{sharedTxDerivative{sharedTx: e}, tx}, nil
}

func (e *SharedTx) CopyFrom(ctx context.Context, tableName pgx.Identifier, columnNames []string, rowSrc pgx.CopyFromSource) (int64, error) {
	e.lock()
	defer e.unlock()

	return e.inner.CopyFrom(ctx, tableName, columnNames, rowSrc)
}

func (e *SharedTx) Exec(ctx context.Context, query string, args ...any) (pgconn.CommandTag, error) {
	e.lock()
	defer e.unlock()

	return e.inner.Exec(ctx, query, args...)
}

func (e *SharedTx) Query(ctx context.Context, query string, args ...any) (pgx.Rows, error) {
	e.lock()
	// no unlock until rows close or return on error condition

	rows, err := e.inner.Query(ctx, query, args...)
	if err != nil {
		e.unlock()
		return nil, err
	}

	// executor is unlocked when rows are closed
	return &SharedTxRows{sharedTxDerivative{sharedTx: e}, rows}, nil
}

func (e *SharedTx) QueryRow(ctx context.Context, query string, args ...any) pgx.Row {
	e.lock()
	// no unlock until row scan

	row := e.inner.QueryRow(ctx, query, args...)

	// executor is unlocked when row is scanned
	return &SharedTxRow{sharedTxDerivative{sharedTx: e}, row}
}

// These are all implemented so that a SharedTx can be used as a pgx.Tx, but are
// all non-functional.
func (e *SharedTx) Conn() *pgx.Conn                  { panic("not implemented") }
func (e *SharedTx) Commit(ctx context.Context) error { panic("not implemented") }
func (e *SharedTx) LargeObjects() pgx.LargeObjects   { panic("not implemented") }
func (e *SharedTx) Prepare(ctx context.Context, name, sql string) (*pgconn.StatementDescription, error) {
	panic("not implemented")
}
func (e *SharedTx) Rollback(ctx context.Context) error { panic("not implemented") }
func (e *SharedTx) SendBatch(ctx context.Context, b *pgx.Batch) pgx.BatchResults {
	panic("not implemented")
}

func (e *SharedTx) lock() {
	select {
	case <-e.wait:
		// success

	case <-time.After(5 * time.Second):
		panic("sharedtx: Timed out trying to acquire lock on SharedTx")
	}
}

func (e *SharedTx) unlock() {
	select {
	case e.wait <- struct{}{}:
	default:
		panic("wait channel was already unlocked which should not happen; BUG!")
	}
}

type sharedTxDerivative struct {
	once     sync.Once // used to guarantee executor only unlocked once
	sharedTx *SharedTx
}

func (d *sharedTxDerivative) unlockParent() {
	d.once.Do(func() {
		d.sharedTx.unlock()
	})
}

// SharedTxRow wraps a pgx.Row such that it unlocks SharedExecutor when
// the row finishes scanning.
type SharedTxRow struct {
	sharedTxDerivative
	innerRow pgx.Row
}

func (r *SharedTxRow) Scan(dest ...any) error {
	defer r.unlockParent()
	return r.innerRow.Scan(dest...)
}

// SharedTxRows wraps a pgx.Rows such that it unlocks SharedExecutor when
// the rows are closed.
type SharedTxRows struct {
	sharedTxDerivative
	innerRows pgx.Rows
}

func (r *SharedTxRows) Close() {
	defer r.unlockParent()
	r.innerRows.Close()
}

// All of these are simple pass throughs.
func (r *SharedTxRows) CommandTag() pgconn.CommandTag { return r.innerRows.CommandTag() }
func (r *SharedTxRows) Conn() *pgx.Conn               { return nil }
func (r *SharedTxRows) Err() error                    { return r.innerRows.Err() }
func (r *SharedTxRows) FieldDescriptions() []pgconn.FieldDescription {
	return r.innerRows.FieldDescriptions()
}
func (r *SharedTxRows) Next() bool                     { return r.innerRows.Next() }
func (r *SharedTxRows) RawValues() [][]byte            { return r.innerRows.RawValues() }
func (r *SharedTxRows) Scan(dest ...any) error         { return r.innerRows.Scan(dest...) }
func (r *SharedTxRows) Values() ([]interface{}, error) { return r.innerRows.Values() }

// SharedSubTx wraps a pgx.Tx such that it unlocks SharedTx when it commits or
// rolls back.
type SharedSubTx struct {
	sharedTxDerivative
	inner pgx.Tx
}

func (tx *SharedSubTx) Begin(ctx context.Context) (pgx.Tx, error) {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.Begin(ctx)
}

func (tx *SharedSubTx) Conn() *pgx.Conn {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.Conn()
}

func (tx *SharedSubTx) CopyFrom(ctx context.Context, tableName pgx.Identifier, columnNames []string, rowSrc pgx.CopyFromSource) (int64, error) {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.CopyFrom(ctx, tableName, columnNames, rowSrc)
}

func (tx *SharedSubTx) Exec(ctx context.Context, query string, args ...any) (pgconn.CommandTag, error) {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.Exec(ctx, query, args...)
}

func (tx *SharedSubTx) LargeObjects() pgx.LargeObjects {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.LargeObjects()
}

func (tx *SharedSubTx) Prepare(ctx context.Context, name, sql string) (*pgconn.StatementDescription, error) {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.Prepare(ctx, name, sql)
}

func (tx *SharedSubTx) Query(ctx context.Context, query string, args ...any) (pgx.Rows, error) {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.Query(ctx, query, args...)
}

func (tx *SharedSubTx) QueryRow(ctx context.Context, query string, args ...any) pgx.Row {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.QueryRow(ctx, query, args...)
}

func (tx *SharedSubTx) SendBatch(ctx context.Context, b *pgx.Batch) pgx.BatchResults {
	// pass through because we're already holding the lock at the executor level
	return tx.inner.SendBatch(ctx, b)
}

func (tx *SharedSubTx) Commit(ctx context.Context) error {
	defer tx.unlockParent()
	return tx.inner.Commit(ctx)
}

func (tx *SharedSubTx) Rollback(ctx context.Context) error {
	defer tx.unlockParent()
	return tx.inner.Rollback(ctx)
}

```

`internal/riverinternaltest/sharedtx/shared_tx_test.go`:

```go
package sharedtx

import (
	"context"
	"testing"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"
	"golang.org/x/sync/errgroup"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestSharedTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	setup := func(t *testing.T) *SharedTx {
		t.Helper()

		return NewSharedTx(riverinternaltest.TestTx(ctx, t))
	}

	t.Run("SharedTxFunctions", func(t *testing.T) {
		t.Parallel()

		sharedTx := setup(t)

		_, err := sharedTx.Exec(ctx, "SELECT 1")
		require.NoError(t, err)

		rows, err := sharedTx.Query(ctx, "SELECT 1")
		require.NoError(t, err)
		rows.Close()

		row := sharedTx.QueryRow(ctx, "SELECT 1")
		var i int
		err = row.Scan(&i)
		require.NoError(t, err)

		require.Len(t, sharedTx.wait, 1)
	})

	t.Run("SharedSubTxFunctions", func(t *testing.T) {
		t.Parallel()

		sharedTx := setup(t)

		sharedSubTx, err := sharedTx.Begin(ctx)
		require.NoError(t, err)

		_, err = sharedSubTx.Exec(ctx, "SELECT 1")
		require.NoError(t, err)

		rows, err := sharedSubTx.Query(ctx, "SELECT 1")
		require.NoError(t, err)
		rows.Close()

		row := sharedSubTx.QueryRow(ctx, "SELECT 1")
		var v int
		err = row.Scan(&v)
		require.NoError(t, err)
		require.Equal(t, 1, v)

		err = sharedSubTx.Commit(ctx)
		require.NoError(t, err)

		require.Len(t, sharedTx.wait, 1)
	})

	t.Run("TransactionCommitAndRollback", func(t *testing.T) {
		t.Parallel()

		sharedTx := setup(t)

		sharedSubTx, err := sharedTx.Begin(ctx)
		require.NoError(t, err)

		row := sharedSubTx.QueryRow(ctx, "SELECT 1")
		var v int
		err = row.Scan(&v)
		require.NoError(t, err)
		require.Equal(t, 1, v)

		err = sharedSubTx.Commit(ctx)
		require.NoError(t, err)

		// An additional rollback will return a tx closed error, but is safe.
		// The parent shared transaction will only be unlocked once.
		err = sharedSubTx.Rollback(ctx)
		require.ErrorIs(t, err, pgx.ErrTxClosed)

		require.Len(t, sharedTx.wait, 1)
	})

	t.Run("ConcurrentUse", func(t *testing.T) {
		t.Parallel()

		sharedTx := setup(t)

		const numIterations = 50
		errGroup, ctx := errgroup.WithContext(ctx)

		for range numIterations {
			errGroup.Go(func() error {
				sharedSubTx, err := sharedTx.Begin(ctx)
				require.NoError(t, err)

				row := sharedSubTx.QueryRow(ctx, "SELECT 1")
				var v int
				err = row.Scan(&v)
				require.NoError(t, err)
				require.Equal(t, 1, v)

				err = sharedSubTx.Commit(ctx)
				require.NoError(t, err)

				return nil
			})
		}

		for range numIterations {
			errGroup.Go(func() error {
				row := sharedTx.QueryRow(ctx, "SELECT 1")
				var v int
				err := row.Scan(&v)
				require.NoError(t, err)
				require.Equal(t, 1, v)

				return nil
			})
		}

		err := errGroup.Wait()
		require.NoError(t, err)

		require.Len(t, sharedTx.wait, 1)
	})

	// Checks specifically that the shared transaction is unlocked correctly on
	// the Query function's error path (normally it's unlocked when the returned
	// rows struct is closed, so an additional unlock operation is required).
	t.Run("QueryUnlocksOnError", func(t *testing.T) {
		t.Parallel()

		sharedTx := setup(t)

		{
			// Roll back the transaction so using it returns an error.
			require.NoError(t, sharedTx.inner.Rollback(ctx))

			_, err := sharedTx.Query(ctx, "SELECT 1") //nolint:sqlclosecheck
			require.ErrorIs(t, err, pgx.ErrTxClosed)

			select {
			case <-sharedTx.wait:
			default:
				require.FailNow(t, "Should have been a value in shared transaction's wait channel")
			}
		}
	})
}

```

`internal/testdb/db_with_pool.go`:

```go
package testdb

import (
	"context"
	"errors"
	"log/slog"
	"sync"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/jackc/puddle/v2"
)

// DBWithPool is a wrapper for a puddle resource for a test database. The
// database is made available via a preconfigured pgxpool.Pool.
type DBWithPool struct {
	res     *puddle.Resource[*poolWithDBName]
	manager *Manager
	dbName  string
	logger  *slog.Logger

	closeOnce sync.Once
}

// Release releases the DBWithPool back to the Manager. This should be called
// when the test is finished with the database.
func (db *DBWithPool) Release() {
	db.closeOnce.Do(db.release)
}

func (db *DBWithPool) release() {
	db.logger.Debug("DBWithPool: release called", "dbName", db.dbName)

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	if err := db.res.Value().pool.Ping(ctx); err != nil {
		// If the pgx pool is already closed, Ping returns puddle.ErrClosedPool.
		// When this happens, we need to re-create the pool.
		if errors.Is(err, puddle.ErrClosedPool) {
			db.logger.Debug("DBWithPool: pool is closed, re-creating", "dbName", db.dbName)

			if err := db.recreatePool(ctx); err != nil {
				db.res.Destroy()
				return
			}
		} else {
			// Log any other ping error but proceed with cleanup.
			db.logger.Debug("DBWithPool: pool ping returned error", "dbName", db.dbName, "err", err)
		}
	}

	if db.manager.cleanup != nil {
		db.logger.Debug("DBWithPool: release calling cleanup", "dbName", db.dbName)
		if err := db.manager.cleanup(ctx, db.res.Value().pool); err != nil {
			db.logger.Error("testdb.DBWithPool: Error during release cleanup", "err", err)

			if err := db.recreatePool(ctx); err != nil {
				db.res.Destroy()
				return
			}
		}
		db.logger.Debug("DBWithPool: release done with cleanup", "dbName", db.dbName)
	}

	// Finally this resource is ready to be reused:
	db.res.Release()
}

// Pool returns the underlying pgxpool.Pool for the test database.
func (db *DBWithPool) Pool() *pgxpool.Pool {
	return db.res.Value().pool
}

func (db *DBWithPool) recreatePool(ctx context.Context) error {
	db.logger.Debug("DBWithPool: recreatePool called", "dbName", db.dbName)
	db.Pool().Close()

	newPgxPool, err := pgxpool.NewWithConfig(ctx, db.res.Value().config)
	if err != nil {
		db.res.Destroy()
		db.logger.Error("DBWithPool: recreatePool failed", "dbName", db.dbName, "err", err)
		return err
	}
	db.logger.Debug("DBWithPool: recreatePool succeeded", "dbName", db.dbName)
	db.res.Value().pool = newPgxPool
	return nil
}

```

`internal/testdb/manager.go`:

```go
package testdb

import (
	"context"
	"fmt"
	"log/slog"
	"os"
	"sync"

	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/jackc/puddle/v2"
)

type PrepareFunc func(ctx context.Context, pool *pgxpool.Pool) error

type CleanupFunc func(ctx context.Context, pool *pgxpool.Pool) error

type poolWithDBName struct {
	pool *pgxpool.Pool

	// We will need to recreate the actual pool each time this DB is reused, so we
	// need the config for creating it:
	config *pgxpool.Config

	// dbName is needed to be able to drop the database in the destructor.
	dbName string
}

// Manager manages a pool of test databases up to a max size. Each DB keeps a
// pgxpool.Pool which is available when one is acquired from the Manager.
// Databases can optionally be prepared with a PrepareFunc before being added
// into the pool, and cleaned up with a CleanupFunc before being returned to the
// pool for reuse.
//
// This setup makes it trivial to run fully isolated tests in parallel.
type Manager struct {
	pud        *puddle.Pool[*poolWithDBName]
	baseConfig *pgxpool.Config
	cleanup    CleanupFunc
	logger     *slog.Logger
	prepare    PrepareFunc

	mu        sync.Mutex // protects nextDbNum
	nextDBNum int
}

// NewManager creates a new Manager with the given databaseURL, maxPoolSize, and
// optional prepare/cleanup funcs.
func NewManager(config *pgxpool.Config, maxPoolSize int32, prepare PrepareFunc, cleanup CleanupFunc) (*Manager, error) {
	manager := &Manager{
		baseConfig: config,
		cleanup:    cleanup,
		logger:     slog.New(slog.NewTextHandler(os.Stdout, nil)),
		prepare:    prepare,
	}

	pool, err := puddle.NewPool(&puddle.Config[*poolWithDBName]{
		Constructor: manager.allocatePool,
		Destructor:  manager.closePool,
		MaxSize:     maxPoolSize,
	})
	if err != nil {
		return nil, err
	}
	manager.pud = pool
	return manager, nil
}

// Acquire returns a DBWithPool which contains a pgxpool.Pool. The DBWithPool
// must be released after use.
func (m *Manager) Acquire(ctx context.Context) (*DBWithPool, error) {
	m.logger.Debug("DBManager: Acquire called")
	res, err := m.pud.Acquire(ctx)
	if err != nil {
		return nil, err
	}
	m.logger.Debug("DBManager: Acquire returned pool", "pool", res.Value().pool, "error", err, "dbName", res.Value().dbName)

	return &DBWithPool{res: res, logger: m.logger, manager: m, dbName: res.Value().dbName}, nil
}

// Close closes the Manager and all of its databases + pools. It blocks until
// all those underlying resources are unused and closed.
func (m *Manager) Close() {
	m.logger.Debug("DBManager: Close called")
	m.pud.Close()
	m.logger.Debug("DBManager: Close returned")
}

func (m *Manager) allocatePool(ctx context.Context) (*poolWithDBName, error) {
	nextDBNum := m.getNextDBNum()
	dbName := fmt.Sprintf("%s_%d", m.baseConfig.ConnConfig.Database, nextDBNum)

	m.logger.Debug("Using test database", "name", dbName)

	newPoolConfig := m.baseConfig.Copy()
	newPoolConfig.ConnConfig.Database = dbName

	pgxp, err := pgxpool.NewWithConfig(ctx, newPoolConfig)
	if err != nil {
		return nil, err
	}

	if m.cleanup != nil {
		m.logger.Debug("DBManager: allocatePool calling cleanup", "dbName", dbName)
		if err := m.cleanup(ctx, pgxp); err != nil {
			m.logger.Error("DBManager: error during allocatePool cleanup", "error", err)
			pgxp.Close()
			return nil, fmt.Errorf("error during cleanup: %w", err)
		}
		m.logger.Debug("DBManager: allocatePool cleanup returned", "dbName", dbName)
	}

	if m.prepare != nil {
		m.logger.Debug("DBManager: allocatePool calling prepare", "dbName", dbName)
		if err = m.prepare(ctx, pgxp); err != nil {
			pgxp.Close()
			return nil, fmt.Errorf("error during prepare: %w", err)
		}
		m.logger.Debug("DBManager: allocatePool prepare returned", "dbName", dbName)
	}

	return &poolWithDBName{
		config: newPoolConfig,
		dbName: dbName,
		pool:   pgxp,
	}, nil
}

func (m *Manager) closePool(pwn *poolWithDBName) {
	// Close the pool so that there are no active connections on the database:
	m.logger.Debug("DBManager: closePool called", "pool", pwn.pool, "dbName", pwn.dbName)
	pwn.pool.Close()
	m.logger.Debug("DBManager: closePool returned")
}

func (m *Manager) getNextDBNum() int {
	m.mu.Lock()
	defer m.mu.Unlock()

	nextNum := m.nextDBNum
	m.nextDBNum++
	return nextNum
}

```

`internal/testdb/manager_test.go`:

```go
package testdb_test

import (
	"context"
	"os"
	"strings"
	"testing"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river/internal/testdb"
)

func getTestDatabaseURL() string {
	if envURL := os.Getenv("TEST_DATABASE_URL"); envURL != "" {
		return envURL
	}
	return "postgres:///river_test?sslmode=disable"
}

func testConfig(t *testing.T) *pgxpool.Config {
	t.Helper()

	config, err := pgxpool.ParseConfig(getTestDatabaseURL())
	if err != nil {
		t.Fatal(err)
	}
	return config
}

func TestManager_AcquireMultiple(t *testing.T) {
	t.Parallel()

	manager, err := testdb.NewManager(testConfig(t), 10, nil, nil)
	if err != nil {
		t.Fatal(err)
	}
	defer manager.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	pool0, err := manager.Acquire(ctx)
	if err != nil {
		t.Fatal(err)
	}
	defer pool0.Release()

	checkDBNameForPool(ctx, t, pool0, "river_test_")

	pool1, err := manager.Acquire(ctx)
	if err != nil {
		t.Fatal(err)
	}
	defer pool1.Release()

	checkDBNameForPool(ctx, t, pool1, "river_test_")
	pool0.Release()

	//  ensure we get db 0 back on subsequent acquire since it was released to the pool:
	pool0Again, err := manager.Acquire(ctx)
	if err != nil {
		t.Fatal(err)
	}
	defer pool0Again.Release()

	checkDBNameForPool(ctx, t, pool0Again, "river_test_")
	pool0Again.Release()
	pool1.Release()

	manager.Close()
}

func TestManager_ReleaseTwice(t *testing.T) {
	t.Parallel()

	manager, err := testdb.NewManager(testConfig(t), 10, nil, nil)
	if err != nil {
		t.Fatal(err)
	}
	defer manager.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	tdb0, err := manager.Acquire(ctx)
	if err != nil {
		t.Fatal(err)
	}
	defer tdb0.Release()

	selectOne(ctx, t, tdb0)

	// explicitly close p0's pgxpool.Pool before Release to ensure it can be fully
	// reused after release:
	tdb0.Pool().Close()
	t.Log("RELEASING P0")
	// tdb0.Release()
	// Calling release twice should be a no-op:
	t.Log("RELEASING P0 AGAIN")
	tdb0.Release()

	//  ensure we get db 0 back on subsequent acquire since it was released to the pool:
	t.Log("REACQUIRING P0")
	tdb1, err := manager.Acquire(ctx)
	if err != nil {
		t.Fatal(err)
	}
	defer tdb1.Release()

	selectOne(ctx, t, tdb1)
	tdb1.Release()

	t.Log("CALLING MANAGER CLOSE MANUALLY")
	manager.Close()
}

func checkDBNameForPool(ctx context.Context, t *testing.T, p *testdb.DBWithPool, expectedPrefix string) {
	t.Helper()

	var currentDBName string
	if err := p.Pool().QueryRow(ctx, "SELECT current_database()").Scan(&currentDBName); err != nil {
		t.Fatal(err)
	}

	if !strings.HasPrefix(currentDBName, expectedPrefix) {
		t.Errorf("expected database name to begin with %q, got %q", expectedPrefix, currentDBName)
	}
}

func selectOne(ctx context.Context, t *testing.T, p *testdb.DBWithPool) {
	t.Helper()

	var one int
	if err := p.Pool().QueryRow(ctx, "SELECT 1").Scan(&one); err != nil {
		t.Fatal(err)
	}

	if one != 1 {
		t.Errorf("expected 1, got %d", one)
	}
}

```

`internal/util/chanutil/debounced_chan.go`:

```go
package chanutil

import (
	"context"
	"sync"
	"time"
)

// DebouncedChan is a channel that will only fire once per cooldown period, at
// the leading edge. If it is called again during the cooldown, the subsequent
// calls are delayed until the cooldown period has elapsed and are also
// coalesced into a single call.
type DebouncedChan struct {
	c           chan struct{}
	cooldown    time.Duration
	ctxDone     <-chan struct{}
	sendLeading bool

	// mu protects variables in group below
	mu                 sync.Mutex
	sendOnTimerExpired bool
	timer              *time.Timer
	timerDone          bool
}

// NewDebouncedChan returns a new DebouncedChan which sends on the channel no
// more often than the cooldown period.
//
// If sendLeading is true, the channel will signal once on C the first time it
// receives a signal, then again once per cooldown period. If sendLeading is
// false, the initial signal isn't sent.
func NewDebouncedChan(ctx context.Context, cooldown time.Duration, sendLeading bool) *DebouncedChan {
	return &DebouncedChan{
		ctxDone:     ctx.Done(),
		c:           make(chan struct{}, 1),
		cooldown:    cooldown,
		sendLeading: sendLeading,
	}
}

// C is the debounced channel. Multiple invocations to Call during the cooldown
// period will deduplicate to a single emission on this channel on the period's
// leading edge (if sendLeading was enabled), and one more on the trailing edge
// for as many periods as invocations continue to come in.
func (d *DebouncedChan) C() <-chan struct{} {
	return d.c
}

// Call invokes the debounced channel, and is the call which will be debounced.
// If multiple invocations of this function are made during the cooldown period,
// they'll be debounced to a single emission on C on the period's leading edge
// (if sendLeading is enabled), and then one fire on the trailing edge of each
// period for as long as Call continues to be invoked. If a timer period elapses
// without an invocation on Call, the timer is stopped and behavior resets the
// next time Call is invoked again.
func (d *DebouncedChan) Call() {
	d.mu.Lock()
	defer d.mu.Unlock()

	// A timer has already been initialized and hasn't already expired. (If it
	// has expired, we'll reset it below.) Set to signal when it does expire.
	if d.timer != nil && !d.timerDone {
		d.sendOnTimerExpired = true
		return
	}

	// No timer had been started yet, or the last one running was expired and
	// will be reset. Send immediately (i.e. n the leading edge of the
	// debounce period), if sendLeading is enabled.
	if d.sendLeading {
		d.nonBlockingSendOnC()
	} else {
		d.sendOnTimerExpired = true
	}

	// Next, start the timer, during which we'll monitor for additional calls,
	// and send at the end of the period if any came in. Create a new timer if
	// this is the first run. Otherwise, reset an existing one.
	if d.timer == nil {
		d.timer = time.NewTimer(d.cooldown)
	} else {
		d.timer.Reset(d.cooldown)
	}
	d.timerDone = false

	go d.waitForTimerLoop()
}

func (d *DebouncedChan) nonBlockingSendOnC() {
	select {
	case d.c <- struct{}{}:
	default:
	}
}

// Waits for the timer to be fired, and loops as long as Call invocations come
// in. If a period elapses without a new Call coming in, the loop returns, and
// DebouncedChan returns to its initial state, waiting for a new Call.
//
// The loop also stops if context becomes done.
func (d *DebouncedChan) waitForTimerLoop() {
	for {
		if stopLoop := d.waitForTimerOnce(); stopLoop {
			break
		}
	}
}

// Waits for the timer to fire once or context becomes done. Returns true if the
// caller should stop looping (i.e. don't wait on the timer again), and false
// otherwise.
func (d *DebouncedChan) waitForTimerOnce() bool {
	select {
	case <-d.ctxDone:
		d.mu.Lock()
		defer d.mu.Unlock()

		if d.timer != nil {
			if !d.timer.Stop() {
				<-d.timer.C
			}
		}

		d.timerDone = true

	case <-d.timer.C:
		d.mu.Lock()
		defer d.mu.Unlock()

		if d.sendOnTimerExpired {
			d.sendOnTimerExpired = false
			d.nonBlockingSendOnC()

			// Wait for another timer expiry, which will fire again if another
			// Call comes in during that time. If no Call comes in, the timer
			// will stop on the next cycle and we return to initial state.
			d.timer.Reset(d.cooldown)
			return false // do _not_ stop looping
		}

		d.timerDone = true
	}

	return true // stop looping
}

```

`internal/util/chanutil/debounced_chan_test.go`:

```go
package chanutil

import (
	"context"
	"math"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
)

func TestDebouncedChan_TriggersImmediately(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	debouncedChan := NewDebouncedChan(ctx, 200*time.Millisecond, true)
	go debouncedChan.Call()

	select {
	case <-debouncedChan.C():
	case <-time.After(50 * time.Millisecond):
		t.Fatal("timed out waiting for debounced chan to trigger")
	}

	// shouldn't trigger immediately again
	go debouncedChan.Call()
	select {
	case <-debouncedChan.C():
		t.Fatal("received from debounced chan unexpectedly")
	case <-time.After(50 * time.Millisecond):
	}

	var wg sync.WaitGroup
	wg.Add(5)
	for range 5 {
		go func() {
			debouncedChan.Call()
			wg.Done()
		}()
	}
	wg.Wait()

	// should trigger again after debounce period
	select {
	case <-debouncedChan.C():
	case <-time.After(250 * time.Millisecond):
		t.Fatal("timed out waiting for debounced chan to trigger")
	}

	// shouldn't trigger immediately again
	select {
	case <-debouncedChan.C():
		t.Fatal("received from debounced chan unexpectedly")
	case <-time.After(50 * time.Millisecond):
	}
}

func TestDebouncedChan_OnlyBuffersOneEvent(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	debouncedChan := NewDebouncedChan(ctx, 100*time.Millisecond, true)
	debouncedChan.Call()
	time.Sleep(150 * time.Millisecond)
	debouncedChan.Call()

	select {
	case <-debouncedChan.C():
	case <-time.After(20 * time.Millisecond):
		t.Fatal("timed out waiting for debounced chan to trigger")
	}

	// shouldn't trigger immediately again
	select {
	case <-debouncedChan.C():
		t.Fatal("received from debounced chan unexpectedly")
	case <-time.After(20 * time.Millisecond):
	}
}

func TestDebouncedChan_SendLeadingDisabled(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	debouncedChan := NewDebouncedChan(ctx, 100*time.Millisecond, false)
	debouncedChan.Call()

	// Expect nothing right away because sendLeading is disabled.
	select {
	case <-debouncedChan.C():
		t.Fatal("received from debounced chan unexpectedly")
	case <-time.After(20 * time.Millisecond):
	}

	time.Sleep(100 * time.Millisecond)

	select {
	case <-debouncedChan.C():
	case <-time.After(20 * time.Millisecond):
		t.Fatal("timed out waiting for debounced chan to trigger")
	}
}

func TestDebouncedChan_ContinuousOperation(t *testing.T) {
	t.Parallel()

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	const (
		cooldown  = 17 * time.Millisecond
		increment = 1 * time.Millisecond
		testTime  = 150 * time.Millisecond
	)

	var (
		debouncedChan = NewDebouncedChan(ctx, cooldown, true)
		goroutineDone = make(chan struct{})
		numSignals    int
	)

	go func() {
		defer close(goroutineDone)
		for {
			select {
			case <-ctx.Done():
				return
			case <-debouncedChan.C():
				numSignals++
			}
		}
	}()

	for tm := increment; tm <= testTime; tm += increment {
		time.Sleep(increment)
		debouncedChan.Call()
	}

	cancel()

	select {
	case <-goroutineDone:
	case <-time.After(3 * time.Second):
		require.FailNow(t, "Timed out waiting for goroutine to finish")
	}

	// Expect number of signals equal to number of cooldown periods that fit
	// into our total test time, and +1 for an initial fire.
	//
	// This almost always lands right on the expected number, but allow a delta
	// of +/-2 to allow the channel to be off by two cycles in either direction.
	// By running at `-count 1000` I can usually reproduce an off-by-one-or-two
	// cycle.
	expectedNumSignal := int(math.Round(float64(testTime)/float64(cooldown))) + 1
	t.Logf("Expected: %d, actual: %d", expectedNumSignal, numSignals)
	require.InDelta(t, expectedNumSignal, numSignals, 2)
}

```

`internal/util/chanutil/main_test.go`:

```go
package chanutil

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`internal/util/dbutil/db_util.go`:

```go
package dbutil

import (
	"context"
	"fmt"

	"github.com/riverqueue/river/riverdriver"
)

// WithTx starts and commits a transaction on a driver executor around
// the given function, allowing the return of a generic value.
func WithTx(ctx context.Context, exec riverdriver.Executor, innerFunc func(ctx context.Context, exec riverdriver.ExecutorTx) error) error {
	_, err := WithTxV(ctx, exec, func(ctx context.Context, tx riverdriver.ExecutorTx) (struct{}, error) {
		return struct{}{}, innerFunc(ctx, tx)
	})
	return err
}

// WithTxV starts and commits a transaction on a driver executor around
// the given function, allowing the return of a generic value.
func WithTxV[T any](ctx context.Context, exec riverdriver.Executor, innerFunc func(ctx context.Context, exec riverdriver.ExecutorTx) (T, error)) (T, error) {
	var defaultRes T

	tx, err := exec.Begin(ctx)
	if err != nil {
		return defaultRes, fmt.Errorf("error beginning transaction: %w", err)
	}
	defer tx.Rollback(ctx)

	res, err := innerFunc(ctx, tx)
	if err != nil {
		return defaultRes, err
	}

	if err := tx.Commit(ctx); err != nil {
		return defaultRes, fmt.Errorf("error committing transaction: %w", err)
	}

	return res, nil
}

```

`internal/util/dbutil/db_util_test.go`:

```go
package dbutil

import (
	"context"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
)

func TestWithTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	dbPool := riverinternaltest.TestDB(ctx, t)
	driver := riverpgxv5.New(dbPool)

	err := WithTx(ctx, driver.GetExecutor(), func(ctx context.Context, exec riverdriver.ExecutorTx) error {
		_, err := exec.Exec(ctx, "SELECT 1")
		require.NoError(t, err)

		return nil
	})
	require.NoError(t, err)
}

func TestWithTxV(t *testing.T) {
	t.Parallel()

	ctx := context.Background()
	dbPool := riverinternaltest.TestDB(ctx, t)
	driver := riverpgxv5.New(dbPool)

	ret, err := WithTxV(ctx, driver.GetExecutor(), func(ctx context.Context, exec riverdriver.ExecutorTx) (int, error) {
		_, err := exec.Exec(ctx, "SELECT 1")
		require.NoError(t, err)

		return 7, nil
	})
	require.NoError(t, err)
	require.Equal(t, 7, ret)
}

```

`internal/util/dbutil/main_test.go`:

```go
package dbutil

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`internal/util/hashutil/hash_util.go`:

```go
package hashutil

import (
	"hash"
	"hash/fnv"
)

// AdvisoryLockHash's job is to produce a pair of keys that'll be used with
// Postgres advisory lock functions like `pg_advisory_xact_lock`.
//
// Advisory locks all share a single 64-bit number space, so it's technically
// possible for completely unrelated locks to collide with each other. It's
// quite unlikely, but this type allows a user to configure a specific 32-bit
// prefix which they can use to guarantee that no River hash will ever conflict
// with one from their application. If no prefix is configured, it'll use the
// entire 64-bit number space.
type AdvisoryLockHash struct {
	configuredPrefix int32
	hash32           hash.Hash32
	hash64           hash.Hash64
}

func NewAdvisoryLockHash(configuredPrefix int32) *AdvisoryLockHash {
	return &AdvisoryLockHash{
		configuredPrefix: configuredPrefix,

		// Not technically required to initialize both, but these just return
		// constants, so overhead is ~zero, and it looks nicer.
		hash32: fnv.New32(),
		hash64: fnv.New64(),
	}
}

// Key generates a pair of keys to be passed into Postgres advisory lock
// functions like `pg_advisory_xact_lock`.
func (h *AdvisoryLockHash) Key() int64 {
	if h.configuredPrefix == 0 {
		// Overflow is okay and allowed.
		return int64(h.hash64.Sum64()) //nolint:gosec
	}

	// Overflow is okay and allowed.
	return int64(uint64(h.configuredPrefix)<<32 | uint64(h.hash32.Sum32())) //nolint:gosec
}

// Write writes bytes to the underlying hash.
func (h *AdvisoryLockHash) Write(data []byte) {
	var activeHash hash.Hash = h.hash32
	if h.configuredPrefix == 0 {
		activeHash = h.hash64
	}

	_, err := activeHash.Write(data)
	if err != nil {
		panic(err) // error basically impossible, so panic for caller convenience
	}
}

```

`internal/util/hashutil/hash_util_test.go`:

```go
package hashutil

import (
	"math"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestAdvisoryLockHash(t *testing.T) {
	t.Parallel()

	t.Run("32BitWithPrefix", func(t *testing.T) {
		t.Parallel()

		hash := NewAdvisoryLockHash(math.MaxInt32)
		hash.Write([]byte("some content for hashing purposes"))
		key := hash.Key()
		require.Equal(t, math.MaxInt32, int(key>>32))
		require.Equal(t, 764501624, int(key&0xffffffff))
	})

	t.Run("64Bit", func(t *testing.T) {
		t.Parallel()

		hash := NewAdvisoryLockHash(0)
		hash.Write([]byte("some content for hashing purposes"))
		key := hash.Key()
		require.Equal(t, int64(1854036014321452184), key)

		// The output hash isn't guaranteed to be larger than MaxInt32 of
		// course, but given a reasonable hash function like FNV that produces
		// good distribution, it's far more likely to be than not. We're using a
		// fixed input in this test, so we know that it'll always be in this case.
		require.Greater(t, int(key), math.MaxInt32)
	})
}

```

`internal/workunit/work_unit.go`:

```go
package workunit

import (
	"context"
	"time"

	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/rivertype"
)

// WorkUnit provides an interface to a struct that wraps a job to be done
// combined with a work function that can execute it. Its main purpose is to
// wrap a struct that contains generic types (like a Worker[T] that needs to be
// invoked with a Job[T]) in such a way as to make it non-generic so that it can
// be used in other non-generic code like jobExecutor.
//
// Implemented by river.wrapperWorkUnit.
type WorkUnit interface {
	// HookLookup procures the a hook lookup bundle for the wrapped job using
	// the given job hook lookup bundle. Hooks are looked up by job args and
	// otherwise not available to jobexecutor.
	HookLookup(lookup *hooklookup.JobHookLookup) hooklookup.HookLookupInterface

	Middleware() []rivertype.WorkerMiddleware
	NextRetry() time.Time
	Timeout() time.Duration
	UnmarshalJob() error
	Work(ctx context.Context) error
}

// WorkUnitFactory provides an interface to a struct that can generate a
// workUnit, a wrapper around a job to be done combined with a work function
// that can execute it.
//
// Implemented by river.workUnitFactoryWrapper.
type WorkUnitFactory interface {
	// Make a workUnit, which wraps a job to be done and work function that can
	// execute it.
	MakeUnit(jobRow *rivertype.JobRow) WorkUnit
}

```

`job.go`:

```go
package river

import (
	"github.com/riverqueue/river/rivertype"
)

// Job represents a single unit of work, holding both the arguments and
// information for a job with args of type T.
type Job[T JobArgs] struct {
	*rivertype.JobRow

	// Args are the arguments for the job.
	Args T
}

// JobArgs is an interface that represents the arguments for a job of type T.
// These arguments are serialized into JSON and stored in the database.
//
// The struct is serialized using `encoding/json`. All exported fields are
// serialized, unless skipped with a struct field tag.
type JobArgs interface {
	// Kind is a string that uniquely identifies the type of job. This must be
	// provided on your job arguments struct.
	Kind() string
}

// JobArgsWithHooks is an interface that job args can implement to attach
// specific hooks (i.e. other than those globally installed to a client) to
// certain kinds of jobs.
type JobArgsWithHooks interface {
	// Hooks returns specific hooks to run for this job type. These will run
	// after the global hooks configured on the client.
	//
	// Warning: Hooks returned should be based on the job type only and be
	// invariant of the specific contents of a job. Hooks are extracted by
	// instantiating a generic instance of the job even when a specific instance
	// is available, so any conditional logic within will be ignored. This is
	// done because although specific job information may be available in some
	// hook contexts like on InsertBegin, it won't be in others like WorkBegin.
	Hooks() []rivertype.Hook
}

// JobArgsWithInsertOpts is an extra interface that a job may implement on top
// of JobArgs to provide insertion-time options for all jobs of this type.
type JobArgsWithInsertOpts interface {
	// InsertOpts returns options for all jobs of this job type, overriding any
	// system defaults. These can also be overridden at insertion time.
	InsertOpts() InsertOpts
}

```

`job_args_reflect_kind_test.go`:

```go
package river

import "reflect"

// JobArgsReflectKind can be embedded on a struct to implement JobArgs such that
// the job's kind will be the name of TKind. Typically, for convenience TKind
// will be the same type that does the embedding. Use of JobArgsReflectKind may
// not be typical, but in combination with WorkFunc, it allows the entirety of a
// job args and worker pair to be implemented inside the body of a function.
//
//	type InFuncWorkFuncArgs struct {
//		JobArgsReflectKind[InFuncWorkFuncArgs]
//		Message `json:"message"`
//	}
//
//	AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[WorkFuncArgs]) error {
//		...
//
// Its major downside compared to a normal JobArgs implementation is that it's
// possible to easily break things accidentally by renaming its type, deploying,
// and then finding that the worker will no longer work any jobs that were
// inserted before the deploy. It also depends on reflection, which likely makes
// it marginally slower.
//
// We're not sure yet whether it's appropriate to expose this publicly, so for
// now we've localized it to the test suite only. When a test case needs a job
// type that won't be reused, it's preferable to make use of JobArgsReflectKind
// so the type doesn't pollute the global namespace.
type JobArgsReflectKind[TKind any] struct{}

func (a JobArgsReflectKind[TKind]) Kind() string { return reflect.TypeOf(a).Name() }

```

`job_complete_tx.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"errors"
	"time"

	"github.com/riverqueue/river/internal/execution"
	"github.com/riverqueue/river/internal/jobexecutor"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivertype"
)

// JobCompleteTx marks the job as completed as part of transaction tx. If tx is
// rolled back, the completion will be as well.
//
// The function needs to know the type of the River database driver, which is
// the same as the one in use by Client, but the other generic parameters can be
// inferred. An invocation should generally look like:
//
//	_, err := river.JobCompleteTx[*riverpgxv5.Driver](ctx, tx, job)
//	if err != nil {
//		// handle error
//	}
//
// Returns the updated, completed job.
func JobCompleteTx[TDriver riverdriver.Driver[TTx], TTx any, TArgs JobArgs](ctx context.Context, tx TTx, job *Job[TArgs]) (*Job[TArgs], error) {
	if job.State != rivertype.JobStateRunning {
		return nil, errors.New("job must be running")
	}

	client := ClientFromContext[TTx](ctx)
	if client == nil {
		return nil, errors.New("client not found in context, can only work within a River worker")
	}

	driver := client.Driver()
	pilot := client.Pilot()

	// extract metadata updates from context
	metadataUpdates, hasMetadataUpdates := jobexecutor.MetadataUpdatesFromWorkContext(ctx)
	hasMetadataUpdates = hasMetadataUpdates && len(metadataUpdates) > 0
	var (
		metadataUpdatesBytes []byte
		err                  error
	)
	if hasMetadataUpdates {
		metadataUpdatesBytes, err = json.Marshal(metadataUpdates)
		if err != nil {
			return nil, err
		}
	}

	execTx := driver.UnwrapExecutor(tx)
	params := riverdriver.JobSetStateCompleted(job.ID, time.Now(), nil)
	rows, err := pilot.JobSetStateIfRunningMany(ctx, execTx, &riverdriver.JobSetStateIfRunningManyParams{
		ID:              []int64{params.ID},
		Attempt:         []*int{params.Attempt},
		ErrData:         [][]byte{params.ErrData},
		FinalizedAt:     []*time.Time{params.FinalizedAt},
		MetadataDoMerge: []bool{hasMetadataUpdates},
		MetadataUpdates: [][]byte{metadataUpdatesBytes},
		ScheduledAt:     []*time.Time{params.ScheduledAt},
		State:           []rivertype.JobState{params.State},
	})
	if err != nil {
		return nil, err
	}
	if len(rows) == 0 {
		if _, isInsideTestWorker := ctx.Value(execution.ContextKeyInsideTestWorker{}).(bool); isInsideTestWorker {
			panic("to use JobCompleteTx in a rivertest.Worker, the job must be inserted into the database first")
		}

		return nil, rivertype.ErrNotFound
	}
	updatedJob := &Job[TArgs]{JobRow: rows[0]}

	if err := json.Unmarshal(updatedJob.EncodedArgs, &updatedJob.Args); err != nil {
		return nil, err
	}

	return updatedJob, nil
}

```

`job_complete_tx_test.go`:

```go
package river

import (
	"context"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/execution"
	"github.com/riverqueue/river/internal/jobexecutor"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

func TestJobCompleteTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type JobArgs struct {
		JobArgsReflectKind[JobArgs]
	}

	type testBundle struct {
		client *Client[pgx.Tx]
		exec   riverdriver.Executor
		tx     pgx.Tx
	}

	setup := func(ctx context.Context, t *testing.T) (context.Context, *testBundle) {
		t.Helper()

		tx := riverinternaltest.TestTx(ctx, t)
		client, err := NewClient(riverpgxv5.New(nil), &Config{
			Logger: riversharedtest.Logger(t),
		})
		require.NoError(t, err)
		ctx = context.WithValue(ctx, rivercommon.ContextKeyClient{}, client)

		return ctx, &testBundle{
			client: client,
			exec:   riverpgxv5.New(nil).UnwrapExecutor(tx),
			tx:     tx,
		}
	}

	t.Run("CompletesJob", func(t *testing.T) {
		t.Parallel()

		ctx, bundle := setup(ctx, t)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{
			State: ptrutil.Ptr(rivertype.JobStateRunning),
		})

		completedJob, err := JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCompleted, completedJob.State)
		require.WithinDuration(t, time.Now(), *completedJob.FinalizedAt, 2*time.Second)

		updatedJob, err := bundle.exec.JobGetByID(ctx, job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCompleted, updatedJob.State)
	})

	t.Run("CompletesJobWithMetadataUpdates", func(t *testing.T) {
		t.Parallel()

		ctx, bundle := setup(ctx, t)

		// Inject valid metadata updates into context
		metadataUpdates := map[string]any{"foo": "bar"}
		ctx = context.WithValue(ctx, jobexecutor.ContextKeyMetadataUpdates, metadataUpdates)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{
			State: ptrutil.Ptr(rivertype.JobStateRunning),
		})

		completedJob, err := JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCompleted, completedJob.State)

		updatedJob, err := bundle.exec.JobGetByID(ctx, job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCompleted, updatedJob.State)
	})

	t.Run("ErrorIfMetadataMarshallingFails", func(t *testing.T) {
		t.Parallel()

		ctx, bundle := setup(ctx, t)

		// Inject invalid metadata updates into context (using a channel which is not JSON marshalable)
		ctx = context.WithValue(ctx, jobexecutor.ContextKeyMetadataUpdates, map[string]any{"foo": make(chan int)})

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{
			State: ptrutil.Ptr(rivertype.JobStateRunning),
		})

		_, err := JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})
		require.ErrorContains(t, err, "unsupported type: chan int")
	})

	t.Run("ErrorIfNotRunning", func(t *testing.T) {
		t.Parallel()

		ctx, bundle := setup(ctx, t)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{})

		_, err := JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})
		require.EqualError(t, err, "job must be running")
	})

	t.Run("ErrorIfJobDoesntExist", func(t *testing.T) {
		t.Parallel()

		ctx, bundle := setup(ctx, t)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{
			State: ptrutil.Ptr(rivertype.JobStateAvailable),
		})

		// delete the job
		_, err := bundle.exec.JobDelete(ctx, job.ID)
		require.NoError(t, err)

		// fake the job's state to work around the check:
		job.State = rivertype.JobStateRunning
		_, err = JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})
		require.ErrorIs(t, err, rivertype.ErrNotFound)
	})

	t.Run("PanicsIfCalledInTestWorkerWithoutInsertingJob", func(t *testing.T) {
		t.Parallel()

		ctx, bundle := setup(ctx, t)
		ctx = context.WithValue(ctx, execution.ContextKeyInsideTestWorker{}, true)

		job := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateAvailable)})
		// delete the job as though it was never inserted:
		_, err := bundle.client.JobDeleteTx(ctx, bundle.tx, job.ID)
		require.NoError(t, err)
		job.State = rivertype.JobStateRunning

		require.PanicsWithValue(t, "to use JobCompleteTx in a rivertest.Worker, the job must be inserted into the database first", func() {
			_, err := JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, &Job[JobArgs]{JobRow: job})
			require.NoError(t, err)
		})
	})
}

```

`job_list_params.go`:

```go
package river

import (
	"encoding/base64"
	"encoding/json"
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/riverqueue/river/internal/dblist"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

// JobListCursor is used to specify a starting point for a paginated
// job list query.
type JobListCursor struct {
	id        int64
	job       *rivertype.JobRow // used for JobListCursorFromJob path; not serialized
	kind      string
	queue     string
	sortField JobListOrderByField
	time      time.Time // may be empty
}

// JobListCursorFromJob creates a JobListCursor from a JobRow.
func JobListCursorFromJob(job *rivertype.JobRow) *JobListCursor {
	// Other fields are initialized when the cursor is used in After below.
	return &JobListCursor{job: job}
}

func jobListCursorFromJobAndParams(job *rivertype.JobRow, listParams *JobListParams) *JobListCursor {
	// A pointer so that we can detect a condition where we accidentally left
	// this value unset.
	var cursorTime *time.Time

	// Don't include a `default` so `exhaustive` lint can detect omissions.
	switch listParams.sortField {
	case JobListOrderByID:
		cursorTime = ptrutil.Ptr(time.Time{})
	case JobListOrderByTime:
		cursorTime = ptrutil.Ptr(jobListTimeValue(job))
	case JobListOrderByFinalizedAt:
		if job.FinalizedAt != nil {
			cursorTime = job.FinalizedAt
		}
	case JobListOrderByScheduledAt:
		cursorTime = &job.ScheduledAt
	}

	if cursorTime == nil {
		panic("invalid sort field")
	}

	return &JobListCursor{
		id:        job.ID,
		kind:      job.Kind,
		queue:     job.Queue,
		sortField: listParams.sortField,
		time:      *cursorTime,
	}
}

// UnmarshalText implements encoding.TextUnmarshaler to decode the cursor from
// a previously marshaled string.
func (c *JobListCursor) UnmarshalText(text []byte) error {
	dst := make([]byte, base64.StdEncoding.DecodedLen(len(text)))
	n, err := base64.StdEncoding.Decode(dst, text)
	if err != nil {
		return err
	}
	dst = dst[:n]

	wrapperValue := jobListPaginationCursorJSON{}
	if err := json.Unmarshal(dst, &wrapperValue); err != nil {
		return err
	}
	*c = JobListCursor{
		id:        wrapperValue.ID,
		kind:      wrapperValue.Kind,
		queue:     wrapperValue.Queue,
		sortField: JobListOrderByField(wrapperValue.SortField),
		time:      wrapperValue.Time,
	}
	return nil
}

// MarshalText implements encoding.TextMarshaler to encode the cursor as an
// opaque string.
func (c JobListCursor) MarshalText() ([]byte, error) {
	if c.job != nil {
		return nil, errors.New("cursor initialized with only a job can't be marshaled; try a cursor from JobListResult instead")
	}

	wrapperValue := jobListPaginationCursorJSON{
		ID:        c.id,
		Kind:      c.kind,
		Queue:     c.queue,
		SortField: string(c.sortField),
		Time:      c.time,
	}
	data, err := json.Marshal(wrapperValue)
	if err != nil {
		return nil, err
	}
	dst := make([]byte, base64.URLEncoding.EncodedLen(len(data)))
	base64.URLEncoding.Encode(dst, data)
	return dst, nil
}

type jobListPaginationCursorJSON struct {
	ID        int64     `json:"id"`
	Kind      string    `json:"kind"`
	Queue     string    `json:"queue"`
	SortField string    `json:"sort_field"`
	Time      time.Time `json:"time"`
}

// SortOrder specifies the direction of a sort.
type SortOrder int

const (
	// SortOrderAsc specifies that the sort should in ascending order.
	SortOrderAsc SortOrder = iota

	// SortOrderDesc specifies that the sort should in descending order.
	SortOrderDesc
)

// JobListOrderByField specifies the field to sort by.
type JobListOrderByField string

const (
	// JobListOrderByID specifies that the sort should be by job ID.
	JobListOrderByID JobListOrderByField = "id"

	// JobListOrderByFinalizedAt specifies that the sort should be by
	// `finalized_at`.
	//
	// This option must be used in conjunction with filtering by only finalized
	// job states.
	JobListOrderByFinalizedAt JobListOrderByField = "finalized_at"

	// JobListOrderByScheduledAt specifies that the sort should be by
	// `scheduled_at`.
	JobListOrderByScheduledAt JobListOrderByField = "scheduled_at"

	// JobListOrderByTime specifies that the sort should be by the "best fit"
	// time field based on listed state. The best fit is determined by looking
	// at the first value given to JobListParams.States. If multiple states are
	// specified, the ones after the first will be ignored.
	//
	// The specific time field used for sorting depends on requested state:
	//
	// * States `available`, `retryable`, or `scheduled` use `scheduled_at`.
	// * State `running` uses `attempted_at`.
	// * States `cancelled`, `completed`, or `discarded` use `finalized_at`.
	JobListOrderByTime JobListOrderByField = "time"
)

// JobListParams specifies the parameters for a JobList query. It must be
// initialized with NewJobListParams. Params can be built by chaining methods on
// the JobListParams object:
//
//	params := NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc).First(100)
type JobListParams struct {
	after            *JobListCursor
	kinds            []string
	metadataFragment string
	overrodeState    bool
	paginationCount  int32
	queues           []string
	sortField        JobListOrderByField
	sortOrder        SortOrder
	states           []rivertype.JobState
}

// NewJobListParams creates a new JobListParams to return available jobs sorted
// by time in ascending order, returning 100 jobs at most.
func NewJobListParams() *JobListParams {
	return &JobListParams{
		paginationCount: 100,
		sortField:       JobListOrderByID,
		sortOrder:       SortOrderAsc,
		states: []rivertype.JobState{
			rivertype.JobStateAvailable,
			rivertype.JobStateCancelled,
			rivertype.JobStateCompleted,
			rivertype.JobStateDiscarded,
			rivertype.JobStatePending,
			rivertype.JobStateRetryable,
			rivertype.JobStateRunning,
			rivertype.JobStateScheduled,
		},
	}
}

func (p *JobListParams) copy() *JobListParams {
	return &JobListParams{
		after:            p.after,
		kinds:            append([]string(nil), p.kinds...),
		metadataFragment: p.metadataFragment,
		overrodeState:    p.overrodeState,
		paginationCount:  p.paginationCount,
		queues:           append([]string(nil), p.queues...),
		sortField:        p.sortField,
		sortOrder:        p.sortOrder,
		states:           append([]rivertype.JobState(nil), p.states...),
	}
}

func (p *JobListParams) toDBParams() (*dblist.JobListParams, error) {
	conditionsBuilder := &strings.Builder{}
	conditions := make([]string, 0, 10)
	namedArgs := make(map[string]any)
	orderBy := make([]dblist.JobListOrderBy, 0, 2)

	var sortOrder dblist.SortOrder
	switch p.sortOrder {
	case SortOrderAsc:
		sortOrder = dblist.SortOrderAsc
	case SortOrderDesc:
		sortOrder = dblist.SortOrderDesc
	default:
		return nil, errors.New("invalid sort order")
	}

	if p.sortField == JobListOrderByFinalizedAt {
		currentNonFinalizedStates := make([]rivertype.JobState, 0, len(p.states))
		for _, state := range p.states {
			//nolint:exhaustive
			switch state {
			case rivertype.JobStateCancelled, rivertype.JobStateCompleted, rivertype.JobStateDiscarded:
			default:
				currentNonFinalizedStates = append(currentNonFinalizedStates, state)
			}
		}
		// This indicates the user overrode the States list with only non-finalized
		// states prior to then requesting FinalizedAt ordering.
		if len(currentNonFinalizedStates) == 0 {
			return nil, fmt.Errorf("cannot order by finalized_at with non-finalized state filters %+v", currentNonFinalizedStates)
		}
	}

	var timeField string
	switch {
	case p.sortField == JobListOrderByID:
		// no time field

	case len(p.states) > 0 && p.sortField == JobListOrderByTime:
		timeField = jobListTimeFieldForState(p.states[0])
		orderBy = append(orderBy, dblist.JobListOrderBy{Expr: timeField, Order: sortOrder})

	default:
		timeField = string(p.sortField)
		orderBy = append(orderBy, dblist.JobListOrderBy{Expr: timeField, Order: sortOrder})
	}

	orderBy = append(orderBy, dblist.JobListOrderBy{Expr: "id", Order: sortOrder})

	if p.metadataFragment != "" {
		conditions = append(conditions, `metadata @> @metadata_fragment::jsonb`)
		namedArgs["metadata_fragment"] = p.metadataFragment
	}

	if p.after != nil {
		if p.after.time.IsZero() { // order by ID only
			if sortOrder == dblist.SortOrderAsc {
				conditions = append(conditions, "(id > @after_id)")
			} else {
				conditions = append(conditions, "(id < @after_id)")
			}
		} else {
			if sortOrder == dblist.SortOrderAsc {
				conditions = append(conditions, fmt.Sprintf(`("%s" > @cursor_time OR ("%s" = @cursor_time AND "id" > @after_id))`, timeField, timeField))
			} else {
				conditions = append(conditions, fmt.Sprintf(`("%s" < @cursor_time OR ("%s" = @cursor_time AND "id" < @after_id))`, timeField, timeField))
			}
			namedArgs["cursor_time"] = p.after.time
		}
		namedArgs["after_id"] = p.after.id
	}

	for i, condition := range conditions {
		if i > 0 {
			conditionsBuilder.WriteString("\n  AND ")
		}
		conditionsBuilder.WriteString(condition)
	}

	dbParams := &dblist.JobListParams{
		Conditions: conditionsBuilder.String(),
		Kinds:      p.kinds,
		LimitCount: p.paginationCount,
		NamedArgs:  namedArgs,
		OrderBy:    orderBy,
		Priorities: nil,
		Queues:     p.queues,
		States:     p.states,
	}

	return dbParams, nil
}

// After returns an updated filter set that will only return jobs
// after the given cursor.
func (p *JobListParams) After(cursor *JobListCursor) *JobListParams {
	paramsCopy := p.copy()
	if cursor.job == nil {
		paramsCopy.after = cursor
	} else {
		paramsCopy.after = jobListCursorFromJobAndParams(cursor.job, paramsCopy)
	}
	return paramsCopy
}

// First returns an updated filter set that will only return the first
// count jobs.
//
// Count must be between 1 and 10000, inclusive, or this will panic.
func (p *JobListParams) First(count int) *JobListParams {
	if count <= 0 {
		panic("count must be > 0")
	}
	if count > 10000 {
		panic("count must be <= 10000")
	}
	paramsCopy := p.copy()
	paramsCopy.paginationCount = int32(count)
	return paramsCopy
}

// Kinds returns an updated filter set that will only return jobs of the given
// kinds.
func (p *JobListParams) Kinds(kinds ...string) *JobListParams {
	paramsCopy := p.copy()
	paramsCopy.kinds = make([]string, len(kinds))
	copy(paramsCopy.kinds, kinds)
	return paramsCopy
}

func (p *JobListParams) Metadata(json string) *JobListParams {
	paramsCopy := p.copy()
	paramsCopy.metadataFragment = json
	return paramsCopy
}

// Queues returns an updated filter set that will only return jobs from the
// given queues.
func (p *JobListParams) Queues(queues ...string) *JobListParams {
	paramsCopy := p.copy()
	paramsCopy.queues = make([]string, len(queues))
	copy(paramsCopy.queues, queues)
	return paramsCopy
}

// OrderBy returns an updated filter set that will sort the results using the
// specified field and direction.
//
// If ordering by FinalizedAt, the States filter will be set to only include
// finalized job states unless it has already been overridden.
func (p *JobListParams) OrderBy(field JobListOrderByField, direction SortOrder) *JobListParams {
	paramsCopy := p.copy()
	switch field {
	case JobListOrderByID, JobListOrderByTime, JobListOrderByScheduledAt:
		paramsCopy.sortField = field
	case JobListOrderByFinalizedAt:
		paramsCopy.sortField = field
		if !p.overrodeState {
			paramsCopy.states = []rivertype.JobState{
				rivertype.JobStateCancelled,
				rivertype.JobStateCompleted,
				rivertype.JobStateDiscarded,
			}
		}
	default:
		panic("invalid order by field")
	}
	paramsCopy.sortField = field
	paramsCopy.sortOrder = direction
	return paramsCopy
}

// States returns an updated filter set that will only return jobs in the given
// states.
func (p *JobListParams) States(states ...rivertype.JobState) *JobListParams {
	paramsCopy := p.copy()
	paramsCopy.states = make([]rivertype.JobState, len(states))
	paramsCopy.overrodeState = true
	copy(paramsCopy.states, states)
	return paramsCopy
}

func jobListTimeFieldForState(state rivertype.JobState) string {
	// Don't include a `default` so `exhaustive` lint can detect omissions.
	switch state {
	case rivertype.JobStateAvailable, rivertype.JobStatePending, rivertype.JobStateRetryable, rivertype.JobStateScheduled:
		return "scheduled_at"
	case rivertype.JobStateRunning:
		return "attempted_at"
	case rivertype.JobStateCancelled, rivertype.JobStateCompleted, rivertype.JobStateDiscarded:
		return "finalized_at"
	}

	return "created_at" // should never happen
}

func jobListTimeValue(job *rivertype.JobRow) time.Time {
	// Don't include a `default` so `exhaustive` lint can detect omissions.
	switch job.State {
	case rivertype.JobStateAvailable, rivertype.JobStatePending, rivertype.JobStateRetryable, rivertype.JobStateScheduled:
		return job.ScheduledAt

	case rivertype.JobStateRunning:
		if job.AttemptedAt == nil {
			// This should never happen unless a job has been manually manipulated.
			return job.CreatedAt
		}
		return *job.AttemptedAt

	case rivertype.JobStateCancelled, rivertype.JobStateCompleted, rivertype.JobStateDiscarded:
		if job.FinalizedAt == nil {
			// This should never happen unless a job has been manually manipulated.
			return job.CreatedAt
		}
		return *job.FinalizedAt
	}

	return job.CreatedAt // should never happen
}

```

`job_list_params_test.go`:

```go
package river

import (
	"encoding/json"
	"fmt"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

func Test_JobListCursor_JobListCursorFromJob(t *testing.T) {
	t.Parallel()

	jobRow := &rivertype.JobRow{
		ID:    4,
		Kind:  "test",
		Queue: "test",
		State: rivertype.JobStateRunning,
	}

	cursor := JobListCursorFromJob(jobRow)
	require.Zero(t, cursor.id)
	require.Equal(t, jobRow, cursor.job)
	require.Zero(t, cursor.kind)
	require.Zero(t, cursor.queue)
	require.Zero(t, cursor.sortField)
	require.Zero(t, cursor.time)
}

func Test_JobListCursor_jobListCursorFromJobAndParams(t *testing.T) {
	t.Parallel()

	t.Run("OrderByID", func(t *testing.T) {
		t.Parallel()

		jobRow := &rivertype.JobRow{
			ID:    4,
			Kind:  "test",
			Queue: "test",
			State: rivertype.JobStateRunning,
		}

		cursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().After(JobListCursorFromJob(jobRow)))
		require.Equal(t, jobRow.ID, cursor.id)
		require.Equal(t, jobRow.Kind, cursor.kind)
		require.Equal(t, jobRow.Queue, cursor.queue)
		require.Zero(t, cursor.time)
	})

	for i, state := range []rivertype.JobState{
		rivertype.JobStateAvailable,
		rivertype.JobStateRetryable,
		rivertype.JobStateScheduled,
	} {
		t.Run(fmt.Sprintf("OrderByTimeScheduledAtUsedFor%sJob", state), func(t *testing.T) {
			t.Parallel()

			now := time.Now().UTC()
			jobRow := &rivertype.JobRow{
				CreatedAt:   now.Add(-11 * time.Second),
				ID:          int64(i),
				Kind:        "test_kind",
				Queue:       "test_queue",
				State:       state,
				ScheduledAt: now.Add(-10 * time.Second),
			}

			cursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))
			require.Equal(t, jobRow.ID, cursor.id)
			require.Equal(t, jobRow.Kind, cursor.kind)
			require.Equal(t, jobRow.Queue, cursor.queue)
			require.Equal(t, jobRow.ScheduledAt, cursor.time)
		})
	}

	for i, state := range []rivertype.JobState{
		rivertype.JobStateCancelled,
		rivertype.JobStateCompleted,
		rivertype.JobStateDiscarded,
	} {
		t.Run(fmt.Sprintf("OrderByTimeFinalizedAtUsedFor%sJob", state), func(t *testing.T) {
			t.Parallel()

			now := time.Now().UTC()
			jobRow := &rivertype.JobRow{
				AttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second)),
				CreatedAt:   now.Add(-11 * time.Second),
				FinalizedAt: ptrutil.Ptr(now.Add(-1 * time.Second)),
				ID:          int64(i),
				Kind:        "test_kind",
				Queue:       "test_queue",
				State:       state,
				ScheduledAt: now.Add(-10 * time.Second),
			}

			cursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))
			require.Equal(t, jobRow.ID, cursor.id)
			require.Equal(t, jobRow.Kind, cursor.kind)
			require.Equal(t, jobRow.Queue, cursor.queue)
			require.Equal(t, *jobRow.FinalizedAt, cursor.time)
		})
	}

	t.Run("OrderByTimeRunningJobUsesAttemptedAt", func(t *testing.T) {
		t.Parallel()

		now := time.Now().UTC()
		jobRow := &rivertype.JobRow{
			AttemptedAt: ptrutil.Ptr(now.Add(-5 * time.Second)),
			CreatedAt:   now.Add(-11 * time.Second),
			ID:          4,
			Kind:        "test",
			Queue:       "test",
			State:       rivertype.JobStateRunning,
			ScheduledAt: now.Add(-10 * time.Second),
		}

		cursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))
		require.Equal(t, jobRow.ID, cursor.id)
		require.Equal(t, jobRow.Kind, cursor.kind)
		require.Equal(t, jobRow.Queue, cursor.queue)
		require.Equal(t, *jobRow.AttemptedAt, cursor.time)
	})

	t.Run("OrderByTimeUnknownJobStateUsesCreatedAt", func(t *testing.T) {
		t.Parallel()

		now := time.Now().UTC()
		jobRow := &rivertype.JobRow{
			CreatedAt:   now.Add(-11 * time.Second),
			ID:          4,
			Kind:        "test",
			Queue:       "test",
			State:       rivertype.JobState("unknown_fake_state"),
			ScheduledAt: now.Add(-10 * time.Second),
		}

		cursor := jobListCursorFromJobAndParams(jobRow, NewJobListParams().OrderBy(JobListOrderByTime, SortOrderAsc))
		require.Equal(t, jobRow.ID, cursor.id)
		require.Equal(t, jobRow.Kind, cursor.kind)
		require.Equal(t, jobRow.Queue, cursor.queue)
		require.Equal(t, jobRow.CreatedAt, cursor.time)
	})
}

func Test_JobListCursor_MarshalJSON(t *testing.T) {
	t.Parallel()

	t.Run("CanMarshalAndUnmarshal", func(t *testing.T) {
		t.Parallel()

		now := time.Now().UTC()
		cursor := &JobListCursor{
			id:    4,
			kind:  "test_kind",
			queue: "test_queue",
			time:  now,
		}

		text, err := json.Marshal(cursor)
		require.NoError(t, err)
		require.NotEqual(t, "", text)

		unmarshaledParams := &JobListCursor{}
		require.NoError(t, json.Unmarshal(text, unmarshaledParams))

		require.Equal(t, cursor, unmarshaledParams)
	})

	t.Run("ErrorsOnJobOnlyCursor", func(t *testing.T) {
		t.Parallel()

		jobRow := &rivertype.JobRow{
			ID:    4,
			Kind:  "test",
			Queue: "test",
			State: rivertype.JobStateRunning,
		}

		cursor := JobListCursorFromJob(jobRow)

		_, err := json.Marshal(cursor)
		require.EqualError(t, err, "json: error calling MarshalText for type *river.JobListCursor: cursor initialized with only a job can't be marshaled; try a cursor from JobListResult instead")
	})
}

```

`job_test.go`:

```go
package river

import (
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivertype"
)

func TestUniqueOpts_isEmpty(t *testing.T) {
	t.Parallel()

	require.True(t, (&UniqueOpts{}).isEmpty())
	require.False(t, (&UniqueOpts{ByArgs: true}).isEmpty())
	require.False(t, (&UniqueOpts{ByPeriod: 1 * time.Nanosecond}).isEmpty())
	require.False(t, (&UniqueOpts{ByQueue: true}).isEmpty())
	require.False(t, (&UniqueOpts{ByState: []rivertype.JobState{rivertype.JobStateAvailable}}).isEmpty())
}

```

`main_test.go`:

```go
package river

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`middleware_defaults.go`:

```go
package river

import (
	"context"

	"github.com/riverqueue/river/rivertype"
)

// MiddlewareDefaults should be embedded on any middleware implementation. It
// helps identify a struct as middleware, and guarantees forward compatibility in
// case additions are necessary to the rivertype.Middleware interface.
type MiddlewareDefaults struct{}

func (d *MiddlewareDefaults) IsMiddleware() bool { return true }

// JobInsertMiddlewareDefaults is an embeddable struct that provides default
// implementations for the rivertype.JobInsertMiddleware. Use of this struct is
// recommended in case rivertype.JobInsertMiddleware is expanded in the future
// so that existing code isn't unexpectedly broken during an upgrade.
//
// Deprecated: Prefer embedding the more general MiddlewareDefaults instead.
type JobInsertMiddlewareDefaults struct{ MiddlewareDefaults }

func (d *JobInsertMiddlewareDefaults) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
	return doInner(ctx)
}

// WorkerInsertMiddlewareDefaults is an embeddable struct that provides default
// implementations for the rivertype.WorkerMiddleware. Use of this struct is
// recommended in case rivertype.WorkerMiddleware is expanded in the future so
// that existing code isn't unexpectedly broken during an upgrade.
//
// Deprecated: Prefer embedding the more general MiddlewareDefaults instead.
type WorkerMiddlewareDefaults struct{ MiddlewareDefaults }

func (d *WorkerMiddlewareDefaults) Work(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {
	return doInner(ctx)
}

```

`middleware_defaults_test.go`:

```go
package river

import "github.com/riverqueue/river/rivertype"

var (
	_ rivertype.JobInsertMiddleware = &JobInsertMiddlewareDefaults{}
	_ rivertype.WorkerMiddleware    = &WorkerMiddlewareDefaults{}
)

```

`middleware_test.go`:

```go
package river

import (
	"context"

	"github.com/riverqueue/river/rivertype"
)

type overridableJobMiddleware struct {
	MiddlewareDefaults

	insertManyFunc func(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error)
	workFunc       func(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error
}

func (m *overridableJobMiddleware) InsertMany(ctx context.Context, manyParams []*rivertype.JobInsertParams, doInner func(ctx context.Context) ([]*rivertype.JobInsertResult, error)) ([]*rivertype.JobInsertResult, error) {
	if m.insertManyFunc != nil {
		return m.insertManyFunc(ctx, manyParams, doInner)
	}
	return doInner(ctx)
}

func (m *overridableJobMiddleware) Work(ctx context.Context, job *rivertype.JobRow, doInner func(ctx context.Context) error) error {
	if m.workFunc != nil {
		return m.workFunc(ctx, job, doInner)
	}
	return doInner(ctx)
}

```

`periodic_job.go`:

```go
package river

import (
	"time"

	"github.com/riverqueue/river/internal/maintenance"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

// PeriodicSchedule is a schedule for a periodic job. Periodic jobs should
// generally have an interval of at least 1 minute, and never less than one
// second.
type PeriodicSchedule interface {
	// Next returns the next time at which the job should be run given the
	// current time.
	Next(current time.Time) time.Time
}

// PeriodicJobConstructor is a function that gets called each time the paired
// PeriodicSchedule is triggered.
//
// A constructor must never block. It may return nil to indicate that no job
// should be inserted.
type PeriodicJobConstructor func() (JobArgs, *InsertOpts)

// PeriodicJob is a configuration for a periodic job.
type PeriodicJob struct {
	constructorFunc PeriodicJobConstructor
	opts            *PeriodicJobOpts
	scheduleFunc    PeriodicSchedule
}

// PeriodicJobOpts are options for a periodic job.
type PeriodicJobOpts struct {
	// RunOnStart can be used to indicate that a periodic job should insert an
	// initial job as a new scheduler is started. This can be used as a hedge
	// for jobs with longer scheduled durations that may not get to expiry
	// before a new scheduler is elected.
	//
	// RunOnStart also applies when a new periodic job is added dynamically with
	// `PeriodicJobs().Add` or `PeriodicJobs().AddMany`. Jobs added this way
	// with RunOnStart set to true are inserted once, then continue with their
	// normal run schedule.
	RunOnStart bool
}

// NewPeriodicJob returns a new PeriodicJob given a schedule and a constructor
// function.
//
// The schedule returns a time until the next time the periodic job should run.
// The helper PeriodicInterval is available for jobs that should run on simple,
// fixed intervals (e.g. every 15 minutes), and a custom schedule or third party
// cron package can be used for more complex scheduling (see the cron example).
// The constructor function is invoked each time a periodic job's schedule
// elapses, returning job arguments to insert along with optional insertion
// options.
//
// The periodic job scheduler is approximate and doesn't guarantee strong
// durability. It's started by the elected leader in a River cluster, and each
// periodic job is assigned an initial run time when that occurs. New run times
// are scheduled each time a job's target run time is reached and a new job
// inserted. However, each scheduler only retains in-memory state, so anytime a
// process quits or a new leader is elected, the whole process starts over
// without regard for the state of the last scheduler. The RunOnStart option
// can be used as a hedge to make sure that jobs with long run durations are
// guaranteed to occasionally run.
func NewPeriodicJob(scheduleFunc PeriodicSchedule, constructorFunc PeriodicJobConstructor, opts *PeriodicJobOpts) *PeriodicJob {
	return &PeriodicJob{
		constructorFunc: constructorFunc,
		opts:            opts,
		scheduleFunc:    scheduleFunc,
	}
}

type neverSchedule struct{}

func (s *neverSchedule) Next(t time.Time) time.Time {
	// Return the maximum future time so that the schedule never runs.
	return time.Unix(1<<63-62135596801, 999999999)
}

// NeverSchedule returns a PeriodicSchedule that never runs.
func NeverSchedule() PeriodicSchedule {
	return &neverSchedule{}
}

type periodicIntervalSchedule struct {
	interval time.Duration
}

// PeriodicInterval returns a simple PeriodicSchedule that runs at the given
// interval.
func PeriodicInterval(interval time.Duration) PeriodicSchedule {
	return &periodicIntervalSchedule{interval}
}

func (s *periodicIntervalSchedule) Next(t time.Time) time.Time {
	return t.Add(s.interval)
}

// PeriodicJobBundle is a bundle of currently configured periodic jobs. It's
// made accessible through Client, where new periodic jobs can be configured,
// and old ones removed.
type PeriodicJobBundle struct {
	clientConfig        *Config
	periodicJobEnqueuer *maintenance.PeriodicJobEnqueuer
}

func newPeriodicJobBundle(config *Config, periodicJobEnqueuer *maintenance.PeriodicJobEnqueuer) *PeriodicJobBundle {
	return &PeriodicJobBundle{
		clientConfig:        config,
		periodicJobEnqueuer: periodicJobEnqueuer,
	}
}

// Adds a new periodic job to the client. The job is queued immediately if
// RunOnStart is enabled, and then scheduled normally.
//
// Returns a periodic job handle which can be used to subsequently remove the
// job if desired.
//
// Adding or removing periodic jobs has no effect unless this client is elected
// leader because only the leader enqueues periodic jobs. To make sure that a
// new periodic job is fully enabled or disabled, it should be added or removed
// from _every_ active River client across all processes.
func (b *PeriodicJobBundle) Add(periodicJob *PeriodicJob) rivertype.PeriodicJobHandle {
	return b.periodicJobEnqueuer.Add(b.toInternal(periodicJob))
}

// AddMany adds many new periodic jobs to the client. The jobs are queued
// immediately if their RunOnStart is enabled, and then scheduled normally.
//
// Returns a periodic job handle which can be used to subsequently remove the
// job if desired.
//
// Adding or removing periodic jobs has no effect unless this client is elected
// leader because only the leader enqueues periodic jobs. To make sure that a
// new periodic job is fully enabled or disabled, it should be added or removed
// from _every_ active River client across all processes.
func (b *PeriodicJobBundle) AddMany(periodicJobs []*PeriodicJob) []rivertype.PeriodicJobHandle {
	return b.periodicJobEnqueuer.AddMany(sliceutil.Map(periodicJobs, b.toInternal))
}

// Clear clears all periodic jobs, cancelling all scheduled runs.
//
// Adding or removing periodic jobs has no effect unless this client is elected
// leader because only the leader enqueues periodic jobs. To make sure that a
// new periodic job is fully enabled or disabled, it should be added or removed
// from _every_ active River client across all processes.
func (b *PeriodicJobBundle) Clear() {
	b.periodicJobEnqueuer.Clear()
}

// Remove removes a periodic job, cancelling all scheduled runs.
//
// Requires the use of the periodic job handle that was returned when the job
// was added.
//
// Adding or removing periodic jobs has no effect unless this client is elected
// leader because only the leader enqueues periodic jobs. To make sure that a
// new periodic job is fully enabled or disabled, it should be added or removed
// from _every_ active River client across all processes.
func (b *PeriodicJobBundle) Remove(periodicJobHandle rivertype.PeriodicJobHandle) {
	b.periodicJobEnqueuer.Remove(periodicJobHandle)
}

// RemoveMany removes many periodic jobs, cancelling all scheduled runs.
//
// Requires the use of the periodic job handles that were returned when the jobs
// were added.
//
// Adding or removing periodic jobs has no effect unless this client is elected
// leader because only the leader enqueues periodic jobs. To make sure that a
// new periodic job is fully enabled or disabled, it should be added or removed
// from _every_ active River client across all processes.
func (b *PeriodicJobBundle) RemoveMany(periodicJobHandles []rivertype.PeriodicJobHandle) {
	b.periodicJobEnqueuer.RemoveMany(periodicJobHandles)
}

// An empty set of periodic job opts used as a default when none are specified.
var periodicJobEmptyOpts PeriodicJobOpts //nolint:gochecknoglobals

// There are two separate periodic job structs so that the top-level River
// package can expose one while still containing most periodic job logic in a
// subpackage. This function converts a top-level periodic job struct (used for
// configuration) to an internal one.
func (b *PeriodicJobBundle) toInternal(periodicJob *PeriodicJob) *maintenance.PeriodicJob {
	opts := &periodicJobEmptyOpts
	if periodicJob.opts != nil {
		opts = periodicJob.opts
	}
	return &maintenance.PeriodicJob{
		ConstructorFunc: func() (*rivertype.JobInsertParams, error) {
			args, options := periodicJob.constructorFunc()
			if args == nil {
				return nil, maintenance.ErrNoJobToInsert
			}
			return insertParamsFromConfigArgsAndOptions(&b.periodicJobEnqueuer.Archetype, b.clientConfig, args, options)
		},
		RunOnStart:   opts.RunOnStart,
		ScheduleFunc: periodicJob.scheduleFunc.Next,
	}
}

```

`periodic_job_test.go`:

```go
package river

import (
	"encoding/json"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/maintenance"
	"github.com/riverqueue/river/rivershared/riversharedtest"
)

func TestNeverSchedule(t *testing.T) {
	t.Parallel()

	t.Run("NextReturnsMaximumTime", func(t *testing.T) {
		t.Parallel()

		schedule := NeverSchedule()
		now := time.Now()
		next := schedule.Next(now)
		require.Equal(t, time.Unix(1<<63-62135596801, 999999999), next)
		require.False(t, next.Before(now))
		// use an arbitrary duration to check that
		// the next schedule is far in the future
		require.Greater(t, next.Year()-now.Year(), 1000)
	})
}

func TestPeriodicJobBundle(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (*PeriodicJobBundle, *testBundle) { //nolint:unparam
		t.Helper()

		periodicJobEnqueuer := maintenance.NewPeriodicJobEnqueuer(
			riversharedtest.BaseServiceArchetype(t),
			&maintenance.PeriodicJobEnqueuerConfig{},
			nil,
		)

		return newPeriodicJobBundle(newTestConfig(t, nil), periodicJobEnqueuer), &testBundle{}
	}

	t.Run("ConstructorFuncGeneratesNewArgsOnEachCall", func(t *testing.T) {
		t.Parallel()

		periodicJobBundle, _ := setup(t)

		type TestJobArgs struct {
			JobArgsReflectKind[TestJobArgs]
			JobNum int `json:"job_num"`
		}

		var jobNum int

		periodicJob := NewPeriodicJob(
			PeriodicInterval(15*time.Minute),
			func() (JobArgs, *InsertOpts) {
				jobNum++
				return TestJobArgs{JobNum: jobNum}, nil
			},
			nil,
		)

		internalPeriodicJob := periodicJobBundle.toInternal(periodicJob)

		insertParams1, err := internalPeriodicJob.ConstructorFunc()
		require.NoError(t, err)
		require.Equal(t, 1, mustUnmarshalJSON[TestJobArgs](t, insertParams1.EncodedArgs).JobNum)

		insertParams2, err := internalPeriodicJob.ConstructorFunc()
		require.NoError(t, err)
		require.Equal(t, 2, mustUnmarshalJSON[TestJobArgs](t, insertParams2.EncodedArgs).JobNum)
	})

	t.Run("ReturningNilDoesntInsertNewJob", func(t *testing.T) {
		t.Parallel()

		periodicJobBundle, _ := setup(t)

		periodicJob := NewPeriodicJob(
			PeriodicInterval(15*time.Minute),
			func() (JobArgs, *InsertOpts) {
				// Returning nil from the constructor function should not insert a new job.
				return nil, nil
			},
			nil,
		)

		internalPeriodicJob := periodicJobBundle.toInternal(periodicJob)

		_, err := internalPeriodicJob.ConstructorFunc()
		require.ErrorIs(t, err, maintenance.ErrNoJobToInsert)
	})
}

func mustUnmarshalJSON[T any](t *testing.T, data []byte) *T {
	t.Helper()

	var val T
	err := json.Unmarshal(data, &val)
	require.NoError(t, err)
	return &val
}

```

`plugin.go`:

```go
package river

import (
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riverpilot"
	"github.com/riverqueue/river/rivershared/startstop"
)

// A plugin API that drivers may implement to extend a River client. Driver
// plugins may, for example, add additional maintenance services.
//
// This should be considered a River internal API and its stability is not
// guaranteed. DO NOT USE.
type driverPlugin[TTx any] interface {
	// PluginInit initializes a plugin with an archetype. It's invoked on
	// Client.NewClient.
	PluginInit(archetype *baseservice.Archetype)

	// PluginPilot returns a custom Pilot implementation.
	PluginPilot() riverpilot.Pilot
}

// A plugin API that pilots may implement to extend a River client. Pilot
// plugins may, for example, add additional maintenance services.
//
// This should be considered a River internal API and its stability is not
// guaranteed. DO NOT USE.
type pilotPlugin interface {
	// PluginMaintenanceServices returns additional maintenance services (will
	// only run on an elected leader) for a River client.
	PluginMaintenanceServices() []startstop.Service

	// PluginServices returns additional non-maintenance services (will run on
	// all clients) for a River client.
	PluginServices() []startstop.Service
}

```

`plugin_test.go`:

```go
package river

import (
	"context"
	"testing"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riverpilot"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstop"
)

func TestClientDriverPlugin(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		pluginDriver *TestDriverWithPlugin
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		pluginDriver := newDriverWithPlugin(t, riverinternaltest.TestDB(ctx, t))

		client, err := NewClient(pluginDriver, newTestConfig(t, nil))
		require.NoError(t, err)

		return client, &testBundle{
			pluginDriver: pluginDriver,
		}
	}

	t.Run("InitCalled", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		startClient(ctx, t, client)

		require.True(t, bundle.pluginDriver.initCalled)
	})
}

var _ driverPlugin[pgx.Tx] = &TestDriverWithPlugin{}

type TestDriverWithPlugin struct {
	*riverpgxv5.Driver
	initCalled bool
	pilot      riverpilot.Pilot
}

func newDriverWithPlugin(t *testing.T, dbPool *pgxpool.Pool) *TestDriverWithPlugin {
	t.Helper()

	return &TestDriverWithPlugin{
		Driver: riverpgxv5.New(dbPool),
	}
}

func (d *TestDriverWithPlugin) PluginInit(archetype *baseservice.Archetype) {
	d.initCalled = true
}

func (d *TestDriverWithPlugin) PluginPilot() riverpilot.Pilot {
	if !d.initCalled {
		panic("expected PluginInit to be called before this function")
	}

	return d.pilot
}

func TestClientPilotPlugin(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		pluginDriver *TestDriverWithPlugin
		pluginPilot  *TestPilotWithPlugin
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		pluginDriver := newDriverWithPlugin(t, riverinternaltest.TestDB(ctx, t))
		pluginPilot := newPilotWithPlugin(t)
		pluginDriver.pilot = pluginPilot

		client, err := NewClient(pluginDriver, newTestConfig(t, nil))
		require.NoError(t, err)

		return client, &testBundle{
			pluginDriver: pluginDriver,
			pluginPilot:  pluginPilot,
		}
	}

	t.Run("ServicesStart", func(t *testing.T) {
		t.Parallel()

		client, bundle := setup(t)

		startClient(ctx, t, client)

		riversharedtest.WaitOrTimeout(t, startstop.WaitAllStartedC(
			bundle.pluginPilot.maintenanceService,
			bundle.pluginPilot.service,
		))
	})
}

var _ pilotPlugin = &TestPilotWithPlugin{}

type TestPilotWithPlugin struct {
	riverpilot.StandardPilot
	maintenanceService startstop.Service
	service            startstop.Service
}

func newPilotWithPlugin(t *testing.T) *TestPilotWithPlugin {
	t.Helper()

	newService := func(name string) startstop.Service {
		return startstop.StartStopFunc(func(ctx context.Context, shouldStart bool, started, stopped func()) error {
			if !shouldStart {
				return nil
			}

			go func() {
				started()
				defer stopped() // this defer should come first so it's last out

				t.Logf("Test service started: %s", name)

				<-ctx.Done()
			}()

			return nil
		})
	}

	return &TestPilotWithPlugin{
		StandardPilot:      riverpilot.StandardPilot{},
		maintenanceService: newService("maintenance service"),
		service:            newService("other service"),
	}
}

func (d *TestPilotWithPlugin) PluginMaintenanceServices() []startstop.Service {
	return []startstop.Service{d.maintenanceService}
}

func (d *TestPilotWithPlugin) PluginServices() []startstop.Service {
	return []startstop.Service{d.service}
}

```

`producer.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"errors"
	"log/slog"
	"strings"
	"sync/atomic"
	"time"

	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/jobexecutor"
	"github.com/riverqueue/river/internal/middlewarelookup"
	"github.com/riverqueue/river/internal/notifier"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/util/chanutil"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/testsignal"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/serviceutil"
	"github.com/riverqueue/river/rivertype"
)

const (
	queuePollIntervalDefault   = 2 * time.Second
	queueReportIntervalDefault = 10 * time.Minute
)

// Test-only properties.
type producerTestSignals struct {
	DeletedExpiredQueueRecords testsignal.TestSignal[struct{}] // notifies when the producer deletes expired queue records
	Paused                     testsignal.TestSignal[struct{}] // notifies when the producer is paused
	PolledQueueConfig          testsignal.TestSignal[struct{}] // notifies when the producer polls for queue settings
	ReportedQueueStatus        testsignal.TestSignal[struct{}] // notifies when the producer reports queue status
	Resumed                    testsignal.TestSignal[struct{}] // notifies when the producer is resumed
	StartedExecutors           testsignal.TestSignal[struct{}] // notifies when runOnce finishes a pass
}

func (ts *producerTestSignals) Init() {
	ts.DeletedExpiredQueueRecords.Init()
	ts.Paused.Init()
	ts.PolledQueueConfig.Init()
	ts.ReportedQueueStatus.Init()
	ts.Resumed.Init()
	ts.StartedExecutors.Init()
}

type producerConfig struct {
	ClientID     string
	Completer    jobcompleter.JobCompleter
	ErrorHandler ErrorHandler

	// FetchCooldown is the minimum amount of time to wait between fetches of new
	// jobs. Jobs will only be fetched *at most* this often, but if no new jobs
	// are coming in via LISTEN/NOTIFY then fetches may be delayed as long as
	// FetchPollInterval.
	FetchCooldown time.Duration

	// FetchPollInterval is the amount of time between periodic fetches for new
	// jobs. Typically new jobs will be picked up ~immediately after insert via
	// LISTEN/NOTIFY, but this provides a fallback.
	FetchPollInterval time.Duration

	HookLookupByJob        *hooklookup.JobHookLookup
	HookLookupGlobal       hooklookup.HookLookupInterface
	JobTimeout             time.Duration
	MaxWorkers             int
	MiddlewareLookupGlobal middlewarelookup.MiddlewareLookupInterface

	// Notifier is a notifier for subscribing to new job inserts and job
	// control. If nil, the producer will operate in poll-only mode.
	Notifier *notifier.Notifier

	Queue string
	// QueueEventCallback gets called when a queue's config changes (such as
	// pausing or resuming) events can be emitted to subscriptions.
	QueueEventCallback func(event *Event)

	// QueuePollInterval is the amount of time between periodic checks for
	// queue setting changes. This is only used in poll-only mode (when no
	// notifier is provided).
	QueuePollInterval time.Duration
	// QueueReportInterval is the amount of time between periodic reports
	// of the queue status.
	QueueReportInterval time.Duration
	RetryPolicy         ClientRetryPolicy
	SchedulerInterval   time.Duration
	Workers             *Workers
}

func (c *producerConfig) mustValidate() *producerConfig {
	if c.Completer == nil {
		panic("producerConfig.Completer is required")
	}
	if c.ClientID == "" {
		panic("producerConfig.ClientID is required")
	}
	if c.FetchCooldown <= 0 {
		panic("producerConfig.FetchCooldown must be great than zero")
	}
	if c.FetchPollInterval <= 0 {
		panic("producerConfig.FetchPollInterval must be greater than zero")
	}
	if c.JobTimeout < -1 {
		panic("producerConfig.JobTimeout must be greater or equal to zero")
	}
	if c.MaxWorkers == 0 {
		panic("producerConfig.MaxWorkers is required")
	}
	if c.Queue == "" {
		panic("producerConfig.Queue is required")
	}
	if c.QueuePollInterval == 0 {
		c.QueuePollInterval = queuePollIntervalDefault
	}
	if c.QueuePollInterval <= 0 {
		panic("producerConfig.QueueSettingsPollInterval must be greater than zero")
	}
	if c.QueueReportInterval == 0 {
		c.QueueReportInterval = queueReportIntervalDefault
	}
	if c.QueueReportInterval <= 0 {
		panic("producerConfig.QueueSettingsReportInterval must be greater than zero")
	}
	if c.RetryPolicy == nil {
		panic("producerConfig.RetryPolicy is required")
	}
	if c.SchedulerInterval == 0 {
		panic("producerConfig.SchedulerInterval is required")
	}
	if c.Workers == nil {
		panic("producerConfig.Workers is required")
	}

	return c
}

// producer manages a fleet of Workers up to a maximum size. It periodically fetches jobs
// from the adapter and dispatches them to Workers. It receives completed job results from Workers.
//
// The producer never fetches more jobs than the number of free Worker slots it
// has available. This is not optimal for throughput compared to pre-fetching
// extra jobs, but it is better for smaller job counts or slower jobs where even
// distribution and minimizing execution latency is more important.
type producer struct {
	baseservice.BaseService
	startstop.BaseStartStop

	// Jobs which are currently being worked. Only used by main goroutine.
	activeJobs map[int64]*jobexecutor.JobExecutor

	completer    jobcompleter.JobCompleter
	config       *producerConfig
	exec         riverdriver.Executor
	errorHandler jobexecutor.ErrorHandler
	workers      *Workers

	// Receives job IDs to cancel. Written by notifier goroutine, only read from
	// main goroutine.
	cancelCh chan int64

	// Set to true when the producer thinks it should trigger another fetch as
	// soon as slots are available. This is written and read by the main
	// goroutine.
	fetchWhenSlotsAreAvailable bool

	// Receives completed jobs from workers. Written by completed workers, only
	// read from main goroutine.
	jobResultCh chan *rivertype.JobRow

	jobTimeout time.Duration

	// An atomic count of the number of jobs actively being worked on. This is
	// written to by the main goroutine, but read by the dispatcher.
	numJobsActive atomic.Int32

	numJobsRan atomic.Uint64
	paused     bool
	// Receives control messages from the notifier goroutine. Written by notifier
	// goroutine, only read from main goroutine.
	queueControlCh chan *jobControlPayload
	retryPolicy    ClientRetryPolicy
	testSignals    producerTestSignals
}

func newProducer(archetype *baseservice.Archetype, exec riverdriver.Executor, config *producerConfig) *producer {
	if archetype == nil {
		panic("archetype is required")
	}
	if exec == nil {
		panic("exec is required")
	}

	var errorHandler jobexecutor.ErrorHandler
	if config.ErrorHandler != nil {
		errorHandler = &errorHandlerAdapter{config.ErrorHandler}
	}

	return baseservice.Init(archetype, &producer{
		activeJobs:     make(map[int64]*jobexecutor.JobExecutor),
		cancelCh:       make(chan int64, 1000),
		completer:      config.Completer,
		config:         config.mustValidate(),
		exec:           exec,
		errorHandler:   errorHandler,
		jobResultCh:    make(chan *rivertype.JobRow, config.MaxWorkers),
		jobTimeout:     config.JobTimeout,
		queueControlCh: make(chan *jobControlPayload, 100),
		retryPolicy:    config.RetryPolicy,
		workers:        config.Workers,
	})
}

// Start starts the producer. It backgrounds a goroutine which is stopped when
// context is cancelled or Stop is invoked.
//
// This variant uses a single context as fetchCtx and workCtx, and is here to
// implement startstop.Service so that the producer can be stored as a service
// variable and used with various service utilities. StartWorkContext below
// should be preferred for production use.
func (p *producer) Start(ctx context.Context) error {
	return p.StartWorkContext(ctx, ctx)
}

func (p *producer) Stop() {
	p.Logger.Debug(p.Name + ": Stopping")
	p.BaseStartStop.Stop()
	p.Logger.Debug(p.Name + ": Stop returned")
}

// Start starts the producer. It backgrounds a goroutine which is stopped when
// context is cancelled or Stop is invoked.
//
// When fetchCtx is cancelled, no more jobs will be fetched; however, if a fetch
// is already in progress, It will be allowed to complete and run any fetched
// jobs. When workCtx is cancelled, any in-progress jobs will have their
// contexts cancelled too.
func (p *producer) StartWorkContext(fetchCtx, workCtx context.Context) error {
	fetchCtx, shouldStart, started, stopped := p.StartInit(fetchCtx)
	if !shouldStart {
		return nil
	}

	queue, err := func() (*rivertype.Queue, error) {
		ctx, cancel := context.WithTimeout(fetchCtx, 10*time.Second)
		defer cancel()

		p.Logger.DebugContext(ctx, p.Name+": Fetching initial queue settings", slog.String("queue", p.config.Queue))
		return p.exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{
			Metadata: []byte("{}"),
			Name:     p.config.Queue,
		})
	}()
	if err != nil {
		stopped()
		if errors.Is(err, startstop.ErrStop) || strings.HasSuffix(err.Error(), "conn closed") || fetchCtx.Err() != nil {
			return nil //nolint:nilerr
		}
		p.Logger.ErrorContext(fetchCtx, p.Name+": Error fetching initial queue settings", slog.String("err", err.Error()))
		return err
	}

	initiallyPaused := queue != nil && (queue.PausedAt != nil)
	p.paused = initiallyPaused

	// TODO: fetcher should have some jitter in it to avoid stampeding issues.
	fetchLimiter := chanutil.NewDebouncedChan(fetchCtx, p.config.FetchCooldown, true)

	var (
		controlSub *notifier.Subscription
		insertSub  *notifier.Subscription
	)
	if p.config.Notifier == nil {
		p.Logger.DebugContext(fetchCtx, p.Name+": No notifier configured; starting in poll mode", "client_id", p.config.ClientID)

		go p.pollForSettingChanges(fetchCtx, initiallyPaused)
	} else {
		var err error

		handleInsertNotification := func(topic notifier.NotificationTopic, payload string) {
			var decoded insertPayload
			if err := json.Unmarshal([]byte(payload), &decoded); err != nil {
				p.Logger.ErrorContext(workCtx, p.Name+": Failed to unmarshal insert notification payload", slog.String("err", err.Error()))
				return
			}
			if decoded.Queue != p.config.Queue {
				return
			}
			p.Logger.DebugContext(workCtx, p.Name+": Received insert notification", slog.String("queue", decoded.Queue))
			fetchLimiter.Call()
		}
		insertSub, err = p.config.Notifier.Listen(fetchCtx, notifier.NotificationTopicInsert, handleInsertNotification)
		if err != nil {
			stopped()
			if strings.HasSuffix(err.Error(), "conn closed") || errors.Is(err, context.Canceled) {
				return nil
			}
			return err
		}

		controlSub, err = p.config.Notifier.Listen(fetchCtx, notifier.NotificationTopicControl, p.handleControlNotification(workCtx))
		if err != nil {
			stopped()
			if strings.HasSuffix(err.Error(), "conn closed") || errors.Is(err, context.Canceled) {
				return nil
			}
			return err
		}
	}

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		p.Logger.DebugContext(fetchCtx, p.Name+": Run loop started", slog.String("queue", p.config.Queue), slog.Bool("paused", p.paused))
		defer func() {
			p.Logger.DebugContext(fetchCtx, p.Name+": Run loop stopped", slog.String("queue", p.config.Queue), slog.Uint64("num_completed_jobs", p.numJobsRan.Load()))
		}()

		if insertSub != nil {
			defer insertSub.Unlisten(fetchCtx)
		}

		if controlSub != nil {
			defer controlSub.Unlisten(fetchCtx)
		}

		go p.heartbeatLogLoop(fetchCtx)
		go p.reportQueueStatusLoop(fetchCtx)
		p.fetchAndRunLoop(fetchCtx, workCtx, fetchLimiter)

		p.executorShutdownLoop()
	}()

	return nil
}

type controlAction string

const (
	controlActionCancel controlAction = "cancel"
	controlActionPause  controlAction = "pause"
	controlActionResume controlAction = "resume"
)

type jobControlPayload struct {
	Action controlAction `json:"action"`
	JobID  int64         `json:"job_id"`
	Queue  string        `json:"queue"`
}

type insertPayload struct {
	Queue string `json:"queue"`
}

func (p *producer) handleControlNotification(workCtx context.Context) func(notifier.NotificationTopic, string) {
	return func(topic notifier.NotificationTopic, payload string) {
		var decoded jobControlPayload
		if err := json.Unmarshal([]byte(payload), &decoded); err != nil {
			p.Logger.ErrorContext(workCtx, p.Name+": Failed to unmarshal job control notification payload", slog.String("err", err.Error()))
			return
		}

		switch decoded.Action {
		case controlActionPause, controlActionResume:
			if decoded.Queue != rivercommon.AllQueuesString && decoded.Queue != p.config.Queue {
				p.Logger.DebugContext(workCtx, p.Name+": Queue control notification for other queue", slog.String("action", string(decoded.Action)))
				return
			}
			select {
			case <-workCtx.Done():
			case p.queueControlCh <- &decoded:
			default:
				p.Logger.WarnContext(workCtx, p.Name+": Queue control notification dropped due to full buffer", slog.String("action", string(decoded.Action)))
			}
		case controlActionCancel:
			if decoded.Queue != p.config.Queue {
				p.Logger.DebugContext(workCtx, p.Name+": Received job cancel notification for other queue",
					slog.String("action", string(decoded.Action)),
					slog.Int64("job_id", decoded.JobID),
					slog.String("queue", decoded.Queue),
				)
				return
			}
			select {
			case <-workCtx.Done():
			case p.cancelCh <- decoded.JobID:
			default:
				p.Logger.WarnContext(workCtx, p.Name+": Job cancel notification dropped due to full buffer", slog.Int64("job_id", decoded.JobID))
			}
		default:
			p.Logger.DebugContext(workCtx, p.Name+": Received job control notification with unknown action",
				slog.String("action", string(decoded.Action)),
				slog.Int64("job_id", decoded.JobID),
				slog.String("queue", decoded.Queue),
			)
		}
	}
}

func (p *producer) fetchAndRunLoop(fetchCtx, workCtx context.Context, fetchLimiter *chanutil.DebouncedChan) {
	// Prime the fetchLimiter so we can make an initial fetch without waiting for
	// an insert notification or a fetch poll.
	fetchLimiter.Call()

	fetchPollTimer := time.NewTimer(p.config.FetchPollInterval)
	go func() {
		for {
			select {
			case <-fetchCtx.Done():
				// Stop fetch timer so no more fetches are triggered.
				if !fetchPollTimer.Stop() {
					<-fetchPollTimer.C
				}
				return
			case <-fetchPollTimer.C:
				fetchLimiter.Call()
				fetchPollTimer.Reset(p.config.FetchPollInterval)
			}
		}
	}()

	fetchResultCh := make(chan producerFetchResult)
	for {
		select {
		case <-fetchCtx.Done():
			return
		case msg := <-p.queueControlCh:
			switch msg.Action {
			case controlActionPause:
				if p.paused {
					continue
				}
				p.paused = true
				p.Logger.DebugContext(workCtx, p.Name+": Paused", slog.String("queue", p.config.Queue), slog.String("queue_in_message", msg.Queue))
				p.testSignals.Paused.Signal(struct{}{})
				if p.config.QueueEventCallback != nil {
					p.config.QueueEventCallback(&Event{Kind: EventKindQueuePaused, Queue: &rivertype.Queue{Name: p.config.Queue}})
				}
			case controlActionResume:
				if !p.paused {
					continue
				}
				p.paused = false
				p.Logger.DebugContext(workCtx, p.Name+": Resumed", slog.String("queue", p.config.Queue), slog.String("queue_in_message", msg.Queue))
				p.testSignals.Resumed.Signal(struct{}{})
				if p.config.QueueEventCallback != nil {
					p.config.QueueEventCallback(&Event{Kind: EventKindQueueResumed, Queue: &rivertype.Queue{Name: p.config.Queue}})
				}
			case controlActionCancel:
				// Separate this case to make linter happy:
				p.Logger.DebugContext(workCtx, p.Name+": Unhandled queue control action", "action", msg.Action)
			default:
				p.Logger.DebugContext(workCtx, p.Name+": Unknown queue control action", "action", msg.Action)
			}
		case jobID := <-p.cancelCh:
			p.maybeCancelJob(jobID)
		case <-fetchLimiter.C():
			if p.paused {
				continue
			}
			p.innerFetchLoop(workCtx, fetchResultCh)
			// Ensure we can't start another fetch when fetchCtx is done, even if
			// the fetchLimiter is also ready to fire:
			select {
			case <-fetchCtx.Done():
				return
			default:
			}
		case result := <-p.jobResultCh:
			p.removeActiveJob(result.ID)
			if p.fetchWhenSlotsAreAvailable {
				// If we missed a fetch because all worker slots were full, or if we
				// fetched the maximum number of jobs on the last attempt, get a little
				// more aggressive triggering the fetch limiter now that we have a slot
				// available.
				p.fetchWhenSlotsAreAvailable = false
				fetchLimiter.Call()
			}
		}
	}
}

func (p *producer) innerFetchLoop(workCtx context.Context, fetchResultCh chan producerFetchResult) {
	limit := p.maxJobsToFetch()
	if limit <= 0 {
		// We have no slots for new jobs, so don't bother fetching. However, since
		// we knew it was time to fetch, we keep track of what happened so we can
		// trigger another fetch as soon as we have open slots.
		p.fetchWhenSlotsAreAvailable = true
		return
	}

	go p.dispatchWork(workCtx, limit, fetchResultCh)

	for {
		select {
		case result := <-fetchResultCh:
			if result.err != nil {
				p.Logger.ErrorContext(workCtx, p.Name+": Error fetching jobs", slog.String("err", result.err.Error()))
			} else if len(result.jobs) > 0 {
				p.startNewExecutors(workCtx, result.jobs)

				if len(result.jobs) == limit {
					// Fetch returned the maximum number of jobs that were requested,
					// implying there may be more in the queue. Trigger another fetch when
					// slots are available.
					p.fetchWhenSlotsAreAvailable = true
				}
			}
			return
		case result := <-p.jobResultCh:
			p.removeActiveJob(result.ID)
		case jobID := <-p.cancelCh:
			p.maybeCancelJob(jobID)
		}
	}
}

func (p *producer) executorShutdownLoop() {
	// No more jobs will be fetched or executed. However, we must wait for all
	// in-progress jobs to complete.
	for {
		if len(p.activeJobs) == 0 {
			break
		}
		result := <-p.jobResultCh
		p.removeActiveJob(result.ID)
	}
}

func (p *producer) addActiveJob(id int64, executor *jobexecutor.JobExecutor) {
	p.numJobsActive.Add(1)
	p.activeJobs[id] = executor
}

func (p *producer) removeActiveJob(id int64) {
	delete(p.activeJobs, id)
	p.numJobsActive.Add(-1)
	p.numJobsRan.Add(1)
}

func (p *producer) maybeCancelJob(id int64) {
	executor, ok := p.activeJobs[id]
	if !ok {
		return
	}
	executor.Cancel()
}

func (p *producer) dispatchWork(workCtx context.Context, count int, fetchResultCh chan<- producerFetchResult) {
	// This intentionally removes any deadlines or cancellation from the parent
	// context because we don't want it to get cancelled if the producer is asked
	// to shut down. In that situation, we want to finish fetching any jobs we are
	// in the midst of fetching, work them, and then stop. Otherwise we'd have a
	// risk of shutting down when we had already fetched jobs in the database,
	// leaving those jobs stranded. We'd then potentially have to release them
	// back to the queue.
	jobs, err := p.exec.JobGetAvailable(context.WithoutCancel(workCtx), &riverdriver.JobGetAvailableParams{
		AttemptedBy: p.config.ClientID,
		Max:         count,
		Queue:       p.config.Queue,
	})
	if err != nil {
		fetchResultCh <- producerFetchResult{err: err}
		return
	}
	fetchResultCh <- producerFetchResult{jobs: jobs}
}

// Periodically logs an informational log line giving some insight into the
// current state of the producer.
func (p *producer) heartbeatLogLoop(ctx context.Context) {
	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()
	type jobCount struct {
		ran    uint64
		active int
	}
	var prevCount jobCount
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			curCount := jobCount{ran: p.numJobsRan.Load(), active: int(p.numJobsActive.Load())}
			if curCount != prevCount {
				p.Logger.InfoContext(ctx, p.Name+": Producer job counts",
					slog.Uint64("num_completed_jobs", curCount.ran),
					slog.Int("num_jobs_running", curCount.active),
					slog.String("queue", p.config.Queue),
				)
			}
			prevCount = curCount
		}
	}
}

func (p *producer) startNewExecutors(workCtx context.Context, jobs []*rivertype.JobRow) {
	for _, job := range jobs {
		workInfo, ok := p.workers.workersMap[job.Kind]

		var workUnit workunit.WorkUnit
		if ok {
			workUnit = workInfo.workUnitFactory.MakeUnit(job)
		}

		// jobCancel will always be called by the executor to prevent leaks.
		jobCtx, jobCancel := context.WithCancelCause(workCtx)

		executor := baseservice.Init(&p.Archetype, &jobexecutor.JobExecutor{
			CancelFunc:               jobCancel,
			ClientJobTimeout:         p.jobTimeout,
			ClientRetryPolicy:        p.retryPolicy,
			Completer:                p.completer,
			DefaultClientRetryPolicy: &DefaultClientRetryPolicy{},
			ErrorHandler:             p.errorHandler,
			HookLookupByJob:          p.config.HookLookupByJob,
			HookLookupGlobal:         p.config.HookLookupGlobal,
			MiddlewareLookupGlobal:   p.config.MiddlewareLookupGlobal,
			InformProducerDoneFunc:   p.handleWorkerDone,
			JobRow:                   job,
			SchedulerInterval:        p.config.SchedulerInterval,
			WorkUnit:                 workUnit,
		})
		p.addActiveJob(job.ID, executor)

		go executor.Execute(jobCtx)
	}

	p.Logger.DebugContext(workCtx, p.Name+": Distributed batch of jobs to executors", "num_jobs", len(jobs))

	p.testSignals.StartedExecutors.Signal(struct{}{})
}

func (p *producer) maxJobsToFetch() int {
	return p.config.MaxWorkers - int(p.numJobsActive.Load())
}

func (p *producer) handleWorkerDone(job *rivertype.JobRow) {
	p.jobResultCh <- job
}

func (p *producer) pollForSettingChanges(ctx context.Context, lastPaused bool) {
	ticker := time.NewTicker(p.config.QueuePollInterval)
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			updatedQueue, err := p.fetchQueueSettings(ctx)
			if err != nil {
				p.Logger.ErrorContext(ctx, p.Name+": Error fetching queue settings", slog.String("err", err.Error()))
				continue
			}
			shouldBePaused := (updatedQueue.PausedAt != nil)
			if lastPaused != shouldBePaused {
				action := controlActionPause
				if !shouldBePaused {
					action = controlActionResume
				}
				payload := &jobControlPayload{
					Action: action,
					Queue:  p.config.Queue,
				}
				p.Logger.DebugContext(ctx, p.Name+": Queue control state changed from polling",
					slog.String("queue", p.config.Queue),
					slog.String("action", string(action)),
					slog.Bool("paused", shouldBePaused),
				)

				select {
				case p.queueControlCh <- payload:
					lastPaused = shouldBePaused
				default:
					p.Logger.WarnContext(ctx, p.Name+": Queue control notification dropped due to full buffer", slog.String("action", string(action)))
				}
			}
			p.testSignals.PolledQueueConfig.Signal(struct{}{})
		}
	}
}

func (p *producer) fetchQueueSettings(ctx context.Context) (*rivertype.Queue, error) {
	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	return p.exec.QueueGet(ctx, p.config.Queue)
}

func (p *producer) reportQueueStatusLoop(ctx context.Context) {
	serviceutil.CancellableSleep(ctx, randutil.DurationBetween(0, time.Second))
	reportTicker := time.NewTicker(p.config.QueueReportInterval)
	for {
		select {
		case <-ctx.Done():
			reportTicker.Stop()
			return
		case <-reportTicker.C:
			p.reportQueueStatusOnce(ctx)
		}
	}
}

func (p *producer) reportQueueStatusOnce(ctx context.Context) {
	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	p.Logger.DebugContext(ctx, p.Name+": Reporting queue status", slog.String("queue", p.config.Queue))
	_, err := p.exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{
		Metadata: []byte("{}"),
		Name:     p.config.Queue,
	})
	if err != nil && errors.Is(context.Cause(ctx), startstop.ErrStop) {
		return
	}
	if err != nil {
		p.Logger.ErrorContext(ctx, p.Name+": Queue status update, error updating in database", slog.String("err", err.Error()))
		return
	}
	p.testSignals.ReportedQueueStatus.Signal(struct{}{})
}

type producerFetchResult struct {
	jobs []*rivertype.JobRow
	err  error
}

type errorHandlerAdapter struct {
	errorHandler ErrorHandler
}

func (e *errorHandlerAdapter) HandleError(ctx context.Context, job *rivertype.JobRow, err error) *jobexecutor.ErrorHandlerResult {
	result := e.errorHandler.HandleError(ctx, job, err)
	return (*jobexecutor.ErrorHandlerResult)(result)
}

func (e *errorHandlerAdapter) HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *jobexecutor.ErrorHandlerResult {
	result := e.errorHandler.HandlePanic(ctx, job, panicVal, trace)
	return (*jobexecutor.ErrorHandlerResult)(result)
}

```

`producer_test.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"fmt"
	"slices"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/maintenance"
	"github.com/riverqueue/river/internal/middlewarelookup"
	"github.com/riverqueue/river/internal/notifier"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/riverinternaltest/sharedtx"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riverpilot"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

const testClientID = "test-client-id"

func Test_Producer_CanSafelyCompleteJobsWhileFetchingNewOnes(t *testing.T) {
	// We have encountered previous data races with the list of active jobs on
	// Producer because we need to know the count of active jobs in order to
	// determine how many we can fetch for the next batch, while we're managing
	// the map of active jobs in a different goroutine.
	//
	// This test attempts to exercise that race condition so that the race
	// detector can tell us if we're protected against it.
	t.Parallel()

	ctx := context.Background()
	require := require.New(t)
	dbPool := riverinternaltest.TestDB(ctx, t)

	const maxJobCount = 10000
	// This doesn't strictly mean that there are no more jobs left to process,
	// merely that the final job we inserted is now being processed, which is
	// close enough for our purposes here.
	lastJobRun := make(chan struct{})

	archetype := riversharedtest.BaseServiceArchetype(t)

	config := newTestConfig(t, nil)
	dbDriver := riverpgxv5.New(dbPool)
	exec := dbDriver.GetExecutor()
	listener := dbDriver.GetListener()

	subscribeCh := make(chan []jobcompleter.CompleterJobUpdated, 100)
	t.Cleanup(riverinternaltest.DiscardContinuously(subscribeCh))

	completer := jobcompleter.NewInlineCompleter(archetype, exec, &riverpilot.StandardPilot{}, subscribeCh)
	t.Cleanup(completer.Stop)

	type WithJobNumArgs struct {
		JobArgsReflectKind[WithJobNumArgs]
		JobNum int `json:"job_num"`
	}

	workers := NewWorkers()
	AddWorker(workers, WorkFunc(func(ctx context.Context, job *Job[WithJobNumArgs]) error {
		var jobArgs WithJobNumArgs
		require.NoError(json.Unmarshal(job.EncodedArgs, &jobArgs))

		if jobArgs.JobNum == maxJobCount-1 {
			select {
			case <-ctx.Done():
			case lastJobRun <- struct{}{}:
			}
		}
		return nil
	}))

	notifier := notifier.New(archetype, listener)

	producer := newProducer(archetype, exec, &producerConfig{
		ClientID:     testClientID,
		Completer:    completer,
		ErrorHandler: newTestErrorHandler(),
		// Fetch constantly to more aggressively trigger the potential data race:
		FetchCooldown:          time.Millisecond,
		FetchPollInterval:      time.Millisecond,
		HookLookupByJob:        hooklookup.NewJobHookLookup(),
		HookLookupGlobal:       hooklookup.NewHookLookup(nil),
		JobTimeout:             JobTimeoutDefault,
		MaxWorkers:             1000,
		MiddlewareLookupGlobal: middlewarelookup.NewMiddlewareLookup(nil),
		Notifier:               notifier,
		Queue:                  rivercommon.QueueDefault,
		QueuePollInterval:      queuePollIntervalDefault,
		QueueReportInterval:    queueReportIntervalDefault,
		RetryPolicy:            &DefaultClientRetryPolicy{},
		SchedulerInterval:      maintenance.JobSchedulerIntervalDefault,
		Workers:                workers,
	})

	params := make([]*riverdriver.JobInsertFastParams, maxJobCount)
	for i := range params {
		insertParams, err := insertParamsFromConfigArgsAndOptions(archetype, config, WithJobNumArgs{JobNum: i}, nil)
		require.NoError(err)

		params[i] = (*riverdriver.JobInsertFastParams)(insertParams)
	}

	ctx, cancel := context.WithTimeout(context.Background(), 20*time.Second)
	t.Cleanup(cancel)

	go func() {
		// The producer should never exceed its MaxWorkerCount. If it does, panic so
		// we can get a trace.
		for {
			select {
			case <-ctx.Done():
				return
			default:
			}
			numActiveJobs := producer.numJobsActive.Load()
			if int(numActiveJobs) > producer.config.MaxWorkers {
				panic(fmt.Sprintf("producer exceeded MaxWorkerCount=%d, actual count=%d", producer.config.MaxWorkers, numActiveJobs))
			}
		}
	}()

	_, err := exec.JobInsertFastMany(ctx, params)
	require.NoError(err)

	require.NoError(producer.StartWorkContext(ctx, ctx))
	t.Cleanup(producer.Stop)

	select {
	case <-lastJobRun:
		t.Logf("Last job reported in; cancelling context")
		cancel()
	case <-ctx.Done():
		t.Error("timed out waiting for last job to run")
	}
}

func TestProducer_PollOnly(t *testing.T) {
	t.Parallel()

	testProducer(t, func(ctx context.Context, t *testing.T) (*producer, chan []jobcompleter.CompleterJobUpdated) {
		t.Helper()

		var (
			archetype = riversharedtest.BaseServiceArchetype(t)
			driver    = riverpgxv5.New(nil)
			tx        = riverinternaltest.TestTx(ctx, t)
		)

		// Wrap with a shared transaction because the producer fetching jobs may
		// conflict with jobs being inserted in test cases.
		tx = sharedtx.NewSharedTx(tx)

		var (
			exec       = driver.UnwrapExecutor(tx)
			jobUpdates = make(chan []jobcompleter.CompleterJobUpdated, 10)
		)

		completer := jobcompleter.NewInlineCompleter(archetype, exec, &riverpilot.StandardPilot{}, jobUpdates)
		{
			require.NoError(t, completer.Start(ctx))
			t.Cleanup(completer.Stop)
		}

		return newProducer(archetype, exec, &producerConfig{
			ClientID:               testClientID,
			Completer:              completer,
			ErrorHandler:           newTestErrorHandler(),
			FetchCooldown:          FetchCooldownDefault,
			FetchPollInterval:      50 * time.Millisecond, // more aggressive than normal because we have no notifier
			HookLookupByJob:        hooklookup.NewJobHookLookup(),
			HookLookupGlobal:       hooklookup.NewHookLookup(nil),
			JobTimeout:             JobTimeoutDefault,
			MaxWorkers:             1_000,
			MiddlewareLookupGlobal: middlewarelookup.NewMiddlewareLookup(nil),
			Notifier:               nil, // no notifier
			Queue:                  rivercommon.QueueDefault,
			QueuePollInterval:      queuePollIntervalDefault,
			QueueReportInterval:    queueReportIntervalDefault,
			RetryPolicy:            &DefaultClientRetryPolicy{},
			SchedulerInterval:      riverinternaltest.SchedulerShortInterval,
			Workers:                NewWorkers(),
		}), jobUpdates
	})
}

func TestProducer_WithNotifier(t *testing.T) {
	t.Parallel()

	testProducer(t, func(ctx context.Context, t *testing.T) (*producer, chan []jobcompleter.CompleterJobUpdated) {
		t.Helper()

		var (
			archetype  = riversharedtest.BaseServiceArchetype(t)
			dbPool     = riverinternaltest.TestDB(ctx, t)
			driver     = riverpgxv5.New(dbPool)
			exec       = driver.GetExecutor()
			jobUpdates = make(chan []jobcompleter.CompleterJobUpdated, 10)
			listener   = driver.GetListener()
		)

		completer := jobcompleter.NewInlineCompleter(archetype, exec, &riverpilot.StandardPilot{}, jobUpdates)
		{
			require.NoError(t, completer.Start(ctx))
			t.Cleanup(completer.Stop)
		}

		notifier := notifier.New(archetype, listener)
		{
			require.NoError(t, notifier.Start(ctx))
			t.Cleanup(notifier.Stop)
		}

		return newProducer(archetype, exec, &producerConfig{
			ClientID:               testClientID,
			Completer:              completer,
			ErrorHandler:           newTestErrorHandler(),
			FetchCooldown:          FetchCooldownDefault,
			FetchPollInterval:      50 * time.Millisecond, // more aggressive than normal so in case we miss the event, tests still pass quickly
			HookLookupByJob:        hooklookup.NewJobHookLookup(),
			HookLookupGlobal:       hooklookup.NewHookLookup(nil),
			JobTimeout:             JobTimeoutDefault,
			MaxWorkers:             1_000,
			MiddlewareLookupGlobal: middlewarelookup.NewMiddlewareLookup(nil),
			Notifier:               notifier,
			Queue:                  rivercommon.QueueDefault,
			QueuePollInterval:      queuePollIntervalDefault,
			QueueReportInterval:    queueReportIntervalDefault,
			RetryPolicy:            &DefaultClientRetryPolicy{},
			SchedulerInterval:      riverinternaltest.SchedulerShortInterval,
			Workers:                NewWorkers(),
		}), jobUpdates
	})
}

func testProducer(t *testing.T, makeProducer func(ctx context.Context, t *testing.T) (*producer, chan []jobcompleter.CompleterJobUpdated)) {
	t.Helper()

	ctx := context.Background()

	type testBundle struct {
		archetype       *baseservice.Archetype
		completer       jobcompleter.JobCompleter
		config          *Config
		exec            riverdriver.Executor
		jobUpdates      chan jobcompleter.CompleterJobUpdated
		timeBeforeStart time.Time
		workers         *Workers
	}

	setup := func(t *testing.T) (*producer, *testBundle) {
		t.Helper()

		timeBeforeStart := time.Now().UTC()

		producer, jobUpdates := makeProducer(ctx, t)
		producer.testSignals.Init()
		config := newTestConfig(t, nil)

		jobUpdatesFlattened := make(chan jobcompleter.CompleterJobUpdated, 10)
		go func() {
			for updates := range jobUpdates {
				for _, update := range updates {
					jobUpdatesFlattened <- update
				}
			}
		}()

		return producer, &testBundle{
			archetype:       &producer.Archetype,
			completer:       producer.completer,
			config:          config,
			exec:            producer.exec,
			jobUpdates:      jobUpdatesFlattened,
			timeBeforeStart: timeBeforeStart,
			workers:         producer.workers,
		}
	}

	mustInsert := func(ctx context.Context, t *testing.T, bundle *testBundle, args JobArgs) {
		t.Helper()

		insertParams, err := insertParamsFromConfigArgsAndOptions(bundle.archetype, bundle.config, args, nil)
		require.NoError(t, err)
		if insertParams.ScheduledAt == nil {
			// Without this, newly inserted jobs will pick up a scheduled_at time
			// that's the current Go time at the time of insertion. If the test is
			// using a transaction, this will be after the `now()` time in the
			// transaction that gets used by default in `JobGetAvailable`, so new jobs
			// won't be visible.
			//
			// To work around this, set all inserted jobs to a time before the start
			// of the test to ensure they're visible.
			insertParams.ScheduledAt = &bundle.timeBeforeStart
		}

		_, err = bundle.exec.JobInsertFastMany(ctx, []*riverdriver.JobInsertFastParams{(*riverdriver.JobInsertFastParams)(insertParams)})
		require.NoError(t, err)
	}

	startProducer := func(t *testing.T, fetchCtx, workCtx context.Context, producer *producer) {
		t.Helper()

		require.NoError(t, producer.StartWorkContext(fetchCtx, workCtx))
		t.Cleanup(producer.Stop)
	}

	t.Run("NoOp", func(t *testing.T) {
		t.Parallel()

		producer, _ := setup(t)

		startProducer(t, ctx, ctx, producer)
	})

	t.Run("SimpleJob", func(t *testing.T) {
		t.Parallel()

		producer, bundle := setup(t)
		AddWorker(bundle.workers, &noOpWorker{})

		mustInsert(ctx, t, bundle, &noOpArgs{})

		startProducer(t, ctx, ctx, producer)

		update := riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)
		require.Equal(t, rivertype.JobStateCompleted, update.Job.State)
	})

	t.Run("RegistersQueueStatus", func(t *testing.T) {
		t.Parallel()

		producer, bundle := setup(t)
		producer.config.QueueReportInterval = 50 * time.Millisecond

		now := time.Now().UTC()
		startProducer(t, ctx, ctx, producer)

		queue, err := bundle.exec.QueueGet(ctx, rivercommon.QueueDefault)
		require.NoError(t, err)
		require.WithinDuration(t, now, queue.CreatedAt, 2*time.Second)
		require.Equal(t, []byte("{}"), queue.Metadata)
		require.Equal(t, rivercommon.QueueDefault, queue.Name)
		require.WithinDuration(t, now, queue.UpdatedAt, 2*time.Second)
		require.Equal(t, queue.CreatedAt, queue.UpdatedAt)

		// Queue status should be updated quickly:
		producer.testSignals.ReportedQueueStatus.WaitOrTimeout()
	})

	t.Run("UnknownJobKind", func(t *testing.T) {
		t.Parallel()

		producer, bundle := setup(t)
		AddWorker(bundle.workers, &noOpWorker{})

		mustInsert(ctx, t, bundle, &noOpArgs{})
		mustInsert(ctx, t, bundle, &callbackArgs{}) // not registered

		startProducer(t, ctx, ctx, producer)

		updates := riversharedtest.WaitOrTimeoutN(t, bundle.jobUpdates, 2)

		// Print updated jobs for debugging.
		for _, update := range updates {
			t.Logf("Job: %+v", update.Job)
		}

		// Order jobs come back in is not guaranteed, which is why this is
		// written somewhat strangely.
		findJob := func(kind string) *rivertype.JobRow {
			index := slices.IndexFunc(updates, func(u jobcompleter.CompleterJobUpdated) bool { return u.Job.Kind == kind })
			require.NotEqualf(t, -1, index, "Job update not found for kind: %s", kind)
			return updates[index].Job
		}

		{
			job := findJob((&callbackArgs{}).Kind())
			require.Equal(t, rivertype.JobStateRetryable, job.State)
			require.Equal(t, (&UnknownJobKindError{Kind: (&callbackArgs{}).Kind()}).Error(), job.Errors[0].Error)
		}
		{
			job := findJob((&noOpArgs{}).Kind())
			require.Equal(t, rivertype.JobStateCompleted, job.State)
		}
	})

	t.Run("CancelledWorkContextCancelsJob", func(t *testing.T) {
		t.Parallel()

		producer, bundle := setup(t)

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		AddWorker(bundle.workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			producer.Logger.InfoContext(ctx, "Job started")
			<-ctx.Done()
			producer.Logger.InfoContext(ctx, "Job stopped after context cancelled")
			return ctx.Err()
		}))

		workCtx, workCancel := context.WithCancel(ctx)
		defer workCancel()

		mustInsert(ctx, t, bundle, &JobArgs{})

		startProducer(t, ctx, workCtx, producer)

		workCancel()

		update := riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)
		require.Equal(t, rivertype.JobStateRetryable, update.Job.State)
	})

	t.Run("MaxWorkers", func(t *testing.T) {
		t.Parallel()

		const (
			maxWorkers = 5
			numJobs    = 10
		)

		producer, bundle := setup(t)
		producer.config.MaxWorkers = maxWorkers

		type JobArgs struct {
			JobArgsReflectKind[JobArgs]
		}

		unpauseWorkers := make(chan struct{})
		defer close(unpauseWorkers)

		AddWorker(bundle.workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			t.Logf("Job paused")
			<-unpauseWorkers
			t.Logf("Job unpaused")
			return ctx.Err()
		}))

		for range numJobs {
			mustInsert(ctx, t, bundle, &JobArgs{})
		}

		startProducer(t, ctx, ctx, producer)

		producer.testSignals.StartedExecutors.WaitOrTimeout()

		// Jobs are still paused as we fetch updated job states.
		updatedJobs, err := bundle.exec.JobGetByKindMany(ctx, []string{(&JobArgs{}).Kind()})
		require.NoError(t, err)

		jobStateCounts := make(map[rivertype.JobState]int)

		for _, updatedJob := range updatedJobs {
			jobStateCounts[updatedJob.State]++
		}

		require.Equal(t, maxWorkers, jobStateCounts[rivertype.JobStateRunning])
		require.Equal(t, numJobs-maxWorkers, jobStateCounts[rivertype.JobStateAvailable])

		require.Equal(t, maxWorkers, int(producer.numJobsActive.Load()))
		require.Zero(t, producer.maxJobsToFetch()) // zero because all slots are occupied
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		producer, _ := setup(t)
		producer.Logger = riversharedtest.LoggerWarn(t) // loop started/stop log is very noisy; suppress
		producer.testSignals = producerTestSignals{}    // deinit so channels don't fill

		startstoptest.Stress(ctx, t, producer)
	})

	t.Run("QueuePausedBeforeStart", func(t *testing.T) {
		t.Parallel()

		producer, bundle := setup(t)
		AddWorker(bundle.workers, &noOpWorker{})

		testfactory.Queue(ctx, t, bundle.exec, &testfactory.QueueOpts{
			Name:     ptrutil.Ptr(rivercommon.QueueDefault),
			PausedAt: ptrutil.Ptr(time.Now()),
		})

		mustInsert(ctx, t, bundle, &noOpArgs{})

		startProducer(t, ctx, ctx, producer)

		select {
		case update := <-bundle.jobUpdates:
			t.Fatalf("Unexpected job update: job=%+v stats=%+v", update.Job, update.JobStats)
		case <-time.After(500 * time.Millisecond):
		}
	})

	testQueuePause := func(t *testing.T, queueNameToPause string) {
		t.Helper()
		t.Parallel()

		producer, bundle := setup(t)
		producer.config.QueuePollInterval = 50 * time.Millisecond
		AddWorker(bundle.workers, &noOpWorker{})

		mustInsert(ctx, t, bundle, &noOpArgs{})

		startProducer(t, ctx, ctx, producer)

		// First job should be executed immediately while resumed:
		update := riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)
		require.Equal(t, rivertype.JobStateCompleted, update.Job.State)

		// Pause the queue and wait for confirmation:
		require.NoError(t, bundle.exec.QueuePause(ctx, queueNameToPause))
		if producer.config.Notifier != nil {
			// also emit notification:
			emitQueueNotification(t, ctx, bundle.exec, queueNameToPause, "pause")
		}
		producer.testSignals.Paused.WaitOrTimeout()

		// Job should not be executed while paused:
		mustInsert(ctx, t, bundle, &noOpArgs{})

		select {
		case update := <-bundle.jobUpdates:
			t.Fatalf("Unexpected job update: %+v", update)
		case <-time.After(500 * time.Millisecond):
		}

		// Resume the queue and wait for confirmation:
		require.NoError(t, bundle.exec.QueueResume(ctx, queueNameToPause))
		if producer.config.Notifier != nil {
			// also emit notification:
			emitQueueNotification(t, ctx, bundle.exec, queueNameToPause, "resume")
		}
		producer.testSignals.Resumed.WaitOrTimeout()

		// Now the 2nd job should execute:
		update = riversharedtest.WaitOrTimeout(t, bundle.jobUpdates)
		require.Equal(t, rivertype.JobStateCompleted, update.Job.State)
	}

	t.Run("QueuePausedDuringOperation", func(t *testing.T) {
		testQueuePause(t, rivercommon.QueueDefault)
	})

	t.Run("QueuePausedAndResumedDuringOperationUsing*", func(t *testing.T) {
		testQueuePause(t, rivercommon.AllQueuesString)
	})

	t.Run("QueueDeletedFromRiverQueueTableDuringOperation", func(t *testing.T) {
		t.Parallel()

		producer, bundle := setup(t)
		producer.config.QueuePollInterval = time.Second
		producer.config.QueueReportInterval = time.Second

		startProducer(t, ctx, ctx, producer)

		// Delete the queue by using a future-dated horizon:
		_, err := bundle.exec.QueueDeleteExpired(ctx, &riverdriver.QueueDeleteExpiredParams{
			Max:              100,
			UpdatedAtHorizon: time.Now().Add(time.Minute),
		})
		require.NoError(t, err)

		producer.testSignals.ReportedQueueStatus.WaitOrTimeout()
		if producer.config.Notifier == nil {
			producer.testSignals.PolledQueueConfig.WaitOrTimeout()
		}
	})
}

func emitQueueNotification(t *testing.T, ctx context.Context, exec riverdriver.Executor, queue, action string) {
	t.Helper()
	err := exec.NotifyMany(ctx, &riverdriver.NotifyManyParams{
		Topic: string(notifier.NotificationTopicControl),
		Payload: []string{
			fmt.Sprintf(`{"queue":"%s","action":"%s"}`, queue, action),
		},
	})
	require.NoError(t, err)
}

```

`queue_list_params.go`:

```go
package river

// QueueListParams specifies the parameters for a QueueList query. It must be
// initialized with NewQueueListParams. Params can be built by chaining methods
// on the QueueListParams object:
//
//	params := NewQueueListParams().First(100)
type QueueListParams struct {
	paginationCount int32
}

// NewQueueListParams creates a new QueueListParams to return available jobs
// sorted by time in ascending order, returning 100 jobs at most.
func NewQueueListParams() *QueueListParams {
	return &QueueListParams{
		paginationCount: 100,
	}
}

func (p *QueueListParams) copy() *QueueListParams {
	return &QueueListParams{
		paginationCount: p.paginationCount,
	}
}

// First returns an updated filter set that will only return the first count
// queues.
//
// Count must be between 1 and 10000, inclusive, or this will panic.
func (p *QueueListParams) First(count int) *QueueListParams {
	if count <= 0 {
		panic("count must be > 0")
	}
	if count > 10000 {
		panic("count must be <= 10000")
	}
	result := p.copy()
	result.paginationCount = int32(count)
	return result
}

```

`queue_pause_opts.go`:

```go
package river

// QueuePauseOpts are optional settings for pausing or resuming a queue.
type QueuePauseOpts struct{}

```

`recorded_output.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"

	"github.com/riverqueue/river/internal/jobexecutor"
	"github.com/riverqueue/river/rivertype"
)

const maxOutputSize = 32 * 1024 * 1024 // 32MB

// RecordOutput records output JSON from a job. The "output" can be any
// JSON-encodable value and will be stored in the database on the job row after
// the current execution attempt completes. Output may be useful for debugging,
// or for storing the result of a job temporarily without needing to create a
// dedicated table to keep it in.
//
// For example, with workflows, it's common for subsequent task to depend on
// something done in an earlier dependency task. Consider the creation of an
// external resource in another API or in an database—it will typically have a
// unique ID that must be used to reference the resource later. A later step
// may require that info in order to complete its work, and the output can be
// a convenient way to store that info.
//
// Output is stored in the job's metadata under the `"output"` key
// ([github.com/riverqueue/river/rivertype.MetadataKeyOutput]).
// This function must be called within an Worker's Work function. It returns an
// error if called anywhere else. As with any stored value, care should be taken
// to ensure that the payload size is not too large. Output is limited to 32MB
// in size for safety, but should be kept much smaller than this.
//
// Only one output can be stored per job. If this function is called more than
// once, the output will be overwritten with the latest value. The output also
// must be recorded _before_ the job finishes executing so that it can be stored
// when the job's row is updated.
//
// Once recorded, the output is stored regardless of the outcome of the
// execution attempt (success, error, panic, etc.).
//
// The output is marshalled to JSON as part of this function and it will return
// an error if the output is not JSON-encodable.
func RecordOutput(ctx context.Context, output any) error {
	metadataUpdates, hasMetadataUpdates := jobexecutor.MetadataUpdatesFromWorkContext(ctx)
	if !hasMetadataUpdates {
		return errors.New("RecordOutput must be called within a Worker")
	}

	metadataUpdatesBytes, err := json.Marshal(output)
	if err != nil {
		return err
	}

	// Postgres JSONB is limited to 255MB, but it would be a bad idea to get
	// anywhere close to that limit here.
	if len(metadataUpdatesBytes) > maxOutputSize {
		return fmt.Errorf("output is too large: %d bytes (max 32 MB)", len(metadataUpdatesBytes))
	}

	metadataUpdates[rivertype.MetadataKeyOutput] = json.RawMessage(metadataUpdatesBytes)
	return nil
}

```

`recorded_output_test.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"strings"
	"testing"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/rivershared/riversharedtest"
)

func Test_RecordedOutput(t *testing.T) {
	t.Parallel()
	ctx := context.Background()

	type JobArgs struct {
		JobArgsReflectKind[JobArgs]
	}

	type myOutput struct {
		Message string `json:"message"`
	}

	type testBundle struct {
		dbPool *pgxpool.Pool
	}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)
		config := newTestConfig(t, nil)
		client := newTestClient(t, dbPool, config)
		t.Cleanup(func() { require.NoError(t, client.Stop(ctx)) })
		return client, &testBundle{dbPool: dbPool}
	}

	t.Run("ValidOutput", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		validOutput := myOutput{Message: "it worked"}
		expectedOutput := `{"output":{"message":"it worked"}}`
		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return RecordOutput(ctx, validOutput)
		}))

		subChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, JobArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		require.JSONEq(t, expectedOutput, string(event.Job.Metadata))

		jobFromDB, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.JSONEq(t, expectedOutput, string(jobFromDB.Metadata))
	})

	t.Run("InvalidOutput", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		// Use an invalid output value (a channel, which cannot be marshaled to JSON)
		var invalidOutput chan int
		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return RecordOutput(ctx, invalidOutput)
		}))

		subChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, JobArgs{}, nil)
		require.NoError(t, err)

		// Wait for the job failure event
		event := riversharedtest.WaitOrTimeout(t, subChan)
		require.Equal(t, EventKindJobFailed, event.Kind)
		require.NotEmpty(t, event.Job.Errors)
		require.Contains(t, event.Job.Errors[0].Error, "json")

		jobFromDB, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		var meta map[string]any
		require.NoError(t, json.Unmarshal(jobFromDB.Metadata, &meta))
		_, ok := meta["output"]
		require.False(t, ok, "output key should not be set in metadata")
	})

	t.Run("PreExistingMetadata", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		newOutput := myOutput{Message: "new output"}
		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			return RecordOutput(ctx, newOutput)
		}))

		subChan := subscribe(t, client)
		startClient(ctx, t, client)

		// Insert a job with pre-existing metadata (including an output key)
		initialMeta := `{"existing":"value","output":"old"}`
		insertRes, err := client.Insert(ctx, JobArgs{}, &InsertOpts{Metadata: []byte(initialMeta)})
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subChan)
		require.Equal(t, EventKindJobCompleted, event.Kind)
		expectedMeta := `{"existing":"value","output":{"message":"new output"}}`
		require.JSONEq(t, expectedMeta, string(event.Job.Metadata))

		// Fetch the job from the database and verify
		jobFromDB, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.JSONEq(t, expectedMeta, string(jobFromDB.Metadata))
	})

	t.Run("OutputTooLarge", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[JobArgs]) error {
			// Record an output of 32MB + 1 byte:
			err := RecordOutput(ctx, strings.Repeat("x", 32*1024*1024+1))
			require.ErrorContains(t, err, "output is too large")
			return err
		}))

		subChan := subscribe(t, client)
		startClient(ctx, t, client)

		insertRes, err := client.Insert(ctx, JobArgs{}, nil)
		require.NoError(t, err)

		event := riversharedtest.WaitOrTimeout(t, subChan)
		require.Equal(t, EventKindJobFailed, event.Kind)
		require.NotEmpty(t, event.Job.Errors)
		require.Contains(t, event.Job.Errors[0].Error, "output is too large")

		jobFromDB, err := client.JobGet(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		var meta map[string]any
		require.NoError(t, json.Unmarshal(jobFromDB.Metadata, &meta))
		_, ok := meta["output"]
		require.False(t, ok, "output key should not be set in metadata")
	})
}

```

`retry_policy.go`:

```go
package river

import (
	"math"
	"math/rand/v2"
	"time"

	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivertype"
)

// ClientRetryPolicy is an interface that can be implemented to provide a retry
// policy for how River deals with failed jobs at the client level (when a
// worker does not define an override for `NextRetry`). Jobs are scheduled to be
// retried in the future up until they've reached the job's max attempts, at
// which pointed they're set as discarded.
//
// The ClientRetryPolicy does not have access to generics and operates on the
// raw JobRow struct with encoded args.
type ClientRetryPolicy interface {
	// NextRetry calculates when the next retry for a failed job should take place
	// given when it was last attempted and its number of attempts, or any other
	// of the job's properties a user-configured retry policy might want to
	// consider.
	NextRetry(job *rivertype.JobRow) time.Time
}

// River's default retry policy.
type DefaultClientRetryPolicy struct {
	timeNowFunc func() time.Time
}

// NextRetry gets the next retry given for the given job, accounting for when it
// was last attempted and what attempt number that was. Reschedules using a
// basic exponential backoff of `ATTEMPT^4`, so after the first failure a new
// try will be scheduled in 1 seconds, 16 seconds after the second, 1 minute and
// 21 seconds after the third, etc.
//
// Snoozes do not count as attempts and do not influence retry behavior.
// Earlier versions of River would allow the attempt to increment each time a
// job was snoozed. Although this has been changed and snoozes now decrement the
// attempt count, we can maintain the same retry schedule even for pre-existing
// jobs by using the number of errors instead of the attempt count. This ensures
// consistent behavior across River versions.
//
// At degenerately high retry counts (>= 310) the policy starts adding the
// equivalent of the maximum of time.Duration to each retry, about 292 years.
// The schedule is no longer exponential past this point.
func (p *DefaultClientRetryPolicy) NextRetry(job *rivertype.JobRow) time.Time {
	// For the purposes of calculating the backoff, we can look solely at the
	// number of errors. If we were to use the raw attempt count, this would be
	// incremented and influenced by snoozes. However the use case for snoozing is
	// to try again later *without* counting as an error.
	//
	// Note that we explicitly add 1 here, because the error hasn't been appended
	// yet at the time this is called (that happens in the completer).
	errorCount := len(job.Errors) + 1

	return p.timeNowUTC().Add(timeutil.SecondsAsDuration(p.retrySeconds(errorCount)))
}

func (p *DefaultClientRetryPolicy) timeNowUTC() time.Time {
	if p.timeNowFunc != nil {
		return p.timeNowFunc()
	}

	return time.Now().UTC()
}

// The maximum value of a duration before it overflows. About 292 years.
const maxDuration time.Duration = 1<<63 - 1

// Same as the above, but changed to a float represented in seconds.
var maxDurationSeconds = maxDuration.Seconds() //nolint:gochecknoglobals

// Gets a number of retry seconds for the given attempt, random jitter included.
func (p *DefaultClientRetryPolicy) retrySeconds(attempt int) float64 {
	retrySeconds := p.retrySecondsWithoutJitter(attempt)

	// After hitting maximum retry durations jitter is no longer applied because
	// it might overflow time.Duration. That's okay though because so much
	// jitter will already have been applied up to this point (jitter measured
	// in decades) that jobs will no longer run anywhere near contemporaneously
	// unless there's been considerable manual intervention.
	if retrySeconds == maxDurationSeconds {
		return maxDurationSeconds
	}

	// Jitter number of seconds +/- 10%.
	retrySeconds += retrySeconds * (rand.Float64()*0.2 - 0.1)

	return retrySeconds
}

// Gets a base number of retry seconds for the given attempt, jitter excluded.
// If the number of seconds returned would overflow time.Duration if it were to
// be made one, returns the maximum number of seconds that can fit in a
// time.Duration instead, approximately 292 years.
func (p *DefaultClientRetryPolicy) retrySecondsWithoutJitter(attempt int) float64 {
	retrySeconds := math.Pow(float64(attempt), 4)
	return min(retrySeconds, maxDurationSeconds)
}

```

`retry_policy_test.go`:

```go
package river

import (
	"fmt"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/rivershared/util/timeutil"
	"github.com/riverqueue/river/rivertype"
)

// Just proves that DefaultRetryPolicy implements the RetryPolicy interface.
var _ ClientRetryPolicy = &DefaultClientRetryPolicy{}

func TestDefaultClientRetryPolicy_NextRetry(t *testing.T) {
	t.Parallel()

	type testBundle struct {
		now time.Time
	}

	setup := func(t *testing.T) (*DefaultClientRetryPolicy, *testBundle) {
		t.Helper()

		var (
			now         = time.Now().UTC()
			timeNowFunc = func() time.Time { return now }
		)

		return &DefaultClientRetryPolicy{timeNowFunc: timeNowFunc}, &testBundle{
			now: now,
		}
	}

	t.Run("Schedule", func(t *testing.T) {
		t.Parallel()

		retryPolicy, bundle := setup(t)

		for attempt := 1; attempt < 10; attempt++ {
			retrySecondsWithoutJitter := retryPolicy.retrySecondsWithoutJitter(attempt)
			allowedDelta := timeutil.SecondsAsDuration(retrySecondsWithoutJitter * 0.2)

			nextRetryAt := retryPolicy.NextRetry(&rivertype.JobRow{
				Attempt:     attempt,
				AttemptedAt: &bundle.now,
				Errors:      make([]rivertype.AttemptError, attempt-1),
			})
			require.WithinDuration(t, bundle.now.Add(timeutil.SecondsAsDuration(retrySecondsWithoutJitter)), nextRetryAt, allowedDelta)
		}
	})

	t.Run("MaxRetryDuration", func(t *testing.T) {
		t.Parallel()

		retryPolicy, bundle := setup(t)

		maxRetryDuration := timeutil.SecondsAsDuration(maxDurationSeconds)

		// First time the maximum will be hit.
		require.Equal(t,
			bundle.now.Add(maxRetryDuration),
			retryPolicy.NextRetry(&rivertype.JobRow{
				Attempt:     310,
				AttemptedAt: &bundle.now,
				Errors:      make([]rivertype.AttemptError, 310-1),
			}),
		)

		// And will be hit on all subsequent attempts as well.
		require.Equal(t,
			bundle.now.Add(maxRetryDuration),
			retryPolicy.NextRetry(&rivertype.JobRow{
				Attempt:     1_000,
				AttemptedAt: &bundle.now,
				Errors:      make([]rivertype.AttemptError, 1_000-1),
			}),
		)
	})
}

func TestDefaultRetryPolicy_retrySeconds(t *testing.T) {
	t.Parallel()

	retryPolicy := &DefaultClientRetryPolicy{}

	for attempt := 1; attempt < rivercommon.MaxAttemptsDefault; attempt++ {
		retrySecondsWithoutJitter := retryPolicy.retrySecondsWithoutJitter(attempt)

		// Jitter is number of seconds +/- 10%.
		retrySecondsMin := timeutil.SecondsAsDuration(retrySecondsWithoutJitter - retrySecondsWithoutJitter*0.1)
		retrySecondsMax := timeutil.SecondsAsDuration(retrySecondsWithoutJitter + retrySecondsWithoutJitter*0.1)

		// Run a number of times just to make sure we never generate a number
		// outside of the expected bounds.
		for range 10 {
			retryDuration := timeutil.SecondsAsDuration(retryPolicy.retrySeconds(attempt))

			require.GreaterOrEqual(t, retryDuration, retrySecondsMin)
			require.Less(t, retryDuration, retrySecondsMax)
		}
	}
}

// This is mostly to give a feeling for what the retry schedule looks like with
// real values.
func TestDefaultRetryPolicy_retrySecondsWithoutJitter(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (*DefaultClientRetryPolicy, *testBundle) { //nolint:unparam
		t.Helper()

		return &DefaultClientRetryPolicy{}, &testBundle{}
	}

	t.Run("Schedule", func(t *testing.T) {
		t.Parallel()

		retryPolicy, _ := setup(t)

		day := 24 * time.Hour

		testCases := []struct {
			attempt       int
			expectedRetry time.Duration
		}{
			{attempt: 1, expectedRetry: 1 * time.Second},
			{attempt: 2, expectedRetry: 16 * time.Second},
			{attempt: 3, expectedRetry: 1*time.Minute + 21*time.Second},
			{attempt: 4, expectedRetry: 4*time.Minute + 16*time.Second},
			{attempt: 5, expectedRetry: 10*time.Minute + 25*time.Second},
			{attempt: 6, expectedRetry: 21*time.Minute + 36*time.Second},
			{attempt: 7, expectedRetry: 40*time.Minute + 1*time.Second},
			{attempt: 8, expectedRetry: 1*time.Hour + 8*time.Minute + 16*time.Second},
			{attempt: 9, expectedRetry: 1*time.Hour + 49*time.Minute + 21*time.Second},
			{attempt: 10, expectedRetry: 2*time.Hour + 46*time.Minute + 40*time.Second},
			{attempt: 11, expectedRetry: 4*time.Hour + 4*time.Minute + 1*time.Second},
			{attempt: 12, expectedRetry: 5*time.Hour + 45*time.Minute + 36*time.Second},
			{attempt: 13, expectedRetry: 7*time.Hour + 56*time.Minute + 1*time.Second},
			{attempt: 14, expectedRetry: 10*time.Hour + 40*time.Minute + 16*time.Second},
			{attempt: 15, expectedRetry: 14*time.Hour + 3*time.Minute + 45*time.Second},
			{attempt: 16, expectedRetry: 18*time.Hour + 12*time.Minute + 16*time.Second},
			{attempt: 17, expectedRetry: 23*time.Hour + 12*time.Minute + 1*time.Second},
			{attempt: 18, expectedRetry: 1*day + 5*time.Hour + 9*time.Minute + 36*time.Second},
			{attempt: 19, expectedRetry: 1*day + 12*time.Hour + 12*time.Minute + 1*time.Second},
			{attempt: 20, expectedRetry: 1*day + 20*time.Hour + 26*time.Minute + 40*time.Second},
			{attempt: 21, expectedRetry: 2*day + 6*time.Hour + 1*time.Minute + 21*time.Second},
			{attempt: 22, expectedRetry: 2*day + 17*time.Hour + 4*time.Minute + 16*time.Second},
			{attempt: 23, expectedRetry: 3*day + 5*time.Hour + 44*time.Minute + 1*time.Second},
			{attempt: 24, expectedRetry: 3*day + 20*time.Hour + 9*time.Minute + 36*time.Second},
		}
		for _, tt := range testCases {
			t.Run(fmt.Sprintf("Attempt_%02d", tt.attempt), func(t *testing.T) {
				t.Parallel()

				require.Equal(t,
					tt.expectedRetry,
					time.Duration(retryPolicy.retrySecondsWithoutJitter(tt.attempt))*time.Second)
			})
		}
	})

	t.Run("MaxDurationSeconds", func(t *testing.T) {
		t.Parallel()

		retryPolicy, _ := setup(t)

		require.NotEqual(t, time.Duration(maxDurationSeconds)*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(309))*time.Second)

		// Attempt number 310 hits the ceiling, and we'll always hit it from thence on.
		require.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(310))*time.Second)
		require.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(311))*time.Second)
		require.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(1_000))*time.Second)
		require.Equal(t, time.Duration(maxDuration.Seconds())*time.Second, time.Duration(retryPolicy.retrySecondsWithoutJitter(1_000_000))*time.Second)
	})
}

func TestDefaultRetryPolicy_stress(t *testing.T) {
	t.Parallel()

	var wg sync.WaitGroup
	retryPolicy := &DefaultClientRetryPolicy{}

	// Hit the source with a bunch of goroutines to help suss out any problems
	// with concurrent safety (when combined with `-race`).
	for range 10 {
		wg.Add(1)
		go func() {
			for range 100 {
				_ = retryPolicy.retrySeconds(7)
			}
			wg.Done()
		}()
	}

	wg.Wait()
}

```

`riverdriver/go.mod`:

```mod
module github.com/riverqueue/river/riverdriver

go 1.23.0

toolchain go1.24.1

require (
	github.com/riverqueue/river/rivertype v0.19.0
	github.com/stretchr/testify v1.10.0
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/kr/text v0.2.0 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

```

`riverdriver/go.sum`:

```sum
github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=
github.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/riverqueue/river/rivertype v0.17.0 h1:YG5OkGMpDNXY6q1p4b3DsNq4FA0E5rwF78ZeMwi4KG0=
github.com/riverqueue/river/rivertype v0.17.0/go.mod h1:DETcejveWlq6bAb8tHkbgJqmXWVLiFhTiEm8j7co1bE=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

```

`riverdriver/river_driver_interface.go`:

```go
// Package riverdriver exposes generic constructs to be implemented by specific
// drivers that wrap third party database packages, with the aim being to keep
// the main River interface decoupled from a specific database package so that
// other packages or other major versions of packages can be supported in future
// River versions.
//
// River currently only supports Pgx v5, and the interface here wrap it with
// only the thinnest possible layer. Adding support for alternate packages will
// require the interface to change substantially, and therefore it should not be
// implemented or invoked by user code. Changes to interfaces in this package
// WILL NOT be considered breaking changes for purposes of River's semantic
// versioning.
package riverdriver

import (
	"context"
	"errors"
	"io/fs"
	"time"

	"github.com/riverqueue/river/rivertype"
)

const AllQueuesString = "*"

const MigrationLineMain = "main"

var (
	ErrClosedPool     = errors.New("underlying driver pool is closed")
	ErrNotImplemented = errors.New("driver does not implement this functionality")
)

// Driver provides a database driver for use with river.Client.
//
// Its purpose is to wrap the interface of a third party database package, with
// the aim being to keep the main River interface decoupled from a specific
// database package so that other packages or major versions of packages can be
// supported in future River versions.
//
// River currently only supports Pgx v5, and this interface wraps it with only
// the thinnest possible layer. Adding support for alternate packages will
// require it to change substantially, and therefore it should not be
// implemented or invoked by user code. Changes to this interface WILL NOT be
// considered breaking changes for purposes of River's semantic versioning.
//
// API is not stable. DO NOT IMPLEMENT.
type Driver[TTx any] interface {
	// GetExecutor gets an executor for the driver.
	//
	// API is not stable. DO NOT USE.
	GetExecutor() Executor

	// GetListener gets a listener for purposes of receiving notifications.
	//
	// API is not stable. DO NOT USE.
	GetListener() Listener

	// GetMigrationFS gets a filesystem containing migrations for the driver.
	//
	// Each set of migration files is expected to exist within the filesystem as
	// `migration/<line>/`. For example:
	//
	//     migration/main/001_create_river_migration.up.sql
	//
	// API is not stable. DO NOT USE.
	GetMigrationFS(line string) fs.FS

	// GetMigrationLines gets supported migration lines from the driver. Most
	// drivers will only support a single line: MigrationLineMain.
	//
	// API is not stable. DO NOT USE.
	GetMigrationLines() []string

	// HasPool returns true if the driver is configured with a database pool.
	//
	// API is not stable. DO NOT USE.
	HasPool() bool

	// SupportsListener gets whether this driver supports a listener. Drivers
	// that don't support a listener support poll only mode only.
	//
	// API is not stable. DO NOT USE.
	SupportsListener() bool

	// UnwrapExecutor gets an executor from a driver transaction.
	//
	// API is not stable. DO NOT USE.
	UnwrapExecutor(tx TTx) ExecutorTx
}

// Executor provides River operations against a database. It may be a database
// pool or transaction.
//
// API is not stable. DO NOT IMPLEMENT.
type Executor interface {
	// Begin begins a new subtransaction. ErrSubTxNotSupported may be returned
	// if the executor is a transaction and the driver doesn't support
	// subtransactions (like riverdriver/riverdatabasesql for database/sql).
	Begin(ctx context.Context) (ExecutorTx, error)

	// ColumnExists checks whether a column for a particular table exists for
	// the schema in the current search schema.
	ColumnExists(ctx context.Context, tableName, columnName string) (bool, error)

	// Exec executes raw SQL. Used for migrations.
	Exec(ctx context.Context, sql string) (struct{}, error)

	JobCancel(ctx context.Context, params *JobCancelParams) (*rivertype.JobRow, error)
	JobCountByState(ctx context.Context, state rivertype.JobState) (int, error)
	JobDelete(ctx context.Context, id int64) (*rivertype.JobRow, error)
	JobDeleteBefore(ctx context.Context, params *JobDeleteBeforeParams) (int, error)
	JobGetAvailable(ctx context.Context, params *JobGetAvailableParams) ([]*rivertype.JobRow, error)
	JobGetByID(ctx context.Context, id int64) (*rivertype.JobRow, error)
	JobGetByIDMany(ctx context.Context, id []int64) ([]*rivertype.JobRow, error)
	JobGetByKindAndUniqueProperties(ctx context.Context, params *JobGetByKindAndUniquePropertiesParams) (*rivertype.JobRow, error)
	JobGetByKindMany(ctx context.Context, kind []string) ([]*rivertype.JobRow, error)
	JobGetStuck(ctx context.Context, params *JobGetStuckParams) ([]*rivertype.JobRow, error)
	JobInsertFastMany(ctx context.Context, params []*JobInsertFastParams) ([]*JobInsertFastResult, error)
	JobInsertFastManyNoReturning(ctx context.Context, params []*JobInsertFastParams) (int, error)
	JobInsertFull(ctx context.Context, params *JobInsertFullParams) (*rivertype.JobRow, error)
	JobList(ctx context.Context, params *JobListParams) ([]*rivertype.JobRow, error)
	JobRescueMany(ctx context.Context, params *JobRescueManyParams) (*struct{}, error)
	JobRetry(ctx context.Context, id int64) (*rivertype.JobRow, error)
	JobSchedule(ctx context.Context, params *JobScheduleParams) ([]*JobScheduleResult, error)
	JobSetStateIfRunningMany(ctx context.Context, params *JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error)
	JobUpdate(ctx context.Context, params *JobUpdateParams) (*rivertype.JobRow, error)
	LeaderAttemptElect(ctx context.Context, params *LeaderElectParams) (bool, error)
	LeaderAttemptReelect(ctx context.Context, params *LeaderElectParams) (bool, error)
	LeaderDeleteExpired(ctx context.Context) (int, error)
	LeaderGetElectedLeader(ctx context.Context) (*Leader, error)
	LeaderInsert(ctx context.Context, params *LeaderInsertParams) (*Leader, error)
	LeaderResign(ctx context.Context, params *LeaderResignParams) (bool, error)

	// MigrationDeleteAssumingMainMany deletes many migrations assuming
	// everything is on the main line. This is suitable for use in databases on
	// a version before the `line` column exists.
	MigrationDeleteAssumingMainMany(ctx context.Context, versions []int) ([]*Migration, error)

	// MigrationDeleteByLineAndVersionMany deletes many migration versions on a
	// particular line.
	MigrationDeleteByLineAndVersionMany(ctx context.Context, line string, versions []int) ([]*Migration, error)

	// MigrationGetAllAssumingMain gets all migrations assuming everything is on
	// the main line. This is suitable for use in databases on a version before
	// the `line` column exists.
	MigrationGetAllAssumingMain(ctx context.Context) ([]*Migration, error)

	// MigrationGetByLine gets all currently applied migrations.
	MigrationGetByLine(ctx context.Context, line string) ([]*Migration, error)

	// MigrationInsertMany inserts many migration versions.
	MigrationInsertMany(ctx context.Context, line string, versions []int) ([]*Migration, error)

	// MigrationInsertManyAssumingMain inserts many migrations, assuming they're
	// on the main line. This operation is necessary for compatibility before
	// the `line` column was added to the migrations table.
	MigrationInsertManyAssumingMain(ctx context.Context, versions []int) ([]*Migration, error)

	NotifyMany(ctx context.Context, params *NotifyManyParams) error
	PGAdvisoryXactLock(ctx context.Context, key int64) (*struct{}, error)

	QueueCreateOrSetUpdatedAt(ctx context.Context, params *QueueCreateOrSetUpdatedAtParams) (*rivertype.Queue, error)
	QueueDeleteExpired(ctx context.Context, params *QueueDeleteExpiredParams) ([]string, error)
	QueueGet(ctx context.Context, name string) (*rivertype.Queue, error)
	QueueList(ctx context.Context, limit int) ([]*rivertype.Queue, error)
	QueuePause(ctx context.Context, name string) error
	QueueResume(ctx context.Context, name string) error

	// TableExists checks whether a table exists for the schema in the current
	// search schema.
	TableExists(ctx context.Context, tableName string) (bool, error)
}

// ExecutorTx is an executor which is a transaction. In addition to standard
// Executor operations, it may be committed or rolled back.
//
// API is not stable. DO NOT IMPLEMENT.
type ExecutorTx interface {
	Executor

	// Commit commits the transaction.
	//
	// API is not stable. DO NOT USE.
	Commit(ctx context.Context) error

	// Rollback rolls back the transaction.
	//
	// API is not stable. DO NOT USE.
	Rollback(ctx context.Context) error
}

// Listener listens for notifications. In Postgres, this is a database
// connection where `LISTEN` has been run.
//
// API is not stable. DO NOT IMPLEMENT.
type Listener interface {
	Close(ctx context.Context) error
	Connect(ctx context.Context) error
	Listen(ctx context.Context, topic string) error
	Ping(ctx context.Context) error
	Unlisten(ctx context.Context, topic string) error
	WaitForNotification(ctx context.Context) (*Notification, error)
}

type Notification struct {
	Payload string
	Topic   string
}

type JobCancelParams struct {
	CancelAttemptedAt time.Time
	ControlTopic      string
	ID                int64
}

type JobDeleteBeforeParams struct {
	CancelledFinalizedAtHorizon time.Time
	CompletedFinalizedAtHorizon time.Time
	DiscardedFinalizedAtHorizon time.Time
	Max                         int
}

type JobGetAvailableParams struct {
	AttemptedBy string
	Max         int
	Now         *time.Time
	Queue       string
}

type JobGetByKindAndUniquePropertiesParams struct {
	Kind           string
	ByArgs         bool
	Args           []byte
	ByCreatedAt    bool
	CreatedAtBegin time.Time
	CreatedAtEnd   time.Time
	ByQueue        bool
	Queue          string
	ByState        bool
	State          []string
}

type JobGetStuckParams struct {
	Max          int
	StuckHorizon time.Time
}

type JobInsertFastParams struct {
	// Args contains the raw underlying job arguments struct. It has already been
	// encoded into EncodedArgs, but the original is kept here for to leverage its
	// struct tags and interfaces, such as for use in unique key generation.
	Args         rivertype.JobArgs
	CreatedAt    *time.Time
	EncodedArgs  []byte
	Kind         string
	MaxAttempts  int
	Metadata     []byte
	Priority     int
	Queue        string
	ScheduledAt  *time.Time
	State        rivertype.JobState
	Tags         []string
	UniqueKey    []byte
	UniqueStates byte
}

type JobInsertFastResult struct {
	Job                      *rivertype.JobRow
	UniqueSkippedAsDuplicate bool
}

type JobInsertFullParams struct {
	Attempt      int
	AttemptedAt  *time.Time
	AttemptedBy  []string
	CreatedAt    *time.Time
	EncodedArgs  []byte
	Errors       [][]byte
	FinalizedAt  *time.Time
	Kind         string
	MaxAttempts  int
	Metadata     []byte
	Priority     int
	Queue        string
	ScheduledAt  *time.Time
	State        rivertype.JobState
	Tags         []string
	UniqueKey    []byte
	UniqueStates byte
}

type JobListParams struct {
	Max           int32
	NamedArgs     map[string]any
	OrderByClause string
	WhereClause   string
}

type JobRescueManyParams struct {
	ID          []int64
	Error       [][]byte
	FinalizedAt []time.Time
	ScheduledAt []time.Time
	State       []string
}

type JobScheduleParams struct {
	Max int
	Now time.Time
}

type JobScheduleResult struct {
	Job               rivertype.JobRow
	ConflictDiscarded bool
}

// JobSetStateIfRunningParams are parameters to update the state of a currently
// running job. Use one of the constructors below to ensure a correct
// combination of parameters.
type JobSetStateIfRunningParams struct {
	ID              int64
	Attempt         *int
	ErrData         []byte
	FinalizedAt     *time.Time
	MetadataDoMerge bool
	MetadataUpdates []byte
	ScheduledAt     *time.Time
	State           rivertype.JobState
}

func JobSetStateCancelled(id int64, finalizedAt time.Time, errData []byte, metadataUpdates []byte) *JobSetStateIfRunningParams {
	return &JobSetStateIfRunningParams{
		ID:              id,
		ErrData:         errData,
		MetadataDoMerge: len(metadataUpdates) > 0,
		MetadataUpdates: metadataUpdates,
		FinalizedAt:     &finalizedAt,
		State:           rivertype.JobStateCancelled,
	}
}

func JobSetStateCompleted(id int64, finalizedAt time.Time, metadataUpdates []byte) *JobSetStateIfRunningParams {
	return &JobSetStateIfRunningParams{
		FinalizedAt:     &finalizedAt,
		ID:              id,
		MetadataDoMerge: len(metadataUpdates) > 0,
		MetadataUpdates: metadataUpdates,
		State:           rivertype.JobStateCompleted,
	}
}

func JobSetStateDiscarded(id int64, finalizedAt time.Time, errData []byte, metadataUpdates []byte) *JobSetStateIfRunningParams {
	return &JobSetStateIfRunningParams{
		ID:              id,
		ErrData:         errData,
		MetadataDoMerge: len(metadataUpdates) > 0,
		MetadataUpdates: metadataUpdates,
		FinalizedAt:     &finalizedAt,
		State:           rivertype.JobStateDiscarded,
	}
}

func JobSetStateErrorAvailable(id int64, scheduledAt time.Time, errData []byte, metadataUpdates []byte) *JobSetStateIfRunningParams {
	return &JobSetStateIfRunningParams{
		ID:              id,
		ErrData:         errData,
		MetadataDoMerge: len(metadataUpdates) > 0,
		MetadataUpdates: metadataUpdates,
		ScheduledAt:     &scheduledAt,
		State:           rivertype.JobStateAvailable,
	}
}

func JobSetStateErrorRetryable(id int64, scheduledAt time.Time, errData []byte, metadataUpdates []byte) *JobSetStateIfRunningParams {
	return &JobSetStateIfRunningParams{
		ID:              id,
		ErrData:         errData,
		MetadataDoMerge: len(metadataUpdates) > 0,
		MetadataUpdates: metadataUpdates,
		ScheduledAt:     &scheduledAt,
		State:           rivertype.JobStateRetryable,
	}
}

func JobSetStateSnoozed(id int64, scheduledAt time.Time, attempt int, metadataUpdates []byte) *JobSetStateIfRunningParams {
	return &JobSetStateIfRunningParams{
		Attempt:         &attempt,
		ID:              id,
		MetadataDoMerge: len(metadataUpdates) > 0,
		MetadataUpdates: metadataUpdates,
		ScheduledAt:     &scheduledAt,
		State:           rivertype.JobStateScheduled,
	}
}

func JobSetStateSnoozedAvailable(id int64, scheduledAt time.Time, attempt int, metadataUpdates []byte) *JobSetStateIfRunningParams {
	return &JobSetStateIfRunningParams{
		Attempt:         &attempt,
		ID:              id,
		MetadataDoMerge: len(metadataUpdates) > 0,
		MetadataUpdates: metadataUpdates,
		ScheduledAt:     &scheduledAt,
		State:           rivertype.JobStateAvailable,
	}
}

// JobSetStateIfRunningManyParams are parameters to update the state of
// currently running jobs. Use one of the constructors below to ensure a correct
// combination of parameters.
type JobSetStateIfRunningManyParams struct {
	ID              []int64
	Attempt         []*int
	ErrData         [][]byte
	FinalizedAt     []*time.Time
	MetadataDoMerge []bool
	MetadataUpdates [][]byte
	ScheduledAt     []*time.Time
	State           []rivertype.JobState
}

type JobUpdateParams struct {
	ID                  int64
	AttemptDoUpdate     bool
	Attempt             int
	AttemptedAtDoUpdate bool
	AttemptedAt         *time.Time
	AttemptedByDoUpdate bool
	AttemptedBy         []string
	ErrorsDoUpdate      bool
	Errors              [][]byte
	FinalizedAtDoUpdate bool
	FinalizedAt         *time.Time
	StateDoUpdate       bool
	State               rivertype.JobState
	// Deprecated and will be removed when advisory lock unique path is removed.
	UniqueKeyDoUpdate bool
	// Deprecated and will be removed when advisory lock unique path is removed.
	UniqueKey []byte
}

// Leader represents a River leader.
//
// API is not stable. DO NOT USE.
type Leader struct {
	ElectedAt time.Time
	ExpiresAt time.Time
	LeaderID  string
}

type LeaderInsertParams struct {
	ElectedAt *time.Time
	ExpiresAt *time.Time
	LeaderID  string
	TTL       time.Duration
}

type LeaderElectParams struct {
	LeaderID string
	TTL      time.Duration
}

type LeaderResignParams struct {
	LeaderID        string
	LeadershipTopic string
}

// Migration represents a River migration.
//
// API is not stable. DO NOT USE.
type Migration struct {
	// CreatedAt is when the migration was initially created.
	//
	// API is not stable. DO NOT USE.
	CreatedAt time.Time

	// Line is the migration line that the migration belongs to.
	//
	// API is not stable. DO NOT USE.
	Line string

	// Version is the version of the migration.
	//
	// API is not stable. DO NOT USE.
	Version int
}

// NotifyManyParams are parameters to issue many pubsub notifications all at
// once for a single topic.
type NotifyManyParams struct {
	Payload []string
	Topic   string
}

type QueueCreateOrSetUpdatedAtParams struct {
	Metadata  []byte
	Name      string
	PausedAt  *time.Time
	UpdatedAt *time.Time
}

type QueueDeleteExpiredParams struct {
	Max              int
	UpdatedAtHorizon time.Time
}

```

`riverdriver/river_driver_interface_test.go`:

```go
package riverdriver

import (
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivertype"
)

func TestJobSetStateCancelled(t *testing.T) {
	t.Parallel()

	t.Run("EmptyMetadata", func(t *testing.T) {
		t.Parallel()
		id := int64(1)
		finalizedAt := time.Now().Truncate(time.Second)
		errData := []byte("error occurred")
		result := JobSetStateCancelled(id, finalizedAt, errData, nil)
		require.Equal(t, id, result.ID)
		require.Equal(t, errData, result.ErrData)
		require.NotNil(t, result.FinalizedAt)
		require.True(t, result.FinalizedAt.Equal(finalizedAt), "expected FinalizedAt to equal %v, got %v", finalizedAt, result.FinalizedAt)
		require.Nil(t, result.MetadataUpdates)
		require.False(t, result.MetadataDoMerge)
		require.Equal(t, rivertype.JobStateCancelled, result.State)
	})

	t.Run("NonEmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(1)
		finalizedAt := time.Now().Truncate(time.Second)
		errData := []byte("error occurred")
		metadata := []byte(`{"key": "value"}`)
		result := JobSetStateCancelled(id, finalizedAt, errData, metadata)
		require.Equal(t, id, result.ID)
		require.Equal(t, errData, result.ErrData)
		require.NotNil(t, result.FinalizedAt)
		require.True(t, result.FinalizedAt.Equal(finalizedAt), "expected FinalizedAt to equal %v, got %v", finalizedAt, result.FinalizedAt)
		require.Equal(t, metadata, result.MetadataUpdates)
		require.True(t, result.MetadataDoMerge)
		require.Equal(t, rivertype.JobStateCancelled, result.State)
	})
}

func TestJobSetStateCompleted(t *testing.T) {
	t.Parallel()

	t.Run("EmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(2)
		finalizedAt := time.Now().Truncate(time.Second)
		result := JobSetStateCompleted(id, finalizedAt, nil)
		require.Equal(t, id, result.ID)
		require.NotNil(t, result.FinalizedAt)
		require.True(t, result.FinalizedAt.Equal(finalizedAt))
		require.True(t, result.FinalizedAt.Equal(finalizedAt), "expected FinalizedAt to equal %v, got %v", finalizedAt, result.FinalizedAt)
		require.False(t, result.MetadataDoMerge)
		require.Nil(t, result.MetadataUpdates)
		require.Equal(t, rivertype.JobStateCompleted, result.State)
	})

	t.Run("NonEmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(2)
		finalizedAt := time.Now().Truncate(time.Second)
		metadata := []byte(`{"key": "value"}`)
		result := JobSetStateCompleted(id, finalizedAt, metadata)
		require.Equal(t, id, result.ID)
		require.NotNil(t, result.FinalizedAt)
		require.True(t, result.FinalizedAt.Equal(finalizedAt))
		require.True(t, result.MetadataDoMerge)
		require.Equal(t, metadata, result.MetadataUpdates)
		require.Equal(t, rivertype.JobStateCompleted, result.State)
	})
}

func TestJobSetStateDiscarded(t *testing.T) {
	t.Parallel()

	t.Run("EmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(3)
		finalizedAt := time.Now().Truncate(time.Second)
		errData := []byte("discard error")
		result := JobSetStateDiscarded(id, finalizedAt, errData, nil)
		require.Equal(t, id, result.ID)
		require.Equal(t, errData, result.ErrData)
		require.NotNil(t, result.FinalizedAt)
		require.True(t, result.FinalizedAt.Equal(finalizedAt))
		require.False(t, result.MetadataDoMerge)
		require.Nil(t, result.MetadataUpdates)
		require.Equal(t, rivertype.JobStateDiscarded, result.State)
	})

	t.Run("NonEmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(3)
		finalizedAt := time.Now().Truncate(time.Second)
		errData := []byte("discard error")
		metadata := []byte(`{"key": "value"}`)
		result := JobSetStateDiscarded(id, finalizedAt, errData, metadata)
		require.Equal(t, id, result.ID)
		require.Equal(t, errData, result.ErrData)
		require.NotNil(t, result.FinalizedAt)
		require.True(t, result.FinalizedAt.Equal(finalizedAt))
		require.Equal(t, metadata, result.MetadataUpdates)
		require.True(t, result.MetadataDoMerge)
		require.Equal(t, rivertype.JobStateDiscarded, result.State)
	})
}

func TestJobSetStateErrorAvailable(t *testing.T) {
	t.Parallel()

	t.Run("EmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(4)
		scheduledAt := time.Now().Truncate(time.Second)
		errData := []byte("error available")
		result := JobSetStateErrorAvailable(id, scheduledAt, errData, nil)
		require.Equal(t, id, result.ID)
		require.Equal(t, errData, result.ErrData)
		require.False(t, result.MetadataDoMerge)
		require.Nil(t, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, rivertype.JobStateAvailable, result.State)
	})

	t.Run("NonEmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(4)
		scheduledAt := time.Now().Truncate(time.Second)
		errData := []byte("error available")
		metadata := []byte(`{"key": "value"}`)
		result := JobSetStateErrorAvailable(id, scheduledAt, errData, metadata)
		require.Equal(t, id, result.ID)
		require.True(t, result.MetadataDoMerge)
		require.Equal(t, metadata, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, errData, result.ErrData)
	})
}

func TestJobSetStateErrorRetryable(t *testing.T) {
	t.Parallel()

	t.Run("EmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(5)
		scheduledAt := time.Now().Truncate(time.Second)
		errData := []byte("retryable error")
		result := JobSetStateErrorRetryable(id, scheduledAt, errData, nil)
		require.Equal(t, id, result.ID)
		require.False(t, result.MetadataDoMerge)
		require.Nil(t, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, errData, result.ErrData)
		require.Equal(t, rivertype.JobStateRetryable, result.State)
	})

	t.Run("NonEmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(5)
		scheduledAt := time.Now().Truncate(time.Second)
		errData := []byte("retryable error")
		metadata := []byte(`{"key": "value"}`)
		result := JobSetStateErrorRetryable(id, scheduledAt, errData, metadata)
		require.Equal(t, id, result.ID)
		require.True(t, result.MetadataDoMerge)
		require.Equal(t, metadata, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, errData, result.ErrData)
	})
}

func TestJobSetStateSnoozed(t *testing.T) { //nolint:dupl
	t.Parallel()

	t.Run("EmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(6)
		scheduledAt := time.Now().Truncate(time.Second)
		attempt := 2
		result := JobSetStateSnoozed(id, scheduledAt, attempt, nil)
		require.Equal(t, id, result.ID)
		require.NotNil(t, result.Attempt)
		require.Equal(t, attempt, *result.Attempt)
		require.False(t, result.MetadataDoMerge)
		require.Nil(t, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, rivertype.JobStateScheduled, result.State)
	})

	t.Run("NonEmptyMetadata", func(t *testing.T) {
		t.Parallel()
		id := int64(6)
		scheduledAt := time.Now().Truncate(time.Second)
		attempt := 2
		metadata := []byte("snoozed metadata")
		result := JobSetStateSnoozed(id, scheduledAt, attempt, metadata)
		require.Equal(t, id, result.ID)
		require.NotNil(t, result.Attempt)
		require.Equal(t, attempt, *result.Attempt)
		require.True(t, result.MetadataDoMerge)
		require.Equal(t, metadata, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, rivertype.JobStateScheduled, result.State)
	})
}

func TestJobSetStateSnoozedAvailable(t *testing.T) { //nolint:dupl
	t.Parallel()

	t.Run("EmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(7)
		scheduledAt := time.Now().Truncate(time.Second)
		attempt := 3
		result := JobSetStateSnoozedAvailable(id, scheduledAt, attempt, nil)
		require.Equal(t, id, result.ID)
		require.NotNil(t, result.Attempt)
		require.Equal(t, attempt, *result.Attempt)
		require.False(t, result.MetadataDoMerge)
		require.Nil(t, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, rivertype.JobStateAvailable, result.State)
	})

	t.Run("NonEmptyMetadata", func(t *testing.T) {
		t.Parallel()

		id := int64(7)
		scheduledAt := time.Now().Truncate(time.Second)
		attempt := 3
		metadata := []byte("snoozed available metadata")
		result := JobSetStateSnoozedAvailable(id, scheduledAt, attempt, metadata)
		require.Equal(t, id, result.ID)
		require.NotNil(t, result.Attempt)
		require.Equal(t, attempt, *result.Attempt)
		require.True(t, result.MetadataDoMerge)
		require.Equal(t, metadata, result.MetadataUpdates)
		require.NotNil(t, result.ScheduledAt)
		require.True(t, result.ScheduledAt.Equal(scheduledAt))
		require.Equal(t, rivertype.JobStateAvailable, result.State)
	})
}

```

`riverdriver/riverdatabasesql/go.mod`:

```mod
module github.com/riverqueue/river/riverdriver/riverdatabasesql

go 1.23.0

toolchain go1.24.1

require (
	github.com/jackc/pgx/v5 v5.7.3
	github.com/lib/pq v1.10.9
	github.com/riverqueue/river v0.19.0
	github.com/riverqueue/river/riverdriver v0.19.0
	github.com/riverqueue/river/rivershared v0.19.0
	github.com/riverqueue/river/rivertype v0.19.0
	github.com/stretchr/testify v1.10.0
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/tidwall/gjson v1.18.0 // indirect
	github.com/tidwall/match v1.1.1 // indirect
	github.com/tidwall/pretty v1.2.1 // indirect
	github.com/tidwall/sjson v1.2.5 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

```

`riverdriver/riverdatabasesql/go.sum`:

```sum
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
github.com/jackc/pgx/v5 v5.7.3 h1:PO1wNKj/bTAwxSJnO1Z4Ai8j4magtqg2SLNjEDzcXQo=
github.com/jackc/pgx/v5 v5.7.3/go.mod h1:ncY89UGWxg82EykZUwSpUKEfccBGGYq1xjrOpsbsfGQ=
github.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo=
github.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=
github.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=
github.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/riverqueue/river v0.19.0 h1:WRh/NXhp+WEEY0HpCYgr4wSRllugYBt30HtyQ3jlz08=
github.com/riverqueue/river v0.19.0/go.mod h1:YJ7LA2uBdqFHQJzKyYc+X6S04KJeiwsS1yU5a1rynlk=
github.com/riverqueue/river/riverdriver v0.19.0 h1:NyHz5DfB13paT2lvaO0CKmwy4SFLbA7n6MFRGRtwii4=
github.com/riverqueue/river/riverdriver v0.19.0/go.mod h1:Soxi08hHkEvopExAp6ADG2437r4coSiB4QpuIL5E28k=
github.com/riverqueue/river/rivershared v0.19.0 h1:TZvFM6CC+QgwQQUMQ5Ueuhx25ptgqcKqZQGsdLJnFeE=
github.com/riverqueue/river/rivershared v0.19.0/go.mod h1:JAvmohuC5lounVk8e3zXZIs07Da3klzEeJo1qDQIbjw=
github.com/riverqueue/river/rivertype v0.19.0 h1:5rwgdh21pVcU9WjrHIIO9qC2dOMdRrrZ/HZZOE0JRyY=
github.com/riverqueue/river/rivertype v0.19.0/go.mod h1:DETcejveWlq6bAb8tHkbgJqmXWVLiFhTiEm8j7co1bE=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/tidwall/gjson v1.14.2/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/gjson v1.18.0 h1:FIDeeyB800efLX89e5a8Y0BNH+LOngJyGrIWxG2FKQY=
github.com/tidwall/gjson v1.18.0/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/match v1.1.1 h1:+Ho715JplO36QYgwN9PGYNhgZvoUSc9X2c80KVTi+GA=
github.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=
github.com/tidwall/pretty v1.2.0/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/pretty v1.2.1 h1:qjsOFOWWQl+N3RsoF5/ssm1pHmJJwhjlSbZ51I6wMl4=
github.com/tidwall/pretty v1.2.1/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/sjson v1.2.5 h1:kLy8mja+1c9jlljvWTlSazM7cKDRfJuR/bOJhcY5NcY=
github.com/tidwall/sjson v1.2.5/go.mod h1:Fvgq9kS/6ociJEDnK0Fk1cpYF4FIW6ZF7LAe+6jwd28=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
golang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/sync v0.12.0 h1:MHc5BpPuC30uJk597Ri8TV3CNZcTLu6B6z4lJy+g6Jw=
golang.org/x/sync v0.12.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/text v0.23.0 h1:D71I7dUrlY+VX0gQShAThNGHFxZ13dGLBHQLVl1mJlY=
golang.org/x/text v0.23.0/go.mod h1:/BLNzu4aZCJ1+kcD0DNRotWKage4q2rGVAg4o22unh4=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

```

`riverdriver/riverdatabasesql/internal/dbsqlc/db.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0

package dbsqlc

import (
	"context"
	"database/sql"
)

type DBTX interface {
	ExecContext(context.Context, string, ...interface{}) (sql.Result, error)
	PrepareContext(context.Context, string) (*sql.Stmt, error)
	QueryContext(context.Context, string, ...interface{}) (*sql.Rows, error)
	QueryRowContext(context.Context, string, ...interface{}) *sql.Row
}

func New() *Queries {
	return &Queries{}
}

type Queries struct {
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/models.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0

package dbsqlc

import (
	"database/sql/driver"
	"fmt"
	"time"

	"github.com/riverqueue/river/riverdriver/riverdatabasesql/internal/pgtypealias"
)

type RiverJobState string

const (
	RiverJobStateAvailable RiverJobState = "available"
	RiverJobStateCancelled RiverJobState = "cancelled"
	RiverJobStateCompleted RiverJobState = "completed"
	RiverJobStateDiscarded RiverJobState = "discarded"
	RiverJobStatePending   RiverJobState = "pending"
	RiverJobStateRetryable RiverJobState = "retryable"
	RiverJobStateRunning   RiverJobState = "running"
	RiverJobStateScheduled RiverJobState = "scheduled"
)

func (e *RiverJobState) Scan(src interface{}) error {
	switch s := src.(type) {
	case []byte:
		*e = RiverJobState(s)
	case string:
		*e = RiverJobState(s)
	default:
		return fmt.Errorf("unsupported scan type for RiverJobState: %T", src)
	}
	return nil
}

type NullRiverJobState struct {
	RiverJobState RiverJobState
	Valid         bool // Valid is true if RiverJobState is not NULL
}

// Scan implements the Scanner interface.
func (ns *NullRiverJobState) Scan(value interface{}) error {
	if value == nil {
		ns.RiverJobState, ns.Valid = "", false
		return nil
	}
	ns.Valid = true
	return ns.RiverJobState.Scan(value)
}

// Value implements the driver Valuer interface.
func (ns NullRiverJobState) Value() (driver.Value, error) {
	if !ns.Valid {
		return nil, nil
	}
	return string(ns.RiverJobState), nil
}

type RiverClient struct {
	ID        string
	CreatedAt time.Time
	Metadata  string
	PausedAt  *time.Time
	UpdatedAt time.Time
}

type RiverClientQueue struct {
	RiverClientID    string
	Name             string
	CreatedAt        time.Time
	MaxWorkers       int64
	Metadata         string
	NumJobsCompleted int64
	NumJobsRunning   int64
	UpdatedAt        time.Time
}

type RiverJob struct {
	ID           int64
	Args         string
	Attempt      int16
	AttemptedAt  *time.Time
	AttemptedBy  []string
	CreatedAt    time.Time
	Errors       []string
	FinalizedAt  *time.Time
	Kind         string
	MaxAttempts  int16
	Metadata     string
	Priority     int16
	Queue        string
	State        RiverJobState
	ScheduledAt  time.Time
	Tags         []string
	UniqueKey    []byte
	UniqueStates pgtypealias.Bits
}

type RiverLeader struct {
	ElectedAt time.Time
	ExpiresAt time.Time
	LeaderID  string
	Name      string
}

type RiverMigration struct {
	Line      string
	Version   int64
	CreatedAt time.Time
}

type RiverQueue struct {
	Name      string
	CreatedAt time.Time
	Metadata  string
	PausedAt  *time.Time
	UpdatedAt time.Time
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/pg_misc.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: pg_misc.sql

package dbsqlc

import (
	"context"

	"github.com/lib/pq"
)

const pGAdvisoryXactLock = `-- name: PGAdvisoryXactLock :exec
SELECT pg_advisory_xact_lock($1)
`

func (q *Queries) PGAdvisoryXactLock(ctx context.Context, db DBTX, key int64) error {
	_, err := db.ExecContext(ctx, pGAdvisoryXactLock, key)
	return err
}

const pGNotifyMany = `-- name: PGNotifyMany :exec
WITH topic_to_notify AS (
    SELECT
        concat(current_schema(), '.', $1::text) AS topic,
        unnest($2::text[]) AS payload
)
SELECT pg_notify(
    topic_to_notify.topic,
    topic_to_notify.payload
  )
FROM topic_to_notify
`

type PGNotifyManyParams struct {
	Topic   string
	Payload []string
}

func (q *Queries) PGNotifyMany(ctx context.Context, db DBTX, arg *PGNotifyManyParams) error {
	_, err := db.ExecContext(ctx, pGNotifyMany, arg.Topic, pq.Array(arg.Payload))
	return err
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/river_client.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_client.sql

package dbsqlc

import (
	"context"
	"time"
)

const clientCreateOrSetUpdatedAt = `-- name: ClientCreateOrSetUpdatedAt :one
INSERT INTO river_client (
    id,
    metadata,
    paused_at,
    updated_at
) VALUES (
    $1,
    coalesce($2::jsonb, '{}'::jsonb),
    coalesce($3::timestamptz, NULL),
    coalesce($4::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce($4::timestamptz, now())
RETURNING id, created_at, metadata, paused_at, updated_at
`

type ClientCreateOrSetUpdatedAtParams struct {
	ID        string
	Metadata  string
	PausedAt  *time.Time
	UpdatedAt *time.Time
}

func (q *Queries) ClientCreateOrSetUpdatedAt(ctx context.Context, db DBTX, arg *ClientCreateOrSetUpdatedAtParams) (*RiverClient, error) {
	row := db.QueryRowContext(ctx, clientCreateOrSetUpdatedAt,
		arg.ID,
		arg.Metadata,
		arg.PausedAt,
		arg.UpdatedAt,
	)
	var i RiverClient
	err := row.Scan(
		&i.ID,
		&i.CreatedAt,
		&i.Metadata,
		&i.PausedAt,
		&i.UpdatedAt,
	)
	return &i, err
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/river_client_queue.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_client_queue.sql

package dbsqlc

import (
	"context"
	"time"

	"github.com/lib/pq"
)

const clientQueueCreateOrSetUpdatedAtMany = `-- name: ClientQueueCreateOrSetUpdatedAtMany :one
INSERT INTO river_client_queue (
    metadata,
    name,
    paused_at,
    river_client_id,
    updated_at
) VALUES (
    coalesce($1::jsonb, '{}'),
    unnest($2::text[]),
    coalesce($3::timestamptz, NULL),
    $4,
    coalesce($5::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce($5::timestamptz, now())
RETURNING river_client_id, name, created_at, max_workers, metadata, num_jobs_completed, num_jobs_running, updated_at
`

type ClientQueueCreateOrSetUpdatedAtManyParams struct {
	Metadata      string
	Name          []string
	PausedAt      *time.Time
	RiverClientID string
	UpdatedAt     *time.Time
}

func (q *Queries) ClientQueueCreateOrSetUpdatedAtMany(ctx context.Context, db DBTX, arg *ClientQueueCreateOrSetUpdatedAtManyParams) (*RiverClientQueue, error) {
	row := db.QueryRowContext(ctx, clientQueueCreateOrSetUpdatedAtMany,
		arg.Metadata,
		pq.Array(arg.Name),
		arg.PausedAt,
		arg.RiverClientID,
		arg.UpdatedAt,
	)
	var i RiverClientQueue
	err := row.Scan(
		&i.RiverClientID,
		&i.Name,
		&i.CreatedAt,
		&i.MaxWorkers,
		&i.Metadata,
		&i.NumJobsCompleted,
		&i.NumJobsRunning,
		&i.UpdatedAt,
	)
	return &i, err
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/river_job.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_job.sql

package dbsqlc

import (
	"context"
	"time"

	"github.com/lib/pq"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql/internal/pgtypealias"
)

const jobCancel = `-- name: JobCancel :one
WITH locked_job AS (
    SELECT
        id, queue, state, finalized_at
    FROM river_job
    WHERE river_job.id = $1
    FOR UPDATE
),
notification AS (
    SELECT
        id,
        pg_notify(
            concat(current_schema(), '.', $2::text),
            json_build_object('action', 'cancel', 'job_id', id, 'queue', queue)::text
        )
    FROM
        locked_job
    WHERE
        state NOT IN ('cancelled', 'completed', 'discarded')
        AND finalized_at IS NULL
),
updated_job AS (
    UPDATE river_job
    SET
        -- If the job is actively running, we want to let its current client and
        -- producer handle the cancellation. Otherwise, immediately cancel it.
        state = CASE WHEN state = 'running' THEN state ELSE 'cancelled' END,
        finalized_at = CASE WHEN state = 'running' THEN finalized_at ELSE now() END,
        -- Mark the job as cancelled by query so that the rescuer knows not to
        -- rescue it, even if it gets stuck in the running state:
        metadata = jsonb_set(metadata, '{cancel_attempted_at}'::text[], $3::jsonb, true)
    FROM notification
    WHERE river_job.id = notification.id
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1::bigint
    AND id NOT IN (SELECT id FROM updated_job)
UNION
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM updated_job
`

type JobCancelParams struct {
	ID                int64
	ControlTopic      string
	CancelAttemptedAt string
}

func (q *Queries) JobCancel(ctx context.Context, db DBTX, arg *JobCancelParams) (*RiverJob, error) {
	row := db.QueryRowContext(ctx, jobCancel, arg.ID, arg.ControlTopic, arg.CancelAttemptedAt)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		pq.Array(&i.AttemptedBy),
		&i.CreatedAt,
		pq.Array(&i.Errors),
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		pq.Array(&i.Tags),
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobCountByState = `-- name: JobCountByState :one
SELECT count(*)
FROM river_job
WHERE state = $1
`

func (q *Queries) JobCountByState(ctx context.Context, db DBTX, state RiverJobState) (int64, error) {
	row := db.QueryRowContext(ctx, jobCountByState, state)
	var count int64
	err := row.Scan(&count)
	return count, err
}

const jobDelete = `-- name: JobDelete :one
WITH job_to_delete AS (
    SELECT id
    FROM river_job
    WHERE river_job.id = $1
    FOR UPDATE
),
deleted_job AS (
    DELETE
    FROM river_job
    USING job_to_delete
    WHERE river_job.id = job_to_delete.id
        -- Do not touch running jobs:
        AND river_job.state != 'running'
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1::bigint
    AND id NOT IN (SELECT id FROM deleted_job)
UNION
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM deleted_job
`

func (q *Queries) JobDelete(ctx context.Context, db DBTX, id int64) (*RiverJob, error) {
	row := db.QueryRowContext(ctx, jobDelete, id)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		pq.Array(&i.AttemptedBy),
		&i.CreatedAt,
		pq.Array(&i.Errors),
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		pq.Array(&i.Tags),
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobDeleteBefore = `-- name: JobDeleteBefore :one
WITH deleted_jobs AS (
    DELETE FROM river_job
    WHERE id IN (
        SELECT id
        FROM river_job
        WHERE
            (state = 'cancelled' AND finalized_at < $1::timestamptz) OR
            (state = 'completed' AND finalized_at < $2::timestamptz) OR
            (state = 'discarded' AND finalized_at < $3::timestamptz)
        ORDER BY id
        LIMIT $4::bigint
    )
    RETURNING id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
)
SELECT count(*)
FROM deleted_jobs
`

type JobDeleteBeforeParams struct {
	CancelledFinalizedAtHorizon time.Time
	CompletedFinalizedAtHorizon time.Time
	DiscardedFinalizedAtHorizon time.Time
	Max                         int64
}

func (q *Queries) JobDeleteBefore(ctx context.Context, db DBTX, arg *JobDeleteBeforeParams) (int64, error) {
	row := db.QueryRowContext(ctx, jobDeleteBefore,
		arg.CancelledFinalizedAtHorizon,
		arg.CompletedFinalizedAtHorizon,
		arg.DiscardedFinalizedAtHorizon,
		arg.Max,
	)
	var count int64
	err := row.Scan(&count)
	return count, err
}

const jobGetAvailable = `-- name: JobGetAvailable :many
WITH locked_jobs AS (
    SELECT
        id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
    FROM
        river_job
    WHERE
        state = 'available'
        AND queue = $2::text
        AND scheduled_at <= coalesce($3::timestamptz, now())
    ORDER BY
        priority ASC,
        scheduled_at ASC,
        id ASC
    LIMIT $4::integer
    FOR UPDATE
    SKIP LOCKED
)
UPDATE
    river_job
SET
    state = 'running',
    attempt = river_job.attempt + 1,
    attempted_at = now(),
    attempted_by = array_append(river_job.attempted_by, $1::text)
FROM
    locked_jobs
WHERE
    river_job.id = locked_jobs.id
RETURNING
    river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
`

type JobGetAvailableParams struct {
	AttemptedBy string
	Queue       string
	Now         *time.Time
	Max         int32
}

func (q *Queries) JobGetAvailable(ctx context.Context, db DBTX, arg *JobGetAvailableParams) ([]*RiverJob, error) {
	rows, err := db.QueryContext(ctx, jobGetAvailable,
		arg.AttemptedBy,
		arg.Queue,
		arg.Now,
		arg.Max,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			pq.Array(&i.AttemptedBy),
			&i.CreatedAt,
			pq.Array(&i.Errors),
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			pq.Array(&i.Tags),
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobGetByID = `-- name: JobGetByID :one
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1
LIMIT 1
`

func (q *Queries) JobGetByID(ctx context.Context, db DBTX, id int64) (*RiverJob, error) {
	row := db.QueryRowContext(ctx, jobGetByID, id)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		pq.Array(&i.AttemptedBy),
		&i.CreatedAt,
		pq.Array(&i.Errors),
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		pq.Array(&i.Tags),
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobGetByIDMany = `-- name: JobGetByIDMany :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = any($1::bigint[])
ORDER BY id
`

func (q *Queries) JobGetByIDMany(ctx context.Context, db DBTX, id []int64) ([]*RiverJob, error) {
	rows, err := db.QueryContext(ctx, jobGetByIDMany, pq.Array(id))
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			pq.Array(&i.AttemptedBy),
			&i.CreatedAt,
			pq.Array(&i.Errors),
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			pq.Array(&i.Tags),
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobGetByKindAndUniqueProperties = `-- name: JobGetByKindAndUniqueProperties :one
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE kind = $1
    AND CASE WHEN $2::boolean THEN args = $3 ELSE true END
    AND CASE WHEN $4::boolean THEN tstzrange($5::timestamptz, $6::timestamptz, '[)') @> created_at ELSE true END
    AND CASE WHEN $7::boolean THEN queue = $8 ELSE true END
    AND CASE WHEN $9::boolean THEN state::text = any($10::text[]) ELSE true END
`

type JobGetByKindAndUniquePropertiesParams struct {
	Kind           string
	ByArgs         bool
	Args           string
	ByCreatedAt    bool
	CreatedAtBegin time.Time
	CreatedAtEnd   time.Time
	ByQueue        bool
	Queue          string
	ByState        bool
	State          []string
}

func (q *Queries) JobGetByKindAndUniqueProperties(ctx context.Context, db DBTX, arg *JobGetByKindAndUniquePropertiesParams) (*RiverJob, error) {
	row := db.QueryRowContext(ctx, jobGetByKindAndUniqueProperties,
		arg.Kind,
		arg.ByArgs,
		arg.Args,
		arg.ByCreatedAt,
		arg.CreatedAtBegin,
		arg.CreatedAtEnd,
		arg.ByQueue,
		arg.Queue,
		arg.ByState,
		pq.Array(arg.State),
	)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		pq.Array(&i.AttemptedBy),
		&i.CreatedAt,
		pq.Array(&i.Errors),
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		pq.Array(&i.Tags),
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobGetByKindMany = `-- name: JobGetByKindMany :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE kind = any($1::text[])
ORDER BY id
`

func (q *Queries) JobGetByKindMany(ctx context.Context, db DBTX, kind []string) ([]*RiverJob, error) {
	rows, err := db.QueryContext(ctx, jobGetByKindMany, pq.Array(kind))
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			pq.Array(&i.AttemptedBy),
			&i.CreatedAt,
			pq.Array(&i.Errors),
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			pq.Array(&i.Tags),
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobGetStuck = `-- name: JobGetStuck :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE state = 'running'
    AND attempted_at < $1::timestamptz
ORDER BY id
LIMIT $2
`

type JobGetStuckParams struct {
	StuckHorizon time.Time
	Max          int32
}

func (q *Queries) JobGetStuck(ctx context.Context, db DBTX, arg *JobGetStuckParams) ([]*RiverJob, error) {
	rows, err := db.QueryContext(ctx, jobGetStuck, arg.StuckHorizon, arg.Max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			pq.Array(&i.AttemptedBy),
			&i.CreatedAt,
			pq.Array(&i.Errors),
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			pq.Array(&i.Tags),
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobInsertFastMany = `-- name: JobInsertFastMany :many
INSERT INTO river_job(
    args,
    created_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) SELECT
    unnest($1::jsonb[]),
    unnest($2::timestamptz[]),
    unnest($3::text[]),
    unnest($4::smallint[]),
    unnest($5::jsonb[]),
    unnest($6::smallint[]),
    unnest($7::text[]),
    unnest($8::timestamptz[]),
    -- To avoid requiring pgx users to register the OID of the river_job_state[]
    -- type, we cast the array to text[] and then to river_job_state.
    unnest($9::text[])::river_job_state,
    -- Unnest on a multi-dimensional array will fully flatten the array, so we
    -- encode the tag list as a comma-separated string and split it in the
    -- query.
    string_to_array(unnest($10::text[]), ','),

    unnest($11::bytea[]),
    unnest($12::bit(8)[])

ON CONFLICT (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state)
    -- Something needs to be updated for a row to be returned on a conflict.
    DO UPDATE SET kind = EXCLUDED.kind
RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states, (xmax != 0) AS unique_skipped_as_duplicate
`

type JobInsertFastManyParams struct {
	Args         []string
	CreatedAt    []time.Time
	Kind         []string
	MaxAttempts  []int16
	Metadata     []string
	Priority     []int16
	Queue        []string
	ScheduledAt  []time.Time
	State        []string
	Tags         []string
	UniqueKey    []pgtypealias.NullBytea
	UniqueStates []pgtypealias.Bits
}

type JobInsertFastManyRow struct {
	RiverJob                 RiverJob
	UniqueSkippedAsDuplicate bool
}

func (q *Queries) JobInsertFastMany(ctx context.Context, db DBTX, arg *JobInsertFastManyParams) ([]*JobInsertFastManyRow, error) {
	rows, err := db.QueryContext(ctx, jobInsertFastMany,
		pq.Array(arg.Args),
		pq.Array(arg.CreatedAt),
		pq.Array(arg.Kind),
		pq.Array(arg.MaxAttempts),
		pq.Array(arg.Metadata),
		pq.Array(arg.Priority),
		pq.Array(arg.Queue),
		pq.Array(arg.ScheduledAt),
		pq.Array(arg.State),
		pq.Array(arg.Tags),
		pq.Array(arg.UniqueKey),
		pq.Array(arg.UniqueStates),
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*JobInsertFastManyRow
	for rows.Next() {
		var i JobInsertFastManyRow
		if err := rows.Scan(
			&i.RiverJob.ID,
			&i.RiverJob.Args,
			&i.RiverJob.Attempt,
			&i.RiverJob.AttemptedAt,
			pq.Array(&i.RiverJob.AttemptedBy),
			&i.RiverJob.CreatedAt,
			pq.Array(&i.RiverJob.Errors),
			&i.RiverJob.FinalizedAt,
			&i.RiverJob.Kind,
			&i.RiverJob.MaxAttempts,
			&i.RiverJob.Metadata,
			&i.RiverJob.Priority,
			&i.RiverJob.Queue,
			&i.RiverJob.State,
			&i.RiverJob.ScheduledAt,
			pq.Array(&i.RiverJob.Tags),
			&i.RiverJob.UniqueKey,
			&i.RiverJob.UniqueStates,
			&i.UniqueSkippedAsDuplicate,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobInsertFastManyNoReturning = `-- name: JobInsertFastManyNoReturning :execrows
INSERT INTO river_job(
    args,
    created_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) SELECT
    unnest($1::jsonb[]),
    unnest($2::timestamptz[]),
    unnest($3::text[]),
    unnest($4::smallint[]),
    unnest($5::jsonb[]),
    unnest($6::smallint[]),
    unnest($7::text[]),
    unnest($8::timestamptz[]),
    unnest($9::river_job_state[]),

    -- lib/pq really, REALLY does not play nicely with multi-dimensional arrays,
    -- so instead we pack each set of tags into a string, send them through,
    -- then unpack them here into an array to put in each row. This isn't
    -- necessary in the Pgx driver where copyfrom is used instead.
    string_to_array(unnest($10::text[]), ','),

    unnest($11::bytea[]),
    unnest($12::bit(8)[])

ON CONFLICT (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state)
DO NOTHING
`

type JobInsertFastManyNoReturningParams struct {
	Args         []string
	CreatedAt    []time.Time
	Kind         []string
	MaxAttempts  []int16
	Metadata     []string
	Priority     []int16
	Queue        []string
	ScheduledAt  []time.Time
	State        []RiverJobState
	Tags         []string
	UniqueKey    []pgtypealias.NullBytea
	UniqueStates []pgtypealias.Bits
}

func (q *Queries) JobInsertFastManyNoReturning(ctx context.Context, db DBTX, arg *JobInsertFastManyNoReturningParams) (int64, error) {
	result, err := db.ExecContext(ctx, jobInsertFastManyNoReturning,
		pq.Array(arg.Args),
		pq.Array(arg.CreatedAt),
		pq.Array(arg.Kind),
		pq.Array(arg.MaxAttempts),
		pq.Array(arg.Metadata),
		pq.Array(arg.Priority),
		pq.Array(arg.Queue),
		pq.Array(arg.ScheduledAt),
		pq.Array(arg.State),
		pq.Array(arg.Tags),
		pq.Array(arg.UniqueKey),
		pq.Array(arg.UniqueStates),
	)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

const jobInsertFull = `-- name: JobInsertFull :one
INSERT INTO river_job(
    args,
    attempt,
    attempted_at,
    attempted_by,
    created_at,
    errors,
    finalized_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) VALUES (
    $1::jsonb,
    coalesce($2::smallint, 0),
    $3,
    coalesce($4::text[], '{}'),
    coalesce($5::timestamptz, now()),
    $6,
    $7,
    $8,
    $9::smallint,
    coalesce($10::jsonb, '{}'),
    $11,
    $12,
    coalesce($13::timestamptz, now()),
    $14,
    coalesce($15::varchar(255)[], '{}'),
    $16,
    $17
) RETURNING id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
`

type JobInsertFullParams struct {
	Args         string
	Attempt      int16
	AttemptedAt  *time.Time
	AttemptedBy  []string
	CreatedAt    *time.Time
	Errors       []string
	FinalizedAt  *time.Time
	Kind         string
	MaxAttempts  int16
	Metadata     string
	Priority     int16
	Queue        string
	ScheduledAt  *time.Time
	State        RiverJobState
	Tags         []string
	UniqueKey    []byte
	UniqueStates pgtypealias.Bits
}

func (q *Queries) JobInsertFull(ctx context.Context, db DBTX, arg *JobInsertFullParams) (*RiverJob, error) {
	row := db.QueryRowContext(ctx, jobInsertFull,
		arg.Args,
		arg.Attempt,
		arg.AttemptedAt,
		pq.Array(arg.AttemptedBy),
		arg.CreatedAt,
		pq.Array(arg.Errors),
		arg.FinalizedAt,
		arg.Kind,
		arg.MaxAttempts,
		arg.Metadata,
		arg.Priority,
		arg.Queue,
		arg.ScheduledAt,
		arg.State,
		pq.Array(arg.Tags),
		arg.UniqueKey,
		arg.UniqueStates,
	)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		pq.Array(&i.AttemptedBy),
		&i.CreatedAt,
		pq.Array(&i.Errors),
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		pq.Array(&i.Tags),
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobList = `-- name: JobList :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */
ORDER BY /* TEMPLATE_BEGIN: order_by_clause */ id /* TEMPLATE_END */
LIMIT $1::int
`

func (q *Queries) JobList(ctx context.Context, db DBTX, max int32) ([]*RiverJob, error) {
	rows, err := db.QueryContext(ctx, jobList, max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			pq.Array(&i.AttemptedBy),
			&i.CreatedAt,
			pq.Array(&i.Errors),
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			pq.Array(&i.Tags),
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobRescueMany = `-- name: JobRescueMany :exec
UPDATE river_job
SET
    errors = array_append(errors, updated_job.error),
    finalized_at = updated_job.finalized_at,
    scheduled_at = updated_job.scheduled_at,
    state = updated_job.state
FROM (
    SELECT
        unnest($1::bigint[]) AS id,
        unnest($2::jsonb[]) AS error,
        nullif(unnest($3::timestamptz[]), '0001-01-01 00:00:00 +0000') AS finalized_at,
        unnest($4::timestamptz[]) AS scheduled_at,
        unnest($5::text[])::river_job_state AS state
) AS updated_job
WHERE river_job.id = updated_job.id
`

type JobRescueManyParams struct {
	ID          []int64
	Error       []string
	FinalizedAt []time.Time
	ScheduledAt []time.Time
	State       []string
}

// Run by the rescuer to queue for retry or discard depending on job state.
func (q *Queries) JobRescueMany(ctx context.Context, db DBTX, arg *JobRescueManyParams) error {
	_, err := db.ExecContext(ctx, jobRescueMany,
		pq.Array(arg.ID),
		pq.Array(arg.Error),
		pq.Array(arg.FinalizedAt),
		pq.Array(arg.ScheduledAt),
		pq.Array(arg.State),
	)
	return err
}

const jobRetry = `-- name: JobRetry :one
WITH job_to_update AS (
    SELECT id
    FROM river_job
    WHERE river_job.id = $1
    FOR UPDATE
),
updated_job AS (
    UPDATE river_job
    SET
        state = 'available',
        scheduled_at = now(),
        max_attempts = CASE WHEN attempt = max_attempts THEN max_attempts + 1 ELSE max_attempts END,
        finalized_at = NULL
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
        -- Do not touch running jobs:
        AND river_job.state != 'running'
        -- If the job is already available with a prior scheduled_at, leave it alone.
        AND NOT (river_job.state = 'available' AND river_job.scheduled_at < now())
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1::bigint
    AND id NOT IN (SELECT id FROM updated_job)
UNION
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM updated_job
`

func (q *Queries) JobRetry(ctx context.Context, db DBTX, id int64) (*RiverJob, error) {
	row := db.QueryRowContext(ctx, jobRetry, id)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		pq.Array(&i.AttemptedBy),
		&i.CreatedAt,
		pq.Array(&i.Errors),
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		pq.Array(&i.Tags),
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobSchedule = `-- name: JobSchedule :many
WITH jobs_to_schedule AS (
    SELECT
        id,
        unique_key,
        unique_states,
        priority,
        scheduled_at
    FROM river_job
    WHERE
        state IN ('retryable', 'scheduled')
        AND queue IS NOT NULL
        AND priority >= 0
        AND scheduled_at <= $1::timestamptz
    ORDER BY
        priority,
        scheduled_at,
        id
    LIMIT $2::bigint
    FOR UPDATE
),
jobs_with_rownum AS (
    SELECT
        id, unique_key, unique_states, priority, scheduled_at,
        CASE
            WHEN unique_key IS NOT NULL AND unique_states IS NOT NULL THEN
                ROW_NUMBER() OVER (
                    PARTITION BY unique_key
                    ORDER BY priority, scheduled_at, id
                )
            ELSE NULL
        END AS row_num
    FROM jobs_to_schedule
),
unique_conflicts AS (
    SELECT river_job.unique_key
    FROM river_job
    JOIN jobs_with_rownum
        ON river_job.unique_key = jobs_with_rownum.unique_key
        AND river_job.id != jobs_with_rownum.id
    WHERE
        river_job.unique_key IS NOT NULL
        AND river_job.unique_states IS NOT NULL
        AND river_job_state_in_bitmask(river_job.unique_states, river_job.state)
),
job_updates AS (
    SELECT
        job.id,
        job.unique_key,
        job.unique_states,
        CASE
            WHEN job.row_num IS NULL THEN 'available'::river_job_state
            WHEN uc.unique_key IS NOT NULL THEN 'discarded'::river_job_state
            WHEN job.row_num = 1 THEN 'available'::river_job_state
            ELSE 'discarded'::river_job_state
        END AS new_state,
        (job.row_num IS NOT NULL AND (uc.unique_key IS NOT NULL OR job.row_num > 1)) AS finalized_at_do_update,
        (job.row_num IS NOT NULL AND (uc.unique_key IS NOT NULL OR job.row_num > 1)) AS metadata_do_update
    FROM jobs_with_rownum job
    LEFT JOIN unique_conflicts uc ON job.unique_key = uc.unique_key
),
updated_jobs AS (
    UPDATE river_job
    SET
        state        = job_updates.new_state,
        finalized_at = CASE WHEN job_updates.finalized_at_do_update THEN $1::timestamptz
                            ELSE river_job.finalized_at END,
        metadata     = CASE WHEN job_updates.metadata_do_update THEN river_job.metadata || '{"unique_key_conflict": "scheduler_discarded"}'::jsonb
                            ELSE river_job.metadata END
    FROM job_updates
    WHERE river_job.id = job_updates.id
    RETURNING
        river_job.id,
        job_updates.new_state = 'discarded'::river_job_state AS conflict_discarded
)
SELECT
    river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states,
    updated_jobs.conflict_discarded
FROM river_job
JOIN updated_jobs ON river_job.id = updated_jobs.id
`

type JobScheduleParams struct {
	Now time.Time
	Max int64
}

type JobScheduleRow struct {
	RiverJob          RiverJob
	ConflictDiscarded bool
}

func (q *Queries) JobSchedule(ctx context.Context, db DBTX, arg *JobScheduleParams) ([]*JobScheduleRow, error) {
	rows, err := db.QueryContext(ctx, jobSchedule, arg.Now, arg.Max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*JobScheduleRow
	for rows.Next() {
		var i JobScheduleRow
		if err := rows.Scan(
			&i.RiverJob.ID,
			&i.RiverJob.Args,
			&i.RiverJob.Attempt,
			&i.RiverJob.AttemptedAt,
			pq.Array(&i.RiverJob.AttemptedBy),
			&i.RiverJob.CreatedAt,
			pq.Array(&i.RiverJob.Errors),
			&i.RiverJob.FinalizedAt,
			&i.RiverJob.Kind,
			&i.RiverJob.MaxAttempts,
			&i.RiverJob.Metadata,
			&i.RiverJob.Priority,
			&i.RiverJob.Queue,
			&i.RiverJob.State,
			&i.RiverJob.ScheduledAt,
			pq.Array(&i.RiverJob.Tags),
			&i.RiverJob.UniqueKey,
			&i.RiverJob.UniqueStates,
			&i.ConflictDiscarded,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobSetStateIfRunningMany = `-- name: JobSetStateIfRunningMany :many
WITH job_input AS (
    SELECT
        unnest($1::bigint[]) AS id,
        unnest($2::boolean[]) AS attempt_do_update,
        unnest($3::int[]) AS attempt,
        unnest($4::boolean[]) AS errors_do_update,
        unnest($5::jsonb[]) AS errors,
        unnest($6::boolean[]) AS finalized_at_do_update,
        unnest($7::timestamptz[]) AS finalized_at,
        unnest($8::boolean[]) AS metadata_do_merge,
        unnest($9::jsonb[]) AS metadata_updates,
        unnest($10::boolean[]) AS scheduled_at_do_update,
        unnest($11::timestamptz[]) AS scheduled_at,
        -- To avoid requiring pgx users to register the OID of the river_job_state[]
        -- type, we cast the array to text[] and then to river_job_state.
        unnest($12::text[])::river_job_state AS state
),
job_to_update AS (
    SELECT
        river_job.id,
        job_input.attempt,
        job_input.attempt_do_update,
        job_input.errors,
        job_input.errors_do_update,
        job_input.finalized_at,
        job_input.finalized_at_do_update,
        job_input.metadata_do_merge,
        job_input.metadata_updates,
        job_input.scheduled_at,
        job_input.scheduled_at_do_update,
        (job_input.state IN ('retryable', 'scheduled') AND river_job.metadata ? 'cancel_attempted_at') AS should_cancel,
        job_input.state
    FROM river_job
    JOIN job_input ON river_job.id = job_input.id
    WHERE river_job.state = 'running' OR job_input.metadata_do_merge
    FOR UPDATE
),
updated_running AS (
    UPDATE river_job
    SET
        attempt      = CASE WHEN NOT job_to_update.should_cancel AND job_to_update.attempt_do_update THEN job_to_update.attempt
                            ELSE river_job.attempt END,
        errors       = CASE WHEN job_to_update.errors_do_update THEN array_append(river_job.errors, job_to_update.errors)
                            ELSE river_job.errors END,
        finalized_at = CASE WHEN job_to_update.should_cancel THEN now()
                            WHEN job_to_update.finalized_at_do_update THEN job_to_update.finalized_at
                            ELSE river_job.finalized_at END,
        metadata     = CASE WHEN job_to_update.metadata_do_merge
                            THEN river_job.metadata || job_to_update.metadata_updates
                            ELSE river_job.metadata END,
        scheduled_at = CASE WHEN NOT job_to_update.should_cancel AND job_to_update.scheduled_at_do_update THEN job_to_update.scheduled_at
                            ELSE river_job.scheduled_at END,
        state        = CASE WHEN job_to_update.should_cancel THEN 'cancelled'::river_job_state
                            ELSE job_to_update.state END
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
      AND river_job.state = 'running'
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
),
updated_metadata_only AS (
    UPDATE river_job
    SET metadata = river_job.metadata || job_to_update.metadata_updates
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
        AND river_job.id NOT IN (SELECT id FROM updated_running)
        AND river_job.state != 'running'
        AND job_to_update.metadata_do_merge
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id IN (SELECT id FROM job_input)
    AND id NOT IN (SELECT id FROM updated_metadata_only)
    AND id NOT IN (SELECT id FROM updated_running)
UNION SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states FROM updated_metadata_only
UNION SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states FROM updated_running
`

type JobSetStateIfRunningManyParams struct {
	IDs                 []int64
	AttemptDoUpdate     []bool
	Attempt             []int32
	ErrorsDoUpdate      []bool
	Errors              []string
	FinalizedAtDoUpdate []bool
	FinalizedAt         []time.Time
	MetadataDoMerge     []bool
	MetadataUpdates     []string
	ScheduledAtDoUpdate []bool
	ScheduledAt         []time.Time
	State               []string
}

func (q *Queries) JobSetStateIfRunningMany(ctx context.Context, db DBTX, arg *JobSetStateIfRunningManyParams) ([]*RiverJob, error) {
	rows, err := db.QueryContext(ctx, jobSetStateIfRunningMany,
		pq.Array(arg.IDs),
		pq.Array(arg.AttemptDoUpdate),
		pq.Array(arg.Attempt),
		pq.Array(arg.ErrorsDoUpdate),
		pq.Array(arg.Errors),
		pq.Array(arg.FinalizedAtDoUpdate),
		pq.Array(arg.FinalizedAt),
		pq.Array(arg.MetadataDoMerge),
		pq.Array(arg.MetadataUpdates),
		pq.Array(arg.ScheduledAtDoUpdate),
		pq.Array(arg.ScheduledAt),
		pq.Array(arg.State),
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			pq.Array(&i.AttemptedBy),
			&i.CreatedAt,
			pq.Array(&i.Errors),
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			pq.Array(&i.Tags),
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobUpdate = `-- name: JobUpdate :one
UPDATE river_job
SET
    attempt = CASE WHEN $1::boolean THEN $2 ELSE attempt END,
    attempted_at = CASE WHEN $3::boolean THEN $4 ELSE attempted_at END,
    attempted_by = CASE WHEN $5::boolean THEN $6 ELSE attempted_by END,
    errors = CASE WHEN $7::boolean THEN $8::jsonb[] ELSE errors END,
    finalized_at = CASE WHEN $9::boolean THEN $10 ELSE finalized_at END,
    state = CASE WHEN $11::boolean THEN $12 ELSE state END
WHERE id = $13
RETURNING id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
`

type JobUpdateParams struct {
	AttemptDoUpdate     bool
	Attempt             int16
	AttemptedAtDoUpdate bool
	AttemptedAt         *time.Time
	AttemptedByDoUpdate bool
	AttemptedBy         []string
	ErrorsDoUpdate      bool
	Errors              []string
	FinalizedAtDoUpdate bool
	FinalizedAt         *time.Time
	StateDoUpdate       bool
	State               RiverJobState
	ID                  int64
}

// A generalized update for any property on a job. This brings in a large number
// of parameters and therefore may be more suitable for testing than production.
func (q *Queries) JobUpdate(ctx context.Context, db DBTX, arg *JobUpdateParams) (*RiverJob, error) {
	row := db.QueryRowContext(ctx, jobUpdate,
		arg.AttemptDoUpdate,
		arg.Attempt,
		arg.AttemptedAtDoUpdate,
		arg.AttemptedAt,
		arg.AttemptedByDoUpdate,
		pq.Array(arg.AttemptedBy),
		arg.ErrorsDoUpdate,
		pq.Array(arg.Errors),
		arg.FinalizedAtDoUpdate,
		arg.FinalizedAt,
		arg.StateDoUpdate,
		arg.State,
		arg.ID,
	)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		pq.Array(&i.AttemptedBy),
		&i.CreatedAt,
		pq.Array(&i.Errors),
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		pq.Array(&i.Tags),
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/river_leader.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_leader.sql

package dbsqlc

import (
	"context"
	"time"
)

const leaderAttemptElect = `-- name: LeaderAttemptElect :execrows
INSERT INTO river_leader(leader_id, elected_at, expires_at)
    VALUES ($1, now(), now() + $2::interval)
ON CONFLICT (name)
    DO NOTHING
`

type LeaderAttemptElectParams struct {
	LeaderID string
	TTL      time.Duration
}

func (q *Queries) LeaderAttemptElect(ctx context.Context, db DBTX, arg *LeaderAttemptElectParams) (int64, error) {
	result, err := db.ExecContext(ctx, leaderAttemptElect, arg.LeaderID, arg.TTL)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

const leaderAttemptReelect = `-- name: LeaderAttemptReelect :execrows
INSERT INTO river_leader(leader_id, elected_at, expires_at)
    VALUES ($1, now(), now() + $2::interval)
ON CONFLICT (name)
    DO UPDATE SET
        expires_at = now() + $2
    WHERE
        river_leader.leader_id = $1
`

type LeaderAttemptReelectParams struct {
	LeaderID string
	TTL      time.Duration
}

func (q *Queries) LeaderAttemptReelect(ctx context.Context, db DBTX, arg *LeaderAttemptReelectParams) (int64, error) {
	result, err := db.ExecContext(ctx, leaderAttemptReelect, arg.LeaderID, arg.TTL)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

const leaderDeleteExpired = `-- name: LeaderDeleteExpired :execrows
DELETE FROM river_leader
WHERE expires_at < now()
`

func (q *Queries) LeaderDeleteExpired(ctx context.Context, db DBTX) (int64, error) {
	result, err := db.ExecContext(ctx, leaderDeleteExpired)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

const leaderGetElectedLeader = `-- name: LeaderGetElectedLeader :one
SELECT elected_at, expires_at, leader_id, name
FROM river_leader
`

func (q *Queries) LeaderGetElectedLeader(ctx context.Context, db DBTX) (*RiverLeader, error) {
	row := db.QueryRowContext(ctx, leaderGetElectedLeader)
	var i RiverLeader
	err := row.Scan(
		&i.ElectedAt,
		&i.ExpiresAt,
		&i.LeaderID,
		&i.Name,
	)
	return &i, err
}

const leaderInsert = `-- name: LeaderInsert :one
INSERT INTO river_leader(
    elected_at,
    expires_at,
    leader_id
) VALUES (
    coalesce($1::timestamptz, now()),
    coalesce($2::timestamptz, now() + $3::interval),
    $4
) RETURNING elected_at, expires_at, leader_id, name
`

type LeaderInsertParams struct {
	ElectedAt *time.Time
	ExpiresAt *time.Time
	TTL       time.Duration
	LeaderID  string
}

func (q *Queries) LeaderInsert(ctx context.Context, db DBTX, arg *LeaderInsertParams) (*RiverLeader, error) {
	row := db.QueryRowContext(ctx, leaderInsert,
		arg.ElectedAt,
		arg.ExpiresAt,
		arg.TTL,
		arg.LeaderID,
	)
	var i RiverLeader
	err := row.Scan(
		&i.ElectedAt,
		&i.ExpiresAt,
		&i.LeaderID,
		&i.Name,
	)
	return &i, err
}

const leaderResign = `-- name: LeaderResign :execrows
WITH currently_held_leaders AS (
  SELECT elected_at, expires_at, leader_id, name
  FROM river_leader
  WHERE leader_id = $1::text
  FOR UPDATE
),
notified_resignations AS (
    SELECT pg_notify(
        concat(current_schema(), '.', $2::text),
        json_build_object('leader_id', leader_id, 'action', 'resigned')::text
    )
    FROM currently_held_leaders
)
DELETE FROM river_leader USING notified_resignations
`

type LeaderResignParams struct {
	LeaderID        string
	LeadershipTopic string
}

func (q *Queries) LeaderResign(ctx context.Context, db DBTX, arg *LeaderResignParams) (int64, error) {
	result, err := db.ExecContext(ctx, leaderResign, arg.LeaderID, arg.LeadershipTopic)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/river_migration.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_migration.sql

package dbsqlc

import (
	"context"
	"time"

	"github.com/lib/pq"
)

const columnExists = `-- name: ColumnExists :one
SELECT EXISTS (
    SELECT column_name
    FROM information_schema.columns 
    WHERE table_name = $1::text
        AND table_schema = CURRENT_SCHEMA
        AND column_name = $2::text
)
`

type ColumnExistsParams struct {
	TableName  string
	ColumnName string
}

func (q *Queries) ColumnExists(ctx context.Context, db DBTX, arg *ColumnExistsParams) (bool, error) {
	row := db.QueryRowContext(ctx, columnExists, arg.TableName, arg.ColumnName)
	var exists bool
	err := row.Scan(&exists)
	return exists, err
}

const riverMigrationDeleteAssumingMainMany = `-- name: RiverMigrationDeleteAssumingMainMany :many
DELETE FROM river_migration
WHERE version = any($1::bigint[])
RETURNING
    created_at,
    version
`

type RiverMigrationDeleteAssumingMainManyRow struct {
	CreatedAt time.Time
	Version   int64
}

func (q *Queries) RiverMigrationDeleteAssumingMainMany(ctx context.Context, db DBTX, version []int64) ([]*RiverMigrationDeleteAssumingMainManyRow, error) {
	rows, err := db.QueryContext(ctx, riverMigrationDeleteAssumingMainMany, pq.Array(version))
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigrationDeleteAssumingMainManyRow
	for rows.Next() {
		var i RiverMigrationDeleteAssumingMainManyRow
		if err := rows.Scan(&i.CreatedAt, &i.Version); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationDeleteByLineAndVersionMany = `-- name: RiverMigrationDeleteByLineAndVersionMany :many
DELETE FROM river_migration
WHERE line = $1
    AND version = any($2::bigint[])
RETURNING line, version, created_at
`

type RiverMigrationDeleteByLineAndVersionManyParams struct {
	Line    string
	Version []int64
}

func (q *Queries) RiverMigrationDeleteByLineAndVersionMany(ctx context.Context, db DBTX, arg *RiverMigrationDeleteByLineAndVersionManyParams) ([]*RiverMigration, error) {
	rows, err := db.QueryContext(ctx, riverMigrationDeleteByLineAndVersionMany, arg.Line, pq.Array(arg.Version))
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigration
	for rows.Next() {
		var i RiverMigration
		if err := rows.Scan(&i.Line, &i.Version, &i.CreatedAt); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationGetAllAssumingMain = `-- name: RiverMigrationGetAllAssumingMain :many
SELECT
    created_at,
    version
FROM river_migration
ORDER BY version
`

type RiverMigrationGetAllAssumingMainRow struct {
	CreatedAt time.Time
	Version   int64
}

// This is a compatibility query for getting existing migrations before the
// `line` column was added to the table in version 005. We need to make sure to
// only select non-line properties so the query doesn't error on older schemas.
// (Even if we use `SELECT *` below, sqlc materializes it to a list of column
// names in the generated query.)
func (q *Queries) RiverMigrationGetAllAssumingMain(ctx context.Context, db DBTX) ([]*RiverMigrationGetAllAssumingMainRow, error) {
	rows, err := db.QueryContext(ctx, riverMigrationGetAllAssumingMain)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigrationGetAllAssumingMainRow
	for rows.Next() {
		var i RiverMigrationGetAllAssumingMainRow
		if err := rows.Scan(&i.CreatedAt, &i.Version); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationGetByLine = `-- name: RiverMigrationGetByLine :many
SELECT line, version, created_at
FROM river_migration
WHERE line = $1
ORDER BY version
`

func (q *Queries) RiverMigrationGetByLine(ctx context.Context, db DBTX, line string) ([]*RiverMigration, error) {
	rows, err := db.QueryContext(ctx, riverMigrationGetByLine, line)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigration
	for rows.Next() {
		var i RiverMigration
		if err := rows.Scan(&i.Line, &i.Version, &i.CreatedAt); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationInsert = `-- name: RiverMigrationInsert :one
INSERT INTO river_migration (
    line,
    version
) VALUES (
    $1,
    $2
) RETURNING line, version, created_at
`

type RiverMigrationInsertParams struct {
	Line    string
	Version int64
}

func (q *Queries) RiverMigrationInsert(ctx context.Context, db DBTX, arg *RiverMigrationInsertParams) (*RiverMigration, error) {
	row := db.QueryRowContext(ctx, riverMigrationInsert, arg.Line, arg.Version)
	var i RiverMigration
	err := row.Scan(&i.Line, &i.Version, &i.CreatedAt)
	return &i, err
}

const riverMigrationInsertMany = `-- name: RiverMigrationInsertMany :many
INSERT INTO river_migration (
    line,
    version
)
SELECT
    $1,
    unnest($2::bigint[])
RETURNING line, version, created_at
`

type RiverMigrationInsertManyParams struct {
	Line    string
	Version []int64
}

func (q *Queries) RiverMigrationInsertMany(ctx context.Context, db DBTX, arg *RiverMigrationInsertManyParams) ([]*RiverMigration, error) {
	rows, err := db.QueryContext(ctx, riverMigrationInsertMany, arg.Line, pq.Array(arg.Version))
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigration
	for rows.Next() {
		var i RiverMigration
		if err := rows.Scan(&i.Line, &i.Version, &i.CreatedAt); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationInsertManyAssumingMain = `-- name: RiverMigrationInsertManyAssumingMain :many
INSERT INTO river_migration (
    version
)
SELECT
    unnest($1::bigint[])
RETURNING
    created_at,
    version
`

type RiverMigrationInsertManyAssumingMainRow struct {
	CreatedAt time.Time
	Version   int64
}

func (q *Queries) RiverMigrationInsertManyAssumingMain(ctx context.Context, db DBTX, version []int64) ([]*RiverMigrationInsertManyAssumingMainRow, error) {
	rows, err := db.QueryContext(ctx, riverMigrationInsertManyAssumingMain, pq.Array(version))
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigrationInsertManyAssumingMainRow
	for rows.Next() {
		var i RiverMigrationInsertManyAssumingMainRow
		if err := rows.Scan(&i.CreatedAt, &i.Version); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const tableExists = `-- name: TableExists :one
SELECT CASE WHEN to_regclass($1) IS NULL THEN false
            ELSE true END
`

func (q *Queries) TableExists(ctx context.Context, db DBTX, tableName string) (bool, error) {
	row := db.QueryRowContext(ctx, tableExists, tableName)
	var column_1 bool
	err := row.Scan(&column_1)
	return column_1, err
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/river_queue.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_queue.sql

package dbsqlc

import (
	"context"
	"database/sql"
	"time"
)

const queueCreateOrSetUpdatedAt = `-- name: QueueCreateOrSetUpdatedAt :one
INSERT INTO river_queue(
    created_at,
    metadata,
    name,
    paused_at,
    updated_at
) VALUES (
    now(),
    coalesce($1::jsonb, '{}'::jsonb),
    $2::text,
    coalesce($3::timestamptz, NULL),
    coalesce($4::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce($4::timestamptz, now())
RETURNING name, created_at, metadata, paused_at, updated_at
`

type QueueCreateOrSetUpdatedAtParams struct {
	Metadata  string
	Name      string
	PausedAt  *time.Time
	UpdatedAt *time.Time
}

func (q *Queries) QueueCreateOrSetUpdatedAt(ctx context.Context, db DBTX, arg *QueueCreateOrSetUpdatedAtParams) (*RiverQueue, error) {
	row := db.QueryRowContext(ctx, queueCreateOrSetUpdatedAt,
		arg.Metadata,
		arg.Name,
		arg.PausedAt,
		arg.UpdatedAt,
	)
	var i RiverQueue
	err := row.Scan(
		&i.Name,
		&i.CreatedAt,
		&i.Metadata,
		&i.PausedAt,
		&i.UpdatedAt,
	)
	return &i, err
}

const queueDeleteExpired = `-- name: QueueDeleteExpired :many
DELETE FROM river_queue
WHERE name IN (
    SELECT name
    FROM river_queue
    WHERE updated_at < $1::timestamptz
    ORDER BY name ASC
    LIMIT $2::bigint
)
RETURNING name, created_at, metadata, paused_at, updated_at
`

type QueueDeleteExpiredParams struct {
	UpdatedAtHorizon time.Time
	Max              int64
}

func (q *Queries) QueueDeleteExpired(ctx context.Context, db DBTX, arg *QueueDeleteExpiredParams) ([]*RiverQueue, error) {
	rows, err := db.QueryContext(ctx, queueDeleteExpired, arg.UpdatedAtHorizon, arg.Max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverQueue
	for rows.Next() {
		var i RiverQueue
		if err := rows.Scan(
			&i.Name,
			&i.CreatedAt,
			&i.Metadata,
			&i.PausedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const queueGet = `-- name: QueueGet :one
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
WHERE name = $1::text
`

func (q *Queries) QueueGet(ctx context.Context, db DBTX, name string) (*RiverQueue, error) {
	row := db.QueryRowContext(ctx, queueGet, name)
	var i RiverQueue
	err := row.Scan(
		&i.Name,
		&i.CreatedAt,
		&i.Metadata,
		&i.PausedAt,
		&i.UpdatedAt,
	)
	return &i, err
}

const queueList = `-- name: QueueList :many
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
ORDER BY name ASC
LIMIT $1::integer
`

func (q *Queries) QueueList(ctx context.Context, db DBTX, limitCount int32) ([]*RiverQueue, error) {
	rows, err := db.QueryContext(ctx, queueList, limitCount)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverQueue
	for rows.Next() {
		var i RiverQueue
		if err := rows.Scan(
			&i.Name,
			&i.CreatedAt,
			&i.Metadata,
			&i.PausedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const queuePause = `-- name: QueuePause :execresult
WITH queue_to_update AS (
    SELECT name, paused_at
    FROM river_queue
    WHERE CASE WHEN $1::text = '*' THEN true ELSE name = $1 END
    FOR UPDATE
),
updated_queue AS (
    UPDATE river_queue
    SET
        paused_at = now(),
        updated_at = now()
    FROM queue_to_update
    WHERE river_queue.name = queue_to_update.name
        AND river_queue.paused_at IS NULL
    RETURNING river_queue.name, river_queue.created_at, river_queue.metadata, river_queue.paused_at, river_queue.updated_at
)
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
WHERE name = $1
    AND name NOT IN (SELECT name FROM updated_queue)
UNION
SELECT name, created_at, metadata, paused_at, updated_at
FROM updated_queue
`

func (q *Queries) QueuePause(ctx context.Context, db DBTX, name string) (sql.Result, error) {
	return db.ExecContext(ctx, queuePause, name)
}

const queueResume = `-- name: QueueResume :execresult
WITH queue_to_update AS (
    SELECT name
    FROM river_queue
    WHERE CASE WHEN $1::text = '*' THEN true ELSE river_queue.name = $1::text END
    FOR UPDATE
),
updated_queue AS (
    UPDATE river_queue
    SET
        paused_at = NULL,
        updated_at = now()
    FROM queue_to_update
    WHERE river_queue.name = queue_to_update.name
    RETURNING river_queue.name, river_queue.created_at, river_queue.metadata, river_queue.paused_at, river_queue.updated_at
)
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
WHERE name = $1
    AND name NOT IN (SELECT name FROM updated_queue)
UNION
SELECT name, created_at, metadata, paused_at, updated_at
FROM updated_queue
`

func (q *Queries) QueueResume(ctx context.Context, db DBTX, name string) (sql.Result, error) {
	return db.ExecContext(ctx, queueResume, name)
}

```

`riverdriver/riverdatabasesql/internal/dbsqlc/sqlc.yaml`:

```yaml
version: "2"
sql:
  - engine: "postgresql"
    queries:
      - ../../../riverpgxv5/internal/dbsqlc/pg_misc.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_client.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_client_queue.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_job.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_leader.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_migration.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_queue.sql
    schema:
      - ../../../riverpgxv5/internal/dbsqlc/pg_misc.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_client.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_client_queue.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_job.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_leader.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_migration.sql
      - ../../../riverpgxv5/internal/dbsqlc/river_queue.sql
    gen:
      go:
        package: "dbsqlc"
        sql_package: "database/sql"
        out: "."
        emit_exact_table_names: true
        emit_methods_with_db_argument: true
        emit_params_struct_pointers: true
        emit_result_struct_pointers: true

        rename:
          ids: "IDs"
          ttl: "TTL"

        overrides:
          - db_type: "bytea"
            go_type:
              import: "github.com/riverqueue/river/riverdriver/riverdatabasesql/internal/pgtypealias"
              type: "NullBytea"

          # `database/sql` really does not play nicely with json/jsonb. If it's
          # left as `[]byte` or `json.RawMessage`, `database/sql` will try to
          # encode it as binary (with a \x) which Postgres won't accept as
          # json/jsonb at all. Using a custom struct crashed and burned, even
          # with a custom scanner implementation. This is the only way I could
          # get it to work: strings are compatible with our use of bytes slices,
          # but Postgres will also accept them as json/jsonb.
          - db_type: "jsonb"
            go_type: "string"

          - db_type: "pg_catalog.interval"
            go_type: "time.Duration"

          - db_type: "timestamptz"
            go_type: "time.Time"

          - db_type: "timestamptz"
            go_type:
              type: "time.Time"
              pointer: true
            nullable: true

          # There doesn't appear to be a good type that's suitable for database/sql other
          # than the ones in pgtype. It's not great to make the database/sql driver take
          # a dependency on pgx, but the reality is most users will (or should) be using
          # pgx anyway.
          #
          # Unfortunately due to some sqlc limitations, you can't just use the
          # pgtype package directly (it tries to use the non-v5 import path and
          # you end up with duplicate pgtype imports). So there's an alias
          # package that exposes it indirectly.
          - db_type: "pg_catalog.bit"
            go_type:
              import: "github.com/riverqueue/river/riverdriver/riverdatabasesql/internal/pgtypealias"
              type: "Bits"

          - db_type: "pg_catalog.bit"
            go_type:
              import: "github.com/riverqueue/river/riverdriver/riverdatabasesql/internal/pgtypealias"
              type: "Bits"
            nullable: true

```

`riverdriver/riverdatabasesql/internal/pgtypealias/null_bytea.go`:

```go
package pgtypealias

import (
	"database/sql/driver"
	"encoding/hex"
	"fmt"
)

// NullBytea is a custom type for Postgres bytea that returns SQL NULL when
// the underlying slice is nil or empty. This override takes over for the base
// type `bytea`, so that when sqlc generates code for arrays of bytea, each
// element is a NullBytea and properly handles nil values. This is in contrast
// to the default behavior of pq.Array in this scenario.
//
// See https://github.com/riverqueue/river/issues/650 for more information.
type NullBytea []byte //nolint:recvcheck

// Value implements the driver.Valuer interface. It returns nil when the
// underlying slice is nil or empty, ensuring that missing values are sent as
// SQL NULL.
func (nb NullBytea) Value() (driver.Value, error) {
	if len(nb) == 0 {
		return nil, nil //nolint:nilnil
	}

	// Encode the byte slice as a hex format string with \x prefix:
	result := make([]byte, 2+hex.EncodedLen(len(nb)))
	result[0] = '\\'
	result[1] = 'x'
	hex.Encode(result[2:], nb)
	return result, nil
}

// Scan implements the sql.Scanner interface.
func (nb *NullBytea) Scan(src interface{}) error {
	if src == nil {
		*nb = nil
		return nil
	}
	b, ok := src.([]byte)
	if !ok {
		return fmt.Errorf("nullBytea.Scan: got %T, expected []byte", src)
	}
	*nb = append((*nb)[0:0], b...)
	return nil
}

```

`riverdriver/riverdatabasesql/internal/pgtypealias/pgtype_alias.go`:

```go
// package pgtypealias exists to work around sqlc bugs with being able to
// reference v5 the pgtype package from within a dbsql package.
package pgtypealias

import "github.com/jackc/pgx/v5/pgtype"

type Bits struct {
	pgtype.Bits
}

```

`riverdriver/riverdatabasesql/migration/main/001_create_river_migration.down.sql`:

```sql
DROP TABLE river_migration;
```

`riverdriver/riverdatabasesql/migration/main/001_create_river_migration.up.sql`:

```sql
CREATE TABLE river_migration(
  id bigserial PRIMARY KEY,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  version bigint NOT NULL,
  CONSTRAINT version CHECK (version >= 1)
);

CREATE UNIQUE INDEX ON river_migration USING btree(version);
```

`riverdriver/riverdatabasesql/migration/main/002_initial_schema.down.sql`:

```sql
DROP TABLE river_job;
DROP FUNCTION river_job_notify;
DROP TYPE river_job_state;

DROP TABLE river_leader;
```

`riverdriver/riverdatabasesql/migration/main/002_initial_schema.up.sql`:

```sql
CREATE TYPE river_job_state AS ENUM(
  'available',
  'cancelled',
  'completed',
  'discarded',
  'retryable',
  'running',
  'scheduled'
);

CREATE TABLE river_job(
  -- 8 bytes
  id bigserial PRIMARY KEY,

  -- 8 bytes (4 bytes + 2 bytes + 2 bytes)
  --
  -- `state` is kept near the top of the table for operator convenience -- when
  -- looking at jobs with `SELECT *` it'll appear first after ID. The other two
  -- fields aren't as important but are kept adjacent to `state` for alignment
  -- to get an 8-byte block.
  state river_job_state NOT NULL DEFAULT 'available',
  attempt smallint NOT NULL DEFAULT 0,
  max_attempts smallint NOT NULL,

  -- 8 bytes each (no alignment needed)
  attempted_at timestamptz,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  finalized_at timestamptz,
  scheduled_at timestamptz NOT NULL DEFAULT NOW(),

  -- 2 bytes (some wasted padding probably)
  priority smallint NOT NULL DEFAULT 1,

  -- types stored out-of-band
  args jsonb,
  attempted_by text[],
  errors jsonb[],
  kind text NOT NULL,
  metadata jsonb NOT NULL DEFAULT '{}',
  queue text NOT NULL DEFAULT 'default',
  tags varchar(255)[],

  CONSTRAINT finalized_or_finalized_at_null CHECK ((state IN ('cancelled', 'completed', 'discarded') AND finalized_at IS NOT NULL) OR finalized_at IS NULL),
  CONSTRAINT max_attempts_is_positive CHECK (max_attempts > 0),
  CONSTRAINT priority_in_range CHECK (priority >= 1 AND priority <= 4),
  CONSTRAINT queue_length CHECK (char_length(queue) > 0 AND char_length(queue) < 128),
  CONSTRAINT kind_length CHECK (char_length(kind) > 0 AND char_length(kind) < 128)
);

-- We may want to consider adding another property here after `kind` if it seems
-- like it'd be useful for something.
CREATE INDEX river_job_kind ON river_job USING btree(kind);

CREATE INDEX river_job_state_and_finalized_at_index ON river_job USING btree(state, finalized_at) WHERE finalized_at IS NOT NULL;

CREATE INDEX river_job_prioritized_fetching_index ON river_job USING btree(state, queue, priority, scheduled_at, id);

CREATE INDEX river_job_args_index ON river_job USING GIN(args);

CREATE INDEX river_job_metadata_index ON river_job USING GIN(metadata);

CREATE OR REPLACE FUNCTION river_job_notify()
  RETURNS TRIGGER
  AS $$
DECLARE
  payload json;
BEGIN
  IF NEW.state = 'available' THEN
    -- Notify will coalesce duplicate notifications within a transaction, so
    -- keep these payloads generalized:
    payload = json_build_object('queue', NEW.queue);
    PERFORM
      pg_notify('river_insert', payload::text);
  END IF;
  RETURN NULL;
END;
$$
LANGUAGE plpgsql;

CREATE TRIGGER river_notify
  AFTER INSERT ON river_job
  FOR EACH ROW
  EXECUTE PROCEDURE river_job_notify();

CREATE UNLOGGED TABLE river_leader(
  -- 8 bytes each (no alignment needed)
  elected_at timestamptz NOT NULL,
  expires_at timestamptz NOT NULL,

  -- types stored out-of-band
  leader_id text NOT NULL,
  name text PRIMARY KEY,

  CONSTRAINT name_length CHECK (char_length(name) > 0 AND char_length(name) < 128),
  CONSTRAINT leader_id_length CHECK (char_length(leader_id) > 0 AND char_length(leader_id) < 128)
);

```

`riverdriver/riverdatabasesql/migration/main/003_river_job_tags_non_null.down.sql`:

```sql
ALTER TABLE river_job ALTER COLUMN tags DROP NOT NULL,
                      ALTER COLUMN tags DROP DEFAULT;

```

`riverdriver/riverdatabasesql/migration/main/003_river_job_tags_non_null.up.sql`:

```sql
ALTER TABLE river_job ALTER COLUMN tags SET DEFAULT '{}';
UPDATE river_job SET tags = '{}' WHERE tags IS NULL;
ALTER TABLE river_job ALTER COLUMN tags SET NOT NULL;

```

`riverdriver/riverdatabasesql/migration/main/004_pending_and_more.down.sql`:

```sql
ALTER TABLE river_job ALTER COLUMN args DROP NOT NULL;

ALTER TABLE river_job ALTER COLUMN metadata DROP NOT NULL;
ALTER TABLE river_job ALTER COLUMN metadata DROP DEFAULT;

-- It is not possible to safely remove 'pending' from the river_job_state enum,
-- so leave it in place.

ALTER TABLE river_job DROP CONSTRAINT finalized_or_finalized_at_null;
ALTER TABLE river_job ADD CONSTRAINT finalized_or_finalized_at_null CHECK (
  (state IN ('cancelled', 'completed', 'discarded') AND finalized_at IS NOT NULL) OR finalized_at IS NULL
);

CREATE OR REPLACE FUNCTION river_job_notify()
  RETURNS TRIGGER
  AS $$
DECLARE
  payload json;
BEGIN
  IF NEW.state = 'available' THEN
    -- Notify will coalesce duplicate notifications within a transaction, so
    -- keep these payloads generalized:
    payload = json_build_object('queue', NEW.queue);
    PERFORM
      pg_notify('river_insert', payload::text);
  END IF;
  RETURN NULL;
END;
$$
LANGUAGE plpgsql;

CREATE TRIGGER river_notify
  AFTER INSERT ON river_job
  FOR EACH ROW
  EXECUTE PROCEDURE river_job_notify();

DROP TABLE river_queue;

ALTER TABLE river_leader
    ALTER COLUMN name DROP DEFAULT,
    DROP CONSTRAINT name_length,
    ADD CONSTRAINT name_length CHECK (char_length(name) > 0 AND char_length(name) < 128);
```

`riverdriver/riverdatabasesql/migration/main/004_pending_and_more.up.sql`:

```sql
-- The args column never had a NOT NULL constraint or default value at the
-- database level, though we tried to ensure one at the application level.
ALTER TABLE river_job ALTER COLUMN args SET DEFAULT '{}';
UPDATE river_job SET args = '{}' WHERE args IS NULL;
ALTER TABLE river_job ALTER COLUMN args SET NOT NULL;
ALTER TABLE river_job ALTER COLUMN args DROP DEFAULT;

-- The metadata column never had a NOT NULL constraint or default value at the
-- database level, though we tried to ensure one at the application level.
ALTER TABLE river_job ALTER COLUMN metadata SET DEFAULT '{}';
UPDATE river_job SET metadata = '{}' WHERE metadata IS NULL;
ALTER TABLE river_job ALTER COLUMN metadata SET NOT NULL;

-- The 'pending' job state will be used for upcoming functionality:
ALTER TYPE river_job_state ADD VALUE IF NOT EXISTS 'pending' AFTER 'discarded';

ALTER TABLE river_job DROP CONSTRAINT finalized_or_finalized_at_null;
ALTER TABLE river_job ADD CONSTRAINT finalized_or_finalized_at_null CHECK (
    (finalized_at IS NULL AND state NOT IN ('cancelled', 'completed', 'discarded')) OR
    (finalized_at IS NOT NULL AND state IN ('cancelled', 'completed', 'discarded'))
);

DROP TRIGGER river_notify ON river_job;
DROP FUNCTION river_job_notify;

CREATE TABLE river_queue(
  name text PRIMARY KEY NOT NULL,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  metadata jsonb NOT NULL DEFAULT '{}' ::jsonb,
  paused_at timestamptz,
  updated_at timestamptz NOT NULL
);

ALTER TABLE river_leader
    ALTER COLUMN name SET DEFAULT 'default',
    DROP CONSTRAINT name_length,
    ADD CONSTRAINT name_length CHECK (name = 'default');
```

`riverdriver/riverdatabasesql/migration/main/005_migration_unique_client.down.sql`:

```sql
--
-- Revert to migration table based only on `(version)`.
--
-- If any non-main migrations are present, 005 is considered irreversible.
--

DO
$body$
BEGIN
    -- Tolerate users who may be using their own migration system rather than
    -- River's. If they are, they will have skipped version 001 containing
    -- `CREATE TABLE river_migration`, so this table won't exist.
    IF (SELECT to_regclass('river_migration') IS NOT NULL) THEN
        IF EXISTS (
            SELECT *
            FROM river_migration
            WHERE line <> 'main'
        ) THEN
            RAISE EXCEPTION 'Found non-main migration lines in the database; version 005 migration is irreversible because it would result in loss of migration information.';
        END IF;

        ALTER TABLE river_migration
            RENAME TO river_migration_old;

        CREATE TABLE river_migration(
            id bigserial PRIMARY KEY,
            created_at timestamptz NOT NULL DEFAULT NOW(),
            version bigint NOT NULL,
            CONSTRAINT version CHECK (version >= 1)
        );

        CREATE UNIQUE INDEX ON river_migration USING btree(version);

        INSERT INTO river_migration
            (created_at, version)
        SELECT created_at, version
        FROM river_migration_old;

        DROP TABLE river_migration_old;
    END IF;
END;
$body$
LANGUAGE 'plpgsql'; 

--
-- Drop `river_job.unique_key`.
--

ALTER TABLE river_job
    DROP COLUMN unique_key;

--
-- Drop `river_client` and derivative.
--

DROP TABLE river_client_queue;
DROP TABLE river_client;

```

`riverdriver/riverdatabasesql/migration/main/005_migration_unique_client.up.sql`:

```sql
--
-- Rebuild the migration table so it's based on `(line, version)`.
--

DO
$body$
BEGIN
    -- Tolerate users who may be using their own migration system rather than
    -- River's. If they are, they will have skipped version 001 containing
    -- `CREATE TABLE river_migration`, so this table won't exist.
    IF (SELECT to_regclass('river_migration') IS NOT NULL) THEN
        ALTER TABLE river_migration
            RENAME TO river_migration_old;

        CREATE TABLE river_migration(
            line TEXT NOT NULL,
            version bigint NOT NULL,
            created_at timestamptz NOT NULL DEFAULT NOW(),
            CONSTRAINT line_length CHECK (char_length(line) > 0 AND char_length(line) < 128),
            CONSTRAINT version_gte_1 CHECK (version >= 1),
            PRIMARY KEY (line, version)
        );

        INSERT INTO river_migration
            (created_at, line, version)
        SELECT created_at, 'main', version
        FROM river_migration_old;

        DROP TABLE river_migration_old;
    END IF;
END;
$body$
LANGUAGE 'plpgsql'; 

--
-- Add `river_job.unique_key` and bring up an index on it.
--

-- These statements use `IF NOT EXISTS` to allow users with a `river_job` table
-- of non-trivial size to build the index `CONCURRENTLY` out of band of this
-- migration, then follow by completing the migration.
ALTER TABLE river_job
    ADD COLUMN IF NOT EXISTS unique_key bytea;

CREATE UNIQUE INDEX IF NOT EXISTS river_job_kind_unique_key_idx ON river_job (kind, unique_key) WHERE unique_key IS NOT NULL;

--
-- Create `river_client` and derivative.
--
-- This feature hasn't quite yet been implemented, but we're taking advantage of
-- the migration to add the schema early so that we can add it later without an
-- additional migration.
--

CREATE UNLOGGED TABLE river_client (
    id text PRIMARY KEY NOT NULL,
    created_at timestamptz NOT NULL DEFAULT now(),
    metadata jsonb NOT NULL DEFAULT '{}',
    paused_at timestamptz,
    updated_at timestamptz NOT NULL,
    CONSTRAINT name_length CHECK (char_length(id) > 0 AND char_length(id) < 128)
);

-- Differs from `river_queue` in that it tracks the queue state for a particular
-- active client.
CREATE UNLOGGED TABLE river_client_queue (
    river_client_id text NOT NULL REFERENCES river_client (id) ON DELETE CASCADE,
    name text NOT NULL,
    created_at timestamptz NOT NULL DEFAULT now(),
    max_workers bigint NOT NULL DEFAULT 0,
    metadata jsonb NOT NULL DEFAULT '{}',
    num_jobs_completed bigint NOT NULL DEFAULT 0,
    num_jobs_running bigint NOT NULL DEFAULT 0,
    updated_at timestamptz NOT NULL,
    PRIMARY KEY (river_client_id, name),
    CONSTRAINT name_length CHECK (char_length(name) > 0 AND char_length(name) < 128),
    CONSTRAINT num_jobs_completed_zero_or_positive CHECK (num_jobs_completed >= 0),
    CONSTRAINT num_jobs_running_zero_or_positive CHECK (num_jobs_running >= 0)
);
```

`riverdriver/riverdatabasesql/migration/main/006_bulk_unique.down.sql`:

```sql

--
-- Drop `river_job.unique_states` and its index.
--

DROP INDEX river_job_unique_idx;

ALTER TABLE river_job
    DROP COLUMN unique_states;

CREATE UNIQUE INDEX IF NOT EXISTS river_job_kind_unique_key_idx ON river_job (kind, unique_key) WHERE unique_key IS NOT NULL;

--
-- Drop `river_job_state_in_bitmask` function.
--
DROP FUNCTION river_job_state_in_bitmask;

```

`riverdriver/riverdatabasesql/migration/main/006_bulk_unique.up.sql`:

```sql

CREATE OR REPLACE FUNCTION river_job_state_in_bitmask(bitmask BIT(8), state river_job_state)
RETURNS boolean
LANGUAGE SQL
IMMUTABLE
AS $$
    SELECT CASE state
        WHEN 'available' THEN get_bit(bitmask, 7)
        WHEN 'cancelled' THEN get_bit(bitmask, 6)
        WHEN 'completed' THEN get_bit(bitmask, 5)
        WHEN 'discarded' THEN get_bit(bitmask, 4)
        WHEN 'pending' THEN get_bit(bitmask, 3)
        WHEN 'retryable' THEN get_bit(bitmask, 2)
        WHEN 'running' THEN get_bit(bitmask, 1)
        WHEN 'scheduled' THEN get_bit(bitmask, 0)
        ELSE 0
    END = 1;
$$;

--
-- Add `river_job.unique_states` and bring up an index on it.
--
-- This column may exist already if users manually created the column and index
-- as instructed in the changelog so the index could be created `CONCURRENTLY`.
--
ALTER TABLE river_job ADD COLUMN IF NOT EXISTS unique_states BIT(8);

-- This statement uses `IF NOT EXISTS` to allow users with a `river_job` table
-- of non-trivial size to build the index `CONCURRENTLY` out of band of this
-- migration, then follow by completing the migration.
CREATE UNIQUE INDEX IF NOT EXISTS river_job_unique_idx ON river_job (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state);

-- Remove the old unique index. Users who are actively using the unique jobs
-- feature and who wish to avoid deploy downtime may want od drop this in a
-- subsequent migration once all jobs using the old unique system have been
-- completed (i.e. no more rows with non-null unique_key and null
-- unique_states).
DROP INDEX river_job_kind_unique_key_idx;

```

`riverdriver/riverdatabasesql/river_database_sql_driver.go`:

```go
// Package riverdatabasesql bundles a River driver for Go's built-in
// database/sql, making it interoperable with ORMs like Bun and GORM. It's
// generally still powered under the hood by Pgx because it's the only
// maintained, fully functional Postgres driver in the Go ecosystem, but it uses
// some lib/pq constructs internally by virtue of being implemented with Sqlc.
package riverdatabasesql

import (
	"context"
	"database/sql"
	"embed"
	"encoding/json"
	"errors"
	"fmt"
	"io/fs"
	"math"
	"strconv"
	"strings"
	"time"

	"github.com/jackc/pgx/v5/pgtype"

	"github.com/riverqueue/river/internal/dbunique"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql/internal/dbsqlc"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql/internal/pgtypealias"
	"github.com/riverqueue/river/rivershared/sqlctemplate"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
	"github.com/riverqueue/river/rivertype"
)

//go:embed migration/*/*.sql
var migrationFS embed.FS

// Driver is an implementation of riverdriver.Driver for database/sql.
type Driver struct {
	dbPool   *sql.DB
	replacer sqlctemplate.Replacer
}

// New returns a new database/sql River driver for use with River.
//
// It takes an sql.DB to use for use with River. The pool should already be
// configured to use the schema specified in the client's Schema field. The pool
// must not be closed while associated River objects are running.
func New(dbPool *sql.DB) *Driver {
	return &Driver{
		dbPool: dbPool,
	}
}

func (d *Driver) GetExecutor() riverdriver.Executor {
	return &Executor{d.dbPool, templateReplaceWrapper{d.dbPool, &d.replacer}, d}
}

func (d *Driver) GetListener() riverdriver.Listener { panic(riverdriver.ErrNotImplemented) }
func (d *Driver) GetMigrationFS(line string) fs.FS {
	if line == riverdriver.MigrationLineMain {
		return migrationFS
	}
	panic("migration line does not exist: " + line)
}
func (d *Driver) GetMigrationLines() []string { return []string{riverdriver.MigrationLineMain} }
func (d *Driver) HasPool() bool               { return d.dbPool != nil }
func (d *Driver) SupportsListener() bool      { return false }

func (d *Driver) UnwrapExecutor(tx *sql.Tx) riverdriver.ExecutorTx {
	// Allows UnwrapExecutor to be invoked even if driver is nil.
	var replacer *sqlctemplate.Replacer
	if d == nil {
		replacer = &sqlctemplate.Replacer{}
	} else {
		replacer = &d.replacer
	}

	return &ExecutorTx{Executor: Executor{nil, templateReplaceWrapper{tx, replacer}, d}, tx: tx}
}

type Executor struct {
	dbPool *sql.DB
	dbtx   templateReplaceWrapper
	driver *Driver
}

func (e *Executor) Begin(ctx context.Context) (riverdriver.ExecutorTx, error) {
	tx, err := e.dbPool.BeginTx(ctx, nil)
	if err != nil {
		return nil, err
	}
	return &ExecutorTx{Executor: Executor{nil, templateReplaceWrapper{tx, &e.driver.replacer}, e.driver}, tx: tx}, nil
}

func (e *Executor) ColumnExists(ctx context.Context, tableName, columnName string) (bool, error) {
	exists, err := dbsqlc.New().ColumnExists(ctx, e.dbtx, &dbsqlc.ColumnExistsParams{
		ColumnName: columnName,
		TableName:  tableName,
	})
	return exists, interpretError(err)
}

func (e *Executor) Exec(ctx context.Context, sql string) (struct{}, error) {
	_, err := e.dbtx.ExecContext(ctx, sql)
	return struct{}{}, interpretError(err)
}

func (e *Executor) JobCancel(ctx context.Context, params *riverdriver.JobCancelParams) (*rivertype.JobRow, error) {
	cancelledAt, err := params.CancelAttemptedAt.MarshalJSON()
	if err != nil {
		return nil, err
	}

	job, err := dbsqlc.New().JobCancel(ctx, e.dbtx, &dbsqlc.JobCancelParams{
		ID:                params.ID,
		CancelAttemptedAt: string(cancelledAt),
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobCountByState(ctx context.Context, state rivertype.JobState) (int, error) {
	numJobs, err := dbsqlc.New().JobCountByState(ctx, e.dbtx, dbsqlc.RiverJobState(state))
	if err != nil {
		return 0, err
	}
	return int(numJobs), nil
}

func (e *Executor) JobDelete(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobDelete(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	if job.State == dbsqlc.RiverJobStateRunning {
		return nil, rivertype.ErrJobRunning
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobDeleteBefore(ctx context.Context, params *riverdriver.JobDeleteBeforeParams) (int, error) {
	numDeleted, err := dbsqlc.New().JobDeleteBefore(ctx, e.dbtx, &dbsqlc.JobDeleteBeforeParams{
		CancelledFinalizedAtHorizon: params.CancelledFinalizedAtHorizon,
		CompletedFinalizedAtHorizon: params.CompletedFinalizedAtHorizon,
		DiscardedFinalizedAtHorizon: params.DiscardedFinalizedAtHorizon,
		Max:                         int64(params.Max),
	})
	return int(numDeleted), interpretError(err)
}

func (e *Executor) JobGetAvailable(ctx context.Context, params *riverdriver.JobGetAvailableParams) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetAvailable(ctx, e.dbtx, &dbsqlc.JobGetAvailableParams{
		AttemptedBy: params.AttemptedBy,
		Max:         int32(min(params.Max, math.MaxInt32)), //nolint:gosec
		Now:         params.Now,
		Queue:       params.Queue,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobGetByID(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobGetByID(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobGetByIDMany(ctx context.Context, id []int64) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetByIDMany(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobGetByKindAndUniqueProperties(ctx context.Context, params *riverdriver.JobGetByKindAndUniquePropertiesParams) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobGetByKindAndUniqueProperties(ctx, e.dbtx, &dbsqlc.JobGetByKindAndUniquePropertiesParams{
		Args:           valutil.ValOrDefault(string(params.Args), "{}"),
		ByArgs:         params.ByArgs,
		ByCreatedAt:    params.ByCreatedAt,
		ByQueue:        params.ByQueue,
		ByState:        params.ByState,
		CreatedAtBegin: params.CreatedAtBegin,
		CreatedAtEnd:   params.CreatedAtEnd,
		Kind:           params.Kind,
		Queue:          params.Queue,
		State:          params.State,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobGetByKindMany(ctx context.Context, kind []string) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetByKindMany(ctx, e.dbtx, kind)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobGetStuck(ctx context.Context, params *riverdriver.JobGetStuckParams) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetStuck(ctx, e.dbtx, &dbsqlc.JobGetStuckParams{Max: int32(min(params.Max, math.MaxInt32)), StuckHorizon: params.StuckHorizon}) //nolint:gosec
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobInsertFastMany(ctx context.Context, params []*riverdriver.JobInsertFastParams) ([]*riverdriver.JobInsertFastResult, error) {
	insertJobsParams := &dbsqlc.JobInsertFastManyParams{
		Args:         make([]string, len(params)),
		CreatedAt:    make([]time.Time, len(params)),
		Kind:         make([]string, len(params)),
		MaxAttempts:  make([]int16, len(params)),
		Metadata:     make([]string, len(params)),
		Priority:     make([]int16, len(params)),
		Queue:        make([]string, len(params)),
		ScheduledAt:  make([]time.Time, len(params)),
		State:        make([]string, len(params)),
		Tags:         make([]string, len(params)),
		UniqueKey:    make([]pgtypealias.NullBytea, len(params)),
		UniqueStates: make([]pgtypealias.Bits, len(params)),
	}
	now := time.Now().UTC()

	for i := 0; i < len(params); i++ {
		params := params[i]

		createdAt := now
		if params.CreatedAt != nil {
			createdAt = *params.CreatedAt
		}

		scheduledAt := now
		if params.ScheduledAt != nil {
			scheduledAt = *params.ScheduledAt
		}

		tags := params.Tags
		if tags == nil {
			tags = []string{}
		}

		defaultObject := "{}"

		insertJobsParams.Args[i] = valutil.ValOrDefault(string(params.EncodedArgs), defaultObject)
		insertJobsParams.CreatedAt[i] = createdAt
		insertJobsParams.Kind[i] = params.Kind
		insertJobsParams.MaxAttempts[i] = int16(min(params.MaxAttempts, math.MaxInt16)) //nolint:gosec
		insertJobsParams.Metadata[i] = valutil.ValOrDefault(string(params.Metadata), defaultObject)
		insertJobsParams.Priority[i] = int16(min(params.Priority, math.MaxInt16)) //nolint:gosec
		insertJobsParams.Queue[i] = params.Queue
		insertJobsParams.ScheduledAt[i] = scheduledAt
		insertJobsParams.State[i] = string(params.State)
		insertJobsParams.Tags[i] = strings.Join(tags, ",")
		insertJobsParams.UniqueKey[i] = sliceutil.FirstNonEmpty(params.UniqueKey)
		insertJobsParams.UniqueStates[i] = pgtypealias.Bits{Bits: pgtype.Bits{Bytes: []byte{params.UniqueStates}, Len: 8, Valid: params.UniqueStates != 0}}
	}

	items, err := dbsqlc.New().JobInsertFastMany(ctx, e.dbtx, insertJobsParams)
	if err != nil {
		return nil, interpretError(err)
	}

	return mapSliceError(items, func(row *dbsqlc.JobInsertFastManyRow) (*riverdriver.JobInsertFastResult, error) {
		job, err := jobRowFromInternal(&row.RiverJob)
		if err != nil {
			return nil, err
		}
		return &riverdriver.JobInsertFastResult{Job: job, UniqueSkippedAsDuplicate: row.UniqueSkippedAsDuplicate}, nil
	})
}

func (e *Executor) JobInsertFastManyNoReturning(ctx context.Context, params []*riverdriver.JobInsertFastParams) (int, error) {
	insertJobsParams := &dbsqlc.JobInsertFastManyNoReturningParams{
		Args:         make([]string, len(params)),
		CreatedAt:    make([]time.Time, len(params)),
		Kind:         make([]string, len(params)),
		MaxAttempts:  make([]int16, len(params)),
		Metadata:     make([]string, len(params)),
		Priority:     make([]int16, len(params)),
		Queue:        make([]string, len(params)),
		ScheduledAt:  make([]time.Time, len(params)),
		State:        make([]dbsqlc.RiverJobState, len(params)),
		Tags:         make([]string, len(params)),
		UniqueKey:    make([]pgtypealias.NullBytea, len(params)),
		UniqueStates: make([]pgtypealias.Bits, len(params)),
	}
	now := time.Now().UTC()

	for i := 0; i < len(params); i++ {
		params := params[i]

		createdAt := now
		if params.CreatedAt != nil {
			createdAt = *params.CreatedAt
		}

		scheduledAt := now
		if params.ScheduledAt != nil {
			scheduledAt = *params.ScheduledAt
		}

		tags := params.Tags
		if tags == nil {
			tags = []string{}
		}

		defaultObject := "{}"

		insertJobsParams.Args[i] = valutil.ValOrDefault(string(params.EncodedArgs), defaultObject)
		insertJobsParams.CreatedAt[i] = createdAt
		insertJobsParams.Kind[i] = params.Kind
		insertJobsParams.MaxAttempts[i] = int16(min(params.MaxAttempts, math.MaxInt16)) //nolint:gosec
		insertJobsParams.Metadata[i] = valutil.ValOrDefault(string(params.Metadata), defaultObject)
		insertJobsParams.Priority[i] = int16(min(params.Priority, math.MaxInt16)) //nolint:gosec
		insertJobsParams.Queue[i] = params.Queue
		insertJobsParams.ScheduledAt[i] = scheduledAt
		insertJobsParams.State[i] = dbsqlc.RiverJobState(params.State)
		insertJobsParams.Tags[i] = strings.Join(tags, ",")
		insertJobsParams.UniqueKey[i] = params.UniqueKey
		insertJobsParams.UniqueStates[i] = pgtypealias.Bits{Bits: pgtype.Bits{Bytes: []byte{params.UniqueStates}, Len: 8, Valid: params.UniqueStates != 0}}
	}

	numInserted, err := dbsqlc.New().JobInsertFastManyNoReturning(ctx, e.dbtx, insertJobsParams)
	if err != nil {
		return 0, interpretError(err)
	}

	return int(numInserted), nil
}

func (e *Executor) JobInsertFull(ctx context.Context, params *riverdriver.JobInsertFullParams) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobInsertFull(ctx, e.dbtx, &dbsqlc.JobInsertFullParams{
		Attempt:      int16(min(params.Attempt, math.MaxInt16)), //nolint:gosec
		AttemptedAt:  params.AttemptedAt,
		AttemptedBy:  params.AttemptedBy,
		Args:         string(params.EncodedArgs),
		CreatedAt:    params.CreatedAt,
		Errors:       sliceutil.Map(params.Errors, func(e []byte) string { return string(e) }),
		FinalizedAt:  params.FinalizedAt,
		Kind:         params.Kind,
		MaxAttempts:  int16(min(params.MaxAttempts, math.MaxInt16)), //nolint:gosec
		Metadata:     valutil.ValOrDefault(string(params.Metadata), "{}"),
		Priority:     int16(min(params.Priority, math.MaxInt16)), //nolint:gosec
		Queue:        params.Queue,
		ScheduledAt:  params.ScheduledAt,
		State:        dbsqlc.RiverJobState(params.State),
		Tags:         params.Tags,
		UniqueKey:    params.UniqueKey,
		UniqueStates: pgtypealias.Bits{Bits: pgtype.Bits{Bytes: []byte{params.UniqueStates}, Len: 8, Valid: params.UniqueStates != 0}},
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobList(ctx context.Context, params *riverdriver.JobListParams) ([]*rivertype.JobRow, error) {
	whereClause, err := replaceNamed(params.WhereClause, params.NamedArgs)
	if err != nil {
		return nil, err
	}

	ctx = sqlctemplate.WithReplacements(ctx, map[string]sqlctemplate.Replacement{
		"order_by_clause": {Value: params.OrderByClause},
		"where_clause":    {Value: whereClause},
	}, nil) // named params not passed because they've already been replaced above

	jobs, err := dbsqlc.New().JobList(ctx, e.dbtx, params.Max)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func escapeSinglePostgresValue(value any) string {
	switch typedValue := value.(type) {
	case bool:
		return strconv.FormatBool(typedValue)
	case float32:
		// The `-1` arg tells Go to represent the number with as few digits as
		// possible. i.e. No unnecessary trailing zeroes.
		return strconv.FormatFloat(float64(typedValue), 'f', -1, 32)
	case float64:
		// The `-1` arg tells Go to represent the number with as few digits as
		// possible. i.e. No unnecessary trailing zeroes.
		return strconv.FormatFloat(typedValue, 'f', -1, 64)
	case int, int16, int32, int64, uint, uint16, uint32, uint64:
		return fmt.Sprintf("%d", value)
	case string:
		return "'" + strings.ReplaceAll(typedValue, "'", "''") + "'"
	default:
		// unreachable as long as new types aren't added to the switch in `replacedNamed` below
		panic("type not supported")
	}
}

func makePostgresArray[T any](values []T) string {
	var sb strings.Builder
	sb.WriteString("ARRAY[")

	for i, value := range values {
		sb.WriteString(escapeSinglePostgresValue(value))

		if i < len(values)-1 {
			sb.WriteString(",")
		}
	}

	sb.WriteString("]")
	return sb.String()
}

// `database/sql` has an `sql.Named` system that should theoretically work for
// named parameters, but neither Pgx or lib/pq implement it, so just use dumb
// string replacement given we're only injecting a very basic value anyway.
func replaceNamed(query string, namedArgs map[string]any) (string, error) {
	for name, value := range namedArgs {
		var escapedValue string

		switch typedValue := value.(type) {
		case bool, float32, float64, int, int16, int32, int64, string, uint, uint16, uint32, uint64:
			escapedValue = escapeSinglePostgresValue(value)

			// This is pretty awkward, but typedValue reverts back to `any` if
			// any of these conditions are combined together, and that prevents
			// us from ranging over the slice. Technically only `[]string` is
			// needed right now, but I included other slice types just so there
			// isn't a surprise later on.
		case []bool:
			escapedValue = makePostgresArray(typedValue)
		case []float32:
			escapedValue = makePostgresArray(typedValue)
		case []float64:
			escapedValue = makePostgresArray(typedValue)
		case []int:
			escapedValue = makePostgresArray(typedValue)
		case []int16:
			escapedValue = makePostgresArray(typedValue)
		case []int32:
			escapedValue = makePostgresArray(typedValue)
		case []int64:
			escapedValue = makePostgresArray(typedValue)
		case []string:
			escapedValue = makePostgresArray(typedValue)
		case []uint:
			escapedValue = makePostgresArray(typedValue)
		case []uint16:
			escapedValue = makePostgresArray(typedValue)
		case []uint32:
			escapedValue = makePostgresArray(typedValue)
		case []uint64:
			escapedValue = makePostgresArray(typedValue)
		default:
			return "", fmt.Errorf("named query parameter @%s is not a supported type", name)
		}

		newQuery := strings.Replace(query, "@"+name, escapedValue, 1)
		if newQuery == query {
			return "", fmt.Errorf("named query parameter @%s not found in query", name)
		}
		query = newQuery
	}

	return query, nil
}

func (e *Executor) JobRescueMany(ctx context.Context, params *riverdriver.JobRescueManyParams) (*struct{}, error) {
	err := dbsqlc.New().JobRescueMany(ctx, e.dbtx, &dbsqlc.JobRescueManyParams{
		ID:          params.ID,
		Error:       sliceutil.Map(params.Error, func(e []byte) string { return string(e) }),
		FinalizedAt: params.FinalizedAt,
		ScheduledAt: params.ScheduledAt,
		State:       params.State,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return &struct{}{}, nil
}

func (e *Executor) JobRetry(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobRetry(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobSchedule(ctx context.Context, params *riverdriver.JobScheduleParams) ([]*riverdriver.JobScheduleResult, error) {
	scheduleResults, err := dbsqlc.New().JobSchedule(ctx, e.dbtx, &dbsqlc.JobScheduleParams{
		Max: int64(params.Max),
		Now: params.Now,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(scheduleResults, func(result *dbsqlc.JobScheduleRow) (*riverdriver.JobScheduleResult, error) {
		job, err := jobRowFromInternal(&result.RiverJob)
		if err != nil {
			return nil, err
		}
		return &riverdriver.JobScheduleResult{ConflictDiscarded: result.ConflictDiscarded, Job: *job}, nil
	})
}

func (e *Executor) JobSetStateIfRunningMany(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
	setStateParams := &dbsqlc.JobSetStateIfRunningManyParams{
		IDs:                 params.ID,
		Attempt:             make([]int32, len(params.ID)),
		AttemptDoUpdate:     make([]bool, len(params.ID)),
		Errors:              make([]string, len(params.ID)),
		ErrorsDoUpdate:      make([]bool, len(params.ID)),
		FinalizedAt:         make([]time.Time, len(params.ID)),
		FinalizedAtDoUpdate: make([]bool, len(params.ID)),
		MetadataDoMerge:     make([]bool, len(params.ID)),
		MetadataUpdates:     make([]string, len(params.ID)),
		ScheduledAt:         make([]time.Time, len(params.ID)),
		ScheduledAtDoUpdate: make([]bool, len(params.ID)),
		State:               make([]string, len(params.ID)),
	}

	const defaultObject = "{}"

	for i := range len(params.ID) {
		setStateParams.Errors[i] = valutil.ValOrDefault(string(params.ErrData[i]), defaultObject)
		if params.Attempt[i] != nil {
			setStateParams.AttemptDoUpdate[i] = true
			setStateParams.Attempt[i] = int32(*params.Attempt[i]) //nolint:gosec
		}
		if params.ErrData[i] != nil {
			setStateParams.ErrorsDoUpdate[i] = true
		}
		if params.FinalizedAt[i] != nil {
			setStateParams.FinalizedAtDoUpdate[i] = true
			setStateParams.FinalizedAt[i] = *params.FinalizedAt[i]
		}
		if params.MetadataDoMerge[i] {
			setStateParams.MetadataDoMerge[i] = true
			setStateParams.MetadataUpdates[i] = string(params.MetadataUpdates[i])
		} else {
			// Work around JSON encoding issues with database/sql which blow up on nil
			// JSON values:
			setStateParams.MetadataUpdates[i] = "{}"
		}
		if params.ScheduledAt[i] != nil {
			setStateParams.ScheduledAtDoUpdate[i] = true
			setStateParams.ScheduledAt[i] = *params.ScheduledAt[i]
		}
		setStateParams.State[i] = string(params.State[i])
	}

	jobs, err := dbsqlc.New().JobSetStateIfRunningMany(ctx, e.dbtx, setStateParams)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobUpdate(ctx context.Context, params *riverdriver.JobUpdateParams) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobUpdate(ctx, e.dbtx, &dbsqlc.JobUpdateParams{
		ID:                  params.ID,
		Attempt:             int16(min(params.Attempt, math.MaxInt16)), //nolint:gosec
		AttemptDoUpdate:     params.AttemptDoUpdate,
		AttemptedAt:         params.AttemptedAt,
		AttemptedAtDoUpdate: params.AttemptedAtDoUpdate,
		AttemptedBy:         params.AttemptedBy,
		AttemptedByDoUpdate: params.AttemptedByDoUpdate,
		ErrorsDoUpdate:      params.ErrorsDoUpdate,
		Errors:              sliceutil.Map(params.Errors, func(e []byte) string { return string(e) }),
		FinalizedAtDoUpdate: params.FinalizedAtDoUpdate,
		FinalizedAt:         params.FinalizedAt,
		StateDoUpdate:       params.StateDoUpdate,
		State:               dbsqlc.RiverJobState(params.State),
	})
	if err != nil {
		return nil, interpretError(err)
	}

	return jobRowFromInternal(job)
}

func (e *Executor) LeaderAttemptElect(ctx context.Context, params *riverdriver.LeaderElectParams) (bool, error) {
	numElectionsWon, err := dbsqlc.New().LeaderAttemptElect(ctx, e.dbtx, &dbsqlc.LeaderAttemptElectParams{
		LeaderID: params.LeaderID,
		TTL:      params.TTL,
	})
	if err != nil {
		return false, interpretError(err)
	}
	return numElectionsWon > 0, nil
}

func (e *Executor) LeaderAttemptReelect(ctx context.Context, params *riverdriver.LeaderElectParams) (bool, error) {
	numElectionsWon, err := dbsqlc.New().LeaderAttemptReelect(ctx, e.dbtx, &dbsqlc.LeaderAttemptReelectParams{
		LeaderID: params.LeaderID,
		TTL:      params.TTL,
	})
	if err != nil {
		return false, interpretError(err)
	}
	return numElectionsWon > 0, nil
}

func (e *Executor) LeaderDeleteExpired(ctx context.Context) (int, error) {
	numDeleted, err := dbsqlc.New().LeaderDeleteExpired(ctx, e.dbtx)
	if err != nil {
		return 0, interpretError(err)
	}
	return int(numDeleted), nil
}

func (e *Executor) LeaderGetElectedLeader(ctx context.Context) (*riverdriver.Leader, error) {
	leader, err := dbsqlc.New().LeaderGetElectedLeader(ctx, e.dbtx)
	if err != nil {
		return nil, interpretError(err)
	}
	return leaderFromInternal(leader), nil
}

func (e *Executor) LeaderInsert(ctx context.Context, params *riverdriver.LeaderInsertParams) (*riverdriver.Leader, error) {
	leader, err := dbsqlc.New().LeaderInsert(ctx, e.dbtx, &dbsqlc.LeaderInsertParams{
		ElectedAt: params.ElectedAt,
		ExpiresAt: params.ExpiresAt,
		LeaderID:  params.LeaderID,
		TTL:       params.TTL,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return leaderFromInternal(leader), nil
}

func (e *Executor) LeaderResign(ctx context.Context, params *riverdriver.LeaderResignParams) (bool, error) {
	numResigned, err := dbsqlc.New().LeaderResign(ctx, e.dbtx, &dbsqlc.LeaderResignParams{
		LeaderID:        params.LeaderID,
		LeadershipTopic: params.LeadershipTopic,
	})
	if err != nil {
		return false, interpretError(err)
	}
	return numResigned > 0, nil
}

func (e *Executor) MigrationDeleteAssumingMainMany(ctx context.Context, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationDeleteAssumingMainMany(ctx, e.dbtx,
		sliceutil.Map(versions, func(v int) int64 { return int64(v) }))
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, func(internal *dbsqlc.RiverMigrationDeleteAssumingMainManyRow) *riverdriver.Migration {
		return &riverdriver.Migration{
			CreatedAt: internal.CreatedAt.UTC(),
			Line:      riverdriver.MigrationLineMain,
			Version:   int(internal.Version),
		}
	}), nil
}

func (e *Executor) MigrationDeleteByLineAndVersionMany(ctx context.Context, line string, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationDeleteByLineAndVersionMany(ctx, e.dbtx, &dbsqlc.RiverMigrationDeleteByLineAndVersionManyParams{
		Line:    line,
		Version: sliceutil.Map(versions, func(v int) int64 { return int64(v) }),
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, migrationFromInternal), nil
}

func (e *Executor) MigrationGetAllAssumingMain(ctx context.Context) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationGetAllAssumingMain(ctx, e.dbtx)
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, func(internal *dbsqlc.RiverMigrationGetAllAssumingMainRow) *riverdriver.Migration {
		return &riverdriver.Migration{
			CreatedAt: internal.CreatedAt.UTC(),
			Line:      riverdriver.MigrationLineMain,
			Version:   int(internal.Version),
		}
	}), nil
}

func (e *Executor) MigrationGetByLine(ctx context.Context, line string) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationGetByLine(ctx, e.dbtx, line)
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, migrationFromInternal), nil
}

func (e *Executor) MigrationInsertMany(ctx context.Context, line string, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationInsertMany(ctx, e.dbtx, &dbsqlc.RiverMigrationInsertManyParams{
		Line:    line,
		Version: sliceutil.Map(versions, func(v int) int64 { return int64(v) }),
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, migrationFromInternal), nil
}

func (e *Executor) MigrationInsertManyAssumingMain(ctx context.Context, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationInsertManyAssumingMain(ctx, e.dbtx,
		sliceutil.Map(versions, func(v int) int64 { return int64(v) }),
	)
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, func(internal *dbsqlc.RiverMigrationInsertManyAssumingMainRow) *riverdriver.Migration {
		return &riverdriver.Migration{
			CreatedAt: internal.CreatedAt.UTC(),
			Line:      riverdriver.MigrationLineMain,
			Version:   int(internal.Version),
		}
	}), nil
}

func (e *Executor) NotifyMany(ctx context.Context, params *riverdriver.NotifyManyParams) error {
	return dbsqlc.New().PGNotifyMany(ctx, e.dbtx, &dbsqlc.PGNotifyManyParams{
		Payload: params.Payload,
		Topic:   params.Topic,
	})
}

func (e *Executor) PGAdvisoryXactLock(ctx context.Context, key int64) (*struct{}, error) {
	err := dbsqlc.New().PGAdvisoryXactLock(ctx, e.dbtx, key)
	return &struct{}{}, interpretError(err)
}

func (e *Executor) QueueCreateOrSetUpdatedAt(ctx context.Context, params *riverdriver.QueueCreateOrSetUpdatedAtParams) (*rivertype.Queue, error) {
	queue, err := dbsqlc.New().QueueCreateOrSetUpdatedAt(ctx, e.dbtx, &dbsqlc.QueueCreateOrSetUpdatedAtParams{
		Metadata:  valutil.ValOrDefault(string(params.Metadata), "{}"),
		Name:      params.Name,
		PausedAt:  params.PausedAt,
		UpdatedAt: params.UpdatedAt,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return queueFromInternal(queue), nil
}

func (e *Executor) QueueDeleteExpired(ctx context.Context, params *riverdriver.QueueDeleteExpiredParams) ([]string, error) {
	queues, err := dbsqlc.New().QueueDeleteExpired(ctx, e.dbtx, &dbsqlc.QueueDeleteExpiredParams{
		Max:              int64(params.Max),
		UpdatedAtHorizon: params.UpdatedAtHorizon,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	queueNames := make([]string, len(queues))
	for i, q := range queues {
		queueNames[i] = q.Name
	}
	return queueNames, nil
}

func (e *Executor) QueueGet(ctx context.Context, name string) (*rivertype.Queue, error) {
	queue, err := dbsqlc.New().QueueGet(ctx, e.dbtx, name)
	if err != nil {
		return nil, interpretError(err)
	}
	return queueFromInternal(queue), nil
}

func (e *Executor) QueueList(ctx context.Context, limit int) ([]*rivertype.Queue, error) {
	internalQueues, err := dbsqlc.New().QueueList(ctx, e.dbtx, int32(min(limit, math.MaxInt32))) //nolint:gosec
	if err != nil {
		return nil, interpretError(err)
	}
	queues := make([]*rivertype.Queue, len(internalQueues))
	for i, q := range internalQueues {
		queues[i] = queueFromInternal(q)
	}
	return queues, nil
}

func (e *Executor) QueuePause(ctx context.Context, name string) error {
	res, err := dbsqlc.New().QueuePause(ctx, e.dbtx, name)
	if err != nil {
		return interpretError(err)
	}
	rowsAffected, err := res.RowsAffected()
	if err != nil {
		return interpretError(err)
	}
	if rowsAffected == 0 && name != riverdriver.AllQueuesString {
		return rivertype.ErrNotFound
	}
	return nil
}

func (e *Executor) QueueResume(ctx context.Context, name string) error {
	res, err := dbsqlc.New().QueueResume(ctx, e.dbtx, name)
	if err != nil {
		return interpretError(err)
	}
	rowsAffected, err := res.RowsAffected()
	if err != nil {
		return interpretError(err)
	}
	if rowsAffected == 0 && name != riverdriver.AllQueuesString {
		return rivertype.ErrNotFound
	}
	return nil
}

func (e *Executor) TableExists(ctx context.Context, tableName string) (bool, error) {
	exists, err := dbsqlc.New().TableExists(ctx, e.dbtx, tableName)
	return exists, interpretError(err)
}

type ExecutorTx struct {
	Executor
	tx *sql.Tx
}

func (t *ExecutorTx) Begin(ctx context.Context) (riverdriver.ExecutorTx, error) {
	return (&ExecutorSubTx{Executor: Executor{nil, templateReplaceWrapper{t.tx, &t.driver.replacer}, t.driver}, savepointNum: 0, single: &singleTransaction{}, tx: t.tx}).Begin(ctx)
}

func (t *ExecutorTx) Commit(ctx context.Context) error {
	// unfortunately, `database/sql` does not take a context ...
	return t.tx.Commit()
}

func (t *ExecutorTx) Rollback(ctx context.Context) error {
	// unfortunately, `database/sql` does not take a context ...
	return t.tx.Rollback()
}

type ExecutorSubTx struct {
	Executor
	savepointNum int
	single       *singleTransaction
	tx           *sql.Tx
}

const savepointPrefix = "river_savepoint_"

func (t *ExecutorSubTx) Begin(ctx context.Context) (riverdriver.ExecutorTx, error) {
	if err := t.single.begin(); err != nil {
		return nil, err
	}

	nextSavepointNum := t.savepointNum + 1
	_, err := t.Exec(ctx, fmt.Sprintf("SAVEPOINT %s%02d", savepointPrefix, nextSavepointNum))
	if err != nil {
		return nil, err
	}
	return &ExecutorSubTx{Executor: Executor{nil, templateReplaceWrapper{t.tx, &t.driver.replacer}, t.driver}, savepointNum: nextSavepointNum, single: &singleTransaction{parent: t.single}, tx: t.tx}, nil
}

func (t *ExecutorSubTx) Commit(ctx context.Context) error {
	defer t.single.setDone()

	if t.single.done {
		return errors.New("tx is closed") // mirrors pgx's behavior for this condition
	}

	// Release destroys a savepoint, keeping all the effects of commands that
	// were run within it (so it's effectively COMMIT for savepoints).
	_, err := t.Exec(ctx, fmt.Sprintf("RELEASE %s%02d", savepointPrefix, t.savepointNum))
	if err != nil {
		return err
	}

	return nil
}

func (t *ExecutorSubTx) Rollback(ctx context.Context) error {
	defer t.single.setDone()

	if t.single.done {
		return errors.New("tx is closed") // mirrors pgx's behavior for this condition
	}

	_, err := t.Exec(ctx, fmt.Sprintf("ROLLBACK TO %s%02d", savepointPrefix, t.savepointNum))
	if err != nil {
		return err
	}

	return nil
}

func interpretError(err error) error {
	if errors.Is(err, sql.ErrNoRows) {
		return rivertype.ErrNotFound
	}
	return err
}

// Not strictly necessary, but a small struct designed to help us route out
// problems where `Begin` might be called multiple times on the same
// subtransaction, which would silently produce the wrong result.
type singleTransaction struct {
	done            bool
	parent          *singleTransaction
	subTxInProgress bool
}

func (t *singleTransaction) begin() error {
	if t.subTxInProgress {
		return errors.New("subtransaction already in progress")
	}
	t.subTxInProgress = true
	return nil
}

func (t *singleTransaction) setDone() {
	t.done = true
	if t.parent != nil {
		t.parent.subTxInProgress = false
	}
}

type templateReplaceWrapper struct {
	dbtx     dbsqlc.DBTX
	replacer *sqlctemplate.Replacer
}

func (w templateReplaceWrapper) ExecContext(ctx context.Context, sql string, args ...interface{}) (sql.Result, error) {
	sql, args = w.replacer.Run(ctx, sql, args)
	return w.dbtx.ExecContext(ctx, sql, args...)
}

func (w templateReplaceWrapper) PrepareContext(ctx context.Context, sql string) (*sql.Stmt, error) {
	sql, _ = w.replacer.Run(ctx, sql, nil)
	return w.dbtx.PrepareContext(ctx, sql)
}

func (w templateReplaceWrapper) QueryContext(ctx context.Context, sql string, args ...interface{}) (*sql.Rows, error) {
	sql, args = w.replacer.Run(ctx, sql, args)
	return w.dbtx.QueryContext(ctx, sql, args...)
}

func (w templateReplaceWrapper) QueryRowContext(ctx context.Context, sql string, args ...interface{}) *sql.Row {
	sql, args = w.replacer.Run(ctx, sql, args)
	return w.dbtx.QueryRowContext(ctx, sql, args...)
}

func jobRowFromInternal(internal *dbsqlc.RiverJob) (*rivertype.JobRow, error) {
	var attemptedAt *time.Time
	if internal.AttemptedAt != nil {
		t := internal.AttemptedAt.UTC()
		attemptedAt = &t
	}

	errors := make([]rivertype.AttemptError, len(internal.Errors))
	for i, rawError := range internal.Errors {
		if err := json.Unmarshal([]byte(rawError), &errors[i]); err != nil {
			return nil, err
		}
	}

	var finalizedAt *time.Time
	if internal.FinalizedAt != nil {
		t := internal.FinalizedAt.UTC()
		finalizedAt = &t
	}

	var uniqueStatesByte byte
	if internal.UniqueStates.Valid && len(internal.UniqueStates.Bytes) > 0 {
		uniqueStatesByte = internal.UniqueStates.Bytes[0]
	}

	return &rivertype.JobRow{
		ID:           internal.ID,
		Attempt:      max(int(internal.Attempt), 0),
		AttemptedAt:  attemptedAt,
		AttemptedBy:  internal.AttemptedBy,
		CreatedAt:    internal.CreatedAt.UTC(),
		EncodedArgs:  []byte(internal.Args),
		Errors:       errors,
		FinalizedAt:  finalizedAt,
		Kind:         internal.Kind,
		MaxAttempts:  max(int(internal.MaxAttempts), 0),
		Metadata:     []byte(internal.Metadata),
		Priority:     max(int(internal.Priority), 0),
		Queue:        internal.Queue,
		ScheduledAt:  internal.ScheduledAt.UTC(),
		State:        rivertype.JobState(internal.State),
		Tags:         internal.Tags,
		UniqueKey:    internal.UniqueKey,
		UniqueStates: dbunique.UniqueBitmaskToStates(uniqueStatesByte),
	}, nil
}

func leaderFromInternal(internal *dbsqlc.RiverLeader) *riverdriver.Leader {
	return &riverdriver.Leader{
		ElectedAt: internal.ElectedAt.UTC(),
		ExpiresAt: internal.ExpiresAt.UTC(),
		LeaderID:  internal.LeaderID,
	}
}

// mapSliceError manipulates a slice and transforms it to a slice of another
// type, returning the first error that occurred invoking the map function, if
// there was one.
func mapSliceError[T any, R any](collection []T, mapFunc func(T) (R, error)) ([]R, error) {
	if collection == nil {
		return nil, nil
	}

	result := make([]R, len(collection))

	for i, item := range collection {
		var err error
		result[i], err = mapFunc(item)
		if err != nil {
			return nil, err
		}
	}

	return result, nil
}

func migrationFromInternal(internal *dbsqlc.RiverMigration) *riverdriver.Migration {
	return &riverdriver.Migration{
		CreatedAt: internal.CreatedAt.UTC(),
		Line:      internal.Line,
		Version:   int(internal.Version),
	}
}

func queueFromInternal(internal *dbsqlc.RiverQueue) *rivertype.Queue {
	var pausedAt *time.Time
	if internal.PausedAt != nil {
		t := internal.PausedAt.UTC()
		pausedAt = &t
	}
	return &rivertype.Queue{
		CreatedAt: internal.CreatedAt.UTC(),
		Metadata:  []byte(internal.Metadata),
		Name:      internal.Name,
		PausedAt:  pausedAt,
		UpdatedAt: internal.UpdatedAt.UTC(),
	}
}

```

`riverdriver/riverdatabasesql/river_database_sql_driver_test.go`:

```go
package riverdatabasesql

import (
	"database/sql"
	"errors"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivertype"
)

// Verify interface compliance.
var _ riverdriver.Driver[*sql.Tx] = New(nil)

func TestNew(t *testing.T) {
	t.Parallel()

	t.Run("AllowsNilDatabasePool", func(t *testing.T) {
		t.Parallel()

		dbPool := &sql.DB{}
		driver := New(dbPool)
		require.Equal(t, dbPool, driver.dbPool)
	})

	t.Run("AllowsNilDatabasePool", func(t *testing.T) {
		t.Parallel()

		driver := New(nil)
		require.Nil(t, driver.dbPool)
	})
}

func TestInterpretError(t *testing.T) {
	t.Parallel()

	require.EqualError(t, interpretError(errors.New("an error")), "an error")
	require.ErrorIs(t, interpretError(sql.ErrNoRows), rivertype.ErrNotFound)
	require.NoError(t, interpretError(nil))
}

func TestReplaceNamed(t *testing.T) {
	t.Parallel()

	testCases := []struct {
		Desc        string
		ExpectedSQL string
		InputSQL    string
		InputArgs   map[string]any
	}{
		{Desc: "Boolean", ExpectedSQL: "SELECT true", InputSQL: "SELECT @bool", InputArgs: map[string]any{"bool": true}},
		{Desc: "Float32", ExpectedSQL: "SELECT 1.23", InputSQL: "SELECT @float32", InputArgs: map[string]any{"float32": float32(1.23)}},
		{Desc: "Float64", ExpectedSQL: "SELECT 1.23", InputSQL: "SELECT @float64", InputArgs: map[string]any{"float64": float64(1.23)}},
		{Desc: "Int", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @int", InputArgs: map[string]any{"int": 123}},
		{Desc: "Int16", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @int16", InputArgs: map[string]any{"int16": int16(123)}},
		{Desc: "Int32", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @int32", InputArgs: map[string]any{"int32": int32(123)}},
		{Desc: "Int64", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @int64", InputArgs: map[string]any{"int64": int64(123)}},
		{Desc: "String", ExpectedSQL: "SELECT 'string value'", InputSQL: "SELECT @string", InputArgs: map[string]any{"string": "string value"}},
		{Desc: "StringWithQuote", ExpectedSQL: "SELECT 'string value with '' quote'", InputSQL: "SELECT @string", InputArgs: map[string]any{"string": "string value with ' quote"}},
		{Desc: "Uint", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @uint", InputArgs: map[string]any{"uint": uint(123)}},
		{Desc: "Uint16", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @uint16", InputArgs: map[string]any{"uint16": uint16(123)}},
		{Desc: "Uint32", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @uint32", InputArgs: map[string]any{"uint32": uint32(123)}},
		{Desc: "Uint64", ExpectedSQL: "SELECT 123", InputSQL: "SELECT @uint64", InputArgs: map[string]any{"uint64": uint64(123)}},

		{Desc: "SliceBoolean", ExpectedSQL: "SELECT ARRAY[false,true]", InputSQL: "SELECT @slice_bool", InputArgs: map[string]any{"slice_bool": []bool{false, true}}},
		{Desc: "SliceFloat32", ExpectedSQL: "SELECT ARRAY[1.23,1.24]", InputSQL: "SELECT @slice_float32", InputArgs: map[string]any{"slice_float32": []float32{1.23, 1.24}}},
		{Desc: "SliceFloat64", ExpectedSQL: "SELECT ARRAY[1.23,1.24]", InputSQL: "SELECT @slice_float64", InputArgs: map[string]any{"slice_float64": []float64{1.23, 1.24}}},
		{Desc: "SliceInt", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_int", InputArgs: map[string]any{"slice_int": []int{123, 124}}},
		{Desc: "SliceInt16", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_int16", InputArgs: map[string]any{"slice_int16": []int16{123, 124}}},
		{Desc: "SliceInt32", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_int32", InputArgs: map[string]any{"slice_int32": []int32{123, 124}}},
		{Desc: "SliceInt64", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_int64", InputArgs: map[string]any{"slice_int64": []int64{123, 124}}},
		{Desc: "SliceString", ExpectedSQL: "SELECT ARRAY['string 1','string 2']", InputSQL: "SELECT @slice_string", InputArgs: map[string]any{"slice_string": []string{"string 1", "string 2"}}},
		{Desc: "SliceUint", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_uint", InputArgs: map[string]any{"slice_uint": []uint{123, 124}}},
		{Desc: "SliceUint16", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_uint16", InputArgs: map[string]any{"slice_uint16": []uint16{123, 124}}},
		{Desc: "SliceUint32", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_uint32", InputArgs: map[string]any{"slice_uint32": []uint32{123, 124}}},
		{Desc: "SliceUint64", ExpectedSQL: "SELECT ARRAY[123,124]", InputSQL: "SELECT @slice_uint64", InputArgs: map[string]any{"slice_uint64": []uint64{123, 124}}},
	}
	for _, tt := range testCases {
		t.Run(tt.Desc, func(t *testing.T) {
			t.Parallel()

			actualSQL, err := replaceNamed(tt.InputSQL, tt.InputArgs)
			require.NoError(t, err)
			require.Equal(t, tt.ExpectedSQL, actualSQL)
		})
	}
}

```

`riverdriver/riverpgxv5/go.mod`:

```mod
module github.com/riverqueue/river/riverdriver/riverpgxv5

go 1.23.0

toolchain go1.24.1

require (
	github.com/jackc/pgx/v5 v5.7.3
	github.com/jackc/puddle/v2 v2.2.2
	github.com/riverqueue/river v0.19.0
	github.com/riverqueue/river/riverdriver v0.19.0
	github.com/riverqueue/river/rivershared v0.19.0
	github.com/riverqueue/river/rivertype v0.19.0
	github.com/stretchr/testify v1.10.0
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/jackc/pgpassfile v1.0.0 // indirect
	github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/tidwall/gjson v1.18.0 // indirect
	github.com/tidwall/match v1.1.1 // indirect
	github.com/tidwall/pretty v1.2.1 // indirect
	github.com/tidwall/sjson v1.2.5 // indirect
	golang.org/x/crypto v0.31.0 // indirect
	golang.org/x/sync v0.12.0 // indirect
	golang.org/x/text v0.23.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

```

`riverdriver/riverpgxv5/go.sum`:

```sum
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=
github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
github.com/jackc/pgx/v5 v5.7.3 h1:PO1wNKj/bTAwxSJnO1Z4Ai8j4magtqg2SLNjEDzcXQo=
github.com/jackc/pgx/v5 v5.7.3/go.mod h1:ncY89UGWxg82EykZUwSpUKEfccBGGYq1xjrOpsbsfGQ=
github.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo=
github.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=
github.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/riverqueue/river v0.19.0 h1:WRh/NXhp+WEEY0HpCYgr4wSRllugYBt30HtyQ3jlz08=
github.com/riverqueue/river v0.19.0/go.mod h1:YJ7LA2uBdqFHQJzKyYc+X6S04KJeiwsS1yU5a1rynlk=
github.com/riverqueue/river/riverdriver v0.19.0 h1:NyHz5DfB13paT2lvaO0CKmwy4SFLbA7n6MFRGRtwii4=
github.com/riverqueue/river/riverdriver v0.19.0/go.mod h1:Soxi08hHkEvopExAp6ADG2437r4coSiB4QpuIL5E28k=
github.com/riverqueue/river/rivershared v0.19.0 h1:TZvFM6CC+QgwQQUMQ5Ueuhx25ptgqcKqZQGsdLJnFeE=
github.com/riverqueue/river/rivershared v0.19.0/go.mod h1:JAvmohuC5lounVk8e3zXZIs07Da3klzEeJo1qDQIbjw=
github.com/riverqueue/river/rivertype v0.19.0 h1:5rwgdh21pVcU9WjrHIIO9qC2dOMdRrrZ/HZZOE0JRyY=
github.com/riverqueue/river/rivertype v0.19.0/go.mod h1:DETcejveWlq6bAb8tHkbgJqmXWVLiFhTiEm8j7co1bE=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/tidwall/gjson v1.14.2/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/gjson v1.18.0 h1:FIDeeyB800efLX89e5a8Y0BNH+LOngJyGrIWxG2FKQY=
github.com/tidwall/gjson v1.18.0/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=
github.com/tidwall/match v1.1.1 h1:+Ho715JplO36QYgwN9PGYNhgZvoUSc9X2c80KVTi+GA=
github.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=
github.com/tidwall/pretty v1.2.0/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/pretty v1.2.1 h1:qjsOFOWWQl+N3RsoF5/ssm1pHmJJwhjlSbZ51I6wMl4=
github.com/tidwall/pretty v1.2.1/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=
github.com/tidwall/sjson v1.2.5 h1:kLy8mja+1c9jlljvWTlSazM7cKDRfJuR/bOJhcY5NcY=
github.com/tidwall/sjson v1.2.5/go.mod h1:Fvgq9kS/6ociJEDnK0Fk1cpYF4FIW6ZF7LAe+6jwd28=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
golang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/sync v0.12.0 h1:MHc5BpPuC30uJk597Ri8TV3CNZcTLu6B6z4lJy+g6Jw=
golang.org/x/sync v0.12.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/text v0.23.0 h1:D71I7dUrlY+VX0gQShAThNGHFxZ13dGLBHQLVl1mJlY=
golang.org/x/text v0.23.0/go.mod h1:/BLNzu4aZCJ1+kcD0DNRotWKage4q2rGVAg4o22unh4=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

```

`riverdriver/riverpgxv5/internal/dbsqlc/copyfrom.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: copyfrom.go

package dbsqlc

import (
	"context"
)

// iteratorForJobInsertFastManyCopyFrom implements pgx.CopyFromSource.
type iteratorForJobInsertFastManyCopyFrom struct {
	rows                 []*JobInsertFastManyCopyFromParams
	skippedFirstNextCall bool
}

func (r *iteratorForJobInsertFastManyCopyFrom) Next() bool {
	if len(r.rows) == 0 {
		return false
	}
	if !r.skippedFirstNextCall {
		r.skippedFirstNextCall = true
		return true
	}
	r.rows = r.rows[1:]
	return len(r.rows) > 0
}

func (r iteratorForJobInsertFastManyCopyFrom) Values() ([]interface{}, error) {
	return []interface{}{
		r.rows[0].Args,
		r.rows[0].CreatedAt,
		r.rows[0].Kind,
		r.rows[0].MaxAttempts,
		r.rows[0].Metadata,
		r.rows[0].Priority,
		r.rows[0].Queue,
		r.rows[0].ScheduledAt,
		r.rows[0].State,
		r.rows[0].Tags,
		r.rows[0].UniqueKey,
		r.rows[0].UniqueStates,
	}, nil
}

func (r iteratorForJobInsertFastManyCopyFrom) Err() error {
	return nil
}

func (q *Queries) JobInsertFastManyCopyFrom(ctx context.Context, db DBTX, arg []*JobInsertFastManyCopyFromParams) (int64, error) {
	return db.CopyFrom(ctx, []string{"river_job"}, []string{"args", "created_at", "kind", "max_attempts", "metadata", "priority", "queue", "scheduled_at", "state", "tags", "unique_key", "unique_states"}, &iteratorForJobInsertFastManyCopyFrom{rows: arg})
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/db.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0

package dbsqlc

import (
	"context"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgconn"
)

type DBTX interface {
	Exec(context.Context, string, ...interface{}) (pgconn.CommandTag, error)
	Query(context.Context, string, ...interface{}) (pgx.Rows, error)
	QueryRow(context.Context, string, ...interface{}) pgx.Row
	CopyFrom(ctx context.Context, tableName pgx.Identifier, columnNames []string, rowSrc pgx.CopyFromSource) (int64, error)
}

func New() *Queries {
	return &Queries{}
}

type Queries struct {
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/models.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0

package dbsqlc

import (
	"database/sql/driver"
	"fmt"
	"time"

	"github.com/jackc/pgx/v5/pgtype"
)

type RiverJobState string

const (
	RiverJobStateAvailable RiverJobState = "available"
	RiverJobStateCancelled RiverJobState = "cancelled"
	RiverJobStateCompleted RiverJobState = "completed"
	RiverJobStateDiscarded RiverJobState = "discarded"
	RiverJobStatePending   RiverJobState = "pending"
	RiverJobStateRetryable RiverJobState = "retryable"
	RiverJobStateRunning   RiverJobState = "running"
	RiverJobStateScheduled RiverJobState = "scheduled"
)

func (e *RiverJobState) Scan(src interface{}) error {
	switch s := src.(type) {
	case []byte:
		*e = RiverJobState(s)
	case string:
		*e = RiverJobState(s)
	default:
		return fmt.Errorf("unsupported scan type for RiverJobState: %T", src)
	}
	return nil
}

type NullRiverJobState struct {
	RiverJobState RiverJobState
	Valid         bool // Valid is true if RiverJobState is not NULL
}

// Scan implements the Scanner interface.
func (ns *NullRiverJobState) Scan(value interface{}) error {
	if value == nil {
		ns.RiverJobState, ns.Valid = "", false
		return nil
	}
	ns.Valid = true
	return ns.RiverJobState.Scan(value)
}

// Value implements the driver Valuer interface.
func (ns NullRiverJobState) Value() (driver.Value, error) {
	if !ns.Valid {
		return nil, nil
	}
	return string(ns.RiverJobState), nil
}

type RiverClient struct {
	ID        string
	CreatedAt time.Time
	Metadata  []byte
	PausedAt  *time.Time
	UpdatedAt time.Time
}

type RiverClientQueue struct {
	RiverClientID    string
	Name             string
	CreatedAt        time.Time
	MaxWorkers       int64
	Metadata         []byte
	NumJobsCompleted int64
	NumJobsRunning   int64
	UpdatedAt        time.Time
}

type RiverJob struct {
	ID           int64
	Args         []byte
	Attempt      int16
	AttemptedAt  *time.Time
	AttemptedBy  []string
	CreatedAt    time.Time
	Errors       [][]byte
	FinalizedAt  *time.Time
	Kind         string
	MaxAttempts  int16
	Metadata     []byte
	Priority     int16
	Queue        string
	State        RiverJobState
	ScheduledAt  time.Time
	Tags         []string
	UniqueKey    []byte
	UniqueStates pgtype.Bits
}

type RiverLeader struct {
	ElectedAt time.Time
	ExpiresAt time.Time
	LeaderID  string
	Name      string
}

type RiverMigration struct {
	Line      string
	Version   int64
	CreatedAt time.Time
}

type RiverQueue struct {
	Name      string
	CreatedAt time.Time
	Metadata  []byte
	PausedAt  *time.Time
	UpdatedAt time.Time
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/pg_misc.sql`:

```sql
-- name: PGAdvisoryXactLock :exec
SELECT pg_advisory_xact_lock(@key);

-- name: PGNotifyMany :exec
WITH topic_to_notify AS (
    SELECT
        concat(current_schema(), '.', @topic::text) AS topic,
        unnest(@payload::text[]) AS payload
)
SELECT pg_notify(
    topic_to_notify.topic,
    topic_to_notify.payload
  )
FROM topic_to_notify;

```

`riverdriver/riverpgxv5/internal/dbsqlc/pg_misc.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: pg_misc.sql

package dbsqlc

import (
	"context"
)

const pGAdvisoryXactLock = `-- name: PGAdvisoryXactLock :exec
SELECT pg_advisory_xact_lock($1)
`

func (q *Queries) PGAdvisoryXactLock(ctx context.Context, db DBTX, key int64) error {
	_, err := db.Exec(ctx, pGAdvisoryXactLock, key)
	return err
}

const pGNotifyMany = `-- name: PGNotifyMany :exec
WITH topic_to_notify AS (
    SELECT
        concat(current_schema(), '.', $1::text) AS topic,
        unnest($2::text[]) AS payload
)
SELECT pg_notify(
    topic_to_notify.topic,
    topic_to_notify.payload
  )
FROM topic_to_notify
`

type PGNotifyManyParams struct {
	Topic   string
	Payload []string
}

func (q *Queries) PGNotifyMany(ctx context.Context, db DBTX, arg *PGNotifyManyParams) error {
	_, err := db.Exec(ctx, pGNotifyMany, arg.Topic, arg.Payload)
	return err
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_client.sql`:

```sql
CREATE UNLOGGED TABLE river_client (
    id text PRIMARY KEY NOT NULL,
    created_at timestamptz NOT NULL DEFAULT CURRENT_TIMESTAMP,
    metadata jsonb NOT NULL DEFAULT '{}' ::jsonb,
    paused_at timestamptz,
    updated_at timestamptz NOT NULL,
    CONSTRAINT name_length CHECK (char_length(id) > 0 AND char_length(id) < 128)
);

-- name: ClientCreateOrSetUpdatedAt :one
INSERT INTO river_client (
    id,
    metadata,
    paused_at,
    updated_at
) VALUES (
    @id,
    coalesce(@metadata::jsonb, '{}'::jsonb),
    coalesce(sqlc.narg('paused_at')::timestamptz, NULL),
    coalesce(sqlc.narg('updated_at')::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce(sqlc.narg('updated_at')::timestamptz, now())
RETURNING *;
```

`riverdriver/riverpgxv5/internal/dbsqlc/river_client.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_client.sql

package dbsqlc

import (
	"context"
	"time"
)

const clientCreateOrSetUpdatedAt = `-- name: ClientCreateOrSetUpdatedAt :one
INSERT INTO river_client (
    id,
    metadata,
    paused_at,
    updated_at
) VALUES (
    $1,
    coalesce($2::jsonb, '{}'::jsonb),
    coalesce($3::timestamptz, NULL),
    coalesce($4::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce($4::timestamptz, now())
RETURNING id, created_at, metadata, paused_at, updated_at
`

type ClientCreateOrSetUpdatedAtParams struct {
	ID        string
	Metadata  []byte
	PausedAt  *time.Time
	UpdatedAt *time.Time
}

func (q *Queries) ClientCreateOrSetUpdatedAt(ctx context.Context, db DBTX, arg *ClientCreateOrSetUpdatedAtParams) (*RiverClient, error) {
	row := db.QueryRow(ctx, clientCreateOrSetUpdatedAt,
		arg.ID,
		arg.Metadata,
		arg.PausedAt,
		arg.UpdatedAt,
	)
	var i RiverClient
	err := row.Scan(
		&i.ID,
		&i.CreatedAt,
		&i.Metadata,
		&i.PausedAt,
		&i.UpdatedAt,
	)
	return &i, err
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_client_queue.sql`:

```sql
CREATE UNLOGGED TABLE river_client_queue (
    river_client_id text NOT NULL REFERENCES river_client (id) ON DELETE CASCADE,
    name text NOT NULL,
    created_at timestamptz NOT NULL DEFAULT now(),
    max_workers bigint NOT NULL DEFAULT 0,
    metadata jsonb NOT NULL DEFAULT '{}',
    num_jobs_completed bigint NOT NULL DEFAULT 0,
    num_jobs_running bigint NOT NULL DEFAULT 0,
    updated_at timestamptz NOT NULL,
    PRIMARY KEY (river_client_id, name),
    CONSTRAINT name_length CHECK (char_length(name) > 0 AND char_length(name) < 128),
    CONSTRAINT num_jobs_completed_zero_or_positive CHECK (num_jobs_completed >= 0),
    CONSTRAINT num_jobs_running_zero_or_positive CHECK (num_jobs_running >= 0)
);

-- name: ClientQueueCreateOrSetUpdatedAtMany :one
INSERT INTO river_client_queue (
    metadata,
    name,
    paused_at,
    river_client_id,
    updated_at
) VALUES (
    coalesce(@metadata::jsonb, '{}'),
    unnest(@name::text[]),
    coalesce(sqlc.narg('paused_at')::timestamptz, NULL),
    @river_client_id,
    coalesce(sqlc.narg('updated_at')::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce(sqlc.narg('updated_at')::timestamptz, now())
RETURNING *;
```

`riverdriver/riverpgxv5/internal/dbsqlc/river_client_queue.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_client_queue.sql

package dbsqlc

import (
	"context"
	"time"
)

const clientQueueCreateOrSetUpdatedAtMany = `-- name: ClientQueueCreateOrSetUpdatedAtMany :one
INSERT INTO river_client_queue (
    metadata,
    name,
    paused_at,
    river_client_id,
    updated_at
) VALUES (
    coalesce($1::jsonb, '{}'),
    unnest($2::text[]),
    coalesce($3::timestamptz, NULL),
    $4,
    coalesce($5::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce($5::timestamptz, now())
RETURNING river_client_id, name, created_at, max_workers, metadata, num_jobs_completed, num_jobs_running, updated_at
`

type ClientQueueCreateOrSetUpdatedAtManyParams struct {
	Metadata      []byte
	Name          []string
	PausedAt      *time.Time
	RiverClientID string
	UpdatedAt     *time.Time
}

func (q *Queries) ClientQueueCreateOrSetUpdatedAtMany(ctx context.Context, db DBTX, arg *ClientQueueCreateOrSetUpdatedAtManyParams) (*RiverClientQueue, error) {
	row := db.QueryRow(ctx, clientQueueCreateOrSetUpdatedAtMany,
		arg.Metadata,
		arg.Name,
		arg.PausedAt,
		arg.RiverClientID,
		arg.UpdatedAt,
	)
	var i RiverClientQueue
	err := row.Scan(
		&i.RiverClientID,
		&i.Name,
		&i.CreatedAt,
		&i.MaxWorkers,
		&i.Metadata,
		&i.NumJobsCompleted,
		&i.NumJobsRunning,
		&i.UpdatedAt,
	)
	return &i, err
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_job.sql`:

```sql
CREATE TYPE river_job_state AS ENUM(
    'available',
    'cancelled',
    'completed',
    'discarded',
    'pending',
    'retryable',
    'running',
    'scheduled'
);

CREATE TABLE river_job(
    id bigserial PRIMARY KEY,
    args jsonb NOT NULL DEFAULT '{}',
    attempt smallint NOT NULL DEFAULT 0,
    attempted_at timestamptz,
    attempted_by text[],
    created_at timestamptz NOT NULL DEFAULT NOW(),
    errors jsonb[],
    finalized_at timestamptz,
    kind text NOT NULL,
    max_attempts smallint NOT NULL,
    metadata jsonb NOT NULL DEFAULT '{}',
    priority smallint NOT NULL DEFAULT 1,
    queue text NOT NULL DEFAULT 'default',
    state river_job_state NOT NULL DEFAULT 'available',
    scheduled_at timestamptz NOT NULL DEFAULT NOW(),
    tags varchar(255)[] NOT NULL DEFAULT '{}',
    unique_key bytea,
    unique_states bit(8),
    CONSTRAINT finalized_or_finalized_at_null CHECK (
        (finalized_at IS NULL AND state NOT IN ('cancelled', 'completed', 'discarded')) OR
        (finalized_at IS NOT NULL AND state IN ('cancelled', 'completed', 'discarded'))
    ),
    CONSTRAINT priority_in_range CHECK (priority >= 1 AND priority <= 4),
    CONSTRAINT queue_length CHECK (char_length(queue) > 0 AND char_length(queue) < 128),
    CONSTRAINT kind_length CHECK (char_length(kind) > 0 AND char_length(kind) < 128)
);

-- name: JobCancel :one
WITH locked_job AS (
    SELECT
        id, queue, state, finalized_at
    FROM river_job
    WHERE river_job.id = @id
    FOR UPDATE
),
notification AS (
    SELECT
        id,
        pg_notify(
            concat(current_schema(), '.', @control_topic::text),
            json_build_object('action', 'cancel', 'job_id', id, 'queue', queue)::text
        )
    FROM
        locked_job
    WHERE
        state NOT IN ('cancelled', 'completed', 'discarded')
        AND finalized_at IS NULL
),
updated_job AS (
    UPDATE river_job
    SET
        -- If the job is actively running, we want to let its current client and
        -- producer handle the cancellation. Otherwise, immediately cancel it.
        state = CASE WHEN state = 'running' THEN state ELSE 'cancelled' END,
        finalized_at = CASE WHEN state = 'running' THEN finalized_at ELSE now() END,
        -- Mark the job as cancelled by query so that the rescuer knows not to
        -- rescue it, even if it gets stuck in the running state:
        metadata = jsonb_set(metadata, '{cancel_attempted_at}'::text[], @cancel_attempted_at::jsonb, true)
    FROM notification
    WHERE river_job.id = notification.id
    RETURNING river_job.*
)
SELECT *
FROM river_job
WHERE id = @id::bigint
    AND id NOT IN (SELECT id FROM updated_job)
UNION
SELECT *
FROM updated_job;

-- name: JobCountByState :one
SELECT count(*)
FROM river_job
WHERE state = @state;

-- name: JobDelete :one
WITH job_to_delete AS (
    SELECT id
    FROM river_job
    WHERE river_job.id = @id
    FOR UPDATE
),
deleted_job AS (
    DELETE
    FROM river_job
    USING job_to_delete
    WHERE river_job.id = job_to_delete.id
        -- Do not touch running jobs:
        AND river_job.state != 'running'
    RETURNING river_job.*
)
SELECT *
FROM river_job
WHERE id = @id::bigint
    AND id NOT IN (SELECT id FROM deleted_job)
UNION
SELECT *
FROM deleted_job;

-- name: JobDeleteBefore :one
WITH deleted_jobs AS (
    DELETE FROM river_job
    WHERE id IN (
        SELECT id
        FROM river_job
        WHERE
            (state = 'cancelled' AND finalized_at < @cancelled_finalized_at_horizon::timestamptz) OR
            (state = 'completed' AND finalized_at < @completed_finalized_at_horizon::timestamptz) OR
            (state = 'discarded' AND finalized_at < @discarded_finalized_at_horizon::timestamptz)
        ORDER BY id
        LIMIT @max::bigint
    )
    RETURNING *
)
SELECT count(*)
FROM deleted_jobs;

-- name: JobGetAvailable :many
WITH locked_jobs AS (
    SELECT
        *
    FROM
        river_job
    WHERE
        state = 'available'
        AND queue = @queue::text
        AND scheduled_at <= coalesce(sqlc.narg('now')::timestamptz, now())
    ORDER BY
        priority ASC,
        scheduled_at ASC,
        id ASC
    LIMIT @max::integer
    FOR UPDATE
    SKIP LOCKED
)
UPDATE
    river_job
SET
    state = 'running',
    attempt = river_job.attempt + 1,
    attempted_at = now(),
    attempted_by = array_append(river_job.attempted_by, @attempted_by::text)
FROM
    locked_jobs
WHERE
    river_job.id = locked_jobs.id
RETURNING
    river_job.*;

-- name: JobGetByKindAndUniqueProperties :one
SELECT *
FROM river_job
WHERE kind = @kind
    AND CASE WHEN @by_args::boolean THEN args = @args ELSE true END
    AND CASE WHEN @by_created_at::boolean THEN tstzrange(@created_at_begin::timestamptz, @created_at_end::timestamptz, '[)') @> created_at ELSE true END
    AND CASE WHEN @by_queue::boolean THEN queue = @queue ELSE true END
    AND CASE WHEN @by_state::boolean THEN state::text = any(@state::text[]) ELSE true END;

-- name: JobGetByKindMany :many
SELECT *
FROM river_job
WHERE kind = any(@kind::text[])
ORDER BY id;

-- name: JobGetByID :one
SELECT *
FROM river_job
WHERE id = @id
LIMIT 1;

-- name: JobGetByIDMany :many
SELECT *
FROM river_job
WHERE id = any(@id::bigint[])
ORDER BY id;

-- name: JobGetStuck :many
SELECT *
FROM river_job
WHERE state = 'running'
    AND attempted_at < @stuck_horizon::timestamptz
ORDER BY id
LIMIT @max;

-- name: JobInsertFastMany :many
INSERT INTO river_job(
    args,
    created_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) SELECT
    unnest(@args::jsonb[]),
    unnest(@created_at::timestamptz[]),
    unnest(@kind::text[]),
    unnest(@max_attempts::smallint[]),
    unnest(@metadata::jsonb[]),
    unnest(@priority::smallint[]),
    unnest(@queue::text[]),
    unnest(@scheduled_at::timestamptz[]),
    -- To avoid requiring pgx users to register the OID of the river_job_state[]
    -- type, we cast the array to text[] and then to river_job_state.
    unnest(@state::text[])::river_job_state,
    -- Unnest on a multi-dimensional array will fully flatten the array, so we
    -- encode the tag list as a comma-separated string and split it in the
    -- query.
    string_to_array(unnest(@tags::text[]), ','),

    unnest(@unique_key::bytea[]),
    unnest(@unique_states::bit(8)[])

ON CONFLICT (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state)
    -- Something needs to be updated for a row to be returned on a conflict.
    DO UPDATE SET kind = EXCLUDED.kind
RETURNING sqlc.embed(river_job), (xmax != 0) AS unique_skipped_as_duplicate;

-- name: JobInsertFastManyNoReturning :execrows
INSERT INTO river_job(
    args,
    created_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) SELECT
    unnest(@args::jsonb[]),
    unnest(@created_at::timestamptz[]),
    unnest(@kind::text[]),
    unnest(@max_attempts::smallint[]),
    unnest(@metadata::jsonb[]),
    unnest(@priority::smallint[]),
    unnest(@queue::text[]),
    unnest(@scheduled_at::timestamptz[]),
    unnest(@state::river_job_state[]),

    -- lib/pq really, REALLY does not play nicely with multi-dimensional arrays,
    -- so instead we pack each set of tags into a string, send them through,
    -- then unpack them here into an array to put in each row. This isn't
    -- necessary in the Pgx driver where copyfrom is used instead.
    string_to_array(unnest(@tags::text[]), ','),

    unnest(@unique_key::bytea[]),
    unnest(@unique_states::bit(8)[])

ON CONFLICT (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state)
DO NOTHING;

-- name: JobInsertFull :one
INSERT INTO river_job(
    args,
    attempt,
    attempted_at,
    attempted_by,
    created_at,
    errors,
    finalized_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) VALUES (
    @args::jsonb,
    coalesce(@attempt::smallint, 0),
    @attempted_at,
    coalesce(@attempted_by::text[], '{}'),
    coalesce(sqlc.narg('created_at')::timestamptz, now()),
    @errors,
    @finalized_at,
    @kind,
    @max_attempts::smallint,
    coalesce(@metadata::jsonb, '{}'),
    @priority,
    @queue,
    coalesce(sqlc.narg('scheduled_at')::timestamptz, now()),
    @state,
    coalesce(@tags::varchar(255)[], '{}'),
    @unique_key,
    @unique_states
) RETURNING *;

-- name: JobList :many
SELECT *
FROM river_job
WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */
ORDER BY /* TEMPLATE_BEGIN: order_by_clause */ id /* TEMPLATE_END */
LIMIT @max::int;

-- Run by the rescuer to queue for retry or discard depending on job state.
-- name: JobRescueMany :exec
UPDATE river_job
SET
    errors = array_append(errors, updated_job.error),
    finalized_at = updated_job.finalized_at,
    scheduled_at = updated_job.scheduled_at,
    state = updated_job.state
FROM (
    SELECT
        unnest(@id::bigint[]) AS id,
        unnest(@error::jsonb[]) AS error,
        nullif(unnest(@finalized_at::timestamptz[]), '0001-01-01 00:00:00 +0000') AS finalized_at,
        unnest(@scheduled_at::timestamptz[]) AS scheduled_at,
        unnest(@state::text[])::river_job_state AS state
) AS updated_job
WHERE river_job.id = updated_job.id;

-- name: JobRetry :one
WITH job_to_update AS (
    SELECT id
    FROM river_job
    WHERE river_job.id = @id
    FOR UPDATE
),
updated_job AS (
    UPDATE river_job
    SET
        state = 'available',
        scheduled_at = now(),
        max_attempts = CASE WHEN attempt = max_attempts THEN max_attempts + 1 ELSE max_attempts END,
        finalized_at = NULL
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
        -- Do not touch running jobs:
        AND river_job.state != 'running'
        -- If the job is already available with a prior scheduled_at, leave it alone.
        AND NOT (river_job.state = 'available' AND river_job.scheduled_at < now())
    RETURNING river_job.*
)
SELECT *
FROM river_job
WHERE id = @id::bigint
    AND id NOT IN (SELECT id FROM updated_job)
UNION
SELECT *
FROM updated_job;

-- name: JobSchedule :many
WITH jobs_to_schedule AS (
    SELECT
        id,
        unique_key,
        unique_states,
        priority,
        scheduled_at
    FROM river_job
    WHERE
        state IN ('retryable', 'scheduled')
        AND queue IS NOT NULL
        AND priority >= 0
        AND scheduled_at <= @now::timestamptz
    ORDER BY
        priority,
        scheduled_at,
        id
    LIMIT @max::bigint
    FOR UPDATE
),
jobs_with_rownum AS (
    SELECT
        *,
        CASE
            WHEN unique_key IS NOT NULL AND unique_states IS NOT NULL THEN
                ROW_NUMBER() OVER (
                    PARTITION BY unique_key
                    ORDER BY priority, scheduled_at, id
                )
            ELSE NULL
        END AS row_num
    FROM jobs_to_schedule
),
unique_conflicts AS (
    SELECT river_job.unique_key
    FROM river_job
    JOIN jobs_with_rownum
        ON river_job.unique_key = jobs_with_rownum.unique_key
        AND river_job.id != jobs_with_rownum.id
    WHERE
        river_job.unique_key IS NOT NULL
        AND river_job.unique_states IS NOT NULL
        AND river_job_state_in_bitmask(river_job.unique_states, river_job.state)
),
job_updates AS (
    SELECT
        job.id,
        job.unique_key,
        job.unique_states,
        CASE
            WHEN job.row_num IS NULL THEN 'available'::river_job_state
            WHEN uc.unique_key IS NOT NULL THEN 'discarded'::river_job_state
            WHEN job.row_num = 1 THEN 'available'::river_job_state
            ELSE 'discarded'::river_job_state
        END AS new_state,
        (job.row_num IS NOT NULL AND (uc.unique_key IS NOT NULL OR job.row_num > 1)) AS finalized_at_do_update,
        (job.row_num IS NOT NULL AND (uc.unique_key IS NOT NULL OR job.row_num > 1)) AS metadata_do_update
    FROM jobs_with_rownum job
    LEFT JOIN unique_conflicts uc ON job.unique_key = uc.unique_key
),
updated_jobs AS (
    UPDATE river_job
    SET
        state        = job_updates.new_state,
        finalized_at = CASE WHEN job_updates.finalized_at_do_update THEN @now::timestamptz
                            ELSE river_job.finalized_at END,
        metadata     = CASE WHEN job_updates.metadata_do_update THEN river_job.metadata || '{"unique_key_conflict": "scheduler_discarded"}'::jsonb
                            ELSE river_job.metadata END
    FROM job_updates
    WHERE river_job.id = job_updates.id
    RETURNING
        river_job.id,
        job_updates.new_state = 'discarded'::river_job_state AS conflict_discarded
)
SELECT
    sqlc.embed(river_job),
    updated_jobs.conflict_discarded
FROM river_job
JOIN updated_jobs ON river_job.id = updated_jobs.id;

-- name: JobSetStateIfRunningMany :many
WITH job_input AS (
    SELECT
        unnest(@ids::bigint[]) AS id,
        unnest(@attempt_do_update::boolean[]) AS attempt_do_update,
        unnest(@attempt::int[]) AS attempt,
        unnest(@errors_do_update::boolean[]) AS errors_do_update,
        unnest(@errors::jsonb[]) AS errors,
        unnest(@finalized_at_do_update::boolean[]) AS finalized_at_do_update,
        unnest(@finalized_at::timestamptz[]) AS finalized_at,
        unnest(@metadata_do_merge::boolean[]) AS metadata_do_merge,
        unnest(@metadata_updates::jsonb[]) AS metadata_updates,
        unnest(@scheduled_at_do_update::boolean[]) AS scheduled_at_do_update,
        unnest(@scheduled_at::timestamptz[]) AS scheduled_at,
        -- To avoid requiring pgx users to register the OID of the river_job_state[]
        -- type, we cast the array to text[] and then to river_job_state.
        unnest(@state::text[])::river_job_state AS state
),
job_to_update AS (
    SELECT
        river_job.id,
        job_input.attempt,
        job_input.attempt_do_update,
        job_input.errors,
        job_input.errors_do_update,
        job_input.finalized_at,
        job_input.finalized_at_do_update,
        job_input.metadata_do_merge,
        job_input.metadata_updates,
        job_input.scheduled_at,
        job_input.scheduled_at_do_update,
        (job_input.state IN ('retryable', 'scheduled') AND river_job.metadata ? 'cancel_attempted_at') AS should_cancel,
        job_input.state
    FROM river_job
    JOIN job_input ON river_job.id = job_input.id
    WHERE river_job.state = 'running' OR job_input.metadata_do_merge
    FOR UPDATE
),
updated_running AS (
    UPDATE river_job
    SET
        attempt      = CASE WHEN NOT job_to_update.should_cancel AND job_to_update.attempt_do_update THEN job_to_update.attempt
                            ELSE river_job.attempt END,
        errors       = CASE WHEN job_to_update.errors_do_update THEN array_append(river_job.errors, job_to_update.errors)
                            ELSE river_job.errors END,
        finalized_at = CASE WHEN job_to_update.should_cancel THEN now()
                            WHEN job_to_update.finalized_at_do_update THEN job_to_update.finalized_at
                            ELSE river_job.finalized_at END,
        metadata     = CASE WHEN job_to_update.metadata_do_merge
                            THEN river_job.metadata || job_to_update.metadata_updates
                            ELSE river_job.metadata END,
        scheduled_at = CASE WHEN NOT job_to_update.should_cancel AND job_to_update.scheduled_at_do_update THEN job_to_update.scheduled_at
                            ELSE river_job.scheduled_at END,
        state        = CASE WHEN job_to_update.should_cancel THEN 'cancelled'::river_job_state
                            ELSE job_to_update.state END
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
      AND river_job.state = 'running'
    RETURNING river_job.*
),
updated_metadata_only AS (
    UPDATE river_job
    SET metadata = river_job.metadata || job_to_update.metadata_updates
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
        AND river_job.id NOT IN (SELECT id FROM updated_running)
        AND river_job.state != 'running'
        AND job_to_update.metadata_do_merge
    RETURNING river_job.*
)
SELECT *
FROM river_job
WHERE id IN (SELECT id FROM job_input)
    AND id NOT IN (SELECT id FROM updated_metadata_only)
    AND id NOT IN (SELECT id FROM updated_running)
UNION SELECT * FROM updated_metadata_only
UNION SELECT * FROM updated_running;

-- A generalized update for any property on a job. This brings in a large number
-- of parameters and therefore may be more suitable for testing than production.
-- name: JobUpdate :one
UPDATE river_job
SET
    attempt = CASE WHEN @attempt_do_update::boolean THEN @attempt ELSE attempt END,
    attempted_at = CASE WHEN @attempted_at_do_update::boolean THEN @attempted_at ELSE attempted_at END,
    attempted_by = CASE WHEN @attempted_by_do_update::boolean THEN @attempted_by ELSE attempted_by END,
    errors = CASE WHEN @errors_do_update::boolean THEN @errors::jsonb[] ELSE errors END,
    finalized_at = CASE WHEN @finalized_at_do_update::boolean THEN @finalized_at ELSE finalized_at END,
    state = CASE WHEN @state_do_update::boolean THEN @state ELSE state END
WHERE id = @id
RETURNING *;

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_job.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_job.sql

package dbsqlc

import (
	"context"
	"time"

	"github.com/jackc/pgx/v5/pgtype"
)

const jobCancel = `-- name: JobCancel :one
WITH locked_job AS (
    SELECT
        id, queue, state, finalized_at
    FROM river_job
    WHERE river_job.id = $1
    FOR UPDATE
),
notification AS (
    SELECT
        id,
        pg_notify(
            concat(current_schema(), '.', $2::text),
            json_build_object('action', 'cancel', 'job_id', id, 'queue', queue)::text
        )
    FROM
        locked_job
    WHERE
        state NOT IN ('cancelled', 'completed', 'discarded')
        AND finalized_at IS NULL
),
updated_job AS (
    UPDATE river_job
    SET
        -- If the job is actively running, we want to let its current client and
        -- producer handle the cancellation. Otherwise, immediately cancel it.
        state = CASE WHEN state = 'running' THEN state ELSE 'cancelled' END,
        finalized_at = CASE WHEN state = 'running' THEN finalized_at ELSE now() END,
        -- Mark the job as cancelled by query so that the rescuer knows not to
        -- rescue it, even if it gets stuck in the running state:
        metadata = jsonb_set(metadata, '{cancel_attempted_at}'::text[], $3::jsonb, true)
    FROM notification
    WHERE river_job.id = notification.id
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1::bigint
    AND id NOT IN (SELECT id FROM updated_job)
UNION
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM updated_job
`

type JobCancelParams struct {
	ID                int64
	ControlTopic      string
	CancelAttemptedAt []byte
}

func (q *Queries) JobCancel(ctx context.Context, db DBTX, arg *JobCancelParams) (*RiverJob, error) {
	row := db.QueryRow(ctx, jobCancel, arg.ID, arg.ControlTopic, arg.CancelAttemptedAt)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		&i.AttemptedBy,
		&i.CreatedAt,
		&i.Errors,
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		&i.Tags,
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobCountByState = `-- name: JobCountByState :one
SELECT count(*)
FROM river_job
WHERE state = $1
`

func (q *Queries) JobCountByState(ctx context.Context, db DBTX, state RiverJobState) (int64, error) {
	row := db.QueryRow(ctx, jobCountByState, state)
	var count int64
	err := row.Scan(&count)
	return count, err
}

const jobDelete = `-- name: JobDelete :one
WITH job_to_delete AS (
    SELECT id
    FROM river_job
    WHERE river_job.id = $1
    FOR UPDATE
),
deleted_job AS (
    DELETE
    FROM river_job
    USING job_to_delete
    WHERE river_job.id = job_to_delete.id
        -- Do not touch running jobs:
        AND river_job.state != 'running'
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1::bigint
    AND id NOT IN (SELECT id FROM deleted_job)
UNION
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM deleted_job
`

func (q *Queries) JobDelete(ctx context.Context, db DBTX, id int64) (*RiverJob, error) {
	row := db.QueryRow(ctx, jobDelete, id)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		&i.AttemptedBy,
		&i.CreatedAt,
		&i.Errors,
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		&i.Tags,
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobDeleteBefore = `-- name: JobDeleteBefore :one
WITH deleted_jobs AS (
    DELETE FROM river_job
    WHERE id IN (
        SELECT id
        FROM river_job
        WHERE
            (state = 'cancelled' AND finalized_at < $1::timestamptz) OR
            (state = 'completed' AND finalized_at < $2::timestamptz) OR
            (state = 'discarded' AND finalized_at < $3::timestamptz)
        ORDER BY id
        LIMIT $4::bigint
    )
    RETURNING id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
)
SELECT count(*)
FROM deleted_jobs
`

type JobDeleteBeforeParams struct {
	CancelledFinalizedAtHorizon time.Time
	CompletedFinalizedAtHorizon time.Time
	DiscardedFinalizedAtHorizon time.Time
	Max                         int64
}

func (q *Queries) JobDeleteBefore(ctx context.Context, db DBTX, arg *JobDeleteBeforeParams) (int64, error) {
	row := db.QueryRow(ctx, jobDeleteBefore,
		arg.CancelledFinalizedAtHorizon,
		arg.CompletedFinalizedAtHorizon,
		arg.DiscardedFinalizedAtHorizon,
		arg.Max,
	)
	var count int64
	err := row.Scan(&count)
	return count, err
}

const jobGetAvailable = `-- name: JobGetAvailable :many
WITH locked_jobs AS (
    SELECT
        id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
    FROM
        river_job
    WHERE
        state = 'available'
        AND queue = $2::text
        AND scheduled_at <= coalesce($3::timestamptz, now())
    ORDER BY
        priority ASC,
        scheduled_at ASC,
        id ASC
    LIMIT $4::integer
    FOR UPDATE
    SKIP LOCKED
)
UPDATE
    river_job
SET
    state = 'running',
    attempt = river_job.attempt + 1,
    attempted_at = now(),
    attempted_by = array_append(river_job.attempted_by, $1::text)
FROM
    locked_jobs
WHERE
    river_job.id = locked_jobs.id
RETURNING
    river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
`

type JobGetAvailableParams struct {
	AttemptedBy string
	Queue       string
	Now         *time.Time
	Max         int32
}

func (q *Queries) JobGetAvailable(ctx context.Context, db DBTX, arg *JobGetAvailableParams) ([]*RiverJob, error) {
	rows, err := db.Query(ctx, jobGetAvailable,
		arg.AttemptedBy,
		arg.Queue,
		arg.Now,
		arg.Max,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			&i.AttemptedBy,
			&i.CreatedAt,
			&i.Errors,
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			&i.Tags,
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobGetByID = `-- name: JobGetByID :one
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1
LIMIT 1
`

func (q *Queries) JobGetByID(ctx context.Context, db DBTX, id int64) (*RiverJob, error) {
	row := db.QueryRow(ctx, jobGetByID, id)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		&i.AttemptedBy,
		&i.CreatedAt,
		&i.Errors,
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		&i.Tags,
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobGetByIDMany = `-- name: JobGetByIDMany :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = any($1::bigint[])
ORDER BY id
`

func (q *Queries) JobGetByIDMany(ctx context.Context, db DBTX, id []int64) ([]*RiverJob, error) {
	rows, err := db.Query(ctx, jobGetByIDMany, id)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			&i.AttemptedBy,
			&i.CreatedAt,
			&i.Errors,
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			&i.Tags,
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobGetByKindAndUniqueProperties = `-- name: JobGetByKindAndUniqueProperties :one
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE kind = $1
    AND CASE WHEN $2::boolean THEN args = $3 ELSE true END
    AND CASE WHEN $4::boolean THEN tstzrange($5::timestamptz, $6::timestamptz, '[)') @> created_at ELSE true END
    AND CASE WHEN $7::boolean THEN queue = $8 ELSE true END
    AND CASE WHEN $9::boolean THEN state::text = any($10::text[]) ELSE true END
`

type JobGetByKindAndUniquePropertiesParams struct {
	Kind           string
	ByArgs         bool
	Args           []byte
	ByCreatedAt    bool
	CreatedAtBegin time.Time
	CreatedAtEnd   time.Time
	ByQueue        bool
	Queue          string
	ByState        bool
	State          []string
}

func (q *Queries) JobGetByKindAndUniqueProperties(ctx context.Context, db DBTX, arg *JobGetByKindAndUniquePropertiesParams) (*RiverJob, error) {
	row := db.QueryRow(ctx, jobGetByKindAndUniqueProperties,
		arg.Kind,
		arg.ByArgs,
		arg.Args,
		arg.ByCreatedAt,
		arg.CreatedAtBegin,
		arg.CreatedAtEnd,
		arg.ByQueue,
		arg.Queue,
		arg.ByState,
		arg.State,
	)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		&i.AttemptedBy,
		&i.CreatedAt,
		&i.Errors,
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		&i.Tags,
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobGetByKindMany = `-- name: JobGetByKindMany :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE kind = any($1::text[])
ORDER BY id
`

func (q *Queries) JobGetByKindMany(ctx context.Context, db DBTX, kind []string) ([]*RiverJob, error) {
	rows, err := db.Query(ctx, jobGetByKindMany, kind)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			&i.AttemptedBy,
			&i.CreatedAt,
			&i.Errors,
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			&i.Tags,
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobGetStuck = `-- name: JobGetStuck :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE state = 'running'
    AND attempted_at < $1::timestamptz
ORDER BY id
LIMIT $2
`

type JobGetStuckParams struct {
	StuckHorizon time.Time
	Max          int32
}

func (q *Queries) JobGetStuck(ctx context.Context, db DBTX, arg *JobGetStuckParams) ([]*RiverJob, error) {
	rows, err := db.Query(ctx, jobGetStuck, arg.StuckHorizon, arg.Max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			&i.AttemptedBy,
			&i.CreatedAt,
			&i.Errors,
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			&i.Tags,
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobInsertFastMany = `-- name: JobInsertFastMany :many
INSERT INTO river_job(
    args,
    created_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) SELECT
    unnest($1::jsonb[]),
    unnest($2::timestamptz[]),
    unnest($3::text[]),
    unnest($4::smallint[]),
    unnest($5::jsonb[]),
    unnest($6::smallint[]),
    unnest($7::text[]),
    unnest($8::timestamptz[]),
    -- To avoid requiring pgx users to register the OID of the river_job_state[]
    -- type, we cast the array to text[] and then to river_job_state.
    unnest($9::text[])::river_job_state,
    -- Unnest on a multi-dimensional array will fully flatten the array, so we
    -- encode the tag list as a comma-separated string and split it in the
    -- query.
    string_to_array(unnest($10::text[]), ','),

    unnest($11::bytea[]),
    unnest($12::bit(8)[])

ON CONFLICT (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state)
    -- Something needs to be updated for a row to be returned on a conflict.
    DO UPDATE SET kind = EXCLUDED.kind
RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states, (xmax != 0) AS unique_skipped_as_duplicate
`

type JobInsertFastManyParams struct {
	Args         [][]byte
	CreatedAt    []time.Time
	Kind         []string
	MaxAttempts  []int16
	Metadata     [][]byte
	Priority     []int16
	Queue        []string
	ScheduledAt  []time.Time
	State        []string
	Tags         []string
	UniqueKey    [][]byte
	UniqueStates []pgtype.Bits
}

type JobInsertFastManyRow struct {
	RiverJob                 RiverJob
	UniqueSkippedAsDuplicate bool
}

func (q *Queries) JobInsertFastMany(ctx context.Context, db DBTX, arg *JobInsertFastManyParams) ([]*JobInsertFastManyRow, error) {
	rows, err := db.Query(ctx, jobInsertFastMany,
		arg.Args,
		arg.CreatedAt,
		arg.Kind,
		arg.MaxAttempts,
		arg.Metadata,
		arg.Priority,
		arg.Queue,
		arg.ScheduledAt,
		arg.State,
		arg.Tags,
		arg.UniqueKey,
		arg.UniqueStates,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*JobInsertFastManyRow
	for rows.Next() {
		var i JobInsertFastManyRow
		if err := rows.Scan(
			&i.RiverJob.ID,
			&i.RiverJob.Args,
			&i.RiverJob.Attempt,
			&i.RiverJob.AttemptedAt,
			&i.RiverJob.AttemptedBy,
			&i.RiverJob.CreatedAt,
			&i.RiverJob.Errors,
			&i.RiverJob.FinalizedAt,
			&i.RiverJob.Kind,
			&i.RiverJob.MaxAttempts,
			&i.RiverJob.Metadata,
			&i.RiverJob.Priority,
			&i.RiverJob.Queue,
			&i.RiverJob.State,
			&i.RiverJob.ScheduledAt,
			&i.RiverJob.Tags,
			&i.RiverJob.UniqueKey,
			&i.RiverJob.UniqueStates,
			&i.UniqueSkippedAsDuplicate,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobInsertFastManyNoReturning = `-- name: JobInsertFastManyNoReturning :execrows
INSERT INTO river_job(
    args,
    created_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) SELECT
    unnest($1::jsonb[]),
    unnest($2::timestamptz[]),
    unnest($3::text[]),
    unnest($4::smallint[]),
    unnest($5::jsonb[]),
    unnest($6::smallint[]),
    unnest($7::text[]),
    unnest($8::timestamptz[]),
    unnest($9::river_job_state[]),

    -- lib/pq really, REALLY does not play nicely with multi-dimensional arrays,
    -- so instead we pack each set of tags into a string, send them through,
    -- then unpack them here into an array to put in each row. This isn't
    -- necessary in the Pgx driver where copyfrom is used instead.
    string_to_array(unnest($10::text[]), ','),

    unnest($11::bytea[]),
    unnest($12::bit(8)[])

ON CONFLICT (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state)
DO NOTHING
`

type JobInsertFastManyNoReturningParams struct {
	Args         [][]byte
	CreatedAt    []time.Time
	Kind         []string
	MaxAttempts  []int16
	Metadata     [][]byte
	Priority     []int16
	Queue        []string
	ScheduledAt  []time.Time
	State        []RiverJobState
	Tags         []string
	UniqueKey    [][]byte
	UniqueStates []pgtype.Bits
}

func (q *Queries) JobInsertFastManyNoReturning(ctx context.Context, db DBTX, arg *JobInsertFastManyNoReturningParams) (int64, error) {
	result, err := db.Exec(ctx, jobInsertFastManyNoReturning,
		arg.Args,
		arg.CreatedAt,
		arg.Kind,
		arg.MaxAttempts,
		arg.Metadata,
		arg.Priority,
		arg.Queue,
		arg.ScheduledAt,
		arg.State,
		arg.Tags,
		arg.UniqueKey,
		arg.UniqueStates,
	)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected(), nil
}

const jobInsertFull = `-- name: JobInsertFull :one
INSERT INTO river_job(
    args,
    attempt,
    attempted_at,
    attempted_by,
    created_at,
    errors,
    finalized_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) VALUES (
    $1::jsonb,
    coalesce($2::smallint, 0),
    $3,
    coalesce($4::text[], '{}'),
    coalesce($5::timestamptz, now()),
    $6,
    $7,
    $8,
    $9::smallint,
    coalesce($10::jsonb, '{}'),
    $11,
    $12,
    coalesce($13::timestamptz, now()),
    $14,
    coalesce($15::varchar(255)[], '{}'),
    $16,
    $17
) RETURNING id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
`

type JobInsertFullParams struct {
	Args         []byte
	Attempt      int16
	AttemptedAt  *time.Time
	AttemptedBy  []string
	CreatedAt    *time.Time
	Errors       [][]byte
	FinalizedAt  *time.Time
	Kind         string
	MaxAttempts  int16
	Metadata     []byte
	Priority     int16
	Queue        string
	ScheduledAt  *time.Time
	State        RiverJobState
	Tags         []string
	UniqueKey    []byte
	UniqueStates pgtype.Bits
}

func (q *Queries) JobInsertFull(ctx context.Context, db DBTX, arg *JobInsertFullParams) (*RiverJob, error) {
	row := db.QueryRow(ctx, jobInsertFull,
		arg.Args,
		arg.Attempt,
		arg.AttemptedAt,
		arg.AttemptedBy,
		arg.CreatedAt,
		arg.Errors,
		arg.FinalizedAt,
		arg.Kind,
		arg.MaxAttempts,
		arg.Metadata,
		arg.Priority,
		arg.Queue,
		arg.ScheduledAt,
		arg.State,
		arg.Tags,
		arg.UniqueKey,
		arg.UniqueStates,
	)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		&i.AttemptedBy,
		&i.CreatedAt,
		&i.Errors,
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		&i.Tags,
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobList = `-- name: JobList :many
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */
ORDER BY /* TEMPLATE_BEGIN: order_by_clause */ id /* TEMPLATE_END */
LIMIT $1::int
`

func (q *Queries) JobList(ctx context.Context, db DBTX, max int32) ([]*RiverJob, error) {
	rows, err := db.Query(ctx, jobList, max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			&i.AttemptedBy,
			&i.CreatedAt,
			&i.Errors,
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			&i.Tags,
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobRescueMany = `-- name: JobRescueMany :exec
UPDATE river_job
SET
    errors = array_append(errors, updated_job.error),
    finalized_at = updated_job.finalized_at,
    scheduled_at = updated_job.scheduled_at,
    state = updated_job.state
FROM (
    SELECT
        unnest($1::bigint[]) AS id,
        unnest($2::jsonb[]) AS error,
        nullif(unnest($3::timestamptz[]), '0001-01-01 00:00:00 +0000') AS finalized_at,
        unnest($4::timestamptz[]) AS scheduled_at,
        unnest($5::text[])::river_job_state AS state
) AS updated_job
WHERE river_job.id = updated_job.id
`

type JobRescueManyParams struct {
	ID          []int64
	Error       [][]byte
	FinalizedAt []time.Time
	ScheduledAt []time.Time
	State       []string
}

// Run by the rescuer to queue for retry or discard depending on job state.
func (q *Queries) JobRescueMany(ctx context.Context, db DBTX, arg *JobRescueManyParams) error {
	_, err := db.Exec(ctx, jobRescueMany,
		arg.ID,
		arg.Error,
		arg.FinalizedAt,
		arg.ScheduledAt,
		arg.State,
	)
	return err
}

const jobRetry = `-- name: JobRetry :one
WITH job_to_update AS (
    SELECT id
    FROM river_job
    WHERE river_job.id = $1
    FOR UPDATE
),
updated_job AS (
    UPDATE river_job
    SET
        state = 'available',
        scheduled_at = now(),
        max_attempts = CASE WHEN attempt = max_attempts THEN max_attempts + 1 ELSE max_attempts END,
        finalized_at = NULL
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
        -- Do not touch running jobs:
        AND river_job.state != 'running'
        -- If the job is already available with a prior scheduled_at, leave it alone.
        AND NOT (river_job.state = 'available' AND river_job.scheduled_at < now())
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id = $1::bigint
    AND id NOT IN (SELECT id FROM updated_job)
UNION
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM updated_job
`

func (q *Queries) JobRetry(ctx context.Context, db DBTX, id int64) (*RiverJob, error) {
	row := db.QueryRow(ctx, jobRetry, id)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		&i.AttemptedBy,
		&i.CreatedAt,
		&i.Errors,
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		&i.Tags,
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

const jobSchedule = `-- name: JobSchedule :many
WITH jobs_to_schedule AS (
    SELECT
        id,
        unique_key,
        unique_states,
        priority,
        scheduled_at
    FROM river_job
    WHERE
        state IN ('retryable', 'scheduled')
        AND queue IS NOT NULL
        AND priority >= 0
        AND scheduled_at <= $1::timestamptz
    ORDER BY
        priority,
        scheduled_at,
        id
    LIMIT $2::bigint
    FOR UPDATE
),
jobs_with_rownum AS (
    SELECT
        id, unique_key, unique_states, priority, scheduled_at,
        CASE
            WHEN unique_key IS NOT NULL AND unique_states IS NOT NULL THEN
                ROW_NUMBER() OVER (
                    PARTITION BY unique_key
                    ORDER BY priority, scheduled_at, id
                )
            ELSE NULL
        END AS row_num
    FROM jobs_to_schedule
),
unique_conflicts AS (
    SELECT river_job.unique_key
    FROM river_job
    JOIN jobs_with_rownum
        ON river_job.unique_key = jobs_with_rownum.unique_key
        AND river_job.id != jobs_with_rownum.id
    WHERE
        river_job.unique_key IS NOT NULL
        AND river_job.unique_states IS NOT NULL
        AND river_job_state_in_bitmask(river_job.unique_states, river_job.state)
),
job_updates AS (
    SELECT
        job.id,
        job.unique_key,
        job.unique_states,
        CASE
            WHEN job.row_num IS NULL THEN 'available'::river_job_state
            WHEN uc.unique_key IS NOT NULL THEN 'discarded'::river_job_state
            WHEN job.row_num = 1 THEN 'available'::river_job_state
            ELSE 'discarded'::river_job_state
        END AS new_state,
        (job.row_num IS NOT NULL AND (uc.unique_key IS NOT NULL OR job.row_num > 1)) AS finalized_at_do_update,
        (job.row_num IS NOT NULL AND (uc.unique_key IS NOT NULL OR job.row_num > 1)) AS metadata_do_update
    FROM jobs_with_rownum job
    LEFT JOIN unique_conflicts uc ON job.unique_key = uc.unique_key
),
updated_jobs AS (
    UPDATE river_job
    SET
        state        = job_updates.new_state,
        finalized_at = CASE WHEN job_updates.finalized_at_do_update THEN $1::timestamptz
                            ELSE river_job.finalized_at END,
        metadata     = CASE WHEN job_updates.metadata_do_update THEN river_job.metadata || '{"unique_key_conflict": "scheduler_discarded"}'::jsonb
                            ELSE river_job.metadata END
    FROM job_updates
    WHERE river_job.id = job_updates.id
    RETURNING
        river_job.id,
        job_updates.new_state = 'discarded'::river_job_state AS conflict_discarded
)
SELECT
    river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states,
    updated_jobs.conflict_discarded
FROM river_job
JOIN updated_jobs ON river_job.id = updated_jobs.id
`

type JobScheduleParams struct {
	Now time.Time
	Max int64
}

type JobScheduleRow struct {
	RiverJob          RiverJob
	ConflictDiscarded bool
}

func (q *Queries) JobSchedule(ctx context.Context, db DBTX, arg *JobScheduleParams) ([]*JobScheduleRow, error) {
	rows, err := db.Query(ctx, jobSchedule, arg.Now, arg.Max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*JobScheduleRow
	for rows.Next() {
		var i JobScheduleRow
		if err := rows.Scan(
			&i.RiverJob.ID,
			&i.RiverJob.Args,
			&i.RiverJob.Attempt,
			&i.RiverJob.AttemptedAt,
			&i.RiverJob.AttemptedBy,
			&i.RiverJob.CreatedAt,
			&i.RiverJob.Errors,
			&i.RiverJob.FinalizedAt,
			&i.RiverJob.Kind,
			&i.RiverJob.MaxAttempts,
			&i.RiverJob.Metadata,
			&i.RiverJob.Priority,
			&i.RiverJob.Queue,
			&i.RiverJob.State,
			&i.RiverJob.ScheduledAt,
			&i.RiverJob.Tags,
			&i.RiverJob.UniqueKey,
			&i.RiverJob.UniqueStates,
			&i.ConflictDiscarded,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobSetStateIfRunningMany = `-- name: JobSetStateIfRunningMany :many
WITH job_input AS (
    SELECT
        unnest($1::bigint[]) AS id,
        unnest($2::boolean[]) AS attempt_do_update,
        unnest($3::int[]) AS attempt,
        unnest($4::boolean[]) AS errors_do_update,
        unnest($5::jsonb[]) AS errors,
        unnest($6::boolean[]) AS finalized_at_do_update,
        unnest($7::timestamptz[]) AS finalized_at,
        unnest($8::boolean[]) AS metadata_do_merge,
        unnest($9::jsonb[]) AS metadata_updates,
        unnest($10::boolean[]) AS scheduled_at_do_update,
        unnest($11::timestamptz[]) AS scheduled_at,
        -- To avoid requiring pgx users to register the OID of the river_job_state[]
        -- type, we cast the array to text[] and then to river_job_state.
        unnest($12::text[])::river_job_state AS state
),
job_to_update AS (
    SELECT
        river_job.id,
        job_input.attempt,
        job_input.attempt_do_update,
        job_input.errors,
        job_input.errors_do_update,
        job_input.finalized_at,
        job_input.finalized_at_do_update,
        job_input.metadata_do_merge,
        job_input.metadata_updates,
        job_input.scheduled_at,
        job_input.scheduled_at_do_update,
        (job_input.state IN ('retryable', 'scheduled') AND river_job.metadata ? 'cancel_attempted_at') AS should_cancel,
        job_input.state
    FROM river_job
    JOIN job_input ON river_job.id = job_input.id
    WHERE river_job.state = 'running' OR job_input.metadata_do_merge
    FOR UPDATE
),
updated_running AS (
    UPDATE river_job
    SET
        attempt      = CASE WHEN NOT job_to_update.should_cancel AND job_to_update.attempt_do_update THEN job_to_update.attempt
                            ELSE river_job.attempt END,
        errors       = CASE WHEN job_to_update.errors_do_update THEN array_append(river_job.errors, job_to_update.errors)
                            ELSE river_job.errors END,
        finalized_at = CASE WHEN job_to_update.should_cancel THEN now()
                            WHEN job_to_update.finalized_at_do_update THEN job_to_update.finalized_at
                            ELSE river_job.finalized_at END,
        metadata     = CASE WHEN job_to_update.metadata_do_merge
                            THEN river_job.metadata || job_to_update.metadata_updates
                            ELSE river_job.metadata END,
        scheduled_at = CASE WHEN NOT job_to_update.should_cancel AND job_to_update.scheduled_at_do_update THEN job_to_update.scheduled_at
                            ELSE river_job.scheduled_at END,
        state        = CASE WHEN job_to_update.should_cancel THEN 'cancelled'::river_job_state
                            ELSE job_to_update.state END
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
      AND river_job.state = 'running'
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
),
updated_metadata_only AS (
    UPDATE river_job
    SET metadata = river_job.metadata || job_to_update.metadata_updates
    FROM job_to_update
    WHERE river_job.id = job_to_update.id
        AND river_job.id NOT IN (SELECT id FROM updated_running)
        AND river_job.state != 'running'
        AND job_to_update.metadata_do_merge
    RETURNING river_job.id, river_job.args, river_job.attempt, river_job.attempted_at, river_job.attempted_by, river_job.created_at, river_job.errors, river_job.finalized_at, river_job.kind, river_job.max_attempts, river_job.metadata, river_job.priority, river_job.queue, river_job.state, river_job.scheduled_at, river_job.tags, river_job.unique_key, river_job.unique_states
)
SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
FROM river_job
WHERE id IN (SELECT id FROM job_input)
    AND id NOT IN (SELECT id FROM updated_metadata_only)
    AND id NOT IN (SELECT id FROM updated_running)
UNION SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states FROM updated_metadata_only
UNION SELECT id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states FROM updated_running
`

type JobSetStateIfRunningManyParams struct {
	IDs                 []int64
	AttemptDoUpdate     []bool
	Attempt             []int32
	ErrorsDoUpdate      []bool
	Errors              [][]byte
	FinalizedAtDoUpdate []bool
	FinalizedAt         []time.Time
	MetadataDoMerge     []bool
	MetadataUpdates     [][]byte
	ScheduledAtDoUpdate []bool
	ScheduledAt         []time.Time
	State               []string
}

func (q *Queries) JobSetStateIfRunningMany(ctx context.Context, db DBTX, arg *JobSetStateIfRunningManyParams) ([]*RiverJob, error) {
	rows, err := db.Query(ctx, jobSetStateIfRunningMany,
		arg.IDs,
		arg.AttemptDoUpdate,
		arg.Attempt,
		arg.ErrorsDoUpdate,
		arg.Errors,
		arg.FinalizedAtDoUpdate,
		arg.FinalizedAt,
		arg.MetadataDoMerge,
		arg.MetadataUpdates,
		arg.ScheduledAtDoUpdate,
		arg.ScheduledAt,
		arg.State,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverJob
	for rows.Next() {
		var i RiverJob
		if err := rows.Scan(
			&i.ID,
			&i.Args,
			&i.Attempt,
			&i.AttemptedAt,
			&i.AttemptedBy,
			&i.CreatedAt,
			&i.Errors,
			&i.FinalizedAt,
			&i.Kind,
			&i.MaxAttempts,
			&i.Metadata,
			&i.Priority,
			&i.Queue,
			&i.State,
			&i.ScheduledAt,
			&i.Tags,
			&i.UniqueKey,
			&i.UniqueStates,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const jobUpdate = `-- name: JobUpdate :one
UPDATE river_job
SET
    attempt = CASE WHEN $1::boolean THEN $2 ELSE attempt END,
    attempted_at = CASE WHEN $3::boolean THEN $4 ELSE attempted_at END,
    attempted_by = CASE WHEN $5::boolean THEN $6 ELSE attempted_by END,
    errors = CASE WHEN $7::boolean THEN $8::jsonb[] ELSE errors END,
    finalized_at = CASE WHEN $9::boolean THEN $10 ELSE finalized_at END,
    state = CASE WHEN $11::boolean THEN $12 ELSE state END
WHERE id = $13
RETURNING id, args, attempt, attempted_at, attempted_by, created_at, errors, finalized_at, kind, max_attempts, metadata, priority, queue, state, scheduled_at, tags, unique_key, unique_states
`

type JobUpdateParams struct {
	AttemptDoUpdate     bool
	Attempt             int16
	AttemptedAtDoUpdate bool
	AttemptedAt         *time.Time
	AttemptedByDoUpdate bool
	AttemptedBy         []string
	ErrorsDoUpdate      bool
	Errors              [][]byte
	FinalizedAtDoUpdate bool
	FinalizedAt         *time.Time
	StateDoUpdate       bool
	State               RiverJobState
	ID                  int64
}

// A generalized update for any property on a job. This brings in a large number
// of parameters and therefore may be more suitable for testing than production.
func (q *Queries) JobUpdate(ctx context.Context, db DBTX, arg *JobUpdateParams) (*RiverJob, error) {
	row := db.QueryRow(ctx, jobUpdate,
		arg.AttemptDoUpdate,
		arg.Attempt,
		arg.AttemptedAtDoUpdate,
		arg.AttemptedAt,
		arg.AttemptedByDoUpdate,
		arg.AttemptedBy,
		arg.ErrorsDoUpdate,
		arg.Errors,
		arg.FinalizedAtDoUpdate,
		arg.FinalizedAt,
		arg.StateDoUpdate,
		arg.State,
		arg.ID,
	)
	var i RiverJob
	err := row.Scan(
		&i.ID,
		&i.Args,
		&i.Attempt,
		&i.AttemptedAt,
		&i.AttemptedBy,
		&i.CreatedAt,
		&i.Errors,
		&i.FinalizedAt,
		&i.Kind,
		&i.MaxAttempts,
		&i.Metadata,
		&i.Priority,
		&i.Queue,
		&i.State,
		&i.ScheduledAt,
		&i.Tags,
		&i.UniqueKey,
		&i.UniqueStates,
	)
	return &i, err
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_job_copyfrom.sql`:

```sql
-- name: JobInsertFastManyCopyFrom :copyfrom
INSERT INTO river_job(
    args,
    created_at,
    kind,
    max_attempts,
    metadata,
    priority,
    queue,
    scheduled_at,
    state,
    tags,
    unique_key,
    unique_states
) VALUES (
    @args,
    @created_at,
    @kind,
    @max_attempts,
    @metadata,
    @priority,
    @queue,
    @scheduled_at,
    @state,
    @tags,
    @unique_key,
    @unique_states
);

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_job_copyfrom.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_job_copyfrom.sql

package dbsqlc

import (
	"time"

	"github.com/jackc/pgx/v5/pgtype"
)

type JobInsertFastManyCopyFromParams struct {
	Args         []byte
	CreatedAt    time.Time
	Kind         string
	MaxAttempts  int16
	Metadata     []byte
	Priority     int16
	Queue        string
	ScheduledAt  time.Time
	State        RiverJobState
	Tags         []string
	UniqueKey    []byte
	UniqueStates pgtype.Bits
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_leader.sql`:

```sql
CREATE UNLOGGED TABLE river_leader(
    elected_at timestamptz NOT NULL,
    expires_at timestamptz NOT NULL,
    leader_id text NOT NULL,
    name text PRIMARY KEY DEFAULT 'default',
    CONSTRAINT name_length CHECK (name = 'default'),
    CONSTRAINT leader_id_length CHECK (char_length(leader_id) > 0 AND char_length(leader_id) < 128)
);

-- name: LeaderAttemptElect :execrows
INSERT INTO river_leader(leader_id, elected_at, expires_at)
    VALUES (@leader_id, now(), now() + @ttl::interval)
ON CONFLICT (name)
    DO NOTHING;

-- name: LeaderAttemptReelect :execrows
INSERT INTO river_leader(leader_id, elected_at, expires_at)
    VALUES (@leader_id, now(), now() + @ttl::interval)
ON CONFLICT (name)
    DO UPDATE SET
        expires_at = now() + @ttl
    WHERE
        river_leader.leader_id = @leader_id;

-- name: LeaderDeleteExpired :execrows
DELETE FROM river_leader
WHERE expires_at < now();

-- name: LeaderGetElectedLeader :one
SELECT *
FROM river_leader;

-- name: LeaderInsert :one
INSERT INTO river_leader(
    elected_at,
    expires_at,
    leader_id
) VALUES (
    coalesce(sqlc.narg('elected_at')::timestamptz, now()),
    coalesce(sqlc.narg('expires_at')::timestamptz, now() + @ttl::interval),
    @leader_id
) RETURNING *;

-- name: LeaderResign :execrows
WITH currently_held_leaders AS (
  SELECT *
  FROM river_leader
  WHERE leader_id = @leader_id::text
  FOR UPDATE
),
notified_resignations AS (
    SELECT pg_notify(
        concat(current_schema(), '.', @leadership_topic::text),
        json_build_object('leader_id', leader_id, 'action', 'resigned')::text
    )
    FROM currently_held_leaders
)
DELETE FROM river_leader USING notified_resignations;
```

`riverdriver/riverpgxv5/internal/dbsqlc/river_leader.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_leader.sql

package dbsqlc

import (
	"context"
	"time"
)

const leaderAttemptElect = `-- name: LeaderAttemptElect :execrows
INSERT INTO river_leader(leader_id, elected_at, expires_at)
    VALUES ($1, now(), now() + $2::interval)
ON CONFLICT (name)
    DO NOTHING
`

type LeaderAttemptElectParams struct {
	LeaderID string
	TTL      time.Duration
}

func (q *Queries) LeaderAttemptElect(ctx context.Context, db DBTX, arg *LeaderAttemptElectParams) (int64, error) {
	result, err := db.Exec(ctx, leaderAttemptElect, arg.LeaderID, arg.TTL)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected(), nil
}

const leaderAttemptReelect = `-- name: LeaderAttemptReelect :execrows
INSERT INTO river_leader(leader_id, elected_at, expires_at)
    VALUES ($1, now(), now() + $2::interval)
ON CONFLICT (name)
    DO UPDATE SET
        expires_at = now() + $2
    WHERE
        river_leader.leader_id = $1
`

type LeaderAttemptReelectParams struct {
	LeaderID string
	TTL      time.Duration
}

func (q *Queries) LeaderAttemptReelect(ctx context.Context, db DBTX, arg *LeaderAttemptReelectParams) (int64, error) {
	result, err := db.Exec(ctx, leaderAttemptReelect, arg.LeaderID, arg.TTL)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected(), nil
}

const leaderDeleteExpired = `-- name: LeaderDeleteExpired :execrows
DELETE FROM river_leader
WHERE expires_at < now()
`

func (q *Queries) LeaderDeleteExpired(ctx context.Context, db DBTX) (int64, error) {
	result, err := db.Exec(ctx, leaderDeleteExpired)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected(), nil
}

const leaderGetElectedLeader = `-- name: LeaderGetElectedLeader :one
SELECT elected_at, expires_at, leader_id, name
FROM river_leader
`

func (q *Queries) LeaderGetElectedLeader(ctx context.Context, db DBTX) (*RiverLeader, error) {
	row := db.QueryRow(ctx, leaderGetElectedLeader)
	var i RiverLeader
	err := row.Scan(
		&i.ElectedAt,
		&i.ExpiresAt,
		&i.LeaderID,
		&i.Name,
	)
	return &i, err
}

const leaderInsert = `-- name: LeaderInsert :one
INSERT INTO river_leader(
    elected_at,
    expires_at,
    leader_id
) VALUES (
    coalesce($1::timestamptz, now()),
    coalesce($2::timestamptz, now() + $3::interval),
    $4
) RETURNING elected_at, expires_at, leader_id, name
`

type LeaderInsertParams struct {
	ElectedAt *time.Time
	ExpiresAt *time.Time
	TTL       time.Duration
	LeaderID  string
}

func (q *Queries) LeaderInsert(ctx context.Context, db DBTX, arg *LeaderInsertParams) (*RiverLeader, error) {
	row := db.QueryRow(ctx, leaderInsert,
		arg.ElectedAt,
		arg.ExpiresAt,
		arg.TTL,
		arg.LeaderID,
	)
	var i RiverLeader
	err := row.Scan(
		&i.ElectedAt,
		&i.ExpiresAt,
		&i.LeaderID,
		&i.Name,
	)
	return &i, err
}

const leaderResign = `-- name: LeaderResign :execrows
WITH currently_held_leaders AS (
  SELECT elected_at, expires_at, leader_id, name
  FROM river_leader
  WHERE leader_id = $1::text
  FOR UPDATE
),
notified_resignations AS (
    SELECT pg_notify(
        concat(current_schema(), '.', $2::text),
        json_build_object('leader_id', leader_id, 'action', 'resigned')::text
    )
    FROM currently_held_leaders
)
DELETE FROM river_leader USING notified_resignations
`

type LeaderResignParams struct {
	LeaderID        string
	LeadershipTopic string
}

func (q *Queries) LeaderResign(ctx context.Context, db DBTX, arg *LeaderResignParams) (int64, error) {
	result, err := db.Exec(ctx, leaderResign, arg.LeaderID, arg.LeadershipTopic)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected(), nil
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_migration.sql`:

```sql
CREATE TABLE river_migration(
    line TEXT NOT NULL,
    version bigint NOT NULL,
    created_at timestamptz NOT NULL DEFAULT NOW(),
    CONSTRAINT line_length CHECK (char_length(line) > 0 AND char_length(line) < 128),
    CONSTRAINT version_gte_1 CHECK (version >= 1),
    PRIMARY KEY (line, version)
);

-- name: RiverMigrationDeleteAssumingMainMany :many
DELETE FROM river_migration
WHERE version = any(@version::bigint[])
RETURNING
    created_at,
    version;

-- name: RiverMigrationDeleteByLineAndVersionMany :many
DELETE FROM river_migration
WHERE line = @line
    AND version = any(@version::bigint[])
RETURNING *;

-- This is a compatibility query for getting existing migrations before the
-- `line` column was added to the table in version 005. We need to make sure to
-- only select non-line properties so the query doesn't error on older schemas.
-- (Even if we use `SELECT *` below, sqlc materializes it to a list of column
-- names in the generated query.)
--
-- name: RiverMigrationGetAllAssumingMain :many
SELECT
    created_at,
    version
FROM river_migration
ORDER BY version;

-- name: RiverMigrationGetByLine :many
SELECT *
FROM river_migration
WHERE line = @line
ORDER BY version;

-- name: RiverMigrationInsert :one
INSERT INTO river_migration (
    line,
    version
) VALUES (
    @line,
    @version
) RETURNING *;

-- name: RiverMigrationInsertMany :many
INSERT INTO river_migration (
    line,
    version
)
SELECT
    @line,
    unnest(@version::bigint[])
RETURNING *;

-- name: RiverMigrationInsertManyAssumingMain :many
INSERT INTO river_migration (
    version
)
SELECT
    unnest(@version::bigint[])
RETURNING
    created_at,
    version;

-- name: ColumnExists :one
SELECT EXISTS (
    SELECT column_name
    FROM information_schema.columns 
    WHERE table_name = @table_name::text
        AND table_schema = CURRENT_SCHEMA
        AND column_name = @column_name::text
);

-- name: TableExists :one
SELECT CASE WHEN to_regclass(@table_name) IS NULL THEN false
            ELSE true END;
```

`riverdriver/riverpgxv5/internal/dbsqlc/river_migration.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_migration.sql

package dbsqlc

import (
	"context"
	"time"
)

const columnExists = `-- name: ColumnExists :one
SELECT EXISTS (
    SELECT column_name
    FROM information_schema.columns 
    WHERE table_name = $1::text
        AND table_schema = CURRENT_SCHEMA
        AND column_name = $2::text
)
`

type ColumnExistsParams struct {
	TableName  string
	ColumnName string
}

func (q *Queries) ColumnExists(ctx context.Context, db DBTX, arg *ColumnExistsParams) (bool, error) {
	row := db.QueryRow(ctx, columnExists, arg.TableName, arg.ColumnName)
	var exists bool
	err := row.Scan(&exists)
	return exists, err
}

const riverMigrationDeleteAssumingMainMany = `-- name: RiverMigrationDeleteAssumingMainMany :many
DELETE FROM river_migration
WHERE version = any($1::bigint[])
RETURNING
    created_at,
    version
`

type RiverMigrationDeleteAssumingMainManyRow struct {
	CreatedAt time.Time
	Version   int64
}

func (q *Queries) RiverMigrationDeleteAssumingMainMany(ctx context.Context, db DBTX, version []int64) ([]*RiverMigrationDeleteAssumingMainManyRow, error) {
	rows, err := db.Query(ctx, riverMigrationDeleteAssumingMainMany, version)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigrationDeleteAssumingMainManyRow
	for rows.Next() {
		var i RiverMigrationDeleteAssumingMainManyRow
		if err := rows.Scan(&i.CreatedAt, &i.Version); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationDeleteByLineAndVersionMany = `-- name: RiverMigrationDeleteByLineAndVersionMany :many
DELETE FROM river_migration
WHERE line = $1
    AND version = any($2::bigint[])
RETURNING line, version, created_at
`

type RiverMigrationDeleteByLineAndVersionManyParams struct {
	Line    string
	Version []int64
}

func (q *Queries) RiverMigrationDeleteByLineAndVersionMany(ctx context.Context, db DBTX, arg *RiverMigrationDeleteByLineAndVersionManyParams) ([]*RiverMigration, error) {
	rows, err := db.Query(ctx, riverMigrationDeleteByLineAndVersionMany, arg.Line, arg.Version)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigration
	for rows.Next() {
		var i RiverMigration
		if err := rows.Scan(&i.Line, &i.Version, &i.CreatedAt); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationGetAllAssumingMain = `-- name: RiverMigrationGetAllAssumingMain :many
SELECT
    created_at,
    version
FROM river_migration
ORDER BY version
`

type RiverMigrationGetAllAssumingMainRow struct {
	CreatedAt time.Time
	Version   int64
}

// This is a compatibility query for getting existing migrations before the
// `line` column was added to the table in version 005. We need to make sure to
// only select non-line properties so the query doesn't error on older schemas.
// (Even if we use `SELECT *` below, sqlc materializes it to a list of column
// names in the generated query.)
func (q *Queries) RiverMigrationGetAllAssumingMain(ctx context.Context, db DBTX) ([]*RiverMigrationGetAllAssumingMainRow, error) {
	rows, err := db.Query(ctx, riverMigrationGetAllAssumingMain)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigrationGetAllAssumingMainRow
	for rows.Next() {
		var i RiverMigrationGetAllAssumingMainRow
		if err := rows.Scan(&i.CreatedAt, &i.Version); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationGetByLine = `-- name: RiverMigrationGetByLine :many
SELECT line, version, created_at
FROM river_migration
WHERE line = $1
ORDER BY version
`

func (q *Queries) RiverMigrationGetByLine(ctx context.Context, db DBTX, line string) ([]*RiverMigration, error) {
	rows, err := db.Query(ctx, riverMigrationGetByLine, line)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigration
	for rows.Next() {
		var i RiverMigration
		if err := rows.Scan(&i.Line, &i.Version, &i.CreatedAt); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationInsert = `-- name: RiverMigrationInsert :one
INSERT INTO river_migration (
    line,
    version
) VALUES (
    $1,
    $2
) RETURNING line, version, created_at
`

type RiverMigrationInsertParams struct {
	Line    string
	Version int64
}

func (q *Queries) RiverMigrationInsert(ctx context.Context, db DBTX, arg *RiverMigrationInsertParams) (*RiverMigration, error) {
	row := db.QueryRow(ctx, riverMigrationInsert, arg.Line, arg.Version)
	var i RiverMigration
	err := row.Scan(&i.Line, &i.Version, &i.CreatedAt)
	return &i, err
}

const riverMigrationInsertMany = `-- name: RiverMigrationInsertMany :many
INSERT INTO river_migration (
    line,
    version
)
SELECT
    $1,
    unnest($2::bigint[])
RETURNING line, version, created_at
`

type RiverMigrationInsertManyParams struct {
	Line    string
	Version []int64
}

func (q *Queries) RiverMigrationInsertMany(ctx context.Context, db DBTX, arg *RiverMigrationInsertManyParams) ([]*RiverMigration, error) {
	rows, err := db.Query(ctx, riverMigrationInsertMany, arg.Line, arg.Version)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigration
	for rows.Next() {
		var i RiverMigration
		if err := rows.Scan(&i.Line, &i.Version, &i.CreatedAt); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const riverMigrationInsertManyAssumingMain = `-- name: RiverMigrationInsertManyAssumingMain :many
INSERT INTO river_migration (
    version
)
SELECT
    unnest($1::bigint[])
RETURNING
    created_at,
    version
`

type RiverMigrationInsertManyAssumingMainRow struct {
	CreatedAt time.Time
	Version   int64
}

func (q *Queries) RiverMigrationInsertManyAssumingMain(ctx context.Context, db DBTX, version []int64) ([]*RiverMigrationInsertManyAssumingMainRow, error) {
	rows, err := db.Query(ctx, riverMigrationInsertManyAssumingMain, version)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverMigrationInsertManyAssumingMainRow
	for rows.Next() {
		var i RiverMigrationInsertManyAssumingMainRow
		if err := rows.Scan(&i.CreatedAt, &i.Version); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const tableExists = `-- name: TableExists :one
SELECT CASE WHEN to_regclass($1) IS NULL THEN false
            ELSE true END
`

func (q *Queries) TableExists(ctx context.Context, db DBTX, tableName string) (bool, error) {
	row := db.QueryRow(ctx, tableExists, tableName)
	var column_1 bool
	err := row.Scan(&column_1)
	return column_1, err
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/river_queue.sql`:

```sql
CREATE TABLE river_queue(
  name text PRIMARY KEY NOT NULL,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  metadata jsonb NOT NULL DEFAULT '{}' ::jsonb,
  paused_at timestamptz,
  updated_at timestamptz NOT NULL
);

-- name: QueueCreateOrSetUpdatedAt :one
INSERT INTO river_queue(
    created_at,
    metadata,
    name,
    paused_at,
    updated_at
) VALUES (
    now(),
    coalesce(@metadata::jsonb, '{}'::jsonb),
    @name::text,
    coalesce(sqlc.narg('paused_at')::timestamptz, NULL),
    coalesce(sqlc.narg('updated_at')::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce(sqlc.narg('updated_at')::timestamptz, now())
RETURNING *;

-- name: QueueDeleteExpired :many
DELETE FROM river_queue
WHERE name IN (
    SELECT name
    FROM river_queue
    WHERE updated_at < @updated_at_horizon::timestamptz
    ORDER BY name ASC
    LIMIT @max::bigint
)
RETURNING *;

-- name: QueueGet :one
SELECT *
FROM river_queue
WHERE name = @name::text;

-- name: QueueList :many
SELECT *
FROM river_queue
ORDER BY name ASC
LIMIT @limit_count::integer;

-- name: QueuePause :execresult
WITH queue_to_update AS (
    SELECT name, paused_at
    FROM river_queue
    WHERE CASE WHEN @name::text = '*' THEN true ELSE name = @name END
    FOR UPDATE
),
updated_queue AS (
    UPDATE river_queue
    SET
        paused_at = now(),
        updated_at = now()
    FROM queue_to_update
    WHERE river_queue.name = queue_to_update.name
        AND river_queue.paused_at IS NULL
    RETURNING river_queue.*
)
SELECT *
FROM river_queue
WHERE name = @name
    AND name NOT IN (SELECT name FROM updated_queue)
UNION
SELECT *
FROM updated_queue;

-- name: QueueResume :execresult
WITH queue_to_update AS (
    SELECT name
    FROM river_queue
    WHERE CASE WHEN @name::text = '*' THEN true ELSE river_queue.name = @name::text END
    FOR UPDATE
),
updated_queue AS (
    UPDATE river_queue
    SET
        paused_at = NULL,
        updated_at = now()
    FROM queue_to_update
    WHERE river_queue.name = queue_to_update.name
    RETURNING river_queue.*
)
SELECT *
FROM river_queue
WHERE name = @name
    AND name NOT IN (SELECT name FROM updated_queue)
UNION
SELECT *
FROM updated_queue;
```

`riverdriver/riverpgxv5/internal/dbsqlc/river_queue.sql.go`:

```go
// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.28.0
// source: river_queue.sql

package dbsqlc

import (
	"context"
	"time"

	"github.com/jackc/pgx/v5/pgconn"
)

const queueCreateOrSetUpdatedAt = `-- name: QueueCreateOrSetUpdatedAt :one
INSERT INTO river_queue(
    created_at,
    metadata,
    name,
    paused_at,
    updated_at
) VALUES (
    now(),
    coalesce($1::jsonb, '{}'::jsonb),
    $2::text,
    coalesce($3::timestamptz, NULL),
    coalesce($4::timestamptz, now())
) ON CONFLICT (name) DO UPDATE
SET
    updated_at = coalesce($4::timestamptz, now())
RETURNING name, created_at, metadata, paused_at, updated_at
`

type QueueCreateOrSetUpdatedAtParams struct {
	Metadata  []byte
	Name      string
	PausedAt  *time.Time
	UpdatedAt *time.Time
}

func (q *Queries) QueueCreateOrSetUpdatedAt(ctx context.Context, db DBTX, arg *QueueCreateOrSetUpdatedAtParams) (*RiverQueue, error) {
	row := db.QueryRow(ctx, queueCreateOrSetUpdatedAt,
		arg.Metadata,
		arg.Name,
		arg.PausedAt,
		arg.UpdatedAt,
	)
	var i RiverQueue
	err := row.Scan(
		&i.Name,
		&i.CreatedAt,
		&i.Metadata,
		&i.PausedAt,
		&i.UpdatedAt,
	)
	return &i, err
}

const queueDeleteExpired = `-- name: QueueDeleteExpired :many
DELETE FROM river_queue
WHERE name IN (
    SELECT name
    FROM river_queue
    WHERE updated_at < $1::timestamptz
    ORDER BY name ASC
    LIMIT $2::bigint
)
RETURNING name, created_at, metadata, paused_at, updated_at
`

type QueueDeleteExpiredParams struct {
	UpdatedAtHorizon time.Time
	Max              int64
}

func (q *Queries) QueueDeleteExpired(ctx context.Context, db DBTX, arg *QueueDeleteExpiredParams) ([]*RiverQueue, error) {
	rows, err := db.Query(ctx, queueDeleteExpired, arg.UpdatedAtHorizon, arg.Max)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverQueue
	for rows.Next() {
		var i RiverQueue
		if err := rows.Scan(
			&i.Name,
			&i.CreatedAt,
			&i.Metadata,
			&i.PausedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const queueGet = `-- name: QueueGet :one
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
WHERE name = $1::text
`

func (q *Queries) QueueGet(ctx context.Context, db DBTX, name string) (*RiverQueue, error) {
	row := db.QueryRow(ctx, queueGet, name)
	var i RiverQueue
	err := row.Scan(
		&i.Name,
		&i.CreatedAt,
		&i.Metadata,
		&i.PausedAt,
		&i.UpdatedAt,
	)
	return &i, err
}

const queueList = `-- name: QueueList :many
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
ORDER BY name ASC
LIMIT $1::integer
`

func (q *Queries) QueueList(ctx context.Context, db DBTX, limitCount int32) ([]*RiverQueue, error) {
	rows, err := db.Query(ctx, queueList, limitCount)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*RiverQueue
	for rows.Next() {
		var i RiverQueue
		if err := rows.Scan(
			&i.Name,
			&i.CreatedAt,
			&i.Metadata,
			&i.PausedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const queuePause = `-- name: QueuePause :execresult
WITH queue_to_update AS (
    SELECT name, paused_at
    FROM river_queue
    WHERE CASE WHEN $1::text = '*' THEN true ELSE name = $1 END
    FOR UPDATE
),
updated_queue AS (
    UPDATE river_queue
    SET
        paused_at = now(),
        updated_at = now()
    FROM queue_to_update
    WHERE river_queue.name = queue_to_update.name
        AND river_queue.paused_at IS NULL
    RETURNING river_queue.name, river_queue.created_at, river_queue.metadata, river_queue.paused_at, river_queue.updated_at
)
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
WHERE name = $1
    AND name NOT IN (SELECT name FROM updated_queue)
UNION
SELECT name, created_at, metadata, paused_at, updated_at
FROM updated_queue
`

func (q *Queries) QueuePause(ctx context.Context, db DBTX, name string) (pgconn.CommandTag, error) {
	return db.Exec(ctx, queuePause, name)
}

const queueResume = `-- name: QueueResume :execresult
WITH queue_to_update AS (
    SELECT name
    FROM river_queue
    WHERE CASE WHEN $1::text = '*' THEN true ELSE river_queue.name = $1::text END
    FOR UPDATE
),
updated_queue AS (
    UPDATE river_queue
    SET
        paused_at = NULL,
        updated_at = now()
    FROM queue_to_update
    WHERE river_queue.name = queue_to_update.name
    RETURNING river_queue.name, river_queue.created_at, river_queue.metadata, river_queue.paused_at, river_queue.updated_at
)
SELECT name, created_at, metadata, paused_at, updated_at
FROM river_queue
WHERE name = $1
    AND name NOT IN (SELECT name FROM updated_queue)
UNION
SELECT name, created_at, metadata, paused_at, updated_at
FROM updated_queue
`

func (q *Queries) QueueResume(ctx context.Context, db DBTX, name string) (pgconn.CommandTag, error) {
	return db.Exec(ctx, queueResume, name)
}

```

`riverdriver/riverpgxv5/internal/dbsqlc/sqlc.yaml`:

```yaml
version: "2"
sql:
  - engine: "postgresql"
    queries:
      - pg_misc.sql
      - river_client.sql
      - river_client_queue.sql
      - river_job.sql
      - river_job_copyfrom.sql
      - river_leader.sql
      - river_migration.sql
      - river_queue.sql
    schema:
      - pg_misc.sql
      - river_client.sql
      - river_client_queue.sql
      - river_job.sql
      - river_leader.sql
      - river_migration.sql
      - river_queue.sql
    gen:
      go:
        package: "dbsqlc"
        sql_package: "pgx/v5"
        out: "."
        emit_exact_table_names: true
        emit_methods_with_db_argument: true
        emit_params_struct_pointers: true
        emit_result_struct_pointers: true

        rename:
          ids: "IDs"
          ttl: "TTL"

        overrides:
          - db_type: "pg_catalog.interval"
            go_type: "time.Duration"

          - db_type: "timestamptz"
            go_type: "time.Time"

          - db_type: "timestamptz"
            go_type:
              type: "time.Time"
              pointer: true
            nullable: true

```

`riverdriver/riverpgxv5/migration/main/001_create_river_migration.down.sql`:

```sql
DROP TABLE river_migration;
```

`riverdriver/riverpgxv5/migration/main/001_create_river_migration.up.sql`:

```sql
CREATE TABLE river_migration(
  id bigserial PRIMARY KEY,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  version bigint NOT NULL,
  CONSTRAINT version CHECK (version >= 1)
);

CREATE UNIQUE INDEX ON river_migration USING btree(version);
```

`riverdriver/riverpgxv5/migration/main/002_initial_schema.down.sql`:

```sql
DROP TABLE river_job;
DROP FUNCTION river_job_notify;
DROP TYPE river_job_state;

DROP TABLE river_leader;
```

`riverdriver/riverpgxv5/migration/main/002_initial_schema.up.sql`:

```sql
CREATE TYPE river_job_state AS ENUM(
  'available',
  'cancelled',
  'completed',
  'discarded',
  'retryable',
  'running',
  'scheduled'
);

CREATE TABLE river_job(
  -- 8 bytes
  id bigserial PRIMARY KEY,

  -- 8 bytes (4 bytes + 2 bytes + 2 bytes)
  --
  -- `state` is kept near the top of the table for operator convenience -- when
  -- looking at jobs with `SELECT *` it'll appear first after ID. The other two
  -- fields aren't as important but are kept adjacent to `state` for alignment
  -- to get an 8-byte block.
  state river_job_state NOT NULL DEFAULT 'available',
  attempt smallint NOT NULL DEFAULT 0,
  max_attempts smallint NOT NULL,

  -- 8 bytes each (no alignment needed)
  attempted_at timestamptz,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  finalized_at timestamptz,
  scheduled_at timestamptz NOT NULL DEFAULT NOW(),

  -- 2 bytes (some wasted padding probably)
  priority smallint NOT NULL DEFAULT 1,

  -- types stored out-of-band
  args jsonb,
  attempted_by text[],
  errors jsonb[],
  kind text NOT NULL,
  metadata jsonb NOT NULL DEFAULT '{}',
  queue text NOT NULL DEFAULT 'default',
  tags varchar(255)[],

  CONSTRAINT finalized_or_finalized_at_null CHECK ((state IN ('cancelled', 'completed', 'discarded') AND finalized_at IS NOT NULL) OR finalized_at IS NULL),
  CONSTRAINT max_attempts_is_positive CHECK (max_attempts > 0),
  CONSTRAINT priority_in_range CHECK (priority >= 1 AND priority <= 4),
  CONSTRAINT queue_length CHECK (char_length(queue) > 0 AND char_length(queue) < 128),
  CONSTRAINT kind_length CHECK (char_length(kind) > 0 AND char_length(kind) < 128)
);

-- We may want to consider adding another property here after `kind` if it seems
-- like it'd be useful for something.
CREATE INDEX river_job_kind ON river_job USING btree(kind);

CREATE INDEX river_job_state_and_finalized_at_index ON river_job USING btree(state, finalized_at) WHERE finalized_at IS NOT NULL;

CREATE INDEX river_job_prioritized_fetching_index ON river_job USING btree(state, queue, priority, scheduled_at, id);

CREATE INDEX river_job_args_index ON river_job USING GIN(args);

CREATE INDEX river_job_metadata_index ON river_job USING GIN(metadata);

CREATE OR REPLACE FUNCTION river_job_notify()
  RETURNS TRIGGER
  AS $$
DECLARE
  payload json;
BEGIN
  IF NEW.state = 'available' THEN
    -- Notify will coalesce duplicate notifications within a transaction, so
    -- keep these payloads generalized:
    payload = json_build_object('queue', NEW.queue);
    PERFORM
      pg_notify('river_insert', payload::text);
  END IF;
  RETURN NULL;
END;
$$
LANGUAGE plpgsql;

CREATE TRIGGER river_notify
  AFTER INSERT ON river_job
  FOR EACH ROW
  EXECUTE PROCEDURE river_job_notify();

CREATE UNLOGGED TABLE river_leader(
  -- 8 bytes each (no alignment needed)
  elected_at timestamptz NOT NULL,
  expires_at timestamptz NOT NULL,

  -- types stored out-of-band
  leader_id text NOT NULL,
  name text PRIMARY KEY,

  CONSTRAINT name_length CHECK (char_length(name) > 0 AND char_length(name) < 128),
  CONSTRAINT leader_id_length CHECK (char_length(leader_id) > 0 AND char_length(leader_id) < 128)
);

```

`riverdriver/riverpgxv5/migration/main/003_river_job_tags_non_null.down.sql`:

```sql
ALTER TABLE river_job ALTER COLUMN tags DROP NOT NULL,
                      ALTER COLUMN tags DROP DEFAULT;

```

`riverdriver/riverpgxv5/migration/main/003_river_job_tags_non_null.up.sql`:

```sql
ALTER TABLE river_job ALTER COLUMN tags SET DEFAULT '{}';
UPDATE river_job SET tags = '{}' WHERE tags IS NULL;
ALTER TABLE river_job ALTER COLUMN tags SET NOT NULL;

```

`riverdriver/riverpgxv5/migration/main/004_pending_and_more.down.sql`:

```sql
ALTER TABLE river_job ALTER COLUMN args DROP NOT NULL;

ALTER TABLE river_job ALTER COLUMN metadata DROP NOT NULL;
ALTER TABLE river_job ALTER COLUMN metadata DROP DEFAULT;

-- It is not possible to safely remove 'pending' from the river_job_state enum,
-- so leave it in place.

ALTER TABLE river_job DROP CONSTRAINT finalized_or_finalized_at_null;
ALTER TABLE river_job ADD CONSTRAINT finalized_or_finalized_at_null CHECK (
  (state IN ('cancelled', 'completed', 'discarded') AND finalized_at IS NOT NULL) OR finalized_at IS NULL
);

CREATE OR REPLACE FUNCTION river_job_notify()
  RETURNS TRIGGER
  AS $$
DECLARE
  payload json;
BEGIN
  IF NEW.state = 'available' THEN
    -- Notify will coalesce duplicate notifications within a transaction, so
    -- keep these payloads generalized:
    payload = json_build_object('queue', NEW.queue);
    PERFORM
      pg_notify('river_insert', payload::text);
  END IF;
  RETURN NULL;
END;
$$
LANGUAGE plpgsql;

CREATE TRIGGER river_notify
  AFTER INSERT ON river_job
  FOR EACH ROW
  EXECUTE PROCEDURE river_job_notify();

DROP TABLE river_queue;

ALTER TABLE river_leader
    ALTER COLUMN name DROP DEFAULT,
    DROP CONSTRAINT name_length,
    ADD CONSTRAINT name_length CHECK (char_length(name) > 0 AND char_length(name) < 128);
```

`riverdriver/riverpgxv5/migration/main/004_pending_and_more.up.sql`:

```sql
-- The args column never had a NOT NULL constraint or default value at the
-- database level, though we tried to ensure one at the application level.
ALTER TABLE river_job ALTER COLUMN args SET DEFAULT '{}';
UPDATE river_job SET args = '{}' WHERE args IS NULL;
ALTER TABLE river_job ALTER COLUMN args SET NOT NULL;
ALTER TABLE river_job ALTER COLUMN args DROP DEFAULT;

-- The metadata column never had a NOT NULL constraint or default value at the
-- database level, though we tried to ensure one at the application level.
ALTER TABLE river_job ALTER COLUMN metadata SET DEFAULT '{}';
UPDATE river_job SET metadata = '{}' WHERE metadata IS NULL;
ALTER TABLE river_job ALTER COLUMN metadata SET NOT NULL;

-- The 'pending' job state will be used for upcoming functionality:
ALTER TYPE river_job_state ADD VALUE IF NOT EXISTS 'pending' AFTER 'discarded';

ALTER TABLE river_job DROP CONSTRAINT finalized_or_finalized_at_null;
ALTER TABLE river_job ADD CONSTRAINT finalized_or_finalized_at_null CHECK (
    (finalized_at IS NULL AND state NOT IN ('cancelled', 'completed', 'discarded')) OR
    (finalized_at IS NOT NULL AND state IN ('cancelled', 'completed', 'discarded'))
);

DROP TRIGGER river_notify ON river_job;
DROP FUNCTION river_job_notify;

CREATE TABLE river_queue(
  name text PRIMARY KEY NOT NULL,
  created_at timestamptz NOT NULL DEFAULT NOW(),
  metadata jsonb NOT NULL DEFAULT '{}' ::jsonb,
  paused_at timestamptz,
  updated_at timestamptz NOT NULL
);

ALTER TABLE river_leader
    ALTER COLUMN name SET DEFAULT 'default',
    DROP CONSTRAINT name_length,
    ADD CONSTRAINT name_length CHECK (name = 'default');
```

`riverdriver/riverpgxv5/migration/main/005_migration_unique_client.down.sql`:

```sql
--
-- Revert to migration table based only on `(version)`.
--
-- If any non-main migrations are present, 005 is considered irreversible.
--

DO
$body$
BEGIN
    -- Tolerate users who may be using their own migration system rather than
    -- River's. If they are, they will have skipped version 001 containing
    -- `CREATE TABLE river_migration`, so this table won't exist.
    IF (SELECT to_regclass('river_migration') IS NOT NULL) THEN
        IF EXISTS (
            SELECT *
            FROM river_migration
            WHERE line <> 'main'
        ) THEN
            RAISE EXCEPTION 'Found non-main migration lines in the database; version 005 migration is irreversible because it would result in loss of migration information.';
        END IF;

        ALTER TABLE river_migration
            RENAME TO river_migration_old;

        CREATE TABLE river_migration(
            id bigserial PRIMARY KEY,
            created_at timestamptz NOT NULL DEFAULT NOW(),
            version bigint NOT NULL,
            CONSTRAINT version CHECK (version >= 1)
        );

        CREATE UNIQUE INDEX ON river_migration USING btree(version);

        INSERT INTO river_migration
            (created_at, version)
        SELECT created_at, version
        FROM river_migration_old;

        DROP TABLE river_migration_old;
    END IF;
END;
$body$
LANGUAGE 'plpgsql'; 

--
-- Drop `river_job.unique_key`.
--

ALTER TABLE river_job
    DROP COLUMN unique_key;

--
-- Drop `river_client` and derivative.
--

DROP TABLE river_client_queue;
DROP TABLE river_client;

```

`riverdriver/riverpgxv5/migration/main/005_migration_unique_client.up.sql`:

```sql
--
-- Rebuild the migration table so it's based on `(line, version)`.
--

DO
$body$
BEGIN
    -- Tolerate users who may be using their own migration system rather than
    -- River's. If they are, they will have skipped version 001 containing
    -- `CREATE TABLE river_migration`, so this table won't exist.
    IF (SELECT to_regclass('river_migration') IS NOT NULL) THEN
        ALTER TABLE river_migration
            RENAME TO river_migration_old;

        CREATE TABLE river_migration(
            line TEXT NOT NULL,
            version bigint NOT NULL,
            created_at timestamptz NOT NULL DEFAULT NOW(),
            CONSTRAINT line_length CHECK (char_length(line) > 0 AND char_length(line) < 128),
            CONSTRAINT version_gte_1 CHECK (version >= 1),
            PRIMARY KEY (line, version)
        );

        INSERT INTO river_migration
            (created_at, line, version)
        SELECT created_at, 'main', version
        FROM river_migration_old;

        DROP TABLE river_migration_old;
    END IF;
END;
$body$
LANGUAGE 'plpgsql'; 

--
-- Add `river_job.unique_key` and bring up an index on it.
--

-- These statements use `IF NOT EXISTS` to allow users with a `river_job` table
-- of non-trivial size to build the index `CONCURRENTLY` out of band of this
-- migration, then follow by completing the migration.
ALTER TABLE river_job
    ADD COLUMN IF NOT EXISTS unique_key bytea;

CREATE UNIQUE INDEX IF NOT EXISTS river_job_kind_unique_key_idx ON river_job (kind, unique_key) WHERE unique_key IS NOT NULL;

--
-- Create `river_client` and derivative.
--
-- This feature hasn't quite yet been implemented, but we're taking advantage of
-- the migration to add the schema early so that we can add it later without an
-- additional migration.
--

CREATE UNLOGGED TABLE river_client (
    id text PRIMARY KEY NOT NULL,
    created_at timestamptz NOT NULL DEFAULT now(),
    metadata jsonb NOT NULL DEFAULT '{}',
    paused_at timestamptz,
    updated_at timestamptz NOT NULL,
    CONSTRAINT name_length CHECK (char_length(id) > 0 AND char_length(id) < 128)
);

-- Differs from `river_queue` in that it tracks the queue state for a particular
-- active client.
CREATE UNLOGGED TABLE river_client_queue (
    river_client_id text NOT NULL REFERENCES river_client (id) ON DELETE CASCADE,
    name text NOT NULL,
    created_at timestamptz NOT NULL DEFAULT now(),
    max_workers bigint NOT NULL DEFAULT 0,
    metadata jsonb NOT NULL DEFAULT '{}',
    num_jobs_completed bigint NOT NULL DEFAULT 0,
    num_jobs_running bigint NOT NULL DEFAULT 0,
    updated_at timestamptz NOT NULL,
    PRIMARY KEY (river_client_id, name),
    CONSTRAINT name_length CHECK (char_length(name) > 0 AND char_length(name) < 128),
    CONSTRAINT num_jobs_completed_zero_or_positive CHECK (num_jobs_completed >= 0),
    CONSTRAINT num_jobs_running_zero_or_positive CHECK (num_jobs_running >= 0)
);
```

`riverdriver/riverpgxv5/migration/main/006_bulk_unique.down.sql`:

```sql

--
-- Drop `river_job.unique_states` and its index.
--

DROP INDEX river_job_unique_idx;

ALTER TABLE river_job
    DROP COLUMN unique_states;

CREATE UNIQUE INDEX IF NOT EXISTS river_job_kind_unique_key_idx ON river_job (kind, unique_key) WHERE unique_key IS NOT NULL;

--
-- Drop `river_job_state_in_bitmask` function.
--
DROP FUNCTION river_job_state_in_bitmask;

```

`riverdriver/riverpgxv5/migration/main/006_bulk_unique.up.sql`:

```sql

CREATE OR REPLACE FUNCTION river_job_state_in_bitmask(bitmask BIT(8), state river_job_state)
RETURNS boolean
LANGUAGE SQL
IMMUTABLE
AS $$
    SELECT CASE state
        WHEN 'available' THEN get_bit(bitmask, 7)
        WHEN 'cancelled' THEN get_bit(bitmask, 6)
        WHEN 'completed' THEN get_bit(bitmask, 5)
        WHEN 'discarded' THEN get_bit(bitmask, 4)
        WHEN 'pending' THEN get_bit(bitmask, 3)
        WHEN 'retryable' THEN get_bit(bitmask, 2)
        WHEN 'running' THEN get_bit(bitmask, 1)
        WHEN 'scheduled' THEN get_bit(bitmask, 0)
        ELSE 0
    END = 1;
$$;

--
-- Add `river_job.unique_states` and bring up an index on it.
--
-- This column may exist already if users manually created the column and index
-- as instructed in the changelog so the index could be created `CONCURRENTLY`.
--
ALTER TABLE river_job ADD COLUMN IF NOT EXISTS unique_states BIT(8);

-- This statement uses `IF NOT EXISTS` to allow users with a `river_job` table
-- of non-trivial size to build the index `CONCURRENTLY` out of band of this
-- migration, then follow by completing the migration.
CREATE UNIQUE INDEX IF NOT EXISTS river_job_unique_idx ON river_job (unique_key)
    WHERE unique_key IS NOT NULL
      AND unique_states IS NOT NULL
      AND river_job_state_in_bitmask(unique_states, state);

-- Remove the old unique index. Users who are actively using the unique jobs
-- feature and who wish to avoid deploy downtime may want od drop this in a
-- subsequent migration once all jobs using the old unique system have been
-- completed (i.e. no more rows with non-null unique_key and null
-- unique_states).
DROP INDEX river_job_kind_unique_key_idx;

```

`riverdriver/riverpgxv5/river_pgx_v5_driver.go`:

```go
// Package riverpgxv5 provides a River driver implementation for Pgx v5.
//
// This is currently the only supported driver for River and will therefore be
// used by all projects using River, but the code is organized this way so that
// other database packages can be supported in future River versions.
package riverpgxv5

import (
	"context"
	"embed"
	"encoding/json"
	"errors"
	"io/fs"
	"math"
	"strings"
	"sync"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgconn"
	"github.com/jackc/pgx/v5/pgtype"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/jackc/puddle/v2"

	"github.com/riverqueue/river/internal/dbunique"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5/internal/dbsqlc"
	"github.com/riverqueue/river/rivershared/sqlctemplate"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

//go:embed migration/*/*.sql
var migrationFS embed.FS

// Driver is an implementation of riverdriver.Driver for Pgx v5.
type Driver struct {
	dbPool   *pgxpool.Pool
	replacer sqlctemplate.Replacer
}

// New returns a new Pgx v5 River driver for use with River.
//
// It takes a pgxpool.Pool to use for use with River. The pool should already be
// configured to use the schema specified in the client's Schema field. The pool
// must not be closed while associated River objects are running.
//
// The database pool may be nil. If it is, a client that it's sent into will not
// be able to start up (calls to Start will error) and the Insert and InsertMany
// functions will be disabled, but the transactional-variants InsertTx and
// InsertManyTx continue to function. This behavior may be particularly useful
// in testing so that inserts can be performed and verified on a test
// transaction that will be rolled back.
func New(dbPool *pgxpool.Pool) *Driver {
	return &Driver{
		dbPool: dbPool,
	}
}

func (d *Driver) GetExecutor() riverdriver.Executor {
	return &Executor{templateReplaceWrapper{d.dbPool, &d.replacer}, d}
}
func (d *Driver) GetListener() riverdriver.Listener { return &Listener{dbPool: d.dbPool} }
func (d *Driver) GetMigrationFS(line string) fs.FS {
	if line == riverdriver.MigrationLineMain {
		return migrationFS
	}
	panic("migration line does not exist: " + line)
}
func (d *Driver) GetMigrationLines() []string { return []string{riverdriver.MigrationLineMain} }
func (d *Driver) HasPool() bool               { return d.dbPool != nil }
func (d *Driver) SupportsListener() bool      { return true }

func (d *Driver) UnwrapExecutor(tx pgx.Tx) riverdriver.ExecutorTx {
	// Allows UnwrapExecutor to be invoked even if driver is nil.
	var replacer *sqlctemplate.Replacer
	if d == nil {
		replacer = &sqlctemplate.Replacer{}
	} else {
		replacer = &d.replacer
	}

	return &ExecutorTx{Executor: Executor{templateReplaceWrapper{tx, replacer}, d}, tx: tx}
}

type Executor struct {
	dbtx   templateReplaceWrapper
	driver *Driver
}

func (e *Executor) Begin(ctx context.Context) (riverdriver.ExecutorTx, error) {
	tx, err := e.dbtx.Begin(ctx)
	if err != nil {
		return nil, err
	}
	return &ExecutorTx{Executor: Executor{templateReplaceWrapper{tx, &e.driver.replacer}, e.driver}, tx: tx}, nil
}

func (e *Executor) ColumnExists(ctx context.Context, tableName, columnName string) (bool, error) {
	exists, err := dbsqlc.New().ColumnExists(ctx, e.dbtx, &dbsqlc.ColumnExistsParams{
		ColumnName: columnName,
		TableName:  tableName,
	})
	return exists, interpretError(err)
}

func (e *Executor) Exec(ctx context.Context, sql string) (struct{}, error) {
	_, err := e.dbtx.Exec(ctx, sql)
	return struct{}{}, interpretError(err)
}

func (e *Executor) JobCancel(ctx context.Context, params *riverdriver.JobCancelParams) (*rivertype.JobRow, error) {
	cancelledAt, err := params.CancelAttemptedAt.MarshalJSON()
	if err != nil {
		return nil, err
	}

	job, err := dbsqlc.New().JobCancel(ctx, e.dbtx, &dbsqlc.JobCancelParams{
		ID:                params.ID,
		CancelAttemptedAt: cancelledAt,
		ControlTopic:      params.ControlTopic,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobCountByState(ctx context.Context, state rivertype.JobState) (int, error) {
	numJobs, err := dbsqlc.New().JobCountByState(ctx, e.dbtx, dbsqlc.RiverJobState(state))
	if err != nil {
		return 0, err
	}
	return int(numJobs), nil
}

func (e *Executor) JobDelete(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobDelete(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	if job.State == dbsqlc.RiverJobStateRunning {
		return nil, rivertype.ErrJobRunning
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobDeleteBefore(ctx context.Context, params *riverdriver.JobDeleteBeforeParams) (int, error) {
	numDeleted, err := dbsqlc.New().JobDeleteBefore(ctx, e.dbtx, &dbsqlc.JobDeleteBeforeParams{
		CancelledFinalizedAtHorizon: params.CancelledFinalizedAtHorizon,
		CompletedFinalizedAtHorizon: params.CompletedFinalizedAtHorizon,
		DiscardedFinalizedAtHorizon: params.DiscardedFinalizedAtHorizon,
		Max:                         int64(params.Max),
	})
	return int(numDeleted), interpretError(err)
}

func (e *Executor) JobGetAvailable(ctx context.Context, params *riverdriver.JobGetAvailableParams) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetAvailable(ctx, e.dbtx, &dbsqlc.JobGetAvailableParams{
		AttemptedBy: params.AttemptedBy,
		Max:         int32(min(params.Max, math.MaxInt32)), //nolint:gosec
		Now:         params.Now,
		Queue:       params.Queue,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobGetByID(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobGetByID(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobGetByIDMany(ctx context.Context, id []int64) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetByIDMany(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobGetByKindAndUniqueProperties(ctx context.Context, params *riverdriver.JobGetByKindAndUniquePropertiesParams) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobGetByKindAndUniqueProperties(ctx, e.dbtx, (*dbsqlc.JobGetByKindAndUniquePropertiesParams)(params))
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobGetByKindMany(ctx context.Context, kind []string) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetByKindMany(ctx, e.dbtx, kind)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobGetStuck(ctx context.Context, params *riverdriver.JobGetStuckParams) ([]*rivertype.JobRow, error) {
	jobs, err := dbsqlc.New().JobGetStuck(ctx, e.dbtx, &dbsqlc.JobGetStuckParams{Max: int32(min(params.Max, math.MaxInt32)), StuckHorizon: params.StuckHorizon}) //nolint:gosec
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobInsertFastMany(ctx context.Context, params []*riverdriver.JobInsertFastParams) ([]*riverdriver.JobInsertFastResult, error) {
	insertJobsParams := &dbsqlc.JobInsertFastManyParams{
		Args:         make([][]byte, len(params)),
		CreatedAt:    make([]time.Time, len(params)),
		Kind:         make([]string, len(params)),
		MaxAttempts:  make([]int16, len(params)),
		Metadata:     make([][]byte, len(params)),
		Priority:     make([]int16, len(params)),
		Queue:        make([]string, len(params)),
		ScheduledAt:  make([]time.Time, len(params)),
		State:        make([]string, len(params)),
		Tags:         make([]string, len(params)),
		UniqueKey:    make([][]byte, len(params)),
		UniqueStates: make([]pgtype.Bits, len(params)),
	}
	now := time.Now().UTC()

	for i := 0; i < len(params); i++ {
		params := params[i]

		createdAt := now
		if params.CreatedAt != nil {
			createdAt = *params.CreatedAt
		}

		scheduledAt := now
		if params.ScheduledAt != nil {
			scheduledAt = *params.ScheduledAt
		}

		tags := params.Tags
		if tags == nil {
			tags = []string{}
		}

		defaultObject := []byte("{}")

		insertJobsParams.Args[i] = sliceutil.FirstNonEmpty(params.EncodedArgs, defaultObject)
		insertJobsParams.CreatedAt[i] = createdAt
		insertJobsParams.Kind[i] = params.Kind
		insertJobsParams.MaxAttempts[i] = int16(min(params.MaxAttempts, math.MaxInt16)) //nolint:gosec
		insertJobsParams.Metadata[i] = sliceutil.FirstNonEmpty(params.Metadata, defaultObject)
		insertJobsParams.Priority[i] = int16(min(params.Priority, math.MaxInt16)) //nolint:gosec
		insertJobsParams.Queue[i] = params.Queue
		insertJobsParams.ScheduledAt[i] = scheduledAt
		insertJobsParams.State[i] = string(params.State)
		insertJobsParams.Tags[i] = strings.Join(tags, ",")
		insertJobsParams.UniqueKey[i] = sliceutil.FirstNonEmpty(params.UniqueKey)
		insertJobsParams.UniqueStates[i] = pgtype.Bits{Bytes: []byte{params.UniqueStates}, Len: 8, Valid: params.UniqueStates != 0}
	}

	items, err := dbsqlc.New().JobInsertFastMany(ctx, e.dbtx, insertJobsParams)
	if err != nil {
		return nil, interpretError(err)
	}

	return mapSliceError(items, func(row *dbsqlc.JobInsertFastManyRow) (*riverdriver.JobInsertFastResult, error) {
		job, err := jobRowFromInternal(&row.RiverJob)
		if err != nil {
			return nil, err
		}
		return &riverdriver.JobInsertFastResult{Job: job, UniqueSkippedAsDuplicate: row.UniqueSkippedAsDuplicate}, nil
	})
}

func (e *Executor) JobInsertFastManyNoReturning(ctx context.Context, params []*riverdriver.JobInsertFastParams) (int, error) {
	insertJobsParams := make([]*dbsqlc.JobInsertFastManyCopyFromParams, len(params))
	now := time.Now().UTC()

	for i := 0; i < len(params); i++ {
		params := params[i]

		createdAt := now
		if params.CreatedAt != nil {
			createdAt = *params.CreatedAt
		}

		metadata := params.Metadata
		if metadata == nil {
			metadata = []byte("{}")
		}

		scheduledAt := now
		if params.ScheduledAt != nil {
			scheduledAt = *params.ScheduledAt
		}

		tags := params.Tags
		if tags == nil {
			tags = []string{}
		}

		insertJobsParams[i] = &dbsqlc.JobInsertFastManyCopyFromParams{
			Args:         params.EncodedArgs,
			CreatedAt:    createdAt,
			Kind:         params.Kind,
			MaxAttempts:  int16(min(params.MaxAttempts, math.MaxInt16)), //nolint:gosec
			Metadata:     metadata,
			Priority:     int16(min(params.Priority, math.MaxInt16)), //nolint:gosec
			Queue:        params.Queue,
			ScheduledAt:  scheduledAt,
			State:        dbsqlc.RiverJobState(params.State),
			Tags:         tags,
			UniqueKey:    params.UniqueKey,
			UniqueStates: pgtype.Bits{Bytes: []byte{params.UniqueStates}, Len: 8, Valid: params.UniqueStates != 0},
		}
	}

	numInserted, err := dbsqlc.New().JobInsertFastManyCopyFrom(ctx, e.dbtx, insertJobsParams)
	if err != nil {
		return 0, interpretError(err)
	}

	return int(numInserted), nil
}

func (e *Executor) JobInsertFull(ctx context.Context, params *riverdriver.JobInsertFullParams) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobInsertFull(ctx, e.dbtx, &dbsqlc.JobInsertFullParams{
		Attempt:      int16(min(params.Attempt, math.MaxInt16)), //nolint:gosec
		AttemptedAt:  params.AttemptedAt,
		AttemptedBy:  params.AttemptedBy,
		Args:         params.EncodedArgs,
		CreatedAt:    params.CreatedAt,
		Errors:       params.Errors,
		FinalizedAt:  params.FinalizedAt,
		Kind:         params.Kind,
		MaxAttempts:  int16(min(params.MaxAttempts, math.MaxInt16)), //nolint:gosec
		Metadata:     params.Metadata,
		Priority:     int16(min(params.Priority, math.MaxInt16)), //nolint:gosec
		Queue:        params.Queue,
		ScheduledAt:  params.ScheduledAt,
		State:        dbsqlc.RiverJobState(params.State),
		Tags:         params.Tags,
		UniqueKey:    params.UniqueKey,
		UniqueStates: pgtype.Bits{Bytes: []byte{params.UniqueStates}, Len: 8, Valid: params.UniqueStates != 0},
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobList(ctx context.Context, params *riverdriver.JobListParams) ([]*rivertype.JobRow, error) {
	ctx = sqlctemplate.WithReplacements(ctx, map[string]sqlctemplate.Replacement{
		"order_by_clause": {Value: params.OrderByClause},
		"where_clause":    {Value: params.WhereClause},
	}, params.NamedArgs)

	jobs, err := dbsqlc.New().JobList(ctx, e.dbtx, params.Max)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobRescueMany(ctx context.Context, params *riverdriver.JobRescueManyParams) (*struct{}, error) {
	err := dbsqlc.New().JobRescueMany(ctx, e.dbtx, (*dbsqlc.JobRescueManyParams)(params))
	if err != nil {
		return nil, interpretError(err)
	}
	return &struct{}{}, nil
}

func (e *Executor) JobRetry(ctx context.Context, id int64) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobRetry(ctx, e.dbtx, id)
	if err != nil {
		return nil, interpretError(err)
	}
	return jobRowFromInternal(job)
}

func (e *Executor) JobSchedule(ctx context.Context, params *riverdriver.JobScheduleParams) ([]*riverdriver.JobScheduleResult, error) {
	scheduleResults, err := dbsqlc.New().JobSchedule(ctx, e.dbtx, &dbsqlc.JobScheduleParams{
		Max: int64(params.Max),
		Now: params.Now,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(scheduleResults, func(result *dbsqlc.JobScheduleRow) (*riverdriver.JobScheduleResult, error) {
		job, err := jobRowFromInternal(&result.RiverJob)
		if err != nil {
			return nil, err
		}
		return &riverdriver.JobScheduleResult{ConflictDiscarded: result.ConflictDiscarded, Job: *job}, nil
	})
}

func (e *Executor) JobSetStateIfRunningMany(ctx context.Context, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
	setStateParams := &dbsqlc.JobSetStateIfRunningManyParams{
		IDs:                 params.ID,
		Attempt:             make([]int32, len(params.ID)),
		AttemptDoUpdate:     make([]bool, len(params.ID)),
		Errors:              params.ErrData,
		ErrorsDoUpdate:      make([]bool, len(params.ID)),
		FinalizedAt:         make([]time.Time, len(params.ID)),
		FinalizedAtDoUpdate: make([]bool, len(params.ID)),
		MetadataDoMerge:     make([]bool, len(params.ID)),
		MetadataUpdates:     make([][]byte, len(params.ID)),
		ScheduledAt:         make([]time.Time, len(params.ID)),
		ScheduledAtDoUpdate: make([]bool, len(params.ID)),
		State:               make([]string, len(params.ID)),
	}

	for i := range len(params.ID) {
		if params.Attempt[i] != nil {
			setStateParams.AttemptDoUpdate[i] = true
			setStateParams.Attempt[i] = int32(*params.Attempt[i]) //nolint:gosec
		}
		if params.ErrData[i] != nil {
			setStateParams.ErrorsDoUpdate[i] = true
		}
		if params.FinalizedAt[i] != nil {
			setStateParams.FinalizedAtDoUpdate[i] = true
			setStateParams.FinalizedAt[i] = *params.FinalizedAt[i]
		}
		if params.MetadataDoMerge[i] {
			setStateParams.MetadataDoMerge[i] = true
			setStateParams.MetadataUpdates[i] = params.MetadataUpdates[i]
		}
		if params.ScheduledAt[i] != nil {
			setStateParams.ScheduledAtDoUpdate[i] = true
			setStateParams.ScheduledAt[i] = *params.ScheduledAt[i]
		}
		setStateParams.State[i] = string(params.State[i])
	}

	jobs, err := dbsqlc.New().JobSetStateIfRunningMany(ctx, e.dbtx, setStateParams)
	if err != nil {
		return nil, interpretError(err)
	}
	return mapSliceError(jobs, jobRowFromInternal)
}

func (e *Executor) JobUpdate(ctx context.Context, params *riverdriver.JobUpdateParams) (*rivertype.JobRow, error) {
	job, err := dbsqlc.New().JobUpdate(ctx, e.dbtx, &dbsqlc.JobUpdateParams{
		ID:                  params.ID,
		AttemptedAtDoUpdate: params.AttemptedAtDoUpdate,
		Attempt:             int16(min(params.Attempt, math.MaxInt16)), //nolint:gosec
		AttemptDoUpdate:     params.AttemptDoUpdate,
		AttemptedAt:         params.AttemptedAt,
		AttemptedBy:         params.AttemptedBy,
		AttemptedByDoUpdate: params.AttemptedByDoUpdate,
		ErrorsDoUpdate:      params.ErrorsDoUpdate,
		Errors:              params.Errors,
		FinalizedAtDoUpdate: params.FinalizedAtDoUpdate,
		FinalizedAt:         params.FinalizedAt,
		StateDoUpdate:       params.StateDoUpdate,
		State:               dbsqlc.RiverJobState(params.State),
	})
	if err != nil {
		return nil, interpretError(err)
	}

	return jobRowFromInternal(job)
}

func (e *Executor) LeaderAttemptElect(ctx context.Context, params *riverdriver.LeaderElectParams) (bool, error) {
	numElectionsWon, err := dbsqlc.New().LeaderAttemptElect(ctx, e.dbtx, &dbsqlc.LeaderAttemptElectParams{
		LeaderID: params.LeaderID,
		TTL:      params.TTL,
	})
	if err != nil {
		return false, interpretError(err)
	}
	return numElectionsWon > 0, nil
}

func (e *Executor) LeaderAttemptReelect(ctx context.Context, params *riverdriver.LeaderElectParams) (bool, error) {
	numElectionsWon, err := dbsqlc.New().LeaderAttemptReelect(ctx, e.dbtx, &dbsqlc.LeaderAttemptReelectParams{
		LeaderID: params.LeaderID,
		TTL:      params.TTL,
	})
	if err != nil {
		return false, interpretError(err)
	}
	return numElectionsWon > 0, nil
}

func (e *Executor) LeaderDeleteExpired(ctx context.Context) (int, error) {
	numDeleted, err := dbsqlc.New().LeaderDeleteExpired(ctx, e.dbtx)
	if err != nil {
		return 0, interpretError(err)
	}
	return int(numDeleted), nil
}

func (e *Executor) LeaderGetElectedLeader(ctx context.Context) (*riverdriver.Leader, error) {
	leader, err := dbsqlc.New().LeaderGetElectedLeader(ctx, e.dbtx)
	if err != nil {
		return nil, interpretError(err)
	}
	return leaderFromInternal(leader), nil
}

func (e *Executor) LeaderInsert(ctx context.Context, params *riverdriver.LeaderInsertParams) (*riverdriver.Leader, error) {
	leader, err := dbsqlc.New().LeaderInsert(ctx, e.dbtx, &dbsqlc.LeaderInsertParams{
		ElectedAt: params.ElectedAt,
		ExpiresAt: params.ExpiresAt,
		LeaderID:  params.LeaderID,
		TTL:       params.TTL,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return leaderFromInternal(leader), nil
}

func (e *Executor) LeaderResign(ctx context.Context, params *riverdriver.LeaderResignParams) (bool, error) {
	numResigned, err := dbsqlc.New().LeaderResign(ctx, e.dbtx, &dbsqlc.LeaderResignParams{
		LeaderID:        params.LeaderID,
		LeadershipTopic: params.LeadershipTopic,
	})
	if err != nil {
		return false, interpretError(err)
	}
	return numResigned > 0, nil
}

func (e *Executor) MigrationDeleteAssumingMainMany(ctx context.Context, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationDeleteAssumingMainMany(ctx, e.dbtx,
		sliceutil.Map(versions, func(v int) int64 { return int64(v) }))
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, func(internal *dbsqlc.RiverMigrationDeleteAssumingMainManyRow) *riverdriver.Migration {
		return &riverdriver.Migration{
			CreatedAt: internal.CreatedAt.UTC(),
			Line:      riverdriver.MigrationLineMain,
			Version:   int(internal.Version),
		}
	}), nil
}

func (e *Executor) MigrationDeleteByLineAndVersionMany(ctx context.Context, line string, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationDeleteByLineAndVersionMany(ctx, e.dbtx, &dbsqlc.RiverMigrationDeleteByLineAndVersionManyParams{
		Line:    line,
		Version: sliceutil.Map(versions, func(v int) int64 { return int64(v) }),
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, migrationFromInternal), nil
}

func (e *Executor) MigrationGetAllAssumingMain(ctx context.Context) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationGetAllAssumingMain(ctx, e.dbtx)
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, func(internal *dbsqlc.RiverMigrationGetAllAssumingMainRow) *riverdriver.Migration {
		return &riverdriver.Migration{
			CreatedAt: internal.CreatedAt.UTC(),
			Line:      riverdriver.MigrationLineMain,
			Version:   int(internal.Version),
		}
	}), nil
}

func (e *Executor) MigrationGetByLine(ctx context.Context, line string) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationGetByLine(ctx, e.dbtx, line)
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, migrationFromInternal), nil
}

func (e *Executor) MigrationInsertMany(ctx context.Context, line string, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationInsertMany(ctx, e.dbtx, &dbsqlc.RiverMigrationInsertManyParams{
		Line:    line,
		Version: sliceutil.Map(versions, func(v int) int64 { return int64(v) }),
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, migrationFromInternal), nil
}

func (e *Executor) MigrationInsertManyAssumingMain(ctx context.Context, versions []int) ([]*riverdriver.Migration, error) {
	migrations, err := dbsqlc.New().RiverMigrationInsertManyAssumingMain(ctx, e.dbtx,
		sliceutil.Map(versions, func(v int) int64 { return int64(v) }),
	)
	if err != nil {
		return nil, interpretError(err)
	}
	return sliceutil.Map(migrations, func(internal *dbsqlc.RiverMigrationInsertManyAssumingMainRow) *riverdriver.Migration {
		return &riverdriver.Migration{
			CreatedAt: internal.CreatedAt.UTC(),
			Line:      riverdriver.MigrationLineMain,
			Version:   int(internal.Version),
		}
	}), nil
}

func (e *Executor) NotifyMany(ctx context.Context, params *riverdriver.NotifyManyParams) error {
	return dbsqlc.New().PGNotifyMany(ctx, e.dbtx, &dbsqlc.PGNotifyManyParams{
		Payload: params.Payload,
		Topic:   params.Topic,
	})
}

func (e *Executor) PGAdvisoryXactLock(ctx context.Context, key int64) (*struct{}, error) {
	err := dbsqlc.New().PGAdvisoryXactLock(ctx, e.dbtx, key)
	return &struct{}{}, interpretError(err)
}

func (e *Executor) QueueCreateOrSetUpdatedAt(ctx context.Context, params *riverdriver.QueueCreateOrSetUpdatedAtParams) (*rivertype.Queue, error) {
	queue, err := dbsqlc.New().QueueCreateOrSetUpdatedAt(ctx, e.dbtx, &dbsqlc.QueueCreateOrSetUpdatedAtParams{
		Metadata:  params.Metadata,
		Name:      params.Name,
		PausedAt:  params.PausedAt,
		UpdatedAt: params.UpdatedAt,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	return queueFromInternal(queue), nil
}

func (e *Executor) QueueDeleteExpired(ctx context.Context, params *riverdriver.QueueDeleteExpiredParams) ([]string, error) {
	queues, err := dbsqlc.New().QueueDeleteExpired(ctx, e.dbtx, &dbsqlc.QueueDeleteExpiredParams{
		Max:              int64(params.Max),
		UpdatedAtHorizon: params.UpdatedAtHorizon,
	})
	if err != nil {
		return nil, interpretError(err)
	}
	queueNames := make([]string, len(queues))
	for i, q := range queues {
		queueNames[i] = q.Name
	}
	return queueNames, nil
}

func (e *Executor) QueueGet(ctx context.Context, name string) (*rivertype.Queue, error) {
	queue, err := dbsqlc.New().QueueGet(ctx, e.dbtx, name)
	if err != nil {
		return nil, interpretError(err)
	}
	return queueFromInternal(queue), nil
}

func (e *Executor) QueueList(ctx context.Context, limit int) ([]*rivertype.Queue, error) {
	internalQueues, err := dbsqlc.New().QueueList(ctx, e.dbtx, int32(min(limit, math.MaxInt32))) //nolint:gosec
	if err != nil {
		return nil, interpretError(err)
	}
	queues := make([]*rivertype.Queue, len(internalQueues))
	for i, q := range internalQueues {
		queues[i] = queueFromInternal(q)
	}
	return queues, nil
}

func (e *Executor) QueuePause(ctx context.Context, name string) error {
	res, err := dbsqlc.New().QueuePause(ctx, e.dbtx, name)
	if err != nil {
		return interpretError(err)
	}
	if res.RowsAffected() == 0 && name != riverdriver.AllQueuesString {
		return rivertype.ErrNotFound
	}
	return nil
}

func (e *Executor) QueueResume(ctx context.Context, name string) error {
	res, err := dbsqlc.New().QueueResume(ctx, e.dbtx, name)
	if err != nil {
		return interpretError(err)
	}
	if res.RowsAffected() == 0 && name != riverdriver.AllQueuesString {
		return rivertype.ErrNotFound
	}
	return nil
}

func (e *Executor) TableExists(ctx context.Context, tableName string) (bool, error) {
	exists, err := dbsqlc.New().TableExists(ctx, e.dbtx, tableName)
	return exists, interpretError(err)
}

type ExecutorTx struct {
	Executor
	tx pgx.Tx
}

func (t *ExecutorTx) Commit(ctx context.Context) error {
	return t.tx.Commit(ctx)
}

func (t *ExecutorTx) Rollback(ctx context.Context) error {
	return t.tx.Rollback(ctx)
}

type Listener struct {
	conn   *pgx.Conn
	dbPool *pgxpool.Pool
	prefix string
	mu     sync.Mutex
}

func (l *Listener) Close(ctx context.Context) error {
	l.mu.Lock()
	defer l.mu.Unlock()

	if l.conn == nil {
		return nil
	}

	// Release below would take care of cleanup and potentially put the
	// connection back into rotation, but in case a Listen was invoked without a
	// subsequent Unlisten on the same topic, close the connection explicitly to
	// guarantee no other caller will receive a partially tainted connection.
	err := l.conn.Close(ctx)

	// Even in the event of an error, make sure conn is set back to nil so that
	// the listener can be reused.
	l.conn = nil

	return err
}

func (l *Listener) Connect(ctx context.Context) error {
	l.mu.Lock()
	defer l.mu.Unlock()

	if l.conn != nil {
		return errors.New("connection already established")
	}

	poolConn, err := l.dbPool.Acquire(ctx)
	if err != nil {
		return err
	}

	var schema string
	if err := poolConn.QueryRow(ctx, "SELECT current_schema();").Scan(&schema); err != nil {
		poolConn.Release()
		return err
	}

	l.prefix = schema + "."
	// Assume full ownership of the conn so that it doesn't get released back to
	// the pool or auto-closed by the pool.
	l.conn = poolConn.Hijack()

	return nil
}

func (l *Listener) Listen(ctx context.Context, topic string) error {
	l.mu.Lock()
	defer l.mu.Unlock()

	_, err := l.conn.Exec(ctx, "LISTEN \""+l.prefix+topic+"\"")
	return err
}

func (l *Listener) Ping(ctx context.Context) error {
	l.mu.Lock()
	defer l.mu.Unlock()

	return l.conn.Ping(ctx)
}

func (l *Listener) Unlisten(ctx context.Context, topic string) error {
	l.mu.Lock()
	defer l.mu.Unlock()

	_, err := l.conn.Exec(ctx, "UNLISTEN \""+l.prefix+topic+"\"")
	return err
}

func (l *Listener) WaitForNotification(ctx context.Context) (*riverdriver.Notification, error) {
	l.mu.Lock()
	defer l.mu.Unlock()

	notification, err := l.conn.WaitForNotification(ctx)
	if err != nil {
		return nil, err
	}

	return &riverdriver.Notification{
		Topic:   strings.TrimPrefix(notification.Channel, l.prefix),
		Payload: notification.Payload,
	}, nil
}

type templateReplaceWrapper struct {
	dbtx interface {
		dbsqlc.DBTX
		Begin(ctx context.Context) (pgx.Tx, error)
	}
	replacer *sqlctemplate.Replacer
}

func (w templateReplaceWrapper) Begin(ctx context.Context) (pgx.Tx, error) {
	return w.dbtx.Begin(ctx)
}

func (w templateReplaceWrapper) Exec(ctx context.Context, sql string, args ...interface{}) (pgconn.CommandTag, error) {
	sql, args = w.replacer.Run(ctx, sql, args)
	return w.dbtx.Exec(ctx, sql, args...)
}

func (w templateReplaceWrapper) Query(ctx context.Context, sql string, args ...interface{}) (pgx.Rows, error) {
	sql, args = w.replacer.Run(ctx, sql, args)
	return w.dbtx.Query(ctx, sql, args...)
}

func (w templateReplaceWrapper) QueryRow(ctx context.Context, sql string, args ...interface{}) pgx.Row {
	sql, args = w.replacer.Run(ctx, sql, args)
	return w.dbtx.QueryRow(ctx, sql, args...)
}

func (w templateReplaceWrapper) CopyFrom(ctx context.Context, tableName pgx.Identifier, columnNames []string, rowSrc pgx.CopyFromSource) (int64, error) {
	return w.dbtx.CopyFrom(ctx, tableName, columnNames, rowSrc)
}

func interpretError(err error) error {
	if errors.Is(err, puddle.ErrClosedPool) {
		return riverdriver.ErrClosedPool
	}
	if errors.Is(err, pgx.ErrNoRows) {
		return rivertype.ErrNotFound
	}
	return err
}

func jobRowFromInternal(internal *dbsqlc.RiverJob) (*rivertype.JobRow, error) {
	var attemptedAt *time.Time
	if internal.AttemptedAt != nil {
		t := internal.AttemptedAt.UTC()
		attemptedAt = &t
	}

	errors := make([]rivertype.AttemptError, len(internal.Errors))
	for i, rawError := range internal.Errors {
		if err := json.Unmarshal(rawError, &errors[i]); err != nil {
			return nil, err
		}
	}

	var finalizedAt *time.Time
	if internal.FinalizedAt != nil {
		t := internal.FinalizedAt.UTC()
		finalizedAt = &t
	}

	var uniqueStatesByte byte
	if internal.UniqueStates.Valid && len(internal.UniqueStates.Bytes) > 0 {
		uniqueStatesByte = internal.UniqueStates.Bytes[0]
	}

	return &rivertype.JobRow{
		ID:           internal.ID,
		Attempt:      max(int(internal.Attempt), 0),
		AttemptedAt:  attemptedAt,
		AttemptedBy:  internal.AttemptedBy,
		CreatedAt:    internal.CreatedAt.UTC(),
		EncodedArgs:  internal.Args,
		Errors:       errors,
		FinalizedAt:  finalizedAt,
		Kind:         internal.Kind,
		MaxAttempts:  max(int(internal.MaxAttempts), 0),
		Metadata:     internal.Metadata,
		Priority:     max(int(internal.Priority), 0),
		Queue:        internal.Queue,
		ScheduledAt:  internal.ScheduledAt.UTC(),
		State:        rivertype.JobState(internal.State),
		Tags:         internal.Tags,
		UniqueKey:    internal.UniqueKey,
		UniqueStates: dbunique.UniqueBitmaskToStates(uniqueStatesByte),
	}, nil
}

func leaderFromInternal(internal *dbsqlc.RiverLeader) *riverdriver.Leader {
	return &riverdriver.Leader{
		ElectedAt: internal.ElectedAt.UTC(),
		ExpiresAt: internal.ExpiresAt.UTC(),
		LeaderID:  internal.LeaderID,
	}
}

// mapSliceError manipulates a slice and transforms it to a slice of another
// type, returning the first error that occurred invoking the map function, if
// there was one.
func mapSliceError[T any, R any](collection []T, mapFunc func(T) (R, error)) ([]R, error) {
	if collection == nil {
		return nil, nil
	}

	result := make([]R, len(collection))

	for i, item := range collection {
		var err error
		result[i], err = mapFunc(item)
		if err != nil {
			return nil, err
		}
	}

	return result, nil
}

func migrationFromInternal(internal *dbsqlc.RiverMigration) *riverdriver.Migration {
	return &riverdriver.Migration{
		CreatedAt: internal.CreatedAt.UTC(),
		Line:      internal.Line,
		Version:   int(internal.Version),
	}
}

func queueFromInternal(internal *dbsqlc.RiverQueue) *rivertype.Queue {
	var pausedAt *time.Time
	if internal.PausedAt != nil {
		t := internal.PausedAt.UTC()
		pausedAt = &t
	}
	return &rivertype.Queue{
		CreatedAt: internal.CreatedAt.UTC(),
		Metadata:  internal.Metadata,
		Name:      internal.Name,
		PausedAt:  pausedAt,
		UpdatedAt: internal.UpdatedAt.UTC(),
	}
}

```

`riverdriver/riverpgxv5/river_pgx_v5_driver_test.go`:

```go
package riverpgxv5

import (
	"context"
	"errors"
	"fmt"
	"net"
	"os"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/jackc/puddle/v2"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivertype"
)

// Verify interface compliance.
var _ riverdriver.Driver[pgx.Tx] = New(nil)

func TestNew(t *testing.T) {
	t.Parallel()

	t.Run("AllowsNilDatabasePool", func(t *testing.T) {
		t.Parallel()

		dbPool := &pgxpool.Pool{}
		driver := New(dbPool)
		require.Equal(t, dbPool, driver.dbPool)
	})

	t.Run("AllowsNilDatabasePool", func(t *testing.T) {
		t.Parallel()

		driver := New(nil)
		require.Nil(t, driver.dbPool)
	})
}

func TestListener_Close(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("NoOpWithoutConn", func(t *testing.T) {
		t.Parallel()

		listener := &Listener{dbPool: testPool(ctx, t, nil)}
		require.Nil(t, listener.conn)
		require.NoError(t, listener.Close(ctx))
	})

	t.Run("UnsetsConnEvenOnError", func(t *testing.T) {
		t.Parallel()

		var connStub *connStub

		config := testPoolConfig()
		config.ConnConfig.DialFunc = func(ctx context.Context, network, addr string) (net.Conn, error) {
			// Dialer settings come from pgx's default internal one (not exported unfortunately).
			conn, err := (&net.Dialer{KeepAlive: 5 * time.Minute}).Dial(network, addr)
			if err != nil {
				return nil, err
			}

			connStub = newConnStub(conn)
			return connStub, nil
		}

		listener := &Listener{dbPool: testPool(ctx, t, config)}

		require.NoError(t, listener.Connect(ctx))
		require.NotNil(t, listener.conn)

		expectedErr := errors.New("conn close error")
		connStub.closeFunc = func() error {
			t.Logf("Close invoked; returning error")
			return expectedErr
		}

		require.ErrorIs(t, listener.Close(ctx), expectedErr)

		// Despite error, internal connection still unset.
		require.Nil(t, listener.conn)
	})
}

func TestInterpretError(t *testing.T) {
	t.Parallel()

	require.EqualError(t, interpretError(errors.New("an error")), "an error")
	require.ErrorIs(t, interpretError(puddle.ErrClosedPool), riverdriver.ErrClosedPool)
	require.ErrorIs(t, interpretError(pgx.ErrNoRows), rivertype.ErrNotFound)
	require.NoError(t, interpretError(nil))
}

// connStub implements net.Conn and allows us to stub particular functions like
// Close that are otherwise nigh impossible to test.
type connStub struct {
	net.Conn

	closeFunc func() error
}

func newConnStub(conn net.Conn) *connStub {
	return &connStub{
		Conn: conn,

		closeFunc: conn.Close,
	}
}

func (c *connStub) Close() error {
	return c.closeFunc()
}

func testPoolConfig() *pgxpool.Config {
	databaseURL := "postgres://localhost/river_test?sslmode=disable"
	if url := os.Getenv("TEST_DATABASE_URL"); url != "" {
		databaseURL = url
	}

	config, err := pgxpool.ParseConfig(databaseURL)
	if err != nil {
		panic(fmt.Sprintf("error parsing database URL: %v", err))
	}
	config.ConnConfig.ConnectTimeout = 10 * time.Second
	config.ConnConfig.RuntimeParams["timezone"] = "UTC"

	return config
}

func testPool(ctx context.Context, t *testing.T, config *pgxpool.Config) *pgxpool.Pool {
	t.Helper()

	if config == nil {
		config = testPoolConfig()
	}

	dbPool, err := pgxpool.NewWithConfig(ctx, config)
	require.NoError(t, err)
	t.Cleanup(dbPool.Close)
	return dbPool
}

```

`rivermigrate/example_migrate_database_sql_test.go`:

```go
package rivermigrate_test

import (
	"context"
	"database/sql"
	"fmt"
	"strings"

	_ "github.com/jackc/pgx/v5/stdlib"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql"
	"github.com/riverqueue/river/rivermigrate"
)

// Example_migrateDatabaseSQL demonstrates the use of River's Go migration API
// through Go's built-in database/sql package.
func Example_migrateDatabaseSQL() {
	ctx := context.Background()

	// Use a dedicated Postgres schema for this example so we can migrate and drop it at will:
	schemaName := "migration_example_dbsql"
	url := riverinternaltest.DatabaseURL("river_test_example") + "&search_path=" + schemaName
	dbPool, err := sql.Open("pgx", url)
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	driver := riverdatabasesql.New(dbPool)
	migrator, err := rivermigrate.New(driver, nil)
	if err != nil {
		panic(err)
	}

	// Create the schema used for this example. Drop it when we're done.
	// This isn't necessary outside this test.
	if _, err := dbPool.ExecContext(ctx, "CREATE SCHEMA IF NOT EXISTS "+schemaName); err != nil {
		panic(err)
	}
	defer dropRiverSchema(ctx, driver, schemaName)

	printVersions := func(res *rivermigrate.MigrateResult) {
		for _, version := range res.Versions {
			fmt.Printf("Migrated [%s] version %d\n", strings.ToUpper(string(res.Direction)), version.Version)
		}
	}

	// Migrate to version 3. An actual call may want to omit all MigrateOpts,
	// which will default to applying all available up migrations.
	res, err := migrator.Migrate(ctx, rivermigrate.DirectionUp, &rivermigrate.MigrateOpts{
		TargetVersion: 3,
	})
	if err != nil {
		panic(err)
	}
	printVersions(res)

	// Migrate down by three steps. Down migrating defaults to running only one
	// step unless overridden by an option like MaxSteps or TargetVersion.
	res, err = migrator.Migrate(ctx, rivermigrate.DirectionDown, &rivermigrate.MigrateOpts{
		MaxSteps: 3,
	})
	if err != nil {
		panic(err)
	}
	printVersions(res)

	// Output:
	// Migrated [UP] version 1
	// Migrated [UP] version 2
	// Migrated [UP] version 3
	// Migrated [DOWN] version 3
	// Migrated [DOWN] version 2
	// Migrated [DOWN] version 1
}

```

`rivermigrate/example_migrate_test.go`:

```go
package rivermigrate_test

import (
	"context"
	"fmt"
	"strings"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivermigrate"
)

// Example_migrate demonstrates the use of River's Go migration API by migrating
// up and down.
func Example_migrate() {
	ctx := context.Background()

	// Use a dedicated Postgres schema for this example so we can migrate and drop it at will:
	schemaName := "migration_example"
	poolConfig := riverinternaltest.DatabaseConfig("river_test_example")
	poolConfig.ConnConfig.RuntimeParams["search_path"] = schemaName

	dbPool, err := pgxpool.NewWithConfig(ctx, poolConfig)
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	driver := riverpgxv5.New(dbPool)
	migrator, err := rivermigrate.New(driver, nil)
	if err != nil {
		panic(err)
	}

	// Create the schema used for this example. Drop it when we're done.
	// This isn't necessary outside this test.
	if _, err := dbPool.Exec(ctx, "CREATE SCHEMA IF NOT EXISTS "+schemaName); err != nil {
		panic(err)
	}
	defer dropRiverSchema(ctx, driver, schemaName)

	printVersions := func(res *rivermigrate.MigrateResult) {
		for _, version := range res.Versions {
			fmt.Printf("Migrated [%s] version %d\n", strings.ToUpper(string(res.Direction)), version.Version)
		}
	}

	// Migrate to version 3. An actual call may want to omit all MigrateOpts,
	// which will default to applying all available up migrations.
	res, err := migrator.Migrate(ctx, rivermigrate.DirectionUp, &rivermigrate.MigrateOpts{
		TargetVersion: 3,
	})
	if err != nil {
		panic(err)
	}
	printVersions(res)

	// Migrate down by three steps. Down migrating defaults to running only one
	// step unless overridden by an option like MaxSteps or TargetVersion.
	res, err = migrator.Migrate(ctx, rivermigrate.DirectionDown, &rivermigrate.MigrateOpts{
		MaxSteps: 3,
	})
	if err != nil {
		panic(err)
	}
	printVersions(res)

	// Output:
	// Migrated [UP] version 1
	// Migrated [UP] version 2
	// Migrated [UP] version 3
	// Migrated [DOWN] version 3
	// Migrated [DOWN] version 2
	// Migrated [DOWN] version 1
}

func dropRiverSchema[TTx any](ctx context.Context, driver riverdriver.Driver[TTx], schemaName string) {
	_, err := driver.GetExecutor().Exec(ctx, "DROP SCHEMA IF EXISTS "+schemaName+" CASCADE;")
	if err != nil {
		panic(err)
	}
}

```

`rivermigrate/main_test.go`:

```go
package rivermigrate_test

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`rivermigrate/migration/alternate/001_premier.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/001_premier.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/002_deuxieme.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/002_deuxieme.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/003_troisieme.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/003_troisieme.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/004_quatrieme.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/004_quatrieme.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/005_cinquieme.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/005_cinquieme.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/006_sixieme.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/alternate/006_sixieme.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/commit_required/001_first.down.sql`:

```sql
DROP TYPE foobar;

```

`rivermigrate/migration/commit_required/001_first.up.sql`:

```sql
-- create a foobar enum with values foo, bar:
CREATE TYPE foobar AS ENUM ('foo', 'bar');

```

`rivermigrate/migration/commit_required/002_second.down.sql`:

```sql
-- not truly reversible, can't remove enum values.

```

`rivermigrate/migration/commit_required/002_second.up.sql`:

```sql
ALTER TYPE foobar ADD VALUE 'baz' AFTER 'bar';

```

`rivermigrate/migration/commit_required/003_third.down.sql`:

```sql
DROP FUNCTION foobar_in_bitmask;

```

`rivermigrate/migration/commit_required/003_third.up.sql`:

```sql
CREATE OR REPLACE FUNCTION foobar_in_bitmask(bitmask BIT(8), val foobar)
RETURNS boolean
LANGUAGE SQL
IMMUTABLE
AS $$
    SELECT CASE val
        WHEN 'foo' THEN get_bit(bitmask, 7)
        WHEN 'bar' THEN get_bit(bitmask, 6)
        -- Because the enum value 'baz' was added in migration 2 and not part
        -- of the original enum, we can't use it in an immutable SQL function
        -- unless the new enum value migration has been committed.
        WHEN 'baz' THEN get_bit(bitmask, 5)
        ELSE 0
    END = 1;
$$;

```

`rivermigrate/migration/main/001_first.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/main/001_first.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/main/002_second.down.sql`:

```sql
SELECT 1;

```

`rivermigrate/migration/main/002_second.up.sql`:

```sql
SELECT 1;

```

`rivermigrate/river_migrate.go`:

```go
// Package rivermigrate provides a Go API for running migrations as alternative
// to migrating via the bundled CLI.
package rivermigrate

import (
	"context"
	"errors"
	"fmt"
	"io"
	"io/fs"
	"log/slog"
	"maps"
	"os"
	"slices"
	"strconv"
	"strings"
	"time"

	"github.com/riverqueue/river/internal/util/dbutil"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/levenshtein"
	"github.com/riverqueue/river/rivershared/util/maputil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivershared/util/valutil"
)

// The migrate version where the `line` column was added. Meaningful in that the
// migrator has to behave a little differently depending on whether it's working
// with versions before or after this boundary.
const migrateVersionLineColumnAdded = 5

// Migration is a bundled migration containing a version (e.g. 1, 2, 3), and SQL
// for up and down directions.
type Migration struct {
	// Name is a human-friendly name for the migration derived from its
	// filename.
	Name string

	// SQLDown is the s SQL for the migration's down direction.
	SQLDown string

	// SQLUp is the s SQL for the migration's up direction.
	SQLUp string

	// Version is the integer version number of this migration.
	Version int
}

// Config contains configuration for Migrator.
type Config struct {
	// Line is the migration line to use. Most drivers will only have a single
	// line, which is `main`.
	//
	// Defaults to `main`.
	Line string

	// Logger is the structured logger to use for logging purposes. If none is
	// specified, logs will be emitted to STDOUT with messages at warn level
	// or higher.
	Logger *slog.Logger
}

// Migrator is a database migration tool for River which can run up or down
// migrations in order to establish the schema that the queue needs to run.
type Migrator[TTx any] struct {
	baseservice.BaseService

	driver     riverdriver.Driver[TTx]
	line       string
	migrations map[int]Migration // allows us to inject test migrations
}

// New returns a new migrator with the given database driver and configuration.
// The config parameter may be omitted as nil.
//
// Two drivers are supported for migrations, one for Pgx v5 and one for the
// built-in database/sql package for use with migration frameworks like Goose.
// See packages riverpgxv5 and riverdatabasesql respectively.
//
// The function takes a generic parameter TTx representing a transaction type,
// but it can be omitted because it'll generally always be inferred from the
// driver. For example:
//
//	import "github.com/riverqueue/river/riverdriver/riverpgxv5"
//	import "github.com/riverqueue/rivermigrate"
//
//	...
//
//	dbPool, err := pgxpool.New(ctx, os.Getenv("DATABASE_URL"))
//	if err != nil {
//		// handle error
//	}
//	defer dbPool.Close()
//
//	migrator, err := rivermigrate.New(riverpgxv5.New(dbPool), nil)
//	if err != nil {
//		// handle error
//	}
func New[TTx any](driver riverdriver.Driver[TTx], config *Config) (*Migrator[TTx], error) {
	if config == nil {
		config = &Config{}
	}

	line := valutil.ValOrDefault(config.Line, riverdriver.MigrationLineMain)

	logger := config.Logger
	if logger == nil {
		logger = slog.New(slog.NewTextHandler(os.Stdout, &slog.HandlerOptions{
			Level: slog.LevelWarn,
		}))
	}

	archetype := &baseservice.Archetype{
		Logger: logger,
		Time:   &baseservice.UnStubbableTimeGenerator{},
	}

	if !slices.Contains(driver.GetMigrationLines(), line) {
		const minLevenshteinDistance = 2

		var suggestedLines []string
		for _, existingLine := range driver.GetMigrationLines() {
			if distance := levenshtein.ComputeDistance(existingLine, line); distance <= minLevenshteinDistance {
				suggestedLines = append(suggestedLines, "`"+existingLine+"`")
			}
		}

		errorStr := "migration line does not exist: " + line
		switch {
		case len(suggestedLines) == 1:
			errorStr += fmt.Sprintf(" (did you mean %s?)", suggestedLines[0])
		case len(suggestedLines) > 1:
			errorStr += fmt.Sprintf(" (did you mean one of %v?)", strings.Join(suggestedLines, ", "))
		}

		return nil, errors.New(errorStr)
	}

	riverMigrations, err := migrationsFromFS(driver.GetMigrationFS(line), line)
	if err != nil {
		// If there's ever a problem here, it's a very fundamental internal
		// River one, so it's okay to panic.
		panic(err)
	}

	return baseservice.Init(archetype, &Migrator[TTx]{
		driver:     driver,
		line:       line,
		migrations: validateAndInit(riverMigrations),
	}), nil
}

// ExistingVersions gets the existing set of versions that have been migrated in
// the database, ordered by version.
func (m *Migrator[TTx]) ExistingVersions(ctx context.Context) ([]Migration, error) {
	migrations, err := m.existingMigrations(ctx, m.driver.GetExecutor())
	if err != nil {
		return nil, err
	}

	versions, err := m.versionsFromDriver(migrations)
	if err != nil {
		return nil, err
	}

	return versions, nil
}

// ExistingVersions gets the existing set of versions that have been migrated in
// the database, ordered by version.
//
// This variant checks for existing versions in a transaction.
func (m *Migrator[TTx]) ExistingVersionsTx(ctx context.Context, tx TTx) ([]Migration, error) {
	migrations, err := m.existingMigrations(ctx, m.driver.UnwrapExecutor(tx))
	if err != nil {
		return nil, err
	}

	versions, err := m.versionsFromDriver(migrations)
	if err != nil {
		return nil, err
	}

	return versions, nil
}

func (m *Migrator[TTx]) versionsFromDriver(migrations []*riverdriver.Migration) ([]Migration, error) {
	versions := make([]Migration, len(migrations))
	for i, existingMigration := range migrations {
		migration, ok := m.migrations[existingMigration.Version]
		if !ok {
			return nil, fmt.Errorf("migration %d not found in migrator bundle", existingMigration.Version)
		}
		versions[i] = migration
	}
	return versions, nil
}

// MigrateOpts are options for a migrate operation.
type MigrateOpts struct {
	DryRun bool

	// MaxSteps is the maximum number of migrations to apply either up or down.
	// When migrating in the up direction, migrates an unlimited number of steps
	// by default. When migrating in the down direction, migrates only a single
	// step by default (set TargetVersion to -1 to apply unlimited steps down).
	// Set to -1 to apply no migrations (for testing/checking purposes).
	MaxSteps int

	// TargetVersion is a specific migration version to apply migrations to. The
	// version must exist and it must be in the possible list of migrations to
	// apply. e.g. If requesting an up migration with version 3, version 3 must
	// not already be applied.
	//
	// When applying migrations up, migrations are applied including the target
	// version, so when starting at version 0 and requesting version 3, versions
	// 1, 2, and 3 would be applied. When applying migrations down, down
	// migrations are applied excluding the target version, so when starting at
	// version 5 and requesting version 3, down migrations for versions 5 and 4
	// would be applied, leaving the final schema at version 3.
	//
	// When migrating down, TargetVersion can be set to the special value of -1
	// to apply all down migrations (i.e. River schema is removed completely).
	TargetVersion int
}

// MigrateResult is the result of a migrate operation.
type MigrateResult struct {
	// Direction is the direction that migration occurred (up or down).
	Direction Direction

	// Versions are migration versions that were added (for up migrations) or
	// removed (for down migrations) for this run.
	Versions []MigrateVersion
}

// MigrateVersion is the result for a single applied migration.
type MigrateVersion struct {
	// Duration is the amount of time it took to apply the migration.
	Duration time.Duration

	// Name is a human-friendly name for the migration derived from its
	// filename.
	Name string

	// SQL is the SQL that was applied along with the migration.
	SQL string

	// Version is the version of the migration applied.
	Version int
}

func migrateVersionToInt(version MigrateVersion) int { return version.Version }

type Direction string

const (
	DirectionDown Direction = "down"
	DirectionUp   Direction = "up"
)

// AllVersions gets information on all known migration versions.
func (m *Migrator[TTx]) AllVersions() []Migration {
	migrations := maputil.Values(m.migrations)
	slices.SortFunc(migrations, func(v1, v2 Migration) int { return v1.Version - v2.Version })
	return migrations
}

// GetVersion gets information about a specific migration version. An error is
// returned if a versions is requested that doesn't exist.
func (m *Migrator[TTx]) GetVersion(version int) (Migration, error) {
	migration, ok := m.migrations[version]
	if !ok {
		availableVersions := maputil.Keys(m.migrations)
		slices.Sort(availableVersions)
		return Migration{}, fmt.Errorf("migration %d not found (available versions: %v)", version, availableVersions)
	}

	return migration, nil
}

// Migrate migrates the database in the given direction (up or down). The opts
// parameter may be omitted for convenience.
//
// By default, applies all outstanding migrations when moving in the up
// direction, but for safety, only one step when moving in the down direction.
// To migrate more than one step down, MigrateOpts.MaxSteps or
// MigrateOpts.TargetVersion are available. Setting MigrateOpts.TargetVersion to
// -1 will apply every available downstep so that River's schema is removed
// completely.
//
//	res, err := migrator.Migrate(ctx, rivermigrate.DirectionUp, nil)
//	if err != nil {
//		// handle error
//	}
func (m *Migrator[TTx]) Migrate(ctx context.Context, direction Direction, opts *MigrateOpts) (*MigrateResult, error) {
	exec := m.driver.GetExecutor()
	switch direction {
	case DirectionDown:
		return m.migrateDown(ctx, exec, direction, opts)
	case DirectionUp:
		return m.migrateUp(ctx, exec, direction, opts)
	}

	panic("invalid direction: " + direction)
}

// Migrate migrates the database in the given direction (up or down). The opts
// parameter may be omitted for convenience.
//
// By default, applies all outstanding migrations when moving in the up
// direction, but for safety, only one step when moving in the down direction.
// To migrate more than one step down, MigrateOpts.MaxSteps or
// MigrateOpts.TargetVersion are available. Setting MigrateOpts.TargetVersion to
// -1 will apply every available downstep so that River's schema is removed
// completely.
//
//	res, err := migrator.MigrateTx(ctx, tx, rivermigrate.DirectionUp, nil)
//	if err != nil {
//		// handle error
//	}
//
// This variant lets a caller run migrations within a transaction. Postgres DDL
// is transactional, so migration changes aren't visible until the transaction
// commits, and are rolled back if the transaction rolls back.
//
// Deprecated: Use Migrate instead. Certain migrations cannot be batched together
// in a single transaction, so this method is not recommended.
func (m *Migrator[TTx]) MigrateTx(ctx context.Context, tx TTx, direction Direction, opts *MigrateOpts) (*MigrateResult, error) {
	switch direction {
	case DirectionDown:
		return m.migrateDown(ctx, m.driver.UnwrapExecutor(tx), direction, opts)
	case DirectionUp:
		return m.migrateUp(ctx, m.driver.UnwrapExecutor(tx), direction, opts)
	}

	panic("invalid direction: " + direction)
}

// ValidateResult is the result of a validation operation.
type ValidateResult struct {
	// Messages contain informational messages of what wasn't valid in case of a
	// failed validation. Always empty if OK is true.
	Messages []string

	// OK is true if validation completed with no problems.
	OK bool
}

// Validate validates the current state of migrations, returning an unsuccessful
// validation and usable message in case there are migrations that haven't yet
// been applied.
func (m *Migrator[TTx]) Validate(ctx context.Context) (*ValidateResult, error) {
	return dbutil.WithTxV(ctx, m.driver.GetExecutor(), func(ctx context.Context, tx riverdriver.ExecutorTx) (*ValidateResult, error) {
		return m.validate(ctx, tx)
	})
}

// Validate validates the current state of migrations, returning an unsuccessful
// validation and usable message in case there are migrations that haven't yet
// been applied.
//
// This variant lets a caller validate within a transaction.
func (m *Migrator[TTx]) ValidateTx(ctx context.Context, tx TTx) (*ValidateResult, error) {
	return m.validate(ctx, m.driver.UnwrapExecutor(tx))
}

// migrateDown runs down migrations.
func (m *Migrator[TTx]) migrateDown(ctx context.Context, exec riverdriver.Executor, direction Direction, opts *MigrateOpts) (*MigrateResult, error) {
	existingMigrations, err := m.existingMigrations(ctx, exec)
	if err != nil {
		return nil, err
	}
	existingMigrationsMap := sliceutil.KeyBy(existingMigrations,
		func(m *riverdriver.Migration) (int, struct{}) { return m.Version, struct{}{} })

	targetMigrations := maps.Clone(m.migrations)
	for version := range targetMigrations {
		if _, ok := existingMigrationsMap[version]; !ok {
			delete(targetMigrations, version)
		}
	}

	sortedTargetMigrations := maputil.Values(targetMigrations)
	slices.SortFunc(sortedTargetMigrations, func(a, b Migration) int { return b.Version - a.Version }) // reverse order

	res, err := m.applyMigrations(ctx, exec, direction, opts, sortedTargetMigrations)
	if err != nil {
		return nil, err
	}

	// If we did no work, leave early. This allows a zero-migrated database
	// that's being no-op downmigrated again to succeed because otherwise
	// the delete below would cause it to error.
	if len(res.Versions) < 1 {
		return res, nil
	}

	// Migration version 1 is special-cased because if it was downmigrated
	// it means the `river_migration` table is no longer present so there's
	// nothing to delete out of.
	if slices.ContainsFunc(res.Versions, func(v MigrateVersion) bool { return v.Version == 1 }) {
		return res, nil
	}

	if !opts.DryRun && len(res.Versions) > 0 {
		versions := sliceutil.Map(res.Versions, migrateVersionToInt)

		// Version 005 is hard-coded here because that's the version in which
		// the migration `line` comes in. If migration to a point equal or above
		// 005, we can remove migrations with a line included, but otherwise we
		// must omit the `line` column from queries because it doesn't exist.
		if m.line == riverdriver.MigrationLineMain && slices.Min(versions) <= migrateVersionLineColumnAdded {
			if _, err := exec.MigrationDeleteAssumingMainMany(ctx, versions); err != nil {
				return nil, fmt.Errorf("error inserting migration rows for versions %+v assuming main: %w", res.Versions, err)
			}
		} else {
			if _, err := exec.MigrationDeleteByLineAndVersionMany(ctx, m.line, versions); err != nil {
				return nil, fmt.Errorf("error deleting migration rows for versions %+v on line %q: %w", res.Versions, m.line, err)
			}
		}
	}

	return res, nil
}

// migrateUp runs up migrations.
func (m *Migrator[TTx]) migrateUp(ctx context.Context, exec riverdriver.Executor, direction Direction, opts *MigrateOpts) (*MigrateResult, error) {
	existingMigrations, err := m.existingMigrations(ctx, exec)
	if err != nil {
		return nil, err
	}

	targetMigrations := maps.Clone(m.migrations)
	for _, migrateRow := range existingMigrations {
		delete(targetMigrations, migrateRow.Version)
	}

	sortedTargetMigrations := maputil.Values(targetMigrations)
	slices.SortFunc(sortedTargetMigrations, func(a, b Migration) int { return a.Version - b.Version })

	res, err := m.applyMigrations(ctx, exec, direction, opts, sortedTargetMigrations)
	if err != nil {
		return nil, err
	}

	if (opts == nil || !opts.DryRun) && len(res.Versions) > 0 {
		versions := sliceutil.Map(res.Versions, migrateVersionToInt)

		// Version 005 is hard-coded here because that's the version in which
		// the migration `line` comes in. If migration to a point equal or above
		// 005, we can insert migrations with a line included, but otherwise we
		// must omit the `line` column from queries because it doesn't exist.
		if m.line == riverdriver.MigrationLineMain && slices.Max(versions) < migrateVersionLineColumnAdded {
			if _, err := exec.MigrationInsertManyAssumingMain(ctx, versions); err != nil {
				return nil, fmt.Errorf("error inserting migration rows for versions %+v assuming main: %w", res.Versions, err)
			}
		} else {
			if _, err := exec.MigrationInsertMany(ctx,
				m.line,
				versions,
			); err != nil {
				return nil, fmt.Errorf("error inserting migration rows for versions %+v on line %q: %w", res.Versions, m.line, err)
			}
		}
	}

	return res, nil
}

// validate validates current migration state.
func (m *Migrator[TTx]) validate(ctx context.Context, exec riverdriver.Executor) (*ValidateResult, error) {
	existingMigrations, err := m.existingMigrations(ctx, exec)
	if err != nil {
		return nil, err
	}

	targetMigrations := maps.Clone(m.migrations)
	for _, migrateRow := range existingMigrations {
		delete(targetMigrations, migrateRow.Version)
	}

	notOKWithMessage := func(message string) *ValidateResult {
		m.Logger.InfoContext(ctx, m.Name+": "+message)
		return &ValidateResult{Messages: []string{message}}
	}

	if len(targetMigrations) > 0 {
		sortedTargetMigrations := maputil.Keys(targetMigrations)
		slices.Sort(sortedTargetMigrations)

		return notOKWithMessage(fmt.Sprintf("Unapplied migrations: %v", sortedTargetMigrations)), nil
	}

	return &ValidateResult{OK: true}, nil
}

// Common code shared between the up and down migration directions that walks
// through each target migration and applies it, logging appropriately.
func (m *Migrator[TTx]) applyMigrations(ctx context.Context, exec riverdriver.Executor, direction Direction, opts *MigrateOpts, sortedTargetMigrations []Migration) (*MigrateResult, error) {
	if opts == nil {
		opts = &MigrateOpts{}
	}

	var maxSteps int
	switch {
	case opts.MaxSteps != 0:
		maxSteps = opts.MaxSteps
	case direction == DirectionDown && opts.TargetVersion == 0:
		maxSteps = 1
	}

	switch {
	case maxSteps < 0:
		sortedTargetMigrations = []Migration{}
	case maxSteps > 0:
		sortedTargetMigrations = sortedTargetMigrations[0:min(maxSteps, len(sortedTargetMigrations))]
	}

	if opts.TargetVersion > 0 {
		if _, ok := m.migrations[opts.TargetVersion]; !ok {
			return nil, fmt.Errorf("version %d is not a valid River migration version", opts.TargetVersion)
		}

		targetIndex := slices.IndexFunc(sortedTargetMigrations, func(b Migration) bool { return b.Version == opts.TargetVersion })
		if targetIndex == -1 {
			return nil, fmt.Errorf("version %d is not in target list of valid migrations to apply", opts.TargetVersion)
		}

		// Replace target list with list up to target index. Migrations are
		// sorted according to the direction we're migrating in, so when down
		// migration, the list is already reversed, so this will truncate it so
		// it's the most current migration down to the target.
		sortedTargetMigrations = sortedTargetMigrations[0 : targetIndex+1]

		if direction == DirectionDown && len(sortedTargetMigrations) > 0 {
			sortedTargetMigrations = sortedTargetMigrations[0 : len(sortedTargetMigrations)-1]
		}
	}

	res := &MigrateResult{Direction: direction, Versions: make([]MigrateVersion, 0, len(sortedTargetMigrations))}

	// Short circuit early if there's nothing to do.
	if len(sortedTargetMigrations) < 1 {
		m.Logger.InfoContext(ctx, m.Name+": No migrations to apply")
		return res, nil
	}

	for _, versionBundle := range sortedTargetMigrations {
		var sql string
		switch direction {
		case DirectionDown:
			sql = versionBundle.SQLDown
		case DirectionUp:
			sql = versionBundle.SQLUp
		}

		var duration time.Duration

		if !opts.DryRun {
			start := time.Now()

			// Similar to ActiveRecord migrations, we wrap each individual migration
			// in its own transaction.  Without this, certain migrations that require
			// a commit on a preexisting operation (such as adding an enum value to be
			// used in an immutable function) cannot succeed.
			err := dbutil.WithTx(ctx, exec, func(ctx context.Context, exec riverdriver.ExecutorTx) error {
				_, err := exec.Exec(ctx, sql)
				if err != nil {
					return fmt.Errorf("error applying version %03d [%s]: %w",
						versionBundle.Version, strings.ToUpper(string(direction)), err)
				}
				return nil
			})
			if err != nil {
				return nil, err
			}
			duration = time.Since(start)
		}

		m.Logger.InfoContext(ctx, m.Name+": Applied migration",
			slog.String("direction", string(direction)),
			slog.Bool("dry_run", opts.DryRun),
			slog.Duration("duration", duration),
			slog.Int("version", versionBundle.Version),
		)

		res.Versions = append(res.Versions, MigrateVersion{Duration: duration, Name: versionBundle.Name, SQL: sql, Version: versionBundle.Version})
	}

	return res, nil
}

// Get existing migrations that've already been run in the database. This is
// encapsulated to run a check in a subtransaction and the handle the case of
// the `river_migration` table not existing yet. (The subtransaction is needed
// because otherwise the existing transaction would become aborted on an
// unsuccessful `river_migration` check.)
func (m *Migrator[TTx]) existingMigrations(ctx context.Context, exec riverdriver.Executor) ([]*riverdriver.Migration, error) {
	migrateTableExists, err := exec.TableExists(ctx, "river_migration")
	if err != nil {
		return nil, fmt.Errorf("error checking if `%s` exists: %w", "river_migration", err)
	}
	if !migrateTableExists {
		if m.line != riverdriver.MigrationLineMain {
			return nil, errors.New("can't add a non-main migration line until `river_migration` is raised; fully migrate the main migration line and try again")
		}

		return nil, nil
	}

	lineColumnExists, err := exec.ColumnExists(ctx, "river_migration", "line")
	if err != nil {
		return nil, fmt.Errorf("error checking if `%s.%s` exists: %w", "river_migration", "line", err)
	}

	if !lineColumnExists {
		if m.line != riverdriver.MigrationLineMain {
			return nil, errors.New("can't add a non-main migration line until `river_migration.line` is raised; fully migrate the main migration line and try again")
		}

		migrations, err := exec.MigrationGetAllAssumingMain(ctx)
		if err != nil {
			return nil, fmt.Errorf("error getting existing migrations: %w", err)
		}

		return migrations, nil
	}

	migrations, err := exec.MigrationGetByLine(ctx, m.line)
	if err != nil {
		return nil, fmt.Errorf("error getting existing migrations for line %q: %w", m.line, err)
	}

	return migrations, nil
}

// Reads a series of migration bundles from a file system, which practically
// speaking will always be the embedded FS read from the contents of the
// `migration/<line>/` subdirectory.
func migrationsFromFS(migrationFS fs.FS, line string) ([]Migration, error) {
	const subdir = "migration"

	var (
		lastBundle *Migration
		migrations []Migration
	)

	err := fs.WalkDir(migrationFS, subdir, func(path string, entry fs.DirEntry, err error) error {
		if err != nil {
			return fmt.Errorf("error walking FS: %w", err)
		}

		// The WalkDir callback is invoked for each embedded subdirectory and
		// file. For our purposes here, we're only interested in files.
		if entry.IsDir() {
			return nil
		}

		filename := path

		// Invoked with the full path name. Strip `migration/` from the front so
		// we have a name that we can parse with.
		if !strings.HasPrefix(filename, subdir) {
			return fmt.Errorf("expected path %q to start with subdir %q", path, subdir)
		}
		filename = filename[len(subdir)+1:]

		// Ignore any migrations that don't belong to the line we're reading.
		if !strings.HasPrefix(filename, line) {
			return nil
		}
		filename = filename[len(line)+1:]

		versionStr, name, ok := strings.Cut(filename, "_")
		if !ok {
			return fmt.Errorf("expected name to start with version string like '001_': %q", filename)
		}

		version, err := strconv.Atoi(versionStr)
		if err != nil {
			return fmt.Errorf("error parsing version %q: %w", versionStr, err)
		}

		// Non-version name for the migration. So for `002_initial_schema` it
		// would be `initial schema`.
		name, _, _ = strings.Cut(name, ".")
		name = strings.ReplaceAll(name, "_", " ")

		// This works because `fs.WalkDir` guarantees lexical order, so all 001*
		// files always appear before all 002* files, etc.
		if lastBundle == nil || lastBundle.Version != version {
			migrations = append(migrations, Migration{Name: name, Version: version})
			lastBundle = &migrations[len(migrations)-1]
		}

		file, err := migrationFS.Open(path)
		if err != nil {
			return fmt.Errorf("error opening file %q: %w", path, err)
		}

		contents, err := io.ReadAll(file)
		if err != nil {
			return fmt.Errorf("error reading file %q: %w", path, err)
		}

		switch {
		case strings.HasSuffix(filename, ".down.sql"):
			lastBundle.SQLDown = string(contents)
		case strings.HasSuffix(filename, ".up.sql"):
			lastBundle.SQLUp = string(contents)
		default:
			return fmt.Errorf("file %q should end with either '.down.sql' or '.up.sql'", filename)
		}

		return nil
	})
	if err != nil {
		return nil, err
	}

	if len(migrations) < 1 {
		return nil, fmt.Errorf("no migrations found for line: %q", line)
	}

	return migrations, nil
}

// Validates and fully initializes a set of migrations to reduce the probability
// of configuration problems as new migrations are introduced. e.g. Checks for
// missing fields or accidentally duplicated version numbers from copy/pasta
// problems.
func validateAndInit(versions []Migration) map[int]Migration {
	lastVersion := 0
	migrations := make(map[int]Migration, len(versions))

	for _, versionBundle := range versions {
		if versionBundle.SQLDown == "" {
			panic(fmt.Sprintf("version bundle should specify Down: %+v", versionBundle))
		}
		if versionBundle.SQLUp == "" {
			panic(fmt.Sprintf("version bundle should specify Up: %+v", versionBundle))
		}
		if versionBundle.Version == 0 {
			panic(fmt.Sprintf("version bundle should specify Version: %+v", versionBundle))
		}

		if _, ok := migrations[versionBundle.Version]; ok {
			panic(fmt.Sprintf("duplicate version: %03d", versionBundle.Version))
		}
		if versionBundle.Version <= lastVersion {
			panic(fmt.Sprintf("versions should be ascending; current: %03d, last: %03d", versionBundle.Version, lastVersion))
		}
		if versionBundle.Version > lastVersion+1 {
			panic(fmt.Sprintf("versions shouldn't skip a sequence number; current: %03d, last: %03d", versionBundle.Version, lastVersion))
		}

		lastVersion = versionBundle.Version
		migrations[versionBundle.Version] = versionBundle
	}

	return migrations
}

```

`rivermigrate/river_migrate_test.go`:

```go
package rivermigrate

import (
	"context"
	"database/sql"
	"embed"
	"fmt"
	"io/fs"
	"log/slog"
	"os"
	"slices"
	"testing"

	"github.com/jackc/pgerrcode"
	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgconn"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/jackc/pgx/v5/stdlib"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/internal/util/dbutil"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverdatabasesql"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/util/randutil"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
)

const (
	// The name of an actual migration line embedded in our test data below.
	migrationLineAlternate                = "alternate"
	migrationLineAlternateMaxVersion      = 6
	migrationLineCommitRequired           = "commit_required"
	migrationLineCommitRequiredMaxVersion = 3
)

//go:embed migration/*/*.sql
var migrationFS embed.FS

// A test driver with the same migrations as the standard Pgx driver, but which
// includes an alternate line so we can test that those work.
type driverWithAlternateLine struct {
	*riverpgxv5.Driver
}

func (d *driverWithAlternateLine) GetMigrationFS(line string) fs.FS {
	switch line {
	case riverdriver.MigrationLineMain:
		return d.Driver.GetMigrationFS(line)
	case migrationLineAlternate:
		return migrationFS
	case migrationLineAlternate + "2":
		panic(line + " is only meant for testing line suggestions")
	case migrationLineCommitRequired:
		return migrationFS
	}
	panic("migration line does not exist: " + line)
}

func (d *driverWithAlternateLine) GetMigrationLines() []string {
	return append(d.Driver.GetMigrationLines(), migrationLineAlternate, migrationLineAlternate+"2", migrationLineCommitRequired)
}

func TestMigrator(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	migrationsBundle := buildTestMigrationsBundle(t)

	type testBundle struct {
		dbPool *pgxpool.Pool
		driver *driverWithAlternateLine
		logger *slog.Logger
	}

	setup := func(t *testing.T) (*Migrator[pgx.Tx], *testBundle) {
		t.Helper()

		// Not all migrations can be executed together in a single transaction.
		// Examples include `CREATE INDEX CONCURRENTLY`, or adding an enum value
		// that's used by a later migration. As such, the migrator and its tests
		// must use a full database with commits between each migration.
		//
		// To make this easier to clean up afterward, we create a new, clean schema
		// for each test run and then drop it afterward.
		baseDBPool := riverinternaltest.TestDB(ctx, t)
		schemaName := "river_migrate_test_" + randutil.Hex(8)
		_, err := baseDBPool.Exec(ctx, "CREATE SCHEMA "+schemaName)
		require.NoError(t, err)

		t.Cleanup(func() {
			_, err := baseDBPool.Exec(ctx, fmt.Sprintf("DROP SCHEMA %s CASCADE", schemaName))
			require.NoError(t, err)
		})

		newSchemaConfig := baseDBPool.Config()
		newSchemaConfig.ConnConfig.RuntimeParams["search_path"] = schemaName
		newSchemaPool, err := pgxpool.NewWithConfig(ctx, newSchemaConfig)
		require.NoError(t, err)

		bundle := &testBundle{
			dbPool: newSchemaPool,
			driver: &driverWithAlternateLine{Driver: riverpgxv5.New(newSchemaPool)},
			logger: riversharedtest.Logger(t),
		}

		migrator, err := New(bundle.driver, &Config{Logger: bundle.logger})
		require.NoError(t, err)
		migrator.migrations = migrationsBundle.WithTestVersionsMap

		return migrator, bundle
	}

	// Gets a migrator using the driver for `database/sql`.
	setupDatabaseSQLMigrator := func(t *testing.T, bundle *testBundle) (*Migrator[*sql.Tx], *sql.Tx) {
		t.Helper()

		stdPool := stdlib.OpenDBFromPool(bundle.dbPool)
		t.Cleanup(func() { require.NoError(t, stdPool.Close()) })

		tx, err := stdPool.BeginTx(ctx, nil)
		require.NoError(t, err)
		t.Cleanup(func() { require.NoError(t, tx.Rollback()) })

		driver := riverdatabasesql.New(stdPool)
		migrator, err := New(driver, &Config{Logger: bundle.logger})
		require.NoError(t, err)
		migrator.migrations = migrationsBundle.WithTestVersionsMap

		return migrator, tx
	}

	t.Run("NewUnknownLine", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		_, err := New(bundle.driver, &Config{Line: "unknown_line"})
		require.EqualError(t, err, "migration line does not exist: unknown_line")

		_, err = New(bundle.driver, &Config{Line: "mai"})
		require.EqualError(t, err, "migration line does not exist: mai (did you mean `main`?)")

		_, err = New(bundle.driver, &Config{Line: "maim"})
		require.EqualError(t, err, "migration line does not exist: maim (did you mean `main`?)")

		_, err = New(bundle.driver, &Config{Line: "maine"})
		require.EqualError(t, err, "migration line does not exist: maine (did you mean `main`?)")

		_, err = New(bundle.driver, &Config{Line: "ma"})
		require.EqualError(t, err, "migration line does not exist: ma (did you mean `main`?)")

		// Too far off.
		_, err = New(bundle.driver, &Config{Line: "m"})
		require.EqualError(t, err, "migration line does not exist: m")

		_, err = New(bundle.driver, &Config{Line: "alternat"})
		require.EqualError(t, err, "migration line does not exist: alternat (did you mean one of `alternate`, `alternate2`?)")
	})

	t.Run("AllVersions", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		migrations := migrator.AllVersions()
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion), sliceutil.Map(migrations, migrationToInt))
	})

	t.Run("ExistingMigrationsDefault", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{TargetVersion: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		migrations, err := migrator.ExistingVersions(ctx)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion), sliceutil.Map(migrations, migrationToInt))
	})

	t.Run("ExistingMigrationsEmpty", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		migrations, err := migrator.ExistingVersions(ctx)
		require.NoError(t, err)
		require.Equal(t, []int{}, sliceutil.Map(migrations, migrationToInt))
	})

	t.Run("ExistingMigrationsTxDefaultLine", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{TargetVersion: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		tx, err := bundle.dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { require.NoError(t, tx.Rollback(ctx)) })

		migrations, err := migrator.ExistingVersionsTx(ctx, tx)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion), sliceutil.Map(migrations, migrationToInt))
	})

	t.Run("ExistingMigrationsTxEmpty", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		tx, err := bundle.dbPool.Begin(ctx)
		require.NoError(t, err)
		t.Cleanup(func() { require.NoError(t, tx.Rollback(ctx)) })

		migrations, err := migrator.ExistingVersionsTx(ctx, tx)
		require.NoError(t, err)
		require.Equal(t, []int{}, sliceutil.Map(migrations, migrationToInt))
	})

	t.Run("ExistingMigrationsTxFullyMigrated", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		migrations, err := migrator.ExistingVersions(ctx)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion), sliceutil.Map(migrations, migrationToInt))
	})

	t.Run("MigrateDownDefault", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		// The migration version in which `river_job` comes in.
		const migrateVersionIncludingRiverJob = 2

		// Run two initial times to get to the version before river_job is dropped.
		// Defaults to only running one step when moving in the down direction.
		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrateVersionIncludingRiverJob})
		require.NoError(t, err)
		require.Equal(t, DirectionUp, res.Direction)
		require.Equal(t, seqOneTo(migrateVersionIncludingRiverJob), sliceutil.Map(res.Versions, migrateVersionToInt))

		err = dbExecError(ctx, bundle.driver.GetExecutor(), "SELECT * FROM river_job")
		require.NoError(t, err)

		// Run once more to go down one more step
		{
			res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{})
			require.NoError(t, err)
			require.Equal(t, DirectionDown, res.Direction)
			require.Equal(t, []int{migrateVersionIncludingRiverJob}, sliceutil.Map(res.Versions, migrateVersionToInt))

			version := res.Versions[0]
			require.Equal(t, "initial schema", version.Name)

			err = dbExecError(ctx, bundle.driver.GetExecutor(), "SELECT * FROM river_job")
			require.Error(t, err)
		}
	})

	t.Run("MigrateDownAfterUp", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{})
		require.NoError(t, err)
		require.Equal(t, []int{migrationsBundle.WithTestVersionsMaxVersion}, sliceutil.Map(res.Versions, migrateVersionToInt))
	})

	t.Run("MigrateDownWithMaxSteps", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{MaxSteps: 2})
		require.NoError(t, err)
		require.Equal(t, []int{migrationsBundle.WithTestVersionsMaxVersion, migrationsBundle.WithTestVersionsMaxVersion - 1},
			sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion-2),
			sliceutil.Map(migrations, driverMigrationToInt))

		err = dbExecError(ctx, bundle.driver.GetExecutor(), "SELECT name FROM test_table")
		require.Error(t, err)
	})

	t.Run("MigrateDownWithPool", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		// We don't actually migrate anything (max steps = -1) because doing so
		// would mess with the test database, but this still runs most code to
		// check that the function generally works.
		res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{MaxSteps: -1})
		require.NoError(t, err)
		require.Equal(t, []int{}, sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("MigrateDownWithDatabaseSQLDriver", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)
		migrator, tx := setupDatabaseSQLMigrator(t, bundle)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{MaxSteps: 1})
		require.NoError(t, err)
		require.Equal(t, []int{migrationsBundle.MaxVersion}, sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := migrator.driver.UnwrapExecutor(tx).MigrationGetAllAssumingMain(ctx)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion-1),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("MigrateDownWithTargetVersion", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{TargetVersion: 4})
		require.NoError(t, err)
		require.Equal(t, seqDownTo(migrationsBundle.WithTestVersionsMaxVersion, 5),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetAllAssumingMain(ctx)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(4),
			sliceutil.Map(migrations, driverMigrationToInt))

		err = dbExecError(ctx, bundle.driver.GetExecutor(), "SELECT name FROM test_table")
		require.Error(t, err)
	})

	t.Run("MigrateDownWithTargetVersionMinusOne", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{TargetVersion: -1})
		require.NoError(t, err)
		require.Equal(t, seqDownTo(migrationsBundle.WithTestVersionsMaxVersion, 1),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		err = dbExecError(ctx, bundle.driver.GetExecutor(), "SELECT name FROM river_migrate")
		require.Error(t, err)
	})

	t.Run("MigrateDownWithTargetVersionInvalid", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		// migration doesn't exist
		{
			_, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{TargetVersion: migrationsBundle.MaxVersion + 77})
			require.EqualError(t, err, fmt.Sprintf("version %d is not a valid River migration version", migrationsBundle.MaxVersion+77))
		}

		// migration exists but not one that's applied
		{
			_, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{TargetVersion: migrationsBundle.MaxVersion + 1})
			require.EqualError(t, err, fmt.Sprintf("version %d is not in target list of valid migrations to apply", migrationsBundle.MaxVersion+1))
		}
	})

	t.Run("MigrateDownDryRun", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionDown, &MigrateOpts{DryRun: true})
		require.NoError(t, err)
		require.Equal(t, []int{migrationsBundle.WithTestVersionsMaxVersion}, sliceutil.Map(res.Versions, migrateVersionToInt))

		// Migrate down returned a result above for a migration that was
		// removed, but because we're in a dry run, the database still shows
		// this version.
		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("GetVersion", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		{
			migrateVersion, err := migrator.GetVersion(migrationsBundle.WithTestVersionsMaxVersion)
			require.NoError(t, err)
			require.Equal(t, migrationsBundle.WithTestVersionsMaxVersion, migrateVersion.Version)
		}

		{
			_, err := migrator.GetVersion(99_999)
			availableVersions := seqOneTo(migrationsBundle.WithTestVersionsMaxVersion)
			require.EqualError(t, err, fmt.Sprintf("migration %d not found (available versions: %v)", 99_999, availableVersions))
		}
	})

	t.Run("MigrateNilOpts", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionUp, nil)
		require.NoError(t, err)
		require.Equal(t, []int{migrationsBundle.MaxVersion + 1, migrationsBundle.MaxVersion + 2}, sliceutil.Map(res.Versions, migrateVersionToInt))
	})

	t.Run("MigrateUpDefault", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		// Run an initial time
		{
			res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
			require.NoError(t, err)
			require.Equal(t, DirectionUp, res.Direction)
			require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion),
				sliceutil.Map(res.Versions, migrateVersionToInt))

			migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
			require.NoError(t, err)
			require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion),
				sliceutil.Map(migrations, driverMigrationToInt))

			_, err = bundle.dbPool.Exec(ctx, "SELECT * FROM test_table")
			require.NoError(t, err)
		}

		// Run once more to verify idempotency
		{
			res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
			require.NoError(t, err)
			require.Equal(t, DirectionUp, res.Direction)
			require.Equal(t, []int{}, sliceutil.Map(res.Versions, migrateVersionToInt))

			migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
			require.NoError(t, err)
			require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion),
				sliceutil.Map(migrations, driverMigrationToInt))

			_, err = bundle.dbPool.Exec(ctx, "SELECT * FROM test_table")
			require.NoError(t, err)
		}
	})

	t.Run("MigrateUpWithMaxSteps", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: 1})
		require.NoError(t, err)
		require.Equal(t, []int{migrationsBundle.WithTestVersionsMaxVersion - 1},
			sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion-1),
			sliceutil.Map(migrations, driverMigrationToInt))

		// Column `name` is only added in the second test version.
		err = dbExecError(ctx, bundle.driver.GetExecutor(), "SELECT name FROM test_table")
		require.Error(t, err)

		var pgErr *pgconn.PgError
		require.ErrorAs(t, err, &pgErr)
		require.Equal(t, pgerrcode.UndefinedColumn, pgErr.Code)
	})

	t.Run("MigrateUpWithPool", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion), sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("MigrateUpWithDatabaseSQLDriver", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)
		migrator, tx := setupDatabaseSQLMigrator(t, bundle)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: 1})
		require.NoError(t, err)
		require.Equal(t, []int{migrationsBundle.MaxVersion + 1}, sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := migrator.driver.UnwrapExecutor(tx).MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion+1),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("MigrateUpWithTargetVersion", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{TargetVersion: migrationsBundle.MaxVersion + 2})
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion+2), sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("MigrateUpWithTargetVersionInvalid", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		// migration doesn't exist
		{
			_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{TargetVersion: 77})
			require.EqualError(t, err, "version 77 is not a valid River migration version")
		}

		// migration exists but already applied
		{
			_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{TargetVersion: 3})
			require.EqualError(t, err, "version 3 is not in target list of valid migrations to apply")
		}
	})

	t.Run("MigrateUpDryRun", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{DryRun: true})
		require.NoError(t, err)
		require.Equal(t, DirectionUp, res.Direction)
		require.Equal(t, []int{migrationsBundle.WithTestVersionsMaxVersion - 1, migrationsBundle.WithTestVersionsMaxVersion},
			sliceutil.Map(res.Versions, migrateVersionToInt))

		// Migrate up returned a result above for migrations that were applied,
		// but because we're in a dry run, the database still shows the test
		// migration versions not applied.
		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("ValidateSuccess", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		// Migrate all the way up.
		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		res, err := migrator.Validate(ctx)
		require.NoError(t, err)
		require.Equal(t, &ValidateResult{OK: true}, res)
	})

	t.Run("ValidateUnappliedMigrations", func(t *testing.T) {
		t.Parallel()

		migrator, _ := setup(t)

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		res, err := migrator.Validate(ctx)
		require.NoError(t, err)
		require.Equal(t, &ValidateResult{
			Messages: []string{fmt.Sprintf("Unapplied migrations: [%d %d]", migrationsBundle.MaxVersion+1, migrationsBundle.MaxVersion+2)},
		}, res)
	})

	t.Run("MigrateUpThenDownToZeroAndBackUp", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		requireMigrationTableExists := func(expectedExists bool) {
			migrationExists, err := bundle.driver.GetExecutor().TableExists(ctx, "river_migration")
			require.NoError(t, err)
			require.Equal(t, expectedExists, migrationExists)
		}

		// We start off with a clean schema so it has no tables:
		requireMigrationTableExists(false)

		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{TargetVersion: migrationsBundle.MaxVersion})
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		requireMigrationTableExists(true)

		res, err = migrator.Migrate(ctx, DirectionDown, &MigrateOpts{TargetVersion: -1})
		require.NoError(t, err)
		require.Equal(t, seqDownTo(migrationsBundle.MaxVersion, 1),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		requireMigrationTableExists(false)

		res, err = migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.WithTestVersionsMaxVersion),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("AlternateLineUpAndDown", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		// Run the main migration line all the way up.
		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: migrationsBundle.MaxVersion})
		require.NoError(t, err)

		// We have to reinitialize the alternateMigrator because the migrations bundle is
		// set in the constructor.
		alternateMigrator, err := New(bundle.driver, &Config{
			Line:   migrationLineAlternate,
			Logger: bundle.logger,
		})
		require.NoError(t, err)

		res, err := alternateMigrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationLineAlternateMaxVersion),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		migrations, err := bundle.driver.GetExecutor().MigrationGetByLine(ctx, migrationLineAlternate)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationLineAlternateMaxVersion),
			sliceutil.Map(migrations, driverMigrationToInt))

		res, err = alternateMigrator.Migrate(ctx, DirectionDown, &MigrateOpts{TargetVersion: -1})
		require.NoError(t, err)
		require.Equal(t, seqDownTo(migrationLineAlternateMaxVersion, 1),
			sliceutil.Map(res.Versions, migrateVersionToInt))

		// The main migration line should not have been touched.
		migrations, err = bundle.driver.GetExecutor().MigrationGetByLine(ctx, riverdriver.MigrationLineMain)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationsBundle.MaxVersion),
			sliceutil.Map(migrations, driverMigrationToInt))
	})

	t.Run("AlternateLineBeforeLineColumn", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		// Main line to just before the `line` column was added.
		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{TargetVersion: 4})
		require.NoError(t, err)

		alternateMigrator, err := New(bundle.driver, &Config{
			Line:   migrationLineAlternate,
			Logger: bundle.logger,
		})
		require.NoError(t, err)

		// Alternate line not allowed because `river_job.line` doesn't exist.
		_, err = alternateMigrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.EqualError(t, err, "can't add a non-main migration line until `river_migration.line` is raised; fully migrate the main migration line and try again")

		// Main line to zero.
		_, err = migrator.Migrate(ctx, DirectionDown, &MigrateOpts{TargetVersion: -1})
		require.NoError(t, err)

		// Alternate line not allowed because `river_job` doesn't exist.
		_, err = alternateMigrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.EqualError(t, err, "can't add a non-main migration line until `river_migration` is raised; fully migrate the main migration line and try again")
	})

	// Demonstrates that even when not using River's internal migration system,
	// version 005 is still able to run.
	//
	// This is special because it's the first time the table's changed since
	// version 001.
	t.Run("Version005ToleratesRiverMigrateNotPresent", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		// Migrate down to version 004.
		res, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{MaxSteps: 4})
		require.NoError(t, err)
		require.Equal(t, DirectionUp, res.Direction)
		require.Equal(t, seqOneTo(4), sliceutil.Map(res.Versions, migrateVersionToInt))

		// Drop `river_migration` table as if version 001 had never originally run.
		_, err = bundle.dbPool.Exec(ctx, "DROP TABLE river_migration")
		require.NoError(t, err)

		// Run version 005 to make sure it can tolerate the absence of
		// `river_migration`. Note that we have to run the version's SQL
		// directly because using the migrator will try to interact with
		// `river_migration`, which is no longer present.
		_, err = bundle.dbPool.Exec(ctx, migrationsBundle.WithTestVersionsMap[5].SQLUp)
		require.NoError(t, err)

		// And the version 005 down migration to verify the same.
		_, err = bundle.dbPool.Exec(ctx, migrationsBundle.WithTestVersionsMap[5].SQLDown)
		require.NoError(t, err)
	})

	t.Run("MigrationsWithCommitRequired", func(t *testing.T) {
		t.Parallel()

		migrator, bundle := setup(t)

		t.Cleanup(func() {
			tx, err := bundle.dbPool.Begin(ctx)
			require.NoError(t, err)
			defer tx.Rollback(ctx)

			// Clean up the types we created.
			_, err = tx.Exec(ctx, "DROP FUNCTION IF EXISTS foobar_in_bitmask")
			require.NoError(t, err)

			_, err = tx.Exec(ctx, "DROP TYPE IF EXISTS foobar")
			require.NoError(t, err)

			_, err = tx.Exec(ctx, "DELETE FROM river_migration WHERE line = $1", migrationLineCommitRequired)
			require.NoError(t, err)

			require.NoError(t, tx.Commit(ctx))
		})

		_, err := migrator.Migrate(ctx, DirectionUp, &MigrateOpts{})
		require.NoError(t, err)

		// We have to reinitialize the commitRequiredMigrator because the migrations
		// bundle is set in the constructor.
		commitRequiredMigrator, err := New(bundle.driver, &Config{
			Line:   migrationLineCommitRequired,
			Logger: bundle.logger,
		})
		require.NoError(t, err)

		res, err := commitRequiredMigrator.Migrate(ctx, DirectionUp, nil)
		require.NoError(t, err)
		require.Equal(t, DirectionUp, res.Direction)
		require.Equal(t, []int{1, 2, 3}, sliceutil.Map(res.Versions, migrateVersionToInt))
	})
}

// This test uses a custom set of test-only migration files on the file system
// in `rivermigrate/migrate/*`.
func TestMigrationsFromFS(t *testing.T) {
	t.Parallel()

	t.Run("Main", func(t *testing.T) {
		t.Parallel()

		// This is not the actual main line, but rather one embedded in this
		// package's test data (see `rivermigrate/migrate/*`).
		migrations, err := migrationsFromFS(migrationFS, "main")
		require.NoError(t, err)
		require.Equal(t, []int{1, 2}, sliceutil.Map(migrations, migrationToInt))

		migration := migrations[0]
		require.Equal(t, "first", migration.Name)
		require.Equal(t, "SELECT 1;\n", migration.SQLDown)
		require.Equal(t, "SELECT 1;\n", migration.SQLUp)
	})

	t.Run("Alternate", func(t *testing.T) {
		t.Parallel()

		migrations, err := migrationsFromFS(migrationFS, migrationLineAlternate)
		require.NoError(t, err)
		require.Equal(t, seqOneTo(migrationLineAlternateMaxVersion), sliceutil.Map(migrations, migrationToInt))

		migration := migrations[0]
		require.Equal(t, "premier", migration.Name)
		require.Equal(t, "SELECT 1;\n", migration.SQLDown)
		require.Equal(t, "SELECT 1;\n", migration.SQLUp)
	})

	t.Run("DoesNotExist", func(t *testing.T) {
		t.Parallel()

		_, err := migrationsFromFS(migrationFS, "does_not_exist")
		require.EqualError(t, err, `no migrations found for line: "does_not_exist"`)
	})
}

// A bundle of migrations for use in tests. An original set of migrations are
// read from riverpgxv5, then augmented with a couple additional migrations used
// for test purposes.
type testMigrationsBundle struct {
	MaxVersion                 int
	WithTestVersionsMap        map[int]Migration
	WithTestVersionsMaxVersion int
}

func buildTestMigrationsBundle(t *testing.T) *testMigrationsBundle {
	t.Helper()

	// `migration/` subdir is added by migrationsFromFS
	migrationFS := os.DirFS("../riverdriver/riverpgxv5")

	migrations, err := migrationsFromFS(migrationFS, riverdriver.MigrationLineMain)
	require.NoError(t, err)

	// We base our test migrations on the actual line of migrations, so get
	// their maximum version number which we'll use to define test version
	// numbers so that the tests don't break anytime we add a new one.
	migrationsMaxVersion := migrations[len(migrations)-1].Version

	testVersions := []Migration{
		{
			Version: migrationsMaxVersion + 1,
			SQLUp:   "CREATE TABLE test_table(id bigserial PRIMARY KEY);",
			SQLDown: "DROP TABLE test_table;",
		},
		{
			Version: migrationsMaxVersion + 2,
			SQLUp:   "ALTER TABLE test_table ADD COLUMN name varchar(200); CREATE INDEX idx_test_table_name ON test_table(name);",
			SQLDown: "DROP INDEX idx_test_table_name; ALTER TABLE test_table DROP COLUMN name;",
		},
	}

	return &testMigrationsBundle{
		MaxVersion:                 migrationsMaxVersion,
		WithTestVersionsMap:        validateAndInit(append(migrations, testVersions...)),
		WithTestVersionsMaxVersion: migrationsMaxVersion + len(testVersions),
	}
}

// A command returning an error aborts the transaction. This is a shortcut to
// execute a command in a subtransaction so that we can verify an error, but
// continue to use the original transaction.
func dbExecError(ctx context.Context, exec riverdriver.Executor, sql string) error {
	return dbutil.WithTx(ctx, exec, func(ctx context.Context, exec riverdriver.ExecutorTx) error {
		_, err := exec.Exec(ctx, sql)
		return err
	})
}

func driverMigrationToInt(r *riverdriver.Migration) int { return r.Version }
func migrationToInt(migration Migration) int            { return migration.Version }

// Produces a sequence down to one. UpperLimit is included.
func seqOneTo(upperLimit int) []int {
	seq := make([]int, 0, upperLimit)

	for i := 1; i <= upperLimit; i++ {
		seq = append(seq, i)
	}

	return seq
}

func seqDownTo(upperLimit, lowerLimit int) []int {
	seq := make([]int, 0, upperLimit-lowerLimit+1)

	for i := lowerLimit; i <= upperLimit; i++ {
		seq = append(seq, i)
	}

	slices.Reverse(seq)
	return seq
}

```

`rivershared/README.md`:

```md
# rivershared

Shared internal River utilities. No API compatibility guarantees are made.
```

`rivershared/baseservice/base_service.go`:

```go
// Package baseservice contains structs and initialization functions for
// "service-like" objects that provide commonly needed facilities so that they
// don't have to be redefined on every struct. The word "service" is used quite
// loosely here in that it may be applied to many long-lived object that aren't
// strictly services (e.g. adapters).
package baseservice

import (
	"log/slog"
	"reflect"
	"regexp"
	"time"

	"github.com/riverqueue/river/rivertype"
)

// Archetype contains the set of base service properties that are immutable, or
// otherwise safe for services to copy from another service. The struct is also
// embedded in BaseService, so these properties are available on services
// directly.
type Archetype struct {
	// Logger is a structured logger.
	Logger *slog.Logger

	// Time returns a time generator to get the current time in UTC. Normally
	// it's implemented as [UnStubbableTimeGenerator] which just calls
	// through to `time.Now().UTC()`, but is riverinternaltest.timeStub in tests
	// to allow the current time to be stubbed. Services should try to use this
	// function instead of the vanilla ones from the `time` package for testing
	// purposes.
	Time TimeGeneratorWithStub
}

// NewArchetype returns a new archetype. This function is most suitable for
// non-test usage wherein nothing should be stubbed.
func NewArchetype(logger *slog.Logger) *Archetype {
	return &Archetype{
		Logger: logger,
		Time:   &UnStubbableTimeGenerator{},
	}
}

// BaseService is a struct that's meant to be embedded on "service-like" objects
// (e.g. client, producer, queue maintainer) and which provides a number of
// convenient properties that are widely needed so that they don't have to be
// defined on every individual service and can easily be copied from each other.
//
// An initial Archetype should be defined near the program's entrypoint
// (currently in Client), and then each service should invoke Init along with
// the archetype to initialize its own base service. This is often done in the
// service's constructor, but if it doesn't have one, it's the job of the caller
// which instantiates it to invoke Init.
type BaseService struct {
	Archetype

	// Name is a name of the service. It should generally be used to prefix all
	// log lines the service emits.
	Name string
}

func (s *BaseService) GetBaseService() *BaseService { return s }

// withBaseService is an interface to a struct that embeds BaseService. An
// implementation is provided automatically by BaseService, and it's largely
// meant for internal use.
type withBaseService interface {
	GetBaseService() *BaseService
}

// Init initializes a base service from an archetype. It returns the same
// service that was passed into it for convenience.
func Init[TService withBaseService](archetype *Archetype, service TService) TService {
	baseService := service.GetBaseService()

	baseService.Logger = archetype.Logger
	baseService.Name = simplifyLogName(reflect.TypeOf(service).Elem().Name())
	baseService.Time = archetype.Time

	return service
}

type TimeGeneratorWithStub interface {
	rivertype.TimeGenerator

	// StubNowUTC stubs the current time. It will panic if invoked outside of
	// tests. Returns the same time passed as parameter for convenience.
	StubNowUTC(nowUTC time.Time) time.Time
}

// TimeGeneratorWithStubWrapper provides a wrapper around TimeGenerator that
// implements missing TimeGeneratorWithStub functions. This is used so that we
// only need to expose the minimal TimeGenerator interface publicly, but can
// keep a stubbable version of widely available for internal use.
type TimeGeneratorWithStubWrapper struct {
	rivertype.TimeGenerator
}

func (g *TimeGeneratorWithStubWrapper) StubNowUTC(nowUTC time.Time) time.Time {
	panic("time not stubbable outside tests")
}

// UnStubbableTimeGenerator is a TimeGenerator implementation that can't be
// stubbed. It's always the generator used outside of tests.
type UnStubbableTimeGenerator struct{}

func (g *UnStubbableTimeGenerator) NowUTC() time.Time       { return time.Now() }
func (g *UnStubbableTimeGenerator) NowUTCOrNil() *time.Time { return nil }

func (g *UnStubbableTimeGenerator) StubNowUTC(nowUTC time.Time) time.Time {
	panic("time not stubbable outside tests")
}

var stripGenericTypePathRE = regexp.MustCompile(`\[([\[\]\*]*).*/([^/]+)\]`)

// Simplifies the name of a Go type that uses generics for cleaner logging output.
//
// So this:
//
//	QueryCacher[[]*github.com/riverqueue/riverui/internal/dbsqlc.JobCountByStateRow]
//
// Becomes this:
//
//	QueryCacher[[]*dbsqlc.JobCountByStateRow]
func simplifyLogName(name string) string {
	return stripGenericTypePathRE.ReplaceAllString(name, `[$1$2]`)
}

```

`rivershared/baseservice/base_service_test.go`:

```go
package baseservice

import (
	"log/slog"
	"os"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
)

func TestInit(t *testing.T) {
	t.Parallel()

	archetype := archetype()

	myService := Init(archetype, &MyService{})
	require.NotNil(t, myService.Logger)
	require.Equal(t, "MyService", myService.Name)
	require.WithinDuration(t, time.Now().UTC(), myService.Time.NowUTC(), 2*time.Second)
}

type MyService struct {
	BaseService
}

func archetype() *Archetype {
	return &Archetype{
		Logger: slog.New(slog.NewTextHandler(os.Stdout, nil)),
		Time:   &UnStubbableTimeGenerator{},
	}
}

func TestSimplifyLogName(t *testing.T) {
	t.Parallel()

	require.Equal(t, "NotGeneric", simplifyLogName("NotGeneric"))

	// Simplified for use during debugging. Real generics will tend to have
	// fully qualified paths and not look like this.
	require.Equal(t, "Simple[int]", simplifyLogName("Simple[int]"))
	require.Equal(t, "Simple[*int]", simplifyLogName("Simple[*int]"))
	require.Equal(t, "Simple[[]int]", simplifyLogName("Simple[[]int]"))
	require.Equal(t, "Simple[[]*int]", simplifyLogName("Simple[[]*int]"))

	// More realistic examples.
	require.Equal(t, "QueryCacher[dbsqlc.JobCountByStateRow]", simplifyLogName("QueryCacher[github.com/riverqueue/riverui/internal/dbsqlc.JobCountByStateRow]"))
	require.Equal(t, "QueryCacher[*dbsqlc.JobCountByStateRow]", simplifyLogName("QueryCacher[*github.com/riverqueue/riverui/internal/dbsqlc.JobCountByStateRow]"))
	require.Equal(t, "QueryCacher[[]dbsqlc.JobCountByStateRow]", simplifyLogName("QueryCacher[[]github.com/riverqueue/riverui/internal/dbsqlc.JobCountByStateRow]"))
	require.Equal(t, "QueryCacher[[]*dbsqlc.JobCountByStateRow]", simplifyLogName("QueryCacher[[]*github.com/riverqueue/riverui/internal/dbsqlc.JobCountByStateRow]"))
}

```

`rivershared/cmd/update-mod-go/main.go`:

```go
// update-mod-go provides a command to help bump the `go/`toolchain` directives
// in the `go.mod`s of River's internal dependencies across the project. It's
// used to check that all directives match in CI, and to give us an easy way of
// updating them all at once during upgrades.
//
// To check that directives match, run with `CHECK`:
//
//	CHECK=true make update-mod-go
//
// To upgrade a `go`/`toolchain` directive, change it in the workspace's
// `go.work`, then run the program:
//
//	make update-mod-go
package main

import (
	"errors"
	"fmt"
	"os"
	"path"

	"golang.org/x/mod/modfile"
)

func main() {
	if err := run(); err != nil {
		fmt.Fprintf(os.Stderr, "failure: %s\n", err)
		os.Exit(1)
	}
}

func run() error {
	checkOnly := os.Getenv("CHECK") == "true"

	if len(os.Args) != 2 {
		return errors.New("expected exactly one arg, which should be the path to a go.work file")
	}

	workFilename := os.Args[1]

	workFileData, err := os.ReadFile(workFilename)
	if err != nil {
		return fmt.Errorf("error reading file %q: %w", workFilename, err)
	}

	workFile, err := modfile.ParseWork(workFilename, workFileData, nil)
	if err != nil {
		return fmt.Errorf("error parsing file %q: %w", workFilename, err)
	}

	var (
		workGoVersion     = workFile.Go.Version
		workToolchainName = workFile.Toolchain.Name
	)

	for _, workUse := range workFile.Use {
		if _, err := parseAndUpdateGoModFile(checkOnly, "./"+path.Join(workUse.Path, "go.mod"), workFilename, workGoVersion, workToolchainName); err != nil {
			return err
		}
	}

	if checkOnly {
		fmt.Printf("go/toolchain directives in all go.mod files match workspace\n")
	}

	return nil
}

func parseAndUpdateGoModFile(checkOnly bool, modFilename, workFilename, workGoVersion, workToolchainName string) (bool, error) {
	modFileData, err := os.ReadFile(modFilename)
	if err != nil {
		return false, fmt.Errorf("error reading file %q: %w", modFilename, err)
	}

	modFile, err := modfile.Parse(modFilename, modFileData, nil)
	if err != nil {
		return false, fmt.Errorf("error parsing file %q: %w", modFilename, err)
	}

	var anyMismatch bool

	fmt.Printf("%s\n", modFilename)

	if workGoVersion != modFile.Go.Version {
		if checkOnly {
			return false, fmt.Errorf("go directive of %q (%s) doesn't match %q (%s)", modFilename, modFile.Go.Version, workFilename, workGoVersion)
		}

		anyMismatch = true
		if err := modFile.AddGoStmt(workGoVersion); err != nil {
			return false, fmt.Errorf("error adding go statement: %w", err)
		}
		fmt.Printf("    set go to %s for %s\n", workGoVersion, modFilename)
	}

	if workToolchainName != modFile.Toolchain.Name {
		if checkOnly {
			return false, fmt.Errorf("toolchain directive of %q (%s) doesn't match %q (%s)", modFilename, modFile.Toolchain.Name, workFilename, workToolchainName)
		}

		anyMismatch = true
		if err := modFile.AddToolchainStmt(workToolchainName); err != nil {
			return false, fmt.Errorf("error adding toolchain statement: %w", err)
		}
		fmt.Printf("    set toolchain to %s for %s\n", workToolchainName, modFilename)
	}

	if !checkOnly {
		if anyMismatch {
			updatedFileData, err := modFile.Format()
			if err != nil {
				return false, fmt.Errorf("error formatting file %q after update: %w", modFilename, err)
			}

			if err := os.WriteFile(modFilename, updatedFileData, 0o600); err != nil {
				return false, fmt.Errorf("error writing file %q after update: %w", modFilename, err)
			}
		} else {
			fmt.Printf("    no changes\n")
		}
	}

	return anyMismatch, nil
}

```

`rivershared/cmd/update-mod-go/main_test.go`:

```go
package main

import (
	"fmt"
	"os"
	"testing"

	"github.com/stretchr/testify/require"
	"golang.org/x/mod/modfile"
)

const sampleGoMod = `module github.com/riverqueue/river

go 1.22

toolchain go1.23.1

require (
	github.com/riverqueue/river/riverdriver v0.0.0-00010101000000-000000000000
	github.com/riverqueue/river/riverdriver/riverdatabasesql v0.0.0-00010101000000-000000000000
	github.com/riverqueue/river/riverdriver/riverpgxv5 v0.0.12
)`

func TestParseAndUpdateGoModFile(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (string, *testBundle) { //nolint:unparam
		t.Helper()

		file, err := os.CreateTemp(t.TempDir(), "go.mod")
		require.NoError(t, err)
		t.Cleanup(func() { os.Remove(file.Name()) })

		_, err = file.WriteString(sampleGoMod)
		require.NoError(t, err)
		require.NoError(t, file.Close())

		return file.Name(), &testBundle{}
	}

	requireDirectives := func(t *testing.T, filename, goVersion, toolchainName string) {
		t.Helper()

		fileData, err := os.ReadFile(filename)
		require.NoError(t, err)

		modFile, err := modfile.Parse(filename, fileData, nil)
		require.NoError(t, err)

		require.Equal(t, goVersion, modFile.Go.Version)
		require.Equal(t, toolchainName, modFile.Toolchain.Name)
	}

	t.Run("WritesChanges", func(t *testing.T) {
		t.Parallel()

		filename, _ := setup(t)

		anyMismatch, err := parseAndUpdateGoModFile(false, filename, "go.work", "1.23", "go1.23.2")
		require.NoError(t, err)
		require.True(t, anyMismatch)

		// Reread the file that the command above just wrote and make sure the right
		// changes were made.
		requireDirectives(t, filename, "1.23", "go1.23.2")

		// Running again is allowed and should be idempotent. This time it'll
		// return that no changes were made.
		anyMismatch, err = parseAndUpdateGoModFile(false, filename, "go.work", "1.23", "go1.23.2")
		require.NoError(t, err)
		require.False(t, anyMismatch)
	})

	t.Run("NoChanges", func(t *testing.T) {
		t.Parallel()

		filename, _ := setup(t)

		anyMismatch, err := parseAndUpdateGoModFile(false, filename, "go.work", "1.22", "go1.23.1")
		require.NoError(t, err)
		require.False(t, anyMismatch)

		// Expect no changes made in file.
		requireDirectives(t, filename, "1.22", "go1.23.1")
	})

	t.Run("CheckOnlyGoMismatch", func(t *testing.T) {
		t.Parallel()

		filename, _ := setup(t)

		_, err := parseAndUpdateGoModFile(true, filename, "go.work", "1.23", "go1.23.1")
		require.EqualError(t, err, fmt.Sprintf("go directive of %q (%s) doesn't match %q (%s)", filename, "1.22", "go.work", "1.23"))
	})

	t.Run("CheckOnlyToolchainMismatch", func(t *testing.T) {
		t.Parallel()

		filename, _ := setup(t)

		_, err := parseAndUpdateGoModFile(true, filename, "go.work", "1.22", "go1.23.2")
		require.EqualError(t, err, fmt.Sprintf("toolchain directive of %q (%s) doesn't match %q (%s)", filename, "go1.23.1", "go.work", "go1.23.2"))
	})

	t.Run("CheckOnlyNoChanges", func(t *testing.T) {
		t.Parallel()

		filename, _ := setup(t)

		anyMismatch, err := parseAndUpdateGoModFile(true, filename, "go.work", "1.22", "go1.23.1")
		require.NoError(t, err)
		require.False(t, anyMismatch)

		requireDirectives(t, filename, "1.22", "go1.23.1")
	})
}

```

`rivershared/cmd/update-mod-version/main.go`:

```go
// update-mod-version provides a command to help bump the versions of River's
// internal dependencies in the `go.mod` files of submodules across the project.
// It's used to make the release process less error prone and less painful.
//
// Run it with a make target:
//
//	VERSION=v0.x.y make update-mod-version
package main

import (
	"errors"
	"fmt"
	"os"
	"path"
	"strings"

	"golang.org/x/mod/modfile"
	"golang.org/x/mod/semver"
)

func main() {
	if err := run(); err != nil {
		fmt.Fprintf(os.Stderr, "failure: %s\n", err)
		os.Exit(1)
	}
}

func run() error {
	// Allows secondary River repositories to make use of this command by
	// specifying their own prefix.
	packagePrefix := os.Getenv("PACKAGE_PREFIX")
	if packagePrefix == "" {
		return errors.New("expected to find PACKAGE_PREFIX in env")
	}

	version := os.Getenv("VERSION")
	if version == "" {
		return errors.New("expected to find VERSION in env")
	}

	if !semver.IsValid(version) {
		return fmt.Errorf("invalid semver version: %s", version)
	}

	if len(os.Args) != 2 {
		return errors.New("expected exactly one arg, which should be the path to a go.work file")
	}

	workFilename := os.Args[1]

	workFileData, err := os.ReadFile(workFilename)
	if err != nil {
		return fmt.Errorf("error reading file %q: %w", workFilename, err)
	}

	workFile, err := modfile.ParseWork(workFilename, workFileData, nil)
	if err != nil {
		return fmt.Errorf("error parsing file %q: %w", workFilename, err)
	}

	for _, workUse := range workFile.Use {
		if _, err := parseAndUpdateGoModFile("./"+path.Join(workUse.Path, "go.mod"), packagePrefix, version); err != nil {
			return err
		}
	}
	return nil
}

func parseAndUpdateGoModFile(filename, packagePrefix, version string) (bool, error) {
	fileData, err := os.ReadFile(filename)
	if err != nil {
		return false, fmt.Errorf("error reading file %q: %w", filename, err)
	}

	modFile, err := modfile.Parse(filename, fileData, nil)
	if err != nil {
		return false, fmt.Errorf("error parsing file %q: %w", filename, err)
	}

	var anyChanges bool

	fmt.Printf("%s\n", filename)

	for _, require := range modFile.Require {
		if !strings.HasPrefix(require.Mod.Path, packagePrefix) {
			continue
		}

		if require.Mod.Version == version {
			continue
		}

		anyChanges = true
		requirePath := require.Mod.Path

		// Not obvious from the name, but AddRequire replaces an existing
		// require statement if it exists, preserving any comments on it.
		if err := modFile.AddRequire(requirePath, version); err != nil {
			return false, fmt.Errorf("error adding require %q: %w", require.Mod.Path, err)
		}

		fmt.Printf("    set version to %s for %s\n", version, requirePath)
	}

	if anyChanges {
		updatedFileData, err := modFile.Format()
		if err != nil {
			return false, fmt.Errorf("error formatting file %q after update: %w", filename, err)
		}

		if err := os.WriteFile(filename, updatedFileData, 0o600); err != nil {
			return false, fmt.Errorf("error writing file %q after update: %w", filename, err)
		}
	} else {
		fmt.Printf("    no changes\n")
	}

	return anyChanges, nil
}

```

`rivershared/cmd/update-mod-version/main_test.go`:

```go
package main

import (
	"os"
	"strings"
	"testing"

	"github.com/stretchr/testify/require"
	"golang.org/x/mod/modfile"
	"golang.org/x/mod/module"
)

const sampleGoMod = `module github.com/riverqueue/river

go 1.22

toolchain go1.22.5

require (
	github.com/riverqueue/river/riverdriver v0.0.0-00010101000000-000000000000
	github.com/riverqueue/river/riverdriver/riverdatabasesql v0.0.0-00010101000000-000000000000
	github.com/riverqueue/river/riverdriver/riverpgxv5 v0.0.12
)

require (
	github.com/riverqueue/river/rivershared v0.0.12 // indirect
)`

func TestParseAndUpdateGoModFile(t *testing.T) {
	t.Parallel()

	type testBundle struct{}

	setup := func(t *testing.T) (string, *testBundle) {
		t.Helper()

		file, err := os.CreateTemp(t.TempDir(), "go.mod")
		require.NoError(t, err)
		t.Cleanup(func() { os.Remove(file.Name()) })

		_, err = file.WriteString(sampleGoMod)
		require.NoError(t, err)
		require.NoError(t, file.Close())

		return file.Name(), &testBundle{}
	}

	t.Run("WritesChanges", func(t *testing.T) {
		t.Parallel()

		filename, _ := setup(t)

		const packagePrefix = "github.com/riverqueue/river"

		anyChanges, err := parseAndUpdateGoModFile(filename, packagePrefix, "v0.0.13")
		require.NoError(t, err)
		require.True(t, anyChanges)

		// Reread the file that the command above just wrote and make sure the right
		// changes were made.
		fileData, err := os.ReadFile(filename)
		require.NoError(t, err)

		modFile, err := modfile.Parse(filename, fileData, nil)
		require.NoError(t, err)

		versions := make([]module.Version, 0, len(modFile.Require))
		for _, require := range modFile.Require {
			if !strings.HasPrefix(require.Mod.Path, packagePrefix) {
				continue
			}

			versions = append(versions, require.Mod)
		}

		require.Equal(t, []module.Version{
			{Path: "github.com/riverqueue/river/riverdriver", Version: "v0.0.13"},
			{Path: "github.com/riverqueue/river/riverdriver/riverdatabasesql", Version: "v0.0.13"},
			{Path: "github.com/riverqueue/river/riverdriver/riverpgxv5", Version: "v0.0.13"},
			{Path: "github.com/riverqueue/river/rivershared", Version: "v0.0.13"},
		}, versions)

		// Running again is allowed and should be idempotent. This time it'll
		// return that no changes were made.
		anyChanges, err = parseAndUpdateGoModFile(filename, packagePrefix, "v0.0.13")
		require.NoError(t, err)
		require.False(t, anyChanges)
	})
}

```

`rivershared/go.mod`:

```mod
module github.com/riverqueue/river/rivershared

go 1.23.0

toolchain go1.24.1

require (
	github.com/riverqueue/river v0.19.0
	github.com/riverqueue/river/riverdriver v0.19.0
	github.com/riverqueue/river/rivertype v0.19.0
	github.com/stretchr/testify v1.10.0
	go.uber.org/goleak v1.3.0
	golang.org/x/mod v0.24.0
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/jackc/pgerrcode v0.0.0-20240316143900-6e2875d9b438 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

```

`rivershared/go.sum`:

```sum
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/jackc/pgerrcode v0.0.0-20240316143900-6e2875d9b438 h1:Dj0L5fhJ9F82ZJyVOmBx6msDp/kfd1t9GRfny/mfJA0=
github.com/jackc/pgerrcode v0.0.0-20240316143900-6e2875d9b438/go.mod h1:a/s9Lp5W7n/DD0VrVoyJ00FbP2ytTPDVOivvn2bMlds=
github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=
github.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/riverqueue/river v0.18.0 h1:sGHeTOL9MR8+pMIVHRm59fzet8Ron/xjF3Yq/PSGb78=
github.com/riverqueue/river v0.18.0/go.mod h1:oapX5xb/L2YnkE801QubDZ0COHxVxEGVY37icPzghhU=
github.com/riverqueue/river v0.19.0/go.mod h1:YJ7LA2uBdqFHQJzKyYc+X6S04KJeiwsS1yU5a1rynlk=
github.com/riverqueue/river/riverdriver v0.18.0 h1:a2haR5I0MQLHjLCSVFpUEeJALCLemRl5zCztucysm1E=
github.com/riverqueue/river/riverdriver v0.18.0/go.mod h1:Mj45PbHabEnBv/nSah0J1/tg6hrX/SNeXtcYcSqMzxQ=
github.com/riverqueue/river/riverdriver v0.19.0/go.mod h1:Soxi08hHkEvopExAp6ADG2437r4coSiB4QpuIL5E28k=
github.com/riverqueue/river/rivertype v0.18.0 h1:YsXR5NbLAzniurGO0+zcISWMKq7Y71xkIe2oi86OAsE=
github.com/riverqueue/river/rivertype v0.18.0/go.mod h1:DETcejveWlq6bAb8tHkbgJqmXWVLiFhTiEm8j7co1bE=
github.com/riverqueue/river/rivertype v0.19.0/go.mod h1:DETcejveWlq6bAb8tHkbgJqmXWVLiFhTiEm8j7co1bE=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
golang.org/x/mod v0.24.0 h1:ZfthKaKaT4NrhGVZHO1/WDTwGES4De8KtWO0SIbNJMU=
golang.org/x/mod v0.24.0/go.mod h1:IXM97Txy2VM4PJ3gI61r1YEk/gAj6zAHN3AdZt6S9Ww=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

```

`rivershared/levenshtein/License.txt`:

```txt
The MIT License (MIT)

Copyright (c) 2015 Agniva De Sarker

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

`rivershared/levenshtein/levenshtein.go`:

```go
// Package levenshtein is a Go implementation to calculate Levenshtein Distance.
//
// Vendored from this repository:
// https://github.com/agnivade/levenshtein
//
// Implementation taken from
// https://gist.github.com/andrei-m/982927#gistcomment-1931258
package levenshtein

import "unicode/utf8"

// minLengthThreshold is the length of the string beyond which
// an allocation will be made. Strings smaller than this will be
// zero alloc.
const minLengthThreshold = 32

// ComputeDistance computes the levenshtein distance between the two
// strings passed as an argument. The return value is the levenshtein distance
//
// Works on runes (Unicode code points) but does not normalize
// the input strings. See https://blog.golang.org/normalization
// and the golang.org/x/text/unicode/norm package.
func ComputeDistance(str1, str2 string) int {
	if len(str1) == 0 {
		return utf8.RuneCountInString(str2)
	}

	if len(str2) == 0 {
		return utf8.RuneCountInString(str1)
	}

	if str1 == str2 {
		return 0
	}

	// We need to convert to []rune if the strings are non-ASCII.
	// This could be avoided by using utf8.RuneCountInString
	// and then doing some juggling with rune indices,
	// but leads to far more bounds checks. It is a reasonable trade-off.
	runeSlice1 := []rune(str1)
	runeSlice2 := []rune(str2)

	// swap to save some memory O(min(a,b)) instead of O(a)
	if len(runeSlice1) > len(runeSlice2) {
		runeSlice1, runeSlice2 = runeSlice2, runeSlice1
	}
	lenRuneSlice1 := len(runeSlice1)
	lenRuneSlice2 := len(runeSlice2)

	// Init the row.
	var distances []uint16
	if lenRuneSlice1+1 > minLengthThreshold {
		distances = make([]uint16, lenRuneSlice1+1)
	} else {
		// We make a small optimization here for small strings. Because a slice
		// of constant length is effectively an array, it does not allocate. So
		// we can re-slice it to the right length as long as it is below a
		// desired threshold.
		distances = make([]uint16, minLengthThreshold)
		distances = distances[:lenRuneSlice1+1]
	}

	// we start from 1 because index 0 is already 0.
	for i := 1; i < len(distances); i++ {
		distances[i] = uint16(i) //nolint:gosec
	}

	// Make a dummy bounds check to prevent the 2 bounds check down below. The
	// one inside the loop is particularly costly.
	_ = distances[lenRuneSlice1]

	// fill in the rest
	for i := 1; i <= lenRuneSlice2; i++ {
		prev := uint16(i) //nolint:gosec
		for j := 1; j <= lenRuneSlice1; j++ {
			current := distances[j-1] // match
			if runeSlice2[i-1] != runeSlice1[j-1] {
				current = min(min(distances[j-1]+1, prev+1), distances[j]+1)
			}
			distances[j-1] = prev
			prev = current
		}
		distances[lenRuneSlice1] = prev
	}
	return int(distances[lenRuneSlice1])
}

```

`rivershared/levenshtein/levenshtein_test.go`:

```go
package levenshtein_test

import (
	"testing"

	"github.com/riverqueue/river/rivershared/levenshtein"
)

func TestSanity(t *testing.T) {
	t.Parallel()

	tests := []struct {
		str1, str2 string
		want       int
	}{
		{"", "hello", 5},
		{"hello", "", 5},
		{"hello", "hello", 0},
		{"ab", "aa", 1},
		{"ab", "ba", 2},
		{"ab", "aaa", 2},
		{"bbb", "a", 3},
		{"kitten", "sitting", 3},
		{"distance", "difference", 5},
		{"levenshtein", "frankenstein", 6},
		{"resume and cafe", "resumes and cafes", 2},
		{"a very long string that is meant to exceed", "another very long string that is meant to exceed", 6},
	}
	for i, d := range tests {
		n := levenshtein.ComputeDistance(d.str1, d.str2)
		if n != d.want {
			t.Errorf("Test[%d]: ComputeDistance(%q,%q) returned %v, want %v",
				i, d.str1, d.str2, n, d.want)
		}
	}
}

func TestUnicode(t *testing.T) {
	t.Parallel()

	tests := []struct {
		str1, str2 string
		want       int
	}{
		// Testing acutes and umlauts
		{"resumé and café", "resumés and cafés", 2},
		{"resume and cafe", "resumé and café", 2},
		{"Hafþór Júlíus Björnsson", "Hafþor Julius Bjornsson", 4},
		// Only 2 characters are less in the 2nd string
		{"།་གམ་འས་པ་་མ།", "།་གམའས་པ་་མ", 2},
	}
	for i, d := range tests {
		n := levenshtein.ComputeDistance(d.str1, d.str2)
		if n != d.want {
			t.Errorf("Test[%d]: ComputeDistance(%q,%q) returned %v, want %v",
				i, d.str1, d.str2, n, d.want)
		}
	}
}

// Benchmarks
// ----------------------------------------------.
var sink int //nolint:gochecknoglobals

func BenchmarkSimple(b *testing.B) {
	tests := []struct {
		a, b string
		name string
	}{
		// ASCII
		{"levenshtein", "frankenstein", "ASCII"},
		// Testing acutes and umlauts
		{"resumé and café", "resumés and cafés", "French"},
		{"Hafþór Júlíus Björnsson", "Hafþor Julius Bjornsson", "Nordic"},
		{"a very long string that is meant to exceed", "another very long string that is meant to exceed", "long string"},
		// Only 2 characters are less in the 2nd string
		{"།་གམ་འས་པ་་མ།", "།་གམའས་པ་་མ", "Tibetan"},
	}
	tmp := 0
	for _, test := range tests {
		b.Run(test.name, func(b *testing.B) {
			for range b.N {
				tmp = levenshtein.ComputeDistance(test.a, test.b)
			}
		})
	}
	sink = tmp
}

```

`rivershared/riverpilot/pilot.go`:

```go
package riverpilot

import (
	"context"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivertype"
)

// A Pilot bridges the gap between the River client and the driver, implementing
// higher level functionality on top of the driver's underlying queries. It
// tracks closely to the underlying driver's API, but may add additional
// functionality or logic wrapping the queries.
//
// This should be considered a River internal API and its stability is not
// guaranteed. DO NOT USE.
type Pilot interface {
	JobInsertMany(
		ctx context.Context,
		tx riverdriver.ExecutorTx,
		params []*riverdriver.JobInsertFastParams,
	) ([]*riverdriver.JobInsertFastResult, error)

	JobSetStateIfRunningMany(ctx context.Context, tx riverdriver.ExecutorTx, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error)

	PilotInit(archetype *baseservice.Archetype)
}

```

`rivershared/riverpilot/standard.go`:

```go
package riverpilot

import (
	"context"

	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivertype"
)

type StandardPilot struct{}

func (p *StandardPilot) JobInsertMany(
	ctx context.Context,
	tx riverdriver.ExecutorTx,
	params []*riverdriver.JobInsertFastParams,
) ([]*riverdriver.JobInsertFastResult, error) {
	return tx.JobInsertFastMany(ctx, params)
}

func (p *StandardPilot) JobSetStateIfRunningMany(ctx context.Context, tx riverdriver.ExecutorTx, params *riverdriver.JobSetStateIfRunningManyParams) ([]*rivertype.JobRow, error) {
	return tx.JobSetStateIfRunningMany(ctx, params)
}

func (p *StandardPilot) PilotInit(archetype *baseservice.Archetype) {
	// Noop
}

```

`rivershared/riversharedtest/riversharedtest.go`:

```go
package riversharedtest

import (
	"cmp"
	"context"
	"errors"
	"fmt"
	"log/slog"
	"os"
	"sync"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/stretchr/testify/require"
	"go.uber.org/goleak"

	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/slogtest"
)

// BaseServiceArchetype returns a new base service suitable for use in tests.
// Returns a new instance so that it's not possible to accidentally taint a
// shared object.
func BaseServiceArchetype(tb testing.TB) *baseservice.Archetype {
	tb.Helper()

	return &baseservice.Archetype{
		Logger: Logger(tb),
		Time:   &TimeStub{},
	}
}

// A pool and mutex to protect it, lazily initialized by TestTx. Once open, this
// pool is never explicitly closed, instead closing implicitly as the package
// tests finish.
var (
	dbPool   *pgxpool.Pool //nolint:gochecknoglobals
	dbPoolMu sync.RWMutex  //nolint:gochecknoglobals
)

// DBPool gets a lazily initialized database pool for `TEST_DATABASE_URL` or
// `river_test` if the former isn't specified.
func DBPool(ctx context.Context, tb testing.TB) *pgxpool.Pool {
	tb.Helper()

	tryPool := func() *pgxpool.Pool {
		dbPoolMu.RLock()
		defer dbPoolMu.RUnlock()
		return dbPool
	}

	if dbPool := tryPool(); dbPool != nil {
		return dbPool
	}

	dbPoolMu.Lock()
	defer dbPoolMu.Unlock()

	// Multiple goroutines may have passed the initial `nil` check on start
	// up, so check once more to make sure pool hasn't been set yet.
	if dbPool != nil {
		return dbPool
	}

	dbPool, err := pgxpool.New(ctx, cmp.Or(
		os.Getenv("TEST_DATABASE_URL"),
		"postgres://localhost:5432/river_test",
	))
	require.NoError(tb, err)

	return dbPool
}

// Logger returns a logger suitable for use in tests.
//
// Defaults to informational verbosity. If env is set with `RIVER_DEBUG=true`,
// debug level verbosity is activated.
func Logger(tb testing.TB) *slog.Logger {
	tb.Helper()

	if os.Getenv("RIVER_DEBUG") == "1" || os.Getenv("RIVER_DEBUG") == "true" {
		return slogtest.NewLogger(tb, &slog.HandlerOptions{Level: slog.LevelDebug})
	}

	return slogtest.NewLogger(tb, nil)
}

// Logger returns a logger suitable for use in tests which outputs only at warn
// or above. Useful in tests where particularly noisy log output is expected.
func LoggerWarn(tb testing.TB) *slog.Logger {
	tb.Helper()
	return slogtest.NewLogger(tb, &slog.HandlerOptions{Level: slog.LevelWarn})
}

// TestTx starts a test transaction that's rolled back automatically as the test
// case is cleaning itself up.
//
// This variant uses the default database pool from DBPool that points to
// `TEST_DATABASE_URL` or `river_test` if the former wasn't specified.
func TestTx(ctx context.Context, tb testing.TB) pgx.Tx {
	tb.Helper()
	return TestTxPool(ctx, tb, DBPool(ctx, tb))
}

// TestTxPool starts a test transaction that's rolled back automatically as the
// test case is cleaning itself up.
//
// This variant starts the test transaction on the specified database pool.
func TestTxPool(ctx context.Context, tb testing.TB, dbPool *pgxpool.Pool) pgx.Tx {
	tb.Helper()

	tx, err := dbPool.Begin(ctx)
	require.NoError(tb, err)

	tb.Cleanup(func() {
		// Tests may inerit context from `t.Context()` which is cancelled after
		// tests run and before calling clean up. We need a non-cancelled
		// context to issue rollback here, so use a bit of a bludgeon to do so
		// with `context.WithoutCancel()`.
		ctx := context.WithoutCancel(ctx)

		err := tx.Rollback(ctx)

		if err == nil {
			return
		}

		// Try to look for an error on rollback because it does occasionally
		// reveal a real problem in the way a test is written. However, allow
		// tests to roll back their transaction early if they like, so ignore
		// `ErrTxClosed`.
		if errors.Is(err, pgx.ErrTxClosed) {
			return
		}

		// In case of a cancelled context during a database operation, which
		// happens in many tests, pgx seems to not only roll back the
		// transaction, but closes the connection, and returns this error on
		// rollback. Allow this error since it's hard to prevent it in our flows
		// that use contexts heavily.
		if err.Error() == "conn closed" {
			return
		}

		// Similar to the above, but a newly appeared error that wraps the
		// above. As far as I can tell, no error variables are available to use
		// with `errors.Is`.
		if err.Error() == "failed to deallocate cached statement(s): conn closed" {
			return
		}

		require.NoError(tb, err)
	})

	return tx
}

// TimeStub implements baseservice.TimeGeneratorWithStub to allow time to be
// stubbed in tests.
//
// It exists separately from rivertest.TimeStub to avoid a circular dependency.
type TimeStub struct {
	mu     sync.RWMutex
	nowUTC *time.Time
}

func (t *TimeStub) NowUTC() time.Time {
	t.mu.RLock()
	defer t.mu.RUnlock()

	if t.nowUTC == nil {
		return time.Now().UTC()
	}

	return *t.nowUTC
}

func (t *TimeStub) NowUTCOrNil() *time.Time {
	t.mu.RLock()
	defer t.mu.RUnlock()

	return t.nowUTC
}

func (t *TimeStub) StubNowUTC(nowUTC time.Time) time.Time {
	t.mu.Lock()
	defer t.mu.Unlock()

	t.nowUTC = &nowUTC
	return nowUTC
}

// WaitOrTimeout tries to wait on the given channel for a value to come through,
// and returns it if one does, but times out after a reasonable amount of time.
// Useful to guarantee that test cases don't hang forever, even in the event of
// something wrong.
func WaitOrTimeout[T any](tb testing.TB, waitChan <-chan T) T {
	tb.Helper()

	timeout := WaitTimeout()

	select {
	case value := <-waitChan:
		return value
	case <-time.After(timeout):
		require.FailNowf(tb, "WaitOrTimeout timed out",
			"WaitOrTimeout timed out after waiting %s", timeout)
	}
	return *new(T) // unreachable
}

// WaitOrTimeoutN tries to wait on the given channel for N values to come
// through, and returns it if they do, but times out after a reasonable amount
// of time.  Useful to guarantee that test cases don't hang forever, even in the
// event of something wrong.
func WaitOrTimeoutN[T any](tb testing.TB, waitChan <-chan T, numValues int) []T {
	tb.Helper()

	var (
		timeout  = WaitTimeout()
		deadline = time.Now().Add(timeout)
		values   = make([]T, 0, numValues)
	)

	for {
		select {
		case value := <-waitChan:
			values = append(values, value)

			if len(values) >= numValues {
				return values
			}

		case <-time.After(time.Until(deadline)):
			require.FailNowf(tb, "WaitOrTimeout timed out",
				"WaitOrTimeout timed out after waiting %s (received %d value(s), wanted %d)", timeout, len(values), numValues)
			return nil
		}
	}
}

// WaitTimeout returns a duration broadly appropriate for waiting on an expected
// event in a test, and which is used for `TestSignal.WaitOrTimeout` in the main
// package and `WaitOrTimeout` above. Its main purpose is to allow a little
// extra leeway in GitHub Actions where we occasionally seem to observe subpar
// performance which leads to timeouts and test intermittency, while still
// keeping a tight a timeout for local test runs where this is never a problem.
func WaitTimeout() time.Duration {
	if os.Getenv("GITHUB_ACTIONS") == "true" {
		return 10 * time.Second
	}

	return 3 * time.Second
}

var IgnoredKnownGoroutineLeaks = []goleak.Option{ //nolint:gochecknoglobals
	// This goroutine contains a 500 ms uninterruptible sleep that may still be
	// running by the time the test suite finishes and cause a failure. This
	// might be something that should be fixed in pgx, but ignore it for the
	// time being lest we have intermittent tests.
	//
	// We opened an issue on pgx, but it may or may not be one that gets fixed:
	//
	// https://github.com/jackc/pgx/issues/1641
	goleak.IgnoreTopFunction("github.com/jackc/pgx/v5/pgxpool.(*Pool).backgroundHealthCheck"),

	// Similar to the above, may be sitting in a sleep when the program finishes
	// and there's not much we can do about it.
	goleak.IgnoreAnyFunction("github.com/jackc/pgx/v5/pgxpool.(*Pool).triggerHealthCheck.func1"),
}

// WrapTestMain performs some common setup and teardown that should be shared
// amongst all packages. e.g. Configures a manager for test databases on setup,
// and checks for no goroutine leaks on teardown.
func WrapTestMain(m *testing.M) {
	status := m.Run()

	if status == 0 {
		if err := goleak.Find(IgnoredKnownGoroutineLeaks...); err != nil {
			fmt.Fprintf(os.Stderr, "goleak: Errors on successful test run: %v\n", err)
			status = 1
		}
	}

	os.Exit(status)
}

```

`rivershared/riversharedtest/riversharedtest_test.go`:

```go
package riversharedtest

import (
	"context"
	"testing"
	"time"

	"github.com/jackc/pgerrcode"
	"github.com/jackc/pgx/v5/pgconn"
	"github.com/stretchr/testify/require"
)

func TestDBPool(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	pool := DBPool(ctx, t)
	_, err := pool.Exec(ctx, "SELECT 1")
	require.NoError(t, err)
}

func TestTestTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type PoolOrTx interface {
		Exec(ctx context.Context, sql string, arguments ...any) (commandTag pgconn.CommandTag, err error)
	}

	checkTestTable := func(ctx context.Context, poolOrTx PoolOrTx) error {
		_, err := poolOrTx.Exec(ctx, "SELECT * FROM river_shared_test_tx_table")
		return err
	}

	// Test cleanups are invoked in the order of last added, first called. When
	// TestTx is called below it adds a cleanup, so we want to make sure that
	// this cleanup, which checks that the database remains pristine, is invoked
	// after the TestTx cleanup, so we add it first.
	t.Cleanup(func() {
		// Tests may inherit context from `t.Context()` which is cancelled after
		// tests run and before calling clean up. We need a non-cancelled
		// context to issue rollback here, so use a bit of a bludgeon to do so
		// with `context.WithoutCancel()`.
		ctx := context.WithoutCancel(ctx)

		err := checkTestTable(ctx, DBPool(ctx, t))
		require.Error(t, err)

		var pgErr *pgconn.PgError
		require.ErrorAs(t, err, &pgErr)
		require.Equal(t, pgerrcode.UndefinedTable, pgErr.Code)
	})

	tx := TestTx(ctx, t)

	_, err := tx.Exec(ctx, "CREATE TABLE river_shared_test_tx_table (id bigint)")
	require.NoError(t, err)

	err = checkTestTable(ctx, tx)
	require.NoError(t, err)
}

func TestWaitOrTimeout(t *testing.T) {
	t.Parallel()

	// Inject a few extra numbers to make sure we pick only one.
	numChan := make(chan int, 5)
	for i := range 5 {
		numChan <- i
	}

	num := WaitOrTimeout(t, numChan)
	require.Equal(t, 0, num)
}

func TestWaitOrTimeoutN(t *testing.T) {
	t.Parallel()

	// Inject a few extra numbers to make sure we pick the right number.
	numChan := make(chan int, 5)
	for i := range 5 {
		numChan <- i
	}

	nums := WaitOrTimeoutN(t, numChan, 3)
	require.Equal(t, []int{0, 1, 2}, nums)
}

func TestTimeStub(t *testing.T) {
	t.Parallel()

	t.Run("BasicUsage", func(t *testing.T) {
		t.Parallel()

		initialTime := time.Now().UTC()

		timeStub := &TimeStub{}

		timeStub.StubNowUTC(initialTime)
		require.Equal(t, initialTime, timeStub.NowUTC())

		newTime := timeStub.StubNowUTC(initialTime.Add(1 * time.Second))
		require.Equal(t, newTime, timeStub.NowUTC())
	})

	t.Run("Stress", func(t *testing.T) {
		t.Parallel()

		timeStub := &TimeStub{}

		for range 10 {
			go func() {
				for range 50 {
					timeStub.StubNowUTC(time.Now().UTC())
					_ = timeStub.NowUTC()
				}
			}()
		}
	})
}

```

`rivershared/slogtest/slog_test_handler.go`:

```go
package slogtest

import (
	"bytes"
	"context"
	"io"
	"log/slog"
	"sync"
	"testing"
)

// NewLogger returns a new slog text logger that outputs to `t.Log`. This helps
// keep test output better formatted, and allows it to be differentiated in case
// of a failure during a parallel test suite run.
func NewLogger(tb testing.TB, opts *slog.HandlerOptions) *slog.Logger {
	tb.Helper()

	var buf bytes.Buffer

	textHandler := slog.NewTextHandler(&buf, opts)

	return slog.New(&slogTestHandler{
		buf:   &buf,
		inner: textHandler,
		mu:    &sync.Mutex{},
		tb:    tb,
	})
}

type slogTestHandler struct {
	buf   *bytes.Buffer
	inner slog.Handler
	mu    *sync.Mutex
	tb    testing.TB
}

func (b *slogTestHandler) Enabled(ctx context.Context, level slog.Level) bool {
	return b.inner.Enabled(ctx, level)
}

func (b *slogTestHandler) Handle(ctx context.Context, rec slog.Record) error {
	b.mu.Lock()
	defer b.mu.Unlock()

	err := b.inner.Handle(ctx, rec)
	if err != nil {
		return err
	}

	output, err := io.ReadAll(b.buf)
	if err != nil {
		return err
	}

	// t.Log adds its own newline, so trim the one from slog.
	output = bytes.TrimSuffix(output, []byte("\n"))

	// Register as a helper, but unfortunately still not enough to fix the
	// reported callsite of the log line and it'll still show `logger.go` from
	// slog's internals. See explanation and discussion here:
	//
	// https://github.com/neilotoole/slogt#deficiency
	b.tb.Helper()

	b.tb.Log(string(output))

	return nil
}

func (b *slogTestHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &slogTestHandler{
		buf:   b.buf,
		inner: b.inner.WithAttrs(attrs),
		mu:    b.mu,
		tb:    b.tb,
	}
}

func (b *slogTestHandler) WithGroup(name string) slog.Handler {
	return &slogTestHandler{
		buf:   b.buf,
		inner: b.inner.WithGroup(name),
		mu:    b.mu,
		tb:    b.tb,
	}
}

```

`rivershared/slogtest/slog_test_handler_test.go`:

```go
package slogtest

import (
	"log/slog"
	"sync"
	"testing"
)

// This test doesn't assert anything due to the inherent difficulty of testing
// this test helper, but it can be run with `-test.v` to observe that it's
// working correctly.
func TestSlogTestHandler_levels(t *testing.T) {
	t.Parallel()

	testCases := []struct {
		desc  string
		level slog.Level
	}{
		{desc: "Debug", level: slog.LevelDebug},
		{desc: "Info", level: slog.LevelInfo},
		{desc: "Warn", level: slog.LevelWarn},
		{desc: "Error", level: slog.LevelError},
	}
	for _, tt := range testCases {
		t.Run(tt.desc, func(t *testing.T) {
			t.Parallel()

			logger := NewLogger(t, &slog.HandlerOptions{Level: tt.level})

			logger.Debug("debug message")
			logger.Info("info message")
			logger.Warn("warn message")
			logger.Error("error message")
		})
	}
}

func TestSlogTestHandler_stress(t *testing.T) {
	t.Parallel()

	var (
		logger = NewLogger(t, nil)
		wg     sync.WaitGroup
	)

	for range 10 {
		wg.Add(1)
		go func() {
			for range 100 {
				logger.Info("message", "key", "value")
			}
			wg.Done()
		}()
	}

	wg.Wait()
}

```

`rivershared/sqlctemplate/sqlc_template.go`:

```go
// Package sqlctemplate provides a way of making arbitrary text replacement in
// sqlc queries which normally only allow parameters which are in places valid
// in a prepared statement. For example, it can be used to insert a schema name
// as a prefix to tables referenced in sqlc, which is otherwise impossible.
//
// Replacement is carried out from within invocations of sqlc's generated DBTX
// interface, after sqlc generated code runs, but before queries are executed.
// This is accomplished by implementing DBTX, calling Replacer.Run from within
// them, and injecting parameters in with WithReplacements (which is unfortunately
// the only way of injecting them).
//
// Templates are modeled as SQL comments so that they're still parseable as
// valid SQL. An example use of the basic /* TEMPLATE ... */ syntax:
//
//	-- name: JobCountByState :one
//	SELECT count(*)
//	FROM /* TEMPLATE: schema */river_job
//	WHERE state = @state;
//
// An open/close syntax is also available for when SQL is required before
// processing for the query to be valid. For example, a WHERE or ORDER BY clause
// can't be empty, so the SQL includes a sentinel value that's parseable which
// is then replaced later with template values:
//
//	-- name: JobList :many
//	SELECT *
//	FROM river_job
//	WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */
//	ORDER BY /* TEMPLATE_BEGIN: order_by_clause */ id /* TEMPLATE_END */
//	LIMIT @max::int;
//
// Be careful not to place a template on a line by itself because sqlc will
// strip any lines that start with a comment. For example, this does NOT work:
//
//	-- name: JobList :many
//	SELECT *
//	FROM river_job
//	/* TEMPLATE_BEGIN: where_clause */
//	LIMIT @max::int;
package sqlctemplate

import (
	"context"
	"errors"
	"fmt"
	"regexp"
	"slices"
	"strconv"
	"strings"
	"sync"

	"github.com/riverqueue/river/rivershared/util/maputil"
)

// Context container added by WithReplacements.
type contextContainer struct {
	// NamedArgs and their values to be replaced after templates in Replacements
	// are rendered.
	NamedArgs map[string]any

	// Replacements maps template names to replacement values.
	Replacements map[string]Replacement
}

type contextKey struct{}

// Replacement defines a replacement for a template value in some input SQL.
type Replacement struct {
	// Stable is whether the replacement value is expected to be stable for any
	// number of times Replacer.Run is called with the same given input SQL. If
	// all replacements are stable, then the output of Replacer.Run is cached so
	// that it doesn't have to be processed again. Replacements should be not be
	// stable if they depend on input parameters.
	Stable bool

	// Value is the value which the template should be replaced with. For a /*
	// TEMPLATE ... */ tag, replaces template and the comment containing it. For
	// a /* TEMPLATE_BEGIN ... */ ... /* TEMPLATE_END */ tag pair, replaces both
	// templates, comments, and the value between them.
	Value string
}

// Replacer replaces templates with template values. As an optimization, it
// contains an internal cache to short circuit SQL that has entirely stable
// template replacements and whose output is invariant of input parameters.
//
// The struct is written so that it's safe to use as a value and doesn't need to
// be initialized with a constructor. This lets it default to a usable instance
// on drivers that may themselves not be initialized.
type Replacer struct {
	cache   map[replacerCacheKey]string
	cacheMu sync.RWMutex
}

var (
	templateBeginEndRE = regexp.MustCompile(`/\* TEMPLATE_BEGIN: (.*?) \*/ .*? /\* TEMPLATE_END \*/`)
	templateRE         = regexp.MustCompile(`/\* TEMPLATE: (.*?) \*/`)
)

// Run replaces any tempates in input SQL with values from context added via
// WithReplacements.
//
// args aren't used for replacements in the input SQL, but are needed to
// determine which placeholder number (e.g. $1, $2, $3, ...) we should start
// with to replace any template named args. The returned args value should then
// be used as query input as named args from context may have been added to it.
func (r *Replacer) Run(ctx context.Context, sql string, args []any) (string, []any) {
	sql, namedArgs, err := r.RunSafely(ctx, sql, args)
	if err != nil {
		panic(err)
	}
	return sql, namedArgs
}

// RunSafely is the same as Run, but returns an error in case of missing or
// extra templates.
func (r *Replacer) RunSafely(ctx context.Context, sql string, args []any) (string, []any, error) {
	var (
		container, containerOK = ctx.Value(contextKey{}).(*contextContainer)
		sqlContainsTemplate    = strings.Contains(sql, "/* TEMPLATE")
	)
	switch {
	case !containerOK && !sqlContainsTemplate:
		// Neither context container or template in SQL; short circuit fast because there's no work to do.
		return sql, args, nil

	case containerOK && !sqlContainsTemplate:
		return "", nil, errors.New("sqlctemplate found context container but SQL contains no templates; bug?")

	case !containerOK && sqlContainsTemplate:
		return "", nil, errors.New("sqlctemplate found template(s) in SQL, but no context container; bug?")
	}

	cacheKey, cacheEligible := replacerCacheKeyFrom(sql, container)
	if cacheEligible {
		r.cacheMu.RLock()
		var (
			cachedSQL   string
			cachedSQLOK bool
		)
		if r.cache != nil { // protect against map not initialized yet
			cachedSQL, cachedSQLOK = r.cache[cacheKey]
		}
		r.cacheMu.RUnlock()

		// If all input templates were stable, the finished SQL will have been cached.
		if cachedSQLOK {
			if len(container.NamedArgs) > 0 {
				args = append(args, maputil.Values(container.NamedArgs)...)
			}
			return cachedSQL, args, nil
		}
	}

	var (
		templatesExpected = maputil.Keys(container.Replacements)
		templatesMissing  []string // not preallocated because we don't expect any missing parameters in the common case
	)

	replaceTemplate := func(sql string, templateRE *regexp.Regexp) string {
		return templateRE.ReplaceAllStringFunc(sql, func(templateStr string) string {
			// Really dumb, but Go doesn't provide any way to get submatches in a
			// function, so we have to match twice.
			//     https://github.com/golang/go/issues/5690
			matches := templateRE.FindStringSubmatch(templateStr)

			template := matches[1]

			if replacement, ok := container.Replacements[template]; ok {
				templatesExpected = slices.DeleteFunc(templatesExpected, func(p string) bool { return p == template })
				return replacement.Value
			} else {
				templatesMissing = append(templatesMissing, template)
			}

			return templateStr
		})
	}

	updatedSQL := sql
	updatedSQL = replaceTemplate(updatedSQL, templateBeginEndRE)
	updatedSQL = replaceTemplate(updatedSQL, templateRE)

	if len(templatesExpected) > 0 {
		return "", nil, errors.New("sqlctemplate params present in context but missing in SQL: " + strings.Join(templatesExpected, ", "))
	}

	if len(templatesMissing) > 0 {
		return "", nil, errors.New("sqlctemplate params present in SQL but missing in context: " + strings.Join(templatesMissing, ", "))
	}

	if len(container.NamedArgs) > 0 {
		placeholderNum := len(args)

		// For the benefit of the test suite's output being predictable, sort
		// named args before processing them.
		sortedNamedArgs := maputil.Keys(container.NamedArgs)
		slices.Sort(sortedNamedArgs)
		for _, arg := range sortedNamedArgs {
			placeholderNum++

			var (
				symbol      = "@" + arg
				symbolIndex = strings.Index(updatedSQL, symbol)
				val         = container.NamedArgs[arg]
			)

			if symbolIndex == -1 {
				return "", nil, fmt.Errorf("sqltemplate expected to find named arg %q, but it wasn't present", symbol)
			}

			// ReplaceAll because an input parameter may appear multiple times.
			updatedSQL = strings.ReplaceAll(updatedSQL, symbol, "$"+strconv.Itoa(placeholderNum))
			args = append(args, val)
		}
	}

	if cacheEligible {
		r.cacheMu.Lock()
		if r.cache == nil {
			r.cache = make(map[replacerCacheKey]string)
		}
		r.cache[cacheKey] = updatedSQL
		r.cacheMu.Unlock()
	}

	return updatedSQL, args, nil
}

// WithReplacements adds sqlctemplate templates to the given context (they go in
// context because it's the only way to get them down into the innards of sqlc).
// namedArgs can also be passed in to replace arguments found in
//
// If sqlctemplate params are already present in context, the two sets are
// merged, with the new params taking precedent.
func WithReplacements(ctx context.Context, replacements map[string]Replacement, namedArgs map[string]any) context.Context {
	if container, ok := ctx.Value(contextKey{}).(*contextContainer); ok {
		for arg, val := range namedArgs {
			container.NamedArgs[arg] = val
		}
		for template, replacement := range replacements {
			container.Replacements[template] = replacement
		}
		return ctx
	}

	if namedArgs == nil {
		namedArgs = make(map[string]any)
	}

	return context.WithValue(ctx, contextKey{}, &contextContainer{
		NamedArgs:    namedArgs,
		Replacements: replacements,
	})
}

// Comparable struct that's used as a key for template caching.
type replacerCacheKey struct {
	namedArgs         string // all arg names concatenated together
	replacementValues string // all values concatenated together
	sql               string
}

// Builds a cache key for the given SQL and context container.
//
// A key is only built if the given SQL/templates are cacheable, which means all
// template values must be stable. The second return value is a boolean
// indicating whether a cache key was built or not. If false, the input is not
// eligible for caching, and no check against the cache should be made.
func replacerCacheKeyFrom(sql string, container *contextContainer) (replacerCacheKey, bool) {
	// Only eligible for caching if all replacements are stable.
	for _, replacement := range container.Replacements {
		if !replacement.Stable {
			return replacerCacheKey{}, false
		}
	}

	var (
		namedArgsBuilder strings.Builder

		// Named args must be sorted for key stability because Go maps don't
		// provide any ordering guarantees.
		sortedNamedArgs = maputil.Keys(container.NamedArgs)
	)
	slices.Sort(sortedNamedArgs)
	for _, namedArg := range sortedNamedArgs {
		namedArgsBuilder.WriteRune('@') // useful as separator because not valid in the name of a named arg
		namedArgsBuilder.WriteString(namedArg)
	}

	var (
		replacementValuesBuilder strings.Builder
		sortedReplacements       = maputil.Keys(container.Replacements)
	)
	slices.Sort(sortedReplacements)
	for _, template := range sortedReplacements {
		replacementValuesBuilder.WriteRune('•') // use a separator that SQL would reject under most circumstances (this may be imperfect)
		replacementValuesBuilder.WriteString(container.Replacements[template].Value)
	}

	return replacerCacheKey{
		namedArgs:         namedArgsBuilder.String(),
		replacementValues: replacementValuesBuilder.String(),
		sql:               sql,
	}, true
}

```

`rivershared/sqlctemplate/sqlc_template_test.go`:

```go
package sqlctemplate

import (
	"context"
	"sync"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestReplacer(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (*Replacer, *testBundle) { //nolint:unparam
		t.Helper()

		return &Replacer{}, &testBundle{}
	}

	t.Run("NoContainerError", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		_, _, err := replacer.RunSafely(ctx, `
			SELECT /* TEMPLATE: schema */river_job;
		`, nil)
		require.EqualError(t, err, "sqlctemplate found template(s) in SQL, but no context container; bug?")
	})

	t.Run("NoTemplateError", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{}, nil)

		_, _, err := replacer.RunSafely(ctx, `
			SELECT 1;
		`, nil)
		require.EqualError(t, err, "sqlctemplate found context container but SQL contains no templates; bug?")
	})

	t.Run("NoContainerOrTemplate", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			SELECT 1;
		`, nil)
		require.NoError(t, err)
		require.Equal(t, `
			SELECT 1;
		`, updatedSQL)
		require.Nil(t, args)
	})

	t.Run("BasicTemplate", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"schema": {Value: "test_schema."},
		}, nil)

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			-- name: JobCountByState :one
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job
			WHERE state = @state;
		`, nil)
		require.NoError(t, err)
		require.Nil(t, args)
		require.Equal(t, `
			-- name: JobCountByState :one
			SELECT count(*)
			FROM test_schema.river_job
			WHERE state = @state;
		`, updatedSQL)
	})

	t.Run("BeginEndTemplate", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"order_by_clause": {Value: "kind, id"},
			"where_clause":    {Value: "kind = 'no_op'"},
		}, nil)

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			-- name: JobList :many
			SELECT *
			FROM river_job
			WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */
			ORDER BY /* TEMPLATE_BEGIN: order_by_clause */ id /* TEMPLATE_END */
			LIMIT @max::int;
		`, nil)
		require.NoError(t, err)
		require.Nil(t, args)
		require.Equal(t, `
			-- name: JobList :many
			SELECT *
			FROM river_job
			WHERE kind = 'no_op'
			ORDER BY kind, id
			LIMIT @max::int;
		`, updatedSQL)
	})

	t.Run("RepeatedTemplate", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"schema": {Value: "test_schema."},
		}, nil)

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job r1
				INNER JOIN /* TEMPLATE: schema */river_job r2 ON r1.id = r2.id;
		`, nil)
		require.NoError(t, err)
		require.Nil(t, args)
		require.Equal(t, `
			SELECT count(*)
			FROM test_schema.river_job r1
				INNER JOIN test_schema.river_job r2 ON r1.id = r2.id;
		`, updatedSQL)
	})

	t.Run("AllTemplatesStableCached", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"schema": {Stable: true, Value: "test_schema."},
		}, nil)

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job;
		`, nil)
		require.NoError(t, err)
		require.Nil(t, args)
		require.Equal(t, `
			SELECT count(*)
			FROM test_schema.river_job;
		`, updatedSQL)

		require.Len(t, replacer.cache, 1)

		// Invoke again to make sure we get back the same result.
		updatedSQL, args, err = replacer.RunSafely(ctx, `
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job;
		`, nil)
		require.NoError(t, err)
		require.Nil(t, args)
		require.Equal(t, `
			SELECT count(*)
			FROM test_schema.river_job;
		`, updatedSQL)
	})

	t.Run("AnyTemplateNotStableNotCached", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"schema":       {Stable: true, Value: "test_schema."},
			"where_clause": {Value: "kind = 'no_op'"},
		}, nil)

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job
			WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */;
		`, nil)
		require.NoError(t, err)
		require.Nil(t, args)
		require.Equal(t, `
			SELECT count(*)
			FROM test_schema.river_job
			WHERE kind = 'no_op';
		`, updatedSQL)

		require.Empty(t, replacer.cache)
	})

	t.Run("CacheBasedOnInputValues", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		// SQL stays constant across all runs.
		const sql = `
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job
			WHERE kind = @kind
				AND state = @state;
			`

		// Initially cached value
		{
			ctx := WithReplacements(ctx, map[string]Replacement{
				"schema": {Stable: true, Value: "test_schema."},
			}, nil)

			_, _, err := replacer.RunSafely(ctx, sql, nil)
			require.NoError(t, err)
		}
		require.Len(t, replacer.cache, 1)

		// Same SQL, but new value.
		{
			ctx := WithReplacements(ctx, map[string]Replacement{
				"schema": {Stable: true, Value: "other_schema."},
			}, nil)

			_, _, err := replacer.RunSafely(ctx, sql, nil)
			require.NoError(t, err)
		}
		require.Len(t, replacer.cache, 2)

		// Named arg added to the mix.
		{
			ctx := WithReplacements(ctx, map[string]Replacement{
				"schema": {Stable: true, Value: "test_schema."},
			}, map[string]any{
				"kind": "kind_value",
			})

			_, _, err := replacer.RunSafely(ctx, sql, nil)
			require.NoError(t, err)
		}
		require.Len(t, replacer.cache, 3)

		// Different named arg _value_ (i.e. still same named arg) can still use
		// the previous cached SQL.
		{
			ctx := WithReplacements(ctx, map[string]Replacement{
				"schema": {Stable: true, Value: "test_schema."},
			}, map[string]any{
				"kind": "other_kind_value",
			})

			_, _, err := replacer.RunSafely(ctx, sql, nil)
			require.NoError(t, err)
		}
		require.Len(t, replacer.cache, 3) // unchanged

		// New named arg adds a new cache value.
		{
			ctx := WithReplacements(ctx, map[string]Replacement{
				"schema": {Stable: true, Value: "test_schema."},
			}, map[string]any{
				"kind":  "kind_value",
				"state": "state_value",
			})

			_, _, err := replacer.RunSafely(ctx, sql, nil)
			require.NoError(t, err)
		}
		require.Len(t, replacer.cache, 4)

		// Different input SQL.
		{
			ctx := WithReplacements(ctx, map[string]Replacement{
				"schema": {Stable: true, Value: "test_schema."},
			}, nil)

			_, _, err := replacer.RunSafely(ctx, `
			SELECT /* TEMPLATE: schema */river_job;
		`, nil)
			require.NoError(t, err)
		}
		require.Len(t, replacer.cache, 5)
	})

	t.Run("NamedArgsNoInitialArgs", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"where_clause": {Value: "kind = @kind"},
		}, map[string]any{
			"kind": "no_op",
		})

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			SELECT count(*)
			FROM river_job
			WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */;
		`, nil)
		require.NoError(t, err)
		require.Equal(t, []any{"no_op"}, args)
		require.Equal(t, `
			SELECT count(*)
			FROM river_job
			WHERE kind = $1;
		`, updatedSQL)
	})

	t.Run("NamedArgsWithInitialArgs", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"where_clause": {Value: "kind = @kind"},
		}, map[string]any{
			"kind": "no_op",
		})

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			SELECT count(*)
			FROM river_job
			WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */
				AND status = $1;
		`, []any{"succeeded"})
		require.NoError(t, err)
		require.Equal(t, []any{"succeeded", "no_op"}, args)
		require.Equal(t, `
			SELECT count(*)
			FROM river_job
			WHERE kind = $2
				AND status = $1;
		`, updatedSQL)
	})

	t.Run("MultipleWithReplacementsOverrides", func(t *testing.T) {
		t.Parallel()

		replacer, _ := setup(t)

		ctx := WithReplacements(ctx, map[string]Replacement{
			"schema":       {Stable: true, Value: "test_schema."},
			"where_clause": {Value: "kind = @kind"},
		}, map[string]any{
			"kind": "no_op",
		})

		ctx = WithReplacements(ctx, map[string]Replacement{
			"where_clause": {Value: "kind = @kind AND status = @status"},
		}, map[string]any{
			"status": "succeeded",
		})

		updatedSQL, args, err := replacer.RunSafely(ctx, `
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job
			WHERE /* TEMPLATE_BEGIN: where_clause */ 1 /* TEMPLATE_END */;
		`, nil)
		require.NoError(t, err)
		require.Equal(t, []any{"no_op", "succeeded"}, args)
		require.Equal(t, `
			SELECT count(*)
			FROM test_schema.river_job
			WHERE kind = $1 AND status = $2;
		`, updatedSQL)
	})

	t.Run("Stress", func(t *testing.T) {
		t.Parallel()

		const (
			clearCacheIterations = 10
			numIterations        = 50
		)

		replacer, _ := setup(t)

		periodicallyClearCache := func(i int, replacer *Replacer) {
			if i+1%clearCacheIterations == 0 { // +1 so we don't clear cache on i == 0
				replacer.cacheMu.Lock()
				replacer.cache = nil
				replacer.cacheMu.Unlock()
			}
		}

		var wg sync.WaitGroup

		wg.Add(1)
		go func() {
			defer wg.Done()

			for i := range numIterations {
				ctx := WithReplacements(ctx, map[string]Replacement{
					"schema": {Value: "test_schema."},
				}, nil)

				updatedSQL, _, err := replacer.RunSafely(ctx, `
			SELECT count(*) FROM /* TEMPLATE: schema */river_job;
		`, nil)
				require.NoError(t, err)
				require.Equal(t, `
			SELECT count(*) FROM test_schema.river_job;
		`, updatedSQL)

				periodicallyClearCache(i, replacer)
			}
		}()

		wg.Add(1)
		go func() {
			defer wg.Done()

			for i := range numIterations {
				ctx := WithReplacements(ctx, map[string]Replacement{
					"schema": {Stable: true, Value: "test_schema."},
				}, nil)

				updatedSQL, _, err := replacer.RunSafely(ctx, `
			SELECT distinct(kind) FROM /* TEMPLATE: schema */river_job;
		`, nil)
				require.NoError(t, err)
				require.Equal(t, `
			SELECT distinct(kind) FROM test_schema.river_job;
		`, updatedSQL)

				periodicallyClearCache(i, replacer)
			}
		}()

		wg.Add(1)
		go func() {
			defer wg.Done()

			for i := range numIterations {
				ctx := WithReplacements(ctx, map[string]Replacement{
					"schema": {Stable: true, Value: "test_schema."},
				}, nil)

				updatedSQL, _, err := replacer.RunSafely(ctx, `
			SELECT count(*) FROM /* TEMPLATE: schema */river_job WHERE status = 'succeeded';
		`, nil)
				require.NoError(t, err)
				require.Equal(t, `
			SELECT count(*) FROM test_schema.river_job WHERE status = 'succeeded';
		`, updatedSQL)

				periodicallyClearCache(i, replacer)
			}
		}()

		wg.Wait()
	})
}

func BenchmarkReplacer(b *testing.B) {
	ctx := context.Background()

	runReplacer := func(b *testing.B, replacer *Replacer, stable bool) {
		b.Helper()

		ctx := WithReplacements(ctx, map[string]Replacement{
			"schema": {Stable: stable, Value: "test_schema."},
		}, nil)

		_, _, err := replacer.RunSafely(ctx, `
			-- name: JobCountByState :one
			SELECT count(*)
			FROM /* TEMPLATE: schema */river_job
			WHERE state = @state;
		`, nil)
		require.NoError(b, err)
	}

	b.Run("WithCache", func(b *testing.B) {
		var replacer Replacer
		for range b.N {
			runReplacer(b, &replacer, true)
		}
	})

	b.Run("WithoutCache", func(b *testing.B) {
		var replacer Replacer
		for range b.N {
			runReplacer(b, &replacer, false)
		}
	})
}

```

`rivershared/startstop/main_test.go`:

```go
package startstop

import (
	"testing"

	"github.com/riverqueue/river/rivershared/riversharedtest"
)

func TestMain(m *testing.M) {
	riversharedtest.WrapTestMain(m)
}

```

`rivershared/startstop/start_stop.go`:

```go
package startstop

import (
	"context"
	"errors"
	"sync"
)

// ErrStop is an error injected into WithCancelCause when context is canceled
// because a service is stopping. Makes it possible to differentiate a
// controlled stop from a context cancellation.
var ErrStop = errors.New("service stopped")

// Service is a generalized interface for a service that starts and stops,
// usually one backed by embedding BaseStartStop.
type Service interface {
	// Start starts a service. Services are responsible for backgrounding
	// themselves, so this function should be invoked synchronously. Services
	// may return an error if they have trouble starting up, so the caller
	// should wait and respond to the error if necessary.
	Start(ctx context.Context) error

	// Started returns a channel that's closed when a service finishes starting,
	// or if failed to start and is stopped instead. It can be used in
	// conjunction with WaitAllStarted to verify startup of a constellation of
	// services.
	Started() <-chan struct{}

	// Stop stops a service. Services are responsible for making sure their stop
	// is complete before returning so a caller can wait on this invocation
	// synchronously and be guaranteed the service is fully stopped. Services
	// are expected to be able to tolerate (1) being stopped without having been
	// started, and (2) being double-stopped.
	Stop()
}

// ServiceWithStopped is a Service that can also return a Stopped channel. I've
// kept this as a separate interface for the time being because I'm not sure
// this is strictly necessary to be part of startstop.
type serviceWithStopped interface {
	Service

	// Stopped returns a channel that can be waited on for the service to be
	// stopped. This function is only safe to invoke after successfully waiting on a
	// service's Start, and a reference to it must be taken _before_ invoking Stop.
	Stopped() <-chan struct{}
}

// BaseStartStop is a helper that can be embedded on a queue maintenance service
// and which will provide the basic necessities to safely implement the Service
// interface in a way that's not racy and can tolerate a number of edge cases.
// It's packaged separately so that it doesn't leak its internal variables into
// services that use it.
//
// Services should implement their own Start function which invokes StartInit
// first thing, return if told not to start, spawn a goroutine with their main
// run block otherwise, and make sure to defer a close on the stop channel
// returned by StartInit within that goroutine.
//
// A Stop implementation is provided automatically and it's not necessary to
// override it.
type BaseStartStop struct {
	cancelFunc context.CancelCauseFunc
	isRunning  bool
	mu         sync.Mutex
	started    chan struct{}
	stopped    chan struct{}
}

// StartInit should be invoked at the beginning of a service's Start function.
// It returns a context for the service to use, a boolean indicating whether it
// should start (which will be false if the service is already started), and a
// stopped channel. Services should defer a close on the stop channel in their
// main run loop.
//
//	func (s *Service) Start(ctx context.Context) error {
//	    ctx, shouldStart, stopped := s.StartInit(ctx)
//	    if !shouldStart {
//	        return nil
//	    }
//
//	    go func() {
//	        defer close(stopped)
//
//	        <-ctx.Done()
//
//	        ...
//	    }()
//
//	    return nil
//	}
//
// Be careful to also close it in the event of startup errors, otherwise a
// service that failed to start once will never be able to start up.
//
//	ctx, shouldStart, stopped := s.StartInit(ctx)
//	if !shouldStart {
//	    return nil
//	}
//
//	if err := possibleStartUpError(); err != nil {
//	    close(stopped)
//	    return err
//	}
//
//	...
func (s *BaseStartStop) StartInit(ctx context.Context) (context.Context, bool, func(), func()) {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.isRunning {
		return ctx, false, nil, nil
	}

	s.isRunning = true

	// Only allocate a started or stopped channels when not preallocated by
	// Started or Stopped.
	if s.started == nil {
		s.started = make(chan struct{})
	}
	if s.stopped == nil {
		s.stopped = make(chan struct{})
	}

	ctx, s.cancelFunc = context.WithCancelCause(ctx)

	closeStartedOnce := sync.OnceFunc(func() { close(s.started) })

	return ctx, true, closeStartedOnce, func() {
		// Also close the started channel (in case it wasn't already), just in
		// case `started()` was never invoked and someone is waiting on it.
		closeStartedOnce()

		close(s.stopped)
	}
}

// Started returns a channel that's closed when a service finishes starting, or
// if failed to start and is stopped instead. It can be used in conjunction with
// WaitAllStarted to verify startup of a constellation of services.
func (s *BaseStartStop) Started() <-chan struct{} {
	s.mu.Lock()
	defer s.mu.Unlock()

	// If the call to Started is before the service was actually started,
	// preallocate the started channel so that regardless of whether the wait
	// started before or after the service started, it will still do the right
	// thing.
	if s.started == nil {
		s.started = make(chan struct{})
	}

	return s.started
}

// Stop is an automatically provided implementation for the maintenance Service
// interface's Stop.
func (s *BaseStartStop) Stop() {
	shouldStop, stopped, finalizeStop := s.StopInit()
	if !shouldStop {
		return
	}

	<-stopped
	finalizeStop(true)
}

// StopInit provides a way to build a more customized Stop implementation. It
// should be avoided unless there's an exceptional reason not to because Stop
// should be fine in the vast majority of situations.
//
// It returns a boolean indicating whether the service should do any additional
// work to stop (false is returned if the service was never started), a stopped
// channel to wait on for full stop, and a finalizeStop function that should be
// deferred in the stop function to ensure that locks are cleaned up and the
// struct is reset after stopping.
//
//	func (s *Service) Stop(ctx context.Context) error {
//	    shouldStop, stopped, finalizeStop := s.StopInit(ctx)
//	    if !shouldStop {
//	        return
//	    }
//
//	    defer finalizeStop(true)
//
//	    ...
//	}
//
// finalizeStop takes a boolean which indicates where the service should indeed
// be considered stopped. This should usually be true, but callers can pass
// false to cancel the stop action, keeping the service from starting again, and
// potentially allowing the service to try another stop.
func (s *BaseStartStop) StopInit() (bool, <-chan struct{}, func(didStop bool)) {
	s.mu.Lock()

	// Tolerate being told to stop without having been started.
	if !s.isRunning {
		s.mu.Unlock()
		return false, nil, func(didStop bool) {}
	}

	s.cancelFunc(ErrStop)

	return true, s.stopped, func(didStop bool) {
		defer s.mu.Unlock()
		if didStop {
			s.isRunning = false
			s.started = nil
			s.stopped = nil
		}
	}
}

// Stopped returns a channel that can be waited on for the service to be
// stopped. This function may be used to return a stopped channel before a
// service is started or while it's running, but a reference to it must be taken
// _before_ invoking Stop.
func (s *BaseStartStop) Stopped() <-chan struct{} {
	s.mu.Lock()
	defer s.mu.Unlock()

	// If the call to Stopped is before the service was actually started,
	// preallocate the stopped channel so that regardless of whether the wait
	// started before or after the service started, it will still do the right
	// thing.
	if s.stopped == nil {
		s.stopped = make(chan struct{})
	}

	return s.stopped
}

// StoppedWithoutLock returns a channel that can be waited on for the service to
// be stopped.
//
// Unlike Stopped, this returns the struct's internal channel directly without
// preallocation and without taking a lock on the mutex (making it safe to call
// while StopInit is ongoing). Most users of BaseStartStop shouldn't use this
// variant and it basically exists for river.Client so it can provide slightly
// different stop channel semantics compared to BaseStartStop's.
func (s *BaseStartStop) StoppedUnsafe() <-chan struct{} { return s.stopped }

type startStopFunc struct {
	BaseStartStop
	startFunc func(ctx context.Context, shouldStart bool, started, stopped func()) error
}

// StartStopFunc produces a `startstop.Service` from a function. It's useful for
// very small services that don't necessarily need a whole struct defined for
// them.
func StartStopFunc(startFunc func(ctx context.Context, shouldStart bool, started, stopped func()) error) *startStopFunc {
	return &startStopFunc{
		startFunc: startFunc,
	}
}

func (s *startStopFunc) Start(ctx context.Context) error {
	return s.startFunc(s.StartInit(ctx))
}

// StartAll starts all given services. If any service returns an error while
// being started, that error is returned, and any services that were started
// successfully up to that point are stopped.
func StartAll(ctx context.Context, services ...Service) error {
	for i, service := range services {
		if err := service.Start(ctx); err != nil {
			StopAllParallel(services[0:i]...)

			return err
		}
	}
	return nil
}

// StopAllParallel stops all the given services in parallel and waits until
// they've all stopped successfully.
func StopAllParallel(services ...Service) {
	var wg sync.WaitGroup
	wg.Add(len(services))

	for i := range services {
		service := services[i]
		go func() {
			defer wg.Done()
			service.Stop()
		}()
	}

	wg.Wait()
}

// WaitAllStarted waits until all the given services are started (or stopped in
// a degenerate start scenario, like if context is cancelled while starting up).
//
// Unlike StopAllParallel, WaitAllStarted doesn't bother with parallelism
// because the services themselves have already backgrounded themselves, and we
// have to wait until the slowest service has started anyway.
func WaitAllStarted(services ...Service) {
	<-WaitAllStartedC(services...)
}

// WaitAllStartedC waits until all the given services are started (or stopped in
// a degenerate start scenario, like if context is cancelled while starting up).
//
// This variant returns a channel so that a caller can apply a timeout branch
// with `select` if they'd like. For the most part this shouldn't be needed
// though, as long as each service individually is confirmed to be able to start
// and stop itself in a healthy way. (i.e. Never dies for any reason before
// managing to call `started()` or `stopped()`).
//
// Unlike StopAllParallel, WaitAllStartedC doesn't bother with parallelism
// because the services themselves have already background themselves, and we
// have to wait until the slowest service has started anyway.
func WaitAllStartedC(services ...Service) <-chan struct{} {
	allStarted := make(chan struct{})

	go func() {
		defer close(allStarted)
		for _, service := range services {
			<-service.Started()
		}
	}()

	return allStarted
}

```

`rivershared/startstop/start_stop_test.go`:

```go
package startstop

import (
	"context"
	"errors"
	"sync"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riversharedtest"
)

type sampleService struct {
	baseservice.BaseService
	BaseStartStop

	// Optional error that may be returned on startup.
	startErr error

	// Some simple state in the service which a started service taints. The
	// purpose of this variable is to allow us to detect a data race allowed by
	// BaseStartStop.
	state bool
}

func (s *sampleService) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	if s.startErr != nil {
		stopped()
		return s.startErr
	}

	go func() {
		// Set this before confirming started.
		s.state = true

		started()
		defer stopped()

		<-ctx.Done()
	}()

	return nil
}

func testService(t *testing.T, newService func(t *testing.T) serviceWithStopped) {
	t.Helper()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (serviceWithStopped, *testBundle) {
		t.Helper()

		return newService(t), &testBundle{}
	}

	t.Run("StopAndStart", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		require.NoError(t, service.Start(ctx))
		service.Stop()
	})

	t.Run("DoubleStop", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		require.NoError(t, service.Start(ctx))
		service.Stop()
		service.Stop()
	})

	t.Run("StopWithoutStart", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		service.Stop()
	})

	t.Run("StartedChannel", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		riversharedtest.WaitOrTimeout(t, service.Started())
	})

	t.Run("StoppedChannel", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		require.NoError(t, service.Start(ctx))

		// A reference to stopped must be procured _before_ stopping the service
		// because the stopped channel is deinitialized as part of the stop
		// procedure.
		stopped := service.Stopped()
		service.Stop()
		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		var wg sync.WaitGroup

		for range 10 {
			wg.Add(1)
			go func() {
				for range 50 {
					require.NoError(t, service.Start(ctx))
					service.Stop()
				}
				wg.Done()
			}()
		}

		wg.Wait()
	})

	t.Run("StartedPreallocated", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		// Make sure we get the start channel before the service is started.
		started := service.Started()

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		riversharedtest.WaitOrTimeout(t, started)
	})
}

func TestBaseStartStop(t *testing.T) {
	t.Parallel()

	testService(t, func(t *testing.T) serviceWithStopped { t.Helper(); return &sampleService{} })
}

func TestBaseStartStopFunc(t *testing.T) {
	t.Parallel()

	makeFunc := func(t *testing.T) serviceWithStopped {
		t.Helper()

		// Some simple state in the service which a started service taints. The
		// purpose of this variable is to allow us to detect a data race allowed by
		// BaseStartStop.
		var state bool

		return StartStopFunc(func(ctx context.Context, shouldStart bool, started, stopped func()) error {
			if !shouldStart {
				return nil
			}

			go func() {
				started()
				defer stopped()
				state = true
				t.Logf("State: %t", state) // here so variable doesn't register as unused
				<-ctx.Done()
			}()

			return nil
		})
	}

	testService(t, makeFunc)
}

func TestErrStop(t *testing.T) {
	t.Parallel()

	var workCtx context.Context

	startStop := StartStopFunc(func(ctx context.Context, shouldStart bool, started, stopped func()) error {
		if !shouldStart {
			return nil
		}

		workCtx = ctx //nolint:fatcontext

		go func() {
			started()
			defer stopped()
			<-ctx.Done()
		}()

		return nil
	})

	ctx := context.Background()

	require.NoError(t, startStop.Start(ctx))
	<-startStop.Started()
	startStop.Stop()
	require.ErrorIs(t, context.Cause(workCtx), ErrStop)
}

// BaseStartStop tests that need specific internal implementation (like ones we
// can add to sampleService) to be able to verify.
func TestSampleService(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (*sampleService, *testBundle) { //nolint:unparam
		t.Helper()

		return &sampleService{}, &testBundle{}
	}

	t.Run("StartedChannel", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		riversharedtest.WaitOrTimeout(t, service.Started())
		require.True(t, service.state)
	})

	t.Run("StartError", func(t *testing.T) {
		t.Parallel()

		service, _ := setup(t)
		service.startErr = errors.New("error on start")

		require.ErrorIs(t, service.Start(ctx), service.startErr)

		riversharedtest.WaitOrTimeout(t, service.Started()) // start channel also closed on erroneous start
		riversharedtest.WaitOrTimeout(t, service.Stopped())
	})
}

// A service with the more unusual case.
type sampleServiceWithStopInit struct {
	baseservice.BaseService
	BaseStartStop

	didStop bool

	// Some simple state in the service which a started service taints. The
	// purpose of this variable is to allow us to detect a data race allowed by
	// BaseStartStop.
	state bool
}

func (s *sampleServiceWithStopInit) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	go func() {
		started()
		defer stopped()
		s.state = true
		<-ctx.Done()
	}()

	return nil
}

func (s *sampleServiceWithStopInit) Stop() {
	shouldStop, stopped, finalizeStop := s.StopInit()
	if !shouldStop {
		return
	}

	<-stopped
	finalizeStop(s.didStop)
}

func TestWithStopInit(t *testing.T) {
	t.Parallel()

	testService(t, func(t *testing.T) serviceWithStopped {
		t.Helper()
		return &sampleServiceWithStopInit{didStop: true}
	})

	ctx := context.Background()

	type testBundle struct{}

	setup := func() (*sampleServiceWithStopInit, *testBundle) {
		return &sampleServiceWithStopInit{}, &testBundle{}
	}

	t.Run("FinalizeDidStop", func(t *testing.T) {
		t.Parallel()

		service, _ := setup()
		service.didStop = true // will set stopped

		require.NoError(t, service.Start(ctx))

		service.Stop()

		require.Nil(t, service.started)
		require.Nil(t, service.stopped)
	})

	t.Run("FinalizeDidNotStop", func(t *testing.T) {
		t.Parallel()

		service, _ := setup()
		service.didStop = false // will NOT set stopped

		require.NoError(t, service.Start(ctx))

		service.Stop()

		// service is still started because didStop was set to false
		require.NotNil(t, service.started)
		require.NotNil(t, service.stopped)
	})
}

func TestStopped(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("AllocatesOnStart", func(t *testing.T) {
		t.Parallel()

		service := &sampleService{}

		require.Nil(t, service.stopped)

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		stopped := service.Stopped()
		require.NotNil(t, stopped)
		require.NotNil(t, service.stopped)

		service.Stop()

		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("PreallocatesBeforeStart", func(t *testing.T) {
		t.Parallel()

		service := &sampleService{}

		stopped := service.Stopped()

		require.NotNil(t, stopped)
		require.NotNil(t, service.stopped)

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		service.Stop()

		riversharedtest.WaitOrTimeout(t, stopped)
	})
}

func TestStoppedUnsafe(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("AllocatesOnStart", func(t *testing.T) {
		t.Parallel()

		service := &sampleService{}

		require.Nil(t, service.stopped)

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		stopped := service.StoppedUnsafe()
		require.NotNil(t, stopped)
		require.NotNil(t, service.stopped)

		service.Stop()

		riversharedtest.WaitOrTimeout(t, stopped)
	})

	t.Run("NotPreallocatedBeforeStart", func(t *testing.T) {
		t.Parallel()

		service := &sampleService{}

		require.Nil(t, service.StoppedUnsafe())
	})
}

func TestStartAll(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("StartsAllServices", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{}
		)

		t.Cleanup(service1.Stop)
		t.Cleanup(service2.Stop)
		t.Cleanup(service3.Stop)

		err := StartAll(ctx, service1, service2, service3)
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, WaitAllStartedC(service1, service2, service3))
	})

	t.Run("ReturnsFirstError", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{startErr: errors.New("a start error")}
		)

		t.Cleanup(service1.Stop)
		t.Cleanup(service2.Stop)
		t.Cleanup(service3.Stop)

		// References must be invoked before anything is stopped.
		var (
			stopped1 = service1.Stopped()
			stopped2 = service2.Stopped()
		)

		err := StartAll(ctx, service1, service2, service3)
		require.EqualError(t, err, "a start error")

		// The first two services should have been stopped after the third
		// service failed to start.
		riversharedtest.WaitOrTimeout(t, stopped1)
		riversharedtest.WaitOrTimeout(t, stopped2)
	})

	// Same as the above except with only a single service. Exists to make sure
	// that there's nothing wrong with the way we're indexing a slice of
	// services when stopping on error.
	t.Run("ErrorWithOneService", func(t *testing.T) {
		t.Parallel()

		service := &sampleService{startErr: errors.New("a start error")}

		t.Cleanup(service.Stop)

		err := StartAll(ctx, service)
		require.EqualError(t, err, "a start error")
	})
}

func TestStarted(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("AllocatesOnStart", func(t *testing.T) {
		t.Parallel()

		service := &sampleService{}

		require.Nil(t, service.started)

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		require.NotNil(t, service.started)

		riversharedtest.WaitOrTimeout(t, service.Started())
	})

	t.Run("PreallocatesBeforeStart", func(t *testing.T) {
		t.Parallel()

		service := &sampleService{}

		started := service.Started()

		require.NotNil(t, started)
		require.NotNil(t, service.started)

		require.NoError(t, service.Start(ctx))
		t.Cleanup(service.Stop)

		riversharedtest.WaitOrTimeout(t, started)
	})
}

func TestStopAllParallel(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("Started", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{}
		)

		require.NoError(t, service1.Start(ctx))
		require.NoError(t, service2.Start(ctx))
		require.NoError(t, service3.Start(ctx))

		var (
			stopped1 = service1.Stopped()
			stopped2 = service2.Stopped()
			stopped3 = service3.Stopped()
		)

		StopAllParallel(
			service1,
			service2,
			service3,
		)

		riversharedtest.WaitOrTimeout(t, stopped1)
		riversharedtest.WaitOrTimeout(t, stopped2)
		riversharedtest.WaitOrTimeout(t, stopped3)
	})

	// We can't use the stopped channels in this case because they're only
	// initiated when a service is started.
	t.Run("NotStarted", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{}
		)

		StopAllParallel(
			service1,
			service2,
			service3,
		)
	})
}

func TestWaitAllStarted(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("WaitsForStart", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{}
		)

		require.NoError(t, service1.Start(ctx))
		require.NoError(t, service2.Start(ctx))
		require.NoError(t, service3.Start(ctx))

		t.Cleanup(service1.Stop)
		t.Cleanup(service2.Stop)
		t.Cleanup(service3.Stop)

		WaitAllStarted(service1, service2, service3)

		require.True(t, service1.state)
		require.True(t, service2.state)
		require.True(t, service3.state)
	})

	t.Run("WithStartError", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{startErr: errors.New("error on start")}
		)

		require.NoError(t, service1.Start(ctx))
		require.NoError(t, service2.Start(ctx))
		require.ErrorIs(t, service3.Start(ctx), service3.startErr)

		t.Cleanup(service1.Stop)
		t.Cleanup(service2.Stop)
		t.Cleanup(service3.Stop)

		WaitAllStarted(service1, service2, service3)
	})
}

func TestWaitAllStartedC(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("WaitsForStart", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{}
		)

		require.NoError(t, service1.Start(ctx))
		require.NoError(t, service2.Start(ctx))
		require.NoError(t, service3.Start(ctx))

		t.Cleanup(service1.Stop)
		t.Cleanup(service2.Stop)
		t.Cleanup(service3.Stop)

		riversharedtest.WaitOrTimeout(t, WaitAllStartedC(service1, service2, service3))

		require.True(t, service1.state)
		require.True(t, service2.state)
		require.True(t, service3.state)
	})

	t.Run("WithStartError", func(t *testing.T) {
		t.Parallel()

		var (
			service1 = &sampleService{}
			service2 = &sampleService{}
			service3 = &sampleService{startErr: errors.New("error on start")}
		)

		require.NoError(t, service1.Start(ctx))
		require.NoError(t, service2.Start(ctx))
		require.ErrorIs(t, service3.Start(ctx), service3.startErr)

		t.Cleanup(service1.Stop)
		t.Cleanup(service2.Stop)
		t.Cleanup(service3.Stop)

		riversharedtest.WaitOrTimeout(t, WaitAllStartedC(service1, service2, service3))
	})
}

```

`rivershared/startstoptest/startstoptest.go`:

```go
package startstoptest

import (
	"context"
	"errors"
	"strings"
	"sync"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/startstop"
)

// Stress is a test helper that puts stress on a service's start and stop
// functions so that we can detect any data races that it might have due to
// improper use of BaseStopStart.
func Stress(ctx context.Context, tb testingT, svc startstop.Service) {
	StressErr(ctx, tb, svc, nil)
}

// StressErr is the same as Stress except that the given allowedStartErr is
// tolerated on start (either no error or an error that is allowedStartErr is
// allowed). This is useful for services that may want to return an error if
// they're shut down as they're still starting up.
func StressErr(ctx context.Context, tb testingT, svc startstop.Service, allowedStartErr error) { //nolint:varnamelen
	tb.Helper()

	var wg sync.WaitGroup

	isAllowedStartError := func(err error) bool {
		if allowedStartErr != nil {
			if errors.Is(err, allowedStartErr) {
				return true
			}
		}

		// Always allow this one because a fairly common intermittent failure is
		// to produce an I/O timeout while trying to connect to Postgres.
		//
		//     write failed: write tcp 127.0.0.1:60976->127.0.0.1:5432: i/o timeout
		//
		if strings.HasSuffix(err.Error(), "i/o timeout") {
			return true
		}

		return false
	}

	for range 10 {
		wg.Add(1)
		go func() {
			defer wg.Done()

			for range 50 {
				err := svc.Start(ctx)
				if err != nil && !isAllowedStartError(err) {
					require.NoError(tb, err)
				}

				stopped := make(chan struct{})

				go func() {
					defer close(stopped)
					svc.Stop()
				}()

				select {
				case <-stopped:
				case <-time.After(5 * time.Second):
					require.FailNow(tb, "Timed out waiting for service to stop")
				}
			}
		}()
	}

	wg.Wait()
}

// Minimal interface for *testing.B/*testing.T that lets us test a failure
// condition for our test helpers above.
type testingT interface {
	Errorf(format string, args ...interface{})
	FailNow()
	Helper()
}

```

`rivershared/startstoptest/startstoptest_test.go`:

```go
package startstoptest

import (
	"context"
	"errors"
	"log/slog"
	"sync/atomic"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstop"
)

type MyService struct {
	startstop.BaseStartStop
	logger   *slog.Logger
	startErr error
}

func (s *MyService) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := s.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	if s.startErr != nil {
		stopped()
		return s.startErr
	}

	go func() {
		started()
		defer stopped()

		s.logger.DebugContext(ctx, "Service started")
		defer s.logger.DebugContext(ctx, "Service stopped")

		<-ctx.Done()
	}()

	return nil
}

func TestStress(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	Stress(ctx, t, &MyService{logger: riversharedtest.Logger(t)})
}

func TestStressErr(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	startErr := errors.New("error returned on start")

	StressErr(ctx, t, &MyService{logger: riversharedtest.Logger(t), startErr: startErr}, startErr)

	mockT := newMockTestingT(t)
	StressErr(ctx, mockT, &MyService{logger: riversharedtest.Logger(t), startErr: errors.New("different error")}, startErr)
	require.True(t, mockT.failed.Load())
}

type mockTestingT struct {
	failed atomic.Bool
	tb     testing.TB
}

func newMockTestingT(tb testing.TB) *mockTestingT {
	tb.Helper()
	return &mockTestingT{tb: tb}
}

func (t *mockTestingT) Errorf(format string, args ...interface{}) {}
func (t *mockTestingT) FailNow()                                  { t.failed.Store(true) }
func (t *mockTestingT) Helper()                                   { t.tb.Helper() }

```

`rivershared/testfactory/test_factory.go`:

```go
// Package testfactory provides low level helpers for inserting records directly
// into the database.
package testfactory

import (
	"context"
	"encoding/json"
	"fmt"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

type JobOpts struct {
	Attempt      *int
	AttemptedAt  *time.Time
	AttemptedBy  []string
	CreatedAt    *time.Time
	EncodedArgs  []byte
	Errors       [][]byte
	FinalizedAt  *time.Time
	Kind         *string
	MaxAttempts  *int
	Metadata     json.RawMessage
	Priority     *int
	Queue        *string
	ScheduledAt  *time.Time
	State        *rivertype.JobState
	Tags         []string
	UniqueKey    []byte
	UniqueStates byte
}

func Job(ctx context.Context, tb testing.TB, exec riverdriver.Executor, opts *JobOpts) *rivertype.JobRow {
	tb.Helper()

	job, err := exec.JobInsertFull(ctx, Job_Build(tb, opts))
	require.NoError(tb, err)
	return job
}

func Job_Build(tb testing.TB, opts *JobOpts) *riverdriver.JobInsertFullParams { //nolint:stylecheck
	tb.Helper()

	encodedArgs := opts.EncodedArgs
	if opts.EncodedArgs == nil {
		encodedArgs = []byte("{}")
	}

	finalizedAt := opts.FinalizedAt
	if finalizedAt == nil && (opts.State != nil && (*opts.State == rivertype.JobStateCompleted ||
		*opts.State == rivertype.JobStateCancelled ||
		*opts.State == rivertype.JobStateDiscarded)) {
		finalizedAt = ptrutil.Ptr(time.Now())
	}

	metadata := opts.Metadata
	if opts.Metadata == nil {
		metadata = []byte("{}")
	}

	tags := opts.Tags
	if tags == nil {
		tags = []string{}
	}

	return &riverdriver.JobInsertFullParams{
		Attempt:      ptrutil.ValOrDefault(opts.Attempt, 0),
		AttemptedAt:  opts.AttemptedAt,
		AttemptedBy:  opts.AttemptedBy,
		CreatedAt:    opts.CreatedAt,
		EncodedArgs:  encodedArgs,
		Errors:       opts.Errors,
		FinalizedAt:  finalizedAt,
		Kind:         ptrutil.ValOrDefault(opts.Kind, "fake_job"),
		MaxAttempts:  ptrutil.ValOrDefault(opts.MaxAttempts, rivercommon.MaxAttemptsDefault),
		Metadata:     metadata,
		Priority:     ptrutil.ValOrDefault(opts.Priority, rivercommon.PriorityDefault),
		Queue:        ptrutil.ValOrDefault(opts.Queue, rivercommon.QueueDefault),
		ScheduledAt:  opts.ScheduledAt,
		State:        ptrutil.ValOrDefault(opts.State, rivertype.JobStateAvailable),
		Tags:         tags,
		UniqueKey:    opts.UniqueKey,
		UniqueStates: opts.UniqueStates,
	}
}

type LeaderOpts struct {
	ElectedAt *time.Time
	ExpiresAt *time.Time
	LeaderID  *string
}

func Leader(ctx context.Context, tb testing.TB, exec riverdriver.Executor, opts *LeaderOpts) *riverdriver.Leader {
	tb.Helper()

	leader, err := exec.LeaderInsert(ctx, &riverdriver.LeaderInsertParams{
		ElectedAt: opts.ElectedAt,
		ExpiresAt: opts.ExpiresAt,
		LeaderID:  ptrutil.ValOrDefault(opts.LeaderID, "test-client-id"),
		TTL:       10 * time.Second,
	})
	require.NoError(tb, err)
	return leader
}

type MigrationOpts struct {
	Line    *string
	Version *int
}

func Migration(ctx context.Context, tb testing.TB, exec riverdriver.Executor, opts *MigrationOpts) *riverdriver.Migration {
	tb.Helper()

	migration, err := exec.MigrationInsertMany(ctx,
		ptrutil.ValOrDefault(opts.Line, riverdriver.MigrationLineMain),
		[]int{ptrutil.ValOrDefaultFunc(opts.Version, nextSeq)},
	)
	require.NoError(tb, err)
	return migration[0]
}

var seq int64 = 1 //nolint:gochecknoglobals

func nextSeq() int {
	return int(atomic.AddInt64(&seq, 1))
}

type QueueOpts struct {
	Metadata  []byte
	Name      *string
	PausedAt  *time.Time
	UpdatedAt *time.Time
}

func Queue(ctx context.Context, tb testing.TB, exec riverdriver.Executor, opts *QueueOpts) *rivertype.Queue {
	tb.Helper()

	if opts == nil {
		opts = &QueueOpts{}
	}

	metadata := opts.Metadata
	if len(opts.Metadata) == 0 {
		metadata = []byte("{}")
	}

	queue, err := exec.QueueCreateOrSetUpdatedAt(ctx, &riverdriver.QueueCreateOrSetUpdatedAtParams{
		Metadata:  metadata,
		Name:      ptrutil.ValOrDefaultFunc(opts.Name, func() string { return fmt.Sprintf("queue_%05d", nextSeq()) }),
		PausedAt:  opts.PausedAt,
		UpdatedAt: opts.UpdatedAt,
	})
	require.NoError(tb, err)
	return queue
}

```

`rivershared/testsignal/test_signal.go`:

```go
package testsignal

import (
	"fmt"
	"time"

	"github.com/riverqueue/river/rivershared/riversharedtest"
)

// TestSignalWaiter provides an interface for TestSignal which only exposes
// waiting on the signal. This is useful for minimizing functionality across
// package boundaries.
type TestSignalWaiter[T any] interface {
	WaitOrTimeout() T
}

// TestSignal is a channel wrapper designed to allow tests to wait on certain
// events (to test difficult concurrent conditions without intermittency) while
// also having minimal impact on the production code that calls into it.
//
// Its default value produces a state where its safe to call Signal to signal
// into it, but where doing so will have no effect. Entities that embed it
// should by convention provide a TestSignalsInit function that tests can invoke
// and which calls Init on all member test signals, after which it becomes
// possible for tests to WaitOrTimeout on them.
type TestSignal[T any] struct {
	internalChan chan T
}

const testSignalInternalChanSize = 50

// Init initializes the test signal for use. This should only ever be called
// from tests.
func (s *TestSignal[T]) Init() {
	s.internalChan = make(chan T, testSignalInternalChanSize)
}

// Signal signals the test signal. In production where the signal hasn't been
// initialized, this no ops harmlessly. In tests, the value is written to an
// internal asynchronous channel which can be waited with WaitOrTimeout.
func (s *TestSignal[T]) Signal(val T) {
	// Occurs in the case of a raw signal that hasn't been initialized (which is
	// what should always be happening outside of tests).
	if s.internalChan == nil {
		return
	}

	select { // never block on send
	case s.internalChan <- val:
	default:
		panic("test only signal channel is full")
	}
}

// WaitC returns a channel on which a value from the test signal can be waited
// upon.
func (s TestSignal[T]) WaitC() <-chan T {
	if s.internalChan == nil {
		panic("test only signal is not initialized; called outside of tests?")
	}

	return s.internalChan
}

// WaitOrTimeout waits on the next value injected by Signal. This should only be
// used in tests, and can only be used if Init has been invoked on the test
// signal.
func (s *TestSignal[T]) WaitOrTimeout() T {
	if s.internalChan == nil {
		panic("test only signal is not initialized; called outside of tests?")
	}

	timeout := riversharedtest.WaitTimeout()

	select {
	case value := <-s.internalChan:
		return value
	case <-time.After(timeout):
		panic(fmt.Sprintf("timed out waiting on test signal after %s", timeout))
	}
}

```

`rivershared/testsignal/test_signal_test.go`:

```go
package testsignal

import (
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/riversharedtest"
)

func TestTestSignal(t *testing.T) {
	t.Parallel()

	t.Run("Uninitialized", func(t *testing.T) {
		t.Parallel()

		signal := TestSignal[struct{}]{}

		// Signal can safely be invoked any number of times.
		for range testSignalInternalChanSize + 1 {
			signal.Signal(struct{}{})
		}
	})
	t.Run("Initialized", func(t *testing.T) {
		t.Parallel()

		signal := TestSignal[struct{}]{}
		signal.Init()

		// Signal can be invoked many times, but not infinitely
		for range testSignalInternalChanSize {
			signal.Signal(struct{}{})
		}

		// Another signal will panic because the internal channel is full.
		require.PanicsWithValue(t, "test only signal channel is full", func() {
			signal.Signal(struct{}{})
		})

		// And we can now wait on all the emitted signals.
		for range testSignalInternalChanSize {
			signal.WaitOrTimeout()
		}
	})

	t.Run("WaitC", func(t *testing.T) {
		t.Parallel()

		signal := TestSignal[struct{}]{}
		signal.Init()

		select {
		case <-signal.WaitC():
			require.FailNow(t, "Test signal should not have fired")
		default:
		}

		signal.Signal(struct{}{})

		select {
		case <-signal.WaitC():
		default:
			require.FailNow(t, "Test signal should have fired")
		}
	})

	t.Run("WaitOrTimeout", func(t *testing.T) {
		t.Parallel()

		signal := TestSignal[struct{}]{}
		signal.Init()

		signal.Signal(struct{}{})

		signal.WaitOrTimeout()
	})
}

// Marked as non-parallel because `t.Setenv` is not compatible with `t.Parallel`.
func TestWaitTimeout(t *testing.T) {
	t.Setenv("GITHUB_ACTIONS", "")
	require.Equal(t, 3*time.Second, riversharedtest.WaitTimeout())

	t.Setenv("GITHUB_ACTIONS", "true")
	require.Equal(t, 10*time.Second, riversharedtest.WaitTimeout())
}

```

`rivershared/util/maputil/map_util.go`:

```go
// Package maputil contains helpers related to maps, usually ones that are
// generic-related.
package maputil

// Keys creates an array of the map keys.
func Keys[K comparable, V any](in map[K]V) []K {
	result := make([]K, 0, len(in))

	for k := range in {
		result = append(result, k)
	}

	return result
}

// Values creates an array of the map values.
func Values[K comparable, V any](in map[K]V) []V {
	result := make([]V, 0, len(in))

	for _, v := range in {
		result = append(result, v)
	}

	return result
}

```

`rivershared/util/maputil/map_util_test.go`:

```go
package maputil

import (
	"sort"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestKeys(t *testing.T) {
	t.Parallel()

	is := require.New(t)

	r1 := Keys(map[string]int{"foo": 1, "bar": 2})
	sort.Strings(r1)

	is.Equal([]string{"bar", "foo"}, r1)
}

func TestValues(t *testing.T) {
	t.Parallel()

	is := require.New(t)

	r1 := Values(map[string]int{"foo": 1, "bar": 2})
	sort.Ints(r1)

	is.Equal([]int{1, 2}, r1)
}

```

`rivershared/util/ptrutil/ptr_util.go`:

```go
package ptrutil

// Ptr returns a pointer to the given value.
func Ptr[T any](v T) *T {
	return &v
}

// ValOrDefault returns the value of the given pointer as long as it's non-nil,
// and the specified default value otherwise.
func ValOrDefault[T any](ptr *T, defaultVal T) T {
	if ptr != nil {
		return *ptr
	}
	return defaultVal
}

// ValOrDefaultFunc returns the value of the given pointer as long as it's
// non-nil, or invokes the given function to produce a default value otherwise.
func ValOrDefaultFunc[T any](ptr *T, defaultFunc func() T) T {
	if ptr != nil {
		return *ptr
	}
	return defaultFunc()
}

```

`rivershared/util/ptrutil/ptr_util_test.go`:

```go
package ptrutil

import (
	"testing"

	"github.com/stretchr/testify/require"
)

func TestPtr(t *testing.T) {
	t.Parallel()

	{
		v := "s"
		require.Equal(t, &v, Ptr("s"))
	}

	{
		v := 7
		require.Equal(t, &v, Ptr(7))
	}
}

func TestValOrDefault(t *testing.T) {
	t.Parallel()

	val := "val"
	require.Equal(t, val, ValOrDefault(&val, "default"))
	require.Equal(t, "default", ValOrDefault((*string)(nil), "default"))
}

func TestValOrDefaultFunc(t *testing.T) {
	t.Parallel()

	val := "val"
	require.Equal(t, val, ValOrDefaultFunc(&val, func() string { return "default" }))
	require.Equal(t, "default", ValOrDefaultFunc((*string)(nil), func() string { return "default" }))
}

```

`rivershared/util/randutil/rand_util.go`:

```go
package randutil

import (
	cryptorand "crypto/rand"
	"encoding/hex"
	"math/rand/v2"
	"time"
)

// DurationBetween generates a random duration in the range of [lowerLimit, upperLimit).
func DurationBetween(lowerLimit, upperLimit time.Duration) time.Duration {
	return time.Duration(IntBetween(int(lowerLimit), int(upperLimit)))
}

func Hex(length int) string {
	bytes := make([]byte, length)
	if _, err := cryptorand.Read(bytes); err != nil {
		panic(err)
	}
	return hex.EncodeToString(bytes)
}

// IntBetween generates a random number in the range of [lowerLimit, upperLimit).
func IntBetween(lowerLimit, upperLimit int) int {
	return rand.IntN(upperLimit-lowerLimit) + lowerLimit
}

```

`rivershared/util/randutil/rand_util_test.go`:

```go
package randutil

import (
	cryptorand "crypto/rand"
	"math/big"
	"math/rand/v2"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
)

func TestDurationBetween(t *testing.T) {
	t.Parallel()

	const lowerLimit, upperLimit = 5 * time.Second, 8 * time.Second

	// Not exactly a super exhaustive test, but choose a relatively small range,
	// generate numbers and check they're within bounds, and run enough times
	// that we'd expect an offender to be generated if one was likely to be.
	for range int(upperLimit/time.Second-lowerLimit/time.Second) * 2 {
		n := DurationBetween(lowerLimit, upperLimit)
		require.GreaterOrEqual(t, n, lowerLimit)
		require.Less(t, n, upperLimit)
	}
}

func TestIntBetween(t *testing.T) {
	t.Parallel()

	const lowerLimit, upperLimit = 5, 8

	// Not exactly a super exhaustive test, but choose a relatively small range,
	// generate numbers and check they're within bounds, and run enough times
	// that we'd expect an offender to be generated if one was likely to be.
	for range int(upperLimit-lowerLimit) * 2 {
		n := IntBetween(lowerLimit, upperLimit)
		require.GreaterOrEqual(t, n, lowerLimit)
		require.Less(t, n, upperLimit)
	}
}

//
// On my Macbook, the non-crypto source is about ~20 faster:
//
// $ go test ./internal/util/randutil -bench Bench
// goos: darwin
// goarch: arm64
// pkg: github.com/riverqueue/river/internal/util/randutil
// BenchmarkConcurrentSafeSource-8         80612518                14.68 ns/op
// BenchmarkCryptoSource-8                  3806643               316.7 ns/op
// PASS
// ok      github.com/riverqueue/river/internal/util/randutil 3.552s
//

func BenchmarkRandV2(b *testing.B) {
	for range b.N {
		_ = rand.IntN(1984)
	}
}

func BenchmarkCryptoSource(b *testing.B) {
	intN := func(upperLimit int64) int64 {
		nBig, err := cryptorand.Int(cryptorand.Reader, big.NewInt(upperLimit))
		if err != nil {
			panic(err)
		}
		return nBig.Int64()
	}

	for range b.N {
		_ = intN(1984)
	}
}

```

`rivershared/util/serviceutil/service_util.go`:

```go
package serviceutil

import (
	"context"
	"math"
	"math/rand"
	"time"

	"github.com/riverqueue/river/rivershared/util/timeutil"
)

// CancellableSleep sleeps for the given duration, but returns early if context
// has been cancelled.
func CancellableSleep(ctx context.Context, sleepDuration time.Duration) {
	timer := time.NewTimer(sleepDuration)

	select {
	case <-ctx.Done():
		if !timer.Stop() {
			<-timer.C
		}
	case <-timer.C:
	}
}

// CancellableSleep sleeps for the given duration, but returns early if context
// has been cancelled.
//
// This variant returns a channel that should be waited on and which will be
// closed when either the sleep or context is done.
func CancellableSleepC(ctx context.Context, sleepDuration time.Duration) <-chan struct{} {
	doneChan := make(chan struct{})

	go func() {
		defer close(doneChan)
		CancellableSleep(ctx, sleepDuration)
	}()

	return doneChan
}

// MaxAttemptsBeforeResetDefault is the number of attempts during exponential
// backoff after which attempts is reset so that sleep durations aren't flung
// into a ridiculously distant future. This constant is typically injected into
// the CancellableSleepExponentialBackoff function. It could technically take
// another value instead, but shouldn't unless there's a good reason to do so.
const MaxAttemptsBeforeResetDefault = 10

// ExponentialBackoff returns a duration for a reasonable exponential backoff
// interval for a service based on the given attempt number, which can then be
// fed into CancellableSleep to perform the sleep. Uses a 2**N second algorithm,
// +/- 10% random jitter. Sleep is cancelled if the given context is cancelled.
//
// Attempt should start at one for the first backoff/failure.
func ExponentialBackoff(attempt, maxAttemptsBeforeReset int) time.Duration {
	retrySeconds := exponentialBackoffSecondsWithoutJitter(attempt, maxAttemptsBeforeReset)

	// Jitter number of seconds +/- 10%.
	retrySeconds += retrySeconds * (rand.Float64()*0.2 - 0.1)

	return timeutil.SecondsAsDuration(retrySeconds)
}

func exponentialBackoffSecondsWithoutJitter(attempt, maxAttemptsBeforeReset int) float64 {
	// It's easier for callers and more intuitive if attempt starts at one, but
	// subtract one before sending it the exponent so we start at only one
	// second of sleep instead of two.
	attempt--

	// We use a different exponential backoff algorithm here compared to the
	// default retry policy (2**N versus N**4) because it results in more
	// retries sooner. When it comes to exponential backoffs in services we
	// never want to sleep for hours/days, unlike with failed jobs.
	return math.Pow(2, float64(attempt%maxAttemptsBeforeReset))
}

```

`rivershared/util/serviceutil/service_util_test.go`:

```go
package serviceutil

import (
	"context"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
)

func TestCancellableSleep(t *testing.T) {
	t.Parallel()

	testCancellableSleep := func(t *testing.T, startSleepFunc func(ctx context.Context) <-chan struct{}) {
		t.Helper()

		ctx := context.Background()

		ctx, cancel := context.WithCancel(ctx)
		t.Cleanup(cancel)

		sleepDone := startSleepFunc(ctx)

		// Wait a very nominal amount of time just to make sure that some sleep is
		// actually happening.
		select {
		case <-sleepDone:
			require.FailNow(t, "Sleep returned sooner than expected")
		case <-time.After(50 * time.Millisecond):
		}

		cancel()

		select {
		case <-sleepDone:
		case <-time.After(50 * time.Millisecond):
			require.FailNow(t, "Timed out waiting for sleep to finish after cancel")
		}
	}

	// Starts sleep for sleep functions that don't return a channel, returning a
	// channel that's closed when sleep finishes.
	startSleep := func(sleepFunc func()) <-chan struct{} {
		sleepDone := make(chan struct{})
		go func() {
			defer close(sleepDone)
			sleepFunc()
		}()
		return sleepDone
	}

	t.Run("CancellableSleep", func(t *testing.T) {
		t.Parallel()

		testCancellableSleep(t, func(ctx context.Context) <-chan struct{} {
			return startSleep(func() {
				CancellableSleep(ctx, 5*time.Second)
			})
		})
	})

	t.Run("CancellableSleepC", func(t *testing.T) {
		t.Parallel()

		testCancellableSleep(t, func(ctx context.Context) <-chan struct{} {
			return CancellableSleepC(ctx, 5*time.Second)
		})
	})
}

func TestExponentialBackoff(t *testing.T) {
	t.Parallel()

	require.InDelta(t, 1.0, ExponentialBackoff(1, MaxAttemptsBeforeResetDefault).Seconds(), 1.0*0.1)
	require.InDelta(t, 2.0, ExponentialBackoff(2, MaxAttemptsBeforeResetDefault).Seconds(), 2.0*0.1)
	require.InDelta(t, 4.0, ExponentialBackoff(3, MaxAttemptsBeforeResetDefault).Seconds(), 4.0*0.1)
	require.InDelta(t, 8.0, ExponentialBackoff(4, MaxAttemptsBeforeResetDefault).Seconds(), 8.0*0.1)
	require.InDelta(t, 16.0, ExponentialBackoff(5, MaxAttemptsBeforeResetDefault).Seconds(), 16.0*0.1)
	require.InDelta(t, 32.0, ExponentialBackoff(6, MaxAttemptsBeforeResetDefault).Seconds(), 32.0*0.1)
}

func TestExponentialBackoffSecondsWithoutJitter(t *testing.T) {
	t.Parallel()

	require.Equal(t, 1, int(exponentialBackoffSecondsWithoutJitter(1, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 2, int(exponentialBackoffSecondsWithoutJitter(2, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 4, int(exponentialBackoffSecondsWithoutJitter(3, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 8, int(exponentialBackoffSecondsWithoutJitter(4, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 16, int(exponentialBackoffSecondsWithoutJitter(5, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 32, int(exponentialBackoffSecondsWithoutJitter(6, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 64, int(exponentialBackoffSecondsWithoutJitter(7, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 128, int(exponentialBackoffSecondsWithoutJitter(8, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 256, int(exponentialBackoffSecondsWithoutJitter(9, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 512, int(exponentialBackoffSecondsWithoutJitter(10, MaxAttemptsBeforeResetDefault)))
	require.Equal(t, 1, int(exponentialBackoffSecondsWithoutJitter(11, MaxAttemptsBeforeResetDefault))) // resets
}

```

`rivershared/util/sliceutil/slice_util.go`:

```go
// Package sliceutil contains helpers related to slices, usually ones that are
// generic-related, and are broadly useful, but which the Go core team, in its
// infinite wisdom, has decided are too much power for the  unwashed mashes, and
// therefore omitted from the utilities in `slices`.
package sliceutil

// FirstNonEmpty returns the first non-empty slice from the input, or nil if
// all input slices are empty.
func FirstNonEmpty[T any](inputs ...[]T) []T {
	for _, input := range inputs {
		if len(input) > 0 {
			return input
		}
	}
	return nil
}

// GroupBy returns an object composed of keys generated from the results of
// running each element of collection through keyFunc.
func GroupBy[T any, U comparable](collection []T, keyFunc func(T) U) map[U][]T {
	result := map[U][]T{}

	for _, item := range collection {
		key := keyFunc(item)

		result[key] = append(result[key], item)
	}

	return result
}

// KeyBy converts a slice into a map using the key/value tuples returned by
// tupleFunc. If any two pairs would have the same key, the last one wins. Go
// maps are unordered and the order of the new map isn't guaranteed to the same
// as the original slice.
func KeyBy[T any, K comparable, V any](collection []T, tupleFunc func(item T) (K, V)) map[K]V {
	result := make(map[K]V, len(collection))

	for _, t := range collection {
		k, v := tupleFunc(t)
		result[k] = v
	}

	return result
}

// Map manipulates a slice and transforms it to a slice of another type.
func Map[T any, R any](collection []T, mapFunc func(T) R) []R {
	result := make([]R, len(collection))

	for i, item := range collection {
		result[i] = mapFunc(item)
	}

	return result
}

```

`rivershared/util/sliceutil/slice_util_test.go`:

```go
package sliceutil

import (
	"fmt"
	"strconv"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestDefaultIfEmpty(t *testing.T) {
	t.Parallel()

	result1 := FirstNonEmpty([]int{1, 2, 3}, []int{4, 5, 6})
	result2 := FirstNonEmpty([]int{}, []int{4, 5, 6})
	result3 := FirstNonEmpty(nil, []int{4, 5, 6})

	require.Len(t, result1, 3)
	require.Len(t, result2, 3)
	require.Len(t, result3, 3)
	require.Equal(t, []int{1, 2, 3}, result1)
	require.Equal(t, []int{4, 5, 6}, result2)
	require.Equal(t, []int{4, 5, 6}, result3)
}

func TestGroupBy(t *testing.T) {
	t.Parallel()

	result1 := GroupBy([]int{0, 1, 2, 3, 4, 5}, func(i int) int {
		return i % 3
	})

	require.Len(t, result1, 3)
	require.Equal(t, map[int][]int{
		0: {0, 3},
		1: {1, 4},
		2: {2, 5},
	}, result1)
}

func TestKeyBy(t *testing.T) {
	t.Parallel()

	type foo struct {
		baz string
		bar int
	}
	transform := func(f *foo) (string, int) {
		return f.baz, f.bar
	}
	testCases := []struct {
		in     []*foo
		expect map[string]int
	}{
		{
			in:     []*foo{{baz: "apple", bar: 1}},
			expect: map[string]int{"apple": 1},
		},
		{
			in:     []*foo{{baz: "apple", bar: 1}, {baz: "banana", bar: 2}},
			expect: map[string]int{"apple": 1, "banana": 2},
		},
		{
			in:     []*foo{{baz: "apple", bar: 1}, {baz: "apple", bar: 2}},
			expect: map[string]int{"apple": 2},
		},
	}
	for i, tt := range testCases {
		t.Run(fmt.Sprintf("test_%d", i), func(t *testing.T) {
			t.Parallel()

			require.Equal(t, KeyBy(tt.in, transform), tt.expect)
		})
	}
}

func TestMap(t *testing.T) {
	t.Parallel()

	result1 := Map([]int{1, 2, 3, 4}, func(x int) string {
		return "Hello"
	})
	result2 := Map([]int64{1, 2, 3, 4}, func(x int64) string {
		return strconv.FormatInt(x, 10)
	})

	require.Len(t, result1, 4)
	require.Len(t, result2, 4)
	require.Equal(t, []string{"Hello", "Hello", "Hello", "Hello"}, result1)
	require.Equal(t, []string{"1", "2", "3", "4"}, result2)
}

```

`rivershared/util/slogutil/slog_util.go`:

```go
package slogutil

import (
	"context"
	"fmt"
	"log/slog"
)

// SlogMessageOnlyHandler is a trivial slog handler that prints only messages.
// All attributes and groups are ignored. It's useful in example tests where it
// produces output that's normalized so we match against it (normally, all log
// lines include timestamps so it's not possible to have reproducible output).
type SlogMessageOnlyHandler struct {
	Level slog.Level
}

func (h *SlogMessageOnlyHandler) Enabled(ctx context.Context, level slog.Level) bool {
	return level >= h.Level
}

func (h *SlogMessageOnlyHandler) Handle(ctx context.Context, record slog.Record) error {
	fmt.Printf("%s\n", record.Message)
	return nil
}

func (h *SlogMessageOnlyHandler) WithAttrs(attrs []slog.Attr) slog.Handler { return h }
func (h *SlogMessageOnlyHandler) WithGroup(name string) slog.Handler       { return h }

```

`rivershared/util/timeutil/main_test.go`:

```go
package timeutil_test

import (
	"testing"

	"github.com/riverqueue/river/rivershared/riversharedtest"
)

func TestMain(m *testing.M) {
	riversharedtest.WrapTestMain(m)
}

```

`rivershared/util/timeutil/time_util.go`:

```go
package timeutil

import (
	"context"
	"time"
)

// SecondsAsDuration is a simple shortcut for converting seconds represented as
// a float to a `time.Duration`.
func SecondsAsDuration(seconds float64) time.Duration {
	return time.Duration(seconds * float64(time.Second))
}

// TickerWithInitialTick is similar to `time.Ticker`, except that it fires once
// immediately upon initialization. It also respects context cancellation and
// prefers to be stopped that way rather than an explicit `Stop` function.
type TickerWithInitialTick struct {
	// C fires once on initial startup, then after each interval has passed.
	C <-chan time.Time

	interval time.Duration
	tickChan chan time.Time
}

// NewTickerWithInitialTick creates a new ticker similar to `time.Ticker`,
// except that it fires once immediately upon initialization. It also respects
// context cancellation and prefers to be stopped that way rather than an
// explicit `Stop` function.
func NewTickerWithInitialTick(ctx context.Context, interval time.Duration) *TickerWithInitialTick {
	// Channel of size one combined with non-blocking send are modeled on how
	// Go's internal ticker works. Ticks may be dropped if the caller falls behind.
	tickChan := make(chan time.Time, 1)

	timer := &TickerWithInitialTick{
		C:        tickChan,
		interval: interval,
		tickChan: tickChan,
	}
	go timer.runLoop(ctx)
	return timer
}

// Sends a non-blocking tick into the ticker's channel. Ticks may be dropped if
// the caller falls behind.
func (t *TickerWithInitialTick) nonBlockingTick(tm time.Time) {
	select {
	case t.tickChan <- tm:
	default:
	}
}

func (t *TickerWithInitialTick) runLoop(ctx context.Context) {
	// Return immediately if context is done.
	select {
	case <-ctx.Done():
		return
	default:
	}

	t.nonBlockingTick(time.Now())

	ticker := time.NewTicker(t.interval)
	for {
		select {
		case <-ctx.Done():
			ticker.Stop()
			return

		case tm := <-ticker.C:
			t.nonBlockingTick(tm)
		}
	}
}

```

`rivershared/util/timeutil/time_util_test.go`:

```go
package timeutil_test

import (
	"context"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/util/timeutil"
)

func TestSecondsAsDuration(t *testing.T) {
	t.Parallel()

	require.Equal(t, 1*time.Second, timeutil.SecondsAsDuration(1.0))
}

func TestTickerWithInitialTick(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	t.Run("TicksImmediately", func(t *testing.T) {
		t.Parallel()

		ctx, cancel := context.WithCancel(ctx)
		t.Cleanup(cancel)

		ticker := timeutil.NewTickerWithInitialTick(ctx, 1*time.Hour)
		riversharedtest.WaitOrTimeout(t, ticker.C)
	})

	t.Run("TicksPeriodically", func(t *testing.T) {
		t.Parallel()

		ctx, cancel := context.WithCancel(ctx)
		t.Cleanup(cancel)

		ticker := timeutil.NewTickerWithInitialTick(ctx, 100*time.Microsecond)
		for i := range 10 {
			t.Logf("Waiting on tick %d", i)
			riversharedtest.WaitOrTimeout(t, ticker.C)
		}
	})
}

```

`rivershared/util/valutil/val_util.go`:

```go
package valutil

// ValOrDefault returns the given value if it's non-zero, and otherwise returns
// the default.
func ValOrDefault[T comparable](val, defaultVal T) T {
	var zero T
	if val != zero {
		return val
	}
	return defaultVal
}

// ValOrDefault returns the given value if it's non-zero, and otherwise invokes
// defaultFunc to produce a default value.
func ValOrDefaultFunc[T comparable](val T, defaultFunc func() T) T {
	var zero T
	if val != zero {
		return val
	}
	return defaultFunc()
}

// FirstNonZero returns the first argument that is non-zero, or the zero value if
// all are zero.
func FirstNonZero[T comparable](values ...T) T {
	var zero T
	for _, val := range values {
		if val != zero {
			return val
		}
	}
	return zero
}

```

`rivershared/util/valutil/val_util_test.go`:

```go
package valutil

import (
	"testing"

	"github.com/stretchr/testify/require"
)

func TestValOrDefault(t *testing.T) {
	t.Parallel()

	require.Equal(t, 1, ValOrDefault(0, 1))
	require.Equal(t, 5, ValOrDefault(5, 1))

	require.Equal(t, "default", ValOrDefault("", "default"))
	require.Equal(t, "hello", ValOrDefault("hello", "default"))
}

func TestValOrDefaultFunc(t *testing.T) {
	t.Parallel()

	require.Equal(t, 1, ValOrDefaultFunc(0, func() int { return 1 }))
	require.Equal(t, 5, ValOrDefaultFunc(5, func() int { return 1 }))

	require.Equal(t, "default", ValOrDefaultFunc("", func() string { return "default" }))
	require.Equal(t, "hello", ValOrDefaultFunc("hello", func() string { return "default" }))
}

```

`rivertest/example_require_inserted_test.go`:

```go
package rivertest_test

import (
	"context"
	"fmt"
	"log/slog"
	"testing"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
	"github.com/riverqueue/river/rivertest"
)

type RequiredArgs struct {
	Message string `json:"message"`
}

func (RequiredArgs) Kind() string { return "required" }

type RequiredWorker struct {
	river.WorkerDefaults[RequiredArgs]
}

func (w *RequiredWorker) Work(ctx context.Context, job *river.Job[RequiredArgs]) error { return nil }

// Example_requireInserted demonstrates the use of the RequireInserted test
// assertion, which verifies that a single job was inserted.
func Example_requireInserted() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &RequiredWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger:  slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Workers: workers,
	})
	if err != nil {
		panic(err)
	}

	tx, err := dbPool.Begin(ctx)
	if err != nil {
		panic(err)
	}
	defer tx.Rollback(ctx)

	_, err = riverClient.InsertTx(ctx, tx, &RequiredArgs{
		Message: "Hello.",
	}, nil)
	if err != nil {
		panic(err)
	}

	// Required for purposes of our example here, but in reality t will be the
	// *testing.T that comes from a test's argument.
	t := &testing.T{}

	job := rivertest.RequireInsertedTx[*riverpgxv5.Driver](ctx, t, tx, &RequiredArgs{}, nil)
	fmt.Printf("Test passed with message: %s\n", job.Args.Message)

	// Verify the same job again, and this time that it was inserted at the
	// default priority and default queue.
	_ = rivertest.RequireInsertedTx[*riverpgxv5.Driver](ctx, t, tx, &RequiredArgs{}, &rivertest.RequireInsertedOpts{
		Priority: 1,
		Queue:    river.QueueDefault,
	})

	// Insert and verify one on a pool instead of transaction.
	_, err = riverClient.Insert(ctx, &RequiredArgs{Message: "Hello from pool."}, nil)
	if err != nil {
		panic(err)
	}
	_ = rivertest.RequireInserted(ctx, t, riverpgxv5.New(dbPool), &RequiredArgs{}, nil)

	// Output:
	// Test passed with message: Hello.
}

```

`rivertest/example_require_many_inserted_test.go`:

```go
package rivertest_test

import (
	"context"
	"fmt"
	"log/slog"
	"testing"

	"github.com/jackc/pgx/v5/pgxpool"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/util/slogutil"
	"github.com/riverqueue/river/rivertest"
)

type FirstRequiredArgs struct {
	Message string `json:"message"`
}

func (FirstRequiredArgs) Kind() string { return "first_required" }

type FirstRequiredWorker struct {
	river.WorkerDefaults[FirstRequiredArgs]
}

func (w *FirstRequiredWorker) Work(ctx context.Context, job *river.Job[FirstRequiredArgs]) error {
	return nil
}

type SecondRequiredArgs struct {
	Message string `json:"message"`
}

func (SecondRequiredArgs) Kind() string { return "second_required" }

type SecondRequiredWorker struct {
	river.WorkerDefaults[SecondRequiredArgs]
}

func (w *SecondRequiredWorker) Work(ctx context.Context, job *river.Job[SecondRequiredArgs]) error {
	return nil
}

// Example_requireManyInserted demonstrates the use of the RequireManyInserted test
// assertion, which requires that multiple jobs of the specified kinds were
// inserted.
func Example_requireManyInserted() {
	ctx := context.Background()

	dbPool, err := pgxpool.NewWithConfig(ctx, riverinternaltest.DatabaseConfig("river_test_example"))
	if err != nil {
		panic(err)
	}
	defer dbPool.Close()

	// Required for the purpose of this test, but not necessary in real usage.
	if err := riverinternaltest.TruncateRiverTables(ctx, dbPool); err != nil {
		panic(err)
	}

	workers := river.NewWorkers()
	river.AddWorker(workers, &FirstRequiredWorker{})
	river.AddWorker(workers, &SecondRequiredWorker{})

	riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{
		Logger:  slog.New(&slogutil.SlogMessageOnlyHandler{Level: slog.LevelWarn}),
		Workers: workers,
	})
	if err != nil {
		panic(err)
	}

	tx, err := dbPool.Begin(ctx)
	if err != nil {
		panic(err)
	}
	defer tx.Rollback(ctx)

	_, err = riverClient.InsertTx(ctx, tx, &FirstRequiredArgs{Message: "Hello from first."}, nil)
	if err != nil {
		panic(err)
	}

	_, err = riverClient.InsertTx(ctx, tx, &SecondRequiredArgs{Message: "Hello from second."}, nil)
	if err != nil {
		panic(err)
	}

	_, err = riverClient.InsertTx(ctx, tx, &FirstRequiredArgs{Message: "Hello from first (again)."}, nil)
	if err != nil {
		panic(err)
	}

	// Required for purposes of our example here, but in reality t will be the
	// *testing.T that comes from a test's argument.
	t := &testing.T{}

	jobs := rivertest.RequireManyInsertedTx[*riverpgxv5.Driver](ctx, t, tx, []rivertest.ExpectedJob{
		{Args: &FirstRequiredArgs{}},
		{Args: &SecondRequiredArgs{}},
		{Args: &FirstRequiredArgs{}},
	})
	for i, job := range jobs {
		fmt.Printf("Job %d args: %s\n", i, string(job.EncodedArgs))
	}

	// Verify again, and this time that the second job was inserted at the
	// default priority and default queue.
	_ = rivertest.RequireManyInsertedTx[*riverpgxv5.Driver](ctx, t, tx, []rivertest.ExpectedJob{
		{Args: &SecondRequiredArgs{}, Opts: &rivertest.RequireInsertedOpts{
			Priority: 1,
			Queue:    river.QueueDefault,
		}},
	})

	// Insert and verify one on a pool instead of transaction.
	_, err = riverClient.Insert(ctx, &FirstRequiredArgs{Message: "Hello from pool."}, nil)
	if err != nil {
		panic(err)
	}
	_ = rivertest.RequireManyInserted(ctx, t, riverpgxv5.New(dbPool), []rivertest.ExpectedJob{
		{Args: &FirstRequiredArgs{}},
	})

	// Output:
	// Job 0 args: {"message": "Hello from first."}
	// Job 1 args: {"message": "Hello from second."}
	// Job 2 args: {"message": "Hello from first (again)."}
}

```

`rivertest/main_test.go`:

```go
package rivertest_test

import (
	"testing"

	"github.com/riverqueue/river/internal/riverinternaltest"
)

func TestMain(m *testing.M) {
	riverinternaltest.WrapTestMain(m)
}

```

`rivertest/rivertest.go`:

```go
// Package rivertest contains test assertions that can be used in a project's
// tests to verify that certain actions occurred from the main river package.
package rivertest

import (
	"context"
	"encoding/json"
	"fmt"
	"slices"
	"strings"
	"testing"
	"time"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/rivercommon"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

// testingT is an interface wrapper around *testing.T that's implemented by all
// of *testing.T, *testing.F, and *testing.B.
//
// It's used internally to verify that River's test assertions are working as
// expected.
type testingT interface {
	Errorf(format string, args ...any)
	FailNow()
	Helper()
	Log(args ...any)
	Logf(format string, args ...any)
}

// Options for RequireInserted functions including expectations for various
// queuing properties that stem from InsertOpts.
//
// Multiple properties set on this struct increase the specificity on a job to
// match, acting like an AND condition on each.
//
// In the case of RequireInserted or RequireInsertedMany, if multiple properties
// are set, a job must match all of them to be considered a successful match.
//
// In the case of RequireNotInserted, if multiple properties are set, a test
// failure is triggered only if all match. If any one of them was different, an
// inserted job isn't considered a match, and RequireNotInserted succeeds.
type RequireInsertedOpts struct {
	// MaxAttempts is the expected maximum number of total attempts for the
	// inserted job.
	//
	// No assertion is made if left the zero value.
	MaxAttempts int

	// Priority is the expected priority for the inserted job.
	//
	// No assertion is made if left the zero value.
	Priority int

	// Queue is the expected queue name of the inserted job.
	//
	// No assertion is made if left the zero value.
	Queue string

	// ScheduledAt is the expected scheduled at time of the inserted job. Times
	// are truncated to the microsecond level for comparison to account for the
	// difference between Go storing times to nanoseconds and Postgres storing
	// only to microsecond precision.
	//
	// No assertion is made if left the zero value.
	ScheduledAt time.Time

	// State is the expected state of the inserted job.
	//
	// No assertion is made if left the zero value.
	State rivertype.JobState

	// Tags are the expected tags of the inserted job.
	//
	// No assertion is made if left the zero value.
	Tags []string
}

// RequireInserted is a test helper that verifies that a job of the given kind
// was inserted for work, failing the test if it wasn't. If found, the inserted
// job is returned so that further assertions can be made against it.
//
//	job := RequireInserted(ctx, t, riverpgxv5.New(dbPool), &Job1Args{}, nil)
//
// This variant takes a driver that wraps a database pool. See also
// RequireManyInsertedTx which takes a transaction.
//
// A RequireInsertedOpts struct can be provided as the last argument, and if it is,
// its properties (e.g. max attempts, priority, queue name) will act as required
// assertions in the inserted job row.
//
// The assertion will fail if more than one job of the given kind was found
// because at that point the job to return is ambiguous. Use RequireManyInserted
// to cover that case instead.
func RequireInserted[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, tb testing.TB, driver TDriver, expectedJob TArgs, opts *RequireInsertedOpts) *river.Job[TArgs] {
	tb.Helper()
	return requireInserted(ctx, tb, driver, expectedJob, opts)
}

func requireInserted[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, t testingT, driver TDriver, expectedJob TArgs, opts *RequireInsertedOpts) *river.Job[TArgs] {
	t.Helper()
	actualArgs, err := requireInsertedErr[TDriver](ctx, t, driver.GetExecutor(), expectedJob, opts)
	if err != nil {
		failure(t, "Internal failure: %s", err)
	}
	return actualArgs
}

// RequireInsertedTx is a test helper that verifies that a job of the given kind
// was inserted for work, failing the test if it wasn't. If found, the inserted
// job is returned so that further assertions can be made against it.
//
//	job := RequireInsertedTx[*riverpgxv5.Driver](ctx, t, tx, &Job1Args{}, nil)
//
// This variant takes a transaction. See also RequireInserted which takes a
// driver that wraps a database pool.
//
// A RequireInsertedOpts struct can be provided as the last argument, and if it is,
// its properties (e.g. max attempts, priority, queue name) will act as required
// assertions in the inserted job row.
//
// The assertion will fail if more than one job of the given kind was found
// because at that point the job to return is ambiguous. Use RequireManyInserted
// to cover that case instead.
func RequireInsertedTx[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, tb testing.TB, tx TTx, expectedJob TArgs, opts *RequireInsertedOpts) *river.Job[TArgs] {
	tb.Helper()
	return requireInsertedTx[TDriver](ctx, tb, tx, expectedJob, opts)
}

// Internal function used by the tests so that the exported version can take
// `testing.TB` instead of `testing.T`.
func requireInsertedTx[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, t testingT, tx TTx, expectedJob TArgs, opts *RequireInsertedOpts) *river.Job[TArgs] {
	t.Helper()
	var driver TDriver
	actualArgs, err := requireInsertedErr[TDriver](ctx, t, driver.UnwrapExecutor(tx), expectedJob, opts)
	if err != nil {
		failure(t, "Internal failure: %s", err)
	}
	return actualArgs
}

func requireInsertedErr[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, t testingT, exec riverdriver.Executor, expectedJob TArgs, opts *RequireInsertedOpts) (*river.Job[TArgs], error) {
	t.Helper()

	// Returned ordered by ID.
	jobRows, err := exec.JobGetByKindMany(ctx, []string{expectedJob.Kind()})
	if err != nil {
		return nil, fmt.Errorf("error querying jobs: %w", err)
	}

	if len(jobRows) < 1 {
		failure(t, "No jobs found with kind: %s", expectedJob.Kind())
		return nil, nil //nolint:nilnil
	}

	if len(jobRows) > 1 {
		failure(t, "More than one job found with kind: %s (you might want RequireManyInserted instead)", expectedJob.Kind())
		return nil, nil //nolint:nilnil
	}

	jobRow := jobRows[0]

	var actualArgs TArgs
	if err := json.Unmarshal(jobRow.EncodedArgs, &actualArgs); err != nil {
		return nil, fmt.Errorf("error unmarshaling job args: %w", err)
	}

	if opts != nil {
		if !compareJobToInsertOpts(t, jobRow, opts, -1, false) {
			return nil, nil //nolint:nilnil
		}
	}

	return &river.Job[TArgs]{JobRow: jobRow, Args: actualArgs}, nil
}

// RequireNotInserted is a test helper that verifies that a job of the given
// kind was not inserted for work, failing the test if one was.
//
//	job := RequireNotInserted(ctx, t, riverpgxv5.New(dbPool), &Job1Args{}, nil)
//
// This variant takes a driver that wraps a database pool. See also
// RequireNotInsertedTx which takes a transaction.
//
// A RequireInsertedOpts struct can be provided as the last argument, and if it
// is, its properties (e.g. max attempts, priority, queue name) will act as
// requirements on a found row. If any fields are set, then the test will fail
// if a job is found that matches all of them. If any property doesn't match a
// found row, the row isn't considered a match, and the assertion doesn't fail.
//
// If more rows than one were found, the assertion fails if any of them match
// the given opts.
func RequireNotInserted[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, tb testing.TB, driver TDriver, expectedJob TArgs, opts *RequireInsertedOpts) {
	tb.Helper()
	requireNotInserted(ctx, tb, driver, expectedJob, opts)
}

func requireNotInserted[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, t testingT, driver TDriver, expectedJob TArgs, opts *RequireInsertedOpts) {
	t.Helper()
	err := requireNotInsertedErr[TDriver](ctx, t, driver.GetExecutor(), expectedJob, opts)
	if err != nil {
		failure(t, "Internal failure: %s", err)
	}
}

// RequireNotInsertedTx is a test helper that verifies that a job of the given
// kind was not inserted for work, failing the test if one was.
//
//	job := RequireInsertedTx[*riverpgxv5.Driver](ctx, t, tx, &Job1Args{}, nil)
//
// This variant takes a transaction. See also RequireNotInserted which takes a
// driver that wraps a database pool.
//
// A RequireInsertedOpts struct can be provided as the last argument, and if it
// is, its properties (e.g. max attempts, priority, queue name) will act as
// requirements on a found row. If any fields are set, then the test will fail
// if a job is found that matches all of them. If any property doesn't match a
// found row, the row isn't considered a match, and the assertion doesn't fail.
//
// If more rows than one were found, the assertion fails if any of them match
// the given opts.
func RequireNotInsertedTx[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, tb testing.TB, tx TTx, expectedJob TArgs, opts *RequireInsertedOpts) {
	tb.Helper()
	requireNotInsertedTx[TDriver](ctx, tb, tx, expectedJob, opts)
}

// Internal function used by the tests so that the exported version can take
// `testing.TB` instead of `testing.T`.
func requireNotInsertedTx[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, t testingT, tx TTx, expectedJob TArgs, opts *RequireInsertedOpts) {
	t.Helper()
	var driver TDriver
	err := requireNotInsertedErr[TDriver](ctx, t, driver.UnwrapExecutor(tx), expectedJob, opts)
	if err != nil {
		failure(t, "Internal failure: %s", err)
	}
}

func requireNotInsertedErr[TDriver riverdriver.Driver[TTx], TTx any, TArgs river.JobArgs](ctx context.Context, t testingT, exec riverdriver.Executor, expectedJob TArgs, opts *RequireInsertedOpts) error {
	t.Helper()

	// Returned ordered by ID.
	jobRows, err := exec.JobGetByKindMany(ctx, []string{expectedJob.Kind()})
	if err != nil {
		return fmt.Errorf("error querying jobs: %w", err)
	}

	if len(jobRows) < 1 {
		return nil
	}

	if len(jobRows) > 0 && opts == nil {
		failure(t, "%d jobs found with kind, but expected to find none: %s", len(jobRows), expectedJob.Kind())
		return nil
	}

	// If any of these job rows failed assertions against opts, then the test
	// fails, but if they all succeed, then we consider no matching jobs to have
	// been inserted, and the test succeeds.
	for _, jobRow := range jobRows {
		var actualArgs TArgs
		if err := json.Unmarshal(jobRow.EncodedArgs, &actualArgs); err != nil {
			return fmt.Errorf("error unmarshaling job args: %w", err)
		}

		if opts != nil {
			if !compareJobToInsertOpts(t, jobRow, opts, -1, true) {
				return nil
			}
		}
	}

	return nil
}

// ExpectedJob is a single job to expect encapsulating job args and possible
// insertion options.
type ExpectedJob struct {
	// Args are job arguments to expect.
	Args river.JobArgs

	// Opts are options for the specific required job including insertion
	// options to assert against.
	Opts *RequireInsertedOpts
}

// RequireManyInserted is a test helper that verifies that jobs of the given
// kinds were inserted for work, failing the test if they weren't, or were
// inserted in the wrong order. If found, the inserted jobs are returned so that
// further assertions can be made against them.
//
//	job := RequireManyInserted(ctx, t, riverpgxv5.New(dbPool), []river.JobArgs{
//		&Job1Args{},
//	})
//
// This variant takes a driver that wraps a database pool. See also
// RequireManyInsertedTx which takes a transaction.
//
// A RequireInsertedOpts struct can be provided for each expected job, and if it is,
// its properties (e.g. max attempts, priority, queue name) will act as required
// assertions for the corresponding inserted job row.
//
// The assertion expects emitted jobs to have occurred exactly in the order and
// the number specified, and will fail in case this expectation isn't met. So if
// a job of a certain kind is emitted multiple times, it must be expected
// multiple times.
func RequireManyInserted[TDriver riverdriver.Driver[TTx], TTx any](ctx context.Context, tb testing.TB, driver TDriver, expectedJobs []ExpectedJob) []*rivertype.JobRow {
	tb.Helper()
	return requireManyInserted(ctx, tb, driver, expectedJobs)
}

func requireManyInserted[TDriver riverdriver.Driver[TTx], TTx any](ctx context.Context, t testingT, driver TDriver, expectedJobs []ExpectedJob) []*rivertype.JobRow {
	t.Helper()
	actualArgs, err := requireManyInsertedErr[TDriver](ctx, t, driver.GetExecutor(), expectedJobs)
	if err != nil {
		failure(t, "Internal failure: %s", err)
	}
	return actualArgs
}

// RequireManyInsertedTx is a test helper that verifies that jobs of the given
// kinds were inserted for work, failing the test if they weren't, or were
// inserted in the wrong order. If found, the inserted jobs are returned so that
// further assertions can be made against them.
//
//	job := RequireManyInsertedTx[*riverpgxv5.Driver](ctx, t, tx, []river.JobArgs{
//		&Job1Args{},
//	})
//
// This variant takes a transaction. See also RequireManyInserted which takes a
// driver that wraps a database pool.
//
// A RequireInsertedOpts struct can be provided for each expected job, and if it is,
// its properties (e.g. max attempts, priority, queue name) will act as required
// assertions for the corresponding inserted job row.
//
// The assertion expects emitted jobs to have occurred exactly in the order and
// the number specified, and will fail in case this expectation isn't met. So if
// a job of a certain kind is emitted multiple times, it must be expected
// multiple times.
func RequireManyInsertedTx[TDriver riverdriver.Driver[TTx], TTx any](ctx context.Context, tb testing.TB, tx TTx, expectedJobs []ExpectedJob) []*rivertype.JobRow {
	tb.Helper()
	return requireManyInsertedTx[TDriver](ctx, tb, tx, expectedJobs)
}

// Internal function used by the tests so that the exported version can take
// `testing.TB` instead of `testing.T`.
func requireManyInsertedTx[TDriver riverdriver.Driver[TTx], TTx any](ctx context.Context, t testingT, tx TTx, expectedJobs []ExpectedJob) []*rivertype.JobRow {
	t.Helper()
	var driver TDriver
	actualArgs, err := requireManyInsertedErr[TDriver](ctx, t, driver.UnwrapExecutor(tx), expectedJobs)
	if err != nil {
		failure(t, "Internal failure: %s", err)
	}
	return actualArgs
}

func requireManyInsertedErr[TDriver riverdriver.Driver[TTx], TTx any](ctx context.Context, t testingT, exec riverdriver.Executor, expectedJobs []ExpectedJob) ([]*rivertype.JobRow, error) {
	t.Helper()

	expectedArgsKinds := sliceutil.Map(expectedJobs, func(j ExpectedJob) string { return j.Args.Kind() })

	// Returned ordered by ID.
	jobRows, err := exec.JobGetByKindMany(ctx, expectedArgsKinds)
	if err != nil {
		return nil, fmt.Errorf("error querying jobs: %w", err)
	}

	actualArgsKinds := sliceutil.Map(jobRows, func(j *rivertype.JobRow) string { return j.Kind })

	if !slices.Equal(expectedArgsKinds, actualArgsKinds) {
		failure(t, "Inserted jobs didn't match expectation; expected: %+v, actual: %+v",
			expectedArgsKinds, actualArgsKinds)
		return nil, nil
	}

	for i, jobRow := range jobRows {
		if expectedJobs[i].Opts != nil {
			if !compareJobToInsertOpts(t, jobRow, expectedJobs[i].Opts, i, false) {
				return nil, nil
			}
		}
	}

	return jobRows, nil
}

const rfc3339Micro = "2006-01-02T15:04:05.999999Z07:00"

// The last boolean indicates whether the function's being invoked for
// RequireInserted versus RequireNotInserted. Each need to perform similar
// equality checks (thereby using a single helper), but their semantics for
// succeeds versus failure are orthogonal.
//
// RequireInserted only succeeds if every property is equal. In case any is not
// equal, a set of failures is built up, and a final failure message of them all
// combined emitted at the end.
//
// RequireNotInserted succeeds if any property is not equal. In case of any
// inequality, it returns early and passes the calling test. If case of any
// equality, a set of failures is built up, and a final failure message of them
// all combined emitted at the end.
func compareJobToInsertOpts(t testingT, jobRow *rivertype.JobRow, expectedOpts *RequireInsertedOpts, index int, requireNotInserted bool) bool {
	t.Helper()

	// Adds an index position for the case of multiple expected jobs. Wrapped in
	// a function so that the string is only marshaled if needed.
	positionStr := func() string {
		if index == -1 {
			return ""
		}
		return fmt.Sprintf(" (expected job slice index %d)", index)
	}

	var failures []string

	if expectedOpts.MaxAttempts != 0 {
		if jobRow.MaxAttempts == expectedOpts.MaxAttempts {
			if requireNotInserted {
				failures = append(failures, fmt.Sprintf("max attempts equal to excluded %d", expectedOpts.MaxAttempts))
			}
		} else {
			if requireNotInserted {
				return true // any one property doesn't match; assertion passes
			} else {
				failures = append(failures, fmt.Sprintf("max attempts %d not equal to expected %d", jobRow.MaxAttempts, expectedOpts.MaxAttempts))
			}
		}
	}

	if expectedOpts.Priority != 0 {
		if jobRow.Priority == expectedOpts.Priority {
			if requireNotInserted {
				failures = append(failures, fmt.Sprintf("priority equal to excluded %d", expectedOpts.Priority))
			}
		} else {
			if requireNotInserted {
				return true // any one property doesn't match; assertion passes
			} else {
				failures = append(failures, fmt.Sprintf("priority %d not equal to expected %d", jobRow.Priority, expectedOpts.Priority))
			}
		}
	}

	if expectedOpts.Queue != "" {
		if jobRow.Queue == expectedOpts.Queue {
			if requireNotInserted {
				failures = append(failures, fmt.Sprintf("queue equal to excluded '%s'", expectedOpts.Queue))
			}
		} else {
			if requireNotInserted {
				return true // any one property doesn't match; assertion passes
			} else {
				failures = append(failures, fmt.Sprintf("queue '%s' not equal to expected '%s'", jobRow.Queue, expectedOpts.Queue))
			}
		}
	}

	// We have to be more careful when comparing times because Postgres only
	// stores them to microsecond-level precision and the given time is likely
	// to still have nanos.
	var (
		actualScheduledAt   = jobRow.ScheduledAt.Truncate(time.Microsecond)
		expectedScheduledAt = expectedOpts.ScheduledAt.Truncate(time.Microsecond)
	)
	if expectedOpts.ScheduledAt != (time.Time{}) {
		if actualScheduledAt.Equal(expectedScheduledAt) {
			if requireNotInserted {
				failures = append(failures, fmt.Sprintf("scheduled at equal to excluded %s", expectedScheduledAt.Format(rfc3339Micro))) //nolint:perfsprint
			}
		} else {
			if requireNotInserted {
				return true // any one property doesn't match; assertion passes
			} else {
				failures = append(failures, fmt.Sprintf("scheduled at %s not equal to expected %s", actualScheduledAt.Format(rfc3339Micro), expectedScheduledAt.Format(rfc3339Micro)))
			}
		}
	}

	if expectedOpts.State != "" {
		if jobRow.State == expectedOpts.State {
			if requireNotInserted {
				failures = append(failures, fmt.Sprintf("state equal to excluded '%s'", expectedOpts.State))
			}
		} else {
			if requireNotInserted {
				return true // any one property doesn't match; assertion passes
			} else {
				failures = append(failures, fmt.Sprintf("state '%s' not equal to expected '%s'", jobRow.State, expectedOpts.State))
			}
		}
	}

	if len(expectedOpts.Tags) > 0 {
		if slices.Equal(jobRow.Tags, expectedOpts.Tags) {
			if requireNotInserted {
				failures = append(failures, fmt.Sprintf("tags equal to excluded %+v", expectedOpts.Tags))
			}
		} else {
			if requireNotInserted {
				return true // any one property doesn't match; assertion passes
			} else {
				failures = append(failures, fmt.Sprintf("tags %+v not equal to expected %+v", jobRow.Tags, expectedOpts.Tags))
			}
		}
	}

	if len(failures) < 1 {
		return true
	}

	// In the case of RequireInserted, we'll have built up failures for all
	// properties that failed, and are ready to emit a final failure message.
	//
	// In the case of RequireNotInserted, we'll have returned early already if
	// any property did not match (meaning a job was inserted but it overall did
	// not match all requested conditions, so the RequireNotInserted will not
	// fail). If all properties matched, then like with RequireInserted, we'll
	// have built up failures and are ready to emit a final failure message.
	failure(t, "Job with kind '%s'%s %s", jobRow.Kind, positionStr(), strings.Join(failures, ", "))
	return false
}

// failure takes a printf-style directive and is a shortcut for failing an
// assertion.
func failure(t testingT, format string, a ...any) {
	t.Helper()
	t.Log(failureString(format, a...))
	t.FailNow()
}

// failureString wraps a printf-style formatting directive with a River header
// and footer common to all failure messages.
func failureString(format string, a ...any) string {
	return "\n    River assertion failure:\n    " + fmt.Sprintf(format, a...) + "\n"
}

// WorkContext returns a realistic context that can be used to test JobArgs.Work
// implementations.
//
// In particular, adds a client to the context so that river.ClientFromContext is
// usable in the test suite.
func WorkContext[TTx any](ctx context.Context, client *river.Client[TTx]) context.Context {
	return context.WithValue(ctx, rivercommon.ContextKeyClient{}, client)
}

```

`rivertest/rivertest_test.go`:

```go
package rivertest

import (
	"bytes"
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivertype"
)

// Gives us a nice, stable time to test against.
var testTime = time.Date(2023, 10, 30, 10, 45, 23, 123456, time.UTC) //nolint:gochecknoglobals

type Job1Args struct {
	String string `json:"string"`
}

func (Job1Args) Kind() string { return "job1" }

type Job2Args struct {
	Int int `json:"int"`
}

func (Job2Args) Kind() string { return "job2" }

// The tests for this function are quite minimal because it uses the same
// implementation as the `*Tx` variant, so most of the test happens below.
func TestRequireInserted(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
		mockT  *MockT
	}

	setup := func(t *testing.T) (*river.Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)

		riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{})
		require.NoError(t, err)

		return riverClient, &testBundle{
			dbPool: dbPool,
			mockT:  NewMockT(t),
		}
	}

	t.Run("VerifiesInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.Insert(ctx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		job := requireInserted(ctx, t, riverpgxv5.New(bundle.dbPool), &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "foo", job.Args.String)
	})
}

func TestRequireInsertedTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		mockT *MockT
		tx    pgx.Tx
	}

	setup := func(t *testing.T) (*river.Client[pgx.Tx], *testBundle) {
		t.Helper()

		riverClient, err := river.NewClient(riverpgxv5.New(nil), &river.Config{})
		require.NoError(t, err)

		return riverClient, &testBundle{
			mockT: NewMockT(t),
			tx:    riverinternaltest.TestTx(ctx, t),
		}
	}

	t.Run("VerifiesInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		job := requireInsertedTx[*riverpgxv5.Driver](ctx, t, bundle.tx, &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "foo", job.Args.String)
	})

	t.Run("VerifiesMultiple", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		_, err = riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, nil)
		require.NoError(t, err)

		job1 := requireInsertedTx[*riverpgxv5.Driver](ctx, t, bundle.tx, &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "foo", job1.Args.String)

		job2 := requireInsertedTx[*riverpgxv5.Driver](ctx, t, bundle.tx, &Job2Args{}, nil)
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, 123, job2.Args.Int)
	})

	t.Run("TransactionVisibility", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		// Start a second transaction with different visibility.
		otherTx := riverinternaltest.TestTx(ctx, t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		// Visible in the original transaction.
		job := requireInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "foo", job.Args.String)

		// Not visible in the second transaction.
		_ = requireInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, otherTx, &Job1Args{}, nil)
		require.True(t, bundle.mockT.Failed)
	})

	t.Run("FailsWithoutInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		_ = requireInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job2Args{}, nil)
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("No jobs found with kind: job2")+"\n",
			bundle.mockT.LogOutput())
	})

	t.Run("FailsWithTooManyInserts", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertManyTx(ctx, bundle.tx, []river.InsertManyParams{
			{Args: Job1Args{String: "foo"}},
			{Args: Job1Args{String: "bar"}},
		})
		require.NoError(t, err)

		_ = requireInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job1Args{}, nil)
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("More than one job found with kind: job1 (you might want RequireManyInserted instead)")+"\n",
			bundle.mockT.LogOutput())
	})

	t.Run("FailsOnWrongJobInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, nil)
		require.NoError(t, err)

		_ = requireInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job1Args{}, nil)
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("No jobs found with kind: job1")+"\n",
			bundle.mockT.LogOutput())
	})

	t.Run("InsertOpts", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		// Verify custom insertion options.
		insertRes, err := riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, &river.InsertOpts{
			MaxAttempts: 78,
			Priority:    2,
			Queue:       "another_queue",
			ScheduledAt: testTime,
			Tags:        []string{"tag1"},
		})
		require.NoError(t, err)
		job := insertRes.Job

		emptyOpts := func() *RequireInsertedOpts { return &RequireInsertedOpts{} }

		sameOpts := func() *RequireInsertedOpts {
			return &RequireInsertedOpts{
				MaxAttempts: 78,
				Priority:    2,
				Queue:       "another_queue",
				ScheduledAt: testTime,
				State:       rivertype.JobStateScheduled,
				Tags:        []string{"tag1"},
			}
		}

		t.Run("MaxAttempts", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			opts.MaxAttempts = 77
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' max attempts 78 not equal to expected 77")+"\n",
				mockT.LogOutput())
		})

		t.Run("Priority", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			opts.Priority = 3
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' priority 2 not equal to expected 3")+"\n",
				mockT.LogOutput())
		})

		t.Run("Queue", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			opts.Queue = "wrong_queue"
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' queue 'another_queue' not equal to expected 'wrong_queue'")+"\n",
				mockT.LogOutput())
		})

		t.Run("ScheduledAt", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			opts.ScheduledAt = testTime.Add(3*time.Minute + 23*time.Second + 123*time.Microsecond)
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' scheduled at 2023-10-30T10:45:23.000123Z not equal to expected 2023-10-30T10:48:46.000246Z")+"\n",
				mockT.LogOutput())
		})

		t.Run("State", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			opts.State = rivertype.JobStateCancelled
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' state 'scheduled' not equal to expected 'cancelled'")+"\n",
				mockT.LogOutput())
		})

		t.Run("Tags", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			opts.Tags = []string{"tag2"}
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' tags [tag1] not equal to expected [tag2]")+"\n",
				mockT.LogOutput())
		})

		t.Run("MultiplePropertiesSucceed", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.MaxAttempts = job.MaxAttempts
			opts.Priority = job.Priority
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.False(t, mockT.Failed, "Should have succeeded, but failed with: "+mockT.LogOutput())
		})

		t.Run("MultiplePropertiesFails", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			opts.MaxAttempts = 77
			opts.Priority = 3
			_ = requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' max attempts 78 not equal to expected 77, priority 2 not equal to expected 3")+"\n",
				mockT.LogOutput())
		})

		t.Run("AllSameSucceeds", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			requireInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.False(t, mockT.Failed)
		})
	})
}

// The tests for this function are quite minimal because it uses the same
// implementation as the `*Tx` variant, so most of the test happens below.
func TestRequireNotInserted(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
		mockT  *MockT
	}

	setup := func(t *testing.T) (*river.Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)

		riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{})
		require.NoError(t, err)

		return riverClient, &testBundle{
			dbPool: dbPool,
			mockT:  NewMockT(t),
		}
	}

	t.Run("VerifiesNotInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.Insert(ctx, Job2Args{Int: 123}, nil)
		require.NoError(t, err)

		requireNotInserted(ctx, t, riverpgxv5.New(bundle.dbPool), &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)
	})
}

func TestRequireNotInsertedTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		mockT *MockT
		tx    pgx.Tx
	}

	setup := func(t *testing.T) (*river.Client[pgx.Tx], *testBundle) {
		t.Helper()

		riverClient, err := river.NewClient(riverpgxv5.New(nil), &river.Config{})
		require.NoError(t, err)

		return riverClient, &testBundle{
			mockT: NewMockT(t),
			tx:    riverinternaltest.TestTx(ctx, t),
		}
	}

	t.Run("VerifiesInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, nil)
		require.NoError(t, err)

		requireNotInsertedTx[*riverpgxv5.Driver](ctx, t, bundle.tx, &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)
	})

	t.Run("VerifiesMultiple", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		requireNotInsertedTx[*riverpgxv5.Driver](ctx, t, bundle.tx, &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)

		requireNotInsertedTx[*riverpgxv5.Driver](ctx, t, bundle.tx, &Job2Args{}, nil)
		require.False(t, bundle.mockT.Failed)
	})

	t.Run("TransactionVisibility", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		// Start a second transaction with different visibility.
		otherTx := riverinternaltest.TestTx(ctx, t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		// Not visible in the second transaction.
		requireNotInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, otherTx, &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)

		// Visible in the original transaction.
		requireNotInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job1Args{}, nil)
		require.True(t, bundle.mockT.Failed)
	})

	t.Run("SucceedsWithoutInsert", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		requireNotInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job2Args{}, nil)
		require.False(t, bundle.mockT.Failed)
	})

	t.Run("FailsWithTooManyInserts", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertManyTx(ctx, bundle.tx, []river.InsertManyParams{
			{Args: Job1Args{String: "foo"}},
			{Args: Job1Args{String: "bar"}},
		})
		require.NoError(t, err)

		requireNotInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job1Args{}, nil)
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("2 jobs found with kind, but expected to find none: job1")+"\n",
			bundle.mockT.LogOutput())
	})

	t.Run("SucceedsOnWrongJobInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, nil)
		require.NoError(t, err)

		requireNotInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, &Job1Args{}, nil)
		require.False(t, bundle.mockT.Failed)
	})

	t.Run("InsertOpts", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		// Verify custom insertion options.
		insertRes, err := riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, &river.InsertOpts{
			MaxAttempts: 78,
			Priority:    2,
			Queue:       "another_queue",
			ScheduledAt: testTime,
			Tags:        []string{"tag1"},
		})
		require.NoError(t, err)
		job := insertRes.Job

		emptyOpts := func() *RequireInsertedOpts { return &RequireInsertedOpts{} }

		sameOpts := func() *RequireInsertedOpts {
			return &RequireInsertedOpts{
				MaxAttempts: 78,
				Priority:    2,
				Queue:       "another_queue",
				ScheduledAt: testTime,
				State:       rivertype.JobStateScheduled,
				Tags:        []string{"tag1"},
			}
		}

		t.Run("MaxAttempts", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.MaxAttempts = job.MaxAttempts
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' max attempts equal to excluded %d", job.MaxAttempts)+"\n",
				mockT.LogOutput())
		})

		t.Run("Priority", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.Priority = job.Priority
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' priority equal to excluded %d", job.Priority)+"\n",
				mockT.LogOutput())
		})

		t.Run("Queue", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.Queue = job.Queue
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' queue equal to excluded '%s'", job.Queue)+"\n",
				mockT.LogOutput())
		})

		t.Run("ScheduledAt", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.ScheduledAt = job.ScheduledAt
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' scheduled at equal to excluded %s", opts.ScheduledAt.Format(rfc3339Micro))+"\n",
				mockT.LogOutput())
		})

		t.Run("State", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.State = job.State
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' state equal to excluded '%s'", job.State)+"\n",
				mockT.LogOutput())
		})

		t.Run("Tags", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.Tags = job.Tags
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' tags equal to excluded %+v", job.Tags)+"\n",
				mockT.LogOutput())
		})

		t.Run("MultiplePropertiesSucceed", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.MaxAttempts = job.MaxAttempts // one property matches job, but the other does not
			opts.Priority = 3
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.False(t, mockT.Failed, "Should have succeeded, but failed with: "+mockT.LogOutput())
		})

		t.Run("MultiplePropertiesFail", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.MaxAttempts = job.MaxAttempts
			opts.Priority = job.Priority
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' max attempts equal to excluded %d, priority equal to excluded %d", job.MaxAttempts, job.Priority)+"\n",
				mockT.LogOutput())
		})

		t.Run("AllSameFails", func(t *testing.T) {
			mockT := NewMockT(t)
			opts := sameOpts()
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' max attempts equal to excluded %d, priority equal to excluded %d, queue equal to excluded '%s', scheduled at equal to excluded %s, state equal to excluded '%s', tags equal to excluded %+v", job.MaxAttempts, job.Priority, job.Queue, job.ScheduledAt.Format(rfc3339Micro), job.State, job.Tags)+"\n",
				mockT.LogOutput())
		})

		t.Run("FailsWithTooManyInserts", func(t *testing.T) {
			_, err := riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, &river.InsertOpts{
				Priority: 3,
			})
			require.NoError(t, err)

			mockT := NewMockT(t)
			opts := emptyOpts()
			opts.Priority = 3
			requireNotInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, &Job2Args{}, opts)
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' priority equal to excluded %d", 3)+"\n",
				mockT.LogOutput())
		})
	})
}

// The tests for this function are quite minimal because it uses the same
// implementation as the `*Tx` variant, so most of the test happens below.
func TestRequireManyInserted(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		dbPool *pgxpool.Pool
		mockT  *MockT
	}

	setup := func(t *testing.T) (*river.Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)

		riverClient, err := river.NewClient(riverpgxv5.New(dbPool), &river.Config{})
		require.NoError(t, err)

		return riverClient, &testBundle{
			dbPool: dbPool,
			mockT:  NewMockT(t),
		}
	}

	t.Run("VerifiesInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.Insert(ctx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		jobs := requireManyInserted(ctx, bundle.mockT, riverpgxv5.New(bundle.dbPool), []ExpectedJob{
			{Args: &Job1Args{}},
		})
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "job1", jobs[0].Kind)
	})
}

func TestRequireManyInsertedTx(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		mockT *MockT
		tx    pgx.Tx
	}

	setup := func(t *testing.T) (*river.Client[pgx.Tx], *testBundle) {
		t.Helper()

		riverClient, err := river.NewClient(riverpgxv5.New(nil), &river.Config{})
		require.NoError(t, err)

		return riverClient, &testBundle{
			mockT: NewMockT(t),
			tx:    riverinternaltest.TestTx(ctx, t),
		}
	}

	t.Run("VerifiesInsert", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		jobs := requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
		})
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "job1", jobs[0].Kind)
	})

	t.Run("TransactionVisibility", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		// Start a second transaction with different visibility.
		otherTx := riverinternaltest.TestTx(ctx, t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		// Visible in the original transaction.
		jobs := requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
		})
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "job1", jobs[0].Kind)

		// Not visible in the second transaction.
		_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, otherTx, []ExpectedJob{
			{Args: &Job1Args{}},
		})
		require.True(t, bundle.mockT.Failed)
	})

	t.Run("VerifiesMultipleDifferentKind", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		_, err = riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, nil)
		require.NoError(t, err)

		jobs := requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
			{Args: &Job2Args{}},
		})
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "job1", jobs[0].Kind)
		require.Equal(t, "job2", jobs[1].Kind)
	})

	t.Run("VerifiesMultipleSameKind", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertManyTx(ctx, bundle.tx, []river.InsertManyParams{
			{Args: Job1Args{String: "foo"}},
			{Args: Job1Args{String: "bar"}},
		})
		require.NoError(t, err)

		jobs := requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
			{Args: &Job1Args{}},
		})
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "job1", jobs[0].Kind)
		require.Equal(t, "job1", jobs[1].Kind)
	})

	t.Run("VerifiesMultitude", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertManyTx(ctx, bundle.tx, []river.InsertManyParams{
			{Args: Job1Args{String: "foo"}},
			{Args: Job1Args{String: "bar"}},
			{Args: Job2Args{Int: 123}},
			{Args: Job2Args{Int: 456}},
			{Args: Job1Args{String: "baz"}},
		})
		require.NoError(t, err)

		jobs := requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
			{Args: &Job1Args{}},
			{Args: &Job2Args{}},
			{Args: &Job2Args{}},
			{Args: &Job1Args{}},
		})
		require.False(t, bundle.mockT.Failed)
		require.Equal(t, "job1", jobs[0].Kind)
		require.Equal(t, "job1", jobs[1].Kind)
		require.Equal(t, "job2", jobs[2].Kind)
		require.Equal(t, "job2", jobs[3].Kind)
		require.Equal(t, "job1", jobs[4].Kind)
	})

	t.Run("VerifiesInsertOpts", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		// Verify default insertion options.
		_, err := riverClient.InsertTx(ctx, bundle.tx, Job1Args{String: "foo"}, nil)
		require.NoError(t, err)

		_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{
				Args: &Job1Args{},
				Opts: &RequireInsertedOpts{
					MaxAttempts: river.MaxAttemptsDefault,
					Priority:    1,
					Queue:       river.QueueDefault,
				},
			},
		})
		require.False(t, bundle.mockT.Failed)

		// Verify custom insertion options.
		_, err = riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, &river.InsertOpts{
			MaxAttempts: 78,
			Priority:    2,
			Queue:       "another_queue",
			ScheduledAt: testTime,
			Tags:        []string{"tag1"},
		})
		require.NoError(t, err)

		_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{
				Args: &Job2Args{},
				Opts: &RequireInsertedOpts{
					MaxAttempts: 78,
					Priority:    2,
					Queue:       "another_queue",
					ScheduledAt: testTime,
					Tags:        []string{"tag1"},
				},
			},
		})
		require.False(t, bundle.mockT.Failed)
	})

	t.Run("FailsWithoutInsert", func(t *testing.T) {
		t.Parallel()

		_, bundle := setup(t)

		_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
		})
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("Inserted jobs didn't match expectation; expected: [job1], actual: []")+"\n",
			bundle.mockT.LogOutput())
	})

	t.Run("FailsWithTooManyInserts", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertManyTx(ctx, bundle.tx, []river.InsertManyParams{
			{Args: Job1Args{String: "foo"}},
			{Args: Job1Args{String: "bar"}},
		})
		require.NoError(t, err)

		_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
		})
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("Inserted jobs didn't match expectation; expected: [job1], actual: [job1 job1]")+"\n",
			bundle.mockT.LogOutput())
	})

	t.Run("FailsWrongInsertOrder", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertManyTx(ctx, bundle.tx, []river.InsertManyParams{
			{Args: Job2Args{Int: 123}},
			{Args: Job1Args{String: "foo"}},
		})
		require.NoError(t, err)

		_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
			{Args: &Job2Args{}},
		})
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("Inserted jobs didn't match expectation; expected: [job1 job2], actual: [job2 job1]")+"\n",
			bundle.mockT.LogOutput())
	})

	t.Run("FailsMultitude", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		_, err := riverClient.InsertManyTx(ctx, bundle.tx, []river.InsertManyParams{
			{Args: Job1Args{String: "foo"}},
			{Args: Job1Args{String: "bar"}},
			{Args: Job2Args{Int: 123}},
			{Args: Job2Args{Int: 456}},
			{Args: Job2Args{Int: 789}},
		})
		require.NoError(t, err)

		_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, bundle.mockT, bundle.tx, []ExpectedJob{
			{Args: &Job1Args{}},
			{Args: &Job1Args{}},
			{Args: &Job2Args{}},
			{Args: &Job2Args{}},
			{Args: &Job1Args{}},
		})
		require.True(t, bundle.mockT.Failed)
		require.Equal(t,
			failureString("Inserted jobs didn't match expectation; expected: [job1 job1 job2 job2 job1], actual: [job1 job1 job2 job2 job2]")+"\n", //nolint:dupword
			bundle.mockT.LogOutput())
	})

	t.Run("FailsOnInsertOpts", func(t *testing.T) {
		t.Parallel()

		riverClient, bundle := setup(t)

		// Verify custom insertion options.
		_, err := riverClient.InsertTx(ctx, bundle.tx, Job2Args{Int: 123}, &river.InsertOpts{
			MaxAttempts: 78,
			Priority:    2,
			Queue:       "another_queue",
			ScheduledAt: testTime,
			Tags:        []string{"tag1"},
		})
		require.NoError(t, err)

		// Max attempts
		{
			mockT := NewMockT(t)
			_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, []ExpectedJob{
				{
					Args: &Job2Args{},
					Opts: &RequireInsertedOpts{
						MaxAttempts: 77,
						Priority:    2,
						Queue:       "another_queue",
						ScheduledAt: testTime,
						State:       rivertype.JobStateScheduled,
						Tags:        []string{"tag1"},
					},
				},
			})
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' (expected job slice index 0) max attempts 78 not equal to expected 77")+"\n",
				mockT.LogOutput())
		}

		// Priority
		{
			mockT := NewMockT(t)
			_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, []ExpectedJob{
				{
					Args: &Job2Args{},
					Opts: &RequireInsertedOpts{
						MaxAttempts: 78,
						Priority:    3,
						Queue:       "another_queue",
						ScheduledAt: testTime,
						State:       rivertype.JobStateScheduled,
						Tags:        []string{"tag1"},
					},
				},
			})
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' (expected job slice index 0) priority 2 not equal to expected 3")+"\n",
				mockT.LogOutput())
		}

		// Queue
		{
			mockT := NewMockT(t)
			_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, []ExpectedJob{
				{
					Args: &Job2Args{},
					Opts: &RequireInsertedOpts{
						MaxAttempts: 78,
						Priority:    2,
						Queue:       "wrong-queue",
						ScheduledAt: testTime,
						State:       rivertype.JobStateScheduled,
						Tags:        []string{"tag1"},
					},
				},
			})
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' (expected job slice index 0) queue 'another_queue' not equal to expected 'wrong-queue'")+"\n",
				mockT.LogOutput())
		}

		// Scheduled at
		{
			mockT := NewMockT(t)
			_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, []ExpectedJob{
				{
					Args: &Job2Args{},
					Opts: &RequireInsertedOpts{
						MaxAttempts: 78,
						Priority:    2,
						Queue:       "another_queue",
						ScheduledAt: testTime.Add(3*time.Minute + 23*time.Second + 123*time.Microsecond),
						State:       rivertype.JobStateScheduled,
						Tags:        []string{"tag1"},
					},
				},
			})
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' (expected job slice index 0) scheduled at 2023-10-30T10:45:23.000123Z not equal to expected 2023-10-30T10:48:46.000246Z")+"\n",
				mockT.LogOutput())
		}

		// State
		{
			mockT := NewMockT(t)
			_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, []ExpectedJob{
				{
					Args: &Job2Args{},
					Opts: &RequireInsertedOpts{
						MaxAttempts: 78,
						Priority:    2,
						Queue:       "another_queue",
						State:       rivertype.JobStateCancelled,
						ScheduledAt: testTime,
						Tags:        []string{"tag1"},
					},
				},
			})
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' (expected job slice index 0) state 'scheduled' not equal to expected 'cancelled'")+"\n",
				mockT.LogOutput())
		}

		// Tags
		{
			mockT := NewMockT(t)
			_ = requireManyInsertedTx[*riverpgxv5.Driver](ctx, mockT, bundle.tx, []ExpectedJob{
				{
					Args: &Job2Args{},
					Opts: &RequireInsertedOpts{
						MaxAttempts: 78,
						Priority:    2,
						Queue:       "another_queue",
						ScheduledAt: testTime,
						State:       rivertype.JobStateScheduled,
						Tags:        []string{"tag2"},
					},
				},
			})
			require.True(t, mockT.Failed)
			require.Equal(t,
				failureString("Job with kind 'job2' (expected job slice index 0) tags [tag1] not equal to expected [tag2]")+"\n",
				mockT.LogOutput())
		}
	})
}

func TestWorkContext(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(ctx context.Context, t *testing.T) (context.Context, *testBundle) {
		t.Helper()

		client, err := river.NewClient(riverpgxv5.New(nil), &river.Config{})
		require.NoError(t, err)

		return WorkContext(ctx, client), &testBundle{}
	}

	t.Run("ClientFromContext", func(t *testing.T) {
		t.Parallel()

		ctx, _ := setup(ctx, t)

		client := river.ClientFromContext[pgx.Tx](ctx)
		require.NotNil(t, client)
	})
}

// MockT mocks testingT (or *testing.T). It's used to let us verify our test
// helpers.
type MockT struct {
	Failed    bool
	logOutput bytes.Buffer
	tb        testing.TB
}

func NewMockT(tb testing.TB) *MockT {
	tb.Helper()
	return &MockT{tb: tb}
}

func (t *MockT) Errorf(format string, args ...any) {
	_, _ = format, args
}

func (t *MockT) FailNow() {
	t.Failed = true
}

func (t *MockT) Helper() {}

func (t *MockT) Log(args ...any) {
	t.tb.Log(args...)

	t.logOutput.WriteString(fmt.Sprint(args...))
	t.logOutput.WriteString("\n")
}

func (t *MockT) Logf(format string, args ...any) {
	t.tb.Logf(format, args...)

	t.logOutput.WriteString(fmt.Sprintf(format, args...))
	t.logOutput.WriteString("\n")
}

func (t *MockT) LogOutput() string {
	return t.logOutput.String()
}

```

`rivertest/time_stub.go`:

```go
package rivertest

import (
	"sync"
	"time"
)

// TimeStub implements rivertype.TimeGenerator to allow time to be stubbed in
// tests. It is implemented in a thread-safe manner with a mutex, allowing the
// current time to be stubbed at any time with StubNowUTC.
type TimeStub struct {
	mu     sync.RWMutex
	nowUTC *time.Time
}

// NowUTC returns the current time. This may be a stubbed time if the time has
// been actively stubbed in a test.
func (t *TimeStub) NowUTC() time.Time {
	t.mu.RLock()
	defer t.mu.RUnlock()

	if t.nowUTC == nil {
		return time.Now().UTC()
	}

	return *t.nowUTC
}

// NowUTCOrNil returns if the currently stubbed time _if_ the current time
// is stubbed, and returns nil otherwise. This is generally useful in cases
// where a component may want to use a stubbed time if the time is stubbed,
// but to fall back to a database time default otherwise.
func (t *TimeStub) NowUTCOrNil() *time.Time {
	t.mu.RLock()
	defer t.mu.RUnlock()

	return t.nowUTC
}

// StubNowUTC stubs the current time. It will panic if invoked outside of tests.
// Returns the same time passed as parameter for convenience.
func (t *TimeStub) StubNowUTC(nowUTC time.Time) time.Time {
	t.mu.Lock()
	defer t.mu.Unlock()

	t.nowUTC = &nowUTC
	return nowUTC
}

```

`rivertest/time_stub_test.go`:

```go
package rivertest

import (
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivertype"
)

// Ensure that TimeStub implements rivertype.TimeGenerator.
var _ rivertype.TimeGenerator = &TimeStub{}

func TestTimeStub(t *testing.T) {
	t.Parallel()

	stub := &TimeStub{}

	now := time.Now().UTC()

	require.WithinDuration(t, now, stub.NowUTC(), 2*time.Second)
	require.Nil(t, stub.NowUTCOrNil())

	stub.StubNowUTC(now)
	require.Equal(t, now, stub.NowUTC())
	require.Equal(t, &now, stub.NowUTCOrNil())
}

```

`rivertest/worker.go`:

```go
package rivertest

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"testing"
	"time"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/execution"
	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/jobexecutor"
	"github.com/riverqueue/river/internal/maintenance"
	"github.com/riverqueue/river/internal/middlewarelookup"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

// Worker makes it easier to test river workers. Once built, the worker can be
// used to insert and work any number jobs:
//
//	testWorker := rivertest.NewWorker(t, driver, config, worker)
//	result, err := testWorker.Work(ctx, t, tx, args, nil)
//	if err != nil {
//		t.Fatalf("failed to work job: %s", err)
//	}
//	if result.Kind != river.EventKindJobCompleted {
//		t.Fatalf("expected job to be completed, got %s", result.Kind)
//	}
//
// An existing job (inserted using external logic) can also be worked:
//
//	job := client.InsertTx(ctx, tx, args, nil)
//	// ...
//	result, err := worker.WorkJob(ctx, t, tx, job)
//	if err != nil {
//		t.Fatalf("failed to work job: %s", err)
//	}
//
// In all cases the underlying [river.Worker] will be called with the job
// transitioned into a running state. The execution environment has a realistic
// River context with access to all River features, including
// [river.ClientFromContext] and worker middleware.
type Worker[T river.JobArgs, TTx any] struct {
	client *river.Client[TTx]
	config *river.Config
	worker river.Worker[T]
}

// NewWorker creates a new test Worker for testing the provided [river.Worker].
// The worker uses the provided driver and River config to populate default
// values on test jobs and to configure the execution environment.
//
// A pool-less driver is recommended for most usage, as individual job inserts
// and executions will happen within a provided transaction. This enables many
// parallel test cases to run with full isolation, each in their own
// transaction.
//
// The worker is configured to disable unique enforcement by default, as this
// would otherwise prevent conflicting jobs from being tested in parallel.
func NewWorker[T river.JobArgs, TTx any](tb testing.TB, driver riverdriver.Driver[TTx], config *river.Config, worker river.Worker[T]) *Worker[T, TTx] {
	tb.Helper()

	config = config.WithDefaults()
	config.Test.DisableUniqueEnforcement = true

	client, err := river.NewClient(driver, config)
	if err != nil {
		tb.Fatalf("failed to create client: %s", err)
	}

	return &Worker[T, TTx]{
		client: client,
		config: config,
		worker: worker,
	}
}

func (w *Worker[T, TTx]) insertJob(ctx context.Context, tb testing.TB, tx TTx, args T, opts *river.InsertOpts) (*rivertype.JobRow, error) {
	tb.Helper()

	result, err := w.client.InsertTx(ctx, tx, args, opts)
	if err != nil {
		return nil, err
	}
	return result.Job, nil
}

// Work inserts a job with the provided arguments and optional insert options,
// then works it. The transaction is used for all work-related database
// operations so that the caller can easily roll back at the end of the test to
// maintain a clean database state.
//
// The returned WorkResult contains the updated job row from the database
// _after_ it has been worked, as well as the kind of event that occurred.
//
// The returned error only reflects _real_ errors and does not include
// explicitly returned snooze or cancel errors from the worker.
func (w *Worker[T, TTx]) Work(ctx context.Context, tb testing.TB, tx TTx, args T, opts *river.InsertOpts) (*WorkResult, error) {
	tb.Helper()

	job, err := w.insertJob(ctx, tb, tx, args, opts)
	if err != nil {
		return nil, fmt.Errorf("failed to insert job: %w", err)
	}
	return w.WorkJob(ctx, tb, tx, job)
}

// WorkJob works an existing job already in the database. The job must be
// inserted using external logic prior to calling this method. The transaction
// is used for all work-related database operations so that the caller can
// easily roll back at the end of the test to maintain a clean database state.
//
// The returned WorkResult contains the updated job row from the database
// _after_ it has been worked, as well as the kind of event that occurred.
//
// The returned error only reflects _real_ errors and does not include
// explicitly returned snooze or cancel errors from the worker.
func (w *Worker[T, TTx]) WorkJob(ctx context.Context, tb testing.TB, tx TTx, job *rivertype.JobRow) (*WorkResult, error) {
	tb.Helper()
	return w.workJob(ctx, tb, tx, job)
}

func (w *Worker[T, TTx]) workJob(ctx context.Context, tb testing.TB, tx TTx, job *rivertype.JobRow) (*WorkResult, error) {
	tb.Helper()

	timeGen := w.config.Test.Time
	if timeGen == nil {
		timeGen = &baseservice.UnStubbableTimeGenerator{}
	}

	exec := w.client.Driver().UnwrapExecutor(tx)
	subscribeCh := make(chan []jobcompleter.CompleterJobUpdated, 1)
	archetype := riversharedtest.BaseServiceArchetype(tb)
	if withStub, ok := timeGen.(baseservice.TimeGeneratorWithStub); ok {
		archetype.Time = withStub
	} else {
		archetype.Time = &baseservice.TimeGeneratorWithStubWrapper{TimeGenerator: timeGen}
	}
	completer := jobcompleter.NewInlineCompleter(archetype, exec, w.client.Pilot(), subscribeCh)

	updatedJobRow, err := exec.JobUpdate(ctx, &riverdriver.JobUpdateParams{
		ID:                  job.ID,
		Attempt:             job.Attempt + 1,
		AttemptDoUpdate:     true,
		AttemptedAt:         ptrutil.Ptr(timeGen.NowUTC()),
		AttemptedAtDoUpdate: true,
		AttemptedBy:         append(job.AttemptedBy, w.config.ID),
		AttemptedByDoUpdate: true,
		StateDoUpdate:       true,
		State:               rivertype.JobStateRunning,
	})
	if err != nil && !errors.Is(err, rivertype.ErrNotFound) {
		return nil, fmt.Errorf("test worker internal error: failed to update job to running state: %w", err)
	}
	job = updatedJobRow

	workUnit := (&workUnitFactoryWrapper[T]{worker: w.worker}).MakeUnit(job)

	// populate river client into context:
	ctx = WorkContext(ctx, w.client)
	// TODO: remove ContextKeyInsideTestWorker
	ctx = context.WithValue(ctx, execution.ContextKeyInsideTestWorker{}, true)

	// jobCancel will always be called by the executor to prevent leaks.
	jobCtx, jobCancel := context.WithCancelCause(ctx)

	executionDone := make(chan struct{})

	var resultErr error

	executor := baseservice.Init(archetype, &jobexecutor.JobExecutor{
		CancelFunc:               jobCancel,
		ClientJobTimeout:         w.config.JobTimeout,
		ClientRetryPolicy:        w.config.RetryPolicy,
		Completer:                completer,
		DefaultClientRetryPolicy: &river.DefaultClientRetryPolicy{},
		ErrorHandler: &errorHandlerWrapper{
			HandleErrorFunc: func(ctx context.Context, job *rivertype.JobRow, err error) *jobexecutor.ErrorHandlerResult {
				resultErr = err
				return nil
			},
			HandlePanicFunc: func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *jobexecutor.ErrorHandlerResult {
				resultErr = &PanicError{Cause: panicVal, Trace: trace}
				return nil
			},
		},
		InformProducerDoneFunc: func(job *rivertype.JobRow) { close(executionDone) },
		HookLookupGlobal:       hooklookup.NewHookLookup(w.config.Hooks),
		HookLookupByJob:        hooklookup.NewJobHookLookup(),
		JobRow:                 job,
		MiddlewareLookupGlobal: middlewarelookup.NewMiddlewareLookup(w.config.Middleware),
		SchedulerInterval:      maintenance.JobSchedulerIntervalDefault,
		WorkUnit:               workUnit,
	})

	executor.Execute(jobCtx)
	<-executionDone

	select {
	case completerResult := <-subscribeCh:
		if len(completerResult) == 0 {
			tb.Fatal("test worker internal error: empty job completion received")
		}
		if len(completerResult) > 1 {
			tb.Fatalf("test worker internal error: received %d job completions, expected 1", len(completerResult))
		}
		return completerResultToWorkResult(tb, completerResult[0]), resultErr
	default:
		tb.Fatal("test worker internal error: no job completions received")
	}
	panic("unreachable")
}

// WorkResult is the result of working a job in the test Worker.
type WorkResult struct {
	// EventKind is the kind of event that occurred following execution.
	EventKind river.EventKind

	// Job is the updated job row from the database _after_ it has been worked.
	Job *rivertype.JobRow
}

func completerResultToWorkResult(tb testing.TB, completerResult jobcompleter.CompleterJobUpdated) *WorkResult {
	tb.Helper()

	var kind river.EventKind
	switch completerResult.Job.State {
	case rivertype.JobStateCancelled:
		kind = river.EventKindJobCancelled
	case rivertype.JobStateCompleted:
		kind = river.EventKindJobCompleted
	case rivertype.JobStateScheduled:
		kind = river.EventKindJobSnoozed
	case rivertype.JobStateAvailable, rivertype.JobStateDiscarded, rivertype.JobStateRetryable, rivertype.JobStateRunning:
		kind = river.EventKindJobFailed
	case rivertype.JobStatePending:
		panic("test worker internal error: completion subscriber unexpectedly received job in pending state, river bug")
	default:
		// linter exhaustive rule prevents this from being reached
		panic("test worker internal error: unreachable state to distribute, river bug")
	}

	return &WorkResult{
		EventKind: kind,
		Job:       completerResult.Job,
	}
}

type PanicError struct {
	Cause any
	Trace string
}

func (e *PanicError) Error() string {
	return fmt.Sprintf("rivertest.PanicError: %v\n%s", e.Cause, e.Trace)
}

func (e *PanicError) Is(target error) bool {
	_, ok := target.(*PanicError)
	return ok
}

type errorHandlerWrapper struct {
	HandleErrorFunc func(ctx context.Context, job *rivertype.JobRow, err error) *jobexecutor.ErrorHandlerResult
	HandlePanicFunc func(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *jobexecutor.ErrorHandlerResult
}

func (h *errorHandlerWrapper) HandleError(ctx context.Context, job *rivertype.JobRow, err error) *jobexecutor.ErrorHandlerResult {
	return h.HandleErrorFunc(ctx, job, err)
}

func (h *errorHandlerWrapper) HandlePanic(ctx context.Context, job *rivertype.JobRow, panicVal any, trace string) *jobexecutor.ErrorHandlerResult {
	return h.HandlePanicFunc(ctx, job, panicVal, trace)
}

// TODO: move work_unit_wrapper.go so I don't need to copy paste it here:

// workUnitFactoryWrapper wraps a Worker to implement workUnitFactory.
type workUnitFactoryWrapper[T river.JobArgs] struct {
	worker river.Worker[T]
}

func (w *workUnitFactoryWrapper[T]) MakeUnit(jobRow *rivertype.JobRow) workunit.WorkUnit {
	return &wrapperWorkUnit[T]{jobRow: jobRow, worker: w.worker}
}

// wrapperWorkUnit implements workUnit for a job and Worker.
type wrapperWorkUnit[T river.JobArgs] struct {
	job    *river.Job[T] // not set until after UnmarshalJob is invoked
	jobRow *rivertype.JobRow
	worker river.Worker[T]
}

func (w *wrapperWorkUnit[T]) HookLookup(lookup *hooklookup.JobHookLookup) hooklookup.HookLookupInterface {
	var job T
	return lookup.ByJobArgs(job)
}

func (w *wrapperWorkUnit[T]) Middleware() []rivertype.WorkerMiddleware {
	return w.worker.Middleware(w.jobRow)
}
func (w *wrapperWorkUnit[T]) NextRetry() time.Time           { return w.worker.NextRetry(w.job) }
func (w *wrapperWorkUnit[T]) Timeout() time.Duration         { return w.worker.Timeout(w.job) }
func (w *wrapperWorkUnit[T]) Work(ctx context.Context) error { return w.worker.Work(ctx, w.job) }

func (w *wrapperWorkUnit[T]) UnmarshalJob() error {
	w.job = &river.Job[T]{
		JobRow: w.jobRow,
	}

	return json.Unmarshal(w.jobRow.EncodedArgs, &w.job.Args)
}

```

`rivertest/worker_test.go`:

```go
package rivertest

import (
	"context"
	"errors"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river"
	"github.com/riverqueue/river/internal/execution"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

type testArgs struct {
	Value string `json:"value"`
}

func (testArgs) Kind() string { return "rivertest_work_test" }

func TestPanicError(t *testing.T) {
	t.Parallel()

	panicErr := &PanicError{Cause: errors.New("test panic error"), Trace: "test trace"}
	require.Equal(t, "rivertest.PanicError: test panic error\ntest trace", panicErr.Error())
}

func TestWorker_NewWorker(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		config *river.Config
		driver *riverpgxv5.Driver
		tx     pgx.Tx
	}

	setup := func(t *testing.T) *testBundle {
		t.Helper()

		return &testBundle{
			config: &river.Config{ID: "rivertest-worker"},
			driver: riverpgxv5.New(nil),
			tx:     riverinternaltest.TestTx(ctx, t),
		}
	}

	t.Run("HandlesNilRiverConfig", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			return nil
		})
		tw := NewWorker(t, bundle.driver, nil, worker)
		require.NotNil(t, tw.config)
	})
}

func TestWorker_Work(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		config *river.Config
		driver *riverpgxv5.Driver
		tx     pgx.Tx
	}

	setup := func(t *testing.T) *testBundle {
		t.Helper()

		var (
			config = &river.Config{ID: "rivertest-worker"}
			driver = riverpgxv5.New(nil)
			tx     = riverinternaltest.TestTx(ctx, t)
		)

		return &testBundle{
			config: config,
			driver: driver,
			tx:     tx,
		}
	}

	t.Run("WorkASimpleJob", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			require.Equal(t, testArgs{Value: "test"}, job.Args)
			require.Equal(t, 1, job.JobRow.Attempt)
			require.NotNil(t, job.JobRow.AttemptedAt)
			require.WithinDuration(t, time.Now(), *job.JobRow.AttemptedAt, 5*time.Second)
			require.Equal(t, []string{"rivertest-worker"}, job.JobRow.AttemptedBy)
			require.WithinDuration(t, time.Now(), job.JobRow.CreatedAt, 5*time.Second)
			require.JSONEq(t, `{"value": "test"}`, string(job.JobRow.EncodedArgs))
			require.Empty(t, job.JobRow.Errors)
			require.Nil(t, job.JobRow.FinalizedAt)
			require.Positive(t, job.JobRow.ID)
			require.Equal(t, "rivertest_work_test", job.JobRow.Kind)
			require.Equal(t, river.MaxAttemptsDefault, job.JobRow.MaxAttempts)
			require.Equal(t, []byte(`{}`), job.JobRow.Metadata)
			require.Equal(t, river.PriorityDefault, job.JobRow.Priority)
			require.Equal(t, river.QueueDefault, job.JobRow.Queue)
			require.WithinDuration(t, time.Now(), job.JobRow.ScheduledAt, 2*time.Second)
			require.Equal(t, rivertype.JobStateRunning, job.JobRow.State)
			require.Equal(t, []string{}, job.JobRow.Tags)
			require.Nil(t, job.JobRow.UniqueKey)

			_, hasContextKeyInsideTestWorker := ctx.Value(execution.ContextKeyInsideTestWorker{}).(bool)
			require.True(t, hasContextKeyInsideTestWorker)

			return nil
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)
		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)
	})

	t.Run("Reusable", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			return nil
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)
		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)
		res, err = tw.Work(ctx, t, bundle.tx, testArgs{Value: "test2"}, nil)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)
	})

	t.Run("SetsCustomInsertOpts", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		hourFromNow := time.Now().UTC().Add(1 * time.Hour)

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			require.Equal(t, testArgs{Value: "test3"}, job.Args)
			require.Equal(t, 1, job.JobRow.Attempt)
			require.NotNil(t, job.JobRow.AttemptedAt)
			require.WithinDuration(t, time.Now().UTC(), *job.JobRow.AttemptedAt, 2*time.Second)
			require.Equal(t, []string{"rivertest-worker"}, job.JobRow.AttemptedBy)
			require.WithinDuration(t, time.Now().UTC(), job.JobRow.CreatedAt, 2*time.Second)
			require.JSONEq(t, `{"value": "test3"}`, string(job.JobRow.EncodedArgs))
			require.Empty(t, job.JobRow.Errors)
			require.Nil(t, job.JobRow.FinalizedAt)
			require.Positive(t, job.JobRow.ID)
			require.Equal(t, "rivertest_work_test", job.JobRow.Kind)
			require.Equal(t, 420, job.JobRow.MaxAttempts)
			require.JSONEq(t, `{"key": "value"}`, string(job.JobRow.Metadata))
			require.Equal(t, 3, job.JobRow.Priority)
			require.Equal(t, "custom_queue", job.JobRow.Queue)
			require.WithinDuration(t, hourFromNow, job.JobRow.ScheduledAt, 2*time.Second)
			require.Equal(t, rivertype.JobStateRunning, job.JobRow.State)
			require.Equal(t, []string{"tag1", "tag2"}, job.JobRow.Tags)

			return nil
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		// You can also pass in custom insert options:
		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test3"}, &river.InsertOpts{
			MaxAttempts: 420,
			Metadata:    []byte(`{"key": "value"}`),
			Pending:     true, // ignored but added to ensure non-default behavior
			Priority:    3,
			Queue:       "custom_queue",
			ScheduledAt: hourFromNow,
			Tags:        []string{"tag1", "tag2"},
		})
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)
	})

	t.Run("UniqueOptsAreIgnored", func(t *testing.T) {
		t.Parallel()
		// UniqueOpts must be ignored because otherwise there's a likelihood of
		// conflicts with parallel tests inserting jobs with the same unique key.

		bundle := setup(t)

		stubTime := &riversharedtest.TimeStub{}
		now := time.Now().UTC()
		stubTime.StubNowUTC(now)
		bundle.config.Test.Time = stubTime

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			require.Empty(t, job.JobRow.UniqueKey)
			require.Empty(t, job.JobRow.UniqueStates)
			return nil
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, &river.InsertOpts{
			UniqueOpts: river.UniqueOpts{ByPeriod: 1 * time.Hour},
		})
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)
	})

	t.Run("ReturnsASnoozeEventKindWhenSnoozed", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			return river.JobSnooze(time.Hour)
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobSnoozed, res.EventKind)
	})

	t.Run("ReturnsACancelEventKindWhenCancelled", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			return river.JobCancel(nil)
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCancelled, res.EventKind)
	})

	t.Run("UsesACustomClockWhenProvided", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)
		hourFromNow := time.Now().UTC().Add(1 * time.Hour)
		timeStub := &TimeStub{}
		timeStub.StubNowUTC(hourFromNow)
		bundle.config.Test.Time = timeStub

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			require.WithinDuration(t, hourFromNow, *job.JobRow.AttemptedAt, time.Millisecond)
			require.WithinDuration(t, hourFromNow, job.JobRow.CreatedAt, time.Millisecond)
			require.WithinDuration(t, hourFromNow, job.JobRow.ScheduledAt, time.Millisecond)
			return nil
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)
		require.WithinDuration(t, hourFromNow, *res.Job.FinalizedAt, time.Millisecond)
	})

	t.Run("ErrorFromWorker", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		errToReturn := errors.New("test error")
		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			return errToReturn
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.ErrorIs(t, err, errToReturn)
		require.Equal(t, river.EventKindJobFailed, res.EventKind)
	})

	t.Run("PanicFromWorker", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		errToReturn := errors.New("test panic error")
		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			panic(errToReturn)
		})
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.ErrorIs(t, err, &PanicError{})
		require.ErrorContains(t, err, "test panic error")
		require.Equal(t, river.EventKindJobFailed, res.EventKind)

		var panicErr *PanicError
		require.ErrorAs(t, err, &panicErr)
		require.Equal(t, errToReturn, panicErr.Cause)
		require.Contains(t, panicErr.Trace, "github.com/riverqueue/river/rivertest.TestWorker_Work")
		require.Len(t, res.Job.Errors, 1)
		require.Contains(t, res.Job.Errors[0].Error, "test panic error")
	})

	t.Run("ErrorsWithAlreadyClosedTransaction", func(t *testing.T) {
		t.Parallel()

		bundle := setup(t)

		// Immediately roll back the transaction to force an error:
		require.NoError(t, bundle.tx.Rollback(ctx))

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error { return nil })
		tw := NewWorker(t, bundle.driver, bundle.config, worker)

		res, err := tw.Work(ctx, t, bundle.tx, testArgs{Value: "test"}, nil)
		require.ErrorContains(t, err, "failed to insert job: tx is closed")
		require.Nil(t, res)
	})
}

func TestWorker_WorkJob(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		client   *river.Client[pgx.Tx]
		config   *river.Config
		driver   *riverpgxv5.Driver
		tx       pgx.Tx
		workFunc func(ctx context.Context, job *river.Job[testArgs]) error
	}

	setup := func(t *testing.T) (*Worker[testArgs, pgx.Tx], *testBundle) {
		t.Helper()

		var (
			config = &river.Config{ID: "rivertest-workjob"}
			driver = riverpgxv5.New(nil)
		)

		client, err := river.NewClient(driver, config)
		require.NoError(t, err)

		bundle := &testBundle{
			client:   client,
			config:   config,
			driver:   driver,
			tx:       riverinternaltest.TestTx(ctx, t),
			workFunc: func(ctx context.Context, job *river.Job[testArgs]) error { return nil },
		}

		worker := river.WorkFunc(func(ctx context.Context, job *river.Job[testArgs]) error {
			return bundle.workFunc(ctx, job)
		})

		return NewWorker(t, driver, config, worker), bundle
	}

	t.Run("Success", func(t *testing.T) {
		t.Parallel()

		testWorker, bundle := setup(t)

		bundle.workFunc = func(ctx context.Context, job *river.Job[testArgs]) error {
			require.WithinDuration(t, time.Now(), *job.JobRow.AttemptedAt, 5*time.Second)
			require.Equal(t, []string{"rivertest-workjob"}, job.JobRow.AttemptedBy)
			require.Equal(t, rivertype.JobStateRunning, job.State)
			return nil
		}

		insertRes, err := bundle.client.InsertTx(ctx, bundle.tx, testArgs{}, nil)
		require.NoError(t, err)

		res, err := testWorker.WorkJob(ctx, t, bundle.tx, insertRes.Job)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)
	})

	t.Run("JobCompleteTxWithInsertedJobRow", func(t *testing.T) {
		t.Parallel()

		testWorker, bundle := setup(t)

		args := testArgs{}
		insertRes, err := bundle.client.InsertTx(ctx, bundle.tx, args, nil)
		require.NoError(t, err)

		bundle.workFunc = func(ctx context.Context, job *river.Job[testArgs]) error {
			updatedJob, err := bundle.driver.UnwrapExecutor(bundle.tx).JobGetByID(ctx, insertRes.Job.ID)
			require.NoError(t, err)
			require.Equal(t, rivertype.JobStateRunning, updatedJob.State)

			_, err = river.JobCompleteTx[*riverpgxv5.Driver](ctx, bundle.tx, job)
			require.NoError(t, err)

			return nil
		}

		res, err := testWorker.WorkJob(ctx, t, bundle.tx, insertRes.Job)
		require.NoError(t, err)
		require.Equal(t, river.EventKindJobCompleted, res.EventKind)

		updatedJob, err := bundle.driver.UnwrapExecutor(bundle.tx).JobGetByID(ctx, insertRes.Job.ID)
		require.NoError(t, err)
		require.Equal(t, rivertype.JobStateCompleted, updatedJob.State)
	})

	t.Run("ErrorsWhenGivenAlreadyCompletedJob", func(t *testing.T) {
		t.Parallel()

		ctx := context.Background()
		testWorker, bundle := setup(t)

		job := testfactory.Job(ctx, t, bundle.driver.UnwrapExecutor(bundle.tx), &testfactory.JobOpts{
			EncodedArgs: []byte(`{"value": "test"}`),
			Kind:        ptrutil.Ptr("rivertest_work_test"),
			State:       ptrutil.Ptr(rivertype.JobStateCompleted),
		})

		res, err := testWorker.WorkJob(ctx, t, bundle.tx, job)
		require.ErrorContains(t, err, "failed to update job to running state")
		require.Nil(t, res)
	})
}

```

`rivertype/execution_error.go`:

```go
package rivertype

import (
	"errors"
	"fmt"
	"time"
)

var ErrJobCancelledRemotely = JobCancel(errors.New("job cancelled remotely"))

// JobCancel wraps err and can be returned from a Worker's Work method to cancel
// the job at the end of execution. Regardless of whether or not the job has any
// remaining attempts, this will ensure the job does not execute again.
//
// This function primarily exists for cross module compatibility. Users should
// use river.JobCancel instead.
func JobCancel(err error) error {
	return &JobCancelError{err: err}
}

// JobCancelError is the error type returned by JobCancel. It should not be
// initialized directly, but is returned from the [JobCancel] function and can
// be used for test assertions.
type JobCancelError struct {
	err error
}

func (e *JobCancelError) Error() string {
	if e.err == nil {
		return "JobCancelError: <nil>"
	}
	// should not ever be called, but add a prefix just in case:
	return "JobCancelError: " + e.err.Error()
}

func (e *JobCancelError) Is(target error) bool {
	_, ok := target.(*JobCancelError)
	return ok
}

func (e *JobCancelError) Unwrap() error { return e.err }

// JobSnoozeError is the error type returned by JobSnooze. It should not be
// initialized directly, but is returned from the [JobSnooze] function and can
// be used for test assertions.
type JobSnoozeError struct {
	Duration time.Duration
}

func (e *JobSnoozeError) Error() string {
	// should not ever be called, but add a prefix just in case:
	return fmt.Sprintf("JobSnoozeError: %s", e.Duration)
}

func (e *JobSnoozeError) Is(target error) bool {
	_, ok := target.(*JobSnoozeError)
	return ok
}

// UnknownJobKindError is returned when a Client fetches and attempts to
// work a job that has not been registered on the Client's Workers bundle (using
// AddWorker).
type UnknownJobKindError struct {
	// Kind is the string that was returned by the JobArgs Kind method.
	Kind string
}

// Error returns the error string.
func (e *UnknownJobKindError) Error() string {
	return "job kind is not registered in the client's Workers bundle: " + e.Kind
}

// Is implements the interface used by errors.Is to determine if errors are
// equivalent. It returns true for any other UnknownJobKindError without
// regard to the Kind string so it is possible to detect this type of error
// with:
//
//	errors.Is(err, &UnknownJobKindError{})
func (e *UnknownJobKindError) Is(target error) bool {
	_, ok := target.(*UnknownJobKindError)
	return ok
}

```

`rivertype/go.mod`:

```mod
module github.com/riverqueue/river/rivertype

go 1.23.0

toolchain go1.24.1

require github.com/stretchr/testify v1.10.0

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/kr/pretty v0.3.0 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/rogpeppe/go-internal v1.12.0 // indirect
	gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

```

`rivertype/go.sum`:

```sum
github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
github.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
github.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=
github.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=
github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/rogpeppe/go-internal v1.6.1/go.mod h1:xXDCJY+GAPziupqXw64V24skbSoqbTEfhy4qGm1nDQc=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

```

`rivertype/river_type.go`:

```go
// Package rivertype stores some of the lowest level River primitives so they
// can be shared amongst a number of packages including the top-level river
// package, database drivers, and internal utilities.
package rivertype

import (
	"context"
	"encoding/json"
	"errors"
	"time"
)

// MetadataKeyOutput is the metadata key used to store recorded job output.
const MetadataKeyOutput = "output"

// ErrNotFound is returned when a query by ID does not match any existing
// rows. For example, attempting to cancel a job that doesn't exist will
// return this error.
var ErrNotFound = errors.New("not found")

// ErrJobRunning is returned when a job is attempted to be deleted while it's
// running.
var ErrJobRunning = errors.New("running jobs cannot be deleted")

// JobArgs is an interface that should be implemented by the arguments to a job.
// This definition duplicates the JobArgs interface in the river package so that
// it can be used in other packages without creating a circular dependency.
type JobArgs interface {
	// Kind returns a unique string that identifies the type of job. It's used to
	// determine which worker should work the job.
	Kind() string
}

// JobInsertResult is the result of a job insert, containing the inserted job
// along with some other useful metadata.
type JobInsertResult struct {
	// Job is a struct containing the database persisted properties of the
	// inserted job.
	Job *JobRow

	// UniqueSkippedAsDuplicate is true if for a unique job, the insertion was
	// skipped due to an equivalent job matching unique property already being
	// present.
	UniqueSkippedAsDuplicate bool
}

// JobRow contains the properties of a job that are persisted to the database.
// Use of `Job[T]` will generally be preferred in user-facing code like worker
// interfaces.
type JobRow struct {
	// ID of the job. Generated as part of a Postgres sequence and generally
	// ascending in nature, but there may be gaps in it as transactions roll
	// back.
	ID int64

	// Attempt is the attempt number of the job. Jobs are inserted at 0, the
	// number is incremented to 1 the first time work its worked, and may
	// increment further if it errors. Attempt will decrement on snooze so that
	// repeated snoozes don't increment this value.
	Attempt int

	// AttemptedAt is the time that the job was last worked. Starts out as `nil`
	// on a new insert.
	AttemptedAt *time.Time

	// AttemptedBy is the set of client IDs that have worked this job.
	AttemptedBy []string

	// CreatedAt is when the job record was created.
	CreatedAt time.Time

	// EncodedArgs is the job's JobArgs encoded as JSON.
	EncodedArgs []byte

	// Errors is a set of errors that occurred when the job was worked, one for
	// each attempt. Ordered from earliest error to the latest error.
	Errors []AttemptError

	// FinalizedAt is the time at which the job was "finalized", meaning it was
	// either completed successfully or errored for the last time such that
	// it'll no longer be retried.
	FinalizedAt *time.Time

	// Kind uniquely identifies the type of job and instructs which worker
	// should work it. It is set at insertion time via `Kind()` on the
	// `JobArgs`.
	Kind string

	// MaxAttempts is the maximum number of attempts that the job will be tried
	// before it errors for the last time and will no longer be worked.
	//
	// Extracted (in order of precedence) from job-specific InsertOpts
	// on Insert, from the worker level InsertOpts from JobArgsWithInsertOpts,
	// or from a client's default value.
	MaxAttempts int

	// Metadata is a field for storing arbitrary metadata on a job. It should
	// always be a valid JSON object payload, and users should not overwrite or
	// remove anything stored in this field by River.
	Metadata []byte

	// Priority is the priority of the job, with 1 being the highest priority and
	// 4 being the lowest. When fetching available jobs to work, the highest
	// priority jobs will always be fetched before any lower priority jobs are
	// fetched. Note that if your workers are swamped with more high-priority jobs
	// then they can handle, lower priority jobs may not be fetched.
	Priority int

	// Queue is the name of the queue where the job will be worked. Queues can
	// be configured independently and be used to isolate jobs.
	//
	// Extracted from either specific InsertOpts on Insert, or InsertOpts from
	// JobArgsWithInsertOpts, or a client's default value.
	Queue string

	// ScheduledAt is when the job is scheduled to become available to be
	// worked. Jobs default to running immediately, but may be scheduled
	// for the future when they're inserted. They may also be scheduled for
	// later because they were snoozed or because they errored and have
	// additional retry attempts remaining.
	ScheduledAt time.Time

	// State is the state of job like `available` or `completed`. Jobs are
	// `available` when they're first inserted.
	State JobState

	// Tags are an arbitrary list of keywords to add to the job. They have no
	// functional behavior and are meant entirely as a user-specified construct
	// to help group and categorize jobs.
	Tags []string

	// UniqueKey is a unique key for the job within its kind that's used for
	// unique job insertions. It's generated by hashing an inserted job's unique
	// opts configuration.
	UniqueKey []byte

	// UniqueStates is the set of states where uniqueness is enforced for this
	// job. Equivalent to the default set of unique states unless
	// UniqueOpts.ByState was assigned a custom value.
	UniqueStates []JobState
}

// Output returns the previously recorded output for the job, if any. The return
// value is a raw JSON payload from the output that was recorded by the job, or
// nil if no output was recorded.
func (j *JobRow) Output() []byte {
	type metadataWithOutput struct {
		Output json.RawMessage `json:"output"`
	}

	var metadata metadataWithOutput
	if err := json.Unmarshal(j.Metadata, &metadata); err != nil {
		return nil
	}

	return metadata.Output
}

// JobState is the state of a job. Jobs start their lifecycle as either
// JobStateAvailable or JobStateScheduled, and if all goes well, transition to
// JobStateCompleted after they're worked.
type JobState string

const (
	// JobStateAvailable is the state for jobs that are immediately eligible to
	// be worked.
	JobStateAvailable JobState = "available"

	// JobStateCancelled is the state for jobs that have been manually cancelled
	// by user request.
	//
	// Cancelled jobs are reaped by the job cleaner service after a configured
	// amount of time (default 24 hours).
	JobStateCancelled JobState = "cancelled"

	// JobStateCompleted is the state for jobs that have successfully run to
	// completion.
	//
	// Completed jobs are reaped by the job cleaner service after a configured
	// amount of time (default 24 hours).
	JobStateCompleted JobState = "completed"

	// JobStateDiscarded is the state for jobs that have errored enough times
	// that they're no longer eligible to be retried. Manual user invention
	// is required for them to be tried again.
	//
	// Discarded jobs are reaped by the job cleaner service after a configured
	// amount of time (default 7 days).
	JobStateDiscarded JobState = "discarded"

	// JobStatePending is a state for jobs to be parked while waiting for some
	// external action before they can be worked. Jobs in pending will never be
	// worked or deleted unless moved out of this state by the user.
	JobStatePending JobState = "pending"

	// JobStateRetryable is the state for jobs that have errored, but will be
	// retried.
	//
	// The job scheduler service changes them to JobStateAvailable when they're
	// ready to be worked (their `scheduled_at` timestamp comes due).
	//
	// Jobs that will be retried very soon in the future may be changed to
	// JobStateAvailable immediately instead of JobStateRetryable so that they
	// don't have to wait for the job scheduler to run.
	JobStateRetryable JobState = "retryable"

	// JobStateRunning are jobs which are actively running.
	//
	// If River can't update state of a running job (in the case of a program
	// crash, underlying hardware failure, or job that doesn't return from its
	// Work function), that job will be left as JobStateRunning, and will
	// require a pass by the job rescuer service to be set back to
	// JobStateAvailable and be eligible for another run attempt.
	JobStateRunning JobState = "running"

	// JobStateScheduled is the state for jobs that are scheduled for the
	// future.
	//
	// The job scheduler service changes them to JobStateAvailable when they're
	// ready to be worked (their `scheduled_at` timestamp comes due).
	JobStateScheduled JobState = "scheduled"
)

// JobStates returns all possible job states.
func JobStates() []JobState {
	return []JobState{
		JobStateAvailable,
		JobStateCancelled,
		JobStateCompleted,
		JobStateDiscarded,
		JobStatePending,
		JobStateRetryable,
		JobStateRunning,
		JobStateScheduled,
	}
}

// AttemptError is an error from a single job attempt that failed due to an
// error or a panic.
type AttemptError struct {
	// At is the time at which the error occurred.
	At time.Time `json:"at"`

	// Attempt is the attempt number on which the error occurred (maps to
	// Attempt on a job row).
	Attempt int `json:"attempt"`

	// Error contains the stringified error of an error returned from a job or a
	// panic value in case of a panic.
	Error string `json:"error"`

	// Trace contains a stack trace from a job that panicked. The trace is
	// produced by invoking `debug.Trace()`.
	Trace string `json:"trace"`
}

type JobInsertParams struct {
	Args         JobArgs
	CreatedAt    *time.Time
	EncodedArgs  []byte
	Kind         string
	MaxAttempts  int
	Metadata     []byte
	Priority     int
	Queue        string
	ScheduledAt  *time.Time
	State        JobState
	Tags         []string
	UniqueKey    []byte
	UniqueStates byte
}

// Hook is an arbitrary interface for a plugin "hook" which will execute some
// arbitrary code at a predefined step in the job lifecycle.
//
// This interface is left purposely non-specific. Hook structs should embed
// river.HookDefaults to inherit an IsHook implementation, then implement one
// of the more specific hook interfaces like HookInsertBegin or HookWorkBegin. A
// hook struct may also implement multiple specific hook interfaces which are
// logically related and benefit from being grouped together.
//
// Hooks differ from middleware in that they're invoked at a specific lifecycle
// phase, but finish immediately instead of wrapping an inner call like a
// middleware does. One of the main ramifications of this different is that a
// hook cannot modify context in any useful way to pass down into the stack.
// Like a normal function, any changes it makes to its context are discarded on
// return.
//
// All else equal, hooks should generally be preferred over middleware because
// they don't add anything to the call stack. Call stacks that get overly deep
// can become a bit of an operational nightmare because they get hard to read.
//
// In a language with more specific type capabilities, this interface would be a
// union type. In Go we implement it somewhat awkwardly so that we can get
// future extensibility, but also some typing guarantees to prevent misuse (i.e.
// if Hook was an empty interface, then any object could be passed as a hook,
// but having a single function to implement forces the caller to make some
// token motions in the direction of implementing hooks).
//
// List of hook interfaces that may be implemented:
// - HookInsertBegin
// - HookWorkBegin
type Hook interface {
	// IsHook is a sentinel function to check that a type is implementing Hook
	// on purpose and not by accident (Hook would otherwise be an empty
	// interface). Hooks should embed river.HookDefaults to pick up an
	// implementation for this function automatically.
	IsHook() bool
}

// HookInsertBegin is an interface to a hook that runs before job insertion.
type HookInsertBegin interface {
	Hook

	InsertBegin(ctx context.Context, params *JobInsertParams) error
}

// HookWorkBegin is an interface to a hook that runs after a job has been locked
// for work and before it's worked.
type HookWorkBegin interface {
	Hook

	WorkBegin(ctx context.Context, job *JobRow) error
}

// Middleware is an arbitrary interface for a struct which will execute some
// arbitrary code at a predefined step in the job lifecycle.
//
// This interface is left purposely non-specific. Middleware structs should
// embed river.MiddlewareDefaults to inherit an IsMiddleware implementation,
// then implement a more specific hook interface like JobInsertMiddleware or
// WorkerMiddleware. A middleware struct may also implement multiple specific
// hook interfaces which are logically related and benefit from being grouped
// together.
//
// Hooks differ from middleware in that they're invoked at a specific lifecycle
// phase, but finish immediately instead of wrapping an inner call like a
// middleware does. One of the main ramifications of this different is that a
// hook cannot modify context in any useful way to pass down into the stack.
// Like a normal function, any changes it makes to its context are discarded on
// return.
//
// Middleware differs from hooks in that they wrap a specific lifecycle phase,
// staying on the callstack for the duration of the step while they call into a
// doInner function that executes the step and the rest of the middleware stack.
// The main ramification of this difference is that middleware can modify
// context for the step and any other middleware inner relative to it.
//
// All else equal, hooks should generally be preferred over middleware because
// they don't add anything to the call stack. Call stacks that get overly deep
// can become a bit of an operational nightmare because they get hard to read.
//
// In a language with more specific type capabilities, this interface would be a
// union type. In Go we implement it somewhat awkwardly so that we can get
// future extensibility, but also some typing guarantees to prevent misuse (i.e.
// if Hook was an empty interface, then any object could be passed as a hook,
// but having a single function to implement forces the caller to make some
// token motions in the direction of implementing hooks).
//
// List of middleware interfaces that may be implemented:
// - JobInsertMiddleware
// - WorkerMiddleware
type Middleware interface {
	// IsMiddleware is a sentinel function to check that a type is implementing
	// Middleware on purpose and not by accident (Middleware would otherwise be
	// an empty interface). Middleware should embed river.MiddlewareDefaults to
	// pick up an implementation for this function automatically.
	IsMiddleware() bool
}

// JobInsertMiddleware provides an interface for middleware that integrations
// can use to encapsulate common logic around job insertion.
//
// Implementations should embed river.JobMiddlewareDefaults to inherit default
// implementations for phases where no custom code is needed, and for forward
// compatibility in case new functions are added to this interface.
type JobInsertMiddleware interface {
	Middleware

	// InsertMany is invoked around a batch insert operation. Implementations
	// must always include a call to doInner to call down the middleware stack
	// and perform the batch insertion, and may run custom code before and after.
	//
	// Returning an error from this function will fail the overarching insert
	// operation, even if the inner insertion originally succeeded.
	InsertMany(ctx context.Context, manyParams []*JobInsertParams, doInner func(context.Context) ([]*JobInsertResult, error)) ([]*JobInsertResult, error)
}

// WorkerMiddleware provides an interface for middleware that integrations can
// use to encapsulate common logic when a job is worked.
type WorkerMiddleware interface {
	Middleware

	// Work is invoked after a job's JSON args being unmarshaled and before the
	// job is worked. Implementations must always include a call to doInner to
	// call down the middleware stack and perform the batch insertion, and may run
	// custom code before and after.
	//
	// Returning an error from this function will fail the overarching work
	// operation, even if the inner work originally succeeded.
	Work(ctx context.Context, job *JobRow, doInner func(context.Context) error) error
}

// PeriodicJobHandle is a reference to a dynamically added periodic job
// (returned by the use of `Client.PeriodicJobs().Add()`) which can be used to
// subsequently remove the periodic job with `Remove()`.
type PeriodicJobHandle int

// Queue is a configuration for a queue that is currently (or recently was) in
// use by a client.
type Queue struct {
	// CreatedAt is the time at which the queue first began being worked by a
	// client. Unused queues are deleted after a retention period, so this only
	// reflects the most recent time the queue was created if there was a long
	// gap.
	CreatedAt time.Time
	// Metadata is a field for storing arbitrary metadata on a queue. It is
	// currently reserved for River's internal use and should not be modified by
	// users.
	Metadata []byte
	// Name is the name of the queue.
	Name string
	// PausedAt is the time the queue was paused, if any. When a paused queue is
	// resumed, this field is set to nil.
	PausedAt *time.Time
	// UpdatedAt is the last time the queue was updated. This field is updated
	// periodically any time an active Client is configured to work the queue,
	// even if the queue is paused.
	//
	// If UpdatedAt has not been updated for awhile, the queue record will be
	// deleted from the table by a maintenance process.
	UpdatedAt time.Time
}

// UniqueOptsByStateDefault is the set of job states that are used to determine
// uniqueness unless unique job states have been overridden with
// UniqueOpts.ByState. So for example, with this default set a new unique job
// may be inserted even if another job already exists, as long as that other job
// is set `cancelled` or `discarded`.
func UniqueOptsByStateDefault() []JobState {
	return []JobState{
		JobStateAvailable,
		JobStateCompleted,
		JobStatePending,
		JobStateRetryable,
		JobStateRunning,
		JobStateScheduled,
	}
}

```

`rivertype/river_type_test.go`:

```go
package rivertype_test

import (
	"go/ast"
	"go/parser"
	"go/token"
	"os"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/rivertype"
)

func TestJobRow_Output(t *testing.T) {
	t.Parallel()

	t.Run("SimpleStringOutput", func(t *testing.T) {
		t.Parallel()

		jobRow := &rivertype.JobRow{
			Metadata: []byte(`{"output": "test"}`),
		}
		require.Equal(t, []byte(`"test"`), jobRow.Output())
	})

	t.Run("ComplexObjectOutput", func(t *testing.T) {
		t.Parallel()
		jobRow := &rivertype.JobRow{
			Metadata: []byte(`{"output": {"foo": {"bar": "baz"}}}`),
		}
		require.JSONEq(t, `{"foo": {"bar": "baz"}}`, string(jobRow.Output()))
	})

	t.Run("NoOutput", func(t *testing.T) {
		t.Parallel()
		jobRow := &rivertype.JobRow{
			Metadata: []byte(`{}`),
		}
		require.Nil(t, jobRow.Output())
	})

	t.Run("InvalidMetadata", func(t *testing.T) {
		t.Parallel()

		jobRow := &rivertype.JobRow{
			Metadata: []byte(`not-json`),
		}
		require.Nil(t, jobRow.Output())
	})
}

func TestJobStates(t *testing.T) {
	t.Parallel()

	jobStates := rivertype.JobStates()

	// One easy check that doesn't require the source file reading below.
	require.Contains(t, jobStates, rivertype.JobStateAvailable)

	// Get all job state names from the corresponding source file and make sure
	// they're included in JobStates. Helps check that we didn't add a new value
	// but forgot to add it to the full list of constant values.
	for _, nameAndValue := range allValuesForStringConstantType(t, "river_type.go", "JobState") {
		t.Logf("Checking for job state: %s / %s", nameAndValue.Name, nameAndValue.Value)
		require.Contains(t, jobStates, rivertype.JobState(nameAndValue.Value))
	}
}

// stringConstantNameAndValue is a name and value for a string constant like
// `JobStateAvailable` + `available`.
type stringConstantNameAndValue struct{ Name, Value string }

// allValuesForStringConstantType reads a Go source file and looks for all
// values for the named string constant.
func allValuesForStringConstantType(t *testing.T, srcFile, typeName string) []stringConstantNameAndValue {
	t.Helper()

	fset := token.NewFileSet()

	src, err := os.ReadFile(srcFile)
	require.NoError(t, err)

	f, err := parser.ParseFile(fset, srcFile, src, parser.ParseComments)
	require.NoError(t, err)

	var valueNames []stringConstantNameAndValue

	for _, decl := range f.Decls {
		if gen, ok := decl.(*ast.GenDecl); ok && gen.Tok == token.CONST {
			for _, spec := range gen.Specs {
				// Always ast.ValueSpec for token.CONST.
				valueSpec := spec.(*ast.ValueSpec) //nolint:forcetypeassert

				typeIdent, ok := valueSpec.Type.(*ast.Ident)
				if !ok || typeIdent.Name != typeName {
					continue
				}

				for i, nameIdent := range valueSpec.Names {
					// Force type assert because we expect one of our constants
					// to be defined as a basic type literal like this.
					basicLitExpr := valueSpec.Values[i].(*ast.BasicLit) //nolint:forcetypeassert

					valueNames = append(valueNames, stringConstantNameAndValue{
						Name:  nameIdent.Name,
						Value: basicLitExpr.Value[1 : len(basicLitExpr.Value)-1], // strip quote on either side
					})
				}
			}
		}
	}

	if len(valueNames) < 1 {
		require.FailNow(t, "No values found", "No values found for source file and constant type: %s / %s", srcFile, typeName)
	}

	return valueNames
}

```

`rivertype/time_generator.go`:

```go
package rivertype

import "time"

// TimeGenerator generates a current time in UTC. In test environments it's
// implemented by riverinternaltest.timeStub which lets the current time be
// stubbed. Otherwise, it's implemented as UnStubbableTimeGenerator which
// doesn't allow stubbing.
type TimeGenerator interface {
	// NowUTC returns the current time. This may be a stubbed time if the time
	// has been actively stubbed in a test.
	NowUTC() time.Time

	// NowUTCOrNil returns if the currently stubbed time _if_ the current time
	// is stubbed, and returns nil otherwise. This is generally useful in cases
	// where a component may want to use a stubbed time if the time is stubbed,
	// but to fall back to a database time default otherwise.
	NowUTCOrNil() *time.Time
}

```

`subscription_manager.go`:

```go
package river

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/jobstats"
	"github.com/riverqueue/river/rivershared/baseservice"
	"github.com/riverqueue/river/rivershared/startstop"
	"github.com/riverqueue/river/rivershared/util/sliceutil"
	"github.com/riverqueue/river/rivertype"
)

type subscriptionManager struct {
	baseservice.BaseService
	startstop.BaseStartStop

	subscribeCh <-chan []jobcompleter.CompleterJobUpdated

	statsMu        sync.Mutex // protects stats fields
	statsAggregate jobstats.JobStatistics
	statsNumJobs   int

	mu               sync.Mutex // protects subscription fields
	subscriptions    map[int]*eventSubscription
	subscriptionsSeq int // used for generating simple IDs
}

func newSubscriptionManager(archetype *baseservice.Archetype, subscribeCh <-chan []jobcompleter.CompleterJobUpdated) *subscriptionManager {
	return baseservice.Init(archetype, &subscriptionManager{
		subscribeCh:   subscribeCh,
		subscriptions: make(map[int]*eventSubscription),
	})
}

// ResetSubscribeChan is used to change the channel that the subscription
// manager listens on. It must only be called when the subscription manager is
// stopped.
func (sm *subscriptionManager) ResetSubscribeChan(subscribeCh <-chan []jobcompleter.CompleterJobUpdated) {
	sm.subscribeCh = subscribeCh
}

func (sm *subscriptionManager) Start(ctx context.Context) error {
	ctx, shouldStart, started, stopped := sm.StartInit(ctx)
	if !shouldStart {
		return nil
	}

	go func() {
		started()
		defer stopped() // this defer should come first so it's last out

		sm.Logger.DebugContext(ctx, sm.Name+": Run loop started")
		defer sm.Logger.DebugContext(ctx, sm.Name+": Run loop stopped")

		// On shutdown, close and remove all active subscriptions.
		defer func() {
			sm.mu.Lock()
			defer sm.mu.Unlock()

			for subID, sub := range sm.subscriptions {
				close(sub.Chan)
				delete(sm.subscriptions, subID)
			}
		}()

		for {
			select {
			case <-ctx.Done():
				// Distribute remaining subscriptions until the channel is
				// closed. This does make the subscription manager a little
				// problematic in that it requires the subscription channel to
				// be closed before it will fully stop. This always happens in
				// the case of a real client by virtue of the completer always
				// stopping at the same time as the subscription manager, but
				// one has to be careful in tests.
				sm.Logger.DebugContext(ctx, sm.Name+": Stopping; distributing subscriptions until channel is closed")
				for updates := range sm.subscribeCh {
					sm.distributeJobUpdates(updates)
				}

				return

			case updates := <-sm.subscribeCh:
				sm.distributeJobUpdates(updates)
			}
		}
	}()

	return nil
}

func (sm *subscriptionManager) logStats(ctx context.Context, svcName string) {
	sm.statsMu.Lock()
	defer sm.statsMu.Unlock()

	sm.Logger.DebugContext(ctx, svcName+": Job stats (since last stats line)",
		"num_jobs_run", sm.statsNumJobs,
		"average_complete_duration", sm.safeDurationAverage(sm.statsAggregate.CompleteDuration, sm.statsNumJobs),
		"average_queue_wait_duration", sm.safeDurationAverage(sm.statsAggregate.QueueWaitDuration, sm.statsNumJobs),
		"average_run_duration", sm.safeDurationAverage(sm.statsAggregate.RunDuration, sm.statsNumJobs))

	sm.statsAggregate = jobstats.JobStatistics{}
	sm.statsNumJobs = 0
}

// Handles a potential divide by zero.
func (sm *subscriptionManager) safeDurationAverage(d time.Duration, n int) time.Duration {
	if n == 0 {
		return 0
	}
	return d / time.Duration(n)
}

// Receives updates from the completer and prompts the client to update
// statistics and distribute jobs into any listening subscriber channels.
// (Subscriber channels are non-blocking so this should be quite fast.)
func (sm *subscriptionManager) distributeJobUpdates(updates []jobcompleter.CompleterJobUpdated) {
	func() {
		sm.statsMu.Lock()
		defer sm.statsMu.Unlock()

		for _, update := range updates {
			stats := update.JobStats
			sm.statsAggregate.CompleteDuration += stats.CompleteDuration
			sm.statsAggregate.QueueWaitDuration += stats.QueueWaitDuration
			sm.statsAggregate.RunDuration += stats.RunDuration
			sm.statsNumJobs++
		}
	}()

	sm.mu.Lock()
	defer sm.mu.Unlock()

	// Quick path so we don't need to allocate anything if no one is listening.
	if len(sm.subscriptions) < 1 {
		return
	}

	for _, update := range updates {
		sm.distributeJobEvent(update.Job, jobStatisticsFromInternal(update.JobStats))
	}
}

// Distribute a single event into any listening subscriber channels.
//
// Job events should specify the job and stats, while queue events should only specify
// the queue.
//
// MUST be called with sm.mu already held.
func (sm *subscriptionManager) distributeJobEvent(job *rivertype.JobRow, stats *JobStatistics) {
	var event *Event
	switch job.State {
	case rivertype.JobStateCancelled:
		event = &Event{Kind: EventKindJobCancelled, Job: job, JobStats: stats}
	case rivertype.JobStateCompleted:
		event = &Event{Kind: EventKindJobCompleted, Job: job, JobStats: stats}
	case rivertype.JobStateScheduled:
		event = &Event{Kind: EventKindJobSnoozed, Job: job, JobStats: stats}
	case rivertype.JobStateAvailable, rivertype.JobStateDiscarded, rivertype.JobStateRetryable, rivertype.JobStateRunning:
		event = &Event{Kind: EventKindJobFailed, Job: job, JobStats: stats}
	case rivertype.JobStatePending:
		panic("completion subscriber unexpectedly received job in pending state, river bug")
	default:
		// linter exhaustive rule prevents this from being reached
		panic("unreachable state to distribute, river bug")
	}

	// All subscription channels are non-blocking so this is always fast and
	// there's no risk of falling behind what producers are sending.
	for _, sub := range sm.subscriptions {
		if sub.ListensFor(event.Kind) {
			// TODO: THIS IS UNSAFE AND WILL LEAD TO DROPPED EVENTS.
			//
			// We are allocating subscriber channels with a fixed size of 1000, but
			// potentially processing job events in batches of 5000 (batch completer
			// max batch size). It's probably not possible for the subscriber to keep
			// up with these bursts.
			select {
			case sub.Chan <- event:
			default:
			}
		}
	}
}

func (sm *subscriptionManager) distributeQueueEvent(event *Event) {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	// All subscription channels are non-blocking so this is always fast and
	// there's no risk of falling behind what producers are sending.
	for _, sub := range sm.subscriptions {
		if sub.ListensFor(event.Kind) {
			select {
			case sub.Chan <- event:
			default:
			}
		}
	}
}

// Special internal variant that lets us inject an overridden size.
func (sm *subscriptionManager) SubscribeConfig(config *SubscribeConfig) (<-chan *Event, func()) {
	if config.ChanSize < 0 {
		panic("SubscribeConfig.ChanSize must be greater or equal to 1")
	}
	if config.ChanSize == 0 {
		config.ChanSize = subscribeChanSizeDefault
	}

	for _, kind := range config.Kinds {
		if _, ok := allKinds[kind]; !ok {
			panic(fmt.Errorf("unknown event kind: %s", kind))
		}
	}

	subChan := make(chan *Event, config.ChanSize)

	sm.mu.Lock()
	defer sm.mu.Unlock()

	// Just gives us an easy way of removing the subscription again later.
	subID := sm.subscriptionsSeq
	sm.subscriptionsSeq++

	sm.subscriptions[subID] = &eventSubscription{
		Chan:  subChan,
		Kinds: sliceutil.KeyBy(config.Kinds, func(k EventKind) (EventKind, struct{}) { return k, struct{}{} }),
	}

	cancel := func() {
		sm.mu.Lock()
		defer sm.mu.Unlock()

		// May no longer be present in case this was called after a stop.
		sub, ok := sm.subscriptions[subID]
		if !ok {
			return
		}

		close(sub.Chan)

		delete(sm.subscriptions, subID)
	}

	return subChan, cancel
}

```

`subscription_manager_test.go`:

```go
package river

import (
	"context"
	"testing"
	"time"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/jobcompleter"
	"github.com/riverqueue/river/internal/jobstats"
	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/riverdriver"
	"github.com/riverqueue/river/riverdriver/riverpgxv5"
	"github.com/riverqueue/river/rivershared/riversharedtest"
	"github.com/riverqueue/river/rivershared/startstoptest"
	"github.com/riverqueue/river/rivershared/testfactory"
	"github.com/riverqueue/river/rivershared/util/ptrutil"
	"github.com/riverqueue/river/rivertype"
)

func Test_SubscriptionManager(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct {
		exec        riverdriver.Executor
		subscribeCh chan []jobcompleter.CompleterJobUpdated
		tx          pgx.Tx
	}

	setup := func(t *testing.T) (*subscriptionManager, *testBundle) {
		t.Helper()

		tx := riverinternaltest.TestTx(ctx, t)
		exec := riverpgxv5.New(nil).UnwrapExecutor(tx)

		subscribeCh := make(chan []jobcompleter.CompleterJobUpdated, 1)
		manager := newSubscriptionManager(riversharedtest.BaseServiceArchetype(t), subscribeCh)

		require.NoError(t, manager.Start(ctx))
		t.Cleanup(manager.Stop)

		return manager, &testBundle{
			exec:        exec,
			subscribeCh: subscribeCh,
			tx:          tx,
		}
	}

	t.Run("DistributesRequestedEventsToSubscribers", func(t *testing.T) {
		t.Parallel()

		manager, bundle := setup(t)
		t.Cleanup(func() { close(bundle.subscribeCh) })

		sub, cancelSub := manager.SubscribeConfig(&SubscribeConfig{ChanSize: 10, Kinds: []EventKind{EventKindJobCompleted, EventKindJobSnoozed}})
		t.Cleanup(cancelSub)

		// Send some events
		job1 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCompleted), FinalizedAt: ptrutil.Ptr(time.Now())})
		job2 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateCancelled), FinalizedAt: ptrutil.Ptr(time.Now())})
		job3 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateRetryable)})
		job4 := testfactory.Job(ctx, t, bundle.exec, &testfactory.JobOpts{State: ptrutil.Ptr(rivertype.JobStateScheduled)})

		makeStats := func(complete, wait, run time.Duration) *jobstats.JobStatistics {
			return &jobstats.JobStatistics{
				CompleteDuration:  complete,
				QueueWaitDuration: wait,
				RunDuration:       run,
			}
		}

		bundle.subscribeCh <- []jobcompleter.CompleterJobUpdated{
			{Job: job1, JobStats: makeStats(101, 102, 103)}, // completed, should be sent
			{Job: job2, JobStats: makeStats(201, 202, 203)}, // cancelled, should be skipped
		}
		bundle.subscribeCh <- []jobcompleter.CompleterJobUpdated{
			{Job: job3, JobStats: makeStats(301, 302, 303)}, // retryable, should be skipped
			{Job: job4, JobStats: makeStats(401, 402, 403)}, // snoozed/scheduled, should be sent
		}

		received := riversharedtest.WaitOrTimeoutN(t, sub, 2)
		require.Equal(t, job1.ID, received[0].Job.ID)
		require.Equal(t, rivertype.JobStateCompleted, received[0].Job.State)
		require.Equal(t, time.Duration(101), received[0].JobStats.CompleteDuration)
		require.Equal(t, time.Duration(102), received[0].JobStats.QueueWaitDuration)
		require.Equal(t, time.Duration(103), received[0].JobStats.RunDuration)
		require.Equal(t, job4.ID, received[1].Job.ID)
		require.Equal(t, rivertype.JobStateScheduled, received[1].Job.State)
		require.Equal(t, time.Duration(401), received[1].JobStats.CompleteDuration)
		require.Equal(t, time.Duration(402), received[1].JobStats.QueueWaitDuration)
		require.Equal(t, time.Duration(403), received[1].JobStats.RunDuration)

		cancelSub()
		select {
		case value, stillOpen := <-sub:
			require.False(t, stillOpen, "subscription channel should be closed")
			require.Nil(t, value, "subscription channel should be closed")
		default:
			require.Fail(t, "subscription channel should have been closed")
		}
	})

	t.Run("StartStopRepeatedly", func(t *testing.T) {
		// This service does not use the typical `startstoptest.Stress()` test
		// because there are some additional steps required after a `Stop` for the
		// subsequent `Start` to succeed. It's also not friendly for multiple
		// concurrent calls to `Start` and `Stop`, but this is fine because the only
		// usage within `Client` is already protected by a mutex.
		t.Parallel()

		manager, bundle := setup(t)

		subscribeCh := bundle.subscribeCh
		for range 100 {
			close(subscribeCh)
			manager.Stop()

			subscribeCh = make(chan []jobcompleter.CompleterJobUpdated, 1)
			manager.ResetSubscribeChan(subscribeCh)

			require.NoError(t, manager.Start(ctx))
		}
		close(subscribeCh)
	})

	t.Run("StartStopStress", func(t *testing.T) {
		t.Parallel()

		svc, bundle := setup(t)

		// Close the subscription channel in advance so that stops can leave
		// successfully.
		close(bundle.subscribeCh)

		startstoptest.Stress(ctx, t, svc)
	})
}

```

`work_unit_wrapper.go`:

```go
package river

import (
	"context"
	"encoding/json"
	"time"

	"github.com/riverqueue/river/internal/hooklookup"
	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/rivertype"
)

// workUnitFactoryWrapper wraps a Worker to implement workUnitFactory.
type workUnitFactoryWrapper[T JobArgs] struct {
	worker Worker[T]
}

func (w *workUnitFactoryWrapper[T]) MakeUnit(jobRow *rivertype.JobRow) workunit.WorkUnit {
	return &wrapperWorkUnit[T]{jobRow: jobRow, worker: w.worker}
}

// wrapperWorkUnit implements workUnit for a job and Worker.
type wrapperWorkUnit[T JobArgs] struct {
	job    *Job[T] // not set until after UnmarshalJob is invoked
	jobRow *rivertype.JobRow
	worker Worker[T]
}

func (w *wrapperWorkUnit[T]) HookLookup(lookup *hooklookup.JobHookLookup) hooklookup.HookLookupInterface {
	var job T
	return lookup.ByJobArgs(job)
}

func (w *wrapperWorkUnit[T]) Middleware() []rivertype.WorkerMiddleware {
	return w.worker.Middleware(w.jobRow)
}
func (w *wrapperWorkUnit[T]) NextRetry() time.Time           { return w.worker.NextRetry(w.job) }
func (w *wrapperWorkUnit[T]) Timeout() time.Duration         { return w.worker.Timeout(w.job) }
func (w *wrapperWorkUnit[T]) Work(ctx context.Context) error { return w.worker.Work(ctx, w.job) }

func (w *wrapperWorkUnit[T]) UnmarshalJob() error {
	w.job = &Job[T]{
		JobRow: w.jobRow,
	}

	return json.Unmarshal(w.jobRow.EncodedArgs, &w.job.Args)
}

```

`worker.go`:

```go
package river

import (
	"context"
	"fmt"
	"time"

	"github.com/riverqueue/river/internal/workunit"
	"github.com/riverqueue/river/rivertype"
)

// Worker is an interface that can perform a job with args of type T. A typical
// implementation will be a JSON-serializable `JobArgs` struct that implements
// `Kind()`, along with a Worker that embeds WorkerDefaults and implements `Work()`.
// Workers may optionally override other methods to provide job-specific
// configuration for all jobs of that type:
//
//	type SleepArgs struct {
//		Duration time.Duration `json:"duration"`
//	}
//
//	func (SleepArgs) Kind() string { return "sleep" }
//
//	type SleepWorker struct {
//		WorkerDefaults[SleepArgs]
//	}
//
//	func (w *SleepWorker) Work(ctx context.Context, job *Job[SleepArgs]) error {
//		select {
//		case <-ctx.Done():
//			return ctx.Err()
//		case <-time.After(job.Args.Duration):
//			return nil
//		}
//	}
//
// In addition to fulfilling the Worker interface, workers must be registered
// with the client using the AddWorker function.
type Worker[T JobArgs] interface {
	// Middleware returns the type-specific middleware for this job.
	Middleware(job *rivertype.JobRow) []rivertype.WorkerMiddleware

	// NextRetry calculates when the next retry for a failed job should take
	// place given when it was last attempted and its number of attempts, or any
	// other of the job's properties a user-configured retry policy might want
	// to consider.
	//
	// Note that this method on a worker overrides any client-level retry policy.
	// To use the client-level retry policy, return an empty `time.Time{}` or
	// include WorkerDefaults to do this for you.
	NextRetry(job *Job[T]) time.Time

	// Timeout is the maximum amount of time the job is allowed to run before
	// its context is cancelled. A timeout of zero (the default) means the job
	// will inherit the Client-level timeout. A timeout of -1 means the job's
	// context will never time out.
	Timeout(job *Job[T]) time.Duration

	// Work performs the job and returns an error if the job failed. The context
	// will be configured with a timeout according to the worker settings and may
	// be cancelled for other reasons.
	//
	// If no error is returned, the job is assumed to have succeeded and will be
	// marked completed.
	//
	// It is important for any worker to respect context cancellation to enable
	// the client to respond to shutdown requests; there is no way to cancel a
	// running job that does not respect context cancellation, other than
	// terminating the process.
	Work(ctx context.Context, job *Job[T]) error
}

// WorkerDefaults is an empty struct that can be embedded in your worker
// struct to make it fulfill the Worker interface with default values.
type WorkerDefaults[T JobArgs] struct{}

func (w WorkerDefaults[T]) Hooks(*rivertype.JobRow) []rivertype.Hook { return nil }

func (w WorkerDefaults[T]) Middleware(*rivertype.JobRow) []rivertype.WorkerMiddleware { return nil }

// NextRetry returns an empty time.Time{} to avoid setting any job or
// Worker-specific overrides on the next retry time. This means that the
// Client-level retry policy schedule will be used instead.
func (w WorkerDefaults[T]) NextRetry(*Job[T]) time.Time { return time.Time{} }

// Timeout returns the job-specific timeout. Override this method to set a
// job-specific timeout, otherwise the Client-level timeout will be applied.
func (w WorkerDefaults[T]) Timeout(*Job[T]) time.Duration { return 0 }

// AddWorker registers a Worker on the provided Workers bundle. Each Worker must
// be registered so that the Client knows it should handle a specific kind of
// job (as returned by its `Kind()` method).
//
// Use by explicitly specifying a JobArgs type and then passing an instance of a
// worker for the same type:
//
//	river.AddWorker(workers, &SortWorker{})
//
// Note that AddWorker can panic in some situations, such as if the worker is
// already registered or if its configuration is otherwise invalid. This default
// probably makes sense for most applications because you wouldn't want to start
// an application with invalid hardcoded runtime configuration. If you want to
// avoid panics, use AddWorkerSafely instead.
func AddWorker[T JobArgs](workers *Workers, worker Worker[T]) {
	if err := AddWorkerSafely[T](workers, worker); err != nil {
		panic(err)
	}
}

// AddWorkerSafely registers a worker on the provided Workers bundle. Unlike AddWorker,
// AddWorkerSafely does not panic and instead returns an error if the worker
// is already registered or if its configuration is invalid.
//
// Use by explicitly specifying a JobArgs type and then passing an instance of a
// worker for the same type:
//
//	river.AddWorkerSafely[SortArgs](workers, &SortWorker{}).
func AddWorkerSafely[T JobArgs](workers *Workers, worker Worker[T]) error {
	var jobArgs T
	return workers.add(jobArgs, &workUnitFactoryWrapper[T]{worker: worker})
}

// Workers is a list of available job workers. A Worker must be registered for
// each type of Job to be handled.
//
// Use the top-level AddWorker function combined with a Workers to register a
// worker.
type Workers struct {
	workersMap map[string]workerInfo // job kind -> worker info
}

// workerInfo bundles information about a registered worker for later lookup
// in a Workers bundle.
type workerInfo struct {
	jobArgs         JobArgs
	workUnitFactory workunit.WorkUnitFactory
}

// NewWorkers initializes a new registry of available job workers.
//
// Use the top-level AddWorker function combined with a Workers registry to
// register each available worker.
func NewWorkers() *Workers {
	return &Workers{
		workersMap: make(map[string]workerInfo),
	}
}

func (w Workers) add(jobArgs JobArgs, workUnitFactory workunit.WorkUnitFactory) error {
	kind := jobArgs.Kind()

	if _, ok := w.workersMap[kind]; ok {
		return fmt.Errorf("worker for kind %q is already registered", kind)
	}

	w.workersMap[kind] = workerInfo{
		jobArgs:         jobArgs,
		workUnitFactory: workUnitFactory,
	}

	return nil
}

// workFunc implements JobArgs and is used to wrap a function given to WorkFunc.
type workFunc[T JobArgs] struct {
	WorkerDefaults[T]
	kind string
	f    func(context.Context, *Job[T]) error
}

func (wf *workFunc[T]) Kind() string {
	return wf.kind
}

func (wf *workFunc[T]) Work(ctx context.Context, job *Job[T]) error {
	return wf.f(ctx, job)
}

// WorkFunc wraps a function to implement the Worker interface. A job args
// struct implementing JobArgs will still be required to specify a Kind.
//
// For example:
//
//	river.AddWorker(workers, river.WorkFunc(func(ctx context.Context, job *river.Job[WorkFuncArgs]) error {
//		fmt.Printf("Message: %s", job.Args.Message)
//		return nil
//	}))
func WorkFunc[T JobArgs](f func(context.Context, *Job[T]) error) Worker[T] {
	return &workFunc[T]{f: f, kind: (*new(T)).Kind()}
}

```

`worker_test.go`:

```go
package river

import (
	"context"
	"testing"

	"github.com/jackc/pgx/v5"
	"github.com/stretchr/testify/require"

	"github.com/riverqueue/river/internal/riverinternaltest"
	"github.com/riverqueue/river/rivershared/riversharedtest"
)

func TestWork(t *testing.T) {
	t.Parallel()

	workers := NewWorkers()

	AddWorker(workers, &noOpWorker{})
	require.Contains(t, workers.workersMap, (noOpArgs{}).Kind())

	require.PanicsWithError(t, `worker for kind "noOp" is already registered`, func() {
		AddWorker(workers, &noOpWorker{})
	})

	fn := func(ctx context.Context, job *Job[callbackArgs]) error { return nil }
	ch := callbackWorker{fn: fn}

	// function worker
	AddWorker(workers, &ch)
	require.Contains(t, workers.workersMap, (callbackArgs{}).Kind())
}

type configurableArgs struct {
	uniqueOpts UniqueOpts
}

func (a configurableArgs) Kind() string { return "configurable" }
func (a configurableArgs) InsertOpts() InsertOpts {
	return InsertOpts{UniqueOpts: a.uniqueOpts}
}

type configurableWorker struct {
	WorkerDefaults[configurableArgs]
}

func (w *configurableWorker) Work(ctx context.Context, job *Job[configurableArgs]) error {
	return nil
}

func TestWorkers_add(t *testing.T) {
	t.Parallel()

	workers := NewWorkers()

	err := workers.add(noOpArgs{}, &workUnitFactoryWrapper[noOpArgs]{worker: &noOpWorker{}})
	require.NoError(t, err)

	// Different worker kind.
	err = workers.add(configurableArgs{}, &workUnitFactoryWrapper[configurableArgs]{worker: &configurableWorker{}})
	require.NoError(t, err)

	err = workers.add(noOpArgs{}, &workUnitFactoryWrapper[noOpArgs]{worker: &noOpWorker{}})
	require.EqualError(t, err, `worker for kind "noOp" is already registered`)
}

type WorkFuncArgs struct{}

func (WorkFuncArgs) Kind() string { return "work_func" }

type StructWithFunc struct {
	WorkChan chan struct{}
}

func (s *StructWithFunc) Work(ctx context.Context, job *Job[WorkFuncArgs]) error {
	s.WorkChan <- struct{}{}
	return nil
}

func TestWorkFunc(t *testing.T) {
	t.Parallel()

	ctx := context.Background()

	type testBundle struct{}

	setup := func(t *testing.T) (*Client[pgx.Tx], *testBundle) {
		t.Helper()

		dbPool := riverinternaltest.TestDB(ctx, t)

		client := newTestClient(t, dbPool, newTestConfig(t, nil))
		startClient(ctx, t, client)

		return client, &testBundle{}
	}

	t.Run("RoundTrip", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		workChan := make(chan struct{})
		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[WorkFuncArgs]) error {
			workChan <- struct{}{}
			return nil
		}))

		_, err := client.Insert(ctx, &WorkFuncArgs{}, nil)
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, workChan)
	})

	t.Run("StructFunction", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		structWithFunc := &StructWithFunc{
			WorkChan: make(chan struct{}),
		}

		AddWorker(client.config.Workers, WorkFunc(structWithFunc.Work))

		_, err := client.Insert(ctx, &WorkFuncArgs{}, nil)
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, structWithFunc.WorkChan)
	})

	t.Run("JobArgsReflectKind", func(t *testing.T) {
		t.Parallel()

		client, _ := setup(t)

		type InFuncWorkFuncArgs struct {
			JobArgsReflectKind[InFuncWorkFuncArgs]
		}

		workChan := make(chan struct{})
		AddWorker(client.config.Workers, WorkFunc(func(ctx context.Context, job *Job[InFuncWorkFuncArgs]) error {
			workChan <- struct{}{}
			return nil
		}))

		_, err := client.Insert(ctx, &InFuncWorkFuncArgs{}, nil)
		require.NoError(t, err)

		riversharedtest.WaitOrTimeout(t, workChan)
	})
}

```