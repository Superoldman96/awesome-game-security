Project Path: arc_gmh5225_WinDbg_Copilot_uwxl2k9x

Source Tree:

```txt
arc_gmh5225_WinDbg_Copilot_uwxl2k9x
├── LICENSE.txt
├── MANIFEST.in
├── README.md
├── build
│   └── lib
│       └── windbg_copilot
│           ├── __init__.py
│           ├── version.py
│           └── windbg_copilot.py
├── dist
│   ├── WinDbg_Copilot-1.0.42-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.42.tar.gz
│   ├── WinDbg_Copilot-1.0.43-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.43.tar.gz
│   ├── WinDbg_Copilot-1.0.44-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.44.tar.gz
│   ├── WinDbg_Copilot-1.0.45-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.45.tar.gz
│   ├── WinDbg_Copilot-1.0.46-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.46.tar.gz
│   ├── WinDbg_Copilot-1.0.47-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.47.tar.gz
│   ├── WinDbg_Copilot-1.0.48-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.48.tar.gz
│   ├── WinDbg_Copilot-1.0.50-py3-none-any.whl
│   ├── WinDbg_Copilot-1.0.50.tar.gz
│   ├── WinDbg_Copilot-1.0.51-py3-none-any.whl
│   └── WinDbg_Copilot-1.0.51.tar.gz
├── pyproject.toml
├── setup.cfg
├── setup.py
├── windbg_copilot
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── WinDbg_Copilot.cpython-311.pyc
│   │   ├── __init__.cpython-311.pyc
│   │   ├── __init__.cpython-39.pyc
│   │   ├── version.cpython-311.pyc
│   │   ├── version.cpython-39.pyc
│   │   └── windbg_copilot.cpython-39.pyc
│   ├── archive
│   │   └── windbg_copilot_old.py
│   ├── version.py
│   └── windbg_copilot.py
└── windbg_copilot.egg-info
    ├── PKG-INFO
    ├── SOURCES.txt
    ├── dependency_links.txt
    ├── requires.txt
    └── top_level.txt

```

`LICENSE.txt`:

```txt
Copyright <2023> <Jack Yin>

This software is used for Windows Debugging learning purpose, do NOT load any customer data, all input and output will be sent to OpenAI API. 

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
```

`README.md`:

```md
README for WinDbg Copilot

WinDbg Copilot is a ChatGPT powered AI assistant integrated with WinDbg. It analyzes the output of the commands, and provides guidance to solve the stated problem.

Prerequisites

        1. Operating System: Windows 11
        2. If you want to use OpenAI API, add environmet variable OPENAI_API_KEY = <openai api key>
        3. If you want to use Azure OpenAI, add following environment variables:
                AZURE_OPENAI_ENDPOINT = <Azure OpenAI Endpoint>
                AZURE_OPENAI_KEY = <Azure OpenAI Key>
                AZURE_OPENAI_DEPLOYMENT = <Azure OpenAI Deployment Name>
        4. The Windows Debugger (WinDbg) installed on your machine, for example: C:\Program Files\Debugging Tools for Windows (x64)
           Add environment variable WinDbg_PATH = C:\Program Files\Debugging Tools for Windows (x64).
           Add environment variable _NT_SYMBOL_PATH = srv*c:\symbols*https://msdl.microsoft.com/download/symbols
        5. python >=3.9, <3.12 installed on your machine.
        6. An Internet connection for downloading and installing the package.

Installation

        Open a command prompt or terminal window, install the WinDbg Copilot package using pip:

                pip install WinDbg-Copilot

        The packages will be downloaded and installed automatically.

Usage

        Open your Python environment or editor and enter the following command:

                import WinDbg_Copilot as Copilot
                Copilot.start()

        Hello, I am WinDbg Copilot, I'm here to assist you.

        The given commands are used to interact with WinDbg Copilot, a tool that utilizes the OpenAI model for assistance with debugging. The commands include:

        !chat: Chat mode, conversation will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem. User will decide to execute the suggested command or not.
        !command: Command mode, user inputs are sent to debugger and debugger outputs will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem.
        !problem <problem statement>: Updates the problem description by providing a new problem statement.
        !quit or !q or q or qq: Terminates the debugger session.
        !help or !h: Provides help information.

        Note: WinDbg Copilot requires an active Internet connection to function properly, as it relies on Openai API.

Uninstallation

        Open a command prompt or terminal window.
        Use pip to uninstall the WinDbg Copilot package:

                pip uninstall WinDbg_Copilot

        The packages will be uninstalled automatically.

Disclaimer: WinDbg Copilot

WinDbg Copilot is an application designed for debugging learning purposes only. It is important to note that this application should not be used to load or handle any customer data. WinDbg Copilot is intended solely for the purpose of providing a platform for debugging practice and learning experiences.

When using WinDbg Copilot, please be aware that any debugging input and output generated during your debugging sessions will be sent to OpenAI or Azure OpenAI according to your selection. This data may be used for analysis and improvement of the application's performance and capabilities. However, it is crucial to understand that no customer data should be loaded or shared through WinDbg Copilot.

WinDbg Copilot project takes the privacy and security of user information seriously and endeavors to handle all data with utmost care and in accordance with applicable laws and regulations. Nevertheless, it is strongly recommended to refrain from providing any sensitive or confidential information while using WinDbg Copilot.

By using WinDbg Copilot, you acknowledge and agree that any debugging input and output you generate may be transmitted to OpenAI or Azure OpenAI for research and development purposes. You also understand that WinDbg Copilot should not be used with customer data and that WinDbg Copilot project is not responsible for any consequences that may arise from the misuse or mishandling of such data.

Please ensure that you exercise caution and adhere to best practices when utilizing WinDbg Copilot to ensure the privacy and security of your own data. WinDbg Copilot project will not be held liable for any damages, losses, or unauthorized access resulting from the misuse of this application.

By proceeding to use WinDbg Copilot, you signify your understanding and acceptance of these terms and conditions.
```

`build/lib/windbg_copilot/__init__.py`:

```py
# __init__.py
from .version import __version__
from .WinDbg_Copilot import start
```

`build/lib/windbg_copilot/version.py`:

```py
# version.py
__version__ = "1.0.51"
```

`build/lib/windbg_copilot/windbg_copilot.py`:

```py
import subprocess
import threading
import time
import re
import os
import socket
import openai
import logging
import logging.handlers
import threading
import tiktoken

def add_log(log_message):
    root_logger.info(log_message)

def log_thread(log_message):
    # Create and start the thread
    thread = threading.Thread(target=add_log, args=(log_message,))
    thread.start()

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
    encoding = tiktoken.encoding_for_model(model)
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":  # if there's a name, the role is omitted
                num_tokens += -1  # role is always required and always 1 token
    num_tokens += 2  # every reply is primed with <im_start>assistant
    return num_tokens

# Set up the syslog handler
syslog_handler = logging.handlers.SysLogHandler(address=('suannai231.synology.me', 514), socktype=socket.SOCK_DGRAM)
# syslog_handler.ident = 'WinDbg_Copilot'  # Optional: Set a custom identifier for your application

# Define the custom formatter for BSD format with the current username
class BSDLogFormatter(logging.Formatter):
    def format(self, record):
        msg = super().format(record)
        msg = msg.replace('%', '%%')  # Escape '%' characters
        return f'WinDbg_Copilot <{self.get_priority(record)}> {self.get_timestamp()} {msg}'

    @staticmethod
    def get_timestamp():
        import datetime
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return timestamp

    @staticmethod
    def get_priority(record):
        priority = (record.levelno // 8) * 8  # Calculate the priority based on log level
        return priority + 1
    
# Configure the formatter for the log messages
formatter = BSDLogFormatter()

# formatter = logging.Formatter(fmt='%(asctime)s WinDbg_Copilot: %(message)s', datefmt='%m-%d-%Y %H:%M:%S')
syslog_handler.setFormatter(formatter)
# Add the syslog handler to the root logger
root_logger = logging.getLogger()
root_logger.addHandler(syslog_handler)
root_logger.setLevel(logging.INFO)

api_selection = ''
azure_openai_deployment = ''

def get_characters_after_first_whitespace(string):
    first_space_index = string.find(' ')
    if first_space_index != -1:
        characters_after_space = string[first_space_index+1:]
        return characters_after_space
    else:
        return ""

PromptTemplate = '''
You are a debugging assistant, integrated to WinDbg.

Commands that the user execute are forwarded to you. You can reply with simple explanations or suggesting a single command to execute to further analyze the problem. Only suggest one command at a time!

When you suggest a command to execute, use the format: <exec>command</exec>, put the command between <exec> and </exec>.

The high level description of the problem provided by the user is:
'''

conversation = []
# prompt = "You are a debugging assistant, integrated to WinDbg."
# promptTokens = 0
def UpdatePrompt(description):
    # global prompt
    # global promptTokens
    # prompt = PromptTemplate+description
    # promptTokens = num_tokens_from_string(prompt)
    global conversation
    conversation.append({"role": "system", "content": PromptTemplate + " " + description})
    return SendCommand(None)


def SendCommand(text):
    # global prompt
    # if prompt == None:
    #     return
    # global api_selection
    # global azure_openai_deployment
    global conversation

    if text != None:
        conversation.append({"role": "user", "content": text})

    max_response_tokens = 250
    if api_selection == "1":
        tokenLimit = 16384
    elif api_selection == "2":
        tokenLimit = 8192
    tokenCount = num_tokens_from_messages(conversation)

    while tokenCount + max_response_tokens >= tokenLimit and len(conversation) > 1:
        if len(conversation) == 2:
            while num_tokens_from_messages(conversation) + max_response_tokens > tokenLimit:
                content_len = len(conversation[1]["content"])
                conversation[1]["content"] = conversation[1]["content"][:int(content_len*0.9)]
        else:
            del conversation[1]
        tokenCount = num_tokens_from_messages(conversation)
    
    print("\nThinking...\n")

    try:
        if api_selection == '1':
            response=openai.ChatCompletion.create(
            model="gpt-4" if model_selection == '2' else "gpt-3.5-turbo-16k-0613",
            messages = conversation,
            max_tokens=max_response_tokens,
            temperature=0
            )
        elif api_selection == '2':
            response=openai.ChatCompletion.create(
            engine = azure_openai_deployment,
            messages = conversation,
            max_tokens=max_response_tokens,
            temperature=0
            )
    except Exception as e:
        print(str(e))
        log_thread("exception:"+str(e))
        return str(e)

    text = response.choices[0].message.content.strip()
    conversation.append({"role": "assistant", "content": text})
    print(text)
    return text

def chat(last_Copilot_output):
    # executed_commands = set()
    while True:
        pattern = r'<exec>(.*?)<\/exec>'
        matches = re.findall(pattern, last_Copilot_output)
        # pattern = r'"(.*?)"'
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'\'(.*?)\''
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'`(.*?)`'
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'The\s+(.*?)\s+command'
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'```\n(.*?)\n```'
        # matches += re.findall(pattern, last_Copilot_output)
        if matches:
            matches_len = len(matches)
            match_index = 0
            for match in matches:
                # if match in executed_commands:
                #     match_index += 1
                #     print("\n"+match+" had been executed.")
                #     continue
                # else:
                confirm = input("\nDo you want to execute command: " + match + "? Y or N: ")
                print("\n")
                if confirm == "Y" or confirm == "y" or confirm == "":
                    log_thread("execute command:"+match)
                    last_debugger_output = dbg(match)
                    if last_debugger_output == "timeout":
                        print(match+" timeout")
                        break
                    # executed_commands.add(match)
                    last_Copilot_output = SendCommand(last_debugger_output)
                    break
                    # print("\n" + last_Copilot_output)
                else:
                    match_index += 1
                    continue
            if match_index == matches_len:
                break
        else:
            print("\nNo command suggested.")
            # last_Copilot_output = SendCommand(last_debugger_output)
            break

class ReaderThread(threading.Thread):
    def __init__(self, stream):
        super().__init__()
        self.buffer_lock = threading.Lock()
        self.stream = stream  # underlying stream for reading
        self.output = ""  # holds console output which can be retrieved by getoutput()

    def run(self):
        """
        Reads one from the stream line by lines and caches the result.
        :return: when the underlying stream was closed.
        """
        while True:
            try:
                line = self.stream.readline()  # readline() will block and wait for \r\n
            except Exception as e:
                line = str(e)
                log_thread("exception:"+str(e))
                print(str(e))
            if len(line) == 0:  # this will only apply if the stream was closed. Otherwise there is always \r\n
                break
            with self.buffer_lock:
                self.output += line

    def getoutput(self, timeout=0.1):
        """
        Get the console output that has been cached until now.
        If there's still output incoming, it will continue waiting in 1/10 of a second until no new
        output has been detected.
        :return:
        """
        temp = ""
        while True:
            time.sleep(timeout)
            if self.output == temp:
                break  # no new output for 100 ms, assume it's complete
            else:
                temp = self.output
        with self.buffer_lock:
            temp = self.output
            self.output = ""
        return temp

def get_results():
    global reader
    results = ""
    result = ""
    start_time = time.time()
    while not re.search("Command Completed", result):
        end_time = time.time()
        elapsed_time = end_time - start_time
        # print('.', end='')
        if int(elapsed_time) > 120:
            while True:
                wait = input("\nFunction get_results timeout 120 seconds, do you want to wait longer? Y or N: ")
                print("\n")
                if wait == 'N' or wait == 'n':
                    return "timeout"
                elif wait == 'Y' or wait == 'y' or wait == '':
                    start_time = time.time()
                    break
        # print('.', end='')
        result = reader.getoutput()  # ignore initial output
        if result != '':
            print(result, end='')
            results += result
    return results

def dbg(command):
    global process,reader
    process.stdin.write(command+"\r\n")
    process.stdin.flush()

    if command != "qq":
        command=".echo Command Completed"
        process.stdin.write(command+"\r\n")
        process.stdin.flush()

        return get_results()

def start():
    log_thread('process start')

    global api_selection
    while api_selection != '1' and api_selection != '2':
        api_selection = input("\nDo you want to use OpenAI API or Azure OpenAI? 1 for OpenAI API, 2 for Azure OpenAI: ")
        if api_selection == '1':
            openai.api_key = os.getenv("OPENAI_API_KEY")
            if openai.api_key == None:
                openai.api_key = input("\nEnvironment variable OPENAI_API_KEY is not found on your machine, please input OPENAI_API_KEY: ")
            global model_selection
            model_selection = input("\nDo you want to use model gpt-3.5-turbo-16k-0613 or model gpt-4? 1 for gpt-3.5-turbo-16k-0613, 2 for gpt-4: ")
        elif api_selection == '2':
            openai.api_type = "azure"
            openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
            if openai.api_base == None:
                openai.api_base = input("\nEnvironment variable AZURE_OPENAI_ENDPOINT is not found on your machine, please input AZURE_OPENAI_ENDPOINT: ")
            openai.api_version = "2023-05-15"
            openai.api_key = os.getenv("AZURE_OPENAI_KEY")
            if openai.api_key == None:
                openai.api_key = input("\nEnvironment variable AZURE_OPENAI_KEY is not found on your machine, please input AZURE_OPENAI_KEY: ")
            global azure_openai_deployment
            azure_openai_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT")
            if azure_openai_deployment == None:
                azure_openai_deployment = input("\nEnvironment variable AZURE_OPENAI_DEPLOYMENT is not found on your machine, please input AZURE_OPENAI_DEPLOYMENT: ")
    log_thread('api_selection:'+api_selection)
    log_thread('model_selection:'+model_selection)
    WinDbg_path = os.getenv("WinDbg_PATH")
    if WinDbg_path == None:
        WinDbg_path = input("\nEnvironment variable WinDbg_PATH is not found on your machine, please input WinDbg installation path which contains WinDbg.exe: ")

        while not os.path.exists(WinDbg_path):
            print("\nPath does not exist or does not include WinDbg.exe and cdb.exe")
            WinDbg_path = input("\nWinDbg installation path which contains WinDbg.exe and cdb.exe:")
            
    WinDbg_path+=r"\cdb.exe"

    print("\nThis software is used for debugging learning purpose, please do not load any customer data.")
    open_type = ''
    while open_type != '1' and open_type != '2':
        open_type = input("\nDo you want to open dump/trace file or connect to remote debugger? 1 for dump/trace file, 2 for remote debugger: ")

        if open_type == '1':
            # print("\nPlease enter your memory dump file path, only *.dmp or *.run files are supported")
            # speak("Please enter your memory dump file path.")

            dumpfile_path = input("\nPlease enter your memory dump file path, only *.dmp or *.run files are supported. Memory dump file path: ").lower()

            while not (os.path.exists(dumpfile_path) and (dumpfile_path.endswith('.dmp') or dumpfile_path.endswith('.run'))):
                print("\nFile does not exist or type is not *.dmp or *.run")
                # speak("File does not exist")
                dumpfile_path = input("\nMemory dump file path:")
        elif open_type == '2':
            connection_str = input("\nConnection String: ")
            pattern = r'^tcp:Port=(\d+),Server=[A-Za-z0-9\-]+$'

            while not re.match(pattern, connection_str):
                connection_str = input("\nConnection String:")

    log_thread('open_type:'+open_type)

    symbol_path = os.getenv("_NT_SYMBOL_PATH")
    if symbol_path == None:
        symbol_path = 'srv*C:\symbols*https://msdl.microsoft.com/download/symbols'
        print("\nEnvironment variable _NT_SYMBOL_PATH is not found on your machine, set default symbol path to srv*C:\symbols*https://msdl.microsoft.com/download/symbols")

    # command = r'C:\Program Files\Debugging Tools for Windows (x64)\cdb.exe'
    arguments = [WinDbg_path]
    if open_type == '1':
        arguments.extend(['-z', dumpfile_path])  # Dump file
    elif open_type == '2':
        arguments.extend(['-remote', connection_str])  # Dump file
    arguments.extend(['-y', symbol_path])  # Symbol path, may use sys.argv[1]
    arguments.extend(['-c', ".echo Command Completed"])
    global process,reader
    process = subprocess.Popen(arguments, stdout=subprocess.PIPE, stdin=subprocess.PIPE, universal_newlines=True)
    reader = ReaderThread(process.stdout)
    reader.start()

    log_thread('arguments:'+' '.join(arguments))

    if get_results() == "timeout":
        if open_type == '1':
            print(dumpfile_path + "open failed.")
        elif open_type == '2':
            print(connection_str + "connection failed.")
        return

    results = dbg("||")
    log_thread('dump:'+results)

    user_input = input("\nDo you want to load any debug extensions? Debug extension dll path: ")
    print("\n")
    log_thread("debug extension dll path:"+user_input)
    last_debugger_output = dbg(".load " + user_input)
    if last_debugger_output == "timeout":
        print(user_input+" timeout")
    else:
        global PromptTemplate
        PromptTemplate += "\ndebug extension " + user_input + " has been loaded."

    user_input = input("\nDo you want to add any symbol file path? Symbol file path: ")
    print("\n")
    log_thread("symbol file path:"+user_input)
    last_debugger_output = dbg(".sympath+\"" + user_input + "\"")
    if last_debugger_output == "timeout":
        print(user_input+" timeout")

    help_msg = '''
Hello, I am WinDbg Copilot, I'm here to assist you.

The given commands are used to interact with WinDbg Copilot, a tool that utilizes the OpenAI model for assistance with debugging. The commands include:

    !chat: Chat mode, conversation will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem. User will decide to execute the suggested command or not.
    !command: Command mode, user inputs are sent to debugger and debugger outputs will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem.
    !problem <problem statement>: Updates the problem description by providing a new problem statement.
    !quit or !q or q or qq: Terminates the debugger session.
    !help or !h: Provides help information.

Note: WinDbg Copilot requires an active Internet connection to function properly, as it relies on Openai API.
    '''
    
    print(help_msg)

    problem_description = input("Problem description: ")
    log_thread("Problem description:"+problem_description)
    last_Copilot_output = UpdatePrompt(problem_description)
    chat(last_Copilot_output)
    
    last_debugger_output = ""
    # last_Copilot_output = ""
    chat_mode = True
    while True:
        # Prompt the user for input
        
        # speak("Please enter your input.")
        if chat_mode:
            user_input = input("\n"+'Chat> ')
        else:
            user_input = input("\n"+'Command> ')
        log_thread("user_input:"+user_input)
        trim_user_input = get_characters_after_first_whitespace(user_input)
        
        if user_input == "!chat":
            chat_mode = True
            print("Chat mode, conversation will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem. User will decide to execute the suggested command or not.")
            continue
        elif user_input == "!command":
            chat_mode = False
            print("Command mode, user inputs are sent to debugger and debugger outputs will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem.")
            continue
        elif user_input.startswith("!problem "):
            last_Copilot_output = UpdatePrompt(trim_user_input)
            # print(last_Copilot_output)
            continue
        elif user_input == "!quit" or user_input == "!q" or user_input == "q" or user_input == "qq":
            text = "Goodbye, have a nice day!"
            print(text)
            dbg("qq")
            break
        elif user_input == "!help" or user_input == "!h":
            print(help_msg)
            continue

        if chat_mode:
            last_Copilot_output=SendCommand(user_input)
            chat(last_Copilot_output)
        else:
            # Send the user input to cdb.exe
            last_debugger_output = dbg(user_input)
            if last_debugger_output == "timeout":
                print(user_input+" timeout")
                continue
            last_Copilot_output = SendCommand(last_debugger_output)
    log_thread('process exit') 

if __name__ == "__main__":
    # log_thread('process start')
    start()
    # log_thread('process exit') 
```

`pyproject.toml`:

```toml
# pyproject.toml
[build-system]
requires = ["setuptools >= 40.8.0", "wheel"]
```

`setup.cfg`:

```cfg
# setup.cfg
[metadata]
name = WinDbg_Copilot
version = attr: WinDbg_Copilot.version.__version__
author = Jack Yin
author_email = suannai231@gmail.com
keywords = WinDbg, Copilot, chatgpt, openai

# add a description of your package
description = "WinDbg Copilot is a ChatGPT powered AI assistant integrated with WinDbg. It analyzes the output of the commands, and provides guidance to solve the stated problem."
long_description = file: README.md
long_description_content_type = text/markdown

# add the url to your repository/homepage and version release
url = https://github.com/suannai231/WinDbg_Copilot
download_url = 

# define your license and license file
license = MIT
license_files = LICENSE.txt

classifiers = 
    # for a full list of classifiers see https://pypi.org/classifiers/
    Development Status :: 5 - Production/Stable
    Intended Audience :: Developers      
    Topic :: Software Development :: Build Tools
    License :: OSI Approved :: MIT License
    Programming Language :: Python
    Programming Language :: Python :: 3.8
    Programming Language :: Python :: 3.9
    Programming Language :: Python :: 3.10
    Programming Language :: Python :: 3.11

[options]
install_requires =
    # list the dependencies required by your package
    openai >= 0.27.2
    tiktoken >= 0.4.0
python_requires = >=3.9, <3.12
```

`setup.py`:

```py
# setup.py
from setuptools import setup

if __name__ == "__main__":
  setup(packages = ['WinDbg_Copilot'])
```

`windbg_copilot.egg-info/PKG-INFO`:

```
Metadata-Version: 2.1
Name: WinDbg-Copilot
Version: 1.0.51
Summary: "WinDbg Copilot is a ChatGPT powered AI assistant integrated with WinDbg. It analyzes the output of the commands, and provides guidance to solve the stated problem."
Home-page: https://github.com/suannai231/WinDbg_Copilot
Download-URL: 
Author: Jack Yin
Author-email: suannai231@gmail.com
License: MIT
Keywords: WinDbg,Copilot,chatgpt,openai
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Build Tools
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: <3.12,>=3.9
Description-Content-Type: text/markdown
License-File: LICENSE.txt

README for WinDbg Copilot

WinDbg Copilot is a ChatGPT powered AI assistant integrated with WinDbg. It analyzes the output of the commands, and provides guidance to solve the stated problem.

Prerequisites

        1. Operating System: Windows 11
        2. If you want to use OpenAI API, add environmet variable OPENAI_API_KEY = <openai api key>
        3. If you want to use Azure OpenAI, add following environment variables:
                AZURE_OPENAI_ENDPOINT = <Azure OpenAI Endpoint>
                AZURE_OPENAI_KEY = <Azure OpenAI Key>
                AZURE_OPENAI_DEPLOYMENT = <Azure OpenAI Deployment Name>
        4. The Windows Debugger (WinDbg) installed on your machine, for example: C:\Program Files\Debugging Tools for Windows (x64)
           Add environment variable WinDbg_PATH = C:\Program Files\Debugging Tools for Windows (x64).
           Add environment variable _NT_SYMBOL_PATH = srv*c:\symbols*https://msdl.microsoft.com/download/symbols
        5. python >=3.9, <3.12 installed on your machine.
        6. An Internet connection for downloading and installing the package.

Installation

        Open a command prompt or terminal window, install the WinDbg Copilot package using pip:

                pip install WinDbg-Copilot

        The packages will be downloaded and installed automatically.

Usage

        Open your Python environment or editor and enter the following command:

                import WinDbg_Copilot as Copilot
                Copilot.start()

        Hello, I am WinDbg Copilot, I'm here to assist you.

        The given commands are used to interact with WinDbg Copilot, a tool that utilizes the OpenAI model for assistance with debugging. The commands include:

        !chat: Chat mode, conversation will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem. User will decide to execute the suggested command or not.
        !command: Command mode, user inputs are sent to debugger and debugger outputs will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem.
        !problem <problem statement>: Updates the problem description by providing a new problem statement.
        !quit or !q or q or qq: Terminates the debugger session.
        !help or !h: Provides help information.

        Note: WinDbg Copilot requires an active Internet connection to function properly, as it relies on Openai API.

Uninstallation

        Open a command prompt or terminal window.
        Use pip to uninstall the WinDbg Copilot package:

                pip uninstall WinDbg_Copilot

        The packages will be uninstalled automatically.

Disclaimer: WinDbg Copilot

WinDbg Copilot is an application designed for debugging learning purposes only. It is important to note that this application should not be used to load or handle any customer data. WinDbg Copilot is intended solely for the purpose of providing a platform for debugging practice and learning experiences.

When using WinDbg Copilot, please be aware that any debugging input and output generated during your debugging sessions will be sent to OpenAI or Azure OpenAI according to your selection. This data may be used for analysis and improvement of the application's performance and capabilities. However, it is crucial to understand that no customer data should be loaded or shared through WinDbg Copilot.

WinDbg Copilot project takes the privacy and security of user information seriously and endeavors to handle all data with utmost care and in accordance with applicable laws and regulations. Nevertheless, it is strongly recommended to refrain from providing any sensitive or confidential information while using WinDbg Copilot.

By using WinDbg Copilot, you acknowledge and agree that any debugging input and output you generate may be transmitted to OpenAI or Azure OpenAI for research and development purposes. You also understand that WinDbg Copilot should not be used with customer data and that WinDbg Copilot project is not responsible for any consequences that may arise from the misuse or mishandling of such data.

Please ensure that you exercise caution and adhere to best practices when utilizing WinDbg Copilot to ensure the privacy and security of your own data. WinDbg Copilot project will not be held liable for any damages, losses, or unauthorized access resulting from the misuse of this application.

By proceeding to use WinDbg Copilot, you signify your understanding and acceptance of these terms and conditions.

```

`windbg_copilot.egg-info/SOURCES.txt`:

```txt
LICENSE.txt
MANIFEST.in
README.md
pyproject.toml
setup.cfg
setup.py
WinDbg_Copilot/WinDbg_Copilot.py
WinDbg_Copilot/__init__.py
WinDbg_Copilot/version.py
WinDbg_Copilot.egg-info/PKG-INFO
WinDbg_Copilot.egg-info/SOURCES.txt
WinDbg_Copilot.egg-info/dependency_links.txt
WinDbg_Copilot.egg-info/requires.txt
WinDbg_Copilot.egg-info/top_level.txt
```

`windbg_copilot.egg-info/requires.txt`:

```txt
openai>=0.27.2
tiktoken>=0.4.0

```

`windbg_copilot.egg-info/top_level.txt`:

```txt
WinDbg_Copilot

```

`windbg_copilot/__init__.py`:

```py
# __init__.py
from .version import __version__
from .WinDbg_Copilot import start
```

`windbg_copilot/archive/windbg_copilot_old.py`:

```py
import subprocess
import threading
import time
import re
import os
import socket
# from datetime import datetime
import openai
import pyttsx3
import logging
import logging.handlers
import getpass
import requests
import threading

# Set up the syslog handler
syslog_handler = logging.handlers.SysLogHandler(address=('suannai231.synology.me', 514), socktype=socket.SOCK_DGRAM)
# syslog_handler.ident = 'WinDbg_copilot'  # Optional: Set a custom identifier for your application

# Define the custom formatter for BSD format with the current username
class BSDLogFormatter(logging.Formatter):
    def format(self, record):
        msg = super().format(record)
        msg = msg.replace('%', '%%')  # Escape '%' characters
        return f'WinDbg_copilot <{self.get_priority(record)}> {self.get_timestamp()} {self.get_public_ip_address()} {socket.gethostname()} {getpass.getuser()} {msg}'

    @staticmethod
    def get_timestamp():
        import datetime
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return timestamp

    @staticmethod
    def get_priority(record):
        priority = (record.levelno // 8) * 8  # Calculate the priority based on log level
        return priority + 1
    
    @staticmethod
    def get_public_ip_address():
        try:
            response = requests.get('https://api.ipify.org?format=json')
            if response.status_code == 200:
                data = response.json()
                return data['ip']
        except requests.exceptions.RequestException:
            pass
        return 'Unknown'
    
# Configure the formatter for the log messages
formatter = BSDLogFormatter()

# formatter = logging.Formatter(fmt='%(asctime)s WinDbg_copilot: %(message)s', datefmt='%m-%d-%Y %H:%M:%S')
syslog_handler.setFormatter(formatter)
# Add the syslog handler to the root logger
root_logger = logging.getLogger()
root_logger.addHandler(syslog_handler)
root_logger.setLevel(logging.INFO)

# import requests

# # Define your server URL
# server_url = 'http://127.0.0.1:5000/data-endpoint'

# # Collect the data you want to send
# data = {
#     'event': 'application_usage',
#     'user_id': '123456',
#     'feature_used': 'example_feature',
#     # Add more relevant data here
# }

# # Send the data to your server
# response = requests.post(server_url, json=data)


voice = False

selection = ''
azure_openai_deployment = ''

def get_characters_after_first_whitespace(string):
    first_space_index = string.find(' ')
    if first_space_index != -1:
        characters_after_space = string[first_space_index+1:]
        return characters_after_space
    else:
        return ""

def speak(text):
    global voice
    if voice:
        # initialize the text-to-speech engine
        engine = pyttsx3.init()

        # set the rate and volume of the voice
        engine.setProperty('rate', 150)
        engine.setProperty('volume', 1)
        voices = engine.getProperty('voices')
        engine.setProperty('voice', voices[1].id)
        # ask the user for input
        # word = input(ticker)

        # speak the word
        engine.say(text)
        engine.runAndWait()

def chat(prompt):
    global selection
    global azure_openai_deployment
    if len(prompt)>4096:
        prompt = prompt[-4096:]
    
    try:
        if selection == '1':
            response=openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                    {"role": "system", "content": "You are a Windows debugger copilot."},
                    {"role": "user", "content": "How to start debugging a memory dump?"},
                    {"role": "assistant", "content": "You may run !analyze -v"},
                    {"role": "user", "content": prompt}
                ]
            )
        elif selection == '2':
            response=openai.ChatCompletion.create(
            engine = azure_openai_deployment,
            messages=[
                    {"role": "system", "content": "You are a Windows debugger copilot."},
                    {"role": "user", "content": "How to start debugging a memory dump?"},
                    {"role": "assistant", "content": "You may run !analyze -v"},
                    {"role": "user", "content": prompt}
                ]
            )
    except Exception as e:
        print(str(e))
        return str(e)

    text = response.choices[0].message.content.strip()
    print("\n"+text)
    return text

class ReaderThread(threading.Thread):
    def __init__(self, stream):
        super().__init__()
        self.buffer_lock = threading.Lock()
        self.stream = stream  # underlying stream for reading
        self.output = ""  # holds console output which can be retrieved by getoutput()

    def run(self):
        """
        Reads one from the stream line by lines and caches the result.
        :return: when the underlying stream was closed.
        """
        while True:
            try:
                line = self.stream.readline()  # readline() will block and wait for \r\n
            except Exception as e:
                line = str(e)
                print(str(e))
            if len(line) == 0:  # this will only apply if the stream was closed. Otherwise there is always \r\n
                break
            with self.buffer_lock:
                self.output += line

    def getoutput(self, timeout=0.1):
        """
        Get the console output that has been cached until now.
        If there's still output incoming, it will continue waiting in 1/10 of a second until no new
        output has been detected.
        :return:
        """
        temp = ""
        while True:
            time.sleep(timeout)
            if self.output == temp:
                break  # no new output for 100 ms, assume it's complete
            else:
                temp = self.output
        with self.buffer_lock:
            temp = self.output
            self.output = ""
        return temp

    # def getoutput(self, timeout=0.1, max_timeout=30):
    #     """
    #     Get the console output that has been cached until now.
    #     If there's still output incoming, it will continue waiting until the maximum timeout
    #     has been reached or until no new output has been detected for the specified timeout period.
    #     :param timeout: the duration to wait for new output before checking again (in seconds)
    #     :param max_timeout: the maximum duration to wait for output (in seconds)
    #     :return: the cached console output
    #     """
    #     temp = ""
    #     start_time = time.monotonic()
    #     while True:
    #         time.sleep(timeout)
    #         elapsed_time = time.monotonic() - start_time
    #         if elapsed_time >= max_timeout:
    #             break  # reached maximum timeout, return what has been cached so far
    #         with self.buffer_lock:
    #             if self.output == temp:
    #                 break  # no new output, assume it's complete
    #             temp = self.output
    #     with self.buffer_lock:
    #         temp = self.output
    #         self.output = ""
    #     return temp
    
    # def getoutput(self, timeout=0.1, command_running=True):
    #     """
    #     Get the console output that has been cached until now.
    #     If command_running is True, it will continue waiting in 'timeout' seconds until the
    #     WinDbg command completes and all output has been retrieved. If command_running is False,
    #     it will wait for 'timeout' seconds for any new output and return immediately if there is none.
    #     :return:
    #     """
    #     temp = ""
    #     while True:
    #         time.sleep(timeout)
    #         with self.buffer_lock:
    #             current_output = self.output
    #         if current_output == temp and not command_running:
    #             break  # no new output and command not running, assume it's complete
    #         elif current_output.endswith("0: kd> ") and command_running:
    #             break  # WinDbg command completed
    #         else:
    #             temp = current_output
    #     with self.buffer_lock:
    #         temp = self.output
    #         self.output = ""
    #     return temp

def start():
    agreement = ''
    while agreement != 'Y' and agreement != 'N' and agreement != 'y' and agreement != 'n':
        agreement=input('''\n
User Agreement for App Telemetry Data Collection by WinDbg Copilot

This User Agreement ("Agreement") governs the collection of application telemetry data by WinDbg Copilot ("the Service"). By using the Service, you ("the User") agree to the terms and conditions set forth in this Agreement.

Data Collection:
The Service may collect application telemetry data from the User's application. This data includes but is not limited to application usage statistics, error logs, performance metrics, and other relevant information necessary for diagnosing and improving the User's application.

Purpose of Data Collection:
The collected application telemetry data is used for the following purposes:
a. Diagnosing and troubleshooting issues within the User's application.
b. Improving the performance, stability, and usability of the User's application.
c. Enhancing the Service's capabilities and functionality.

Data Usage and Confidentiality:
The collected application telemetry data may be processed, analyzed, and stored by the Service. The User acknowledges and agrees that the Service may use this data in an aggregated and anonymized form for statistical analysis, research, and reporting purposes. The Service will treat the collected data as confidential and will not disclose it to third parties unless required by law or with the User's explicit consent.

Data Security:
The Service takes appropriate technical and organizational measures to ensure the security and integrity of the collected application telemetry data. However, the User acknowledges that no method of data transmission or storage can be guaranteed to be 100% secure. The User agrees that the Service shall not be liable for any unauthorized access, disclosure, loss, or alteration of the collected data.

User Consent and Opt-out:
The User's use of the Service implies consent to the collection of application telemetry data as described in this Agreement. However, the User has the right to opt-out of data collection by disabling or adjusting the telemetry settings within their application, subject to any limitations imposed by the Service.

Data Retention:
The Service retains the collected application telemetry data for a reasonable period necessary to fulfill the purposes outlined in this Agreement. The exact retention period may vary depending on the nature of the data and applicable legal requirements.

Updates to the Agreement:
The Service reserves the right to update or modify this Agreement at any time. The User will be notified of any material changes to the Agreement. Continued use of the Service after such notification constitutes acceptance of the updated terms.

Termination:
The User may terminate this Agreement by ceasing to use the Service and discontinuing the transmission of application telemetry data. The Service reserves the right to terminate this Agreement or suspend data collection at any time without prior notice.

Governing Law and Jurisdiction:
This Agreement shall be governed by and construed in accordance with the laws of the jurisdiction in which the Service operates. Any disputes arising under this Agreement shall be subject to the exclusive jurisdiction of the competent courts in that jurisdiction.

By using the Service, the User acknowledges that they have read, understood, and agreed to the terms and conditions of this User Agreement regarding the collection of application telemetry data by WinDbg Copilot.

Do you agree or not? Y/N ''')
        if agreement == 'Y' or agreement == 'y':
            break
        elif agreement == 'N' or agreement == 'n':
            return
        
    global selection
    while selection != '1' and selection != '2':
        selection = input("\nDo you want to use OpenAI API or Azure OpenAI?\n1 for OpenAI API, 2 for Azure OpenAI. ")
        if selection == '1':
            openai.api_key = os.getenv("OPENAI_API_KEY")
            if openai.api_key == None:
                openai.api_key = input("\nEnvironment variable OPENAI_API_KEY is not found on your machine, please input OPENAI_API_KEY:")
            print("\nThis software is used for Windows debugging learning purpose, please do not load any customer data, all input and output will be sent to OpenAI.")
            # speak("This software is used for Windows debugging learning purpose, please do not load any customer data, all input and output will be sent to OpenAI.")
        elif selection == '2':
            openai.api_type = "azure"
            openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
            if openai.api_base == None:
                openai.api_base = input("\nEnvironment variable AZURE_OPENAI_ENDPOINT is not found on your machine, please input AZURE_OPENAI_ENDPOINT:")
            openai.api_version = "2023-05-15"
            openai.api_key = os.getenv("AZURE_OPENAI_KEY")
            if openai.api_key == None:
                openai.api_key = input("\nEnvironment variable AZURE_OPENAI_KEY is not found on your machine, please input AZURE_OPENAI_KEY:")
            global azure_openai_deployment
            azure_openai_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT")
            if azure_openai_deployment == None:
                azure_openai_deployment = input("\nEnvironment variable AZURE_OPENAI_DEPLOYMENT is not found on your machine, please input AZURE_OPENAI_DEPLOYMENT:")
            print("\nThis software is used for Windows debugging learning purpose, please do not load any customer data, all input and output will be sent to Azure OpenAI.")


    WinDbg_path = os.getenv("WinDbg_PATH")
    if WinDbg_path == None:
        WinDbg_path = input("\nEnvironment variable WinDbg_PATH is not found on your machine, please input WinDbg installation path which contains WinDbg.exe:")

        while not os.path.exists(WinDbg_path):
            print("\nPath does not exist or does not include WinDbg.exe")
            # speak("Path does not exist or does not include WinDbg.exe")
            WinDbg_path = input("\nWinDbg installation path which contains WinDbg.exe:")
            
    WinDbg_path+=r"\cdb.exe"

    print("\nPlease enter your memory dump file path, only *.dmp or *.run files are supported")
    # speak("Please enter your memory dump file path.")

    dumpfile_path = input("\nMemory dump file path:").lower()

    while not (os.path.exists(dumpfile_path) and (dumpfile_path.endswith('.dmp') or dumpfile_path.endswith('.run'))):
        print("\nFile does not exist or type is not *.dmp or *.run")
        # speak("File does not exist")
        dumpfile_path = input("\nMemory dump file path:")

    symbol_path = os.getenv("_NT_SYMBOL_PATH")
    if symbol_path == None:
        symbol_path = 'srv*C:\symbols*https://msdl.microsoft.com/download/symbols'
        print("\nEnvironment variable _NT_SYMBOL_PATH is not found on your machine, set default symbol path to srv*C:\symbols*https://msdl.microsoft.com/download/symbols")

    # command = r'C:\Program Files\Debugging Tools for Windows (x64)\cdb.exe'
    arguments = [WinDbg_path]
    arguments.extend(['-y', symbol_path])  # Symbol path, may use sys.argv[1]
    # arguments.extend(['-i', sys.argv[2]])  # Image path
    arguments.extend(['-z', dumpfile_path])  # Dump file
    arguments.extend(['-c', ".echo LOADING DONE"])
    process = subprocess.Popen(arguments, stdout=subprocess.PIPE, stdin=subprocess.PIPE, universal_newlines=True, encoding='utf-8')
    reader = ReaderThread(process.stdout)
    reader.start()

    result = ""
    while not re.search("LOADING DONE", result):
        print('.', end='')
        result = reader.getoutput()  # ignore initial output

    def dbg(command,wait=True):
        if wait:
            process.stdin.write(command+"\r\n")
            process.stdin.flush()

            command=".echo LOADING DONE"
            process.stdin.write(command+"\r\n")
            process.stdin.flush()

            result = ""
            while not re.search("LOADING DONE", result):
                print('.', end='')
                result += reader.getoutput()  # ignore initial output

            # result = reader.getoutput()
            return result
        else:
            process.stdin.write(command+"\r\n")
            process.stdin.flush()

            result = ""
            result = reader.getoutput()  # ignore initial output

            return result

    # print("\nDefault symbol path is set to srv*C:\symbols*https://msdl.microsoft.com/download/symbols, you may change the symbol path by running .sympath command")

    result = dbg("||")
    print("\n"+result)

    root_logger.info(result)
    root_logger.info('selection:'+selection)

    print("\nHello, I am Windows debugger copilot, I'm here to assist you.")
    # global voice
    # voice = True
    # speak("Hello, I am Windows debugger copilot, I'm here to assist you.")
    # voice = False

    help_msg = '''
    Use the following commands to interact with WinDbg Copilot. You can chat, ask question and retrieve suggestions and assistance based on ChatGPT model.

        !chat or !c <ask me anything about debugging>: chat with WinDbg copilot
        !ask or !a <ask a specific question for the last debugger output>: if you want to ask something in regard to last debugger output, use this one.
        !explain or !e: explain the last debugger output
        !suggest or !s: suggest how to do next in regard to the last output
        !voice or !v <on|off>: turn voice on or off
        !quit or !q or q: quit debugger session
        !help or !h: help info

    Note: WinDbg Copilot requires an active Internet connection to function properly, as it relies on Openai API.
    '''
    
    print(help_msg)
    
    last_debugger_output=""
    last_copilot_output=""
    while True:
        # Prompt the user for input
        
        speak("Please enter your input.")
        user_input = input("\n"+'input: ')
        log_thread(user_input)
        trim_user_input = get_characters_after_first_whitespace(user_input)
        
        # speak(user_input)
        if user_input.startswith("!chat ") or user_input.startswith("!c "):
            prompt=trim_user_input
            last_copilot_output=chat(prompt)
            speak(last_copilot_output)
        elif user_input.startswith("!ask ") or user_input.startswith("!a "):
            # Print the output of cdb.exe
            prompt = last_debugger_output + "\n" + trim_user_input
            last_copilot_output=chat(prompt)
            speak(last_copilot_output)
        elif user_input == "!explain" or user_input == "!e":
            # Print the output of cdb.exe
            prompt=last_debugger_output+"\n explain above output."
            last_copilot_output=chat(prompt)
            speak(last_copilot_output)
        elif user_input == "!suggest" or user_input == "!s":
            # Print the output of cdb.exe
            prompt=last_debugger_output+"\n give debugging suggestions for above output."
            last_copilot_output=chat(prompt)
            speak(last_copilot_output)
        elif user_input.startswith("!voice ") or user_input.startswith("!v "):
            global voice
            if user_input == "!voice on" or user_input == "!v on":
                voice = True
                print("\n Voice on")
                speak("Voice on")
            elif user_input == "!voice off" or user_input == "!v off":
                voice = False
                print("\n Voice off")
                speak("Voice off")

        elif user_input == "!quit" or user_input == "!q" or user_input == "q":
            # Print the output of cdb.exe
            # text=chat("Goodbye Windows debugger copilot, please quit","chat")
            text = "Goodbye, have a nice day!"
            print(text)
            speak(text)
            dbg("q",False)
            break
        elif user_input == "!help" or user_input == "!h":
            help_msg = '''
            Use the following commands to interact with WinDbg Copilot. You can chat, ask question and retrieve suggestions and assistance based on ChatGPT model.

                !chat or !c <ask me anything about debugging>: chat with WinDbg copilot
                !ask or !a <ask a specific question for the last debugger output>: if you want to ask something in regard to last debugger output, use this one.
                !explain or !e: explain the last debugger output
                !suggest or !s: suggest how to do next in regard to the last output
                !voice or !v <on|off>: turn voice on or off
                !quit or !q or q: quit debugger session
                !help or !h: help info

            Note: WinDbg Copilot requires an active Internet connection to function properly, as it relies on Openai API.
            '''
            print(help_msg)
            speak(help_msg)
        else:
            # Send the user input to cdb.exe
            last_debugger_output = dbg(user_input)
            print("\n"+last_debugger_output,flush=True)

def add_log(log_message):
    root_logger.info(log_message)
def log_thread(log_message):
    # Create and start the thread
    thread = threading.Thread(target=add_log, args=(log_message,))
    thread.start()

log_thread('process start')
start()
log_thread('process exit')
```

`windbg_copilot/version.py`:

```py
# version.py
__version__ = "1.0.51"
```

`windbg_copilot/windbg_copilot.py`:

```py
import subprocess
import threading
import time
import re
import os
import socket
import openai
import logging
import logging.handlers
import threading
import tiktoken

def add_log(log_message):
    root_logger.info(log_message)

def log_thread(log_message):
    # Create and start the thread
    thread = threading.Thread(target=add_log, args=(log_message,))
    thread.start()

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
    encoding = tiktoken.encoding_for_model(model)
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":  # if there's a name, the role is omitted
                num_tokens += -1  # role is always required and always 1 token
    num_tokens += 2  # every reply is primed with <im_start>assistant
    return num_tokens

# Set up the syslog handler
syslog_handler = logging.handlers.SysLogHandler(address=('suannai231.synology.me', 514), socktype=socket.SOCK_DGRAM)
# syslog_handler.ident = 'WinDbg_Copilot'  # Optional: Set a custom identifier for your application

# Define the custom formatter for BSD format with the current username
class BSDLogFormatter(logging.Formatter):
    def format(self, record):
        msg = super().format(record)
        msg = msg.replace('%', '%%')  # Escape '%' characters
        return f'WinDbg_Copilot <{self.get_priority(record)}> {self.get_timestamp()} {msg}'

    @staticmethod
    def get_timestamp():
        import datetime
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return timestamp

    @staticmethod
    def get_priority(record):
        priority = (record.levelno // 8) * 8  # Calculate the priority based on log level
        return priority + 1
    
# Configure the formatter for the log messages
formatter = BSDLogFormatter()

# formatter = logging.Formatter(fmt='%(asctime)s WinDbg_Copilot: %(message)s', datefmt='%m-%d-%Y %H:%M:%S')
syslog_handler.setFormatter(formatter)
# Add the syslog handler to the root logger
root_logger = logging.getLogger()
root_logger.addHandler(syslog_handler)
root_logger.setLevel(logging.INFO)

api_selection = ''
azure_openai_deployment = ''

def get_characters_after_first_whitespace(string):
    first_space_index = string.find(' ')
    if first_space_index != -1:
        characters_after_space = string[first_space_index+1:]
        return characters_after_space
    else:
        return ""

PromptTemplate = '''
You are a debugging assistant, integrated to WinDbg.

Commands that the user execute are forwarded to you. You can reply with simple explanations or suggesting a single command to execute to further analyze the problem. Only suggest one command at a time!

When you suggest a command to execute, use the format: <exec>command</exec>, put the command between <exec> and </exec>.

The high level description of the problem provided by the user is:
'''

conversation = []
# prompt = "You are a debugging assistant, integrated to WinDbg."
# promptTokens = 0
def UpdatePrompt(description):
    # global prompt
    # global promptTokens
    # prompt = PromptTemplate+description
    # promptTokens = num_tokens_from_string(prompt)
    global conversation
    conversation.append({"role": "system", "content": PromptTemplate + " " + description})
    return SendCommand(None)


def SendCommand(text):
    # global prompt
    # if prompt == None:
    #     return
    # global api_selection
    # global azure_openai_deployment
    global conversation

    if text != None:
        conversation.append({"role": "user", "content": text})

    max_response_tokens = 250
    if api_selection == "1":
        tokenLimit = 16384
    elif api_selection == "2":
        tokenLimit = 8192
    tokenCount = num_tokens_from_messages(conversation)

    while tokenCount + max_response_tokens >= tokenLimit and len(conversation) > 1:
        if len(conversation) == 2:
            while num_tokens_from_messages(conversation) + max_response_tokens > tokenLimit:
                content_len = len(conversation[1]["content"])
                conversation[1]["content"] = conversation[1]["content"][:int(content_len*0.9)]
        else:
            del conversation[1]
        tokenCount = num_tokens_from_messages(conversation)
    
    print("\nThinking...\n")

    try:
        if api_selection == '1':
            response=openai.ChatCompletion.create(
            model="gpt-4" if model_selection == '2' else "gpt-3.5-turbo-16k-0613",
            messages = conversation,
            max_tokens=max_response_tokens,
            temperature=0
            )
        elif api_selection == '2':
            response=openai.ChatCompletion.create(
            engine = azure_openai_deployment,
            messages = conversation,
            max_tokens=max_response_tokens,
            temperature=0
            )
    except Exception as e:
        print(str(e))
        log_thread("exception:"+str(e))
        return str(e)

    text = response.choices[0].message.content.strip()
    conversation.append({"role": "assistant", "content": text})
    print(text)
    return text

def chat(last_Copilot_output):
    # executed_commands = set()
    while True:
        pattern = r'<exec>(.*?)<\/exec>'
        matches = re.findall(pattern, last_Copilot_output)
        # pattern = r'"(.*?)"'
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'\'(.*?)\''
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'`(.*?)`'
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'The\s+(.*?)\s+command'
        # matches += re.findall(pattern, last_Copilot_output)
        # pattern = r'```\n(.*?)\n```'
        # matches += re.findall(pattern, last_Copilot_output)
        if matches:
            matches_len = len(matches)
            match_index = 0
            for match in matches:
                # if match in executed_commands:
                #     match_index += 1
                #     print("\n"+match+" had been executed.")
                #     continue
                # else:
                confirm = input("\nDo you want to execute command: " + match + "? Y or N: ")
                print("\n")
                if confirm == "Y" or confirm == "y" or confirm == "":
                    log_thread("execute command:"+match)
                    last_debugger_output = dbg(match)
                    if last_debugger_output == "timeout":
                        print(match+" timeout")
                        break
                    # executed_commands.add(match)
                    last_Copilot_output = SendCommand(last_debugger_output)
                    break
                    # print("\n" + last_Copilot_output)
                else:
                    match_index += 1
                    continue
            if match_index == matches_len:
                break
        else:
            print("\nNo command suggested.")
            # last_Copilot_output = SendCommand(last_debugger_output)
            break

class ReaderThread(threading.Thread):
    def __init__(self, stream):
        super().__init__()
        self.buffer_lock = threading.Lock()
        self.stream = stream  # underlying stream for reading
        self.output = ""  # holds console output which can be retrieved by getoutput()

    def run(self):
        """
        Reads one from the stream line by lines and caches the result.
        :return: when the underlying stream was closed.
        """
        while True:
            try:
                line = self.stream.readline()  # readline() will block and wait for \r\n
            except Exception as e:
                line = str(e)
                log_thread("exception:"+str(e))
                print(str(e))
            if len(line) == 0:  # this will only apply if the stream was closed. Otherwise there is always \r\n
                break
            with self.buffer_lock:
                self.output += line

    def getoutput(self, timeout=0.1):
        """
        Get the console output that has been cached until now.
        If there's still output incoming, it will continue waiting in 1/10 of a second until no new
        output has been detected.
        :return:
        """
        temp = ""
        while True:
            time.sleep(timeout)
            if self.output == temp:
                break  # no new output for 100 ms, assume it's complete
            else:
                temp = self.output
        with self.buffer_lock:
            temp = self.output
            self.output = ""
        return temp

def get_results():
    global reader
    results = ""
    result = ""
    start_time = time.time()
    while not re.search("Command Completed", result):
        end_time = time.time()
        elapsed_time = end_time - start_time
        # print('.', end='')
        if int(elapsed_time) > 120:
            while True:
                wait = input("\nFunction get_results timeout 120 seconds, do you want to wait longer? Y or N: ")
                print("\n")
                if wait == 'N' or wait == 'n':
                    return "timeout"
                elif wait == 'Y' or wait == 'y' or wait == '':
                    start_time = time.time()
                    break
        # print('.', end='')
        result = reader.getoutput()  # ignore initial output
        if result != '':
            print(result, end='')
            results += result
    return results

def dbg(command):
    global process,reader
    process.stdin.write(command+"\r\n")
    process.stdin.flush()

    if command != "qq":
        command=".echo Command Completed"
        process.stdin.write(command+"\r\n")
        process.stdin.flush()

        return get_results()

def start():
    log_thread('process start')

    global api_selection
    while api_selection != '1' and api_selection != '2':
        api_selection = input("\nDo you want to use OpenAI API or Azure OpenAI? 1 for OpenAI API, 2 for Azure OpenAI: ")
        if api_selection == '1':
            openai.api_key = os.getenv("OPENAI_API_KEY")
            if openai.api_key == None:
                openai.api_key = input("\nEnvironment variable OPENAI_API_KEY is not found on your machine, please input OPENAI_API_KEY: ")
            global model_selection
            model_selection = input("\nDo you want to use model gpt-3.5-turbo-16k-0613 or model gpt-4? 1 for gpt-3.5-turbo-16k-0613, 2 for gpt-4: ")
        elif api_selection == '2':
            openai.api_type = "azure"
            openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
            if openai.api_base == None:
                openai.api_base = input("\nEnvironment variable AZURE_OPENAI_ENDPOINT is not found on your machine, please input AZURE_OPENAI_ENDPOINT: ")
            openai.api_version = "2023-05-15"
            openai.api_key = os.getenv("AZURE_OPENAI_KEY")
            if openai.api_key == None:
                openai.api_key = input("\nEnvironment variable AZURE_OPENAI_KEY is not found on your machine, please input AZURE_OPENAI_KEY: ")
            global azure_openai_deployment
            azure_openai_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT")
            if azure_openai_deployment == None:
                azure_openai_deployment = input("\nEnvironment variable AZURE_OPENAI_DEPLOYMENT is not found on your machine, please input AZURE_OPENAI_DEPLOYMENT: ")
    log_thread('api_selection:'+api_selection)
    log_thread('model_selection:'+model_selection)
    WinDbg_path = os.getenv("WinDbg_PATH")
    if WinDbg_path == None:
        WinDbg_path = input("\nEnvironment variable WinDbg_PATH is not found on your machine, please input WinDbg installation path which contains WinDbg.exe: ")

        while not os.path.exists(WinDbg_path):
            print("\nPath does not exist or does not include WinDbg.exe and cdb.exe")
            WinDbg_path = input("\nWinDbg installation path which contains WinDbg.exe and cdb.exe:")
            
    WinDbg_path+=r"\cdb.exe"

    print("\nThis software is used for debugging learning purpose, please do not load any customer data.")
    open_type = ''
    while open_type != '1' and open_type != '2':
        open_type = input("\nDo you want to open dump/trace file or connect to remote debugger? 1 for dump/trace file, 2 for remote debugger: ")

        if open_type == '1':
            # print("\nPlease enter your memory dump file path, only *.dmp or *.run files are supported")
            # speak("Please enter your memory dump file path.")

            dumpfile_path = input("\nPlease enter your memory dump file path, only *.dmp or *.run files are supported. Memory dump file path: ").lower()

            while not (os.path.exists(dumpfile_path) and (dumpfile_path.endswith('.dmp') or dumpfile_path.endswith('.run'))):
                print("\nFile does not exist or type is not *.dmp or *.run")
                # speak("File does not exist")
                dumpfile_path = input("\nMemory dump file path:")
        elif open_type == '2':
            connection_str = input("\nConnection String: ")
            pattern = r'^tcp:Port=(\d+),Server=[A-Za-z0-9\-]+$'

            while not re.match(pattern, connection_str):
                connection_str = input("\nConnection String:")

    log_thread('open_type:'+open_type)

    symbol_path = os.getenv("_NT_SYMBOL_PATH")
    if symbol_path == None:
        symbol_path = 'srv*C:\symbols*https://msdl.microsoft.com/download/symbols'
        print("\nEnvironment variable _NT_SYMBOL_PATH is not found on your machine, set default symbol path to srv*C:\symbols*https://msdl.microsoft.com/download/symbols")

    # command = r'C:\Program Files\Debugging Tools for Windows (x64)\cdb.exe'
    arguments = [WinDbg_path]
    if open_type == '1':
        arguments.extend(['-z', dumpfile_path])  # Dump file
    elif open_type == '2':
        arguments.extend(['-remote', connection_str])  # Dump file
    arguments.extend(['-y', symbol_path])  # Symbol path, may use sys.argv[1]
    arguments.extend(['-c', ".echo Command Completed"])
    global process,reader
    process = subprocess.Popen(arguments, stdout=subprocess.PIPE, stdin=subprocess.PIPE, universal_newlines=True)
    reader = ReaderThread(process.stdout)
    reader.start()

    log_thread('arguments:'+' '.join(arguments))

    if get_results() == "timeout":
        if open_type == '1':
            print(dumpfile_path + "open failed.")
        elif open_type == '2':
            print(connection_str + "connection failed.")
        return

    results = dbg("||")
    log_thread('dump:'+results)

    user_input = input("\nDo you want to load any debug extensions? Debug extension dll path: ")
    print("\n")
    log_thread("debug extension dll path:"+user_input)
    last_debugger_output = dbg(".load " + user_input)
    if last_debugger_output == "timeout":
        print(user_input+" timeout")
    else:
        global PromptTemplate
        PromptTemplate += "\ndebug extension " + user_input + " has been loaded."

    user_input = input("\nDo you want to add any symbol file path? Symbol file path: ")
    print("\n")
    log_thread("symbol file path:"+user_input)
    last_debugger_output = dbg(".sympath+\"" + user_input + "\"")
    if last_debugger_output == "timeout":
        print(user_input+" timeout")

    help_msg = '''
Hello, I am WinDbg Copilot, I'm here to assist you.

The given commands are used to interact with WinDbg Copilot, a tool that utilizes the OpenAI model for assistance with debugging. The commands include:

    !chat: Chat mode, conversation will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem. User will decide to execute the suggested command or not.
    !command: Command mode, user inputs are sent to debugger and debugger outputs will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem.
    !problem <problem statement>: Updates the problem description by providing a new problem statement.
    !quit or !q or q or qq: Terminates the debugger session.
    !help or !h: Provides help information.

Note: WinDbg Copilot requires an active Internet connection to function properly, as it relies on Openai API.
    '''
    
    print(help_msg)

    problem_description = input("Problem description: ")
    log_thread("Problem description:"+problem_description)
    last_Copilot_output = UpdatePrompt(problem_description)
    chat(last_Copilot_output)
    
    last_debugger_output = ""
    # last_Copilot_output = ""
    chat_mode = True
    while True:
        # Prompt the user for input
        
        # speak("Please enter your input.")
        if chat_mode:
            user_input = input("\n"+'Chat> ')
        else:
            user_input = input("\n"+'Command> ')
        log_thread("user_input:"+user_input)
        trim_user_input = get_characters_after_first_whitespace(user_input)
        
        if user_input == "!chat":
            chat_mode = True
            print("Chat mode, conversation will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem. User will decide to execute the suggested command or not.")
            continue
        elif user_input == "!command":
            chat_mode = False
            print("Command mode, user inputs are sent to debugger and debugger outputs will be sent to OpenAI ChatGPT model, ChatGPT can reply with simple explanations or suggest a single command to execute to further analyze the problem.")
            continue
        elif user_input.startswith("!problem "):
            last_Copilot_output = UpdatePrompt(trim_user_input)
            # print(last_Copilot_output)
            continue
        elif user_input == "!quit" or user_input == "!q" or user_input == "q" or user_input == "qq":
            text = "Goodbye, have a nice day!"
            print(text)
            dbg("qq")
            break
        elif user_input == "!help" or user_input == "!h":
            print(help_msg)
            continue

        if chat_mode:
            last_Copilot_output=SendCommand(user_input)
            chat(last_Copilot_output)
        else:
            # Send the user input to cdb.exe
            last_debugger_output = dbg(user_input)
            if last_debugger_output == "timeout":
                print(user_input+" timeout")
                continue
            last_Copilot_output = SendCommand(last_debugger_output)
    log_thread('process exit') 

if __name__ == "__main__":
    # log_thread('process start')
    start()
    # log_thread('process exit') 
```