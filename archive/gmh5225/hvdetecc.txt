Project Path: arc_gmh5225_hvdetecc_39m_o02l

Source Tree:

```txt
arc_gmh5225_hvdetecc_39m_o02l
├── benchmark.hpp
├── environment.cpp
├── hwid
│   ├── ahci.hpp
│   ├── bios.cpp
│   ├── bios.hpp
│   ├── disk_id.cpp
│   ├── disk_id.hpp
│   └── nvme.hpp
├── hwid.cpp
├── hypervisor_detection.cpp
├── interrupt_guard.hpp
├── main.cpp
├── os.cpp
└── upause.hpp

```

`benchmark.hpp`:

```hpp
#pragma once
#include <xstd/intrinsics.hpp>
#include <mcrt/interface.hpp>
#include <ia32.hpp>
#include <ia32/perfmon.hpp>
#include <ia32/memory.hpp>
#include <sdk/halp/api.hpp>
#include <sdk/mm/api.hpp>

// Benchmarking logic.
//
namespace benchmark
{
	// Timer information.
	// - Perf capability should be set externally.
	//
	inline int8_t has_mperf = 0;  // 0=no,1=yes,2=yes+rd_only
	inline int8_t has_aperf = 0;
	inline int8_t has_pperf = 0;
	inline int8_t has_irperf = 0;

	// Define all metrics:
	//
	template<ia32::pmu::event_id E>
	struct dynamic_pmc
	{
		inline static bool setup()
		{
			return ia32::pmu::dynamic_set_state(
				0,
				E,
				ia32::pmu::ctr_enable | ia32::pmu::ctr_supervisor,
				true
			);
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			auto v = ia32::read_pmc( 0 );
			ia32::serialize();
			return v;
		}
		inline static void rundown()
		{
			ia32::pmu::dynamic_disable( 0 );
		}
	};
	template<ia32::pmu::event_id E>
	struct fixed_pmc
	{
		inline static const uint32_t index = ia32::pmu::fixed_counter_v<true, E>;

		inline static bool setup()
		{
			auto lindex = ia32::pmu::fixed_set_state(
				E,
				ia32::pmu::ctr_enable | ia32::pmu::ctr_supervisor,
				true
			);
			return lindex != UINT32_MAX;
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			auto v = ia32::read_pmc( index, true );
			ia32::serialize();
			return v;
		}
		inline static void rundown()
		{
			ia32::pmu::fixed_set_state( E, 0 );
		}
	};
	struct tsc
	{
		inline static bool setup()
		{
			return true;
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			if ( first )
				return ia32::read_tsc();
			else
				return ia32::read_tscp().first;
		}
		inline static void rundown()
		{
		}
	};
	struct mperf
	{
		inline static uint64_t msr = 0;
		inline static bool setup()
		{
			switch ( has_mperf )
			{
				case 2:
					msr = IA32_MPERF | 0xC0000000;
					if ( ia32::read_msr( msr ) != 0 )
						return true;
					[[fallthrough]];
				case 1:
					msr = IA32_MPERF;
					if ( ia32::read_msr( msr ) != 0 )
						return true;
					[[fallthrough]];
				default:
					return false;
			}
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			auto v = ia32::read_msr( msr );
			ia32::serialize();
			return v;
		}
		inline static void rundown()
		{
		}
	};
	struct aperf
	{
		inline static uint64_t msr = 0;
		inline static bool setup()
		{
			switch( has_aperf )
			{
				case 2:
					msr = IA32_APERF | 0xC0000000;
					if ( ia32::read_msr( msr ) != 0 )
						return true;
					[[fallthrough]];
				case 1:
					msr = IA32_APERF;
					if ( ia32::read_msr( msr ) != 0 )
						return true;
					[[fallthrough]];
				default:
					return false;
			}
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			auto v = ia32::read_msr( msr );
			ia32::serialize();
			return v;
		}
		inline static void rundown()
		{
		}
	};
	struct pperf
	{
		inline static uint64_t msr = 0;
		inline static bool setup()
		{
			if ( !has_pperf ) return false;
			return ia32::read_msr( IA32_PPERF ) != 0;
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			auto v = ia32::read_msr( IA32_PPERF );
			ia32::serialize();
			return v;
		}
		inline static void rundown()
		{
		}
	};
	struct pkg_energy
	{
		inline static bool setup()
		{
			return ia32::read_msr( IA32_PKG_ENERGY_STATUS ) != 0;
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			auto v = ia32::read_msr( IA32_PKG_ENERGY_STATUS );
			ia32::serialize();
			return v;
		}
		inline static void rundown() {}
	};
	struct dram_energy
	{
		inline static bool setup()
		{
			return ia32::read_msr( IA32_MSR_DRAM_ENERGY_STATUS ) != 0;
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			auto v = ia32::read_msr( IA32_MSR_DRAM_ENERGY_STATUS );
			ia32::serialize();
			return v;
		}
		inline static void rundown(){}
	};
	struct tlb_persistance
	{
		using page_entry_t = std::tuple<volatile uint8_t*, ia32::pt_entry_64*, ia32::pt_entry_64>;
		static constexpr size_t count = 1 /*one zero page*/ + 64 /*probes*/;
		inline static auto pages = []() -> const std::array<page_entry_t, count>&
		{
			static std::array<page_entry_t, count> page_list = {};
			for ( size_t i = 0; i != count; i++ )
			{
				uint8_t* page = mm::allocate_independent_pages( 0x1000, -1ll );
				*page = i == 0 ? 0 : 1;
				auto pte = ia32::mem::get_pte( page );
				page_list[ i ] = { page, pte, *pte };
			}

			crt::atexit( []()
			{
				for ( auto& [page, pte, vpte] : page_list )
				{
					*pte = vpte;
					mm::free_independent_pages( page, 0x1000 );
				}
			} );

			return page_list;
		}();

		inline static bool setup() { return true; }
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			if ( first )
			{
				auto& [zp, zpt, zptv] = pages[ 0 ];

				// For each page:
				//
				for ( size_t n = 1; n != count; n++ )
				{
					// Probe the pages with PFN pointing at 1.
					//
					auto& [tp, tpt, tptv] = pages[ n ];
					tpt->page_frame_number = tptv.page_frame_number;
					for ( size_t n = 0; n != 12; n++ )
						ia32::touch( tp, true );

					// Set the PFN to point at zero page, do not invalidate the TLB.
					//
					tpt->page_frame_number = zptv.page_frame_number;
				}

				// Serialize memory stores, serialize instruction stream.
				//
				ia32::sfence();
				ia32::serialize();
				return 0;
			}
			else
			{
				// Serialize instruction stream.
				//
				ia32::serialize();

				// Access each page starting from the LRU, sum the values read.
				//
				size_t counter = 0;
				for ( size_t n = 1; n != count; n++ )
					counter += *std::get<0>( pages[ n ] );

				// Serialize loads.
				//
				ia32::lfence();
				return counter;
			}
		}
		inline static void rundown(){}
	};
	struct mp_clock
	{
		inline static std::atomic<uint64_t> timestamp = 0;
		inline static uint8_t* jump_point = 0;

		[[gnu::naked, gnu::noinline, no_split]] static void timer()
		{
			__asm
			{
				lea rax, [rip+p]
				mov [jump_point], rax

				xor eax, eax
				lea rcx, [timestamp]
				x:
					inc rax
					mov [rcx], rax
				p:
				jmp x
			}
		}
		inline static bool setup()
		{
			return ia32::is_intel() && timestamp != 0;
		}
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			return timestamp.load();
		}
		inline static void rundown(){}
	};
	struct hpet
	{
		struct hpet_clock
		{
			uint8_t pad1[ 0xF0 ];
			std::atomic<uint64_t> value;
		};
		inline static hpet_clock* const base = *( hpet_clock** ) &halp::hpet_base_address;
		inline static bool setup() { return base; }
		FORCE_INLINE static uint64_t fetch( bool first )
		{
			if ( first )
			{
				auto v1 = base->value.load();
				while ( base->value.compare_exchange_strong( v1, v1 ) )
					yield_cpu();
				return v1;
			}
			else
			{
				ia32::serialize();
				return base->value.load();
			}
		}
		inline static void rundown() {}
	};

	// Define the single metric helper.
	//
	static constexpr int test_count = 48;
	template<typename Metric>
	[[gnu::flatten, no_split, no_obfuscate]] inline static std::optional<uint32_t> run_single( void( *fn )() )
	{
		interrupt_counters ctrs = {};
		interrupt_guard _g{ &ctrs };
		if ( !Metric::setup() )
			return std::nullopt;
		Metric::fetch( true );
		if ( ctrs.has_exception() )
			return std::nullopt;

		// Flush CPU caches.
		//
		ia32::wbinvd();
		ia32::flush_tlb();

		std::array<uint32_t, test_count> results = {};
		for ( int n = -4; n != test_count; n++ )
		{
			// Stall the execution engine and let L1d/DSB//TLB fill.
			//
			for ( size_t n = 0; n != 16; n++ )
			{
				if ( !( ia32::read_tsc() % 0xDEADBEEF ) )
					fn();
				ia32::touch( fn );
				ia32::touch( ia32::get_sp() - 16 * 8 );
				ia32::mfence();
			}

			// Serialize execution, do the measurement, serialize again.
			//
			ia32::serialize();
			auto m1 = Metric::fetch( true );
			fn();
			auto m2 = Metric::fetch( false );
			ia32::serialize();

			// Write the result.
			//
			results[ std::max( n, 0 ) ] = uint32_t( m2 - m1 );
		}

		Metric::rundown();
		std::sort( results.begin(), results.end() );
		return xstd::percentile( results, 0.5 );
	}

	// Define the wrapper testing using every metric.
	//
	inline static cbor::object_t run( void( *fn )() )
	{
		cbor::object_t results = {};
		if ( auto v = run_single<fixed_pmc<ia32::pmu::event_id::clock_tsc>>( fn ) )
			results[ "pmcTsc" ] = cbor::fp_t( *v );
		if ( auto v = run_single<fixed_pmc<ia32::pmu::event_id::clock_core>>( fn ) )
			results[ "pmcCore" ] = cbor::fp_t( *v );
		if ( auto v = run_single<tsc>( fn ) )
			results[ "tsc" ] = cbor::fp_t( *v );
		if ( auto v = run_single<mp_clock>( fn ) )
			results[ "mpc" ] = cbor::fp_t( *v );
		if ( auto v = run_single<mperf>( fn ) )
			results[ "mperf" ] = cbor::fp_t( *v );
		if ( auto v = run_single<aperf>( fn ) )
			results[ "aperf" ] = cbor::fp_t( *v );
		if ( auto v = run_single<pperf>( fn ) )
			results[ "pperf" ] = cbor::fp_t( *v );
		if ( auto v = run_single<hpet>( fn ) )
			results[ "hpet" ] = cbor::fp_t( *v );
		if ( auto v = run_single<tlb_persistance>( fn ) )
			results[ "tlb" ] = cbor::fp_t( *v );
		if ( auto v = run_single<dram_energy>( fn ) )
			results[ "poDram" ] = cbor::fp_t( *v );
		if ( auto v = run_single<pkg_energy>( fn ) )
			results[ "poPkg" ] = cbor::fp_t( *v );
		return results;
	}

	// Lambda wrappers.
	//
	template<xstd::StatelessLambda F>
	inline static auto wrap_fixed_duration( F )
	{
		static const uint64_t cycles_1ms = crt::to_cycles( 1ms );

		return []() __attribute__((flatten, __no_obfuscate__, __no_split__, __enforce_alignment__(64)))
		{
			auto f = F{};
			auto t = ia32::read_tsc() + cycles_1ms;
			while ( ia32::read_tsc() <= t )
				f();
		};
	}
	template<xstd::StatelessLambda F>
	inline static constexpr auto wrap_no_obfuscation( F )
	{
		return []() __attribute__((flatten, __no_obfuscate__, __no_split__, __enforce_alignment__(64)))
		{
			F{}();
		};
	}
};

```

`environment.cpp`:

```cpp
#include <string_view>
#include <ia32.hpp>
#include <ia32/memory.hpp>
#include <xstd/text.hpp>
#include <xstd/hashable.hpp>
#include <ntpp.hpp>
#include <ntpp/ci.hpp>
#include <sdk/kuser/api.hpp>
#include <sdk/hal/api.hpp>
#include <sdk/nt/work_queue_item_t.hpp>
#include <sdk/nt/device_object_t.hpp>
#include <sdk/nt/devobj_extension_t.hpp>
#include <sdk/nt/driver_object_t.hpp>

// Validates the system environment.
//
extern "C" [[gnu::dllexport]] transport::packet* envValidate()
{
	cbor::array_t detections = {};

	// Find the patchguard context.
	//
	auto* nt_base = *( win::image_x64_t** ) &ps::ntos_image_base;
	auto* nt_hdrs = nt_base->get_nt_headers();
	void** pg_context = nullptr;
	for ( auto& scn : nt_hdrs->sections() )
	{
		if ( xstd::make_ahash( scn.name.to_string() ) != ".data"_ahash )
			continue;

		// Find the fixed context data.
		//
		const void* ctx_suffix[] =
		{
			&ke::bug_check_ex,
			&ke::bug_check2,
			&ki::bug_check_debug_break
		};
		auto begin = nt_base->raw_to_ptr<uint8_t>( scn.virtual_address );
		auto end = nt_base->raw_to_ptr<uint8_t>( scn.virtual_address + std::min( scn.virtual_size, scn.size_raw_data ) );
		auto it = std::search( begin + 8, end, ( const uint8_t* ) std::begin( ctx_suffix ), ( const uint8_t* ) std::end( ctx_suffix ) );
		if ( it == end )
			continue;

		pg_context = ( void** ) ( it - 8 );
		break;
	}

	// Patchguard should be setting the offset 0x00, if not set, it never ran.
	//
	if ( pg_context && !*pg_context )
		detections.emplace_back( cbor::object_t{ { "flag", "pg.noPgBoot" } } );

	// Verify the code integrity of every driver.
	//
	for ( ldr::km::data_table_entry_t* img : ntpp::module_list{} )
	{
		// Read the image.
		//
		std::wstring_view full_path{ img->full_dll_name };
		auto data = ntpp::read_file( full_path );
		if ( !data )
		{
			// If this is a dump driver, try finding the real entry.
			//
			if ( size_t n = full_path.find( L"\\dump_" ); n != std::string::npos )
			{
				std::wstring new_path{ full_path };
				new_path.erase( n + xstd::strlen( L"\\dump" ), 1 );
				if ( data = ntpp::read_file( new_path ); !data )
				{
					new_path.erase( n + 1, xstd::strlen( L"dump" ) );
					data = ntpp::read_file( new_path );
				}
			}
		}

		// If we could read it:
		//
		if ( data )
		{
			// Skip if the checksum does not match.
			//
			auto* mem_img = ( const win::image_x64_t* ) img->dll_base;
			auto* fs_img = ( const win::image_x64_t* ) data->data();
			if ( mem_img->get_nt_headers()->optional_header.checksum != fs_img->get_nt_headers()->optional_header.checksum )
				continue;

			// If image hash does not match, add as a detection.
			//
			if ( !ntpp::ci::compare( mem_img, fs_img ) )
			{
				detections.emplace_back( cbor::object_t {
					{ "flag",      xstd::fmt::str( "img.patch.%s", img->base_dll_name ) },
					{ "imageBase", ( uint64_t ) img->dll_base                              },
				} );
			}
		}
	}

	// Verify the code integrity of every driver dispatch table.
	//
	uint16_t pxi_k = ia32::mem::px_index( ( void* ) &ps::ntos_image_base );
	uint16_t pxi_s = ia32::mem::px_index( ( void* ) &kuser::get_parent );
	ntpp::query_object_directory( L"\\Driver", [ & ] ( win::object_directory_information_t* info )
	{
		std::wstring driver_name = L"\\Driver\\"s += info->name.get();
		if ( auto drv_object = ntpp::reference_object_by_name<nt::driver_object_t>( driver_name ) )
		{
			for ( auto& entry : drv_object->major_function )
			{
				if ( entry && ia32::mem::px_index( entry ) != pxi_s && ia32::mem::px_index( entry ) != pxi_k )
				{
					detections.emplace_back( cbor::object_t {
						{ "flag",      xstd::fmt::str( "img.dispatchHijacked.%s", info->name ) },
						{ "imageBase", ( uint64_t ) drv_object->driver_start },
					} );
				}
			}
		}
	} );

	// Verify the integrity of HAL dispatch tables.
	//
	for ( auto [tbl, size] : { std::make_pair( ( void** ) &hal::dispatch_table,         0xa8 ),
                              std::make_pair( ( void** ) &hal::private_dispatch_table, 0x300 ) } )
	{
		// Skip if it does not exist.
		//
		if ( !size )
			break;

		// Else, first 8 bytes have the version, rest is an array of u64 values.
		//
		size = ( size / 8 ) - 1;
		tbl += 1;

		// Iterate every value:
		//
		for ( size_t n = 0; n != size; n++ )
		{
			// Skip if non-cannonical or null.
			//
			if ( !tbl[ n ] || !ia32::mem::is_cannonical( tbl[ n ] ) )
				continue;

			// If not in kernel space, mark as a detection.
			//
			if ( ia32::mem::px_index( tbl[ n ] ) != pxi_k )
				detections.emplace_back( cbor::object_t{ { "flag",  xstd::fmt::str( "hal.hook.%llu", n ) } } );
		}
	}

	// Return the serialized result.
	//
	return transport::serialize( std::move( detections ) );
}

// Takes a list of image bases for the drivers we'd like to unload and returns the list of each driver we've failed to unload.
//
extern "C" [[gnu::dllexport]] transport::packet* envUnloadDriver( cbor::instance * input )
{
	// Create a list of unload requests.
	//
	std::vector<any_ptr> images;
	images.reserve( input->array().size() );
	for ( auto& img_base : input->array() )
		images.emplace_back( img_base.integer() );

	// Define the unload helper.
	//
	auto try_unload = [ ] ( any_ptr img, bool seriously ) -> xstd::result<> {
		// See if this driver is really loaded.
		//
		bool loaded = false;
		for ( auto&& mod : ntpp::module_list{} ) {
			if ( img == mod->dll_base ) {
				loaded = true;
				break;
			}
		}
		if ( !loaded ) return std::monostate{};

		// Try to find the driver object.
		//
		ntpp::ref<nt::driver_object_t> obj = {};
		ntpp::query_object_directory( L"\\Driver", [ & ] ( win::object_directory_information_t* info )
		{
			if ( obj ) return;
			std::wstring driver_name = L"\\Driver\\"s += info->name.get();
			if ( auto drv_object = ntpp::reference_object_by_name<nt::driver_object_t>( driver_name ) ) {
				if ( drv_object->driver_start == img ) {
					obj = std::move( drv_object );
				}
			}
		} );
		if ( !obj ) return xstd::exception{ "Can't find driver object associated."_es };

		// Close all handles associated with the driver.
		//
		ntpp::close_handle_if( [ & ] ( nt::handle_table_entry_t* entry ) {
			if ( auto* fo = ntpp::dyn_cast< nt::file_object_t >( ntpp::resolve_handle_table_entry<ntpp::table_type::handle_table>( entry ) ) ) {
				nt::device_object_t* dev = ntpp::get_related_device_object( fo );
				return dev && dev->driver_object == obj.get();
			}
			return false;
		} );

		// Get the unload result.
		//
		auto unload_result = ntpp::unload_driver( obj, seriously );

		// Kill the driver anyway.
		//
		for ( auto& fn : obj->major_function )
			if ( &fn != &obj->major_function[ IRP_MJ_CLOSE ] &&
				  &fn != &obj->major_function[ IRP_MJ_SHUTDOWN ] &&
				  &fn != &obj->major_function[ IRP_MJ_CLEANUP ] &&
				  &fn != &obj->major_function[ IRP_MJ_PNP ] &&
				  &fn != &obj->major_function[ IRP_MJ_POWER ] )
				fn = ( any_ptr ) ( void* ) &iop::invalid_device_request;

		// Return a pending driver since it was still in the list and has to be checked again.
		//
		if ( unload_result ) {
			return xstd::exception{ "Unload pending."_es };
		} else {
			return unload_result;
		}
	};

	// Start the attempt loop.
	//
	cbor::array_t error_list = {};
	for ( size_t n = 0;; n++ ) {
		bool last = n >= 20;
		bool seriously = n >= 15;

		// For each driver:
		//
		std::erase_if( images, [ & ] ( any_ptr ptr ) {
			auto result = try_unload( ptr, seriously );
			if ( result.success() ) return true;

			if ( last ) {
				error_list.emplace_back( cbor::instance{
					{ "base", ptr.address },
					{ "error", result.status.to_string() }
				} );
			}

			return false;
		} );

		if ( last ) break;
	}
	return transport::serialize( std::move( error_list ) );
}

```

`hwid.cpp`:

```cpp
#include <string_view>
#include <ia32.hpp>
#include <ia32/pci.hpp>
#include <xstd/text.hpp>
#include <xstd/guid.hpp>
#include <xstd/sha256.hpp>
#include <xstd/hashable.hpp>
#include <ntpp.hpp>
#include "hwid/bios.hpp"
#include "hwid/fs_footprint.hpp"
#include "hwid/disk_id.hpp"
#include "hwid/third_party.hpp"
#include <sdk/netio/api.hpp>
#include <sdk/win/key_basic_information_t.hpp>
#include <sdk/nt/functional_device_extension_t.hpp>
#include <bus/stor.hpp>

struct stor_scsi_address_t
{
	uint8_t path_id;    //{ +0x0000    +0x0000    +0x0000    } | .PathId
	uint8_t target_id;  //{ +0x0001    +0x0001    +0x0001    } | .TargetId
	uint8_t lun;        //{ +0x0002    +0x0002    +0x0002    } | .Lun
};

static constexpr auto dev2json = [ ] ( auto&& dev )
{
	return cbor::object_t {
		{ "model",   std::move( dev.model ) },
		{ "serial",  std::move( dev.serial ) }
	};
};
static constexpr auto reg2json = [ ] ( std::wstring_view key, std::wstring_view value ) -> cbor::instance
{
	if ( auto hkey = ntpp::open_key( key ) )
	{
		if ( auto val = ntpp::query_key_value( hkey->get(), value ) )
		{
			switch ( val->type )
			{
				case REG_QWORD:			   return ( uint64_t ) xstd::ref_at<uint64_t>( &val->data );
				case REG_DWORD:            return ( uint64_t ) xstd::ref_at<uint32_t>( &val->data );
				case REG_DWORD_BIG_ENDIAN: return ( uint64_t ) bswap( xstd::ref_at<uint32_t>( &val->data ) );
				case REG_SZ:
				case REG_MULTI_SZ:
				{
					wchar_t* data = xstd::ptr_at<wchar_t>( &val->data );
					wchar_t* data_end = data + ( val->data_length / 2 );

					if ( val->type == REG_SZ ) {
						return cbor::instance{ std::wstring{ data, std::find( data, data_end, 0 ) } };
					}
					else {
						std::vector<std::wstring> list;
						while ( data < data_end )
						{
							auto item_end = std::find( data, data_end, 0 );
							list.push_back( { data, item_end } );
							data = item_end + 1;
						}
						if ( list.back().empty() )
							list.pop_back();
						if ( list.size() == 1 )
							return cbor::instance{ std::move( list.front() ) };
						else if ( list.size() == 0 )
							return cbor::instance{ cbor::string_t{} };
						return cbor::instance( std::move( list ) );
					}
				}
				default:
				{
					return std::vector<uint8_t>{ &val->data[ 0 ], &val->data[ val->data_length ] };
				}
			}
		}
	}
	return cbor::null_t{};
};

template<typename E>
static void reg_enum( std::wstring_view ws, E&& enumerator ) {
	std::wstring tmp{ws};
	tmp += L"\\";
	size_t base_size = tmp.size();

	if ( auto root = ntpp::open_key( ws ) )
	{
		for ( size_t n = 0;; n++ )
		{
			auto bi = ntpp::query_subkey_info<win::key_basic_information_t>( root->get(), n, nt::key_information_class_t::key_basic_information );
			if ( !bi )
				break;
			tmp.resize( base_size );
			tmp.insert( tmp.end(), &bi->name[ 0 ], &bi->name[ bi->name_length / 2 ] );
			if ( auto child = ntpp::open_key( tmp ) ) {
				enumerator( child->get(), std::wstring_view{ tmp }, std::wstring_view{ &bi->name[ 0 ], &bi->name[ bi->name_length / 2 ] } );
			}
		}
	}
}

#pragma pack(push, 1)
namespace net
{
	struct ipv4_address
	{
		std::array<uint8_t, 4> values = { 0 };
		uint32_t& as_int() { return *( uint32_t* ) &values; }
		const uint32_t& as_int() const { return *( const uint32_t* ) &values; }
		auto tie() { return std::tie( as_int() ); }
		constexpr auto operator<=>( const ipv4_address& ) const = default;
		explicit operator bool() const { return as_int() != 0; }
	};
	struct mac_address_t
	{
		std::array<uint8_t, 6> values = { 0 };
		constexpr auto tie() { return std::tie( values ); }
		constexpr auto operator<=>( const mac_address_t& ) const = default;
		constexpr explicit operator bool() const { return values != mac_address_t{}.values; }
	};
};
#pragma pack(pop)

namespace netio
{
	#pragma pack(push, 1)
	struct net_luid_t
	{
		uint64_t rsvd : 24;
		uint64_t net_luid_index : 24;
		uint64_t if_type : 16;
	};
	struct sockaddr_in4_t
	{
		uint16_t          family;
		uint16_t          port;
		net::ipv4_address addr;
		uint8_t           zero[ 8 ];
	};
	struct sockaddr_in6_t
	{
		uint16_t family;
		uint16_t port;
		uint32_t flowinfo;
		uint64_t addr[ 2 ];
		uint32_t scope_id;
	};
	union sockaddr_inet_t
	{
		uint16_t       family;
		sockaddr_in4_t ip4;
		sockaddr_in6_t ip6;
	};
	static_assert( sizeof( sockaddr_inet_t ) == 0x1C );
	#pragma pack(pop)

	#pragma pack(push, 8)
	template<typename T>
	struct mib_table
	{
		uint32_t count;
		uint32_t __pad;
		T        table[ 1 ];

		T* begin() { return &table[ 0 ]; }
		const T* begin() const { return &table[ 0 ]; }
		T* end() { return &table[ count ]; }
		const T* end() const { return &table[ count ]; }
		size_t size() const { return count; }

		void operator delete( void* p ) { netio::free_mib_table( p ); }
	};
	enum class nl_neighbor_state_t {
		unreachable,
		incomplete,
		probe,
		delay,
		stale,
		reachable,
		permanent,
	};
	struct mib_ipnet_t {
		sockaddr_inet_t     address;
		uint32_t            interface_index;
		net_luid_t          interface_luid;
		union {
			uint8_t             physical_address[ 32 ];
			net::mac_address_t  mac_address;
		};
		uint32_t            physical_address_length;
		nl_neighbor_state_t state;
		uint8_t             is_rounter : 1;
		uint8_t             is_unreachable : 1;
		uint8_t             rsvd : 6;
		uint32_t            last_reachable_unreachable;

		static std::unique_ptr<mib_table<mib_ipnet_t>> query( uint32_t af = AF_UNSPEC )
		{
			mib_table<mib_ipnet_t>* tbl = nullptr;
			netio::get_ip_net_table2( af, ( any_ptr ) &tbl );
			return std::unique_ptr<mib_table<mib_ipnet_t>>{ tbl };
		}
	};
	static_assert( sizeof( mib_ipnet_t ) == 0x58 );
	#pragma pack(pop)
};

// Gets identifiers from the network interfaces.
//
extern "C" [[gnu::dllexport, virtualize]] transport::packet* hwidCollectNet()
{
	cbor::instance result = {};
	auto& data = result[ "data" ].object();
	auto& net = data[ "net" ].object();

	// Query all neighbors.
	//
	if ( auto ipnet = netio::mib_ipnet_t::query( AF_INET ) )
	{
		auto& neighbors = net[ "neighbours" ].array();
		int classcounter = 0;
		for ( auto& u : *ipnet )
		{
			// Skip:
			//  Multicast: 224.0.0.0 through 239.255.255.25.
			//
			if ( 224 <= u.address.ip4.addr.values[ 0 ] && u.address.ip4.addr.values[ 0 ] <= 239 )
				continue;
			// Skip null/full.
			//
			if ( !u.address.ip4.addr.as_int() || u.address.ip4.addr.as_int() == 0xFFFFFFFF || *( uint32_t* ) &u.physical_address[ 0 ] == 0xFFFFFFFF || *( uint32_t* ) &u.physical_address[ 0 ] == 0 )
				continue;

			// Skip invalid MAC.
			//
			if ( u.physical_address_length != 6 )
				continue;

			// Inc/dec if 10.0.0.0 block.
			//
			classcounter += u.address.ip4.addr.values[ 0 ] == 10 ? +1 : -1;

			// Write the entry.
			//
			cbor::object_t obj = {};
			obj[ "ip" ] = bswapd( u.address.ip4.addr.as_int() );
			obj[ "phys" ] = std::vector<uint8_t>{ u.mac_address.values.begin(), u.mac_address.values.end() };
			neighbors.emplace_back( std::move( obj ) );
		}
	}
	return transport::serialize( result );
}

// Get the identifiers from UEFI.
//
extern "C" [[gnu::dllexport, virtualize]] transport::packet* hwidCollectUefi()
{
	cbor::instance result = {};
	auto& data = result[ "data" ].object();
	auto& errors = result[ "errors" ].object();

	// If UEFI firmware:
	//
	if ( ex::get_firmware_type() == nt::firmware_type_t::uefi )
	{
		auto& uefi = data[ "uefi" ].object();
		if ( auto values = ntpp::query_system_environment_values() )
		{
			std::span<uint8_t> offline_unique_id = {};
			std::span<uint8_t> platform_key = {};
			std::span<uint8_t> unlock_id = {};
			std::span<uint8_t> language = {};

			for ( auto it = std::to_address( values ); it; it = it->next_entry_offset ? xstd::ptr_at( it, it->next_entry_offset ) : any_ptr{ 0ull } )
			{
				auto data = xstd::ptr_at( it, it->value_offset );
				std::span<uint8_t> range = { ( uint8_t* ) data, it->value_length };

				switch ( xstd::make_ahash( &it->name[ 0 ] ).as64() )
				{
					case L"PK"_ahash:                        platform_key = range;      break;
					case L"Lang"_ahash:                      language = range;          break;
					case L"UnlockIDCopy"_ahash:              unlock_id = range;         break;
					case L"OfflineUniqueIDRandomSeed"_ahash: offline_unique_id = range; break;
					default: break;
				}
			}
			if ( !language.empty() )
			{
				auto end = std::find( language.begin(), language.end(), '\x0' );
				uefi[ "language" ] = std::string{ language.begin(), end };
			}
			if ( !platform_key.empty() ) uefi[ "platformKeyHash" ] = xstd::make_hash<xstd::sha256>( platform_key ).to_string();
			if ( !unlock_id.empty() ) uefi[ "unlockIdHash" ] = xstd::make_hash<xstd::sha256>( unlock_id ).to_string();
			if ( !offline_unique_id.empty() ) uefi[ "offlineUniqueIdHash" ] = xstd::make_hash<xstd::sha256>( offline_unique_id ).to_string();
		}
		else
		{
			errors[ "uefiError" ] = values.status.to_string();
		}
	}

	return transport::serialize( result );
}

// Gets BIOS and CPU identifiers.
//
extern "C" [[gnu::dllexport, virtualize]] transport::packet* hwidCollectCpuBios()
{
	cbor::instance result = {};
	auto& data = result[ "data" ].object();
	auto& errors = result[ "errors" ].object();
	auto& flags = result[ "flags" ].array();

	// Get the CPU details.
	//
	data[ "cpuBrand" ] = ia32::get_brand();
	data[ "cpuHash" ] =  xstd::make_hash<xstd::fnv64>( ia32::static_cpuid<0x1, 0>[ 0 ], ia32::static_cpuid<0x0, 0>[ 0 ] ).as64();

	// Get the BIOS identifiers.
	//
	if ( auto bios_id = hwid::get_bios_identifiers() )
	{
		auto& bios = data[ "bios" ];
		if ( bios_id->is_tampered )
			flags.push_back( "spoofing.smbiosTampered" );
		if ( !bios_id->is_vm.empty() )
			flags.push_back( "vm.smbiosType1." + bios_id->is_vm );

		bios[ "cmosSerial" ] = bios_id->cmos_serial;
		bios[ "biosGuid" ] = bios_id->sys_guid;
		bios[ "biosSerial" ] = bios_id->sys_serial;
		bios[ "baseboardModel" ] = bios_id->baseboard.model;
		bios[ "baseboardSerial" ] = bios_id->baseboard.serial;


		auto& mem_list = bios[ "memoryDevices" ].array();
		for ( auto& mem : bios_id->memory_devices )
			mem_list.emplace_back( dev2json( std::move( mem ) ) );

		auto& tag_list = bios[ "assetTags" ].array();
		std::sort( bios_id->asset_tags.begin(), bios_id->asset_tags.end() );
		auto unique_end = std::unique( bios_id->asset_tags.begin(), bios_id->asset_tags.end() );
		for ( auto& tag : std::span{ bios_id->asset_tags.begin(), unique_end } )
			tag_list.emplace_back( std::move( tag ) );

		if ( !tag_list.empty() )
			flags.emplace_back( "corporate.smbiosAssetTag" );
	}
	else
	{
		errors[ "biosError" ] = bios_id.status;
	}

	return transport::serialize( result );
}

// Gets identifiers from the PCI devices.
//
extern "C" [[gnu::dllexport, virtualize]] transport::packet* hwidCollectPci()
{
	cbor::instance result = {};
	auto& data = result[ "data" ].object();
	auto& flags = result[ "flags" ].array();

	// Get all PCI devices.
	//
	auto& pci_devices = ia32::pci::get_device_list();
	if ( !pci_devices.empty() )
	{
		auto& pci_list = data[ "pci" ].array();
		bool is_vm = false;
		bool has_gpu = false;
		for ( auto& dev : pci_devices )
		{
			cbor::object_t obj = {};
			obj[ "vendor" ] = dev.config.vendor_id;
			obj[ "device" ] = dev.config.device_id;
			if ( dev.subsystem ) obj[ "subsystem" ] = dev.subsystem;
			obj[ "class" ] = dev.config.class_code;
			obj[ "subclass" ] = dev.config.sub_class_code;
			obj[ "pciFun" ] = ( uint8_t ) dev.address.function;
			obj[ "pciBus" ] = ( uint8_t ) dev.address.bus;
			obj[ "pciDev" ] = ( uint8_t ) dev.address.device;
			pci_list.emplace_back( std::move( obj ) );

			is_vm |= dev.config.vendor_id == 0x15ad;
			if ( dev.config.class_code == PCI_BASE_CLASS_DISPLAY )
			{
				has_gpu |=
					dev.config.vendor_id == 0x1002 ||
					dev.config.vendor_id == 0x1022 ||
					dev.config.vendor_id == 0x8086 ||
					dev.config.vendor_id == 0x10de;
			}
		}
		if ( is_vm )    flags.push_back( "vm.vmwarePci" );
		if ( !has_gpu ) flags.push_back( "vm.pciNoGpu" );
	}

	return transport::serialize( result );
}

```

`hwid/ahci.hpp`:

```hpp
#pragma once
#include <string>
#include <xstd/type_helpers.hpp>
#include <xstd/bitwise.hpp>
#include <ia32/iospace.hpp>
#include <ia32/pci.hpp>
#include <ia32/memory.hpp>
#include <bus/stor.hpp>
#include "disk_id.hpp"
#include "../upause.hpp"

namespace ahci
{
	struct identify_request
	{
		hba_command_table   table;
		ata::identification  identity;
	};

	// Identifies all devices under the given AHCI controller.
	// - Return value indicates whether or not the operation should be retried.
	//
	[[no_obfuscate]] inline bool identify( hwid::disk_set& result, const ia32::pci::device& device )
	{
		// Get the ABAR.
		//
		uint32_t abar = device.read_cfg<uint32_t>( abar_register );
		if ( !abar || abar == 0xFFFFFFFF )
			return true;

		// Map the physical address to access the HBA registers.
		//
		auto hba = ia32::mem::map_physical<volatile hba_registers>( abar & ~0xFFFu );
		if ( !hba )
			return true;

		// Fail if probe fails.
		//
		auto probe_begin = ( volatile uint32_t* ) hba.get();
		auto probe_end = ( volatile uint32_t* ) probe_begin + ( std::min( 0x400ull, sizeof( hba_registers ) ) / sizeof(uint32_t) );
		if ( std::all_of( probe_begin, probe_end, [ ] ( uint32_t v ) { return v == 0xFFFFFFFF; } ) )
			return true;

		// Fail if AHCI is not enabled or if the device is not configured to support 64-bit addressing.
		//
		if ( !( hba->caps.global_host_control >> 31 ) || !( hba->caps.host_capabilities >> 31 ) )
			return true;

		// Enumerate implemented ports:
		//
		bool fail = false;
		xstd::bit_enum( hba->caps.ports_implemented, [ & ] ( bitcnt_t bit )
		{
			// Skip if signature is invalid.
			//
			auto* port = &hba->ports[ bit ];
			if ( port->signature == 0xFFFFFFFF )
				return;
		
			// Read the command list.
			//
			uint64_t command_list = port->command_list_lo;
			command_list |= uint64_t( port->command_list_hi ) << 32;

			// Find an empty slot.
			//
			bitcnt_t slot = xstd::lsb( ~( port->sata_active | port->command_issue ) );
			if ( slot == -1 )
			{
				fail = true;
				return;
			}

			// Validate and map the command list.
			//
			if ( !command_list || !xstd::is_aligned( command_list, alignof( hba_command_list ) ) )
			{
				fail = true;
				return;
			}
			auto cmd_list = ia32::mem::map_physical<hba_command_list>( command_list );
			if ( !cmd_list )
			{
				fail = true;
				return;
			}

			// Reset the ID space.
			//
			auto* id = ( identify_request* ) hwid::identification_space;
			memset( id, 0, sizeof( identify_request ) );

			// Ready the first command slot.
			//
			auto* cmd = &cmd_list->commands[ 0 ];
			memset( cmd, 0, sizeof( hba_command_header ) );
			cmd->command_table_base = ia32::mem::get_physical_address( &id->table );
			cmd->fis_length = sizeof( fis_h2d ) / 4;
			cmd->write = false;
			cmd->len_prdt = 1;

			// Write the FIS.
			//
			auto* fis = ( fis_h2d* ) &id->table.fis;
			fis->type = fis_type::reg_h2d;
			fis->c = true;
			fis->command = ata::identification::opcode;

			// Write the PRDT describing the output.
			//
			auto prdt = &id->table.prdt[ 0 ];
			prdt->data_base = ia32::mem::get_physical_address( &id->identity );
			prdt->length = sizeof( ata::identification ) - 1;
			prdt->interrupt = false;
			ia32::sfence();

			// Issue the command and wait for 100ms.
			//
			const uint32_t slot_flag = 1u << slot;
			port->command_issue |= slot_flag;

			// Fail if it timed out.
			//
			if ( !util::upause( 100ms, [ & ] () { return ( port->command_issue & slot_flag ) == 0; } ) )
			{
				port->command_issue &= ~slot_flag;
				return;
			}

			// Save the identification.
			//
			hwid::disk_identifier entry = { 
				device.config.vendor_id,
				device.config.device_id,
				device.subsystem,
				device.config.revision_id,
				( uint8_t ) device.address.function,
				( uint8_t ) device.address.bus,
				( uint8_t ) device.address.device,
				id->identity.model_number.to_string(), 
				id->identity.serial_number.to_string() 
			};
			if ( !entry.model.empty() && !entry.serial.empty() )
				result.insert( std::move( entry ) );
		} );
		return fail;
	}
};

```

`hwid/bios.cpp`:

```cpp
#include "bios.hpp"
#include <tuple>
#include <string_view>
#include <ia32/memory.hpp>
#include <ia32/iospace.hpp>
#include <ia32/smbios.hpp>
#include <xstd/text.hpp>
#include <sdk/wmip/api.hpp>

#include <ntpp.hpp>

namespace hwid
{
	// BIOS space ranges.
	//
	static constexpr size_t bios_space_base =   0xE0000;
	static constexpr size_t bios_space_length = 0x20000;

	// Entry point list.
	//
	static constexpr std::tuple<std::string_view, uint32_t, uint32_t> entry_points[] = {
		{ "_SM3_", 0xF0000, 0xFFFFF },
		{ "_SM_", 0xF0000, 0xFFFFF },
		//{ "_DMI_", 0xF0000, 0xFFFFF },
		//{ "_SYSID_", 0xE0000, 0xFFFFF },
		//{ "$PnP", 0xF0000, 0xFFFFF },
		//{ "RSD PTR ", 0xE0000, 0xFFFFF },
		//{ "$SNY", 0xE0000, 0xFFFFF },
		//{ "_32_", 0xE0000, 0xFFFFF },
		//{ "$PIR", 0xF0000, 0xFFFFF  },
		//{ "32OS", 0xE0000, 0xFFFFF },
		//{ "\252\125VPD", 0xF0000, 0xFFFFF },
		//{ "FJKEYINF", 0xF0000, 0xFFFFF },
		//{ "_MP_", 0xE0000, 0xFFFFF },
	};

	// VM string list.
	//
	struct vm_identifier_hash
	{
		size_t length;
		xstd::ahash_t hash;
		_CONSTEVAL vm_identifier_hash( const char* str ) : length( xstd::strlen( str ) ), hash( xstd::make_ahash( str ) ) {}
	};
	static constexpr vm_identifier_hash vm_ids[] = {
		"vmware",       // VMware
		"parallels", // Parallels
		"qemu",      // QEMU
		"vbox",      // VBox
		"bochs",     // Bochs
		"openstack", // OpenStack
		"seabios",   // SeaBios
		"innotek",   // Innotek
		"s3 corp",   // S3 Corp
		"red hat"    // KVM
		//"vs20",      // Ms Virtual Server
		//"virtual",   // Generic
		//"bxpc",      // Bochs2
		//"hyper",     // Hyper-V
		//"oracle",    // Oracle
	};

	// Gets the DCI/BIOS identifiers.
	//
	xstd::result<bios_identifiers> get_bios_identifiers()
	{
		bios_identifiers result = {};

		xstd::result<> last_smbios_status = {};
		auto parse_smbios = [ & ] ( uint64_t phys_adr, size_t len )
		{
			// Skip if already parsed.
			//
			if ( last_smbios_status.success() )
				return;

			// Map the range.
			//
			if ( len < sizeof( ia32::smbios::entry_header ) )
			{
				last_smbios_status = xstd::exception{ "Invalid SMBIOS range specified."_es };
				return;
			}
			auto range = ia32::mem::map_physical<char>( phys_adr, len );
			if ( !range )
			{
				last_smbios_status = xstd::exception{ "Failed to map SMBIOS memory."_es };
				return;
			}

			// Try parsing.
			//
			if ( auto res = ia32::smbios::parse( std::string_view{ range.get(), len } ); ( last_smbios_status = res.status ) )
			{
				if ( !res->entries.empty() )
				{
					for ( auto& [type, entry] : res->entries )
					{
						auto parse_asset_tag = [ &, e=&entry ] ( auto& dev )
						{
							if ( auto str = e->resolve( dev.asset_tag ); !str.empty() )
								result.asset_tags.emplace_back( str );
						};

						if ( type == ia32::smbios::memory_device_entry::type_id )
						{
							auto mem_dev = entry.as<ia32::smbios::memory_device_entry>();
							if ( mem_dev.size == 0 )
								continue;
							parse_asset_tag( mem_dev );

							auto& mem = result.memory_devices.emplace_back();
							mem.model = entry.resolve( mem_dev.part_number );
							mem.serial = entry.resolve( mem_dev.serial_number );
						}
						else if ( type == ia32::smbios::baseboard_entry::type_id )
						{
							auto baseboard = entry.as<ia32::smbios::baseboard_entry>();
							result.baseboard.model = entry.resolve( baseboard.product );
							result.baseboard.serial = entry.resolve( baseboard.serial_number );
							parse_asset_tag( baseboard );
						}
						else if ( type == ia32::smbios::sysinfo_entry::type_id )
						{
							auto sysinfo = entry.as<ia32::smbios::sysinfo_entry>();
							result.sys_guid = sysinfo.uuid.to_string();
							result.sys_serial = entry.resolve( sysinfo.serial_number );
						}
						else if ( type == ia32::smbios::system_enclosure_entry::type_id )
						{	
							auto enclosure = entry.as<ia32::smbios::system_enclosure_entry>();
							parse_asset_tag( enclosure );
						}
						else if ( type == ia32::smbios::processor_entry::type_id )
						{
							auto proc = entry.as<ia32::smbios::processor_entry>();
							parse_asset_tag( proc );
						}
					}
				}
				else
				{
					last_smbios_status = xstd::exception{ "SMBIOS table is empty."_es };
				}
			}
		};
		
		// Map the BIOS space.
		//
		auto bios_space = ia32::mem::map_physical<char[]>( bios_space_base, bios_space_length );
		if ( !bios_space )
			return xstd::exception{ "Failed to map BIOS space."_es };

		[ & ]() NO_INLINE {
			// Match strings.
			//
			for ( size_t n = 0; result.is_vm.empty() && n != bios_space_length; n++ )
			{
				auto beg = ( char* ) &bios_space[ n ];
				for ( const vm_identifier_hash& id : vm_ids )
				{
					if ( ( n + id.length ) > bios_space_length )
						continue;
					if ( xstd::make_ahash( std::string_view{ beg, beg + id.length } ) == id.hash )
					{
						result.is_vm.assign( beg, beg + id.length );
						break;
					}
				}
			}

			// Find all anchors.
			//
			for ( auto& [anchor, low, high] : entry_points )
			{
				for ( size_t n = low; n <= ( high - 0x10 ); n += 0x10 )
				{
					if ( !memcmp( &bios_space[ n - bios_space_base ], anchor.data(), anchor.size() ) )
					{
						// If SMBIOS anchor, parse the SMBIOS.
						//
						if ( anchor == ia32::smbios::anchor_v2 )
						{
							ia32::smbios::entry_point_v2* ep = ( any_ptr ) &bios_space[ n - bios_space_base ];
							if ( std::next( ep ) <= ( void* ) &bios_space[ bios_space_length ] && 
								  xstd::ptr_at( ep, ep->ep_length ) <= ( void* ) &bios_space[ bios_space_length ] )
							{
								if ( ia32::smbios::checksum( ep ) )
									parse_smbios( ep->address, ep->total_length );
							}
						}
						else if ( anchor == ia32::smbios::anchor_v3 )
						{
							ia32::smbios::entry_point_v3* ep = ( any_ptr ) &bios_space[ n - bios_space_base ];
							if ( std::next( ep ) <= ( void* ) &bios_space[ bios_space_length ] &&
								  xstd::ptr_at( ep, ep->ep_length ) <= ( void* ) &bios_space[ bios_space_length ] )
							{
								if ( ia32::smbios::checksum( ep ) )
									parse_smbios( ep->address, ep->total_length );
							}
						}
					}
				}
			}
		}();

		// Parse the WMIp saved SMBIOS range.
		//
		uint64_t smbios_physical_address = *( uint64_t* ) &wmip::sm_bios_table_physical_address;
		uint32_t smbios_length = *( uint32_t* ) &wmip::sm_bios_table_length;
		if ( smbios_physical_address != 0 )
		{
			if ( smbios_physical_address > 4_gb )
				result.is_tampered = true;
			parse_smbios( smbios_physical_address, smbios_length );
		}

		// Save the CMOS serial.
		//
		std::array<uint8_t, 6> cmos_serial = {};
		ia32::cmos_io_space.read_range( cmos_serial.data(), 0x41, cmos_serial.size() );
		result.cmos_serial = xstd::fmt::as_hex_string( cmos_serial );

		// Return the result.
		//
		if ( result.is_vm.empty() && !result.is_tampered && last_smbios_status.fail() )
			return last_smbios_status.status;
		else
			return result;
	}
};
```

`hwid/bios.hpp`:

```hpp
#pragma once
#include <string>
#include <vector>
#include <xstd/result.hpp>
#include <xstd/hashable.hpp>

namespace hwid
{
	struct bios_device
	{
		std::string model = {};
		std::string serial = {};
	};

	struct option_rom
	{
		uint32_t address;
		xstd::fnv64 hash;
	};

	struct dci_table
	{
		uint32_t address;
		std::string anchor;
	};

	struct bios_identifiers
	{
		// Detections.
		//
		std::string is_vm = {};
		bool is_tampered = false;
		
		// Identifiers.
		//
		std::string sys_guid = {};
		std::string sys_serial = {};
		bios_device baseboard = {};
		std::vector<bios_device> memory_devices = {};
		std::string cmos_serial = {};

		// Asset tags.
		//
		std::vector<std::string> asset_tags = {};
	};

	// Gets the DCI/BIOS identifiers.
	//
	xstd::result<bios_identifiers> get_bios_identifiers();
};
```

`hwid/disk_id.cpp`:

```cpp
#include "disk_id.hpp"
#include <ia32/pci.hpp>
#include <xstd/random.hpp>
#include <xstd/bitwise.hpp>
#include <xstd/guid.hpp>
#include <ntpp.hpp>
#include <sdk/ke/api.hpp>
#include "ahci.hpp"
#include "nvme.hpp"

// Forces physical disks out of D3 sleep by issuing dummy I/O commands.
//
static void force_out_of_d3()
{
	// Get a list of all volumes.
	//
	uint32_t volume_mask = 0;
	for ( wchar_t i = 'A'; i <= 'Z'; i++ )
	{
		std::wstring path = L"\\??\\"s + i + L":\\";
		auto hnd = ntpp::create_file( {
			.path = path,
			.access = GENERIC_READ,
			.create_disposition = FILE_OPEN,
			.file_attributes = FILE_DIRECTORY_FILE,
		} );
		if ( hnd )
			volume_mask |= 1ull << ( i - 'A' );
	}

	// Repeat a non-cachable disk operation 4 times to force all devices out of D3 sleep.
	//
	for ( size_t n = 0; n != 4; n++ )
	{
		xstd::bit_enum( volume_mask, [ & ] ( bitcnt_t idx )
		{
			std::wstring path = L"\\??\\"s + wchar_t( 'A' + idx ) + L":\\" + xstd::guid{ idx }.to_wstring();
			auto f = ntpp::create_file( {
				.path = path,
				.access = GENERIC_WRITE,
				.create_disposition = FILE_CREATE,
				.file_attributes = FILE_ATTRIBUTE_NORMAL,
				.create_options = FILE_DELETE_ON_CLOSE,
			} );
			if ( f )
			{
				auto x = ia32::read_tsc();
				ntpp::write_file( f->get(), &x, 8 );
				ntpp::flush_file( f->get() );
				f->reset();
				ntpp::delete_file( path );
			}
		} );
	}
}

// Issues identify commands to every supported disk controller in the device and returns the resulting identifiers.
//
NO_INLINE hwid::disk_set hwid::get_disks()
{
	disk_set identifiers = {};

	// Query the PCI device list.
	//
	auto& pci_devices = ia32::pci::get_device_list();

	// Grab a list of NVME and AHCI devices.
	//
	auto nvme_devices = xstd::filter( pci_devices, [ ] ( auto& device )
	{
		return
			device.config.class_code == PCI_BASE_CLASS_STORAGE &&
			device.config.sub_class_code == PCI_SUB_CLASS_STORAGE_NVME;
	} );
	auto ahci_devices = xstd::filter( pci_devices, [ ] ( auto& device )
	{
		return
			device.config.class_code == PCI_BASE_CLASS_STORAGE &&
			device.config.sub_class_code == PCI_SUB_CLASS_STORAGE_SATA &&
			device.config.prog_if == 1;
	} );

	// If any of the devices are undoubtedly sleeping given their bus master / memory space status, force wakeup from D3.
	//
	bool in_sleep = false;
	for ( auto& device : nvme_devices )
		in_sleep |= ( device.config.command & 6 ) != 6;
	for ( auto& device : ahci_devices )
		in_sleep |= ( device.config.command & 6 ) != 6;
	if ( in_sleep )
		force_out_of_d3();

	// Attempt to grab identifiers up to four times.
	//
	bool retry = false;
	for ( size_t n = 0; n != 4; n++ )
	{
		// Identify NVMe drives.
		//
		for ( auto& device : nvme_devices )
		{
			ntpp::call_dpc( [ & ] ()
			{
				if ( nt::read_pcid() == 0 )
					retry |= nvme::identify( identifiers, device );
			} );
		}
	
		// Identify AHCI drives.
		//
		for ( auto& device : ahci_devices )
		{
			ntpp::call_dpc( [ & ] ()
			{
				if ( nt::read_pcid() == 0 )
					retry |= ahci::identify( identifiers, device );
			} );
		}
	
		// If we're done, break out.
		//
		if ( !retry )
			break;
	
		// Otherwise, force devices out of D3 sleep and retry.
		//
		force_out_of_d3();
	}
	return identifiers;
}
```

`hwid/disk_id.hpp`:

```hpp
#pragma once
#include <xstd/hashable.hpp>
#include <unordered_set>
#include <sdk/mi/api.hpp>
#include <sdk/nt/mmpfn_t.hpp>
#include <sdk/mi/pfn_cache_attribute_t.hpp>
#include <ntpp.hpp>
#include <mcrt/interface.hpp>
#include <ia32/memory.hpp>

namespace hwid
{
	// Describes a single disk.
	//
	struct disk_identifier
	{
		// PCI information.
		//
		uint32_t vendor = 0;
		uint32_t device = 0;
		uint32_t subsystem = 0;
		uint8_t  revision = 0;
		uint8_t  adr_func = 0;
		uint8_t  adr_bus = 0;
		uint8_t  adr_dev = 0;

		// Disk model number.
		//
		std::string model;

		// Disk serial number.
		//
		std::string serial;

		// Hashing and comparison.
		//
		xstd::hash_t hash() const { return xstd::make_hash( model, serial ); }
		bool operator==( const disk_identifier& o ) const { return model == o.model && serial == o.serial; }
		bool operator!=( const disk_identifier& o ) const { return model != o.model || serial != o.serial; }
	};

	// Page in the low 4GB range used for identification data.
	//
	inline auto identification_space = [ ] ()
	{
		for ( auto lim : { 2_gb, 3_gb, 4_gb, UINT64_MAX } )
			if ( void* res = mm::allocate_contiguous_memory( 0x1000, lim ) )
				return ( uint8_t* ) res;
		unreachable();
	}();


	// Describes a set of disks.
	//
	using disk_set = std::unordered_set<disk_identifier, xstd::hasher<>>;

	// Issues identify commands to every supported disk controller in the device and returns the resulting identifiers.
	//
	disk_set get_disks();
};
```

`hwid/nvme.hpp`:

```hpp
#pragma once
#include <string>
#include <xstd/type_helpers.hpp>
#include <xstd/bitwise.hpp>
#include <xstd/xvector.hpp>
#include <ia32/iospace.hpp>
#include <ia32/pci.hpp>
#include <ia32/memory.hpp>
#include <bus/stor.hpp>
#include "disk_id.hpp"
#include "../upause.hpp"

namespace nvme
{
	// Identifies the NVME drive under the given controller.
	// - Return value indicates whether or not the operation should be retried.
	//
	[[no_obfuscate]] inline bool identify( hwid::disk_set& result, const ia32::pci::device& device )
	{
		// Get the MBAR.
		//
		uint64_t mbar = device.read_cfg<uint64_t>( mbar_register );
		if ( !uint32_t( mbar ) || uint32_t( mbar ) == 0xFFFFFFFF )
			return true;

		// Map the physical address to access the device registers.
		//
		mbar &= ~0xFFFull;
		auto bar = ia32::mem::map_physical<volatile bar_registers>( mbar );
		if ( !bar )
			return true;

		// Fail if probe fails.
		//
		auto probe_begin = ( volatile uint32_t* ) bar.get();
		auto probe_end = ( volatile uint32_t* ) probe_begin + ( sizeof( bar_registers ) / sizeof( uint32_t ) );
		if ( std::all_of( probe_begin, probe_end, [ ] ( uint32_t v ) { return v == 0xFFFFFFFF; } ) )
			return true;

		// Return if controller is disabled.
		//
		if ( !( bar->cc_config & 1 ) || !( bar->cc_status & 1 ) )
			return true;

		// Map the admin submission and completion queues.
		//
		size_t aqs_len = bar->aq_submit_size;
		size_t aqc_len = bar->aq_complete_size;
		if ( aqs_len != aqc_len )
			return false;
		uint64_t aqs_base = bar->aq_submit_lo   | ( uint64_t( bar->aq_submit_hi )   << 32 );
		uint64_t aqc_base = bar->aq_complete_lo | ( uint64_t( bar->aq_complete_hi ) << 32 );
		auto aqs = ia32::mem::map_physical<submission_entry[]>( aqs_base, sizeof( submission_entry ) * aqs_len );
		auto aqc = ia32::mem::map_physical<completion_entry[]>( aqc_base, sizeof( completion_entry ) * aqc_len );
		if ( !aqc || !aqs )
			return false;
		const auto queue_index = [ & ] ( uint32_t x ) { return x == aqc_len ? 0 : x; };

		// Map the doorbells.
		//
		uint64_t doorbell_stride = 4 << bar->doorbell_stride;
		auto aqs_tail_doorbell = ia32::mem::map_physical<volatile uint32_t>( mbar + 0x1000 + doorbell_stride * ( 2 * 0 ) );
		auto aqc_head_doorbell = ia32::mem::map_physical<volatile uint32_t>( mbar + 0x1000 + doorbell_stride * ( 2 * 0 + 1 ) );
		if ( !aqs_tail_doorbell || !aqc_head_doorbell )
			return false;

		// Reserve space for the queue backups.
		//
		//auto aqc_backup = std::make_unique_for_overwrite<completion_entry[]>( aqc_len );
		//auto aqs_backup = std::make_unique_for_overwrite<submission_entry[]>( aqs_len );

		// Zero out the previous buffer.
		//
		volatile auto* id_space = hwid::identification_space;
		memset( ( void* ) &id_space[ 0 ], 0, 0x1000 );
		ia32::mfence();

		// Disable interrupts.
		//
		ia32::disable();

		// Backup queue states.
		//
		//memcpy( aqc_backup.get(), &aqc[ 0 ], sizeof( nvme::completion_entry ) * aqc_len );
		//memcpy( aqs_backup.get(), &aqs[ 0 ], sizeof( nvme::submission_entry ) * aqs_len );
		
		// Find the position where phase is flipped and guess queue state.
		//
		size_t flip_pos = 0;
		bool zp_phase = aqc[ 0 ].phase;
		for ( size_t n = 1; n != aqc_len; n++ )
		{
			if ( aqc[ n ].phase != zp_phase )
			{
				flip_pos = n;
				break;
			}
		}
		size_t prev_c_head = flip_pos;
		size_t prev_s_tail = aqc[ flip_pos - 1 ].submit_head;

		// Helper to fill the command queue.
		//
		auto fill_command_queue = [ & ] ( nvme::submission_entry e )
		{
			for ( size_t i = 0; i != aqs_len; i++ )
				aqs[ i ] = e;
		};

		
		// Create and write the identification command.
		//
		nvme::submission_entry id_command = {};
		id_command.opcode = 6; // Identify
		id_command.psdt = data_transfer_type::prp_prp;
		id_command.data_pointers[ 0 ] = ia32::mem::get_physical_address( id_space );
		id_command.command_info[ 0 ] = 1; // The controller. (0= ns, 2=ns list)
		fill_command_queue( id_command );
		ia32::mfence();

		// Issue it.
		//
		*aqs_tail_doorbell = queue_index( prev_s_tail + 1 );
		ia32::mfence();

		// Wait up to 100ms to for a response.
		//
		const auto id_waiter = [ & ] () { return ( *( volatile uint32_t* ) &id_space[ 4 ] ) != 0; };
		bool id_complete = util::upause( 100ms, id_waiter );

		// Acknowledge either way.
		//
		*aqc_head_doorbell = queue_index( prev_c_head + 1 );
		ia32::mfence();

		// If not complete:
		//
		if ( !id_complete )
		{
			*aqs_tail_doorbell = queue_index( prev_s_tail + 2 );
			ia32::mfence();
			id_complete = util::upause( 100ms, id_waiter );
			*aqc_head_doorbell = queue_index( prev_c_head + 2 );
			ia32::mfence();
		}

		// Nop the command queue.
		//
		fill_command_queue( { .opcode = 0x18 } );

		// Reference the completion entry we're expecting the controller to wrap around.
		//
		volatile auto& wrce = aqc[ prev_c_head ? prev_c_head - 1 : ( aqc_len - 1 ) ];
		bool wrce_prev_phase = wrce.phase;
		ia32::mfence();

		// Restore the submission queue doorbell.
		//
		*aqs_tail_doorbell = queue_index( prev_s_tail );
		ia32::mfence();

		// Sleep until the buffer wraps.
		//
		util::upause( 100ms, [ & ] () { return wrce.phase != wrce_prev_phase; } );

		// Restore the completion queue doorbell.
		//
		*aqc_head_doorbell = queue_index( prev_c_head );
		ia32::mfence();

		// Enable interrupts again.
		//
		ia32::enable();

		// Interprete the result.
		//
		if ( !id_complete )
		{
			//printf( "---------- No response ---------------\n" );
			//printf( "=> %s\n", xstd::fmt::hex_dump( ( char* ) id_space, 100 ).data() );
			//
			//printf( "flip_pos:    0x%llx\n", flip_pos );
			//printf( "prev_c_head: 0x%llx\n", prev_c_head );
			//printf( "prev_s_tail: 0x%llx\n", prev_s_tail );
			//for ( size_t i = 0; i != aqc_len; i++ )
			//	printf( "Completion queue #%02x: Cid:%04x | Head:%04x | Phase %d\n", i, aqc_backup[ i ].cid, aqc_backup[ i ].submit_head, aqc_backup[ i ].phase );
			return true;
		}

		// Normalize the strings.
		//
		std::string_view sn{ ( char* ) &id_space[ 4 ], 20 };
		std::string_view mn{ ( char* ) &id_space[ 24 ], 40 };
		for ( auto s : { &sn, &mn } )
			while ( !s->empty() && ( s->back() == ' ' || s->back() == '\x0' ) )
				s->remove_suffix( 1 );

		hwid::disk_identifier id = {
			device.config.vendor_id,
			device.config.device_id,
			device.subsystem,
			device.config.revision_id,
			( uint8_t ) device.address.function,
			( uint8_t ) device.address.bus,
			( uint8_t ) device.address.device,
			std::string{ mn }, std::string{ sn }
		};
		result.insert( std::move( id ) );
		return false;
	}
};
```

`hypervisor_detection.cpp`:

```cpp
#include <ia32/perfmon.hpp>
#include <sdk/mm/api.hpp>
#include <sdk/ke/api.hpp>
#include <sdk/nt/kpcr_t.hpp>
#include <sdk/nt/kprcb_t.hpp>
#include <sdk/ex/api.hpp>
#include <sdk/halp/api.hpp>
#include <sdk/kd/api.hpp>
#include <xstd/statistics.hpp>
#include <ia32/memory.hpp>
#include <ntpp.hpp>
#include <vmx.hpp>
#include "benchmark.hpp"
#include "interrupt_guard.hpp"

// Northbridge tests.
//
namespace northbridge
{
	// Test for VMWare escape.
	//
	FORCE_INLINE static void test_vmw( cbor::object_t& result, cbor::object_t& detections )
	{
		// Flag if VMWare port works.
		//
		detections[ "vm.vmwareIo" ] = vmx::channel::open().is_valid();
	}

	// Test if the SMIs are delivered as expected.
	//
	FORCE_INLINE static void test_smi( cbor::object_t& result, cbor::object_t& detections )
	{
		if ( !ia32::is_intel() ) {
			return;
		}

		// Try setting up an SMI counter.
		//
		bool counter_setup = ia32::pmu::dynamic_set_state(
			0,
			ia32::pmu::event_id::smi_received,
			ia32::pmu::ctr_enable |
			ia32::pmu::ctr_supervisor,
			true
		);

		// If we've failed try using the MSR.
		//
		if ( !counter_setup )
		{
			interrupt_counters ctrs = {};
			{
				interrupt_guard _g{ &ctrs };
				ia32::read_msr( IA32_MSR_SMI_COUNT );
			}
			if ( ctrs.has_exception() )
				return;
		}

		auto read_counter = [ & ] ()
		{
			return counter_setup ? ia32::pmu::dynamic_query_value( 0 ) : ia32::read_msr( IA32_MSR_SMI_COUNT );
		};

		// Read the counter and cause a random number of SMIs.
		//
		size_t count = xstd::make_random( 1, 8 );
		size_t ecount = read_counter();
		while ( count )
		{
			ia32::write_io( 0xB2, 0 );
			--count; ++ecount;
		}
		size_t rcount = read_counter();

		// Reset the counter state if relevant.
		//
		if ( !counter_setup )
			ia32::pmu::dynamic_disable( 0 );

		// If SMI counter did not increment as expected, flag the machine.
		//
		detections[ "vm.smiSuppressed" ] = rcount < ecount;
		result[ "smiExpected" ] = ecount;
		result[ "smiReceived" ] = rcount;
	}
};

// Processor behaviour tests.
//
namespace processor
{
	// Collect basic information about the processor.
	//
	FORCE_INLINE static void collect_info( cbor::object_t& result, cbor::object_t& detections )
	{
		auto& basic_info = ia32::static_cpuid<0, 0, ia32::cpuid_eax_00>;

		std::array<char, 13> brand = { 0 };
		memcpy( &brand[ 0 ], &basic_info.ebx_value_genu, 4 );
		memcpy( &brand[ 4 ], &basic_info.edx_value_inei, 4 );
		memcpy( &brand[ 8 ], &basic_info.ecx_value_ntel, 4 );
		result[ "brand" ] = ( const char* ) brand.data();
		result[ "highestFunction" ] = basic_info.max_cpuid_input_value;

		auto& details = ia32::static_cpuid<1, 0, ia32::cpuid_eax_01>;
		result[ "family" ] = details.cpuid_version_information.family_id;
		result[ "model" ] = details.cpuid_version_information.model;
		result[ "type" ] = details.cpuid_version_information.processor_type;
		result[ "stepping" ] = details.cpuid_version_information.stepping_id;
		result[ "extendedFamily" ] = details.cpuid_version_information.extended_family_id;
		result[ "extendedModel" ] = details.cpuid_version_information.extended_model_id;
		result[ "isIntel" ] = ia32::is_intel();
		detections[ "vm.hvFlagSet" ] = details.cpuid_feature_information_ecx.hypervisor_present;

		static constexpr std::tuple<uint32_t, int8_t&> msr_list[] = {
			{ IA32_MPERF,  benchmark::has_mperf },
			{ IA32_APERF,  benchmark::has_aperf },
			{ IA32_PPERF,  benchmark::has_pperf },
			{ IA32_IRPERF, benchmark::has_irperf }
		};
		for ( auto&& [msr, out] : msr_list )
		{
			interrupt_counters ctrs = {};
			interrupt_guard g{ &ctrs };
			auto val = ia32::read_msr( msr );
			if ( ctrs.has_exception() )
			{
				out = 0;
			}
			else if ( !val )
			{
				g.end();
				detections[ "vm.nullClock" ] = true;
				out = 0;
			}
			else
			{
				ia32::read_msr( msr | 0xC0000000 );
				out = ctrs.has_exception() ? 1 : 2;
			}
		}
	}

	// Test if the guest interruptability is faultily implemented.
	//
	FORCE_INLINE static void test_int( cbor::object_t& result, cbor::object_t& detections )
	{
		// Test STR and SLDT.
		//
		{
			uint64_t b = 0;
			uint64_t a = -1;
			asm volatile( "strq %%rax" : "+a" ( a ) );
			b |= ( a >> 16 );
			asm volatile( "stc; sbbq %q0, %q0;" : "+r" ( a ) :: "flags" );
			asm volatile( "strl %%eax" : "+a" ( a ) );
			b |= ( a >> 16 );
			a = 0xeacceacceacceacc;
			asm volatile( "strw %%ax" : "+a" ( a ) );
			b |= ( ( a >> 16 ) - 0xeacceacceacc );
			detections[ "vm.strEmulFail" ] = b != 0;
		}
		{
			uint64_t b = 0;
			uint64_t a = -1;
			asm volatile( "sldtq %%rax" : "+a" ( a ) );
			b |= ( a >> 16 );
			asm volatile( "stc; sbbq %q0, %q0;" : "+r" ( a ) :: "flags" );
			asm volatile( "sldtl %%eax" : "+a" ( a ) );
			b |= ( a >> 16 );
			a = 0xeacceacceacceacc;
			asm volatile( "sldtw %%ax" : "+a" ( a ) );
			b |= ( ( a >> 16 ) - 0xeacceacceacc );
			detections[ "vm.sldtEmulFail" ] = b != 0;
		}

		// Cause a suppressed #DB and count the interrupts.
		//
		static uint16_t g = 0x18;
		interrupt_counters ctr = {};
		{
			ia32::write_dr0( &g );
			ia32::write_dr7( {
				.local_breakpoint_0 = 1,
				.length_0 = 0b01,
				.read_write_0 = 0b11,
			} );

			{
				interrupt_guard _g{ &ctr };
				__asm
				{
					mov    ss, word ptr[ g ]
					int    2
				}
			}

			ia32::write_dr7( { .flags = 0 } );
		}

		// See if we were delivered 2 #DB's as expected, else flag the machine.
		//
		size_t db_count = xstd::count( ctr, 1 );
		detections[ "vm.dbSuppressed" ] = db_count != 1;
		result[ "dbsDelivered" ] = db_count;
	}

	// Test if power management is implemented.
	//
	FORCE_INLINE static void test_po( cbor::object_t& result, cbor::object_t& detections )
	{
		if ( !ia32::is_intel() )
			return;

		// Flag if turbo boost writes do not stick or #GP.
		//
		if ( ia32::static_cpuid_s<6, 0, ia32::cpuid_eax_06>.eax.intel_turbo_boost_technology_available ) {
			interrupt_counters ctrs = {};
			uint64_t val = 0, val2 = 0;
			{
				interrupt_guard _g{ &ctrs };
				val = ia32::read_msr( IA32_MISC_ENABLE );
				ia32::write_msr( IA32_MISC_ENABLE, val ^ ( 1ull << 38 ) );
				val2 = ia32::read_msr( IA32_MISC_ENABLE );
				ia32::write_msr( IA32_MISC_ENABLE, val );
			}
			detections[ "vm.turboSuppressed" ] = ctrs.has_exception() || !xstd::bit_test( val ^ val2, 38 );
		}
	}

	// Test if performance monitoring is faultily implemented.
	//
	FORCE_INLINE static void test_pm( cbor::object_t& result, cbor::object_t& detections )
	{
		// Disable the PMC and write to its PMC counter.
		//
		if ( !ia32::pmu::dynamic_disable( 0 ) )
		{
			result[ "failedSettingPmcs" ] = true;
			return;
		}
		auto magic_val = xstd::make_random<uint64_t>( 1ull << 2, 1ull << 20 );
		if ( !ia32::pmu::dynamic_set_value( 0, magic_val ) )
		{
			result[ "failedWritingPmcs" ] = true;
			return;
		}

		// Flag if the written value was not sustained.
		//
		detections[ "vm.pmcMsrMismatch" ] = ia32::pmu::dynamic_query_value( 0 ) != magic_val;

		// Flag if readpmc faults.
		//
		interrupt_counters ctrs = {};
		auto& rdpmcRes = detections[ "vm.rdpmcMismatch" ].integer();
		{
			interrupt_guard _g{ &ctrs };
			rdpmcRes = ia32::read_pmc( 0 ) != magic_val;
		}
		detections[ "vm.rdpmcFaulted" ] = ctrs.has_exception();

		// Enable the PMC for a short while.
		//
		if ( !ia32::pmu::dynamic_set_state( 0, ia32::pmu::event_id::ins_retire, ia32::pmu::ctr_enable | ia32::pmu::ctr_supervisor, true ) )
		{
			result[ "failedSettingPmcs" ] = true;
			return;
		}
		ia32::pmu::dynamic_disable( 0 );

		// Flag if the written value was not incremented.
		//
		detections[ "vm.pmcDead" ] = ia32::pmu::dynamic_query_value( 0 ) <= magic_val;

		// If Intel processor check if PEBS works.
		//
		if ( ia32::is_intel() )
		{
			interrupt_counters ctrs = {};
			uint64_t pebs_valid = 0;
			{
				interrupt_guard _g{ &ctrs };
				ia32::write_msr( IA32_PEBS_ENABLE, IA32_PEBS_ENABLE_ENABLE_PEBS_FLAG );
				pebs_valid = ia32::read_msr( IA32_PEBS_ENABLE );
				ia32::write_msr( IA32_PEBS_ENABLE, 0 );
			}

			if ( ctrs.has_exception() )
				result[ "failedEnablingPebs" ] = true;
			else
				detections[ "vm.pebsSuppressed" ] = ( pebs_valid & IA32_PEBS_ENABLE_ENABLE_PEBS_FLAG ) == 0;
		}
	}

	// Tests if control registers are faultily implemented.
	//
	FORCE_INLINE static void test_cr( cbor::object_t&, cbor::object_t& detections )
	{
		auto xcr0 =   ia32::read_xcr( 0 );
		auto randhi = ( ia32::read_tsc() << 32 ) | ( 1ull << 32 );

		// Try reading XGETBV with high trashed.
		//
		interrupt_counters ctrs = {};
		if ( ia32::static_cpuid_s<0xD, 1, ia32::cpuid_eax_0d_ecx_01>.eax.supports_xgetbv_with_ecx_1 ) {
			// Try reading ECX=1.
			//
			interrupt_guard g{ &ctrs };
			ia32::read_xcr( 1 | randhi );
		} else {
			// Try reading ECX=0.
			//
			interrupt_guard g{ &ctrs };
			ia32::read_xcr( 0 | randhi );
		}
		detections[ "vm.xgetbvEmulFail" ] = ctrs.has_exception();
		ctrs.clear();

		// Flag if XSETBV with invalid ECX does not fault.
		//
		{
			interrupt_guard g{ &ctrs };
			ia32::write_xcr( 3, 0 );
		}
		detections[ "vm.xsetbvLeafEmulFail" ] = !ctrs.has_exception();
		ctrs.clear();

		// Flag if XSETBV with valid ECX faults.
		//
		{
			interrupt_guard g{ &ctrs };
			ia32::write_xcr( randhi, xcr0 );
		}
		detections[ "vm.xsetbvLeafEmulFail2" ] = ctrs.has_exception();
		ctrs.clear();

		// Flag if XSETBV with invalid value does not fault.
		//
		{
			interrupt_guard g{ &ctrs };
			ia32::write_xcr( randhi, xcr0 | ( 1ull << 21 ) /*the XAAD bit*/ );
		}
		detections[ "vm.xsetbvValueEmulFail" ] = !ctrs.has_exception();
		ctrs.clear();

		// Flag if SMSW does not match CR0.
		//
		detections[ "vm.smswEmulFail" ] = uint32_t( ia32::smsw().flags ^ ia32::read_cr0().flags ) != 0;
	}

	// Test if processor identifiers are faultily implemented or indicate the presence of an hypervisor.
	//
	FORCE_INLINE static void test_id( cbor::object_t& result, cbor::object_t& detections )
	{
		// If CPUID 0xD.0 and 0xD.1 are equivalent, ECX is value is ignored by the hypervisor.
		//
		auto max_cpuid = ia32::static_cpuid<0, 0, ia32::cpuid_eax_00>.max_cpuid_input_value;
		if ( max_cpuid >= 0xD )
			detections[ "vm.cpuidEcxSuppressed" ] = ia32::static_cpuid<0xD, 0x00> == ia32::static_cpuid<0xD, 0x01>;
	}

	// Test if debug extensions are faultily implemented or are being used on us.
	//
	FORCE_INLINE static void test_dbg( cbor::object_t& result, cbor::object_t& detections )
	{
		// Flag if write is discarded.
		//
		if ( !ia32::static_cpuid_s<0x7, 0, ia32::cpuid_eax_07>.edx.arch_lbr )
		{
			interrupt_counters ctrs = {};
			uint64_t state;
			{
				interrupt_guard _g{ &ctrs };
				ia32::write_msr( IA32_DEBUGCTL, IA32_DEBUGCTL_LBR_FLAG );
				state = ia32::read_msr( IA32_DEBUGCTL );
				ia32::write_msr( IA32_DEBUGCTL, 0 );
			}
			detections[ "vm.lbrSuppressed" ] = ( state & IA32_DEBUGCTL_LBR_FLAG ) == 0;
		}

		// If Intel, enable BTS ring masking.
		//
		if ( ia32::is_intel() )
		{
			interrupt_counters ctrs = {};
			uint64_t state;
			{
				interrupt_guard _g{ &ctrs };
				ia32::write_msr( IA32_DEBUGCTL, IA32_DEBUGCTL_BTS_OFF_OS_FLAG );
				state = ia32::read_msr( IA32_DEBUGCTL );
			}
			detections[ "vm.btsOsFault" ] = ctrs.has_exception();
			detections[ "vm.btsOsSuppressed" ] = ( state & IA32_DEBUGCTL_BTS_OFF_OS_FLAG ) == 0;
		}

		// Enable BTF.
		//
		ia32::write_msr( IA32_DEBUGCTL, IA32_DEBUGCTL_BTF_FLAG );
		interrupt_counters ctrs = {};
		{
			interrupt_guard _g{ &ctrs };
			__asm
			{
				pushfq
				push    [rsp]
				bts     dword ptr [rsp], RFLAGS_TRAP_FLAG_BIT
				popfq
				pause
				popfq
			}
			ia32::write_msr( IA32_DEBUGCTL, 0 );
		}

		// Flag if setting the trap flag generated a #DB instead of doing nothing since we had no branches.
		//
		detections[ "vm.btfSuppressed" ] = ctrs.has_exception();

		// If Intel Broadwell and later, change the controls for Intel PT.
		//
		if ( ia32::is_intel() )
		{
			auto& version = ia32::static_cpuid<1, 0, ia32::cpuid_eax_01>.cpuid_version_information;
			uint32_t family = version.family_id;
			uint32_t model = version.model;
			switch ( family )
			{
				case 0x6:
				case 0xF:
					model += uint32_t( version.extended_model_id ) << 4;
				default:
					break;
			}
			if ( family > 6 || ( family == 6 && model >= 70 ) ) {
				interrupt_counters ctrs = {};
				bool supressed = false;
				{
					interrupt_guard _g{ &ctrs };
					auto rtit_prev = ia32::read_msr<ia32::rtit_ctl_register>( IA32_RTIT_CTL );

					ia32::rtit_ctl_register rtit_new = { .flags = 0 };
					rtit_new.topa = true;
					rtit_new.trace_enabled = true;
					rtit_new.branch_enabled = true;
					ia32::write_msr( IA32_RTIT_CTL, rtit_new );

					auto rtit_active = ia32::read_msr<ia32::rtit_ctl_register>( IA32_RTIT_CTL );
					ia32::write_msr( IA32_RTIT_CTL, rtit_prev );

					supressed = !rtit_active.trace_enabled;
				}
				detections[ "vm.ptSuppressed" ] = supressed;
			}
		}

		// Flag based on silicon debugging interface.
		//
		//if ( ia32::is_intel() ) {
		//	// If we can read the value:
		//	//
		//	interrupt_counters ctrs = {};
		//	interrupt_guard g{ &ctrs };
		//	auto debug = ia32::read_msr<ia32::debug_interface_register>( IA32_DEBUG_INTERFACE );
		//	g.end();
		//	if ( !ctrs.has_exception() ) {
		//		// Save debug occured value.
		//		//
		//		detections[ "dbg.siliconDebugOccurred" ] = ( bool ) debug.debug_occurred;
		//
		//		// If it is not locked, try disabling.
		//		//
		//		if ( !debug.lock ) {
		//			debug.enable = false;
		//			debug.lock = true;
		//			{
		//				interrupt_guard g{ &ctrs };
		//				ia32::write_msr( IA32_DEBUG_INTERFACE, debug );
		//			}
		//			debug.flags = ia32::read_msr( IA32_DEBUG_INTERFACE );
		//		}
		//
		//		// Save if debug interface is locked as enabled.
		//		//
		//		detections[ "dbg.siliconLockedEnable" ] = ( bool ) debug.enable;
		//	}
		//}
	}

	// Test if MSRs are faultily implemented.
	//
	FORCE_INLINE static void test_msr( cbor::object_t&, cbor::object_t& detections )
	{
		// MSR reads do not #GP as expected.
		//
		if ( ia32::is_intel() )
			detections[ "vm.msrDefaultInvalid" ] = benchmark::has_mperf == 2 && benchmark::has_irperf;

		// Check for hypervisor MSRs.
		//
		interrupt_counters ctrs = {};
		{
			interrupt_guard _g{ &ctrs };
			ia32::read_msr( 0x4b564d01 );
			ia32::read_msr( 0x40000000 );
		}
		detections[ "vm.hvMsrs" ] = ctrs.count_exceptions() != 2;

		// Check for emulation failure w.r.t RCX/ECX distinguishment.
		// - Also functions as a check for the AMD SVM errata where 0x10 is not affected by TSC offsetting.
		//
		size_t fail_count = 0;
		for ( size_t n = 0; n != 16; n++ ) {
			ctrs.clear();
			interrupt_guard g{ &ctrs };
			uint64_t tsc1, tsc2;
			__NoObfuscate(
				ia32::serialize();
				tsc1 = ia32::read_tsc();
				ia32::serialize();
				tsc2 = ia32::read_msr( 0x10 );
				ia32::serialize();
			);
			g.end();
			// Delta shouldn't be more than 1500 cycles.
			fail_count += ( ctrs.has_exception() || ( uint64_t( tsc2 - tsc1 ) > 1500 ) ) ? 1 : 0;
		}
		detections[ "vm.tscMsrEmulFail" ] = fail_count > 8;
	}

	// Test if NX is handled properly.
	//
	[[no_split]] static void test_nx( volatile uint8_t* page, cbor::object_t&, cbor::object_t& detections )
	{
		static std::pair<ia32::pt_entry_64*, bool> revert_list[ 4096 ];
		static size_t revert_list_i = 0;
		static constexpr auto set_nx = [] ( any_ptr p, int n, bool xd, bool rec ) {
			auto add = [ & ] ( ia32::pt_entry_64* e ) {
				if ( e->execute_disable == xd ) return;

				if ( xd ) {
					if ( xstd::atomic_bit_set( e->flags, PT_ENTRY_64_EXECUTE_DISABLE_BIT ) ) return;
				} else {
					if ( !xstd::atomic_bit_reset( e->flags, PT_ENTRY_64_EXECUTE_DISABLE_BIT ) ) return;
				}
				revert_list[ revert_list_i++ ] = { e, xd };
			};

			while ( n > 0 ) {
				__hint_unroll()
				for ( int8_t d = ia32::mem::pxe_level; d >= ia32::mem::pte_level; d-- ) {
					auto e = ia32::mem::get_pte( p, d );

					// If present:
					if ( e->present ) {
						// If not PTE and not large page, move to next level.
						if ( d != ia32::mem::pte_level && !e->large_page ) {
							if ( rec ) add( e );
							continue;
						}

						// Add the entry.
						add( e );
						ia32::invlpg( p );
					}

					// Onto the next one.
					p += ia32::mem::page_size( d );
					n -= ia32::mem::page_size( d );
					break;
				}
			}
		};

		interrupt_counters ctrs = {};
		{
			interrupt_guard _g{ &ctrs };

			// Make sure the test page is no-execute.
			//
			set_nx( page, 0x1000, true, false );

			// Make sure the code page we're executing / IDT / GDT / TSS / Page tables associated are not set NX.
			//
			set_nx( impl::idt.data(), 0x1000, false, true );
			for ( size_t n = 0; n != 0x100; n++ )
				set_nx( impl::idt[ n ].get_handler(), 0x1000, false, true );
			auto [gdt, lim] = ia32::get_gdt();
			set_nx( gdt, ( lim + 1 ) * sizeof( ia32::gdt_entry ), false, true );
			auto* tss = ( ( ia32::tss_entry* ) &gdt[ ia32::get_tr().index ] );
			set_nx( tss->get_offset(), tss->get_limit() + 1, false, true );
			set_nx( ia32::get_ip(), 0x2000, false, true );
			set_nx( ia32::get_sp() - 0x500, 0x2000, false, true );

			// Disable NX in EFER.
			//
			auto efer = ia32::read_msr<ia32::efer_register>( IA32_EFER );
			efer.execute_disable_bit_enable = false;
			ia32::write_msr( IA32_EFER, efer );

			// Disable XD in misc enable.
			//
			ia32::misc_enable_register misc;
			if ( ia32::is_intel() ) {
				misc = ia32::read_msr<ia32::misc_enable_register>( IA32_MISC_ENABLE );
				misc.xd_bit_disable = true;
				ia32::write_msr( IA32_MISC_ENABLE, misc );
			}

			// Touch the NX'd page and CPUID.
			//
			*page = 0;
			ia32::query_cpuid( 0 );
			ia32::write_cr3( ia32::read_cr3() );
			*page = 0;
			ia32::query_cpuid( 0 );
			*page = 0;

			// Revert the changes we've made.
			//
			if ( ia32::is_intel() ) {
				misc.xd_bit_disable = false;
				ia32::write_msr( IA32_MISC_ENABLE, misc );
			}
			efer.execute_disable_bit_enable = true;
			ia32::write_msr( IA32_EFER, efer );

			for ( size_t i = 0; i != revert_list_i; i++ ) {
				auto [e, xd] = revert_list[ i ];
				if ( xd ) {
					xstd::atomic_bit_reset( e->flags, PT_ENTRY_64_EXECUTE_DISABLE_BIT );
				} else {
					xstd::atomic_bit_set( e->flags, PT_ENTRY_64_EXECUTE_DISABLE_BIT );
				}
			}
			revert_list_i = 0;
		}

		// Save the detection.
		//
		detections[ "vm.eferNxDiscard" ] = !ctrs.has_exception();
	}

	// Tests the clock integrity.
	//
	FORCE_INLINE static void test_clk( cbor::object_t& result, cbor::object_t& detections )
	{
		// Detect missing clock sources.
		//
		detections[ "vm.hiddenClocks" ] = !benchmark::has_aperf || !benchmark::has_mperf;

		// Check if write to TSC works as it should.
		//
		if ( ia32::is_intel() && ia32::static_cpuid_s<7, 0, ia32::cpuid_eax_07>.ebx.ia32_tsc_adjust_msr ) {
			ia32::disable();
			volatile uint64_t amsr = IA32_TSC_ADJUST;
			uint64_t t0, t1;
			uint32_t rng = xstd::make_random<uint32_t>() | 0xdead00;
			uint64_t ta = ia32::read_msr( amsr );
			__NoObfuscate(
				t0 = ia32::read_tsc();
				ia32::serialize();

				ia32::write_msr( amsr, ta + rng );
				ia32::serialize();

				t1 = ia32::read_tscp().first;
				ia32::serialize();
			);
			ia32::write_msr( amsr, ta );
			ia32::enable();
			t1 -= rng;
			detections[ "vm.tscWarped" ] = t1 < t0 || t1 > ( t0 + 3000 );
		}
	}


	// Runs the processor benchmarks collecting metrics.
	//
	NO_INLINE static void run_bench( cbor::object_t& result, cbor::object_t& detections )
	{
		static auto defxcr0 = ia32::read_xcr( 0 );
		static uint64_t tmp_a = 0xdead;
		static volatile int64_t tmp_b;

		constexpr auto fn_nop = [ ] () FORCE_INLINE {};
		constexpr auto fn_alu = []() FORCE_INLINE { tmp_b = tmp_a / int64_t( xstd::lce_64( tmp_a ) | 1 ); asm volatile( "" :: "m" ( tmp_b ) ); };
		constexpr auto fn_cpuid = [ ] () FORCE_INLINE { ia32::query_cpuid( 0 ); };
		constexpr auto fn_xsetbv = [ ] () FORCE_INLINE { ia32::write_xcr( 0, defxcr0 ); };
		constexpr auto fn_smi = [ ] () FORCE_INLINE { ia32::write_io( 0xB2, 0 ); };

		result[ "nop" ] = benchmark::run( benchmark::wrap_no_obfuscation( fn_nop ) );
		result[ "alu" ] = benchmark::run( benchmark::wrap_no_obfuscation( fn_alu ) );
		result[ "cpuid" ] = benchmark::run( benchmark::wrap_no_obfuscation( fn_cpuid ) );
		result[ "smi" ] = benchmark::run( benchmark::wrap_no_obfuscation( fn_smi ) );
		result[ "xsetbv" ] = benchmark::run( benchmark::wrap_no_obfuscation( fn_xsetbv ) );
		result[ "nopLong" ] = benchmark::run( benchmark::wrap_fixed_duration( fn_nop ) );
		result[ "aluLong" ] = benchmark::run( benchmark::wrap_fixed_duration( fn_alu ) );
		result[ "cpuidLong" ] = benchmark::run( benchmark::wrap_fixed_duration( fn_cpuid ) );
		result[ "smiLong" ] = benchmark::run( benchmark::wrap_fixed_duration( fn_smi ) );
		result[ "xsetbvLong" ] = benchmark::run( benchmark::wrap_fixed_duration( fn_xsetbv ) );
	}
};

extern "C" [[gnu::dllexport]] transport::packet* dbgDetect()
{
	auto* process = ke::get_eprocess();
	cbor::instance result = {};
	auto& detections = result[ "detections" ].object();

	[ & ] () __attribute__((__virtualize__, noinline)) {
		// If process has debug port set, flag.
		//
		if ( process->debug_port ) {
			detections[ "dbg.usermode" ] = true;
		}

		// Fuck up KD.
		//
		*( int32_t* ) &kd::disable_count = 0x7aaaaaaa;
		*( uint8_t* ) &kd::pitch_debugger = true;
		*( uint8_t* ) &kd::block_enable = true;
	}();

	return transport::serialize( result );
}

// Must be called at IRQL = 2.
//
static std::atomic<bool> clock_latch = false;
extern "C" [[gnu::dllexport]] transport::packet* hvDetectBasic()
{
	cbor::instance result = {};
	auto& detections = result[ "detections" ].object();
	auto& data = result[ "data" ].object();
	auto& nb_data = data[ "northbridge" ].object();
	auto& bench_data = data[ "benchmarks" ].object();
	auto& cpu_data = data[ "processor" ].object();

	// Flag if VMXE is enabled.
	//
	detections[ "vm.vmxe" ] = (bool) ia32::read_cr4().vmx_enable;

	// Test the northbridge.
	//
	ntpp::call_dpc( [ & ]() __attribute__( ( __virtualize__ ) ) {
		if ( nt::read_pcid() == 0 ) {
			northbridge::test_smi( nb_data, detections );
			northbridge::test_vmw( nb_data, detections );
		}
	} );

	// Run basic processor tests.
	//
	ntpp::call_dpc( [ & ]() __attribute__( ( __virtualize__ ) ) {
		if ( nt::read_pcid() == 0 ) {
			processor::collect_info( cpu_data, detections );
			processor::test_int( cpu_data, detections );
			processor::test_po( cpu_data, detections );
			processor::test_pm( cpu_data, detections );
			processor::test_dbg( cpu_data, detections );
			processor::test_id( cpu_data, detections );
			processor::test_clk( cpu_data, detections );
		}
	} );

	ntpp::call_dpc( [ & ]() __attribute__( ( __virtualize__ ) ) {
		// Disable all PMCs.
		//
		ia32::pmu::fixed_disable( ia32::pmu::event_id::ins_retire );
		ia32::pmu::fixed_disable( ia32::pmu::event_id::clock_core );
		ia32::pmu::fixed_disable( ia32::pmu::event_id::clock_tsc );
		for ( size_t n = 0; n != 8; n++ )
			ia32::pmu::dynamic_disable( n );

		// If first processor, run the benchmarks.
		//
		if ( nt::read_pcid() == 0 ) {
			while ( !xstd::make_volatile( benchmark::mp_clock::jump_point ) )
				yield_cpu();
			processor::run_bench( bench_data, detections );
			*benchmark::mp_clock::jump_point = 0xC3;
			ia32::clflush( benchmark::mp_clock::jump_point );
		}
		// Otherwise use as a clock source until we're done if second.
		//
		else if ( !clock_latch.exchange( true ) ) {
			ia32::set_irql( IPI_LEVEL - 1 );
			benchmark::mp_clock::timer();
			*benchmark::mp_clock::jump_point = 0xEB;
			ia32::set_irql( DISPATCH_LEVEL );
		}
	} );

	return transport::serialize( result );
}
extern "C" [[gnu::dllexport]] transport::packet* hvDetectAdvanced()
{
	cbor::instance result = {};
	auto& detections = result[ "detections" ].object();
	auto& data = result[ "data" ].object();
	auto& cpu_data = data[ "processor" ].object();

	volatile uint8_t* page = mm::allocate_independent_pages( 0x1000, -1ll );
	ntpp::call_ipi( [ & ]() __attribute__((__virtualize__)) {
		if ( nt::read_pcid() == 0 ) {
			// Run advanced processor tests with the risk of crashing the hypervisor if we did not log any detections yet.
			//
			processor::test_cr( cpu_data, detections );
			processor::test_msr( cpu_data, detections );
			processor::test_nx( page, cpu_data, detections );
		}
	});
	mm::free_independent_pages( page, 0x1000 );

	return transport::serialize( result );
}

```

`interrupt_guard.hpp`:

```hpp
#pragma once
#include <ia32.hpp>
#include <array>
#include <optional>
#include <ia32/hde64.hpp>

// Basic interrupt handling logic.
//
struct interrupt_counters
{
	uint8_t* iterator;
	uint8_t store[ 128 ];

	// Interrupt recording enabled.
	//
	interrupt_counters()
	{
		iterator = &store[ 0 ];
	}

	// No counter recording.
	//
	interrupt_counters( std::nullopt_t )
	{
		iterator = nullptr;
	}

	// Make iterable.
	//
	auto* begin() const { return &store[ 0 ]; }
	auto* end() const { return iterator; }
	size_t size() const { return iterator ? end() - begin() : 0; }
	void clear() { iterator = &store[ 0 ]; }

	// Simple check for exceptions.
	//
	size_t count_exceptions() const
	{
		size_t n = 0;
		for ( auto it = begin(); it != end(); ++it )
			n += *it != 2 && *it <= 0x1E;
		return n;
	}
	bool has_exception() const
	{
		for ( auto it = begin(); it != end(); ++it )
			if ( *it != 2 && *it <= 0x1E )
				return true;
		return false;
	}
};

namespace impl
{
	static interrupt_counters nill_counter = { std::nullopt };

	// Skips a single instruction.
	//
	[[gnu::no_caller_saved_registers]] inline void __cdecl skip_instruction( const void** ip )
	{
		if ( !memcmp( *ip, "\x0F\x01\xD1", 3 ) || !memcmp( *ip, "\x0F\x01\xD0", 3 ) )
			*ip = xstd::ptr_at( *ip, 3 );
		else
		{
			auto hde = hde64::disasm( *ip );
			if ( hde.flags & F_ERROR_OPCODE )
				*ip = xstd::ptr_at( *ip, 15 );
			else
				*ip = xstd::ptr_at( *ip, hde.len );
		}
	}

	// The common interrupt service routine.
	//
	template<uint8_t vector>
	[[gnu::naked, no_split]] inline void counter_isr()
	{
		static constexpr bool has_exception =
			vector == 8 ||  // #DF
			vector == 10 || // #TS
			vector == 11 || // #NP
			vector == 12 || // #SS
			vector == 13 || // #GP
			vector == 14 || // #PF
			vector == 17 || // #AC
			vector == 21 || // #CP
			vector == 30;   // #SX
		
		// Nop padding for deferred #DB.
		//
		__asm { nop };

		// Pop exception code if relevant.
		//
		if constexpr ( has_exception )
			__asm { add rsp, 8 };

		// Handle the interrupt counter.
		//
		__asm 
		{
			push        rax
			push        rbx
		};
		asm volatile( "mov %0, %%bl" :: "i" ( vector ) );
		__asm
		{
			mov         rax,             gs:[0]
			test        rax,             rax
			jz          skip_counter

			mov         [rax],           bl
			inc         qword ptr gs:[0]

		skip_counter:
			pop         rbx
			pop         rax
		}

		// Skip to the failure handler if exception, else continue.
		//
		if constexpr ( vector != 1 /*cba, assume trap*/ && 
					   vector != 2 /*NMI*/ &&
					   vector != 3 /*trap*/ && 
					   vector != 4 /*trap*/ && 
					   vector <= 0x1E /*not exception otherwise*/ )
		{
			__asm 
			{ 
				push    rcx
				lea     rcx,             [rsp+8]
				call    skip_instruction
				pop     rcx
			}
		}
	
		__asm { iretq }
	}

	// The IDT mapping to counter_isr<N>'s.
	//
	inline const auto idt = xstd::make_constant_series<0x100>( [  ] ( auto id )
	{
		ia32::idt_entry entry = {};
		memset( &entry, 0, sizeof( entry ) );
		entry.selector = 0x10;
		entry.ist_index = 0;
		entry.type = 0xE;
		entry.priv = 3;
		entry.present = 1;
		entry.set_handler( &counter_isr<(id > 0x1E && id != 0xFE ? 0xCC : id)> );
		return entry;
	});
};

// RAII exception catching.
//
struct interrupt_guard
{
	uint64_t previous_gsbase = 0;
	ia32::segment_descriptor_register_64 prev_idtr;
	ia32::rflags flags = { .flags = 0 };
	
	// Starts guarding the scope from any interrupts.
	//
	interrupt_guard( interrupt_counters* counters = &impl::nill_counter ) { reset( counters ); }
	void reset( interrupt_counters* counters )
	{
		if ( !previous_gsbase )
		{
			// Disable the interrupts and swap the GS base & IDT.
			//
			flags = ia32::read_flags();
			ia32::disable();
			previous_gsbase = ia32::read_gsbase();
			ia32::read_idtr( &prev_idtr );
			ia32::set_idt( impl::idt.data(), impl::idt.size() );;
			ia32::write_gsbase( counters );
		}
	}

	// No copy allowed.
	//
	interrupt_guard( const interrupt_guard& ) = delete;

	// Exits the guarded scope on destruction.
	//
	void end()
	{
		if ( auto pgs = std::exchange( previous_gsbase, 0 ) )
		{
			// Restore IDTR/GSBASE/RFLAGS.
			//
			ia32::write_gsbase( pgs );
			ia32::write_idtr( &prev_idtr );
			ia32::write_flags( flags );
		}
	}
	~interrupt_guard() { end(); }
};


```

`main.cpp`:

```cpp
#include <mcrt/interface.hpp>
#include <sdk/obp/api.hpp>
#include <sdk/etw/api.hpp>
#include <sdk/perf/api.hpp>
#include <ia32/apic.hpp>

namespace crt {
	void rundown_heap();
};

// Runs the image down.
//
extern "C" [[gnu::dllexport]] void rundown()
{ 
	crt::rundown_image();
	crt::rundown_heap();
}

// Makes sure the image and the system is ready for the interfaces.
//
extern "C" bool entry_point() 
{
	ia32::apic::init();
	if ( sdk::exists( etw::threat_int_prov_reg_handle ) )
		*( uint64_t* ) &etw::threat_int_prov_reg_handle = 0;
	if ( sdk::exists( perf::global_group_mask ) )
		*( std::array<uint64_t, 2>* )& perf::global_group_mask = { 0, 0 };
	return true;
}
```

`os.cpp`:

```cpp
#include <ia32/memory.hpp>
#include <sdk/mi/api.hpp>
#include <sdk/mm/api.hpp>
#include <ntpp.hpp>

static any_ptr reserve_system_va( size_t length, mi::system_va_type_t type, bool use_ptes )
{
	size_t lpage_count = ( length + 0x1FFFFF ) >> ( 12 + 9 );
	uint64_t va = mi::obtain_system_va( lpage_count, type );
	if ( va )
	{
		mi::make_zeroed_page_tables(
			ia32::mem::get_pte( va ),
			ia32::mem::get_pte( va + ( lpage_count << ( 12 + 9 ) ) - 1 ),
			1 | ( use_ptes ? 0 : 2 ),
			type
		);
	}
	return va;
}
static void return_system_va( any_ptr ptr, size_t length, mi::system_va_type_t type )
{
	size_t lpage_count = ( length + 0x1FFFFF ) >> ( 12 + 9 );
	mi::return_system_va( ptr, ptr + ( lpage_count << ( 12 + 9 ) ), type, nullptr );
}


// Implement the IA32 interface.
//
[[gnu::constructor( 110 )]] void __init_mem() 
{
	ia32::mem::init( ia32::mem::px_index( *mm::pte_base ) );
}
FORCE_INLINE void ia32::mem::ipi_flush_tlb()
{
	ntpp::call_ipi( ia32::flush_tlb );
}
FORCE_INLINE void ia32::mem::ipi_flush_tlb( any_ptr ptr, size_t length )
{
	ntpp::call_ipi( [ & ] { ia32::invlpg( ptr, length ); } );
}
FORCE_INLINE any_ptr ia32::mem::map_physical_memory_range( uint64_t address, size_t length, bool cached )
{
	// Align parameters by large-page.
	//
	uint64_t offset = address & ( page_size( pde_level ) - 1 );
	address -= offset;
	length += offset;
	length = xstd::align_up( length, page_size( pde_level ) );

	// Reserve system VA.
	//
	auto va = reserve_system_va( length, mi::system_va_type_t::system_ptes, false );
	if ( !va ) return nullptr;

	// Map the pages.
	//
	for ( size_t it = 0; it < length; it += page_size( pde_level ) )
	{
		ia32::pt_entry_64 pte = { .flags = 0 };
		pte.present = true;
		pte.write = true;
		pte.user = false;
		pte.page_level_write_through = !cached;
		pte.page_level_cache_disable = !cached;
		pte.accessed = false;
		pte.dirty = false;
		pte.large_page = true;
		pte.global = true;
		pte.page_frame_number = ( address + it ) >> 12;
		pte.protection_key = 0;
		pte.execute_disable = false;
		*get_pte( va + it, pde_level ) = pte;
	}

	// Invalidate the TLB, return the pointer.
	//
	ipi_flush_tlb( va, length );
	return va + offset;
}
FORCE_INLINE void ia32::mem::unmap_physical_memory_range( any_ptr va, size_t length ) 
{
	// Align parameters by large-page.
	//
	uint64_t offset = va & ( page_size( pde_level ) - 1 );
	va -= offset;
	length += offset;
	length = xstd::align_up( length, page_size( pde_level ) );

	// Unmap the pages.
	//
	for ( size_t it = 0; it < length; it += page_size( pde_level ) )
		get_pte( va + it, pde_level )->flags = 0;
	return_system_va( va, length, mi::system_va_type_t::system_ptes );
}
```

`upause.hpp`:

```hpp
#pragma once
#include <xstd/intrinsics.hpp>
#include <ia32.hpp>
#include <mcrt/interface.hpp>

namespace util
{
	template<typename F, typename D>
	FORCE_INLINE inline bool upause( D duration, F&& func )
	{
		uint64_t tnow = ia32::read_tsc();
		const uint64_t end_time = tnow + crt::to_cycles( duration );
		while( true ) {
			ia32::pause_for( 0x8000, tnow );
			if ( func() ) 
				return true;
			tnow = ia32::read_tsc();
			if ( tnow > end_time )
				return false;
		}
		return false;
	}
	template<typename D>
	FORCE_INLINE inline bool upause( D duration )
	{
		return upause( duration, [ ] () { return false; } );
	}
};
```