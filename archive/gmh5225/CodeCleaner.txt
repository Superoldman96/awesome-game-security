Project Path: arc_gmh5225_CodeCleaner_iuhbdljq

Source Tree:

```txt
arc_gmh5225_CodeCleaner_iuhbdljq
├── CodeCleaner
│   ├── CodeCleaner.vcxproj
│   ├── CodeCleaner.vcxproj.filters
│   ├── asmjit
│   │   ├── asmjit
│   │   │   ├── a64.h
│   │   │   ├── arm.h
│   │   │   ├── asmjit-scope-begin.h
│   │   │   ├── asmjit-scope-end.h
│   │   │   ├── asmjit.h
│   │   │   ├── core
│   │   │   │   ├── api-build_p.h
│   │   │   │   ├── api-config.h
│   │   │   │   ├── archcommons.h
│   │   │   │   ├── archtraits.cpp
│   │   │   │   ├── archtraits.h
│   │   │   │   ├── assembler.cpp
│   │   │   │   ├── assembler.h
│   │   │   │   ├── builder.cpp
│   │   │   │   ├── builder.h
│   │   │   │   ├── builder_p.h
│   │   │   │   ├── codebuffer.h
│   │   │   │   ├── codeholder.cpp
│   │   │   │   ├── codeholder.h
│   │   │   │   ├── codewriter.cpp
│   │   │   │   ├── codewriter_p.h
│   │   │   │   ├── compiler.cpp
│   │   │   │   ├── compiler.h
│   │   │   │   ├── compilerdefs.h
│   │   │   │   ├── constpool.cpp
│   │   │   │   ├── constpool.h
│   │   │   │   ├── cpuinfo.cpp
│   │   │   │   ├── cpuinfo.h
│   │   │   │   ├── emithelper.cpp
│   │   │   │   ├── emithelper_p.h
│   │   │   │   ├── emitter.cpp
│   │   │   │   ├── emitter.h
│   │   │   │   ├── emitterutils.cpp
│   │   │   │   ├── emitterutils_p.h
│   │   │   │   ├── environment.cpp
│   │   │   │   ├── environment.h
│   │   │   │   ├── errorhandler.cpp
│   │   │   │   ├── errorhandler.h
│   │   │   │   ├── formatter.cpp
│   │   │   │   ├── formatter.h
│   │   │   │   ├── formatter_p.h
│   │   │   │   ├── func.cpp
│   │   │   │   ├── func.h
│   │   │   │   ├── funcargscontext.cpp
│   │   │   │   ├── funcargscontext_p.h
│   │   │   │   ├── globals.cpp
│   │   │   │   ├── globals.h
│   │   │   │   ├── inst.cpp
│   │   │   │   ├── inst.h
│   │   │   │   ├── instdb.cpp
│   │   │   │   ├── instdb_p.h
│   │   │   │   ├── jitallocator.cpp
│   │   │   │   ├── jitallocator.h
│   │   │   │   ├── jitruntime.cpp
│   │   │   │   ├── jitruntime.h
│   │   │   │   ├── logger.cpp
│   │   │   │   ├── logger.h
│   │   │   │   ├── misc_p.h
│   │   │   │   ├── operand.cpp
│   │   │   │   ├── operand.h
│   │   │   │   ├── osutils.cpp
│   │   │   │   ├── osutils.h
│   │   │   │   ├── osutils_p.h
│   │   │   │   ├── raassignment_p.h
│   │   │   │   ├── rabuilders_p.h
│   │   │   │   ├── radefs_p.h
│   │   │   │   ├── ralocal.cpp
│   │   │   │   ├── ralocal_p.h
│   │   │   │   ├── rapass.cpp
│   │   │   │   ├── rapass_p.h
│   │   │   │   ├── rastack.cpp
│   │   │   │   ├── rastack_p.h
│   │   │   │   ├── string.cpp
│   │   │   │   ├── string.h
│   │   │   │   ├── support.cpp
│   │   │   │   ├── support.h
│   │   │   │   ├── support_p.h
│   │   │   │   ├── target.cpp
│   │   │   │   ├── target.h
│   │   │   │   ├── type.cpp
│   │   │   │   ├── type.h
│   │   │   │   ├── virtmem.cpp
│   │   │   │   ├── virtmem.h
│   │   │   │   ├── zone.cpp
│   │   │   │   ├── zone.h
│   │   │   │   ├── zonehash.cpp
│   │   │   │   ├── zonehash.h
│   │   │   │   ├── zonelist.cpp
│   │   │   │   ├── zonelist.h
│   │   │   │   ├── zonestack.cpp
│   │   │   │   ├── zonestack.h
│   │   │   │   ├── zonestring.h
│   │   │   │   ├── zonetree.cpp
│   │   │   │   ├── zonetree.h
│   │   │   │   ├── zonevector.cpp
│   │   │   │   └── zonevector.h
│   │   │   ├── core.h
│   │   │   └── x86.h
│   │   ├── asmjit.dll
│   │   └── asmjit_x64.lib
│   ├── capstone
│   │   ├── capstone_x64.lib
│   │   └── include
│   │       ├── capstone
│   │       │   ├── arm.h
│   │       │   ├── arm64.h
│   │       │   ├── capstone.h
│   │       │   ├── evm.h
│   │       │   ├── m680x.h
│   │       │   ├── m68k.h
│   │       │   ├── mips.h
│   │       │   ├── platform.h
│   │       │   ├── ppc.h
│   │       │   ├── sparc.h
│   │       │   ├── systemz.h
│   │       │   ├── tms320c64x.h
│   │       │   ├── x86.h
│   │       │   └── xcore.h
│   │       ├── platform.h
│   │       └── windowsce
│   │           ├── intrin.h
│   │           └── stdint.h
│   ├── plugin.cpp
│   ├── plugin.h
│   ├── pluginmain.cpp
│   ├── pluginmain.h
│   └── pluginsdk
│       ├── DeviceNameResolver
│       │   ├── DeviceNameResolver.h
│       │   ├── DeviceNameResolver_x64.a
│       │   ├── DeviceNameResolver_x64.lib
│       │   ├── DeviceNameResolver_x86.a
│       │   └── DeviceNameResolver_x86.lib
│       ├── TitanEngine
│       │   ├── TitanEngine.h
│       │   ├── TitanEngine_x64.a
│       │   ├── TitanEngine_x64.lib
│       │   ├── TitanEngine_x86.a
│       │   └── TitanEngine_x86.lib
│       ├── XEDParse
│       │   ├── XEDParse.h
│       │   ├── XEDParse_x64.a
│       │   ├── XEDParse_x64.lib
│       │   ├── XEDParse_x86.a
│       │   └── XEDParse_x86.lib
│       ├── _dbgfunctions.h
│       ├── _plugin_types.h
│       ├── _plugins.h
│       ├── _scriptapi.h
│       ├── _scriptapi_argument.h
│       ├── _scriptapi_assembler.h
│       ├── _scriptapi_bookmark.h
│       ├── _scriptapi_comment.h
│       ├── _scriptapi_debug.h
│       ├── _scriptapi_flag.h
│       ├── _scriptapi_function.h
│       ├── _scriptapi_gui.h
│       ├── _scriptapi_label.h
│       ├── _scriptapi_memory.h
│       ├── _scriptapi_misc.h
│       ├── _scriptapi_module.h
│       ├── _scriptapi_pattern.h
│       ├── _scriptapi_register.h
│       ├── _scriptapi_stack.h
│       ├── _scriptapi_symbol.h
│       ├── bridgegraph.h
│       ├── bridgelist.h
│       ├── bridgemain.h
│       ├── dbghelp
│       │   ├── dbghelp.h
│       │   ├── dbghelp_x64.a
│       │   ├── dbghelp_x64.lib
│       │   ├── dbghelp_x86.a
│       │   └── dbghelp_x86.lib
│       ├── jansson
│       │   ├── jansson.h
│       │   ├── jansson_config.h
│       │   ├── jansson_x64.a
│       │   ├── jansson_x64.lib
│       │   ├── jansson_x64dbg.h
│       │   ├── jansson_x86.a
│       │   └── jansson_x86.lib
│       ├── lz4
│       │   ├── lz4.h
│       │   ├── lz4_x64.a
│       │   ├── lz4_x64.lib
│       │   ├── lz4_x86.a
│       │   ├── lz4_x86.lib
│       │   ├── lz4file.h
│       │   └── lz4hc.h
│       ├── x32bridge.lib
│       ├── x32dbg.lib
│       ├── x64bridge.lib
│       └── x64dbg.lib
└── CodeCleaner.sln

```

`CodeCleaner.sln`:

```sln

Microsoft Visual Studio Solution File, Format Version 12.00
# Visual Studio Version 17
VisualStudioVersion = 17.1.31911.260
MinimumVisualStudioVersion = 10.0.40219.1
Project("{8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942}") = "CodeCleaner", "CodeCleaner\CodeCleaner.vcxproj", "{D35E5F15-B018-4F5E-87FA-F6CAFC47FEE0}"
EndProject
Global
	GlobalSection(SolutionConfigurationPlatforms) = preSolution
		Release|x64 = Release|x64
		Release|x86 = Release|x86
	EndGlobalSection
	GlobalSection(ProjectConfigurationPlatforms) = postSolution
		{D35E5F15-B018-4F5E-87FA-F6CAFC47FEE0}.Release|x64.ActiveCfg = Release|x64
		{D35E5F15-B018-4F5E-87FA-F6CAFC47FEE0}.Release|x64.Build.0 = Release|x64
		{D35E5F15-B018-4F5E-87FA-F6CAFC47FEE0}.Release|x86.ActiveCfg = Release|Win32
		{D35E5F15-B018-4F5E-87FA-F6CAFC47FEE0}.Release|x86.Build.0 = Release|Win32
	EndGlobalSection
	GlobalSection(SolutionProperties) = preSolution
		HideSolutionNode = FALSE
	EndGlobalSection
	GlobalSection(ExtensibilityGlobals) = postSolution
		SolutionGuid = {A9B33C93-31A3-4368-ADF2-6EC2E4C5DF5D}
	EndGlobalSection
EndGlobal

```

`CodeCleaner/CodeCleaner.vcxproj`:

```vcxproj
<?xml version="1.0" encoding="utf-8"?>
<Project DefaultTargets="Build" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup Label="ProjectConfigurations">
    <ProjectConfiguration Include="Release|Win32">
      <Configuration>Release</Configuration>
      <Platform>Win32</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|x64">
      <Configuration>Release</Configuration>
      <Platform>x64</Platform>
    </ProjectConfiguration>
  </ItemGroup>
  <ItemGroup>
    <ClInclude Include="plugin.h" />
    <ClInclude Include="pluginmain.h" />
  </ItemGroup>
  <ItemGroup>
    <ClCompile Include="plugin.cpp" />
    <ClCompile Include="pluginmain.cpp" />
  </ItemGroup>
  <PropertyGroup Label="Globals">
    <VCProjectVersion>16.0</VCProjectVersion>
    <Keyword>Win32Proj</Keyword>
    <ProjectGuid>{d35e5f15-b018-4f5e-87fa-f6cafc47fee0}</ProjectGuid>
    <RootNamespace>CodeCleaner</RootNamespace>
    <WindowsTargetPlatformVersion>10.0</WindowsTargetPlatformVersion>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.Default.props" />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'" Label="Configuration">
    <ConfigurationType>DynamicLibrary</ConfigurationType>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>v143</PlatformToolset>
    <WholeProgramOptimization>true</WholeProgramOptimization>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'" Label="Configuration">
    <ConfigurationType>DynamicLibrary</ConfigurationType>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>v143</PlatformToolset>
    <WholeProgramOptimization>true</WholeProgramOptimization>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.props" />
  <ImportGroup Label="ExtensionSettings">
  </ImportGroup>
  <ImportGroup Label="Shared">
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <PropertyGroup Label="UserMacros" />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <LinkIncremental>false</LinkIncremental>
    <IncludePath>$(ProjectDir)pluginsdk;$(ProjectDir)capstone\include;$(ProjectDir)asmjit;$(IncludePath)</IncludePath>
    <TargetExt>.dp32</TargetExt>
    <LibraryPath>$(ProjectDir)capstone;$(ProjectDir)asmjit;$(LibraryPath)</LibraryPath>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <LinkIncremental>false</LinkIncremental>
    <IncludePath>$(ProjectDir)pluginsdk;$(ProjectDir)capstone\include;$(ProjectDir)asmjit;$(IncludePath)</IncludePath>
    <TargetExt>.dp64</TargetExt>
    <OutDir>$(SolutionDir)$(Configuration)\</OutDir>
    <IntDir>$(Configuration)\</IntDir>
    <LibraryPath>$(ProjectDir)capstone;$(ProjectDir)asmjit;$(LibraryPath)</LibraryPath>
  </PropertyGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <FunctionLevelLinking>true</FunctionLevelLinking>
      <IntrinsicFunctions>true</IntrinsicFunctions>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>WIN32;NDEBUG;CODECLEANER_EXPORTS;_WINDOWS;_USRDLL;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
      <PrecompiledHeader>NotUsing</PrecompiledHeader>
      <PrecompiledHeaderFile>pch.h</PrecompiledHeaderFile>
    </ClCompile>
    <Link>
      <SubSystem>Windows</SubSystem>
      <EnableCOMDATFolding>true</EnableCOMDATFolding>
      <OptimizeReferences>true</OptimizeReferences>
      <GenerateDebugInformation>false</GenerateDebugInformation>
      <EnableUAC>false</EnableUAC>
    </Link>
  </ItemDefinitionGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <FunctionLevelLinking>true</FunctionLevelLinking>
      <IntrinsicFunctions>true</IntrinsicFunctions>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>NDEBUG;CODECLEANER_EXPORTS;_WINDOWS;_USRDLL;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
      <PrecompiledHeader>NotUsing</PrecompiledHeader>
      <PrecompiledHeaderFile>pch.h</PrecompiledHeaderFile>
    </ClCompile>
    <Link>
      <SubSystem>Windows</SubSystem>
      <EnableCOMDATFolding>true</EnableCOMDATFolding>
      <OptimizeReferences>true</OptimizeReferences>
      <GenerateDebugInformation>false</GenerateDebugInformation>
      <EnableUAC>false</EnableUAC>
      <AdditionalDependencies>capstone_x64.lib;asmjit_x64.lib;%(AdditionalDependencies)</AdditionalDependencies>
    </Link>
  </ItemDefinitionGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.targets" />
  <ImportGroup Label="ExtensionTargets">
  </ImportGroup>
</Project>
```

`CodeCleaner/CodeCleaner.vcxproj.filters`:

```filters
<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup>
    <Filter Include="源文件">
      <UniqueIdentifier>{4FC737F1-C7A5-4376-A066-2A32D752A2FF}</UniqueIdentifier>
      <Extensions>cpp;c;cc;cxx;c++;cppm;ixx;def;odl;idl;hpj;bat;asm;asmx</Extensions>
    </Filter>
    <Filter Include="头文件">
      <UniqueIdentifier>{93995380-89BD-4b04-88EB-625FBE52EBFB}</UniqueIdentifier>
      <Extensions>h;hh;hpp;hxx;h++;hm;inl;inc;ipp;xsd</Extensions>
    </Filter>
    <Filter Include="资源文件">
      <UniqueIdentifier>{67DA6AB6-F800-4c08-8B7A-83BB121AAD01}</UniqueIdentifier>
      <Extensions>rc;ico;cur;bmp;dlg;rc2;rct;bin;rgs;gif;jpg;jpeg;jpe;resx;tiff;tif;png;wav;mfcribbon-ms</Extensions>
    </Filter>
  </ItemGroup>
  <ItemGroup>
    <ClInclude Include="pluginmain.h">
      <Filter>头文件</Filter>
    </ClInclude>
    <ClInclude Include="plugin.h">
      <Filter>头文件</Filter>
    </ClInclude>
  </ItemGroup>
  <ItemGroup>
    <ClCompile Include="pluginmain.cpp">
      <Filter>源文件</Filter>
    </ClCompile>
    <ClCompile Include="plugin.cpp">
      <Filter>源文件</Filter>
    </ClCompile>
  </ItemGroup>
</Project>
```

`CodeCleaner/asmjit/asmjit/a64.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_A64_H_INCLUDED
#define ASMJIT_A64_H_INCLUDED

//! \addtogroup asmjit_a64
//!
//! ### Emitters
//!
//!   - \ref a64::Assembler - AArch64 assembler (must read, provides examples).
//!   - \ref a64::Builder - AArch64 builder.
//!   - \ref a64::Compiler - AArch64 compiler.
//!   - \ref a64::Emitter - AArch64 emitter (abstract).
//!
//! ### Supported Instructions
//!
//!   - Emitters:
//!     - \ref a64::EmitterExplicitT - Provides all instructions that use explicit operands, provides also utility
//!       functions. The member functions provided are part of all AArch64 emitters.
//!
//!   - Instruction representation:
//!     - \ref a64::Inst::Id - instruction identifiers.
//!
//! ### Register Operands
//!
//!   - \ref arm::Reg - Base class of all AArch32/AArch64 registers.
//!     - \ref a64::Gp - General purpose register (AArch64):
//!       - \ref a64::GpW - 32-bit general purpose register (AArch64).
//!       - \ref a64::GpX - 64-bit general purpose register (AArch64).
//!     - \ref a64::Vec - Vector (SIMD) register:
//!       - \ref a64::VecB - 8-bit SIMD register.
//!       - \ref a64::VecH - 16-bit SIMD register.
//!       - \ref a64::VecS - 32-bit SIMD register.
//!       - \ref a64::VecD - 64-bit SIMD register.
//!       - \ref a64::VecV - 128-bit SIMD register.
//!
//! ### Memory Operands
//!
//!   - \ref arm::Mem - AArch32/AArch64 memory operand that provides support for all ARM addressing features
//!     including base, index, pre/post increment, and ARM-specific shift addressing and index extending.
//!
//! ### Other
//!
//!   - \ref arm::Shift - Shift operation and value.
//!   - \ref arm::Utils - Utilities that can help during code generation for AArch32 and AArch64.

#include "./arm.h"
#include "./arm/a64assembler.h"
#include "./arm/a64builder.h"
#include "./arm/a64compiler.h"
#include "./arm/a64emitter.h"
#include "./arm/a64globals.h"
#include "./arm/a64instdb.h"
#include "./arm/a64operand.h"

#endif // ASMJIT_A64_H_INCLUDED


```

`CodeCleaner/asmjit/asmjit/arm.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_ARM_H_INCLUDED
#define ASMJIT_ARM_H_INCLUDED

//! \addtogroup asmjit_arm
//!
//! ### Namespaces
//!
//!   - \ref arm - arm namespace provides common functionality for both AArch32 and AArch64 backends.
//!   - \ref a32 - a32 namespace provides support for AArch32 architecture. In addition it includes
//!     \ref arm namespace, so you can only use a single namespace when targeting AArch32 architecture.
//!   - \ref a64 - a64 namespace provides support for AArch64 architecture. In addition it includes
//!     \ref arm namespace, so you can only use a single namespace when targeting AArch64 architecture.
//!
//! ### Emitters
//!
//!   - AArch32
//!     - \ref a32::Assembler - AArch32 assembler (must read, provides examples).
//!     - \ref a32::Builder - AArch32 builder.
//!     - \ref a32::Compiler - AArch32 compiler.
//!     - \ref a32::Emitter - AArch32 emitter (abstract).
//!
//!   - AArch64
//!     - \ref a64::Assembler - AArch64 assembler (must read, provides examples).
//!     - \ref a64::Builder - AArch64 builder.
//!     - \ref a64::Compiler - AArch64 compiler.
//!     - \ref a64::Emitter - AArch64 emitter (abstract).
//!
//! ### Supported Instructions
//!
//!   - AArch32:
//!     - Emitters:
//!       - \ref a32::EmitterExplicitT - Provides all instructions that use explicit operands, provides also
//!         utility functions. The member functions provided are part of all AArch32 emitters.
//!     - Instruction representation:
//!       - \ref a32::Inst::Id - instruction identifiers.
//!
//!   - AArch64:
//!     - Emitters:
//!       - \ref a64::EmitterExplicitT - Provides all instructions that use explicit operands, provides also
//!         utility functions. The member functions provided are part of all AArch64 emitters.
//!     - Instruction representation:
//!       - \ref a64::Inst::Id - instruction identifiers.
//!
//! ### Register Operands
//!
//!   - \ref arm::Reg - Base class of all AArch32/AArch64 registers.
//!     - \ref a32::Gp - 32-bit general purpose register used by AArch32:
//!     - \ref a64::Gp - 32-bit or 64-bit general purpose register used by AArch64:
//!       - \ref a64::GpW - 32-bit register (AArch64).
//!       - \ref a64::GpX - 64-bit register (AArch64).
//!     - \ref arm::BaseVec - Base vector (SIMD) register.
//!       - \ref a32::Vec - Vector (SIMD) register (AArch32):
//!         - \ref a32::VecS - 32-bit SIMD register (AArch32).
//!         - \ref a32::VecD - 64-bit SIMD register (AArch32).
//!         - \ref a32::VecV - 128-bit SIMD register (AArch32).
//!       - \ref a64::Vec - Vector (SIMD) register (AArch64):
//!         - \ref a64::VecB - 8-bit SIMD register (AArch64).
//!         - \ref a64::VecH - 16-bit SIMD register (AArch64).
//!         - \ref a64::VecS - 32-bit SIMD register (AArch64).
//!         - \ref a64::VecD - 64-bit SIMD register (AArch64).
//!         - \ref a64::VecV - 128-bit SIMD register (AArch64).
//!
//! ### Memory Operands
//!
//!   - \ref arm::Mem - AArch32/AArch64 memory operand that provides support for all ARM addressing features
//!     including base, index, pre/post increment, and ARM-specific shift addressing and index extending.
//!
//! ### Other
//!
//!   - \ref arm::Shift - Shift operation and value (both AArch32 and AArch64).
//!   - \ref arm::DataType - Data type that is part of an instruction in AArch32 mode.
//!   - \ref arm::Utils - Utilities that can help during code generation for AArch32 and AArch64.

#include "./core.h"
#include "./arm/armglobals.h"
#include "./arm/armoperand.h"
#include "./arm/armutils.h"

#endif // ASMJIT_ARM_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/asmjit-scope-begin.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifdef _WIN32
  #pragma push_macro("min")
  #pragma push_macro("max")

  #ifdef min
    #undef min
  #endif

  #ifdef max
    #undef max
  #endif
#endif

```

`CodeCleaner/asmjit/asmjit/asmjit-scope-end.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifdef _WIN32
  #pragma pop_macro("min")
  #pragma pop_macro("max")
#endif

```

`CodeCleaner/asmjit/asmjit/asmjit.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// SPDX-License-Identifier: Zlib
// Official GitHub Repository: https://github.com/asmjit/asmjit
//
// Copyright (c) 2008-2024 The AsmJit Authors
//
// This software is provided 'as-is', without any express or implied
// warranty. In no event will the authors be held liable for any damages
// arising from the use of this software.
//
// Permission is granted to anyone to use this software for any purpose,
// including commercial applications, and to alter it and redistribute it
// freely, subject to the following restrictions:
//
// 1. The origin of this software must not be misrepresented; you must not
//    claim that you wrote the original software. If you use this software
//    in a product, an acknowledgment in the product documentation would be
//    appreciated but is not required.
// 2. Altered source versions must be plainly marked as such, and must not be
//    misrepresented as being the original software.
// 3. This notice may not be removed or altered from any source distribution.

#ifndef ASMJIT_ASMJIT_H_INCLUDED
#define ASMJIT_ASMJIT_H_INCLUDED

#include "./core.h"

#ifndef ASMJIT_NO_X86
  #include "./x86.h"
#endif

#endif // ASMJIT_ASMJIT_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_H_INCLUDED
#define ASMJIT_CORE_H_INCLUDED

//! Root namespace used by AsmJit.
namespace asmjit {

//! \mainpage API Reference
//!
//! AsmJit C++ API reference documentation generated by Doxygen.
//!
//! AsmJit library uses one global namespace called \ref asmjit, which provides the whole functionality. Core
//! functionality is within \ref asmjit namespace and architecture specific functionality is always in its own
//! namespace. For example \ref asmjit::x86 provides both 32-bit and 64-bit X86 code generation.
//!
//! \section doc_groups Documentation Groups
//!
//! AsmJit documentation is structured into groups. Groups can be followed in order to learn AsmJit, but knowledge
//! from multiple groups is required to use AsmJit properly:
//!
//! $$DOCS_GROUP_OVERVIEW$$
//!
//! \note It's important to understand that in order to learn AsmJit all groups are important. Some groups can be
//! omitted if a particular tool is out of interest - for example \ref asmjit_assembler users don't need to know
//! about \ref asmjit_builder, but it's not the opposite. \ref asmjit_builder users should know about \ref
//! asmjit_assembler as it also uses operands, labels, and other concepts. Similarly \ref asmjit_compiler users
//! should know how both \ref asmjit_assembler and \ref asmjit_builder tools work.
//!
//! \section where_to_start Where To Start
//!
//! AsmJit \ref asmjit_core provides the following two classes that are essential from the code generation perspective:
//!
//!   - \ref CodeHolder provides functionality to temporarily hold the generated code. It stores all the necessary
//!     information about the code - code buffers, sections, labels, symbols, and information about relocations.
//!
//!   - \ref BaseEmitter provides interface used by emitter implementations. The interface provides basic building
//!     blocks that are then implemented by \ref BaseAssembler, \ref BaseBuilder, and \ref BaseCompiler.
//!
//! Code emitters:
//!
//!  - \ref asmjit_assembler - provides direct machine code generation.
//!
//!  - \ref asmjit_builder - provides intermediate code generation that can be processed before it's serialized to
//!    \ref BaseAssembler.
//!
//!  - \ref asmjit_compiler - provides high-level code generation with built-in register allocation.
//!
//!  - \ref FuncNode - provides insight into how function looks from the Compiler perspective and how it's stored in
//!     a node-list.
//!
//! \section main_recommendations Recommendations
//!
//! The following steps are recommended for all AsmJit users:
//!
//!   - Make sure that you use \ref Logger, see \ref asmjit_logging.
//!
//!   - Make sure that you use \ref ErrorHandler, see \ref asmjit_error_handling.
//!
//!   - Instruction validation in your debug builds can reveal problems too. AsmJit provides validation at instruction
//!     level that can be enabled via \ref BaseEmitter::addDiagnosticOptions(). See \ref DiagnosticOptions for more
//!     details.
//!
//!   - If you are a Compiler user, use diagnostic options and read carefully if anything suspicious pops out.
//!     Diagnostic options can be enabled via \ref BaseEmitter::addDiagnosticOptions(). If unsure which ones to use,
//!     enable annotations and all debug options: `DiagnosticOptions::kRAAnnotate | DiagnosticOptions::kRADebugAll`.
//!
//!   - Make sure you put a breakpoint into \ref DebugUtils::errored() function if you have a problem with AsmJit
//!     returning errors during instruction encoding or register allocation. Having an active breakpoint there can
//!     help to reveal the origin of the error, to inspect variables and other conditions that caused it.
//!
//! The reason for using \ref Logger and \ref ErrorHandler is that they provide a very useful information about what's
//! happening inside emitters. In many cases the information provided by these two is crucial to quickly identify and
//! fix issues that happen during development (for example wrong instruction, address, or register used). In addition,
//! output from \ref Logger is always necessary when filling bug reports. In other words, using logging and proper error
//! handling can save a lot of time during the development and can also save users from submitting issues.
//!
//! \section other_pages Other Pages
//!
//!   - <a href="annotated.html">Class List</a> - List of classes sorted alphabetically
//!   - <a href="namespaceasmjit.html">AsmJit Namespace</a> - List of symbols provided by `asmjit` namespace


//! \defgroup asmjit_build Build Instructions
//! \brief Build instructions, supported environments, and feature selection.
//!
//! ### Overview
//!
//! AsmJit is designed to be easy embeddable in any project. However, it depends on some compile-time definitions that
//! can be used to enable or disable features to decrease the resulting binary size. A typical way of building AsmJit
//! is to use [cmake](https://www.cmake.org), but it's also possible to just include AsmJit source code in your project
//! and to just build it. The easiest way to include AsmJit in your project is to just include **src** directory in
//! your project and to define \ref ASMJIT_STATIC. AsmJit can be just updated from time to time without any changes to
//! this integration process. Do not embed AsmJit's `test` files in such case as these are used exclusively for testing.
//!
//! \section supported_environment Supported Operating Systems, Compilers, and Build Tools
//!
//! ### Supported C++ Compilers
//!
//!   - Requirements:
//!
//!     - AsmJit won't build without C++11 enabled. If you use older GCC or Clang you would have to enable at least
//!       C++11 standard through compiler flags.
//!
//!   - Tested:
//!
//!     - **Clang** - Tested by GitHub Actions - Clang 10+ is officially supported and tested by CI, older Clang versions
//!       having C++11 should work, but are not tested anymore due to upgraded CI images.
//!
//!     - **GNU** - Tested by GitHub Actions - GCC 7+ is officially supported, older GCC versions from 4.8+ having C++11
//!       enabled should also work, but are not tested anymore due to upgraded CI images.
//!
//!     - **MINGW** - Reported to work, but not tested in our CI environment (help welcome).
//!
//!     - **MSVC** - Tested by GitHub Actions - VS2019+ is officially supported, VS2015 and VS2017 is reported to work,
//!       but not tested by CI anymore.
//!
//! ### Supported Operating Systems and Platforms
//!
//!   - Tested:
//!
//!     - **BSD** - FreeBSD, NetBSD, and OpenBSD tested by GitHub Actions (only recent images are tested by CI). BSD
//!       runners only test BSD images with clang compiler.
//!
//!     - **Linux** - Tested by GitHub Actions (only recent Ubuntu images are tested by CI, in general any distribution
//!       should be supported as AsmJit has no dependencies).
//!
//!     - **Mac OS** - Tested by GitHub Actions.
//!
//!     - **Windows** - Tested by GitHub Actions - (Windows 7+ is officially supported).
//!
//!     - **Emscripten** - Works if compiled with \ref ASMJIT_NO_JIT. AsmJit cannot generate WASM code, but can be
//!       used to generate X86/X64/AArch64 code within a browser, for example.
//!
//!   - Untested:
//!
//!     - **Haiku** - Reported to work, not tested by CI.
//!
//!     - **Other** operating systems would require some testing and support in the following files:
//!       - [core/api-config.h](https://github.com/asmjit/asmjit/tree/master/src/asmjit/core/api-config.h)
//!       - [core/osutils.cpp](https://github.com/asmjit/asmjit/tree/master/src/asmjit/core/osutils.cpp)
//!       - [core/virtmem.cpp](https://github.com/asmjit/asmjit/tree/master/src/asmjit/core/virtmem.cpp)
//!
//! ### Supported Backends / Architectures
//!
//!   - **X86** and **X86_64** - Both 32-bit and 64-bit backends tested on CI.
//!   - **AArch64** - Tested on CI (Native Apple runners and Linux emulated via QEMU).
//!
//! \section build_mode Static Builds and Embedding
//!
//! These definitions can be used to enable static library build. Embed is used when AsmJit's source code is embedded
//! directly in another project, implies static build as well.
//!
//!   - \ref ASMJIT_EMBED - Asmjit is embedded, implies \ref ASMJIT_STATIC.
//!   - \ref ASMJIT_STATIC - Enable static-library build.
//!
//! \note Projects that use AsmJit statically must define \ref ASMJIT_STATIC in all compilation units that use AsmJit,
//! otherwise AsmJit would use dynamic library imports in \ref ASMJIT_API decorator. The recommendation is to define
//! this macro across the whole project that uses AsmJit this way.
//!
//! \section cmake_integration CMake Integration
//!
//! AsmJit has a first-class CMake support. When consuming AsmJit as a cmake dependency, just use `asmjit::asmjit`
//! as a link dependency, which would instrument cmake to setup everything else, including include paths, and build
//! flags (either defining `ASMJIT_STATIC` or not, and possibly defining other AsmJit feature macros). For example
//! considering that AsmJit was fetched to `3rdparty/asmjit` directory in your project as an external dependency,
//! you can just use the following CMake snippet that integrates AsmJit with your own CMake project:
//!
//! ```cmake
//! cmake_minimum_required(VERSION 3.30)
//!
//! project(asmjit_consumer C CXX)    # Both C and CXX are required.
//! set(CMAKE_CXX_STANDARD 17)        # C++11 and never is supported.
//!
//! set(ASMJIT_DIR "3rdparty/asmjit") # Location of AsmJit.
//! set(ASMJIT_STATIC TRUE)           # Force static build.
//!
//! add_subdirectory("${ASMJIT_DIR}") # This adds AsmJit as a part of your project.
//!
//! add_executable(asmjit_consumer asmjit_consumer.cpp)
//! target_link_libraries(
//!   asmjit_consumer asmjit::asmjit) # This adds AsmJit as a dependency to your target.
//! ```
//!
//! \section build_type Build Type Configuration
//!
//! These definitions control whether asserts are active or not. By default AsmJit would autodetect build configuration
//! from existing pre-processor definitions, but this behavior can be overridden, for example to enable debug asserts
//! in release configuration.
//!
//!   - \ref ASMJIT_BUILD_DEBUG - Overrides build configuration to debug, asserts will be enabled in this case.
//!   - \ref ASMJIT_BUILD_RELEASE - Overrides build configuration to release, asserts will be disabled in this case.
//!
//! \note There is usually no need to override the build configuration. AsmJit detects the build configuration by
//! checking whether `NDEBUG` is defined and automatically defines \ref ASMJIT_BUILD_RELEASE if configuration overrides
//! were not used. We only recommend using build configuration overrides in special situations, like using AsmJit in
//! release configuration with asserts enabled for whatever reason.
//!
//! \section build_backends AsmJit Backends
//!
//! AsmJit currently supports only X86/X64 backend, but the plan is to add more backends in the future. By default
//! AsmJit builds only the host backend, which is auto-detected at compile-time, but this can be overridden.
//!
//!   - \ref ASMJIT_NO_X86 - Disables both X86 and X86_64 backends.
//!   - \ref ASMJIT_NO_AARCH64 - Disables AArch64 backend.
//!   - \ref ASMJIT_NO_FOREIGN - Disables the support for foreign architecture backends, only keeps a native backend.
//!
//! \section build_options Build Options
//!
//!   - \ref ASMJIT_NO_DEPRECATED - Disables deprecated API at compile time so it won't be available and the
//!     compilation will fail if there is attempt to use such API. This includes deprecated classes, namespaces,
//!     enumerations, and functions.
//!
//!   - \ref ASMJIT_NO_SHM_OPEN - Disables functionality that uses `shm_open()`.
//!
//!   - \ref ASMJIT_NO_ABI_NAMESPACE - Disables inline ABI namespace within `asmjit` namespace. This is only provided
//!     for users that control all the dependencies (even transitive ones) and that make sure that no two AsmJit
//!     versions are used at the same time. This option can be debugging a little simpler as there would not be ABI
//!     tag after `asmjit::` namespace. Otherwise asmjit would look like `asmjit::_abi_1_13::`, for example.
//!
//! \section build_features Build Features
//!
//! AsmJit builds by default all supported features, which includes all emitters, logging, instruction validation and
//! introspection, and JIT memory allocation. Features can be disabled at compile time by using `ASMJIT_NO_...`
//! definitions.
//!   - \ref ASMJIT_NO_JIT - Disables JIT memory management and \ref JitRuntime.
//!
//!   - \ref ASMJIT_NO_TEXT - Disables everything that contains string representation of AsmJit constants, should
//!     be used together with \ref ASMJIT_NO_LOGGING as logging doesn't make sense without the ability to query
//!     instruction names, register names, etc...
//!
//!   - \ref ASMJIT_NO_LOGGING - Disables \ref Logger and \ref Formatter.
//!
//!   - \ref ASMJIT_NO_VALIDATION - Disables validation API.
//!
//!   - \ref ASMJIT_NO_INTROSPECTION - Disables instruction introspection API, must be used together with \ref
//!     ASMJIT_NO_COMPILER as \ref asmjit_compiler requires introspection for its liveness analysis and register
//!     allocation.
//!
//!   - \ref ASMJIT_NO_BUILDER - Disables \ref asmjit_builder functionality completely. This implies \ref
//!     ASMJIT_NO_COMPILER as \ref asmjit_compiler cannot be used without \ref asmjit_builder.
//!
//!   - \ref ASMJIT_NO_COMPILER - Disables \ref asmjit_compiler functionality completely.
//!
//! \note It's not recommended to disable features if you plan to build AsmJit as a shared library that will be
//! used by multiple projects that you don't control how AsmJit was built (for example AsmJit in a Linux distribution).
//! The possibility to disable certain features exists mainly for customized AsmJit builds.


//! \defgroup asmjit_breaking_changes Breaking Changes
//! \brief Documentation of breaking changes
//!
//! ### Overview
//!
//! AsmJit is a live project that is being actively developed. Deprecating the existing API in favor of a new
//! one is preferred, but it's not always possible if the changes are significant. AsmJit authors prefer to do
//! accumulated breaking changes at once instead of breaking the API often. This page documents deprecated and
//! removed APIs and should serve as a how-to guide for people that want to port existing code to work with the
//! newest AsmJit.
//!
//! \section tips Tips
//!
//! Useful tips before you start:
//!
//!   - Visit our [Public Gitter Chat](https://app.gitter.im/#/room/#asmjit:gitter.im) if you need a quick help.
//!
//!   - Build AsmJit with `ASMJIT_NO_DEPRECATED` macro defined to make sure that you are not using deprecated
//!     functionality at all. Deprecated functions are decorated with `ASMJIT_DEPRECATED()` macro, but sometimes
//!     it's not possible to decorate everything like classes, which are used by deprecated functions as well,
//!     because some compilers would warn about that. If your project compiles fine with `ASMJIT_NO_DEPRECATED`
//!     it's not using anything, which was deprecated.
//!
//! \section api_changes API Changes
//!
//! ### Changes committed at XXXX-XX-XX
//!
//! Core changes:
//!
//!   - Removed AVX512 functionality that was never used on x86 hardware as Xeon Phi was never supported by AsmJit:
//!
//!     - AVX512_ER
//!     - AVX512_PF
//!     - AVX512_4FMAPS
//!     - AVX512_4VNNIW
//!
//!   - Instruction 'vcvtneps2bf16' no longer accepts memory operand without explicit size (to minimize ambiguity)
//!
//! ### Changes committed at 2024-01-01
//!
//! Core changes:
//!
//!   - Renamed equality functions `eq()` to `equals()` - Only related to `String`, `ZoneVector`, and `CpuFeatures`.
//!     Old function names were deprecated.
//!
//!   - Removed `CallConvId::kNone` in favor of `CallConvId::kCDecl`, which is now the default calling convention.
//!
//!   - Deprecated `CallConvId::kHost` in favor of `CallConvId::kCDecl` - host calling convention is now not part
//!     of CallConvId, it can be calculated from CallConvId and Environment instead.
//!
//! ### Changes committed at 2023-12-27
//!
//! Core changes:
//!
//!   - Renamed `a64::Vec::ElementType` to `a64::VecElementType` and made it a typed enum. This enum was used mostly
//!     internally, but there is a public API using it, so it's a breaking change.
//!
//!   - Refactored `FuncSignature`, `FuncSignatureT`, and `FuncSignatureBuilder`. There is only `FuncSignature` now,
//!     which acts as a function signature holder and builder. Replace `FuncSignatureBuilder` with `FuncSignature`
//!     and use `FuncSignature::build<args>` instead of `FuncSignatureT<args>`. The old API has been deprecated.
//!
//!   - The maximum number of function arguments was raised from 16 to 32.
//!
//! ### Changes committed at 2023-12-26
//!
//! Core changes:
//!
//!   - Reworked InstNode and InstExNode to be friendlier to static analysis and to not cause undefined behavior.
//!     InstNode has no operands visually embedded within the struct so there is no _opArray (which was internal).
//!     This means that sizeof(InstNode) changed, but since it's allocated by AsmJit this should be fine. Moreover,
//!     there is no longer InstExNode as that was more a hack, instead there is now InstNodeWithOperands, which is
//!     a template and specifies the number of operands embedded (InstNode accesses these). All nodes that inherited
//!     InstExNode now just inherit InstNodeWithOperands<InstNode::kBaseOpCapacity>, which would provide the same
//!     number of nodes as InstNode.
//!
//!   - Moved GP and Vec registers from asmjit::arm namespace to asmjit::a64 namespace. At this time there was
//!     no prior deprecation as having arm::Vec would collide with a64::Vec as arm namespace is used within a64
//!     namespace. Just change `arm::Gp` to `a64::Gp` and `arm::Vec` to `a64::Vec`.
//!
//! ### Changes committed at 2023-09-10
//!
//! Core changes:
//!
//!   - Changed allocation API to work with spans (JitAllocator).
//!
//!     - This change is required to support more hardened platforms in the future that make it very difficult
//!       to write JIT compilers.
//!     - `JitAllocator::Span` now represents a memory that the user can access. It abstracts both regular and
//!       dual mappings.
//!     - The `Span` is mostly designed to make it possible to write into it, so in general the read+execute
//!       pointer is what user is intended to keep. Use `span.rx()` to access RX pointer. `Span` is not needed
//!       after the memory it references has been modified, only remember `span.rx()` pointer, which is then
//!       used to deallocate or change the memory the span references.
//!     - Use a new `JitAllocator::alloc()` to allocate a `Span`, then pass the populated Span to `JitAllocator`
//!       write API such as `JitAllocator::write()` - note that JitAllocator can also establish a scope, so you
//!       can use a lambda function that would perform the write, but since it's going through JitAllocator it's
//!       able to ensure that the memory is actually writable.
//!     - If you need to repopulate a `Span` from rx pointer, use `JitAllocator::query(<span-out>, rx)` to get it.
//!     - Study what JitRuntime is doing to better understand how this new API works in detail.
//!     - Users of JitRuntime do not have to do anything as JitRuntime properly abstracts the allocation.
//!
//!   - Renamed some X86 CPU features to make them compatible with architecture manuals:
//!
//!     - Changed `AVX512_CDI` to `AVX512_CD`.
//!     - Changed `AVX512_ERI` to `AVX512_ER`.
//!     - Changed `AVX512_PFI` to `AVX512_PF`.
//!
//!     - Old names were deprecated.
//!
//! ### Changes committed at 2021-12-13
//!
//! Core changes:
//!
//!   - Removed old deprecated API.
//!
//!   - Many enumerations were changed to enum class, and many public APIs were changed to use such enums instead
//!     of uint32_t. This change makes some APIs backward incompatible - there are no deprecations this time.
//!
//!   - Extracted operand signature manipulation to `OperandSignature`.
//!   - Setting function arguments through `Compiler::setArg()` was deprecated, use FuncNode::setArg() instead.
//!   - Moved `{arch}::Features::k` to `CpuFeatures::{arch}::k`.
//!   - Moved `BaseEmitter::kEncodingOption` to `EncodingOptions::k`.
//!   - Moved `BaseEmitter::kFlag` to `EmitterFlags::k`.
//!   - Moved `BaseEmitter::kType` to `EmitterType::k`.
//!   - Moved `BaseEmitter::kValidationOption` to `DiagnosticOptions::kValidate`.
//!   - Moved `BaseFeatures` to `CpuFeatures`.
//!   - Moved `BaseInst::kControl` to `InstControlFlow::k`.
//!   - Moved `BaseInst::kOption` and `x86::Inst::kOption` to `InstOptions::k`.
//!   - Moved `BaseNode::kNode` to `NodeType::k`.
//!   - Moved `BaseReg::kGroup` and `x86::Reg::kGroup` to `RegGroup::k`.
//!   - Moved `BaseReg::kType` and `x86::Reg::kType` to `RegType::k`.
//!   - Moved `CallConv::kFlag` to `CallConvFlags::k`.
//!   - Moved `CallConv::kId` to `CallConvId::k`.
//!   - Moved `CallConv::kStrategy` to `CallConvStrategy::k`.
//!   - Moved `CodeBuffer::kFlag` to `CodeBufferFlags`.
//!   - Moved `ConstPool::kScope` to `ConstPoolScope::k`.
//!   - Moved `Environment::kArch` to `Arch::k`.
//!   - Moved `Environment::kSubArch` to `SubArch::k`.
//!   - Moved `Environment::kFormat` to `OjectFormat::k`.
//!   - Moved `Environment::kPlatform` to `Platform::k`.
//!   - Moved `Environment::kAbi` to `PlatformABI::k`.
//!   - Moved `Environment::kVendor` to `Vendor::k`.
//!   - Moved `FormatOptions::kFlag` to `FormatFlags::k` and `DiagnosticOptions::k` (Compiler diagnostics flags).
//!   - Moved `FormatOptions::kIndentation` to `FormatIndentationGroup::k`.
//!   - Moved `FuncFrame::kAttr` to `FuncAttributes::k`.
//!   - Moved `Globals::kReset` to `ResetPolicy::k`.
//!   - Moved `InstDB::kAvx512Flag` to `InstDB::Avx512Flags::k`.
//!   - Moved `InstDB::kFlag` to `InstDB::InstFlags::k`.
//!   - Moved `InstDB::kMemFlag` to `InstDB::OpFlags::kMem`.
//!   - Moved `InstDB::kMode` to `InstDB::Mode::k`.
//!   - Moved `InstDB::kOpFlag` to `InstDB::OpFlags::k{OpType}...`.
//!   - Moved `JitAllocator::kOption` to `JitAllocatorOptions::k`.
//!   - Moved `Label::kType` to `LabelType::k`.
//!   - Moved `Operand::kOpType` to `OperandType::k`.
//!   - Moved `OpRWInfo::kFlag` to `OpRWFlags::k`.
//!   - Moved `Type::kId` to `TypeId::k`.
//!   - Moved `VirtMem::k` to `VirtMem::MemoryFlags::k`.
//!
//! ### Changes committed at 2020-05-30
//!
//! AsmJit has been cleaned up significantly, many todo items have been fixed and many functions and classes have
//! been redesigned, some in an incompatible way.
//!
//! Core changes:
//!
//!   - `Imm` operand has now only `Imm::value()` and `Imm::valueAs()` functions that return its value content,
//!     and `Imm::setValue()` function that sets the content. Functions like `setI8()`, `setU8()` were deprecated.
//!
//!     Old functions were deprecated, but code using them should still compile.
//!
//!   - `ArchInfo` has been replaced with `Environment`. Environment provides more details about the architecture,
//!     but drops some properties that were used by arch info - `gpSize(`) and `gpCount()`. `gpSize()` can be replaced
//!     with `registerSize()` getter, which returns a native register size of the architecture the environment uses.
//!     However, `gpCount()` was removed - at the moment `ArchTraits` can be used to access such properties.
//!
//!     Some other functions were renamed, like `ArchInfo::isX86Family()` is now `Environment::isFamilyX86()`, etc.
//!     The reason for changing the order was support for more properties and all the accessors now start with the
//!     type of the property, like `Environment::isPlatformWindows()`.
//!
//!     This function causes many other classes to provide `environment()` getter instead of `archInfo()` getter.
//!     In addition, AsmJit now uses `arch()` to get an architecture instead of `archId()`. `ArchInfo::kIdXXX` was
//!     renamed to `Environment::kArchXXX`.
//!
//!     Some functions were deprecated, some removed...
//!
//!   - `CodeInfo` has been removed in favor of `Environment`. If you used `CodeInfo` to set architecture and base
//!     address, this is now possible with `Environment` and setting base address explicitly by `CodeHolder::init()`
//!     - the first argument is `Environment`, and the second argument is base address, which defaults to
//!     `Globals::kNoBaseAddress`.
//!
//!     CodeInfo class was deprecated, but the code using it should still compile with warnings.
//!
//!   - `CallConv` has been updated to offer a more unified way of representing calling conventions - many calling
//!     conventions were abstracted to follow standard naming like `CallConvId::kCDecl` or `CallConvId::kStdCall`.
//!
//!     This change means that other APIs like `FuncDetail::init()` now require both, calling convention and target
//!     `Environment`.
//!
//!   - `Logging` namespace has been renamed to `Formatter`, which now provides general functionality for formatting
//!     in AsmJit.
//!
//!     Logging namespace should still work, but its use is deprecated. Unfortunately this will be without deprecation
//!     warnings, so make sure you don't use it.
//!
//!   - `Data64`, `Data128`, and `Data256` structs were deprecated and should no longer be used. There is no replacement,
//!     AsmJit users should simply create their own structures if they need them or use the new repeated embed API in
//!     emitters, see `BaseEmitter::embedDataArray()`.
//!
//! Emitter changes:
//!
//!   - `BaseEmitter::emit()` function signature has been changed to accept 3 operands by reference and the rest 3
//!     operands as a continuous array. This change is purely cosmetic and shouldn't affect users as emit() has many
//!     overloads that dispatch to the right function.
//!
//!   - `x86::Emitter` (Assembler, Builder, Compiler) deprecates embed utilities like `dint8()`, `duint8()`, `duint16()`,
//!     `dxmm()`, etc... in favor of a new and more powerful `BaseEmitter::embedDataArray()`. This function also allows
//!     emitting repeated values and/or patterns, which is used by helpers `BaseEmitter::embedUInt8()`, and others...
//!
//!   - Validation is now available through `BaseEmitter::DiagnosticOptions`, which can be enabled/disabled through
//!     `BaseEmitter::addDiagnosticOptions()` and `BaseEmitter::clearDiagnosticOptions()`, respectively. Validation
//!     options now separate between encoding and Builder/Compiler so it's possible to choose the granularity required.
//!
//! Builder changes:
//!
//!   - Internal functions for creating nodes were redesigned. They now accept a pointer to the node created as
//!     a first parameter. These changes should not affect AsmJit users as these functions were used internally.
//!
//! Compiler changes:
//!
//!   - `FuncCallNode` has been renamed to `InvokeNode`. Additionally, function calls should now use
//!     `x86::Compiler::invoke()` instead of `call()`. The reason behind this is to remove the confusion between a
//!     `call` instruction and AsmJit's `call()` intrinsic, which is now `invoke()`.
//!
//!   - Creating new nodes also changed. Now the preferred way of invoking a function is to call
//!     `x86::Compiler::invoke()` where the first argument is `InvokeNode**`. The function now returns an error and
//!     would call `ErrorHandler` in case of a failure. Error handling was unspecified in the past - the function was
//!     marked noexcept, but called error handler, which could throw.
//!
//!     The reason behind this change is to make the API consistent with other changes and to also make it possible
//!     to inspect the possible error. In  the previous API it returned a new node or `nullptr` in case of error,
//!     which the user couldn't inspect unless there was an attached `ErrorHandler`.
//!
//! Samples:
//!
//! ```
//! #include <asmjit/x86.h>
//!
//! using namespace asmjit;
//!
//! // The basic setup of JitRuntime and CodeHolder changed, use environment()
//! // instead of codeInfo().
//! void basicSetup() {
//!   JitRuntime rt;
//!   CodeHolder code(rt.environment());
//! }
//!
//! // Calling a function (Compiler) changed - use invoke() instead of call().
//! void functionInvocation(x86::Compiler& cc) {
//!   InvokeNode* invokeNode;
//!   cc.invoke(&invokeNode, targetOperand, FuncSignature::build<...>(...));
//! }
//! ```


//! \defgroup asmjit_core Core
//! \brief Globals, code storage, and emitter interface.
//!
//! ### Overview
//!
//! AsmJit library uses \ref CodeHolder to hold code during code generation and emitters inheriting from \ref
//! BaseEmitter to emit code. CodeHolder uses containers to manage its data:
//!
//!   - \ref Section - stores information about a code or data section.
//!   - \ref CodeBuffer - stores actual code or data, part of \ref Section.
//!   - \ref LabelEntry - stores information about a label - its name, offset, section where it belongs to, and
//!     other bits.
//!   - \ref LabelLink - stores information about yet unbound label, which was  already used by the assembler.
//!   - \ref RelocEntry - stores information about a relocation.
//!   - \ref AddressTableEntry - stores information about an address, which was used in a jump or call. Such
//!     address may need relocation.
//!
//! To generate code you would need to instantiate at least the following classes:
//!
//!   - \ref CodeHolder - to hold code during code generation.
//!   - \ref BaseEmitter - to emit code into \ref CodeHolder.
//!   - \ref Target (optional) - most likely \ref JitRuntime to keep the generated code in executable memory. \ref
//!     Target can be customized by inheriting from it.
//!
//! There are also other core classes that are important:
//!
//!   - \ref Environment - describes where the code will run. Environment brings the concept of target triples or
//!     tuples into AsmJit, which means that users can specify target architecture, platform, and ABI.
//!   - \ref TypeId - encapsulates lightweight type functionality that can be used to describe primitive and vector
//!     types. Types are used by higher level utilities, for example by \ref asmjit_function and \ref asmjit_compiler.
//!   - \ref CpuInfo - encapsulates CPU information - stores both CPU information and CPU features described by \ref
//!     CpuFeatures.
//!
//! AsmJit also provides global constants:
//!
//!   - \ref Globals - namespace that provides global constants.
//!   - \ref ByteOrder - byte-order constants and functionality.
//!
//! \note CodeHolder examples use \ref x86::Assembler as abstract interfaces cannot be used to generate code.
//!
//! \section code_holder_and_emitters CodeHolder & Emitters
//!
//! The example below shows how the mentioned classes interact to generate X86 code:
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! // Signature of the generated function.
//! typedef int (*Func)(void);
//!
//! int main() {
//!   JitRuntime rt;                    // Runtime specialized for JIT code execution.
//!
//!   CodeHolder code;                  // Holds code and relocation information.
//!   code.init(rt.environment(),       // Initialize code to match the JIT environment.
//!             rt.cpuFeatures());
//!
//!   x86::Assembler a(&code);          // Create and attach x86::Assembler to code.
//!   a.mov(x86::eax, 1);               // Move one to eax register.
//!   a.ret();                          // Return from function.
//!   // ===== x86::Assembler is no longer needed from here and can be destroyed =====
//!
//!   Func fn;                          // Holds address to the generated function.
//!   Error err = rt.add(&fn, &code);   // Add the generated code to the runtime.
//!   if (err) return 1;                // Handle a possible error returned by AsmJit.
//!   // ===== CodeHolder is no longer needed from here and can be destroyed =====
//!
//!   int result = fn();                // Execute the generated code.
//!   printf("%d\n", result);           // Print the resulting "1".
//!
//!   // All classes use RAII, all resources will be released before `main()` returns,
//!   // the generated function can be, however, released explicitly if you intend to
//!   // reuse or keep the runtime alive, which you should in a production-ready code.
//!   rt.release(fn);
//!
//!   return 0;
//! }
//! ```
//!
//! The example above used \ref x86::Assembler as an emitter. AsmJit provides the following emitters that offer various
//! levels of abstraction:
//!
//!   - \ref asmjit_assembler - Low-level emitter that emits directly to \ref CodeBuffer.
//!   - \ref asmjit_builder - Low-level emitter that emits to a \ref BaseNode list.
//!   - \ref asmjit_compiler - High-level emitter that provides register allocation.
//!
//! \section targets_and_jit_runtime Targets and JitRuntime
//!
//! AsmJit's \ref Target is an interface that provides basic target abstraction. At the moment AsmJit provides only
//! one implementation called \ref JitRuntime, which as the name suggests provides JIT code target and execution
//! runtime. \ref JitRuntime provides all the necessary stuff to implement a simple JIT compiler with basic memory
//! management. It only provides \ref JitRuntime::add() and \ref JitRuntime::release() functions that are used to
//! either add code to the runtime or release it. \ref JitRuntime doesn't do any decisions on when the code should be
//! released, the decision is up to the developer.
//!
//! See more at \ref asmjit_virtual_memory group.
//!
//! ### More About Environment
//!
//! In the previous example the \ref Environment is retrieved from \ref JitRuntime. It's logical as \ref JitRuntime
//! always returns an \ref Environment that is compatible with the host. For example if your application runs on X86_64
//! CPU the \ref Environment returned will use \ref Arch::kX64 architecture in contrast to \ref Arch::kX86, which will
//! be used in 32-bit mode on an X86 target.
//!
//! AsmJit allows to setup the \ref Environment manually and to select a different architecture and ABI when necessary.
//! So let's do something else this time, let's always generate a 32-bit code and print its binary representation. To
//! do that, we can create our own \ref Environment and initialize it to \ref Arch::kX86.
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! int main(int argc, char* argv[]) {
//!   using namespace asmjit::x86;
//!
//!   // Create a custom environment initialized to 32-bit X86 architecture.
//!   Environment env;
//!   env.setArch(Arch::kX86);
//!
//!   CodeHolder code;                  // Create a CodeHolder.
//!   code.init(env);                   // Initialize CodeHolder with custom environment.
//!
//!   // Generate a 32-bit function that sums 4 floats and looks like:
//!   //   void func(float* dst, const float* a, const float* b)
//!   x86::Assembler a(&code);          // Create and attach x86::Assembler to `code`.
//!
//!   a.mov(eax, dword_ptr(esp, 4));    // Load the destination pointer.
//!   a.mov(ecx, dword_ptr(esp, 8));    // Load the first source pointer.
//!   a.mov(edx, dword_ptr(esp, 12));   // Load the second source pointer.
//!
//!   a.movups(xmm0, ptr(ecx));         // Load 4 floats from [ecx] to XMM0.
//!   a.movups(xmm1, ptr(edx));         // Load 4 floats from [edx] to XMM1.
//!   a.addps(xmm0, xmm1);              // Add 4 floats in XMM1 to XMM0.
//!   a.movups(ptr(eax), xmm0);         // Store the result to [eax].
//!   a.ret();                          // Return from function.
//!
//!   // We have no Runtime this time, it's on us what we do with the code.
//!   // CodeHolder stores code in Section, which provides some basic properties
//!   // and CodeBuffer structure. We are interested in section's CodeBuffer.
//!   //
//!   // NOTE: The first section is always '.text', it can be retrieved by
//!   // code.sectionById(0) or simply by code.textSection().
//!   CodeBuffer& buffer = code.textSection()->buffer();
//!
//!   // Print the machine-code generated or do something else with it...
//!   //   8B4424048B4C24048B5424040F28010F58010F2900C3
//!   for (size_t i = 0; i < buffer.length; i++)
//!     printf("%02X", buffer.data[i]);
//!
//!   return 0;
//! }
//! ```
//!
//! \section explicit_code_relocation Explicit Code Relocation
//!
//! In addition to \ref Environment, \ref CodeHolder can be configured to specify a base-address (or a virtual base
//! address in a linker terminology), which could be static (useful when you know the location where the target's
//! machine code will be) or dynamic. AsmJit assumes dynamic base-address by default and relocates the code held by
//! \ref CodeHolder to a user provided address on-demand. To be able to relocate to a user provided address it needs
//! to store some information about relocations, which is represented by \ref RelocEntry. Relocation entries are only
//! required if you call external functions from the generated code that cannot be encoded by using a 32-bit
//! displacement (64-bit displacements are not provided by aby supported architecture).
//!
//! There is also a concept called \ref LabelLink - label link is a lightweight data structure that doesn't have any
//! identifier and is stored in \ref LabelEntry as a single-linked list. Label link represents either unbound yet used
//! label and cross-sections links (only relevant to code that uses multiple sections). Since crossing sections is
//! something that cannot be resolved immediately these links persist until offsets of these sections are assigned and
//! until \ref CodeHolder::resolveUnresolvedLinks() is called. It's an error if you end up with code that has
//! unresolved label links after flattening. You can verify it by calling \ref CodeHolder::hasUnresolvedLinks(), which
//! inspects the value returned by \ref CodeHolder::unresolvedLinkCount().
//!
//! AsmJit can flatten code that uses multiple sections by assigning each section an incrementing offset that respects
//! its alignment. Use \ref CodeHolder::flatten() to do that. After the sections are flattened their offsets and
//! virtual sizes are adjusted to respect each section's buffer size and alignment. The \ref
//! CodeHolder::resolveUnresolvedLinks() function must be called before relocating the code held by \ref CodeHolder.
//! You can also flatten your code manually by iterating over all sections and calculating their offsets (relative to
//! base) by your own algorithm. In that case \ref CodeHolder::flatten() should not be called, however,
//! \ref CodeHolder::resolveUnresolvedLinks() should be.
//!
//! The example below shows how to use a built-in virtual memory allocator \ref JitAllocator instead of using \ref
//! JitRuntime (just in case you want to use your own memory management) and how to relocate the generated code
//! into your own memory block - you can use your own virtual memory allocator if you prefer that, but that's OS
//! specific and not covered by the documentation.
//!
//! The following code is similar to the previous one, but implements a function working in both 32-bit and 64-bit
//! environments:
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! typedef void (*SumIntsFunc)(int* dst, const int* a, const int* b);
//!
//! int main() {
//!   // Create a custom environment that matches the current host environment.
//!   Environment env = Environment::host();
//!   CpuFeatures cpuFeatures = CpuInfo::host().features();
//!
//!   CodeHolder code;                  // Create a CodeHolder.
//!   code.init(env, cpuFeatures);      // Initialize CodeHolder with environment.
//!
//!   x86::Assembler a(&code);          // Create and attach x86::Assembler to `code`.
//!
//!   // Signature: 'void func(int* dst, const int* a, const int* b)'.
//!   x86::Gp dst;
//!   x86::Gp src_a;
//!   x86::Gp src_b;
//!
//!   // Handle the difference between 32-bit and 64-bit calling conventions
//!   // (arguments passed through stack vs. arguments passed by registers).
//!   if (env.is32Bit()) {
//!     dst   = x86::eax;
//!     src_a = x86::ecx;
//!     src_b = x86::edx;
//!     a.mov(dst  , x86::dword_ptr(x86::esp, 4));
//!     a.mov(src_a, x86::dword_ptr(x86::esp, 8));
//!     a.mov(src_b, x86::dword_ptr(x86::esp, 12));
//!   }
//!   else {
//!     if (env.isPlatformWindows()) {
//!       dst   = x86::rcx;             // First argument  (destination pointer).
//!       src_a = x86::rdx;             // Second argument (source 'a' pointer).
//!       src_b = x86::r8;              // Third argument  (source 'b' pointer).
//!     }
//!     else {
//!       dst   = x86::rdi;             // First argument  (destination pointer).
//!       src_a = x86::rsi;             // Second argument (source 'a' pointer).
//!       src_b = x86::rdx;             // Third argument  (source 'b' pointer).
//!     }
//!   }
//!
//!   a.movdqu(x86::xmm0, x86::ptr(src_a)); // Load 4 ints from [src_a] to XMM0.
//!   a.movdqu(x86::xmm1, x86::ptr(src_b)); // Load 4 ints from [src_b] to XMM1.
//!   a.paddd(x86::xmm0, x86::xmm1);        // Add 4 ints in XMM1 to XMM0.
//!   a.movdqu(x86::ptr(dst), x86::xmm0);   // Store the result to [dst].
//!   a.ret();                              // Return from function.
//!
//!   // Even when we didn't use multiple sections AsmJit could insert one section
//!   // called '.addrtab' (address table section), which would be filled by data
//!   // required by relocations (absolute jumps and calls). You can omit this code
//!   // if you are 100% sure your code doesn't contain multiple sections and
//!   // such relocations. You can use `CodeHolder::hasAddressTable()` to verify
//!   // whether the address table section does exist.
//!   code.flatten();
//!   code.resolveUnresolvedLinks();
//!
//!   // After the code was generated it can be relocated manually to any memory
//!   // location, however, we need to know it's size before we perform memory
//!   // allocation. `CodeHolder::codeSize()` returns the worst estimated code
//!   // size in case that relocations are not possible without trampolines (in
//!   // that case some extra code at the end of the current code buffer is
//!   // generated during relocation).
//!   size_t estimatedSize = code.codeSize();
//!
//!   // Instead of rolling up our own memory allocator we can use the one AsmJit
//!   // provides. It's decoupled so you don't need to use `JitRuntime` for that.
//!   JitAllocator allocator;
//!
//!   // Allocate an executable virtual memory and handle a possible failure.
//!   JitAllocator::Span span;
//!   Error err = allocator.alloc(span, estimatedSize);
//!
//!   if (err != kErrorOk) { // <- NOTE: This must be checked, always!
//!     return 0;
//!   }
//!
//!   // Now relocate the code to the address provided by the memory allocator.
//!   // Please note that this DOESN'T COPY anything to it. This function will
//!   // store the address in CodeHolder and use relocation entries to patch
//!   // the existing code in all sections to respect the base address provided.
//!   code.relocateToBase((uint64_t)span.rx());
//!
//!   // This is purely optional. There are cases in which the relocation can omit
//!   // unneeded data, which would shrink the size of address table. If that
//!   // happened the codeSize returned after relocateToBase() would be smaller
//!   // than the originally `estimatedSize`.
//!   size_t codeSize = code.codeSize();
//!
//!   // This will copy code from all sections to `p`. Iterating over all sections
//!   // and calling `memcpy()` would work as well, however, this function supports
//!   // additional options that can be used to also zero pad sections' virtual
//!   // size, etc.
//!   //
//!   // With some additional features, copyFlattenData() does roughly the following:
//!   //
//!   // allocator.write([&](JitAllocator::Span& span) {
//!   //   for (Section* section : code.sections()) {
//!   //     uint8_t* p = (uint8_t*)span.rw() + section->offset();
//!   //     memcpy(p, section->data(), section->bufferSize());
//!   //   }
//!   // }
//!   allocator.write([&](JitAllocator::Span& span) {
//!     code.copyFlattenedData(span.rw(), codeSize, CopySectionFlags::kPadSectionBuffer);
//!   });
//!
//!   // Execute the generated function.
//!   int inA[4] = { 4, 3, 2, 1 };
//!   int inB[4] = { 1, 5, 2, 8 };
//!   int out[4];
//!
//!   // This code uses AsmJit's ptr_as_func<> to cast between void* and SumIntsFunc.
//!   SumIntsFunc fn = ptr_as_func<SumIntsFunc>(span.rx());
//!   fn(out, inA, inB);
//!
//!   // Prints {5 8 4 9}
//!   printf("{%d %d %d %d}\n", out[0], out[1], out[2], out[3]);
//!
//!   // Release `fn` is it's no longer needed. It will be destroyed with 'vm'
//!   // instance anyway, but it's a good practice to release it explicitly
//!   // when you know that the function will not be needed anymore.
//!   allocator.release(fn);
//!
//!   return 0;
//! }
//! ```
//!
//! If you know the base-address in advance (before the code generation) it can be passed as a second argument to
//! \ref CodeHolder::init(). In that case the Assembler will know the absolute position of each instruction and
//! would be able to use it during instruction encoding to prevent relocations where possible. The following example
//! shows how to configure the base address:
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! void initializeCodeHolder(CodeHolder& code) {
//!   Environment env = Environment::host();
//!   CpuFeatures cpuFeatures = CpuInfo::host().features();
//!   uint64_t baseAddress = uint64_t(0x1234);
//!
//!   // initialize CodeHolder with environment and custom base address.
//!   code.init(env, cpuFeatures, baseAddress);
//! }
//! ```
//!
//! \section labels Label Offsets and Links
//!
//! When a label that is not yet bound is used by the Assembler, it creates a \ref LabelLink, which is then added to
//! a \ref LabelEntry. These links are also created if a label is used in a different section than in which it was
//! bound. Let's examine some functions that can be used to check whether there are any unresolved links.
//!
//! ```
//! #include <asmjit/core.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! void labelLinksExample(CodeHolder& code, const Label& label) {
//!   // Tests whether the `label` is bound.
//!   bool isBound = code.isLabelBound(label);
//!   printf("Label %u is %s\n", label.id(), isBound ? "bound" : "not bound");
//!
//!   // Returns true if the code contains either referenced, but unbound
//!   // labels, or cross-section label links that are not resolved yet.
//!   bool hasUnresolved = code.hasUnresolvedLinks();  // Boolean answer.
//!   size_t nUnresolved = code.unresolvedLinkCount(); // Count of unresolved links.
//!
//!   printf("Number of unresolved links: %zu\n", nUnresolved);
//! }
//! ```
//!
//! There is no function that would return the number of unbound labels as this is completely unimportant from
//! CodeHolder's perspective. If a label is not used then it doesn't matter whether it's bound or not, only actually
//! used labels matter. After a Label is bound it's possible to query its offset relative to the start of the
//! section where it was bound:
//!
//! ```
//! #include <asmjit/core.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! void labelOffsetExample(CodeHolder& code, const Label& label) {
//!   // Label offset is known after it's bound. The offset provided is relative
//!   // to the start of the section, see below for alternative. If the given
//!   // label is not bound the offset returned will be zero. It's recommended
//!   // to always check whether the label is bound before using its offset.
//!   uint64_t sectionOffset = code.labelOffset(label);
//!   printf("Label offset relative to section: %llu\n", (unsigned long long)sectionOffset);
//!
//!   // If you use multiple sections and want the offset relative to the base.
//!   // NOTE: This function expects that the section has already an offset and
//!   // the label-link was resolved (if this is not true you will still get an
//!   // offset relative to the start of the section).
//!   uint64_t baseOffset = code.labelOffsetFromBase(label);
//!   printf("Label offset relative to base: %llu\n", (unsigned long long)baseOffset);
//! }
//! ```
//!
//! \section code_sections Code & Data Sections
//!
//! AsmJit allows to create multiple sections within the same \ref CodeHolder. A test-case
//! [asmjit_test_x86_sections.cpp](https://github.com/asmjit/asmjit/blob/master/test/asmjit_test_x86_sections.cpp)
//! can be used as a reference point although the following example should also provide a useful insight:
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! void sectionsExample(CodeHolder& code) {
//!   // Text section is always provided as the first section.
//!   Section* text = code.textSection(); // or code.sectionById(0);
//!
//!   // To create another section use CodeHolder::newSection().
//!   Section* data;
//!   Error err = code.newSection(&data,
//!     ".data",                // Section name
//!     SIZE_MAX,               // Name length if the name is not null terminated (or SIZE_MAX).
//!     SectionFlags::kNone,    // Section flags, see SectionFlags.
//!     8,                      // Section alignment, must be power of 2.
//!     0);                     // Section order value (optional, default 0).
//!
//!   // When you switch sections in Assembler, Builder, or Compiler the cursor
//!   // will always move to the end of that section. When you create an Assembler
//!   // the cursor would be placed at the end of the first (.text) section, which
//!   // is initially empty.
//!   x86::Assembler a(&code);
//!   Label L_Data = a.newLabel();
//!
//!   a.mov(x86::eax, x86::ebx); // Emits in .text section.
//!
//!   a.section(data);           // Switches to the end of .data section.
//!   a.bind(L_Data);            // Binds label in this .data section
//!   a.db(0x01);                // Emits byte in .data section.
//!
//!   a.section(text);           // Switches to the end of .text section.
//!   a.add(x86::ebx, x86::eax); // Emits in .text section.
//!
//!   // References a label in .text section, which was bound in .data section.
//!   // This would create a LabelLink even when the L_Data is already bound,
//!   // because the reference crosses sections. See below...
//!   a.lea(x86::rsi, x86::ptr(L_Data));
//! }
//! ```
//!
//! The last line in the example above shows that a LabelLink would be created even for bound labels that cross
//! sections. In this case a referenced label was bound in another section, which means that the link couldn't be
//! resolved at that moment. If your code uses sections, but you wish AsmJit to flatten these sections (you don't
//! plan to flatten them manually) then there is an API for that.
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! // ... (continuing the previous example) ...
//! void sectionsExampleContinued(CodeHolder& code) {
//!   // Suppose we have some code that contains multiple sections and
//!   // we would like to flatten it by using AsmJit's built-in API:
//!   Error err = code.flatten();
//!   if (err) {
//!     // There are many reasons it can fail, so always handle a possible error.
//!     printf("Failed to flatten the code: %s\n", DebugUtils::errorAsString(err));
//!     exit(1);
//!   }
//!
//!   // After flattening all sections would contain assigned offsets
//!   // relative to base. Offsets are 64-bit unsigned integers so we
//!   // cast them to `size_t` for simplicity. On 32-bit targets it's
//!   // guaranteed that the offset cannot be greater than `2^32 - 1`.
//!   printf("Data section offset %zu", size_t(data->offset()));
//!
//!   // The flattening doesn't resolve unresolved label links, this
//!   // has to be done manually as flattening can be done separately.
//!   err = code.resolveUnresolvedLinks();
//!   if (err) {
//!     // This is the kind of error that should always be handled...
//!     printf("Failed to resolve label links: %s\n", DebugUtils::errorAsString(err));
//!     exit(1);
//!   }
//!
//!   if (code.hasUnresolvedLinks()) {
//!     // This would mean either unbound label or some other issue.
//!     printf("The code has %zu unbound labels\n", code.unresolvedLinkCount());
//!     exit(1);
//!   }
//! }
//! ```


//! \defgroup asmjit_assembler Assembler
//! \brief Assembler interface and operands.
//!
//! ### Overview
//!
//! AsmJit's Assembler is used to emit machine code directly into a \ref CodeBuffer. In general, code generation
//! with assembler requires the knowledge of the following:
//!
//!   - \ref BaseAssembler and architecture-specific assemblers:
//!     - \ref x86::Assembler - Assembler implementation targeting X86 and X86_64 architectures.
//!     - \ref a64::Assembler - Assembler implementation targeting AArch64 architecture.
//!   - \ref Operand and its variations:
//!     - \ref BaseReg - Base class for a register operand, inherited by:
//!        - \ref x86::Reg - Register operand specific to X86 and X86_64 architectures.
//!        - \ref arm::Reg - Register operand specific to AArch64 architecture.
//!     - \ref BaseMem - Base class for a memory operand, inherited by:
//!        - \ref x86::Mem - Memory operand specific to X86 architecture.
//!        - \ref arm::Mem - Memory operand specific to AArch64 architecture.
//!     - \ref Imm - Immediate (value) operand.
//!     - \ref Label - Label operand.
//!
//! \note Assembler examples use \ref x86::Assembler as abstract interfaces cannot be used to generate code.
//!
//! \section operand_basics Operand Basics
//!
//! Let's start with operands. \ref Operand is a data structure that defines a data layout of any operand. It can be
//! inherited, but any class inheriting it cannot add any members to it, only the existing layout can be reused.
//! AsmJit allows to construct operands dynamically, to store them, and to query a complete information about them
//! at run-time. Operands are small (always 16 bytes per \ref Operand) and can be copied and passed by value. Please
//! never allocate individual operands dynamically by using a `new` keyword - it would work, but then you would have
//! to be responsible for deleting such operands. In AsmJit operands are always part of some other data structures
//! like \ref InstNode, which is part of \ref asmjit_builder tool.
//!
//! Operands contain only identifiers, but not pointers to any code-generation data. For example \ref Label operand
//! only provides label identifier, but not a pointer to \ref LabelEntry structure. In AsmJit such IDs are used to
//! link stuff together without having to deal with pointers.
//!
//! AsmJit's operands all inherit from a base class called \ref Operand. Operands have the following properties that
//! are commonly accessible by getters and setters:
//!
//!   - \ref Operand - Base operand, which only provides accessors that are common to all operand types.
//!   - \ref BaseReg - Describes either physical or virtual register. Physical registers have id that matches the
//!     target's machine id directly whereas virtual registers must be allocated into physical registers by a register
//!     allocator pass. Register operand provides:
//!     - Register Type (\ref RegType) - Unique id that describes each possible register provided by the target
//!       architecture - for example X86 backend provides general purpose registers (GPB-LO, GPB-HI, GPW, GPD, and GPQ)
//!       and all types of other registers like K, MM, BND, XMM, YMM, ZMM, and TMM.
//!     - Register Group (\ref RegGroup) - Groups multiple register types under a single group - for example all
//!       general-purpose registers (of all sizes) on X86 are part of \ref RegGroup::kGp and all SIMD registers
//!      (XMM, YMM, ZMM) are part of \ref RegGroup::kVec.
//!     - Register Size - Contains the size of the register in bytes. If the size depends on the mode (32-bit vs
//!       64-bit) then generally the higher size is used (for example RIP register has size 8 by default).
//!     - Register Id - Contains physical or virtual id of the register.
//!   - \ref BaseMem - Used to reference a memory location. Memory operand provides:
//!     - Base Register - A base register type and id (physical or virtual).
//!     - Index Register - An index register type and id (physical or virtual).
//!     - Offset - Displacement or absolute address to be referenced (32-bit if base register is used and 64-bit if
//!       base register is not used).
//!     - Flags that can describe various architecture dependent information (like scale and segment-override on X86).
//!   - \ref Imm - Immediate values are usually part of instructions (encoded within the instruction itself) or data.
//!   - \ref Label - used to reference a location in code or data. Labels must be created by the \ref BaseEmitter or
//!     by \ref CodeHolder. Each label has its unique id per \ref CodeHolder instance.
//!
//! \section operand_manipulation Operand Manipulation
//!
//! AsmJit allows to construct operands dynamically, to store them, and to query a complete information about them at
//! run-time. Operands are small (always 16 bytes per `Operand`) and should be always copied (by value) if you intend
//! to store them (don't create operands by using `new` keyword, it's not recommended). Operands are safe to be passed
//! to `memcpy()` and `memset()`, which becomes handy when working with arrays of operands. If you set all members of
//! an \ref Operand to zero the operand would become NONE operand, which is the same as a default constructed Operand.
//!
//! The example below illustrates how operands can be used and modified even without using any other code generation
//! classes. The example uses X86 architecture-specific operands.
//!
//! ```
//! #include <asmjit/x86.h>
//!
//! using namespace asmjit;
//!
//! // Registers can be copied, it's a common practice.
//! x86::Gp dstRegByValue() { return x86::ecx; }
//!
//! void usingOperandsExample(x86::Assembler& a) {
//!   // Gets `ecx` register returned by a function.
//!   x86::Gp dst = dstRegByValue();
//!   // Gets `rax` register directly from the provided `x86` namespace.
//!   x86::Gp src = x86::rax;
//!   // Constructs `r10` dynamically.
//!   x86::Gp idx = x86::gpq(10);
//!   // Constructs [src + idx] memory address - referencing [rax + r10].
//!   x86::Mem m = x86::ptr(src, idx);
//!
//!   // Examine `m`: Returns `RegType::kX86_Gpq`.
//!   m.indexType();
//!   // Examine `m`: Returns 10 (`r10`).
//!   m.indexId();
//!
//!   // Reconstruct `idx` stored in mem:
//!   x86::Gp idx_2 = x86::Gp::fromTypeAndId(m.indexType(), m.indexId());
//!
//!   // True, `idx` and idx_2` are identical.
//!   idx == idx_2;
//!
//!   // Possible - op will still be the same as `m`.
//!   Operand op = m;
//!   // True (can be casted to BaseMem or architecture-specific Mem).
//!   op.isMem();
//!
//!   // True, `op` is just a copy of `m`.
//!   m == op;
//!
//!   // Static cast is fine and valid here.
//!   static_cast<BaseMem&>(op).addOffset(1);
//!   // However, using `as<T>()` to cast to a derived type is preferred.
//!   op.as<BaseMem>().addOffset(1);
//!   // False, `op` now points to [rax + r10 + 2], which is not [rax + r10].
//!   m == op;
//!
//!   // Emitting 'mov' - type safe way.
//!   a.mov(dst, m);
//!   // Not possible, `mov` doesn't provide mov(x86::Gp, Operand) overload.
//!   a.mov(dst, op);
//!
//!   // Type-unsafe, but possible.
//!   a.emit(x86::Inst::kIdMov, dst, m);
//!   // Also possible, `emit()` is type-less and can be used with raw Operand.
//!   a.emit(x86::Inst::kIdMov, dst, op);
//! }
//! ```
//!
//! Some operands have to be created explicitly by emitters. For example labels must be created by \ref
//! BaseEmitter::newLabel(), which creates a label entry and returns a \ref Label operand with the id that refers
//! to it. Such label then can be used by emitters.
//!
//! \section memory_operands Memory Operands
//!
//! Some architectures like X86 provide a complex memory addressing model that allows to encode addresses having a
//! BASE register, INDEX register with a possible scale (left shift), and displacement (called offset in AsmJit).
//! Memory address on X86 can also specify memory segment (segment-override in X86 terminology) and some instructions
//! (gather / scatter) require INDEX to be a \ref x86::Vec register instead of a general-purpose register.
//!
//! AsmJit allows to encode and work with all forms of addresses mentioned and implemented by X86. In addition, it
//! also allows to construct absolute 64-bit memory address operands, which is only allowed in one form of 'mov'
//! instruction.
//!
//! ```
//! #include <asmjit/x86.h>
//!
//! using namespace asmjit;
//!
//! void testX86Mem() {
//!   // Makes it easier to access x86 stuff...
//!   using namespace asmjit::x86;
//!
//!   // BASE + OFFSET.
//!   Mem a = ptr(rax);                 // a = [rax]
//!   Mem b = ptr(rax, 15);             // b = [rax + 15]
//!
//!   // BASE + INDEX << SHIFT - Shift is in BITS as used by X86!
//!   Mem c = ptr(rax, rbx);            // c = [rax + rbx]
//!   Mem d = ptr(rax, rbx, 2);         // d = [rax + rbx << 2]
//!   Mem e = ptr(rax, rbx, 2, 15);     // e = [rax + rbx << 2 + 15]
//!
//!   // BASE + VM (Vector Index) (encoded as MOD+VSIB).
//!   Mem f = ptr(rax, xmm1);           // f = [rax + xmm1]
//!   Mem g = ptr(rax, xmm1, 2);        // g = [rax + xmm1 << 2]
//!   Mem h = ptr(rax, xmm1, 2, 15);    // h = [rax + xmm1 << 2 + 15]
//!
//!   // Absolute address:
//!   uint64_t addr = (uint64_t)0x1234;
//!   Mem i = ptr(addr);                // i = [0x1234]
//!   Mem j = ptr(addr, rbx);           // j = [0x1234 + rbx]
//!   Mem k = ptr(addr, rbx, 2);        // k = [0x1234 + rbx << 2]
//!
//!   // LABEL - Will be encoded as RIP (64-bit) or absolute address (32-bit).
//!   Label L = ...;
//!   Mem m = ptr(L);                   // m = [L]
//!   Mem n = ptr(L, rbx);              // n = [L + rbx]
//!   Mem o = ptr(L, rbx, 2);           // o = [L + rbx << 2]
//!   Mem p = ptr(L, rbx, 2, 15);       // p = [L + rbx << 2 + 15]
//!
//!   // RIP - 64-bit only (RIP can't use INDEX).
//!   Mem q = ptr(rip, 24);             // q = [rip + 24]
//! }
//! ```
//!
//! Memory operands can optionally contain memory size. This is required by instructions where the memory size cannot
//! be deduced from other operands, like `inc` and `dec` on X86:
//!
//! ```
//! #include <asmjit/x86.h>
//!
//! using namespace asmjit;
//!
//! void testX86Mem() {
//!   // The same as: dword ptr [rax + rbx].
//!   x86::Mem a = x86::dword_ptr(x86::rax, x86::rbx);
//!
//!   // The same as: qword ptr [rdx + rsi << 0 + 1].
//!   x86::Mem b = x86::qword_ptr(x86::rdx, x86::rsi, 0, 1);
//! }
//! ```
//!
//! Memory operands provide API that can be used to access its properties:
//!
//! ```
//! #include <asmjit/x86.h>
//!
//! using namespace asmjit;
//!
//! void testX86Mem() {
//!   // The same as: dword ptr [rax + 12].
//!   x86::Mem mem = x86::dword_ptr(x86::rax, 12);
//!
//!   mem.hasBase();                    // true.
//!   mem.hasIndex();                   // false.
//!   mem.size();                       // 4.
//!   mem.offset();                     // 12.
//!
//!   mem.setSize(0);                   // Sets the size to 0 (makes it size-less).
//!   mem.addOffset(-1);                // Adds -1 to the offset and makes it 11.
//!   mem.setOffset(0);                 // Sets the offset to 0.
//!   mem.setBase(x86::rcx);            // Changes BASE to RCX.
//!   mem.setIndex(x86::rax);           // Changes INDEX to RAX.
//!   mem.hasIndex();                   // true.
//! }
//! // ...
//! ```
//!
//! Making changes to memory operand is very comfortable when emitting loads
//! and stores:
//!
//! ```
//! #include <asmjit/x86.h>
//!
//! using namespace asmjit;
//!
//! void testX86Mem(CodeHolder& code) {
//!   x86::Assembler a(code);           // Your initialized x86::Assembler.
//!   x86::Mem mSrc = x86::ptr(eax);    // Construct [eax] memory operand.
//!
//!   // One way of emitting bunch of loads is to use `mem.adjusted()`, which
//!   // returns a new memory operand and keeps the source operand unchanged.
//!   a.movaps(x86::xmm0, mSrc);        // No adjustment needed to load [eax].
//!   a.movaps(x86::xmm1, mSrc.adjusted(16)); // Loads from [eax + 16].
//!   a.movaps(x86::xmm2, mSrc.adjusted(32)); // Loads from [eax + 32].
//!   a.movaps(x86::xmm3, mSrc.adjusted(48)); // Loads from [eax + 48].
//!
//!   // ... do something with xmm0-3 ...
//!
//!   // Another way of adjusting memory is to change the operand in-place.
//!   // If you want to keep the original operand you can simply clone it.
//!   x86::Mem mDst = mSrc.clone();     // Clone mSrc.
//!
//!   a.movaps(mDst, x86::xmm0);        // Stores xmm0 to [eax].
//!   mDst.addOffset(16);               // Adds 16 to `mDst`.
//!
//!   a.movaps(mDst, x86::xmm1);        // Stores to [eax + 16] .
//!   mDst.addOffset(16);               // Adds 16 to `mDst`.
//!
//!   a.movaps(mDst, x86::xmm2);        // Stores to [eax + 32].
//!   mDst.addOffset(16);               // Adds 16 to `mDst`.
//!
//!   a.movaps(mDst, x86::xmm3);        // Stores to [eax + 48].
//! }
//! ```
//!
//! \section examples Assembler Examples
//!
//!   - \ref x86::Assembler provides many X86/X64 examples.


//! \defgroup asmjit_builder Builder
//! \brief Builder interface, nodes, and passes.
//!
//! ### Overview
//!
//! Both \ref BaseBuilder and \ref BaseCompiler interfaces describe emitters that emit into a representation that
//! allows further processing. The code stored in such representation is completely safe to be patched, simplified,
//! reordered, obfuscated, removed, injected, analyzed, or processed some other way. Each instruction, label,
//! directive, or other building block is stored as \ref BaseNode (or derived class like \ref InstNode or \ref
//! LabelNode) and contains all the information necessary to pass that node later to the assembler.
//!
//! \ref BaseBuilder is an emitter that inherits from \ref BaseEmitter interface. It was designed to provide a maximum
//! compatibility with the existing \ref BaseAssembler emitter so users can move from assembler to builder when needed,
//! for example to implement post-processing, which is not possible with Assembler.
//!
//! \section builder_nodes Builder Nodes
//!
//! \ref BaseBuilder doesn't generate machine code directly, it uses an intermediate representation based on nodes,
//! however, it allows to serialize to \ref BaseAssembler when the code is ready to be encoded.
//!
//! There are multiple node types used by both \ref BaseBuilder and \ref BaseCompiler :
//!
//!   - Basic nodes:
//!     - \ref BaseNode - Base class for all nodes.
//!     - \ref InstNode - Represents an instruction node.
//!     - \ref AlignNode - Represents an alignment directive (.align).
//!     - \ref LabelNode - Represents a location where to bound a \ref Label.
//!
//!   - Data nodes:
//!     - \ref EmbedDataNode - Represents data.
//!     - \ref EmbedLabelNode - Represents \ref Label address embedded as data.
//!     - \ref EmbedLabelDeltaNode - Represents a difference of two labels embedded in data.
//!     - \ref ConstPoolNode - Represents a constant pool data embedded as data.
//!
//!   - Informative nodes:
//!     - \ref CommentNode - Represents a comment string, doesn't affect code generation.
//!     - \ref SentinelNode - A marker that can be used to remember certain position in code or data, doesn't affect
//!       code generation. Used by \ref FuncNode to mark the end of a function.
//!
//!   - Other nodes are provided by \ref asmjit_compiler infrastructure.
//!
//! \section builder_examples Examples
//!
//!   - \ref x86::Builder - Builder implementation targeting X86 and X86_64 architectures.
//!   - \ref a64::Builder - Builder implementation targeting AArch64 architecture.


//! \defgroup asmjit_compiler Compiler
//! \brief Compiler interface.
//!
//! ### Overview
//!
//! \ref BaseCompiler is a high-level interface, which provides register allocation and support for defining and
//! invoking functions, built on top of \ref BaseBuilder interface At the moment it's the easiest way of generating
//! code in AsmJit as most architecture and OS specifics is properly abstracted and handled by AsmJit automatically.
//! However, abstractions also mean restrictions, which means that \ref BaseCompiler has more limitations than \ref
//! BaseAssembler or \ref BaseBuilder.
//!
//! Since \ref BaseCompiler provides register allocation it also establishes the concept of functions - a function
//! in Compiler sense is a unit in which virtual registers are allocated into physical registers by the register
//! allocator. In addition, it enables to use such virtual registers in function invocations.
//!
//! \ref BaseCompiler automatically handles function calling conventions. It's still architecture dependent, but
//! makes the code generation much easies. Functions are essential; the first-step to generate some code is to define
//! a signature of the function to be generated (before generating the function body itself). Function arguments and
//! return value(s) are handled by assigning virtual registers to them. Similarly, function calls are handled the same
//! way.
//!
//! \section compiler_nodes Compiler Nodes
//!
//! \ref BaseCompiler adds some nodes that are required for function generation and invocation:
//!
//!   - \ref FuncNode - Represents a function definition.
//!   - \ref FuncRetNode - Represents a function return.
//!   - \ref InvokeNode - Represents a function invocation.
//!
//! \ref BaseCompiler also makes the use of passes (\ref Pass) and automatically adds an architecture-dependent
//! register allocator pass to the list of passes when attached to \ref CodeHolder.
//!
//! \section compiler_examples Compiler Examples
//!
//!   - \ref x86::Compiler - Compiler implementation targeting X86 and X86_64 architectures.
//!   - \ref a64::Compiler - Compiler implementation targeting AArch64 architecture.
//!
//! \section compiler_tips Compiler Tips
//!
//! Users of AsmJit have done mistakes in the past, this section should provide some useful tips for beginners:
//!
//!   - Virtual registers in compiler are bound to a single function. At the moment the implementation doesn't
//!     care whether a single virtual register is used in multiple functions, but it sees it as two independent
//!     virtual registers in that case. This means that virtual registers cannot be used to implement global
//!     variables. Global variables are basically memory addresses which functions can read from and write to,
//!     and they have to be implemented in the same way.
//!
//!   - Compiler provides a useful debugging functionality, which can be turned on through \ref FormatFlags. Use
//!     \ref Logger::addFlags() to turn on additional logging features when using Compiler.


//! \defgroup asmjit_function Function
//! \brief Function definitions.
//!
//! ### Overview
//!
//! AsmJit provides functionality that can be used to define function signatures and to calculate automatically
//! optimal function frame that can be used directly by a prolog and epilog insertion. This feature was exclusive
//! to AsmJit's Compiler for a very long time, but was abstracted out and is now available for all users regardless
//! of the emitter they use. The following use cases are possible:
//!
//!   - Calculate function frame before the function is generated - this is the only way available to \ref
//!     BaseAssembler users and it will be described in this section.
//!
//!   - Calculate function frame after the function is generated - this way is generally used by \ref BaseBuilder
//!     and \ref BaseCompiler emitters and this way is generally described in \ref asmjit_compiler section.
//!
//! The following concepts are used to describe and create functions in AsmJit:
//!
//!   - \ref TypeId - Type-id is an 8-bit value that describes a platform independent type as we know from C/C++.
//!     It provides abstractions for most common types like `int8_t`, `uint32_t`, `uintptr_t`, `float`, `double`,
//!     and all possible vector types to match ISAs up to AVX512. \ref TypeId was introduced originally for \ref
//!     asmjit_compiler, but it's now used by \ref FuncSignature as well.
//!
//!   - \ref CallConv - Describes a calling convention - this class contains instructions to assign registers and
//!     stack addresses to function arguments and return value(s), but doesn't specify any function signature itself.
//!     Calling conventions are architecture and OS dependent.
//!
//!   - \ref FuncSignature - Describes a function signature, for example `int func(int, int)`. FuncSignature contains
//!     a function calling convention id, return value type, and function arguments. The signature itself is platform
//!     independent and uses \ref TypeId to describe types of function arguments and function return value(s).
//!
//!   - \ref FuncDetail - Architecture and ABI dependent information that describes \ref CallConv and expanded \ref
//!     FuncSignature. Each function argument and return value is represented as \ref FuncValue that contains the
//!     original \ref TypeId enriched with additional information that specifies whether the value is passed or
//!     returned by register (and which register) or by stack. Each value also contains some other metadata that
//!     provide additional information required to handle it properly (for example whether a vector is passed
//!     indirectly by a pointer as required by WIN64 calling convention).
//!
//!   - \ref FuncFrame - Contains information about the function frame that can be used by prolog/epilog inserter
//!     (PEI). Holds call stack size size and alignment, local stack size and alignment, and various attributes that
//!     describe how prolog and epilog should be constructed. `FuncFrame` doesn't know anything about function's
//!     arguments or return values, it hold only information necessary to create a valid and ABI conforming function
//!     prologs and epilogs.
//!
//!   - \ref FuncArgsAssignment - A helper class that can be used to reassign function arguments into user specified
//!     registers. It's architecture and ABI dependent mapping from function arguments described by \ref CallConv
//!     and \ref FuncDetail into registers specified by the user.
//!
//! It's a lot of concepts where each represents one step in a function frame calculation. It can be used to create
//! function prologs, epilogs, and also to calculate information necessary to perform function calls.


//! \defgroup asmjit_logging Logging
//! \brief Logging and formatting.
//!
//! ### Overview
//!
//! The initial phase of a project that generates machine code is not always smooth. Failure cases are common not just
//! at the beginning phase, but also during the development or refactoring. AsmJit provides logging functionality to
//! address this issue. AsmJit does already a good job with function overloading to prevent from emitting unencodable
//! instructions, but it can't prevent from emitting machine code that is correct at instruction level, but doesn't
//! work when it's executed asa whole. Logging has always been an important part of AsmJit's infrastructure and looking
//! at logs can sometimes reveal code generation issues quickly.
//!
//! AsmJit provides API for logging and formatting:
//!
//!   - \ref Logger - A logger that you can pass to \ref CodeHolder and all emitters that inherit from \ref BaseEmitter.
//!
//!   - \ref FormatOptions - Formatting options that can change how instructions and operands are formatted.
//!
//!   - \ref Formatter - A namespace that provides functions that can format input data like \ref Operand, \ref BaseReg,
//!     \ref Label, and \ref BaseNode into \ref String.
//!
//! AsmJit's \ref Logger serves the following purposes:
//!
//!   - Provides a basic foundation for logging.
//!
//!   - Abstract class leaving the implementation on users. The following built-in implementations are provided for
//!     simplicity:
//!
//!     - \ref FileLogger implements logging into a standard `FILE` stream.
//!     - \ref StringLogger serializes all logs into a \ref String instance.
//!
//! AsmJit's \ref FormatOptions provides the following to customize the formatting of instructions and operands through:
//!
//!   - \ref FormatFlags
//!   - \ref FormatIndentationGroup
//!
//! \section logging Logging
//!
//! A \ref Logger is typically attached to a \ref CodeHolder, which propagates it to all attached emitters
//! automatically. The example below illustrates how to use \ref FileLogger that outputs to standard output:
//!
//! ```
//! #include <asmjit/core.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! int main() {
//!   JitRuntime rt;               // Runtime specialized for JIT code execution.
//!   FileLogger logger(stdout);   // Logger should always survive CodeHolder.
//!
//!   CodeHolder code;             // Holds code and relocation information.
//!   code.init(rt.environment(),  // Initialize code to match the JIT environment.
//!             rt.cpuFeatures());
//!   code.setLogger(&logger);     // Attach the `logger` to `code` holder.
//!
//!   // ... code as usual, everything emitted will be logged to `stdout` ...
//!   return 0;
//! }
//! ```
//!
//! If output to FILE stream is not desired it's possible to use \ref StringLogger, which concatenates everything
//! into a multi-line string:
//!
//! ```
//! #include <asmjit/core.h>
//! #include <stdio.h>
//! #include <utility>
//!
//! using namespace asmjit;
//!
//! int main() {
//!   JitRuntime rt;               // Runtime specialized for JIT code execution.
//!   StringLogger logger;         // Logger should always survive CodeHolder.
//!
//!   CodeHolder code;             // Holds code and relocation information.
//!   code.init(rt.environment(),  // Initialize code to match the JIT environment.
//!             rt.cpuFeatures());
//!   code.setLogger(&logger);     // Attach the `logger` to `code` holder.
//!
//!   // ... code as usual, logging will be concatenated to logger string  ...
//!
//!   // You can either use the string from StringLogger directly or you can
//!   // move it. Logger::data() returns its content as null terminated char[].
//!   printf("Logger content: %s\n", logger.data());
//!
//!   // It can be moved into your own string like this:
//!   String content = std::move(logger.content());
//!   printf("The same content: %s\n", content.data());
//!
//!   return 0;
//! }
//! ```
//!
//! \section formatting Formatting
//!
//! AsmJit uses \ref Formatter to format inputs that are then passed to \ref Logger. Formatting is public and can be
//! used by AsmJit users as well. The most important thing to know regarding formatting is that \ref Formatter always
//! appends to the output string, so it can be used to build complex strings without having to concatenate
//! intermediate strings.
//!
//! The first example illustrates how to format operands:
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! void logOperand(Arch arch, const Operand_& op) {
//!   // The emitter is optional (named labels and virtual registers need it).
//!   BaseEmitter* emitter = nullptr;
//!
//!   // No flags by default.
//!   FormatFlags formatFlags = FormatFlags::kNone;
//!
//!   StringTmp<128> sb;
//!   Formatter::formatOperand(sb, formatFlags, emitter, arch, op);
//!   printf("%s\n", sb.data());
//! }
//!
//! void formattingExample() {
//!   using namespace x86;
//!
//!   // Architecture is not part of operand, it must be passed explicitly.
//!   // Format flags. We pass it explicitly also to 'logOperand' to make
//!   // compatible with what AsmJit normally does.
//!   Arch arch = Arch::kX64;
//!
//!   logOperand(arch, rax);                    // Prints 'rax'.
//!   logOperand(arch, ptr(rax, rbx, 2));       // Prints '[rax + rbx * 4]`.
//!   logOperand(arch, dword_ptr(rax, rbx, 2)); // Prints 'dword [rax + rbx * 4]`.
//!   logOperand(arch, imm(42));                // Prints '42'.
//! }
//! ```
//!
//! Next example illustrates how to format whole instructions:
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//! #include <utility>
//!
//! using namespace asmjit;
//!
//! template<typename... Args>
//! void logInstruction(Arch arch, const BaseInst& inst, Args&&... args) {
//!   // The emitter is optional (named labels and virtual registers need it).
//!   BaseEmitter* emitter = nullptr;
//!
//!   // No flags by default.
//!   FormatFlags formatFlags = FormatFlags::kNone;
//!
//!   // The formatter expects operands in an array.
//!   Operand_ operands[] { std::forward<Args>(args)... };
//!
//!   StringTmp<128> sb;
//!   Formatter::formatInstruction(
//!     sb, formatFlags, emitter, arch, inst, operands, sizeof...(args));
//!   printf("%s\n", sb.data());
//! }
//!
//! void formattingExample() {
//!   using namespace x86;
//!
//!   // Architecture is not part of operand, it must be passed explicitly.
//!   // Format flags. We pass it explicitly also to 'logOperand' to make
//!   // compatible with what AsmJit normally does.
//!   Arch arch = Arch::kX64;
//!
//!   // Prints 'mov rax, rcx'.
//!   logInstruction(arch, BaseInst(Inst::kIdMov), rax, rcx);
//!
//!   // Prints 'vaddpd zmm0, zmm1, [rax] {1to8}'.
//!   logInstruction(arch,
//!                  BaseInst(Inst::kIdVaddpd),
//!                  zmm0, zmm1, ptr(rax)._1to8());
//!
//!   // BaseInst abstracts instruction id, instruction options, and extraReg.
//!   // Prints 'lock add [rax], rcx'.
//!   logInstruction(arch,
//!                  BaseInst(Inst::kIdAdd, InstOptions::kX86_Lock),
//!                  ptr(rax), rcx);
//!
//!   // Similarly an extra register (like AVX-512 selector) can be used.
//!   // Prints 'vaddpd zmm0 {k2} {z}, zmm1, [rax]'.
//!   logInstruction(arch,
//!                  BaseInst(Inst::kIdAdd, InstOptions::kX86_ZMask, k2),
//!                  zmm0, zmm1, ptr(rax));
//! }
//! ```
//!
//! And finally, the example below illustrates how to use a built-in function to format the content of
//! \ref BaseBuilder, which consists of nodes:
//!
//! ```
//! #include <asmjit/core.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! void formattingExample(BaseBuilder* builder) {
//!   FormatOptions formatOptions {};
//!
//!   // This also shows how temporary strings can be used.
//!   StringTmp<512> sb;
//!
//!   // FormatNodeList requires the String for output, formatting flags, which
//!   // were zero (no extra flags), and the builder instance, which we have
//!   // provided. An overloaded version also exists, which accepts begin and
//!   // and end nodes, which can be used to only format a range of nodes.
//!   Formatter::formatNodeList(sb, formatOptions, builder);
//!
//!   // You can do whatever else with the string, it's always null terminated,
//!   // so it can be passed to C functions like printf().
//!   printf("%s\n", sb.data());
//! }
//! ```


//! \defgroup asmjit_error_handling Error Handling
//! \brief Error handling.
//!
//! ### Overview
//!
//! AsmJit uses error codes to represent and return errors. Every function that can fail returns an \ref Error code.
//! Exceptions are never thrown by AsmJit itself even in extreme conditions like out-of-memory, but it's possible to
//! override \ref ErrorHandler::handleError() to throw, in that case no error will be returned and exception will be
//! thrown instead. All functions where this can happen are not marked `noexcept`.
//!
//! Errors should never be ignored, however, checking errors after each AsmJit API call would simply over-complicate
//! the whole code generation experience. \ref ErrorHandler exists to make the use of AsmJit API simpler as it allows
//! to customize how errors can be handled:
//!
//!   - Record the error and continue (the way how the error is user-implemented).
//!   - Throw an exception. AsmJit doesn't use exceptions and is completely exception-safe, but it's perfectly legal
//!     to throw an exception from the error handler.
//!   - Use plain old C's `setjmp()` and `longjmp()`. Asmjit always puts Assembler, Builder and Compiler to a
//!     consistent state before calling \ref ErrorHandler::handleError(), so `longjmp()` can be used without issues
//!     to cancel the code-generation if an error occurred. This method can be used if exception handling in your
//!     project is turned off and you still want some comfort. In most cases it should be safe as AsmJit uses \ref
//!     Zone memory and the ownership of memory it allocates always ends with the instance that allocated it. If
//!     using this approach please never jump outside the life-time of \ref CodeHolder and \ref BaseEmitter.
//!
//! \section using_error_handler Using ErrorHandler
//!
//! An example of attaching \ref ErrorHandler to \ref CodeHolder.
//!
//! ```
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! // A simple error handler implementation, extend according to your needs.
//! class MyErrorHandler : public ErrorHandler {
//! public:
//!   void handleError(Error err, const char* message, BaseEmitter* origin) override {
//!     printf("AsmJit error: %s\n", message);
//!   }
//! };
//!
//! int main() {
//!   JitRuntime rt;
//!
//!   MyErrorHandler myErrorHandler;
//!   CodeHolder code;
//!
//!   code.init(rt.environment(), rt.cpuFeatures());
//!   code.setErrorHandler(&myErrorHandler);
//!
//!   x86::Assembler a(&code);
//!   // ... code generation ...
//!
//!   return 0;
//! }
//! ```
//!
//! Useful classes in error handling group:
//!
//!   - See \ref DebugUtils that provides utilities useful for debugging.
//!   - See \ref Error that lists error codes that AsmJit uses.
//!   - See \ref ErrorHandler for more details about error handling.


//! \defgroup asmjit_instruction_db Instruction DB
//! \brief Instruction database (introspection, read/write, validation, ...).
//!
//! ### Overview
//!
//! AsmJit provides a public instruction database that can be used to query information about a complete instruction.
//! The instruction database requires the knowledge of the following:
//!
//!   - \ref BaseInst - Base instruction that contains instruction id, options, and a possible extra-register that
//!     represents either REP prefix counter or AVX-512 selector (mask).
//!
//!   - \ref Operand - Represents operands of an instruction.
//!
//! Each instruction can be then queried for the following information:
//!
//!   - \ref InstRWInfo - Read/write information of instruction and its operands (includes \ref OpRWInfo).
//!
//!   - \ref CpuFeatures - CPU features required to execute the instruction.
//!
//! In addition to query functionality AsmJit is also able to validate whether an instruction and its operands are
//! valid. This is useful for making sure that what user tries to emit is correct and it can be also used by other
//! projects that parse user input, like AsmTK project.
//!
//! \section instruction_query Instruction Queries
//!
//! The instruction query API is provided by \ref InstAPI namespace. The following queries are possible:
//!
//!   - \ref InstAPI::queryRWInfo() - queries read/write information of the given instruction and its operands.
//!     Includes also CPU flags read/written.
//!
//!   - \ref InstAPI::queryFeatures() - queries CPU features that are required to execute the given instruction. A full
//!     instruction with operands must be given as some architectures like X86 may require different features for the
//!     same instruction based on its operands.
//!
//!   - <a href="https://github.com/asmjit/asmjit/blob/master/test/asmjit_test_instinfo.cpp">asmjit_test_instinfo.cpp</a>
//!     can be also used as a reference about accessing instruction information.
//!
//! \section instruction_validation Instruction Validation
//!
//! The instruction validation API is provided by \ref InstAPI namespace in the similar fashion like the Query API,
//! however, validation can also be turned on at \ref BaseEmitter level. The following is possible:
//!
//!   - \ref InstAPI::validate() - low-level instruction validation function that is used internally by emitters
//!     if strict validation is enabled.
//!
//!   - \ref BaseEmitter::addDiagnosticOptions() - can be used to enable validation at emitter level, see \ref
//!     DiagnosticOptions.


//! \defgroup asmjit_virtual_memory Virtual Memory
//! \brief Virtual memory management.
//!
//! ### Overview
//!
//! AsmJit's virtual memory management is divided into three main categories:
//!
//!   - Low level interface that provides cross-platform abstractions for virtual memory allocation. Implemented in
//!     \ref VirtMem namespace. This API is a thin wrapper around operating system specific calls such as
//!     `VirtualAlloc()` and `mmap()` and it's intended to be used by AsmJit's higher level API. Low-level virtual
//!     memory functions can be used to allocate virtual memory, change its permissions, and to release it.
//!     Additionally, an API that allows to create dual mapping (to support hardened environments) is provided.
//!
//!   - Middle level API that is provided by \ref JitAllocator, which uses \ref VirtMem internally and offers nicer
//!     API that can be used by users to allocate executable memory conveniently. \ref JitAllocator tries to be smart,
//!     for example automatically using dual mapping or `MAP_JIT` on hardened environments.
//!
//!   - High level API that is provided by \ref JitRuntime, which implements \ref Target interface and uses \ref
//!     JitAllocator under the hood. Since \ref JitRuntime inherits from \ref Target it makes it easy to use with
//!     \ref CodeHolder. Many AsmJit examples use \ref JitRuntime for its simplicity and easy integration.
//!
//! The main difference between \ref VirtMem and \ref JitAllocator is that \ref VirtMem can only be used to allocate
//! whole pages, whereas \ref JitAllocator has `malloc()` like API that allows to allocate smaller quantities that
//! usually represent the size of an assembled function or a chunk of functions that can represent a module, for
//! example. \ref JitAllocator then tracks used space of each page it maintains. Internally, \ref JitAllocator uses
//! two bit arrays to track occupied regions in each allocated block of pages.
//!
//! \section hardened_environments Hardened Environments
//!
//! In the past, allocating virtual memory with Read+Write+Execute (RWX) access permissions was easy. However, modern
//! operating systems and runtime environments often use hardening, which typically prohibits mapping pages with both
//! Write and Execute permissions (known as the W^X policy). This presents a challenge for JIT compilers because
//! generated code for a single function is unlikely to fit in exactly N pages without leaving some space empty. To
//! accommodate this, the execution environment may need to temporarily change the permissions of existing pages to
//! read+write (RW) to insert new code into them, however, sometimes it's not possible to ensure that no thread is
//! executing code in such affected pages in a multithreaded environment, in which multiple threads may be executing
//! generated code.
//!
//! Such restrictions leave a lot of complexity on the application, so AsmJit implements a dual mapping technique to
//! make the life of AsmJit users easier. In this technique, a region of memory is mapped to two different virtual
//! addresses with different access permissions. One virtual address is mapped with read and write (RW) access, which
//! is used by the JIT compiler to write generated code. The other virtual address is mapped with read and execute (RX)
//! access, which is used by the application to execute the generated code.
//!
//! However, implementing dual mapping can be challenging because it typically requires obtaining an anonymous file
//! descriptor on most Unix-like operating systems. This file descriptor is then passed to mmap() twice to create
//! the two mappings. AsmJit handles this challenge by using system-specific techniques such as `memfd_create()` on
//! Linux, `shm_open(SHM_ANON)` on BSD, and `MAP_REMAPDUP` with `mremap()` on NetBSD. The latter approach does not
//! require a file descriptor. If none of these options are available, AsmJit uses a plain `open()` call followed by
//! `unlink()`.
//!
//! The most challenging part is actually obtaining a file descriptor that can be passed to `mmap()` with `PROT_EXEC`.
//! This is still something that may fail, for example the environment could be hardened in a way that this would
//! not be possible at all, and thus dual mapping would not work.
//!
//! Dual mapping is provided by both \ref VirtMem and \ref JitAllocator.


//! \defgroup asmjit_zone Zone Memory
//! \brief Zone memory allocator and containers.
//!
//! ### Overview
//!
//! AsmJit uses zone memory allocation (also known as Arena allocation) to allocate most of the data it uses. It's a
//! fast allocator that allows AsmJit to allocate a lot of small data structures fast and without `malloc()` overhead.
//! Since code generators and all related classes are usually short-lived this approach decreases memory usage and
//! fragmentation as arena-based allocators always allocate larger blocks of memory, which are then split into smaller
//! chunks.
//!
//! Another advantage of zone memory allocation is that since the whole library uses this strategy it's very easy to
//! deallocate everything that a particular instance is holding by simply releasing the memory the allocator holds.
//! This improves destruction time of such objects as there is no destruction at all. Long-lived objects just reset
//! its data in destructor or in their reset() member function for a future reuse. For this purpose all containers in
//! AsmJit are also zone allocated.
//!
//! \section zone_allocation Zone Allocation
//!
//!   - \ref Zone - Incremental zone memory allocator with minimum features. It can only allocate memory without the
//!     possibility to return it back to the allocator.
//!
//!   - \ref ZoneTmp - A temporary \ref Zone with some initial static storage. If the allocation requests fit the
//!     static storage allocated then there will be no dynamic memory allocation during the lifetime of \ref ZoneTmp,
//!     otherwise it would act as \ref Zone with one preallocated block on the stack.
//!
//!   - \ref ZoneAllocator - A wrapper of \ref Zone that provides the capability of returning memory to the allocator.
//!     Such memory is stored in a pool for later reuse.
//!
//! \section zone_containers Zone Allocated Containers
//!
//!   - \ref ZoneString - Zone allocated string.
//!   - \ref ZoneHash - Zone allocated hash table.
//!   - \ref ZoneTree - Zone allocated red-black tree.
//!   - \ref ZoneList - Zone allocated double-linked list.
//!   - \ref ZoneStack - Zone allocated stack.
//!   - \ref ZoneVector - Zone allocated vector.
//!   - \ref ZoneBitVector - Zone allocated vector of bits.
//!
//! \section using_zone_containers Using Zone Allocated Containers
//!
//! The most common data structure exposed by AsmJit is \ref ZoneVector. It's very similar to `std::vector`, but the
//! implementation doesn't use exceptions and uses the mentioned \ref ZoneAllocator for performance reasons. You don't
//! have to worry about allocations as you should not need to add items to AsmJit's data structures directly as there
//! should be API for all required operations.
//!
//! The following APIs in \ref CodeHolder returns \ref ZoneVector reference:
//!
//! ```
//! using namespace asmjit;
//!
//! void example(CodeHolder& code) {
//!   // Contains all emitters attached to CodeHolder.
//!   const ZoneVector<BaseEmitter*>& emitters = code.emitters();
//!
//!   // Contains all section entries managed by CodeHolder.
//!   const ZoneVector<Section*>& sections = code.sections();
//!
//!   // Contains all label entries managed by CodeHolder.
//!   const ZoneVector<LabelEntry*>& labelEntries = code.labelEntries();
//!
//!   // Contains all relocation entries managed by CodeHolder.
//!   const ZoneVector<RelocEntry*>& relocEntries = code.relocEntries();
//! }
//! ```
//!
//! \ref ZoneVector has overloaded array access operator to make it possible to access its elements through operator[].
//! Some standard functions like \ref ZoneVector::empty(), \ref ZoneVector::size(), and \ref ZoneVector::data() are
//! provided as well. Vectors are also iterable through a range-based for loop:
//!
//! ```
//! using namespace asmjit;
//!
//! void example(CodeHolder& code) {
//!   for (LabelEntry* le : code.labelEntries()) {
//!     printf("Label #%u {Bound=%s Offset=%llu}",
//!       le->id(),
//!       le->isBound() ? "true" : "false",
//!       (unsigned long long)le->offset());
//!   }
//! }
//! ```
//!
//! \section design_considerations Design Considerations
//!
//! Zone-allocated containers do not store the allocator within the container. This decision was made to reduce the
//! footprint of such containers as AsmJit tooling, especially Compiler's register allocation, may use many instances
//! of such containers to perform code analysis and register allocation.
//!
//! For example to append an item into a \ref ZoneVector it's required to pass the allocator as the first argument,
//! so it can be used in case that the vector needs a reallocation. Such function also returns an error, which must
//! be propagated to the caller.
//!
//! ```
//! using namespace asmjit
//!
//! Error example(ZoneAllocator* allocator) {
//!   ZoneVector<int> vector;
//!
//!   // Unfortunately, allocator must be provided to all functions that mutate
//!   // the vector. However, AsmJit users should never need to do this as all
//!   // manipulation should be done through public API, which takes care of
//!   // that.
//!   for (int i = 0; i < 100; i++) {
//!     ASMJIT_PROPAGATE(vector.append(allocator, i));
//!   }
//!
//!   // By default vector's destructor doesn't release anything as it knows
//!   // that its content is zone allocated. However, \ref ZoneVector::release
//!   // can be used to explicitly release the vector data to the allocator if
//!   // necessary
//!   vector.release(allocator);
//! }
//! ```
//!
//! Containers like \ref ZoneVector also provide a functionality to reserve a certain number of items before any items
//! are added to it. This approach is used internally in most places as it allows to prepare space for data that will
//! be added to some container before the data itself was created.
//!
//! ```
//! using namespace asmjit
//!
//! Error example(ZoneAllocator* allocator) {
//!   ZoneVector<int> vector;
//!
//!   ASMJIT_PROPAGATE(vector.willGrow(100));
//!   for (int i = 0; i < 100; i++) {
//!     // Cannot fail.
//!     vector.appendUnsafe(allocator, i);
//!   }
//!
//!   vector.release(allocator);
//! }
//! ```


//! \defgroup asmjit_utilities Utilities
//! \brief Utility classes and functions.
//!
//! ### Overview
//!
//! AsmJit uses and provides utility classes and functions, that can be used with AsmJit. The functionality can be
//! divided into the following topics:
//!
//! \section string_utilities String Utilities
//!
//!   - \ref String - AsmJit's string container, which is used internally and which doesn't use exceptions and has
//!     a stable layout, which is not dependent on C++ standard library.
//!
//!   - \ref StringTmp - String that can have base storage allocated on stack. The amount of storage on stack can
//!     be specified as a template parameter.
//!
//!   - \ref FixedString - Fixed string container limited up to N characters.
//!
//! \section codegen_utilities Code Generation Utilities
//!
//!   - \ref ConstPool - Constant pool used by \ref BaseCompiler, but also available to users that may find use of it.
//!
//! \section support_utilities Support Functionality Used by AsmJit
//!
//!   - \ref Support namespace provides many other utility functions and classes that are used by AsmJit, and made
//!     public.


//! \defgroup asmjit_x86 X86 Backend
//! \brief X86/X64 backend.


//! \defgroup asmjit_arm ARM Commons
//! \brief ARM commons shared between AArch32 and AArch64.


//! \defgroup asmjit_a64 AArch64 Backend
//! \brief AArch64 backend.


//! \cond INTERNAL
//! \defgroup asmjit_ra RA
//! \brief Register allocator internals.
//! \endcond

} // {asmjit}

#include "asmjit-scope-begin.h"
#include "core/archtraits.h"
#include "core/assembler.h"
#include "core/builder.h"
#include "core/codeholder.h"
#include "core/compiler.h"
#include "core/constpool.h"
#include "core/cpuinfo.h"
#include "core/emitter.h"
#include "core/environment.h"
#include "core/errorhandler.h"
#include "core/formatter.h"
#include "core/func.h"
#include "core/globals.h"
#include "core/inst.h"
#include "core/jitallocator.h"
#include "core/jitruntime.h"
#include "core/logger.h"
#include "core/operand.h"
#include "core/osutils.h"
#include "core/string.h"
#include "core/support.h"
#include "core/target.h"
#include "core/type.h"
#include "core/virtmem.h"
#include "core/zone.h"
#include "core/zonehash.h"
#include "core/zonelist.h"
#include "core/zonetree.h"
#include "core/zonestack.h"
#include "core/zonestring.h"
#include "core/zonevector.h"
#include "asmjit-scope-end.h"

#endif // ASMJIT_CORE_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/api-build_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_API_BUILD_P_H_INCLUDED
#define ASMJIT_CORE_API_BUILD_P_H_INCLUDED

#define ASMJIT_EXPORTS

// Only turn-off these warnings when building asmjit itself.
#ifdef _MSC_VER
  #ifndef _CRT_SECURE_NO_DEPRECATE
    #define _CRT_SECURE_NO_DEPRECATE
  #endif
  #ifndef _CRT_SECURE_NO_WARNINGS
    #define _CRT_SECURE_NO_WARNINGS
  #endif
#endif

// Dependencies only required for asmjit build, but never exposed through public headers.
#ifdef _WIN32
  #ifndef WIN32_LEAN_AND_MEAN
    #define WIN32_LEAN_AND_MEAN
  #endif
  #ifndef NOMINMAX
    #define NOMINMAX
  #endif
  #include <windows.h>
#else
  // Most production code is compiled with large file support, so do the same.
  #if !defined(_WIN32) && !defined(_LARGEFILE64_SOURCE)
    #define _LARGEFILE64_SOURCE 1
  #endif

  // These OSes use 64-bit API by default.
  #if defined(__APPLE__    ) || \
      defined(__HAIKU__    ) || \
      defined(__bsdi__     ) || \
      defined(__DragonFly__) || \
      defined(__FreeBSD__  ) || \
      defined(__NetBSD__   ) || \
      defined(__OpenBSD__  )
    #define ASMJIT_FILE64_API(NAME) NAME
  #else
    #define ASMJIT_FILE64_API(NAME) NAME##64
  #endif

#endif

#include "./api-config.h"

#if !defined(ASMJIT_BUILD_DEBUG) && defined(__GNUC__) && !defined(__clang__)
  #define ASMJIT_FAVOR_SIZE  __attribute__((__optimize__("Os")))
  #define ASMJIT_FAVOR_SPEED __attribute__((__optimize__("O3")))
#elif ASMJIT_CXX_HAS_ATTRIBUTE(__minsize__, 0)
  #define ASMJIT_FAVOR_SIZE __attribute__((__minsize__))
  #define ASMJIT_FAVOR_SPEED
#else
  #define ASMJIT_FAVOR_SIZE
  #define ASMJIT_FAVOR_SPEED
#endif

// Make sure '#ifdef'ed unit tests are properly highlighted in IDE.
#if !defined(ASMJIT_TEST) && defined(__INTELLISENSE__)
  #define ASMJIT_TEST
#endif

// Include a unit testing package if this is a `asmjit_test_unit` build.
#if defined(ASMJIT_TEST)
  #include "../../../test/broken.h"
#endif

#endif // ASMJIT_CORE_API_BUILD_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/api-config.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_API_CONFIG_H_INCLUDED
#define ASMJIT_CORE_API_CONFIG_H_INCLUDED

// AsmJit Library & ABI Version
// ============================

//! \addtogroup asmjit_core
//! \{

//! Makes a 32-bit integer that represents AsmJit version in `(major << 16) | (minor << 8) | patch` form.
#define ASMJIT_LIBRARY_MAKE_VERSION(major, minor, patch) ((major << 16) | (minor << 8) | (patch))

//! AsmJit library version, see \ref ASMJIT_LIBRARY_MAKE_VERSION for a version format reference.
#define ASMJIT_LIBRARY_VERSION ASMJIT_LIBRARY_MAKE_VERSION(1, 15, 0)

//! \def ASMJIT_ABI_NAMESPACE
//!
//! AsmJit ABI namespace is an inline namespace within \ref asmjit namespace.
//!
//! It's used to make sure that when user links to an incompatible version of AsmJit, it won't link. It has also
//! some additional properties as well. When `ASMJIT_ABI_NAMESPACE` is defined by the user it would override the
//! AsmJit default, which makes it possible to use multiple AsmJit libraries within a single project, totally
//! controlled by users. This is useful especially in cases in which some of such library comes from third party.
#if !defined(ASMJIT_ABI_NAMESPACE)
  #define ASMJIT_ABI_NAMESPACE v1_15
#endif // !ASMJIT_ABI_NAMESPACE

//! \}

// Global Dependencies
// ===================

#include <stdarg.h>
#include <stddef.h>
#include <stdint.h> // We really want std types as globals, not under 'std' namespace.
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include <initializer_list>
#include <limits>
#include <type_traits>
#include <utility>

#if !defined(_WIN32) && !defined(__EMSCRIPTEN__)
  #include <pthread.h>
#endif

// Build Options
// =============

// NOTE: Doxygen cannot document macros that are not defined, that's why we have to define them and then undefine
// them immediately, so it won't use the macros with its own preprocessor.
#ifdef _DOXYGEN
namespace asmjit {

//! \addtogroup asmjit_build
//! \{

//! Asmjit is embedded, implies \ref ASMJIT_STATIC.
#define ASMJIT_EMBED

//! Enables static-library build.
#define ASMJIT_STATIC

//! Defined when AsmJit's build configuration is 'Debug'.
//!
//! \note Can be defined explicitly to bypass auto-detection.
#define ASMJIT_BUILD_DEBUG

//! Defined when AsmJit's build configuration is 'Release'.
//!
//! \note Can be defined explicitly to bypass auto-detection.
#define ASMJIT_BUILD_RELEASE

//! Disables X86/X64 backends.
#define ASMJIT_NO_X86

//! Disables AArch64 backend.
#define ASMJIT_NO_AARCH64

//! Disables non-host backends entirely (useful for JIT compilers to minimize the library size).
#define ASMJIT_NO_FOREIGN

//! Disables deprecated API at compile time (deprecated API won't be available).
#define ASMJIT_NO_DEPRECATED

//! Disables \ref asmjit_builder functionality completely.
#define ASMJIT_NO_BUILDER

//! Disables \ref asmjit_compiler functionality completely.
#define ASMJIT_NO_COMPILER

//! Disables JIT memory management and \ref asmjit::JitRuntime.
#define ASMJIT_NO_JIT

//! Disables \ref asmjit::Logger and \ref asmjit::Formatter.
#define ASMJIT_NO_LOGGING

//! Disables everything that contains text.
#define ASMJIT_NO_TEXT

//! Disables instruction validation API.
#define ASMJIT_NO_VALIDATION

//! Disables instruction introspection API.
#define ASMJIT_NO_INTROSPECTION

// Avoid doxygen preprocessor using feature-selection definitions.
#undef ASMJIT_BUILD_EMBED
#undef ASMJIT_BUILD_STATIC
#undef ASMJIT_BUILD_DEBUG
#undef ASMJIT_BUILD_RELEASE
#undef ASMJIT_NO_X86
#undef ASMJIT_NO_FOREIGN
// (keep ASMJIT_NO_DEPRECATED defined, we don't document deprecated APIs).
#undef ASMJIT_NO_BUILDER
#undef ASMJIT_NO_COMPILER
#undef ASMJIT_NO_JIT
#undef ASMJIT_NO_LOGGING
#undef ASMJIT_NO_TEXT
#undef ASMJIT_NO_VALIDATION
#undef ASMJIT_NO_INTROSPECTION

//! \}

} // {asmjit}
#endif // _DOXYGEN

// ASMJIT_NO_BUILDER implies ASMJIT_NO_COMPILER.
#if defined(ASMJIT_NO_BUILDER) && !defined(ASMJIT_NO_COMPILER)
  #define ASMJIT_NO_COMPILER
#endif

// Prevent compile-time errors caused by misconfiguration.
#if defined(ASMJIT_NO_TEXT) && !defined(ASMJIT_NO_LOGGING)
  #pragma message("'ASMJIT_NO_TEXT' can only be defined when 'ASMJIT_NO_LOGGING' is defined.")
  #undef ASMJIT_NO_TEXT
#endif

#if defined(ASMJIT_NO_INTROSPECTION) && !defined(ASMJIT_NO_COMPILER)
  #pragma message("'ASMJIT_NO_INTROSPECTION' can only be defined when 'ASMJIT_NO_COMPILER' is defined")
  #undef ASMJIT_NO_INTROSPECTION
#endif

// Build Mode
// ==========

// Detect ASMJIT_BUILD_DEBUG and ASMJIT_BUILD_RELEASE if not defined.
#if !defined(ASMJIT_BUILD_DEBUG) && !defined(ASMJIT_BUILD_RELEASE)
  #if !defined(NDEBUG)
    #define ASMJIT_BUILD_DEBUG
  #else
    #define ASMJIT_BUILD_RELEASE
  #endif
#endif

// Target Architecture Detection
// =============================

//! \addtogroup asmjit_core
//! \{

//! \def ASMJIT_ARCH_X86
//!
//! Defined to either 0, 32, or 64 depending on whether the target CPU is X86 (32) or X86_64 (64).

//! \def ASMJIT_ARCH_ARM
//!
//! Defined to either 0, 32, or 64 depending on whether the target CPU is ARM (32) or AArch64 (64).

//! \def ASMJIT_ARCH_MIPS
//!
//! Defined to either 0, 32, or 64 depending on whether the target CPU is MIPS (32) or MISP64 (64).

//! \def ASMJIT_ARCH_RISCV
//!
//! Defined to either 0, 32, or 64 depending on whether the target CPU is RV32 (32) or RV64 (64).

//! \def ASMJIT_ARCH_BITS
//!
//! Defined to either 32 or 64 depending on the target.

//! \def ASMJIT_ARCH_LE
//!
//! Defined to 1 if the target architecture is little endian.

//! \def ASMJIT_ARCH_BE
//!
//! Defined to 1 if the target architecture is big endian.

//! \}

//! \cond NONE

#if defined(_M_X64) || defined(__x86_64__)
  #define ASMJIT_ARCH_X86 64
#elif defined(_M_IX86) || defined(__X86__) || defined(__i386__)
  #define ASMJIT_ARCH_X86 32
#else
  #define ASMJIT_ARCH_X86 0
#endif

#if defined(_M_ARM64) || defined(__arm64__) || defined(__aarch64__)
# define ASMJIT_ARCH_ARM 64
#elif defined(_M_ARM) || defined(_M_ARMT) || defined(__arm__) || defined(__thumb__) || defined(__thumb2__)
  #define ASMJIT_ARCH_ARM 32
#else
  #define ASMJIT_ARCH_ARM 0
#endif

#if defined(_MIPS_ARCH_MIPS64) || defined(__mips64)
  #define ASMJIT_ARCH_MIPS 64
#elif defined(_MIPS_ARCH_MIPS32) || defined(_M_MRX000) || defined(__mips__)
  #define ASMJIT_ARCH_MIPS 32
#else
  #define ASMJIT_ARCH_MIPS 0
#endif

// NOTE `__riscv` is the correct macro in this case as specified by "RISC-V Toolchain Conventions".
#if (defined(__riscv) || defined(__riscv__)) && defined(__riscv_xlen)
  #define ASMJIT_ARCH_RISCV __riscv_xlen
#else
  #define ASMJIT_ARCH_RISCV 0
#endif

#define ASMJIT_ARCH_BITS (ASMJIT_ARCH_X86 | ASMJIT_ARCH_ARM | ASMJIT_ARCH_MIPS | ASMJIT_ARCH_RISCV)
#if ASMJIT_ARCH_BITS == 0
  #undef ASMJIT_ARCH_BITS
  #if defined(__LP64__) || defined(_LP64)
    #define ASMJIT_ARCH_BITS 64
  #else
    #define ASMJIT_ARCH_BITS 32
  #endif
#endif

#if (defined(__ARMEB__))  || \
    (defined(__MIPSEB__)) || \
    (defined(__BYTE_ORDER__) && (__BYTE_ORDER__ == __ORDER_BIG_ENDIAN__))
  #define ASMJIT_ARCH_LE 0
  #define ASMJIT_ARCH_BE 1
#else
  #define ASMJIT_ARCH_LE 1
  #define ASMJIT_ARCH_BE 0
#endif

#if defined(ASMJIT_NO_FOREIGN)
  #if !ASMJIT_ARCH_X86 && !defined(ASMJIT_NO_X86)
    #define ASMJIT_NO_X86
  #endif

  #if ASMJIT_ARCH_ARM != 64 && !defined(ASMJIT_NO_AARCH64)
    #define ASMJIT_NO_AARCH64
  #endif
#endif

//! \endcond

// C++ Compiler and Features Detection
// ===================================

#if defined(__GNUC__) && defined(__has_attribute)
  #define ASMJIT_CXX_HAS_ATTRIBUTE(NAME, CHECK) (__has_attribute(NAME))
#else
  #define ASMJIT_CXX_HAS_ATTRIBUTE(NAME, CHECK) (!(!(CHECK)))
#endif // !ASMJIT_CXX_HAS_ATTRIBUTE

// API Decorators & C++ Extensions
// ===============================

//! \addtogroup asmjit_core
//! \{

//! \def ASMJIT_API
//!
//! A decorator that is used to decorate API that AsmJit exports when built as a shared library.

//! \def ASMJIT_VIRTAPI
//!
//! This is basically a workaround. When using MSVC and marking class as DLL export everything gets exported, which
//! is unwanted in most projects. MSVC automatically exports typeinfo and vtable if at least one symbol of the class
//! is exported. However, GCC has some strange behavior that even if one or more symbol is exported it doesn't export
//! typeinfo unless the class itself is decorated with "visibility(default)" (i.e. ASMJIT_API).

//! \def ASMJIT_FORCE_INLINE
//!
//! Decorator to force inlining of functions, uses either `__attribute__((__always_inline__))` or __forceinline,
//! depending on C++ compiler.

//! \def ASMJIT_INLINE_NODEBUG
//!
//! Like \ref ASMJIT_FORCE_INLINE, but uses additionally `__nodebug__` or `__artificial__` attribute to make the
//! debugging of some AsmJit functions easier, especially getters and one-line abstractions where usually you don't
//! want to step in.

//! \def ASMJIT_NOINLINE
//!
//! Decorator to avoid inlining of functions, uses either `__attribute__((__noinline__))` or `__declspec(noinline)`
//! depending on C++ compiler.

//! \def ASMJIT_NORETURN
//!
//! Decorator that marks functions that should never return. Typically used to implement assertion handlers that
//! terminate, so the function never returns.

//! \def ASMJIT_CDECL
//!
//! CDECL function attribute - either `__attribute__((__cdecl__))` or `__cdecl`.

//! \def ASMJIT_STDCALL
//!
//! STDCALL function attribute - either `__attribute__((__stdcall__))` or `__stdcall`.
//!
//! \note This expands to nothing on non-x86 targets as STDCALL is X86 specific.

//! \def ASMJIT_FASTCALL
//!
//! FASTCALL function attribute - either `__attribute__((__fastcall__))` or `__fastcall`.
//!
//! \note Expands to nothing on non-x86 targets as FASTCALL is X86 specific.

//! \def ASMJIT_REGPARM(N)
//!
//! Expands to `__attribute__((__regparm__(N)))` when compiled by GCC or clang, nothing otherwise.

//! \def ASMJIT_VECTORCALL
//!
//! VECTORCALL function attribute - either `__attribute__((__vectorcall__))` or `__vectorcall`.
//!
//! \note Expands to nothing on non-x86 targets as VECTORCALL is X86 specific.

//! \}

// API (Export / Import).
#if !defined(ASMJIT_STATIC)
  #if defined(_WIN32) && (defined(_MSC_VER) || defined(__MINGW32__))
    #ifdef ASMJIT_EXPORTS
      #define ASMJIT_API __declspec(dllexport)
    #else
      #define ASMJIT_API __declspec(dllimport)
    #endif
  #elif defined(_WIN32) && defined(__GNUC__)
    #ifdef ASMJIT_EXPORTS
      #define ASMJIT_API __attribute__((__dllexport__))
    #else
      #define ASMJIT_API __attribute__((__dllimport__))
    #endif
  #elif defined(__GNUC__)
    #define ASMJIT_API __attribute__((__visibility__("default")))
  #endif
#endif

#if !defined(ASMJIT_API)
  #define ASMJIT_API
#endif

#if !defined(ASMJIT_VARAPI)
  #define ASMJIT_VARAPI extern ASMJIT_API
#endif

#if defined(__GNUC__) && !defined(_WIN32)
  #define ASMJIT_VIRTAPI ASMJIT_API
#else
  #define ASMJIT_VIRTAPI
#endif

// Function attributes.
#if !defined(ASMJIT_BUILD_DEBUG) && defined(__GNUC__)
  #define ASMJIT_FORCE_INLINE inline __attribute__((__always_inline__))
#elif !defined(ASMJIT_BUILD_DEBUG) && defined(_MSC_VER)
  #define ASMJIT_FORCE_INLINE __forceinline
#else
  #define ASMJIT_FORCE_INLINE inline
#endif


#if defined(__clang__)
  #define ASMJIT_INLINE_NODEBUG inline __attribute__((__always_inline__, __nodebug__))
#elif defined(__GNUC__)
  #define ASMJIT_INLINE_NODEBUG inline __attribute__((__always_inline__, __artificial__))
#else
  #define ASMJIT_INLINE_NODEBUG inline
#endif

#if defined(__GNUC__)
  #define ASMJIT_NOINLINE __attribute__((__noinline__))
  #define ASMJIT_NORETURN __attribute__((__noreturn__))
#elif defined(_MSC_VER)
  #define ASMJIT_NOINLINE __declspec(noinline)
  #define ASMJIT_NORETURN __declspec(noreturn)
#else
  #define ASMJIT_NOINLINE
  #define ASMJIT_NORETURN
#endif

// Calling conventions.
#if ASMJIT_ARCH_X86 == 32 && defined(__GNUC__)
  #define ASMJIT_CDECL __attribute__((__cdecl__))
  #define ASMJIT_STDCALL __attribute__((__stdcall__))
  #define ASMJIT_FASTCALL __attribute__((__fastcall__))
  #define ASMJIT_REGPARM(N) __attribute__((__regparm__(N)))
#elif ASMJIT_ARCH_X86 == 32 && defined(_MSC_VER)
  #define ASMJIT_CDECL __cdecl
  #define ASMJIT_STDCALL __stdcall
  #define ASMJIT_FASTCALL __fastcall
  #define ASMJIT_REGPARM(N)
#else
  #define ASMJIT_CDECL
  #define ASMJIT_STDCALL
  #define ASMJIT_FASTCALL
  #define ASMJIT_REGPARM(N)
#endif

#if ASMJIT_ARCH_X86 && defined(_WIN32) && defined(_MSC_VER)
  #define ASMJIT_VECTORCALL __vectorcall
#elif ASMJIT_ARCH_X86 && defined(_WIN32)
  #define ASMJIT_VECTORCALL __attribute__((__vectorcall__))
#else
  #define ASMJIT_VECTORCALL
#endif

// Type alignment (not allowed by C++11 'alignas' keyword).
#if defined(__GNUC__)
  #define ASMJIT_ALIGN_TYPE(TYPE, N) __attribute__((__aligned__(N))) TYPE
#elif defined(_MSC_VER)
  #define ASMJIT_ALIGN_TYPE(TYPE, N) __declspec(align(N)) TYPE
#else
  #define ASMJIT_ALIGN_TYPE(TYPE, N) TYPE
#endif

//! \def ASMJIT_MAY_ALIAS
//!
//! Expands to `__attribute__((__may_alias__))` if supported.
#if defined(__GNUC__)
  #define ASMJIT_MAY_ALIAS __attribute__((__may_alias__))
#else
  #define ASMJIT_MAY_ALIAS
#endif

//! \def ASMJIT_MAYBE_UNUSED
//!
//! Expands to `[[maybe_unused]]` if supported or a compiler attribute instead.
#if __cplusplus >= 201703L
  #define ASMJIT_MAYBE_UNUSED [[maybe_unused]]
#elif defined(__GNUC__)
  #define ASMJIT_MAYBE_UNUSED __attribute__((unused))
#else
  #define ASMJIT_MAYBE_UNUSED
#endif

#if defined(__clang_major__) && __clang_major__ >= 4 && !defined(_DOXYGEN)
  // NOTE: Clang allows to apply this attribute to function arguments, which is what we want. Once GCC decides to
  // support this use, we will enable it for GCC as well. However, until that, it will be clang only, which is
  // what we need for static analysis.
  #define ASMJIT_NONNULL(FUNCTION_ARGUMENT) FUNCTION_ARGUMENT __attribute__((__nonnull__))
#else
  #define ASMJIT_NONNULL(FUNCTION_ARGUMENT) FUNCTION_ARGUMENT
#endif

//! \def ASMJIT_NOEXCEPT_TYPE
//!
//! Defined to `noexcept` in C++17 mode or nothing otherwise. Used by function typedefs.
#if __cplusplus >= 201703L
  #define ASMJIT_NOEXCEPT_TYPE noexcept
#else
  #define ASMJIT_NOEXCEPT_TYPE
#endif

//! \def ASMJIT_ASSUME(...)
//!
//! Macro that tells the C/C++ compiler that the expression `...` evaluates to true.
//!
//! This macro has two purposes:
//!
//!   1. Enable optimizations that would not be possible without the assumption.
//!   2. Hint static analysis tools that a certain condition is true to prevent false positives.
#if defined(__clang__)
  #define ASMJIT_ASSUME(...) __builtin_assume(__VA_ARGS__)
#elif defined(__GNUC__)
  #define ASMJIT_ASSUME(...) do { if (!(__VA_ARGS__)) __builtin_unreachable(); } while (0)
#elif defined(_MSC_VER)
  #define ASMJIT_ASSUME(...) __assume(__VA_ARGS__)
#else
  #define ASMJIT_ASSUME(...) (void)0
#endif

//! \def ASMJIT_LIKELY(...)
//!
//! Condition is likely to be taken (mostly error handling and edge cases).

//! \def ASMJIT_UNLIKELY(...)
//!
//! Condition is unlikely to be taken (mostly error handling and edge cases).
#if defined(__GNUC__)
  #define ASMJIT_LIKELY(...) __builtin_expect(!!(__VA_ARGS__), 1)
  #define ASMJIT_UNLIKELY(...) __builtin_expect(!!(__VA_ARGS__), 0)
#else
  #define ASMJIT_LIKELY(...) (__VA_ARGS__)
  #define ASMJIT_UNLIKELY(...) (__VA_ARGS__)
#endif

//! \def ASMJIT_FALLTHROUGH
//!
//! Portable [[fallthrough]] attribute.
#if defined(__clang__) && __cplusplus >= 201103L
  #define ASMJIT_FALLTHROUGH [[clang::fallthrough]]
#elif defined(__GNUC__) && __GNUC__ >= 7
  #define ASMJIT_FALLTHROUGH __attribute__((__fallthrough__))
#else
  #define ASMJIT_FALLTHROUGH ((void)0) /* fallthrough */
#endif

//! \def ASMJIT_DEPRECATED
//!
//! Marks function, class, struct, enum, or anything else as deprecated.
#if defined(__GNUC__)
  #define ASMJIT_DEPRECATED(MESSAGE) __attribute__((__deprecated__(MESSAGE)))
#elif defined(_MSC_VER)
  #define ASMJIT_DEPRECATED(MESSAGE) __declspec(deprecated(MESSAGE))
#else
  #define ASMJIT_DEPRECATED(MESSAGE)
#endif

// Utilities.
#define ASMJIT_OFFSET_OF(STRUCT, MEMBER) ((int)(intptr_t)((const char*)&((const STRUCT*)0x100)->MEMBER) - 0x100)
#define ASMJIT_ARRAY_SIZE(X) uint32_t(sizeof(X) / sizeof(X[0]))

#if ASMJIT_CXX_HAS_ATTRIBUTE(no_sanitize, 0)
  #define ASMJIT_ATTRIBUTE_NO_SANITIZE_UNDEF __attribute__((__no_sanitize__("undefined")))
#elif defined(__GNUC__) && __GNUC__ >= 5
  #define ASMJIT_ATTRIBUTE_NO_SANITIZE_UNDEF __attribute__((__no_sanitize_undefined__))
#else
  #define ASMJIT_ATTRIBUTE_NO_SANITIZE_UNDEF
#endif

// Diagnostic Macros
// ======================================

#if !defined(__clang__) && !defined(__INTEL_COMPILER) && !defined(_DOXYGEN)
  #if defined(__GNUC__) && __GNUC__ == 4
    // There is a bug in GCC 4.X that has been fixed in GCC 5+, so just silence the warning.
    #define ASMJIT_BEGIN_DIAGNOSTIC_SCOPE                                     \
      _Pragma("GCC diagnostic push")                                          \
      _Pragma("GCC diagnostic ignored \"-Wmissing-field-initializers\"")
    #define ASMJIT_END_DIAGNOSTIC_SCOPE                                       \
      _Pragma("GCC diagnostic pop")
  #elif defined(_MSC_VER)
    #define ASMJIT_BEGIN_DIAGNOSTIC_SCOPE                                     \
      __pragma(warning(push))                                                 \
      __pragma(warning(disable: 4127))  /* conditional expression is const */ \
      __pragma(warning(disable: 4201))  /* nameless struct/union */
    #define ASMJIT_END_DIAGNOSTIC_SCOPE                                       \
      __pragma(warning(pop))
  #endif
#endif

#if !defined(ASMJIT_BEGIN_DIAGNOSTIC_SCOPE) && !defined(ASMJIT_END_DIAGNOSTIC_SCOPE)
  #define ASMJIT_BEGIN_DIAGNOSTIC_SCOPE
  #define ASMJIT_END_DIAGNOSTIC_SCOPE
#endif

// Begin-Namespace & End-Namespace Macros
// ======================================

#if !defined(ASMJIT_NO_ABI_NAMESPACE) && !defined(_DOXYGEN)
  #define ASMJIT_BEGIN_NAMESPACE                                              \
    ASMJIT_BEGIN_DIAGNOSTIC_SCOPE                                             \
    namespace asmjit {                                                        \
    inline namespace ASMJIT_ABI_NAMESPACE {
  #define ASMJIT_END_NAMESPACE                                                \
    }}                                                                        \
    ASMJIT_END_DIAGNOSTIC_SCOPE
#else
  #define ASMJIT_BEGIN_NAMESPACE                                              \
    ASMJIT_BEGIN_DIAGNOSTIC_SCOPE                                             \
    namespace asmjit {
  #define ASMJIT_END_NAMESPACE                                                \
    }                                                                         \
    ASMJIT_END_DIAGNOSTIC_SCOPE
#endif

#define ASMJIT_BEGIN_SUB_NAMESPACE(NAMESPACE) ASMJIT_BEGIN_NAMESPACE namespace NAMESPACE {
#define ASMJIT_END_SUB_NAMESPACE } ASMJIT_END_NAMESPACE

// C++ Utilities
// =============

#define ASMJIT_NONCOPYABLE(Type)                                              \
    Type(const Type& other) = delete;                                         \
    Type& operator=(const Type& other) = delete;

#define ASMJIT_NONCONSTRUCTIBLE(Type)                                         \
    Type() = delete;                                                          \
    Type(const Type& other) = delete;                                         \
    Type& operator=(const Type& other) = delete;

//! \def ASMJIT_DEFINE_ENUM_FLAGS(T)
//!
//! Defines bit operations for enumeration flags.
#ifdef _DOXYGEN
  #define ASMJIT_DEFINE_ENUM_FLAGS(T)
#else
  #define ASMJIT_DEFINE_ENUM_FLAGS(T)                                         \
    static ASMJIT_INLINE_NODEBUG constexpr T operator~(T a) noexcept {        \
      return T(~(std::underlying_type<T>::type)(a));                          \
    }                                                                         \
                                                                              \
    static ASMJIT_INLINE_NODEBUG constexpr T operator|(T a, T b) noexcept {   \
      return T((std::underlying_type<T>::type)(a) |                           \
              (std::underlying_type<T>::type)(b));                            \
    }                                                                         \
    static ASMJIT_INLINE_NODEBUG constexpr T operator&(T a, T b) noexcept {   \
      return T((std::underlying_type<T>::type)(a) &                           \
              (std::underlying_type<T>::type)(b));                            \
    }                                                                         \
    static ASMJIT_INLINE_NODEBUG constexpr T operator^(T a, T b) noexcept {   \
      return T((std::underlying_type<T>::type)(a) ^                           \
              (std::underlying_type<T>::type)(b));                            \
    }                                                                         \
                                                                              \
    static ASMJIT_INLINE_NODEBUG T& operator|=(T& a, T b) noexcept {          \
      a = T((std::underlying_type<T>::type)(a) |                              \
            (std::underlying_type<T>::type)(b));                              \
      return a;                                                               \
    }                                                                         \
    static ASMJIT_INLINE_NODEBUG T& operator&=(T& a, T b) noexcept {          \
      a = T((std::underlying_type<T>::type)(a) &                              \
            (std::underlying_type<T>::type)(b));                              \
      return a;                                                               \
    }                                                                         \
    static ASMJIT_INLINE_NODEBUG T& operator^=(T& a, T b) noexcept {          \
      a = T((std::underlying_type<T>::type)(a) ^                              \
            (std::underlying_type<T>::type)(b));                              \
      return a;                                                               \
    }
#endif

//! \def ASMJIT_DEFINE_ENUM_COMPARE(T)
//!
//! Defines comparison operations for enumeration flags.
#if defined(_DOXYGEN) || (defined(_MSC_VER) && _MSC_VER <= 1900)
  #define ASMJIT_DEFINE_ENUM_COMPARE(T)
#else
  #define ASMJIT_DEFINE_ENUM_COMPARE(T)                                                \
    static ASMJIT_INLINE_NODEBUG bool operator<(T a, T b) noexcept {                   \
      return (std::underlying_type<T>::type)(a) < (std::underlying_type<T>::type)(b);  \
    }                                                                                  \
    static ASMJIT_INLINE_NODEBUG bool operator<=(T a, T b) noexcept {                  \
      return (std::underlying_type<T>::type)(a) <= (std::underlying_type<T>::type)(b); \
    }                                                                                  \
    static ASMJIT_INLINE_NODEBUG bool operator>(T a, T b) noexcept {                   \
      return (std::underlying_type<T>::type)(a) > (std::underlying_type<T>::type)(b);  \
    }                                                                                  \
    static ASMJIT_INLINE_NODEBUG bool operator>=(T a, T b) noexcept {                  \
      return (std::underlying_type<T>::type)(a) >= (std::underlying_type<T>::type)(b); \
    }
#endif

#endif // ASMJIT_CORE_API_CONFIG_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/archcommons.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ARCHCOMMONS_H_INCLUDED
#define ASMJIT_CORE_ARCHCOMMONS_H_INCLUDED

// This file provides architecture-specific classes that are required in the core library. For example Imm operand
// allows to be created from arm::Shift in a const-expr way, so the arm::Shift must be provided. So this header file
// provides everything architecture-specific that is used by the Core API.

#include "../core/globals.h"

ASMJIT_BEGIN_SUB_NAMESPACE(arm)

//! \addtogroup asmjit_arm
//! \{

//! Condition code (both AArch32 & AArch64).
//!
//! \note This enumeration doesn't match condition code that is used in AArch32/AArch64 opcodes. In general this
//! condition code is encoded as `(cc - 2) & 0xF` so that `kAL` condition code is zero and encoded as 0xE in opcode.
//! This makes it easier to use a condition code as an instruction modifier that defaults to 'al'.
enum class CondCode : uint8_t {
  kAL             = 0x00u,      //!< (no condition code) (always)
  kNA             = 0x01u,      //!< (not available)     (special)
  kEQ             = 0x02u,      //!<        Z==1         (any_sign ==)
  kNE             = 0x03u,      //!<        Z==0         (any_sign !=)
  kCS             = 0x04u,      //!< C==1                (unsigned >=)
  kHS             = 0x04u,      //!< C==1                (unsigned >=)
  kLO             = 0x05u,      //!< C==0                (unsigned < )
  kCC             = 0x05u,      //!< C==0                (unsigned < )
  kMI             = 0x06u,      //!<               N==1  (is negative)
  kPL             = 0x07u,      //!<               N==0  (is positive or zero)
  kVS             = 0x08u,      //!<               V==1  (is overflow)
  kVC             = 0x09u,      //!<               V==0  (no overflow)
  kHI             = 0x0Au,      //!< C==1 & Z==0         (unsigned > )
  kLS             = 0x0Bu,      //!< C==0 | Z==1         (unsigned <=)
  kGE             = 0x0Cu,      //!<               N==V  (signed   >=)
  kLT             = 0x0Du,      //!<               N!=V  (signed   < )
  kGT             = 0x0Eu,      //!<        Z==0 & N==V  (signed   > )
  kLE             = 0x0Fu,      //!<        Z==1 | N!=V  (signed   <=)

  kZero           = kEQ,        //!< Zero flag (alias to equal).
  kNotZero        = kNE,        //!< Not zero (alias to Not Equal).

  kEqual          = kEQ,        //!< Equal     `a == b`.
  kNotEqual       = kNE,        //!< Not Equal `a != b`.

  kCarry          = kCS,        //!< Carry flag.
  kNotCarry       = kCC,        //!< Not carry.

  kSign           = kMI,        //!< Sign flag.
  kNotSign        = kPL,        //!< Not sign.

  kNegative       = kMI,        //!< Negative.
  kPositive       = kPL,        //!< Positive or zero.

  kOverflow       = kVS,        //!< Signed overflow.
  kNotOverflow    = kVC,        //!< Not signed overflow.

  kSignedLT       = kLT,        //!< Signed    `a <  b`.
  kSignedLE       = kLE,        //!< Signed    `a <= b`.
  kSignedGT       = kGT,        //!< Signed    `a >  b`.
  kSignedGE       = kGE,        //!< Signed    `a >= b`.

  kUnsignedLT     = kLO,        //!< Unsigned  `a <  b`.
  kUnsignedLE     = kLS,        //!< Unsigned  `a <= b`.
  kUnsignedGT     = kHI,        //!< Unsigned  `a >  b`.
  kUnsignedGE     = kHS,        //!< Unsigned  `a >= b`.

  kBTZero         = kZero,      //!< Tested bit is zero.
  kBTNotZero      = kNotZero,   //!< Tested bit is not zero.

  kAlways         = kAL,        //!< No condition code (always).

  kMaxValue       = 0x0Fu       //!< Maximum value of `CondCode`.
};


//! \cond
static constexpr CondCode _reverseCondTable[] = {
  CondCode::kAL, // AL <- AL
  CondCode::kNA, // NA <- NA
  CondCode::kEQ, // EQ <- EQ
  CondCode::kNE, // NE <- NE
  CondCode::kLS, // LS <- CS
  CondCode::kHI, // HI <- LO
  CondCode::kMI, // MI <- MI
  CondCode::kPL, // PL <- PL
  CondCode::kVS, // VS <- VS
  CondCode::kVC, // VC <- VC
  CondCode::kLO, // LO <- HI
  CondCode::kCS, // CS <- LS
  CondCode::kLE, // LE <- GE
  CondCode::kGT, // GT <- LT
  CondCode::kLT, // LT <- GT
  CondCode::kGE  // GE <- LE
};
//! \endcond

//! Reverses a condition code (reverses the corresponding operands of a comparison).
static ASMJIT_INLINE_NODEBUG constexpr CondCode reverseCond(CondCode cond) noexcept { return _reverseCondTable[uint8_t(cond)]; }
//! Negates a condition code.
static ASMJIT_INLINE_NODEBUG constexpr CondCode negateCond(CondCode cond) noexcept { return CondCode(uint8_t(cond) ^ uint8_t(1)); }

//! Memory offset mode.
//!
//! Describes either fixed, pre-index, or post-index offset modes.
enum class OffsetMode : uint32_t {
  //! Fixed offset mode (either no index at all or a regular index without a write-back).
  kFixed = 0u,
  //! Pre-index "[BASE, #Offset {, <shift>}]!" with write-back.
  kPreIndex = 1u,
  //! Post-index "[BASE], #Offset {, <shift>}" with write-back.
  kPostIndex = 2u
};

//! Shift operation predicate (ARM) describes either SHIFT or EXTEND operation.
//!
//! \note The constants are AsmJit specific. The first 5 values describe real constants on ARM32 and AArch64 hardware,
//! however, the addition constants that describe extend modes are specific to AsmJit and would be translated to the
//! AArch64 specific constants by the assembler.
enum class ShiftOp : uint32_t {
  //! Shift left logical operation (default).
  //!
  //! Available to all ARM architectures.
  kLSL = 0x00u,

  //! Shift right logical operation.
  //!
  //! Available to all ARM architectures.
  kLSR = 0x01u,

  //! Shift right arithmetic operation.
  //!
  //! Available to all ARM architectures.
  kASR = 0x02u,

  //! Rotate right operation (AArch32 only).
  kROR = 0x03u,

  //! Rotate right with carry operation (encoded as `ShiftOp::kROR` with zero) (AArch32 only).
  kRRX = 0x04u,

  //! Shift left by filling low order bits with ones.
  kMSL = 0x05u,

  //! UXTN extend register operation (AArch64 only).
  kUXTB = 0x06u,
  //! UXTH extend register operation (AArch64 only).
  kUXTH = 0x07u,
  //! UXTW extend register operation (AArch64 only).
  kUXTW = 0x08u,
  //! UXTX extend register operation (AArch64 only).
  kUXTX = 0x09u,

  //! SXTB extend register operation (AArch64 only).
  kSXTB = 0x0Au,
  //! SXTH extend register operation (AArch64 only).
  kSXTH = 0x0Bu,
  //! SXTW extend register operation (AArch64 only).
  kSXTW = 0x0Cu,
  //! SXTX extend register operation (AArch64 only).
  kSXTX = 0x0Du

  // NOTE: 0xE and 0xF are used by memory operand to specify POST|PRE offset mode.
};

//! Represents ARM immediate shift operation type and value.
class Shift {
public:
  //! Shift operation.
  ShiftOp _op;
  //! Shift Value.
  uint32_t _value;

  //! Default constructed Shift is not initialized.
  ASMJIT_INLINE_NODEBUG Shift() noexcept = default;

  //! Copy constructor (default)
  ASMJIT_INLINE_NODEBUG constexpr Shift(const Shift& other) noexcept = default;

  //! Constructs Shift from operation `op` and shift `value`.
  ASMJIT_INLINE_NODEBUG constexpr Shift(ShiftOp op, uint32_t value) noexcept
    : _op(op),
      _value(value) {}

  //! Returns the shift operation.
  ASMJIT_INLINE_NODEBUG constexpr ShiftOp op() const noexcept { return _op; }
  //! Sets shift operation to `op`.
  ASMJIT_INLINE_NODEBUG void setOp(ShiftOp op) noexcept { _op = op; }

  //! Returns the shift amount.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t value() const noexcept { return _value; }
  //! Sets shift amount to `value`.
  ASMJIT_INLINE_NODEBUG void setValue(uint32_t value) noexcept { _value = value; }
};

//! \}

ASMJIT_END_SUB_NAMESPACE

ASMJIT_BEGIN_SUB_NAMESPACE(a32)

using namespace arm;

//! Data type that can be encoded with AArch32 instruction identifier.
//!
//! \note Data types are frequently used with AArch32 SIMD instructions. For example `VMAX` instruction can
//! use almost all datatypes in a form `VMAX.F32`, `VMAX.S16`, `VMAX.U32`, etc... Emitter automatically adds
//! the required data type at emit level.
enum class DataType : uint32_t {
  //! No data type specified (default for all general purpose instructions).
  kNone = 0,
  //! 8-bit signed integer, specified as `.s8` in assembly.
  kS8 = 1,
  //! 16-bit signed integer, specified as `.s16` in assembly.
  kS16 = 2,
  //! 32-bit signed integer, specified as `.s32` in assembly.
  kS32 = 3,
  //! 64-bit signed integer, specified as `.s64` in assembly.
  kS64 = 4,
  //! 8-bit unsigned integer, specified as `.u8` in assembly.
  kU8 = 5,
  //! 16-bit unsigned integer, specified as `.u16` in assembly.
  kU16 = 6,
  //! 32-bit unsigned integer, specified as `.u32` in assembly.
  kU32 = 7,
  //! 64-bit unsigned integer, specified as `.u64` in assembly.
  kU64 = 8,
  //! 16-bit floating point (half precision), specified as `.f16` in assembly.
  kF16 = 10,
  //! 32-bit floating point (single precision), specified as `.f32` in assembly.
  kF32 = 11,
  //! 64-bit floating point (double precision), specified as `.f64` in assembly.
  kF64 = 12,
  //! 8-bit polynomial.
  kP8 = 13,
  //! 16-bit BF16 floating point.
  kBF16 = 14,
  //! 64-bit polynomial.
  kP64 = 15,

  //! Maximum value of `DataType`.
  kMaxValue = 15
};

static ASMJIT_INLINE_NODEBUG uint32_t dataTypeSize(DataType dt) noexcept {
  static constexpr uint8_t table[] = { 0, 1, 2, 4, 8, 1, 2, 4, 8, 2, 4, 8, 1, 2, 8 };
  return table[size_t(dt)];
}

ASMJIT_END_SUB_NAMESPACE

ASMJIT_BEGIN_SUB_NAMESPACE(a64)
using namespace arm;
ASMJIT_END_SUB_NAMESPACE

#endif // ASMJIT_CORE_ARCHCOMMONS_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/archtraits.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/archtraits.h"
#include "../core/environment.h"
#include "../core/misc_p.h"

#if !defined(ASMJIT_NO_X86)
  #include "../x86/x86archtraits_p.h"
#endif

#if !defined(ASMJIT_NO_AARCH64)
  #include "../arm/a64archtraits_p.h"
#endif

ASMJIT_BEGIN_NAMESPACE

static const constexpr ArchTraits noArchTraits = {
  // SP/FP/LR/PC.
  0xFF, 0xFF, 0xFF, 0xFF,

  // Reserved,
  { 0, 0, 0 },

  // HW stack alignment.
  0,

  // Min/Max stack offset.
  0, 0,

  // ISA features [Gp, Vec, Other0, Other1].
  {{
    InstHints::kNoHints,
    InstHints::kNoHints,
    InstHints::kNoHints,
    InstHints::kNoHints
  }},

  // RegTypeToSignature.
  #define V(index) OperandSignature{0}
  {{ ASMJIT_LOOKUP_TABLE_32(V, 0) }},
  #undef V

  // RegTypeToTypeId.
  #define V(index) TypeId::kVoid
  {{ ASMJIT_LOOKUP_TABLE_32(V, 0) }},
  #undef V

  // TypeIdToRegType.
  #define V(index) RegType::kNone
  {{ ASMJIT_LOOKUP_TABLE_32(V, 0) }},
  #undef V

  // Word names of 8-bit, 16-bit, 32-bit, and 64-bit quantities.
  {
    ArchTypeNameId::kByte,
    ArchTypeNameId::kHalf,
    ArchTypeNameId::kWord,
    ArchTypeNameId::kQuad
  }
};

ASMJIT_VARAPI const ArchTraits _archTraits[uint32_t(Arch::kMaxValue) + 1] = {
  // No architecture.
  noArchTraits,

  // X86/X86 architectures.
#if !defined(ASMJIT_NO_X86)
  x86::x86ArchTraits,
  x86::x64ArchTraits,
#else
  noArchTraits,
  noArchTraits,
#endif

  // RISCV32/RISCV64 architectures.
  noArchTraits,
  noArchTraits,

  // ARM architecture
  noArchTraits,

  // AArch64 architecture.
#if !defined(ASMJIT_NO_AARCH64)
  a64::a64ArchTraits,
#else
  noArchTraits,
#endif

  // ARM/Thumb architecture.
  noArchTraits,

  // Reserved.
  noArchTraits,

  // MIPS32/MIPS64
  noArchTraits,
  noArchTraits
};

ASMJIT_FAVOR_SIZE Error ArchUtils::typeIdToRegSignature(Arch arch, TypeId typeId, TypeId* typeIdOut, OperandSignature* regSignatureOut) noexcept {
  const ArchTraits& archTraits = ArchTraits::byArch(arch);

  // TODO: Remove this, should never be used like this.
  // Passed RegType instead of TypeId?
  if (uint32_t(typeId) <= uint32_t(RegType::kMaxValue))
    typeId = archTraits.regTypeToTypeId(RegType(uint32_t(typeId)));

  if (ASMJIT_UNLIKELY(!TypeUtils::isValid(typeId)))
    return DebugUtils::errored(kErrorInvalidTypeId);

  // First normalize architecture dependent types.
  if (TypeUtils::isAbstract(typeId)) {
    bool is32Bit = Environment::is32Bit(arch);
    if (typeId == TypeId::kIntPtr)
      typeId = is32Bit ? TypeId::kInt32 : TypeId::kInt64;
    else
      typeId = is32Bit ? TypeId::kUInt32 : TypeId::kUInt64;
  }

  // Type size helps to construct all groups of registers.
  // TypeId is invalid if the size is zero.
  uint32_t size = TypeUtils::sizeOf(typeId);
  if (ASMJIT_UNLIKELY(!size))
    return DebugUtils::errored(kErrorInvalidTypeId);

  if (ASMJIT_UNLIKELY(typeId == TypeId::kFloat80))
    return DebugUtils::errored(kErrorInvalidUseOfF80);

  RegType regType = RegType::kNone;
  if (TypeUtils::isBetween(typeId, TypeId::_kBaseStart, TypeId::_kVec32Start)) {
    regType = archTraits._typeIdToRegType[uint32_t(typeId) - uint32_t(TypeId::_kBaseStart)];
    if (regType == RegType::kNone) {
      if (typeId == TypeId::kInt64 || typeId == TypeId::kUInt64)
        return DebugUtils::errored(kErrorInvalidUseOfGpq);
      else
        return DebugUtils::errored(kErrorInvalidTypeId);
    }
  }
  else {
    if (size <= 8 && archTraits._regSignature[RegType::kVec64].isValid())
      regType = RegType::kVec64;
    else if (size <= 16 && archTraits._regSignature[RegType::kVec128].isValid())
      regType = RegType::kVec128;
    else if (size == 32 && archTraits._regSignature[RegType::kVec256].isValid())
      regType = RegType::kVec256;
    else if (archTraits._regSignature[RegType::kVec512].isValid())
      regType = RegType::kVec512;
    else
      return DebugUtils::errored(kErrorInvalidTypeId);
  }

  *typeIdOut = typeId;
  *regSignatureOut = archTraits.regTypeToSignature(regType);
  return kErrorOk;
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/archtraits.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ARCHTRAITS_H_INCLUDED
#define ASMJIT_CORE_ARCHTRAITS_H_INCLUDED

#include "../core/operand.h"
#include "../core/support.h"
#include "../core/type.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

//! Instruction set architecture (ISA).
enum class Arch : uint8_t {
  //! Unknown or uninitialized ISA.
  kUnknown = 0,

  //! 32-bit X86 ISA.
  kX86 = 1,
  //! 64-bit X86 ISA also known as X64, X86_64, and AMD64.
  kX64 = 2,

  //! 32-bit RISC-V ISA.
  kRISCV32 = 3,
  //! 64-bit RISC-V ISA.
  kRISCV64 = 4,

  //! 32-bit ARM ISA (little endian).
  kARM = 5,
  //! 64-bit ARM ISA in (little endian).
  kAArch64 = 6,
  //! 32-bit ARM ISA in Thumb mode (little endian).
  kThumb = 7,

  // 8 is not used at the moment, even numbers are 64-bit architectures.

  //! 32-bit MIPS ISA in (little endian).
  kMIPS32_LE = 9,
  //! 64-bit MIPS ISA in (little endian).
  kMIPS64_LE = 10,

  //! 32-bit ARM ISA (big endian).
  kARM_BE = 11,
  //! 64-bit ARM ISA in (big endian).
  kAArch64_BE = 12,
  //! 32-bit ARM ISA in Thumb mode (big endian).
  kThumb_BE = 13,

  // 14 is not used at the moment, even numbers are 64-bit architectures.

  //! 32-bit MIPS ISA in (big endian).
  kMIPS32_BE = 15,
  //! 64-bit MIPS ISA in (big endian).
  kMIPS64_BE = 16,

  //! Maximum value of `Arch`.
  kMaxValue = kMIPS64_BE,

  //! Mask used by 32-bit ISAs (odd are 32-bit, even are 64-bit).
  k32BitMask = 0x01,
  //! First big-endian architecture.
  kBigEndian = kARM_BE,

  //! ISA detected at compile-time (ISA of the host).
  kHost =
#if defined(_DOXYGEN)
    DETECTED_AT_COMPILE_TIME
#else
    ASMJIT_ARCH_X86 == 32 ? kX86 :
    ASMJIT_ARCH_X86 == 64 ? kX64 :

    ASMJIT_ARCH_RISCV == 32 ? kRISCV32 :
    ASMJIT_ARCH_RISCV == 64 ? kRISCV64 :

    ASMJIT_ARCH_ARM == 32 && ASMJIT_ARCH_LE ? kARM :
    ASMJIT_ARCH_ARM == 32 && ASMJIT_ARCH_BE ? kARM_BE :
    ASMJIT_ARCH_ARM == 64 && ASMJIT_ARCH_LE ? kAArch64 :
    ASMJIT_ARCH_ARM == 64 && ASMJIT_ARCH_BE ? kAArch64_BE :

    ASMJIT_ARCH_MIPS == 32 && ASMJIT_ARCH_LE ? kMIPS32_LE :
    ASMJIT_ARCH_MIPS == 32 && ASMJIT_ARCH_BE ? kMIPS32_BE :
    ASMJIT_ARCH_MIPS == 64 && ASMJIT_ARCH_LE ? kMIPS64_LE :
    ASMJIT_ARCH_MIPS == 64 && ASMJIT_ARCH_BE ? kMIPS64_BE :

    kUnknown
#endif
};

//! Sub-architecture.
enum class SubArch : uint8_t {
  //! Unknown or uninitialized architecture sub-type.
  kUnknown = 0,

  //! Maximum value of `SubArch`.
  kMaxValue = kUnknown,

  //! Sub-architecture detected at compile-time (sub-architecture of the host).
  kHost =
#if defined(_DOXYGEN)
    DETECTED_AT_COMPILE_TIME
#else
    kUnknown
#endif
};

//! Identifier used to represent names of different data types across architectures.
enum class ArchTypeNameId : uint8_t {
  //! Describes 'db' (X86/X86_64 convention, always 8-bit quantity).
  kDB = 0,
  //! Describes 'dw' (X86/X86_64 convention, always 16-bit word).
  kDW,
  //! Describes 'dd' (X86/X86_64 convention, always 32-bit word).
  kDD,
  //! Describes 'dq' (X86/X86_64 convention, always 64-bit word).
  kDQ,
  //! Describes 'byte' (always 8-bit quantity).
  kByte,
  //! Describes 'half' (most likely 16-bit word).
  kHalf,
  //! Describes 'word' (either 16-bit or 32-bit word).
  kWord,
  //! Describes 'hword' (most likely 16-bit word).
  kHWord,
  //! Describes 'dword' (either 32-bit or 64-bit word).
  kDWord,
  //! Describes 'qword' (64-bit word).
  kQWord,
  //! Describes 'xword' (64-bit word).
  kXWord,
  //! Describes 'short' (always 16-bit word).
  kShort,
  //! Describes 'long' (most likely 32-bit word).
  kLong,
  //! Describes 'quad' (64-bit word).
  kQuad,

  //! Maximum value of `ArchTypeNameId`.
  kMaxValue = kQuad
};

//! Instruction feature hints for each register group provided by \ref ArchTraits.
//!
//! Instruction feature hints describe miscellaneous instructions provided by the architecture that can be used by
//! register allocator to make certain things simpler - like register swaps or emitting register push/pop sequences.
//!
//! \remarks Instruction feature hints are only defined for register groups that can be used with \ref
//! asmjit_compiler infrastructure. Register groups that are not managed by Compiler are not provided by
//! \ref ArchTraits and cannot be queried.
enum class InstHints : uint8_t {
  //! No feature hints.
  kNoHints = 0,

  //! Architecture supports a register swap by using a single instruction.
  kRegSwap = 0x01u,
  //! Architecture provides push/pop instructions.
  kPushPop = 0x02u
};
ASMJIT_DEFINE_ENUM_FLAGS(InstHints)

//! Architecture traits used by Function API and Compiler's register allocator.
struct ArchTraits {
  //! \name Members
  //! \{

  //! Stack pointer register id.
  uint8_t _spRegId;
  //! Frame pointer register id.
  uint8_t _fpRegId;
  //! Link register id.
  uint8_t _linkRegId;
  //! Instruction pointer (or program counter) register id, if accessible.
  uint8_t _ipRegId;

  // Reserved.
  uint8_t _reserved[3];
  //! Hardware stack alignment requirement.
  uint8_t _hwStackAlignment;

  //! Minimum addressable offset on stack guaranteed for all instructions.
  uint32_t _minStackOffset;
  //! Maximum addressable offset on stack depending on specific instruction.
  uint32_t _maxStackOffset;

  //! Flags for each virtual register group.
  Support::Array<InstHints, Globals::kNumVirtGroups> _instHints;

  //! Maps register type into a signature, that provides group, size and can be used to construct register operands.
  Support::Array<OperandSignature, uint32_t(RegType::kMaxValue) + 1> _regSignature;
  //! Maps a register to type-id, see \ref TypeId.
  Support::Array<TypeId, uint32_t(RegType::kMaxValue) + 1> _regTypeToTypeId;
  //! Maps scalar TypeId values (from TypeId::_kIdBaseStart) to register types, see \ref TypeId.
  Support::Array<RegType, 32> _typeIdToRegType;

  //! Word name identifiers of 8-bit, 16-bit, 32-bit, and 64-bit quantities that appear in formatted text.
  ArchTypeNameId _typeNameIdTable[4];

  //! \}

  //! \name Accessors
  //! \{

  //! Returns stack pointer register id.
  ASMJIT_INLINE_NODEBUG uint32_t spRegId() const noexcept { return _spRegId; }
  //! Returns stack frame register id.
  ASMJIT_INLINE_NODEBUG uint32_t fpRegId() const noexcept { return _fpRegId; }
  //! Returns link register id, if the architecture provides it.
  ASMJIT_INLINE_NODEBUG uint32_t linkRegId() const noexcept { return _linkRegId; }
  //! Returns instruction pointer register id, if the architecture provides it.
  ASMJIT_INLINE_NODEBUG uint32_t ipRegId() const noexcept { return _ipRegId; }

  //! Returns a hardware stack alignment requirement.
  //!
  //! \note This is a hardware constraint. Architectures that don't constrain it would return the lowest alignment
  //! (1), however, some architectures may constrain the alignment, for example AArch64 requires 16-byte alignment.
  ASMJIT_INLINE_NODEBUG uint32_t hwStackAlignment() const noexcept { return _hwStackAlignment; }

  //! Tests whether the architecture provides link register, which is used across function calls. If the link
  //! register is not provided then a function call pushes the return address on stack (X86/X64).
  ASMJIT_INLINE_NODEBUG bool hasLinkReg() const noexcept { return _linkRegId != BaseReg::kIdBad; }

  //! Returns minimum addressable offset on stack guaranteed for all instructions.
  ASMJIT_INLINE_NODEBUG uint32_t minStackOffset() const noexcept { return _minStackOffset; }
  //! Returns maximum addressable offset on stack depending on specific instruction.
  ASMJIT_INLINE_NODEBUG uint32_t maxStackOffset() const noexcept { return _maxStackOffset; }

  //! Returns ISA flags of the given register `group`.
  ASMJIT_INLINE_NODEBUG InstHints instFeatureHints(RegGroup group) const noexcept { return _instHints[group]; }
  //! Tests whether the given register `group` has the given `flag` set.
  ASMJIT_INLINE_NODEBUG bool hasInstHint(RegGroup group, InstHints feature) const noexcept { return Support::test(_instHints[group], feature); }
  //! Tests whether the ISA provides register swap instruction for the given register `group`.
  ASMJIT_INLINE_NODEBUG bool hasInstRegSwap(RegGroup group) const noexcept { return hasInstHint(group, InstHints::kRegSwap); }
  //! Tests whether the ISA provides push/pop instructions for the given register `group`.
  ASMJIT_INLINE_NODEBUG bool hasInstPushPop(RegGroup group) const noexcept { return hasInstHint(group, InstHints::kPushPop); }

  ASMJIT_INLINE_NODEBUG bool hasRegType(RegType type) const noexcept {
    return type <= RegType::kMaxValue && _regSignature[type].isValid();
  }

  //! Returns an operand signature from the given register `type` of this architecture.
  ASMJIT_INLINE_NODEBUG OperandSignature regTypeToSignature(RegType type) const noexcept { return _regSignature[type]; }
  //! Returns a register from the given register `type` of this architecture.
  ASMJIT_INLINE_NODEBUG RegGroup regTypeToGroup(RegType type) const noexcept { return _regSignature[type].regGroup(); }
  //! Returns a register size the given register `type` of this architecture.
  ASMJIT_INLINE_NODEBUG uint32_t regTypeToSize(RegType type) const noexcept { return _regSignature[type].size(); }
  //! Returns a corresponding `TypeId` from the given register `type` of this architecture.
  ASMJIT_INLINE_NODEBUG TypeId regTypeToTypeId(RegType type) const noexcept { return _regTypeToTypeId[type]; }

  //! Returns a table of ISA word names that appear in formatted text. Word names are ISA dependent.
  //!
  //! The index of this table is log2 of the size:
  //!   - [0] 8-bits
  //!   - [1] 16-bits
  //!   - [2] 32-bits
  //!   - [3] 64-bits
  ASMJIT_INLINE_NODEBUG const ArchTypeNameId* typeNameIdTable() const noexcept { return _typeNameIdTable; }

  //! Returns an ISA word name identifier of the given `index`, see \ref typeNameIdTable() for more details.
  ASMJIT_INLINE_NODEBUG ArchTypeNameId typeNameIdByIndex(uint32_t index) const noexcept { return _typeNameIdTable[index]; }

  //! \}

  //! \name Statics
  //! \{

  //! Returns a const reference to `ArchTraits` for the given architecture `arch`.
  static ASMJIT_INLINE_NODEBUG const ArchTraits& byArch(Arch arch) noexcept;

  //! \}
};

ASMJIT_VARAPI const ArchTraits _archTraits[uint32_t(Arch::kMaxValue) + 1];

//! \cond
ASMJIT_INLINE_NODEBUG const ArchTraits& ArchTraits::byArch(Arch arch) noexcept { return _archTraits[uint32_t(arch)]; }
//! \endcond

//! Architecture utilities.
namespace ArchUtils {

ASMJIT_API Error typeIdToRegSignature(Arch arch, TypeId typeId, TypeId* typeIdOut, OperandSignature* regSignatureOut) noexcept;

} // {ArchUtils}

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ARCHTRAITS_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/assembler.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/assembler.h"
#include "../core/codewriter_p.h"
#include "../core/constpool.h"
#include "../core/emitterutils_p.h"
#include "../core/formatter.h"
#include "../core/logger.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// BaseAssembler - Construction & Destruction
// ==========================================

BaseAssembler::BaseAssembler() noexcept
  : BaseEmitter(EmitterType::kAssembler) {}

BaseAssembler::~BaseAssembler() noexcept {}

// BaseAssembler - Buffer Management
// =================================

Error BaseAssembler::setOffset(size_t offset) {
  if (ASMJIT_UNLIKELY(!_code))
    return reportError(DebugUtils::errored(kErrorNotInitialized));

  size_t size = Support::max<size_t>(_section->bufferSize(), this->offset());
  if (ASMJIT_UNLIKELY(offset > size))
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  _bufferPtr = _bufferData + offset;
  return kErrorOk;
}

// BaseAssembler - Section Management
// ==================================

static void BaseAssembler_initSection(BaseAssembler* self, Section* section) noexcept {
  uint8_t* p = section->_buffer._data;

  self->_section = section;
  self->_bufferData = p;
  self->_bufferPtr  = p + section->_buffer._size;
  self->_bufferEnd  = p + section->_buffer._capacity;
}

Error BaseAssembler::section(Section* section) {
  if (ASMJIT_UNLIKELY(!_code))
    return reportError(DebugUtils::errored(kErrorNotInitialized));

  if (!_code->isSectionValid(section->id()) || _code->_sections[section->id()] != section)
    return reportError(DebugUtils::errored(kErrorInvalidSection));

#ifndef ASMJIT_NO_LOGGING
  if (_logger)
    _logger->logf(".section %s {#%u}\n", section->name(), section->id());
#endif

  BaseAssembler_initSection(this, section);
  return kErrorOk;
}

// BaseAssembler - Label Management
// ================================

Label BaseAssembler::newLabel() {
  uint32_t labelId = Globals::kInvalidId;
  if (ASMJIT_LIKELY(_code)) {
    LabelEntry* le;
    Error err = _code->newLabelEntry(&le);
    if (ASMJIT_UNLIKELY(err))
      reportError(err);
    else
      labelId = le->id();
  }
  return Label(labelId);
}

Label BaseAssembler::newNamedLabel(const char* name, size_t nameSize, LabelType type, uint32_t parentId) {
  uint32_t labelId = Globals::kInvalidId;
  if (ASMJIT_LIKELY(_code)) {
    LabelEntry* le;
    Error err = _code->newNamedLabelEntry(&le, name, nameSize, type, parentId);
    if (ASMJIT_UNLIKELY(err))
      reportError(err);
    else
      labelId = le->id();
  }
  return Label(labelId);
}

Error BaseAssembler::bind(const Label& label) {
  if (ASMJIT_UNLIKELY(!_code))
    return reportError(DebugUtils::errored(kErrorNotInitialized));

  Error err = _code->bindLabel(label, _section->id(), offset());

#ifndef ASMJIT_NO_LOGGING
  if (_logger)
    EmitterUtils::logLabelBound(this, label);
#endif

  resetInlineComment();
  if (err)
    return reportError(err);

  return kErrorOk;
}

// BaseAssembler - Embed
// =====================

Error BaseAssembler::embed(const void* data, size_t dataSize) {
  if (ASMJIT_UNLIKELY(!_code))
    return reportError(DebugUtils::errored(kErrorNotInitialized));

  if (dataSize == 0)
    return kErrorOk;

  CodeWriter writer(this);
  ASMJIT_PROPAGATE(writer.ensureSpace(this, dataSize));

  writer.emitData(data, dataSize);
  writer.done(this);

#ifndef ASMJIT_NO_LOGGING
  if (_logger) {
    StringTmp<512> sb;
    Formatter::formatData(sb, _logger->flags(), arch(), TypeId::kUInt8, data, dataSize, 1);
    sb.append('\n');
    _logger->log(sb);
  }
#endif

  return kErrorOk;
}

Error BaseAssembler::embedDataArray(TypeId typeId, const void* data, size_t itemCount, size_t repeatCount) {
  uint32_t deabstractDelta = TypeUtils::deabstractDeltaOfSize(registerSize());
  TypeId finalTypeId = TypeUtils::deabstract(typeId, deabstractDelta);

  if (ASMJIT_UNLIKELY(!TypeUtils::isValid(finalTypeId)))
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  if (itemCount == 0 || repeatCount == 0)
    return kErrorOk;

  uint32_t typeSize = TypeUtils::sizeOf(finalTypeId);
  Support::FastUInt8 of = 0;

  size_t dataSize = Support::mulOverflow(itemCount, size_t(typeSize), &of);
  size_t totalSize = Support::mulOverflow(dataSize, repeatCount, &of);

  if (ASMJIT_UNLIKELY(of))
    return reportError(DebugUtils::errored(kErrorOutOfMemory));

  CodeWriter writer(this);
  ASMJIT_PROPAGATE(writer.ensureSpace(this, totalSize));

  for (size_t i = 0; i < repeatCount; i++)
    writer.emitData(data, dataSize);

  writer.done(this);

#ifndef ASMJIT_NO_LOGGING
  if (_logger) {
    StringTmp<512> sb;
    Formatter::formatData(sb, _logger->flags(), arch(), typeId, data, itemCount, repeatCount);
    sb.append('\n');
    _logger->log(sb);
  }
#endif

  return kErrorOk;
}

#ifndef ASMJIT_NO_LOGGING
static const TypeId dataTypeIdBySize[9] = {
  TypeId::kVoid,   // [0] (invalid)
  TypeId::kUInt8,  // [1] (uint8_t)
  TypeId::kUInt16, // [2] (uint16_t)
  TypeId::kVoid,   // [3] (invalid)
  TypeId::kUInt32, // [4] (uint32_t)
  TypeId::kVoid,   // [5] (invalid)
  TypeId::kVoid,   // [6] (invalid)
  TypeId::kVoid,   // [7] (invalid)
  TypeId::kUInt64  // [8] (uint64_t)
};
#endif

Error BaseAssembler::embedConstPool(const Label& label, const ConstPool& pool) {
  if (ASMJIT_UNLIKELY(!_code))
    return reportError(DebugUtils::errored(kErrorNotInitialized));

  if (ASMJIT_UNLIKELY(!isLabelValid(label)))
    return reportError(DebugUtils::errored(kErrorInvalidLabel));

  ASMJIT_PROPAGATE(align(AlignMode::kData, uint32_t(pool.alignment())));
  ASMJIT_PROPAGATE(bind(label));

  size_t size = pool.size();
  if (!size)
    return kErrorOk;

  CodeWriter writer(this);
  ASMJIT_PROPAGATE(writer.ensureSpace(this, size));

#ifndef ASMJIT_NO_LOGGING
  uint8_t* data = writer.cursor();
#endif

  pool.fill(writer.cursor());
  writer.advance(size);
  writer.done(this);

#ifndef ASMJIT_NO_LOGGING
  if (_logger) {
    uint32_t dataSizeLog2 = Support::min<uint32_t>(Support::ctz(pool.minItemSize()), 3);
    uint32_t dataSize = 1 << dataSizeLog2;

    StringTmp<512> sb;
    Formatter::formatData(sb, _logger->flags(), arch(), dataTypeIdBySize[dataSize], data, size >> dataSizeLog2);
    sb.append('\n');
    _logger->log(sb);
  }
#endif

  return kErrorOk;
}

Error BaseAssembler::embedLabel(const Label& label, size_t dataSize) {
  if (ASMJIT_UNLIKELY(!_code))
    return reportError(DebugUtils::errored(kErrorNotInitialized));

  ASMJIT_ASSERT(_code != nullptr);
  RelocEntry* re;
  LabelEntry* le = _code->labelEntry(label);

  if (ASMJIT_UNLIKELY(!le))
    return reportError(DebugUtils::errored(kErrorInvalidLabel));

  if (dataSize == 0)
    dataSize = registerSize();

  if (ASMJIT_UNLIKELY(!Support::isPowerOf2(dataSize) || dataSize > 8))
    return reportError(DebugUtils::errored(kErrorInvalidOperandSize));

  CodeWriter writer(this);
  ASMJIT_PROPAGATE(writer.ensureSpace(this, dataSize));

#ifndef ASMJIT_NO_LOGGING
  if (_logger) {
    StringTmp<256> sb;
    sb.append('.');
    Formatter::formatDataType(sb, _logger->flags(), arch(), dataTypeIdBySize[dataSize]);
    sb.append(' ');
    Formatter::formatLabel(sb, FormatFlags::kNone, this, label.id());
    sb.append('\n');
    _logger->log(sb);
  }
#endif

  Error err = _code->newRelocEntry(&re, RelocType::kRelToAbs);
  if (ASMJIT_UNLIKELY(err))
    return reportError(err);

  re->_sourceSectionId = _section->id();
  re->_sourceOffset = offset();
  re->_format.resetToSimpleValue(OffsetType::kUnsignedOffset, dataSize);

  if (le->isBound()) {
    re->_targetSectionId = le->section()->id();
    re->_payload = le->offset();
  }
  else {
    OffsetFormat of;
    of.resetToSimpleValue(OffsetType::kUnsignedOffset, dataSize);

    LabelLink* link = _code->newLabelLink(le, _section->id(), offset(), 0, of);
    if (ASMJIT_UNLIKELY(!link))
      return reportError(DebugUtils::errored(kErrorOutOfMemory));

    link->relocId = re->id();
  }

  // Emit dummy DWORD/QWORD depending on the data size.
  writer.emitZeros(dataSize);
  writer.done(this);

  return kErrorOk;
}

Error BaseAssembler::embedLabelDelta(const Label& label, const Label& base, size_t dataSize) {
  if (ASMJIT_UNLIKELY(!_code))
    return reportError(DebugUtils::errored(kErrorNotInitialized));

  LabelEntry* labelEntry = _code->labelEntry(label);
  LabelEntry* baseEntry = _code->labelEntry(base);

  if (ASMJIT_UNLIKELY(!labelEntry || !baseEntry))
    return reportError(DebugUtils::errored(kErrorInvalidLabel));

  if (dataSize == 0)
    dataSize = registerSize();

  if (ASMJIT_UNLIKELY(!Support::isPowerOf2(dataSize) || dataSize > 8))
    return reportError(DebugUtils::errored(kErrorInvalidOperandSize));

  CodeWriter writer(this);
  ASMJIT_PROPAGATE(writer.ensureSpace(this, dataSize));

#ifndef ASMJIT_NO_LOGGING
  if (_logger) {
    StringTmp<256> sb;
    sb.append('.');
    Formatter::formatDataType(sb, _logger->flags(), arch(), dataTypeIdBySize[dataSize]);
    sb.append(" (");
    Formatter::formatLabel(sb, FormatFlags::kNone, this, label.id());
    sb.append(" - ");
    Formatter::formatLabel(sb, FormatFlags::kNone, this, base.id());
    sb.append(")\n");
    _logger->log(sb);
  }
#endif

  // If both labels are bound within the same section it means the delta can be calculated now.
  if (labelEntry->isBound() && baseEntry->isBound() && labelEntry->section() == baseEntry->section()) {
    uint64_t delta = labelEntry->offset() - baseEntry->offset();
    writer.emitValueLE(delta, dataSize);
  }
  else {
    RelocEntry* re;
    Error err = _code->newRelocEntry(&re, RelocType::kExpression);
    if (ASMJIT_UNLIKELY(err))
      return reportError(err);

    Expression* exp = _code->_zone.newT<Expression>();
    if (ASMJIT_UNLIKELY(!exp))
      return reportError(DebugUtils::errored(kErrorOutOfMemory));

    exp->reset();
    exp->opType = ExpressionOpType::kSub;
    exp->setValueAsLabel(0, labelEntry);
    exp->setValueAsLabel(1, baseEntry);

    re->_format.resetToSimpleValue(OffsetType::kSignedOffset, dataSize);
    re->_sourceSectionId = _section->id();
    re->_sourceOffset = offset();
    re->_payload = (uint64_t)(uintptr_t)exp;

    writer.emitZeros(dataSize);
  }

  writer.done(this);
  return kErrorOk;
}

// BaseAssembler - Comment
// =======================

Error BaseAssembler::comment(const char* data, size_t size) {
  if (!hasEmitterFlag(EmitterFlags::kLogComments)) {
    if (!hasEmitterFlag(EmitterFlags::kAttached))
      return reportError(DebugUtils::errored(kErrorNotInitialized));
    return kErrorOk;
  }

#ifndef ASMJIT_NO_LOGGING
  // Logger cannot be NULL if `EmitterFlags::kLogComments` is set.
  ASMJIT_ASSERT(_logger != nullptr);

  _logger->log(data, size);
  _logger->log("\n", 1);
  return kErrorOk;
#else
  DebugUtils::unused(data, size);
  return kErrorOk;
#endif
}

// BaseAssembler - Events
// ======================

Error BaseAssembler::onAttach(CodeHolder* code) noexcept {
  ASMJIT_PROPAGATE(Base::onAttach(code));

  // Attach to the end of the .text section.
  BaseAssembler_initSection(this, code->_sections[0]);

  return kErrorOk;
}

Error BaseAssembler::onDetach(CodeHolder* code) noexcept {
  _section    = nullptr;
  _bufferData = nullptr;
  _bufferEnd  = nullptr;
  _bufferPtr  = nullptr;
  return Base::onDetach(code);
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/assembler.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ASSEMBLER_H_INCLUDED
#define ASMJIT_CORE_ASSEMBLER_H_INCLUDED

#include "../core/codeholder.h"
#include "../core/emitter.h"
#include "../core/operand.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_assembler
//! \{

//! Base assembler.
//!
//! This is a base class that provides interface used by architecture specific assembler implementations. Assembler
//! doesn't hold any data, instead it's attached to \ref CodeHolder, which provides all the data that Assembler needs
//! and which can be altered by it.
//!
//! Check out architecture specific assemblers for more details and examples:
//!
//!   - \ref x86::Assembler - X86/X64 assembler implementation.
//!   - \ref a64::Assembler - AArch64 assembler implementation.
class ASMJIT_VIRTAPI BaseAssembler : public BaseEmitter {
public:
  ASMJIT_NONCOPYABLE(BaseAssembler)
  typedef BaseEmitter Base;

  //! Current section where the assembling happens.
  Section* _section = nullptr;
  //! Start of the CodeBuffer of the current section.
  uint8_t* _bufferData = nullptr;
  //! End (first invalid byte) of the current section.
  uint8_t* _bufferEnd = nullptr;
  //! Pointer in the CodeBuffer of the current section.
  uint8_t* _bufferPtr = nullptr;

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `BaseAssembler` instance.
  ASMJIT_API BaseAssembler() noexcept;
  //! Destroys the `BaseAssembler` instance.
  ASMJIT_API ~BaseAssembler() noexcept override;

  //! \}

  //! \name Code-Buffer Management
  //! \{

  //! Returns the capacity of the current CodeBuffer.
  ASMJIT_INLINE_NODEBUG size_t bufferCapacity() const noexcept { return (size_t)(_bufferEnd - _bufferData); }
  //! Returns the number of remaining bytes in the current CodeBuffer.
  ASMJIT_INLINE_NODEBUG size_t remainingSpace() const noexcept { return (size_t)(_bufferEnd - _bufferPtr); }

  //! Returns the current position in the CodeBuffer.
  ASMJIT_INLINE_NODEBUG size_t offset() const noexcept { return (size_t)(_bufferPtr - _bufferData); }

  //! Sets the current position in the CodeBuffer to `offset`.
  //!
  //! \note The `offset` cannot be greater than buffer size even if it's within the buffer's capacity.
  ASMJIT_API Error setOffset(size_t offset);

  //! Returns the start of the CodeBuffer in the current section.
  ASMJIT_INLINE_NODEBUG uint8_t* bufferData() const noexcept { return _bufferData; }
  //! Returns the end (first invalid byte) in the current section.
  ASMJIT_INLINE_NODEBUG uint8_t* bufferEnd() const noexcept { return _bufferEnd; }
  //! Returns the current pointer in the CodeBuffer in the current section.
  ASMJIT_INLINE_NODEBUG uint8_t* bufferPtr() const noexcept { return _bufferPtr; }

  //! \}

  //! \name Section Management
  //! \{

  //! Returns the current section.
  ASMJIT_INLINE_NODEBUG Section* currentSection() const noexcept { return _section; }

  ASMJIT_API Error section(Section* section) override;

  //! \}

  //! \name Label Management
  //! \{

  ASMJIT_API Label newLabel() override;
  ASMJIT_API Label newNamedLabel(const char* name, size_t nameSize = SIZE_MAX, LabelType type = LabelType::kGlobal, uint32_t parentId = Globals::kInvalidId) override;
  ASMJIT_API Error bind(const Label& label) override;

  //! \}

  //! \name Embed
  //! \{

  ASMJIT_API Error embed(const void* data, size_t dataSize) override;
  ASMJIT_API Error embedDataArray(TypeId typeId, const void* data, size_t itemCount, size_t repeatCount = 1) override;
  ASMJIT_API Error embedConstPool(const Label& label, const ConstPool& pool) override;

  ASMJIT_API Error embedLabel(const Label& label, size_t dataSize = 0) override;
  ASMJIT_API Error embedLabelDelta(const Label& label, const Label& base, size_t dataSize = 0) override;

  //! \}

  //! \name Comment
  //! \{

  ASMJIT_API Error comment(const char* data, size_t size = SIZE_MAX) override;

  //! \}

  //! \name Events
  //! \{

  ASMJIT_API Error onAttach(CodeHolder* code) noexcept override;
  ASMJIT_API Error onDetach(CodeHolder* code) noexcept override;

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ASSEMBLER_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/builder.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_BUILDER

#include "../core/builder.h"
#include "../core/emitterutils_p.h"
#include "../core/errorhandler.h"
#include "../core/formatter.h"
#include "../core/logger.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// PostponedErrorHandler (Internal)
// ================================

//! Postponed error handler that never throws. Used as a temporal error handler
//! to run passes. If error occurs, the caller is notified and will call the
//! real error handler, that can throw.
class PostponedErrorHandler : public ErrorHandler {
public:
  void handleError(Error err, const char* message, BaseEmitter* origin) override {
    DebugUtils::unused(err, origin);
    _message.assign(message);
  }

  StringTmp<128> _message;
};

// BaseBuilder - Utilities
// =======================

static void BaseBuilder_deletePasses(BaseBuilder* self) noexcept {
  for (Pass* pass : self->_passes)
    pass->~Pass();
  self->_passes.reset();
}

// BaseBuilder - Construction & Destruction
// ========================================

BaseBuilder::BaseBuilder() noexcept
  : BaseEmitter(EmitterType::kBuilder),
    _codeZone(32u * 1024u),
    _dataZone(16u * 1024u),
    _passZone(64u * 1024u),
    _allocator(&_codeZone) {}

BaseBuilder::~BaseBuilder() noexcept {
  BaseBuilder_deletePasses(this);
}

// BaseBuilder - Node Management
// =============================

Error BaseBuilder::newInstNode(InstNode** out, InstId instId, InstOptions instOptions, uint32_t opCount) {
  uint32_t opCapacity = InstNode::capacityOfOpCount(opCount);
  ASMJIT_ASSERT(opCapacity >= InstNode::kBaseOpCapacity);

  InstNode* node = _allocator.allocT<InstNode>(InstNode::nodeSizeOfOpCapacity(opCapacity));
  if (ASMJIT_UNLIKELY(!node))
    return reportError(DebugUtils::errored(kErrorOutOfMemory));

  *out = new(Support::PlacementNew{node}) InstNode(this, instId, instOptions, opCount, opCapacity);
  return kErrorOk;
}


Error BaseBuilder::newLabelNode(LabelNode** out) {
  *out = nullptr;

  ASMJIT_PROPAGATE(_newNodeT<LabelNode>(out));
  return registerLabelNode(*out);
}

Error BaseBuilder::newAlignNode(AlignNode** out, AlignMode alignMode, uint32_t alignment) {
  *out = nullptr;
  return _newNodeT<AlignNode>(out, alignMode, alignment);
}

Error BaseBuilder::newEmbedDataNode(EmbedDataNode** out, TypeId typeId, const void* data, size_t itemCount, size_t repeatCount) {
  *out = nullptr;

  uint32_t deabstractDelta = TypeUtils::deabstractDeltaOfSize(registerSize());
  TypeId finalTypeId = TypeUtils::deabstract(typeId, deabstractDelta);

  if (ASMJIT_UNLIKELY(!TypeUtils::isValid(finalTypeId)))
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  uint32_t typeSize = TypeUtils::sizeOf(finalTypeId);
  Support::FastUInt8 of = 0;

  size_t dataSize = Support::mulOverflow(itemCount, size_t(typeSize), &of);
  if (ASMJIT_UNLIKELY(of))
    return reportError(DebugUtils::errored(kErrorOutOfMemory));

  EmbedDataNode* node;
  ASMJIT_PROPAGATE(_newNodeT<EmbedDataNode>(&node));

  node->_embed._typeId = typeId;
  node->_embed._typeSize = uint8_t(typeSize);
  node->_itemCount = itemCount;
  node->_repeatCount = repeatCount;

  uint8_t* dstData = node->_inlineData;
  if (dataSize > EmbedDataNode::kInlineBufferSize) {
    dstData = static_cast<uint8_t*>(_dataZone.alloc(dataSize, 8));
    if (ASMJIT_UNLIKELY(!dstData))
      return reportError(DebugUtils::errored(kErrorOutOfMemory));
    node->_externalData = dstData;
  }

  if (data)
    memcpy(dstData, data, dataSize);

  *out = node;
  return kErrorOk;
}

Error BaseBuilder::newConstPoolNode(ConstPoolNode** out) {
  *out = nullptr;

  ASMJIT_PROPAGATE(_newNodeT<ConstPoolNode>(out));
  return registerLabelNode(*out);
}

Error BaseBuilder::newCommentNode(CommentNode** out, const char* data, size_t size) {
  *out = nullptr;

  if (data) {
    if (size == SIZE_MAX)
      size = strlen(data);

    if (size > 0) {
      data = static_cast<char*>(_dataZone.dup(data, size, true));
      if (ASMJIT_UNLIKELY(!data))
        return reportError(DebugUtils::errored(kErrorOutOfMemory));
    }
  }

  return _newNodeT<CommentNode>(out, data);
}

BaseNode* BaseBuilder::addNode(BaseNode* node) noexcept {
  ASMJIT_ASSERT(!node->_prev);
  ASMJIT_ASSERT(!node->_next);
  ASMJIT_ASSERT(!node->isActive());

  if (!_cursor) {
    if (_nodeList.empty()) {
      _nodeList.reset(node, node);
    }
    else {
      node->_next = _nodeList.first();
      _nodeList._first->_prev = node;
      _nodeList._first = node;
    }
  }
  else {
    BaseNode* prev = _cursor;
    BaseNode* next = _cursor->next();

    node->_prev = prev;
    node->_next = next;

    prev->_next = node;
    if (next)
      next->_prev = node;
    else
      _nodeList._last = node;
  }

  node->addFlags(NodeFlags::kIsActive);
  if (node->isSection())
    _dirtySectionLinks = true;

  _cursor = node;
  return node;
}

BaseNode* BaseBuilder::addAfter(BaseNode* node, BaseNode* ref) noexcept {
  ASMJIT_ASSERT(!node->_prev);
  ASMJIT_ASSERT(!node->_next);

  BaseNode* prev = ref;
  BaseNode* next = ref->next();

  node->_prev = prev;
  node->_next = next;

  node->addFlags(NodeFlags::kIsActive);
  if (node->isSection())
    _dirtySectionLinks = true;

  prev->_next = node;
  if (next)
    next->_prev = node;
  else
    _nodeList._last = node;

  return node;
}

BaseNode* BaseBuilder::addBefore(BaseNode* node, BaseNode* ref) noexcept {
  ASMJIT_ASSERT(!node->_prev);
  ASMJIT_ASSERT(!node->_next);
  ASMJIT_ASSERT(!node->isActive());
  ASMJIT_ASSERT(ref->isActive());

  BaseNode* prev = ref->prev();
  BaseNode* next = ref;

  node->_prev = prev;
  node->_next = next;

  node->addFlags(NodeFlags::kIsActive);
  if (node->isSection())
    _dirtySectionLinks = true;

  next->_prev = node;
  if (prev)
    prev->_next = node;
  else
    _nodeList._first = node;

  return node;
}

BaseNode* BaseBuilder::removeNode(BaseNode* node) noexcept {
  if (!node->isActive())
    return node;

  BaseNode* prev = node->prev();
  BaseNode* next = node->next();

  if (_nodeList._first == node)
    _nodeList._first = next;
  else
    prev->_next = next;

  if (_nodeList._last == node)
    _nodeList._last  = prev;
  else
    next->_prev = prev;

  node->_prev = nullptr;
  node->_next = nullptr;
  node->clearFlags(NodeFlags::kIsActive);
  if (node->isSection())
    _dirtySectionLinks = true;

  if (_cursor == node)
    _cursor = prev;

  return node;
}

void BaseBuilder::removeNodes(BaseNode* first, BaseNode* last) noexcept {
  if (first == last) {
    removeNode(first);
    return;
  }

  if (!first->isActive())
    return;

  BaseNode* prev = first->prev();
  BaseNode* next = last->next();

  if (_nodeList._first == first)
    _nodeList._first = next;
  else
    prev->_next = next;

  if (_nodeList._last == last)
    _nodeList._last  = prev;
  else
    next->_prev = prev;

  BaseNode* node = first;
  uint32_t didRemoveSection = false;

  for (;;) {
    next = node->next();
    ASMJIT_ASSERT(next != nullptr);

    node->_prev = nullptr;
    node->_next = nullptr;
    node->clearFlags(NodeFlags::kIsActive);
    didRemoveSection |= uint32_t(node->isSection());

    if (_cursor == node)
      _cursor = prev;

    if (node == last)
      break;
    node = next;
  }

  if (didRemoveSection)
    _dirtySectionLinks = true;
}

BaseNode* BaseBuilder::setCursor(BaseNode* node) noexcept {
  BaseNode* old = _cursor;
  _cursor = node;
  return old;
}

// BaseBuilder - Sections
// ======================

Error BaseBuilder::sectionNodeOf(SectionNode** out, uint32_t sectionId) {
  *out = nullptr;

  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  if (ASMJIT_UNLIKELY(!_code->isSectionValid(sectionId)))
    return reportError(DebugUtils::errored(kErrorInvalidSection));

  if (sectionId >= _sectionNodes.size()) {
    Error err = _sectionNodes.reserve(&_allocator, sectionId + 1);
    if (ASMJIT_UNLIKELY(err != kErrorOk))
      return reportError(err);
  }

  SectionNode* node = nullptr;
  if (sectionId < _sectionNodes.size())
    node = _sectionNodes[sectionId];

  if (!node) {
    ASMJIT_PROPAGATE(_newNodeT<SectionNode>(&node, sectionId));

    // We have already reserved enough space, this cannot fail now.
    if (sectionId >= _sectionNodes.size())
      _sectionNodes.resize(&_allocator, sectionId + 1);

    _sectionNodes[sectionId] = node;
  }

  *out = node;
  return kErrorOk;
}

Error BaseBuilder::section(Section* section) {
  SectionNode* node;
  ASMJIT_PROPAGATE(sectionNodeOf(&node, section->id()));
  ASMJIT_ASSUME(node != nullptr);

  if (!node->isActive()) {
    // Insert the section at the end if it was not part of the code.
    addAfter(node, lastNode());
    _cursor = node;
  }
  else {
    // This is a bit tricky. We cache section links to make sure that
    // switching sections doesn't involve traversal in linked-list unless
    // the position of the section has changed.
    if (hasDirtySectionLinks())
      updateSectionLinks();

    if (node->_nextSection)
      _cursor = node->_nextSection->_prev;
    else
      _cursor = _nodeList.last();
  }

  return kErrorOk;
}

void BaseBuilder::updateSectionLinks() noexcept {
  if (!_dirtySectionLinks)
    return;

  BaseNode* node_ = _nodeList.first();
  SectionNode* currentSection = nullptr;

  while (node_) {
    if (node_->isSection()) {
      if (currentSection)
        currentSection->_nextSection = node_->as<SectionNode>();
      currentSection = node_->as<SectionNode>();
    }
    node_ = node_->next();
  }

  if (currentSection)
    currentSection->_nextSection = nullptr;

  _dirtySectionLinks = false;
}

// BaseBuilder - Labels
// ====================

Error BaseBuilder::labelNodeOf(LabelNode** out, uint32_t labelId) {
  *out = nullptr;

  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  uint32_t index = labelId;
  if (ASMJIT_UNLIKELY(index >= _code->labelCount()))
    return DebugUtils::errored(kErrorInvalidLabel);

  if (index >= _labelNodes.size())
    ASMJIT_PROPAGATE(_labelNodes.resize(&_allocator, index + 1));

  LabelNode* node = _labelNodes[index];
  if (!node) {
    ASMJIT_PROPAGATE(_newNodeT<LabelNode>(&node, labelId));
    _labelNodes[index] = node;
  }

  *out = node;
  return kErrorOk;
}

Error BaseBuilder::registerLabelNode(LabelNode* node) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  LabelEntry* le;
  ASMJIT_PROPAGATE(_code->newLabelEntry(&le));
  uint32_t labelId = le->id();

  // We just added one label so it must be true.
  ASMJIT_ASSERT(_labelNodes.size() < labelId + 1);
  ASMJIT_PROPAGATE(_labelNodes.resize(&_allocator, labelId + 1));

  _labelNodes[labelId] = node;
  node->_labelId = labelId;

  return kErrorOk;
}

static Error BaseBuilder_newLabelInternal(BaseBuilder* self, uint32_t labelId) {
  ASMJIT_ASSERT(self->_labelNodes.size() < labelId + 1);

  uint32_t growBy = labelId - self->_labelNodes.size();
  Error err = self->_labelNodes.willGrow(&self->_allocator, growBy);

  if (ASMJIT_UNLIKELY(err))
    return self->reportError(err);

  LabelNode* node;
  ASMJIT_PROPAGATE(self->_newNodeT<LabelNode>(&node, labelId));

  self->_labelNodes.resize(&self->_allocator, labelId + 1);
  self->_labelNodes[labelId] = node;
  node->_labelId = labelId;
  return kErrorOk;
}

Label BaseBuilder::newLabel() {
  uint32_t labelId = Globals::kInvalidId;
  LabelEntry* le;

  if (_code &&
      _code->newLabelEntry(&le) == kErrorOk &&
      BaseBuilder_newLabelInternal(this, le->id()) == kErrorOk) {
    labelId = le->id();
  }

  return Label(labelId);
}

Label BaseBuilder::newNamedLabel(const char* name, size_t nameSize, LabelType type, uint32_t parentId) {
  uint32_t labelId = Globals::kInvalidId;
  LabelEntry* le;

  if (_code &&
      _code->newNamedLabelEntry(&le, name, nameSize, type, parentId) == kErrorOk &&
      BaseBuilder_newLabelInternal(this, le->id()) == kErrorOk) {
    labelId = le->id();
  }

  return Label(labelId);
}

Error BaseBuilder::bind(const Label& label) {
  LabelNode* node;
  ASMJIT_PROPAGATE(labelNodeOf(&node, label));

  addNode(node);
  return kErrorOk;
}

// BaseBuilder - Passes
// ====================

ASMJIT_FAVOR_SIZE Pass* BaseBuilder::passByName(const char* name) const noexcept {
  for (Pass* pass : _passes)
    if (strcmp(pass->name(), name) == 0)
      return pass;
  return nullptr;
}

ASMJIT_FAVOR_SIZE Error BaseBuilder::addPass(Pass* pass) noexcept {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  if (ASMJIT_UNLIKELY(pass == nullptr)) {
    // Since this is directly called by `addPassT()` we treat `null` argument
    // as out-of-memory condition. Otherwise it would be API misuse.
    return DebugUtils::errored(kErrorOutOfMemory);
  }
  else if (ASMJIT_UNLIKELY(pass->_cb)) {
    // Kinda weird, but okay...
    if (pass->_cb == this)
      return kErrorOk;
    return DebugUtils::errored(kErrorInvalidState);
  }

  ASMJIT_PROPAGATE(_passes.append(&_allocator, pass));
  pass->_cb = this;
  return kErrorOk;
}

ASMJIT_FAVOR_SIZE Error BaseBuilder::deletePass(Pass* pass) noexcept {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  if (ASMJIT_UNLIKELY(pass == nullptr))
    return DebugUtils::errored(kErrorInvalidArgument);

  if (pass->_cb != nullptr) {
    if (pass->_cb != this)
      return DebugUtils::errored(kErrorInvalidState);

    uint32_t index = _passes.indexOf(pass);
    ASMJIT_ASSERT(index != Globals::kNotFound);

    pass->_cb = nullptr;
    _passes.removeAt(index);
  }

  pass->~Pass();
  return kErrorOk;
}

Error BaseBuilder::runPasses() {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  if (_passes.empty())
    return kErrorOk;

  ErrorHandler* prev = errorHandler();
  PostponedErrorHandler postponed;

  Error err = kErrorOk;
  setErrorHandler(&postponed);

  for (Pass* pass : _passes) {
    _passZone.reset();
    err = pass->run(&_passZone, _logger);
    if (err)
      break;
  }
  _passZone.reset();
  setErrorHandler(prev);

  if (ASMJIT_UNLIKELY(err))
    return reportError(err, !postponed._message.empty() ? postponed._message.data() : nullptr);

  return kErrorOk;
}

// BaseBuilder - Emit
// ==================

Error BaseBuilder::_emit(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt) {
  uint32_t opCount = EmitterUtils::opCountFromEmitArgs(o0, o1, o2, opExt);
  InstOptions options = instOptions() | forcedInstOptions();

  if (Support::test(options, InstOptions::kReserved)) {
    if (ASMJIT_UNLIKELY(!_code))
      return DebugUtils::errored(kErrorNotInitialized);

#ifndef ASMJIT_NO_VALIDATION
    // Strict validation.
    if (hasDiagnosticOption(DiagnosticOptions::kValidateIntermediate)) {
      Operand_ opArray[Globals::kMaxOpCount];
      EmitterUtils::opArrayFromEmitArgs(opArray, o0, o1, o2, opExt);

      ValidationFlags validationFlags = isCompiler() ? ValidationFlags::kEnableVirtRegs : ValidationFlags::kNone;
      Error err = _funcs.validate(BaseInst(instId, options, _extraReg), opArray, opCount, validationFlags);

      if (ASMJIT_UNLIKELY(err)) {
#ifndef ASMJIT_NO_LOGGING
        return EmitterUtils::logInstructionFailed(this, err, instId, options, o0, o1, o2, opExt);
#else
        resetState();
        return reportError(err);
#endif
      }
    }
#endif

    // Clear instruction options that should never be part of a regular instruction.
    options &= ~InstOptions::kReserved;
  }

  uint32_t opCapacity = InstNode::capacityOfOpCount(opCount);
  ASMJIT_ASSERT(opCapacity >= InstNode::kBaseOpCapacity);

  InstNode* node = _allocator.allocT<InstNode>(InstNode::nodeSizeOfOpCapacity(opCapacity));
  const char* comment = inlineComment();

  resetInstOptions();
  resetInlineComment();

  if (ASMJIT_UNLIKELY(!node)) {
    resetExtraReg();
    return reportError(DebugUtils::errored(kErrorOutOfMemory));
  }

  node = new(Support::PlacementNew{node}) InstNode(this, instId, options, opCount, opCapacity);
  node->setExtraReg(extraReg());
  node->setOp(0, o0);
  node->setOp(1, o1);
  node->setOp(2, o2);
  for (uint32_t i = 3; i < opCount; i++)
    node->setOp(i, opExt[i - 3]);
  node->resetOpRange(opCount, opCapacity);

  if (comment)
    node->setInlineComment(static_cast<char*>(_dataZone.dup(comment, strlen(comment), true)));

  addNode(node);
  resetExtraReg();
  return kErrorOk;
}

// BaseBuilder - Align
// ===================

Error BaseBuilder::align(AlignMode alignMode, uint32_t alignment) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  AlignNode* node;
  ASMJIT_PROPAGATE(newAlignNode(&node, alignMode, alignment));
  ASMJIT_ASSUME(node != nullptr);

  addNode(node);
  return kErrorOk;
}

// BaseBuilder - Embed
// ===================

Error BaseBuilder::embed(const void* data, size_t dataSize) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  EmbedDataNode* node;
  ASMJIT_PROPAGATE(newEmbedDataNode(&node, TypeId::kUInt8, data, dataSize));
  ASMJIT_ASSUME(node != nullptr);

  addNode(node);
  return kErrorOk;
}

Error BaseBuilder::embedDataArray(TypeId typeId, const void* data, size_t itemCount, size_t itemRepeat) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  EmbedDataNode* node;
  ASMJIT_PROPAGATE(newEmbedDataNode(&node, typeId, data, itemCount, itemRepeat));
  ASMJIT_ASSUME(node != nullptr);

  addNode(node);
  return kErrorOk;
}

Error BaseBuilder::embedConstPool(const Label& label, const ConstPool& pool) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  if (!isLabelValid(label))
    return reportError(DebugUtils::errored(kErrorInvalidLabel));

  ASMJIT_PROPAGATE(align(AlignMode::kData, uint32_t(pool.alignment())));
  ASMJIT_PROPAGATE(bind(label));

  EmbedDataNode* node;
  ASMJIT_PROPAGATE(newEmbedDataNode(&node, TypeId::kUInt8, nullptr, pool.size()));
  ASMJIT_ASSUME(node != nullptr);

  pool.fill(node->data());
  addNode(node);
  return kErrorOk;
}

// BaseBuilder - EmbedLabel & EmbedLabelDelta
// ==========================================
//
// If dataSize is zero it means that the size is the same as target register width, however,
// if it's provided we really want to validate whether it's within the possible range.

static inline bool BaseBuilder_checkDataSize(size_t dataSize) noexcept {
  return !dataSize || (Support::isPowerOf2(dataSize) && dataSize <= 8);
}

Error BaseBuilder::embedLabel(const Label& label, size_t dataSize) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  if (!BaseBuilder_checkDataSize(dataSize))
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  EmbedLabelNode* node;
  ASMJIT_PROPAGATE(_newNodeT<EmbedLabelNode>(&node, label.id(), uint32_t(dataSize)));

  addNode(node);
  return kErrorOk;
}

Error BaseBuilder::embedLabelDelta(const Label& label, const Label& base, size_t dataSize) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  if (!BaseBuilder_checkDataSize(dataSize))
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  EmbedLabelDeltaNode* node;
  ASMJIT_PROPAGATE(_newNodeT<EmbedLabelDeltaNode>(&node, label.id(), base.id(), uint32_t(dataSize)));

  addNode(node);
  return kErrorOk;
}

// BaseBuilder - Comment
// =====================

Error BaseBuilder::comment(const char* data, size_t size) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  CommentNode* node;
  ASMJIT_PROPAGATE(newCommentNode(&node, data, size));
  ASMJIT_ASSUME(node != nullptr);

  addNode(node);
  return kErrorOk;
}

// BaseBuilder - SerializeTo
// =========================

Error BaseBuilder::serializeTo(BaseEmitter* dst) {
  Error err = kErrorOk;
  BaseNode* node_ = _nodeList.first();

  Operand_ opArray[Globals::kMaxOpCount];

  do {
    dst->setInlineComment(node_->inlineComment());

    if (node_->isInst()) {
      InstNode* node = node_->as<InstNode>();

      // NOTE: Inlined to remove one additional call per instruction.
      dst->setInstOptions(node->options());
      dst->setExtraReg(node->extraReg());

      const Operand_* op = node->operands();
      const Operand_* opExt = EmitterUtils::noExt;

      uint32_t opCount = node->opCount();
      if (opCount > 3) {
        uint32_t i = 4;
        opArray[3] = op[3];

        while (i < opCount) {
          opArray[i].copyFrom(op[i]);
          i++;
        }
        while (i < Globals::kMaxOpCount) {
          opArray[i].reset();
          i++;
        }
        opExt = opArray + 3;
      }

      err = dst->_emit(node->id(), op[0], op[1], op[2], opExt);
    }
    else if (node_->isLabel()) {
      if (node_->isConstPool()) {
        ConstPoolNode* node = node_->as<ConstPoolNode>();
        err = dst->embedConstPool(node->label(), node->constPool());
      }
      else {
        LabelNode* node = node_->as<LabelNode>();
        err = dst->bind(node->label());
      }
    }
    else if (node_->isAlign()) {
      AlignNode* node = node_->as<AlignNode>();
      err = dst->align(node->alignMode(), node->alignment());
    }
    else if (node_->isEmbedData()) {
      EmbedDataNode* node = node_->as<EmbedDataNode>();
      err = dst->embedDataArray(node->typeId(), node->data(), node->itemCount(), node->repeatCount());
    }
    else if (node_->isEmbedLabel()) {
      EmbedLabelNode* node = node_->as<EmbedLabelNode>();
      err = dst->embedLabel(node->label(), node->dataSize());
    }
    else if (node_->isEmbedLabelDelta()) {
      EmbedLabelDeltaNode* node = node_->as<EmbedLabelDeltaNode>();
      err = dst->embedLabelDelta(node->label(), node->baseLabel(), node->dataSize());
    }
    else if (node_->isSection()) {
      SectionNode* node = node_->as<SectionNode>();
      err = dst->section(_code->sectionById(node->id()));
    }
    else if (node_->isComment()) {
      CommentNode* node = node_->as<CommentNode>();
      err = dst->comment(node->inlineComment());
    }

    if (err) break;
    node_ = node_->next();
  } while (node_);

  return err;
}

// BaseBuilder - Events
// ====================

Error BaseBuilder::onAttach(CodeHolder* code) noexcept {
  ASMJIT_PROPAGATE(Base::onAttach(code));

  SectionNode* initialSection;
  Error err = sectionNodeOf(&initialSection, 0);

  if (!err)
    err = _passes.willGrow(&_allocator, 8);

  if (ASMJIT_UNLIKELY(err)) {
    onDetach(code);
    return err;
  }

  ASMJIT_ASSUME(initialSection != nullptr);
  _cursor = initialSection;
  _nodeList.reset(initialSection, initialSection);
  initialSection->setFlags(NodeFlags::kIsActive);

  return kErrorOk;
}

Error BaseBuilder::onDetach(CodeHolder* code) noexcept {
  BaseBuilder_deletePasses(this);
  _sectionNodes.reset();
  _labelNodes.reset();

  _allocator.reset(&_codeZone);
  _codeZone.reset();
  _dataZone.reset();
  _passZone.reset();

  _nodeFlags = NodeFlags::kNone;
  _cursor = nullptr;
  _nodeList.reset();

  return Base::onDetach(code);
}

// Pass - Construction & Destruction
// =================================

Pass::Pass(const char* name) noexcept
  : _name(name) {}
Pass::~Pass() noexcept {}

// Pass - Interface
// ================

// [[pure virtual]]
Error Pass::run(Zone* zone, Logger* logger) {
  DebugUtils::unused(zone, logger);
  return DebugUtils::errored(kErrorInvalidState);
}

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_BUILDER

```

`CodeCleaner/asmjit/asmjit/core/builder.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_BUILDER_H_INCLUDED
#define ASMJIT_CORE_BUILDER_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_BUILDER

#include "../core/assembler.h"
#include "../core/codeholder.h"
#include "../core/constpool.h"
#include "../core/formatter.h"
#include "../core/inst.h"
#include "../core/operand.h"
#include "../core/string.h"
#include "../core/support.h"
#include "../core/type.h"
#include "../core/zone.h"
#include "../core/zonevector.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_builder
//! \{

class BaseBuilder;
class Pass;

class BaseNode;
class InstNode;
class SectionNode;
class LabelNode;
class AlignNode;
class EmbedDataNode;
class EmbedLabelNode;
class ConstPoolNode;
class CommentNode;
class SentinelNode;
class LabelDeltaNode;

//! Type of node used by \ref BaseBuilder and \ref BaseCompiler.
enum class NodeType : uint8_t {
  //! Invalid node (internal, don't use).
  kNone = 0,

  // [BaseBuilder]

  //! Node is \ref InstNode.
  kInst = 1,
  //! Node is \ref SectionNode.
  kSection = 2,
  //! Node is \ref LabelNode.
  kLabel = 3,
  //! Node is \ref AlignNode.
  kAlign = 4,
  //! Node is \ref EmbedDataNode.
  kEmbedData = 5,
  //! Node is \ref EmbedLabelNode.
  kEmbedLabel = 6,
  //! Node is \ref EmbedLabelDeltaNode.
  kEmbedLabelDelta = 7,
  //! Node is \ref ConstPoolNode.
  kConstPool = 8,
  //! Node is \ref CommentNode.
  kComment = 9,
  //! Node is \ref SentinelNode.
  kSentinel = 10,

  // [BaseCompiler]

  //! Node is \ref JumpNode (acts as InstNode).
  kJump = 15,
  //! Node is \ref FuncNode (acts as LabelNode).
  kFunc = 16,
  //! Node is \ref FuncRetNode (acts as InstNode).
  kFuncRet = 17,
  //! Node is \ref InvokeNode (acts as InstNode).
  kInvoke = 18,

  // [UserDefined]

  //! First id of a user-defined node.
  kUser = 32
};

//! Node flags, specify what the node is and/or does.
enum class NodeFlags : uint8_t {
  //! No flags.
  kNone = 0,
  //! Node is code that can be executed (instruction, label, align, etc...).
  kIsCode = 0x01u,
  //! Node is data that cannot be executed (data, const-pool, etc...).
  kIsData = 0x02u,
  //! Node is informative, can be removed and ignored.
  kIsInformative = 0x04u,
  //! Node can be safely removed if unreachable.
  kIsRemovable = 0x08u,
  //! Node does nothing when executed (label, align, explicit nop).
  kHasNoEffect = 0x10u,
  //! Node is an instruction or acts as it.
  kActsAsInst = 0x20u,
  //! Node is a label or acts as it.
  kActsAsLabel = 0x40u,
  //! Node is active (part of the code).
  kIsActive = 0x80u
};
ASMJIT_DEFINE_ENUM_FLAGS(NodeFlags)

//! Type of the sentinel (purely informative purpose).
enum class SentinelType : uint8_t {
  //! Type of the sentinel is not known.
  kUnknown = 0u,
  //! This is a sentinel used at the end of \ref FuncNode.
  kFuncEnd = 1u
};

//! Node list.
//!
//! A double-linked list of pointers to \ref BaseNode, managed by \ref BaseBuilder or \ref BaseCompiler.
//!
//! \note At the moment NodeList is just a view, but it's planned that it will get more functionality in the future.
class NodeList {
public:
  //! \name Members
  //! \{

  //! First node in the list or nullptr if there are no nodes in the list.
  BaseNode* _first = nullptr;
  //! Last node in the list or nullptr if there are no nodes in the list.
  BaseNode* _last = nullptr;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG NodeList() noexcept {}

  ASMJIT_INLINE_NODEBUG NodeList(BaseNode* first, BaseNode* last) noexcept
    : _first(first),
      _last(last) {}

  //! \}

  //! \name Reset
  //! \{

  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _first = nullptr;
    _last = nullptr;
  }

  ASMJIT_INLINE_NODEBUG void reset(BaseNode* first, BaseNode* last) noexcept {
    _first = first;
    _last = last;
  }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _first == nullptr; }

  ASMJIT_INLINE_NODEBUG BaseNode* first() const noexcept { return _first; }
  ASMJIT_INLINE_NODEBUG BaseNode* last() const noexcept { return _last; }

  //! \}
};

//! Builder interface.
//!
//! `BaseBuilder` interface was designed to be used as a \ref BaseAssembler replacement in case pre-processing or
//! post-processing of the generated code is required. The code can be modified during or after code generation.
//! Pre processing or post processing can be done manually or through a \ref Pass object. \ref BaseBuilder stores
//! the emitted code as a double-linked list of nodes, which allows O(1) insertion and removal during processing.
//!
//! Check out architecture specific builders for more details and examples:
//!
//!   - \ref x86::Builder - X86/X64 builder implementation.
//!   - \ref a64::Builder - AArch64 builder implementation.
class ASMJIT_VIRTAPI BaseBuilder : public BaseEmitter {
public:
  ASMJIT_NONCOPYABLE(BaseBuilder)
  typedef BaseEmitter Base;

  //! \name Members
  //! \{

  //! Base zone used to allocate nodes and passes.
  Zone _codeZone;
  //! Data zone used to allocate data and names.
  Zone _dataZone;
  //! Pass zone, passed to `Pass::run()`.
  Zone _passZone;
  //! Allocator that uses `_codeZone`.
  ZoneAllocator _allocator;

  //! Array of `Pass` objects.
  ZoneVector<Pass*> _passes {};
  //! Maps section indexes to `LabelNode` nodes.
  ZoneVector<SectionNode*> _sectionNodes {};
  //! Maps label indexes to `LabelNode` nodes.
  ZoneVector<LabelNode*> _labelNodes {};

  //! Current node (cursor).
  BaseNode* _cursor = nullptr;
  //! First and last nodes.
  NodeList _nodeList;

  //! Flags assigned to each new node.
  NodeFlags _nodeFlags = NodeFlags::kNone;
  //! The sections links are dirty (used internally).
  bool _dirtySectionLinks = false;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `BaseBuilder` instance.
  ASMJIT_API BaseBuilder() noexcept;
  //! Destroys the `BaseBuilder` instance.
  ASMJIT_API ~BaseBuilder() noexcept override;

  //! \}

  //! \name Node Management
  //! \{

  ASMJIT_INLINE_NODEBUG NodeList nodeList() const noexcept { return _nodeList; }

  //! Returns the first node.
  ASMJIT_INLINE_NODEBUG BaseNode* firstNode() const noexcept { return _nodeList.first(); }
  //! Returns the last node.
  ASMJIT_INLINE_NODEBUG BaseNode* lastNode() const noexcept { return _nodeList.last(); }

  //! Allocates and instantiates a new node of type `T` and returns its instance. If the allocation fails `nullptr`
  //! is returned.
  //!
  //! The template argument `T` must be a type that is extends \ref BaseNode.
  //!
  //! \remarks The pointer returned (if non-null) is owned by the Builder or Compiler. When the Builder/Compiler
  //! is destroyed it destroys all nodes it created so no manual memory management is required.
  template<typename T, typename... Args>
  inline Error _newNodeT(T** ASMJIT_NONNULL(out), Args&&... args) {
    *out = _allocator.newT<T>(this, std::forward<Args>(args)...);
    if (ASMJIT_UNLIKELY(!*out))
      return reportError(DebugUtils::errored(kErrorOutOfMemory));
    return kErrorOk;
  }

  //! Creates a new \ref InstNode.
  ASMJIT_API Error newInstNode(InstNode** ASMJIT_NONNULL(out), InstId instId, InstOptions instOptions, uint32_t opCount);
  //! Creates a new \ref LabelNode.
  ASMJIT_API Error newLabelNode(LabelNode** ASMJIT_NONNULL(out));
  //! Creates a new \ref AlignNode.
  ASMJIT_API Error newAlignNode(AlignNode** ASMJIT_NONNULL(out), AlignMode alignMode, uint32_t alignment);
  //! Creates a new \ref EmbedDataNode.
  ASMJIT_API Error newEmbedDataNode(EmbedDataNode** ASMJIT_NONNULL(out), TypeId typeId, const void* data, size_t itemCount, size_t repeatCount = 1);
  //! Creates a new \ref ConstPoolNode.
  ASMJIT_API Error newConstPoolNode(ConstPoolNode** ASMJIT_NONNULL(out));
  //! Creates a new \ref CommentNode.
  ASMJIT_API Error newCommentNode(CommentNode** ASMJIT_NONNULL(out), const char* data, size_t size);

  //! Adds `node` after the current and sets the current node to the given `node`.
  ASMJIT_API BaseNode* addNode(BaseNode* ASMJIT_NONNULL(node)) noexcept;
  //! Inserts the given `node` after `ref`.
  ASMJIT_API BaseNode* addAfter(BaseNode* ASMJIT_NONNULL(node), BaseNode* ASMJIT_NONNULL(ref)) noexcept;
  //! Inserts the given `node` before `ref`.
  ASMJIT_API BaseNode* addBefore(BaseNode* ASMJIT_NONNULL(node), BaseNode* ASMJIT_NONNULL(ref)) noexcept;
  //! Removes the given `node`.
  ASMJIT_API BaseNode* removeNode(BaseNode* ASMJIT_NONNULL(node)) noexcept;
  //! Removes multiple nodes.
  ASMJIT_API void removeNodes(BaseNode* first, BaseNode* last) noexcept;

  //! Returns the cursor.
  //!
  //! When the Builder/Compiler is created it automatically creates a '.text' \ref SectionNode, which will be the
  //! initial one. When instructions are added they are always added after the cursor and the cursor is changed
  //! to be that newly added node. Use `setCursor()` to change where new nodes are inserted.
  ASMJIT_INLINE_NODEBUG BaseNode* cursor() const noexcept { return _cursor; }

  //! Sets the current node to `node` and return the previous one.
  ASMJIT_API BaseNode* setCursor(BaseNode* node) noexcept;

  //! Sets the current node without returning the previous node.
  //!
  //! Only use this function if you are concerned about performance and want this inlined (for example if you set
  //! the cursor in a loop, etc...).
  ASMJIT_INLINE_NODEBUG void _setCursor(BaseNode* node) noexcept { _cursor = node; }

  //! \}

  //! \name Section Management
  //! \{

  //! Returns a vector of SectionNode objects.
  //!
  //! \note If a section of some id is not associated with the Builder/Compiler it would be null, so always check
  //! for nulls if you iterate over the vector.
  ASMJIT_INLINE_NODEBUG const ZoneVector<SectionNode*>& sectionNodes() const noexcept {
    return _sectionNodes;
  }

  //! Tests whether the `SectionNode` of the given `sectionId` was registered.
  ASMJIT_INLINE_NODEBUG bool hasRegisteredSectionNode(uint32_t sectionId) const noexcept {
    return sectionId < _sectionNodes.size() && _sectionNodes[sectionId] != nullptr;
  }

  //! Returns or creates a `SectionNode` that matches the given `sectionId`.
  //!
  //! \remarks This function will either get the existing `SectionNode` or create it in case it wasn't created before.
  //! You can check whether a section has a registered `SectionNode` by using `BaseBuilder::hasRegisteredSectionNode()`.
  ASMJIT_API Error sectionNodeOf(SectionNode** ASMJIT_NONNULL(out), uint32_t sectionId);

  ASMJIT_API Error section(Section* ASMJIT_NONNULL(section)) override;

  //! Returns whether the section links of active section nodes are dirty. You can update these links by calling
  //! `updateSectionLinks()` in such case.
  ASMJIT_INLINE_NODEBUG bool hasDirtySectionLinks() const noexcept { return _dirtySectionLinks; }

  //! Updates links of all active section nodes.
  ASMJIT_API void updateSectionLinks() noexcept;

  //! \}

  //! \name Label Management
  //! \{

  //! Returns a vector of \ref LabelNode nodes.
  //!
  //! \note If a label of some id is not associated with the Builder/Compiler it would be null, so always check for
  //! nulls if you iterate over the vector.
  ASMJIT_INLINE_NODEBUG const ZoneVector<LabelNode*>& labelNodes() const noexcept { return _labelNodes; }

  //! Tests whether the `LabelNode` of the given `labelId` was registered.
  ASMJIT_INLINE_NODEBUG bool hasRegisteredLabelNode(uint32_t labelId) const noexcept {
    return labelId < _labelNodes.size() && _labelNodes[labelId] != nullptr;
  }

  //! \overload
  ASMJIT_INLINE_NODEBUG bool hasRegisteredLabelNode(const Label& label) const noexcept {
    return hasRegisteredLabelNode(label.id());
  }

  //! Gets or creates a \ref LabelNode that matches the given `labelId`.
  //!
  //! \remarks This function will either get the existing `LabelNode` or create it in case it wasn't created before.
  //! You can check whether a label has a registered `LabelNode` by calling \ref BaseBuilder::hasRegisteredLabelNode().
  ASMJIT_API Error labelNodeOf(LabelNode** ASMJIT_NONNULL(out), uint32_t labelId);

  //! \overload
  ASMJIT_INLINE_NODEBUG Error labelNodeOf(LabelNode** ASMJIT_NONNULL(out), const Label& label) {
    return labelNodeOf(out, label.id());
  }

  //! Registers this \ref LabelNode (internal).
  //!
  //! This function is used internally to register a newly created `LabelNode` with this instance of Builder/Compiler.
  //! Use \ref labelNodeOf() functions to get back \ref LabelNode from a label or its identifier.
  ASMJIT_API Error registerLabelNode(LabelNode* ASMJIT_NONNULL(node));

  ASMJIT_API Label newLabel() override;
  ASMJIT_API Label newNamedLabel(const char* name, size_t nameSize = SIZE_MAX, LabelType type = LabelType::kGlobal, uint32_t parentId = Globals::kInvalidId) override;
  ASMJIT_API Error bind(const Label& label) override;

  //! \}

  //! \name Passes
  //! \{

  //! Returns a vector of `Pass` instances that will be executed by `runPasses()`.
  ASMJIT_INLINE_NODEBUG const ZoneVector<Pass*>& passes() const noexcept { return _passes; }

  //! Allocates and instantiates a new pass of type `T` and returns its instance. If the allocation fails `nullptr` is
  //! returned.
  //!
  //! The template argument `T` must be a type that is extends \ref Pass.
  //!
  //! \remarks The pointer returned (if non-null) is owned by the Builder or Compiler. When the Builder/Compiler is
  //! destroyed it destroys all passes it created so no manual memory management is required.
  template<typename T>
  inline T* newPassT() noexcept { return _codeZone.newT<T>(); }

  //! \overload
  template<typename T, typename... Args>
  inline T* newPassT(Args&&... args) noexcept { return _codeZone.newT<T>(std::forward<Args>(args)...); }

  template<typename T>
  inline Error addPassT() { return addPass(newPassT<T>()); }

  template<typename T, typename... Args>
  inline Error addPassT(Args&&... args) { return addPass(newPassT<T, Args...>(std::forward<Args>(args)...)); }

  //! Returns `Pass` by name.
  //!
  //! If the pass having the given `name` doesn't exist `nullptr` is returned.
  ASMJIT_API Pass* passByName(const char* name) const noexcept;
  //! Adds `pass` to the list of passes.
  ASMJIT_API Error addPass(Pass* pass) noexcept;
  //! Removes `pass` from the list of passes and delete it.
  ASMJIT_API Error deletePass(Pass* pass) noexcept;

  //! Runs all passes in order.
  ASMJIT_API Error runPasses();

  //! \}

  //! \name Emit
  //! \{

  ASMJIT_API Error _emit(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt) override;

  //! \}

  //! \name Align
  //! \{

  ASMJIT_API Error align(AlignMode alignMode, uint32_t alignment) override;

  //! \}

  //! \name Embed
  //! \{

  ASMJIT_API Error embed(const void* data, size_t dataSize) override;
  ASMJIT_API Error embedDataArray(TypeId typeId, const void* data, size_t count, size_t repeat = 1) override;
  ASMJIT_API Error embedConstPool(const Label& label, const ConstPool& pool) override;

  ASMJIT_API Error embedLabel(const Label& label, size_t dataSize = 0) override;
  ASMJIT_API Error embedLabelDelta(const Label& label, const Label& base, size_t dataSize = 0) override;

  //! \}

  //! \name Comment
  //! \{

  ASMJIT_API Error comment(const char* data, size_t size = SIZE_MAX) override;

  //! \}

  //! \name Serialization
  //! \{

  //! Serializes everything the given emitter `dst`.
  //!
  //! Although not explicitly required the emitter will most probably be of Assembler type. The reason is that
  //! there is no known use of serializing nodes held by Builder/Compiler into another Builder-like emitter.
  ASMJIT_API Error serializeTo(BaseEmitter* dst);

  //! \}

  //! \name Events
  //! \{

  ASMJIT_API Error onAttach(CodeHolder* code) noexcept override;
  ASMJIT_API Error onDetach(CodeHolder* code) noexcept override;

  //! \}
};

//! Base node.
//!
//! Every node represents a building-block used by \ref BaseBuilder. It can be instruction, data, label, comment,
//! directive, or any other high-level representation that can be transformed to the building blocks mentioned.
//! Every class that inherits \ref BaseBuilder can define its own high-level nodes that can be later lowered to
//! basic nodes like instructions.
class BaseNode {
public:
  ASMJIT_NONCOPYABLE(BaseNode)

  //! \name Members
  //! \{

  union {
    struct {
      //! Previous node.
      BaseNode* _prev;
      //! Next node.
      BaseNode* _next;
    };
    //! Links (an alternative view to previous and next nodes).
    BaseNode* _links[2];
  };

  //! Data shared between all types of nodes.
  struct AnyData {
    //! Node type.
    NodeType _nodeType;
    //! Node flags.
    NodeFlags _nodeFlags;
    //! Not used by BaseNode.
    uint8_t _reserved0;
    //! Not used by BaseNode.
    uint8_t _reserved1;
  };

  //! Data used by \ref AlignNode.
  struct AlignData {
    //! Node type.
    NodeType _nodeType;
    //! Node flags.
    NodeFlags _nodeFlags;
    //! Align mode.
    AlignMode _alignMode;
    //! Not used by AlignNode.
    uint8_t _reserved;
  };

  //! Data used by \ref InstNode.
  struct InstData {
    //! Node type.
    NodeType _nodeType;
    //! Node flags.
    NodeFlags _nodeFlags;
    //! Instruction operands count (used).
    uint8_t _opCount;
    //! Instruction operands capacity (allocated).
    uint8_t _opCapacity;
  };

  //! Data used by \ref EmbedDataNode.
  struct EmbedData {
    //! Node type.
    NodeType _nodeType;
    //! Node flags.
    NodeFlags _nodeFlags;
    //! Type id.
    TypeId _typeId;
    //! Size of `_typeId`.
    uint8_t _typeSize;
  };

  //! Data used by \ref SentinelNode.
  struct SentinelData {
    //! Node type.
    NodeType _nodeType;
    //! Node flags.
    NodeFlags _nodeFlags;
    //! Sentinel type.
    SentinelType _sentinelType;
    //! Not used by BaseNode.
    uint8_t _reserved1;
  };

  //! Data that can have different meaning depending on \ref NodeType.
  union {
    //! Data useful by any node type.
    AnyData _any;
    //! Data specific to \ref AlignNode.
    AlignData _alignData;
    //! Data specific to \ref InstNode.
    InstData _inst;
    //! Data specific to \ref EmbedDataNode.
    EmbedData _embed;
    //! Data specific to \ref SentinelNode.
    SentinelData _sentinel;
  };

  //! Node position in code (should be unique).
  uint32_t _position;

  //! Value reserved for AsmJit users never touched by AsmJit itself.
  union {
    //! User data as 64-bit integer.
    uint64_t _userDataU64;
    //! User data as pointer.
    void* _userDataPtr;
  };

  //! Data used exclusively by the current `Pass`.
  void* _passData;

  //! Inline comment/annotation or nullptr if not used.
  const char* _inlineComment;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `BaseNode` - always use `BaseBuilder` to allocate nodes.
  ASMJIT_INLINE_NODEBUG BaseNode(BaseBuilder* cb, NodeType nodeType, NodeFlags nodeFlags = NodeFlags::kNone) noexcept {
    _prev = nullptr;
    _next = nullptr;
    _any._nodeType = nodeType;
    _any._nodeFlags = nodeFlags | cb->_nodeFlags;
    _any._reserved0 = 0;
    _any._reserved1 = 0;
    _position = 0;
    _userDataU64 = 0;
    _passData = nullptr;
    _inlineComment = nullptr;
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Casts this node to `T*`.
  template<typename T>
  ASMJIT_INLINE_NODEBUG T* as() noexcept { return static_cast<T*>(this); }
  //! Casts this node to `const T*`.
  template<typename T>
  ASMJIT_INLINE_NODEBUG const T* as() const noexcept { return static_cast<const T*>(this); }

  //! Returns previous node or `nullptr` if this node is either first or not part of Builder/Compiler node-list.
  ASMJIT_INLINE_NODEBUG BaseNode* prev() const noexcept { return _prev; }
  //! Returns next node or `nullptr` if this node is either last or not part of Builder/Compiler node-list.
  ASMJIT_INLINE_NODEBUG BaseNode* next() const noexcept { return _next; }

  //! Returns the type of the node, see \ref NodeType.
  ASMJIT_INLINE_NODEBUG NodeType type() const noexcept { return _any._nodeType; }

  //! Sets the type of the node, see `NodeType` (internal).
  //!
  //! \remarks You should never set a type of a node to anything else than the initial value. This function is only
  //! provided for users that use custom nodes and need to change the type either during construction or later.
  ASMJIT_INLINE_NODEBUG void setType(NodeType type) noexcept { _any._nodeType = type; }

  //! Tests whether this node is either `InstNode` or extends it.
  ASMJIT_INLINE_NODEBUG bool isInst() const noexcept { return hasFlag(NodeFlags::kActsAsInst); }
  //! Tests whether this node is `SectionNode`.
  ASMJIT_INLINE_NODEBUG bool isSection() const noexcept { return type() == NodeType::kSection; }
  //! Tests whether this node is either `LabelNode` or extends it.
  ASMJIT_INLINE_NODEBUG bool isLabel() const noexcept { return hasFlag(NodeFlags::kActsAsLabel); }
  //! Tests whether this node is `AlignNode`.
  ASMJIT_INLINE_NODEBUG bool isAlign() const noexcept { return type() == NodeType::kAlign; }
  //! Tests whether this node is `EmbedDataNode`.
  ASMJIT_INLINE_NODEBUG bool isEmbedData() const noexcept { return type() == NodeType::kEmbedData; }
  //! Tests whether this node is `EmbedLabelNode`.
  ASMJIT_INLINE_NODEBUG bool isEmbedLabel() const noexcept { return type() == NodeType::kEmbedLabel; }
  //! Tests whether this node is `EmbedLabelDeltaNode`.
  ASMJIT_INLINE_NODEBUG bool isEmbedLabelDelta() const noexcept { return type() == NodeType::kEmbedLabelDelta; }
  //! Tests whether this node is `ConstPoolNode`.
  ASMJIT_INLINE_NODEBUG bool isConstPool() const noexcept { return type() == NodeType::kConstPool; }
  //! Tests whether this node is `CommentNode`.
  ASMJIT_INLINE_NODEBUG bool isComment() const noexcept { return type() == NodeType::kComment; }
  //! Tests whether this node is `SentinelNode`.
  ASMJIT_INLINE_NODEBUG bool isSentinel() const noexcept { return type() == NodeType::kSentinel; }

  //! Tests whether this node is `FuncNode`.
  ASMJIT_INLINE_NODEBUG bool isFunc() const noexcept { return type() == NodeType::kFunc; }
  //! Tests whether this node is `FuncRetNode`.
  ASMJIT_INLINE_NODEBUG bool isFuncRet() const noexcept { return type() == NodeType::kFuncRet; }
  //! Tests whether this node is `InvokeNode`.
  ASMJIT_INLINE_NODEBUG bool isInvoke() const noexcept { return type() == NodeType::kInvoke; }

  //! Returns the node flags.
  ASMJIT_INLINE_NODEBUG NodeFlags flags() const noexcept { return _any._nodeFlags; }
  //! Tests whether the node has the given `flag` set.
  ASMJIT_INLINE_NODEBUG bool hasFlag(NodeFlags flag) const noexcept { return Support::test(_any._nodeFlags, flag); }
  //! Replaces node flags with `flags`.
  ASMJIT_INLINE_NODEBUG void setFlags(NodeFlags flags) noexcept { _any._nodeFlags = flags; }
  //! Adds the given `flags` to node flags.
  ASMJIT_INLINE_NODEBUG void addFlags(NodeFlags flags) noexcept { _any._nodeFlags |= flags; }
  //! Clears the given `flags` from node flags.
  ASMJIT_INLINE_NODEBUG void clearFlags(NodeFlags flags) noexcept { _any._nodeFlags &= ~flags; }

  //! Tests whether the node is code that can be executed.
  ASMJIT_INLINE_NODEBUG bool isCode() const noexcept { return hasFlag(NodeFlags::kIsCode); }
  //! Tests whether the node is data that cannot be executed.
  ASMJIT_INLINE_NODEBUG bool isData() const noexcept { return hasFlag(NodeFlags::kIsData); }
  //! Tests whether the node is informative only (is never encoded like comment, etc...).
  ASMJIT_INLINE_NODEBUG bool isInformative() const noexcept { return hasFlag(NodeFlags::kIsInformative); }
  //! Tests whether the node is removable if it's in an unreachable code block.
  ASMJIT_INLINE_NODEBUG bool isRemovable() const noexcept { return hasFlag(NodeFlags::kIsRemovable); }
  //! Tests whether the node has no effect when executed (label, .align, nop, ...).
  ASMJIT_INLINE_NODEBUG bool hasNoEffect() const noexcept { return hasFlag(NodeFlags::kHasNoEffect); }
  //! Tests whether the node is part of the code.
  ASMJIT_INLINE_NODEBUG bool isActive() const noexcept { return hasFlag(NodeFlags::kIsActive); }

  //! Tests whether the node has a position assigned.
  //!
  //! \remarks Returns `true` if node position is non-zero.
  ASMJIT_INLINE_NODEBUG bool hasPosition() const noexcept { return _position != 0; }
  //! Returns node position.
  ASMJIT_INLINE_NODEBUG uint32_t position() const noexcept { return _position; }
  //! Sets node position.
  //!
  //! Node position is a 32-bit unsigned integer that is used by Compiler to track where the node is relatively to
  //! the start of the function. It doesn't describe a byte position in a binary, instead it's just a pseudo position
  //! used by liveness analysis and other tools around Compiler.
  //!
  //! If you don't use Compiler then you may use `position()` and `setPosition()` freely for your own purposes if
  //! the 32-bit value limit is okay for you.
  ASMJIT_INLINE_NODEBUG void setPosition(uint32_t position) noexcept { _position = position; }

  //! Returns user data casted to `T*`.
  //!
  //! User data is dedicated to be used only by AsmJit users and not touched by the library. The data is of a pointer
  //! size so you can either store a pointer or `int64_t` value through `setUserDataAsPtr()`, `setUserDataAsInt64()`
  //! and `setUserDataAsUInt64()`.
  template<typename T>
  ASMJIT_INLINE_NODEBUG T* userDataAsPtr() const noexcept { return static_cast<T*>(_userDataPtr); }
  //! Returns user data casted to `int64_t`.
  ASMJIT_INLINE_NODEBUG int64_t userDataAsInt64() const noexcept { return int64_t(_userDataU64); }
  //! Returns user data casted to `uint64_t`.
  ASMJIT_INLINE_NODEBUG uint64_t userDataAsUInt64() const noexcept { return _userDataU64; }

  //! Sets user data to `data`.
  template<typename T>
  ASMJIT_INLINE_NODEBUG void setUserDataAsPtr(T* data) noexcept { _userDataPtr = static_cast<void*>(data); }
  //! Sets used data to the given 64-bit signed `value`.
  ASMJIT_INLINE_NODEBUG void setUserDataAsInt64(int64_t value) noexcept { _userDataU64 = uint64_t(value); }
  //! Sets used data to the given 64-bit unsigned `value`.
  ASMJIT_INLINE_NODEBUG void setUserDataAsUInt64(uint64_t value) noexcept { _userDataU64 = value; }

  //! Resets user data to zero / nullptr.
  ASMJIT_INLINE_NODEBUG void resetUserData() noexcept { _userDataU64 = 0; }

  //! Tests whether the node has an associated pass data.
  ASMJIT_INLINE_NODEBUG bool hasPassData() const noexcept { return _passData != nullptr; }
  //! Returns the node pass data - data used during processing & transformations.
  template<typename T>
  ASMJIT_INLINE_NODEBUG T* passData() const noexcept { return (T*)_passData; }
  //! Sets the node pass data to `data`.
  template<typename T>
  ASMJIT_INLINE_NODEBUG void setPassData(T* data) noexcept { _passData = (void*)data; }
  //! Resets the node pass data to nullptr.
  ASMJIT_INLINE_NODEBUG void resetPassData() noexcept { _passData = nullptr; }

  //! Tests whether the node has an inline comment/annotation.
  ASMJIT_INLINE_NODEBUG bool hasInlineComment() const noexcept { return _inlineComment != nullptr; }
  //! Returns an inline comment/annotation string.
  ASMJIT_INLINE_NODEBUG const char* inlineComment() const noexcept { return _inlineComment; }
  //! Sets an inline comment/annotation string to `s`.
  ASMJIT_INLINE_NODEBUG void setInlineComment(const char* s) noexcept { _inlineComment = s; }
  //! Resets an inline comment/annotation string to nullptr.
  ASMJIT_INLINE_NODEBUG void resetInlineComment() noexcept { _inlineComment = nullptr; }

  //! \}
};

//! Instruction node.
//!
//! Wraps an instruction with its options and operands.
class InstNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(InstNode)

  //! \name Constants
  //! \{

  //! The number of embedded operands for a default \ref InstNode instance that are always allocated as a part of
  //! the instruction itself. Minimum embedded operands is 4, but in 32-bit more pointers are smaller and we can
  //! embed 5. The rest (up to 6 operands) is considered extended.
  //!
  //! The number of operands InstNode holds is decided when \ref InstNode is created.
  static constexpr uint32_t kBaseOpCapacity = uint32_t((128 - sizeof(BaseNode) - sizeof(BaseInst)) / sizeof(Operand_));

  //! Count of maximum number of operands \ref InstNode can hold.
  static constexpr uint32_t kFullOpCapacity = Globals::kMaxOpCount;

  //! \}

  //! \name Members
  //! \{

  //! Base instruction data.
  BaseInst _baseInst;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `InstNode` instance.
  ASMJIT_INLINE_NODEBUG InstNode(BaseBuilder* cb, InstId instId, InstOptions options, uint32_t opCount, uint32_t opCapacity = kBaseOpCapacity) noexcept
    : BaseNode(cb, NodeType::kInst, NodeFlags::kIsCode | NodeFlags::kIsRemovable | NodeFlags::kActsAsInst),
      _baseInst(instId, options) {
    _inst._opCapacity = uint8_t(opCapacity);
    _inst._opCount = uint8_t(opCount);
  }

  //! \cond INTERNAL
  //! Reset all built-in operands, including `extraReg`.
  ASMJIT_INLINE_NODEBUG void _resetOps() noexcept {
    _baseInst.resetExtraReg();
    resetOpRange(0, opCapacity());
  }
  //! \endcond

  //! \}

  //! \name Instruction Object
  //! \{

  ASMJIT_INLINE_NODEBUG BaseInst& baseInst() noexcept { return _baseInst; }
  ASMJIT_INLINE_NODEBUG const BaseInst& baseInst() const noexcept { return _baseInst; }

  //! \}

  //! \name Instruction Id
  //! \{

  //! Returns the instruction id, see `BaseInst::Id`.
  ASMJIT_INLINE_NODEBUG InstId id() const noexcept { return _baseInst.id(); }
  //! Returns the instruction real id, see `BaseInst::Id`.
  ASMJIT_INLINE_NODEBUG InstId realId() const noexcept { return _baseInst.realId(); }

  //! Sets the instruction id to `id`, see `BaseInst::Id`.
  ASMJIT_INLINE_NODEBUG void setId(InstId id) noexcept { _baseInst.setId(id); }

  //! \}

  //! \name Instruction Options
  //! \{

  //! Returns instruction options, see \ref InstOptions for more details.
  ASMJIT_INLINE_NODEBUG InstOptions options() const noexcept { return _baseInst.options(); }
  //! Tests whether instruction has the given \option` set/enabled.
  ASMJIT_INLINE_NODEBUG bool hasOption(InstOptions option) const noexcept { return _baseInst.hasOption(option); }
  //! Sets instruction `options` to the provided value, resetting all others.
  ASMJIT_INLINE_NODEBUG void setOptions(InstOptions options) noexcept { _baseInst.setOptions(options); }
  //! Adds instruction `options` to the instruction.
  ASMJIT_INLINE_NODEBUG void addOptions(InstOptions options) noexcept { _baseInst.addOptions(options); }
  //! Clears instruction `options` of the instruction (disables the given options).
  ASMJIT_INLINE_NODEBUG void clearOptions(InstOptions options) noexcept { _baseInst.clearOptions(options); }
  //! Resets instruction options to none - disabling all instruction options.
  ASMJIT_INLINE_NODEBUG void resetOptions() noexcept { _baseInst.resetOptions(); }

  //! \}

  //! \name Extra Register
  //! \{

  //! Tests whether the node has an extra register operand.
  ASMJIT_INLINE_NODEBUG bool hasExtraReg() const noexcept { return _baseInst.hasExtraReg(); }
  //! Returns extra register operand.
  ASMJIT_INLINE_NODEBUG RegOnly& extraReg() noexcept { return _baseInst.extraReg(); }
  //! \overload
  ASMJIT_INLINE_NODEBUG const RegOnly& extraReg() const noexcept { return _baseInst.extraReg(); }
  //! Sets extra register operand to `reg`.
  ASMJIT_INLINE_NODEBUG void setExtraReg(const BaseReg& reg) noexcept { _baseInst.setExtraReg(reg); }
  //! Sets extra register operand to `reg`.
  ASMJIT_INLINE_NODEBUG void setExtraReg(const RegOnly& reg) noexcept { _baseInst.setExtraReg(reg); }
  //! Resets extra register operand.
  ASMJIT_INLINE_NODEBUG void resetExtraReg() noexcept { _baseInst.resetExtraReg(); }

  //! \}

  //! \name Instruction Operands
  //! \{

  //! Returns operand count.
  ASMJIT_INLINE_NODEBUG uint32_t opCount() const noexcept { return _inst._opCount; }
  //! Returns operand capacity.
  ASMJIT_INLINE_NODEBUG uint32_t opCapacity() const noexcept { return _inst._opCapacity; }

  //! Sets operand count.
  ASMJIT_INLINE_NODEBUG void setOpCount(uint32_t opCount) noexcept { _inst._opCount = uint8_t(opCount); }

  //! Returns operands array.
  ASMJIT_INLINE_NODEBUG Operand* operands() noexcept {
    return reinterpret_cast<Operand*>(reinterpret_cast<uint8_t*>(this) + sizeof(InstNode));
  }

  //! Returns operands array (const).
  ASMJIT_INLINE_NODEBUG const Operand* operands() const noexcept {
    return reinterpret_cast<const Operand*>(reinterpret_cast<const uint8_t*>(this) + sizeof(InstNode));
  }

  //! Returns operand at the given `index`.
  inline Operand& op(uint32_t index) noexcept {
    ASMJIT_ASSERT(index < opCapacity());

    Operand* ops = operands();
    return ops[index].as<Operand>();
  }

  //! Returns operand at the given `index` (const).
  inline const Operand& op(uint32_t index) const noexcept {
    ASMJIT_ASSERT(index < opCapacity());

    const Operand* ops = operands();
    return ops[index].as<Operand>();
  }

  //! Sets operand at the given `index` to `op`.
  inline void setOp(uint32_t index, const Operand_& op) noexcept {
    ASMJIT_ASSERT(index < opCapacity());

    Operand* ops = operands();
    ops[index].copyFrom(op);
  }

  //! Resets operand at the given `index` to none.
  inline void resetOp(uint32_t index) noexcept {
    ASMJIT_ASSERT(index < opCapacity());

    Operand* ops = operands();
    ops[index].reset();
  }

  //! Resets operands at `[start, end)` range.
  inline void resetOpRange(uint32_t start, uint32_t end) noexcept {
    Operand* ops = operands();
    for (uint32_t i = start; i < end; i++)
      ops[i].reset();
  }

  //! \}

  //! \name Utilities
  //! \{

  //! Tests whether the given operand type `opType` is used by the instruction.
  inline bool hasOpType(OperandType opType) const noexcept {
    const Operand* ops = operands();
    for (uint32_t i = 0, count = opCount(); i < count; i++)
      if (ops[i].opType() == opType)
        return true;
    return false;
  }

  //! Tests whether the instruction uses at least one register operand.
  inline bool hasRegOp() const noexcept { return hasOpType(OperandType::kReg); }
  //! Tests whether the instruction uses at least one memory operand.
  inline bool hasMemOp() const noexcept { return hasOpType(OperandType::kMem); }
  //! Tests whether the instruction uses at least one immediate operand.
  inline bool hasImmOp() const noexcept { return hasOpType(OperandType::kImm); }
  //! Tests whether the instruction uses at least one label operand.
  inline bool hasLabelOp() const noexcept { return hasOpType(OperandType::kLabel); }

  //! Returns the index of the given operand type `opType`.
  //!
  //! \note If the operand type wa found, the value returned represents its index in \ref operands()
  //! array, otherwise \ref Globals::kNotFound is returned to signalize that the operand was not found.
  inline uint32_t indexOfOpType(OperandType opType) const noexcept {
    uint32_t i = 0;
    uint32_t count = opCount();
    const Operand* ops = operands();

    while (i < count) {
      if (ops[i].opType() == opType)
        return i;
      i++;
    }

    return Globals::kNotFound;
  }

  //! A shortcut that calls `indexOfOpType(OperandType::kMem)`.
  inline uint32_t indexOfMemOp() const noexcept { return indexOfOpType(OperandType::kMem); }
  //! A shortcut that calls `indexOfOpType(OperandType::kImm)`.
  inline uint32_t indexOfImmOp() const noexcept { return indexOfOpType(OperandType::kImm); }
  //! A shortcut that calls `indexOfOpType(OperandType::kLabel)`.
  inline uint32_t indexOfLabelOp() const noexcept { return indexOfOpType(OperandType::kLabel); }

  //! \}

  //! \name Rewriting
  //! \{

  //! \cond INTERNAL

  //! Returns uint32_t[] view that represents BaseInst::RegOnly and instruction operands.
  ASMJIT_INLINE_NODEBUG uint32_t* _getRewriteArray() noexcept { return &_baseInst._extraReg._id; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const uint32_t* _getRewriteArray() const noexcept { return &_baseInst._extraReg._id; }

  //! Maximum value of rewrite id - 6 operands each having 4 slots is 24, one RegOnly having 2 slots => 26.
  static constexpr uint32_t kMaxRewriteId = 26 - 1;

  //! Returns a rewrite index of the given pointer to `id`.
  //!
  //! This function returns a value that can be then passed to `\ref rewriteIdAtIndex() function. It can address
  //! any id from any operand that is used by the instruction in addition to \ref BaseInst::regOnly field, which
  //! can also be used by the register allocator.
  inline uint32_t getRewriteIndex(const uint32_t* id) const noexcept {
    const uint32_t* array = _getRewriteArray();
    ASMJIT_ASSERT(array <= id);

    size_t index = (size_t)(id - array);
    ASMJIT_ASSERT(index <= kMaxRewriteId);

    return uint32_t(index);
  }

  //! Rewrites the given `index` to the provided identifier `id`.
  //!
  //! \note This is an internal function that is used by a \ref BaseCompiler implementation to rewrite virtual
  //! registers to physical registers. The rewriter in this case sees all operands as array of uint32 values
  //! and the given `index` describes a position in this array. For example a single \ref Operand would be
  //! decomposed to 4 uint32_t values, where the first at index 0 would be operand signature, next would be
  //! base id, etc... This is a comfortable way of patching operands without having to check for their types.
  inline void rewriteIdAtIndex(uint32_t index, uint32_t id) noexcept {
    ASMJIT_ASSERT(index <= kMaxRewriteId);

    uint32_t* array = _getRewriteArray();
    array[index] = id;
  }
  //! \endcond

  //! \}

  //! \name Static Functions
  //! \{

  //! \cond INTERNAL

  //! Returns the capacity required for the given operands count `opCount`.
  //!
  //! There are only two capacities used - \ref kBaseOpCapacity and \ref kFullOpCapacity, so this function
  //! is used to decide between these two. The general rule is that instructions that can be represented with
  //! \ref kBaseOpCapacity would use this value, and all others would take \ref kFullOpCapacity.
  static ASMJIT_INLINE_NODEBUG constexpr uint32_t capacityOfOpCount(uint32_t opCount) noexcept {
    return opCount <= kBaseOpCapacity ? kBaseOpCapacity : kFullOpCapacity;
  }

  //! Calculates the size of \ref InstNode required to hold at most `opCapacity` operands.
  //!
  //! This function is used internally to allocate \ref InstNode.
  static ASMJIT_INLINE_NODEBUG constexpr size_t nodeSizeOfOpCapacity(uint32_t opCapacity) noexcept {
    return sizeof(InstNode) + opCapacity * sizeof(Operand);
  }
  //! \endcond

  //! \}
};

//! Instruction node with embedded operands following \ref InstNode layout.
//!
//! \note This is used to make tools such as static analysis and compilers happy about the layout. There were two
//! instruction nodes in the past, having the second extend the operand array of the first, but that has caused
//! undefined behavior and made recent tools unhappy about that.
template<uint32_t kN>
class InstNodeWithOperands : public InstNode {
public:
  Operand_ _operands[kN];

  //! Creates a new `InstNodeWithOperands` instance.
  ASMJIT_INLINE_NODEBUG InstNodeWithOperands(BaseBuilder* cb, InstId instId, InstOptions options, uint32_t opCount) noexcept
    : InstNode(cb, instId, options, opCount, kN) {}
};

//! Section node.
class SectionNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(SectionNode)

  //! \name Members
  //! \{

  //! Section id.
  uint32_t _id;

  //! Next section node that follows this section.
  //!
  //! This link is only valid when the section is active (is part of the code) and when `Builder::hasDirtySectionLinks()`
  //! returns `false`. If you intend to use this field you should always call `Builder::updateSectionLinks()` before you
  //! do so.
  SectionNode* _nextSection;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `SectionNode` instance.
  ASMJIT_INLINE_NODEBUG SectionNode(BaseBuilder* cb, uint32_t sectionId = 0) noexcept
    : BaseNode(cb, NodeType::kSection, NodeFlags::kHasNoEffect),
      _id(sectionId),
      _nextSection(nullptr) {}

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the section id.
  ASMJIT_INLINE_NODEBUG uint32_t id() const noexcept { return _id; }

  //! \}
};

//! Label node.
class LabelNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(LabelNode)

  //! \name Members
  //! \{

  //! Label identifier.
  uint32_t _labelId;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `LabelNode` instance.
  ASMJIT_INLINE_NODEBUG LabelNode(BaseBuilder* cb, uint32_t labelId = 0) noexcept
    : BaseNode(cb, NodeType::kLabel, NodeFlags::kHasNoEffect | NodeFlags::kActsAsLabel),
      _labelId(labelId) {}

  //! \}

  //! \name Accessors
  //! \{

  //! Returns \ref Label representation of the \ref LabelNode.
  ASMJIT_INLINE_NODEBUG Label label() const noexcept { return Label(_labelId); }
  //! Returns the id of the label.
  ASMJIT_INLINE_NODEBUG uint32_t labelId() const noexcept { return _labelId; }

  //! \}
};

//! Align directive (BaseBuilder).
//!
//! Wraps `.align` directive.
class AlignNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(AlignNode)

  //! \name Members
  //! \{

  //! Alignment (in bytes).
  uint32_t _alignment;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `AlignNode` instance.
  ASMJIT_INLINE_NODEBUG AlignNode(BaseBuilder* cb, AlignMode alignMode, uint32_t alignment) noexcept
    : BaseNode(cb, NodeType::kAlign, NodeFlags::kIsCode | NodeFlags::kHasNoEffect) {

    _alignData._alignMode = alignMode;
    _alignment = alignment;
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns align mode.
  ASMJIT_INLINE_NODEBUG AlignMode alignMode() const noexcept { return _alignData._alignMode; }
  //! Sets align mode to `alignMode`.
  ASMJIT_INLINE_NODEBUG void setAlignMode(AlignMode alignMode) noexcept { _alignData._alignMode = alignMode; }

  //! Returns align offset in bytes.
  ASMJIT_INLINE_NODEBUG uint32_t alignment() const noexcept { return _alignment; }
  //! Sets align offset in bytes to `offset`.
  ASMJIT_INLINE_NODEBUG void setAlignment(uint32_t alignment) noexcept { _alignment = alignment; }

  //! \}
};

//! Embed data node.
//!
//! Wraps `.data` directive. The node contains data that will be placed at the node's position in the assembler
//! stream. The data is considered to be RAW; no analysis nor byte-order conversion is performed on RAW data.
class EmbedDataNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(EmbedDataNode)

  //! \cond INTERNAL
  enum : uint32_t {
    kInlineBufferSize = 128 - (sizeof(BaseNode) + sizeof(size_t) * 2)
  };
  //! \endcond

  //! \name Members
  //! \{

  size_t _itemCount;
  size_t _repeatCount;

  union {
    uint8_t* _externalData;
    uint8_t _inlineData[kInlineBufferSize];
  };

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `EmbedDataNode` instance.
  ASMJIT_INLINE_NODEBUG EmbedDataNode(BaseBuilder* cb) noexcept
    : BaseNode(cb, NodeType::kEmbedData, NodeFlags::kIsData),
      _itemCount(0),
      _repeatCount(0) {
    _embed._typeId = TypeId::kUInt8;
    _embed._typeSize = uint8_t(1);
    memset(_inlineData, 0, kInlineBufferSize);
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns data type as \ref TypeId.
  ASMJIT_INLINE_NODEBUG TypeId typeId() const noexcept { return _embed._typeId; }
  //! Returns the size of a single data element.
  ASMJIT_INLINE_NODEBUG uint32_t typeSize() const noexcept { return _embed._typeSize; }

  //! Returns a pointer to the data casted to `uint8_t`.
  ASMJIT_INLINE_NODEBUG uint8_t* data() const noexcept {
    return dataSize() <= kInlineBufferSize ? const_cast<uint8_t*>(_inlineData) : _externalData;
  }

  //! Returns a pointer to the data casted to `T`.
  template<typename T>
  ASMJIT_INLINE_NODEBUG T* dataAs() const noexcept { return reinterpret_cast<T*>(data()); }

  //! Returns the number of (typed) items in the array.
  ASMJIT_INLINE_NODEBUG size_t itemCount() const noexcept { return _itemCount; }

  //! Returns how many times the data is repeated (default 1).
  //!
  //! Repeated data is useful when defining constants for SIMD, for example.
  ASMJIT_INLINE_NODEBUG size_t repeatCount() const noexcept { return _repeatCount; }

  //! Returns the size of the data, not considering the number of times it repeats.
  //!
  //! \note The returned value is the same as `typeSize() * itemCount()`.
  ASMJIT_INLINE_NODEBUG size_t dataSize() const noexcept { return typeSize() * _itemCount; }

  //! \}
};

//! Label data node.
class EmbedLabelNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(EmbedLabelNode)

  //! \name Members
  //! \{

  uint32_t _labelId;
  uint32_t _dataSize;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `EmbedLabelNode` instance.
  ASMJIT_INLINE_NODEBUG EmbedLabelNode(BaseBuilder* cb, uint32_t labelId = 0, uint32_t dataSize = 0) noexcept
    : BaseNode(cb, NodeType::kEmbedLabel, NodeFlags::kIsData),
      _labelId(labelId),
      _dataSize(dataSize) {}

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the label to embed as \ref Label operand.
  ASMJIT_INLINE_NODEBUG Label label() const noexcept { return Label(_labelId); }
  //! Returns the id of the label.
  ASMJIT_INLINE_NODEBUG uint32_t labelId() const noexcept { return _labelId; }

  //! Sets the label id from `label` operand.
  ASMJIT_INLINE_NODEBUG void setLabel(const Label& label) noexcept { setLabelId(label.id()); }
  //! Sets the label id (use with caution, improper use can break a lot of things).
  ASMJIT_INLINE_NODEBUG void setLabelId(uint32_t labelId) noexcept { _labelId = labelId; }

  //! Returns the data size.
  ASMJIT_INLINE_NODEBUG uint32_t dataSize() const noexcept { return _dataSize; }
  //! Sets the data size.
  ASMJIT_INLINE_NODEBUG void setDataSize(uint32_t dataSize) noexcept { _dataSize = dataSize; }

  //! \}
};

//! Label data node.
class EmbedLabelDeltaNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(EmbedLabelDeltaNode)

  //! \name Members
  //! \{

  uint32_t _labelId;
  uint32_t _baseLabelId;
  uint32_t _dataSize;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `EmbedLabelDeltaNode` instance.
  ASMJIT_INLINE_NODEBUG EmbedLabelDeltaNode(BaseBuilder* cb, uint32_t labelId = 0, uint32_t baseLabelId = 0, uint32_t dataSize = 0) noexcept
    : BaseNode(cb, NodeType::kEmbedLabelDelta, NodeFlags::kIsData),
      _labelId(labelId),
      _baseLabelId(baseLabelId),
      _dataSize(dataSize) {}

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the label as `Label` operand.
  ASMJIT_INLINE_NODEBUG Label label() const noexcept { return Label(_labelId); }
  //! Returns the id of the label.
  ASMJIT_INLINE_NODEBUG uint32_t labelId() const noexcept { return _labelId; }

  //! Sets the label id from `label` operand.
  ASMJIT_INLINE_NODEBUG void setLabel(const Label& label) noexcept { setLabelId(label.id()); }
  //! Sets the label id.
  ASMJIT_INLINE_NODEBUG void setLabelId(uint32_t labelId) noexcept { _labelId = labelId; }

  //! Returns the base label as `Label` operand.
  ASMJIT_INLINE_NODEBUG Label baseLabel() const noexcept { return Label(_baseLabelId); }
  //! Returns the id of the base label.
  ASMJIT_INLINE_NODEBUG uint32_t baseLabelId() const noexcept { return _baseLabelId; }

  //! Sets the base label id from `label` operand.
  ASMJIT_INLINE_NODEBUG void setBaseLabel(const Label& baseLabel) noexcept { setBaseLabelId(baseLabel.id()); }
  //! Sets the base label id.
  ASMJIT_INLINE_NODEBUG void setBaseLabelId(uint32_t baseLabelId) noexcept { _baseLabelId = baseLabelId; }

  //! Returns the size of the embedded label address.
  ASMJIT_INLINE_NODEBUG uint32_t dataSize() const noexcept { return _dataSize; }
  //! Sets the size of the embedded label address.
  ASMJIT_INLINE_NODEBUG void setDataSize(uint32_t dataSize) noexcept { _dataSize = dataSize; }

  //! \}
};

//! A node that wraps `ConstPool`.
class ConstPoolNode : public LabelNode {
public:
  ASMJIT_NONCOPYABLE(ConstPoolNode)

  //! \name Members
  //! \{

  ConstPool _constPool;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `ConstPoolNode` instance.
  ASMJIT_INLINE_NODEBUG ConstPoolNode(BaseBuilder* cb, uint32_t id = 0) noexcept
    : LabelNode(cb, id),
      _constPool(&cb->_codeZone) {

    setType(NodeType::kConstPool);
    addFlags(NodeFlags::kIsData);
    clearFlags(NodeFlags::kIsCode | NodeFlags::kHasNoEffect);
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the constant-pool is empty.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _constPool.empty(); }
  //! Returns the size of the constant-pool in bytes.
  ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return _constPool.size(); }
  //! Returns minimum alignment.
  ASMJIT_INLINE_NODEBUG size_t alignment() const noexcept { return _constPool.alignment(); }

  //! Returns the wrapped `ConstPool` instance.
  ASMJIT_INLINE_NODEBUG ConstPool& constPool() noexcept { return _constPool; }
  //! Returns the wrapped `ConstPool` instance (const).
  ASMJIT_INLINE_NODEBUG const ConstPool& constPool() const noexcept { return _constPool; }

  //! \}

  //! \name Utilities
  //! \{

  //! See `ConstPool::add()`.
  ASMJIT_INLINE_NODEBUG Error add(const void* data, size_t size, size_t& dstOffset) noexcept {
    return _constPool.add(data, size, dstOffset);
  }

  //! \}
};

//! Comment node.
class CommentNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(CommentNode)

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `CommentNode` instance.
  ASMJIT_INLINE_NODEBUG CommentNode(BaseBuilder* cb, const char* comment) noexcept
    : BaseNode(cb, NodeType::kComment, NodeFlags::kIsInformative | NodeFlags::kHasNoEffect | NodeFlags::kIsRemovable) {
    _inlineComment = comment;
  }

  //! \}
};

//! Sentinel node.
//!
//! Sentinel is a marker that is completely ignored by the code builder. It's used to remember a position in a code
//! as it never gets removed by any pass.
class SentinelNode : public BaseNode {
public:
  ASMJIT_NONCOPYABLE(SentinelNode)

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `SentinelNode` instance.
  ASMJIT_INLINE_NODEBUG SentinelNode(BaseBuilder* cb, SentinelType sentinelType = SentinelType::kUnknown) noexcept
    : BaseNode(cb, NodeType::kSentinel, NodeFlags::kIsInformative | NodeFlags::kHasNoEffect) {

    _sentinel._sentinelType = sentinelType;
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the type of the sentinel.
  ASMJIT_INLINE_NODEBUG SentinelType sentinelType() const noexcept {
    return _sentinel._sentinelType;
  }

  //! Sets the type of the sentinel.
  ASMJIT_INLINE_NODEBUG void setSentinelType(SentinelType type) noexcept {
    _sentinel._sentinelType = type;
  }

  //! \}
};

//! Pass can be used to implement code transformations, analysis, and lowering.
class ASMJIT_VIRTAPI Pass {
public:
  ASMJIT_BASE_CLASS(Pass)
  ASMJIT_NONCOPYABLE(Pass)

  //! \name Members
  //! \{

  //! BaseBuilder this pass is assigned to.
  BaseBuilder* _cb = nullptr;
  //! Name of the pass.
  const char* _name = nullptr;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_API Pass(const char* name) noexcept;
  ASMJIT_API virtual ~Pass() noexcept;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns \ref BaseBuilder associated with the pass.
  ASMJIT_INLINE_NODEBUG const BaseBuilder* cb() const noexcept { return _cb; }
  //! Returns the name of the pass.
  ASMJIT_INLINE_NODEBUG const char* name() const noexcept { return _name; }

  //! \}

  //! \name Pass Interface
  //! \{

  //! Processes the code stored in Builder or Compiler.
  //!
  //! This is the only function that is called by the `BaseBuilder` to process the code. It passes `zone`,
  //! which will be reset after the `run()` finishes.
  ASMJIT_API virtual Error run(Zone* zone, Logger* logger);

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_BUILDER
#endif // ASMJIT_CORE_BUILDER_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/builder_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_BUILDER_P_H_INCLUDED
#define ASMJIT_CORE_BUILDER_P_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_BUILDER

#include "../core/builder.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_builder
//! \{

static inline void BaseBuilder_assignInlineComment(BaseBuilder* self, BaseNode* node, const char* comment) noexcept {
  if (comment)
    node->setInlineComment(static_cast<char*>(self->_dataZone.dup(comment, strlen(comment), true)));
}

static inline void BaseBuilder_assignInstState(BaseBuilder* self, InstNode* node, const BaseEmitter::State& state) noexcept {
  node->setOptions(state.options);
  node->setExtraReg(state.extraReg);
  BaseBuilder_assignInlineComment(self, node, state.comment);
}

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_BUILDER
#endif // ASMJIT_CORE_BUILDER_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/codebuffer.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_CODEBUFFER_H_INCLUDED
#define ASMJIT_CORE_CODEBUFFER_H_INCLUDED

#include "../core/globals.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

//! Flags used by \ref CodeBuffer.
enum class CodeBufferFlags : uint32_t {
  //! No flags.
  kNone = 0,
  //! Buffer is external (not allocated by asmjit).
  kIsExternal = 0x00000001u,
  //! Buffer is fixed (cannot be reallocated).
  kIsFixed = 0x00000002u
};
ASMJIT_DEFINE_ENUM_FLAGS(CodeBufferFlags)

//! Code or data buffer.
struct CodeBuffer {
  //! \name Members
  //! \{

  //! The content of the buffer (data).
  uint8_t* _data;
  //! Number of bytes of `data` used.
  size_t _size;
  //! Buffer capacity (in bytes).
  size_t _capacity;
  //! Buffer flags.
  CodeBufferFlags _flags;

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Returns a reference to the byte at the given `index`.
  inline uint8_t& operator[](size_t index) noexcept {
    ASMJIT_ASSERT(index < _size);
    return _data[index];
  }
  //! \overload
  inline const uint8_t& operator[](size_t index) const noexcept {
    ASMJIT_ASSERT(index < _size);
    return _data[index];
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns code buffer flags.
  ASMJIT_INLINE_NODEBUG CodeBufferFlags flags() const noexcept { return _flags; }
  //! Tests whether the code buffer has the given `flag` set.
  ASMJIT_INLINE_NODEBUG bool hasFlag(CodeBufferFlags flag) const noexcept { return Support::test(_flags, flag); }

  //! Tests whether this code buffer has a fixed size.
  //!
  //! Fixed size means that the code buffer is fixed and cannot grow.
  ASMJIT_INLINE_NODEBUG bool isFixed() const noexcept { return hasFlag(CodeBufferFlags::kIsFixed); }

  //! Tests whether the data in this code buffer is external.
  //!
  //! External data can only be provided by users, it's never used by AsmJit.
  ASMJIT_INLINE_NODEBUG bool isExternal() const noexcept { return hasFlag(CodeBufferFlags::kIsExternal); }

  //! Tests whether the data in this code buffer is allocated (non-null).
  ASMJIT_INLINE_NODEBUG bool isAllocated() const noexcept { return _data != nullptr; }

  //! Tests whether the code buffer is empty.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return !_size; }

  //! Returns the size of the data.
  ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return _size; }
  //! Returns the capacity of the data.
  ASMJIT_INLINE_NODEBUG size_t capacity() const noexcept { return _capacity; }

  //! Returns the pointer to the data the buffer references.
  ASMJIT_INLINE_NODEBUG uint8_t* data() noexcept { return _data; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const uint8_t* data() const noexcept { return _data; }

  //! \}

  //! \name Iterators
  //! \{

  ASMJIT_INLINE_NODEBUG uint8_t* begin() noexcept { return _data; }
  ASMJIT_INLINE_NODEBUG const uint8_t* begin() const noexcept { return _data; }

  ASMJIT_INLINE_NODEBUG uint8_t* end() noexcept { return _data + _size; }
  ASMJIT_INLINE_NODEBUG const uint8_t* end() const noexcept { return _data + _size; }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_CODEBUFFER_H_INCLUDED


```

`CodeCleaner/asmjit/asmjit/core/codeholder.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/assembler.h"
#include "../core/codewriter_p.h"
#include "../core/logger.h"
#include "../core/support.h"

#include <algorithm>
#include <tuple>

ASMJIT_BEGIN_NAMESPACE

// Globals
// =======

static const char CodeHolder_addrTabName[] = ".addrtab";

//! Encode MOD byte.
static inline uint32_t x86EncodeMod(uint32_t m, uint32_t o, uint32_t rm) noexcept {
  return (m << 6) | (o << 3) | rm;
}

// LabelLinkIterator
// =================

class LabelLinkIterator {
public:
  inline LabelLinkIterator(LabelEntry* le) noexcept { reset(le); }

  inline explicit operator bool() const noexcept { return isValid(); }
  inline bool isValid() const noexcept { return _link != nullptr; }

  inline LabelLink* link() const noexcept { return _link; }
  inline LabelLink* operator->() const noexcept { return _link; }

  inline void reset(LabelEntry* le) noexcept {
    _pPrev = &le->_links;
    _link = *_pPrev;
  }

  inline void next() noexcept {
    _pPrev = &_link->next;
    _link = *_pPrev;
  }

  inline void resolveAndNext(CodeHolder* code) noexcept {
    LabelLink* linkToDelete = _link;

    _link = _link->next;
    *_pPrev = _link;

    code->_unresolvedLinkCount--;
    code->_allocator.release(linkToDelete, sizeof(LabelLink));
  }

  LabelLink** _pPrev;
  LabelLink* _link;
};

// CodeHolder - Utilities
// ======================

static void CodeHolder_resetInternal(CodeHolder* self, ResetPolicy resetPolicy) noexcept {
  uint32_t i;
  const ZoneVector<BaseEmitter*>& emitters = self->emitters();

  i = emitters.size();
  while (i)
    self->detach(emitters[--i]);

  // Reset everything into its construction state.
  self->_environment.reset();
  self->_cpuFeatures.reset();
  self->_baseAddress = Globals::kNoBaseAddress;
  self->_logger = nullptr;
  self->_errorHandler = nullptr;

  // Reset all sections.
  uint32_t numSections = self->_sections.size();
  for (i = 0; i < numSections; i++) {
    Section* section = self->_sections[i];
    if (section->_buffer.data() && !section->_buffer.isExternal())
      ::free(section->_buffer._data);
    section->_buffer._data = nullptr;
    section->_buffer._capacity = 0;
  }

  // Reset zone allocator and all containers using it.
  ZoneAllocator* allocator = self->allocator();

  self->_emitters.reset();
  self->_namedLabels.reset();
  self->_relocations.reset();
  self->_labelEntries.reset();
  self->_sections.reset();
  self->_sectionsByOrder.reset();

  self->_unresolvedLinkCount = 0;
  self->_addressTableSection = nullptr;
  self->_addressTableEntries.reset();

  allocator->reset(&self->_zone);
  self->_zone.reset(resetPolicy);
}

static void CodeHolder_onSettingsUpdated(CodeHolder* self) noexcept {
  // Notify all attached emitters about a settings update.
  for (BaseEmitter* emitter : self->emitters()) {
    emitter->onSettingsUpdated();
  }
}

// CodeHolder - Construction & Destruction
// =======================================

CodeHolder::CodeHolder(const Support::Temporary* temporary) noexcept
  : _environment(),
    _cpuFeatures{},
    _baseAddress(Globals::kNoBaseAddress),
    _logger(nullptr),
    _errorHandler(nullptr),
    _zone(16u * 1024u, 1, temporary),
    _allocator(&_zone),
    _unresolvedLinkCount(0),
    _addressTableSection(nullptr) {}

CodeHolder::~CodeHolder() noexcept {
  CodeHolder_resetInternal(this, ResetPolicy::kHard);
}

// CodeHolder - Initialization & Reset
// ===================================

inline void CodeHolder_setSectionDefaultName(
  Section* section,
  char c0 = 0, char c1 = 0, char c2 = 0, char c3 = 0,
  char c4 = 0, char c5 = 0, char c6 = 0, char c7 = 0) noexcept {

  section->_name.u32[0] = Support::bytepack32_4x8(uint8_t(c0), uint8_t(c1), uint8_t(c2), uint8_t(c3));
  section->_name.u32[1] = Support::bytepack32_4x8(uint8_t(c4), uint8_t(c5), uint8_t(c6), uint8_t(c7));
}

Error CodeHolder::init(const Environment& environment, uint64_t baseAddress) noexcept {
  return init(environment, CpuFeatures{}, baseAddress);
}

Error CodeHolder::init(const Environment& environment, const CpuFeatures& cpuFeatures, uint64_t baseAddress) noexcept {
  // Cannot reinitialize if it's locked or there is one or more emitter attached.
  if (isInitialized())
    return DebugUtils::errored(kErrorAlreadyInitialized);

  // If we are just initializing there should be no emitters attached.
  ASMJIT_ASSERT(_emitters.empty());

  // Create a default section and insert it to the `_sections` array.
  Error err = _sections.willGrow(&_allocator) |
              _sectionsByOrder.willGrow(&_allocator);
  if (err == kErrorOk) {
    Section* section = _allocator.allocZeroedT<Section>();
    if (ASMJIT_LIKELY(section)) {
      section->_flags = SectionFlags::kExecutable | SectionFlags::kReadOnly;
      CodeHolder_setSectionDefaultName(section, '.', 't', 'e', 'x', 't');
      _sections.appendUnsafe(section);
      _sectionsByOrder.appendUnsafe(section);
    }
    else {
      err = DebugUtils::errored(kErrorOutOfMemory);
    }
  }

  if (ASMJIT_UNLIKELY(err)) {
    _zone.reset();
    return err;
  }
  else {
    _environment = environment;
    _cpuFeatures = cpuFeatures;
    _baseAddress = baseAddress;
    return kErrorOk;
  }
}

void CodeHolder::reset(ResetPolicy resetPolicy) noexcept {
  CodeHolder_resetInternal(this, resetPolicy);
}

// CodeHolder - Attach / Detach
// ============================

Error CodeHolder::attach(BaseEmitter* emitter) noexcept {
  // Catch a possible misuse of the API.
  if (ASMJIT_UNLIKELY(!emitter))
    return DebugUtils::errored(kErrorInvalidArgument);

  // Invalid emitter, this should not be possible.
  EmitterType type = emitter->emitterType();
  if (ASMJIT_UNLIKELY(type == EmitterType::kNone || uint32_t(type) > uint32_t(EmitterType::kMaxValue)))
    return DebugUtils::errored(kErrorInvalidState);

  uint64_t archMask = emitter->_archMask;
  if (ASMJIT_UNLIKELY(!(archMask & (uint64_t(1) << uint32_t(arch())))))
    return DebugUtils::errored(kErrorInvalidArch);

  // This is suspicious, but don't fail if `emitter` is already attached
  // to this code holder. This is not error, but it's not recommended.
  if (emitter->_code != nullptr) {
    if (emitter->_code == this)
      return kErrorOk;
    return DebugUtils::errored(kErrorInvalidState);
  }

  // Reserve the space now as we cannot fail after `onAttach()` succeeded.
  ASMJIT_PROPAGATE(_emitters.willGrow(&_allocator, 1));
  ASMJIT_PROPAGATE(emitter->onAttach(this));

  // Connect CodeHolder <-> BaseEmitter.
  ASMJIT_ASSERT(emitter->_code == this);
  _emitters.appendUnsafe(emitter);

  return kErrorOk;
}

Error CodeHolder::detach(BaseEmitter* emitter) noexcept {
  if (ASMJIT_UNLIKELY(!emitter))
    return DebugUtils::errored(kErrorInvalidArgument);

  if (ASMJIT_UNLIKELY(emitter->_code != this))
    return DebugUtils::errored(kErrorInvalidState);

  // NOTE: We always detach if we were asked to, if error happens during
  // `emitter->onDetach()` we just propagate it, but the BaseEmitter will
  // be detached.
  Error err = kErrorOk;
  if (!emitter->isDestroyed())
    err = emitter->onDetach(this);

  // Disconnect CodeHolder <-> BaseEmitter.
  uint32_t index = _emitters.indexOf(emitter);
  ASMJIT_ASSERT(index != Globals::kNotFound);

  _emitters.removeAt(index);
  emitter->_code = nullptr;

  return err;
}

// CodeHolder - Logging
// ====================

void CodeHolder::setLogger(Logger* logger) noexcept {
#ifndef ASMJIT_NO_LOGGING
  _logger = logger;
  CodeHolder_onSettingsUpdated(this);
#else
  DebugUtils::unused(logger);
#endif
}

// CodeHolder - Error Handling
// ===========================

void CodeHolder::setErrorHandler(ErrorHandler* errorHandler) noexcept {
  _errorHandler = errorHandler;
  CodeHolder_onSettingsUpdated(this);
}

// CodeHolder - Code Buffer
// ========================

static Error CodeHolder_reserveInternal(CodeHolder* self, CodeBuffer* cb, size_t n) noexcept {
  uint8_t* oldData = cb->_data;
  uint8_t* newData;

  if (oldData && !cb->isExternal())
    newData = static_cast<uint8_t*>(::realloc(oldData, n));
  else
    newData = static_cast<uint8_t*>(::malloc(n));

  if (ASMJIT_UNLIKELY(!newData))
    return DebugUtils::errored(kErrorOutOfMemory);

  cb->_data = newData;
  cb->_capacity = n;

  // Update pointers used by assemblers, if attached.
  for (BaseEmitter* emitter : self->emitters()) {
    if (emitter->isAssembler()) {
      BaseAssembler* a = static_cast<BaseAssembler*>(emitter);
      if (&a->_section->_buffer == cb) {
        size_t offset = a->offset();

        a->_bufferData = newData;
        a->_bufferEnd  = newData + n;
        a->_bufferPtr  = newData + offset;
      }
    }
  }

  return kErrorOk;
}

Error CodeHolder::growBuffer(CodeBuffer* cb, size_t n) noexcept {
  // The size of the section must be valid.
  size_t size = cb->size();
  if (ASMJIT_UNLIKELY(n > std::numeric_limits<uintptr_t>::max() - size))
    return DebugUtils::errored(kErrorOutOfMemory);

  // We can now check if growing the buffer is really necessary. It's unlikely
  // that this function is called while there is still room for `n` bytes.
  size_t capacity = cb->capacity();
  size_t required = cb->size() + n;
  if (ASMJIT_UNLIKELY(required <= capacity))
    return kErrorOk;

  if (cb->isFixed())
    return DebugUtils::errored(kErrorTooLarge);

  size_t kInitialCapacity = 8096;
  if (capacity < kInitialCapacity)
    capacity = kInitialCapacity;
  else
    capacity += Globals::kAllocOverhead;

  do {
    size_t old = capacity;
    if (capacity < Globals::kGrowThreshold)
      capacity *= 2;
    else
      capacity += Globals::kGrowThreshold;

    // Overflow.
    if (ASMJIT_UNLIKELY(old > capacity))
      return DebugUtils::errored(kErrorOutOfMemory);
  } while (capacity - Globals::kAllocOverhead < required);

  return CodeHolder_reserveInternal(this, cb, capacity - Globals::kAllocOverhead);
}

Error CodeHolder::reserveBuffer(CodeBuffer* cb, size_t n) noexcept {
  size_t capacity = cb->capacity();

  if (n <= capacity)
    return kErrorOk;

  if (cb->isFixed())
    return DebugUtils::errored(kErrorTooLarge);

  return CodeHolder_reserveInternal(this, cb, n);
}

// CodeHolder - Sections
// =====================

Error CodeHolder::newSection(Section** sectionOut, const char* name, size_t nameSize, SectionFlags flags, uint32_t alignment, int32_t order) noexcept {
  *sectionOut = nullptr;

  if (nameSize == SIZE_MAX)
    nameSize = strlen(name);

  if (alignment == 0)
    alignment = 1;

  if (ASMJIT_UNLIKELY(!Support::isPowerOf2(alignment)))
    return DebugUtils::errored(kErrorInvalidArgument);

  if (ASMJIT_UNLIKELY(nameSize > Globals::kMaxSectionNameSize))
    return DebugUtils::errored(kErrorInvalidSectionName);

  uint32_t sectionId = _sections.size();
  if (ASMJIT_UNLIKELY(sectionId == Globals::kInvalidId))
    return DebugUtils::errored(kErrorTooManySections);

  ASMJIT_PROPAGATE(_sections.willGrow(&_allocator));
  ASMJIT_PROPAGATE(_sectionsByOrder.willGrow(&_allocator));

  Section* section = _allocator.allocZeroedT<Section>();
  if (ASMJIT_UNLIKELY(!section))
    return DebugUtils::errored(kErrorOutOfMemory);

  section->_id = sectionId;
  section->_flags = flags;
  section->_alignment = alignment;
  section->_order = order;
  memcpy(section->_name.str, name, nameSize);

  Section** insertPosition = std::lower_bound(_sectionsByOrder.begin(), _sectionsByOrder.end(), section, [](const Section* a, const Section* b) {
    return std::make_tuple(a->order(), a->id()) < std::make_tuple(b->order(), b->id());
  });

  _sections.appendUnsafe(section);
  _sectionsByOrder.insertUnsafe((size_t)(insertPosition - _sectionsByOrder.data()), section);

  *sectionOut = section;
  return kErrorOk;
}

Section* CodeHolder::sectionByName(const char* name, size_t nameSize) const noexcept {
  if (nameSize == SIZE_MAX)
    nameSize = strlen(name);

  // This could be also put in a hash-table similarly like we do with labels,
  // however it's questionable as the number of sections should be pretty low
  // in general. Create an issue if this becomes a problem.
  if (nameSize <= Globals::kMaxSectionNameSize) {
    for (Section* section : _sections)
      if (memcmp(section->_name.str, name, nameSize) == 0 && section->_name.str[nameSize] == '\0')
        return section;
  }

  return nullptr;
}

Section* CodeHolder::ensureAddressTableSection() noexcept {
  if (_addressTableSection)
    return _addressTableSection;

  newSection(&_addressTableSection,
             CodeHolder_addrTabName,
             sizeof(CodeHolder_addrTabName) - 1,
             SectionFlags::kNone,
             _environment.registerSize(),
             std::numeric_limits<int32_t>::max());
  return _addressTableSection;
}

Error CodeHolder::addAddressToAddressTable(uint64_t address) noexcept {
  AddressTableEntry* entry = _addressTableEntries.get(address);
  if (entry)
    return kErrorOk;

  Section* section = ensureAddressTableSection();
  if (ASMJIT_UNLIKELY(!section))
    return DebugUtils::errored(kErrorOutOfMemory);

  entry = _zone.newT<AddressTableEntry>(address);
  if (ASMJIT_UNLIKELY(!entry))
    return DebugUtils::errored(kErrorOutOfMemory);

  _addressTableEntries.insert(entry);
  section->_virtualSize += _environment.registerSize();

  return kErrorOk;
}

// CodeHolder - Labels & Symbols
// =============================

//! Only used to lookup a label from `_namedLabels`.
class LabelByName {
public:
  inline LabelByName(const char* key, size_t keySize, uint32_t hashCode, uint32_t parentId) noexcept
    : _key(key),
      _keySize(uint32_t(keySize)),
      _hashCode(hashCode),
      _parentId(parentId) {}

  inline uint32_t hashCode() const noexcept { return _hashCode; }

  inline bool matches(const LabelEntry* entry) const noexcept {
    return entry->nameSize() == _keySize &&
           entry->parentId() == _parentId &&
           ::memcmp(entry->name(), _key, _keySize) == 0;
  }

  const char* _key;
  uint32_t _keySize;
  uint32_t _hashCode;
  uint32_t _parentId;
};

// Returns a hash of `name` and fixes `nameSize` if it's `SIZE_MAX`.
static uint32_t CodeHolder_hashNameAndGetSize(const char* name, size_t& nameSize) noexcept {
  uint32_t hashCode = 0;
  if (nameSize == SIZE_MAX) {
    size_t i = 0;
    for (;;) {
      uint8_t c = uint8_t(name[i]);
      if (!c) break;
      hashCode = Support::hashRound(hashCode, c);
      i++;
    }
    nameSize = i;
  }
  else {
    for (size_t i = 0; i < nameSize; i++) {
      uint8_t c = uint8_t(name[i]);
      if (ASMJIT_UNLIKELY(!c)) return DebugUtils::errored(kErrorInvalidLabelName);
      hashCode = Support::hashRound(hashCode, c);
    }
  }
  return hashCode;
}

LabelLink* CodeHolder::newLabelLink(LabelEntry* le, uint32_t sectionId, size_t offset, intptr_t rel, const OffsetFormat& format) noexcept {
  LabelLink* link = _allocator.allocT<LabelLink>();
  if (ASMJIT_UNLIKELY(!link)) return nullptr;

  link->next = le->_links;
  le->_links = link;

  link->sectionId = sectionId;
  link->relocId = Globals::kInvalidId;
  link->offset = offset;
  link->rel = rel;
  link->format = format;

  _unresolvedLinkCount++;
  return link;
}

Error CodeHolder::newLabelEntry(LabelEntry** entryOut) noexcept {
  *entryOut = nullptr;

  uint32_t labelId = _labelEntries.size();
  if (ASMJIT_UNLIKELY(labelId == Globals::kInvalidId))
    return DebugUtils::errored(kErrorTooManyLabels);

  ASMJIT_PROPAGATE(_labelEntries.willGrow(&_allocator));
  LabelEntry* le = _allocator.allocZeroedT<LabelEntry>();

  if (ASMJIT_UNLIKELY(!le))
    return DebugUtils::errored(kErrorOutOfMemory);

  le->_setId(labelId);
  le->_parentId = Globals::kInvalidId;
  le->_offset = 0;
  _labelEntries.appendUnsafe(le);

  *entryOut = le;
  return kErrorOk;
}

Error CodeHolder::newNamedLabelEntry(LabelEntry** entryOut, const char* name, size_t nameSize, LabelType type, uint32_t parentId) noexcept {
  *entryOut = nullptr;
  uint32_t hashCode = CodeHolder_hashNameAndGetSize(name, nameSize);

  if (ASMJIT_UNLIKELY(nameSize == 0)) {
    if (type == LabelType::kAnonymous)
      return newLabelEntry(entryOut);
    else
      return DebugUtils::errored(kErrorInvalidLabelName);
  }

  if (ASMJIT_UNLIKELY(nameSize > Globals::kMaxLabelNameSize))
    return DebugUtils::errored(kErrorLabelNameTooLong);

  switch (type) {
    case LabelType::kAnonymous: {
      // Anonymous labels cannot have a parent (or more specifically, parent is useless here).
      if (ASMJIT_UNLIKELY(parentId != Globals::kInvalidId))
        return DebugUtils::errored(kErrorInvalidParentLabel);

      uint32_t labelId = _labelEntries.size();
      if (ASMJIT_UNLIKELY(labelId == Globals::kInvalidId))
        return DebugUtils::errored(kErrorTooManyLabels);

      ASMJIT_PROPAGATE(_labelEntries.willGrow(&_allocator));
      LabelEntry* le = _allocator.allocZeroedT<LabelEntry>();

      if (ASMJIT_UNLIKELY(!le))
        return DebugUtils::errored(kErrorOutOfMemory);

      // NOTE: This LabelEntry has a name, but we leave its hashCode as zero as it's anonymous.
      le->_setId(labelId);
      le->_parentId = Globals::kInvalidId;
      le->_offset = 0;
      ASMJIT_PROPAGATE(le->_name.setData(&_zone, name, nameSize));

      _labelEntries.appendUnsafe(le);

      *entryOut = le;
      return kErrorOk;
    }

    case LabelType::kLocal: {
      if (ASMJIT_UNLIKELY(parentId >= _labelEntries.size()))
        return DebugUtils::errored(kErrorInvalidParentLabel);

      hashCode ^= parentId;
      break;
    }

    case LabelType::kGlobal:
    case LabelType::kExternal: {
      if (ASMJIT_UNLIKELY(parentId != Globals::kInvalidId))
        return DebugUtils::errored(kErrorInvalidParentLabel);
      break;
    }

    default: {
      return DebugUtils::errored(kErrorInvalidArgument);
    }
  }

  // Don't allow to insert duplicates. Local labels allow duplicates that have
  // different id, this is already accomplished by having a different hashes
  // between the same label names having different parent labels.
  LabelEntry* le = _namedLabels.get(LabelByName(name, nameSize, hashCode, parentId));
  if (ASMJIT_UNLIKELY(le))
    return DebugUtils::errored(kErrorLabelAlreadyDefined);

  Error err = kErrorOk;
  uint32_t labelId = _labelEntries.size();

  if (ASMJIT_UNLIKELY(labelId == Globals::kInvalidId))
    return DebugUtils::errored(kErrorTooManyLabels);

  ASMJIT_PROPAGATE(_labelEntries.willGrow(&_allocator));
  le = _allocator.allocZeroedT<LabelEntry>();

  if (ASMJIT_UNLIKELY(!le))
    return DebugUtils::errored(kErrorOutOfMemory);

  le->_hashCode = hashCode;
  le->_setId(labelId);
  le->_type = type;
  le->_parentId = parentId;
  le->_offset = 0;
  ASMJIT_PROPAGATE(le->_name.setData(&_zone, name, nameSize));

  _labelEntries.appendUnsafe(le);
  _namedLabels.insert(allocator(), le);

  *entryOut = le;
  return err;
}

uint32_t CodeHolder::labelIdByName(const char* name, size_t nameSize, uint32_t parentId) noexcept {
  uint32_t hashCode = CodeHolder_hashNameAndGetSize(name, nameSize);
  if (ASMJIT_UNLIKELY(!nameSize))
    return 0;

  if (parentId != Globals::kInvalidId)
    hashCode ^= parentId;

  LabelEntry* le = _namedLabels.get(LabelByName(name, nameSize, hashCode, parentId));
  return le ? le->id() : uint32_t(Globals::kInvalidId);
}

ASMJIT_API Error CodeHolder::resolveUnresolvedLinks() noexcept {
  if (!hasUnresolvedLinks())
    return kErrorOk;

  Error err = kErrorOk;
  for (LabelEntry* le : labelEntries()) {
    if (!le->isBound())
      continue;

    LabelLinkIterator link(le);
    if (link) {
      Support::FastUInt8 of = 0;
      Section* toSection = le->section();
      uint64_t toOffset = Support::addOverflow(toSection->offset(), le->offset(), &of);

      do {
        uint32_t linkSectionId = link->sectionId;
        if (link->relocId == Globals::kInvalidId) {
          Section* fromSection = sectionById(linkSectionId);
          size_t linkOffset = link->offset;

          CodeBuffer& buf = _sections[linkSectionId]->buffer();
          ASMJIT_ASSERT(linkOffset < buf.size());

          // Calculate the offset relative to the start of the virtual base.
          Support::FastUInt8 localOF = of;
          uint64_t fromOffset = Support::addOverflow<uint64_t>(fromSection->offset(), linkOffset, &localOF);
          int64_t displacement = int64_t(toOffset - fromOffset + uint64_t(int64_t(link->rel)));

          if (!localOF) {
            ASMJIT_ASSERT(size_t(linkOffset) < buf.size());
            ASMJIT_ASSERT(buf.size() - size_t(linkOffset) >= link->format.valueSize());

            // Overwrite a real displacement in the CodeBuffer.
            if (CodeWriterUtils::writeOffset(buf._data + linkOffset, displacement, link->format)) {
              link.resolveAndNext(this);
              continue;
            }
          }

          err = DebugUtils::errored(kErrorInvalidDisplacement);
          // Falls through to `link.next()`.
        }

        link.next();
      } while (link);
    }
  }

  return err;
}

ASMJIT_API Error CodeHolder::bindLabel(const Label& label, uint32_t toSectionId, uint64_t toOffset) noexcept {
  LabelEntry* le = labelEntry(label);
  if (ASMJIT_UNLIKELY(!le))
    return DebugUtils::errored(kErrorInvalidLabel);

  if (ASMJIT_UNLIKELY(toSectionId > _sections.size()))
    return DebugUtils::errored(kErrorInvalidSection);

  // Label can be bound only once.
  if (ASMJIT_UNLIKELY(le->isBound()))
    return DebugUtils::errored(kErrorLabelAlreadyBound);

  // Bind the label.
  Section* section = _sections[toSectionId];
  le->_section = section;
  le->_offset = toOffset;

  Error err = kErrorOk;
  CodeBuffer& buf = section->buffer();

  // Fix all links to this label we have collected so far if they are within
  // the same section. We ignore any inter-section links as these have to be
  // fixed later.
  LabelLinkIterator link(le);
  while (link) {
    uint32_t linkSectionId = link->sectionId;
    size_t linkOffset = link->offset;

    uint32_t relocId = link->relocId;
    if (relocId != Globals::kInvalidId) {
      // Adjust relocation data only.
      RelocEntry* re = _relocations[relocId];
      re->_payload += toOffset;
      re->_targetSectionId = toSectionId;
    }
    else {
      if (linkSectionId != toSectionId) {
        link.next();
        continue;
      }

      ASMJIT_ASSERT(linkOffset < buf.size());
      int64_t displacement = int64_t(toOffset - uint64_t(linkOffset) + uint64_t(int64_t(link->rel)));

      // Size of the value we are going to patch. Only BYTE/DWORD is allowed.
      ASMJIT_ASSERT(buf.size() - size_t(linkOffset) >= link->format.regionSize());

      // Overwrite a real displacement in the CodeBuffer.
      if (!CodeWriterUtils::writeOffset(buf._data + linkOffset, displacement, link->format)) {
        err = DebugUtils::errored(kErrorInvalidDisplacement);
        link.next();
        continue;
      }
    }

    link.resolveAndNext(this);
  }

  return err;
}

// CodeHolder - Relocations
// ========================

Error CodeHolder::newRelocEntry(RelocEntry** dst, RelocType relocType) noexcept {
  ASMJIT_PROPAGATE(_relocations.willGrow(&_allocator));

  uint32_t relocId = _relocations.size();
  if (ASMJIT_UNLIKELY(relocId == Globals::kInvalidId))
    return DebugUtils::errored(kErrorTooManyRelocations);

  RelocEntry* re = _allocator.allocZeroedT<RelocEntry>();
  if (ASMJIT_UNLIKELY(!re))
    return DebugUtils::errored(kErrorOutOfMemory);

  re->_id = relocId;
  re->_relocType = relocType;
  re->_sourceSectionId = Globals::kInvalidId;
  re->_targetSectionId = Globals::kInvalidId;
  _relocations.appendUnsafe(re);

  *dst = re;
  return kErrorOk;
}

// CodeHolder - Expression Evaluation
// ==================================

static Error CodeHolder_evaluateExpression(CodeHolder* self, Expression* exp, uint64_t* out) noexcept {
  uint64_t value[2];
  for (size_t i = 0; i < 2; i++) {
    uint64_t v;
    switch (exp->valueType[i]) {
      case ExpressionValueType::kNone: {
        v = 0;
        break;
      }

      case ExpressionValueType::kConstant: {
        v = exp->value[i].constant;
        break;
      }

      case ExpressionValueType::kLabel: {
        LabelEntry* le = exp->value[i].label;
        if (!le->isBound())
          return DebugUtils::errored(kErrorExpressionLabelNotBound);
        v = le->section()->offset() + le->offset();
        break;
      }

      case ExpressionValueType::kExpression: {
        Expression* nested = exp->value[i].expression;
        ASMJIT_PROPAGATE(CodeHolder_evaluateExpression(self, nested, &v));
        break;
      }

      default:
        return DebugUtils::errored(kErrorInvalidState);
    }

    value[i] = v;
  }

  uint64_t result;
  uint64_t& a = value[0];
  uint64_t& b = value[1];

  switch (exp->opType) {
    case ExpressionOpType::kAdd:
      result = a + b;
      break;

    case ExpressionOpType::kSub:
      result = a - b;
      break;

    case ExpressionOpType::kMul:
      result = a * b;
      break;

    case ExpressionOpType::kSll:
      result = (b > 63) ? uint64_t(0) : uint64_t(a << b);
      break;

    case ExpressionOpType::kSrl:
      result = (b > 63) ? uint64_t(0) : uint64_t(a >> b);
      break;

    case ExpressionOpType::kSra:
      result = Support::sar(a, Support::min<uint64_t>(b, 63));
      break;

    default:
      return DebugUtils::errored(kErrorInvalidState);
  }

  *out = result;
  return kErrorOk;
}

// CodeHolder - Utilities
// ======================

Error CodeHolder::flatten() noexcept {
  uint64_t offset = 0;
  for (Section* section : _sectionsByOrder) {
    uint64_t realSize = section->realSize();
    if (realSize) {
      uint64_t alignedOffset = Support::alignUp(offset, section->alignment());
      if (ASMJIT_UNLIKELY(alignedOffset < offset))
        return DebugUtils::errored(kErrorTooLarge);

      Support::FastUInt8 of = 0;
      offset = Support::addOverflow(alignedOffset, realSize, &of);

      if (ASMJIT_UNLIKELY(of))
        return DebugUtils::errored(kErrorTooLarge);
    }
  }

  // Now we know that we can assign offsets of all sections properly.
  Section* prev = nullptr;
  offset = 0;
  for (Section* section : _sectionsByOrder) {
    uint64_t realSize = section->realSize();
    if (realSize)
      offset = Support::alignUp(offset, section->alignment());
    section->_offset = offset;

    // Make sure the previous section extends a bit to cover the alignment.
    if (prev)
      prev->_virtualSize = offset - prev->_offset;

    prev = section;
    offset += realSize;
  }

  return kErrorOk;
}

size_t CodeHolder::codeSize() const noexcept {
  Support::FastUInt8 of = 0;
  uint64_t offset = 0;

  for (Section* section : _sectionsByOrder) {
    uint64_t realSize = section->realSize();

    if (realSize) {
      uint64_t alignedOffset = Support::alignUp(offset, section->alignment());
      ASMJIT_ASSERT(alignedOffset >= offset);
      offset = Support::addOverflow(alignedOffset, realSize, &of);
    }
  }

  if ((sizeof(uint64_t) > sizeof(size_t) && offset > uint64_t(SIZE_MAX)) || of)
    return SIZE_MAX;

  return size_t(offset);
}

Error CodeHolder::relocateToBase(uint64_t baseAddress) noexcept {
  // Base address must be provided.
  if (ASMJIT_UNLIKELY(baseAddress == Globals::kNoBaseAddress))
    return DebugUtils::errored(kErrorInvalidArgument);

  _baseAddress = baseAddress;
  uint32_t addressSize = _environment.registerSize();

  Section* addressTableSection = _addressTableSection;
  uint32_t addressTableEntryCount = 0;
  uint8_t* addressTableEntryData = nullptr;

  if (addressTableSection) {
    ASMJIT_PROPAGATE(
      reserveBuffer(&addressTableSection->_buffer, size_t(addressTableSection->virtualSize())));
    addressTableEntryData = addressTableSection->_buffer.data();
  }

  // Relocate all recorded locations.
  for (const RelocEntry* re : _relocations) {
    // Possibly deleted or optimized-out entry.
    if (re->relocType() == RelocType::kNone)
      continue;

    Section* sourceSection = sectionById(re->sourceSectionId());
    Section* targetSection = nullptr;

    if (re->targetSectionId() != Globals::kInvalidId)
      targetSection = sectionById(re->targetSectionId());

    uint64_t value = re->payload();
    uint64_t sectionOffset = sourceSection->offset();
    uint64_t sourceOffset = re->sourceOffset();

    // Make sure that the `RelocEntry` doesn't go out of bounds.
    size_t regionSize = re->format().regionSize();
    if (ASMJIT_UNLIKELY(re->sourceOffset() >= sourceSection->bufferSize() ||
                        sourceSection->bufferSize() - size_t(re->sourceOffset()) < regionSize))
      return DebugUtils::errored(kErrorInvalidRelocEntry);

    uint8_t* buffer = sourceSection->data();

    switch (re->relocType()) {
      case RelocType::kExpression: {
        Expression* expression = (Expression*)(uintptr_t(value));
        ASMJIT_PROPAGATE(CodeHolder_evaluateExpression(this, expression, &value));
        break;
      }

      case RelocType::kAbsToAbs: {
        break;
      }

      case RelocType::kRelToAbs: {
        // Value is currently a relative offset from the start of its section.
        // We have to convert it to an absolute offset (including base address).
        if (ASMJIT_UNLIKELY(!targetSection))
          return DebugUtils::errored(kErrorInvalidRelocEntry);

        //value += baseAddress + sectionOffset + sourceOffset + regionSize;
        value += baseAddress + targetSection->offset();
        break;
      }

      case RelocType::kAbsToRel: {
        value -= baseAddress + sectionOffset + sourceOffset + regionSize;

        // Sign extend as we are not interested in the high 32-bit word in a 32-bit address space.
        if (addressSize <= 4)
          value = uint64_t(int64_t(int32_t(value & 0xFFFFFFFFu)));
        else if (!Support::isInt32(int64_t(value)))
          return DebugUtils::errored(kErrorRelocOffsetOutOfRange);

        break;
      }

      case RelocType::kX64AddressEntry: {
        size_t valueOffset = size_t(re->sourceOffset()) + re->format().valueOffset();
        if (re->format().valueSize() != 4 || valueOffset < 2)
          return DebugUtils::errored(kErrorInvalidRelocEntry);

        // First try whether a relative 32-bit displacement would work.
        value -= baseAddress + sectionOffset + sourceOffset + regionSize;
        if (!Support::isInt32(int64_t(value))) {
          // Relative 32-bit displacement is not possible, use '.addrtab' section.
          AddressTableEntry* atEntry = _addressTableEntries.get(re->payload());
          if (ASMJIT_UNLIKELY(!atEntry))
            return DebugUtils::errored(kErrorInvalidRelocEntry);

          // Cannot be null as we have just matched the `AddressTableEntry`.
          ASMJIT_ASSERT(addressTableSection != nullptr);

          if (!atEntry->hasAssignedSlot())
            atEntry->_slot = addressTableEntryCount++;

          size_t atEntryIndex = size_t(atEntry->slot()) * addressSize;
          uint64_t addrSrc = sectionOffset + sourceOffset + regionSize;
          uint64_t addrDst = addressTableSection->offset() + uint64_t(atEntryIndex);

          value = addrDst - addrSrc;
          if (!Support::isInt32(int64_t(value)))
            return DebugUtils::errored(kErrorRelocOffsetOutOfRange);

          // Bytes that replace [REX, OPCODE] bytes.
          uint32_t byte0 = 0xFF;
          uint32_t byte1 = buffer[valueOffset - 1];

          if (byte1 == 0xE8) {
            // Patch CALL/MOD byte to FF /2 (-> 0x15).
            byte1 = x86EncodeMod(0, 2, 5);
          }
          else if (byte1 == 0xE9) {
            // Patch JMP/MOD byte to FF /4 (-> 0x25).
            byte1 = x86EncodeMod(0, 4, 5);
          }
          else {
            return DebugUtils::errored(kErrorInvalidRelocEntry);
          }

          // Patch `jmp/call` instruction.
          buffer[valueOffset - 2] = uint8_t(byte0);
          buffer[valueOffset - 1] = uint8_t(byte1);

          Support::writeU64uLE(addressTableEntryData + atEntryIndex, re->payload());
        }
        break;
      }

      default:
        return DebugUtils::errored(kErrorInvalidRelocEntry);
    }

    if (!CodeWriterUtils::writeOffset(buffer + re->sourceOffset(), int64_t(value), re->format())) {
      return DebugUtils::errored(kErrorInvalidRelocEntry);
    }
  }

  // Fixup the virtual size of the address table if it's the last section.
  if (_sectionsByOrder.last() == addressTableSection) {
    ASMJIT_ASSERT(addressTableSection != nullptr);

    size_t addressTableSize = addressTableEntryCount * addressSize;
    addressTableSection->_buffer._size = addressTableSize;
    addressTableSection->_virtualSize = addressTableSize;
  }

  return kErrorOk;
}

Error CodeHolder::copySectionData(void* dst, size_t dstSize, uint32_t sectionId, CopySectionFlags copyFlags) noexcept {
  if (ASMJIT_UNLIKELY(!isSectionValid(sectionId)))
    return DebugUtils::errored(kErrorInvalidSection);

  Section* section = sectionById(sectionId);
  size_t bufferSize = section->bufferSize();

  if (ASMJIT_UNLIKELY(dstSize < bufferSize))
    return DebugUtils::errored(kErrorInvalidArgument);

  memcpy(dst, section->data(), bufferSize);

  if (bufferSize < dstSize && Support::test(copyFlags, CopySectionFlags::kPadSectionBuffer)) {
    size_t paddingSize = dstSize - bufferSize;
    memset(static_cast<uint8_t*>(dst) + bufferSize, 0, paddingSize);
  }

  return kErrorOk;
}

Error CodeHolder::copyFlattenedData(void* dst, size_t dstSize, CopySectionFlags copyFlags) noexcept {
  size_t end = 0;
  for (Section* section : _sectionsByOrder) {
    if (section->offset() > dstSize)
      return DebugUtils::errored(kErrorInvalidArgument);

    size_t bufferSize = section->bufferSize();
    size_t offset = size_t(section->offset());

    if (ASMJIT_UNLIKELY(dstSize - offset < bufferSize))
      return DebugUtils::errored(kErrorInvalidArgument);

    uint8_t* dstTarget = static_cast<uint8_t*>(dst) + offset;
    size_t paddingSize = 0;
    memcpy(dstTarget, section->data(), bufferSize);

    if (Support::test(copyFlags, CopySectionFlags::kPadSectionBuffer) && bufferSize < section->virtualSize()) {
      paddingSize = Support::min<size_t>(dstSize - offset, size_t(section->virtualSize())) - bufferSize;
      memset(dstTarget + bufferSize, 0, paddingSize);
    }

    end = Support::max(end, offset + bufferSize + paddingSize);
  }

  if (end < dstSize && Support::test(copyFlags, CopySectionFlags::kPadTargetBuffer)) {
    memset(static_cast<uint8_t*>(dst) + end, 0, dstSize - end);
  }

  return kErrorOk;
}

// CodeHolder - Tests
// ==================

#if defined(ASMJIT_TEST)
UNIT(code_holder) {
  CodeHolder code;

  INFO("Verifying CodeHolder::init()");
  Environment env;
  env.init(Arch::kX86);

  code.init(env);
  EXPECT_EQ(code.arch(), Arch::kX86);

  INFO("Verifying named labels");
  LabelEntry* le;
  EXPECT_EQ(code.newNamedLabelEntry(&le, "NamedLabel", SIZE_MAX, LabelType::kGlobal), kErrorOk);
  EXPECT_EQ(strcmp(le->name(), "NamedLabel"), 0);
  EXPECT_EQ(code.labelIdByName("NamedLabel"), le->id());

  INFO("Verifying section ordering");
  Section* section1;
  EXPECT_EQ(code.newSection(&section1, "high-priority", SIZE_MAX, SectionFlags::kNone, 1, -1), kErrorOk);
  EXPECT_EQ(code.sections()[1], section1);
  EXPECT_EQ(code.sectionsByOrder()[0], section1);

  Section* section0;
  EXPECT_EQ(code.newSection(&section0, "higher-priority", SIZE_MAX, SectionFlags::kNone, 1, -2), kErrorOk);
  EXPECT_EQ(code.sections()[2], section0);
  EXPECT_EQ(code.sectionsByOrder()[0], section0);
  EXPECT_EQ(code.sectionsByOrder()[1], section1);

  Section* section3;
  EXPECT_EQ(code.newSection(&section3, "low-priority", SIZE_MAX, SectionFlags::kNone, 1, 2), kErrorOk);
  EXPECT_EQ(code.sections()[3], section3);
  EXPECT_EQ(code.sectionsByOrder()[3], section3);
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/codeholder.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_CODEHOLDER_H_INCLUDED
#define ASMJIT_CORE_CODEHOLDER_H_INCLUDED

#include "../core/archtraits.h"
#include "../core/codebuffer.h"
#include "../core/errorhandler.h"
#include "../core/operand.h"
#include "../core/string.h"
#include "../core/support.h"
#include "../core/target.h"
#include "../core/zone.h"
#include "../core/zonehash.h"
#include "../core/zonestring.h"
#include "../core/zonetree.h"
#include "../core/zonevector.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

class BaseEmitter;
class CodeHolder;
class LabelEntry;
class Logger;

//! Operator type that can be used within an \ref Expression.
enum class ExpressionOpType : uint8_t {
  //! Addition.
  kAdd = 0,
  //! Subtraction.
  kSub = 1,
  //! Multiplication
  kMul = 2,
  //! Logical left shift.
  kSll = 3,
  //! Logical right shift.
  kSrl = 4,
  //! Arithmetic right shift.
  kSra = 5
};

//! Value type that can be used within an \ref Expression.
enum class ExpressionValueType : uint8_t {
  //! No value or invalid.
  kNone = 0,
  //! Value is 64-bit unsigned integer (constant).
  kConstant = 1,
  //! Value is \ref LabelEntry, which references a \ref Label.
  kLabel = 2,
  //! Value is \ref Expression
  kExpression = 3
};

//! Expression node that can reference constants, labels, and another expressions.
struct Expression {
  //! Expression value.
  union Value {
    //! Constant.
    uint64_t constant;
    //! Pointer to another expression.
    Expression* expression;
    //! Pointer to \ref LabelEntry.
    LabelEntry* label;
  };

  //! \name Members
  //! \{

  //! Operation type.
  ExpressionOpType opType;
  //! Value types of \ref value.
  ExpressionValueType valueType[2];
  //! Reserved for future use, should be initialized to zero.
  uint8_t reserved[5];
  //! Expression left and right values.
  Value value[2];

  //! \}

  //! \name Accessors
  //! \{

  //! Resets the whole expression.
  //!
  //! Changes both values to \ref ExpressionValueType::kNone.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = Expression{}; }

  //! Sets the value type at `index` to \ref ExpressionValueType::kConstant and its content to `constant`.
  ASMJIT_INLINE_NODEBUG void setValueAsConstant(size_t index, uint64_t constant) noexcept {
    valueType[index] = ExpressionValueType::kConstant;
    value[index].constant = constant;
  }

  //! Sets the value type at `index` to \ref ExpressionValueType::kLabel and its content to `labelEntry`.
  ASMJIT_INLINE_NODEBUG void setValueAsLabel(size_t index, LabelEntry* labelEntry) noexcept {
    valueType[index] = ExpressionValueType::kLabel;
    value[index].label = labelEntry;
  }

  //! Sets the value type at `index` to \ref ExpressionValueType::kExpression and its content to `expression`.
  ASMJIT_INLINE_NODEBUG void setValueAsExpression(size_t index, Expression* expression) noexcept {
    valueType[index] = ExpressionValueType::kExpression;
    value[index].expression = expression;
  }

  //! \}
};

//! Section flags, used by \ref Section.
enum class SectionFlags : uint32_t {
  //! No flags.
  kNone = 0,
  //! Executable (.text sections).
  kExecutable = 0x00000001u,
  //! Read-only (.text and .data sections).
  kReadOnly = 0x00000002u,
  //! Zero initialized by the loader (BSS).
  kZeroInitialized = 0x00000004u,
  //! Info / comment flag.
  kComment = 0x00000008u,
  //! Section created implicitly, can be deleted by \ref Target.
  kImplicit = 0x80000000u
};
ASMJIT_DEFINE_ENUM_FLAGS(SectionFlags)

//! Flags that can be used with \ref CodeHolder::copySectionData() and \ref CodeHolder::copyFlattenedData().
enum class CopySectionFlags : uint32_t {
  //! No flags.
  kNone = 0,

  //! If virtual size of a section is greater than the size of its \ref CodeBuffer then all bytes between the buffer
  //! size and virtual size will be zeroed. If this option is not set then those bytes would be left as is, which
  //! means that if the user didn't initialize them they would have a previous content, which may be unwanted.
  kPadSectionBuffer = 0x00000001u,

  //! Clears the target buffer if the flattened data is less than the destination size. This option works
  //! only with \ref CodeHolder::copyFlattenedData() as it processes multiple sections. It is ignored by
  //! \ref CodeHolder::copySectionData().
  kPadTargetBuffer = 0x00000002u
};
ASMJIT_DEFINE_ENUM_FLAGS(CopySectionFlags)

//! Section entry.
class Section {
public:
  //! \name Members
  //! \{

  //! Section id.
  uint32_t _id;
  //! Section flags.
  SectionFlags _flags;
  //! Section alignment requirements (0 if no requirements).
  uint32_t _alignment;
  //! Order (lower value means higher priority).
  int32_t _order;
  //! Offset of this section from base-address.
  uint64_t _offset;
  //! Virtual size of the section (zero initialized sections).
  uint64_t _virtualSize;
  //! Section name (max 35 characters, PE allows max 8).
  FixedString<Globals::kMaxSectionNameSize + 1> _name;
  //! Code or data buffer.
  CodeBuffer _buffer;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the section id.
  ASMJIT_INLINE_NODEBUG uint32_t id() const noexcept { return _id; }
  //! Returns the section name, as a null terminated string.
  ASMJIT_INLINE_NODEBUG const char* name() const noexcept { return _name.str; }

  //! Returns the section data.
  ASMJIT_INLINE_NODEBUG uint8_t* data() noexcept { return _buffer.data(); }
  //! \overload
  ASMJIT_INLINE_NODEBUG const uint8_t* data() const noexcept { return _buffer.data(); }

  //! Returns the section flags.
  ASMJIT_INLINE_NODEBUG SectionFlags flags() const noexcept { return _flags; }
  //! Tests whether the section has the given `flag`.
  ASMJIT_INLINE_NODEBUG bool hasFlag(SectionFlags flag) const noexcept { return Support::test(_flags, flag); }
  //! Adds `flags` to the section flags.
  ASMJIT_INLINE_NODEBUG void addFlags(SectionFlags flags) noexcept { _flags |= flags; }
  //! Removes `flags` from the section flags.
  ASMJIT_INLINE_NODEBUG void clearFlags(SectionFlags flags) noexcept { _flags &= ~flags; }

  //! Returns the minimum section alignment
  ASMJIT_INLINE_NODEBUG uint32_t alignment() const noexcept { return _alignment; }
  //! Sets the minimum section alignment
  ASMJIT_INLINE_NODEBUG void setAlignment(uint32_t alignment) noexcept { _alignment = alignment; }

  //! Returns the section order, which has a higher priority than section id.
  ASMJIT_INLINE_NODEBUG int32_t order() const noexcept { return _order; }

  //! Returns the section offset, relative to base.
  ASMJIT_INLINE_NODEBUG uint64_t offset() const noexcept { return _offset; }
  //! Set the section offset.
  ASMJIT_INLINE_NODEBUG void setOffset(uint64_t offset) noexcept { _offset = offset; }

  //! Returns the virtual size of the section.
  //!
  //! Virtual size is initially zero and is never changed by AsmJit. It's normal if virtual size is smaller than
  //! size returned by `bufferSize()` as the buffer stores real data emitted by assemblers or appended by users.
  //!
  //! Use `realSize()` to get the real and final size of this section.
  ASMJIT_INLINE_NODEBUG uint64_t virtualSize() const noexcept { return _virtualSize; }
  //! Sets the virtual size of the section.
  ASMJIT_INLINE_NODEBUG void setVirtualSize(uint64_t virtualSize) noexcept { _virtualSize = virtualSize; }

  //! Returns the buffer size of the section.
  ASMJIT_INLINE_NODEBUG size_t bufferSize() const noexcept { return _buffer.size(); }
  //! Returns the real size of the section calculated from virtual and buffer sizes.
  ASMJIT_INLINE_NODEBUG uint64_t realSize() const noexcept { return Support::max<uint64_t>(virtualSize(), bufferSize()); }

  //! Returns the `CodeBuffer` used by this section.
  ASMJIT_INLINE_NODEBUG CodeBuffer& buffer() noexcept { return _buffer; }
  //! Returns the `CodeBuffer` used by this section (const).
  ASMJIT_INLINE_NODEBUG const CodeBuffer& buffer() const noexcept { return _buffer; }

  //! \}
};

//! Entry in an address table.
class AddressTableEntry : public ZoneTreeNodeT<AddressTableEntry> {
public:
  ASMJIT_NONCOPYABLE(AddressTableEntry)

  //! \name Members
  //! \{

  //! Address.
  uint64_t _address;
  //! Slot.
  uint32_t _slot;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG explicit AddressTableEntry(uint64_t address) noexcept
    : _address(address),
      _slot(0xFFFFFFFFu) {}

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG uint64_t address() const noexcept { return _address; }
  ASMJIT_INLINE_NODEBUG uint32_t slot() const noexcept { return _slot; }

  ASMJIT_INLINE_NODEBUG bool hasAssignedSlot() const noexcept { return _slot != 0xFFFFFFFFu; }

  ASMJIT_INLINE_NODEBUG bool operator<(const AddressTableEntry& other) const noexcept { return _address < other._address; }
  ASMJIT_INLINE_NODEBUG bool operator>(const AddressTableEntry& other) const noexcept { return _address > other._address; }

  ASMJIT_INLINE_NODEBUG bool operator<(uint64_t queryAddress) const noexcept { return _address < queryAddress; }
  ASMJIT_INLINE_NODEBUG bool operator>(uint64_t queryAddress) const noexcept { return _address > queryAddress; }

  //! \}
};

//! Offset format type, used by \ref OffsetFormat.
enum class OffsetType : uint8_t {
  // Common Offset Formats
  // ---------------------

  //! A value having `_immBitCount` bits and shifted by `_immBitShift`.
  //!
  //! This offset type is sufficient for many targets that store offset as a continuous set bits within an
  //! instruction word / sequence of bytes.
  kSignedOffset,

  //! An unsigned value having `_immBitCount` bits and shifted by `_immBitShift`.
  kUnsignedOffset,

  // AArch64 Specific Offset Formats
  // -------------------------------

  //! AArch64 ADR format of `[.|immlo:2|.....|immhi:19|.....]`.
  kAArch64_ADR,

  //! AArch64 ADRP format of `[.|immlo:2|.....|immhi:19|.....]` (4kB pages).
  kAArch64_ADRP,

  // AArch32 Specific Offset Formats (T16 & T32)
  // -------------------------------------------

  //! AArch32 THUMBv2 immediate encoding of 'ADR' instruction (12-bit payload and sign bit):
  //!
  //!   `|.....|imm:1|..N.N|......|imm:3|....|imm:8|`
  //!
  //! Where `N` is one if the offset is negative. The immediate is encoded as absolute value of the offset if negative.
  kThumb32_ADR,

  //! AArch32 THUMBv2 immediate encoding of 'BLX' instruction (23-bit immediate payload, multiplied by 4):
  //!
  //!   `|.....|imm[22]|imm[19:10]|..|ja|1|jb|imm[9:0]|0`
  //!
  //! Where:
  //!
  //!   - `ja` is calculated as imm[22] ^ imm[21] ^ 1.
  //!   - `jb` is calculated as imm[22] ^ imm[20] ^ 1.
  kThumb32_BLX,

  //! AArch32 THUMBv2 immediate encoding of 'B' instruction without `<cond>` (24-bit immediate payload, multiplied by 2):
  //!
  //!   `|.....|imm[23]|imm[20:11]|..|ja|1|jb|imm[10:0]`
  //!
  //! Where:
  //!
  //!   - `ja` is calculated as imm[23] ^ imm[22] ^ 1.
  //!   - `jb` is calculated as imm[23] ^ imm[21] ^ 1.
  kThumb32_B,

  //! AArch32 THUMBv2 immediate encoding of 'B' instruction with `<cond>` (20-bit immediate payload, multiplied by 2).
  //!
  //!   `|.....|imm[19]|....|imm[16:11]|..|ja|1|jb|imm[10:0]`
  //!
  //! Where:
  //!
  //!   - `ja` is calculated as imm[19] ^ imm[18] ^ 1.
  //!   - `jb` is calculated as imm[19] ^ imm[17] ^ 1.
  kThumb32_BCond,

  // AArch32 Specific Offset Formats (A32)
  // -------------------------------------

  //! AArch32 ADR instruction, which uses a standard 12-bit immediate encoding that is used by other ARM instructions.
  kAArch32_ADR,

  //! AArch32 signed offset that is similar to `kSignedOffset`, however it uses absolute value of the offset and its
  //! sign is encoded in 23rd bit of the opcode.
  //!
  //!   `|........|U.......|........|........|`
  //!
  kAArch32_U23_SignedOffset,

  //! AArch32 offset format that encodes 8-bit offset as:
  //!
  //!   `|........|U.......|....|imm[7:4]|....|imm[3:0]|`
  //!
  //! in a 32-bit word, where U is a sign of the displacement and the displacement itself is encoded as its absolute
  //! value.
  kAArch32_U23_0To3At0_4To7At8,

  //! AArch32 offset format that encodes a signed 25-bit offset as:
  //!
  //!   `|.......|imm[0]|imm[24:1]|`
  //!
  //! in a 32-bit word.
  kAArch32_1To24At0_0At24,

  //! Maximum value of `OffsetFormatType`.
  kMaxValue = kAArch32_1To24At0_0At24
};

//! Provides information about formatting offsets, absolute addresses, or their parts. Offset format is used by both
//! \ref RelocEntry and \ref LabelLink. The illustration below describes the relation of region size and offset size.
//! Region size is the size of the whole unit whereas offset size is the size of the unit that will be patched.
//!
//! ```
//! +-> Code buffer |   The subject of the relocation (region)  |
//! |               | (Word-Offset)  (Word-Size)                |
//! |xxxxxxxxxxxxxxx|................|*PATCHED*|................|xxxxxxxxxxxx->
//!                                  |         |
//!     [Word Offset points here]----+         +--- [WordOffset + WordSize]
//! ```
//!
//! Once the offset word has been located it can be patched like this:
//!
//! ```
//!                               |ImmDiscardLSB (discard LSB bits).
//!                               |..
//! [0000000000000iiiiiiiiiiiiiiiiiDD] - Offset value (32-bit)
//! [000000000000000iiiiiiiiiiiiiiiii] - Offset value after discard LSB.
//! [00000000000iiiiiiiiiiiiiiiii0000] - Offset value shifted by ImmBitShift.
//! [xxxxxxxxxxxiiiiiiiiiiiiiiiiixxxx] - Patched word (32-bit)
//!             |...............|
//!               (ImmBitCount) +- ImmBitShift
//! ```
struct OffsetFormat {
  //! \name Members
  //! \{

  //! Type of the offset.
  OffsetType _type;
  //! Encoding flags.
  uint8_t _flags;
  //! Size of the region (in bytes) containing the offset value, if the offset value is part of an instruction,
  //! otherwise it would be the same as `_valueSize`.
  uint8_t _regionSize;
  //! Size of the offset value, in bytes (1, 2, 4, or 8).
  uint8_t _valueSize;
  //! Offset of the offset value, in bytes, relative to the start of the region or data. Value offset would be
  //! zero if both region size and value size are equal.
  uint8_t _valueOffset;
  //! Size of the offset immediate value in bits.
  uint8_t _immBitCount;
  //! Shift of the offset immediate value in bits in the target word.
  uint8_t _immBitShift;
  //! Number of least significant bits to discard before writing the immediate to the destination. All discarded
  //! bits must be zero otherwise the value is invalid.
  uint8_t _immDiscardLsb;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the type of the offset.
  ASMJIT_INLINE_NODEBUG OffsetType type() const noexcept { return _type; }

  //! Returns whether the offset is encoded as an absolute value of the offset with additional field(s) that represent
  //! the sign (AArch32 U/N fields in the opcode).
  //!
  //! If true, the offset itself is always positive and a separate U/N field is used to indicate the sign of the offset
  //! (usually `U==1` means ADD, but sometimes `N==1` means negative offset, which implies SUB).
  ASMJIT_INLINE_NODEBUG bool hasSignBit() const noexcept {
    return _type == OffsetType::kThumb32_ADR ||
           _type == OffsetType::kAArch32_ADR ||
           _type == OffsetType::kAArch32_U23_SignedOffset ||
           _type == OffsetType::kAArch32_U23_0To3At0_4To7At8;
  }

  //! Returns flags.
  ASMJIT_INLINE_NODEBUG uint32_t flags() const noexcept { return _flags; }
  //! Returns the size of the region/instruction where the offset is encoded.
  ASMJIT_INLINE_NODEBUG uint32_t regionSize() const noexcept { return _regionSize; }
  //! Returns the offset of the word relative to the start of the region where the offset is.
  ASMJIT_INLINE_NODEBUG uint32_t valueOffset() const noexcept { return _valueOffset; }
  //! Returns the size of the data-type (word) that contains the offset, in bytes.
  ASMJIT_INLINE_NODEBUG uint32_t valueSize() const noexcept { return _valueSize; }
  //! Returns the count of bits of the offset value in the data it's stored in.
  ASMJIT_INLINE_NODEBUG uint32_t immBitCount() const noexcept { return _immBitCount; }
  //! Returns the bit-shift of the offset value in the data it's stored in.
  ASMJIT_INLINE_NODEBUG uint32_t immBitShift() const noexcept { return _immBitShift; }
  //! Returns the number of least significant bits of the offset value, that must be zero and that are not part of
  //! the encoded data.
  ASMJIT_INLINE_NODEBUG uint32_t immDiscardLsb() const noexcept { return _immDiscardLsb; }

  //! Resets this offset format to a simple data value of `dataSize` bytes.
  //!
  //! The region will be the same size as data and immediate bits would correspond to `dataSize * 8`. There will be
  //! no immediate bit shift or discarded bits.
  inline void resetToSimpleValue(OffsetType type, size_t valueSize) noexcept {
    ASMJIT_ASSERT(valueSize <= 8u);

    _type = type;
    _flags = uint8_t(0);
    _regionSize = uint8_t(valueSize);
    _valueSize = uint8_t(valueSize);
    _valueOffset = uint8_t(0);
    _immBitCount = uint8_t(valueSize * 8u);
    _immBitShift = uint8_t(0);
    _immDiscardLsb = uint8_t(0);
  }

  inline void resetToImmValue(OffsetType type, size_t valueSize, uint32_t immBitShift, uint32_t immBitCount, uint32_t immDiscardLsb) noexcept {
    ASMJIT_ASSERT(valueSize <= 8u);
    ASMJIT_ASSERT(immBitShift < valueSize * 8u);
    ASMJIT_ASSERT(immBitCount <= 64u);
    ASMJIT_ASSERT(immDiscardLsb <= 64u);

    _type = type;
    _flags = uint8_t(0);
    _regionSize = uint8_t(valueSize);
    _valueSize = uint8_t(valueSize);
    _valueOffset = uint8_t(0);
    _immBitCount = uint8_t(immBitCount);
    _immBitShift = uint8_t(immBitShift);
    _immDiscardLsb = uint8_t(immDiscardLsb);
  }

  inline void setRegion(size_t regionSize, size_t valueOffset) noexcept {
    _regionSize = uint8_t(regionSize);
    _valueOffset = uint8_t(valueOffset);
  }

  inline void setLeadingAndTrailingSize(size_t leadingSize, size_t trailingSize) noexcept {
    _regionSize = uint8_t(leadingSize + trailingSize + _valueSize);
    _valueOffset = uint8_t(leadingSize);
  }

  //! \}
};

//! Relocation type.
enum class RelocType : uint32_t {
  //! None/deleted (no relocation).
  kNone = 0,
  //! Expression evaluation, `_payload` is pointer to `Expression`.
  kExpression = 1,
  //! Relocate absolute to absolute.
  kAbsToAbs = 2,
  //! Relocate relative to absolute.
  kRelToAbs = 3,
  //! Relocate absolute to relative.
  kAbsToRel = 4,
  //! Relocate absolute to relative or use trampoline.
  kX64AddressEntry = 5
};

//! Relocation entry.
struct RelocEntry {
  //! \name Members
  //! \{

  //! Relocation id.
  uint32_t _id;
  //! Type of the relocation.
  RelocType _relocType;
  //! Format of the relocated value.
  OffsetFormat _format;
  //! Source section id.
  uint32_t _sourceSectionId;
  //! Target section id.
  uint32_t _targetSectionId;
  //! Source offset (relative to start of the section).
  uint64_t _sourceOffset;
  //! Payload (target offset, target address, expression, etc).
  uint64_t _payload;

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG uint32_t id() const noexcept { return _id; }

  ASMJIT_INLINE_NODEBUG RelocType relocType() const noexcept { return _relocType; }
  ASMJIT_INLINE_NODEBUG const OffsetFormat& format() const noexcept { return _format; }

  ASMJIT_INLINE_NODEBUG uint32_t sourceSectionId() const noexcept { return _sourceSectionId; }
  ASMJIT_INLINE_NODEBUG uint32_t targetSectionId() const noexcept { return _targetSectionId; }

  ASMJIT_INLINE_NODEBUG uint64_t sourceOffset() const noexcept { return _sourceOffset; }
  ASMJIT_INLINE_NODEBUG uint64_t payload() const noexcept { return _payload; }

  ASMJIT_INLINE_NODEBUG Expression* payloadAsExpression() const noexcept {
    return reinterpret_cast<Expression*>(uintptr_t(_payload));
  }

  //! \}
};

//! Type of the \ref Label.
enum class LabelType : uint8_t {
  //! Anonymous label that can optionally have a name, which is only used for debugging purposes.
  kAnonymous = 0,
  //! Local label (always has parentId).
  kLocal = 1,
  //! Global label (never has parentId).
  kGlobal = 2,
  //! External label (references an external symbol).
  kExternal = 3,

  //! Maximum value of `LabelType`.
  kMaxValue = kExternal
};

//! Data structure used to link either unbound labels or cross-section links.
struct LabelLink {
  //! Next link (single-linked list).
  LabelLink* next;
  //! Section id where the label is bound.
  uint32_t sectionId;
  //! Relocation id or Globals::kInvalidId.
  uint32_t relocId;
  //! Label offset relative to the start of the section.
  size_t offset;
  //! Inlined rel8/rel32.
  intptr_t rel;
  //! Offset format information.
  OffsetFormat format;
};

//! Label entry.
//!
//! Contains the following properties:
//!   - Label id - This is the only thing that is set to the `Label` operand.
//!   - Label name - Optional, used mostly to create executables and libraries.
//!   - Label type - Type of the label, default `LabelType::kAnonymous`.
//!   - Label parent id - Derived from many assemblers that allow to define a local label that falls under a global
//!     label. This allows to define many labels of the same name that have different parent (global) label.
//!   - Offset - offset of the label bound by `Assembler`.
//!   - Links - single-linked list that contains locations of code that has to be patched when the label gets bound.
//!     Every use of unbound label adds one link to `_links` list.
//!   - HVal - Hash value of label's name and optionally parentId.
//!   - HashNext - Hash-table implementation detail.
class LabelEntry : public ZoneHashNode {
public:
  //! \name Constants
  //! \{

  enum : uint32_t {
    //! SSO size of \ref _name.
    //!
    //! \cond INTERNAL
    //! Let's round the size of `LabelEntry` to 64 bytes (as `ZoneAllocator` has granularity of 32 bytes anyway). This
    //! gives `_name` the remaining space, which is should be 16 bytes on 64-bit and 28 bytes on 32-bit architectures.
    //! \endcond
    kStaticNameSize = 64 - (sizeof(ZoneHashNode) + 8 + sizeof(Section*) + sizeof(size_t) + sizeof(LabelLink*))
  };

  //! \}

  //! \name Members
  //! \{

  //! Type of the label.
  LabelType _type;
  //! Must be zero.
  uint8_t _reserved[3];
  //! Label parent id or zero.
  uint32_t _parentId;
  //! Label offset relative to the start of the `_section`.
  uint64_t _offset;
  //! Section where the label was bound.
  Section* _section;
  //! Label links.
  LabelLink* _links;
  //! Label name.
  ZoneString<kStaticNameSize> _name;

  //! \}

  //! \name Accessors
  //! \{

  // NOTE: Label id is stored in `_customData`, which is provided by ZoneHashNode to fill a padding that a C++
  // compiler targeting 64-bit CPU will add to align the structure to 64-bits.

  //! Returns label id.
  ASMJIT_INLINE_NODEBUG uint32_t id() const noexcept { return _customData; }
  //! Sets label id (internal, used only by `CodeHolder`).
  ASMJIT_INLINE_NODEBUG void _setId(uint32_t id) noexcept { _customData = id; }

  //! Returns label type.
  ASMJIT_INLINE_NODEBUG LabelType type() const noexcept { return _type; }

  //! Tests whether the label has a parent label.
  ASMJIT_INLINE_NODEBUG bool hasParent() const noexcept { return _parentId != Globals::kInvalidId; }
  //! Returns label's parent id.
  ASMJIT_INLINE_NODEBUG uint32_t parentId() const noexcept { return _parentId; }

  //! Returns the section where the label was bound.
  //!
  //! If the label was not yet bound the return value is `nullptr`.
  ASMJIT_INLINE_NODEBUG Section* section() const noexcept { return _section; }

  //! Tests whether the label has name.
  ASMJIT_INLINE_NODEBUG bool hasName() const noexcept { return !_name.empty(); }

  //! Returns the label's name.
  //!
  //! \note Local labels will return their local name without their parent part, for example ".L1".
  ASMJIT_INLINE_NODEBUG const char* name() const noexcept { return _name.data(); }

  //! Returns size of label's name.
  //!
  //! \note Label name is always null terminated, so you can use `strlen()` to get it, however, it's also cached in
  //! `LabelEntry` itself, so if you want to know the size the fastest way is to call `LabelEntry::nameSize()`.
  ASMJIT_INLINE_NODEBUG uint32_t nameSize() const noexcept { return _name.size(); }

  //! Returns links associated with this label.
  ASMJIT_INLINE_NODEBUG LabelLink* links() const noexcept { return _links; }

  //! Tests whether the label is bound.
  ASMJIT_INLINE_NODEBUG bool isBound() const noexcept { return _section != nullptr; }
  //! Tests whether the label is bound to a the given `sectionId`.
  ASMJIT_INLINE_NODEBUG bool isBoundTo(Section* section) const noexcept { return _section == section; }

  //! Returns the label offset (only useful if the label is bound).
  ASMJIT_INLINE_NODEBUG uint64_t offset() const noexcept { return _offset; }

  //! Returns the hash-value of label's name and its parent label (if any).
  //!
  //! Label hash is calculated as `HASH(Name) ^ ParentId`. The hash function is implemented in `Support::hashString()`
  //! and `Support::hashRound()`.
  ASMJIT_INLINE_NODEBUG uint32_t hashCode() const noexcept { return _hashCode; }

  //! \}
};

//! Holds assembled code and data (including sections, labels, and relocation information).
//!
//! CodeHolder connects emitters with their targets. It provides them interface that can be used to query information
//! about the target environment (architecture, etc...) and API to create labels, sections, relocations, and to write
//! data to a \ref CodeBuffer, which is always part of \ref Section. More than one emitter can be attached to a single
//! CodeHolder instance at a time, which is used in practice
//!
//! CodeHolder provides interface for all emitter types. Assemblers use CodeHolder to write into \ref CodeBuffer, and
//! higher level emitters like Builder and Compiler use CodeHolder to manage labels and sections so higher level code
//! can be serialized to Assembler by \ref BaseEmitter::finalize() and \ref BaseBuilder::serializeTo().
//!
//! In order to use CodeHolder, it must be first initialized by \ref init(). After the CodeHolder has been successfully
//! initialized it can be used to hold assembled code, sections, labels, relocations, and to attach / detach code
//! emitters. After the end of code generation it can be used to query physical locations of labels and to relocate
//! the assembled code into the right address.
//!
//! \note \ref CodeHolder has an ability to attach an \ref ErrorHandler, however, the error handler is not triggered
//! by \ref CodeHolder itself, it's instead propagated to all emitters that attach to it.
class CodeHolder {
public:
  ASMJIT_NONCOPYABLE(CodeHolder)

  //! \name Members
  //! \{

  //! Environment information.
  Environment _environment;
  //! CPU features of the target architecture.
  CpuFeatures _cpuFeatures;
  //! Base address or \ref Globals::kNoBaseAddress.
  uint64_t _baseAddress;

  //! Attached `Logger`, used by all consumers.
  Logger* _logger;
  //! Attached `ErrorHandler`.
  ErrorHandler* _errorHandler;

  //! Code zone (used to allocate core structures).
  Zone _zone;
  //! Zone allocator, used to manage internal containers.
  ZoneAllocator _allocator;

  //! Attached emitters.
  ZoneVector<BaseEmitter*> _emitters;
  //! Section entries.
  ZoneVector<Section*> _sections;
  //! Section entries sorted by section order and then section id.
  ZoneVector<Section*> _sectionsByOrder;
  //! Label entries.
  ZoneVector<LabelEntry*> _labelEntries;
  //! Relocation entries.
  ZoneVector<RelocEntry*> _relocations;
  //! Label name -> LabelEntry (only named labels).
  ZoneHash<LabelEntry> _namedLabels;

  //! Count of label links, which are not resolved.
  size_t _unresolvedLinkCount;
  //! Pointer to an address table section (or null if this section doesn't exist).
  Section* _addressTableSection;
  //! Address table entries.
  ZoneTree<AddressTableEntry> _addressTableEntries;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates an uninitialized CodeHolder (you must init() it before it can be used).
  //!
  //! An optional `temporary` argument can be used to initialize the first block of \ref Zone that the CodeHolder
  //! uses into a temporary memory provided by the user.
  ASMJIT_API explicit CodeHolder(const Support::Temporary* temporary = nullptr) noexcept;

  //! \overload
  ASMJIT_INLINE_NODEBUG explicit CodeHolder(const Support::Temporary& temporary) noexcept
    : CodeHolder(&temporary) {}

  //! Destroys the CodeHolder and frees all resources it has allocated.
  ASMJIT_API ~CodeHolder() noexcept;

  //! Tests whether the `CodeHolder` has been initialized.
  //!
  //! Emitters can be only attached to initialized `CodeHolder` instances.
  ASMJIT_INLINE_NODEBUG bool isInitialized() const noexcept { return _environment.isInitialized(); }

  //! Initializes CodeHolder to hold code described by the given `environment` and `baseAddress`.
  ASMJIT_API Error init(const Environment& environment, uint64_t baseAddress = Globals::kNoBaseAddress) noexcept;
  //! Initializes CodeHolder to hold code described by the given `environment`, `cpuFeatures`, and `baseAddress`.
  ASMJIT_API Error init(const Environment& environment, const CpuFeatures& cpuFeatures, uint64_t baseAddress = Globals::kNoBaseAddress) noexcept;
  //! Detaches all code-generators attached and resets the `CodeHolder`.
  ASMJIT_API void reset(ResetPolicy resetPolicy = ResetPolicy::kSoft) noexcept;

  //! \}

  //! \name Attach & Detach
  //! \{

  //! Attaches an emitter to this `CodeHolder`.
  ASMJIT_API Error attach(BaseEmitter* emitter) noexcept;
  //! Detaches an emitter from this `CodeHolder`.
  ASMJIT_API Error detach(BaseEmitter* emitter) noexcept;

  //! \}

  //! \name Allocators
  //! \{

  //! Returns the allocator that the `CodeHolder` uses.
  //!
  //! \note This should be only used for AsmJit's purposes. Code holder uses arena allocator to allocate everything,
  //! so anything allocated through this allocator will be invalidated by \ref CodeHolder::reset() or by CodeHolder's
  //! destructor.
  ASMJIT_INLINE_NODEBUG ZoneAllocator* allocator() const noexcept { return const_cast<ZoneAllocator*>(&_allocator); }

  //! \}

  //! \name Code & Architecture
  //! \{

  //! Returns the target environment information.
  ASMJIT_INLINE_NODEBUG const Environment& environment() const noexcept { return _environment; }

  //! Returns the target architecture.
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return environment().arch(); }
  //! Returns the target sub-architecture.
  ASMJIT_INLINE_NODEBUG SubArch subArch() const noexcept { return environment().subArch(); }

  //! Returns the minimum CPU features of the target architecture.
  ASMJIT_INLINE_NODEBUG const CpuFeatures& cpuFeatures() const noexcept { return _cpuFeatures; }

  //! Tests whether a static base-address is set.
  ASMJIT_INLINE_NODEBUG bool hasBaseAddress() const noexcept { return _baseAddress != Globals::kNoBaseAddress; }
  //! Returns a static base-address or \ref Globals::kNoBaseAddress, if not set.
  ASMJIT_INLINE_NODEBUG uint64_t baseAddress() const noexcept { return _baseAddress; }

  //! \}

  //! \name Emitters
  //! \{

  //! Returns a vector of attached emitters.
  ASMJIT_INLINE_NODEBUG const ZoneVector<BaseEmitter*>& emitters() const noexcept { return _emitters; }

  //! \}

  //! \name Logging
  //! \{

  //! Returns the attached logger.
  ASMJIT_INLINE_NODEBUG Logger* logger() const noexcept { return _logger; }
  //! Attaches a `logger` to CodeHolder and propagates it to all attached emitters.
  ASMJIT_API void setLogger(Logger* logger) noexcept;
  //! Resets the logger to none.
  ASMJIT_INLINE_NODEBUG void resetLogger() noexcept { setLogger(nullptr); }

  //! \name Error Handling
  //! \{

  //! Tests whether the CodeHolder has an attached error handler, see \ref ErrorHandler.
  ASMJIT_INLINE_NODEBUG bool hasErrorHandler() const noexcept { return _errorHandler != nullptr; }
  //! Returns the attached error handler.
  ASMJIT_INLINE_NODEBUG ErrorHandler* errorHandler() const noexcept { return _errorHandler; }
  //! Attach an error handler to this `CodeHolder`.
  ASMJIT_API void setErrorHandler(ErrorHandler* errorHandler) noexcept;
  //! Resets the error handler to none.
  ASMJIT_INLINE_NODEBUG void resetErrorHandler() noexcept { setErrorHandler(nullptr); }

  //! \}

  //! \name Code Buffer
  //! \{

  //! Makes sure that at least `n` bytes can be added to CodeHolder's buffer `cb`.
  //!
  //! \note The buffer `cb` must be managed by `CodeHolder` - otherwise the behavior of the function is undefined.
  ASMJIT_API Error growBuffer(CodeBuffer* cb, size_t n) noexcept;

  //! Reserves the size of `cb` to at least `n` bytes.
  //!
  //! \note The buffer `cb` must be managed by `CodeHolder` - otherwise the behavior of the function is undefined.
  ASMJIT_API Error reserveBuffer(CodeBuffer* cb, size_t n) noexcept;

  //! \}

  //! \name Sections
  //! \{

  //! Returns an array of `Section*` records.
  ASMJIT_INLINE_NODEBUG const ZoneVector<Section*>& sections() const noexcept { return _sections; }
  //! Returns an array of `Section*` records sorted according to section order first, then section id.
  ASMJIT_INLINE_NODEBUG const ZoneVector<Section*>& sectionsByOrder() const noexcept { return _sectionsByOrder; }
  //! Returns the number of sections.
  ASMJIT_INLINE_NODEBUG uint32_t sectionCount() const noexcept { return _sections.size(); }

  //! Tests whether the given `sectionId` is valid.
  ASMJIT_INLINE_NODEBUG bool isSectionValid(uint32_t sectionId) const noexcept { return sectionId < _sections.size(); }

  //! Creates a new section and return its pointer in `sectionOut`.
  //!
  //! Returns `Error`, does not report a possible error to `ErrorHandler`.
  ASMJIT_API Error newSection(Section** sectionOut, const char* name, size_t nameSize = SIZE_MAX, SectionFlags flags = SectionFlags::kNone, uint32_t alignment = 1, int32_t order = 0) noexcept;

  //! Returns a section entry of the given index.
  ASMJIT_INLINE_NODEBUG Section* sectionById(uint32_t sectionId) const noexcept { return _sections[sectionId]; }

  //! Returns section-id that matches the given `name`.
  //!
  //! If there is no such section `Section::kInvalidId` is returned.
  ASMJIT_API Section* sectionByName(const char* name, size_t nameSize = SIZE_MAX) const noexcept;

  //! Returns '.text' section (section that commonly represents code).
  //!
  //! \note Text section is always the first section in \ref CodeHolder::sections() array.
  ASMJIT_INLINE_NODEBUG Section* textSection() const noexcept { return _sections[0]; }

  //! Tests whether '.addrtab' section exists.
  ASMJIT_INLINE_NODEBUG bool hasAddressTable() const noexcept { return _addressTableSection != nullptr; }

  //! Returns '.addrtab' section.
  //!
  //! This section is used exclusively by AsmJit to store absolute 64-bit
  //! addresses that cannot be encoded in instructions like 'jmp' or 'call'.
  //!
  //! \note This section is created on demand, the returned pointer can be null.
  ASMJIT_INLINE_NODEBUG Section* addressTableSection() const noexcept { return _addressTableSection; }

  //! Ensures that '.addrtab' section exists (creates it if it doesn't) and
  //! returns it. Can return `nullptr` on out of memory condition.
  ASMJIT_API Section* ensureAddressTableSection() noexcept;

  //! Used to add an address to an address table.
  //!
  //! This implicitly calls `ensureAddressTableSection()` and then creates `AddressTableEntry` that is inserted
  //! to `_addressTableEntries`. If the address already exists this operation does nothing as the same addresses
  //! use the same slot.
  //!
  //! This function should be considered internal as it's used by assemblers to insert an absolute address into the
  //! address table. Inserting address into address table without creating a particular relocation entry makes no sense.
  ASMJIT_API Error addAddressToAddressTable(uint64_t address) noexcept;

  //! \}

  //! \name Labels & Symbols
  //! \{

  //! Returns array of `LabelEntry*` records.
  ASMJIT_INLINE_NODEBUG const ZoneVector<LabelEntry*>& labelEntries() const noexcept { return _labelEntries; }

  //! Returns number of labels created.
  ASMJIT_INLINE_NODEBUG uint32_t labelCount() const noexcept { return _labelEntries.size(); }

  //! Tests whether the label having `id` is valid (i.e. created by `newLabelEntry()`).
  ASMJIT_INLINE_NODEBUG bool isLabelValid(uint32_t labelId) const noexcept {
    return labelId < _labelEntries.size();
  }

  //! Tests whether the `label` is valid (i.e. created by `newLabelEntry()`).
  ASMJIT_INLINE_NODEBUG bool isLabelValid(const Label& label) const noexcept {
    return label.id() < _labelEntries.size();
  }

  //! \overload
  ASMJIT_INLINE_NODEBUG bool isLabelBound(uint32_t labelId) const noexcept {
    return isLabelValid(labelId) && _labelEntries[labelId]->isBound();
  }

  //! Tests whether the `label` is already bound.
  //!
  //! Returns `false` if the `label` is not valid.
  ASMJIT_INLINE_NODEBUG bool isLabelBound(const Label& label) const noexcept {
    return isLabelBound(label.id());
  }

  //! Returns LabelEntry of the given label `id`.
  ASMJIT_INLINE_NODEBUG LabelEntry* labelEntry(uint32_t labelId) const noexcept {
    return isLabelValid(labelId) ? _labelEntries[labelId] : static_cast<LabelEntry*>(nullptr);
  }

  //! Returns LabelEntry of the given `label`.
  ASMJIT_INLINE_NODEBUG LabelEntry* labelEntry(const Label& label) const noexcept {
    return labelEntry(label.id());
  }

  //! Returns offset of a `Label` by its `labelId`.
  //!
  //! The offset returned is relative to the start of the section. Zero offset is returned for unbound labels,
  //! which is their initial offset value.
  ASMJIT_INLINE_NODEBUG uint64_t labelOffset(uint32_t labelId) const noexcept {
    ASMJIT_ASSERT(isLabelValid(labelId));
    return _labelEntries[labelId]->offset();
  }

  //! \overload
  ASMJIT_INLINE_NODEBUG uint64_t labelOffset(const Label& label) const noexcept {
    return labelOffset(label.id());
  }

  //! Returns offset of a label by it's `labelId` relative to the base offset.
  //!
  //! \remarks The offset of the section where the label is bound must be valid in order to use this function,
  //! otherwise the value returned will not be reliable.
  inline uint64_t labelOffsetFromBase(uint32_t labelId) const noexcept {
    ASMJIT_ASSERT(isLabelValid(labelId));
    const LabelEntry* le = _labelEntries[labelId];
    return (le->isBound() ? le->section()->offset() : uint64_t(0)) + le->offset();
  }

  //! \overload
  inline uint64_t labelOffsetFromBase(const Label& label) const noexcept {
    return labelOffsetFromBase(label.id());
  }

  //! Creates a new anonymous label and return its id in `idOut`.
  //!
  //! Returns `Error`, does not report error to `ErrorHandler`.
  ASMJIT_API Error newLabelEntry(LabelEntry** entryOut) noexcept;

  //! Creates a new named \ref LabelEntry of the given label `type`.
  //!
  //! \param entryOut Where to store the created \ref LabelEntry.
  //! \param name The name of the label.
  //! \param nameSize The length of `name` argument, or `SIZE_MAX` if `name` is a null terminated string, which
  //!        means that the `CodeHolder` will use `strlen()` to determine the length.
  //! \param type The type of the label to create, see \ref LabelType.
  //! \param parentId Parent id of a local label, otherwise it must be \ref Globals::kInvalidId.
  //! \retval Always returns \ref Error, does not report a possible error to the attached \ref ErrorHandler.
  //!
  //! AsmJit has a support for local labels (\ref LabelType::kLocal) which require a parent label id (parentId).
  //! The names of local labels can conflict with names of other local labels that have a different parent. In
  //! addition, AsmJit supports named anonymous labels, which are useful only for debugging purposes as the
  //! anonymous name will have a name, which will be formatted, but the label itself cannot be queried by such
  //! name.
  ASMJIT_API Error newNamedLabelEntry(LabelEntry** entryOut, const char* name, size_t nameSize, LabelType type, uint32_t parentId = Globals::kInvalidId) noexcept;

  //! Returns a label by name.
  //!
  //! If the named label doesn't a default constructed \ref Label is returned,
  //! which has its id set to \ref Globals::kInvalidId.
  ASMJIT_INLINE_NODEBUG Label labelByName(const char* name, size_t nameSize = SIZE_MAX, uint32_t parentId = Globals::kInvalidId) noexcept {
    return Label(labelIdByName(name, nameSize, parentId));
  }

  //! Returns a label id by name.
  //!
  //! If the named label doesn't exist \ref Globals::kInvalidId is returned.
  ASMJIT_API uint32_t labelIdByName(const char* name, size_t nameSize = SIZE_MAX, uint32_t parentId = Globals::kInvalidId) noexcept;

  //! Tests whether there are any unresolved label links.
  ASMJIT_INLINE_NODEBUG bool hasUnresolvedLinks() const noexcept { return _unresolvedLinkCount != 0; }
  //! Returns the number of label links, which are unresolved.
  ASMJIT_INLINE_NODEBUG size_t unresolvedLinkCount() const noexcept { return _unresolvedLinkCount; }

  //! Creates a new label-link used to store information about yet unbound labels.
  //!
  //! Returns `null` if the allocation failed.
  ASMJIT_API LabelLink* newLabelLink(LabelEntry* le, uint32_t sectionId, size_t offset, intptr_t rel, const OffsetFormat& format) noexcept;

  //! Resolves cross-section links (`LabelLink`) associated with each label that was used as a destination in code
  //! of a different section. It's only useful to people that use multiple sections as it will do nothing if the code
  //! only contains a single section in which cross-section links are not possible.
  ASMJIT_API Error resolveUnresolvedLinks() noexcept;

  //! Binds a label to a given `sectionId` and `offset` (relative to start of the section).
  //!
  //! This function is generally used by `BaseAssembler::bind()` to do the heavy lifting.
  ASMJIT_API Error bindLabel(const Label& label, uint32_t sectionId, uint64_t offset) noexcept;

  //! \}

  //! \name Relocations
  //! \{

  //! Tests whether the code contains relocation entries.
  ASMJIT_INLINE_NODEBUG bool hasRelocEntries() const noexcept { return !_relocations.empty(); }
  //! Returns array of `RelocEntry*` records.
  ASMJIT_INLINE_NODEBUG const ZoneVector<RelocEntry*>& relocEntries() const noexcept { return _relocations; }

  //! Returns a RelocEntry of the given `id`.
  ASMJIT_INLINE_NODEBUG RelocEntry* relocEntry(uint32_t id) const noexcept { return _relocations[id]; }

  //! Creates a new relocation entry of type `relocType`.
  //!
  //! Additional fields can be set after the relocation entry was created.
  ASMJIT_API Error newRelocEntry(RelocEntry** dst, RelocType relocType) noexcept;

  //! \}

  //! \name Utilities
  //! \{

  //! Flattens all sections by recalculating their offsets, starting at 0.
  //!
  //! \note This should never be called more than once.
  ASMJIT_API Error flatten() noexcept;

  //! Returns computed the size of code & data of all sections.
  //!
  //! \note All sections will be iterated over and the code size returned would represent the minimum code size of
  //! all combined sections after applying minimum alignment. Code size may decrease after calling `flatten()` and
  //! `relocateToBase()`.
  ASMJIT_API size_t codeSize() const noexcept;

  //! Relocates the code to the given `baseAddress`.
  //!
  //! \param baseAddress Absolute base address where the code will be relocated to. Please note that nothing is
  //! copied to such base address, it's just an absolute value used by the relocation code to resolve all stored
  //! relocations.
  //!
  //! \note This should never be called more than once.
  ASMJIT_API Error relocateToBase(uint64_t baseAddress) noexcept;

  //! Copies a single section into `dst`.
  ASMJIT_API Error copySectionData(void* dst, size_t dstSize, uint32_t sectionId, CopySectionFlags copyFlags = CopySectionFlags::kNone) noexcept;

  //! Copies all sections into `dst`.
  //!
  //! This should only be used if the data was flattened and there are no gaps between the sections. The `dstSize`
  //! is always checked and the copy will never write anything outside the provided buffer.
  ASMJIT_API Error copyFlattenedData(void* dst, size_t dstSize, CopySectionFlags copyFlags = CopySectionFlags::kNone) noexcept;

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_CODEHOLDER_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/codewriter.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/codeholder.h"
#include "../core/codewriter_p.h"
#include "../arm/armutils.h"

ASMJIT_BEGIN_NAMESPACE

bool CodeWriterUtils::encodeOffset32(uint32_t* dst, int64_t offset64, const OffsetFormat& format) noexcept {
  uint32_t bitCount = format.immBitCount();
  uint32_t bitShift = format.immBitShift();
  uint32_t discardLsb = format.immDiscardLsb();

  // Invalid offset (should not happen).
  if (!bitCount || bitCount > format.valueSize() * 8u)
    return false;

  uint32_t value;
  uint32_t u = 0;
  bool unsignedLogic = format.type() == OffsetType::kUnsignedOffset;

  // First handle all offsets that use additional field for their sign and the offset is encoded as its
  // absolute value.
  if (format.hasSignBit()) {
    u = uint32_t(offset64 >= 0);
    if (u == 0)
      offset64 = -offset64;
    unsignedLogic = true;
  }

  // First handle all unsigned offset types.
  if (unsignedLogic) {
    if (discardLsb) {
      ASMJIT_ASSERT(discardLsb <= 32);
      if ((offset64 & Support::lsbMask<uint32_t>(discardLsb)) != 0)
        return false;
      offset64 = int64_t(uint64_t(offset64) >> discardLsb);
    }

    value = uint32_t(offset64 & Support::lsbMask<uint32_t>(bitCount));
    if (value != offset64)
      return false;
  }
  else {
    // The rest of OffsetType options are all signed.
    if (discardLsb) {
      ASMJIT_ASSERT(discardLsb <= 32);
      if ((offset64 & Support::lsbMask<uint32_t>(discardLsb)) != 0)
        return false;
      offset64 >>= discardLsb;
    }

    if (!Support::isInt32(offset64))
      return false;

    value = uint32_t(int32_t(offset64));
    if (!Support::isEncodableOffset32(int32_t(value), bitCount))
      return false;
  }

  switch (format.type()) {
    case OffsetType::kSignedOffset:
    case OffsetType::kUnsignedOffset: {
      *dst = (value & Support::lsbMask<uint32_t>(bitCount)) << bitShift;
      return true;
    }

    // Opcode: {.....|imm:1|..N.N|......|imm:3|....|imm:8}
    case OffsetType::kThumb32_ADR: {
      // Sanity checks.
      if (format.valueSize() != 4 || bitCount != 12 || bitShift != 0)
        return false;

      uint32_t imm8 = (value & 0x00FFu);
      uint32_t imm3 = (value & 0x0700u) << (12 - 8);
      uint32_t imm1 = (value & 0x0800u) << (26 - 11);
      uint32_t n = u ^ 1u;

      *dst = imm8 | imm3 | imm1 | (n << 21) | (n << 23);
      return true;
    }

    // Opcode: {....|.|imm[22]|imm[19:10]|..|ja|.|jb|imm[9:0]|.}
    case OffsetType::kThumb32_BLX:
      // The calculation is the same as `B`, but the first LSB bit must be zero, so account for that.
      value <<= 1;
      ASMJIT_FALLTHROUGH;

    // Opcode: {....|.|imm[23]|imm[20:11]|..|ja|.|jb|imm[10:0]}
    case OffsetType::kThumb32_B: {
      // Sanity checks.
      if (format.valueSize() != 4)
        return false;

      uint32_t ia = (value & 0x0007FFu);
      uint32_t ib = (value & 0x1FF800u) << (16 - 11);
      uint32_t ic = (value & 0x800000u) << (26 - 23);
      uint32_t ja = ((~value >> 23) ^ (value >> 22)) & 1u;
      uint32_t jb = ((~value >> 23) ^ (value >> 21)) & 1u;

      *dst = ia | ib | ic | (ja << 14) | (jb << 11);
      return true;
    }

    // Opcode: {....|.|imm[19]|....|imm[16:11]|..|ja|.|jb|imm[10:0]}
    case OffsetType::kThumb32_BCond: {
      // Sanity checks.
      if (format.valueSize() != 4 || bitCount != 20 || bitShift != 0)
        return false;

      uint32_t ia = (value & 0x0007FFu);
      uint32_t ib = (value & 0x01F800u) << (16 - 11);
      uint32_t ic = (value & 0x080000u) << (26 - 19);
      uint32_t ja = ((~value >> 19) ^ (value >> 22)) & 1u;
      uint32_t jb = ((~value >> 19) ^ (value >> 21)) & 1u;

      *dst = ia | ib | ic | (ja << 14) | (jb << 11);
      return true;
    }

    case OffsetType::kAArch32_ADR: {
      uint32_t encodedImm;
      if (!arm::Utils::encodeAArch32Imm(value, &encodedImm))
        return false;

      *dst = (Support::bitMask(22) << u) | (encodedImm << bitShift);
      return true;
    }

    case OffsetType::kAArch32_U23_SignedOffset: {
      *dst = (value << bitShift) | (u << 23);
      return true;
    }

    case OffsetType::kAArch32_U23_0To3At0_4To7At8: {
      // Sanity checks.
      if (format.valueSize() != 4 || bitCount != 8 || bitShift != 0)
        return false;

      uint32_t immLo = (value & 0x0Fu);
      uint32_t immHi = (value & 0xF0u) << (8 - 4);

      *dst = immLo | immHi | (u << 23);
      return true;
    }

    case OffsetType::kAArch32_1To24At0_0At24: {
      // Sanity checks.
      if (format.valueSize() != 4 || bitCount != 25 || bitShift != 0)
        return false;

      uint32_t immLo = (value & 0x0000001u) << 24;
      uint32_t immHi = (value & 0x1FFFFFEu) >> 1;

      *dst = immLo | immHi;
      return true;
    }

    case OffsetType::kAArch64_ADR:
    case OffsetType::kAArch64_ADRP: {
      // Sanity checks.
      if (format.valueSize() != 4 || bitCount != 21 || bitShift != 5)
        return false;

      uint32_t immLo = value & 0x3u;
      uint32_t immHi = (value >> 2) & Support::lsbMask<uint32_t>(19);

      *dst = (immLo << 29) | (immHi << 5);
      return true;
    }

    default:
      return false;
  }
}

bool CodeWriterUtils::encodeOffset64(uint64_t* dst, int64_t offset64, const OffsetFormat& format) noexcept {
  uint32_t bitCount = format.immBitCount();
  uint32_t discardLsb = format.immDiscardLsb();

  if (!bitCount || bitCount > format.valueSize() * 8u)
    return false;

  uint64_t value;

  // First handle all unsigned offset types.
  if (format.type() == OffsetType::kUnsignedOffset) {
    if (discardLsb) {
      ASMJIT_ASSERT(discardLsb <= 32);
      if ((offset64 & Support::lsbMask<uint32_t>(discardLsb)) != 0)
        return false;
      offset64 = int64_t(uint64_t(offset64) >> discardLsb);
    }

    value = uint64_t(offset64) & Support::lsbMask<uint64_t>(bitCount);
    if (value != uint64_t(offset64))
      return false;
  }
  else {
    // The rest of OffsetType options are all signed.
    if (discardLsb) {
      ASMJIT_ASSERT(discardLsb <= 32);
      if ((offset64 & Support::lsbMask<uint32_t>(discardLsb)) != 0)
        return false;
      offset64 >>= discardLsb;
    }

    if (!Support::isEncodableOffset64(offset64, bitCount))
      return false;

    value = uint64_t(offset64);
  }

  switch (format.type()) {
    case OffsetType::kSignedOffset:
    case OffsetType::kUnsignedOffset: {
      *dst = (value & Support::lsbMask<uint64_t>(bitCount)) << format.immBitShift();
      return true;
    }

    default:
      return false;
  }
}

bool CodeWriterUtils::writeOffset(void* dst, int64_t offset64, const OffsetFormat& format) noexcept {
  // Offset the destination by ValueOffset so the `dst` points to the
  // patched word instead of the beginning of the patched region.
  dst = static_cast<char*>(dst) + format.valueOffset();

  switch (format.valueSize()) {
    case 1: {
      uint32_t mask;
      if (!encodeOffset32(&mask, offset64, format))
        return false;

      Support::writeU8(dst, uint8_t(Support::readU8(dst) | mask));
      return true;
    }

    case 2: {
      uint32_t mask;
      if (!encodeOffset32(&mask, offset64, format))
        return false;

      Support::writeU16uLE(dst, uint16_t(Support::readU16uLE(dst) | mask));
      return true;
    }

    case 4: {
      uint32_t mask;
      if (!encodeOffset32(&mask, offset64, format)) {
        return false;
      }

      Support::writeU32uLE(dst, Support::readU32uLE(dst) | mask);
      return true;
    }

    case 8: {
      uint64_t mask;
      if (!encodeOffset64(&mask, offset64, format))
        return false;

      Support::writeU64uLE(dst, Support::readU64uLE(dst) | mask);
      return true;
    }

    default:
      return false;
  }
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/codewriter_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_CODEBUFFERWRITER_P_H_INCLUDED
#define ASMJIT_CORE_CODEBUFFERWRITER_P_H_INCLUDED

#include "../core/assembler.h"
#include "../core/codebuffer.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_assembler
//! \{

struct OffsetFormat;

//! Helper that is used to write into a \ref CodeBuffer held by \ref BaseAssembler.
class CodeWriter {
public:
  uint8_t* _cursor;

  ASMJIT_FORCE_INLINE explicit CodeWriter(BaseAssembler* a) noexcept
    : _cursor(a->_bufferPtr) {}

  ASMJIT_FORCE_INLINE Error ensureSpace(BaseAssembler* a, size_t n) noexcept {
    size_t remainingSpace = (size_t)(a->_bufferEnd - _cursor);
    if (ASMJIT_UNLIKELY(remainingSpace < n)) {
      CodeBuffer& buffer = a->_section->_buffer;
      Error err = a->_code->growBuffer(&buffer, n);
      if (ASMJIT_UNLIKELY(err))
        return a->reportError(err);
      _cursor = a->_bufferPtr;
    }
    return kErrorOk;
  }

  ASMJIT_FORCE_INLINE uint8_t* cursor() const noexcept { return _cursor; }
  ASMJIT_FORCE_INLINE void setCursor(uint8_t* cursor) noexcept { _cursor = cursor; }
  ASMJIT_FORCE_INLINE void advance(size_t n) noexcept { _cursor += n; }

  ASMJIT_FORCE_INLINE size_t offsetFrom(uint8_t* from) const noexcept {
    ASMJIT_ASSERT(_cursor >= from);
    return (size_t)(_cursor - from);
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void emit8(T val) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    _cursor[0] = uint8_t(U(val) & U(0xFF));
    _cursor++;
  }

  template<typename T, typename Y>
  ASMJIT_FORCE_INLINE void emit8If(T val, Y cond) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    ASMJIT_ASSERT(size_t(cond) <= 1u);

    _cursor[0] = uint8_t(U(val) & U(0xFF));
    _cursor += size_t(cond);
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void emit16uLE(T val) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    Support::writeU16uLE(_cursor, uint16_t(U(val) & 0xFFFFu));
    _cursor += 2;
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void emit16uBE(T val) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    Support::writeU16uBE(_cursor, uint16_t(U(val) & 0xFFFFu));
    _cursor += 2;
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void emit32uLE(T val) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    Support::writeU32uLE(_cursor, uint32_t(U(val) & 0xFFFFFFFFu));
    _cursor += 4;
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void emit32uBE(T val) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    Support::writeU32uBE(_cursor, uint32_t(U(val) & 0xFFFFFFFFu));
    _cursor += 4;
  }

  ASMJIT_FORCE_INLINE void emitData(const void* data, size_t size) noexcept {
    ASMJIT_ASSERT(size != 0);
    memcpy(_cursor, data, size);
    _cursor += size;
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void emitValueLE(const T& value, size_t size) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    ASMJIT_ASSERT(size <= sizeof(T));

    U v = U(value);
    for (uint32_t i = 0; i < size; i++) {
      _cursor[i] = uint8_t(v & 0xFFu);
      v >>= 8;
    }
    _cursor += size;
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void emitValueBE(const T& value, size_t size) noexcept {
    typedef typename std::make_unsigned<T>::type U;
    ASMJIT_ASSERT(size <= sizeof(T));

    U v = U(value);
    for (uint32_t i = 0; i < size; i++) {
      _cursor[i] = uint8_t(v >> (sizeof(T) - 8));
      v <<= 8;
    }
    _cursor += size;
  }

  ASMJIT_FORCE_INLINE void emitZeros(size_t size) noexcept {
    ASMJIT_ASSERT(size != 0);
    memset(_cursor, 0, size);
    _cursor += size;
  }

  ASMJIT_FORCE_INLINE void remove8(uint8_t* where) noexcept {
    ASMJIT_ASSERT(where < _cursor);

    uint8_t* p = where;
    while (++p != _cursor)
      p[-1] = p[0];
    _cursor--;
  }

  template<typename T>
  ASMJIT_FORCE_INLINE void insert8(uint8_t* where, T val) noexcept {
    uint8_t* p = _cursor;

    while (p != where) {
      p[0] = p[-1];
      p--;
    }

    *p = uint8_t(val & 0xFF);
    _cursor++;
  }

  ASMJIT_FORCE_INLINE void done(BaseAssembler* a) noexcept {
    CodeBuffer& buffer = a->_section->_buffer;
    size_t newSize = (size_t)(_cursor - a->_bufferData);
    ASMJIT_ASSERT(newSize <= buffer.capacity());

    a->_bufferPtr = _cursor;
    buffer._size = Support::max(buffer._size, newSize);
  }
};

//! Code writer utilities.
namespace CodeWriterUtils {

bool encodeOffset32(uint32_t* dst, int64_t offset64, const OffsetFormat& format) noexcept;
bool encodeOffset64(uint64_t* dst, int64_t offset64, const OffsetFormat& format) noexcept;

bool writeOffset(void* dst, int64_t offset64, const OffsetFormat& format) noexcept;

} // {CodeWriterUtils}

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_CODEBUFFERWRITER_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/compiler.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/assembler.h"
#include "../core/builder_p.h"
#include "../core/compiler.h"
#include "../core/cpuinfo.h"
#include "../core/logger.h"
#include "../core/rapass_p.h"
#include "../core/rastack_p.h"
#include "../core/support.h"
#include "../core/type.h"

ASMJIT_BEGIN_NAMESPACE

// GlobalConstPoolPass
// ===================

class GlobalConstPoolPass : public Pass {
public:
  typedef Pass Base;
public:
  ASMJIT_NONCOPYABLE(GlobalConstPoolPass)

  GlobalConstPoolPass() noexcept : Pass("GlobalConstPoolPass") {}

  Error run(Zone* zone, Logger* logger) override {
    DebugUtils::unused(zone, logger);

    // Flush the global constant pool.
    BaseCompiler* compiler = static_cast<BaseCompiler*>(_cb);
    ConstPoolNode* globalConstPool = compiler->_constPools[uint32_t(ConstPoolScope::kGlobal)];

    if (globalConstPool) {
      compiler->addAfter(globalConstPool, compiler->lastNode());
      compiler->_constPools[uint32_t(ConstPoolScope::kGlobal)] = nullptr;
    }

    return kErrorOk;
  }
};

// BaseCompiler - Construction & Destruction
// =========================================

BaseCompiler::BaseCompiler() noexcept
  : BaseBuilder(),
    _func(nullptr),
    _vRegZone(4u * 1024u),
    _vRegArray(),
    _constPools { nullptr, nullptr } {
  _emitterType = EmitterType::kCompiler;
  _validationFlags = ValidationFlags::kEnableVirtRegs;
}
BaseCompiler::~BaseCompiler() noexcept {}

// BaseCompiler - Function Management
// ==================================

Error BaseCompiler::newFuncNode(FuncNode** out, const FuncSignature& signature) {
  *out = nullptr;

  // Create FuncNode together with all the required surrounding nodes.
  FuncNode* funcNode;
  ASMJIT_PROPAGATE(_newNodeT<FuncNode>(&funcNode));
  ASMJIT_PROPAGATE(newLabelNode(&funcNode->_exitNode));
  ASMJIT_PROPAGATE(_newNodeT<SentinelNode>(&funcNode->_end, SentinelType::kFuncEnd));

  // Initialize the function's detail info.
  Error err = funcNode->detail().init(signature, environment());
  if (ASMJIT_UNLIKELY(err))
    return reportError(err);

  // If the Target guarantees greater stack alignment than required by the calling convention
  // then override it as we can prevent having to perform dynamic stack alignment
  uint32_t environmentStackAlignment = _environment.stackAlignment();

  if (funcNode->_funcDetail._callConv.naturalStackAlignment() < environmentStackAlignment)
    funcNode->_funcDetail._callConv.setNaturalStackAlignment(environmentStackAlignment);

  // Initialize the function frame.
  err = funcNode->_frame.init(funcNode->_funcDetail);
  if (ASMJIT_UNLIKELY(err))
    return reportError(err);

  // Allocate space for function arguments.
  funcNode->_args = nullptr;
  if (funcNode->argCount() != 0) {
    funcNode->_args = _allocator.allocT<FuncNode::ArgPack>(funcNode->argCount() * sizeof(FuncNode::ArgPack));
    if (ASMJIT_UNLIKELY(!funcNode->_args))
      return reportError(DebugUtils::errored(kErrorOutOfMemory));
    memset(funcNode->_args, 0, funcNode->argCount() * sizeof(FuncNode::ArgPack));
  }

  ASMJIT_PROPAGATE(registerLabelNode(funcNode));

  *out = funcNode;
  return kErrorOk;
}

Error BaseCompiler::addFuncNode(FuncNode** out, const FuncSignature& signature) {
  State state = _grabState();

  ASMJIT_PROPAGATE(newFuncNode(out, signature));
  ASMJIT_ASSUME(*out != nullptr);

  BaseBuilder_assignInlineComment(this, *out, state.comment);

  addFunc(*out);
  return kErrorOk;
}

Error BaseCompiler::newFuncRetNode(FuncRetNode** out, const Operand_& o0, const Operand_& o1) {
  uint32_t opCount = !o1.isNone() ? 2u : !o0.isNone() ? 1u : 0u;
  FuncRetNode* node;

  ASMJIT_PROPAGATE(_newNodeT<FuncRetNode>(&node));
  ASMJIT_ASSUME(node != nullptr);

  node->setOpCount(opCount);
  node->setOp(0, o0);
  node->setOp(1, o1);
  node->resetOpRange(2, node->opCapacity());

  *out = node;
  return kErrorOk;
}

Error BaseCompiler::addFuncRetNode(FuncRetNode** out, const Operand_& o0, const Operand_& o1) {
  State state = _grabState();

  ASMJIT_PROPAGATE(newFuncRetNode(out, o0, o1));
  ASMJIT_ASSUME(*out != nullptr);

  BaseBuilder_assignInlineComment(this, *out, state.comment);

  addNode(*out);
  return kErrorOk;
}

FuncNode* BaseCompiler::addFunc(FuncNode* func) {
  _func = func;

  addNode(func);                 // Function node.
  BaseNode* prev = cursor();     // {CURSOR}.
  addNode(func->exitNode());     // Function exit label.
  addNode(func->endNode());      // Function end sentinel.

  _setCursor(prev);
  return func;
}

Error BaseCompiler::endFunc() {
  FuncNode* func = _func;
  resetState();

  if (ASMJIT_UNLIKELY(!func))
    return reportError(DebugUtils::errored(kErrorInvalidState));

  // Add the local constant pool at the end of the function (if exists).
  ConstPoolNode* localConstPool = _constPools[uint32_t(ConstPoolScope::kLocal)];
  if (localConstPool) {
    setCursor(func->endNode()->prev());
    addNode(localConstPool);
    _constPools[uint32_t(ConstPoolScope::kLocal)] = nullptr;
  }

  // Mark as finished.
  _func = nullptr;

  SentinelNode* end = func->endNode();
  setCursor(end);

  return kErrorOk;
}

// BaseCompiler - Function Invocation
// ==================================

Error BaseCompiler::newInvokeNode(InvokeNode** out, InstId instId, const Operand_& o0, const FuncSignature& signature) {
  InvokeNode* node;
  ASMJIT_PROPAGATE(_newNodeT<InvokeNode>(&node, instId, InstOptions::kNone));

  node->setOpCount(1);
  node->setOp(0, o0);
  node->resetOpRange(1, node->opCapacity());

  Error err = node->detail().init(signature, environment());
  if (ASMJIT_UNLIKELY(err))
    return reportError(err);

  // Skip the allocation if there are no arguments.
  uint32_t argCount = signature.argCount();
  if (argCount) {
    node->_args = static_cast<InvokeNode::OperandPack*>(_allocator.alloc(argCount * sizeof(InvokeNode::OperandPack)));
    if (!node->_args)
      return reportError(DebugUtils::errored(kErrorOutOfMemory));
    memset(node->_args, 0, argCount * sizeof(InvokeNode::OperandPack));
  }

  *out = node;
  return kErrorOk;
}

Error BaseCompiler::addInvokeNode(InvokeNode** out, InstId instId, const Operand_& o0, const FuncSignature& signature) {
  State state = _grabState();

  ASMJIT_PROPAGATE(newInvokeNode(out, instId, o0, signature));
  ASMJIT_ASSUME(*out != nullptr);

  BaseBuilder_assignInstState(this, *out, state);
  addNode(*out);
  return kErrorOk;
}

// BaseCompiler - Virtual Registers
// ================================

static void BaseCompiler_assignGenericName(BaseCompiler* self, VirtReg* vReg) {
  uint32_t index = unsigned(Operand::virtIdToIndex(vReg->_id));

  char buf[64];
  int size = snprintf(buf, ASMJIT_ARRAY_SIZE(buf), "%%%u", unsigned(index));

  ASMJIT_ASSERT(size > 0 && size < int(ASMJIT_ARRAY_SIZE(buf)));
  vReg->_name.setData(&self->_dataZone, buf, unsigned(size));
}

Error BaseCompiler::newVirtReg(VirtReg** out, TypeId typeId, OperandSignature signature, const char* name) {
  *out = nullptr;
  uint32_t index = _vRegArray.size();

  if (ASMJIT_UNLIKELY(index >= uint32_t(Operand::kVirtIdCount)))
    return reportError(DebugUtils::errored(kErrorTooManyVirtRegs));

  if (ASMJIT_UNLIKELY(_vRegArray.willGrow(&_allocator) != kErrorOk))
    return reportError(DebugUtils::errored(kErrorOutOfMemory));

  VirtReg* vReg = _vRegZone.allocZeroedT<VirtReg>();
  if (ASMJIT_UNLIKELY(!vReg))
    return reportError(DebugUtils::errored(kErrorOutOfMemory));

  uint32_t size = TypeUtils::sizeOf(typeId);
  uint32_t alignment = Support::min<uint32_t>(size, 64);

  vReg = new(Support::PlacementNew{vReg}) VirtReg(signature, Operand::indexToVirtId(index), size, alignment, typeId);

#ifndef ASMJIT_NO_LOGGING
  if (name && name[0] != '\0')
    vReg->_name.setData(&_dataZone, name, SIZE_MAX);
  else
    BaseCompiler_assignGenericName(this, vReg);
#else
  DebugUtils::unused(name);
#endif

  _vRegArray.appendUnsafe(vReg);
  *out = vReg;

  return kErrorOk;
}

Error BaseCompiler::_newReg(BaseReg* out, TypeId typeId, const char* name) {
  OperandSignature regSignature;
  out->reset();

  Error err = ArchUtils::typeIdToRegSignature(arch(), typeId, &typeId, &regSignature);
  if (ASMJIT_UNLIKELY(err))
    return reportError(err);

  VirtReg* vReg;
  ASMJIT_PROPAGATE(newVirtReg(&vReg, typeId, regSignature, name));
  ASMJIT_ASSUME(vReg != nullptr);

  out->_initReg(regSignature, vReg->id());
  return kErrorOk;
}

Error BaseCompiler::_newRegFmt(BaseReg* out, TypeId typeId, const char* fmt, ...) {
  va_list ap;
  StringTmp<256> sb;

  va_start(ap, fmt);
  sb.appendVFormat(fmt, ap);
  va_end(ap);

  return _newReg(out, typeId, sb.data());
}

Error BaseCompiler::_newReg(BaseReg* out, const BaseReg& ref, const char* name) {
  out->reset();

  OperandSignature regSignature;
  TypeId typeId;

  if (isVirtRegValid(ref)) {
    VirtReg* vRef = virtRegByReg(ref);
    typeId = vRef->typeId();

    // NOTE: It's possible to cast one register type to another if it's the same register group. However, VirtReg
    // always contains the TypeId that was used to create the register. This means that in some cases we may end
    // up having different size of `ref` and `vRef`. In such case we adjust the TypeId to match the `ref` register
    // type instead of the original register type, which should be the expected behavior.
    uint32_t typeSize = TypeUtils::sizeOf(typeId);
    uint32_t refSize = ref.size();

    if (typeSize != refSize) {
      if (TypeUtils::isInt(typeId)) {
        // GP register - change TypeId to match `ref`, but keep sign of `vRef`.
        switch (refSize) {
          case  1: typeId = TypeId(uint32_t(TypeId::kInt8 ) | (uint32_t(typeId) & 1)); break;
          case  2: typeId = TypeId(uint32_t(TypeId::kInt16) | (uint32_t(typeId) & 1)); break;
          case  4: typeId = TypeId(uint32_t(TypeId::kInt32) | (uint32_t(typeId) & 1)); break;
          case  8: typeId = TypeId(uint32_t(TypeId::kInt64) | (uint32_t(typeId) & 1)); break;
          default: typeId = TypeId::kVoid; break;
        }
      }
      else if (TypeUtils::isMmx(typeId)) {
        // MMX register - always use 64-bit.
        typeId = TypeId::kMmx64;
      }
      else if (TypeUtils::isMask(typeId)) {
        // Mask register - change TypeId to match `ref` size.
        switch (refSize) {
          case  1: typeId = TypeId::kMask8; break;
          case  2: typeId = TypeId::kMask16; break;
          case  4: typeId = TypeId::kMask32; break;
          case  8: typeId = TypeId::kMask64; break;
          default: typeId = TypeId::kVoid; break;
        }
      }
      else {
        // Vector register - change TypeId to match `ref` size, keep vector metadata.
        TypeId scalarTypeId = TypeUtils::scalarOf(typeId);
        switch (refSize) {
          case 16: typeId = TypeUtils::scalarToVector(scalarTypeId, TypeId::_kVec128Start); break;
          case 32: typeId = TypeUtils::scalarToVector(scalarTypeId, TypeId::_kVec256Start); break;
          case 64: typeId = TypeUtils::scalarToVector(scalarTypeId, TypeId::_kVec512Start); break;
          default: typeId = TypeId::kVoid; break;
        }
      }

      if (typeId == TypeId::kVoid)
        return reportError(DebugUtils::errored(kErrorInvalidState));
    }
  }
  else {
    typeId = ArchTraits::byArch(arch()).regTypeToTypeId(ref.type());
  }

  Error err = ArchUtils::typeIdToRegSignature(arch(), typeId, &typeId, &regSignature);
  if (ASMJIT_UNLIKELY(err))
    return reportError(err);

  VirtReg* vReg;
  ASMJIT_PROPAGATE(newVirtReg(&vReg, typeId, regSignature, name));
  ASMJIT_ASSUME(vReg != nullptr);

  out->_initReg(regSignature, vReg->id());
  return kErrorOk;
}

Error BaseCompiler::_newRegFmt(BaseReg* out, const BaseReg& ref, const char* fmt, ...) {
  va_list ap;
  StringTmp<256> sb;

  va_start(ap, fmt);
  sb.appendVFormat(fmt, ap);
  va_end(ap);

  return _newReg(out, ref, sb.data());
}

Error BaseCompiler::_newStack(BaseMem* out, uint32_t size, uint32_t alignment, const char* name) {
  out->reset();

  if (size == 0)
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  if (alignment == 0)
    alignment = 1;

  if (!Support::isPowerOf2(alignment))
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  if (alignment > 64)
    alignment = 64;

  VirtReg* vReg;
  ASMJIT_PROPAGATE(newVirtReg(&vReg, TypeId::kVoid, OperandSignature{0}, name));
  ASMJIT_ASSUME(vReg != nullptr);

  vReg->_virtSize = size;
  vReg->_isStack = true;
  vReg->_alignment = uint8_t(alignment);

  // Set the memory operand to GPD/GPQ and its id to VirtReg.
  *out = BaseMem(OperandSignature::fromOpType(OperandType::kMem) |
                 OperandSignature::fromMemBaseType(_gpSignature.regType()) |
                 OperandSignature::fromBits(OperandSignature::kMemRegHomeFlag),
                 vReg->id(), 0, 0);
  return kErrorOk;
}

Error BaseCompiler::setStackSize(uint32_t virtId, uint32_t newSize, uint32_t newAlignment) {
  if (!isVirtIdValid(virtId))
    return DebugUtils::errored(kErrorInvalidVirtId);

  if (newAlignment && !Support::isPowerOf2(newAlignment))
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  if (newAlignment > 64)
    newAlignment = 64;

  VirtReg* vReg = virtRegById(virtId);
  if (newSize)
    vReg->_virtSize = newSize;

  if (newAlignment)
    vReg->_alignment = uint8_t(newAlignment);

  // This is required if the RAPass is already running. There is a chance that a stack-slot has been already
  // allocated and in that case it has to be updated as well, otherwise we would allocate wrong amount of memory.
  RAWorkReg* workReg = vReg->_workReg;
  if (workReg && workReg->_stackSlot) {
    workReg->_stackSlot->_size = vReg->_virtSize;
    workReg->_stackSlot->_alignment = vReg->_alignment;
  }

  return kErrorOk;
}

Error BaseCompiler::_newConst(BaseMem* out, ConstPoolScope scope, const void* data, size_t size) {
  out->reset();

  if (uint32_t(scope) > 1)
    return reportError(DebugUtils::errored(kErrorInvalidArgument));

  if (!_constPools[uint32_t(scope)])
    ASMJIT_PROPAGATE(newConstPoolNode(&_constPools[uint32_t(scope)]));

  ConstPoolNode* pool = _constPools[uint32_t(scope)];
  size_t off;
  Error err = pool->add(data, size, off);

  if (ASMJIT_UNLIKELY(err))
    return reportError(err);

  *out = BaseMem(OperandSignature::fromOpType(OperandType::kMem) |
                 OperandSignature::fromMemBaseType(RegType::kLabelTag) |
                 OperandSignature::fromSize(uint32_t(size)),
                 pool->labelId(), 0, int32_t(off));
  return kErrorOk;
}

void BaseCompiler::rename(const BaseReg& reg, const char* fmt, ...) {
  if (!reg.isVirtReg()) return;

  VirtReg* vReg = virtRegById(reg.id());
  if (!vReg) return;

  if (fmt && fmt[0] != '\0') {
    char buf[128];
    va_list ap;

    va_start(ap, fmt);
    vsnprintf(buf, ASMJIT_ARRAY_SIZE(buf), fmt, ap);
    va_end(ap);

    vReg->_name.setData(&_dataZone, buf, SIZE_MAX);
  }
  else {
    BaseCompiler_assignGenericName(this, vReg);
  }
}

// BaseCompiler - Jump Annotations
// ===============================

Error BaseCompiler::newJumpNode(JumpNode** out, InstId instId, InstOptions instOptions, const Operand_& o0, JumpAnnotation* annotation) {
  JumpNode* node = _allocator.allocT<JumpNode>();
  uint32_t opCount = 1;

  *out = node;
  if (ASMJIT_UNLIKELY(!node))
    return reportError(DebugUtils::errored(kErrorOutOfMemory));

  node = new(Support::PlacementNew{node}) JumpNode(this, instId, instOptions, opCount, annotation);
  node->setOp(0, o0);
  node->resetOpRange(opCount, JumpNode::kBaseOpCapacity);

  return kErrorOk;
}

Error BaseCompiler::emitAnnotatedJump(InstId instId, const Operand_& o0, JumpAnnotation* annotation) {
  State state = _grabState();

  JumpNode* node;
  ASMJIT_PROPAGATE(newJumpNode(&node, instId, state.options, o0, annotation));

  node->setExtraReg(state.extraReg);
  BaseBuilder_assignInlineComment(this, node, state.comment);

  addNode(node);
  return kErrorOk;
}

JumpAnnotation* BaseCompiler::newJumpAnnotation() {
  if (_jumpAnnotations.grow(&_allocator, 1) != kErrorOk) {
    reportError(DebugUtils::errored(kErrorOutOfMemory));
    return nullptr;
  }

  uint32_t id = _jumpAnnotations.size();
  JumpAnnotation* jumpAnnotation = _allocator.newT<JumpAnnotation>(this, id);

  if (!jumpAnnotation) {
    reportError(DebugUtils::errored(kErrorOutOfMemory));
    return nullptr;
  }

  _jumpAnnotations.appendUnsafe(jumpAnnotation);
  return jumpAnnotation;
}

// BaseCompiler - Events
// =====================

Error BaseCompiler::onAttach(CodeHolder* code) noexcept {
  ASMJIT_PROPAGATE(Base::onAttach(code));

  const ArchTraits& archTraits = ArchTraits::byArch(code->arch());
  RegType nativeRegType = Environment::is32Bit(code->arch()) ? RegType::kGp32 : RegType::kGp64;
  _gpSignature = archTraits.regTypeToSignature(nativeRegType);

  Error err = addPassT<GlobalConstPoolPass>();
  if (ASMJIT_UNLIKELY(err)) {
    onDetach(code);
    return err;
  }

  return kErrorOk;
}

Error BaseCompiler::onDetach(CodeHolder* code) noexcept {
  _func = nullptr;
  _constPools[uint32_t(ConstPoolScope::kLocal)] = nullptr;
  _constPools[uint32_t(ConstPoolScope::kGlobal)] = nullptr;

  _vRegArray.reset();
  _vRegZone.reset();

  return Base::onDetach(code);
}

// FuncPass - Construction & Destruction
// =====================================

FuncPass::FuncPass(const char* name) noexcept
  : Pass(name) {}

// FuncPass - Run
// ==============

Error FuncPass::run(Zone* zone, Logger* logger) {
  BaseNode* node = cb()->firstNode();
  if (!node) return kErrorOk;

  do {
    if (node->type() == NodeType::kFunc) {
      FuncNode* func = node->as<FuncNode>();
      node = func->endNode();
      ASMJIT_PROPAGATE(runOnFunction(zone, logger, func));
    }

    // Find a function by skipping all nodes that are not `NodeType::kFunc`.
    do {
      node = node->next();
    } while (node && node->type() != NodeType::kFunc);
  } while (node);

  return kErrorOk;
}

// [[pure virtual]]
Error FuncPass::runOnFunction(Zone* zone, Logger* logger, FuncNode* func) {
  DebugUtils::unused(zone, logger, func);
  return DebugUtils::errored(kErrorInvalidState);
}

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER

```

`CodeCleaner/asmjit/asmjit/core/compiler.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_COMPILER_H_INCLUDED
#define ASMJIT_CORE_COMPILER_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/assembler.h"
#include "../core/builder.h"
#include "../core/constpool.h"
#include "../core/compilerdefs.h"
#include "../core/func.h"
#include "../core/inst.h"
#include "../core/operand.h"
#include "../core/support.h"
#include "../core/zone.h"
#include "../core/zonevector.h"

ASMJIT_BEGIN_NAMESPACE

class JumpAnnotation;
class JumpNode;
class FuncNode;
class FuncRetNode;
class InvokeNode;

//! \addtogroup asmjit_compiler
//! \{

//! Code emitter that uses virtual registers and performs register allocation.
//!
//! Compiler is a high-level code-generation tool that provides register allocation and automatic handling of function
//! calling conventions. It was primarily designed for merging multiple parts of code into a function without worrying
//! about registers and function calling conventions.
//!
//! BaseCompiler can be used, with a minimum effort, to handle 32-bit and 64-bit code generation within a single code
//! base.
//!
//! BaseCompiler is based on BaseBuilder and contains all the features it provides. It means that the code it stores
//! can be modified (removed, added, injected) and analyzed. When the code is finalized the compiler can emit the code
//! into an Assembler to translate the abstract representation into a machine code.
//!
//! Check out architecture specific compilers for more details and examples:
//!
//!   - \ref x86::Compiler - X86/X64 compiler implementation.
//!   - \ref a64::Compiler - AArch64 compiler implementation.
class ASMJIT_VIRTAPI BaseCompiler : public BaseBuilder {
public:
  ASMJIT_NONCOPYABLE(BaseCompiler)
  typedef BaseBuilder Base;

  //! \name Members
  //! \{

  //! Current function.
  FuncNode* _func;
  //! Allocates `VirtReg` objects.
  Zone _vRegZone;
  //! Stores array of `VirtReg` pointers.
  ZoneVector<VirtReg*> _vRegArray;
  //! Stores jump annotations.
  ZoneVector<JumpAnnotation*> _jumpAnnotations;

  //! Local and global constant pools.
  //!
  //! Local constant pool is flushed with each function, global constant pool is flushed only by \ref finalize().
  ConstPoolNode* _constPools[2];

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `BaseCompiler` instance.
  ASMJIT_API BaseCompiler() noexcept;
  //! Destroys the `BaseCompiler` instance.
  ASMJIT_API ~BaseCompiler() noexcept override;

  //! \}

  //! \name Function Management
  //! \{

  //! Creates a new \ref FuncNode.
  ASMJIT_API Error newFuncNode(FuncNode** ASMJIT_NONNULL(out), const FuncSignature& signature);
  //! Creates a new \ref FuncNode adds it to the instruction stream.
  ASMJIT_API Error addFuncNode(FuncNode** ASMJIT_NONNULL(out), const FuncSignature& signature);

  //! Creates a new \ref FuncRetNode.
  ASMJIT_API Error newFuncRetNode(FuncRetNode** ASMJIT_NONNULL(out), const Operand_& o0, const Operand_& o1);
  //! Creates a new \ref FuncRetNode and adds it to the instruction stream.
  ASMJIT_API Error addFuncRetNode(FuncRetNode** ASMJIT_NONNULL(out), const Operand_& o0, const Operand_& o1);

  //! Returns the current function.
  ASMJIT_INLINE_NODEBUG FuncNode* func() const noexcept { return _func; }

  //! Creates a new \ref FuncNode with the given `signature` and returns it.
  inline FuncNode* newFunc(const FuncSignature& signature) {
    FuncNode* node;
    newFuncNode(&node, signature);
    return node;
  }

  //! Creates a new \ref FuncNode with the given `signature`, adds it to the instruction stream by using
  //! the \ref addFunc(FuncNode*) overload, and returns it.
  inline FuncNode* addFunc(const FuncSignature& signature) {
    FuncNode* node;
    addFuncNode(&node, signature);
    return node;
  }

  //! Adds a function `node` to the instruction stream.
  ASMJIT_API FuncNode* addFunc(FuncNode* ASMJIT_NONNULL(func));
  //! Emits a sentinel that marks the end of the current function.
  ASMJIT_API Error endFunc();

  inline Error addRet(const Operand_& o0, const Operand_& o1) {
    FuncRetNode* node;
    return addFuncRetNode(&node, o0, o1);
  }

  //! \}

  //! \name Function Invocation
  //! \{

  //! Creates a new \ref InvokeNode.
  ASMJIT_API Error newInvokeNode(InvokeNode** ASMJIT_NONNULL(out), InstId instId, const Operand_& o0, const FuncSignature& signature);
  //! Creates a new \ref InvokeNode and adds it to the instruction stream.
  ASMJIT_API Error addInvokeNode(InvokeNode** ASMJIT_NONNULL(out), InstId instId, const Operand_& o0, const FuncSignature& signature);

  //! \}

  //! \name Virtual Registers
  //! \{

  //! Creates a new virtual register representing the given `typeId` and `signature`.
  //!
  //! \note This function is public, but it's not generally recommended to be used by AsmJit users, use architecture
  //! specific `newReg()` functionality instead or functions like \ref _newReg() and \ref _newRegFmt().
  ASMJIT_API Error newVirtReg(VirtReg** ASMJIT_NONNULL(out), TypeId typeId, OperandSignature signature, const char* name);

  //! Creates a new virtual register of the given `typeId` and stores it to `out` operand.
  ASMJIT_API Error _newReg(BaseReg* ASMJIT_NONNULL(out), TypeId typeId, const char* name = nullptr);

  //! Creates a new virtual register of the given `typeId` and stores it to `out` operand.
  //!
  //! \note This version accepts a snprintf() format `fmt` followed by a variadic arguments.
  ASMJIT_API Error _newRegFmt(BaseReg* ASMJIT_NONNULL(out), TypeId typeId, const char* fmt, ...);
  //! \overload
  inline Error _newRegFmt(BaseReg* ASMJIT_NONNULL(out), TypeId typeId) { return _newRegFmt(out, typeId, nullptr); }

  //! Creates a new virtual register compatible with the provided reference register `ref`.
  ASMJIT_API Error _newReg(BaseReg* ASMJIT_NONNULL(out), const BaseReg& ref, const char* name = nullptr);

  //! Creates a new virtual register compatible with the provided reference register `ref`.
  //!
  //! \note This version accepts a snprintf() format `fmt` followed by a variadic arguments.
  ASMJIT_API Error _newRegFmt(BaseReg* ASMJIT_NONNULL(out), const BaseReg& ref, const char* fmt, ...);

  //! Tests whether the given `id` is a valid virtual register id.
  ASMJIT_INLINE_NODEBUG bool isVirtIdValid(uint32_t id) const noexcept {
    uint32_t index = Operand::virtIdToIndex(id);
    return index < _vRegArray.size();
  }
  //! Tests whether the given `reg` is a virtual register having a valid id.
  ASMJIT_INLINE_NODEBUG bool isVirtRegValid(const BaseReg& reg) const noexcept {
    return isVirtIdValid(reg.id());
  }

  //! Returns \ref VirtReg associated with the given `id`.
  inline VirtReg* virtRegById(uint32_t id) const noexcept {
    ASMJIT_ASSERT(isVirtIdValid(id));
    return _vRegArray[Operand::virtIdToIndex(id)];
  }

  //! Returns \ref VirtReg associated with the given `reg`.
  ASMJIT_INLINE_NODEBUG VirtReg* virtRegByReg(const BaseReg& reg) const noexcept { return virtRegById(reg.id()); }

  //! Returns \ref VirtReg associated with the given virtual register `index`.
  //!
  //! \note This is not the same as virtual register id. The conversion between id and its index is implemented
  //! by \ref Operand_::virtIdToIndex() and \ref Operand_::indexToVirtId() functions.
  ASMJIT_INLINE_NODEBUG VirtReg* virtRegByIndex(uint32_t index) const noexcept { return _vRegArray[index]; }

  //! Returns an array of all virtual registers managed by the Compiler.
  ASMJIT_INLINE_NODEBUG const ZoneVector<VirtReg*>& virtRegs() const noexcept { return _vRegArray; }

  //! \name Stack
  //! \{

  //! Creates a new stack of the given `size` and `alignment` and stores it to `out`.
  //!
  //! \note `name` can be used to give the stack a name, for debugging purposes.
  ASMJIT_API Error _newStack(BaseMem* ASMJIT_NONNULL(out), uint32_t size, uint32_t alignment, const char* name = nullptr);

  //! Updates the stack size of a stack created by `_newStack()` by its `virtId`.
  ASMJIT_API Error setStackSize(uint32_t virtId, uint32_t newSize, uint32_t newAlignment = 0);

  //! Updates the stack size of a stack created by `_newStack()`.
  ASMJIT_INLINE_NODEBUG Error setStackSize(const BaseMem& mem, uint32_t newSize, uint32_t newAlignment = 0) {
    return setStackSize(mem.id(), newSize, newAlignment);
  }

  //! \}

  //! \name Constants
  //! \{

  //! Creates a new constant of the given `scope` (see \ref ConstPoolScope).
  //!
  //! This function adds a constant of the given `size` to the built-in \ref ConstPool and stores the reference to that
  //! constant to the `out` operand.
  ASMJIT_API Error _newConst(BaseMem* ASMJIT_NONNULL(out), ConstPoolScope scope, const void* data, size_t size);

  //! \}

  //! \name Miscellaneous
  //! \{

  //! Rename the given virtual register `reg` to a formatted string `fmt`.
  ASMJIT_API void rename(const BaseReg& reg, const char* fmt, ...);

  //! \}

  //! \name Jump Annotations
  //! \{

  ASMJIT_INLINE_NODEBUG const ZoneVector<JumpAnnotation*>& jumpAnnotations() const noexcept {
    return _jumpAnnotations;
  }

  ASMJIT_API Error newJumpNode(JumpNode** ASMJIT_NONNULL(out), InstId instId, InstOptions instOptions, const Operand_& o0, JumpAnnotation* annotation);
  ASMJIT_API Error emitAnnotatedJump(InstId instId, const Operand_& o0, JumpAnnotation* annotation);

  //! Returns a new `JumpAnnotation` instance, which can be used to aggregate possible targets of a jump where the
  //! target is not a label, for example to implement jump tables.
  ASMJIT_API JumpAnnotation* newJumpAnnotation();

  //! \}

  //! \name Events
  //! \{

  ASMJIT_API Error onAttach(CodeHolder* code) noexcept override;
  ASMJIT_API Error onDetach(CodeHolder* code) noexcept override;

  //! \}
};

//! Jump annotation used to annotate jumps.
//!
//! \ref BaseCompiler allows to emit jumps where the target is either register or memory operand. Such jumps cannot be
//! trivially inspected, so instead of doing heuristics AsmJit allows to annotate such jumps with possible targets.
//! Register allocator then uses the annotation to construct control-flow, which is then used by liveness analysis and
//! other tools to prepare ground for register allocation.
class JumpAnnotation {
public:
  ASMJIT_NONCOPYABLE(JumpAnnotation)

  //! \name Members
  //! \{

  //! Compiler that owns this JumpAnnotation.
  BaseCompiler* _compiler;
  //! Annotation identifier.
  uint32_t _annotationId;
  //! Vector of label identifiers, see \ref labelIds().
  ZoneVector<uint32_t> _labelIds;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG JumpAnnotation(BaseCompiler* ASMJIT_NONNULL(compiler), uint32_t annotationId) noexcept
    : _compiler(compiler),
      _annotationId(annotationId) {}

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the compiler that owns this JumpAnnotation.
  ASMJIT_INLINE_NODEBUG BaseCompiler* compiler() const noexcept { return _compiler; }
  //! Returns the annotation id.
  ASMJIT_INLINE_NODEBUG uint32_t annotationId() const noexcept { return _annotationId; }
  //! Returns a vector of label identifiers that lists all targets of the jump.
  ASMJIT_INLINE_NODEBUG const ZoneVector<uint32_t>& labelIds() const noexcept { return _labelIds; }

  //! Tests whether the given `label` is a target of this JumpAnnotation.
  ASMJIT_INLINE_NODEBUG bool hasLabel(const Label& label) const noexcept { return hasLabelId(label.id()); }
  //! Tests whether the given `labelId` is a target of this JumpAnnotation.
  ASMJIT_INLINE_NODEBUG bool hasLabelId(uint32_t labelId) const noexcept { return _labelIds.contains(labelId); }

  //! \}

  //! \name Annotation Building API
  //! \{

  //! Adds the `label` to the list of targets of this JumpAnnotation.
  ASMJIT_INLINE_NODEBUG Error addLabel(const Label& label) noexcept { return addLabelId(label.id()); }
  //! Adds the `labelId` to the list of targets of this JumpAnnotation.
  ASMJIT_INLINE_NODEBUG Error addLabelId(uint32_t labelId) noexcept { return _labelIds.append(&_compiler->_allocator, labelId); }

  //! \}
};

//! Jump instruction with \ref JumpAnnotation.
//!
//! \note This node should be only used to represent jump where the jump target cannot be deduced by examining
//! instruction operands. For example if the jump target is register or memory location. This pattern is often
//! used to perform indirect jumps that use jump table, e.g. to implement `switch{}` statement.
class JumpNode : public InstNodeWithOperands<InstNode::kBaseOpCapacity> {
public:
  ASMJIT_NONCOPYABLE(JumpNode)

  //! \name Members
  //! \{

  JumpAnnotation* _annotation;

  //! \}

  //! \name Construction & Destruction
  //! \{

  inline JumpNode(BaseCompiler* ASMJIT_NONNULL(cc), InstId instId, InstOptions options, uint32_t opCount, JumpAnnotation* annotation) noexcept
    : InstNodeWithOperands(cc, instId, options, opCount),
      _annotation(annotation) {
    setType(NodeType::kJump);
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether this JumpNode has associated a \ref JumpAnnotation.
  ASMJIT_INLINE_NODEBUG bool hasAnnotation() const noexcept { return _annotation != nullptr; }
  //! Returns the \ref JumpAnnotation associated with this jump, or `nullptr`.
  ASMJIT_INLINE_NODEBUG JumpAnnotation* annotation() const noexcept { return _annotation; }
  //! Sets the \ref JumpAnnotation associated with this jump to `annotation`.
  ASMJIT_INLINE_NODEBUG void setAnnotation(JumpAnnotation* annotation) noexcept { _annotation = annotation; }

  //! \}
};

//! Function node represents a function used by \ref BaseCompiler.
//!
//! A function is composed of the following:
//!
//!   - Function entry, \ref FuncNode acts as a label, so the entry is implicit. To get the entry, simply use
//!     \ref FuncNode::label(), which is the same as \ref LabelNode::label().
//!
//!   - Function exit, which is represented by \ref FuncNode::exitNode(). A helper function
//!     \ref FuncNode::exitLabel() exists and returns an exit label instead of node.
//!
//!   - Function \ref FuncNode::endNode() sentinel. This node marks the end of a function - there should be no
//!     code that belongs to the function after this node, but the Compiler doesn't enforce that at the moment.
//!
//!   - Function detail, see \ref FuncNode::detail().
//!
//!   - Function frame, see \ref FuncNode::frame().
//!
//!   - Function arguments mapped to virtual registers, see \ref FuncNode::argPacks().
//!
//! In a node list, the function and its body looks like the following:
//!
//! \code{.unparsed}
//! [...]       - Anything before the function.
//!
//! [FuncNode]  - Entry point of the function, acts as a label as well.
//!   <Prolog>  - Prolog inserted by the register allocator.
//!   {...}     - Function body - user code basically.
//! [ExitLabel] - Exit label
//!   <Epilog>  - Epilog inserted by the register allocator.
//!   <Return>  - Return inserted by the register allocator.
//!   {...}     - Can contain data or user code (error handling, special cases, ...).
//! [FuncEnd]   - End sentinel
//!
//! [...]       - Anything after the function.
//! \endcode
//!
//! When a function is added to the instruction stream by \ref BaseCompiler::addFunc() it actually inserts 3 nodes
//! (FuncNode, ExitLabel, and FuncEnd) and sets the current cursor to be FuncNode. When \ref BaseCompiler::endFunc()
//! is called the cursor is set to FuncEnd. This guarantees that user can use ExitLabel as a marker after additional
//! code or data can be placed, which is a common practice.
class FuncNode : public LabelNode {
public:
  ASMJIT_NONCOPYABLE(FuncNode)

  //! Arguments pack.
  struct ArgPack {
    RegOnly _data[Globals::kMaxValuePack];

    inline void reset() noexcept {
      for (size_t valueIndex = 0; valueIndex < Globals::kMaxValuePack; valueIndex++)
        _data[valueIndex].reset();
    }

    inline RegOnly& operator[](size_t valueIndex) noexcept { return _data[valueIndex]; }
    inline const RegOnly& operator[](size_t valueIndex) const noexcept { return _data[valueIndex]; }
  };

  //! \name Members
  //! \{

  //! Function detail.
  FuncDetail _funcDetail;
  //! Function frame.
  FuncFrame _frame;
  //! Function exit label.
  LabelNode* _exitNode;
  //! Function end (sentinel).
  SentinelNode* _end;
  //! Argument packs.
  ArgPack* _args;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `FuncNode` instance.
  //!
  //! Always use `BaseCompiler::addFunc()` to create a new `FuncNode`.
  inline FuncNode(BaseBuilder* ASMJIT_NONNULL(cb)) noexcept
    : LabelNode(cb),
      _funcDetail(),
      _frame(),
      _exitNode(nullptr),
      _end(nullptr),
      _args(nullptr) {
    setType(NodeType::kFunc);
  }

  //! \}

  //! \{
  //! \name Accessors

  //! Returns function exit `LabelNode`.
  ASMJIT_INLINE_NODEBUG LabelNode* exitNode() const noexcept { return _exitNode; }
  //! Returns function exit label.
  ASMJIT_INLINE_NODEBUG Label exitLabel() const noexcept { return _exitNode->label(); }

  //! Returns "End of Func" sentinel node.
  ASMJIT_INLINE_NODEBUG SentinelNode* endNode() const noexcept { return _end; }

  //! Returns function detail.
  ASMJIT_INLINE_NODEBUG FuncDetail& detail() noexcept { return _funcDetail; }
  //! Returns function detail.
  ASMJIT_INLINE_NODEBUG const FuncDetail& detail() const noexcept { return _funcDetail; }

  //! Returns function frame.
  ASMJIT_INLINE_NODEBUG FuncFrame& frame() noexcept { return _frame; }
  //! Returns function frame.
  ASMJIT_INLINE_NODEBUG const FuncFrame& frame() const noexcept { return _frame; }

  //! Returns function attributes.
  ASMJIT_INLINE_NODEBUG FuncAttributes attributes() const noexcept { return _frame.attributes(); }
  //! Adds `attrs` to the function attributes.
  ASMJIT_INLINE_NODEBUG void addAttributes(FuncAttributes attrs) noexcept { _frame.addAttributes(attrs); }

  //! Returns arguments count.
  ASMJIT_INLINE_NODEBUG uint32_t argCount() const noexcept { return _funcDetail.argCount(); }
  //! Returns argument packs.
  ASMJIT_INLINE_NODEBUG ArgPack* argPacks() const noexcept { return _args; }

  //! Tests whether the function has a return value.
  ASMJIT_INLINE_NODEBUG bool hasRet() const noexcept { return _funcDetail.hasRet(); }

  //! Returns argument pack at `argIndex`.
  inline ArgPack& argPack(size_t argIndex) const noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    return _args[argIndex];
  }

  //! Sets argument at `argIndex`.
  inline void setArg(size_t argIndex, const BaseReg& vReg) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    _args[argIndex][0].init(vReg);
  }

  //! \overload
  inline void setArg(size_t argIndex, const RegOnly& vReg) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    _args[argIndex][0].init(vReg);
  }

  //! Sets argument at `argIndex` and `valueIndex`.
  inline void setArg(size_t argIndex, size_t valueIndex, const BaseReg& vReg) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    _args[argIndex][valueIndex].init(vReg);
  }

  //! \overload
  inline void setArg(size_t argIndex, size_t valueIndex, const RegOnly& vReg) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    _args[argIndex][valueIndex].init(vReg);
  }

  //! Resets argument pack at `argIndex`.
  inline void resetArg(size_t argIndex) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    _args[argIndex].reset();
  }

  //! Resets argument pack at `argIndex`.
  inline void resetArg(size_t argIndex, size_t valueIndex) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    _args[argIndex][valueIndex].reset();
  }

  //! \}
};

//! Function return, used by \ref BaseCompiler.
class FuncRetNode : public InstNodeWithOperands<InstNode::kBaseOpCapacity> {
public:
  ASMJIT_NONCOPYABLE(FuncRetNode)

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `FuncRetNode` instance.
  inline FuncRetNode(BaseBuilder* ASMJIT_NONNULL(cb)) noexcept
    : InstNodeWithOperands(cb, BaseInst::kIdAbstract, InstOptions::kNone, 0) {
    _any._nodeType = NodeType::kFuncRet;
  }

  //! \}
};

//! Function invocation, used by \ref BaseCompiler.
class InvokeNode : public InstNodeWithOperands<InstNode::kBaseOpCapacity> {
public:
  ASMJIT_NONCOPYABLE(InvokeNode)

  //! Operand pack provides multiple operands that can be associated with a single return value of function
  //! argument. Sometimes this is necessary to express an argument or return value that requires multiple
  //! registers, for example 64-bit value in 32-bit mode or passing / returning homogeneous data structures.
  struct OperandPack {
    //! Operands.
    Operand_ _data[Globals::kMaxValuePack];

    //! Reset the pack by resetting all operands in the pack.
    inline void reset() noexcept {
      for (size_t valueIndex = 0; valueIndex < Globals::kMaxValuePack; valueIndex++)
        _data[valueIndex].reset();
    }

    //! Returns an operand at the given `valueIndex`.
    inline Operand& operator[](size_t valueIndex) noexcept {
      ASMJIT_ASSERT(valueIndex < Globals::kMaxValuePack);
      return _data[valueIndex].as<Operand>();
    }

    //! Returns an operand at the given `valueIndex` (const).
    const inline Operand& operator[](size_t valueIndex) const noexcept {
      ASMJIT_ASSERT(valueIndex < Globals::kMaxValuePack);
      return _data[valueIndex].as<Operand>();
    }
  };

  //! \name Members
  //! \{

  //! Function detail.
  FuncDetail _funcDetail;
  //! Function return value(s).
  OperandPack _rets;
  //! Function arguments.
  OperandPack* _args;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `InvokeNode` instance.
  inline InvokeNode(BaseBuilder* ASMJIT_NONNULL(cb), InstId instId, InstOptions options) noexcept
    : InstNodeWithOperands(cb, instId, options, 0),
      _funcDetail(),
      _args(nullptr) {
    setType(NodeType::kInvoke);
    _resetOps();
    _rets.reset();
    addFlags(NodeFlags::kIsRemovable);
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Sets the function signature.
  inline Error init(const FuncSignature& signature, const Environment& environment) noexcept {
    return _funcDetail.init(signature, environment);
  }

  //! Returns the function detail.
  ASMJIT_INLINE_NODEBUG FuncDetail& detail() noexcept { return _funcDetail; }
  //! Returns the function detail.
  ASMJIT_INLINE_NODEBUG const FuncDetail& detail() const noexcept { return _funcDetail; }

  //! Returns the target operand.
  ASMJIT_INLINE_NODEBUG Operand& target() noexcept { return op(0); }
  //! \overload
  ASMJIT_INLINE_NODEBUG const Operand& target() const noexcept { return op(0); }

  //! Returns the number of function return values.
  ASMJIT_INLINE_NODEBUG bool hasRet() const noexcept { return _funcDetail.hasRet(); }
  //! Returns the number of function arguments.
  ASMJIT_INLINE_NODEBUG uint32_t argCount() const noexcept { return _funcDetail.argCount(); }

  //! Returns operand pack representing function return value(s).
  ASMJIT_INLINE_NODEBUG OperandPack& retPack() noexcept { return _rets; }
  //! Returns operand pack representing function return value(s).
  ASMJIT_INLINE_NODEBUG const OperandPack& retPack() const noexcept { return _rets; }

  //! Returns the return value at the given `valueIndex`.
  ASMJIT_INLINE_NODEBUG Operand& ret(size_t valueIndex = 0) noexcept { return _rets[valueIndex]; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const Operand& ret(size_t valueIndex = 0) const noexcept { return _rets[valueIndex]; }

  //! Returns operand pack representing function return value(s).
  inline OperandPack& argPack(size_t argIndex) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    return _args[argIndex];
  }
  //! \overload
  inline const OperandPack& argPack(size_t argIndex) const noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    return _args[argIndex];
  }

  //! Returns a function argument at the given `argIndex`.
  inline Operand& arg(size_t argIndex, size_t valueIndex) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    return _args[argIndex][valueIndex];
  }
  //! \overload
  inline const Operand& arg(size_t argIndex, size_t valueIndex) const noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    return _args[argIndex][valueIndex];
  }

  //! Sets the function return value at `i` to `op`.
  inline void _setRet(size_t valueIndex, const Operand_& op) noexcept { _rets[valueIndex] = op; }
  //! Sets the function argument at `i` to `op`.
  inline void _setArg(size_t argIndex, size_t valueIndex, const Operand_& op) noexcept {
    ASMJIT_ASSERT(argIndex < argCount());
    _args[argIndex][valueIndex] = op;
  }

  //! Sets the function return value at `valueIndex` to `reg`.
  ASMJIT_INLINE_NODEBUG void setRet(size_t valueIndex, const BaseReg& reg) noexcept { _setRet(valueIndex, reg); }

  //! Sets the first function argument in a value-pack at `argIndex` to `reg`.
  ASMJIT_INLINE_NODEBUG void setArg(size_t argIndex, const BaseReg& reg) noexcept { _setArg(argIndex, 0, reg); }
  //! Sets the first function argument in a value-pack at `argIndex` to `imm`.
  ASMJIT_INLINE_NODEBUG void setArg(size_t argIndex, const Imm& imm) noexcept { _setArg(argIndex, 0, imm); }

  //! Sets the function argument at `argIndex` and `valueIndex` to `reg`.
  ASMJIT_INLINE_NODEBUG void setArg(size_t argIndex, size_t valueIndex, const BaseReg& reg) noexcept { _setArg(argIndex, valueIndex, reg); }
  //! Sets the function argument at `argIndex` and `valueIndex` to `imm`.
  ASMJIT_INLINE_NODEBUG void setArg(size_t argIndex, size_t valueIndex, const Imm& imm) noexcept { _setArg(argIndex, valueIndex, imm); }

  //! \}
};

//! Function pass extends \ref Pass with \ref FuncPass::runOnFunction().
class ASMJIT_VIRTAPI FuncPass : public Pass {
public:
  ASMJIT_NONCOPYABLE(FuncPass)
  typedef Pass Base;

  //! \name Construction & Destruction
  //! \{

  ASMJIT_API FuncPass(const char* name) noexcept;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the associated `BaseCompiler`.
  ASMJIT_INLINE_NODEBUG BaseCompiler* cc() const noexcept { return static_cast<BaseCompiler*>(_cb); }

  //! \}

  //! \name Pass Interface
  //! \{

  //! Calls `runOnFunction()` on each `FuncNode` node found.
  ASMJIT_API Error run(Zone* zone, Logger* logger) override;

  //! Called once per `FuncNode`.
  ASMJIT_API virtual Error runOnFunction(Zone* zone, Logger* logger, FuncNode* func);

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER
#endif // ASMJIT_CORE_COMPILER_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/compilerdefs.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_COMPILERDEFS_H_INCLUDED
#define ASMJIT_CORE_COMPILERDEFS_H_INCLUDED

#include "../core/api-config.h"
#include "../core/operand.h"
#include "../core/type.h"
#include "../core/zonestring.h"

ASMJIT_BEGIN_NAMESPACE

class RAWorkReg;

//! \addtogroup asmjit_compiler
//! \{

//! Public virtual register interface, managed by \ref BaseCompiler.
//!
//! When a virtual register is created by \ref BaseCompiler a `VirtReg` is linked with the register operand id it
//! returns. This `VirtReg` can be accessed via \ref BaseCompiler::virtRegByReg() function, which returns a pointer
//! to `VirtReg`.
//!
//! In general, `VirtReg` should be only introspected as it contains important variables that are needed and managed
//! by AsmJit, however, the `VirtReg` API can also be used to influence register allocation. For example there is
//! a \ref VirtReg::setWeight() function, which could be used to increase a weight of a virtual register (thus make
//! it hard to spill, for example). In addition, there is a \ref VirtReg::setHomeIdHint() function, which can be used
//! to do an initial assignment of a physical register of a virtual register. However, AsmJit could still override
//! the physical register assigned in some special cases.
class VirtReg {
public:
  ASMJIT_NONCOPYABLE(VirtReg)

  //! \name Members
  //! \{

  //! Virtual register signature.
  OperandSignature _signature {};
  //! Virtual register id.
  uint32_t _id = 0;
  //! Virtual register size (can be smaller than `_signature._size`).
  uint32_t _virtSize = 0;
  //! Virtual register alignment (for spilling).
  uint8_t _alignment = 0;
  //! Type-id.
  TypeId _typeId = TypeId::kVoid;
  //! Virtual register weight for alloc/spill decisions.
  uint8_t _weight = 1;
  //! True if this is a fixed register, never reallocated.
  uint8_t _isFixed : 1;
  //! True if the virtual register is only used as a stack (never accessed as register).
  uint8_t _isStack : 1;
  //! True if this virtual register has assigned stack offset (can be only valid after register allocation pass).
  uint8_t _hasStackSlot : 1;
  uint8_t _reservedBits : 5;
  //! Home register hint for the register allocator (initially unassigned).
  uint8_t _homeIdHint = BaseReg::kIdBad;

  //! Stack offset assigned by the register allocator relative to stack pointer (can be negative as well).
  int32_t _stackOffset = 0;

  //! Reserved for future use (padding).
  uint32_t _reservedU32 = 0;

  //! Virtual register name (user provided or automatically generated).
  ZoneString<16> _name {};

  // The following members are used exclusively by RAPass. They are initialized when the VirtReg is created to
  // null pointers and then changed during RAPass execution. RAPass sets them back to NULL before it returns.

  //! Reference to `RAWorkReg`, used during register allocation.
  RAWorkReg* _workReg = nullptr;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG VirtReg(OperandSignature signature, uint32_t id, uint32_t virtSize, uint32_t alignment, TypeId typeId) noexcept
    : _signature(signature),
      _id(id),
      _virtSize(virtSize),
      _alignment(uint8_t(alignment)),
      _typeId(typeId),
      _isFixed(0),
      _isStack(0),
      _hasStackSlot(0),
      _reservedBits(0) {}

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the virtual register id.
  ASMJIT_INLINE_NODEBUG uint32_t id() const noexcept { return _id; }

  //! Returns the virtual register name.
  ASMJIT_INLINE_NODEBUG const char* name() const noexcept { return _name.data(); }
  //! Returns the size of the virtual register name.
  ASMJIT_INLINE_NODEBUG uint32_t nameSize() const noexcept { return _name.size(); }

  //! Returns a register signature of this virtual register.
  ASMJIT_INLINE_NODEBUG OperandSignature signature() const noexcept { return _signature; }
  //! Returns a virtual register type (maps to the physical register type as well).
  ASMJIT_INLINE_NODEBUG RegType type() const noexcept { return _signature.regType(); }
  //! Returns a virtual register group (maps to the physical register group as well).
  ASMJIT_INLINE_NODEBUG RegGroup group() const noexcept { return _signature.regGroup(); }

  //! Returns a real size of the register this virtual register maps to.
  //!
  //! For example if this is a 128-bit SIMD register used for a scalar single precision floating point value then
  //! its virtSize would be 4, however, the `regSize` would still say 16 (128-bits), because it's the smallest size
  //! of that register type.
  ASMJIT_INLINE_NODEBUG uint32_t regSize() const noexcept { return _signature.size(); }

  //! Returns the virtual register size.
  //!
  //! The virtual register size describes how many bytes the virtual register needs to store its content. It can be
  //! smaller than the physical register size, see `regSize()`.
  ASMJIT_INLINE_NODEBUG uint32_t virtSize() const noexcept { return _virtSize; }

  //! Returns the virtual register alignment.
  ASMJIT_INLINE_NODEBUG uint32_t alignment() const noexcept { return _alignment; }

  //! Returns the virtual register type id.
  ASMJIT_INLINE_NODEBUG TypeId typeId() const noexcept { return _typeId; }

  //! Returns the virtual register weight - the register allocator can use it as explicit hint for alloc/spill
  //! decisions.
  ASMJIT_INLINE_NODEBUG uint32_t weight() const noexcept { return _weight; }
  //! Sets the virtual register weight (0 to 255) - the register allocator can use it as explicit hint for
  //! alloc/spill decisions and initial bin-packing.
  ASMJIT_INLINE_NODEBUG void setWeight(uint32_t weight) noexcept { _weight = uint8_t(weight); }

  //! Returns whether the virtual register is always allocated to a fixed physical register (and never reallocated).
  //!
  //! \note This is only used for special purposes and it's mostly internal.
  ASMJIT_INLINE_NODEBUG bool isFixed() const noexcept { return bool(_isFixed); }

  //! Tests whether the virtual register is in fact a stack that only uses the virtual register id.
  //!
  //! \note It's an error if a stack is accessed as a register.
  ASMJIT_INLINE_NODEBUG bool isStack() const noexcept { return bool(_isStack); }

  //! Tests whether this virtual register (or stack) has assigned a stack offset.
  //!
  //! If this is a virtual register that was never allocated on stack, it would return false, otherwise if
  //! it's a virtual register that was spilled or explicitly allocated stack, the return value would be true.
  ASMJIT_INLINE_NODEBUG bool hasStackSlot() const noexcept { return bool(_hasStackSlot); }

  //! Assigns a stack offset of this virtual register to `stackOffset` and sets `_hasStackSlot` to true.
  ASMJIT_INLINE_NODEBUG void assignStackSlot(int32_t stackOffset) noexcept {
    _hasStackSlot = 1;
    _stackOffset = stackOffset;
  }

  //! Tests whether this virtual register has assigned a physical register as a hint to the register allocator.
  ASMJIT_INLINE_NODEBUG bool hasHomeIdHint() const noexcept { return _homeIdHint != BaseReg::kIdBad; }
  //! Returns a physical register hint, which will be used by the register allocator.
  ASMJIT_INLINE_NODEBUG uint32_t homeIdHint() const noexcept { return _homeIdHint; }
  //! Assigns a physical register hint, which will be used by the register allocator.
  ASMJIT_INLINE_NODEBUG void setHomeIdHint(uint32_t homeId) noexcept { _homeIdHint = uint8_t(homeId); }
  //! Resets a physical register hint.
  ASMJIT_INLINE_NODEBUG void resetHomeIdHint() noexcept { _homeIdHint = BaseReg::kIdBad; }

  //! Returns a stack offset associated with a virtual register or explicit stack allocation.
  //!
  //! \note Always verify that the stack offset has been assigned by calling \ref hasStackSlot(). The return
  //! value will be zero when the stack offset was not assigned.
  ASMJIT_INLINE_NODEBUG int32_t stackOffset() const noexcept { return _stackOffset; }

  //! Tests whether the virtual register has an associated `RAWorkReg` at the moment.
  ASMJIT_INLINE_NODEBUG bool hasWorkReg() const noexcept { return _workReg != nullptr; }
  //! Returns an associated RAWorkReg with this virtual register (only valid during register allocation).
  ASMJIT_INLINE_NODEBUG RAWorkReg* workReg() const noexcept { return _workReg; }
  //! Associates a RAWorkReg with this virtual register (used by register allocator).
  ASMJIT_INLINE_NODEBUG void setWorkReg(RAWorkReg* workReg) noexcept { _workReg = workReg; }
  //! Reset the RAWorkReg association (used by register allocator).
  ASMJIT_INLINE_NODEBUG void resetWorkReg() noexcept { _workReg = nullptr; }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_COMPILERDEFS_H_INCLUDED


```

`CodeCleaner/asmjit/asmjit/core/constpool.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/constpool.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// ConstPool - Construction & Destruction
// ======================================

ConstPool::ConstPool(Zone* zone) noexcept { reset(zone); }
ConstPool::~ConstPool() noexcept {}

// ConstPool - Reset
// =================

void ConstPool::reset(Zone* zone) noexcept {
  _zone = zone;

  size_t dataSize = 1;
  for (size_t i = 0; i < ASMJIT_ARRAY_SIZE(_tree); i++) {
    _tree[i].reset();
    _tree[i].setDataSize(dataSize);
    _gaps[i] = nullptr;
    dataSize <<= 1;
  }

  _gapPool = nullptr;
  _size = 0;
  _alignment = 0;
  _minItemSize = 0;
}

// ConstPool - Operations
// ======================

static inline ConstPool::Gap* ConstPool_allocGap(ConstPool* self) noexcept {
  ConstPool::Gap* gap = self->_gapPool;
  if (!gap)
    return self->_zone->allocT<ConstPool::Gap>();

  self->_gapPool = gap->_next;
  return gap;
}

static inline void ConstPool_freeGap(ConstPool* self, ConstPool::Gap* gap) noexcept {
  gap->_next = self->_gapPool;
  self->_gapPool = gap;
}

static void ConstPool_addGap(ConstPool* self, size_t offset, size_t size) noexcept {
  ASMJIT_ASSERT(size > 0);

  while (size > 0) {
    size_t gapIndex;
    size_t gapSize;

    if (size >= 32 && Support::isAligned<size_t>(offset, 32)) {
      gapIndex = ConstPool::kIndex32;
      gapSize = 32;
    }
    else if (size >= 16 && Support::isAligned<size_t>(offset, 16)) {
      gapIndex = ConstPool::kIndex16;
      gapSize = 16;
    }
    else if (size >= 8 && Support::isAligned<size_t>(offset, 8)) {
      gapIndex = ConstPool::kIndex8;
      gapSize = 8;
    }
    else if (size >= 4 && Support::isAligned<size_t>(offset, 4)) {
      gapIndex = ConstPool::kIndex4;
      gapSize = 4;
    }
    else if (size >= 2 && Support::isAligned<size_t>(offset, 2)) {
      gapIndex = ConstPool::kIndex2;
      gapSize = 2;
    }
    else {
      gapIndex = ConstPool::kIndex1;
      gapSize = 1;
    }

    // We don't have to check for errors here, if this failed nothing really happened (just the gap won't be
    // visible) and it will fail again at place where the same check would generate `kErrorOutOfMemory` error.
    ConstPool::Gap* gap = ConstPool_allocGap(self);
    if (!gap)
      return;

    gap->_next = self->_gaps[gapIndex];
    self->_gaps[gapIndex] = gap;

    gap->_offset = offset;
    gap->_size = gapSize;

    offset += gapSize;
    size -= gapSize;
  }
}

Error ConstPool::add(const void* data, size_t size, size_t& dstOffset) noexcept {
  size_t treeIndex;

  if (size == 64)
    treeIndex = kIndex64;
  else if (size == 32)
    treeIndex = kIndex32;
  else if (size == 16)
    treeIndex = kIndex16;
  else if (size == 8)
    treeIndex = kIndex8;
  else if (size == 4)
    treeIndex = kIndex4;
  else if (size == 2)
    treeIndex = kIndex2;
  else if (size == 1)
    treeIndex = kIndex1;
  else
    return DebugUtils::errored(kErrorInvalidArgument);

  ConstPool::Node* node = _tree[treeIndex].get(data);
  if (node) {
    dstOffset = node->_offset;
    return kErrorOk;
  }

  // Before incrementing the current offset try if there is a gap that can be used for the requested data.
  size_t offset = ~size_t(0);
  size_t gapIndex = treeIndex;

  while (gapIndex != kIndexCount - 1) {
    ConstPool::Gap* gap = _gaps[treeIndex];

    // Check if there is a gap.
    if (gap) {
      size_t gapOffset = gap->_offset;
      size_t gapSize = gap->_size;

      // Destroy the gap for now.
      _gaps[treeIndex] = gap->_next;
      ConstPool_freeGap(this, gap);

      offset = gapOffset;
      ASMJIT_ASSERT(Support::isAligned<size_t>(offset, size));

      gapSize -= size;
      if (gapSize > 0)
        ConstPool_addGap(this, gapOffset, gapSize);
    }

    gapIndex++;
  }

  if (offset == ~size_t(0)) {
    // Get how many bytes have to be skipped so the address is aligned accordingly to the 'size'.
    size_t diff = Support::alignUpDiff<size_t>(_size, size);

    if (diff != 0) {
      ConstPool_addGap(this, _size, diff);
      _size += diff;
    }

    offset = _size;
    _size += size;
  }

  // Add the initial node to the right index.
  node = ConstPool::Tree::_newNode(_zone, data, size, offset, false);
  if (ASMJIT_UNLIKELY(!node))
    return DebugUtils::errored(kErrorOutOfMemory);

  _tree[treeIndex].insert(node);
  _alignment = Support::max<size_t>(_alignment, size);

  dstOffset = offset;

  // Now create a bunch of shared constants that are based on the data pattern. We stop at size 4,
  // it probably doesn't make sense to split constants down to 1 byte.
  size_t pCount = 1;
  size_t smallerSize = size;

  while (smallerSize > 4) {
    pCount <<= 1;
    smallerSize >>= 1;

    ASMJIT_ASSERT(treeIndex != 0);
    treeIndex--;

    const uint8_t* pData = static_cast<const uint8_t*>(data);
    for (size_t i = 0; i < pCount; i++, pData += smallerSize) {
      node = _tree[treeIndex].get(pData);
      if (node) continue;

      node = ConstPool::Tree::_newNode(_zone, pData, smallerSize, offset + (i * smallerSize), true);
      _tree[treeIndex].insert(node);
    }
  }

  if (_minItemSize == 0)
    _minItemSize = size;
  else
    _minItemSize = Support::min(_minItemSize, size);

  return kErrorOk;
}

// ConstPool - Reset
// =================

struct ConstPoolFill {
  inline ConstPoolFill(uint8_t* dst, size_t dataSize) noexcept :
    _dst(dst),
    _dataSize(dataSize) {}

  inline void operator()(const ConstPool::Node* node) noexcept {
    if (!node->_shared)
      memcpy(_dst + node->_offset, node->data(), _dataSize);
  }

  uint8_t* _dst;
  size_t _dataSize;
};

void ConstPool::fill(void* dst) const noexcept {
  // Clears possible gaps, asmjit should never emit garbage to the output.
  memset(dst, 0, _size);

  ConstPoolFill filler(static_cast<uint8_t*>(dst), 1);
  for (size_t i = 0; i < ASMJIT_ARRAY_SIZE(_tree); i++) {
    _tree[i].forEach(filler);
    filler._dataSize <<= 1;
  }
}

// ConstPool - Tests
// =================

#if defined(ASMJIT_TEST)
UNIT(const_pool) {
  Zone zone(32u * 1024u);
  ConstPool pool(&zone);

  uint32_t i;
  uint32_t kCount = BrokenAPI::hasArg("--quick") ? 1000 : 1000000;

  INFO("Adding %u constants to the pool", kCount);
  {
    size_t prevOffset;
    size_t curOffset;
    uint64_t c = 0x0101010101010101u;

    EXPECT_EQ(pool.add(&c, 8, prevOffset), kErrorOk);
    EXPECT_EQ(prevOffset, 0u);

    for (i = 1; i < kCount; i++) {
      c++;
      EXPECT_EQ(pool.add(&c, 8, curOffset), kErrorOk);
      EXPECT_EQ(prevOffset + 8, curOffset);
      EXPECT_EQ(pool.size(), (i + 1) * 8);
      prevOffset = curOffset;
    }

    EXPECT_EQ(pool.alignment(), 8u);
  }

  INFO("Retrieving %u constants from the pool", kCount);
  {
    uint64_t c = 0x0101010101010101u;

    for (i = 0; i < kCount; i++) {
      size_t offset;
      EXPECT_EQ(pool.add(&c, 8, offset), kErrorOk);
      EXPECT_EQ(offset, i * 8);
      c++;
    }
  }

  INFO("Checking if the constants were split into 4-byte patterns");
  {
    uint32_t c = 0x01010101u;
    size_t offset;

    EXPECT_EQ(pool.add(&c, 4, offset), kErrorOk);
    EXPECT_EQ(offset, 0u);

    // NOTE: We have to adjust the offset to successfully test this on big endian architectures.
    size_t baseOffset = size_t(ASMJIT_ARCH_BE ? 4 : 0);

    for (i = 1; i < kCount; i++) {
      c++;
      EXPECT_EQ(pool.add(&c, 4, offset), kErrorOk);
      EXPECT_EQ(offset, baseOffset + i * 8);
    }
  }

  INFO("Adding 2 byte constant to misalign the current offset");
  {
    uint16_t c = 0xFFFF;
    size_t offset;

    EXPECT_EQ(pool.add(&c, 2, offset), kErrorOk);
    EXPECT_EQ(offset, kCount * 8);
    EXPECT_EQ(pool.alignment(), 8u);
  }

  INFO("Adding 8 byte constant to check if pool gets aligned again");
  {
    uint64_t c = 0xFFFFFFFFFFFFFFFFu;
    size_t offset;

    EXPECT_EQ(pool.add(&c, 8, offset), kErrorOk);
    EXPECT_EQ(offset, kCount * 8 + 8u);
  }

  INFO("Adding 2 byte constant to verify the gap is filled");
  {
    uint16_t c = 0xFFFE;
    size_t offset;

    EXPECT_EQ(pool.add(&c, 2, offset), kErrorOk);
    EXPECT_EQ(offset, kCount * 8 + 2);
    EXPECT_EQ(pool.alignment(), 8u);
  }

  INFO("Checking reset functionality");
  {
    pool.reset(&zone);
    zone.reset();

    EXPECT_EQ(pool.size(), 0u);
    EXPECT_EQ(pool.alignment(), 0u);
  }

  INFO("Checking pool alignment when combined constants are added");
  {
    uint8_t bytes[32] = { 0 };
    size_t offset;

    pool.add(bytes, 1, offset);
    EXPECT_EQ(pool.size(), 1u);
    EXPECT_EQ(pool.alignment(), 1u);
    EXPECT_EQ(offset, 0u);

    pool.add(bytes, 2, offset);
    EXPECT_EQ(pool.size(), 4u);
    EXPECT_EQ(pool.alignment(), 2u);
    EXPECT_EQ(offset, 2u);

    pool.add(bytes, 4, offset);
    EXPECT_EQ(pool.size(), 8u);
    EXPECT_EQ(pool.alignment(), 4u);
    EXPECT_EQ(offset, 4u);

    pool.add(bytes, 4, offset);
    EXPECT_EQ(pool.size(), 8u);
    EXPECT_EQ(pool.alignment(), 4u);
    EXPECT_EQ(offset, 4u);

    pool.add(bytes, 32, offset);
    EXPECT_EQ(pool.size(), 64u);
    EXPECT_EQ(pool.alignment(), 32u);
    EXPECT_EQ(offset, 32u);
  }
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/constpool.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_CONSTPOOL_H_INCLUDED
#define ASMJIT_CORE_CONSTPOOL_H_INCLUDED

#include "../core/support.h"
#include "../core/zone.h"
#include "../core/zonetree.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_utilities
//! \{

//! Constant pool scope.
enum class ConstPoolScope : uint32_t {
  //! Local constant, always embedded right after the current function.
  kLocal = 0,
  //! Global constant, embedded at the end of the currently compiled code.
  kGlobal = 1,

  //! Maximum value of `ConstPoolScope`.
  kMaxValue = kGlobal
};

//! Constant pool.
//!
//! Constant pool is designed to hold 1, 2, 4, 8, 16, 32, and 64 byte constants. It's not designed to hold constants
//! having arbitrary length like strings and arrays.
class ConstPool {
public:
  ASMJIT_NONCOPYABLE(ConstPool)

  //! \cond INTERNAL

  //! Index of a given size in const-pool table.
  enum Index : uint32_t {
    kIndex1 = 0,
    kIndex2 = 1,
    kIndex4 = 2,
    kIndex8 = 3,
    kIndex16 = 4,
    kIndex32 = 5,
    kIndex64 = 6,
    kIndexCount = 7
  };

  //! Zone-allocated const-pool gap created by two differently aligned constants.
  struct Gap {
    //! Pointer to the next gap
    Gap* _next;
    //! Offset of the gap.
    size_t _offset;
    //! Remaining bytes of the gap (basically a gap size).
    size_t _size;
  };

  //! Zone-allocated const-pool node.
  class Node : public ZoneTreeNodeT<Node> {
  public:
    ASMJIT_NONCOPYABLE(Node)

    //! If this constant is shared with another.
    uint32_t _shared : 1;
    //! Data offset from the beginning of the pool.
    uint32_t _offset;

    ASMJIT_INLINE_NODEBUG Node(size_t offset, bool shared) noexcept
      : ZoneTreeNodeT<Node>(),
        _shared(shared),
        _offset(uint32_t(offset)) {}

    ASMJIT_INLINE_NODEBUG void* data() const noexcept {
      return static_cast<void*>(const_cast<ConstPool::Node*>(this) + 1);
    }
  };

  //! Data comparer used internally.
  class Compare {
  public:
    size_t _dataSize;

    ASMJIT_INLINE_NODEBUG Compare(size_t dataSize) noexcept
      : _dataSize(dataSize) {}

    ASMJIT_INLINE_NODEBUG int operator()(const Node& a, const Node& b) const noexcept {
      return ::memcmp(a.data(), b.data(), _dataSize);
    }

    ASMJIT_INLINE_NODEBUG int operator()(const Node& a, const void* data) const noexcept {
      return ::memcmp(a.data(), data, _dataSize);
    }
  };

  //! Zone-allocated const-pool tree.
  struct Tree {
    //! RB tree.
    ZoneTree<Node> _tree;
    //! Size of the tree (number of nodes).
    size_t _size;
    //! Size of the data.
    size_t _dataSize;

    ASMJIT_INLINE_NODEBUG explicit Tree(size_t dataSize = 0) noexcept
      : _tree(),
        _size(0),
        _dataSize(dataSize) {}

    ASMJIT_INLINE_NODEBUG void reset() noexcept {
      _tree.reset();
      _size = 0;
    }

    ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _size == 0; }
    ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return _size; }

    inline void setDataSize(size_t dataSize) noexcept {
      ASMJIT_ASSERT(empty());
      _dataSize = dataSize;
    }

    ASMJIT_INLINE_NODEBUG Node* get(const void* data) noexcept {
      Compare cmp(_dataSize);
      return _tree.get(data, cmp);
    }

    ASMJIT_INLINE_NODEBUG void insert(Node* node) noexcept {
      Compare cmp(_dataSize);
      _tree.insert(node, cmp);
      _size++;
    }

    template<typename Visitor>
    inline void forEach(Visitor& visitor) const noexcept {
      Node* node = _tree.root();
      if (!node) return;

      Node* stack[Globals::kMaxTreeHeight];
      size_t top = 0;

      for (;;) {
        Node* left = node->left();
        if (left != nullptr) {
          ASMJIT_ASSERT(top != Globals::kMaxTreeHeight);
          stack[top++] = node;

          node = left;
          continue;
        }

        for (;;) {
          visitor(node);
          node = node->right();

          if (node != nullptr)
            break;

          if (top == 0)
            return;

          node = stack[--top];
        }
      }
    }

    static inline Node* _newNode(Zone* zone, const void* data, size_t size, size_t offset, bool shared) noexcept {
      Node* node = zone->allocT<Node>(Support::alignUp(sizeof(Node) + size, alignof(Node)));
      if (ASMJIT_UNLIKELY(!node)) return nullptr;

      node = new(Support::PlacementNew{node}) Node(offset, shared);
      memcpy(node->data(), data, size);
      return node;
    }
  };

  //! \endcond

  //! \name Members
  //! \{

  //! Zone allocator.
  Zone* _zone;
  //! Tree per size.
  Tree _tree[kIndexCount];
  //! Gaps per size.
  Gap* _gaps[kIndexCount];
  //! Gaps pool
  Gap* _gapPool;

  //! Size of the pool (in bytes).
  size_t _size;
  //! Required pool alignment.
  size_t _alignment;
  //! Minimum item size in the pool.
  size_t _minItemSize;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new constant pool that would use `zone` as a memory allocator.
  ASMJIT_API explicit ConstPool(Zone* zone) noexcept;
  //! Destroys this constant pool.
  ASMJIT_API ~ConstPool() noexcept;

  //! \}

  //! \name Reset
  //! \{

  //! Resets this constant pool and its allocator to `zone`.
  ASMJIT_API void reset(Zone* zone) noexcept;

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the constant-pool is empty.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _size == 0; }
  //! Returns the size of the constant-pool in bytes.
  ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return _size; }
  //! Returns minimum alignment.
  ASMJIT_INLINE_NODEBUG size_t alignment() const noexcept { return _alignment; }
  //! Returns the minimum size of all items added to the constant pool.
  ASMJIT_INLINE_NODEBUG size_t minItemSize() const noexcept { return _minItemSize; }

  //! \}

  //! \name Utilities
  //! \{

  //! Adds a constant to the constant pool.
  //!
  //! The constant must have known size, which is 1, 2, 4, 8, 16 or 32 bytes. The constant is added to the pool only
  //! if it doesn't not exist, otherwise cached value is returned.
  //!
  //! AsmJit is able to subdivide added constants, so for example if you add 8-byte constant 0x1122334455667788 it
  //! will create the following slots:
  //!
  //!   8-byte: 0x1122334455667788
  //!   4-byte: 0x11223344, 0x55667788
  //!
  //! The reason is that when combining MMX/SSE/AVX code some patterns are used frequently. However, AsmJit is not
  //! able to reallocate a constant that has been already added. For example if you try to add 4-byte constant and
  //! then 8-byte constant having the same 4-byte pattern as the previous one, two independent slots will be used.
  ASMJIT_API Error add(const void* data, size_t size, size_t& dstOffset) noexcept;

  //! Fills the destination with the content of this constant pool.
  ASMJIT_API void fill(void* dst) const noexcept;
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_CONSTPOOL_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/cpuinfo.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/cpuinfo.h"
#include "../core/support.h"

#include <atomic>

// Required by `__cpuidex()` and `_xgetbv()`.
#if ASMJIT_ARCH_X86
  #if defined(_MSC_VER)
    #include <intrin.h>
  #endif
#endif // ASMJIT_ARCH_X86

#if ASMJIT_ARCH_ARM
  // Required by various utilities that are required by features detection.
  #if !defined(_WIN32)
    #include <errno.h>
    #include <sys/utsname.h>
  #endif

  //! Required to detect CPU and features on Apple platforms.
  #if defined(__APPLE__)
    #include <mach/machine.h>
    #include <sys/types.h>
    #include <sys/sysctl.h>
  #endif

  #if (defined(__linux__) || defined(__FreeBSD__))
    // Required by `getauxval()` on Linux and FreeBSD.
    #include <sys/auxv.h>
    #define ASMJIT_ARM_DETECT_VIA_HWCAPS
  #endif

  #if ASMJIT_ARCH_ARM >= 64 && defined(__GNUC__) && defined(__linux__) && 0
    // This feature is disabled at the moment - it works, but it seems linux supports ARM features
    // via HWCAPS pretty well and the most recent features need to access more registers that were
    // not originally accessible, which would break on some systems.
    #define ASMJIT_ARM_DETECT_VIA_CPUID
  #endif

  #if ASMJIT_ARCH_ARM >= 64 && defined(__OpenBSD__)
    #include <machine/cpu.h>
    #include <sys/sysctl.h>
  #endif

  #if ASMJIT_ARCH_ARM >= 64 && defined(__NetBSD__)
    #include <sys/sysctl.h>
  #endif

#endif // ASMJIT_ARCH_ARM

#if !defined(_WIN32) && (ASMJIT_ARCH_X86 || ASMJIT_ARCH_ARM)
  #include <unistd.h>
#endif

// Unfortunately when compiling in C++11 mode MSVC would warn about unused functions as
// [[maybe_unused]] attribute is not used in that case (it's used only by C++17 mode and later).
#if defined(_MSC_VER)
  #pragma warning(push)
  #pragma warning(disable: 4505) // unreferenced local function has been removed.
#endif // _MSC_VER

ASMJIT_BEGIN_NAMESPACE

// CpuInfo - Detect - Compatibility
// ================================

// CPU features detection is a minefield on non-X86 platforms. The following list describes which
// operating systems and architectures are supported and the status of the implementation:
//
//   * X86, X86_64:
//     - All OSes supported
//     - Detection is based on using a CPUID instruction, which is a user-space instruction, so there
//       is no need to use any OS specific APIs or syscalls to detect all features provided by the CPU.
//
//   * ARM32:
//     - Linux   - HWCAPS based detection.
//     - FreeBSD - HWCAPS based detection (shared with Linux code).
//     - NetBSD  - NOT IMPLEMENTED!
//     - OpenBSD - NOT IMPLEMENTED!
//     - Apple   - sysctlbyname() based detection (this architecture is deprecated on Apple HW).
//     - Windows - IsProcessorFeaturePresent() based detection (only detects a subset of features).
//     - Others  - NOT IMPLEMENTED!
//
//   * ARM64:
//     - Linux   - HWCAPS and CPUID based detection.
//     - FreeBSD - HWCAPS and CPUID based detection (shared with Linux code).
//     - NetBSD  - CPUID based detection (reading CPUID via sysctl's cpu0 info)
//     - OpenBSD - CPUID based detection (reading CPUID via sysctl's CTL_MACHDEP).
//     - Apple   - sysctlbyname() based detection with FamilyId matrix (record for each family id).
//     - Windows - IsProcessorFeaturePresent() based detection (only detects a subset of features).
//     - Others  - NOT IMPLEMENTED!
//
//   * Others
//     - NOT IMPLEMENTED!

// CpuInfo - Detect - HW-Thread Count
// ==================================

#if defined(_WIN32)
static inline uint32_t detectHWThreadCount() noexcept {
  SYSTEM_INFO info;
  ::GetSystemInfo(&info);
  return info.dwNumberOfProcessors;
}
#elif defined(_SC_NPROCESSORS_ONLN)
static inline uint32_t detectHWThreadCount() noexcept {
  long res = ::sysconf(_SC_NPROCESSORS_ONLN);
  return res <= 0 ? uint32_t(1) : uint32_t(res);
}
#else
static inline uint32_t detectHWThreadCount() noexcept {
  return 1;
}
#endif

// CpuInfo - Detect - X86
// ======================

// X86 and X86_64 detection is based on CPUID.

#if ASMJIT_ARCH_X86

namespace x86 {

typedef CpuFeatures::X86 Ext;

struct cpuid_t { uint32_t eax, ebx, ecx, edx; };
struct xgetbv_t { uint32_t eax, edx; };

// Executes `cpuid` instruction.
static inline void cpuidQuery(cpuid_t* out, uint32_t inEax, uint32_t inEcx = 0) noexcept {
#if defined(_MSC_VER)
  __cpuidex(reinterpret_cast<int*>(out), inEax, inEcx);
#elif defined(__GNUC__) && ASMJIT_ARCH_X86 == 32
  __asm__ __volatile__(
    "mov %%ebx, %%edi\n"
    "cpuid\n"
    "xchg %%edi, %%ebx\n" : "=a"(out->eax), "=D"(out->ebx), "=c"(out->ecx), "=d"(out->edx) : "a"(inEax), "c"(inEcx));
#elif defined(__GNUC__) && ASMJIT_ARCH_X86 == 64
  __asm__ __volatile__(
    "mov %%rbx, %%rdi\n"
    "cpuid\n"
    "xchg %%rdi, %%rbx\n" : "=a"(out->eax), "=D"(out->ebx), "=c"(out->ecx), "=d"(out->edx) : "a"(inEax), "c"(inEcx));
#else
  #error "[asmjit] x86::cpuidQuery() - Unsupported compiler."
#endif
}

// Executes 'xgetbv' instruction.
static inline void xgetbvQuery(xgetbv_t* out, uint32_t inEcx) noexcept {
#if defined(_MSC_VER)
  uint64_t value = _xgetbv(inEcx);
  out->eax = uint32_t(value & 0xFFFFFFFFu);
  out->edx = uint32_t(value >> 32);
#elif defined(__GNUC__)
  uint32_t outEax;
  uint32_t outEdx;

  // Replaced, because the world is not perfect:
  //   __asm__ __volatile__("xgetbv" : "=a"(outEax), "=d"(outEdx) : "c"(inEcx));
  __asm__ __volatile__(".byte 0x0F, 0x01, 0xD0" : "=a"(outEax), "=d"(outEdx) : "c"(inEcx));

  out->eax = outEax;
  out->edx = outEdx;
#else
  out->eax = 0;
  out->edx = 0;
#endif
}

// Map a 12-byte vendor string returned by `cpuid` into a `CpuInfo::Vendor` ID.
static inline void simplifyCpuVendor(CpuInfo& cpu, uint32_t d0, uint32_t d1, uint32_t d2) noexcept {
  struct Vendor {
    char normalized[8];
    union { char text[12]; uint32_t d[3]; };
  };

  static const Vendor table[] = {
    { { 'A', 'M', 'D'                     }, {{ 'A', 'u', 't', 'h', 'e', 'n', 't', 'i', 'c', 'A', 'M', 'D' }} },
    { { 'I', 'N', 'T', 'E', 'L'           }, {{ 'G', 'e', 'n', 'u', 'i', 'n', 'e', 'I', 'n', 't', 'e', 'l' }} },
    { { 'V', 'I', 'A'                     }, {{ 'C', 'e', 'n', 't', 'a', 'u', 'r', 'H', 'a', 'u', 'l', 's' }} },
    { { 'V', 'I', 'A'                     }, {{ 'V', 'I', 'A',  0 , 'V', 'I', 'A',  0 , 'V', 'I', 'A',  0  }} },
    { { 'U', 'N', 'K', 'N', 'O', 'W', 'N' }, {{ 0                                                          }} }
  };

  uint32_t i;
  for (i = 0; i < ASMJIT_ARRAY_SIZE(table) - 1; i++)
    if (table[i].d[0] == d0 && table[i].d[1] == d1 && table[i].d[2] == d2)
      break;
  memcpy(cpu._vendor.str, table[i].normalized, 8);
}

static ASMJIT_FAVOR_SIZE void simplifyCpuBrand(char* s) noexcept {
  char* d = s;

  char c = s[0];
  char prev = 0;

  // Used to always clear the current character to ensure that the result
  // doesn't contain garbage after a new null terminator is placed at the end.
  s[0] = '\0';

  for (;;) {
    if (!c)
      break;

    if (!(c == ' ' && (prev == '@' || s[1] == ' ' || s[1] == '@' || s[1] == '\0'))) {
      *d++ = c;
      prev = c;
    }

    c = *++s;
    s[0] = '\0';
  }

  d[0] = '\0';
}

static ASMJIT_FAVOR_SIZE void detectX86Cpu(CpuInfo& cpu) noexcept {
  using Support::bitTest;

  cpuid_t regs;
  xgetbv_t xcr0 { 0, 0 };
  CpuFeatures::X86& features = cpu.features().x86();

  cpu._wasDetected = true;
  cpu._maxLogicalProcessors = 1;

  // We are gonna execute CPUID, which was introduced by I486, so it's the requirement.
  features.add(Ext::kI486);

  // CPUID EAX=0x00 (Basic CPUID Information)
  // ----------------------------------------

  // Get vendor string/id.
  cpuidQuery(&regs, 0x0);

  uint32_t maxId = regs.eax;
  uint32_t maxSubLeafId_0x7 = 0;

  simplifyCpuVendor(cpu, regs.ebx, regs.edx, regs.ecx);

  // CPUID EAX=0x01 (Basic CPUID Information)
  // ----------------------------------------

  if (maxId >= 0x01u) {
    // Get feature flags in ECX/EDX and family/model in EAX.
    cpuidQuery(&regs, 0x1);

    // Fill family and model fields.
    uint32_t modelId  = (regs.eax >> 4) & 0x0F;
    uint32_t familyId = (regs.eax >> 8) & 0x0F;

    // Use extended family and model fields.
    if (familyId == 0x06u || familyId == 0x0Fu)
      modelId += (((regs.eax >> 16) & 0x0Fu) << 4);

    if (familyId == 0x0Fu)
      familyId += ((regs.eax >> 20) & 0xFFu);

    cpu._modelId = modelId;
    cpu._familyId = familyId;
    cpu._brandId = (regs.ebx) & 0xFF;
    cpu._processorType = (regs.eax >> 12) & 0x03;
    cpu._maxLogicalProcessors = (regs.ebx >> 16) & 0xFF;
    cpu._stepping = (regs.eax) & 0x0F;
    cpu._cacheLineSize  = ((regs.ebx >> 8) & 0xFF) * 8;

    features.addIf(bitTest(regs.ecx,  0), Ext::kSSE3);
    features.addIf(bitTest(regs.ecx,  1), Ext::kPCLMULQDQ);
    features.addIf(bitTest(regs.ecx,  3), Ext::kMONITOR);
    features.addIf(bitTest(regs.ecx,  5), Ext::kVMX);
    features.addIf(bitTest(regs.ecx,  6), Ext::kSMX);
    features.addIf(bitTest(regs.ecx,  9), Ext::kSSSE3);
    features.addIf(bitTest(regs.ecx, 13), Ext::kCMPXCHG16B);
    features.addIf(bitTest(regs.ecx, 19), Ext::kSSE4_1);
    features.addIf(bitTest(regs.ecx, 20), Ext::kSSE4_2);
    features.addIf(bitTest(regs.ecx, 22), Ext::kMOVBE);
    features.addIf(bitTest(regs.ecx, 23), Ext::kPOPCNT);
    features.addIf(bitTest(regs.ecx, 25), Ext::kAESNI);
    features.addIf(bitTest(regs.ecx, 26), Ext::kXSAVE);
    features.addIf(bitTest(regs.ecx, 27), Ext::kOSXSAVE);
    features.addIf(bitTest(regs.ecx, 30), Ext::kRDRAND);
    features.addIf(bitTest(regs.edx,  0), Ext::kFPU);
    features.addIf(bitTest(regs.edx,  4), Ext::kRDTSC);
    features.addIf(bitTest(regs.edx,  5), Ext::kMSR);
    features.addIf(bitTest(regs.edx,  8), Ext::kCMPXCHG8B);
    features.addIf(bitTest(regs.edx, 15), Ext::kCMOV);
    features.addIf(bitTest(regs.edx, 19), Ext::kCLFLUSH);
    features.addIf(bitTest(regs.edx, 23), Ext::kMMX);
    features.addIf(bitTest(regs.edx, 24), Ext::kFXSR);
    features.addIf(bitTest(regs.edx, 25), Ext::kSSE, Ext::kMMX2);
    features.addIf(bitTest(regs.edx, 26), Ext::kSSE2, Ext::kSSE);
    features.addIf(bitTest(regs.edx, 28), Ext::kMT);

    // Get the content of XCR0 if supported by the CPU and enabled by the OS.
    if (features.hasXSAVE() && features.hasOSXSAVE()) {
      xgetbvQuery(&xcr0, 0);
    }

    // Detect AVX+.
    if (bitTest(regs.ecx, 28)) {
      // - XCR0[2:1] == 11b
      //   XMM & YMM states need to be enabled by OS.
      if ((xcr0.eax & 0x00000006u) == 0x00000006u) {
        features.add(Ext::kAVX);
        features.addIf(bitTest(regs.ecx, 12), Ext::kFMA);
        features.addIf(bitTest(regs.ecx, 29), Ext::kF16C);
      }
    }
  }

  constexpr uint32_t kXCR0_AMX_Bits = 0x3u << 17;
  bool amxEnabled = (xcr0.eax & kXCR0_AMX_Bits) == kXCR0_AMX_Bits;

#if defined(__APPLE__)
  // Apple platform provides on-demand AVX512 support. When an AVX512 instruction is used the first time it results
  // in #UD, which would cause the thread being promoted to use AVX512 support by the OS in addition to enabling the
  // necessary bits in XCR0 register.
  bool avx512Enabled = true;
#else
  // - XCR0[2:1] ==  11b - XMM/YMM states need to be enabled by OS.
  // - XCR0[7:5] == 111b - Upper 256-bit of ZMM0-XMM15 and ZMM16-ZMM31 need to be enabled by OS.
  constexpr uint32_t kXCR0_AVX512_Bits = (0x3u << 1) | (0x7u << 5);
  bool avx512Enabled = (xcr0.eax & kXCR0_AVX512_Bits) == kXCR0_AVX512_Bits;
#endif

  bool avx10Enabled = false;

  // CPUID EAX=0x07 ECX=0 (Structured Extended Feature Flags Enumeration Leaf)
  // -------------------------------------------------------------------------

  if (maxId >= 0x07u) {
    cpuidQuery(&regs, 0x7);

    maxSubLeafId_0x7 = regs.eax;

    features.addIf(bitTest(regs.ebx,  0), Ext::kFSGSBASE);
    features.addIf(bitTest(regs.ebx,  3), Ext::kBMI);
    features.addIf(bitTest(regs.ebx,  7), Ext::kSMEP);
    features.addIf(bitTest(regs.ebx,  8), Ext::kBMI2);
    features.addIf(bitTest(regs.ebx,  9), Ext::kERMS);
    features.addIf(bitTest(regs.ebx, 18), Ext::kRDSEED);
    features.addIf(bitTest(regs.ebx, 19), Ext::kADX);
    features.addIf(bitTest(regs.ebx, 20), Ext::kSMAP);
    features.addIf(bitTest(regs.ebx, 23), Ext::kCLFLUSHOPT);
    features.addIf(bitTest(regs.ebx, 24), Ext::kCLWB);
    features.addIf(bitTest(regs.ebx, 29), Ext::kSHA);
    features.addIf(bitTest(regs.ecx,  0), Ext::kPREFETCHWT1);
    features.addIf(bitTest(regs.ecx,  4), Ext::kOSPKE);
    features.addIf(bitTest(regs.ecx,  5), Ext::kWAITPKG);
    features.addIf(bitTest(regs.ecx,  7), Ext::kCET_SS);
    features.addIf(bitTest(regs.ecx,  8), Ext::kGFNI);
    features.addIf(bitTest(regs.ecx,  9), Ext::kVAES);
    features.addIf(bitTest(regs.ecx, 10), Ext::kVPCLMULQDQ);
    features.addIf(bitTest(regs.ecx, 22), Ext::kRDPID);
    features.addIf(bitTest(regs.ecx, 23), Ext::kKL);
    features.addIf(bitTest(regs.ecx, 25), Ext::kCLDEMOTE);
    features.addIf(bitTest(regs.ecx, 27), Ext::kMOVDIRI);
    features.addIf(bitTest(regs.ecx, 28), Ext::kMOVDIR64B);
    features.addIf(bitTest(regs.ecx, 29), Ext::kENQCMD);
    features.addIf(bitTest(regs.edx,  4), Ext::kFSRM);
    features.addIf(bitTest(regs.edx,  5), Ext::kUINTR);
    features.addIf(bitTest(regs.edx, 14), Ext::kSERIALIZE);
    features.addIf(bitTest(regs.edx, 16), Ext::kTSXLDTRK);
    features.addIf(bitTest(regs.edx, 18), Ext::kPCONFIG);
    features.addIf(bitTest(regs.edx, 20), Ext::kCET_IBT);

    if (bitTest(regs.ebx, 5) && features.hasAVX()) {
      features.add(Ext::kAVX2);
    }

    if (avx512Enabled && bitTest(regs.ebx, 16)) {
      features.add(Ext::kAVX512_F);

      features.addIf(bitTest(regs.ebx, 17), Ext::kAVX512_DQ);
      features.addIf(bitTest(regs.ebx, 21), Ext::kAVX512_IFMA);
      features.addIf(bitTest(regs.ebx, 28), Ext::kAVX512_CD);
      features.addIf(bitTest(regs.ebx, 30), Ext::kAVX512_BW);
      features.addIf(bitTest(regs.ebx, 31), Ext::kAVX512_VL);
      features.addIf(bitTest(regs.ecx,  1), Ext::kAVX512_VBMI);
      features.addIf(bitTest(regs.ecx,  6), Ext::kAVX512_VBMI2);
      features.addIf(bitTest(regs.ecx, 11), Ext::kAVX512_VNNI);
      features.addIf(bitTest(regs.ecx, 12), Ext::kAVX512_BITALG);
      features.addIf(bitTest(regs.ecx, 14), Ext::kAVX512_VPOPCNTDQ);
      features.addIf(bitTest(regs.edx,  8), Ext::kAVX512_VP2INTERSECT);
      features.addIf(bitTest(regs.edx, 23), Ext::kAVX512_FP16);
    }

    if (amxEnabled) {
      features.addIf(bitTest(regs.edx, 22), Ext::kAMX_BF16);
      features.addIf(bitTest(regs.edx, 24), Ext::kAMX_TILE);
      features.addIf(bitTest(regs.edx, 25), Ext::kAMX_INT8);
    }
  }

  // CPUID EAX=0x07 ECX=1 (Structured Extended Feature Enumeration Sub-leaf)
  // -----------------------------------------------------------------------

  if (maxSubLeafId_0x7 >= 1) {
    cpuidQuery(&regs, 0x7, 1);

    features.addIf(bitTest(regs.eax,  0), Ext::kSHA512);
    features.addIf(bitTest(regs.eax,  1), Ext::kSM3);
    features.addIf(bitTest(regs.eax,  2), Ext::kSM4);
    features.addIf(bitTest(regs.eax,  3), Ext::kRAO_INT);
    features.addIf(bitTest(regs.eax,  7), Ext::kCMPCCXADD);
    features.addIf(bitTest(regs.eax, 10), Ext::kFZRM);
    features.addIf(bitTest(regs.eax, 11), Ext::kFSRS);
    features.addIf(bitTest(regs.eax, 12), Ext::kFSRC);
    features.addIf(bitTest(regs.eax, 19), Ext::kWRMSRNS);
    features.addIf(bitTest(regs.eax, 22), Ext::kHRESET);
    features.addIf(bitTest(regs.eax, 26), Ext::kLAM);
    features.addIf(bitTest(regs.eax, 27), Ext::kMSRLIST);
    features.addIf(bitTest(regs.eax, 31), Ext::kMOVRS);
    features.addIf(bitTest(regs.ecx,  5), Ext::kMSR_IMM);
    features.addIf(bitTest(regs.ebx,  1), Ext::kTSE);
    features.addIf(bitTest(regs.edx, 14), Ext::kPREFETCHI);
    features.addIf(bitTest(regs.edx, 18), Ext::kCET_SSS);
    features.addIf(bitTest(regs.edx, 21), Ext::kAPX_F);

    if (features.hasAVX2()) {
      features.addIf(bitTest(regs.eax,  4), Ext::kAVX_VNNI);
      features.addIf(bitTest(regs.eax, 23), Ext::kAVX_IFMA);
      features.addIf(bitTest(regs.edx,  4), Ext::kAVX_VNNI_INT8);
      features.addIf(bitTest(regs.edx,  5), Ext::kAVX_NE_CONVERT);
      features.addIf(bitTest(regs.edx, 10), Ext::kAVX_VNNI_INT16);
    }

    if (features.hasAVX512_F()) {
      features.addIf(bitTest(regs.eax,  5), Ext::kAVX512_BF16);
    }

    if (features.hasAVX512_F()) {
      avx10Enabled = Support::bitTest(regs.edx, 19);
    }

    if (amxEnabled) {
      features.addIf(bitTest(regs.eax, 21), Ext::kAMX_FP16);
      features.addIf(bitTest(regs.edx,  8), Ext::kAMX_COMPLEX);
    }
  }

  // CPUID EAX=0x0D ECX=1 (Processor Extended State Enumeration Sub-leaf)
  // --------------------------------------------------------------------

  if (maxId >= 0x0Du) {
    cpuidQuery(&regs, 0xD, 1);

    features.addIf(bitTest(regs.eax, 0), Ext::kXSAVEOPT);
    features.addIf(bitTest(regs.eax, 1), Ext::kXSAVEC);
    features.addIf(bitTest(regs.eax, 3), Ext::kXSAVES);
  }

  // CPUID EAX=0x0E ECX=0 (Processor Trace Enumeration Main Leaf)
  // ------------------------------------------------------------

  if (maxId >= 0x0Eu) {
    cpuidQuery(&regs, 0x0E, 0);

    features.addIf(bitTest(regs.ebx, 4), Ext::kPTWRITE);
  }

  // CPUID EAX=0x19 ECX=0 (Key Locker Leaf)
  // --------------------------------------

  if (maxId >= 0x19u && features.hasKL()) {
    cpuidQuery(&regs, 0x19, 0);

    features.addIf(bitTest(regs.ebx, 0), Ext::kAESKLE);
    features.addIf(bitTest(regs.ebx, 0) && bitTest(regs.ebx, 2), Ext::kAESKLEWIDE_KL);
  }

  // CPUID EAX=0x1E ECX=1 (TMUL Information Sub-leaf)
  // ------------------------------------------------

  if (maxId >= 0x1Eu && features.hasAMX_TILE()) {
    cpuidQuery(&regs, 0x1E, 1);

    // NOTE: Some AMX flags are mirrored here from CPUID[0x07, 0x00].
    features.addIf(bitTest(regs.eax, 0), Ext::kAMX_INT8);
    features.addIf(bitTest(regs.eax, 1), Ext::kAMX_BF16);
    features.addIf(bitTest(regs.eax, 2), Ext::kAMX_COMPLEX);
    features.addIf(bitTest(regs.eax, 3), Ext::kAMX_FP16);
    features.addIf(bitTest(regs.eax, 4), Ext::kAMX_FP8);
    features.addIf(bitTest(regs.eax, 5), Ext::kAMX_TRANSPOSE);
    features.addIf(bitTest(regs.eax, 6), Ext::kAMX_TF32);
    features.addIf(bitTest(regs.eax, 7), Ext::kAMX_AVX512);
    features.addIf(bitTest(regs.eax, 8), Ext::kAMX_MOVRS);
  }

  // CPUID EAX=0x24 ECX=0 (AVX10 Information)
  // ----------------------------------------

  if (maxId >= 0x24u && avx10Enabled) {
    // EAX output is the maximum supported sub-leaf.
    cpuidQuery(&regs, 0x24, 0);

    // AVX10 Converged Vector ISA version.
    uint32_t ver = regs.ebx & 0xFFu;

    features.addIf(ver >= 1u, Ext::kAVX10_1);
    features.addIf(ver >= 2u, Ext::kAVX10_2);
  }

  // CPUID EAX=0x80000000...maxId
  // ----------------------------

  maxId = 0x80000000u;
  uint32_t i = maxId;

  // The highest EAX that we understand.
  constexpr uint32_t kHighestProcessedEAX = 0x8000001Fu;

  // Several CPUID calls are required to get the whole brand string. It's easier
  // to copy one DWORD at a time instead of copying the string a byte by byte.
  uint32_t* brand = cpu._brand.u32;
  do {
    cpuidQuery(&regs, i);
    switch (i) {
      case 0x80000000u:
        maxId = Support::min<uint32_t>(regs.eax, kHighestProcessedEAX);
        break;

      case 0x80000001u:
        features.addIf(bitTest(regs.ecx,  0), Ext::kLAHFSAHF);
        features.addIf(bitTest(regs.ecx,  2), Ext::kSVM);
        features.addIf(bitTest(regs.ecx,  5), Ext::kLZCNT);
        features.addIf(bitTest(regs.ecx,  6), Ext::kSSE4A);
        features.addIf(bitTest(regs.ecx,  7), Ext::kMSSE);
        features.addIf(bitTest(regs.ecx,  8), Ext::kPREFETCHW);
        features.addIf(bitTest(regs.ecx, 12), Ext::kSKINIT);
        features.addIf(bitTest(regs.ecx, 15), Ext::kLWP);
        features.addIf(bitTest(regs.ecx, 21), Ext::kTBM);
        features.addIf(bitTest(regs.ecx, 29), Ext::kMONITORX);
        features.addIf(bitTest(regs.edx, 20), Ext::kNX);
        features.addIf(bitTest(regs.edx, 21), Ext::kFXSROPT);
        features.addIf(bitTest(regs.edx, 22), Ext::kMMX2);
        features.addIf(bitTest(regs.edx, 27), Ext::kRDTSCP);
        features.addIf(bitTest(regs.edx, 29), Ext::kPREFETCHW);
        features.addIf(bitTest(regs.edx, 30), Ext::k3DNOW2, Ext::kMMX2);
        features.addIf(bitTest(regs.edx, 31), Ext::kPREFETCHW);

        if (features.hasAVX()) {
          features.addIf(bitTest(regs.ecx, 11), Ext::kXOP);
          features.addIf(bitTest(regs.ecx, 16), Ext::kFMA4);
        }

        // This feature seems to be only supported by AMD.
        if (cpu.isVendor("AMD")) {
          features.addIf(bitTest(regs.ecx,  4), Ext::kALTMOVCR8);
        }
        break;

      case 0x80000002u:
      case 0x80000003u:
      case 0x80000004u:
        *brand++ = regs.eax;
        *brand++ = regs.ebx;
        *brand++ = regs.ecx;
        *brand++ = regs.edx;

        // Go directly to the next one we are interested in.
        if (i == 0x80000004u)
          i = 0x80000008u - 1;
        break;

      case 0x80000008u:
        features.addIf(bitTest(regs.ebx,  0), Ext::kCLZERO);
        features.addIf(bitTest(regs.ebx,  0), Ext::kRDPRU);
        features.addIf(bitTest(regs.ebx,  8), Ext::kMCOMMIT);
        features.addIf(bitTest(regs.ebx,  9), Ext::kWBNOINVD);

        // Go directly to the next one we are interested in.
        i = 0x8000001Fu - 1;
        break;

      case 0x8000001Fu:
        features.addIf(bitTest(regs.eax,  0), Ext::kSME);
        features.addIf(bitTest(regs.eax,  1), Ext::kSEV);
        features.addIf(bitTest(regs.eax,  3), Ext::kSEV_ES);
        features.addIf(bitTest(regs.eax,  4), Ext::kSEV_SNP);
        features.addIf(bitTest(regs.eax,  6), Ext::kRMPQUERY);
        break;
    }
  } while (++i <= maxId);

  // Simplify CPU brand string a bit by removing some unnecessary spaces.
  simplifyCpuBrand(cpu._brand.str);
}

} // {x86}

#endif // ASMJIT_ARCH_X86

// CpuInfo - Detect - ARM
// ======================

// Implement the most code outside the platform specific #ifdefs to minimize breaking the detection on
// platforms that don't run on our CI infrastructure. The problem with the detection is that every OS
// requires a specific implementation as ARM features cannot be detected in user-mode without OS enablement.

// The most relevant and accurate information can be found here:
//   https://github.com/llvm-project/llvm/blob/master/lib/Target/AArch64/AArch64.td
//   https://github.com/apple/llvm-project/blob/apple/main/llvm/lib/Target/AArch64/AArch64.td (Apple fork)
//
// Other resources:
//   https://en.wikipedia.org/wiki/AArch64
//   https://en.wikipedia.org/wiki/Apple_silicon#List_of_Apple_processors
//   https://developer.arm.com/downloads/-/exploration-tools/feature-names-for-a-profile
//   https://developer.arm.com/architectures/learn-the-architecture/understanding-the-armv8-x-extensions/single-page

#if ASMJIT_ARCH_ARM

namespace arm {

// ARM commonly refers to CPU features using FEAT_ prefix, we use Ext:: to make it compatible with other parts.
typedef CpuFeatures::ARM Ext;

// CpuInfo - Detect - ARM - OS Kernel Version
// ==========================================

#if defined(__linux__)
struct UNameKernelVersion {
  int parts[3];

  inline bool atLeast(int major, int minor, int patch = 0) const noexcept {
    if (parts[0] >= major) {
      if (parts[0] > major)
        return true;

      if (parts[1] >= minor) {
        if (parts[1] > minor)
          return true;

        return parts[2] >= patch;
      }
    }

    return false;
  }
};

ASMJIT_MAYBE_UNUSED
static UNameKernelVersion getUNameKernelVersion() noexcept {
  UNameKernelVersion ver{};
  ver.parts[0] = -1;

  utsname buffer;
  if (uname(&buffer) != 0)
    return ver;

  size_t count = 0;
  char* p = buffer.release;
  while (*p) {
    uint32_t c = uint8_t(*p);
    if (c >= uint32_t('0') && c <= uint32_t('9')) {
      ver.parts[count] = int(strtol(p, &p, 10));
      if (++count == 3)
        break;
    }
    else if (c == '.' || c == '-') {
      p++;
    }
    else {
      break;
    }
  }

  return ver;
}
#endif // __linux__

// CpuInfo - Detect - ARM - Baseline Features of ARM Architectures
// ===============================================================

ASMJIT_MAYBE_UNUSED
static inline void populateBaseAArch32Features(CpuFeatures::ARM& features) noexcept {
  // No baseline flags at the moment.
  DebugUtils::unused(features);
}

ASMJIT_MAYBE_UNUSED
static inline void populateBaseAArch64Features(CpuFeatures::ARM& features) noexcept {
  // AArch64 is based on ARMv8.0 and later.
  features.add(Ext::kARMv6);
  features.add(Ext::kARMv7);
  features.add(Ext::kARMv8a);

  // AArch64 comes with these features by default.
  features.add(Ext::kASIMD);
  features.add(Ext::kFP);
  features.add(Ext::kIDIVA);
}

static inline void populateBaseARMFeatures(CpuInfo& cpu) noexcept {
#if ASMJIT_ARCH_ARM == 32
  populateBaseAArch32Features(cpu.features().arm());
#else
  populateBaseAArch64Features(cpu.features().arm());
#endif
}

// CpuInfo - Detect - ARM - Mandatory Features of ARM Architectures
// ================================================================

// Populates mandatory ARMv8.[v]A features.
ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void populateARMv8AFeatures(CpuFeatures::ARM& features, uint32_t v) noexcept {
  switch (v) {
    default:
      ASMJIT_FALLTHROUGH;
    case 9: // ARMv8.9
      features.add(Ext::kCLRBHB, Ext::kCSSC, Ext::kPRFMSLC, Ext::kSPECRES2, Ext::kRAS2);
      ASMJIT_FALLTHROUGH;
    case 8: // ARMv8.8
      features.add(Ext::kHBC, Ext::kMOPS, Ext::kNMI);
      ASMJIT_FALLTHROUGH;
    case 7: // ARMv8.7
      features.add(Ext::kHCX, Ext::kPAN3, Ext::kWFXT, Ext::kXS);
      ASMJIT_FALLTHROUGH;
    case 6: // ARMv8.6
      features.add(Ext::kAMU1_1, Ext::kBF16, Ext::kECV, Ext::kFGT, Ext::kI8MM);
      ASMJIT_FALLTHROUGH;
    case 5: // ARMv8.5
      features.add(Ext::kBTI, Ext::kCSV2, Ext::kDPB2, Ext::kFLAGM2, Ext::kFRINTTS, Ext::kSB, Ext::kSPECRES, Ext::kSSBS);
      ASMJIT_FALLTHROUGH;
    case 4: // ARMv8.4
      features.add(Ext::kAMU1, Ext::kDIT, Ext::kDOTPROD, Ext::kFLAGM,
                   Ext::kLRCPC2, Ext::kLSE2, Ext::kMPAM, Ext::kNV,
                   Ext::kSEL2, Ext::kTLBIOS, Ext::kTLBIRANGE, Ext::kTRF);
      ASMJIT_FALLTHROUGH;
    case 3: // ARMv8.3
      features.add(Ext::kCCIDX, Ext::kFCMA, Ext::kJSCVT, Ext::kLRCPC, Ext::kPAUTH);
      ASMJIT_FALLTHROUGH;
    case 2: // ARMv8.2
      features.add(Ext::kDPB, Ext::kPAN2, Ext::kRAS, Ext::kUAO);
      ASMJIT_FALLTHROUGH;
    case 1: // ARMv8.1
      features.add(Ext::kCRC32, Ext::kLOR, Ext::kLSE, Ext::kPAN, Ext::kRDM, Ext::kVHE);
      ASMJIT_FALLTHROUGH;
    case 0: // ARMv8.0
      features.add(Ext::kASIMD, Ext::kFP, Ext::kIDIVA, Ext::kVFP_D32);
      break;
  }
}

// Populates mandatory ARMv9.[v] features.
ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void populateARMv9AFeatures(CpuFeatures::ARM& features, uint32_t v) noexcept {
  populateARMv8AFeatures(features, v <= 4u ? 5u + v : 9u);

  switch (v) {
    default:
      ASMJIT_FALLTHROUGH;
    case 4: // ARMv9.4 - based on ARMv8.9.
      ASMJIT_FALLTHROUGH;
    case 3: // ARMv9.3 - based on ARMv8.8.
      ASMJIT_FALLTHROUGH;
    case 2: // ARMv9.2 - based on ARMv8.7.
      ASMJIT_FALLTHROUGH;
    case 1: // ARMv9.1 - based on ARMv8.6.
      ASMJIT_FALLTHROUGH;
    case 0: // ARMv9.0 - based on ARMv8.5.
      features.add(Ext::kRME, Ext::kSVE, Ext::kSVE2);
      break;
  }
}

// CpuInfo - Detect - ARM - CPUID Based Features
// =============================================

// This implements detection based on the content of CPUID registers. The following code doesn't actually read any
// of the registers so it's an implementation that can theoretically be tested / used in mocks.

// Merges a feature that contains 0b1111 when it doesn't exist and starts at 0b0000 when it does.
ASMJIT_MAYBE_UNUSED
static ASMJIT_FORCE_INLINE void mergeAArch64CPUIDFeatureNA(CpuFeatures::ARM& features, uint64_t regBits, uint32_t offset,
  Ext::Id f0,
  Ext::Id f1 = Ext::kNone,
  Ext::Id f2 = Ext::kNone,
  Ext::Id f3 = Ext::kNone) noexcept {

  uint32_t val = uint32_t((regBits >> offset) & 0xFu);

  // If val == 0b1111 then the feature is not implemented in this case (some early extensions).
  if (val == 0xFu)
    return;

  if (f0 != Ext::kNone) features.add(f0);
  if (f1 != Ext::kNone) features.addIf(val >= 1, f1);
  if (f2 != Ext::kNone) features.addIf(val >= 2, f2);
  if (f3 != Ext::kNone) features.addIf(val >= 3, f3);
}

// Merges a feature identified by a single bit at `offset`.
ASMJIT_MAYBE_UNUSED
static ASMJIT_FORCE_INLINE void mergeAArch64CPUIDFeature1B(CpuFeatures::ARM& features, uint64_t regBits, uint32_t offset, Ext::Id f1) noexcept {
  features.addIf((regBits & (uint64_t(1) << offset)) != 0, f1);
}

// Merges a feature-list starting from 0b01 when it does (0b00 means feature not supported).
ASMJIT_MAYBE_UNUSED
static ASMJIT_FORCE_INLINE void mergeAArch64CPUIDFeature2B(CpuFeatures::ARM& features, uint64_t regBits, uint32_t offset, Ext::Id f1, Ext::Id f2, Ext::Id f3) noexcept {
  uint32_t val = uint32_t((regBits >> offset) & 0x3u);

  if (f1 != Ext::kNone) features.addIf(val >= 1, f1);
  if (f2 != Ext::kNone) features.addIf(val >= 2, f2);
  if (f3 != Ext::kNone) features.addIf(val == 3, f3);
}

// Merges a feature-list starting from 0b0001 when it does (0b0000 means feature not supported).
ASMJIT_MAYBE_UNUSED
static ASMJIT_FORCE_INLINE void mergeAArch64CPUIDFeature4B(CpuFeatures::ARM& features, uint64_t regBits, uint32_t offset,
  Ext::Id f1,
  Ext::Id f2 = Ext::kNone,
  Ext::Id f3 = Ext::kNone,
  Ext::Id f4 = Ext::kNone) noexcept {

  uint32_t val = uint32_t((regBits >> offset) & 0xFu);

  // if val == 0 it means that this feature is not supported.

  if (f1 != Ext::kNone) features.addIf(val >= 1, f1);
  if (f2 != Ext::kNone) features.addIf(val >= 2, f2);
  if (f3 != Ext::kNone) features.addIf(val >= 3, f3);
  if (f4 != Ext::kNone) features.addIf(val >= 4, f4);
}

// Merges a feature that is identified by an exact bit-combination of 4 bits.
ASMJIT_MAYBE_UNUSED
static ASMJIT_FORCE_INLINE void mergeAArch64CPUIDFeature4S(CpuFeatures::ARM& features, uint64_t regBits, uint32_t offset, uint32_t value, Ext::Id f1) noexcept {
  features.addIf(uint32_t((regBits >> offset) & 0xFu) == value, f1);
}

#define MERGE_FEATURE_NA(identifier, reg, offset, ...) mergeAArch64CPUIDFeatureNA(cpu.features().arm(), reg, offset, __VA_ARGS__)
#define MERGE_FEATURE_1B(identifier, reg, offset, ...) mergeAArch64CPUIDFeature1B(cpu.features().arm(), reg, offset, __VA_ARGS__)
#define MERGE_FEATURE_2B(identifier, reg, offset, ...) mergeAArch64CPUIDFeature2B(cpu.features().arm(), reg, offset, __VA_ARGS__)
#define MERGE_FEATURE_4B(identifier, reg, offset, ...) mergeAArch64CPUIDFeature4B(cpu.features().arm(), reg, offset, __VA_ARGS__)
#define MERGE_FEATURE_4S(identifier, reg, offset, ...) mergeAArch64CPUIDFeature4S(cpu.features().arm(), reg, offset, __VA_ARGS__)

// Detects features based on the content of ID_AA64PFR0_EL1 and ID_AA64PFR1_EL1 registers.
ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64PFR0_AA64PFR1(CpuInfo& cpu, uint64_t fpr0, uint64_t fpr1) noexcept {
  // ID_AA64PFR0_EL1
  // ===============

  // FP and AdvSIMD bits should match (i.e. if FP features FP16, ASIMD must feature it too).
  MERGE_FEATURE_NA("FP bits [19:16]"          , fpr0, 16, Ext::kFP, Ext::kFP16);
  MERGE_FEATURE_NA("AdvSIMD bits [23:20]"     , fpr0, 20, Ext::kASIMD, Ext::kFP16);
  /*
  MERGE_FEATURE_4B("GIC bits [27:24]"         , fpr0, 24, ...);
  */
  MERGE_FEATURE_4B("RAS bits [31:28]"         , fpr0, 28, Ext::kRAS, Ext::kRAS1_1, Ext::kRAS2);
  MERGE_FEATURE_4B("SVE bits [35:32]"         , fpr0, 32, Ext::kSVE);
  MERGE_FEATURE_4B("SEL2 bits [39:36]"        , fpr0, 36, Ext::kSEL2);
  MERGE_FEATURE_4B("MPAM bits [43:40]"        , fpr0, 40, Ext::kMPAM);
  MERGE_FEATURE_4B("AMU bits [47:44]"         , fpr0, 44, Ext::kAMU1, Ext::kAMU1_1);
  MERGE_FEATURE_4B("DIT bits [51:48]"         , fpr0, 48, Ext::kDIT);
  MERGE_FEATURE_4B("RME bits [55:52]"         , fpr0, 52, Ext::kRME);
  MERGE_FEATURE_4B("CSV2 bits [59:56]"        , fpr0, 56, Ext::kCSV2, Ext::kCSV2, Ext::kCSV2, Ext::kCSV2_3);
  MERGE_FEATURE_4B("CSV3 bits [63:60]"        , fpr0, 60, Ext::kCSV3);

  // ID_AA64PFR1_EL1
  // ===============

  MERGE_FEATURE_4B("BT bits [3:0]"            , fpr1,  0, Ext::kBTI);
  MERGE_FEATURE_4B("SSBS bits [7:4]"          , fpr1,  4, Ext::kSSBS, Ext::kSSBS2);
  MERGE_FEATURE_4B("MTE bits [11:8]"          , fpr1,  8, Ext::kMTE, Ext::kMTE2, Ext::kMTE3);
  /*
  MERGE_FEATURE_4B("RAS_frac bits [15:12]"    , fpr1, 12, ...);
  MERGE_FEATURE_4B("MPAM_frac bits [19:16]"   , fpr1, 16, ...);
  */
  MERGE_FEATURE_4B("SME bits [27:24]"         , fpr1, 24, Ext::kSME, Ext::kSME2);
  MERGE_FEATURE_4B("RNDR_trap bits [31:28]"   , fpr1, 28, Ext::kRNG_TRAP);
  /*
  MERGE_FEATURE_4B("CSV2_frac bits [35:32]"   , fpr1, 32, ...);
  */
  MERGE_FEATURE_4B("NMI bits [39:36]"         , fpr1, 36, Ext::kNMI);
  /*
  MERGE_FEATURE_4B("MTE_frac bits [43:40]"    , fpr1, 40, ...);
  */
  MERGE_FEATURE_4B("GCS bits [47:44]"         , fpr1, 44, Ext::kGCS);
  MERGE_FEATURE_4B("THE bits [51:48]"         , fpr1, 48, Ext::kTHE);

  // MTEX extensions are only available when MTE3 is available.
  if (cpu.features().arm().hasMTE3())
    MERGE_FEATURE_4B("MTEX bits [55:52]"      , fpr1, 52, Ext::kMTE4);

  /*
  MERGE_FEATURE_4B("DF2 bits [59:56]"         , fpr1, 56, ...);
  */
  MERGE_FEATURE_4B("PFAR bits [63:60]"        , fpr1, 60, Ext::kPFAR);

  // ID_AA64PFR0_EL1 + ID_AA64PFR1_EL1
  // =================================

  uint32_t rasMain = uint32_t((fpr0 >> 28) & 0xFu);
  uint32_t rasFrac = uint32_t((fpr1 >> 12) & 0xFu);

  if (rasMain == 1 && rasFrac == 1) {
    cpu.features().arm().add(Ext::kRAS1_1);
  }

  uint32_t mpamMain = uint32_t((fpr0 >> 40) & 0xFu);
  uint32_t mpamFrac = uint32_t((fpr1 >> 16) & 0xFu);

  if (mpamMain || mpamFrac)
    cpu.features().arm().add(Ext::kMPAM);
}

// Detects features based on the content of ID_AA64ISAR0_EL1 and ID_AA64ISAR1_EL1 registers.
ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64ISAR0_AA64ISAR1(CpuInfo& cpu, uint64_t isar0, uint64_t isar1) noexcept {
  // ID_AA64ISAR0_EL1
  // ================

  MERGE_FEATURE_4B("AES bits [7:4]"           , isar0,  4, Ext::kAES, Ext::kPMULL);
  MERGE_FEATURE_4B("SHA1 bits [11:8]"         , isar0,  8, Ext::kSHA1);
  MERGE_FEATURE_4B("SHA2 bits [15:12]"        , isar0, 12, Ext::kSHA256, Ext::kSHA512);
  MERGE_FEATURE_4B("CRC32 bits [19:16]"       , isar0, 16, Ext::kCRC32);
  MERGE_FEATURE_4B("Atomic bits [23:20]"      , isar0, 20, Ext::kNone, Ext::kLSE, Ext::kLSE128);
  MERGE_FEATURE_4B("TME bits [27:24]"         , isar0, 24, Ext::kTME);
  MERGE_FEATURE_4B("RDM bits [31:28]"         , isar0, 28, Ext::kRDM);
  MERGE_FEATURE_4B("SHA3 bits [35:32]"        , isar0, 32, Ext::kSHA3);
  MERGE_FEATURE_4B("SM3 bits [39:36]"         , isar0, 36, Ext::kSM3);
  MERGE_FEATURE_4B("SM4 bits [43:40]"         , isar0, 40, Ext::kSM4);
  MERGE_FEATURE_4B("DP bits [47:44]"          , isar0, 44, Ext::kDOTPROD);
  MERGE_FEATURE_4B("FHM bits [51:48]"         , isar0, 48, Ext::kFHM);
  MERGE_FEATURE_4B("TS bits [55:52]"          , isar0, 52, Ext::kFLAGM, Ext::kFLAGM2);
  /*
  MERGE_FEATURE_4B("TLB bits [59:56]"         , isar0, 56, ...);
  */
  MERGE_FEATURE_4B("RNDR bits [63:60]"        , isar0, 60, Ext::kFLAGM, Ext::kRNG);

  // ID_AA64ISAR1_EL1
  // ================

  MERGE_FEATURE_4B("DPB bits [3:0]"           , isar1,  0, Ext::kDPB, Ext::kDPB2);
  /*
  MERGE_FEATURE_4B("APA bits [7:4]"           , isar1,  4, ...);
  MERGE_FEATURE_4B("API bits [11:8]"          , isar1,  8, ...);
  */
  MERGE_FEATURE_4B("JSCVT bits [15:12]"       , isar1, 12, Ext::kJSCVT);
  MERGE_FEATURE_4B("FCMA bits [19:16]"        , isar1, 16, Ext::kFCMA);
  MERGE_FEATURE_4B("LRCPC bits [23:20]"       , isar1, 20, Ext::kLRCPC, Ext::kLRCPC2, Ext::kLRCPC3);
  /*
  MERGE_FEATURE_4B("GPA bits [27:24]"         , isar1, 24, ...);
  MERGE_FEATURE_4B("GPI bits [31:28]"         , isar1, 28, ...);
  */
  MERGE_FEATURE_4B("FRINTTS bits [35:32]"     , isar1, 32, Ext::kFRINTTS);
  MERGE_FEATURE_4B("SB bits [39:36]"          , isar1, 36, Ext::kSB);
  MERGE_FEATURE_4B("SPECRES bits [43:40]"     , isar1, 40, Ext::kSPECRES, Ext::kSPECRES2);
  MERGE_FEATURE_4B("BF16 bits [47:44]"        , isar1, 44, Ext::kBF16, Ext::kEBF16);
  MERGE_FEATURE_4B("DGH bits [51:48]"         , isar1, 48, Ext::kDGH);
  MERGE_FEATURE_4B("I8MM bits [55:52]"        , isar1, 52, Ext::kI8MM);
  MERGE_FEATURE_4B("XS bits [59:56]"          , isar1, 56, Ext::kXS);
  MERGE_FEATURE_4B("LS64 bits [63:60]"        , isar1, 60, Ext::kLS64, Ext::kLS64_V, Ext::kLS64_ACCDATA);
}

// Detects features based on the content of ID_AA64ISAR2_EL1 register.
ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64ISAR2(CpuInfo& cpu, uint64_t isar2) noexcept {
  MERGE_FEATURE_4B("WFxT bits [3:0]"          , isar2,  0, Ext::kNone, Ext::kWFXT);
  MERGE_FEATURE_4B("RPRES bits [7:4]"         , isar2,  4, Ext::kRPRES);
  /*
  MERGE_FEATURE_4B("GPA3 bits [11:8]"         , isar2,  8, ...);
  MERGE_FEATURE_4B("APA3 bits [15:12]"        , isar2, 12, ...);
  */
  MERGE_FEATURE_4B("MOPS bits [19:16]"        , isar2, 16, Ext::kMOPS);
  MERGE_FEATURE_4B("BC bits [23:20]"          , isar2, 20, Ext::kHBC);
  MERGE_FEATURE_4B("PAC_frac bits [27:24]"    , isar2, 24, Ext::kCONSTPACFIELD);
  MERGE_FEATURE_4B("CLRBHB bits [31:28]"      , isar2, 28, Ext::kCLRBHB);
  MERGE_FEATURE_4B("SYSREG128 bits [35:32]"   , isar2, 32, Ext::kSYSREG128);
  MERGE_FEATURE_4B("SYSINSTR128 bits [39:36]" , isar2, 36, Ext::kSYSINSTR128);
  MERGE_FEATURE_4B("PRFMSLC bits [43:40]"     , isar2, 40, Ext::kPRFMSLC);
  MERGE_FEATURE_4B("RPRFM bits [51:48]"       , isar2, 48, Ext::kRPRFM);
  MERGE_FEATURE_4B("CSSC bits [55:52]"        , isar2, 52, Ext::kCSSC);
  MERGE_FEATURE_4B("LUT bits [59:56]"         , isar2, 56, Ext::kLUT);
}

// TODO: This register is not accessed at the moment.
#if 0
// Detects features based on the content of ID_AA64ISAR3_EL1register.
ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64ISAR3(CpuInfo& cpu, uint64_t isar3) noexcept {
  // ID_AA64ISAR3_EL1
  // ================

  MERGE_FEATURE_4B("CPA bits [3:0]"           , isar3, 0, Ext::kCPA, Ext::kCPA2);
  MERGE_FEATURE_4B("FAMINMAX bits [7:4]"      , isar3, 4, Ext::kFAMINMAX);
  MERGE_FEATURE_4B("TLBIW bits [11:8]"        , isar3, 8, Ext::kTLBIW);
}
#endif

ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64MMFR0(CpuInfo& cpu, uint64_t mmfr0) noexcept {
  // ID_AA64MMFR0_EL1
  // ================

  /*
  MERGE_FEATURE_4B("PARange bits [3:0]"       , mmfr0,  0, ...);
  MERGE_FEATURE_4B("ASIDBits bits [7:4]"      , mmfr0,  4, ...);
  MERGE_FEATURE_4B("BigEnd bits [11:8]"       , mmfr0,  8, ...);
  MERGE_FEATURE_4B("SNSMem bits [15:12]"      , mmfr0, 12, ...);
  MERGE_FEATURE_4B("BigEndEL0 bits [19:16]"   , mmfr0, 16, ...);
  MERGE_FEATURE_4B("TGran16 bits [23:20]"     , mmfr0, 20, ...);
  MERGE_FEATURE_4B("TGran64 bits [27:24]"     , mmfr0, 24, ...);
  MERGE_FEATURE_4B("TGran4 bits [31:28]"      , mmfr0, 28, ...);
  MERGE_FEATURE_4B("TGran16_2 bits [35:32]"   , mmfr0, 32, ...);
  MERGE_FEATURE_4B("TGran64_2 bits [39:36]"   , mmfr0, 36, ...);
  MERGE_FEATURE_4B("TGran4_2 bits [43:40]"    , mmfr0, 40, ...);
  MERGE_FEATURE_4B("ExS bits [47:44]"         , mmfr0, 44, ...);
  */
  MERGE_FEATURE_4B("FGT bits [59:56]"         , mmfr0, 56, Ext::kFGT, Ext::kFGT2);
  MERGE_FEATURE_4B("ECV bits [63:60]"         , mmfr0, 60, Ext::kECV);
}

ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64MMFR1(CpuInfo& cpu, uint64_t mmfr1) noexcept {
  // ID_AA64MMFR1_EL1
  // ================

  MERGE_FEATURE_4B("HAFDBS bits [3:0]"        , mmfr1,  0, Ext::kHAFDBS, Ext::kNone, Ext::kHAFT, Ext::kHDBSS);
  MERGE_FEATURE_4B("VMIDBits bits [7:4]"      , mmfr1,  4, Ext::kVMID16);
  MERGE_FEATURE_4B("VH bits [11:8]"           , mmfr1,  8, Ext::kVHE);
  MERGE_FEATURE_4B("HPDS bits [15:12]"        , mmfr1, 12, Ext::kHPDS, Ext::kHPDS2);
  MERGE_FEATURE_4B("LO bits [19:16]"          , mmfr1, 16, Ext::kLOR);
  MERGE_FEATURE_4B("PAN bits [23:20]"         , mmfr1, 20, Ext::kPAN, Ext::kPAN2, Ext::kPAN3);
  /*
  MERGE_FEATURE_4B("SpecSEI bits [27:24]"     , mmfr1, 24, ...);
  */
  MERGE_FEATURE_4B("XNX bits [31:28]"         , mmfr1, 28, Ext::kXNX);
  /*
  MERGE_FEATURE_4B("TWED bits [35:32]"        , mmfr1, 32, ...);
  MERGE_FEATURE_4B("ETS bits [39:36]"         , mmfr1, 36, ...);
  */
  MERGE_FEATURE_4B("HCX bits [43:40]"         , mmfr1, 40, Ext::kHCX);
  MERGE_FEATURE_4B("AFP bits [47:44]"         , mmfr1, 44, Ext::kAFP);
  /*
  MERGE_FEATURE_4B("nTLBPA bits [51:48]"      , mmfr1, 48, ...);
  MERGE_FEATURE_4B("TIDCP1 bits [55:52]"      , mmfr1, 52, ...);
  */
  MERGE_FEATURE_4B("CMOW bits [59:56]"        , mmfr1, 56, Ext::kCMOW);
  MERGE_FEATURE_4B("ECBHB bits [63:60]"       , mmfr1, 60, Ext::kECBHB);
}

ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64MMFR2(CpuInfo& cpu, uint64_t mmfr2) noexcept {
  // ID_AA64MMFR2_EL1
  // ================

  /*
  MERGE_FEATURE_4B("CnP bits [3:0]"           , mmfr2,  0, ...);
  */
  MERGE_FEATURE_4B("UAO bits [7:4]"           , mmfr2,  4, Ext::kUAO);
  /*
  MERGE_FEATURE_4B("LSM bits [11:8]"          , mmfr2,  8, ...);
  MERGE_FEATURE_4B("IESB bits [15:12]"        , mmfr2, 12, ...);
  */
  MERGE_FEATURE_4B("VARange bits [19:16]"     , mmfr2, 16, Ext::kLVA, Ext::kLVA3);
  MERGE_FEATURE_4B("CCIDX bits [23:20]"       , mmfr2, 20, Ext::kCCIDX);
  MERGE_FEATURE_4B("NV bits [27:24]"          , mmfr2, 24, Ext::kNV, Ext::kNV2);
  /*
  MERGE_FEATURE_4B("ST bits [31:28]"          , mmfr2, 28, ...);
  */
  MERGE_FEATURE_4B("AT bits [35:32]"          , mmfr2, 32, Ext::kLSE2);
  /*
  MERGE_FEATURE_4B("IDS bits [39:36]"         , mmfr2, 36, ...);
  MERGE_FEATURE_4B("FWB bits [43:40]"         , mmfr2, 40, ...);
  MERGE_FEATURE_4B("TTL bits [51:48]"         , mmfr2, 48, ...);
  MERGE_FEATURE_4B("BBM bits [55:52]"         , mmfr2, 52, ...);
  MERGE_FEATURE_4B("EVT bits [59:56]"         , mmfr2, 56, ...);
  MERGE_FEATURE_4B("E0PD bits [63:60]"        , mmfr2, 60, ...);
  */
}

// Detects features based on the content of ID_AA64ZFR0_EL1 register.
ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64ZFR0(CpuInfo& cpu, uint64_t zfr0) noexcept {
  MERGE_FEATURE_4B("SVEver bits [3:0]"        , zfr0,  0, Ext::kSVE2, Ext::kSVE2_1);
  MERGE_FEATURE_4B("AES bits [7:4]"           , zfr0,  4, Ext::kSVE_AES, Ext::kSVE_PMULL128);
  MERGE_FEATURE_4B("BitPerm bits [19:16]"     , zfr0, 16, Ext::kSVE_BITPERM);
  MERGE_FEATURE_4B("BF16 bits [23:20]"        , zfr0, 20, Ext::kSVE_BF16, Ext::kSVE_EBF16);
  MERGE_FEATURE_4B("B16B16 bits [27:24]"      , zfr0, 24, Ext::kSVE_B16B16);
  MERGE_FEATURE_4B("SHA3 bits [35:32]"        , zfr0, 32, Ext::kSVE_SHA3);
  MERGE_FEATURE_4B("SM4 bits [43:40]"         , zfr0, 40, Ext::kSVE_SM4);
  MERGE_FEATURE_4B("I8MM bits [47:44]"        , zfr0, 44, Ext::kSVE_I8MM);
  MERGE_FEATURE_4B("F32MM bits [55:52]"       , zfr0, 52, Ext::kSVE_F32MM);
  MERGE_FEATURE_4B("F64MM bits [59:56]"       , zfr0, 56, Ext::kSVE_F64MM);
}

ASMJIT_MAYBE_UNUSED
static inline void detectAArch64FeaturesViaCPUID_AA64SMFR0(CpuInfo& cpu, uint64_t smfr0) noexcept {
  MERGE_FEATURE_1B("SF8DP2 bit [28]"          , smfr0, 29, Ext::kSSVE_FP8DOT2);
  MERGE_FEATURE_1B("SF8DP4 bit [29]"          , smfr0, 29, Ext::kSSVE_FP8DOT4);
  MERGE_FEATURE_1B("SF8FMA bit [30]"          , smfr0, 30, Ext::kSSVE_FP8FMA);
  MERGE_FEATURE_1B("F32F32 bit [32]"          , smfr0, 32, Ext::kSME_F32F32);
  MERGE_FEATURE_1B("BI32I32 bit [33]"         , smfr0, 33, Ext::kSME_BI32I32);
  MERGE_FEATURE_1B("B16F32 bit [34]"          , smfr0, 34, Ext::kSME_B16F32);
  MERGE_FEATURE_1B("F16F32 bit [35]"          , smfr0, 35, Ext::kSME_F16F32);
  MERGE_FEATURE_4S("I8I32 bits [39:36]"       , smfr0, 36, 0xF, Ext::kSME_I8I32);
  MERGE_FEATURE_1B("F8F32 bit [40]"           , smfr0, 40, Ext::kSME_F8F32);
  MERGE_FEATURE_1B("F8F16 bit [41]"           , smfr0, 41, Ext::kSME_F8F16);
  MERGE_FEATURE_1B("F16F16 bit [42]"          , smfr0, 42, Ext::kSME_F16F16);
  MERGE_FEATURE_1B("B16B16 bit [43]"          , smfr0, 43, Ext::kSME_B16B16);
  MERGE_FEATURE_4S("I16I32 bits [47:44]"      , smfr0, 44, 0x5, Ext::kSME_I16I32);
  MERGE_FEATURE_1B("F64F64 bit [48]"          , smfr0, 48, Ext::kSME_F64F64);
  MERGE_FEATURE_4S("I16I64 bits [55:52]"      , smfr0, 52, 0xF, Ext::kSME_I16I64);
  MERGE_FEATURE_4B("SMEver bits [59:56]"      , smfr0, 56, Ext::kSME2, Ext::kSME2_1);
  MERGE_FEATURE_1B("LUTv2 bit [60]"           , smfr0, 60, Ext::kSME_LUTv2);
  MERGE_FEATURE_1B("FA64 bit [63]"            , smfr0, 63, Ext::kSME_FA64);
}

#undef MERGE_FEATURE_4S
#undef MERGE_FEATURE_4B
#undef MERGE_FEATURE_2B
#undef MERGE_FEATURE_1B
#undef MERGE_FEATURE_NA

// CpuInfo - Detect - ARM - CPU Vendor Features
// ============================================

// CPU features detection based on Apple family ID.
enum class AppleFamilyId : uint32_t {
  // Apple design.
  kSWIFT              = 0x1E2D6381u, // Apple A6/A6X (ARMv7s).
  kCYCLONE            = 0x37A09642u, // Apple A7 (ARMv8.0-A).
  kTYPHOON            = 0x2C91A47Eu, // Apple A8 (ARMv8.0-A).
  kTWISTER            = 0x92FB37C8u, // Apple A9 (ARMv8.0-A).
  kHURRICANE          = 0x67CEEE93u, // Apple A10 (ARMv8.1-A).
  kMONSOON_MISTRAL    = 0xE81E7EF6u, // Apple A11 (ARMv8.2-A).
  kVORTEX_TEMPEST     = 0x07D34B9Fu, // Apple A12 (ARMv8.3-A).
  kLIGHTNING_THUNDER  = 0x462504D2u, // Apple A13 (ARMv8.4-A).
  kFIRESTORM_ICESTORM = 0x1B588BB3u, // Apple A14/M1 (ARMv8.5-A).
  kAVALANCHE_BLIZZARD = 0XDA33D83Du, // Apple A15/M2.
  kEVEREST_SAWTOOTH   = 0X8765EDEAu  // Apple A16.
};

ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE bool detectARMFeaturesViaAppleFamilyId(CpuInfo& cpu) noexcept {
  typedef AppleFamilyId Id;
  CpuFeatures::ARM& features = cpu.features().arm();

  switch (cpu.familyId()) {
    // Apple A7-A9 (ARMv8.0-A).
    case uint32_t(Id::kCYCLONE):
    case uint32_t(Id::kTYPHOON):
    case uint32_t(Id::kTWISTER):
      populateARMv8AFeatures(features, 0);
      features.add(Ext::kAES, Ext::kPMU, Ext::kPMULL, Ext::kSHA1, Ext::kSHA256);
      return true;

    // Apple A10 (ARMv8.0-A).
    case uint32_t(Id::kHURRICANE):
      populateARMv8AFeatures(features, 0);
      features.add(Ext::kAES, Ext::kCRC32, Ext::kLOR, Ext::kPAN, Ext::kPMU, Ext::kPMULL, Ext::kRDM, Ext::kSHA1,
                   Ext::kSHA256, Ext::kVHE);
      return true;

    // Apple A11 (ARMv8.2-A).
    case uint32_t(Id::kMONSOON_MISTRAL):
      populateARMv8AFeatures(features, 2);
      features.add(Ext::kAES, Ext::kFP16, Ext::kFP16CONV, Ext::kPMU, Ext::kPMULL, Ext::kSHA1, Ext::kSHA256);
      return true;

    // Apple A12 (ARMv8.3-A).
    case uint32_t(Id::kVORTEX_TEMPEST):
      populateARMv8AFeatures(features, 3);
      features.add(Ext::kAES, Ext::kFP16, Ext::kFP16CONV, Ext::kPMU, Ext::kPMULL, Ext::kSHA1, Ext::kSHA256);
      return true;

    // Apple A13 (ARMv8.4-A).
    case uint32_t(Id::kLIGHTNING_THUNDER):
      populateARMv8AFeatures(features, 4);
      features.add(Ext::kAES, Ext::kFHM, Ext::kFP16, Ext::kFP16CONV, Ext::kPMU, Ext::kPMULL, Ext::kSHA1,
                   Ext::kSHA256, Ext::kSHA3, Ext::kSHA512);
      return true;

    // Apple A14/M1 (ARMv8.5-A).
    case uint32_t(Id::kFIRESTORM_ICESTORM):
      populateARMv8AFeatures(features, 4);
      features.add(Ext::kAES, Ext::kCSV2, Ext::kCSV3, Ext::kDPB2, Ext::kECV, Ext::kFHM, Ext::kFLAGM2,
                   Ext::kFP16, Ext::kFP16CONV, Ext::kFRINTTS, Ext::kPMU, Ext::kPMULL, Ext::kSB,
                   Ext::kSHA1, Ext::kSHA256, Ext::kSHA3, Ext::kSHA512, Ext::kSSBS);
      return true;

    // Apple A15/M2.
    case uint32_t(Id::kAVALANCHE_BLIZZARD):
      populateARMv8AFeatures(features, 6);
      features.add(Ext::kAES, Ext::kFHM, Ext::kFP16, Ext::kFP16CONV, Ext::kPMU, Ext::kPMULL, Ext::kSHA1,
                   Ext::kSHA256, Ext::kSHA3, Ext::kSHA512);
      return true;

    // Apple A16.
    case uint32_t(Id::kEVEREST_SAWTOOTH):
      populateARMv8AFeatures(features, 6);
      features.add(Ext::kAES, Ext::kFHM, Ext::kFP16, Ext::kFP16CONV, Ext::kHCX, Ext::kPMU, Ext::kPMULL,
                   Ext::kSHA1, Ext::kSHA256, Ext::kSHA3, Ext::kSHA512);
      return true;

    default:
      return false;
  }
}

// CpuInfo - Detect - ARM - Compile Flags Features
// ===============================================

// Detects ARM version by macros defined at compile time. This means that AsmJit will report features forced at
// compile time that should always be provided by the target CPU. This also means that if we don't provide any
// means to detect CPU features the features reported by AsmJit will at least not report less features than the
// target it was compiled to.

#if ASMJIT_ARCH_ARM == 32
ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void detectAArch32FeaturesViaCompilerFlags(CpuInfo& cpu) noexcept {
  DebugUtils::unused(cpu);

  // ARM targets have no baseline at the moment.
#if defined(__ARM_ARCH_7A__)
  cpu.addFeature(CpuFeatures::ARM::kARMv7);
#endif

#if defined(__ARM_ARCH_8A__)
  cpu.addFeature(CpuFeatures::ARM::kARMv8a);
#endif

#if defined(__TARGET_ARCH_THUMB)
  cpu.addFeature(CpuFeatures::ARM::kTHUMB);
#if __TARGET_ARCH_THUMB >= 4
  cpu.addFeature(CpuFeatures::ARM::kTHUMBv2);
#endif
#endif

#if defined(__ARM_FEATURE_FMA)
  cpu.addFeature(Ext::kFP);
#endif

#if defined(__ARM_NEON)
  cpu.addFeature(Ext::kASIMD);
#endif

#if defined(__ARM_FEATURE_IDIV) && defined(__TARGET_ARCH_THUMB)
  cpu.addFeature(Ext::kIDIVT);
#endif
#if defined(__ARM_FEATURE_IDIV) && !defined(__TARGET_ARCH_THUMB)
  cpu.addFeature(Ext::kIDIVA);
#endif
}
#endif // ASMJIT_ARCH_ARM == 32

#if ASMJIT_ARCH_ARM == 64
ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void detectAArch64FeaturesViaCompilerFlags(CpuInfo& cpu) noexcept {
  DebugUtils::unused(cpu);

#if defined(__ARM_ARCH_9_5A__)
  populateARMv9AFeatures(cpu.features().arm(), 5);
#elif defined(__ARM_ARCH_9_4A__)
  populateARMv9AFeatures(cpu.features().arm(), 4);
#elif defined(__ARM_ARCH_9_3A__)
  populateARMv9AFeatures(cpu.features().arm(), 3);
#elif defined(__ARM_ARCH_9_2A__)
  populateARMv9AFeatures(cpu.features().arm(), 2);
#elif defined(__ARM_ARCH_9_1A__)
  populateARMv9AFeatures(cpu.features().arm(), 1);
#elif defined(__ARM_ARCH_9A__)
  populateARMv9AFeatures(cpu.features().arm(), 0);
#elif defined(__ARM_ARCH_8_9A__)
  populateARMv8AFeatures(cpu.features().arm(), 9);
#elif defined(__ARM_ARCH_8_8A__)
  populateARMv8AFeatures(cpu.features().arm(), 8);
#elif defined(__ARM_ARCH_8_7A__)
  populateARMv8AFeatures(cpu.features().arm(), 7);
#elif defined(__ARM_ARCH_8_6A__)
  populateARMv8AFeatures(cpu.features().arm(), 6);
#elif defined(__ARM_ARCH_8_5A__)
  populateARMv8AFeatures(cpu.features().arm(), 5);
#elif defined(__ARM_ARCH_8_4A__)
  populateARMv8AFeatures(cpu.features().arm(), 4);
#elif defined(__ARM_ARCH_8_3A__)
  populateARMv8AFeatures(cpu.features().arm(), 3);
#elif defined(__ARM_ARCH_8_2A__)
  populateARMv8AFeatures(cpu.features().arm(), 2);
#elif defined(__ARM_ARCH_8_1A__)
  populateARMv8AFeatures(cpu.features().arm(), 1);
#else
  populateARMv8AFeatures(cpu.features().arm(), 0);
#endif

#if defined(__ARM_FEATURE_AES)
  cpu.addFeature(Ext::kAES);
#endif

#if defined(__ARM_FEATURE_BF16_SCALAR_ARITHMETIC) && defined(__ARM_FEATURE_BF16_VECTOR_ARITHMETIC)
  cpu.addFeature(Ext::kBF16);
#endif

#if defined(__ARM_FEATURE_CRC32)
  cpu.addFeature(Ext::kCRC32);
#endif

#if defined(__ARM_FEATURE_CRYPTO)
  cpu.addFeature(Ext::kAES, Ext::kSHA1, Ext::kSHA256);
#endif

#if defined(__ARM_FEATURE_DOTPROD)
  cpu.addFeature(Ext::kDOTPROD);
#endif

#if defined(__ARM_FEATURE_FP16FML) || defined(__ARM_FEATURE_FP16_FML)
  cpu.addFeature(Ext::kFHM);
#endif

#if defined(__ARM_FEATURE_FP16_SCALAR_ARITHMETIC)
  cpu.addFeature(Ext::kFP16);
#endif

#if defined(__ARM_FEATURE_FRINT)
  cpu.addFeature(Ext::kFRINTTS);
#endif

#if defined(__ARM_FEATURE_JCVT)
  cpu.addFeature(Ext::kJSCVT);
#endif

#if defined(__ARM_FEATURE_MATMUL_INT8)
  cpu.addFeature(Ext::kI8MM);
#endif

#if defined(__ARM_FEATURE_ATOMICS)
  cpu.addFeature(Ext::kLSE);
#endif

#if defined(__ARM_FEATURE_MEMORY_TAGGING)
  cpu.addFeature(Ext::kMTE);
#endif

#if defined(__ARM_FEATURE_QRDMX)
  cpu.addFeature(Ext::kRDM);
#endif

#if defined(__ARM_FEATURE_RNG)
  cpu.addFeature(Ext::kRNG);
#endif

#if defined(__ARM_FEATURE_SHA2)
  cpu.addFeature(Ext::kSHA256);
#endif

#if defined(__ARM_FEATURE_SHA3)
  cpu.addFeature(Ext::kSHA3);
#endif

#if defined(__ARM_FEATURE_SHA512)
  cpu.addFeature(Ext::kSHA512);
#endif

#if defined(__ARM_FEATURE_SM3)
  cpu.addFeature(Ext::kSM3);
#endif

#if defined(__ARM_FEATURE_SM4)
  cpu.addFeature(Ext::kSM4);
#endif

#if defined(__ARM_FEATURE_SVE) || defined(__ARM_FEATURE_SVE_VECTOR_OPERATORS)
  cpu.addFeature(Ext::kSVE);
#endif

#if defined(__ARM_FEATURE_SVE_MATMUL_INT8)
  cpu.addFeature(Ext::kSVE_I8MM);
#endif

#if defined(__ARM_FEATURE_SVE_MATMUL_FP32)
  cpu.addFeature(Ext::kSVE_F32MM);
#endif

#if defined(__ARM_FEATURE_SVE_MATMUL_FP64)
  cpu.addFeature(Ext::kSVE_F64MM);
#endif

#if defined(__ARM_FEATURE_SVE2)
  cpu.addFeature(Ext::kSVE2);
#endif

#if defined(__ARM_FEATURE_SVE2_AES)
  cpu.addFeature(Ext::kSVE_AES);
#endif

#if defined(__ARM_FEATURE_SVE2_BITPERM)
  cpu.addFeature(Ext::kSVE_BITPERM);
#endif

#if defined(__ARM_FEATURE_SVE2_SHA3)
  cpu.addFeature(Ext::kSVE_SHA3);
#endif

#if defined(__ARM_FEATURE_SVE2_SM4)
  cpu.addFeature(Ext::kSVE_SM4);
#endif

#if defined(__ARM_FEATURE_TME)
  cpu.addFeature(Ext::kTME);
#endif
}
#endif // ASMJIT_ARCH_ARM == 64

ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void detectARMFeaturesViaCompilerFlags(CpuInfo& cpu) noexcept {
#if ASMJIT_ARCH_ARM == 32
  detectAArch32FeaturesViaCompilerFlags(cpu);
#else
  detectAArch64FeaturesViaCompilerFlags(cpu);
#endif // ASMJIT_ARCH_ARM
}

// CpuInfo - Detect - ARM - Post Processing ARM Features
// =====================================================

// Postprocesses AArch32 features.
ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void postProcessAArch32Features(CpuFeatures::ARM& features) noexcept {
  DebugUtils::unused(features);
}

// Postprocesses AArch64 features.
//
// The only reason to use this function is to deduce some flags from others.
ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void postProcessAArch64Features(CpuFeatures::ARM& features) noexcept {
  if (features.hasFP16())
    features.add(Ext::kFP16CONV);

  if (features.hasMTE3())
    features.add(Ext::kMTE2);

  if (features.hasMTE2())
    features.add(Ext::kMTE);

  if (features.hasSSBS2())
    features.add(Ext::kSSBS);
}

ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void postProcessARMCpuInfo(CpuInfo& cpu) noexcept {
#if ASMJIT_ARCH_ARM == 32
  postProcessAArch32Features(cpu.features().arm());
#else
  postProcessAArch64Features(cpu.features().arm());
#endif // ASMJIT_ARCH_ARM
}

// CpuInfo - Detect - ARM - Detect by Reading CPUID Registers
// ==========================================================

// Support CPUID-based detection on AArch64.
#if defined(ASMJIT_ARM_DETECT_VIA_CPUID)

// Since the register ID is encoded with the instruction we have to create a function for each register ID to read.
#define ASMJIT_AARCH64_DEFINE_CPUID_READ_FN(func, regId)  \
ASMJIT_MAYBE_UNUSED                                       \
static inline uint64_t func() noexcept {                  \
  uint64_t output;                                        \
  __asm__ __volatile__("mrs %0, " #regId : "=r"(output)); \
  return output;                                          \
}

// NOTE: Older tools don't know the IDs. For example Ubuntu on RPI (GCC 9) won't compile ID_AA64ISAR2_EL1 in 2023.
ASMJIT_AARCH64_DEFINE_CPUID_READ_FN(aarch64ReadPFR0, ID_AA64PFR0_EL1)
ASMJIT_AARCH64_DEFINE_CPUID_READ_FN(aarch64ReadPFR1, ID_AA64PFR1_EL1)
ASMJIT_AARCH64_DEFINE_CPUID_READ_FN(aarch64ReadISAR0, ID_AA64ISAR0_EL1)
ASMJIT_AARCH64_DEFINE_CPUID_READ_FN(aarch64ReadISAR1, ID_AA64ISAR1_EL1)
ASMJIT_AARCH64_DEFINE_CPUID_READ_FN(aarch64ReadISAR2, S3_0_C0_C6_2) // ID_AA64ISAR2_EL1
ASMJIT_AARCH64_DEFINE_CPUID_READ_FN(aarch64ReadZFR0, S3_0_C0_C4_4) // ID_AA64ZFR0_EL1

#undef ASMJIT_AARCH64_DEFINE_CPUID_READ_FN

// Detects AArch64 features by reading CPUID bits directly from CPUID registers. This is the most reliable method
// as the OS doesn't have to know all supported extensions this way (if there is something missing in HWCAPS then
// there is no way to detect such feature without reading CPUID bits).
//
// This function uses MSR instructions, which means that it reads registers that cannot be read in user-mode. The
// OS typically implements this feature by handling SIGILL internally and providing a filtered content of these
// registers back to the user - at least this is what Linux documentation states - everything implementation
// dependent is zeroed, only the bits that are used for CPU feature identification would be present.
//
// References:
//   - https://docs.kernel.org/arch/arm64/cpu-feature-registers.html
ASMJIT_MAYBE_UNUSED
static ASMJIT_FAVOR_SIZE void detectAArch64FeaturesViaCPUID(CpuInfo& cpu) noexcept {
  populateBaseARMFeatures(cpu);

  detectAArch64FeaturesViaCPUID_AA64PFR0_AA64PFR1(cpu,
    aarch64ReadPFR0(),
    aarch64ReadPFR1());

  detectAArch64FeaturesViaCPUID_AA64ISAR0_AA64ISAR1(cpu,
    aarch64ReadISAR0(),
    aarch64ReadISAR1());

  // TODO: Fix this on FreeBSD - I don't know what kernel version allows to access the registers below...

#if defined(__linux__)
  UNameKernelVersion kVer = getUNameKernelVersion();

  // Introduced in Linux 4.19 by "arm64: add ID_AA64ISAR2_EL1 sys register"), so we want at least 4.20.
  if (kVer.atLeast(4, 20)) {
    detectAArch64FeaturesViaCPUID_AA64ISAR2(cpu, aarch64ReadISAR2());
  }

  // Introduced in Linux 5.10 by "arm64: Expose SVE2 features for userspace", so we want at least 5.11.
  if (kVer.atLeast(5, 11) && cpu.features().arm().hasAny(Ext::kSVE, Ext::kSME)) {
    // Only read CPU_ID_AA64ZFR0 when either SVE or SME is available.
    detectAArch64FeaturesViaCPUID_AA64ZFR0(cpu, aarch64ReadZFR0());
  }
#endif
}

#endif // ASMJIT_ARM_DETECT_VIA_CPUID

// CpuInfo - Detect - ARM - Detect by Windows API
// ==============================================

#if defined(_WIN32)
struct WinPFPMapping {
  uint8_t featureId;
  uint8_t pfpFeatureId;
};

static ASMJIT_FAVOR_SIZE void detectPFPFeatures(CpuInfo& cpu, const WinPFPMapping* mapping, size_t size) noexcept {
  for (size_t i = 0; i < size; i++) {
    if (::IsProcessorFeaturePresent(mapping[i].pfpFeatureId)) {
      cpu.addFeature(mapping[i].featureId);
    }
  }
}

//! Detect ARM CPU features on Windows.
//!
//! The detection is based on `IsProcessorFeaturePresent()` API call.
static ASMJIT_FAVOR_SIZE void detectARMCpu(CpuInfo& cpu) noexcept {
  cpu._wasDetected = true;
  populateBaseARMFeatures(cpu);

  CpuFeatures::ARM& features = cpu.features().arm();

  // Win32 for ARM requires ARMv7 with DSP extensions, VFPv3 (FP), and uses THUMBv2 by default.
#if ASMJIT_ARCH_ARM == 32
  features.add(Ext::kTHUMB);
  features.add(Ext::kTHUMBv2);
  features.add(Ext::kARMv6);
  features.add(Ext::kARMv7);
  features.add(Ext::kEDSP);
#endif

  // Windows for ARM requires FP and ASIMD.
  features.add(Ext::kFP);
  features.add(Ext::kASIMD);

  // Detect additional CPU features by calling `IsProcessorFeaturePresent()`.
  static const WinPFPMapping mapping[] = {
#if ASMJIT_ARCH_ARM == 32
    { uint8_t(Ext::kVFP_D32)  , 18 }, // PF_ARM_VFP_32_REGISTERS_AVAILABLE
    { uint8_t(Ext::kIDIVT)    , 24 }, // PF_ARM_DIVIDE_INSTRUCTION_AVAILABLE
    { uint8_t(Ext::kFMAC)     , 27 }, // PF_ARM_FMAC_INSTRUCTIONS_AVAILABLE
    { uint8_t(Ext::kARMv8a)   , 29 }, // PF_ARM_V8_INSTRUCTIONS_AVAILABLE
#endif
    { uint8_t(Ext::kAES)      , 30 }, // PF_ARM_V8_CRYPTO_INSTRUCTIONS_AVAILABLE
    { uint8_t(Ext::kCRC32)    , 31 }, // PF_ARM_V8_CRC32_INSTRUCTIONS_AVAILABLE
    { uint8_t(Ext::kLSE)      , 34 }, // PF_ARM_V81_ATOMIC_INSTRUCTIONS_AVAILABLE
    { uint8_t(Ext::kDOTPROD)  , 43 }, // PF_ARM_V82_DP_INSTRUCTIONS_AVAILABLE
    { uint8_t(Ext::kJSCVT)    , 44 }, // PF_ARM_V83_JSCVT_INSTRUCTIONS_AVAILABLE
    { uint8_t(Ext::kLRCPC)    , 45 }  // PF_ARM_V83_LRCPC_INSTRUCTIONS_AVAILABLE
  };
  detectPFPFeatures(cpu, mapping, ASMJIT_ARRAY_SIZE(mapping));

  // Windows can only report ARMv8A at the moment.
  if (features.hasARMv8a()) {
    populateARMv8AFeatures(cpu.features().arm(), 0);
  }

  // Windows provides several instructions under a single flag:
  if (features.hasAES()) {
    features.add(Ext::kPMULL, Ext::kSHA1, Ext::kSHA256);
  }

  postProcessARMCpuInfo(cpu);
}

// CpuInfo - Detect - ARM - Detect by Reading HWCAPS
// =================================================

#elif defined(ASMJIT_ARM_DETECT_VIA_HWCAPS)

#ifndef AT_HWCAP
  #define AT_HWCAP 16
#endif // !AT_HWCAP

#ifndef AT_HWCAP2
  #define AT_HWCAP2 26
#endif // !AT_HWCAP2

#if defined(__linux__)
static void getAuxValues(unsigned long* vals, const unsigned long* tags, size_t count) noexcept {
  for (size_t i = 0; i < count; i++) {
    vals[i] = getauxval(tags[i]);
  }
}
#elif defined(__FreeBSD__)
static void getAuxValues(unsigned long* vals, const unsigned long* tags, size_t count) noexcept {
  for (size_t i = 0; i < count; i++) {
    unsigned long result = 0;
    if (elf_aux_info(int(tags[i]), &result, int(sizeof(unsigned long))) != 0)
      result = 0;
    vals[i] = result;
  }
}
#else
#error "[asmjit] getAuxValues() - Unsupported OS."
#endif

struct HWCapMapping {
  uint8_t featureId;
  uint8_t hwCapBit;
};

static const unsigned long hwCapTags[2] = { AT_HWCAP, AT_HWCAP2 };

static ASMJIT_FAVOR_SIZE void mergeHWCaps(CpuInfo& cpu, unsigned long mask, const HWCapMapping* mapping, size_t size) noexcept {
  for (size_t i = 0; i < size; i++) {
    cpu.features().addIf(Support::bitTest(mask, mapping[i].hwCapBit), mapping[i].featureId);
  }
}

#if ASMJIT_ARCH_ARM == 32

// Reference:
//   - https://github.com/torvalds/linux/blob/master/arch/arm/include/uapi/asm/hwcap.h
static const HWCapMapping hwCapMapping[] = {
  { uint8_t(Ext::kEDSP)         , 7  }, // HWCAP_EDSP
  { uint8_t(Ext::kASIMD)        , 12 }, // HWCAP_NEON
  { uint8_t(Ext::kFP)           , 13 }, // HWCAP_VFPv3
  { uint8_t(Ext::kFMAC)         , 16 }, // HWCAP_VFPv4
  { uint8_t(Ext::kIDIVA)        , 17 }, // HWCAP_IDIVA
  { uint8_t(Ext::kIDIVT)        , 18 }, // HWCAP_IDIVT
  { uint8_t(Ext::kVFP_D32)      , 19 }, // HWCAP_VFPD32
  { uint8_t(Ext::kFP16CONV)     , 22 }, // HWCAP_FPHP
  { uint8_t(Ext::kFP16)         , 23 }, // HWCAP_ASIMDHP
  { uint8_t(Ext::kDOTPROD)      , 24 }, // HWCAP_ASIMDDP
  { uint8_t(Ext::kFHM)          , 25 }, // HWCAP_ASIMDFHM
  { uint8_t(Ext::kBF16)         , 26 }, // HWCAP_ASIMDBF16
  { uint8_t(Ext::kI8MM)         , 27 }  // HWCAP_I8MM
};

static const HWCapMapping hwCap2Mapping[] = {
  { uint8_t(Ext::kAES)          , 0  }, // HWCAP2_AES
  { uint8_t(Ext::kPMULL)        , 1  }, // HWCAP2_PMULL
  { uint8_t(Ext::kSHA1)         , 2  }, // HWCAP2_SHA1
  { uint8_t(Ext::kSHA256)       , 3  }, // HWCAP2_SHA2
  { uint8_t(Ext::kCRC32)        , 4  }, // HWCAP2_CRC32
  { uint8_t(Ext::kSB)           , 5  }, // HWCAP2_SB
  { uint8_t(Ext::kSSBS)         , 6  }  // HWCAP2_SSBS
};

static ASMJIT_FAVOR_SIZE void detectARMCpu(CpuInfo& cpu) noexcept {
  cpu._wasDetected = true;
  populateBaseARMFeatures(cpu);

  unsigned long hwCapMasks[2] {};
  getAuxValues(hwCapMasks, hwCapTags, 2u);

  mergeHWCaps(cpu, hwCapMasks[0], hwCapMapping, ASMJIT_ARRAY_SIZE(hwCapMapping));
  mergeHWCaps(cpu, hwCapMasks[1], hwCap2Mapping, ASMJIT_ARRAY_SIZE(hwCap2Mapping));

  CpuFeatures::ARM& features = cpu.features().arm();

  // ARMv7 provides FP|ASIMD.
  if (features.hasFP() || features.hasASIMD())
    features.add(CpuFeatures::ARM::kARMv7);

  // ARMv8 provives AES, CRC32, PMULL, SHA1, and SHA256.
  if (features.hasAES() || features.hasCRC32() || features.hasPMULL() || features.hasSHA1() || features.hasSHA256())
    features.add(CpuFeatures::ARM::kARMv8a);

  postProcessARMCpuInfo(cpu);
}

#else

// Reference:
//   - https://docs.kernel.org/arch/arm64/elf_hwcaps.html
//   - https://github.com/torvalds/linux/blob/master/arch/arm64/include/uapi/asm/hwcap.h
static const HWCapMapping hwCapMapping[] = {
  { uint8_t(Ext::kFP)           , 0  }, // HWCAP_FP
  { uint8_t(Ext::kASIMD)        , 1  }, // HWCAP_ASIMD
  /*
  { uint8_t(Ext::k)             , 2  }, // HWCAP_EVTSTRM
  */
  { uint8_t(Ext::kAES)          , 3  }, // HWCAP_AES
  { uint8_t(Ext::kPMULL)        , 4  }, // HWCAP_PMULL
  { uint8_t(Ext::kSHA1)         , 5  }, // HWCAP_SHA1
  { uint8_t(Ext::kSHA256)       , 6  }, // HWCAP_SHA2
  { uint8_t(Ext::kCRC32)        , 7  }, // HWCAP_CRC32
  { uint8_t(Ext::kLSE)          , 8  }, // HWCAP_ATOMICS
  { uint8_t(Ext::kFP16CONV)     , 9  }, // HWCAP_FPHP
  { uint8_t(Ext::kFP16)         , 10 }, // HWCAP_ASIMDHP
  { uint8_t(Ext::kCPUID)        , 11 }, // HWCAP_CPUID
  { uint8_t(Ext::kRDM)          , 12 }, // HWCAP_ASIMDRDM
  { uint8_t(Ext::kJSCVT)        , 13 }, // HWCAP_JSCVT
  { uint8_t(Ext::kFCMA)         , 14 }, // HWCAP_FCMA
  { uint8_t(Ext::kLRCPC)        , 15 }, // HWCAP_LRCPC
  { uint8_t(Ext::kDPB)          , 16 }, // HWCAP_DCPOP
  { uint8_t(Ext::kSHA3)         , 17 }, // HWCAP_SHA3
  { uint8_t(Ext::kSM3)          , 18 }, // HWCAP_SM3
  { uint8_t(Ext::kSM4)          , 19 }, // HWCAP_SM4
  { uint8_t(Ext::kDOTPROD)      , 20 }, // HWCAP_ASIMDDP
  { uint8_t(Ext::kSHA512)       , 21 }, // HWCAP_SHA512
  { uint8_t(Ext::kSVE)          , 22 }, // HWCAP_SVE
  { uint8_t(Ext::kFHM)          , 23 }, // HWCAP_ASIMDFHM
  { uint8_t(Ext::kDIT)          , 24 }, // HWCAP_DIT
  { uint8_t(Ext::kLSE2)         , 25 }, // HWCAP_USCAT
  { uint8_t(Ext::kLRCPC2)       , 26 }, // HWCAP_ILRCPC
  { uint8_t(Ext::kFLAGM)        , 27 }, // HWCAP_FLAGM
  { uint8_t(Ext::kSSBS)         , 28 }, // HWCAP_SSBS
  { uint8_t(Ext::kSB)           , 29 }  // HWCAP_SB
  /*
  { uint8_t(Ext::k)             , 30 }, // HWCAP_PACA
  { uint8_t(Ext::k)             , 31 }  // HWCAP_PACG
  */
};

static const HWCapMapping hwCap2Mapping[] = {
  { uint8_t(Ext::kDPB2)         , 0  }, // HWCAP2_DCPODP
  { uint8_t(Ext::kSVE2)         , 1  }, // HWCAP2_SVE2
  { uint8_t(Ext::kSVE_AES)      , 2  }, // HWCAP2_SVEAES
  { uint8_t(Ext::kSVE_PMULL128) , 3  }, // HWCAP2_SVEPMULL
  { uint8_t(Ext::kSVE_BITPERM)  , 4  }, // HWCAP2_SVEBITPERM
  { uint8_t(Ext::kSVE_SHA3)     , 5  }, // HWCAP2_SVESHA3
  { uint8_t(Ext::kSVE_SM4)      , 6  }, // HWCAP2_SVESM4
  { uint8_t(Ext::kFLAGM2)       , 7  }, // HWCAP2_FLAGM2
  { uint8_t(Ext::kFRINTTS)      , 8  }, // HWCAP2_FRINT
  { uint8_t(Ext::kSVE_I8MM)     , 9  }, // HWCAP2_SVEI8MM
  { uint8_t(Ext::kSVE_F32MM)    , 10 }, // HWCAP2_SVEF32MM
  { uint8_t(Ext::kSVE_F64MM)    , 11 }, // HWCAP2_SVEF64MM
  { uint8_t(Ext::kSVE_BF16)     , 12 }, // HWCAP2_SVEBF16
  { uint8_t(Ext::kI8MM)         , 13 }, // HWCAP2_I8MM
  { uint8_t(Ext::kBF16)         , 14 }, // HWCAP2_BF16
  { uint8_t(Ext::kDGH)          , 15 }, // HWCAP2_DGH
  { uint8_t(Ext::kRNG)          , 16 }, // HWCAP2_RNG
  { uint8_t(Ext::kBTI)          , 17 }, // HWCAP2_BTI
  { uint8_t(Ext::kMTE)          , 18 }, // HWCAP2_MTE
  { uint8_t(Ext::kECV)          , 19 }, // HWCAP2_ECV
  { uint8_t(Ext::kAFP)          , 20 }, // HWCAP2_AFP
  { uint8_t(Ext::kRPRES)        , 21 }, // HWCAP2_RPRES
  { uint8_t(Ext::kMTE3)         , 22 }, // HWCAP2_MTE3
  { uint8_t(Ext::kSME)          , 23 }, // HWCAP2_SME
  { uint8_t(Ext::kSME_I16I64)   , 24 }, // HWCAP2_SME_I16I64
  { uint8_t(Ext::kSME_F64F64)   , 25 }, // HWCAP2_SME_F64F64
  { uint8_t(Ext::kSME_I8I32)    , 26 }, // HWCAP2_SME_I8I32
  { uint8_t(Ext::kSME_F16F32)   , 27 }, // HWCAP2_SME_F16F32
  { uint8_t(Ext::kSME_B16F32)   , 28 }, // HWCAP2_SME_B16F32
  { uint8_t(Ext::kSME_F32F32)   , 29 }, // HWCAP2_SME_F32F32
  { uint8_t(Ext::kSME_FA64)     , 30 }, // HWCAP2_SME_FA64
  { uint8_t(Ext::kWFXT)         , 31 }, // HWCAP2_WFXT
  { uint8_t(Ext::kEBF16)        , 32 }, // HWCAP2_EBF16
  { uint8_t(Ext::kSVE_EBF16)    , 33 }, // HWCAP2_SVE_EBF16
  { uint8_t(Ext::kCSSC)         , 34 }, // HWCAP2_CSSC
  { uint8_t(Ext::kRPRFM)        , 35 }, // HWCAP2_RPRFM
  { uint8_t(Ext::kSVE2_1)       , 36 }, // HWCAP2_SVE2P1
  { uint8_t(Ext::kSME2)         , 37 }, // HWCAP2_SME2
  { uint8_t(Ext::kSME2_1)       , 38 }, // HWCAP2_SME2P1
  { uint8_t(Ext::kSME_I16I32)   , 39 }, // HWCAP2_SME_I16I32
  { uint8_t(Ext::kSME_BI32I32)  , 40 }, // HWCAP2_SME_BI32I32
  { uint8_t(Ext::kSME_B16B16)   , 41 }, // HWCAP2_SME_B16B16
  { uint8_t(Ext::kSME_F16F16)   , 42 }, // HWCAP2_SME_F16F16
  { uint8_t(Ext::kMOPS)         , 43 }, // HWCAP2_MOPS
  { uint8_t(Ext::kHBC)          , 44 }, // HWCAP2_HBC
  { uint8_t(Ext::kSVE_B16B16)   , 45 }, // HWCAP2_SVE_B16B16
  { uint8_t(Ext::kLRCPC3)       , 46 }, // HWCAP2_LRCPC3
  { uint8_t(Ext::kLSE128)       , 47 }, // HWCAP2_LSE128
};

static ASMJIT_FAVOR_SIZE void detectARMCpu(CpuInfo& cpu) noexcept {
  cpu._wasDetected = true;
  populateBaseARMFeatures(cpu);

  unsigned long hwCapMasks[2] {};
  getAuxValues(hwCapMasks, hwCapTags, 2u);

  mergeHWCaps(cpu, hwCapMasks[0], hwCapMapping, ASMJIT_ARRAY_SIZE(hwCapMapping));
  mergeHWCaps(cpu, hwCapMasks[1], hwCap2Mapping, ASMJIT_ARRAY_SIZE(hwCap2Mapping));

#if defined(ASMJIT_ARM_DETECT_VIA_CPUID)
  if (cpu.features().arm().hasCPUID()) {
    detectAArch64FeaturesViaCPUID(cpu);
    return;
  }
#endif // ASMJIT_ARM_DETECT_VIA_CPUID

  postProcessARMCpuInfo(cpu);
}

#endif // ASMJIT_ARCH_ARM

// CpuInfo - Detect - ARM - Detect by NetBSD API That Reads CPUID
// ==============================================================

#elif defined(__NetBSD__) && ASMJIT_ARCH_ARM >= 64

//! Position of AArch64 registers in a`aarch64_sysctl_cpu_id` struct, which is filled by sysctl().
struct NetBSDAArch64Regs {
  enum ID : uint32_t {
    k64_MIDR      = 0,    //!< Main ID Register.
    k64_REVIDR    = 8,    //!< Revision ID Register.
    k64_MPIDR     = 16,   //!< Multiprocessor Affinity Register.
    k64_AA64DFR0  = 24,   //!< A64 Debug Feature Register 0.
    k64_AA64DFR1  = 32,   //!< A64 Debug Feature Register 1.
    k64_AA64ISAR0 = 40,   //!< A64 Instruction Set Attribute Register 0.
    k64_AA64ISAR1 = 48,   //!< A64 Instruction Set Attribute Register 1.
    k64_AA64MMFR0 = 56,   //!< A64 Memory Model Feature Register 0.
    k64_AA64MMFR1 = 64,   //!< A64 Memory Model Feature Register 1.
    k64_AA64MMFR2 = 72,   //!< A64 Memory Model Feature Register 2.
    k64_AA64PFR0  = 80,   //!< A64 Processor Feature Register 0.
    k64_AA64PFR1  = 88,   //!< A64 Processor Feature Register 1.
    k64_AA64ZFR0  = 96,   //!< A64 SVE Feature ID Register 0.
    k32_MVFR0     = 104,  //!< Media and VFP Feature Register 0.
    k32_MVFR1     = 108,  //!< Media and VFP Feature Register 1.
    k32_MVFR2     = 112,  //!< Media and VFP Feature Register 2.
    k32_PAD       = 116,  //!< Padding (not used).
    k64_CLIDR     = 120,  //!< Cache Level ID Register.
    k64_CTR       = 128   //!< Cache Type Register.
  };

  enum Limits : uint32_t {
    kBufferSize   = 136
  };

  uint64_t data[kBufferSize / 8u];

  ASMJIT_INLINE_NODEBUG uint64_t r64(uint32_t index) const noexcept {
    ASMJIT_ASSERT(index % 8u == 0u);
    return data[index / 8u];
  }

  ASMJIT_INLINE_NODEBUG uint32_t r32(uint32_t index) const noexcept {
    ASMJIT_ASSERT(index % 4u == 0u);
    uint32_t shift = (index % 8) * 8;
    return uint32_t((r64(index) >> shift) & 0xFFFFFFFFu);
  }
};

static ASMJIT_FAVOR_SIZE void detectARMCpu(CpuInfo& cpu) noexcept {
  using Regs = NetBSDAArch64Regs;

  populateBaseARMFeatures(cpu);

  Regs regs {};
  size_t len = sizeof(regs);
  const char sysctlCpuPath[] = "machdep.cpu0.cpu_id";

  if (sysctlbyname(sysctlCpuPath, &regs, &len, nullptr, 0) == 0) {
    detectAArch64FeaturesViaCPUID_AA64PFR0_AA64PFR1(cpu,
      regs.r64(Regs::k64_AA64PFR0),
      regs.r64(Regs::k64_AA64PFR1));

    detectAArch64FeaturesViaCPUID_AA64ISAR0_AA64ISAR1(cpu,
      regs.r64(Regs::k64_AA64ISAR0),
      regs.r64(Regs::k64_AA64ISAR1));

    // TODO: AA64ISAR2 should be added when it's provided by NetBSD.
    // detectAArch64FeaturesViaCPUID_AA64ISAR2(cpu, regs.r64Regs::k64_AA64ISAR2));

    detectAArch64FeaturesViaCPUID_AA64MMFR0(cpu, regs.r64(Regs::k64_AA64MMFR0));
    detectAArch64FeaturesViaCPUID_AA64MMFR1(cpu, regs.r64(Regs::k64_AA64MMFR1));
    detectAArch64FeaturesViaCPUID_AA64MMFR2(cpu, regs.r64(Regs::k64_AA64MMFR2));

    // Only read CPU_ID_AA64ZFR0 when either SVE or SME is available.
    if (cpu.features().arm().hasAny(Ext::kSVE, Ext::kSME)) {
      detectAArch64FeaturesViaCPUID_AA64ZFR0(cpu, regs.r64(Regs::k64_AA64ZFR0));

      // TODO: AA64SMFR0 should be added when it's provided by NetBSD.
      // if (cpu.features().arm().hasSME()) {
      //   detectAArch64FeaturesViaCPUID_AA64SMFR0(cpu, regs.r64(Regs::k64_kAA64SMFR0));
      // }
    }
  }

  postProcessARMCpuInfo(cpu);
}

// CpuInfo - Detect - ARM - Detect by OpenBSD API That Reads CPUID
// ===============================================================

#elif defined(__OpenBSD__) && ASMJIT_ARCH_ARM >= 64

// Supported CPUID registers on OpenBSD (CTL_MACHDEP definitions):
//   - https://github.com/openbsd/src/blob/master/sys/arch/arm64/include/cpu.h
enum class OpenBSDAArch64CPUID {
  kAA64ISAR0 = 2,
  kAA64ISAR1 = 3,
  kAA64ISAR2 = 4,
  kAA64MMFR0 = 5,
  kAA64MMFR1 = 6,
  kAA64MMFR2 = 7,
  kAA64PFR0 = 8,
  kAA64PFR1 = 9,
  kAA64SMFR0 = 10,
  kAA64ZFR0 = 11
};

static uint64_t openbsdReadAArch64CPUID(OpenBSDAArch64CPUID id) noexcept {
  uint64_t bits = 0;
  size_t size = sizeof(bits);
  int name[2] = { CTL_MACHDEP, int(id) };

  return (sysctl(name, 2, &bits, &size, NULL, 0) < 0) ? uint64_t(0) : bits;
}

static ASMJIT_FAVOR_SIZE void detectARMCpu(CpuInfo& cpu) noexcept {
  typedef OpenBSDAArch64CPUID ID;

  populateBaseARMFeatures(cpu);

  detectAArch64FeaturesViaCPUID_AA64PFR0_AA64PFR1(cpu,
    openbsdReadAArch64CPUID(ID::kAA64PFR0),
    openbsdReadAArch64CPUID(ID::kAA64PFR1));

  detectAArch64FeaturesViaCPUID_AA64ISAR0_AA64ISAR1(cpu,
    openbsdReadAArch64CPUID(ID::kAA64ISAR0),
    openbsdReadAArch64CPUID(ID::kAA64ISAR1));

  detectAArch64FeaturesViaCPUID_AA64ISAR2(cpu, openbsdReadAArch64CPUID(ID::kAA64ISAR2));
  detectAArch64FeaturesViaCPUID_AA64MMFR0(cpu, openbsdReadAArch64CPUID(ID::kAA64MMFR0));
  detectAArch64FeaturesViaCPUID_AA64MMFR1(cpu, openbsdReadAArch64CPUID(ID::kAA64MMFR1));
  detectAArch64FeaturesViaCPUID_AA64MMFR2(cpu, openbsdReadAArch64CPUID(ID::kAA64MMFR2));

  // Only read CPU_ID_AA64ZFR0 when either SVE or SME is available.
  if (cpu.features().arm().hasAny(Ext::kSVE, Ext::kSME)) {
    detectAArch64FeaturesViaCPUID_AA64ZFR0(cpu, openbsdReadAArch64CPUID(ID::kAA64ZFR0));

    if (cpu.features().arm().hasSME())
      detectAArch64FeaturesViaCPUID_AA64SMFR0(cpu, openbsdReadAArch64CPUID(ID::kAA64SMFR0));
  }

  postProcessARMCpuInfo(cpu);
}

// CpuInfo - Detect - ARM - Detect by Apple API (sysctlbyname)
// ===========================================================

#elif defined(__APPLE__)

enum class AppleFeatureType : uint8_t {
  kHWOptional,
  kHWOptionalArmFEAT
};

struct AppleFeatureMapping {
  AppleFeatureType type;
  char name[18];
  uint8_t featureId;
};

template<typename T>
static inline bool appleSysctlByName(const char* sysctlName, T* dst, size_t size = sizeof(T)) noexcept {
  return sysctlbyname(sysctlName, dst, &size, nullptr, 0) == 0;
}

static ASMJIT_FAVOR_SIZE long appleDetectARMFeatureViaSysctl(AppleFeatureType type, const char* featureName) noexcept {
  static const char hwOptionalPrefix[] = "hw.optional.";
  static const char hwOptionalArmFeatPrefix[] = "hw.optional.arm.FEAT_";

  char sysctlName[128];

  const char* prefix = type == AppleFeatureType::kHWOptional ? hwOptionalPrefix : hwOptionalArmFeatPrefix;
  size_t prefixSize = (type == AppleFeatureType::kHWOptional ? sizeof(hwOptionalPrefix) : sizeof(hwOptionalArmFeatPrefix)) - 1u;
  size_t featureNameSize = strlen(featureName);

  if (featureNameSize < 128 - prefixSize) {
    memcpy(sysctlName, prefix, prefixSize);
    memcpy(sysctlName + prefixSize, featureName, featureNameSize + 1u); // Include NULL terminator.

    long val = 0;
    if (appleSysctlByName<long>(sysctlName, &val))
      return val;
  }

  return 0;
}

static ASMJIT_FAVOR_SIZE void appleDetectARMFeaturesViaSysctl(CpuInfo& cpu) noexcept {
  typedef AppleFeatureType FT;

  // Based on:
  //   - https://developer.apple.com/documentation/kernel/1387446-sysctlbyname/determining_instruction_set_characteristics
  static const AppleFeatureMapping mappings[] = {
    // Determine Advanced SIMD and Floating Point Capabilities:
    { FT::kHWOptional       , "AdvSIMD_HPFPCvt", uint8_t(Ext::kFP16CONV)  },
    { FT::kHWOptional       , "neon_hpfp"      , uint8_t(Ext::kFP16CONV)  },
    { FT::kHWOptionalArmFEAT, "BF16"           , uint8_t(Ext::kBF16)      },
    { FT::kHWOptionalArmFEAT, "DotProd"        , uint8_t(Ext::kDOTPROD)   },
    { FT::kHWOptionalArmFEAT, "FCMA"           , uint8_t(Ext::kFCMA)      },
    { FT::kHWOptional       , "armv8_3_compnum", uint8_t(Ext::kFCMA)      },
    { FT::kHWOptionalArmFEAT, "FHM"            , uint8_t(Ext::kFHM)       },
    { FT::kHWOptional       , "armv8_2_fhm"    , uint8_t(Ext::kFHM)       },
    { FT::kHWOptionalArmFEAT, "FP16"           , uint8_t(Ext::kFP16)      },
    { FT::kHWOptional       , "neon_fp16"      , uint8_t(Ext::kFP16)      },
    { FT::kHWOptionalArmFEAT, "FRINTTS"        , uint8_t(Ext::kFRINTTS)   },
    { FT::kHWOptionalArmFEAT, "I8MM"           , uint8_t(Ext::kI8MM)      },
    { FT::kHWOptionalArmFEAT, "JSCVT"          , uint8_t(Ext::kJSCVT)     },
    { FT::kHWOptionalArmFEAT, "RDM"            , uint8_t(Ext::kRDM)       },

    // Determine Integer Capabilities:
    { FT::kHWOptional       , "armv8_crc32"    , uint8_t(Ext::kCRC32)     },
    { FT::kHWOptionalArmFEAT, "FlagM"          , uint8_t(Ext::kFLAGM)     },
    { FT::kHWOptionalArmFEAT, "FlagM2"         , uint8_t(Ext::kFLAGM2)    },

    // Determine Atomic and Memory Ordering Instruction Capabilities:
    { FT::kHWOptionalArmFEAT, "LRCPC"          , uint8_t(Ext::kLRCPC)     },
    { FT::kHWOptionalArmFEAT, "LRCPC2"         , uint8_t(Ext::kLRCPC2)    },
    { FT::kHWOptional       , "armv8_1_atomics", uint8_t(Ext::kLSE)       },
    { FT::kHWOptionalArmFEAT, "LSE"            , uint8_t(Ext::kLSE)       },
    { FT::kHWOptionalArmFEAT, "LSE2"           , uint8_t(Ext::kLSE2)      },

    // Determine Encryption Capabilities:
    { FT::kHWOptionalArmFEAT, "AES"            , uint8_t(Ext::kAES)       },
    { FT::kHWOptionalArmFEAT, "PMULL"          , uint8_t(Ext::kPMULL)     },
    { FT::kHWOptionalArmFEAT, "SHA1"           , uint8_t(Ext::kSHA1)      },
    { FT::kHWOptionalArmFEAT, "SHA256"         , uint8_t(Ext::kSHA256)    },
    { FT::kHWOptionalArmFEAT, "SHA512"         , uint8_t(Ext::kSHA512)    },
    { FT::kHWOptional       , "armv8_2_sha512" , uint8_t(Ext::kSHA512)    },
    { FT::kHWOptionalArmFEAT, "SHA3"           , uint8_t(Ext::kSHA3)      },
    { FT::kHWOptional       , "armv8_2_sha3"   , uint8_t(Ext::kSHA3)      },

    // Determine General Capabilities:
    { FT::kHWOptionalArmFEAT, "BTI"            , uint8_t(Ext::kBTI)       },
    { FT::kHWOptionalArmFEAT, "DPB"            , uint8_t(Ext::kDPB)       },
    { FT::kHWOptionalArmFEAT, "DPB2"           , uint8_t(Ext::kDPB2)      },
    { FT::kHWOptionalArmFEAT, "ECV"            , uint8_t(Ext::kECV)       },
    { FT::kHWOptionalArmFEAT, "SB"             , uint8_t(Ext::kSB)        },
    { FT::kHWOptionalArmFEAT, "SSBS"           , uint8_t(Ext::kSSBS)      }
  };

  for (size_t i = 0; i < ASMJIT_ARRAY_SIZE(mappings); i++) {
    const AppleFeatureMapping& mapping = mappings[i];
    if (!cpu.features().arm().has(mapping.featureId) && appleDetectARMFeatureViaSysctl(mapping.type, mapping.name)) {
      cpu.features().arm().add(mapping.featureId);
    }
  }
}

static ASMJIT_FAVOR_SIZE void detectARMCpu(CpuInfo& cpu) noexcept {
  cpu._wasDetected = true;
  populateBaseARMFeatures(cpu);

  appleSysctlByName<uint32_t>("hw.cpufamily", &cpu._familyId);
  appleSysctlByName<uint32_t>("hw.cachelinesize", &cpu._cacheLineSize);
  appleSysctlByName<uint32_t>("machdep.cpu.logical_per_package", &cpu._maxLogicalProcessors);
  appleSysctlByName<char>("machdep.cpu.brand_string", cpu._brand.str, sizeof(cpu._brand.str));

  memcpy(cpu._vendor.str, "APPLE", 6);

  bool cpuFeaturesPopulated = detectARMFeaturesViaAppleFamilyId(cpu);
  if (!cpuFeaturesPopulated)
    appleDetectARMFeaturesViaSysctl(cpu);
  postProcessARMCpuInfo(cpu);
}

// CpuInfo - Detect - ARM - Detect by Fallback (Using Compiler Flags)
// ==================================================================

#else

#if ASMJIT_ARCH_ARM == 32
  #pragma message("[asmjit] Disabling runtime CPU detection - unsupported OS/CPU combination (Unknown OS with AArch32 CPU)")
#else
  #pragma message("[asmjit] Disabling runtime CPU detection - unsupported OS/CPU combination (Unknown OS with AArch64 CPU)")
#endif

static ASMJIT_FAVOR_SIZE void detectARMCpu(CpuInfo& cpu) noexcept {
  populateBaseARMFeatures(cpu);
  detectARMFeaturesViaCompilerFlags(cpu);
  postProcessARMCpuInfo(cpu);
}
#endif

} // {arm}

#endif

// CpuInfo - Detect - Host
// =======================

const CpuInfo& CpuInfo::host() noexcept {
  static std::atomic<uint32_t> cpuInfoInitialized;
  static CpuInfo cpuInfoGlobal(Globals::NoInit);

  // This should never cause a problem as the resulting information should always
  // be the same. In the worst case it would just be overwritten non-atomically.
  if (!cpuInfoInitialized.load(std::memory_order_relaxed)) {
    CpuInfo cpuInfoLocal;

    cpuInfoLocal._arch = Arch::kHost;
    cpuInfoLocal._subArch = SubArch::kHost;

#if ASMJIT_ARCH_X86
    x86::detectX86Cpu(cpuInfoLocal);
#elif ASMJIT_ARCH_ARM
    arm::detectARMCpu(cpuInfoLocal);
#endif

    cpuInfoLocal._hwThreadCount = detectHWThreadCount();
    cpuInfoGlobal = cpuInfoLocal;
    cpuInfoInitialized.store(1, std::memory_order_seq_cst);
  }

  return cpuInfoGlobal;
}

#if defined(_MSC_VER)
  #pragma warning(pop)
#endif // _MSC_VER

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/cpuinfo.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_CPUINFO_H_INCLUDED
#define ASMJIT_CORE_CPUINFO_H_INCLUDED

#include "../core/archtraits.h"
#include "../core/environment.h"
#include "../core/globals.h"
#include "../core/string.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

//! CPU features information.
//!
//! Each feature is represented by a single bit in an embedded bit array.
class CpuFeatures {
public:
  //! \name Constants
  //! \{

  //! \cond INTERNAL
  enum : uint32_t {
    kMaxFeatures = 256,
    kNumBitWords = kMaxFeatures / Support::kBitWordSizeInBits
  };
  //! \endcond

  //! A word that is used to represents feature bits.
  typedef Support::BitWord BitWord;
  //! Iterator that can iterate all CPU features set.
  typedef Support::BitVectorIterator<BitWord> Iterator;

  typedef Support::Array<BitWord, kNumBitWords> Bits;

  //! \}

  //! \name Data
  //! \{

  //! CPU features data.
  struct Data {
    //! \name Members
    //! \{

    //! Data bits.
    Bits _bits;

    //! \}

    //! \name Overloaded Operators
    //! \{

    ASMJIT_INLINE_NODEBUG bool operator==(const Data& other) const noexcept { return  equals(other); }
    ASMJIT_INLINE_NODEBUG bool operator!=(const Data& other) const noexcept { return !equals(other); }

    //! \}

    //! \name Accessors
    //! \{

    //! Returns true if there are no features set.
    ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _bits.aggregate<Support::Or>(0) == 0; }

    //! Returns all features as array of bitwords (see \ref Support::BitWord).
    ASMJIT_INLINE_NODEBUG BitWord* bits() noexcept { return _bits.data(); }
    //! Returns all features as array of bitwords (const).
    ASMJIT_INLINE_NODEBUG const BitWord* bits() const noexcept { return _bits.data(); }

    //! Returns the number of BitWords returned by \ref bits().
    ASMJIT_INLINE_NODEBUG size_t bitWordCount() const noexcept { return kNumBitWords; }

    //! Returns \ref Support::BitVectorIterator, that can be used to iterate over all features efficiently.
    ASMJIT_INLINE_NODEBUG Iterator iterator() const noexcept { return Iterator(_bits.data(), kNumBitWords); }

    //! Tests whether the feature `featureId` is present.
    template<typename FeatureId>
    ASMJIT_INLINE_NODEBUG bool has(const FeatureId& featureId) const noexcept {
      ASMJIT_ASSERT(uint32_t(featureId) < kMaxFeatures);

      uint32_t idx = uint32_t(featureId) / Support::kBitWordSizeInBits;
      uint32_t bit = uint32_t(featureId) % Support::kBitWordSizeInBits;

      return bool((_bits[idx] >> bit) & 0x1);
    }

    //! \cond NONE
    template<typename FeatureId>
    ASMJIT_INLINE_NODEBUG bool hasAny(const FeatureId& featureId) const noexcept {
      return has(featureId);
    }
    //! \endcond

    //! Tests whether any feature given is present.
    //!
    //! \note This is a variadic function template that can be used with multiple features.
    template<typename FeatureId, typename... Args>
    ASMJIT_INLINE_NODEBUG bool hasAny(const FeatureId& featureId, Args&&... otherFeatureIds) const noexcept {
      return bool(unsigned(has(featureId)) | unsigned(hasAny(std::forward<Args>(otherFeatureIds)...)));
    }

    //! Tests whether all features as defined by `other` are present.
    ASMJIT_INLINE_NODEBUG bool hasAll(const Data& other) const noexcept {
      uint32_t result = 1;
      for (uint32_t i = 0; i < kNumBitWords; i++)
        result &= uint32_t((_bits[i] & other._bits[i]) == other._bits[i]);
      return bool(result);
    }

    //! \}

    //! \name Manipulation
    //! \{

    //! Clears all features set.
    ASMJIT_INLINE_NODEBUG void reset() noexcept { _bits.fill(0); }

    //! Adds the given CPU `featureId` to the list of features.
    template<typename FeatureId>
    ASMJIT_INLINE_NODEBUG void add(const FeatureId& featureId) noexcept {
      ASMJIT_ASSERT(uint32_t(featureId) < kMaxFeatures);

      uint32_t idx = uint32_t(featureId) / Support::kBitWordSizeInBits;
      uint32_t bit = uint32_t(featureId) % Support::kBitWordSizeInBits;

      _bits[idx] |= BitWord(1) << bit;
    }

    template<typename FeatureId, typename... Args>
    ASMJIT_INLINE_NODEBUG void add(const FeatureId& featureId, Args&&... otherFeatureIds) noexcept {
      add(featureId);
      add(std::forward<Args>(otherFeatureIds)...);
    }

    template<typename FeatureId>
    ASMJIT_INLINE_NODEBUG void addIf(bool condition, const FeatureId& featureId) noexcept {
      ASMJIT_ASSERT(uint32_t(featureId) < kMaxFeatures);

      uint32_t idx = uint32_t(featureId) / Support::kBitWordSizeInBits;
      uint32_t bit = uint32_t(featureId) % Support::kBitWordSizeInBits;

      _bits[idx] |= BitWord(condition) << bit;
    }

    template<typename FeatureId, typename... Args>
    ASMJIT_INLINE_NODEBUG void addIf(bool condition, const FeatureId& featureId, Args&&... otherFeatureIds) noexcept {
      addIf(condition, featureId);
      addIf(condition, std::forward<Args>(otherFeatureIds)...);
    }

    //! Removes the given CPU `featureId` from the list of features.
    template<typename FeatureId>
    ASMJIT_INLINE_NODEBUG void remove(const FeatureId& featureId) noexcept {
      ASMJIT_ASSERT(uint32_t(featureId) < kMaxFeatures);

      uint32_t idx = uint32_t(featureId) / Support::kBitWordSizeInBits;
      uint32_t bit = uint32_t(featureId) % Support::kBitWordSizeInBits;

      _bits[idx] &= ~(BitWord(1) << bit);
    }

    template<typename FeatureId, typename... Args>
    ASMJIT_INLINE_NODEBUG void remove(const FeatureId& featureId, Args&&... otherFeatureIds) noexcept {
      remove(featureId);
      remove(std::forward<Args>(otherFeatureIds)...);
    }

    //! Tests whether this CPU features data matches `other`.
    ASMJIT_INLINE_NODEBUG bool equals(const Data& other) const noexcept { return _bits == other._bits; }

    //! \}
  };

  //! X86 specific features data.
  struct X86 : public Data {
    //! X86 CPU feature identifiers.
    enum Id : uint8_t {
      // @EnumValuesBegin{"enum": "CpuFeatures::X86"}@
      kNone,                     //!< No feature (never set, used internally).

      kMT,                       //!< CPU has multi-threading capabilities.
      kNX,                       //!< CPU has Not-Execute-Bit aka DEP (data-execution prevention).
      kADX,                      //!< CPU has ADX              (multi-precision add-carry instruction extensions).
      kALTMOVCR8,                //!< CPU has LOCK MOV R<->CR0 (supports `MOV R<->CR8` via `LOCK MOV R<->CR0` in 32-bit mode) {AMD}.
      kAPX_F,                    //!< CPU has APX_F            (advanced performance extensions - 32 GP registers, REX2 prefix, ...) {X86_64}.
      kBMI,                      //!< CPU has BMI              (bit manipulation instructions #1).
      kBMI2,                     //!< CPU has BMI2             (bit manipulation instructions #2).
      kCET_IBT,                  //!< CPU has CET-IBT          (indirect branch tracking).
      kCET_SS,                   //!< CPU has CET-SS.
      kCET_SSS,                  //!< CPU has CET-SSS.
      kCLDEMOTE,                 //!< CPU has CLDEMOTE         (cache line demote).
      kCLFLUSH,                  //!< CPU has CLFUSH           (cache Line flush).
      kCLFLUSHOPT,               //!< CPU has CLFUSHOPT        (cache Line flush - optimized).
      kCLWB,                     //!< CPU has CLWB.
      kCLZERO,                   //!< CPU has CLZERO.
      kCMOV,                     //!< CPU has CMOV             (CMOV and FCMOV instructions).
      kCMPCCXADD,                //!< CPU has CMPCCXADD.
      kCMPXCHG16B,               //!< CPU has CMPXCHG16B       (compare-exchange 16 bytes) {X86_64}.
      kCMPXCHG8B,                //!< CPU has CMPXCHG8B        (compare-exchange 8 bytes).
      kENCLV,                    //!< CPU has ENCLV.
      kENQCMD,                   //!< CPU has ENQCMD           (enqueue stores).
      kERMS,                     //!< CPU has ERMS             (enhanced REP MOVSB/STOSB).
      kFSGSBASE,                 //!< CPU has FSGSBASE.
      kFSRM,                     //!< CPU has FSRM             (fast short REP MOVSB).
      kFSRC,                     //!< CPU has FSRC             (fast short REP CMPSB|SCASB).
      kFSRS,                     //!< CPU has FSRS             (fast short REP STOSB)
      kFXSR,                     //!< CPU has FXSR             (FXSAVE/FXRSTOR instructions).
      kFXSROPT,                  //!< CPU has FXSROTP          (FXSAVE/FXRSTOR is optimized).
      kFZRM,                     //!< CPU has FZRM             (fast zero-length REP MOVSB).
      kHRESET,                   //!< CPU has HRESET.
      kI486,                     //!< CPU has I486 features    (I486+ support).
      kINVLPGB,                  //!< CPU has INVLPGB.
      kLAHFSAHF,                 //!< CPU has LAHF/SAHF        (LAHF/SAHF in 64-bit mode) {X86_64}.
      kLAM,                      //!< CPU has LAM              (linear address masking) {X86_64}.
      kLWP,                      //!< CPU has LWP              (lightweight profiling) {AMD}.
      kLZCNT,                    //!< CPU has LZCNT            (LZCNT instruction).
      kMCOMMIT,                  //!< CPU has MCOMMIT          (MCOMMIT instruction).
      kMONITOR,                  //!< CPU has MONITOR          (MONITOR/MWAIT instructions).
      kMONITORX,                 //!< CPU has MONITORX         (MONITORX/MWAITX instructions).
      kMOVBE,                    //!< CPU has MOVBE            (move with byte-order swap).
      kMOVDIR64B,                //!< CPU has MOVDIR64B        (move 64 bytes as direct store).
      kMOVDIRI,                  //!< CPU has MOVDIRI          (move dword/qword as direct store).
      kMOVRS,                    //!< CPU has MOVRS            (move from shared memory).
      kMPX,                      //!< CPU has MPX              (memory protection extensions).
      kMSR,                      //!< CPU has MSR              (RDMSR/WRMSR instructions).
      kMSRLIST,                  //!< CPU has MSRLIST.
      kMSR_IMM,                  //!< CPU has MSR_IMM          (RDMSR/WRMSR immediate encoding).
      kMSSE,                     //!< CPU has MSSE             (misaligned SSE support).
      kOSXSAVE,                  //!< CPU has OSXSAVE          (XSAVE enabled by OS).
      kOSPKE,                    //!< CPU has OSPKE            (PKE enabled by OS).
      kPCONFIG,                  //!< CPU has PCONFIG          (PCONFIG instruction).
      kPOPCNT,                   //!< CPU has POPCNT           (POPCNT instruction).
      kPREFETCHI,                //!< CPU has PREFETCHI.
      kPREFETCHW,                //!< CPU has PREFETCHW.
      kPREFETCHWT1,              //!< CPU has PREFETCHWT1.
      kPTWRITE,                  //!< CPU has PTWRITE.
      kRAO_INT,                  //!< CPU has RAO_INT          (AADD, AAND, AOR, AXOR instructions).
      kRMPQUERY,                 //!< CPU has RMPQUERY         (RMPQUERY instruction).
      kRDPID,                    //!< CPU has RDPID            (RDPID instruction).
      kRDPRU,                    //!< CPU has RDPRU            (RDPRU instruction).
      kRDRAND,                   //!< CPU has RDRAND           (RDRAND instruction).
      kRDSEED,                   //!< CPU has RDSEED           (RDSEED instruction).
      kRDTSC,                    //!< CPU has RDTSC.
      kRDTSCP,                   //!< CPU has RDTSCP.
      kRTM,                      //!< CPU has RTM              (RTM instructions - deprecated).
      kSEAM,                     //!< CPU has SEAM.
      kSERIALIZE,                //!< CPU has SERIALIZE.
      kSEV,                      //!< CPU has SEV              (secure encrypted virtualization).
      kSEV_ES,                   //!< CPU has SEV_ES           (SEV encrypted state).
      kSEV_SNP,                  //!< CPU has SEV_SNP          (SEV secure nested paging).
      kSKINIT,                   //!< CPU has SKINIT           (SKINIT/STGI instructions) {AMD}.
      kSMAP,                     //!< CPU has SMAP             (supervisor-mode access prevention).
      kSME,                      //!< CPU has SME              (secure memory encryption).
      kSMEP,                     //!< CPU has SMEP             (supervisor-mode execution prevention).
      kSMX,                      //!< CPU has SMX              (safer mode extensions).
      kSVM,                      //!< CPU has SVM              (virtualization) {AMD}.
      kTBM,                      //!< CPU has TBM              (trailing bit manipulation) {AMD}.
      kTSE,                      //!< CPU has TSE.
      kTSXLDTRK,                 //!< CPU has TSXLDTRK.
      kUINTR,                    //!< CPU has UINTR            (user interrupts).
      kVMX,                      //!< CPU has VMX              (virtualization) {INTEL}.
      kWAITPKG,                  //!< CPU has WAITPKG          (UMONITOR, UMWAIT, TPAUSE).
      kWBNOINVD,                 //!< CPU has WBNOINVD.
      kWRMSRNS,                  //!< CPU has WRMSRNS.
      kXSAVE,                    //!< CPU has XSAVE.
      kXSAVEC,                   //!< CPU has XSAVEC.
      kXSAVEOPT,                 //!< CPU has XSAVEOPT.
      kXSAVES,                   //!< CPU has XSAVES.

      kFPU,                      //!< CPU has FPU              (FPU support).
      kMMX,                      //!< CPU has MMX              (MMX base instructions) (deprecated).
      kMMX2,                     //!< CPU has MMX2             (MMX2 extensions or initial SSE extensions) (deprecated).
      k3DNOW,                    //!< CPU has 3DNOW            (3DNOW base instructions) {AMD} (deprecated).
      k3DNOW2,                   //!< CPU has 3DNOW2           (enhanced 3DNOW) {AMD} (deprecated).
      kGEODE,                    //!< CPU has GEODE extensions (GEODE 3DNOW additions) (deprecated).

      kSSE,                      //!< CPU has SSE              (SSE instructions).
      kSSE2,                     //!< CPU has SSE2             (SSE2 instructions).
      kSSE3,                     //!< CPU has SSE3             (SSE3 instructions).
      kSSSE3,                    //!< CPU has SSSE3            (SSSE3 instructions).
      kSSE4_1,                   //!< CPU has SSE4.1           (SSE4.1 instructions).
      kSSE4_2,                   //!< CPU has SSE4.2           (SSE4.2 instructions).
      kSSE4A,                    //!< CPU has SSE4A            (SSE4.A instructions) {AMD} (deprecated).
      kPCLMULQDQ,                //!< CPU has PCLMULQDQ        (packed carry-less multiplication).

      kAVX,                      //!< CPU has AVX              (advanced vector extensions).
      kAVX2,                     //!< CPU has AVX2             (advanced vector extensions 2).
      kAVX_IFMA,                 //!< CPU has AVX_IFMA         (AVX/VEX encoding of vpmadd52huq/vpmadd52luq).
      kAVX_NE_CONVERT,           //!< CPU has AVX_NE_CONVERT.
      kAVX_VNNI,                 //!< CPU has AVX_VNNI         (AVX/VEX encoding of vpdpbusd/vpdpbusds/vpdpwssd/vpdpwssds).
      kAVX_VNNI_INT16,           //!< CPU has AVX_VNNI_INT16.
      kAVX_VNNI_INT8,            //!< CPU has AVX_VNNI_INT8.
      kF16C,                     //!< CPU has F16C             (AVX FP16 conversion instructions).
      kFMA,                      //!< CPU has FMA              (AVX fused-multiply-add - 3 operand form).
      kFMA4,                     //!< CPU has FMA4             (AVX fused-multiply-add - 4 operand form) (deprecated).
      kXOP,                      //!< CPU has XOP              (XOP instructions) {AMD} (deprecated).

      kAVX512_BF16,              //!< CPU has AVX512_BF16      (AVX512 BFLOAT16 support instructions).
      kAVX512_BITALG,            //!< CPU has AVX512_BITALG    (AVX512 VPOPCNT[B|W] and VPSHUFBITQMB instructions).
      kAVX512_BW,                //!< CPU has AVX512_BW        (AVX512 integer BYTE|WORD instructions).
      kAVX512_CD,                //!< CPU has AVX512_CD        (AVX512 conflict detection DWORD|QWORD instructions).
      kAVX512_DQ,                //!< CPU has AVX512_DQ        (AVX512 integer DWORD|QWORD instructions).
      kAVX512_F,                 //!< CPU has AVX512_F         (AVX512 foundation).
      kAVX512_FP16,              //!< CPU has AVX512_FP16      (AVX512 FP16 instructions).
      kAVX512_IFMA,              //!< CPU has AVX512_IFMA      (AVX512 integer fused-multiply-add using 52-bit precision).
      kAVX512_VBMI,              //!< CPU has AVX512_VBMI      (AVX512 vector byte manipulation instructions).
      kAVX512_VBMI2,             //!< CPU has AVX512_VBMI2     (AVX512 vector byte manipulation instructions v2).
      kAVX512_VL,                //!< CPU has AVX512_VL        (AVX512 vector length extensions).
      kAVX512_VNNI,              //!< CPU has AVX512_VNNI      (AVX512 vector neural network instructions).
      kAVX512_VP2INTERSECT,      //!< CPU has AVX512_VP2INTERSECT
      kAVX512_VPOPCNTDQ,         //!< CPU has AVX512_VPOPCNTDQ (AVX512 VPOPCNT[D|Q] instructions).

      kAESNI,                    //!< CPU has AESNI            (AES encode/decode instructions).
      kGFNI,                     //!< CPU has GFNI             (galois field instructions).
      kSHA,                      //!< CPU has SHA              (SHA-1 and SHA-256 instructions).
      kSHA512,                   //!< CPU has SHA512           (SHA-512 instructions).
      kSM3,                      //!< CPU has SM3              (SM3 hash extensions).
      kSM4,                      //!< CPU has SM4              (SM4 cipher extensions).
      kVAES,                     //!< CPU has VAES             (vector AES 256|512 bit support).
      kVPCLMULQDQ,               //!< CPU has VPCLMULQDQ       (vector PCLMULQDQ 256|512-bit support).

      kKL,                       //!< CPU has KL               (Key Locker).
      kAESKLE,                   //!< CPU has AESKLE           (AESKLE).
      kAESKLEWIDE_KL,            //!< CPU has AESKLE+WIDEKL+KL (AESKLE & WIDEKL instructions and KL enabled)

      kAVX10_1,                  //!< CPU has AVX10.1/512      (AVX10.1 with 512-bit vectors).
      kAVX10_2,                  //!< CPU has AVX10.2/512      (AVX10.2 with 512-bit vectors).

      kAMX_AVX512,               //!< CPU has AMX_AVX512       (AMX-AVX512 instructions).
      kAMX_BF16,                 //!< CPU has AMX_BF16         (AMX-BF16 instructions).
      kAMX_COMPLEX,              //!< CPU has AMX_COMPLEX      (AMX-COMPLEX instructions).
      kAMX_FP16,                 //!< CPU has AMX_FP16         (AMX-FP16 instructions).
      kAMX_FP8,                  //!< CPU has AMX_FP8          (AMX-FP8 instructions).
      kAMX_INT8,                 //!< CPU has AMX_INT8         (AMX-INT8 instructions).
      kAMX_MOVRS,                //!< CPU has AMX_MOVRS        (AMX-MOVRS instructions).
      kAMX_TF32,                 //!< CPU has AMX_TF32         (AMX-TF32 instructions).
      kAMX_TILE,                 //!< CPU has AMX_TILE         (advanced matrix extensions).
      kAMX_TRANSPOSE,            //!< CPU has AMX_TRANSPOSE    (AMX-TRANSPOSE instructions).
      // @EnumValuesEnd@

      kMaxValue = kAMX_TILE
    };

    #define ASMJIT_X86_FEATURE(FEATURE) \
      /*! Tests whether FEATURE is present. */ \
      ASMJIT_INLINE_NODEBUG bool has##FEATURE() const noexcept { return has(X86::k##FEATURE); }

    ASMJIT_X86_FEATURE(MT)
    ASMJIT_X86_FEATURE(NX)
    ASMJIT_X86_FEATURE(ADX)
    ASMJIT_X86_FEATURE(ALTMOVCR8)
    ASMJIT_X86_FEATURE(APX_F)
    ASMJIT_X86_FEATURE(BMI)
    ASMJIT_X86_FEATURE(BMI2)
    ASMJIT_X86_FEATURE(CET_IBT)
    ASMJIT_X86_FEATURE(CET_SS)
    ASMJIT_X86_FEATURE(CET_SSS)
    ASMJIT_X86_FEATURE(CLDEMOTE)
    ASMJIT_X86_FEATURE(CLFLUSH)
    ASMJIT_X86_FEATURE(CLFLUSHOPT)
    ASMJIT_X86_FEATURE(CLWB)
    ASMJIT_X86_FEATURE(CLZERO)
    ASMJIT_X86_FEATURE(CMOV)
    ASMJIT_X86_FEATURE(CMPXCHG16B)
    ASMJIT_X86_FEATURE(CMPXCHG8B)
    ASMJIT_X86_FEATURE(ENCLV)
    ASMJIT_X86_FEATURE(ENQCMD)
    ASMJIT_X86_FEATURE(ERMS)
    ASMJIT_X86_FEATURE(FSGSBASE)
    ASMJIT_X86_FEATURE(FSRM)
    ASMJIT_X86_FEATURE(FSRC)
    ASMJIT_X86_FEATURE(FSRS)
    ASMJIT_X86_FEATURE(FXSR)
    ASMJIT_X86_FEATURE(FXSROPT)
    ASMJIT_X86_FEATURE(FZRM)
    ASMJIT_X86_FEATURE(HRESET)
    ASMJIT_X86_FEATURE(I486)
    ASMJIT_X86_FEATURE(INVLPGB)
    ASMJIT_X86_FEATURE(LAHFSAHF)
    ASMJIT_X86_FEATURE(LAM)
    ASMJIT_X86_FEATURE(LWP)
    ASMJIT_X86_FEATURE(LZCNT)
    ASMJIT_X86_FEATURE(MCOMMIT)
    ASMJIT_X86_FEATURE(MONITOR)
    ASMJIT_X86_FEATURE(MONITORX)
    ASMJIT_X86_FEATURE(MOVBE)
    ASMJIT_X86_FEATURE(MOVDIR64B)
    ASMJIT_X86_FEATURE(MOVDIRI)
    ASMJIT_X86_FEATURE(MOVRS)
    ASMJIT_X86_FEATURE(MPX)
    ASMJIT_X86_FEATURE(MSR)
    ASMJIT_X86_FEATURE(MSRLIST)
    ASMJIT_X86_FEATURE(MSR_IMM)
    ASMJIT_X86_FEATURE(MSSE)
    ASMJIT_X86_FEATURE(OSXSAVE)
    ASMJIT_X86_FEATURE(OSPKE)
    ASMJIT_X86_FEATURE(PCONFIG)
    ASMJIT_X86_FEATURE(POPCNT)
    ASMJIT_X86_FEATURE(PREFETCHI)
    ASMJIT_X86_FEATURE(PREFETCHW)
    ASMJIT_X86_FEATURE(PREFETCHWT1)
    ASMJIT_X86_FEATURE(PTWRITE)
    ASMJIT_X86_FEATURE(RAO_INT)
    ASMJIT_X86_FEATURE(RMPQUERY)
    ASMJIT_X86_FEATURE(RDPID)
    ASMJIT_X86_FEATURE(RDPRU)
    ASMJIT_X86_FEATURE(RDRAND)
    ASMJIT_X86_FEATURE(RDSEED)
    ASMJIT_X86_FEATURE(RDTSC)
    ASMJIT_X86_FEATURE(RDTSCP)
    ASMJIT_X86_FEATURE(RTM)
    ASMJIT_X86_FEATURE(SEAM)
    ASMJIT_X86_FEATURE(SERIALIZE)
    ASMJIT_X86_FEATURE(SEV)
    ASMJIT_X86_FEATURE(SEV_ES)
    ASMJIT_X86_FEATURE(SEV_SNP)
    ASMJIT_X86_FEATURE(SKINIT)
    ASMJIT_X86_FEATURE(SMAP)
    ASMJIT_X86_FEATURE(SMEP)
    ASMJIT_X86_FEATURE(SMX)
    ASMJIT_X86_FEATURE(SVM)
    ASMJIT_X86_FEATURE(TBM)
    ASMJIT_X86_FEATURE(TSE)
    ASMJIT_X86_FEATURE(TSXLDTRK)
    ASMJIT_X86_FEATURE(UINTR)
    ASMJIT_X86_FEATURE(VMX)
    ASMJIT_X86_FEATURE(WAITPKG)
    ASMJIT_X86_FEATURE(WBNOINVD)
    ASMJIT_X86_FEATURE(WRMSRNS)
    ASMJIT_X86_FEATURE(XSAVE)
    ASMJIT_X86_FEATURE(XSAVEC)
    ASMJIT_X86_FEATURE(XSAVEOPT)
    ASMJIT_X86_FEATURE(XSAVES)

    ASMJIT_X86_FEATURE(FPU)
    ASMJIT_X86_FEATURE(MMX)
    ASMJIT_X86_FEATURE(MMX2)
    ASMJIT_X86_FEATURE(3DNOW)
    ASMJIT_X86_FEATURE(3DNOW2)
    ASMJIT_X86_FEATURE(GEODE)

    ASMJIT_X86_FEATURE(SSE)
    ASMJIT_X86_FEATURE(SSE2)
    ASMJIT_X86_FEATURE(SSE3)
    ASMJIT_X86_FEATURE(SSSE3)
    ASMJIT_X86_FEATURE(SSE4_1)
    ASMJIT_X86_FEATURE(SSE4_2)
    ASMJIT_X86_FEATURE(SSE4A)
    ASMJIT_X86_FEATURE(PCLMULQDQ)

    ASMJIT_X86_FEATURE(AVX)
    ASMJIT_X86_FEATURE(AVX2)
    ASMJIT_X86_FEATURE(AVX_IFMA)
    ASMJIT_X86_FEATURE(AVX_NE_CONVERT)
    ASMJIT_X86_FEATURE(AVX_VNNI)
    ASMJIT_X86_FEATURE(AVX_VNNI_INT16)
    ASMJIT_X86_FEATURE(AVX_VNNI_INT8)
    ASMJIT_X86_FEATURE(F16C)
    ASMJIT_X86_FEATURE(FMA)
    ASMJIT_X86_FEATURE(FMA4)
    ASMJIT_X86_FEATURE(XOP)

    ASMJIT_X86_FEATURE(AVX512_BF16)
    ASMJIT_X86_FEATURE(AVX512_BITALG)
    ASMJIT_X86_FEATURE(AVX512_BW)
    ASMJIT_X86_FEATURE(AVX512_CD)
    ASMJIT_X86_FEATURE(AVX512_DQ)
    ASMJIT_X86_FEATURE(AVX512_F)
    ASMJIT_X86_FEATURE(AVX512_FP16)
    ASMJIT_X86_FEATURE(AVX512_IFMA)
    ASMJIT_X86_FEATURE(AVX512_VBMI)
    ASMJIT_X86_FEATURE(AVX512_VBMI2)
    ASMJIT_X86_FEATURE(AVX512_VL)
    ASMJIT_X86_FEATURE(AVX512_VNNI)
    ASMJIT_X86_FEATURE(AVX512_VP2INTERSECT)
    ASMJIT_X86_FEATURE(AVX512_VPOPCNTDQ)

    ASMJIT_X86_FEATURE(AESNI)
    ASMJIT_X86_FEATURE(GFNI)
    ASMJIT_X86_FEATURE(SHA)
    ASMJIT_X86_FEATURE(SHA512)
    ASMJIT_X86_FEATURE(SM3)
    ASMJIT_X86_FEATURE(SM4)
    ASMJIT_X86_FEATURE(VAES)
    ASMJIT_X86_FEATURE(VPCLMULQDQ)

    ASMJIT_X86_FEATURE(KL)
    ASMJIT_X86_FEATURE(AESKLE)
    ASMJIT_X86_FEATURE(AESKLEWIDE_KL)

    ASMJIT_X86_FEATURE(AVX10_1)
    ASMJIT_X86_FEATURE(AVX10_2)

    ASMJIT_X86_FEATURE(AMX_AVX512)
    ASMJIT_X86_FEATURE(AMX_BF16)
    ASMJIT_X86_FEATURE(AMX_COMPLEX)
    ASMJIT_X86_FEATURE(AMX_FP16)
    ASMJIT_X86_FEATURE(AMX_FP8)
    ASMJIT_X86_FEATURE(AMX_INT8)
    ASMJIT_X86_FEATURE(AMX_MOVRS)
    ASMJIT_X86_FEATURE(AMX_TF32)
    ASMJIT_X86_FEATURE(AMX_TILE)
    ASMJIT_X86_FEATURE(AMX_TRANSPOSE)

    #undef ASMJIT_X86_FEATURE
  };

  //! ARM specific features data.
  //!
  //! Naming reference:
  //!   - https://developer.arm.com/downloads/-/exploration-tools/feature-names-for-a-profile
  struct ARM : public Data {
    //! ARM CPU feature identifiers.
    enum Id : uint8_t {
      // @EnumValuesBegin{"enum": "CpuFeatures::ARM"}@
      kNone = 0,                 //!< No feature (never set, used internally).

      kARMv6,                    //!< CPU is at least ARMv6 {A32}.
      kARMv7,                    //!< CPU is at least ARMv7 {A32}.
      kARMv8a,                   //!< CPU is at least ARMv8A.
      kTHUMB,                    //!< CPU has THUMB               (16-bit THUMB encoding) {A32}.
      kTHUMBv2,                  //!< CPU has THUMBv2             (32-bit THUMB encoding) {A32}.

      kABLE,                     //!< CPU has ABLE                (address breakpoint linking extension) {A64}.
      kADERR,                    //!< CPU has ADERR               (asynchronous device error exceptions) {A64}.
      kAES,                      //!< CPU has AES                 (ASIMD AES instructions).
      kAFP,                      //!< CPU has AFP                 (alternate floating-point behavior) {A64}.
      kAIE,                      //!< CPU has AIE                 (memory attribute index enhancement) {A64}.
      kAMU1,                     //!< CPU has AMUv1               (activity monitors extension version 1) {A64}.
      kAMU1_1,                   //!< CPU has AMUv1p1             (activity monitors extension version 1.1) {A64}.
      kANERR,                    //!< CPU has ANERR               (asynchronous normal error exception) {A64}.
      kASIMD,                    //!< CPU has ASIMD               (NEON on ARM/THUMB).
      kBF16,                     //!< CPU has BF16                (BFloat16 instructions) {A64}.
      kBRBE,                     //!< CPU has BRBE                (branch record buffer extension) {A64}.
      kBTI,                      //!< CPU has BTI                 (branch target identification).
      kBWE,                      //!< CPU has BWE                 (breakpoint mismatch and range extension) {A64}.
      kCCIDX,                    //!< CPU has CCIDX               (extend of the CCSIDR number of sets).
      kCHK,                      //!< CPU has CHK                 (check feature status - CHKFEAT instruction) {A64}.
      kCLRBHB,                   //!< CPU has CLRBHB              (clear BHB instruction).
      kCMOW,                     //!< CPU has CMOW                (control for cache maintenance permission) {A64}.
      kCONSTPACFIELD,            //!< CPU has CONSTPACFIELD       (PAC algorithm enhancement) {A64}.
      kCPA,                      //!< CPU has CPA                 (instruction-only Checked Pointer Arithmetic) {A64}.
      kCPA2,                     //!< CPU has CPA2                (checked Pointer Arithmetic) {A64}.
      kCPUID,                    //!< CPU has CPUID               (CPUID registers accessible in user-space).
      kCRC32,                    //!< CPU has CRC32               (CRC32 instructions).
      kCSSC,                     //!< CPU has CSSC                (common short sequence compression) {A64}.
      kCSV2,                     //!< CPU has CSV2                (cache speculation variant 2 version 2.1) {A64}.
      kCSV2_3,                   //!< CPU has CSV2_3              (cache speculation variant 2 version 3) {A64}.
      kCSV3,                     //!< CPU has CSV3                (cache speculation Variant 3) {A64}.
      kD128,                     //!< CPU has D128                (128-bit translation tables, 56 bit PA) {A64}.
      kDGH,                      //!< CPU has DGH                 (data gathering hint) {A64}.
      kDIT,                      //!< CPU has DIT                 (data independent timing of instructions).
      kDOTPROD,                  //!< CPU has DOTPROD             (ASIMD Int8 dot product instructions).
      kDPB,                      //!< CPU has DPB                 (DC CVAP instruction) {A64}.
      kDPB2,                     //!< CPU has DPB2                (DC CVADP instruction) {A64}.
      kEBEP,                     //!< CPU has EBEP                (exception-based event profiling) {A64}.
      kEBF16,                    //!< CPU has EBF16               (extended BFloat16 mode) {A64}.
      kECBHB,                    //!< CPU has ECBHB               (exploitative control using branch history information) {A64}.
      kECV,                      //!< CPU has ECV                 (enhanced counter virtualization).
      kEDHSR,                    //!< CPU has EDHSR               (support for EDHSR) {A64}.
      kEDSP,                     //!< CPU has EDSP                (ARM/THUMB only).
      kFAMINMAX,                 //!< CPU has FAMINMAX            (floating-point maximum and minimum absolute value instructions) {A64}.
      kFCMA,                     //!< CPU has FCMA                (FCADD/FCMLA).
      kFGT,                      //!< CPU has FGT                 (fine-grained traps).
      kFGT2,                     //!< CPU has FGT2                (fine-grained traps 2).
      kFHM,                      //!< CPU has FHM                 (half-precision floating-point FMLAL instructions).
      kFLAGM,                    //!< CPU has FLAGM               (condition flag manipulation) {A64}.
      kFLAGM2,                   //!< CPU has FLAGM2              (condition flag manipulation version v2) {A64}.
      kFMAC,                     //!< CPU has FMAC                (ARM/THUMB only).
      kFP,                       //!< CPU has FP                  (floating-point) (on 32-bit ARM this means VFPv3).
      kFP16,                     //!< CPU has FP16                (half-precision floating-point data processing).
      kFP16CONV,                 //!< CPU has FP16CONV            (half-precision float conversion).
      kFP8,                      //!< CPU has FP8                 (FP8 convert instructions) {A64}.
      kFP8DOT2,                  //!< CPU has FP8DOT2             (FP8 2-way dot product to half-precision instructions) {A64}.
      kFP8DOT4,                  //!< CPU has FP8DOT4             (FP8 4-way dot product to single-precision instructions) {A64}.
      kFP8FMA,                   //!< CPU has FP8FMA              (FP8 multiply-accumulate to half-precision and single-precision instructions) {A64}.
      kFPMR,                     //!< CPU has FPMR                (floating-point Mode Register) {A64}.
      kFRINTTS,                  //!< CPU has FRINTTS             (FRINT[32|64][X|Z] instructions) {A64}.
      kGCS,                      //!< CPU has GCS                 (guarded control stack extension) {A64}.
      kHACDBS,                   //!< CPU has HACDBS              (hardware accelerator for cleaning Dirty state) {A64}.
      kHAFDBS,                   //!< CPU has HAFDBS              (hardware management of the access flag and dirty state) {A64}.
      kHAFT,                     //!< CPU has HAFT                (hardware managed access flag for table descriptors) {A64}.
      kHDBSS,                    //!< CPU has HDBSS               (hardware Dirty state tracking Structure) {A64}.
      kHBC,                      //!< CPU has HBC                 (hinted conditional branches) {A64}.
      kHCX,                      //!< CPU has HCX                 (support for the HCRX_EL2 register) {A64}.
      kHPDS,                     //!< CPU has HPDS                (hierarchical permission disables in translation tables	) {A64}.
      kHPDS2,                    //!< CPU has HPDS2               (hierarchical permission disables) {A64}.
      kI8MM,                     //!< CPU has I8MM                (int8 matrix multiplication) {A64}.
      kIDIVA,                    //!< CPU has IDIV                (hardware SDIV and UDIV in ARM mode).
      kIDIVT,                    //!< CPU has IDIV                (hardware SDIV and UDIV in THUMB mode).
      kITE,                      //!< CPU has ITE                 (instrumentation extension) {A64}.
      kJSCVT,                    //!< CPU has JSCVT               (JavaScript FJCVTS conversion instruction) {A64}.
      kLOR,                      //!< CPU has LOR                 (limited ordering regions extension).
      kLRCPC,                    //!< CPU has LRCPC               (load-acquire RCpc instructions) {A64}.
      kLRCPC2,                   //!< CPU has LRCPC2              (load-acquire RCpc instructions v2) {A64}.
      kLRCPC3,                   //!< CPU has LRCPC3              (load-Acquire RCpc instructions v3) {A64}.
      kLS64,                     //!< CPU has LS64                (64 byte loads/stores without return) {A64}.
      kLS64_ACCDATA,             //!< CPU has LS64_ACCDATA        (64-byte EL0 stores with return) {A64}.
      kLS64_V,                   //!< CPU has LS64_V              (64-byte stores with return) {A64}.
      kLSE,                      //!< CPU has LSE                 (large system extensions) {A64}.
      kLSE128,                   //!< CPU has LSE128              (128-bit atomics) {A64}.
      kLSE2,                     //!< CPU has LSE2                (large system extensions v2) {A64}.
      kLUT,                      //!< CPU has LUT                 (lookup table instructions with 2-bit and 4-bit indices) {A64}.
      kLVA,                      //!< CPU has LVA                 (large VA support) {A64}.
      kLVA3,                     //!< CPU has LVA3                (56-bit VA) {A64}.
      kMEC,                      //!< CPU has MEC                 (memory encryption contexts) {A64}.
      kMOPS,                     //!< CPU has MOPS                (memcpy and memset acceleration instructions) {A64}.
      kMPAM,                     //!< CPU has MPAM                (memory system partitioning and monitoring extension) {A64}.
      kMTE,                      //!< CPU has MTE                 (instruction-only memory tagging extension) {A64}.
      kMTE2,                     //!< CPU has MTE2                (full memory tagging extension) {A64}.
      kMTE3,                     //!< CPU has MTE3                (MTE asymmetric fault handling) {A64}.
      kMTE4,                     //!< CPU has MTE4                (MTE v4) {A64}.
      kMTE_ASYM_FAULT,           //!< CPU has MTE_ASYM_FAULT      (memory tagging asymmetric faults) {A64}.
      kMTE_ASYNC,                //!< CPU has MTE_ASYNC           (memory tagging asynchronous faulting) {A64}.
      kMTE_CANONICAL_TAGS,       //!< CPU has MTE_CANONICAL_TAGS  (canonical tag checking for untagged memory) {A64}.
      kMTE_NO_ADDRESS_TAGS,      //!< CPU has MTE_NO_ADDRESS_TAGS (memory tagging with address tagging disabled) {A64}.
      kMTE_PERM_S1,              //!< CPU has MTE_PERM_S1         (allocation tag access permission) {A64}.
      kMTE_STORE_ONLY,           //!< CPU has MTE_STORE_ONLY      (store-only tag checking) {A64}.
      kMTE_TAGGED_FAR,           //!< CPU has MTE_TAGGED_FAR      (FAR_ELx on a tag check fault) {A64}.
      kMTPMU,                    //!< CPU has MTPMU               (multi-threaded PMU extensions) {A64}.
      kNMI,                      //!< CPU has NMI                 (non-maskable Interrupt) {A64}.
      kNV,                       //!< CPU has NV                  (nested virtualization enchancement) {A64}.
      kNV2,                      //!< CPU has NV2                 (enhanced support for nested virtualization) {A64}.
      kPAN,                      //!< CPU has PAN                 (privileged access-never extension) {A64}.
      kPAN2,                     //!< CPU has PAN2                (PAN s1e1R and s1e1W variants) {A64}.
      kPAN3,                     //!< CPU has PAN3                (support for SCTLR_ELx.EPAN) {A64}.
      kPAUTH,                    //!< CPU has PAUTH               (pointer authentication extension) {A64}.
      kPFAR,                     //!< CPU has PFAR                (physical fault address registers) {A64}.
      kPMU,                      //!< CPU has PMU                 {A64}.
      kPMULL,                    //!< CPU has PMULL               (ASIMD PMULL instructions) {A64}.
      kPRFMSLC,                  //!< CPU has PRFMSLC             (PRFM instructions support the SLC target) {A64}.
      kRAS,                      //!< CPU has RAS                 (reliability, availability and serviceability extensions).
      kRAS1_1,                   //!< CPU has RASv1p1             (RAS v1.1).
      kRAS2,                     //!< CPU has RASv2               (RAS v2).
      kRASSA2,                   //!< CPU has RASSAv2             (RAS v2 system architecture).
      kRDM,                      //!< CPU has RDM                 (rounding double multiply accumulate) {A64}.
      kRME,                      //!< CPU has RME                 (memory encryption contexts extension) {A64}.
      kRNG,                      //!< CPU has RNG                 (random number generation).
      kRNG_TRAP,                 //!< CPU has RNG_TRAP            (random number trap to EL3 field) {A64}.
      kRPRES,                    //!< CPU has RPRES               (increased precision of reciprocal estimate and RSQRT estimate) {A64}.
      kRPRFM,                    //!< CPU has RPRFM               (range prefetch hint instruction).
      kS1PIE,                    //!< CPU has S1PIE               (permission model enhancements) {A64}.
      kS1POE,                    //!< CPU has S1POE               (permission model enhancements) {A64}.
      kS2PIE,                    //!< CPU has S2PIE               (permission model enhancements) {A64}.
      kS2POE,                    //!< CPU has S2POE               (permission model enhancements) {A64}.
      kSB,                       //!< CPU has SB                  (speculative barrier).
      kSCTLR2,                   //!< CPU has SCTLR2              (extension to SCTLR_ELx) {A64}.
      kSEBEP,                    //!< CPU has SEBEP               (synchronous exception-based event profiling) {A64}.
      kSEL2,                     //!< CPU has SEL2                (secure EL2) {A64}.
      kSHA1,                     //!< CPU has SHA1                (ASIMD SHA1 instructions).
      kSHA256,                   //!< CPU has SHA256              (ASIMD SHA256 instructions).
      kSHA3,                     //!< CPU has SHA3                (ASIMD EOR3, RAX1, XAR, and BCAX instructions).
      kSHA512,                   //!< CPU has SHA512              (ASIMD SHA512 instructions).
      kSM3,                      //!< CPU has SM3                 (ASIMD SM3 instructions).
      kSM4,                      //!< CPU has SM4                 (ASIMD SM4 instructions).
      kSME,                      //!< CPU has SME                 (SME v1 - scalable matrix extension) {A64}.
      kSME2,                     //!< CPU has SME2                (SME v2) {A64}.
      kSME2_1,                   //!< CPU has SME2p1              (SME v2.1) {A64}.
      kSME_B16B16,               //!< CPU has SME_B16B16          (SME non-widening BFloat16 to BFloat16 arithmetic) {A64}.
      kSME_B16F32,               //!< CPU has SME_B16F32          (BFMOPA and BFMOPS instructions that accumulate BFloat16 outer products into single-precision tiles) {A64}.
      kSME_BI32I32,              //!< CPU has SME_BI32I32         (BMOPA and BMOPS instructions that accumulate 1-bit binary outer products into 32-bit integer tiles) {A64}.
      kSME_F16F16,               //!< CPU has SME_F16F16          (SME2.1 non-widening half-precision FP16 to FP16 arithmetic) {A64}.
      kSME_F16F32,               //!< CPU has SME_F16F32          {A64}.
      kSME_F32F32,               //!< CPU has SME_F32F32          {A64}.
      kSME_F64F64,               //!< CPU has SME_F64F64          {A64}.
      kSME_F8F16,                //!< CPU has SME_F8F16           (SME2 ZA-targeting FP8 multiply-accumulate, dot product, and outer product to half-precision instructions) {A64}.
      kSME_F8F32,                //!< CPU has SME_F8F32           (SME2 ZA-targeting FP8 multiply-accumulate, dot product, and outer product to single-precision instructions) {A64}.
      kSME_FA64,                 //!< CPU has SME_FA64            {A64}.
      kSME_I16I32,               //!< CPU has SME_I16I32          {A64}.
      kSME_I16I64,               //!< CPU has SME_I16I64          {A64}.
      kSME_I8I32,                //!< CPU has SME_I8I32           {A64}.
      kSME_LUTv2,                //!< CPU has SME_LUTv2           (lookup table instructions with 4-bit indices and 8-bit elements) {A64}.
      kSPE,                      //!< CPU has SPE                 (statistical profiling extension) {A64}.
      kSPE1_1,                   //!< CPU has SPEv1p1             (statistical profiling extensions version 1.1) {A64}.
      kSPE1_2,                   //!< CPU has SPEv1p2             (statistical profiling extensions version 1.2) {A64}.
      kSPE1_3,                   //!< CPU has SPEv1p3             (statistical profiling extensions version 1.3) {A64}.
      kSPE1_4,                   //!< CPU has SPEv1p4             (statistical profiling extensions version 1.4) {A64}.
      kSPE_ALTCLK,               //!< CPU has SPE_ALTCLK          (statistical profiling alternate clock domain extension) {A64}.
      kSPE_CRR,                  //!< CPU has SPE_CRR             (statistical profiling call return branch records) {A64}.
      kSPE_EFT,                  //!< CPU has SPE_EFT             (statistical profiling extended filtering by type) {A64}.
      kSPE_FDS,                  //!< CPU has SPE_FDS             (statistical profiling data source filtering) {A64}.
      kSPE_FPF,                  //!< CPU has SPE_FPF             (statistical profiling floating-point flag extension) {A64}.
      kSPE_SME,                  //!< CPU has SPE_SME             (statistical profiling extensions for SME) {A64}.
      kSPECRES,                  //!< CPU has SPECRES             (speculation restriction instructions).
      kSPECRES2,                 //!< CPU has SPECRES2            (clear other speculative predictions).
      kSPMU,                     //!< CPU has SPMU                (system performance monitors extension) {A64}.
      kSSBS,                     //!< CPU has SSBS                (speculative store bypass safe instruction).
      kSSBS2,                    //!< CPU has SSBS2               (MRS and MSR instructions for SSBS).
      kSSVE_FP8DOT2,             //!< CPU has SSVE_FP8DOT2        (SVE2 FP8 2-way dot product to half-precision instructions in Streaming SVE mode) {A64}.
      kSSVE_FP8DOT4,             //!< CPU has SSVE_FP8DOT4        (SVE2 FP8 4-way dot product to single-precision instructions in Streaming SVE mode) {A64}.
      kSSVE_FP8FMA,              //!< CPU has SSVE_FP8FMA         (SVE2 FP8 multiply-accumulate to half-precision and single-precision instructions in Streaming SVE mode) {A64}.
      kSVE,                      //!< CPU has SVE                 (SVE v1 - scalable vector extension) {A64}.
      kSVE2,                     //!< CPU has SVE2                (SVE v2) {A64}.
      kSVE2_1,                   //!< CPU has SVE2p1              (SVE v2.1) {A64}.
      kSVE_AES,                  //!< CPU has SVE_AES             (SVE AES instructions) {A64}.
      kSVE_B16B16,               //!< CPU has SVE_B16B16          (SVE non-widening BFloat16 to BFloat16 arithmetic) {A64}.
      kSVE_BF16,                 //!< CPU has SVE_BF16            (SVE BF16 instructions) {A64}.
      kSVE_BITPERM,              //!< CPU has SVE_BITPERM         (SVE bit permute) {A64}.
      kSVE_EBF16,                //!< CPU has SVE_EBF16           (SVE extended BFloat16 mode) {A64}.
      kSVE_F32MM,                //!< CPU has SVE_F32MM           (SVE single-precision floating-point matrix multiply instruction) {A64}.
      kSVE_F64MM,                //!< CPU has SVE_F64MM           (SVE double-precision floating-point matrix multiply instruction) {A64}.
      kSVE_I8MM,                 //!< CPU has SVE_I8MM            (SVE int8 matrix multiplication) {A64}.
      kSVE_PMULL128,             //!< CPU has SVE_PMULL128        (SVE PMULL instructions) {A64}.
      kSVE_SHA3,                 //!< CPU has SVE_SHA3            (SVE SHA-3 instructions) {A64}.
      kSVE_SM4,                  //!< CPU has SVE_SM4             (SVE SM4 instructions {A64}.
      kSYSINSTR128,              //!< CPU has SYSINSTR128         (128-bit system instructions) {A64}.
      kSYSREG128,                //!< CPU has SYSREG128           (128-bit system registers) {A64}.
      kTHE,                      //!< CPU has THE                 (translation hardening extension).
      kTLBIOS,                   //!< CPU has TLBIOS              (TLBI instructions in Outer Shareable domain) {A64}.
      kTLBIRANGE,                //!< CPU has TLBIRANGE           (TLBI range instructions) {A64}.
      kTLBIW,                    //!< CPU has TLBIW               (TLBI VMALL for dirty state) {A64}.
      kTME,                      //!< CPU has TME                 (transactional memory extensions).
      kTRF,                      //!< CPU has TRF                 (self-hosted trace extensions).
      kUAO,                      //!< CPU has UAO                 (AArch64 v8.2 UAO PState) {A64}.
      kVFP_D32,                  //!< CPU has VFP_D32             (32 VFP-D registers) (ARM/THUMB only).
      kVHE,                      //!< CPU has VHE                 (virtual host extension).
      kVMID16,                   //!< CPU has VMID16              (16-bit VMID) {A64}.
      kWFXT,                     //!< CPU has WFxT                (WFE and WFI instructions with timeout) {A64}.
      kXNX,                      //!< CPU has XNX                 (translation table stage 2 unprivileged execute-never) {A64}.
      kXS,                       //!< CPU has XS                  (XS attribute in TLBI and DSB instructions) {A64}.
      // @EnumValuesEnd@

      kMaxValue = kXS
    };

    #define ASMJIT_ARM_FEATURE(FEATURE) \
      /*! Tests whether FEATURE is present. */ \
      ASMJIT_INLINE_NODEBUG bool has##FEATURE() const noexcept { return has(ARM::k##FEATURE); }

    ASMJIT_ARM_FEATURE(THUMB)
    ASMJIT_ARM_FEATURE(THUMBv2)

    ASMJIT_ARM_FEATURE(ARMv6)
    ASMJIT_ARM_FEATURE(ARMv7)
    ASMJIT_ARM_FEATURE(ARMv8a)

    ASMJIT_ARM_FEATURE(ABLE)
    ASMJIT_ARM_FEATURE(ADERR)
    ASMJIT_ARM_FEATURE(AES)
    ASMJIT_ARM_FEATURE(AFP)
    ASMJIT_ARM_FEATURE(AIE)
    ASMJIT_ARM_FEATURE(AMU1)
    ASMJIT_ARM_FEATURE(AMU1_1)
    ASMJIT_ARM_FEATURE(ANERR)
    ASMJIT_ARM_FEATURE(ASIMD)
    ASMJIT_ARM_FEATURE(BF16)
    ASMJIT_ARM_FEATURE(BRBE)
    ASMJIT_ARM_FEATURE(BTI)
    ASMJIT_ARM_FEATURE(BWE)
    ASMJIT_ARM_FEATURE(CCIDX)
    ASMJIT_ARM_FEATURE(CHK)
    ASMJIT_ARM_FEATURE(CLRBHB)
    ASMJIT_ARM_FEATURE(CMOW)
    ASMJIT_ARM_FEATURE(CONSTPACFIELD)
    ASMJIT_ARM_FEATURE(CPA)
    ASMJIT_ARM_FEATURE(CPA2)
    ASMJIT_ARM_FEATURE(CPUID)
    ASMJIT_ARM_FEATURE(CRC32)
    ASMJIT_ARM_FEATURE(CSSC)
    ASMJIT_ARM_FEATURE(CSV2)
    ASMJIT_ARM_FEATURE(CSV2_3)
    ASMJIT_ARM_FEATURE(CSV3)
    ASMJIT_ARM_FEATURE(D128)
    ASMJIT_ARM_FEATURE(DGH)
    ASMJIT_ARM_FEATURE(DIT)
    ASMJIT_ARM_FEATURE(DOTPROD)
    ASMJIT_ARM_FEATURE(DPB)
    ASMJIT_ARM_FEATURE(DPB2)
    ASMJIT_ARM_FEATURE(EBEP)
    ASMJIT_ARM_FEATURE(EBF16)
    ASMJIT_ARM_FEATURE(ECBHB)
    ASMJIT_ARM_FEATURE(ECV)
    ASMJIT_ARM_FEATURE(EDHSR)
    ASMJIT_ARM_FEATURE(EDSP)
    ASMJIT_ARM_FEATURE(FAMINMAX)
    ASMJIT_ARM_FEATURE(FCMA)
    ASMJIT_ARM_FEATURE(FGT)
    ASMJIT_ARM_FEATURE(FGT2)
    ASMJIT_ARM_FEATURE(FHM)
    ASMJIT_ARM_FEATURE(FLAGM)
    ASMJIT_ARM_FEATURE(FLAGM2)
    ASMJIT_ARM_FEATURE(FMAC)
    ASMJIT_ARM_FEATURE(FP)
    ASMJIT_ARM_FEATURE(FP16)
    ASMJIT_ARM_FEATURE(FP16CONV)
    ASMJIT_ARM_FEATURE(FP8)
    ASMJIT_ARM_FEATURE(FP8DOT2)
    ASMJIT_ARM_FEATURE(FP8DOT4)
    ASMJIT_ARM_FEATURE(FP8FMA)
    ASMJIT_ARM_FEATURE(FPMR)
    ASMJIT_ARM_FEATURE(FRINTTS)
    ASMJIT_ARM_FEATURE(GCS)
    ASMJIT_ARM_FEATURE(HACDBS)
    ASMJIT_ARM_FEATURE(HAFDBS)
    ASMJIT_ARM_FEATURE(HAFT)
    ASMJIT_ARM_FEATURE(HDBSS)
    ASMJIT_ARM_FEATURE(HBC)
    ASMJIT_ARM_FEATURE(HCX)
    ASMJIT_ARM_FEATURE(HPDS)
    ASMJIT_ARM_FEATURE(HPDS2)
    ASMJIT_ARM_FEATURE(I8MM)
    ASMJIT_ARM_FEATURE(IDIVA)
    ASMJIT_ARM_FEATURE(IDIVT)
    ASMJIT_ARM_FEATURE(ITE)
    ASMJIT_ARM_FEATURE(JSCVT)
    ASMJIT_ARM_FEATURE(LOR)
    ASMJIT_ARM_FEATURE(LRCPC)
    ASMJIT_ARM_FEATURE(LRCPC2)
    ASMJIT_ARM_FEATURE(LRCPC3)
    ASMJIT_ARM_FEATURE(LS64)
    ASMJIT_ARM_FEATURE(LS64_ACCDATA)
    ASMJIT_ARM_FEATURE(LS64_V)
    ASMJIT_ARM_FEATURE(LSE)
    ASMJIT_ARM_FEATURE(LSE128)
    ASMJIT_ARM_FEATURE(LSE2)
    ASMJIT_ARM_FEATURE(LUT)
    ASMJIT_ARM_FEATURE(LVA)
    ASMJIT_ARM_FEATURE(LVA3)
    ASMJIT_ARM_FEATURE(MEC)
    ASMJIT_ARM_FEATURE(MOPS)
    ASMJIT_ARM_FEATURE(MPAM)
    ASMJIT_ARM_FEATURE(MTE)
    ASMJIT_ARM_FEATURE(MTE2)
    ASMJIT_ARM_FEATURE(MTE3)
    ASMJIT_ARM_FEATURE(MTE4)
    ASMJIT_ARM_FEATURE(MTE_ASYM_FAULT)
    ASMJIT_ARM_FEATURE(MTE_ASYNC)
    ASMJIT_ARM_FEATURE(MTE_CANONICAL_TAGS)
    ASMJIT_ARM_FEATURE(MTE_NO_ADDRESS_TAGS)
    ASMJIT_ARM_FEATURE(MTE_PERM_S1)
    ASMJIT_ARM_FEATURE(MTE_STORE_ONLY)
    ASMJIT_ARM_FEATURE(MTE_TAGGED_FAR)
    ASMJIT_ARM_FEATURE(MTPMU)
    ASMJIT_ARM_FEATURE(NMI)
    ASMJIT_ARM_FEATURE(NV)
    ASMJIT_ARM_FEATURE(NV2)
    ASMJIT_ARM_FEATURE(PAN)
    ASMJIT_ARM_FEATURE(PAN2)
    ASMJIT_ARM_FEATURE(PAN3)
    ASMJIT_ARM_FEATURE(PAUTH)
    ASMJIT_ARM_FEATURE(PFAR)
    ASMJIT_ARM_FEATURE(PMU)
    ASMJIT_ARM_FEATURE(PMULL)
    ASMJIT_ARM_FEATURE(PRFMSLC)
    ASMJIT_ARM_FEATURE(RAS)
    ASMJIT_ARM_FEATURE(RAS1_1)
    ASMJIT_ARM_FEATURE(RAS2)
    ASMJIT_ARM_FEATURE(RASSA2)
    ASMJIT_ARM_FEATURE(RDM)
    ASMJIT_ARM_FEATURE(RME)
    ASMJIT_ARM_FEATURE(RNG)
    ASMJIT_ARM_FEATURE(RNG_TRAP)
    ASMJIT_ARM_FEATURE(RPRES)
    ASMJIT_ARM_FEATURE(RPRFM)
    ASMJIT_ARM_FEATURE(S1PIE)
    ASMJIT_ARM_FEATURE(S1POE)
    ASMJIT_ARM_FEATURE(S2PIE)
    ASMJIT_ARM_FEATURE(S2POE)
    ASMJIT_ARM_FEATURE(SB)
    ASMJIT_ARM_FEATURE(SCTLR2)
    ASMJIT_ARM_FEATURE(SEBEP)
    ASMJIT_ARM_FEATURE(SEL2)
    ASMJIT_ARM_FEATURE(SHA1)
    ASMJIT_ARM_FEATURE(SHA256)
    ASMJIT_ARM_FEATURE(SHA3)
    ASMJIT_ARM_FEATURE(SHA512)
    ASMJIT_ARM_FEATURE(SM3)
    ASMJIT_ARM_FEATURE(SM4)
    ASMJIT_ARM_FEATURE(SME)
    ASMJIT_ARM_FEATURE(SME2)
    ASMJIT_ARM_FEATURE(SME2_1)
    ASMJIT_ARM_FEATURE(SME_B16B16)
    ASMJIT_ARM_FEATURE(SME_B16F32)
    ASMJIT_ARM_FEATURE(SME_BI32I32)
    ASMJIT_ARM_FEATURE(SME_F16F16)
    ASMJIT_ARM_FEATURE(SME_F16F32)
    ASMJIT_ARM_FEATURE(SME_F32F32)
    ASMJIT_ARM_FEATURE(SME_F64F64)
    ASMJIT_ARM_FEATURE(SME_F8F16)
    ASMJIT_ARM_FEATURE(SME_F8F32)
    ASMJIT_ARM_FEATURE(SME_FA64)
    ASMJIT_ARM_FEATURE(SME_I16I32)
    ASMJIT_ARM_FEATURE(SME_I16I64)
    ASMJIT_ARM_FEATURE(SME_I8I32)
    ASMJIT_ARM_FEATURE(SME_LUTv2)
    ASMJIT_ARM_FEATURE(SPE)
    ASMJIT_ARM_FEATURE(SPE1_1)
    ASMJIT_ARM_FEATURE(SPE1_2)
    ASMJIT_ARM_FEATURE(SPE1_3)
    ASMJIT_ARM_FEATURE(SPE1_4)
    ASMJIT_ARM_FEATURE(SPE_ALTCLK)
    ASMJIT_ARM_FEATURE(SPE_CRR)
    ASMJIT_ARM_FEATURE(SPE_EFT)
    ASMJIT_ARM_FEATURE(SPE_FDS)
    ASMJIT_ARM_FEATURE(SPE_FPF)
    ASMJIT_ARM_FEATURE(SPE_SME)
    ASMJIT_ARM_FEATURE(SPECRES)
    ASMJIT_ARM_FEATURE(SPECRES2)
    ASMJIT_ARM_FEATURE(SPMU)
    ASMJIT_ARM_FEATURE(SSBS)
    ASMJIT_ARM_FEATURE(SSBS2)
    ASMJIT_ARM_FEATURE(SSVE_FP8DOT2)
    ASMJIT_ARM_FEATURE(SSVE_FP8DOT4)
    ASMJIT_ARM_FEATURE(SSVE_FP8FMA)
    ASMJIT_ARM_FEATURE(SVE)
    ASMJIT_ARM_FEATURE(SVE2)
    ASMJIT_ARM_FEATURE(SVE2_1)
    ASMJIT_ARM_FEATURE(SVE_AES)
    ASMJIT_ARM_FEATURE(SVE_B16B16)
    ASMJIT_ARM_FEATURE(SVE_BF16)
    ASMJIT_ARM_FEATURE(SVE_BITPERM)
    ASMJIT_ARM_FEATURE(SVE_EBF16)
    ASMJIT_ARM_FEATURE(SVE_F32MM)
    ASMJIT_ARM_FEATURE(SVE_F64MM)
    ASMJIT_ARM_FEATURE(SVE_I8MM)
    ASMJIT_ARM_FEATURE(SVE_PMULL128)
    ASMJIT_ARM_FEATURE(SVE_SHA3)
    ASMJIT_ARM_FEATURE(SVE_SM4)
    ASMJIT_ARM_FEATURE(SYSINSTR128)
    ASMJIT_ARM_FEATURE(SYSREG128)
    ASMJIT_ARM_FEATURE(THE)
    ASMJIT_ARM_FEATURE(TLBIOS)
    ASMJIT_ARM_FEATURE(TLBIRANGE)
    ASMJIT_ARM_FEATURE(TLBIW)
    ASMJIT_ARM_FEATURE(TME)
    ASMJIT_ARM_FEATURE(TRF)
    ASMJIT_ARM_FEATURE(UAO)
    ASMJIT_ARM_FEATURE(VFP_D32)
    ASMJIT_ARM_FEATURE(VHE)
    ASMJIT_ARM_FEATURE(VMID16)
    ASMJIT_ARM_FEATURE(WFXT)
    ASMJIT_ARM_FEATURE(XNX)
    ASMJIT_ARM_FEATURE(XS)

    #undef ASMJIT_ARM_FEATURE
  };

  static_assert(uint32_t(X86::kMaxValue) < kMaxFeatures, "The number of X86 CPU features cannot exceed CpuFeatures::kMaxFeatures");
  static_assert(uint32_t(ARM::kMaxValue) < kMaxFeatures, "The number of ARM CPU features cannot exceed CpuFeatures::kMaxFeatures");

  //! \}

  //! \name Members
  //! \{

  Data _data {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG CpuFeatures() noexcept {}
  ASMJIT_INLINE_NODEBUG CpuFeatures(const CpuFeatures& other) noexcept = default;
  ASMJIT_INLINE_NODEBUG explicit CpuFeatures(const Data& other) noexcept : _data{other._bits} {}
  ASMJIT_INLINE_NODEBUG explicit CpuFeatures(Globals::NoInit_) noexcept {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG CpuFeatures& operator=(const CpuFeatures& other) noexcept = default;

  ASMJIT_INLINE_NODEBUG bool operator==(const CpuFeatures& other) const noexcept { return  equals(other); }
  ASMJIT_INLINE_NODEBUG bool operator!=(const CpuFeatures& other) const noexcept { return !equals(other); }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns true if there are no features set.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _data.empty(); }

  //! Casts this base class into a derived type `T`.
  template<typename T = Data>
  ASMJIT_INLINE_NODEBUG T& data() noexcept { return static_cast<T&>(_data); }

  //! Casts this base class into a derived type `T` (const).
  template<typename T = Data>
  ASMJIT_INLINE_NODEBUG const T& data() const noexcept { return static_cast<const T&>(_data); }

  //! Returns CpuFeatures::Data as \ref CpuFeatures::X86.
  ASMJIT_INLINE_NODEBUG X86& x86() noexcept { return data<X86>(); }
  //! Returns CpuFeatures::Data as \ref CpuFeatures::X86 (const).
  ASMJIT_INLINE_NODEBUG const X86& x86() const noexcept { return data<X86>(); }

  //! Returns CpuFeatures::Data as \ref CpuFeatures::ARM.
  ASMJIT_INLINE_NODEBUG ARM& arm() noexcept { return data<ARM>(); }
  //! Returns CpuFeatures::Data as \ref CpuFeatures::ARM (const).
  ASMJIT_INLINE_NODEBUG const ARM& arm() const noexcept { return data<ARM>(); }

  //! Returns all features as array of bitwords (see \ref Support::BitWord).
  ASMJIT_INLINE_NODEBUG BitWord* bits() noexcept { return _data.bits(); }
  //! Returns all features as array of bitwords (const).
  ASMJIT_INLINE_NODEBUG const BitWord* bits() const noexcept { return _data.bits(); }
  //! Returns the number of BitWords returned by \ref bits().
  ASMJIT_INLINE_NODEBUG size_t bitWordCount() const noexcept { return _data.bitWordCount(); }

  //! Returns \ref Support::BitVectorIterator, that can be used to iterate over all features efficiently.
  ASMJIT_INLINE_NODEBUG Iterator iterator() const noexcept { return _data.iterator(); }

  //! Tests whether the feature `featureId` is present.
  template<typename FeatureId>
  ASMJIT_INLINE_NODEBUG bool has(const FeatureId& featureId) const noexcept { return _data.has(featureId); }

  //! Tests whether any of the features is present.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG bool hasAny(Args&&... args) const noexcept { return _data.hasAny(std::forward<Args>(args)...); }

  //! Tests whether all features as defined by `other` are present.
  ASMJIT_INLINE_NODEBUG bool hasAll(const CpuFeatures& other) const noexcept { return _data.hasAll(other._data); }

  //! \}

  //! \name Manipulation
  //! \{

  //! Clears all features set.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { _data.reset(); }

  //! Adds the given CPU `featureId` to the list of features.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG void add(Args&&... args) noexcept { return _data.add(std::forward<Args>(args)...); }

  //! Adds the given CPU `featureId` to the list of features if `condition` is true.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG void addIf(bool condition, Args&&... args) noexcept { return _data.addIf(condition, std::forward<Args>(args)...); }

  //! Removes the given CPU `featureId` from the list of features.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG void remove(Args&&... args) noexcept { return _data.remove(std::forward<Args>(args)...); }

  //! Tests whether this CPU features matches `other`.
  ASMJIT_INLINE_NODEBUG bool equals(const CpuFeatures& other) const noexcept { return _data.equals(other._data); }

  //! \}
};

//! CPU information.
class CpuInfo {
public:
  //! \name Members
  //! \{

  //! Architecture.
  Arch _arch {};
  //! Sub-architecture.
  SubArch _subArch {};
  //! True if the CPU was detected, false if the detection failed or it's not available.
  bool _wasDetected {};
  //! Reserved for future use.
  uint8_t _reserved {};
  //! CPU family ID.
  uint32_t _familyId {};
  //! CPU model ID.
  uint32_t _modelId {};
  //! CPU brand ID.
  uint32_t _brandId {};
  //! CPU stepping.
  uint32_t _stepping {};
  //! Processor type.
  uint32_t _processorType {};
  //! Maximum number of addressable IDs for logical processors.
  uint32_t _maxLogicalProcessors {};
  //! Cache line size (in bytes).
  uint32_t _cacheLineSize {};
  //! Number of hardware threads.
  uint32_t _hwThreadCount {};

  //! CPU vendor string.
  FixedString<16> _vendor {};
  //! CPU brand string.
  FixedString<64> _brand {};
  //! CPU features.
  CpuFeatures _features {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new CpuInfo instance.
  ASMJIT_INLINE_NODEBUG CpuInfo() noexcept {}
  //! Creates a copy of `other` instance.
  ASMJIT_INLINE_NODEBUG CpuInfo(const CpuInfo& other) noexcept = default;

  //! Creates an unitialized `CpuInfo` instance.
  ASMJIT_INLINE_NODEBUG explicit CpuInfo(Globals::NoInit_) noexcept
    : _features(Globals::NoInit) {};

  //! \}

  //! \name CPU Information Detection
  //! \{

  //! Returns the host CPU information.
  //!
  //! \note The returned reference is global - it's setup only once and then shared.
  ASMJIT_API static const CpuInfo& host() noexcept;

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Copy assignment.
  ASMJIT_INLINE_NODEBUG CpuInfo& operator=(const CpuInfo& other) noexcept = default;

  //! \}

  //! \name Initialization & Reset
  //! \{

  //! Initializes CpuInfo architecture and sub-architecture members to `arch` and `subArch`, respectively.
  ASMJIT_INLINE_NODEBUG void initArch(Arch arch, SubArch subArch = SubArch::kUnknown) noexcept {
    _arch = arch;
    _subArch = subArch;
  }

  //! Resets this \ref CpuInfo to a default constructed state.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = CpuInfo{}; }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the CPU architecture this information relates to.
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return _arch; }

  //! Returns the CPU sub-architecture this information relates to.
  ASMJIT_INLINE_NODEBUG SubArch subArch() const noexcept { return _subArch; }

  //! Returns whether the CPU was detected successfully.
  //!
  //! If the returned value is false it means that AsmJit either failed to detect the CPU or it doesn't have
  //! implementation targeting the host architecture and operating system.
  ASMJIT_INLINE_NODEBUG bool wasDetected() const noexcept { return _wasDetected; }

  //! Returns the CPU family ID.
  //!
  //! The information provided depends on architecture and OS:
  //!   - X86:
  //!     - Family identifier matches the FamilyId read by using CPUID.
  //!   - ARM:
  //!     - Apple - returns Apple Family identifier returned by sysctlbyname("hw.cpufamily").
  ASMJIT_INLINE_NODEBUG uint32_t familyId() const noexcept { return _familyId; }

  //! Returns the CPU model ID.
  //!
  //! The information provided depends on architecture and OS:
  //!   - X86:
  //!     - Model identifier matches the ModelId read by using CPUID.
  ASMJIT_INLINE_NODEBUG uint32_t modelId() const noexcept { return _modelId; }

  //! Returns the CPU brand id.
  //!
  //! The information provided depends on architecture and OS:
  //!   - X86:
  //!     - Brand identifier matches the BrandId read by using CPUID.
  ASMJIT_INLINE_NODEBUG uint32_t brandId() const noexcept { return _brandId; }

  //! Returns the CPU stepping.
  //!
  //! The information provided depends on architecture and OS:
  //!   - X86:
  //!     - Stepping identifier matches the Stepping information read by using CPUID.
  ASMJIT_INLINE_NODEBUG uint32_t stepping() const noexcept { return _stepping; }

  //! Returns the processor type.
  //!
  //! The information provided depends on architecture and OS:
  //!   - X86:
  //!     - Processor type identifier matches the ProcessorType read by using CPUID.
  ASMJIT_INLINE_NODEBUG uint32_t processorType() const noexcept { return _processorType; }

  //! Returns the maximum number of logical processors.
  ASMJIT_INLINE_NODEBUG uint32_t maxLogicalProcessors() const noexcept { return _maxLogicalProcessors; }

  //! Returns the size of a CPU cache line.
  //!
  //! On a multi-architecture system this should return the smallest cache line of all CPUs.
  ASMJIT_INLINE_NODEBUG uint32_t cacheLineSize() const noexcept { return _cacheLineSize; }

  //! Returns number of hardware threads available.
  ASMJIT_INLINE_NODEBUG uint32_t hwThreadCount() const noexcept { return _hwThreadCount; }

  //! Returns a CPU vendor string.
  ASMJIT_INLINE_NODEBUG const char* vendor() const noexcept { return _vendor.str; }
  //! Tests whether the CPU vendor string is equal to `s`.
  ASMJIT_INLINE_NODEBUG bool isVendor(const char* s) const noexcept { return _vendor.equals(s); }

  //! Returns a CPU brand string.
  ASMJIT_INLINE_NODEBUG const char* brand() const noexcept { return _brand.str; }

  //! Returns CPU features.
  ASMJIT_INLINE_NODEBUG CpuFeatures& features() noexcept { return _features; }
  //! Returns CPU features (const).
  ASMJIT_INLINE_NODEBUG const CpuFeatures& features() const noexcept { return _features; }

  //! Tests whether the CPU has the given `feature`.
  template<typename FeatureId>
  ASMJIT_INLINE_NODEBUG bool hasFeature(const FeatureId& featureId) const noexcept { return _features.has(featureId); }

  //! Adds the given CPU `featureId` to the list of features.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG void addFeature(Args&&... args) noexcept { return _features.add(std::forward<Args>(args)...); }

  //! Removes the given CPU `featureId` from the list of features.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG void removeFeature(Args&&... args) noexcept { return _features.remove(std::forward<Args>(args)...); }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_CPUINFO_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/emithelper.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/archtraits.h"
#include "../core/emithelper_p.h"
#include "../core/formatter.h"
#include "../core/funcargscontext_p.h"
#include "../core/radefs_p.h"

// Can be used for debugging...
// #define ASMJIT_DUMP_ARGS_ASSIGNMENT

ASMJIT_BEGIN_NAMESPACE

// BaseEmitHelper - Formatting
// ===========================

#ifdef ASMJIT_DUMP_ARGS_ASSIGNMENT
static void dumpFuncValue(String& sb, Arch arch, const FuncValue& value) noexcept {
  Formatter::formatTypeId(sb, value.typeId());
  sb.append('@');

  if (value.isIndirect())
    sb.append('[');

  if (value.isReg())
    Formatter::formatRegister(sb, 0, nullptr, arch, value.regType(), value.regId());
  else if (value.isStack())
    sb.appendFormat("[%d]", value.stackOffset());
  else
    sb.append("<none>");

  if (value.isIndirect())
    sb.append(']');
}

static void dumpAssignment(String& sb, const FuncArgsContext& ctx) noexcept {
  typedef FuncArgsContext::Var Var;

  Arch arch = ctx.arch();
  uint32_t varCount = ctx.varCount();

  for (uint32_t i = 0; i < varCount; i++) {
    const Var& var = ctx.var(i);
    const FuncValue& dst = var.out;
    const FuncValue& cur = var.cur;

    sb.appendFormat("Var%u: ", i);
    dumpFuncValue(sb, arch, dst);
    sb.append(" <- ");
    dumpFuncValue(sb, arch, cur);

    if (var.isDone())
      sb.append(" {Done}");

    sb.append('\n');
  }
}
#endif

// BaseEmitHelper - Abstract
// =========================

Error BaseEmitHelper::emitRegMove(const Operand_& dst_, const Operand_& src_, TypeId typeId, const char* comment) {
  DebugUtils::unused(dst_, src_, typeId, comment);
  return DebugUtils::errored(kErrorInvalidState);
}

Error BaseEmitHelper::emitRegSwap(const BaseReg& a, const BaseReg& b, const char* comment) {
  DebugUtils::unused(a, b, comment);
  return DebugUtils::errored(kErrorInvalidState);
}

Error BaseEmitHelper::emitArgMove(const BaseReg& dst_, TypeId dstTypeId, const Operand_& src_, TypeId srcTypeId, const char* comment) {
  DebugUtils::unused(dst_, dstTypeId, src_, srcTypeId, comment);
  return DebugUtils::errored(kErrorInvalidState);
}

// BaseEmitHelper - EmitArgsAssignment
// ===================================

ASMJIT_FAVOR_SIZE Error BaseEmitHelper::emitArgsAssignment(const FuncFrame& frame, const FuncArgsAssignment& args) {
  typedef FuncArgsContext::Var Var;
  typedef FuncArgsContext::WorkData WorkData;

  enum WorkFlags : uint32_t {
    kWorkNone      = 0x00,
    kWorkDidSome   = 0x01,
    kWorkPending   = 0x02,
    kWorkPostponed = 0x04
  };

  Arch arch = frame.arch();
  const ArchTraits& archTraits = ArchTraits::byArch(arch);

  RAConstraints constraints;
  FuncArgsContext ctx;

  ASMJIT_PROPAGATE(constraints.init(arch));
  ASMJIT_PROPAGATE(ctx.initWorkData(frame, args, &constraints));

#ifdef ASMJIT_DUMP_ARGS_ASSIGNMENT
  {
    String sb;
    dumpAssignment(sb, ctx);
    printf("%s\n", sb.data());
  }
#endif

  auto& workData = ctx._workData;
  uint32_t varCount = ctx._varCount;
  uint32_t saVarId = ctx._saVarId;

  BaseReg sp = BaseReg(_emitter->_gpSignature, archTraits.spRegId());
  BaseReg sa = sp;

  if (frame.hasDynamicAlignment()) {
    if (frame.hasPreservedFP())
      sa.setId(archTraits.fpRegId());
    else
      sa.setId(saVarId < varCount ? ctx._vars[saVarId].cur.regId() : frame.saRegId());
  }

  // Register to stack and stack to stack moves must be first as now we have
  // the biggest chance of having as many as possible unassigned registers.

  if (ctx._stackDstMask) {
    // Base address of all arguments passed by stack.
    BaseMem baseArgPtr(sa, int32_t(frame.saOffset(sa.id())));
    BaseMem baseStackPtr(sp, 0);

    for (uint32_t varId = 0; varId < varCount; varId++) {
      Var& var = ctx._vars[varId];

      if (!var.out.isStack())
        continue;

      FuncValue& cur = var.cur;
      FuncValue& out = var.out;

      ASMJIT_ASSERT(cur.isReg() || cur.isStack());
      BaseReg reg;

      BaseMem dstStackPtr = baseStackPtr.cloneAdjusted(out.stackOffset());
      BaseMem srcStackPtr = baseArgPtr.cloneAdjusted(cur.stackOffset());

      if (cur.isIndirect()) {
        if (cur.isStack()) {
          // TODO: Indirect stack.
          return DebugUtils::errored(kErrorInvalidAssignment);
        }
        else {
          srcStackPtr.setBaseId(cur.regId());
        }
      }

      if (cur.isReg() && !cur.isIndirect()) {
        WorkData& wd = workData[archTraits.regTypeToGroup(cur.regType())];
        uint32_t regId = cur.regId();

        reg.setSignatureAndId(archTraits.regTypeToSignature(cur.regType()), regId);
        wd.unassign(varId, regId);
      }
      else {
        // Stack to reg move - tricky since we move stack to stack we can decide which register to use. In general
        // we follow the rule that IntToInt moves will use GP regs with possibility to signature or zero extend,
        // and all other moves will either use GP or VEC regs depending on the size of the move.
        OperandSignature signature = getSuitableRegForMemToMemMove(arch, out.typeId(), cur.typeId());
        if (ASMJIT_UNLIKELY(!signature.isValid()))
          return DebugUtils::errored(kErrorInvalidState);

        WorkData& wd = workData[signature.regGroup()];
        RegMask availableRegs = wd.availableRegs();
        if (ASMJIT_UNLIKELY(!availableRegs))
          return DebugUtils::errored(kErrorInvalidState);

        uint32_t availableId = Support::ctz(availableRegs);
        reg.setSignatureAndId(signature, availableId);

        ASMJIT_PROPAGATE(emitArgMove(reg, out.typeId(), srcStackPtr, cur.typeId()));
      }

      if (cur.isIndirect() && cur.isReg())
        workData[RegGroup::kGp].unassign(varId, cur.regId());

      // Register to stack move.
      ASMJIT_PROPAGATE(emitRegMove(dstStackPtr, reg, cur.typeId()));
      var.markDone();
    }
  }

  // Shuffle all registers that are currently assigned accordingly to target assignment.

  uint32_t workFlags = kWorkNone;
  for (;;) {
    for (uint32_t varId = 0; varId < varCount; varId++) {
      Var& var = ctx._vars[varId];
      if (var.isDone() || !var.cur.isReg())
        continue;

      FuncValue& cur = var.cur;
      FuncValue& out = var.out;

      RegGroup curGroup = archTraits.regTypeToGroup(cur.regType());
      RegGroup outGroup = archTraits.regTypeToGroup(out.regType());

      uint32_t curId = cur.regId();
      uint32_t outId = out.regId();

      if (curGroup != outGroup) {
        // TODO: Conversion is not supported.
        return DebugUtils::errored(kErrorInvalidAssignment);
      }
      else {
        WorkData& wd = workData[outGroup];
        if (!wd.isAssigned(outId) || curId == outId) {
EmitMove:
          ASMJIT_PROPAGATE(
            emitArgMove(
              BaseReg(archTraits.regTypeToSignature(out.regType()), outId), out.typeId(),
              BaseReg(archTraits.regTypeToSignature(cur.regType()), curId), cur.typeId()));

          // Only reassign if this is not a sign/zero extension that happens on the same in/out register.
          if (curId != outId)
            wd.reassign(varId, outId, curId);

          cur.initReg(out.regType(), outId, out.typeId());

          if (outId == out.regId())
            var.markDone();
          workFlags |= kWorkDidSome | kWorkPending;
        }
        else {
          uint32_t altId = wd._physToVarId[outId];
          Var& altVar = ctx._vars[altId];

          if (!altVar.out.isInitialized() || (altVar.out.isReg() && altVar.out.regId() == curId)) {
            // Only few architectures provide swap operations, and only for few register groups.
            if (archTraits.hasInstRegSwap(curGroup)) {
              RegType highestType = Support::max(cur.regType(), altVar.cur.regType());
              if (Support::isBetween(highestType, RegType::kGp8Lo, RegType::kGp16))
                highestType = RegType::kGp32;

              OperandSignature signature = archTraits.regTypeToSignature(highestType);
              ASMJIT_PROPAGATE(
                emitRegSwap(BaseReg(signature, outId), BaseReg(signature, curId)));

              wd.swap(varId, curId, altId, outId);
              cur.setRegId(outId);
              var.markDone();
              altVar.cur.setRegId(curId);

              if (altVar.out.isInitialized())
                altVar.markDone();
              workFlags |= kWorkDidSome;
            }
            else {
              // If there is a scratch register it can be used to perform the swap.
              RegMask availableRegs = wd.availableRegs();
              if (availableRegs) {
                RegMask inOutRegs = wd.dstRegs();
                if (availableRegs & ~inOutRegs)
                  availableRegs &= ~inOutRegs;
                outId = Support::ctz(availableRegs);
                goto EmitMove;
              }
              else {
                workFlags |= kWorkPending;
              }
            }
          }
          else {
            workFlags |= kWorkPending;
          }
        }
      }
    }

    if (!(workFlags & kWorkPending))
      break;

    // If we did nothing twice it means that something is really broken.
    if ((workFlags & (kWorkDidSome | kWorkPostponed)) == kWorkPostponed)
      return DebugUtils::errored(kErrorInvalidState);

    workFlags = (workFlags & kWorkDidSome) ? kWorkNone : kWorkPostponed;
  }

  // Load arguments passed by stack into registers. This is pretty simple and
  // it never requires multiple iterations like the previous phase.

  if (ctx._hasStackSrc) {
    uint32_t iterCount = 1;
    if (frame.hasDynamicAlignment() && !frame.hasPreservedFP())
      sa.setId(saVarId < varCount ? ctx._vars[saVarId].cur.regId() : frame.saRegId());

    // Base address of all arguments passed by stack.
    BaseMem baseArgPtr(sa, int32_t(frame.saOffset(sa.id())));

    for (uint32_t iter = 0; iter < iterCount; iter++) {
      for (uint32_t varId = 0; varId < varCount; varId++) {
        Var& var = ctx._vars[varId];
        if (var.isDone())
          continue;

        if (var.cur.isStack()) {
          ASMJIT_ASSERT(var.out.isReg());

          uint32_t outId = var.out.regId();
          RegType outType = var.out.regType();

          RegGroup group = archTraits.regTypeToGroup(outType);
          WorkData& wd = workData[group];

          if (outId == sa.id() && group == RegGroup::kGp) {
            // This register will be processed last as we still need `saRegId`.
            if (iterCount == 1) {
              iterCount++;
              continue;
            }
            wd.unassign(wd._physToVarId[outId], outId);
          }

          BaseReg dstReg = BaseReg(archTraits.regTypeToSignature(outType), outId);
          BaseMem srcMem = baseArgPtr.cloneAdjusted(var.cur.stackOffset());

          ASMJIT_PROPAGATE(emitArgMove(
            dstReg, var.out.typeId(),
            srcMem, var.cur.typeId()));

          wd.assign(varId, outId);
          var.cur.initReg(outType, outId, var.cur.typeId(), FuncValue::kFlagIsDone);
        }
      }
    }
  }

  return kErrorOk;
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/emithelper_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_EMITHELPER_P_H_INCLUDED
#define ASMJIT_CORE_EMITHELPER_P_H_INCLUDED

#include "../core/emitter.h"
#include "../core/operand.h"
#include "../core/type.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_core
//! \{

//! Helper class that provides utilities for each supported architecture.
class BaseEmitHelper {
public:
  BaseEmitter* _emitter;

  ASMJIT_INLINE_NODEBUG explicit BaseEmitHelper(BaseEmitter* emitter = nullptr) noexcept
    : _emitter(emitter) {}

  ASMJIT_INLINE_NODEBUG virtual ~BaseEmitHelper() noexcept = default;

  ASMJIT_INLINE_NODEBUG BaseEmitter* emitter() const noexcept { return _emitter; }
  ASMJIT_INLINE_NODEBUG void setEmitter(BaseEmitter* emitter) noexcept { _emitter = emitter; }

  //! Emits a pure move operation between two registers or the same type or between a register and its home
  //! slot. This function does not handle register conversion.
  virtual Error emitRegMove(
    const Operand_& dst_,
    const Operand_& src_, TypeId typeId, const char* comment = nullptr);

  //! Emits swap between two registers.
  virtual Error emitRegSwap(
    const BaseReg& a,
    const BaseReg& b, const char* comment = nullptr);

  //! Emits move from a function argument (either register or stack) to a register.
  //!
  //! This function can handle the necessary conversion from one argument to another, and from one register type
  //! to another, if it's possible. Any attempt of conversion that requires third register of a different group
  //! (for example conversion from K to MMX on X86/X64) will fail.
  virtual Error emitArgMove(
    const BaseReg& dst_, TypeId dstTypeId,
    const Operand_& src_, TypeId srcTypeId, const char* comment = nullptr);

  Error emitArgsAssignment(const FuncFrame& frame, const FuncArgsAssignment& args);
};

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_EMITHELPER_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/emitter.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/emitterutils_p.h"
#include "../core/errorhandler.h"
#include "../core/logger.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// BaseEmitter - Construction & Destruction
// ========================================

BaseEmitter::BaseEmitter(EmitterType emitterType) noexcept
  : _emitterType(emitterType) {}

BaseEmitter::~BaseEmitter() noexcept {
  if (_code) {
    _addEmitterFlags(EmitterFlags::kDestroyed);
    _code->detach(this);
  }
}

// BaseEmitter - Finalize
// ======================

Error BaseEmitter::finalize() {
  // Does nothing by default, overridden by `BaseBuilder` and `BaseCompiler`.
  return kErrorOk;
}

// BaseEmitter - Internals
// =======================

static constexpr EmitterFlags kEmitterPreservedFlags = EmitterFlags::kOwnLogger | EmitterFlags::kOwnErrorHandler;

static ASMJIT_NOINLINE void BaseEmitter_updateForcedOptions(BaseEmitter* self) noexcept {
  bool emitComments = false;
  bool hasDiagnosticOptions = false;

  if (self->emitterType() == EmitterType::kAssembler) {
    // Assembler: Don't emit comments if logger is not attached.
    emitComments = self->_code != nullptr && self->_logger != nullptr;
    hasDiagnosticOptions = self->hasDiagnosticOption(DiagnosticOptions::kValidateAssembler);
  }
  else {
    // Builder/Compiler: Always emit comments, we cannot assume they won't be used.
    emitComments = self->_code != nullptr;
    hasDiagnosticOptions = self->hasDiagnosticOption(DiagnosticOptions::kValidateIntermediate);
  }

  if (emitComments)
    self->_addEmitterFlags(EmitterFlags::kLogComments);
  else
    self->_clearEmitterFlags(EmitterFlags::kLogComments);

  // The reserved option tells emitter (Assembler/Builder/Compiler) that there may be either a border
  // case (CodeHolder not attached, for example) or that logging or validation is required.
  if (self->_code == nullptr || self->_logger || hasDiagnosticOptions)
    self->_forcedInstOptions |= InstOptions::kReserved;
  else
    self->_forcedInstOptions &= ~InstOptions::kReserved;
}

// BaseEmitter - Diagnostic Options
// ================================

void BaseEmitter::addDiagnosticOptions(DiagnosticOptions options) noexcept {
  _diagnosticOptions |= options;
  BaseEmitter_updateForcedOptions(this);
}

void BaseEmitter::clearDiagnosticOptions(DiagnosticOptions options) noexcept {
  _diagnosticOptions &= ~options;
  BaseEmitter_updateForcedOptions(this);
}

// BaseEmitter - Logging
// =====================

void BaseEmitter::setLogger(Logger* logger) noexcept {
#ifndef ASMJIT_NO_LOGGING
  if (logger) {
    _logger = logger;
    _addEmitterFlags(EmitterFlags::kOwnLogger);
  }
  else {
    _logger = nullptr;
    _clearEmitterFlags(EmitterFlags::kOwnLogger);
    if (_code)
      _logger = _code->logger();
  }
  BaseEmitter_updateForcedOptions(this);
#else
  DebugUtils::unused(logger);
#endif
}

// BaseEmitter - Error Handling
// ============================

void BaseEmitter::setErrorHandler(ErrorHandler* errorHandler) noexcept {
  if (errorHandler) {
    _errorHandler = errorHandler;
    _addEmitterFlags(EmitterFlags::kOwnErrorHandler);
  }
  else {
    _errorHandler = nullptr;
    _clearEmitterFlags(EmitterFlags::kOwnErrorHandler);
    if (_code)
      _errorHandler = _code->errorHandler();
  }
}

Error BaseEmitter::reportError(Error err, const char* message) {
  ErrorHandler* eh = _errorHandler;
  if (eh) {
    if (!message)
      message = DebugUtils::errorAsString(err);
    eh->handleError(err, message, this);
  }
  return err;
}

// BaseEmitter - Sections
// ======================

// [[pure virtual]]
Error BaseEmitter::section(Section* section) {
  DebugUtils::unused(section);
  return DebugUtils::errored(kErrorInvalidState);
}

// BaseEmitter - Labels
// ====================

// [[pure virtual]]
Label BaseEmitter::newLabel() {
  return Label(Globals::kInvalidId);
}

// [[pure virtual]]
Label BaseEmitter::newNamedLabel(const char* name, size_t nameSize, LabelType type, uint32_t parentId) {
  DebugUtils::unused(name, nameSize, type, parentId);
  return Label(Globals::kInvalidId);
}

Label BaseEmitter::labelByName(const char* name, size_t nameSize, uint32_t parentId) noexcept {
  return Label(_code ? _code->labelIdByName(name, nameSize, parentId) : Globals::kInvalidId);
}

// [[pure virtual]]
Error BaseEmitter::bind(const Label& label) {
  DebugUtils::unused(label);
  return DebugUtils::errored(kErrorInvalidState);
}

bool BaseEmitter::isLabelValid(uint32_t labelId) const noexcept {
  return _code && labelId < _code->labelCount();
}

// BaseEmitter - Emit (Low-Level)
// ==============================

using EmitterUtils::noExt;

Error BaseEmitter::_emitI(InstId instId) {
  return _emit(instId, noExt[0], noExt[1], noExt[2], noExt);
}

Error BaseEmitter::_emitI(InstId instId, const Operand_& o0) {
  return _emit(instId, o0, noExt[1], noExt[2], noExt);
}

Error BaseEmitter::_emitI(InstId instId, const Operand_& o0, const Operand_& o1) {
  return _emit(instId, o0, o1, noExt[2], noExt);
}

Error BaseEmitter::_emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2) {
  return _emit(instId, o0, o1, o2, noExt);
}

Error BaseEmitter::_emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_& o3) {
  Operand_ opExt[3] = { o3 };
  return _emit(instId, o0, o1, o2, opExt);
}

Error BaseEmitter::_emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_& o3, const Operand_& o4) {
  Operand_ opExt[3] = { o3, o4 };
  return _emit(instId, o0, o1, o2, opExt);
}

Error BaseEmitter::_emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_& o3, const Operand_& o4, const Operand_& o5) {
  Operand_ opExt[3] = { o3, o4, o5 };
  return _emit(instId, o0, o1, o2, opExt);
}

// [[pure virtual]]
Error BaseEmitter::_emit(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* oExt) {
  DebugUtils::unused(instId, o0, o1, o2, oExt);
  return DebugUtils::errored(kErrorInvalidState);
}

Error BaseEmitter::_emitOpArray(InstId instId, const Operand_* operands, size_t opCount) {
  const Operand_* op = operands;
  Operand_ opExt[3];

  switch (opCount) {
    case 0:
      return _emit(instId, noExt[0], noExt[1], noExt[2], noExt);

    case 1:
      return _emit(instId, op[0], noExt[1], noExt[2], noExt);

    case 2:
      return _emit(instId, op[0], op[1], noExt[2], noExt);

    case 3:
      return _emit(instId, op[0], op[1], op[2], noExt);

    case 4:
      opExt[0] = op[3];
      opExt[1].reset();
      opExt[2].reset();
      return _emit(instId, op[0], op[1], op[2], opExt);

    case 5:
      opExt[0] = op[3];
      opExt[1] = op[4];
      opExt[2].reset();
      return _emit(instId, op[0], op[1], op[2], opExt);

    case 6:
      return _emit(instId, op[0], op[1], op[2], op + 3);

    default:
      return DebugUtils::errored(kErrorInvalidArgument);
  }
}

// BaseEmitter - Emit Utilities
// ============================

Error BaseEmitter::emitProlog(const FuncFrame& frame) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  return _funcs.emitProlog(this, frame);
}

Error BaseEmitter::emitEpilog(const FuncFrame& frame) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  return _funcs.emitEpilog(this, frame);
}

Error BaseEmitter::emitArgsAssignment(const FuncFrame& frame, const FuncArgsAssignment& args) {
  if (ASMJIT_UNLIKELY(!_code))
    return DebugUtils::errored(kErrorNotInitialized);

  return _funcs.emitArgsAssignment(this, frame, args);
}

// BaseEmitter - Align
// ===================

// [[pure virtual]]
Error BaseEmitter::align(AlignMode alignMode, uint32_t alignment) {
  DebugUtils::unused(alignMode, alignment);
  return DebugUtils::errored(kErrorInvalidState);
}

// BaseEmitter - Embed
// ===================

// [[pure virtual]]
Error BaseEmitter::embed(const void* data, size_t dataSize) {
  DebugUtils::unused(data, dataSize);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseEmitter::embedDataArray(TypeId typeId, const void* data, size_t itemCount, size_t repeatCount) {
  DebugUtils::unused(typeId, data, itemCount, repeatCount);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseEmitter::embedConstPool(const Label& label, const ConstPool& pool) {
  DebugUtils::unused(label, pool);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseEmitter::embedLabel(const Label& label, size_t dataSize) {
  DebugUtils::unused(label, dataSize);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseEmitter::embedLabelDelta(const Label& label, const Label& base, size_t dataSize) {
  DebugUtils::unused(label, base, dataSize);
  return DebugUtils::errored(kErrorInvalidState);
}

// BaseEmitter - Comment
// =====================

// [[pure virtual]]
Error BaseEmitter::comment(const char* data, size_t size) {
  DebugUtils::unused(data, size);
  return DebugUtils::errored(kErrorInvalidState);
}

Error BaseEmitter::commentf(const char* fmt, ...) {
  if (!hasEmitterFlag(EmitterFlags::kLogComments)) {
    if (!hasEmitterFlag(EmitterFlags::kAttached))
      return reportError(DebugUtils::errored(kErrorNotInitialized));
    return kErrorOk;
  }

#ifndef ASMJIT_NO_LOGGING
  StringTmp<1024> sb;

  va_list ap;
  va_start(ap, fmt);
  Error err = sb.appendVFormat(fmt, ap);
  va_end(ap);

  ASMJIT_PROPAGATE(err);
  return comment(sb.data(), sb.size());
#else
  DebugUtils::unused(fmt);
  return kErrorOk;
#endif
}

Error BaseEmitter::commentv(const char* fmt, va_list ap) {
  if (!hasEmitterFlag(EmitterFlags::kLogComments)) {
    if (!hasEmitterFlag(EmitterFlags::kAttached))
      return reportError(DebugUtils::errored(kErrorNotInitialized));
    return kErrorOk;
  }

#ifndef ASMJIT_NO_LOGGING
  StringTmp<1024> sb;
  Error err = sb.appendVFormat(fmt, ap);

  ASMJIT_PROPAGATE(err);
  return comment(sb.data(), sb.size());
#else
  DebugUtils::unused(fmt, ap);
  return kErrorOk;
#endif
}

// BaseEmitter - Events
// ====================

Error BaseEmitter::onAttach(CodeHolder* code) noexcept {
  _code = code;
  _environment = code->environment();
  _addEmitterFlags(EmitterFlags::kAttached);

  const ArchTraits& archTraits = ArchTraits::byArch(code->arch());
  RegType nativeRegType = Environment::is32Bit(code->arch()) ? RegType::kGp32 : RegType::kGp64;
  _gpSignature = archTraits.regTypeToSignature(nativeRegType);

  onSettingsUpdated();
  return kErrorOk;
}

Error BaseEmitter::onDetach(CodeHolder* code) noexcept {
  DebugUtils::unused(code);

  if (!hasOwnLogger())
    _logger = nullptr;

  if (!hasOwnErrorHandler())
    _errorHandler = nullptr;

  _clearEmitterFlags(~kEmitterPreservedFlags);
  _instructionAlignment = uint8_t(0);
  _forcedInstOptions = InstOptions::kReserved;
  _privateData = 0;

  _environment.reset();
  _gpSignature.reset();

  _instOptions = InstOptions::kNone;
  _extraReg.reset();
  _inlineComment = nullptr;
  _funcs.reset();

  return kErrorOk;
}

void BaseEmitter::onSettingsUpdated() noexcept {
  // Only called when attached to CodeHolder by CodeHolder.
  ASMJIT_ASSERT(_code != nullptr);

  if (!hasOwnLogger())
    _logger = _code->logger();

  if (!hasOwnErrorHandler())
    _errorHandler = _code->errorHandler();

  BaseEmitter_updateForcedOptions(this);
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/emitter.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_EMITTER_H_INCLUDED
#define ASMJIT_CORE_EMITTER_H_INCLUDED

#include "../core/archtraits.h"
#include "../core/codeholder.h"
#include "../core/formatter.h"
#include "../core/inst.h"
#include "../core/operand.h"
#include "../core/type.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

class ConstPool;
class FuncFrame;
class FuncArgsAssignment;

//! Align mode, used by \ref BaseEmitter::align().
enum class AlignMode : uint8_t {
  //! Align executable code.
  kCode = 0,
  //! Align non-executable code.
  kData = 1,
  //! Align by a sequence of zeros.
  kZero = 2,

  //! Maximum value of `AlignMode`.
  kMaxValue = kZero
};

//! Emitter type used by \ref BaseEmitter.
enum class EmitterType : uint8_t {
  //! Unknown or uninitialized.
  kNone = 0,
  //! Emitter inherits from \ref BaseAssembler.
  kAssembler = 1,
  //! Emitter inherits from \ref BaseBuilder.
  kBuilder = 2,
  //! Emitter inherits from \ref BaseCompiler.
  kCompiler = 3,

  //! Maximum value of `EmitterType`.
  kMaxValue = kCompiler
};

//! Emitter flags, used by \ref BaseEmitter.
enum class EmitterFlags : uint8_t {
  //! No flags.
  kNone = 0u,
  //! Emitter is attached to CodeHolder.
  kAttached = 0x01u,
  //! The emitter must emit comments.
  kLogComments = 0x08u,
  //! The emitter has its own \ref Logger (not propagated from \ref CodeHolder).
  kOwnLogger = 0x10u,
  //! The emitter has its own \ref ErrorHandler (not propagated from \ref CodeHolder).
  kOwnErrorHandler = 0x20u,
  //! The emitter was finalized.
  kFinalized = 0x40u,
  //! The emitter was destroyed.
  //!
  //! This flag is used for a very short time when an emitter is being destroyed by
  //! CodeHolder.
  kDestroyed = 0x80u
};
ASMJIT_DEFINE_ENUM_FLAGS(EmitterFlags)

//! Encoding options.
enum class EncodingOptions : uint32_t {
  //! No encoding options.
  kNone = 0,

  //! Emit instructions that are optimized for size, if possible.
  //!
  //! Default: false.
  //!
  //! X86 Specific
  //! ------------
  //!
  //! When this option is set it the assembler will try to fix instructions if possible into operation equivalent
  //! instructions that take less bytes by taking advantage of implicit zero extension. For example instruction
  //! like `mov r64, imm` and `and r64, imm` can be translated to `mov r32, imm` and `and r32, imm` when the
  //! immediate constant is lesser than `2^31`.
  kOptimizeForSize = 0x00000001u,

  //! Emit optimized code-alignment sequences.
  //!
  //! Default: false.
  //!
  //! X86 Specific
  //! ------------
  //!
  //! Default align sequence used by X86 architecture is one-byte (0x90) opcode that is often shown by disassemblers
  //! as NOP. However there are more optimized align sequences for 2-11 bytes that may execute faster on certain CPUs.
  //! If this feature is enabled AsmJit will generate specialized sequences for alignment between 2 to 11 bytes.
  kOptimizedAlign = 0x00000002u,

  //! Emit jump-prediction hints.
  //!
  //! Default: false.
  //!
  //! X86 Specific
  //! ------------
  //!
  //! Jump prediction is usually based on the direction of the jump. If the jump is backward it is usually predicted as
  //! taken; and if the jump is forward it is usually predicted as not-taken. The reason is that loops generally use
  //! backward jumps and conditions usually use forward jumps. However this behavior can be overridden by using
  //! instruction prefixes. If this option is enabled these hints will be emitted.
  //!
  //! This feature is disabled by default, because the only processor that used to take into consideration prediction
  //! hints was P4. Newer processors implement heuristics for branch prediction and ignore static hints. This means
  //! that this feature can be only used for annotation purposes.
  kPredictedJumps = 0x00000010u
};
ASMJIT_DEFINE_ENUM_FLAGS(EncodingOptions)

//! Diagnostic options are used to tell emitters and their passes to perform diagnostics when emitting or processing
//! user code. These options control validation and extra diagnostics that can be performed by higher level emitters.
//!
//! Instruction Validation
//! ----------------------
//!
//! \ref BaseAssembler implementation perform by default only basic checks that are necessary to identify all
//! variations of an instruction so the correct encoding can be selected. This is fine for production-ready code
//! as the assembler doesn't have to perform checks that would slow it down. However, sometimes these checks are
//! beneficial especially when the project that uses AsmJit is in a development phase, in which mistakes happen
//! often. To make the experience of using AsmJit seamless it offers validation features that can be controlled
//! by \ref DiagnosticOptions.
//!
//! Compiler Diagnostics
//! --------------------
//!
//! Diagnostic options work with \ref BaseCompiler passes (precisely with its register allocation pass). These options
//! can be used to enable logging of all operations that the Compiler does.
enum class DiagnosticOptions : uint32_t {
  //! No validation options.
  kNone = 0,

  //! Perform strict validation in \ref BaseAssembler::emit() implementations.
  //!
  //! This flag ensures that each instruction is checked before it's encoded into a binary representation. This flag
  //! is only relevant for \ref BaseAssembler implementations, but can be set in any other emitter type, in that case
  //! if that emitter needs to create an assembler on its own, for the purpose of \ref BaseEmitter::finalize() it
  //! would propagate this flag to such assembler so all instructions passed to it are explicitly validated.
  //!
  //! Default: false.
  kValidateAssembler = 0x00000001u,

  //! Perform strict validation in \ref BaseBuilder::emit() and \ref BaseCompiler::emit() implementations.
  //!
  //! This flag ensures that each instruction is checked before an \ref InstNode representing the instruction is
  //! created by \ref BaseBuilder or \ref BaseCompiler. This option could be more useful than \ref kValidateAssembler
  //! in cases in which there is an invalid instruction passed to an assembler, which was invalid much earlier, most
  //! likely when such instruction was passed to Builder/Compiler.
  //!
  //! This is a separate option that was introduced, because it's possible to manipulate the instruction stream
  //! emitted by \ref BaseBuilder and \ref BaseCompiler - this means that it's allowed to emit invalid instructions
  //! (for example with missing operands) that will be fixed later before finalizing it.
  //!
  //! Default: false.
  kValidateIntermediate = 0x00000002u,

  //! Annotate all nodes processed by register allocator (Compiler/RA).
  //!
  //! \note Annotations don't need debug options, however, some debug options like `kRADebugLiveness` may influence
  //! their output (for example the mentioned option would add liveness information to per-instruction annotation).
  kRAAnnotate = 0x00000080u,

  //! Debug CFG generation and other related algorithms / operations (Compiler/RA).
  kRADebugCFG = 0x00000100u,

  //! Debug liveness analysis (Compiler/RA).
  kRADebugLiveness = 0x00000200u,

  //! Debug register allocation assignment (Compiler/RA).
  kRADebugAssignment = 0x00000400u,

  //! Debug the removal of code part of unreachable blocks.
  kRADebugUnreachable = 0x00000800u,

  //! Enable all debug options (Compiler/RA).
  kRADebugAll = 0x0000FF00u,
};
ASMJIT_DEFINE_ENUM_FLAGS(DiagnosticOptions)

//! Provides a base foundation to emitting code - specialized by \ref BaseAssembler and \ref BaseBuilder.
class ASMJIT_VIRTAPI BaseEmitter {
public:
  ASMJIT_BASE_CLASS(BaseEmitter)
  ASMJIT_NONCOPYABLE(BaseEmitter)

  //! \name Members
  //! \{

  //! See \ref EmitterType.
  EmitterType _emitterType = EmitterType::kNone;
  //! See \ref EmitterFlags.
  EmitterFlags _emitterFlags = EmitterFlags::kNone;
  //! Instruction alignment.
  uint8_t _instructionAlignment = 0u;
  //! \cond
  uint8_t _reservedBaseEmitter = 0u;
  //! \endcond
  //! Validation flags in case validation is used.
  //!
  //! \note Validation flags are specific to the emitter and they are setup at construction time and then never
  //! changed.
  ValidationFlags _validationFlags = ValidationFlags::kNone;
  //! Validation options.
  DiagnosticOptions _diagnosticOptions = DiagnosticOptions::kNone;

  //! All supported architectures in a bit-mask, where LSB is the bit with a zero index.
  uint64_t _archMask = 0;

  //! Encoding options.
  EncodingOptions _encodingOptions = EncodingOptions::kNone;

  //! Forced instruction options, combined with \ref _instOptions by \ref emit().
  InstOptions _forcedInstOptions = InstOptions::kReserved;
  //! Internal private data used freely by any emitter.
  uint32_t _privateData = 0;

  //! CodeHolder the emitter is attached to.
  CodeHolder* _code = nullptr;
  //! Attached \ref Logger.
  Logger* _logger = nullptr;
  //! Attached \ref ErrorHandler.
  ErrorHandler* _errorHandler = nullptr;

  //! Describes the target environment, matches \ref CodeHolder::environment().
  Environment _environment {};
  //! Native GP register signature and signature related information.
  OperandSignature _gpSignature {};

  //! Emitter state that can be used to specify options and inline comment of a next node or instruction.
  struct State {
    InstOptions options;
    RegOnly extraReg;
    const char* comment;
  };

  //! Next instruction options (affects the next instruction).
  InstOptions _instOptions = InstOptions::kNone;
  //! Extra register (op-mask {k} on AVX-512) (affects the next instruction).
  RegOnly _extraReg {};
  //! Inline comment of the next instruction (affects the next instruction).
  const char* _inlineComment = nullptr;

  //! Function callbacks used by emitter implementation.
  //!
  //! These are typically shared between Assembler/Builder/Compiler of a single backend.
  struct Funcs {
    typedef Error (ASMJIT_CDECL* EmitProlog)(BaseEmitter* emitter, const FuncFrame& frame);
    typedef Error (ASMJIT_CDECL* EmitEpilog)(BaseEmitter* emitter, const FuncFrame& frame);
    typedef Error (ASMJIT_CDECL* EmitArgsAssignment)(BaseEmitter* emitter, const FuncFrame& frame, const FuncArgsAssignment& args);

    typedef Error (ASMJIT_CDECL* FormatInstruction)(
      String& sb,
      FormatFlags formatFlags,
      const BaseEmitter* emitter,
      Arch arch,
      const BaseInst& inst, const Operand_* operands, size_t opCount) ASMJIT_NOEXCEPT_TYPE;

    typedef Error (ASMJIT_CDECL* ValidateFunc)(const BaseInst& inst, const Operand_* operands, size_t opCount, ValidationFlags validationFlags) ASMJIT_NOEXCEPT_TYPE;

    //! Emit prolog implementation.
    EmitProlog emitProlog;
    //! Emit epilog implementation.
    EmitEpilog emitEpilog;
    //! Emit arguments assignment implementation.
    EmitArgsAssignment emitArgsAssignment;
    //! Instruction formatter implementation.
    FormatInstruction formatInstruction;
    //! Instruction validation implementation.
    ValidateFunc validate;

    //! Resets all functions to nullptr.
    ASMJIT_INLINE_NODEBUG void reset() noexcept {
      emitProlog = nullptr;
      emitEpilog = nullptr;
      emitArgsAssignment = nullptr;
      validate = nullptr;
    }
  };

  Funcs _funcs {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_API explicit BaseEmitter(EmitterType emitterType) noexcept;
  ASMJIT_API virtual ~BaseEmitter() noexcept;

  //! \}

  //! \name Cast
  //! \{

  template<typename T>
  ASMJIT_INLINE_NODEBUG T* as() noexcept { return reinterpret_cast<T*>(this); }

  template<typename T>
  ASMJIT_INLINE_NODEBUG const T* as() const noexcept { return reinterpret_cast<const T*>(this); }

  //! \}

  //! \name Emitter Type & Flags
  //! \{

  //! Returns the type of this emitter, see `EmitterType`.
  ASMJIT_INLINE_NODEBUG EmitterType emitterType() const noexcept { return _emitterType; }
  //! Returns emitter flags , see `Flags`.
  ASMJIT_INLINE_NODEBUG EmitterFlags emitterFlags() const noexcept { return _emitterFlags; }

  //! Tests whether the emitter inherits from `BaseAssembler`.
  ASMJIT_INLINE_NODEBUG bool isAssembler() const noexcept { return _emitterType == EmitterType::kAssembler; }
  //! Tests whether the emitter inherits from `BaseBuilder`.
  //!
  //! \note Both Builder and Compiler emitters would return `true`.
  ASMJIT_INLINE_NODEBUG bool isBuilder() const noexcept { return uint32_t(_emitterType) >= uint32_t(EmitterType::kBuilder); }
  //! Tests whether the emitter inherits from `BaseCompiler`.
  ASMJIT_INLINE_NODEBUG bool isCompiler() const noexcept { return _emitterType == EmitterType::kCompiler; }

  //! Tests whether the emitter has the given `flag` enabled.
  ASMJIT_INLINE_NODEBUG bool hasEmitterFlag(EmitterFlags flag) const noexcept { return Support::test(_emitterFlags, flag); }
  //! Tests whether the emitter is finalized.
  ASMJIT_INLINE_NODEBUG bool isFinalized() const noexcept { return hasEmitterFlag(EmitterFlags::kFinalized); }
  //! Tests whether the emitter is destroyed (only used during destruction).
  ASMJIT_INLINE_NODEBUG bool isDestroyed() const noexcept { return hasEmitterFlag(EmitterFlags::kDestroyed); }

  //! \}

  //! \cond INTERNAL
  //! \name Internal Functions
  //! \{

  ASMJIT_INLINE_NODEBUG void _addEmitterFlags(EmitterFlags flags) noexcept { _emitterFlags |= flags; }
  ASMJIT_INLINE_NODEBUG void _clearEmitterFlags(EmitterFlags flags) noexcept { _emitterFlags &= _emitterFlags & ~flags; }

  //! \}
  //! \endcond

  //! \name Target Information
  //! \{

  //! Returns the CodeHolder this emitter is attached to.
  ASMJIT_INLINE_NODEBUG CodeHolder* code() const noexcept { return _code; }

  //! Returns the target environment.
  //!
  //! The returned \ref Environment reference matches \ref CodeHolder::environment().
  ASMJIT_INLINE_NODEBUG const Environment& environment() const noexcept { return _environment; }

  //! Tests whether the target architecture is 32-bit.
  ASMJIT_INLINE_NODEBUG bool is32Bit() const noexcept { return environment().is32Bit(); }
  //! Tests whether the target architecture is 64-bit.
  ASMJIT_INLINE_NODEBUG bool is64Bit() const noexcept { return environment().is64Bit(); }

  //! Returns the target architecture type.
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return environment().arch(); }
  //! Returns the target architecture sub-type.
  ASMJIT_INLINE_NODEBUG SubArch subArch() const noexcept { return environment().subArch(); }

  //! Returns the target architecture's GP register size (4 or 8 bytes).
  ASMJIT_INLINE_NODEBUG uint32_t registerSize() const noexcept { return environment().registerSize(); }

  //! Returns a signature of a native general purpose register (either 32-bit or 64-bit depending on the architecture).
  ASMJIT_INLINE_NODEBUG OperandSignature gpSignature() const noexcept { return _gpSignature; }

  //! Returns instruction alignment.
  //!
  //! The following values are returned based on the target architecture:
  //!   - X86 and X86_64 - instruction alignment is 1
  //!   - AArch32 - instruction alignment is 4 in A32 mode and 2 in THUMB mode.
  //!   - AArch64 - instruction alignment is 4
  ASMJIT_INLINE_NODEBUG uint32_t instructionAlignment() const noexcept { return _instructionAlignment; }

  //! \}

  //! \name Initialization & Finalization
  //! \{

  //! Tests whether the emitter is initialized (i.e. attached to \ref CodeHolder).
  ASMJIT_INLINE_NODEBUG bool isInitialized() const noexcept { return _code != nullptr; }

  //! Finalizes this emitter.
  //!
  //! Materializes the content of the emitter by serializing it to the attached \ref CodeHolder through an architecture
  //! specific \ref BaseAssembler. This function won't do anything if the emitter inherits from \ref BaseAssembler as
  //! assemblers emit directly to a \ref CodeBuffer held by \ref CodeHolder. However, if this is an emitter that
  //! inherits from \ref BaseBuilder or \ref BaseCompiler then these emitters need the materialization phase as they
  //! store their content in a representation not visible to \ref CodeHolder.
  ASMJIT_API virtual Error finalize();

  //! \}

  //! \name Logging
  //! \{

  //! Tests whether the emitter has a logger.
  ASMJIT_INLINE_NODEBUG bool hasLogger() const noexcept { return _logger != nullptr; }

  //! Tests whether the emitter has its own logger.
  //!
  //! Own logger means that it overrides the possible logger that may be used by \ref CodeHolder this emitter is
  //! attached to.
  ASMJIT_INLINE_NODEBUG bool hasOwnLogger() const noexcept { return hasEmitterFlag(EmitterFlags::kOwnLogger); }

  //! Returns the logger this emitter uses.
  //!
  //! The returned logger is either the emitter's own logger or it's logger used by \ref CodeHolder this emitter
  //! is attached to.
  ASMJIT_INLINE_NODEBUG Logger* logger() const noexcept { return _logger; }

  //! Sets or resets the logger of the emitter.
  //!
  //! If the `logger` argument is non-null then the logger will be considered emitter's own logger, see \ref
  //! hasOwnLogger() for more details. If the given `logger` is null then the emitter will automatically use logger
  //! that is attached to the \ref CodeHolder this emitter is attached to.
  ASMJIT_API void setLogger(Logger* logger) noexcept;

  //! Resets the logger of this emitter.
  //!
  //! The emitter will bail to using a logger attached to \ref CodeHolder this emitter is attached to, or no logger
  //! at all if \ref CodeHolder doesn't have one.
  ASMJIT_INLINE_NODEBUG void resetLogger() noexcept { return setLogger(nullptr); }

  //! \}

  //! \name Error Handling
  //! \{

  //! Tests whether the emitter has an error handler attached.
  ASMJIT_INLINE_NODEBUG bool hasErrorHandler() const noexcept { return _errorHandler != nullptr; }

  //! Tests whether the emitter has its own error handler.
  //!
  //! Own error handler means that it overrides the possible error handler that may be used by \ref CodeHolder this
  //! emitter is attached to.
  ASMJIT_INLINE_NODEBUG bool hasOwnErrorHandler() const noexcept { return hasEmitterFlag(EmitterFlags::kOwnErrorHandler); }

  //! Returns the error handler this emitter uses.
  //!
  //! The returned error handler is either the emitter's own error handler or it's error handler used by
  //! \ref CodeHolder this emitter is attached to.
  ASMJIT_INLINE_NODEBUG ErrorHandler* errorHandler() const noexcept { return _errorHandler; }

  //! Sets or resets the error handler of the emitter.
  ASMJIT_API void setErrorHandler(ErrorHandler* errorHandler) noexcept;

  //! Resets the error handler.
  ASMJIT_INLINE_NODEBUG void resetErrorHandler() noexcept { setErrorHandler(nullptr); }

  //! Handles the given error in the following way:
  //!   1. If the emitter has \ref ErrorHandler attached, it calls its \ref ErrorHandler::handleError() member function
  //!      first, and then returns the error. The `handleError()` function may throw.
  //!   2. if the emitter doesn't have \ref ErrorHandler, the error is simply returned.
  ASMJIT_API Error reportError(Error err, const char* message = nullptr);

  //! \}

  //! \name Encoding Options
  //! \{

  //! Returns encoding options.
  ASMJIT_INLINE_NODEBUG EncodingOptions encodingOptions() const noexcept { return _encodingOptions; }
  //! Tests whether the encoding `option` is set.
  ASMJIT_INLINE_NODEBUG bool hasEncodingOption(EncodingOptions option) const noexcept { return Support::test(_encodingOptions, option); }

  //! Enables the given encoding `options`.
  ASMJIT_INLINE_NODEBUG void addEncodingOptions(EncodingOptions options) noexcept { _encodingOptions |= options; }
  //! Disables the given encoding `options`.
  ASMJIT_INLINE_NODEBUG void clearEncodingOptions(EncodingOptions options) noexcept { _encodingOptions &= ~options; }

  //! \}

  //! \name Diagnostic Options
  //! \{

  //! Returns the emitter's diagnostic options.
  ASMJIT_INLINE_NODEBUG DiagnosticOptions diagnosticOptions() const noexcept { return _diagnosticOptions; }

  //! Tests whether the given `option` is present in the emitter's diagnostic options.
  ASMJIT_INLINE_NODEBUG bool hasDiagnosticOption(DiagnosticOptions option) const noexcept { return Support::test(_diagnosticOptions, option); }

  //! Activates the given diagnostic `options`.
  //!
  //! This function is used to activate explicit validation options that will be then used by all emitter
  //! implementations. There are in general two possibilities:
  //!
  //!   - Architecture specific assembler is used. In this case a \ref DiagnosticOptions::kValidateAssembler can be
  //!     used to turn on explicit validation that will be used before an instruction is emitted. This means that
  //!     internally an extra step will be performed to make sure that the instruction is correct. This is needed,
  //!     because by default assemblers prefer speed over strictness.
  //!
  //!     This option should be used in debug builds as it's pretty expensive.
  //!
  //!   - Architecture specific builder or compiler is used. In this case the user can turn on
  //!     \ref DiagnosticOptions::kValidateIntermediate option that adds explicit validation step before the Builder
  //!     or Compiler creates an \ref InstNode to represent an emitted instruction. Error will be returned if the
  //!     instruction is ill-formed. In addition, also \ref DiagnosticOptions::kValidateAssembler can be used, which
  //!     would not be consumed by Builder / Compiler directly, but it would be propagated to an architecture specific
  //!     \ref BaseAssembler implementation it creates during \ref BaseEmitter::finalize().
  ASMJIT_API void addDiagnosticOptions(DiagnosticOptions options) noexcept;

  //! Deactivates the given validation `options`.
  //!
  //! See \ref addDiagnosticOptions() and \ref DiagnosticOptions for more details.
  ASMJIT_API void clearDiagnosticOptions(DiagnosticOptions options) noexcept;

  //! \}

  //! \name Instruction Options
  //! \{

  //! Returns forced instruction options.
  //!
  //! Forced instruction options are merged with next instruction options before the instruction is encoded. These
  //! options have some bits reserved that are used by error handling, logging, and instruction validation purposes.
  //! Other options are globals that affect each instruction.
  ASMJIT_INLINE_NODEBUG InstOptions forcedInstOptions() const noexcept { return _forcedInstOptions; }

  //! Returns options of the next instruction.
  ASMJIT_INLINE_NODEBUG InstOptions instOptions() const noexcept { return _instOptions; }
  //! Returns options of the next instruction.
  ASMJIT_INLINE_NODEBUG void setInstOptions(InstOptions options) noexcept { _instOptions = options; }
  //! Adds options of the next instruction.
  ASMJIT_INLINE_NODEBUG void addInstOptions(InstOptions options) noexcept { _instOptions |= options; }
  //! Resets options of the next instruction.
  ASMJIT_INLINE_NODEBUG void resetInstOptions() noexcept { _instOptions = InstOptions::kNone; }

  //! Tests whether the extra register operand is valid.
  ASMJIT_INLINE_NODEBUG bool hasExtraReg() const noexcept { return _extraReg.isReg(); }
  //! Returns an extra operand that will be used by the next instruction (architecture specific).
  ASMJIT_INLINE_NODEBUG const RegOnly& extraReg() const noexcept { return _extraReg; }
  //! Sets an extra operand that will be used by the next instruction (architecture specific).
  ASMJIT_INLINE_NODEBUG void setExtraReg(const BaseReg& reg) noexcept { _extraReg.init(reg); }
  //! Sets an extra operand that will be used by the next instruction (architecture specific).
  ASMJIT_INLINE_NODEBUG void setExtraReg(const RegOnly& reg) noexcept { _extraReg.init(reg); }
  //! Resets an extra operand that will be used by the next instruction (architecture specific).
  ASMJIT_INLINE_NODEBUG void resetExtraReg() noexcept { _extraReg.reset(); }

  //! Returns comment/annotation of the next instruction.
  ASMJIT_INLINE_NODEBUG const char* inlineComment() const noexcept { return _inlineComment; }
  //! Sets comment/annotation of the next instruction.
  //!
  //! \note This string is set back to null by `_emit()`, but until that it has to remain valid as the Emitter is not
  //! required to make a copy of it (and it would be slow to do that for each instruction).
  ASMJIT_INLINE_NODEBUG void setInlineComment(const char* s) noexcept { _inlineComment = s; }
  //! Resets the comment/annotation to nullptr.
  ASMJIT_INLINE_NODEBUG void resetInlineComment() noexcept { _inlineComment = nullptr; }

  //! \}

  //! \name Emitter State
  //! \{

  //! Resets the emitter state, which contains instruction options, extra register, and inline comment.
  //!
  //! Emitter can have a state that describes instruction options and extra register used by the instruction. Most
  //! instructions don't need nor use the state, however, if an instruction uses a prefix such as REX or REP prefix,
  //! which is set explicitly, then the state would contain it. This allows to mimic the syntax of assemblers such
  //! as X86. For example `rep().movs(...)` would map to a `REP MOVS` instuction on X86. The same applies to various
  //! hints and the use of a mask register in AVX-512 mode.
  ASMJIT_INLINE_NODEBUG void resetState() noexcept {
    resetInstOptions();
    resetExtraReg();
    resetInlineComment();
  }

  //! \cond INTERNAL

  //! Grabs the current emitter state and resets the emitter state at the same time, returning the state the emitter
  //! had before the state was reset.
  ASMJIT_INLINE_NODEBUG State _grabState() noexcept {
    State s{_instOptions | _forcedInstOptions, _extraReg, _inlineComment};
    resetState();
    return s;
  }
  //! \endcond

  //! \}

  //! \name Sections
  //! \{

  //! Switches the given `section`.
  //!
  //! Once switched, everything is added to the given `section`.
  ASMJIT_API virtual Error section(Section* section);

  //! \}

  //! \name Labels
  //! \{

  //! Creates a new label.
  ASMJIT_API virtual Label newLabel();
  //! Creates a new named label.
  ASMJIT_API virtual Label newNamedLabel(const char* name, size_t nameSize = SIZE_MAX, LabelType type = LabelType::kGlobal, uint32_t parentId = Globals::kInvalidId);

  //! Creates a new anonymous label with a name, which can only be used for debugging purposes.
  ASMJIT_INLINE_NODEBUG Label newAnonymousLabel(const char* name, size_t nameSize = SIZE_MAX) { return newNamedLabel(name, nameSize, LabelType::kAnonymous); }
  //! Creates a new external label.
  ASMJIT_INLINE_NODEBUG Label newExternalLabel(const char* name, size_t nameSize = SIZE_MAX) { return newNamedLabel(name, nameSize, LabelType::kExternal); }

  //! Returns `Label` by `name`.
  //!
  //! Returns invalid Label in case that the name is invalid or label was not found.
  //!
  //! \note This function doesn't trigger ErrorHandler in case the name is invalid or no such label exist. You must
  //! always check the validity of the `Label` returned.
  ASMJIT_API Label labelByName(const char* name, size_t nameSize = SIZE_MAX, uint32_t parentId = Globals::kInvalidId) noexcept;

  //! Binds the `label` to the current position of the current section.
  //!
  //! \note Attempt to bind the same label multiple times will return an error.
  ASMJIT_API virtual Error bind(const Label& label);

  //! Tests whether the label `id` is valid (i.e. registered).
  ASMJIT_API bool isLabelValid(uint32_t labelId) const noexcept;
  //! Tests whether the `label` is valid (i.e. registered).
  ASMJIT_INLINE_NODEBUG bool isLabelValid(const Label& label) const noexcept { return isLabelValid(label.id()); }

  //! \}

  //! \name Emit
  //! \{

  // NOTE: These `emit()` helpers are designed to address a code-bloat generated by C++ compilers to call a function
  // having many arguments. Each parameter to `_emit()` requires some code to pass it, which means that if we default
  // to 5 arguments in `_emit()` and instId the C++ compiler would have to generate a virtual function call having 5
  // parameters and additional `this` argument, which is quite a lot. Since by default most instructions have 2 to 3
  // operands it's better to introduce helpers that pass from 0 to 6 operands that help to reduce the size of emit(...)
  // function call.

  //! Emits an instruction (internal).
  ASMJIT_API Error _emitI(InstId instId);
  //! \overload
  ASMJIT_API Error _emitI(InstId instId, const Operand_& o0);
  //! \overload
  ASMJIT_API Error _emitI(InstId instId, const Operand_& o0, const Operand_& o1);
  //! \overload
  ASMJIT_API Error _emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2);
  //! \overload
  ASMJIT_API Error _emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_& o3);
  //! \overload
  ASMJIT_API Error _emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_& o3, const Operand_& o4);
  //! \overload
  ASMJIT_API Error _emitI(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_& o3, const Operand_& o4, const Operand_& o5);

  //! Emits an instruction `instId` with the given `operands`.
  //!
  //! This is the most universal way of emitting code, which accepts an instruction identifier and instruction
  //! operands. This is called an "unchecked" API as emit doesn't provide any type checks at compile-time. This
  //! allows to emit instruction with just \ref Operand instances, which could be handy in some cases - for
  //! example emitting generic code where you don't know whether some operand is register, memory, or immediate.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG Error emit(InstId instId, Args&&... operands) {
    return _emitI(instId, Support::ForwardOp<Args>::forward(operands)...);
  }

  //! Similar to \ref emit(), but uses array of `operands` instead.
  ASMJIT_INLINE_NODEBUG Error emitOpArray(InstId instId, const Operand_* operands, size_t opCount) {
    return _emitOpArray(instId, operands, opCount);
  }

  //! Similar to \ref emit(), but emits instruction with both instruction options and extra register, followed
  //! by an array of `operands`.
  ASMJIT_FORCE_INLINE Error emitInst(const BaseInst& inst, const Operand_* operands, size_t opCount) {
    setInstOptions(inst.options());
    setExtraReg(inst.extraReg());
    return _emitOpArray(inst.id(), operands, opCount);
  }

  //! \}

  //! \cond INTERNAL
  //! \name Emit Internals
  //! \{

  //! Emits an instruction - all 6 operands must be defined.
  ASMJIT_API virtual Error _emit(InstId instId, const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* oExt);
  //! Emits instruction having operands stored in array.
  ASMJIT_API virtual Error _emitOpArray(InstId instId, const Operand_* operands, size_t opCount);

  //! \}
  //! \endcond

  //! \name Emit Utilities
  //! \{

  //! Emits a function prolog described by the given function `frame`.
  ASMJIT_API Error emitProlog(const FuncFrame& frame);
  //! Emits a function epilog described by the given function `frame`.
  ASMJIT_API Error emitEpilog(const FuncFrame& frame);
  //! Emits code that reassigns function `frame` arguments to the given `args`.
  ASMJIT_API Error emitArgsAssignment(const FuncFrame& frame, const FuncArgsAssignment& args);

  //! \}

  //! \name Align
  //! \{

  //! Aligns the current CodeBuffer position to the `alignment` specified.
  //!
  //! The sequence that is used to fill the gap between the aligned location and the current location depends on the
  //! align `mode`, see \ref AlignMode. The `alignment` argument specifies alignment in bytes, so for example when
  //! it's `32` it means that the code buffer will be aligned to `32` bytes.
  ASMJIT_API virtual Error align(AlignMode alignMode, uint32_t alignment);

  //! \}

  //! \name Embed
  //! \{

  //! Embeds raw data into the \ref CodeBuffer.
  ASMJIT_API virtual Error embed(const void* data, size_t dataSize);

  //! Embeds a typed data array.
  //!
  //! This is the most flexible function for embedding data as it allows to:
  //!
  //!   - Assign a `typeId` to the data, so the emitter knows the type of items stored in `data`. Binary data should
  //!     use \ref TypeId::kUInt8.
  //!
  //!   - Repeat the given data `repeatCount` times, so the data can be used as a fill pattern for example, or as a
  //!     pattern used by SIMD instructions.
  ASMJIT_API virtual Error embedDataArray(TypeId typeId, const void* data, size_t itemCount, size_t repeatCount = 1);

  //! Embeds int8_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedInt8(int8_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kInt8, &value, 1, repeatCount); }
  //! Embeds uint8_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedUInt8(uint8_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kUInt8, &value, 1, repeatCount); }
  //! Embeds int16_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedInt16(int16_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kInt16, &value, 1, repeatCount); }
  //! Embeds uint16_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedUInt16(uint16_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kUInt16, &value, 1, repeatCount); }
  //! Embeds int32_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedInt32(int32_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kInt32, &value, 1, repeatCount); }
  //! Embeds uint32_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedUInt32(uint32_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kUInt32, &value, 1, repeatCount); }
  //! Embeds int64_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedInt64(int64_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kInt64, &value, 1, repeatCount); }
  //! Embeds uint64_t `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedUInt64(uint64_t value, size_t repeatCount = 1) { return embedDataArray(TypeId::kUInt64, &value, 1, repeatCount); }
  //! Embeds a floating point `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedFloat(float value, size_t repeatCount = 1) { return embedDataArray(TypeId(TypeUtils::TypeIdOfT<float>::kTypeId), &value, 1, repeatCount); }
  //! Embeds a floating point `value` repeated by `repeatCount`.
  ASMJIT_INLINE_NODEBUG Error embedDouble(double value, size_t repeatCount = 1) { return embedDataArray(TypeId(TypeUtils::TypeIdOfT<double>::kTypeId), &value, 1, repeatCount); }

  //! Embeds a constant pool at the current offset by performing the following:
  //!   1. Aligns by using AlignMode::kData to the minimum `pool` alignment.
  //!   2. Binds the ConstPool label so it's bound to an aligned location.
  //!   3. Emits ConstPool content.
  ASMJIT_API virtual Error embedConstPool(const Label& label, const ConstPool& pool);

  //! Embeds an absolute `label` address as data.
  //!
  //! The `dataSize` is an optional argument that can be used to specify the size of the address data. If it's zero
  //! (default) the address size is deduced from the target architecture (either 4 or 8 bytes).
  ASMJIT_API virtual Error embedLabel(const Label& label, size_t dataSize = 0);

  //! Embeds a delta (distance) between the `label` and `base` calculating it as `label - base`. This function was
  //! designed to make it easier to embed lookup tables where each index is a relative distance of two labels.
  ASMJIT_API virtual Error embedLabelDelta(const Label& label, const Label& base, size_t dataSize = 0);

  //! \}

  //! \name Comment
  //! \{

  //! Emits a comment stored in `data` with an optional `size` parameter.
  ASMJIT_API virtual Error comment(const char* data, size_t size = SIZE_MAX);

  //! Emits a formatted comment specified by `fmt` and variable number of arguments.
  ASMJIT_API Error commentf(const char* fmt, ...);
  //! Emits a formatted comment specified by `fmt` and `ap`.
  ASMJIT_API Error commentv(const char* fmt, va_list ap);

  //! \}

  //! \name Events
  //! \{

  //! Called after the emitter was attached to `CodeHolder`.
  ASMJIT_API virtual Error onAttach(CodeHolder* ASMJIT_NONNULL(code)) noexcept;
  //! Called after the emitter was detached from `CodeHolder`.
  ASMJIT_API virtual Error onDetach(CodeHolder* ASMJIT_NONNULL(code)) noexcept;

  //! Called when \ref CodeHolder has updated an important setting, which involves the following:
  //!
  //!   - \ref Logger has been changed (\ref CodeHolder::setLogger() has been called).
  //!
  //!   - \ref ErrorHandler has been changed (\ref CodeHolder::setErrorHandler() has been called).
  //!
  //! This function ensures that the settings are properly propagated from \ref CodeHolder to the emitter.
  //!
  //! \note This function is virtual and can be overridden, however, if you do so, always call \ref
  //! BaseEmitter::onSettingsUpdated() within your own implementation to ensure that the emitter is
  //! in a consistent state.
  ASMJIT_API virtual void onSettingsUpdated() noexcept;

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_EMITTER_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/emitterutils.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/assembler.h"
#include "../core/emitterutils_p.h"
#include "../core/formatter_p.h"
#include "../core/logger.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

namespace EmitterUtils {

#ifndef ASMJIT_NO_LOGGING

Error finishFormattedLine(String& sb, const FormatOptions& formatOptions, const uint8_t* binData, size_t binSize, size_t offsetSize, size_t immSize, const char* comment) noexcept {
  ASMJIT_ASSERT(binSize >= offsetSize);
  const size_t kNoBinSize = SIZE_MAX;

  size_t commentSize = comment ? Support::strLen(comment, Globals::kMaxCommentSize) : 0;

  if ((binSize != 0 && binSize != kNoBinSize) || commentSize) {
    char sep = ';';
    size_t padding = Formatter::paddingFromOptions(formatOptions, FormatPaddingGroup::kRegularLine);

    for (size_t i = (binSize == kNoBinSize); i < 2; i++) {
      ASMJIT_PROPAGATE(sb.padEnd(padding));

      if (sep) {
        ASMJIT_PROPAGATE(sb.append(sep));
        ASMJIT_PROPAGATE(sb.append(' '));
      }

      // Append binary data or comment.
      if (i == 0) {
        ASMJIT_PROPAGATE(sb.appendHex(binData, binSize - offsetSize - immSize));
        ASMJIT_PROPAGATE(sb.appendChars('.', offsetSize * 2));
        ASMJIT_PROPAGATE(sb.appendHex(binData + binSize - immSize, immSize));
        if (commentSize == 0) break;
      }
      else {
        ASMJIT_PROPAGATE(sb.append(comment, commentSize));
      }

      sep = '|';
      padding += Formatter::paddingFromOptions(formatOptions, FormatPaddingGroup::kMachineCode);
    }
  }

  return sb.append('\n');
}

void logLabelBound(BaseAssembler* self, const Label& label) noexcept {
  Logger* logger = self->logger();

  StringTmp<512> sb;
  size_t binSize = logger->hasFlag(FormatFlags::kMachineCode) ? size_t(0) : SIZE_MAX;

  sb.appendChars(' ', logger->indentation(FormatIndentationGroup::kLabel));
  Formatter::formatLabel(sb, logger->flags(), self, label.id());
  sb.append(':');
  finishFormattedLine(sb, logger->options(), nullptr, binSize, 0, 0, self->_inlineComment);
  logger->log(sb.data(), sb.size());
}

void logInstructionEmitted(
  BaseAssembler* self,
  InstId instId,
  InstOptions options,
  const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt,
  uint32_t relSize, uint32_t immSize, uint8_t* afterCursor) {

  Logger* logger = self->logger();
  ASMJIT_ASSERT(logger != nullptr);

  StringTmp<256> sb;
  FormatFlags formatFlags = logger->flags();

  uint8_t* beforeCursor = self->bufferPtr();
  intptr_t emittedSize = (intptr_t)(afterCursor - beforeCursor);

  Operand_ opArray[Globals::kMaxOpCount];
  opArrayFromEmitArgs(opArray, o0, o1, o2, opExt);

  sb.appendChars(' ', logger->indentation(FormatIndentationGroup::kCode));
  self->_funcs.formatInstruction(sb, formatFlags, self, self->arch(), BaseInst(instId, options, self->extraReg()), opArray, Globals::kMaxOpCount);

  if (Support::test(formatFlags, FormatFlags::kMachineCode))
    finishFormattedLine(sb, logger->options(), self->bufferPtr(), size_t(emittedSize), relSize, immSize, self->inlineComment());
  else
    finishFormattedLine(sb, logger->options(), nullptr, SIZE_MAX, 0, 0, self->inlineComment());
  logger->log(sb);
}

Error logInstructionFailed(
  BaseEmitter* self,
  Error err,
  InstId instId,
  InstOptions options,
  const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt) {

  StringTmp<256> sb;
  sb.append(DebugUtils::errorAsString(err));
  sb.append(": ");

  Operand_ opArray[Globals::kMaxOpCount];
  opArrayFromEmitArgs(opArray, o0, o1, o2, opExt);

  self->_funcs.formatInstruction(sb, FormatFlags::kRegType, self, self->arch(), BaseInst(instId, options, self->extraReg()), opArray, Globals::kMaxOpCount);

  if (self->inlineComment()) {
    sb.append(" ; ");
    sb.append(self->inlineComment());
  }

  self->resetState();
  return self->reportError(err, sb.data());
}

#endif

} // {EmitterUtils}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/emitterutils_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_EMITTERUTILS_P_H_INCLUDED
#define ASMJIT_CORE_EMITTERUTILS_P_H_INCLUDED

#include "../core/emitter.h"
#include "../core/operand.h"

ASMJIT_BEGIN_NAMESPACE

class BaseAssembler;
class FormatOptions;

//! \cond INTERNAL
//! \addtogroup asmjit_core
//! \{

//! Utilities used by various emitters, mostly Assembler implementations.
namespace EmitterUtils {

//! Default paddings used by Emitter utils and Formatter.

static constexpr Operand noExt[3] = { {}, {}, {} };

enum kOpIndex : uint32_t {
  kOp3 = 0,
  kOp4 = 1,
  kOp5 = 2
};

static ASMJIT_FORCE_INLINE uint32_t opCountFromEmitArgs(const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt) noexcept {
  uint32_t opCount = 0;

  if (opExt[kOp3].isNone()) {
    if (!o0.isNone()) opCount = 1;
    if (!o1.isNone()) opCount = 2;
    if (!o2.isNone()) opCount = 3;
  }
  else {
    opCount = 4;
    if (!opExt[kOp4].isNone()) {
      opCount = 5 + uint32_t(!opExt[kOp5].isNone());
    }
  }

  return opCount;
}

static ASMJIT_FORCE_INLINE void opArrayFromEmitArgs(Operand_ dst[Globals::kMaxOpCount], const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt) noexcept {
  dst[0].copyFrom(o0);
  dst[1].copyFrom(o1);
  dst[2].copyFrom(o2);
  dst[3].copyFrom(opExt[kOp3]);
  dst[4].copyFrom(opExt[kOp4]);
  dst[5].copyFrom(opExt[kOp5]);
}

#ifndef ASMJIT_NO_LOGGING
Error finishFormattedLine(String& sb, const FormatOptions& formatOptions, const uint8_t* binData, size_t binSize, size_t offsetSize, size_t immSize, const char* comment) noexcept;

void logLabelBound(BaseAssembler* self, const Label& label) noexcept;

void logInstructionEmitted(
  BaseAssembler* self,
  InstId instId,
  InstOptions options,
  const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt,
  uint32_t relSize, uint32_t immSize, uint8_t* afterCursor);

Error logInstructionFailed(
  BaseEmitter* self,
  Error err,
  InstId instId,
  InstOptions options,
  const Operand_& o0, const Operand_& o1, const Operand_& o2, const Operand_* opExt);
#endif

}

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_EMITTERUTILS_P_H_INCLUDED


```

`CodeCleaner/asmjit/asmjit/core/environment.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/environment.h"

ASMJIT_BEGIN_NAMESPACE

// X86 Target
// ----------
//
//   - 32-bit - Linux, OSX, BSD, and apparently also Haiku guarantee 16-byte
//              stack alignment. Other operating systems are assumed to have
//              4-byte alignment by default for safety reasons.
//   - 64-bit - stack must be aligned to 16 bytes.
//
// ARM Target
// ----------
//
//   - 32-bit - Stack must be aligned to 8 bytes.
//   - 64-bit - Stack must be aligned to 16 bytes (hardware requirement).
uint32_t Environment::stackAlignment() const noexcept {
  if (is64Bit()) {
    // Assume 16-byte alignment on any 64-bit target.
    return 16;
  }
  else {
    // The following platforms use 16-byte alignment in 32-bit mode.
    if (isPlatformLinux() ||
        isPlatformBSD() ||
        isPlatformApple() ||
        isPlatformHaiku()) {
      return 16u;
    }

    if (isFamilyARM())
      return 8;

    // Bail to 4-byte alignment if we don't know.
    return 4;
  }
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/environment.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ENVIRONMENT_H_INCLUDED
#define ASMJIT_CORE_ENVIRONMENT_H_INCLUDED

#include "../core/archtraits.h"

#if defined(__APPLE__)
  #include <TargetConditionals.h>
#endif

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

//! Vendor.
//!
//! \note AsmJit doesn't use vendor information at the moment. It's provided for future use, if required.
enum class Vendor : uint8_t {
  //! Unknown or uninitialized platform vendor.
  kUnknown = 0,

  //! Maximum value of `Vendor`.
  kMaxValue = kUnknown,

  //! Platform vendor detected at compile-time.
  kHost =
#if defined(_DOXYGEN)
    DETECTED_AT_COMPILE_TIME
#else
    kUnknown
#endif
};

//! Platform - runtime environment or operating system.
enum class Platform : uint8_t {
  //! Unknown or uninitialized platform.
  kUnknown = 0,

  //! Windows OS.
  kWindows,

  //! Other platform that is not Windows, most likely POSIX based.
  kOther,

  //! Linux OS.
  kLinux,
  //! GNU/Hurd OS.
  kHurd,

  //! FreeBSD OS.
  kFreeBSD,
  //! OpenBSD OS.
  kOpenBSD,
  //! NetBSD OS.
  kNetBSD,
  //! DragonFly BSD OS.
  kDragonFlyBSD,

  //! Haiku OS.
  kHaiku,

  //! Apple OSX.
  kOSX,
  //! Apple iOS.
  kIOS,
  //! Apple TVOS.
  kTVOS,
  //! Apple WatchOS.
  kWatchOS,

  //! Emscripten platform.
  kEmscripten,

  //! Maximum value of `Platform`.
  kMaxValue = kEmscripten,

  //! Platform detected at compile-time (platform of the host).
  kHost =
#if defined(_DOXYGEN)
    DETECTED_AT_COMPILE_TIME
#elif defined(__EMSCRIPTEN__)
    kEmscripten
#elif defined(_WIN32)
    kWindows
#elif defined(__linux__)
    kLinux
#elif defined(__gnu_hurd__)
    kHurd
#elif defined(__FreeBSD__)
    kFreeBSD
#elif defined(__OpenBSD__)
    kOpenBSD
#elif defined(__NetBSD__)
    kNetBSD
#elif defined(__DragonFly__)
    kDragonFlyBSD
#elif defined(__HAIKU__)
    kHaiku
#elif defined(__APPLE__) && TARGET_OS_OSX
    kOSX
#elif defined(__APPLE__) && TARGET_OS_TV
    kTVOS
#elif defined(__APPLE__) && TARGET_OS_WATCH
    kWatchOS
#elif defined(__APPLE__) && TARGET_OS_IPHONE
    kIOS
#else
    kOther
#endif
};

//! Platform ABI (application binary interface).
enum class PlatformABI : uint8_t {
  //! Unknown or uninitialized environment.
  kUnknown = 0,
  //! Microsoft ABI.
  kMSVC,
  //! GNU ABI.
  kGNU,
  //! Android Environment / ABI.
  kAndroid,
  //! Cygwin ABI.
  kCygwin,
  //! Darwin ABI.
  kDarwin,

  //! Maximum value of `PlatformABI`.
  kMaxValue,

  //! Host ABI detected at compile-time.
  kHost =
#if defined(_DOXYGEN)
    DETECTED_AT_COMPILE_TIME
#elif defined(_MSC_VER)
    kMSVC
#elif defined(__CYGWIN__)
    kCygwin
#elif defined(__MINGW32__) || defined(__GLIBC__)
    kGNU
#elif defined(__ANDROID__)
    kAndroid
#elif defined(__APPLE__)
    kDarwin
#else
    kUnknown
#endif
};

//! Floating point ABI (ARM).
enum class FloatABI : uint8_t {
  kHardFloat = 0,
  kSoftFloat,

  kHost =
#if ASMJIT_ARCH_ARM == 32 && defined(__SOFTFP__)
  kSoftFloat
#else
  kHardFloat
#endif
};

//! Object format.
//!
//! \note AsmJit doesn't really use anything except \ref ObjectFormat::kUnknown and \ref ObjectFormat::kJIT at
//! the moment. Object file formats are provided for future extensibility and a possibility to generate object
//! files at some point.
enum class ObjectFormat : uint8_t {
  //! Unknown or uninitialized object format.
  kUnknown = 0,

  //! JIT code generation object, most likely \ref JitRuntime or a custom
  //! \ref Target implementation.
  kJIT,

  //! Executable and linkable format (ELF).
  kELF,
  //! Common object file format.
  kCOFF,
  //! Extended COFF object format.
  kXCOFF,
  //! Mach object file format.
  kMachO,

  //! Maximum value of `ObjectFormat`.
  kMaxValue
};

//! Represents an environment, which is usually related to a \ref Target.
//!
//! Environment has usually an 'arch-subarch-vendor-os-abi' format, which is sometimes called "Triple" (historically
//! it used to be 3 only parts) or "Tuple", which is a convention used by Debian Linux.
//!
//! AsmJit doesn't support all possible combinations or architectures and ABIs, however, it models the environment
//! similarly to other compilers for future extensibility.
class Environment {
public:
  //! \name Members
  //! \{

  //! Architecture.
  Arch _arch = Arch::kUnknown;
  //! Sub-architecture type.
  SubArch _subArch = SubArch::kUnknown;
  //! Vendor type.
  Vendor _vendor = Vendor::kUnknown;
  //! Platform.
  Platform _platform = Platform::kUnknown;
  //! Platform ABI.
  PlatformABI _platformABI = PlatformABI::kUnknown;
  //! Object format.
  ObjectFormat _objectFormat = ObjectFormat::kUnknown;
  //! Floating point ABI.
  FloatABI _floatABI = FloatABI::kHardFloat;
  //! Reserved for future use, must be zero.
  uint8_t _reserved = 0;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a default initialized environment (all values either unknown or set to safe defaults).
  ASMJIT_INLINE_NODEBUG constexpr Environment() noexcept = default;
  //! Creates a copy of `other` instance.
  ASMJIT_INLINE_NODEBUG constexpr Environment(const Environment& other) noexcept = default;

  //! Creates \ref Environment initialized to `arch`, `subArch`, `vendor`, `platform`, `platformABI`, `objectFormat`,
  //! and `floatABI`.
  ASMJIT_INLINE_NODEBUG constexpr explicit Environment(
    Arch arch,
    SubArch subArch = SubArch::kUnknown,
    Vendor vendor = Vendor::kUnknown,
    Platform platform = Platform::kUnknown,
    PlatformABI platformABI = PlatformABI::kUnknown,
    ObjectFormat objectFormat = ObjectFormat::kUnknown,
    FloatABI floatABI = FloatABI::kHardFloat) noexcept
    : _arch(arch),
      _subArch(subArch),
      _vendor(vendor),
      _platform(platform),
      _platformABI(platformABI),
      _objectFormat(objectFormat),
      _floatABI(floatABI) {}

  //! Returns the host environment constructed from preprocessor macros defined by the compiler.
  //!
  //! The returned environment should precisely match the target host architecture, sub-architecture, platform,
  //! and ABI.
  static ASMJIT_INLINE_NODEBUG Environment host() noexcept {
    return Environment(Arch::kHost, SubArch::kHost, Vendor::kHost, Platform::kHost, PlatformABI::kHost, ObjectFormat::kUnknown, FloatABI::kHost);
  }

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG Environment& operator=(const Environment& other) noexcept = default;

  ASMJIT_INLINE_NODEBUG bool operator==(const Environment& other) const noexcept { return  equals(other); }
  ASMJIT_INLINE_NODEBUG bool operator!=(const Environment& other) const noexcept { return !equals(other); }

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the environment is not set up.
  //!
  //! Returns true if all members are zero, and thus unknown.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept {
    // Unfortunately compilers won't optimize fields are checked one by one...
    return _packed() == 0;
  }

  //! Tests whether the environment is initialized, which means it must have
  //! a valid architecture.
  ASMJIT_INLINE_NODEBUG bool isInitialized() const noexcept {
    return _arch != Arch::kUnknown;
  }

  ASMJIT_INLINE_NODEBUG uint64_t _packed() const noexcept {
    uint64_t x;
    memcpy(&x, this, 8);
    return x;
  }

  //! Resets all members of the environment to zero / unknown.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = Environment{}; }

  //! Tests whether this environment is equal to `other`.
  ASMJIT_INLINE_NODEBUG bool equals(const Environment& other) const noexcept { return _packed() == other._packed(); }

  //! Returns the architecture.
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return _arch; }
  //! Returns the sub-architecture.
  ASMJIT_INLINE_NODEBUG SubArch subArch() const noexcept { return _subArch; }
  //! Returns vendor.
  ASMJIT_INLINE_NODEBUG Vendor vendor() const noexcept { return _vendor; }
  //! Returns target's platform or operating system.
  ASMJIT_INLINE_NODEBUG Platform platform() const noexcept { return _platform; }
  //! Returns target's ABI.
  ASMJIT_INLINE_NODEBUG PlatformABI platformABI() const noexcept { return _platformABI; }
  //! Returns target's object format.
  ASMJIT_INLINE_NODEBUG ObjectFormat objectFormat() const noexcept { return _objectFormat; }
  //! Returns floating point ABI.
  ASMJIT_INLINE_NODEBUG FloatABI floatABI() const noexcept { return _floatABI; }

  //! Initializes \ref Environment to `arch`, `subArch`, `vendor`, `platform`, `platformABI`, `objectFormat`,
  //! and `floatABI`.
  inline void init(
    Arch arch,
    SubArch subArch = SubArch::kUnknown,
    Vendor vendor = Vendor::kUnknown,
    Platform platform = Platform::kUnknown,
    PlatformABI platformABI = PlatformABI::kUnknown,
    ObjectFormat objectFormat = ObjectFormat::kUnknown,
    FloatABI floatABI = FloatABI::kHardFloat) noexcept {

    _arch = arch;
    _subArch = subArch;
    _vendor = vendor;
    _platform = platform;
    _platformABI = platformABI;
    _objectFormat = objectFormat;
    _floatABI = floatABI;
    _reserved = 0;
  }

  //! Tests whether this environment describes a 32-bit X86.
  ASMJIT_INLINE_NODEBUG bool isArchX86() const noexcept { return _arch == Arch::kX86; }
  //! Tests whether this environment describes a 64-bit X86.
  ASMJIT_INLINE_NODEBUG bool isArchX64() const noexcept { return _arch == Arch::kX64; }
  //! Tests whether this environment describes a 32-bit ARM.
  ASMJIT_INLINE_NODEBUG bool isArchARM() const noexcept { return isArchARM(_arch); }
  //! Tests whether this environment describes a 32-bit ARM in THUMB mode.
  ASMJIT_INLINE_NODEBUG bool isArchThumb() const noexcept { return isArchThumb(_arch); }
  //! Tests whether this environment describes a 64-bit X86.
  ASMJIT_INLINE_NODEBUG bool isArchAArch64() const noexcept { return isArchAArch64(_arch); }
  //! Tests whether this environment describes a 32-bit MIPS.
  ASMJIT_INLINE_NODEBUG bool isArchMIPS32() const noexcept { return isArchMIPS32(_arch); }
  //! Tests whether this environment describes a 64-bit MIPS.
  ASMJIT_INLINE_NODEBUG bool isArchMIPS64() const noexcept { return isArchMIPS64(_arch); }
  //! Tests whether this environment describes a 32-bit RISC-V.
  ASMJIT_INLINE_NODEBUG bool isArchRISCV32() const noexcept { return _arch == Arch::kRISCV32; }
  //! Tests whether this environment describes a 64-bit RISC-V.
  ASMJIT_INLINE_NODEBUG bool isArchRISCV64() const noexcept { return _arch == Arch::kRISCV64; }

  //! Tests whether the architecture is 32-bit.
  ASMJIT_INLINE_NODEBUG bool is32Bit() const noexcept { return is32Bit(_arch); }
  //! Tests whether the architecture is 64-bit.
  ASMJIT_INLINE_NODEBUG bool is64Bit() const noexcept { return is64Bit(_arch); }

  //! Tests whether the architecture is little endian.
  ASMJIT_INLINE_NODEBUG bool isLittleEndian() const noexcept { return isLittleEndian(_arch); }
  //! Tests whether the architecture is big endian.
  ASMJIT_INLINE_NODEBUG bool isBigEndian() const noexcept { return isBigEndian(_arch); }

  //! Tests whether this architecture is of X86 family.
  ASMJIT_INLINE_NODEBUG bool isFamilyX86() const noexcept { return isFamilyX86(_arch); }
  //! Tests whether this architecture family is ARM, THUMB, or AArch64.
  ASMJIT_INLINE_NODEBUG bool isFamilyARM() const noexcept { return isFamilyARM(_arch); }
  //! Tests whether this architecture family is AArch32 (ARM or THUMB).
  ASMJIT_INLINE_NODEBUG bool isFamilyAArch32() const noexcept { return isFamilyAArch32(_arch); }
  //! Tests whether this architecture family is AArch64.
  ASMJIT_INLINE_NODEBUG bool isFamilyAArch64() const noexcept { return isFamilyAArch64(_arch); }
  //! Tests whether this architecture family is MISP or MIPS64.
  ASMJIT_INLINE_NODEBUG bool isFamilyMIPS() const noexcept { return isFamilyMIPS(_arch); }
  //! Tests whether this architecture family is RISC-V (both 32-bit and 64-bit).
  ASMJIT_INLINE_NODEBUG bool isFamilyRISCV() const noexcept { return isFamilyRISCV(_arch); }

  //! Tests whether the environment platform is Windows.
  ASMJIT_INLINE_NODEBUG bool isPlatformWindows() const noexcept { return _platform == Platform::kWindows; }
  //! Tests whether the environment platform is Linux.
  ASMJIT_INLINE_NODEBUG bool isPlatformLinux() const noexcept { return _platform == Platform::kLinux; }
  //! Tests whether the environment platform is Hurd.
  ASMJIT_INLINE_NODEBUG bool isPlatformHurd() const noexcept { return _platform == Platform::kHurd; }
  //! Tests whether the environment platform is Haiku.
  ASMJIT_INLINE_NODEBUG bool isPlatformHaiku() const noexcept { return _platform == Platform::kHaiku; }

  //! Tests whether the environment platform is any BSD.
  ASMJIT_INLINE_NODEBUG bool isPlatformBSD() const noexcept {
    return _platform == Platform::kFreeBSD ||
           _platform == Platform::kOpenBSD ||
           _platform == Platform::kNetBSD ||
           _platform == Platform::kDragonFlyBSD;
  }

  //! Tests whether the environment platform is any Apple platform (OSX, iOS, TVOS, WatchOS).
  ASMJIT_INLINE_NODEBUG bool isPlatformApple() const noexcept {
    return _platform == Platform::kOSX ||
           _platform == Platform::kIOS ||
           _platform == Platform::kTVOS ||
           _platform == Platform::kWatchOS;
  }

  //! Tests whether the ABI is MSVC.
  ASMJIT_INLINE_NODEBUG bool isMSVC() const noexcept { return _platformABI == PlatformABI::kMSVC; }
  //! Tests whether the ABI is GNU.
  ASMJIT_INLINE_NODEBUG bool isGNU() const noexcept { return _platformABI == PlatformABI::kGNU; }
  //! Tests whether the ABI is GNU.
  ASMJIT_INLINE_NODEBUG bool isDarwin() const noexcept { return _platformABI == PlatformABI::kDarwin; }

  //! Returns a calculated stack alignment for this environment.
  ASMJIT_API uint32_t stackAlignment() const noexcept;

  //! Returns a native register size of this architecture.
  ASMJIT_INLINE_NODEBUG uint32_t registerSize() const noexcept { return registerSizeFromArch(_arch); }

  //! Sets the architecture to `arch`.
  ASMJIT_INLINE_NODEBUG void setArch(Arch arch) noexcept { _arch = arch; }
  //! Sets the sub-architecture to `subArch`.
  ASMJIT_INLINE_NODEBUG void setSubArch(SubArch subArch) noexcept { _subArch = subArch; }
  //! Sets the vendor to `vendor`.
  ASMJIT_INLINE_NODEBUG void setVendor(Vendor vendor) noexcept { _vendor = vendor; }
  //! Sets the platform to `platform`.
  ASMJIT_INLINE_NODEBUG void setPlatform(Platform platform) noexcept { _platform = platform; }
  //! Sets the ABI to `platformABI`.
  ASMJIT_INLINE_NODEBUG void setPlatformABI(PlatformABI platformABI) noexcept { _platformABI = platformABI; }
  //! Sets the object format to `objectFormat`.
  ASMJIT_INLINE_NODEBUG void setObjectFormat(ObjectFormat objectFormat) noexcept { _objectFormat = objectFormat; }

  //! Sets floating point ABI to `floatABI`.
  ASMJIT_INLINE_NODEBUG void setFloatABI(FloatABI floatABI) noexcept { _floatABI = floatABI; }

  //! \}

  //! \name Static Utilities
  //! \{

  static ASMJIT_INLINE_NODEBUG bool isDefinedArch(Arch arch) noexcept {
    return uint32_t(arch) <= uint32_t(Arch::kMaxValue);
  }

  static ASMJIT_INLINE_NODEBUG bool isValidArch(Arch arch) noexcept {
    return arch != Arch::kUnknown && uint32_t(arch) <= uint32_t(Arch::kMaxValue);
  }

  //! Tests whether the given architecture `arch` is 32-bit.
  static ASMJIT_INLINE_NODEBUG bool is32Bit(Arch arch) noexcept {
    return (uint32_t(arch) & uint32_t(Arch::k32BitMask)) == uint32_t(Arch::k32BitMask);
  }

  //! Tests whether the given architecture `arch` is 64-bit.
  static ASMJIT_INLINE_NODEBUG bool is64Bit(Arch arch) noexcept {
    return (uint32_t(arch) & uint32_t(Arch::k32BitMask)) == 0;
  }

  //! Tests whether the given architecture `arch` is little endian.
  static ASMJIT_INLINE_NODEBUG bool isLittleEndian(Arch arch) noexcept {
    return uint32_t(arch) < uint32_t(Arch::kBigEndian);
  }

  //! Tests whether the given architecture `arch` is big endian.
  static ASMJIT_INLINE_NODEBUG bool isBigEndian(Arch arch) noexcept {
    return uint32_t(arch) >= uint32_t(Arch::kBigEndian);
  }

  //! Tests whether the given architecture is Thumb or Thumb_BE.
  static ASMJIT_INLINE_NODEBUG bool isArchThumb(Arch arch) noexcept {
    return arch == Arch::kThumb || arch == Arch::kThumb_BE;
  }

  //! Tests whether the given architecture is ARM or ARM_BE.
  static ASMJIT_INLINE_NODEBUG bool isArchARM(Arch arch) noexcept {
    return arch == Arch::kARM || arch == Arch::kARM_BE;
  }

  //! Tests whether the given architecture is AArch64 or AArch64_BE.
  static ASMJIT_INLINE_NODEBUG bool isArchAArch64(Arch arch) noexcept {
    return arch == Arch::kAArch64 || arch == Arch::kAArch64_BE;
  }

  //! Tests whether the given architecture is MIPS32_LE or MIPS32_BE.
  static ASMJIT_INLINE_NODEBUG bool isArchMIPS32(Arch arch) noexcept {
    return arch == Arch::kMIPS32_LE || arch == Arch::kMIPS32_BE;
  }

  //! Tests whether the given architecture is MIPS64_LE or MIPS64_BE.
  static ASMJIT_INLINE_NODEBUG bool isArchMIPS64(Arch arch) noexcept {
    return arch == Arch::kMIPS64_LE || arch == Arch::kMIPS64_BE;
  }

  //! Tests whether the given architecture family is X86 or X64.
  static ASMJIT_INLINE_NODEBUG bool isFamilyX86(Arch arch) noexcept {
    return arch == Arch::kX86 || arch == Arch::kX64;
  }

  //! Tests whether the given architecture family is AArch32 (ARM or THUMB).
  static ASMJIT_INLINE_NODEBUG bool isFamilyAArch32(Arch arch) noexcept {
    return isArchARM(arch) || isArchThumb(arch);
  }

  //! Tests whether the given architecture family is AArch64.
  static ASMJIT_INLINE_NODEBUG bool isFamilyAArch64(Arch arch) noexcept {
    return isArchAArch64(arch);
  }

  //! Tests whether the given architecture family is ARM, THUMB, or AArch64.
  static ASMJIT_INLINE_NODEBUG bool isFamilyARM(Arch arch) noexcept {
    return isFamilyAArch32(arch) || isFamilyAArch64(arch);
  }

  //! Tests whether the given architecture family is MIPS or MIPS64.
  static ASMJIT_INLINE_NODEBUG bool isFamilyMIPS(Arch arch) noexcept {
    return isArchMIPS32(arch) || isArchMIPS64(arch);
  }

  //! Tests whether the given architecture family is RISC-V (both 32-bit and 64-bit).
  static ASMJIT_INLINE_NODEBUG bool isFamilyRISCV(Arch arch) noexcept {
    return arch == Arch::kRISCV32 || arch == Arch::kRISCV64;
  }

  //! Returns a native general purpose register size from the given architecture.
  static ASMJIT_INLINE_NODEBUG uint32_t registerSizeFromArch(Arch arch) noexcept {
    return is32Bit(arch) ? 4u : 8u;
  }

  //! \}
};

static_assert(sizeof(Environment) == 8,
              "Environment must occupy exactly 8 bytes.");

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ENVIRONMENT_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/errorhandler.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/errorhandler.h"

ASMJIT_BEGIN_NAMESPACE

ErrorHandler::ErrorHandler() noexcept {}
ErrorHandler::~ErrorHandler() noexcept {}

void ErrorHandler::handleError(Error err, const char* message, BaseEmitter* origin) {
  DebugUtils::unused(err, message, origin);
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/errorhandler.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ERRORHANDLER_H_INCLUDED
#define ASMJIT_CORE_ERRORHANDLER_H_INCLUDED

#include "../core/globals.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_error_handling
//! \{

class BaseEmitter;

//! Error handler can be used to override the default behavior of error handling.
//!
//! It's available to all classes that inherit `BaseEmitter`. Override \ref ErrorHandler::handleError() to implement
//! your own error handler.
//!
//! The following use-cases are supported:
//!
//!   - Record the error and continue code generation. This is the simplest approach that can be used to at least log
//!     possible errors.
//!   - Throw an exception. AsmJit doesn't use exceptions and is completely exception-safe, but it's perfectly legal
//!     to throw an exception from the error handler.
//!   - Use plain old C's `setjmp()` and `longjmp()`. Asmjit always puts Assembler, Builder and Compiler to
//!     a consistent state before calling \ref handleError(), so `longjmp()` can be used without issues to cancel the
//!     code generation if an error occurred. This method can be used if exception handling in your project is turned
//!     off and you still want some comfort. In most cases it should be safe as AsmJit uses \ref Zone memory and the
//!     ownership of memory it allocates always ends with the instance that allocated it. If using this approach please
//!     never jump outside the life-time of \ref CodeHolder and \ref BaseEmitter.
//!
//! \ref ErrorHandler can be attached to \ref CodeHolder or \ref BaseEmitter, which has a priority. The example below
//! uses error handler that just prints the error, but lets AsmJit continue:
//!
//! ```
//! // Error Handling #1 - Logging and returning Error.
//! #include <asmjit/x86.h>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! // Error handler that just prints the error and lets AsmJit ignore it.
//! class SimpleErrorHandler : public ErrorHandler {
//! public:
//!   Error err;
//!
//!   inline SimpleErrorHandler() : err(kErrorOk) {}
//!
//!   void handleError(Error err, const char* message, BaseEmitter* origin) override {
//!     this->err = err;
//!     fprintf(stderr, "ERROR: %s\n", message);
//!   }
//! };
//!
//! int main() {
//!   JitRuntime rt;
//!   SimpleErrorHandler eh;
//!
//!   CodeHolder code;
//!   code.init(rt.environment(), rt.cpuFeatures());
//!   code.setErrorHandler(&eh);
//!
//!   // Try to emit instruction that doesn't exist.
//!   x86::Assembler a(&code);
//!   a.emit(x86::Inst::kIdMov, x86::xmm0, x86::xmm1);
//!
//!   if (eh.err) {
//!     // Assembler failed!
//!     return 1;
//!   }
//!
//!   return 0;
//! }
//! ```
//!
//! If error happens during instruction emitting / encoding the assembler behaves transactionally - the output buffer
//! won't advance if encoding failed, thus either a fully encoded instruction or nothing is emitted. The error handling
//! shown above is useful, but it's still not the best way of dealing with errors in AsmJit. The following example
//! shows how to use exception handling to handle errors in a more C++ way:
//!
//! ```
//! // Error Handling #2 - Throwing an exception.
//! #include <asmjit/x86.h>
//! #include <exception>
//! #include <string>
//! #include <stdio.h>
//!
//! using namespace asmjit;
//!
//! // Error handler that throws a user-defined `AsmJitException`.
//! class AsmJitException : public std::exception {
//! public:
//!   Error err;
//!   std::string message;
//!
//!   AsmJitException(Error err, const char* message) noexcept
//!     : err(err),
//!       message(message) {}
//!
//!   const char* what() const noexcept override { return message.c_str(); }
//! };
//!
//! class ThrowableErrorHandler : public ErrorHandler {
//! public:
//!   // Throw is possible, functions that use ErrorHandler are never 'noexcept'.
//!   void handleError(Error err, const char* message, BaseEmitter* origin) override {
//!     throw AsmJitException(err, message);
//!   }
//! };
//!
//! int main() {
//!   JitRuntime rt;
//!   ThrowableErrorHandler eh;
//!
//!   CodeHolder code;
//!   code.init(rt.environment(), rt.cpuFeatures());
//!   code.setErrorHandler(&eh);
//!
//!   x86::Assembler a(&code);
//!
//!   // Try to emit instruction that doesn't exist.
//!   try {
//!     a.emit(x86::Inst::kIdMov, x86::xmm0, x86::xmm1);
//!   }
//!   catch (const AsmJitException& ex) {
//!     printf("EXCEPTION THROWN: %s\n", ex.what());
//!     return 1;
//!   }
//!
//!   return 0;
//! }
//! ```
//!
//! If C++ exceptions are not what you like or your project turns off them completely there is still a way of reducing
//! the error handling to a minimum by using a standard setjmp/longjmp approach. AsmJit is exception-safe and cleans
//! up everything before calling the ErrorHandler, so any approach is safe. You can simply jump from the error handler
//! without causing any side-effects or memory leaks. The following example demonstrates how it could be done:
//!
//! ```
//! // Error Handling #3 - Using setjmp/longjmp if exceptions are not allowed.
//! #include <asmjit/x86.h>
//! #include <setjmp.h>
//! #include <stdio.h>
//!
//! class LongJmpErrorHandler : public asmjit::ErrorHandler {
//! public:
//!   inline LongJmpErrorHandler() : err(asmjit::kErrorOk) {}
//!
//!   void handleError(asmjit::Error err, const char* message, asmjit::BaseEmitter* origin) override {
//!     this->err = err;
//!     longjmp(state, 1);
//!   }
//!
//!   jmp_buf state;
//!   asmjit::Error err;
//! };
//!
//! int main(int argc, char* argv[]) {
//!   using namespace asmjit;
//!
//!   JitRuntime rt;
//!   LongJmpErrorHandler eh;
//!
//!   CodeHolder code;
//!   code.init(rt.environment(), rt.cpuFeatures());
//!   code.setErrorHandler(&eh);
//!
//!   x86::Assembler a(&code);
//!
//!   if (!setjmp(eh.state)) {
//!     // Try to emit instruction that doesn't exist.
//!     a.emit(x86::Inst::kIdMov, x86::xmm0, x86::xmm1);
//!   }
//!   else {
//!     Error err = eh.err;
//!     printf("ASMJIT ERROR: 0x%08X [%s]\n", err, DebugUtils::errorAsString(err));
//!   }
//!
//!   return 0;
//! }
//! ```
class ASMJIT_VIRTAPI ErrorHandler {
public:
  ASMJIT_BASE_CLASS(ErrorHandler)

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `ErrorHandler` instance.
  ASMJIT_API ErrorHandler() noexcept;
  //! Destroys the `ErrorHandler` instance.
  ASMJIT_API virtual ~ErrorHandler() noexcept;

  //! \}

  //! \name Interface
  //! \{

  //! Error handler (must be reimplemented).
  //!
  //! Error handler is called after an error happened and before it's propagated to the caller. There are multiple
  //! ways how the error handler can be used:
  //!
  //! 1. User-based error handling without throwing exception or using C's`longjmp()`. This is for users that don't
  //!     use exceptions and want customized error handling.
  //!
  //! 2. Throwing an exception. AsmJit doesn't use exceptions and is completely exception-safe, but you can throw
  //!     exception from your error handler if this way is the preferred way of handling errors in your project.
  //!
  //! 3. Using plain old C's `setjmp()` and `longjmp()`. Asmjit always puts `BaseEmitter` to a consistent state before
  //!    calling `handleError()`  so `longjmp()` can be used without any issues to cancel the code generation if an
  //!    error occurred. There is no difference between exceptions and `longjmp()` from AsmJit's perspective, however,
  //!    never jump outside of `CodeHolder` and `BaseEmitter` scope as you would leak memory.
  ASMJIT_API virtual void handleError(Error err, const char* message, BaseEmitter* origin);

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ERRORHANDLER_H_INCLUDED


```

`CodeCleaner/asmjit/asmjit/core/formatter.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_LOGGING

#include "../core/archtraits.h"
#include "../core/builder.h"
#include "../core/codeholder.h"
#include "../core/compiler.h"
#include "../core/emitter.h"
#include "../core/formatter_p.h"
#include "../core/string.h"
#include "../core/support.h"
#include "../core/type.h"

#if !defined(ASMJIT_NO_X86)
  #include "../x86/x86formatter_p.h"
#endif

#if !defined(ASMJIT_NO_AARCH64)
  #include "../arm/a64formatter_p.h"
#endif

ASMJIT_BEGIN_NAMESPACE

#if defined(ASMJIT_NO_COMPILER)
class VirtReg;
#endif

namespace Formatter {

static const char wordNameTable[][8] = {
  "db",
  "dw",
  "dd",
  "dq",
  "byte",
  "half",
  "word",
  "hword",
  "dword",
  "qword",
  "xword",
  "short",
  "long",
  "quad"
};


Error formatTypeId(String& sb, TypeId typeId) noexcept {
  if (typeId == TypeId::kVoid)
    return sb.append("void");

  if (!TypeUtils::isValid(typeId))
    return sb.append("unknown");

  const char* typeName = nullptr;
  uint32_t typeSize = TypeUtils::sizeOf(typeId);
  TypeId scalarType = TypeUtils::scalarOf(typeId);

  switch (scalarType) {
    case TypeId::kIntPtr : typeName = "intptr" ; break;
    case TypeId::kUIntPtr: typeName = "uintptr"; break;
    case TypeId::kInt8   : typeName = "int8"   ; break;
    case TypeId::kUInt8  : typeName = "uint8"  ; break;
    case TypeId::kInt16  : typeName = "int16"  ; break;
    case TypeId::kUInt16 : typeName = "uint16" ; break;
    case TypeId::kInt32  : typeName = "int32"  ; break;
    case TypeId::kUInt32 : typeName = "uint32" ; break;
    case TypeId::kInt64  : typeName = "int64"  ; break;
    case TypeId::kUInt64 : typeName = "uint64" ; break;
    case TypeId::kFloat32: typeName = "float32"; break;
    case TypeId::kFloat64: typeName = "float64"; break;
    case TypeId::kFloat80: typeName = "float80"; break;
    case TypeId::kMask8  : typeName = "mask8"  ; break;
    case TypeId::kMask16 : typeName = "mask16" ; break;
    case TypeId::kMask32 : typeName = "mask32" ; break;
    case TypeId::kMask64 : typeName = "mask64" ; break;
    case TypeId::kMmx32  : typeName = "mmx32"  ; break;
    case TypeId::kMmx64  : typeName = "mmx64"  ; break;

    default:
      typeName = "unknown";
      break;
  }

  uint32_t baseSize = TypeUtils::sizeOf(scalarType);
  if (typeSize > baseSize) {
    uint32_t count = typeSize / baseSize;
    return sb.appendFormat("%sx%u", typeName, unsigned(count));
  }
  else {
    return sb.append(typeName);
  }
}

Error formatFeature(
  String& sb,
  Arch arch,
  uint32_t featureId) noexcept {

#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch))
    return x86::FormatterInternal::formatFeature(sb, featureId);
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyARM(arch))
    return arm::FormatterInternal::formatFeature(sb, featureId);
#endif

  return kErrorInvalidArch;
}

Error formatLabel(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  uint32_t labelId) noexcept {

  DebugUtils::unused(formatFlags);

  if (emitter && emitter->code()) {
    const LabelEntry* le = emitter->code()->labelEntry(labelId);
    if (ASMJIT_UNLIKELY(!le))
      return sb.appendFormat("<InvalidLabel:%u>", labelId);

    if (le->hasName()) {
      if (le->hasParent()) {
        uint32_t parentId = le->parentId();
        const LabelEntry* pe = emitter->code()->labelEntry(parentId);

        if (ASMJIT_UNLIKELY(!pe))
          ASMJIT_PROPAGATE(sb.appendFormat("<InvalidLabel:%u>", labelId));
        else if (ASMJIT_UNLIKELY(!pe->hasName()))
          ASMJIT_PROPAGATE(sb.appendFormat("L%u", parentId));
        else
          ASMJIT_PROPAGATE(sb.append(pe->name()));

        ASMJIT_PROPAGATE(sb.append('.'));
      }

      if (le->type() == LabelType::kAnonymous)
        ASMJIT_PROPAGATE(sb.appendFormat("L%u@", labelId));
      return sb.append(le->name());
    }
  }

  return sb.appendFormat("L%u", labelId);
}

Error formatRegister(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  Arch arch,
  RegType regType,
  uint32_t regId) noexcept {

#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch))
    return x86::FormatterInternal::formatRegister(sb, formatFlags, emitter, arch, regType, regId);
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyARM(arch))
    return arm::FormatterInternal::formatRegister(sb, formatFlags, emitter, arch, regType, regId);
#endif

  return kErrorInvalidArch;
}

Error formatOperand(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  Arch arch,
  const Operand_& op) noexcept {

#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch))
    return x86::FormatterInternal::formatOperand(sb, formatFlags, emitter, arch, op);
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyARM(arch))
    return arm::FormatterInternal::formatOperand(sb, formatFlags, emitter, arch, op);
#endif

  return kErrorInvalidArch;
}

ASMJIT_API Error formatDataType(
  String& sb,
  FormatFlags formatFlags,
  Arch arch,
  TypeId typeId) noexcept
{
  DebugUtils::unused(formatFlags);

  if (ASMJIT_UNLIKELY(uint32_t(arch) > uint32_t(Arch::kMaxValue)))
    return DebugUtils::errored(kErrorInvalidArch);

  uint32_t typeSize = TypeUtils::sizeOf(typeId);
  if (typeSize == 0 || typeSize > 8)
    return DebugUtils::errored(kErrorInvalidState);

  uint32_t typeSizeLog2 = Support::ctz(typeSize);
  return sb.append(wordNameTable[size_t(ArchTraits::byArch(arch).typeNameIdByIndex(typeSizeLog2))]);
}

static Error formatDataHelper(String& sb, const char* typeName, uint32_t typeSize, const uint8_t* data, size_t itemCount) noexcept {
  sb.append('.');
  sb.append(typeName);
  sb.append(' ');

  for (size_t i = 0; i < itemCount; i++) {
    uint64_t v = 0;

    if (i != 0)
      ASMJIT_PROPAGATE(sb.append(", ", 2));

    switch (typeSize) {
      case 1: v = data[0]; break;
      case 2: v = Support::readU16u(data); break;
      case 4: v = Support::readU32u(data); break;
      case 8: v = Support::readU64u(data); break;
    }

    ASMJIT_PROPAGATE(sb.appendUInt(v, 16, typeSize * 2, StringFormatFlags::kAlternate));
    data += typeSize;
  }

  return kErrorOk;
}

Error formatData(
  String& sb,
  FormatFlags formatFlags,
  Arch arch,
  TypeId typeId, const void* data, size_t itemCount, size_t repeatCount) noexcept
{
  DebugUtils::unused(formatFlags);

  if (ASMJIT_UNLIKELY(!Environment::isDefinedArch(arch)))
    return DebugUtils::errored(kErrorInvalidArch);

  uint32_t typeSize = TypeUtils::sizeOf(typeId);
  if (typeSize == 0)
    return DebugUtils::errored(kErrorInvalidState);

  if (!Support::isPowerOf2(typeSize)) {
    itemCount *= typeSize;
    typeSize = 1;
  }

  while (typeSize > 8u) {
    typeSize >>= 1;
    itemCount <<= 1;
  }

  uint32_t typeSizeLog2 = Support::ctz(typeSize);
  const char* wordName = wordNameTable[size_t(ArchTraits::byArch(arch).typeNameIdByIndex(typeSizeLog2))];

  if (repeatCount > 1)
    ASMJIT_PROPAGATE(sb.appendFormat(".repeat %zu ", repeatCount));

  return formatDataHelper(sb, wordName, typeSize, static_cast<const uint8_t*>(data), itemCount);
}

Error formatInstruction(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  Arch arch,
  const BaseInst& inst, const Operand_* operands, size_t opCount) noexcept {

#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch))
    return x86::FormatterInternal::formatInstruction(sb, formatFlags, emitter, arch, inst, operands, opCount);
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyAArch64(arch))
    return a64::FormatterInternal::formatInstruction(sb, formatFlags, emitter, arch, inst, operands, opCount);
#endif

  return kErrorInvalidArch;
}

#ifndef ASMJIT_NO_BUILDER

#ifndef ASMJIT_NO_COMPILER
static Error formatFuncValue(String& sb, FormatFlags formatFlags, const BaseEmitter* emitter, FuncValue value) noexcept {
  TypeId typeId = value.typeId();
  ASMJIT_PROPAGATE(formatTypeId(sb, typeId));

  if (value.isAssigned()) {
    ASMJIT_PROPAGATE(sb.append('@'));

    if (value.isIndirect())
      ASMJIT_PROPAGATE(sb.append('['));

    // NOTE: It should be either reg or stack, but never both. We
    // use two IFs on purpose so if the FuncValue is both it would
    // show in logs.
    if (value.isReg()) {
      ASMJIT_PROPAGATE(formatRegister(sb, formatFlags, emitter, emitter->arch(), value.regType(), value.regId()));
    }

    if (value.isStack()) {
      ASMJIT_PROPAGATE(sb.appendFormat("[%d]", int(value.stackOffset())));
    }

    if (value.isIndirect())
      ASMJIT_PROPAGATE(sb.append(']'));
  }

  return kErrorOk;
}

static Error formatFuncValuePack(
  String& sb,
  FormatFlags formatFlags,
  const BaseCompiler* cc,
  const FuncValuePack& pack,
  const RegOnly* vRegs) noexcept {

  size_t count = pack.count();
  if (!count)
    return sb.append("void");

  if (count > 1)
    sb.append('[');

  for (uint32_t valueIndex = 0; valueIndex < count; valueIndex++) {
    const FuncValue& value = pack[valueIndex];
    if (!value)
      break;

    if (valueIndex)
      ASMJIT_PROPAGATE(sb.append(", "));

    ASMJIT_PROPAGATE(formatFuncValue(sb, formatFlags, cc, value));

    if (vRegs) {
      const VirtReg* virtReg = nullptr;
      static const char nullReg[] = "<none>";

      if (vRegs[valueIndex].isReg() && cc->isVirtIdValid(vRegs[valueIndex].id()))
        virtReg = cc->virtRegById(vRegs[valueIndex].id());

      ASMJIT_PROPAGATE(sb.appendFormat(" %s", virtReg ? virtReg->name() : nullReg));
    }
  }

  if (count > 1)
    sb.append(']');

  return kErrorOk;
}

static Error formatFuncRets(
  String& sb,
  FormatFlags formatFlags,
  const BaseCompiler* cc,
  const FuncDetail& fd) noexcept {

  return formatFuncValuePack(sb, formatFlags, cc, fd.retPack(), nullptr);
}

static Error formatFuncArgs(
  String& sb,
  FormatFlags formatFlags,
  const BaseCompiler* cc,
  const FuncDetail& fd,
  const FuncNode::ArgPack* argPacks) noexcept {

  uint32_t argCount = fd.argCount();
  if (!argCount)
    return sb.append("void");

  for (uint32_t argIndex = 0; argIndex < argCount; argIndex++) {
    if (argIndex)
      ASMJIT_PROPAGATE(sb.append(", "));

    ASMJIT_PROPAGATE(formatFuncValuePack(sb, formatFlags, cc, fd.argPack(argIndex), argPacks[argIndex]._data));
  }

  return kErrorOk;
}
#endif

Error formatNode(
  String& sb,
  const FormatOptions& formatOptions,
  const BaseBuilder* builder,
  const BaseNode* node) noexcept {

  if (node->hasPosition() && formatOptions.hasFlag(FormatFlags::kPositions))
    ASMJIT_PROPAGATE(sb.appendFormat("<%05u> ", node->position()));

  size_t startLineIndex = sb.size();

  switch (node->type()) {
    case NodeType::kInst:
    case NodeType::kJump: {
      const InstNode* instNode = node->as<InstNode>();
      ASMJIT_PROPAGATE(builder->_funcs.formatInstruction(sb, formatOptions.flags(), builder,
        builder->arch(),
        instNode->baseInst(), instNode->operands(), instNode->opCount()));
      break;
    }

    case NodeType::kSection: {
      const SectionNode* sectionNode = node->as<SectionNode>();
      if (builder->_code->isSectionValid(sectionNode->id())) {
        const Section* section = builder->_code->sectionById(sectionNode->id());
        ASMJIT_PROPAGATE(sb.appendFormat(".section %s", section->name()));
      }
      break;
    }

    case NodeType::kLabel: {
      const LabelNode* labelNode = node->as<LabelNode>();
      ASMJIT_PROPAGATE(formatLabel(sb, formatOptions.flags(), builder, labelNode->labelId()));
      ASMJIT_PROPAGATE(sb.append(":"));
      break;
    }

    case NodeType::kAlign: {
      const AlignNode* alignNode = node->as<AlignNode>();
      ASMJIT_PROPAGATE(sb.appendFormat(".align %u (%s)",
        alignNode->alignment(),
        alignNode->alignMode() == AlignMode::kCode ? "code" : "data"));
      break;
    }

    case NodeType::kEmbedData: {
      const EmbedDataNode* embedNode = node->as<EmbedDataNode>();
      ASMJIT_PROPAGATE(sb.append('.'));
      ASMJIT_PROPAGATE(formatDataType(sb, formatOptions.flags(), builder->arch(), embedNode->typeId()));
      ASMJIT_PROPAGATE(sb.appendFormat(" {Count=%zu Repeat=%zu TotalSize=%zu}", embedNode->itemCount(), embedNode->repeatCount(), embedNode->dataSize()));
      break;
    }

    case NodeType::kEmbedLabel: {
      const EmbedLabelNode* embedNode = node->as<EmbedLabelNode>();
      ASMJIT_PROPAGATE(sb.append(".label "));
      ASMJIT_PROPAGATE(formatLabel(sb, formatOptions.flags(), builder, embedNode->labelId()));
      break;
    }

    case NodeType::kEmbedLabelDelta: {
      const EmbedLabelDeltaNode* embedNode = node->as<EmbedLabelDeltaNode>();
      ASMJIT_PROPAGATE(sb.append(".label ("));
      ASMJIT_PROPAGATE(formatLabel(sb, formatOptions.flags(), builder, embedNode->labelId()));
      ASMJIT_PROPAGATE(sb.append(" - "));
      ASMJIT_PROPAGATE(formatLabel(sb, formatOptions.flags(), builder, embedNode->baseLabelId()));
      ASMJIT_PROPAGATE(sb.append(")"));
      break;
    }

    case NodeType::kConstPool: {
      const ConstPoolNode* constPoolNode = node->as<ConstPoolNode>();
      ASMJIT_PROPAGATE(sb.appendFormat("[ConstPool Size=%zu Alignment=%zu]", constPoolNode->size(), constPoolNode->alignment()));
      break;
    };

    case NodeType::kComment: {
      const CommentNode* commentNode = node->as<CommentNode>();
      return sb.appendFormat("; %s", commentNode->inlineComment());
    }

    case NodeType::kSentinel: {
      const SentinelNode* sentinelNode = node->as<SentinelNode>();
      const char* sentinelName = nullptr;

      switch (sentinelNode->sentinelType()) {
        case SentinelType::kFuncEnd:
          sentinelName = "[FuncEnd]";
          break;

        default:
          sentinelName = "[Sentinel]";
          break;
      }

      ASMJIT_PROPAGATE(sb.append(sentinelName));
      break;
    }

#ifndef ASMJIT_NO_COMPILER
    case NodeType::kFunc: {
      const FuncNode* funcNode = node->as<FuncNode>();

      if (builder->isCompiler()) {
        ASMJIT_PROPAGATE(formatLabel(sb, formatOptions.flags(), builder, funcNode->labelId()));
        ASMJIT_PROPAGATE(sb.append(": "));

        ASMJIT_PROPAGATE(formatFuncRets(sb, formatOptions.flags(), static_cast<const BaseCompiler*>(builder), funcNode->detail()));
        ASMJIT_PROPAGATE(sb.append(" Func("));
        ASMJIT_PROPAGATE(formatFuncArgs(sb, formatOptions.flags(), static_cast<const BaseCompiler*>(builder), funcNode->detail(), funcNode->argPacks()));
        ASMJIT_PROPAGATE(sb.append(")"));
      }
      break;
    }

    case NodeType::kFuncRet: {
      const FuncRetNode* retNode = node->as<FuncRetNode>();
      ASMJIT_PROPAGATE(sb.append("[FuncRet]"));

      for (uint32_t i = 0; i < 2; i++) {
        const Operand_& op = retNode->op(i);
        if (!op.isNone()) {
          ASMJIT_PROPAGATE(sb.append(i == 0 ? " " : ", "));
          ASMJIT_PROPAGATE(formatOperand(sb, formatOptions.flags(), builder, builder->arch(), op));
        }
      }
      break;
    }

    case NodeType::kInvoke: {
      const InvokeNode* invokeNode = node->as<InvokeNode>();
      ASMJIT_PROPAGATE(builder->_funcs.formatInstruction(sb, formatOptions.flags(), builder,
        builder->arch(),
        invokeNode->baseInst(), invokeNode->operands(), invokeNode->opCount()));
      break;
    }
#endif

    default: {
      ASMJIT_PROPAGATE(sb.appendFormat("[UserNode:%u]", node->type()));
      break;
    }
  }

  if (node->hasInlineComment()) {
    size_t requiredPadding = paddingFromOptions(formatOptions, FormatPaddingGroup::kRegularLine);
    size_t currentPadding = sb.size() - startLineIndex;

    if (currentPadding < requiredPadding)
      ASMJIT_PROPAGATE(sb.appendChars(' ', requiredPadding - currentPadding));

    ASMJIT_PROPAGATE(sb.append("; "));
    ASMJIT_PROPAGATE(sb.append(node->inlineComment()));
  }

  return kErrorOk;
}

Error formatNodeList(
  String& sb,
  const FormatOptions& formatOptions,
  const BaseBuilder* builder) noexcept {

  return formatNodeList(sb, formatOptions, builder, builder->firstNode(), nullptr);
}

Error formatNodeList(
  String& sb,
  const FormatOptions& formatOptions,
  const BaseBuilder* builder,
  const BaseNode* begin,
  const BaseNode* end) noexcept {

  const BaseNode* node = begin;
  while (node != end) {
    ASMJIT_PROPAGATE(formatNode(sb, formatOptions, builder, node));
    ASMJIT_PROPAGATE(sb.append('\n'));
    node = node->next();
  }
  return kErrorOk;
}
#endif

} // {Formatter}

ASMJIT_END_NAMESPACE

#endif

```

`CodeCleaner/asmjit/asmjit/core/formatter.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_FORMATTER_H_INCLUDED
#define ASMJIT_CORE_FORMATTER_H_INCLUDED

#include "../core/globals.h"
#include "../core/inst.h"
#include "../core/string.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_logging
//! \{

class BaseBuilder;
class BaseEmitter;
class BaseNode;
struct Operand_;

//! Format flags used by \ref Logger and \ref FormatOptions.
enum class FormatFlags : uint32_t {
  //! No formatting flags.
  kNone = 0u,

  //! Show also a binary representation of each logged instruction (Assembler).
  kMachineCode = 0x00000001u,
  //! Show aliases of some instructions that have them.
  //!
  //! This option is now mostly for x86/x64 to show aliases of instructions such as `cmov<cc>`, `j<cc>`, `set<cc>`,
  //! etc...
  kShowAliases = 0x00000008u,
  //! Show a text explanation of some immediate values.
  kExplainImms = 0x00000010u,
  //! Use hexadecimal notation of immediate values.
  kHexImms = 0x00000020u,
  //! Use hexadecimal notation of addresses and offsets in addresses.
  kHexOffsets = 0x00000040u,
  //! Show casts between virtual register types (Compiler).
  kRegCasts = 0x00000100u,
  //! Show positions associated with nodes (Compiler).
  kPositions = 0x00000200u,
  //! Always format a register type (Compiler).
  kRegType = 0x00000400u
};
ASMJIT_DEFINE_ENUM_FLAGS(FormatFlags)

//! Format indentation group, used by \ref FormatOptions.
enum class FormatIndentationGroup : uint32_t {
  //! Indentation used for instructions and directives.
  kCode = 0u,
  //! Indentation used for labels and function nodes.
  kLabel = 1u,
  //! Indentation used for comments (not inline comments).
  kComment = 2u,

  //! \cond INTERNAL
  //! Reserved for future use.
  kReserved = 3u,
  //! \endcond

  //! Maximum value of `FormatIndentationGroup`.
  kMaxValue = kReserved
};

//! Format padding group, used by \ref FormatOptions.
enum class FormatPaddingGroup : uint32_t {
  //! Describes padding of a regular line, which can represent instruction, data, or assembler directives.
  kRegularLine = 0,
  //! Describes padding of machine code dump that is visible next to the instruction, if enabled.
  kMachineCode = 1,

  //! Maximum value of `FormatPaddingGroup`.
  kMaxValue = kMachineCode
};

//! Formatting options used by \ref Logger and \ref Formatter.
class FormatOptions {
public:
  //! \name Members
  //! \{

  //! Format flags.
  FormatFlags _flags = FormatFlags::kNone;
  //! Indentations for each indentation group.
  Support::Array<uint8_t, uint32_t(FormatIndentationGroup::kMaxValue) + 1> _indentation {};
  //! Paddings for each padding group.
  Support::Array<uint16_t, uint32_t(FormatPaddingGroup::kMaxValue) + 1> _padding {};

  //! \}

  //! \name Reset
  //! \{

  //! Resets FormatOptions to its default initialized state.
  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _flags = FormatFlags::kNone;
    _indentation.fill(uint8_t(0));
    _padding.fill(uint16_t(0));
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns format flags.
  ASMJIT_INLINE_NODEBUG FormatFlags flags() const noexcept { return _flags; }
  //! Tests whether the given `flag` is set in format flags.
  ASMJIT_INLINE_NODEBUG bool hasFlag(FormatFlags flag) const noexcept { return Support::test(_flags, flag); }

  //! Resets all format flags to `flags`.
  ASMJIT_INLINE_NODEBUG void setFlags(FormatFlags flags) noexcept { _flags = flags; }
  //! Adds `flags` to format flags.
  ASMJIT_INLINE_NODEBUG void addFlags(FormatFlags flags) noexcept { _flags |= flags; }
  //! Removes `flags` from format flags.
  ASMJIT_INLINE_NODEBUG void clearFlags(FormatFlags flags) noexcept { _flags &= ~flags; }

  //! Returns indentation for the given indentation `group`.
  ASMJIT_INLINE_NODEBUG uint8_t indentation(FormatIndentationGroup group) const noexcept { return _indentation[group]; }
  //! Sets indentation for the given indentation `group`.
  ASMJIT_INLINE_NODEBUG void setIndentation(FormatIndentationGroup group, uint32_t n) noexcept { _indentation[group] = uint8_t(n); }
  //! Resets indentation for the given indentation `group` to zero.
  ASMJIT_INLINE_NODEBUG void resetIndentation(FormatIndentationGroup group) noexcept { _indentation[group] = uint8_t(0); }

  //! Returns padding for the given padding `group`.
  ASMJIT_INLINE_NODEBUG size_t padding(FormatPaddingGroup group) const noexcept { return _padding[group]; }
  //! Sets padding for the given padding `group`.
  ASMJIT_INLINE_NODEBUG void setPadding(FormatPaddingGroup group, size_t n) noexcept { _padding[group] = uint16_t(n); }
  //! Resets padding for the given padding `group` to zero, which means that a default padding will be used
  //! based on the target architecture properties.
  ASMJIT_INLINE_NODEBUG void resetPadding(FormatPaddingGroup group) noexcept { _padding[group] = uint16_t(0); }

  //! \}
};

//! Provides formatting functionality to format operands, instructions, and nodes.
namespace Formatter {

#ifndef ASMJIT_NO_LOGGING

//! Appends a formatted `typeId` to the output string `sb`.
ASMJIT_API Error formatTypeId(
  String& sb,
  TypeId typeId) noexcept;

//! Appends a formatted `featureId` to the output string `sb`.
//!
//! See \ref CpuFeatures.
ASMJIT_API Error formatFeature(
  String& sb,
  Arch arch,
  uint32_t featureId) noexcept;

//! Appends a formatted register to the output string `sb`.
//!
//! \note Emitter is optional, but it's required to format virtual registers, which won't be formatted properly
//! if the `emitter` is not provided.
ASMJIT_API Error formatRegister(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  Arch arch,
  RegType regType,
  uint32_t regId) noexcept;

//! Appends a formatted label to the output string `sb`.
//!
//! \note Emitter is optional, but it's required to format named labels properly, otherwise the formatted as
//! it is an anonymous label.
ASMJIT_API Error formatLabel(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  uint32_t labelId) noexcept;

//! Appends a formatted operand to the output string `sb`.
//!
//! \note Emitter is optional, but it's required to format named labels and virtual registers. See
//! \ref formatRegister() and \ref formatLabel() for more details.
ASMJIT_API Error formatOperand(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  Arch arch,
  const Operand_& op) noexcept;

//! Appends a formatted data-type to the output string `sb`.
ASMJIT_API Error formatDataType(
  String& sb,
  FormatFlags formatFlags,
  Arch arch,
  TypeId typeId) noexcept;

//! Appends a formatted data to the output string `sb`.
ASMJIT_API Error formatData(
  String& sb,
  FormatFlags formatFlags,
  Arch arch,
  TypeId typeId, const void* data, size_t itemCount, size_t repeatCount = 1) noexcept;

//! Appends a formatted instruction to the output string `sb`.
//!
//! \note Emitter is optional, but it's required to format named labels and virtual registers. See
//! \ref formatRegister() and \ref formatLabel() for more details.
ASMJIT_API Error formatInstruction(
  String& sb,
  FormatFlags formatFlags,
  const BaseEmitter* emitter,
  Arch arch,
  const BaseInst& inst, const Operand_* operands, size_t opCount) noexcept;

#ifndef ASMJIT_NO_BUILDER
//! Appends a formatted node to the output string `sb`.
//!
//! The `node` must belong to the provided `builder`.
ASMJIT_API Error formatNode(
  String& sb,
  const FormatOptions& formatOptions,
  const BaseBuilder* builder,
  const BaseNode* node) noexcept;

//! Appends formatted nodes to the output string `sb`.
//!
//! All nodes that are part of the given `builder` will be appended.
ASMJIT_API Error formatNodeList(
  String& sb,
  const FormatOptions& formatOptions,
  const BaseBuilder* builder) noexcept;

//! Appends formatted nodes to the output string `sb`.
//!
//! This function works the same as \ref formatNode(), but appends more nodes to the output string,
//! separating each node with a newline '\n' character.
ASMJIT_API Error formatNodeList(
  String& sb,
  const FormatOptions& formatOptions,
  const BaseBuilder* builder,
  const BaseNode* begin,
  const BaseNode* end) noexcept;
#endif

#endif

} // {Formatter}

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_FORMATTER_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/formatter_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_FORMATTER_P_H_INCLUDED
#define ASMJIT_CORE_FORMATTER_P_H_INCLUDED

#include "../core/formatter.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_logging
//! \{

namespace Formatter {

static ASMJIT_FORCE_INLINE size_t paddingFromOptions(const FormatOptions& formatOptions, FormatPaddingGroup group) noexcept {
  static constexpr uint16_t _defaultPaddingTable[uint32_t(FormatPaddingGroup::kMaxValue) + 1] = { 44, 26 };
  static_assert(uint32_t(FormatPaddingGroup::kMaxValue) + 1 == 2, "If a new group is defined it must be added here");

  size_t padding = formatOptions.padding(group);
  return padding ? padding : size_t(_defaultPaddingTable[uint32_t(group)]);
}

} // {Formatter}

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_FORMATTER_H_P_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/func.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/archtraits.h"
#include "../core/func.h"
#include "../core/operand.h"
#include "../core/type.h"
#include "../core/funcargscontext_p.h"

#if !defined(ASMJIT_NO_X86)
  #include "../x86/x86func_p.h"
#endif

#if !defined(ASMJIT_NO_AARCH64)
  #include "../arm/a64func_p.h"
#endif

ASMJIT_BEGIN_NAMESPACE

// CallConv - Initialization & Reset
// =================================

ASMJIT_FAVOR_SIZE Error CallConv::init(CallConvId ccId, const Environment& environment) noexcept {
  reset();

#if !defined(ASMJIT_NO_X86)
  if (environment.isFamilyX86())
    return x86::FuncInternal::initCallConv(*this, ccId, environment);
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (environment.isFamilyAArch64())
    return a64::FuncInternal::initCallConv(*this, ccId, environment);
#endif

  return DebugUtils::errored(kErrorInvalidArgument);
}

// FuncDetail - Init / Reset
// =========================

ASMJIT_FAVOR_SIZE Error FuncDetail::init(const FuncSignature& signature, const Environment& environment) noexcept {
  CallConvId ccId = signature.callConvId();
  uint32_t argCount = signature.argCount();

  if (ASMJIT_UNLIKELY(argCount > Globals::kMaxFuncArgs))
    return DebugUtils::errored(kErrorInvalidArgument);

  CallConv& cc = _callConv;
  ASMJIT_PROPAGATE(cc.init(ccId, environment));

  uint32_t registerSize = Environment::registerSizeFromArch(cc.arch());
  uint32_t deabstractDelta = TypeUtils::deabstractDeltaOfSize(registerSize);

  const TypeId* signatureArgs = signature.args();
  for (uint32_t argIndex = 0; argIndex < argCount; argIndex++) {
    FuncValuePack& argPack = _args[argIndex];
    argPack[0].initTypeId(TypeUtils::deabstract(signatureArgs[argIndex], deabstractDelta));
  }

  _argCount = uint8_t(argCount);
  _vaIndex = uint8_t(signature.vaIndex());

  TypeId ret = signature.ret();
  if (ret != TypeId::kVoid)
    _rets[0].initTypeId(TypeUtils::deabstract(ret, deabstractDelta));

#if !defined(ASMJIT_NO_X86)
  if (environment.isFamilyX86())
    return x86::FuncInternal::initFuncDetail(*this, signature, registerSize);
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (environment.isFamilyAArch64())
    return a64::FuncInternal::initFuncDetail(*this, signature);
#endif

  // We should never bubble here as if `cc.init()` succeeded then there has to be an implementation for the current
  // architecture. However, stay safe.
  return DebugUtils::errored(kErrorInvalidArgument);
}

// FuncFrame - Init
// ================

ASMJIT_FAVOR_SIZE Error FuncFrame::init(const FuncDetail& func) noexcept {
  Arch arch = func.callConv().arch();
  if (!Environment::isValidArch(arch))
    return DebugUtils::errored(kErrorInvalidArch);

  const ArchTraits& archTraits = ArchTraits::byArch(arch);

  // Initializing FuncFrame means making a copy of some properties of `func`. Properties like `_localStackSize` will
  // be set by the user before the frame is finalized.
  reset();

  _arch = arch;
  _spRegId = uint8_t(archTraits.spRegId());
  _saRegId = uint8_t(BaseReg::kIdBad);

  uint32_t naturalStackAlignment = func.callConv().naturalStackAlignment();
  uint32_t minDynamicAlignment = Support::max<uint32_t>(naturalStackAlignment, 16);

  if (minDynamicAlignment == naturalStackAlignment)
    minDynamicAlignment <<= 1;

  _naturalStackAlignment = uint8_t(naturalStackAlignment);
  _minDynamicAlignment = uint8_t(minDynamicAlignment);
  _redZoneSize = uint8_t(func.redZoneSize());
  _spillZoneSize = uint8_t(func.spillZoneSize());
  _finalStackAlignment = uint8_t(_naturalStackAlignment);

  if (func.hasFlag(CallConvFlags::kCalleePopsStack)) {
    _calleeStackCleanup = uint16_t(func.argStackSize());
  }

  // Initial masks of dirty and preserved registers.
  for (RegGroup group : RegGroupVirtValues{}) {
    _dirtyRegs[group] = func.usedRegs(group);
    _preservedRegs[group] = func.preservedRegs(group);
  }

  // Exclude stack pointer - this register is never included in saved GP regs.
  _preservedRegs[RegGroup::kGp] &= ~Support::bitMask(archTraits.spRegId());

  // The size and alignment of save/restore area of registers for each virtual register group
  _saveRestoreRegSize = func.callConv()._saveRestoreRegSize;
  _saveRestoreAlignment = func.callConv()._saveRestoreAlignment;

  return kErrorOk;
}

// FuncFrame - Finalize
// ====================

ASMJIT_FAVOR_SIZE Error FuncFrame::finalize() noexcept {
  if (!Environment::isValidArch(arch()))
    return DebugUtils::errored(kErrorInvalidArch);

  const ArchTraits& archTraits = ArchTraits::byArch(arch());

  uint32_t registerSize = _saveRestoreRegSize[RegGroup::kGp];
  uint32_t vectorSize = _saveRestoreRegSize[RegGroup::kVec];
  uint32_t returnAddressSize = archTraits.hasLinkReg() ? 0u : registerSize;

  // The final stack alignment must be updated accordingly to call and local stack alignments.
  uint32_t stackAlignment = _finalStackAlignment;
  ASMJIT_ASSERT(stackAlignment == Support::max(_naturalStackAlignment,
                                               _callStackAlignment,
                                               _localStackAlignment));

  bool hasFP = hasPreservedFP();
  bool hasDA = hasDynamicAlignment();

  uint32_t kSp = archTraits.spRegId();
  uint32_t kFp = archTraits.fpRegId();
  uint32_t kLr = archTraits.linkRegId();

  // Make frame pointer dirty if the function uses it.
  if (hasFP) {
    _dirtyRegs[RegGroup::kGp] |= Support::bitMask(kFp);

    // Currently required by ARM, if this works differently across architectures we would have to generalize most
    // likely in CallConv.
    if (kLr != BaseReg::kIdBad)
      _dirtyRegs[RegGroup::kGp] |= Support::bitMask(kLr);
  }

  // These two are identical if the function doesn't align its stack dynamically.
  uint32_t saRegId = _saRegId;
  if (saRegId == BaseReg::kIdBad)
    saRegId = kSp;

  // Fix stack arguments base-register from SP to FP in case it was not picked before and the function performs
  // dynamic stack alignment.
  if (hasDA && saRegId == kSp)
    saRegId = kFp;

  // Mark as dirty any register but SP if used as SA pointer.
  if (saRegId != kSp)
    _dirtyRegs[RegGroup::kGp] |= Support::bitMask(saRegId);

  _spRegId = uint8_t(kSp);
  _saRegId = uint8_t(saRegId);

  // Setup stack size used to save preserved registers.
  uint32_t saveRestoreSizes[2] {};
  for (RegGroup group : RegGroupVirtValues{})
    saveRestoreSizes[size_t(!archTraits.hasInstPushPop(group))]
      += Support::alignUp(Support::popcnt(savedRegs(group)) * saveRestoreRegSize(group), saveRestoreAlignment(group));

  _pushPopSaveSize  = uint16_t(saveRestoreSizes[0]);
  _extraRegSaveSize = uint16_t(saveRestoreSizes[1]);

  uint32_t v = 0;                            // The beginning of the stack frame relative to SP after prolog.
  v += callStackSize();                      // Count 'callStackSize'      <- This is used to call functions.
  v  = Support::alignUp(v, stackAlignment);  // Align to function's stack alignment.

  _localStackOffset = v;                     // Store 'localStackOffset'   <- Function's local stack starts here.
  v += localStackSize();                     // Count 'localStackSize'     <- Function's local stack ends here.

  // If the function's stack must be aligned, calculate the alignment necessary to store vector registers, and set
  // `FuncAttributes::kAlignedVecSR` to inform PEI that it can use instructions that perform aligned stores/loads.
  if (stackAlignment >= vectorSize && _extraRegSaveSize) {
    addAttributes(FuncAttributes::kAlignedVecSR);
    v = Support::alignUp(v, vectorSize);     // Align 'extraRegSaveOffset'.
  }

  _extraRegSaveOffset = v;                   // Store 'extraRegSaveOffset' <- Non-GP save/restore starts here.
  v += _extraRegSaveSize;                    // Count 'extraRegSaveSize'   <- Non-GP save/restore ends here.

  // Calculate if dynamic alignment (DA) slot (stored as offset relative to SP) is required and its offset.
  if (hasDA && !hasFP) {
    _daOffset = v;                           // Store 'daOffset'           <- DA pointer would be stored here.
    v += registerSize;                       // Count 'daOffset'.
  }
  else {
    _daOffset = FuncFrame::kTagInvalidOffset;
  }

  // Link Register
  // -------------
  //
  // The stack is aligned after the function call as the return address is stored in a link register. Some
  // architectures may require to always have aligned stack after PUSH/POP operation, which is represented
  // by ArchTraits::stackAlignmentConstraint().
  //
  // No Link Register (X86/X64)
  // --------------------------
  //
  // The return address should be stored after GP save/restore regs. It has the same size as `registerSize`
  // (basically the native register/pointer size). We don't adjust it now as `v` now contains the exact size
  // that the function requires to adjust (call frame + stack frame, vec stack size). The stack (if we consider
  // this size) is misaligned now, as it's always aligned before the function call - when `call()` is executed
  // it pushes the current EIP|RIP onto the stack, and misaligns it by 12 or 8 bytes (depending on the
  // architecture). So count number of bytes needed to align it up to the function's CallFrame (the beginning).
  if (v || hasFuncCalls() || !returnAddressSize)
    v += Support::alignUpDiff(v + pushPopSaveSize() + returnAddressSize, stackAlignment);

  _pushPopSaveOffset = v;                    // Store 'pushPopSaveOffset'  <- Function's push/pop save/restore starts here.
  _stackAdjustment = v;                      // Store 'stackAdjustment'    <- SA used by 'add SP, SA' and 'sub SP, SA'.
  v += _pushPopSaveSize;                     // Count 'pushPopSaveSize'    <- Function's push/pop save/restore ends here.
  _finalStackSize = v;                       // Store 'finalStackSize'     <- Final stack used by the function.

  if (!archTraits.hasLinkReg())
    v += registerSize;                       // Count 'ReturnAddress'      <- As CALL pushes onto stack.

  // If the function performs dynamic stack alignment then the stack-adjustment must be aligned.
  if (hasDA)
    _stackAdjustment = Support::alignUp(_stackAdjustment, stackAlignment);

  // Calculate where the function arguments start relative to SP.
  _saOffsetFromSP = hasDA ? FuncFrame::kTagInvalidOffset : v;

  // Calculate where the function arguments start relative to FP or user-provided register.
  _saOffsetFromSA = hasFP ? returnAddressSize + registerSize      // Return address + frame pointer.
                          : returnAddressSize + _pushPopSaveSize; // Return address + all push/pop regs.

  return kErrorOk;
}

// FuncArgsAssignment - UpdateFuncFrame
// ====================================

ASMJIT_FAVOR_SIZE Error FuncArgsAssignment::updateFuncFrame(FuncFrame& frame) const noexcept {
  Arch arch = frame.arch();
  const FuncDetail* func = funcDetail();

  if (!func)
    return DebugUtils::errored(kErrorInvalidState);

  RAConstraints constraints;
  ASMJIT_PROPAGATE(constraints.init(arch));

  FuncArgsContext ctx;
  ASMJIT_PROPAGATE(ctx.initWorkData(frame, *this, &constraints));
  ASMJIT_PROPAGATE(ctx.markDstRegsDirty(frame));
  ASMJIT_PROPAGATE(ctx.markScratchRegs(frame));
  ASMJIT_PROPAGATE(ctx.markStackArgsReg(frame));
  return kErrorOk;
}

// Func API - Tests
// ================

#if defined(ASMJIT_TEST)
UNIT(func_signature) {
  FuncSignature signature;
  signature.setRetT<int8_t>();
  signature.addArgT<int16_t>();
  signature.addArg(TypeId::kInt32);

  EXPECT_EQ(signature, FuncSignature::build<int8_t, int16_t, int32_t>());
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/func.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_FUNC_H_INCLUDED
#define ASMJIT_CORE_FUNC_H_INCLUDED

#include "../core/archtraits.h"
#include "../core/environment.h"
#include "../core/operand.h"
#include "../core/type.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_function
//! \{

//! Calling convention id.
//!
//! Calling conventions can be divided into the following groups:
//!
//!   - Universal - calling conventions are applicable to any target. They will be converted to a target dependent
//!     calling convention at runtime by \ref CallConv::init() with some help from \ref Environment. The purpose of
//!     these calling conventions is to make using functions less target dependent and closer to C and C++.
//!
//!   - Target specific - calling conventions that are used by a particular architecture and ABI. For example
//!     Windows 64-bit calling convention and AMD64 SystemV calling convention.
enum class CallConvId : uint8_t {
  // Universal Calling Conventions
  // -----------------------------

  //! Standard function call or explicit `__cdecl` where it can be specified.
  //!
  //! This is a universal calling convention, which is used to initialize specific calling conventions based on
  //! architecture, platform, and its ABI.
  kCDecl = 0,

  //! `__stdcall` on targets that support this calling convention (X86).
  //!
  //! \note This calling convention is only supported on 32-bit X86. If used on environment that doesn't support
  //! this calling convention it will be replaced by \ref CallConvId::kCDecl.
  kStdCall = 1,

  //! `__fastcall` on targets that support this calling convention (X86).
  //!
  //! \note This calling convention is only supported on 32-bit X86. If used on environment that doesn't support
  //! this calling convention it will be replaced by \ref CallConvId::kCDecl.
  kFastCall = 2,

  //! `__vectorcall` on targets that support this calling convention (X86/X64).
  //!
  //! \note This calling convention is only supported on 32-bit and 64-bit X86 architecture on Windows platform.
  //! If used on environment that doesn't support this calling it will be replaced by \ref CallConvId::kCDecl.
  kVectorCall = 3,

  //! `__thiscall` on targets that support this calling convention (X86).
  //!
  //! \note This calling convention is only supported on 32-bit X86 Windows platform. If used on environment that
  //! doesn't support this calling convention it will be replaced by \ref CallConvId::kCDecl.
  kThisCall = 4,

  //! `__attribute__((regparm(1)))` convention (GCC and Clang).
  kRegParm1 = 5,
  //! `__attribute__((regparm(2)))` convention (GCC and Clang).
  kRegParm2 = 6,
  //! `__attribute__((regparm(3)))` convention (GCC and Clang).
  kRegParm3 = 7,

  //! AsmJit specific calling convention designed for calling functions inside a multimedia code that don't use many
  //! registers internally, but are long enough to be called and not inlined. These functions are usually used to
  //! calculate trigonometric functions, logarithms, etc...
  kLightCall2 = 16,
  kLightCall3 = 17,
  kLightCall4 = 18,

  // ABI-Specific Calling Conventions
  // --------------------------------

  //! Soft-float calling convention (AArch32).
  //!
  //! Floating point arguments are passed via general purpose registers.
  kSoftFloat = 30,

  //! Hard-float calling convention (AArch32).
  //!
  //! Floating point arguments are passed via SIMD registers.
  kHardFloat = 31,

  //! X64 System-V calling convention.
  kX64SystemV = 32,
  //! X64 Windows calling convention.
  kX64Windows = 33,

  //! Maximum value of `CallConvId`.
  kMaxValue = kX64Windows
};

//! Strategy used by calling conventions to assign registers to function arguments.
//!
//! Calling convention strategy describes how AsmJit should convert function arguments used by \ref FuncSignature
//! into register identifiers and stack offsets. The \ref CallConvStrategy::kDefault strategy assigns registers
//! and then stack whereas \ref CallConvStrategy::kX64Windows strategy does register shadowing as defined by WIN64
//! calling convention, which is only used by 64-bit Windows.
enum class CallConvStrategy : uint8_t {
  //! Default register assignment strategy.
  kDefault = 0,
  //! Windows 64-bit ABI register assignment strategy.
  kX64Windows = 1,
  //! Windows 64-bit __vectorcall register assignment strategy.
  kX64VectorCall = 2,
  //! Apple's AArch64 calling convention (differs compared to AArch64 calling convention used by Linux).
  kAArch64Apple = 3,

  //! Maximum value of `CallConvStrategy`.
  kMaxValue = kX64VectorCall
};

//! Calling convention flags.
enum class CallConvFlags : uint32_t {
  //! No flags.
  kNone = 0,
  //! Callee is responsible for cleaning up the stack.
  kCalleePopsStack = 0x0001u,
  //! Pass vector arguments indirectly (as a pointer).
  kIndirectVecArgs = 0x0002u,
  //! Pass F32 and F64 arguments via VEC128 register.
  kPassFloatsByVec = 0x0004u,
  //! Pass MMX and vector arguments via stack if the function has variable arguments.
  kPassVecByStackIfVA = 0x0008u,
  //! MMX registers are passed and returned via GP registers.
  kPassMmxByGp = 0x0010u,
  //! MMX registers are passed and returned via XMM registers.
  kPassMmxByXmm = 0x0020u,
  //! Calling convention can be used with variable arguments.
  kVarArgCompatible = 0x0080u
};
ASMJIT_DEFINE_ENUM_FLAGS(CallConvFlags)

//! Function calling convention.
//!
//! Function calling convention is a scheme that defines how function parameters are passed and how function
//! returns its result. AsmJit defines a variety of architecture and OS specific calling conventions and also
//! provides a compile time detection to make the code-generation easier.
struct CallConv {
  //! \name Constants
  //! \{

  //! Maximum number of register arguments per register group.
  //!
  //! \note This is not really AsmJit's limitation, it's just the number that makes sense considering all common
  //! calling conventions. Usually even conventions that use registers to pass function arguments are limited to 8
  //! and less arguments passed via registers per group.
  static constexpr uint32_t kMaxRegArgsPerGroup = 16;

  //! \}

  //! \name Members
  //! \{

  //! Target architecture.
  Arch _arch;
  //! Calling convention id.
  CallConvId _id;
  //! Register assignment strategy.
  CallConvStrategy _strategy;

  //! Red zone size (AMD64 == 128 bytes).
  uint8_t _redZoneSize;
  //! Spill zone size (WIN-X64 == 32 bytes).
  uint8_t _spillZoneSize;
  //! Natural stack alignment as defined by OS/ABI.
  uint8_t _naturalStackAlignment;

  //! \cond INTERNAL
  //! Reserved for future use.
  uint8_t _reserved[2];
  //! \endcond

  //! Calling convention flags.
  CallConvFlags _flags;

  //! Size to save/restore per register group.
  Support::Array<uint8_t, Globals::kNumVirtGroups> _saveRestoreRegSize;
  //! Alignment of save/restore groups.
  Support::Array<uint8_t, Globals::kNumVirtGroups> _saveRestoreAlignment;

  //! Mask of all passed registers, per group.
  Support::Array<RegMask, Globals::kNumVirtGroups> _passedRegs;
  //! Mask of all preserved registers, per group.
  Support::Array<RegMask, Globals::kNumVirtGroups> _preservedRegs;

  //! Passed registers' order.
  union RegOrder {
    //! Passed registers, ordered.
    uint8_t id[kMaxRegArgsPerGroup];
    //! Packed IDs in `uint32_t` array.
    uint32_t packed[(kMaxRegArgsPerGroup + 3) / 4];
  };

  //! Passed registers' order, per register group.
  Support::Array<RegOrder, Globals::kNumVirtGroups> _passedOrder;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Initializes this calling convention to the given `ccId` based on the `environment`.
  //!
  //! See \ref CallConvId and \ref Environment for more details.
  ASMJIT_API Error init(CallConvId ccId, const Environment& environment) noexcept;

  //! Resets this CallConv struct into a defined state.
  //!
  //! It's recommended to reset the \ref CallConv struct in case you would like create a custom calling convention
  //! as it prevents from using an uninitialized data (CallConv doesn't have a constructor that would initialize it,
  //! it's just a struct).
  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    *this = CallConv{};
    memset(_passedOrder.data(), 0xFF, sizeof(_passedOrder));
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the target architecture of this calling convention.
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return _arch; }
  //! Sets the target architecture of this calling convention.
  ASMJIT_INLINE_NODEBUG void setArch(Arch arch) noexcept { _arch = arch; }

  //! Returns the calling convention id.
  ASMJIT_INLINE_NODEBUG CallConvId id() const noexcept { return _id; }
  //! Sets the calling convention id.
  ASMJIT_INLINE_NODEBUG void setId(CallConvId ccId) noexcept { _id = ccId; }

  //! Returns the strategy used to assign registers to arguments.
  ASMJIT_INLINE_NODEBUG CallConvStrategy strategy() const noexcept { return _strategy; }
  //! Sets the strategy used to assign registers to arguments.
  ASMJIT_INLINE_NODEBUG void setStrategy(CallConvStrategy ccStrategy) noexcept { _strategy = ccStrategy; }

  //! Tests whether the calling convention has the given `flag` set.
  ASMJIT_INLINE_NODEBUG bool hasFlag(CallConvFlags flag) const noexcept { return Support::test(_flags, flag); }
  //! Returns the calling convention flags, see `Flags`.
  ASMJIT_INLINE_NODEBUG CallConvFlags flags() const noexcept { return _flags; }
  //! Adds the calling convention flags, see `Flags`.
  ASMJIT_INLINE_NODEBUG void setFlags(CallConvFlags flag) noexcept { _flags = flag; };
  //! Adds the calling convention flags, see `Flags`.
  ASMJIT_INLINE_NODEBUG void addFlags(CallConvFlags flags) noexcept { _flags |= flags; };

  //! Tests whether this calling convention specifies 'RedZone'.
  ASMJIT_INLINE_NODEBUG bool hasRedZone() const noexcept { return _redZoneSize != 0; }
  //! Tests whether this calling convention specifies 'SpillZone'.
  ASMJIT_INLINE_NODEBUG bool hasSpillZone() const noexcept { return _spillZoneSize != 0; }

  //! Returns size of 'RedZone'.
  ASMJIT_INLINE_NODEBUG uint32_t redZoneSize() const noexcept { return _redZoneSize; }
  //! Returns size of 'SpillZone'.
  ASMJIT_INLINE_NODEBUG uint32_t spillZoneSize() const noexcept { return _spillZoneSize; }

  //! Sets size of 'RedZone'.
  ASMJIT_INLINE_NODEBUG void setRedZoneSize(uint32_t size) noexcept { _redZoneSize = uint8_t(size); }
  //! Sets size of 'SpillZone'.
  ASMJIT_INLINE_NODEBUG void setSpillZoneSize(uint32_t size) noexcept { _spillZoneSize = uint8_t(size); }

  //! Returns a natural stack alignment.
  ASMJIT_INLINE_NODEBUG uint32_t naturalStackAlignment() const noexcept { return _naturalStackAlignment; }
  //! Sets a natural stack alignment.
  //!
  //! This function can be used to override the default stack alignment in case that you know that it's alignment is
  //! different. For example it allows to implement custom calling conventions that guarantee higher stack alignment.
  ASMJIT_INLINE_NODEBUG void setNaturalStackAlignment(uint32_t value) noexcept { _naturalStackAlignment = uint8_t(value); }

  //! Returns the size of a register (or its part) to be saved and restored of the given `group`.
  ASMJIT_INLINE_NODEBUG uint32_t saveRestoreRegSize(RegGroup group) const noexcept { return _saveRestoreRegSize[group]; }
  //! Sets the size of a vector register (or its part) to be saved and restored.
  ASMJIT_INLINE_NODEBUG void setSaveRestoreRegSize(RegGroup group, uint32_t size) noexcept { _saveRestoreRegSize[group] = uint8_t(size); }

  //! Returns the alignment of a save-restore area of the given `group`.
  ASMJIT_INLINE_NODEBUG uint32_t saveRestoreAlignment(RegGroup group) const noexcept { return _saveRestoreAlignment[group]; }
  //! Sets the alignment of a save-restore area of the given `group`.
  ASMJIT_INLINE_NODEBUG void setSaveRestoreAlignment(RegGroup group, uint32_t alignment) noexcept { _saveRestoreAlignment[group] = uint8_t(alignment); }

  //! Returns the order of passed registers of the given `group`.
  inline const uint8_t* passedOrder(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _passedOrder[size_t(group)].id;
  }

  //! Returns the mask of passed registers of the given `group`.
  inline RegMask passedRegs(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _passedRegs[size_t(group)];
  }

  inline void _setPassedPacked(RegGroup group, uint32_t p0, uint32_t p1, uint32_t p2, uint32_t p3) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);

    _passedOrder[group].packed[0] = p0;
    _passedOrder[group].packed[1] = p1;
    _passedOrder[group].packed[2] = p2;
    _passedOrder[group].packed[3] = p3;
  }

  //! Resets the order and mask of passed registers.
  inline void setPassedToNone(RegGroup group) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);

    _setPassedPacked(group, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu, 0xFFFFFFFFu);
    _passedRegs[size_t(group)] = 0u;
  }

  //! Sets the order and mask of passed registers.
  inline void setPassedOrder(RegGroup group, uint32_t a0, uint32_t a1 = 0xFF, uint32_t a2 = 0xFF, uint32_t a3 = 0xFF, uint32_t a4 = 0xFF, uint32_t a5 = 0xFF, uint32_t a6 = 0xFF, uint32_t a7 = 0xFF) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);

    // NOTE: This should always be called with all arguments known at compile time, so even if it looks scary it
    // should be translated into few instructions.
    _setPassedPacked(group, Support::bytepack32_4x8(a0, a1, a2, a3),
                            Support::bytepack32_4x8(a4, a5, a6, a7),
                            0xFFFFFFFFu,
                            0xFFFFFFFFu);

    _passedRegs[group] = (a0 != 0xFF ? 1u << a0 : 0u) |
                         (a1 != 0xFF ? 1u << a1 : 0u) |
                         (a2 != 0xFF ? 1u << a2 : 0u) |
                         (a3 != 0xFF ? 1u << a3 : 0u) |
                         (a4 != 0xFF ? 1u << a4 : 0u) |
                         (a5 != 0xFF ? 1u << a5 : 0u) |
                         (a6 != 0xFF ? 1u << a6 : 0u) |
                         (a7 != 0xFF ? 1u << a7 : 0u) ;
  }

  //! Returns preserved register mask of the given `group`.
  inline RegMask preservedRegs(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _preservedRegs[group];
  }

  //! Sets preserved register mask of the given `group`.
  inline void setPreservedRegs(RegGroup group, RegMask regs) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    _preservedRegs[group] = regs;
  }

  //! \}
};

//! Function signature.
//!
//! Contains information about a function return type, count of arguments, and their TypeIds. Function signature
//! is a low level structure which doesn't contain platform specific or calling convention specific information.
//! It's typically used to describe function arguments in a C-API like form, which is then used to calculate a
//! \ref FuncDetail instance, which then maps function signature into a platform and calling convention specific
//! format.
//!
//! Function signature can be built either dynamically by using \ref addArg() and \ref addArgT() functionality,
//! or dynamically by using a template-based \ref FuncSignature::build() function, which maps template types
//! into a function signature.
struct FuncSignature {
  //! \name Constants
  //! \{

  //! Doesn't have variable number of arguments (`...`).
  static constexpr uint8_t kNoVarArgs = 0xFFu;

  //! \}

  //! \name Members
  //! \{

  //! Calling convention id.
  CallConvId _ccId = CallConvId::kCDecl;
  //! Count of arguments.
  uint8_t _argCount = 0;
  //! Index of a first VA or `kNoVarArgs`.
  uint8_t _vaIndex = kNoVarArgs;
  //! Return value TypeId.
  TypeId _ret = TypeId::kVoid;
  //! Reserved for future use.
  uint8_t _reserved[4] {};
  //! Function argument TypeIds.
  TypeId _args[Globals::kMaxFuncArgs] {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Default constructed function signature, initialized to \ref CallConvId::kCDecl, having no return value and no arguments.
  ASMJIT_FORCE_INLINE constexpr FuncSignature() = default;

  //! Copy constructor, which is initialized to the same function signature as `other`.
  ASMJIT_FORCE_INLINE constexpr FuncSignature(const FuncSignature& other) = default;

  //! Initializes the function signature with calling convention id `ccId` and variable argument's index `vaIndex`.
  ASMJIT_FORCE_INLINE constexpr FuncSignature(CallConvId ccId, uint32_t vaIndex = kNoVarArgs) noexcept
    : _ccId(ccId),
      _vaIndex(uint8_t(vaIndex)) {}

  //! Initializes the function signature with calling convention id `ccId`, `vaIndex`, return value, and function arguments.
  template<typename... Args>
  ASMJIT_FORCE_INLINE constexpr FuncSignature(CallConvId ccId, uint32_t vaIndex, TypeId ret, Args&&...args) noexcept
    : _ccId(ccId),
      _argCount(uint8_t(sizeof...(args))),
      _vaIndex(uint8_t(vaIndex)),
      _ret(ret),
      _args{std::forward<Args>(args)...} {}

  //! Builds a function signature based on `RetValueAndArgs`. The first template argument is a function return type,
  //! and function arguments follow.
  //!
  //! \note This function returns a new function signature, which can be passed to functions where it's required. It's
  //! a convenience function that allows to build function signature statically based on types known at compile time,
  //! which is common in JIT code generation.
  template<typename... RetValueAndArgs>
  static ASMJIT_INLINE_NODEBUG constexpr FuncSignature build(CallConvId ccId = CallConvId::kCDecl, uint32_t vaIndex = kNoVarArgs) noexcept {
    return FuncSignature(ccId, vaIndex, (TypeId(TypeUtils::TypeIdOfT<RetValueAndArgs>::kTypeId))... );
  }

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Copy assignment - function signature can be copied by value.
  ASMJIT_FORCE_INLINE FuncSignature& operator=(const FuncSignature& other) noexcept = default;

  //! Compares this function signature with `other` for equality..
  ASMJIT_FORCE_INLINE bool operator==(const FuncSignature& other) const noexcept { return equals(other); }
  //! Compares this function signature with `other` for inequality..
  ASMJIT_FORCE_INLINE bool operator!=(const FuncSignature& other) const noexcept { return !equals(other); }

  //! \}

  //! \name Initialization & Reset
  //! \{

  //! Resets this function signature to a default constructed state.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = FuncSignature{}; }

  //! \}

  //! \name Equality & Comparison
  //! \{

  //! Compares this function signature with `other` for equality..
  ASMJIT_INLINE_NODEBUG bool equals(const FuncSignature& other) const noexcept {
    return _ccId == other._ccId &&
           _argCount == other._argCount &&
           _vaIndex == other._vaIndex &&
           _ret == other._ret &&
           memcmp(_args, other._args, sizeof(_args)) == 0;
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the calling convention.
  ASMJIT_INLINE_NODEBUG CallConvId callConvId() const noexcept { return _ccId; }
  //! Sets the calling convention to `ccId`;
  ASMJIT_INLINE_NODEBUG void setCallConvId(CallConvId ccId) noexcept { _ccId = ccId; }

  //! Tests whether the function signature has a return value.
  ASMJIT_INLINE_NODEBUG bool hasRet() const noexcept { return _ret != TypeId::kVoid; }
  //! Returns the type of the return value.
  ASMJIT_INLINE_NODEBUG TypeId ret() const noexcept { return _ret; }
  //! Sets the return type to `retType`.
  ASMJIT_INLINE_NODEBUG void setRet(TypeId retType) noexcept { _ret = retType; }
  //! Sets the return type based on `T`.
  template<typename T>
  ASMJIT_INLINE_NODEBUG void setRetT() noexcept { setRet(TypeId(TypeUtils::TypeIdOfT<T>::kTypeId)); }


  //! Returns the array of function arguments' types.
  ASMJIT_INLINE_NODEBUG const TypeId* args() const noexcept { return _args; }
  //! Returns the number of function arguments.
  ASMJIT_INLINE_NODEBUG uint32_t argCount() const noexcept { return _argCount; }

  //! Returns the type of the argument at index `i`.
  inline TypeId arg(uint32_t i) const noexcept {
    ASMJIT_ASSERT(i < _argCount);
    return _args[i];
  }

  //! Sets the argument at index `index` to `argType`.
  inline void setArg(uint32_t index, TypeId argType) noexcept {
    ASMJIT_ASSERT(index < _argCount);
    _args[index] = argType;
  }
  //! Sets the argument at index `i` to the type based on `T`.
  template<typename T>
  inline void setArgT(uint32_t index) noexcept { setArg(index, TypeId(TypeUtils::TypeIdOfT<T>::kTypeId)); }

  //! Tests whether an argument can be added to the signature, use before calling \ref addArg() and \ref addArgT().
  //!
  //! \note If you know that you are not adding more arguments than \ref Globals::kMaxFuncArgs then it's not necessary
  //! to use this function. However, if you are adding arguments based on user input, for example, then either check
  //! the number of arguments before using function signature or use \ref canAddArg() before actually adding them to
  //! the function signature.
  inline bool canAddArg() const noexcept { return _argCount < Globals::kMaxFuncArgs; }

  //! Appends an argument of `type` to the function prototype.
  inline void addArg(TypeId type) noexcept {
    ASMJIT_ASSERT(_argCount < Globals::kMaxFuncArgs);
    _args[_argCount++] = type;
  }

  //! Appends an argument of type based on `T` to the function prototype.
  template<typename T>
  inline void addArgT() noexcept { addArg(TypeId(TypeUtils::TypeIdOfT<T>::kTypeId)); }

  //! Tests whether the function has variable number of arguments (...).
  ASMJIT_INLINE_NODEBUG bool hasVarArgs() const noexcept { return _vaIndex != kNoVarArgs; }
  //! Returns the variable arguments (...) index, `kNoVarArgs` if none.
  ASMJIT_INLINE_NODEBUG uint32_t vaIndex() const noexcept { return _vaIndex; }
  //! Sets the variable arguments (...) index to `index`.
  ASMJIT_INLINE_NODEBUG void setVaIndex(uint32_t index) noexcept { _vaIndex = uint8_t(index); }
  //! Resets the variable arguments index (making it a non-va function).
  ASMJIT_INLINE_NODEBUG void resetVaIndex() noexcept { _vaIndex = kNoVarArgs; }

  //! \}
};

//! Argument or return value (or its part) as defined by `FuncSignature`, but with register or stack address
//! (and other metadata) assigned.
struct FuncValue {
  //! \name Constants
  //! \{

  enum Bits : uint32_t {
    kTypeIdShift      = 0,             //!< TypeId shift.
    kTypeIdMask       = 0x000000FFu,   //!< TypeId mask.

    kFlagIsReg        = 0x00000100u,   //!< Passed by register.
    kFlagIsStack      = 0x00000200u,   //!< Passed by stack.
    kFlagIsIndirect   = 0x00000400u,   //!< Passed indirectly by reference (internally a pointer).
    kFlagIsDone       = 0x00000800u,   //!< Used internally by arguments allocator.

    kStackOffsetShift = 12,            //!< Stack offset shift.
    kStackOffsetMask  = 0xFFFFF000u,   //!< Stack offset mask (must occupy MSB bits).

    kRegIdShift       = 16,            //!< RegId shift.
    kRegIdMask        = 0x00FF0000u,   //!< RegId mask.

    kRegTypeShift     = 24,            //!< RegType shift.
    kRegTypeMask      = 0xFF000000u    //!< RegType mask.
  };

  //! \}

  //! \name Members
  //! \{

  uint32_t _data;

  //! \}

  //! \name Initialization & Reset
  //!
  //! These initialize the whole `FuncValue` to either register or stack. Useful when you know all of these
  //! properties and wanna just set it up.
  //!
  //! \{

  //! Initializes this `FuncValue` only to the `typeId` provided - the rest of the values will be cleared.
  ASMJIT_INLINE_NODEBUG void initTypeId(TypeId typeId) noexcept {
    _data = uint32_t(typeId) << kTypeIdShift;
  }

  //! Initializes this `FuncValue` to a register of `regType`, `regId`, and assigns its `typeId` and `flags`.
  ASMJIT_INLINE_NODEBUG void initReg(RegType regType, uint32_t regId, TypeId typeId, uint32_t flags = 0) noexcept {
    _data = (uint32_t(regType) << kRegTypeShift) | (regId << kRegIdShift) | (uint32_t(typeId) << kTypeIdShift) | kFlagIsReg | flags;
  }

  //! Initializes this `FuncValue` to a stack at the given `offset` and assigns its `typeId`.
  ASMJIT_INLINE_NODEBUG void initStack(int32_t offset, TypeId typeId) noexcept {
    _data = (uint32_t(offset) << kStackOffsetShift) | (uint32_t(typeId) << kTypeIdShift) | kFlagIsStack;
  }

  //! Resets the value to its unassigned state.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { _data = 0; }

  //! \}

  //! \name Assign
  //!
  //! These initialize only part of `FuncValue`, useful when building `FuncValue` incrementally. The caller
  //! should first init the type-id by calling `initTypeId` and then continue building either register or stack.
  //!
  //! \{

  //! Assigns a register of `regType` and `regId`.
  inline void assignRegData(RegType regType, uint32_t regId) noexcept {
    ASMJIT_ASSERT((_data & (kRegTypeMask | kRegIdMask)) == 0);
    _data |= (uint32_t(regType) << kRegTypeShift) | (regId << kRegIdShift) | kFlagIsReg;
  }

  //! Assigns a stack location at `offset`.
  inline void assignStackOffset(int32_t offset) noexcept {
    ASMJIT_ASSERT((_data & kStackOffsetMask) == 0);
    _data |= (uint32_t(offset) << kStackOffsetShift) | kFlagIsStack;
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns true if the value is initialized (explicit bool cast).
  ASMJIT_INLINE_NODEBUG explicit operator bool() const noexcept { return _data != 0; }

  //! \cond INTERNAL
  ASMJIT_INLINE_NODEBUG void _replaceValue(uint32_t mask, uint32_t value) noexcept { _data = (_data & ~mask) | value; }
  //! \endcond

  //! Tests whether the `FuncValue` has a flag `flag` set.
  ASMJIT_INLINE_NODEBUG bool hasFlag(uint32_t flag) const noexcept { return Support::test(_data, flag); }
  //! Adds `flags` to `FuncValue`.
  ASMJIT_INLINE_NODEBUG void addFlags(uint32_t flags) noexcept { _data |= flags; }
  //! Clears `flags` of `FuncValue`.
  ASMJIT_INLINE_NODEBUG void clearFlags(uint32_t flags) noexcept { _data &= ~flags; }

  //! Tests whether the value is initialized (i.e. contains a valid data).
  ASMJIT_INLINE_NODEBUG bool isInitialized() const noexcept { return _data != 0; }
  //! Tests whether the argument is passed by register.
  ASMJIT_INLINE_NODEBUG bool isReg() const noexcept { return hasFlag(kFlagIsReg); }
  //! Tests whether the argument is passed by stack.
  ASMJIT_INLINE_NODEBUG bool isStack() const noexcept { return hasFlag(kFlagIsStack); }
  //! Tests whether the argument is passed by register.
  ASMJIT_INLINE_NODEBUG bool isAssigned() const noexcept { return hasFlag(kFlagIsReg | kFlagIsStack); }
  //! Tests whether the argument is passed through a pointer (used by WIN64 to pass XMM|YMM|ZMM).
  ASMJIT_INLINE_NODEBUG bool isIndirect() const noexcept { return hasFlag(kFlagIsIndirect); }

  //! Tests whether the argument was already processed (used internally).
  ASMJIT_INLINE_NODEBUG bool isDone() const noexcept { return hasFlag(kFlagIsDone); }

  //! Returns a register type of the register used to pass function argument or return value.
  ASMJIT_INLINE_NODEBUG RegType regType() const noexcept { return RegType((_data & kRegTypeMask) >> kRegTypeShift); }
  //! Sets a register type of the register used to pass function argument or return value.
  ASMJIT_INLINE_NODEBUG void setRegType(RegType regType) noexcept { _replaceValue(kRegTypeMask, uint32_t(regType) << kRegTypeShift); }

  //! Returns a physical id of the register used to pass function argument or return value.
  ASMJIT_INLINE_NODEBUG uint32_t regId() const noexcept { return (_data & kRegIdMask) >> kRegIdShift; }
  //! Sets a physical id of the register used to pass function argument or return value.
  ASMJIT_INLINE_NODEBUG void setRegId(uint32_t regId) noexcept { _replaceValue(kRegIdMask, regId << kRegIdShift); }

  //! Returns a stack offset of this argument.
  ASMJIT_INLINE_NODEBUG int32_t stackOffset() const noexcept { return int32_t(_data & kStackOffsetMask) >> kStackOffsetShift; }
  //! Sets a stack offset of this argument.
  ASMJIT_INLINE_NODEBUG void setStackOffset(int32_t offset) noexcept { _replaceValue(kStackOffsetMask, uint32_t(offset) << kStackOffsetShift); }

  //! Tests whether the argument or return value has associated `TypeId`.
  ASMJIT_INLINE_NODEBUG bool hasTypeId() const noexcept { return Support::test(_data, kTypeIdMask); }
  //! Returns a TypeId of this argument or return value.
  ASMJIT_INLINE_NODEBUG TypeId typeId() const noexcept { return TypeId((_data & kTypeIdMask) >> kTypeIdShift); }
  //! Sets a TypeId of this argument or return value.
  ASMJIT_INLINE_NODEBUG void setTypeId(TypeId typeId) noexcept { _replaceValue(kTypeIdMask, uint32_t(typeId) << kTypeIdShift); }

  //! \}
};

//! Contains multiple `FuncValue` instances in an array so functions that use multiple registers for arguments or
//! return values can represent all inputs and outputs.
struct FuncValuePack {
public:
  //! \name Members
  //! \{

  //! Values of the pack.
  FuncValue _values[Globals::kMaxValuePack];

  //! \}

  //! \name Initialization & Reset
  //! \{

  //! Resets all values in the pack.
  inline void reset() noexcept {
    for (size_t i = 0; i < Globals::kMaxValuePack; i++)
      _values[i].reset();
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Calculates how many values are in the pack, checking for non-values from the end.
  inline uint32_t count() const noexcept {
    uint32_t n = Globals::kMaxValuePack;
    while (n && !_values[n - 1])
      n--;
    return n;
  }

  //! Returns values in this value in the pack.
  //!
  //! \note The returned array has exactly \ref Globals::kMaxValuePack elements.
  ASMJIT_INLINE_NODEBUG FuncValue* values() noexcept { return _values; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const FuncValue* values() const noexcept { return _values; }

  //! Resets a value at the given `index` in the pack, which makes it unassigned.
  inline void resetValue(size_t index) noexcept {
    ASMJIT_ASSERT(index < Globals::kMaxValuePack);
    _values[index].reset();
  }

  //! Tests whether the value at the given `index` in the pack is assigned.
  inline bool hasValue(size_t index) noexcept {
    ASMJIT_ASSERT(index < Globals::kMaxValuePack);
    return _values[index].isInitialized();
  }

  //! Assigns a register at the given `index` to `reg` and an optional `typeId`.
  inline void assignReg(size_t index, const BaseReg& reg, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(index < Globals::kMaxValuePack);
    ASMJIT_ASSERT(reg.isPhysReg());
    _values[index].initReg(reg.type(), reg.id(), typeId);
  }

  //! Assigns a register at the given `index` to `regType`, `regId`, and an optional `typeId`.
  inline void assignReg(size_t index, RegType regType, uint32_t regId, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(index < Globals::kMaxValuePack);
    _values[index].initReg(regType, regId, typeId);
  }

  //! Assigns a stack location at the given `index` to `offset` and an optional `typeId`.
  inline void assignStack(size_t index, int32_t offset, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(index < Globals::kMaxValuePack);
    _values[index].initStack(offset, typeId);
  }

  //! Accesses the value in the pack at the given `index`.
  //!
  //! \note The maximum index value is `Globals::kMaxValuePack - 1`.
  inline FuncValue& operator[](size_t index) {
    ASMJIT_ASSERT(index < Globals::kMaxValuePack);
    return _values[index];
  }
  //! \overload
  inline const FuncValue& operator[](size_t index) const {
    ASMJIT_ASSERT(index < Globals::kMaxValuePack);
    return _values[index];
  }

  //! \}
};

//! Attributes are designed in a way that all are initially false, and user or \ref FuncFrame finalizer adds
//! them when necessary.
enum class FuncAttributes : uint32_t {
  //! No attributes.
  kNoAttributes = 0,

  //! Function has variable number of arguments.
  kHasVarArgs = 0x00000001u,
  //! Preserve frame pointer (don't omit FP).
  kHasPreservedFP = 0x00000010u,
  //! Function calls other functions (is not leaf).
  kHasFuncCalls = 0x00000020u,
  //! Function has aligned save/restore of vector registers.
  kAlignedVecSR = 0x00000040u,
  //! Function must begin with an instruction that marks a start of a branch or function.
  //!
  //!   * `ENDBR32/ENDBR64` instruction is inserted at the beginning of the function (X86, X86_64).
  //!   * `BTI` instruction is inserted at the beginning of the function (AArch64)
  kIndirectBranchProtection = 0x00000080u,
  //! FuncFrame is finalized and can be used by prolog/epilog inserter (PEI).
  kIsFinalized = 0x00000800u,

  // X86 Specific Attributes
  // -----------------------

  //! Enables the use of AVX within the function's body, prolog, and epilog (X86).
  //!
  //! This flag instructs prolog and epilog emitter to use AVX instead of SSE for manipulating XMM registers.
  kX86_AVXEnabled = 0x00010000u,

  //! Enables the use of AVX-512 within the function's body, prolog, and epilog (X86).
  //!
  //! This flag instructs Compiler register allocator to use additional 16 registers introduced by AVX-512.
  //! Additionally, if the functions saves full width of ZMM registers (custom calling conventions only) then
  //! the prolog/epilog inserter would use AVX-512 move instructions to emit the save and restore sequence.
  kX86_AVX512Enabled = 0x00020000u,

  //! This flag instructs the epilog writer to emit EMMS instruction before RET (X86).
  kX86_MMXCleanup = 0x00040000u,

  //! This flag instructs the epilog writer to emit VZEROUPPER instruction before RET (X86).
  kX86_AVXCleanup = 0x00080000u
};
ASMJIT_DEFINE_ENUM_FLAGS(FuncAttributes)

//! Function detail - \ref CallConv and expanded \ref FuncSignature.
//!
//! Function detail is architecture and OS dependent representation of a function. It contains a materialized
//! calling convention and expanded function signature so all arguments have assigned either register type/id
//! or stack address.
class FuncDetail {
public:
  //! \name Constants
  //! \{

  //! Function doesn't have a variable number of arguments (`...`).
  static constexpr uint8_t kNoVarArgs = 0xFFu;

  //! \}

  //! \name Members
  //! \{

  //! Calling convention.
  CallConv _callConv {};
  //! Number of function arguments.
  uint8_t _argCount = 0;
  //! Variable arguments index of `kNoVarArgs`.
  uint8_t _vaIndex = 0;
  //! Reserved for future use.
  uint16_t _reserved = 0;
  //! Registers that contain arguments.
  Support::Array<RegMask, Globals::kNumVirtGroups> _usedRegs {};
  //! Size of arguments passed by stack.
  uint32_t _argStackSize = 0;
  //! Function return value(s).
  FuncValuePack _rets {};
  //! Function arguments.
  FuncValuePack _args[Globals::kMaxFuncArgs] {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a default constructed \ref FuncDetail.
  ASMJIT_INLINE_NODEBUG FuncDetail() noexcept {}

  //! Copy constructor.
  //!
  //! Function details are copyable.
  ASMJIT_INLINE_NODEBUG FuncDetail(const FuncDetail& other) noexcept = default;

  //! Initializes this `FuncDetail` to the given signature.
  ASMJIT_API Error init(const FuncSignature& signature, const Environment& environment) noexcept;

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Assignment operator, copies `other` to this \ref FuncDetail.
  ASMJIT_INLINE_NODEBUG FuncDetail& operator=(const FuncDetail& other) noexcept = default;

  //! \}

  //! \name Reset
  //! \{

  //! Resets the function detail to its default constructed state.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = FuncDetail{}; }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the function's calling convention, see `CallConv`.
  ASMJIT_INLINE_NODEBUG const CallConv& callConv() const noexcept { return _callConv; }

  //! Returns the associated calling convention flags, see `CallConv::Flags`.
  ASMJIT_INLINE_NODEBUG CallConvFlags flags() const noexcept { return _callConv.flags(); }
  //! Checks whether a CallConv `flag` is set, see `CallConv::Flags`.
  ASMJIT_INLINE_NODEBUG bool hasFlag(CallConvFlags ccFlag) const noexcept { return _callConv.hasFlag(ccFlag); }

  //! Tests whether the function has a return value.
  ASMJIT_INLINE_NODEBUG bool hasRet() const noexcept { return bool(_rets[0]); }
  //! Returns the number of function arguments.
  ASMJIT_INLINE_NODEBUG uint32_t argCount() const noexcept { return _argCount; }

  //! Returns function return values.
  ASMJIT_INLINE_NODEBUG FuncValuePack& retPack() noexcept { return _rets; }
  //! Returns function return values.
  ASMJIT_INLINE_NODEBUG const FuncValuePack& retPack() const noexcept { return _rets; }

  //! Returns a function return value associated with the given `valueIndex`.
  ASMJIT_INLINE_NODEBUG FuncValue& ret(size_t valueIndex = 0) noexcept { return _rets[valueIndex]; }
  //! Returns a function return value associated with the given `valueIndex` (const).
  ASMJIT_INLINE_NODEBUG const FuncValue& ret(size_t valueIndex = 0) const noexcept { return _rets[valueIndex]; }

  //! Returns function argument packs array.
  ASMJIT_INLINE_NODEBUG FuncValuePack* argPacks() noexcept { return _args; }
  //! Returns function argument packs array (const).
  ASMJIT_INLINE_NODEBUG const FuncValuePack* argPacks() const noexcept { return _args; }

  //! Returns function argument pack at the given `argIndex`.
  inline FuncValuePack& argPack(size_t argIndex) noexcept {
    ASMJIT_ASSERT(argIndex < Globals::kMaxFuncArgs);
    return _args[argIndex];
  }

  //! Returns function argument pack at the given `argIndex` (const).
  inline const FuncValuePack& argPack(size_t argIndex) const noexcept {
    ASMJIT_ASSERT(argIndex < Globals::kMaxFuncArgs);
    return _args[argIndex];
  }

  //! Returns an argument at `valueIndex` from the argument pack at the given `argIndex`.
  inline FuncValue& arg(size_t argIndex, size_t valueIndex = 0) noexcept {
    ASMJIT_ASSERT(argIndex < Globals::kMaxFuncArgs);
    return _args[argIndex][valueIndex];
  }

  //! Returns an argument at `valueIndex` from the argument pack at the given `argIndex` (const).
  inline const FuncValue& arg(size_t argIndex, size_t valueIndex = 0) const noexcept {
    ASMJIT_ASSERT(argIndex < Globals::kMaxFuncArgs);
    return _args[argIndex][valueIndex];
  }

  //! Resets an argument at the given `argIndex`.
  //!
  //! If the argument is a parameter pack (has multiple values) all values are reset.
  inline void resetArg(size_t argIndex) noexcept {
    ASMJIT_ASSERT(argIndex < Globals::kMaxFuncArgs);
    _args[argIndex].reset();
  }

  //! Tests whether the function has variable arguments.
  ASMJIT_INLINE_NODEBUG bool hasVarArgs() const noexcept { return _vaIndex != kNoVarArgs; }
  //! Returns an index of a first variable argument.
  ASMJIT_INLINE_NODEBUG uint32_t vaIndex() const noexcept { return _vaIndex; }

  //! Tests whether the function passes one or more argument by stack.
  ASMJIT_INLINE_NODEBUG bool hasStackArgs() const noexcept { return _argStackSize != 0; }
  //! Returns stack size needed for function arguments passed on the stack.
  ASMJIT_INLINE_NODEBUG uint32_t argStackSize() const noexcept { return _argStackSize; }

  //! Returns red zone size.
  ASMJIT_INLINE_NODEBUG uint32_t redZoneSize() const noexcept { return _callConv.redZoneSize(); }
  //! Returns spill zone size.
  ASMJIT_INLINE_NODEBUG uint32_t spillZoneSize() const noexcept { return _callConv.spillZoneSize(); }
  //! Returns natural stack alignment.
  ASMJIT_INLINE_NODEBUG uint32_t naturalStackAlignment() const noexcept { return _callConv.naturalStackAlignment(); }

  //! Returns a mask of all passed registers of the given register `group`.
  ASMJIT_INLINE_NODEBUG RegMask passedRegs(RegGroup group) const noexcept { return _callConv.passedRegs(group); }
  //! Returns a mask of all preserved registers of the given register `group`.
  ASMJIT_INLINE_NODEBUG RegMask preservedRegs(RegGroup group) const noexcept { return _callConv.preservedRegs(group); }

  //! Returns a mask of all used registers of the given register `group`.
  inline RegMask usedRegs(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _usedRegs[size_t(group)];
  }

  //! Adds `regs` to the mask of used registers of the given register `group`.
  inline void addUsedRegs(RegGroup group, RegMask regs) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    _usedRegs[size_t(group)] |= regs;
  }

  //! \}
};

//! Function frame.
//!
//! Function frame is used directly by prolog and epilog insertion (PEI) utils. It provides information necessary to
//! insert a proper and ABI conforming prolog and epilog. Function frame calculation is based on `CallConv` and
//! other function attributes.
//!
//! SSE vs AVX vs AVX-512
//! ---------------------
//!
//! Function frame provides a way to tell prolog/epilog inserter to use AVX instructions instead of SSE. Use
//! `setAvxEnabled()` and `setAvx512Enabled()`  to enable AVX and/or AVX-512, respectively. Enabling AVX-512
//! is mostly for Compiler as it would use 32 SIMD registers instead of 16 when enabled.
//!
//! \note If your code uses AVX instructions and AVX is not enabled there would be a performance hit in case that
//! some registers had to be saved/restored in function's prolog/epilog, respectively. Thus, it's recommended to
//! always let the function frame know about the use of AVX.
//!
//! Function Frame Structure
//! ------------------------
//!
//! Various properties can contribute to the size and structure of the function frame. The function frame in most
//! cases won't use all of the properties illustrated (for example Spill Zone and Red Zone are never used together).
//!
//! ```
//!   +-----------------------------+
//!   | Arguments Passed by Stack   |
//!   +-----------------------------+
//!   | Spill Zone                  |
//!   +-----------------------------+ <- Stack offset (args) starts from here.
//!   | Return Address, if Pushed   |
//!   +-----------------------------+ <- Stack pointer (SP) upon entry.
//!   | Save/Restore Stack.         |
//!   +-----------------------------+-----------------------------+
//!   | Local Stack                 |                             |
//!   +-----------------------------+          Final Stack        |
//!   | Call Stack                  |                             |
//!   +-----------------------------+-----------------------------+ <- SP after prolog.
//!   | Red Zone                    |
//!   +-----------------------------+
//! ```
class FuncFrame {
public:
  //! \name Constants
  //! \{

  enum : uint32_t {
    //! Tag used to inform that some offset is invalid.
    kTagInvalidOffset = 0xFFFFFFFFu
  };

  //! \}

  //! \name Types
  //! \{

  using RegMasks = Support::Array<RegMask, Globals::kNumVirtGroups>;

  //! \}

  //! \name Members
  //! \{

  //! Function attributes.
  FuncAttributes _attributes {};

  //! Target architecture.
  Arch _arch {};
  //! SP register ID (to access call stack and local stack).
  uint8_t _spRegId = uint8_t(BaseReg::kIdBad);
  //! SA register ID (to access stack arguments).
  uint8_t _saRegId = uint8_t(BaseReg::kIdBad);

  //! Red zone size (copied from CallConv).
  uint8_t _redZoneSize = 0;
  //! Spill zone size (copied from CallConv).
  uint8_t _spillZoneSize = 0;
  //! Natural stack alignment (copied from CallConv).
  uint8_t _naturalStackAlignment = 0;
  //! Minimum stack alignment to turn on dynamic alignment.
  uint8_t _minDynamicAlignment = 0;

  //! Call stack alignment.
  uint8_t _callStackAlignment = 0;
  //! Local stack alignment.
  uint8_t _localStackAlignment = 0;
  //! Final stack alignment.
  uint8_t _finalStackAlignment = 0;

  //! Adjustment of the stack before returning (X86-STDCALL).
  uint16_t _calleeStackCleanup = 0;

  //! Call stack size.
  uint32_t _callStackSize = 0;
  //! Local stack size.
  uint32_t _localStackSize = 0;
  //! Final stack size (sum of call stack and local stack).
  uint32_t _finalStackSize = 0;

  //! Local stack offset (non-zero only if call stack is used).
  uint32_t _localStackOffset = 0;
  //! Offset relative to SP that contains previous SP (before alignment).
  uint32_t _daOffset = 0;
  //! Offset of the first stack argument relative to SP.
  uint32_t _saOffsetFromSP = 0;
  //! Offset of the first stack argument relative to SA (_saRegId or FP).
  uint32_t _saOffsetFromSA = 0;

  //! Local stack adjustment in prolog/epilog.
  uint32_t _stackAdjustment = 0;

  //! Registers that are dirty.
  RegMasks _dirtyRegs {};
  //! Registers that must be preserved (copied from CallConv).
  RegMasks _preservedRegs {};
  //! Size to save/restore per register group.
  Support::Array<uint8_t, Globals::kNumVirtGroups> _saveRestoreRegSize {};
  //! Alignment of save/restore area per register group.
  Support::Array<uint8_t, Globals::kNumVirtGroups> _saveRestoreAlignment {};

  //! Stack size required to save registers with push/pop.
  uint16_t _pushPopSaveSize = 0;
  //! Stack size required to save extra registers that cannot use push/pop.
  uint16_t _extraRegSaveSize = 0;
  //! Offset where registers saved/restored via push/pop are stored
  uint32_t _pushPopSaveOffset = 0;
  //! Offset where extra registers that cannot use push/pop are stored.
  uint32_t _extraRegSaveOffset = 0;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a default constructed function frame, which has initialized all members to their default values.
  ASMJIT_INLINE_NODEBUG FuncFrame() noexcept = default;
  //! Creates a copy of `other` function frame.
  ASMJIT_INLINE_NODEBUG FuncFrame(const FuncFrame& other) noexcept = default;

  //! \}

  //! \name Initialization & Reset
  //! \{

  //! Initializes the function frame based on `func` detail.
  ASMJIT_API Error init(const FuncDetail& func) noexcept;
  //! Resets the function frame into its default constructed state.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = FuncFrame{}; }

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Copy assignment - function frame is copy assignable.
  ASMJIT_INLINE_NODEBUG FuncFrame& operator=(const FuncFrame& other) noexcept = default;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the target architecture of the function frame.
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return _arch; }

  //! Returns function frame attributes, see `Attributes`.
  ASMJIT_INLINE_NODEBUG FuncAttributes attributes() const noexcept { return _attributes; }
  //! Checks whether the FuncFame contains an attribute `attr`.
  ASMJIT_INLINE_NODEBUG bool hasAttribute(FuncAttributes attr) const noexcept { return Support::test(_attributes, attr); }
  //! Adds attributes `attrs` to the FuncFrame.
  ASMJIT_INLINE_NODEBUG void addAttributes(FuncAttributes attrs) noexcept { _attributes |= attrs; }
  //! Clears attributes `attrs` from the FrameFrame.
  ASMJIT_INLINE_NODEBUG void clearAttributes(FuncAttributes attrs) noexcept { _attributes &= ~attrs; }

  //! Tests whether the function has variable number of arguments.
  ASMJIT_INLINE_NODEBUG bool hasVarArgs() const noexcept { return hasAttribute(FuncAttributes::kHasVarArgs); }
  //! Sets the variable arguments flag.
  ASMJIT_INLINE_NODEBUG void setVarArgs() noexcept { addAttributes(FuncAttributes::kHasVarArgs); }
  //! Resets variable arguments flag.
  ASMJIT_INLINE_NODEBUG void resetVarArgs() noexcept { clearAttributes(FuncAttributes::kHasVarArgs); }

  //! Tests whether the function preserves frame pointer (EBP|ESP on X86).
  ASMJIT_INLINE_NODEBUG bool hasPreservedFP() const noexcept { return hasAttribute(FuncAttributes::kHasPreservedFP); }
  //! Enables preserved frame pointer.
  ASMJIT_INLINE_NODEBUG void setPreservedFP() noexcept { addAttributes(FuncAttributes::kHasPreservedFP); }
  //! Disables preserved frame pointer.
  ASMJIT_INLINE_NODEBUG void resetPreservedFP() noexcept { clearAttributes(FuncAttributes::kHasPreservedFP); }

  //! Tests whether the function calls other functions.
  ASMJIT_INLINE_NODEBUG bool hasFuncCalls() const noexcept { return hasAttribute(FuncAttributes::kHasFuncCalls); }
  //! Sets `FuncAttributes::kHasFuncCalls` to true.
  ASMJIT_INLINE_NODEBUG void setFuncCalls() noexcept { addAttributes(FuncAttributes::kHasFuncCalls); }
  //! Sets `FuncAttributes::kHasFuncCalls` to false.
  ASMJIT_INLINE_NODEBUG void resetFuncCalls() noexcept { clearAttributes(FuncAttributes::kHasFuncCalls); }

  //! Tests whether the function uses indirect branch protection, see \ref FuncAttributes::kIndirectBranchProtection.
  ASMJIT_INLINE_NODEBUG bool hasIndirectBranchProtection() const noexcept { return hasAttribute(FuncAttributes::kIndirectBranchProtection); }
  //! Enabled indirect branch protection (sets `FuncAttributes::kIndirectBranchProtection` attribute to true).
  ASMJIT_INLINE_NODEBUG void setIndirectBranchProtection() noexcept { addAttributes(FuncAttributes::kIndirectBranchProtection); }
  //! Disables indirect branch protection (sets `FuncAttributes::kIndirectBranchProtection` attribute to false).
  ASMJIT_INLINE_NODEBUG void resetIndirectBranchProtection() noexcept { clearAttributes(FuncAttributes::kIndirectBranchProtection); }

  //! Tests whether the function has AVX enabled.
  ASMJIT_INLINE_NODEBUG bool isAvxEnabled() const noexcept { return hasAttribute(FuncAttributes::kX86_AVXEnabled); }
  //! Enables AVX use.
  ASMJIT_INLINE_NODEBUG void setAvxEnabled() noexcept { addAttributes(FuncAttributes::kX86_AVXEnabled); }
  //! Disables AVX use.
  ASMJIT_INLINE_NODEBUG void resetAvxEnabled() noexcept { clearAttributes(FuncAttributes::kX86_AVXEnabled); }

  //! Tests whether the function has AVX-512 enabled.
  ASMJIT_INLINE_NODEBUG bool isAvx512Enabled() const noexcept { return hasAttribute(FuncAttributes::kX86_AVX512Enabled); }
  //! Enables AVX-512 use.
  ASMJIT_INLINE_NODEBUG void setAvx512Enabled() noexcept { addAttributes(FuncAttributes::kX86_AVX512Enabled); }
  //! Disables AVX-512 use.
  ASMJIT_INLINE_NODEBUG void resetAvx512Enabled() noexcept { clearAttributes(FuncAttributes::kX86_AVX512Enabled); }

  //! Tests whether the function has MMX cleanup - 'emms' instruction in epilog.
  ASMJIT_INLINE_NODEBUG bool hasMmxCleanup() const noexcept { return hasAttribute(FuncAttributes::kX86_MMXCleanup); }
  //! Enables MMX cleanup.
  ASMJIT_INLINE_NODEBUG void setMmxCleanup() noexcept { addAttributes(FuncAttributes::kX86_MMXCleanup); }
  //! Disables MMX cleanup.
  ASMJIT_INLINE_NODEBUG void resetMmxCleanup() noexcept { clearAttributes(FuncAttributes::kX86_MMXCleanup); }

  //! Tests whether the function has AVX cleanup - 'vzeroupper' instruction in epilog.
  ASMJIT_INLINE_NODEBUG bool hasAvxCleanup() const noexcept { return hasAttribute(FuncAttributes::kX86_AVXCleanup); }
  //! Enables AVX cleanup.
  ASMJIT_INLINE_NODEBUG void setAvxCleanup() noexcept { addAttributes(FuncAttributes::kX86_AVXCleanup); }
  //! Disables AVX cleanup.
  ASMJIT_INLINE_NODEBUG void resetAvxCleanup() noexcept { clearAttributes(FuncAttributes::kX86_AVXCleanup); }

  //! Tests whether the function uses call stack.
  ASMJIT_INLINE_NODEBUG bool hasCallStack() const noexcept { return _callStackSize != 0; }
  //! Tests whether the function uses local stack.
  ASMJIT_INLINE_NODEBUG bool hasLocalStack() const noexcept { return _localStackSize != 0; }
  //! Tests whether vector registers can be saved and restored by using aligned reads and writes.
  ASMJIT_INLINE_NODEBUG bool hasAlignedVecSR() const noexcept { return hasAttribute(FuncAttributes::kAlignedVecSR); }
  //! Tests whether the function has to align stack dynamically.
  ASMJIT_INLINE_NODEBUG bool hasDynamicAlignment() const noexcept { return _finalStackAlignment >= _minDynamicAlignment; }

  //! Tests whether the calling convention specifies 'RedZone'.
  ASMJIT_INLINE_NODEBUG bool hasRedZone() const noexcept { return _redZoneSize != 0; }
  //! Tests whether the calling convention specifies 'SpillZone'.
  ASMJIT_INLINE_NODEBUG bool hasSpillZone() const noexcept { return _spillZoneSize != 0; }

  //! Returns the size of 'RedZone'.
  ASMJIT_INLINE_NODEBUG uint32_t redZoneSize() const noexcept { return _redZoneSize; }
  //! Returns the size of 'SpillZone'.
  ASMJIT_INLINE_NODEBUG uint32_t spillZoneSize() const noexcept { return _spillZoneSize; }

  //! Resets the size of red zone, which would disable it entirely.
  //!
  //! \note Red zone is currently only used by an AMD64 SystemV calling convention, which expects 128
  //! bytes of stack to be accessible below stack pointer. These bytes are then accessible within the
  //! function and Compiler can use this space as a spill area. However, sometimes it's better to
  //! disallow the use of red zone in case that a user wants to use this stack for a custom purpose.
  ASMJIT_INLINE_NODEBUG void resetRedZone() noexcept { _redZoneSize = 0; }

  //! Returns natural stack alignment (guaranteed stack alignment upon entry).
  ASMJIT_INLINE_NODEBUG uint32_t naturalStackAlignment() const noexcept { return _naturalStackAlignment; }
  //! Returns natural stack alignment (guaranteed stack alignment upon entry).
  ASMJIT_INLINE_NODEBUG uint32_t minDynamicAlignment() const noexcept { return _minDynamicAlignment; }

  //! Tests whether the callee must adjust SP before returning (X86-STDCALL only)
  ASMJIT_INLINE_NODEBUG bool hasCalleeStackCleanup() const noexcept { return _calleeStackCleanup != 0; }
  //! Returns home many bytes of the stack the callee must adjust before returning (X86-STDCALL only)
  ASMJIT_INLINE_NODEBUG uint32_t calleeStackCleanup() const noexcept { return _calleeStackCleanup; }

  //! Returns call stack alignment.
  ASMJIT_INLINE_NODEBUG uint32_t callStackAlignment() const noexcept { return _callStackAlignment; }
  //! Returns local stack alignment.
  ASMJIT_INLINE_NODEBUG uint32_t localStackAlignment() const noexcept { return _localStackAlignment; }
  //! Returns final stack alignment (the maximum value of call, local, and natural stack alignments).
  ASMJIT_INLINE_NODEBUG uint32_t finalStackAlignment() const noexcept { return _finalStackAlignment; }

  //! Sets call stack alignment.
  //!
  //! \note This also updates the final stack alignment.
  inline void setCallStackAlignment(uint32_t alignment) noexcept {
    _callStackAlignment = uint8_t(alignment);
    _finalStackAlignment = Support::max(_naturalStackAlignment, _callStackAlignment, _localStackAlignment);
  }

  //! Sets local stack alignment.
  //!
  //! \note This also updates the final stack alignment.
  inline void setLocalStackAlignment(uint32_t value) noexcept {
    _localStackAlignment = uint8_t(value);
    _finalStackAlignment = Support::max(_naturalStackAlignment, _callStackAlignment, _localStackAlignment);
  }

  //! Combines call stack alignment with `alignment`, updating it to the greater value.
  //!
  //! \note This also updates the final stack alignment.
  inline void updateCallStackAlignment(uint32_t alignment) noexcept {
    _callStackAlignment = uint8_t(Support::max<uint32_t>(_callStackAlignment, alignment));
    _finalStackAlignment = Support::max(_finalStackAlignment, _callStackAlignment);
  }

  //! Combines local stack alignment with `alignment`, updating it to the greater value.
  //!
  //! \note This also updates the final stack alignment.
  inline void updateLocalStackAlignment(uint32_t alignment) noexcept {
    _localStackAlignment = uint8_t(Support::max<uint32_t>(_localStackAlignment, alignment));
    _finalStackAlignment = Support::max(_finalStackAlignment, _localStackAlignment);
  }

  //! Returns call stack size.
  ASMJIT_INLINE_NODEBUG uint32_t callStackSize() const noexcept { return _callStackSize; }
  //! Returns local stack size.
  ASMJIT_INLINE_NODEBUG uint32_t localStackSize() const noexcept { return _localStackSize; }

  //! Sets call stack size.
  ASMJIT_INLINE_NODEBUG void setCallStackSize(uint32_t size) noexcept { _callStackSize = size; }
  //! Sets local stack size.
  ASMJIT_INLINE_NODEBUG void setLocalStackSize(uint32_t size) noexcept { _localStackSize = size; }

  //! Combines call stack size with `size`, updating it to the greater value.
  ASMJIT_INLINE_NODEBUG void updateCallStackSize(uint32_t size) noexcept { _callStackSize = Support::max(_callStackSize, size); }
  //! Combines local stack size with `size`, updating it to the greater value.
  ASMJIT_INLINE_NODEBUG void updateLocalStackSize(uint32_t size) noexcept { _localStackSize = Support::max(_localStackSize, size); }

  //! Returns final stack size (only valid after the FuncFrame is finalized).
  ASMJIT_INLINE_NODEBUG uint32_t finalStackSize() const noexcept { return _finalStackSize; }

  //! Returns an offset to access the local stack (non-zero only if call stack is used).
  ASMJIT_INLINE_NODEBUG uint32_t localStackOffset() const noexcept { return _localStackOffset; }

  //! Tests whether the function prolog/epilog requires a memory slot for storing unaligned SP.
  ASMJIT_INLINE_NODEBUG bool hasDAOffset() const noexcept { return _daOffset != kTagInvalidOffset; }
  //! Returns a memory offset used to store DA (dynamic alignment) slot (relative to SP).
  ASMJIT_INLINE_NODEBUG uint32_t daOffset() const noexcept { return _daOffset; }

  ASMJIT_INLINE_NODEBUG uint32_t saOffset(uint32_t regId) const noexcept {
    return regId == _spRegId ? saOffsetFromSP()
                             : saOffsetFromSA();
  }

  ASMJIT_INLINE_NODEBUG uint32_t saOffsetFromSP() const noexcept { return _saOffsetFromSP; }
  ASMJIT_INLINE_NODEBUG uint32_t saOffsetFromSA() const noexcept { return _saOffsetFromSA; }

  //! Returns mask of registers of the given register `group` that are modified by the function. The engine would
  //! then calculate which registers must be saved & restored by the function by using the data provided by the
  //! calling convention.
  inline RegMask dirtyRegs(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _dirtyRegs[group];
  }

  //! Sets which registers (as a mask) are modified by the function.
  //!
  //! \remarks Please note that this will completely overwrite the existing register mask, use `addDirtyRegs()`
  //! to modify the existing register mask.
  inline void setDirtyRegs(RegGroup group, RegMask regs) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    _dirtyRegs[group] = regs;
  }

  //! Adds which registers (as a mask) are modified by the function.
  inline void addDirtyRegs(RegGroup group, RegMask regs) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    _dirtyRegs[group] |= regs;
  }

  //! \overload
  inline void addDirtyRegs(const BaseReg& reg) noexcept {
    ASMJIT_ASSERT(reg.id() < Globals::kMaxPhysRegs);
    addDirtyRegs(reg.group(), Support::bitMask(reg.id()));
  }

  //! \overload
  template<typename... Args>
  inline void addDirtyRegs(const BaseReg& reg, Args&&... args) noexcept {
    addDirtyRegs(reg);
    addDirtyRegs(std::forward<Args>(args)...);
  }

  //! A helper function to set all registers from all register groups dirty.
  //!
  //! \note This should not be used in general as it's the most pessimistic case. However, it can be used for testing
  //! or in cases in which all registers are considered clobbered.
  ASMJIT_INLINE_NODEBUG void setAllDirty() noexcept {
    for (size_t i = 0; i < ASMJIT_ARRAY_SIZE(_dirtyRegs); i++)
      _dirtyRegs[i] = 0xFFFFFFFFu;
  }

  //! A helper function to set all registers from the given register `group` dirty.
  inline void setAllDirty(RegGroup group) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    _dirtyRegs[group] = 0xFFFFFFFFu;
  }

  //! Returns a calculated mask of registers of the given `group` that will be saved and restored in the function's
  //! prolog and epilog, respectively. The register mask is calculated from both `dirtyRegs` (provided by user) and
  //! `preservedMask` (provided by the calling convention).
  inline RegMask savedRegs(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _dirtyRegs[group] & _preservedRegs[group];
  }

  //! Returns all dirty registers as a Support::Array<> type.
  ASMJIT_INLINE_NODEBUG const RegMasks& dirtyRegs() const noexcept { return _dirtyRegs; }

  //! Returns all preserved registers as a Support::Array<> type.
  ASMJIT_INLINE_NODEBUG const RegMasks& preservedRegs() const noexcept { return _preservedRegs; }

  //! Returns the mask of preserved registers of the given register `group`.
  //!
  //! Preserved registers are those that must survive the function call unmodified. The function can only modify
  //! preserved registers it they are saved and restored in function's prolog and epilog, respectively.
  inline RegMask preservedRegs(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _preservedRegs[group];
  }

  //! Returns the size of a save-restore are for the required register `group`.
  inline uint32_t saveRestoreRegSize(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _saveRestoreRegSize[group];
  }

  inline uint32_t saveRestoreAlignment(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _saveRestoreAlignment[group];
  }

  ASMJIT_INLINE_NODEBUG bool hasSARegId() const noexcept { return _saRegId != BaseReg::kIdBad; }
  ASMJIT_INLINE_NODEBUG uint32_t saRegId() const noexcept { return _saRegId; }
  ASMJIT_INLINE_NODEBUG void setSARegId(uint32_t regId) { _saRegId = uint8_t(regId); }
  ASMJIT_INLINE_NODEBUG void resetSARegId() { setSARegId(BaseReg::kIdBad); }

  //! Returns stack size required to save/restore registers via push/pop.
  ASMJIT_INLINE_NODEBUG uint32_t pushPopSaveSize() const noexcept { return _pushPopSaveSize; }
  //! Returns an offset to the stack where registers are saved via push/pop.
  ASMJIT_INLINE_NODEBUG uint32_t pushPopSaveOffset() const noexcept { return _pushPopSaveOffset; }

  //! Returns stack size required to save/restore extra registers that don't use push/pop/
  //!
  //! \note On X86 this covers all registers except GP registers, on other architectures it can be always
  //! zero (for example AArch64 saves all registers via push/pop like instructions, so this would be zero).
  ASMJIT_INLINE_NODEBUG uint32_t extraRegSaveSize() const noexcept { return _extraRegSaveSize; }
  //! Returns an offset to the stack where extra registers are saved.
  ASMJIT_INLINE_NODEBUG uint32_t extraRegSaveOffset() const noexcept { return _extraRegSaveOffset; }

  //! Tests whether the functions contains stack adjustment.
  ASMJIT_INLINE_NODEBUG bool hasStackAdjustment() const noexcept { return _stackAdjustment != 0; }
  //! Returns function's stack adjustment used in function's prolog and epilog.
  //!
  //! If the returned value is zero it means that the stack is not adjusted. This can mean both that the stack
  //! is not used and/or the stack is only adjusted by instructions that pust/pop registers into/from stack.
  ASMJIT_INLINE_NODEBUG uint32_t stackAdjustment() const noexcept { return _stackAdjustment; }

  //! \}

  //! \name Finalization
  //! \{

  ASMJIT_API Error finalize() noexcept;

  //! \}
};

//! A helper class that can be used to assign a physical register for each function argument. Use with
//! `BaseEmitter::emitArgsAssignment()`.
class FuncArgsAssignment {
public:
  //! \name Members
  //! \{

  //! Function detail.
  const FuncDetail* _funcDetail {};
  //! Register that can be used to access arguments passed by stack.
  uint8_t _saRegId = uint8_t(BaseReg::kIdBad);
  //! Reserved for future use.
  uint8_t _reserved[3] {};
  //! Mapping of each function argument.
  FuncValuePack _argPacks[Globals::kMaxFuncArgs] {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates either a default initialized `FuncArgsAssignment` or to assignment that links to `fd`, if non-null.
  ASMJIT_INLINE_NODEBUG explicit FuncArgsAssignment(const FuncDetail* fd = nullptr) noexcept { reset(fd); }

  //! Copy constructor.
  ASMJIT_INLINE_NODEBUG FuncArgsAssignment(const FuncArgsAssignment& other) noexcept = default;

  //! Resets this `FuncArgsAssignment` to either default constructed state or to assignment that links to `fd`,
  //! if non-null.
  inline void reset(const FuncDetail* fd = nullptr) noexcept {
    _funcDetail = fd;
    _saRegId = uint8_t(BaseReg::kIdBad);
    memset(_reserved, 0, sizeof(_reserved));
    memset(_argPacks, 0, sizeof(_argPacks));
  }

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Copy assignment.
  ASMJIT_INLINE_NODEBUG FuncArgsAssignment& operator=(const FuncArgsAssignment& other) noexcept = default;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the associated \ref FuncDetail of this `FuncArgsAssignment`.
  ASMJIT_INLINE_NODEBUG const FuncDetail* funcDetail() const noexcept { return _funcDetail; }
  //! Associates \ref FuncDetails with this `FuncArgsAssignment`.
  ASMJIT_INLINE_NODEBUG void setFuncDetail(const FuncDetail* fd) noexcept { _funcDetail = fd; }

  ASMJIT_INLINE_NODEBUG bool hasSARegId() const noexcept { return _saRegId != BaseReg::kIdBad; }
  ASMJIT_INLINE_NODEBUG uint32_t saRegId() const noexcept { return _saRegId; }
  ASMJIT_INLINE_NODEBUG void setSARegId(uint32_t regId) { _saRegId = uint8_t(regId); }
  ASMJIT_INLINE_NODEBUG void resetSARegId() { _saRegId = uint8_t(BaseReg::kIdBad); }

  //! Returns assigned argument at `argIndex` and `valueIndex`.
  //!
  //! \note `argIndex` refers to he function argument and `valueIndex` refers to a value pack (in case multiple
  //! values are passed as a single argument).
  inline FuncValue& arg(size_t argIndex, size_t valueIndex) noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    return _argPacks[argIndex][valueIndex];
  }
  //! \overload
  inline const FuncValue& arg(size_t argIndex, size_t valueIndex) const noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    return _argPacks[argIndex][valueIndex];
  }

  //! Tests whether argument at `argIndex` and `valueIndex` has been assigned.
  inline bool isAssigned(size_t argIndex, size_t valueIndex) const noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    return _argPacks[argIndex][valueIndex].isAssigned();
  }

  //! Assigns register at `argIndex` and value index of 0 to `reg` and an optional `typeId`.
  inline void assignReg(size_t argIndex, const BaseReg& reg, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    ASMJIT_ASSERT(reg.isPhysReg());
    _argPacks[argIndex][0].initReg(reg.type(), reg.id(), typeId);
  }

  //! Assigns register at `argIndex` and value index of 0 to `regType`, `regId`, and an optional `typeId`.
  inline void assignReg(size_t argIndex, RegType regType, uint32_t regId, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    _argPacks[argIndex][0].initReg(regType, regId, typeId);
  }

  //! Assigns stack at `argIndex` and value index of 0 to `offset` and an optional `typeId`.
  inline void assignStack(size_t argIndex, int32_t offset, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    _argPacks[argIndex][0].initStack(offset, typeId);
  }

  //! Assigns register at `argIndex` and `valueIndex` to `reg` and an optional `typeId`.
  inline void assignRegInPack(size_t argIndex, size_t valueIndex, const BaseReg& reg, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    ASMJIT_ASSERT(reg.isPhysReg());
    _argPacks[argIndex][valueIndex].initReg(reg.type(), reg.id(), typeId);
  }

  //! Assigns register at `argIndex` and `valueIndex` to `regType`, `regId`, and an optional `typeId`.
  inline void assignRegInPack(size_t argIndex, size_t valueIndex, RegType regType, uint32_t regId, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    _argPacks[argIndex][valueIndex].initReg(regType, regId, typeId);
  }

  //! Assigns stack at `argIndex` and `valueIndex` to `offset` and an optional `typeId`.
  inline void assignStackInPack(size_t argIndex, size_t valueIndex, int32_t offset, TypeId typeId = TypeId::kVoid) noexcept {
    ASMJIT_ASSERT(argIndex < ASMJIT_ARRAY_SIZE(_argPacks));
    _argPacks[argIndex][valueIndex].initStack(offset, typeId);
  }

  // NOTE: All `assignAll()` methods are shortcuts to assign all arguments at once, however, since registers are
  // passed all at once these initializers don't provide any way to pass TypeId and/or to keep any argument between
  // the arguments passed unassigned.
  inline void _assignAllInternal(size_t argIndex, const BaseReg& reg) noexcept {
    assignReg(argIndex, reg);
  }

  template<typename... Args>
  inline void _assignAllInternal(size_t argIndex, const BaseReg& reg, Args&&... args) noexcept {
    assignReg(argIndex, reg);
    _assignAllInternal(argIndex + 1, std::forward<Args>(args)...);
  }

  //! Assigns all argument at once.
  //!
  //! \note This function can be only used if the arguments don't contain value packs (multiple values per argument).
  template<typename... Args>
  inline void assignAll(Args&&... args) noexcept {
    _assignAllInternal(0, std::forward<Args>(args)...);
  }

  //! \}

  //! \name Utilities
  //! \{

  //! Update `FuncFrame` based on function's arguments assignment.
  //!
  //! \note This function must be called in order to use `BaseEmitter::emitArgsAssignment()`, otherwise the \ref FuncFrame
  //! would not contain the information necessary to assign all arguments into the registers and/or stack specified.
  ASMJIT_API Error updateFuncFrame(FuncFrame& frame) const noexcept;

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_FUNC_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/funcargscontext.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/funcargscontext_p.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_core
//! \{

FuncArgsContext::FuncArgsContext() noexcept {
  for (RegGroup group : RegGroupVirtValues{}) {
    _workData[size_t(group)].reset();
  }
}

ASMJIT_FAVOR_SIZE Error FuncArgsContext::initWorkData(const FuncFrame& frame, const FuncArgsAssignment& args, const RAConstraints* constraints) noexcept {
  Arch arch = frame.arch();
  const FuncDetail& func = *args.funcDetail();

  _archTraits = &ArchTraits::byArch(arch);
  _constraints = constraints;
  _arch = arch;

  // Initialize `_archRegs`.
  for (RegGroup group : RegGroupVirtValues{}) {
    _workData[group]._archRegs = _constraints->availableRegs(group);
  }

  if (frame.hasPreservedFP()) {
    _workData[size_t(RegGroup::kGp)]._archRegs &= ~Support::bitMask(archTraits().fpRegId());
  }

  uint32_t reassignmentFlagMask = 0;

  // Extract information from all function arguments/assignments and build Var[] array.
  uint32_t varId = 0;
  for (uint32_t argIndex = 0; argIndex < Globals::kMaxFuncArgs; argIndex++) {
    for (uint32_t valueIndex = 0; valueIndex < Globals::kMaxValuePack; valueIndex++) {
      const FuncValue& dst_ = args.arg(argIndex, valueIndex);
      if (!dst_.isAssigned()) {
        continue;
      }

      const FuncValue& src_ = func.arg(argIndex, valueIndex);
      if (ASMJIT_UNLIKELY(!src_.isAssigned())) {
        return DebugUtils::errored(kErrorInvalidState);
      }

      Var& var = _vars[varId];
      var.init(src_, dst_);

      FuncValue& src = var.cur;
      FuncValue& dst = var.out;

      RegGroup dstGroup = RegGroup::kMaxValue;
      uint32_t dstId = BaseReg::kIdBad;
      WorkData* dstWd = nullptr;

      // Not supported.
      if (src.isIndirect()) {
        return DebugUtils::errored(kErrorInvalidAssignment);
      }

      if (dst.isReg()) {
        RegType dstType = dst.regType();
        if (ASMJIT_UNLIKELY(!archTraits().hasRegType(dstType))) {
          return DebugUtils::errored(kErrorInvalidRegType);
        }

        // Copy TypeId from source if the destination doesn't have it. The RA used by BaseCompiler would never
        // leave TypeId undefined, but users of FuncAPI can just assign phys regs without specifying their types.
        if (!dst.hasTypeId()) {
          dst.setTypeId(archTraits().regTypeToTypeId(dst.regType()));
        }

        dstGroup = archTraits().regTypeToGroup(dstType);
        if (ASMJIT_UNLIKELY(dstGroup > RegGroup::kMaxVirt)) {
          return DebugUtils::errored(kErrorInvalidRegGroup);
        }

        dstWd = &_workData[dstGroup];
        dstId = dst.regId();

        if (ASMJIT_UNLIKELY(dstId >= 32 || !Support::bitTest(dstWd->archRegs(), dstId))) {
          return DebugUtils::errored(kErrorInvalidPhysId);
        }

        if (ASMJIT_UNLIKELY(Support::bitTest(dstWd->dstRegs(), dstId))) {
          return DebugUtils::errored(kErrorOverlappedRegs);
        }

        dstWd->_dstRegs  |= Support::bitMask(dstId);
        dstWd->_dstShuf  |= Support::bitMask(dstId);
        dstWd->_usedRegs |= Support::bitMask(dstId);
      }
      else {
        if (!dst.hasTypeId()) {
          dst.setTypeId(src.typeId());
        }

        OperandSignature signature = getSuitableRegForMemToMemMove(arch, dst.typeId(), src.typeId());
        if (ASMJIT_UNLIKELY(!signature.isValid())) {
          return DebugUtils::errored(kErrorInvalidState);
        }
        _stackDstMask = uint8_t(_stackDstMask | Support::bitMask(signature.regGroup()));
      }

      if (src.isReg()) {
        uint32_t srcId = src.regId();
        RegGroup srcGroup = archTraits().regTypeToGroup(src.regType());

        if (dstGroup == srcGroup) {
          ASMJIT_ASSERT(dstWd != nullptr);
          dstWd->assign(varId, srcId);

          reassignmentFlagMask |= uint32_t(dstId != srcId) << uint32_t(dstGroup);

          if (dstId == srcId) {
            // The best case, register is allocated where it is expected to be. However, we should
            // not mark this as done if both registers are GP and sign or zero extension is required.
            if (dstGroup != RegGroup::kGp) {
              var.markDone();
            }
            else {
              TypeId dt = dst.typeId();
              TypeId st = src.typeId();

              uint32_t dstSize = TypeUtils::sizeOf(dt);
              uint32_t srcSize = TypeUtils::sizeOf(st);

              if (dt == TypeId::kVoid || st == TypeId::kVoid || dstSize <= srcSize) {
                var.markDone();
              }
            }
          }
        }
        else {
          if (ASMJIT_UNLIKELY(srcGroup > RegGroup::kMaxVirt)) {
            return DebugUtils::errored(kErrorInvalidState);
          }

          WorkData& srcData = _workData[size_t(srcGroup)];
          srcData.assign(varId, srcId);
          reassignmentFlagMask |= 1u << uint32_t(dstGroup);
        }
      }
      else {
        if (dstWd)
          dstWd->_numStackArgs++;
        _hasStackSrc = true;
      }

      varId++;
    }
  }

  // Initialize WorkData::workRegs.
  for (RegGroup group : RegGroupVirtValues{}) {
    _workData[group]._workRegs =
      (_workData[group].archRegs() & (frame.dirtyRegs(group) | ~frame.preservedRegs(group))) | _workData[group].dstRegs() | _workData[group].assignedRegs();
    _workData[group]._needsScratch = (reassignmentFlagMask >> uint32_t(group)) & 1u;
  }

  // Create a variable that represents `SARegId` if necessary.
  bool saRegRequired = _hasStackSrc && frame.hasDynamicAlignment() && !frame.hasPreservedFP();

  WorkData& gpRegs = _workData[RegGroup::kGp];
  uint32_t saCurRegId = frame.saRegId();
  uint32_t saOutRegId = args.saRegId();

  if (saCurRegId != BaseReg::kIdBad) {
    // Check if the provided `SARegId` doesn't collide with input registers.
    if (ASMJIT_UNLIKELY(gpRegs.isAssigned(saCurRegId))) {
      return DebugUtils::errored(kErrorOverlappedRegs);
    }
  }

  if (saOutRegId != BaseReg::kIdBad) {
    // Check if the provided `SARegId` doesn't collide with argument assignments.
    if (ASMJIT_UNLIKELY(Support::bitTest(gpRegs.dstRegs(), saOutRegId))) {
      return DebugUtils::errored(kErrorOverlappedRegs);
    }
    saRegRequired = true;
  }

  if (saRegRequired) {
    TypeId ptrTypeId = Environment::is32Bit(arch) ? TypeId::kUInt32 : TypeId::kUInt64;
    RegType ptrRegType = Environment::is32Bit(arch) ? RegType::kGp32 : RegType::kGp64;

    _saVarId = uint8_t(varId);
    _hasPreservedFP = frame.hasPreservedFP();

    Var& var = _vars[varId];
    var.reset();

    if (saCurRegId == BaseReg::kIdBad) {
      if (saOutRegId != BaseReg::kIdBad && !gpRegs.isAssigned(saOutRegId)) {
        saCurRegId = saOutRegId;
      }
      else {
        RegMask availableRegs = gpRegs.availableRegs();
        if (!availableRegs) {
          availableRegs = gpRegs.archRegs() & ~gpRegs.workRegs();
        }

        if (ASMJIT_UNLIKELY(!availableRegs)) {
          return DebugUtils::errored(kErrorNoMorePhysRegs);
        }

        saCurRegId = Support::ctz(availableRegs);
      }
    }

    var.cur.initReg(ptrRegType, saCurRegId, ptrTypeId);
    gpRegs.assign(varId, saCurRegId);
    gpRegs._workRegs |= Support::bitMask(saCurRegId);

    if (saOutRegId != BaseReg::kIdBad) {
      var.out.initReg(ptrRegType, saOutRegId, ptrTypeId);
      gpRegs._dstRegs  |= Support::bitMask(saOutRegId);
      gpRegs._workRegs |= Support::bitMask(saOutRegId);
    }
    else {
      var.markDone();
    }

    varId++;
  }

  _varCount = varId;

  // Detect register swaps.
  for (varId = 0; varId < _varCount; varId++) {
    Var& var = _vars[varId];
    if (var.cur.isReg() && var.out.isReg()) {
      uint32_t srcId = var.cur.regId();
      uint32_t dstId = var.out.regId();

      RegGroup group = archTraits().regTypeToGroup(var.cur.regType());
      if (group != archTraits().regTypeToGroup(var.out.regType())) {
        continue;
      }

      WorkData& wd = _workData[group];
      if (wd.isAssigned(dstId)) {
        Var& other = _vars[wd._physToVarId[dstId]];
        if (archTraits().regTypeToGroup(other.out.regType()) == group && other.out.regId() == srcId) {
          wd._numSwaps++;
          _regSwapsMask = uint8_t(_regSwapsMask | Support::bitMask(group));
        }
      }
    }
  }

  return kErrorOk;
}

ASMJIT_FAVOR_SIZE Error FuncArgsContext::markDstRegsDirty(FuncFrame& frame) noexcept {
  for (RegGroup group : RegGroupVirtValues{}) {
    WorkData& wd = _workData[group];
    uint32_t regs = wd.usedRegs() | wd._dstShuf;

    wd._workRegs |= regs;
    frame.addDirtyRegs(group, regs);
  }

  return kErrorOk;
}

ASMJIT_FAVOR_SIZE Error FuncArgsContext::markScratchRegs(FuncFrame& frame) noexcept {
  uint32_t groupMask = 0;

  // Handle stack to stack moves.
  groupMask |= _stackDstMask;

  // Handle register swaps.
  groupMask |= _regSwapsMask & ~Support::bitMask(RegGroup::kGp);

  if (!groupMask)
    return kErrorOk;

  // Selects one dirty register per affected group that can be used as a scratch register.
  for (RegGroup group : RegGroupVirtValues{}) {
    if (Support::bitTest(groupMask, group)) {
      WorkData& wd = _workData[group];
      if (wd._needsScratch) {
        // Initially, pick some clobbered or dirty register.
        RegMask workRegs = wd.workRegs();
        RegMask regs = workRegs & ~(wd.usedRegs() | wd._dstShuf);

        // If that didn't work out pick some register which is not in 'used'.
        if (!regs) {
          regs = workRegs & ~wd.usedRegs();
        }

        // If that didn't work out pick any other register that is allocable.
        // This last resort case will, however, result in marking one more
        // register dirty.
        if (!regs) {
          regs = wd.archRegs() & ~workRegs;
        }

        // If that didn't work out we will have to use XORs instead of MOVs.
        if (!regs) {
          continue;
        }

        RegMask regMask = Support::blsi(regs);
        wd._workRegs |= regMask;
        frame.addDirtyRegs(group, regMask);
      }
    }
  }

  return kErrorOk;
}

ASMJIT_FAVOR_SIZE Error FuncArgsContext::markStackArgsReg(FuncFrame& frame) noexcept {
  if (_saVarId != kVarIdNone) {
    const Var& var = _vars[_saVarId];
    frame.setSARegId(var.cur.regId());
  }
  else if (frame.hasPreservedFP()) {
    frame.setSARegId(archTraits().fpRegId());
  }

  return kErrorOk;
}

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/funcargscontext_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_FUNCARGSCONTEXT_P_H_INCLUDED
#define ASMJIT_CORE_FUNCARGSCONTEXT_P_H_INCLUDED

#include "../core/archtraits.h"
#include "../core/environment.h"
#include "../core/func.h"
#include "../core/operand.h"
#include "../core/radefs_p.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_core
//! \{

static inline OperandSignature getSuitableRegForMemToMemMove(Arch arch, TypeId dstTypeId, TypeId srcTypeId) noexcept {
  const ArchTraits& archTraits = ArchTraits::byArch(arch);

  uint32_t dstSize = TypeUtils::sizeOf(dstTypeId);
  uint32_t srcSize = TypeUtils::sizeOf(srcTypeId);
  uint32_t maxSize = Support::max<uint32_t>(dstSize, srcSize);
  uint32_t regSize = Environment::registerSizeFromArch(arch);

  OperandSignature signature{0};
  if (maxSize <= regSize || (TypeUtils::isInt(dstTypeId) && TypeUtils::isInt(srcTypeId)))
    signature = maxSize <= 4 ? archTraits.regTypeToSignature(RegType::kGp32)
                             : archTraits.regTypeToSignature(RegType::kGp64);
  else if (maxSize <= 8 && archTraits.hasRegType(RegType::kVec64))
    signature = archTraits.regTypeToSignature(RegType::kVec64);
  else if (maxSize <= 16 && archTraits.hasRegType(RegType::kVec128))
    signature = archTraits.regTypeToSignature(RegType::kVec128);
  else if (maxSize <= 32 && archTraits.hasRegType(RegType::kVec256))
    signature = archTraits.regTypeToSignature(RegType::kVec256);
  else if (maxSize <= 64 && archTraits.hasRegType(RegType::kVec512))
    signature = archTraits.regTypeToSignature(RegType::kVec512);

  return signature;
}

class FuncArgsContext {
public:
  enum VarId : uint32_t {
    kVarIdNone = 0xFF
  };

  //! Contains information about a single argument or SA register that may need shuffling.
  struct Var {
    FuncValue cur;
    FuncValue out;

    inline void init(const FuncValue& cur_, const FuncValue& out_) noexcept {
      cur = cur_;
      out = out_;
    }

    //! Reset the value to its unassigned state.
    inline void reset() noexcept {
      cur.reset();
      out.reset();
    }

    ASMJIT_INLINE_NODEBUG bool isDone() const noexcept { return cur.isDone(); }
    ASMJIT_INLINE_NODEBUG void markDone() noexcept { cur.addFlags(FuncValue::kFlagIsDone); }
  };

  struct WorkData {
    //! All allocable registers provided by the architecture.
    RegMask _archRegs;
    //! All registers that can be used by the shuffler.
    RegMask _workRegs;
    //! Registers used by the shuffler (all).
    RegMask _usedRegs;
    //! Assigned registers.
    RegMask _assignedRegs;
    //! Destination registers assigned to arguments or SA.
    RegMask _dstRegs;
    //! Destination registers that require shuffling.
    RegMask _dstShuf;
    //! Number of register swaps.
    uint8_t _numSwaps;
    //! Number of stack loads.
    uint8_t _numStackArgs;
    //! Whether this work data would need reassignment.
    uint8_t _needsScratch;
    //! Reserved (only used as padding).
    uint8_t _reserved[5];
    //! Physical ID to variable ID mapping.
    uint8_t _physToVarId[32];

    inline void reset() noexcept {
      _archRegs = 0;
      _workRegs = 0;
      _usedRegs = 0;
      _assignedRegs = 0;
      _dstRegs = 0;
      _dstShuf = 0;
      _numSwaps = 0;
      _numStackArgs = 0;
      _needsScratch = 0;
      memset(_reserved, 0, sizeof(_reserved));
      memset(_physToVarId, kVarIdNone, 32);
    }

    inline bool isAssigned(uint32_t regId) const noexcept {
      ASMJIT_ASSERT(regId < 32);
      return Support::bitTest(_assignedRegs, regId);
    }

    inline void assign(uint32_t varId, uint32_t regId) noexcept {
      ASMJIT_ASSERT(!isAssigned(regId));
      ASMJIT_ASSERT(_physToVarId[regId] == kVarIdNone);

      _physToVarId[regId] = uint8_t(varId);
      _assignedRegs ^= Support::bitMask(regId);
    }

    inline void reassign(uint32_t varId, uint32_t newId, uint32_t oldId) noexcept {
      ASMJIT_ASSERT( isAssigned(oldId));
      ASMJIT_ASSERT(!isAssigned(newId));
      ASMJIT_ASSERT(_physToVarId[oldId] == varId);
      ASMJIT_ASSERT(_physToVarId[newId] == kVarIdNone);

      _physToVarId[oldId] = uint8_t(kVarIdNone);
      _physToVarId[newId] = uint8_t(varId);
      _assignedRegs ^= Support::bitMask(newId) ^ Support::bitMask(oldId);
    }

    inline void swap(uint32_t aVarId, uint32_t aRegId, uint32_t bVarId, uint32_t bRegId) noexcept {
      ASMJIT_ASSERT(isAssigned(aRegId));
      ASMJIT_ASSERT(isAssigned(bRegId));
      ASMJIT_ASSERT(_physToVarId[aRegId] == aVarId);
      ASMJIT_ASSERT(_physToVarId[bRegId] == bVarId);

      _physToVarId[aRegId] = uint8_t(bVarId);
      _physToVarId[bRegId] = uint8_t(aVarId);
    }

    inline void unassign(uint32_t varId, uint32_t regId) noexcept {
      ASMJIT_ASSERT(isAssigned(regId));
      ASMJIT_ASSERT(_physToVarId[regId] == varId);

      DebugUtils::unused(varId);
      _physToVarId[regId] = uint8_t(kVarIdNone);
      _assignedRegs ^= Support::bitMask(regId);
    }

    ASMJIT_INLINE_NODEBUG RegMask archRegs() const noexcept { return _archRegs; }
    ASMJIT_INLINE_NODEBUG RegMask workRegs() const noexcept { return _workRegs; }
    ASMJIT_INLINE_NODEBUG RegMask usedRegs() const noexcept { return _usedRegs; }
    ASMJIT_INLINE_NODEBUG RegMask assignedRegs() const noexcept { return _assignedRegs; }
    ASMJIT_INLINE_NODEBUG RegMask dstRegs() const noexcept { return _dstRegs; }
    ASMJIT_INLINE_NODEBUG RegMask availableRegs() const noexcept { return _workRegs & ~_assignedRegs; }
  };

  //! Architecture traits.
  const ArchTraits* _archTraits = nullptr;
  //! Architecture constraints.
  const RAConstraints* _constraints = nullptr;
  //! Target architecture.
  Arch _arch = Arch::kUnknown;
  //! Has arguments passed via stack (SRC).
  bool _hasStackSrc = false;
  //! Has preserved frame-pointer (FP).
  bool _hasPreservedFP = false;
  //! Has arguments assigned to stack (DST).
  uint8_t _stackDstMask = 0;
  //! Register swap groups (bit-mask).
  uint8_t _regSwapsMask = 0;
  uint8_t _saVarId = kVarIdNone;
  uint32_t _varCount = 0;
  Support::Array<WorkData, Globals::kNumVirtGroups> _workData;
  Var _vars[Globals::kMaxFuncArgs * Globals::kMaxValuePack + 1];

  FuncArgsContext() noexcept;

  ASMJIT_INLINE_NODEBUG const ArchTraits& archTraits() const noexcept { return *_archTraits; }
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return _arch; }

  ASMJIT_INLINE_NODEBUG uint32_t varCount() const noexcept { return _varCount; }
  ASMJIT_INLINE_NODEBUG size_t indexOf(const Var* var) const noexcept { return (size_t)(var - _vars); }

  ASMJIT_INLINE_NODEBUG Var& var(size_t varId) noexcept { return _vars[varId]; }
  ASMJIT_INLINE_NODEBUG const Var& var(size_t varId) const noexcept { return _vars[varId]; }

  Error initWorkData(const FuncFrame& frame, const FuncArgsAssignment& args, const RAConstraints* constraints) noexcept;
  Error markScratchRegs(FuncFrame& frame) noexcept;
  Error markDstRegsDirty(FuncFrame& frame) noexcept;
  Error markStackArgsReg(FuncFrame& frame) noexcept;
};

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_FUNCARGSCONTEXT_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/globals.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/globals.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// DebugUtils - Error As String
// ============================

ASMJIT_FAVOR_SIZE const char* DebugUtils::errorAsString(Error err) noexcept {
#ifndef ASMJIT_NO_TEXT
  // @EnumStringBegin{"enum": "ErrorCode", "output": "sError", "strip": "kError"}@
  static const char sErrorString[] =
    "Ok\0"
    "OutOfMemory\0"
    "InvalidArgument\0"
    "InvalidState\0"
    "InvalidArch\0"
    "NotInitialized\0"
    "AlreadyInitialized\0"
    "FeatureNotEnabled\0"
    "TooManyHandles\0"
    "TooLarge\0"
    "NoCodeGenerated\0"
    "InvalidDirective\0"
    "InvalidLabel\0"
    "TooManyLabels\0"
    "LabelAlreadyBound\0"
    "LabelAlreadyDefined\0"
    "LabelNameTooLong\0"
    "InvalidLabelName\0"
    "InvalidParentLabel\0"
    "InvalidSection\0"
    "TooManySections\0"
    "InvalidSectionName\0"
    "TooManyRelocations\0"
    "InvalidRelocEntry\0"
    "RelocOffsetOutOfRange\0"
    "InvalidAssignment\0"
    "InvalidInstruction\0"
    "InvalidRegType\0"
    "InvalidRegGroup\0"
    "InvalidPhysId\0"
    "InvalidVirtId\0"
    "InvalidElementIndex\0"
    "InvalidPrefixCombination\0"
    "InvalidLockPrefix\0"
    "InvalidXAcquirePrefix\0"
    "InvalidXReleasePrefix\0"
    "InvalidRepPrefix\0"
    "InvalidRexPrefix\0"
    "InvalidExtraReg\0"
    "InvalidKMaskUse\0"
    "InvalidKZeroUse\0"
    "InvalidBroadcast\0"
    "InvalidEROrSAE\0"
    "InvalidAddress\0"
    "InvalidAddressIndex\0"
    "InvalidAddressScale\0"
    "InvalidAddress64Bit\0"
    "InvalidAddress64BitZeroExtension\0"
    "InvalidDisplacement\0"
    "InvalidSegment\0"
    "InvalidImmediate\0"
    "InvalidOperandSize\0"
    "AmbiguousOperandSize\0"
    "OperandSizeMismatch\0"
    "InvalidOption\0"
    "OptionAlreadyDefined\0"
    "InvalidTypeId\0"
    "InvalidUseOfGpbHi\0"
    "InvalidUseOfGpq\0"
    "InvalidUseOfF80\0"
    "NotConsecutiveRegs\0"
    "ConsecutiveRegsAllocation\0"
    "IllegalVirtReg\0"
    "TooManyVirtRegs\0"
    "NoMorePhysRegs\0"
    "OverlappedRegs\0"
    "OverlappingStackRegWithRegArg\0"
    "ExpressionLabelNotBound\0"
    "ExpressionOverflow\0"
    "FailedToOpenAnonymousMemory\0"
    "FailedToOpenFile\0"
    "ProtectionFailure\0"
    "<Unknown>\0";

  static const uint16_t sErrorIndex[] = {
    0, 3, 15, 31, 44, 56, 71, 90, 108, 123, 132, 148, 165, 178, 192, 210, 230,
    247, 264, 283, 298, 314, 333, 352, 370, 392, 410, 429, 444, 460, 474, 488,
    508, 533, 551, 573, 595, 612, 629, 645, 661, 677, 694, 709, 724, 744, 764,
    784, 817, 837, 852, 869, 888, 909, 929, 943, 964, 978, 996, 1012, 1028, 1047,
    1073, 1088, 1104, 1119, 1134, 1164, 1188, 1207, 1235, 1252, 1270
  };
  // @EnumStringEnd@

  return sErrorString + sErrorIndex[Support::min<Error>(err, kErrorCount)];
#else
  DebugUtils::unused(err);
  static const char noMessage[] = "";
  return noMessage;
#endif
}

// DebugUtils - Debug Output
// =========================

ASMJIT_FAVOR_SIZE void DebugUtils::debugOutput(const char* str) noexcept {
#if defined(_WIN32)
  ::OutputDebugStringA(str);
#else
  ::fputs(str, stderr);
#endif
}

// DebugUtils - Fatal Errors
// =========================

ASMJIT_FAVOR_SIZE void DebugUtils::assertionFailed(const char* file, int line, const char* msg) noexcept {
  char str[1024];

  snprintf(str, 1024,
    "[asmjit] Assertion failed at %s (line %d):\n"
    "[asmjit] %s\n", file, line, msg);

  debugOutput(str);
  ::abort();
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/globals.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_GLOBALS_H_INCLUDED
#define ASMJIT_CORE_GLOBALS_H_INCLUDED

#include "../core/api-config.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_utilities
//! \{
namespace Support {

//! Cast designed to cast between function and void* pointers.
template<typename Dst, typename Src>
static inline Dst ptr_cast_impl(Src p) noexcept { return (Dst)p; }

//! Helper to implement placement new/delete without relying on `<new>` header.
struct PlacementNew { void* ptr; };

} // {Support}

#if defined(ASMJIT_NO_STDCXX)
namespace Support {
  ASMJIT_FORCE_INLINE void* operatorNew(size_t n) noexcept { return malloc(n); }
  ASMJIT_FORCE_INLINE void operatorDelete(void* p) noexcept { if (p) free(p); }
} // {Support}

#define ASMJIT_BASE_CLASS(TYPE)                                                                          \
  ASMJIT_FORCE_INLINE void* operator new(size_t n) noexcept { return Support::operatorNew(n); }          \
  ASMJIT_FORCE_INLINE void operator delete(void* ptr) noexcept { Support::operatorDelete(ptr); }         \
                                                                                                         \
  ASMJIT_FORCE_INLINE void* operator new(size_t, void* ptr) noexcept { return ptr; }                     \
  ASMJIT_FORCE_INLINE void operator delete(void*, void*) noexcept {}                                     \
                                                                                                         \
  ASMJIT_FORCE_INLINE void* operator new(size_t, Support::PlacementNew ptr) noexcept { return ptr.ptr; } \
  ASMJIT_FORCE_INLINE void operator delete(void*, Support::PlacementNew) noexcept {}
#else
#define ASMJIT_BASE_CLASS(TYPE)
#endif

//! \}
//! \endcond

//! \addtogroup asmjit_core
//! \{

//! Byte order.
enum class ByteOrder {
  //! Little endian.
  kLE = 0,
  //! Big endian.
  kBE = 1,
  //! Native byte order of the target architecture.
  kNative = ASMJIT_ARCH_LE ? kLE : kBE,
  //! Swapped byte order of the target architecture.
  kSwapped = ASMJIT_ARCH_LE ? kBE : kLE
};

//! A policy that can be used with some `reset()` member functions.
enum class ResetPolicy : uint32_t {
  //! Soft reset, doesn't deallocate memory (default).
  kSoft = 0,
  //! Hard reset, releases all memory used, if any.
  kHard = 1
};

//! Contains typedefs, constants, and variables used globally by AsmJit.
namespace Globals {

//! Host memory allocator overhead.
static constexpr uint32_t kAllocOverhead = uint32_t(sizeof(intptr_t) * 4);

//! Host memory allocator alignment.
static constexpr uint32_t kAllocAlignment = 8;

//! Aggressive growing strategy threshold.
static constexpr uint32_t kGrowThreshold = 1024 * 1024 * 16;

//! Maximum depth of RB-Tree is:
//!
//!   `2 * log2(n + 1)`
//!
//! Size of RB node is at least two pointers (without data), so a theoretical architecture limit would be:
//!
//!   `2 * log2(addressableMemorySize / sizeof(Node) + 1)`
//!
//! Which yields 30 on 32-bit arch and 61 on 64-bit arch. The final value was adjusted by +1 for safety reasons.
static constexpr uint32_t kMaxTreeHeight = (ASMJIT_ARCH_BITS == 32 ? 30 : 61) + 1;

//! Maximum number of operands per a single instruction.
static constexpr uint32_t kMaxOpCount = 6;

//! Maximum arguments of a function supported by the Compiler / Function API.
static constexpr uint32_t kMaxFuncArgs = 32;

//! The number of values that can be assigned to a single function argument or return value.
static constexpr uint32_t kMaxValuePack = 4;

//! Maximum number of physical registers AsmJit can use per register group.
static constexpr uint32_t kMaxPhysRegs = 32;

//! Maximum alignment.
static constexpr uint32_t kMaxAlignment = 64;

//! Maximum label or symbol size in bytes.
static constexpr uint32_t kMaxLabelNameSize = 2048;

//! Maximum section name size.
static constexpr uint32_t kMaxSectionNameSize = 35;

//! Maximum size of comment.
static constexpr uint32_t kMaxCommentSize = 1024;

//! Invalid identifier.
static constexpr uint32_t kInvalidId = 0xFFFFFFFFu;

//! Returned by `indexOf()` and similar when working with containers that use 32-bit index/size.
static constexpr uint32_t kNotFound = 0xFFFFFFFFu;

//! Invalid base address.
static constexpr uint64_t kNoBaseAddress = ~uint64_t(0);

//! Number of virtual register groups.
static constexpr uint32_t kNumVirtGroups = 4;

struct Init_ {};
struct NoInit_ {};

//! A decorator used to initialize.
static const constexpr Init_ Init {};
//! A decorator used to not initialize.
static const constexpr NoInit_ NoInit {};

} // {Globals}

//! Casts a `void*` pointer `func` to a function pointer `Func`.
template<typename Func>
static ASMJIT_INLINE_NODEBUG Func ptr_as_func(void* func) noexcept { return Support::ptr_cast_impl<Func, void*>(func); }

//! Casts a function pointer `func` to a void pointer `void*`.
template<typename Func>
static ASMJIT_INLINE_NODEBUG void* func_as_ptr(Func func) noexcept { return Support::ptr_cast_impl<void*, Func>(func); }

//! \}

//! \addtogroup asmjit_error_handling
//! \{

//! AsmJit error type (uint32_t).
typedef uint32_t Error;

//! AsmJit error codes.
enum ErrorCode : uint32_t {
  // @EnumValuesBegin{"enum": "ErrorCode"}@

  //! No error (success).
  kErrorOk = 0,

  //! Out of memory.
  kErrorOutOfMemory,

  //! Invalid argument.
  kErrorInvalidArgument,

  //! Invalid state.
  //!
  //! If this error is returned it means that either you are doing something wrong or AsmJit caught itself by
  //! doing something wrong. This error should never be ignored.
  kErrorInvalidState,

  //! Invalid or incompatible architecture.
  kErrorInvalidArch,

  //! The object is not initialized.
  kErrorNotInitialized,
  //! The object is already initialized.
  kErrorAlreadyInitialized,

  //! Either a built-in feature was disabled at compile time and it's not available or the feature is not
  //! available on the target platform.
  //!
  //! For example trying to allocate large pages on unsupported platform would return this error.
  kErrorFeatureNotEnabled,

  //! Too many handles (Windows) or file descriptors (Unix/Posix).
  kErrorTooManyHandles,
  //! Code generated is larger than allowed.
  kErrorTooLarge,

  //! No code generated.
  //!
  //! Returned by runtime if the \ref CodeHolder contains no code.
  kErrorNoCodeGenerated,

  //! Invalid directive.
  kErrorInvalidDirective,
  //! Attempt to use uninitialized label.
  kErrorInvalidLabel,
  //! Label index overflow - a single \ref BaseAssembler instance can hold almost 2^32 (4 billion) labels. If
  //! there is an attempt to create more labels then this error is returned.
  kErrorTooManyLabels,
  //! Label is already bound.
  kErrorLabelAlreadyBound,
  //! Label is already defined (named labels).
  kErrorLabelAlreadyDefined,
  //! Label name is too long.
  kErrorLabelNameTooLong,
  //! Label must always be local if it's anonymous (without a name).
  kErrorInvalidLabelName,
  //! Parent id passed to \ref CodeHolder::newNamedLabelEntry() was either invalid or parent is not supported
  //! by the requested `LabelType`.
  kErrorInvalidParentLabel,

  //! Invalid section.
  kErrorInvalidSection,
  //! Too many sections (section index overflow).
  kErrorTooManySections,
  //! Invalid section name (most probably too long).
  kErrorInvalidSectionName,

  //! Relocation index overflow (too many relocations).
  kErrorTooManyRelocations,
  //! Invalid relocation entry.
  kErrorInvalidRelocEntry,
  //! Reloc entry contains address that is out of range (unencodable).
  kErrorRelocOffsetOutOfRange,

  //! Invalid assignment to a register, function argument, or function return value.
  kErrorInvalidAssignment,
  //! Invalid instruction.
  kErrorInvalidInstruction,
  //! Invalid register type.
  kErrorInvalidRegType,
  //! Invalid register group.
  kErrorInvalidRegGroup,
  //! Invalid physical register id.
  kErrorInvalidPhysId,
  //! Invalid virtual register id.
  kErrorInvalidVirtId,
  //! Invalid element index (ARM).
  kErrorInvalidElementIndex,
  //! Invalid prefix combination (X86|X64).
  kErrorInvalidPrefixCombination,
  //! Invalid LOCK prefix (X86|X64).
  kErrorInvalidLockPrefix,
  //! Invalid XACQUIRE prefix (X86|X64).
  kErrorInvalidXAcquirePrefix,
  //! Invalid XRELEASE prefix (X86|X64).
  kErrorInvalidXReleasePrefix,
  //! Invalid REP prefix (X86|X64).
  kErrorInvalidRepPrefix,
  //! Invalid REX prefix (X86|X64).
  kErrorInvalidRexPrefix,
  //! Invalid {...} register (X86|X64).
  kErrorInvalidExtraReg,
  //! Invalid {k} use (not supported by the instruction) (X86|X64).
  kErrorInvalidKMaskUse,
  //! Invalid {k}{z} use (not supported by the instruction) (X86|X64).
  kErrorInvalidKZeroUse,
  //! Invalid broadcast - Currently only related to invalid use of AVX-512 {1tox} (X86|X64).
  kErrorInvalidBroadcast,
  //! Invalid 'embedded-rounding' {er} or 'suppress-all-exceptions' {sae} (AVX-512) (X86|X64).
  kErrorInvalidEROrSAE,
  //! Invalid address used (not encodable).
  kErrorInvalidAddress,
  //! Invalid index register used in memory address (not encodable).
  kErrorInvalidAddressIndex,
  //! Invalid address scale (not encodable).
  kErrorInvalidAddressScale,
  //! Invalid use of 64-bit address.
  kErrorInvalidAddress64Bit,
  //! Invalid use of 64-bit address that require 32-bit zero-extension (X64).
  kErrorInvalidAddress64BitZeroExtension,
  //! Invalid displacement (not encodable).
  kErrorInvalidDisplacement,
  //! Invalid segment (X86).
  kErrorInvalidSegment,

  //! Invalid immediate (out of bounds on X86 and invalid pattern on ARM).
  kErrorInvalidImmediate,

  //! Invalid operand size.
  kErrorInvalidOperandSize,
  //! Ambiguous operand size (memory has zero size while it's required to determine the operation type.
  kErrorAmbiguousOperandSize,
  //! Mismatching operand size (size of multiple operands doesn't match the operation size).
  kErrorOperandSizeMismatch,

  //! Invalid option.
  kErrorInvalidOption,
  //! Option already defined.
  kErrorOptionAlreadyDefined,

  //! Invalid TypeId.
  kErrorInvalidTypeId,
  //! Invalid use of a 8-bit GPB-HIGH register.
  kErrorInvalidUseOfGpbHi,
  //! Invalid use of a 64-bit GPQ register in 32-bit mode.
  kErrorInvalidUseOfGpq,
  //! Invalid use of an 80-bit float (\ref TypeId::kFloat80).
  kErrorInvalidUseOfF80,
  //! Instruction requires the use of consecutive registers, but registers in operands weren't (AVX512, ASIMD load/store, etc...).
  kErrorNotConsecutiveRegs,
  //! Failed to allocate consecutive registers - allocable registers either too restricted or a bug in RW info.
  kErrorConsecutiveRegsAllocation,

  //! Illegal virtual register - reported by instruction validation.
  kErrorIllegalVirtReg,
  //! AsmJit cannot create more virtual registers.
  kErrorTooManyVirtRegs,

  //! AsmJit requires a physical register, but no one is available.
  kErrorNoMorePhysRegs,
  //! A variable has been assigned more than once to a function argument (BaseCompiler).
  kErrorOverlappedRegs,
  //! Invalid register to hold stack arguments offset.
  kErrorOverlappingStackRegWithRegArg,

  //! Unbound label cannot be evaluated by expression.
  kErrorExpressionLabelNotBound,
  //! Arithmetic overflow during expression evaluation.
  kErrorExpressionOverflow,

  //! Failed to open anonymous memory handle or file descriptor.
  kErrorFailedToOpenAnonymousMemory,

  //! Failed to open a file.
  //!
  //! \note This is a generic error that is used by internal filesystem API.
  kErrorFailedToOpenFile,

  //! Protection failure can be returned from a virtual memory allocator or when trying to change memory access
  //! permissions.
  kErrorProtectionFailure,

  // @EnumValuesEnd@

  //! Count of AsmJit error codes.
  kErrorCount
};

//! Debugging utilities.
namespace DebugUtils {

//! \cond INTERNAL
//! Used to silence warnings about unused arguments or variables.
template<typename... Args>
static ASMJIT_INLINE_NODEBUG void unused(Args&&...) noexcept {}
//! \endcond

//! Returns the error `err` passed.
//!
//! Provided for debugging purposes. Putting a breakpoint inside `errored` can help with tracing the origin of any
//! error reported / returned by AsmJit.
static constexpr Error errored(Error err) noexcept { return err; }

//! Returns a printable version of `asmjit::Error` code.
ASMJIT_API const char* errorAsString(Error err) noexcept;

//! Called to output debugging message(s).
ASMJIT_API void debugOutput(const char* str) noexcept;

//! Called on assertion failure.
//!
//! \param file Source file name where it happened.
//! \param line Line in the source file.
//! \param msg Message to display.
//!
//! If you have problems with assertion failures a breakpoint can be put at \ref assertionFailed() function
//! (asmjit/core/globals.cpp). A call stack will be available when such assertion failure is triggered. AsmJit
//! always returns errors on failures, assertions are a last resort and usually mean unrecoverable state due to out
//! of range array access or totally invalid arguments like nullptr where a valid pointer should be provided, etc...
ASMJIT_API void ASMJIT_NORETURN assertionFailed(const char* file, int line, const char* msg) noexcept;

} // {DebugUtils}

//! \def ASMJIT_ASSERT(...)
//!
//! AsmJit's own assert macro used in AsmJit code-base.
#if defined(ASMJIT_BUILD_DEBUG)
#define ASMJIT_ASSERT(...)                                                     \
  do {                                                                         \
    if (ASMJIT_LIKELY(__VA_ARGS__))                                            \
      break;                                                                   \
    ::asmjit::DebugUtils::assertionFailed(__FILE__, __LINE__, #__VA_ARGS__);   \
  } while (0)
#else
#define ASMJIT_ASSERT(...) ((void)0)
#endif

//! \def ASMJIT_PROPAGATE(...)
//!
//! Propagates a possible `Error` produced by `...` to the caller by returning the error immediately. Used by AsmJit
//! internally, but kept public for users that want to use the same technique to propagate errors to the caller.
#define ASMJIT_PROPAGATE(...)               \
  do {                                      \
    ::asmjit::Error _err = __VA_ARGS__;     \
    if (ASMJIT_UNLIKELY(_err))              \
      return _err;                          \
  } while (0)

//! \}

ASMJIT_END_NAMESPACE

//! Implementation of a placement new so we don't have to depend on `<new>`.
ASMJIT_INLINE_NODEBUG void* operator new(size_t, const asmjit::Support::PlacementNew& p) noexcept {
#if defined(_MSC_VER) && !defined(__clang__)
  __assume(p.ptr != nullptr); // Otherwise MSVC would emit a nullptr check.
#endif
  return p.ptr;
}

ASMJIT_INLINE_NODEBUG void operator delete(void*, const asmjit::Support::PlacementNew&) noexcept {}

#endif // ASMJIT_CORE_GLOBALS_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/inst.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/archtraits.h"
#include "../core/inst.h"

#if !defined(ASMJIT_NO_X86)
  #include "../x86/x86instapi_p.h"
#endif

#if !defined(ASMJIT_NO_AARCH64)
  #include "../arm/a64instapi_p.h"
#endif

ASMJIT_BEGIN_NAMESPACE

// InstAPI - InstId <-> String
// ===========================

#ifndef ASMJIT_NO_TEXT
Error InstAPI::instIdToString(Arch arch, InstId instId, InstStringifyOptions options, String& output) noexcept {
#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch)) {
    return x86::InstInternal::instIdToString(instId, options, output);
  }
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyAArch64(arch)) {
    return a64::InstInternal::instIdToString(instId, options, output);
  }
#endif

  return DebugUtils::errored(kErrorInvalidArch);
}

InstId InstAPI::stringToInstId(Arch arch, const char* s, size_t len) noexcept {
#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch)) {
    return x86::InstInternal::stringToInstId(s, len);
  }
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyAArch64(arch)) {
    return a64::InstInternal::stringToInstId(s, len);
  }
#endif

  return 0;
}
#endif // !ASMJIT_NO_TEXT

// InstAPI - Validate
// ==================

#ifndef ASMJIT_NO_VALIDATION
Error InstAPI::validate(Arch arch, const BaseInst& inst, const Operand_* operands, size_t opCount, ValidationFlags validationFlags) noexcept {
#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch)) {
    if (arch == Arch::kX86) {
      return x86::InstInternal::validateX86(inst, operands, opCount, validationFlags);
    }
    else {
      return x86::InstInternal::validateX64(inst, operands, opCount, validationFlags);
    }
  }
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyAArch64(arch)) {
    return a64::InstInternal::validate(inst, operands, opCount, validationFlags);
  }
#endif

  return DebugUtils::errored(kErrorInvalidArch);
}
#endif // !ASMJIT_NO_VALIDATION

// InstAPI - QueryRWInfo
// =====================

#ifndef ASMJIT_NO_INTROSPECTION
Error InstAPI::queryRWInfo(Arch arch, const BaseInst& inst, const Operand_* operands, size_t opCount, InstRWInfo* out) noexcept {
  if (ASMJIT_UNLIKELY(opCount > Globals::kMaxOpCount)) {
    return DebugUtils::errored(kErrorInvalidArgument);
  }

#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch)) {
    return x86::InstInternal::queryRWInfo(arch, inst, operands, opCount, out);
  }
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyAArch64(arch)) {
    return a64::InstInternal::queryRWInfo(inst, operands, opCount, out);
  }
#endif

  return DebugUtils::errored(kErrorInvalidArch);
}
#endif // !ASMJIT_NO_INTROSPECTION

// InstAPI - QueryFeatures
// =======================

#ifndef ASMJIT_NO_INTROSPECTION
Error InstAPI::queryFeatures(Arch arch, const BaseInst& inst, const Operand_* operands, size_t opCount, CpuFeatures* out) noexcept {
#if !defined(ASMJIT_NO_X86)
  if (Environment::isFamilyX86(arch)) {
    return x86::InstInternal::queryFeatures(arch, inst, operands, opCount, out);
  }
#endif

#if !defined(ASMJIT_NO_AARCH64)
  if (Environment::isFamilyAArch64(arch)) {
    return a64::InstInternal::queryFeatures(inst, operands, opCount, out);
  }
#endif

  return DebugUtils::errored(kErrorInvalidArch);
}
#endif // !ASMJIT_NO_INTROSPECTION

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/inst.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_INST_H_INCLUDED
#define ASMJIT_CORE_INST_H_INCLUDED

#include "../core/cpuinfo.h"
#include "../core/operand.h"
#include "../core/string.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_instruction_db
//! \{

//! Describes an instruction id and modifiers used together with the id.
//!
//! Each architecture has a set of valid instructions indexed from 0. Instruction with 0 id is, however, a special
//! instruction that describes a "no instruction" or "invalid instruction". Different architectures can assign a.
//! different instruction to the same id, each architecture typically has its own instructions indexed from 1.
//!
//! Instruction identifiers listed by architecture:
//!
//!   - \ref x86::Inst (X86 and X86_64)
//!   - \ref a64::Inst (AArch64)
typedef uint32_t InstId;

//! Instruction id parts.
//!
//! A mask that specifies a bit-layout of \ref InstId.
enum class InstIdParts : uint32_t {
  // Common Masks
  // ------------

  //! Real id without any modifiers (always 16 least significant bits).
  kRealId   = 0x0000FFFFu,
  //! Instruction is abstract (or virtual, IR, etc...).
  kAbstract = 0x80000000u,

  // ARM Specific
  // ------------

  //! AArch32 first data type, used by ASIMD instructions (`inst.dt.dt2`).
  kA32_DT   = 0x000F0000u,
  //! AArch32 second data type, used by ASIMD instructions (`inst.dt.dt2`).
  kA32_DT2  = 0x00F00000u,
  //! AArch32/AArch64 condition code.
  kARM_Cond = 0x78000000u
};

//! Instruction options.
//!
//! Instruction options complement instruction identifier and attributes.
enum class InstOptions : uint32_t {
  //! No options.
  kNone = 0,

  //! Used internally by emitters for handling errors and rare cases.
  kReserved = 0x00000001u,

  //! Prevents following a jump during compilation (Compiler).
  kUnfollow = 0x00000002u,

  //! Overwrite the destination operand(s) (Compiler).
  //!
  //! Hint that is important for register liveness analysis. It tells the compiler that the destination operand will
  //! be overwritten now or by adjacent instructions. Compiler knows when a register is completely overwritten by a
  //! single instruction, for example you don't have to mark "movaps" or "pxor x, x", however, if a pair of
  //! instructions is used and the first of them doesn't completely overwrite the content of the destination,
  //! Compiler fails to mark that register as dead.
  //!
  //! X86 Specific
  //! ------------
  //!
  //!   - All instructions that always overwrite at least the size of the register the virtual-register uses, for
  //!     example "mov", "movq", "movaps" don't need the overwrite option to be used - conversion, shuffle, and
  //!     other miscellaneous instructions included.
  //!
  //!   - All instructions that clear the destination register if all operands are the same, for example "xor x, x",
  //!     "pcmpeqb x x", etc...
  //!
  //!   - Consecutive instructions that partially overwrite the variable until there is no old content require
  //!     `BaseCompiler::overwrite()` to be used. Some examples (not always the best use cases thought):
  //!
  //!     - `movlps xmm0, ?` followed by `movhps xmm0, ?` and vice versa
  //!     - `movlpd xmm0, ?` followed by `movhpd xmm0, ?` and vice versa
  //!     - `mov al, ?` followed by `and ax, 0xFF`
  //!     - `mov al, ?` followed by `mov ah, al`
  //!     - `pinsrq xmm0, ?, 0` followed by `pinsrq xmm0, ?, 1`
  //!
  //!   - If the allocated virtual register is used temporarily for scalar operations. For example if you allocate a
  //!     full vector like `x86::Compiler::newXmm()` and then use that vector for scalar operations you should use
  //!     `overwrite()` directive:
  //!
  //!     - `sqrtss x, y` - only LO element of `x` is changed, if you don't
  //!       use HI elements, use `compiler.overwrite().sqrtss(x, y)`.
  kOverwrite = 0x00000004u,

  //! Emit short-form of the instruction.
  kShortForm = 0x00000010u,
  //! Emit long-form of the instruction.
  kLongForm = 0x00000020u,

  //! Conditional jump is likely to be taken.
  kTaken = 0x00000040u,
  //! Conditional jump is unlikely to be taken.
  kNotTaken = 0x00000080u,

  // X86 & X64 Options
  // -----------------

  //! Use ModMR instead of ModRM if applicable.
  kX86_ModMR = 0x00000100u,
  //! Use ModRM instead of ModMR if applicable.
  kX86_ModRM = 0x00000200u,
  //! Use 3-byte VEX prefix if possible (AVX) (must be 0x00000400).
  kX86_Vex3 = 0x00000400u,
  //! Use VEX prefix when both VEX|EVEX prefixes are available (HINT: AVX_VNNI).
  kX86_Vex = 0x00000800u,
  //! Use 4-byte EVEX prefix if possible (AVX-512) (must be 0x00001000).
  kX86_Evex = 0x00001000u,

  //! LOCK prefix (lock-enabled instructions only).
  kX86_Lock = 0x00002000u,
  //! REP prefix (string instructions only).
  kX86_Rep = 0x00004000u,
  //! REPNE prefix (string instructions only).
  kX86_Repne = 0x00008000u,

  //! XACQUIRE prefix (only allowed instructions).
  kX86_XAcquire = 0x00010000u,
  //! XRELEASE prefix (only allowed instructions).
  kX86_XRelease = 0x00020000u,

  //! AVX-512: embedded-rounding {er} and implicit {sae}.
  kX86_ER = 0x00040000u,
  //! AVX-512: suppress-all-exceptions {sae}.
  kX86_SAE = 0x00080000u,
  //! AVX-512: round-to-nearest (even) {rn-sae} (bits 00).
  kX86_RN_SAE = 0x00000000u,
  //! AVX-512: round-down (toward -inf) {rd-sae} (bits 01).
  kX86_RD_SAE = 0x00200000u,
  //! AVX-512: round-up (toward +inf) {ru-sae} (bits 10).
  kX86_RU_SAE = 0x00400000u,
  //! AVX-512: round-toward-zero (truncate) {rz-sae} (bits 11).
  kX86_RZ_SAE = 0x00600000u,
  //! AVX-512: Use zeroing {k}{z} instead of merging {k}.
  kX86_ZMask = 0x00800000u,

  //! AVX-512: Mask to get embedded rounding bits (2 bits).
  kX86_ERMask = kX86_RZ_SAE,
  //! AVX-512: Mask of all possible AVX-512 options except EVEX prefix flag.
  kX86_AVX512Mask = 0x00FC0000u,

  //! Force REX.B and/or VEX.B field (X64 only).
  kX86_OpCodeB = 0x01000000u,
  //! Force REX.X and/or VEX.X field (X64 only).
  kX86_OpCodeX = 0x02000000u,
  //! Force REX.R and/or VEX.R field (X64 only).
  kX86_OpCodeR = 0x04000000u,
  //! Force REX.W and/or VEX.W field (X64 only).
  kX86_OpCodeW = 0x08000000u,
  //! Force REX prefix (X64 only).
  kX86_Rex = 0x40000000u,
  //! Invalid REX prefix (set by X86 or when AH|BH|CH|DH regs are used on X64).
  kX86_InvalidRex = 0x80000000u
};
ASMJIT_DEFINE_ENUM_FLAGS(InstOptions)

//! Instruction control flow.
enum class InstControlFlow : uint32_t {
  //! Regular instruction.
  kRegular = 0u,
  //! Unconditional jump.
  kJump = 1u,
  //! Conditional jump (branch).
  kBranch = 2u,
  //! Function call.
  kCall = 3u,
  //! Function return.
  kReturn = 4u,

  //! Maximum value of `InstType`.
  kMaxValue = kReturn
};

//! Hint that is used when both input operands to the instruction are the same.
//!
//! Provides hints to the instruction RW query regarding special cases in which two or more operands are the same
//! registers. This is required by instructions such as XOR, AND, OR, SUB, etc... These hints will influence the
//! RW operations query.
enum class InstSameRegHint : uint8_t {
  //! No special handling.
  kNone = 0,
  //! Operands become read-only, the operation doesn't change the content - `X & X` and similar.
  kRO = 1,
  //! Operands become write-only, the content of the input(s) don't matter - `X ^ X`, `X - X`, and similar.
  kWO = 2
};

//! Options that can be used when converting instruction IDs to strings.
enum class InstStringifyOptions : uint32_t {
  //! No options.
  kNone = 0x00000000u,

  //! Stringify a full instruction name with known aliases.
  //!
  //! This option is designed for architectures where instruction aliases are common, for example X86, and where
  //! multiple aliases can be used in assembly code to distinguish between intention - for example instructions
  //! such as JZ and JE are the same, but the first is used in a context of equality to zero, and the second is
  //! used when two values equal (for example JE next to CMP).
  kAliases = 0x00000001u
};
ASMJIT_DEFINE_ENUM_FLAGS(InstStringifyOptions)

//! Instruction id, options, and extraReg in a single structure. This structure exists mainly to simplify analysis
//! and validation API that requires `BaseInst` and `Operand[]` array.
class BaseInst {
public:
  //! \name Members
  //! \{

  //! Instruction id with modifiers.
  InstId _id;
  //! Instruction options.
  InstOptions _options;
  //! Extra register used by the instruction (either REP register or AVX-512 selector).
  RegOnly _extraReg;

  enum Id : uint32_t {
    //! Invalid or uninitialized instruction id.
    kIdNone = 0x00000000u,
    //! Abstract instruction (BaseBuilder and BaseCompiler).
    kIdAbstract = 0x80000000u
  };

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new BaseInst instance with `id` and `options` set.
  //!
  //! Default values of `id` and `options` are zero, which means 'none' instruction. Such instruction is guaranteed
  //! to never exist for any architecture supported by AsmJit.
  ASMJIT_INLINE_NODEBUG explicit BaseInst(InstId instId = 0, InstOptions options = InstOptions::kNone) noexcept
    : _id(instId),
      _options(options),
      _extraReg() {}

  ASMJIT_INLINE_NODEBUG BaseInst(InstId instId, InstOptions options, const RegOnly& extraReg) noexcept
    : _id(instId),
      _options(options),
      _extraReg(extraReg) {}

  ASMJIT_INLINE_NODEBUG BaseInst(InstId instId, InstOptions options, const BaseReg& extraReg) noexcept
    : _id(instId),
      _options(options),
      _extraReg { extraReg.signature(), extraReg.id() } {}

  //! \}

  //! \name Instruction id and modifiers
  //! \{

  //! Returns the instruction id with modifiers.
  ASMJIT_INLINE_NODEBUG InstId id() const noexcept { return _id; }
  //! Sets the instruction id and modiiers from `id`.
  ASMJIT_INLINE_NODEBUG void setId(InstId id) noexcept { _id = id; }
  //! Resets the instruction id and modifiers to zero, see \ref kIdNone.
  ASMJIT_INLINE_NODEBUG void resetId() noexcept { _id = 0; }

  //! Returns a real instruction id that doesn't contain any modifiers.
  ASMJIT_INLINE_NODEBUG InstId realId() const noexcept { return _id & uint32_t(InstIdParts::kRealId); }

  template<InstIdParts kPart>
  ASMJIT_INLINE_NODEBUG uint32_t getInstIdPart() const noexcept {
    return (uint32_t(_id) & uint32_t(kPart)) >> Support::ConstCTZ<uint32_t(kPart)>::value;
  }

  template<InstIdParts kPart>
  ASMJIT_INLINE_NODEBUG void setInstIdPart(uint32_t value) noexcept {
    _id = (_id & ~uint32_t(kPart)) | (value << Support::ConstCTZ<uint32_t(kPart)>::value);
  }

  //! \}

  //! \name Instruction Options
  //! \{

  ASMJIT_INLINE_NODEBUG InstOptions options() const noexcept { return _options; }
  ASMJIT_INLINE_NODEBUG bool hasOption(InstOptions option) const noexcept { return Support::test(_options, option); }
  ASMJIT_INLINE_NODEBUG void setOptions(InstOptions options) noexcept { _options = options; }
  ASMJIT_INLINE_NODEBUG void addOptions(InstOptions options) noexcept { _options |= options; }
  ASMJIT_INLINE_NODEBUG void clearOptions(InstOptions options) noexcept { _options &= ~options; }
  ASMJIT_INLINE_NODEBUG void resetOptions() noexcept { _options = InstOptions::kNone; }

  //! \}

  //! \name Extra Register
  //! \{

  ASMJIT_INLINE_NODEBUG bool hasExtraReg() const noexcept { return _extraReg.isReg(); }
  ASMJIT_INLINE_NODEBUG RegOnly& extraReg() noexcept { return _extraReg; }
  ASMJIT_INLINE_NODEBUG const RegOnly& extraReg() const noexcept { return _extraReg; }
  ASMJIT_INLINE_NODEBUG void setExtraReg(const BaseReg& reg) noexcept { _extraReg.init(reg); }
  ASMJIT_INLINE_NODEBUG void setExtraReg(const RegOnly& reg) noexcept { _extraReg.init(reg); }
  ASMJIT_INLINE_NODEBUG void resetExtraReg() noexcept { _extraReg.reset(); }

  //! \}

  //! \name ARM Specific
  //! \{

  ASMJIT_INLINE_NODEBUG arm::CondCode armCondCode() const noexcept { return (arm::CondCode)getInstIdPart<InstIdParts::kARM_Cond>(); }
  ASMJIT_INLINE_NODEBUG void setArmCondCode(arm::CondCode cc) noexcept { setInstIdPart<InstIdParts::kARM_Cond>(uint32_t(cc)); }

  ASMJIT_INLINE_NODEBUG a32::DataType armDt() const noexcept { return (a32::DataType)getInstIdPart<InstIdParts::kA32_DT>(); }
  ASMJIT_INLINE_NODEBUG a32::DataType armDt2() const noexcept { return (a32::DataType)getInstIdPart<InstIdParts::kA32_DT2>(); }

  //! \}

  //! \name Statics
  //! \{

  static ASMJIT_INLINE_NODEBUG constexpr InstId composeARMInstId(uint32_t id, arm::CondCode cc) noexcept {
    return id | (uint32_t(cc) << Support::ConstCTZ<uint32_t(InstIdParts::kARM_Cond)>::value);
  }

  static ASMJIT_INLINE_NODEBUG constexpr InstId composeARMInstId(uint32_t id, a32::DataType dt, arm::CondCode cc = arm::CondCode::kAL) noexcept {
    return id | (uint32_t(dt) << Support::ConstCTZ<uint32_t(InstIdParts::kA32_DT)>::value)
              | (uint32_t(cc) << Support::ConstCTZ<uint32_t(InstIdParts::kARM_Cond)>::value);
  }

  static ASMJIT_INLINE_NODEBUG constexpr InstId composeARMInstId(uint32_t id, a32::DataType dt, a32::DataType dt2, arm::CondCode cc = arm::CondCode::kAL) noexcept {
    return id | (uint32_t(dt) << Support::ConstCTZ<uint32_t(InstIdParts::kA32_DT)>::value)
              | (uint32_t(dt2) << Support::ConstCTZ<uint32_t(InstIdParts::kA32_DT2)>::value)
              | (uint32_t(cc) << Support::ConstCTZ<uint32_t(InstIdParts::kARM_Cond)>::value);
  }

  static ASMJIT_INLINE_NODEBUG constexpr InstId extractRealId(uint32_t id) noexcept {
    return id & uint32_t(InstIdParts::kRealId);
  }

  static ASMJIT_INLINE_NODEBUG constexpr arm::CondCode extractARMCondCode(uint32_t id) noexcept {
    return (arm::CondCode)((uint32_t(id) & uint32_t(InstIdParts::kARM_Cond)) >> Support::ConstCTZ<uint32_t(InstIdParts::kARM_Cond)>::value);
  }

  //! \}
};

//! CPU read/write flags used by \ref InstRWInfo.
//!
//! These flags can be used to get a basic overview about CPU specifics flags used by instructions.
enum class CpuRWFlags : uint32_t {
  //! No flags.
  kNone = 0x00000000u,

  // Common RW Flags (0x000000FF)
  // ----------------------------

  //! Signed overflow flag.
  kOF = 0x00000001u,
  //! Carry flag.
  kCF = 0x00000002u,
  //! Zero and/or equality flag (1 if zero/equal).
  kZF = 0x00000004u,
  //! Sign flag (negative/sign, if set).
  kSF = 0x00000008u,

  // X86 Specific RW Flags
  // ----------------------------------

  //! Carry flag (X86, X86_64).
  kX86_CF = kCF,
  //! Overflow flag (X86, X86_64).
  kX86_OF = kOF,
  //! Sign flag (X86, X86_64).
  kX86_SF = kSF,
  //! Zero flag (X86, X86_64).
  kX86_ZF = kZF,

  //! Adjust flag (X86, X86_64).
  kX86_AF = 0x00000100u,
  //! Parity flag (X86, X86_64).
  kX86_PF = 0x00000200u,
  //! Direction flag (X86, X86_64).
  kX86_DF = 0x00000400u,
  //! Interrupt enable flag (X86, X86_64).
  kX86_IF = 0x00000800u,

  //! Alignment check flag (X86, X86_64).
  kX86_AC = 0x00001000u,

  //! FPU C0 status flag (X86, X86_64).
  kX86_C0 = 0x00010000u,
  //! FPU C1 status flag (X86, X86_64).
  kX86_C1 = 0x00020000u,
  //! FPU C2 status flag (X86, X86_64).
  kX86_C2 = 0x00040000u,
  //! FPU C3 status flag (X86, X86_64).
  kX86_C3 = 0x00080000u,

  // ARM Specific RW Flags
  // ----------------------------------

  kARM_V = kOF,
  kARM_C = kCF,
  kARM_Z = kZF,
  kARM_N = kSF,
  kARM_Q = 0x00000100u,
  kARM_GE = 0x00000200u
};
ASMJIT_DEFINE_ENUM_FLAGS(CpuRWFlags)

//! Operand read/write flags describe how the operand is accessed and some additional features.
enum class OpRWFlags : uint32_t {
  //! No flags.
  kNone = 0,

  //! Operand is read.
  kRead = 0x00000001u,

  //! Operand is written.
  kWrite = 0x00000002u,

  //! Operand is both read and written.
  kRW = 0x00000003u,

  //! Register operand can be replaced by a memory operand.
  kRegMem = 0x00000004u,

  //! The register must be allocated to the index of the previous register + 1.
  //!
  //! This flag is used by all architectures to describe instructions that use consecutive registers, where only the
  //! first one is encoded in the instruction, and the others are just a sequence that starts with the first one. On
  //! X86/X86_64 architecture this is used by instructions such as VP2INTERSECTD and VP2INTERSECTQ. On ARM/AArch64
  //! this is used by vector load and store instructions that can load or store multiple registers at once.
  kConsecutive = 0x00000008u,

  //! The `extendByteMask()` represents a zero extension.
  kZExt = 0x00000010u,

  //! The register must have assigned a unique physical ID, which cannot be assigned to any other register.
  kUnique = 0x00000080u,

  //! Register operand must use \ref OpRWInfo::physId().
  kRegPhysId = 0x00000100u,
  //! Base register of a memory operand must use \ref OpRWInfo::physId().
  kMemPhysId = 0x00000200u,

  //! This memory operand is only used to encode registers and doesn't access memory.
  //!
  //! X86 Specific
  //! ------------
  //!
  //! Instructions that use such feature include BNDLDX, BNDSTX, and LEA.
  kMemFake = 0x000000400u,

  //! Base register of the memory operand will be read.
  kMemBaseRead = 0x00001000u,
  //! Base register of the memory operand will be written.
  kMemBaseWrite = 0x00002000u,
  //! Base register of the memory operand will be read & written.
  kMemBaseRW = 0x00003000u,

  //! Index register of the memory operand will be read.
  kMemIndexRead = 0x00004000u,
  //! Index register of the memory operand will be written.
  kMemIndexWrite = 0x00008000u,
  //! Index register of the memory operand will be read & written.
  kMemIndexRW = 0x0000C000u,

  //! Base register of the memory operand will be modified before the operation.
  kMemBasePreModify = 0x00010000u,
  //! Base register of the memory operand will be modified after the operation.
  kMemBasePostModify = 0x00020000u
};
ASMJIT_DEFINE_ENUM_FLAGS(OpRWFlags)

// Don't remove these asserts. Read/Write flags are used extensively
// by Compiler and they must always be compatible with constants below.
static_assert(uint32_t(OpRWFlags::kRead) == 0x1, "OpRWFlags::kRead flag must be 0x1");
static_assert(uint32_t(OpRWFlags::kWrite) == 0x2, "OpRWFlags::kWrite flag must be 0x2");
static_assert(uint32_t(OpRWFlags::kRegMem) == 0x4, "OpRWFlags::kRegMem flag must be 0x4");

//! Read/Write information related to a single operand, used by \ref InstRWInfo.
struct OpRWInfo {
  //! \name Members
  //! \{

  //! Read/Write flags.
  OpRWFlags _opFlags;
  //! Physical register index, if required.
  uint8_t _physId;
  //! Size of a possible memory operand that can replace a register operand.
  uint8_t _rmSize;
  //! If non-zero, then this is a consecutive lead register, and the value describes how many registers follow.
  uint8_t _consecutiveLeadCount;
  //! Reserved for future use.
  uint8_t _reserved[1];
  //! Read bit-mask where each bit represents one byte read from Reg/Mem.
  uint64_t _readByteMask;
  //! Write bit-mask where each bit represents one byte written to Reg/Mem.
  uint64_t _writeByteMask;
  //! Zero/Sign extend bit-mask where each bit represents one byte written to Reg/Mem.
  uint64_t _extendByteMask;

  //! \}

  //! \name Reset
  //! \{

  //! Resets this operand information to all zeros.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = OpRWInfo{}; }

  //! Resets this operand info (resets all members) and set common information
  //! to the given `opFlags`, `regSize`, and possibly `physId`.
  inline void reset(OpRWFlags opFlags, uint32_t regSize, uint32_t physId = BaseReg::kIdBad) noexcept {
    _opFlags = opFlags;
    _physId = uint8_t(physId);
    _rmSize = Support::test(opFlags, OpRWFlags::kRegMem) ? uint8_t(regSize) : uint8_t(0);
    _consecutiveLeadCount = 0;
    _resetReserved();

    uint64_t mask = Support::lsbMask<uint64_t>(Support::min<uint32_t>(regSize, 64));

    _readByteMask = Support::test(opFlags, OpRWFlags::kRead) ? mask : uint64_t(0);
    _writeByteMask = Support::test(opFlags, OpRWFlags::kWrite) ? mask : uint64_t(0);
    _extendByteMask = 0;
  }

  ASMJIT_INLINE_NODEBUG void _resetReserved() noexcept {
    _reserved[0] = 0;
  }

  //! \}

  //! \name Operand Flags
  //! \{

  //! Returns operand flags.
  ASMJIT_INLINE_NODEBUG OpRWFlags opFlags() const noexcept { return _opFlags; }
  //! Tests whether operand flags contain the given `flag`.
  ASMJIT_INLINE_NODEBUG bool hasOpFlag(OpRWFlags flag) const noexcept { return Support::test(_opFlags, flag); }

  //! Adds the given `flags` to operand flags.
  ASMJIT_INLINE_NODEBUG void addOpFlags(OpRWFlags flags) noexcept { _opFlags |= flags; }
  //! Removes the given `flags` from operand flags.
  ASMJIT_INLINE_NODEBUG void clearOpFlags(OpRWFlags flags) noexcept { _opFlags &= ~flags; }

  //! Tests whether this operand is read from.
  ASMJIT_INLINE_NODEBUG bool isRead() const noexcept { return hasOpFlag(OpRWFlags::kRead); }
  //! Tests whether this operand is written to.
  ASMJIT_INLINE_NODEBUG bool isWrite() const noexcept { return hasOpFlag(OpRWFlags::kWrite); }
  //! Tests whether this operand is both read and write.
  ASMJIT_INLINE_NODEBUG bool isReadWrite() const noexcept { return (_opFlags & OpRWFlags::kRW) == OpRWFlags::kRW; }
  //! Tests whether this operand is read only.
  ASMJIT_INLINE_NODEBUG bool isReadOnly() const noexcept { return (_opFlags & OpRWFlags::kRW) == OpRWFlags::kRead; }
  //! Tests whether this operand is write only.
  ASMJIT_INLINE_NODEBUG bool isWriteOnly() const noexcept { return (_opFlags & OpRWFlags::kRW) == OpRWFlags::kWrite; }

  //! Returns the type of a lead register, which is followed by consecutive registers.
  ASMJIT_INLINE_NODEBUG uint32_t consecutiveLeadCount() const noexcept { return _consecutiveLeadCount; }

  //! Tests whether this operand is Reg/Mem
  //!
  //! Reg/Mem operands can use either register or memory.
  ASMJIT_INLINE_NODEBUG bool isRm() const noexcept { return hasOpFlag(OpRWFlags::kRegMem); }

  //! Tests whether the operand will be zero extended.
  ASMJIT_INLINE_NODEBUG bool isZExt() const noexcept { return hasOpFlag(OpRWFlags::kZExt); }

  //! Tests whether the operand must have allocated a unique physical id that cannot be shared with other register
  //! operands.
  ASMJIT_INLINE_NODEBUG bool isUnique() const noexcept { return hasOpFlag(OpRWFlags::kUnique); }

  //! \}

  //! \name Memory Flags
  //! \{

  //! Tests whether this is a fake memory operand, which is only used, because of encoding. Fake memory operands do
  //! not access any memory, they are only used to encode registers.
  ASMJIT_INLINE_NODEBUG bool isMemFake() const noexcept { return hasOpFlag(OpRWFlags::kMemFake); }

  //! Tests whether the instruction's memory BASE register is used.
  ASMJIT_INLINE_NODEBUG bool isMemBaseUsed() const noexcept { return hasOpFlag(OpRWFlags::kMemBaseRW); }
  //! Tests whether the instruction reads from its BASE registers.
  ASMJIT_INLINE_NODEBUG bool isMemBaseRead() const noexcept { return hasOpFlag(OpRWFlags::kMemBaseRead); }
  //! Tests whether the instruction writes to its BASE registers.
  ASMJIT_INLINE_NODEBUG bool isMemBaseWrite() const noexcept { return hasOpFlag(OpRWFlags::kMemBaseWrite); }
  //! Tests whether the instruction reads and writes from/to its BASE registers.
  ASMJIT_INLINE_NODEBUG bool isMemBaseReadWrite() const noexcept { return (_opFlags & OpRWFlags::kMemBaseRW) == OpRWFlags::kMemBaseRW; }
  //! Tests whether the instruction only reads from its BASE registers.
  ASMJIT_INLINE_NODEBUG bool isMemBaseReadOnly() const noexcept { return (_opFlags & OpRWFlags::kMemBaseRW) == OpRWFlags::kMemBaseRead; }
  //! Tests whether the instruction only writes to its BASE registers.
  ASMJIT_INLINE_NODEBUG bool isMemBaseWriteOnly() const noexcept { return (_opFlags & OpRWFlags::kMemBaseRW) == OpRWFlags::kMemBaseWrite; }

  //! Tests whether the instruction modifies the BASE register before it uses it to calculate the target address.
  ASMJIT_INLINE_NODEBUG bool isMemBasePreModify() const noexcept { return hasOpFlag(OpRWFlags::kMemBasePreModify); }
  //! Tests whether the instruction modifies the BASE register after it uses it to calculate the target address.
  ASMJIT_INLINE_NODEBUG bool isMemBasePostModify() const noexcept { return hasOpFlag(OpRWFlags::kMemBasePostModify); }

  //! Tests whether the instruction's memory INDEX register is used.
  ASMJIT_INLINE_NODEBUG bool isMemIndexUsed() const noexcept { return hasOpFlag(OpRWFlags::kMemIndexRW); }
  //! Tests whether the instruction reads the INDEX registers.
  ASMJIT_INLINE_NODEBUG bool isMemIndexRead() const noexcept { return hasOpFlag(OpRWFlags::kMemIndexRead); }
  //! Tests whether the instruction writes to its INDEX registers.
  ASMJIT_INLINE_NODEBUG bool isMemIndexWrite() const noexcept { return hasOpFlag(OpRWFlags::kMemIndexWrite); }
  //! Tests whether the instruction reads and writes from/to its INDEX registers.
  ASMJIT_INLINE_NODEBUG bool isMemIndexReadWrite() const noexcept { return (_opFlags & OpRWFlags::kMemIndexRW) == OpRWFlags::kMemIndexRW; }
  //! Tests whether the instruction only reads from its INDEX registers.
  ASMJIT_INLINE_NODEBUG bool isMemIndexReadOnly() const noexcept { return (_opFlags & OpRWFlags::kMemIndexRW) == OpRWFlags::kMemIndexRead; }
  //! Tests whether the instruction only writes to its INDEX registers.
  ASMJIT_INLINE_NODEBUG bool isMemIndexWriteOnly() const noexcept { return (_opFlags & OpRWFlags::kMemIndexRW) == OpRWFlags::kMemIndexWrite; }

  //! \}

  //! \name Physical Register ID
  //! \{

  //! Returns a physical id of the register that is fixed for this operand.
  //!
  //! Returns \ref BaseReg::kIdBad if any register can be used.
  ASMJIT_INLINE_NODEBUG uint32_t physId() const noexcept { return _physId; }
  //! Tests whether \ref physId() would return a valid physical register id.
  ASMJIT_INLINE_NODEBUG bool hasPhysId() const noexcept { return _physId != BaseReg::kIdBad; }
  //! Sets physical register id, which would be fixed for this operand.
  ASMJIT_INLINE_NODEBUG void setPhysId(uint32_t physId) noexcept { _physId = uint8_t(physId); }

  //! \}

  //! \name Reg/Mem Information
  //! \{

  //! Returns Reg/Mem size of the operand.
  ASMJIT_INLINE_NODEBUG uint32_t rmSize() const noexcept { return _rmSize; }
  //! Sets Reg/Mem size of the operand.
  ASMJIT_INLINE_NODEBUG void setRmSize(uint32_t rmSize) noexcept { _rmSize = uint8_t(rmSize); }

  //! \}

  //! \name Read & Write Masks
  //! \{

  //! Returns read mask.
  ASMJIT_INLINE_NODEBUG uint64_t readByteMask() const noexcept { return _readByteMask; }
  //! Returns write mask.
  ASMJIT_INLINE_NODEBUG uint64_t writeByteMask() const noexcept { return _writeByteMask; }
  //! Returns extend mask.
  ASMJIT_INLINE_NODEBUG uint64_t extendByteMask() const noexcept { return _extendByteMask; }

  //! Sets read mask.
  ASMJIT_INLINE_NODEBUG void setReadByteMask(uint64_t mask) noexcept { _readByteMask = mask; }
  //! Sets write mask.
  ASMJIT_INLINE_NODEBUG void setWriteByteMask(uint64_t mask) noexcept { _writeByteMask = mask; }
  //! Sets extend mask.
  ASMJIT_INLINE_NODEBUG void setExtendByteMask(uint64_t mask) noexcept { _extendByteMask = mask; }

  //! \}
};

//! Flags used by \ref InstRWInfo.
enum class InstRWFlags : uint32_t {
  //! No flags.
  kNone = 0x00000000u,

  //! Describes a move operation.
  //!
  //! This flag is used by RA to eliminate moves that are guaranteed to be moves only.
  kMovOp = 0x00000001u
};
ASMJIT_DEFINE_ENUM_FLAGS(InstRWFlags)

//! Read/Write information of an instruction.
struct InstRWInfo {
  //! \name Members
  //! \{

  //! Instruction flags (there are no flags at the moment, this field is reserved).
  InstRWFlags _instFlags;
  //! CPU flags read.
  CpuRWFlags _readFlags;
  //! CPU flags written.
  CpuRWFlags _writeFlags;
  //! Count of operands.
  uint8_t _opCount;
  //! CPU feature required for replacing register operand with memory operand.
  uint8_t _rmFeature;
  //! Reserved for future use.
  uint8_t _reserved[18];
  //! Read/Write info of extra register (rep{} or kz{}).
  OpRWInfo _extraReg;
  //! Read/Write info of instruction operands.
  OpRWInfo _operands[Globals::kMaxOpCount];

  //! \}

  //! \name Commons
  //! \{

  //! Resets this RW information to all zeros.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = InstRWInfo{}; }

  //! \}

  //! \name Instruction Flags
  //! \{

  //! Returns flags associated with the instruction, see \ref InstRWFlags.
  ASMJIT_INLINE_NODEBUG InstRWFlags instFlags() const noexcept { return _instFlags; }

  //! Tests whether the instruction flags contain `flag`.
  ASMJIT_INLINE_NODEBUG bool hasInstFlag(InstRWFlags flag) const noexcept { return Support::test(_instFlags, flag); }

  //! Tests whether the instruction flags contain \ref InstRWFlags::kMovOp.
  ASMJIT_INLINE_NODEBUG bool isMovOp() const noexcept { return hasInstFlag(InstRWFlags::kMovOp); }

  //! \}

  //! \name CPU Flags Information
  //! \{

  //! Returns a mask of CPU flags read.
  ASMJIT_INLINE_NODEBUG CpuRWFlags readFlags() const noexcept { return _readFlags; }
  //! Returns a mask of CPU flags written.
  ASMJIT_INLINE_NODEBUG CpuRWFlags writeFlags() const noexcept { return _writeFlags; }

  //! \}

  //! \name Reg/Mem Information
  //! \{

  //! Returns the CPU feature required to replace a register operand with memory operand. If the returned feature is
  //! zero (none) then this instruction either doesn't provide memory operand combination or there is no extra CPU
  //! feature required.
  //!
  //! X86 Specific
  //! ------------
  //!
  //! Some AVX+ instructions may require extra features for replacing registers with memory operands, for example
  //! VPSLLDQ instruction only supports `vpslldq reg, reg, imm` combination on AVX/AVX2 capable CPUs and requires
  //! AVX-512 for `vpslldq reg, mem, imm` combination.
  ASMJIT_INLINE_NODEBUG uint32_t rmFeature() const noexcept { return _rmFeature; }

  //! \}

  //! \name Operand Read/Write Information
  //! \{

  //! Returns RW information of extra register operand (extraReg).
  ASMJIT_INLINE_NODEBUG const OpRWInfo& extraReg() const noexcept { return _extraReg; }

  //! Returns RW information of all instruction's operands.
  ASMJIT_INLINE_NODEBUG const OpRWInfo* operands() const noexcept { return _operands; }

  //! Returns RW information of the operand at the given `index`.
  inline const OpRWInfo& operand(size_t index) const noexcept {
    ASMJIT_ASSERT(index < Globals::kMaxOpCount);
    return _operands[index];
  }

  //! Returns the number of operands this instruction has.
  ASMJIT_INLINE_NODEBUG uint32_t opCount() const noexcept { return _opCount; }

  //! \}
};

//! Validation flags that can be used with \ref InstAPI::validate().
enum class ValidationFlags : uint32_t {
  //! No flags.
  kNone = 0,
  //! Allow virtual registers in the instruction.
  kEnableVirtRegs = 0x01u
};
ASMJIT_DEFINE_ENUM_FLAGS(ValidationFlags)

//! Instruction API.
namespace InstAPI {

#ifndef ASMJIT_NO_TEXT
//! Appends the name of the instruction specified by `instId` and `options` into the `output` string.
//!
//! \note Instruction options would only affect instruction prefix & suffix, other options would be ignored.
//! If `instOptions` is zero then only raw instruction name (without any additional text) will be appended.
ASMJIT_API Error instIdToString(Arch arch, InstId instId, InstStringifyOptions options, String& output) noexcept;

ASMJIT_DEPRECATED("Use `instIdToString()` with `InstStringifyOptions` parameter")
static inline Error instIdToString(Arch arch, InstId instId, String& output) noexcept {
  return instIdToString(arch, instId, InstStringifyOptions::kNone, output);
}

//! Parses an instruction name in the given string `s`. Length is specified by `len` argument, which can be
//! `SIZE_MAX` if `s` is known to be null terminated.
//!
//! Returns the parsed instruction id or \ref BaseInst::kIdNone if no such instruction exists.
ASMJIT_API InstId stringToInstId(Arch arch, const char* s, size_t len) noexcept;
#endif // !ASMJIT_NO_TEXT

#ifndef ASMJIT_NO_VALIDATION
//! Validates the given instruction considering the given `validationFlags`.
ASMJIT_API Error validate(Arch arch, const BaseInst& inst, const Operand_* operands, size_t opCount, ValidationFlags validationFlags = ValidationFlags::kNone) noexcept;
#endif // !ASMJIT_NO_VALIDATION

#ifndef ASMJIT_NO_INTROSPECTION
//! Gets Read/Write information of the given instruction.
ASMJIT_API Error queryRWInfo(Arch arch, const BaseInst& inst, const Operand_* operands, size_t opCount, InstRWInfo* out) noexcept;

//! Gets CPU features required by the given instruction.
ASMJIT_API Error queryFeatures(Arch arch, const BaseInst& inst, const Operand_* operands, size_t opCount, CpuFeatures* out) noexcept;
#endif // !ASMJIT_NO_INTROSPECTION

} // {InstAPI}

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_INST_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/instdb.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/instdb_p.h"

ASMJIT_BEGIN_NAMESPACE

namespace InstNameUtils {

static constexpr uint32_t kBufferSize = 32;

static ASMJIT_FORCE_INLINE char decode5BitChar(uint32_t c) noexcept {
  uint32_t base = c <= 26 ? uint32_t('a') - 1u : uint32_t('0') - 27u;
  return char(base + c);
}

static ASMJIT_FORCE_INLINE size_t decodeToBuffer(char nameOut[kBufferSize], uint32_t nameValue, InstStringifyOptions options, const char* stringTable) noexcept {
  size_t i;

  if (nameValue & 0x80000000u) {
    // Small string of 5-bit characters.
    //
    // NOTE: Small string optimization never provides additional
    // aliases formatting, so we don't have to consider `options`.
    for (i = 0; i < 6; i++, nameValue >>= 5) {
      uint32_t c = nameValue & 0x1F;
      if (c == 0)
        break;
      nameOut[i] = decode5BitChar(c);
    }
    return i;
  }
  else {
    size_t prefixBase = nameValue & 0xFFFu;
    size_t prefixSize = (nameValue >> 12) & 0xFu;

    size_t suffixBase = (nameValue >> 16) & 0xFFFu;
    size_t suffixSize = (nameValue >> 28) & 0x7u;

    if (Support::test(options, InstStringifyOptions::kAliases) && suffixBase == 0xFFFu) {
      // Alias formatting immediately follows the instruction name in string table.
      // The first character specifies the length and then string data follows.
      prefixBase += prefixSize;
      prefixSize = uint8_t(stringTable[prefixBase]);
      ASMJIT_ASSERT(prefixSize <= kBufferSize);

      prefixBase += 1; // Skip the byte that specifies the length of a formatted alias.
    }

    for (i = 0; i < prefixSize; i++) {
      nameOut[i] = stringTable[prefixBase + i];
    }

    char* suffixOut = nameOut + prefixSize;
    for (i = 0; i < suffixSize; i++) {
      suffixOut[i] = stringTable[suffixBase + i];
    }

    return prefixSize + suffixSize;
  }
}

Error decode(uint32_t nameValue, InstStringifyOptions options, const char* stringTable, String& output) noexcept {
  char nameData[kBufferSize];
  size_t nameSize = decodeToBuffer(nameData, nameValue, options, stringTable);

  return output.append(nameData, nameSize);
}

InstId findInstruction(const char* s, size_t len, const uint32_t* nameTable, const char* stringTable, const InstNameIndex& nameIndex) noexcept {
  ASMJIT_ASSERT(s != nullptr);
  ASMJIT_ASSERT(len > 0u);

  uint32_t prefix = uint32_t(s[0]) - uint32_t('a');
  if (ASMJIT_UNLIKELY(prefix > uint32_t('z') - uint32_t('a'))) {
    return BaseInst::kIdNone;
  }

  size_t base = nameIndex.data[prefix].start;
  size_t end = nameIndex.data[prefix].end;

  if (ASMJIT_UNLIKELY(!base)) {
    return BaseInst::kIdNone;
  }

  char nameData[kBufferSize];
  for (size_t lim = end - base; lim != 0; lim >>= 1) {
    size_t instId = base + (lim >> 1);
    size_t nameSize = decodeToBuffer(nameData, nameTable[instId], InstStringifyOptions::kNone, stringTable);

    int result = Support::compareStringViews(s, len, nameData, nameSize);
    if (result < 0) {
      continue;
    }

    if (result > 0) {
      base = instId + 1;
      lim--;
      continue;
    }

    return InstId(instId);
  }

  return BaseInst::kIdNone;
}


uint32_t findAlias(const char* s, size_t len, const uint32_t* nameTable, const char* stringTable, uint32_t aliasNameCount) noexcept {
  ASMJIT_ASSERT(s != nullptr);
  ASMJIT_ASSERT(len > 0u);

  size_t base = 0;
  char nameData[kBufferSize];

  for (size_t lim = size_t(aliasNameCount) - base; lim != 0; lim >>= 1) {
    size_t index = base + (lim >> 1);
    size_t nameSize = decodeToBuffer(nameData, nameTable[index], InstStringifyOptions::kNone, stringTable);

    int result = Support::compareStringViews(s, len, nameData, nameSize);
    if (result < 0) {
      continue;
    }

    if (result > 0) {
      base = index + 1;
      lim--;
      continue;
    }

    return uint32_t(index);
  }

  return Globals::kInvalidId;
}

} // {InstNameUtils}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/instdb_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_INSTDB_P_H_INCLUDED
#define ASMJIT_CORE_INSTDB_P_H_INCLUDED

#include "../core/inst.h"
#include "../core/string.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_instruction_db
//! \{

struct InstNameIndex {
  struct Span {
    uint16_t start;
    uint16_t end;
  };

  Span data[26];
  uint16_t maxNameLength;
};

namespace InstNameUtils {

Error decode(uint32_t nameValue, InstStringifyOptions options, const char* stringTable, String& output) noexcept;
InstId findInstruction(const char* s, size_t len, const uint32_t* nameTable, const char* stringTable, const InstNameIndex& nameIndex) noexcept;
uint32_t findAlias(const char* s, size_t len, const uint32_t* nameTable, const char* stringTable, uint32_t aliasNameCount) noexcept;

} // {InstNameUtils}

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_INSTDB_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/jitallocator.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_JIT

#include "../core/archtraits.h"
#include "../core/jitallocator.h"
#include "../core/osutils_p.h"
#include "../core/support.h"
#include "../core/virtmem.h"
#include "../core/zone.h"
#include "../core/zonelist.h"
#include "../core/zonetree.h"

ASMJIT_BEGIN_NAMESPACE

// JitAllocator - Constants
// ========================

//! Number of pools to use when `JitAllocatorOptions::kUseMultiplePools` is set.
//!
//! Each pool increases granularity twice to make memory management more
//! efficient. Ideal number of pools appears to be 3 to 4 as it distributes
//! small and large functions properly.
static constexpr uint32_t kJitAllocatorMultiPoolCount = 3;

//! Minimum granularity (and the default granularity for pool #0).
static constexpr uint32_t kJitAllocatorBaseGranularity = 64;

//! Maximum block size (32MB).
static constexpr uint32_t kJitAllocatorMaxBlockSize = 1024 * 1024 * 64;

// JitAllocator - Fill Pattern
// ===========================

static inline uint32_t JitAllocator_defaultFillPattern() noexcept {
#if ASMJIT_ARCH_X86
  // X86 and X86_64 - 4x 'int3' instruction.
  return 0xCCCCCCCCu;
#else
  // Unknown...
  return 0u;
#endif
}

// JitAllocator - BitVectorRangeIterator
// =====================================

template<typename T, uint32_t B>
class BitVectorRangeIterator {
public:
  const T* _ptr;
  size_t _idx;
  size_t _end;
  T _bitWord;

  enum : uint32_t { kBitWordSize = Support::bitSizeOf<T>() };
  enum : T { kXorMask = B == 0 ? Support::allOnes<T>() : T(0) };

  ASMJIT_FORCE_INLINE BitVectorRangeIterator(const T* data, size_t numBitWords) noexcept {
    init(data, numBitWords);
  }

  ASMJIT_FORCE_INLINE BitVectorRangeIterator(const T* data, size_t numBitWords, size_t start, size_t end) noexcept {
    init(data, numBitWords, start, end);
  }

  ASMJIT_FORCE_INLINE void init(const T* data, size_t numBitWords) noexcept {
    init(data, numBitWords, 0, numBitWords * kBitWordSize);
  }

  ASMJIT_FORCE_INLINE void init(const T* data, size_t numBitWords, size_t start, size_t end) noexcept {
    ASMJIT_ASSERT(numBitWords >= (end + kBitWordSize - 1) / kBitWordSize);
    DebugUtils::unused(numBitWords);

    size_t idx = Support::alignDown(start, kBitWordSize);
    const T* ptr = data + (idx / kBitWordSize);

    T bitWord = 0;
    if (idx < end)
      bitWord = (*ptr ^ kXorMask) & (Support::allOnes<T>() << (start % kBitWordSize));

    _ptr = ptr;
    _idx = idx;
    _end = end;
    _bitWord = bitWord;
  }

  ASMJIT_FORCE_INLINE bool nextRange(size_t* rangeStart, size_t* rangeEnd, size_t rangeHint = std::numeric_limits<size_t>::max()) noexcept {
    // Skip all empty BitWords.
    while (_bitWord == 0) {
      _idx += kBitWordSize;
      if (_idx >= _end)
        return false;
      _bitWord = (*++_ptr) ^ kXorMask;
    }

    size_t i = Support::ctz(_bitWord);

    *rangeStart = _idx + i;
    _bitWord = ~(_bitWord ^ ~(Support::allOnes<T>() << i));

    if (_bitWord == 0) {
      *rangeEnd = Support::min(_idx + kBitWordSize, _end);
      while (*rangeEnd - *rangeStart < rangeHint) {
        _idx += kBitWordSize;
        if (_idx >= _end)
          break;

        _bitWord = (*++_ptr) ^ kXorMask;
        if (_bitWord != Support::allOnes<T>()) {
          size_t j = Support::ctz(~_bitWord);
          *rangeEnd = Support::min(_idx + j, _end);
          _bitWord = _bitWord ^ ~(Support::allOnes<T>() << j);
          break;
        }

        *rangeEnd = Support::min(_idx + kBitWordSize, _end);
        _bitWord = 0;
        continue;
      }

      return true;
    }
    else {
      size_t j = Support::ctz(_bitWord);
      *rangeEnd = Support::min(_idx + j, _end);

      _bitWord = ~(_bitWord ^ ~(Support::allOnes<T>() << j));
      return true;
    }
  }
};

// JitAllocator - Pool
// ===================

class JitAllocatorBlock;

class JitAllocatorPool {
public:
  ASMJIT_NONCOPYABLE(JitAllocatorPool)

  //! Double linked list of blocks.
  ZoneList<JitAllocatorBlock> blocks;
  //! Where to start looking first.
  JitAllocatorBlock* cursor = nullptr;

  //! Count of blocks.
  uint32_t blockCount = 0;
  //! Allocation granularity.
  uint16_t granularity = 0;
  //! Log2(granularity).
  uint8_t granularityLog2 = 0;
  //! Count of empty blocks (either 0 or 1 as we won't keep more blocks empty).
  uint8_t emptyBlockCount = 0;

  //! Number of bits reserved across all blocks.
  size_t totalAreaSize[2] {};
  //! Number of bits used across all blocks.
  size_t totalAreaUsed[2] {};
  //! Overhead of all blocks (in bytes).
  size_t totalOverheadBytes = 0;

  inline JitAllocatorPool(uint32_t granularity) noexcept
    : blocks(),
      granularity(uint16_t(granularity)),
      granularityLog2(uint8_t(Support::ctz(granularity))) {}

  inline void reset() noexcept {
    blocks.reset();
    cursor = nullptr;
    blockCount = 0u;
    totalAreaSize[0] = 0u;
    totalAreaSize[1] = 0u;
    totalAreaUsed[0] = 0u;
    totalAreaUsed[1] = 0u;
    totalOverheadBytes = 0u;
  }

  inline size_t byteSizeFromAreaSize(uint32_t areaSize) const noexcept { return size_t(areaSize) * granularity; }
  inline uint32_t areaSizeFromByteSize(size_t size) const noexcept { return uint32_t((size + granularity - 1) >> granularityLog2); }

  inline size_t bitWordCountFromAreaSize(uint32_t areaSize) const noexcept {
    using namespace Support;
    return alignUp<size_t>(areaSize, kBitWordSizeInBits) / kBitWordSizeInBits;
  }
};

// JitAllocator - Block
// ====================

class JitAllocatorBlock : public ZoneTreeNodeT<JitAllocatorBlock>,
                          public ZoneListNode<JitAllocatorBlock> {
public:
  ASMJIT_NONCOPYABLE(JitAllocatorBlock)

  enum Flags : uint32_t {
    //! Block has initial padding, see \ref JitAllocatorOptions::kDisableInitialPadding.
    kFlagInitialPadding = 0x00000001u,
    //! Block is empty.
    kFlagEmpty = 0x00000002u,
    //! Block is dirty (largestUnusedArea, searchStart, searchEnd).
    kFlagDirty = 0x00000004u,
    //! Block represents memory that is using large pages.
    kFlagLargePages = 0x00000008u,
    //! Block represents memory that is dual-mapped.
    kFlagDualMapped = 0x00000010u
  };

  static_assert(kFlagInitialPadding == 1, "JitAllocatorBlock::kFlagInitialPadding must be equal to 1");

  static inline uint32_t initialAreaStartByFlags(uint32_t flags) noexcept { return flags & kFlagInitialPadding; }

  //! Link to the pool that owns this block.
  JitAllocatorPool* _pool {};
  //! Virtual memory mapping - either single mapping (both pointers equal) or
  //! dual mapping, where one pointer is Read+Execute and the second Read+Write.
  VirtMem::DualMapping _mapping {};
  //! Virtual memory size (block size) [bytes].
  size_t _blockSize = 0;

  //! Block flags.
  uint32_t _flags = 0;
  //! Size of the whole block area (bit-vector size).
  uint32_t _areaSize = 0;
  //! Used area (number of bits in bit-vector used).
  uint32_t _areaUsed = 0;
  //! The largest unused continuous area in the bit-vector (or `areaSize` to initiate rescan).
  uint32_t _largestUnusedArea = 0;
  //! Start of a search range (for unused bits).
  uint32_t _searchStart = 0;
  //! End of a search range (for unused bits).
  uint32_t _searchEnd = 0;

  //! Used bit-vector (0 = unused, 1 = used).
  Support::BitWord* _usedBitVector {};
  //! Stop bit-vector (0 = don't care, 1 = stop).
  Support::BitWord* _stopBitVector {};

  inline JitAllocatorBlock(
    JitAllocatorPool* pool,
    VirtMem::DualMapping mapping,
    size_t blockSize,
    uint32_t blockFlags,
    Support::BitWord* usedBitVector,
    Support::BitWord* stopBitVector,
    uint32_t areaSize) noexcept
    : ZoneTreeNodeT(),
      _pool(pool),
      _mapping(mapping),
      _blockSize(blockSize),
      _flags(blockFlags),
      _areaSize(areaSize),
      _areaUsed(0),          // Will be initialized by clearBlock().
      _largestUnusedArea(0), // Will be initialized by clearBlock().
      _searchStart(0),       // Will be initialized by clearBlock().
      _searchEnd(0),         // Will be initialized by clearBlock().
      _usedBitVector(usedBitVector),
      _stopBitVector(stopBitVector) {

    clearBlock();
  }

  inline JitAllocatorPool* pool() const noexcept { return _pool; }

  inline uint8_t* rxPtr() const noexcept { return static_cast<uint8_t*>(_mapping.rx); }
  inline uint8_t* rwPtr() const noexcept { return static_cast<uint8_t*>(_mapping.rw); }

  inline bool hasFlag(uint32_t f) const noexcept { return (_flags & f) != 0; }
  inline void addFlags(uint32_t f) noexcept { _flags |= f; }
  inline void clearFlags(uint32_t f) noexcept { _flags &= ~f; }

  inline bool empty() const noexcept { return hasFlag(kFlagEmpty); }
  inline bool isDirty() const noexcept { return hasFlag(kFlagDirty); }
  inline void makeDirty() noexcept { addFlags(kFlagDirty); }

  inline bool hasLargePages() const noexcept { return hasFlag(kFlagLargePages); }
  inline bool hasInitialPadding() const noexcept { return hasFlag(kFlagInitialPadding); }

  inline uint32_t initialAreaStart() const noexcept { return initialAreaStartByFlags(_flags); }

  inline size_t blockSize() const noexcept { return _blockSize; }

  inline uint32_t areaSize() const noexcept { return _areaSize; }
  inline uint32_t areaUsed() const noexcept { return _areaUsed; }
  inline uint32_t areaAvailable() const noexcept { return _areaSize - _areaUsed; }
  inline uint32_t largestUnusedArea() const noexcept { return _largestUnusedArea; }

  inline void decreaseUsedArea(uint32_t value) noexcept {
    _areaUsed -= value;
    _pool->totalAreaUsed[size_t(hasLargePages())] -= value;
  }

  inline void clearBlock() noexcept {
    bool bit = hasInitialPadding();
    size_t numBitWords = _pool->bitWordCountFromAreaSize(_areaSize);

    memset(_usedBitVector, 0, numBitWords * sizeof(Support::BitWord));
    memset(_stopBitVector, 0, numBitWords * sizeof(Support::BitWord));

    Support::bitVectorSetBit(_usedBitVector, 0, bit);
    Support::bitVectorSetBit(_stopBitVector, 0, bit);

    uint32_t start = initialAreaStartByFlags(_flags);
    _areaUsed = start;
    _largestUnusedArea = _areaSize - start;
    _searchStart = start;
    _searchEnd = _areaSize;

    addFlags(JitAllocatorBlock::kFlagEmpty);
    clearFlags(JitAllocatorBlock::kFlagDirty);
  }

  inline void markAllocatedArea(uint32_t allocatedAreaStart, uint32_t allocatedAreaEnd) noexcept {
    uint32_t allocatedAreaSize = allocatedAreaEnd - allocatedAreaStart;

    // Mark the newly allocated space as occupied and also the sentinel.
    Support::bitVectorFill(_usedBitVector, allocatedAreaStart, allocatedAreaSize);
    Support::bitVectorSetBit(_stopBitVector, allocatedAreaEnd - 1, true);

    // Update search region and statistics.
    _pool->totalAreaUsed[size_t(hasLargePages())] += allocatedAreaSize;
    _areaUsed += allocatedAreaSize;

    if (areaAvailable() == 0) {
      _searchStart = _areaSize;
      _searchEnd = 0;
      _largestUnusedArea = 0;

      clearFlags(kFlagDirty | kFlagEmpty);
    }
    else {
      if (_searchStart == allocatedAreaStart)
        _searchStart = allocatedAreaEnd;
      if (_searchEnd == allocatedAreaEnd)
        _searchEnd = allocatedAreaStart;

      addFlags(kFlagDirty);
      clearFlags(kFlagEmpty);
    }
  }

  inline void markReleasedArea(uint32_t releasedAreaStart, uint32_t releasedAreaEnd) noexcept {
    uint32_t releasedAreaSize = releasedAreaEnd - releasedAreaStart;

    // Update the search region and statistics.
    _pool->totalAreaUsed[size_t(hasLargePages())] -= releasedAreaSize;
    _areaUsed -= releasedAreaSize;
    _searchStart = Support::min(_searchStart, releasedAreaStart);
    _searchEnd = Support::max(_searchEnd, releasedAreaEnd);

    // Unmark occupied bits and also the sentinel.
    Support::bitVectorClear(_usedBitVector, releasedAreaStart, releasedAreaSize);
    Support::bitVectorSetBit(_stopBitVector, releasedAreaEnd - 1, false);

    if (areaUsed() == initialAreaStart()) {
      _searchStart = initialAreaStart();
      _searchEnd = _areaSize;
      _largestUnusedArea = _areaSize - initialAreaStart();
      addFlags(kFlagEmpty);
      clearFlags(kFlagDirty);
    }
    else {
      addFlags(kFlagDirty);
    }
  }

  inline void markShrunkArea(uint32_t shrunkAreaStart, uint32_t shrunkAreaEnd) noexcept {
    uint32_t shrunkAreaSize = shrunkAreaEnd - shrunkAreaStart;

    // Shrunk area cannot start at zero as it would mean that we have shrunk the first
    // block to zero bytes, which is not allowed as such block must be released instead.
    ASMJIT_ASSERT(shrunkAreaStart != 0);
    ASMJIT_ASSERT(shrunkAreaSize != 0);

    // Update the search region and statistics.
    _pool->totalAreaUsed[size_t(hasLargePages())] -= shrunkAreaSize;
    _areaUsed -= shrunkAreaSize;
    _searchStart = Support::min(_searchStart, shrunkAreaStart);
    _searchEnd = Support::max(_searchEnd, shrunkAreaEnd);

    // Unmark the released space and move the sentinel.
    Support::bitVectorClear(_usedBitVector, shrunkAreaStart, shrunkAreaSize);
    Support::bitVectorSetBit(_stopBitVector, shrunkAreaEnd - 1, false);
    Support::bitVectorSetBit(_stopBitVector, shrunkAreaStart - 1, true);

    addFlags(kFlagDirty);
  }

  // RBTree default CMP uses '<' and '>' operators.
  inline bool operator<(const JitAllocatorBlock& other) const noexcept { return rxPtr() < other.rxPtr(); }
  inline bool operator>(const JitAllocatorBlock& other) const noexcept { return rxPtr() > other.rxPtr(); }

  // Special implementation for querying blocks by `key`, which must be in `[BlockPtr, BlockPtr + BlockSize)` range.
  inline bool operator<(const uint8_t* key) const noexcept { return rxPtr() + _blockSize <= key; }
  inline bool operator>(const uint8_t* key) const noexcept { return rxPtr() > key; }
};

// JitAllocator - PrivateImpl
// ==========================

class JitAllocatorPrivateImpl : public JitAllocator::Impl {
public:
  //! Lock for thread safety.
  mutable Lock lock;
  //! System page size (also a minimum block size).
  uint32_t pageSize;
  //! Number of active allocations.
  size_t allocationCount;

  //! Blocks from all pools in RBTree.
  ZoneTree<JitAllocatorBlock> tree;
  //! Allocator pools.
  JitAllocatorPool* pools;
  //! Number of allocator pools.
  size_t poolCount;

  inline JitAllocatorPrivateImpl(JitAllocatorPool* pools, size_t poolCount) noexcept
    : JitAllocator::Impl {},
      pageSize(0),
      allocationCount(0),
      pools(pools),
      poolCount(poolCount) {}
  inline ~JitAllocatorPrivateImpl() noexcept {}
};

static const JitAllocator::Impl JitAllocatorImpl_none {};
static const JitAllocator::CreateParams JitAllocatorParams_none {};

// JitAllocator - Utilities
// ========================

static inline JitAllocatorPrivateImpl* JitAllocatorImpl_new(const JitAllocator::CreateParams* params) noexcept {
  VirtMem::Info vmInfo = VirtMem::info();

  if (!params)
    params = &JitAllocatorParams_none;

  JitAllocatorOptions options = params->options;
  uint32_t blockSize = params->blockSize;
  uint32_t granularity = params->granularity;
  uint32_t fillPattern = params->fillPattern;

  // Setup pool count to [1..3].
  size_t poolCount = 1;
  if (Support::test(options, JitAllocatorOptions::kUseMultiplePools))
    poolCount = kJitAllocatorMultiPoolCount;

  // Setup block size [64kB..256MB].
  if (blockSize < 64 * 1024 || blockSize > 256 * 1024 * 1024 || !Support::isPowerOf2(blockSize))
    blockSize = vmInfo.pageGranularity;

  // Setup granularity [64..256].
  if (granularity < 64 || granularity > 256 || !Support::isPowerOf2(granularity))
    granularity = kJitAllocatorBaseGranularity;

  // Setup fill-pattern.
  if (uint32_t(options & JitAllocatorOptions::kCustomFillPattern) == 0)
    fillPattern = JitAllocator_defaultFillPattern();

  size_t size = sizeof(JitAllocatorPrivateImpl) + sizeof(JitAllocatorPool) * poolCount;
  void* p = ::malloc(size);
  if (ASMJIT_UNLIKELY(!p))
    return nullptr;

  VirtMem::HardenedRuntimeInfo hardenedRtInfo = VirtMem::hardenedRuntimeInfo();
  if (Support::test(hardenedRtInfo.flags, VirtMem::HardenedRuntimeFlags::kEnabled)) {
    // If we are running within a hardened environment (mapping RWX is not allowed) then we have to use dual mapping
    // or other runtime capabilities like Apple specific MAP_JIT. There is no point in not enabling these as otherwise
    // the allocation would fail and JitAllocator would not be able to allocate memory.
    if (!Support::test(hardenedRtInfo.flags, VirtMem::HardenedRuntimeFlags::kMapJit))
      options |= JitAllocatorOptions::kUseDualMapping;
  }

  JitAllocatorPool* pools = reinterpret_cast<JitAllocatorPool*>((uint8_t*)p + sizeof(JitAllocatorPrivateImpl));
  JitAllocatorPrivateImpl* impl = new(Support::PlacementNew{p}) JitAllocatorPrivateImpl(pools, poolCount);

  impl->options = options;
  impl->blockSize = blockSize;
  impl->granularity = granularity;
  impl->fillPattern = fillPattern;
  impl->pageSize = vmInfo.pageSize;

  for (size_t poolId = 0; poolId < poolCount; poolId++)
    new(Support::PlacementNew{&pools[poolId]}) JitAllocatorPool(granularity << poolId);

  return impl;
}

static inline void JitAllocatorImpl_destroy(JitAllocatorPrivateImpl* impl) noexcept {
  impl->~JitAllocatorPrivateImpl();
  ::free(impl);
}

static inline size_t JitAllocatorImpl_sizeToPoolId(const JitAllocatorPrivateImpl* impl, size_t size) noexcept {
  size_t poolId = impl->poolCount - 1;
  size_t granularity = size_t(impl->granularity) << poolId;

  while (poolId) {
    if (Support::alignUp(size, granularity) == size)
      break;
    poolId--;
    granularity >>= 1;
  }

  return poolId;
}

static inline size_t JitAllocatorImpl_bitVectorSizeToByteSize(uint32_t areaSize) noexcept {
  using Support::kBitWordSizeInBits;
  return ((areaSize + kBitWordSizeInBits - 1u) / kBitWordSizeInBits) * sizeof(Support::BitWord);
}

static inline size_t JitAllocatorImpl_calculateIdealBlockSize(JitAllocatorPrivateImpl* impl, JitAllocatorPool* pool, size_t allocationSize) noexcept {
  JitAllocatorBlock* last = pool->blocks.last();
  size_t blockSize = last ? last->blockSize() : size_t(impl->blockSize);

  // We have to increase the allocationSize if we know that the block must provide padding.
  if (!Support::test(impl->options, JitAllocatorOptions::kDisableInitialPadding)) {
    size_t granularity = pool->granularity;
    if (SIZE_MAX - allocationSize < granularity)
      return 0; // Overflown
    allocationSize += granularity;
  }

  if (blockSize < kJitAllocatorMaxBlockSize)
    blockSize *= 2u;

  if (allocationSize > blockSize) {
    blockSize = Support::alignUp(allocationSize, impl->blockSize);
    if (ASMJIT_UNLIKELY(blockSize < allocationSize))
      return 0; // Overflown.
  }

  return blockSize;
}

ASMJIT_NOINLINE
ASMJIT_FAVOR_SPEED static void JitAllocatorImpl_fillPattern(void* mem, uint32_t pattern, size_t byteSize) noexcept {
  // NOTE: This is always used to fill a pattern in allocated / freed memory. The allocation has always
  // a granularity that is greater than the pattern, however, when shrink() is used, we may end up having
  // an unaligned start, so deal with it here and then copy aligned pattern in the loop.
  if ((uintptr_t(mem) & 0x1u) && byteSize >= 1u) {
    static_cast<uint8_t*>(mem)[0] = uint8_t(pattern & 0xFF);
    mem = static_cast<uint8_t*>(mem) + 1;
    byteSize--;
  }

  if ((uintptr_t(mem) & 0x2u) && byteSize >= 2u) {
    static_cast<uint16_t*>(mem)[0] = uint16_t(pattern & 0xFFFF);
    mem = static_cast<uint16_t*>(mem) + 1;
    byteSize -= 2;
  }

  // Something would be seriously broken if we end up with aligned `mem`, but unaligned `byteSize`.
  ASMJIT_ASSERT((byteSize & 0x3u) == 0u);

  uint32_t* mem32 = static_cast<uint32_t*>(mem);
  size_t n = byteSize / 4u;

  for (size_t i = 0; i < n; i++)
    mem32[i] = pattern;
}

// Allocate a new `JitAllocatorBlock` for the given `blockSize`.
//
// NOTE: The block doesn't have `kFlagEmpty` flag set, because the new block
// is only allocated when it's actually needed, so it would be cleared anyway.
static Error JitAllocatorImpl_newBlock(JitAllocatorPrivateImpl* impl, JitAllocatorBlock** dst, JitAllocatorPool* pool, size_t blockSize) noexcept {
  using Support::BitWord;
  using Support::kBitWordSizeInBits;

  uint32_t blockFlags = 0;
  if (!Support::test(impl->options, JitAllocatorOptions::kDisableInitialPadding))
    blockFlags |= JitAllocatorBlock::kFlagInitialPadding;

  VirtMem::DualMapping virtMem {};
  VirtMem::MemoryFlags memFlags = VirtMem::MemoryFlags::kAccessRWX;

  if (Support::test(impl->options, JitAllocatorOptions::kUseDualMapping)) {
    ASMJIT_PROPAGATE(VirtMem::allocDualMapping(&virtMem, blockSize, memFlags));
    blockFlags |= JitAllocatorBlock::kFlagDualMapped;
  }
  else {
    bool allocateRegularPages = true;
    if (Support::test(impl->options, JitAllocatorOptions::kUseLargePages)) {
      size_t largePageSize = VirtMem::largePageSize();
      bool tryLargePage = blockSize >= largePageSize || Support::test(impl->options, JitAllocatorOptions::kAlignBlockSizeToLargePage);

      // Only proceed if we can actually allocate large pages.
      if (largePageSize && tryLargePage) {
        size_t largeBlockSize = Support::alignUp(blockSize, largePageSize);
        Error err = VirtMem::alloc(&virtMem.rx, largeBlockSize, memFlags | VirtMem::MemoryFlags::kMMapLargePages);

        // Fallback to regular pages if large page(s) allocation failed.
        if (err == kErrorOk) {
          allocateRegularPages = false;
          blockSize = largeBlockSize;
          blockFlags |= JitAllocatorBlock::kFlagLargePages;
        }
      }
    }

    // Called either if large pages were not requested or large page(s) allocation failed.
    if (allocateRegularPages) {
      ASMJIT_PROPAGATE(VirtMem::alloc(&virtMem.rx, blockSize, memFlags));
    }

    virtMem.rw = virtMem.rx;
  }

  uint32_t areaSize = uint32_t((blockSize + pool->granularity - 1) >> pool->granularityLog2);
  uint32_t numBitWords = (areaSize + kBitWordSizeInBits - 1u) / kBitWordSizeInBits;
  uint8_t* blockPtr = static_cast<uint8_t*>(::malloc(sizeof(JitAllocatorBlock) + size_t(numBitWords) * 2u * sizeof(BitWord)));

  // Out of memory...
  if (ASMJIT_UNLIKELY(blockPtr == nullptr)) {
    if (Support::test(impl->options, JitAllocatorOptions::kUseDualMapping))
      VirtMem::releaseDualMapping(&virtMem, blockSize);
    else
      VirtMem::release(virtMem.rx, blockSize);
    return DebugUtils::errored(kErrorOutOfMemory);
  }

  // Fill the allocated virtual memory if secure mode is enabled.
  if (Support::test(impl->options, JitAllocatorOptions::kFillUnusedMemory)) {
    VirtMem::ProtectJitReadWriteScope scope(virtMem.rw, blockSize);
    JitAllocatorImpl_fillPattern(virtMem.rw, impl->fillPattern, blockSize);
  }

  BitWord* bitWords = reinterpret_cast<BitWord*>(blockPtr + sizeof(JitAllocatorBlock));
  *dst = new(Support::PlacementNew{blockPtr}) JitAllocatorBlock(pool, virtMem, blockSize, blockFlags, bitWords, bitWords + numBitWords, areaSize);
  return kErrorOk;
}

static void JitAllocatorImpl_deleteBlock(JitAllocatorPrivateImpl* impl, JitAllocatorBlock* block) noexcept {
  DebugUtils::unused(impl);

  if (block->hasFlag(JitAllocatorBlock::kFlagDualMapped))
    VirtMem::releaseDualMapping(&block->_mapping, block->blockSize());
  else
    VirtMem::release(block->rxPtr(), block->blockSize());

  ::free(block);
}

static void JitAllocatorImpl_insertBlock(JitAllocatorPrivateImpl* impl, JitAllocatorBlock* block) noexcept {
  JitAllocatorPool* pool = block->pool();

  if (!pool->cursor)
    pool->cursor = block;

  // Add to RBTree and List.
  impl->tree.insert(block);
  pool->blocks.append(block);

  // Update statistics.
  size_t statIndex = size_t(block->hasLargePages());
  pool->blockCount++;
  pool->totalAreaSize[statIndex] += block->areaSize();
  pool->totalAreaUsed[statIndex] += block->areaUsed();
  pool->totalOverheadBytes += sizeof(JitAllocatorBlock) + JitAllocatorImpl_bitVectorSizeToByteSize(block->areaSize()) * 2u;
}

static void JitAllocatorImpl_removeBlock(JitAllocatorPrivateImpl* impl, JitAllocatorBlock* block) noexcept {
  JitAllocatorPool* pool = block->pool();

  // Remove from RBTree and List.
  if (pool->cursor == block)
    pool->cursor = block->hasPrev() ? block->prev() : block->next();

  impl->tree.remove(block);
  pool->blocks.unlink(block);

  // Update statistics.
  size_t statIndex = size_t(block->hasLargePages());
  pool->blockCount--;
  pool->totalAreaSize[statIndex] -= block->areaSize();
  pool->totalAreaUsed[statIndex] -= block->areaUsed();
  pool->totalOverheadBytes -= sizeof(JitAllocatorBlock) + JitAllocatorImpl_bitVectorSizeToByteSize(block->areaSize()) * 2u;
}

static void JitAllocatorImpl_wipeOutBlock(JitAllocatorPrivateImpl* impl, JitAllocatorBlock* block) noexcept {
  if (block->hasFlag(JitAllocatorBlock::kFlagEmpty))
    return;

  JitAllocatorPool* pool = block->pool();
  if (Support::test(impl->options, JitAllocatorOptions::kFillUnusedMemory)) {
    VirtMem::protectJitMemory(VirtMem::ProtectJitAccess::kReadWrite);

    uint32_t granularity = pool->granularity;
    uint8_t* rwPtr = block->rwPtr();
    BitVectorRangeIterator<Support::BitWord, 0> it(block->_usedBitVector, pool->bitWordCountFromAreaSize(block->areaSize()));

    size_t rangeStart;
    size_t rangeEnd;

    while (it.nextRange(&rangeStart, &rangeEnd)) {
      uint8_t* spanPtr = rwPtr + rangeStart * granularity;
      size_t spanSize = (rangeEnd - rangeStart) * granularity;

      JitAllocatorImpl_fillPattern(spanPtr, impl->fillPattern, spanSize);
      VirtMem::flushInstructionCache(spanPtr, spanSize);
    }
    VirtMem::protectJitMemory(VirtMem::ProtectJitAccess::kReadExecute);
  }

  block->clearBlock();
}

// JitAllocator - Construction & Destruction
// =========================================

JitAllocator::JitAllocator(const CreateParams* params) noexcept {
  _impl = JitAllocatorImpl_new(params);
  if (ASMJIT_UNLIKELY(!_impl))
    _impl = const_cast<JitAllocator::Impl*>(&JitAllocatorImpl_none);
}

JitAllocator::~JitAllocator() noexcept {
  if (_impl == &JitAllocatorImpl_none)
    return;

  reset(ResetPolicy::kHard);
  JitAllocatorImpl_destroy(static_cast<JitAllocatorPrivateImpl*>(_impl));
}

// JitAllocator - Reset
// ====================

void JitAllocator::reset(ResetPolicy resetPolicy) noexcept {
  if (_impl == &JitAllocatorImpl_none)
    return;

  JitAllocatorPrivateImpl* impl = static_cast<JitAllocatorPrivateImpl*>(_impl);
  impl->tree.reset();
  size_t poolCount = impl->poolCount;

  for (size_t poolId = 0; poolId < poolCount; poolId++) {
    JitAllocatorPool& pool = impl->pools[poolId];
    JitAllocatorBlock* block = pool.blocks.first();

    pool.reset();

    if (block) {
      JitAllocatorBlock* blockToKeep = nullptr;
      if (resetPolicy != ResetPolicy::kHard && uint32_t(impl->options & JitAllocatorOptions::kImmediateRelease) == 0) {
        blockToKeep = block;
        block = block->next();
      }

      while (block) {
        JitAllocatorBlock* next = block->next();
        JitAllocatorImpl_deleteBlock(impl, block);
        block = next;
      }

      if (blockToKeep) {
        blockToKeep->_listNodes[0] = nullptr;
        blockToKeep->_listNodes[1] = nullptr;
        JitAllocatorImpl_wipeOutBlock(impl, blockToKeep);
        JitAllocatorImpl_insertBlock(impl, blockToKeep);
        pool.emptyBlockCount = 1;
      }
    }
  }
}

// JitAllocator - Statistics
// =========================

JitAllocator::Statistics JitAllocator::statistics() const noexcept {
  Statistics statistics;
  statistics.reset();

  if (ASMJIT_LIKELY(_impl != &JitAllocatorImpl_none)) {
    JitAllocatorPrivateImpl* impl = static_cast<JitAllocatorPrivateImpl*>(_impl);
    LockGuard guard(impl->lock);

    size_t poolCount = impl->poolCount;
    for (size_t poolId = 0; poolId < poolCount; poolId++) {
      const JitAllocatorPool& pool = impl->pools[poolId];
      statistics._blockCount   += size_t(pool.blockCount);
      statistics._reservedSize += size_t(pool.totalAreaSize[0] + pool.totalAreaSize[1]) * pool.granularity;
      statistics._usedSize     += size_t(pool.totalAreaUsed[0] + pool.totalAreaUsed[1]) * pool.granularity;
      statistics._overheadSize += size_t(pool.totalOverheadBytes);
    }

    statistics._allocationCount = impl->allocationCount;
  }

  return statistics;
}

// JitAllocator - Alloc & Release
// ==============================

Error JitAllocator::alloc(Span& out, size_t size) noexcept {
  out = Span{};

  if (ASMJIT_UNLIKELY(_impl == &JitAllocatorImpl_none))
    return DebugUtils::errored(kErrorNotInitialized);

  JitAllocatorPrivateImpl* impl = static_cast<JitAllocatorPrivateImpl*>(_impl);
  constexpr uint32_t kNoIndex = std::numeric_limits<uint32_t>::max();

  // Align to the minimum granularity by default.
  size = Support::alignUp<size_t>(size, impl->granularity);
  if (ASMJIT_UNLIKELY(size == 0))
    return DebugUtils::errored(kErrorInvalidArgument);

  if (ASMJIT_UNLIKELY(size > std::numeric_limits<uint32_t>::max() / 2))
    return DebugUtils::errored(kErrorTooLarge);

  LockGuard guard(impl->lock);
  JitAllocatorPool* pool = &impl->pools[JitAllocatorImpl_sizeToPoolId(impl, size)];

  uint32_t areaIndex = kNoIndex;
  uint32_t areaSize = uint32_t(pool->areaSizeFromByteSize(size));

  // Try to find the requested memory area in existing blocks.
  JitAllocatorBlock* block = pool->blocks.first();
  if (block) {
    JitAllocatorBlock* initial = block;
    do {
      JitAllocatorBlock* next = block->hasNext() ? block->next() : pool->blocks.first();
      if (block->areaAvailable() >= areaSize) {
        if (block->isDirty() || block->largestUnusedArea() >= areaSize) {
          BitVectorRangeIterator<Support::BitWord, 0> it(block->_usedBitVector, pool->bitWordCountFromAreaSize(block->areaSize()), block->_searchStart, block->_searchEnd);

          size_t rangeStart = 0;
          size_t rangeEnd = block->areaSize();

          size_t searchStart = SIZE_MAX;
          size_t largestArea = 0;

          while (it.nextRange(&rangeStart, &rangeEnd, areaSize)) {
            size_t rangeSize = rangeEnd - rangeStart;
            if (rangeSize >= areaSize) {
              areaIndex = uint32_t(rangeStart);
              break;
            }

            searchStart = Support::min(searchStart, rangeStart);
            largestArea = Support::max(largestArea, rangeSize);
          }

          if (areaIndex != kNoIndex)
            break;

          if (searchStart != SIZE_MAX) {
            // Because we have iterated over the entire block, we can now mark the
            // largest unused area that can be used to cache the next traversal.
            size_t searchEnd = rangeEnd;

            block->_searchStart = uint32_t(searchStart);
            block->_searchEnd = uint32_t(searchEnd);
            block->_largestUnusedArea = uint32_t(largestArea);
            block->clearFlags(JitAllocatorBlock::kFlagDirty);
          }
        }
      }

      block = next;
    } while (block != initial);
  }

  // Allocate a new block if there is no region of a required size.
  if (areaIndex == kNoIndex) {
    size_t blockSize = JitAllocatorImpl_calculateIdealBlockSize(impl, pool, size);
    if (ASMJIT_UNLIKELY(!blockSize))
      return DebugUtils::errored(kErrorOutOfMemory);

    ASMJIT_PROPAGATE(JitAllocatorImpl_newBlock(impl, &block, pool, blockSize));
    areaIndex = block->initialAreaStart();

    JitAllocatorImpl_insertBlock(impl, block);
    block->_searchStart += areaSize;
    block->_largestUnusedArea -= areaSize;
  }
  else if (block->hasFlag(JitAllocatorBlock::kFlagEmpty)) {
    pool->emptyBlockCount--;
    block->clearFlags(JitAllocatorBlock::kFlagEmpty);
  }

  // Update statistics.
  impl->allocationCount++;
  block->markAllocatedArea(areaIndex, areaIndex + areaSize);

  // Return a span referencing the allocated memory.
  size_t offset = pool->byteSizeFromAreaSize(areaIndex);
  ASMJIT_ASSERT(offset <= block->blockSize() - size);

  out._rx = block->rxPtr() + offset;
  out._rw = block->rwPtr() + offset;
  out._size = size;
  out._block = static_cast<void*>(block);

  return kErrorOk;
}

Error JitAllocator::release(void* rx) noexcept {
  if (ASMJIT_UNLIKELY(_impl == &JitAllocatorImpl_none))
    return DebugUtils::errored(kErrorNotInitialized);

  if (ASMJIT_UNLIKELY(!rx))
    return DebugUtils::errored(kErrorInvalidArgument);

  JitAllocatorPrivateImpl* impl = static_cast<JitAllocatorPrivateImpl*>(_impl);
  LockGuard guard(impl->lock);

  JitAllocatorBlock* block = impl->tree.get(static_cast<uint8_t*>(rx));
  if (ASMJIT_UNLIKELY(!block))
    return DebugUtils::errored(kErrorInvalidState);

  // Offset relative to the start of the block.
  JitAllocatorPool* pool = block->pool();
  size_t offset = (size_t)((uint8_t*)rx - block->rxPtr());

  // The first bit representing the allocated area and its size.
  uint32_t areaIndex = uint32_t(offset >> pool->granularityLog2);
  uint32_t areaEnd = uint32_t(Support::bitVectorIndexOf(block->_stopBitVector, areaIndex, true)) + 1;
  uint32_t areaSize = areaEnd - areaIndex;

  impl->allocationCount--;
  block->markReleasedArea(areaIndex, areaEnd);

  // Fill the released memory if the secure mode is enabled.
  if (Support::test(impl->options, JitAllocatorOptions::kFillUnusedMemory)) {
    uint8_t* spanPtr = block->rwPtr() + areaIndex * pool->granularity;
    size_t spanSize = areaSize * pool->granularity;

    VirtMem::ProtectJitReadWriteScope scope(spanPtr, spanSize);
    JitAllocatorImpl_fillPattern(spanPtr, impl->fillPattern, spanSize);
  }

  // Release the whole block if it became empty.
  if (block->empty()) {
    if (pool->emptyBlockCount || Support::test(impl->options, JitAllocatorOptions::kImmediateRelease)) {
      JitAllocatorImpl_removeBlock(impl, block);
      JitAllocatorImpl_deleteBlock(impl, block);
    }
    else {
      pool->emptyBlockCount++;
    }
  }

  return kErrorOk;
}

static Error JitAllocatorImpl_shrink(JitAllocatorPrivateImpl* impl, JitAllocator::Span& span, size_t newSize, bool alreadyUnderWriteScope) noexcept {
  JitAllocatorBlock* block = static_cast<JitAllocatorBlock*>(span._block);
  if (ASMJIT_UNLIKELY(!block))
    return DebugUtils::errored(kErrorInvalidArgument);

  LockGuard guard(impl->lock);

  // Offset relative to the start of the block.
  JitAllocatorPool* pool = block->pool();
  size_t offset = (size_t)((uint8_t*)span.rx() - block->rxPtr());

  // The first bit representing the allocated area and its size.
  uint32_t areaStart = uint32_t(offset >> pool->granularityLog2);

  // Don't trust `span.size()` - if it has been already truncated we would be off...
  bool isUsed = Support::bitVectorGetBit(block->_usedBitVector, areaStart);
  if (ASMJIT_UNLIKELY(!isUsed))
    return DebugUtils::errored(kErrorInvalidArgument);

  uint32_t areaEnd = uint32_t(Support::bitVectorIndexOf(block->_stopBitVector, areaStart, true)) + 1;
  uint32_t areaPrevSize = areaEnd - areaStart;
  uint32_t spanPrevSize = areaPrevSize * pool->granularity;
  uint32_t areaShrunkSize = pool->areaSizeFromByteSize(newSize);

  if (ASMJIT_UNLIKELY(areaShrunkSize > areaPrevSize))
    return DebugUtils::errored(kErrorInvalidArgument);

  uint32_t areaDiff = areaPrevSize - areaShrunkSize;
  if (areaDiff) {
    block->markShrunkArea(areaStart + areaShrunkSize, areaEnd);
    span._size = pool->byteSizeFromAreaSize(areaShrunkSize);
  }

  // Fill released memory if the secure mode is enabled.
  if (newSize < spanPrevSize && Support::test(impl->options, JitAllocatorOptions::kFillUnusedMemory)) {
    uint8_t* spanPtr = block->rwPtr() + (areaStart + areaShrunkSize) * pool->granularity;
    size_t spanSize = areaDiff * pool->granularity;

    if (!alreadyUnderWriteScope) {
      VirtMem::ProtectJitReadWriteScope scope(spanPtr, spanSize, VirtMem::CachePolicy::kNeverFlush);
      JitAllocatorImpl_fillPattern(spanPtr, impl->fillPattern, spanSize);
    }
    else {
      JitAllocatorImpl_fillPattern(spanPtr, impl->fillPattern, spanSize);
    }
  }

  return kErrorOk;
}

Error JitAllocator::shrink(Span& span, size_t newSize) noexcept {
  if (ASMJIT_UNLIKELY(_impl == &JitAllocatorImpl_none))
    return DebugUtils::errored(kErrorNotInitialized);

  if (ASMJIT_UNLIKELY(!span.rx()))
    return DebugUtils::errored(kErrorInvalidArgument);

  if (ASMJIT_UNLIKELY(newSize == 0)) {
    Error err = release(span.rx());
    span = Span{};
    return err;
  }

  return JitAllocatorImpl_shrink(static_cast<JitAllocatorPrivateImpl*>(_impl), span, newSize, false);
}

Error JitAllocator::query(Span& out, void* rx) const noexcept {
  out = Span{};

  if (ASMJIT_UNLIKELY(_impl == &JitAllocatorImpl_none))
    return DebugUtils::errored(kErrorNotInitialized);

  JitAllocatorPrivateImpl* impl = static_cast<JitAllocatorPrivateImpl*>(_impl);
  LockGuard guard(impl->lock);
  JitAllocatorBlock* block = impl->tree.get(static_cast<uint8_t*>(rx));

  if (ASMJIT_UNLIKELY(!block))
    return DebugUtils::errored(kErrorInvalidArgument);

  // Offset relative to the start of the block.
  JitAllocatorPool* pool = block->pool();
  size_t offset = (size_t)((uint8_t*)rx - block->rxPtr());

  // The first bit representing the allocated area and its size.
  uint32_t areaStart = uint32_t(offset >> pool->granularityLog2);

  bool isUsed = Support::bitVectorGetBit(block->_usedBitVector, areaStart);
  if (ASMJIT_UNLIKELY(!isUsed))
    return DebugUtils::errored(kErrorInvalidArgument);

  uint32_t areaEnd = uint32_t(Support::bitVectorIndexOf(block->_stopBitVector, areaStart, true)) + 1;
  size_t byteOffset = pool->byteSizeFromAreaSize(areaStart);
  size_t byteSize = pool->byteSizeFromAreaSize(areaEnd - areaStart);

  out._rx = static_cast<uint8_t*>(block->_mapping.rx) + byteOffset;
  out._rw = static_cast<uint8_t*>(block->_mapping.rw) + byteOffset;
  out._size = byteSize;
  out._block = static_cast<void*>(block);

  return kErrorOk;
}

// JitAllocator - Write
// ====================

static ASMJIT_FORCE_INLINE VirtMem::CachePolicy JitAllocator_defaultPolicyForSpan(const JitAllocator::Span& span) noexcept {
  if (Support::test(span.flags(), JitAllocator::Span::Flags::kInstructionCacheClean))
    return VirtMem::CachePolicy::kNeverFlush;
  else
    return VirtMem::CachePolicy::kFlushAfterWrite;
}

Error JitAllocator::write(Span& span, size_t offset, const void* src, size_t size, VirtMem::CachePolicy policy) noexcept {
  if (ASMJIT_UNLIKELY(span._block == nullptr || offset > span.size() || span.size() - offset < size))
    return DebugUtils::errored(kErrorInvalidArgument);

  if (ASMJIT_UNLIKELY(size == 0))
    return kErrorOk;

  if (policy == VirtMem::CachePolicy::kDefault)
    policy = JitAllocator_defaultPolicyForSpan(span);

  VirtMem::ProtectJitReadWriteScope writeScope(span.rx(), span.size(), policy);
  memcpy(static_cast<uint8_t*>(span.rw()) + offset, src, size);
  return kErrorOk;
}

Error JitAllocator::write(Span& span, WriteFunc writeFunc, void* userData, VirtMem::CachePolicy policy) noexcept {
  if (ASMJIT_UNLIKELY(span._block == nullptr) || span.size() == 0)
    return DebugUtils::errored(kErrorInvalidArgument);

  size_t size = span.size();
  if (ASMJIT_UNLIKELY(size == 0))
    return kErrorOk;

  if (policy == VirtMem::CachePolicy::kDefault)
    policy = JitAllocator_defaultPolicyForSpan(span);

  VirtMem::ProtectJitReadWriteScope writeScope(span.rx(), span.size(), policy);
  ASMJIT_PROPAGATE(writeFunc(span, userData));

  // Check whether span.truncate() has been called.
  if (span.size() != size) {
    // OK, this is a bit awkward... However, shrink wants the original span and newSize, so we have to swap.
    std::swap(span._size, size);
    return JitAllocatorImpl_shrink(static_cast<JitAllocatorPrivateImpl*>(_impl), span, size, true);
  }

  return kErrorOk;
}

// JitAllocator - Write Scope
// ==========================

Error JitAllocator::beginWriteScope(WriteScopeData& scope, VirtMem::CachePolicy policy) noexcept {
  scope._allocator = this;
  scope._data[0] = size_t(policy);
  return kErrorOk;
}

Error JitAllocator::endWriteScope(WriteScopeData& scope) noexcept {
  if (ASMJIT_UNLIKELY(!scope._allocator))
    return DebugUtils::errored(kErrorInvalidArgument);

  return kErrorOk;
}

Error JitAllocator::flushWriteScope(WriteScopeData& scope) noexcept {
  if (ASMJIT_UNLIKELY(!scope._allocator))
    return DebugUtils::errored(kErrorInvalidArgument);

  return kErrorOk;
}

Error JitAllocator::scopedWrite(WriteScopeData& scope, Span& span, size_t offset, const void* src, size_t size) noexcept {
  if (ASMJIT_UNLIKELY(!scope._allocator))
    return DebugUtils::errored(kErrorInvalidArgument);

  VirtMem::CachePolicy policy = VirtMem::CachePolicy(scope._data[0]);
  return scope._allocator->write(span, offset, src, size, policy);
}

Error JitAllocator::scopedWrite(WriteScopeData& scope, Span& span, WriteFunc writeFunc, void* userData) noexcept {
  if (ASMJIT_UNLIKELY(!scope._allocator))
    return DebugUtils::errored(kErrorInvalidArgument);

  VirtMem::CachePolicy policy = VirtMem::CachePolicy(scope._data[0]);
  return scope._allocator->write(span, writeFunc, userData, policy);
}

// JitAllocator - Tests
// ====================

#if defined(ASMJIT_TEST)
// A pseudo random number generator based on a paper by Sebastiano Vigna:
//   http://vigna.di.unimi.it/ftp/papers/xorshiftplus.pdf
class Random {
public:
  // Constants suggested as `23/18/5`.
  enum Steps : uint32_t {
    kStep1_SHL = 23,
    kStep2_SHR = 18,
    kStep3_SHR = 5
  };

  inline explicit Random(uint64_t seed = 0) noexcept { reset(seed); }
  inline Random(const Random& other) noexcept = default;

  inline void reset(uint64_t seed = 0) noexcept {
    // The number is arbitrary, it means nothing.
    constexpr uint64_t kZeroSeed = 0x1F0A2BE71D163FA0u;

    // Generate the state data by using splitmix64.
    for (uint32_t i = 0; i < 2; i++) {
      seed += 0x9E3779B97F4A7C15u;
      uint64_t x = seed;
      x = (x ^ (x >> 30)) * 0xBF58476D1CE4E5B9u;
      x = (x ^ (x >> 27)) * 0x94D049BB133111EBu;
      x = (x ^ (x >> 31));
      _state[i] = x != 0 ? x : kZeroSeed;
    }
  }

  inline uint32_t nextUInt32() noexcept {
    return uint32_t(nextUInt64() >> 32);
  }

  inline uint64_t nextUInt64() noexcept {
    uint64_t x = _state[0];
    uint64_t y = _state[1];

    x ^= x << kStep1_SHL;
    y ^= y >> kStep3_SHR;
    x ^= x >> kStep2_SHR;
    x ^= y;

    _state[0] = y;
    _state[1] = x;
    return x + y;
  }

  uint64_t _state[2];
};

namespace JitAllocatorUtils {
  static void fillPattern64(void* p_, uint64_t pattern, size_t sizeInBytes) noexcept {
    uint64_t* p = static_cast<uint64_t*>(p_);
    size_t n = sizeInBytes / 8u;

    for (size_t i = 0; i < n; i++)
      p[i] = pattern;
  }

  static bool verifyPattern64(const void* p_, uint64_t pattern, size_t sizeInBytes) noexcept {
    const uint64_t* p = static_cast<const uint64_t*>(p_);
    size_t n = sizeInBytes / 8u;

    for (size_t i = 0; i < n; i++) {
      if (p[i] != pattern) {
        INFO("Pattern verification failed at 0x%p [%zu * 8]: value(0x%016llX) != expected(0x%016llX)",
          p,
          i,
          (unsigned long long)p[i],
          (unsigned long long)pattern);
        return false;
      }
    }

    return true;
  }
}

// Helper class to verify that JitAllocator doesn't return addresses that overlap.
class JitAllocatorWrapper {
public:
  // Address to a memory region of a given size.
  class Range {
  public:
    inline Range(uint8_t* addr, size_t size) noexcept
      : addr(addr),
        size(size) {}
    uint8_t* addr;
    size_t size;
  };

  // Based on JitAllocator::Block, serves our purpose well...
  class Record : public ZoneTreeNodeT<Record>,
                 public Range {
  public:
    //! Read/write address, in case this is a dual mapping.
    void* _rw;
    //! Describes a pattern used to fill the allocated memory.
    uint64_t pattern;

    inline Record(void* rx, void* rw, size_t size, uint64_t pattern)
      : ZoneTreeNodeT<Record>(),
        Range(static_cast<uint8_t*>(rx), size),
        _rw(rw),
        pattern(pattern) {}

    inline void* rx() const noexcept { return addr; }
    inline void* rw() const noexcept { return _rw; }

    inline bool operator<(const Record& other) const noexcept { return addr < other.addr; }
    inline bool operator>(const Record& other) const noexcept { return addr > other.addr; }

    inline bool operator<(const uint8_t* key) const noexcept { return addr + size <= key; }
    inline bool operator>(const uint8_t* key) const noexcept { return addr > key; }
  };

  Zone _zone;
  ZoneAllocator _heap;
  ZoneTree<Record> _records;
  JitAllocator _allocator;
  Random _rng;

  explicit JitAllocatorWrapper(const JitAllocator::CreateParams* params) noexcept
    : _zone(1024u * 1024u),
      _heap(&_zone),
      _allocator(params),
      _rng(0x123456789u) {}

  void _insert(void* pRX, void* pRW, size_t size) noexcept {
    uint8_t* p = static_cast<uint8_t*>(pRX);
    uint8_t* pEnd = p + size - 1;

    Record* record;

    record = _records.get(p);
    EXPECT_NULL(record)
      .message("Address [%p:%p] collides with a newly allocated [%p:%p]\n", record->addr, record->addr + record->size, p, p + size);

    record = _records.get(pEnd);
    EXPECT_NULL(record)
      .message("Address [%p:%p] collides with a newly allocated [%p:%p]\n", record->addr, record->addr + record->size, p, p + size);

    uint64_t pattern = _rng.nextUInt64();
    record = _heap.newT<Record>(pRX, pRW, size, pattern);
    EXPECT_NOT_NULL(record);

    {
      VirtMem::ProtectJitReadWriteScope scope(pRW, size);
      JitAllocatorUtils::fillPattern64(pRW, pattern, size);
    }

    VirtMem::flushInstructionCache(pRX, size);
    EXPECT_TRUE(JitAllocatorUtils::verifyPattern64(pRX, pattern, size));

    _records.insert(record);
  }

  void _remove(void* p) noexcept {
    Record* record = _records.get(static_cast<uint8_t*>(p));
    EXPECT_NOT_NULL(record);

    EXPECT_TRUE(JitAllocatorUtils::verifyPattern64(record->rx(), record->pattern, record->size));
    EXPECT_TRUE(JitAllocatorUtils::verifyPattern64(record->rw(), record->pattern, record->size));

    _records.remove(record);
    _heap.release(record, sizeof(Record));
  }

  void* alloc(size_t size) noexcept {
    JitAllocator::Span span;
    Error err = _allocator.alloc(span, size);
    EXPECT_EQ(err, kErrorOk)
      .message("JitAllocator failed to allocate %zu bytes\n", size);

    _insert(span.rx(), span.rw(), size);
    return span.rx();
  }

  void release(void* p) noexcept {
    _remove(p);
    EXPECT_EQ(_allocator.release(p), kErrorOk)
      .message("JitAllocator failed to release '%p'\n", p);
  }

  void shrink(void* p, size_t newSize) noexcept {
    Record* record = _records.get(static_cast<uint8_t*>(p));
    EXPECT_NOT_NULL(record);

    if (!newSize)
      return release(p);

    JitAllocator::Span span;
    EXPECT_EQ(_allocator.query(span, p), kErrorOk);
    Error err = _allocator.shrink(span, newSize);
    EXPECT_EQ(err, kErrorOk)
      .message("JitAllocator failed to shrink %p to %zu bytes\n", p, newSize);

    record->size = newSize;
  }
};

static void JitAllocatorTest_shuffle(void** ptrArray, size_t count, Random& prng) noexcept {
  for (size_t i = 0; i < count; ++i)
    std::swap(ptrArray[i], ptrArray[size_t(prng.nextUInt32() % count)]);
}

static void JitAllocatorTest_usage(JitAllocator& allocator) noexcept {
  JitAllocator::Statistics stats = allocator.statistics();
  INFO("    Block Count       : %9llu [Blocks]"        , (unsigned long long)(stats.blockCount()));
  INFO("    Reserved (VirtMem): %9llu [Bytes]"         , (unsigned long long)(stats.reservedSize()));
  INFO("    Used     (VirtMem): %9llu [Bytes] (%.1f%%)", (unsigned long long)(stats.usedSize()), stats.usedSizeAsPercent());
  INFO("    Overhead (HeapMem): %9llu [Bytes] (%.1f%%)", (unsigned long long)(stats.overheadSize()), stats.overheadSizeAsPercent());
}

template<typename T, size_t kPatternSize, bool Bit>
static void BitVectorRangeIterator_testRandom(Random& rnd, size_t count) noexcept {
  for (size_t i = 0; i < count; i++) {
    T in[kPatternSize];
    T out[kPatternSize];

    for (size_t j = 0; j < kPatternSize; j++) {
      in[j] = T(uint64_t(rnd.nextUInt32() & 0xFFu) * 0x0101010101010101);
      out[j] = Bit == 0 ? Support::allOnes<T>() : T(0);
    }

    {
      BitVectorRangeIterator<T, Bit> it(in, kPatternSize);
      size_t rangeStart, rangeEnd;
      while (it.nextRange(&rangeStart, &rangeEnd)) {
        if (Bit)
          Support::bitVectorFill(out, rangeStart, rangeEnd - rangeStart);
        else
          Support::bitVectorClear(out, rangeStart, rangeEnd - rangeStart);
      }
    }

    for (size_t j = 0; j < kPatternSize; j++) {
      EXPECT_EQ(in[j], out[j])
        .message("Invalid pattern detected at [%zu] (%llX != %llX)", j, (unsigned long long)in[j], (unsigned long long)out[j]);
    }
  }
}

static void test_jit_allocator_reset_empty() noexcept {
  JitAllocator allocator;
  allocator.reset(ResetPolicy::kSoft);
}

static void test_jit_allocator_alloc_release() noexcept {
  size_t kCount = BrokenAPI::hasArg("--quick") ? 20000 : 100000;

  struct TestParams {
    const char* name;
    JitAllocatorOptions options;
    uint32_t blockSize;
    uint32_t granularity;
  };

  using Opt = JitAllocatorOptions;

  VirtMem::HardenedRuntimeInfo hri = VirtMem::hardenedRuntimeInfo();

  TestParams testParams[] = {
    { "Default"                                    , Opt::kNone, 0, 0 },
    { "16MB blocks"                                , Opt::kNone, 16 * 1024 * 1024, 0 },
    { "256B granularity"                           , Opt::kNone, 0, 256 },
    { "kUseMultiplePools"                          , Opt::kUseMultiplePools, 0, 0 },
    { "kFillUnusedMemory"                          , Opt::kFillUnusedMemory, 0, 0 },
    { "kImmediateRelease"                          , Opt::kImmediateRelease, 0, 0 },
    { "kDisableInitialPadding"                     , Opt::kDisableInitialPadding, 0, 0 },
    { "kUseLargePages"                             , Opt::kUseLargePages, 0, 0 },
    { "kUseLargePages | kFillUnusedMemory"         , Opt::kUseLargePages | Opt::kFillUnusedMemory, 0, 0 },
    { "kUseLargePages | kAlignBlockSizeToLargePage", Opt::kUseLargePages | Opt::kAlignBlockSizeToLargePage, 0, 0 },
    { "kUseDualMapping"                            , Opt::kUseDualMapping , 0, 0 },
    { "kUseDualMapping | kFillUnusedMemory"        , Opt::kUseDualMapping | Opt::kFillUnusedMemory, 0, 0 }
  };

  INFO("BitVectorRangeIterator<uint32_t>");
  {
    Random rnd;
    BitVectorRangeIterator_testRandom<uint32_t, 64, 0>(rnd, kCount);
  }

  INFO("BitVectorRangeIterator<uint64_t>");
  {
    Random rnd;
    BitVectorRangeIterator_testRandom<uint64_t, 64, 0>(rnd, kCount);
  }

  for (uint32_t testId = 0; testId < ASMJIT_ARRAY_SIZE(testParams); testId++) {
    // Don't try to allocate dual-mapping if dual mapping is not possible - it would fail the test.
    if (Support::test(testParams[testId].options, JitAllocatorOptions::kUseDualMapping) &&
        !Support::test(hri.flags, VirtMem::HardenedRuntimeFlags::kDualMapping)) {
      continue;
    }

    INFO("JitAllocator(%s)", testParams[testId].name);

    JitAllocator::CreateParams params {};
    params.options = testParams[testId].options;
    params.blockSize = testParams[testId].blockSize;
    params.granularity = testParams[testId].granularity;

    size_t fixedBlockSize = 256;

    JitAllocatorWrapper wrapper(&params);
    Random prng(100);

    size_t i;

    INFO("  Memory alloc/release test - %d allocations", kCount);

    void** ptrArray = (void**)::malloc(sizeof(void*) * size_t(kCount));
    EXPECT_NOT_NULL(ptrArray);

    // Random blocks tests...
    INFO("  Allocating random blocks...");
    for (i = 0; i < kCount; i++)
      ptrArray[i] = wrapper.alloc((prng.nextUInt32() % 1024) + 8);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Releasing all allocated blocks from the beginning...");
    for (i = 0; i < kCount; i++)
      wrapper.release(ptrArray[i]);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Allocating random blocks again...", kCount);
    for (i = 0; i < kCount; i++)
      ptrArray[i] = wrapper.alloc((prng.nextUInt32() % 1024) + 8);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Shuffling allocated blocks...");
    JitAllocatorTest_shuffle(ptrArray, unsigned(kCount), prng);

    INFO("  Releasing 50%% of allocated blocks...");
    for (i = 0; i < kCount / 2; i++)
      wrapper.release(ptrArray[i]);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Allocating 50%% more blocks again...");
    for (i = 0; i < kCount / 2; i++)
      ptrArray[i] = wrapper.alloc((prng.nextUInt32() % 1024) + 8);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Releasing all allocated blocks from the end...");
    for (i = 0; i < kCount; i++)
      wrapper.release(ptrArray[kCount - i - 1]);
    JitAllocatorTest_usage(wrapper._allocator);

    // Fixed blocks tests...
    INFO("  Allocating %zuB blocks...", fixedBlockSize);
    for (i = 0; i < kCount / 2; i++)
      ptrArray[i] = wrapper.alloc(fixedBlockSize);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Shrinking each %zuB block to 1 byte", fixedBlockSize);
    for (i = 0; i < kCount / 2; i++)
      wrapper.shrink(ptrArray[i], 1);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Allocating more 64B blocks...", 64);
    for (i = kCount / 2; i < kCount; i++)
      ptrArray[i] = wrapper.alloc(64);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Releasing all blocks from the beginning...");
    for (i = 0; i < kCount; i++)
      wrapper.release(ptrArray[i]);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Allocating %zuB blocks...", fixedBlockSize);
    for (i = 0; i < kCount; i++)
      ptrArray[i] = wrapper.alloc(fixedBlockSize);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Shuffling allocated blocks...");
    JitAllocatorTest_shuffle(ptrArray, unsigned(kCount), prng);

    INFO("  Releasing 50%% of allocated blocks...");
    for (i = 0; i < kCount / 2; i++)
      wrapper.release(ptrArray[i]);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Allocating 50%% more %zuB blocks again...", fixedBlockSize);
    for (i = 0; i < kCount / 2; i++)
      ptrArray[i] = wrapper.alloc(fixedBlockSize);
    JitAllocatorTest_usage(wrapper._allocator);

    INFO("  Releasing all allocated blocks from the end...");
    for (i = 0; i < kCount; i++)
      wrapper.release(ptrArray[kCount - i - 1]);
    JitAllocatorTest_usage(wrapper._allocator);

    ::free(ptrArray);
  }
}

static void test_jit_allocator_query() noexcept {
  JitAllocator allocator;
  size_t allocatedSize = 100;

  JitAllocator::Span allocatedSpan;
  EXPECT_EQ(allocator.alloc(allocatedSpan, allocatedSize), kErrorOk);
  EXPECT_NOT_NULL(allocatedSpan.rx());
  EXPECT_GE(allocatedSpan.size(), allocatedSize);

  JitAllocator::Span queriedSpan;
  EXPECT_EQ(allocator.query(queriedSpan, allocatedSpan.rx()), kErrorOk);
  EXPECT_EQ(allocatedSpan.rx(), queriedSpan.rx());
  EXPECT_EQ(allocatedSpan.rw(), queriedSpan.rw());
  EXPECT_EQ(allocatedSpan.size(), queriedSpan.size());
}

UNIT(jit_allocator) {
  test_jit_allocator_reset_empty();
  test_jit_allocator_alloc_release();
  test_jit_allocator_query();
}
#endif // ASMJIT_TEST

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_JIT

```

`CodeCleaner/asmjit/asmjit/core/jitallocator.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_JITALLOCATOR_H_INCLUDED
#define ASMJIT_CORE_JITALLOCATOR_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_JIT

#include "../core/globals.h"
#include "../core/support.h"
#include "../core/virtmem.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_virtual_memory
//! \{

//! Options used by \ref JitAllocator.
enum class JitAllocatorOptions : uint32_t {
  //! No options.
  kNone = 0,

  //! Enables the use of an anonymous memory-mapped memory that is mapped into two buffers having a different pointer.
  //! The first buffer has read and execute permissions and the second buffer has read+write permissions.
  //!
  //! See \ref VirtMem::allocDualMapping() for more details about this feature.
  //!
  //! \remarks Dual mapping would be automatically turned on by \ref JitAllocator in case of hardened runtime that
  //! enforces `W^X` policy, so specifying this flag is essentially forcing to use dual mapped pages even when RWX
  //! pages can be allocated and dual mapping is not necessary.
  kUseDualMapping = 0x00000001u,

  //! Enables the use of multiple pools with increasing granularity instead of a single pool. This flag would enable
  //! 3 internal pools in total having 64, 128, and 256 bytes granularity.
  //!
  //! This feature is only recommended for users that generate a lot of code and would like to minimize the overhead
  //! of `JitAllocator` itself by having blocks of different allocation granularities. Using this feature only for
  //! few allocations won't pay off as the allocator may need to create more blocks initially before it can take the
  //! advantage of variable block granularity.
  kUseMultiplePools = 0x00000002u,

  //! Always fill reserved memory by a fill-pattern.
  //!
  //! Causes a new block to be cleared by the fill pattern and freshly released memory to be cleared before making
  //! it ready for another use.
  kFillUnusedMemory = 0x00000004u,

  //! When this flag is set the allocator would immediately release unused blocks during `release()` or `reset()`.
  //! When this flag is not set the allocator would keep one empty block in each pool to prevent excessive virtual
  //! memory allocations and deallocations in border cases, which involve constantly allocating and deallocating a
  //! single block caused by repetitive calling `alloc()` and `release()` when the allocator has either no blocks
  //! or have all blocks fully occupied.
  kImmediateRelease = 0x00000008u,

  //! This flag enables placing functions (or allocating memory) at the very beginning of each memory mapped region.
  //!
  //! Initially, this was the default behavior. However, LLVM developers working on undefined behavior sanitizer
  //! (UBSAN) decided that they want to store metadata before each function and to access such metadata before an
  //! indirect function call. This means that the instrumented code always reads from `[fnPtr - 8]` to decode whether
  //! the function has his metadata present. However, reading 8 bytes below a function means that if a function is
  //! placed at the very beginning of a memory mapped region, it could try to read bytes that are inaccessible. And
  //! since AsmJit can be compiled as a shared library and used by applications instrumented by UBSAN, it's not
  //! possible to conditionally compile the support only when necessary.
  //!
  //! \remarks This flag controls a workaround to make it possible to use LLVM UBSAN with AsmJit's \ref JitAllocator.
  //! There is no undefined behavior even when `kDisableInitialPadding` is used, however, that doesn't really matter
  //! as LLVM's UBSAN introduces one, and according to LLVM developers it's a "trade-off". This flag is safe to use
  //! when the code is not instrumented with LLVM's UBSAN.
  kDisableInitialPadding = 0x00000010u,

  //! Enables the use of large pages, if they are supported and the process can actually allocate them.
  //!
  //! \remarks This flag is a hint - if large pages can be allocated, JitAllocator would try to allocate them.
  //! However, if the allocation fails, it will still try to fallback to use regular pages as \ref JitAllocator
  //! is designed to minimize allocation failures, so a regular page is better than no page at all. Also, if a
  //! block \ref JitAllocator wants to allocate is too small to consume a whole large page, regular page(s) will
  //! be allocated as well.
  kUseLargePages = 0x00000020u,

  //! Forces \ref JitAllocator to always align block size to be at least as big as a large page, if large pages are
  //! enabled. This option does nothing if large pages are disabled.
  //!
  //! \remarks If \ref kUseLargePages option is used, the allocator would prefer large pages only when allocating a
  //! block that has a sufficient size. Usually the allocator first allocates smaller block and when more requests
  //! come it will start increasing the block size of next allocations. This option makes it sure that even the first
  //! allocation would be the same as a minimum large page when large pages are enabled and can be allocated.
  kAlignBlockSizeToLargePage = 0x00000040u,

  //! Use a custom fill pattern, must be combined with `kFlagFillUnusedMemory`.
  kCustomFillPattern = 0x10000000u
};
ASMJIT_DEFINE_ENUM_FLAGS(JitAllocatorOptions)

//! A simple implementation of memory manager that uses `asmjit::VirtMem`
//! functions to manage virtual memory for JIT compiled code.
//!
//! Implementation notes:
//!
//! - Granularity of allocated blocks is different than granularity for a typical C malloc. In addition, the allocator
//!   can use several memory pools having a different granularity to minimize the maintenance overhead. Multiple pools
//!   feature requires `kFlagUseMultiplePools` flag to be set.
//!
//! - The allocator doesn't store any information in executable memory, instead, the implementation uses two
//!   bit-vectors to manage allocated memory of each allocator-block. The first bit-vector called 'used' is used to
//!   track used memory (where each bit represents memory size defined by granularity) and the second bit vector called
//!   'stop' is used as a sentinel to mark where the allocated area ends.
//!
//! - Internally, the allocator also uses RB tree to keep track of all blocks across all pools. Each inserted block is
//!   added to the tree so it can be matched fast during `release()` and `shrink()`.
class JitAllocator {
public:
  ASMJIT_NONCOPYABLE(JitAllocator)

  //! Visible \ref JitAllocator implementation data.
  struct Impl {
    //! Allocator options.
    JitAllocatorOptions options;
    //! Base block size (0 if the allocator is not initialized).
    uint32_t blockSize;
    //! Base granularity (0 if the allocator is not initialized).
    uint32_t granularity;
    //! A pattern that is used to fill unused memory if secure mode is enabled.
    uint32_t fillPattern;
  };

  //! \name Members
  //! \{

  //! Allocator implementation (private).
  Impl* _impl;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Parameters that can be passed to `JitAllocator` constructor.
  //!
  //! Use it like this:
  //!
  //! ```
  //! // Zero initialize (zero means the default value) and change what you need.
  //! JitAllocator::CreateParams params {};
  //! params.blockSize = 1024 * 1024;
  //!
  //! // Create the allocator.
  //! JitAllocator allocator(&params);
  //! ```
  struct CreateParams {
    //! Allocator options.
    //!
    //! No options are used by default.
    JitAllocatorOptions options = JitAllocatorOptions::kNone;

    //! Base size of a single block in bytes (default 64kB).
    //!
    //! \remarks Block size must be equal to or greater than page size and must be power of 2. If the input is not
    //! valid then the default block size will be used instead.
    uint32_t blockSize = 0;

    //! Base granularity (and also natural alignment) of allocations in bytes (default 64).
    //!
    //! Since the `JitAllocator` uses bit-arrays to mark used memory the granularity also specifies how many bytes
    //! correspond to a single bit in such bit-array. Higher granularity means more waste of virtual memory (as it
    //! increases the natural alignment), but smaller bit-arrays as less bits would be required per a single block.
    uint32_t granularity = 0;

    //! Patter to use to fill unused memory.
    //!
    //! Only used if \ref JitAllocatorOptions::kCustomFillPattern is set.
    uint32_t fillPattern = 0;

    // Reset the content of `CreateParams`.
    ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = CreateParams{}; }
  };

  //! Creates a `JitAllocator` instance.
  ASMJIT_API explicit JitAllocator(const CreateParams* params = nullptr) noexcept;
  //! Destroys the `JitAllocator` instance and release all blocks held.
  ASMJIT_API ~JitAllocator() noexcept;

  ASMJIT_INLINE_NODEBUG bool isInitialized() const noexcept { return _impl->blockSize == 0; }

  //! Free all allocated memory - makes all pointers returned by `alloc()` invalid.
  //!
  //! \remarks This function is not thread-safe as it's designed to be used when nobody else is using allocator.
  //! The reason is that there is no point of calling `reset()` when the allocator is still in use.
  ASMJIT_API void reset(ResetPolicy resetPolicy = ResetPolicy::kSoft) noexcept;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns allocator options, see `Flags`.
  ASMJIT_INLINE_NODEBUG JitAllocatorOptions options() const noexcept { return _impl->options; }
  //! Tests whether the allocator has the given `option` set.
  ASMJIT_INLINE_NODEBUG bool hasOption(JitAllocatorOptions option) const noexcept { return uint32_t(_impl->options & option) != 0; }

  //! Returns a base block size (a minimum size of block that the allocator would allocate).
  ASMJIT_INLINE_NODEBUG uint32_t blockSize() const noexcept { return _impl->blockSize; }
  //! Returns granularity of the allocator.
  ASMJIT_INLINE_NODEBUG uint32_t granularity() const noexcept { return _impl->granularity; }
  //! Returns pattern that is used to fill unused memory if `kFlagUseFillPattern` is set.
  ASMJIT_INLINE_NODEBUG uint32_t fillPattern() const noexcept { return _impl->fillPattern; }

  //! \}

  //! \name Alloc & Release
  //! \{

  //! A memory reference returned by \ref JitAllocator::alloc().
  //!
  //! Span contains everything needed to actually write new code to the memory chunk it references.
  class Span {
  public:
    //! \name Constants
    //! \{

    //! Span flags
    enum class Flags : uint32_t {
      //! No flags.
      kNone = 0u,

      //! The process has never executed the region of the span.
      //!
      //! If this flag is set on a \ref Span it would mean that the allocator can avoid flushing
      //! instruction cache after a code has been written to it.
      kInstructionCacheClean = 0x00000001u
    };

    //! \}

    //! \name Members
    //! \{

    //! Address of memory that has Read and Execute permissions.
    void* _rx = nullptr;

    //! Address of memory that has Read and Write permissions.
    void* _rw = nullptr;

    //! Size of the span in bytes (rounded up to the allocation granularity).
    size_t _size = 0;

    //! Pointer that references a memory block maintained by \ref JitAllocator.
    //!
    //! This pointer is considered private and should never be used nor inspected outside of AsmJit.
    void* _block = nullptr;

    //! Span flags.
    Flags _flags = Flags::kNone;

    //! Reserved for future use.
    uint32_t _reserved = 0;

    //! \}

    //! \name Accessors
    //! \{

    //! Returns a pointer having Read & Execute permissions (references executable memory).
    //!
    //! This pointer is never NULL if the allocation succeeded, it points to an executable memory.
    ASMJIT_INLINE_NODEBUG void* rx() const noexcept { return _rx; }

    //! Returns a pointer having Read & Write permissions (references writable memory).
    //!
    //! Depending on the type of the allocation strategy this could either be:
    //!
    //!   - the same address as returned by `rx()` if the allocator uses RWX mapping (pages have all of Read, Write,
    //!     and Execute permissions) or MAP_JIT, which requires either \ref VirtMem::ProtectJitReadWriteScope or to
    //!     call \ref VirtMem::protectJitMemory() manually.
    //!   - a valid pointer, but not the same as `rx` - this would be valid if dual mapping is used.
    //!   - NULL pointer, in case that the allocation strategy doesn't use RWX, MAP_JIT, or dual mapping. In this
    //!     case only \ref JitAllocator can copy new code into the executable memory referenced by \ref Span.
    //!
    //! \note If `rw()` returns a non-null pointer it's important to use either VirtMem::protectJitMemory() or
    //! \ref VirtMem::ProtectJitReadWriteScope to guard the write, because in case of `MAP_JIT` it would temporarily
    //! switch the permissions of the pointer to RW (that's per thread permissions).
    //!
    //! If \ref VirtMem::ProtectJitReadWriteScope is not used it's important to clear the instruction cache via
    //! \ref VirtMem::flushInstructionCache() after the write is done.
    ASMJIT_INLINE_NODEBUG void* rw() const noexcept { return _rw; }

    //! Returns size of this span, aligned to the allocator granularity.
    ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return _size; }

    //! Returns span flags.
    ASMJIT_INLINE_NODEBUG Flags flags() const noexcept { return _flags; }

    //! Shrinks this span to `newSize`.
    //!
    //! \note This is the only function that is able to change the size of a span, and it's only use case is to
    //! shrink the span size during \ref JitAllocator::write(). When the writer detects that the span size shrunk,
    //! it will automatically shrink the memory used by the span, and propagate the new aligned size to the caller.
    ASMJIT_INLINE_NODEBUG void shrink(size_t newSize) noexcept { _size = Support::min(_size, newSize); }

    //! Returns whether \ref rw() returns a non-null pointer.
    ASMJIT_INLINE_NODEBUG bool isDirectlyWritable() const noexcept { return _rw != nullptr; }

    //! \}
  };

  //! Allocates a new memory span of the requested `size`.
  ASMJIT_API Error alloc(Span& out, size_t size) noexcept;

  //! Releases a memory block returned by `alloc()`.
  //!
  //! \remarks This function is thread-safe.
  ASMJIT_API Error release(void* rx) noexcept;

  //! Frees extra memory allocated with `rx` by shrinking it to the given `newSize`.
  //!
  //! \remarks This function is thread-safe.
  ASMJIT_API Error shrink(Span& span, size_t newSize) noexcept;

  //! Queries information about an allocated memory block that contains the given `rx`, and writes it to `out`.
  //!
  //! If the pointer is matched, the function returns `kErrorOk` and fills `out` with the corresponding span.
  ASMJIT_API Error query(Span& out, void* rx) const noexcept;

  //! \}

  //! \name Write Operations
  //! \{

  typedef Error (ASMJIT_CDECL* WriteFunc)(Span& span, void* userData) ASMJIT_NOEXCEPT_TYPE;

  ASMJIT_API Error write(
    Span& span,
    size_t offset,
    const void* src,
    size_t size,
    VirtMem::CachePolicy policy = VirtMem::CachePolicy::kDefault) noexcept;

  ASMJIT_API Error write(
    Span& span,
    WriteFunc writeFunc,
    void* userData,
    VirtMem::CachePolicy policy = VirtMem::CachePolicy::kDefault) noexcept;

  template<class Lambda>
  ASMJIT_FORCE_INLINE Error write(
    Span& span,
    Lambda&& lambdaFunc,
    VirtMem::CachePolicy policy = VirtMem::CachePolicy::kDefault) noexcept {

    WriteFunc wrapperFunc = [](Span& span, void* userData) noexcept -> Error {
      Lambda& lambdaFunc = *static_cast<Lambda*>(userData);
      return lambdaFunc(span);
    };
    return write(span, wrapperFunc, (void*)(&lambdaFunc), policy);
  }

  //! \}

  //! \name Write Operations with Scope
  //! \{

  //! \cond INTERNAL

  //! Write scope data.
  //!
  //! This is mostly for internal purposes, please use \ref WriteScope instead.
  struct WriteScopeData {
    //! \name Members
    //! \{

    //! Link to the allocator.
    JitAllocator* _allocator;
    //! Cache policy passed to \ref JitAllocator::beginWriteScope().
    VirtMem::CachePolicy _policy;
    //! Internal flags used by the implementation.
    uint32_t _flags;
    //! Internal data used by the implementation.
    size_t _data[64];

    //! \}
  };

  //! Begins a write `scope`.
  //!
  //! This is mostly for internal purposes, please use \ref WriteScope constructor instead.
  ASMJIT_API Error beginWriteScope(WriteScopeData& scope, VirtMem::CachePolicy policy = VirtMem::CachePolicy::kDefault) noexcept;

  //! Ends a write `scope`.
  //!
  //! This is mostly for internal purposes, please use \ref WriteScope destructor instead.
  ASMJIT_API Error endWriteScope(WriteScopeData& scope) noexcept;

  //! Flushes accumulated changes in a write `scope`.
  //!
  //! This is mostly for internal purposes, please use \ref WriteScope destructor or \ref WriteScope::flush() instead.
  ASMJIT_API Error flushWriteScope(WriteScopeData& scope) noexcept;

  //! Alternative to `JitAllocator::write(span, offset, src, size)`, but under a write `scope`.
  //!
  //! This is mostly for internal purposes, please use \ref WriteScope::write() instead.
  ASMJIT_API Error scopedWrite(WriteScopeData& scope, Span& span, size_t offset, const void* src, size_t size) noexcept;

  //! Alternative to `JitAllocator::write(span, writeFunc, userData)`, but under a write `scope`.
  //!
  //! This is mostly for internal purposes, please use \ref WriteScope::write() instead.
  ASMJIT_API Error scopedWrite(WriteScopeData& scope, Span& span, WriteFunc writeFunc, void* userData) noexcept;

  //! Alternative to `JitAllocator::write(span, [lambda])`, but under a write `scope`.
  //!
  //! This is mostly for internal purposes, please use \ref WriteScope::write() instead.
  template<class Lambda>
  inline Error scopedWrite(WriteScopeData& scope, Span& span, Lambda&& lambdaFunc) noexcept {
    WriteFunc wrapperFunc = [](Span& span, void* userData) noexcept -> Error {
      Lambda& lambdaFunc = *static_cast<Lambda*>(userData);
      return lambdaFunc(span);
    };
    return scopedWrite(scope, span, wrapperFunc, (void*)(&lambdaFunc));
  }

  //! \endcond

  //! Write scope can be used to create a single scope that is optimized for writing multiple spans.
  class WriteScope : public WriteScopeData {
  public:
    ASMJIT_NONCOPYABLE(WriteScope)

    //! \name Construction & Destruction
    //! \{

    // Begins a write scope.
    inline explicit WriteScope(JitAllocator* allocator, VirtMem::CachePolicy policy = VirtMem::CachePolicy::kDefault) noexcept {
      allocator->beginWriteScope(*this, policy);
    }

    // Ends a write scope.
    inline ~WriteScope() noexcept {
      if (_allocator)
        _allocator->endWriteScope(*this);
    }

    //! \}

    //! \name Accessors
    //! \{

    ASMJIT_INLINE_NODEBUG JitAllocator* allocator() const noexcept { return _allocator; }
    ASMJIT_INLINE_NODEBUG VirtMem::CachePolicy policy() const noexcept { return _policy; }

    //! \}

    //! \name Operations
    //! \{

    //! Similar to `JitAllocator::write(span, offset, src, size)`, but under a write scope.
    ASMJIT_INLINE_NODEBUG Error write(Span& span, size_t offset, const void* src, size_t size) noexcept {
      return _allocator->scopedWrite(*this, span, offset, src, size);
    }

    //! Similar to `JitAllocator::write(span, writeFunc, userData)`, but under a write scope.
    ASMJIT_INLINE_NODEBUG Error write(Span& span, WriteFunc writeFunc, void* userData) noexcept {
      return _allocator->scopedWrite(*this, span, writeFunc, userData);
    }

    //! Similar to `JitAllocator::write(span, <lambda>)`, but under a write scope.
    template<class Lambda>
    ASMJIT_INLINE_NODEBUG Error write(Span& span, Lambda&& lambdaFunc) noexcept {
      return _allocator->scopedWrite(*this, span, lambdaFunc);
    }

    //! Flushes accumulated changes in this write scope.
    ASMJIT_INLINE_NODEBUG Error flush() noexcept {
      return _allocator->flushWriteScope(*this);
    }

    //! \}
  };

  //! \}

  //! \name Statistics
  //! \{

  //! Statistics about `JitAllocator`.
  struct Statistics {
    //! Number of blocks `JitAllocator` maintains.
    size_t _blockCount;
    //! Number of active allocations.
    size_t _allocationCount;
    //! How many bytes are currently used / allocated.
    size_t _usedSize;
    //! How many bytes are currently reserved by the allocator.
    size_t _reservedSize;
    //! Allocation overhead (in bytes) required to maintain all blocks.
    size_t _overheadSize;

    //! Resets the statistics to all zeros.
    ASMJIT_INLINE_NODEBUG void reset() noexcept { *this = Statistics{}; }

    //! Returns count of blocks managed by `JitAllocator` at the moment.
    ASMJIT_INLINE_NODEBUG size_t blockCount() const noexcept { return _blockCount; }
    //! Returns the number of active allocations.
    ASMJIT_INLINE_NODEBUG size_t allocationCount() const noexcept { return _allocationCount; }

    //! Returns how many bytes are currently used.
    ASMJIT_INLINE_NODEBUG size_t usedSize() const noexcept { return _usedSize; }
    //! Returns the number of bytes unused by the allocator at the moment.
    ASMJIT_INLINE_NODEBUG size_t unusedSize() const noexcept { return _reservedSize - _usedSize; }
    //! Returns the total number of bytes reserved by the allocator (sum of sizes of all blocks).
    ASMJIT_INLINE_NODEBUG size_t reservedSize() const noexcept { return _reservedSize; }
    //! Returns the number of bytes the allocator needs to manage the allocated memory.
    ASMJIT_INLINE_NODEBUG size_t overheadSize() const noexcept { return _overheadSize; }

    ASMJIT_INLINE_NODEBUG double usedSizeAsPercent() const noexcept {
      return (double(usedSize()) / (double(reservedSize()) + 1e-16)) * 100.0;
    }

    ASMJIT_INLINE_NODEBUG double unusedSizeAsPercent() const noexcept {
      return (double(unusedSize()) / (double(reservedSize()) + 1e-16)) * 100.0;
    }

    ASMJIT_INLINE_NODEBUG double overheadSizeAsPercent() const noexcept {
      return (double(overheadSize()) / (double(reservedSize()) + 1e-16)) * 100.0;
    }
  };

  //! Returns JIT allocator statistics.
  //!
  //! \remarks This function is thread-safe.
  ASMJIT_API Statistics statistics() const noexcept;

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif
#endif

```

`CodeCleaner/asmjit/asmjit/core/jitruntime.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_JIT

#include "../core/cpuinfo.h"
#include "../core/jitruntime.h"

ASMJIT_BEGIN_NAMESPACE

JitRuntime::JitRuntime(const JitAllocator::CreateParams* params) noexcept
  : _allocator(params) {
  _environment = Environment::host();
  _environment.setObjectFormat(ObjectFormat::kJIT);
  _cpuFeatures = CpuInfo::host().features();
}

JitRuntime::~JitRuntime() noexcept {}

Error JitRuntime::_add(void** dst, CodeHolder* code) noexcept {
  *dst = nullptr;

  ASMJIT_PROPAGATE(code->flatten());
  ASMJIT_PROPAGATE(code->resolveUnresolvedLinks());

  size_t estimatedCodeSize = code->codeSize();
  if (ASMJIT_UNLIKELY(estimatedCodeSize == 0))
    return DebugUtils::errored(kErrorNoCodeGenerated);

  JitAllocator::Span span;
  ASMJIT_PROPAGATE(_allocator.alloc(span, estimatedCodeSize));

  // Relocate the code.
  Error err = code->relocateToBase(uintptr_t(span.rx()));
  if (ASMJIT_UNLIKELY(err)) {
    _allocator.release(span.rx());
    return err;
  }

  // Recalculate the final code size and shrink the memory we allocated for it
  // in case that some relocations didn't require records in an address table.
  size_t codeSize = code->codeSize();
  ASMJIT_ASSERT(codeSize <= estimatedCodeSize);

  _allocator.write(span, [&](JitAllocator::Span& span) noexcept -> Error {
    uint8_t* rw = static_cast<uint8_t*>(span.rw());

    for (Section* section : code->_sections) {
      size_t offset = size_t(section->offset());
      size_t bufferSize = size_t(section->bufferSize());
      size_t virtualSize = size_t(section->virtualSize());

      ASMJIT_ASSERT(offset + bufferSize <= span.size());
      memcpy(rw + offset, section->data(), bufferSize);

      if (virtualSize > bufferSize) {
        ASMJIT_ASSERT(offset + virtualSize <= span.size());
        memset(rw + offset + bufferSize, 0, virtualSize - bufferSize);
      }
    }

    span.shrink(codeSize);
    return kErrorOk;
  });

  *dst = span.rx();
  return kErrorOk;
}

Error JitRuntime::_release(void* p) noexcept {
  return _allocator.release(p);
}

ASMJIT_END_NAMESPACE

#endif

```

`CodeCleaner/asmjit/asmjit/core/jitruntime.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_JITRUNTIME_H_INCLUDED
#define ASMJIT_CORE_JITRUNTIME_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_JIT

#include "../core/codeholder.h"
#include "../core/jitallocator.h"
#include "../core/target.h"

ASMJIT_BEGIN_NAMESPACE

class CodeHolder;

//! \addtogroup asmjit_virtual_memory
//! \{

//! JIT execution runtime is a special `Target` that is designed to store and execute a generated code.
//!
//! JIT runtime is the easiest way of using AsmJit as it abstracts allocation and deallocation of virtual memory
//! where executable code can be placed and from which it can be executed as well.
class ASMJIT_VIRTAPI JitRuntime : public Target {
public:
  ASMJIT_NONCOPYABLE(JitRuntime)

  //! Virtual memory allocator.
  JitAllocator _allocator;

  //! \name Construction & Destruction
  //! \{

  //! Creates a `JitRuntime` instance.
  ASMJIT_API explicit JitRuntime(const JitAllocator::CreateParams* params = nullptr) noexcept;
  //! Destroys the `JitRuntime` instance.
  ASMJIT_API ~JitRuntime() noexcept override;

  //! \}

  //! \name Accessors
  //! \{

  //! Resets the \ref JitRuntime, freeing everything that was allocated by it.
  //!
  //! Depending on `resetPolicy` the currently held memory can be either freed entirely when ResetPolicy::kHard is used,
  //! or the allocator can keep some of it for next allocations when ResetPolicy::kSoft is used, which is the default
  //! behavior.
  ASMJIT_INLINE_NODEBUG void reset(ResetPolicy resetPolicy = ResetPolicy::kSoft) noexcept {
    _allocator.reset(resetPolicy);
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the associated `JitAllocator`.
  ASMJIT_INLINE_NODEBUG JitAllocator* allocator() const noexcept { return const_cast<JitAllocator*>(&_allocator); }

  //! \}

  //! \name Utilities
  //! \{

  // NOTE: To allow passing function pointers to `add()` and `release()` the
  // virtual methods are prefixed with `_` and called from templates instead.

  //! Allocates memory needed for a code stored in the `CodeHolder` and relocates the code to the pointer allocated.
  //!
  //! The beginning of the memory allocated for the function is returned in `dst`. If failed `Error` code is returned
  //! and `dst` is explicitly set to `nullptr`  (this means that you don't have to set it to null before calling `add()`).
  template<typename Func>
  ASMJIT_INLINE_NODEBUG Error add(Func* dst, CodeHolder* code) noexcept {
    return _add(Support::ptr_cast_impl<void**, Func*>(dst), code);
  }

  //! Releases `p` which was obtained by calling `add()`.
  template<typename Func>
  ASMJIT_INLINE_NODEBUG Error release(Func p) noexcept {
    return _release(Support::ptr_cast_impl<void*, Func>(p));
  }

  //! Type-unsafe version of `add()`.
  ASMJIT_API virtual Error _add(void** dst, CodeHolder* code) noexcept;

  //! Type-unsafe version of `release()`.
  ASMJIT_API virtual Error _release(void* p) noexcept;

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif
#endif

```

`CodeCleaner/asmjit/asmjit/core/logger.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_LOGGING

#include "../core/logger.h"
#include "../core/string.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// Logger - Implementation
// =======================

Logger::Logger() noexcept
  : _options() {}
Logger::~Logger() noexcept {}

// [[pure virtual]]
Error Logger::_log(const char* data, size_t size) noexcept {
  DebugUtils::unused(data, size);

  // Do not error in this case - the logger would just sink to /dev/null.
  return kErrorOk;
}

Error Logger::logf(const char* fmt, ...) noexcept {
  Error err;
  va_list ap;

  va_start(ap, fmt);
  err = logv(fmt, ap);
  va_end(ap);

  return err;
}

Error Logger::logv(const char* fmt, va_list ap) noexcept {
  StringTmp<2048> sb;
  ASMJIT_PROPAGATE(sb.appendVFormat(fmt, ap));
  return log(sb);
}

// FileLogger - Implementation
// ===========================

FileLogger::FileLogger(FILE* file) noexcept
  : _file(file) {}
FileLogger::~FileLogger() noexcept {}

Error FileLogger::_log(const char* data, size_t size) noexcept {
  if (!_file)
    return kErrorOk;

  if (size == SIZE_MAX)
    size = strlen(data);

  fwrite(data, 1, size, _file);
  return kErrorOk;
}

// StringLogger - Implementation
// =============================

StringLogger::StringLogger() noexcept {}
StringLogger::~StringLogger() noexcept {}

Error StringLogger::_log(const char* data, size_t size) noexcept {
  return _content.append(data, size);
}

ASMJIT_END_NAMESPACE

#endif

```

`CodeCleaner/asmjit/asmjit/core/logger.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_LOGGING_H_INCLUDED
#define ASMJIT_CORE_LOGGING_H_INCLUDED

#include "../core/inst.h"
#include "../core/string.h"
#include "../core/formatter.h"

#ifndef ASMJIT_NO_LOGGING

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_logging
//! \{

//! Logging interface.
//!
//! This class can be inherited and reimplemented to fit into your own logging needs. When reimplementing a logger
//! use \ref Logger::_log() method to log customize the output.
//!
//! There are two `Logger` implementations offered by AsmJit:
//!   - \ref FileLogger - logs into a `FILE*`.
//!   - \ref StringLogger - concatenates all logs into a \ref String.
class ASMJIT_VIRTAPI Logger {
public:
  ASMJIT_BASE_CLASS(Logger)
  ASMJIT_NONCOPYABLE(Logger)

  //! Format options.
  FormatOptions _options;

  //! \name Construction & Destruction
  //! \{

  //! Creates a `Logger` instance.
  ASMJIT_API Logger() noexcept;
  //! Destroys the `Logger` instance.
  ASMJIT_API virtual ~Logger() noexcept;

  //! \}

  //! \name Format Options
  //! \{

  //! Returns \ref FormatOptions of this logger.
  ASMJIT_INLINE_NODEBUG FormatOptions& options() noexcept { return _options; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const FormatOptions& options() const noexcept { return _options; }
  //! Sets formatting options of this Logger to `options`.
  ASMJIT_INLINE_NODEBUG void setOptions(const FormatOptions& options) noexcept { _options = options; }
  //! Resets formatting options of this Logger to defaults.
  ASMJIT_INLINE_NODEBUG void resetOptions() noexcept { _options.reset(); }

  //! Returns formatting flags.
  ASMJIT_INLINE_NODEBUG FormatFlags flags() const noexcept { return _options.flags(); }
  //! Tests whether the logger has the given `flag` enabled.
  ASMJIT_INLINE_NODEBUG bool hasFlag(FormatFlags flag) const noexcept { return _options.hasFlag(flag); }
  //! Sets formatting flags to `flags`.
  ASMJIT_INLINE_NODEBUG void setFlags(FormatFlags flags) noexcept { _options.setFlags(flags); }
  //! Enables the given formatting `flags`.
  ASMJIT_INLINE_NODEBUG void addFlags(FormatFlags flags) noexcept { _options.addFlags(flags); }
  //! Disables the given formatting `flags`.
  ASMJIT_INLINE_NODEBUG void clearFlags(FormatFlags flags) noexcept { _options.clearFlags(flags); }

  //! Returns indentation of a given indentation `group`.
  ASMJIT_INLINE_NODEBUG uint32_t indentation(FormatIndentationGroup type) const noexcept { return _options.indentation(type); }
  //! Sets indentation of the given indentation `group` to `n` spaces.
  ASMJIT_INLINE_NODEBUG void setIndentation(FormatIndentationGroup type, uint32_t n) noexcept { _options.setIndentation(type, n); }
  //! Resets indentation of the given indentation `group` to 0 spaces.
  ASMJIT_INLINE_NODEBUG void resetIndentation(FormatIndentationGroup type) noexcept { _options.resetIndentation(type); }

  //! Returns padding of a given padding `group`.
  ASMJIT_INLINE_NODEBUG size_t padding(FormatPaddingGroup type) const noexcept { return _options.padding(type); }
  //! Sets padding of a given padding `group` to `n`.
  ASMJIT_INLINE_NODEBUG void setPadding(FormatPaddingGroup type, uint32_t n) noexcept { _options.setPadding(type, n); }
  //! Resets padding of a given padding `group` to 0, which means that a default will be used.
  ASMJIT_INLINE_NODEBUG void resetPadding(FormatPaddingGroup type) noexcept { _options.resetPadding(type); }

  //! \}

  //! \name Logging Interface
  //! \{

  //! Logs `str` - must be reimplemented.
  //!
  //! The function can accept either a null terminated string if `size` is `SIZE_MAX` or a non-null terminated
  //! string of the given `size`. The function cannot assume that the data is null terminated and must handle
  //! non-null terminated inputs.
  ASMJIT_API virtual Error _log(const char* data, size_t size) noexcept;

  //! Logs string `str`, which is either null terminated or having size `size`.
  ASMJIT_INLINE_NODEBUG Error log(const char* data, size_t size = SIZE_MAX) noexcept { return _log(data, size); }
  //! Logs content of a string `str`.
  ASMJIT_INLINE_NODEBUG Error log(const String& str) noexcept { return _log(str.data(), str.size()); }

  //! Formats the message by using `snprintf()` and then passes the formatted string to \ref _log().
  ASMJIT_API Error logf(const char* fmt, ...) noexcept;

  //! Formats the message by using `vsnprintf()` and then passes the formatted string to \ref _log().
  ASMJIT_API Error logv(const char* fmt, va_list ap) noexcept;

  //! \}
};

//! Logger that can log to a `FILE*`.
class ASMJIT_VIRTAPI FileLogger : public Logger {
public:
  ASMJIT_NONCOPYABLE(FileLogger)

  FILE* _file;

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `FileLogger` that logs to `FILE*`.
  ASMJIT_API FileLogger(FILE* file = nullptr) noexcept;
  //! Destroys the `FileLogger`.
  ASMJIT_API ~FileLogger() noexcept override;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the logging output stream or null if the logger has no output stream.
  ASMJIT_INLINE_NODEBUG FILE* file() const noexcept { return _file; }

  //! Sets the logging output stream to `stream` or null.
  //!
  //! \note If the `file` is null the logging will be disabled. When a logger is attached to `CodeHolder` or any
  //! emitter the logging API will always be called regardless of the output file. This means that if you really
  //! want to disable logging at emitter level you must not attach a logger to it.
  ASMJIT_INLINE_NODEBUG void setFile(FILE* file) noexcept { _file = file; }

  //! \}

  ASMJIT_API Error _log(const char* data, size_t size = SIZE_MAX) noexcept override;
};

//! Logger that stores everything in an internal string buffer.
class ASMJIT_VIRTAPI StringLogger : public Logger {
public:
  ASMJIT_NONCOPYABLE(StringLogger)

  //! Logger data as string.
  String _content;

  //! \name Construction & Destruction
  //! \{

  //! Create new `StringLogger`.
  ASMJIT_API StringLogger() noexcept;
  //! Destroys the `StringLogger`.
  ASMJIT_API ~StringLogger() noexcept override;

  //! \}

  //! \name Logger Data Accessors
  //! \{

  //! Returns the content of the logger as \ref String.
  //!
  //! It can be moved, if desired.
  ASMJIT_INLINE_NODEBUG String& content() noexcept { return _content; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const String& content() const noexcept { return _content; }

  //! Returns aggregated logger data as `char*` pointer.
  //!
  //! The pointer is owned by `StringLogger`, it can't be modified or freed.
  ASMJIT_INLINE_NODEBUG const char* data() const noexcept { return _content.data(); }
  //! Returns size of the data returned by `data()`.
  ASMJIT_INLINE_NODEBUG size_t dataSize() const noexcept { return _content.size(); }

  //! \}

  //! \name Logger Data Manipulation
  //! \{

  //! Clears the accumulated logger data.
  ASMJIT_INLINE_NODEBUG void clear() noexcept { _content.clear(); }

  //! \}

  ASMJIT_API Error _log(const char* data, size_t size = SIZE_MAX) noexcept override;
};

//! \}

ASMJIT_END_NAMESPACE

#endif

#endif // ASMJIT_CORE_LOGGER_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/misc_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_MISC_P_H_INCLUDED
#define ASMJIT_CORE_MISC_P_H_INCLUDED

#include "../core/api-config.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_utilities
//! \{

#define ASMJIT_LOOKUP_TABLE_4(T, I) T((I)), T((I+1)), T((I+2)), T((I+3))
#define ASMJIT_LOOKUP_TABLE_8(T, I) ASMJIT_LOOKUP_TABLE_4(T, I), ASMJIT_LOOKUP_TABLE_4(T, I + 4)
#define ASMJIT_LOOKUP_TABLE_16(T, I) ASMJIT_LOOKUP_TABLE_8(T, I), ASMJIT_LOOKUP_TABLE_8(T, I + 8)
#define ASMJIT_LOOKUP_TABLE_32(T, I) ASMJIT_LOOKUP_TABLE_16(T, I), ASMJIT_LOOKUP_TABLE_16(T, I + 16)
#define ASMJIT_LOOKUP_TABLE_40(T, I) ASMJIT_LOOKUP_TABLE_16(T, I), ASMJIT_LOOKUP_TABLE_16(T, I + 16), ASMJIT_LOOKUP_TABLE_8(T, I + 32)
#define ASMJIT_LOOKUP_TABLE_64(T, I) ASMJIT_LOOKUP_TABLE_32(T, I), ASMJIT_LOOKUP_TABLE_32(T, I + 32)
#define ASMJIT_LOOKUP_TABLE_128(T, I) ASMJIT_LOOKUP_TABLE_64(T, I), ASMJIT_LOOKUP_TABLE_64(T, I + 64)
#define ASMJIT_LOOKUP_TABLE_256(T, I) ASMJIT_LOOKUP_TABLE_128(T, I), ASMJIT_LOOKUP_TABLE_128(T, I + 128)
#define ASMJIT_LOOKUP_TABLE_512(T, I) ASMJIT_LOOKUP_TABLE_256(T, I), ASMJIT_LOOKUP_TABLE_256(T, I + 256)
#define ASMJIT_LOOKUP_TABLE_1024(T, I) ASMJIT_LOOKUP_TABLE_512(T, I), ASMJIT_LOOKUP_TABLE_512(T, I + 512)

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_MISC_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/operand.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/operand.h"

ASMJIT_BEGIN_NAMESPACE

// Operand - Tests
// ===============

#if defined(ASMJIT_TEST)
enum class StrongEnumForImmTests : uint32_t {
  kValue0,
  kValue0xFFFFFFFF = 0xFFFFFFFFu
};

UNIT(operand) {
  INFO("Checking operand sizes");
  EXPECT_EQ(sizeof(Operand), 16u);
  EXPECT_EQ(sizeof(BaseReg), 16u);
  EXPECT_EQ(sizeof(BaseMem), 16u);
  EXPECT_EQ(sizeof(Imm), 16u);
  EXPECT_EQ(sizeof(Label), 16u);

  INFO("Checking basic functionality of Operand");
  Operand a, b;
  Operand dummy;

  EXPECT_TRUE(a.isNone());
  EXPECT_FALSE(a.isReg());
  EXPECT_FALSE(a.isMem());
  EXPECT_FALSE(a.isImm());
  EXPECT_FALSE(a.isLabel());
  EXPECT_EQ(a, b);
  EXPECT_EQ(a._data[0], 0u);
  EXPECT_EQ(a._data[1], 0u);

  INFO("Checking basic functionality of Label");
  Label label;
  EXPECT_FALSE(label.isValid());
  EXPECT_EQ(label.id(), Globals::kInvalidId);

  INFO("Checking basic functionality of BaseReg");
  EXPECT_TRUE(BaseReg().isReg());
  EXPECT_FALSE(BaseReg().isValid());
  EXPECT_EQ(BaseReg()._data[0], 0u);
  EXPECT_EQ(BaseReg()._data[1], 0u);
  EXPECT_FALSE(dummy.as<BaseReg>().isValid());

  // Create some register (not specific to any architecture).
  OperandSignature rSig = OperandSignature::fromOpType(OperandType::kReg) |
                          OperandSignature::fromRegType(RegType::kVec128) |
                          OperandSignature::fromRegGroup(RegGroup::kVec) |
                          OperandSignature::fromSize(8);
  BaseReg r1(rSig, 5);

  EXPECT_TRUE(r1.isValid());
  EXPECT_TRUE(r1.isReg());
  EXPECT_TRUE(r1.isReg(RegType::kVec128));
  EXPECT_TRUE(r1.isPhysReg());
  EXPECT_FALSE(r1.isVirtReg());
  EXPECT_EQ(r1.signature(), rSig);
  EXPECT_EQ(r1.type(), RegType::kVec128);
  EXPECT_EQ(r1.group(), RegGroup::kVec);
  EXPECT_EQ(r1.size(), 8u);
  EXPECT_EQ(r1.id(), 5u);
  EXPECT_TRUE(r1.isReg(RegType::kVec128, 5)); // RegType and Id.
  EXPECT_EQ(r1._data[0], 0u);
  EXPECT_EQ(r1._data[1], 0u);

  // The same type of register having different id.
  BaseReg r2(r1, 6);
  EXPECT_TRUE(r2.isValid());
  EXPECT_TRUE(r2.isReg());
  EXPECT_TRUE(r2.isReg(RegType::kVec128));
  EXPECT_TRUE(r2.isPhysReg());
  EXPECT_FALSE(r2.isVirtReg());
  EXPECT_EQ(r2.signature(), rSig);
  EXPECT_EQ(r2.type(), r1.type());
  EXPECT_EQ(r2.group(), r1.group());
  EXPECT_EQ(r2.size(), r1.size());
  EXPECT_EQ(r2.id(), 6u);
  EXPECT_TRUE(r2.isReg(RegType::kVec128, 6));

  r1.reset();
  EXPECT_FALSE(r1.isReg());
  EXPECT_FALSE(r1.isValid());

  INFO("Checking basic functionality of BaseMem");
  BaseMem m;
  EXPECT_TRUE(m.isMem());
  EXPECT_EQ(m, BaseMem());
  EXPECT_FALSE(m.hasBase());
  EXPECT_FALSE(m.hasIndex());
  EXPECT_FALSE(m.hasOffset());
  EXPECT_TRUE(m.isOffset64Bit());
  EXPECT_EQ(m.offset(), 0);

  m.setOffset(-1);
  EXPECT_EQ(m.offsetLo32(), -1);
  EXPECT_EQ(m.offset(), -1);

  int64_t x = int64_t(0xFF00FF0000000001u);
  int32_t xHi = int32_t(0xFF00FF00u);

  m.setOffset(x);
  EXPECT_EQ(m.offset(), x);
  EXPECT_EQ(m.offsetLo32(), 1);
  EXPECT_EQ(m.offsetHi32(), xHi);

  INFO("Checking basic functionality of Imm");
  Imm immValue(-42);
  EXPECT_EQ(immValue.type(), ImmType::kInt);
  EXPECT_EQ(Imm(-1).value(), -1);
  EXPECT_EQ(imm(-1).value(), -1);
  EXPECT_EQ(immValue.value(), -42);
  EXPECT_EQ(imm(0xFFFFFFFF).value(), int64_t(0xFFFFFFFF));

  Imm immDouble(0.4);
  EXPECT_EQ(immDouble.type(), ImmType::kDouble);
  EXPECT_EQ(immDouble.valueAs<double>(), 0.4);
  EXPECT_EQ(immDouble, imm(0.4));

  EXPECT_EQ(Imm(StrongEnumForImmTests::kValue0).value(), 0);
  EXPECT_EQ(Imm(StrongEnumForImmTests::kValue0xFFFFFFFF).value(), 0xFFFFFFFFu);
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/operand.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_OPERAND_H_INCLUDED
#define ASMJIT_CORE_OPERAND_H_INCLUDED

#include "../core/archcommons.h"
#include "../core/support.h"
#include "../core/type.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_assembler
//! \{

//! Operand type used by \ref Operand_.
enum class OperandType : uint32_t {
  //! Not an operand or not initialized.
  kNone = 0,
  //! Operand is a register.
  kReg = 1,
  //! Operand is a memory.
  kMem = 2,
  //! Operand is a register-list.
  kRegList = 3,
  //! Operand is an immediate value.
  kImm = 4,
  //! Operand is a label.
  kLabel = 5,

  //! Maximum value of `OperandType`.
  kMaxValue = kRegList
};

static_assert(uint32_t(OperandType::kMem) == uint32_t(OperandType::kReg) + 1,
              "AsmJit requires that `OperandType::kMem` equals to `OperandType::kReg + 1`");

//! Register mask is a convenience typedef that describes a mask where each bit describes a physical register id
//! in the same \ref RegGroup. At the moment 32 bits are enough as AsmJit doesn't support any architecture that
//! would provide more than 32 registers for a register group.
typedef uint32_t RegMask;

//! Register type.
//!
//! Provides a unique type that can be used to identify a register or its view.
enum class RegType : uint8_t {
  //! No register - unused, invalid, multiple meanings.
  kNone = 0,

  //! This is not a register type. This value is reserved for a \ref Label that's used in \ref BaseMem as a base.
  //!
  //! Label tag is used as a sub-type, forming a unique signature across all operand types as 0x1 is never associated
  //! with any register type. This means that a memory operand's BASE register can be constructed from virtually any
  //! operand (register vs. label) by just assigning its type (register type or label-tag) and operand id.
  kLabelTag = 1,

  //! Universal type describing program counter (PC) or instruction pointer (IP) register, if the target architecture
  //! actually exposes it as a separate register type, which most modern architectures do.
  kPC = 2,

  //! 8-bit low general purpose register (X86).
  kGp8Lo = 3,
  //! 8-bit high general purpose register (X86).
  kGp8Hi = 4,
  //! 16-bit general purpose register (X86).
  kGp16 = 5,
  //! 32-bit general purpose register (X86|AArch32|AArch64).
  kGp32 = 6,
  //! 64-bit general purpose register (X86|AArch64).
  kGp64 = 7,
  //! 8-bit view of a vector register (AArch64).
  kVec8 = 8,
  //! 16-bit view of a vector register (AArch64).
  kVec16 = 9,
  //! 32-bit view of a vector register (AArch32|AArch64).
  kVec32 = 10,
  //! 64-bit view of a vector register (AArch32|AArch64).
  //!
  //! \note This is never used for MMX registers on X86, MMX registers have its own category.
  kVec64 = 11,
  //! 128-bit view of a vector register (X86|AArch32|AArch64).
  kVec128 = 12,
  //! 256-bit view of a vector register (X86).
  kVec256 = 13,
  //! 512-bit view of a vector register (X86).
  kVec512 = 14,
  //! 1024-bit view of a vector register (future).
  kVec1024 = 15,
  //! View of a vector register, which width is implementation specific (AArch64).
  kVecNLen = 16,

  //! Mask register (X86).
  kMask = 17,

  //! Start of architecture dependent register types.
  kExtra = 18,

  // X86 Specific Register Types
  // ---------------------------

  //! Instruction pointer (RIP), only addressable in \ref x86::Mem in 64-bit targets.
  kX86_Rip = kPC,
  //! Low GPB register (AL, BL, CL, DL, ...).
  kX86_GpbLo = kGp8Lo,
  //! High GPB register (AH, BH, CH, DH only).
  kX86_GpbHi = kGp8Hi,
  //! GPW register.
  kX86_Gpw = kGp16,
  //! GPD register.
  kX86_Gpd = kGp32,
  //! GPQ register (64-bit).
  kX86_Gpq = kGp64,
  //! XMM register (SSE+).
  kX86_Xmm = kVec128,
  //! YMM register (AVX+).
  kX86_Ymm = kVec256,
  //! ZMM register (AVX512+).
  kX86_Zmm = kVec512,
  //! K register (AVX512+).
  kX86_KReg = kMask,
  //! MMX register.
  kX86_Mm = kExtra + 0,
  //! Segment register (None, ES, CS, SS, DS, FS, GS).
  kX86_SReg = kExtra + 1,
  //! Control register (CR).
  kX86_CReg = kExtra + 2,
  //! Debug register (DR).
  kX86_DReg = kExtra + 3,
  //! FPU (x87) register.
  kX86_St = kExtra + 4,
  //! Bound register (BND).
  kX86_Bnd = kExtra + 5,
  //! TMM register (AMX_TILE)
  kX86_Tmm = kExtra + 6,

  // ARM Specific Register Types
  // ---------------------------

  //! Program pointer (PC) register (AArch64).
  kARM_PC = kPC,
  //! 32-bit general purpose register (R or W).
  kARM_GpW = kGp32,
  //! 64-bit general purpose register (X).
  kARM_GpX = kGp64,
  //! 8-bit view of VFP/ASIMD register (B).
  kARM_VecB = kVec8,
  //! 16-bit view of VFP/ASIMD register (H).
  kARM_VecH = kVec16,
  //! 32-bit view of VFP/ASIMD register (S).
  kARM_VecS = kVec32,
  //! 64-bit view of VFP/ASIMD register (D).
  kARM_VecD = kVec64,
  //! 128-bit view of VFP/ASIMD register (Q).
  kARM_VecQ = kVec128,
  //! 128-bit view of VFP/ASIMD register (V).
  kARM_VecV = kVec128,

  //! Maximum value of `RegType`.
  kMaxValue = 31
};
ASMJIT_DEFINE_ENUM_COMPARE(RegType)

//! Register group.
//!
//! Provides a unique value that identifies groups of registers and their views.
enum class RegGroup : uint8_t {
  //! General purpose register group compatible with all backends.
  kGp = 0,
  //! Vector register group compatible with all backends.
  //!
  //! Describes X86 XMM|YMM|ZMM registers ARM/AArch64 V registers.
  kVec = 1,

  //! Mask register group compatible with all backends that can use masking.
  kMask = 2,
  //! Extra virtual group #3 that can be used by Compiler for register allocation.
  kExtraVirt3 = 3,

  //! Program counter group.
  kPC = 4,

  //! Extra non-virtual group that can be used by registers not managed by Compiler.
  kExtraNonVirt = 5,

  // X86 Specific Register Groups
  // ----------------------------

  //! K register group (KReg) - maps to \ref RegGroup::kMask (X86, X86_64).
  kX86_K = kMask,
  //! MMX register group (MM) - maps to \ref RegGroup::kExtraVirt3 (X86, X86_64).
  kX86_MM = kExtraVirt3,

  //! Instruction pointer (X86, X86_64).
  kX86_Rip = kPC,
  //! Segment register group (X86, X86_64).
  kX86_SReg = kExtraNonVirt + 0,
  //! CR register group (X86, X86_64).
  kX86_CReg = kExtraNonVirt + 1,
  //! DR register group (X86, X86_64).
  kX86_DReg = kExtraNonVirt + 2,
  //! FPU register group (X86, X86_64).
  kX86_St = kExtraNonVirt + 3,
  //! BND register group (X86, X86_64).
  kX86_Bnd = kExtraNonVirt + 4,
  //! TMM register group (X86, X86_64).
  kX86_Tmm = kExtraNonVirt + 5,

  //! First group - only used in loops.
  k0 = 0,
  //! Last value of a virtual register that is managed by \ref BaseCompiler.
  kMaxVirt = Globals::kNumVirtGroups - 1,
  //! Maximum value of `RegGroup`.
  kMaxValue = 15
};
ASMJIT_DEFINE_ENUM_COMPARE(RegGroup)

typedef Support::EnumValues<RegGroup, RegGroup::kGp, RegGroup::kMaxVirt> RegGroupVirtValues;

//! Operand signature is a 32-bit number describing \ref Operand and some of its payload.
//!
//! In AsmJit operand signature is used to store additional payload of register, memory, and immediate operands.
//! In practice the biggest pressure on OperandSignature is from \ref BaseMem and architecture specific memory
//! operands that need to store additional payload that cannot be stored elsewhere as values of all other members
//! are fully specified by \ref BaseMem.
struct OperandSignature {
  //! \name Constants
  //! \{

  enum : uint32_t {
    // Operand type (3 least significant bits).
    // |........|........|........|.....XXX|
    kOpTypeShift = 0,
    kOpTypeMask = 0x07u << kOpTypeShift,

    // Register type (5 bits).
    // |........|........|........|XXXXX...|
    kRegTypeShift = 3,
    kRegTypeMask = 0x1Fu << kRegTypeShift,

    // Register group (4 bits).
    // |........|........|....XXXX|........|
    kRegGroupShift = 8,
    kRegGroupMask = 0x0Fu << kRegGroupShift,

    // Memory base type (5 bits).
    // |........|........|........|XXXXX...|
    kMemBaseTypeShift = 3,
    kMemBaseTypeMask = 0x1Fu << kMemBaseTypeShift,

    // Memory index type (5 bits).
    // |........|........|...XXXXX|........|
    kMemIndexTypeShift = 8,
    kMemIndexTypeMask = 0x1Fu << kMemIndexTypeShift,

    // Memory base+index combined (10 bits).
    // |........|........|...XXXXX|XXXXX...|
    kMemBaseIndexShift = 3,
    kMemBaseIndexMask = 0x3FFu << kMemBaseIndexShift,

    // This memory operand represents a home-slot or stack (Compiler) (1 bit).
    // |........|........|..X.....|........|
    kMemRegHomeShift = 13,
    kMemRegHomeFlag = 0x01u << kMemRegHomeShift,

    // Immediate type (1 bit).
    // |........|........|........|....X...|
    kImmTypeShift = 3,
    kImmTypeMask = 0x01u << kImmTypeShift,

    // Predicate used by either registers or immediate values (4 bits).
    // |........|XXXX....|........|........|
    kPredicateShift = 20,
    kPredicateMask = 0x0Fu << kPredicateShift,

    // Operand size (8 most significant bits).
    // |XXXXXXXX|........|........|........|
    kSizeShift = 24,
    kSizeMask = 0xFFu << kSizeShift
  };

  //! \}

  //! \name Members
  //! \{

  uint32_t _bits;

  //! \}

  //! \name Overloaded Operators
  //!
  //! Overloaded operators make `OperandSignature` behave like regular integer.
  //!
  //! \{

  ASMJIT_INLINE_NODEBUG constexpr bool operator!() const noexcept { return _bits == 0; }
  ASMJIT_INLINE_NODEBUG constexpr explicit operator bool() const noexcept { return _bits != 0; }

  ASMJIT_INLINE_NODEBUG OperandSignature& operator|=(uint32_t x) noexcept { _bits |= x; return *this; }
  ASMJIT_INLINE_NODEBUG OperandSignature& operator&=(uint32_t x) noexcept { _bits &= x; return *this; }
  ASMJIT_INLINE_NODEBUG OperandSignature& operator^=(uint32_t x) noexcept { _bits ^= x; return *this; }

  ASMJIT_INLINE_NODEBUG OperandSignature& operator|=(const OperandSignature& other) noexcept { return operator|=(other._bits); }
  ASMJIT_INLINE_NODEBUG OperandSignature& operator&=(const OperandSignature& other) noexcept { return operator&=(other._bits); }
  ASMJIT_INLINE_NODEBUG OperandSignature& operator^=(const OperandSignature& other) noexcept { return operator^=(other._bits); }

  ASMJIT_INLINE_NODEBUG constexpr OperandSignature operator~() const noexcept { return OperandSignature{~_bits}; }

  ASMJIT_INLINE_NODEBUG constexpr OperandSignature operator|(uint32_t x) const noexcept { return OperandSignature{_bits | x}; }
  ASMJIT_INLINE_NODEBUG constexpr OperandSignature operator&(uint32_t x) const noexcept { return OperandSignature{_bits & x}; }
  ASMJIT_INLINE_NODEBUG constexpr OperandSignature operator^(uint32_t x) const noexcept { return OperandSignature{_bits ^ x}; }

  ASMJIT_INLINE_NODEBUG constexpr OperandSignature operator|(const OperandSignature& other) const noexcept { return OperandSignature{_bits | other._bits}; }
  ASMJIT_INLINE_NODEBUG constexpr OperandSignature operator&(const OperandSignature& other) const noexcept { return OperandSignature{_bits & other._bits}; }
  ASMJIT_INLINE_NODEBUG constexpr OperandSignature operator^(const OperandSignature& other) const noexcept { return OperandSignature{_bits ^ other._bits}; }

  ASMJIT_INLINE_NODEBUG constexpr bool operator==(uint32_t x) const noexcept { return _bits == x; }
  ASMJIT_INLINE_NODEBUG constexpr bool operator!=(uint32_t x) const noexcept { return _bits != x; }

  ASMJIT_INLINE_NODEBUG constexpr bool operator==(const OperandSignature& other) const noexcept { return _bits == other._bits; }
  ASMJIT_INLINE_NODEBUG constexpr bool operator!=(const OperandSignature& other) const noexcept { return _bits != other._bits; }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG void reset() noexcept { _bits = 0; }

  ASMJIT_INLINE_NODEBUG constexpr uint32_t bits() const noexcept { return _bits; }
  ASMJIT_INLINE_NODEBUG void setBits(uint32_t bits) noexcept { _bits = bits; }

  template<uint32_t kFieldMask>
  ASMJIT_INLINE_NODEBUG constexpr bool hasField() const noexcept {
    return (_bits & kFieldMask) != 0;
  }

  template<uint32_t kFieldMask>
  ASMJIT_INLINE_NODEBUG constexpr bool hasField(uint32_t value) const noexcept {
    return (_bits & kFieldMask) != value << Support::ConstCTZ<kFieldMask>::value;
  }

  template<uint32_t kFieldMask>
  ASMJIT_INLINE_NODEBUG constexpr uint32_t getField() const noexcept {
    return (_bits >> Support::ConstCTZ<kFieldMask>::value) & (kFieldMask >> Support::ConstCTZ<kFieldMask>::value);
  }

  template<uint32_t kFieldMask>
  ASMJIT_INLINE_NODEBUG void setField(uint32_t value) noexcept {
    ASMJIT_ASSERT(((value << Support::ConstCTZ<kFieldMask>::value) & ~kFieldMask) == 0);
    _bits = (_bits & ~kFieldMask) | (value << Support::ConstCTZ<kFieldMask>::value);
  }

  ASMJIT_INLINE_NODEBUG constexpr OperandSignature subset(uint32_t mask) const noexcept { return OperandSignature{_bits & mask}; }

  template<uint32_t kFieldMask, uint32_t kFieldShift = Support::ConstCTZ<kFieldMask>::value>
  ASMJIT_INLINE_NODEBUG constexpr OperandSignature replacedValue(uint32_t value) const noexcept { return OperandSignature{(_bits & ~kFieldMask) | (value << kFieldShift)}; }

  template<uint32_t kFieldMask>
  ASMJIT_INLINE_NODEBUG constexpr bool matchesSignature(const OperandSignature& signature) const noexcept {
    return (_bits & kFieldMask) == signature._bits;
  }

  template<uint32_t kFieldMask>
  ASMJIT_INLINE_NODEBUG constexpr bool matchesFields(uint32_t bits) const noexcept {
    return (_bits & kFieldMask) == bits;
  }

  template<uint32_t kFieldMask>
  ASMJIT_INLINE_NODEBUG constexpr bool matchesFields(const OperandSignature& fields) const noexcept {
    return (_bits & kFieldMask) == fields._bits;
  }

  ASMJIT_INLINE_NODEBUG constexpr bool isValid() const noexcept { return _bits != 0; }

  ASMJIT_INLINE_NODEBUG constexpr OperandType opType() const noexcept { return (OperandType)getField<kOpTypeMask>(); }

  ASMJIT_INLINE_NODEBUG constexpr RegType regType() const noexcept { return (RegType)getField<kRegTypeMask>(); }
  ASMJIT_INLINE_NODEBUG constexpr RegGroup regGroup() const noexcept { return (RegGroup)getField<kRegGroupMask>(); }

  ASMJIT_INLINE_NODEBUG constexpr RegType memBaseType() const noexcept { return (RegType)getField<kMemBaseTypeMask>(); }
  ASMJIT_INLINE_NODEBUG constexpr RegType memIndexType() const noexcept { return (RegType)getField<kMemIndexTypeMask>(); }

  ASMJIT_INLINE_NODEBUG constexpr uint32_t predicate() const noexcept { return getField<kPredicateMask>(); }
  ASMJIT_INLINE_NODEBUG constexpr uint32_t size() const noexcept { return getField<kSizeMask>(); }

  ASMJIT_INLINE_NODEBUG void setOpType(OperandType opType) noexcept { setField<kOpTypeMask>(uint32_t(opType)); }
  ASMJIT_INLINE_NODEBUG void setRegType(RegType regType) noexcept { setField<kRegTypeMask>(uint32_t(regType)); }
  ASMJIT_INLINE_NODEBUG void setRegGroup(RegGroup regGroup) noexcept { setField<kRegGroupMask>(uint32_t(regGroup)); }

  ASMJIT_INLINE_NODEBUG void setMemBaseType(RegType baseType) noexcept { setField<kMemBaseTypeMask>(uint32_t(baseType)); }
  ASMJIT_INLINE_NODEBUG void setMemIndexType(RegType indexType) noexcept { setField<kMemIndexTypeMask>(uint32_t(indexType)); }

  ASMJIT_INLINE_NODEBUG void setPredicate(uint32_t predicate) noexcept { setField<kPredicateMask>(predicate); }
  ASMJIT_INLINE_NODEBUG void setSize(uint32_t size) noexcept { setField<kSizeMask>(size); }

  //! \}

  //! \name Static Constructors
  //! \{

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromBits(uint32_t bits) noexcept {
    return OperandSignature{bits};
  }

  template<uint32_t kFieldMask, typename T>
  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromValue(const T& value) noexcept {
    return OperandSignature{uint32_t(value) << Support::ConstCTZ<kFieldMask>::value};
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromOpType(OperandType opType) noexcept {
    return OperandSignature{uint32_t(opType) << kOpTypeShift};
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromRegType(RegType regType) noexcept {
    return OperandSignature{uint32_t(regType) << kRegTypeShift};
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromRegGroup(RegGroup regGroup) noexcept {
    return OperandSignature{uint32_t(regGroup) << kRegGroupShift};
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromRegTypeAndGroup(RegType regType, RegGroup regGroup) noexcept {
    return fromRegType(regType) | fromRegGroup(regGroup);
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromMemBaseType(RegType baseType) noexcept {
    return OperandSignature{uint32_t(baseType) << kMemBaseTypeShift};
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromMemIndexType(RegType indexType) noexcept {
    return OperandSignature{uint32_t(indexType) << kMemIndexTypeShift};
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromPredicate(uint32_t predicate) noexcept {
    return OperandSignature{predicate << kPredicateShift};
  }

  static ASMJIT_INLINE_NODEBUG constexpr OperandSignature fromSize(uint32_t size) noexcept {
    return OperandSignature{size << kSizeShift};
  }

  //! \}
};

//! Base class representing an operand in AsmJit (non-default constructed version).
//!
//! Contains no initialization code and can be used safely to define an array of operands that won't be initialized.
//! This is a \ref Operand base structure designed to be statically initialized, static const, or to be used by user
//! code to define an array of operands without having them default initialized at construction time.
//!
//! The key difference between \ref Operand and \ref Operand_ is:
//!
//! ```
//! Operand_ xArray[10];    // Not initialized, contains garbage.
//! Operand_ yArray[10] {}; // All operands initialized to none explicitly (zero initialized).
//! Operand  yArray[10];    // All operands initialized to none implicitly (zero initialized).
//! ```
struct Operand_ {
  //! \name Types
  //! \{

  typedef OperandSignature Signature;

  //! \}

  //! \name Constants
  //! \{

  // Indexes to `_data` array.
  enum DataIndex : uint32_t {
    kDataMemIndexId = 0,
    kDataMemOffsetLo = 1,

    kDataImmValueLo = ASMJIT_ARCH_LE ? 0 : 1,
    kDataImmValueHi = ASMJIT_ARCH_LE ? 1 : 0
  };

  //! Constants useful for VirtId <-> Index translation.
  enum VirtIdConstants : uint32_t {
    //! Minimum valid packed-id.
    kVirtIdMin = 256,
    //! Maximum valid packed-id, excludes Globals::kInvalidId.
    kVirtIdMax = Globals::kInvalidId - 1,
    //! Count of valid packed-ids.
    kVirtIdCount = uint32_t(kVirtIdMax - kVirtIdMin + 1)
  };

  //! \}

  //! \name Members
  //! \{

  //! Provides operand type and additional payload.
  Signature _signature;
  //! Either base id as used by memory operand or any id as used by others.
  uint32_t _baseId;

  //! Data specific to the operand type.
  //!
  //! The reason we don't use union is that we have `constexpr` constructors that construct operands and other
  //!`constexpr` functions that return whether another Operand or something else. These cannot generally work with
  //! unions so we also cannot use `union` if we want to be standard compliant.
  uint32_t _data[2];

  //! \}

  //! Tests whether the given `id` is a valid virtual register id. Since AsmJit supports both physical and virtual
  //! registers it must be able to distinguish between these two. The idea is that physical registers are always
  //! limited in size, so virtual identifiers start from `kVirtIdMin` and end at `kVirtIdMax`.
  static ASMJIT_INLINE_NODEBUG bool isVirtId(uint32_t id) noexcept { return id - kVirtIdMin < uint32_t(kVirtIdCount); }
  //! Converts a real-id into a packed-id that can be stored in Operand.
  static ASMJIT_INLINE_NODEBUG uint32_t indexToVirtId(uint32_t id) noexcept { return id + kVirtIdMin; }
  //! Converts a packed-id back to real-id.
  static ASMJIT_INLINE_NODEBUG uint32_t virtIdToIndex(uint32_t id) noexcept { return id - kVirtIdMin; }

  //! \name Construction & Destruction
  //! \{

  //! \cond INTERNAL
  //! Initializes a `BaseReg` operand from `signature` and register `id`.
  ASMJIT_INLINE_NODEBUG void _initReg(const Signature& signature, uint32_t id) noexcept {
    _signature = signature;
    _baseId = id;
    _data[0] = 0;
    _data[1] = 0;
  }
  //! \endcond

  //! Initializes the operand from `other` operand (used by operator overloads).
  ASMJIT_INLINE_NODEBUG void copyFrom(const Operand_& other) noexcept {
    _signature._bits = other._signature._bits;
    _baseId = other._baseId;
    _data[0] = other._data[0];
    _data[1] = other._data[1];
  }

  //! Resets the `Operand` to none.
  //!
  //! None operand is defined the following way:
  //!   - Its signature is zero (OperandType::kNone, and the rest zero as well).
  //!   - Its id is `0`.
  //!   - The reserved8_4 field is set to `0`.
  //!   - The reserved12_4 field is set to zero.
  //!
  //! In other words, reset operands have all members set to zero. Reset operand must match the Operand state
  //! right after its construction. Alternatively, if you have an array of operands, you can simply use `memset()`.
  //!
  //! ```
  //! using namespace asmjit;
  //!
  //! Operand a;
  //! Operand b;
  //! assert(a == b);
  //!
  //! b = x86::eax;
  //! assert(a != b);
  //!
  //! b.reset();
  //! assert(a == b);
  //!
  //! memset(&b, 0, sizeof(Operand));
  //! assert(a == b);
  //! ```
  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _signature.reset();
    _baseId = 0;
    _data[0] = 0;
    _data[1] = 0;
  }

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Tests whether this operand is the same as `other`.
  ASMJIT_INLINE_NODEBUG constexpr bool operator==(const Operand_& other) const noexcept { return  equals(other); }
  //! Tests whether this operand is not the same as `other`.
  ASMJIT_INLINE_NODEBUG constexpr bool operator!=(const Operand_& other) const noexcept { return !equals(other); }

  //! \}

  //! \name Cast
  //! \{

  //! Casts this operand to `T` type.
  template<typename T>
  ASMJIT_INLINE_NODEBUG T& as() noexcept { return static_cast<T&>(*this); }

  //! Casts this operand to `T` type (const).
  template<typename T>
  ASMJIT_INLINE_NODEBUG const T& as() const noexcept { return static_cast<const T&>(*this); }

  //! \}

  //! \name Equality
  //! \{

  //! Tests whether the operand is 100% equal to `other` operand.
  //!
  //! \note This basically performs a binary comparison, if aby bit is
  //! different the operands are not equal.
  ASMJIT_INLINE_NODEBUG constexpr bool equals(const Operand_& other) const noexcept {
    return bool(unsigned(_signature == other._signature) &
                unsigned(_baseId    == other._baseId   ) &
                unsigned(_data[0]   == other._data[0]  ) &
                unsigned(_data[1]   == other._data[1]  ));
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the operand's signature matches the signature of the `other` operand.
  ASMJIT_INLINE_NODEBUG constexpr bool hasSignature(const Operand_& other) const noexcept { return _signature == other._signature; }
  //! Tests whether the operand's signature matches the given signature `sign`.
  ASMJIT_INLINE_NODEBUG constexpr bool hasSignature(const Signature& other) const noexcept { return _signature == other; }

  //! Returns operand signature as unsigned 32-bit integer.
  //!
  //! Signature is first 4 bytes of the operand data. It's used mostly for operand checking as it's
  //! much faster to check packed 4 bytes at once than having to check these bytes individually.
  ASMJIT_INLINE_NODEBUG constexpr Signature signature() const noexcept { return _signature; }

  //! Sets the operand signature, see `signature()`.
  //!
  //! \note Improper use of `setSignature()` can lead to hard-to-debug errors.
  ASMJIT_INLINE_NODEBUG void setSignature(const Signature& signature) noexcept { _signature = signature; }
  //! \overload
  ASMJIT_INLINE_NODEBUG void setSignature(uint32_t signature) noexcept { _signature._bits = signature; }

  //! Returns the type of the operand, see `OpType`.
  ASMJIT_INLINE_NODEBUG constexpr OperandType opType() const noexcept { return _signature.opType(); }
  //! Tests whether the operand is none (`OperandType::kNone`).
  ASMJIT_INLINE_NODEBUG constexpr bool isNone() const noexcept { return _signature == Signature::fromBits(0); }
  //! Tests whether the operand is a register (`OperandType::kReg`).
  ASMJIT_INLINE_NODEBUG constexpr bool isReg() const noexcept { return opType() == OperandType::kReg; }
  //! Tests whether the operand is a register-list.
  //!
  //! \note Register-list is currently only used by 32-bit ARM architecture.
  ASMJIT_INLINE_NODEBUG constexpr bool isRegList() const noexcept { return opType() == OperandType::kRegList; }
  //! Tests whether the operand is a memory location (`OperandType::kMem`).
  ASMJIT_INLINE_NODEBUG constexpr bool isMem() const noexcept { return opType() == OperandType::kMem; }
  //! Tests whether the operand is an immediate (`OperandType::kImm`).
  ASMJIT_INLINE_NODEBUG constexpr bool isImm() const noexcept { return opType() == OperandType::kImm; }
  //! Tests whether the operand is a label (`OperandType::kLabel`).
  ASMJIT_INLINE_NODEBUG constexpr bool isLabel() const noexcept { return opType() == OperandType::kLabel; }

  //! Tests whether the operand is a physical register.
  ASMJIT_INLINE_NODEBUG constexpr bool isPhysReg() const noexcept { return isReg() && _baseId < 0xFFu; }
  //! Tests whether the operand is a virtual register.
  ASMJIT_INLINE_NODEBUG constexpr bool isVirtReg() const noexcept { return isReg() && _baseId > 0xFFu; }

  //! Returns the operand id.
  //!
  //! The value returned should be interpreted accordingly to the operand type:
  //!   * None  - Should be `0`.
  //!   * Reg   - Physical or virtual register id.
  //!   * Mem   - Multiple meanings - BASE address (register or label id), or high value of a 64-bit absolute address.
  //!   * Imm   - Should be `0`.
  //!   * Label - Label id if it was created by using `newLabel()` or `Globals::kInvalidId` if the label is invalid or
  //!             not initialized.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t id() const noexcept { return _baseId; }

  //! Tests whether the operand is a register matching the given register `type`.
  ASMJIT_INLINE_NODEBUG constexpr bool isReg(RegType type) const noexcept {
    return _signature.subset(Signature::kOpTypeMask | Signature::kRegTypeMask) == (Signature::fromOpType(OperandType::kReg) | Signature::fromRegType(type));
  }

  //! Tests whether the operand is a register of the provided register group `regGroup`.
  ASMJIT_INLINE_NODEBUG constexpr bool isReg(RegGroup regGroup) const noexcept {
    return _signature.subset(Signature::kOpTypeMask | Signature::kRegGroupMask) == (Signature::fromOpType(OperandType::kReg) | Signature::fromRegGroup(regGroup));
  }

  //! Tests whether the operand is register and of register type `regType` and `regId`.
  ASMJIT_INLINE_NODEBUG constexpr bool isReg(RegType regType, uint32_t regId) const noexcept { return isReg(regType) && _baseId == regId; }
  //! Tests whether the operand is register and of register group `regGroup` and `regId`.
  ASMJIT_INLINE_NODEBUG constexpr bool isReg(RegGroup regGroup, uint32_t regId) const noexcept { return isReg(regGroup) && _baseId == regId; }

  //! Tests whether the register is a general purpose register (any size).
  ASMJIT_INLINE_NODEBUG constexpr bool isGp() const noexcept { return isReg(RegGroup::kGp); }
  //! Tests whether the register is a 32-bit general purpose register.
  ASMJIT_INLINE_NODEBUG constexpr bool isGp32() const noexcept { return isReg(RegType::kGp32); }
  //! Tests whether the register is a 64-bit general purpose register.
  ASMJIT_INLINE_NODEBUG constexpr bool isGp64() const noexcept { return isReg(RegType::kGp64); }

  //! Tests whether the register is a vector register of any size.
  ASMJIT_INLINE_NODEBUG constexpr bool isVec() const noexcept { return isReg(RegGroup::kVec); }
  //! Tests whether the register is an 8-bit vector register or view (AArch64).
  ASMJIT_INLINE_NODEBUG constexpr bool isVec8() const noexcept { return isReg(RegType::kVec8); }
  //! Tests whether the register is a 16-bit vector register or view (AArch64).
  ASMJIT_INLINE_NODEBUG constexpr bool isVec16() const noexcept { return isReg(RegType::kVec16); }
  //! Tests whether the register is a 32-bit vector register or view (AArch32, AArch64).
  ASMJIT_INLINE_NODEBUG constexpr bool isVec32() const noexcept { return isReg(RegType::kVec32); }
  //! Tests whether the register is a 64-bit vector register or view (AArch32, AArch64).
  ASMJIT_INLINE_NODEBUG constexpr bool isVec64() const noexcept { return isReg(RegType::kVec64); }
  //! Tests whether the register is a 128-bit vector register or view (AArch32, AArch64, X86, X86_64).
  ASMJIT_INLINE_NODEBUG constexpr bool isVec128() const noexcept { return isReg(RegType::kVec128); }
  //! Tests whether the register is a 256-bit vector register or view (X86, X86_64).
  ASMJIT_INLINE_NODEBUG constexpr bool isVec256() const noexcept { return isReg(RegType::kVec256); }
  //! Tests whether the register is a 512-bit vector register or view (X86, X86_64).
  ASMJIT_INLINE_NODEBUG constexpr bool isVec512() const noexcept { return isReg(RegType::kVec512); }

  //! Tests whether the register is a mask register of any size.
  ASMJIT_INLINE_NODEBUG constexpr bool isMask() const noexcept { return isReg(RegGroup::kMask); }

  //! Tests whether the operand is a register matching the given register `type`.
  ASMJIT_INLINE_NODEBUG constexpr bool isRegList(RegType type) const noexcept {
    return _signature.subset(Signature::kOpTypeMask | Signature::kRegTypeMask) == (Signature::fromOpType(OperandType::kRegList) | Signature::fromRegType(type));
  }

  //! Tests whether the operand is a register or memory.
  //!
  //! \note This is useful on X86 and X86_64 architectures as many instructions support Reg/Mem operand combination.
  //! So if the user code works with just \ref Operand, it's possible to check whether the operand is either a register
  //! or memory location with a single check.
  ASMJIT_INLINE_NODEBUG constexpr bool isRegOrMem() const noexcept {
    return Support::isBetween<uint32_t>(uint32_t(opType()), uint32_t(OperandType::kReg), uint32_t(OperandType::kMem));
  }

  //! Tests whether the operand is a register, register-list, or memory.
  //!
  //! \note This is useful on 32-bit ARM architecture to check whether an operand references a register. It can be
  //! used in other architectures too, but it would work identically to \ref isRegOrMem() as other architectures
  //! don't provide register lists.
  ASMJIT_INLINE_NODEBUG constexpr bool isRegOrRegListOrMem() const noexcept {
    return Support::isBetween<uint32_t>(uint32_t(opType()), uint32_t(OperandType::kReg), uint32_t(OperandType::kRegList));
  }

  //! \}

  //! \name Accessors (X86 Specific)
  //! \{

  //! Returns a size of a register or an X86 memory operand.
  //!
  //! \remarks At the moment only X86 and X86_64 memory operands have a size - other memory operands can use bits
  //! that represent size as an additional payload. This means that memory size is architecture specific and should
  //! be accessed via \ref x86::Mem::size(). Sometimes when the user knows that the operand is either a register or
  //! memory operand this function can be helpful as it avoids casting, but it only works when it targets X86 and X86_64.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t x86RmSize() const noexcept { return _signature.size(); }

  //! \}
};

//! Base class representing an operand in AsmJit (default constructed version).
class Operand : public Operand_ {
public:
  //! \name Construction & Destruction
  //! \{

  //! Creates `kOpNone` operand having all members initialized to zero.
  ASMJIT_INLINE_NODEBUG constexpr Operand() noexcept
    : Operand_{ Signature::fromOpType(OperandType::kNone), 0u, { 0u, 0u }} {}

  //! Creates a cloned `other` operand.
  ASMJIT_INLINE_NODEBUG constexpr Operand(const Operand& other) noexcept = default;

  //! Creates a cloned `other` operand.
  ASMJIT_INLINE_NODEBUG constexpr explicit Operand(const Operand_& other)
    : Operand_(other) {}

  //! Creates an operand initialized to raw `[u0, u1, u2, u3]` values.
  ASMJIT_INLINE_NODEBUG constexpr Operand(Globals::Init_, const Signature& u0, uint32_t u1, uint32_t u2, uint32_t u3) noexcept
    : Operand_{{u0._bits}, u1, {u2, u3}} {}

  //! Creates an uninitialized operand (dangerous).
  ASMJIT_INLINE_NODEBUG explicit Operand(Globals::NoInit_) noexcept {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG Operand& operator=(const Operand& other) noexcept = default;
  ASMJIT_INLINE_NODEBUG Operand& operator=(const Operand_& other) noexcept { return operator=(static_cast<const Operand&>(other)); }

  //! \}

  //! \name Clone
  //! \{

  //! Clones this operand and returns its copy.
  ASMJIT_INLINE_NODEBUG constexpr Operand clone() const noexcept { return Operand(*this); }

  //! \}
};

static_assert(sizeof(Operand) == 16, "asmjit::Operand must be exactly 16 bytes long");

//! Label (jump target or data location).
//!
//! Label represents a location in code typically used as a jump target, but may be also a reference to some data or
//! a static variable. Label has to be explicitly created by BaseEmitter.
//!
//! Example of using labels:
//!
//! ```
//! // Create some emitter (for example x86::Assembler).
//! x86::Assembler a;
//!
//! // Create Label instance.
//! Label L1 = a.newLabel();
//!
//! // ... your code ...
//!
//! // Using label.
//! a.jump(L1);
//!
//! // ... your code ...
//!
//! // Bind label to the current position, see `BaseEmitter::bind()`.
//! a.bind(L1);
//! ```
class Label : public Operand {
public:
  //! \name Construction & Destruction
  //! \{

  //! Creates a label operand without ID (you must set the ID to make it valid).
  ASMJIT_INLINE_NODEBUG constexpr Label() noexcept
    : Operand(Globals::Init, Signature::fromOpType(OperandType::kLabel), Globals::kInvalidId, 0, 0) {}

  //! Creates a cloned label operand of `other`.
  ASMJIT_INLINE_NODEBUG constexpr Label(const Label& other) noexcept
    : Operand(other) {}

  //! Creates a label operand of the given `id`.
  ASMJIT_INLINE_NODEBUG constexpr explicit Label(uint32_t id) noexcept
    : Operand(Globals::Init, Signature::fromOpType(OperandType::kLabel), id, 0, 0) {}

  ASMJIT_INLINE_NODEBUG explicit Label(Globals::NoInit_) noexcept
    : Operand(Globals::NoInit) {}

  //! Resets the label, will reset all properties and set its ID to `Globals::kInvalidId`.
  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _signature = Signature::fromOpType(OperandType::kLabel);
    _baseId = Globals::kInvalidId;
    _data[0] = 0;
    _data[1] = 0;
  }

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG Label& operator=(const Label& other) noexcept = default;

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the label was created by CodeHolder and/or an attached emitter.
  ASMJIT_INLINE_NODEBUG constexpr bool isValid() const noexcept { return _baseId != Globals::kInvalidId; }
  //! Sets the label `id`.
  ASMJIT_INLINE_NODEBUG void setId(uint32_t id) noexcept { _baseId = id; }

  //! \}
};

//! \cond INTERNAL
//! Default register traits.
struct BaseRegTraits {
  enum : uint32_t {
    //! \ref TypeId representing this register type, could be \ref TypeId::kVoid if such type doesn't exist.
    kTypeId = uint32_t(TypeId::kVoid),
    //! RegType is not valid by default.
    kValid = 0,

    //! Zero type by default (defaults to None).
    kType = uint32_t(RegType::kNone),
    //! Zero group by default (defaults to GP).
    kGroup = uint32_t(RegGroup::kGp),
    //! No size by default.
    kSize = 0,

    //! Empty signature by default (not even having operand type set to register).
    kSignature = 0
  };
};
//! \endcond

//! Physical or virtual register operand (base).
class BaseReg : public Operand {
public:
  //! \name Constants
  //! \{

  enum : uint32_t {
    //! None or any register (mostly internal).
    kIdBad = 0xFFu,

    kBaseSignatureMask =
      Signature::kOpTypeMask   |
      Signature::kRegTypeMask  |
      Signature::kRegGroupMask |
      Signature::kSizeMask,

    kTypeNone = uint32_t(RegType::kNone),
    kSignature = Signature::fromOpType(OperandType::kReg).bits()
  };

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a dummy register operand.
  ASMJIT_INLINE_NODEBUG constexpr BaseReg() noexcept
    : Operand(Globals::Init, Signature::fromOpType(OperandType::kReg), kIdBad, 0, 0) {}

  //! Creates a new register operand which is the same as `other` .
  ASMJIT_INLINE_NODEBUG constexpr BaseReg(const BaseReg& other) noexcept
    : Operand(other) {}

  //! Creates a new register operand compatible with `other`, but with a different `id`.
  ASMJIT_INLINE_NODEBUG constexpr BaseReg(const BaseReg& other, uint32_t id) noexcept
    : Operand(Globals::Init, other._signature, id, 0, 0) {}

  //! Creates a register initialized to the given `signature` and `id`.
  ASMJIT_INLINE_NODEBUG constexpr BaseReg(const Signature& signature, uint32_t id) noexcept
    : Operand(Globals::Init, signature, id, 0, 0) {}

  ASMJIT_INLINE_NODEBUG explicit BaseReg(Globals::NoInit_) noexcept
    : Operand(Globals::NoInit) {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG BaseReg& operator=(const BaseReg& other) noexcept = default;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns base signature of the register associated with each register type.
  //!
  //! Base signature only contains the operand type, register type, register group, and register size. It doesn't
  //! contain element type, predicate, or other architecture-specific data. Base signature is a signature that is
  //! provided by architecture-specific `RegTraits`, like \ref x86::RegTraits.
  ASMJIT_INLINE_NODEBUG constexpr OperandSignature baseSignature() const noexcept { return _signature & kBaseSignatureMask; }

  //! Tests whether the operand's base signature matches the given signature `sign`.
  ASMJIT_INLINE_NODEBUG constexpr bool hasBaseSignature(uint32_t signature) const noexcept { return baseSignature() == signature; }
  //! Tests whether the operand's base signature matches the given signature `sign`.
  ASMJIT_INLINE_NODEBUG constexpr bool hasBaseSignature(const OperandSignature& signature) const noexcept { return baseSignature() == signature; }
  //! Tests whether the operand's base signature matches the base signature of the `other` operand.
  ASMJIT_INLINE_NODEBUG constexpr bool hasBaseSignature(const BaseReg& other) const noexcept { return baseSignature() == other.baseSignature(); }

  //! Tests whether this register is the same as `other`.
  //!
  //! This is just an optimization. Registers by default only use the first 8 bytes of Operand data, so this method
  //! takes advantage of this knowledge and only compares these 8 bytes. If both operands were created correctly
  //! both \ref equals() and \ref isSame() should give the same answer, however, if any of these two contains garbage
  //! or other metadata in the upper 8 bytes then \ref isSame() may return `true` in cases in which \ref equals()
  //! returns false.
  ASMJIT_INLINE_NODEBUG constexpr bool isSame(const BaseReg& other) const noexcept {
    return (_signature == other._signature) & (_baseId == other._baseId);
  }

  //! Tests whether the register is valid (either virtual or physical).
  ASMJIT_INLINE_NODEBUG constexpr bool isValid() const noexcept { return bool(unsigned(_signature != 0) & unsigned(_baseId != kIdBad)); }

  //! Tests whether this is a physical register.
  ASMJIT_INLINE_NODEBUG constexpr bool isPhysReg() const noexcept { return _baseId < kIdBad; }
  //! Tests whether this is a virtual register.
  ASMJIT_INLINE_NODEBUG constexpr bool isVirtReg() const noexcept { return _baseId > kIdBad; }

  //! Tests whether the register type matches `type` - same as `isReg(type)`, provided for convenience.
  ASMJIT_INLINE_NODEBUG constexpr bool isType(RegType type) const noexcept { return _signature.subset(Signature::kRegTypeMask) == Signature::fromRegType(type); }
  //! Tests whether the register group matches `group`.
  ASMJIT_INLINE_NODEBUG constexpr bool isGroup(RegGroup group) const noexcept { return _signature.subset(Signature::kRegGroupMask) == Signature::fromRegGroup(group); }

  //! Tests whether the register is a general purpose register (any size).
  ASMJIT_INLINE_NODEBUG constexpr bool isGp() const noexcept { return isGroup(RegGroup::kGp); }
  //! Tests whether the register is a vector register of any size.
  ASMJIT_INLINE_NODEBUG constexpr bool isVec() const noexcept { return isGroup(RegGroup::kVec); }
  //! Tests whether the register is a mask register of any size.
  ASMJIT_INLINE_NODEBUG constexpr bool isMask() const noexcept { return isGroup(RegGroup::kMask); }

  using Operand_::isReg;

  //! Same as `isType()`, provided for convenience.
  ASMJIT_INLINE_NODEBUG constexpr bool isReg(RegType rType) const noexcept { return isType(rType); }
  //! Tests whether the register type matches `type` and register id matches `id`.
  ASMJIT_INLINE_NODEBUG constexpr bool isReg(RegType rType, uint32_t id) const noexcept { return isType(rType) && this->id() == id; }

  //! Returns the register type.
  ASMJIT_INLINE_NODEBUG constexpr RegType type() const noexcept { return _signature.regType(); }
  //! Returns the register group.
  ASMJIT_INLINE_NODEBUG constexpr RegGroup group() const noexcept { return _signature.regGroup(); }

  //! Tests whether the register specifies a size (i.e. the size is not zero).
  ASMJIT_INLINE_NODEBUG constexpr bool hasSize() const noexcept { return _signature.hasField<Signature::kSizeMask>(); }
  //! Tests whether the register size matches size `s`.
  ASMJIT_INLINE_NODEBUG constexpr bool hasSize(uint32_t s) const noexcept { return size() == s; }

  //! Returns the size of the register in bytes. If the register size depends on architecture (like `x86::CReg` and
  //! `x86::DReg`) the size returned should be the greatest possible (so it should return 64-bit size in such case).
  ASMJIT_INLINE_NODEBUG constexpr uint32_t size() const noexcept { return _signature.getField<Signature::kSizeMask>(); }

  //! Returns operation predicate of the register (ARM/AArch64).
  //!
  //! The meaning depends on architecture, for example on ARM hardware this describes \ref arm::ShiftOp
  //! of the register.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t predicate() const noexcept { return _signature.getField<Signature::kPredicateMask>(); }

  //! Sets operation predicate of the register to `predicate` (ARM/AArch64).
  //!
  //! The meaning depends on architecture, for example on ARM hardware this describes \ref arm::ShiftOp
  //! of the register.
  ASMJIT_INLINE_NODEBUG void setPredicate(uint32_t predicate) noexcept { _signature.setField<Signature::kPredicateMask>(predicate); }

  //! Resets shift operation type of the register to the default value (ARM/AArch64).
  ASMJIT_INLINE_NODEBUG void resetPredicate() noexcept { _signature.setField<Signature::kPredicateMask>(0); }

  //! Clones the register operand.
  ASMJIT_INLINE_NODEBUG constexpr BaseReg clone() const noexcept { return BaseReg(*this); }

  //! Casts this register to `RegT` by also changing its signature.
  //!
  //! \note Improper use of `cloneAs()` can lead to hard-to-debug errors.
  template<typename RegT>
  ASMJIT_INLINE_NODEBUG constexpr RegT cloneAs() const noexcept { return RegT(Signature(RegT::kSignature), id()); }

  //! Casts this register to `other` by also changing its signature.
  //!
  //! \note Improper use of `cloneAs()` can lead to hard-to-debug errors.
  template<typename RegT>
  ASMJIT_INLINE_NODEBUG constexpr RegT cloneAs(const RegT& other) const noexcept { return RegT(other.signature(), id()); }

  //! Sets the register id to `id`.
  ASMJIT_INLINE_NODEBUG void setId(uint32_t id) noexcept { _baseId = id; }

  //! Sets a 32-bit operand signature based on traits of `RegT`.
  template<typename RegT>
  ASMJIT_INLINE_NODEBUG void setSignatureT() noexcept { _signature = RegT::kSignature; }

  //! Sets the register `signature` and `id`.
  ASMJIT_INLINE_NODEBUG void setSignatureAndId(const OperandSignature& signature, uint32_t id) noexcept {
    _signature = signature;
    _baseId = id;
  }

  //! \}

  //! \name Static Functions
  //! \{

  //! Tests whether the `op` operand is a general purpose register.
  static ASMJIT_INLINE_NODEBUG bool isGp(const Operand_& op) noexcept {
    // Check operand type and register group. Not interested in register type and size.
    return op.signature().subset(Signature::kOpTypeMask | Signature::kRegGroupMask) == (Signature::fromOpType(OperandType::kReg) | Signature::fromRegGroup(RegGroup::kGp));
  }

  //! Tests whether the `op` operand is a vector register.
  static ASMJIT_INLINE_NODEBUG bool isVec(const Operand_& op) noexcept {
    // Check operand type and register group. Not interested in register type and size.
    return op.signature().subset(Signature::kOpTypeMask | Signature::kRegGroupMask) == (Signature::fromOpType(OperandType::kReg) | Signature::fromRegGroup(RegGroup::kVec));
  }

  //! Tests whether the `op` is a general purpose register of the given `id`.
  static ASMJIT_INLINE_NODEBUG bool isGp(const Operand_& op, uint32_t id) noexcept { return bool(unsigned(isGp(op)) & unsigned(op.id() == id)); }
  //! Tests whether the `op` is a vector register of the given `id`.
  static ASMJIT_INLINE_NODEBUG bool isVec(const Operand_& op, uint32_t id) noexcept { return bool(unsigned(isVec(op)) & unsigned(op.id() == id)); }

  //! \}
};

//! RegOnly is 8-byte version of `BaseReg` that allows to store either register or nothing.
//!
//! It's designed to decrease the space consumed by an extra "operand" in \ref BaseEmitter and \ref InstNode.
struct RegOnly {
  //! \name Types
  //! \{

  typedef OperandSignature Signature;

  //! \}

  //! Operand signature - only \ref OperandType::kNone and \ref OperandType::kReg are supported.
  Signature _signature;
  //! Physical or virtual register id.
  uint32_t _id;

  //! \name Construction & Destruction
  //! \{

  //! Initializes the `RegOnly` instance to hold register `signature` and `id`.
  ASMJIT_INLINE_NODEBUG void init(const OperandSignature& signature, uint32_t id) noexcept {
    _signature = signature;
    _id = id;
  }

  ASMJIT_INLINE_NODEBUG void init(const BaseReg& reg) noexcept { init(reg.signature(), reg.id()); }
  ASMJIT_INLINE_NODEBUG void init(const RegOnly& reg) noexcept { init(reg.signature(), reg.id()); }

  //! Resets the `RegOnly` members to zeros (none).
  ASMJIT_INLINE_NODEBUG void reset() noexcept { init(Signature::fromBits(0), 0); }

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether this ExtraReg is none (same as calling `Operand_::isNone()`).
  ASMJIT_INLINE_NODEBUG constexpr bool isNone() const noexcept { return _signature == 0; }
  //! Tests whether the register is valid (either virtual or physical).
  ASMJIT_INLINE_NODEBUG constexpr bool isReg() const noexcept { return _signature != 0; }

  //! Tests whether this is a physical register.
  ASMJIT_INLINE_NODEBUG constexpr bool isPhysReg() const noexcept { return _id < BaseReg::kIdBad; }
  //! Tests whether this is a virtual register (used by `BaseCompiler`).
  ASMJIT_INLINE_NODEBUG constexpr bool isVirtReg() const noexcept { return _id > BaseReg::kIdBad; }

  //! Returns the register signature or 0 if no register is assigned.
  ASMJIT_INLINE_NODEBUG constexpr OperandSignature signature() const noexcept { return _signature; }
  //! Returns the register id.
  //!
  //! \note Always check whether the register is assigned before using the returned identifier as
  //! non-assigned `RegOnly` instance would return zero id, which is still a valid register id.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t id() const noexcept { return _id; }

  //! Sets the register id.
  ASMJIT_INLINE_NODEBUG void setId(uint32_t id) noexcept { _id = id; }

  //! Returns the register type.
  ASMJIT_INLINE_NODEBUG constexpr RegType type() const noexcept { return _signature.regType(); }
  //! Returns the register group.
  ASMJIT_INLINE_NODEBUG constexpr RegGroup group() const noexcept { return _signature.regGroup(); }

  //! \}

  //! \name Utilities
  //! \{

  //! Converts this ExtraReg to a real `RegT` operand.
  template<typename RegT>
  ASMJIT_INLINE_NODEBUG constexpr RegT toReg() const noexcept { return RegT(_signature, _id); }

  //! \}
};

//! \cond INTERNAL
//! Adds a template specialization for `REG_TYPE` into the local `RegTraits`.
#define ASMJIT_DEFINE_REG_TRAITS(REG_TYPE, GROUP, SIZE, TYPE_ID)                         \
template<>                                                                               \
struct RegTraits<REG_TYPE> {                                                             \
  static constexpr uint32_t kValid = 1;                                                  \
  static constexpr RegType kType = REG_TYPE;                                             \
  static constexpr RegGroup kGroup = GROUP;                                              \
  static constexpr uint32_t kSize = SIZE;                                                \
  static constexpr TypeId kTypeId = TYPE_ID;                                             \
                                                                                         \
  static constexpr uint32_t kSignature =                                                 \
    (OperandSignature::fromOpType(OperandType::kReg) |                                   \
     OperandSignature::fromRegType(kType)            |                                   \
     OperandSignature::fromRegGroup(kGroup)          |                                   \
     OperandSignature::fromSize(kSize)).bits();                                          \
                                                                                         \
}

//! Adds constructors and member functions to a class that implements abstract register. Abstract register is register
//! that doesn't have type or signature yet, it's a base class like `x86::Reg` or `arm::Reg`.
#define ASMJIT_DEFINE_ABSTRACT_REG(REG, BASE)                                            \
public:                                                                                  \
  /*! Default constructor that only setups basics. */                                    \
  ASMJIT_INLINE_NODEBUG constexpr REG() noexcept                                         \
    : BASE(Signature{kSignature}, kIdBad) {}                                             \
                                                                                         \
  /*! Makes a copy of the `other` register operand. */                                   \
  ASMJIT_INLINE_NODEBUG constexpr REG(const REG& other) noexcept                         \
    : BASE(other) {}                                                                     \
                                                                                         \
  /*! Makes a copy of the `other` register having id set to `id` */                      \
  ASMJIT_INLINE_NODEBUG constexpr REG(const BaseReg& other, uint32_t id) noexcept        \
    : BASE(other, id) {}                                                                 \
                                                                                         \
  /*! Creates a register based on `signature` and `id`. */                               \
  ASMJIT_INLINE_NODEBUG constexpr REG(const OperandSignature& sgn, uint32_t id) noexcept \
    : BASE(sgn, id) {}                                                                   \
                                                                                         \
  /*! Creates a completely uninitialized REG register operand (garbage). */              \
  ASMJIT_INLINE_NODEBUG explicit REG(Globals::NoInit_) noexcept                          \
    : BASE(Globals::NoInit) {}                                                           \
                                                                                         \
  /*! Creates a new register from register type and id. */                               \
  static ASMJIT_INLINE_NODEBUG REG fromTypeAndId(RegType type, uint32_t id) noexcept {   \
    return REG(signatureOf(type), id);                                                   \
  }                                                                                      \
                                                                                         \
  /*! Clones the register operand. */                                                    \
  ASMJIT_INLINE_NODEBUG constexpr REG clone() const noexcept { return REG(*this); }      \
                                                                                         \
  ASMJIT_INLINE_NODEBUG REG& operator=(const REG& other) noexcept = default;

//! Adds constructors and member functions to a class that implements final register. Final registers MUST HAVE a valid
//! signature.
#define ASMJIT_DEFINE_FINAL_REG(REG, BASE, TRAITS)                                       \
public:                                                                                  \
  static constexpr RegType kThisType = TRAITS::kType;                                    \
  static constexpr RegGroup kThisGroup = TRAITS::kGroup;                                 \
  static constexpr uint32_t kThisSize  = TRAITS::kSize;                                  \
  static constexpr uint32_t kSignature = TRAITS::kSignature;                             \
                                                                                         \
  ASMJIT_DEFINE_ABSTRACT_REG(REG, BASE)                                                  \
                                                                                         \
  /*! Creates a register operand having its id set to `id`. */                           \
  ASMJIT_INLINE_NODEBUG constexpr explicit REG(uint32_t id) noexcept                     \
    : BASE(Signature{kSignature}, id) {}
//! \endcond

//! List of physical registers (base).
//!
//! \note List of registers is only used by some ARM instructions at the moment.
class BaseRegList : public Operand {
public:
  //! \name Constants
  //! \{

  enum : uint32_t {
    kSignature = Signature::fromOpType(OperandType::kRegList).bits()
  };

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a dummy register operand.
  ASMJIT_INLINE_NODEBUG constexpr BaseRegList() noexcept
    : Operand(Globals::Init, Signature::fromOpType(OperandType::kRegList), 0, 0, 0) {}

  //! Creates a new register operand which is the same as `other` .
  ASMJIT_INLINE_NODEBUG constexpr BaseRegList(const BaseRegList& other) noexcept
    : Operand(other) {}

  //! Creates a new register operand compatible with `other`, but with a different `id`.
  ASMJIT_INLINE_NODEBUG constexpr BaseRegList(const BaseRegList& other, RegMask regMask) noexcept
    : Operand(Globals::Init, other._signature, regMask, 0, 0) {}

  //! Creates a register initialized to the given `signature` and `id`.
  ASMJIT_INLINE_NODEBUG constexpr BaseRegList(const Signature& signature, RegMask regMask) noexcept
    : Operand(Globals::Init, signature, regMask, 0, 0) {}

  ASMJIT_INLINE_NODEBUG explicit BaseRegList(Globals::NoInit_) noexcept
    : Operand(Globals::NoInit) {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG BaseRegList& operator=(const BaseRegList& other) noexcept = default;

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the register-list is valid, which means it has a type and at least a single register in the list.
  ASMJIT_INLINE_NODEBUG constexpr bool isValid() const noexcept { return bool(unsigned(_signature != 0u) & unsigned(_baseId != 0u)); }

  //! Tests whether the register type matches `type` - same as `isReg(type)`, provided for convenience.
  ASMJIT_INLINE_NODEBUG constexpr bool isType(RegType type) const noexcept { return _signature.subset(Signature::kRegTypeMask) == Signature::fromRegType(type); }
  //! Tests whether the register group matches `group`.
  ASMJIT_INLINE_NODEBUG constexpr bool isGroup(RegGroup group) const noexcept { return _signature.subset(Signature::kRegGroupMask) == Signature::fromRegGroup(group); }

  //! Tests whether the register is a general purpose register (any size).
  ASMJIT_INLINE_NODEBUG constexpr bool isGp() const noexcept { return isGroup(RegGroup::kGp); }
  //! Tests whether the register is a vector register.
  ASMJIT_INLINE_NODEBUG constexpr bool isVec() const noexcept { return isGroup(RegGroup::kVec); }

  //! Returns the register type.
  ASMJIT_INLINE_NODEBUG constexpr RegType type() const noexcept { return _signature.regType(); }
  //! Returns the register group.
  ASMJIT_INLINE_NODEBUG constexpr RegGroup group() const noexcept { return _signature.regGroup(); }
  //! Returns the size of a single register in this register-list or 0 if unspecified.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t size() const noexcept { return _signature.getField<Signature::kSizeMask>(); }

  //! Returns the register list as a mask, where each bit represents one physical register.
  ASMJIT_INLINE_NODEBUG constexpr RegMask list() const noexcept { return _baseId; }
  //! Sets the register list to `mask`.
  ASMJIT_INLINE_NODEBUG void setList(RegMask mask) noexcept { _baseId = mask; }
  //! Remoes all registers from the register-list by making the underlying register-mask zero.
  ASMJIT_INLINE_NODEBUG void resetList() noexcept { _baseId = 0; }

  //! Adds registers passed by a register `mask` to the register-list.
  ASMJIT_INLINE_NODEBUG void addList(RegMask mask) noexcept { _baseId |= mask; }
  //! Removes registers passed by a register `mask` to the register-list.
  ASMJIT_INLINE_NODEBUG void clearList(RegMask mask) noexcept { _baseId &= ~mask; }
  //! Uses AND operator to combine the current register-list with other register `mask`.
  ASMJIT_INLINE_NODEBUG void andList(RegMask mask) noexcept { _baseId &= mask; }
  //! Uses XOR operator to combine the current register-list with other register `mask`.
  ASMJIT_INLINE_NODEBUG void xorList(RegMask mask) noexcept { _baseId ^= mask; }

  //! Checks whether a physical register `physId` is in the register-list.
  ASMJIT_INLINE_NODEBUG bool hasReg(uint32_t physId) const noexcept { return physId < 32u ? (_baseId & (1u << physId)) != 0 : false; }
  //! Adds a physical register `physId` to the register-list.
  ASMJIT_INLINE_NODEBUG void addReg(uint32_t physId) noexcept { addList(1u << physId); }
  //! Removes a physical register `physId` from the register-list.
  ASMJIT_INLINE_NODEBUG void clearReg(uint32_t physId) noexcept { clearList(1u << physId); }

  //! Clones the register-list operand.
  ASMJIT_INLINE_NODEBUG constexpr BaseRegList clone() const noexcept { return BaseRegList(*this); }

  //! Casts this register to `RegT` by also changing its signature.
  //!
  //! \note Improper use of `cloneAs()` can lead to hard-to-debug errors.
  template<typename RegListT>
  ASMJIT_INLINE_NODEBUG constexpr RegListT cloneAs() const noexcept { return RegListT(Signature(RegListT::kSignature), list()); }

  //! Casts this register to `other` by also changing its signature.
  //!
  //! \note Improper use of `cloneAs()` can lead to hard-to-debug errors.
  template<typename RegListT>
  ASMJIT_INLINE_NODEBUG constexpr RegListT cloneAs(const RegListT& other) const noexcept { return RegListT(other.signature(), list()); }

  //! \}
};

template<typename RegT>
class RegListT : public BaseRegList {
public:
  //! \name Construction & Destruction
  //! \{

  //! Creates a dummy register operand.
  ASMJIT_INLINE_NODEBUG constexpr RegListT() noexcept
    : BaseRegList() {}

  //! Creates a new register operand which is the same as `other` .
  ASMJIT_INLINE_NODEBUG constexpr RegListT(const RegListT& other) noexcept
    : BaseRegList(other) {}

  //! Creates a new register operand compatible with `other`, but with a different `id`.
  ASMJIT_INLINE_NODEBUG constexpr RegListT(const RegListT& other, RegMask regMask) noexcept
    : BaseRegList(other, regMask) {}

  //! Creates a register initialized to the given `signature` and `id`.
  ASMJIT_INLINE_NODEBUG constexpr RegListT(const Signature& signature, RegMask regMask) noexcept
    : BaseRegList(signature, regMask) {}

  //! Creates a register initialized to the given `signature` and `regs`.
  ASMJIT_INLINE_NODEBUG RegListT(const Signature& signature, std::initializer_list<RegT> regs) noexcept
    : BaseRegList(signature, RegMask(0)) { addRegs(regs); }

  ASMJIT_INLINE_NODEBUG explicit RegListT(Globals::NoInit_) noexcept
    : BaseRegList(Globals::NoInit) {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG RegListT& operator=(const RegListT& other) noexcept = default;

  //! \}

  //! \name Accessors
  //! \{

  using BaseRegList::addList;
  using BaseRegList::clearList;
  using BaseRegList::andList;
  using BaseRegList::xorList;

  //! Adds registers to this register-list as provided by `other` register-list.
  ASMJIT_INLINE_NODEBUG void addList(const RegListT<RegT>& other) noexcept { addList(other.list()); }
  //! Removes registers contained in `other` register-list.
  ASMJIT_INLINE_NODEBUG void clearList(const RegListT<RegT>& other) noexcept { clearList(other.list()); }
  //! Uses AND operator to combine the current register-list with `other` register-list.
  ASMJIT_INLINE_NODEBUG void andList(const RegListT<RegT>& other) noexcept { andList(other.list()); }
  //! Uses XOR operator to combine the current register-list with `other` register-list.
  ASMJIT_INLINE_NODEBUG void xorList(const RegListT<RegT>& other) noexcept { xorList(other.list()); }

  using BaseRegList::addReg;
  using BaseRegList::clearReg;

  ASMJIT_INLINE_NODEBUG void addReg(const RegT& reg) noexcept {
    if (reg.id() < 32u)
      addReg(reg.id());
  }

  ASMJIT_INLINE_NODEBUG void addRegs(std::initializer_list<RegT> regs) noexcept {
    for (const RegT& reg : regs)
      addReg(reg);
  }

  ASMJIT_INLINE_NODEBUG void clearReg(const RegT& reg) noexcept {
    if (reg.id() < 32u)
      clearReg(reg.id());
  }

  ASMJIT_INLINE_NODEBUG void clearRegs(std::initializer_list<RegT> regs) noexcept {
    for (const RegT& reg : regs)
      clearReg(reg);
  }

  //! \}
};

//! Base class for all memory operands.
//!
//! The data is split into the following parts:
//!
//!   - BASE - Base register or label - requires 36 bits total. 4 bits are used to encode the type of the BASE operand
//!     (label vs. register type) and the remaining 32 bits define the BASE id, which can be a physical or virtual
//!     register index. If BASE type is zero, which is never used as a register type and label doesn't use it as well
//!     then BASE field contains a high DWORD of a possible 64-bit absolute address, which is possible on X64.
//!
//!   - INDEX - Index register (or theoretically Label, which doesn't make sense). Encoding is similar to BASE - it
//!     also requires 36 bits and splits the encoding to INDEX type (4 bits defining the register type) and 32-bit id.
//!
//!   - OFFSET - A relative offset of the address. Basically if BASE is specified the relative displacement adjusts
//!     BASE and an optional INDEX. if BASE is not specified then the OFFSET should be considered as ABSOLUTE address
//!     (at least on X86). In that case its low 32 bits are stored in DISPLACEMENT field and the remaining high 32
//!     bits are stored in BASE.
//!
//!   - OTHER - There is rest 8 bits that can be used for whatever purpose. For example \ref x86::Mem operand uses
//!     these bits to store segment override prefix and index shift (or scale).
class BaseMem : public Operand {
public:
  //! \name Construction & Destruction
  //! \{

  //! Creates a default `BaseMem` operand, that points to [0].
  ASMJIT_INLINE_NODEBUG constexpr BaseMem() noexcept
      : Operand(Globals::Init, Signature::fromOpType(OperandType::kMem), 0, 0, 0) {}

  //! Creates a `BaseMem` operand that is a clone of `other`.
  ASMJIT_INLINE_NODEBUG constexpr BaseMem(const BaseMem& other) noexcept
    : Operand(other) {}

  //! Creates a `BaseMem` operand from `baseReg` and `offset`.
  //!
  //! \note This is an architecture independent constructor that can be used to create an architecture
  //! independent memory operand to be used in portable code that can handle multiple architectures.
  ASMJIT_INLINE_NODEBUG constexpr explicit BaseMem(const BaseReg& baseReg, int32_t offset = 0) noexcept
    : Operand(Globals::Init,
              Signature::fromOpType(OperandType::kMem) | Signature::fromMemBaseType(baseReg.type()),
              baseReg.id(),
              0,
              uint32_t(offset)) {}

  //! \cond INTERNAL
  //! Creates a `BaseMem` operand from 4 integers as used by `Operand_` struct.
  ASMJIT_INLINE_NODEBUG constexpr BaseMem(const OperandSignature& u0, uint32_t baseId, uint32_t indexId, int32_t offset) noexcept
    : Operand(Globals::Init, u0, baseId, indexId, uint32_t(offset)) {}
  //! \endcond

  //! Creates a completely uninitialized `BaseMem` operand.
  ASMJIT_INLINE_NODEBUG explicit BaseMem(Globals::NoInit_) noexcept
    : Operand(Globals::NoInit) {}

  //! Resets the memory operand - after the reset the memory points to [0].
  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _signature = Signature::fromOpType(OperandType::kMem);
    _baseId = 0;
    _data[0] = 0;
    _data[1] = 0;
  }

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG BaseMem& operator=(const BaseMem& other) noexcept { copyFrom(other); return *this; }

  //! \}

  //! \name Accessors
  //! \{

  //! Clones the memory operand.
  ASMJIT_INLINE_NODEBUG constexpr BaseMem clone() const noexcept { return BaseMem(*this); }

  //! Creates a new copy of this memory operand adjusted by `off`.
  ASMJIT_INLINE_NODEBUG BaseMem cloneAdjusted(int64_t off) const noexcept {
    BaseMem result(*this);
    result.addOffset(off);
    return result;
  }

  //! Tests whether this memory operand is a register home (only used by \ref asmjit_compiler)
  ASMJIT_INLINE_NODEBUG constexpr bool isRegHome() const noexcept { return _signature.hasField<Signature::kMemRegHomeFlag>(); }
  //! Mark this memory operand as register home (only used by \ref asmjit_compiler).
  ASMJIT_INLINE_NODEBUG void setRegHome() noexcept { _signature |= Signature::kMemRegHomeFlag; }
  //! Marks this operand to not be a register home (only used by \ref asmjit_compiler).
  ASMJIT_INLINE_NODEBUG void clearRegHome() noexcept { _signature &= ~Signature::kMemRegHomeFlag; }

  //! Tests whether the memory operand has a BASE register or label specified.
  ASMJIT_INLINE_NODEBUG constexpr bool hasBase() const noexcept {
    return (_signature & Signature::kMemBaseTypeMask) != 0;
  }

  //! Tests whether the memory operand has an INDEX register specified.
  ASMJIT_INLINE_NODEBUG constexpr bool hasIndex() const noexcept {
    return (_signature & Signature::kMemIndexTypeMask) != 0;
  }

  //! Tests whether the memory operand has BASE or INDEX register.
  ASMJIT_INLINE_NODEBUG constexpr bool hasBaseOrIndex() const noexcept {
    return (_signature & Signature::kMemBaseIndexMask) != 0;
  }

  //! Tests whether the memory operand has BASE and INDEX register.
  ASMJIT_INLINE_NODEBUG constexpr bool hasBaseAndIndex() const noexcept {
    return (_signature & Signature::kMemBaseTypeMask) != 0 && (_signature & Signature::kMemIndexTypeMask) != 0;
  }

  //! Tests whether the BASE operand is a label.
  ASMJIT_INLINE_NODEBUG constexpr bool hasBaseLabel() const noexcept {
    return _signature.subset(Signature::kMemBaseTypeMask) == Signature::fromMemBaseType(RegType::kLabelTag);
  }

  //! Tests whether the BASE operand is a register (registers start after `RegType::kLabelTag`).
  ASMJIT_INLINE_NODEBUG constexpr bool hasBaseReg() const noexcept {
    return _signature.subset(Signature::kMemBaseTypeMask).bits() > Signature::fromMemBaseType(RegType::kLabelTag).bits();
  }

  //! Tests whether the INDEX operand is a register (registers start after `RegType::kLabelTag`).
  ASMJIT_INLINE_NODEBUG constexpr bool hasIndexReg() const noexcept {
    return _signature.subset(Signature::kMemIndexTypeMask).bits() > Signature::fromMemIndexType(RegType::kLabelTag).bits();
  }

  //! Returns the type of the BASE register (0 if this memory operand doesn't use the BASE register).
  //!
  //! \note If the returned type is one (a value never associated to a register type) the BASE is not register, but it
  //! is a label. One equals to `kLabelTag`. You should always check `hasBaseLabel()` before using `baseId()` result.
  ASMJIT_INLINE_NODEBUG constexpr RegType baseType() const noexcept { return _signature.memBaseType(); }

  //! Returns the type of an INDEX register (0 if this memory operand doesn't
  //! use the INDEX register).
  ASMJIT_INLINE_NODEBUG constexpr RegType indexType() const noexcept { return _signature.memIndexType(); }

  //! This is used internally for BASE+INDEX validation.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t baseAndIndexTypes() const noexcept { return _signature.getField<Signature::kMemBaseIndexMask>(); }

  //! Returns both BASE (4:0 bits) and INDEX (9:5 bits) types combined into a single value.
  //!
  //! \remarks Returns id of the BASE register or label (if the BASE was specified as label).
  ASMJIT_INLINE_NODEBUG constexpr uint32_t baseId() const noexcept { return _baseId; }

  //! Returns the id of the INDEX register.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t indexId() const noexcept { return _data[kDataMemIndexId]; }

  //! Sets the id of the BASE register (without modifying its type).
  ASMJIT_INLINE_NODEBUG void setBaseId(uint32_t id) noexcept { _baseId = id; }
  //! Sets the register type of the BASE register (without modifying its id).
  ASMJIT_INLINE_NODEBUG void setBaseType(RegType regType) noexcept { _signature.setMemBaseType(regType); }

  //! Sets the id of the INDEX register (without modifying its type).
  ASMJIT_INLINE_NODEBUG void setIndexId(uint32_t id) noexcept { _data[kDataMemIndexId] = id; }
  //! Sets the register type of the INDEX register (without modifying its id).
  ASMJIT_INLINE_NODEBUG void setIndexType(RegType regType) noexcept { _signature.setMemIndexType(regType); }

  //! Sets the base register to type and id of the given `base` operand.
  ASMJIT_INLINE_NODEBUG void setBase(const BaseReg& base) noexcept { return _setBase(base.type(), base.id()); }
  //! Sets the index register to type and id of the given `index` operand.
  ASMJIT_INLINE_NODEBUG void setIndex(const BaseReg& index) noexcept { return _setIndex(index.type(), index.id()); }

  //! \cond INTERNAL
  ASMJIT_INLINE_NODEBUG void _setBase(RegType type, uint32_t id) noexcept {
    _signature.setField<Signature::kMemBaseTypeMask>(uint32_t(type));
    _baseId = id;
  }

  ASMJIT_INLINE_NODEBUG void _setIndex(RegType type, uint32_t id) noexcept {
    _signature.setField<Signature::kMemIndexTypeMask>(uint32_t(type));
    _data[kDataMemIndexId] = id;
  }
  //! \endcond

  //! Resets the memory operand's BASE register or label.
  ASMJIT_INLINE_NODEBUG void resetBase() noexcept { _setBase(RegType::kNone, 0); }
  //! Resets the memory operand's INDEX register.
  ASMJIT_INLINE_NODEBUG void resetIndex() noexcept { _setIndex(RegType::kNone, 0); }

  //! Tests whether the memory operand has a 64-bit offset or absolute address.
  //!
  //! If this is true then `hasBase()` must always report false.
  ASMJIT_INLINE_NODEBUG constexpr bool isOffset64Bit() const noexcept { return baseType() == RegType::kNone; }

  //! Tests whether the memory operand has a non-zero offset or absolute address.
  ASMJIT_INLINE_NODEBUG constexpr bool hasOffset() const noexcept {
    return (_data[kDataMemOffsetLo] | uint32_t(_baseId & Support::bitMaskFromBool<uint32_t>(isOffset64Bit()))) != 0;
  }

  //! Returns either relative offset or absolute address as 64-bit integer.
  ASMJIT_INLINE_NODEBUG constexpr int64_t offset() const noexcept {
    return isOffset64Bit() ? int64_t(uint64_t(_data[kDataMemOffsetLo]) | (uint64_t(_baseId) << 32))
                           : int64_t(int32_t(_data[kDataMemOffsetLo])); // Sign extend 32-bit offset.
  }

  //! Returns a 32-bit low part of a 64-bit offset or absolute address.
  ASMJIT_INLINE_NODEBUG constexpr int32_t offsetLo32() const noexcept { return int32_t(_data[kDataMemOffsetLo]); }
  //! Returns a 32-but high part of a 64-bit offset or absolute address.
  //!
  //! \note This function is UNSAFE and returns garbage if `isOffset64Bit()`
  //! returns false. Never use it blindly without checking it first.
  ASMJIT_INLINE_NODEBUG constexpr int32_t offsetHi32() const noexcept { return int32_t(_baseId); }

  //! Sets a 64-bit offset or an absolute address to `offset`.
  //!
  //! \note This functions attempts to set both high and low parts of a 64-bit offset, however, if the operand has
  //! a BASE register it will store only the low 32 bits of the offset / address as there is no way to store both
  //! BASE and 64-bit offset, and there is currently no architecture that has such capability targeted by AsmJit.
  inline void setOffset(int64_t offset) noexcept {
    uint32_t lo = uint32_t(uint64_t(offset) & 0xFFFFFFFFu);
    uint32_t hi = uint32_t(uint64_t(offset) >> 32);
    uint32_t hiMsk = Support::bitMaskFromBool<uint32_t>(isOffset64Bit());

    _data[kDataMemOffsetLo] = lo;
    _baseId = (hi & hiMsk) | (_baseId & ~hiMsk);
  }
  //! Sets a low 32-bit offset to `offset` (don't use without knowing how BaseMem works).
  inline void setOffsetLo32(int32_t offset) noexcept { _data[kDataMemOffsetLo] = uint32_t(offset); }

  //! Adjusts the offset by `offset`.
  //!
  //! \note This is a fast function that doesn't use the HI 32-bits of a 64-bit offset. Use it only if you know that
  //! there is a BASE register and the offset is only 32 bits anyway.

  //! Adjusts the memory operand offset by a `offset`.
  inline void addOffset(int64_t offset) noexcept {
    if (isOffset64Bit()) {
      int64_t result = offset + int64_t(uint64_t(_data[kDataMemOffsetLo]) | (uint64_t(_baseId) << 32));
      _data[kDataMemOffsetLo] = uint32_t(uint64_t(result) & 0xFFFFFFFFu);
      _baseId                 = uint32_t(uint64_t(result) >> 32);
    }
    else {
      _data[kDataMemOffsetLo] += uint32_t(uint64_t(offset) & 0xFFFFFFFFu);
    }
  }

  //! Adds `offset` to a low 32-bit offset part (don't use without knowing how BaseMem works).
  ASMJIT_INLINE_NODEBUG void addOffsetLo32(int32_t offset) noexcept { _data[kDataMemOffsetLo] += uint32_t(offset); }

  //! Resets the memory offset to zero.
  ASMJIT_INLINE_NODEBUG void resetOffset() noexcept { setOffset(0); }

  //! Resets the lo part of the memory offset to zero (don't use without knowing how BaseMem works).
  ASMJIT_INLINE_NODEBUG void resetOffsetLo32() noexcept { setOffsetLo32(0); }

  //! \}
};

//! Type of the an immediate value.
enum class ImmType : uint32_t {
  //! Immediate is integer.
  kInt = 0,
  //! Immediate is a floating point stored as double-precision.
  kDouble = 1
};

//! Immediate operands are encoded with instruction data.
class Imm : public Operand {
public:
  //! \cond INTERNAL
  template<typename T>
  struct IsConstexprConstructibleAsImmType
    : public std::integral_constant<bool, std::is_enum<T>::value ||
                                          std::is_pointer<T>::value ||
                                          std::is_integral<T>::value ||
                                          std::is_function<T>::value> {};

  template<typename T>
  struct IsConvertibleToImmType
    : public std::integral_constant<bool, IsConstexprConstructibleAsImmType<T>::value ||
                                          std::is_floating_point<T>::value> {};
  //! \endcond

  //! \name Construction & Destruction
  //! \{

  //! Creates a new immediate value (initial value is 0).
  ASMJIT_INLINE_NODEBUG constexpr Imm() noexcept
    : Operand(Globals::Init, Signature::fromOpType(OperandType::kImm), 0, 0, 0) {}

  //! Creates a new immediate value from `other`.
  ASMJIT_INLINE_NODEBUG constexpr Imm(const Imm& other) noexcept
    : Operand(other) {}

  //! Creates a new immediate value from ARM/AArch64 specific `shift`.
  ASMJIT_INLINE_NODEBUG constexpr Imm(const arm::Shift& shift) noexcept
    : Operand(Globals::Init,
              Signature::fromOpType(OperandType::kImm) | Signature::fromPredicate(uint32_t(shift.op())),
              0,
              Support::unpackU32At0(shift.value()),
              Support::unpackU32At1(shift.value())) {}

  //! Creates a new signed immediate value, assigning the value to `val` and an architecture-specific predicate
  //! to `predicate`.
  //!
  //! \note Predicate is currently only used by ARM architectures.
  template<typename T, typename = typename std::enable_if<IsConstexprConstructibleAsImmType<typename std::decay<T>::type>::value>::type>
  ASMJIT_INLINE_NODEBUG constexpr Imm(const T& val, const uint32_t predicate = 0) noexcept
    : Operand(Globals::Init,
              Signature::fromOpType(OperandType::kImm) | Signature::fromPredicate(predicate),
              0,
              Support::unpackU32At0(int64_t(val)),
              Support::unpackU32At1(int64_t(val))) {}

  ASMJIT_INLINE_NODEBUG Imm(const float& val, const uint32_t predicate = 0) noexcept
    : Operand(Globals::Init,
              Signature::fromOpType(OperandType::kImm) | Signature::fromPredicate(predicate),
              0,
              0,
              0) { setValue(val); }

  ASMJIT_INLINE_NODEBUG Imm(const double& val, const uint32_t predicate = 0) noexcept
    : Operand(Globals::Init,
              Signature::fromOpType(OperandType::kImm) | Signature::fromPredicate(predicate),
              0,
              0,
              0) { setValue(val); }

  ASMJIT_INLINE_NODEBUG explicit Imm(Globals::NoInit_) noexcept
    : Operand(Globals::NoInit) {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  //! Assigns the value of the `other` operand to this immediate.
  ASMJIT_INLINE_NODEBUG Imm& operator=(const Imm& other) noexcept { copyFrom(other); return *this; }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns immediate type.
  ASMJIT_INLINE_NODEBUG constexpr ImmType type() const noexcept { return (ImmType)_signature.getField<Signature::kImmTypeMask>(); }
  //! Sets the immediate type to `type`.
  ASMJIT_INLINE_NODEBUG void setType(ImmType type) noexcept { _signature.setField<Signature::kImmTypeMask>(uint32_t(type)); }
  //! Resets immediate type to \ref ImmType::kInt.
  ASMJIT_INLINE_NODEBUG void resetType() noexcept { setType(ImmType::kInt); }

  //! Returns operation predicate of the immediate.
  //!
  //! The meaning depends on architecture, for example on ARM hardware this describes \ref arm::ShiftOp
  //! of the immediate.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t predicate() const noexcept { return _signature.getField<Signature::kPredicateMask>(); }

  //! Sets operation predicate of the immediate to `predicate`.
  //!
  //! The meaning depends on architecture, for example on ARM hardware this describes \ref arm::ShiftOp
  //! of the immediate.
  ASMJIT_INLINE_NODEBUG void setPredicate(uint32_t predicate) noexcept { _signature.setField<Signature::kPredicateMask>(predicate); }

  //! Resets the shift operation type of the immediate to the default value (no operation).
  ASMJIT_INLINE_NODEBUG void resetPredicate() noexcept { _signature.setField<Signature::kPredicateMask>(0); }

  //! Returns the immediate value as `int64_t`, which is the internal format Imm uses.
  ASMJIT_INLINE_NODEBUG constexpr int64_t value() const noexcept {
    return int64_t((uint64_t(_data[kDataImmValueHi]) << 32) | _data[kDataImmValueLo]);
  }

  //! Tests whether this immediate value is integer of any size.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t isInt() const noexcept { return type() == ImmType::kInt; }
  //! Tests whether this immediate value is a double precision floating point value.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t isDouble() const noexcept { return type() == ImmType::kDouble; }

  //! Tests whether the immediate can be casted to 8-bit signed integer.
  ASMJIT_INLINE_NODEBUG constexpr bool isInt8() const noexcept { return type() == ImmType::kInt && Support::isInt8(value()); }
  //! Tests whether the immediate can be casted to 8-bit unsigned integer.
  ASMJIT_INLINE_NODEBUG constexpr bool isUInt8() const noexcept { return type() == ImmType::kInt && Support::isUInt8(value()); }
  //! Tests whether the immediate can be casted to 16-bit signed integer.
  ASMJIT_INLINE_NODEBUG constexpr bool isInt16() const noexcept { return type() == ImmType::kInt && Support::isInt16(value()); }
  //! Tests whether the immediate can be casted to 16-bit unsigned integer.
  ASMJIT_INLINE_NODEBUG constexpr bool isUInt16() const noexcept { return type() == ImmType::kInt && Support::isUInt16(value()); }
  //! Tests whether the immediate can be casted to 32-bit signed integer.
  ASMJIT_INLINE_NODEBUG constexpr bool isInt32() const noexcept { return type() == ImmType::kInt && Support::isInt32(value()); }
  //! Tests whether the immediate can be casted to 32-bit unsigned integer.
  ASMJIT_INLINE_NODEBUG constexpr bool isUInt32() const noexcept { return type() == ImmType::kInt && _data[kDataImmValueHi] == 0; }

  //! Returns the immediate value casted to `T`.
  //!
  //! The value is masked before it's casted to `T` so the returned value is simply the representation of `T`
  //! considering the original value's lowest bits.
  template<typename T>
  ASMJIT_INLINE_NODEBUG T valueAs() const noexcept { return Support::immediateToT<T>(value()); }

  //! Returns low 32-bit signed integer.
  ASMJIT_INLINE_NODEBUG constexpr int32_t int32Lo() const noexcept { return int32_t(_data[kDataImmValueLo]); }
  //! Returns high 32-bit signed integer.
  ASMJIT_INLINE_NODEBUG constexpr int32_t int32Hi() const noexcept { return int32_t(_data[kDataImmValueHi]); }
  //! Returns low 32-bit signed integer.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t uint32Lo() const noexcept { return _data[kDataImmValueLo]; }
  //! Returns high 32-bit signed integer.
  ASMJIT_INLINE_NODEBUG constexpr uint32_t uint32Hi() const noexcept { return _data[kDataImmValueHi]; }

  //! Sets immediate value to `val`, the value is casted to a signed 64-bit integer.
  template<typename T>
  ASMJIT_INLINE_NODEBUG void setValue(const T& val) noexcept {
    _setValueInternal(Support::immediateFromT(val), std::is_floating_point<T>::value ? ImmType::kDouble : ImmType::kInt);
  }

  ASMJIT_INLINE_NODEBUG void _setValueInternal(int64_t val, ImmType type) noexcept {
    setType(type);
    _data[kDataImmValueHi] = uint32_t(uint64_t(val) >> 32);
    _data[kDataImmValueLo] = uint32_t(uint64_t(val) & 0xFFFFFFFFu);
  }

  //! \}

  //! \name Utilities
  //! \{

  //! Clones the immediate operand.
  ASMJIT_INLINE_NODEBUG constexpr Imm clone() const noexcept { return Imm(*this); }

  ASMJIT_INLINE_NODEBUG void signExtend8Bits() noexcept { setValue(int64_t(valueAs<int8_t>())); }
  ASMJIT_INLINE_NODEBUG void signExtend16Bits() noexcept { setValue(int64_t(valueAs<int16_t>())); }
  ASMJIT_INLINE_NODEBUG void signExtend32Bits() noexcept { setValue(int64_t(valueAs<int32_t>())); }

  ASMJIT_INLINE_NODEBUG void zeroExtend8Bits() noexcept { setValue(valueAs<uint8_t>()); }
  ASMJIT_INLINE_NODEBUG void zeroExtend16Bits() noexcept { setValue(valueAs<uint16_t>()); }
  ASMJIT_INLINE_NODEBUG void zeroExtend32Bits() noexcept { _data[kDataImmValueHi] = 0u; }

  //! \}
};

//! Creates a new immediate operand.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr Imm imm(const T& val) noexcept { return Imm(val); }

//! \}

namespace Globals {
  //! \ingroup asmjit_assembler
  //!
  //! A default-constructed operand of `Operand_::kOpNone` type.
  static constexpr const Operand none;
}

//! \cond INTERNAL
namespace Support {

template<typename T, bool kIsImm>
struct ForwardOpImpl {
  static ASMJIT_INLINE_NODEBUG const T& forward(const T& value) noexcept { return value; }
};

template<typename T>
struct ForwardOpImpl<T, true> {
  static ASMJIT_INLINE_NODEBUG Imm forward(const T& value) noexcept { return Imm(value); }
};

//! Either forwards operand T or returns a new operand that wraps it if T is a type convertible to operand.
//! At the moment this is only used to convert integers, floats, and enumarations to \ref Imm operands.
template<typename T>
struct ForwardOp : public ForwardOpImpl<T, Imm::IsConvertibleToImmType<typename std::decay<T>::type>::value> {};

} // {Support}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_OPERAND_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/osutils.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/osutils_p.h"
#include "../core/support.h"

#if !defined(_WIN32)
  #include <fcntl.h>
  #include <unistd.h>
#endif

ASMJIT_BEGIN_NAMESPACE

#if !defined(_WIN32)
Error OSUtils::readFile(const char* name, String& dst, size_t maxSize) noexcept {
  char* buffer = dst.prepare(String::ModifyOp::kAssign, maxSize);
  if (ASMJIT_UNLIKELY(!buffer))
    return DebugUtils::errored(kErrorOutOfMemory);

  int fd = ASMJIT_FILE64_API(::open)(name, O_RDONLY);
  if (fd < 0) {
    dst.clear();
    return DebugUtils::errored(kErrorFailedToOpenFile);
  }

  intptr_t len = ::read(fd, buffer, maxSize);
  if (len >= 0) {
    buffer[len] = '\0';
    dst._setSize(size_t(len));
  }

  ::close(fd);
  return kErrorOk;
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/osutils.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_OSUTILS_H_INCLUDED
#define ASMJIT_CORE_OSUTILS_H_INCLUDED

#include "../core/globals.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_utilities
//! \{

//! \cond INTERNAL
//! Lock.
//!
//! Lock is internal, it cannot be used outside of AsmJit, however, its internal
//! layout is exposed as it's used by some other classes, which are public.
class Lock {
public:
  ASMJIT_NONCOPYABLE(Lock)

#if defined(_WIN32)
#pragma pack(push, 8)
  struct ASMJIT_MAY_ALIAS Handle {
    void* DebugInfo;
    long LockCount;
    long RecursionCount;
    void* OwningThread;
    void* LockSemaphore;
    unsigned long* SpinCount;
  };
  Handle _handle;
#pragma pack(pop)
#elif !defined(__EMSCRIPTEN__)
  typedef pthread_mutex_t Handle;
  Handle _handle;
#endif

  ASMJIT_INLINE_NODEBUG Lock() noexcept;
  ASMJIT_INLINE_NODEBUG ~Lock() noexcept;

  ASMJIT_INLINE_NODEBUG void lock() noexcept;
  ASMJIT_INLINE_NODEBUG void unlock() noexcept;
};
//! \endcond

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_OSUTILS_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/osutils_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_OSUTILS_P_H_INCLUDED
#define ASMJIT_CORE_OSUTILS_P_H_INCLUDED

#include "../core/osutils.h"
#include "../core/string.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_utilities
//! \{

#if defined(_WIN32)

// Windows implementation.
static_assert(sizeof(Lock::Handle) == sizeof(CRITICAL_SECTION), "asmjit::Lock::Handle layout must match CRITICAL_SECTION");
static_assert(alignof(Lock::Handle) == alignof(CRITICAL_SECTION), "asmjit::Lock::Handle alignment must match CRITICAL_SECTION");

ASMJIT_INLINE_NODEBUG Lock::Lock() noexcept { InitializeCriticalSection(reinterpret_cast<CRITICAL_SECTION*>(&_handle)); }
ASMJIT_INLINE_NODEBUG Lock::~Lock() noexcept { DeleteCriticalSection(reinterpret_cast<CRITICAL_SECTION*>(&_handle)); }
ASMJIT_INLINE_NODEBUG void Lock::lock() noexcept { EnterCriticalSection(reinterpret_cast<CRITICAL_SECTION*>(&_handle)); }
ASMJIT_INLINE_NODEBUG void Lock::unlock() noexcept { LeaveCriticalSection(reinterpret_cast<CRITICAL_SECTION*>(&_handle)); }

#elif !defined(__EMSCRIPTEN__)

// PThread implementation.
#ifdef PTHREAD_MUTEX_INITIALIZER
ASMJIT_INLINE_NODEBUG Lock::Lock() noexcept : _handle(PTHREAD_MUTEX_INITIALIZER) {}
#else
ASMJIT_INLINE_NODEBUG Lock::Lock() noexcept { pthread_mutex_init(&_handle, nullptr); }
#endif
ASMJIT_INLINE_NODEBUG Lock::~Lock() noexcept { pthread_mutex_destroy(&_handle); }
ASMJIT_INLINE_NODEBUG void Lock::lock() noexcept { pthread_mutex_lock(&_handle); }
ASMJIT_INLINE_NODEBUG void Lock::unlock() noexcept { pthread_mutex_unlock(&_handle); }

#else

// Dummy implementation - Emscripten or other unsupported platform.
ASMJIT_INLINE_NODEBUG Lock::Lock() noexcept {}
ASMJIT_INLINE_NODEBUG Lock::~Lock() noexcept {}
ASMJIT_INLINE_NODEBUG void Lock::lock() noexcept {}
ASMJIT_INLINE_NODEBUG void Lock::unlock() noexcept {}

#endif

//! Scoped lock.
class LockGuard {
public:
  ASMJIT_NONCOPYABLE(LockGuard)

  Lock& _target;

  ASMJIT_INLINE_NODEBUG LockGuard(Lock& target) noexcept
    : _target(target) { _target.lock(); }
  ASMJIT_INLINE_NODEBUG ~LockGuard() noexcept { _target.unlock(); }
};

#if !defined(_WIN32)
namespace OSUtils {

//! Reads a file, only used on non-Windows platforms to access /sys or other files when necessary.
Error readFile(const char* name, String& dst, size_t maxSize) noexcept;

} // {OSUtils}
#endif

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_OSUTILS_P_H_INCLUDED


```

`CodeCleaner/asmjit/asmjit/core/raassignment_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_RAASSIGNMENT_P_H_INCLUDED
#define ASMJIT_CORE_RAASSIGNMENT_P_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/radefs_p.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_ra
//! \{

//! Holds the current register assignment.
//!
//! Has two purposes:
//!
//!   1. Holds register assignment of a local register allocator (see \ref RALocalAllocator).
//!   2. Holds register assignment of the entry of basic blocks (see \ref RABlock).
class RAAssignment {
public:
  ASMJIT_NONCOPYABLE(RAAssignment)

  enum Ids : uint32_t {
    kPhysNone = 0xFF,
    kWorkNone = RAWorkReg::kIdNone
  };

  enum DirtyBit : uint32_t {
    kClean = 0,
    kDirty = 1
  };

  struct Layout {
    //! Index of architecture registers per group.
    RARegIndex physIndex;
    //! Count of architecture registers per group.
    RARegCount physCount;
    //! Count of physical registers of all groups.
    uint32_t physTotal;
    //! Count of work registers.
    uint32_t workCount;
    //! WorkRegs data (vector).
    const RAWorkRegs* workRegs;

    inline void reset() noexcept {
      physIndex.reset();
      physCount.reset();
      physTotal = 0;
      workCount = 0;
      workRegs = nullptr;
    }
  };

  struct PhysToWorkMap {
    //! Assigned registers (each bit represents one physical reg).
    RARegMask assigned;
    //! Dirty registers (spill slot out of sync or no spill slot).
    RARegMask dirty;
    //! PhysReg to WorkReg mapping.
    uint32_t workIds[1 /* ... */];

    static ASMJIT_INLINE_NODEBUG size_t sizeOf(size_t count) noexcept {
      return sizeof(PhysToWorkMap) - sizeof(uint32_t) + count * sizeof(uint32_t);
    }

    inline void reset(size_t count) noexcept {
      assigned.reset();
      dirty.reset();

      for (size_t i = 0; i < count; i++)
        workIds[i] = kWorkNone;
    }

    inline void copyFrom(const PhysToWorkMap* other, size_t count) noexcept {
      size_t size = sizeOf(count);
      memcpy(this, other, size);
    }

    inline void unassign(RegGroup group, uint32_t physId, uint32_t indexInWorkIds) noexcept {
      assigned.clear(group, Support::bitMask(physId));
      dirty.clear(group, Support::bitMask(physId));
      workIds[indexInWorkIds] = kWorkNone;
    }
  };

  struct WorkToPhysMap {
    //! WorkReg to PhysReg mapping
    uint8_t physIds[1 /* ... */];

    static inline size_t sizeOf(size_t count) noexcept {
      return size_t(count) * sizeof(uint8_t);
    }

    inline void reset(size_t count) noexcept {
      for (size_t i = 0; i < count; i++)
        physIds[i] = kPhysNone;
    }

    inline void copyFrom(const WorkToPhysMap* other, size_t count) noexcept {
      size_t size = sizeOf(count);
      if (ASMJIT_LIKELY(size))
        memcpy(this, other, size);
    }
  };

  //! \name Members
  //! \{

  //! Physical registers layout.
  Layout _layout;
  //! WorkReg to PhysReg mapping.
  WorkToPhysMap* _workToPhysMap;
  //! PhysReg to WorkReg mapping and assigned/dirty bits.
  PhysToWorkMap* _physToWorkMap;
  //! Optimization to translate PhysRegs to WorkRegs faster.
  Support::Array<uint32_t*, Globals::kNumVirtGroups> _physToWorkIds;

  //! \}

  //! \name Construction & Destruction
  //! \{

  inline RAAssignment() noexcept {
    _layout.reset();
    resetMaps();
  }

  ASMJIT_FORCE_INLINE void initLayout(const RARegCount& physCount, const RAWorkRegs& workRegs) noexcept {
    // Layout must be initialized before data.
    ASMJIT_ASSERT(_physToWorkMap == nullptr);
    ASMJIT_ASSERT(_workToPhysMap == nullptr);

    _layout.physIndex.buildIndexes(physCount);
    _layout.physCount = physCount;
    _layout.physTotal = uint32_t(_layout.physIndex[RegGroup::kMaxVirt]) +
                        uint32_t(_layout.physCount[RegGroup::kMaxVirt]) ;
    _layout.workCount = workRegs.size();
    _layout.workRegs = &workRegs;
  }

  ASMJIT_FORCE_INLINE void initMaps(PhysToWorkMap* physToWorkMap, WorkToPhysMap* workToPhysMap) noexcept {
    _physToWorkMap = physToWorkMap;
    _workToPhysMap = workToPhysMap;
    for (RegGroup group : RegGroupVirtValues{})
      _physToWorkIds[group] = physToWorkMap->workIds + _layout.physIndex.get(group);
  }

  ASMJIT_FORCE_INLINE void resetMaps() noexcept {
    _physToWorkMap = nullptr;
    _workToPhysMap = nullptr;
    _physToWorkIds.fill(nullptr);
  }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG PhysToWorkMap* physToWorkMap() const noexcept { return _physToWorkMap; }
  ASMJIT_INLINE_NODEBUG WorkToPhysMap* workToPhysMap() const noexcept { return _workToPhysMap; }

  ASMJIT_INLINE_NODEBUG RARegMask& assigned() noexcept { return _physToWorkMap->assigned; }
  ASMJIT_INLINE_NODEBUG const RARegMask& assigned() const noexcept { return _physToWorkMap->assigned; }
  ASMJIT_INLINE_NODEBUG uint32_t assigned(RegGroup group) const noexcept { return _physToWorkMap->assigned[group]; }

  ASMJIT_INLINE_NODEBUG RARegMask& dirty() noexcept { return _physToWorkMap->dirty; }
  ASMJIT_INLINE_NODEBUG const RARegMask& dirty() const noexcept { return _physToWorkMap->dirty; }
  ASMJIT_INLINE_NODEBUG RegMask dirty(RegGroup group) const noexcept { return _physToWorkMap->dirty[group]; }

  inline uint32_t workToPhysId(RegGroup group, uint32_t workId) const noexcept {
    DebugUtils::unused(group);
    ASMJIT_ASSERT(workId != kWorkNone);
    ASMJIT_ASSERT(workId < _layout.workCount);
    return _workToPhysMap->physIds[workId];
  }

  inline uint32_t physToWorkId(RegGroup group, uint32_t physId) const noexcept {
    ASMJIT_ASSERT(physId < Globals::kMaxPhysRegs);
    return _physToWorkIds[group][physId];
  }

  inline bool isPhysAssigned(RegGroup group, uint32_t physId) const noexcept {
    ASMJIT_ASSERT(physId < Globals::kMaxPhysRegs);
    return Support::bitTest(_physToWorkMap->assigned[group], physId);
  }

  inline bool isPhysDirty(RegGroup group, uint32_t physId) const noexcept {
    ASMJIT_ASSERT(physId < Globals::kMaxPhysRegs);
    return Support::bitTest(_physToWorkMap->dirty[group], physId);
  }

  //! \}

  //! \name Assignment
  //!
  //! These are low-level allocation helpers that are used to update the current mappings between physical and
  //! virt/work registers and also to update masks that represent allocated and dirty registers. These functions
  //! don't emit any code; they are only used to update and keep all mappings in sync.
  //!
  //! \{

  //! Assign [VirtReg/WorkReg] to a physical register.
  inline void assign(RegGroup group, uint32_t workId, uint32_t physId, bool dirty) noexcept {
    ASMJIT_ASSERT(workToPhysId(group, workId) == kPhysNone);
    ASMJIT_ASSERT(physToWorkId(group, physId) == kWorkNone);
    ASMJIT_ASSERT(!isPhysAssigned(group, physId));
    ASMJIT_ASSERT(!isPhysDirty(group, physId));

    _workToPhysMap->physIds[workId] = uint8_t(physId);
    _physToWorkIds[group][physId] = workId;

    RegMask regMask = Support::bitMask(physId);
    _physToWorkMap->assigned[group] |= regMask;
    _physToWorkMap->dirty[group] |= regMask & Support::bitMaskFromBool<RegMask>(dirty);

    verify();
  }

  //! Reassign [VirtReg/WorkReg] to `dstPhysId` from `srcPhysId`.
  inline void reassign(RegGroup group, uint32_t workId, uint32_t dstPhysId, uint32_t srcPhysId) noexcept {
    ASMJIT_ASSERT(dstPhysId != srcPhysId);
    ASMJIT_ASSERT(workToPhysId(group, workId) == srcPhysId);
    ASMJIT_ASSERT(physToWorkId(group, srcPhysId) == workId);
    ASMJIT_ASSERT(isPhysAssigned(group, srcPhysId) == true);
    ASMJIT_ASSERT(isPhysAssigned(group, dstPhysId) == false);

    _workToPhysMap->physIds[workId] = uint8_t(dstPhysId);
    _physToWorkIds[group][srcPhysId] = kWorkNone;
    _physToWorkIds[group][dstPhysId] = workId;

    RegMask srcMask = Support::bitMask(srcPhysId);
    RegMask dstMask = Support::bitMask(dstPhysId);

    bool dirty = (_physToWorkMap->dirty[group] & srcMask) != 0;
    RegMask regMask = dstMask | srcMask;

    _physToWorkMap->assigned[group] ^= regMask;
    _physToWorkMap->dirty[group] ^= regMask & Support::bitMaskFromBool<RegMask>(dirty);

    verify();
  }

  inline void swap(RegGroup group, uint32_t aWorkId, uint32_t aPhysId, uint32_t bWorkId, uint32_t bPhysId) noexcept {
    ASMJIT_ASSERT(aPhysId != bPhysId);
    ASMJIT_ASSERT(workToPhysId(group, aWorkId) == aPhysId);
    ASMJIT_ASSERT(workToPhysId(group, bWorkId) == bPhysId);
    ASMJIT_ASSERT(physToWorkId(group, aPhysId) == aWorkId);
    ASMJIT_ASSERT(physToWorkId(group, bPhysId) == bWorkId);
    ASMJIT_ASSERT(isPhysAssigned(group, aPhysId));
    ASMJIT_ASSERT(isPhysAssigned(group, bPhysId));

    _workToPhysMap->physIds[aWorkId] = uint8_t(bPhysId);
    _workToPhysMap->physIds[bWorkId] = uint8_t(aPhysId);
    _physToWorkIds[group][aPhysId] = bWorkId;
    _physToWorkIds[group][bPhysId] = aWorkId;

    RegMask aMask = Support::bitMask(aPhysId);
    RegMask bMask = Support::bitMask(bPhysId);
    RegMask flipMask = Support::bitMaskFromBool<RegMask>(((_physToWorkMap->dirty[group] & aMask) != 0) ^ ((_physToWorkMap->dirty[group] & bMask) != 0));
    RegMask regMask = aMask | bMask;
    _physToWorkMap->dirty[group] ^= regMask & flipMask;

    verify();
  }

  //! Unassign [VirtReg/WorkReg] from a physical register.
  inline void unassign(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    ASMJIT_ASSERT(physId < Globals::kMaxPhysRegs);
    ASMJIT_ASSERT(workToPhysId(group, workId) == physId);
    ASMJIT_ASSERT(physToWorkId(group, physId) == workId);
    ASMJIT_ASSERT(isPhysAssigned(group, physId));

    _workToPhysMap->physIds[workId] = kPhysNone;
    _physToWorkIds[group][physId] = kWorkNone;

    RegMask regMask = Support::bitMask(physId);
    _physToWorkMap->assigned[group] &= ~regMask;
    _physToWorkMap->dirty[group] &= ~regMask;

    verify();
  }

  inline void makeClean(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    DebugUtils::unused(workId);
    RegMask regMask = Support::bitMask(physId);
    _physToWorkMap->dirty[group] &= ~regMask;
  }

  inline void makeDirty(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    DebugUtils::unused(workId);
    RegMask regMask = Support::bitMask(physId);
    _physToWorkMap->dirty[group] |= regMask;
  }

  //! \}

  //! \name Utilities
  //! \{

  ASMJIT_FORCE_INLINE void swap(RAAssignment& other) noexcept {
    std::swap(_workToPhysMap, other._workToPhysMap);
    std::swap(_physToWorkMap, other._physToWorkMap);
    _physToWorkIds.swap(other._physToWorkIds);
  }

  inline void assignWorkIdsFromPhysIds() noexcept {
    memset(_workToPhysMap, uint8_t(BaseReg::kIdBad), WorkToPhysMap::sizeOf(_layout.workCount));

    for (RegGroup group : RegGroupVirtValues{}) {
      uint32_t physBaseIndex = _layout.physIndex[group];
      Support::BitWordIterator<RegMask> it(_physToWorkMap->assigned[group]);

      while (it.hasNext()) {
        uint32_t physId = it.next();
        uint32_t workId = _physToWorkMap->workIds[physBaseIndex + physId];

        ASMJIT_ASSERT(workId != kWorkNone);
        _workToPhysMap->physIds[workId] = uint8_t(physId);
      }
    }
  }

  inline void copyFrom(const PhysToWorkMap* physToWorkMap) noexcept {
    memcpy(_physToWorkMap, physToWorkMap, PhysToWorkMap::sizeOf(_layout.physTotal));
    assignWorkIdsFromPhysIds();
  }

  inline void copyFrom(const PhysToWorkMap* physToWorkMap, const WorkToPhysMap* workToPhysMap) noexcept {
    memcpy(_physToWorkMap, physToWorkMap, PhysToWorkMap::sizeOf(_layout.physTotal));
    memcpy(_workToPhysMap, workToPhysMap, WorkToPhysMap::sizeOf(_layout.workCount));
  }

  inline void copyFrom(const RAAssignment& other) noexcept {
    copyFrom(other.physToWorkMap(), other.workToPhysMap());
  }

  // Not really useful outside of debugging.
  bool equals(const RAAssignment& other) const noexcept {
    // Layout should always match.
    if (_layout.physIndex != other._layout.physIndex ||
        _layout.physCount != other._layout.physCount ||
        _layout.physTotal != other._layout.physTotal ||
        _layout.workCount != other._layout.workCount ||
        _layout.workRegs  != other._layout.workRegs)
      return false;

    uint32_t physTotal = _layout.physTotal;
    uint32_t workCount = _layout.workCount;

    for (uint32_t physId = 0; physId < physTotal; physId++) {
      uint32_t thisWorkId = _physToWorkMap->workIds[physId];
      uint32_t otherWorkId = other._physToWorkMap->workIds[physId];
      if (thisWorkId != otherWorkId)
        return false;
    }

    for (uint32_t workId = 0; workId < workCount; workId++) {
      uint32_t thisPhysId = _workToPhysMap->physIds[workId];
      uint32_t otherPhysId = other._workToPhysMap->physIds[workId];
      if (thisPhysId != otherPhysId)
        return false;
    }

    if (_physToWorkMap->assigned != other._physToWorkMap->assigned ||
        _physToWorkMap->dirty    != other._physToWorkMap->dirty    )
      return false;

    return true;
  }

#if defined(ASMJIT_BUILD_DEBUG)
  ASMJIT_NOINLINE void verify() noexcept {
    // Verify WorkToPhysMap.
    {
      for (uint32_t workId = 0; workId < _layout.workCount; workId++) {
        uint32_t physId = _workToPhysMap->physIds[workId];
        if (physId != kPhysNone) {
          const RAWorkReg* workReg = _layout.workRegs->at(workId);
          RegGroup group = workReg->group();
          ASMJIT_ASSERT(_physToWorkIds[group][physId] == workId);
        }
      }
    }

    // Verify PhysToWorkMap.
    {
      for (RegGroup group : RegGroupVirtValues{}) {
        uint32_t physCount = _layout.physCount[group];
        for (uint32_t physId = 0; physId < physCount; physId++) {
          uint32_t workId = _physToWorkIds[group][physId];
          if (workId != kWorkNone) {
            ASMJIT_ASSERT(_workToPhysMap->physIds[workId] == physId);
          }
        }
      }
    }
  }
#else
  inline void verify() noexcept {}
#endif

  //! \}
};

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER
#endif // ASMJIT_CORE_RAASSIGNMENT_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/rabuilders_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_RABUILDERS_P_H_INCLUDED
#define ASMJIT_CORE_RABUILDERS_P_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/formatter.h"
#include "../core/rapass_p.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_ra
//! \{

template<typename This>
class RACFGBuilderT {
public:
  enum : uint32_t {
    kRootIndentation = 2,
    kCodeIndentation = 4,

    // NOTE: This is a bit hacky. There are some nodes which are processed twice (see `onBeforeInvoke()` and
    // `onBeforeRet()`) as they can insert some nodes around them. Since we don't have any flags to mark these
    // we just use their position that is [at that time] unassigned.
    kNodePositionDidOnBefore = 0xFFFFFFFFu
  };

  //! \name Members
  //! \{

  BaseRAPass* _pass = nullptr;
  BaseCompiler* _cc = nullptr;
  RABlock* _curBlock = nullptr;
  RABlock* _retBlock = nullptr;
  FuncNode* _funcNode = nullptr;
  RARegsStats _blockRegStats {};
  uint32_t _exitLabelId = Globals::kInvalidId;
  ZoneVector<uint32_t> _sharedAssignmentsMap {};

  // Only used by logging, it's fine to be here to prevent more #ifdefs...
  bool _hasCode = false;
  RABlock* _lastLoggedBlock = nullptr;

#ifndef ASMJIT_NO_LOGGING
  Logger* _logger = nullptr;
  FormatOptions _formatOptions {};
  StringTmp<512> _sb;
#endif

  //! \}

  inline RACFGBuilderT(BaseRAPass* pass) noexcept
    : _pass(pass),
      _cc(pass->cc()) {
#ifndef ASMJIT_NO_LOGGING
    _logger = _pass->hasDiagnosticOption(DiagnosticOptions::kRADebugCFG) ? _pass->logger() : nullptr;
    if (_logger)
      _formatOptions = _logger->options();
#endif
  }

  ASMJIT_INLINE_NODEBUG BaseCompiler* cc() const noexcept { return _cc; }

  //! \name Run
  //! \{

  //! Called per function by an architecture-specific CFG builder.
  Error run() noexcept {
    log("[BuildCFG]\n");
    ASMJIT_PROPAGATE(prepare());

    logNode(_funcNode, kRootIndentation);
    logBlock(_curBlock, kRootIndentation);

    RABlock* entryBlock = _curBlock;
    BaseNode* node = _funcNode->next();

    if (ASMJIT_UNLIKELY(!node)) {
      return DebugUtils::errored(kErrorInvalidState);
    }

    _curBlock->setFirst(_funcNode);
    _curBlock->setLast(_funcNode);

    RAInstBuilder ib;
    ZoneVector<RABlock*> blocksWithUnknownJumps;

    for (;;) {
      BaseNode* next = node->next();
      ASMJIT_ASSERT(node->position() == 0 || node->position() == kNodePositionDidOnBefore);

      if (node->isInst()) {
        // Instruction | Jump | Invoke | Return
        // ------------------------------------

        // Handle `InstNode`, `InvokeNode`, and `FuncRetNode`. All of them share the same interface that provides
        // operands that have read/write semantics.
        if (ASMJIT_UNLIKELY(!_curBlock)) {
          // Unreachable code has to be removed, we cannot allocate registers in such code as we cannot do proper
          // liveness analysis in such case.
          removeNode(node);
          node = next;
          continue;
        }

        _hasCode = true;

        if (node->isInvoke() || node->isFuncRet()) {
          if (node->position() != kNodePositionDidOnBefore) {
            // Call and Reg are complicated as they may insert some surrounding code around them. The simplest
            // approach is to get the previous node, call the `onBefore()` handlers and then check whether
            // anything changed and restart if so. By restart we mean that the current `node` would go back to
            // the first possible inserted node by `onBeforeInvoke()` or `onBeforeRet()`.
            BaseNode* prev = node->prev();

            if (node->type() == NodeType::kInvoke) {
              ASMJIT_PROPAGATE(static_cast<This*>(this)->onBeforeInvoke(node->as<InvokeNode>()));
            }
            else {
              ASMJIT_PROPAGATE(static_cast<This*>(this)->onBeforeRet(node->as<FuncRetNode>()));
            }

            if (prev != node->prev()) {
              // If this was the first node in the block and something was
              // inserted before it then we have to update the first block.
              if (_curBlock->first() == node) {
                _curBlock->setFirst(prev->next());
              }

              node->setPosition(kNodePositionDidOnBefore);
              node = prev->next();

              // `onBeforeInvoke()` and `onBeforeRet()` can only insert instructions.
              ASMJIT_ASSERT(node->isInst());
            }

            // Necessary if something was inserted after `node`, but nothing before.
            next = node->next();
          }
          else {
            // Change the position back to its original value.
            node->setPosition(0);
          }
        }

        InstNode* inst = node->as<InstNode>();
        logNode(inst, kCodeIndentation);

        InstControlFlow cf = InstControlFlow::kRegular;
        ib.reset(_curBlock->blockId());
        ASMJIT_PROPAGATE(static_cast<This*>(this)->onInst(inst, cf, ib));

        if (node->isInvoke()) {
          ASMJIT_PROPAGATE(static_cast<This*>(this)->onInvoke(inst->as<InvokeNode>(), ib));
        }

        if (node->isFuncRet()) {
          ASMJIT_PROPAGATE(static_cast<This*>(this)->onRet(inst->as<FuncRetNode>(), ib));
          cf = InstControlFlow::kReturn;
        }

        if (cf == InstControlFlow::kJump) {
          uint32_t fixedRegCount = 0;
          for (RATiedReg& tiedReg : ib) {
            RAWorkReg* workReg = _pass->workRegById(tiedReg.workId());
            if (workReg->group() == RegGroup::kGp) {
              uint32_t useId = tiedReg.useId();
              if (useId == BaseReg::kIdBad) {
                useId = _pass->_scratchRegIndexes[fixedRegCount++];
                tiedReg.setUseId(useId);
              }
              _curBlock->addExitScratchGpRegs(Support::bitMask(useId));
            }
          }
        }

        ASMJIT_PROPAGATE(_pass->assignRAInst(inst, _curBlock, ib));
        _blockRegStats.combineWith(ib._stats);

        if (cf != InstControlFlow::kRegular) {
          // Support for conditional and unconditional jumps.
          if (cf == InstControlFlow::kJump || cf == InstControlFlow::kBranch) {
            _curBlock->setLast(node);
            _curBlock->addFlags(RABlockFlags::kHasTerminator);
            _curBlock->makeConstructed(_blockRegStats);

            if (!inst->hasOption(InstOptions::kUnfollow)) {
              // Jmp/Jcc/Call/Loop/etc...
              uint32_t opCount = inst->opCount();
              const Operand* opArray = inst->operands();

              // Cannot jump anywhere without operands.
              if (ASMJIT_UNLIKELY(!opCount)) {
                return DebugUtils::errored(kErrorInvalidState);
              }

              if (opArray[opCount - 1].isLabel()) {
                // Labels are easy for constructing the control flow.
                LabelNode* labelNode;
                ASMJIT_PROPAGATE(cc()->labelNodeOf(&labelNode, opArray[opCount - 1].as<Label>()));

                RABlock* targetBlock = _pass->newBlockOrExistingAt(labelNode);
                if (ASMJIT_UNLIKELY(!targetBlock)) {
                  return DebugUtils::errored(kErrorOutOfMemory);
                }

                targetBlock->makeTargetable();
                ASMJIT_PROPAGATE(_curBlock->appendSuccessor(targetBlock));
              }
              else {
                // Not a label - could be jump with reg/mem operand, which means that it can go anywhere. Such jumps
                // must either be annotated so the CFG can be properly constructed, otherwise we assume the worst case
                // - can jump to any basic block.
                JumpAnnotation* jumpAnnotation = nullptr;
                _curBlock->addFlags(RABlockFlags::kHasJumpTable);

                if (inst->type() == NodeType::kJump) {
                  jumpAnnotation = inst->as<JumpNode>()->annotation();
                }

                if (jumpAnnotation) {
                  uint64_t timestamp = _pass->nextTimestamp();
                  for (uint32_t id : jumpAnnotation->labelIds()) {
                    LabelNode* labelNode;
                    ASMJIT_PROPAGATE(cc()->labelNodeOf(&labelNode, id));

                    RABlock* targetBlock = _pass->newBlockOrExistingAt(labelNode);
                    if (ASMJIT_UNLIKELY(!targetBlock)) {
                      return DebugUtils::errored(kErrorOutOfMemory);
                    }

                    // Prevents adding basic-block successors multiple times.
                    if (!targetBlock->hasTimestamp(timestamp)) {
                      targetBlock->setTimestamp(timestamp);
                      targetBlock->makeTargetable();
                      ASMJIT_PROPAGATE(_curBlock->appendSuccessor(targetBlock));
                    }
                  }
                  ASMJIT_PROPAGATE(shareAssignmentAcrossSuccessors(_curBlock));
                }
                else {
                  ASMJIT_PROPAGATE(blocksWithUnknownJumps.append(_pass->allocator(), _curBlock));
                }
              }
            }

            if (cf == InstControlFlow::kJump) {
              // Unconditional jump makes the code after the jump unreachable, which will be removed instantly during
              // the CFG construction; as we cannot allocate registers for instructions that are not part of any block.
              // Of course we can leave these instructions as they are, however, that would only postpone the problem
              // as assemblers can't encode instructions that use virtual registers.
              _curBlock = nullptr;
            }
            else {
              node = next;
              if (ASMJIT_UNLIKELY(!node))
                return DebugUtils::errored(kErrorInvalidState);

              RABlock* consecutiveBlock;
              if (node->type() == NodeType::kLabel) {
                if (node->hasPassData()) {
                  consecutiveBlock = node->passData<RABlock>();
                }
                else {
                  consecutiveBlock = _pass->newBlock(node);
                  if (ASMJIT_UNLIKELY(!consecutiveBlock)) {
                    return DebugUtils::errored(kErrorOutOfMemory);
                  }
                  node->setPassData<RABlock>(consecutiveBlock);
                }
              }
              else {
                consecutiveBlock = _pass->newBlock(node);
                if (ASMJIT_UNLIKELY(!consecutiveBlock)) {
                  return DebugUtils::errored(kErrorOutOfMemory);
                }
              }

              _curBlock->addFlags(RABlockFlags::kHasConsecutive);
              ASMJIT_PROPAGATE(_curBlock->prependSuccessor(consecutiveBlock));

              _curBlock = consecutiveBlock;
              _hasCode = false;
              _blockRegStats.reset();

              if (_curBlock->isConstructed())
                break;
              ASMJIT_PROPAGATE(_pass->addBlock(consecutiveBlock));

              logBlock(_curBlock, kRootIndentation);
              continue;
            }
          }

          if (cf == InstControlFlow::kReturn) {
            _curBlock->setLast(node);
            _curBlock->makeConstructed(_blockRegStats);
            ASMJIT_PROPAGATE(_curBlock->appendSuccessor(_retBlock));

            _curBlock = nullptr;
          }
        }
      }
      else if (node->type() == NodeType::kLabel) {
        // Label - Basic-Block Management
        // ------------------------------

        if (!_curBlock) {
          // If the current code is unreachable the label makes it reachable again. We may remove the whole block in
          // the future if it's not referenced though.
          _curBlock = node->passData<RABlock>();

          if (_curBlock) {
            // If the label has a block assigned we can either continue with it or skip it if the block has been
            // constructed already.
            if (_curBlock->isConstructed()) {
              break;
            }
          }
          else {
            // No block assigned - create a new one and assign it.
            _curBlock = _pass->newBlock(node);
            if (ASMJIT_UNLIKELY(!_curBlock)) {
              return DebugUtils::errored(kErrorOutOfMemory);
            }
            node->setPassData<RABlock>(_curBlock);
          }

          _curBlock->makeTargetable();
          _hasCode = false;
          _blockRegStats.reset();
          ASMJIT_PROPAGATE(_pass->addBlock(_curBlock));
        }
        else {
          if (node->hasPassData()) {
            RABlock* consecutive = node->passData<RABlock>();
            consecutive->makeTargetable();

            if (_curBlock == consecutive) {
              // The label currently processed is part of the current block. This is only possible for multiple labels
              // that are right next to each other or labels that are separated by non-code nodes like directives and
              // comments.
              if (ASMJIT_UNLIKELY(_hasCode)) {
                return DebugUtils::errored(kErrorInvalidState);
              }
            }
            else {
              // Label makes the current block constructed. There is a chance that the Label is not used, but we don't
              // know that at this point. In the worst case there would be two blocks next to each other, it's just fine.
              ASMJIT_ASSERT(_curBlock->last() != node);
              _curBlock->setLast(node->prev());
              _curBlock->addFlags(RABlockFlags::kHasConsecutive);
              _curBlock->makeConstructed(_blockRegStats);

              ASMJIT_PROPAGATE(_curBlock->appendSuccessor(consecutive));
              ASMJIT_PROPAGATE(_pass->addBlock(consecutive));

              _curBlock = consecutive;
              _hasCode = false;
              _blockRegStats.reset();
            }
          }
          else {
            // First time we see this label.
            if (_hasCode || _curBlock == entryBlock) {
              // Cannot continue the current block if it already contains some code or it's a block entry. We need to
              // create a new block and make it a successor.
              ASMJIT_ASSERT(_curBlock->last() != node);
              _curBlock->setLast(node->prev());
              _curBlock->addFlags(RABlockFlags::kHasConsecutive);
              _curBlock->makeConstructed(_blockRegStats);

              RABlock* consecutive = _pass->newBlock(node);
              if (ASMJIT_UNLIKELY(!consecutive)) {
                return DebugUtils::errored(kErrorOutOfMemory);
              }
              consecutive->makeTargetable();

              ASMJIT_PROPAGATE(_curBlock->appendSuccessor(consecutive));
              ASMJIT_PROPAGATE(_pass->addBlock(consecutive));

              _curBlock = consecutive;
              _hasCode = false;
              _blockRegStats.reset();
            }

            node->setPassData<RABlock>(_curBlock);
          }
        }

        if (_curBlock && _curBlock != _lastLoggedBlock) {
          logBlock(_curBlock, kRootIndentation);
        }
        logNode(node, kRootIndentation);

        // Unlikely: Assume that the exit label is reached only once per function.
        if (ASMJIT_UNLIKELY(node->as<LabelNode>()->labelId() == _exitLabelId)) {
          _curBlock->setLast(node);
          _curBlock->makeConstructed(_blockRegStats);
          ASMJIT_PROPAGATE(_pass->addExitBlock(_curBlock));

          _curBlock = nullptr;
        }
      }
      else {
        // Other Nodes | Function Exit
        // ---------------------------

        logNode(node, kCodeIndentation);

        if (node->type() == NodeType::kSentinel) {
          if (node == _funcNode->endNode()) {
            // Make sure we didn't flow here if this is the end of the function sentinel.
            if (ASMJIT_UNLIKELY(_curBlock && _hasCode))
              return DebugUtils::errored(kErrorInvalidState);
            break;
          }
        }
        else if (node->type() == NodeType::kFunc) {
          // RAPass can only compile a single function at a time. If we
          // encountered a function it must be the current one, bail if not.
          if (ASMJIT_UNLIKELY(node != _funcNode))
            return DebugUtils::errored(kErrorInvalidState);
          // PASS if this is the first node.
        }
        else {
          // PASS if this is a non-interesting or unknown node.
        }
      }

      // Advance to the next node.
      node = next;

      // NOTE: We cannot encounter a NULL node, because every function must be terminated by a sentinel (`stop`)
      // node. If we encountered a NULL node it means that something went wrong and this node list is corrupted;
      // bail in such case.
      if (ASMJIT_UNLIKELY(!node))
        return DebugUtils::errored(kErrorInvalidState);
    }

    if (_pass->hasDanglingBlocks())
      return DebugUtils::errored(kErrorInvalidState);

    for (RABlock* block : blocksWithUnknownJumps)
      handleBlockWithUnknownJump(block);

    return _pass->initSharedAssignments(_sharedAssignmentsMap);
  }

  //! \}

  //! \name Prepare
  //! \{

  //! Prepares the CFG builder of the current function.
  Error prepare() noexcept {
    FuncNode* func = _pass->func();
    BaseNode* node = nullptr;

    // Create entry and exit blocks.
    _funcNode = func;
    _retBlock = _pass->newBlockOrExistingAt(func->exitNode(), &node);

    if (ASMJIT_UNLIKELY(!_retBlock))
      return DebugUtils::errored(kErrorOutOfMemory);

    _retBlock->makeTargetable();
    ASMJIT_PROPAGATE(_pass->addExitBlock(_retBlock));

    if (node != func) {
      _curBlock = _pass->newBlock();
      if (ASMJIT_UNLIKELY(!_curBlock))
        return DebugUtils::errored(kErrorOutOfMemory);
    }
    else {
      // Function that has no code at all.
      _curBlock = _retBlock;
    }

    // Reset everything we may need.
    _blockRegStats.reset();
    _exitLabelId = func->exitNode()->labelId();

    // Initially we assume there is no code in the function body.
    _hasCode = false;

    return _pass->addBlock(_curBlock);
  }

  //! \}

  //! \name Utilities
  //! \{

  //! Called when a `node` is removed, e.g. because of a dead code elimination.
  void removeNode(BaseNode* node) noexcept {
    logNode(node, kRootIndentation, "<Removed>");
    cc()->removeNode(node);
  }

  //! Handles block with unknown jump, which could be a jump to a jump table.
  //!
  //! If we encounter such block we basically insert all existing blocks as successors except the function entry
  //! block and a natural successor, if such block exists.
  Error handleBlockWithUnknownJump(RABlock* block) noexcept {
    RABlocks& blocks = _pass->blocks();
    size_t blockCount = blocks.size();

    // NOTE: Iterate from `1` as the first block is the entry block, we don't
    // allow the entry to be a successor of any block.
    RABlock* consecutive = block->consecutive();
    for (size_t i = 1; i < blockCount; i++) {
      RABlock* candidate = blocks[i];
      if (candidate == consecutive || !candidate->isTargetable())
        continue;
      block->appendSuccessor(candidate);
    }

    return shareAssignmentAcrossSuccessors(block);
  }

  Error shareAssignmentAcrossSuccessors(RABlock* block) noexcept {
    if (block->successors().size() <= 1)
      return kErrorOk;

    RABlock* consecutive = block->consecutive();
    uint32_t sharedAssignmentId = Globals::kInvalidId;

    for (RABlock* successor : block->successors()) {
      if (successor == consecutive)
        continue;

      if (successor->hasSharedAssignmentId()) {
        if (sharedAssignmentId == Globals::kInvalidId)
          sharedAssignmentId = successor->sharedAssignmentId();
        else
          _sharedAssignmentsMap[successor->sharedAssignmentId()] = sharedAssignmentId;
      }
      else {
        if (sharedAssignmentId == Globals::kInvalidId)
          ASMJIT_PROPAGATE(newSharedAssignmentId(&sharedAssignmentId));
        successor->setSharedAssignmentId(sharedAssignmentId);
      }
    }
    return kErrorOk;
  }

  Error newSharedAssignmentId(uint32_t* out) noexcept {
    uint32_t id = _sharedAssignmentsMap.size();
    ASMJIT_PROPAGATE(_sharedAssignmentsMap.append(_pass->allocator(), id));

    *out = id;
    return kErrorOk;
  }

  //! \}

  //! \name Logging
  //! \{

#ifndef ASMJIT_NO_LOGGING
  template<typename... Args>
  inline void log(const char* fmt, Args&&... args) noexcept {
    if (_logger)
      _logger->logf(fmt, std::forward<Args>(args)...);
  }

  inline void logBlock(RABlock* block, uint32_t indentation = 0) noexcept {
    if (_logger)
      _logBlock(block, indentation);
  }

  inline void logNode(BaseNode* node, uint32_t indentation = 0, const char* action = nullptr) noexcept {
    if (_logger)
      _logNode(node, indentation, action);
  }

  void _logBlock(RABlock* block, uint32_t indentation) noexcept {
    _sb.clear();
    _sb.appendChars(' ', indentation);
    _sb.appendFormat("{#%u}\n", block->blockId());
    _logger->log(_sb);
    _lastLoggedBlock = block;
  }

  void _logNode(BaseNode* node, uint32_t indentation, const char* action) noexcept {
    _sb.clear();
    _sb.appendChars(' ', indentation);
    if (action) {
      _sb.append(action);
      _sb.append(' ');
    }
    Formatter::formatNode(_sb, _formatOptions, cc(), node);
    _sb.append('\n');
    _logger->log(_sb);
  }
#else
  template<typename... Args>
  inline void log(const char* fmt, Args&&... args) noexcept {
    DebugUtils::unused(fmt);
    DebugUtils::unused(std::forward<Args>(args)...);
  }

  inline void logBlock(RABlock* block, uint32_t indentation = 0) noexcept {
    DebugUtils::unused(block, indentation);
  }

  inline void logNode(BaseNode* node, uint32_t indentation = 0, const char* action = nullptr) noexcept {
    DebugUtils::unused(node, indentation, action);
  }
#endif

  //! \}
};

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER
#endif // ASMJIT_CORE_RABUILDERS_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/radefs_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_RADEFS_P_H_INCLUDED
#define ASMJIT_CORE_RADEFS_P_H_INCLUDED

#include "../core/api-config.h"
#include "../core/archtraits.h"
#include "../core/compilerdefs.h"
#include "../core/logger.h"
#include "../core/operand.h"
#include "../core/support.h"
#include "../core/type.h"
#include "../core/zone.h"
#include "../core/zonevector.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_ra
//! \{

#ifndef ASMJIT_NO_LOGGING
# define ASMJIT_RA_LOG_FORMAT(...)  \
  do {                              \
    if (logger)                     \
      logger->logf(__VA_ARGS__);    \
  } while (0)
# define ASMJIT_RA_LOG_COMPLEX(...) \
  do {                              \
    if (logger) {                   \
      __VA_ARGS__                   \
    }                               \
  } while (0)
#else
# define ASMJIT_RA_LOG_FORMAT(...) ((void)0)
# define ASMJIT_RA_LOG_COMPLEX(...) ((void)0)
#endif

class BaseRAPass;
class RABlock;
class BaseNode;
struct RAStackSlot;

typedef ZoneVector<RABlock*> RABlocks;
typedef ZoneVector<RAWorkReg*> RAWorkRegs;

//! Maximum number of consecutive registers aggregated from all supported backends.
static constexpr uint32_t kMaxConsecutiveRegs = 4;

//! Provides architecture constraints used by register allocator.
class RAConstraints {
public:
  //! \name Members
  //! \{

  Support::Array<RegMask, Globals::kNumVirtGroups> _availableRegs {};

  //! \}

  ASMJIT_NOINLINE Error init(Arch arch) noexcept {
    switch (arch) {
      case Arch::kX86:
      case Arch::kX64: {
        uint32_t registerCount = arch == Arch::kX86 ? 8 : 16;
        _availableRegs[RegGroup::kGp] = Support::lsbMask<RegMask>(registerCount) & ~Support::bitMask(4u);
        _availableRegs[RegGroup::kVec] = Support::lsbMask<RegMask>(registerCount);
        _availableRegs[RegGroup::kMask] = Support::lsbMask<RegMask>(8);
        _availableRegs[RegGroup::kExtraVirt3] = Support::lsbMask<RegMask>(8);
        return kErrorOk;
      }

      case Arch::kAArch64: {
        _availableRegs[RegGroup::kGp] = 0xFFFFFFFFu & ~Support::bitMask(18, 31u);
        _availableRegs[RegGroup::kVec] = 0xFFFFFFFFu;
        _availableRegs[RegGroup::kMask] = 0;
        _availableRegs[RegGroup::kExtraVirt3] = 0;
        return kErrorOk;
      }

      default:
        return DebugUtils::errored(kErrorInvalidArch);
    }
  }

  inline RegMask availableRegs(RegGroup group) const noexcept { return _availableRegs[group]; }
};

enum class RAStrategyType : uint8_t {
  kSimple  = 0,
  kComplex = 1
};
ASMJIT_DEFINE_ENUM_COMPARE(RAStrategyType)

enum class RAStrategyFlags : uint8_t {
  kNone = 0
};
ASMJIT_DEFINE_ENUM_FLAGS(RAStrategyFlags)

//! Register allocation strategy.
//!
//! The idea is to select the best register allocation strategy for each virtual register group based on the
//! complexity of the code.
struct RAStrategy {
  //! \name Members
  //! \{

  RAStrategyType _type = RAStrategyType::kSimple;
  RAStrategyFlags _flags = RAStrategyFlags::kNone;

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _type = RAStrategyType::kSimple;
    _flags = RAStrategyFlags::kNone;
  }

  ASMJIT_INLINE_NODEBUG RAStrategyType type() const noexcept { return _type; }
  ASMJIT_INLINE_NODEBUG void setType(RAStrategyType type) noexcept { _type = type; }

  ASMJIT_INLINE_NODEBUG bool isSimple() const noexcept { return _type == RAStrategyType::kSimple; }
  ASMJIT_INLINE_NODEBUG bool isComplex() const noexcept { return _type >= RAStrategyType::kComplex; }

  ASMJIT_INLINE_NODEBUG RAStrategyFlags flags() const noexcept { return _flags; }
  ASMJIT_INLINE_NODEBUG bool hasFlag(RAStrategyFlags flag) const noexcept { return Support::test(_flags, flag); }
  ASMJIT_INLINE_NODEBUG void addFlags(RAStrategyFlags flags) noexcept { _flags |= flags; }

  //! \}
};

//! Count of virtual or physical registers per group.
//!
//! \note This class uses 8-bit integers to represent counters, it's only used in places where this is sufficient,
//! for example total count of machine's physical registers, count of virtual registers per instruction, etc...
//! There is also `RALiveCount`, which uses 32-bit integers and is indeed much safer.
struct RARegCount {
  //! \name Members
  //! \{

  union {
    uint8_t _regs[4];
    uint32_t _packed;
  };

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Resets all counters to zero.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { _packed = 0; }

  //! \}

  //! \name Overloaded Operators
  //! \{

  inline uint8_t& operator[](RegGroup group) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _regs[size_t(group)];
  }

  inline const uint8_t& operator[](RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    return _regs[size_t(group)];
  }

  ASMJIT_INLINE_NODEBUG bool operator==(const RARegCount& other) const noexcept { return _packed == other._packed; }
  ASMJIT_INLINE_NODEBUG bool operator!=(const RARegCount& other) const noexcept { return _packed != other._packed; }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the count of registers by the given register `group`.
  inline uint32_t get(RegGroup group) const noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);

    uint32_t shift = Support::byteShiftOfDWordStruct(uint32_t(group));
    return (_packed >> shift) & uint32_t(0xFF);
  }

  //! Sets the register count by a register `group`.
  inline void set(RegGroup group, uint32_t n) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    ASMJIT_ASSERT(n <= 0xFF);

    uint32_t shift = Support::byteShiftOfDWordStruct(uint32_t(group));
    _packed = (_packed & ~uint32_t(0xFF << shift)) + (n << shift);
  }

  //! Adds the register count by a register `group`.
  inline void add(RegGroup group, uint32_t n = 1) noexcept {
    ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);
    ASMJIT_ASSERT(0xFF - uint32_t(_regs[size_t(group)]) >= n);

    uint32_t shift = Support::byteShiftOfDWordStruct(uint32_t(group));
    _packed += n << shift;
  }

  //! \}
};

//! Provides mapping that can be used to fast index architecture register groups.
struct RARegIndex : public RARegCount {
  //! Build register indexes based on the given `count` of registers.
  ASMJIT_FORCE_INLINE void buildIndexes(const RARegCount& count) noexcept {
    uint32_t x = uint32_t(count._regs[0]);
    uint32_t y = uint32_t(count._regs[1]) + x;
    uint32_t z = uint32_t(count._regs[2]) + y;

    ASMJIT_ASSERT(y <= 0xFF);
    ASMJIT_ASSERT(z <= 0xFF);
    _packed = Support::bytepack32_4x8(0, x, y, z);
  }
};

//! Registers mask.
struct RARegMask {
  //! \name Types
  //! \{

  using RegMasks = Support::Array<RegMask, Globals::kNumVirtGroups>;

  //! \}

  //! \name Members
  //! \{

  RegMasks _masks;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Initializes from other `RARegMask`.
  ASMJIT_INLINE_NODEBUG void init(const RARegMask& other) noexcept { _masks = other._masks; }
  //! Initializes directly from an array of masks.
  ASMJIT_INLINE_NODEBUG void init(const RegMasks& masks) noexcept { _masks = masks; }

  //! Resets all register masks to zero.
  ASMJIT_INLINE_NODEBUG void reset() noexcept { _masks.fill(0); }

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG bool operator==(const RARegMask& other) const noexcept { return _masks == other._masks; }
  ASMJIT_INLINE_NODEBUG bool operator!=(const RARegMask& other) const noexcept { return _masks != other._masks; }

  template<typename Index>
  inline uint32_t& operator[](const Index& index) noexcept { return _masks[index]; }

  template<typename Index>
  inline const uint32_t& operator[](const Index& index) const noexcept { return _masks[index]; }

  //! \}

  //! \name Utilities
  //! \{

  //! Tests whether all register masks are zero (empty).
  inline bool empty() const noexcept {
    return _masks.aggregate<Support::Or>() == 0;
  }

  inline bool has(RegGroup group, RegMask mask = 0xFFFFFFFFu) const noexcept {
    return (_masks[group] & mask) != 0;
  }

  template<class Operator>
  inline void op(const RARegMask& other) noexcept {
    _masks.combine<Operator>(other._masks);
  }

  template<class Operator>
  inline void op(RegGroup group, RegMask mask) noexcept {
    _masks[group] = Operator::op(_masks[group], mask);
  }

  inline void clear(RegGroup group, RegMask mask) noexcept {
    _masks[group] = _masks[group] & ~mask;
  }

  //! \}
};

//! Information associated with each instruction, propagated to blocks, loops, and the whole function. This
//! information can be used to do minor decisions before the register allocator tries to do its job. For
//! example to use fast register allocation inside a block or loop it cannot have clobbered and/or fixed
//! registers, etc...
class RARegsStats {
public:
  //! \name Constants
  //! \{

  enum Index : uint32_t {
    kIndexUsed       = 0,
    kIndexFixed      = 8,
    kIndexClobbered  = 16
  };

  enum Mask : uint32_t {
    kMaskUsed        = 0xFFu << kIndexUsed,
    kMaskFixed       = 0xFFu << kIndexFixed,
    kMaskClobbered   = 0xFFu << kIndexClobbered
  };

  //! \}

  //! \name Members
  //! \{

  uint32_t _packed = 0;

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG void reset() noexcept { _packed = 0; }
  ASMJIT_INLINE_NODEBUG void combineWith(const RARegsStats& other) noexcept { _packed |= other._packed; }

  ASMJIT_INLINE_NODEBUG bool hasUsed() const noexcept { return (_packed & kMaskUsed) != 0u; }
  ASMJIT_INLINE_NODEBUG bool hasUsed(RegGroup group) const noexcept { return (_packed & Support::bitMask(kIndexUsed + uint32_t(group))) != 0u; }
  ASMJIT_INLINE_NODEBUG void makeUsed(RegGroup group) noexcept { _packed |= Support::bitMask(kIndexUsed + uint32_t(group)); }

  ASMJIT_INLINE_NODEBUG bool hasFixed() const noexcept { return (_packed & kMaskFixed) != 0u; }
  ASMJIT_INLINE_NODEBUG bool hasFixed(RegGroup group) const noexcept { return (_packed & Support::bitMask(kIndexFixed + uint32_t(group))) != 0u; }
  ASMJIT_INLINE_NODEBUG void makeFixed(RegGroup group) noexcept { _packed |= Support::bitMask(kIndexFixed + uint32_t(group)); }

  ASMJIT_INLINE_NODEBUG bool hasClobbered() const noexcept { return (_packed & kMaskClobbered) != 0u; }
  ASMJIT_INLINE_NODEBUG bool hasClobbered(RegGroup group) const noexcept { return (_packed & Support::bitMask(kIndexClobbered + uint32_t(group))) != 0u; }
  ASMJIT_INLINE_NODEBUG void makeClobbered(RegGroup group) noexcept { _packed |= Support::bitMask(kIndexClobbered + uint32_t(group)); }

  //! \}
};

//! Count of live registers, per group.
class RALiveCount {
public:
  //! \name Members
  //! \{

  Support::Array<uint32_t, Globals::kNumVirtGroups> n {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG RALiveCount() noexcept = default;
  ASMJIT_INLINE_NODEBUG RALiveCount(const RALiveCount& other) noexcept = default;

  ASMJIT_INLINE_NODEBUG void init(const RALiveCount& other) noexcept { n = other.n; }
  ASMJIT_INLINE_NODEBUG void reset() noexcept { n.fill(0); }

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG RALiveCount& operator=(const RALiveCount& other) noexcept = default;

  inline uint32_t& operator[](RegGroup group) noexcept { return n[group]; }
  inline const uint32_t& operator[](RegGroup group) const noexcept { return n[group]; }

  //! \}

  //! \name Utilities
  //! \{

  template<class Operator>
  inline void op(const RALiveCount& other) noexcept { n.combine<Operator>(other.n); }

  //! \}
};

struct RALiveInterval {
  //! \name Constants
  //! \{

  enum : uint32_t {
    kNaN = 0,
    kInf = 0xFFFFFFFFu
  };

  //! \}

  //! \name Members
  //! \{

  uint32_t a, b;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG RALiveInterval() noexcept : a(0), b(0) {}
  ASMJIT_INLINE_NODEBUG RALiveInterval(uint32_t a, uint32_t b) noexcept : a(a), b(b) {}
  ASMJIT_INLINE_NODEBUG RALiveInterval(const RALiveInterval& other) noexcept : a(other.a), b(other.b) {}

  ASMJIT_INLINE_NODEBUG void init(uint32_t aVal, uint32_t bVal) noexcept {
    a = aVal;
    b = bVal;
  }
  ASMJIT_INLINE_NODEBUG void init(const RALiveInterval& other) noexcept { init(other.a, other.b); }
  ASMJIT_INLINE_NODEBUG void reset() noexcept { init(0, 0); }

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG RALiveInterval& operator=(const RALiveInterval& other) = default;

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool isValid() const noexcept { return a < b; }
  ASMJIT_INLINE_NODEBUG uint32_t width() const noexcept { return b - a; }

  //! \}
};

//! Live span with payload of type `T`.
template<typename T>
class RALiveSpan : public RALiveInterval, public T {
public:
  //! \name Types
  //! \{

  typedef T DataType;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG RALiveSpan() noexcept : RALiveInterval(), T() {}
  ASMJIT_INLINE_NODEBUG RALiveSpan(const RALiveSpan<T>& other) noexcept : RALiveInterval(other), T() {}
  ASMJIT_INLINE_NODEBUG RALiveSpan(const RALiveInterval& interval, const T& data) noexcept : RALiveInterval(interval), T(data) {}
  ASMJIT_INLINE_NODEBUG RALiveSpan(uint32_t a, uint32_t b) noexcept : RALiveInterval(a, b), T() {}
  ASMJIT_INLINE_NODEBUG RALiveSpan(uint32_t a, uint32_t b, const T& data) noexcept : RALiveInterval(a, b), T(data) {}

  ASMJIT_INLINE_NODEBUG void init(const RALiveSpan<T>& other) noexcept {
    RALiveInterval::init(static_cast<const RALiveInterval&>(other));
    T::init(static_cast<const T&>(other));
  }

  ASMJIT_INLINE_NODEBUG void init(const RALiveSpan<T>& span, const T& data) noexcept {
    RALiveInterval::init(static_cast<const RALiveInterval&>(span));
    T::init(data);
  }

  ASMJIT_INLINE_NODEBUG void init(const RALiveInterval& interval, const T& data) noexcept {
    RALiveInterval::init(interval);
    T::init(data);
  }

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG RALiveSpan& operator=(const RALiveSpan& other) {
    init(other);
    return *this;
  }

  //! \}
};

//! Vector of `RALiveSpan<T>` with additional convenience API.
template<typename T>
class RALiveSpans {
public:
  ASMJIT_NONCOPYABLE(RALiveSpans)

  typedef typename T::DataType DataType;
  ZoneVector<T> _data;

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG RALiveSpans() noexcept : _data() {}

  ASMJIT_INLINE_NODEBUG void reset() noexcept { _data.reset(); }
  ASMJIT_INLINE_NODEBUG void release(ZoneAllocator* allocator) noexcept { _data.release(allocator); }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _data.empty(); }
  ASMJIT_INLINE_NODEBUG uint32_t size() const noexcept { return _data.size(); }

  ASMJIT_INLINE_NODEBUG T* data() noexcept { return _data.data(); }
  ASMJIT_INLINE_NODEBUG const T* data() const noexcept { return _data.data(); }

  ASMJIT_INLINE_NODEBUG bool isOpen() const noexcept {
    uint32_t size = _data.size();
    return size > 0 && _data[size - 1].b == RALiveInterval::kInf;
  }

  //! \}

  //! \name Utilities
  //! \{

  ASMJIT_INLINE_NODEBUG void swap(RALiveSpans<T>& other) noexcept { _data.swap(other._data); }

  //! Open the current live span.
  ASMJIT_FORCE_INLINE Error openAt(ZoneAllocator* allocator, uint32_t start, uint32_t end) noexcept {
    bool wasOpen;
    return openAt(allocator, start, end, wasOpen);
  }

  ASMJIT_FORCE_INLINE Error openAt(ZoneAllocator* allocator, uint32_t start, uint32_t end, bool& wasOpen) noexcept {
    uint32_t size = _data.size();
    wasOpen = false;

    if (size > 0) {
      T& last = _data[size - 1];
      if (last.b >= start) {
        wasOpen = last.b > start;
        last.b = end;
        return kErrorOk;
      }
    }

    return _data.append(allocator, T(start, end));
  }

  ASMJIT_FORCE_INLINE void closeAt(uint32_t end) noexcept {
    ASMJIT_ASSERT(!empty());

    uint32_t size = _data.size();
    _data[size - 1].b = end;
  }

  //! Returns the sum of width of all spans.
  //!
  //! \note Don't overuse, this iterates over all spans so it's O(N). It should be only called once and then cached.
  inline uint32_t width() const noexcept {
    uint32_t width = 0;
    for (const T& span : _data)
      width += span.width();
    return width;
  }

  ASMJIT_INLINE_NODEBUG T& operator[](uint32_t index) noexcept { return _data[index]; }
  ASMJIT_INLINE_NODEBUG const T& operator[](uint32_t index) const noexcept { return _data[index]; }

  ASMJIT_INLINE_NODEBUG bool intersects(const RALiveSpans<T>& other) const noexcept {
    return intersects(*this, other);
  }

  ASMJIT_FORCE_INLINE Error nonOverlappingUnionOf(ZoneAllocator* allocator, const RALiveSpans<T>& x, const RALiveSpans<T>& y, const DataType& yData) noexcept {
    uint32_t finalSize = x.size() + y.size();
    ASMJIT_PROPAGATE(_data.growingReserve(allocator, finalSize));

    T* dstPtr = _data.data();
    const T* xSpan = x.data();
    const T* ySpan = y.data();

    const T* xEnd = xSpan + x.size();
    const T* yEnd = ySpan + y.size();

    // Loop until we have intersection or either `xSpan == xEnd` or `ySpan == yEnd`, which means that there is no
    // intersection. We advance either `xSpan` or `ySpan` depending on their ranges.
    if (xSpan != xEnd && ySpan != yEnd) {
      uint32_t xa, ya;
      xa = xSpan->a;
      for (;;) {
        while (ySpan->b <= xa) {
          dstPtr->init(*ySpan, yData);
          dstPtr++;
          if (++ySpan == yEnd)
            goto Done;
        }

        ya = ySpan->a;
        while (xSpan->b <= ya) {
          *dstPtr++ = *xSpan;
          if (++xSpan == xEnd)
            goto Done;
        }

        // We know that `xSpan->b > ySpan->a`, so check if `ySpan->b > xSpan->a`.
        xa = xSpan->a;
        if (ySpan->b > xa)
          return 0xFFFFFFFFu;
      }
    }

  Done:
    while (xSpan != xEnd) {
      *dstPtr++ = *xSpan++;
    }

    while (ySpan != yEnd) {
      dstPtr->init(*ySpan, yData);
      dstPtr++;
      ySpan++;
    }

    _data._setEndPtr(dstPtr);
    return kErrorOk;
  }

  static ASMJIT_FORCE_INLINE bool intersects(const RALiveSpans<T>& x, const RALiveSpans<T>& y) noexcept {
    const T* xSpan = x.data();
    const T* ySpan = y.data();

    const T* xEnd = xSpan + x.size();
    const T* yEnd = ySpan + y.size();

    // Loop until we have intersection or either `xSpan == xEnd` or `ySpan == yEnd`, which means that there is no
    // intersection. We advance either `xSpan` or `ySpan` depending on their end positions.
    if (xSpan == xEnd || ySpan == yEnd)
      return false;

    uint32_t xa, ya;
    xa = xSpan->a;

    for (;;) {
      while (ySpan->b <= xa)
        if (++ySpan == yEnd)
          return false;

      ya = ySpan->a;
      while (xSpan->b <= ya)
        if (++xSpan == xEnd)
          return false;

      // We know that `xSpan->b > ySpan->a`, so check if `ySpan->b > xSpan->a`.
      xa = xSpan->a;
      if (ySpan->b > xa)
        return true;
    }
  }

  //! \}
};

//! Statistics about a register liveness.
class RALiveStats {
public:
  uint32_t _width = 0;
  float _freq = 0.0f;
  float _priority = 0.0f;

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG uint32_t width() const noexcept { return _width; }
  ASMJIT_INLINE_NODEBUG float freq() const noexcept { return _freq; }
  ASMJIT_INLINE_NODEBUG float priority() const noexcept { return _priority; }

  //! \}
};

struct LiveRegData {
  uint32_t id;

  ASMJIT_INLINE_NODEBUG explicit LiveRegData(uint32_t id = BaseReg::kIdBad) noexcept : id(id) {}
  ASMJIT_INLINE_NODEBUG LiveRegData(const LiveRegData& other) noexcept = default;

  ASMJIT_INLINE_NODEBUG void init(const LiveRegData& other) noexcept { id = other.id; }

  ASMJIT_INLINE_NODEBUG bool operator==(const LiveRegData& other) const noexcept { return id == other.id; }
  ASMJIT_INLINE_NODEBUG bool operator!=(const LiveRegData& other) const noexcept { return id != other.id; }
};

typedef RALiveSpan<LiveRegData> LiveRegSpan;
typedef RALiveSpans<LiveRegSpan> LiveRegSpans;

//! Flags used by \ref RATiedReg.
//!
//! Register access information is encoded in 4 flags in total:
//!
//!   - `kRead`  - Register is Read    (ReadWrite if combined with `kWrite`).
//!   - `kWrite` - Register is Written (ReadWrite if combined with `kRead`).
//!   - `kUse`   - Encoded as Read or ReadWrite.
//!   - `kOut`   - Encoded as WriteOnly.
//!
//! Let's describe all of these on two X86 instructions:
//!
//!   - ADD x{R|W|Use},  x{R|Use}              -> {x:R|W|Use            }
//!   - LEA x{  W|Out}, [x{R|Use} + x{R|Out}]  -> {x:R|W|Use|Out        }
//!   - ADD x{R|W|Use},  y{R|Use}              -> {x:R|W|Use     y:R|Use}
//!   - LEA x{  W|Out}, [x{R|Use} + y{R|Out}]  -> {x:R|W|Use|Out y:R|Use}
//!
//! It should be obvious from the example above how these flags get created. Each operand contains READ/WRITE
//! information, which is then merged to RATiedReg's flags. However, we also need to represent the possibility
//! to view the operation as two independent operations - USE and OUT, because the register allocator first
//! allocates USE registers, and then assigns OUT registers independently of USE registers.
enum class RATiedFlags : uint32_t {
  //! No flags.
  kNone = 0,

  // Access Flags
  // ------------

  //! Register is read.
  kRead = uint32_t(OpRWFlags::kRead),
  //! Register is written.
  kWrite = uint32_t(OpRWFlags::kWrite),
  //! Register both read and written.
  kRW = uint32_t(OpRWFlags::kRW),

  // Use / Out Flags
  // ---------------

  //! Register has a USE slot (read/rw).
  kUse = 0x00000004u,
  //! Register has an OUT slot (write-only).
  kOut = 0x00000008u,
  //! Register in USE slot can be patched to memory.
  kUseRM = 0x00000010u,
  //! Register in OUT slot can be patched to memory.
  kOutRM = 0x00000020u,

  //! Register has a fixed USE slot.
  kUseFixed = 0x00000040u,
  //! Register has a fixed OUT slot.
  kOutFixed = 0x00000080u,
  //! Register USE slot has been allocated.
  kUseDone = 0x00000100u,
  //! Register OUT slot has been allocated.
  kOutDone = 0x00000200u,

  // Consecutive Flags / Data
  // ------------------------

  kUseConsecutive = 0x00000400u,
  kOutConsecutive = 0x00000800u,
  kLeadConsecutive = 0x00001000u,
  kConsecutiveData = 0x00006000u,

  // Other Constraints
  // -----------------

  kUnique = 0x00008000u,

  // Liveness Flags
  // --------------

  //! Register must be duplicated (function call only).
  kDuplicate = 0x00010000u,
  //! Last occurrence of this VirtReg in basic block.
  kLast = 0x00020000u,
  //! Kill this VirtReg after use.
  kKill = 0x00040000u,

  // X86 Specific Flags
  // ------------------

  // Architecture specific flags are used during RATiedReg building to ensure that architecture-specific constraints
  // are handled properly. These flags are not really needed after RATiedReg[] is built and copied to `RAInst`.

  //! This RATiedReg references GPB-LO or GPB-HI.
  kX86_Gpb = 0x01000000u,

  // Instruction Flags (Never used by RATiedReg)
  // -------------------------------------------

  //! Instruction has been patched to address a memory location instead of a register.
  //!
  //! This is currently only possible on X86 or X86_64 targets. It informs rewriter to rewrite the instruction if
  //! necessary.
  kInst_RegToMemPatched = 0x40000000u,

  //! Instruction is transformable to another instruction if necessary.
  //!
  //! This is flag that is only used by \ref RAInst to inform register allocator that the instruction has some
  //! constraints that can only be solved by transforming the instruction into another instruction, most likely
  //! by changing its InstId.
  kInst_IsTransformable = 0x80000000u
};
ASMJIT_DEFINE_ENUM_FLAGS(RATiedFlags)

static_assert(uint32_t(RATiedFlags::kRead ) == 0x1, "RATiedFlags::kRead must be 0x1");
static_assert(uint32_t(RATiedFlags::kWrite) == 0x2, "RATiedFlags::kWrite must be 0x2");
static_assert(uint32_t(RATiedFlags::kRW   ) == 0x3, "RATiedFlags::kRW must be 0x3");

//! Tied register merges one ore more register operand into a single entity. It contains information about its access
//! (Read|Write) and allocation slots (Use|Out) that are used by the register allocator and liveness analysis.
struct RATiedReg {
  //! \name Members
  //! \{

  //! WorkReg id.
  uint32_t _workId;
  //! WorkReg id that is an immediate consecutive parent of this register, or Globals::kInvalidId if it has no parent.
  uint32_t _consecutiveParent;
  //! Allocation flags.
  RATiedFlags _flags;

  union {
    struct {
      //! How many times the VirtReg is referenced in all operands.
      uint8_t _refCount;
      //! Size of a memory operand in case that it's use instead of the register.
      uint8_t _rmSize;
      //! Physical register for use operation (ReadOnly / ReadWrite).
      uint8_t _useId;
      //! Physical register for out operation (WriteOnly).
      uint8_t _outId;
    };
    //! Packed data.
    uint32_t _packed;
  };

  //! Registers where inputs {R|X} can be allocated to.
  RegMask _useRegMask;
  //! Registers where outputs {W} can be allocated to.
  RegMask _outRegMask;
  //! Indexes used to rewrite USE regs.
  uint32_t _useRewriteMask;
  //! Indexes used to rewrite OUT regs.
  uint32_t _outRewriteMask;

  //! \}

  //! \name Statics
  //! \{

  static inline RATiedFlags consecutiveDataToFlags(uint32_t offset) noexcept {
    ASMJIT_ASSERT(offset < 4);
    constexpr uint32_t kOffsetShift = Support::ConstCTZ<uint32_t(RATiedFlags::kConsecutiveData)>::value;
    return (RATiedFlags)(offset << kOffsetShift);
  }

  static inline uint32_t consecutiveDataFromFlags(RATiedFlags flags) noexcept {
    constexpr uint32_t kOffsetShift = Support::ConstCTZ<uint32_t(RATiedFlags::kConsecutiveData)>::value;
    return uint32_t(flags & RATiedFlags::kConsecutiveData) >> kOffsetShift;
  }

  //! \}

  //! \name Construction & Destruction
  //! \{

  inline void init(uint32_t workId, RATiedFlags flags, RegMask useRegMask, uint32_t useId, uint32_t useRewriteMask, RegMask outRegMask, uint32_t outId, uint32_t outRewriteMask, uint32_t rmSize = 0, uint32_t consecutiveParent = Globals::kInvalidId) noexcept {
    _workId = workId;
    _consecutiveParent = consecutiveParent;
    _flags = flags;
    _refCount = 1;
    _rmSize = uint8_t(rmSize);
    _useId = uint8_t(useId);
    _outId = uint8_t(outId);
    _useRegMask = useRegMask;
    _outRegMask = outRegMask;
    _useRewriteMask = useRewriteMask;
    _outRewriteMask = outRewriteMask;
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the associated WorkReg id.
  ASMJIT_INLINE_NODEBUG uint32_t workId() const noexcept { return _workId; }

  ASMJIT_INLINE_NODEBUG bool hasConsecutiveParent() const noexcept { return _consecutiveParent != Globals::kInvalidId; }
  ASMJIT_INLINE_NODEBUG uint32_t consecutiveParent() const noexcept { return _consecutiveParent; }
  ASMJIT_INLINE_NODEBUG uint32_t consecutiveData() const noexcept { return consecutiveDataFromFlags(_flags); }

  //! Returns TiedReg flags.
  ASMJIT_INLINE_NODEBUG RATiedFlags flags() const noexcept { return _flags; }
  //! Checks if the given `flag` is set.
  ASMJIT_INLINE_NODEBUG bool hasFlag(RATiedFlags flag) const noexcept { return Support::test(_flags, flag); }
  //! Adds tied register flags.
  ASMJIT_INLINE_NODEBUG void addFlags(RATiedFlags flags) noexcept { _flags |= flags; }

  //! Tests whether the register is read (writes `true` also if it's Read/Write).
  ASMJIT_INLINE_NODEBUG bool isRead() const noexcept { return hasFlag(RATiedFlags::kRead); }
  //! Tests whether the register is written (writes `true` also if it's Read/Write).
  ASMJIT_INLINE_NODEBUG bool isWrite() const noexcept { return hasFlag(RATiedFlags::kWrite); }
  //! Tests whether the register is read only.
  ASMJIT_INLINE_NODEBUG bool isReadOnly() const noexcept { return (_flags & RATiedFlags::kRW) == RATiedFlags::kRead; }
  //! Tests whether the register is write only.
  ASMJIT_INLINE_NODEBUG bool isWriteOnly() const noexcept { return (_flags & RATiedFlags::kRW) == RATiedFlags::kWrite; }
  //! Tests whether the register is read and written.
  ASMJIT_INLINE_NODEBUG bool isReadWrite() const noexcept { return (_flags & RATiedFlags::kRW) == RATiedFlags::kRW; }

  //! Tests whether the tied register has use operand (Read/ReadWrite).
  ASMJIT_INLINE_NODEBUG bool isUse() const noexcept { return hasFlag(RATiedFlags::kUse); }
  //! Tests whether the tied register has out operand (Write).
  ASMJIT_INLINE_NODEBUG bool isOut() const noexcept { return hasFlag(RATiedFlags::kOut); }

  //! Tests whether the tied register has \ref RATiedFlags::kLeadConsecutive flag set.
  ASMJIT_INLINE_NODEBUG bool isLeadConsecutive() const noexcept { return hasFlag(RATiedFlags::kLeadConsecutive); }
  //! Tests whether the tied register has \ref RATiedFlags::kUseConsecutive flag set.
  ASMJIT_INLINE_NODEBUG bool isUseConsecutive() const noexcept { return hasFlag(RATiedFlags::kUseConsecutive); }
  //! Tests whether the tied register has \ref RATiedFlags::kOutConsecutive flag set.
  ASMJIT_INLINE_NODEBUG bool isOutConsecutive() const noexcept { return hasFlag(RATiedFlags::kOutConsecutive); }

  //! Tests whether the tied register must be unique (cannot be allocated to any other allocated register).
  ASMJIT_INLINE_NODEBUG bool isUnique() const noexcept { return hasFlag(RATiedFlags::kUnique); }

  //! Tests whether the tied register has any consecutive flag.
  ASMJIT_INLINE_NODEBUG bool hasAnyConsecutiveFlag() const noexcept { return hasFlag(RATiedFlags::kLeadConsecutive | RATiedFlags::kUseConsecutive | RATiedFlags::kOutConsecutive); }

  //! Tests whether the USE slot can be patched to memory operand.
  ASMJIT_INLINE_NODEBUG bool hasUseRM() const noexcept { return hasFlag(RATiedFlags::kUseRM); }
  //! Tests whether the OUT slot can be patched to memory operand.
  ASMJIT_INLINE_NODEBUG bool hasOutRM() const noexcept { return hasFlag(RATiedFlags::kOutRM); }

  ASMJIT_INLINE_NODEBUG uint32_t rmSize() const noexcept { return _rmSize; }

  inline void makeReadOnly() noexcept {
    _flags = (_flags & ~(RATiedFlags::kOut | RATiedFlags::kWrite)) | RATiedFlags::kUse;
    _useRewriteMask |= _outRewriteMask;
    _outRewriteMask = 0;
  }

  inline void makeWriteOnly() noexcept {
    _flags = (_flags & ~(RATiedFlags::kUse | RATiedFlags::kRead)) | RATiedFlags::kOut;
    _outRewriteMask |= _useRewriteMask;
    _useRewriteMask = 0;
  }

  //! Tests whether the register would duplicate.
  ASMJIT_INLINE_NODEBUG bool isDuplicate() const noexcept { return hasFlag(RATiedFlags::kDuplicate); }

  //! Tests whether the register (and the instruction it's part of) appears last in the basic block.
  ASMJIT_INLINE_NODEBUG bool isLast() const noexcept { return hasFlag(RATiedFlags::kLast); }
  //! Tests whether the register should be killed after USEd and/or OUTed.
  ASMJIT_INLINE_NODEBUG bool isKill() const noexcept { return hasFlag(RATiedFlags::kKill); }

  //! Tests whether the register is OUT or KILL (used internally by local register allocator).
  ASMJIT_INLINE_NODEBUG bool isOutOrKill() const noexcept { return hasFlag(RATiedFlags::kOut | RATiedFlags::kKill); }

  //! Returns a register mask that describes allocable USE registers (Read/ReadWrite access).
  ASMJIT_INLINE_NODEBUG RegMask useRegMask() const noexcept { return _useRegMask; }
  //! Returns a register mask that describes allocable OUT registers (WriteOnly access).
  ASMJIT_INLINE_NODEBUG RegMask outRegMask() const noexcept { return _outRegMask; }

  ASMJIT_INLINE_NODEBUG uint32_t refCount() const noexcept { return _refCount; }
  ASMJIT_INLINE_NODEBUG void addRefCount(uint32_t n = 1) noexcept { _refCount = uint8_t(_refCount + n); }

  //! Tests whether the register must be allocated to a fixed physical register before it's used.
  ASMJIT_INLINE_NODEBUG bool hasUseId() const noexcept { return _useId != BaseReg::kIdBad; }
  //! Tests whether the register must be allocated to a fixed physical register before it's written.
  ASMJIT_INLINE_NODEBUG bool hasOutId() const noexcept { return _outId != BaseReg::kIdBad; }

  //! Returns a physical register id used for 'use' operation.
  ASMJIT_INLINE_NODEBUG uint32_t useId() const noexcept { return _useId; }
  //! Returns a physical register id used for 'out' operation.
  ASMJIT_INLINE_NODEBUG uint32_t outId() const noexcept { return _outId; }

  ASMJIT_INLINE_NODEBUG uint32_t useRewriteMask() const noexcept { return _useRewriteMask; }
  ASMJIT_INLINE_NODEBUG uint32_t outRewriteMask() const noexcept { return _outRewriteMask; }

  //! Sets a physical register used for 'use' operation.
  ASMJIT_INLINE_NODEBUG void setUseId(uint32_t index) noexcept { _useId = uint8_t(index); }
  //! Sets a physical register used for 'out' operation.
  ASMJIT_INLINE_NODEBUG void setOutId(uint32_t index) noexcept { _outId = uint8_t(index); }

  ASMJIT_INLINE_NODEBUG bool isUseDone() const noexcept { return hasFlag(RATiedFlags::kUseDone); }
  ASMJIT_INLINE_NODEBUG bool isOutDone() const noexcept { return hasFlag(RATiedFlags::kOutDone); }

  ASMJIT_INLINE_NODEBUG void markUseDone() noexcept { addFlags(RATiedFlags::kUseDone); }
  ASMJIT_INLINE_NODEBUG void markOutDone() noexcept { addFlags(RATiedFlags::kOutDone); }

  //! \}
};

//! Flags used by \ref RAWorkReg.
enum class RAWorkRegFlags : uint32_t {
  //! No flags.
  kNone = 0,

  //! This register has already been allocated.
  kAllocated = 0x00000001u,
  //! Has been coalesced to another WorkReg.
  kCoalesced = 0x00000002u,

  //! This register is used across multiple basic blocks - this can be used as an optimization.
  kMultipleBasicBlocks = 0x00000004u,

  //! Set when this register is used as a LEAD consecutive register at least once.
  kLeadConsecutive = 0x00000010u,
  //! Used to mark consecutive registers during processing.
  kProcessedConsecutive = 0x00000020u,

  //! Stack slot has to be allocated.
  kStackUsed = 0x00000100u,
  //! Stack allocation is preferred.
  kStackPreferred = 0x00000200u,
  //! Marked for stack argument reassignment.
  kStackArgToStack = 0x00000400u
};
ASMJIT_DEFINE_ENUM_FLAGS(RAWorkRegFlags)

//! Work register provides additional data of \ref VirtReg that is used by register allocator.
//!
//! In general when a virtual register is found by register allocator it maps it to \ref RAWorkReg
//! and then only works with it. The reason for such mapping is that users can create many virtual
//! registers, which are not used inside a register allocation scope (which is currently always a
//! function). So register allocator basically scans the function for virtual registers and maps
//! them into WorkRegs, which receive a temporary ID (workId), which starts from zero. This WorkId
//! is then used in bit-arrays and other mappings.
class RAWorkReg {
public:
  ASMJIT_NONCOPYABLE(RAWorkReg)

  //! \name Constants
  //! \{

  enum : uint32_t {
    kIdNone = 0xFFFFFFFFu
  };

  enum : uint32_t {
    kNoArgIndex = 0xFFu
  };

  //! \}

  //! \name Members
  //! \{

  //! RAPass specific ID used during analysis and allocation.
  uint32_t _workId = 0;
  //! Copy of ID used by \ref VirtReg.
  uint32_t _virtId = 0;

  //! Permanent association with \ref VirtReg.
  VirtReg* _virtReg = nullptr;
  //! Temporary association with \ref RATiedReg.
  RATiedReg* _tiedReg = nullptr;
  //! Stack slot associated with the register.
  RAStackSlot* _stackSlot = nullptr;

  //! Copy of a signature used by \ref VirtReg.
  OperandSignature _signature {};
  //! RAPass specific flags used during analysis and allocation.
  RAWorkRegFlags _flags = RAWorkRegFlags::kNone;

  //! The identifier of a basic block this register lives in.
  //!
  //! If this register is used by multiple basic blocks, the id would always be `kIdNone`. However, if the register
  //! lives in a single basic block, the id would be a valid block id, and `_flags` would not contain `kMultipleBasicBlocks`.
  uint32_t _singleBasicBlockId = kIdNone;

  //! Constains all USE ids collected from all instructions.
  //!
  //! If this mask is non-zero and not a power of two, it means that the register is used multiple times in
  //! instructions where it requires to have a different use ID. This means that in general it's not possible
  //! to keep this register in a single home.
  RegMask _useIdMask = 0;
  //! Preferred mask of registers (if non-zero) to allocate this register to.
  //!
  //! If this mask is zero it means that either there is no intersection of preferred registers collected from all
  //! TiedRegs or there is no preference at all (the register can be allocated to any register all the time).
  RegMask _preferredMask = 0xFFFFFFFFu;
  //! Consecutive mask, which was collected from all instructions where this register was used as a lead consecutive
  //! register.
  RegMask _consecutiveMask = 0xFFFFFFFFu;
  //! IDs of all physical registers that are clobbered during the lifetime of this WorkReg.
  //!
  //! This mask should be updated by `RAPass::buildLiveness()`, because it's global and should
  //! be updated after unreachable code has been removed.
  RegMask _clobberSurvivalMask = 0;
  //! IDs of all physical registers this WorkReg has been allocated to.
  RegMask _allocatedMask = 0;

  //! A byte-mask where each bit represents one valid byte of the register.
  uint64_t _regByteMask = 0;

  //! Argument index (or `kNoArgIndex` if none).
  uint8_t _argIndex = kNoArgIndex;
  //! Argument value index in the pack (0 by default).
  uint8_t _argValueIndex = 0;
  //! Global home register ID (if any, assigned by RA).
  uint8_t _homeRegId = BaseReg::kIdBad;
  //! Global hint register ID (provided by RA or user).
  uint8_t _hintRegId = BaseReg::kIdBad;

  //! Live spans of the `VirtReg`.
  LiveRegSpans _liveSpans {};
  //! Live statistics.
  RALiveStats _liveStats {};

  //! All nodes that read/write this VirtReg/WorkReg.
  ZoneVector<BaseNode*> _refs {};
  //! All nodes that write to this VirtReg/WorkReg.
  ZoneVector<BaseNode*> _writes {};

  //! Contains work IDs of all immediate consecutive registers of this register.
  //!
  //! \note This bit array only contains immediate consecutives. This means that if this is a register that is
  //! followed by 3 more registers, then it would still have only a single immediate. The rest registers would
  //! have immediate consecutive registers as well, except the last one.
  ZoneBitVector _immediateConsecutives {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG RAWorkReg(VirtReg* vReg, uint32_t workId) noexcept
    : _workId(workId),
      _virtId(vReg->id()),
      _virtReg(vReg),
      _signature(vReg->signature()),
      _hintRegId(uint8_t(vReg->homeIdHint())) {}

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG uint32_t workId() const noexcept { return _workId; }
  ASMJIT_INLINE_NODEBUG uint32_t virtId() const noexcept { return _virtId; }

  ASMJIT_INLINE_NODEBUG const char* name() const noexcept { return _virtReg->name(); }
  ASMJIT_INLINE_NODEBUG uint32_t nameSize() const noexcept { return _virtReg->nameSize(); }

  ASMJIT_INLINE_NODEBUG TypeId typeId() const noexcept { return _virtReg->typeId(); }

  ASMJIT_INLINE_NODEBUG RAWorkRegFlags flags() const noexcept { return _flags; }
  ASMJIT_INLINE_NODEBUG bool hasFlag(RAWorkRegFlags flag) const noexcept { return Support::test(_flags, flag); }
  ASMJIT_INLINE_NODEBUG void addFlags(RAWorkRegFlags flags) noexcept { _flags |= flags; }

  ASMJIT_INLINE_NODEBUG bool isAllocated() const noexcept { return hasFlag(RAWorkRegFlags::kAllocated); }
  ASMJIT_INLINE_NODEBUG void markAllocated() noexcept { addFlags(RAWorkRegFlags::kAllocated); }

  ASMJIT_INLINE_NODEBUG bool isWithinSingleBasicBlock() const noexcept { return !hasFlag(RAWorkRegFlags::kMultipleBasicBlocks); }
  ASMJIT_INLINE_NODEBUG uint32_t singleBasicBlockId() const noexcept { return _singleBasicBlockId; }

  //! Called when this register appeared in a basic block having `blockId`.
  //!
  //! This function just sets the basic block of this work register, and then later, when this register is processed
  //! again it's compared with all other basic blocks it appears in so the flag `kMultipleBasicBlocks` can be properly
  //! set when the compared basic blocks differ.
  ASMJIT_INLINE_NODEBUG void assignBasicBlock(uint32_t blockId) noexcept { _singleBasicBlockId = blockId; }

  //! Marks this register as using multiple basic blocks, which means reseting the single basic block identifier and
  //! adding `kMultipleBasicBlocks` flag.
  ASMJIT_INLINE_NODEBUG void markUseOfMultipleBasicBlocks() noexcept {
    _singleBasicBlockId = Globals::kInvalidId;
    addFlags(RAWorkRegFlags::kMultipleBasicBlocks);
  }

  ASMJIT_INLINE_NODEBUG bool isLeadConsecutive() const noexcept { return hasFlag(RAWorkRegFlags::kLeadConsecutive); }
  ASMJIT_INLINE_NODEBUG void markLeadConsecutive() noexcept { addFlags(RAWorkRegFlags::kLeadConsecutive); }

  ASMJIT_INLINE_NODEBUG bool isProcessedConsecutive() const noexcept { return hasFlag(RAWorkRegFlags::kProcessedConsecutive); }
  ASMJIT_INLINE_NODEBUG void markProcessedConsecutive() noexcept { addFlags(RAWorkRegFlags::kProcessedConsecutive); }

  ASMJIT_INLINE_NODEBUG bool isStackUsed() const noexcept { return hasFlag(RAWorkRegFlags::kStackUsed); }
  ASMJIT_INLINE_NODEBUG void markStackUsed() noexcept { addFlags(RAWorkRegFlags::kStackUsed); }

  ASMJIT_INLINE_NODEBUG bool isStackPreferred() const noexcept { return hasFlag(RAWorkRegFlags::kStackPreferred); }
  ASMJIT_INLINE_NODEBUG void markStackPreferred() noexcept { addFlags(RAWorkRegFlags::kStackPreferred); }

  //! Tests whether this RAWorkReg has been coalesced with another one (cannot be used anymore).
  ASMJIT_INLINE_NODEBUG bool isCoalesced() const noexcept { return hasFlag(RAWorkRegFlags::kCoalesced); }

  ASMJIT_INLINE_NODEBUG OperandSignature signature() const noexcept { return _signature; }
  ASMJIT_INLINE_NODEBUG RegType type() const noexcept { return _signature.regType(); }
  ASMJIT_INLINE_NODEBUG RegGroup group() const noexcept { return _signature.regGroup(); }

  ASMJIT_INLINE_NODEBUG VirtReg* virtReg() const noexcept { return _virtReg; }

  ASMJIT_INLINE_NODEBUG bool hasTiedReg() const noexcept { return _tiedReg != nullptr; }
  ASMJIT_INLINE_NODEBUG RATiedReg* tiedReg() const noexcept { return _tiedReg; }
  ASMJIT_INLINE_NODEBUG void setTiedReg(RATiedReg* tiedReg) noexcept { _tiedReg = tiedReg; }
  ASMJIT_INLINE_NODEBUG void resetTiedReg() noexcept { _tiedReg = nullptr; }

  ASMJIT_INLINE_NODEBUG bool hasStackSlot() const noexcept { return _stackSlot != nullptr; }
  ASMJIT_INLINE_NODEBUG RAStackSlot* stackSlot() const noexcept { return _stackSlot; }

  ASMJIT_INLINE_NODEBUG LiveRegSpans& liveSpans() noexcept { return _liveSpans; }
  ASMJIT_INLINE_NODEBUG const LiveRegSpans& liveSpans() const noexcept { return _liveSpans; }

  ASMJIT_INLINE_NODEBUG RALiveStats& liveStats() noexcept { return _liveStats; }
  ASMJIT_INLINE_NODEBUG const RALiveStats& liveStats() const noexcept { return _liveStats; }

  ASMJIT_INLINE_NODEBUG bool hasArgIndex() const noexcept { return _argIndex != kNoArgIndex; }
  ASMJIT_INLINE_NODEBUG uint32_t argIndex() const noexcept { return _argIndex; }
  ASMJIT_INLINE_NODEBUG uint32_t argValueIndex() const noexcept { return _argValueIndex; }

  inline void setArgIndex(uint32_t argIndex, uint32_t valueIndex) noexcept {
    _argIndex = uint8_t(argIndex);
    _argValueIndex = uint8_t(valueIndex);
  }

  ASMJIT_INLINE_NODEBUG bool hasHomeRegId() const noexcept { return _homeRegId != BaseReg::kIdBad; }
  ASMJIT_INLINE_NODEBUG uint32_t homeRegId() const noexcept { return _homeRegId; }
  ASMJIT_INLINE_NODEBUG void setHomeRegId(uint32_t physId) noexcept { _homeRegId = uint8_t(physId); }

  ASMJIT_INLINE_NODEBUG bool hasHintRegId() const noexcept { return _hintRegId != BaseReg::kIdBad; }
  ASMJIT_INLINE_NODEBUG uint32_t hintRegId() const noexcept { return _hintRegId; }
  ASMJIT_INLINE_NODEBUG void setHintRegId(uint32_t physId) noexcept { _hintRegId = uint8_t(physId); }

  ASMJIT_INLINE_NODEBUG RegMask useIdMask() const noexcept { return _useIdMask; }
  ASMJIT_INLINE_NODEBUG bool hasUseIdMask() const noexcept { return _useIdMask != 0u; }
  ASMJIT_INLINE_NODEBUG bool hasMultipleUseIds() const noexcept { return _useIdMask != 0u && !Support::isPowerOf2(_useIdMask); }
  ASMJIT_INLINE_NODEBUG void addUseIdMask(RegMask mask) noexcept { _useIdMask |= mask; }

  ASMJIT_INLINE_NODEBUG RegMask preferredMask() const noexcept { return _preferredMask; }
  ASMJIT_INLINE_NODEBUG bool hasPreferredMask() const noexcept { return _preferredMask != 0xFFFFFFFFu; }
  ASMJIT_INLINE_NODEBUG void restrictPreferredMask(RegMask mask) noexcept { _preferredMask &= mask; }

  ASMJIT_INLINE_NODEBUG RegMask consecutiveMask() const noexcept { return _consecutiveMask; }
  ASMJIT_INLINE_NODEBUG bool hasConsecutiveMask() const noexcept { return _consecutiveMask != 0xFFFFFFFFu; }
  ASMJIT_INLINE_NODEBUG void restrictConsecutiveMask(RegMask mask) noexcept { _consecutiveMask &= mask; }

  ASMJIT_INLINE_NODEBUG RegMask clobberSurvivalMask() const noexcept { return _clobberSurvivalMask; }
  ASMJIT_INLINE_NODEBUG void addClobberSurvivalMask(RegMask mask) noexcept { _clobberSurvivalMask |= mask; }

  ASMJIT_INLINE_NODEBUG RegMask allocatedMask() const noexcept { return _allocatedMask; }
  ASMJIT_INLINE_NODEBUG void addAllocatedMask(RegMask mask) noexcept { _allocatedMask |= mask; }

  ASMJIT_INLINE_NODEBUG uint64_t regByteMask() const noexcept { return _regByteMask; }
  ASMJIT_INLINE_NODEBUG void setRegByteMask(uint64_t mask) noexcept { _regByteMask = mask; }

  ASMJIT_INLINE_NODEBUG bool hasImmediateConsecutives() const noexcept { return !_immediateConsecutives.empty(); }
  ASMJIT_INLINE_NODEBUG const ZoneBitVector& immediateConsecutives() const noexcept { return _immediateConsecutives; }

  inline Error addImmediateConsecutive(ZoneAllocator* allocator, uint32_t workId) noexcept {
    if (_immediateConsecutives.size() <= workId)
      ASMJIT_PROPAGATE(_immediateConsecutives.resize(allocator, workId + 1));

    _immediateConsecutives.setBit(workId, true);
    return kErrorOk;
  }

  //! \}
};

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_RADEFS_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/ralocal.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/ralocal_p.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// RALocalAllocator - Utilities
// ============================

static ASMJIT_FORCE_INLINE RATiedReg* RALocal_findTiedRegByWorkId(RATiedReg* tiedRegs, size_t count, uint32_t workId) noexcept {
  for (size_t i = 0; i < count; i++)
    if (tiedRegs[i].workId() == workId)
      return &tiedRegs[i];
  return nullptr;
}

// RALocalAllocator - Initialization & Reset
// =========================================

Error RALocalAllocator::init() noexcept {
  PhysToWorkMap* physToWorkMap;
  WorkToPhysMap* workToPhysMap;

  physToWorkMap = _pass->newPhysToWorkMap();
  workToPhysMap = _pass->newWorkToPhysMap();
  if (!physToWorkMap || !workToPhysMap)
    return DebugUtils::errored(kErrorOutOfMemory);

  _curAssignment.initLayout(_pass->_physRegCount, _pass->workRegs());
  _curAssignment.initMaps(physToWorkMap, workToPhysMap);

  physToWorkMap = _pass->newPhysToWorkMap();
  workToPhysMap = _pass->newWorkToPhysMap();
  _tmpWorkToPhysMap = _pass->newWorkToPhysMap();

  if (!physToWorkMap || !workToPhysMap || !_tmpWorkToPhysMap)
    return DebugUtils::errored(kErrorOutOfMemory);

  _tmpAssignment.initLayout(_pass->_physRegCount, _pass->workRegs());
  _tmpAssignment.initMaps(physToWorkMap, workToPhysMap);

  return kErrorOk;
}

// RALocalAllocator - Assignment
// =============================

Error RALocalAllocator::makeInitialAssignment() noexcept {
  FuncNode* func = _pass->func();
  RABlock* entry = _pass->entryBlock();

  ZoneBitVector& liveIn = entry->liveIn();
  uint32_t argCount = func->argCount();
  uint32_t numIter = 1;

  for (uint32_t iter = 0; iter < numIter; iter++) {
    for (uint32_t argIndex = 0; argIndex < argCount; argIndex++) {
      for (uint32_t valueIndex = 0; valueIndex < Globals::kMaxValuePack; valueIndex++) {
        // Unassigned argument.
        const RegOnly& regArg = func->argPack(argIndex)[valueIndex];
        if (!regArg.isReg() || !_cc->isVirtIdValid(regArg.id()))
          continue;

        VirtReg* virtReg = _cc->virtRegById(regArg.id());

        // Unreferenced argument.
        RAWorkReg* workReg = virtReg->workReg();
        if (!workReg)
          continue;

        // Overwritten argument.
        uint32_t workId = workReg->workId();
        if (!liveIn.bitAt(workId))
          continue;

        RegGroup group = workReg->group();
        if (_curAssignment.workToPhysId(group, workId) != RAAssignment::kPhysNone)
          continue;

        RegMask allocableRegs = _availableRegs[group] & ~_curAssignment.assigned(group);
        if (iter == 0) {
          // First iteration: Try to allocate to home RegId.
          if (workReg->hasHomeRegId()) {
            uint32_t physId = workReg->homeRegId();
            if (Support::bitTest(allocableRegs, physId)) {
              _curAssignment.assign(group, workId, physId, true);
              _pass->_argsAssignment.assignRegInPack(argIndex, valueIndex, workReg->type(), physId, workReg->typeId());
              continue;
            }
          }

          numIter = 2;
        }
        else {
          // Second iteration: Pick any other register if the is an unassigned one or assign to stack.
          if (allocableRegs) {
            uint32_t physId = Support::ctz(allocableRegs);
            _curAssignment.assign(group, workId, physId, true);
            _pass->_argsAssignment.assignRegInPack(argIndex, valueIndex, workReg->type(), physId, workReg->typeId());
          }
          else {
            // This register will definitely need stack, create the slot now and assign also `argIndex`
            // to it. We will patch `_argsAssignment` later after RAStackAllocator finishes.
            RAStackSlot* slot = _pass->getOrCreateStackSlot(workReg);
            if (ASMJIT_UNLIKELY(!slot))
              return DebugUtils::errored(kErrorOutOfMemory);

            // This means STACK_ARG may be moved to STACK.
            workReg->addFlags(RAWorkRegFlags::kStackArgToStack);
            _pass->_numStackArgsToStackSlots++;
          }
        }
      }
    }
  }

  return kErrorOk;
}

Error RALocalAllocator::replaceAssignment(const PhysToWorkMap* physToWorkMap) noexcept {
  _curAssignment.copyFrom(physToWorkMap);
  return kErrorOk;
}

Error RALocalAllocator::switchToAssignment(PhysToWorkMap* dstPhysToWorkMap, const ZoneBitVector& liveIn, bool dstReadOnly, bool tryMode) noexcept {
  RAAssignment dst;
  RAAssignment& cur = _curAssignment;

  dst.initLayout(_pass->_physRegCount, _pass->workRegs());
  dst.initMaps(dstPhysToWorkMap, _tmpWorkToPhysMap);
  dst.assignWorkIdsFromPhysIds();

  for (RegGroup group : RegGroupVirtValues{}) {
    // STEP 1
    // ------
    //
    //   - KILL all registers that are not live at `dst`,
    //   - SPILL all registers that are not assigned at `dst`.

    if (!tryMode) {
      Support::BitWordIterator<RegMask> it(cur.assigned(group));
      while (it.hasNext()) {
        uint32_t physId = it.next();
        uint32_t workId = cur.physToWorkId(group, physId);

        // Must be true as we iterate over assigned registers.
        ASMJIT_ASSERT(workId != RAAssignment::kWorkNone);

        // KILL if it's not live on entry.
        if (!liveIn.bitAt(workId)) {
          onKillReg(group, workId, physId);
          continue;
        }

        // SPILL if it's not assigned on entry.
        uint32_t altId = dst.workToPhysId(group, workId);
        if (altId == RAAssignment::kPhysNone) {
          ASMJIT_PROPAGATE(onSpillReg(group, workId, physId));
        }
      }
    }

    // STEP 2
    // ------
    //
    //   - MOVE and SWAP registers from their current assignments into their DST assignments.
    //   - Build `willLoadRegs` mask of registers scheduled for `onLoadReg()`.

    // Current run-id (1 means more aggressive decisions).
    int32_t runId = -1;
    // Remaining registers scheduled for `onLoadReg()`.
    RegMask willLoadRegs = 0;
    // Remaining registers to be allocated in this loop.
    RegMask affectedRegs = dst.assigned(group);

    while (affectedRegs) {
      if (++runId == 2) {
        if (!tryMode)
          return DebugUtils::errored(kErrorInvalidState);

        // Stop in `tryMode` if we haven't done anything in past two rounds.
        break;
      }

      Support::BitWordIterator<RegMask> it(affectedRegs);
      while (it.hasNext()) {
        uint32_t physId = it.next();
        RegMask physMask = Support::bitMask<RegMask>(physId);

        uint32_t curWorkId = cur.physToWorkId(group, physId);
        uint32_t dstWorkId = dst.physToWorkId(group, physId);

        // The register must have assigned `dstWorkId` as we only iterate over assigned regs.
        ASMJIT_ASSERT(dstWorkId != RAAssignment::kWorkNone);

        if (curWorkId != RAAssignment::kWorkNone) {
          // Both assigned.
          if (curWorkId != dstWorkId) {
            // Wait a bit if this is the first run, we may avoid this if `curWorkId` moves out.
            if (runId <= 0)
              continue;

            uint32_t altPhysId = cur.workToPhysId(group, dstWorkId);
            if (altPhysId == RAAssignment::kPhysNone)
              continue;

            // Reset as we will do some changes to the current assignment.
            runId = -1;

            if (_archTraits->hasInstRegSwap(group)) {
              ASMJIT_PROPAGATE(onSwapReg(group, curWorkId, physId, dstWorkId, altPhysId));
            }
            else {
              // SPILL the reg if it's not dirty in DST, otherwise try to MOVE.
              if (!cur.isPhysDirty(group, physId)) {
                ASMJIT_PROPAGATE(onKillReg(group, curWorkId, physId));
              }
              else {
                RegMask allocableRegs = _pass->_availableRegs[group] & ~cur.assigned(group);

                // If possible don't conflict with assigned regs at DST.
                if (allocableRegs & ~dst.assigned(group))
                  allocableRegs &= ~dst.assigned(group);

                if (allocableRegs) {
                  // MOVE is possible, thus preferred.
                  uint32_t tmpPhysId = Support::ctz(allocableRegs);

                  ASMJIT_PROPAGATE(onMoveReg(group, curWorkId, tmpPhysId, physId));
                  _clobberedRegs[group] |= Support::bitMask(tmpPhysId);
                }
                else {
                  // MOVE is impossible, must SPILL.
                  ASMJIT_PROPAGATE(onSpillReg(group, curWorkId, physId));
                }
              }

              goto Cleared;
            }
          }
        }
        else {
Cleared:
          // DST assigned, CUR unassigned.
          uint32_t altPhysId = cur.workToPhysId(group, dstWorkId);
          if (altPhysId == RAAssignment::kPhysNone) {
            if (liveIn.bitAt(dstWorkId))
              willLoadRegs |= physMask; // Scheduled for `onLoadReg()`.
            affectedRegs &= ~physMask;  // Unaffected from now.
            continue;
          }
          ASMJIT_PROPAGATE(onMoveReg(group, dstWorkId, physId, altPhysId));
        }

        // Both DST and CUR assigned to the same reg or CUR just moved to DST.
        if ((dst.dirty(group) & physMask) != (cur.dirty(group) & physMask)) {
          if ((dst.dirty(group) & physMask) == 0) {
            // CUR dirty, DST not dirty (the assert is just to visualize the condition).
            ASMJIT_ASSERT(!dst.isPhysDirty(group, physId) && cur.isPhysDirty(group, physId));

            // If `dstReadOnly` is true it means that that block was already processed and we cannot change from
            // CLEAN to DIRTY. In that case the register has to be saved as it cannot enter the block DIRTY.
            if (dstReadOnly)
              ASMJIT_PROPAGATE(onSaveReg(group, dstWorkId, physId));
            else
              dst.makeDirty(group, dstWorkId, physId);
          }
          else {
            // DST dirty, CUR not dirty (the assert is just to visualize the condition).
            ASMJIT_ASSERT(dst.isPhysDirty(group, physId) && !cur.isPhysDirty(group, physId));

            cur.makeDirty(group, dstWorkId, physId);
          }
        }

        // Must match now...
        ASMJIT_ASSERT(dst.physToWorkId(group, physId) == cur.physToWorkId(group, physId));
        ASMJIT_ASSERT(dst.isPhysDirty(group, physId) == cur.isPhysDirty(group, physId));

        runId = -1;
        affectedRegs &= ~physMask;
      }
    }

    // STEP 3
    // ------
    //
    //   - Load registers specified by `willLoadRegs`.

    {
      Support::BitWordIterator<RegMask> it(willLoadRegs);
      while (it.hasNext()) {
        uint32_t physId = it.next();

        if (!cur.isPhysAssigned(group, physId)) {
          uint32_t workId = dst.physToWorkId(group, physId);

          // The algorithm is broken if it tries to load a register that is not in LIVE-IN.
          ASMJIT_ASSERT(liveIn.bitAt(workId) == true);

          ASMJIT_PROPAGATE(onLoadReg(group, workId, physId));
          if (dst.isPhysDirty(group, physId))
            cur.makeDirty(group, workId, physId);
          ASMJIT_ASSERT(dst.isPhysDirty(group, physId) == cur.isPhysDirty(group, physId));
        }
        else {
          // Not possible otherwise.
          ASMJIT_ASSERT(tryMode == true);
        }
      }
    }
  }

  if (!tryMode) {
    // Here is a code that dumps the conflicting part if something fails here:
    // if (!dst.equals(cur)) {
    //   uint32_t physTotal = dst._layout.physTotal;
    //   uint32_t workCount = dst._layout.workCount;
    //
    //   fprintf(stderr, "Dirty    DST=0x%08X CUR=0x%08X\n", dst.dirty(RegGroup::kGp), cur.dirty(RegGroup::kGp));
    //   fprintf(stderr, "Assigned DST=0x%08X CUR=0x%08X\n", dst.assigned(RegGroup::kGp), cur.assigned(RegGroup::kGp));
    //
    //   for (uint32_t physId = 0; physId < physTotal; physId++) {
    //     uint32_t dstWorkId = dst._physToWorkMap->workIds[physId];
    //     uint32_t curWorkId = cur._physToWorkMap->workIds[physId];
    //     if (dstWorkId != curWorkId)
    //       fprintf(stderr, "[PhysIdWork] PhysId=%u WorkId[DST(%u) != CUR(%u)]\n", physId, dstWorkId, curWorkId);
    //   }
    //
    //   for (uint32_t workId = 0; workId < workCount; workId++) {
    //     uint32_t dstPhysId = dst._workToPhysMap->physIds[workId];
    //     uint32_t curPhysId = cur._workToPhysMap->physIds[workId];
    //     if (dstPhysId != curPhysId)
    //       fprintf(stderr, "[WorkToPhys] WorkId=%u PhysId[DST(%u) != CUR(%u)]\n", workId, dstPhysId, curPhysId);
    //   }
    // }
    ASMJIT_ASSERT(dst.equals(cur));
  }

  return kErrorOk;
}

Error RALocalAllocator::spillScratchGpRegsBeforeEntry(RegMask scratchRegs) noexcept {
  RegGroup group = RegGroup::kGp;
  Support::BitWordIterator<RegMask> it(scratchRegs);

  while (it.hasNext()) {
    uint32_t physId = it.next();
    if (_curAssignment.isPhysAssigned(group, physId)) {
      uint32_t workId = _curAssignment.physToWorkId(group, physId);
      ASMJIT_PROPAGATE(onSpillReg(group, workId, physId));
    }
  }

  return kErrorOk;
}

// RALocalAllocator - Allocation
// =============================

Error RALocalAllocator::allocInst(InstNode* node) noexcept {
  RAInst* raInst = node->passData<RAInst>();

  RATiedReg* outTiedRegs[Globals::kMaxPhysRegs];
  RATiedReg* dupTiedRegs[Globals::kMaxPhysRegs];
  RATiedReg* consecutiveRegs[kMaxConsecutiveRegs];

  // The cursor must point to the previous instruction for a possible instruction insertion.
  _cc->_setCursor(node->prev());

  _node = node;
  _raInst = raInst;
  _tiedTotal = raInst->_tiedTotal;
  _tiedCount = raInst->_tiedCount;

  // Whether we already replaced register operand with memory operand.
  bool rmAllocated = false;

  for (RegGroup group : RegGroupVirtValues{}) {
    uint32_t i, count = this->tiedCount(group);
    RATiedReg* tiedRegs = this->tiedRegs(group);

    RegMask willUse = _raInst->_usedRegs[group];
    RegMask willOut = _raInst->_clobberedRegs[group];
    RegMask willFree = 0;

    uint32_t usePending = count;
    uint32_t outTiedCount = 0;
    uint32_t dupTiedCount = 0;
    uint32_t consecutiveMask = 0;

    // STEP 1
    // ------
    //
    // Calculate `willUse` and `willFree` masks based on tied registers we have. In addition, aggregate information
    // regarding consecutive registers used by this instruction. We need that to make USE/OUT assignments.
    //
    // We don't do any assignment decisions at this stage as we just need to collect some information first. Then,
    // after we populate all masks needed we can finally make some decisions in the second loop. The main reason
    // for this is that we really need `willFree` to make assignment decisions for `willUse`, because if we mark
    // some registers that will be freed, we can consider them in decision making afterwards.

    for (i = 0; i < count; i++) {
      RATiedReg* tiedReg = &tiedRegs[i];

      if (tiedReg->hasAnyConsecutiveFlag()) {
        uint32_t consecutiveOffset = tiedReg->isLeadConsecutive() ? uint32_t(0) : tiedReg->consecutiveData();

        if (ASMJIT_UNLIKELY(Support::bitTest(consecutiveMask, consecutiveOffset)))
          return DebugUtils::errored(kErrorInvalidState);

        consecutiveMask |= Support::bitMask(consecutiveOffset);
        consecutiveRegs[consecutiveOffset] = tiedReg;
      }

      // Add OUT and KILL to `outPending` for CLOBBERing and/or OUT assignment.
      if (tiedReg->isOutOrKill())
        outTiedRegs[outTiedCount++] = tiedReg;

      if (tiedReg->isDuplicate())
        dupTiedRegs[dupTiedCount++] = tiedReg;

      if (!tiedReg->isUse()) {
        tiedReg->markUseDone();
        usePending--;
        continue;
      }

      // Don't assign anything here if this is a consecutive USE - we will handle this in STEP 2 instead.
      if (tiedReg->isUseConsecutive())
        continue;

      uint32_t workId = tiedReg->workId();
      uint32_t assignedId = _curAssignment.workToPhysId(group, workId);

      if (tiedReg->hasUseId()) {
        // If the register has `useId` it means it can only be allocated in that register.
        RegMask useMask = Support::bitMask(tiedReg->useId());

        // RAInstBuilder must have collected `usedRegs` on-the-fly.
        ASMJIT_ASSERT((willUse & useMask) != 0);

        if (assignedId == tiedReg->useId()) {
          // If the register is already allocated in this one, mark it done and continue.
          tiedReg->markUseDone();
          if (tiedReg->isWrite())
            _curAssignment.makeDirty(group, workId, assignedId);
          usePending--;
          willUse |= useMask;
        }
        else {
          willFree |= useMask & _curAssignment.assigned(group);
        }
      }
      else {
        // Check if the register must be moved to `allocableRegs`.
        RegMask allocableRegs = tiedReg->useRegMask();
        if (assignedId != RAAssignment::kPhysNone) {
          RegMask assignedMask = Support::bitMask(assignedId);
          if ((allocableRegs & ~willUse) & assignedMask) {
            tiedReg->setUseId(assignedId);
            tiedReg->markUseDone();
            if (tiedReg->isWrite())
              _curAssignment.makeDirty(group, workId, assignedId);
            usePending--;
            willUse |= assignedMask;
          }
          else {
            willFree |= assignedMask;
          }
        }
      }
    }

    // STEP 2
    // ------
    //
    // Verify that all the consecutive registers are really consecutive. Terminate if there is a gap. In addition,
    // decide which USE ids will be used in case that this consecutive sequence is USE (OUT registers are allocated
    // in a different step).
    uint32_t consecutiveCount = 0;

    if (consecutiveMask) {
      if ((consecutiveMask & (consecutiveMask + 1u)) != 0)
        return DebugUtils::errored(kErrorInvalidState);

      // Count of trailing ones is the count of consecutive registers. There cannot be gap.
      consecutiveCount = Support::ctz(~consecutiveMask);

      // Prioritize allocation that would result in least moves even when moving registers away from their homes.
      RATiedReg* lead = consecutiveRegs[0];

      // Assign the best possible USE Ids to all consecutives.
      if (lead->isUseConsecutive()) {
        uint32_t bestScore = 0;
        uint32_t bestLeadReg = 0xFFFFFFFF;
        RegMask allocableRegs = (_availableRegs[group] | willFree) & ~willUse;

        uint32_t assignments[kMaxConsecutiveRegs];

        for (i = 0; i < consecutiveCount; i++)
          assignments[i] = _curAssignment.workToPhysId(group, consecutiveRegs[i]->workId());

        Support::BitWordIterator<uint32_t> it(lead->useRegMask());
        while (it.hasNext()) {
          uint32_t regIndex = it.next();
          if (Support::bitTest(lead->useRegMask(), regIndex)) {
            uint32_t score = 15;

            for (i = 0; i < consecutiveCount; i++) {
              uint32_t consecutiveIndex = regIndex + i;
              if (!Support::bitTest(allocableRegs, consecutiveIndex)) {
                score = 0;
                break;
              }

              RAWorkReg* workReg = workRegById(consecutiveRegs[i]->workId());
              score += uint32_t(workReg->homeRegId() == consecutiveIndex);
              score += uint32_t(assignments[i] == consecutiveIndex) * 2;
            }

            if (score > bestScore) {
              bestScore = score;
              bestLeadReg = regIndex;
            }
          }
        }

        if (bestLeadReg == 0xFFFFFFFF)
          return DebugUtils::errored(kErrorConsecutiveRegsAllocation);

        for (i = 0; i < consecutiveCount; i++) {
          uint32_t consecutiveIndex = bestLeadReg + i;

          RATiedReg* tiedReg = consecutiveRegs[i];
          RegMask useMask = Support::bitMask(consecutiveIndex);

          uint32_t workId = tiedReg->workId();
          uint32_t assignedId = _curAssignment.workToPhysId(group, workId);

          tiedReg->setUseId(consecutiveIndex);

          if (assignedId == consecutiveIndex) {
            // If the register is already allocated in this one, mark it done and continue.
            tiedReg->markUseDone();
            if (tiedReg->isWrite())
              _curAssignment.makeDirty(group, workId, assignedId);
            usePending--;
            willUse |= useMask;
          }
          else {
            willUse |= useMask;
            willFree |= useMask & _curAssignment.assigned(group);
          }
        }
      }
    }

    // STEP 3
    // ------
    //
    // Do some decision making to find the best candidates of registers that need to be assigned, moved, and/or
    // spilled. Only USE registers are considered here, OUT will be decided later after all CLOBBERed and OUT
    // registers are unassigned.

    if (usePending) {
      // TODO: Not sure `liveRegs` should be used, maybe willUse and willFree would be enough and much more clear.

      // All registers that are currently alive without registers that will be freed.
      RegMask liveRegs = _curAssignment.assigned(group) & ~willFree;

      for (i = 0; i < count; i++) {
        RATiedReg* tiedReg = &tiedRegs[i];
        if (tiedReg->isUseDone())
          continue;

        uint32_t workId = tiedReg->workId();
        uint32_t assignedId = _curAssignment.workToPhysId(group, workId);

        // REG/MEM: Patch register operand to memory operand if not allocated.
        if (!rmAllocated && tiedReg->hasUseRM()) {
          if (assignedId == RAAssignment::kPhysNone && Support::isPowerOf2(tiedReg->useRewriteMask())) {
            RAWorkReg* workReg = workRegById(tiedReg->workId());
            uint32_t opIndex = Support::ctz(tiedReg->useRewriteMask()) / uint32_t(sizeof(Operand) / sizeof(uint32_t));
            uint32_t rmSize = tiedReg->rmSize();

            if (rmSize <= workReg->virtReg()->virtSize()) {
              Operand& op = node->operands()[opIndex];
              op = _pass->workRegAsMem(workReg);

              // NOTE: We cannot use `x86::Mem::setSize()` from here, so let's manipulate the signature directly.
              op._signature.setSize(rmSize);

              tiedReg->_useRewriteMask = 0;

              tiedReg->markUseDone();
              raInst->addFlags(RATiedFlags::kInst_RegToMemPatched);
              usePending--;

              rmAllocated = true;
              continue;
            }
          }
        }

        if (!tiedReg->hasUseId()) {
          // DECIDE where to assign the USE register.
          RegMask allocableRegs = tiedReg->useRegMask() & ~(willFree | willUse);
          uint32_t useId = decideOnAssignment(group, workId, assignedId, allocableRegs);

          RegMask useMask = Support::bitMask(useId);
          willUse |= useMask;
          willFree |= useMask & liveRegs;
          tiedReg->setUseId(useId);

          if (assignedId != RAAssignment::kPhysNone) {
            RegMask assignedMask = Support::bitMask(assignedId);

            willFree |= assignedMask;
            liveRegs &= ~assignedMask;

            // OPTIMIZATION: Assign the USE register here if it's possible.
            if (!(liveRegs & useMask)) {
              ASMJIT_PROPAGATE(onMoveReg(group, workId, useId, assignedId));
              tiedReg->markUseDone();
              if (tiedReg->isWrite())
                _curAssignment.makeDirty(group, workId, useId);
              usePending--;
            }
          }
          else {
            // OPTIMIZATION: Assign the USE register here if it's possible.
            if (!(liveRegs & useMask)) {
              ASMJIT_PROPAGATE(onLoadReg(group, workId, useId));
              tiedReg->markUseDone();
              if (tiedReg->isWrite())
                _curAssignment.makeDirty(group, workId, useId);
              usePending--;
            }
          }

          liveRegs |= useMask;
        }
      }
    }

    // Initially all used regs will be marked as clobbered.
    RegMask clobberedByInst = willUse | willOut;

    // STEP 4
    // ------
    //
    // Free all registers that we marked as `willFree`. Only registers that are not USEd by the instruction are
    // considered as we don't want to free regs we need.

    if (willFree) {
      RegMask allocableRegs = _availableRegs[group] & ~(_curAssignment.assigned(group) | willFree | willUse | willOut);
      Support::BitWordIterator<RegMask> it(willFree);

      do {
        uint32_t assignedId = it.next();
        if (_curAssignment.isPhysAssigned(group, assignedId)) {
          uint32_t workId = _curAssignment.physToWorkId(group, assignedId);

          // DECIDE whether to MOVE or SPILL.
          if (allocableRegs) {
            uint32_t reassignedId = decideOnReassignment(group, workId, assignedId, allocableRegs, raInst);
            if (reassignedId != RAAssignment::kPhysNone) {
              ASMJIT_PROPAGATE(onMoveReg(group, workId, reassignedId, assignedId));
              allocableRegs ^= Support::bitMask(reassignedId);
              _clobberedRegs[group] |= Support::bitMask(reassignedId);
              continue;
            }
          }

          ASMJIT_PROPAGATE(onSpillReg(group, workId, assignedId));
        }
      } while (it.hasNext());
    }

    // STEP 5
    // ------
    //
    // ALLOCATE / SHUFFLE all registers that we marked as `willUse` and weren't allocated yet. This is a bit
    // complicated as the allocation is iterative. In some cases we have to wait before allocating a particular
    // physical register as it's still occupied by some other one, which we need to move before we can use it.
    // In this case we skip it and allocate another one instead (making it free for the next iteration).
    //
    // NOTE: Iterations are mostly important for complicated allocations like function calls, where there can
    // be up to N registers used at once. Asm instructions won't run the loop more than once in 99.9% of cases
    // as they use 2 to 3 registers in average.

    if (usePending) {
      bool mustSwap = false;
      do {
        uint32_t oldPending = usePending;

        for (i = 0; i < count; i++) {
          RATiedReg* thisTiedReg = &tiedRegs[i];
          if (thisTiedReg->isUseDone()) {
            continue;
          }

          uint32_t thisWorkId = thisTiedReg->workId();
          uint32_t thisPhysId = _curAssignment.workToPhysId(group, thisWorkId);

          // This would be a bug, fatal one!
          uint32_t targetPhysId = thisTiedReg->useId();
          ASMJIT_ASSERT(targetPhysId != thisPhysId);

          uint32_t targetWorkId = _curAssignment.physToWorkId(group, targetPhysId);
          if (targetWorkId != RAAssignment::kWorkNone) {
            RAWorkReg* targetWorkReg = workRegById(targetWorkId);

            // Swapping two registers can solve two allocation tasks by emitting just a single instruction. However,
            // swap is only available on few architectures and it's definitely not available for each register group.
            // Calling `onSwapReg()` before checking these would be fatal.
            if (_archTraits->hasInstRegSwap(group) && thisPhysId != RAAssignment::kPhysNone) {
              ASMJIT_PROPAGATE(onSwapReg(group, thisWorkId, thisPhysId, targetWorkId, targetPhysId));

              thisTiedReg->markUseDone();
              if (thisTiedReg->isWrite()) {
                _curAssignment.makeDirty(group, thisWorkId, targetPhysId);
              }
              usePending--;

              // Double-hit.
              RATiedReg* targetTiedReg = RALocal_findTiedRegByWorkId(tiedRegs, count, targetWorkReg->workId());
              if (targetTiedReg && targetTiedReg->useId() == thisPhysId) {
                targetTiedReg->markUseDone();
                if (targetTiedReg->isWrite()) {
                  _curAssignment.makeDirty(group, targetWorkId, thisPhysId);
                }
                usePending--;
              }
              continue;
            }

            if (!mustSwap)
              continue;

            // Only branched here if the previous iteration did nothing. This is essentially a SWAP operation without
            // having a dedicated instruction for that purpose (vector registers, etc...). The simplest way to handle
            // such case is to SPILL the target register or MOVE it to another register so the loop can continue.
            RegMask availableRegs = _availableRegs[group] & ~_curAssignment.assigned(group);
            if (availableRegs) {
              uint32_t tmpRegId = pickBestSuitableRegister(group, availableRegs);

              ASMJIT_PROPAGATE(onMoveReg(group, thisWorkId, tmpRegId, thisPhysId));
              _clobberedRegs[group] |= Support::bitMask(tmpRegId);

              // NOTE: This register is not done, we have just moved it to another physical spot, and we will have to
              // move it again into the correct spot once it's free (since this is essentially doing a swap operation
              // via moves).
              break;
            }

            ASMJIT_PROPAGATE(onSpillReg(group, targetWorkId, targetPhysId));
          }

          if (thisPhysId != RAAssignment::kPhysNone) {
            ASMJIT_PROPAGATE(onMoveReg(group, thisWorkId, targetPhysId, thisPhysId));

            thisTiedReg->markUseDone();
            if (thisTiedReg->isWrite()) {
              _curAssignment.makeDirty(group, thisWorkId, targetPhysId);
            }
            usePending--;
          }
          else {
            ASMJIT_PROPAGATE(onLoadReg(group, thisWorkId, targetPhysId));

            thisTiedReg->markUseDone();
            if (thisTiedReg->isWrite()) {
              _curAssignment.makeDirty(group, thisWorkId, targetPhysId);
            }
            usePending--;
          }
        }

        mustSwap = (oldPending == usePending);
      } while (usePending);
    }

    // STEP 6
    // ------
    //
    // KILL registers marked as KILL/OUT.

    uint32_t outPending = outTiedCount;
    if (outTiedCount) {
      for (i = 0; i < outTiedCount; i++) {
        RATiedReg* tiedReg = outTiedRegs[i];

        uint32_t workId = tiedReg->workId();
        uint32_t physId = _curAssignment.workToPhysId(group, workId);

        // Must check if it's allocated as KILL can be related to OUT (like KILL immediately after OUT, which could
        // mean the register is not assigned).
        if (physId != RAAssignment::kPhysNone) {
          ASMJIT_PROPAGATE(onKillReg(group, workId, physId));
          willOut &= ~Support::bitMask(physId);
        }

        // We still maintain number of pending registers for OUT assignment. So, if this is only KILL, not OUT, we
        // can safely decrement it.
        outPending -= !tiedReg->isOut();
      }
    }

    // STEP 7
    // ------
    //
    // SPILL registers that will be CLOBBERed. Since OUT and KILL were already processed this is used mostly to
    // handle function CALLs.

    if (willOut) {
      Support::BitWordIterator<RegMask> it(willOut);
      do {
        uint32_t physId = it.next();
        uint32_t workId = _curAssignment.physToWorkId(group, physId);

        if (workId == RAAssignment::kWorkNone) {
          continue;
        }

        ASMJIT_PROPAGATE(onSpillReg(group, workId, physId));
      } while (it.hasNext());
    }

    // STEP 8
    // ------
    //
    // Duplication.

    for (i = 0; i < dupTiedCount; i++) {
      RATiedReg* tiedReg = dupTiedRegs[i];
      uint32_t workId = tiedReg->workId();
      uint32_t srcId = tiedReg->useId();

      Support::BitWordIterator<RegMask> it(tiedReg->useRegMask());
      while (it.hasNext()) {
        uint32_t dstId = it.next();
        if (dstId == srcId) {
          continue;
        }
        _pass->emitMove(workId, dstId, srcId);
      }
    }

    // STEP 9
    // ------
    //
    // Vector registers can be clobbered partially by invoke - find if that's the case and clobber when necessary.

    if (node->isInvoke() && group == RegGroup::kVec) {
      const InvokeNode* invokeNode = node->as<InvokeNode>();

      RegMask maybeClobberedRegs = invokeNode->detail().callConv().preservedRegs(group) & _curAssignment.assigned(group);
      if (maybeClobberedRegs) {
        uint32_t saveRestoreVecSize = invokeNode->detail().callConv().saveRestoreRegSize(group);
        Support::BitWordIterator<RegMask> it(maybeClobberedRegs);

        do {
          uint32_t physId = it.next();
          uint32_t workId = _curAssignment.physToWorkId(group, physId);

          RAWorkReg* workReg = workRegById(workId);
          uint32_t virtSize = workReg->virtReg()->virtSize();

          if (virtSize > saveRestoreVecSize) {
            ASMJIT_PROPAGATE(onSpillReg(group, workId, physId));
          }

        } while (it.hasNext());
      }
    }

    // STEP 10
    // -------
    //
    // Assign OUT registers.

    if (outPending) {
      // Live registers, we need a separate register (outside of `_curAssignment) to hold these because of KILLed
      // registers. If we KILL a register here it will go out from `_curAssignment`, but we cannot assign to it in
      // here.
      RegMask liveRegs = _curAssignment.assigned(group);

      // Must avoid as they have been already OUTed (added during the loop).
      RegMask outRegs = 0;

      // Must avoid as they collide with already allocated ones.
      RegMask avoidRegs = willUse & ~clobberedByInst;

      // Assign the best possible OUT ids of all consecutives.
      if (consecutiveCount) {
        RATiedReg* lead = consecutiveRegs[0];
        if (lead->isOutConsecutive()) {
          uint32_t bestScore = 0;
          uint32_t bestLeadReg = 0xFFFFFFFF;
          RegMask allocableRegs = _availableRegs[group] & ~(outRegs | avoidRegs);

          Support::BitWordIterator<uint32_t> it(lead->outRegMask());
          while (it.hasNext()) {
            uint32_t regIndex = it.next();
            if (Support::bitTest(lead->outRegMask(), regIndex)) {
              uint32_t score = 15;

              for (i = 0; i < consecutiveCount; i++) {
                uint32_t consecutiveIndex = regIndex + i;
                if (!Support::bitTest(allocableRegs, consecutiveIndex)) {
                  score = 0;
                  break;
                }

                RAWorkReg* workReg = workRegById(consecutiveRegs[i]->workId());
                score += uint32_t(workReg->homeRegId() == consecutiveIndex);
              }

              if (score > bestScore) {
                bestScore = score;
                bestLeadReg = regIndex;
              }
            }
          }

          if (bestLeadReg == 0xFFFFFFFF) {
            return DebugUtils::errored(kErrorConsecutiveRegsAllocation);
          }

          for (i = 0; i < consecutiveCount; i++) {
            uint32_t consecutiveIndex = bestLeadReg + i;
            RATiedReg* tiedReg = consecutiveRegs[i];
            tiedReg->setOutId(consecutiveIndex);
          }
        }
      }

      // Allocate OUT registers.
      for (i = 0; i < outTiedCount; i++) {
        RATiedReg* tiedReg = outTiedRegs[i];
        if (!tiedReg->isOut())
          continue;

        RegMask avoidOut = avoidRegs;
        if (tiedReg->isUnique())
          avoidOut |= willUse;

        uint32_t workId = tiedReg->workId();
        uint32_t assignedId = _curAssignment.workToPhysId(group, workId);

        if (assignedId != RAAssignment::kPhysNone) {
          ASMJIT_PROPAGATE(onKillReg(group, workId, assignedId));
        }

        uint32_t physId = tiedReg->outId();
        if (physId == RAAssignment::kPhysNone) {
          RegMask allocableRegs = tiedReg->outRegMask() & ~(outRegs | avoidOut);

          if (!(allocableRegs & ~liveRegs)) {
            // There are no more registers, decide which one to spill.
            uint32_t spillWorkId;
            physId = decideOnSpillFor(group, workId, allocableRegs & liveRegs, &spillWorkId);
            ASMJIT_PROPAGATE(onSpillReg(group, spillWorkId, physId));
          }
          else {
            physId = decideOnAssignment(group, workId, RAAssignment::kPhysNone, allocableRegs & ~liveRegs);
          }
        }

        // OUTs are CLOBBERed thus cannot be ASSIGNed right now.
        ASMJIT_ASSERT(!_curAssignment.isPhysAssigned(group, physId));

        if (!tiedReg->isKill()) {
          ASMJIT_PROPAGATE(onAssignReg(group, workId, physId, true));
        }

        tiedReg->setOutId(physId);
        tiedReg->markOutDone();

        outRegs |= Support::bitMask(physId);
        liveRegs &= ~Support::bitMask(physId);
        outPending--;
      }

      clobberedByInst |= outRegs;
      ASMJIT_ASSERT(outPending == 0);
    }

    _clobberedRegs[group] |= clobberedByInst;
  }

  return kErrorOk;
}

Error RALocalAllocator::spillAfterAllocation(InstNode* node) noexcept {
  // This is experimental feature that would spill registers that don't have home-id and are last in this basic block.
  // This prevents saving these regs in other basic blocks and then restoring them (mostly relevant for loops).
  RAInst* raInst = node->passData<RAInst>();
  uint32_t count = raInst->tiedCount();

  for (uint32_t i = 0; i < count; i++) {
    RATiedReg* tiedReg = raInst->tiedAt(i);
    if (tiedReg->isLast()) {
      uint32_t workId = tiedReg->workId();
      RAWorkReg* workReg = workRegById(workId);
      if (!workReg->hasHomeRegId()) {
        RegGroup group = workReg->group();
        uint32_t assignedId = _curAssignment.workToPhysId(group, workId);
        if (assignedId != RAAssignment::kPhysNone) {
          _cc->_setCursor(node);
          ASMJIT_PROPAGATE(onSpillReg(group, workId, assignedId));
        }
      }
    }
  }

  return kErrorOk;
}

Error RALocalAllocator::allocBranch(InstNode* node, RABlock* target, RABlock* cont) noexcept {
  // TODO: This should be used to make the branch allocation better.
  DebugUtils::unused(cont);

  // The cursor must point to the previous instruction for a possible instruction insertion.
  _cc->_setCursor(node->prev());

  // Use TryMode of `switchToAssignment()` if possible.
  if (target->hasEntryAssignment()) {
    ASMJIT_PROPAGATE(switchToAssignment(target->entryPhysToWorkMap(), target->liveIn(), target->isAllocated(), true));
  }

  ASMJIT_PROPAGATE(allocInst(node));
  ASMJIT_PROPAGATE(spillRegsBeforeEntry(target));

  if (target->hasEntryAssignment()) {
    BaseNode* injectionPoint = _pass->extraBlock()->prev();
    BaseNode* prevCursor = _cc->setCursor(injectionPoint);

    _tmpAssignment.copyFrom(_curAssignment);
    ASMJIT_PROPAGATE(switchToAssignment(target->entryPhysToWorkMap(), target->liveIn(), target->isAllocated(), false));

    BaseNode* curCursor = _cc->cursor();
    if (curCursor != injectionPoint) {
      // Additional instructions emitted to switch from the current state to the `target` state. This means
      // that we have to move these instructions into an independent code block and patch the jump location.
      Operand& targetOp = node->op(node->opCount() - 1);
      if (ASMJIT_UNLIKELY(!targetOp.isLabel())) {
        return DebugUtils::errored(kErrorInvalidState);
      }

      Label trampoline = _cc->newLabel();
      Label savedTarget = targetOp.as<Label>();

      // Patch `target` to point to the `trampoline` we just created.
      targetOp = trampoline;

      // Clear a possible SHORT form as we have no clue now if the SHORT form would be encodable after patching
      // the target to `trampoline` (X86 specific).
      node->clearOptions(InstOptions::kShortForm);

      // Finalize the switch assignment sequence.
      ASMJIT_PROPAGATE(_pass->emitJump(savedTarget));
      _cc->_setCursor(injectionPoint);
      _cc->bind(trampoline);
    }

    _cc->_setCursor(prevCursor);
    _curAssignment.swap(_tmpAssignment);
  }
  else {
    ASMJIT_PROPAGATE(_pass->setBlockEntryAssignment(target, block(), _curAssignment));
  }

  return kErrorOk;
}

Error RALocalAllocator::allocJumpTable(InstNode* node, const RABlocks& targets, RABlock* cont) noexcept {
  // TODO: Do we really need to use `cont`?
  DebugUtils::unused(cont);

  if (targets.empty())
    return DebugUtils::errored(kErrorInvalidState);

  // The cursor must point to the previous instruction for a possible instruction insertion.
  _cc->_setCursor(node->prev());

  // All `targets` should have the same sharedAssignmentId, we just read the first.
  RABlock* anyTarget = targets[0];
  if (!anyTarget->hasSharedAssignmentId())
    return DebugUtils::errored(kErrorInvalidState);

  RASharedAssignment& sharedAssignment = _pass->_sharedAssignments[anyTarget->sharedAssignmentId()];

  ASMJIT_PROPAGATE(allocInst(node));

  if (!sharedAssignment.empty()) {
    ASMJIT_PROPAGATE(switchToAssignment(
      sharedAssignment.physToWorkMap(),
      sharedAssignment.liveIn(),
      true,  // Read-only.
      false  // Try-mode.
    ));
  }

  ASMJIT_PROPAGATE(spillRegsBeforeEntry(anyTarget));

  if (sharedAssignment.empty()) {
    ASMJIT_PROPAGATE(_pass->setBlockEntryAssignment(anyTarget, block(), _curAssignment));
  }

  return kErrorOk;
}

// RALocalAllocator - Decision Making
// ==================================

uint32_t RALocalAllocator::decideOnAssignment(RegGroup group, uint32_t workId, uint32_t physId, RegMask allocableRegs) const noexcept {
  ASMJIT_ASSERT(allocableRegs != 0);
  DebugUtils::unused(group, physId);

  RAWorkReg* workReg = workRegById(workId);

  // Prefer home register id, if possible.
  if (workReg->hasHomeRegId()) {
    uint32_t homeId = workReg->homeRegId();
    if (Support::bitTest(allocableRegs, homeId)) {
      return homeId;
    }
  }

  // Prefer registers used upon block entries.
  RegMask previouslyAssignedRegs = workReg->allocatedMask();
  if (allocableRegs & previouslyAssignedRegs) {
    allocableRegs &= previouslyAssignedRegs;
  }

  return pickBestSuitableRegister(group, allocableRegs);
}

uint32_t RALocalAllocator::decideOnReassignment(RegGroup group, uint32_t workId, uint32_t physId, RegMask allocableRegs, RAInst* raInst) const noexcept {
  ASMJIT_ASSERT(allocableRegs != 0);
  DebugUtils::unused(physId);

  RAWorkReg* workReg = workRegById(workId);

  // Prefer reassignment back to HomeId, if possible.
  if (workReg->hasHomeRegId()) {
    if (Support::bitTest(allocableRegs, workReg->homeRegId())) {
      return workReg->homeRegId();
    }
  }

  // Prefer assignment to a temporary register in case this register is killed by the instruction (or has an out slot).
  const RATiedReg* tiedReg = raInst->tiedRegForWorkReg(group, workId);
  if (tiedReg && tiedReg->isOutOrKill()) {
    return Support::ctz(allocableRegs);
  }

  // Prefer reassignment if this register is only used within a single basic block.
  if (workReg->isWithinSingleBasicBlock()) {
    RegMask filteredRegs = allocableRegs & ~workReg->clobberSurvivalMask();
    if (filteredRegs) {
      return pickBestSuitableRegister(group, filteredRegs);
    }
  }

  // TODO: [Register Allocator] This could be improved.

  // Decided to SPILL.
  return RAAssignment::kPhysNone;
}

uint32_t RALocalAllocator::decideOnSpillFor(RegGroup group, uint32_t workId, RegMask spillableRegs, uint32_t* spillWorkId) const noexcept {
  // May be used in the future to decide which register would be best to spill so `workId` can be assigned.
  DebugUtils::unused(workId);
  ASMJIT_ASSERT(spillableRegs != 0);

  Support::BitWordIterator<RegMask> it(spillableRegs);
  uint32_t bestPhysId = it.next();
  uint32_t bestWorkId = _curAssignment.physToWorkId(group, bestPhysId);

  // Avoid calculating the cost model if there is only one spillable register.
  if (it.hasNext()) {
    uint32_t bestCost = calculateSpillCost(group, bestWorkId, bestPhysId);
    do {
      uint32_t localPhysId = it.next();
      uint32_t localWorkId = _curAssignment.physToWorkId(group, localPhysId);
      uint32_t localCost = calculateSpillCost(group, localWorkId, localPhysId);

      if (localCost < bestCost) {
        bestCost = localCost;
        bestPhysId = localPhysId;
        bestWorkId = localWorkId;
      }
    } while (it.hasNext());
  }

  *spillWorkId = bestWorkId;
  return bestPhysId;
}

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER

```

`CodeCleaner/asmjit/asmjit/core/ralocal_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_RALOCAL_P_H_INCLUDED
#define ASMJIT_CORE_RALOCAL_P_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/raassignment_p.h"
#include "../core/radefs_p.h"
#include "../core/rapass_p.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_ra
//! \{

//! Local register allocator.
class RALocalAllocator {
public:
  ASMJIT_NONCOPYABLE(RALocalAllocator)

  using PhysToWorkMap = RAAssignment::PhysToWorkMap;
  using WorkToPhysMap = RAAssignment::WorkToPhysMap;

  //! Link to `BaseRAPass`.
  BaseRAPass* _pass {};
  //! Link to `BaseCompiler`.
  BaseCompiler* _cc {};

  //! Architecture traits.
  const ArchTraits* _archTraits {};
  //! Registers available to the allocator.
  RARegMask _availableRegs {};
  //! Registers clobbered by the allocator.
  RARegMask _clobberedRegs {};
  //! Registers that must be preserved by the function (clobbering means saving & restoring in function prolog & epilog).
  RARegMask _funcPreservedRegs {};

  //! Register assignment (current).
  RAAssignment _curAssignment {};
  //! Register assignment used temporarily during assignment switches.
  RAAssignment _tmpAssignment {};

  //! Link to the current `RABlock`.
  RABlock* _block {};
  //! InstNode.
  InstNode* _node {};
  //! RA instruction.
  RAInst* _raInst {};

  //! Count of all TiedReg's.
  uint32_t _tiedTotal {};
  //! TiedReg's total counter.
  RARegCount _tiedCount {};

  //! Temporary workToPhysMap that can be used freely by the allocator.
  WorkToPhysMap* _tmpWorkToPhysMap {};

  //! \name Construction & Destruction
  //! \{

  inline explicit RALocalAllocator(BaseRAPass* pass) noexcept
    : _pass(pass),
      _cc(pass->cc()),
      _archTraits(pass->_archTraits),
      _availableRegs(pass->_availableRegs) {
    _funcPreservedRegs.init(pass->func()->frame().preservedRegs());
  }

  Error init() noexcept;

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG RAWorkReg* workRegById(uint32_t workId) const noexcept { return _pass->workRegById(workId); }
  ASMJIT_INLINE_NODEBUG PhysToWorkMap* physToWorkMap() const noexcept { return _curAssignment.physToWorkMap(); }
  ASMJIT_INLINE_NODEBUG WorkToPhysMap* workToPhysMap() const noexcept { return _curAssignment.workToPhysMap(); }

  //! Returns the currently processed block.
  ASMJIT_INLINE_NODEBUG RABlock* block() const noexcept { return _block; }
  //! Sets the currently processed block.
  ASMJIT_INLINE_NODEBUG void setBlock(RABlock* block) noexcept { _block = block; }

  //! Returns the currently processed `InstNode`.
  ASMJIT_INLINE_NODEBUG InstNode* node() const noexcept { return _node; }
  //! Returns the currently processed `RAInst`.
  ASMJIT_INLINE_NODEBUG RAInst* raInst() const noexcept { return _raInst; }

  //! Returns all tied regs as `RATiedReg` array.
  ASMJIT_INLINE_NODEBUG RATiedReg* tiedRegs() const noexcept { return _raInst->tiedRegs(); }
  //! Returns tied registers grouped by the given `group`.
  ASMJIT_INLINE_NODEBUG RATiedReg* tiedRegs(RegGroup group) const noexcept { return _raInst->tiedRegs(group); }

  //! Returns count of all TiedRegs used by the instruction.
  ASMJIT_INLINE_NODEBUG uint32_t tiedCount() const noexcept { return _tiedTotal; }
  //! Returns count of TiedRegs used by the given register `group`.
  ASMJIT_INLINE_NODEBUG uint32_t tiedCount(RegGroup group) const noexcept { return _tiedCount.get(group); }

  ASMJIT_INLINE_NODEBUG bool isGroupUsed(RegGroup group) const noexcept { return _tiedCount[group] != 0; }

  //! \}

  //! \name Assignment
  //! \{

  Error makeInitialAssignment() noexcept;

  Error replaceAssignment(const PhysToWorkMap* physToWorkMap) noexcept;

  //! Switch to the given assignment by reassigning all register and emitting code that reassigns them.
  //! This is always used to switch to a previously stored assignment.
  //!
  //! If `tryMode` is true then the final assignment doesn't have to be exactly same as specified by `dstPhysToWorkMap`
  //! and `dstWorkToPhysMap`. This mode is only used before conditional jumps that already have assignment to generate
  //! a code sequence that is always executed regardless of the flow.
  Error switchToAssignment(PhysToWorkMap* dstPhysToWorkMap, const ZoneBitVector& liveIn, bool dstReadOnly, bool tryMode) noexcept;

  ASMJIT_INLINE_NODEBUG Error spillRegsBeforeEntry(RABlock* block) noexcept {
    return spillScratchGpRegsBeforeEntry(block->entryScratchGpRegs());
  }

  Error spillScratchGpRegsBeforeEntry(uint32_t scratchRegs) noexcept;

  //! \}

  //! \name Allocation
  //! \{

  Error allocInst(InstNode* node) noexcept;
  Error spillAfterAllocation(InstNode* node) noexcept;

  Error allocBranch(InstNode* node, RABlock* target, RABlock* cont) noexcept;
  Error allocJumpTable(InstNode* node, const RABlocks& targets, RABlock* cont) noexcept;

  //! \}

  //! \name Decision Making
  //! \{

  enum CostModel : uint32_t {
    kCostOfFrequency = 1048576,
    kCostOfDirtyFlag = kCostOfFrequency / 4
  };

  ASMJIT_INLINE_NODEBUG uint32_t costByFrequency(float freq) const noexcept {
    return uint32_t(int32_t(freq * float(kCostOfFrequency)));
  }

  ASMJIT_FORCE_INLINE uint32_t calculateSpillCost(RegGroup group, uint32_t workId, uint32_t assignedId) const noexcept {
    RAWorkReg* workReg = workRegById(workId);
    uint32_t cost = costByFrequency(workReg->liveStats().freq());

    if (_curAssignment.isPhysDirty(group, assignedId))
      cost += kCostOfDirtyFlag;

    return cost;
  }

  ASMJIT_FORCE_INLINE uint32_t pickBestSuitableRegister(RegGroup group, RegMask allocableRegs) const noexcept {
    // These are registers must be preserved by the function itself.
    RegMask preservedRegs = _funcPreservedRegs[group];

    // Reduce the set by removing preserved registers when possible.
    if (allocableRegs & ~preservedRegs) {
      allocableRegs &= ~preservedRegs;
    }

    return Support::ctz(allocableRegs);
  }

  //! Decides on register assignment.
  uint32_t decideOnAssignment(RegGroup group, uint32_t workId, uint32_t assignedId, RegMask allocableRegs) const noexcept;

  //! Decides on whether to MOVE or SPILL the given WorkReg, because it's allocated in a physical register that have
  //! to be used by another WorkReg.
  //!
  //! The function must return either `RAAssignment::kPhysNone`, which means that the WorkReg of `workId` should be
  //! spilled, or a valid physical register ID, which means that the register should be moved to that physical register
  //! instead.
  uint32_t decideOnReassignment(RegGroup group, uint32_t workId, uint32_t assignedId, RegMask allocableRegs, RAInst* raInst) const noexcept;

  //! Decides on best spill given a register mask `spillableRegs`
  uint32_t decideOnSpillFor(RegGroup group, uint32_t workId, RegMask spillableRegs, uint32_t* spillWorkId) const noexcept;

  //! \}

  //! \name Emit
  //! \{

  //! Emits a move between a destination and source register, and fixes the
  //! register assignment.
  inline Error onMoveReg(RegGroup group, uint32_t workId, uint32_t dstPhysId, uint32_t srcPhysId) noexcept {
    if (dstPhysId == srcPhysId) {
      return kErrorOk;
    }

    _curAssignment.reassign(group, workId, dstPhysId, srcPhysId);
    return _pass->emitMove(workId, dstPhysId, srcPhysId);
  }

  //! Emits a swap between two physical registers and fixes their assignment.
  //!
  //! \note Target must support this operation otherwise this would ASSERT.
  inline Error onSwapReg(RegGroup group, uint32_t aWorkId, uint32_t aPhysId, uint32_t bWorkId, uint32_t bPhysId) noexcept {
    _curAssignment.swap(group, aWorkId, aPhysId, bWorkId, bPhysId);
    return _pass->emitSwap(aWorkId, aPhysId, bWorkId, bPhysId);
  }

  //! Emits a load from [VirtReg/WorkReg]'s spill slot to a physical register
  //! and makes it assigned and clean.
  inline Error onLoadReg(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    _curAssignment.assign(group, workId, physId, RAAssignment::kClean);
    return _pass->emitLoad(workId, physId);
  }

  //! Emits a save a physical register to a [VirtReg/WorkReg]'s spill slot,
  //! keeps it assigned, and makes it clean.
  inline Error onSaveReg(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    ASMJIT_ASSERT(_curAssignment.workToPhysId(group, workId) == physId);
    ASMJIT_ASSERT(_curAssignment.physToWorkId(group, physId) == workId);

    _curAssignment.makeClean(group, workId, physId);
    return _pass->emitSave(workId, physId);
  }

  //! Assigns a register, the content of it is undefined at this point.
  inline Error onAssignReg(RegGroup group, uint32_t workId, uint32_t physId, bool dirty) noexcept {
    _curAssignment.assign(group, workId, physId, dirty);
    return kErrorOk;
  }

  //! Spills a variable/register, saves the content to the memory-home if modified.
  inline Error onSpillReg(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    if (_curAssignment.isPhysDirty(group, physId))
      ASMJIT_PROPAGATE(onSaveReg(group, workId, physId));
    return onKillReg(group, workId, physId);
  }

  inline Error onDirtyReg(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    _curAssignment.makeDirty(group, workId, physId);
    return kErrorOk;
  }

  inline Error onKillReg(RegGroup group, uint32_t workId, uint32_t physId) noexcept {
    _curAssignment.unassign(group, workId, physId);
    return kErrorOk;
  }

  //! \}
};

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER
#endif // ASMJIT_CORE_RALOCAL_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/rapass.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/formatter.h"
#include "../core/ralocal_p.h"
#include "../core/rapass_p.h"
#include "../core/support.h"
#include "../core/type.h"
#include "../core/zonestack.h"

ASMJIT_BEGIN_NAMESPACE

// RABlock - Control Flow
// ======================

Error RABlock::appendSuccessor(RABlock* successor) noexcept {
  RABlock* predecessor = this;

  if (predecessor->hasSuccessor(successor))
    return kErrorOk;

  ASMJIT_PROPAGATE(successor->_predecessors.willGrow(allocator()));
  ASMJIT_PROPAGATE(predecessor->_successors.willGrow(allocator()));

  predecessor->_successors.appendUnsafe(successor);
  successor->_predecessors.appendUnsafe(predecessor);

  return kErrorOk;
}

Error RABlock::prependSuccessor(RABlock* successor) noexcept {
  RABlock* predecessor = this;

  if (predecessor->hasSuccessor(successor))
    return kErrorOk;

  ASMJIT_PROPAGATE(successor->_predecessors.willGrow(allocator()));
  ASMJIT_PROPAGATE(predecessor->_successors.willGrow(allocator()));

  predecessor->_successors.prependUnsafe(successor);
  successor->_predecessors.prependUnsafe(predecessor);

  return kErrorOk;
}

// BaseRAPass - Construction & Destruction
// =======================================

BaseRAPass::BaseRAPass() noexcept : FuncPass("BaseRAPass") {}
BaseRAPass::~BaseRAPass() noexcept {}

// BaseRAPass - RunOnFunction
// ==========================

static void BaseRAPass_reset(BaseRAPass* self, FuncDetail* funcDetail) noexcept {
  ZoneAllocator* allocator = self->allocator();

  self->_blocks.reset();
  self->_exits.reset();
  self->_pov.reset();
  self->_workRegs.reset();
  self->_instructionCount = 0;
  self->_createdBlockCount = 0;

  self->_sharedAssignments.reset();
  self->_lastTimestamp = 0;

  self->_archTraits = nullptr;
  self->_physRegIndex.reset();
  self->_physRegCount.reset();
  self->_physRegTotal = 0;
  self->_scratchRegIndexes.fill(BaseReg::kIdBad);

  self->_availableRegs.reset();
  self->_availableRegCount.reset();
  self->_clobberedRegs.reset();

  self->_workRegs.reset();
  self->_workRegsOfGroup.forEach([](RAWorkRegs& regs) { regs.reset(); });
  self->_strategy.forEach([](RAStrategy& strategy) { strategy.reset(); });
  self->_globalLiveSpans.fill(nullptr);
  self->_globalMaxLiveCount.reset();
  self->_temporaryMem.reset();

  self->_stackAllocator.reset(allocator);
  self->_argsAssignment.reset(funcDetail);
  self->_numStackArgsToStackSlots = 0;
  self->_maxWorkRegNameSize = 0;
}

static void BaseRAPass_resetVirtRegData(BaseRAPass* self) noexcept {
  for (RAWorkReg* wReg : self->_workRegs) {
    VirtReg* vReg = wReg->virtReg();

    // Update the information regarding the stack of the virtual register.
    if (wReg->hasStackSlot()) {
      RAStackSlot* slot = wReg->stackSlot();
      vReg->assignStackSlot(slot->offset());
    }

    // Reset work reg association so it cannot be used by accident (RAWorkReg data will be destroyed).
    vReg->_workReg = nullptr;
  }
}

Error BaseRAPass::runOnFunction(Zone* zone, Logger* logger, FuncNode* func) {
  _allocator.reset(zone);

#ifndef ASMJIT_NO_LOGGING
  _logger = logger;
  _formatOptions.reset();
  _diagnosticOptions = _cb->diagnosticOptions();

  if (logger) {
    _formatOptions = logger->options();
  }
  else {
    _diagnosticOptions &= ~(DiagnosticOptions::kRADebugCFG |
                            DiagnosticOptions::kRADebugUnreachable);
  }
#else
  DebugUtils::unused(logger);
#endif

  // Initialize all core structures to use `zone` and `func`.
  BaseNode* end = func->endNode();
  _func = func;
  _stop = end->next();
  _extraBlock = end;

  BaseRAPass_reset(this, &_func->_funcDetail);

  // Initialize architecture-specific members.
  onInit();

  // Perform all allocation steps required.
  Error err = onPerformAllSteps();

  // Must be called regardless of the allocation status.
  onDone();

  // Reset possible connections introduced by the register allocator.
  BaseRAPass_resetVirtRegData(this);

  // Reset all core structures and everything that depends on the passed `Zone`.
  BaseRAPass_reset(this, nullptr);
  _allocator.reset(nullptr);

#ifndef ASMJIT_NO_LOGGING
  _logger = nullptr;
  _formatOptions.reset();
  _diagnosticOptions = DiagnosticOptions::kNone;
#endif

  _func = nullptr;
  _stop = nullptr;
  _extraBlock = nullptr;

  // Reset `Zone` as nothing should persist between `runOnFunction()` calls.
  zone->reset();

  // We alter the compiler cursor, because it doesn't make sense to reference it after the compilation - some
  // nodes may disappear and the old cursor can go out anyway.
  cc()->_setCursor(cc()->lastNode());

  return err;
}

Error BaseRAPass::onPerformAllSteps() noexcept {
  ASMJIT_PROPAGATE(buildCFG());
  ASMJIT_PROPAGATE(buildCFGViews());
  ASMJIT_PROPAGATE(removeUnreachableCode());

  ASMJIT_PROPAGATE(buildCFGDominators());
  ASMJIT_PROPAGATE(buildLiveness());
  ASMJIT_PROPAGATE(assignArgIndexToWorkRegs());

#ifndef ASMJIT_NO_LOGGING
  if (hasDiagnosticOption(DiagnosticOptions::kRAAnnotate))
    ASMJIT_PROPAGATE(annotateCode());
#endif

  ASMJIT_PROPAGATE(runGlobalAllocator());
  ASMJIT_PROPAGATE(runLocalAllocator());

  ASMJIT_PROPAGATE(updateStackFrame());
  ASMJIT_PROPAGATE(insertPrologEpilog());

  ASMJIT_PROPAGATE(rewrite());

  return kErrorOk;
}

// BaseRAPass - Events
// ===================

void BaseRAPass::onInit() noexcept {}
void BaseRAPass::onDone() noexcept {}

// BaseRAPass - CFG - Basic Block Management
// =========================================

RABlock* BaseRAPass::newBlock(BaseNode* initialNode) noexcept {
  RABlock* block = zone()->newT<RABlock>(this);
  if (ASMJIT_UNLIKELY(!block))
    return nullptr;

  block->setFirst(initialNode);
  block->setLast(initialNode);

  _createdBlockCount++;
  return block;
}

RABlock* BaseRAPass::newBlockOrExistingAt(LabelNode* cbLabel, BaseNode** stoppedAt) noexcept {
  if (cbLabel->hasPassData())
    return cbLabel->passData<RABlock>();

  FuncNode* func = this->func();
  BaseNode* node = cbLabel->prev();
  RABlock* block = nullptr;

  // Try to find some label, but terminate the loop on any code. We try hard to coalesce code that contains two
  // consecutive labels or a combination of non-code nodes between 2 or more labels.
  //
  // Possible cases that would share the same basic block:
  //
  //   1. Two or more consecutive labels:
  //     Label1:
  //     Label2:
  //
  //   2. Two or more labels separated by non-code nodes:
  //     Label1:
  //     ; Some comment...
  //     .align 16
  //     Label2:
  size_t nPendingLabels = 0;

  while (node) {
    if (node->type() == NodeType::kLabel) {
      // Function has a different NodeType, just make sure this was not messed up as we must never associate
      // BasicBlock with a `func` itself.
      ASMJIT_ASSERT(node != func);

      block = node->passData<RABlock>();
      if (block) {
        // Exit node has always a block associated with it. If we went here it means that `cbLabel` passed here
        // is after the end of the function and cannot be merged with the function exit block.
        if (node == func->exitNode())
          block = nullptr;
        break;
      }

      nPendingLabels++;
    }
    else if (node->type() == NodeType::kAlign) {
      // Align node is fine.
    }
    else {
      break;
    }

    node = node->prev();
  }

  if (stoppedAt)
    *stoppedAt = node;

  if (!block) {
    block = newBlock();
    if (ASMJIT_UNLIKELY(!block))
      return nullptr;
  }

  cbLabel->setPassData<RABlock>(block);
  node = cbLabel;

  while (nPendingLabels) {
    node = node->prev();
    for (;;) {
      if (node->type() == NodeType::kLabel) {
        node->setPassData<RABlock>(block);
        nPendingLabels--;
        break;
      }

      node = node->prev();
      ASMJIT_ASSERT(node != nullptr);
    }
  }

  if (!block->first()) {
    block->setFirst(node);
    block->setLast(cbLabel);
  }

  return block;
}

Error BaseRAPass::addBlock(RABlock* block) noexcept {
  ASMJIT_PROPAGATE(_blocks.willGrow(allocator()));

  block->_blockId = blockCount();
  _blocks.appendUnsafe(block);
  return kErrorOk;
}

// BaseRAPass - CFG - Build
// ========================

// [[pure virtual]]
Error BaseRAPass::buildCFG() noexcept {
  return DebugUtils::errored(kErrorInvalidState);
}

Error BaseRAPass::initSharedAssignments(const ZoneVector<uint32_t>& sharedAssignmentsMap) noexcept {
  if (sharedAssignmentsMap.empty())
    return kErrorOk;

  uint32_t count = 0;
  for (RABlock* block : _blocks) {
    if (block->hasSharedAssignmentId()) {
      uint32_t sharedAssignmentId = sharedAssignmentsMap[block->sharedAssignmentId()];
      block->setSharedAssignmentId(sharedAssignmentId);
      count = Support::max(count, sharedAssignmentId + 1);
    }
  }

  ASMJIT_PROPAGATE(_sharedAssignments.resize(allocator(), count));

  // Aggregate all entry scratch GP regs from blocks of the same assignment to the assignment itself. It will then be
  // used instead of RABlock's own scratch regs mask, as shared assignments have precedence.
  for (RABlock* block : _blocks) {
    if (block->hasJumpTable()) {
      const RABlocks& successors = block->successors();
      if (!successors.empty()) {
        RABlock* firstSuccessor = successors[0];
        // NOTE: Shared assignments connect all possible successors so we only need the first to propagate exit scratch
        // GP registers.
        if (firstSuccessor->hasSharedAssignmentId()) {
          RASharedAssignment& sa = _sharedAssignments[firstSuccessor->sharedAssignmentId()];
          sa.addEntryScratchGpRegs(block->exitScratchGpRegs());
        }
        else {
          // This is only allowed if there is a single successor - in that case shared assignment is not necessary.
          ASMJIT_ASSERT(successors.size() == 1u);
        }
      }
    }
    if (block->hasSharedAssignmentId()) {
      RASharedAssignment& sa = _sharedAssignments[block->sharedAssignmentId()];
      sa.addEntryScratchGpRegs(block->_entryScratchGpRegs);
    }
  }

  return kErrorOk;
}

// BaseRAPass - CFG - Views Order
// ==============================

class RABlockVisitItem {
public:
  RABlock* _block {};
  uint32_t _index {};

  inline RABlockVisitItem(RABlock* block, uint32_t index) noexcept
    : _block(block),
      _index(index) {}

  inline RABlockVisitItem(const RABlockVisitItem& other) noexcept = default;
  inline RABlockVisitItem& operator=(const RABlockVisitItem& other) noexcept = default;

  inline RABlock* block() const noexcept { return _block; }
  inline uint32_t index() const noexcept { return _index; }
};

Error BaseRAPass::buildCFGViews() noexcept {
#ifndef ASMJIT_NO_LOGGING
  Logger* logger = getLoggerIf(DiagnosticOptions::kRADebugCFG);
  ASMJIT_RA_LOG_FORMAT("[BuildCFGViews]\n");
#endif

  uint32_t count = blockCount();
  if (ASMJIT_UNLIKELY(!count)) return kErrorOk;

  ASMJIT_PROPAGATE(_pov.reserve(allocator(), count));

  ZoneStack<RABlockVisitItem> stack;
  ASMJIT_PROPAGATE(stack.init(allocator()));

  ZoneBitVector visited;
  ASMJIT_PROPAGATE(visited.resize(allocator(), count));

  RABlock* current = _blocks[0];
  uint32_t i = 0;

  for (;;) {
    for (;;) {
      if (i >= current->successors().size())
        break;

      // Skip if already visited.
      RABlock* child = current->successors()[i++];
      if (visited.bitAt(child->blockId()))
        continue;

      // Mark as visited to prevent visiting the same block multiple times.
      visited.setBit(child->blockId(), true);

      // Add the current block on the stack, we will get back to it later.
      ASMJIT_PROPAGATE(stack.append(RABlockVisitItem(current, i)));
      current = child;
      i = 0;
    }

    current->makeReachable();
    current->_povOrder = _pov.size();
    _pov.appendUnsafe(current);

    if (stack.empty())
      break;

    RABlockVisitItem top = stack.pop();
    current = top.block();
    i = top.index();
  }

  ASMJIT_RA_LOG_COMPLEX({
    StringTmp<1024> sb;
    for (RABlock* block : blocks()) {
      sb.clear();
      if (block->hasSuccessors()) {
        sb.appendFormat("  #%u -> {", block->blockId());
        _dumpBlockIds(sb, block->successors());
        sb.append("}\n");
      }
      else {
        sb.appendFormat("  #%u -> {Exit}\n", block->blockId());
      }
      logger->log(sb);
    }
  });

  visited.release(allocator());
  return kErrorOk;
}

// BaseRAPass - CFG - Dominators
// =============================

static ASMJIT_FORCE_INLINE RABlock* intersectBlocks(RABlock* b1, RABlock* b2) noexcept {
  while (b1 != b2) {
    while (b2->povOrder() > b1->povOrder()) b1 = b1->iDom();
    while (b1->povOrder() > b2->povOrder()) b2 = b2->iDom();
  }
  return b1;
}

// Based on "A Simple, Fast Dominance Algorithm".
Error BaseRAPass::buildCFGDominators() noexcept {
#ifndef ASMJIT_NO_LOGGING
  Logger* logger = getLoggerIf(DiagnosticOptions::kRADebugCFG);
  ASMJIT_RA_LOG_FORMAT("[BuildCFGDominators]\n");
#endif

  if (_blocks.empty())
    return kErrorOk;

  RABlock* entryBlock = this->entryBlock();
  entryBlock->setIDom(entryBlock);

  bool changed = true;

#ifndef ASMJIT_NO_LOGGING
  uint32_t numIters = 0;
#endif

  while (changed) {
    changed = false;

#ifndef ASMJIT_NO_LOGGING
    numIters++;
#endif

    uint32_t i = _pov.size();
    while (i) {
      RABlock* block = _pov[--i];
      if (block == entryBlock)
        continue;

      RABlock* iDom = nullptr;
      const RABlocks& preds = block->predecessors();

      uint32_t j = preds.size();
      while (j) {
        RABlock* p = preds[--j];
        if (!p->iDom())
          continue;
        iDom = !iDom ? p : intersectBlocks(iDom, p);
      }

      if (block->iDom() != iDom) {
        ASMJIT_ASSUME(iDom != nullptr);
        ASMJIT_RA_LOG_FORMAT("  IDom of #%u -> #%u\n", block->blockId(), iDom->blockId());
        block->setIDom(iDom);
        changed = true;
      }
    }
  }

  ASMJIT_RA_LOG_FORMAT("  Done (%u iterations)\n", numIters);
  return kErrorOk;
}

bool BaseRAPass::_strictlyDominates(const RABlock* a, const RABlock* b) const noexcept {
  ASMJIT_ASSERT(a != nullptr); // There must be at least one block if this function is
  ASMJIT_ASSERT(b != nullptr); // called, as both `a` and `b` must be valid blocks.
  ASMJIT_ASSERT(a != b);       // Checked by `dominates()` and `strictlyDominates()`.

  // Nothing strictly dominates the entry block.
  const RABlock* entryBlock = this->entryBlock();
  if (a == entryBlock)
    return false;

  const RABlock* iDom = b->iDom();
  while (iDom != a && iDom != entryBlock)
    iDom = iDom->iDom();

  return iDom != entryBlock;
}

const RABlock* BaseRAPass::_nearestCommonDominator(const RABlock* a, const RABlock* b) const noexcept {
  ASMJIT_ASSERT(a != nullptr); // There must be at least one block if this function is
  ASMJIT_ASSERT(b != nullptr); // called, as both `a` and `b` must be valid blocks.
  ASMJIT_ASSERT(a != b);       // Checked by `dominates()` and `properlyDominates()`.

  if (a == b)
    return a;

  // If `a` strictly dominates `b` then `a` is the nearest common dominator.
  if (_strictlyDominates(a, b))
    return a;

  // If `b` strictly dominates `a` then `b` is the nearest common dominator.
  if (_strictlyDominates(b, a))
    return b;

  const RABlock* entryBlock = this->entryBlock();
  uint64_t timestamp = nextTimestamp();

  // Mark all A's dominators.
  const RABlock* block = a->iDom();
  while (block != entryBlock) {
    block->setTimestamp(timestamp);
    block = block->iDom();
  }

  // Check all B's dominators against marked dominators of A.
  block = b->iDom();
  while (block != entryBlock) {
    if (block->hasTimestamp(timestamp))
      return block;
    block = block->iDom();
  }

  return entryBlock;
}

// BaseRAPass - CFG - Utilities
// ============================

Error BaseRAPass::removeUnreachableCode() noexcept {
  uint32_t numAllBlocks = blockCount();
  uint32_t numReachableBlocks = reachableBlockCount();

  // All reachable -> nothing to do.
  if (numAllBlocks == numReachableBlocks)
    return kErrorOk;

#ifndef ASMJIT_NO_LOGGING
  StringTmp<256> sb;
  Logger* logger = getLoggerIf(DiagnosticOptions::kRADebugUnreachable);
  ASMJIT_RA_LOG_FORMAT("[RemoveUnreachableCode - detected %u of %u unreachable blocks]\n", numAllBlocks - numReachableBlocks, numAllBlocks);
#endif

  for (uint32_t i = 0; i < numAllBlocks; i++) {
    RABlock* block = _blocks[i];
    if (block->isReachable())
      continue;

    ASMJIT_RA_LOG_FORMAT("  Removing code from unreachable block {%u}\n", i);
    BaseNode* first = block->first();
    BaseNode* last = block->last();

    BaseNode* beforeFirst = first->prev();
    BaseNode* afterLast = last->next();

    BaseNode* node = first;
    while (node != afterLast) {
      BaseNode* next = node->next();

      if (node->isCode() || node->isRemovable()) {
#ifndef ASMJIT_NO_LOGGING
        if (logger) {
          sb.clear();
          Formatter::formatNode(sb, _formatOptions, cc(), node);
          logger->logf("    %s\n", sb.data());
        }
#endif
        cc()->removeNode(node);
      }
      node = next;
    }

    if (beforeFirst->next() == afterLast) {
      block->setFirst(nullptr);
      block->setLast(nullptr);
    }
    else {
      block->setFirst(beforeFirst->next());
      block->setLast(afterLast->prev());
    }
  }

  return kErrorOk;
}

BaseNode* BaseRAPass::findSuccessorStartingAt(BaseNode* node) noexcept {
  while (node && (node->isInformative() || node->hasNoEffect()))
    node = node->next();
  return node;
}

bool BaseRAPass::isNextTo(BaseNode* node, BaseNode* target) noexcept {
  for (;;) {
    node = node->next();
    if (node == target)
      return true;

    if (!node)
      return false;

    if (node->isCode() || node->isData())
      return false;
  }
}

// BaseRAPass - Registers - VirtReg / WorkReg Mapping
// ==================================================

Error BaseRAPass::_asWorkReg(VirtReg* vReg, RAWorkReg** out) noexcept {
  // Checked by `asWorkReg()` - must be true.
  ASMJIT_ASSERT(vReg->_workReg == nullptr);

  RegGroup group = vReg->group();
  ASMJIT_ASSERT(group <= RegGroup::kMaxVirt);

  RAWorkRegs& wRegs = workRegs();
  RAWorkRegs& wRegsByGroup = workRegs(group);

  ASMJIT_PROPAGATE(wRegs.willGrow(allocator()));
  ASMJIT_PROPAGATE(wRegsByGroup.willGrow(allocator()));

  RAWorkReg* wReg = zone()->newT<RAWorkReg>(vReg, wRegs.size());
  if (ASMJIT_UNLIKELY(!wReg))
    return DebugUtils::errored(kErrorOutOfMemory);

  vReg->setWorkReg(wReg);
  if (!vReg->isStack())
    wReg->setRegByteMask(Support::lsbMask<uint64_t>(vReg->virtSize()));
  wRegs.appendUnsafe(wReg);
  wRegsByGroup.appendUnsafe(wReg);

  // Only used by RA logging.
  _maxWorkRegNameSize = Support::max(_maxWorkRegNameSize, vReg->nameSize());

  *out = wReg;
  return kErrorOk;
}

RAAssignment::WorkToPhysMap* BaseRAPass::newWorkToPhysMap() noexcept {
  uint32_t count = workRegCount();
  size_t size = WorkToPhysMap::sizeOf(count);

  // If no registers are used it could be zero, in that case return a dummy
  // map instead of NULL.
  if (ASMJIT_UNLIKELY(!size)) {
    static const RAAssignment::WorkToPhysMap nullMap = {{ 0 }};
    return const_cast<RAAssignment::WorkToPhysMap*>(&nullMap);
  }

  WorkToPhysMap* map = zone()->allocT<WorkToPhysMap>(size);
  if (ASMJIT_UNLIKELY(!map))
    return nullptr;

  map->reset(count);
  return map;
}

RAAssignment::PhysToWorkMap* BaseRAPass::newPhysToWorkMap() noexcept {
  uint32_t count = physRegTotal();
  size_t size = PhysToWorkMap::sizeOf(count);

  PhysToWorkMap* map = zone()->allocT<PhysToWorkMap>(size);
  if (ASMJIT_UNLIKELY(!map))
    return nullptr;

  map->reset(count);
  return map;
}

// BaseRAPass - Registers - Liveness Analysis and Statistics
// =========================================================

namespace LiveOps {
  typedef ZoneBitVector::BitWord BitWord;

  struct In {
    static ASMJIT_FORCE_INLINE BitWord op(BitWord dst, BitWord out, BitWord gen, BitWord kill) noexcept {
      DebugUtils::unused(dst);
      return (out | gen) & ~kill;
    }
  };

  template<typename Operator>
  static ASMJIT_FORCE_INLINE bool op(BitWord* dst, const BitWord* a, uint32_t n) noexcept {
    BitWord changed = 0;

    for (uint32_t i = 0; i < n; i++) {
      BitWord before = dst[i];
      BitWord after = Operator::op(before, a[i]);

      dst[i] = after;
      changed |= (before ^ after);
    }

    return changed != 0;
  }

  template<typename Operator>
  static ASMJIT_FORCE_INLINE bool op(BitWord* dst, const BitWord* a, const BitWord* b, uint32_t n) noexcept {
    BitWord changed = 0;

    for (uint32_t i = 0; i < n; i++) {
      BitWord before = dst[i];
      BitWord after = Operator::op(before, a[i], b[i]);

      dst[i] = after;
      changed |= (before ^ after);
    }

    return changed != 0;
  }

  template<typename Operator>
  static ASMJIT_FORCE_INLINE bool op(BitWord* dst, const BitWord* a, const BitWord* b, const BitWord* c, uint32_t n) noexcept {
    BitWord changed = 0;

#if defined(_MSC_VER) && _MSC_VER <= 1938
    // MSVC workaround (see #427).
    //
    // MSVC incorrectly auto-vectorizes this loop when used with <In> operator. For some reason it trashes a content
    // of a register, which causes the result to be incorrect. It's a compiler bug we have to prevent unfortunately.
    #pragma loop(no_vector)
#endif
    for (uint32_t i = 0; i < n; i++) {
      BitWord before = dst[i];
      BitWord after = Operator::op(before, a[i], b[i], c[i]);

      dst[i] = after;
      changed |= (before ^ after);
    }

    return changed != 0;
  }

  static ASMJIT_NOINLINE bool recalcInOut(RABlock* block, uint32_t numBitWords, bool initial = false) noexcept {
    bool changed = initial;

    const RABlocks& successors = block->successors();
    uint32_t numSuccessors = successors.size();

    // Calculate `OUT` based on `IN` of all successors.
    for (uint32_t i = 0; i < numSuccessors; i++)
      changed |= op<Support::Or>(block->liveOut().data(), successors[i]->liveIn().data(), numBitWords);

    // Calculate `IN` based on `OUT`, `GEN`, and `KILL` bits.
    if (changed)
      changed = op<In>(block->liveIn().data(), block->liveOut().data(), block->gen().data(), block->kill().data(), numBitWords);

    return changed;
  }
}

ASMJIT_FAVOR_SPEED Error BaseRAPass::buildLiveness() noexcept {
#ifndef ASMJIT_NO_LOGGING
  Logger* logger = getLoggerIf(DiagnosticOptions::kRADebugLiveness);
  StringTmp<512> sb;
#endif

  ASMJIT_RA_LOG_FORMAT("[BuildLiveness]\n");

  uint32_t i;

  uint32_t numAllBlocks = blockCount();
  uint32_t numReachableBlocks = reachableBlockCount();

  uint32_t numWorkRegs = workRegCount();
  uint32_t numBitWords = ZoneBitVector::_wordsPerBits(numWorkRegs);

  if (!numWorkRegs) {
    ASMJIT_RA_LOG_FORMAT("  Done (no virtual registers)\n");
    return kErrorOk;
  }

  ZoneVector<uint32_t> nUsesPerWorkReg; // Number of USEs of each RAWorkReg.
  ZoneVector<uint32_t> nOutsPerWorkReg; // Number of OUTs of each RAWorkReg.
  ZoneVector<uint32_t> nInstsPerBlock;  // Number of instructions of each RABlock.

  ASMJIT_PROPAGATE(nUsesPerWorkReg.resize(allocator(), numWorkRegs));
  ASMJIT_PROPAGATE(nOutsPerWorkReg.resize(allocator(), numWorkRegs));
  ASMJIT_PROPAGATE(nInstsPerBlock.resize(allocator(), numAllBlocks));

  // Calculate GEN/KILL of Each Block
  // --------------------------------

  for (i = 0; i < numReachableBlocks; i++) {
    RABlock* block = _pov[i];
    ASMJIT_PROPAGATE(block->resizeLiveBits(numWorkRegs));

    BaseNode* node = block->last();
    BaseNode* stop = block->first();

    uint32_t nInsts = 0;
    for (;;) {
      if (node->isInst()) {
        InstNode* inst = node->as<InstNode>();
        RAInst* raInst = inst->passData<RAInst>();
        ASMJIT_ASSERT(raInst != nullptr);

        RATiedReg* tiedRegs = raInst->tiedRegs();
        uint32_t count = raInst->tiedCount();

        for (uint32_t j = 0; j < count; j++) {
          RATiedReg* tiedReg = &tiedRegs[j];
          uint32_t workId = tiedReg->workId();

          // Update `nUses` and `nOuts`.
          nUsesPerWorkReg[workId] += 1u;
          nOutsPerWorkReg[workId] += uint32_t(tiedReg->isWrite());

          // Mark as:
          //   KILL - if this VirtReg is killed afterwards.
          //   LAST - if this VirtReg is last in this basic block.
          if (block->kill().bitAt(workId))
            tiedReg->addFlags(RATiedFlags::kKill);
          else if (!block->gen().bitAt(workId))
            tiedReg->addFlags(RATiedFlags::kLast);

          if (tiedReg->isWriteOnly()) {
            // KILL.
            block->kill().setBit(workId, true);
          }
          else {
            // GEN.
            block->kill().setBit(workId, false);
            block->gen().setBit(workId, true);
          }

          if (tiedReg->isLeadConsecutive()) {
            RAWorkReg* workReg = workRegById(workId);
            workReg->markLeadConsecutive();
          }

          if (tiedReg->hasConsecutiveParent()) {
            RAWorkReg* consecutiveParentReg = workRegById(tiedReg->consecutiveParent());
            ASMJIT_PROPAGATE(consecutiveParentReg->addImmediateConsecutive(allocator(), workId));
          }
        }

        nInsts++;
      }

      if (node == stop)
        break;

      node = node->prev();
      ASMJIT_ASSERT(node != nullptr);
    }

    nInstsPerBlock[block->blockId()] = nInsts;
  }

  // Calculate IN/OUT of Each Block
  // ------------------------------

#ifndef ASMJIT_NO_LOGGING
  uint32_t numVisits = numReachableBlocks;
#endif

  {
    ZoneStack<RABlock*> workList;
    ZoneBitVector workBits;

    ASMJIT_PROPAGATE(workList.init(allocator()));
    ASMJIT_PROPAGATE(workBits.resize(allocator(), blockCount(), true));

    for (i = 0; i < numReachableBlocks; i++) {
      RABlock* block = _pov[i];
      LiveOps::recalcInOut(block, numBitWords, true);
      ASMJIT_PROPAGATE(workList.append(block));
    }

    while (!workList.empty()) {
      RABlock* block = workList.popFirst();
      uint32_t blockId = block->blockId();

      workBits.setBit(blockId, false);
      if (LiveOps::recalcInOut(block, numBitWords)) {
        const RABlocks& predecessors = block->predecessors();
        uint32_t numPredecessors = predecessors.size();

        for (uint32_t j = 0; j < numPredecessors; j++) {
          RABlock* pred = predecessors[j];
          if (!workBits.bitAt(pred->blockId())) {
            workBits.setBit(pred->blockId(), true);
            ASMJIT_PROPAGATE(workList.append(pred));
          }
        }
      }
#ifndef ASMJIT_NO_LOGGING
      numVisits++;
#endif
    }

    workList.reset();
    workBits.release(allocator());
  }

  ASMJIT_RA_LOG_COMPLEX({
    logger->logf("  LiveIn/Out Done (%u visits)\n", numVisits);
    for (i = 0; i < numAllBlocks; i++) {
      RABlock* block = _blocks[i];

      ASMJIT_PROPAGATE(sb.assignFormat("  {#%u}\n", block->blockId()));
      ASMJIT_PROPAGATE(_dumpBlockLiveness(sb, block));

      logger->log(sb);
    }
  });

  // Reserve the space in each `RAWorkReg` for references
  // ----------------------------------------------------

  for (i = 0; i < numWorkRegs; i++) {
    RAWorkReg* workReg = workRegById(i);
    ASMJIT_PROPAGATE(workReg->_refs.reserve(allocator(), nUsesPerWorkReg[i]));
    ASMJIT_PROPAGATE(workReg->_writes.reserve(allocator(), nOutsPerWorkReg[i]));
  }

  // These are not needed anymore, so release the memory now so other allocations can reuse it.
  nUsesPerWorkReg.release(allocator());
  nOutsPerWorkReg.release(allocator());

  // Assign block and instruction positions, build LiveCount and LiveSpans
  // ---------------------------------------------------------------------

  // This is a starting position, reserving [0, 1] for function arguments.
  uint32_t position = 2;

  for (i = 0; i < numAllBlocks; i++) {
    RABlock* block = _blocks[i];
    if (!block->isReachable())
      continue;

    uint32_t blockId = block->blockId();

    BaseNode* node = block->first();
    BaseNode* stop = block->last();

    uint32_t endPosition = position + nInstsPerBlock[i] * 2;
    block->setFirstPosition(position);
    block->setEndPosition(endPosition);

    RALiveCount curLiveCount;
    RALiveCount maxLiveCount;

    // Process LIVE-IN.
    ZoneBitVector::ForEachBitSet it(block->liveIn());
    while (it.hasNext()) {
      RAWorkReg* workReg = _workRegs[uint32_t(it.next())];
      curLiveCount[workReg->group()]++;
      ASMJIT_PROPAGATE(workReg->liveSpans().openAt(allocator(), position, endPosition));
    }

    for (;;) {
      if (node->isInst()) {
        InstNode* inst = node->as<InstNode>();
        RAInst* raInst = inst->passData<RAInst>();

        // Impossible - each processed instruction node must have an associated RAInst.
        ASMJIT_ASSERT(raInst != nullptr);

        RATiedReg* tiedRegs = raInst->tiedRegs();
        uint32_t count = raInst->tiedCount();

        inst->setPosition(position);
        raInst->_liveCount = curLiveCount;

        for (uint32_t j = 0; j < count; j++) {
          RATiedReg* tiedReg = &tiedRegs[j];
          uint32_t workId = tiedReg->workId();

          // Create refs and writes.
          RAWorkReg* workReg = workRegById(workId);
          workReg->_refs.appendUnsafe(node);
          if (tiedReg->isWrite()) {
            workReg->_writes.appendUnsafe(node);
          }

          if (workReg->singleBasicBlockId() != blockId) {
            workReg->markUseOfMultipleBasicBlocks();
          }

          // We couldn't calculate this in previous steps, but since we know all LIVE-OUT at this point it becomes
          // trivial. If this is the last instruction that uses this `workReg` and it's not LIVE-OUT then it is
          // KILLed here.
          if (tiedReg->isLast() && !block->liveOut().bitAt(workId)) {
            tiedReg->addFlags(RATiedFlags::kKill);
          }

          LiveRegSpans& liveSpans = workReg->liveSpans();
          bool wasOpen;
          ASMJIT_PROPAGATE(liveSpans.openAt(allocator(), position + !tiedReg->isRead(), endPosition, wasOpen));

          RegGroup group = workReg->group();
          if (!wasOpen) {
            curLiveCount[group]++;
            raInst->_liveCount[group]++;
          }

          if (tiedReg->isKill()) {
            liveSpans.closeAt(position + !tiedReg->isRead() + 1);
            curLiveCount[group]--;
          }

          // Update `RAWorkReg::useIdMask` and `RAWorkReg::hintRegId`.
          if (tiedReg->hasUseId()) {
            uint32_t useId = tiedReg->useId();
            workReg->addUseIdMask(Support::bitMask(useId));
            if (!workReg->hasHintRegId() && !Support::bitTest(raInst->_clobberedRegs[group], useId)) {
              workReg->setHintRegId(useId);
            }
          }

          if (tiedReg->useRegMask()) {
            workReg->restrictPreferredMask(tiedReg->useRegMask());
            if (workReg->isLeadConsecutive()) {
              workReg->restrictConsecutiveMask(tiedReg->useRegMask());
            }
          }

          if (tiedReg->outRegMask()) {
            workReg->restrictPreferredMask(tiedReg->outRegMask());
            if (workReg->isLeadConsecutive()) {
              workReg->restrictConsecutiveMask(tiedReg->outRegMask());
            }
          }

          // Update `RAWorkReg::clobberedSurvivalMask`.
          if (raInst->_clobberedRegs[group] && !tiedReg->isOutOrKill()) {
            workReg->addClobberSurvivalMask(raInst->_clobberedRegs[group]);
          }
        }

        if (node->isInvoke()) {
          func()->frame().updateCallStackAlignment(node->as<InvokeNode>()->detail().naturalStackAlignment());
        }

        position += 2;
        maxLiveCount.op<Support::Max>(raInst->_liveCount);
      }

      if (node == stop)
        break;

      node = node->next();
      ASMJIT_ASSERT(node != nullptr);
    }

    block->_maxLiveCount = maxLiveCount;
    _globalMaxLiveCount.op<Support::Max>(maxLiveCount);
    ASMJIT_ASSERT(position == block->endPosition());
  }

  // Calculate WorkReg statistics
  // ----------------------------

  for (i = 0; i < numWorkRegs; i++) {
    RAWorkReg* workReg = _workRegs[i];

    LiveRegSpans& spans = workReg->liveSpans();
    uint32_t width = spans.width();
    float freq = width ? float(double(workReg->_refs.size()) / double(width)) : float(0);

    RALiveStats& stats = workReg->liveStats();
    stats._width = width;
    stats._freq = freq;
    stats._priority = freq + float(int(workReg->virtReg()->weight())) * 0.01f;
  }

  ASMJIT_RA_LOG_COMPLEX({
    sb.clear();
    _dumpLiveSpans(sb);
    logger->log(sb);
  });

  nInstsPerBlock.release(allocator());

  return kErrorOk;
}

Error BaseRAPass::assignArgIndexToWorkRegs() noexcept {
  ZoneBitVector& liveIn = entryBlock()->liveIn();
  uint32_t argCount = func()->argCount();

  for (uint32_t argIndex = 0; argIndex < argCount; argIndex++) {
    for (uint32_t valueIndex = 0; valueIndex < Globals::kMaxValuePack; valueIndex++) {
      // Unassigned argument.
      const RegOnly& regArg = func()->argPack(argIndex)[valueIndex];
      if (!regArg.isReg() || !cc()->isVirtIdValid(regArg.id()))
        continue;

      VirtReg* virtReg = cc()->virtRegById(regArg.id());
      if (!virtReg)
        continue;

      // Unreferenced argument.
      RAWorkReg* workReg = virtReg->workReg();
      if (!workReg)
        continue;

      // Overwritten argument.
      uint32_t workId = workReg->workId();
      if (!liveIn.bitAt(workId))
        continue;

      workReg->setArgIndex(argIndex, valueIndex);
      const FuncValue& arg = func()->detail().arg(argIndex, valueIndex);

      if (arg.isReg() && _archTraits->regTypeToGroup(arg.regType()) == workReg->group()) {
        workReg->setHintRegId(arg.regId());
      }
    }
  }

  return kErrorOk;
}

// BaseRAPass - Allocation - Global
// ================================

#ifndef ASMJIT_NO_LOGGING
static void RAPass_dumpSpans(String& sb, uint32_t index, const LiveRegSpans& liveSpans) noexcept {
  sb.appendFormat("  %02u: ", index);

  for (uint32_t i = 0; i < liveSpans.size(); i++) {
    const LiveRegSpan& liveSpan = liveSpans[i];
    if (i) sb.append(", ");
    sb.appendFormat("[%u:%u@%u]", liveSpan.a, liveSpan.b, liveSpan.id);
  }

  sb.append('\n');
}
#endif

Error BaseRAPass::runGlobalAllocator() noexcept {
  ASMJIT_PROPAGATE(initGlobalLiveSpans());

  for (RegGroup group : RegGroupVirtValues{}) {
    ASMJIT_PROPAGATE(binPack(group));
  }

  return kErrorOk;
}

ASMJIT_FAVOR_SPEED Error BaseRAPass::initGlobalLiveSpans() noexcept {
  for (RegGroup group : RegGroupVirtValues{}) {
    size_t physCount = _physRegCount[group];
    LiveRegSpans* liveSpans = nullptr;

    if (physCount) {
      liveSpans = allocator()->allocT<LiveRegSpans>(physCount * sizeof(LiveRegSpans));
      if (ASMJIT_UNLIKELY(!liveSpans))
        return DebugUtils::errored(kErrorOutOfMemory);

      for (size_t physId = 0; physId < physCount; physId++)
        new(Support::PlacementNew{&liveSpans[physId]}) LiveRegSpans();
    }

    _globalLiveSpans[group] = liveSpans;
  }

  return kErrorOk;
}

struct RAConsecutiveReg {
  RAWorkReg* workReg;
  RAWorkReg* parentReg;
};

ASMJIT_FAVOR_SPEED Error BaseRAPass::binPack(RegGroup group) noexcept {
  if (workRegCount(group) == 0)
    return kErrorOk;

#ifndef ASMJIT_NO_LOGGING
  Logger* logger = getLoggerIf(DiagnosticOptions::kRADebugAssignment);
  StringTmp<512> sb;

  ASMJIT_RA_LOG_FORMAT("[BinPack] Available=%u (0x%08X) Count=%u RegGroup=%u\n",
    Support::popcnt(_availableRegs[group]),
    _availableRegs[group],
    workRegCount(group),
    uint32_t(group));
#endif

  uint32_t i;
  uint32_t physCount = _physRegCount[group];

  RAWorkRegs workRegs;
  ZoneVector<RAConsecutiveReg> consecutiveRegs;
  LiveRegSpans tmpSpans;

  ASMJIT_PROPAGATE(workRegs.concat(allocator(), this->workRegs(group)));
  workRegs.sort([](const RAWorkReg* a, const RAWorkReg* b) noexcept {
    return b->liveStats().priority() - a->liveStats().priority();
  });

  uint32_t numWorkRegs = workRegs.size();
  RegMask availableRegs = _availableRegs[group];
  RegMask preservedRegs = func()->frame().preservedRegs(group);

  // First try to pack everything that provides register-id hint as these are most likely function arguments and fixed
  // (pre-colored) virtual registers.
  if (!workRegs.empty()) {
    uint32_t dstIndex = 0;

    for (i = 0; i < numWorkRegs; i++) {
      RAWorkReg* workReg = workRegs[i];

      if (workReg->isLeadConsecutive()) {
        ASMJIT_PROPAGATE(consecutiveRegs.append(allocator(), RAConsecutiveReg{workReg, nullptr}));
        workReg->markProcessedConsecutive();
      }

      if (workReg->hasHintRegId()) {
        uint32_t physId = workReg->hintRegId();
        if (Support::bitTest(availableRegs, physId)) {
          LiveRegSpans& live = _globalLiveSpans[group][physId];
          Error err = tmpSpans.nonOverlappingUnionOf(allocator(), live, workReg->liveSpans(), LiveRegData(workReg->virtId()));

          if (err == kErrorOk) {
            live.swap(tmpSpans);
            workReg->setHomeRegId(physId);
            workReg->markAllocated();
            continue;
          }

          if (err != 0xFFFFFFFFu)
            return err;
        }
      }

      workRegs[dstIndex++] = workReg;
    }

    workRegs._setSize(dstIndex);
    numWorkRegs = dstIndex;
  }

  // Allocate consecutive registers - both leads and all consecutives. This is important and prioritized over the rest,
  // because once a lead is allocated we really need to allocate its consecutives, otherwise we may bin pack other
  // registers into their places, which would result in wrong hints to the local allocator, and then into many moves
  // or spills.
  if (!consecutiveRegs.empty()) {
    // This loop appends all other consecutive registers into `consecutiveRegs` array. Leads are at the beginning,
    // non-leads follow.
    i = 0;
    for (;;) {
      uint32_t stop = consecutiveRegs.size();
      if (i == stop)
        break;

      while (i < stop) {
        RAWorkReg* workReg = consecutiveRegs[i].workReg;
        if (workReg->hasImmediateConsecutives()) {
          ZoneBitVector::ForEachBitSet it(workReg->immediateConsecutives());
          while (it.hasNext()) {
            uint32_t consecutiveWorkId = uint32_t(it.next());
            RAWorkReg* consecutiveReg = workRegById(consecutiveWorkId);
            if (!consecutiveReg->isProcessedConsecutive()) {
              ASMJIT_PROPAGATE(consecutiveRegs.append(allocator(), RAConsecutiveReg{consecutiveReg, workReg}));
              consecutiveReg->markProcessedConsecutive();
            }
          }
        }
        i++;
      }
    }

    uint32_t numConsecutiveRegs = consecutiveRegs.size();
    for (i = 0; i < numConsecutiveRegs; i++) {
      RAWorkReg* workReg = consecutiveRegs[i].workReg;
      if (workReg->isAllocated())
        continue;

      RAWorkReg* parentReg = consecutiveRegs[i].parentReg;
      RegMask physRegs = 0;

      if (!parentReg) {
        physRegs = availableRegs & workReg->preferredMask();
        if (!physRegs) {
          physRegs = availableRegs & workReg->consecutiveMask();

          // NOTE: This should never be true as it would mean we would never allocate this virtual register
          // (not here, and not later when local register allocator processes RATiedReg sets).
          if (ASMJIT_UNLIKELY(!physRegs))
            return DebugUtils::errored(kErrorConsecutiveRegsAllocation);
        }
      }
      else if (parentReg->hasHomeRegId()) {
        uint32_t consecutiveId = parentReg->homeRegId() + 1;

        // NOTE: We don't support wrapping. If this goes beyond all allocable registers there is something wrong.
        if (consecutiveId > 31 || !Support::bitTest(availableRegs, consecutiveId))
          return DebugUtils::errored(kErrorConsecutiveRegsAllocation);

        workReg->setHintRegId(consecutiveId);
        physRegs = Support::bitMask(consecutiveId);
      }

      while (physRegs) {
        uint32_t physId = Support::bitSizeOf<RegMask>() - 1 - Support::clz(physRegs);

        LiveRegSpans& live = _globalLiveSpans[group][physId];
        Error err = tmpSpans.nonOverlappingUnionOf(allocator(), live, workReg->liveSpans(), LiveRegData(workReg->virtId()));

        if (err == kErrorOk) {
          workReg->setHomeRegId(physId);
          workReg->markAllocated();
          live.swap(tmpSpans);
          break;
        }

        if (ASMJIT_UNLIKELY(err != 0xFFFFFFFFu))
          return err;

        physRegs ^= Support::bitMask(physId);
      }
    }
  }

  // Try to pack the rest.
  if (!workRegs.empty()) {
    uint32_t dstIndex = 0;

    for (i = 0; i < numWorkRegs; i++) {
      RAWorkReg* workReg = workRegs[i];

      if (workReg->isAllocated())
        continue;

      RegMask remainingPhysRegs = availableRegs;
      if (remainingPhysRegs & workReg->preferredMask())
        remainingPhysRegs &= workReg->preferredMask();

      RegMask physRegs = remainingPhysRegs & ~preservedRegs;
      remainingPhysRegs &= preservedRegs;

      for (;;) {
        if (!physRegs) {
          if (!remainingPhysRegs)
            break;
          physRegs = remainingPhysRegs;
          remainingPhysRegs = 0;
        }

        uint32_t physId = Support::ctz(physRegs);

        if (workReg->clobberSurvivalMask()) {
          RegMask preferredMask = (physRegs | remainingPhysRegs) & workReg->clobberSurvivalMask();
          if (preferredMask) {
            if (preferredMask & ~remainingPhysRegs)
              preferredMask &= ~remainingPhysRegs;
            physId = Support::ctz(preferredMask);
          }
        }

        LiveRegSpans& live = _globalLiveSpans[group][physId];
        Error err = tmpSpans.nonOverlappingUnionOf(allocator(), live, workReg->liveSpans(), LiveRegData(workReg->virtId()));

        if (err == kErrorOk) {
          workReg->setHomeRegId(physId);
          workReg->markAllocated();
          live.swap(tmpSpans);
          break;
        }

        if (ASMJIT_UNLIKELY(err != 0xFFFFFFFFu))
          return err;

        physRegs &= ~Support::bitMask(physId);
        remainingPhysRegs &= ~Support::bitMask(physId);
      }

      // Keep it in `workRegs` if it was not allocated.
      if (!physRegs)
        workRegs[dstIndex++] = workReg;
    }

    workRegs._setSize(dstIndex);
    numWorkRegs = dstIndex;
  }

  ASMJIT_RA_LOG_COMPLEX({
    for (uint32_t physId = 0; physId < physCount; physId++) {
      LiveRegSpans& live = _globalLiveSpans[group][physId];
      if (live.empty())
        continue;

      sb.clear();
      RAPass_dumpSpans(sb, physId, live);
      logger->log(sb);
    }
  });

  // Maybe unused if logging is disabled.
  DebugUtils::unused(physCount);

  if (workRegs.empty()) {
    ASMJIT_RA_LOG_FORMAT("  Completed.\n");
  }
  else {
    _strategy[group].setType(RAStrategyType::kComplex);
    for (RAWorkReg* workReg : workRegs)
      workReg->markStackPreferred();

    ASMJIT_RA_LOG_COMPLEX({
      uint32_t count = workRegs.size();
      sb.clear();
      sb.appendFormat("  Unassigned (%u): ", count);
      for (i = 0; i < numWorkRegs; i++) {
        RAWorkReg* workReg = workRegs[i];
        if (i) sb.append(", ");
        sb.append(workReg->name());
      }
      sb.append('\n');
      logger->log(sb);
    });
  }

  return kErrorOk;
}

// BaseRAPass - Allocation - Local
// ===============================

Error BaseRAPass::runLocalAllocator() noexcept {
  RALocalAllocator lra(this);
  ASMJIT_PROPAGATE(lra.init());

  if (!blockCount())
    return kErrorOk;

  // The allocation is done when this reaches zero.
  uint32_t blocksRemaining = reachableBlockCount();

  // Current block.
  uint32_t blockId = 0;
  RABlock* block = _blocks[blockId];

  // The first block (entry) must always be reachable.
  ASMJIT_ASSERT(block->isReachable());

  // Assign function arguments for the initial block. The `lra` is valid now.
  lra.makeInitialAssignment();
  ASMJIT_PROPAGATE(setBlockEntryAssignment(block, block, lra._curAssignment));

  // The loop starts from the first block and iterates blocks in order, however, the algorithm also allows to jump to
  // any other block when finished if it's a jump target. In-order iteration just makes sure that all blocks are visited.
  for (;;) {
    BaseNode* first = block->first();
    BaseNode* last = block->last();
    BaseNode* terminator = block->hasTerminator() ? last : nullptr;

    BaseNode* beforeFirst = first->prev();
    BaseNode* afterLast = last->next();

    bool unconditionalJump = false;
    RABlock* consecutive = nullptr;

    if (block->hasSuccessors())
      consecutive = block->successors()[0];

    lra.setBlock(block);
    block->makeAllocated();

    BaseNode* node = first;
    while (node != afterLast) {
      BaseNode* next = node->next();
      if (node->isInst()) {
        InstNode* inst = node->as<InstNode>();

        if (ASMJIT_UNLIKELY(inst == terminator)) {
          const RABlocks& successors = block->successors();
          if (block->hasConsecutive()) {
            ASMJIT_PROPAGATE(lra.allocBranch(inst, successors.last(), successors.first()));

            node = next;
            continue;
          }
          else if (successors.size() > 1) {
            RABlock* cont = block->hasConsecutive() ? successors.first() : nullptr;
            ASMJIT_PROPAGATE(lra.allocJumpTable(inst, successors, cont));

            node = next;
            continue;
          }
          else {
            // Otherwise this is an unconditional jump, special handling isn't required.
            unconditionalJump = true;
          }
        }

        ASMJIT_PROPAGATE(lra.allocInst(inst));
        if (inst->type() == NodeType::kInvoke)
          ASMJIT_PROPAGATE(emitPreCall(inst->as<InvokeNode>()));
        else
          ASMJIT_PROPAGATE(lra.spillAfterAllocation(inst));
      }
      node = next;
    }

    if (consecutive) {
      BaseNode* prev = afterLast ? afterLast->prev() : cc()->lastNode();
      cc()->_setCursor(unconditionalJump ? prev->prev() : prev);

      if (consecutive->hasEntryAssignment()) {
        ASMJIT_PROPAGATE(lra.switchToAssignment(consecutive->entryPhysToWorkMap(), consecutive->liveIn(), consecutive->isAllocated(), false));
      }
      else {
        ASMJIT_PROPAGATE(lra.spillRegsBeforeEntry(consecutive));
        ASMJIT_PROPAGATE(setBlockEntryAssignment(consecutive, block, lra._curAssignment));
        lra._curAssignment.copyFrom(consecutive->entryPhysToWorkMap());
      }
    }

    // Important as the local allocator can insert instructions before
    // and after any instruction within the basic block.
    block->setFirst(beforeFirst->next());
    block->setLast(afterLast ? afterLast->prev() : cc()->lastNode());

    if (--blocksRemaining == 0)
      break;

    // Switch to the next consecutive block, if any.
    if (consecutive) {
      block = consecutive;
      if (!block->isAllocated())
        continue;
    }

    // Get the next block.
    for (;;) {
      if (++blockId >= blockCount())
        blockId = 0;

      block = _blocks[blockId];
      if (!block->isReachable() || block->isAllocated() || !block->hasEntryAssignment())
        continue;

      break;
    }

    // If we switched to some block we have to update the local allocator.
    lra.replaceAssignment(block->entryPhysToWorkMap());
  }

  _clobberedRegs.op<Support::Or>(lra._clobberedRegs);
  return kErrorOk;
}

Error BaseRAPass::setBlockEntryAssignment(RABlock* block, const RABlock* fromBlock, const RAAssignment& fromAssignment) noexcept {
  if (block->hasSharedAssignmentId()) {
    uint32_t sharedAssignmentId = block->sharedAssignmentId();

    // Shouldn't happen. Entry assignment of a block that has a shared-state will assign to all blocks
    // with the same sharedAssignmentId. It's a bug if the shared state has been already assigned.
    if (!_sharedAssignments[sharedAssignmentId].empty())
      return DebugUtils::errored(kErrorInvalidState);

    return setSharedAssignment(sharedAssignmentId, fromAssignment);
  }

  PhysToWorkMap* physToWorkMap = clonePhysToWorkMap(fromAssignment.physToWorkMap());
  if (ASMJIT_UNLIKELY(!physToWorkMap))
    return DebugUtils::errored(kErrorOutOfMemory);

  block->setEntryAssignment(physToWorkMap);

  // True if this is the first (entry) block, nothing to do in this case.
  if (block == fromBlock) {
    // Entry block should never have a shared state.
    if (block->hasSharedAssignmentId())
      return DebugUtils::errored(kErrorInvalidState);

    return kErrorOk;
  }

  const ZoneBitVector& liveOut = fromBlock->liveOut();
  const ZoneBitVector& liveIn = block->liveIn();

  // It's possible that `fromBlock` has LIVE-OUT regs that `block` doesn't
  // have in LIVE-IN, these have to be unassigned.
  {
    ZoneBitVector::ForEachBitOp<Support::AndNot> it(liveOut, liveIn);
    while (it.hasNext()) {
      uint32_t workId = uint32_t(it.next());
      RAWorkReg* workReg = workRegById(workId);

      RegGroup group = workReg->group();
      uint32_t physId = fromAssignment.workToPhysId(group, workId);

      if (physId != RAAssignment::kPhysNone)
        physToWorkMap->unassign(group, physId, _physRegIndex.get(group) + physId);
    }
  }

  return blockEntryAssigned(physToWorkMap);
}

Error BaseRAPass::setSharedAssignment(uint32_t sharedAssignmentId, const RAAssignment& fromAssignment) noexcept {
  ASMJIT_ASSERT(_sharedAssignments[sharedAssignmentId].empty());

  PhysToWorkMap* physToWorkMap = clonePhysToWorkMap(fromAssignment.physToWorkMap());
  if (ASMJIT_UNLIKELY(!physToWorkMap))
    return DebugUtils::errored(kErrorOutOfMemory);

  _sharedAssignments[sharedAssignmentId].assignPhysToWorkMap(physToWorkMap);

  ZoneBitVector& sharedLiveIn = _sharedAssignments[sharedAssignmentId]._liveIn;
  ASMJIT_PROPAGATE(sharedLiveIn.resize(allocator(), workRegCount()));

  Support::Array<uint32_t, Globals::kNumVirtGroups> sharedAssigned {};
  for (RABlock* block : blocks()) {
    if (block->sharedAssignmentId() == sharedAssignmentId) {
      ASMJIT_ASSERT(!block->hasEntryAssignment());

      PhysToWorkMap* entryPhysToWorkMap = clonePhysToWorkMap(fromAssignment.physToWorkMap());
      if (ASMJIT_UNLIKELY(!entryPhysToWorkMap))
        return DebugUtils::errored(kErrorOutOfMemory);

      block->setEntryAssignment(entryPhysToWorkMap);

      const ZoneBitVector& liveIn = block->liveIn();
      sharedLiveIn.or_(liveIn);

      for (RegGroup group : RegGroupVirtValues{}) {
        sharedAssigned[group] |= entryPhysToWorkMap->assigned[group];

        uint32_t physBaseIndex = _physRegIndex.get(group);
        Support::BitWordIterator<RegMask> it(entryPhysToWorkMap->assigned[group]);

        while (it.hasNext()) {
          uint32_t physId = it.next();
          uint32_t workId = entryPhysToWorkMap->workIds[physBaseIndex + physId];

          if (!liveIn.bitAt(workId))
            entryPhysToWorkMap->unassign(group, physId, physBaseIndex + physId);
        }
      }
    }
  }

  for (RegGroup group : RegGroupVirtValues{}) {
    uint32_t physBaseIndex = _physRegIndex.get(group);
    Support::BitWordIterator<RegMask> it(_availableRegs[group] & ~sharedAssigned[group]);

    while (it.hasNext()) {
      uint32_t physId = it.next();
      if (Support::bitTest(physToWorkMap->assigned[group], physId))
        physToWorkMap->unassign(group, physId, physBaseIndex + physId);
    }
  }

  return blockEntryAssigned(physToWorkMap);
}

Error BaseRAPass::blockEntryAssigned(const PhysToWorkMap* physToWorkMap) noexcept {
  // Complex allocation strategy requires to record register assignments upon block entry (or per shared state).
  for (RegGroup group : RegGroupVirtValues{}) {
    if (!_strategy[group].isComplex())
      continue;

    uint32_t physBaseIndex = _physRegIndex[group];
    Support::BitWordIterator<RegMask> it(physToWorkMap->assigned[group]);

    while (it.hasNext()) {
      uint32_t physId = it.next();
      uint32_t workId = physToWorkMap->workIds[physBaseIndex + physId];

      RAWorkReg* workReg = workRegById(workId);
      workReg->addAllocatedMask(Support::bitMask(physId));
    }
  }

  return kErrorOk;
}

// BaseRAPass - Allocation - Utilities
// ===================================

Error BaseRAPass::useTemporaryMem(BaseMem& out, uint32_t size, uint32_t alignment) noexcept {
  ASMJIT_ASSERT(alignment <= 64);

  if (_temporaryMem.isNone()) {
    ASMJIT_PROPAGATE(cc()->_newStack(&_temporaryMem.as<BaseMem>(), size, alignment));
  }
  else {
    ASMJIT_ASSERT(_temporaryMem.as<BaseMem>().isRegHome());

    uint32_t virtId = _temporaryMem.as<BaseMem>().baseId();
    VirtReg* virtReg = cc()->virtRegById(virtId);

    cc()->setStackSize(virtId, Support::max(virtReg->virtSize(), size),
                               Support::max(virtReg->alignment(), alignment));
  }

  out = _temporaryMem.as<BaseMem>();
  return kErrorOk;
}

// BaseRAPass - Allocation - Prolog & Epilog
// =========================================

Error BaseRAPass::updateStackFrame() noexcept {
  // Update some StackFrame information that we updated during allocation. The only information we don't have at the
  // moment is final local stack size, which is calculated last.
  FuncFrame& frame = func()->frame();
  for (RegGroup group : RegGroupVirtValues{})
    frame.addDirtyRegs(group, _clobberedRegs[group]);
  frame.setLocalStackAlignment(_stackAllocator.alignment());

  // If there are stack arguments that are not assigned to registers upon entry and the function doesn't require
  // dynamic stack alignment we keep these arguments where they are. This will also mark all stack slots that match
  // these arguments as allocated.
  if (_numStackArgsToStackSlots)
    ASMJIT_PROPAGATE(_markStackArgsToKeep());

  // Calculate offsets of all stack slots and update StackSize to reflect the calculated local stack size.
  ASMJIT_PROPAGATE(_stackAllocator.calculateStackFrame());
  frame.setLocalStackSize(_stackAllocator.stackSize());

  // Update the stack frame based on `_argsAssignment` and finalize it. Finalization means to apply final calculation
  // to the stack layout.
  ASMJIT_PROPAGATE(_argsAssignment.updateFuncFrame(frame));
  ASMJIT_PROPAGATE(frame.finalize());

  // StackAllocator allocates all stots starting from [0], adjust them when necessary.
  if (frame.localStackOffset() != 0)
    ASMJIT_PROPAGATE(_stackAllocator.adjustSlotOffsets(int32_t(frame.localStackOffset())));

  // Again, if there are stack arguments allocated in function's stack we have to handle them. This handles all cases
  // (either regular or dynamic stack alignment).
  if (_numStackArgsToStackSlots)
    ASMJIT_PROPAGATE(_updateStackArgs());

  return kErrorOk;
}

Error BaseRAPass::_markStackArgsToKeep() noexcept {
  FuncFrame& frame = func()->frame();
  bool hasSAReg = frame.hasPreservedFP() || !frame.hasDynamicAlignment();

  RAWorkRegs& workRegs = _workRegs;
  uint32_t numWorkRegs = workRegCount();

  for (uint32_t workId = 0; workId < numWorkRegs; workId++) {
    RAWorkReg* workReg = workRegs[workId];
    if (workReg->hasFlag(RAWorkRegFlags::kStackArgToStack)) {
      ASMJIT_ASSERT(workReg->hasArgIndex());
      const FuncValue& srcArg = _func->detail().arg(workReg->argIndex());

      // If the register doesn't have stack slot then we failed. It doesn't make much sense as it was marked as
      // `kFlagStackArgToStack`, which requires the WorkReg was live-in upon function entry.
      RAStackSlot* slot = workReg->stackSlot();
      if (ASMJIT_UNLIKELY(!slot))
        return DebugUtils::errored(kErrorInvalidState);

      if (hasSAReg && srcArg.isStack() && !srcArg.isIndirect()) {
        uint32_t typeSize = TypeUtils::sizeOf(srcArg.typeId());
        if (typeSize == slot->size()) {
          slot->addFlags(RAStackSlot::kFlagStackArg);
          continue;
        }
      }

      // NOTE: Update StackOffset here so when `_argsAssignment.updateFuncFrame()` is called it will take into
      // consideration moving to stack slots. Without this we may miss some scratch registers later.
      FuncValue& dstArg = _argsAssignment.arg(workReg->argIndex(), workReg->argValueIndex());
      dstArg.assignStackOffset(0);
    }
  }

  return kErrorOk;
}

Error BaseRAPass::_updateStackArgs() noexcept {
  FuncFrame& frame = func()->frame();
  RAWorkRegs& workRegs = _workRegs;
  uint32_t numWorkRegs = workRegCount();

  for (uint32_t workId = 0; workId < numWorkRegs; workId++) {
    RAWorkReg* workReg = workRegs[workId];
    if (workReg->hasFlag(RAWorkRegFlags::kStackArgToStack)) {
      ASMJIT_ASSERT(workReg->hasArgIndex());
      RAStackSlot* slot = workReg->stackSlot();

      if (ASMJIT_UNLIKELY(!slot))
        return DebugUtils::errored(kErrorInvalidState);

      if (slot->isStackArg()) {
        const FuncValue& srcArg = _func->detail().arg(workReg->argIndex());
        if (frame.hasPreservedFP()) {
          slot->setBaseRegId(_fp.id());
          slot->setOffset(int32_t(frame.saOffsetFromSA()) + srcArg.stackOffset());
        }
        else {
          slot->setOffset(int32_t(frame.saOffsetFromSP()) + srcArg.stackOffset());
        }
      }
      else {
        FuncValue& dstArg = _argsAssignment.arg(workReg->argIndex(), workReg->argValueIndex());
        dstArg.setStackOffset(slot->offset());
      }
    }
  }

  return kErrorOk;
}

Error BaseRAPass::insertPrologEpilog() noexcept {
  FuncFrame& frame = _func->frame();

  cc()->_setCursor(func());
  ASMJIT_PROPAGATE(cc()->emitProlog(frame));
  ASMJIT_PROPAGATE(_iEmitHelper->emitArgsAssignment(frame, _argsAssignment));

  cc()->_setCursor(func()->exitNode());
  ASMJIT_PROPAGATE(cc()->emitEpilog(frame));

  return kErrorOk;
}

// BaseRAPass - Rewriter
// =====================

Error BaseRAPass::rewrite() noexcept {
  return _rewrite(_func, _stop);
}

// [[pure virtual]]
Error BaseRAPass::_rewrite(BaseNode* first, BaseNode* stop) noexcept {
  DebugUtils::unused(first, stop);
  return DebugUtils::errored(kErrorInvalidState);
}

// BaseRAPass - Emit
// =================

// [[pure virtual]]
Error BaseRAPass::emitMove(uint32_t workId, uint32_t dstPhysId, uint32_t srcPhysId) noexcept {
  DebugUtils::unused(workId, dstPhysId, srcPhysId);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseRAPass::emitSwap(uint32_t aWorkId, uint32_t aPhysId, uint32_t bWorkId, uint32_t bPhysId) noexcept {
  DebugUtils::unused(aWorkId, aPhysId, bWorkId, bPhysId);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseRAPass::emitLoad(uint32_t workId, uint32_t dstPhysId) noexcept {
  DebugUtils::unused(workId, dstPhysId);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseRAPass::emitSave(uint32_t workId, uint32_t srcPhysId) noexcept {
  DebugUtils::unused(workId, srcPhysId);
  return DebugUtils::errored(kErrorInvalidState);
}

// [[pure virtual]]
Error BaseRAPass::emitJump(const Label& label) noexcept {
  DebugUtils::unused(label);
  return DebugUtils::errored(kErrorInvalidState);
}

Error BaseRAPass::emitPreCall(InvokeNode* invokeNode) noexcept {
  DebugUtils::unused(invokeNode);
  return DebugUtils::errored(kErrorOk);
}

// BaseRAPass - Logging
// ====================

#ifndef ASMJIT_NO_LOGGING
static void RAPass_formatLiveness(BaseRAPass* pass, String& sb, const RAInst* raInst) noexcept {
  const RATiedReg* tiedRegs = raInst->tiedRegs();
  uint32_t tiedCount = raInst->tiedCount();

  for (uint32_t i = 0; i < tiedCount; i++) {
    const RATiedReg& tiedReg = tiedRegs[i];

    if (i != 0)
      sb.append(' ');

    sb.appendFormat("%s{", pass->workRegById(tiedReg.workId())->name());
    sb.append(tiedReg.isReadWrite() ? 'X' :
              tiedReg.isRead()      ? 'R' :
              tiedReg.isWrite()     ? 'W' : '?');

    if (tiedReg.isLeadConsecutive())
      sb.appendFormat("|Lead[%u]", tiedReg.consecutiveData() + 1u);

    if (tiedReg.hasUseId())
      sb.appendFormat("|Use=%u", tiedReg.useId());
    else if (tiedReg.isUse())
      sb.append("|Use");

    if (tiedReg.isUseConsecutive() && !tiedReg.isLeadConsecutive())
      sb.appendFormat("+%u", tiedReg.consecutiveData());

    if (tiedReg.hasOutId())
      sb.appendFormat("|Out=%u", tiedReg.outId());
    else if (tiedReg.isOut())
      sb.append("|Out");

    if (tiedReg.isOutConsecutive() && !tiedReg.isLeadConsecutive())
      sb.appendFormat("+%u", tiedReg.consecutiveData());

    if (tiedReg.isLast())
      sb.append("|Last");

    if (tiedReg.isKill())
      sb.append("|Kill");

    sb.append("}");
  }
}

ASMJIT_FAVOR_SIZE Error BaseRAPass::annotateCode() noexcept {
  StringTmp<1024> sb;

  for (const RABlock* block : _blocks) {
    BaseNode* node = block->first();
    if (!node) continue;

    BaseNode* last = block->last();
    for (;;) {
      sb.clear();
      Formatter::formatNode(sb, _formatOptions, cc(), node);

      if (hasDiagnosticOption(DiagnosticOptions::kRADebugLiveness) && node->isInst() && node->hasPassData()) {
        const RAInst* raInst = node->passData<RAInst>();
        if (raInst->tiedCount() > 0) {
          sb.padEnd(40);
          sb.append(" | ");
          RAPass_formatLiveness(this, sb, raInst);
        }
      }

      node->setInlineComment(static_cast<char*>(cc()->_dataZone.dup(sb.data(), sb.size(), true)));
      if (node == last)
        break;
      node = node->next();
    }
  }

  return kErrorOk;
}

ASMJIT_FAVOR_SIZE Error BaseRAPass::_dumpBlockIds(String& sb, const RABlocks& blocks) noexcept {
  for (uint32_t i = 0, size = blocks.size(); i < size; i++) {
    const RABlock* block = blocks[i];
    if (i != 0)
      ASMJIT_PROPAGATE(sb.appendFormat(", #%u", block->blockId()));
    else
      ASMJIT_PROPAGATE(sb.appendFormat("#%u", block->blockId()));
  }
  return kErrorOk;
}

ASMJIT_FAVOR_SIZE Error BaseRAPass::_dumpBlockLiveness(String& sb, const RABlock* block) noexcept {
  for (uint32_t liveType = 0; liveType < RABlock::kLiveCount; liveType++) {
    const char* bitsName = liveType == RABlock::kLiveIn  ? "IN  " :
                           liveType == RABlock::kLiveOut ? "OUT " :
                           liveType == RABlock::kLiveGen ? "GEN " : "KILL";

    const ZoneBitVector& bits = block->_liveBits[liveType];
    uint32_t size = bits.size();
    ASMJIT_ASSERT(size <= workRegCount());

    uint32_t n = 0;
    for (uint32_t workId = 0; workId < size; workId++) {
      if (bits.bitAt(workId)) {
        RAWorkReg* wReg = workRegById(workId);

        if (!n)
          sb.appendFormat("    %s [", bitsName);
        else
          sb.append(", ");

        sb.append(wReg->name());
        n++;
      }
    }

    if (n)
      sb.append("]\n");
  }

  return kErrorOk;
}

ASMJIT_FAVOR_SIZE Error BaseRAPass::_dumpLiveSpans(String& sb) noexcept {
  uint32_t numWorkRegs = _workRegs.size();
  uint32_t maxSize = _maxWorkRegNameSize;

  for (uint32_t workId = 0; workId < numWorkRegs; workId++) {
    RAWorkReg* workReg = _workRegs[workId];

    sb.append("  ");

    size_t oldSize = sb.size();
    sb.append(workReg->name());
    sb.padEnd(oldSize + maxSize);

    RALiveStats& stats = workReg->liveStats();
    sb.appendFormat(" {id:%04u width: %-4u freq: %0.4f priority=%0.4f}",
      workReg->virtId(),
      stats.width(),
      stats.freq(),
      stats.priority());
    sb.append(": ");

    LiveRegSpans& liveSpans = workReg->liveSpans();
    for (uint32_t x = 0; x < liveSpans.size(); x++) {
      const LiveRegSpan& liveSpan = liveSpans[x];
      if (x)
        sb.append(", ");
      sb.appendFormat("[%u:%u]", liveSpan.a, liveSpan.b);
    }

    sb.append('\n');
  }

  return kErrorOk;
}
#endif

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER

```

`CodeCleaner/asmjit/asmjit/core/rapass_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_RAPASS_P_H_INCLUDED
#define ASMJIT_CORE_RAPASS_P_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/compiler.h"
#include "../core/emithelper_p.h"
#include "../core/raassignment_p.h"
#include "../core/radefs_p.h"
#include "../core/rastack_p.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_ra
//! \{

//! Flags used by \ref RABlock.
enum class RABlockFlags : uint32_t {
  //! No flags.
  kNone = 0,

  //! Block has been constructed from nodes.
  kIsConstructed = 0x00000001u,
  //! Block is reachable (set by `buildCFGViews()`).
  kIsReachable = 0x00000002u,
  //! Block is a target (has an associated label or multiple labels).
  kIsTargetable = 0x00000004u,
  //! Block has been allocated.
  kIsAllocated = 0x00000008u,
  //! Block is a function-exit.
  kIsFuncExit = 0x00000010u,

  //! Block has a terminator (jump, conditional jump, ret).
  kHasTerminator = 0x00000100u,
  //! Block naturally flows to the next block.
  kHasConsecutive = 0x00000200u,
  //! Block has a jump to a jump-table at the end.
  kHasJumpTable = 0x00000400u,
  //! Block contains fixed registers (pre-colored).
  kHasFixedRegs = 0x00000800u,
  //! Block contains function calls.
  kHasFuncCalls = 0x00001000u
};
ASMJIT_DEFINE_ENUM_FLAGS(RABlockFlags)

//! Basic block used by register allocator pass.
class RABlock {
public:
  ASMJIT_NONCOPYABLE(RABlock)

  typedef RAAssignment::PhysToWorkMap PhysToWorkMap;
  typedef RAAssignment::WorkToPhysMap WorkToPhysMap;

  //! \name Constants
  //! \{

  enum : uint32_t {
    //! Unassigned block id.
    kUnassignedId = 0xFFFFFFFFu
  };

  enum LiveType : uint32_t {
    kLiveIn = 0,
    kLiveOut = 1,
    kLiveGen = 2,
    kLiveKill = 3,
    kLiveCount = 4
  };

  //! \}

  //! \name Members
  //! \{

  //! Register allocator pass.
  BaseRAPass* _ra;

  //! Block id (indexed from zero).
  uint32_t _blockId = kUnassignedId;
  //! Block flags, see `Flags`.
  RABlockFlags _flags = RABlockFlags::kNone;

  //! First `BaseNode` of this block (inclusive).
  BaseNode* _first = nullptr;
  //! Last `BaseNode` of this block (inclusive).
  BaseNode* _last = nullptr;

  //! Initial position of this block (inclusive).
  uint32_t _firstPosition = 0;
  //! End position of this block (exclusive).
  uint32_t _endPosition = 0;

  //! Weight of this block (default 0, each loop adds one).
  uint32_t _weight = 0;
  //! Post-order view order, used during POV construction.
  uint32_t _povOrder = 0;

  //! Basic statistics about registers.
  RARegsStats _regsStats = RARegsStats();
  //! Maximum live-count per register group.
  RALiveCount _maxLiveCount = RALiveCount();

  //! Timestamp (used by block visitors).
  mutable uint64_t _timestamp = 0;
  //! Immediate dominator of this block.
  RABlock* _idom = nullptr;

  //! Block predecessors.
  RABlocks _predecessors {};
  //! Block successors.
  RABlocks _successors {};

  //! Liveness in/out/use/kill.
  ZoneBitVector _liveBits[kLiveCount] {};

  //! Shared assignment it or `Globals::kInvalidId` if this block doesn't have shared assignment.
  //! See \ref RASharedAssignment for more details.
  uint32_t _sharedAssignmentId = Globals::kInvalidId;
  //! Scratch registers that cannot be allocated upon block entry.
  RegMask _entryScratchGpRegs = 0;
  //! Scratch registers used at exit, by a terminator instruction.
  RegMask _exitScratchGpRegs = 0;

  //! Register assignment on entry.
  PhysToWorkMap* _entryPhysToWorkMap = nullptr;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG RABlock(BaseRAPass* ra) noexcept
    : _ra(ra) {}

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG BaseRAPass* pass() const noexcept { return _ra; }
  ASMJIT_INLINE_NODEBUG ZoneAllocator* allocator() const noexcept;

  ASMJIT_INLINE_NODEBUG uint32_t blockId() const noexcept { return _blockId; }
  ASMJIT_INLINE_NODEBUG RABlockFlags flags() const noexcept { return _flags; }

  ASMJIT_INLINE_NODEBUG bool hasFlag(RABlockFlags flag) const noexcept { return Support::test(_flags, flag); }
  ASMJIT_INLINE_NODEBUG void addFlags(RABlockFlags flags) noexcept { _flags |= flags; }

  ASMJIT_INLINE_NODEBUG bool isAssigned() const noexcept { return _blockId != kUnassignedId; }

  ASMJIT_INLINE_NODEBUG bool isConstructed() const noexcept { return hasFlag(RABlockFlags::kIsConstructed); }
  ASMJIT_INLINE_NODEBUG bool isReachable() const noexcept { return hasFlag(RABlockFlags::kIsReachable); }
  ASMJIT_INLINE_NODEBUG bool isTargetable() const noexcept { return hasFlag(RABlockFlags::kIsTargetable); }
  ASMJIT_INLINE_NODEBUG bool isAllocated() const noexcept { return hasFlag(RABlockFlags::kIsAllocated); }
  ASMJIT_INLINE_NODEBUG bool isFuncExit() const noexcept { return hasFlag(RABlockFlags::kIsFuncExit); }
  ASMJIT_INLINE_NODEBUG bool hasTerminator() const noexcept { return hasFlag(RABlockFlags::kHasTerminator); }
  ASMJIT_INLINE_NODEBUG bool hasConsecutive() const noexcept { return hasFlag(RABlockFlags::kHasConsecutive); }
  ASMJIT_INLINE_NODEBUG bool hasJumpTable() const noexcept { return hasFlag(RABlockFlags::kHasJumpTable); }

  ASMJIT_INLINE_NODEBUG void makeConstructed(const RARegsStats& regStats) noexcept {
    _flags |= RABlockFlags::kIsConstructed;
    _regsStats.combineWith(regStats);
  }

  ASMJIT_INLINE_NODEBUG void makeReachable() noexcept { _flags |= RABlockFlags::kIsReachable; }
  ASMJIT_INLINE_NODEBUG void makeTargetable() noexcept { _flags |= RABlockFlags::kIsTargetable; }
  ASMJIT_INLINE_NODEBUG void makeAllocated() noexcept { _flags |= RABlockFlags::kIsAllocated; }

  ASMJIT_INLINE_NODEBUG const RARegsStats& regsStats() const noexcept { return _regsStats; }

  ASMJIT_INLINE_NODEBUG bool hasPredecessors() const noexcept { return !_predecessors.empty(); }
  ASMJIT_INLINE_NODEBUG bool hasSuccessors() const noexcept { return !_successors.empty(); }

  ASMJIT_INLINE_NODEBUG bool hasSuccessor(RABlock* block) noexcept {
    if (block->_predecessors.size() < _successors.size())
      return block->_predecessors.contains(this);
    else
      return _successors.contains(block);
  }

  ASMJIT_INLINE_NODEBUG const RABlocks& predecessors() const noexcept { return _predecessors; }
  ASMJIT_INLINE_NODEBUG const RABlocks& successors() const noexcept { return _successors; }

  ASMJIT_INLINE_NODEBUG BaseNode* first() const noexcept { return _first; }
  ASMJIT_INLINE_NODEBUG BaseNode* last() const noexcept { return _last; }

  ASMJIT_INLINE_NODEBUG void setFirst(BaseNode* node) noexcept { _first = node; }
  ASMJIT_INLINE_NODEBUG void setLast(BaseNode* node) noexcept { _last = node; }

  ASMJIT_INLINE_NODEBUG uint32_t firstPosition() const noexcept { return _firstPosition; }
  ASMJIT_INLINE_NODEBUG void setFirstPosition(uint32_t position) noexcept { _firstPosition = position; }

  ASMJIT_INLINE_NODEBUG uint32_t endPosition() const noexcept { return _endPosition; }
  ASMJIT_INLINE_NODEBUG void setEndPosition(uint32_t position) noexcept { _endPosition = position; }

  ASMJIT_INLINE_NODEBUG uint32_t povOrder() const noexcept { return _povOrder; }

  ASMJIT_INLINE_NODEBUG RegMask entryScratchGpRegs() const noexcept;
  ASMJIT_INLINE_NODEBUG RegMask exitScratchGpRegs() const noexcept { return _exitScratchGpRegs; }

  ASMJIT_INLINE_NODEBUG void addEntryScratchGpRegs(RegMask regMask) noexcept { _entryScratchGpRegs |= regMask; }
  ASMJIT_INLINE_NODEBUG void addExitScratchGpRegs(RegMask regMask) noexcept { _exitScratchGpRegs |= regMask; }

  ASMJIT_INLINE_NODEBUG bool hasSharedAssignmentId() const noexcept { return _sharedAssignmentId != Globals::kInvalidId; }
  ASMJIT_INLINE_NODEBUG uint32_t sharedAssignmentId() const noexcept { return _sharedAssignmentId; }
  ASMJIT_INLINE_NODEBUG void setSharedAssignmentId(uint32_t id) noexcept { _sharedAssignmentId = id; }

  ASMJIT_INLINE_NODEBUG uint64_t timestamp() const noexcept { return _timestamp; }
  ASMJIT_INLINE_NODEBUG bool hasTimestamp(uint64_t ts) const noexcept { return _timestamp == ts; }
  ASMJIT_INLINE_NODEBUG void setTimestamp(uint64_t ts) const noexcept { _timestamp = ts; }
  ASMJIT_INLINE_NODEBUG void resetTimestamp() const noexcept { _timestamp = 0; }

  ASMJIT_INLINE_NODEBUG RABlock* consecutive() const noexcept { return hasConsecutive() ? _successors[0] : nullptr; }

  ASMJIT_INLINE_NODEBUG RABlock* iDom() noexcept { return _idom; }
  ASMJIT_INLINE_NODEBUG const RABlock* iDom() const noexcept { return _idom; }
  ASMJIT_INLINE_NODEBUG void setIDom(RABlock* block) noexcept { _idom = block; }

  ASMJIT_INLINE_NODEBUG ZoneBitVector& liveIn() noexcept { return _liveBits[kLiveIn]; }
  ASMJIT_INLINE_NODEBUG const ZoneBitVector& liveIn() const noexcept { return _liveBits[kLiveIn]; }

  ASMJIT_INLINE_NODEBUG ZoneBitVector& liveOut() noexcept { return _liveBits[kLiveOut]; }
  ASMJIT_INLINE_NODEBUG const ZoneBitVector& liveOut() const noexcept { return _liveBits[kLiveOut]; }

  ASMJIT_INLINE_NODEBUG ZoneBitVector& gen() noexcept { return _liveBits[kLiveGen]; }
  ASMJIT_INLINE_NODEBUG const ZoneBitVector& gen() const noexcept { return _liveBits[kLiveGen]; }

  ASMJIT_INLINE_NODEBUG ZoneBitVector& kill() noexcept { return _liveBits[kLiveKill]; }
  ASMJIT_INLINE_NODEBUG const ZoneBitVector& kill() const noexcept { return _liveBits[kLiveKill]; }

  inline Error resizeLiveBits(uint32_t size) noexcept {
    ASMJIT_PROPAGATE(_liveBits[kLiveIn  ].resize(allocator(), size));
    ASMJIT_PROPAGATE(_liveBits[kLiveOut ].resize(allocator(), size));
    ASMJIT_PROPAGATE(_liveBits[kLiveGen ].resize(allocator(), size));
    ASMJIT_PROPAGATE(_liveBits[kLiveKill].resize(allocator(), size));
    return kErrorOk;
  }

  ASMJIT_INLINE_NODEBUG bool hasEntryAssignment() const noexcept { return _entryPhysToWorkMap != nullptr; }
  ASMJIT_INLINE_NODEBUG PhysToWorkMap* entryPhysToWorkMap() const noexcept { return _entryPhysToWorkMap; }
  ASMJIT_INLINE_NODEBUG void setEntryAssignment(PhysToWorkMap* physToWorkMap) noexcept { _entryPhysToWorkMap = physToWorkMap; }

  //! \}

  //! \name Utilities
  //! \{

  //! Adds a successor to this block, and predecessor to `successor`, making connection on both sides.
  //!
  //! This API must be used to manage successors and predecessors, never manage it manually.
  Error appendSuccessor(RABlock* successor) noexcept;

  //! Similar to `appendSuccessor()`, but does prepend instead append.
  //!
  //! This function is used to add a natural flow (always first) to the block.
  Error prependSuccessor(RABlock* successor) noexcept;

  //! \}
};

//! Register allocator's data associated with each `InstNode`.
class RAInst {
public:
  ASMJIT_NONCOPYABLE(RAInst)

  //! \name Members
  //! \{

  //! Parent block.
  RABlock* _block;
  //! Instruction RW flags.
  InstRWFlags _instRWFlags;
  //! Aggregated RATiedFlags from all operands & instruction specific flags.
  RATiedFlags _flags;
  //! Total count of RATiedReg's.
  uint32_t _tiedTotal;
  //! Index of RATiedReg's per register group.
  RARegIndex _tiedIndex;
  //! Count of RATiedReg's per register group.
  RARegCount _tiedCount;
  //! Number of live, and thus interfering VirtReg's at this point.
  RALiveCount _liveCount;
  //! Fixed physical registers used.
  RARegMask _usedRegs;
  //! Clobbered registers (by a function call).
  RARegMask _clobberedRegs;
  //! Tied registers.
  RATiedReg _tiedRegs[1];

  //! \}

  //! \name Construction & Destruction
  //! \{

  inline RAInst(RABlock* block, InstRWFlags instRWFlags, RATiedFlags tiedFlags, uint32_t tiedTotal, const RARegMask& clobberedRegs) noexcept {
    _block = block;
    _instRWFlags = instRWFlags;
    _flags = tiedFlags;
    _tiedTotal = tiedTotal;
    _tiedIndex.reset();
    _tiedCount.reset();
    _liveCount.reset();
    _usedRegs.reset();
    _clobberedRegs = clobberedRegs;
  }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns instruction RW flags.
  ASMJIT_INLINE_NODEBUG InstRWFlags instRWFlags() const noexcept { return _instRWFlags; };
  //! Tests whether the given `flag` is present in instruction RW flags.
  ASMJIT_INLINE_NODEBUG bool hasInstRWFlag(InstRWFlags flag) const noexcept { return Support::test(_instRWFlags, flag); }
  //! Adds `flags` to instruction RW flags.
  ASMJIT_INLINE_NODEBUG void addInstRWFlags(InstRWFlags flags) noexcept { _instRWFlags |= flags; }

  //! Returns the instruction flags.
  ASMJIT_INLINE_NODEBUG RATiedFlags flags() const noexcept { return _flags; }
  //! Tests whether the instruction has flag `flag`.
  ASMJIT_INLINE_NODEBUG bool hasFlag(RATiedFlags flag) const noexcept { return Support::test(_flags, flag); }
  //! Replaces the existing instruction flags with `flags`.
  ASMJIT_INLINE_NODEBUG void setFlags(RATiedFlags flags) noexcept { _flags = flags; }
  //! Adds instruction `flags` to this RAInst.
  ASMJIT_INLINE_NODEBUG void addFlags(RATiedFlags flags) noexcept { _flags |= flags; }
  //! Clears instruction `flags` from  this RAInst.
  ASMJIT_INLINE_NODEBUG void clearFlags(RATiedFlags flags) noexcept { _flags &= ~flags; }

  //! Tests whether one operand of this instruction has been patched from Reg to Mem.
  ASMJIT_INLINE_NODEBUG bool isRegToMemPatched() const noexcept { return hasFlag(RATiedFlags::kInst_RegToMemPatched); }
  //! Tests whether this instruction can be transformed to another instruction if necessary.
  ASMJIT_INLINE_NODEBUG bool isTransformable() const noexcept { return hasFlag(RATiedFlags::kInst_IsTransformable); }

  //! Returns the associated block with this RAInst.
  ASMJIT_INLINE_NODEBUG RABlock* block() const noexcept { return _block; }

  //! Returns tied registers (all).
  ASMJIT_INLINE_NODEBUG RATiedReg* tiedRegs() const noexcept { return const_cast<RATiedReg*>(_tiedRegs); }
  //! Returns tied registers for a given `group`.
  ASMJIT_INLINE_NODEBUG RATiedReg* tiedRegs(RegGroup group) const noexcept { return const_cast<RATiedReg*>(_tiedRegs) + _tiedIndex.get(group); }

  //! Returns count of all tied registers.
  ASMJIT_INLINE_NODEBUG uint32_t tiedCount() const noexcept { return _tiedTotal; }
  //! Returns count of tied registers of a given `group`.
  ASMJIT_INLINE_NODEBUG uint32_t tiedCount(RegGroup group) const noexcept { return _tiedCount[group]; }

  //! Returns `RATiedReg` at the given `index`.
  inline RATiedReg* tiedAt(uint32_t index) const noexcept {
    ASMJIT_ASSERT(index < _tiedTotal);
    return tiedRegs() + index;
  }

  //! Returns `RATiedReg` at the given `index` of the given register `group`.
  inline RATiedReg* tiedOf(RegGroup group, uint32_t index) const noexcept {
    ASMJIT_ASSERT(index < _tiedCount.get(group));
    return tiedRegs(group) + index;
  }

  inline const RATiedReg* tiedRegForWorkReg(RegGroup group, uint32_t workId) const noexcept {
    const RATiedReg* array = tiedRegs(group);
    size_t count = tiedCount(group);

    for (size_t i = 0; i < count; i++) {
      const RATiedReg* tiedReg = &array[i];
      if (tiedReg->workId() == workId) {
        return tiedReg;
      }
    }

    return nullptr;
  }

  inline void setTiedAt(uint32_t index, RATiedReg& tied) noexcept {
    ASMJIT_ASSERT(index < _tiedTotal);
    _tiedRegs[index] = tied;
  }

  //! \name Static Functions
  //! \{

  static ASMJIT_INLINE_NODEBUG size_t sizeOf(uint32_t tiedRegCount) noexcept {
    return sizeof(RAInst) - sizeof(RATiedReg) + tiedRegCount * sizeof(RATiedReg);
  }

  //! \}
};

//! A helper class that is used to build an array of RATiedReg items that are then copied to `RAInst`.
class RAInstBuilder {
public:
  ASMJIT_NONCOPYABLE(RAInstBuilder)

  //! \name Members
  //! \{

  //! Basic block id.
  uint32_t _basicBlockId;
  //! Instruction RW flags.
  InstRWFlags _instRWFlags;

  //! Flags combined from all RATiedReg's.
  RATiedFlags _aggregatedFlags;
  //! Flags that will be cleared before storing the aggregated flags to `RAInst`.
  RATiedFlags _forbiddenFlags;
  RARegCount _count;
  RARegsStats _stats;

  RARegMask _used;
  RARegMask _clobbered;

  //! Current tied register in `_tiedRegs`.
  RATiedReg* _cur;
  //! Array of temporary tied registers.
  RATiedReg _tiedRegs[128];

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG explicit RAInstBuilder(uint32_t blockId = Globals::kInvalidId) noexcept { reset(blockId); }

  ASMJIT_INLINE_NODEBUG void init(uint32_t blockId) noexcept { reset(blockId); }
  ASMJIT_INLINE_NODEBUG void reset(uint32_t blockId) noexcept {
    _basicBlockId = blockId;
    _instRWFlags = InstRWFlags::kNone;
    _aggregatedFlags = RATiedFlags::kNone;
    _forbiddenFlags = RATiedFlags::kNone;
    _count.reset();
    _stats.reset();
    _used.reset();
    _clobbered.reset();
    _cur = _tiedRegs;
  }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG InstRWFlags instRWFlags() const noexcept { return _instRWFlags; }
  ASMJIT_INLINE_NODEBUG bool hasInstRWFlag(InstRWFlags flag) const noexcept { return Support::test(_instRWFlags, flag); }
  ASMJIT_INLINE_NODEBUG void addInstRWFlags(InstRWFlags flags) noexcept { _instRWFlags |= flags; }
  ASMJIT_INLINE_NODEBUG void clearInstRWFlags(InstRWFlags flags) noexcept { _instRWFlags &= ~flags; }

  ASMJIT_INLINE_NODEBUG RATiedFlags aggregatedFlags() const noexcept { return _aggregatedFlags; }
  ASMJIT_INLINE_NODEBUG void addAggregatedFlags(RATiedFlags flags) noexcept { _aggregatedFlags |= flags; }

  ASMJIT_INLINE_NODEBUG RATiedFlags forbiddenFlags() const noexcept { return _forbiddenFlags; }
  ASMJIT_INLINE_NODEBUG void addForbiddenFlags(RATiedFlags flags) noexcept { _forbiddenFlags |= flags; }

  //! Returns the number of tied registers added to the builder.
  ASMJIT_INLINE_NODEBUG uint32_t tiedRegCount() const noexcept { return uint32_t((size_t)(_cur - _tiedRegs)); }

  ASMJIT_INLINE_NODEBUG RATiedReg* begin() noexcept { return _tiedRegs; }
  ASMJIT_INLINE_NODEBUG RATiedReg* end() noexcept { return _cur; }

  ASMJIT_INLINE_NODEBUG const RATiedReg* begin() const noexcept { return _tiedRegs; }
  ASMJIT_INLINE_NODEBUG const RATiedReg* end() const noexcept { return _cur; }

  //! Returns `RATiedReg` at the given `index`.
  inline RATiedReg* operator[](size_t index) noexcept {
    ASMJIT_ASSERT(index < tiedRegCount());
    return &_tiedRegs[index];
  }

  //! Returns `RATiedReg` at the given `index`. (const).
  inline const RATiedReg* operator[](size_t index) const noexcept {
    ASMJIT_ASSERT(index < tiedRegCount());
    return &_tiedRegs[index];
  }

  //! \}

  //! \name Utilities
  //! \{

  Error add(
    RAWorkReg* workReg,
    RATiedFlags flags,
    RegMask useRegMask, uint32_t useId, uint32_t useRewriteMask,
    RegMask outRegMask, uint32_t outId, uint32_t outRewriteMask,
    uint32_t rmSize = 0,
    uint32_t consecutiveParent = Globals::kInvalidId
  ) noexcept {
    RegGroup group = workReg->group();
    RATiedReg* tiedReg = workReg->tiedReg();

    if (useId != BaseReg::kIdBad) {
      _stats.makeFixed(group);
      _used[group] |= Support::bitMask(useId);
      flags |= RATiedFlags::kUseFixed;
    }

    if (outId != BaseReg::kIdBad) {
      _clobbered[group] |= Support::bitMask(outId);
      flags |= RATiedFlags::kOutFixed;
    }

    _aggregatedFlags |= flags;
    _stats.makeUsed(group);

    if (!tiedReg) {
      // Would happen when the builder is not reset properly after each instruction - so catch that!
      ASMJIT_ASSERT(tiedRegCount() < ASMJIT_ARRAY_SIZE(_tiedRegs));

      tiedReg = _cur++;
      tiedReg->init(workReg->workId(), flags, useRegMask, useId, useRewriteMask, outRegMask, outId, outRewriteMask, rmSize, consecutiveParent);

      workReg->setTiedReg(tiedReg);
      workReg->assignBasicBlock(_basicBlockId);

      _count.add(group);
      return kErrorOk;
    }
    else {
      if (consecutiveParent != tiedReg->consecutiveParent()) {
        if (tiedReg->consecutiveParent() != Globals::kInvalidId)
          return DebugUtils::errored(kErrorInvalidState);
        tiedReg->_consecutiveParent = consecutiveParent;
      }

      if (useId != BaseReg::kIdBad) {
        if (ASMJIT_UNLIKELY(tiedReg->hasUseId()))
          return DebugUtils::errored(kErrorOverlappedRegs);
        tiedReg->setUseId(useId);
      }

      if (outId != BaseReg::kIdBad) {
        if (ASMJIT_UNLIKELY(tiedReg->hasOutId()))
          return DebugUtils::errored(kErrorOverlappedRegs);
        tiedReg->setOutId(outId);
      }

      tiedReg->addRefCount();
      tiedReg->addFlags(flags);
      tiedReg->_useRegMask &= useRegMask;
      tiedReg->_useRewriteMask |= useRewriteMask;
      tiedReg->_outRegMask &= outRegMask;
      tiedReg->_outRewriteMask |= outRewriteMask;
      tiedReg->_rmSize = uint8_t(Support::max<uint32_t>(tiedReg->rmSize(), rmSize));
      return kErrorOk;
    }
  }

  Error addCallArg(RAWorkReg* workReg, uint32_t useId) noexcept {
    ASMJIT_ASSERT(useId != BaseReg::kIdBad);

    RATiedFlags flags = RATiedFlags::kUse | RATiedFlags::kRead | RATiedFlags::kUseFixed;
    RegGroup group = workReg->group();
    RegMask allocable = Support::bitMask(useId);

    _aggregatedFlags |= flags;
    _used[group] |= allocable;
    _stats.makeFixed(group);
    _stats.makeUsed(group);

    RATiedReg* tiedReg = workReg->tiedReg();
    if (!tiedReg) {
      // Could happen when the builder is not reset properly after each instruction.
      ASMJIT_ASSERT(tiedRegCount() < ASMJIT_ARRAY_SIZE(_tiedRegs));

      tiedReg = _cur++;
      tiedReg->init(workReg->workId(), flags, allocable, useId, 0, allocable, BaseReg::kIdBad, 0);

      workReg->setTiedReg(tiedReg);
      workReg->assignBasicBlock(_basicBlockId);

      _count.add(group);
      return kErrorOk;
    }
    else {
      if (tiedReg->hasUseId()) {
        flags |= RATiedFlags::kDuplicate;
        tiedReg->_useRegMask |= allocable;
      }
      else {
        tiedReg->setUseId(useId);
        tiedReg->_useRegMask &= allocable;
      }

      tiedReg->addRefCount();
      tiedReg->addFlags(flags);
      return kErrorOk;
    }
  }

  Error addCallRet(RAWorkReg* workReg, uint32_t outId) noexcept {
    ASMJIT_ASSERT(outId != BaseReg::kIdBad);

    RATiedFlags flags = RATiedFlags::kOut | RATiedFlags::kWrite | RATiedFlags::kOutFixed;
    RegGroup group = workReg->group();
    RegMask outRegs = Support::bitMask(outId);

    _aggregatedFlags |= flags;
    _used[group] |= outRegs;
    _stats.makeFixed(group);
    _stats.makeUsed(group);

    RATiedReg* tiedReg = workReg->tiedReg();
    if (!tiedReg) {
      // Could happen when the builder is not reset properly after each instruction.
      ASMJIT_ASSERT(tiedRegCount() < ASMJIT_ARRAY_SIZE(_tiedRegs));

      tiedReg = _cur++;
      tiedReg->init(workReg->workId(), flags, Support::allOnes<RegMask>(), BaseReg::kIdBad, 0, outRegs, outId, 0);

      workReg->setTiedReg(tiedReg);
      workReg->assignBasicBlock(_basicBlockId);

      _count.add(group);
      return kErrorOk;
    }
    else {
      if (tiedReg->hasOutId())
        return DebugUtils::errored(kErrorOverlappedRegs);

      tiedReg->addRefCount();
      tiedReg->addFlags(flags);
      tiedReg->setOutId(outId);
      return kErrorOk;
    }
  }

  //! \}
};

//! Intersection of multiple register assignments.
//!
//! See \ref RAAssignment for more information about register assignments.
class RASharedAssignment {
public:
  typedef RAAssignment::PhysToWorkMap PhysToWorkMap;
  typedef RAAssignment::WorkToPhysMap WorkToPhysMap;

  //! \name Members
  //! \{

  //! Bit-mask of registers that cannot be used upon a block entry, for each block that has this shared assignment.
  //! Scratch registers can come from ISA limits (like jecx/loop instructions on x86) or because the registers are
  //! used by jump/branch instruction that uses registers to perform an indirect jump.
  RegMask _entryScratchGpRegs = 0;
  //! Union of all live-in registers.
  ZoneBitVector _liveIn {};
  //! Register assignment (PhysToWork).
  PhysToWorkMap* _physToWorkMap = nullptr;

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _physToWorkMap == nullptr; }

  ASMJIT_INLINE_NODEBUG RegMask entryScratchGpRegs() const noexcept { return _entryScratchGpRegs; }
  ASMJIT_INLINE_NODEBUG void addEntryScratchGpRegs(RegMask mask) noexcept { _entryScratchGpRegs |= mask; }

  ASMJIT_INLINE_NODEBUG const ZoneBitVector& liveIn() const noexcept { return _liveIn; }

  ASMJIT_INLINE_NODEBUG PhysToWorkMap* physToWorkMap() const noexcept { return _physToWorkMap; }
  ASMJIT_INLINE_NODEBUG void assignPhysToWorkMap(PhysToWorkMap* physToWorkMap) noexcept { _physToWorkMap = physToWorkMap; }

  //! \}
};

//! Register allocation pass used by `BaseCompiler`.
class BaseRAPass : public FuncPass {
public:
  ASMJIT_NONCOPYABLE(BaseRAPass)
  typedef FuncPass Base;

  enum : uint32_t {
    kCallArgWeight = 80
  };

  typedef RAAssignment::PhysToWorkMap PhysToWorkMap;
  typedef RAAssignment::WorkToPhysMap WorkToPhysMap;

  //! \name Members
  //! \{

  //! Allocator that uses zone passed to `runOnFunction()`.
  ZoneAllocator _allocator {};
  //! Emit helper.
  BaseEmitHelper* _iEmitHelper = nullptr;

  //! Logger, disabled if null.
  Logger* _logger = nullptr;
  //! Format options, copied from Logger, or zeroed if there is no logger.
  FormatOptions _formatOptions {};
  //! Diagnostic options, copied from Emitter, or zeroed if there is no logger.
  DiagnosticOptions _diagnosticOptions {};

  //! Function being processed.
  FuncNode* _func = nullptr;
  //! Stop node.
  BaseNode* _stop = nullptr;
  //! Node that is used to insert extra code after the function body.
  BaseNode* _extraBlock = nullptr;

  //! Blocks (first block is the entry, always exists).
  RABlocks _blocks {};
  //! Function exit blocks (usually one, but can contain more).
  RABlocks _exits {};
  //! Post order view (POV).
  RABlocks _pov {};

  //! Number of instruction nodes.
  uint32_t _instructionCount = 0;
  //! Number of created blocks (internal).
  uint32_t _createdBlockCount = 0;

  //! Shared assignment blocks.
  ZoneVector<RASharedAssignment> _sharedAssignments {};

  //! Timestamp generator (incremental).
  mutable uint64_t _lastTimestamp = 0;

  //! Architecture traits.
  const ArchTraits* _archTraits = nullptr;
  //! Index to physical registers in `RAAssignment::PhysToWorkMap`.
  RARegIndex _physRegIndex = RARegIndex();
  //! Count of physical registers in `RAAssignment::PhysToWorkMap`.
  RARegCount _physRegCount = RARegCount();
  //! Total number of physical registers.
  uint32_t _physRegTotal = 0;
  //! Indexes of a possible scratch registers that can be selected if necessary.
  Support::Array<uint8_t, 2> _scratchRegIndexes {};

  //! Registers available for allocation.
  RARegMask _availableRegs = RARegMask();
  //! Count of physical registers per group.
  RARegCount _availableRegCount = RARegCount();
  //! Registers clobbered by the function.
  RARegMask _clobberedRegs = RARegMask();

  //! Work registers (registers used by the function).
  RAWorkRegs _workRegs;
  //! Work registers per register group.
  Support::Array<RAWorkRegs, Globals::kNumVirtGroups> _workRegsOfGroup;

  //! Register allocation strategy per register group.
  Support::Array<RAStrategy, Globals::kNumVirtGroups> _strategy;
  //! Global max live-count (from all blocks) per register group.
  RALiveCount _globalMaxLiveCount = RALiveCount();
  //! Global live spans per register group.
  Support::Array<LiveRegSpans*, Globals::kNumVirtGroups> _globalLiveSpans {};
  //! Temporary stack slot.
  Operand _temporaryMem = Operand();

  //! Stack pointer.
  BaseReg _sp = BaseReg();
  //! Frame pointer.
  BaseReg _fp = BaseReg();
  //! Stack manager.
  RAStackAllocator _stackAllocator {};
  //! Function arguments assignment.
  FuncArgsAssignment _argsAssignment {};
  //! Some StackArgs have to be assigned to StackSlots.
  uint32_t _numStackArgsToStackSlots = 0;

  //! Maximum name-size computed from all WorkRegs.
  uint32_t _maxWorkRegNameSize = 0;
  //! Temporary string builder used to format comments.
  StringTmp<80> _tmpString;

  //! \}

  //! \name Construction & Destruction
  //! \{

  BaseRAPass() noexcept;
  ~BaseRAPass() noexcept override;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns \ref Logger passed to \ref runOnFunction().
  ASMJIT_INLINE_NODEBUG Logger* logger() const noexcept { return _logger; }

  //! Returns either a valid logger if the given `option` is set and logging is enabled, or nullptr.
  ASMJIT_INLINE_NODEBUG Logger* getLoggerIf(DiagnosticOptions option) const noexcept { return Support::test(_diagnosticOptions, option) ? _logger : nullptr; }

  //! Returns whether the diagnostic `option` is enabled.
  //!
  //! \note Returns false if there is no logger (as diagnostics without logging make no sense).
  ASMJIT_INLINE_NODEBUG bool hasDiagnosticOption(DiagnosticOptions option) const noexcept { return Support::test(_diagnosticOptions, option); }

  //! Returns \ref Zone passed to \ref runOnFunction().
  ASMJIT_INLINE_NODEBUG Zone* zone() const noexcept { return _allocator.zone(); }
  //! Returns \ref ZoneAllocator used by the register allocator.
  ASMJIT_INLINE_NODEBUG ZoneAllocator* allocator() const noexcept { return const_cast<ZoneAllocator*>(&_allocator); }

  ASMJIT_INLINE_NODEBUG const ZoneVector<RASharedAssignment>& sharedAssignments() const { return _sharedAssignments; }
  ASMJIT_INLINE_NODEBUG uint32_t sharedAssignmentCount() const noexcept { return _sharedAssignments.size(); }

  //! Returns the current function node.
  ASMJIT_INLINE_NODEBUG FuncNode* func() const noexcept { return _func; }
  //! Returns the stop of the current function.
  ASMJIT_INLINE_NODEBUG BaseNode* stop() const noexcept { return _stop; }

  //! Returns an extra block used by the current function being processed.
  ASMJIT_INLINE_NODEBUG BaseNode* extraBlock() const noexcept { return _extraBlock; }
  //! Sets an extra block, see `extraBlock()`.
  ASMJIT_INLINE_NODEBUG void setExtraBlock(BaseNode* node) noexcept { _extraBlock = node; }

  ASMJIT_INLINE_NODEBUG uint32_t endPosition() const noexcept { return _instructionCount * 2; }

  ASMJIT_INLINE_NODEBUG const RARegMask& availableRegs() const noexcept { return _availableRegs; }
  ASMJIT_INLINE_NODEBUG const RARegMask& clobberedRegs() const noexcept { return _clobberedRegs; }

  //! \}

  //! \name Utilities
  //! \{

  inline void makeUnavailable(RegGroup group, uint32_t regId) noexcept {
    _availableRegs[group] &= ~Support::bitMask(regId);
    _availableRegCount[group]--;
  }

  //! Runs the register allocator for the given `func`.
  Error runOnFunction(Zone* zone, Logger* logger, FuncNode* func) override;

  //! Performs all allocation steps sequentially, called by `runOnFunction()`.
  Error onPerformAllSteps() noexcept;

  //! \}

  //! \name Events
  //! \{

  //! Called by \ref runOnFunction() before the register allocation to initialize
  //! architecture-specific data and constraints.
  virtual void onInit() noexcept;

  //! Called by \ref runOnFunction(` after register allocation to clean everything
  //! up. Called even if the register allocation failed.
  virtual void onDone() noexcept;

  //! \}

  //! \name CFG - Basic-Block Management
  //! \{

  //! Returns the function's entry block.
  inline RABlock* entryBlock() noexcept {
    ASMJIT_ASSERT(!_blocks.empty());
    return _blocks[0];
  }

  //! \overload
  inline const RABlock* entryBlock() const noexcept {
    ASMJIT_ASSERT(!_blocks.empty());
    return _blocks[0];
  }

  //! Returns all basic blocks of this function.
  ASMJIT_INLINE_NODEBUG RABlocks& blocks() noexcept { return _blocks; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const RABlocks& blocks() const noexcept { return _blocks; }

  //! Returns the count of basic blocks (returns size of `_blocks` array).
  ASMJIT_INLINE_NODEBUG uint32_t blockCount() const noexcept { return _blocks.size(); }
  //! Returns the count of reachable basic blocks (returns size of `_pov` array).
  ASMJIT_INLINE_NODEBUG uint32_t reachableBlockCount() const noexcept { return _pov.size(); }

  //! Tests whether the CFG has dangling blocks - these were created by `newBlock()`, but not added to CFG through
  //! `addBlocks()`. If `true` is returned and the  CFG is constructed it means that something is missing and it's
  //! incomplete.
  //!
  //! \note This is only used to check if the number of created blocks matches the number of added blocks.
  ASMJIT_INLINE_NODEBUG bool hasDanglingBlocks() const noexcept { return _createdBlockCount != blockCount(); }

  //! Gest a next timestamp to be used to mark CFG blocks.
  ASMJIT_INLINE_NODEBUG uint64_t nextTimestamp() const noexcept { return ++_lastTimestamp; }

  //! Creates a new `RABlock` instance.
  //!
  //! \note New blocks don't have ID assigned until they are added to the block array by calling `addBlock()`.
  RABlock* newBlock(BaseNode* initialNode = nullptr) noexcept;

  //! Tries to find a neighboring LabelNode (without going through code) that is already connected with `RABlock`.
  //! If no label is found then a new RABlock is created and assigned to all possible labels in a backward direction.
  RABlock* newBlockOrExistingAt(LabelNode* cbLabel, BaseNode** stoppedAt = nullptr) noexcept;

  //! Adds the given `block` to the block list and assign it a unique block id.
  Error addBlock(RABlock* block) noexcept;

  inline Error addExitBlock(RABlock* block) noexcept {
    block->addFlags(RABlockFlags::kIsFuncExit);
    return _exits.append(allocator(), block);
  }

  ASMJIT_FORCE_INLINE RAInst* newRAInst(RABlock* block, InstRWFlags instRWFlags, RATiedFlags flags, uint32_t tiedRegCount, const RARegMask& clobberedRegs) noexcept {
    void* p = zone()->alloc(RAInst::sizeOf(tiedRegCount));
    if (ASMJIT_UNLIKELY(!p))
      return nullptr;
    return new(Support::PlacementNew{p}) RAInst(block, instRWFlags, flags, tiedRegCount, clobberedRegs);
  }

  ASMJIT_FORCE_INLINE Error assignRAInst(BaseNode* node, RABlock* block, RAInstBuilder& ib) noexcept {
    uint32_t tiedRegCount = ib.tiedRegCount();
    RAInst* raInst = newRAInst(block, ib.instRWFlags(), ib.aggregatedFlags(), tiedRegCount, ib._clobbered);

    if (ASMJIT_UNLIKELY(!raInst))
      return DebugUtils::errored(kErrorOutOfMemory);

    RARegIndex index;
    RATiedFlags flagsFilter = ~ib.forbiddenFlags();

    index.buildIndexes(ib._count);
    raInst->_tiedIndex = index;
    raInst->_tiedCount = ib._count;

    for (uint32_t i = 0; i < tiedRegCount; i++) {
      RATiedReg* tiedReg = ib[i];
      RAWorkReg* workReg = workRegById(tiedReg->workId());

      workReg->resetTiedReg();
      RegGroup group = workReg->group();

      if (tiedReg->hasUseId()) {
        block->addFlags(RABlockFlags::kHasFixedRegs);
        raInst->_usedRegs[group] |= Support::bitMask(tiedReg->useId());
      }

      if (tiedReg->hasOutId()) {
        block->addFlags(RABlockFlags::kHasFixedRegs);
      }

      RATiedReg& dst = raInst->_tiedRegs[index[group]++];
      dst = *tiedReg;
      dst._flags &= flagsFilter;

      if (!tiedReg->isDuplicate())
        dst._useRegMask &= ~ib._used[group];
    }

    node->setPassData<RAInst>(raInst);
    return kErrorOk;
  }

  //! \}

  //! \name CFG - Build CFG
  //! \{

  //! Traverse the whole function and do the following:
  //!
  //!   1. Construct CFG (represented by `RABlock`) by populating `_blocks` and `_exits`. Blocks describe the control
  //!      flow of the function and contain some additional information that is used by the register allocator.
  //!
  //!   2. Remove unreachable code immediately. This is not strictly necessary for BaseCompiler itself as the register
  //!      allocator cannot reach such nodes, but keeping instructions that use virtual registers would fail during
  //!      instruction encoding phase (Assembler).
  //!
  //!   3. `RAInst` is created for each `InstNode` or compatible. It contains information that is essential for further
  //!      analysis and register allocation.
  //!
  //! Use `RACFGBuilderT` template that provides the necessary boilerplate.
  virtual Error buildCFG() noexcept;

  //! Called after the CFG is built.
  Error initSharedAssignments(const ZoneVector<uint32_t>& sharedAssignmentsMap) noexcept;

  //! \}

  //! \name CFG - Views Order
  //! \{

  //! Constructs CFG views (only POV at the moment).
  Error buildCFGViews() noexcept;

  //! \}

  //! \name CFG - Dominators
  //! \{

  // Terminology:
  //   - A node `X` dominates a node `Z` if any path from the entry point to `Z` has to go through `X`.
  //   - A node `Z` post-dominates a node `X` if any path from `X` to the end of the graph has to go through `Z`.

  //! Constructs a dominator-tree from CFG.
  Error buildCFGDominators() noexcept;

  bool _strictlyDominates(const RABlock* a, const RABlock* b) const noexcept;
  const RABlock* _nearestCommonDominator(const RABlock* a, const RABlock* b) const noexcept;

  //! Tests whether the basic block `a` dominates `b` - non-strict, returns true when `a == b`.
  ASMJIT_INLINE_NODEBUG bool dominates(const RABlock* a, const RABlock* b) const noexcept { return a == b ? true : _strictlyDominates(a, b); }
  //! Tests whether the basic block `a` dominates `b` - strict dominance check, returns false when `a == b`.
  ASMJIT_INLINE_NODEBUG bool strictlyDominates(const RABlock* a, const RABlock* b) const noexcept { return a == b ? false : _strictlyDominates(a, b); }

  //! Returns a nearest common dominator of `a` and `b`.
  ASMJIT_INLINE_NODEBUG RABlock* nearestCommonDominator(RABlock* a, RABlock* b) const noexcept { return const_cast<RABlock*>(_nearestCommonDominator(a, b)); }
  //! Returns a nearest common dominator of `a` and `b` (const).
  ASMJIT_INLINE_NODEBUG const RABlock* nearestCommonDominator(const RABlock* a, const RABlock* b) const noexcept { return _nearestCommonDominator(a, b); }

  //! \}

  //! \name CFG - Utilities
  //! \{

  Error removeUnreachableCode() noexcept;

  //! Returns `node` or some node after that is ideal for beginning a new block. This function is mostly used after
  //! a conditional or unconditional jump to select the successor node. In some cases the next node could be a label,
  //! which means it could have assigned some block already.
  BaseNode* findSuccessorStartingAt(BaseNode* node) noexcept;

  //! Returns `true` of the `node` can flow to `target` without reaching code nor data. It's used to eliminate jumps
  //! to labels that are next right to them.
  bool isNextTo(BaseNode* node, BaseNode* target) noexcept;

  //! \}

  //! \name Virtual Register Management
  //! \{

  //! Returns a native size of the general-purpose register of the target architecture.
  ASMJIT_INLINE_NODEBUG uint32_t registerSize() const noexcept { return _sp.size(); }
  ASMJIT_INLINE_NODEBUG uint32_t availableRegCount(RegGroup group) const noexcept { return _availableRegCount[group]; }

  ASMJIT_INLINE_NODEBUG RAWorkReg* workRegById(uint32_t workId) const noexcept { return _workRegs[workId]; }

  ASMJIT_INLINE_NODEBUG RAWorkRegs& workRegs() noexcept { return _workRegs; }
  ASMJIT_INLINE_NODEBUG RAWorkRegs& workRegs(RegGroup group) noexcept { return _workRegsOfGroup[group]; }

  ASMJIT_INLINE_NODEBUG const RAWorkRegs& workRegs() const noexcept { return _workRegs; }
  ASMJIT_INLINE_NODEBUG const RAWorkRegs& workRegs(RegGroup group) const noexcept { return _workRegsOfGroup[group]; }

  ASMJIT_INLINE_NODEBUG uint32_t workRegCount() const noexcept { return _workRegs.size(); }
  ASMJIT_INLINE_NODEBUG uint32_t workRegCount(RegGroup group) const noexcept { return _workRegsOfGroup[group].size(); }

  inline void _buildPhysIndex() noexcept {
    _physRegIndex.buildIndexes(_physRegCount);
    _physRegTotal = uint32_t(_physRegIndex[RegGroup::kMaxVirt]) +
                    uint32_t(_physRegCount[RegGroup::kMaxVirt]) ;
  }
  ASMJIT_INLINE_NODEBUG uint32_t physRegIndex(RegGroup group) const noexcept { return _physRegIndex[group]; }
  ASMJIT_INLINE_NODEBUG uint32_t physRegTotal() const noexcept { return _physRegTotal; }

  Error _asWorkReg(VirtReg* vReg, RAWorkReg** out) noexcept;

  //! Creates `RAWorkReg` data for the given `vReg`. The function does nothing
  //! if `vReg` already contains link to `RAWorkReg`. Called by `constructBlocks()`.
  inline Error asWorkReg(VirtReg* vReg, RAWorkReg** out) noexcept {
    *out = vReg->workReg();
    return *out ? kErrorOk : _asWorkReg(vReg, out);
  }

  ASMJIT_FORCE_INLINE Error virtIndexAsWorkReg(uint32_t vIndex, RAWorkReg** out) noexcept {
    const ZoneVector<VirtReg*>& virtRegs = cc()->virtRegs();
    if (ASMJIT_UNLIKELY(vIndex >= virtRegs.size()))
      return DebugUtils::errored(kErrorInvalidVirtId);
    return asWorkReg(virtRegs[vIndex], out);
  }

  inline RAStackSlot* getOrCreateStackSlot(RAWorkReg* workReg) noexcept {
    RAStackSlot* slot = workReg->stackSlot();

    if (slot)
      return slot;

    slot = _stackAllocator.newSlot(_sp.id(), workReg->virtReg()->virtSize(), workReg->virtReg()->alignment(), RAStackSlot::kFlagRegHome);
    workReg->_stackSlot = slot;
    workReg->markStackUsed();
    return slot;
  }

  inline BaseMem workRegAsMem(RAWorkReg* workReg) noexcept {
    getOrCreateStackSlot(workReg);
    return BaseMem(OperandSignature::fromOpType(OperandType::kMem) |
                   OperandSignature::fromMemBaseType(_sp.type()) |
                   OperandSignature::fromBits(OperandSignature::kMemRegHomeFlag),
                   workReg->virtId(), 0, 0);
  }

  WorkToPhysMap* newWorkToPhysMap() noexcept;
  PhysToWorkMap* newPhysToWorkMap() noexcept;

  inline PhysToWorkMap* clonePhysToWorkMap(const PhysToWorkMap* map) noexcept {
    size_t size = PhysToWorkMap::sizeOf(_physRegTotal);
    return static_cast<PhysToWorkMap*>(zone()->dupAligned(map, size, sizeof(uint32_t)));
  }

  //! \name Liveness Analysis & Statistics
  //! \{

  //! 1. Calculates GEN/KILL/IN/OUT of each block.
  //! 2. Calculates live spans and basic statistics of each work register.
  Error buildLiveness() noexcept;

  //! Assigns argIndex to WorkRegs. Must be called after the liveness analysis
  //! finishes as it checks whether the argument is live upon entry.
  Error assignArgIndexToWorkRegs() noexcept;

  //! \}

  //! \name Register Allocation - Global
  //! \{

  //! Runs a global register allocator.
  Error runGlobalAllocator() noexcept;

  //! Initializes data structures used for global live spans.
  Error initGlobalLiveSpans() noexcept;

  Error binPack(RegGroup group) noexcept;

  //! \}

  //! \name Register Allocation - Local
  //! \{

  //! Runs a local register allocator.
  Error runLocalAllocator() noexcept;
  Error setBlockEntryAssignment(RABlock* block, const RABlock* fromBlock, const RAAssignment& fromAssignment) noexcept;
  Error setSharedAssignment(uint32_t sharedAssignmentId, const RAAssignment& fromAssignment) noexcept;

  //! Called after the RA assignment has been assigned to a block.
  //!
  //! This cannot change the assignment, but can examine it.
  Error blockEntryAssigned(const PhysToWorkMap* physToWorkMap) noexcept;

  //! \}

  //! \name Register Allocation Utilities
  //! \{

  Error useTemporaryMem(BaseMem& out, uint32_t size, uint32_t alignment) noexcept;

  //! \}

  //! \name Function Prolog & Epilog
  //! \{

  virtual Error updateStackFrame() noexcept;
  Error _markStackArgsToKeep() noexcept;
  Error _updateStackArgs() noexcept;
  Error insertPrologEpilog() noexcept;

  //! \}

  //! \name Instruction Rewriter
  //! \{

  Error rewrite() noexcept;
  virtual Error _rewrite(BaseNode* first, BaseNode* stop) noexcept;

  //! \}

#ifndef ASMJIT_NO_LOGGING
  //! \name Logging
  //! \{

  Error annotateCode() noexcept;

  Error _dumpBlockIds(String& sb, const RABlocks& blocks) noexcept;
  Error _dumpBlockLiveness(String& sb, const RABlock* block) noexcept;
  Error _dumpLiveSpans(String& sb) noexcept;

  //! \}
#endif

  //! \name Emit
  //! \{

  virtual Error emitMove(uint32_t workId, uint32_t dstPhysId, uint32_t srcPhysId) noexcept;
  virtual Error emitSwap(uint32_t aWorkId, uint32_t aPhysId, uint32_t bWorkId, uint32_t bPhysId) noexcept;

  virtual Error emitLoad(uint32_t workId, uint32_t dstPhysId) noexcept;
  virtual Error emitSave(uint32_t workId, uint32_t srcPhysId) noexcept;

  virtual Error emitJump(const Label& label) noexcept;
  virtual Error emitPreCall(InvokeNode* invokeNode) noexcept;

  //! \}
};

inline ZoneAllocator* RABlock::allocator() const noexcept { return _ra->allocator(); }

inline RegMask RABlock::entryScratchGpRegs() const noexcept {
  RegMask regs = _entryScratchGpRegs;
  if (hasSharedAssignmentId())
    regs = _ra->_sharedAssignments[_sharedAssignmentId].entryScratchGpRegs();
  return regs;
}

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER
#endif // ASMJIT_CORE_RAPASS_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/rastack.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/rastack_p.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// RAStackAllocator - Slots
// ========================

RAStackSlot* RAStackAllocator::newSlot(uint32_t baseRegId, uint32_t size, uint32_t alignment, uint32_t flags) noexcept {
  if (ASMJIT_UNLIKELY(_slots.willGrow(allocator(), 1) != kErrorOk))
    return nullptr;

  RAStackSlot* slot = allocator()->allocT<RAStackSlot>();
  if (ASMJIT_UNLIKELY(!slot))
    return nullptr;

  slot->_baseRegId = uint8_t(baseRegId);
  slot->_alignment = uint8_t(Support::max<uint32_t>(alignment, 1));
  slot->_flags = uint16_t(flags);
  slot->_useCount = 0;
  slot->_size = size;

  slot->_weight = 0;
  slot->_offset = 0;

  _alignment = Support::max<uint32_t>(_alignment, alignment);
  _slots.appendUnsafe(slot);
  return slot;
}

// RAStackAllocator - Utilities
// ============================

struct RAStackGap {
  inline RAStackGap() noexcept
    : offset(0),
      size(0) {}

  inline RAStackGap(uint32_t offset, uint32_t size) noexcept
    : offset(offset),
      size(size) {}

  inline RAStackGap(const RAStackGap& other) noexcept
    : offset(other.offset),
      size(other.size) {}

  uint32_t offset;
  uint32_t size;
};

Error RAStackAllocator::calculateStackFrame() noexcept {
  // Base weight added to all registers regardless of their size and alignment.
  uint32_t kBaseRegWeight = 16;

  // STEP 1:
  //
  // Update usage based on the size of the slot. We boost smaller slots in a way that 32-bit register has a higher
  // priority than a 128-bit register, however, if one 128-bit register is used 4 times more than some other 32-bit
  // register it will overweight it.
  for (RAStackSlot* slot : _slots) {
    uint32_t alignment = slot->alignment();
    ASMJIT_ASSERT(alignment > 0);

    uint32_t power = Support::min<uint32_t>(Support::ctz(alignment), 6);
    uint64_t weight;

    if (slot->isRegHome())
      weight = kBaseRegWeight + (uint64_t(slot->useCount()) * (7 - power));
    else
      weight = power;

    // If overflown, which has less chance of winning a lottery, just use max possible weight. In such case it
    // probably doesn't matter at all.
    if (weight > 0xFFFFFFFFu)
      weight = 0xFFFFFFFFu;

    slot->setWeight(uint32_t(weight));
  }

  // STEP 2:
  //
  // Sort stack slots based on their newly calculated weight (in descending order).
  _slots.sort([](const RAStackSlot* a, const RAStackSlot* b) noexcept {
    return a->weight() >  b->weight() ? 1 :
           a->weight() == b->weight() ? 0 : -1;
  });

  // STEP 3:
  //
  // Calculate offset of each slot. We start from the slot that has the highest weight and advance to slots with
  // lower weight. It could look that offsets start from the first slot in our list and then simply increase, but
  // it's not always the case as we also try to fill all gaps introduced by the fact that slots are sorted by
  // weight and not by size & alignment, so when we need to align some slot we distribute the gap caused by the
  // alignment to `gaps`.
  uint32_t offset = 0;
  ZoneVector<RAStackGap> gaps[kSizeCount - 1];

  for (RAStackSlot* slot : _slots) {
    if (slot->isStackArg())
      continue;

    uint32_t slotAlignment = slot->alignment();
    uint32_t alignedOffset = Support::alignUp(offset, slotAlignment);

    // Try to find a slot within gaps first, before advancing the `offset`.
    bool foundGap = false;
    uint32_t gapSize = 0;
    uint32_t gapOffset = 0;

    {
      uint32_t slotSize = slot->size();
      if (slotSize < (1u << uint32_t(ASMJIT_ARRAY_SIZE(gaps)))) {
        // Iterate from the lowest to the highest possible.
        uint32_t index = Support::ctz(slotSize);
        do {
          if (!gaps[index].empty()) {
            RAStackGap gap = gaps[index].pop();

            ASMJIT_ASSERT(Support::isAligned(gap.offset, slotAlignment));
            slot->setOffset(int32_t(gap.offset));

            gapSize = gap.size - slotSize;
            gapOffset = gap.offset - slotSize;

            foundGap = true;
            break;
          }
        } while (++index < uint32_t(ASMJIT_ARRAY_SIZE(gaps)));
      }
    }

    // No gap found, we may create a new one(s) if the current offset is not aligned.
    if (!foundGap && offset != alignedOffset) {
      gapSize = alignedOffset - offset;
      gapOffset = alignedOffset;

      offset = alignedOffset;
    }

    // True if we have found a gap and not filled all of it or we aligned the current offset.
    if (gapSize) {
      uint32_t gapEnd = gapSize + gapOffset;
      while (gapOffset < gapEnd) {
        uint32_t index = Support::ctz(gapOffset);
        uint32_t slotSize = 1u << index;

        // Weird case, better to bail...
        if (gapEnd - gapOffset < slotSize)
          break;

        ASMJIT_PROPAGATE(gaps[index].append(allocator(), RAStackGap(gapOffset, slotSize)));
        gapOffset += slotSize;
      }
    }

    if (!foundGap) {
      ASMJIT_ASSERT(Support::isAligned(offset, slotAlignment));
      slot->setOffset(int32_t(offset));
      offset += slot->size();
    }
  }

  _stackSize = Support::alignUp(offset, _alignment);
  return kErrorOk;
}

Error RAStackAllocator::adjustSlotOffsets(int32_t offset) noexcept {
  for (RAStackSlot* slot : _slots)
    if (!slot->isStackArg())
      slot->_offset += offset;
  return kErrorOk;
}

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER

```

`CodeCleaner/asmjit/asmjit/core/rastack_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_RASTACK_P_H_INCLUDED
#define ASMJIT_CORE_RASTACK_P_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_COMPILER

#include "../core/radefs_p.h"

ASMJIT_BEGIN_NAMESPACE

//! \cond INTERNAL
//! \addtogroup asmjit_ra
//! \{

//! Stack slot.
struct RAStackSlot {
  //! Stack slot flags.
  //!
  //! TODO: kFlagStackArg is not used by the current implementation, do we need to keep it?
  enum Flags : uint16_t {
    //! Stack slot is register home slot.
    kFlagRegHome = 0x0001u,
    //! Stack slot position matches argument passed via stack.
    kFlagStackArg = 0x0002u
  };

  enum ArgIndex : uint32_t {
    kNoArgIndex = 0xFF
  };

  //! \name Members
  //! \{

  //! Base register used to address the stack.
  uint8_t _baseRegId;
  //! Minimum alignment required by the slot.
  uint8_t _alignment;
  //! Reserved for future use.
  uint16_t _flags;
  //! Size of memory required by the slot.
  uint32_t _size;

  //! Usage counter (one unit equals one memory access).
  uint32_t _useCount;
  //! Weight of the slot, calculated by \ref RAStackAllocator::calculateStackFrame().
  uint32_t _weight;
  //! Stack offset, calculated by \ref RAStackAllocator::calculateStackFrame().
  int32_t _offset;

  //! \}

  //! \name Accessors
  //! \{

  inline uint32_t baseRegId() const noexcept { return _baseRegId; }
  inline void setBaseRegId(uint32_t id) noexcept { _baseRegId = uint8_t(id); }

  inline uint32_t size() const noexcept { return _size; }
  inline uint32_t alignment() const noexcept { return _alignment; }

  inline uint32_t flags() const noexcept { return _flags; }
  inline bool hasFlag(uint32_t flag) const noexcept { return (_flags & flag) != 0; }
  inline void addFlags(uint32_t flags) noexcept { _flags = uint16_t(_flags | flags); }

  inline bool isRegHome() const noexcept { return hasFlag(kFlagRegHome); }
  inline bool isStackArg() const noexcept { return hasFlag(kFlagStackArg); }

  inline uint32_t useCount() const noexcept { return _useCount; }
  inline void addUseCount(uint32_t n = 1) noexcept { _useCount += n; }

  inline uint32_t weight() const noexcept { return _weight; }
  inline void setWeight(uint32_t weight) noexcept { _weight = weight; }

  inline int32_t offset() const noexcept { return _offset; }
  inline void setOffset(int32_t offset) noexcept { _offset = offset; }

  //! \}
};

typedef ZoneVector<RAStackSlot*> RAStackSlots;

//! Stack allocator.
class RAStackAllocator {
public:
  ASMJIT_NONCOPYABLE(RAStackAllocator)

  enum Size : uint32_t {
    kSize1     = 0,
    kSize2     = 1,
    kSize4     = 2,
    kSize8     = 3,
    kSize16    = 4,
    kSize32    = 5,
    kSize64    = 6,
    kSizeCount = 7
  };

  //! \name Members
  //! \{

  //! Allocator used to allocate internal data.
  ZoneAllocator* _allocator {};
  //! Count of bytes used by all slots.
  uint32_t _bytesUsed {};
  //! Calculated stack size (can be a bit greater than `_bytesUsed`).
  uint32_t _stackSize {};
  //! Minimum stack alignment.
  uint32_t _alignment = 1;
  //! Stack slots vector.
  RAStackSlots _slots;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG RAStackAllocator() noexcept {}

  ASMJIT_INLINE_NODEBUG void reset(ZoneAllocator* allocator) noexcept {
    _allocator = allocator;
    _bytesUsed = 0;
    _stackSize = 0;
    _alignment = 1;
    _slots.reset();
  }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneAllocator* allocator() const noexcept { return _allocator; }

  ASMJIT_INLINE_NODEBUG uint32_t bytesUsed() const noexcept { return _bytesUsed; }
  ASMJIT_INLINE_NODEBUG uint32_t stackSize() const noexcept { return _stackSize; }
  ASMJIT_INLINE_NODEBUG uint32_t alignment() const noexcept { return _alignment; }

  ASMJIT_INLINE_NODEBUG RAStackSlots& slots() noexcept { return _slots; }
  ASMJIT_INLINE_NODEBUG const RAStackSlots& slots() const noexcept { return _slots; }
  ASMJIT_INLINE_NODEBUG uint32_t slotCount() const noexcept { return _slots.size(); }

  //! \}

  //! \name Utilities
  //! \{

  RAStackSlot* newSlot(uint32_t baseRegId, uint32_t size, uint32_t alignment, uint32_t flags = 0) noexcept;

  Error calculateStackFrame() noexcept;
  Error adjustSlotOffsets(int32_t offset) noexcept;

  //! \}
};

//! \}
//! \endcond

ASMJIT_END_NAMESPACE

#endif // !ASMJIT_NO_COMPILER
#endif // ASMJIT_CORE_RASTACK_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/string.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/string.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// String - Globals
// ================

static const char String_baseN[] = "0123456789ABCDEF";

constexpr size_t kMinAllocSize = 128;
constexpr size_t kMaxAllocSize = SIZE_MAX - Globals::kGrowThreshold;

// Based on ZoneVector_growCapacity().
//
// NOTE: The sizes here include null terminators - that way we can have aligned allocations that are power of 2s
// initially.
static ASMJIT_FORCE_INLINE size_t String_growCapacity(size_t byteSize, size_t minimumByteSize) noexcept {
  static constexpr size_t kGrowThreshold = Globals::kGrowThreshold;

  ASMJIT_ASSERT(minimumByteSize < kMaxAllocSize);

  // This is more than exponential growth at the beginning.
  if (byteSize < kMinAllocSize) {
    byteSize = kMinAllocSize;
  }
  else if (byteSize < 512) {
    byteSize = 512;
  }

  if (byteSize < minimumByteSize) {
    // Exponential growth before we reach `kGrowThreshold`.
    byteSize = Support::alignUpPowerOf2(minimumByteSize);

    // Bail to `minimumByteSize` in case of overflow - most likely whatever that is happening afterwards would just fail.
    if (byteSize < minimumByteSize) {
      return minimumByteSize;
    }

    // Pretty much chunked growth advancing by `kGrowThreshold` after we exceed it.
    if (byteSize > kGrowThreshold) {
      // Align to kGrowThreshold.
      size_t remainder = minimumByteSize % kGrowThreshold;

      byteSize = minimumByteSize + remainder;

      // Bail to `minimumByteSize` in case of overflow.
      if (byteSize < minimumByteSize)
        return minimumByteSize;
    }
  }

  return Support::min<size_t>(byteSize, kMaxAllocSize);
}

// String - Clear & Reset
// ======================

Error String::reset() noexcept {
  if (_type == kTypeLarge)
    ::free(_large.data);

  _resetInternal();
  return kErrorOk;
}

Error String::clear() noexcept {
  if (isLargeOrExternal()) {
    _large.size = 0;
    _large.data[0] = '\0';
  }
  else {
    _raw.uptr[0] = 0;
  }

  return kErrorOk;
}

// String - Prepare
// ================

char* String::prepare(ModifyOp op, size_t size) noexcept {
  char* curData;
  size_t curSize;
  size_t curCapacity;

  if (isLargeOrExternal()) {
    curData = _large.data;
    curSize = _large.size;
    curCapacity = _large.capacity;
  }
  else {
    curData = _small.data;
    curSize = _small.type;
    curCapacity = kSSOCapacity;
  }

  if (op == ModifyOp::kAssign) {
    if (size > curCapacity) {
      // Prevent arithmetic overflow.
      if (ASMJIT_UNLIKELY(size >= kMaxAllocSize))
        return nullptr;

      size_t newCapacity = Support::alignUp<size_t>(size + 1, kMinAllocSize);
      char* newData = static_cast<char*>(::malloc(newCapacity));

      if (ASMJIT_UNLIKELY(!newData))
        return nullptr;

      if (_type == kTypeLarge)
        ::free(curData);

      _large.type = kTypeLarge;
      _large.size = size;
      _large.capacity = newCapacity - 1;
      _large.data = newData;

      newData[size] = '\0';
      return newData;
    }
    else {
      _setSize(size);
      curData[size] = '\0';
      return curData;
    }
  }
  else {
    // Prevent arithmetic overflow.
    if (ASMJIT_UNLIKELY(size >= kMaxAllocSize - curSize - 1))
      return nullptr;

    size_t newSize = size + curSize;
    size_t newSizePlusOne = newSize + 1;

    if (newSize > curCapacity) {
      size_t newCapacityPlusOne = String_growCapacity(size + 1u, newSizePlusOne);
      ASMJIT_ASSERT(newCapacityPlusOne >= newSizePlusOne);

      if (ASMJIT_UNLIKELY(newCapacityPlusOne < newSizePlusOne))
        return nullptr;

      char* newData = static_cast<char*>(::malloc(newCapacityPlusOne));
      if (ASMJIT_UNLIKELY(!newData))
        return nullptr;

      memcpy(newData, curData, curSize);

      if (_type == kTypeLarge)
        ::free(curData);

      _large.type = kTypeLarge;
      _large.size = newSize;
      _large.capacity = newCapacityPlusOne - 1;
      _large.data = newData;

      newData[newSize] = '\0';
      return newData + curSize;
    }
    else {
      _setSize(newSize);
      curData[newSize] = '\0';
      return curData + curSize;
    }
  }
}

// String - Assign
// ===============

Error String::assign(const char* data, size_t size) noexcept {
  char* dst = nullptr;

  // Null terminated string without `size` specified.
  if (size == SIZE_MAX)
    size = data ? strlen(data) : size_t(0);

  if (isLargeOrExternal()) {
    if (size <= _large.capacity) {
      dst = _large.data;
      _large.size = size;
    }
    else {
      size_t capacityPlusOne = Support::alignUp(size + 1, 32);
      if (ASMJIT_UNLIKELY(capacityPlusOne < size))
        return DebugUtils::errored(kErrorOutOfMemory);

      dst = static_cast<char*>(::malloc(capacityPlusOne));
      if (ASMJIT_UNLIKELY(!dst))
        return DebugUtils::errored(kErrorOutOfMemory);

      if (_type == kTypeLarge)
        ::free(_large.data);

      _large.type = kTypeLarge;
      _large.data = dst;
      _large.size = size;
      _large.capacity = capacityPlusOne - 1;
    }
  }
  else {
    if (size <= kSSOCapacity) {
      ASMJIT_ASSERT(size < 0xFFu);

      dst = _small.data;
      _small.type = uint8_t(size);
    }
    else {
      dst = static_cast<char*>(::malloc(size + 1));
      if (ASMJIT_UNLIKELY(!dst))
        return DebugUtils::errored(kErrorOutOfMemory);

      _large.type = kTypeLarge;
      _large.data = dst;
      _large.size = size;
      _large.capacity = size;
    }
  }

  // Optionally copy data from `data` and null-terminate.
  if (data && size) {
    // NOTE: It's better to use `memmove()`. If, for any reason, somebody uses
    // this function to substring the same string it would work as expected.
    ::memmove(dst, data, size);
  }

  dst[size] = '\0';
  return kErrorOk;
}

// String - Operations
// ===================

Error String::_opString(ModifyOp op, const char* str, size_t size) noexcept {
  if (size == SIZE_MAX)
    size = str ? strlen(str) : size_t(0);

  if (!size)
    return kErrorOk;

  char* p = prepare(op, size);
  if (!p)
    return DebugUtils::errored(kErrorOutOfMemory);

  memcpy(p, str, size);
  return kErrorOk;
}

Error String::_opChar(ModifyOp op, char c) noexcept {
  char* p = prepare(op, 1);
  if (!p)
    return DebugUtils::errored(kErrorOutOfMemory);

  *p = c;
  return kErrorOk;
}

Error String::_opChars(ModifyOp op, char c, size_t n) noexcept {
  if (!n)
    return kErrorOk;

  char* p = prepare(op, n);
  if (!p)
    return DebugUtils::errored(kErrorOutOfMemory);

  memset(p, c, n);
  return kErrorOk;
}

Error String::padEnd(size_t n, char c) noexcept {
  size_t size = this->size();
  return n > size ? appendChars(c, n - size) : kErrorOk;
}

Error String::_opNumber(ModifyOp op, uint64_t i, uint32_t base, size_t width, StringFormatFlags flags) noexcept {
  if (base == 0)
    base = 10;

  char buf[128];
  char* p = buf + ASMJIT_ARRAY_SIZE(buf);

  uint64_t orig = i;
  char sign = '\0';

  // Format Sign
  // -----------

  if (Support::test(flags, StringFormatFlags::kSigned) && int64_t(i) < 0) {
    i = Support::neg(i);
    sign = '-';
  }
  else if (Support::test(flags, StringFormatFlags::kShowSign)) {
    sign = '+';
  }
  else if (Support::test(flags, StringFormatFlags::kShowSpace)) {
    sign = ' ';
  }

  // Format Number
  // -------------

  switch (base) {
    case 2:
    case 8:
    case 16: {
      uint32_t shift = Support::ctz(base);
      uint32_t mask = base - 1;

      do {
        uint64_t d = i >> shift;
        size_t r = size_t(i & mask);

        *--p = String_baseN[r];
        i = d;
      } while (i);

      break;
    }

    case 10: {
      do {
        uint64_t d = i / 10;
        uint64_t r = i % 10;

        *--p = char(uint32_t('0') + uint32_t(r));
        i = d;
      } while (i);

      break;
    }

    default:
      return DebugUtils::errored(kErrorInvalidArgument);
  }

  size_t numberSize = (size_t)(buf + ASMJIT_ARRAY_SIZE(buf) - p);

  // Alternate Form
  // --------------

  if (Support::test(flags, StringFormatFlags::kAlternate)) {
    if (base == 8) {
      if (orig != 0)
        *--p = '0';
    }
    if (base == 16) {
      *--p = 'x';
      *--p = '0';
    }
  }

  // String Width
  // ------------

  if (sign != 0)
    *--p = sign;

  if (width > 256)
    width = 256;

  if (width <= numberSize)
    width = 0;
  else
    width -= numberSize;

  // Finalize
  // --------

  size_t prefixSize = (size_t)(buf + ASMJIT_ARRAY_SIZE(buf) - p) - numberSize;
  char* data = prepare(op, prefixSize + width + numberSize);

  if (!data)
    return DebugUtils::errored(kErrorOutOfMemory);

  memcpy(data, p, prefixSize);
  data += prefixSize;

  memset(data, '0', width);
  data += width;

  memcpy(data, p + prefixSize, numberSize);
  return kErrorOk;
}

Error String::_opHex(ModifyOp op, const void* data, size_t size, char separator) noexcept {
  char* dst;
  const uint8_t* src = static_cast<const uint8_t*>(data);

  if (!size)
    return kErrorOk;

  if (separator) {
    if (ASMJIT_UNLIKELY(size >= SIZE_MAX / 3))
      return DebugUtils::errored(kErrorOutOfMemory);

    dst = prepare(op, size * 3 - 1);
    if (ASMJIT_UNLIKELY(!dst))
      return DebugUtils::errored(kErrorOutOfMemory);

    size_t i = 0;
    for (;;) {
      dst[0] = String_baseN[(src[0] >> 4) & 0xF];
      dst[1] = String_baseN[(src[0]     ) & 0xF];
      if (++i == size)
        break;
      // This makes sure that the separator is only put between two hexadecimal bytes.
      dst[2] = separator;
      dst += 3;
      src++;
    }
  }
  else {
    if (ASMJIT_UNLIKELY(size >= SIZE_MAX / 2))
      return DebugUtils::errored(kErrorOutOfMemory);

    dst = prepare(op, size * 2);
    if (ASMJIT_UNLIKELY(!dst))
      return DebugUtils::errored(kErrorOutOfMemory);

    for (size_t i = 0; i < size; i++, dst += 2, src++) {
      dst[0] = String_baseN[(src[0] >> 4) & 0xF];
      dst[1] = String_baseN[(src[0]     ) & 0xF];
    }
  }

  return kErrorOk;
}

Error String::_opFormat(ModifyOp op, const char* fmt, ...) noexcept {
  Error err;
  va_list ap;

  va_start(ap, fmt);
  err = _opVFormat(op, fmt, ap);
  va_end(ap);

  return err;
}

Error String::_opVFormat(ModifyOp op, const char* fmt, va_list ap) noexcept {
  size_t startAt = (op == ModifyOp::kAssign) ? size_t(0) : size();
  size_t remainingCapacity = capacity() - startAt;

  char buf[1024];
  int fmtResult;
  size_t outputSize;

  va_list apCopy;
  va_copy(apCopy, ap);

  if (remainingCapacity >= 128) {
    fmtResult = vsnprintf(data() + startAt, remainingCapacity, fmt, ap);
    outputSize = size_t(fmtResult);

    if (ASMJIT_LIKELY(outputSize <= remainingCapacity)) {
      _setSize(startAt + outputSize);
      return kErrorOk;
    }
  }
  else {
    fmtResult = vsnprintf(buf, ASMJIT_ARRAY_SIZE(buf), fmt, ap);
    outputSize = size_t(fmtResult);

    if (ASMJIT_LIKELY(outputSize < ASMJIT_ARRAY_SIZE(buf)))
      return _opString(op, buf, outputSize);
  }

  if (ASMJIT_UNLIKELY(fmtResult < 0))
    return DebugUtils::errored(kErrorInvalidState);

  char* p = prepare(op, outputSize);
  if (ASMJIT_UNLIKELY(!p))
    return DebugUtils::errored(kErrorOutOfMemory);

  fmtResult = vsnprintf(p, outputSize + 1, fmt, apCopy);
  ASMJIT_ASSERT(size_t(fmtResult) == outputSize);

  return kErrorOk;
}

Error String::truncate(size_t newSize) noexcept {
  if (isLargeOrExternal()) {
    if (newSize < _large.size) {
      _large.data[newSize] = '\0';
      _large.size = newSize;
    }
  }
  else {
    if (newSize < _type) {
      _small.data[newSize] = '\0';
      _small.type = uint8_t(newSize);
    }
  }

  return kErrorOk;
}

bool String::equals(const char* other, size_t size) const noexcept {
  const char* aData = data();
  const char* bData = other;

  size_t aSize = this->size();
  size_t bSize = size;

  if (bSize == SIZE_MAX) {
    size_t i;
    for (i = 0; i < aSize; i++)
      if (aData[i] != bData[i] || bData[i] == 0)
        return false;
    return bData[i] == 0;
  }
  else {
    if (aSize != bSize)
      return false;
    return ::memcmp(aData, bData, aSize) == 0;
  }
}

// String - Tests
// ==============

#if defined(ASMJIT_TEST)
static void test_string_grow() noexcept {
  String s;
  size_t c = s.capacity();

  INFO("Testing string grow strategy (SSO capacity: %zu)", c);
  for (size_t i = 0; i < 1000000; i++) {
    s.append('x');
    if (s.capacity() != c) {
      c = s.capacity();
      INFO("  String reallocated to new capacity: %zu", c);
    }
  }

  // We don't expect a 1 million character string to occupy 4MiB, for example. So verify that!
  EXPECT_LT(c, size_t(4 * 1024 * 1024));
}

UNIT(core_string) {
  String s;

  INFO("Testing string functionality");

  EXPECT_FALSE(s.isLargeOrExternal());
  EXPECT_FALSE(s.isExternal());

  EXPECT_EQ(s.assign('a'), kErrorOk);
  EXPECT_EQ(s.size(), 1u);
  EXPECT_EQ(s.capacity(), String::kSSOCapacity);
  EXPECT_EQ(s.data()[0], 'a');
  EXPECT_EQ(s.data()[1], '\0');
  EXPECT_TRUE(s.equals("a"));
  EXPECT_TRUE(s.equals("a", 1));

  EXPECT_EQ(s.assignChars('b', 4), kErrorOk);
  EXPECT_EQ(s.size(), 4u);
  EXPECT_EQ(s.capacity(), String::kSSOCapacity);
  EXPECT_EQ(s.data()[0], 'b');
  EXPECT_EQ(s.data()[1], 'b');
  EXPECT_EQ(s.data()[2], 'b');
  EXPECT_EQ(s.data()[3], 'b');
  EXPECT_EQ(s.data()[4], '\0');
  EXPECT_TRUE(s.equals("bbbb"));
  EXPECT_TRUE(s.equals("bbbb", 4));

  EXPECT_EQ(s.assign("abc"), kErrorOk);
  EXPECT_EQ(s.size(), 3u);
  EXPECT_EQ(s.capacity(), String::kSSOCapacity);
  EXPECT_EQ(s.data()[0], 'a');
  EXPECT_EQ(s.data()[1], 'b');
  EXPECT_EQ(s.data()[2], 'c');
  EXPECT_EQ(s.data()[3], '\0');
  EXPECT_TRUE(s.equals("abc"));
  EXPECT_TRUE(s.equals("abc", 3));

  const char* large = "Large string that will not fit into SSO buffer";
  EXPECT_EQ(s.assign(large), kErrorOk);
  EXPECT_TRUE(s.isLargeOrExternal());
  EXPECT_EQ(s.size(), strlen(large));
  EXPECT_GT(s.capacity(), String::kSSOCapacity);
  EXPECT_TRUE(s.equals(large));
  EXPECT_TRUE(s.equals(large, strlen(large)));

  const char* additional = " (additional content)";
  EXPECT_TRUE(s.isLargeOrExternal());
  EXPECT_EQ(s.append(additional), kErrorOk);
  EXPECT_EQ(s.size(), strlen(large) + strlen(additional));

  EXPECT_EQ(s.clear(), kErrorOk);
  EXPECT_EQ(s.size(), 0u);
  EXPECT_TRUE(s.empty());
  EXPECT_EQ(s.data()[0], '\0');
  EXPECT_TRUE(s.isLargeOrExternal()); // Clear should never release the memory.

  EXPECT_EQ(s.appendUInt(1234), kErrorOk);
  EXPECT_TRUE(s.equals("1234"));

  EXPECT_EQ(s.assignUInt(0xFFFF, 16, 0, StringFormatFlags::kAlternate), kErrorOk);
  EXPECT_TRUE(s.equals("0xFFFF"));

  StringTmp<64> sTmp;
  EXPECT_TRUE(sTmp.isLargeOrExternal());
  EXPECT_TRUE(sTmp.isExternal());
  EXPECT_EQ(sTmp.appendChars(' ', 1000), kErrorOk);
  EXPECT_FALSE(sTmp.isExternal());

  test_string_grow();
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/string.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_STRING_H_INCLUDED
#define ASMJIT_CORE_STRING_H_INCLUDED

#include "../core/support.h"
#include "../core/zone.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_utilities
//! \{

//! Format flags used by \ref String API.
enum class StringFormatFlags : uint32_t {
  //! No flags.
  kNone = 0x00000000u,
  //! Show sign.
  kShowSign = 0x00000001u,
  //! Show space.
  kShowSpace = 0x00000002u,
  //! Alternate form (use 0x when formatting HEX number).
  kAlternate = 0x00000004u,
  //! The input is signed.
  kSigned = 0x80000000u
};
ASMJIT_DEFINE_ENUM_FLAGS(StringFormatFlags)

//! Fixed string - only useful for strings that would never exceed `N - 1` characters; always null-terminated.
template<size_t N>
union FixedString {
  //! \name Constants
  //! \{

  // This cannot be constexpr as GCC 4.8 refuses constexpr members of unions.
  enum : uint32_t {
    kNumUInt32Words = uint32_t((N + sizeof(uint32_t) - 1) / sizeof(uint32_t))
  };

  //! \}

  //! \name Members
  //! \{

  char str[kNumUInt32Words * sizeof(uint32_t)];
  uint32_t u32[kNumUInt32Words];

  //! \}

  //! \name Utilities
  //! \{

  inline bool equals(const char* other) const noexcept { return strcmp(str, other) == 0; }

  //! \}
};

//! A simple non-reference counted string that uses small string optimization (SSO).
//!
//! This string has 3 allocation possibilities:
//!
//!   1. Small    - embedded buffer is used for up to `kSSOCapacity` characters. This should handle most small
//!                 strings and thus avoid dynamic memory allocation for most use-cases.
//!
//!   2. Large    - string that doesn't fit into an embedded buffer (or string that was truncated from a larger
//!                 buffer) and is owned by AsmJit. When you destroy the string AsmJit would automatically
//!                 release the large buffer.
//!
//!   3. External - like Large (2), however, the large buffer is not owned by AsmJit and won't be released when
//!                 the string is destroyed or reallocated. This is mostly useful for working with larger temporary
//!                 strings allocated on stack or with immutable strings.
class String {
public:
  ASMJIT_NONCOPYABLE(String)

  //! String operation.
  enum class ModifyOp : uint32_t {
    //! Assignment - a new content replaces the current one.
    kAssign = 0,
    //! Append - a new content is appended to the string.
    kAppend = 1
  };

  //! \cond INTERNAL
  enum : uint32_t {
    kLayoutSize = 32,
    kSSOCapacity = kLayoutSize - 2
  };

  //! String type.
  enum Type : uint8_t {
    //! Large string (owned by String).
    kTypeLarge = 0x1Fu,
    //! External string (zone allocated or not owned by String).
    kTypeExternal = 0x20u
  };

  union Raw {
    uint8_t u8[kLayoutSize];
    uint64_t u64[kLayoutSize / sizeof(uint64_t)];
    uintptr_t uptr[kLayoutSize / sizeof(uintptr_t)];
  };

  struct Small {
    uint8_t type;
    char data[kSSOCapacity + 1u];
  };

  struct Large {
    uint8_t type;
    uint8_t reserved[sizeof(uintptr_t) - 1];
    size_t size;
    size_t capacity;
    char* data;
  };

  union {
    uint8_t _type;
    Raw _raw;
    Small _small;
    Large _large;
  };
  //! \endcond

  //! \name Construction & Destruction
  //! \{

  //! Creates a default-initialized string if zero length.
  ASMJIT_INLINE_NODEBUG String() noexcept
    : _small {} {}

  //! Creates a string that takes ownership of the content of the `other` string.
  ASMJIT_INLINE_NODEBUG String(String&& other) noexcept {
    _raw = other._raw;
    other._resetInternal();
  }

  ASMJIT_INLINE_NODEBUG ~String() noexcept {
    reset();
  }

  //! Reset the string into a construction state.
  ASMJIT_API Error reset() noexcept;

  //! \}

  //! \name Overloaded Operators
  //! \{

  inline String& operator=(String&& other) noexcept {
    swap(other);
    other.reset();
    return *this;
  }

  ASMJIT_INLINE_NODEBUG bool operator==(const char* other) const noexcept { return  equals(other); }
  ASMJIT_INLINE_NODEBUG bool operator!=(const char* other) const noexcept { return !equals(other); }

  ASMJIT_INLINE_NODEBUG bool operator==(const String& other) const noexcept { return  equals(other); }
  ASMJIT_INLINE_NODEBUG bool operator!=(const String& other) const noexcept { return !equals(other); }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool isExternal() const noexcept { return _type == kTypeExternal; }
  ASMJIT_INLINE_NODEBUG bool isLargeOrExternal() const noexcept { return _type >= kTypeLarge; }

  //! Tests whether the string is empty.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return size() == 0; }
  //! Returns the size of the string.
  ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return isLargeOrExternal() ? size_t(_large.size) : size_t(_type); }
  //! Returns the capacity of the string.
  ASMJIT_INLINE_NODEBUG size_t capacity() const noexcept { return isLargeOrExternal() ? _large.capacity : size_t(kSSOCapacity); }

  //! Returns the data of the string.
  ASMJIT_INLINE_NODEBUG char* data() noexcept { return isLargeOrExternal() ? _large.data : _small.data; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const char* data() const noexcept { return isLargeOrExternal() ? _large.data : _small.data; }

  ASMJIT_INLINE_NODEBUG char* start() noexcept { return data(); }
  ASMJIT_INLINE_NODEBUG const char* start() const noexcept { return data(); }

  ASMJIT_INLINE_NODEBUG char* end() noexcept { return data() + size(); }
  ASMJIT_INLINE_NODEBUG const char* end() const noexcept { return data() + size(); }

  //! \}

  //! \name String Operations
  //! \{

  //! Swaps the content of this string with `other`.
  ASMJIT_INLINE_NODEBUG void swap(String& other) noexcept {
    std::swap(_raw, other._raw);
  }

  //! Clears the content of the string.
  ASMJIT_API Error clear() noexcept;

  ASMJIT_API char* prepare(ModifyOp op, size_t size) noexcept;

  ASMJIT_API Error _opString(ModifyOp op, const char* str, size_t size = SIZE_MAX) noexcept;
  ASMJIT_API Error _opChar(ModifyOp op, char c) noexcept;
  ASMJIT_API Error _opChars(ModifyOp op, char c, size_t n) noexcept;
  ASMJIT_API Error _opNumber(ModifyOp op, uint64_t i, uint32_t base = 0, size_t width = 0, StringFormatFlags flags = StringFormatFlags::kNone) noexcept;
  ASMJIT_API Error _opHex(ModifyOp op, const void* data, size_t size, char separator = '\0') noexcept;
  ASMJIT_API Error _opFormat(ModifyOp op, const char* fmt, ...) noexcept;
  ASMJIT_API Error _opVFormat(ModifyOp op, const char* fmt, va_list ap) noexcept;

  //! Replaces the current of the string with `data` of the given `size`.
  //!
  //! Null terminated strings can set `size` to `SIZE_MAX`.
  ASMJIT_API Error assign(const char* data, size_t size = SIZE_MAX) noexcept;

  //! Replaces the current of the string with `other` string.
  ASMJIT_INLINE_NODEBUG Error assign(const String& other) noexcept {
    return assign(other.data(), other.size());
  }

  //! Replaces the current of the string by a single `c` character.
  ASMJIT_INLINE_NODEBUG Error assign(char c) noexcept {
    return _opChar(ModifyOp::kAssign, c);
  }

  //! Replaces the current of the string by a `c` character, repeated `n` times.
  ASMJIT_INLINE_NODEBUG Error assignChars(char c, size_t n) noexcept {
    return _opChars(ModifyOp::kAssign, c, n);
  }

  //! Replaces the current of the string by a formatted integer `i` (signed).
  ASMJIT_INLINE_NODEBUG Error assignInt(int64_t i, uint32_t base = 0, size_t width = 0, StringFormatFlags flags = StringFormatFlags::kNone) noexcept {
    return _opNumber(ModifyOp::kAssign, uint64_t(i), base, width, flags | StringFormatFlags::kSigned);
  }

  //! Replaces the current of the string by a formatted integer `i` (unsigned).
  ASMJIT_INLINE_NODEBUG Error assignUInt(uint64_t i, uint32_t base = 0, size_t width = 0, StringFormatFlags flags = StringFormatFlags::kNone) noexcept {
    return _opNumber(ModifyOp::kAssign, i, base, width, flags);
  }

  //! Replaces the current of the string by the given `data` converted to a HEX string.
  ASMJIT_INLINE_NODEBUG Error assignHex(const void* data, size_t size, char separator = '\0') noexcept {
    return _opHex(ModifyOp::kAssign, data, size, separator);
  }

  //! Replaces the current of the string by a formatted string `fmt`.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG Error assignFormat(const char* fmt, Args&&... args) noexcept {
    return _opFormat(ModifyOp::kAssign, fmt, std::forward<Args>(args)...);
  }

  //! Replaces the current of the string by a formatted string `fmt` (va_list version).
  ASMJIT_INLINE_NODEBUG Error assignVFormat(const char* fmt, va_list ap) noexcept {
    return _opVFormat(ModifyOp::kAssign, fmt, ap);
  }

  //! Appends `str` having the given size `size` to the string.
  //!
  //! Null terminated strings can set `size` to `SIZE_MAX`.
  ASMJIT_INLINE_NODEBUG Error append(const char* str, size_t size = SIZE_MAX) noexcept {
    return _opString(ModifyOp::kAppend, str, size);
  }

  //! Appends `other` string to this string.
  ASMJIT_INLINE_NODEBUG Error append(const String& other) noexcept {
    return append(other.data(), other.size());
  }

  //! Appends a single `c` character.
  ASMJIT_INLINE_NODEBUG Error append(char c) noexcept {
    return _opChar(ModifyOp::kAppend, c);
  }

  //! Appends `c` character repeated `n` times.
  ASMJIT_INLINE_NODEBUG Error appendChars(char c, size_t n) noexcept {
    return _opChars(ModifyOp::kAppend, c, n);
  }

  //! Appends a formatted integer `i` (signed).
  ASMJIT_INLINE_NODEBUG Error appendInt(int64_t i, uint32_t base = 0, size_t width = 0, StringFormatFlags flags = StringFormatFlags::kNone) noexcept {
    return _opNumber(ModifyOp::kAppend, uint64_t(i), base, width, flags | StringFormatFlags::kSigned);
  }

  //! Appends a formatted integer `i` (unsigned).
  ASMJIT_INLINE_NODEBUG Error appendUInt(uint64_t i, uint32_t base = 0, size_t width = 0, StringFormatFlags flags = StringFormatFlags::kNone) noexcept {
    return _opNumber(ModifyOp::kAppend, i, base, width, flags);
  }

  //! Appends the given `data` converted to a HEX string.
  ASMJIT_INLINE_NODEBUG Error appendHex(const void* data, size_t size, char separator = '\0') noexcept {
    return _opHex(ModifyOp::kAppend, data, size, separator);
  }

  //! Appends a formatted string `fmt` with `args`.
  template<typename... Args>
  ASMJIT_INLINE_NODEBUG Error appendFormat(const char* fmt, Args&&... args) noexcept {
    return _opFormat(ModifyOp::kAppend, fmt, std::forward<Args>(args)...);
  }

  //! Appends a formatted string `fmt` (va_list version).
  ASMJIT_INLINE_NODEBUG Error appendVFormat(const char* fmt, va_list ap) noexcept {
    return _opVFormat(ModifyOp::kAppend, fmt, ap);
  }

  ASMJIT_API Error padEnd(size_t n, char c = ' ') noexcept;

  //! Truncate the string length into `newSize`.
  ASMJIT_API Error truncate(size_t newSize) noexcept;

  ASMJIT_API bool equals(const char* other, size_t size = SIZE_MAX) const noexcept;
  ASMJIT_INLINE_NODEBUG bool equals(const String& other) const noexcept { return equals(other.data(), other.size()); }

  //! \}

  //! \name Internal Functions
  //! \{

  //! Resets string to embedded and makes it empty (zero length, zero first char)
  //!
  //! \note This is always called internally after an external buffer was released as it zeroes all bytes
  //! used by String's embedded storage.
  inline void _resetInternal() noexcept {
    for (size_t i = 0; i < ASMJIT_ARRAY_SIZE(_raw.uptr); i++)
      _raw.uptr[i] = 0;
  }

  inline void _setSize(size_t newSize) noexcept {
    if (isLargeOrExternal())
      _large.size = newSize;
    else
      _small.type = uint8_t(newSize);
  }

  //! \}
};

//! Temporary string builder, has statically allocated `N` bytes.
template<size_t N>
class StringTmp : public String {
public:
  ASMJIT_NONCOPYABLE(StringTmp)

  //! Embedded data.
  char _embeddedData[Support::alignUp(N + 1, sizeof(size_t))];

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG StringTmp() noexcept {
    _resetToTemporary();
  }

  inline void _resetToTemporary() noexcept {
    _large.type = kTypeExternal;
    _large.capacity = ASMJIT_ARRAY_SIZE(_embeddedData) - 1;
    _large.data = _embeddedData;
    _embeddedData[0] = '\0';
  }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_STRING_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/support.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

// Support - Tests
// ===============

#if defined(ASMJIT_TEST)
template<typename T>
static void testArrays(const T* a, const T* b, size_t size) noexcept {
  for (size_t i = 0; i < size; i++)
    EXPECT_EQ(a[i], b[i])
      .message("Mismatch at %u", unsigned(i));
}

static void testAlignment() noexcept {
  INFO("Support::isAligned()");
  EXPECT_FALSE(Support::isAligned<size_t>(0xFFFF, 4u));
  EXPECT_TRUE(Support::isAligned<size_t>(0xFFF4, 4u));
  EXPECT_TRUE(Support::isAligned<size_t>(0xFFF8, 8u));
  EXPECT_TRUE(Support::isAligned<size_t>(0xFFF0, 16u));

  INFO("Support::alignUp()");
  EXPECT_EQ(Support::alignUp<size_t>(0xFFFF, 4), 0x10000u);
  EXPECT_EQ(Support::alignUp<size_t>(0xFFF4, 4), 0x0FFF4u);
  EXPECT_EQ(Support::alignUp<size_t>(0xFFF8, 8), 0x0FFF8u);
  EXPECT_EQ(Support::alignUp<size_t>(0xFFF0, 16), 0x0FFF0u);
  EXPECT_EQ(Support::alignUp<size_t>(0xFFF0, 32), 0x10000u);

  INFO("Support::alignUpDiff()");
  EXPECT_EQ(Support::alignUpDiff<size_t>(0xFFFF, 4), 1u);
  EXPECT_EQ(Support::alignUpDiff<size_t>(0xFFF4, 4), 0u);
  EXPECT_EQ(Support::alignUpDiff<size_t>(0xFFF8, 8), 0u);
  EXPECT_EQ(Support::alignUpDiff<size_t>(0xFFF0, 16), 0u);
  EXPECT_EQ(Support::alignUpDiff<size_t>(0xFFF0, 32), 16u);

  INFO("Support::alignUpPowerOf2()");
  EXPECT_EQ(Support::alignUpPowerOf2<size_t>(0x0000), 0x00000u);
  EXPECT_EQ(Support::alignUpPowerOf2<size_t>(0xFFFF), 0x10000u);
  EXPECT_EQ(Support::alignUpPowerOf2<size_t>(0xF123), 0x10000u);
  EXPECT_EQ(Support::alignUpPowerOf2<size_t>(0x0F00), 0x01000u);
  EXPECT_EQ(Support::alignUpPowerOf2<size_t>(0x0100), 0x00100u);
  EXPECT_EQ(Support::alignUpPowerOf2<size_t>(0x1001), 0x02000u);
}

static void testBitUtils() noexcept {
  uint32_t i;

  INFO("Support::shl() / shr()");
  EXPECT_EQ(Support::shl(int32_t(0x00001111), 16), int32_t(0x11110000u));
  EXPECT_EQ(Support::shl(uint32_t(0x00001111), 16), uint32_t(0x11110000u));
  EXPECT_EQ(Support::shr(int32_t(0x11110000u), 16), int32_t(0x00001111u));
  EXPECT_EQ(Support::shr(uint32_t(0x11110000u), 16), uint32_t(0x00001111u));
  EXPECT_EQ(Support::sar(int32_t(0xFFFF0000u), 16), int32_t(0xFFFFFFFFu));
  EXPECT_EQ(Support::sar(uint32_t(0xFFFF0000u), 16), uint32_t(0xFFFFFFFFu));

  INFO("Support::blsi()");
  for (i = 0; i < 32; i++) EXPECT_EQ(Support::blsi(uint32_t(1) << i), uint32_t(1) << i);
  for (i = 0; i < 31; i++) EXPECT_EQ(Support::blsi(uint32_t(3) << i), uint32_t(1) << i);
  for (i = 0; i < 64; i++) EXPECT_EQ(Support::blsi(uint64_t(1) << i), uint64_t(1) << i);
  for (i = 0; i < 63; i++) EXPECT_EQ(Support::blsi(uint64_t(3) << i), uint64_t(1) << i);

  INFO("Support::ctz()");
  for (i = 0; i < 32; i++) EXPECT_EQ(Support::Internal::clzFallback(uint32_t(1) << i), 31 - i);
  for (i = 0; i < 64; i++) EXPECT_EQ(Support::Internal::clzFallback(uint64_t(1) << i), 63 - i);
  for (i = 0; i < 32; i++) EXPECT_EQ(Support::Internal::ctzFallback(uint32_t(1) << i), i);
  for (i = 0; i < 64; i++) EXPECT_EQ(Support::Internal::ctzFallback(uint64_t(1) << i), i);
  for (i = 0; i < 32; i++) EXPECT_EQ(Support::clz(uint32_t(1) << i), 31 - i);
  for (i = 0; i < 64; i++) EXPECT_EQ(Support::clz(uint64_t(1) << i), 63 - i);
  for (i = 0; i < 32; i++) EXPECT_EQ(Support::ctz(uint32_t(1) << i), i);
  for (i = 0; i < 64; i++) EXPECT_EQ(Support::ctz(uint64_t(1) << i), i);

  INFO("Support::bitMask()");
  EXPECT_EQ(Support::bitMask(0, 1, 7), 0x83u);
  for (i = 0; i < 32; i++)
    EXPECT_EQ(Support::bitMask(i), (1u << i));

  INFO("Support::bitTest()");
  for (i = 0; i < 32; i++) {
    EXPECT_TRUE(Support::bitTest((1 << i), i))
      .message("Support::bitTest(%X, %u) should return true", (1 << i), i);
  }

  INFO("Support::lsbMask<uint32_t>()");
  for (i = 0; i < 32; i++) {
    uint32_t expectedBits = 0;
    for (uint32_t b = 0; b < i; b++)
      expectedBits |= uint32_t(1) << b;
    EXPECT_EQ(Support::lsbMask<uint32_t>(i), expectedBits);
  }

  INFO("Support::lsbMask<uint64_t>()");
  for (i = 0; i < 64; i++) {
    uint64_t expectedBits = 0;
    for (uint32_t b = 0; b < i; b++)
      expectedBits |= uint64_t(1) << b;
    EXPECT_EQ(Support::lsbMask<uint64_t>(i), expectedBits);
  }

  INFO("Support::popcnt()");
  for (i = 0; i < 32; i++) EXPECT_EQ(Support::popcnt((uint32_t(1) << i)), 1u);
  for (i = 0; i < 64; i++) EXPECT_EQ(Support::popcnt((uint64_t(1) << i)), 1u);
  EXPECT_EQ(Support::popcnt(0x000000F0), 4u);
  EXPECT_EQ(Support::popcnt(0x10101010), 4u);
  EXPECT_EQ(Support::popcnt(0xFF000000), 8u);
  EXPECT_EQ(Support::popcnt(0xFFFFFFF7), 31u);
  EXPECT_EQ(Support::popcnt(0x7FFFFFFF), 31u);

  INFO("Support::isPowerOf2()");
  for (i = 0; i < 64; i++) {
    EXPECT_TRUE(Support::isPowerOf2(uint64_t(1) << i));
    EXPECT_FALSE(Support::isPowerOf2((uint64_t(1) << i) ^ 0x001101));
  }
}

static void testIntUtils() noexcept {
  INFO("Support::byteswap()");
  EXPECT_EQ(Support::byteswap16(0x0102), 0x0201u);
  EXPECT_EQ(Support::byteswap32(0x01020304), 0x04030201u);
  EXPECT_EQ(Support::byteswap32(0x01020304), 0x04030201u);
  EXPECT_EQ(Support::byteswap64(uint64_t(0x0102030405060708)), uint64_t(0x0807060504030201));

  INFO("Support::bytepack()");
  union BytePackData {
    uint8_t bytes[4];
    uint32_t u32;
  } bpdata;

  bpdata.u32 = Support::bytepack32_4x8(0x00, 0x11, 0x22, 0x33);
  EXPECT_EQ(bpdata.bytes[0], 0x00);
  EXPECT_EQ(bpdata.bytes[1], 0x11);
  EXPECT_EQ(bpdata.bytes[2], 0x22);
  EXPECT_EQ(bpdata.bytes[3], 0x33);

  INFO("Support::isBetween()");
  EXPECT_TRUE(Support::isBetween<int>(10 , 10, 20));
  EXPECT_TRUE(Support::isBetween<int>(11 , 10, 20));
  EXPECT_TRUE(Support::isBetween<int>(20 , 10, 20));
  EXPECT_FALSE(Support::isBetween<int>(9  , 10, 20));
  EXPECT_FALSE(Support::isBetween<int>(21 , 10, 20));
  EXPECT_FALSE(Support::isBetween<int>(101, 10, 20));

  INFO("Support::isInt8()");
  EXPECT_TRUE(Support::isInt8(-128));
  EXPECT_TRUE(Support::isInt8( 127));
  EXPECT_FALSE(Support::isInt8(-129));
  EXPECT_FALSE(Support::isInt8( 128));

  INFO("Support::isInt16()");
  EXPECT_TRUE(Support::isInt16(-32768));
  EXPECT_TRUE(Support::isInt16( 32767));
  EXPECT_FALSE(Support::isInt16(-32769));
  EXPECT_FALSE(Support::isInt16( 32768));

  INFO("Support::isInt32()");
  EXPECT_TRUE(Support::isInt32( 2147483647    ));
  EXPECT_TRUE(Support::isInt32(-2147483647 - 1));
  EXPECT_FALSE(Support::isInt32(uint64_t(2147483648u)));
  EXPECT_FALSE(Support::isInt32(uint64_t(0xFFFFFFFFu)));
  EXPECT_FALSE(Support::isInt32(uint64_t(0xFFFFFFFFu) + 1));

  INFO("Support::isUInt8()");
  EXPECT_TRUE(Support::isUInt8(0)  );
  EXPECT_TRUE(Support::isUInt8(255));
  EXPECT_FALSE(Support::isUInt8(256));
  EXPECT_FALSE(Support::isUInt8(-1) );

  INFO("Support::isUInt12()");
  EXPECT_TRUE(Support::isUInt12(0)   );
  EXPECT_TRUE(Support::isUInt12(4095));
  EXPECT_FALSE(Support::isUInt12(4096));
  EXPECT_FALSE(Support::isUInt12(-1)  );

  INFO("Support::isUInt16()");
  EXPECT_TRUE(Support::isUInt16(0)    );
  EXPECT_TRUE(Support::isUInt16(65535));
  EXPECT_FALSE(Support::isUInt16(65536));
  EXPECT_FALSE(Support::isUInt16(-1)   );

  INFO("Support::isUInt32()");
  EXPECT_TRUE(Support::isUInt32(uint64_t(0xFFFFFFFF)));
  EXPECT_FALSE(Support::isUInt32(uint64_t(0xFFFFFFFF) + 1));
  EXPECT_FALSE(Support::isUInt32(-1));
}

static void testReadWrite() noexcept {
  INFO("Support::readX() / writeX()");

  uint8_t arr[32] = { 0 };

  Support::writeU16uBE(arr + 1, 0x0102u);
  Support::writeU16uBE(arr + 3, 0x0304u);
  EXPECT_EQ(Support::readU32uBE(arr + 1), 0x01020304u);
  EXPECT_EQ(Support::readU32uLE(arr + 1), 0x04030201u);
  EXPECT_EQ(Support::readU32uBE(arr + 2), 0x02030400u);
  EXPECT_EQ(Support::readU32uLE(arr + 2), 0x00040302u);

  Support::writeU32uLE(arr + 5, 0x05060708u);
  EXPECT_EQ(Support::readU64uBE(arr + 1), 0x0102030408070605u);
  EXPECT_EQ(Support::readU64uLE(arr + 1), 0x0506070804030201u);

  Support::writeU64uLE(arr + 7, 0x1122334455667788u);
  EXPECT_EQ(Support::readU32uBE(arr + 8), 0x77665544u);
}

static void testBitVector() noexcept {
  INFO("Support::bitVectorOp");
  {
    uint32_t vec[3] = { 0 };
    Support::bitVectorFill(vec, 1, 64);
    EXPECT_EQ(vec[0], 0xFFFFFFFEu);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0x00000001u);

    Support::bitVectorClear(vec, 1, 1);
    EXPECT_EQ(vec[0], 0xFFFFFFFCu);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0x00000001u);

    Support::bitVectorFill(vec, 0, 32);
    EXPECT_EQ(vec[0], 0xFFFFFFFFu);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0x00000001u);

    Support::bitVectorClear(vec, 0, 32);
    EXPECT_EQ(vec[0], 0x00000000u);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0x00000001u);

    Support::bitVectorFill(vec, 1, 30);
    EXPECT_EQ(vec[0], 0x7FFFFFFEu);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0x00000001u);

    Support::bitVectorClear(vec, 1, 95);
    EXPECT_EQ(vec[0], 0x00000000u);
    EXPECT_EQ(vec[1], 0x00000000u);
    EXPECT_EQ(vec[2], 0x00000000u);

    Support::bitVectorFill(vec, 32, 64);
    EXPECT_EQ(vec[0], 0x00000000u);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0xFFFFFFFFu);

    Support::bitVectorSetBit(vec, 1, true);
    EXPECT_EQ(vec[0], 0x00000002u);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0xFFFFFFFFu);

    Support::bitVectorSetBit(vec, 95, false);
    EXPECT_EQ(vec[0], 0x00000002u);
    EXPECT_EQ(vec[1], 0xFFFFFFFFu);
    EXPECT_EQ(vec[2], 0x7FFFFFFFu);

    Support::bitVectorClear(vec, 33, 32);
    EXPECT_EQ(vec[0], 0x00000002u);
    EXPECT_EQ(vec[1], 0x00000001u);
    EXPECT_EQ(vec[2], 0x7FFFFFFEu);
  }

  INFO("Support::bitVectorIndexOf");
  {
    uint32_t vec1[1] = { 0x80000000 };
    EXPECT_EQ(Support::bitVectorIndexOf(vec1, 0, true), 31u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec1, 1, true), 31u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec1, 31, true), 31u);

    uint32_t vec2[2] = { 0x00000000, 0x80000000 };
    EXPECT_EQ(Support::bitVectorIndexOf(vec2, 0, true), 63u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec2, 1, true), 63u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec2, 31, true), 63u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec2, 32, true), 63u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec2, 33, true), 63u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec2, 63, true), 63u);

    uint32_t vec3[3] = { 0x00000001, 0x00000000, 0x80000000 };
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 0, true), 0u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 1, true), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 2, true), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 31, true), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 32, true), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 63, true), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 64, true), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec3, 95, true), 95u);

    uint32_t vec4[3] = { ~vec3[0], ~vec3[1], ~vec3[2] };
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 0, false), 0u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 1, false), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 2, false), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 31, false), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 32, false), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 63, false), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 64, false), 95u);
    EXPECT_EQ(Support::bitVectorIndexOf(vec4, 95, false), 95u);
  }

  INFO("Support::BitWordIterator<uint32_t>");
  {
    Support::BitWordIterator<uint32_t> it(0x80000F01u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 0u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 8u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 9u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 10u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 11u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 31u);
    EXPECT_FALSE(it.hasNext());

    // No bits set.
    it.init(0x00000000u);
    EXPECT_FALSE(it.hasNext());

    // Only first bit set.
    it.init(0x00000001u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 0u);
    EXPECT_FALSE(it.hasNext());

    // Only last bit set (special case).
    it.init(0x80000000u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 31u);
    EXPECT_FALSE(it.hasNext());
  }

  INFO("Support::BitWordIterator<uint64_t>");
  {
    Support::BitWordIterator<uint64_t> it(uint64_t(1) << 63);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 63u);
    EXPECT_FALSE(it.hasNext());
  }

  INFO("Support::BitVectorIterator<uint32_t>");
  {
    // Border cases.
    static const uint32_t bitsNone[] = { 0xFFFFFFFFu };
    Support::BitVectorIterator<uint32_t> it(bitsNone, 0);

    EXPECT_FALSE(it.hasNext());
    it.init(bitsNone, 0, 1);
    EXPECT_FALSE(it.hasNext());
    it.init(bitsNone, 0, 128);
    EXPECT_FALSE(it.hasNext());

    static const uint32_t bits1[] = { 0x80000008u, 0x80000001u, 0x00000000u, 0x80000000u, 0x00000000u, 0x00000000u, 0x00003000u };
    it.init(bits1, ASMJIT_ARRAY_SIZE(bits1));

    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 3u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 31u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 32u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 63u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 127u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 204u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 205u);
    EXPECT_FALSE(it.hasNext());

    it.init(bits1, ASMJIT_ARRAY_SIZE(bits1), 4);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 31u);

    it.init(bits1, ASMJIT_ARRAY_SIZE(bits1), 64);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 127u);

    it.init(bits1, ASMJIT_ARRAY_SIZE(bits1), 127);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 127u);

    static const uint32_t bits2[] = { 0x80000000u, 0x80000000u, 0x00000000u, 0x80000000u };
    it.init(bits2, ASMJIT_ARRAY_SIZE(bits2));

    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 31u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 63u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 127u);
    EXPECT_FALSE(it.hasNext());

    static const uint32_t bits3[] = { 0x00000000u, 0x00000000u, 0x00000000u, 0x00000000u };
    it.init(bits3, ASMJIT_ARRAY_SIZE(bits3));
    EXPECT_FALSE(it.hasNext());

    static const uint32_t bits4[] = { 0x00000000u, 0x00000000u, 0x00000000u, 0x80000000u };
    it.init(bits4, ASMJIT_ARRAY_SIZE(bits4));
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 127u);
    EXPECT_FALSE(it.hasNext());
  }

  INFO("Support::BitVectorIterator<uint64_t>");
  {
    static const uint64_t bits1[] = { 0x80000000u, 0x80000000u, 0x00000000u, 0x80000000u };
    Support::BitVectorIterator<uint64_t> it(bits1, ASMJIT_ARRAY_SIZE(bits1));

    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 31u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 95u);
    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 223u);
    EXPECT_FALSE(it.hasNext());

    static const uint64_t bits2[] = { 0x8000000000000000u, 0, 0, 0 };
    it.init(bits2, ASMJIT_ARRAY_SIZE(bits2));

    EXPECT_TRUE(it.hasNext());
    EXPECT_EQ(it.next(), 63u);
    EXPECT_FALSE(it.hasNext());
  }
}

static void testSorting() noexcept {
  INFO("Support::qSort() - Testing qsort and isort of predefined arrays");
  {
    constexpr size_t kArraySize = 11;

    int ref_[kArraySize] = { -4, -2, -1, 0, 1, 9, 12, 13, 14, 19, 22 };
    int arr1[kArraySize] = { 0, 1, -1, 19, 22, 14, -4, 9, 12, 13, -2 };
    int arr2[kArraySize];

    memcpy(arr2, arr1, kArraySize * sizeof(int));

    Support::iSort(arr1, kArraySize);
    Support::qSort(arr2, kArraySize);
    testArrays(arr1, ref_, kArraySize);
    testArrays(arr2, ref_, kArraySize);
  }

  INFO("Support::qSort() - Testing qsort and isort of artificial arrays");
  {
    constexpr size_t kArraySize = 200;

    int arr1[kArraySize];
    int arr2[kArraySize];
    int ref_[kArraySize];

    for (size_t size = 2; size < kArraySize; size++) {
      for (size_t i = 0; i < size; i++) {
        arr1[i] = int(size - 1 - i);
        arr2[i] = int(size - 1 - i);
        ref_[i] = int(i);
      }

      Support::iSort(arr1, size);
      Support::qSort(arr2, size);
      testArrays(arr1, ref_, size);
      testArrays(arr2, ref_, size);
    }
  }

  INFO("Support::qSort() - Testing qsort and isort with an unstable compare function");
  {
    constexpr size_t kArraySize = 5;

    float arr1[kArraySize] = { 1.0f, 0.0f, 3.0f, -1.0f, std::numeric_limits<float>::quiet_NaN() };
    float arr2[kArraySize] = { };

    memcpy(arr2, arr1, kArraySize * sizeof(float));

    // We don't test as it's undefined where the NaN would be.
    Support::iSort(arr1, kArraySize);
    Support::qSort(arr2, kArraySize);
  }
}

UNIT(support) {
  testAlignment();
  testBitUtils();
  testIntUtils();
  testReadWrite();
  testBitVector();
  testSorting();
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/support.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_SUPPORT_H_INCLUDED
#define ASMJIT_CORE_SUPPORT_H_INCLUDED

#include "../core/globals.h"

#if defined(_MSC_VER)
  #include <intrin.h>
#endif

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_utilities
//! \{

//! Contains support classes and functions that may be used by AsmJit source and header files. Anything defined
//! here is considered internal and should not be used outside of AsmJit and related projects like AsmTK.
namespace Support {

// Support - Basic Traits
// ======================

#if ASMJIT_ARCH_X86
typedef uint8_t FastUInt8;
#else
typedef uint32_t FastUInt8;
#endif

//! \cond INTERNAL
namespace Internal {
  template<typename T, size_t Alignment>
  struct AliasedUInt {};

  template<> struct AliasedUInt<uint16_t, 2> { typedef uint16_t ASMJIT_MAY_ALIAS T; };
  template<> struct AliasedUInt<uint32_t, 4> { typedef uint32_t ASMJIT_MAY_ALIAS T; };
  template<> struct AliasedUInt<uint64_t, 8> { typedef uint64_t ASMJIT_MAY_ALIAS T; };

  template<> struct AliasedUInt<uint16_t, 1> { typedef uint16_t ASMJIT_MAY_ALIAS ASMJIT_ALIGN_TYPE(T, 1); };
  template<> struct AliasedUInt<uint32_t, 1> { typedef uint32_t ASMJIT_MAY_ALIAS ASMJIT_ALIGN_TYPE(T, 1); };
  template<> struct AliasedUInt<uint32_t, 2> { typedef uint32_t ASMJIT_MAY_ALIAS ASMJIT_ALIGN_TYPE(T, 2); };
  template<> struct AliasedUInt<uint64_t, 1> { typedef uint64_t ASMJIT_MAY_ALIAS ASMJIT_ALIGN_TYPE(T, 1); };
  template<> struct AliasedUInt<uint64_t, 2> { typedef uint64_t ASMJIT_MAY_ALIAS ASMJIT_ALIGN_TYPE(T, 2); };
  template<> struct AliasedUInt<uint64_t, 4> { typedef uint64_t ASMJIT_MAY_ALIAS ASMJIT_ALIGN_TYPE(T, 4); };

  // StdInt    - Make an int-type by size (signed or unsigned) that is the
  //             same as types defined by <stdint.h>.
  // Int32Or64 - Make an int-type that has at least 32 bits: [u]int[32|64]_t.

  template<size_t Size, unsigned Unsigned>
  struct StdInt {}; // Fail if not specialized.

  template<> struct StdInt<1, 0> { typedef int8_t   Type; };
  template<> struct StdInt<1, 1> { typedef uint8_t  Type; };
  template<> struct StdInt<2, 0> { typedef int16_t  Type; };
  template<> struct StdInt<2, 1> { typedef uint16_t Type; };
  template<> struct StdInt<4, 0> { typedef int32_t  Type; };
  template<> struct StdInt<4, 1> { typedef uint32_t Type; };
  template<> struct StdInt<8, 0> { typedef int64_t  Type; };
  template<> struct StdInt<8, 1> { typedef uint64_t Type; };

  template<typename T, int Unsigned = std::is_unsigned<T>::value>
  struct Int32Or64 : public StdInt<sizeof(T) <= 4 ? size_t(4) : sizeof(T), Unsigned> {};
}
//! \endcond

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isUnsigned() noexcept { return std::is_unsigned<T>::value; }

//! Casts an integer `x` to either `int32_t` or `int64_t` depending on `T`.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr typename Internal::Int32Or64<T, 0>::Type asInt(const T& x) noexcept {
  return (typename Internal::Int32Or64<T, 0>::Type)x;
}

//! Casts an integer `x` to either `uint32_t` or `uint64_t` depending on `T`.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr typename Internal::Int32Or64<T, 1>::Type asUInt(const T& x) noexcept {
  return (typename Internal::Int32Or64<T, 1>::Type)x;
}

//! Casts an integer `x` to either `int32_t`, uint32_t`, `int64_t`, or `uint64_t` depending on `T`.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr typename Internal::Int32Or64<T>::Type asNormalized(const T& x) noexcept {
  return (typename Internal::Int32Or64<T>::Type)x;
}

//! Casts an integer `x` to the same type as defined by `<stdint.h>`.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr typename Internal::StdInt<sizeof(T), isUnsigned<T>()>::Type asStdInt(const T& x) noexcept {
  return (typename Internal::StdInt<sizeof(T), isUnsigned<T>()>::Type)x;
}

//! A helper class that can be used to iterate over enum values.
template<typename T, T from = (T)0, T to = T::kMaxValue>
struct EnumValues {
  typedef typename std::underlying_type<T>::type ValueType;

  struct Iterator {
    ValueType value;

    ASMJIT_INLINE_NODEBUG T operator*() const { return (T)value; }
    ASMJIT_INLINE_NODEBUG void operator++() { ++value; }

    ASMJIT_INLINE_NODEBUG bool operator==(const Iterator& other) const noexcept { return value == other.value; }
    ASMJIT_INLINE_NODEBUG bool operator!=(const Iterator& other) const noexcept { return value != other.value; }
  };

  ASMJIT_INLINE_NODEBUG Iterator begin() const noexcept { return Iterator{ValueType(from)}; }
  ASMJIT_INLINE_NODEBUG Iterator end() const noexcept { return Iterator{ValueType(to) + 1}; }
};

// Support - BitCast
// =================

//! \cond
namespace Internal {
  template<typename DstT, typename SrcT>
  union BitCastUnion {
    ASMJIT_INLINE_NODEBUG BitCastUnion(SrcT src) noexcept : src(src) {}
    SrcT src;
    DstT dst;
  };
}
//! \endcond

//! Bit-casts from `Src` type to `Dst` type.
//!
//! Useful to bit-cast between integers and floating points.
template<typename Dst, typename Src>
static ASMJIT_INLINE_NODEBUG Dst bitCast(const Src& x) noexcept { return Internal::BitCastUnion<Dst, Src>(x).dst; }

// Support - BitOps
// ================

//! Storage used to store a pack of bits (should by compatible with a machine word).
typedef Internal::StdInt<sizeof(uintptr_t), 1>::Type BitWord;

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr uint32_t bitSizeOf() noexcept { return uint32_t(sizeof(T) * 8u); }

//! Number of bits stored in a single `BitWord`.
static constexpr uint32_t kBitWordSizeInBits = bitSizeOf<BitWord>();

//! Returns `0 - x` in a safe way (no undefined behavior), works for unsigned numbers as well.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T neg(const T& x) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return T(U(0) - U(x));
}

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T allOnes() noexcept { return neg<T>(T(1)); }

//! Returns `x << y` (shift left logical) by explicitly casting `x` to an unsigned type and back.
template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X shl(const X& x, const Y& y) noexcept {
  typedef typename std::make_unsigned<X>::type U;
  return X(U(x) << y);
}

//! Returns `x >> y` (shift right logical) by explicitly casting `x` to an unsigned type and back.
template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X shr(const X& x, const Y& y) noexcept {
  typedef typename std::make_unsigned<X>::type U;
  return X(U(x) >> y);
}

//! Returns `x >> y` (shift right arithmetic) by explicitly casting `x` to a signed type and back.
template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X sar(const X& x, const Y& y) noexcept {
  typedef typename std::make_signed<X>::type S;
  return X(S(x) >> y);
}

template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X ror(const X& x, const Y& y) noexcept {
  typedef typename std::make_unsigned<X>::type U;
  return X((U(x) >> y) | (U(x) << (bitSizeOf<U>() - U(y))));
}

//! Returns `x | (x >> y)` - helper used by some bit manipulation helpers.
template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X or_shr(const X& x, const Y& y) noexcept { return X(x | shr(x, y)); }

//! Returns `x & -x` - extracts lowest set isolated bit (like BLSI instruction).
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T blsi(T x) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return T(U(x) & neg(U(x)));
}

//! Tests whether the given value `x` has `n`th bit set.
template<typename T, typename IndexT>
static ASMJIT_INLINE_NODEBUG constexpr bool bitTest(T x, IndexT n) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return (U(x) & (U(1) << asStdInt(n))) != 0;
}

// Tests whether the given `value` is a consecutive mask of bits that starts at
// the least significant bit.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isLsbMask(const T& value) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return value && ((U(value) + 1u) & U(value)) == 0;
}

// Tests whether the given value contains at least one bit or whether it's a
// bit-mask of consecutive bits.
//
// This function is similar to \ref isLsbMask(), but the mask doesn't have to
// start at a least significant bit.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isConsecutiveMask(const T& value) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return value && isLsbMask((U(value) - 1u) | U(value));
}

//! Generates a trailing bit-mask that has `n` least significant (trailing) bits set.
template<typename T, typename CountT>
static ASMJIT_INLINE_NODEBUG constexpr T lsbMask(const CountT& n) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return (sizeof(U) < sizeof(uintptr_t))
    // Prevent undefined behavior by using a larger type than T.
    ? T(U((uintptr_t(1) << n) - uintptr_t(1)))
    // Prevent undefined behavior by checking `n` before shift.
    : n ? T(shr(allOnes<T>(), bitSizeOf<T>() - size_t(n))) : T(0);
}

//! Generates a leading bit-mask that has `n` most significant (leading) bits set.
template<typename T, typename CountT>
static ASMJIT_INLINE_NODEBUG constexpr T msbMask(const CountT& n) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return (sizeof(U) < sizeof(uintptr_t))
    // Prevent undefined behavior by using a larger type than T.
    ? T(allOnes<uintptr_t>() >> (bitSizeOf<uintptr_t>() - n))
    // Prevent undefined behavior by performing `n & (nBits - 1)` so it's always within the range.
    : T(sar(U(n != 0) << (bitSizeOf<U>() - 1), n ? uint32_t(n - 1) : uint32_t(0)));
}

//! Returns a bit-mask that has `x` bit set.
template<typename Index>
static ASMJIT_INLINE_NODEBUG constexpr uint32_t bitMask(const Index& x) noexcept { return (1u << asUInt(x)); }

//! Returns a bit-mask that has `x` bit set (multiple arguments).
template<typename Index, typename... Args>
static ASMJIT_INLINE_NODEBUG constexpr uint32_t bitMask(const Index& x, Args... args) noexcept { return bitMask(x) | bitMask(args...); }

//! Converts a boolean value `b` to zero or full mask (all bits set).
template<typename DstT, typename SrcT>
static ASMJIT_INLINE_NODEBUG constexpr DstT bitMaskFromBool(SrcT b) noexcept {
  typedef typename std::make_unsigned<DstT>::type U;
  return DstT(U(0) - U(b));
}

//! Tests whether `a & b` is non-zero.
template<typename A, typename B>
static inline constexpr bool test(A a, B b) noexcept { return (asUInt(a) & asUInt(b)) != 0; }

//! \cond
namespace Internal {
  // Fills all trailing bits right from the first most significant bit set.
  static ASMJIT_INLINE_NODEBUG constexpr uint8_t fillTrailingBitsImpl(uint8_t x) noexcept { return or_shr(or_shr(or_shr(x, 1), 2), 4); }
  // Fills all trailing bits right from the first most significant bit set.
  static ASMJIT_INLINE_NODEBUG constexpr uint16_t fillTrailingBitsImpl(uint16_t x) noexcept { return or_shr(or_shr(or_shr(or_shr(x, 1), 2), 4), 8); }
  // Fills all trailing bits right from the first most significant bit set.
  static ASMJIT_INLINE_NODEBUG constexpr uint32_t fillTrailingBitsImpl(uint32_t x) noexcept { return or_shr(or_shr(or_shr(or_shr(or_shr(x, 1), 2), 4), 8), 16); }
  // Fills all trailing bits right from the first most significant bit set.
  static ASMJIT_INLINE_NODEBUG constexpr uint64_t fillTrailingBitsImpl(uint64_t x) noexcept { return or_shr(or_shr(or_shr(or_shr(or_shr(or_shr(x, 1), 2), 4), 8), 16), 32); }
}
//! \endcond

// Fills all trailing bits right from the first most significant bit set.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T fillTrailingBits(const T& x) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return T(Internal::fillTrailingBitsImpl(U(x)));
}

// Support - Count Leading/Trailing Zeros
// ======================================

//! \cond
namespace Internal {
namespace {

template<typename T>
struct BitScanData { T x; uint32_t n; };

template<typename T, uint32_t N>
struct BitScanCalc {
  static ASMJIT_INLINE_NODEBUG constexpr BitScanData<T> advanceLeft(const BitScanData<T>& data, uint32_t n) noexcept {
    return BitScanData<T> { data.x << n, data.n + n };
  }

  static ASMJIT_INLINE_NODEBUG constexpr BitScanData<T> advanceRight(const BitScanData<T>& data, uint32_t n) noexcept {
    return BitScanData<T> { data.x >> n, data.n + n };
  }

  static ASMJIT_INLINE_NODEBUG constexpr BitScanData<T> clz(const BitScanData<T>& data) noexcept {
    return BitScanCalc<T, N / 2>::clz(advanceLeft(data, data.x & (allOnes<T>() << (bitSizeOf<T>() - N)) ? uint32_t(0) : N));
  }

  static ASMJIT_INLINE_NODEBUG constexpr BitScanData<T> ctz(const BitScanData<T>& data) noexcept {
    return BitScanCalc<T, N / 2>::ctz(advanceRight(data, data.x & (allOnes<T>() >> (bitSizeOf<T>() - N)) ? uint32_t(0) : N));
  }
};

template<typename T>
struct BitScanCalc<T, 0> {
  static ASMJIT_INLINE_NODEBUG constexpr BitScanData<T> clz(const BitScanData<T>& ctx) noexcept {
    return BitScanData<T> { 0, ctx.n - uint32_t(ctx.x >> (bitSizeOf<T>() - 1)) };
  }

  static ASMJIT_INLINE_NODEBUG constexpr BitScanData<T> ctz(const BitScanData<T>& ctx) noexcept {
    return BitScanData<T> { 0, ctx.n - uint32_t(ctx.x & 0x1) };
  }
};

template<typename T>
ASMJIT_INLINE_NODEBUG constexpr uint32_t clzFallback(const T& x) noexcept {
  return BitScanCalc<T, bitSizeOf<T>() / 2u>::clz(BitScanData<T>{x, 1}).n;
}

template<typename T>
ASMJIT_INLINE_NODEBUG constexpr uint32_t ctzFallback(const T& x) noexcept {
  return BitScanCalc<T, bitSizeOf<T>() / 2u>::ctz(BitScanData<T>{x, 1}).n;
}

template<typename T> ASMJIT_INLINE_NODEBUG uint32_t clzImpl(const T& x) noexcept { return clzFallback(asUInt(x)); }
template<typename T> ASMJIT_INLINE_NODEBUG uint32_t ctzImpl(const T& x) noexcept { return ctzFallback(asUInt(x)); }

#if !defined(ASMJIT_NO_INTRINSICS)
# if defined(__GNUC__)
template<> ASMJIT_INLINE_NODEBUG uint32_t clzImpl(const uint32_t& x) noexcept { return uint32_t(__builtin_clz(x)); }
template<> ASMJIT_INLINE_NODEBUG uint32_t clzImpl(const uint64_t& x) noexcept { return uint32_t(__builtin_clzll(x)); }
template<> ASMJIT_INLINE_NODEBUG uint32_t ctzImpl(const uint32_t& x) noexcept { return uint32_t(__builtin_ctz(x)); }
template<> ASMJIT_INLINE_NODEBUG uint32_t ctzImpl(const uint64_t& x) noexcept { return uint32_t(__builtin_ctzll(x)); }
# elif defined(_MSC_VER)
template<> ASMJIT_INLINE_NODEBUG uint32_t clzImpl(const uint32_t& x) noexcept { unsigned long i; _BitScanReverse(&i, x); return uint32_t(i ^ 31); }
template<> ASMJIT_INLINE_NODEBUG uint32_t ctzImpl(const uint32_t& x) noexcept { unsigned long i; _BitScanForward(&i, x); return uint32_t(i); }
#  if ASMJIT_ARCH_X86 == 64 || ASMJIT_ARCH_ARM == 64
template<> ASMJIT_INLINE_NODEBUG uint32_t clzImpl(const uint64_t& x) noexcept { unsigned long i; _BitScanReverse64(&i, x); return uint32_t(i ^ 63); }
template<> ASMJIT_INLINE_NODEBUG uint32_t ctzImpl(const uint64_t& x) noexcept { unsigned long i; _BitScanForward64(&i, x); return uint32_t(i); }
#  endif
# endif
#endif

} // {anonymous}
} // {Internal}
//! \endcond

//! Count leading zeros in `x` (returns a position of a first bit set in `x`).
//!
//! \note The input MUST NOT be zero, otherwise the result is undefined.
template<typename T>
static ASMJIT_INLINE_NODEBUG uint32_t clz(T x) noexcept { return Internal::clzImpl(asUInt(x)); }

//! Count trailing zeros in `x` (returns a position of a first bit set in `x`).
//!
//! \note The input MUST NOT be zero, otherwise the result is undefined.
template<typename T>
static ASMJIT_INLINE_NODEBUG uint32_t ctz(T x) noexcept { return Internal::ctzImpl(asUInt(x)); }

template<uint64_t kInput>
struct ConstCTZ {
  static constexpr uint32_t value =
    (kInput & (uint64_t(1) <<  0)) ?  0 :
    (kInput & (uint64_t(1) <<  1)) ?  1 :
    (kInput & (uint64_t(1) <<  2)) ?  2 :
    (kInput & (uint64_t(1) <<  3)) ?  3 :
    (kInput & (uint64_t(1) <<  4)) ?  4 :
    (kInput & (uint64_t(1) <<  5)) ?  5 :
    (kInput & (uint64_t(1) <<  6)) ?  6 :
    (kInput & (uint64_t(1) <<  7)) ?  7 :
    (kInput & (uint64_t(1) <<  8)) ?  8 :
    (kInput & (uint64_t(1) <<  9)) ?  9 :
    (kInput & (uint64_t(1) << 10)) ? 10 :
    (kInput & (uint64_t(1) << 11)) ? 11 :
    (kInput & (uint64_t(1) << 12)) ? 12 :
    (kInput & (uint64_t(1) << 13)) ? 13 :
    (kInput & (uint64_t(1) << 14)) ? 14 :
    (kInput & (uint64_t(1) << 15)) ? 15 :
    (kInput & (uint64_t(1) << 16)) ? 16 :
    (kInput & (uint64_t(1) << 17)) ? 17 :
    (kInput & (uint64_t(1) << 18)) ? 18 :
    (kInput & (uint64_t(1) << 19)) ? 19 :
    (kInput & (uint64_t(1) << 20)) ? 20 :
    (kInput & (uint64_t(1) << 21)) ? 21 :
    (kInput & (uint64_t(1) << 22)) ? 22 :
    (kInput & (uint64_t(1) << 23)) ? 23 :
    (kInput & (uint64_t(1) << 24)) ? 24 :
    (kInput & (uint64_t(1) << 25)) ? 25 :
    (kInput & (uint64_t(1) << 26)) ? 26 :
    (kInput & (uint64_t(1) << 27)) ? 27 :
    (kInput & (uint64_t(1) << 28)) ? 28 :
    (kInput & (uint64_t(1) << 29)) ? 29 :
    (kInput & (uint64_t(1) << 30)) ? 30 :
    (kInput & (uint64_t(1) << 31)) ? 31 :
    (kInput & (uint64_t(1) << 32)) ? 32 :
    (kInput & (uint64_t(1) << 33)) ? 33 :
    (kInput & (uint64_t(1) << 34)) ? 34 :
    (kInput & (uint64_t(1) << 35)) ? 35 :
    (kInput & (uint64_t(1) << 36)) ? 36 :
    (kInput & (uint64_t(1) << 37)) ? 37 :
    (kInput & (uint64_t(1) << 38)) ? 38 :
    (kInput & (uint64_t(1) << 39)) ? 39 :
    (kInput & (uint64_t(1) << 40)) ? 40 :
    (kInput & (uint64_t(1) << 41)) ? 41 :
    (kInput & (uint64_t(1) << 42)) ? 42 :
    (kInput & (uint64_t(1) << 43)) ? 43 :
    (kInput & (uint64_t(1) << 44)) ? 44 :
    (kInput & (uint64_t(1) << 45)) ? 45 :
    (kInput & (uint64_t(1) << 46)) ? 46 :
    (kInput & (uint64_t(1) << 47)) ? 47 :
    (kInput & (uint64_t(1) << 48)) ? 48 :
    (kInput & (uint64_t(1) << 49)) ? 49 :
    (kInput & (uint64_t(1) << 50)) ? 50 :
    (kInput & (uint64_t(1) << 51)) ? 51 :
    (kInput & (uint64_t(1) << 52)) ? 52 :
    (kInput & (uint64_t(1) << 53)) ? 53 :
    (kInput & (uint64_t(1) << 54)) ? 54 :
    (kInput & (uint64_t(1) << 55)) ? 55 :
    (kInput & (uint64_t(1) << 56)) ? 56 :
    (kInput & (uint64_t(1) << 57)) ? 57 :
    (kInput & (uint64_t(1) << 58)) ? 58 :
    (kInput & (uint64_t(1) << 59)) ? 59 :
    (kInput & (uint64_t(1) << 60)) ? 60 :
    (kInput & (uint64_t(1) << 61)) ? 61 :
    (kInput & (uint64_t(1) << 62)) ? 62 :
    (kInput & (uint64_t(1) << 63)) ? 63 : 64;
};

// Support - PopCnt
// ================

// Based on the following resource:
//   http://graphics.stanford.edu/~seander/bithacks.html
//
// Alternatively, for a very small number of bits in `x`:
//   uint32_t n = 0;
//   while (x) {
//     x &= x - 1;
//     n++;
//   }
//   return n;

//! \cond
namespace Internal {
  static ASMJIT_INLINE_NODEBUG uint32_t constPopcntImpl(uint32_t x) noexcept {
    x = x - ((x >> 1) & 0x55555555u);
    x = (x & 0x33333333u) + ((x >> 2) & 0x33333333u);
    return (((x + (x >> 4)) & 0x0F0F0F0Fu) * 0x01010101u) >> 24;
  }

  static ASMJIT_INLINE_NODEBUG uint32_t constPopcntImpl(uint64_t x) noexcept {
#if ASMJIT_ARCH_BITS >= 64
    x = x - ((x >> 1) & 0x5555555555555555u);
    x = (x & 0x3333333333333333u) + ((x >> 2) & 0x3333333333333333u);
    return uint32_t((((x + (x >> 4)) & 0x0F0F0F0F0F0F0F0Fu) * 0x0101010101010101u) >> 56);
#else
    return constPopcntImpl(uint32_t(x >> 32)) +
           constPopcntImpl(uint32_t(x & 0xFFFFFFFFu));
#endif
  }

  static ASMJIT_INLINE_NODEBUG uint32_t popcntImpl(uint32_t x) noexcept {
#if defined(__GNUC__)
    return uint32_t(__builtin_popcount(x));
#else
    return constPopcntImpl(asUInt(x));
#endif
  }

  static ASMJIT_INLINE_NODEBUG uint32_t popcntImpl(uint64_t x) noexcept {
#if defined(__GNUC__)
    return uint32_t(__builtin_popcountll(x));
#else
    return constPopcntImpl(asUInt(x));
#endif
  }
}
//! \endcond

//! Calculates count of bits in `x`.
template<typename T>
static ASMJIT_INLINE_NODEBUG uint32_t popcnt(T x) noexcept { return Internal::popcntImpl(asUInt(x)); }

//! Calculates count of bits in `x` (useful in constant expressions).
template<typename T>
static ASMJIT_INLINE_NODEBUG uint32_t constPopcnt(T x) noexcept { return Internal::constPopcntImpl(asUInt(x)); }

// Support - Min/Max
// =================

// NOTE: These are constexpr `min()` and `max()` implementations that are not
// exactly the same as `std::min()` and `std::max()`. The return value is not
// a reference to `a` or `b` but it's a new value instead.

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T min(const T& a, const T& b) noexcept { return b < a ? b : a; }

template<typename T, typename... Args>
static ASMJIT_INLINE_NODEBUG constexpr T min(const T& a, const T& b, Args&&... args) noexcept { return min(min(a, b), std::forward<Args>(args)...); }

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T max(const T& a, const T& b) noexcept { return a < b ? b : a; }

template<typename T, typename... Args>
static ASMJIT_INLINE_NODEBUG constexpr T max(const T& a, const T& b, Args&&... args) noexcept { return max(max(a, b), std::forward<Args>(args)...); }

// Support - Immediate Helpers
// ===========================

namespace Internal {
  template<typename T, bool IsFloat>
  struct ImmConv {
    static ASMJIT_INLINE_NODEBUG int64_t fromT(const T& x) noexcept { return int64_t(x); }
    static ASMJIT_INLINE_NODEBUG T toT(int64_t x) noexcept { return T(uint64_t(x) & Support::allOnes<typename std::make_unsigned<T>::type>()); }
  };

  template<typename T>
  struct ImmConv<T, true> {
    static ASMJIT_INLINE_NODEBUG int64_t fromT(const T& x) noexcept { return int64_t(bitCast<int64_t>(double(x))); }
    static ASMJIT_INLINE_NODEBUG T toT(int64_t x) noexcept { return T(bitCast<double>(x)); }
  };
}

template<typename T>
static ASMJIT_INLINE_NODEBUG int64_t immediateFromT(const T& x) noexcept { return Internal::ImmConv<T, std::is_floating_point<T>::value>::fromT(x); }

template<typename T>
static ASMJIT_INLINE_NODEBUG T immediateToT(int64_t x) noexcept { return Internal::ImmConv<T, std::is_floating_point<T>::value>::toT(x); }

// Support - Overflow Arithmetic
// =============================

//! \cond
namespace Internal {
  template<typename T>
  inline T addOverflowFallback(T x, T y, FastUInt8* of) noexcept {
    typedef typename std::make_unsigned<T>::type U;

    U result = U(U(x) + U(y));
    *of = FastUInt8(*of | FastUInt8(isUnsigned<T>() ? result < U(x) : T((U(x) ^ ~U(y)) & (U(x) ^ result)) < 0));
    return T(result);
  }

  template<typename T>
  inline T subOverflowFallback(T x, T y, FastUInt8* of) noexcept {
    typedef typename std::make_unsigned<T>::type U;

    U result = U(x) - U(y);
    *of = FastUInt8(*of | FastUInt8(isUnsigned<T>() ? result > U(x) : T((U(x) ^ U(y)) & (U(x) ^ result)) < 0));
    return T(result);
  }

  template<typename T>
  inline T mulOverflowFallback(T x, T y, FastUInt8* of) noexcept {
    typedef typename Internal::StdInt<sizeof(T) * 2, isUnsigned<T>()>::Type I;
    typedef typename std::make_unsigned<I>::type U;

    U mask = allOnes<U>();
    if (std::is_signed<T>::value) {
      U prod = U(I(x)) * U(I(y));
      *of = FastUInt8(*of | FastUInt8(I(prod) < I(std::numeric_limits<T>::lowest()) || I(prod) > I(std::numeric_limits<T>::max())));
      return T(I(prod & mask));
    }
    else {
      U prod = U(x) * U(y);
      *of = FastUInt8(*of | FastUInt8((prod & ~mask) != 0));
      return T(prod & mask);
    }
  }

  template<>
  inline int64_t mulOverflowFallback(int64_t x, int64_t y, FastUInt8* of) noexcept {
    int64_t result = int64_t(uint64_t(x) * uint64_t(y));
    *of = FastUInt8(*of | FastUInt8(x && (result / x != y)));
    return result;
  }

  template<>
  inline uint64_t mulOverflowFallback(uint64_t x, uint64_t y, FastUInt8* of) noexcept {
    uint64_t result = x * y;
    *of = FastUInt8(*of | FastUInt8(y != 0 && allOnes<uint64_t>() / y < x));
    return result;
  }

  // These can be specialized.
  template<typename T> inline T addOverflowImpl(const T& x, const T& y, FastUInt8* of) noexcept { return addOverflowFallback(x, y, of); }
  template<typename T> inline T subOverflowImpl(const T& x, const T& y, FastUInt8* of) noexcept { return subOverflowFallback(x, y, of); }
  template<typename T> inline T mulOverflowImpl(const T& x, const T& y, FastUInt8* of) noexcept { return mulOverflowFallback(x, y, of); }

#if defined(__GNUC__) && !defined(ASMJIT_NO_INTRINSICS)
#if defined(__clang__) || __GNUC__ >= 5
#define ASMJIT_ARITH_OVERFLOW_SPECIALIZE(FUNC, T, RESULT_T, BUILTIN)     \
    template<>                                                             \
    inline T FUNC(const T& x, const T& y, FastUInt8* of) noexcept {        \
      RESULT_T result;                                                     \
      *of = FastUInt8(*of | (BUILTIN((RESULT_T)x, (RESULT_T)y, &result))); \
      return T(result);                                                    \
    }
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(addOverflowImpl, int32_t , int               , __builtin_sadd_overflow  )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(addOverflowImpl, uint32_t, unsigned int      , __builtin_uadd_overflow  )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(addOverflowImpl, int64_t , long long         , __builtin_saddll_overflow)
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(addOverflowImpl, uint64_t, unsigned long long, __builtin_uaddll_overflow)
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(subOverflowImpl, int32_t , int               , __builtin_ssub_overflow  )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(subOverflowImpl, uint32_t, unsigned int      , __builtin_usub_overflow  )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(subOverflowImpl, int64_t , long long         , __builtin_ssubll_overflow)
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(subOverflowImpl, uint64_t, unsigned long long, __builtin_usubll_overflow)
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(mulOverflowImpl, int32_t , int               , __builtin_smul_overflow  )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(mulOverflowImpl, uint32_t, unsigned int      , __builtin_umul_overflow  )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(mulOverflowImpl, int64_t , long long         , __builtin_smulll_overflow)
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(mulOverflowImpl, uint64_t, unsigned long long, __builtin_umulll_overflow)
#undef ASMJIT_ARITH_OVERFLOW_SPECIALIZE
#endif
#endif

  // There is a bug in MSVC that makes these specializations unusable, maybe in the future...
#if defined(_MSC_VER) && 0
#define ASMJIT_ARITH_OVERFLOW_SPECIALIZE(FUNC, T, ALT_T, BUILTIN)        \
    template<>                                                             \
    inline T FUNC(T x, T y, FastUInt8* of) noexcept {                      \
      ALT_T result;                                                        \
      *of = FastUInt8(*of | BUILTIN(0, (ALT_T)x, (ALT_T)y, &result));      \
      return T(result);                                                    \
    }
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(addOverflowImpl, uint32_t, unsigned int      , _addcarry_u32 )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(subOverflowImpl, uint32_t, unsigned int      , _subborrow_u32)
#if ARCH_BITS >= 64
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(addOverflowImpl, uint64_t, unsigned __int64  , _addcarry_u64 )
  ASMJIT_ARITH_OVERFLOW_SPECIALIZE(subOverflowImpl, uint64_t, unsigned __int64  , _subborrow_u64)
#endif
#undef ASMJIT_ARITH_OVERFLOW_SPECIALIZE
#endif
} // {Internal}
//! \endcond

template<typename T>
static inline T addOverflow(const T& x, const T& y, FastUInt8* of) noexcept { return T(Internal::addOverflowImpl(asStdInt(x), asStdInt(y), of)); }

template<typename T>
static inline T subOverflow(const T& x, const T& y, FastUInt8* of) noexcept { return T(Internal::subOverflowImpl(asStdInt(x), asStdInt(y), of)); }

template<typename T>
static inline T mulOverflow(const T& x, const T& y, FastUInt8* of) noexcept { return T(Internal::mulOverflowImpl(asStdInt(x), asStdInt(y), of)); }

// Support - Alignment
// ===================

template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr bool isAligned(X base, Y alignment) noexcept {
  typedef typename Internal::StdInt<sizeof(X), 1>::Type U;
  return ((U)base % (U)alignment) == 0;
}

//! Tests whether the `x` is a power of two (only one bit is set).
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isPowerOf2(T x) noexcept {
  typedef typename std::make_unsigned<T>::type U;
  return x && !(U(x) & (U(x) - U(1)));
}

template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X alignUp(X x, Y alignment) noexcept {
  typedef typename Internal::StdInt<sizeof(X), 1>::Type U;
  return (X)( ((U)x + ((U)(alignment) - 1u)) & ~((U)(alignment) - 1u) );
}

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T alignUpPowerOf2(T x) noexcept {
  typedef typename Internal::StdInt<sizeof(T), 1>::Type U;
  return (T)(fillTrailingBits(U(x) - 1u) + 1u);
}

//! Returns either zero or a positive difference between `base` and `base` when
//! aligned to `alignment`.
template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr typename Internal::StdInt<sizeof(X), 1>::Type alignUpDiff(X base, Y alignment) noexcept {
  typedef typename Internal::StdInt<sizeof(X), 1>::Type U;
  return alignUp(U(base), alignment) - U(base);
}

template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X alignDown(X x, Y alignment) noexcept {
  typedef typename Internal::StdInt<sizeof(X), 1>::Type U;
  return (X)( (U)x & ~((U)(alignment) - 1u) );
}

// Support - NumGranularized
// =========================

//! Calculates the number of elements that would be required if `base` is
//! granularized by `granularity`. This function can be used to calculate
//! the number of BitWords to represent N bits, for example.
template<typename X, typename Y>
static ASMJIT_INLINE_NODEBUG constexpr X numGranularized(X base, Y granularity) noexcept {
  typedef typename Internal::StdInt<sizeof(X), 1>::Type U;
  return X((U(base) + U(granularity) - 1) / U(granularity));
}

// Support - IsBetween
// ===================

//! Checks whether `x` is greater than or equal to `a` and lesser than or equal to `b`.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isBetween(const T& x, const T& a, const T& b) noexcept {
  return x >= a && x <= b;
}

// Support - IsInt & IsUInt
// ========================

//! Checks whether the given integer `x` can be casted to a 4-bit signed integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isInt4(T x) noexcept {
  typedef typename std::make_signed<T>::type S;
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? isBetween<S>(S(x), -8, 7) : U(x) <= U(7u);
}

//! Checks whether the given integer `x` can be casted to a 7-bit signed integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isInt7(T x) noexcept {
  typedef typename std::make_signed<T>::type S;
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? isBetween<S>(S(x), -64, 63) : U(x) <= U(63u);
}

//! Checks whether the given integer `x` can be casted to an 8-bit signed integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isInt8(T x) noexcept {
  typedef typename std::make_signed<T>::type S;
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? sizeof(T) <= 1 || isBetween<S>(S(x), -128, 127) : U(x) <= U(127u);
}

//! Checks whether the given integer `x` can be casted to a 9-bit signed integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isInt9(T x) noexcept {
  typedef typename std::make_signed<T>::type S;
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? sizeof(T) <= 1 || isBetween<S>(S(x), -256, 255)
                                  : sizeof(T) <= 1 || U(x) <= U(255u);
}

//! Checks whether the given integer `x` can be casted to a 10-bit signed integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isInt10(T x) noexcept {
  typedef typename std::make_signed<T>::type S;
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? sizeof(T) <= 1 || isBetween<S>(S(x), -512, 511)
                                  : sizeof(T) <= 1 || U(x) <= U(511u);
}

//! Checks whether the given integer `x` can be casted to a 16-bit signed integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isInt16(T x) noexcept {
  typedef typename std::make_signed<T>::type S;
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? sizeof(T) <= 2 || isBetween<S>(S(x), -32768, 32767)
                                  : sizeof(T) <= 1 || U(x) <= U(32767u);
}

//! Checks whether the given integer `x` can be casted to a 32-bit signed integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isInt32(T x) noexcept {
  typedef typename std::make_signed<T>::type S;
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? sizeof(T) <= 4 || isBetween<S>(S(x), -2147483647 - 1, 2147483647)
                                  : sizeof(T) <= 2 || U(x) <= U(2147483647u);
}

//! Checks whether the given integer `x` can be casted to a 4-bit unsigned integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt4(T x) noexcept {
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? x >= T(0) && x <= T(15)
                                  : U(x) <= U(15u);
}

//! Checks whether the given integer `x` can be casted to an 8-bit unsigned integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt8(T x) noexcept {
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? (sizeof(T) <= 1 || T(x) <= T(255)) && x >= T(0)
                                  : (sizeof(T) <= 1 || U(x) <= U(255u));
}

//! Checks whether the given integer `x` can be casted to a 12-bit unsigned integer (ARM specific).
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt12(T x) noexcept {
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? (sizeof(T) <= 1 || T(x) <= T(4095)) && x >= T(0)
                                  : (sizeof(T) <= 1 || U(x) <= U(4095u));
}

//! Checks whether the given integer `x` can be casted to a 16-bit unsigned integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt16(T x) noexcept {
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? (sizeof(T) <= 2 || T(x) <= T(65535)) && x >= T(0)
                                  : (sizeof(T) <= 2 || U(x) <= U(65535u));
}

//! Checks whether the given integer `x` can be casted to a 32-bit unsigned integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt32(T x) noexcept {
  typedef typename std::make_unsigned<T>::type U;

  return std::is_signed<T>::value ? (sizeof(T) <= 4 || T(x) <= T(4294967295u)) && x >= T(0)
                                  : (sizeof(T) <= 4 || U(x) <= U(4294967295u));
}

//! Checks whether the given integer `x` can be casted to a 32-bit unsigned integer.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr bool isIntOrUInt32(T x) noexcept {
  return sizeof(T) <= 4 ? true : (uint32_t(uint64_t(x) >> 32) + 1u) <= 1u;
}

static bool ASMJIT_INLINE_NODEBUG isEncodableOffset32(int32_t offset, uint32_t nBits) noexcept {
  uint32_t nRev = 32 - nBits;
  return Support::sar(Support::shl(offset, nRev), nRev) == offset;
}

static bool ASMJIT_INLINE_NODEBUG isEncodableOffset64(int64_t offset, uint32_t nBits) noexcept {
  uint32_t nRev = 64 - nBits;
  return Support::sar(Support::shl(offset, nRev), nRev) == offset;
}

// Support - ByteSwap
// ==================

static ASMJIT_INLINE_NODEBUG uint16_t byteswap16(uint16_t x) noexcept {
  return uint16_t(((x >> 8) & 0xFFu) | ((x & 0xFFu) << 8));
}

static ASMJIT_INLINE_NODEBUG uint32_t byteswap32(uint32_t x) noexcept {
  return (x << 24) | (x >> 24) | ((x << 8) & 0x00FF0000u) | ((x >> 8) & 0x0000FF00);
}

static ASMJIT_INLINE_NODEBUG uint64_t byteswap64(uint64_t x) noexcept {
#if (defined(__GNUC__) || defined(__clang__)) && !defined(ASMJIT_NO_INTRINSICS)
  return uint64_t(__builtin_bswap64(uint64_t(x)));
#elif defined(_MSC_VER) && !defined(ASMJIT_NO_INTRINSICS)
  return uint64_t(_byteswap_uint64(uint64_t(x)));
#else
  return (uint64_t(byteswap32(uint32_t(uint64_t(x) >> 32        )))      ) |
         (uint64_t(byteswap32(uint32_t(uint64_t(x) & 0xFFFFFFFFu))) << 32) ;
#endif
}

// Support - BytePack & Unpack
// ===========================

//! Pack four 8-bit integer into a 32-bit integer as it is an array of `{b0,b1,b2,b3}`.
static ASMJIT_INLINE_NODEBUG constexpr uint32_t bytepack32_4x8(uint32_t a, uint32_t b, uint32_t c, uint32_t d) noexcept {
  return ASMJIT_ARCH_LE ? (a | (b << 8) | (c << 16) | (d << 24))
                        : (d | (c << 8) | (b << 16) | (a << 24));
}

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr uint32_t unpackU32At0(T x) noexcept { return ASMJIT_ARCH_LE ? uint32_t(uint64_t(x) & 0xFFFFFFFFu) : uint32_t(uint64_t(x) >> 32); }
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr uint32_t unpackU32At1(T x) noexcept { return ASMJIT_ARCH_BE ? uint32_t(uint64_t(x) & 0xFFFFFFFFu) : uint32_t(uint64_t(x) >> 32); }

// Support - Position of byte (in bit-shift)
// =========================================

static ASMJIT_INLINE_NODEBUG uint32_t byteShiftOfDWordStruct(uint32_t index) noexcept {
  return ASMJIT_ARCH_LE ? index * 8 : (uint32_t(sizeof(uint32_t)) - 1u - index) * 8;
}

// Support - String Utilities
// ==========================

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T asciiToLower(T c) noexcept { return T(c ^ T(T(c >= T('A') && c <= T('Z')) << 5)); }

template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr T asciiToUpper(T c) noexcept { return T(c ^ T(T(c >= T('a') && c <= T('z')) << 5)); }

static ASMJIT_INLINE_NODEBUG size_t strLen(const char* s, size_t maxSize) noexcept {
  size_t i = 0;
  while (i < maxSize && s[i] != '\0')
    i++;
  return i;
}

static ASMJIT_INLINE_NODEBUG constexpr uint32_t hashRound(uint32_t hash, uint32_t c) noexcept { return hash * 65599 + c; }

// Gets a hash of the given string `data` of size `size`. Size must be valid
// as this function doesn't check for a null terminator and allows it in the
// middle of the string.
static ASMJIT_INLINE_NODEBUG uint32_t hashString(const char* data, size_t size) noexcept {
  uint32_t hashCode = 0;
  for (uint32_t i = 0; i < size; i++)
    hashCode = hashRound(hashCode, uint8_t(data[i]));
  return hashCode;
}

static ASMJIT_INLINE_NODEBUG const char* findPackedString(const char* p, uint32_t id) noexcept {
  uint32_t i = 0;
  while (i < id) {
    while (p[0])
      p++;
    p++;
    i++;
  }
  return p;
}

//! Compares two string views.
static ASMJIT_FORCE_INLINE int compareStringViews(const char* aData, size_t aSize, const char* bData, size_t bSize) noexcept {
  size_t size = Support::min(aSize, bSize);

  for (size_t i = 0; i < size; i++) {
    int c = int(uint8_t(aData[i])) - int(uint8_t(bData[i]));
    if (c != 0)
      return c;
  }

  return int(aSize) - int(bSize);
}

// Support - Memory Read Access - 8 Bits
// =====================================

static ASMJIT_INLINE_NODEBUG uint8_t readU8(const void* p) noexcept { return static_cast<const uint8_t*>(p)[0]; }
static ASMJIT_INLINE_NODEBUG int8_t readI8(const void* p) noexcept { return static_cast<const int8_t*>(p)[0]; }

// Support - Memory Read Access - 16 Bits
// ======================================

template<ByteOrder BO, size_t Alignment>
static ASMJIT_INLINE_NODEBUG uint16_t readU16x(const void* p) noexcept {
  typedef typename Internal::AliasedUInt<uint16_t, Alignment>::T U16AlignedToN;
  uint16_t x = static_cast<const U16AlignedToN*>(p)[0];
  return BO == ByteOrder::kNative ? x : byteswap16(x);
}

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint16_t readU16u(const void* p) noexcept { return readU16x<ByteOrder::kNative, Alignment>(p); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint16_t readU16uLE(const void* p) noexcept { return readU16x<ByteOrder::kLE, Alignment>(p); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint16_t readU16uBE(const void* p) noexcept { return readU16x<ByteOrder::kBE, Alignment>(p); }

static ASMJIT_INLINE_NODEBUG uint16_t readU16a(const void* p) noexcept { return readU16x<ByteOrder::kNative, 2>(p); }
static ASMJIT_INLINE_NODEBUG uint16_t readU16aLE(const void* p) noexcept { return readU16x<ByteOrder::kLE, 2>(p); }
static ASMJIT_INLINE_NODEBUG uint16_t readU16aBE(const void* p) noexcept { return readU16x<ByteOrder::kBE, 2>(p); }

template<ByteOrder BO, size_t Alignment>
static ASMJIT_INLINE_NODEBUG int16_t readI16x(const void* p) noexcept { return int16_t(readU16x<BO, Alignment>(p)); }

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int16_t readI16u(const void* p) noexcept { return int16_t(readU16x<ByteOrder::kNative, Alignment>(p)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int16_t readI16uLE(const void* p) noexcept { return int16_t(readU16x<ByteOrder::kLE, Alignment>(p)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int16_t readI16uBE(const void* p) noexcept { return int16_t(readU16x<ByteOrder::kBE, Alignment>(p)); }

static ASMJIT_INLINE_NODEBUG int16_t readI16a(const void* p) noexcept { return int16_t(readU16x<ByteOrder::kNative, 2>(p)); }
static ASMJIT_INLINE_NODEBUG int16_t readI16aLE(const void* p) noexcept { return int16_t(readU16x<ByteOrder::kLE, 2>(p)); }
static ASMJIT_INLINE_NODEBUG int16_t readI16aBE(const void* p) noexcept { return int16_t(readU16x<ByteOrder::kBE, 2>(p)); }

// Support - Memory Read Access - 24 Bits
// ======================================

template<ByteOrder BO = ByteOrder::kNative>
static inline uint32_t readU24u(const void* p) noexcept {
  uint32_t b0 = readU8(static_cast<const uint8_t*>(p) + (BO == ByteOrder::kLE ? 2u : 0u));
  uint32_t b1 = readU8(static_cast<const uint8_t*>(p) + 1u);
  uint32_t b2 = readU8(static_cast<const uint8_t*>(p) + (BO == ByteOrder::kLE ? 0u : 2u));
  return (b0 << 16) | (b1 << 8) | b2;
}

static inline uint32_t readU24uLE(const void* p) noexcept { return readU24u<ByteOrder::kLE>(p); }
static inline uint32_t readU24uBE(const void* p) noexcept { return readU24u<ByteOrder::kBE>(p); }

// Support - Memory Read Access - 32 Bits
// ======================================

template<ByteOrder BO, size_t Alignment>
static ASMJIT_INLINE_NODEBUG uint32_t readU32x(const void* p) noexcept {
  typedef typename Internal::AliasedUInt<uint32_t, Alignment>::T U32AlignedToN;
  uint32_t x = static_cast<const U32AlignedToN*>(p)[0];
  return BO == ByteOrder::kNative ? x : byteswap32(x);
}

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint32_t readU32u(const void* p) noexcept { return readU32x<ByteOrder::kNative, Alignment>(p); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint32_t readU32uLE(const void* p) noexcept { return readU32x<ByteOrder::kLE, Alignment>(p); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint32_t readU32uBE(const void* p) noexcept { return readU32x<ByteOrder::kBE, Alignment>(p); }

static ASMJIT_INLINE_NODEBUG uint32_t readU32a(const void* p) noexcept { return readU32x<ByteOrder::kNative, 4>(p); }
static ASMJIT_INLINE_NODEBUG uint32_t readU32aLE(const void* p) noexcept { return readU32x<ByteOrder::kLE, 4>(p); }
static ASMJIT_INLINE_NODEBUG uint32_t readU32aBE(const void* p) noexcept { return readU32x<ByteOrder::kBE, 4>(p); }

template<ByteOrder BO, size_t Alignment>
static ASMJIT_INLINE_NODEBUG uint32_t readI32x(const void* p) noexcept { return int32_t(readU32x<BO, Alignment>(p)); }

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int32_t readI32u(const void* p) noexcept { return int32_t(readU32x<ByteOrder::kNative, Alignment>(p)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int32_t readI32uLE(const void* p) noexcept { return int32_t(readU32x<ByteOrder::kLE, Alignment>(p)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int32_t readI32uBE(const void* p) noexcept { return int32_t(readU32x<ByteOrder::kBE, Alignment>(p)); }

static ASMJIT_INLINE_NODEBUG int32_t readI32a(const void* p) noexcept { return int32_t(readU32x<ByteOrder::kNative, 4>(p)); }
static ASMJIT_INLINE_NODEBUG int32_t readI32aLE(const void* p) noexcept { return int32_t(readU32x<ByteOrder::kLE, 4>(p)); }
static ASMJIT_INLINE_NODEBUG int32_t readI32aBE(const void* p) noexcept { return int32_t(readU32x<ByteOrder::kBE, 4>(p)); }

// Support - Memory Read Access - 64 Bits
// ======================================

template<ByteOrder BO, size_t Alignment>
static ASMJIT_INLINE_NODEBUG uint64_t readU64x(const void* p) noexcept {
  typedef typename Internal::AliasedUInt<uint64_t, Alignment>::T U64AlignedToN;
  uint64_t x = static_cast<const U64AlignedToN*>(p)[0];
  return BO == ByteOrder::kNative ? x : byteswap64(x);
}

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint64_t readU64u(const void* p) noexcept { return readU64x<ByteOrder::kNative, Alignment>(p); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint64_t readU64uLE(const void* p) noexcept { return readU64x<ByteOrder::kLE, Alignment>(p); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG uint64_t readU64uBE(const void* p) noexcept { return readU64x<ByteOrder::kBE, Alignment>(p); }

static ASMJIT_INLINE_NODEBUG uint64_t readU64a(const void* p) noexcept { return readU64x<ByteOrder::kNative, 8>(p); }
static ASMJIT_INLINE_NODEBUG uint64_t readU64aLE(const void* p) noexcept { return readU64x<ByteOrder::kLE, 8>(p); }
static ASMJIT_INLINE_NODEBUG uint64_t readU64aBE(const void* p) noexcept { return readU64x<ByteOrder::kBE, 8>(p); }

template<ByteOrder BO, size_t Alignment>
static ASMJIT_INLINE_NODEBUG int64_t readI64x(const void* p) noexcept { return int64_t(readU64x<BO, Alignment>(p)); }

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int64_t readI64u(const void* p) noexcept { return int64_t(readU64x<ByteOrder::kNative, Alignment>(p)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int64_t readI64uLE(const void* p) noexcept { return int64_t(readU64x<ByteOrder::kLE, Alignment>(p)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG int64_t readI64uBE(const void* p) noexcept { return int64_t(readU64x<ByteOrder::kBE, Alignment>(p)); }

static ASMJIT_INLINE_NODEBUG int64_t readI64a(const void* p) noexcept { return int64_t(readU64x<ByteOrder::kNative, 8>(p)); }
static ASMJIT_INLINE_NODEBUG int64_t readI64aLE(const void* p) noexcept { return int64_t(readU64x<ByteOrder::kLE, 8>(p)); }
static ASMJIT_INLINE_NODEBUG int64_t readI64aBE(const void* p) noexcept { return int64_t(readU64x<ByteOrder::kBE, 8>(p)); }

// Support - Memory Write Access - 8 Bits
// ======================================

static ASMJIT_INLINE_NODEBUG void writeU8(void* p, uint8_t x) noexcept { static_cast<uint8_t*>(p)[0] = x; }
static ASMJIT_INLINE_NODEBUG void writeI8(void* p, int8_t x) noexcept { static_cast<int8_t*>(p)[0] = x; }

// Support - Memory Write Access - 16 Bits
// =======================================

template<ByteOrder BO = ByteOrder::kNative, size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU16x(void* p, uint16_t x) noexcept {
  typedef typename Internal::AliasedUInt<uint16_t, Alignment>::T U16AlignedToN;
  static_cast<U16AlignedToN*>(p)[0] = BO == ByteOrder::kNative ? x : byteswap16(x);
}

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU16uLE(void* p, uint16_t x) noexcept { writeU16x<ByteOrder::kLE, Alignment>(p, x); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU16uBE(void* p, uint16_t x) noexcept { writeU16x<ByteOrder::kBE, Alignment>(p, x); }

static ASMJIT_INLINE_NODEBUG void writeU16a(void* p, uint16_t x) noexcept { writeU16x<ByteOrder::kNative, 2>(p, x); }
static ASMJIT_INLINE_NODEBUG void writeU16aLE(void* p, uint16_t x) noexcept { writeU16x<ByteOrder::kLE, 2>(p, x); }
static ASMJIT_INLINE_NODEBUG void writeU16aBE(void* p, uint16_t x) noexcept { writeU16x<ByteOrder::kBE, 2>(p, x); }


template<ByteOrder BO = ByteOrder::kNative, size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI16x(void* p, int16_t x) noexcept { writeU16x<BO, Alignment>(p, uint16_t(x)); }

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI16uLE(void* p, int16_t x) noexcept { writeU16x<ByteOrder::kLE, Alignment>(p, uint16_t(x)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI16uBE(void* p, int16_t x) noexcept { writeU16x<ByteOrder::kBE, Alignment>(p, uint16_t(x)); }

static ASMJIT_INLINE_NODEBUG void writeI16a(void* p, int16_t x) noexcept { writeU16x<ByteOrder::kNative, 2>(p, uint16_t(x)); }
static ASMJIT_INLINE_NODEBUG void writeI16aLE(void* p, int16_t x) noexcept { writeU16x<ByteOrder::kLE, 2>(p, uint16_t(x)); }
static ASMJIT_INLINE_NODEBUG void writeI16aBE(void* p, int16_t x) noexcept { writeU16x<ByteOrder::kBE, 2>(p, uint16_t(x)); }

// Support - Memory Write Access - 24 Bits
// =======================================

template<ByteOrder BO = ByteOrder::kNative>
static inline void writeU24u(void* p, uint32_t v) noexcept {
  static_cast<uint8_t*>(p)[0] = uint8_t((v >> (BO == ByteOrder::kLE ?  0 : 16)) & 0xFFu);
  static_cast<uint8_t*>(p)[1] = uint8_t((v >> 8) & 0xFFu);
  static_cast<uint8_t*>(p)[2] = uint8_t((v >> (BO == ByteOrder::kLE ? 16 :  0)) & 0xFFu);
}

static inline void writeU24uLE(void* p, uint32_t v) noexcept { writeU24u<ByteOrder::kLE>(p, v); }
static inline void writeU24uBE(void* p, uint32_t v) noexcept { writeU24u<ByteOrder::kBE>(p, v); }

// Support - Memory Write Access - 32 Bits
// =======================================

template<ByteOrder BO = ByteOrder::kNative, size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU32x(void* p, uint32_t x) noexcept {
  typedef typename Internal::AliasedUInt<uint32_t, Alignment>::T U32AlignedToN;
  static_cast<U32AlignedToN*>(p)[0] = (BO == ByteOrder::kNative) ? x : Support::byteswap32(x);
}

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU32u(void* p, uint32_t x) noexcept { writeU32x<ByteOrder::kNative, Alignment>(p, x); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU32uLE(void* p, uint32_t x) noexcept { writeU32x<ByteOrder::kLE, Alignment>(p, x); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU32uBE(void* p, uint32_t x) noexcept { writeU32x<ByteOrder::kBE, Alignment>(p, x); }

static ASMJIT_INLINE_NODEBUG void writeU32a(void* p, uint32_t x) noexcept { writeU32x<ByteOrder::kNative, 4>(p, x); }
static ASMJIT_INLINE_NODEBUG void writeU32aLE(void* p, uint32_t x) noexcept { writeU32x<ByteOrder::kLE, 4>(p, x); }
static ASMJIT_INLINE_NODEBUG void writeU32aBE(void* p, uint32_t x) noexcept { writeU32x<ByteOrder::kBE, 4>(p, x); }

template<ByteOrder BO = ByteOrder::kNative, size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI32x(void* p, int32_t x) noexcept { writeU32x<BO, Alignment>(p, uint32_t(x)); }

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI32u(void* p, int32_t x) noexcept { writeU32x<ByteOrder::kNative, Alignment>(p, uint32_t(x)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI32uLE(void* p, int32_t x) noexcept { writeU32x<ByteOrder::kLE, Alignment>(p, uint32_t(x)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI32uBE(void* p, int32_t x) noexcept { writeU32x<ByteOrder::kBE, Alignment>(p, uint32_t(x)); }

static ASMJIT_INLINE_NODEBUG void writeI32a(void* p, int32_t x) noexcept { writeU32x<ByteOrder::kNative, 4>(p, uint32_t(x)); }
static ASMJIT_INLINE_NODEBUG void writeI32aLE(void* p, int32_t x) noexcept { writeU32x<ByteOrder::kLE, 4>(p, uint32_t(x)); }
static ASMJIT_INLINE_NODEBUG void writeI32aBE(void* p, int32_t x) noexcept { writeU32x<ByteOrder::kBE, 4>(p, uint32_t(x)); }

// Support - Memory Write Access - 64 Bits
// =======================================

template<ByteOrder BO = ByteOrder::kNative, size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU64x(void* p, uint64_t x) noexcept {
  typedef typename Internal::AliasedUInt<uint64_t, Alignment>::T U64AlignedToN;
  static_cast<U64AlignedToN*>(p)[0] = BO == ByteOrder::kNative ? x : byteswap64(x);
}

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU64u(void* p, uint64_t x) noexcept { writeU64x<ByteOrder::kNative, Alignment>(p, x); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU64uLE(void* p, uint64_t x) noexcept { writeU64x<ByteOrder::kLE, Alignment>(p, x); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeU64uBE(void* p, uint64_t x) noexcept { writeU64x<ByteOrder::kBE, Alignment>(p, x); }

static ASMJIT_INLINE_NODEBUG void writeU64a(void* p, uint64_t x) noexcept { writeU64x<ByteOrder::kNative, 8>(p, x); }
static ASMJIT_INLINE_NODEBUG void writeU64aLE(void* p, uint64_t x) noexcept { writeU64x<ByteOrder::kLE, 8>(p, x); }
static ASMJIT_INLINE_NODEBUG void writeU64aBE(void* p, uint64_t x) noexcept { writeU64x<ByteOrder::kBE, 8>(p, x); }

template<ByteOrder BO = ByteOrder::kNative, size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI64x(void* p, int64_t x) noexcept { writeU64x<BO, Alignment>(p, uint64_t(x)); }

template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI64u(void* p, int64_t x) noexcept { writeU64x<ByteOrder::kNative, Alignment>(p, uint64_t(x)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI64uLE(void* p, int64_t x) noexcept { writeU64x<ByteOrder::kLE, Alignment>(p, uint64_t(x)); }
template<size_t Alignment = 1>
static ASMJIT_INLINE_NODEBUG void writeI64uBE(void* p, int64_t x) noexcept { writeU64x<ByteOrder::kBE, Alignment>(p, uint64_t(x)); }

static ASMJIT_INLINE_NODEBUG void writeI64a(void* p, int64_t x) noexcept { writeU64x<ByteOrder::kNative, 8>(p, uint64_t(x)); }
static ASMJIT_INLINE_NODEBUG void writeI64aLE(void* p, int64_t x) noexcept { writeU64x<ByteOrder::kLE, 8>(p, uint64_t(x)); }
static ASMJIT_INLINE_NODEBUG void writeI64aBE(void* p, int64_t x) noexcept { writeU64x<ByteOrder::kBE, 8>(p, uint64_t(x)); }

// Support - Operators
// ===================

//! \cond INTERNAL
struct Set    { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { DebugUtils::unused(x); return  y; } };
struct SetNot { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { DebugUtils::unused(x); return ~y; } };
struct And    { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return  x &  y; } };
struct AndNot { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return  x & ~y; } };
struct NotAnd { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return ~x &  y; } };
struct Or     { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return  x |  y; } };
struct Xor    { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return  x ^  y; } };
struct Add    { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return  x +  y; } };
struct Sub    { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return  x -  y; } };
struct Min    { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return min<T>(x, y); } };
struct Max    { template<typename T> static ASMJIT_INLINE_NODEBUG T op(T x, T y) noexcept { return max<T>(x, y); } };
//! \endcond

// Support - BitWordIterator
// =========================

//! Iterates over each bit in a number which is set to 1.
//!
//! Example of use:
//!
//! ```
//! uint32_t bitsToIterate = 0x110F;
//! Support::BitWordIterator<uint32_t> it(bitsToIterate);
//!
//! while (it.hasNext()) {
//!   uint32_t bitIndex = it.next();
//!   std::printf("Bit at %u is set\n", unsigned(bitIndex));
//! }
//! ```
template<typename T>
class BitWordIterator {
public:
  ASMJIT_INLINE_NODEBUG explicit BitWordIterator(T bitWord) noexcept
    : _bitWord(bitWord) {}

  ASMJIT_INLINE_NODEBUG void init(T bitWord) noexcept { _bitWord = bitWord; }
  ASMJIT_INLINE_NODEBUG bool hasNext() const noexcept { return _bitWord != 0; }

  ASMJIT_FORCE_INLINE uint32_t next() noexcept {
    ASMJIT_ASSERT(_bitWord != 0);
    uint32_t index = ctz(_bitWord);
    _bitWord &= T(_bitWord - 1);
    return index;
  }

  T _bitWord;
};

// Support - BitVectorOps
// ======================

//! \cond
namespace Internal {
  template<typename T, class OperatorT, class FullWordOpT>
  static ASMJIT_FORCE_INLINE void bitVectorOp(T* buf, size_t index, size_t count) noexcept {
    if (count == 0)
      return;

    const size_t kTSizeInBits = bitSizeOf<T>();
    size_t vecIndex = index / kTSizeInBits; // T[]
    size_t bitIndex = index % kTSizeInBits; // T[][]

    buf += vecIndex;

    // The first BitWord requires special handling to preserve bits outside the fill region.
    const T kFillMask = allOnes<T>();
    size_t firstNBits = min<size_t>(kTSizeInBits - bitIndex, count);

    buf[0] = OperatorT::op(buf[0], (kFillMask >> (kTSizeInBits - firstNBits)) << bitIndex);
    buf++;
    count -= firstNBits;

    // All bits between the first and last affected BitWords can be just filled.
    while (count >= kTSizeInBits) {
      buf[0] = FullWordOpT::op(buf[0], kFillMask);
      buf++;
      count -= kTSizeInBits;
    }

    // The last BitWord requires special handling as well
    if (count)
      buf[0] = OperatorT::op(buf[0], kFillMask >> (kTSizeInBits - count));
  }
}
//! \endcond

//! Sets bit in a bit-vector `buf` at `index`.
template<typename T>
static ASMJIT_INLINE_NODEBUG bool bitVectorGetBit(T* buf, size_t index) noexcept {
  const size_t kTSizeInBits = bitSizeOf<T>();

  size_t vecIndex = index / kTSizeInBits;
  size_t bitIndex = index % kTSizeInBits;

  return bool((buf[vecIndex] >> bitIndex) & 0x1u);
}

//! Sets bit in a bit-vector `buf` at `index` to `value`.
template<typename T>
static ASMJIT_INLINE_NODEBUG void bitVectorSetBit(T* buf, size_t index, bool value) noexcept {
  const size_t kTSizeInBits = bitSizeOf<T>();

  size_t vecIndex = index / kTSizeInBits;
  size_t bitIndex = index % kTSizeInBits;

  T bitMask = T(1u) << bitIndex;
  if (value)
    buf[vecIndex] |= bitMask;
  else
    buf[vecIndex] &= ~bitMask;
}

//! Sets bit in a bit-vector `buf` at `index` to `value`.
template<typename T>
static ASMJIT_INLINE_NODEBUG void bitVectorFlipBit(T* buf, size_t index) noexcept {
  const size_t kTSizeInBits = bitSizeOf<T>();

  size_t vecIndex = index / kTSizeInBits;
  size_t bitIndex = index % kTSizeInBits;

  T bitMask = T(1u) << bitIndex;
  buf[vecIndex] ^= bitMask;
}

//! Fills `count` bits in bit-vector `buf` starting at bit-index `index`.
template<typename T>
static ASMJIT_INLINE_NODEBUG void bitVectorFill(T* buf, size_t index, size_t count) noexcept { Internal::bitVectorOp<T, Or, Set>(buf, index, count); }

//! Clears `count` bits in bit-vector `buf` starting at bit-index `index`.
template<typename T>
static ASMJIT_INLINE_NODEBUG void bitVectorClear(T* buf, size_t index, size_t count) noexcept { Internal::bitVectorOp<T, AndNot, SetNot>(buf, index, count); }

template<typename T>
static ASMJIT_FORCE_INLINE size_t bitVectorIndexOf(T* buf, size_t start, bool value) noexcept {
  const size_t kTSizeInBits = bitSizeOf<T>();
  size_t vecIndex = start / kTSizeInBits; // T[]
  size_t bitIndex = start % kTSizeInBits; // T[][]

  T* p = buf + vecIndex;

  // We always look for zeros, if value is `true` we have to flip all bits before the search.
  const T kFillMask = allOnes<T>();
  const T kFlipMask = value ? T(0) : kFillMask;

  // The first BitWord requires special handling as there are some bits we want to ignore.
  T bits = (*p ^ kFlipMask) & (kFillMask << bitIndex);
  for (;;) {
    if (bits)
      return (size_t)(p - buf) * kTSizeInBits + ctz(bits);
    bits = *++p ^ kFlipMask;
  }
}

// Support - BitVectorIterator
// ===========================

template<typename T>
class BitVectorIterator {
public:
  const T* _ptr;
  size_t _idx;
  size_t _end;
  T _current;

  ASMJIT_INLINE_NODEBUG BitVectorIterator(const BitVectorIterator& other) noexcept = default;

  ASMJIT_INLINE_NODEBUG BitVectorIterator(const T* data, size_t numBitWords, size_t start = 0) noexcept {
    init(data, numBitWords, start);
  }

  ASMJIT_FORCE_INLINE void init(const T* data, size_t numBitWords, size_t start = 0) noexcept {
    const T* ptr = data + (start / bitSizeOf<T>());
    size_t idx = alignDown(start, bitSizeOf<T>());
    size_t end = numBitWords * bitSizeOf<T>();

    T bitWord = T(0);
    if (idx < end) {
      bitWord = *ptr++ & (allOnes<T>() << (start % bitSizeOf<T>()));
      while (!bitWord && (idx += bitSizeOf<T>()) < end)
        bitWord = *ptr++;
    }

    _ptr = ptr;
    _idx = idx;
    _end = end;
    _current = bitWord;
  }

  ASMJIT_INLINE_NODEBUG bool hasNext() const noexcept {
    return _current != T(0);
  }

  ASMJIT_FORCE_INLINE size_t next() noexcept {
    T bitWord = _current;
    ASMJIT_ASSERT(bitWord != T(0));

    uint32_t bit = ctz(bitWord);
    bitWord &= T(bitWord - 1u);

    size_t n = _idx + bit;
    while (!bitWord && (_idx += bitSizeOf<T>()) < _end)
      bitWord = *_ptr++;

    _current = bitWord;
    return n;
  }

  ASMJIT_FORCE_INLINE size_t peekNext() const noexcept {
    ASMJIT_ASSERT(_current != T(0));
    return _idx + ctz(_current);
  }
};

// Support - BitVectorOpIterator
// =============================

template<typename T, class OperatorT>
class BitVectorOpIterator {
public:
  enum : uint32_t {
    kTSizeInBits = bitSizeOf<T>()
  };

  const T* _aPtr;
  const T* _bPtr;
  size_t _idx;
  size_t _end;
  T _current;

  ASMJIT_INLINE_NODEBUG BitVectorOpIterator(const T* aData, const T* bData, size_t numBitWords, size_t start = 0) noexcept {
    init(aData, bData, numBitWords, start);
  }

  ASMJIT_FORCE_INLINE void init(const T* aData, const T* bData, size_t numBitWords, size_t start = 0) noexcept {
    const T* aPtr = aData + (start / bitSizeOf<T>());
    const T* bPtr = bData + (start / bitSizeOf<T>());
    size_t idx = alignDown(start, bitSizeOf<T>());
    size_t end = numBitWords * bitSizeOf<T>();

    T bitWord = T(0);
    if (idx < end) {
      bitWord = OperatorT::op(*aPtr++, *bPtr++) & (allOnes<T>() << (start % bitSizeOf<T>()));
      while (!bitWord && (idx += kTSizeInBits) < end)
        bitWord = OperatorT::op(*aPtr++, *bPtr++);
    }

    _aPtr = aPtr;
    _bPtr = bPtr;
    _idx = idx;
    _end = end;
    _current = bitWord;
  }

  ASMJIT_INLINE_NODEBUG bool hasNext() noexcept {
    return _current != T(0);
  }

  ASMJIT_FORCE_INLINE size_t next() noexcept {
    T bitWord = _current;
    ASMJIT_ASSERT(bitWord != T(0));

    uint32_t bit = ctz(bitWord);
    bitWord &= T(bitWord - 1u);

    size_t n = _idx + bit;
    while (!bitWord && (_idx += kTSizeInBits) < _end)
      bitWord = OperatorT::op(*_aPtr++, *_bPtr++);

    _current = bitWord;
    return n;
  }
};

// Support - Sorting
// =================

//! Sort order.
enum class SortOrder : uint32_t {
  //!< Ascending order.
  kAscending  = 0,
  //!< Descending order.
  kDescending = 1
};

//! A helper class that provides comparison of any user-defined type that
//! implements `<` and `>` operators (primitive types are supported as well).
template<SortOrder kOrder = SortOrder::kAscending>
struct Compare {
  template<typename A, typename B>
  ASMJIT_INLINE_NODEBUG int operator()(const A& a, const B& b) const noexcept {
    return kOrder == SortOrder::kAscending ? int(a > b) - int(a < b) : int(a < b) - int(a > b);
  }
};

//! Insertion sort.
template<typename T, typename CompareT = Compare<SortOrder::kAscending>>
static inline void iSort(T* base, size_t size, const CompareT& cmp = CompareT()) noexcept {
  for (T* pm = base + 1; pm < base + size; pm++)
    for (T* pl = pm; pl > base && cmp(pl[-1], pl[0]) > 0; pl--)
      std::swap(pl[-1], pl[0]);
}

//! \cond
namespace Internal {
  //! Quick-sort implementation.
  template<typename T, class CompareT>
  struct QSortImpl {
    enum : size_t {
      kStackSize = 64 * 2,
      kISortThreshold = 7
    };

    // Based on "PDCLib - Public Domain C Library" and rewritten to C++.
    static void sort(T* base, size_t size, const CompareT& cmp) noexcept {
      T* end = base + size;
      T* stack[kStackSize];
      T** stackptr = stack;

      for (;;) {
        if ((size_t)(end - base) > kISortThreshold) {
          // We work from second to last - first will be pivot element.
          T* pi = base + 1;
          T* pj = end - 1;
          std::swap(base[(size_t)(end - base) / 2], base[0]);

          if (cmp(*pi  , *pj  ) > 0) std::swap(*pi  , *pj  );
          if (cmp(*base, *pj  ) > 0) std::swap(*base, *pj  );
          if (cmp(*pi  , *base) > 0) std::swap(*pi  , *base);

          // Now we have the median for pivot element, entering main loop.
          for (;;) {
            while (pi < pj   && cmp(*++pi, *base) < 0) continue; // Move `i` right until `*i >= pivot`.
            while (pj > base && cmp(*--pj, *base) > 0) continue; // Move `j` left  until `*j <= pivot`.

            if (pi > pj) break;
            std::swap(*pi, *pj);
          }

          // Move pivot into correct place.
          std::swap(*base, *pj);

          // Larger subfile base / end to stack, sort smaller.
          if (pj - base > end - pi) {
            // Left is larger.
            *stackptr++ = base;
            *stackptr++ = pj;
            base = pi;
          }
          else {
            // Right is larger.
            *stackptr++ = pi;
            *stackptr++ = end;
            end = pj;
          }
          ASMJIT_ASSERT(stackptr <= stack + kStackSize);
        }
        else {
          // UB sanitizer doesn't like applying offset to a nullptr base.
          if (base != end)
            iSort(base, (size_t)(end - base), cmp);

          if (stackptr == stack)
            break;

          end = *--stackptr;
          base = *--stackptr;
        }
      }
    }
  };
}
//! \endcond

//! Quick sort implementation.
//!
//! The main reason to provide a custom qsort implementation is that we needed something that will
//! never throw `bad_alloc` exception. This implementation doesn't use dynamic memory allocation.
template<typename T, class CompareT = Compare<SortOrder::kAscending>>
static ASMJIT_INLINE_NODEBUG void qSort(T* base, size_t size, const CompareT& cmp = CompareT()) noexcept {
  Internal::QSortImpl<T, CompareT>::sort(base, size, cmp);
}

// Support - ReverseIterator
// =========================

//! Reverse iterator to avoid including `<iterator>` header for iteration over arrays, specialized for
//! AsmJit use (noexcept by design).
template<typename T>
class ArrayReverseIterator {
public:
  //! \name Members
  //! \{

  T* _ptr {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG constexpr ArrayReverseIterator() noexcept = default;
  ASMJIT_INLINE_NODEBUG constexpr ArrayReverseIterator(const ArrayReverseIterator& other) noexcept = default;
  ASMJIT_INLINE_NODEBUG constexpr ArrayReverseIterator(T* ptr) noexcept : _ptr(ptr) {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG ArrayReverseIterator& operator=(const ArrayReverseIterator& other) noexcept = default;

  ASMJIT_INLINE_NODEBUG bool operator==(const T* other) const noexcept { return _ptr == other; }
  ASMJIT_INLINE_NODEBUG bool operator==(const ArrayReverseIterator& other) const noexcept { return _ptr == other._ptr; }

  ASMJIT_INLINE_NODEBUG bool operator!=(const T* other) const noexcept { return _ptr != other; }
  ASMJIT_INLINE_NODEBUG bool operator!=(const ArrayReverseIterator& other) const noexcept { return _ptr != other._ptr; }

  ASMJIT_INLINE_NODEBUG bool operator<(const T* other) const noexcept { return _ptr < other; }
  ASMJIT_INLINE_NODEBUG bool operator<(const ArrayReverseIterator& other) const noexcept { return _ptr < other._ptr; }

  ASMJIT_INLINE_NODEBUG bool operator<=(const T* other) const noexcept { return _ptr <= other; }
  ASMJIT_INLINE_NODEBUG bool operator<=(const ArrayReverseIterator& other) const noexcept { return _ptr <= other._ptr; }

  ASMJIT_INLINE_NODEBUG bool operator>(const T* other) const noexcept { return _ptr > other; }
  ASMJIT_INLINE_NODEBUG bool operator>(const ArrayReverseIterator& other) const noexcept { return _ptr > other._ptr; }

  ASMJIT_INLINE_NODEBUG bool operator>=(const T* other) const noexcept { return _ptr >= other; }
  ASMJIT_INLINE_NODEBUG bool operator>=(const ArrayReverseIterator& other) const noexcept { return _ptr >= other._ptr; }

  ASMJIT_INLINE_NODEBUG ArrayReverseIterator& operator++() noexcept { _ptr--; return *this; }
  ASMJIT_INLINE_NODEBUG ArrayReverseIterator& operator--() noexcept { _ptr++; return *this; }

  ASMJIT_INLINE_NODEBUG ArrayReverseIterator operator++(int) noexcept { ArrayReverseIterator prev(*this); _ptr--; return prev; }
  ASMJIT_INLINE_NODEBUG ArrayReverseIterator operator--(int) noexcept { ArrayReverseIterator prev(*this); _ptr++; return prev; }

  template<typename Diff> ASMJIT_INLINE_NODEBUG ArrayReverseIterator operator+(const Diff& n) noexcept { return ArrayReverseIterator(_ptr -= n); }
  template<typename Diff> ASMJIT_INLINE_NODEBUG ArrayReverseIterator operator-(const Diff& n) noexcept { return ArrayReverseIterator(_ptr += n); }

  template<typename Diff> ASMJIT_INLINE_NODEBUG ArrayReverseIterator& operator+=(const Diff& n) noexcept { _ptr -= n; return *this; }
  template<typename Diff> ASMJIT_INLINE_NODEBUG ArrayReverseIterator& operator-=(const Diff& n) noexcept { _ptr += n; return *this; }

  ASMJIT_INLINE_NODEBUG constexpr T& operator*() const noexcept { return _ptr[-1]; }
  ASMJIT_INLINE_NODEBUG constexpr T* operator->() const noexcept { return &_ptr[-1]; }

  template<typename Diff> ASMJIT_INLINE_NODEBUG T& operator[](const Diff& n) noexcept { return *(_ptr - n - 1); }

  ASMJIT_INLINE_NODEBUG operator T*() const noexcept { return _ptr; }

  //! \}
};

// Support - Array
// ===============

//! Array type, similar to std::array<T, N>, with the possibility to use enums in operator[].
//!
//! \note The array has C semantics - the elements in the array are not initialized.
template<typename T, size_t N>
struct Array {
  //! \name Members
  //! \{

  //! The underlying array data, use \ref data() to access it.
  T _data[N];

  //! \}

  //! \cond
  // std compatibility.
  typedef T value_type;
  typedef size_t size_type;
  typedef ptrdiff_t difference_type;

  typedef value_type& reference;
  typedef const value_type& const_reference;

  typedef value_type* pointer;
  typedef const value_type* const_pointer;

  typedef pointer iterator;
  typedef const_pointer const_iterator;
  //! \endcond

  //! \name Overloaded Operators
  //! \{

  template<typename Index>
  inline T& operator[](const Index& index) noexcept {
    typedef typename Internal::StdInt<sizeof(Index), 1>::Type U;
    ASMJIT_ASSERT(U(index) < N);
    return _data[U(index)];
  }

  template<typename Index>
  inline const T& operator[](const Index& index) const noexcept {
    typedef typename Internal::StdInt<sizeof(Index), 1>::Type U;
    ASMJIT_ASSERT(U(index) < N);
    return _data[U(index)];
  }

  inline bool operator==(const Array& other) const noexcept {
    for (size_t i = 0; i < N; i++)
      if (_data[i] != other._data[i])
        return false;
    return true;
  }

  inline bool operator!=(const Array& other) const noexcept {
    return !operator==(other);
  }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return false; }
  ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return N; }

  ASMJIT_INLINE_NODEBUG T* data() noexcept { return _data; }
  ASMJIT_INLINE_NODEBUG const T* data() const noexcept { return _data; }

  ASMJIT_INLINE_NODEBUG T& front() noexcept { return _data[0]; }
  ASMJIT_INLINE_NODEBUG const T& front() const noexcept { return _data[0]; }

  ASMJIT_INLINE_NODEBUG T& back() noexcept { return _data[N - 1]; }
  ASMJIT_INLINE_NODEBUG const T& back() const noexcept { return _data[N - 1]; }

  ASMJIT_INLINE_NODEBUG T* begin() noexcept { return _data; }
  ASMJIT_INLINE_NODEBUG T* end() noexcept { return _data + N; }

  ASMJIT_INLINE_NODEBUG const T* begin() const noexcept { return _data; }
  ASMJIT_INLINE_NODEBUG const T* end() const noexcept { return _data + N; }

  ASMJIT_INLINE_NODEBUG const T* cbegin() const noexcept { return _data; }
  ASMJIT_INLINE_NODEBUG const T* cend() const noexcept { return _data + N; }

  //! \}

  //! \name Utilities
  //! \{

  inline void swap(Array& other) noexcept {
    for (size_t i = 0; i < N; i++)
      std::swap(_data[i], other._data[i]);
  }

  inline void fill(const T& value) noexcept {
    for (size_t i = 0; i < N; i++)
      _data[i] = value;
  }

  inline void copyFrom(const Array& other) noexcept {
    for (size_t i = 0; i < N; i++)
      _data[i] = other._data[i];
  }

  template<typename Operator>
  inline void combine(const Array& other) noexcept {
    for (size_t i = 0; i < N; i++)
      _data[i] = Operator::op(_data[i], other._data[i]);
  }

  template<typename Operator>
  inline T aggregate(T initialValue = T()) const noexcept {
    T value = initialValue;
    for (size_t i = 0; i < N; i++)
      value = Operator::op(value, _data[i]);
    return value;
  }

  template<typename Fn>
  inline void forEach(Fn&& fn) noexcept {
    for (size_t i = 0; i < N; i++)
      fn(_data[i]);
  }
  //! \}
};

// Support::Temporary
// ==================

//! Used to pass a temporary buffer to:
//!
//!   - Containers that use user-passed buffer as an initial storage (still can grow).
//!   - Zone allocator that would use the temporary buffer as a first block.
struct Temporary {
  //! \name Members
  //! \{

  void* _data;
  size_t _size;

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG constexpr Temporary(const Temporary& other) noexcept = default;
  ASMJIT_INLINE_NODEBUG constexpr Temporary(void* data, size_t size) noexcept
    : _data(data),
      _size(size) {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG Temporary& operator=(const Temporary& other) noexcept = default;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the data storage.
  template<typename T = void>
  ASMJIT_INLINE_NODEBUG constexpr T* data() const noexcept { return static_cast<T*>(_data); }
  //! Returns the data storage size in bytes.
  ASMJIT_INLINE_NODEBUG constexpr size_t size() const noexcept { return _size; }

  //! \}
};

} // {Support}

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_SUPPORT_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/support_p.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_SUPPORT_P_H_INCLUDED
#define ASMJIT_CORE_SUPPORT_P_H_INCLUDED

#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_utilities
//! \{

namespace Support {

//! \cond INTERNAL
//! \endcond

} // {Support}

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_SUPPORT_P_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/target.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/target.h"

ASMJIT_BEGIN_NAMESPACE

Target::Target() noexcept
  : _environment{},
    _cpuFeatures{} {}
Target::~Target() noexcept {}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/target.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_TARGET_H_INCLUDED
#define ASMJIT_CORE_TARGET_H_INCLUDED

#include "../core/archtraits.h"
#include "../core/cpuinfo.h"
#include "../core/func.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

//! Target is an abstract class that describes a machine code target.
class ASMJIT_VIRTAPI Target {
public:
  ASMJIT_BASE_CLASS(Target)
  ASMJIT_NONCOPYABLE(Target)

  //! Target environment information.
  Environment _environment;
  //! Target CPU features.
  CpuFeatures _cpuFeatures;

  //! \name Construction & Destruction
  //! \{

  //! Creates a `Target` instance.
  ASMJIT_API Target() noexcept;
  //! Destroys the `Target` instance.
  ASMJIT_API virtual ~Target() noexcept;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns target's environment.
  ASMJIT_INLINE_NODEBUG const Environment& environment() const noexcept { return _environment; }
  //! Returns the target architecture.
  ASMJIT_INLINE_NODEBUG Arch arch() const noexcept { return _environment.arch(); }
  //! Returns the target sub-architecture.
  ASMJIT_INLINE_NODEBUG SubArch subArch() const noexcept { return _environment.subArch(); }

  //! Returns target CPU features.
  ASMJIT_INLINE_NODEBUG const CpuFeatures& cpuFeatures() const noexcept { return _cpuFeatures; }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_TARGET_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/type.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/misc_p.h"
#include "../core/type.h"

ASMJIT_BEGIN_NAMESPACE

namespace TypeUtils {

template<uint32_t Index>
struct ScalarOfTypeId {
  enum : uint32_t {
    kTypeId = uint32_t(
      isScalar(TypeId(Index)) ? TypeId(Index) :
      isMask8 (TypeId(Index)) ? TypeId::kUInt8 :
      isMask16(TypeId(Index)) ? TypeId::kUInt16 :
      isMask32(TypeId(Index)) ? TypeId::kUInt32 :
      isMask64(TypeId(Index)) ? TypeId::kUInt64 :
      isMmx32 (TypeId(Index)) ? TypeId::kUInt32 :
      isMmx64 (TypeId(Index)) ? TypeId::kUInt64 :
      isVec32 (TypeId(Index)) ? TypeId((Index - uint32_t(TypeId::_kVec32Start ) + uint32_t(TypeId::kInt8)) & 0xFF) :
      isVec64 (TypeId(Index)) ? TypeId((Index - uint32_t(TypeId::_kVec64Start ) + uint32_t(TypeId::kInt8)) & 0xFF) :
      isVec128(TypeId(Index)) ? TypeId((Index - uint32_t(TypeId::_kVec128Start) + uint32_t(TypeId::kInt8)) & 0xFF) :
      isVec256(TypeId(Index)) ? TypeId((Index - uint32_t(TypeId::_kVec256Start) + uint32_t(TypeId::kInt8)) & 0xFF) :
      isVec512(TypeId(Index)) ? TypeId((Index - uint32_t(TypeId::_kVec512Start) + uint32_t(TypeId::kInt8)) & 0xFF) : TypeId::kVoid)
  };
};

template<uint32_t Index>
struct SizeOfTypeId {
  enum : uint32_t {
    kTypeSize =
      isInt8   (TypeId(Index)) ?  1 :
      isUInt8  (TypeId(Index)) ?  1 :
      isInt16  (TypeId(Index)) ?  2 :
      isUInt16 (TypeId(Index)) ?  2 :
      isInt32  (TypeId(Index)) ?  4 :
      isUInt32 (TypeId(Index)) ?  4 :
      isInt64  (TypeId(Index)) ?  8 :
      isUInt64 (TypeId(Index)) ?  8 :
      isFloat32(TypeId(Index)) ?  4 :
      isFloat64(TypeId(Index)) ?  8 :
      isFloat80(TypeId(Index)) ? 10 :
      isMask8  (TypeId(Index)) ?  1 :
      isMask16 (TypeId(Index)) ?  2 :
      isMask32 (TypeId(Index)) ?  4 :
      isMask64 (TypeId(Index)) ?  8 :
      isMmx32  (TypeId(Index)) ?  4 :
      isMmx64  (TypeId(Index)) ?  8 :
      isVec32  (TypeId(Index)) ?  4 :
      isVec64  (TypeId(Index)) ?  8 :
      isVec128 (TypeId(Index)) ? 16 :
      isVec256 (TypeId(Index)) ? 32 :
      isVec512 (TypeId(Index)) ? 64 : 0
  };
};

const TypeData _typeData = {
  #define VALUE(x) TypeId(ScalarOfTypeId<x>::kTypeId)
  { ASMJIT_LOOKUP_TABLE_256(VALUE, 0) },
  #undef VALUE

  #define VALUE(x) SizeOfTypeId<x>::kTypeSize
  { ASMJIT_LOOKUP_TABLE_256(VALUE, 0) }
  #undef VALUE
};

} // {TypeUtils}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/type.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_TYPE_H_INCLUDED
#define ASMJIT_CORE_TYPE_H_INCLUDED

#include "../core/globals.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_core
//! \{

//! Type identifier provides a minimalist type system used across AsmJit library.
//!
//! This is an additional information that can be used to describe a value-type of physical or virtual register. It's
//! used mostly by BaseCompiler to describe register representation (the group of data stored in the register and the
//! width used) and it's also used by APIs that allow to describe and work with function signatures.
enum class TypeId : uint8_t {
  //! Void type.
  kVoid = 0,

  _kBaseStart = 32,
  _kBaseEnd = 44,

  _kIntStart = 32,
  _kIntEnd = 41,

  //! Abstract signed integer type that has a native size.
  kIntPtr = 32,
  //! Abstract unsigned integer type that has a native size.
  kUIntPtr = 33,

  //! 8-bit signed integer type.
  kInt8 = 34,
  //! 8-bit unsigned integer type.
  kUInt8 = 35,
  //! 16-bit signed integer type.
  kInt16 = 36,
  //! 16-bit unsigned integer type.
  kUInt16 = 37,
  //! 32-bit signed integer type.
  kInt32 = 38,
  //! 32-bit unsigned integer type.
  kUInt32 = 39,
  //! 64-bit signed integer type.
  kInt64 = 40,
  //! 64-bit unsigned integer type.
  kUInt64 = 41,

  _kFloatStart  = 42,
  _kFloatEnd = 44,

  //! 32-bit floating point type.
  kFloat32 = 42,
  //! 64-bit floating point type.
  kFloat64 = 43,
  //! 80-bit floating point type.
  kFloat80 = 44,

  _kMaskStart = 45,
  _kMaskEnd = 48,

  //! 8-bit opmask register (K).
  kMask8 = 45,
  //! 16-bit opmask register (K).
  kMask16 = 46,
  //! 32-bit opmask register (K).
  kMask32 = 47,
  //! 64-bit opmask register (K).
  kMask64 = 48,

  _kMmxStart = 49,
  _kMmxEnd = 50,

  //! 64-bit MMX register only used for 32 bits.
  kMmx32 = 49,
  //! 64-bit MMX register.
  kMmx64 = 50,

  _kVec32Start  = 51,
  _kVec32End = 60,

  kInt8x4 = 51,
  kUInt8x4 = 52,
  kInt16x2 = 53,
  kUInt16x2 = 54,
  kInt32x1 = 55,
  kUInt32x1 = 56,
  kFloat32x1 = 59,

  _kVec64Start  = 61,
  _kVec64End = 70,

  kInt8x8 = 61,
  kUInt8x8 = 62,
  kInt16x4 = 63,
  kUInt16x4 = 64,
  kInt32x2 = 65,
  kUInt32x2 = 66,
  kInt64x1 = 67,
  kUInt64x1 = 68,
  kFloat32x2 = 69,
  kFloat64x1 = 70,

  _kVec128Start = 71,
  _kVec128End = 80,

  kInt8x16 = 71,
  kUInt8x16 = 72,
  kInt16x8 = 73,
  kUInt16x8 = 74,
  kInt32x4 = 75,
  kUInt32x4 = 76,
  kInt64x2 = 77,
  kUInt64x2 = 78,
  kFloat32x4 = 79,
  kFloat64x2 = 80,

  _kVec256Start = 81,
  _kVec256End = 90,

  kInt8x32 = 81,
  kUInt8x32 = 82,
  kInt16x16 = 83,
  kUInt16x16 = 84,
  kInt32x8 = 85,
  kUInt32x8 = 86,
  kInt64x4 = 87,
  kUInt64x4 = 88,
  kFloat32x8 = 89,
  kFloat64x4 = 90,

  _kVec512Start = 91,
  _kVec512End = 100,

  kInt8x64 = 91,
  kUInt8x64 = 92,
  kInt16x32 = 93,
  kUInt16x32 = 94,
  kInt32x16 = 95,
  kUInt32x16 = 96,
  kInt64x8 = 97,
  kUInt64x8 = 98,
  kFloat32x16 = 99,
  kFloat64x8 = 100,

  kLastAssigned = kFloat64x8,

  kMaxValue = 255
};
ASMJIT_DEFINE_ENUM_COMPARE(TypeId)

//! Type identifier utilities.
namespace TypeUtils {

struct TypeData {
  TypeId scalarOf[uint32_t(TypeId::kMaxValue) + 1];
  uint8_t sizeOf[uint32_t(TypeId::kMaxValue) + 1];
};
ASMJIT_VARAPI const TypeData _typeData;

//! Returns the scalar type of `typeId`.
static ASMJIT_INLINE_NODEBUG TypeId scalarOf(TypeId typeId) noexcept { return _typeData.scalarOf[uint32_t(typeId)]; }

//! Returns the size [in bytes] of `typeId`.
static ASMJIT_INLINE_NODEBUG uint32_t sizeOf(TypeId typeId) noexcept { return _typeData.sizeOf[uint32_t(typeId)]; }

//! Tests whether a given type `typeId` is between `a` and `b`.
static ASMJIT_INLINE_NODEBUG constexpr bool isBetween(TypeId typeId, TypeId a, TypeId b) noexcept {
  return Support::isBetween(uint32_t(typeId), uint32_t(a), uint32_t(b));
}

//! Tests whether a given type `typeId` is \ref TypeId::kVoid.
static ASMJIT_INLINE_NODEBUG constexpr bool isVoid(TypeId typeId) noexcept { return typeId == TypeId::kVoid; }
//! Tests whether a given type `typeId` is a valid non-void type.
static ASMJIT_INLINE_NODEBUG constexpr bool isValid(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kIntStart, TypeId::_kVec512End); }
//! Tests whether a given type `typeId` is scalar (has no vector part).
static ASMJIT_INLINE_NODEBUG constexpr bool isScalar(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kBaseStart, TypeId::_kBaseEnd); }
//! Tests whether a given type `typeId` is abstract, which means that its size depends on register size.
static ASMJIT_INLINE_NODEBUG constexpr bool isAbstract(TypeId typeId) noexcept { return isBetween(typeId, TypeId::kIntPtr, TypeId::kUIntPtr); }

//! Tests whether a given type is a scalar integer (signed or unsigned) of any size.
static ASMJIT_INLINE_NODEBUG constexpr bool isInt(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kIntStart, TypeId::_kIntEnd); }
//! Tests whether a given type is a scalar 8-bit integer (signed).
static ASMJIT_INLINE_NODEBUG constexpr bool isInt8(TypeId typeId) noexcept { return typeId == TypeId::kInt8; }
//! Tests whether a given type is a scalar 8-bit integer (unsigned).
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt8(TypeId typeId) noexcept { return typeId == TypeId::kUInt8; }
//! Tests whether a given type is a scalar 16-bit integer (signed).
static ASMJIT_INLINE_NODEBUG constexpr bool isInt16(TypeId typeId) noexcept { return typeId == TypeId::kInt16; }
//! Tests whether a given type is a scalar 16-bit integer (unsigned).
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt16(TypeId typeId) noexcept { return typeId == TypeId::kUInt16; }
//! Tests whether a given type is a scalar 32-bit integer (signed).
static ASMJIT_INLINE_NODEBUG constexpr bool isInt32(TypeId typeId) noexcept { return typeId == TypeId::kInt32; }
//! Tests whether a given type is a scalar 32-bit integer (unsigned).
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt32(TypeId typeId) noexcept { return typeId == TypeId::kUInt32; }
//! Tests whether a given type is a scalar 64-bit integer (signed).
static ASMJIT_INLINE_NODEBUG constexpr bool isInt64(TypeId typeId) noexcept { return typeId == TypeId::kInt64; }
//! Tests whether a given type is a scalar 64-bit integer (unsigned).
static ASMJIT_INLINE_NODEBUG constexpr bool isUInt64(TypeId typeId) noexcept { return typeId == TypeId::kUInt64; }

//! Tests whether a given type is an 8-bit general purpose register representing either signed or unsigned 8-bit integer.
static ASMJIT_INLINE_NODEBUG constexpr bool isGp8(TypeId typeId) noexcept { return isBetween(typeId, TypeId::kInt8, TypeId::kUInt8); }
//! Tests whether a given type is a 16-bit general purpose register representing either signed or unsigned 16-bit integer
static ASMJIT_INLINE_NODEBUG constexpr bool isGp16(TypeId typeId) noexcept { return isBetween(typeId, TypeId::kInt16, TypeId::kUInt16); }
//! Tests whether a given type is a 32-bit general purpose register representing either signed or unsigned 32-bit integer
static ASMJIT_INLINE_NODEBUG constexpr bool isGp32(TypeId typeId) noexcept { return isBetween(typeId, TypeId::kInt32, TypeId::kUInt32); }
//! Tests whether a given type is a 64-bit general purpose register representing either signed or unsigned 64-bit integer
static ASMJIT_INLINE_NODEBUG constexpr bool isGp64(TypeId typeId) noexcept { return isBetween(typeId, TypeId::kInt64, TypeId::kUInt64); }

//! Tests whether a given type is a scalar floating point of any size.
static ASMJIT_INLINE_NODEBUG constexpr bool isFloat(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kFloatStart, TypeId::_kFloatEnd); }
//! Tests whether a given type is a scalar 32-bit float.
static ASMJIT_INLINE_NODEBUG constexpr bool isFloat32(TypeId typeId) noexcept { return typeId == TypeId::kFloat32; }
//! Tests whether a given type is a scalar 64-bit float.
static ASMJIT_INLINE_NODEBUG constexpr bool isFloat64(TypeId typeId) noexcept { return typeId == TypeId::kFloat64; }
//! Tests whether a given type is a scalar 80-bit float.
static ASMJIT_INLINE_NODEBUG constexpr bool isFloat80(TypeId typeId) noexcept { return typeId == TypeId::kFloat80; }

//! Tests whether a given type is a mask register of any size.
static ASMJIT_INLINE_NODEBUG constexpr bool isMask(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kMaskStart, TypeId::_kMaskEnd); }
//! Tests whether a given type is an 8-bit mask register.
static ASMJIT_INLINE_NODEBUG constexpr bool isMask8(TypeId typeId) noexcept { return typeId == TypeId::kMask8; }
//! Tests whether a given type is an 16-bit mask register.
static ASMJIT_INLINE_NODEBUG constexpr bool isMask16(TypeId typeId) noexcept { return typeId == TypeId::kMask16; }
//! Tests whether a given type is an 32-bit mask register.
static ASMJIT_INLINE_NODEBUG constexpr bool isMask32(TypeId typeId) noexcept { return typeId == TypeId::kMask32; }
//! Tests whether a given type is an 64-bit mask register.
static ASMJIT_INLINE_NODEBUG constexpr bool isMask64(TypeId typeId) noexcept { return typeId == TypeId::kMask64; }

//! Tests whether a given type is an MMX register.
//!
//! \note MMX functionality is in general deprecated on X86 architecture. AsmJit provides it just for completeness.
static ASMJIT_INLINE_NODEBUG constexpr bool isMmx(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kMmxStart, TypeId::_kMmxEnd); }
//! Tests whether a given type is an MMX register, which only uses the low 32 bits of data (only specific cases).
//!
//! \note MMX functionality is in general deprecated on X86 architecture. AsmJit provides it just for completeness.
static ASMJIT_INLINE_NODEBUG constexpr bool isMmx32(TypeId typeId) noexcept { return typeId == TypeId::kMmx32; }
//! Tests whether a given type is an MMX register, which uses 64 bits of data (default).
//!
//! \note MMX functionality is in general deprecated on X86 architecture. AsmJit provides it just for completeness.
static ASMJIT_INLINE_NODEBUG constexpr bool isMmx64(TypeId typeId) noexcept { return typeId == TypeId::kMmx64; }

//! Tests whether a given type is a vector register of any size.
static ASMJIT_INLINE_NODEBUG constexpr bool isVec(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kVec32Start, TypeId::_kVec512End); }
//! Tests whether a given type is a 32-bit or 32-bit view of a vector register.
static ASMJIT_INLINE_NODEBUG constexpr bool isVec32(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kVec32Start, TypeId::_kVec32End); }
//! Tests whether a given type is a 64-bit or 64-bit view of a vector register.
static ASMJIT_INLINE_NODEBUG constexpr bool isVec64(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kVec64Start, TypeId::_kVec64End); }
//! Tests whether a given type is a 128-bit or 128-bit view of a vector register.
static ASMJIT_INLINE_NODEBUG constexpr bool isVec128(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kVec128Start, TypeId::_kVec128End); }
//! Tests whether a given type is a 256-bit or 256-bit view of a vector register.
static ASMJIT_INLINE_NODEBUG constexpr bool isVec256(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kVec256Start, TypeId::_kVec256End); }
//! Tests whether a given type is a 512-bit or 512-bit view of a vector register.
static ASMJIT_INLINE_NODEBUG constexpr bool isVec512(TypeId typeId) noexcept { return isBetween(typeId, TypeId::_kVec512Start, TypeId::_kVec512End); }

//! \cond
enum TypeCategory : uint32_t {
  kTypeCategoryUnknown = 0,
  kTypeCategoryEnum = 1,
  kTypeCategoryIntegral = 2,
  kTypeCategoryFloatingPoint = 3,
  kTypeCategoryFunction = 4
};

template<typename T, TypeCategory kCategory>
struct TypeIdOfT_ByCategory {}; // Fails if not specialized.

template<typename T>
struct TypeIdOfT_ByCategory<T, kTypeCategoryIntegral> {
  enum : uint32_t {
    kTypeId = uint32_t(
      (sizeof(T) == 1 &&  std::is_signed<T>::value) ? TypeId::kInt8 :
      (sizeof(T) == 1 && !std::is_signed<T>::value) ? TypeId::kUInt8 :
      (sizeof(T) == 2 &&  std::is_signed<T>::value) ? TypeId::kInt16 :
      (sizeof(T) == 2 && !std::is_signed<T>::value) ? TypeId::kUInt16 :
      (sizeof(T) == 4 &&  std::is_signed<T>::value) ? TypeId::kInt32 :
      (sizeof(T) == 4 && !std::is_signed<T>::value) ? TypeId::kUInt32 :
      (sizeof(T) == 8 &&  std::is_signed<T>::value) ? TypeId::kInt64 :
      (sizeof(T) == 8 && !std::is_signed<T>::value) ? TypeId::kUInt64 : TypeId::kVoid)
  };
};

template<typename T>
struct TypeIdOfT_ByCategory<T, kTypeCategoryFloatingPoint> {
  enum : uint32_t {
    kTypeId = uint32_t(
      (sizeof(T) == 4 ) ? TypeId::kFloat32 :
      (sizeof(T) == 8 ) ? TypeId::kFloat64 :
      (sizeof(T) >= 10) ? TypeId::kFloat80 : TypeId::kVoid)
  };
};

template<typename T>
struct TypeIdOfT_ByCategory<T, kTypeCategoryEnum>
  : public TypeIdOfT_ByCategory<typename std::underlying_type<T>::type, kTypeCategoryIntegral> {};

template<typename T>
struct TypeIdOfT_ByCategory<T, kTypeCategoryFunction> {
  enum : uint32_t {
    kTypeId = uint32_t(TypeId::kUIntPtr)
  };
};
//! \endcond

//! TypeIdOfT<> template allows to get a TypeId from a C++ type `T`.
#ifdef _DOXYGEN
template<typename T>
struct TypeIdOfT {
  //! TypeId of C++ type `T`.
  static constexpr TypeId kTypeId = _TypeIdDeducedAtCompileTime_;
};
#else
template<typename T>
struct TypeIdOfT
  : public TypeIdOfT_ByCategory<T,
    std::is_enum<T>::value           ? kTypeCategoryEnum          :
    std::is_integral<T>::value       ? kTypeCategoryIntegral      :
    std::is_floating_point<T>::value ? kTypeCategoryFloatingPoint :
    std::is_function<T>::value       ? kTypeCategoryFunction      : kTypeCategoryUnknown> {};
#endif

//! \cond
template<typename T>
struct TypeIdOfT<T*> {
  enum : uint32_t {
    kTypeId = uint32_t(TypeId::kUIntPtr)
  };
};

template<typename T>
struct TypeIdOfT<T&> {
  enum : uint32_t {
    kTypeId = uint32_t(TypeId::kUIntPtr)
  };
};
//! \endcond

//! Returns a corresponding \ref TypeId of `T` type.
template<typename T>
static ASMJIT_INLINE_NODEBUG constexpr TypeId typeIdOfT() noexcept { return TypeId(TypeIdOfT<T>::kTypeId); }

//! Returns offset needed to convert a `kIntPtr` and `kUIntPtr` TypeId into a type that matches `registerSize`
//! (general-purpose register size). If you find such TypeId it's then only about adding the offset to it.
//!
//! For example:
//!
//! ```
//! uint32_t registerSize = /* 4 or 8 */;
//! uint32_t deabstractDelta = TypeUtils::deabstractDeltaOfSize(registerSize);
//!
//! TypeId typeId = 'some type-id';
//!
//! // Normalize some typeId into a non-abstract typeId.
//! if (TypeUtils::isAbstract(typeId)) typeId += deabstractDelta;
//!
//! // The same, but by using TypeUtils::deabstract() function.
//! typeId = TypeUtils::deabstract(typeId, deabstractDelta);
//! ```
static ASMJIT_INLINE_NODEBUG constexpr uint32_t deabstractDeltaOfSize(uint32_t registerSize) noexcept {
  return registerSize >= 8 ? uint32_t(TypeId::kInt64) - uint32_t(TypeId::kIntPtr)
                           : uint32_t(TypeId::kInt32) - uint32_t(TypeId::kIntPtr);
}

//! Deabstracts a given `typeId` into a native type by using `deabstractDelta`, which was previously
//! calculated by calling \ref deabstractDeltaOfSize() with a target native register size.
static ASMJIT_INLINE_NODEBUG constexpr TypeId deabstract(TypeId typeId, uint32_t deabstractDelta) noexcept {
  return isAbstract(typeId) ? TypeId(uint32_t(typeId) + deabstractDelta) : typeId;
}

static ASMJIT_INLINE_NODEBUG constexpr TypeId scalarToVector(TypeId scalarTypeId, TypeId vecStartId) noexcept {
  return TypeId(uint32_t(vecStartId) + uint32_t(scalarTypeId) - uint32_t(TypeId::kInt8));
}

} // {TypeUtils}

//! Provides type identifiers that can be used in templates instead of native types.
namespace Type {

//! bool as C++ type-name.
struct Bool {};
//! int8_t as C++ type-name.
struct Int8 {};
//! uint8_t as C++ type-name.
struct UInt8 {};
//! int16_t as C++ type-name.
struct Int16 {};
//! uint16_t as C++ type-name.
struct UInt16 {};
//! int32_t as C++ type-name.
struct Int32 {};
//! uint32_t as C++ type-name.
struct UInt32 {};
//! int64_t as C++ type-name.
struct Int64 {};
//! uint64_t as C++ type-name.
struct UInt64 {};
//! intptr_t as C++ type-name.
struct IntPtr {};
//! uintptr_t as C++ type-name.
struct UIntPtr {};
//! float as C++ type-name.
struct Float32 {};
//! double as C++ type-name.
struct Float64 {};

} // {Type}

//! \cond
#define ASMJIT_DEFINE_TYPE_ID(T, TYPE_ID) \
namespace TypeUtils {                     \
  template<>                              \
  struct TypeIdOfT<T> {                   \
    enum : uint32_t {                     \
      kTypeId = uint32_t(TYPE_ID)         \
    };                                    \
  };                                      \
}

ASMJIT_DEFINE_TYPE_ID(void         , TypeId::kVoid);
ASMJIT_DEFINE_TYPE_ID(Type::Bool   , TypeId::kUInt8);
ASMJIT_DEFINE_TYPE_ID(Type::Int8   , TypeId::kInt8);
ASMJIT_DEFINE_TYPE_ID(Type::UInt8  , TypeId::kUInt8);
ASMJIT_DEFINE_TYPE_ID(Type::Int16  , TypeId::kInt16);
ASMJIT_DEFINE_TYPE_ID(Type::UInt16 , TypeId::kUInt16);
ASMJIT_DEFINE_TYPE_ID(Type::Int32  , TypeId::kInt32);
ASMJIT_DEFINE_TYPE_ID(Type::UInt32 , TypeId::kUInt32);
ASMJIT_DEFINE_TYPE_ID(Type::Int64  , TypeId::kInt64);
ASMJIT_DEFINE_TYPE_ID(Type::UInt64 , TypeId::kUInt64);
ASMJIT_DEFINE_TYPE_ID(Type::IntPtr , TypeId::kIntPtr);
ASMJIT_DEFINE_TYPE_ID(Type::UIntPtr, TypeId::kUIntPtr);
ASMJIT_DEFINE_TYPE_ID(Type::Float32, TypeId::kFloat32);
ASMJIT_DEFINE_TYPE_ID(Type::Float64, TypeId::kFloat64);
//! \endcond

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_TYPE_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/virtmem.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#ifndef ASMJIT_NO_JIT

#include "../core/osutils_p.h"
#include "../core/string.h"
#include "../core/support.h"
#include "../core/virtmem.h"

#if !defined(_WIN32)
  #include <errno.h>
  #include <fcntl.h>
  #include <sys/mman.h>
  #include <sys/stat.h>
  #include <sys/types.h>
  #include <unistd.h>

  #if !ASMJIT_ARCH_X86
    #include <sys/time.h> // required by gettimeofday()
  #endif

  // Linux has a `memfd_create` syscall that we would like to use, if available.
  #if defined(__linux__)
    #include <sys/syscall.h>
    #include <sys/utsname.h>

    #ifndef MAP_HUGETLB
      #define MAP_HUGETLB 0x40000
    #endif // MAP_HUGETLB

    #ifndef MAP_HUGE_SHIFT
      #define MAP_HUGE_SHIFT 26
    #endif // MAP_HUGE_SHIFT

    #if !defined(MFD_CLOEXEC)
      #define MFD_CLOEXEC 0x0001u
    #endif // MFD_CLOEXEC

    #if !defined(MFD_NOEXEC_SEAL)
      #define MFD_NOEXEC_SEAL 0x0008u
    #endif // MFD_NOEXEC_SEAL

    #if !defined(MFD_EXEC)
      #define MFD_EXEC 0x0010u
    #endif // MFD_EXEC

    #ifndef MFD_HUGETLB
      #define MFD_HUGETLB 0x0004
    #endif // MFD_HUGETLB

    #ifndef MFD_HUGE_SHIFT
      #define MFD_HUGE_SHIFT 26
    #endif // MFD_HUGE_SHIFT
  #endif

  // Apple recently introduced MAP_JIT flag, which we want to use.
  #if defined(__APPLE__)
    #include <pthread.h>
    #include <TargetConditionals.h>
    #if TARGET_OS_OSX
      #include <sys/utsname.h>
      #include <libkern/OSCacheControl.h> // sys_icache_invalidate().
    #endif
    // Older SDK doesn't define `MAP_JIT`.
    #ifndef MAP_JIT
      #define MAP_JIT 0x800
    #endif
  #endif

  // BSD/MAC: `MAP_ANONYMOUS` is not defined, `MAP_ANON` is.
  #if !defined(MAP_ANONYMOUS)
    #define MAP_ANONYMOUS MAP_ANON
  #endif

  // Android NDK doesn't provide `shm_open()` and `shm_unlink()`.
  #if !defined(__BIONIC__) && !defined(ASMJIT_NO_SHM_OPEN)
    #define ASMJIT_HAS_SHM_OPEN
  #endif

  #if defined(__APPLE__) || defined(__BIONIC__) || !defined(ASMJIT_HAS_SHM_OPEN)
    #define ASMJIT_VM_SHM_DETECT 0
  #else
    #define ASMJIT_VM_SHM_DETECT 1
  #endif

  #if defined(__APPLE__) && TARGET_OS_OSX
    #if ASMJIT_ARCH_X86 != 0
      #define ASMJIT_ANONYMOUS_MEMORY_USE_MACH_VM_REMAP
    #endif
    #if ASMJIT_ARCH_ARM >= 64
      #define ASMJIT_HAS_PTHREAD_JIT_WRITE_PROTECT_NP
    #endif
  #endif

  #if defined(__APPLE__) && ASMJIT_ARCH_X86 == 0
    #define ASMJIT_NO_DUAL_MAPPING
  #endif

  #if defined(__NetBSD__) && defined(MAP_REMAPDUP) && defined(PROT_MPROTECT)
    #define ASMJIT_ANONYMOUS_MEMORY_USE_REMAPDUP
  #endif

  #if !defined(ASMJIT_ANONYMOUS_MEMORY_USE_REMAPDUP) && \
      !defined(ASMJIT_ANONYMOUS_MEMORY_USE_MACH_VM_REMAP) && \
      !defined(ASMJIT_NO_DUAL_MAPPING)
    #define ASMJIT_ANONYMOUS_MEMORY_USE_FD
  #endif
#endif

#include <atomic>

#if defined(ASMJIT_ANONYMOUS_MEMORY_USE_MACH_VM_REMAP)
#include <mach/mach.h>
#include <mach/mach_time.h>

extern "C" {

#ifdef mig_external
mig_external
#else
extern
#endif
kern_return_t mach_vm_remap(
  vm_map_t target_task,
  mach_vm_address_t *target_address,
  mach_vm_size_t size,
  mach_vm_offset_t mask,
  int flags,
  vm_map_t src_task,
  mach_vm_address_t src_address,
  boolean_t copy,
  vm_prot_t *cur_protection,
  vm_prot_t *max_protection,
  vm_inherit_t inheritance
);

} // {extern "C"}
#endif

ASMJIT_BEGIN_SUB_NAMESPACE(VirtMem)

// Virtual Memory Utilities
// ========================

ASMJIT_MAYBE_UNUSED
static const constexpr MemoryFlags dualMappingFilter[2] = {
  MemoryFlags::kAccessWrite | MemoryFlags::kMMapMaxAccessWrite,
  MemoryFlags::kAccessExecute | MemoryFlags::kMMapMaxAccessExecute
};

// Virtual Memory [Windows]
// ========================

#if defined(_WIN32)

struct ScopedHandle {
  inline ScopedHandle() noexcept
    : value(nullptr) {}

  inline ~ScopedHandle() noexcept {
    if (value != nullptr)
      ::CloseHandle(value);
  }

  HANDLE value;
};

static void detectVMInfo(Info& vmInfo) noexcept {
  SYSTEM_INFO systemInfo;

  ::GetSystemInfo(&systemInfo);
  vmInfo.pageSize = Support::alignUpPowerOf2<uint32_t>(systemInfo.dwPageSize);
  vmInfo.pageGranularity = systemInfo.dwAllocationGranularity;
}

static size_t detectLargePageSize() noexcept {
  return ::GetLargePageMinimum();
}

static bool hasDualMappingSupport() noexcept {
  // TODO: This assumption works on X86 platforms, this may not work on AArch64.
  return true;
}

// Returns windows-specific protectFlags from \ref MemoryFlags.
static DWORD protectFlagsFromMemoryFlags(MemoryFlags memoryFlags) noexcept {
  DWORD protectFlags;

  // READ|WRITE|EXECUTE.
  if (Support::test(memoryFlags, MemoryFlags::kAccessExecute))
    protectFlags = Support::test(memoryFlags, MemoryFlags::kAccessWrite) ? PAGE_EXECUTE_READWRITE : PAGE_EXECUTE_READ;
  else if (Support::test(memoryFlags, MemoryFlags::kAccessRW))
    protectFlags = Support::test(memoryFlags, MemoryFlags::kAccessWrite) ? PAGE_READWRITE : PAGE_READONLY;
  else
    protectFlags = PAGE_NOACCESS;

  // Any other flags to consider?
  return protectFlags;
}

static DWORD desiredAccessFromMemoryFlags(MemoryFlags memoryFlags) noexcept {
  DWORD access = Support::test(memoryFlags, MemoryFlags::kAccessWrite) ? FILE_MAP_WRITE : FILE_MAP_READ;
  if (Support::test(memoryFlags, MemoryFlags::kAccessExecute))
    access |= FILE_MAP_EXECUTE;
  return access;
}

static HardenedRuntimeFlags getHardenedRuntimeFlags() noexcept {
  HardenedRuntimeFlags flags = HardenedRuntimeFlags::kNone;

  if (hasDualMappingSupport())
    flags |= HardenedRuntimeFlags::kDualMapping;

  return flags;
}

Error alloc(void** p, size_t size, MemoryFlags memoryFlags) noexcept {
  *p = nullptr;
  if (size == 0)
    return DebugUtils::errored(kErrorInvalidArgument);

  DWORD allocationType = MEM_COMMIT | MEM_RESERVE;
  DWORD protectFlags = protectFlagsFromMemoryFlags(memoryFlags);

  if (Support::test(memoryFlags, MemoryFlags::kMMapLargePages)) {
    size_t lpSize = largePageSize();

    // Does it make sense to call VirtualAlloc() if we failed to query large page size?
    if (lpSize == 0)
      return DebugUtils::errored(kErrorFeatureNotEnabled);

    if (!Support::isAligned(size, lpSize))
      return DebugUtils::errored(kErrorInvalidArgument);

    allocationType |= MEM_LARGE_PAGES;
  }

  void* result = ::VirtualAlloc(nullptr, size, allocationType, protectFlags);
  if (!result)
    return DebugUtils::errored(kErrorOutOfMemory);

  *p = result;
  return kErrorOk;
}

Error release(void* p, size_t size) noexcept {
  DebugUtils::unused(size);
  // NOTE: If the `dwFreeType` parameter is MEM_RELEASE, `size` parameter must be zero.
  constexpr DWORD dwFreeType = MEM_RELEASE;
  if (ASMJIT_UNLIKELY(!::VirtualFree(p, 0, dwFreeType)))
    return DebugUtils::errored(kErrorInvalidArgument);
  return kErrorOk;
}

Error protect(void* p, size_t size, MemoryFlags memoryFlags) noexcept {
  DWORD protectFlags = protectFlagsFromMemoryFlags(memoryFlags);
  DWORD oldFlags;

  if (::VirtualProtect(p, size, protectFlags, &oldFlags))
    return kErrorOk;

  return DebugUtils::errored(kErrorInvalidArgument);
}

Error allocDualMapping(DualMapping* dm, size_t size, MemoryFlags memoryFlags) noexcept {
  dm->rx = nullptr;
  dm->rw = nullptr;

  if (size == 0)
    return DebugUtils::errored(kErrorInvalidArgument);

  ScopedHandle handle;
  handle.value = ::CreateFileMappingW(
    INVALID_HANDLE_VALUE,
    nullptr,
    PAGE_EXECUTE_READWRITE,
    (DWORD)(uint64_t(size) >> 32),
    (DWORD)(size & 0xFFFFFFFFu),
    nullptr);

  if (ASMJIT_UNLIKELY(!handle.value))
    return DebugUtils::errored(kErrorOutOfMemory);

  void* ptr[2];
  for (uint32_t i = 0; i < 2; i++) {
    MemoryFlags accessFlags = memoryFlags & ~dualMappingFilter[i];
    DWORD desiredAccess = desiredAccessFromMemoryFlags(accessFlags);
    ptr[i] = ::MapViewOfFile(handle.value, desiredAccess, 0, 0, size);

    if (ptr[i] == nullptr) {
      if (i == 1u)
        ::UnmapViewOfFile(ptr[0]);
      return DebugUtils::errored(kErrorOutOfMemory);
    }
  }

  dm->rx = ptr[0];
  dm->rw = ptr[1];
  return kErrorOk;
}

Error releaseDualMapping(DualMapping* dm, size_t size) noexcept {
  DebugUtils::unused(size);
  bool failed = false;

  if (!::UnmapViewOfFile(dm->rx))
    failed = true;

  if (dm->rx != dm->rw && !UnmapViewOfFile(dm->rw))
    failed = true;

  if (failed)
    return DebugUtils::errored(kErrorInvalidArgument);

  dm->rx = nullptr;
  dm->rw = nullptr;
  return kErrorOk;
}

#endif

// Virtual Memory [Unix]
// =====================

#if !defined(_WIN32)

// Virtual Memory [Unix] - Utilities
// =================================

#if defined(__linux__) || (defined(__APPLE__) && TARGET_OS_OSX)
struct KernelVersion {
  long ver[2];

  inline long major() const noexcept { return ver[0]; }
  inline long minor() const noexcept { return ver[1]; }

  inline bool eq(long major, long minor) const noexcept { return ver[0] == major && ver[1] == minor; }
  inline bool ge(long major, long minor) const noexcept { return ver[0] > major || (ver[0] == major && ver[1] >= minor); }
};

ASMJIT_MAYBE_UNUSED
static KernelVersion getKernelVersion() noexcept {
  KernelVersion out {};
  struct utsname buf {};

  uname(&buf);

  size_t i = 0;
  char* p = buf.release;

  while (*p && i < 2u) {
    uint8_t c = uint8_t(*p);
    if (c >= uint8_t('0') && c <= uint8_t('9')) {
      out.ver[i] = strtol(p, &p, 10);
      i++;
      continue;
    }

    p++;
  }

  return out;
}
#endif // getKernelVersion

// Translates libc errors specific to VirtualMemory mapping to `asmjit::Error`.
ASMJIT_MAYBE_UNUSED
static Error asmjitErrorFromErrno(int e) noexcept {
  switch (e) {
    case EACCES:
    case EAGAIN:
    case ENODEV:
    case EPERM:
      return kErrorInvalidState;

    case EFBIG:
    case ENOMEM:
    case EOVERFLOW:
      return kErrorOutOfMemory;

    case EMFILE:
    case ENFILE:
      return kErrorTooManyHandles;

    default:
      return kErrorInvalidArgument;
  }
}

ASMJIT_MAYBE_UNUSED
static MemoryFlags maxAccessFlagsToRegularAccessFlags(MemoryFlags memoryFlags) noexcept {
  static constexpr uint32_t kMaxProtShift = Support::ConstCTZ<uint32_t(MemoryFlags::kMMapMaxAccessRead)>::value;
  return MemoryFlags(uint32_t(memoryFlags & MemoryFlags::kMMapMaxAccessRWX) >> kMaxProtShift);
}

ASMJIT_MAYBE_UNUSED
static MemoryFlags regularAccessFlagsToMaxAccessFlags(MemoryFlags memoryFlags) noexcept {
  static constexpr uint32_t kMaxProtShift = Support::ConstCTZ<uint32_t(MemoryFlags::kMMapMaxAccessRead)>::value;
  return MemoryFlags(uint32_t(memoryFlags & MemoryFlags::kAccessRWX) << kMaxProtShift);
}

// Returns `mmap()` protection flags from \ref MemoryFlags.
ASMJIT_MAYBE_UNUSED
static int mmProtFromMemoryFlags(MemoryFlags memoryFlags) noexcept {
  int protection = 0;
  if (Support::test(memoryFlags, MemoryFlags::kAccessRead)) protection |= PROT_READ;
  if (Support::test(memoryFlags, MemoryFlags::kAccessWrite)) protection |= PROT_READ | PROT_WRITE;
  if (Support::test(memoryFlags, MemoryFlags::kAccessExecute)) protection |= PROT_READ | PROT_EXEC;
  return protection;
}

// Returns maximum protection flags from `memoryFlags`.
//
// Uses:
//   - `PROT_MPROTECT()` on NetBSD.
//   - `PROT_MAX()` when available on other BSDs.
ASMJIT_MAYBE_UNUSED
static inline int mmMaxProtFromMemoryFlags(MemoryFlags memoryFlags) noexcept {
  MemoryFlags acc = maxAccessFlagsToRegularAccessFlags(memoryFlags);
  if (acc != MemoryFlags::kNone) {
#if defined(__NetBSD__) && defined(PROT_MPROTECT)
    return PROT_MPROTECT(mmProtFromMemoryFlags(acc));
#elif defined(PROT_MAX)
    return PROT_MAX(mmProtFromMemoryFlags(acc));
#else
    return 0;
#endif
  }

  return 0;
}

static void detectVMInfo(Info& vmInfo) noexcept {
  uint32_t pageSize = uint32_t(::getpagesize());

  vmInfo.pageSize = pageSize;
  vmInfo.pageGranularity = Support::max<uint32_t>(pageSize, 65536);
}

static size_t detectLargePageSize() noexcept {
#if defined(__APPLE__) && defined(VM_FLAGS_SUPERPAGE_SIZE_2MB) && ASMJIT_ARCH_X86
  return 2u * 1024u * 1024u;
#elif defined(__FreeBSD__)
  Support::Array<size_t, 2> pageSize;
  // TODO: Does it return unsigned?
  return (getpagesizes(pageSize.data(), 2) < 2) ? 0 : uint32_t(pageSize[1]);
#elif defined(__linux__)
  StringTmp<128> storage;
  if (OSUtils::readFile("/sys/kernel/mm/transparent_hugepage/hpage_pmd_size", storage, 16) != kErrorOk || storage.empty())
    return 0u;

  // The first value should be the size of the page (hpage_pmd_size).
  size_t largePageSize = 0;

  const char* buf = storage.data();
  size_t bufSize = storage.size();

  for (size_t i = 0; i < bufSize; i++) {
    uint32_t digit = uint32_t(uint8_t(buf[i]) - uint8_t('0'));
    if (digit >= 10u)
      break;
    largePageSize = largePageSize * 10 + digit;
  }

  if (Support::isPowerOf2(largePageSize))
    return largePageSize;
  else
    return 0u;
#else
  return 0u;
#endif
}

// Virtual Memory [Posix] - Anonymous Memory
// =========================================

#if defined(ASMJIT_ANONYMOUS_MEMORY_USE_FD)

// Some operating systems don't allow /dev/shm to be executable. On Linux this happens when /dev/shm is mounted with
// 'noexec', which is enforced by systemd. Other operating systems like MacOS also restrict executable permissions
// regarding /dev/shm, so we use a runtime detection before attempting to allocate executable memory. Sometimes we
// don't need the detection as we know it would always result in `AnonymousMemoryStrategy::kTmpDir`.
enum class AnonymousMemoryStrategy : uint32_t {
  kUnknown = 0,
  kDevShm = 1,
  kTmpDir = 2
};

#if !defined(SHM_ANON)
static const char* getTmpDir() noexcept {
  const char* tmpDir = getenv("TMPDIR");
  return tmpDir ? tmpDir : "/tmp";
}
#endif

#if defined(__linux__) && defined(__NR_memfd_create)
static uint32_t getMfdExecFlag() noexcept {
  static std::atomic<uint32_t> cachedMfdExecSupported;
  uint32_t val = cachedMfdExecSupported.load();

  if (val == 0u) {
    KernelVersion ver = getKernelVersion();
    val = uint32_t(ver.ge(6, 3)) + 1u;
    cachedMfdExecSupported.store(val);
  }

  return val == 2u ? uint32_t(MFD_EXEC) : uint32_t(0u);
}
#endif // __linux__ && __NR_memfd_create


// It's not fully random, just to avoid collisions when opening TMP or SHM file.
ASMJIT_MAYBE_UNUSED
static uint64_t generateRandomBits(uintptr_t stackPtr, uint32_t attempt) noexcept {
  static std::atomic<uint32_t> internalCounter;

#if defined(__GNUC__) && ASMJIT_ARCH_X86
  // Use RDTSC instruction to avoid gettimeofday() as we just need some "random" bits.
  uint64_t mix = __builtin_ia32_rdtsc();
#else
  struct timeval tm {};
  uint64_t mix = 1; // only used when gettimeofday() fails, which is unlikely.
  if (gettimeofday(&tm, nullptr) == 0) {
    mix = uint64_t(tm.tv_usec) ^ uint64_t(tm.tv_sec);
  }
#endif

  uint64_t bits = (uint64_t(stackPtr) & 0x1010505000055590u) - mix * 773703683;
  bits = (bits >> 33) ^ (bits << 7) ^ (attempt * 87178291199);
  return bits + uint64_t(++internalCounter) * 10619863;
}

class AnonymousMemory {
public:
  enum FileType : uint32_t {
    kFileTypeNone,
    kFileTypeShm,
    kFileTypeTmp
  };

  int _fd;
  FileType _fileType;
  StringTmp<128> _tmpName;

  inline AnonymousMemory() noexcept
    : _fd(-1),
      _fileType(kFileTypeNone),
      _tmpName() {}

  inline ~AnonymousMemory() noexcept {
    unlink();
    close();
  }

  inline int fd() const noexcept { return _fd; }

  Error open(bool preferTmpOverDevShm) noexcept {
#if defined(__linux__) && defined(__NR_memfd_create)
    // Linux specific 'memfd_create' - if the syscall returns `ENOSYS` it means
    // it's not available and we will never call it again (would be pointless).
    //
    // NOTE: There is also memfd_create() libc function in FreeBSD, but it internally
    // uses `shm_open(SHM_ANON, ...)` so it's not needed to add support for it (it's
    // not a syscall as in Linux).

    // Zero initialized, if ever changed to '1' that would mean the syscall is not
    // available and we must use `shm_open()` and `shm_unlink()` (or regular `open()`).
    static volatile uint32_t memfd_create_not_supported;

    if (!memfd_create_not_supported) {
      _fd = (int)syscall(__NR_memfd_create, "vmem", MFD_CLOEXEC | getMfdExecFlag());
      if (ASMJIT_LIKELY(_fd >= 0))
        return kErrorOk;

      int e = errno;
      if (e == ENOSYS)
        memfd_create_not_supported = 1;
      else
        return DebugUtils::errored(asmjitErrorFromErrno(e));
    }
#endif // __linux__ && __NR_memfd_create

#if defined(ASMJIT_HAS_SHM_OPEN) && defined(SHM_ANON)
    // Originally FreeBSD extension, apparently works in other BSDs too.
    DebugUtils::unused(preferTmpOverDevShm);
    _fd = ::shm_open(SHM_ANON, O_RDWR | O_CREAT | O_EXCL, S_IRUSR | S_IWUSR);

    if (ASMJIT_LIKELY(_fd >= 0))
      return kErrorOk;
    else
      return DebugUtils::errored(asmjitErrorFromErrno(errno));
#else
    // POSIX API. We have to generate somehow a unique name, so use `generateRandomBits()` helper. To prevent
    // having file collisions we use `shm_open()` with flags that require creation of the file so we never open
    // an existing shared memory.
    static const char kShmFormat[] = "/shm-id-%016llX";
    uint32_t kRetryCount = 100;

    for (uint32_t i = 0; i < kRetryCount; i++) {
      bool useTmp = !ASMJIT_VM_SHM_DETECT || preferTmpOverDevShm;
      uint64_t bits = generateRandomBits((uintptr_t)this, i);

      if (useTmp) {
        _tmpName.assign(getTmpDir());
        _tmpName.appendFormat(kShmFormat, (unsigned long long)bits);
        _fd = ASMJIT_FILE64_API(::open)(_tmpName.data(), O_RDWR | O_CREAT | O_EXCL, 0);
        if (ASMJIT_LIKELY(_fd >= 0)) {
          _fileType = kFileTypeTmp;
          return kErrorOk;
        }
      }
#if defined(ASMJIT_HAS_SHM_OPEN)
      else {
        _tmpName.assignFormat(kShmFormat, (unsigned long long)bits);
        _fd = ::shm_open(_tmpName.data(), O_RDWR | O_CREAT | O_EXCL, S_IRUSR | S_IWUSR);
        if (ASMJIT_LIKELY(_fd >= 0)) {
          _fileType = kFileTypeShm;
          return kErrorOk;
        }
      }
#endif

      int e = errno;
      if (e != EEXIST)
        return DebugUtils::errored(asmjitErrorFromErrno(e));
    }

    return DebugUtils::errored(kErrorFailedToOpenAnonymousMemory);
#endif
  }

  void unlink() noexcept {
    FileType type = _fileType;
    _fileType = kFileTypeNone;

#ifdef ASMJIT_HAS_SHM_OPEN
    if (type == kFileTypeShm) {
      ::shm_unlink(_tmpName.data());
      return;
    }
#endif

    if (type == kFileTypeTmp) {
      ::unlink(_tmpName.data());
      return;
    }
  }

  void close() noexcept {
    if (_fd >= 0) {
      ::close(_fd);
      _fd = -1;
    }
  }

  Error allocate(size_t size) noexcept {
    // TODO: Improve this by using `posix_fallocate()` when available.
    if (ASMJIT_FILE64_API(ftruncate)(_fd, off_t(size)) != 0)
      return DebugUtils::errored(asmjitErrorFromErrno(errno));

    return kErrorOk;
  }
};

#if ASMJIT_VM_SHM_DETECT
static Error detectAnonymousMemoryStrategy(AnonymousMemoryStrategy* strategyOut) noexcept {
  AnonymousMemory anonMem;
  Info vmInfo = info();

  ASMJIT_PROPAGATE(anonMem.open(false));
  ASMJIT_PROPAGATE(anonMem.allocate(vmInfo.pageSize));

  void* ptr = mmap(nullptr, vmInfo.pageSize, PROT_READ | PROT_EXEC, MAP_SHARED, anonMem.fd(), 0);
  if (ptr == MAP_FAILED) {
    int e = errno;
    if (e == EINVAL) {
      *strategyOut = AnonymousMemoryStrategy::kTmpDir;
      return kErrorOk;
    }
    return DebugUtils::errored(asmjitErrorFromErrno(e));
  }
  else {
    munmap(ptr, vmInfo.pageSize);
    *strategyOut = AnonymousMemoryStrategy::kDevShm;
    return kErrorOk;
  }
}
#endif

static Error getAnonymousMemoryStrategy(AnonymousMemoryStrategy* strategyOut) noexcept {
#if ASMJIT_VM_SHM_DETECT
  // Initially don't assume anything. It has to be tested whether '/dev/shm' was mounted with 'noexec' flag or not.
  static std::atomic<uint32_t> cachedStrategy;

  AnonymousMemoryStrategy strategy = static_cast<AnonymousMemoryStrategy>(cachedStrategy.load());
  if (strategy == AnonymousMemoryStrategy::kUnknown) {
    ASMJIT_PROPAGATE(detectAnonymousMemoryStrategy(&strategy));
    cachedStrategy.store(static_cast<uint32_t>(strategy));
  }

  *strategyOut = strategy;
  return kErrorOk;
#else
  *strategyOut = AnonymousMemoryStrategy::kTmpDir;
  return kErrorOk;
#endif
}

#endif // ASMJIT_ANONYMOUS_MEMORY_USE_FD

// Virtual Memory [Posix] - Hardened Runtime & MAP_JIT
// ===================================================

// Detects whether the current process is hardened, which means that pages that have WRITE and EXECUTABLE flags
// cannot be normally allocated. On OSX + AArch64 such allocation requires MAP_JIT flag, other platforms don't
// support this combination.
static bool hasHardenedRuntime() noexcept {
#if defined(__APPLE__) && TARGET_OS_OSX && ASMJIT_ARCH_ARM >= 64
  // OSX on AArch64 has always hardened runtime enabled.
  return true;
#else
  static std::atomic<uint32_t> cachedHardenedFlag;

  enum HardenedFlag : uint32_t {
    kHardenedFlagUnknown  = 0,
    kHardenedFlagDisabled = 1,
    kHardenedFlagEnabled  = 2
  };

  uint32_t flag = cachedHardenedFlag.load();
  if (flag == kHardenedFlagUnknown) {
    size_t pageSize = size_t(::getpagesize());
    void* ptr = mmap(nullptr, pageSize, PROT_READ | PROT_WRITE | PROT_EXEC, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);

    if (ptr == MAP_FAILED) {
      flag = kHardenedFlagEnabled;
    }
    else {
      flag = kHardenedFlagDisabled;
      munmap(ptr, pageSize);
    }

    cachedHardenedFlag.store(flag);
  }

  return flag == kHardenedFlagEnabled;
#endif
}

// Detects whether MAP_JIT is available.
static inline bool hasMapJitSupport() noexcept {
#if defined(__APPLE__) && TARGET_OS_OSX && ASMJIT_ARCH_X86 == 0
  // Apple platforms always use hardened runtime + MAP_JIT on non-x86 hardware:
  //   - https://developer.apple.com/documentation/apple_silicon/porting_just-in-time_compilers_to_apple_silicon
  return true;
#elif defined(__APPLE__) && TARGET_OS_OSX
  // MAP_JIT flag required to run unsigned JIT code is only supported by kernel version 10.14+ (Mojave).
  static std::atomic<uint32_t> cachedMapJitSupport;
  uint32_t val = cachedMapJitSupport.load();

  if (val == 0u) {
    KernelVersion ver = getKernelVersion();
    val = uint32_t(ver.ge(18, 0)) + 1u;
    cachedMapJitSupport.store(val);
  }

  return val == 2u;
#else
  // MAP_JIT is not available (it's only available on OSX).
  return false;
#endif
}

// Returns either MAP_JIT or 0 based on `memoryFlags` and the host operating system.
static inline int mmMapJitFromMemoryFlags(MemoryFlags memoryFlags) noexcept {
#if defined(__APPLE__)
  // Always use MAP_JIT flag if user asked for it (could be used for testing on non-hardened processes) and detect
  // whether it must be used when the process is actually hardened (in that case it doesn't make sense to rely on
  // user `memoryFlags`).
  //
  // MAP_JIT is not required when dual-mapping memory and is incompatible with MAP_SHARED, so it will not be
  // added when the latter is enabled.
  bool useMapJit = (Support::test(memoryFlags, MemoryFlags::kMMapEnableMapJit) || hasHardenedRuntime())
                   && !Support::test(memoryFlags, MemoryFlags::kMapShared);
  if (useMapJit)
    return hasMapJitSupport() ? int(MAP_JIT) : 0;
  else
    return 0;
#else
  DebugUtils::unused(memoryFlags);
  return 0;
#endif
}

static inline bool hasDualMappingSupport() noexcept {
#if defined(ASMJIT_NO_DUAL_MAPPING)
  return false;
#else
  return true;
#endif
}

static HardenedRuntimeFlags getHardenedRuntimeFlags() noexcept {
  HardenedRuntimeFlags flags = HardenedRuntimeFlags::kNone;

  if (hasHardenedRuntime())
    flags |= HardenedRuntimeFlags::kEnabled;

  if (hasMapJitSupport())
    flags |= HardenedRuntimeFlags::kMapJit;

  if (hasDualMappingSupport())
    flags |= HardenedRuntimeFlags::kDualMapping;

  return flags;
}

static Error mapMemory(void** p, size_t size, MemoryFlags memoryFlags, int fd = -1, off_t offset = 0) noexcept {
  *p = nullptr;
  if (size == 0)
    return DebugUtils::errored(kErrorInvalidArgument);

  int protection = mmProtFromMemoryFlags(memoryFlags) | mmMaxProtFromMemoryFlags(memoryFlags);
  int mmFlags = mmMapJitFromMemoryFlags(memoryFlags);

  mmFlags |= Support::test(memoryFlags, MemoryFlags::kMapShared) ? MAP_SHARED : MAP_PRIVATE;
  if (fd == -1)
    mmFlags |= MAP_ANONYMOUS;

  bool useLargePages = Support::test(memoryFlags, VirtMem::MemoryFlags::kMMapLargePages);

  if (useLargePages) {
#if defined(__linux__)
    size_t lpSize = largePageSize();
    if (lpSize == 0)
      return DebugUtils::errored(kErrorFeatureNotEnabled);

    if (!Support::isAligned(size, lpSize))
      return DebugUtils::errored(kErrorInvalidArgument);

    unsigned lpSizeLog2 = Support::ctz(lpSize);
    mmFlags |= int(unsigned(MAP_HUGETLB) | (lpSizeLog2 << MAP_HUGE_SHIFT));
#else
    return DebugUtils::errored(kErrorFeatureNotEnabled);
#endif // __linux__
  }

  void* ptr = mmap(nullptr, size, protection, mmFlags, fd, offset);
  if (ptr == MAP_FAILED)
    return DebugUtils::errored(asmjitErrorFromErrno(errno));

#if defined(MADV_HUGEPAGE)
  if (useLargePages) {
    madvise(ptr, size, MADV_HUGEPAGE);
  }
#endif

  *p = ptr;
  return kErrorOk;
}

static Error unmapMemory(void* p, size_t size) noexcept {
  if (ASMJIT_UNLIKELY(munmap(p, size) != 0))
    return DebugUtils::errored(asmjitErrorFromErrno(errno));

  return kErrorOk;
}

Error alloc(void** p, size_t size, MemoryFlags memoryFlags) noexcept {
  return mapMemory(p, size, memoryFlags);
}

Error release(void* p, size_t size) noexcept {
  return unmapMemory(p, size);
}

Error protect(void* p, size_t size, MemoryFlags memoryFlags) noexcept {
  int protection = mmProtFromMemoryFlags(memoryFlags);
  if (mprotect(p, size, protection) == 0)
    return kErrorOk;

  return DebugUtils::errored(asmjitErrorFromErrno(errno));
}

// Virtual Memory [Posix] - Dual Mapping
// =====================================

#if !defined(ASMJIT_NO_DUAL_MAPPING)
static Error unmapDualMapping(DualMapping* dm, size_t size) noexcept {
  Error err1 = unmapMemory(dm->rx, size);
  Error err2 = kErrorOk;

  if (dm->rx != dm->rw)
    err2 = unmapMemory(dm->rw, size);

  // We can report only one error, so report the first...
  if (err1 || err2)
    return DebugUtils::errored(err1 ? err1 : err2);

  dm->rx = nullptr;
  dm->rw = nullptr;
  return kErrorOk;
}
#endif // !ASMJIT_NO_DUAL_MAPPING

#if defined(ASMJIT_ANONYMOUS_MEMORY_USE_REMAPDUP)
static Error allocDualMappingUsingRemapdup(DualMapping* dmOut, size_t size, MemoryFlags memoryFlags) noexcept {
  MemoryFlags maxAccessFlags = regularAccessFlagsToMaxAccessFlags(memoryFlags);
  MemoryFlags finalFlags = memoryFlags | maxAccessFlags | MemoryFlags::kMapShared;

  MemoryFlags rxFlags = finalFlags & ~(MemoryFlags::kAccessWrite | MemoryFlags::kMMapMaxAccessWrite);
  MemoryFlags rwFlags = finalFlags & ~(MemoryFlags::kAccessExecute);

  // Allocate RW mapping.
  DualMapping dm {};
  ASMJIT_PROPAGATE(mapMemory(&dm.rw, size, rwFlags));

  // Allocate RX mapping.
  dm.rx = mremap(dm.rw, size, nullptr, size, MAP_REMAPDUP);
  if (dm.rx == MAP_FAILED) {
    int e = errno;
    munmap(dm.rw, size);
    return DebugUtils::errored(asmjitErrorFromErrno(e));
  }

  if (mprotect(dm.rx, size, mmProtFromMemoryFlags(rxFlags)) != 0) {
    int e = errno;
    unmapDualMapping(&dm, size);
    return DebugUtils::errored(asmjitErrorFromErrno(e));
  }

  *dmOut = dm;
  return kErrorOk;
}
#endif

#if defined(ASMJIT_ANONYMOUS_MEMORY_USE_MACH_VM_REMAP)
static Error asmjitErrorFromKernResult(kern_return_t result) noexcept {
  switch (result) {
    case KERN_PROTECTION_FAILURE:
      return DebugUtils::errored(kErrorProtectionFailure);
    case KERN_NO_SPACE:
      return DebugUtils::errored(kErrorOutOfMemory);
    case KERN_INVALID_ARGUMENT:
      return DebugUtils::errored(kErrorInvalidArgument);
    default:
      return DebugUtils::errored(kErrorInvalidState);
  }
}

static Error allocDualMappingUsingMachVmRemap(DualMapping* dmOut, size_t size, MemoryFlags memoryFlags) noexcept {
  DualMapping dm {};

  MemoryFlags mmapFlags = MemoryFlags::kAccessReadWrite | (memoryFlags & MemoryFlags::kMapShared);
  ASMJIT_PROPAGATE(mapMemory(&dm.rx, size, mmapFlags));

  vm_prot_t curProt;
  vm_prot_t maxProt;

  int rwProtectFlags = VM_PROT_READ | VM_PROT_WRITE;
  int rxProtectFlags = VM_PROT_READ;

  if (Support::test(memoryFlags, MemoryFlags::kAccessExecute))
    rxProtectFlags |= VM_PROT_EXECUTE;

  kern_return_t result {};
  do {
    vm_map_t task = mach_task_self();
    mach_vm_address_t remappedAddr {};

#if defined(VM_FLAGS_RANDOM_ADDR)
    int remapFlags = VM_FLAGS_ANYWHERE | VM_FLAGS_RANDOM_ADDR;
#else
    int remapFlags = VM_FLAGS_ANYWHERE;
#endif

    // Try to remap the existing memory into a different address.
    result = mach_vm_remap(
      task,                       // target_task
      &remappedAddr,              // target_address
      size,                       // size
      0,                          // mask
      remapFlags,                 // flags
      task,                       // src_task
      (mach_vm_address_t)dm.rx,   // src_address
      false,                      // copy
      &curProt,                   // cur_protection
      &maxProt,                   // max_protection
      VM_INHERIT_DEFAULT);        // inheritance

    if (result != KERN_SUCCESS)
      break;

    dm.rw = (void*)remappedAddr;

    // Now, try to change permissions of both map regions into RW and RX. The vm_protect()
    // API is used twice as we also want to set maximum permissions, so nobody would be
    // allowed to change the RX region back to RW or RWX (if RWX is allowed).
    uint32_t i;
    for (i = 0; i < 2; i++) {
      bool setMaximum = (i == 0);

      result = vm_protect(
        task,                       // target_task
        (vm_address_t)dm.rx,        // address
        size,                       // size
        setMaximum,                 // set_maximum
        rxProtectFlags);            // new_protection

      if (result != KERN_SUCCESS)
        break;

      result = vm_protect(task,     // target_task
        (vm_address_t)dm.rw,        // address
        size,                       // size
        setMaximum,                 // set_maximum
        rwProtectFlags);            // new_protection

      if (result != KERN_SUCCESS)
        break;
    }
  } while (0);

  if (result != KERN_SUCCESS) {
    unmapDualMapping(&dm, size);
    return DebugUtils::errored(asmjitErrorFromKernResult(result));
  }

  *dmOut = dm;
  return kErrorOk;
}
#endif // ASMJIT_ANONYMOUS_MEMORY_USE_MACH_VM_REMAP

#if defined(ASMJIT_ANONYMOUS_MEMORY_USE_FD)
static Error allocDualMappingUsingFile(DualMapping* dm, size_t size, MemoryFlags memoryFlags) noexcept {
  bool preferTmpOverDevShm = Support::test(memoryFlags, MemoryFlags::kMappingPreferTmp);
  if (!preferTmpOverDevShm) {
    AnonymousMemoryStrategy strategy;
    ASMJIT_PROPAGATE(getAnonymousMemoryStrategy(&strategy));
    preferTmpOverDevShm = (strategy == AnonymousMemoryStrategy::kTmpDir);
  }

  AnonymousMemory anonMem;
  ASMJIT_PROPAGATE(anonMem.open(preferTmpOverDevShm));
  ASMJIT_PROPAGATE(anonMem.allocate(size));

  void* ptr[2];
  for (uint32_t i = 0; i < 2; i++) {
    MemoryFlags restrictedMemoryFlags = memoryFlags & ~dualMappingFilter[i];
    Error err = mapMemory(&ptr[i], size, restrictedMemoryFlags | MemoryFlags::kMapShared, anonMem.fd(), 0);
    if (err != kErrorOk) {
      if (i == 1)
        unmapMemory(ptr[0], size);
      return err;
    }
  }

  dm->rx = ptr[0];
  dm->rw = ptr[1];
  return kErrorOk;
}
#endif // ASMJIT_ANONYMOUS_MEMORY_USE_FD

Error allocDualMapping(DualMapping* dm, size_t size, MemoryFlags memoryFlags) noexcept {
  dm->rx = nullptr;
  dm->rw = nullptr;

#if defined(ASMJIT_NO_DUAL_MAPPING)
  DebugUtils::unused(size, memoryFlags);
  return DebugUtils::errored(kErrorFeatureNotEnabled);
#else
  if (off_t(size) <= 0)
    return DebugUtils::errored(size == 0 ? kErrorInvalidArgument : kErrorTooLarge);

#if defined(ASMJIT_ANONYMOUS_MEMORY_USE_REMAPDUP)
  return allocDualMappingUsingRemapdup(dm, size, memoryFlags);
#elif defined(ASMJIT_ANONYMOUS_MEMORY_USE_MACH_VM_REMAP)
  return allocDualMappingUsingMachVmRemap(dm, size, memoryFlags);
#elif defined(ASMJIT_ANONYMOUS_MEMORY_USE_FD)
  return allocDualMappingUsingFile(dm, size, memoryFlags);
#else
  #error "[asmjit] VirtMem::allocDualMapping() doesn't have implementation for the target OS or architecture"
#endif
#endif // ASMJIT_NO_DUAL_MAPPING
}

Error releaseDualMapping(DualMapping* dm, size_t size) noexcept {
#if defined(ASMJIT_NO_DUAL_MAPPING)
  DebugUtils::unused(dm, size);
  return DebugUtils::errored(kErrorFeatureNotEnabled);
#else
  return unmapDualMapping(dm, size);
#endif // ASMJIT_NO_DUAL_MAPPING
}
#endif

// Virtual Memory - Flush Instruction Cache
// ========================================

void flushInstructionCache(void* p, size_t size) noexcept {
#if ASMJIT_ARCH_X86 || defined(__EMSCRIPTEN__)
  // X86/X86_64 architecture doesn't require to do anything to flush instruction cache.
  DebugUtils::unused(p, size);
#elif defined(__APPLE__)
  sys_icache_invalidate(p, size);
#elif defined(_WIN32)
  // Windows has a built-in support in `kernel32.dll`.
  FlushInstructionCache(GetCurrentProcess(), p, size);
#elif defined(__GNUC__)
  char* start = static_cast<char*>(p);
  char* end = start + size;
  __builtin___clear_cache(start, end);
#else
  #pragma message("[asmjit] VirtMem::flushInstructionCache() doesn't have implementation for the target OS and compiler")
  DebugUtils::unused(p, size);
#endif
}

// Virtual Memory - Memory Info
// ============================

Info info() noexcept {
  static std::atomic<uint32_t> vmInfoInitialized;
  static Info vmInfo;

  if (!vmInfoInitialized.load()) {
    Info localMemInfo;
    detectVMInfo(localMemInfo);

    vmInfo = localMemInfo;
    vmInfoInitialized.store(1u);
  }

  return vmInfo;
}

size_t largePageSize() noexcept {
  static std::atomic<size_t> largePageSize;
  static constexpr size_t kNotAvailable = 1;

  size_t size = largePageSize.load();
  if (ASMJIT_LIKELY(size > kNotAvailable))
    return size;

  if (size == kNotAvailable)
    return 0;

  size = detectLargePageSize();
  largePageSize.store(size != 0 ? size : kNotAvailable);
  return size;
}

// Virtual Memory - Hardened Runtime Info
// ======================================

HardenedRuntimeInfo hardenedRuntimeInfo() noexcept {
  return HardenedRuntimeInfo { getHardenedRuntimeFlags() };
}

// Virtual Memory - Project JIT Memory
// ===================================

void protectJitMemory(ProtectJitAccess access) noexcept {
#if defined(ASMJIT_HAS_PTHREAD_JIT_WRITE_PROTECT_NP)
  pthread_jit_write_protect_np(static_cast<int>(access));
#else
  DebugUtils::unused(access);
#endif
}

ASMJIT_END_SUB_NAMESPACE

// Virtual Memory - Tests
// ======================

#if defined(ASMJIT_TEST)
ASMJIT_BEGIN_NAMESPACE

UNIT(virt_mem) {
  VirtMem::Info vmInfo = VirtMem::info();

  INFO("VirtMem::info():");
  INFO("  pageSize: %zu", size_t(vmInfo.pageSize));
  INFO("  pageGranularity: %zu", size_t(vmInfo.pageGranularity));

  INFO("VirtMem::largePageSize():");
  INFO("  largePageSize: %zu", size_t(VirtMem::largePageSize()));

  VirtMem::HardenedRuntimeInfo hardenedRtInfo = VirtMem::hardenedRuntimeInfo();
  VirtMem::HardenedRuntimeFlags hardenedFlags = hardenedRtInfo.flags;

  INFO("VirtMem::hardenedRuntimeInfo():");
  INFO("  flags:");
  INFO("    kEnabled: %s"    , Support::test(hardenedFlags, VirtMem::HardenedRuntimeFlags::kEnabled    ) ? "true" : "false");
  INFO("    kMapJit: %s"     , Support::test(hardenedFlags, VirtMem::HardenedRuntimeFlags::kMapJit     ) ? "true" : "false");
  INFO("    kDualMapping: %s", Support::test(hardenedFlags, VirtMem::HardenedRuntimeFlags::kDualMapping) ? "true" : "false");
}

ASMJIT_END_NAMESPACE
#endif // ASMJIT_TEST

#endif // !ASMJIT_NO_JIT

```

`CodeCleaner/asmjit/asmjit/core/virtmem.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_VIRTMEM_H_INCLUDED
#define ASMJIT_CORE_VIRTMEM_H_INCLUDED

#include "../core/api-config.h"
#ifndef ASMJIT_NO_JIT

#include "../core/globals.h"
#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_virtual_memory
//! \{

//! Virtual memory management.
namespace VirtMem {

//! Describes whether instruction cache should be flushed after a write operation.
enum class CachePolicy : uint32_t {
  //! Default policy.
  //!
  //! In some places this would mean `kFlushAfterWrite` and in some places it would mean `kNeverFlush`.
  //! For example if it's known that an address has never been used before to execute code.
  kDefault = 0,

  //! Flush instruction cache after a write operation.
  kFlushAfterWrite = 1,

  //! Avoid flushing instruction cache after a write operation.
  kNeverFlush = 2
};

//! Flushes instruction cache in the given region.
//!
//! Only useful on non-x86 architectures, however, it's a good practice to call it on any platform to make your
//! code more portable.
ASMJIT_API void flushInstructionCache(void* p, size_t size) noexcept;

//! Virtual memory information.
struct Info {
  //! Virtual memory page size.
  uint32_t pageSize;
  //! Virtual memory page granularity.
  uint32_t pageGranularity;
};

//! Returns virtual memory information, see `VirtMem::Info` for more details.
ASMJIT_API Info info() noexcept;

//! Returns the size of the smallest large page supported.
//!
//! AsmJit only uses the smallest large page at the moment as these are usually perfectly sized for executable
//! memory allocation (standard size is 2MB, but different sizes are possible).
//!
//! Returns either the detected large page size or 0, if large page support is either not supported by AsmJit
//! or not accessible to the process.
ASMJIT_API size_t largePageSize() noexcept;

//! Virtual memory access and mmap-specific flags.
enum class MemoryFlags : uint32_t {
  //! No flags.
  kNone = 0,

  //! Memory is readable.
  kAccessRead = 0x00000001u,

  //! Memory is writable.
  kAccessWrite = 0x00000002u,

  //! Memory is executable.
  kAccessExecute = 0x00000004u,

  //! A combination of \ref kAccessRead and \ref kAccessWrite.
  kAccessReadWrite = kAccessRead | kAccessWrite,

  //! A combination of \ref kAccessRead, \ref kAccessWrite.
  kAccessRW = kAccessRead | kAccessWrite,

  //! A combination of \ref kAccessRead and \ref kAccessExecute.
  kAccessRX = kAccessRead | kAccessExecute,

  //! A combination of \ref kAccessRead, \ref kAccessWrite, and \ref kAccessExecute.
  kAccessRWX = kAccessRead | kAccessWrite | kAccessExecute,

  //! Use a `MAP_JIT` flag available on Apple platforms (introduced by Mojave), which allows JIT code to be
  //! executed in a MAC bundle.
  //!
  //! This flag may be turned on by the allocator if there is no other way of allocating executable memory.
  //!
  //! \note This flag can only be used with \ref VirtMem::alloc(), `MAP_JIT` only works on OSX and not on iOS.
  //! When a process uses `fork()` the child process has no access to the pages mapped with `MAP_JIT`.
  kMMapEnableMapJit = 0x00000010u,

  //! Pass `PROT_MAX(PROT_READ)` or `PROT_MPROTECT(PROT_READ)` to `mmap()` on platforms that support it.
  //!
  //! This flag allows to set a "maximum access" that the memory page can get during its lifetime. Use
  //! \ref VirtMem::protect() to change the access flags.
  //!
  //! \note This flag can only be used with \ref VirtMem::alloc() and \ref VirtMem::allocDualMapping().
  //! However \ref VirtMem::allocDualMapping() may automatically use this if \ref kAccessRead is used.
  kMMapMaxAccessRead = 0x00000020u,

  //! Pass `PROT_MAX(PROT_WRITE)` or `PROT_MPROTECT(PROT_WRITE)` to `mmap()` on platforms that support it.
  //!
  //! This flag allows to set a "maximum access" that the memory page can get during its lifetime. Use
  //! \ref VirtMem::protect() to change the access flags.
  //!
  //! \note This flag can only be used with \ref VirtMem::alloc() and \ref VirtMem::allocDualMapping().
  //! However \ref VirtMem::allocDualMapping() may automatically use this if \ref kAccessWrite is used.
  kMMapMaxAccessWrite = 0x00000040u,

  //! Pass `PROT_MAX(PROT_EXEC)` or `PROT_MPROTECT(PROT_EXEC)` to `mmap()` on platforms that support it.
  //!
  //! This flag allows to set a "maximum access" that the memory page can get during its lifetime. Use
  //! \ref VirtMem::protect() to change the access flags.
  //!
  //! \note This flag can only be used with \ref VirtMem::alloc() and \ref VirtMem::allocDualMapping().
  //! However \ref VirtMem::allocDualMapping() may automatically use this if \ref kAccessExecute is used.
  kMMapMaxAccessExecute = 0x00000080u,

  //! A combination of \ref kMMapMaxAccessRead and \ref kMMapMaxAccessWrite.
  kMMapMaxAccessReadWrite = kMMapMaxAccessRead | kMMapMaxAccessWrite,

  //! A combination of \ref kMMapMaxAccessRead and \ref kMMapMaxAccessWrite.
  kMMapMaxAccessRW = kMMapMaxAccessRead | kMMapMaxAccessWrite,

  //! A combination of \ref kMMapMaxAccessRead and \ref kMMapMaxAccessExecute.
  kMMapMaxAccessRX = kMMapMaxAccessRead | kMMapMaxAccessExecute,

  //! A combination of \ref kMMapMaxAccessRead, \ref kMMapMaxAccessWrite, \ref kMMapMaxAccessExecute.
  kMMapMaxAccessRWX = kMMapMaxAccessRead | kMMapMaxAccessWrite | kMMapMaxAccessExecute,

  //! Use `MAP_SHARED` when calling mmap().
  //!
  //! \note In some cases `MAP_SHARED` may be set automatically. For example, some dual mapping implementations must
  //! use `MAP_SHARED` instead of `MAP_PRIVATE` to ensure that the OS would not apply copy on write on RW page, which
  //! would cause RX page not having the updated content.
  kMapShared = 0x00000100u,

  //! Request large memory mapped pages.
  //!
  //! \remarks If this option is used and large page(s) cannot be mapped, the allocation will fail. Fallback to
  //! regular pages must be done by the user in this case. Higher level API such as \ref JitAllocator provides an
  //! additional mechanism to allocate regular page(s) when large page(s) allocation fails.
  kMMapLargePages = 0x00000200u,

  //! Not an access flag, only used by `allocDualMapping()` to override the default allocation strategy to always use
  //! a 'tmp' directory instead of "/dev/shm" (on POSIX platforms). Please note that this flag will be ignored if the
  //! operating system allows to allocate an executable memory by a different API than `open()` or `shm_open()`. For
  //! example on Linux `memfd_create()` is preferred and on BSDs `shm_open(SHM_ANON, ...)` is used if SHM_ANON is
  //! defined.
  //!
  //! \note This flag can only be used with \ref VirtMem::alloc().
  kMappingPreferTmp = 0x80000000u
};
ASMJIT_DEFINE_ENUM_FLAGS(MemoryFlags)

//! Allocates virtual memory by either using `mmap()` (POSIX) or `VirtualAlloc()` (Windows).
//!
//! \note `size` should be aligned to page size, use \ref VirtMem::info() to obtain it. Invalid size will not be
//! corrected by the implementation and the allocation would not succeed in such case.
ASMJIT_API Error alloc(void** p, size_t size, MemoryFlags flags) noexcept;

//! Releases virtual memory previously allocated by \ref VirtMem::alloc().
//!
//! \note The size must be the same as used by \ref VirtMem::alloc(). If the size is not the same value the call
//! will fail on any POSIX system, but pass on Windows, because it's implemented differently.
ASMJIT_API Error release(void* p, size_t size) noexcept;

//! A cross-platform wrapper around `mprotect()` (POSIX) and `VirtualProtect()` (Windows).
ASMJIT_API Error protect(void* p, size_t size, MemoryFlags flags) noexcept;

//! Dual memory mapping used to map an anonymous memory into two memory regions where one region is read-only, but
//! executable, and the second region is read+write, but not executable. See \ref VirtMem::allocDualMapping() for
//! more details.
struct DualMapping {
  //! Pointer to data with 'Read+Execute' access (this memory is not writable).
  void* rx;
  //! Pointer to data with 'Read+Write' access (this memory is not executable).
  void* rw;
};

//! Allocates virtual memory and creates two views of it where the first view has no write access. This is an addition
//! to the API that should be used in cases in which the operating system either enforces W^X security policy or the
//! application wants to use this policy by default to improve security and prevent an accidental (or purposed)
//! self-modifying code.
//!
//! The memory returned in the `dm` are two independent mappings of the same shared memory region. You must use
//! \ref VirtMem::releaseDualMapping() to release it when it's no longer needed. Never use `VirtMem::release()` to
//! release the memory returned by `allocDualMapping()` as that would fail on Windows.
//!
//! \remarks Both pointers in `dm` would be set to `nullptr` if the function fails.
ASMJIT_API Error allocDualMapping(DualMapping* dm, size_t size, MemoryFlags flags) noexcept;

//! Releases virtual memory mapping previously allocated by \ref VirtMem::allocDualMapping().
//!
//! \remarks Both pointers in `dm` would be set to `nullptr` if the function succeeds.
ASMJIT_API Error releaseDualMapping(DualMapping* dm, size_t size) noexcept;

//! Hardened runtime flags.
enum class HardenedRuntimeFlags : uint32_t {
  //! No flags.
  kNone = 0,

  //! Hardened runtime is enabled - it's not possible to have "Write & Execute" memory protection. The runtime
  //! enforces W^X (either write or execute).
  //!
  //! \note If the runtime is hardened it means that an operating system specific protection is used. For example
  //! on Apple OSX it's possible to allocate memory with MAP_JIT flag and then use `pthread_jit_write_protect_np()`
  //! to temporarily swap access permissions for the current thread. Dual mapping is also a possibility on X86/X64
  //! architecture.
  kEnabled = 0x00000001u,

  //! Read+Write+Execute can only be allocated with MAP_JIT flag (Apple specific, only available on Apple platforms).
  kMapJit = 0x00000002u,

  //! Read+Write+Execute can be allocated with dual mapping approach (one region with RW and the other with RX).
  kDualMapping = 0x00000004u
};
ASMJIT_DEFINE_ENUM_FLAGS(HardenedRuntimeFlags)

//! Hardened runtime information.
struct HardenedRuntimeInfo {
  //! \name Members
  //! \{

  //! Hardened runtime flags.
  HardenedRuntimeFlags flags;

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the hardened runtime `flag` is set.
  ASMJIT_INLINE_NODEBUG bool hasFlag(HardenedRuntimeFlags flag) const noexcept { return Support::test(flags, flag); }

  //! \}
};

//! Returns runtime features provided by the OS.
ASMJIT_API HardenedRuntimeInfo hardenedRuntimeInfo() noexcept;

//! Values that can be used with `protectJitMemory()` function.
enum class ProtectJitAccess : uint32_t {
  //! Protect JIT memory with Read+Write permissions.
  kReadWrite = 0,
  //! Protect JIT memory with Read+Execute permissions.
  kReadExecute = 1
};

//! Protects access of memory mapped with MAP_JIT flag for the current thread.
//!
//! \note This feature is only available on Apple hardware (AArch64) at the moment and uses a non-portable
//! `pthread_jit_write_protect_np()` call when available.
//!
//! This function must be called before and after a memory mapped with MAP_JIT flag is modified. Example:
//!
//! ```
//! void* codePtr = ...;
//! size_t codeSize = ...;
//!
//! VirtMem::protectJitMemory(VirtMem::ProtectJitAccess::kReadWrite);
//! memcpy(codePtr, source, codeSize);
//! VirtMem::protectJitMemory(VirtMem::ProtectJitAccess::kReadExecute);
//! VirtMem::flushInstructionCache(codePtr, codeSize);
//! ```
//!
//! See \ref ProtectJitReadWriteScope, which makes it simpler than the code above.
ASMJIT_API void protectJitMemory(ProtectJitAccess access) noexcept;

//! JIT protection scope that prepares the given memory block to be written to in the current thread.
//!
//! It calls `VirtMem::protectJitMemory(VirtMem::ProtectJitAccess::kReadWrite)` at construction time and
//! `VirtMem::protectJitMemory(VirtMem::ProtectJitAccess::kReadExecute)` combined with `flushInstructionCache()`
//! in destructor. The purpose of this class is to make writing to JIT memory easier.
class ProtectJitReadWriteScope {
public:
  ASMJIT_NONCOPYABLE(ProtectJitReadWriteScope)

  //! \name Members
  //! \{

  void* _rxPtr;
  size_t _size;
  CachePolicy _policy;

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Makes the given memory block RW protected.
  ASMJIT_FORCE_INLINE ProtectJitReadWriteScope(
    void* rxPtr,
    size_t size,
    CachePolicy policy = CachePolicy::kDefault) noexcept
    : _rxPtr(rxPtr),
      _size(size),
      _policy(policy) {
    protectJitMemory(ProtectJitAccess::kReadWrite);
  }

  //! Makes the memory block RX protected again and flushes instruction cache.
  ASMJIT_FORCE_INLINE  ~ProtectJitReadWriteScope() noexcept {
    protectJitMemory(ProtectJitAccess::kReadExecute);

    if (_policy != CachePolicy::kNeverFlush)
      flushInstructionCache(_rxPtr, _size);
  }

  //! \}
};

} // VirtMem

//! \}

ASMJIT_END_NAMESPACE

#endif
#endif // ASMJIT_CORE_VIRTMEM_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/zone.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/support.h"
#include "../core/zone.h"

ASMJIT_BEGIN_NAMESPACE

// Zone - Globals
// ==============

// Zero size block used by `Zone` that doesn't have any memory allocated. Should be allocated in read-only memory
// and should never be modified.
const Zone::Block Zone::_zeroBlock {};

static inline void Zone_assignZeroBlock(Zone* zone) noexcept {
  Zone::Block* block = const_cast<Zone::Block*>(&zone->_zeroBlock);
  zone->_ptr = block->data();
  zone->_end = block->data();
  zone->_block = block;
}

static inline void Zone_assignBlock(Zone* zone, Zone::Block* block) noexcept {
  size_t alignment = zone->blockAlignment();
  zone->_ptr = Support::alignUp(block->data(), alignment);
  zone->_end = block->data() + block->size;
  zone->_block = block;
}

// Zone - Initialization & Reset
// =============================

void Zone::_init(size_t blockSize, size_t blockAlignment, const Support::Temporary* temporary) noexcept {
  ASMJIT_ASSERT(blockSize >= kMinBlockSize);
  ASMJIT_ASSERT(blockSize <= kMaxBlockSize);
  ASMJIT_ASSERT(blockAlignment <= 64);

  Zone_assignZeroBlock(this);

  size_t blockSizeShift = Support::bitSizeOf<size_t>() - Support::clz(blockSize);
  size_t blockAlignmentShift = Support::bitSizeOf<size_t>() - Support::clz(blockAlignment | (size_t(1) << 3));

  _blockAlignmentShift = uint8_t(blockAlignmentShift);
  _minimumBlockSizeShift = uint8_t(blockSizeShift);
  _maximumBlockSizeShift = uint8_t(25); // (1 << 25) Equals 32 MiB blocks (should be enough for all cases)
  _hasStaticBlock = uint8_t(temporary != nullptr);
  _reserved = uint8_t(0u);
  _blockCount = size_t(temporary != nullptr);

  // Setup the first [temporary] block, if necessary.
  if (temporary) {
    Block* block = temporary->data<Block>();
    block->prev = nullptr;
    block->next = nullptr;

    ASMJIT_ASSERT(temporary->size() >= kBlockSize);
    block->size = temporary->size() - kBlockSize;

    Zone_assignBlock(this, block);
    _blockCount = 1u;
  }
}

void Zone::reset(ResetPolicy resetPolicy) noexcept {
  Block* cur = _block;

  // Can't be altered.
  if (cur == &_zeroBlock) {
    return;
  }

  if (resetPolicy == ResetPolicy::kHard) {
    bool hasStatic = hasStaticBlock();
    Block* initial = const_cast<Zone::Block*>(&_zeroBlock);

    _ptr = initial->data();
    _end = initial->data();
    _block = initial;
    _blockCount = size_t(hasStatic);

    // Since cur can be in the middle of the double-linked list, we have to traverse both directions (`prev` and
    // `next`) separately to visit all.
    Block* next = cur->next;
    do {
      Block* prev = cur->prev;

      if (prev == nullptr && hasStatic) {
        // If this is the first block and this Zone is actually a ZoneTmp then the first block cannot be freed.
        cur->prev = nullptr;
        cur->next = nullptr;
        Zone_assignBlock(this, cur);
        break;
      }

      ::free(cur);
      cur = prev;
    } while (cur);

    cur = next;
    while (cur) {
      next = cur->next;
      ::free(cur);
      cur = next;
    }
  }
  else {
    while (cur->prev) {
      cur = cur->prev;
    }
    Zone_assignBlock(this, cur);
  }
}

// Zone - Alloc
// ============

void* Zone::_alloc(size_t size, size_t alignment) noexcept {
  Block* curBlock = _block;
  Block* next = curBlock->next;

  size_t defaultBlockAlignment = blockAlignment();
  size_t requiredBlockAlignment = Support::max<size_t>(alignment, defaultBlockAlignment);

  // If the `Zone` has been cleared the current block doesn't have to be the last one. Check if there is a block
  // that can be used instead of allocating a new one. If there is a `next` block it's completely unused, we don't
  // have to check for remaining bytes in that case.
  if (next) {
    uint8_t* ptr = Support::alignUp(next->data(), requiredBlockAlignment);
    uint8_t* end = next->data() + next->size;

    if (size <= (size_t)(end - ptr)) {
      _block = next;
      _ptr = ptr + size;
      _end = end;
      return static_cast<void*>(ptr);
    }
  }

  // Calculates the "default" size of a next block - in most cases this would be enough for the allocation. In
  // general we want to gradually increase block size when more and more blocks are allocated until the maximum
  // block size. Since we use shifts (aka log2(size) sizes) we just need block count and minumum/maximum block
  // size shift to calculate the final size.
  size_t defaultBlockSizeShift = Support::min<size_t>(_blockCount + _minimumBlockSizeShift, _maximumBlockSizeShift);
  size_t defaultBlockSize = size_t(1) << defaultBlockSizeShift;

  // Allocate a new block. We have to accommodate all possible overheads so after the memory is allocated and then
  // properly aligned there will be size for the requested memory. In 99.9999% cases this is never a problem, but
  // we must be sure that even rare border cases would allocate properly.
  size_t alignmentOverhead = requiredBlockAlignment - Support::min<size_t>(requiredBlockAlignment, Globals::kAllocAlignment);
  size_t blockSizeOverhead = kBlockSize + Globals::kAllocOverhead + alignmentOverhead;

  // If the requested size is larger than a default calculated block size -> increase block size so the allocation
  // would be enough to fit the requested size.
  size_t finalBlockSize = defaultBlockSize;

  if (ASMJIT_UNLIKELY(size > defaultBlockSize - blockSizeOverhead)) {
    if (ASMJIT_UNLIKELY(size > SIZE_MAX - blockSizeOverhead)) {
      // This would probably never happen in practice - however, it needs to be done to stop malicious cases like
      // `alloc(SIZE_MAX)`.
      return nullptr;
    }
    finalBlockSize = size + alignmentOverhead + kBlockSize;
  }
  else {
    finalBlockSize -= Globals::kAllocOverhead;
  }

  // Allocate new block.
  Block* newBlock = static_cast<Block*>(::malloc(finalBlockSize));

  if (ASMJIT_UNLIKELY(!newBlock)) {
    return nullptr;
  }

  // finalBlockSize includes the struct size, which must be avoided when assigning the size to a newly allocated block.
  size_t realBlockSize = finalBlockSize - kBlockSize;

  // Align the pointer to `minimumAlignment` and adjust the size of this block accordingly. It's the same as using
  // `minimumAlignment - Support::alignUpDiff()`, just written differently.
  newBlock->prev = nullptr;
  newBlock->next = nullptr;
  newBlock->size = realBlockSize;

  if (curBlock != &_zeroBlock) {
    newBlock->prev = curBlock;
    curBlock->next = newBlock;

    // Does only happen if there is a next block, but the requested memory can't fit into it. In this case a new
    // buffer is allocated and inserted between the current block and the next one.
    if (next) {
      newBlock->next = next;
      next->prev = newBlock;
    }
  }

  uint8_t* ptr = Support::alignUp(newBlock->data(), requiredBlockAlignment);
  uint8_t* end = newBlock->data() + realBlockSize;

  _ptr = ptr + size;
  _end = end;
  _block = newBlock;
  _blockCount++;

  ASMJIT_ASSERT(_ptr <= _end);
  return static_cast<void*>(ptr);
}

void* Zone::allocZeroed(size_t size, size_t alignment) noexcept {
  void* p = alloc(size, alignment);
  if (ASMJIT_UNLIKELY(!p))
    return p;
  return memset(p, 0, size);
}

void* Zone::dup(const void* data, size_t size, bool nullTerminate) noexcept {
  if (ASMJIT_UNLIKELY(!data || !size))
    return nullptr;

  ASMJIT_ASSERT(size != SIZE_MAX);
  uint8_t* m = allocT<uint8_t>(size + nullTerminate);
  if (ASMJIT_UNLIKELY(!m)) return nullptr;

  memcpy(m, data, size);
  if (nullTerminate) m[size] = '\0';

  return static_cast<void*>(m);
}

char* Zone::sformat(const char* fmt, ...) noexcept {
  if (ASMJIT_UNLIKELY(!fmt))
    return nullptr;

  char buf[512];
  size_t size;
  va_list ap;

  va_start(ap, fmt);
  size = unsigned(vsnprintf(buf, ASMJIT_ARRAY_SIZE(buf) - 1, fmt, ap));
  va_end(ap);

  buf[size++] = 0;
  return static_cast<char*>(dup(buf, size));
}

// ZoneAllocator - Utilities
// =========================

#if defined(ASMJIT_BUILD_DEBUG)
static bool ZoneAllocator_hasDynamicBlock(ZoneAllocator* self, ZoneAllocator::DynamicBlock* block) noexcept {
  ZoneAllocator::DynamicBlock* cur = self->_dynamicBlocks;
  while (cur) {
    if (cur == block)
      return true;
    cur = cur->next;
  }
  return false;
}
#endif

// ZoneAllocator - Initialization & Reset
// ======================================

void ZoneAllocator::reset(Zone* zone) noexcept {
  // Free dynamic blocks.
  DynamicBlock* block = _dynamicBlocks;
  while (block) {
    DynamicBlock* next = block->next;
    ::free(block);
    block = next;
  }

  _zone = zone;
  memset(_slots, 0, sizeof(_slots));
  _dynamicBlocks = nullptr;
}

// asmjit::ZoneAllocator - Alloc & Release
// =======================================

void* ZoneAllocator::_alloc(size_t size, size_t& allocatedSize) noexcept {
  ASMJIT_ASSERT(isInitialized());

  // Use the memory pool only if the requested block has a reasonable size.
  uint32_t slot;
  if (_getSlotIndex(size, slot, allocatedSize)) {
    // Slot reuse.
    uint8_t* p = reinterpret_cast<uint8_t*>(_slots[slot]);
    size = allocatedSize;

    if (p) {
      _slots[slot] = reinterpret_cast<Slot*>(p)->next;
      return p;
    }

    _zone->align(kBlockAlignment);
    p = _zone->ptr();
    size_t remain = (size_t)(_zone->end() - p);

    if (ASMJIT_LIKELY(remain >= size)) {
      _zone->setPtr(p + size);
      return p;
    }
    else {
      // Distribute the remaining memory to suitable slots, if possible.
      if (remain >= kLoGranularity) {
        do {
          size_t distSize = Support::min<size_t>(remain, kLoMaxSize);
          uint32_t distSlot = uint32_t((distSize - kLoGranularity) / kLoGranularity);
          ASMJIT_ASSERT(distSlot < kLoCount);

          reinterpret_cast<Slot*>(p)->next = _slots[distSlot];
          _slots[distSlot] = reinterpret_cast<Slot*>(p);

          p += distSize;
          remain -= distSize;
        } while (remain >= kLoGranularity);
        _zone->setPtr(p);
      }

      p = static_cast<uint8_t*>(_zone->_alloc(size, kBlockAlignment));
      if (ASMJIT_UNLIKELY(!p)) {
        allocatedSize = 0;
        return nullptr;
      }

      return p;
    }
  }
  else {
    // Allocate a dynamic block.
    size_t blockOverhead = sizeof(DynamicBlock) + sizeof(DynamicBlock*) + kBlockAlignment;

    // Handle a possible overflow.
    if (ASMJIT_UNLIKELY(blockOverhead >= SIZE_MAX - size))
      return nullptr;

    void* p = ::malloc(size + blockOverhead);
    if (ASMJIT_UNLIKELY(!p)) {
      allocatedSize = 0;
      return nullptr;
    }

    // Link as first in `_dynamicBlocks` double-linked list.
    DynamicBlock* block = static_cast<DynamicBlock*>(p);
    DynamicBlock* next = _dynamicBlocks;

    if (next)
      next->prev = block;

    block->prev = nullptr;
    block->next = next;
    _dynamicBlocks = block;

    // Align the pointer to the guaranteed alignment and store `DynamicBlock`
    // at the beginning of the memory block, so `_releaseDynamic()` can find it.
    p = Support::alignUp(static_cast<uint8_t*>(p) + sizeof(DynamicBlock) + sizeof(DynamicBlock*), kBlockAlignment);
    reinterpret_cast<DynamicBlock**>(p)[-1] = block;

    allocatedSize = size;
    return p;
  }
}

void* ZoneAllocator::_allocZeroed(size_t size, size_t& allocatedSize) noexcept {
  ASMJIT_ASSERT(isInitialized());

  void* p = _alloc(size, allocatedSize);
  if (ASMJIT_UNLIKELY(!p)) return p;
  return memset(p, 0, allocatedSize);
}

void ZoneAllocator::_releaseDynamic(void* p, size_t size) noexcept {
  DebugUtils::unused(size);
  ASMJIT_ASSERT(isInitialized());

  // Pointer to `DynamicBlock` is stored at [-1].
  DynamicBlock* block = reinterpret_cast<DynamicBlock**>(p)[-1];
  ASMJIT_ASSERT(ZoneAllocator_hasDynamicBlock(this, block));

  // Unlink and free.
  DynamicBlock* prev = block->prev;
  DynamicBlock* next = block->next;

  if (prev)
    prev->next = next;
  else
    _dynamicBlocks = next;

  if (next)
    next->prev = prev;

  ::free(block);
}

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/zone.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ZONE_H_INCLUDED
#define ASMJIT_CORE_ZONE_H_INCLUDED

#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_zone
//! \{

//! Zone memory.
//!
//! Zone is an incremental memory allocator that allocates memory by simply incrementing a pointer. It allocates
//! blocks of memory by using C's `malloc()`, but divides these blocks into smaller segments requested by calling
//! `Zone::alloc()` and friends.
//!
//! Zone has no function to release the allocated memory. It has to be released all at once by calling `reset()`.
//! If you need a more friendly allocator that also supports `release()`, consider using `Zone` with `ZoneAllocator`.
class Zone {
public:
  ASMJIT_NONCOPYABLE(Zone)

  //! \cond INTERNAL

  //! A single block of memory managed by `Zone`.
  struct Block {
    inline uint8_t* data() const noexcept {
      return const_cast<uint8_t*>(reinterpret_cast<const uint8_t*>(this) + sizeof(*this));
    }

    //! Link to the previous block.
    Block* prev;
    //! Link to the next block.
    Block* next;
    //! Size of the block.
    size_t size;
  };

  enum Limits : size_t {
    kMinBlockSize = 256, // The number is ridiculously small, but still possible.
    kMaxBlockSize = size_t(1) << (sizeof(size_t) * 8 - 1),

    kMinAlignment = 1,
    kMaxAlignment = 64,

    kBlockSize = sizeof(Block),
    kBlockOverhead = kBlockSize + Globals::kAllocOverhead
  };

  //! Pointer in the current block.
  uint8_t* _ptr;
  //! End of the current block.
  uint8_t* _end;
  //! Current block.
  Block* _block;

  //! Block alignment shift
  uint8_t _blockAlignmentShift;
  //! Minimum log2(blockSize) to allocate.
  uint8_t _minimumBlockSizeShift;
  //! Maximum log2(blockSize) to allocate.
  uint8_t _maximumBlockSizeShift;
  //! True when the Zone has a static block (static blocks are used by ZoneTmp).
  uint8_t _hasStaticBlock;
  //! Reserved for future use, must be zero.
  uint32_t _reserved;
  //! Count of allocated blocks.
  size_t _blockCount;

  static ASMJIT_API const Block _zeroBlock;

  //! \endcond

  //! \name Construction & Destruction
  //! \{

  //! Creates a new Zone.
  //!
  //! The `blockSize` parameter describes the default size of the block. If the `size` parameter passed to `alloc()`
  //! is greater than the default size `Zone` will allocate and use a larger block, but it will not change the
  //! default `blockSize`.
  //!
  //! It's not required, but it's good practice to set `blockSize` to a reasonable value that depends on the usage
  //! of `Zone`. Greater block sizes are generally safer and perform better than unreasonably low block sizes.
  ASMJIT_INLINE_NODEBUG explicit Zone(size_t minimumBlockSize, size_t blockAlignment = 1) noexcept {
    _init(minimumBlockSize, blockAlignment, nullptr);
  }

  //! Creates a new Zone with a first block pointing to a `temporary` memory.
  ASMJIT_INLINE_NODEBUG Zone(size_t minimumBlockSize, size_t blockAlignment, const Support::Temporary& temporary) noexcept {
    _init(minimumBlockSize, blockAlignment, &temporary);
  }

  //! \overload
  ASMJIT_INLINE_NODEBUG Zone(size_t minimumBlockSize, size_t blockAlignment, const Support::Temporary* temporary) noexcept {
    _init(minimumBlockSize, blockAlignment, temporary);
  }

  //! Moves an existing `Zone`.
  //!
  //! \note You cannot move an existing `ZoneTmp` as it uses embedded storage. Attempting to move `ZoneTmp` would
  //! cause an undefined behavior (covered by assertions in debug mode).
  inline Zone(Zone&& other) noexcept
    : _ptr(other._ptr),
      _end(other._end),
      _block(other._block),
      _blockAlignmentShift(other._blockAlignmentShift),
      _minimumBlockSizeShift(other._minimumBlockSizeShift),
      _maximumBlockSizeShift(other._maximumBlockSizeShift),
      _hasStaticBlock(other._hasStaticBlock),
      _reserved(other._reserved),
      _blockCount(other._blockCount) {
    ASMJIT_ASSERT(!other.hasStaticBlock());
    other._block = const_cast<Block*>(&_zeroBlock);
    other._ptr = other._block->data();
    other._end = other._block->data();
    other._blockCount = 0u;
  }

  //! Destroys the `Zone` instance.
  //!
  //! This will destroy the `Zone` instance and release all blocks of memory allocated by it. It performs implicit
  //! `reset(ResetPolicy::kHard)`.
  ASMJIT_INLINE_NODEBUG ~Zone() noexcept { reset(ResetPolicy::kHard); }

  ASMJIT_API void _init(size_t blockSize, size_t blockAlignment, const Support::Temporary* temporary) noexcept;

  //! Resets the `Zone` invalidating all blocks allocated.
  //!
  //! See `Globals::ResetPolicy` for more details.
  ASMJIT_API void reset(ResetPolicy resetPolicy = ResetPolicy::kSoft) noexcept;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the default block alignment.
  ASMJIT_INLINE_NODEBUG size_t blockAlignment() const noexcept { return size_t(1) << _blockAlignmentShift; }
  //! Returns a minimum block size.
  ASMJIT_INLINE_NODEBUG size_t minimumBlockSize() const noexcept { return size_t(1) << _minimumBlockSizeShift; }
  //! Returns a maximum block size.
  ASMJIT_INLINE_NODEBUG size_t maximumBlockSize() const noexcept { return size_t(1) << _maximumBlockSizeShift; }
  //! Tests whether this `Zone` is actually a `ZoneTmp` that uses temporary memory.
  ASMJIT_INLINE_NODEBUG uint8_t hasStaticBlock() const noexcept { return _hasStaticBlock; }

  //! Returns remaining size of the current block.
  ASMJIT_INLINE_NODEBUG size_t remainingSize() const noexcept { return (size_t)(_end - _ptr); }

  //! Returns the current zone cursor (dangerous).
  //!
  //! This is a function that can be used to get exclusive access to the current block's memory buffer.
  template<typename T = uint8_t>
  ASMJIT_INLINE_NODEBUG T* ptr() noexcept { return reinterpret_cast<T*>(_ptr); }

  //! Returns the end of the current zone block, only useful if you use `ptr()`.
  template<typename T = uint8_t>
  ASMJIT_INLINE_NODEBUG T* end() noexcept { return reinterpret_cast<T*>(_end); }

  //! Sets the current zone pointer to `ptr` (must be within the current block).
  template<typename T>
  inline void setPtr(T* ptr) noexcept {
    uint8_t* p = reinterpret_cast<uint8_t*>(ptr);
    ASMJIT_ASSERT(p >= _ptr && p <= _end);
    _ptr = p;
  }

  //! Sets the end zone pointer to `end` (must be within the current block).
  template<typename T>
  inline void setEnd(T* end) noexcept {
    uint8_t* p = reinterpret_cast<uint8_t*>(end);
    ASMJIT_ASSERT(p >= _ptr && p <= _end);
    _end = p;
  }

  //! \}

  //! \name Utilities
  //! \{

  inline void swap(Zone& other) noexcept {
    // This could lead to a disaster.
    ASMJIT_ASSERT(!this->hasStaticBlock());
    ASMJIT_ASSERT(!other.hasStaticBlock());

    std::swap(_ptr, other._ptr);
    std::swap(_end, other._end);
    std::swap(_block, other._block);

    std::swap(_blockAlignmentShift, other._blockAlignmentShift);
    std::swap(_minimumBlockSizeShift, other._minimumBlockSizeShift);
    std::swap(_maximumBlockSizeShift, other._maximumBlockSizeShift);
    std::swap(_hasStaticBlock, other._hasStaticBlock);
    std::swap(_reserved, other._reserved);
    std::swap(_blockCount, other._blockCount);
  }

  //! Aligns the current pointer to `alignment`.
  ASMJIT_INLINE_NODEBUG void align(size_t alignment) noexcept {
    _ptr = Support::min(Support::alignUp(_ptr, alignment), _end);
  }

  //! Ensures the remaining size is at least equal or greater than `size`.
  //!
  //! \note This function doesn't respect any alignment. If you need to ensure there is enough room for an aligned
  //! allocation you need to call `align()` before calling `ensure()`.
  ASMJIT_INLINE_NODEBUG Error ensure(size_t size) noexcept {
    if (ASMJIT_LIKELY(size <= remainingSize()))
      return kErrorOk;
    else
      return _alloc(0, 1) ? kErrorOk : DebugUtils::errored(kErrorOutOfMemory);
  }

  //! \}

  //! \name Allocation
  //! \{

  //! Allocates the requested memory specified by `size`.
  //!
  //! Pointer returned is valid until the `Zone` instance is destroyed or reset by calling `reset()`. If you plan to
  //! make an instance of C++ from the given pointer use placement `new` and `delete` operators:
  //!
  //! ```
  //! using namespace asmjit;
  //!
  //! class Object { ... };
  //!
  //! // Create Zone with default block size of 65536 bytes (the maximum size per alloc() would be slightly less).
  //! Zone zone(65536);
  //!
  //! // Create your objects using zone object allocating, for example:
  //! Object* obj = static_cast<Object*>( zone.alloc(sizeof(Object)) );
  //!
  //! if (!obj) {
  //!   // Handle out of memory error.
  //! }
  //!
  //! // Placement `new` and `delete` operators can be used to instantiate it.
  //! new(obj) Object();
  //!
  //! // ... lifetime of your objects ...
  //!
  //! // To destroy the instance (if required).
  //! obj->~Object();
  //!
  //! // Reset or destroy `Zone`.
  //! zone.reset();
  //! ```
  inline void* alloc(size_t size) noexcept {
    if (ASMJIT_UNLIKELY(size > remainingSize()))
      return _alloc(size, 1);

    uint8_t* ptr = _ptr;
    _ptr += size;
    return static_cast<void*>(ptr);
  }

  //! Allocates the requested memory specified by `size` and `alignment`.
  inline void* alloc(size_t size, size_t alignment) noexcept {
    ASMJIT_ASSERT(Support::isPowerOf2(alignment));
    uint8_t* ptr = Support::alignUp(_ptr, alignment);

    if (ptr >= _end || size > (size_t)(_end - ptr))
      return _alloc(size, alignment);

    _ptr = ptr + size;
    return static_cast<void*>(ptr);
  }

  //! Allocates the requested memory specified by `size` without doing any checks.
  //!
  //! Can only be called if `remainingSize()` returns size at least equal to `size`.
  inline void* allocNoCheck(size_t size) noexcept {
    ASMJIT_ASSERT(remainingSize() >= size);

    uint8_t* ptr = _ptr;
    _ptr += size;
    return static_cast<void*>(ptr);
  }

  //! Allocates the requested memory specified by `size` and `alignment` without doing any checks.
  //!
  //! Performs the same operation as `Zone::allocNoCheck(size)` with `alignment` applied.
  inline void* allocNoCheck(size_t size, size_t alignment) noexcept {
    ASMJIT_ASSERT(Support::isPowerOf2(alignment));

    uint8_t* ptr = Support::alignUp(_ptr, alignment);
    ASMJIT_ASSERT(size <= (size_t)(_end - ptr));

    _ptr = ptr + size;
    return static_cast<void*>(ptr);
  }

  //! Allocates `size` bytes of zeroed memory. See `alloc()` for more details.
  ASMJIT_API void* allocZeroed(size_t size, size_t alignment = 1) noexcept;

  //! Like `alloc()`, but the return pointer is casted to `T*`.
  template<typename T>
  inline T* allocT(size_t size = sizeof(T), size_t alignment = alignof(T)) noexcept {
    return static_cast<T*>(alloc(size, alignment));
  }

  //! Like `allocNoCheck()`, but the return pointer is casted to `T*`.
  template<typename T>
  inline T* allocNoCheckT(size_t size = sizeof(T), size_t alignment = alignof(T)) noexcept {
    return static_cast<T*>(allocNoCheck(size, alignment));
  }

  //! Like `allocZeroed()`, but the return pointer is casted to `T*`.
  template<typename T>
  inline T* allocZeroedT(size_t size = sizeof(T), size_t alignment = alignof(T)) noexcept {
    return static_cast<T*>(allocZeroed(size, alignment));
  }

  //! Like `new(std::nothrow) T(...)`, but allocated by `Zone`.
  template<typename T>
  inline T* newT() noexcept {
    void* p = alloc(sizeof(T), alignof(T));
    if (ASMJIT_UNLIKELY(!p))
      return nullptr;
    return new(Support::PlacementNew{p}) T();
  }

  //! Like `new(std::nothrow) T(...)`, but allocated by `Zone`.
  template<typename T, typename... Args>
  inline T* newT(Args&&... args) noexcept {
    void* p = alloc(sizeof(T), alignof(T));
    if (ASMJIT_UNLIKELY(!p))
      return nullptr;
    return new(Support::PlacementNew{p}) T(std::forward<Args>(args)...);
  }

  //! \cond INTERNAL
  //!
  //! Internal alloc function used by other inlines.
  ASMJIT_API void* _alloc(size_t size, size_t alignment) noexcept;
  //! \endcond

  //! Helper to duplicate data.
  ASMJIT_API void* dup(const void* data, size_t size, bool nullTerminate = false) noexcept;

  //! Helper to duplicate data.
  inline void* dupAligned(const void* data, size_t size, size_t alignment, bool nullTerminate = false) noexcept {
    align(alignment);
    return dup(data, size, nullTerminate);
  }

  //! Helper to duplicate a formatted string, maximum size is 256 bytes.
  ASMJIT_API char* sformat(const char* str, ...) noexcept;

  //! \}

#if !defined(ASMJIT_NO_DEPRECATED)
  ASMJIT_DEPRECATED("Use Zone::minimumBlockSize() instead of Zone::blockSize()")
  ASMJIT_INLINE_NODEBUG size_t blockSize() const noexcept { return minimumBlockSize(); }

  ASMJIT_DEPRECATED("Use Zone::hasStaticBlock() instead of Zone::isTemporary()")
  ASMJIT_INLINE_NODEBUG bool isTemporary() const noexcept { return hasStaticBlock() != 0u; }
#endif
};

//! \ref Zone with `N` bytes of a static storage, used for the initial block.
//!
//! Temporary zones are used in cases where it's known that some memory will be required, but in many cases it won't
//! exceed N bytes, so the whole operation can be performed without a dynamic memory allocation.
template<size_t N>
class ZoneTmp : public Zone {
public:
  ASMJIT_NONCOPYABLE(ZoneTmp)

  //! Temporary storage, embedded after \ref Zone.
  struct Storage {
    char data[N];
  } _storage;

  //! Creates a temporary zone. Dynamic block size is specified by `blockSize`.
  inline explicit ZoneTmp(size_t blockSize, size_t blockAlignment = 1) noexcept
    : Zone(blockSize, blockAlignment, Support::Temporary(_storage.data, N)) {}
};

//! Zone-based memory allocator that uses an existing `Zone` and provides a `release()` functionality on top of it.
//! It uses `Zone` only for chunks that can be pooled, and uses libc `malloc()` for chunks that are large.
//!
//! The advantage of ZoneAllocator is that it can allocate small chunks of memory really fast, and these chunks,
//! when released, will be reused by consecutive calls to `alloc()`. Also, since ZoneAllocator uses `Zone`, you can
//! turn any `Zone` into a `ZoneAllocator`, and use it in your `Pass` when necessary.
//!
//! ZoneAllocator is used by AsmJit containers to make containers having only few elements fast (and lightweight)
//! and to allow them to grow and use dynamic blocks when require more storage.
class ZoneAllocator {
public:
  ASMJIT_NONCOPYABLE(ZoneAllocator)

  //! \cond INTERNAL

  // In short, we pool chunks of these sizes:
  //   [32, 64, 96, 128, 192, 256, 320, 384, 448, 512]

  enum : uint32_t {
    //! How many bytes per a low granularity pool (has to be at least 16).
    kLoGranularity = 32,
    //! Number of slots of a low granularity pool.
    kLoCount = 4,
    //! Maximum size of a block that can be allocated in a low granularity pool.
    kLoMaxSize = kLoGranularity * kLoCount,

    //! How many bytes per a high granularity pool.
    kHiGranularity = 64,
    //! Number of slots of a high granularity pool.
    kHiCount = 6,
    //! Maximum size of a block that can be allocated in a high granularity pool.
    kHiMaxSize = kLoMaxSize + kHiGranularity * kHiCount,

    //! Alignment of every pointer returned by `alloc()`.
    kBlockAlignment = kLoGranularity
  };

  //! Single-linked list used to store unused chunks.
  struct Slot {
    //! Link to a next slot in a single-linked list.
    Slot* next;
  };

  //! A block of memory that has been allocated dynamically and is not part of block-list used by the allocator.
  //! This is used to keep track of all these blocks so they can be freed by `reset()` if not freed explicitly.
  struct DynamicBlock {
    DynamicBlock* prev;
    DynamicBlock* next;
  };

  //! \endcond

  //! \name Members
  //! \{

  //! Zone used to allocate memory that fits into slots.
  Zone* _zone {};
  //! Indexed slots containing released memory.
  Slot* _slots[kLoCount + kHiCount] {};
  //! Dynamic blocks for larger allocations (no slots).
  DynamicBlock* _dynamicBlocks {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  //! Creates a new `ZoneAllocator`.
  //!
  //! \note To use it, you must first `init()` it.
  ASMJIT_INLINE_NODEBUG ZoneAllocator() noexcept {}

  //! Creates a new `ZoneAllocator` initialized to use `zone`.
  ASMJIT_INLINE_NODEBUG explicit ZoneAllocator(Zone* zone) noexcept
    : _zone(zone) {}

  //! Destroys the `ZoneAllocator`.
  ASMJIT_INLINE_NODEBUG ~ZoneAllocator() noexcept { reset(); }

  //! Tests whether the `ZoneAllocator` is initialized (i.e. has `Zone`).
  ASMJIT_INLINE_NODEBUG bool isInitialized() const noexcept { return _zone != nullptr; }

  //! Convenience function to initialize the `ZoneAllocator` with `zone`.
  //!
  //! It's the same as calling `reset(zone)`.
  ASMJIT_INLINE_NODEBUG void init(Zone* zone) noexcept { reset(zone); }

  //! Resets this `ZoneAllocator` and also forget about the current `Zone` which is attached (if any). Reset
  //! optionally attaches a new `zone` passed, or keeps the `ZoneAllocator` in an uninitialized state, if
  //! `zone` is null.
  ASMJIT_API void reset(Zone* zone = nullptr) noexcept;

  //! \}

  //! \name Accessors
  //! \{

  //! Returns the assigned `Zone` of this allocator or null if this `ZoneAllocator` is not initialized.
  ASMJIT_INLINE_NODEBUG Zone* zone() const noexcept { return _zone; }

  //! \}

  //! \cond
  //! \name Internals
  //! \{

  //! Returns the slot index to be used for `size`. Returns `true` if a valid slot has been written to `slot` and
  //! `allocatedSize` has been filled with slot exact size (`allocatedSize` can be equal or slightly greater than
  //! `size`).
  static inline bool _getSlotIndex(size_t size, uint32_t& slot) noexcept {
    ASMJIT_ASSERT(size > 0);
    if (size > kHiMaxSize)
      return false;

    if (size <= kLoMaxSize)
      slot = uint32_t((size - 1) / kLoGranularity);
    else
      slot = uint32_t((size - kLoMaxSize - 1) / kHiGranularity) + kLoCount;

    return true;
  }

  //! \overload
  static inline bool _getSlotIndex(size_t size, uint32_t& slot, size_t& allocatedSize) noexcept {
    ASMJIT_ASSERT(size > 0);
    if (size > kHiMaxSize)
      return false;

    if (size <= kLoMaxSize) {
      slot = uint32_t((size - 1) / kLoGranularity);
      allocatedSize = Support::alignUp(size, kLoGranularity);
    }
    else {
      slot = uint32_t((size - kLoMaxSize - 1) / kHiGranularity) + kLoCount;
      allocatedSize = Support::alignUp(size, kHiGranularity);
    }

    return true;
  }

  //! \}
  //! \endcond

  //! \name Allocation
  //! \{

  //! \cond INTERNAL
  ASMJIT_API void* _alloc(size_t size, size_t& allocatedSize) noexcept;
  ASMJIT_API void* _allocZeroed(size_t size, size_t& allocatedSize) noexcept;
  ASMJIT_API void _releaseDynamic(void* p, size_t size) noexcept;
  //! \endcond

  //! Allocates `size` bytes of memory, ideally from an available pool.
  //!
  //! \note `size` can't be zero, it will assert in debug mode in such case.
  inline void* alloc(size_t size) noexcept {
    ASMJIT_ASSERT(isInitialized());
    size_t allocatedSize;
    return _alloc(size, allocatedSize);
  }

  //! Like `alloc(size)`, but provides a second argument `allocatedSize` that provides a way to know how big
  //! the block returned actually is. This is useful for containers to prevent growing too early.
  inline void* alloc(size_t size, size_t& allocatedSize) noexcept {
    ASMJIT_ASSERT(isInitialized());
    return _alloc(size, allocatedSize);
  }

  //! Like `alloc()`, but the return pointer is casted to `T*`.
  template<typename T>
  inline T* allocT(size_t size = sizeof(T)) noexcept {
    return static_cast<T*>(alloc(size));
  }

  //! Like `alloc(size)`, but returns zeroed memory.
  inline void* allocZeroed(size_t size) noexcept {
    ASMJIT_ASSERT(isInitialized());
    size_t allocatedSize;
    return _allocZeroed(size, allocatedSize);
  }

  //! Like `alloc(size, allocatedSize)`, but returns zeroed memory.
  inline void* allocZeroed(size_t size, size_t& allocatedSize) noexcept {
    ASMJIT_ASSERT(isInitialized());
    return _allocZeroed(size, allocatedSize);
  }

  //! Like `allocZeroed()`, but the return pointer is casted to `T*`.
  template<typename T>
  inline T* allocZeroedT(size_t size = sizeof(T)) noexcept {
    return static_cast<T*>(allocZeroed(size));
  }

  //! Like `new(std::nothrow) T(...)`, but allocated by `Zone`.
  template<typename T>
  inline T* newT() noexcept {
    void* p = allocT<T>();
    if (ASMJIT_UNLIKELY(!p))
      return nullptr;
    return new(Support::PlacementNew{p}) T();
  }
  //! Like `new(std::nothrow) T(...)`, but allocated by `Zone`.
  template<typename T, typename... Args>
  inline T* newT(Args&&... args) noexcept {
    void* p = allocT<T>();
    if (ASMJIT_UNLIKELY(!p))
      return nullptr;
    return new(Support::PlacementNew{p}) T(std::forward<Args>(args)...);
  }

  //! Releases the memory previously allocated by `alloc()`. The `size` argument has to be the same as used to call
  //! `alloc()` or `allocatedSize` returned  by `alloc()`.
  inline void release(void* p, size_t size) noexcept {
    ASMJIT_ASSERT(isInitialized());
    ASMJIT_ASSERT(p != nullptr);
    ASMJIT_ASSERT(size != 0);

    uint32_t slot;
    if (_getSlotIndex(size, slot)) {
      static_cast<Slot*>(p)->next = static_cast<Slot*>(_slots[slot]);
      _slots[slot] = static_cast<Slot*>(p);
    }
    else {
      _releaseDynamic(p, size);
    }
  }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ZONE_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/zonehash.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/support.h"
#include "../core/zone.h"
#include "../core/zonehash.h"

ASMJIT_BEGIN_NAMESPACE

// ZoneHashBase - Prime Numbers
// ============================

#define ASMJIT_POPULATE_PRIMES(ENTRY) \
  ENTRY(2         , 0x80000000, 32), /* [N * 0x80000000 >> 32] (rcp=2147483648) */ \
  ENTRY(11        , 0xBA2E8BA3, 35), /* [N * 0xBA2E8BA3 >> 35] (rcp=3123612579) */ \
  ENTRY(29        , 0x8D3DCB09, 36), /* [N * 0x8D3DCB09 >> 36] (rcp=2369637129) */ \
  ENTRY(41        , 0xC7CE0C7D, 37), /* [N * 0xC7CE0C7D >> 37] (rcp=3352169597) */ \
  ENTRY(59        , 0x8AD8F2FC, 37), /* [N * 0x8AD8F2FC >> 37] (rcp=2329473788) */ \
  ENTRY(83        , 0xC565C87C, 38), /* [N * 0xC565C87C >> 38] (rcp=3311782012) */ \
  ENTRY(131       , 0xFA232CF3, 39), /* [N * 0xFA232CF3 >> 39] (rcp=4196609267) */ \
  ENTRY(191       , 0xAB8F69E3, 39), /* [N * 0xAB8F69E3 >> 39] (rcp=2878302691) */ \
  ENTRY(269       , 0xF3A0D52D, 40), /* [N * 0xF3A0D52D >> 40] (rcp=4087403821) */ \
  ENTRY(383       , 0xAB1CBDD4, 40), /* [N * 0xAB1CBDD4 >> 40] (rcp=2870787540) */ \
  ENTRY(541       , 0xF246FACC, 41), /* [N * 0xF246FACC >> 41] (rcp=4064737996) */ \
  ENTRY(757       , 0xAD2589A4, 41), /* [N * 0xAD2589A4 >> 41] (rcp=2904918436) */ \
  ENTRY(1061      , 0xF7129426, 42), /* [N * 0xF7129426 >> 42] (rcp=4145189926) */ \
  ENTRY(1499      , 0xAEE116B7, 42), /* [N * 0xAEE116B7 >> 42] (rcp=2933986999) */ \
  ENTRY(2099      , 0xF9C7A737, 43), /* [N * 0xF9C7A737 >> 43] (rcp=4190611255) */ \
  ENTRY(2939      , 0xB263D25C, 43), /* [N * 0xB263D25C >> 43] (rcp=2992886364) */ \
  ENTRY(4111      , 0xFF10E02E, 44), /* [N * 0xFF10E02E >> 44] (rcp=4279296046) */ \
  ENTRY(5779      , 0xB5722823, 44), /* [N * 0xB5722823 >> 44] (rcp=3044157475) */ \
  ENTRY(8087      , 0x81A97405, 44), /* [N * 0x81A97405 >> 44] (rcp=2175366149) */ \
  ENTRY(11321     , 0xB93E91DB, 45), /* [N * 0xB93E91DB >> 45] (rcp=3107885531) */ \
  ENTRY(15859     , 0x843CC26B, 45), /* [N * 0x843CC26B >> 45] (rcp=2218574443) */ \
  ENTRY(22189     , 0xBD06B9EA, 46), /* [N * 0xBD06B9EA >> 46] (rcp=3171334634) */ \
  ENTRY(31051     , 0x8713F186, 46), /* [N * 0x8713F186 >> 46] (rcp=2266231174) */ \
  ENTRY(43451     , 0xC10F1CB9, 47), /* [N * 0xC10F1CB9 >> 47] (rcp=3238993081) */ \
  ENTRY(60869     , 0x89D06A86, 47), /* [N * 0x89D06A86 >> 47] (rcp=2312137350) */ \
  ENTRY(85159     , 0xC502AF3B, 48), /* [N * 0xC502AF3B >> 48] (rcp=3305287483) */ \
  ENTRY(102107    , 0xA44F65AE, 48), /* [N * 0xA44F65AE >> 48] (rcp=2756666798) */ \
  ENTRY(122449    , 0x89038F77, 48), /* [N * 0x89038F77 >> 48] (rcp=2298711927) */ \
  ENTRY(146819    , 0xE48AF7E9, 49), /* [N * 0xE48AF7E9 >> 49] (rcp=3834312681) */ \
  ENTRY(176041    , 0xBE9B145B, 49), /* [N * 0xBE9B145B >> 49] (rcp=3197834331) */ \
  ENTRY(211073    , 0x9EF882BA, 49), /* [N * 0x9EF882BA >> 49] (rcp=2667086522) */ \
  ENTRY(253081    , 0x849571AB, 49), /* [N * 0x849571AB >> 49] (rcp=2224386475) */ \
  ENTRY(303469    , 0xDD239C97, 50), /* [N * 0xDD239C97 >> 50] (rcp=3710098583) */ \
  ENTRY(363887    , 0xB86C196D, 50), /* [N * 0xB86C196D >> 50] (rcp=3094092141) */ \
  ENTRY(436307    , 0x99CFA4E9, 50), /* [N * 0x99CFA4E9 >> 50] (rcp=2580522217) */ \
  ENTRY(523177    , 0x804595C0, 50), /* [N * 0x804595C0 >> 50] (rcp=2152043968) */ \
  ENTRY(627293    , 0xD5F69FCF, 51), /* [N * 0xD5F69FCF >> 51] (rcp=3589709775) */ \
  ENTRY(752177    , 0xB27063BA, 51), /* [N * 0xB27063BA >> 51] (rcp=2993710010) */ \
  ENTRY(901891    , 0x94D170AC, 51), /* [N * 0x94D170AC >> 51] (rcp=2496753836) */ \
  ENTRY(1081369   , 0xF83C9767, 52), /* [N * 0xF83C9767 >> 52] (rcp=4164720487) */ \
  ENTRY(1296563   , 0xCF09435D, 52), /* [N * 0xCF09435D >> 52] (rcp=3473490781) */ \
  ENTRY(1554583   , 0xACAC7198, 52), /* [N * 0xACAC7198 >> 52] (rcp=2896982424) */ \
  ENTRY(1863971   , 0x90033EE3, 52), /* [N * 0x90033EE3 >> 52] (rcp=2416131811) */ \
  ENTRY(2234923   , 0xF0380EBD, 53), /* [N * 0xF0380EBD >> 53] (rcp=4030205629) */ \
  ENTRY(2679673   , 0xC859731E, 53), /* [N * 0xC859731E >> 53] (rcp=3361305374) */ \
  ENTRY(3212927   , 0xA718DE27, 53), /* [N * 0xA718DE27 >> 53] (rcp=2803424807) */ \
  ENTRY(3852301   , 0x8B5D1B4B, 53), /* [N * 0x8B5D1B4B >> 53] (rcp=2338134859) */ \
  ENTRY(4618921   , 0xE8774804, 54), /* [N * 0xE8774804 >> 54] (rcp=3900131332) */ \
  ENTRY(5076199   , 0xD386574E, 54), /* [N * 0xD386574E >> 54] (rcp=3548796750) */ \
  ENTRY(5578757   , 0xC0783FE1, 54), /* [N * 0xC0783FE1 >> 54] (rcp=3229106145) */ \
  ENTRY(6131057   , 0xAF21B08F, 54), /* [N * 0xAF21B08F >> 54] (rcp=2938220687) */ \
  ENTRY(6738031   , 0x9F5AFD6E, 54), /* [N * 0x9F5AFD6E >> 54] (rcp=2673540462) */ \
  ENTRY(7405163   , 0x90FFC3B9, 54), /* [N * 0x90FFC3B9 >> 54] (rcp=2432680889) */ \
  ENTRY(8138279   , 0x83EFECFC, 54), /* [N * 0x83EFECFC >> 54] (rcp=2213539068) */ \
  ENTRY(8943971   , 0xF01AA2EF, 55), /* [N * 0xF01AA2EF >> 55] (rcp=4028277487) */ \
  ENTRY(9829447   , 0xDA7979B2, 55), /* [N * 0xDA7979B2 >> 55] (rcp=3665394098) */ \
  ENTRY(10802581  , 0xC6CB2771, 55), /* [N * 0xC6CB2771 >> 55] (rcp=3335202673) */ \
  ENTRY(11872037  , 0xB4E2C7DD, 55), /* [N * 0xB4E2C7DD >> 55] (rcp=3034761181) */ \
  ENTRY(13047407  , 0xA4974124, 55), /* [N * 0xA4974124 >> 55] (rcp=2761376036) */ \
  ENTRY(14339107  , 0x95C39CF1, 55), /* [N * 0x95C39CF1 >> 55] (rcp=2512624881) */ \
  ENTRY(15758737  , 0x8845C763, 55), /* [N * 0x8845C763 >> 55] (rcp=2286274403) */ \
  ENTRY(17318867  , 0xF7FE593F, 56), /* [N * 0xF7FE593F >> 56] (rcp=4160641343) */ \
  ENTRY(19033439  , 0xE1A75D93, 56), /* [N * 0xE1A75D93 >> 56] (rcp=3785842067) */ \
  ENTRY(20917763  , 0xCD5389B3, 56), /* [N * 0xCD5389B3 >> 56] (rcp=3444804019) */ \
  ENTRY(22988621  , 0xBAD4841A, 56), /* [N * 0xBAD4841A >> 56] (rcp=3134489626) */ \
  ENTRY(25264543  , 0xA9FFF2FF, 56), /* [N * 0xA9FFF2FF >> 56] (rcp=2852123391) */ \
  ENTRY(27765763  , 0x9AAF8BF3, 56), /* [N * 0x9AAF8BF3 >> 56] (rcp=2595195891) */ \
  ENTRY(30514607  , 0x8CC04E18, 56), /* [N * 0x8CC04E18 >> 56] (rcp=2361413144) */ \
  ENTRY(33535561  , 0x80127068, 56), /* [N * 0x80127068 >> 56] (rcp=2148692072) */ \
  ENTRY(36855587  , 0xE911F0BB, 57), /* [N * 0xE911F0BB >> 57] (rcp=3910267067) */ \
  ENTRY(38661533  , 0xDE2ED7BE, 57), /* [N * 0xDE2ED7BE >> 57] (rcp=3727611838) */ \
  ENTRY(40555961  , 0xD3CDF2FD, 57), /* [N * 0xD3CDF2FD >> 57] (rcp=3553489661) */ \
  ENTRY(42543269  , 0xC9E9196C, 57), /* [N * 0xC9E9196C >> 57] (rcp=3387496812) */ \
  ENTRY(44627909  , 0xC07A9EB6, 57), /* [N * 0xC07A9EB6 >> 57] (rcp=3229261494) */ \
  ENTRY(46814687  , 0xB77CEF65, 57), /* [N * 0xB77CEF65 >> 57] (rcp=3078418277) */ \
  ENTRY(49108607  , 0xAEEAC65C, 57), /* [N * 0xAEEAC65C >> 57] (rcp=2934621788) */ \
  ENTRY(51514987  , 0xA6BF0EF0, 57), /* [N * 0xA6BF0EF0 >> 57] (rcp=2797539056) */ \
  ENTRY(54039263  , 0x9EF510B5, 57), /* [N * 0x9EF510B5 >> 57] (rcp=2666860725) */ \
  ENTRY(56687207  , 0x97883B42, 57), /* [N * 0x97883B42 >> 57] (rcp=2542287682) */ \
  ENTRY(59464897  , 0x907430ED, 57), /* [N * 0x907430ED >> 57] (rcp=2423533805) */ \
  ENTRY(62378699  , 0x89B4CA91, 57), /* [N * 0x89B4CA91 >> 57] (rcp=2310326929) */ \
  ENTRY(65435273  , 0x83461568, 57), /* [N * 0x83461568 >> 57] (rcp=2202408296) */ \
  ENTRY(68641607  , 0xFA489AA8, 58), /* [N * 0xFA489AA8 >> 58] (rcp=4199062184) */ \
  ENTRY(72005051  , 0xEE97B1C5, 58), /* [N * 0xEE97B1C5 >> 58] (rcp=4002918853) */ \
  ENTRY(75533323  , 0xE3729293, 58), /* [N * 0xE3729293 >> 58] (rcp=3815936659) */ \
  ENTRY(79234469  , 0xD8D2BBA3, 58), /* [N * 0xD8D2BBA3 >> 58] (rcp=3637689251) */ \
  ENTRY(83116967  , 0xCEB1F196, 58), /* [N * 0xCEB1F196 >> 58] (rcp=3467768214) */ \
  ENTRY(87189709  , 0xC50A4426, 58), /* [N * 0xC50A4426 >> 58] (rcp=3305784358) */ \
  ENTRY(91462061  , 0xBBD6052B, 58), /* [N * 0xBBD6052B >> 58] (rcp=3151365419) */ \
  ENTRY(95943737  , 0xB30FD999, 58), /* [N * 0xB30FD999 >> 58] (rcp=3004160409) */ \
  ENTRY(100644991 , 0xAAB29CED, 58), /* [N * 0xAAB29CED >> 58] (rcp=2863832301) */ \
  ENTRY(105576619 , 0xA2B96421, 58), /* [N * 0xA2B96421 >> 58] (rcp=2730058785) */ \
  ENTRY(110749901 , 0x9B1F8434, 58), /* [N * 0x9B1F8434 >> 58] (rcp=2602533940) */ \
  ENTRY(116176651 , 0x93E08B4A, 58), /* [N * 0x93E08B4A >> 58] (rcp=2480966474) */ \
  ENTRY(121869317 , 0x8CF837E0, 58), /* [N * 0x8CF837E0 >> 58] (rcp=2365077472) */ \
  ENTRY(127840913 , 0x86627F01, 58), /* [N * 0x86627F01 >> 58] (rcp=2254601985) */ \
  ENTRY(134105159 , 0x801B8178, 58), /* [N * 0x801B8178 >> 58] (rcp=2149286264) */ \
  ENTRY(140676353 , 0xF43F294F, 59), /* [N * 0xF43F294F >> 59] (rcp=4097780047) */ \
  ENTRY(147569509 , 0xE8D67089, 59), /* [N * 0xE8D67089 >> 59] (rcp=3906367625) */ \
  ENTRY(154800449 , 0xDDF6243C, 59), /* [N * 0xDDF6243C >> 59] (rcp=3723895868) */ \
  ENTRY(162385709 , 0xD397E6AE, 59), /* [N * 0xD397E6AE >> 59] (rcp=3549947566) */ \
  ENTRY(170342629 , 0xC9B5A65A, 59), /* [N * 0xC9B5A65A >> 59] (rcp=3384125018) */ \
  ENTRY(178689419 , 0xC0499865, 59), /* [N * 0xC0499865 >> 59] (rcp=3226048613) */ \
  ENTRY(187445201 , 0xB74E35FA, 59), /* [N * 0xB74E35FA >> 59] (rcp=3075356154) */ \
  ENTRY(196630033 , 0xAEBE3AC1, 59), /* [N * 0xAEBE3AC1 >> 59] (rcp=2931702465) */ \
  ENTRY(206264921 , 0xA694A37F, 59), /* [N * 0xA694A37F >> 59] (rcp=2794759039) */ \
  ENTRY(216371963 , 0x9ECCA59F, 59), /* [N * 0x9ECCA59F >> 59] (rcp=2664211871) */ \
  ENTRY(226974197 , 0x9761B6AE, 59), /* [N * 0x9761B6AE >> 59] (rcp=2539763374) */ \
  ENTRY(238095983 , 0x904F79A1, 59), /* [N * 0x904F79A1 >> 59] (rcp=2421127585) */ \
  ENTRY(249762697 , 0x8991CD1F, 59), /* [N * 0x8991CD1F >> 59] (rcp=2308033823) */ \
  ENTRY(262001071 , 0x8324BCA5, 59), /* [N * 0x8324BCA5 >> 59] (rcp=2200222885) */ \
  ENTRY(274839137 , 0xFA090732, 60), /* [N * 0xFA090732 >> 60] (rcp=4194895666) */ \
  ENTRY(288306269 , 0xEE5B16ED, 60), /* [N * 0xEE5B16ED >> 60] (rcp=3998947053) */ \
  ENTRY(302433337 , 0xE338CE49, 60), /* [N * 0xE338CE49 >> 60] (rcp=3812150857) */ \
  ENTRY(317252587 , 0xD89BABC0, 60), /* [N * 0xD89BABC0 >> 60] (rcp=3634080704) */ \
  ENTRY(374358107 , 0xB790EF43, 60), /* [N * 0xB790EF43 >> 60] (rcp=3079728963) */ \
  ENTRY(441742621 , 0x9B908414, 60), /* [N * 0x9B908414 >> 60] (rcp=2609939476) */ \
  ENTRY(521256293 , 0x83D596FA, 60), /* [N * 0x83D596FA >> 60] (rcp=2211813114) */ \
  ENTRY(615082441 , 0xDF72B16E, 61), /* [N * 0xDF72B16E >> 61] (rcp=3748835694) */ \
  ENTRY(725797313 , 0xBD5CDB3B, 61), /* [N * 0xBD5CDB3B >> 61] (rcp=3176979259) */ \
  ENTRY(856440829 , 0xA07A14E9, 61), /* [N * 0xA07A14E9 >> 61] (rcp=2692355305) */ \
  ENTRY(1010600209, 0x87FF5289, 61), /* [N * 0x87FF5289 >> 61] (rcp=2281656969) */ \
  ENTRY(1192508257, 0xE6810540, 62), /* [N * 0xE6810540 >> 62] (rcp=3867215168) */ \
  ENTRY(1407159797, 0xC357A480, 62), /* [N * 0xC357A480 >> 62] (rcp=3277300864) */ \
  ENTRY(1660448617, 0xA58B5B4F, 62), /* [N * 0xA58B5B4F >> 62] (rcp=2777373519) */ \
  ENTRY(1959329399, 0x8C4AB55F, 62), /* [N * 0x8C4AB55F >> 62] (rcp=2353706335) */ \
  ENTRY(2312008693, 0xEDC86320, 63), /* [N * 0xEDC86320 >> 63] (rcp=3989332768) */ \
  ENTRY(2728170257, 0xC982C4D2, 63), /* [N * 0xC982C4D2 >> 63] (rcp=3380790482) */ \
  ENTRY(3219240923, 0xAAC599B6, 63)  /* [N * 0xAAC599B6 >> 63] (rcp=2865076662) */


struct HashPrime {
  //! Prime number
  uint32_t prime;
  //! Reciprocal to turn division into multiplication.
  uint32_t rcp;
};

static const HashPrime ZoneHash_primeArray[] = {
  #define E(PRIME, RCP, SHIFT) { PRIME, RCP }
  ASMJIT_POPULATE_PRIMES(E)
  #undef E
};

static const uint8_t ZoneHash_primeShift[] = {
  #define E(PRIME, RCP, SHIFT) uint8_t(SHIFT)
  ASMJIT_POPULATE_PRIMES(E)
  #undef E
};

// ZoneHashBase - Rehash
// =====================

void ZoneHashBase::_rehash(ZoneAllocator* allocator, uint32_t primeIndex) noexcept {
  ASMJIT_ASSERT(primeIndex < ASMJIT_ARRAY_SIZE(ZoneHash_primeArray));
  uint32_t newCount = ZoneHash_primeArray[primeIndex].prime;

  ZoneHashNode** oldData = _data;
  ZoneHashNode** newData = reinterpret_cast<ZoneHashNode**>(
    allocator->allocZeroed(size_t(newCount) * sizeof(ZoneHashNode*)));

  // We can still store nodes into the table, but it will degrade.
  if (ASMJIT_UNLIKELY(newData == nullptr))
    return;

  uint32_t i;
  uint32_t oldCount = _bucketsCount;

  _data = newData;
  _bucketsCount = newCount;
  _bucketsGrow = uint32_t(newCount * 0.9);
  _rcpValue = ZoneHash_primeArray[primeIndex].rcp;
  _rcpShift = ZoneHash_primeShift[primeIndex];
  _primeIndex = uint8_t(primeIndex);

  for (i = 0; i < oldCount; i++) {
    ZoneHashNode* node = oldData[i];
    while (node) {
      ZoneHashNode* next = node->_hashNext;
      uint32_t hashMod = _calcMod(node->_hashCode);

      node->_hashNext = newData[hashMod];
      newData[hashMod] = node;
      node = next;
    }
  }

  if (oldData != _embedded)
    allocator->release(oldData, oldCount * sizeof(ZoneHashNode*));
}

// ZoneHashBase - Operations
// =========================

ZoneHashNode* ZoneHashBase::_insert(ZoneAllocator* allocator, ZoneHashNode* node) noexcept {
  uint32_t hashMod = _calcMod(node->_hashCode);
  ZoneHashNode* next = _data[hashMod];

  node->_hashNext = next;
  _data[hashMod] = node;

  if (++_size > _bucketsGrow) {
    uint32_t primeIndex = Support::min<uint32_t>(_primeIndex + 2, ASMJIT_ARRAY_SIZE(ZoneHash_primeArray) - 1);
    if (primeIndex > _primeIndex)
      _rehash(allocator, primeIndex);
  }

  return node;
}

ZoneHashNode* ZoneHashBase::_remove(ZoneAllocator* allocator, ZoneHashNode* node) noexcept {
  DebugUtils::unused(allocator);
  uint32_t hashMod = _calcMod(node->_hashCode);

  ZoneHashNode** pPrev = &_data[hashMod];
  ZoneHashNode* p = *pPrev;

  while (p) {
    if (p == node) {
      *pPrev = p->_hashNext;
      _size--;
      return node;
    }

    pPrev = &p->_hashNext;
    p = *pPrev;
  }

  return nullptr;
}

// ZoneHashBase - Tests
// ====================

#if defined(ASMJIT_TEST)
struct MyHashNode : public ZoneHashNode {
  inline MyHashNode(uint32_t key) noexcept
    : ZoneHashNode(key),
      _key(key) {}

  uint32_t _key;
};

struct MyKeyMatcher {
  inline MyKeyMatcher(uint32_t key) noexcept
    : _key(key) {}

  inline uint32_t hashCode() const noexcept { return _key; }
  inline bool matches(const MyHashNode* node) const noexcept { return node->_key == _key; }

  uint32_t _key;
};

UNIT(zone_hash) {
  uint32_t kCount = BrokenAPI::hasArg("--quick") ? 1000 : 10000;

  Zone zone(4096);
  ZoneAllocator allocator(&zone);

  ZoneHash<MyHashNode> hashTable;

  uint32_t key;
  INFO("Inserting %u elements to HashTable", unsigned(kCount));
  for (key = 0; key < kCount; key++) {
    hashTable.insert(&allocator, zone.newT<MyHashNode>(key));
  }

  uint32_t count = kCount;
  INFO("Removing %u elements from HashTable and validating each operation", unsigned(kCount));
  do {
    MyHashNode* node;

    for (key = 0; key < count; key++) {
      node = hashTable.get(MyKeyMatcher(key));
      EXPECT_NOT_NULL(node);
      EXPECT_EQ(node->_key, key);
    }

    {
      count--;
      node = hashTable.get(MyKeyMatcher(count));
      hashTable.remove(&allocator, node);

      node = hashTable.get(MyKeyMatcher(count));
      EXPECT_NULL(node);
    }
  } while (count);

  EXPECT_TRUE(hashTable.empty());
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/zonehash.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ZONEHASH_H_INCLUDED
#define ASMJIT_CORE_ZONEHASH_H_INCLUDED

#include "../core/zone.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_zone
//! \{

//! Node used by \ref ZoneHash template.
//!
//! You must provide function `bool eq(const Key& key)` in order to make `ZoneHash::get()` working.
class ZoneHashNode {
public:
  ASMJIT_NONCOPYABLE(ZoneHashNode)

  inline ZoneHashNode(uint32_t hashCode = 0) noexcept
    : _hashNext(nullptr),
      _hashCode(hashCode),
      _customData(0) {}

  //! Next node in the chain, null if it terminates the chain.
  ZoneHashNode* _hashNext;
  //! Precalculated hash-code of key.
  uint32_t _hashCode;
  //! Padding, can be reused by any Node that inherits `ZoneHashNode`.
  uint32_t _customData;
};

//! Base class used by \ref ZoneHash template
class ZoneHashBase {
public:
  ASMJIT_NONCOPYABLE(ZoneHashBase)

  //! Buckets data.
  ZoneHashNode** _data;
  //! Count of records inserted into the hash table.
  size_t _size;
  //! Count of hash buckets.
  uint32_t _bucketsCount;
  //! When buckets array should grow (only checked after insertion).
  uint32_t _bucketsGrow;
  //! Reciprocal value of `_bucketsCount`.
  uint32_t _rcpValue;
  //! How many bits to shift right when hash is multiplied with `_rcpValue`.
  uint8_t _rcpShift;
  //! Prime value index in internal prime array.
  uint8_t _primeIndex;

  //! Embedded data, used by empty hash tables.
  ZoneHashNode* _embedded[1];

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneHashBase() noexcept {
    reset();
  }

  inline ZoneHashBase(ZoneHashBase&& other) noexcept {
    _data = other._data;
    _size = other._size;
    _bucketsCount = other._bucketsCount;
    _bucketsGrow = other._bucketsGrow;
    _rcpValue = other._rcpValue;
    _rcpShift = other._rcpShift;
    _primeIndex = other._primeIndex;
    _embedded[0] = other._embedded[0];

    if (_data == other._embedded) _data = _embedded;
  }

  inline void reset() noexcept {
    _data = _embedded;
    _size = 0;
    _bucketsCount = 1;
    _bucketsGrow = 1;
    _rcpValue = 1;
    _rcpShift = 0;
    _primeIndex = 0;
    _embedded[0] = nullptr;
  }

  inline void release(ZoneAllocator* allocator) noexcept {
    ZoneHashNode** oldData = _data;
    if (oldData != _embedded)
      allocator->release(oldData, _bucketsCount * sizeof(ZoneHashNode*));
    reset();
  }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _size == 0; }
  ASMJIT_INLINE_NODEBUG size_t size() const noexcept { return _size; }

  //! \}

  //! \name Utilities
  //! \{

  inline void _swap(ZoneHashBase& other) noexcept {
    std::swap(_data, other._data);
    std::swap(_size, other._size);
    std::swap(_bucketsCount, other._bucketsCount);
    std::swap(_bucketsGrow, other._bucketsGrow);
    std::swap(_rcpValue, other._rcpValue);
    std::swap(_rcpShift, other._rcpShift);
    std::swap(_primeIndex, other._primeIndex);
    std::swap(_embedded[0], other._embedded[0]);

    if (_data == other._embedded) _data = _embedded;
    if (other._data == _embedded) other._data = other._embedded;
  }

  //! \cond INTERNAL
  inline uint32_t _calcMod(uint32_t hash) const noexcept {
    uint32_t x = uint32_t((uint64_t(hash) * _rcpValue) >> _rcpShift);
    return hash - x * _bucketsCount;
  }

  ASMJIT_API void _rehash(ZoneAllocator* allocator, uint32_t newCount) noexcept;
  ASMJIT_API ZoneHashNode* _insert(ZoneAllocator* allocator, ZoneHashNode* node) noexcept;
  ASMJIT_API ZoneHashNode* _remove(ZoneAllocator* allocator, ZoneHashNode* node) noexcept;
  //! \endcond

  //! \}
};

//! Low-level hash table specialized for storing string keys and POD values.
//!
//! This hash table allows duplicates to be inserted (the API is so low level that it's up to you if you allow it or
//! not, as you should first `get()` the node and then modify it or insert a new node by using `insert()`, depending
//! on the intention).
template<typename NodeT>
class ZoneHash : public ZoneHashBase {
public:
  ASMJIT_NONCOPYABLE(ZoneHash)

  typedef NodeT Node;

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneHash() noexcept
    : ZoneHashBase() {}

  ASMJIT_INLINE_NODEBUG ZoneHash(ZoneHash&& other) noexcept
    : ZoneHash(other) {}

  //! \}

  //! \name Utilities
  //! \{

  ASMJIT_INLINE_NODEBUG void swap(ZoneHash& other) noexcept { ZoneHashBase::_swap(other); }

  template<typename KeyT>
  inline NodeT* get(const KeyT& key) const noexcept {
    uint32_t hashMod = _calcMod(key.hashCode());
    NodeT* node = static_cast<NodeT*>(_data[hashMod]);

    while (node && !key.matches(node))
      node = static_cast<NodeT*>(node->_hashNext);
    return node;
  }

  ASMJIT_INLINE_NODEBUG NodeT* insert(ZoneAllocator* allocator, NodeT* node) noexcept { return static_cast<NodeT*>(_insert(allocator, node)); }
  ASMJIT_INLINE_NODEBUG NodeT* remove(ZoneAllocator* allocator, NodeT* node) noexcept { return static_cast<NodeT*>(_remove(allocator, node)); }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ZONEHASH_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/zonelist.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/zone.h"
#include "../core/zonelist.h"

ASMJIT_BEGIN_NAMESPACE

// ZoneList - Tests
// ================

#if defined(ASMJIT_TEST)
class MyListNode : public ZoneListNode<MyListNode> {};

UNIT(zone_list) {
  Zone zone(4096);
  ZoneList<MyListNode> list;

  MyListNode* a = zone.newT<MyListNode>();
  MyListNode* b = zone.newT<MyListNode>();
  MyListNode* c = zone.newT<MyListNode>();
  MyListNode* d = zone.newT<MyListNode>();

  INFO("Append / Unlink");

  // []
  EXPECT_TRUE(list.empty());

  // [A]
  list.append(a);
  EXPECT_FALSE(list.empty());
  EXPECT_EQ(list.first(), a);
  EXPECT_EQ(list.last(), a);
  EXPECT_NULL(a->prev());
  EXPECT_NULL(a->next());

  // [A, B]
  list.append(b);
  EXPECT_EQ(list.first(), a);
  EXPECT_EQ(list.last(), b);
  EXPECT_NULL(a->prev());
  EXPECT_EQ(a->next(), b);
  EXPECT_EQ(b->prev(), a);
  EXPECT_NULL(b->next());

  // [A, B, C]
  list.append(c);
  EXPECT_EQ(list.first(), a);
  EXPECT_EQ(list.last(), c);
  EXPECT_NULL(a->prev());
  EXPECT_EQ(a->next(), b);
  EXPECT_EQ(b->prev(), a);
  EXPECT_EQ(b->next(), c);
  EXPECT_EQ(c->prev(), b);
  EXPECT_NULL(c->next());

  // [B, C]
  list.unlink(a);
  EXPECT_EQ(list.first(), b);
  EXPECT_EQ(list.last(), c);
  EXPECT_NULL(a->prev());
  EXPECT_NULL(a->next());
  EXPECT_NULL(b->prev());
  EXPECT_EQ(b->next(), c);
  EXPECT_EQ(c->prev(), b);
  EXPECT_NULL(c->next());

  // [B]
  list.unlink(c);
  EXPECT_EQ(list.first(), b);
  EXPECT_EQ(list.last(), b);
  EXPECT_NULL(b->prev());
  EXPECT_NULL(b->next());
  EXPECT_NULL(c->prev());
  EXPECT_NULL(c->next());

  // []
  list.unlink(b);
  EXPECT_TRUE(list.empty());
  EXPECT_NULL(list.first());
  EXPECT_NULL(list.last());
  EXPECT_NULL(b->prev());
  EXPECT_NULL(b->next());

  INFO("Prepend / Unlink");

  // [A]
  list.prepend(a);
  EXPECT_FALSE(list.empty());
  EXPECT_EQ(list.first(), a);
  EXPECT_EQ(list.last(), a);
  EXPECT_NULL(a->prev());
  EXPECT_NULL(a->next());

  // [B, A]
  list.prepend(b);
  EXPECT_EQ(list.first(), b);
  EXPECT_EQ(list.last(), a);
  EXPECT_NULL(b->prev());
  EXPECT_EQ(b->next(), a);
  EXPECT_EQ(a->prev(), b);
  EXPECT_NULL(a->next());

  INFO("InsertAfter / InsertBefore");

  // [B, A, C]
  list.insertAfter(a, c);
  EXPECT_EQ(list.first(), b);
  EXPECT_EQ(list.last(), c);
  EXPECT_NULL(b->prev());
  EXPECT_EQ(b->next(), a);
  EXPECT_EQ(a->prev(), b);
  EXPECT_EQ(a->next(), c);
  EXPECT_EQ(c->prev(), a);
  EXPECT_NULL(c->next());

  // [B, D, A, C]
  list.insertBefore(a, d);
  EXPECT_EQ(list.first(), b);
  EXPECT_EQ(list.last(), c);
  EXPECT_NULL(b->prev());
  EXPECT_EQ(b->next(), d);
  EXPECT_EQ(d->prev(), b);
  EXPECT_EQ(d->next(), a);
  EXPECT_EQ(a->prev(), d);
  EXPECT_EQ(a->next(), c);
  EXPECT_EQ(c->prev(), a);
  EXPECT_NULL(c->next());

  INFO("PopFirst / Pop");

  // [D, A, C]
  EXPECT_EQ(list.popFirst(), b);
  EXPECT_NULL(b->prev());
  EXPECT_NULL(b->next());

  EXPECT_EQ(list.first(), d);
  EXPECT_EQ(list.last(), c);
  EXPECT_NULL(d->prev());
  EXPECT_EQ(d->next(), a);
  EXPECT_EQ(a->prev(), d);
  EXPECT_EQ(a->next(), c);
  EXPECT_EQ(c->prev(), a);
  EXPECT_NULL(c->next());

  // [D, A]
  EXPECT_EQ(list.pop(), c);
  EXPECT_NULL(c->prev());
  EXPECT_NULL(c->next());

  EXPECT_EQ(list.first(), d);
  EXPECT_EQ(list.last(), a);
  EXPECT_NULL(d->prev());
  EXPECT_EQ(d->next(), a);
  EXPECT_EQ(a->prev(), d);
  EXPECT_NULL(a->next());
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/zonelist.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ZONELIST_H_INCLUDED
#define ASMJIT_CORE_ZONELIST_H_INCLUDED

#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_zone
//! \{

//! Node used by \ref ZoneList template.
template<typename NodeT>
class ZoneListNode {
public:
  ASMJIT_NONCOPYABLE(ZoneListNode)

  //! \name Constants
  //! \{

  enum : size_t {
    kNodeIndexPrev = 0,
    kNodeIndexNext = 1
  };

  //! \}

  //! \name Members
  //! \{

  NodeT* _listNodes[2];

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneListNode() noexcept
    : _listNodes { nullptr, nullptr } {}

  ASMJIT_INLINE_NODEBUG ZoneListNode(ZoneListNode&& other) noexcept
    : _listNodes { other._listNodes[0], other._listNodes[1] } {}

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool hasPrev() const noexcept { return _listNodes[kNodeIndexPrev] != nullptr; }
  ASMJIT_INLINE_NODEBUG bool hasNext() const noexcept { return _listNodes[kNodeIndexNext] != nullptr; }

  ASMJIT_INLINE_NODEBUG NodeT* prev() const noexcept { return _listNodes[kNodeIndexPrev]; }
  ASMJIT_INLINE_NODEBUG NodeT* next() const noexcept { return _listNodes[kNodeIndexNext]; }

  //! \}
};

//! Zone allocated list container that uses nodes of `NodeT` type.
template <typename NodeT>
class ZoneList {
public:
  ASMJIT_NONCOPYABLE(ZoneList)

  //! \name Constants
  //! \{

  enum : size_t {
    kNodeIndexFirst = 0,
    kNodeIndexLast = 1
  };

  //! \}

  //! \name Members
  //! \{

  NodeT* _nodes[2] {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneList() noexcept {}

  ASMJIT_INLINE_NODEBUG ZoneList(ZoneList&& other) noexcept
    : _nodes { other._nodes[0], other._nodes[1] } {}

  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _nodes[0] = nullptr;
    _nodes[1] = nullptr;
  }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _nodes[0] == nullptr; }
  ASMJIT_INLINE_NODEBUG NodeT* first() const noexcept { return _nodes[kNodeIndexFirst]; }
  ASMJIT_INLINE_NODEBUG NodeT* last() const noexcept { return _nodes[kNodeIndexLast]; }

  //! \}

  //! \name Utilities
  //! \{

  ASMJIT_INLINE_NODEBUG void swap(ZoneList& other) noexcept {
    std::swap(_nodes[0], other._nodes[0]);
    std::swap(_nodes[1], other._nodes[1]);
  }

  // Can be used to both append and prepend.
  inline void _addNode(NodeT* node, size_t dir) noexcept {
    NodeT* prev = _nodes[dir];

    node->_listNodes[!dir] = prev;
    _nodes[dir] = node;
    if (prev)
      prev->_listNodes[dir] = node;
    else
      _nodes[!dir] = node;
  }

  // Can be used to both append and prepend.
  inline void _insertNode(NodeT* ref, NodeT* node, size_t dir) noexcept {
    ASMJIT_ASSERT(ref != nullptr);

    NodeT* prev = ref;
    NodeT* next = ref->_listNodes[dir];

    prev->_listNodes[dir] = node;
    if (next)
      next->_listNodes[!dir] = node;
    else
      _nodes[dir] = node;

    node->_listNodes[!dir] = prev;
    node->_listNodes[ dir] = next;
  }

  ASMJIT_INLINE_NODEBUG void append(NodeT* node) noexcept { _addNode(node, kNodeIndexLast); }
  ASMJIT_INLINE_NODEBUG void prepend(NodeT* node) noexcept { _addNode(node, kNodeIndexFirst); }

  ASMJIT_INLINE_NODEBUG void insertAfter(NodeT* ref, NodeT* node) noexcept { _insertNode(ref, node, NodeT::kNodeIndexNext); }
  ASMJIT_INLINE_NODEBUG void insertBefore(NodeT* ref, NodeT* node) noexcept { _insertNode(ref, node, NodeT::kNodeIndexPrev); }

  inline NodeT* unlink(NodeT* node) noexcept {
    NodeT* prev = node->prev();
    NodeT* next = node->next();

    if (prev) { prev->_listNodes[1] = next; node->_listNodes[0] = nullptr; } else { _nodes[0] = next; }
    if (next) { next->_listNodes[0] = prev; node->_listNodes[1] = nullptr; } else { _nodes[1] = prev; }

    node->_listNodes[0] = nullptr;
    node->_listNodes[1] = nullptr;

    return node;
  }

  inline NodeT* popFirst() noexcept {
    NodeT* node = _nodes[0];
    ASMJIT_ASSERT(node != nullptr);

    NodeT* next = node->next();
    _nodes[0] = next;

    if (next) {
      next->_listNodes[0] = nullptr;
      node->_listNodes[1] = nullptr;
    }
    else {
      _nodes[1] = nullptr;
    }

    return node;
  }

  inline NodeT* pop() noexcept {
    NodeT* node = _nodes[1];
    ASMJIT_ASSERT(node != nullptr);

    NodeT* prev = node->prev();
    _nodes[1] = prev;

    if (prev) {
      prev->_listNodes[1] = nullptr;
      node->_listNodes[0] = nullptr;
    }
    else {
      _nodes[0] = nullptr;
    }

    return node;
  }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ZONELIST_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/zonestack.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/zone.h"
#include "../core/zonestack.h"

ASMJIT_BEGIN_NAMESPACE

// ZoneStackBase - Initialization & Reset
// ======================================

Error ZoneStackBase::_init(ZoneAllocator* allocator, size_t middleIndex) noexcept {
  ZoneAllocator* oldAllocator = _allocator;

  if (oldAllocator) {
    Block* block = _block[kBlockIndexFirst];
    while (block) {
      Block* next = block->next();
      oldAllocator->release(block, kBlockSize);
      block = next;
    }

    _allocator = nullptr;
    _block[kBlockIndexFirst] = nullptr;
    _block[kBlockIndexLast] = nullptr;
  }

  if (allocator) {
    Block* block = static_cast<Block*>(allocator->alloc(kBlockSize));
    if (ASMJIT_UNLIKELY(!block))
      return DebugUtils::errored(kErrorOutOfMemory);

    block->_link[kBlockIndexPrev] = nullptr;
    block->_link[kBlockIndexNext] = nullptr;
    block->_start = (uint8_t*)block + middleIndex;
    block->_end = (uint8_t*)block + middleIndex;

    _allocator = allocator;
    _block[kBlockIndexFirst] = block;
    _block[kBlockIndexLast] = block;
  }

  return kErrorOk;
}

// ZoneStackBase - Operations
// ==========================

Error ZoneStackBase::_prepareBlock(uint32_t side, size_t initialIndex) noexcept {
  ASMJIT_ASSERT(isInitialized());

  Block* prev = _block[side];
  ASMJIT_ASSERT(!prev->empty());

  Block* block = _allocator->allocT<Block>(kBlockSize);
  if (ASMJIT_UNLIKELY(!block))
    return DebugUtils::errored(kErrorOutOfMemory);

  block->_link[ side] = nullptr;
  block->_link[!side] = prev;
  block->_start = (uint8_t*)block + initialIndex;
  block->_end = (uint8_t*)block + initialIndex;

  prev->_link[side] = block;
  _block[side] = block;

  return kErrorOk;
}

void ZoneStackBase::_cleanupBlock(uint32_t side, size_t middleIndex) noexcept {
  Block* block = _block[side];
  ASMJIT_ASSERT(block->empty());

  Block* prev = block->_link[!side];
  if (prev) {
    ASMJIT_ASSERT(prev->_link[side] == block);
    _allocator->release(block, kBlockSize);

    prev->_link[side] = nullptr;
    _block[side] = prev;
  }
  else if (_block[!side] == block) {
    // If the container becomes empty center both pointers in the remaining block.
    block->_start = (uint8_t*)block + middleIndex;
    block->_end = (uint8_t*)block + middleIndex;
  }
}

// ZoneStack - Tests
// =================

#if defined(ASMJIT_TEST)
template<typename T>
static void test_zone_stack(ZoneAllocator* allocator, const char* typeName) {
  ZoneStack<T> stack;

  INFO("Testing ZoneStack<%s>", typeName);
  INFO("  (%d items per one Block)", ZoneStack<T>::kNumBlockItems);

  EXPECT_EQ(stack.init(allocator), kErrorOk);
  EXPECT_TRUE(stack.empty());

  EXPECT_EQ(stack.append(42), kErrorOk);
  EXPECT_FALSE(stack.empty())
    .message("Stack must not be empty after an item has been appended");
  EXPECT_EQ(stack.pop(), 42)
    .message("Stack.pop() must return the item that has been appended last");
  EXPECT_TRUE(stack.empty())
    .message("Stack must be empty after the last item has been removed");

  EXPECT_EQ(stack.prepend(43), kErrorOk);
  EXPECT_FALSE(stack.empty())
    .message("Stack must not be empty after an item has been prepended");
  EXPECT_EQ(stack.popFirst(), 43)
    .message("Stack.popFirst() must return the item that has been prepended last");
  EXPECT_TRUE(stack.empty())
    .message("Stack must be empty after the last item has been removed");

  int i;
  int iMin =-100000;
  int iMax = 100000;

  INFO("Validating prepend() & popFirst()");
  for (i = iMax; i >= 0; i--) stack.prepend(T(i));
  for (i = 0; i <= iMax; i++) {
    T item = stack.popFirst();
    EXPECT_EQ(i, item)
      .message("Item '%d' didn't match the item '%lld' popped", i, (long long)item);
    if (!stack.empty()) {
      item = stack.popFirst();
      EXPECT_EQ(i + 1, item)
        .message("Item '%d' didn't match the item '%lld' popped", i + 1, (long long)item);
      stack.prepend(item);
    }
  }
  EXPECT_TRUE(stack.empty());

  INFO("Validating append() & pop()");
  for (i = 0; i <= iMax; i++) stack.append(T(i));
  for (i = iMax; i >= 0; i--) {
    T item = stack.pop();
    EXPECT_EQ(i, item)
      .message("Item '%d' didn't match the item '%lld' popped", i, (long long)item);
    if (!stack.empty()) {
      item = stack.pop();
      EXPECT_EQ(i - 1, item)
        .message("Item '%d' didn't match the item '%lld' popped", i - 1, (long long)item);
      stack.append(item);
    }
  }
  EXPECT_TRUE(stack.empty());

  INFO("Validating append()/prepend() & popFirst()");
  for (i = 1; i <= iMax; i++) stack.append(T(i));
  for (i = 0; i >= iMin; i--) stack.prepend(T(i));

  for (i = iMin; i <= iMax; i++) {
    T item = stack.popFirst();
    EXPECT_EQ(i, item);
  }
  EXPECT_TRUE(stack.empty());

  INFO("Validating append()/prepend() & pop()");
  for (i = 0; i >= iMin; i--) stack.prepend(T(i));
  for (i = 1; i <= iMax; i++) stack.append(T(i));

  for (i = iMax; i >= iMin; i--) {
    T item = stack.pop();
    EXPECT_EQ(i, item);
  }
  EXPECT_TRUE(stack.empty());
}

UNIT(zone_stack) {
  Zone zone(8096);
  ZoneAllocator allocator(&zone);

  test_zone_stack<int>(&allocator, "int");
  test_zone_stack<int64_t>(&allocator, "int64_t");
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/zonestack.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ZONESTACK_H_INCLUDED
#define ASMJIT_CORE_ZONESTACK_H_INCLUDED

#include "../core/zone.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_zone
//! \{

//! Base class used by \ref ZoneStack.
class ZoneStackBase {
public:
  ASMJIT_NONCOPYABLE(ZoneStackBase)

  //! \name Constants
  //! \{

  enum : size_t {
    kBlockIndexPrev = 0,
    kBlockIndexNext = 1,

    kBlockIndexFirst = 0,
    kBlockIndexLast = 1,

    kBlockSize = ZoneAllocator::kHiMaxSize
  };

  //! \}

  //! \name Types
  //! \{

  struct Block {
    //! Next and previous blocks.
    Block* _link[2];
    //! Pointer to the start of the array.
    void* _start;
    //! Pointer to the end of the array.
    void* _end;

    ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _start == _end; }
    ASMJIT_INLINE_NODEBUG Block* prev() const noexcept { return _link[kBlockIndexPrev]; }
    ASMJIT_INLINE_NODEBUG Block* next() const noexcept { return _link[kBlockIndexNext]; }

    ASMJIT_INLINE_NODEBUG void setPrev(Block* block) noexcept { _link[kBlockIndexPrev] = block; }
    ASMJIT_INLINE_NODEBUG void setNext(Block* block) noexcept { _link[kBlockIndexNext] = block; }

    template<typename T>
    ASMJIT_INLINE_NODEBUG T* start() const noexcept { return static_cast<T*>(_start); }
    template<typename T>
    ASMJIT_INLINE_NODEBUG void setStart(T* start) noexcept { _start = static_cast<void*>(start); }

    template<typename T>
    ASMJIT_INLINE_NODEBUG T* end() const noexcept { return (T*)_end; }
    template<typename T>
    ASMJIT_INLINE_NODEBUG void setEnd(T* end) noexcept { _end = (void*)end; }

    template<typename T>
    ASMJIT_INLINE_NODEBUG const T* data() const noexcept { return (const T*)((const uint8_t*)(this) + sizeof(Block)); }
    template<typename T>
    ASMJIT_INLINE_NODEBUG T* data() noexcept { return (T*)((uint8_t*)(this) + sizeof(Block)); }

    template<typename T>
    ASMJIT_INLINE_NODEBUG bool canPrepend() const noexcept { return _start > data<void>(); }

    template<typename T>
    ASMJIT_INLINE_NODEBUG bool canAppend() const noexcept {
      size_t kNumBlockItems = (kBlockSize - sizeof(Block)) / sizeof(T);
      size_t kStartBlockIndex = sizeof(Block);
      size_t kEndBlockIndex = kStartBlockIndex + kNumBlockItems * sizeof(T);

      return (uintptr_t)_end <= ((uintptr_t)this + kEndBlockIndex - sizeof(T));
    }
  };

  //! \}

  //! \name Members
  //! \{

  //! Allocator used to allocate data.
  ZoneAllocator* _allocator {};
  //! First and last blocks.
  Block* _block[2] {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneStackBase() noexcept {}
  ASMJIT_INLINE_NODEBUG ~ZoneStackBase() noexcept { reset(); }

  ASMJIT_INLINE_NODEBUG bool isInitialized() const noexcept { return _allocator != nullptr; }
  ASMJIT_API Error _init(ZoneAllocator* allocator, size_t middleIndex) noexcept;
  ASMJIT_INLINE_NODEBUG Error reset() noexcept { return _init(nullptr, 0); }

  //! \}

  //! \name Accessors
  //! \{

  //! Returns `ZoneAllocator` attached to this container.
  ASMJIT_INLINE_NODEBUG ZoneAllocator* allocator() const noexcept { return _allocator; }

  inline bool empty() const noexcept {
    ASMJIT_ASSERT(isInitialized());
    return _block[0]->start<void>() == _block[1]->end<void>();
  }

  //! \}

  //! \cond INTERNAL
  //! \name Internal
  //! \{

  ASMJIT_API Error _prepareBlock(uint32_t side, size_t initialIndex) noexcept;
  ASMJIT_API void _cleanupBlock(uint32_t side, size_t middleIndex) noexcept;

  //! \}
  //! \endcond
};

//! Zone allocated stack container.
template<typename T>
class ZoneStack : public ZoneStackBase {
public:
  ASMJIT_NONCOPYABLE(ZoneStack)

  //! \name Constants
  //! \{

  enum : uint32_t {
    kNumBlockItems   = uint32_t((kBlockSize - sizeof(Block)) / sizeof(T)),
    kStartBlockIndex = uint32_t(sizeof(Block)),
    kMidBlockIndex   = uint32_t(kStartBlockIndex + (kNumBlockItems / 2) * sizeof(T)),
    kEndBlockIndex   = uint32_t(kStartBlockIndex + (kNumBlockItems    ) * sizeof(T))
  };

  //! \}

  //! \name Construction & Destruction
  //! \{

  inline ZoneStack() noexcept {}
  inline ~ZoneStack() noexcept {}

  inline Error init(ZoneAllocator* allocator) noexcept { return _init(allocator, kMidBlockIndex); }

  //! \}

  //! \name Utilities
  //! \{

  inline Error prepend(T item) noexcept {
    ASMJIT_ASSERT(isInitialized());
    Block* block = _block[kBlockIndexFirst];

    if (!block->canPrepend<T>()) {
      ASMJIT_PROPAGATE(_prepareBlock(kBlockIndexFirst, kEndBlockIndex));
      block = _block[kBlockIndexFirst];
    }

    T* ptr = block->start<T>() - 1;
    ASMJIT_ASSERT(ptr >= block->data<T>() && ptr <= block->data<T>() + (kNumBlockItems - 1));
    *ptr = item;
    block->setStart<T>(ptr);
    return kErrorOk;
  }

  inline Error append(T item) noexcept {
    ASMJIT_ASSERT(isInitialized());
    Block* block = _block[kBlockIndexLast];

    if (!block->canAppend<T>()) {
      ASMJIT_PROPAGATE(_prepareBlock(kBlockIndexLast, kStartBlockIndex));
      block = _block[kBlockIndexLast];
    }

    T* ptr = block->end<T>();
    ASMJIT_ASSERT(ptr >= block->data<T>() && ptr <= block->data<T>() + (kNumBlockItems - 1));

    *ptr++ = item;
    block->setEnd(ptr);
    return kErrorOk;
  }

  inline T popFirst() noexcept {
    ASMJIT_ASSERT(isInitialized());
    ASMJIT_ASSERT(!empty());

    Block* block = _block[kBlockIndexFirst];
    ASMJIT_ASSERT(!block->empty());

    T* ptr = block->start<T>();
    T item = *ptr++;

    block->setStart(ptr);
    if (block->empty())
      _cleanupBlock(kBlockIndexFirst, kMidBlockIndex);

    return item;
  }

  inline T pop() noexcept {
    ASMJIT_ASSERT(isInitialized());
    ASMJIT_ASSERT(!empty());

    Block* block = _block[kBlockIndexLast];
    ASMJIT_ASSERT(!block->empty());

    T* ptr = block->end<T>();
    T item = *--ptr;
    ASMJIT_ASSERT(ptr >= block->data<T>());
    ASMJIT_ASSERT(ptr >= block->start<T>());

    block->setEnd(ptr);
    if (block->empty())
      _cleanupBlock(kBlockIndexLast, kMidBlockIndex);

    return item;
  }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ZONESTACK_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/zonestring.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ZONESTRING_H_INCLUDED
#define ASMJIT_CORE_ZONESTRING_H_INCLUDED

#include "../core/globals.h"
#include "../core/zone.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_zone
//! \{

//! A helper class used by \ref ZoneString implementation.
struct ZoneStringBase {
  union {
    struct {
      uint32_t _size;
      char _embedded[sizeof(void*) * 2 - 4];
    };
    struct {
      void* _dummy;
      char* _external;
    };
  };

  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _dummy = nullptr;
    _external = nullptr;
  }

  Error setData(Zone* zone, uint32_t maxEmbeddedSize, const char* str, size_t size) noexcept {
    if (size == SIZE_MAX)
      size = strlen(str);

    if (size <= maxEmbeddedSize) {
      memcpy(_embedded, str, size);
      _embedded[size] = '\0';
    }
    else {
      char* external = static_cast<char*>(zone->dup(str, size, true));
      if (ASMJIT_UNLIKELY(!external))
        return DebugUtils::errored(kErrorOutOfMemory);
      _external = external;
    }

    _size = uint32_t(size);
    return kErrorOk;
  }
};

//! A string template that can be zone allocated.
//!
//! Helps with creating strings that can be either statically allocated if they are small, or externally allocated
//! in case their size exceeds the limit. The `N` represents the size of the whole `ZoneString` structure, based on
//! that size the maximum size of the internal buffer is determined.
template<size_t N>
class ZoneString {
public:
  //! \name Constants
  //! \{

  enum : uint32_t {
    kWholeSize = (N > sizeof(ZoneStringBase)) ? uint32_t(N) : uint32_t(sizeof(ZoneStringBase)),
    kMaxEmbeddedSize = kWholeSize - 5
  };

  //! \}

  //! \name Members
  //! \{

  union {
    ZoneStringBase _base;
    char _wholeData[kWholeSize];
  };

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneString() noexcept { reset(); }
  ASMJIT_INLINE_NODEBUG void reset() noexcept { _base.reset(); }

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the string is empty.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _base._size == 0; }

  //! Returns the string data.
  ASMJIT_INLINE_NODEBUG const char* data() const noexcept { return _base._size <= kMaxEmbeddedSize ? _base._embedded : _base._external; }
  //! Returns the string size.
  ASMJIT_INLINE_NODEBUG uint32_t size() const noexcept { return _base._size; }

  //! Tests whether the string is embedded (e.g. no dynamically allocated).
  ASMJIT_INLINE_NODEBUG bool isEmbedded() const noexcept { return _base._size <= kMaxEmbeddedSize; }

  //! Copies a new `data` of the given `size` to the string.
  //!
  //! If the `size` exceeds the internal buffer the given `zone` will be used to duplicate the data, otherwise
  //! the internal buffer will be used as a storage.
  ASMJIT_INLINE_NODEBUG Error setData(Zone* zone, const char* data, size_t size) noexcept {
    return _base.setData(zone, kMaxEmbeddedSize, data, size);
  }

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ZONESTRING_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/zonetree.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/support.h"
#include "../core/zone.h"
#include "../core/zonetree.h"

ASMJIT_BEGIN_NAMESPACE

// ZoneTreeBase - Tests
// ====================

#if defined(ASMJIT_TEST)
template<typename NodeT>
struct ZoneRBUnit {
  typedef ZoneTree<NodeT> Tree;

  static void verifyTree(Tree& tree) noexcept {
    EXPECT_GT(checkHeight(static_cast<NodeT*>(tree._root)), 0);
  }

  // Check whether the Red-Black tree is valid.
  static int checkHeight(NodeT* node) noexcept {
    if (!node) return 1;

    NodeT* ln = node->left();
    NodeT* rn = node->right();

    // Invalid tree.
    EXPECT_TRUE(ln == nullptr || *ln < *node);
    EXPECT_TRUE(rn == nullptr || *rn > *node);

    // Red violation.
    EXPECT_TRUE(!node->isRed() || (!ZoneTreeNode::_isValidRed(ln) && !ZoneTreeNode::_isValidRed(rn)));

    // Black violation.
    int lh = checkHeight(ln);
    int rh = checkHeight(rn);
    EXPECT_TRUE(!lh || !rh || lh == rh);

    // Only count black links.
    return (lh && rh) ? lh + !node->isRed() : 0;
  }
};

class MyRBNode : public ZoneTreeNodeT<MyRBNode> {
public:
  ASMJIT_NONCOPYABLE(MyRBNode)

  inline explicit MyRBNode(uint32_t key) noexcept
    : _key(key) {}

  inline bool operator<(const MyRBNode& other) const noexcept { return _key < other._key; }
  inline bool operator>(const MyRBNode& other) const noexcept { return _key > other._key; }

  inline bool operator<(uint32_t queryKey) const noexcept { return _key < queryKey; }
  inline bool operator>(uint32_t queryKey) const noexcept { return _key > queryKey; }

  uint32_t _key;
};

UNIT(zone_rbtree) {
  uint32_t kCount = BrokenAPI::hasArg("--quick") ? 1000 : 10000;

  Zone zone(4096);
  ZoneTree<MyRBNode> rbTree;

  uint32_t key;
  INFO("Inserting %u elements to RBTree and validating each operation", unsigned(kCount));
  for (key = 0; key < kCount; key++) {
    rbTree.insert(zone.newT<MyRBNode>(key));
    ZoneRBUnit<MyRBNode>::verifyTree(rbTree);
  }

  uint32_t count = kCount;
  INFO("Removing %u elements from RBTree and validating each operation", unsigned(kCount));
  do {
    MyRBNode* node;

    for (key = 0; key < count; key++) {
      node = rbTree.get(key);
      EXPECT_NOT_NULL(node);
      EXPECT_EQ(node->_key, key);
    }

    node = rbTree.get(--count);
    rbTree.remove(node);
    ZoneRBUnit<MyRBNode>::verifyTree(rbTree);
  } while (count);

  EXPECT_TRUE(rbTree.empty());
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/zonetree.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ZONETREE_H_INCLUDED
#define ASMJIT_CORE_ZONETREE_H_INCLUDED

#include "../core/support.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_zone
//! \{

//! RB-Tree node.
//!
//! The color is stored in a least significant bit of the `left` node.
//!
//! WARNING: Always use accessors to access left and right children.
class ZoneTreeNode {
public:
  ASMJIT_NONCOPYABLE(ZoneTreeNode)

  //! \name Constants
  //! \{

  enum : uintptr_t {
    kRedMask = 0x1,
    kPtrMask = ~kRedMask
  };

  //! \}

  //! \name Members
  //! \{

  uintptr_t _rbNodeData[2] {};

  //! \}

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneTreeNode() noexcept {}

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool isRed() const noexcept { return static_cast<bool>(_rbNodeData[0] & kRedMask); }

  ASMJIT_INLINE_NODEBUG bool hasChild(size_t i) const noexcept { return _rbNodeData[i] > kRedMask; }
  ASMJIT_INLINE_NODEBUG bool hasLeft() const noexcept { return _rbNodeData[0] > kRedMask; }
  ASMJIT_INLINE_NODEBUG bool hasRight() const noexcept { return _rbNodeData[1] != 0; }

  template<typename T = ZoneTreeNode>
  ASMJIT_INLINE_NODEBUG T* child(size_t i) const noexcept { return static_cast<T*>(_getChild(i)); }
  template<typename T = ZoneTreeNode>
  ASMJIT_INLINE_NODEBUG T* left() const noexcept { return static_cast<T*>(_getLeft()); }
  template<typename T = ZoneTreeNode>
  ASMJIT_INLINE_NODEBUG T* right() const noexcept { return static_cast<T*>(_getRight()); }

  //! \}

  //! \cond INTERNAL
  //! \name Internal
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneTreeNode* _getChild(size_t i) const noexcept { return (ZoneTreeNode*)(_rbNodeData[i] & kPtrMask); }
  ASMJIT_INLINE_NODEBUG ZoneTreeNode* _getLeft() const noexcept { return (ZoneTreeNode*)(_rbNodeData[0] & kPtrMask); }
  ASMJIT_INLINE_NODEBUG ZoneTreeNode* _getRight() const noexcept { return (ZoneTreeNode*)(_rbNodeData[1]); }

  ASMJIT_INLINE_NODEBUG void _setChild(size_t i, ZoneTreeNode* node) noexcept { _rbNodeData[i] = (_rbNodeData[i] & kRedMask) | (uintptr_t)node; }
  ASMJIT_INLINE_NODEBUG void _setLeft(ZoneTreeNode* node) noexcept { _rbNodeData[0] = (_rbNodeData[0] & kRedMask) | (uintptr_t)node; }
  ASMJIT_INLINE_NODEBUG void _setRight(ZoneTreeNode* node) noexcept { _rbNodeData[1] = (uintptr_t)node; }

  ASMJIT_INLINE_NODEBUG void _makeRed() noexcept { _rbNodeData[0] |= kRedMask; }
  ASMJIT_INLINE_NODEBUG void _makeBlack() noexcept { _rbNodeData[0] &= kPtrMask; }

  //! Tests whether the node is RED (RED node must be non-null and must have RED flag set).
  static ASMJIT_INLINE_NODEBUG bool _isValidRed(ZoneTreeNode* node) noexcept { return node && node->isRed(); }

  //! \}
  //! \endcond
};

//! RB-Tree node casted to `NodeT`.
template<typename NodeT>
class ZoneTreeNodeT : public ZoneTreeNode {
public:
  ASMJIT_NONCOPYABLE(ZoneTreeNodeT)

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneTreeNodeT() noexcept
    : ZoneTreeNode() {}

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG NodeT* child(size_t i) const noexcept { return static_cast<NodeT*>(_getChild(i)); }
  ASMJIT_INLINE_NODEBUG NodeT* left() const noexcept { return static_cast<NodeT*>(_getLeft()); }
  ASMJIT_INLINE_NODEBUG NodeT* right() const noexcept { return static_cast<NodeT*>(_getRight()); }

  //! \}
};

//! RB-Tree.
template<typename NodeT>
class ZoneTree {
public:
  ASMJIT_NONCOPYABLE(ZoneTree)

  typedef NodeT Node;
  NodeT* _root {};

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneTree() noexcept {}
  ASMJIT_INLINE_NODEBUG ZoneTree(ZoneTree&& other) noexcept
    : _root(other._root) {}
  ASMJIT_INLINE_NODEBUG void reset() noexcept { _root = nullptr; }

  //! \}

  //! \name Accessors
  //! \{

  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _root == nullptr; }
  ASMJIT_INLINE_NODEBUG NodeT* root() const noexcept { return static_cast<NodeT*>(_root); }

  //! \}

  //! \name Utilities
  //! \{

  ASMJIT_INLINE_NODEBUG void swap(ZoneTree& other) noexcept {
    std::swap(_root, other._root);
  }

  template<typename CompareT = Support::Compare<Support::SortOrder::kAscending>>
  void insert(NodeT* ASMJIT_NONNULL(node), const CompareT& cmp = CompareT()) noexcept {
    // Node to insert must not contain garbage.
    ASMJIT_ASSERT(!node->hasLeft());
    ASMJIT_ASSERT(!node->hasRight());
    ASMJIT_ASSERT(!node->isRed());

    if (!_root) {
      _root = node;
      return;
    }

    ZoneTreeNode head;           // False root node,
    head._setRight(_root);       // having root on the right.

    ZoneTreeNode* g = nullptr;   // Grandparent.
    ZoneTreeNode* p = nullptr;   // Parent.
    ZoneTreeNode* t = &head;     // Iterator.
    ZoneTreeNode* q = _root;     // Query.

    size_t dir = 0;              // Direction for accessing child nodes.
    size_t last = 0;             // Not needed to initialize, but makes some tools happy.

    node->_makeRed();            // New nodes are always red and violations fixed appropriately.

    // Search down the tree.
    for (;;) {
      if (!q) {
        // Insert new node at the bottom.
        q = node;
        p->_setChild(dir, node);
      }
      else if (_isValidRed(q->_getLeft()) && _isValidRed(q->_getRight())) {
        // Color flip.
        q->_makeRed();
        q->_getLeft()->_makeBlack();
        q->_getRight()->_makeBlack();
      }

      // Fix red violation.
      if (_isValidRed(q) && _isValidRed(p)) {
        ASMJIT_ASSUME(g != nullptr);
        ASMJIT_ASSUME(p != nullptr);
        t->_setChild(t->_getRight() == g,
                     q == p->_getChild(last) ? _singleRotate(g, !last) : _doubleRotate(g, !last));
      }

      // Stop if found.
      if (q == node)
        break;

      last = dir;
      dir = cmp(*static_cast<NodeT*>(q), *static_cast<NodeT*>(node)) < 0;

      // Update helpers.
      if (g) t = g;

      g = p;
      p = q;
      q = q->_getChild(dir);
    }

    // Update root and make it black.
    _root = static_cast<NodeT*>(head._getRight());
    _root->_makeBlack();
  }

  //! Remove node from RBTree.
  template<typename CompareT = Support::Compare<Support::SortOrder::kAscending>>
  void remove(ZoneTreeNode* ASMJIT_NONNULL(node), const CompareT& cmp = CompareT()) noexcept {
    ZoneTreeNode head;           // False root node,
    head._setRight(_root);       // having root on the right.

    ZoneTreeNode* g = nullptr;   // Grandparent.
    ZoneTreeNode* p = nullptr;   // Parent.
    ZoneTreeNode* q = &head;     // Query.

    ZoneTreeNode* f  = nullptr;  // Found item.
    ZoneTreeNode* gf = nullptr;  // Found grandparent.
    size_t dir = 1;              // Direction (0 or 1).

    // Search and push a red down.
    while (q->hasChild(dir)) {
      size_t last = dir;

      // Update helpers.
      g = p;
      p = q;
      q = q->_getChild(dir);
      dir = cmp(*static_cast<NodeT*>(q), *static_cast<NodeT*>(node)) < 0;

      // Save found node.
      if (q == node) {
        f = q;
        gf = g;
      }

      // Push the red node down.
      if (!_isValidRed(q) && !_isValidRed(q->_getChild(dir))) {
        if (_isValidRed(q->_getChild(!dir))) {
          ZoneTreeNode* child = _singleRotate(q, dir);
          p->_setChild(last, child);
          p = child;
        }
        else if (!_isValidRed(q->_getChild(!dir)) && p->_getChild(!last)) {
          ZoneTreeNode* s = p->_getChild(!last);
          if (!_isValidRed(s->_getChild(!last)) && !_isValidRed(s->_getChild(last))) {
            // Color flip.
            p->_makeBlack();
            s->_makeRed();
            q->_makeRed();
          }
          else {
            ASMJIT_ASSUME(g != nullptr);
            ASMJIT_ASSUME(s != nullptr);

            size_t dir2 = g->_getRight() == p;
            ZoneTreeNode* child = g->_getChild(dir2);

            if (_isValidRed(s->_getChild(last))) {
              child = _doubleRotate(p, last);
              g->_setChild(dir2, child);
            }
            else if (_isValidRed(s->_getChild(!last))) {
              child = _singleRotate(p, last);
              g->_setChild(dir2, child);
            }

            // Ensure correct coloring.
            q->_makeRed();
            child->_makeRed();
            child->_getLeft()->_makeBlack();
            child->_getRight()->_makeBlack();
          }
        }
      }
    }

    // Replace and remove.
    ASMJIT_ASSERT(f != nullptr);
    ASMJIT_ASSERT(f != &head);
    ASMJIT_ASSERT(q != &head);

    p->_setChild(p->_getRight() == q,
                 q->_getChild(q->_getLeft() == nullptr));

    // NOTE: The original algorithm used a trick to just copy 'key/value' to `f` and mark `q` for deletion. But this
    // is unacceptable here as we really want to destroy the passed `node`. So, we have to make sure that we have
    // really removed `f` and not `q`.
    if (f != q) {
      ASMJIT_ASSERT(f != &head);
      ASMJIT_ASSERT(f != gf);

      ZoneTreeNode* n = gf ? gf : &head;
      dir = (n == &head) ? 1  : cmp(*static_cast<NodeT*>(n), *static_cast<NodeT*>(node)) < 0;

      for (;;) {
        if (n->_getChild(dir) == f) {
          n->_setChild(dir, q);
          // RAW copy, including the color.
          q->_rbNodeData[0] = f->_rbNodeData[0];
          q->_rbNodeData[1] = f->_rbNodeData[1];
          break;
        }

        n = n->_getChild(dir);

        // Cannot be true as we know that it must reach `f` in few iterations.
        ASMJIT_ASSERT(n != nullptr);
        dir = cmp(*static_cast<NodeT*>(n), *static_cast<NodeT*>(node)) < 0;
      }
    }

    // Update root and make it black.
    _root = static_cast<NodeT*>(head._getRight());
    if (_root) _root->_makeBlack();
  }

  template<typename KeyT, typename CompareT = Support::Compare<Support::SortOrder::kAscending>>
  inline NodeT* get(const KeyT& key, const CompareT& cmp = CompareT()) const noexcept {
    ZoneTreeNode* node = _root;
    while (node) {
      auto result = cmp(*static_cast<const NodeT*>(node), key);
      if (result == 0) break;

      // Go left or right depending on the `result`.
      node = node->_getChild(result < 0);
    }
    return static_cast<NodeT*>(node);
  }

  //! \}

  //! \cond INTERNAL
  //! \name Internal
  //! \{

  static inline bool _isValidRed(ZoneTreeNode* node) noexcept { return ZoneTreeNode::_isValidRed(node); }

  //! Single rotation.
  static inline ZoneTreeNode* _singleRotate(ZoneTreeNode* ASMJIT_NONNULL(root), size_t dir) noexcept {
    ZoneTreeNode* save = root->_getChild(!dir);
    ASMJIT_ASSUME(save != nullptr);

    ZoneTreeNode* saveChild = save->_getChild(dir);
    root->_setChild(!dir, saveChild);
    save->_setChild( dir, root);
    root->_makeRed();
    save->_makeBlack();
    return save;
  }

  //! Double rotation.
  static inline ZoneTreeNode* _doubleRotate(ZoneTreeNode* ASMJIT_NONNULL(root), size_t dir) noexcept {
    ZoneTreeNode* child = root->_getChild(!dir);
    ASMJIT_ASSUME(child != nullptr);

    root->_setChild(!dir, _singleRotate(child, !dir));
    return _singleRotate(root, dir);
  }

  //! \}
  //! \endcond
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ZONETREE_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/core/zonevector.cpp`:

```cpp
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#include "../core/api-build_p.h"
#include "../core/support.h"
#include "../core/zone.h"
#include "../core/zonevector.h"

ASMJIT_BEGIN_NAMESPACE

// ZoneVectorBase - Helpers
// ========================

// ZoneVector is used as an array to hold short-lived data structures used during code generation. The growing
// strategy is simple - use small capacity at the beginning (very good for ZoneAllocator) and then grow quicker
// to prevent successive reallocations.
static ASMJIT_FORCE_INLINE uint32_t ZoneVector_growCapacity(uint32_t current, uint32_t growMinimum, uint32_t sizeOfT) noexcept {
  static constexpr size_t kGrowThreshold = Globals::kGrowThreshold;

  size_t byteSize = size_t(current) * sizeOfT;
  size_t minimumByteSize = size_t(growMinimum) * sizeOfT;

  // This is more than exponential growth at the beginning.
  if (byteSize < 32) {
    byteSize = 32;
  }
  else if (byteSize < 128) {
    byteSize = 128;
  }
  else if (byteSize < 512) {
    byteSize = 512;
  }

  if (byteSize < minimumByteSize) {
    // Exponential growth before we reach `kGrowThreshold`.
    byteSize = Support::alignUpPowerOf2(minimumByteSize);

    // Bail to `growMinimum` in case of overflow - most likely whatever that is happening afterwards would just fail.
    if (byteSize < minimumByteSize) {
      return growMinimum;
    }

    // Pretty much chunked growth advancing by `kGrowThreshold` after we exceed it.
    // This should not be a common case, so we don't really have to optimize for it.
    if (byteSize > kGrowThreshold) {
      // Align to kGrowThreshold.
      size_t remainder = minimumByteSize % kGrowThreshold;

      byteSize = minimumByteSize + remainder;

      // Bail to `growMinimum` in case of overflow - should never happen as it's unlikely we would hit this on a 32-bit
      // machine (consecutive near 4GiB allocation is impossible, and this should never happen on 64-bit machine as we
      // use 32-bit size & capacity, so overflow of 64 bit integer is not possible. Added just as an extreme measure.
      if (byteSize < minimumByteSize)
        return growMinimum;
    }
  }

  size_t n = byteSize / sizeOfT;
  return uint32_t(Support::min<size_t>(n, 0xFFFFFFFFu));
}

static ASMJIT_FORCE_INLINE bool ZoneVector_byteSizeIsSafe(size_t nBytes, uint32_t n) noexcept {
  if (sizeof(uint32_t) < sizeof(size_t))
    return true; // there is no problem when running on a 64-bit machine.
  else
    return nBytes >= size_t(n);
};

Error ZoneVectorBase::_grow(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept {
  uint32_t capacity = _capacity;
  uint32_t after = _size;

  if (ASMJIT_UNLIKELY(std::numeric_limits<uint32_t>::max() - n < after))
    return DebugUtils::errored(kErrorOutOfMemory);

  after += n;
  if (capacity >= after)
    return kErrorOk;

  return _reserve(allocator, sizeOfT, ZoneVector_growCapacity(capacity, after, sizeOfT));
}

Error ZoneVectorBase::_reserve(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept {
  uint32_t oldCapacity = _capacity;
  if (oldCapacity >= n)
    return kErrorOk;

  size_t nBytes = size_t(n) * sizeOfT;
  if (ASMJIT_UNLIKELY(!ZoneVector_byteSizeIsSafe(nBytes, n)))
    return DebugUtils::errored(kErrorOutOfMemory);

  size_t allocatedBytes;
  uint8_t* newData = static_cast<uint8_t*>(allocator->alloc(nBytes, allocatedBytes));

  if (ASMJIT_UNLIKELY(!newData))
    return DebugUtils::errored(kErrorOutOfMemory);

  uint32_t newCapacity = uint32_t(allocatedBytes / sizeOfT);
  ASMJIT_ASSERT(newCapacity >= n);

  void* oldData = _data;
  if (oldData && _size) {
    memcpy(newData, oldData, size_t(_size) * sizeOfT);
    allocator->release(oldData, size_t(oldCapacity) * sizeOfT);
  }

  _data = newData;
  _capacity = newCapacity;

  return kErrorOk;
}

Error ZoneVectorBase::_growingReserve(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept {
  uint32_t capacity = _capacity;
  if (capacity >= n)
    return kErrorOk;
  return _reserve(allocator, sizeOfT, ZoneVector_growCapacity(capacity, n, sizeOfT));
}

Error ZoneVectorBase::_resize(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept {
  uint32_t size = _size;

  if (_capacity < n) {
    ASMJIT_PROPAGATE(_grow(allocator, sizeOfT, n - size));
    ASMJIT_ASSERT(_capacity >= n);
  }

  if (size < n)
    memset(static_cast<uint8_t*>(_data) + size_t(size) * sizeOfT, 0, size_t(n - size) * sizeOfT);

  _size = n;
  return kErrorOk;
}

// ZoneBitVector - Operations
// ==========================

Error ZoneBitVector::copyFrom(ZoneAllocator* allocator, const ZoneBitVector& other) noexcept {
  BitWord* data = _data;
  uint32_t newSize = other.size();

  if (!newSize) {
    _size = 0;
    return kErrorOk;
  }

  if (newSize > _capacity) {
    // Realloc needed... Calculate the minimum capacity (in bytes) required.
    uint32_t minimumCapacityInBits = Support::alignUp<uint32_t>(newSize, kBitWordSizeInBits);
    if (ASMJIT_UNLIKELY(minimumCapacityInBits < newSize))
      return DebugUtils::errored(kErrorOutOfMemory);

    // Normalize to bytes.
    uint32_t minimumCapacity = minimumCapacityInBits / 8;
    size_t allocatedCapacity;

    BitWord* newData = static_cast<BitWord*>(allocator->alloc(minimumCapacity, allocatedCapacity));
    if (ASMJIT_UNLIKELY(!newData))
      return DebugUtils::errored(kErrorOutOfMemory);

    // `allocatedCapacity` now contains number in bytes, we need bits.
    size_t allocatedCapacityInBits = allocatedCapacity * 8;

    // Arithmetic overflow should normally not happen. If it happens we just
    // change the `allocatedCapacityInBits` to the `minimumCapacityInBits` as
    // this value is still safe to be used to call `_allocator->release(...)`.
    if (ASMJIT_UNLIKELY(allocatedCapacityInBits < allocatedCapacity))
      allocatedCapacityInBits = minimumCapacityInBits;

    if (data)
      allocator->release(data, _capacity / 8);
    data = newData;

    _data = data;
    _capacity = uint32_t(allocatedCapacityInBits);
  }

  _size = newSize;
  _copyBits(data, other.data(), _wordsPerBits(newSize));

  return kErrorOk;
}

Error ZoneBitVector::_resize(ZoneAllocator* allocator, uint32_t newSize, uint32_t idealCapacity, bool newBitsValue) noexcept {
  ASMJIT_ASSERT(idealCapacity >= newSize);

  if (newSize <= _size) {
    // The size after the resize is lesser than or equal to the current size.
    uint32_t idx = newSize / kBitWordSizeInBits;
    uint32_t bit = newSize % kBitWordSizeInBits;

    // Just set all bits outside of the new size in the last word to zero.
    // There is a case that there are not bits to set if `bit` is zero. This
    // happens when `newSize` is a multiply of `kBitWordSizeInBits` like 64, 128,
    // and so on. In that case don't change anything as that would mean settings
    // bits outside of the `_size`.
    if (bit)
      _data[idx] &= (BitWord(1) << bit) - 1u;

    _size = newSize;
    return kErrorOk;
  }

  uint32_t oldSize = _size;
  BitWord* data = _data;

  if (newSize > _capacity) {
    // Realloc needed, calculate the minimum capacity (in bytes) required.
    uint32_t minimumCapacityInBits = Support::alignUp<uint32_t>(idealCapacity, kBitWordSizeInBits);

    if (ASMJIT_UNLIKELY(minimumCapacityInBits < newSize))
      return DebugUtils::errored(kErrorOutOfMemory);

    // Normalize to bytes.
    uint32_t minimumCapacity = minimumCapacityInBits / 8;
    size_t allocatedCapacity;

    BitWord* newData = static_cast<BitWord*>(allocator->alloc(minimumCapacity, allocatedCapacity));
    if (ASMJIT_UNLIKELY(!newData))
      return DebugUtils::errored(kErrorOutOfMemory);

    // `allocatedCapacity` now contains number in bytes, we need bits.
    size_t allocatedCapacityInBits = allocatedCapacity * 8;

    // Arithmetic overflow should normally not happen. If it happens we just
    // change the `allocatedCapacityInBits` to the `minimumCapacityInBits` as
    // this value is still safe to be used to call `_allocator->release(...)`.
    if (ASMJIT_UNLIKELY(allocatedCapacityInBits < allocatedCapacity))
      allocatedCapacityInBits = minimumCapacityInBits;

    _copyBits(newData, data, _wordsPerBits(oldSize));

    if (data)
      allocator->release(data, _capacity / 8);
    data = newData;

    _data = data;
    _capacity = uint32_t(allocatedCapacityInBits);
  }

  // Start (of the old size) and end (of the new size) bits
  uint32_t idx = oldSize / kBitWordSizeInBits;
  uint32_t startBit = oldSize % kBitWordSizeInBits;
  uint32_t endBit = newSize % kBitWordSizeInBits;

  // Set new bits to either 0 or 1. The `pattern` is used to set multiple
  // bits per bit-word and contains either all zeros or all ones.
  BitWord pattern = Support::bitMaskFromBool<BitWord>(newBitsValue);

  // First initialize the last bit-word of the old size.
  if (startBit) {
    uint32_t nBits = 0;

    if (idx == (newSize / kBitWordSizeInBits)) {
      // The number of bit-words is the same after the resize. In that case
      // we need to set only bits necessary in the current last bit-word.
      ASMJIT_ASSERT(startBit < endBit);
      nBits = endBit - startBit;
    }
    else {
      // There is be more bit-words after the resize. In that case we don't
      // have to be extra careful about the last bit-word of the old size.
      nBits = kBitWordSizeInBits - startBit;
    }

    data[idx++] |= pattern << nBits;
  }

  // Initialize all bit-words after the last bit-word of the old size.
  uint32_t endIdx = _wordsPerBits(newSize);
  while (idx < endIdx) data[idx++] = pattern;

  // Clear unused bits of the last bit-word.
  if (endBit)
    data[endIdx - 1] = pattern & ((BitWord(1) << endBit) - 1);

  _size = newSize;
  return kErrorOk;
}

Error ZoneBitVector::_append(ZoneAllocator* allocator, bool value) noexcept {
  uint32_t kThreshold = Globals::kGrowThreshold * 8;
  uint32_t newSize = _size + 1;
  uint32_t idealCapacity = _capacity;

  if (idealCapacity < 128)
    idealCapacity = 128;
  else if (idealCapacity <= kThreshold)
    idealCapacity *= 2;
  else
    idealCapacity += kThreshold;

  if (ASMJIT_UNLIKELY(idealCapacity < _capacity)) {
    if (ASMJIT_UNLIKELY(_size == std::numeric_limits<uint32_t>::max()))
      return DebugUtils::errored(kErrorOutOfMemory);
    idealCapacity = newSize;
  }

  return _resize(allocator, newSize, idealCapacity, value);
}

// ZoneVector / ZoneBitVector - Tests
// ==================================

#if defined(ASMJIT_TEST)
template<typename T>
static void test_zone_vector(ZoneAllocator* allocator, const char* typeName) {
  constexpr uint32_t kMiB = 1024 * 1024;

  int i;
  int kMax = 100000;

  ZoneVector<T> vec;

  INFO("ZoneVector<%s> basic tests", typeName);
  EXPECT_EQ(vec.append(allocator, 0), kErrorOk);
  EXPECT_FALSE(vec.empty());
  EXPECT_EQ(vec.size(), 1u);
  EXPECT_GE(vec.capacity(), 1u);
  EXPECT_EQ(vec.indexOf(0), 0u);
  EXPECT_EQ(vec.indexOf(-11), Globals::kNotFound);

  vec.clear();
  EXPECT_TRUE(vec.empty());
  EXPECT_EQ(vec.size(), 0u);
  EXPECT_EQ(vec.indexOf(0), Globals::kNotFound);

  for (i = 0; i < kMax; i++) {
    EXPECT_EQ(vec.append(allocator, T(i)), kErrorOk);
  }
  EXPECT_FALSE(vec.empty());
  EXPECT_EQ(vec.size(), uint32_t(kMax));
  EXPECT_EQ(vec.indexOf(T(0)), uint32_t(0));
  EXPECT_EQ(vec.indexOf(T(kMax - 1)), uint32_t(kMax - 1));

  EXPECT_EQ(vec.begin()[0], 0);
  EXPECT_EQ(vec.end()[-1], kMax - 1);

  EXPECT_EQ(vec.rbegin()[0], kMax - 1);
  EXPECT_EQ(vec.rend()[-1], 0);

  int64_t fsum = 0;
  int64_t rsum = 0;

  for (const T& item : vec) {
    fsum += item;
  }

  for (auto it = vec.rbegin(); it != vec.rend(); ++it) {
    rsum += *it;
  }

  EXPECT_EQ(fsum, rsum);
  vec.release(allocator);

  INFO("ZoneBitVector::growingReserve()");
  for (uint32_t j = 0; j < 40 / sizeof(T); j += 8) {
    EXPECT_EQ(vec.growingReserve(allocator, j * kMiB), kErrorOk);
    EXPECT_GE(vec.capacity(), j * kMiB);
  }
}

static void test_zone_bitvector(ZoneAllocator* allocator) {
  Zone zone(8096);

  uint32_t i, count;
  uint32_t kMaxCount = 100;

  ZoneBitVector vec;
  EXPECT_TRUE(vec.empty());
  EXPECT_EQ(vec.size(), 0u);

  INFO("ZoneBitVector::resize()");
  for (count = 1; count < kMaxCount; count++) {
    vec.clear();
    EXPECT_EQ(vec.resize(allocator, count, false), kErrorOk);
    EXPECT_EQ(vec.size(), count);

    for (i = 0; i < count; i++)
      EXPECT_FALSE(vec.bitAt(i));

    vec.clear();
    EXPECT_EQ(vec.resize(allocator, count, true), kErrorOk);
    EXPECT_EQ(vec.size(), count);

    for (i = 0; i < count; i++)
      EXPECT_TRUE(vec.bitAt(i));
  }

  INFO("ZoneBitVector::fillBits() / clearBits()");
  for (count = 1; count < kMaxCount; count += 2) {
    vec.clear();
    EXPECT_EQ(vec.resize(allocator, count), kErrorOk);
    EXPECT_EQ(vec.size(), count);

    for (i = 0; i < (count + 1) / 2; i++) {
      bool value = bool(i & 1);
      if (value)
        vec.fillBits(i, count - i * 2);
      else
        vec.clearBits(i, count - i * 2);
    }

    for (i = 0; i < count; i++) {
      EXPECT_EQ(vec.bitAt(i), bool(i & 1));
    }
  }
}

UNIT(zone_vector) {
  Zone zone(8096);
  ZoneAllocator allocator(&zone);

  test_zone_vector<int>(&allocator, "int");
  test_zone_vector<int64_t>(&allocator, "int64_t");
  test_zone_bitvector(&allocator);
}
#endif

ASMJIT_END_NAMESPACE

```

`CodeCleaner/asmjit/asmjit/core/zonevector.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_CORE_ZONEVECTOR_H_INCLUDED
#define ASMJIT_CORE_ZONEVECTOR_H_INCLUDED

#include "../core/support.h"
#include "../core/zone.h"

ASMJIT_BEGIN_NAMESPACE

//! \addtogroup asmjit_zone
//! \{

//! Base class used by \ref ZoneVector template.
class ZoneVectorBase {
public:
  ASMJIT_NONCOPYABLE(ZoneVectorBase)

  // STL compatibility;
  typedef uint32_t size_type;
  typedef ptrdiff_t difference_type;

  //! Vector data (untyped).
  void* _data = nullptr;
  //! Size of the vector.
  size_type _size = 0;
  //! Capacity of the vector.
  size_type _capacity = 0;

protected:
  //! \name Construction & Destruction
  //! \{

  //! Creates a new instance of `ZoneVectorBase`.
  inline ZoneVectorBase() noexcept {}

  inline ZoneVectorBase(ZoneVectorBase&& other) noexcept
    : _data(other._data),
      _size(other._size),
      _capacity(other._capacity) {}

  //! \}

  //! \cond INTERNAL
  //! \name Internal
  //! \{

  inline void _release(ZoneAllocator* allocator, uint32_t sizeOfT) noexcept {
    if (_data != nullptr) {
      allocator->release(_data, _capacity * sizeOfT);
      reset();
    }
  }

  ASMJIT_API Error _grow(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept;
  ASMJIT_API Error _resize(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept;
  ASMJIT_API Error _reserve(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept;
  ASMJIT_API Error _growingReserve(ZoneAllocator* allocator, uint32_t sizeOfT, uint32_t n) noexcept;

  inline void _swap(ZoneVectorBase& other) noexcept {
    std::swap(_data, other._data);
    std::swap(_size, other._size);
    std::swap(_capacity, other._capacity);
  }

  //! \}
  //! \endcond

public:
  //! \name Accessors
  //! \{

  //! Tests whether the vector is empty.
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _size == 0; }
  //! Returns the vector size.
  ASMJIT_INLINE_NODEBUG size_type size() const noexcept { return _size; }
  //! Returns the vector capacity.
  ASMJIT_INLINE_NODEBUG size_type capacity() const noexcept { return _capacity; }

  //! \}

  //! \name Utilities
  //! \{

  //! Makes the vector empty (won't change the capacity or data pointer).
  ASMJIT_INLINE_NODEBUG void clear() noexcept { _size = 0; }
  //! Resets the vector data and set its `size` to zero.
  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _data = nullptr;
    _size = 0;
    _capacity = 0;
  }

  //! Truncates the vector to at most `n` items.
  ASMJIT_INLINE_NODEBUG void truncate(size_type n) noexcept {
    _size = Support::min(_size, n);
  }

  //! Sets size of the vector to `n`. Used internally by some algorithms.
  inline void _setSize(size_type n) noexcept {
    ASMJIT_ASSERT(n <= _capacity);
    _size = n;
  }

  //! \}
};

//! Template used to store and manage array of Zone allocated data.
//!
//! This template has these advantages over other std::vector<>:
//! - Always non-copyable (designed to be non-copyable, we want it).
//! - Optimized for working only with POD types.
//! - Uses ZoneAllocator, thus small vectors are almost for free.
//! - Explicit allocation, ZoneAllocator is not part of the data.
template <typename T>
class ZoneVector : public ZoneVectorBase {
public:
  ASMJIT_NONCOPYABLE(ZoneVector)

  // STL compatibility;
  typedef T value_type;
  typedef T* pointer;
  typedef const T* const_pointer;
  typedef T& reference;
  typedef const T& const_reference;

  typedef T* iterator;
  typedef const T* const_iterator;
  typedef Support::ArrayReverseIterator<T> reverse_iterator;
  typedef Support::ArrayReverseIterator<const T> const_reverse_iterator;

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneVector() noexcept : ZoneVectorBase() {}
  ASMJIT_INLINE_NODEBUG ZoneVector(ZoneVector&& other) noexcept : ZoneVector(other) {}

  //! \}

  //! \name Accessors
  //! \{

  //! Returns vector data.
  ASMJIT_INLINE_NODEBUG T* data() noexcept { return static_cast<T*>(_data); }
  //! Returns vector data (const)
  ASMJIT_INLINE_NODEBUG const T* data() const noexcept { return static_cast<const T*>(_data); }

  //! Returns item at the given index `i` (const).
  inline const T& at(size_t i) const noexcept {
    ASMJIT_ASSERT(i < _size);
    return data()[i];
  }

  inline void _setEndPtr(T* p) noexcept {
    ASMJIT_ASSERT(p >= data() && p <= data() + _capacity);
    _setSize(uint32_t((uintptr_t)(p - data())));
  }

  //! \}

  //! \name STL Compatibility (Iterators)
  //! \{

  ASMJIT_INLINE_NODEBUG iterator begin() noexcept { return iterator(data()); };
  ASMJIT_INLINE_NODEBUG const_iterator begin() const noexcept { return const_iterator(data()); };

  ASMJIT_INLINE_NODEBUG iterator end() noexcept { return iterator(data() + _size); };
  ASMJIT_INLINE_NODEBUG const_iterator end() const noexcept { return const_iterator(data() + _size); };

  ASMJIT_INLINE_NODEBUG reverse_iterator rbegin() noexcept { return reverse_iterator(end()); };
  ASMJIT_INLINE_NODEBUG const_reverse_iterator rbegin() const noexcept { return const_reverse_iterator(end()); };

  ASMJIT_INLINE_NODEBUG reverse_iterator rend() noexcept { return reverse_iterator(begin()); };
  ASMJIT_INLINE_NODEBUG const_reverse_iterator rend() const noexcept { return const_reverse_iterator(begin()); };

  ASMJIT_INLINE_NODEBUG const_iterator cbegin() const noexcept { return const_iterator(data()); };
  ASMJIT_INLINE_NODEBUG const_iterator cend() const noexcept { return const_iterator(data() + _size); };

  ASMJIT_INLINE_NODEBUG const_reverse_iterator crbegin() const noexcept { return const_reverse_iterator(cend()); };
  ASMJIT_INLINE_NODEBUG const_reverse_iterator crend() const noexcept { return const_reverse_iterator(cbegin()); };

  //! \}

  //! \name Utilities
  //! \{

  //! Swaps this vector with `other`.
  ASMJIT_FORCE_INLINE void swap(ZoneVector<T>& other) noexcept { _swap(other); }

  //! Prepends `item` to the vector.
  ASMJIT_FORCE_INLINE Error prepend(ZoneAllocator* allocator, const T& item) noexcept {
    if (ASMJIT_UNLIKELY(_size == _capacity))
      ASMJIT_PROPAGATE(grow(allocator, 1));

    memmove(static_cast<void*>(static_cast<T*>(_data) + 1),
            static_cast<const void*>(_data),
            size_t(_size) * sizeof(T));

    memcpy(static_cast<void*>(_data),
           static_cast<const void*>(&item),
           sizeof(T));

    _size++;
    return kErrorOk;
  }

  //! Inserts an `item` at the specified `index`.
  ASMJIT_FORCE_INLINE Error insert(ZoneAllocator* allocator, size_t index, const T& item) noexcept {
    ASMJIT_ASSERT(index <= _size);

    if (ASMJIT_UNLIKELY(_size == _capacity))
      ASMJIT_PROPAGATE(grow(allocator, 1));

    T* dst = static_cast<T*>(_data) + index;
    memmove(static_cast<void*>(dst + 1),
            static_cast<const void*>(dst),
            size_t(_size - index) * sizeof(T));

    memcpy(static_cast<void*>(dst),
           static_cast<const void*>(&item),
           sizeof(T));

    _size++;
    return kErrorOk;
  }

  //! Appends `item` to the vector.
  ASMJIT_FORCE_INLINE Error append(ZoneAllocator* allocator, const T& item) noexcept {
    if (ASMJIT_UNLIKELY(_size == _capacity))
      ASMJIT_PROPAGATE(grow(allocator, 1));

    memcpy(static_cast<void*>(static_cast<T*>(_data) + _size),
           static_cast<const void*>(&item),
           sizeof(T));

    _size++;
    return kErrorOk;
  }

  //! Appends `other` vector at the end of this vector.
  ASMJIT_FORCE_INLINE Error concat(ZoneAllocator* allocator, const ZoneVector<T>& other) noexcept {
    uint32_t size = other._size;
    if (_capacity - _size < size)
      ASMJIT_PROPAGATE(grow(allocator, size));

    if (size) {
      memcpy(static_cast<void*>(static_cast<T*>(_data) + _size),
             static_cast<const void*>(other._data),
             size_t(size) * sizeof(T));
      _size += size;
    }

    return kErrorOk;
  }

  //! Prepends `item` to the vector (unsafe case).
  //!
  //! Can only be used together with `willGrow()`. If `willGrow(N)` returns `kErrorOk` then N elements
  //! can be added to the vector without checking if there is a place for them. Used mostly internally.
  ASMJIT_FORCE_INLINE void prependUnsafe(const T& item) noexcept {
    ASMJIT_ASSERT(_size < _capacity);
    T* data = static_cast<T*>(_data);

    if (_size) {
      memmove(static_cast<void*>(data + 1),
              static_cast<const void*>(data),
              size_t(_size) * sizeof(T));
    }

    memcpy(static_cast<void*>(data),
           static_cast<const void*>(&item),
           sizeof(T));
    _size++;
  }

  //! Append s`item` to the vector (unsafe case).
  //!
  //! Can only be used together with `willGrow()`. If `willGrow(N)` returns `kErrorOk` then N elements
  //! can be added to the vector without checking if there is a place for them. Used mostly internally.
  ASMJIT_FORCE_INLINE void appendUnsafe(const T& item) noexcept {
    ASMJIT_ASSERT(_size < _capacity);

    memcpy(static_cast<void*>(static_cast<T*>(_data) + _size),
           static_cast<const void*>(&item),
           sizeof(T));
    _size++;
  }

  //! Inserts an `item` at the specified `index` (unsafe case).
  ASMJIT_FORCE_INLINE void insertUnsafe(size_t index, const T& item) noexcept {
    ASMJIT_ASSERT(_size < _capacity);
    ASMJIT_ASSERT(index <= _size);

    T* dst = static_cast<T*>(_data) + index;
    memmove(static_cast<void*>(dst + 1),
            static_cast<const void*>(dst),
            size_t(_size - index) * sizeof(T));

    memcpy(static_cast<void*>(dst),
           static_cast<const void*>(&item),
           sizeof(T));

    _size++;
  }

  //! Concatenates all items of `other` at the end of the vector.
  ASMJIT_FORCE_INLINE void concatUnsafe(const ZoneVector<T>& other) noexcept {
    uint32_t size = other._size;
    ASMJIT_ASSERT(_capacity - _size >= size);

    if (size) {
      memcpy(static_cast<void*>(static_cast<T*>(_data) + _size),
             static_cast<const void*>(other._data),
             size_t(size) * sizeof(T));
      _size += size;
    }
  }

  //! Returns index of the given `val` or `Globals::kNotFound` if it doesn't exist.
  ASMJIT_FORCE_INLINE uint32_t indexOf(const T& val) const noexcept {
    const T* data = static_cast<const T*>(_data);
    uint32_t size = _size;

    for (uint32_t i = 0; i < size; i++)
      if (data[i] == val)
        return i;
    return Globals::kNotFound;
  }

  //! Tests whether the vector contains `val`.
  inline bool contains(const T& val) const noexcept {
    return indexOf(val) != Globals::kNotFound;
  }

  //! Removes item at index `i`.
  inline void removeAt(size_t i) noexcept {
    ASMJIT_ASSERT(i < _size);

    T* data = static_cast<T*>(_data) + i;
    size_t size = --_size - i;

    if (size) {
      memmove(static_cast<void*>(data),
              static_cast<const void*>(data + 1),
              size_t(size) * sizeof(T));
    }
  }

  //! Pops the last element from the vector and returns it.
  inline T pop() noexcept {
    ASMJIT_ASSERT(_size > 0);

    uint32_t index = --_size;
    return data()[index];
  }

  template<typename CompareT = Support::Compare<Support::SortOrder::kAscending>>
  inline void sort(const CompareT& cmp = CompareT()) noexcept {
    Support::qSort<T, CompareT>(data(), size(), cmp);
  }

  //! Returns item at index `i`.
  inline T& operator[](size_t i) noexcept {
    ASMJIT_ASSERT(i < _size);
    return data()[i];
  }

  //! Returns item at index `i`.
  inline const T& operator[](size_t i) const noexcept {
    ASMJIT_ASSERT(i < _size);
    return data()[i];
  }

  //! Returns a reference to the first element of the vector.
  //!
  //! \note The vector must have at least one element. Attempting to use `first()` on empty vector will trigger
  //! an assertion failure in debug builds.
  ASMJIT_INLINE_NODEBUG T& first() noexcept { return operator[](0); }
  //! \overload
  ASMJIT_INLINE_NODEBUG const T& first() const noexcept { return operator[](0); }

  //! Returns a reference to the last element of the vector.
  //!
  //! \note The vector must have at least one element. Attempting to use `last()` on empty vector will trigger
  //! an assertion failure in debug builds.
  inline T& last() noexcept { return operator[](_size - 1); }
  //! \overload
  inline const T& last() const noexcept { return operator[](_size - 1); }

  //! \}

  //! \name Memory Management
  //! \{

  //! Releases the memory held by `ZoneVector<T>` back to the `allocator`.
  inline void release(ZoneAllocator* allocator) noexcept {
    _release(allocator, sizeof(T));
  }

  //! Called to grow the buffer to fit at least `n` elements more.
  inline Error grow(ZoneAllocator* allocator, uint32_t n) noexcept {
    return ZoneVectorBase::_grow(allocator, sizeof(T), n);
  }

  //! Resizes the vector to hold `n` elements.
  //!
  //! If `n` is greater than the current size then the additional elements' content will be initialized to zero.
  //! If `n` is less than the current size then the vector will be truncated to exactly `n` elements.
  inline Error resize(ZoneAllocator* allocator, uint32_t n) noexcept {
    return ZoneVectorBase::_resize(allocator, sizeof(T), n);
  }

  //! Reallocates the internal array to fit at least `n` items.
  inline Error reserve(ZoneAllocator* allocator, uint32_t n) noexcept {
    if (ASMJIT_UNLIKELY(n > _capacity))
      return ZoneVectorBase::_reserve(allocator, sizeof(T), n);
    else
      return Error(kErrorOk);
  }

  //! Reallocates the internal array to fit at least `n` items with growing semantics.
  //!
  //! If the vector is smaller than `n` the same growing calculations will be used as if N items were appended
  //! to an empty vector, which means reserving additional space for more append operations that could follow.
  inline Error growingReserve(ZoneAllocator* allocator, uint32_t n) noexcept {
    if (ASMJIT_UNLIKELY(n > _capacity))
      return ZoneVectorBase::_growingReserve(allocator, sizeof(T), n);
    else
      return Error(kErrorOk);
  }

  inline Error willGrow(ZoneAllocator* allocator, uint32_t n = 1) noexcept {
    return _capacity - _size < n ? grow(allocator, n) : Error(kErrorOk);
  }

  //! \}
};

//! Zone-allocated bit vector.
class ZoneBitVector {
public:
  typedef Support::BitWord BitWord;

  ASMJIT_NONCOPYABLE(ZoneBitVector)

  //! \name Constants
  //! \{

  enum : uint32_t {
    kBitWordSizeInBits = Support::kBitWordSizeInBits
  };

  //! \}

  //! \name Members
  //! \{

  //! Bits.
  BitWord* _data = nullptr;
  //! Size of the bit-vector (in bits).
  uint32_t _size = 0;
  //! Capacity of the bit-vector (in bits).
  uint32_t _capacity = 0;

  //! \}

  //! \cond INTERNAL
  //! \name Internal
  //! \{

  static ASMJIT_INLINE_NODEBUG uint32_t _wordsPerBits(uint32_t nBits) noexcept {
    return ((nBits + kBitWordSizeInBits - 1) / kBitWordSizeInBits);
  }

  static ASMJIT_INLINE_NODEBUG void _zeroBits(BitWord* dst, uint32_t nBitWords) noexcept {
    for (uint32_t i = 0; i < nBitWords; i++)
      dst[i] = 0;
  }

  static ASMJIT_INLINE_NODEBUG void _fillBits(BitWord* dst, uint32_t nBitWords) noexcept {
    for (uint32_t i = 0; i < nBitWords; i++)
      dst[i] = ~BitWord(0);
  }

  static ASMJIT_INLINE_NODEBUG void _copyBits(BitWord* dst, const BitWord* src, uint32_t nBitWords) noexcept {
    for (uint32_t i = 0; i < nBitWords; i++)
      dst[i] = src[i];
  }

  //! \}
  //! \endcond

  //! \name Construction & Destruction
  //! \{

  ASMJIT_INLINE_NODEBUG ZoneBitVector() noexcept {}

  ASMJIT_INLINE_NODEBUG ZoneBitVector(ZoneBitVector&& other) noexcept
    : _data(other._data),
      _size(other._size),
      _capacity(other._capacity) {}

  //! \}

  //! \name Overloaded Operators
  //! \{

  ASMJIT_INLINE_NODEBUG bool operator==(const ZoneBitVector& other) const noexcept { return  equals(other); }
  ASMJIT_INLINE_NODEBUG bool operator!=(const ZoneBitVector& other) const noexcept { return !equals(other); }

  //! \}

  //! \name Accessors
  //! \{

  //! Tests whether the bit-vector is empty (has no bits).
  ASMJIT_INLINE_NODEBUG bool empty() const noexcept { return _size == 0; }
  //! Returns the size of this bit-vector (in bits).
  ASMJIT_INLINE_NODEBUG uint32_t size() const noexcept { return _size; }
  //! Returns the capacity of this bit-vector (in bits).
  ASMJIT_INLINE_NODEBUG uint32_t capacity() const noexcept { return _capacity; }

  //! Returns the size of the `BitWord[]` array in `BitWord` units.
  ASMJIT_INLINE_NODEBUG uint32_t sizeInBitWords() const noexcept { return _wordsPerBits(_size); }
  //! Returns the capacity of the `BitWord[]` array in `BitWord` units.
  ASMJIT_INLINE_NODEBUG uint32_t capacityInBitWords() const noexcept { return _wordsPerBits(_capacity); }

  //! Returns bit-vector data as `BitWord[]`.
  ASMJIT_INLINE_NODEBUG BitWord* data() noexcept { return _data; }
  //! \overload
  ASMJIT_INLINE_NODEBUG const BitWord* data() const noexcept { return _data; }

  //! \}

  //! \name Utilities
  //! \{

  ASMJIT_INLINE_NODEBUG void swap(ZoneBitVector& other) noexcept {
    std::swap(_data, other._data);
    std::swap(_size, other._size);
    std::swap(_capacity, other._capacity);
  }

  ASMJIT_INLINE_NODEBUG void clear() noexcept {
    _size = 0;
  }

  ASMJIT_INLINE_NODEBUG void reset() noexcept {
    _data = nullptr;
    _size = 0;
    _capacity = 0;
  }

  ASMJIT_INLINE_NODEBUG void truncate(uint32_t newSize) noexcept {
    _size = Support::min(_size, newSize);
    _clearUnusedBits();
  }

  inline bool bitAt(uint32_t index) const noexcept {
    ASMJIT_ASSERT(index < _size);
    return Support::bitVectorGetBit(_data, index);
  }

  inline void setBit(uint32_t index, bool value) noexcept {
    ASMJIT_ASSERT(index < _size);
    Support::bitVectorSetBit(_data, index, value);
  }

  inline void flipBit(uint32_t index) noexcept {
    ASMJIT_ASSERT(index < _size);
    Support::bitVectorFlipBit(_data, index);
  }

  ASMJIT_FORCE_INLINE Error append(ZoneAllocator* allocator, bool value) noexcept {
    uint32_t index = _size;
    if (ASMJIT_UNLIKELY(index >= _capacity))
      return _append(allocator, value);

    uint32_t idx = index / kBitWordSizeInBits;
    uint32_t bit = index % kBitWordSizeInBits;

    if (bit == 0)
      _data[idx] = BitWord(value) << bit;
    else
      _data[idx] |= BitWord(value) << bit;

    _size++;
    return kErrorOk;
  }

  ASMJIT_API Error copyFrom(ZoneAllocator* allocator, const ZoneBitVector& other) noexcept;

  ASMJIT_FORCE_INLINE void clearAll() noexcept {
    _zeroBits(_data, _wordsPerBits(_size));
  }

  ASMJIT_FORCE_INLINE void fillAll() noexcept {
    _fillBits(_data, _wordsPerBits(_size));
    _clearUnusedBits();
  }

  ASMJIT_FORCE_INLINE void clearBits(uint32_t start, uint32_t count) noexcept {
    ASMJIT_ASSERT(start <= _size);
    ASMJIT_ASSERT(_size - start >= count);

    Support::bitVectorClear(_data, start, count);
  }

  ASMJIT_FORCE_INLINE void fillBits(uint32_t start, uint32_t count) noexcept {
    ASMJIT_ASSERT(start <= _size);
    ASMJIT_ASSERT(_size - start >= count);

    Support::bitVectorFill(_data, start, count);
  }

  //! Performs a logical bitwise AND between bits specified in this array and bits in `other`. If `other` has less
  //! bits than `this` then all remaining bits are set to zero.
  //!
  //! \note The size of the BitVector is unaffected by this operation.
  ASMJIT_FORCE_INLINE void and_(const ZoneBitVector& other) noexcept {
    BitWord* dst = _data;
    const BitWord* src = other._data;

    uint32_t thisBitWordCount = sizeInBitWords();
    uint32_t otherBitWordCount = other.sizeInBitWords();
    uint32_t commonBitWordCount = Support::min(thisBitWordCount, otherBitWordCount);

    uint32_t i = 0;
    while (i < commonBitWordCount) {
      dst[i] = dst[i] & src[i];
      i++;
    }

    while (i < thisBitWordCount) {
      dst[i] = 0;
      i++;
    }
  }

  //! Performs a logical bitwise AND between bits specified in this array and negated bits in `other`. If `other`
  //! has less bits than `this` then all remaining bits are kept intact.
  //!
  //! \note The size of the BitVector is unaffected by this operation.
  ASMJIT_FORCE_INLINE void andNot(const ZoneBitVector& other) noexcept {
    BitWord* dst = _data;
    const BitWord* src = other._data;

    uint32_t commonBitWordCount = _wordsPerBits(Support::min(_size, other._size));
    for (uint32_t i = 0; i < commonBitWordCount; i++)
      dst[i] = dst[i] & ~src[i];
  }

  //! Performs a logical bitwise OP between bits specified in this array and bits in `other`. If `other` has less
  //! bits than `this` then all remaining bits are kept intact.
  //!
  //! \note The size of the BitVector is unaffected by this operation.
  ASMJIT_FORCE_INLINE void or_(const ZoneBitVector& other) noexcept {
    BitWord* dst = _data;
    const BitWord* src = other._data;

    uint32_t commonBitWordCount = _wordsPerBits(Support::min(_size, other._size));
    for (uint32_t i = 0; i < commonBitWordCount; i++)
      dst[i] = dst[i] | src[i];
    _clearUnusedBits();
  }

  ASMJIT_FORCE_INLINE void _clearUnusedBits() noexcept {
    uint32_t idx = _size / kBitWordSizeInBits;
    uint32_t bit = _size % kBitWordSizeInBits;

    if (!bit)
      return;
    _data[idx] &= (BitWord(1) << bit) - 1u;
  }

  ASMJIT_FORCE_INLINE bool equals(const ZoneBitVector& other) const noexcept {
    if (_size != other._size)
      return false;

    const BitWord* aData = _data;
    const BitWord* bData = other._data;
    uint32_t numBitWords = _wordsPerBits(_size);

    for (uint32_t i = 0; i < numBitWords; i++)
      if (aData[i] != bData[i])
        return false;
    return true;
  }

  //! \}

  //! \name Memory Management
  //! \{

  inline void release(ZoneAllocator* allocator) noexcept {
    if (!_data)
      return;
    allocator->release(_data, _capacity / 8);
    reset();
  }

  ASMJIT_INLINE_NODEBUG Error resize(ZoneAllocator* allocator, uint32_t newSize, bool newBitsValue = false) noexcept {
    return _resize(allocator, newSize, newSize, newBitsValue);
  }

  ASMJIT_API Error _resize(ZoneAllocator* allocator, uint32_t newSize, uint32_t idealCapacity, bool newBitsValue) noexcept;
  ASMJIT_API Error _append(ZoneAllocator* allocator, bool value) noexcept;

  //! \}

  //! \name Iterators
  //! \{

  class ForEachBitSet : public Support::BitVectorIterator<BitWord> {
  public:
    inline explicit ForEachBitSet(const ZoneBitVector& bitVector) noexcept
      : Support::BitVectorIterator<BitWord>(bitVector.data(), bitVector.sizeInBitWords()) {}
  };

  template<class Operator>
  class ForEachBitOp : public Support::BitVectorOpIterator<BitWord, Operator> {
  public:
    inline ForEachBitOp(const ZoneBitVector& a, const ZoneBitVector& b) noexcept
      : Support::BitVectorOpIterator<BitWord, Operator>(a.data(), b.data(), a.sizeInBitWords()) {
      ASMJIT_ASSERT(a.size() == b.size());
    }
  };

  //! \}
};

//! \}

ASMJIT_END_NAMESPACE

#endif // ASMJIT_CORE_ZONEVECTOR_H_INCLUDED

```

`CodeCleaner/asmjit/asmjit/x86.h`:

```h
// This file is part of AsmJit project <https://asmjit.com>
//
// See asmjit.h or LICENSE.md for license and copyright information
// SPDX-License-Identifier: Zlib

#ifndef ASMJIT_X86_H_INCLUDED
#define ASMJIT_X86_H_INCLUDED

//! \addtogroup asmjit_x86
//!
//! ### Namespace
//!
//!   - \ref x86 - x86 namespace provides support for X86/X64 code generation.
//!
//! ### Emitters
//!
//!   - \ref x86::Assembler - X86/X64 assembler (must read, provides examples).
//!   - \ref x86::Builder - X86/X64 builder.
//!   - \ref x86::Compiler - X86/X64 compiler.
//!   - \ref x86::Emitter - X86/X64 emitter (abstract).
//!
//! ### Supported Instructions
//!
//!   - Emitters:
//!     - \ref x86::EmitterExplicitT - Provides all instructions that use explicit operands, provides also utility
//!       functions. The member functions provided are part of all X86 emitters.
//!     - \ref x86::EmitterImplicitT - Provides all instructions that use implicit operands, these cannot be used
//!       with \ref x86::Compiler.
//!
//!   - Instruction representation:
//!     - \ref x86::Inst::Id - Provides instruction identifiers for both X86/X86_64 architectures.
//!     - \ref InstOptions - Provides generic and X86/X86_64 specific options.
//!
//! ### Register Operands
//!
//!   - \ref x86::Reg - Base class for any X86 register.
//!     - \ref x86::Gp - General purpose register:
//!       - \ref x86::GpbLo - 8-bit low register.
//!       - \ref x86::GpbHi - 8-bit high register.
//!       - \ref x86::Gpw - 16-bit register.
//!       - \ref x86::Gpd - 32-bit register.
//!       - \ref x86::Gpq - 64-bit register (X64 only).
//!     - \ref x86::Vec - Vector (SIMD) register:
//!       - \ref x86::Xmm - 128-bit SIMD register (SSE+).
//!       - \ref x86::Ymm - 256-bit SIMD register (AVX+).
//!       - \ref x86::Zmm - 512-bit SIMD register (AVX512+).
//!     - \ref x86::Mm - 64-bit MMX register.
//!     - \ref x86::St - 80-bit FPU register.
//!     - \ref x86::KReg - opmask registers (AVX512+).
//!     - \ref x86::SReg - segment register.
//!     - \ref x86::CReg - control register.
//!     - \ref x86::DReg - debug register.
//!     - \ref x86::Bnd - bound register (discontinued).
//!     - \ref x86::Rip - relative instruction pointer.
//!
//! ### Memory Operands
//!
//!   - \ref x86::Mem - X86/X64 memory operand that provides support for all X86 and X64 addressing features
//!     including absolute addresses, index scales, and segment override prefixes.
//!
//! ### Status and Control Words
//!
//!   - \ref x86::FpuStatusWord - FPU status word bits / decomposition.
//!   - \ref x86::FpuControlWord - FPU control word bits / decomposition.
//!
//! ### Predicates (immediate values)
//!
//!   - \ref x86::CmpImm - `CMP[PD|PS|SD|SS]` predicate (SSE+).
//!   - \ref x86::PCmpStrImm - `[V]PCMP[I|E]STR[I|M]` predicate (SSE4.1+, AVX+).
//!   - \ref x86::RoundImm - `[V]ROUND[PD|PS|SD|SS]` predicate (SSE+, AVX+).
//!   - \ref x86::VCmpImm - `VCMP[PD|PS|SD|SS]` predicate (AVX+).
//!   - \ref x86::VFixupImm - `VFIXUPIMM[PD|PS|SD|SS]` predicate (AVX512+).
//!   - \ref x86::VFPClassImm - `VFPCLASS[PD|PS|SD|SS]` predicate (AVX512+).
//!   - \ref x86::VGetMantImm - `VGETMANT[PD|PS|SD|SS]` predicate (AVX512+).
//!   - \ref x86::VPCmpImm - `VPCMP[U][B|W|D|Q]` predicate (AVX512+).
//!   - \ref x86::VPComImm - `VPCOM[U][B|W|D|Q]` predicate (XOP).
//!   - \ref x86::VRangeImm - `VRANGE[PD|PS|SD|SS]` predicate (AVX512+).
//!   - \ref x86::VReduceImm - `REDUCE[PD|PS|SD|SS]` predicate (AVX512+).
//!   - \ref x86::TLogImm - `VPTERNLOG[D|Q]` predicate and operations (AVX512+).

#include "core.h"

#include "asmjit-scope-begin.h"
#include "x86/x86assembler.h"
#include "x86/x86builder.h"
#include "x86/x86compiler.h"
#include "x86/x86emitter.h"
#include "x86/x86globals.h"
#include "x86/x86instdb.h"
#include "x86/x86operand.h"
#include "asmjit-scope-end.h"

#endif // ASMJIT_X86_H_INCLUDED

```

`CodeCleaner/capstone/include/capstone/arm.h`:

```h
#ifndef CAPSTONE_ARM_H
#define CAPSTONE_ARM_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2013-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// ARM shift type
typedef enum arm_shifter {
	ARM_SFT_INVALID = 0,
	ARM_SFT_ASR,	///< shift with immediate const
	ARM_SFT_LSL,	///< shift with immediate const
	ARM_SFT_LSR,	///< shift with immediate const
	ARM_SFT_ROR,	///< shift with immediate const
	ARM_SFT_RRX,	///< shift with immediate const
	ARM_SFT_ASR_REG,	///< shift with register
	ARM_SFT_LSL_REG,	///< shift with register
	ARM_SFT_LSR_REG,	///< shift with register
	ARM_SFT_ROR_REG,	///< shift with register
	ARM_SFT_RRX_REG,	///< shift with register
} arm_shifter;

/// ARM condition code
typedef enum arm_cc {
	ARM_CC_INVALID = 0,
	ARM_CC_EQ,            ///< Equal                      Equal
	ARM_CC_NE,            ///< Not equal                  Not equal, or unordered
	ARM_CC_HS,            ///< Carry set                  >, ==, or unordered
	ARM_CC_LO,            ///< Carry clear                Less than
	ARM_CC_MI,            ///< Minus, negative            Less than
	ARM_CC_PL,            ///< Plus, positive or zero     >, ==, or unordered
	ARM_CC_VS,            ///< Overflow                   Unordered
	ARM_CC_VC,            ///< No overflow                Not unordered
	ARM_CC_HI,            ///< Unsigned higher            Greater than, or unordered
	ARM_CC_LS,            ///< Unsigned lower or same     Less than or equal
	ARM_CC_GE,            ///< Greater than or equal      Greater than or equal
	ARM_CC_LT,            ///< Less than                  Less than, or unordered
	ARM_CC_GT,            ///< Greater than               Greater than
	ARM_CC_LE,            ///< Less than or equal         <, ==, or unordered
	ARM_CC_AL             ///< Always (unconditional)     Always (unconditional)
} arm_cc;

typedef enum arm_sysreg {
	/// Special registers for MSR
	ARM_SYSREG_INVALID = 0,

	// SPSR* registers can be OR combined
	ARM_SYSREG_SPSR_C = 1,
	ARM_SYSREG_SPSR_X = 2,
	ARM_SYSREG_SPSR_S = 4,
	ARM_SYSREG_SPSR_F = 8,

	// CPSR* registers can be OR combined
	ARM_SYSREG_CPSR_C = 16,
	ARM_SYSREG_CPSR_X = 32,
	ARM_SYSREG_CPSR_S = 64,
	ARM_SYSREG_CPSR_F = 128,

	// independent registers
	ARM_SYSREG_APSR = 256,
	ARM_SYSREG_APSR_G,
	ARM_SYSREG_APSR_NZCVQ,
	ARM_SYSREG_APSR_NZCVQG,

	ARM_SYSREG_IAPSR,
	ARM_SYSREG_IAPSR_G,
	ARM_SYSREG_IAPSR_NZCVQG,
	ARM_SYSREG_IAPSR_NZCVQ,

	ARM_SYSREG_EAPSR,
	ARM_SYSREG_EAPSR_G,
	ARM_SYSREG_EAPSR_NZCVQG,
	ARM_SYSREG_EAPSR_NZCVQ,

	ARM_SYSREG_XPSR,
	ARM_SYSREG_XPSR_G,
	ARM_SYSREG_XPSR_NZCVQG,
	ARM_SYSREG_XPSR_NZCVQ,

	ARM_SYSREG_IPSR,
	ARM_SYSREG_EPSR,
	ARM_SYSREG_IEPSR,

	ARM_SYSREG_MSP,
	ARM_SYSREG_PSP,
	ARM_SYSREG_PRIMASK,
	ARM_SYSREG_BASEPRI,
	ARM_SYSREG_BASEPRI_MAX,
	ARM_SYSREG_FAULTMASK,
	ARM_SYSREG_CONTROL,

	// Banked Registers
	ARM_SYSREG_R8_USR,
	ARM_SYSREG_R9_USR,
	ARM_SYSREG_R10_USR,
	ARM_SYSREG_R11_USR,
	ARM_SYSREG_R12_USR,
	ARM_SYSREG_SP_USR,
	ARM_SYSREG_LR_USR,
	ARM_SYSREG_R8_FIQ,
	ARM_SYSREG_R9_FIQ,
	ARM_SYSREG_R10_FIQ,
	ARM_SYSREG_R11_FIQ,
	ARM_SYSREG_R12_FIQ,
	ARM_SYSREG_SP_FIQ,
	ARM_SYSREG_LR_FIQ,
	ARM_SYSREG_LR_IRQ,
	ARM_SYSREG_SP_IRQ,
	ARM_SYSREG_LR_SVC,
	ARM_SYSREG_SP_SVC,
	ARM_SYSREG_LR_ABT,
	ARM_SYSREG_SP_ABT,
	ARM_SYSREG_LR_UND,
	ARM_SYSREG_SP_UND,
	ARM_SYSREG_LR_MON,
	ARM_SYSREG_SP_MON,
	ARM_SYSREG_ELR_HYP,
	ARM_SYSREG_SP_HYP,

	ARM_SYSREG_SPSR_FIQ,
	ARM_SYSREG_SPSR_IRQ,
	ARM_SYSREG_SPSR_SVC,
	ARM_SYSREG_SPSR_ABT,
	ARM_SYSREG_SPSR_UND,
	ARM_SYSREG_SPSR_MON,
	ARM_SYSREG_SPSR_HYP,
} arm_sysreg;

/// The memory barrier constants map directly to the 4-bit encoding of
/// the option field for Memory Barrier operations.
typedef enum arm_mem_barrier {
	ARM_MB_INVALID = 0,
	ARM_MB_RESERVED_0,
	ARM_MB_OSHLD,
	ARM_MB_OSHST,
	ARM_MB_OSH,
	ARM_MB_RESERVED_4,
	ARM_MB_NSHLD,
	ARM_MB_NSHST,
	ARM_MB_NSH,
	ARM_MB_RESERVED_8,
	ARM_MB_ISHLD,
	ARM_MB_ISHST,
	ARM_MB_ISH,
	ARM_MB_RESERVED_12,
	ARM_MB_LD,
	ARM_MB_ST,
	ARM_MB_SY,
} arm_mem_barrier;

/// Operand type for instruction's operands
typedef enum arm_op_type {
	ARM_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	ARM_OP_REG, ///< = CS_OP_REG (Register operand).
	ARM_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	ARM_OP_MEM, ///< = CS_OP_MEM (Memory operand).
	ARM_OP_FP,  ///< = CS_OP_FP (Floating-Point operand).
	ARM_OP_CIMM = 64, ///< C-Immediate (coprocessor registers)
	ARM_OP_PIMM, ///< P-Immediate (coprocessor registers)
	ARM_OP_SETEND,	///< operand for SETEND instruction
	ARM_OP_SYSREG,	///< MSR/MRS special register operand
} arm_op_type;

/// Operand type for SETEND instruction
typedef enum arm_setend_type {
	ARM_SETEND_INVALID = 0,	///< Uninitialized.
	ARM_SETEND_BE,	///< BE operand.
	ARM_SETEND_LE, ///< LE operand
} arm_setend_type;

typedef enum arm_cpsmode_type {
	ARM_CPSMODE_INVALID = 0,
	ARM_CPSMODE_IE = 2,
	ARM_CPSMODE_ID = 3
} arm_cpsmode_type;

/// Operand type for SETEND instruction
typedef enum arm_cpsflag_type {
	ARM_CPSFLAG_INVALID = 0,
	ARM_CPSFLAG_F = 1,
	ARM_CPSFLAG_I = 2,
	ARM_CPSFLAG_A = 4,
	ARM_CPSFLAG_NONE = 16,	///< no flag
} arm_cpsflag_type;

/// Data type for elements of vector instructions.
typedef enum arm_vectordata_type {
	ARM_VECTORDATA_INVALID = 0,

	// Integer type
	ARM_VECTORDATA_I8,
	ARM_VECTORDATA_I16,
	ARM_VECTORDATA_I32,
	ARM_VECTORDATA_I64,

	// Signed integer type
	ARM_VECTORDATA_S8,
	ARM_VECTORDATA_S16,
	ARM_VECTORDATA_S32,
	ARM_VECTORDATA_S64,

	// Unsigned integer type
	ARM_VECTORDATA_U8,
	ARM_VECTORDATA_U16,
	ARM_VECTORDATA_U32,
	ARM_VECTORDATA_U64,

	// Data type for VMUL/VMULL
	ARM_VECTORDATA_P8,

	// Floating type
	ARM_VECTORDATA_F32,
	ARM_VECTORDATA_F64,

	// Convert float <-> float
	ARM_VECTORDATA_F16F64,	// f16.f64
	ARM_VECTORDATA_F64F16,	// f64.f16
	ARM_VECTORDATA_F32F16,	// f32.f16
	ARM_VECTORDATA_F16F32,	// f32.f16
	ARM_VECTORDATA_F64F32,	// f64.f32
	ARM_VECTORDATA_F32F64,	// f32.f64

	// Convert integer <-> float
	ARM_VECTORDATA_S32F32,	// s32.f32
	ARM_VECTORDATA_U32F32,	// u32.f32
	ARM_VECTORDATA_F32S32,	// f32.s32
	ARM_VECTORDATA_F32U32,	// f32.u32
	ARM_VECTORDATA_F64S16,	// f64.s16
	ARM_VECTORDATA_F32S16,	// f32.s16
	ARM_VECTORDATA_F64S32,	// f64.s32
	ARM_VECTORDATA_S16F64,	// s16.f64
	ARM_VECTORDATA_S16F32,	// s16.f64
	ARM_VECTORDATA_S32F64,	// s32.f64
	ARM_VECTORDATA_U16F64,	// u16.f64
	ARM_VECTORDATA_U16F32,	// u16.f32
	ARM_VECTORDATA_U32F64,	// u32.f64
	ARM_VECTORDATA_F64U16,	// f64.u16
	ARM_VECTORDATA_F32U16,	// f32.u16
	ARM_VECTORDATA_F64U32,	// f64.u32
} arm_vectordata_type;

/// ARM registers
typedef enum arm_reg {
	ARM_REG_INVALID = 0,
	ARM_REG_APSR,
	ARM_REG_APSR_NZCV,
	ARM_REG_CPSR,
	ARM_REG_FPEXC,
	ARM_REG_FPINST,
	ARM_REG_FPSCR,
	ARM_REG_FPSCR_NZCV,
	ARM_REG_FPSID,
	ARM_REG_ITSTATE,
	ARM_REG_LR,
	ARM_REG_PC,
	ARM_REG_SP,
	ARM_REG_SPSR,
	ARM_REG_D0,
	ARM_REG_D1,
	ARM_REG_D2,
	ARM_REG_D3,
	ARM_REG_D4,
	ARM_REG_D5,
	ARM_REG_D6,
	ARM_REG_D7,
	ARM_REG_D8,
	ARM_REG_D9,
	ARM_REG_D10,
	ARM_REG_D11,
	ARM_REG_D12,
	ARM_REG_D13,
	ARM_REG_D14,
	ARM_REG_D15,
	ARM_REG_D16,
	ARM_REG_D17,
	ARM_REG_D18,
	ARM_REG_D19,
	ARM_REG_D20,
	ARM_REG_D21,
	ARM_REG_D22,
	ARM_REG_D23,
	ARM_REG_D24,
	ARM_REG_D25,
	ARM_REG_D26,
	ARM_REG_D27,
	ARM_REG_D28,
	ARM_REG_D29,
	ARM_REG_D30,
	ARM_REG_D31,
	ARM_REG_FPINST2,
	ARM_REG_MVFR0,
	ARM_REG_MVFR1,
	ARM_REG_MVFR2,
	ARM_REG_Q0,
	ARM_REG_Q1,
	ARM_REG_Q2,
	ARM_REG_Q3,
	ARM_REG_Q4,
	ARM_REG_Q5,
	ARM_REG_Q6,
	ARM_REG_Q7,
	ARM_REG_Q8,
	ARM_REG_Q9,
	ARM_REG_Q10,
	ARM_REG_Q11,
	ARM_REG_Q12,
	ARM_REG_Q13,
	ARM_REG_Q14,
	ARM_REG_Q15,
	ARM_REG_R0,
	ARM_REG_R1,
	ARM_REG_R2,
	ARM_REG_R3,
	ARM_REG_R4,
	ARM_REG_R5,
	ARM_REG_R6,
	ARM_REG_R7,
	ARM_REG_R8,
	ARM_REG_R9,
	ARM_REG_R10,
	ARM_REG_R11,
	ARM_REG_R12,
	ARM_REG_S0,
	ARM_REG_S1,
	ARM_REG_S2,
	ARM_REG_S3,
	ARM_REG_S4,
	ARM_REG_S5,
	ARM_REG_S6,
	ARM_REG_S7,
	ARM_REG_S8,
	ARM_REG_S9,
	ARM_REG_S10,
	ARM_REG_S11,
	ARM_REG_S12,
	ARM_REG_S13,
	ARM_REG_S14,
	ARM_REG_S15,
	ARM_REG_S16,
	ARM_REG_S17,
	ARM_REG_S18,
	ARM_REG_S19,
	ARM_REG_S20,
	ARM_REG_S21,
	ARM_REG_S22,
	ARM_REG_S23,
	ARM_REG_S24,
	ARM_REG_S25,
	ARM_REG_S26,
	ARM_REG_S27,
	ARM_REG_S28,
	ARM_REG_S29,
	ARM_REG_S30,
	ARM_REG_S31,

	ARM_REG_ENDING,		// <-- mark the end of the list or registers

	// alias registers
	ARM_REG_R13 = ARM_REG_SP,
	ARM_REG_R14 = ARM_REG_LR,
	ARM_REG_R15 = ARM_REG_PC,

	ARM_REG_SB = ARM_REG_R9,
	ARM_REG_SL = ARM_REG_R10,
	ARM_REG_FP = ARM_REG_R11,
	ARM_REG_IP = ARM_REG_R12,
} arm_reg;

/// Instruction's operand referring to memory
/// This is associated with ARM_OP_MEM operand type above
typedef struct arm_op_mem {
	arm_reg base;	///< base register
	arm_reg index;	///< index register
	int scale;	///< scale for index register (can be 1, or -1)
	int disp;	///< displacement/offset value
	/// left-shift on index register, or 0 if irrelevant
	/// NOTE: this value can also be fetched via operand.shift.value
	int lshift;
} arm_op_mem;

/// Instruction operand
typedef struct cs_arm_op {
	int vector_index;	///< Vector Index for some vector operands (or -1 if irrelevant)

	struct {
		arm_shifter type;
		unsigned int value;
	} shift;

	arm_op_type type;	///< operand type

	union {
		int reg;	///< register value for REG/SYSREG operand
		int32_t imm;			///< immediate value for C-IMM, P-IMM or IMM operand
		double fp;			///< floating point value for FP operand
		arm_op_mem mem;		///< base/index/scale/disp value for MEM operand
		arm_setend_type setend; ///< SETEND instruction's operand type
	};

	/// in some instructions, an operand can be subtracted or added to
	/// the base register,
	/// if TRUE, this operand is subtracted. otherwise, it is added.
	bool subtracted;

	/// How is this operand accessed? (READ, WRITE or READ|WRITE)
	/// This field is combined of cs_ac_type.
	/// NOTE: this field is irrelevant if engine is compiled in DIET mode.
	uint8_t access;

	/// Neon lane index for NEON instructions (or -1 if irrelevant)
	int8_t neon_lane;
} cs_arm_op;

/// Instruction structure
typedef struct cs_arm {
	bool usermode;	///< User-mode registers to be loaded (for LDM/STM instructions)
	int vector_size; 	///< Scalar size for vector instructions
	arm_vectordata_type vector_data; ///< Data type for elements of vector instructions
	arm_cpsmode_type cps_mode;	///< CPS mode for CPS instruction
	arm_cpsflag_type cps_flag;	///< CPS mode for CPS instruction
	arm_cc cc;			///< conditional code for this insn
	bool update_flags;	///< does this insn update flags?
	bool writeback;		///< does this insn write-back?
	arm_mem_barrier mem_barrier;	///< Option for some memory barrier instructions

	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;

	cs_arm_op operands[36];	///< operands for this instruction.
} cs_arm;

/// ARM instruction
typedef enum arm_insn {
	ARM_INS_INVALID = 0,

	ARM_INS_ADC,
	ARM_INS_ADD,
	ARM_INS_ADR,
	ARM_INS_AESD,
	ARM_INS_AESE,
	ARM_INS_AESIMC,
	ARM_INS_AESMC,
	ARM_INS_AND,
	ARM_INS_BFC,
	ARM_INS_BFI,
	ARM_INS_BIC,
	ARM_INS_BKPT,
	ARM_INS_BL,
	ARM_INS_BLX,
	ARM_INS_BX,
	ARM_INS_BXJ,
	ARM_INS_B,
	ARM_INS_CDP,
	ARM_INS_CDP2,
	ARM_INS_CLREX,
	ARM_INS_CLZ,
	ARM_INS_CMN,
	ARM_INS_CMP,
	ARM_INS_CPS,
	ARM_INS_CRC32B,
	ARM_INS_CRC32CB,
	ARM_INS_CRC32CH,
	ARM_INS_CRC32CW,
	ARM_INS_CRC32H,
	ARM_INS_CRC32W,
	ARM_INS_DBG,
	ARM_INS_DMB,
	ARM_INS_DSB,
	ARM_INS_EOR,
	ARM_INS_ERET,
	ARM_INS_VMOV,
	ARM_INS_FLDMDBX,
	ARM_INS_FLDMIAX,
	ARM_INS_VMRS,
	ARM_INS_FSTMDBX,
	ARM_INS_FSTMIAX,
	ARM_INS_HINT,
	ARM_INS_HLT,
	ARM_INS_HVC,
	ARM_INS_ISB,
	ARM_INS_LDA,
	ARM_INS_LDAB,
	ARM_INS_LDAEX,
	ARM_INS_LDAEXB,
	ARM_INS_LDAEXD,
	ARM_INS_LDAEXH,
	ARM_INS_LDAH,
	ARM_INS_LDC2L,
	ARM_INS_LDC2,
	ARM_INS_LDCL,
	ARM_INS_LDC,
	ARM_INS_LDMDA,
	ARM_INS_LDMDB,
	ARM_INS_LDM,
	ARM_INS_LDMIB,
	ARM_INS_LDRBT,
	ARM_INS_LDRB,
	ARM_INS_LDRD,
	ARM_INS_LDREX,
	ARM_INS_LDREXB,
	ARM_INS_LDREXD,
	ARM_INS_LDREXH,
	ARM_INS_LDRH,
	ARM_INS_LDRHT,
	ARM_INS_LDRSB,
	ARM_INS_LDRSBT,
	ARM_INS_LDRSH,
	ARM_INS_LDRSHT,
	ARM_INS_LDRT,
	ARM_INS_LDR,
	ARM_INS_MCR,
	ARM_INS_MCR2,
	ARM_INS_MCRR,
	ARM_INS_MCRR2,
	ARM_INS_MLA,
	ARM_INS_MLS,
	ARM_INS_MOV,
	ARM_INS_MOVT,
	ARM_INS_MOVW,
	ARM_INS_MRC,
	ARM_INS_MRC2,
	ARM_INS_MRRC,
	ARM_INS_MRRC2,
	ARM_INS_MRS,
	ARM_INS_MSR,
	ARM_INS_MUL,
	ARM_INS_MVN,
	ARM_INS_ORR,
	ARM_INS_PKHBT,
	ARM_INS_PKHTB,
	ARM_INS_PLDW,
	ARM_INS_PLD,
	ARM_INS_PLI,
	ARM_INS_QADD,
	ARM_INS_QADD16,
	ARM_INS_QADD8,
	ARM_INS_QASX,
	ARM_INS_QDADD,
	ARM_INS_QDSUB,
	ARM_INS_QSAX,
	ARM_INS_QSUB,
	ARM_INS_QSUB16,
	ARM_INS_QSUB8,
	ARM_INS_RBIT,
	ARM_INS_REV,
	ARM_INS_REV16,
	ARM_INS_REVSH,
	ARM_INS_RFEDA,
	ARM_INS_RFEDB,
	ARM_INS_RFEIA,
	ARM_INS_RFEIB,
	ARM_INS_RSB,
	ARM_INS_RSC,
	ARM_INS_SADD16,
	ARM_INS_SADD8,
	ARM_INS_SASX,
	ARM_INS_SBC,
	ARM_INS_SBFX,
	ARM_INS_SDIV,
	ARM_INS_SEL,
	ARM_INS_SETEND,
	ARM_INS_SHA1C,
	ARM_INS_SHA1H,
	ARM_INS_SHA1M,
	ARM_INS_SHA1P,
	ARM_INS_SHA1SU0,
	ARM_INS_SHA1SU1,
	ARM_INS_SHA256H,
	ARM_INS_SHA256H2,
	ARM_INS_SHA256SU0,
	ARM_INS_SHA256SU1,
	ARM_INS_SHADD16,
	ARM_INS_SHADD8,
	ARM_INS_SHASX,
	ARM_INS_SHSAX,
	ARM_INS_SHSUB16,
	ARM_INS_SHSUB8,
	ARM_INS_SMC,
	ARM_INS_SMLABB,
	ARM_INS_SMLABT,
	ARM_INS_SMLAD,
	ARM_INS_SMLADX,
	ARM_INS_SMLAL,
	ARM_INS_SMLALBB,
	ARM_INS_SMLALBT,
	ARM_INS_SMLALD,
	ARM_INS_SMLALDX,
	ARM_INS_SMLALTB,
	ARM_INS_SMLALTT,
	ARM_INS_SMLATB,
	ARM_INS_SMLATT,
	ARM_INS_SMLAWB,
	ARM_INS_SMLAWT,
	ARM_INS_SMLSD,
	ARM_INS_SMLSDX,
	ARM_INS_SMLSLD,
	ARM_INS_SMLSLDX,
	ARM_INS_SMMLA,
	ARM_INS_SMMLAR,
	ARM_INS_SMMLS,
	ARM_INS_SMMLSR,
	ARM_INS_SMMUL,
	ARM_INS_SMMULR,
	ARM_INS_SMUAD,
	ARM_INS_SMUADX,
	ARM_INS_SMULBB,
	ARM_INS_SMULBT,
	ARM_INS_SMULL,
	ARM_INS_SMULTB,
	ARM_INS_SMULTT,
	ARM_INS_SMULWB,
	ARM_INS_SMULWT,
	ARM_INS_SMUSD,
	ARM_INS_SMUSDX,
	ARM_INS_SRSDA,
	ARM_INS_SRSDB,
	ARM_INS_SRSIA,
	ARM_INS_SRSIB,
	ARM_INS_SSAT,
	ARM_INS_SSAT16,
	ARM_INS_SSAX,
	ARM_INS_SSUB16,
	ARM_INS_SSUB8,
	ARM_INS_STC2L,
	ARM_INS_STC2,
	ARM_INS_STCL,
	ARM_INS_STC,
	ARM_INS_STL,
	ARM_INS_STLB,
	ARM_INS_STLEX,
	ARM_INS_STLEXB,
	ARM_INS_STLEXD,
	ARM_INS_STLEXH,
	ARM_INS_STLH,
	ARM_INS_STMDA,
	ARM_INS_STMDB,
	ARM_INS_STM,
	ARM_INS_STMIB,
	ARM_INS_STRBT,
	ARM_INS_STRB,
	ARM_INS_STRD,
	ARM_INS_STREX,
	ARM_INS_STREXB,
	ARM_INS_STREXD,
	ARM_INS_STREXH,
	ARM_INS_STRH,
	ARM_INS_STRHT,
	ARM_INS_STRT,
	ARM_INS_STR,
	ARM_INS_SUB,
	ARM_INS_SVC,
	ARM_INS_SWP,
	ARM_INS_SWPB,
	ARM_INS_SXTAB,
	ARM_INS_SXTAB16,
	ARM_INS_SXTAH,
	ARM_INS_SXTB,
	ARM_INS_SXTB16,
	ARM_INS_SXTH,
	ARM_INS_TEQ,
	ARM_INS_TRAP,
	ARM_INS_TST,
	ARM_INS_UADD16,
	ARM_INS_UADD8,
	ARM_INS_UASX,
	ARM_INS_UBFX,
	ARM_INS_UDF,
	ARM_INS_UDIV,
	ARM_INS_UHADD16,
	ARM_INS_UHADD8,
	ARM_INS_UHASX,
	ARM_INS_UHSAX,
	ARM_INS_UHSUB16,
	ARM_INS_UHSUB8,
	ARM_INS_UMAAL,
	ARM_INS_UMLAL,
	ARM_INS_UMULL,
	ARM_INS_UQADD16,
	ARM_INS_UQADD8,
	ARM_INS_UQASX,
	ARM_INS_UQSAX,
	ARM_INS_UQSUB16,
	ARM_INS_UQSUB8,
	ARM_INS_USAD8,
	ARM_INS_USADA8,
	ARM_INS_USAT,
	ARM_INS_USAT16,
	ARM_INS_USAX,
	ARM_INS_USUB16,
	ARM_INS_USUB8,
	ARM_INS_UXTAB,
	ARM_INS_UXTAB16,
	ARM_INS_UXTAH,
	ARM_INS_UXTB,
	ARM_INS_UXTB16,
	ARM_INS_UXTH,
	ARM_INS_VABAL,
	ARM_INS_VABA,
	ARM_INS_VABDL,
	ARM_INS_VABD,
	ARM_INS_VABS,
	ARM_INS_VACGE,
	ARM_INS_VACGT,
	ARM_INS_VADD,
	ARM_INS_VADDHN,
	ARM_INS_VADDL,
	ARM_INS_VADDW,
	ARM_INS_VAND,
	ARM_INS_VBIC,
	ARM_INS_VBIF,
	ARM_INS_VBIT,
	ARM_INS_VBSL,
	ARM_INS_VCEQ,
	ARM_INS_VCGE,
	ARM_INS_VCGT,
	ARM_INS_VCLE,
	ARM_INS_VCLS,
	ARM_INS_VCLT,
	ARM_INS_VCLZ,
	ARM_INS_VCMP,
	ARM_INS_VCMPE,
	ARM_INS_VCNT,
	ARM_INS_VCVTA,
	ARM_INS_VCVTB,
	ARM_INS_VCVT,
	ARM_INS_VCVTM,
	ARM_INS_VCVTN,
	ARM_INS_VCVTP,
	ARM_INS_VCVTT,
	ARM_INS_VDIV,
	ARM_INS_VDUP,
	ARM_INS_VEOR,
	ARM_INS_VEXT,
	ARM_INS_VFMA,
	ARM_INS_VFMS,
	ARM_INS_VFNMA,
	ARM_INS_VFNMS,
	ARM_INS_VHADD,
	ARM_INS_VHSUB,
	ARM_INS_VLD1,
	ARM_INS_VLD2,
	ARM_INS_VLD3,
	ARM_INS_VLD4,
	ARM_INS_VLDMDB,
	ARM_INS_VLDMIA,
	ARM_INS_VLDR,
	ARM_INS_VMAXNM,
	ARM_INS_VMAX,
	ARM_INS_VMINNM,
	ARM_INS_VMIN,
	ARM_INS_VMLA,
	ARM_INS_VMLAL,
	ARM_INS_VMLS,
	ARM_INS_VMLSL,
	ARM_INS_VMOVL,
	ARM_INS_VMOVN,
	ARM_INS_VMSR,
	ARM_INS_VMUL,
	ARM_INS_VMULL,
	ARM_INS_VMVN,
	ARM_INS_VNEG,
	ARM_INS_VNMLA,
	ARM_INS_VNMLS,
	ARM_INS_VNMUL,
	ARM_INS_VORN,
	ARM_INS_VORR,
	ARM_INS_VPADAL,
	ARM_INS_VPADDL,
	ARM_INS_VPADD,
	ARM_INS_VPMAX,
	ARM_INS_VPMIN,
	ARM_INS_VQABS,
	ARM_INS_VQADD,
	ARM_INS_VQDMLAL,
	ARM_INS_VQDMLSL,
	ARM_INS_VQDMULH,
	ARM_INS_VQDMULL,
	ARM_INS_VQMOVUN,
	ARM_INS_VQMOVN,
	ARM_INS_VQNEG,
	ARM_INS_VQRDMULH,
	ARM_INS_VQRSHL,
	ARM_INS_VQRSHRN,
	ARM_INS_VQRSHRUN,
	ARM_INS_VQSHL,
	ARM_INS_VQSHLU,
	ARM_INS_VQSHRN,
	ARM_INS_VQSHRUN,
	ARM_INS_VQSUB,
	ARM_INS_VRADDHN,
	ARM_INS_VRECPE,
	ARM_INS_VRECPS,
	ARM_INS_VREV16,
	ARM_INS_VREV32,
	ARM_INS_VREV64,
	ARM_INS_VRHADD,
	ARM_INS_VRINTA,
	ARM_INS_VRINTM,
	ARM_INS_VRINTN,
	ARM_INS_VRINTP,
	ARM_INS_VRINTR,
	ARM_INS_VRINTX,
	ARM_INS_VRINTZ,
	ARM_INS_VRSHL,
	ARM_INS_VRSHRN,
	ARM_INS_VRSHR,
	ARM_INS_VRSQRTE,
	ARM_INS_VRSQRTS,
	ARM_INS_VRSRA,
	ARM_INS_VRSUBHN,
	ARM_INS_VSELEQ,
	ARM_INS_VSELGE,
	ARM_INS_VSELGT,
	ARM_INS_VSELVS,
	ARM_INS_VSHLL,
	ARM_INS_VSHL,
	ARM_INS_VSHRN,
	ARM_INS_VSHR,
	ARM_INS_VSLI,
	ARM_INS_VSQRT,
	ARM_INS_VSRA,
	ARM_INS_VSRI,
	ARM_INS_VST1,
	ARM_INS_VST2,
	ARM_INS_VST3,
	ARM_INS_VST4,
	ARM_INS_VSTMDB,
	ARM_INS_VSTMIA,
	ARM_INS_VSTR,
	ARM_INS_VSUB,
	ARM_INS_VSUBHN,
	ARM_INS_VSUBL,
	ARM_INS_VSUBW,
	ARM_INS_VSWP,
	ARM_INS_VTBL,
	ARM_INS_VTBX,
	ARM_INS_VCVTR,
	ARM_INS_VTRN,
	ARM_INS_VTST,
	ARM_INS_VUZP,
	ARM_INS_VZIP,
	ARM_INS_ADDW,
	ARM_INS_ASR,
	ARM_INS_DCPS1,
	ARM_INS_DCPS2,
	ARM_INS_DCPS3,
	ARM_INS_IT,
	ARM_INS_LSL,
	ARM_INS_LSR,
	ARM_INS_ORN,
	ARM_INS_ROR,
	ARM_INS_RRX,
	ARM_INS_SUBW,
	ARM_INS_TBB,
	ARM_INS_TBH,
	ARM_INS_CBNZ,
	ARM_INS_CBZ,
	ARM_INS_POP,
	ARM_INS_PUSH,

	// special instructions
	ARM_INS_NOP,
	ARM_INS_YIELD,
	ARM_INS_WFE,
	ARM_INS_WFI,
	ARM_INS_SEV,
	ARM_INS_SEVL,
	ARM_INS_VPUSH,
	ARM_INS_VPOP,

	ARM_INS_ENDING,	// <-- mark the end of the list of instructions
} arm_insn;

/// Group of ARM instructions
typedef enum arm_insn_group {
	ARM_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	ARM_GRP_JUMP,	///< = CS_GRP_JUMP
	ARM_GRP_CALL,	///< = CS_GRP_CALL
	ARM_GRP_INT = 4, ///< = CS_GRP_INT
	ARM_GRP_PRIVILEGE = 6, ///< = CS_GRP_PRIVILEGE
	ARM_GRP_BRANCH_RELATIVE, ///< = CS_GRP_BRANCH_RELATIVE

	// Architecture-specific groups
	ARM_GRP_CRYPTO = 128,
	ARM_GRP_DATABARRIER,
	ARM_GRP_DIVIDE,
	ARM_GRP_FPARMV8,
	ARM_GRP_MULTPRO,
	ARM_GRP_NEON,
	ARM_GRP_T2EXTRACTPACK,
	ARM_GRP_THUMB2DSP,
	ARM_GRP_TRUSTZONE,
	ARM_GRP_V4T,
	ARM_GRP_V5T,
	ARM_GRP_V5TE,
	ARM_GRP_V6,
	ARM_GRP_V6T2,
	ARM_GRP_V7,
	ARM_GRP_V8,
	ARM_GRP_VFP2,
	ARM_GRP_VFP3,
	ARM_GRP_VFP4,
	ARM_GRP_ARM,
	ARM_GRP_MCLASS,
	ARM_GRP_NOTMCLASS,
	ARM_GRP_THUMB,
	ARM_GRP_THUMB1ONLY,
	ARM_GRP_THUMB2,
	ARM_GRP_PREV8,
	ARM_GRP_FPVMLX,
	ARM_GRP_MULOPS,
	ARM_GRP_CRC,
	ARM_GRP_DPVFP,
	ARM_GRP_V6M,
	ARM_GRP_VIRTUALIZATION,

	ARM_GRP_ENDING,
} arm_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/arm64.h`:

```h
#ifndef CAPSTONE_ARM64_H
#define CAPSTONE_ARM64_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2013-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// ARM64 shift type
typedef enum arm64_shifter {
	ARM64_SFT_INVALID = 0,
	ARM64_SFT_LSL = 1,
	ARM64_SFT_MSL = 2,
	ARM64_SFT_LSR = 3,
	ARM64_SFT_ASR = 4,
	ARM64_SFT_ROR = 5,
} arm64_shifter;

/// ARM64 extender type
typedef enum arm64_extender {
	ARM64_EXT_INVALID = 0,
	ARM64_EXT_UXTB = 1,
	ARM64_EXT_UXTH = 2,
	ARM64_EXT_UXTW = 3,
	ARM64_EXT_UXTX = 4,
	ARM64_EXT_SXTB = 5,
	ARM64_EXT_SXTH = 6,
	ARM64_EXT_SXTW = 7,
	ARM64_EXT_SXTX = 8,
} arm64_extender;

/// ARM64 condition code
typedef enum arm64_cc {
	ARM64_CC_INVALID = 0,
	ARM64_CC_EQ = 1,     ///< Equal
	ARM64_CC_NE = 2,     ///< Not equal:                 Not equal, or unordered
	ARM64_CC_HS = 3,     ///< Unsigned higher or same:   >, ==, or unordered
	ARM64_CC_LO = 4,     ///< Unsigned lower or same:    Less than
	ARM64_CC_MI = 5,     ///< Minus, negative:           Less than
	ARM64_CC_PL = 6,     ///< Plus, positive or zero:    >, ==, or unordered
	ARM64_CC_VS = 7,     ///< Overflow:                  Unordered
	ARM64_CC_VC = 8,     ///< No overflow:               Ordered
	ARM64_CC_HI = 9,     ///< Unsigned higher:           Greater than, or unordered
	ARM64_CC_LS = 10,     ///< Unsigned lower or same:    Less than or equal
	ARM64_CC_GE = 11,     ///< Greater than or equal:     Greater than or equal
	ARM64_CC_LT = 12,     ///< Less than:                 Less than, or unordered
	ARM64_CC_GT = 13,     ///< Signed greater than:       Greater than
	ARM64_CC_LE = 14,     ///< Signed less than or equal: <, ==, or unordered
	ARM64_CC_AL = 15,     ///< Always (unconditional):    Always (unconditional)
	ARM64_CC_NV = 16,     ///< Always (unconditional):   Always (unconditional)
	//< Note the NV exists purely to disassemble 0b1111. Execution
	//< is "always".
} arm64_cc;

/// System registers
typedef enum arm64_sysreg {
	// System registers for MRS
	ARM64_SYSREG_INVALID           = 0,
	ARM64_SYSREG_MDCCSR_EL0        = 0x9808, // 10  011  0000  0001  000
	ARM64_SYSREG_DBGDTRRX_EL0      = 0x9828, // 10  011  0000  0101  000
	ARM64_SYSREG_MDRAR_EL1         = 0x8080, // 10  000  0001  0000  000
	ARM64_SYSREG_OSLSR_EL1         = 0x808c, // 10  000  0001  0001  100
	ARM64_SYSREG_DBGAUTHSTATUS_EL1 = 0x83f6, // 10  000  0111  1110  110
	ARM64_SYSREG_PMCEID0_EL0       = 0xdce6, // 11  011  1001  1100  110
	ARM64_SYSREG_PMCEID1_EL0       = 0xdce7, // 11  011  1001  1100  111
	ARM64_SYSREG_MIDR_EL1          = 0xc000, // 11  000  0000  0000  000
	ARM64_SYSREG_CCSIDR_EL1        = 0xc800, // 11  001  0000  0000  000
	ARM64_SYSREG_CLIDR_EL1         = 0xc801, // 11  001  0000  0000  001
	ARM64_SYSREG_CTR_EL0           = 0xd801, // 11  011  0000  0000  001
	ARM64_SYSREG_MPIDR_EL1         = 0xc005, // 11  000  0000  0000  101
	ARM64_SYSREG_REVIDR_EL1        = 0xc006, // 11  000  0000  0000  110
	ARM64_SYSREG_AIDR_EL1          = 0xc807, // 11  001  0000  0000  111
	ARM64_SYSREG_DCZID_EL0         = 0xd807, // 11  011  0000  0000  111
	ARM64_SYSREG_ID_PFR0_EL1       = 0xc008, // 11  000  0000  0001  000
	ARM64_SYSREG_ID_PFR1_EL1       = 0xc009, // 11  000  0000  0001  001
	ARM64_SYSREG_ID_DFR0_EL1       = 0xc00a, // 11  000  0000  0001  010
	ARM64_SYSREG_ID_AFR0_EL1       = 0xc00b, // 11  000  0000  0001  011
	ARM64_SYSREG_ID_MMFR0_EL1      = 0xc00c, // 11  000  0000  0001  100
	ARM64_SYSREG_ID_MMFR1_EL1      = 0xc00d, // 11  000  0000  0001  101
	ARM64_SYSREG_ID_MMFR2_EL1      = 0xc00e, // 11  000  0000  0001  110
	ARM64_SYSREG_ID_MMFR3_EL1      = 0xc00f, // 11  000  0000  0001  111
	ARM64_SYSREG_ID_ISAR0_EL1      = 0xc010, // 11  000  0000  0010  000
	ARM64_SYSREG_ID_ISAR1_EL1      = 0xc011, // 11  000  0000  0010  001
	ARM64_SYSREG_ID_ISAR2_EL1      = 0xc012, // 11  000  0000  0010  010
	ARM64_SYSREG_ID_ISAR3_EL1      = 0xc013, // 11  000  0000  0010  011
	ARM64_SYSREG_ID_ISAR4_EL1      = 0xc014, // 11  000  0000  0010  100
	ARM64_SYSREG_ID_ISAR5_EL1      = 0xc015, // 11  000  0000  0010  101
	ARM64_SYSREG_ID_A64PFR0_EL1   = 0xc020, // 11  000  0000  0100  000
	ARM64_SYSREG_ID_A64PFR1_EL1   = 0xc021, // 11  000  0000  0100  001
	ARM64_SYSREG_ID_A64DFR0_EL1   = 0xc028, // 11  000  0000  0101  000
	ARM64_SYSREG_ID_A64DFR1_EL1   = 0xc029, // 11  000  0000  0101  001
	ARM64_SYSREG_ID_A64AFR0_EL1   = 0xc02c, // 11  000  0000  0101  100
	ARM64_SYSREG_ID_A64AFR1_EL1   = 0xc02d, // 11  000  0000  0101  101
	ARM64_SYSREG_ID_A64ISAR0_EL1  = 0xc030, // 11  000  0000  0110  000
	ARM64_SYSREG_ID_A64ISAR1_EL1  = 0xc031, // 11  000  0000  0110  001
	ARM64_SYSREG_ID_A64MMFR0_EL1  = 0xc038, // 11  000  0000  0111  000
	ARM64_SYSREG_ID_A64MMFR1_EL1  = 0xc039, // 11  000  0000  0111  001
	ARM64_SYSREG_MVFR0_EL1         = 0xc018, // 11  000  0000  0011  000
	ARM64_SYSREG_MVFR1_EL1         = 0xc019, // 11  000  0000  0011  001
	ARM64_SYSREG_MVFR2_EL1         = 0xc01a, // 11  000  0000  0011  010
	ARM64_SYSREG_RVBAR_EL1         = 0xc601, // 11  000  1100  0000  001
	ARM64_SYSREG_RVBAR_EL2         = 0xe601, // 11  100  1100  0000  001
	ARM64_SYSREG_RVBAR_EL3         = 0xf601, // 11  110  1100  0000  001
	ARM64_SYSREG_ISR_EL1           = 0xc608, // 11  000  1100  0001  000
	ARM64_SYSREG_CNTPCT_EL0        = 0xdf01, // 11  011  1110  0000  001
	ARM64_SYSREG_CNTVCT_EL0        = 0xdf02,  // 11  011  1110  0000  010

	// Trace registers
	ARM64_SYSREG_TRCSTATR          = 0x8818, // 10  001  0000  0011  000
	ARM64_SYSREG_TRCIDR8           = 0x8806, // 10  001  0000  0000  110
	ARM64_SYSREG_TRCIDR9           = 0x880e, // 10  001  0000  0001  110
	ARM64_SYSREG_TRCIDR10          = 0x8816, // 10  001  0000  0010  110
	ARM64_SYSREG_TRCIDR11          = 0x881e, // 10  001  0000  0011  110
	ARM64_SYSREG_TRCIDR12          = 0x8826, // 10  001  0000  0100  110
	ARM64_SYSREG_TRCIDR13          = 0x882e, // 10  001  0000  0101  110
	ARM64_SYSREG_TRCIDR0           = 0x8847, // 10  001  0000  1000  111
	ARM64_SYSREG_TRCIDR1           = 0x884f, // 10  001  0000  1001  111
	ARM64_SYSREG_TRCIDR2           = 0x8857, // 10  001  0000  1010  111
	ARM64_SYSREG_TRCIDR3           = 0x885f, // 10  001  0000  1011  111
	ARM64_SYSREG_TRCIDR4           = 0x8867, // 10  001  0000  1100  111
	ARM64_SYSREG_TRCIDR5           = 0x886f, // 10  001  0000  1101  111
	ARM64_SYSREG_TRCIDR6           = 0x8877, // 10  001  0000  1110  111
	ARM64_SYSREG_TRCIDR7           = 0x887f, // 10  001  0000  1111  111
	ARM64_SYSREG_TRCOSLSR          = 0x888c, // 10  001  0001  0001  100
	ARM64_SYSREG_TRCPDSR           = 0x88ac, // 10  001  0001  0101  100
	ARM64_SYSREG_TRCDEVAFF0        = 0x8bd6, // 10  001  0111  1010  110
	ARM64_SYSREG_TRCDEVAFF1        = 0x8bde, // 10  001  0111  1011  110
	ARM64_SYSREG_TRCLSR            = 0x8bee, // 10  001  0111  1101  110
	ARM64_SYSREG_TRCAUTHSTATUS     = 0x8bf6, // 10  001  0111  1110  110
	ARM64_SYSREG_TRCDEVARCH        = 0x8bfe, // 10  001  0111  1111  110
	ARM64_SYSREG_TRCDEVID          = 0x8b97, // 10  001  0111  0010  111
	ARM64_SYSREG_TRCDEVTYPE        = 0x8b9f, // 10  001  0111  0011  111
	ARM64_SYSREG_TRCPIDR4          = 0x8ba7, // 10  001  0111  0100  111
	ARM64_SYSREG_TRCPIDR5          = 0x8baf, // 10  001  0111  0101  111
	ARM64_SYSREG_TRCPIDR6          = 0x8bb7, // 10  001  0111  0110  111
	ARM64_SYSREG_TRCPIDR7          = 0x8bbf, // 10  001  0111  0111  111
	ARM64_SYSREG_TRCPIDR0          = 0x8bc7, // 10  001  0111  1000  111
	ARM64_SYSREG_TRCPIDR1          = 0x8bcf, // 10  001  0111  1001  111
	ARM64_SYSREG_TRCPIDR2          = 0x8bd7, // 10  001  0111  1010  111
	ARM64_SYSREG_TRCPIDR3          = 0x8bdf, // 10  001  0111  1011  111
	ARM64_SYSREG_TRCCIDR0          = 0x8be7, // 10  001  0111  1100  111
	ARM64_SYSREG_TRCCIDR1          = 0x8bef, // 10  001  0111  1101  111
	ARM64_SYSREG_TRCCIDR2          = 0x8bf7, // 10  001  0111  1110  111
	ARM64_SYSREG_TRCCIDR3          = 0x8bff, // 10  001  0111  1111  111

	// GICv3 registers
	ARM64_SYSREG_ICC_IAR1_EL1      = 0xc660, // 11  000  1100  1100  000
	ARM64_SYSREG_ICC_IAR0_EL1      = 0xc640, // 11  000  1100  1000  000
	ARM64_SYSREG_ICC_HPPIR1_EL1    = 0xc662, // 11  000  1100  1100  010
	ARM64_SYSREG_ICC_HPPIR0_EL1    = 0xc642, // 11  000  1100  1000  010
	ARM64_SYSREG_ICC_RPR_EL1       = 0xc65b, // 11  000  1100  1011  011
	ARM64_SYSREG_ICH_VTR_EL2       = 0xe659, // 11  100  1100  1011  001
	ARM64_SYSREG_ICH_EISR_EL2      = 0xe65b, // 11  100  1100  1011  011
	ARM64_SYSREG_ICH_ELSR_EL2      = 0xe65d, // 11  100  1100  1011  101
} arm64_sysreg;

typedef enum arm64_msr_reg {
	// System registers for MSR
	ARM64_SYSREG_DBGDTRTX_EL0      = 0x9828, // 10  011  0000  0101  000
	ARM64_SYSREG_OSLAR_EL1         = 0x8084, // 10  000  0001  0000  100
	ARM64_SYSREG_PMSWINC_EL0       = 0xdce4,  // 11  011  1001  1100  100

	// Trace Registers
	ARM64_SYSREG_TRCOSLAR          = 0x8884, // 10  001  0001  0000  100
	ARM64_SYSREG_TRCLAR            = 0x8be6, // 10  001  0111  1100  110

	// GICv3 registers
	ARM64_SYSREG_ICC_EOIR1_EL1     = 0xc661, // 11  000  1100  1100  001
	ARM64_SYSREG_ICC_EOIR0_EL1     = 0xc641, // 11  000  1100  1000  001
	ARM64_SYSREG_ICC_DIR_EL1       = 0xc659, // 11  000  1100  1011  001
	ARM64_SYSREG_ICC_SGI1R_EL1     = 0xc65d, // 11  000  1100  1011  101
	ARM64_SYSREG_ICC_ASGI1R_EL1    = 0xc65e, // 11  000  1100  1011  110
	ARM64_SYSREG_ICC_SGI0R_EL1     = 0xc65f, // 11  000  1100  1011  111
} arm64_msr_reg;

/// System PState Field (MSR instruction)
typedef enum arm64_pstate {
	ARM64_PSTATE_INVALID = 0,
	ARM64_PSTATE_SPSEL = 0x05,
	ARM64_PSTATE_DAIFSET = 0x1e,
	ARM64_PSTATE_DAIFCLR = 0x1f
} arm64_pstate;

/// Vector arrangement specifier (for FloatingPoint/Advanced SIMD insn)
typedef enum arm64_vas {
	ARM64_VAS_INVALID = 0,
	ARM64_VAS_8B,
	ARM64_VAS_16B,
	ARM64_VAS_4H,
	ARM64_VAS_8H,
	ARM64_VAS_2S,
	ARM64_VAS_4S,
	ARM64_VAS_1D,
	ARM64_VAS_2D,
	ARM64_VAS_1Q,
} arm64_vas;

/// Vector element size specifier
typedef enum arm64_vess {
	ARM64_VESS_INVALID = 0,
	ARM64_VESS_B,
	ARM64_VESS_H,
	ARM64_VESS_S,
	ARM64_VESS_D,
} arm64_vess;

/// Memory barrier operands
typedef enum arm64_barrier_op {
	ARM64_BARRIER_INVALID = 0,
	ARM64_BARRIER_OSHLD = 0x1,
	ARM64_BARRIER_OSHST = 0x2,
	ARM64_BARRIER_OSH =   0x3,
	ARM64_BARRIER_NSHLD = 0x5,
	ARM64_BARRIER_NSHST = 0x6,
	ARM64_BARRIER_NSH =   0x7,
	ARM64_BARRIER_ISHLD = 0x9,
	ARM64_BARRIER_ISHST = 0xa,
	ARM64_BARRIER_ISH =   0xb,
	ARM64_BARRIER_LD =    0xd,
	ARM64_BARRIER_ST =    0xe,
	ARM64_BARRIER_SY =    0xf
} arm64_barrier_op;

/// Operand type for instruction's operands
typedef enum arm64_op_type {
	ARM64_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	ARM64_OP_REG, ///< = CS_OP_REG (Register operand).
	ARM64_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	ARM64_OP_MEM, ///< = CS_OP_MEM (Memory operand).
	ARM64_OP_FP,  ///< = CS_OP_FP (Floating-Point operand).
	ARM64_OP_CIMM = 64, ///< C-Immediate
	ARM64_OP_REG_MRS, ///< MRS register operand.
	ARM64_OP_REG_MSR, ///< MSR register operand.
	ARM64_OP_PSTATE, ///< PState operand.
	ARM64_OP_SYS, ///< SYS operand for IC/DC/AT/TLBI instructions.
	ARM64_OP_PREFETCH, ///< Prefetch operand (PRFM).
	ARM64_OP_BARRIER, ///< Memory barrier operand (ISB/DMB/DSB instructions).
} arm64_op_type;

/// TLBI operations
typedef enum arm64_tlbi_op {
	ARM64_TLBI_INVALID = 0,
	ARM64_TLBI_VMALLE1IS,
	ARM64_TLBI_VAE1IS,
	ARM64_TLBI_ASIDE1IS,
	ARM64_TLBI_VAAE1IS,
	ARM64_TLBI_VALE1IS,
	ARM64_TLBI_VAALE1IS,
	ARM64_TLBI_ALLE2IS,
	ARM64_TLBI_VAE2IS,
	ARM64_TLBI_ALLE1IS,
	ARM64_TLBI_VALE2IS,
	ARM64_TLBI_VMALLS12E1IS,
	ARM64_TLBI_ALLE3IS,
	ARM64_TLBI_VAE3IS,
	ARM64_TLBI_VALE3IS,
	ARM64_TLBI_IPAS2E1IS,
	ARM64_TLBI_IPAS2LE1IS,
	ARM64_TLBI_IPAS2E1,
	ARM64_TLBI_IPAS2LE1,
	ARM64_TLBI_VMALLE1,
	ARM64_TLBI_VAE1,
	ARM64_TLBI_ASIDE1,
	ARM64_TLBI_VAAE1,
	ARM64_TLBI_VALE1,
	ARM64_TLBI_VAALE1,
	ARM64_TLBI_ALLE2,
	ARM64_TLBI_VAE2,
	ARM64_TLBI_ALLE1,
	ARM64_TLBI_VALE2,
	ARM64_TLBI_VMALLS12E1,
	ARM64_TLBI_ALLE3,
	ARM64_TLBI_VAE3,
	ARM64_TLBI_VALE3,
} arm64_tlbi_op;

/// AT operations
typedef enum arm64_at_op {
	ARM64_AT_S1E1R,
	ARM64_AT_S1E1W,
	ARM64_AT_S1E0R,
	ARM64_AT_S1E0W,
	ARM64_AT_S1E2R,
	ARM64_AT_S1E2W,
	ARM64_AT_S12E1R,
	ARM64_AT_S12E1W,
	ARM64_AT_S12E0R,
	ARM64_AT_S12E0W,
	ARM64_AT_S1E3R,
	ARM64_AT_S1E3W,
} arm64_at_op;

/// DC operations
typedef enum arm64_dc_op {
	ARM64_DC_INVALID = 0,
	ARM64_DC_ZVA,
	ARM64_DC_IVAC,
	ARM64_DC_ISW,
	ARM64_DC_CVAC,
	ARM64_DC_CSW,
	ARM64_DC_CVAU,
	ARM64_DC_CIVAC,
	ARM64_DC_CISW,
} arm64_dc_op;

/// IC operations
typedef enum arm64_ic_op {
	ARM64_IC_INVALID = 0,
	ARM64_IC_IALLUIS,
	ARM64_IC_IALLU,
	ARM64_IC_IVAU,
} arm64_ic_op;

/// Prefetch operations (PRFM)
typedef enum arm64_prefetch_op {
	ARM64_PRFM_INVALID = 0,
	ARM64_PRFM_PLDL1KEEP = 0x00 + 1,
	ARM64_PRFM_PLDL1STRM = 0x01 + 1,
	ARM64_PRFM_PLDL2KEEP = 0x02 + 1,
	ARM64_PRFM_PLDL2STRM = 0x03 + 1,
	ARM64_PRFM_PLDL3KEEP = 0x04 + 1,
	ARM64_PRFM_PLDL3STRM = 0x05 + 1,
	ARM64_PRFM_PLIL1KEEP = 0x08 + 1,
	ARM64_PRFM_PLIL1STRM = 0x09 + 1,
	ARM64_PRFM_PLIL2KEEP = 0x0a + 1,
	ARM64_PRFM_PLIL2STRM = 0x0b + 1,
	ARM64_PRFM_PLIL3KEEP = 0x0c + 1,
	ARM64_PRFM_PLIL3STRM = 0x0d + 1,
	ARM64_PRFM_PSTL1KEEP = 0x10 + 1,
	ARM64_PRFM_PSTL1STRM = 0x11 + 1,
	ARM64_PRFM_PSTL2KEEP = 0x12 + 1,
	ARM64_PRFM_PSTL2STRM = 0x13 + 1,
	ARM64_PRFM_PSTL3KEEP = 0x14 + 1,
	ARM64_PRFM_PSTL3STRM = 0x15 + 1,
} arm64_prefetch_op;


/// ARM64 registers
typedef enum arm64_reg {
	ARM64_REG_INVALID = 0,

	ARM64_REG_X29,
	ARM64_REG_X30,
	ARM64_REG_NZCV,
	ARM64_REG_SP,
	ARM64_REG_WSP,
	ARM64_REG_WZR,
	ARM64_REG_XZR,
	ARM64_REG_B0,
	ARM64_REG_B1,
	ARM64_REG_B2,
	ARM64_REG_B3,
	ARM64_REG_B4,
	ARM64_REG_B5,
	ARM64_REG_B6,
	ARM64_REG_B7,
	ARM64_REG_B8,
	ARM64_REG_B9,
	ARM64_REG_B10,
	ARM64_REG_B11,
	ARM64_REG_B12,
	ARM64_REG_B13,
	ARM64_REG_B14,
	ARM64_REG_B15,
	ARM64_REG_B16,
	ARM64_REG_B17,
	ARM64_REG_B18,
	ARM64_REG_B19,
	ARM64_REG_B20,
	ARM64_REG_B21,
	ARM64_REG_B22,
	ARM64_REG_B23,
	ARM64_REG_B24,
	ARM64_REG_B25,
	ARM64_REG_B26,
	ARM64_REG_B27,
	ARM64_REG_B28,
	ARM64_REG_B29,
	ARM64_REG_B30,
	ARM64_REG_B31,
	ARM64_REG_D0,
	ARM64_REG_D1,
	ARM64_REG_D2,
	ARM64_REG_D3,
	ARM64_REG_D4,
	ARM64_REG_D5,
	ARM64_REG_D6,
	ARM64_REG_D7,
	ARM64_REG_D8,
	ARM64_REG_D9,
	ARM64_REG_D10,
	ARM64_REG_D11,
	ARM64_REG_D12,
	ARM64_REG_D13,
	ARM64_REG_D14,
	ARM64_REG_D15,
	ARM64_REG_D16,
	ARM64_REG_D17,
	ARM64_REG_D18,
	ARM64_REG_D19,
	ARM64_REG_D20,
	ARM64_REG_D21,
	ARM64_REG_D22,
	ARM64_REG_D23,
	ARM64_REG_D24,
	ARM64_REG_D25,
	ARM64_REG_D26,
	ARM64_REG_D27,
	ARM64_REG_D28,
	ARM64_REG_D29,
	ARM64_REG_D30,
	ARM64_REG_D31,
	ARM64_REG_H0,
	ARM64_REG_H1,
	ARM64_REG_H2,
	ARM64_REG_H3,
	ARM64_REG_H4,
	ARM64_REG_H5,
	ARM64_REG_H6,
	ARM64_REG_H7,
	ARM64_REG_H8,
	ARM64_REG_H9,
	ARM64_REG_H10,
	ARM64_REG_H11,
	ARM64_REG_H12,
	ARM64_REG_H13,
	ARM64_REG_H14,
	ARM64_REG_H15,
	ARM64_REG_H16,
	ARM64_REG_H17,
	ARM64_REG_H18,
	ARM64_REG_H19,
	ARM64_REG_H20,
	ARM64_REG_H21,
	ARM64_REG_H22,
	ARM64_REG_H23,
	ARM64_REG_H24,
	ARM64_REG_H25,
	ARM64_REG_H26,
	ARM64_REG_H27,
	ARM64_REG_H28,
	ARM64_REG_H29,
	ARM64_REG_H30,
	ARM64_REG_H31,
	ARM64_REG_Q0,
	ARM64_REG_Q1,
	ARM64_REG_Q2,
	ARM64_REG_Q3,
	ARM64_REG_Q4,
	ARM64_REG_Q5,
	ARM64_REG_Q6,
	ARM64_REG_Q7,
	ARM64_REG_Q8,
	ARM64_REG_Q9,
	ARM64_REG_Q10,
	ARM64_REG_Q11,
	ARM64_REG_Q12,
	ARM64_REG_Q13,
	ARM64_REG_Q14,
	ARM64_REG_Q15,
	ARM64_REG_Q16,
	ARM64_REG_Q17,
	ARM64_REG_Q18,
	ARM64_REG_Q19,
	ARM64_REG_Q20,
	ARM64_REG_Q21,
	ARM64_REG_Q22,
	ARM64_REG_Q23,
	ARM64_REG_Q24,
	ARM64_REG_Q25,
	ARM64_REG_Q26,
	ARM64_REG_Q27,
	ARM64_REG_Q28,
	ARM64_REG_Q29,
	ARM64_REG_Q30,
	ARM64_REG_Q31,
	ARM64_REG_S0,
	ARM64_REG_S1,
	ARM64_REG_S2,
	ARM64_REG_S3,
	ARM64_REG_S4,
	ARM64_REG_S5,
	ARM64_REG_S6,
	ARM64_REG_S7,
	ARM64_REG_S8,
	ARM64_REG_S9,
	ARM64_REG_S10,
	ARM64_REG_S11,
	ARM64_REG_S12,
	ARM64_REG_S13,
	ARM64_REG_S14,
	ARM64_REG_S15,
	ARM64_REG_S16,
	ARM64_REG_S17,
	ARM64_REG_S18,
	ARM64_REG_S19,
	ARM64_REG_S20,
	ARM64_REG_S21,
	ARM64_REG_S22,
	ARM64_REG_S23,
	ARM64_REG_S24,
	ARM64_REG_S25,
	ARM64_REG_S26,
	ARM64_REG_S27,
	ARM64_REG_S28,
	ARM64_REG_S29,
	ARM64_REG_S30,
	ARM64_REG_S31,
	ARM64_REG_W0,
	ARM64_REG_W1,
	ARM64_REG_W2,
	ARM64_REG_W3,
	ARM64_REG_W4,
	ARM64_REG_W5,
	ARM64_REG_W6,
	ARM64_REG_W7,
	ARM64_REG_W8,
	ARM64_REG_W9,
	ARM64_REG_W10,
	ARM64_REG_W11,
	ARM64_REG_W12,
	ARM64_REG_W13,
	ARM64_REG_W14,
	ARM64_REG_W15,
	ARM64_REG_W16,
	ARM64_REG_W17,
	ARM64_REG_W18,
	ARM64_REG_W19,
	ARM64_REG_W20,
	ARM64_REG_W21,
	ARM64_REG_W22,
	ARM64_REG_W23,
	ARM64_REG_W24,
	ARM64_REG_W25,
	ARM64_REG_W26,
	ARM64_REG_W27,
	ARM64_REG_W28,
	ARM64_REG_W29,
	ARM64_REG_W30,
	ARM64_REG_X0,
	ARM64_REG_X1,
	ARM64_REG_X2,
	ARM64_REG_X3,
	ARM64_REG_X4,
	ARM64_REG_X5,
	ARM64_REG_X6,
	ARM64_REG_X7,
	ARM64_REG_X8,
	ARM64_REG_X9,
	ARM64_REG_X10,
	ARM64_REG_X11,
	ARM64_REG_X12,
	ARM64_REG_X13,
	ARM64_REG_X14,
	ARM64_REG_X15,
	ARM64_REG_X16,
	ARM64_REG_X17,
	ARM64_REG_X18,
	ARM64_REG_X19,
	ARM64_REG_X20,
	ARM64_REG_X21,
	ARM64_REG_X22,
	ARM64_REG_X23,
	ARM64_REG_X24,
	ARM64_REG_X25,
	ARM64_REG_X26,
	ARM64_REG_X27,
	ARM64_REG_X28,

	ARM64_REG_V0,
	ARM64_REG_V1,
	ARM64_REG_V2,
	ARM64_REG_V3,
	ARM64_REG_V4,
	ARM64_REG_V5,
	ARM64_REG_V6,
	ARM64_REG_V7,
	ARM64_REG_V8,
	ARM64_REG_V9,
	ARM64_REG_V10,
	ARM64_REG_V11,
	ARM64_REG_V12,
	ARM64_REG_V13,
	ARM64_REG_V14,
	ARM64_REG_V15,
	ARM64_REG_V16,
	ARM64_REG_V17,
	ARM64_REG_V18,
	ARM64_REG_V19,
	ARM64_REG_V20,
	ARM64_REG_V21,
	ARM64_REG_V22,
	ARM64_REG_V23,
	ARM64_REG_V24,
	ARM64_REG_V25,
	ARM64_REG_V26,
	ARM64_REG_V27,
	ARM64_REG_V28,
	ARM64_REG_V29,
	ARM64_REG_V30,
	ARM64_REG_V31,

	ARM64_REG_ENDING,		// <-- mark the end of the list of registers

	// alias registers

	ARM64_REG_IP0 = ARM64_REG_X16,
	ARM64_REG_IP1 = ARM64_REG_X17,
	ARM64_REG_FP = ARM64_REG_X29,
	ARM64_REG_LR = ARM64_REG_X30,
} arm64_reg;

/// Instruction's operand referring to memory
/// This is associated with ARM64_OP_MEM operand type above
typedef struct arm64_op_mem {
	arm64_reg base;	///< base register
	arm64_reg index;	///< index register
	int32_t disp;	///< displacement/offset value
} arm64_op_mem;

/// Instruction operand
typedef struct cs_arm64_op {
	int vector_index;	///< Vector Index for some vector operands (or -1 if irrelevant)
	arm64_vas vas;		///< Vector Arrangement Specifier
	arm64_vess vess;	///< Vector Element Size Specifier
	struct {
		arm64_shifter type;	///< shifter type of this operand
		unsigned int value;	///< shifter value of this operand
	} shift;
	arm64_extender ext;		///< extender type of this operand
	arm64_op_type type;	///< operand type
	union {
		arm64_reg reg;	///< register value for REG operand
		int64_t imm;		///< immediate value, or index for C-IMM or IMM operand
		double fp;			///< floating point value for FP operand
		arm64_op_mem mem;		///< base/index/scale/disp value for MEM operand
		arm64_pstate pstate;		///< PState field of MSR instruction.
		unsigned int sys;  ///< IC/DC/AT/TLBI operation (see arm64_ic_op, arm64_dc_op, arm64_at_op, arm64_tlbi_op)
		arm64_prefetch_op prefetch;  ///< PRFM operation.
		arm64_barrier_op barrier;  ///< Memory barrier operation (ISB/DMB/DSB instructions).
	};

	/// How is this operand accessed? (READ, WRITE or READ|WRITE)
	/// This field is combined of cs_ac_type.
	/// NOTE: this field is irrelevant if engine is compiled in DIET mode.
	uint8_t access;
} cs_arm64_op;

/// Instruction structure
typedef struct cs_arm64 {
	arm64_cc cc;	///< conditional code for this insn
	bool update_flags;	///< does this insn update flags?
	bool writeback;	///< does this insn request writeback? 'True' means 'yes'

	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;

	cs_arm64_op operands[8]; ///< operands for this instruction.
} cs_arm64;

/// ARM64 instruction
typedef enum arm64_insn {
	ARM64_INS_INVALID = 0,

	ARM64_INS_ABS,
	ARM64_INS_ADC,
	ARM64_INS_ADDHN,
	ARM64_INS_ADDHN2,
	ARM64_INS_ADDP,
	ARM64_INS_ADD,
	ARM64_INS_ADDV,
	ARM64_INS_ADR,
	ARM64_INS_ADRP,
	ARM64_INS_AESD,
	ARM64_INS_AESE,
	ARM64_INS_AESIMC,
	ARM64_INS_AESMC,
	ARM64_INS_AND,
	ARM64_INS_ASR,
	ARM64_INS_B,
	ARM64_INS_BFM,
	ARM64_INS_BIC,
	ARM64_INS_BIF,
	ARM64_INS_BIT,
	ARM64_INS_BL,
	ARM64_INS_BLR,
	ARM64_INS_BR,
	ARM64_INS_BRK,
	ARM64_INS_BSL,
	ARM64_INS_CBNZ,
	ARM64_INS_CBZ,
	ARM64_INS_CCMN,
	ARM64_INS_CCMP,
	ARM64_INS_CLREX,
	ARM64_INS_CLS,
	ARM64_INS_CLZ,
	ARM64_INS_CMEQ,
	ARM64_INS_CMGE,
	ARM64_INS_CMGT,
	ARM64_INS_CMHI,
	ARM64_INS_CMHS,
	ARM64_INS_CMLE,
	ARM64_INS_CMLT,
	ARM64_INS_CMTST,
	ARM64_INS_CNT,
	ARM64_INS_MOV,
	ARM64_INS_CRC32B,
	ARM64_INS_CRC32CB,
	ARM64_INS_CRC32CH,
	ARM64_INS_CRC32CW,
	ARM64_INS_CRC32CX,
	ARM64_INS_CRC32H,
	ARM64_INS_CRC32W,
	ARM64_INS_CRC32X,
	ARM64_INS_CSEL,
	ARM64_INS_CSINC,
	ARM64_INS_CSINV,
	ARM64_INS_CSNEG,
	ARM64_INS_DCPS1,
	ARM64_INS_DCPS2,
	ARM64_INS_DCPS3,
	ARM64_INS_DMB,
	ARM64_INS_DRPS,
	ARM64_INS_DSB,
	ARM64_INS_DUP,
	ARM64_INS_EON,
	ARM64_INS_EOR,
	ARM64_INS_ERET,
	ARM64_INS_EXTR,
	ARM64_INS_EXT,
	ARM64_INS_FABD,
	ARM64_INS_FABS,
	ARM64_INS_FACGE,
	ARM64_INS_FACGT,
	ARM64_INS_FADD,
	ARM64_INS_FADDP,
	ARM64_INS_FCCMP,
	ARM64_INS_FCCMPE,
	ARM64_INS_FCMEQ,
	ARM64_INS_FCMGE,
	ARM64_INS_FCMGT,
	ARM64_INS_FCMLE,
	ARM64_INS_FCMLT,
	ARM64_INS_FCMP,
	ARM64_INS_FCMPE,
	ARM64_INS_FCSEL,
	ARM64_INS_FCVTAS,
	ARM64_INS_FCVTAU,
	ARM64_INS_FCVT,
	ARM64_INS_FCVTL,
	ARM64_INS_FCVTL2,
	ARM64_INS_FCVTMS,
	ARM64_INS_FCVTMU,
	ARM64_INS_FCVTNS,
	ARM64_INS_FCVTNU,
	ARM64_INS_FCVTN,
	ARM64_INS_FCVTN2,
	ARM64_INS_FCVTPS,
	ARM64_INS_FCVTPU,
	ARM64_INS_FCVTXN,
	ARM64_INS_FCVTXN2,
	ARM64_INS_FCVTZS,
	ARM64_INS_FCVTZU,
	ARM64_INS_FDIV,
	ARM64_INS_FMADD,
	ARM64_INS_FMAX,
	ARM64_INS_FMAXNM,
	ARM64_INS_FMAXNMP,
	ARM64_INS_FMAXNMV,
	ARM64_INS_FMAXP,
	ARM64_INS_FMAXV,
	ARM64_INS_FMIN,
	ARM64_INS_FMINNM,
	ARM64_INS_FMINNMP,
	ARM64_INS_FMINNMV,
	ARM64_INS_FMINP,
	ARM64_INS_FMINV,
	ARM64_INS_FMLA,
	ARM64_INS_FMLS,
	ARM64_INS_FMOV,
	ARM64_INS_FMSUB,
	ARM64_INS_FMUL,
	ARM64_INS_FMULX,
	ARM64_INS_FNEG,
	ARM64_INS_FNMADD,
	ARM64_INS_FNMSUB,
	ARM64_INS_FNMUL,
	ARM64_INS_FRECPE,
	ARM64_INS_FRECPS,
	ARM64_INS_FRECPX,
	ARM64_INS_FRINTA,
	ARM64_INS_FRINTI,
	ARM64_INS_FRINTM,
	ARM64_INS_FRINTN,
	ARM64_INS_FRINTP,
	ARM64_INS_FRINTX,
	ARM64_INS_FRINTZ,
	ARM64_INS_FRSQRTE,
	ARM64_INS_FRSQRTS,
	ARM64_INS_FSQRT,
	ARM64_INS_FSUB,
	ARM64_INS_HINT,
	ARM64_INS_HLT,
	ARM64_INS_HVC,
	ARM64_INS_INS,

	ARM64_INS_ISB,
	ARM64_INS_LD1,
	ARM64_INS_LD1R,
	ARM64_INS_LD2R,
	ARM64_INS_LD2,
	ARM64_INS_LD3R,
	ARM64_INS_LD3,
	ARM64_INS_LD4,
	ARM64_INS_LD4R,

	ARM64_INS_LDARB,
	ARM64_INS_LDARH,
	ARM64_INS_LDAR,
	ARM64_INS_LDAXP,
	ARM64_INS_LDAXRB,
	ARM64_INS_LDAXRH,
	ARM64_INS_LDAXR,
	ARM64_INS_LDNP,
	ARM64_INS_LDP,
	ARM64_INS_LDPSW,
	ARM64_INS_LDRB,
	ARM64_INS_LDR,
	ARM64_INS_LDRH,
	ARM64_INS_LDRSB,
	ARM64_INS_LDRSH,
	ARM64_INS_LDRSW,
	ARM64_INS_LDTRB,
	ARM64_INS_LDTRH,
	ARM64_INS_LDTRSB,

	ARM64_INS_LDTRSH,
	ARM64_INS_LDTRSW,
	ARM64_INS_LDTR,
	ARM64_INS_LDURB,
	ARM64_INS_LDUR,
	ARM64_INS_LDURH,
	ARM64_INS_LDURSB,
	ARM64_INS_LDURSH,
	ARM64_INS_LDURSW,
	ARM64_INS_LDXP,
	ARM64_INS_LDXRB,
	ARM64_INS_LDXRH,
	ARM64_INS_LDXR,
	ARM64_INS_LSL,
	ARM64_INS_LSR,
	ARM64_INS_MADD,
	ARM64_INS_MLA,
	ARM64_INS_MLS,
	ARM64_INS_MOVI,
	ARM64_INS_MOVK,
	ARM64_INS_MOVN,
	ARM64_INS_MOVZ,
	ARM64_INS_MRS,
	ARM64_INS_MSR,
	ARM64_INS_MSUB,
	ARM64_INS_MUL,
	ARM64_INS_MVNI,
	ARM64_INS_NEG,
	ARM64_INS_NOT,
	ARM64_INS_ORN,
	ARM64_INS_ORR,
	ARM64_INS_PMULL2,
	ARM64_INS_PMULL,
	ARM64_INS_PMUL,
	ARM64_INS_PRFM,
	ARM64_INS_PRFUM,
	ARM64_INS_RADDHN,
	ARM64_INS_RADDHN2,
	ARM64_INS_RBIT,
	ARM64_INS_RET,
	ARM64_INS_REV16,
	ARM64_INS_REV32,
	ARM64_INS_REV64,
	ARM64_INS_REV,
	ARM64_INS_ROR,
	ARM64_INS_RSHRN2,
	ARM64_INS_RSHRN,
	ARM64_INS_RSUBHN,
	ARM64_INS_RSUBHN2,
	ARM64_INS_SABAL2,
	ARM64_INS_SABAL,

	ARM64_INS_SABA,
	ARM64_INS_SABDL2,
	ARM64_INS_SABDL,
	ARM64_INS_SABD,
	ARM64_INS_SADALP,
	ARM64_INS_SADDLP,
	ARM64_INS_SADDLV,
	ARM64_INS_SADDL2,
	ARM64_INS_SADDL,
	ARM64_INS_SADDW2,
	ARM64_INS_SADDW,
	ARM64_INS_SBC,
	ARM64_INS_SBFM,
	ARM64_INS_SCVTF,
	ARM64_INS_SDIV,
	ARM64_INS_SHA1C,
	ARM64_INS_SHA1H,
	ARM64_INS_SHA1M,
	ARM64_INS_SHA1P,
	ARM64_INS_SHA1SU0,
	ARM64_INS_SHA1SU1,
	ARM64_INS_SHA256H2,
	ARM64_INS_SHA256H,
	ARM64_INS_SHA256SU0,
	ARM64_INS_SHA256SU1,
	ARM64_INS_SHADD,
	ARM64_INS_SHLL2,
	ARM64_INS_SHLL,
	ARM64_INS_SHL,
	ARM64_INS_SHRN2,
	ARM64_INS_SHRN,
	ARM64_INS_SHSUB,
	ARM64_INS_SLI,
	ARM64_INS_SMADDL,
	ARM64_INS_SMAXP,
	ARM64_INS_SMAXV,
	ARM64_INS_SMAX,
	ARM64_INS_SMC,
	ARM64_INS_SMINP,
	ARM64_INS_SMINV,
	ARM64_INS_SMIN,
	ARM64_INS_SMLAL2,
	ARM64_INS_SMLAL,
	ARM64_INS_SMLSL2,
	ARM64_INS_SMLSL,
	ARM64_INS_SMOV,
	ARM64_INS_SMSUBL,
	ARM64_INS_SMULH,
	ARM64_INS_SMULL2,
	ARM64_INS_SMULL,
	ARM64_INS_SQABS,
	ARM64_INS_SQADD,
	ARM64_INS_SQDMLAL,
	ARM64_INS_SQDMLAL2,
	ARM64_INS_SQDMLSL,
	ARM64_INS_SQDMLSL2,
	ARM64_INS_SQDMULH,
	ARM64_INS_SQDMULL,
	ARM64_INS_SQDMULL2,
	ARM64_INS_SQNEG,
	ARM64_INS_SQRDMULH,
	ARM64_INS_SQRSHL,
	ARM64_INS_SQRSHRN,
	ARM64_INS_SQRSHRN2,
	ARM64_INS_SQRSHRUN,
	ARM64_INS_SQRSHRUN2,
	ARM64_INS_SQSHLU,
	ARM64_INS_SQSHL,
	ARM64_INS_SQSHRN,
	ARM64_INS_SQSHRN2,
	ARM64_INS_SQSHRUN,
	ARM64_INS_SQSHRUN2,
	ARM64_INS_SQSUB,
	ARM64_INS_SQXTN2,
	ARM64_INS_SQXTN,
	ARM64_INS_SQXTUN2,
	ARM64_INS_SQXTUN,
	ARM64_INS_SRHADD,
	ARM64_INS_SRI,
	ARM64_INS_SRSHL,
	ARM64_INS_SRSHR,
	ARM64_INS_SRSRA,
	ARM64_INS_SSHLL2,
	ARM64_INS_SSHLL,
	ARM64_INS_SSHL,
	ARM64_INS_SSHR,
	ARM64_INS_SSRA,
	ARM64_INS_SSUBL2,
	ARM64_INS_SSUBL,
	ARM64_INS_SSUBW2,
	ARM64_INS_SSUBW,
	ARM64_INS_ST1,
	ARM64_INS_ST2,
	ARM64_INS_ST3,
	ARM64_INS_ST4,
	ARM64_INS_STLRB,
	ARM64_INS_STLRH,
	ARM64_INS_STLR,
	ARM64_INS_STLXP,
	ARM64_INS_STLXRB,
	ARM64_INS_STLXRH,
	ARM64_INS_STLXR,
	ARM64_INS_STNP,
	ARM64_INS_STP,
	ARM64_INS_STRB,
	ARM64_INS_STR,
	ARM64_INS_STRH,
	ARM64_INS_STTRB,
	ARM64_INS_STTRH,
	ARM64_INS_STTR,
	ARM64_INS_STURB,
	ARM64_INS_STUR,
	ARM64_INS_STURH,
	ARM64_INS_STXP,
	ARM64_INS_STXRB,
	ARM64_INS_STXRH,
	ARM64_INS_STXR,
	ARM64_INS_SUBHN,
	ARM64_INS_SUBHN2,
	ARM64_INS_SUB,
	ARM64_INS_SUQADD,
	ARM64_INS_SVC,
	ARM64_INS_SYSL,
	ARM64_INS_SYS,
	ARM64_INS_TBL,
	ARM64_INS_TBNZ,
	ARM64_INS_TBX,
	ARM64_INS_TBZ,
	ARM64_INS_TRN1,
	ARM64_INS_TRN2,
	ARM64_INS_UABAL2,
	ARM64_INS_UABAL,
	ARM64_INS_UABA,
	ARM64_INS_UABDL2,
	ARM64_INS_UABDL,
	ARM64_INS_UABD,
	ARM64_INS_UADALP,
	ARM64_INS_UADDLP,
	ARM64_INS_UADDLV,
	ARM64_INS_UADDL2,
	ARM64_INS_UADDL,
	ARM64_INS_UADDW2,
	ARM64_INS_UADDW,
	ARM64_INS_UBFM,
	ARM64_INS_UCVTF,
	ARM64_INS_UDIV,
	ARM64_INS_UHADD,
	ARM64_INS_UHSUB,
	ARM64_INS_UMADDL,
	ARM64_INS_UMAXP,
	ARM64_INS_UMAXV,
	ARM64_INS_UMAX,
	ARM64_INS_UMINP,
	ARM64_INS_UMINV,
	ARM64_INS_UMIN,
	ARM64_INS_UMLAL2,
	ARM64_INS_UMLAL,
	ARM64_INS_UMLSL2,
	ARM64_INS_UMLSL,
	ARM64_INS_UMOV,
	ARM64_INS_UMSUBL,
	ARM64_INS_UMULH,
	ARM64_INS_UMULL2,
	ARM64_INS_UMULL,
	ARM64_INS_UQADD,
	ARM64_INS_UQRSHL,
	ARM64_INS_UQRSHRN,
	ARM64_INS_UQRSHRN2,
	ARM64_INS_UQSHL,
	ARM64_INS_UQSHRN,
	ARM64_INS_UQSHRN2,
	ARM64_INS_UQSUB,
	ARM64_INS_UQXTN2,
	ARM64_INS_UQXTN,
	ARM64_INS_URECPE,
	ARM64_INS_URHADD,
	ARM64_INS_URSHL,
	ARM64_INS_URSHR,
	ARM64_INS_URSQRTE,
	ARM64_INS_URSRA,
	ARM64_INS_USHLL2,
	ARM64_INS_USHLL,
	ARM64_INS_USHL,
	ARM64_INS_USHR,
	ARM64_INS_USQADD,
	ARM64_INS_USRA,
	ARM64_INS_USUBL2,
	ARM64_INS_USUBL,
	ARM64_INS_USUBW2,
	ARM64_INS_USUBW,
	ARM64_INS_UZP1,
	ARM64_INS_UZP2,
	ARM64_INS_XTN2,
	ARM64_INS_XTN,
	ARM64_INS_ZIP1,
	ARM64_INS_ZIP2,

	// alias insn
	ARM64_INS_MNEG,
	ARM64_INS_UMNEGL,
	ARM64_INS_SMNEGL,
	ARM64_INS_NOP,
	ARM64_INS_YIELD,
	ARM64_INS_WFE,
	ARM64_INS_WFI,
	ARM64_INS_SEV,
	ARM64_INS_SEVL,
	ARM64_INS_NGC,
	ARM64_INS_SBFIZ,
	ARM64_INS_UBFIZ,
	ARM64_INS_SBFX,
	ARM64_INS_UBFX,
	ARM64_INS_BFI,
	ARM64_INS_BFXIL,
	ARM64_INS_CMN,
	ARM64_INS_MVN,
	ARM64_INS_TST,
	ARM64_INS_CSET,
	ARM64_INS_CINC,
	ARM64_INS_CSETM,
	ARM64_INS_CINV,
	ARM64_INS_CNEG,
	ARM64_INS_SXTB,
	ARM64_INS_SXTH,
	ARM64_INS_SXTW,
	ARM64_INS_CMP,
	ARM64_INS_UXTB,
	ARM64_INS_UXTH,
	ARM64_INS_UXTW,
	ARM64_INS_IC,
	ARM64_INS_DC,
	ARM64_INS_AT,
	ARM64_INS_TLBI,

	ARM64_INS_NEGS,
	ARM64_INS_NGCS,

	ARM64_INS_ENDING,  // <-- mark the end of the list of insn
} arm64_insn;

/// Group of ARM64 instructions
typedef enum arm64_insn_group {
	ARM64_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	ARM64_GRP_JUMP,	///< = CS_GRP_JUMP
	ARM64_GRP_CALL,
	ARM64_GRP_RET,
	ARM64_GRP_INT,
	ARM64_GRP_PRIVILEGE = 6, ///< = CS_GRP_PRIVILEGE
	ARM64_GRP_BRANCH_RELATIVE, ///< = CS_GRP_BRANCH_RELATIVE

	// Architecture-specific groups
	ARM64_GRP_CRYPTO = 128,
	ARM64_GRP_FPARMV8,
	ARM64_GRP_NEON,
	ARM64_GRP_CRC,

	ARM64_GRP_ENDING,  // <-- mark the end of the list of groups
} arm64_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/capstone.h`:

```h
#ifndef CAPSTONE_ENGINE_H
#define CAPSTONE_ENGINE_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2013-2016 */

#ifdef __cplusplus
extern "C" {
#endif

#include <stdarg.h>

#if defined(CAPSTONE_HAS_OSXKERNEL)
#include <libkern/libkern.h>
#else
#include <stdlib.h>
#include <stdio.h>
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#pragma warning(disable:4100)
#define CAPSTONE_API __cdecl
#ifdef CAPSTONE_SHARED
#define CAPSTONE_EXPORT __declspec(dllexport)
#else    // defined(CAPSTONE_STATIC)
#define CAPSTONE_EXPORT
#endif
#else
#define CAPSTONE_API
#if defined(__GNUC__) && !defined(CAPSTONE_STATIC)
#define CAPSTONE_EXPORT __attribute__((visibility("default")))
#else    // defined(CAPSTONE_STATIC)
#define CAPSTONE_EXPORT
#endif
#endif

#ifdef __GNUC__
#define CAPSTONE_DEPRECATED __attribute__((deprecated))
#elif defined(_MSC_VER)
#define CAPSTONE_DEPRECATED __declspec(deprecated)
#else
#pragma message("WARNING: You need to implement CAPSTONE_DEPRECATED for this compiler")
#define CAPSTONE_DEPRECATED
#endif

// Capstone API version
#define CS_API_MAJOR 4
#define CS_API_MINOR 0

// Version for bleeding edge code of the Github's "next" branch.
// Use this if you want the absolutely latest development code.
// This version number will be bumped up whenever we have a new major change.
#define CS_NEXT_VERSION 5

// Capstone package version
#define CS_VERSION_MAJOR CS_API_MAJOR
#define CS_VERSION_MINOR CS_API_MINOR
#define CS_VERSION_EXTRA 2

/// Macro to create combined version which can be compared to
/// result of cs_version() API.
#define CS_MAKE_VERSION(major, minor) ((major << 8) + minor)

/// Maximum size of an instruction mnemonic string.
#define CS_MNEMONIC_SIZE 32

// Handle using with all API
typedef size_t csh;

/// Architecture type
typedef enum cs_arch {
	CS_ARCH_ARM = 0,	///< ARM architecture (including Thumb, Thumb-2)
	CS_ARCH_ARM64,		///< ARM-64, also called AArch64
	CS_ARCH_MIPS,		///< Mips architecture
	CS_ARCH_X86,		///< X86 architecture (including x86 & x86-64)
	CS_ARCH_PPC,		///< PowerPC architecture
	CS_ARCH_SPARC,		///< Sparc architecture
	CS_ARCH_SYSZ,		///< SystemZ architecture
	CS_ARCH_XCORE,		///< XCore architecture
	CS_ARCH_M68K,		///< 68K architecture
	CS_ARCH_TMS320C64X,	///< TMS320C64x architecture
	CS_ARCH_M680X,		///< 680X architecture
	CS_ARCH_EVM,		///< Ethereum architecture
	CS_ARCH_MAX,
	CS_ARCH_ALL = 0xFFFF, // All architectures - for cs_support()
} cs_arch;

// Support value to verify diet mode of the engine.
// If cs_support(CS_SUPPORT_DIET) return True, the engine was compiled
// in diet mode.
#define CS_SUPPORT_DIET (CS_ARCH_ALL + 1)

// Support value to verify X86 reduce mode of the engine.
// If cs_support(CS_SUPPORT_X86_REDUCE) return True, the engine was compiled
// in X86 reduce mode.
#define CS_SUPPORT_X86_REDUCE (CS_ARCH_ALL + 2)

/// Mode type
typedef enum cs_mode {
	CS_MODE_LITTLE_ENDIAN = 0,	///< little-endian mode (default mode)
	CS_MODE_ARM = 0,	///< 32-bit ARM
	CS_MODE_16 = 1 << 1,	///< 16-bit mode (X86)
	CS_MODE_32 = 1 << 2,	///< 32-bit mode (X86)
	CS_MODE_64 = 1 << 3,	///< 64-bit mode (X86, PPC)
	CS_MODE_THUMB = 1 << 4,	///< ARM's Thumb mode, including Thumb-2
	CS_MODE_MCLASS = 1 << 5,	///< ARM's Cortex-M series
	CS_MODE_V8 = 1 << 6,	///< ARMv8 A32 encodings for ARM
	CS_MODE_MICRO = 1 << 4, ///< MicroMips mode (MIPS)
	CS_MODE_MIPS3 = 1 << 5, ///< Mips III ISA
	CS_MODE_MIPS32R6 = 1 << 6, ///< Mips32r6 ISA
	CS_MODE_MIPS2 = 1 << 7, ///< Mips II ISA
	CS_MODE_V9 = 1 << 4, ///< SparcV9 mode (Sparc)
	CS_MODE_QPX = 1 << 4, ///< Quad Processing eXtensions mode (PPC)
	CS_MODE_M68K_000 = 1 << 1, ///< M68K 68000 mode
	CS_MODE_M68K_010 = 1 << 2, ///< M68K 68010 mode
	CS_MODE_M68K_020 = 1 << 3, ///< M68K 68020 mode
	CS_MODE_M68K_030 = 1 << 4, ///< M68K 68030 mode
	CS_MODE_M68K_040 = 1 << 5, ///< M68K 68040 mode
	CS_MODE_M68K_060 = 1 << 6, ///< M68K 68060 mode
	CS_MODE_BIG_ENDIAN = 1 << 31,	///< big-endian mode
	CS_MODE_MIPS32 = CS_MODE_32,	///< Mips32 ISA (Mips)
	CS_MODE_MIPS64 = CS_MODE_64,	///< Mips64 ISA (Mips)
	CS_MODE_M680X_6301 = 1 << 1, ///< M680X Hitachi 6301,6303 mode
	CS_MODE_M680X_6309 = 1 << 2, ///< M680X Hitachi 6309 mode
	CS_MODE_M680X_6800 = 1 << 3, ///< M680X Motorola 6800,6802 mode
	CS_MODE_M680X_6801 = 1 << 4, ///< M680X Motorola 6801,6803 mode
	CS_MODE_M680X_6805 = 1 << 5, ///< M680X Motorola/Freescale 6805 mode
	CS_MODE_M680X_6808 = 1 << 6, ///< M680X Motorola/Freescale/NXP 68HC08 mode
	CS_MODE_M680X_6809 = 1 << 7, ///< M680X Motorola 6809 mode
	CS_MODE_M680X_6811 = 1 << 8, ///< M680X Motorola/Freescale/NXP 68HC11 mode
	CS_MODE_M680X_CPU12 = 1 << 9, ///< M680X Motorola/Freescale/NXP CPU12
					///< used on M68HC12/HCS12
	CS_MODE_M680X_HCS08 = 1 << 10, ///< M680X Freescale/NXP HCS08 mode
} cs_mode;

typedef void* (CAPSTONE_API *cs_malloc_t)(size_t size);
typedef void* (CAPSTONE_API *cs_calloc_t)(size_t nmemb, size_t size);
typedef void* (CAPSTONE_API *cs_realloc_t)(void *ptr, size_t size);
typedef void (CAPSTONE_API *cs_free_t)(void *ptr);
typedef int (CAPSTONE_API *cs_vsnprintf_t)(char *str, size_t size, const char *format, va_list ap);


/// User-defined dynamic memory related functions: malloc/calloc/realloc/free/vsnprintf()
/// By default, Capstone uses system's malloc(), calloc(), realloc(), free() & vsnprintf().
typedef struct cs_opt_mem {
	cs_malloc_t malloc;
	cs_calloc_t calloc;
	cs_realloc_t realloc;
	cs_free_t free;
	cs_vsnprintf_t vsnprintf;
} cs_opt_mem;

/// Customize mnemonic for instructions with alternative name.
/// To reset existing customized instruction to its default mnemonic,
/// call cs_option(CS_OPT_MNEMONIC) again with the same @id and NULL value
/// for @mnemonic.
typedef struct cs_opt_mnem {
	/// ID of instruction to be customized.
	unsigned int id;
	/// Customized instruction mnemonic.
	const char *mnemonic;
} cs_opt_mnem;

/// Runtime option for the disassembled engine
typedef enum cs_opt_type {
	CS_OPT_INVALID = 0,	///< No option specified
	CS_OPT_SYNTAX,	///< Assembly output syntax
	CS_OPT_DETAIL,	///< Break down instruction structure into details
	CS_OPT_MODE,	///< Change engine's mode at run-time
	CS_OPT_MEM,	///< User-defined dynamic memory related functions
	CS_OPT_SKIPDATA, ///< Skip data when disassembling. Then engine is in SKIPDATA mode.
	CS_OPT_SKIPDATA_SETUP, ///< Setup user-defined function for SKIPDATA option
	CS_OPT_MNEMONIC, ///< Customize instruction mnemonic
	CS_OPT_UNSIGNED, ///< print immediate operands in unsigned form
} cs_opt_type;

/// Runtime option value (associated with option type above)
typedef enum cs_opt_value {
	CS_OPT_OFF = 0,  ///< Turn OFF an option - default for CS_OPT_DETAIL, CS_OPT_SKIPDATA, CS_OPT_UNSIGNED.
	CS_OPT_ON = 3, ///< Turn ON an option (CS_OPT_DETAIL, CS_OPT_SKIPDATA).
	CS_OPT_SYNTAX_DEFAULT = 0, ///< Default asm syntax (CS_OPT_SYNTAX).
	CS_OPT_SYNTAX_INTEL, ///< X86 Intel asm syntax - default on X86 (CS_OPT_SYNTAX).
	CS_OPT_SYNTAX_ATT,   ///< X86 ATT asm syntax (CS_OPT_SYNTAX).
	CS_OPT_SYNTAX_NOREGNAME, ///< Prints register name with only number (CS_OPT_SYNTAX)
	CS_OPT_SYNTAX_MASM, ///< X86 Intel Masm syntax (CS_OPT_SYNTAX).
} cs_opt_value;

/// Common instruction operand types - to be consistent across all architectures.
typedef enum cs_op_type {
	CS_OP_INVALID = 0,  ///< uninitialized/invalid operand.
	CS_OP_REG,          ///< Register operand.
	CS_OP_IMM,          ///< Immediate operand.
	CS_OP_MEM,          ///< Memory operand.
	CS_OP_FP,           ///< Floating-Point operand.
} cs_op_type;

/// Common instruction operand access types - to be consistent across all architectures.
/// It is possible to combine access types, for example: CS_AC_READ | CS_AC_WRITE
typedef enum cs_ac_type {
	CS_AC_INVALID = 0,        ///< Uninitialized/invalid access type.
	CS_AC_READ    = 1 << 0,   ///< Operand read from memory or register.
	CS_AC_WRITE   = 1 << 1,   ///< Operand write to memory or register.
} cs_ac_type;

/// Common instruction groups - to be consistent across all architectures.
typedef enum cs_group_type {
	CS_GRP_INVALID = 0,  ///< uninitialized/invalid group.
	CS_GRP_JUMP,    ///< all jump instructions (conditional+direct+indirect jumps)
	CS_GRP_CALL,    ///< all call instructions
	CS_GRP_RET,     ///< all return instructions
	CS_GRP_INT,     ///< all interrupt instructions (int+syscall)
	CS_GRP_IRET,    ///< all interrupt return instructions
	CS_GRP_PRIVILEGE,    ///< all privileged instructions
	CS_GRP_BRANCH_RELATIVE, ///< all relative branching instructions
} cs_group_type;

/**
 User-defined callback function for SKIPDATA option.
 See tests/test_skipdata.c for sample code demonstrating this API.

 @code: the input buffer containing code to be disassembled.
        This is the same buffer passed to cs_disasm().
 @code_size: size (in bytes) of the above @code buffer.
 @offset: the position of the currently-examining byte in the input
      buffer @code mentioned above.
 @user_data: user-data passed to cs_option() via @user_data field in
      cs_opt_skipdata struct below.

 @return: return number of bytes to skip, or 0 to immediately stop disassembling.
*/
typedef size_t (CAPSTONE_API *cs_skipdata_cb_t)(const uint8_t *code, size_t code_size, size_t offset, void *user_data);

/// User-customized setup for SKIPDATA option
typedef struct cs_opt_skipdata {
	/// Capstone considers data to skip as special "instructions".
	/// User can specify the string for this instruction's "mnemonic" here.
	/// By default (if @mnemonic is NULL), Capstone use ".byte".
	const char *mnemonic;

	/// User-defined callback function to be called when Capstone hits data.
	/// If the returned value from this callback is positive (>0), Capstone
	/// will skip exactly that number of bytes & continue. Otherwise, if
	/// the callback returns 0, Capstone stops disassembling and returns
	/// immediately from cs_disasm()
	/// NOTE: if this callback pointer is NULL, Capstone would skip a number
	/// of bytes depending on architectures, as following:
	/// Arm:     2 bytes (Thumb mode) or 4 bytes.
	/// Arm64:   4 bytes.
	/// Mips:    4 bytes.
	/// M680x:   1 byte.
	/// PowerPC: 4 bytes.
	/// Sparc:   4 bytes.
	/// SystemZ: 2 bytes.
	/// X86:     1 bytes.
	/// XCore:   2 bytes.
	/// EVM:     1 bytes.
	cs_skipdata_cb_t callback; 	// default value is NULL

	/// User-defined data to be passed to @callback function pointer.
	void *user_data;
} cs_opt_skipdata;


#include "arm.h"
#include "arm64.h"
#include "m68k.h"
#include "mips.h"
#include "ppc.h"
#include "sparc.h"
#include "systemz.h"
#include "x86.h"
#include "xcore.h"
#include "tms320c64x.h"
#include "m680x.h"
#include "evm.h"

/// NOTE: All information in cs_detail is only available when CS_OPT_DETAIL = CS_OPT_ON
/// Initialized as memset(., 0, offsetof(cs_detail, ARCH)+sizeof(cs_ARCH))
/// by ARCH_getInstruction in arch/ARCH/ARCHDisassembler.c
/// if cs_detail changes, in particular if a field is added after the union,
/// then update arch/ARCH/ARCHDisassembler.c accordingly
typedef struct cs_detail {
	uint16_t regs_read[12]; ///< list of implicit registers read by this insn
	uint8_t regs_read_count; ///< number of implicit registers read by this insn

	uint16_t regs_write[20]; ///< list of implicit registers modified by this insn
	uint8_t regs_write_count; ///< number of implicit registers modified by this insn

	uint8_t groups[8]; ///< list of group this instruction belong to
	uint8_t groups_count; ///< number of groups this insn belongs to

	/// Architecture-specific instruction info
	union {
		cs_x86 x86;     ///< X86 architecture, including 16-bit, 32-bit & 64-bit mode
		cs_arm64 arm64; ///< ARM64 architecture (aka AArch64)
		cs_arm arm;     ///< ARM architecture (including Thumb/Thumb2)
		cs_m68k m68k;   ///< M68K architecture
		cs_mips mips;   ///< MIPS architecture
		cs_ppc ppc;	    ///< PowerPC architecture
		cs_sparc sparc; ///< Sparc architecture
		cs_sysz sysz;   ///< SystemZ architecture
		cs_xcore xcore; ///< XCore architecture
		cs_tms320c64x tms320c64x;  ///< TMS320C64x architecture
		cs_m680x m680x; ///< M680X architecture
		cs_evm evm;	    ///< Ethereum architecture
	};
} cs_detail;

/// Detail information of disassembled instruction
typedef struct cs_insn {
	/// Instruction ID (basically a numeric ID for the instruction mnemonic)
	/// Find the instruction id in the '[ARCH]_insn' enum in the header file
	/// of corresponding architecture, such as 'arm_insn' in arm.h for ARM,
	/// 'x86_insn' in x86.h for X86, etc...
	/// This information is available even when CS_OPT_DETAIL = CS_OPT_OFF
	/// NOTE: in Skipdata mode, "data" instruction has 0 for this id field.
	unsigned int id;

	/// Address (EIP) of this instruction
	/// This information is available even when CS_OPT_DETAIL = CS_OPT_OFF
	uint64_t address;

	/// Size of this instruction
	/// This information is available even when CS_OPT_DETAIL = CS_OPT_OFF
	uint16_t size;

	/// Machine bytes of this instruction, with number of bytes indicated by @size above
	/// This information is available even when CS_OPT_DETAIL = CS_OPT_OFF
	uint8_t bytes[16];

	/// Ascii text of instruction mnemonic
	/// This information is available even when CS_OPT_DETAIL = CS_OPT_OFF
	char mnemonic[CS_MNEMONIC_SIZE];

	/// Ascii text of instruction operands
	/// This information is available even when CS_OPT_DETAIL = CS_OPT_OFF
	char op_str[160];

	/// Pointer to cs_detail.
	/// NOTE: detail pointer is only valid when both requirements below are met:
	/// (1) CS_OP_DETAIL = CS_OPT_ON
	/// (2) Engine is not in Skipdata mode (CS_OP_SKIPDATA option set to CS_OPT_ON)
	///
	/// NOTE 2: when in Skipdata mode, or when detail mode is OFF, even if this pointer
	///     is not NULL, its content is still irrelevant.
	cs_detail *detail;
} cs_insn;


/// Calculate the offset of a disassembled instruction in its buffer, given its position
/// in its array of disassembled insn
/// NOTE: this macro works with position (>=1), not index
#define CS_INSN_OFFSET(insns, post) (insns[post - 1].address - insns[0].address)


/// All type of errors encountered by Capstone API.
/// These are values returned by cs_errno()
typedef enum cs_err {
	CS_ERR_OK = 0,   ///< No error: everything was fine
	CS_ERR_MEM,      ///< Out-Of-Memory error: cs_open(), cs_disasm(), cs_disasm_iter()
	CS_ERR_ARCH,     ///< Unsupported architecture: cs_open()
	CS_ERR_HANDLE,   ///< Invalid handle: cs_op_count(), cs_op_index()
	CS_ERR_CSH,      ///< Invalid csh argument: cs_close(), cs_errno(), cs_option()
	CS_ERR_MODE,     ///< Invalid/unsupported mode: cs_open()
	CS_ERR_OPTION,   ///< Invalid/unsupported option: cs_option()
	CS_ERR_DETAIL,   ///< Information is unavailable because detail option is OFF
	CS_ERR_MEMSETUP, ///< Dynamic memory management uninitialized (see CS_OPT_MEM)
	CS_ERR_VERSION,  ///< Unsupported version (bindings)
	CS_ERR_DIET,     ///< Access irrelevant data in "diet" engine
	CS_ERR_SKIPDATA, ///< Access irrelevant data for "data" instruction in SKIPDATA mode
	CS_ERR_X86_ATT,  ///< X86 AT&T syntax is unsupported (opt-out at compile time)
	CS_ERR_X86_INTEL, ///< X86 Intel syntax is unsupported (opt-out at compile time)
	CS_ERR_X86_MASM, ///< X86 Masm syntax is unsupported (opt-out at compile time)
} cs_err;

/**
 Return combined API version & major and minor version numbers.

 @major: major number of API version
 @minor: minor number of API version

 @return hexical number as (major << 8 | minor), which encodes both
	 major & minor versions.
	 NOTE: This returned value can be compared with version number made
	 with macro CS_MAKE_VERSION

 For example, second API version would return 1 in @major, and 1 in @minor
 The return value would be 0x0101

 NOTE: if you only care about returned value, but not major and minor values,
 set both @major & @minor arguments to NULL.
*/
CAPSTONE_EXPORT
unsigned int CAPSTONE_API cs_version(int *major, int *minor);


/**
 This API can be used to either ask for archs supported by this library,
 or check to see if the library was compile with 'diet' option (or called
 in 'diet' mode).

 To check if a particular arch is supported by this library, set @query to
 arch mode (CS_ARCH_* value).
 To verify if this library supports all the archs, use CS_ARCH_ALL.

 To check if this library is in 'diet' mode, set @query to CS_SUPPORT_DIET.

 @return True if this library supports the given arch, or in 'diet' mode.
*/
CAPSTONE_EXPORT
bool CAPSTONE_API cs_support(int query);

/**
 Initialize CS handle: this must be done before any usage of CS.

 @arch: architecture type (CS_ARCH_*)
 @mode: hardware mode. This is combined of CS_MODE_*
 @handle: pointer to handle, which will be updated at return time

 @return CS_ERR_OK on success, or other value on failure (refer to cs_err enum
 for detailed error).
*/
CAPSTONE_EXPORT
cs_err CAPSTONE_API cs_open(cs_arch arch, cs_mode mode, csh *handle);

/**
 Close CS handle: MUST do to release the handle when it is not used anymore.
 NOTE: this must be only called when there is no longer usage of Capstone,
 not even access to cs_insn array. The reason is the this API releases some
 cached memory, thus access to any Capstone API after cs_close() might crash
 your application.

 In fact,this API invalidate @handle by ZERO out its value (i.e *handle = 0).

 @handle: pointer to a handle returned by cs_open()

 @return CS_ERR_OK on success, or other value on failure (refer to cs_err enum
 for detailed error).
*/
CAPSTONE_EXPORT
cs_err CAPSTONE_API cs_close(csh *handle);

/**
 Set option for disassembling engine at runtime

 @handle: handle returned by cs_open()
 @type: type of option to be set
 @value: option value corresponding with @type

 @return: CS_ERR_OK on success, or other value on failure.
 Refer to cs_err enum for detailed error.

 NOTE: in the case of CS_OPT_MEM, handle's value can be anything,
 so that cs_option(handle, CS_OPT_MEM, value) can (i.e must) be called
 even before cs_open()
*/
CAPSTONE_EXPORT
cs_err CAPSTONE_API cs_option(csh handle, cs_opt_type type, size_t value);

/**
 Report the last error number when some API function fail.
 Like glibc's errno, cs_errno might not retain its old value once accessed.

 @handle: handle returned by cs_open()

 @return: error code of cs_err enum type (CS_ERR_*, see above)
*/
CAPSTONE_EXPORT
cs_err CAPSTONE_API cs_errno(csh handle);


/**
 Return a string describing given error code.

 @code: error code (see CS_ERR_* above)

 @return: returns a pointer to a string that describes the error code
	passed in the argument @code
*/
CAPSTONE_EXPORT
const char * CAPSTONE_API cs_strerror(cs_err code);

/**
 Disassemble binary code, given the code buffer, size, address and number
 of instructions to be decoded.
 This API dynamically allocate memory to contain disassembled instruction.
 Resulting instructions will be put into @*insn

 NOTE 1: this API will automatically determine memory needed to contain
 output disassembled instructions in @insn.

 NOTE 2: caller must free the allocated memory itself to avoid memory leaking.

 NOTE 3: for system with scarce memory to be dynamically allocated such as
 OS kernel or firmware, the API cs_disasm_iter() might be a better choice than
 cs_disasm(). The reason is that with cs_disasm(), based on limited available
 memory, we have to calculate in advance how many instructions to be disassembled,
 which complicates things. This is especially troublesome for the case @count=0,
 when cs_disasm() runs uncontrollably (until either end of input buffer, or
 when it encounters an invalid instruction).
 
 @handle: handle returned by cs_open()
 @code: buffer containing raw binary code to be disassembled.
 @code_size: size of the above code buffer.
 @address: address of the first instruction in given raw code buffer.
 @insn: array of instructions filled in by this API.
	   NOTE: @insn will be allocated by this function, and should be freed
	   with cs_free() API.
 @count: number of instructions to be disassembled, or 0 to get all of them

 @return: the number of successfully disassembled instructions,
 or 0 if this function failed to disassemble the given code

 On failure, call cs_errno() for error code.
*/
CAPSTONE_EXPORT
size_t CAPSTONE_API cs_disasm(csh handle,
		const uint8_t *code, size_t code_size,
		uint64_t address,
		size_t count,
		cs_insn **insn);

/**
  Deprecated function - to be retired in the next version!
  Use cs_disasm() instead of cs_disasm_ex()
*/
CAPSTONE_EXPORT
CAPSTONE_DEPRECATED
size_t CAPSTONE_API cs_disasm_ex(csh handle,
		const uint8_t *code, size_t code_size,
		uint64_t address,
		size_t count,
		cs_insn **insn);

/**
 Free memory allocated by cs_malloc() or cs_disasm() (argument @insn)

 @insn: pointer returned by @insn argument in cs_disasm() or cs_malloc()
 @count: number of cs_insn structures returned by cs_disasm(), or 1
     to free memory allocated by cs_malloc().
*/
CAPSTONE_EXPORT
void CAPSTONE_API cs_free(cs_insn *insn, size_t count);


/**
 Allocate memory for 1 instruction to be used by cs_disasm_iter().

 @handle: handle returned by cs_open()

 NOTE: when no longer in use, you can reclaim the memory allocated for
 this instruction with cs_free(insn, 1)
*/
CAPSTONE_EXPORT
cs_insn * CAPSTONE_API cs_malloc(csh handle);

/**
 Fast API to disassemble binary code, given the code buffer, size, address
 and number of instructions to be decoded.
 This API puts the resulting instruction into a given cache in @insn.
 See tests/test_iter.c for sample code demonstrating this API.

 NOTE 1: this API will update @code, @size & @address to point to the next
 instruction in the input buffer. Therefore, it is convenient to use
 cs_disasm_iter() inside a loop to quickly iterate all the instructions.
 While decoding one instruction at a time can also be achieved with
 cs_disasm(count=1), some benchmarks shown that cs_disasm_iter() can be 30%
 faster on random input.

 NOTE 2: the cache in @insn can be created with cs_malloc() API.

 NOTE 3: for system with scarce memory to be dynamically allocated such as
 OS kernel or firmware, this API is recommended over cs_disasm(), which
 allocates memory based on the number of instructions to be disassembled.
 The reason is that with cs_disasm(), based on limited available memory,
 we have to calculate in advance how many instructions to be disassembled,
 which complicates things. This is especially troublesome for the case
 @count=0, when cs_disasm() runs uncontrollably (until either end of input
 buffer, or when it encounters an invalid instruction).
 
 @handle: handle returned by cs_open()
 @code: buffer containing raw binary code to be disassembled
 @size: size of above code
 @address: address of the first insn in given raw code buffer
 @insn: pointer to instruction to be filled in by this API.

 @return: true if this API successfully decode 1 instruction,
 or false otherwise.

 On failure, call cs_errno() for error code.
*/
CAPSTONE_EXPORT
bool CAPSTONE_API cs_disasm_iter(csh handle,
	const uint8_t **code, size_t *size,
	uint64_t *address, cs_insn *insn);

/**
 Return friendly name of register in a string.
 Find the instruction id from header file of corresponding architecture (arm.h for ARM,
 x86.h for X86, ...)

 WARN: when in 'diet' mode, this API is irrelevant because engine does not
 store register name.

 @handle: handle returned by cs_open()
 @reg_id: register id

 @return: string name of the register, or NULL if @reg_id is invalid.
*/
CAPSTONE_EXPORT
const char * CAPSTONE_API cs_reg_name(csh handle, unsigned int reg_id);

/**
 Return friendly name of an instruction in a string.
 Find the instruction id from header file of corresponding architecture (arm.h for ARM, x86.h for X86, ...)

 WARN: when in 'diet' mode, this API is irrelevant because the engine does not
 store instruction name.

 @handle: handle returned by cs_open()
 @insn_id: instruction id

 @return: string name of the instruction, or NULL if @insn_id is invalid.
*/
CAPSTONE_EXPORT
const char * CAPSTONE_API cs_insn_name(csh handle, unsigned int insn_id);

/**
 Return friendly name of a group id (that an instruction can belong to)
 Find the group id from header file of corresponding architecture (arm.h for ARM, x86.h for X86, ...)

 WARN: when in 'diet' mode, this API is irrelevant because the engine does not
 store group name.

 @handle: handle returned by cs_open()
 @group_id: group id

 @return: string name of the group, or NULL if @group_id is invalid.
*/
CAPSTONE_EXPORT
const char * CAPSTONE_API cs_group_name(csh handle, unsigned int group_id);

/**
 Check if a disassembled instruction belong to a particular group.
 Find the group id from header file of corresponding architecture (arm.h for ARM, x86.h for X86, ...)
 Internally, this simply verifies if @group_id matches any member of insn->groups array.

 NOTE: this API is only valid when detail option is ON (which is OFF by default).

 WARN: when in 'diet' mode, this API is irrelevant because the engine does not
 update @groups array.

 @handle: handle returned by cs_open()
 @insn: disassembled instruction structure received from cs_disasm() or cs_disasm_iter()
 @group_id: group that you want to check if this instruction belong to.

 @return: true if this instruction indeed belongs to the given group, or false otherwise.
*/
CAPSTONE_EXPORT
bool CAPSTONE_API cs_insn_group(csh handle, const cs_insn *insn, unsigned int group_id);

/**
 Check if a disassembled instruction IMPLICITLY used a particular register.
 Find the register id from header file of corresponding architecture (arm.h for ARM, x86.h for X86, ...)
 Internally, this simply verifies if @reg_id matches any member of insn->regs_read array.

 NOTE: this API is only valid when detail option is ON (which is OFF by default)

 WARN: when in 'diet' mode, this API is irrelevant because the engine does not
 update @regs_read array.

 @insn: disassembled instruction structure received from cs_disasm() or cs_disasm_iter()
 @reg_id: register that you want to check if this instruction used it.

 @return: true if this instruction indeed implicitly used the given register, or false otherwise.
*/
CAPSTONE_EXPORT
bool CAPSTONE_API cs_reg_read(csh handle, const cs_insn *insn, unsigned int reg_id);

/**
 Check if a disassembled instruction IMPLICITLY modified a particular register.
 Find the register id from header file of corresponding architecture (arm.h for ARM, x86.h for X86, ...)
 Internally, this simply verifies if @reg_id matches any member of insn->regs_write array.

 NOTE: this API is only valid when detail option is ON (which is OFF by default)

 WARN: when in 'diet' mode, this API is irrelevant because the engine does not
 update @regs_write array.

 @insn: disassembled instruction structure received from cs_disasm() or cs_disasm_iter()
 @reg_id: register that you want to check if this instruction modified it.

 @return: true if this instruction indeed implicitly modified the given register, or false otherwise.
*/
CAPSTONE_EXPORT
bool CAPSTONE_API cs_reg_write(csh handle, const cs_insn *insn, unsigned int reg_id);

/**
 Count the number of operands of a given type.
 Find the operand type in header file of corresponding architecture (arm.h for ARM, x86.h for X86, ...)

 NOTE: this API is only valid when detail option is ON (which is OFF by default)

 @handle: handle returned by cs_open()
 @insn: disassembled instruction structure received from cs_disasm() or cs_disasm_iter()
 @op_type: Operand type to be found.

 @return: number of operands of given type @op_type in instruction @insn,
 or -1 on failure.
*/
CAPSTONE_EXPORT
int CAPSTONE_API cs_op_count(csh handle, const cs_insn *insn, unsigned int op_type);

/**
 Retrieve the position of operand of given type in <arch>.operands[] array.
 Later, the operand can be accessed using the returned position.
 Find the operand type in header file of corresponding architecture (arm.h for ARM, x86.h for X86, ...)

 NOTE: this API is only valid when detail option is ON (which is OFF by default)

 @handle: handle returned by cs_open()
 @insn: disassembled instruction structure received from cs_disasm() or cs_disasm_iter()
 @op_type: Operand type to be found.
 @position: position of the operand to be found. This must be in the range
			[1, cs_op_count(handle, insn, op_type)]

 @return: index of operand of given type @op_type in <arch>.operands[] array
 in instruction @insn, or -1 on failure.
*/
CAPSTONE_EXPORT
int CAPSTONE_API cs_op_index(csh handle, const cs_insn *insn, unsigned int op_type,
		unsigned int position);

/// Type of array to keep the list of registers
typedef uint16_t cs_regs[64];

/**
 Retrieve all the registers accessed by an instruction, either explicitly or
 implicitly.

 WARN: when in 'diet' mode, this API is irrelevant because engine does not
 store registers.

 @handle: handle returned by cs_open()
 @insn: disassembled instruction structure returned from cs_disasm() or cs_disasm_iter()
 @regs_read: on return, this array contains all registers read by instruction.
 @regs_read_count: number of registers kept inside @regs_read array.
 @regs_write: on return, this array contains all registers written by instruction.
 @regs_write_count: number of registers kept inside @regs_write array.

 @return CS_ERR_OK on success, or other value on failure (refer to cs_err enum
 for detailed error).
*/
CAPSTONE_EXPORT
cs_err CAPSTONE_API cs_regs_access(csh handle, const cs_insn *insn,
		cs_regs regs_read, uint8_t *regs_read_count,
		cs_regs regs_write, uint8_t *regs_write_count);

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/evm.h`:

```h
#ifndef CAPSTONE_EVM_H
#define CAPSTONE_EVM_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2013-2018 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// Instruction structure
typedef struct cs_evm {
    unsigned char pop;    ///< number of items popped from the stack
    unsigned char push;   ///< number of items pushed into the stack
    unsigned int  fee;    ///< gas fee for the instruction
} cs_evm;

/// EVM instruction
typedef enum evm_insn {
	EVM_INS_STOP = 0,
	EVM_INS_ADD = 1,
	EVM_INS_MUL = 2,
	EVM_INS_SUB = 3,
	EVM_INS_DIV = 4,
	EVM_INS_SDIV = 5,
	EVM_INS_MOD = 6,
	EVM_INS_SMOD = 7,
	EVM_INS_ADDMOD = 8,
	EVM_INS_MULMOD = 9,
	EVM_INS_EXP = 10,
	EVM_INS_SIGNEXTEND = 11,
	EVM_INS_LT = 16,
	EVM_INS_GT = 17,
	EVM_INS_SLT = 18,
	EVM_INS_SGT = 19,
	EVM_INS_EQ = 20,
	EVM_INS_ISZERO = 21,
	EVM_INS_AND = 22,
	EVM_INS_OR = 23,
	EVM_INS_XOR = 24,
	EVM_INS_NOT = 25,
	EVM_INS_BYTE = 26,
	EVM_INS_SHA3 = 32,
	EVM_INS_ADDRESS = 48,
	EVM_INS_BALANCE = 49,
	EVM_INS_ORIGIN = 50,
	EVM_INS_CALLER = 51,
	EVM_INS_CALLVALUE = 52,
	EVM_INS_CALLDATALOAD = 53,
	EVM_INS_CALLDATASIZE = 54,
	EVM_INS_CALLDATACOPY = 55,
	EVM_INS_CODESIZE = 56,
	EVM_INS_CODECOPY = 57,
	EVM_INS_GASPRICE = 58,
	EVM_INS_EXTCODESIZE = 59,
	EVM_INS_EXTCODECOPY = 60,
	EVM_INS_RETURNDATASIZE = 61,
	EVM_INS_RETURNDATACOPY = 62,
	EVM_INS_BLOCKHASH = 64,
	EVM_INS_COINBASE = 65,
	EVM_INS_TIMESTAMP = 66,
	EVM_INS_NUMBER = 67,
	EVM_INS_DIFFICULTY = 68,
	EVM_INS_GASLIMIT = 69,
	EVM_INS_POP = 80,
	EVM_INS_MLOAD = 81,
	EVM_INS_MSTORE = 82,
	EVM_INS_MSTORE8 = 83,
	EVM_INS_SLOAD = 84,
	EVM_INS_SSTORE = 85,
	EVM_INS_JUMP = 86,
	EVM_INS_JUMPI = 87,
	EVM_INS_PC = 88,
	EVM_INS_MSIZE = 89,
	EVM_INS_GAS = 90,
	EVM_INS_JUMPDEST = 91,
	EVM_INS_PUSH1 = 96,
	EVM_INS_PUSH2 = 97,
	EVM_INS_PUSH3 = 98,
	EVM_INS_PUSH4 = 99,
	EVM_INS_PUSH5 = 100,
	EVM_INS_PUSH6 = 101,
	EVM_INS_PUSH7 = 102,
	EVM_INS_PUSH8 = 103,
	EVM_INS_PUSH9 = 104,
	EVM_INS_PUSH10 = 105,
	EVM_INS_PUSH11 = 106,
	EVM_INS_PUSH12 = 107,
	EVM_INS_PUSH13 = 108,
	EVM_INS_PUSH14 = 109,
	EVM_INS_PUSH15 = 110,
	EVM_INS_PUSH16 = 111,
	EVM_INS_PUSH17 = 112,
	EVM_INS_PUSH18 = 113,
	EVM_INS_PUSH19 = 114,
	EVM_INS_PUSH20 = 115,
	EVM_INS_PUSH21 = 116,
	EVM_INS_PUSH22 = 117,
	EVM_INS_PUSH23 = 118,
	EVM_INS_PUSH24 = 119,
	EVM_INS_PUSH25 = 120,
	EVM_INS_PUSH26 = 121,
	EVM_INS_PUSH27 = 122,
	EVM_INS_PUSH28 = 123,
	EVM_INS_PUSH29 = 124,
	EVM_INS_PUSH30 = 125,
	EVM_INS_PUSH31 = 126,
	EVM_INS_PUSH32 = 127,
	EVM_INS_DUP1 = 128,
	EVM_INS_DUP2 = 129,
	EVM_INS_DUP3 = 130,
	EVM_INS_DUP4 = 131,
	EVM_INS_DUP5 = 132,
	EVM_INS_DUP6 = 133,
	EVM_INS_DUP7 = 134,
	EVM_INS_DUP8 = 135,
	EVM_INS_DUP9 = 136,
	EVM_INS_DUP10 = 137,
	EVM_INS_DUP11 = 138,
	EVM_INS_DUP12 = 139,
	EVM_INS_DUP13 = 140,
	EVM_INS_DUP14 = 141,
	EVM_INS_DUP15 = 142,
	EVM_INS_DUP16 = 143,
	EVM_INS_SWAP1 = 144,
	EVM_INS_SWAP2 = 145,
	EVM_INS_SWAP3 = 146,
	EVM_INS_SWAP4 = 147,
	EVM_INS_SWAP5 = 148,
	EVM_INS_SWAP6 = 149,
	EVM_INS_SWAP7 = 150,
	EVM_INS_SWAP8 = 151,
	EVM_INS_SWAP9 = 152,
	EVM_INS_SWAP10 = 153,
	EVM_INS_SWAP11 = 154,
	EVM_INS_SWAP12 = 155,
	EVM_INS_SWAP13 = 156,
	EVM_INS_SWAP14 = 157,
	EVM_INS_SWAP15 = 158,
	EVM_INS_SWAP16 = 159,
	EVM_INS_LOG0 = 160,
	EVM_INS_LOG1 = 161,
	EVM_INS_LOG2 = 162,
	EVM_INS_LOG3 = 163,
	EVM_INS_LOG4 = 164,
	EVM_INS_CREATE = 240,
	EVM_INS_CALL = 241,
	EVM_INS_CALLCODE = 242,
	EVM_INS_RETURN = 243,
	EVM_INS_DELEGATECALL = 244,
	EVM_INS_CALLBLACKBOX = 245,
	EVM_INS_STATICCALL = 250,
	EVM_INS_REVERT = 253,
	EVM_INS_SUICIDE = 255,

	EVM_INS_INVALID = 512,
	EVM_INS_ENDING,   // <-- mark the end of the list of instructions
} evm_insn;

/// Group of EVM instructions
typedef enum evm_insn_group {
	EVM_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	EVM_GRP_JUMP,          ///< all jump instructions

	EVM_GRP_MATH = 8,      ///< math instructions
	EVM_GRP_STACK_WRITE,   ///< instructions write to stack
	EVM_GRP_STACK_READ,    ///< instructions read from stack
	EVM_GRP_MEM_WRITE,     ///< instructions write to memory
	EVM_GRP_MEM_READ,      ///< instructions read from memory
	EVM_GRP_STORE_WRITE,   ///< instructions write to storage
	EVM_GRP_STORE_READ,    ///< instructions read from storage
	EVM_GRP_HALT,    ///< instructions halt execution

	EVM_GRP_ENDING,   ///< <-- mark the end of the list of groups
} evm_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/m680x.h`:

```h
#ifndef CAPSTONE_M680X_H
#define CAPSTONE_M680X_H

/* Capstone Disassembly Engine */
/* M680X Backend by Wolfgang Schwotzer <wolfgang.schwotzer@gmx.net> 2017 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

#define M680X_OPERAND_COUNT 9

/// M680X registers and special registers
typedef enum m680x_reg {
	M680X_REG_INVALID = 0,

	M680X_REG_A, ///< M6800/1/2/3/9, HD6301/9
	M680X_REG_B, ///< M6800/1/2/3/9, HD6301/9
	M680X_REG_E, ///< HD6309
	M680X_REG_F, ///< HD6309
	M680X_REG_0, ///< HD6309

	M680X_REG_D, ///< M6801/3/9, HD6301/9
	M680X_REG_W, ///< HD6309

	M680X_REG_CC, ///< M6800/1/2/3/9, M6301/9
	M680X_REG_DP, ///< M6809/M6309
	M680X_REG_MD, ///< M6309

	M680X_REG_HX, ///< M6808
	M680X_REG_H, ///< M6808
	M680X_REG_X, ///< M6800/1/2/3/9, M6301/9
	M680X_REG_Y, ///< M6809/M6309
	M680X_REG_S, ///< M6809/M6309
	M680X_REG_U, ///< M6809/M6309
	M680X_REG_V, ///< M6309

	M680X_REG_Q, ///< M6309

	M680X_REG_PC, ///< M6800/1/2/3/9, M6301/9

	M680X_REG_TMP2, ///< CPU12
	M680X_REG_TMP3, ///< CPU12

	M680X_REG_ENDING,   ///< <-- mark the end of the list of registers
} m680x_reg;

/// Operand type for instruction's operands
typedef enum m680x_op_type {
	M680X_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	M680X_OP_REGISTER,    ///< = Register operand.
	M680X_OP_IMMEDIATE,   ///< = Immediate operand.
	M680X_OP_INDEXED,     ///< = Indexed addressing operand.
	M680X_OP_EXTENDED,    ///< = Extended addressing operand.
	M680X_OP_DIRECT,      ///< = Direct addressing operand.
	M680X_OP_RELATIVE,    ///< = Relative addressing operand.
	M680X_OP_CONSTANT,    ///< = constant operand (Displayed as number only).
				///< Used e.g. for a bit index or page number.
} m680x_op_type;

// Supported bit values for mem.idx.offset_bits
#define M680X_OFFSET_NONE      0
#define M680X_OFFSET_BITS_5    5
#define M680X_OFFSET_BITS_8    8
#define M680X_OFFSET_BITS_9    9
#define M680X_OFFSET_BITS_16  16

// Supported bit flags for mem.idx.flags
// These flags can be combined
#define M680X_IDX_INDIRECT     1
#define M680X_IDX_NO_COMMA     2
#define M680X_IDX_POST_INC_DEC 4

/// Instruction's operand referring to indexed addressing
typedef struct m680x_op_idx {
	m680x_reg base_reg;	///< base register (or M680X_REG_INVALID if
				///< irrelevant)
	m680x_reg offset_reg;	///< offset register (or M680X_REG_INVALID if
				///< irrelevant)
	int16_t offset;		///< 5-,8- or 16-bit offset. See also offset_bits.
	uint16_t offset_addr;	///< = offset addr. if base_reg == M680X_REG_PC.
				///< calculated as offset + PC
	uint8_t offset_bits;	///< offset width in bits for indexed addressing
	int8_t inc_dec;		///< inc. or dec. value:
				///<    0: no inc-/decrement
				///<    1 .. 8: increment by 1 .. 8
				///<    -1 .. -8: decrement by 1 .. 8
				///< if flag M680X_IDX_POST_INC_DEC set it is post
				///< inc-/decrement otherwise pre inc-/decrement
	uint8_t flags;		///< 8-bit flags (see above)
} m680x_op_idx;

/// Instruction's memory operand referring to relative addressing (Bcc/LBcc)
typedef struct m680x_op_rel {
	uint16_t address;	///< The absolute address.
				///< calculated as PC + offset. PC is the first
				///< address after the instruction.
	int16_t offset;		///< the offset/displacement value
} m680x_op_rel;

/// Instruction's operand referring to extended addressing
typedef struct m680x_op_ext {
	uint16_t address;      ///< The absolute address
	bool indirect;         ///< true if extended indirect addressing
} m680x_op_ext;

/// Instruction operand
typedef struct cs_m680x_op {
	m680x_op_type type;
	union {
		int32_t imm;		///< immediate value for IMM operand
		m680x_reg reg;		///< register value for REG operand
		m680x_op_idx idx;	///< Indexed addressing operand
		m680x_op_rel rel;	///< Relative address. operand (Bcc/LBcc)
		m680x_op_ext ext;	///< Extended address
		uint8_t direct_addr;	///<</ Direct address (lower 8-bit)
		uint8_t const_val;	///< constant value (bit index, page nr.)
	};
	uint8_t size;			///< size of this operand (in bytes)
	/// How is this operand accessed? (READ, WRITE or READ|WRITE)
	/// This field is combined of cs_ac_type.
	/// NOTE: this field is irrelevant if engine is compiled in DIET 
	uint8_t access;
} cs_m680x_op;

/// Group of M680X instructions
typedef enum m680x_group_type {
	M680X_GRP_INVALID = 0,	/// = CS_GRP_INVALID
	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	M680X_GRP_JUMP,		///< = CS_GRP_JUMP
	// all call instructions
	M680X_GRP_CALL,		///< = CS_GRP_CALL
	// all return instructions
	M680X_GRP_RET,		///< = CS_GRP_RET
	// all interrupt instructions (int+syscall)
	M680X_GRP_INT,		///< = CS_GRP_INT
	// all interrupt return instructions
	M680X_GRP_IRET,		///< = CS_GRP_IRET
	// all privileged instructions
	M680X_GRP_PRIV,		///< = CS_GRP_PRIVILEDGE; not used
	// all relative branching instructions
	M680X_GRP_BRAREL,	///< = CS_GRP_BRANCH_RELATIVE

	// Architecture-specific groups
	M680X_GRP_ENDING,	// <-- mark the end of the list of groups
} m680x_group_type;

// M680X instruction flags:

/// The first (register) operand is part of the
/// instruction mnemonic
#define M680X_FIRST_OP_IN_MNEM    1
/// The second (register) operand is part of the
/// instruction mnemonic
#define M680X_SECOND_OP_IN_MNEM   2

/// The M680X instruction and it's operands
typedef struct cs_m680x {
	uint8_t flags;		///< See: M680X instruction flags
	uint8_t op_count;	///< number of operands for the instruction or 0
	cs_m680x_op operands[M680X_OPERAND_COUNT]; ///< operands for this insn.
} cs_m680x;

/// M680X instruction IDs
typedef enum m680x_insn {
	M680X_INS_INVLD = 0,
	M680X_INS_ABA, ///< M6800/1/2/3
	M680X_INS_ABX,
	M680X_INS_ABY,
	M680X_INS_ADC,
	M680X_INS_ADCA,
	M680X_INS_ADCB,
	M680X_INS_ADCD,
	M680X_INS_ADCR,
	M680X_INS_ADD,
	M680X_INS_ADDA,
	M680X_INS_ADDB,
	M680X_INS_ADDD,
	M680X_INS_ADDE,
	M680X_INS_ADDF,
	M680X_INS_ADDR,
	M680X_INS_ADDW,
	M680X_INS_AIM,
	M680X_INS_AIS,
	M680X_INS_AIX,
	M680X_INS_AND,
	M680X_INS_ANDA,
	M680X_INS_ANDB,
	M680X_INS_ANDCC,
	M680X_INS_ANDD,
	M680X_INS_ANDR,
	M680X_INS_ASL,
	M680X_INS_ASLA,
	M680X_INS_ASLB,
	M680X_INS_ASLD, ///< or LSLD
	M680X_INS_ASR,
	M680X_INS_ASRA,
	M680X_INS_ASRB,
	M680X_INS_ASRD,
	M680X_INS_ASRX,
	M680X_INS_BAND,
	M680X_INS_BCC, ///< or BHS
	M680X_INS_BCLR,
	M680X_INS_BCS, ///< or BLO
	M680X_INS_BEOR,
	M680X_INS_BEQ,
	M680X_INS_BGE,
	M680X_INS_BGND,
	M680X_INS_BGT,
	M680X_INS_BHCC,
	M680X_INS_BHCS,
	M680X_INS_BHI,
	M680X_INS_BIAND,
	M680X_INS_BIEOR,
	M680X_INS_BIH,
	M680X_INS_BIL,
	M680X_INS_BIOR,
	M680X_INS_BIT,
	M680X_INS_BITA,
	M680X_INS_BITB,
	M680X_INS_BITD,
	M680X_INS_BITMD,
	M680X_INS_BLE,
	M680X_INS_BLS,
	M680X_INS_BLT,
	M680X_INS_BMC,
	M680X_INS_BMI,
	M680X_INS_BMS,
	M680X_INS_BNE,
	M680X_INS_BOR,
	M680X_INS_BPL,
	M680X_INS_BRCLR,
	M680X_INS_BRSET,
	M680X_INS_BRA,
	M680X_INS_BRN,
	M680X_INS_BSET,
	M680X_INS_BSR,
	M680X_INS_BVC,
	M680X_INS_BVS,
	M680X_INS_CALL,
	M680X_INS_CBA, ///< M6800/1/2/3
	M680X_INS_CBEQ,
	M680X_INS_CBEQA,
	M680X_INS_CBEQX,
	M680X_INS_CLC, ///< M6800/1/2/3
	M680X_INS_CLI, ///< M6800/1/2/3
	M680X_INS_CLR,
	M680X_INS_CLRA,
	M680X_INS_CLRB,
	M680X_INS_CLRD,
	M680X_INS_CLRE,
	M680X_INS_CLRF,
	M680X_INS_CLRH,
	M680X_INS_CLRW,
	M680X_INS_CLRX,
	M680X_INS_CLV, ///< M6800/1/2/3
	M680X_INS_CMP,
	M680X_INS_CMPA,
	M680X_INS_CMPB,
	M680X_INS_CMPD,
	M680X_INS_CMPE,
	M680X_INS_CMPF,
	M680X_INS_CMPR,
	M680X_INS_CMPS,
	M680X_INS_CMPU,
	M680X_INS_CMPW,
	M680X_INS_CMPX,
	M680X_INS_CMPY,
	M680X_INS_COM,
	M680X_INS_COMA,
	M680X_INS_COMB,
	M680X_INS_COMD,
	M680X_INS_COME,
	M680X_INS_COMF,
	M680X_INS_COMW,
	M680X_INS_COMX,
	M680X_INS_CPD,
	M680X_INS_CPHX,
	M680X_INS_CPS,
	M680X_INS_CPX, ///< M6800/1/2/3
	M680X_INS_CPY,
	M680X_INS_CWAI,
	M680X_INS_DAA,
	M680X_INS_DBEQ,
	M680X_INS_DBNE,
	M680X_INS_DBNZ,
	M680X_INS_DBNZA,
	M680X_INS_DBNZX,
	M680X_INS_DEC,
	M680X_INS_DECA,
	M680X_INS_DECB,
	M680X_INS_DECD,
	M680X_INS_DECE,
	M680X_INS_DECF,
	M680X_INS_DECW,
	M680X_INS_DECX,
	M680X_INS_DES, ///< M6800/1/2/3
	M680X_INS_DEX, ///< M6800/1/2/3
	M680X_INS_DEY,
	M680X_INS_DIV,
	M680X_INS_DIVD,
	M680X_INS_DIVQ,
	M680X_INS_EDIV,
	M680X_INS_EDIVS,
	M680X_INS_EIM,
	M680X_INS_EMACS,
	M680X_INS_EMAXD,
	M680X_INS_EMAXM,
	M680X_INS_EMIND,
	M680X_INS_EMINM,
	M680X_INS_EMUL,
	M680X_INS_EMULS,
	M680X_INS_EOR,
	M680X_INS_EORA,
	M680X_INS_EORB,
	M680X_INS_EORD,
	M680X_INS_EORR,
	M680X_INS_ETBL,
	M680X_INS_EXG,
	M680X_INS_FDIV,
	M680X_INS_IBEQ,
	M680X_INS_IBNE,
	M680X_INS_IDIV,
	M680X_INS_IDIVS,
	M680X_INS_ILLGL,
	M680X_INS_INC,
	M680X_INS_INCA,
	M680X_INS_INCB,
	M680X_INS_INCD,
	M680X_INS_INCE,
	M680X_INS_INCF,
	M680X_INS_INCW,
	M680X_INS_INCX,
	M680X_INS_INS, ///< M6800/1/2/3
	M680X_INS_INX, ///< M6800/1/2/3
	M680X_INS_INY,
	M680X_INS_JMP,
	M680X_INS_JSR,
	M680X_INS_LBCC, ///< or LBHS
	M680X_INS_LBCS, ///< or LBLO
	M680X_INS_LBEQ,
	M680X_INS_LBGE,
	M680X_INS_LBGT,
	M680X_INS_LBHI,
	M680X_INS_LBLE,
	M680X_INS_LBLS,
	M680X_INS_LBLT,
	M680X_INS_LBMI,
	M680X_INS_LBNE,
	M680X_INS_LBPL,
	M680X_INS_LBRA,
	M680X_INS_LBRN,
	M680X_INS_LBSR,
	M680X_INS_LBVC,
	M680X_INS_LBVS,
	M680X_INS_LDA,
	M680X_INS_LDAA, ///< M6800/1/2/3
	M680X_INS_LDAB, ///< M6800/1/2/3
	M680X_INS_LDB,
	M680X_INS_LDBT,
	M680X_INS_LDD,
	M680X_INS_LDE,
	M680X_INS_LDF,
	M680X_INS_LDHX,
	M680X_INS_LDMD,
	M680X_INS_LDQ,
	M680X_INS_LDS,
	M680X_INS_LDU,
	M680X_INS_LDW,
	M680X_INS_LDX,
	M680X_INS_LDY,
	M680X_INS_LEAS,
	M680X_INS_LEAU,
	M680X_INS_LEAX,
	M680X_INS_LEAY,
	M680X_INS_LSL,
	M680X_INS_LSLA,
	M680X_INS_LSLB,
	M680X_INS_LSLD,
	M680X_INS_LSLX,
	M680X_INS_LSR,
	M680X_INS_LSRA,
	M680X_INS_LSRB,
	M680X_INS_LSRD, ///< or ASRD
	M680X_INS_LSRW,
	M680X_INS_LSRX,
	M680X_INS_MAXA,
	M680X_INS_MAXM,
	M680X_INS_MEM,
	M680X_INS_MINA,
	M680X_INS_MINM,
	M680X_INS_MOV,
	M680X_INS_MOVB,
	M680X_INS_MOVW,
	M680X_INS_MUL,
	M680X_INS_MULD,
	M680X_INS_NEG,
	M680X_INS_NEGA,
	M680X_INS_NEGB,
	M680X_INS_NEGD,
	M680X_INS_NEGX,
	M680X_INS_NOP,
	M680X_INS_NSA,
	M680X_INS_OIM,
	M680X_INS_ORA,
	M680X_INS_ORAA, ///< M6800/1/2/3
	M680X_INS_ORAB, ///< M6800/1/2/3
	M680X_INS_ORB,
	M680X_INS_ORCC,
	M680X_INS_ORD,
	M680X_INS_ORR,
	M680X_INS_PSHA, ///< M6800/1/2/3
	M680X_INS_PSHB, ///< M6800/1/2/3
	M680X_INS_PSHC,
	M680X_INS_PSHD,
	M680X_INS_PSHH,
	M680X_INS_PSHS,
	M680X_INS_PSHSW,
	M680X_INS_PSHU,
	M680X_INS_PSHUW,
	M680X_INS_PSHX, ///< M6800/1/2/3
	M680X_INS_PSHY,
	M680X_INS_PULA, ///< M6800/1/2/3
	M680X_INS_PULB, ///< M6800/1/2/3
	M680X_INS_PULC,
	M680X_INS_PULD,
	M680X_INS_PULH,
	M680X_INS_PULS,
	M680X_INS_PULSW,
	M680X_INS_PULU,
	M680X_INS_PULUW,
	M680X_INS_PULX, ///< M6800/1/2/3
	M680X_INS_PULY,
	M680X_INS_REV,
	M680X_INS_REVW,
	M680X_INS_ROL,
	M680X_INS_ROLA,
	M680X_INS_ROLB,
	M680X_INS_ROLD,
	M680X_INS_ROLW,
	M680X_INS_ROLX,
	M680X_INS_ROR,
	M680X_INS_RORA,
	M680X_INS_RORB,
	M680X_INS_RORD,
	M680X_INS_RORW,
	M680X_INS_RORX,
	M680X_INS_RSP,
	M680X_INS_RTC,
	M680X_INS_RTI,
	M680X_INS_RTS,
	M680X_INS_SBA, ///< M6800/1/2/3
	M680X_INS_SBC,
	M680X_INS_SBCA,
	M680X_INS_SBCB,
	M680X_INS_SBCD,
	M680X_INS_SBCR,
	M680X_INS_SEC,
	M680X_INS_SEI,
	M680X_INS_SEV,
	M680X_INS_SEX,
	M680X_INS_SEXW,
	M680X_INS_SLP,
	M680X_INS_STA,
	M680X_INS_STAA, ///< M6800/1/2/3
	M680X_INS_STAB, ///< M6800/1/2/3
	M680X_INS_STB,
	M680X_INS_STBT,
	M680X_INS_STD,
	M680X_INS_STE,
	M680X_INS_STF,
	M680X_INS_STOP,
	M680X_INS_STHX,
	M680X_INS_STQ,
	M680X_INS_STS,
	M680X_INS_STU,
	M680X_INS_STW,
	M680X_INS_STX,
	M680X_INS_STY,
	M680X_INS_SUB,
	M680X_INS_SUBA,
	M680X_INS_SUBB,
	M680X_INS_SUBD,
	M680X_INS_SUBE,
	M680X_INS_SUBF,
	M680X_INS_SUBR,
	M680X_INS_SUBW,
	M680X_INS_SWI,
	M680X_INS_SWI2,
	M680X_INS_SWI3,
	M680X_INS_SYNC,
	M680X_INS_TAB, ///< M6800/1/2/3
	M680X_INS_TAP, ///< M6800/1/2/3
	M680X_INS_TAX,
	M680X_INS_TBA, ///< M6800/1/2/3
	M680X_INS_TBEQ,
	M680X_INS_TBL,
	M680X_INS_TBNE,
	M680X_INS_TEST,
	M680X_INS_TFM,
	M680X_INS_TFR,
	M680X_INS_TIM,
	M680X_INS_TPA, ///< M6800/1/2/3
	M680X_INS_TST,
	M680X_INS_TSTA,
	M680X_INS_TSTB,
	M680X_INS_TSTD,
	M680X_INS_TSTE,
	M680X_INS_TSTF,
	M680X_INS_TSTW,
	M680X_INS_TSTX,
	M680X_INS_TSX, ///< M6800/1/2/3
	M680X_INS_TSY,
	M680X_INS_TXA,
	M680X_INS_TXS, ///< M6800/1/2/3
	M680X_INS_TYS,
	M680X_INS_WAI, ///< M6800/1/2/3
	M680X_INS_WAIT,
	M680X_INS_WAV,
	M680X_INS_WAVR,
	M680X_INS_XGDX, ///< HD6301
	M680X_INS_XGDY,
	M680X_INS_ENDING,   // <-- mark the end of the list of instructions
} m680x_insn;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/m68k.h`:

```h
#ifndef CAPSTONE_M68K_H
#define CAPSTONE_M68K_H

/* Capstone Disassembly Engine */
/* By Daniel Collin <daniel@collin.com>, 2015-2016 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

#define M68K_OPERAND_COUNT 4

/// M68K registers and special registers
typedef enum m68k_reg {
	M68K_REG_INVALID = 0,

	M68K_REG_D0,
	M68K_REG_D1,
	M68K_REG_D2,
	M68K_REG_D3,
	M68K_REG_D4,
	M68K_REG_D5,
	M68K_REG_D6,
	M68K_REG_D7,

	M68K_REG_A0,
	M68K_REG_A1,
	M68K_REG_A2,
	M68K_REG_A3,
	M68K_REG_A4,
	M68K_REG_A5,
	M68K_REG_A6,
	M68K_REG_A7,

	M68K_REG_FP0,
	M68K_REG_FP1,
	M68K_REG_FP2,
	M68K_REG_FP3,
	M68K_REG_FP4,
	M68K_REG_FP5,
	M68K_REG_FP6,
	M68K_REG_FP7,

	M68K_REG_PC,

	M68K_REG_SR,
	M68K_REG_CCR,
	M68K_REG_SFC,
	M68K_REG_DFC,
	M68K_REG_USP,
	M68K_REG_VBR,
	M68K_REG_CACR,
	M68K_REG_CAAR,
	M68K_REG_MSP,
	M68K_REG_ISP,
	M68K_REG_TC,
	M68K_REG_ITT0,
	M68K_REG_ITT1,
	M68K_REG_DTT0,
	M68K_REG_DTT1,
	M68K_REG_MMUSR,
	M68K_REG_URP,
	M68K_REG_SRP,

	M68K_REG_FPCR,
	M68K_REG_FPSR,
	M68K_REG_FPIAR,

	M68K_REG_ENDING,   // <-- mark the end of the list of registers
} m68k_reg;

/// M68K Addressing Modes
typedef enum m68k_address_mode {
	M68K_AM_NONE = 0,			///< No address mode.

	M68K_AM_REG_DIRECT_DATA,		///< Register Direct - Data
	M68K_AM_REG_DIRECT_ADDR,		///< Register Direct - Address

	M68K_AM_REGI_ADDR,				///< Register Indirect - Address
	M68K_AM_REGI_ADDR_POST_INC,		///< Register Indirect - Address with Postincrement
	M68K_AM_REGI_ADDR_PRE_DEC,		///< Register Indirect - Address with Predecrement
	M68K_AM_REGI_ADDR_DISP,			///< Register Indirect - Address with Displacement

	M68K_AM_AREGI_INDEX_8_BIT_DISP,	///< Address Register Indirect With Index- 8-bit displacement
	M68K_AM_AREGI_INDEX_BASE_DISP,	///< Address Register Indirect With Index- Base displacement

	M68K_AM_MEMI_POST_INDEX,		///< Memory indirect - Postindex
	M68K_AM_MEMI_PRE_INDEX,			///< Memory indirect - Preindex

	M68K_AM_PCI_DISP,				///< Program Counter Indirect - with Displacement

	M68K_AM_PCI_INDEX_8_BIT_DISP,	///< Program Counter Indirect with Index - with 8-Bit Displacement
	M68K_AM_PCI_INDEX_BASE_DISP,	///< Program Counter Indirect with Index - with Base Displacement

	M68K_AM_PC_MEMI_POST_INDEX,		///< Program Counter Memory Indirect - Postindexed
	M68K_AM_PC_MEMI_PRE_INDEX,		///< Program Counter Memory Indirect - Preindexed

	M68K_AM_ABSOLUTE_DATA_SHORT,	///< Absolute Data Addressing  - Short
	M68K_AM_ABSOLUTE_DATA_LONG,		///< Absolute Data Addressing  - Long
	M68K_AM_IMMEDIATE,              ///< Immediate value

	M68K_AM_BRANCH_DISPLACEMENT,    ///< Address as displacement from (PC+2) used by branches
} m68k_address_mode;

/// Operand type for instruction's operands
typedef enum m68k_op_type {
	M68K_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	M68K_OP_REG,         ///< = CS_OP_REG (Register operand).
	M68K_OP_IMM,         ///< = CS_OP_IMM (Immediate operand).
	M68K_OP_MEM,         ///< = CS_OP_MEM (Memory operand).
	M68K_OP_FP_SINGLE,   ///< single precision Floating-Point operand
	M68K_OP_FP_DOUBLE,   ///< double precision Floating-Point operand
	M68K_OP_REG_BITS,    ///< Register bits move
	M68K_OP_REG_PAIR,    ///< Register pair in the same op (upper 4 bits for first reg, lower for second)
	M68K_OP_BR_DISP,     ///< Branch displacement
} m68k_op_type;

/// Instruction's operand referring to memory
/// This is associated with M68K_OP_MEM operand type above
typedef struct m68k_op_mem {
	m68k_reg base_reg;      ///< base register (or M68K_REG_INVALID if irrelevant)
	m68k_reg index_reg;     ///< index register (or M68K_REG_INVALID if irrelevant)
	m68k_reg in_base_reg;   ///< indirect base register (or M68K_REG_INVALID if irrelevant)
	uint32_t in_disp; 	    ///< indirect displacement
	uint32_t out_disp;      ///< other displacement
	int16_t disp;	        ///< displacement value
	uint8_t scale;	        ///< scale for index register
	uint8_t bitfield;       ///< set to true if the two values below should be used
	uint8_t width;	        ///< used for bf* instructions
	uint8_t offset;	        ///< used for bf* instructions
	uint8_t index_size;     ///< 0 = w, 1 = l
} m68k_op_mem;

/// Operand type for instruction's operands
typedef enum m68k_op_br_disp_size {
	M68K_OP_BR_DISP_SIZE_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	M68K_OP_BR_DISP_SIZE_BYTE = 1,    ///< signed 8-bit displacement
	M68K_OP_BR_DISP_SIZE_WORD = 2,    ///< signed 16-bit displacement
	M68K_OP_BR_DISP_SIZE_LONG = 4,    ///< signed 32-bit displacement
} m68k_op_br_disp_size;

typedef struct m68k_op_br_disp {
	int32_t disp;	        ///< displacement value
	uint8_t disp_size;		///< Size from m68k_op_br_disp_size type above
} m68k_op_br_disp;

/// Register pair in one operand.
typedef struct cs_m68k_op_reg_pair {
	m68k_reg reg_0;
	m68k_reg reg_1;
} cs_m68k_op_reg_pair;

/// Instruction operand
typedef struct cs_m68k_op {
	union {
		uint64_t imm;               ///< immediate value for IMM operand
		double dimm; 		    ///< double imm
		float simm; 		    ///< float imm
		m68k_reg reg;		    ///< register value for REG operand
		cs_m68k_op_reg_pair reg_pair; ///< register pair in one operand
	};

	m68k_op_mem mem; 	    ///< data when operand is targeting memory
	m68k_op_br_disp br_disp; ///< data when operand is a branch displacement
	uint32_t register_bits; ///< register bits for movem etc. (always in d0-d7, a0-a7, fp0 - fp7 order)
	m68k_op_type type;
	m68k_address_mode address_mode;	///< M68K addressing mode for this op
} cs_m68k_op;

/// Operation size of the CPU instructions
typedef enum m68k_cpu_size {
	M68K_CPU_SIZE_NONE = 0,		///< unsized or unspecified
	M68K_CPU_SIZE_BYTE = 1,		///< 1 byte in size
	M68K_CPU_SIZE_WORD = 2,		///< 2 bytes in size
	M68K_CPU_SIZE_LONG = 4,		///< 4 bytes in size
} m68k_cpu_size;

/// Operation size of the FPU instructions (Notice that FPU instruction can also use CPU sizes if needed)
typedef enum m68k_fpu_size {
	M68K_FPU_SIZE_NONE = 0,		///< unsized like fsave/frestore
	M68K_FPU_SIZE_SINGLE = 4,		///< 4 byte in size (single float)
	M68K_FPU_SIZE_DOUBLE = 8,		///< 8 byte in size (double)
	M68K_FPU_SIZE_EXTENDED = 12,	///< 12 byte in size (extended real format)
} m68k_fpu_size;

/// Type of size that is being used for the current instruction
typedef enum m68k_size_type {
	M68K_SIZE_TYPE_INVALID = 0,

	M68K_SIZE_TYPE_CPU,
	M68K_SIZE_TYPE_FPU,
} m68k_size_type;

/// Operation size of the current instruction (NOT the actually size of instruction)
typedef struct m68k_op_size {
	m68k_size_type type;
	union {
		m68k_cpu_size cpu_size;
		m68k_fpu_size fpu_size;
	};
} m68k_op_size;

/// The M68K instruction and it's operands
typedef struct cs_m68k {
	// Number of operands of this instruction or 0 when instruction has no operand.
	cs_m68k_op operands[M68K_OPERAND_COUNT]; ///< operands for this instruction.
	m68k_op_size op_size;	///< size of data operand works on in bytes (.b, .w, .l, etc)
	uint8_t op_count; ///< number of operands for the instruction
} cs_m68k;

/// M68K instruction
typedef enum m68k_insn {
	M68K_INS_INVALID = 0,

	M68K_INS_ABCD,
	M68K_INS_ADD,
	M68K_INS_ADDA,
	M68K_INS_ADDI,
	M68K_INS_ADDQ,
	M68K_INS_ADDX,
	M68K_INS_AND,
	M68K_INS_ANDI,
	M68K_INS_ASL,
	M68K_INS_ASR,
	M68K_INS_BHS,
	M68K_INS_BLO,
	M68K_INS_BHI,
	M68K_INS_BLS,
	M68K_INS_BCC,
	M68K_INS_BCS,
	M68K_INS_BNE,
	M68K_INS_BEQ,
	M68K_INS_BVC,
	M68K_INS_BVS,
	M68K_INS_BPL,
	M68K_INS_BMI,
	M68K_INS_BGE,
	M68K_INS_BLT,
	M68K_INS_BGT,
	M68K_INS_BLE,
	M68K_INS_BRA,
	M68K_INS_BSR,
	M68K_INS_BCHG,
	M68K_INS_BCLR,
	M68K_INS_BSET,
	M68K_INS_BTST,
	M68K_INS_BFCHG,
	M68K_INS_BFCLR,
	M68K_INS_BFEXTS,
	M68K_INS_BFEXTU,
	M68K_INS_BFFFO,
	M68K_INS_BFINS,
	M68K_INS_BFSET,
	M68K_INS_BFTST,
	M68K_INS_BKPT,
	M68K_INS_CALLM,
	M68K_INS_CAS,
	M68K_INS_CAS2,
	M68K_INS_CHK,
	M68K_INS_CHK2,
	M68K_INS_CLR,
	M68K_INS_CMP,
	M68K_INS_CMPA,
	M68K_INS_CMPI,
	M68K_INS_CMPM,
	M68K_INS_CMP2,
	M68K_INS_CINVL,
	M68K_INS_CINVP,
	M68K_INS_CINVA,
	M68K_INS_CPUSHL,
	M68K_INS_CPUSHP,
	M68K_INS_CPUSHA,
	M68K_INS_DBT,
	M68K_INS_DBF,
	M68K_INS_DBHI,
	M68K_INS_DBLS,
	M68K_INS_DBCC,
	M68K_INS_DBCS,
	M68K_INS_DBNE,
	M68K_INS_DBEQ,
	M68K_INS_DBVC,
	M68K_INS_DBVS,
	M68K_INS_DBPL,
	M68K_INS_DBMI,
	M68K_INS_DBGE,
	M68K_INS_DBLT,
	M68K_INS_DBGT,
	M68K_INS_DBLE,
	M68K_INS_DBRA,
	M68K_INS_DIVS,
	M68K_INS_DIVSL,
	M68K_INS_DIVU,
	M68K_INS_DIVUL,
	M68K_INS_EOR,
	M68K_INS_EORI,
	M68K_INS_EXG,
	M68K_INS_EXT,
	M68K_INS_EXTB,
	M68K_INS_FABS,
	M68K_INS_FSABS,
	M68K_INS_FDABS,
	M68K_INS_FACOS,
	M68K_INS_FADD,
	M68K_INS_FSADD,
	M68K_INS_FDADD,
	M68K_INS_FASIN,
	M68K_INS_FATAN,
	M68K_INS_FATANH,
	M68K_INS_FBF,
	M68K_INS_FBEQ,
	M68K_INS_FBOGT,
	M68K_INS_FBOGE,
	M68K_INS_FBOLT,
	M68K_INS_FBOLE,
	M68K_INS_FBOGL,
	M68K_INS_FBOR,
	M68K_INS_FBUN,
	M68K_INS_FBUEQ,
	M68K_INS_FBUGT,
	M68K_INS_FBUGE,
	M68K_INS_FBULT,
	M68K_INS_FBULE,
	M68K_INS_FBNE,
	M68K_INS_FBT,
	M68K_INS_FBSF,
	M68K_INS_FBSEQ,
	M68K_INS_FBGT,
	M68K_INS_FBGE,
	M68K_INS_FBLT,
	M68K_INS_FBLE,
	M68K_INS_FBGL,
	M68K_INS_FBGLE,
	M68K_INS_FBNGLE,
	M68K_INS_FBNGL,
	M68K_INS_FBNLE,
	M68K_INS_FBNLT,
	M68K_INS_FBNGE,
	M68K_INS_FBNGT,
	M68K_INS_FBSNE,
	M68K_INS_FBST,
	M68K_INS_FCMP,
	M68K_INS_FCOS,
	M68K_INS_FCOSH,
	M68K_INS_FDBF,
	M68K_INS_FDBEQ,
	M68K_INS_FDBOGT,
	M68K_INS_FDBOGE,
	M68K_INS_FDBOLT,
	M68K_INS_FDBOLE,
	M68K_INS_FDBOGL,
	M68K_INS_FDBOR,
	M68K_INS_FDBUN,
	M68K_INS_FDBUEQ,
	M68K_INS_FDBUGT,
	M68K_INS_FDBUGE,
	M68K_INS_FDBULT,
	M68K_INS_FDBULE,
	M68K_INS_FDBNE,
	M68K_INS_FDBT,
	M68K_INS_FDBSF,
	M68K_INS_FDBSEQ,
	M68K_INS_FDBGT,
	M68K_INS_FDBGE,
	M68K_INS_FDBLT,
	M68K_INS_FDBLE,
	M68K_INS_FDBGL,
	M68K_INS_FDBGLE,
	M68K_INS_FDBNGLE,
	M68K_INS_FDBNGL,
	M68K_INS_FDBNLE,
	M68K_INS_FDBNLT,
	M68K_INS_FDBNGE,
	M68K_INS_FDBNGT,
	M68K_INS_FDBSNE,
	M68K_INS_FDBST,
	M68K_INS_FDIV,
	M68K_INS_FSDIV,
	M68K_INS_FDDIV,
	M68K_INS_FETOX,
	M68K_INS_FETOXM1,
	M68K_INS_FGETEXP,
	M68K_INS_FGETMAN,
	M68K_INS_FINT,
	M68K_INS_FINTRZ,
	M68K_INS_FLOG10,
	M68K_INS_FLOG2,
	M68K_INS_FLOGN,
	M68K_INS_FLOGNP1,
	M68K_INS_FMOD,
	M68K_INS_FMOVE,
	M68K_INS_FSMOVE,
	M68K_INS_FDMOVE,
	M68K_INS_FMOVECR,
	M68K_INS_FMOVEM,
	M68K_INS_FMUL,
	M68K_INS_FSMUL,
	M68K_INS_FDMUL,
	M68K_INS_FNEG,
	M68K_INS_FSNEG,
	M68K_INS_FDNEG,
	M68K_INS_FNOP,
	M68K_INS_FREM,
	M68K_INS_FRESTORE,
	M68K_INS_FSAVE,
	M68K_INS_FSCALE,
	M68K_INS_FSGLDIV,
	M68K_INS_FSGLMUL,
	M68K_INS_FSIN,
	M68K_INS_FSINCOS,
	M68K_INS_FSINH,
	M68K_INS_FSQRT,
	M68K_INS_FSSQRT,
	M68K_INS_FDSQRT,
	M68K_INS_FSF,
	M68K_INS_FSBEQ,
	M68K_INS_FSOGT,
	M68K_INS_FSOGE,
	M68K_INS_FSOLT,
	M68K_INS_FSOLE,
	M68K_INS_FSOGL,
	M68K_INS_FSOR,
	M68K_INS_FSUN,
	M68K_INS_FSUEQ,
	M68K_INS_FSUGT,
	M68K_INS_FSUGE,
	M68K_INS_FSULT,
	M68K_INS_FSULE,
	M68K_INS_FSNE,
	M68K_INS_FST,
	M68K_INS_FSSF,
	M68K_INS_FSSEQ,
	M68K_INS_FSGT,
	M68K_INS_FSGE,
	M68K_INS_FSLT,
	M68K_INS_FSLE,
	M68K_INS_FSGL,
	M68K_INS_FSGLE,
	M68K_INS_FSNGLE,
	M68K_INS_FSNGL,
	M68K_INS_FSNLE,
	M68K_INS_FSNLT,
	M68K_INS_FSNGE,
	M68K_INS_FSNGT,
	M68K_INS_FSSNE,
	M68K_INS_FSST,
	M68K_INS_FSUB,
	M68K_INS_FSSUB,
	M68K_INS_FDSUB,
	M68K_INS_FTAN,
	M68K_INS_FTANH,
	M68K_INS_FTENTOX,
	M68K_INS_FTRAPF,
	M68K_INS_FTRAPEQ,
	M68K_INS_FTRAPOGT,
	M68K_INS_FTRAPOGE,
	M68K_INS_FTRAPOLT,
	M68K_INS_FTRAPOLE,
	M68K_INS_FTRAPOGL,
	M68K_INS_FTRAPOR,
	M68K_INS_FTRAPUN,
	M68K_INS_FTRAPUEQ,
	M68K_INS_FTRAPUGT,
	M68K_INS_FTRAPUGE,
	M68K_INS_FTRAPULT,
	M68K_INS_FTRAPULE,
	M68K_INS_FTRAPNE,
	M68K_INS_FTRAPT,
	M68K_INS_FTRAPSF,
	M68K_INS_FTRAPSEQ,
	M68K_INS_FTRAPGT,
	M68K_INS_FTRAPGE,
	M68K_INS_FTRAPLT,
	M68K_INS_FTRAPLE,
	M68K_INS_FTRAPGL,
	M68K_INS_FTRAPGLE,
	M68K_INS_FTRAPNGLE,
	M68K_INS_FTRAPNGL,
	M68K_INS_FTRAPNLE,
	M68K_INS_FTRAPNLT,
	M68K_INS_FTRAPNGE,
	M68K_INS_FTRAPNGT,
	M68K_INS_FTRAPSNE,
	M68K_INS_FTRAPST,
	M68K_INS_FTST,
	M68K_INS_FTWOTOX,
	M68K_INS_HALT,
	M68K_INS_ILLEGAL,
	M68K_INS_JMP,
	M68K_INS_JSR,
	M68K_INS_LEA,
	M68K_INS_LINK,
	M68K_INS_LPSTOP,
	M68K_INS_LSL,
	M68K_INS_LSR,
	M68K_INS_MOVE,
	M68K_INS_MOVEA,
	M68K_INS_MOVEC,
	M68K_INS_MOVEM,
	M68K_INS_MOVEP,
	M68K_INS_MOVEQ,
	M68K_INS_MOVES,
	M68K_INS_MOVE16,
	M68K_INS_MULS,
	M68K_INS_MULU,
	M68K_INS_NBCD,
	M68K_INS_NEG,
	M68K_INS_NEGX,
	M68K_INS_NOP,
	M68K_INS_NOT,
	M68K_INS_OR,
	M68K_INS_ORI,
	M68K_INS_PACK,
	M68K_INS_PEA,
	M68K_INS_PFLUSH,
	M68K_INS_PFLUSHA,
	M68K_INS_PFLUSHAN,
	M68K_INS_PFLUSHN,
	M68K_INS_PLOADR,
	M68K_INS_PLOADW,
	M68K_INS_PLPAR,
	M68K_INS_PLPAW,
	M68K_INS_PMOVE,
	M68K_INS_PMOVEFD,
	M68K_INS_PTESTR,
	M68K_INS_PTESTW,
	M68K_INS_PULSE,
	M68K_INS_REMS,
	M68K_INS_REMU,
	M68K_INS_RESET,
	M68K_INS_ROL,
	M68K_INS_ROR,
	M68K_INS_ROXL,
	M68K_INS_ROXR,
	M68K_INS_RTD,
	M68K_INS_RTE,
	M68K_INS_RTM,
	M68K_INS_RTR,
	M68K_INS_RTS,
	M68K_INS_SBCD,
	M68K_INS_ST,
	M68K_INS_SF,
	M68K_INS_SHI,
	M68K_INS_SLS,
	M68K_INS_SCC,
	M68K_INS_SHS,
	M68K_INS_SCS,
	M68K_INS_SLO,
	M68K_INS_SNE,
	M68K_INS_SEQ,
	M68K_INS_SVC,
	M68K_INS_SVS,
	M68K_INS_SPL,
	M68K_INS_SMI,
	M68K_INS_SGE,
	M68K_INS_SLT,
	M68K_INS_SGT,
	M68K_INS_SLE,
	M68K_INS_STOP,
	M68K_INS_SUB,
	M68K_INS_SUBA,
	M68K_INS_SUBI,
	M68K_INS_SUBQ,
	M68K_INS_SUBX,
	M68K_INS_SWAP,
	M68K_INS_TAS,
	M68K_INS_TRAP,
	M68K_INS_TRAPV,
	M68K_INS_TRAPT,
	M68K_INS_TRAPF,
	M68K_INS_TRAPHI,
	M68K_INS_TRAPLS,
	M68K_INS_TRAPCC,
	M68K_INS_TRAPHS,
	M68K_INS_TRAPCS,
	M68K_INS_TRAPLO,
	M68K_INS_TRAPNE,
	M68K_INS_TRAPEQ,
	M68K_INS_TRAPVC,
	M68K_INS_TRAPVS,
	M68K_INS_TRAPPL,
	M68K_INS_TRAPMI,
	M68K_INS_TRAPGE,
	M68K_INS_TRAPLT,
	M68K_INS_TRAPGT,
	M68K_INS_TRAPLE,
	M68K_INS_TST,
	M68K_INS_UNLK,
	M68K_INS_UNPK,
	M68K_INS_ENDING,   // <-- mark the end of the list of instructions
} m68k_insn;

/// Group of M68K instructions
typedef enum m68k_group_type {
	M68K_GRP_INVALID = 0,  ///< CS_GRUP_INVALID
	M68K_GRP_JUMP,  ///< = CS_GRP_JUMP
	M68K_GRP_RET = 3,  ///< = CS_GRP_RET
	M68K_GRP_IRET = 5, ///< = CS_GRP_IRET
	M68K_GRP_BRANCH_RELATIVE = 7, ///< = CS_GRP_BRANCH_RELATIVE

	M68K_GRP_ENDING,// <-- mark the end of the list of groups
} m68k_group_type;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/mips.h`:

```h
#ifndef CAPSTONE_MIPS_H
#define CAPSTONE_MIPS_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2013-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

// GCC MIPS toolchain has a default macro called "mips" which breaks
// compilation
#undef mips

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// Operand type for instruction's operands
typedef enum mips_op_type {
	MIPS_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	MIPS_OP_REG, ///< = CS_OP_REG (Register operand).
	MIPS_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	MIPS_OP_MEM, ///< = CS_OP_MEM (Memory operand).
} mips_op_type;

/// MIPS registers
typedef enum mips_reg {
	MIPS_REG_INVALID = 0,
	// General purpose registers
	MIPS_REG_PC,

	MIPS_REG_0,
	MIPS_REG_1,
	MIPS_REG_2,
	MIPS_REG_3,
	MIPS_REG_4,
	MIPS_REG_5,
	MIPS_REG_6,
	MIPS_REG_7,
	MIPS_REG_8,
	MIPS_REG_9,
	MIPS_REG_10,
	MIPS_REG_11,
	MIPS_REG_12,
	MIPS_REG_13,
	MIPS_REG_14,
	MIPS_REG_15,
	MIPS_REG_16,
	MIPS_REG_17,
	MIPS_REG_18,
	MIPS_REG_19,
	MIPS_REG_20,
	MIPS_REG_21,
	MIPS_REG_22,
	MIPS_REG_23,
	MIPS_REG_24,
	MIPS_REG_25,
	MIPS_REG_26,
	MIPS_REG_27,
	MIPS_REG_28,
	MIPS_REG_29,
	MIPS_REG_30,
	MIPS_REG_31,

	// DSP registers
	MIPS_REG_DSPCCOND,
	MIPS_REG_DSPCARRY,
	MIPS_REG_DSPEFI,
	MIPS_REG_DSPOUTFLAG,
	MIPS_REG_DSPOUTFLAG16_19,
	MIPS_REG_DSPOUTFLAG20,
	MIPS_REG_DSPOUTFLAG21,
	MIPS_REG_DSPOUTFLAG22,
	MIPS_REG_DSPOUTFLAG23,
	MIPS_REG_DSPPOS,
	MIPS_REG_DSPSCOUNT,

	// ACC registers
	MIPS_REG_AC0,
	MIPS_REG_AC1,
	MIPS_REG_AC2,
	MIPS_REG_AC3,

	// COP registers
	MIPS_REG_CC0,
	MIPS_REG_CC1,
	MIPS_REG_CC2,
	MIPS_REG_CC3,
	MIPS_REG_CC4,
	MIPS_REG_CC5,
	MIPS_REG_CC6,
	MIPS_REG_CC7,

	// FPU registers
	MIPS_REG_F0,
	MIPS_REG_F1,
	MIPS_REG_F2,
	MIPS_REG_F3,
	MIPS_REG_F4,
	MIPS_REG_F5,
	MIPS_REG_F6,
	MIPS_REG_F7,
	MIPS_REG_F8,
	MIPS_REG_F9,
	MIPS_REG_F10,
	MIPS_REG_F11,
	MIPS_REG_F12,
	MIPS_REG_F13,
	MIPS_REG_F14,
	MIPS_REG_F15,
	MIPS_REG_F16,
	MIPS_REG_F17,
	MIPS_REG_F18,
	MIPS_REG_F19,
	MIPS_REG_F20,
	MIPS_REG_F21,
	MIPS_REG_F22,
	MIPS_REG_F23,
	MIPS_REG_F24,
	MIPS_REG_F25,
	MIPS_REG_F26,
	MIPS_REG_F27,
	MIPS_REG_F28,
	MIPS_REG_F29,
	MIPS_REG_F30,
	MIPS_REG_F31,

	MIPS_REG_FCC0,
	MIPS_REG_FCC1,
	MIPS_REG_FCC2,
	MIPS_REG_FCC3,
	MIPS_REG_FCC4,
	MIPS_REG_FCC5,
	MIPS_REG_FCC6,
	MIPS_REG_FCC7,

	// AFPR128
	MIPS_REG_W0,
	MIPS_REG_W1,
	MIPS_REG_W2,
	MIPS_REG_W3,
	MIPS_REG_W4,
	MIPS_REG_W5,
	MIPS_REG_W6,
	MIPS_REG_W7,
	MIPS_REG_W8,
	MIPS_REG_W9,
	MIPS_REG_W10,
	MIPS_REG_W11,
	MIPS_REG_W12,
	MIPS_REG_W13,
	MIPS_REG_W14,
	MIPS_REG_W15,
	MIPS_REG_W16,
	MIPS_REG_W17,
	MIPS_REG_W18,
	MIPS_REG_W19,
	MIPS_REG_W20,
	MIPS_REG_W21,
	MIPS_REG_W22,
	MIPS_REG_W23,
	MIPS_REG_W24,
	MIPS_REG_W25,
	MIPS_REG_W26,
	MIPS_REG_W27,
	MIPS_REG_W28,
	MIPS_REG_W29,
	MIPS_REG_W30,
	MIPS_REG_W31,

	MIPS_REG_HI,
	MIPS_REG_LO,

	MIPS_REG_P0,
	MIPS_REG_P1,
	MIPS_REG_P2,

	MIPS_REG_MPL0,
	MIPS_REG_MPL1,
	MIPS_REG_MPL2,

	MIPS_REG_ENDING,	// <-- mark the end of the list or registers

	// alias registers
	MIPS_REG_ZERO = MIPS_REG_0,
	MIPS_REG_AT = MIPS_REG_1,
	MIPS_REG_V0 = MIPS_REG_2,
	MIPS_REG_V1 = MIPS_REG_3,
	MIPS_REG_A0 = MIPS_REG_4,
	MIPS_REG_A1 = MIPS_REG_5,
	MIPS_REG_A2 = MIPS_REG_6,
	MIPS_REG_A3 = MIPS_REG_7,
	MIPS_REG_T0 = MIPS_REG_8,
	MIPS_REG_T1 = MIPS_REG_9,
	MIPS_REG_T2 = MIPS_REG_10,
	MIPS_REG_T3 = MIPS_REG_11,
	MIPS_REG_T4 = MIPS_REG_12,
	MIPS_REG_T5 = MIPS_REG_13,
	MIPS_REG_T6 = MIPS_REG_14,
	MIPS_REG_T7 = MIPS_REG_15,
	MIPS_REG_S0 = MIPS_REG_16,
	MIPS_REG_S1 = MIPS_REG_17,
	MIPS_REG_S2 = MIPS_REG_18,
	MIPS_REG_S3 = MIPS_REG_19,
	MIPS_REG_S4 = MIPS_REG_20,
	MIPS_REG_S5 = MIPS_REG_21,
	MIPS_REG_S6 = MIPS_REG_22,
	MIPS_REG_S7 = MIPS_REG_23,
	MIPS_REG_T8 = MIPS_REG_24,
	MIPS_REG_T9 = MIPS_REG_25,
	MIPS_REG_K0 = MIPS_REG_26,
	MIPS_REG_K1 = MIPS_REG_27,
	MIPS_REG_GP = MIPS_REG_28,
	MIPS_REG_SP = MIPS_REG_29,
	MIPS_REG_FP = MIPS_REG_30, MIPS_REG_S8 = MIPS_REG_30,
	MIPS_REG_RA = MIPS_REG_31,

	MIPS_REG_HI0 = MIPS_REG_AC0,
	MIPS_REG_HI1 = MIPS_REG_AC1,
	MIPS_REG_HI2 = MIPS_REG_AC2,
	MIPS_REG_HI3 = MIPS_REG_AC3,

	MIPS_REG_LO0 = MIPS_REG_HI0,
	MIPS_REG_LO1 = MIPS_REG_HI1,
	MIPS_REG_LO2 = MIPS_REG_HI2,
	MIPS_REG_LO3 = MIPS_REG_HI3,
} mips_reg;

/// Instruction's operand referring to memory
/// This is associated with MIPS_OP_MEM operand type above
typedef struct mips_op_mem {
	mips_reg base;	///< base register
	int64_t disp;	///< displacement/offset value
} mips_op_mem;

/// Instruction operand
typedef struct cs_mips_op {
	mips_op_type type;	///< operand type
	union {
		mips_reg reg;		///< register value for REG operand
		int64_t imm;		///< immediate value for IMM operand
		mips_op_mem mem;	///< base/index/scale/disp value for MEM operand
	};
} cs_mips_op;

/// Instruction structure
typedef struct cs_mips {
	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;
	cs_mips_op operands[10]; ///< operands for this instruction.
} cs_mips;

/// MIPS instruction
typedef enum mips_insn {
	MIPS_INS_INVALID = 0,

	MIPS_INS_ABSQ_S,
	MIPS_INS_ADD,
	MIPS_INS_ADDIUPC,
	MIPS_INS_ADDIUR1SP,
	MIPS_INS_ADDIUR2,
	MIPS_INS_ADDIUS5,
	MIPS_INS_ADDIUSP,
	MIPS_INS_ADDQH,
	MIPS_INS_ADDQH_R,
	MIPS_INS_ADDQ,
	MIPS_INS_ADDQ_S,
	MIPS_INS_ADDSC,
	MIPS_INS_ADDS_A,
	MIPS_INS_ADDS_S,
	MIPS_INS_ADDS_U,
	MIPS_INS_ADDU16,
	MIPS_INS_ADDUH,
	MIPS_INS_ADDUH_R,
	MIPS_INS_ADDU,
	MIPS_INS_ADDU_S,
	MIPS_INS_ADDVI,
	MIPS_INS_ADDV,
	MIPS_INS_ADDWC,
	MIPS_INS_ADD_A,
	MIPS_INS_ADDI,
	MIPS_INS_ADDIU,
	MIPS_INS_ALIGN,
	MIPS_INS_ALUIPC,
	MIPS_INS_AND,
	MIPS_INS_AND16,
	MIPS_INS_ANDI16,
	MIPS_INS_ANDI,
	MIPS_INS_APPEND,
	MIPS_INS_ASUB_S,
	MIPS_INS_ASUB_U,
	MIPS_INS_AUI,
	MIPS_INS_AUIPC,
	MIPS_INS_AVER_S,
	MIPS_INS_AVER_U,
	MIPS_INS_AVE_S,
	MIPS_INS_AVE_U,
	MIPS_INS_B16,
	MIPS_INS_BADDU,
	MIPS_INS_BAL,
	MIPS_INS_BALC,
	MIPS_INS_BALIGN,
	MIPS_INS_BBIT0,
	MIPS_INS_BBIT032,
	MIPS_INS_BBIT1,
	MIPS_INS_BBIT132,
	MIPS_INS_BC,
	MIPS_INS_BC0F,
	MIPS_INS_BC0FL,
	MIPS_INS_BC0T,
	MIPS_INS_BC0TL,
	MIPS_INS_BC1EQZ,
	MIPS_INS_BC1F,
	MIPS_INS_BC1FL,
	MIPS_INS_BC1NEZ,
	MIPS_INS_BC1T,
	MIPS_INS_BC1TL,
	MIPS_INS_BC2EQZ,
	MIPS_INS_BC2F,
	MIPS_INS_BC2FL,
	MIPS_INS_BC2NEZ,
	MIPS_INS_BC2T,
	MIPS_INS_BC2TL,
	MIPS_INS_BC3F,
	MIPS_INS_BC3FL,
	MIPS_INS_BC3T,
	MIPS_INS_BC3TL,
	MIPS_INS_BCLRI,
	MIPS_INS_BCLR,
	MIPS_INS_BEQ,
	MIPS_INS_BEQC,
	MIPS_INS_BEQL,
	MIPS_INS_BEQZ16,
	MIPS_INS_BEQZALC,
	MIPS_INS_BEQZC,
	MIPS_INS_BGEC,
	MIPS_INS_BGEUC,
	MIPS_INS_BGEZ,
	MIPS_INS_BGEZAL,
	MIPS_INS_BGEZALC,
	MIPS_INS_BGEZALL,
	MIPS_INS_BGEZALS,
	MIPS_INS_BGEZC,
	MIPS_INS_BGEZL,
	MIPS_INS_BGTZ,
	MIPS_INS_BGTZALC,
	MIPS_INS_BGTZC,
	MIPS_INS_BGTZL,
	MIPS_INS_BINSLI,
	MIPS_INS_BINSL,
	MIPS_INS_BINSRI,
	MIPS_INS_BINSR,
	MIPS_INS_BITREV,
	MIPS_INS_BITSWAP,
	MIPS_INS_BLEZ,
	MIPS_INS_BLEZALC,
	MIPS_INS_BLEZC,
	MIPS_INS_BLEZL,
	MIPS_INS_BLTC,
	MIPS_INS_BLTUC,
	MIPS_INS_BLTZ,
	MIPS_INS_BLTZAL,
	MIPS_INS_BLTZALC,
	MIPS_INS_BLTZALL,
	MIPS_INS_BLTZALS,
	MIPS_INS_BLTZC,
	MIPS_INS_BLTZL,
	MIPS_INS_BMNZI,
	MIPS_INS_BMNZ,
	MIPS_INS_BMZI,
	MIPS_INS_BMZ,
	MIPS_INS_BNE,
	MIPS_INS_BNEC,
	MIPS_INS_BNEGI,
	MIPS_INS_BNEG,
	MIPS_INS_BNEL,
	MIPS_INS_BNEZ16,
	MIPS_INS_BNEZALC,
	MIPS_INS_BNEZC,
	MIPS_INS_BNVC,
	MIPS_INS_BNZ,
	MIPS_INS_BOVC,
	MIPS_INS_BPOSGE32,
	MIPS_INS_BREAK,
	MIPS_INS_BREAK16,
	MIPS_INS_BSELI,
	MIPS_INS_BSEL,
	MIPS_INS_BSETI,
	MIPS_INS_BSET,
	MIPS_INS_BZ,
	MIPS_INS_BEQZ,
	MIPS_INS_B,
	MIPS_INS_BNEZ,
	MIPS_INS_BTEQZ,
	MIPS_INS_BTNEZ,
	MIPS_INS_CACHE,
	MIPS_INS_CEIL,
	MIPS_INS_CEQI,
	MIPS_INS_CEQ,
	MIPS_INS_CFC1,
	MIPS_INS_CFCMSA,
	MIPS_INS_CINS,
	MIPS_INS_CINS32,
	MIPS_INS_CLASS,
	MIPS_INS_CLEI_S,
	MIPS_INS_CLEI_U,
	MIPS_INS_CLE_S,
	MIPS_INS_CLE_U,
	MIPS_INS_CLO,
	MIPS_INS_CLTI_S,
	MIPS_INS_CLTI_U,
	MIPS_INS_CLT_S,
	MIPS_INS_CLT_U,
	MIPS_INS_CLZ,
	MIPS_INS_CMPGDU,
	MIPS_INS_CMPGU,
	MIPS_INS_CMPU,
	MIPS_INS_CMP,
	MIPS_INS_COPY_S,
	MIPS_INS_COPY_U,
	MIPS_INS_CTC1,
	MIPS_INS_CTCMSA,
	MIPS_INS_CVT,
	MIPS_INS_C,
	MIPS_INS_CMPI,
	MIPS_INS_DADD,
	MIPS_INS_DADDI,
	MIPS_INS_DADDIU,
	MIPS_INS_DADDU,
	MIPS_INS_DAHI,
	MIPS_INS_DALIGN,
	MIPS_INS_DATI,
	MIPS_INS_DAUI,
	MIPS_INS_DBITSWAP,
	MIPS_INS_DCLO,
	MIPS_INS_DCLZ,
	MIPS_INS_DDIV,
	MIPS_INS_DDIVU,
	MIPS_INS_DERET,
	MIPS_INS_DEXT,
	MIPS_INS_DEXTM,
	MIPS_INS_DEXTU,
	MIPS_INS_DI,
	MIPS_INS_DINS,
	MIPS_INS_DINSM,
	MIPS_INS_DINSU,
	MIPS_INS_DIV,
	MIPS_INS_DIVU,
	MIPS_INS_DIV_S,
	MIPS_INS_DIV_U,
	MIPS_INS_DLSA,
	MIPS_INS_DMFC0,
	MIPS_INS_DMFC1,
	MIPS_INS_DMFC2,
	MIPS_INS_DMOD,
	MIPS_INS_DMODU,
	MIPS_INS_DMTC0,
	MIPS_INS_DMTC1,
	MIPS_INS_DMTC2,
	MIPS_INS_DMUH,
	MIPS_INS_DMUHU,
	MIPS_INS_DMUL,
	MIPS_INS_DMULT,
	MIPS_INS_DMULTU,
	MIPS_INS_DMULU,
	MIPS_INS_DOTP_S,
	MIPS_INS_DOTP_U,
	MIPS_INS_DPADD_S,
	MIPS_INS_DPADD_U,
	MIPS_INS_DPAQX_SA,
	MIPS_INS_DPAQX_S,
	MIPS_INS_DPAQ_SA,
	MIPS_INS_DPAQ_S,
	MIPS_INS_DPAU,
	MIPS_INS_DPAX,
	MIPS_INS_DPA,
	MIPS_INS_DPOP,
	MIPS_INS_DPSQX_SA,
	MIPS_INS_DPSQX_S,
	MIPS_INS_DPSQ_SA,
	MIPS_INS_DPSQ_S,
	MIPS_INS_DPSUB_S,
	MIPS_INS_DPSUB_U,
	MIPS_INS_DPSU,
	MIPS_INS_DPSX,
	MIPS_INS_DPS,
	MIPS_INS_DROTR,
	MIPS_INS_DROTR32,
	MIPS_INS_DROTRV,
	MIPS_INS_DSBH,
	MIPS_INS_DSHD,
	MIPS_INS_DSLL,
	MIPS_INS_DSLL32,
	MIPS_INS_DSLLV,
	MIPS_INS_DSRA,
	MIPS_INS_DSRA32,
	MIPS_INS_DSRAV,
	MIPS_INS_DSRL,
	MIPS_INS_DSRL32,
	MIPS_INS_DSRLV,
	MIPS_INS_DSUB,
	MIPS_INS_DSUBU,
	MIPS_INS_EHB,
	MIPS_INS_EI,
	MIPS_INS_ERET,
	MIPS_INS_EXT,
	MIPS_INS_EXTP,
	MIPS_INS_EXTPDP,
	MIPS_INS_EXTPDPV,
	MIPS_INS_EXTPV,
	MIPS_INS_EXTRV_RS,
	MIPS_INS_EXTRV_R,
	MIPS_INS_EXTRV_S,
	MIPS_INS_EXTRV,
	MIPS_INS_EXTR_RS,
	MIPS_INS_EXTR_R,
	MIPS_INS_EXTR_S,
	MIPS_INS_EXTR,
	MIPS_INS_EXTS,
	MIPS_INS_EXTS32,
	MIPS_INS_ABS,
	MIPS_INS_FADD,
	MIPS_INS_FCAF,
	MIPS_INS_FCEQ,
	MIPS_INS_FCLASS,
	MIPS_INS_FCLE,
	MIPS_INS_FCLT,
	MIPS_INS_FCNE,
	MIPS_INS_FCOR,
	MIPS_INS_FCUEQ,
	MIPS_INS_FCULE,
	MIPS_INS_FCULT,
	MIPS_INS_FCUNE,
	MIPS_INS_FCUN,
	MIPS_INS_FDIV,
	MIPS_INS_FEXDO,
	MIPS_INS_FEXP2,
	MIPS_INS_FEXUPL,
	MIPS_INS_FEXUPR,
	MIPS_INS_FFINT_S,
	MIPS_INS_FFINT_U,
	MIPS_INS_FFQL,
	MIPS_INS_FFQR,
	MIPS_INS_FILL,
	MIPS_INS_FLOG2,
	MIPS_INS_FLOOR,
	MIPS_INS_FMADD,
	MIPS_INS_FMAX_A,
	MIPS_INS_FMAX,
	MIPS_INS_FMIN_A,
	MIPS_INS_FMIN,
	MIPS_INS_MOV,
	MIPS_INS_FMSUB,
	MIPS_INS_FMUL,
	MIPS_INS_MUL,
	MIPS_INS_NEG,
	MIPS_INS_FRCP,
	MIPS_INS_FRINT,
	MIPS_INS_FRSQRT,
	MIPS_INS_FSAF,
	MIPS_INS_FSEQ,
	MIPS_INS_FSLE,
	MIPS_INS_FSLT,
	MIPS_INS_FSNE,
	MIPS_INS_FSOR,
	MIPS_INS_FSQRT,
	MIPS_INS_SQRT,
	MIPS_INS_FSUB,
	MIPS_INS_SUB,
	MIPS_INS_FSUEQ,
	MIPS_INS_FSULE,
	MIPS_INS_FSULT,
	MIPS_INS_FSUNE,
	MIPS_INS_FSUN,
	MIPS_INS_FTINT_S,
	MIPS_INS_FTINT_U,
	MIPS_INS_FTQ,
	MIPS_INS_FTRUNC_S,
	MIPS_INS_FTRUNC_U,
	MIPS_INS_HADD_S,
	MIPS_INS_HADD_U,
	MIPS_INS_HSUB_S,
	MIPS_INS_HSUB_U,
	MIPS_INS_ILVEV,
	MIPS_INS_ILVL,
	MIPS_INS_ILVOD,
	MIPS_INS_ILVR,
	MIPS_INS_INS,
	MIPS_INS_INSERT,
	MIPS_INS_INSV,
	MIPS_INS_INSVE,
	MIPS_INS_J,
	MIPS_INS_JAL,
	MIPS_INS_JALR,
	MIPS_INS_JALRS16,
	MIPS_INS_JALRS,
	MIPS_INS_JALS,
	MIPS_INS_JALX,
	MIPS_INS_JIALC,
	MIPS_INS_JIC,
	MIPS_INS_JR,
	MIPS_INS_JR16,
	MIPS_INS_JRADDIUSP,
	MIPS_INS_JRC,
	MIPS_INS_JALRC,
	MIPS_INS_LB,
	MIPS_INS_LBU16,
	MIPS_INS_LBUX,
	MIPS_INS_LBU,
	MIPS_INS_LD,
	MIPS_INS_LDC1,
	MIPS_INS_LDC2,
	MIPS_INS_LDC3,
	MIPS_INS_LDI,
	MIPS_INS_LDL,
	MIPS_INS_LDPC,
	MIPS_INS_LDR,
	MIPS_INS_LDXC1,
	MIPS_INS_LH,
	MIPS_INS_LHU16,
	MIPS_INS_LHX,
	MIPS_INS_LHU,
	MIPS_INS_LI16,
	MIPS_INS_LL,
	MIPS_INS_LLD,
	MIPS_INS_LSA,
	MIPS_INS_LUXC1,
	MIPS_INS_LUI,
	MIPS_INS_LW,
	MIPS_INS_LW16,
	MIPS_INS_LWC1,
	MIPS_INS_LWC2,
	MIPS_INS_LWC3,
	MIPS_INS_LWL,
	MIPS_INS_LWM16,
	MIPS_INS_LWM32,
	MIPS_INS_LWPC,
	MIPS_INS_LWP,
	MIPS_INS_LWR,
	MIPS_INS_LWUPC,
	MIPS_INS_LWU,
	MIPS_INS_LWX,
	MIPS_INS_LWXC1,
	MIPS_INS_LWXS,
	MIPS_INS_LI,
	MIPS_INS_MADD,
	MIPS_INS_MADDF,
	MIPS_INS_MADDR_Q,
	MIPS_INS_MADDU,
	MIPS_INS_MADDV,
	MIPS_INS_MADD_Q,
	MIPS_INS_MAQ_SA,
	MIPS_INS_MAQ_S,
	MIPS_INS_MAXA,
	MIPS_INS_MAXI_S,
	MIPS_INS_MAXI_U,
	MIPS_INS_MAX_A,
	MIPS_INS_MAX,
	MIPS_INS_MAX_S,
	MIPS_INS_MAX_U,
	MIPS_INS_MFC0,
	MIPS_INS_MFC1,
	MIPS_INS_MFC2,
	MIPS_INS_MFHC1,
	MIPS_INS_MFHI,
	MIPS_INS_MFLO,
	MIPS_INS_MINA,
	MIPS_INS_MINI_S,
	MIPS_INS_MINI_U,
	MIPS_INS_MIN_A,
	MIPS_INS_MIN,
	MIPS_INS_MIN_S,
	MIPS_INS_MIN_U,
	MIPS_INS_MOD,
	MIPS_INS_MODSUB,
	MIPS_INS_MODU,
	MIPS_INS_MOD_S,
	MIPS_INS_MOD_U,
	MIPS_INS_MOVE,
	MIPS_INS_MOVEP,
	MIPS_INS_MOVF,
	MIPS_INS_MOVN,
	MIPS_INS_MOVT,
	MIPS_INS_MOVZ,
	MIPS_INS_MSUB,
	MIPS_INS_MSUBF,
	MIPS_INS_MSUBR_Q,
	MIPS_INS_MSUBU,
	MIPS_INS_MSUBV,
	MIPS_INS_MSUB_Q,
	MIPS_INS_MTC0,
	MIPS_INS_MTC1,
	MIPS_INS_MTC2,
	MIPS_INS_MTHC1,
	MIPS_INS_MTHI,
	MIPS_INS_MTHLIP,
	MIPS_INS_MTLO,
	MIPS_INS_MTM0,
	MIPS_INS_MTM1,
	MIPS_INS_MTM2,
	MIPS_INS_MTP0,
	MIPS_INS_MTP1,
	MIPS_INS_MTP2,
	MIPS_INS_MUH,
	MIPS_INS_MUHU,
	MIPS_INS_MULEQ_S,
	MIPS_INS_MULEU_S,
	MIPS_INS_MULQ_RS,
	MIPS_INS_MULQ_S,
	MIPS_INS_MULR_Q,
	MIPS_INS_MULSAQ_S,
	MIPS_INS_MULSA,
	MIPS_INS_MULT,
	MIPS_INS_MULTU,
	MIPS_INS_MULU,
	MIPS_INS_MULV,
	MIPS_INS_MUL_Q,
	MIPS_INS_MUL_S,
	MIPS_INS_NLOC,
	MIPS_INS_NLZC,
	MIPS_INS_NMADD,
	MIPS_INS_NMSUB,
	MIPS_INS_NOR,
	MIPS_INS_NORI,
	MIPS_INS_NOT16,
	MIPS_INS_NOT,
	MIPS_INS_OR,
	MIPS_INS_OR16,
	MIPS_INS_ORI,
	MIPS_INS_PACKRL,
	MIPS_INS_PAUSE,
	MIPS_INS_PCKEV,
	MIPS_INS_PCKOD,
	MIPS_INS_PCNT,
	MIPS_INS_PICK,
	MIPS_INS_POP,
	MIPS_INS_PRECEQU,
	MIPS_INS_PRECEQ,
	MIPS_INS_PRECEU,
	MIPS_INS_PRECRQU_S,
	MIPS_INS_PRECRQ,
	MIPS_INS_PRECRQ_RS,
	MIPS_INS_PRECR,
	MIPS_INS_PRECR_SRA,
	MIPS_INS_PRECR_SRA_R,
	MIPS_INS_PREF,
	MIPS_INS_PREPEND,
	MIPS_INS_RADDU,
	MIPS_INS_RDDSP,
	MIPS_INS_RDHWR,
	MIPS_INS_REPLV,
	MIPS_INS_REPL,
	MIPS_INS_RINT,
	MIPS_INS_ROTR,
	MIPS_INS_ROTRV,
	MIPS_INS_ROUND,
	MIPS_INS_SAT_S,
	MIPS_INS_SAT_U,
	MIPS_INS_SB,
	MIPS_INS_SB16,
	MIPS_INS_SC,
	MIPS_INS_SCD,
	MIPS_INS_SD,
	MIPS_INS_SDBBP,
	MIPS_INS_SDBBP16,
	MIPS_INS_SDC1,
	MIPS_INS_SDC2,
	MIPS_INS_SDC3,
	MIPS_INS_SDL,
	MIPS_INS_SDR,
	MIPS_INS_SDXC1,
	MIPS_INS_SEB,
	MIPS_INS_SEH,
	MIPS_INS_SELEQZ,
	MIPS_INS_SELNEZ,
	MIPS_INS_SEL,
	MIPS_INS_SEQ,
	MIPS_INS_SEQI,
	MIPS_INS_SH,
	MIPS_INS_SH16,
	MIPS_INS_SHF,
	MIPS_INS_SHILO,
	MIPS_INS_SHILOV,
	MIPS_INS_SHLLV,
	MIPS_INS_SHLLV_S,
	MIPS_INS_SHLL,
	MIPS_INS_SHLL_S,
	MIPS_INS_SHRAV,
	MIPS_INS_SHRAV_R,
	MIPS_INS_SHRA,
	MIPS_INS_SHRA_R,
	MIPS_INS_SHRLV,
	MIPS_INS_SHRL,
	MIPS_INS_SLDI,
	MIPS_INS_SLD,
	MIPS_INS_SLL,
	MIPS_INS_SLL16,
	MIPS_INS_SLLI,
	MIPS_INS_SLLV,
	MIPS_INS_SLT,
	MIPS_INS_SLTI,
	MIPS_INS_SLTIU,
	MIPS_INS_SLTU,
	MIPS_INS_SNE,
	MIPS_INS_SNEI,
	MIPS_INS_SPLATI,
	MIPS_INS_SPLAT,
	MIPS_INS_SRA,
	MIPS_INS_SRAI,
	MIPS_INS_SRARI,
	MIPS_INS_SRAR,
	MIPS_INS_SRAV,
	MIPS_INS_SRL,
	MIPS_INS_SRL16,
	MIPS_INS_SRLI,
	MIPS_INS_SRLRI,
	MIPS_INS_SRLR,
	MIPS_INS_SRLV,
	MIPS_INS_SSNOP,
	MIPS_INS_ST,
	MIPS_INS_SUBQH,
	MIPS_INS_SUBQH_R,
	MIPS_INS_SUBQ,
	MIPS_INS_SUBQ_S,
	MIPS_INS_SUBSUS_U,
	MIPS_INS_SUBSUU_S,
	MIPS_INS_SUBS_S,
	MIPS_INS_SUBS_U,
	MIPS_INS_SUBU16,
	MIPS_INS_SUBUH,
	MIPS_INS_SUBUH_R,
	MIPS_INS_SUBU,
	MIPS_INS_SUBU_S,
	MIPS_INS_SUBVI,
	MIPS_INS_SUBV,
	MIPS_INS_SUXC1,
	MIPS_INS_SW,
	MIPS_INS_SW16,
	MIPS_INS_SWC1,
	MIPS_INS_SWC2,
	MIPS_INS_SWC3,
	MIPS_INS_SWL,
	MIPS_INS_SWM16,
	MIPS_INS_SWM32,
	MIPS_INS_SWP,
	MIPS_INS_SWR,
	MIPS_INS_SWXC1,
	MIPS_INS_SYNC,
	MIPS_INS_SYNCI,
	MIPS_INS_SYSCALL,
	MIPS_INS_TEQ,
	MIPS_INS_TEQI,
	MIPS_INS_TGE,
	MIPS_INS_TGEI,
	MIPS_INS_TGEIU,
	MIPS_INS_TGEU,
	MIPS_INS_TLBP,
	MIPS_INS_TLBR,
	MIPS_INS_TLBWI,
	MIPS_INS_TLBWR,
	MIPS_INS_TLT,
	MIPS_INS_TLTI,
	MIPS_INS_TLTIU,
	MIPS_INS_TLTU,
	MIPS_INS_TNE,
	MIPS_INS_TNEI,
	MIPS_INS_TRUNC,
	MIPS_INS_V3MULU,
	MIPS_INS_VMM0,
	MIPS_INS_VMULU,
	MIPS_INS_VSHF,
	MIPS_INS_WAIT,
	MIPS_INS_WRDSP,
	MIPS_INS_WSBH,
	MIPS_INS_XOR,
	MIPS_INS_XOR16,
	MIPS_INS_XORI,

	//> some alias instructions
	MIPS_INS_NOP,
	MIPS_INS_NEGU,

	//> special instructions
	MIPS_INS_JALR_HB,	// jump and link with Hazard Barrier
	MIPS_INS_JR_HB,		// jump register with Hazard Barrier

	MIPS_INS_ENDING,
} mips_insn;

/// Group of MIPS instructions
typedef enum mips_insn_group {
	MIPS_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	MIPS_GRP_JUMP,	///< = CS_GRP_JUMP
	// all call instructions
	MIPS_GRP_CALL,	///< = CS_GRP_CALL
	// all return instructions
	MIPS_GRP_RET,	///< = CS_GRP_RET
	// all interrupt instructions (int+syscall)
	MIPS_GRP_INT,	///< = CS_GRP_INT
	// all interrupt return instructions
	MIPS_GRP_IRET,	///< = CS_GRP_IRET
	// all privileged instructions
	MIPS_GRP_PRIVILEGE,	///< = CS_GRP_PRIVILEGE
	// all relative branching instructions
	MIPS_GRP_BRANCH_RELATIVE, ///< = CS_GRP_BRANCH_RELATIVE

	// Architecture-specific groups
	MIPS_GRP_BITCOUNT = 128,
	MIPS_GRP_DSP,
	MIPS_GRP_DSPR2,
	MIPS_GRP_FPIDX,
	MIPS_GRP_MSA,
	MIPS_GRP_MIPS32R2,
	MIPS_GRP_MIPS64,
	MIPS_GRP_MIPS64R2,
	MIPS_GRP_SEINREG,
	MIPS_GRP_STDENC,
	MIPS_GRP_SWAP,
	MIPS_GRP_MICROMIPS,
	MIPS_GRP_MIPS16MODE,
	MIPS_GRP_FP64BIT,
	MIPS_GRP_NONANSFPMATH,
	MIPS_GRP_NOTFP64BIT,
	MIPS_GRP_NOTINMICROMIPS,
	MIPS_GRP_NOTNACL,
	MIPS_GRP_NOTMIPS32R6,
	MIPS_GRP_NOTMIPS64R6,
	MIPS_GRP_CNMIPS,
	MIPS_GRP_MIPS32,
	MIPS_GRP_MIPS32R6,
	MIPS_GRP_MIPS64R6,
	MIPS_GRP_MIPS2,
	MIPS_GRP_MIPS3,
	MIPS_GRP_MIPS3_32,
	MIPS_GRP_MIPS3_32R2,
	MIPS_GRP_MIPS4_32,
	MIPS_GRP_MIPS4_32R2,
	MIPS_GRP_MIPS5_32R2,
	MIPS_GRP_GP32BIT,
	MIPS_GRP_GP64BIT,

	MIPS_GRP_ENDING,
} mips_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/platform.h`:

```h
/* Capstone Disassembly Engine */
/* By Axel Souchet & Nguyen Anh Quynh, 2014 */

#ifndef CAPSTONE_PLATFORM_H
#define CAPSTONE_PLATFORM_H


// handle C99 issue (for pre-2013 VisualStudio)
#if !defined(__CYGWIN__) && !defined(__MINGW32__) && !defined(__MINGW64__) && (defined (WIN32) || defined (WIN64) || defined (_WIN32) || defined (_WIN64))
// MSVC

// stdbool.h
#if (_MSC_VER < 1800) || defined(_KERNEL_MODE)
// this system does not have stdbool.h
#ifndef __cplusplus
typedef unsigned char bool;
#define false 0
#define true 1
#endif  // __cplusplus

#else
// VisualStudio 2013+ -> C99 is supported
#include <stdbool.h>
#endif  // (_MSC_VER < 1800) || defined(_KERNEL_MODE)

#else
// not MSVC -> C99 is supported
#include <stdbool.h>
#endif  // !defined(__CYGWIN__) && !defined(__MINGW32__) && !defined(__MINGW64__) && (defined (WIN32) || defined (WIN64) || defined (_WIN32) || defined (_WIN64))


// handle inttypes.h / stdint.h compatibility
#if defined(_WIN32_WCE) && (_WIN32_WCE < 0x800)
#include "windowsce/stdint.h"
#endif  // defined(_WIN32_WCE) && (_WIN32_WCE < 0x800)

#if defined(CAPSTONE_HAS_OSXKERNEL) || (defined(_MSC_VER) && (_MSC_VER <= 1700 || defined(_KERNEL_MODE)))
// this system does not have inttypes.h

#if defined(_MSC_VER) && (_MSC_VER <= 1600 || defined(_KERNEL_MODE))
// this system does not have stdint.h
typedef signed char  int8_t;
typedef signed short int16_t;
typedef signed int   int32_t;
typedef unsigned char  uint8_t;
typedef unsigned short uint16_t;
typedef unsigned int   uint32_t;
typedef signed long long   int64_t;
typedef unsigned long long uint64_t;
#endif  // defined(_MSC_VER) && (_MSC_VER <= 1600 || defined(_KERNEL_MODE))

#if defined(_MSC_VER) && (_MSC_VER < 1600 || defined(_KERNEL_MODE))
#define INT8_MIN         (-127i8 - 1)
#define INT16_MIN        (-32767i16 - 1)
#define INT32_MIN        (-2147483647i32 - 1)
#define INT64_MIN        (-9223372036854775807i64 - 1)
#define INT8_MAX         127i8
#define INT16_MAX        32767i16
#define INT32_MAX        2147483647i32
#define INT64_MAX        9223372036854775807i64
#define UINT8_MAX        0xffui8
#define UINT16_MAX       0xffffui16
#define UINT32_MAX       0xffffffffui32
#define UINT64_MAX       0xffffffffffffffffui64
#endif  // defined(_MSC_VER) && (_MSC_VER < 1600 || defined(_KERNEL_MODE))

#ifdef CAPSTONE_HAS_OSXKERNEL
// this system has stdint.h
#include <stdint.h>
#endif

#define __PRI_8_LENGTH_MODIFIER__ "hh"
#define __PRI_64_LENGTH_MODIFIER__ "ll"

#define PRId8         __PRI_8_LENGTH_MODIFIER__ "d"
#define PRIi8         __PRI_8_LENGTH_MODIFIER__ "i"
#define PRIo8         __PRI_8_LENGTH_MODIFIER__ "o"
#define PRIu8         __PRI_8_LENGTH_MODIFIER__ "u"
#define PRIx8         __PRI_8_LENGTH_MODIFIER__ "x"
#define PRIX8         __PRI_8_LENGTH_MODIFIER__ "X"

#define PRId16        "hd"
#define PRIi16        "hi"
#define PRIo16        "ho"
#define PRIu16        "hu"
#define PRIx16        "hx"
#define PRIX16        "hX"

#if defined(_MSC_VER) && _MSC_VER <= 1700
#define PRId32        "ld"
#define PRIi32        "li"
#define PRIo32        "lo"
#define PRIu32        "lu"
#define PRIx32        "lx"
#define PRIX32        "lX"
#else	// OSX
#define PRId32        "d"
#define PRIi32        "i"
#define PRIo32        "o"
#define PRIu32        "u"
#define PRIx32        "x"
#define PRIX32        "X"
#endif  // defined(_MSC_VER) && _MSC_VER <= 1700

#if defined(_MSC_VER) && _MSC_VER <= 1700
// redefine functions from inttypes.h used in cstool
#define strtoull _strtoui64
#endif

#define PRId64        __PRI_64_LENGTH_MODIFIER__ "d"
#define PRIi64        __PRI_64_LENGTH_MODIFIER__ "i"
#define PRIo64        __PRI_64_LENGTH_MODIFIER__ "o"
#define PRIu64        __PRI_64_LENGTH_MODIFIER__ "u"
#define PRIx64        __PRI_64_LENGTH_MODIFIER__ "x"
#define PRIX64        __PRI_64_LENGTH_MODIFIER__ "X"

#else
// this system has inttypes.h by default
#include <inttypes.h>
#endif  // defined(CAPSTONE_HAS_OSXKERNEL) || (defined(_MSC_VER) && (_MSC_VER <= 1700 || defined(_KERNEL_MODE)))

#endif

```

`CodeCleaner/capstone/include/capstone/ppc.h`:

```h
#ifndef CAPSTONE_PPC_H
#define CAPSTONE_PPC_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2013-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// PPC branch codes for some branch instructions
typedef enum ppc_bc {
	PPC_BC_INVALID  = 0,
	PPC_BC_LT       = (0 << 5) | 12,
	PPC_BC_LE       = (1 << 5) |  4,
	PPC_BC_EQ       = (2 << 5) | 12,
	PPC_BC_GE       = (0 << 5) |  4,
	PPC_BC_GT       = (1 << 5) | 12,
	PPC_BC_NE       = (2 << 5) |  4,
	PPC_BC_UN       = (3 << 5) | 12,
	PPC_BC_NU       = (3 << 5) |  4,

	// extra conditions
	PPC_BC_SO = (4 << 5) | 12,	///< summary overflow
	PPC_BC_NS = (4 << 5) | 4,	///< not summary overflow
} ppc_bc;

/// PPC branch hint for some branch instructions
typedef enum ppc_bh {
	PPC_BH_INVALID = 0,	///< no hint
	PPC_BH_PLUS,	///< PLUS hint
	PPC_BH_MINUS,	///< MINUS hint
} ppc_bh;

/// Operand type for instruction's operands
typedef enum ppc_op_type {
	PPC_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	PPC_OP_REG, ///< = CS_OP_REG (Register operand).
	PPC_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	PPC_OP_MEM, ///< = CS_OP_MEM (Memory operand).
	PPC_OP_CRX = 64,	///< Condition Register field
} ppc_op_type;

/// PPC registers
typedef enum ppc_reg {
	PPC_REG_INVALID = 0,

	PPC_REG_CARRY,
	PPC_REG_CR0,
	PPC_REG_CR1,
	PPC_REG_CR2,
	PPC_REG_CR3,
	PPC_REG_CR4,
	PPC_REG_CR5,
	PPC_REG_CR6,
	PPC_REG_CR7,
	PPC_REG_CTR,
	PPC_REG_F0,
	PPC_REG_F1,
	PPC_REG_F2,
	PPC_REG_F3,
	PPC_REG_F4,
	PPC_REG_F5,
	PPC_REG_F6,
	PPC_REG_F7,
	PPC_REG_F8,
	PPC_REG_F9,
	PPC_REG_F10,
	PPC_REG_F11,
	PPC_REG_F12,
	PPC_REG_F13,
	PPC_REG_F14,
	PPC_REG_F15,
	PPC_REG_F16,
	PPC_REG_F17,
	PPC_REG_F18,
	PPC_REG_F19,
	PPC_REG_F20,
	PPC_REG_F21,
	PPC_REG_F22,
	PPC_REG_F23,
	PPC_REG_F24,
	PPC_REG_F25,
	PPC_REG_F26,
	PPC_REG_F27,
	PPC_REG_F28,
	PPC_REG_F29,
	PPC_REG_F30,
	PPC_REG_F31,
	PPC_REG_LR,
	PPC_REG_R0,
	PPC_REG_R1,
	PPC_REG_R2,
	PPC_REG_R3,
	PPC_REG_R4,
	PPC_REG_R5,
	PPC_REG_R6,
	PPC_REG_R7,
	PPC_REG_R8,
	PPC_REG_R9,
	PPC_REG_R10,
	PPC_REG_R11,
	PPC_REG_R12,
	PPC_REG_R13,
	PPC_REG_R14,
	PPC_REG_R15,
	PPC_REG_R16,
	PPC_REG_R17,
	PPC_REG_R18,
	PPC_REG_R19,
	PPC_REG_R20,
	PPC_REG_R21,
	PPC_REG_R22,
	PPC_REG_R23,
	PPC_REG_R24,
	PPC_REG_R25,
	PPC_REG_R26,
	PPC_REG_R27,
	PPC_REG_R28,
	PPC_REG_R29,
	PPC_REG_R30,
	PPC_REG_R31,
	PPC_REG_V0,
	PPC_REG_V1,
	PPC_REG_V2,
	PPC_REG_V3,
	PPC_REG_V4,
	PPC_REG_V5,
	PPC_REG_V6,
	PPC_REG_V7,
	PPC_REG_V8,
	PPC_REG_V9,
	PPC_REG_V10,
	PPC_REG_V11,
	PPC_REG_V12,
	PPC_REG_V13,
	PPC_REG_V14,
	PPC_REG_V15,
	PPC_REG_V16,
	PPC_REG_V17,
	PPC_REG_V18,
	PPC_REG_V19,
	PPC_REG_V20,
	PPC_REG_V21,
	PPC_REG_V22,
	PPC_REG_V23,
	PPC_REG_V24,
	PPC_REG_V25,
	PPC_REG_V26,
	PPC_REG_V27,
	PPC_REG_V28,
	PPC_REG_V29,
	PPC_REG_V30,
	PPC_REG_V31,
	PPC_REG_VRSAVE,
	PPC_REG_VS0,
	PPC_REG_VS1,
	PPC_REG_VS2,
	PPC_REG_VS3,
	PPC_REG_VS4,
	PPC_REG_VS5,
	PPC_REG_VS6,
	PPC_REG_VS7,
	PPC_REG_VS8,
	PPC_REG_VS9,
	PPC_REG_VS10,
	PPC_REG_VS11,
	PPC_REG_VS12,
	PPC_REG_VS13,
	PPC_REG_VS14,
	PPC_REG_VS15,
	PPC_REG_VS16,
	PPC_REG_VS17,
	PPC_REG_VS18,
	PPC_REG_VS19,
	PPC_REG_VS20,
	PPC_REG_VS21,
	PPC_REG_VS22,
	PPC_REG_VS23,
	PPC_REG_VS24,
	PPC_REG_VS25,
	PPC_REG_VS26,
	PPC_REG_VS27,
	PPC_REG_VS28,
	PPC_REG_VS29,
	PPC_REG_VS30,
	PPC_REG_VS31,
	PPC_REG_VS32,
	PPC_REG_VS33,
	PPC_REG_VS34,
	PPC_REG_VS35,
	PPC_REG_VS36,
	PPC_REG_VS37,
	PPC_REG_VS38,
	PPC_REG_VS39,
	PPC_REG_VS40,
	PPC_REG_VS41,
	PPC_REG_VS42,
	PPC_REG_VS43,
	PPC_REG_VS44,
	PPC_REG_VS45,
	PPC_REG_VS46,
	PPC_REG_VS47,
	PPC_REG_VS48,
	PPC_REG_VS49,
	PPC_REG_VS50,
	PPC_REG_VS51,
	PPC_REG_VS52,
	PPC_REG_VS53,
	PPC_REG_VS54,
	PPC_REG_VS55,
	PPC_REG_VS56,
	PPC_REG_VS57,
	PPC_REG_VS58,
	PPC_REG_VS59,
	PPC_REG_VS60,
	PPC_REG_VS61,
	PPC_REG_VS62,
	PPC_REG_VS63,
	PPC_REG_Q0,
	PPC_REG_Q1,
	PPC_REG_Q2,
	PPC_REG_Q3,
	PPC_REG_Q4,
	PPC_REG_Q5,
	PPC_REG_Q6,
	PPC_REG_Q7,
	PPC_REG_Q8,
	PPC_REG_Q9,
	PPC_REG_Q10,
	PPC_REG_Q11,
	PPC_REG_Q12,
	PPC_REG_Q13,
	PPC_REG_Q14,
	PPC_REG_Q15,
	PPC_REG_Q16,
	PPC_REG_Q17,
	PPC_REG_Q18,
	PPC_REG_Q19,
	PPC_REG_Q20,
	PPC_REG_Q21,
	PPC_REG_Q22,
	PPC_REG_Q23,
	PPC_REG_Q24,
	PPC_REG_Q25,
	PPC_REG_Q26,
	PPC_REG_Q27,
	PPC_REG_Q28,
	PPC_REG_Q29,
	PPC_REG_Q30,
	PPC_REG_Q31,

	// extra registers for PPCMapping.c
	PPC_REG_RM,
	PPC_REG_CTR8,
	PPC_REG_LR8,
	PPC_REG_CR1EQ,
	PPC_REG_X2,

	PPC_REG_ENDING,   // <-- mark the end of the list of registers
} ppc_reg;

/// Instruction's operand referring to memory
/// This is associated with PPC_OP_MEM operand type above
typedef struct ppc_op_mem {
	ppc_reg base;	///< base register
	int32_t disp;	///< displacement/offset value
} ppc_op_mem;

typedef struct ppc_op_crx {
	unsigned int scale;
	ppc_reg reg;
	ppc_bc cond;
} ppc_op_crx;

/// Instruction operand
typedef struct cs_ppc_op {
	ppc_op_type type;	///< operand type
	union {
		ppc_reg reg;	///< register value for REG operand
		int64_t imm;		///< immediate value for IMM operand
		ppc_op_mem mem;		///< base/disp value for MEM operand
		ppc_op_crx crx;		///< operand with condition register
	};
} cs_ppc_op;

/// Instruction structure
typedef struct cs_ppc {
	/// branch code for branch instructions
	ppc_bc bc;

	/// branch hint for branch instructions
	ppc_bh bh;

	/// if update_cr0 = True, then this 'dot' insn updates CR0
	bool update_cr0;

	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;
	cs_ppc_op operands[8]; ///< operands for this instruction.
} cs_ppc;

/// PPC instruction
typedef enum ppc_insn {
	PPC_INS_INVALID = 0,

	PPC_INS_ADD,
	PPC_INS_ADDC,
	PPC_INS_ADDE,
	PPC_INS_ADDI,
	PPC_INS_ADDIC,
	PPC_INS_ADDIS,
	PPC_INS_ADDME,
	PPC_INS_ADDZE,
	PPC_INS_AND,
	PPC_INS_ANDC,
	PPC_INS_ANDIS,
	PPC_INS_ANDI,
	PPC_INS_ATTN,
	PPC_INS_B,
	PPC_INS_BA,
	PPC_INS_BC,
	PPC_INS_BCCTR,
	PPC_INS_BCCTRL,
	PPC_INS_BCL,
	PPC_INS_BCLR,
	PPC_INS_BCLRL,
	PPC_INS_BCTR,
	PPC_INS_BCTRL,
	PPC_INS_BCT,
	PPC_INS_BDNZ,
	PPC_INS_BDNZA,
	PPC_INS_BDNZL,
	PPC_INS_BDNZLA,
	PPC_INS_BDNZLR,
	PPC_INS_BDNZLRL,
	PPC_INS_BDZ,
	PPC_INS_BDZA,
	PPC_INS_BDZL,
	PPC_INS_BDZLA,
	PPC_INS_BDZLR,
	PPC_INS_BDZLRL,
	PPC_INS_BL,
	PPC_INS_BLA,
	PPC_INS_BLR,
	PPC_INS_BLRL,
	PPC_INS_BRINC,
	PPC_INS_CMPB,
	PPC_INS_CMPD,
	PPC_INS_CMPDI,
	PPC_INS_CMPLD,
	PPC_INS_CMPLDI,
	PPC_INS_CMPLW,
	PPC_INS_CMPLWI,
	PPC_INS_CMPW,
	PPC_INS_CMPWI,
	PPC_INS_CNTLZD,
	PPC_INS_CNTLZW,
	PPC_INS_CREQV,
	PPC_INS_CRXOR,
	PPC_INS_CRAND,
	PPC_INS_CRANDC,
	PPC_INS_CRNAND,
	PPC_INS_CRNOR,
	PPC_INS_CROR,
	PPC_INS_CRORC,
	PPC_INS_DCBA,
	PPC_INS_DCBF,
	PPC_INS_DCBI,
	PPC_INS_DCBST,
	PPC_INS_DCBT,
	PPC_INS_DCBTST,
	PPC_INS_DCBZ,
	PPC_INS_DCBZL,
	PPC_INS_DCCCI,
	PPC_INS_DIVD,
	PPC_INS_DIVDU,
	PPC_INS_DIVW,
	PPC_INS_DIVWU,
	PPC_INS_DSS,
	PPC_INS_DSSALL,
	PPC_INS_DST,
	PPC_INS_DSTST,
	PPC_INS_DSTSTT,
	PPC_INS_DSTT,
	PPC_INS_EQV,
	PPC_INS_EVABS,
	PPC_INS_EVADDIW,
	PPC_INS_EVADDSMIAAW,
	PPC_INS_EVADDSSIAAW,
	PPC_INS_EVADDUMIAAW,
	PPC_INS_EVADDUSIAAW,
	PPC_INS_EVADDW,
	PPC_INS_EVAND,
	PPC_INS_EVANDC,
	PPC_INS_EVCMPEQ,
	PPC_INS_EVCMPGTS,
	PPC_INS_EVCMPGTU,
	PPC_INS_EVCMPLTS,
	PPC_INS_EVCMPLTU,
	PPC_INS_EVCNTLSW,
	PPC_INS_EVCNTLZW,
	PPC_INS_EVDIVWS,
	PPC_INS_EVDIVWU,
	PPC_INS_EVEQV,
	PPC_INS_EVEXTSB,
	PPC_INS_EVEXTSH,
	PPC_INS_EVLDD,
	PPC_INS_EVLDDX,
	PPC_INS_EVLDH,
	PPC_INS_EVLDHX,
	PPC_INS_EVLDW,
	PPC_INS_EVLDWX,
	PPC_INS_EVLHHESPLAT,
	PPC_INS_EVLHHESPLATX,
	PPC_INS_EVLHHOSSPLAT,
	PPC_INS_EVLHHOSSPLATX,
	PPC_INS_EVLHHOUSPLAT,
	PPC_INS_EVLHHOUSPLATX,
	PPC_INS_EVLWHE,
	PPC_INS_EVLWHEX,
	PPC_INS_EVLWHOS,
	PPC_INS_EVLWHOSX,
	PPC_INS_EVLWHOU,
	PPC_INS_EVLWHOUX,
	PPC_INS_EVLWHSPLAT,
	PPC_INS_EVLWHSPLATX,
	PPC_INS_EVLWWSPLAT,
	PPC_INS_EVLWWSPLATX,
	PPC_INS_EVMERGEHI,
	PPC_INS_EVMERGEHILO,
	PPC_INS_EVMERGELO,
	PPC_INS_EVMERGELOHI,
	PPC_INS_EVMHEGSMFAA,
	PPC_INS_EVMHEGSMFAN,
	PPC_INS_EVMHEGSMIAA,
	PPC_INS_EVMHEGSMIAN,
	PPC_INS_EVMHEGUMIAA,
	PPC_INS_EVMHEGUMIAN,
	PPC_INS_EVMHESMF,
	PPC_INS_EVMHESMFA,
	PPC_INS_EVMHESMFAAW,
	PPC_INS_EVMHESMFANW,
	PPC_INS_EVMHESMI,
	PPC_INS_EVMHESMIA,
	PPC_INS_EVMHESMIAAW,
	PPC_INS_EVMHESMIANW,
	PPC_INS_EVMHESSF,
	PPC_INS_EVMHESSFA,
	PPC_INS_EVMHESSFAAW,
	PPC_INS_EVMHESSFANW,
	PPC_INS_EVMHESSIAAW,
	PPC_INS_EVMHESSIANW,
	PPC_INS_EVMHEUMI,
	PPC_INS_EVMHEUMIA,
	PPC_INS_EVMHEUMIAAW,
	PPC_INS_EVMHEUMIANW,
	PPC_INS_EVMHEUSIAAW,
	PPC_INS_EVMHEUSIANW,
	PPC_INS_EVMHOGSMFAA,
	PPC_INS_EVMHOGSMFAN,
	PPC_INS_EVMHOGSMIAA,
	PPC_INS_EVMHOGSMIAN,
	PPC_INS_EVMHOGUMIAA,
	PPC_INS_EVMHOGUMIAN,
	PPC_INS_EVMHOSMF,
	PPC_INS_EVMHOSMFA,
	PPC_INS_EVMHOSMFAAW,
	PPC_INS_EVMHOSMFANW,
	PPC_INS_EVMHOSMI,
	PPC_INS_EVMHOSMIA,
	PPC_INS_EVMHOSMIAAW,
	PPC_INS_EVMHOSMIANW,
	PPC_INS_EVMHOSSF,
	PPC_INS_EVMHOSSFA,
	PPC_INS_EVMHOSSFAAW,
	PPC_INS_EVMHOSSFANW,
	PPC_INS_EVMHOSSIAAW,
	PPC_INS_EVMHOSSIANW,
	PPC_INS_EVMHOUMI,
	PPC_INS_EVMHOUMIA,
	PPC_INS_EVMHOUMIAAW,
	PPC_INS_EVMHOUMIANW,
	PPC_INS_EVMHOUSIAAW,
	PPC_INS_EVMHOUSIANW,
	PPC_INS_EVMRA,
	PPC_INS_EVMWHSMF,
	PPC_INS_EVMWHSMFA,
	PPC_INS_EVMWHSMI,
	PPC_INS_EVMWHSMIA,
	PPC_INS_EVMWHSSF,
	PPC_INS_EVMWHSSFA,
	PPC_INS_EVMWHUMI,
	PPC_INS_EVMWHUMIA,
	PPC_INS_EVMWLSMIAAW,
	PPC_INS_EVMWLSMIANW,
	PPC_INS_EVMWLSSIAAW,
	PPC_INS_EVMWLSSIANW,
	PPC_INS_EVMWLUMI,
	PPC_INS_EVMWLUMIA,
	PPC_INS_EVMWLUMIAAW,
	PPC_INS_EVMWLUMIANW,
	PPC_INS_EVMWLUSIAAW,
	PPC_INS_EVMWLUSIANW,
	PPC_INS_EVMWSMF,
	PPC_INS_EVMWSMFA,
	PPC_INS_EVMWSMFAA,
	PPC_INS_EVMWSMFAN,
	PPC_INS_EVMWSMI,
	PPC_INS_EVMWSMIA,
	PPC_INS_EVMWSMIAA,
	PPC_INS_EVMWSMIAN,
	PPC_INS_EVMWSSF,
	PPC_INS_EVMWSSFA,
	PPC_INS_EVMWSSFAA,
	PPC_INS_EVMWSSFAN,
	PPC_INS_EVMWUMI,
	PPC_INS_EVMWUMIA,
	PPC_INS_EVMWUMIAA,
	PPC_INS_EVMWUMIAN,
	PPC_INS_EVNAND,
	PPC_INS_EVNEG,
	PPC_INS_EVNOR,
	PPC_INS_EVOR,
	PPC_INS_EVORC,
	PPC_INS_EVRLW,
	PPC_INS_EVRLWI,
	PPC_INS_EVRNDW,
	PPC_INS_EVSLW,
	PPC_INS_EVSLWI,
	PPC_INS_EVSPLATFI,
	PPC_INS_EVSPLATI,
	PPC_INS_EVSRWIS,
	PPC_INS_EVSRWIU,
	PPC_INS_EVSRWS,
	PPC_INS_EVSRWU,
	PPC_INS_EVSTDD,
	PPC_INS_EVSTDDX,
	PPC_INS_EVSTDH,
	PPC_INS_EVSTDHX,
	PPC_INS_EVSTDW,
	PPC_INS_EVSTDWX,
	PPC_INS_EVSTWHE,
	PPC_INS_EVSTWHEX,
	PPC_INS_EVSTWHO,
	PPC_INS_EVSTWHOX,
	PPC_INS_EVSTWWE,
	PPC_INS_EVSTWWEX,
	PPC_INS_EVSTWWO,
	PPC_INS_EVSTWWOX,
	PPC_INS_EVSUBFSMIAAW,
	PPC_INS_EVSUBFSSIAAW,
	PPC_INS_EVSUBFUMIAAW,
	PPC_INS_EVSUBFUSIAAW,
	PPC_INS_EVSUBFW,
	PPC_INS_EVSUBIFW,
	PPC_INS_EVXOR,
	PPC_INS_EXTSB,
	PPC_INS_EXTSH,
	PPC_INS_EXTSW,
	PPC_INS_EIEIO,
	PPC_INS_FABS,
	PPC_INS_FADD,
	PPC_INS_FADDS,
	PPC_INS_FCFID,
	PPC_INS_FCFIDS,
	PPC_INS_FCFIDU,
	PPC_INS_FCFIDUS,
	PPC_INS_FCMPU,
	PPC_INS_FCPSGN,
	PPC_INS_FCTID,
	PPC_INS_FCTIDUZ,
	PPC_INS_FCTIDZ,
	PPC_INS_FCTIW,
	PPC_INS_FCTIWUZ,
	PPC_INS_FCTIWZ,
	PPC_INS_FDIV,
	PPC_INS_FDIVS,
	PPC_INS_FMADD,
	PPC_INS_FMADDS,
	PPC_INS_FMR,
	PPC_INS_FMSUB,
	PPC_INS_FMSUBS,
	PPC_INS_FMUL,
	PPC_INS_FMULS,
	PPC_INS_FNABS,
	PPC_INS_FNEG,
	PPC_INS_FNMADD,
	PPC_INS_FNMADDS,
	PPC_INS_FNMSUB,
	PPC_INS_FNMSUBS,
	PPC_INS_FRE,
	PPC_INS_FRES,
	PPC_INS_FRIM,
	PPC_INS_FRIN,
	PPC_INS_FRIP,
	PPC_INS_FRIZ,
	PPC_INS_FRSP,
	PPC_INS_FRSQRTE,
	PPC_INS_FRSQRTES,
	PPC_INS_FSEL,
	PPC_INS_FSQRT,
	PPC_INS_FSQRTS,
	PPC_INS_FSUB,
	PPC_INS_FSUBS,
	PPC_INS_ICBI,
	PPC_INS_ICBT,
	PPC_INS_ICCCI,
	PPC_INS_ISEL,
	PPC_INS_ISYNC,
	PPC_INS_LA,
	PPC_INS_LBZ,
	PPC_INS_LBZCIX,
	PPC_INS_LBZU,
	PPC_INS_LBZUX,
	PPC_INS_LBZX,
	PPC_INS_LD,
	PPC_INS_LDARX,
	PPC_INS_LDBRX,
	PPC_INS_LDCIX,
	PPC_INS_LDU,
	PPC_INS_LDUX,
	PPC_INS_LDX,
	PPC_INS_LFD,
	PPC_INS_LFDU,
	PPC_INS_LFDUX,
	PPC_INS_LFDX,
	PPC_INS_LFIWAX,
	PPC_INS_LFIWZX,
	PPC_INS_LFS,
	PPC_INS_LFSU,
	PPC_INS_LFSUX,
	PPC_INS_LFSX,
	PPC_INS_LHA,
	PPC_INS_LHAU,
	PPC_INS_LHAUX,
	PPC_INS_LHAX,
	PPC_INS_LHBRX,
	PPC_INS_LHZ,
	PPC_INS_LHZCIX,
	PPC_INS_LHZU,
	PPC_INS_LHZUX,
	PPC_INS_LHZX,
	PPC_INS_LI,
	PPC_INS_LIS,
	PPC_INS_LMW,
	PPC_INS_LSWI,
	PPC_INS_LVEBX,
	PPC_INS_LVEHX,
	PPC_INS_LVEWX,
	PPC_INS_LVSL,
	PPC_INS_LVSR,
	PPC_INS_LVX,
	PPC_INS_LVXL,
	PPC_INS_LWA,
	PPC_INS_LWARX,
	PPC_INS_LWAUX,
	PPC_INS_LWAX,
	PPC_INS_LWBRX,
	PPC_INS_LWZ,
	PPC_INS_LWZCIX,
	PPC_INS_LWZU,
	PPC_INS_LWZUX,
	PPC_INS_LWZX,
	PPC_INS_LXSDX,
	PPC_INS_LXVD2X,
	PPC_INS_LXVDSX,
	PPC_INS_LXVW4X,
	PPC_INS_MBAR,
	PPC_INS_MCRF,
	PPC_INS_MCRFS,
	PPC_INS_MFCR,
	PPC_INS_MFCTR,
	PPC_INS_MFDCR,
	PPC_INS_MFFS,
	PPC_INS_MFLR,
	PPC_INS_MFMSR,
	PPC_INS_MFOCRF,
	PPC_INS_MFSPR,
	PPC_INS_MFSR,
	PPC_INS_MFSRIN,
	PPC_INS_MFTB,
	PPC_INS_MFVSCR,
	PPC_INS_MSYNC,
	PPC_INS_MTCRF,
	PPC_INS_MTCTR,
	PPC_INS_MTDCR,
	PPC_INS_MTFSB0,
	PPC_INS_MTFSB1,
	PPC_INS_MTFSF,
	PPC_INS_MTFSFI,
	PPC_INS_MTLR,
	PPC_INS_MTMSR,
	PPC_INS_MTMSRD,
	PPC_INS_MTOCRF,
	PPC_INS_MTSPR,
	PPC_INS_MTSR,
	PPC_INS_MTSRIN,
	PPC_INS_MTVSCR,
	PPC_INS_MULHD,
	PPC_INS_MULHDU,
	PPC_INS_MULHW,
	PPC_INS_MULHWU,
	PPC_INS_MULLD,
	PPC_INS_MULLI,
	PPC_INS_MULLW,
	PPC_INS_NAND,
	PPC_INS_NEG,
	PPC_INS_NOP,
	PPC_INS_ORI,
	PPC_INS_NOR,
	PPC_INS_OR,
	PPC_INS_ORC,
	PPC_INS_ORIS,
	PPC_INS_POPCNTD,
	PPC_INS_POPCNTW,
	PPC_INS_QVALIGNI,
	PPC_INS_QVESPLATI,
	PPC_INS_QVFABS,
	PPC_INS_QVFADD,
	PPC_INS_QVFADDS,
	PPC_INS_QVFCFID,
	PPC_INS_QVFCFIDS,
	PPC_INS_QVFCFIDU,
	PPC_INS_QVFCFIDUS,
	PPC_INS_QVFCMPEQ,
	PPC_INS_QVFCMPGT,
	PPC_INS_QVFCMPLT,
	PPC_INS_QVFCPSGN,
	PPC_INS_QVFCTID,
	PPC_INS_QVFCTIDU,
	PPC_INS_QVFCTIDUZ,
	PPC_INS_QVFCTIDZ,
	PPC_INS_QVFCTIW,
	PPC_INS_QVFCTIWU,
	PPC_INS_QVFCTIWUZ,
	PPC_INS_QVFCTIWZ,
	PPC_INS_QVFLOGICAL,
	PPC_INS_QVFMADD,
	PPC_INS_QVFMADDS,
	PPC_INS_QVFMR,
	PPC_INS_QVFMSUB,
	PPC_INS_QVFMSUBS,
	PPC_INS_QVFMUL,
	PPC_INS_QVFMULS,
	PPC_INS_QVFNABS,
	PPC_INS_QVFNEG,
	PPC_INS_QVFNMADD,
	PPC_INS_QVFNMADDS,
	PPC_INS_QVFNMSUB,
	PPC_INS_QVFNMSUBS,
	PPC_INS_QVFPERM,
	PPC_INS_QVFRE,
	PPC_INS_QVFRES,
	PPC_INS_QVFRIM,
	PPC_INS_QVFRIN,
	PPC_INS_QVFRIP,
	PPC_INS_QVFRIZ,
	PPC_INS_QVFRSP,
	PPC_INS_QVFRSQRTE,
	PPC_INS_QVFRSQRTES,
	PPC_INS_QVFSEL,
	PPC_INS_QVFSUB,
	PPC_INS_QVFSUBS,
	PPC_INS_QVFTSTNAN,
	PPC_INS_QVFXMADD,
	PPC_INS_QVFXMADDS,
	PPC_INS_QVFXMUL,
	PPC_INS_QVFXMULS,
	PPC_INS_QVFXXCPNMADD,
	PPC_INS_QVFXXCPNMADDS,
	PPC_INS_QVFXXMADD,
	PPC_INS_QVFXXMADDS,
	PPC_INS_QVFXXNPMADD,
	PPC_INS_QVFXXNPMADDS,
	PPC_INS_QVGPCI,
	PPC_INS_QVLFCDUX,
	PPC_INS_QVLFCDUXA,
	PPC_INS_QVLFCDX,
	PPC_INS_QVLFCDXA,
	PPC_INS_QVLFCSUX,
	PPC_INS_QVLFCSUXA,
	PPC_INS_QVLFCSX,
	PPC_INS_QVLFCSXA,
	PPC_INS_QVLFDUX,
	PPC_INS_QVLFDUXA,
	PPC_INS_QVLFDX,
	PPC_INS_QVLFDXA,
	PPC_INS_QVLFIWAX,
	PPC_INS_QVLFIWAXA,
	PPC_INS_QVLFIWZX,
	PPC_INS_QVLFIWZXA,
	PPC_INS_QVLFSUX,
	PPC_INS_QVLFSUXA,
	PPC_INS_QVLFSX,
	PPC_INS_QVLFSXA,
	PPC_INS_QVLPCLDX,
	PPC_INS_QVLPCLSX,
	PPC_INS_QVLPCRDX,
	PPC_INS_QVLPCRSX,
	PPC_INS_QVSTFCDUX,
	PPC_INS_QVSTFCDUXA,
	PPC_INS_QVSTFCDUXI,
	PPC_INS_QVSTFCDUXIA,
	PPC_INS_QVSTFCDX,
	PPC_INS_QVSTFCDXA,
	PPC_INS_QVSTFCDXI,
	PPC_INS_QVSTFCDXIA,
	PPC_INS_QVSTFCSUX,
	PPC_INS_QVSTFCSUXA,
	PPC_INS_QVSTFCSUXI,
	PPC_INS_QVSTFCSUXIA,
	PPC_INS_QVSTFCSX,
	PPC_INS_QVSTFCSXA,
	PPC_INS_QVSTFCSXI,
	PPC_INS_QVSTFCSXIA,
	PPC_INS_QVSTFDUX,
	PPC_INS_QVSTFDUXA,
	PPC_INS_QVSTFDUXI,
	PPC_INS_QVSTFDUXIA,
	PPC_INS_QVSTFDX,
	PPC_INS_QVSTFDXA,
	PPC_INS_QVSTFDXI,
	PPC_INS_QVSTFDXIA,
	PPC_INS_QVSTFIWX,
	PPC_INS_QVSTFIWXA,
	PPC_INS_QVSTFSUX,
	PPC_INS_QVSTFSUXA,
	PPC_INS_QVSTFSUXI,
	PPC_INS_QVSTFSUXIA,
	PPC_INS_QVSTFSX,
	PPC_INS_QVSTFSXA,
	PPC_INS_QVSTFSXI,
	PPC_INS_QVSTFSXIA,
	PPC_INS_RFCI,
	PPC_INS_RFDI,
	PPC_INS_RFI,
	PPC_INS_RFID,
	PPC_INS_RFMCI,
	PPC_INS_RLDCL,
	PPC_INS_RLDCR,
	PPC_INS_RLDIC,
	PPC_INS_RLDICL,
	PPC_INS_RLDICR,
	PPC_INS_RLDIMI,
	PPC_INS_RLWIMI,
	PPC_INS_RLWINM,
	PPC_INS_RLWNM,
	PPC_INS_SC,
	PPC_INS_SLBIA,
	PPC_INS_SLBIE,
	PPC_INS_SLBMFEE,
	PPC_INS_SLBMTE,
	PPC_INS_SLD,
	PPC_INS_SLW,
	PPC_INS_SRAD,
	PPC_INS_SRADI,
	PPC_INS_SRAW,
	PPC_INS_SRAWI,
	PPC_INS_SRD,
	PPC_INS_SRW,
	PPC_INS_STB,
	PPC_INS_STBCIX,
	PPC_INS_STBU,
	PPC_INS_STBUX,
	PPC_INS_STBX,
	PPC_INS_STD,
	PPC_INS_STDBRX,
	PPC_INS_STDCIX,
	PPC_INS_STDCX,
	PPC_INS_STDU,
	PPC_INS_STDUX,
	PPC_INS_STDX,
	PPC_INS_STFD,
	PPC_INS_STFDU,
	PPC_INS_STFDUX,
	PPC_INS_STFDX,
	PPC_INS_STFIWX,
	PPC_INS_STFS,
	PPC_INS_STFSU,
	PPC_INS_STFSUX,
	PPC_INS_STFSX,
	PPC_INS_STH,
	PPC_INS_STHBRX,
	PPC_INS_STHCIX,
	PPC_INS_STHU,
	PPC_INS_STHUX,
	PPC_INS_STHX,
	PPC_INS_STMW,
	PPC_INS_STSWI,
	PPC_INS_STVEBX,
	PPC_INS_STVEHX,
	PPC_INS_STVEWX,
	PPC_INS_STVX,
	PPC_INS_STVXL,
	PPC_INS_STW,
	PPC_INS_STWBRX,
	PPC_INS_STWCIX,
	PPC_INS_STWCX,
	PPC_INS_STWU,
	PPC_INS_STWUX,
	PPC_INS_STWX,
	PPC_INS_STXSDX,
	PPC_INS_STXVD2X,
	PPC_INS_STXVW4X,
	PPC_INS_SUBF,
	PPC_INS_SUBFC,
	PPC_INS_SUBFE,
	PPC_INS_SUBFIC,
	PPC_INS_SUBFME,
	PPC_INS_SUBFZE,
	PPC_INS_SYNC,
	PPC_INS_TD,
	PPC_INS_TDI,
	PPC_INS_TLBIA,
	PPC_INS_TLBIE,
	PPC_INS_TLBIEL,
	PPC_INS_TLBIVAX,
	PPC_INS_TLBLD,
	PPC_INS_TLBLI,
	PPC_INS_TLBRE,
	PPC_INS_TLBSX,
	PPC_INS_TLBSYNC,
	PPC_INS_TLBWE,
	PPC_INS_TRAP,
	PPC_INS_TW,
	PPC_INS_TWI,
	PPC_INS_VADDCUW,
	PPC_INS_VADDFP,
	PPC_INS_VADDSBS,
	PPC_INS_VADDSHS,
	PPC_INS_VADDSWS,
	PPC_INS_VADDUBM,
	PPC_INS_VADDUBS,
	PPC_INS_VADDUDM,
	PPC_INS_VADDUHM,
	PPC_INS_VADDUHS,
	PPC_INS_VADDUWM,
	PPC_INS_VADDUWS,
	PPC_INS_VAND,
	PPC_INS_VANDC,
	PPC_INS_VAVGSB,
	PPC_INS_VAVGSH,
	PPC_INS_VAVGSW,
	PPC_INS_VAVGUB,
	PPC_INS_VAVGUH,
	PPC_INS_VAVGUW,
	PPC_INS_VCFSX,
	PPC_INS_VCFUX,
	PPC_INS_VCLZB,
	PPC_INS_VCLZD,
	PPC_INS_VCLZH,
	PPC_INS_VCLZW,
	PPC_INS_VCMPBFP,
	PPC_INS_VCMPEQFP,
	PPC_INS_VCMPEQUB,
	PPC_INS_VCMPEQUD,
	PPC_INS_VCMPEQUH,
	PPC_INS_VCMPEQUW,
	PPC_INS_VCMPGEFP,
	PPC_INS_VCMPGTFP,
	PPC_INS_VCMPGTSB,
	PPC_INS_VCMPGTSD,
	PPC_INS_VCMPGTSH,
	PPC_INS_VCMPGTSW,
	PPC_INS_VCMPGTUB,
	PPC_INS_VCMPGTUD,
	PPC_INS_VCMPGTUH,
	PPC_INS_VCMPGTUW,
	PPC_INS_VCTSXS,
	PPC_INS_VCTUXS,
	PPC_INS_VEQV,
	PPC_INS_VEXPTEFP,
	PPC_INS_VLOGEFP,
	PPC_INS_VMADDFP,
	PPC_INS_VMAXFP,
	PPC_INS_VMAXSB,
	PPC_INS_VMAXSD,
	PPC_INS_VMAXSH,
	PPC_INS_VMAXSW,
	PPC_INS_VMAXUB,
	PPC_INS_VMAXUD,
	PPC_INS_VMAXUH,
	PPC_INS_VMAXUW,
	PPC_INS_VMHADDSHS,
	PPC_INS_VMHRADDSHS,
	PPC_INS_VMINUD,
	PPC_INS_VMINFP,
	PPC_INS_VMINSB,
	PPC_INS_VMINSD,
	PPC_INS_VMINSH,
	PPC_INS_VMINSW,
	PPC_INS_VMINUB,
	PPC_INS_VMINUH,
	PPC_INS_VMINUW,
	PPC_INS_VMLADDUHM,
	PPC_INS_VMRGHB,
	PPC_INS_VMRGHH,
	PPC_INS_VMRGHW,
	PPC_INS_VMRGLB,
	PPC_INS_VMRGLH,
	PPC_INS_VMRGLW,
	PPC_INS_VMSUMMBM,
	PPC_INS_VMSUMSHM,
	PPC_INS_VMSUMSHS,
	PPC_INS_VMSUMUBM,
	PPC_INS_VMSUMUHM,
	PPC_INS_VMSUMUHS,
	PPC_INS_VMULESB,
	PPC_INS_VMULESH,
	PPC_INS_VMULESW,
	PPC_INS_VMULEUB,
	PPC_INS_VMULEUH,
	PPC_INS_VMULEUW,
	PPC_INS_VMULOSB,
	PPC_INS_VMULOSH,
	PPC_INS_VMULOSW,
	PPC_INS_VMULOUB,
	PPC_INS_VMULOUH,
	PPC_INS_VMULOUW,
	PPC_INS_VMULUWM,
	PPC_INS_VNAND,
	PPC_INS_VNMSUBFP,
	PPC_INS_VNOR,
	PPC_INS_VOR,
	PPC_INS_VORC,
	PPC_INS_VPERM,
	PPC_INS_VPKPX,
	PPC_INS_VPKSHSS,
	PPC_INS_VPKSHUS,
	PPC_INS_VPKSWSS,
	PPC_INS_VPKSWUS,
	PPC_INS_VPKUHUM,
	PPC_INS_VPKUHUS,
	PPC_INS_VPKUWUM,
	PPC_INS_VPKUWUS,
	PPC_INS_VPOPCNTB,
	PPC_INS_VPOPCNTD,
	PPC_INS_VPOPCNTH,
	PPC_INS_VPOPCNTW,
	PPC_INS_VREFP,
	PPC_INS_VRFIM,
	PPC_INS_VRFIN,
	PPC_INS_VRFIP,
	PPC_INS_VRFIZ,
	PPC_INS_VRLB,
	PPC_INS_VRLD,
	PPC_INS_VRLH,
	PPC_INS_VRLW,
	PPC_INS_VRSQRTEFP,
	PPC_INS_VSEL,
	PPC_INS_VSL,
	PPC_INS_VSLB,
	PPC_INS_VSLD,
	PPC_INS_VSLDOI,
	PPC_INS_VSLH,
	PPC_INS_VSLO,
	PPC_INS_VSLW,
	PPC_INS_VSPLTB,
	PPC_INS_VSPLTH,
	PPC_INS_VSPLTISB,
	PPC_INS_VSPLTISH,
	PPC_INS_VSPLTISW,
	PPC_INS_VSPLTW,
	PPC_INS_VSR,
	PPC_INS_VSRAB,
	PPC_INS_VSRAD,
	PPC_INS_VSRAH,
	PPC_INS_VSRAW,
	PPC_INS_VSRB,
	PPC_INS_VSRD,
	PPC_INS_VSRH,
	PPC_INS_VSRO,
	PPC_INS_VSRW,
	PPC_INS_VSUBCUW,
	PPC_INS_VSUBFP,
	PPC_INS_VSUBSBS,
	PPC_INS_VSUBSHS,
	PPC_INS_VSUBSWS,
	PPC_INS_VSUBUBM,
	PPC_INS_VSUBUBS,
	PPC_INS_VSUBUDM,
	PPC_INS_VSUBUHM,
	PPC_INS_VSUBUHS,
	PPC_INS_VSUBUWM,
	PPC_INS_VSUBUWS,
	PPC_INS_VSUM2SWS,
	PPC_INS_VSUM4SBS,
	PPC_INS_VSUM4SHS,
	PPC_INS_VSUM4UBS,
	PPC_INS_VSUMSWS,
	PPC_INS_VUPKHPX,
	PPC_INS_VUPKHSB,
	PPC_INS_VUPKHSH,
	PPC_INS_VUPKLPX,
	PPC_INS_VUPKLSB,
	PPC_INS_VUPKLSH,
	PPC_INS_VXOR,
	PPC_INS_WAIT,
	PPC_INS_WRTEE,
	PPC_INS_WRTEEI,
	PPC_INS_XOR,
	PPC_INS_XORI,
	PPC_INS_XORIS,
	PPC_INS_XSABSDP,
	PPC_INS_XSADDDP,
	PPC_INS_XSCMPODP,
	PPC_INS_XSCMPUDP,
	PPC_INS_XSCPSGNDP,
	PPC_INS_XSCVDPSP,
	PPC_INS_XSCVDPSXDS,
	PPC_INS_XSCVDPSXWS,
	PPC_INS_XSCVDPUXDS,
	PPC_INS_XSCVDPUXWS,
	PPC_INS_XSCVSPDP,
	PPC_INS_XSCVSXDDP,
	PPC_INS_XSCVUXDDP,
	PPC_INS_XSDIVDP,
	PPC_INS_XSMADDADP,
	PPC_INS_XSMADDMDP,
	PPC_INS_XSMAXDP,
	PPC_INS_XSMINDP,
	PPC_INS_XSMSUBADP,
	PPC_INS_XSMSUBMDP,
	PPC_INS_XSMULDP,
	PPC_INS_XSNABSDP,
	PPC_INS_XSNEGDP,
	PPC_INS_XSNMADDADP,
	PPC_INS_XSNMADDMDP,
	PPC_INS_XSNMSUBADP,
	PPC_INS_XSNMSUBMDP,
	PPC_INS_XSRDPI,
	PPC_INS_XSRDPIC,
	PPC_INS_XSRDPIM,
	PPC_INS_XSRDPIP,
	PPC_INS_XSRDPIZ,
	PPC_INS_XSREDP,
	PPC_INS_XSRSQRTEDP,
	PPC_INS_XSSQRTDP,
	PPC_INS_XSSUBDP,
	PPC_INS_XSTDIVDP,
	PPC_INS_XSTSQRTDP,
	PPC_INS_XVABSDP,
	PPC_INS_XVABSSP,
	PPC_INS_XVADDDP,
	PPC_INS_XVADDSP,
	PPC_INS_XVCMPEQDP,
	PPC_INS_XVCMPEQSP,
	PPC_INS_XVCMPGEDP,
	PPC_INS_XVCMPGESP,
	PPC_INS_XVCMPGTDP,
	PPC_INS_XVCMPGTSP,
	PPC_INS_XVCPSGNDP,
	PPC_INS_XVCPSGNSP,
	PPC_INS_XVCVDPSP,
	PPC_INS_XVCVDPSXDS,
	PPC_INS_XVCVDPSXWS,
	PPC_INS_XVCVDPUXDS,
	PPC_INS_XVCVDPUXWS,
	PPC_INS_XVCVSPDP,
	PPC_INS_XVCVSPSXDS,
	PPC_INS_XVCVSPSXWS,
	PPC_INS_XVCVSPUXDS,
	PPC_INS_XVCVSPUXWS,
	PPC_INS_XVCVSXDDP,
	PPC_INS_XVCVSXDSP,
	PPC_INS_XVCVSXWDP,
	PPC_INS_XVCVSXWSP,
	PPC_INS_XVCVUXDDP,
	PPC_INS_XVCVUXDSP,
	PPC_INS_XVCVUXWDP,
	PPC_INS_XVCVUXWSP,
	PPC_INS_XVDIVDP,
	PPC_INS_XVDIVSP,
	PPC_INS_XVMADDADP,
	PPC_INS_XVMADDASP,
	PPC_INS_XVMADDMDP,
	PPC_INS_XVMADDMSP,
	PPC_INS_XVMAXDP,
	PPC_INS_XVMAXSP,
	PPC_INS_XVMINDP,
	PPC_INS_XVMINSP,
	PPC_INS_XVMSUBADP,
	PPC_INS_XVMSUBASP,
	PPC_INS_XVMSUBMDP,
	PPC_INS_XVMSUBMSP,
	PPC_INS_XVMULDP,
	PPC_INS_XVMULSP,
	PPC_INS_XVNABSDP,
	PPC_INS_XVNABSSP,
	PPC_INS_XVNEGDP,
	PPC_INS_XVNEGSP,
	PPC_INS_XVNMADDADP,
	PPC_INS_XVNMADDASP,
	PPC_INS_XVNMADDMDP,
	PPC_INS_XVNMADDMSP,
	PPC_INS_XVNMSUBADP,
	PPC_INS_XVNMSUBASP,
	PPC_INS_XVNMSUBMDP,
	PPC_INS_XVNMSUBMSP,
	PPC_INS_XVRDPI,
	PPC_INS_XVRDPIC,
	PPC_INS_XVRDPIM,
	PPC_INS_XVRDPIP,
	PPC_INS_XVRDPIZ,
	PPC_INS_XVREDP,
	PPC_INS_XVRESP,
	PPC_INS_XVRSPI,
	PPC_INS_XVRSPIC,
	PPC_INS_XVRSPIM,
	PPC_INS_XVRSPIP,
	PPC_INS_XVRSPIZ,
	PPC_INS_XVRSQRTEDP,
	PPC_INS_XVRSQRTESP,
	PPC_INS_XVSQRTDP,
	PPC_INS_XVSQRTSP,
	PPC_INS_XVSUBDP,
	PPC_INS_XVSUBSP,
	PPC_INS_XVTDIVDP,
	PPC_INS_XVTDIVSP,
	PPC_INS_XVTSQRTDP,
	PPC_INS_XVTSQRTSP,
	PPC_INS_XXLAND,
	PPC_INS_XXLANDC,
	PPC_INS_XXLEQV,
	PPC_INS_XXLNAND,
	PPC_INS_XXLNOR,
	PPC_INS_XXLOR,
	PPC_INS_XXLORC,
	PPC_INS_XXLXOR,
	PPC_INS_XXMRGHW,
	PPC_INS_XXMRGLW,
	PPC_INS_XXPERMDI,
	PPC_INS_XXSEL,
	PPC_INS_XXSLDWI,
	PPC_INS_XXSPLTW,
	PPC_INS_BCA,
	PPC_INS_BCLA,

	// extra & alias instructions
	PPC_INS_SLWI,
	PPC_INS_SRWI,
	PPC_INS_SLDI,

	PPC_INS_BTA,
	PPC_INS_CRSET,
	PPC_INS_CRNOT,
	PPC_INS_CRMOVE,
	PPC_INS_CRCLR,
	PPC_INS_MFBR0,
	PPC_INS_MFBR1,
	PPC_INS_MFBR2,
	PPC_INS_MFBR3,
	PPC_INS_MFBR4,
	PPC_INS_MFBR5,
	PPC_INS_MFBR6,
	PPC_INS_MFBR7,
	PPC_INS_MFXER,
	PPC_INS_MFRTCU,
	PPC_INS_MFRTCL,
	PPC_INS_MFDSCR,
	PPC_INS_MFDSISR,
	PPC_INS_MFDAR,
	PPC_INS_MFSRR2,
	PPC_INS_MFSRR3,
	PPC_INS_MFCFAR,
	PPC_INS_MFAMR,
	PPC_INS_MFPID,
	PPC_INS_MFTBLO,
	PPC_INS_MFTBHI,
	PPC_INS_MFDBATU,
	PPC_INS_MFDBATL,
	PPC_INS_MFIBATU,
	PPC_INS_MFIBATL,
	PPC_INS_MFDCCR,
	PPC_INS_MFICCR,
	PPC_INS_MFDEAR,
	PPC_INS_MFESR,
	PPC_INS_MFSPEFSCR,
	PPC_INS_MFTCR,
	PPC_INS_MFASR,
	PPC_INS_MFPVR,
	PPC_INS_MFTBU,
	PPC_INS_MTCR,
	PPC_INS_MTBR0,
	PPC_INS_MTBR1,
	PPC_INS_MTBR2,
	PPC_INS_MTBR3,
	PPC_INS_MTBR4,
	PPC_INS_MTBR5,
	PPC_INS_MTBR6,
	PPC_INS_MTBR7,
	PPC_INS_MTXER,
	PPC_INS_MTDSCR,
	PPC_INS_MTDSISR,
	PPC_INS_MTDAR,
	PPC_INS_MTSRR2,
	PPC_INS_MTSRR3,
	PPC_INS_MTCFAR,
	PPC_INS_MTAMR,
	PPC_INS_MTPID,
	PPC_INS_MTTBL,
	PPC_INS_MTTBU,
	PPC_INS_MTTBLO,
	PPC_INS_MTTBHI,
	PPC_INS_MTDBATU,
	PPC_INS_MTDBATL,
	PPC_INS_MTIBATU,
	PPC_INS_MTIBATL,
	PPC_INS_MTDCCR,
	PPC_INS_MTICCR,
	PPC_INS_MTDEAR,
	PPC_INS_MTESR,
	PPC_INS_MTSPEFSCR,
	PPC_INS_MTTCR,
	PPC_INS_NOT,
	PPC_INS_MR,
	PPC_INS_ROTLD,
	PPC_INS_ROTLDI,
	PPC_INS_CLRLDI,
	PPC_INS_ROTLWI,
	PPC_INS_CLRLWI,
	PPC_INS_ROTLW,
	PPC_INS_SUB,
	PPC_INS_SUBC,
	PPC_INS_LWSYNC,
	PPC_INS_PTESYNC,
	PPC_INS_TDLT,
	PPC_INS_TDEQ,
	PPC_INS_TDGT,
	PPC_INS_TDNE,
	PPC_INS_TDLLT,
	PPC_INS_TDLGT,
	PPC_INS_TDU,
	PPC_INS_TDLTI,
	PPC_INS_TDEQI,
	PPC_INS_TDGTI,
	PPC_INS_TDNEI,
	PPC_INS_TDLLTI,
	PPC_INS_TDLGTI,
	PPC_INS_TDUI,
	PPC_INS_TLBREHI,
	PPC_INS_TLBRELO,
	PPC_INS_TLBWEHI,
	PPC_INS_TLBWELO,
	PPC_INS_TWLT,
	PPC_INS_TWEQ,
	PPC_INS_TWGT,
	PPC_INS_TWNE,
	PPC_INS_TWLLT,
	PPC_INS_TWLGT,
	PPC_INS_TWU,
	PPC_INS_TWLTI,
	PPC_INS_TWEQI,
	PPC_INS_TWGTI,
	PPC_INS_TWNEI,
	PPC_INS_TWLLTI,
	PPC_INS_TWLGTI,
	PPC_INS_TWUI,
	PPC_INS_WAITRSV,
	PPC_INS_WAITIMPL,
	PPC_INS_XNOP,
	PPC_INS_XVMOVDP,
	PPC_INS_XVMOVSP,
	PPC_INS_XXSPLTD,
	PPC_INS_XXMRGHD,
	PPC_INS_XXMRGLD,
	PPC_INS_XXSWAPD,
	PPC_INS_BT,
	PPC_INS_BF,
	PPC_INS_BDNZT,
	PPC_INS_BDNZF,
	PPC_INS_BDZF,
	PPC_INS_BDZT,
	PPC_INS_BFA,
	PPC_INS_BDNZTA,
	PPC_INS_BDNZFA,
	PPC_INS_BDZTA,
	PPC_INS_BDZFA,
	PPC_INS_BTCTR,
	PPC_INS_BFCTR,
	PPC_INS_BTCTRL,
	PPC_INS_BFCTRL,
	PPC_INS_BTL,
	PPC_INS_BFL,
	PPC_INS_BDNZTL,
	PPC_INS_BDNZFL,
	PPC_INS_BDZTL,
	PPC_INS_BDZFL,
	PPC_INS_BTLA,
	PPC_INS_BFLA,
	PPC_INS_BDNZTLA,
	PPC_INS_BDNZFLA,
	PPC_INS_BDZTLA,
	PPC_INS_BDZFLA,
	PPC_INS_BTLR,
	PPC_INS_BFLR,
	PPC_INS_BDNZTLR,
	PPC_INS_BDZTLR,
	PPC_INS_BDZFLR,
	PPC_INS_BTLRL,
	PPC_INS_BFLRL,
	PPC_INS_BDNZTLRL,
	PPC_INS_BDNZFLRL,
	PPC_INS_BDZTLRL,
	PPC_INS_BDZFLRL,

	// QPX
	PPC_INS_QVFAND,
	PPC_INS_QVFCLR,
	PPC_INS_QVFANDC,
	PPC_INS_QVFCTFB,
	PPC_INS_QVFXOR,
	PPC_INS_QVFOR,
	PPC_INS_QVFNOR,
	PPC_INS_QVFEQU,
	PPC_INS_QVFNOT,
	PPC_INS_QVFORC,
	PPC_INS_QVFNAND,
	PPC_INS_QVFSET,

	PPC_INS_ENDING,   // <-- mark the end of the list of instructions
} ppc_insn;

/// Group of PPC instructions
typedef enum ppc_insn_group {
	PPC_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	PPC_GRP_JUMP,	///< = CS_GRP_JUMP

	// Architecture-specific groups
	PPC_GRP_ALTIVEC = 128,
	PPC_GRP_MODE32,
	PPC_GRP_MODE64,
	PPC_GRP_BOOKE,
	PPC_GRP_NOTBOOKE,
	PPC_GRP_SPE,
	PPC_GRP_VSX,
	PPC_GRP_E500,
	PPC_GRP_PPC4XX,
	PPC_GRP_PPC6XX,
	PPC_GRP_ICBT,
	PPC_GRP_P8ALTIVEC,
	PPC_GRP_P8VECTOR,
	PPC_GRP_QPX,

	PPC_GRP_ENDING,   // <-- mark the end of the list of groups
} ppc_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/sparc.h`:

```h
#ifndef CAPSTONE_SPARC_H
#define CAPSTONE_SPARC_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2014-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

// GCC SPARC toolchain has a default macro called "sparc" which breaks
// compilation
#undef sparc

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// Enums corresponding to Sparc condition codes, both icc's and fcc's.
typedef enum sparc_cc {
	SPARC_CC_INVALID = 0,	///< invalid CC (default)
	// Integer condition codes
	SPARC_CC_ICC_A   =  8+256,  ///< Always
	SPARC_CC_ICC_N   =  0+256,  ///< Never
	SPARC_CC_ICC_NE  =  9+256,  ///< Not Equal
	SPARC_CC_ICC_E   =  1+256,  ///< Equal
	SPARC_CC_ICC_G   = 10+256,  ///< Greater
	SPARC_CC_ICC_LE  =  2+256,  ///< Less or Equal
	SPARC_CC_ICC_GE  = 11+256,  ///< Greater or Equal
	SPARC_CC_ICC_L   =  3+256,  ///< Less
	SPARC_CC_ICC_GU  = 12+256,  ///< Greater Unsigned
	SPARC_CC_ICC_LEU =  4+256,  ///< Less or Equal Unsigned
	SPARC_CC_ICC_CC  = 13+256,  ///< Carry Clear/Great or Equal Unsigned
	SPARC_CC_ICC_CS  =  5+256,  ///< Carry Set/Less Unsigned
	SPARC_CC_ICC_POS = 14+256,  ///< Positive
	SPARC_CC_ICC_NEG =  6+256,  ///< Negative
	SPARC_CC_ICC_VC  = 15+256,  ///< Overflow Clear
	SPARC_CC_ICC_VS  =  7+256,  ///< Overflow Set

	// Floating condition codes
	SPARC_CC_FCC_A   =  8+16+256,  ///< Always
	SPARC_CC_FCC_N   =  0+16+256,  ///< Never
	SPARC_CC_FCC_U   =  7+16+256,  ///< Unordered
	SPARC_CC_FCC_G   =  6+16+256,  ///< Greater
	SPARC_CC_FCC_UG  =  5+16+256,  ///< Unordered or Greater
	SPARC_CC_FCC_L   =  4+16+256,  ///< Less
	SPARC_CC_FCC_UL  =  3+16+256,  ///< Unordered or Less
	SPARC_CC_FCC_LG  =  2+16+256,  ///< Less or Greater
	SPARC_CC_FCC_NE  =  1+16+256,  ///< Not Equal
	SPARC_CC_FCC_E   =  9+16+256,  ///< Equal
	SPARC_CC_FCC_UE  = 10+16+256,  ///< Unordered or Equal
	SPARC_CC_FCC_GE  = 11+16+256,  ///< Greater or Equal
	SPARC_CC_FCC_UGE = 12+16+256,  ///< Unordered or Greater or Equal
	SPARC_CC_FCC_LE  = 13+16+256,  ///< Less or Equal
	SPARC_CC_FCC_ULE = 14+16+256,  ///< Unordered or Less or Equal
	SPARC_CC_FCC_O   = 15+16+256,  ///< Ordered
} sparc_cc;

/// Branch hint
typedef enum sparc_hint {
	SPARC_HINT_INVALID = 0,	///< no hint
	SPARC_HINT_A	= 1 << 0,	///< annul delay slot instruction
	SPARC_HINT_PT	= 1 << 1,	///< branch taken
	SPARC_HINT_PN	= 1 << 2,	///< branch NOT taken
} sparc_hint;

/// Operand type for instruction's operands
typedef enum sparc_op_type {
	SPARC_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	SPARC_OP_REG, ///< = CS_OP_REG (Register operand).
	SPARC_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	SPARC_OP_MEM, ///< = CS_OP_MEM (Memory operand).
} sparc_op_type;

/// SPARC registers
typedef enum sparc_reg {
	SPARC_REG_INVALID = 0,

	SPARC_REG_F0,
	SPARC_REG_F1,
	SPARC_REG_F2,
	SPARC_REG_F3,
	SPARC_REG_F4,
	SPARC_REG_F5,
	SPARC_REG_F6,
	SPARC_REG_F7,
	SPARC_REG_F8,
	SPARC_REG_F9,
	SPARC_REG_F10,
	SPARC_REG_F11,
	SPARC_REG_F12,
	SPARC_REG_F13,
	SPARC_REG_F14,
	SPARC_REG_F15,
	SPARC_REG_F16,
	SPARC_REG_F17,
	SPARC_REG_F18,
	SPARC_REG_F19,
	SPARC_REG_F20,
	SPARC_REG_F21,
	SPARC_REG_F22,
	SPARC_REG_F23,
	SPARC_REG_F24,
	SPARC_REG_F25,
	SPARC_REG_F26,
	SPARC_REG_F27,
	SPARC_REG_F28,
	SPARC_REG_F29,
	SPARC_REG_F30,
	SPARC_REG_F31,
	SPARC_REG_F32,
	SPARC_REG_F34,
	SPARC_REG_F36,
	SPARC_REG_F38,
	SPARC_REG_F40,
	SPARC_REG_F42,
	SPARC_REG_F44,
	SPARC_REG_F46,
	SPARC_REG_F48,
	SPARC_REG_F50,
	SPARC_REG_F52,
	SPARC_REG_F54,
	SPARC_REG_F56,
	SPARC_REG_F58,
	SPARC_REG_F60,
	SPARC_REG_F62,
	SPARC_REG_FCC0,	// Floating condition codes
	SPARC_REG_FCC1,
	SPARC_REG_FCC2,
	SPARC_REG_FCC3,
	SPARC_REG_FP,
	SPARC_REG_G0,
	SPARC_REG_G1,
	SPARC_REG_G2,
	SPARC_REG_G3,
	SPARC_REG_G4,
	SPARC_REG_G5,
	SPARC_REG_G6,
	SPARC_REG_G7,
	SPARC_REG_I0,
	SPARC_REG_I1,
	SPARC_REG_I2,
	SPARC_REG_I3,
	SPARC_REG_I4,
	SPARC_REG_I5,
	SPARC_REG_I7,
	SPARC_REG_ICC,	// Integer condition codes
	SPARC_REG_L0,
	SPARC_REG_L1,
	SPARC_REG_L2,
	SPARC_REG_L3,
	SPARC_REG_L4,
	SPARC_REG_L5,
	SPARC_REG_L6,
	SPARC_REG_L7,
	SPARC_REG_O0,
	SPARC_REG_O1,
	SPARC_REG_O2,
	SPARC_REG_O3,
	SPARC_REG_O4,
	SPARC_REG_O5,
	SPARC_REG_O7,
	SPARC_REG_SP,
	SPARC_REG_Y,

	// special register
	SPARC_REG_XCC,

	SPARC_REG_ENDING,   // <-- mark the end of the list of registers

	// extras
	SPARC_REG_O6 = SPARC_REG_SP,
	SPARC_REG_I6 = SPARC_REG_FP,
} sparc_reg;

/// Instruction's operand referring to memory
/// This is associated with SPARC_OP_MEM operand type above
typedef struct sparc_op_mem {
	uint8_t base;		///< base register, can be safely interpreted as
				///< a value of type `sparc_reg`, but it is only
				///< one byte wide
	uint8_t index;		///< index register, same conditions apply here
	int32_t disp;		///< displacement/offset value
} sparc_op_mem;

/// Instruction operand
typedef struct cs_sparc_op {
	sparc_op_type type;	///< operand type
	union {
		sparc_reg reg;	///< register value for REG operand
		int64_t imm;		///< immediate value for IMM operand
		sparc_op_mem mem;		///< base/disp value for MEM operand
	};
} cs_sparc_op;

/// Instruction structure
typedef struct cs_sparc {
	sparc_cc cc;	///< code condition for this insn
	sparc_hint hint;	///< branch hint: encoding as bitwise OR of sparc_hint.
	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;
	cs_sparc_op operands[4]; ///< operands for this instruction.
} cs_sparc;

/// SPARC instruction
typedef enum sparc_insn {
	SPARC_INS_INVALID = 0,

	SPARC_INS_ADDCC,
	SPARC_INS_ADDX,
	SPARC_INS_ADDXCC,
	SPARC_INS_ADDXC,
	SPARC_INS_ADDXCCC,
	SPARC_INS_ADD,
	SPARC_INS_ALIGNADDR,
	SPARC_INS_ALIGNADDRL,
	SPARC_INS_ANDCC,
	SPARC_INS_ANDNCC,
	SPARC_INS_ANDN,
	SPARC_INS_AND,
	SPARC_INS_ARRAY16,
	SPARC_INS_ARRAY32,
	SPARC_INS_ARRAY8,
	SPARC_INS_B,
	SPARC_INS_JMP,
	SPARC_INS_BMASK,
	SPARC_INS_FB,
	SPARC_INS_BRGEZ,
	SPARC_INS_BRGZ,
	SPARC_INS_BRLEZ,
	SPARC_INS_BRLZ,
	SPARC_INS_BRNZ,
	SPARC_INS_BRZ,
	SPARC_INS_BSHUFFLE,
	SPARC_INS_CALL,
	SPARC_INS_CASX,
	SPARC_INS_CAS,
	SPARC_INS_CMASK16,
	SPARC_INS_CMASK32,
	SPARC_INS_CMASK8,
	SPARC_INS_CMP,
	SPARC_INS_EDGE16,
	SPARC_INS_EDGE16L,
	SPARC_INS_EDGE16LN,
	SPARC_INS_EDGE16N,
	SPARC_INS_EDGE32,
	SPARC_INS_EDGE32L,
	SPARC_INS_EDGE32LN,
	SPARC_INS_EDGE32N,
	SPARC_INS_EDGE8,
	SPARC_INS_EDGE8L,
	SPARC_INS_EDGE8LN,
	SPARC_INS_EDGE8N,
	SPARC_INS_FABSD,
	SPARC_INS_FABSQ,
	SPARC_INS_FABSS,
	SPARC_INS_FADDD,
	SPARC_INS_FADDQ,
	SPARC_INS_FADDS,
	SPARC_INS_FALIGNDATA,
	SPARC_INS_FAND,
	SPARC_INS_FANDNOT1,
	SPARC_INS_FANDNOT1S,
	SPARC_INS_FANDNOT2,
	SPARC_INS_FANDNOT2S,
	SPARC_INS_FANDS,
	SPARC_INS_FCHKSM16,
	SPARC_INS_FCMPD,
	SPARC_INS_FCMPEQ16,
	SPARC_INS_FCMPEQ32,
	SPARC_INS_FCMPGT16,
	SPARC_INS_FCMPGT32,
	SPARC_INS_FCMPLE16,
	SPARC_INS_FCMPLE32,
	SPARC_INS_FCMPNE16,
	SPARC_INS_FCMPNE32,
	SPARC_INS_FCMPQ,
	SPARC_INS_FCMPS,
	SPARC_INS_FDIVD,
	SPARC_INS_FDIVQ,
	SPARC_INS_FDIVS,
	SPARC_INS_FDMULQ,
	SPARC_INS_FDTOI,
	SPARC_INS_FDTOQ,
	SPARC_INS_FDTOS,
	SPARC_INS_FDTOX,
	SPARC_INS_FEXPAND,
	SPARC_INS_FHADDD,
	SPARC_INS_FHADDS,
	SPARC_INS_FHSUBD,
	SPARC_INS_FHSUBS,
	SPARC_INS_FITOD,
	SPARC_INS_FITOQ,
	SPARC_INS_FITOS,
	SPARC_INS_FLCMPD,
	SPARC_INS_FLCMPS,
	SPARC_INS_FLUSHW,
	SPARC_INS_FMEAN16,
	SPARC_INS_FMOVD,
	SPARC_INS_FMOVQ,
	SPARC_INS_FMOVRDGEZ,
	SPARC_INS_FMOVRQGEZ,
	SPARC_INS_FMOVRSGEZ,
	SPARC_INS_FMOVRDGZ,
	SPARC_INS_FMOVRQGZ,
	SPARC_INS_FMOVRSGZ,
	SPARC_INS_FMOVRDLEZ,
	SPARC_INS_FMOVRQLEZ,
	SPARC_INS_FMOVRSLEZ,
	SPARC_INS_FMOVRDLZ,
	SPARC_INS_FMOVRQLZ,
	SPARC_INS_FMOVRSLZ,
	SPARC_INS_FMOVRDNZ,
	SPARC_INS_FMOVRQNZ,
	SPARC_INS_FMOVRSNZ,
	SPARC_INS_FMOVRDZ,
	SPARC_INS_FMOVRQZ,
	SPARC_INS_FMOVRSZ,
	SPARC_INS_FMOVS,
	SPARC_INS_FMUL8SUX16,
	SPARC_INS_FMUL8ULX16,
	SPARC_INS_FMUL8X16,
	SPARC_INS_FMUL8X16AL,
	SPARC_INS_FMUL8X16AU,
	SPARC_INS_FMULD,
	SPARC_INS_FMULD8SUX16,
	SPARC_INS_FMULD8ULX16,
	SPARC_INS_FMULQ,
	SPARC_INS_FMULS,
	SPARC_INS_FNADDD,
	SPARC_INS_FNADDS,
	SPARC_INS_FNAND,
	SPARC_INS_FNANDS,
	SPARC_INS_FNEGD,
	SPARC_INS_FNEGQ,
	SPARC_INS_FNEGS,
	SPARC_INS_FNHADDD,
	SPARC_INS_FNHADDS,
	SPARC_INS_FNOR,
	SPARC_INS_FNORS,
	SPARC_INS_FNOT1,
	SPARC_INS_FNOT1S,
	SPARC_INS_FNOT2,
	SPARC_INS_FNOT2S,
	SPARC_INS_FONE,
	SPARC_INS_FONES,
	SPARC_INS_FOR,
	SPARC_INS_FORNOT1,
	SPARC_INS_FORNOT1S,
	SPARC_INS_FORNOT2,
	SPARC_INS_FORNOT2S,
	SPARC_INS_FORS,
	SPARC_INS_FPACK16,
	SPARC_INS_FPACK32,
	SPARC_INS_FPACKFIX,
	SPARC_INS_FPADD16,
	SPARC_INS_FPADD16S,
	SPARC_INS_FPADD32,
	SPARC_INS_FPADD32S,
	SPARC_INS_FPADD64,
	SPARC_INS_FPMERGE,
	SPARC_INS_FPSUB16,
	SPARC_INS_FPSUB16S,
	SPARC_INS_FPSUB32,
	SPARC_INS_FPSUB32S,
	SPARC_INS_FQTOD,
	SPARC_INS_FQTOI,
	SPARC_INS_FQTOS,
	SPARC_INS_FQTOX,
	SPARC_INS_FSLAS16,
	SPARC_INS_FSLAS32,
	SPARC_INS_FSLL16,
	SPARC_INS_FSLL32,
	SPARC_INS_FSMULD,
	SPARC_INS_FSQRTD,
	SPARC_INS_FSQRTQ,
	SPARC_INS_FSQRTS,
	SPARC_INS_FSRA16,
	SPARC_INS_FSRA32,
	SPARC_INS_FSRC1,
	SPARC_INS_FSRC1S,
	SPARC_INS_FSRC2,
	SPARC_INS_FSRC2S,
	SPARC_INS_FSRL16,
	SPARC_INS_FSRL32,
	SPARC_INS_FSTOD,
	SPARC_INS_FSTOI,
	SPARC_INS_FSTOQ,
	SPARC_INS_FSTOX,
	SPARC_INS_FSUBD,
	SPARC_INS_FSUBQ,
	SPARC_INS_FSUBS,
	SPARC_INS_FXNOR,
	SPARC_INS_FXNORS,
	SPARC_INS_FXOR,
	SPARC_INS_FXORS,
	SPARC_INS_FXTOD,
	SPARC_INS_FXTOQ,
	SPARC_INS_FXTOS,
	SPARC_INS_FZERO,
	SPARC_INS_FZEROS,
	SPARC_INS_JMPL,
	SPARC_INS_LDD,
	SPARC_INS_LD,
	SPARC_INS_LDQ,
	SPARC_INS_LDSB,
	SPARC_INS_LDSH,
	SPARC_INS_LDSW,
	SPARC_INS_LDUB,
	SPARC_INS_LDUH,
	SPARC_INS_LDX,
	SPARC_INS_LZCNT,
	SPARC_INS_MEMBAR,
	SPARC_INS_MOVDTOX,
	SPARC_INS_MOV,
	SPARC_INS_MOVRGEZ,
	SPARC_INS_MOVRGZ,
	SPARC_INS_MOVRLEZ,
	SPARC_INS_MOVRLZ,
	SPARC_INS_MOVRNZ,
	SPARC_INS_MOVRZ,
	SPARC_INS_MOVSTOSW,
	SPARC_INS_MOVSTOUW,
	SPARC_INS_MULX,
	SPARC_INS_NOP,
	SPARC_INS_ORCC,
	SPARC_INS_ORNCC,
	SPARC_INS_ORN,
	SPARC_INS_OR,
	SPARC_INS_PDIST,
	SPARC_INS_PDISTN,
	SPARC_INS_POPC,
	SPARC_INS_RD,
	SPARC_INS_RESTORE,
	SPARC_INS_RETT,
	SPARC_INS_SAVE,
	SPARC_INS_SDIVCC,
	SPARC_INS_SDIVX,
	SPARC_INS_SDIV,
	SPARC_INS_SETHI,
	SPARC_INS_SHUTDOWN,
	SPARC_INS_SIAM,
	SPARC_INS_SLLX,
	SPARC_INS_SLL,
	SPARC_INS_SMULCC,
	SPARC_INS_SMUL,
	SPARC_INS_SRAX,
	SPARC_INS_SRA,
	SPARC_INS_SRLX,
	SPARC_INS_SRL,
	SPARC_INS_STBAR,
	SPARC_INS_STB,
	SPARC_INS_STD,
	SPARC_INS_ST,
	SPARC_INS_STH,
	SPARC_INS_STQ,
	SPARC_INS_STX,
	SPARC_INS_SUBCC,
	SPARC_INS_SUBX,
	SPARC_INS_SUBXCC,
	SPARC_INS_SUB,
	SPARC_INS_SWAP,
	SPARC_INS_TADDCCTV,
	SPARC_INS_TADDCC,
	SPARC_INS_T,
	SPARC_INS_TSUBCCTV,
	SPARC_INS_TSUBCC,
	SPARC_INS_UDIVCC,
	SPARC_INS_UDIVX,
	SPARC_INS_UDIV,
	SPARC_INS_UMULCC,
	SPARC_INS_UMULXHI,
	SPARC_INS_UMUL,
	SPARC_INS_UNIMP,
	SPARC_INS_FCMPED,
	SPARC_INS_FCMPEQ,
	SPARC_INS_FCMPES,
	SPARC_INS_WR,
	SPARC_INS_XMULX,
	SPARC_INS_XMULXHI,
	SPARC_INS_XNORCC,
	SPARC_INS_XNOR,
	SPARC_INS_XORCC,
	SPARC_INS_XOR,

	// alias instructions
	SPARC_INS_RET,
	SPARC_INS_RETL,

	SPARC_INS_ENDING,   // <-- mark the end of the list of instructions
} sparc_insn;

/// Group of SPARC instructions
typedef enum sparc_insn_group {
	SPARC_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	SPARC_GRP_JUMP,	///< = CS_GRP_JUMP

	// Architecture-specific groups
	SPARC_GRP_HARDQUAD = 128,
	SPARC_GRP_V9,
	SPARC_GRP_VIS,
	SPARC_GRP_VIS2,
	SPARC_GRP_VIS3, 
	SPARC_GRP_32BIT,
	SPARC_GRP_64BIT,

	SPARC_GRP_ENDING,   // <-- mark the end of the list of groups
} sparc_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/systemz.h`:

```h
#ifndef CAPSTONE_SYSTEMZ_H
#define CAPSTONE_SYSTEMZ_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2014-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// Enums corresponding to SystemZ condition codes
typedef enum sysz_cc {
	SYSZ_CC_INVALID = 0,	///< invalid CC (default)

	SYSZ_CC_O,
	SYSZ_CC_H,
	SYSZ_CC_NLE,
	SYSZ_CC_L,
	SYSZ_CC_NHE,
	SYSZ_CC_LH,
	SYSZ_CC_NE,
	SYSZ_CC_E,
	SYSZ_CC_NLH,
	SYSZ_CC_HE,
	SYSZ_CC_NL,
	SYSZ_CC_LE,
	SYSZ_CC_NH,
	SYSZ_CC_NO,
} sysz_cc;

/// Operand type for instruction's operands
typedef enum sysz_op_type {
	SYSZ_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	SYSZ_OP_REG, ///< = CS_OP_REG (Register operand).
	SYSZ_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	SYSZ_OP_MEM, ///< = CS_OP_MEM (Memory operand).
	SYSZ_OP_ACREG = 64,	///< Access register operand.
} sysz_op_type;

/// SystemZ registers
typedef enum sysz_reg {
	SYSZ_REG_INVALID = 0,

	SYSZ_REG_0, 
	SYSZ_REG_1, 
	SYSZ_REG_2, 
	SYSZ_REG_3, 
	SYSZ_REG_4, 
	SYSZ_REG_5, 
	SYSZ_REG_6, 
	SYSZ_REG_7, 
	SYSZ_REG_8, 
	SYSZ_REG_9, 
	SYSZ_REG_10,
	SYSZ_REG_11,
	SYSZ_REG_12,
	SYSZ_REG_13,
	SYSZ_REG_14,
	SYSZ_REG_15,
	SYSZ_REG_CC,
	SYSZ_REG_F0,
	SYSZ_REG_F1,
	SYSZ_REG_F2,
	SYSZ_REG_F3,
	SYSZ_REG_F4,
	SYSZ_REG_F5,
	SYSZ_REG_F6,
	SYSZ_REG_F7,
	SYSZ_REG_F8,
	SYSZ_REG_F9,
	SYSZ_REG_F10,
	SYSZ_REG_F11,
	SYSZ_REG_F12,
	SYSZ_REG_F13,
	SYSZ_REG_F14,
	SYSZ_REG_F15,

	SYSZ_REG_R0L,

	SYSZ_REG_ENDING,
} sysz_reg;

/// Instruction's operand referring to memory
/// This is associated with SYSZ_OP_MEM operand type above
typedef struct sysz_op_mem {
	uint8_t base;		///< base register, can be safely interpreted as
				///< a value of type `sysz_reg`, but it is only
				///< one byte wide
	uint8_t index;		///< index register, same conditions apply here
	uint64_t length;	///< BDLAddr operand
	int64_t disp;	///< displacement/offset value
} sysz_op_mem;

/// Instruction operand
typedef struct cs_sysz_op {
	sysz_op_type type;	///< operand type
	union {
		sysz_reg reg;		///< register value for REG operand
		int64_t imm;		///< immediate value for IMM operand
		sysz_op_mem mem;	///< base/disp value for MEM operand
	};
} cs_sysz_op;

// Instruction structure
typedef struct cs_sysz {
	sysz_cc cc;		///< Code condition
	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;
	cs_sysz_op operands[6]; ///< operands for this instruction.
} cs_sysz;

/// SystemZ instruction
typedef enum sysz_insn {
	SYSZ_INS_INVALID = 0,

	SYSZ_INS_A,
	SYSZ_INS_ADB,
	SYSZ_INS_ADBR,
	SYSZ_INS_AEB,
	SYSZ_INS_AEBR,
	SYSZ_INS_AFI,
	SYSZ_INS_AG,
	SYSZ_INS_AGF,
	SYSZ_INS_AGFI,
	SYSZ_INS_AGFR,
	SYSZ_INS_AGHI,
	SYSZ_INS_AGHIK,
	SYSZ_INS_AGR,
	SYSZ_INS_AGRK,
	SYSZ_INS_AGSI,
	SYSZ_INS_AH,
	SYSZ_INS_AHI,
	SYSZ_INS_AHIK,
	SYSZ_INS_AHY,
	SYSZ_INS_AIH,
	SYSZ_INS_AL,
	SYSZ_INS_ALC,
	SYSZ_INS_ALCG,
	SYSZ_INS_ALCGR,
	SYSZ_INS_ALCR,
	SYSZ_INS_ALFI,
	SYSZ_INS_ALG,
	SYSZ_INS_ALGF,
	SYSZ_INS_ALGFI,
	SYSZ_INS_ALGFR,
	SYSZ_INS_ALGHSIK,
	SYSZ_INS_ALGR,
	SYSZ_INS_ALGRK,
	SYSZ_INS_ALHSIK,
	SYSZ_INS_ALR,
	SYSZ_INS_ALRK,
	SYSZ_INS_ALY,
	SYSZ_INS_AR,
	SYSZ_INS_ARK,
	SYSZ_INS_ASI,
	SYSZ_INS_AXBR,
	SYSZ_INS_AY,
	SYSZ_INS_BCR,
	SYSZ_INS_BRC,
	SYSZ_INS_BRCL,
	SYSZ_INS_CGIJ,
	SYSZ_INS_CGRJ,
	SYSZ_INS_CIJ,
	SYSZ_INS_CLGIJ,
	SYSZ_INS_CLGRJ,
	SYSZ_INS_CLIJ,
	SYSZ_INS_CLRJ,
	SYSZ_INS_CRJ,
	SYSZ_INS_BER,
	SYSZ_INS_JE,
	SYSZ_INS_JGE,
	SYSZ_INS_LOCE,
	SYSZ_INS_LOCGE,
	SYSZ_INS_LOCGRE,
	SYSZ_INS_LOCRE,
	SYSZ_INS_STOCE,
	SYSZ_INS_STOCGE,
	SYSZ_INS_BHR,
	SYSZ_INS_BHER,
	SYSZ_INS_JHE,
	SYSZ_INS_JGHE,
	SYSZ_INS_LOCHE,
	SYSZ_INS_LOCGHE,
	SYSZ_INS_LOCGRHE,
	SYSZ_INS_LOCRHE,
	SYSZ_INS_STOCHE,
	SYSZ_INS_STOCGHE,
	SYSZ_INS_JH,
	SYSZ_INS_JGH,
	SYSZ_INS_LOCH,
	SYSZ_INS_LOCGH,
	SYSZ_INS_LOCGRH,
	SYSZ_INS_LOCRH,
	SYSZ_INS_STOCH,
	SYSZ_INS_STOCGH,
	SYSZ_INS_CGIJNLH,
	SYSZ_INS_CGRJNLH,
	SYSZ_INS_CIJNLH,
	SYSZ_INS_CLGIJNLH,
	SYSZ_INS_CLGRJNLH,
	SYSZ_INS_CLIJNLH,
	SYSZ_INS_CLRJNLH,
	SYSZ_INS_CRJNLH,
	SYSZ_INS_CGIJE,
	SYSZ_INS_CGRJE,
	SYSZ_INS_CIJE,
	SYSZ_INS_CLGIJE,
	SYSZ_INS_CLGRJE,
	SYSZ_INS_CLIJE,
	SYSZ_INS_CLRJE,
	SYSZ_INS_CRJE,
	SYSZ_INS_CGIJNLE,
	SYSZ_INS_CGRJNLE,
	SYSZ_INS_CIJNLE,
	SYSZ_INS_CLGIJNLE,
	SYSZ_INS_CLGRJNLE,
	SYSZ_INS_CLIJNLE,
	SYSZ_INS_CLRJNLE,
	SYSZ_INS_CRJNLE,
	SYSZ_INS_CGIJH,
	SYSZ_INS_CGRJH,
	SYSZ_INS_CIJH,
	SYSZ_INS_CLGIJH,
	SYSZ_INS_CLGRJH,
	SYSZ_INS_CLIJH,
	SYSZ_INS_CLRJH,
	SYSZ_INS_CRJH,
	SYSZ_INS_CGIJNL,
	SYSZ_INS_CGRJNL,
	SYSZ_INS_CIJNL,
	SYSZ_INS_CLGIJNL,
	SYSZ_INS_CLGRJNL,
	SYSZ_INS_CLIJNL,
	SYSZ_INS_CLRJNL,
	SYSZ_INS_CRJNL,
	SYSZ_INS_CGIJHE,
	SYSZ_INS_CGRJHE,
	SYSZ_INS_CIJHE,
	SYSZ_INS_CLGIJHE,
	SYSZ_INS_CLGRJHE,
	SYSZ_INS_CLIJHE,
	SYSZ_INS_CLRJHE,
	SYSZ_INS_CRJHE,
	SYSZ_INS_CGIJNHE,
	SYSZ_INS_CGRJNHE,
	SYSZ_INS_CIJNHE,
	SYSZ_INS_CLGIJNHE,
	SYSZ_INS_CLGRJNHE,
	SYSZ_INS_CLIJNHE,
	SYSZ_INS_CLRJNHE,
	SYSZ_INS_CRJNHE,
	SYSZ_INS_CGIJL,
	SYSZ_INS_CGRJL,
	SYSZ_INS_CIJL,
	SYSZ_INS_CLGIJL,
	SYSZ_INS_CLGRJL,
	SYSZ_INS_CLIJL,
	SYSZ_INS_CLRJL,
	SYSZ_INS_CRJL,
	SYSZ_INS_CGIJNH,
	SYSZ_INS_CGRJNH,
	SYSZ_INS_CIJNH,
	SYSZ_INS_CLGIJNH,
	SYSZ_INS_CLGRJNH,
	SYSZ_INS_CLIJNH,
	SYSZ_INS_CLRJNH,
	SYSZ_INS_CRJNH,
	SYSZ_INS_CGIJLE,
	SYSZ_INS_CGRJLE,
	SYSZ_INS_CIJLE,
	SYSZ_INS_CLGIJLE,
	SYSZ_INS_CLGRJLE,
	SYSZ_INS_CLIJLE,
	SYSZ_INS_CLRJLE,
	SYSZ_INS_CRJLE,
	SYSZ_INS_CGIJNE,
	SYSZ_INS_CGRJNE,
	SYSZ_INS_CIJNE,
	SYSZ_INS_CLGIJNE,
	SYSZ_INS_CLGRJNE,
	SYSZ_INS_CLIJNE,
	SYSZ_INS_CLRJNE,
	SYSZ_INS_CRJNE,
	SYSZ_INS_CGIJLH,
	SYSZ_INS_CGRJLH,
	SYSZ_INS_CIJLH,
	SYSZ_INS_CLGIJLH,
	SYSZ_INS_CLGRJLH,
	SYSZ_INS_CLIJLH,
	SYSZ_INS_CLRJLH,
	SYSZ_INS_CRJLH,
	SYSZ_INS_BLR,
	SYSZ_INS_BLER,
	SYSZ_INS_JLE,
	SYSZ_INS_JGLE,
	SYSZ_INS_LOCLE,
	SYSZ_INS_LOCGLE,
	SYSZ_INS_LOCGRLE,
	SYSZ_INS_LOCRLE,
	SYSZ_INS_STOCLE,
	SYSZ_INS_STOCGLE,
	SYSZ_INS_BLHR,
	SYSZ_INS_JLH,
	SYSZ_INS_JGLH,
	SYSZ_INS_LOCLH,
	SYSZ_INS_LOCGLH,
	SYSZ_INS_LOCGRLH,
	SYSZ_INS_LOCRLH,
	SYSZ_INS_STOCLH,
	SYSZ_INS_STOCGLH,
	SYSZ_INS_JL,
	SYSZ_INS_JGL,
	SYSZ_INS_LOCL,
	SYSZ_INS_LOCGL,
	SYSZ_INS_LOCGRL,
	SYSZ_INS_LOCRL,
	SYSZ_INS_LOC,
	SYSZ_INS_LOCG,
	SYSZ_INS_LOCGR,
	SYSZ_INS_LOCR,
	SYSZ_INS_STOCL,
	SYSZ_INS_STOCGL,
	SYSZ_INS_BNER,
	SYSZ_INS_JNE,
	SYSZ_INS_JGNE,
	SYSZ_INS_LOCNE,
	SYSZ_INS_LOCGNE,
	SYSZ_INS_LOCGRNE,
	SYSZ_INS_LOCRNE,
	SYSZ_INS_STOCNE,
	SYSZ_INS_STOCGNE,
	SYSZ_INS_BNHR,
	SYSZ_INS_BNHER,
	SYSZ_INS_JNHE,
	SYSZ_INS_JGNHE,
	SYSZ_INS_LOCNHE,
	SYSZ_INS_LOCGNHE,
	SYSZ_INS_LOCGRNHE,
	SYSZ_INS_LOCRNHE,
	SYSZ_INS_STOCNHE,
	SYSZ_INS_STOCGNHE,
	SYSZ_INS_JNH,
	SYSZ_INS_JGNH,
	SYSZ_INS_LOCNH,
	SYSZ_INS_LOCGNH,
	SYSZ_INS_LOCGRNH,
	SYSZ_INS_LOCRNH,
	SYSZ_INS_STOCNH,
	SYSZ_INS_STOCGNH,
	SYSZ_INS_BNLR,
	SYSZ_INS_BNLER,
	SYSZ_INS_JNLE,
	SYSZ_INS_JGNLE,
	SYSZ_INS_LOCNLE,
	SYSZ_INS_LOCGNLE,
	SYSZ_INS_LOCGRNLE,
	SYSZ_INS_LOCRNLE,
	SYSZ_INS_STOCNLE,
	SYSZ_INS_STOCGNLE,
	SYSZ_INS_BNLHR,
	SYSZ_INS_JNLH,
	SYSZ_INS_JGNLH,
	SYSZ_INS_LOCNLH,
	SYSZ_INS_LOCGNLH,
	SYSZ_INS_LOCGRNLH,
	SYSZ_INS_LOCRNLH,
	SYSZ_INS_STOCNLH,
	SYSZ_INS_STOCGNLH,
	SYSZ_INS_JNL,
	SYSZ_INS_JGNL,
	SYSZ_INS_LOCNL,
	SYSZ_INS_LOCGNL,
	SYSZ_INS_LOCGRNL,
	SYSZ_INS_LOCRNL,
	SYSZ_INS_STOCNL,
	SYSZ_INS_STOCGNL,
	SYSZ_INS_BNOR,
	SYSZ_INS_JNO,
	SYSZ_INS_JGNO,
	SYSZ_INS_LOCNO,
	SYSZ_INS_LOCGNO,
	SYSZ_INS_LOCGRNO,
	SYSZ_INS_LOCRNO,
	SYSZ_INS_STOCNO,
	SYSZ_INS_STOCGNO,
	SYSZ_INS_BOR,
	SYSZ_INS_JO,
	SYSZ_INS_JGO,
	SYSZ_INS_LOCO,
	SYSZ_INS_LOCGO,
	SYSZ_INS_LOCGRO,
	SYSZ_INS_LOCRO,
	SYSZ_INS_STOCO,
	SYSZ_INS_STOCGO,
	SYSZ_INS_STOC,
	SYSZ_INS_STOCG,
	SYSZ_INS_BASR,
	SYSZ_INS_BR,
	SYSZ_INS_BRAS,
	SYSZ_INS_BRASL,
	SYSZ_INS_J,
	SYSZ_INS_JG,
	SYSZ_INS_BRCT,
	SYSZ_INS_BRCTG,
	SYSZ_INS_C,
	SYSZ_INS_CDB,
	SYSZ_INS_CDBR,
	SYSZ_INS_CDFBR,
	SYSZ_INS_CDGBR,
	SYSZ_INS_CDLFBR,
	SYSZ_INS_CDLGBR,
	SYSZ_INS_CEB,
	SYSZ_INS_CEBR,
	SYSZ_INS_CEFBR,
	SYSZ_INS_CEGBR,
	SYSZ_INS_CELFBR,
	SYSZ_INS_CELGBR,
	SYSZ_INS_CFDBR,
	SYSZ_INS_CFEBR,
	SYSZ_INS_CFI,
	SYSZ_INS_CFXBR,
	SYSZ_INS_CG,
	SYSZ_INS_CGDBR,
	SYSZ_INS_CGEBR,
	SYSZ_INS_CGF,
	SYSZ_INS_CGFI,
	SYSZ_INS_CGFR,
	SYSZ_INS_CGFRL,
	SYSZ_INS_CGH,
	SYSZ_INS_CGHI,
	SYSZ_INS_CGHRL,
	SYSZ_INS_CGHSI,
	SYSZ_INS_CGR,
	SYSZ_INS_CGRL,
	SYSZ_INS_CGXBR,
	SYSZ_INS_CH,
	SYSZ_INS_CHF,
	SYSZ_INS_CHHSI,
	SYSZ_INS_CHI,
	SYSZ_INS_CHRL,
	SYSZ_INS_CHSI,
	SYSZ_INS_CHY,
	SYSZ_INS_CIH,
	SYSZ_INS_CL,
	SYSZ_INS_CLC,
	SYSZ_INS_CLFDBR,
	SYSZ_INS_CLFEBR,
	SYSZ_INS_CLFHSI,
	SYSZ_INS_CLFI,
	SYSZ_INS_CLFXBR,
	SYSZ_INS_CLG,
	SYSZ_INS_CLGDBR,
	SYSZ_INS_CLGEBR,
	SYSZ_INS_CLGF,
	SYSZ_INS_CLGFI,
	SYSZ_INS_CLGFR,
	SYSZ_INS_CLGFRL,
	SYSZ_INS_CLGHRL,
	SYSZ_INS_CLGHSI,
	SYSZ_INS_CLGR,
	SYSZ_INS_CLGRL,
	SYSZ_INS_CLGXBR,
	SYSZ_INS_CLHF,
	SYSZ_INS_CLHHSI,
	SYSZ_INS_CLHRL,
	SYSZ_INS_CLI,
	SYSZ_INS_CLIH,
	SYSZ_INS_CLIY,
	SYSZ_INS_CLR,
	SYSZ_INS_CLRL,
	SYSZ_INS_CLST,
	SYSZ_INS_CLY,
	SYSZ_INS_CPSDR,
	SYSZ_INS_CR,
	SYSZ_INS_CRL,
	SYSZ_INS_CS,
	SYSZ_INS_CSG,
	SYSZ_INS_CSY,
	SYSZ_INS_CXBR,
	SYSZ_INS_CXFBR,
	SYSZ_INS_CXGBR,
	SYSZ_INS_CXLFBR,
	SYSZ_INS_CXLGBR,
	SYSZ_INS_CY,
	SYSZ_INS_DDB,
	SYSZ_INS_DDBR,
	SYSZ_INS_DEB,
	SYSZ_INS_DEBR,
	SYSZ_INS_DL,
	SYSZ_INS_DLG,
	SYSZ_INS_DLGR,
	SYSZ_INS_DLR,
	SYSZ_INS_DSG,
	SYSZ_INS_DSGF,
	SYSZ_INS_DSGFR,
	SYSZ_INS_DSGR,
	SYSZ_INS_DXBR,
	SYSZ_INS_EAR,
	SYSZ_INS_FIDBR,
	SYSZ_INS_FIDBRA,
	SYSZ_INS_FIEBR,
	SYSZ_INS_FIEBRA,
	SYSZ_INS_FIXBR,
	SYSZ_INS_FIXBRA,
	SYSZ_INS_FLOGR,
	SYSZ_INS_IC,
	SYSZ_INS_ICY,
	SYSZ_INS_IIHF,
	SYSZ_INS_IIHH,
	SYSZ_INS_IIHL,
	SYSZ_INS_IILF,
	SYSZ_INS_IILH,
	SYSZ_INS_IILL,
	SYSZ_INS_IPM,
	SYSZ_INS_L,
	SYSZ_INS_LA,
	SYSZ_INS_LAA,
	SYSZ_INS_LAAG,
	SYSZ_INS_LAAL,
	SYSZ_INS_LAALG,
	SYSZ_INS_LAN,
	SYSZ_INS_LANG,
	SYSZ_INS_LAO,
	SYSZ_INS_LAOG,
	SYSZ_INS_LARL,
	SYSZ_INS_LAX,
	SYSZ_INS_LAXG,
	SYSZ_INS_LAY,
	SYSZ_INS_LB,
	SYSZ_INS_LBH,
	SYSZ_INS_LBR,
	SYSZ_INS_LCDBR,
	SYSZ_INS_LCEBR,
	SYSZ_INS_LCGFR,
	SYSZ_INS_LCGR,
	SYSZ_INS_LCR,
	SYSZ_INS_LCXBR,
	SYSZ_INS_LD,
	SYSZ_INS_LDEB,
	SYSZ_INS_LDEBR,
	SYSZ_INS_LDGR,
	SYSZ_INS_LDR,
	SYSZ_INS_LDXBR,
	SYSZ_INS_LDXBRA,
	SYSZ_INS_LDY,
	SYSZ_INS_LE,
	SYSZ_INS_LEDBR,
	SYSZ_INS_LEDBRA,
	SYSZ_INS_LER,
	SYSZ_INS_LEXBR,
	SYSZ_INS_LEXBRA,
	SYSZ_INS_LEY,
	SYSZ_INS_LFH,
	SYSZ_INS_LG,
	SYSZ_INS_LGB,
	SYSZ_INS_LGBR,
	SYSZ_INS_LGDR,
	SYSZ_INS_LGF,
	SYSZ_INS_LGFI,
	SYSZ_INS_LGFR,
	SYSZ_INS_LGFRL,
	SYSZ_INS_LGH,
	SYSZ_INS_LGHI,
	SYSZ_INS_LGHR,
	SYSZ_INS_LGHRL,
	SYSZ_INS_LGR,
	SYSZ_INS_LGRL,
	SYSZ_INS_LH,
	SYSZ_INS_LHH,
	SYSZ_INS_LHI,
	SYSZ_INS_LHR,
	SYSZ_INS_LHRL,
	SYSZ_INS_LHY,
	SYSZ_INS_LLC,
	SYSZ_INS_LLCH,
	SYSZ_INS_LLCR,
	SYSZ_INS_LLGC,
	SYSZ_INS_LLGCR,
	SYSZ_INS_LLGF,
	SYSZ_INS_LLGFR,
	SYSZ_INS_LLGFRL,
	SYSZ_INS_LLGH,
	SYSZ_INS_LLGHR,
	SYSZ_INS_LLGHRL,
	SYSZ_INS_LLH,
	SYSZ_INS_LLHH,
	SYSZ_INS_LLHR,
	SYSZ_INS_LLHRL,
	SYSZ_INS_LLIHF,
	SYSZ_INS_LLIHH,
	SYSZ_INS_LLIHL,
	SYSZ_INS_LLILF,
	SYSZ_INS_LLILH,
	SYSZ_INS_LLILL,
	SYSZ_INS_LMG,
	SYSZ_INS_LNDBR,
	SYSZ_INS_LNEBR,
	SYSZ_INS_LNGFR,
	SYSZ_INS_LNGR,
	SYSZ_INS_LNR,
	SYSZ_INS_LNXBR,
	SYSZ_INS_LPDBR,
	SYSZ_INS_LPEBR,
	SYSZ_INS_LPGFR,
	SYSZ_INS_LPGR,
	SYSZ_INS_LPR,
	SYSZ_INS_LPXBR,
	SYSZ_INS_LR,
	SYSZ_INS_LRL,
	SYSZ_INS_LRV,
	SYSZ_INS_LRVG,
	SYSZ_INS_LRVGR,
	SYSZ_INS_LRVR,
	SYSZ_INS_LT,
	SYSZ_INS_LTDBR,
	SYSZ_INS_LTEBR,
	SYSZ_INS_LTG,
	SYSZ_INS_LTGF,
	SYSZ_INS_LTGFR,
	SYSZ_INS_LTGR,
	SYSZ_INS_LTR,
	SYSZ_INS_LTXBR,
	SYSZ_INS_LXDB,
	SYSZ_INS_LXDBR,
	SYSZ_INS_LXEB,
	SYSZ_INS_LXEBR,
	SYSZ_INS_LXR,
	SYSZ_INS_LY,
	SYSZ_INS_LZDR,
	SYSZ_INS_LZER,
	SYSZ_INS_LZXR,
	SYSZ_INS_MADB,
	SYSZ_INS_MADBR,
	SYSZ_INS_MAEB,
	SYSZ_INS_MAEBR,
	SYSZ_INS_MDB,
	SYSZ_INS_MDBR,
	SYSZ_INS_MDEB,
	SYSZ_INS_MDEBR,
	SYSZ_INS_MEEB,
	SYSZ_INS_MEEBR,
	SYSZ_INS_MGHI,
	SYSZ_INS_MH,
	SYSZ_INS_MHI,
	SYSZ_INS_MHY,
	SYSZ_INS_MLG,
	SYSZ_INS_MLGR,
	SYSZ_INS_MS,
	SYSZ_INS_MSDB,
	SYSZ_INS_MSDBR,
	SYSZ_INS_MSEB,
	SYSZ_INS_MSEBR,
	SYSZ_INS_MSFI,
	SYSZ_INS_MSG,
	SYSZ_INS_MSGF,
	SYSZ_INS_MSGFI,
	SYSZ_INS_MSGFR,
	SYSZ_INS_MSGR,
	SYSZ_INS_MSR,
	SYSZ_INS_MSY,
	SYSZ_INS_MVC,
	SYSZ_INS_MVGHI,
	SYSZ_INS_MVHHI,
	SYSZ_INS_MVHI,
	SYSZ_INS_MVI,
	SYSZ_INS_MVIY,
	SYSZ_INS_MVST,
	SYSZ_INS_MXBR,
	SYSZ_INS_MXDB,
	SYSZ_INS_MXDBR,
	SYSZ_INS_N,
	SYSZ_INS_NC,
	SYSZ_INS_NG,
	SYSZ_INS_NGR,
	SYSZ_INS_NGRK,
	SYSZ_INS_NI,
	SYSZ_INS_NIHF,
	SYSZ_INS_NIHH,
	SYSZ_INS_NIHL,
	SYSZ_INS_NILF,
	SYSZ_INS_NILH,
	SYSZ_INS_NILL,
	SYSZ_INS_NIY,
	SYSZ_INS_NR,
	SYSZ_INS_NRK,
	SYSZ_INS_NY,
	SYSZ_INS_O,
	SYSZ_INS_OC,
	SYSZ_INS_OG,
	SYSZ_INS_OGR,
	SYSZ_INS_OGRK,
	SYSZ_INS_OI,
	SYSZ_INS_OIHF,
	SYSZ_INS_OIHH,
	SYSZ_INS_OIHL,
	SYSZ_INS_OILF,
	SYSZ_INS_OILH,
	SYSZ_INS_OILL,
	SYSZ_INS_OIY,
	SYSZ_INS_OR,
	SYSZ_INS_ORK,
	SYSZ_INS_OY,
	SYSZ_INS_PFD,
	SYSZ_INS_PFDRL,
	SYSZ_INS_RISBG,
	SYSZ_INS_RISBHG,
	SYSZ_INS_RISBLG,
	SYSZ_INS_RLL,
	SYSZ_INS_RLLG,
	SYSZ_INS_RNSBG,
	SYSZ_INS_ROSBG,
	SYSZ_INS_RXSBG,
	SYSZ_INS_S,
	SYSZ_INS_SDB,
	SYSZ_INS_SDBR,
	SYSZ_INS_SEB,
	SYSZ_INS_SEBR,
	SYSZ_INS_SG,
	SYSZ_INS_SGF,
	SYSZ_INS_SGFR,
	SYSZ_INS_SGR,
	SYSZ_INS_SGRK,
	SYSZ_INS_SH,
	SYSZ_INS_SHY,
	SYSZ_INS_SL,
	SYSZ_INS_SLB,
	SYSZ_INS_SLBG,
	SYSZ_INS_SLBR,
	SYSZ_INS_SLFI,
	SYSZ_INS_SLG,
	SYSZ_INS_SLBGR,
	SYSZ_INS_SLGF,
	SYSZ_INS_SLGFI,
	SYSZ_INS_SLGFR,
	SYSZ_INS_SLGR,
	SYSZ_INS_SLGRK,
	SYSZ_INS_SLL,
	SYSZ_INS_SLLG,
	SYSZ_INS_SLLK,
	SYSZ_INS_SLR,
	SYSZ_INS_SLRK,
	SYSZ_INS_SLY,
	SYSZ_INS_SQDB,
	SYSZ_INS_SQDBR,
	SYSZ_INS_SQEB,
	SYSZ_INS_SQEBR,
	SYSZ_INS_SQXBR,
	SYSZ_INS_SR,
	SYSZ_INS_SRA,
	SYSZ_INS_SRAG,
	SYSZ_INS_SRAK,
	SYSZ_INS_SRK,
	SYSZ_INS_SRL,
	SYSZ_INS_SRLG,
	SYSZ_INS_SRLK,
	SYSZ_INS_SRST,
	SYSZ_INS_ST,
	SYSZ_INS_STC,
	SYSZ_INS_STCH,
	SYSZ_INS_STCY,
	SYSZ_INS_STD,
	SYSZ_INS_STDY,
	SYSZ_INS_STE,
	SYSZ_INS_STEY,
	SYSZ_INS_STFH,
	SYSZ_INS_STG,
	SYSZ_INS_STGRL,
	SYSZ_INS_STH,
	SYSZ_INS_STHH,
	SYSZ_INS_STHRL,
	SYSZ_INS_STHY,
	SYSZ_INS_STMG,
	SYSZ_INS_STRL,
	SYSZ_INS_STRV,
	SYSZ_INS_STRVG,
	SYSZ_INS_STY,
	SYSZ_INS_SXBR,
	SYSZ_INS_SY,
	SYSZ_INS_TM,
	SYSZ_INS_TMHH,
	SYSZ_INS_TMHL,
	SYSZ_INS_TMLH,
	SYSZ_INS_TMLL,
	SYSZ_INS_TMY,
	SYSZ_INS_X,
	SYSZ_INS_XC,
	SYSZ_INS_XG,
	SYSZ_INS_XGR,
	SYSZ_INS_XGRK,
	SYSZ_INS_XI,
	SYSZ_INS_XIHF,
	SYSZ_INS_XILF,
	SYSZ_INS_XIY,
	SYSZ_INS_XR,
	SYSZ_INS_XRK,
	SYSZ_INS_XY,

	SYSZ_INS_ENDING,   // <-- mark the end of the list of instructions
} sysz_insn;

/// Group of SystemZ instructions
typedef enum sysz_insn_group {
	SYSZ_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	SYSZ_GRP_JUMP,	///< = CS_GRP_JUMP

	// Architecture-specific groups
	SYSZ_GRP_DISTINCTOPS = 128,
	SYSZ_GRP_FPEXTENSION,
	SYSZ_GRP_HIGHWORD,
	SYSZ_GRP_INTERLOCKEDACCESS1,
	SYSZ_GRP_LOADSTOREONCOND,

	SYSZ_GRP_ENDING,   // <-- mark the end of the list of groups
} sysz_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/tms320c64x.h`:

```h
/* Capstone Disassembly Engine */
/* TMS320C64x Backend by Fotis Loukos <me@fotisl.com> 2016 */

#ifndef CAPSTONE_TMS320C64X_H
#define CAPSTONE_TMS320C64X_H

#ifdef __cplusplus
extern "C" {
#endif

#include <stdint.h>
#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

typedef enum tms320c64x_op_type {
	TMS320C64X_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	TMS320C64X_OP_REG, ///< = CS_OP_REG (Register operand).
	TMS320C64X_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	TMS320C64X_OP_MEM, ///< = CS_OP_MEM (Memory operand).
	TMS320C64X_OP_REGPAIR = 64, ///< Register pair for double word ops
} tms320c64x_op_type;

typedef enum tms320c64x_mem_disp {
	TMS320C64X_MEM_DISP_INVALID = 0,
	TMS320C64X_MEM_DISP_CONSTANT,
	TMS320C64X_MEM_DISP_REGISTER,
} tms320c64x_mem_disp;

typedef enum tms320c64x_mem_dir {
	TMS320C64X_MEM_DIR_INVALID = 0,
	TMS320C64X_MEM_DIR_FW,
	TMS320C64X_MEM_DIR_BW,
} tms320c64x_mem_dir;

typedef enum tms320c64x_mem_mod {
	TMS320C64X_MEM_MOD_INVALID = 0,
	TMS320C64X_MEM_MOD_NO,
	TMS320C64X_MEM_MOD_PRE,
	TMS320C64X_MEM_MOD_POST,
} tms320c64x_mem_mod;

typedef struct tms320c64x_op_mem {
	unsigned int	base;	///< base register
	unsigned int	disp;	///< displacement/offset value
	unsigned int	unit;	///< unit of base and offset register
	unsigned int	scaled;	///< offset scaled
	unsigned int	disptype;	///< displacement type
	unsigned int	direction;	///< direction
	unsigned int	modify;	///< modification
} tms320c64x_op_mem;

typedef struct cs_tms320c64x_op {
	tms320c64x_op_type type;	///< operand type
	union {
		unsigned int reg;	///< register value for REG operand or first register for REGPAIR operand
		int32_t imm;		///< immediate value for IMM operand
		tms320c64x_op_mem mem;		///< base/disp value for MEM operand
	};
} cs_tms320c64x_op;

typedef struct cs_tms320c64x {
	uint8_t op_count;
	cs_tms320c64x_op operands[8]; ///< operands for this instruction.
	struct {
		unsigned int reg;
		unsigned int zero;
	} condition;
	struct {
		unsigned int unit;
		unsigned int side;
		unsigned int crosspath;
	} funit;
	unsigned int parallel;
} cs_tms320c64x;

typedef enum tms320c64x_reg {
	TMS320C64X_REG_INVALID = 0,

	TMS320C64X_REG_AMR,
	TMS320C64X_REG_CSR,
	TMS320C64X_REG_DIER,
	TMS320C64X_REG_DNUM,
	TMS320C64X_REG_ECR,
	TMS320C64X_REG_GFPGFR,
	TMS320C64X_REG_GPLYA,
	TMS320C64X_REG_GPLYB,
	TMS320C64X_REG_ICR,
	TMS320C64X_REG_IER,
	TMS320C64X_REG_IERR,
	TMS320C64X_REG_ILC,
	TMS320C64X_REG_IRP,
	TMS320C64X_REG_ISR,
	TMS320C64X_REG_ISTP,
	TMS320C64X_REG_ITSR,
	TMS320C64X_REG_NRP,
	TMS320C64X_REG_NTSR,
	TMS320C64X_REG_REP,
	TMS320C64X_REG_RILC,
	TMS320C64X_REG_SSR,
	TMS320C64X_REG_TSCH,
	TMS320C64X_REG_TSCL,
	TMS320C64X_REG_TSR,
	TMS320C64X_REG_A0,
	TMS320C64X_REG_A1,
	TMS320C64X_REG_A2,
	TMS320C64X_REG_A3,
	TMS320C64X_REG_A4,
	TMS320C64X_REG_A5,
	TMS320C64X_REG_A6,
	TMS320C64X_REG_A7,
	TMS320C64X_REG_A8,
	TMS320C64X_REG_A9,
	TMS320C64X_REG_A10,
	TMS320C64X_REG_A11,
	TMS320C64X_REG_A12,
	TMS320C64X_REG_A13,
	TMS320C64X_REG_A14,
	TMS320C64X_REG_A15,
	TMS320C64X_REG_A16,
	TMS320C64X_REG_A17,
	TMS320C64X_REG_A18,
	TMS320C64X_REG_A19,
	TMS320C64X_REG_A20,
	TMS320C64X_REG_A21,
	TMS320C64X_REG_A22,
	TMS320C64X_REG_A23,
	TMS320C64X_REG_A24,
	TMS320C64X_REG_A25,
	TMS320C64X_REG_A26,
	TMS320C64X_REG_A27,
	TMS320C64X_REG_A28,
	TMS320C64X_REG_A29,
	TMS320C64X_REG_A30,
	TMS320C64X_REG_A31,
	TMS320C64X_REG_B0,
	TMS320C64X_REG_B1,
	TMS320C64X_REG_B2,
	TMS320C64X_REG_B3,
	TMS320C64X_REG_B4,
	TMS320C64X_REG_B5,
	TMS320C64X_REG_B6,
	TMS320C64X_REG_B7,
	TMS320C64X_REG_B8,
	TMS320C64X_REG_B9,
	TMS320C64X_REG_B10,
	TMS320C64X_REG_B11,
	TMS320C64X_REG_B12,
	TMS320C64X_REG_B13,
	TMS320C64X_REG_B14,
	TMS320C64X_REG_B15,
	TMS320C64X_REG_B16,
	TMS320C64X_REG_B17,
	TMS320C64X_REG_B18,
	TMS320C64X_REG_B19,
	TMS320C64X_REG_B20,
	TMS320C64X_REG_B21,
	TMS320C64X_REG_B22,
	TMS320C64X_REG_B23,
	TMS320C64X_REG_B24,
	TMS320C64X_REG_B25,
	TMS320C64X_REG_B26,
	TMS320C64X_REG_B27,
	TMS320C64X_REG_B28,
	TMS320C64X_REG_B29,
	TMS320C64X_REG_B30,
	TMS320C64X_REG_B31,
	TMS320C64X_REG_PCE1,

	TMS320C64X_REG_ENDING,	// <-- mark the end of the list of registers

	// Alias registers
	TMS320C64X_REG_EFR = TMS320C64X_REG_ECR,
	TMS320C64X_REG_IFR = TMS320C64X_REG_ISR,
} tms320c64x_reg;

typedef enum tms320c64x_insn {
	TMS320C64X_INS_INVALID = 0,

	TMS320C64X_INS_ABS,
	TMS320C64X_INS_ABS2,
	TMS320C64X_INS_ADD,
	TMS320C64X_INS_ADD2,
	TMS320C64X_INS_ADD4,
	TMS320C64X_INS_ADDAB,
	TMS320C64X_INS_ADDAD,
	TMS320C64X_INS_ADDAH,
	TMS320C64X_INS_ADDAW,
	TMS320C64X_INS_ADDK,
	TMS320C64X_INS_ADDKPC,
	TMS320C64X_INS_ADDU,
	TMS320C64X_INS_AND,
	TMS320C64X_INS_ANDN,
	TMS320C64X_INS_AVG2,
	TMS320C64X_INS_AVGU4,
	TMS320C64X_INS_B,
	TMS320C64X_INS_BDEC,
	TMS320C64X_INS_BITC4,
	TMS320C64X_INS_BNOP,
	TMS320C64X_INS_BPOS,
	TMS320C64X_INS_CLR,
	TMS320C64X_INS_CMPEQ,
	TMS320C64X_INS_CMPEQ2,
	TMS320C64X_INS_CMPEQ4,
	TMS320C64X_INS_CMPGT,
	TMS320C64X_INS_CMPGT2,
	TMS320C64X_INS_CMPGTU4,
	TMS320C64X_INS_CMPLT,
	TMS320C64X_INS_CMPLTU,
	TMS320C64X_INS_DEAL,
	TMS320C64X_INS_DOTP2,
	TMS320C64X_INS_DOTPN2,
	TMS320C64X_INS_DOTPNRSU2,
	TMS320C64X_INS_DOTPRSU2,
	TMS320C64X_INS_DOTPSU4,
	TMS320C64X_INS_DOTPU4,
	TMS320C64X_INS_EXT,
	TMS320C64X_INS_EXTU,
	TMS320C64X_INS_GMPGTU,
	TMS320C64X_INS_GMPY4,
	TMS320C64X_INS_LDB,
	TMS320C64X_INS_LDBU,
	TMS320C64X_INS_LDDW,
	TMS320C64X_INS_LDH,
	TMS320C64X_INS_LDHU,
	TMS320C64X_INS_LDNDW,
	TMS320C64X_INS_LDNW,
	TMS320C64X_INS_LDW,
	TMS320C64X_INS_LMBD,
	TMS320C64X_INS_MAX2,
	TMS320C64X_INS_MAXU4,
	TMS320C64X_INS_MIN2,
	TMS320C64X_INS_MINU4,
	TMS320C64X_INS_MPY,
	TMS320C64X_INS_MPY2,
	TMS320C64X_INS_MPYH,
	TMS320C64X_INS_MPYHI,
	TMS320C64X_INS_MPYHIR,
	TMS320C64X_INS_MPYHL,
	TMS320C64X_INS_MPYHLU,
	TMS320C64X_INS_MPYHSLU,
	TMS320C64X_INS_MPYHSU,
	TMS320C64X_INS_MPYHU,
	TMS320C64X_INS_MPYHULS,
	TMS320C64X_INS_MPYHUS,
	TMS320C64X_INS_MPYLH,
	TMS320C64X_INS_MPYLHU,
	TMS320C64X_INS_MPYLI,
	TMS320C64X_INS_MPYLIR,
	TMS320C64X_INS_MPYLSHU,
	TMS320C64X_INS_MPYLUHS,
	TMS320C64X_INS_MPYSU,
	TMS320C64X_INS_MPYSU4,
	TMS320C64X_INS_MPYU,
	TMS320C64X_INS_MPYU4,
	TMS320C64X_INS_MPYUS,
	TMS320C64X_INS_MVC,
	TMS320C64X_INS_MVD,
	TMS320C64X_INS_MVK,
	TMS320C64X_INS_MVKL,
	TMS320C64X_INS_MVKLH,
	TMS320C64X_INS_NOP,
	TMS320C64X_INS_NORM,
	TMS320C64X_INS_OR,
	TMS320C64X_INS_PACK2,
	TMS320C64X_INS_PACKH2,
	TMS320C64X_INS_PACKH4,
	TMS320C64X_INS_PACKHL2,
	TMS320C64X_INS_PACKL4,
	TMS320C64X_INS_PACKLH2,
	TMS320C64X_INS_ROTL,
	TMS320C64X_INS_SADD,
	TMS320C64X_INS_SADD2,
	TMS320C64X_INS_SADDU4,
	TMS320C64X_INS_SADDUS2,
	TMS320C64X_INS_SAT,
	TMS320C64X_INS_SET,
	TMS320C64X_INS_SHFL,
	TMS320C64X_INS_SHL,
	TMS320C64X_INS_SHLMB,
	TMS320C64X_INS_SHR,
	TMS320C64X_INS_SHR2,
	TMS320C64X_INS_SHRMB,
	TMS320C64X_INS_SHRU,
	TMS320C64X_INS_SHRU2,
	TMS320C64X_INS_SMPY,
	TMS320C64X_INS_SMPY2,
	TMS320C64X_INS_SMPYH,
	TMS320C64X_INS_SMPYHL,
	TMS320C64X_INS_SMPYLH,
	TMS320C64X_INS_SPACK2,
	TMS320C64X_INS_SPACKU4,
	TMS320C64X_INS_SSHL,
	TMS320C64X_INS_SSHVL,
	TMS320C64X_INS_SSHVR,
	TMS320C64X_INS_SSUB,
	TMS320C64X_INS_STB,
	TMS320C64X_INS_STDW,
	TMS320C64X_INS_STH,
	TMS320C64X_INS_STNDW,
	TMS320C64X_INS_STNW,
	TMS320C64X_INS_STW,
	TMS320C64X_INS_SUB,
	TMS320C64X_INS_SUB2,
	TMS320C64X_INS_SUB4,
	TMS320C64X_INS_SUBAB,
	TMS320C64X_INS_SUBABS4,
	TMS320C64X_INS_SUBAH,
	TMS320C64X_INS_SUBAW,
	TMS320C64X_INS_SUBC,
	TMS320C64X_INS_SUBU,
	TMS320C64X_INS_SWAP4,
	TMS320C64X_INS_UNPKHU4,
	TMS320C64X_INS_UNPKLU4,
	TMS320C64X_INS_XOR,
	TMS320C64X_INS_XPND2,
	TMS320C64X_INS_XPND4,
	// Aliases
	TMS320C64X_INS_IDLE,
	TMS320C64X_INS_MV,
	TMS320C64X_INS_NEG,
	TMS320C64X_INS_NOT,
	TMS320C64X_INS_SWAP2,
	TMS320C64X_INS_ZERO,

	TMS320C64X_INS_ENDING,   // <-- mark the end of the list of instructions
} tms320c64x_insn;

typedef enum tms320c64x_insn_group {
	TMS320C64X_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	TMS320C64X_GRP_JUMP,	///< = CS_GRP_JUMP

	TMS320C64X_GRP_FUNIT_D = 128,
	TMS320C64X_GRP_FUNIT_L,
	TMS320C64X_GRP_FUNIT_M,
	TMS320C64X_GRP_FUNIT_S,
	TMS320C64X_GRP_FUNIT_NO,

	TMS320C64X_GRP_ENDING,   // <-- mark the end of the list of groups
} tms320c64x_insn_group;

typedef enum tms320c64x_funit {
	TMS320C64X_FUNIT_INVALID = 0,
	TMS320C64X_FUNIT_D,
	TMS320C64X_FUNIT_L,
	TMS320C64X_FUNIT_M,
	TMS320C64X_FUNIT_S,
	TMS320C64X_FUNIT_NO
} tms320c64x_funit;

#ifdef __cplusplus
}
#endif

#endif


```

`CodeCleaner/capstone/include/capstone/x86.h`:

```h
#ifndef CAPSTONE_X86_H
#define CAPSTONE_X86_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2013-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

/// Calculate relative address for X86-64, given cs_insn structure
#define X86_REL_ADDR(insn) (((insn).detail->x86.operands[0].type == X86_OP_IMM) \
	? (uint64_t)((insn).detail->x86.operands[0].imm) \
	: (((insn).address + (insn).size) + (uint64_t)(insn).detail->x86.disp))

/// X86 registers
typedef enum x86_reg {
	X86_REG_INVALID = 0,
	X86_REG_AH, X86_REG_AL, X86_REG_AX, X86_REG_BH, X86_REG_BL,
	X86_REG_BP, X86_REG_BPL, X86_REG_BX, X86_REG_CH, X86_REG_CL,
	X86_REG_CS, X86_REG_CX, X86_REG_DH, X86_REG_DI, X86_REG_DIL,
	X86_REG_DL, X86_REG_DS, X86_REG_DX, X86_REG_EAX, X86_REG_EBP,
	X86_REG_EBX, X86_REG_ECX, X86_REG_EDI, X86_REG_EDX, X86_REG_EFLAGS,
	X86_REG_EIP, X86_REG_EIZ, X86_REG_ES, X86_REG_ESI, X86_REG_ESP,
	X86_REG_FPSW, X86_REG_FS, X86_REG_GS, X86_REG_IP, X86_REG_RAX,
	X86_REG_RBP, X86_REG_RBX, X86_REG_RCX, X86_REG_RDI, X86_REG_RDX,
	X86_REG_RIP, X86_REG_RIZ, X86_REG_RSI, X86_REG_RSP, X86_REG_SI,
	X86_REG_SIL, X86_REG_SP, X86_REG_SPL, X86_REG_SS, X86_REG_CR0,
	X86_REG_CR1, X86_REG_CR2, X86_REG_CR3, X86_REG_CR4, X86_REG_CR5,
	X86_REG_CR6, X86_REG_CR7, X86_REG_CR8, X86_REG_CR9, X86_REG_CR10,
	X86_REG_CR11, X86_REG_CR12, X86_REG_CR13, X86_REG_CR14, X86_REG_CR15,
	X86_REG_DR0, X86_REG_DR1, X86_REG_DR2, X86_REG_DR3, X86_REG_DR4,
	X86_REG_DR5, X86_REG_DR6, X86_REG_DR7, X86_REG_DR8, X86_REG_DR9,
	X86_REG_DR10, X86_REG_DR11, X86_REG_DR12, X86_REG_DR13, X86_REG_DR14,
	X86_REG_DR15, X86_REG_FP0, X86_REG_FP1, X86_REG_FP2, X86_REG_FP3,
	X86_REG_FP4, X86_REG_FP5, X86_REG_FP6, X86_REG_FP7,
	X86_REG_K0, X86_REG_K1, X86_REG_K2, X86_REG_K3, X86_REG_K4,
	X86_REG_K5, X86_REG_K6, X86_REG_K7, X86_REG_MM0, X86_REG_MM1,
	X86_REG_MM2, X86_REG_MM3, X86_REG_MM4, X86_REG_MM5, X86_REG_MM6,
	X86_REG_MM7, X86_REG_R8, X86_REG_R9, X86_REG_R10, X86_REG_R11,
	X86_REG_R12, X86_REG_R13, X86_REG_R14, X86_REG_R15,
	X86_REG_ST0, X86_REG_ST1, X86_REG_ST2, X86_REG_ST3,
	X86_REG_ST4, X86_REG_ST5, X86_REG_ST6, X86_REG_ST7,
	X86_REG_XMM0, X86_REG_XMM1, X86_REG_XMM2, X86_REG_XMM3, X86_REG_XMM4,
	X86_REG_XMM5, X86_REG_XMM6, X86_REG_XMM7, X86_REG_XMM8, X86_REG_XMM9,
	X86_REG_XMM10, X86_REG_XMM11, X86_REG_XMM12, X86_REG_XMM13, X86_REG_XMM14,
	X86_REG_XMM15, X86_REG_XMM16, X86_REG_XMM17, X86_REG_XMM18, X86_REG_XMM19,
	X86_REG_XMM20, X86_REG_XMM21, X86_REG_XMM22, X86_REG_XMM23, X86_REG_XMM24,
	X86_REG_XMM25, X86_REG_XMM26, X86_REG_XMM27, X86_REG_XMM28, X86_REG_XMM29,
	X86_REG_XMM30, X86_REG_XMM31, X86_REG_YMM0, X86_REG_YMM1, X86_REG_YMM2,
	X86_REG_YMM3, X86_REG_YMM4, X86_REG_YMM5, X86_REG_YMM6, X86_REG_YMM7,
	X86_REG_YMM8, X86_REG_YMM9, X86_REG_YMM10, X86_REG_YMM11, X86_REG_YMM12,
	X86_REG_YMM13, X86_REG_YMM14, X86_REG_YMM15, X86_REG_YMM16, X86_REG_YMM17,
	X86_REG_YMM18, X86_REG_YMM19, X86_REG_YMM20, X86_REG_YMM21, X86_REG_YMM22,
	X86_REG_YMM23, X86_REG_YMM24, X86_REG_YMM25, X86_REG_YMM26, X86_REG_YMM27,
	X86_REG_YMM28, X86_REG_YMM29, X86_REG_YMM30, X86_REG_YMM31, X86_REG_ZMM0,
	X86_REG_ZMM1, X86_REG_ZMM2, X86_REG_ZMM3, X86_REG_ZMM4, X86_REG_ZMM5,
	X86_REG_ZMM6, X86_REG_ZMM7, X86_REG_ZMM8, X86_REG_ZMM9, X86_REG_ZMM10,
	X86_REG_ZMM11, X86_REG_ZMM12, X86_REG_ZMM13, X86_REG_ZMM14, X86_REG_ZMM15,
	X86_REG_ZMM16, X86_REG_ZMM17, X86_REG_ZMM18, X86_REG_ZMM19, X86_REG_ZMM20,
	X86_REG_ZMM21, X86_REG_ZMM22, X86_REG_ZMM23, X86_REG_ZMM24, X86_REG_ZMM25,
	X86_REG_ZMM26, X86_REG_ZMM27, X86_REG_ZMM28, X86_REG_ZMM29, X86_REG_ZMM30,
	X86_REG_ZMM31, X86_REG_R8B, X86_REG_R9B, X86_REG_R10B, X86_REG_R11B,
	X86_REG_R12B, X86_REG_R13B, X86_REG_R14B, X86_REG_R15B, X86_REG_R8D,
	X86_REG_R9D, X86_REG_R10D, X86_REG_R11D, X86_REG_R12D, X86_REG_R13D,
	X86_REG_R14D, X86_REG_R15D, X86_REG_R8W, X86_REG_R9W, X86_REG_R10W,
	X86_REG_R11W, X86_REG_R12W, X86_REG_R13W, X86_REG_R14W, X86_REG_R15W,

	X86_REG_ENDING		// <-- mark the end of the list of registers
} x86_reg;

// Sub-flags of EFLAGS
#define X86_EFLAGS_MODIFY_AF (1ULL << 0)
#define X86_EFLAGS_MODIFY_CF (1ULL << 1)
#define X86_EFLAGS_MODIFY_SF (1ULL << 2)
#define X86_EFLAGS_MODIFY_ZF (1ULL << 3)
#define X86_EFLAGS_MODIFY_PF (1ULL << 4)
#define X86_EFLAGS_MODIFY_OF (1ULL << 5)
#define X86_EFLAGS_MODIFY_TF (1ULL << 6)
#define X86_EFLAGS_MODIFY_IF (1ULL << 7)
#define X86_EFLAGS_MODIFY_DF (1ULL << 8)
#define X86_EFLAGS_MODIFY_NT (1ULL << 9)
#define X86_EFLAGS_MODIFY_RF (1ULL << 10)
#define X86_EFLAGS_PRIOR_OF (1ULL << 11)
#define X86_EFLAGS_PRIOR_SF (1ULL << 12)
#define X86_EFLAGS_PRIOR_ZF (1ULL << 13)
#define X86_EFLAGS_PRIOR_AF (1ULL << 14)
#define X86_EFLAGS_PRIOR_PF (1ULL << 15)
#define X86_EFLAGS_PRIOR_CF (1ULL << 16)
#define X86_EFLAGS_PRIOR_TF (1ULL << 17)
#define X86_EFLAGS_PRIOR_IF (1ULL << 18)
#define X86_EFLAGS_PRIOR_DF (1ULL << 19)
#define X86_EFLAGS_PRIOR_NT (1ULL << 20)
#define X86_EFLAGS_RESET_OF (1ULL << 21)
#define X86_EFLAGS_RESET_CF (1ULL << 22)
#define X86_EFLAGS_RESET_DF (1ULL << 23)
#define X86_EFLAGS_RESET_IF (1ULL << 24)
#define X86_EFLAGS_RESET_SF (1ULL << 25)
#define X86_EFLAGS_RESET_AF (1ULL << 26)
#define X86_EFLAGS_RESET_TF (1ULL << 27)
#define X86_EFLAGS_RESET_NT (1ULL << 28)
#define X86_EFLAGS_RESET_PF (1ULL << 29)
#define X86_EFLAGS_SET_CF (1ULL << 30)
#define X86_EFLAGS_SET_DF (1ULL << 31)
#define X86_EFLAGS_SET_IF (1ULL << 32)
#define X86_EFLAGS_TEST_OF (1ULL << 33)
#define X86_EFLAGS_TEST_SF (1ULL << 34)
#define X86_EFLAGS_TEST_ZF (1ULL << 35)
#define X86_EFLAGS_TEST_PF (1ULL << 36)
#define X86_EFLAGS_TEST_CF (1ULL << 37)
#define X86_EFLAGS_TEST_NT (1ULL << 38)
#define X86_EFLAGS_TEST_DF (1ULL << 39)
#define X86_EFLAGS_UNDEFINED_OF (1ULL << 40)
#define X86_EFLAGS_UNDEFINED_SF (1ULL << 41)
#define X86_EFLAGS_UNDEFINED_ZF (1ULL << 42)
#define X86_EFLAGS_UNDEFINED_PF (1ULL << 43)
#define X86_EFLAGS_UNDEFINED_AF (1ULL << 44)
#define X86_EFLAGS_UNDEFINED_CF (1ULL << 45)
#define X86_EFLAGS_RESET_RF (1ULL << 46)
#define X86_EFLAGS_TEST_RF (1ULL << 47)
#define X86_EFLAGS_TEST_IF (1ULL << 48)
#define X86_EFLAGS_TEST_TF (1ULL << 49)
#define X86_EFLAGS_TEST_AF (1ULL << 50)
#define X86_EFLAGS_RESET_ZF (1ULL << 51)
#define X86_EFLAGS_SET_OF (1ULL << 52)
#define X86_EFLAGS_SET_SF (1ULL << 53)
#define X86_EFLAGS_SET_ZF (1ULL << 54)
#define X86_EFLAGS_SET_AF (1ULL << 55)
#define X86_EFLAGS_SET_PF (1ULL << 56)
#define X86_EFLAGS_RESET_0F (1ULL << 57)
#define X86_EFLAGS_RESET_AC (1ULL << 58)

#define X86_FPU_FLAGS_MODIFY_C0 (1ULL << 0)
#define X86_FPU_FLAGS_MODIFY_C1 (1ULL << 1)
#define X86_FPU_FLAGS_MODIFY_C2 (1ULL << 2)
#define X86_FPU_FLAGS_MODIFY_C3 (1ULL << 3)
#define X86_FPU_FLAGS_RESET_C0 (1ULL << 4)
#define X86_FPU_FLAGS_RESET_C1 (1ULL << 5)
#define X86_FPU_FLAGS_RESET_C2 (1ULL << 6)
#define X86_FPU_FLAGS_RESET_C3 (1ULL << 7)
#define X86_FPU_FLAGS_SET_C0 (1ULL << 8)
#define X86_FPU_FLAGS_SET_C1 (1ULL << 9)
#define X86_FPU_FLAGS_SET_C2 (1ULL << 10)
#define X86_FPU_FLAGS_SET_C3 (1ULL << 11)
#define X86_FPU_FLAGS_UNDEFINED_C0 (1ULL << 12)
#define X86_FPU_FLAGS_UNDEFINED_C1 (1ULL << 13)
#define X86_FPU_FLAGS_UNDEFINED_C2 (1ULL << 14)
#define X86_FPU_FLAGS_UNDEFINED_C3 (1ULL << 15)
#define X86_FPU_FLAGS_TEST_C0 (1ULL << 16)
#define X86_FPU_FLAGS_TEST_C1 (1ULL << 17)
#define X86_FPU_FLAGS_TEST_C2 (1ULL << 18)
#define X86_FPU_FLAGS_TEST_C3 (1ULL << 19)


/// Operand type for instruction's operands
typedef enum x86_op_type {
	X86_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	X86_OP_REG, ///< = CS_OP_REG (Register operand).
	X86_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	X86_OP_MEM, ///< = CS_OP_MEM (Memory operand).
} x86_op_type;

/// XOP Code Condition type
typedef enum x86_xop_cc {
	X86_XOP_CC_INVALID = 0,	///< Uninitialized.
	X86_XOP_CC_LT,
	X86_XOP_CC_LE,
	X86_XOP_CC_GT,
	X86_XOP_CC_GE,
	X86_XOP_CC_EQ,
	X86_XOP_CC_NEQ,
	X86_XOP_CC_FALSE,
	X86_XOP_CC_TRUE,
} x86_xop_cc;

/// AVX broadcast type
typedef enum x86_avx_bcast {
	X86_AVX_BCAST_INVALID = 0,	///< Uninitialized.
	X86_AVX_BCAST_2,	///< AVX512 broadcast type {1to2}
	X86_AVX_BCAST_4,	///< AVX512 broadcast type {1to4}
	X86_AVX_BCAST_8,	///< AVX512 broadcast type {1to8}
	X86_AVX_BCAST_16,	///< AVX512 broadcast type {1to16}
} x86_avx_bcast;

/// SSE Code Condition type
typedef enum x86_sse_cc {
	X86_SSE_CC_INVALID = 0,	///< Uninitialized.
	X86_SSE_CC_EQ,
	X86_SSE_CC_LT,
	X86_SSE_CC_LE,
	X86_SSE_CC_UNORD,
	X86_SSE_CC_NEQ,
	X86_SSE_CC_NLT,
	X86_SSE_CC_NLE,
	X86_SSE_CC_ORD,
} x86_sse_cc;

/// AVX Code Condition type
typedef enum x86_avx_cc {
	X86_AVX_CC_INVALID = 0,	///< Uninitialized.
	X86_AVX_CC_EQ,
	X86_AVX_CC_LT,
	X86_AVX_CC_LE,
	X86_AVX_CC_UNORD,
	X86_AVX_CC_NEQ,
	X86_AVX_CC_NLT,
	X86_AVX_CC_NLE,
	X86_AVX_CC_ORD,
	X86_AVX_CC_EQ_UQ,
	X86_AVX_CC_NGE,
	X86_AVX_CC_NGT,
	X86_AVX_CC_FALSE,
	X86_AVX_CC_NEQ_OQ,
	X86_AVX_CC_GE,
	X86_AVX_CC_GT,
	X86_AVX_CC_TRUE,
	X86_AVX_CC_EQ_OS,
	X86_AVX_CC_LT_OQ,
	X86_AVX_CC_LE_OQ,
	X86_AVX_CC_UNORD_S,
	X86_AVX_CC_NEQ_US,
	X86_AVX_CC_NLT_UQ,
	X86_AVX_CC_NLE_UQ,
	X86_AVX_CC_ORD_S,
	X86_AVX_CC_EQ_US,
	X86_AVX_CC_NGE_UQ,
	X86_AVX_CC_NGT_UQ,
	X86_AVX_CC_FALSE_OS,
	X86_AVX_CC_NEQ_OS,
	X86_AVX_CC_GE_OQ,
	X86_AVX_CC_GT_OQ,
	X86_AVX_CC_TRUE_US,
} x86_avx_cc;

/// AVX static rounding mode type
typedef enum x86_avx_rm {
	X86_AVX_RM_INVALID = 0,	///< Uninitialized.
	X86_AVX_RM_RN,	///< Round to nearest
	X86_AVX_RM_RD,	///< Round down
	X86_AVX_RM_RU,	///< Round up
	X86_AVX_RM_RZ,	///< Round toward zero
} x86_avx_rm;

/// Instruction prefixes - to be used in cs_x86.prefix[]
typedef enum x86_prefix {
	X86_PREFIX_LOCK		= 	0xf0,	///< lock (cs_x86.prefix[0]
	X86_PREFIX_REP		= 	0xf3,	///< rep (cs_x86.prefix[0]
	X86_PREFIX_REPE		= 	0xf3,	///< repe/repz (cs_x86.prefix[0]
	X86_PREFIX_REPNE	= 	0xf2,	///< repne/repnz (cs_x86.prefix[0]

	X86_PREFIX_CS		= 	0x2e,	///< segment override CS (cs_x86.prefix[1]
	X86_PREFIX_SS		= 	0x36,	///< segment override SS (cs_x86.prefix[1]
	X86_PREFIX_DS		= 	0x3e,	///< segment override DS (cs_x86.prefix[1]
	X86_PREFIX_ES		= 	0x26,	///< segment override ES (cs_x86.prefix[1]
	X86_PREFIX_FS		= 	0x64,	///< segment override FS (cs_x86.prefix[1]
	X86_PREFIX_GS		= 	0x65,	///< segment override GS (cs_x86.prefix[1]

	X86_PREFIX_OPSIZE	=	0x66,	///< operand-size override (cs_x86.prefix[2]
	X86_PREFIX_ADDRSIZE	=	0x67,	///< address-size override (cs_x86.prefix[3]
} x86_prefix;

/// Instruction's operand referring to memory
/// This is associated with X86_OP_MEM operand type above
typedef struct x86_op_mem {
	x86_reg segment; ///< segment register (or X86_REG_INVALID if irrelevant)
	x86_reg base;	///< base register (or X86_REG_INVALID if irrelevant)
	x86_reg index;	///< index register (or X86_REG_INVALID if irrelevant)
	int scale;	///< scale for index register
	int64_t disp;	///< displacement value
} x86_op_mem;

/// Instruction operand
typedef struct cs_x86_op {
		x86_op_type type;	///< operand type
		union {
			x86_reg reg;	  ///< register value for REG operand
			int64_t imm;		///< immediate value for IMM operand
			x86_op_mem mem;		///< base/index/scale/disp value for MEM operand
		};

		/// size of this operand (in bytes).
		uint8_t size;

		/// How is this operand accessed? (READ, WRITE or READ|WRITE)
		/// This field is combined of cs_ac_type.
		/// NOTE: this field is irrelevant if engine is compiled in DIET mode.
		uint8_t access;

		/// AVX broadcast type, or 0 if irrelevant
		x86_avx_bcast avx_bcast;

		/// AVX zero opmask {z}
		bool avx_zero_opmask;
} cs_x86_op;

typedef struct cs_x86_encoding {
	/// ModR/M offset, or 0 when irrelevant
	uint8_t modrm_offset;

	/// Displacement offset, or 0 when irrelevant.
	uint8_t disp_offset;
	uint8_t disp_size;

	/// Immediate offset, or 0 when irrelevant.
	uint8_t imm_offset;
	uint8_t imm_size;
} cs_x86_encoding;

/// Instruction structure
typedef struct cs_x86 {
	/// Instruction prefix, which can be up to 4 bytes.
	/// A prefix byte gets value 0 when irrelevant.
	/// prefix[0] indicates REP/REPNE/LOCK prefix (See X86_PREFIX_REP/REPNE/LOCK above)
	/// prefix[1] indicates segment override (irrelevant for x86_64):
	/// See X86_PREFIX_CS/SS/DS/ES/FS/GS above.
	/// prefix[2] indicates operand-size override (X86_PREFIX_OPSIZE)
	/// prefix[3] indicates address-size override (X86_PREFIX_ADDRSIZE)
	uint8_t prefix[4];

	/// Instruction opcode, which can be from 1 to 4 bytes in size.
	/// This contains VEX opcode as well.
	/// An trailing opcode byte gets value 0 when irrelevant.
	uint8_t opcode[4];

	/// REX prefix: only a non-zero value is relevant for x86_64
	uint8_t rex;

	/// Address size, which can be overridden with above prefix[5].
	uint8_t addr_size;

	/// ModR/M byte
	uint8_t modrm;

	/// SIB value, or 0 when irrelevant.
	uint8_t sib;

	/// Displacement value, valid if encoding.disp_offset != 0
	int64_t disp;

	/// SIB index register, or X86_REG_INVALID when irrelevant.
	x86_reg sib_index;
	/// SIB scale, only applicable if sib_index is valid.
	int8_t sib_scale;
	/// SIB base register, or X86_REG_INVALID when irrelevant.
	x86_reg sib_base;

	/// XOP Code Condition
	x86_xop_cc xop_cc;

	/// SSE Code Condition
	x86_sse_cc sse_cc;

	/// AVX Code Condition
	x86_avx_cc avx_cc;

	/// AVX Suppress all Exception
	bool avx_sae;

	/// AVX static rounding mode
	x86_avx_rm avx_rm;

	
	union {
		/// EFLAGS updated by this instruction.
		/// This can be formed from OR combination of X86_EFLAGS_* symbols in x86.h
		uint64_t eflags;
		/// FPU_FLAGS updated by this instruction.
		/// This can be formed from OR combination of X86_FPU_FLAGS_* symbols in x86.h
		uint64_t fpu_flags;
	};

	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;

	cs_x86_op operands[8];	///< operands for this instruction.

	cs_x86_encoding encoding;  ///< encoding information
} cs_x86;

/// X86 instructions
typedef enum x86_insn {
	X86_INS_INVALID = 0,

	X86_INS_AAA,
	X86_INS_AAD,
	X86_INS_AAM,
	X86_INS_AAS,
	X86_INS_FABS,
	X86_INS_ADC,
	X86_INS_ADCX,
	X86_INS_ADD,
	X86_INS_ADDPD,
	X86_INS_ADDPS,
	X86_INS_ADDSD,
	X86_INS_ADDSS,
	X86_INS_ADDSUBPD,
	X86_INS_ADDSUBPS,
	X86_INS_FADD,
	X86_INS_FIADD,
	X86_INS_FADDP,
	X86_INS_ADOX,
	X86_INS_AESDECLAST,
	X86_INS_AESDEC,
	X86_INS_AESENCLAST,
	X86_INS_AESENC,
	X86_INS_AESIMC,
	X86_INS_AESKEYGENASSIST,
	X86_INS_AND,
	X86_INS_ANDN,
	X86_INS_ANDNPD,
	X86_INS_ANDNPS,
	X86_INS_ANDPD,
	X86_INS_ANDPS,
	X86_INS_ARPL,
	X86_INS_BEXTR,
	X86_INS_BLCFILL,
	X86_INS_BLCI,
	X86_INS_BLCIC,
	X86_INS_BLCMSK,
	X86_INS_BLCS,
	X86_INS_BLENDPD,
	X86_INS_BLENDPS,
	X86_INS_BLENDVPD,
	X86_INS_BLENDVPS,
	X86_INS_BLSFILL,
	X86_INS_BLSI,
	X86_INS_BLSIC,
	X86_INS_BLSMSK,
	X86_INS_BLSR,
	X86_INS_BOUND,
	X86_INS_BSF,
	X86_INS_BSR,
	X86_INS_BSWAP,
	X86_INS_BT,
	X86_INS_BTC,
	X86_INS_BTR,
	X86_INS_BTS,
	X86_INS_BZHI,
	X86_INS_CALL,
	X86_INS_CBW,
	X86_INS_CDQ,
	X86_INS_CDQE,
	X86_INS_FCHS,
	X86_INS_CLAC,
	X86_INS_CLC,
	X86_INS_CLD,
	X86_INS_CLFLUSH,
	X86_INS_CLFLUSHOPT,
	X86_INS_CLGI,
	X86_INS_CLI,
	X86_INS_CLTS,
	X86_INS_CLWB,
	X86_INS_CMC,
	X86_INS_CMOVA,
	X86_INS_CMOVAE,
	X86_INS_CMOVB,
	X86_INS_CMOVBE,
	X86_INS_FCMOVBE,
	X86_INS_FCMOVB,
	X86_INS_CMOVE,
	X86_INS_FCMOVE,
	X86_INS_CMOVG,
	X86_INS_CMOVGE,
	X86_INS_CMOVL,
	X86_INS_CMOVLE,
	X86_INS_FCMOVNBE,
	X86_INS_FCMOVNB,
	X86_INS_CMOVNE,
	X86_INS_FCMOVNE,
	X86_INS_CMOVNO,
	X86_INS_CMOVNP,
	X86_INS_FCMOVNU,
	X86_INS_CMOVNS,
	X86_INS_CMOVO,
	X86_INS_CMOVP,
	X86_INS_FCMOVU,
	X86_INS_CMOVS,
	X86_INS_CMP,
	X86_INS_CMPSB,
	X86_INS_CMPSQ,
	X86_INS_CMPSW,
	X86_INS_CMPXCHG16B,
	X86_INS_CMPXCHG,
	X86_INS_CMPXCHG8B,
	X86_INS_COMISD,
	X86_INS_COMISS,
	X86_INS_FCOMP,
	X86_INS_FCOMIP,
	X86_INS_FCOMI,
	X86_INS_FCOM,
	X86_INS_FCOS,
	X86_INS_CPUID,
	X86_INS_CQO,
	X86_INS_CRC32,
	X86_INS_CVTDQ2PD,
	X86_INS_CVTDQ2PS,
	X86_INS_CVTPD2DQ,
	X86_INS_CVTPD2PS,
	X86_INS_CVTPS2DQ,
	X86_INS_CVTPS2PD,
	X86_INS_CVTSD2SI,
	X86_INS_CVTSD2SS,
	X86_INS_CVTSI2SD,
	X86_INS_CVTSI2SS,
	X86_INS_CVTSS2SD,
	X86_INS_CVTSS2SI,
	X86_INS_CVTTPD2DQ,
	X86_INS_CVTTPS2DQ,
	X86_INS_CVTTSD2SI,
	X86_INS_CVTTSS2SI,
	X86_INS_CWD,
	X86_INS_CWDE,
	X86_INS_DAA,
	X86_INS_DAS,
	X86_INS_DATA16,
	X86_INS_DEC,
	X86_INS_DIV,
	X86_INS_DIVPD,
	X86_INS_DIVPS,
	X86_INS_FDIVR,
	X86_INS_FIDIVR,
	X86_INS_FDIVRP,
	X86_INS_DIVSD,
	X86_INS_DIVSS,
	X86_INS_FDIV,
	X86_INS_FIDIV,
	X86_INS_FDIVP,
	X86_INS_DPPD,
	X86_INS_DPPS,
	X86_INS_RET,
	X86_INS_ENCLS,
	X86_INS_ENCLU,
	X86_INS_ENTER,
	X86_INS_EXTRACTPS,
	X86_INS_EXTRQ,
	X86_INS_F2XM1,
	X86_INS_LCALL,
	X86_INS_LJMP,
	X86_INS_FBLD,
	X86_INS_FBSTP,
	X86_INS_FCOMPP,
	X86_INS_FDECSTP,
	X86_INS_FEMMS,
	X86_INS_FFREE,
	X86_INS_FICOM,
	X86_INS_FICOMP,
	X86_INS_FINCSTP,
	X86_INS_FLDCW,
	X86_INS_FLDENV,
	X86_INS_FLDL2E,
	X86_INS_FLDL2T,
	X86_INS_FLDLG2,
	X86_INS_FLDLN2,
	X86_INS_FLDPI,
	X86_INS_FNCLEX,
	X86_INS_FNINIT,
	X86_INS_FNOP,
	X86_INS_FNSTCW,
	X86_INS_FNSTSW,
	X86_INS_FPATAN,
	X86_INS_FPREM,
	X86_INS_FPREM1,
	X86_INS_FPTAN,
	X86_INS_FFREEP,
	X86_INS_FRNDINT,
	X86_INS_FRSTOR,
	X86_INS_FNSAVE,
	X86_INS_FSCALE,
	X86_INS_FSETPM,
	X86_INS_FSINCOS,
	X86_INS_FNSTENV,
	X86_INS_FXAM,
	X86_INS_FXRSTOR,
	X86_INS_FXRSTOR64,
	X86_INS_FXSAVE,
	X86_INS_FXSAVE64,
	X86_INS_FXTRACT,
	X86_INS_FYL2X,
	X86_INS_FYL2XP1,
	X86_INS_MOVAPD,
	X86_INS_MOVAPS,
	X86_INS_ORPD,
	X86_INS_ORPS,
	X86_INS_VMOVAPD,
	X86_INS_VMOVAPS,
	X86_INS_XORPD,
	X86_INS_XORPS,
	X86_INS_GETSEC,
	X86_INS_HADDPD,
	X86_INS_HADDPS,
	X86_INS_HLT,
	X86_INS_HSUBPD,
	X86_INS_HSUBPS,
	X86_INS_IDIV,
	X86_INS_FILD,
	X86_INS_IMUL,
	X86_INS_IN,
	X86_INS_INC,
	X86_INS_INSB,
	X86_INS_INSERTPS,
	X86_INS_INSERTQ,
	X86_INS_INSD,
	X86_INS_INSW,
	X86_INS_INT,
	X86_INS_INT1,
	X86_INS_INT3,
	X86_INS_INTO,
	X86_INS_INVD,
	X86_INS_INVEPT,
	X86_INS_INVLPG,
	X86_INS_INVLPGA,
	X86_INS_INVPCID,
	X86_INS_INVVPID,
	X86_INS_IRET,
	X86_INS_IRETD,
	X86_INS_IRETQ,
	X86_INS_FISTTP,
	X86_INS_FIST,
	X86_INS_FISTP,
	X86_INS_UCOMISD,
	X86_INS_UCOMISS,
	X86_INS_VCOMISD,
	X86_INS_VCOMISS,
	X86_INS_VCVTSD2SS,
	X86_INS_VCVTSI2SD,
	X86_INS_VCVTSI2SS,
	X86_INS_VCVTSS2SD,
	X86_INS_VCVTTSD2SI,
	X86_INS_VCVTTSD2USI,
	X86_INS_VCVTTSS2SI,
	X86_INS_VCVTTSS2USI,
	X86_INS_VCVTUSI2SD,
	X86_INS_VCVTUSI2SS,
	X86_INS_VUCOMISD,
	X86_INS_VUCOMISS,
	X86_INS_JAE,
	X86_INS_JA,
	X86_INS_JBE,
	X86_INS_JB,
	X86_INS_JCXZ,
	X86_INS_JECXZ,
	X86_INS_JE,
	X86_INS_JGE,
	X86_INS_JG,
	X86_INS_JLE,
	X86_INS_JL,
	X86_INS_JMP,
	X86_INS_JNE,
	X86_INS_JNO,
	X86_INS_JNP,
	X86_INS_JNS,
	X86_INS_JO,
	X86_INS_JP,
	X86_INS_JRCXZ,
	X86_INS_JS,
	X86_INS_KANDB,
	X86_INS_KANDD,
	X86_INS_KANDNB,
	X86_INS_KANDND,
	X86_INS_KANDNQ,
	X86_INS_KANDNW,
	X86_INS_KANDQ,
	X86_INS_KANDW,
	X86_INS_KMOVB,
	X86_INS_KMOVD,
	X86_INS_KMOVQ,
	X86_INS_KMOVW,
	X86_INS_KNOTB,
	X86_INS_KNOTD,
	X86_INS_KNOTQ,
	X86_INS_KNOTW,
	X86_INS_KORB,
	X86_INS_KORD,
	X86_INS_KORQ,
	X86_INS_KORTESTB,
	X86_INS_KORTESTD,
	X86_INS_KORTESTQ,
	X86_INS_KORTESTW,
	X86_INS_KORW,
	X86_INS_KSHIFTLB,
	X86_INS_KSHIFTLD,
	X86_INS_KSHIFTLQ,
	X86_INS_KSHIFTLW,
	X86_INS_KSHIFTRB,
	X86_INS_KSHIFTRD,
	X86_INS_KSHIFTRQ,
	X86_INS_KSHIFTRW,
	X86_INS_KUNPCKBW,
	X86_INS_KXNORB,
	X86_INS_KXNORD,
	X86_INS_KXNORQ,
	X86_INS_KXNORW,
	X86_INS_KXORB,
	X86_INS_KXORD,
	X86_INS_KXORQ,
	X86_INS_KXORW,
	X86_INS_LAHF,
	X86_INS_LAR,
	X86_INS_LDDQU,
	X86_INS_LDMXCSR,
	X86_INS_LDS,
	X86_INS_FLDZ,
	X86_INS_FLD1,
	X86_INS_FLD,
	X86_INS_LEA,
	X86_INS_LEAVE,
	X86_INS_LES,
	X86_INS_LFENCE,
	X86_INS_LFS,
	X86_INS_LGDT,
	X86_INS_LGS,
	X86_INS_LIDT,
	X86_INS_LLDT,
	X86_INS_LMSW,
	X86_INS_OR,
	X86_INS_SUB,
	X86_INS_XOR,
	X86_INS_LODSB,
	X86_INS_LODSD,
	X86_INS_LODSQ,
	X86_INS_LODSW,
	X86_INS_LOOP,
	X86_INS_LOOPE,
	X86_INS_LOOPNE,
	X86_INS_RETF,
	X86_INS_RETFQ,
	X86_INS_LSL,
	X86_INS_LSS,
	X86_INS_LTR,
	X86_INS_XADD,
	X86_INS_LZCNT,
	X86_INS_MASKMOVDQU,
	X86_INS_MAXPD,
	X86_INS_MAXPS,
	X86_INS_MAXSD,
	X86_INS_MAXSS,
	X86_INS_MFENCE,
	X86_INS_MINPD,
	X86_INS_MINPS,
	X86_INS_MINSD,
	X86_INS_MINSS,
	X86_INS_CVTPD2PI,
	X86_INS_CVTPI2PD,
	X86_INS_CVTPI2PS,
	X86_INS_CVTPS2PI,
	X86_INS_CVTTPD2PI,
	X86_INS_CVTTPS2PI,
	X86_INS_EMMS,
	X86_INS_MASKMOVQ,
	X86_INS_MOVD,
	X86_INS_MOVDQ2Q,
	X86_INS_MOVNTQ,
	X86_INS_MOVQ2DQ,
	X86_INS_MOVQ,
	X86_INS_PABSB,
	X86_INS_PABSD,
	X86_INS_PABSW,
	X86_INS_PACKSSDW,
	X86_INS_PACKSSWB,
	X86_INS_PACKUSWB,
	X86_INS_PADDB,
	X86_INS_PADDD,
	X86_INS_PADDQ,
	X86_INS_PADDSB,
	X86_INS_PADDSW,
	X86_INS_PADDUSB,
	X86_INS_PADDUSW,
	X86_INS_PADDW,
	X86_INS_PALIGNR,
	X86_INS_PANDN,
	X86_INS_PAND,
	X86_INS_PAVGB,
	X86_INS_PAVGW,
	X86_INS_PCMPEQB,
	X86_INS_PCMPEQD,
	X86_INS_PCMPEQW,
	X86_INS_PCMPGTB,
	X86_INS_PCMPGTD,
	X86_INS_PCMPGTW,
	X86_INS_PEXTRW,
	X86_INS_PHADDSW,
	X86_INS_PHADDW,
	X86_INS_PHADDD,
	X86_INS_PHSUBD,
	X86_INS_PHSUBSW,
	X86_INS_PHSUBW,
	X86_INS_PINSRW,
	X86_INS_PMADDUBSW,
	X86_INS_PMADDWD,
	X86_INS_PMAXSW,
	X86_INS_PMAXUB,
	X86_INS_PMINSW,
	X86_INS_PMINUB,
	X86_INS_PMOVMSKB,
	X86_INS_PMULHRSW,
	X86_INS_PMULHUW,
	X86_INS_PMULHW,
	X86_INS_PMULLW,
	X86_INS_PMULUDQ,
	X86_INS_POR,
	X86_INS_PSADBW,
	X86_INS_PSHUFB,
	X86_INS_PSHUFW,
	X86_INS_PSIGNB,
	X86_INS_PSIGND,
	X86_INS_PSIGNW,
	X86_INS_PSLLD,
	X86_INS_PSLLQ,
	X86_INS_PSLLW,
	X86_INS_PSRAD,
	X86_INS_PSRAW,
	X86_INS_PSRLD,
	X86_INS_PSRLQ,
	X86_INS_PSRLW,
	X86_INS_PSUBB,
	X86_INS_PSUBD,
	X86_INS_PSUBQ,
	X86_INS_PSUBSB,
	X86_INS_PSUBSW,
	X86_INS_PSUBUSB,
	X86_INS_PSUBUSW,
	X86_INS_PSUBW,
	X86_INS_PUNPCKHBW,
	X86_INS_PUNPCKHDQ,
	X86_INS_PUNPCKHWD,
	X86_INS_PUNPCKLBW,
	X86_INS_PUNPCKLDQ,
	X86_INS_PUNPCKLWD,
	X86_INS_PXOR,
	X86_INS_MONITOR,
	X86_INS_MONTMUL,
	X86_INS_MOV,
	X86_INS_MOVABS,
	X86_INS_MOVBE,
	X86_INS_MOVDDUP,
	X86_INS_MOVDQA,
	X86_INS_MOVDQU,
	X86_INS_MOVHLPS,
	X86_INS_MOVHPD,
	X86_INS_MOVHPS,
	X86_INS_MOVLHPS,
	X86_INS_MOVLPD,
	X86_INS_MOVLPS,
	X86_INS_MOVMSKPD,
	X86_INS_MOVMSKPS,
	X86_INS_MOVNTDQA,
	X86_INS_MOVNTDQ,
	X86_INS_MOVNTI,
	X86_INS_MOVNTPD,
	X86_INS_MOVNTPS,
	X86_INS_MOVNTSD,
	X86_INS_MOVNTSS,
	X86_INS_MOVSB,
	X86_INS_MOVSD,
	X86_INS_MOVSHDUP,
	X86_INS_MOVSLDUP,
	X86_INS_MOVSQ,
	X86_INS_MOVSS,
	X86_INS_MOVSW,
	X86_INS_MOVSX,
	X86_INS_MOVSXD,
	X86_INS_MOVUPD,
	X86_INS_MOVUPS,
	X86_INS_MOVZX,
	X86_INS_MPSADBW,
	X86_INS_MUL,
	X86_INS_MULPD,
	X86_INS_MULPS,
	X86_INS_MULSD,
	X86_INS_MULSS,
	X86_INS_MULX,
	X86_INS_FMUL,
	X86_INS_FIMUL,
	X86_INS_FMULP,
	X86_INS_MWAIT,
	X86_INS_NEG,
	X86_INS_NOP,
	X86_INS_NOT,
	X86_INS_OUT,
	X86_INS_OUTSB,
	X86_INS_OUTSD,
	X86_INS_OUTSW,
	X86_INS_PACKUSDW,
	X86_INS_PAUSE,
	X86_INS_PAVGUSB,
	X86_INS_PBLENDVB,
	X86_INS_PBLENDW,
	X86_INS_PCLMULQDQ,
	X86_INS_PCMPEQQ,
	X86_INS_PCMPESTRI,
	X86_INS_PCMPESTRM,
	X86_INS_PCMPGTQ,
	X86_INS_PCMPISTRI,
	X86_INS_PCMPISTRM,
	X86_INS_PCOMMIT,
	X86_INS_PDEP,
	X86_INS_PEXT,
	X86_INS_PEXTRB,
	X86_INS_PEXTRD,
	X86_INS_PEXTRQ,
	X86_INS_PF2ID,
	X86_INS_PF2IW,
	X86_INS_PFACC,
	X86_INS_PFADD,
	X86_INS_PFCMPEQ,
	X86_INS_PFCMPGE,
	X86_INS_PFCMPGT,
	X86_INS_PFMAX,
	X86_INS_PFMIN,
	X86_INS_PFMUL,
	X86_INS_PFNACC,
	X86_INS_PFPNACC,
	X86_INS_PFRCPIT1,
	X86_INS_PFRCPIT2,
	X86_INS_PFRCP,
	X86_INS_PFRSQIT1,
	X86_INS_PFRSQRT,
	X86_INS_PFSUBR,
	X86_INS_PFSUB,
	X86_INS_PHMINPOSUW,
	X86_INS_PI2FD,
	X86_INS_PI2FW,
	X86_INS_PINSRB,
	X86_INS_PINSRD,
	X86_INS_PINSRQ,
	X86_INS_PMAXSB,
	X86_INS_PMAXSD,
	X86_INS_PMAXUD,
	X86_INS_PMAXUW,
	X86_INS_PMINSB,
	X86_INS_PMINSD,
	X86_INS_PMINUD,
	X86_INS_PMINUW,
	X86_INS_PMOVSXBD,
	X86_INS_PMOVSXBQ,
	X86_INS_PMOVSXBW,
	X86_INS_PMOVSXDQ,
	X86_INS_PMOVSXWD,
	X86_INS_PMOVSXWQ,
	X86_INS_PMOVZXBD,
	X86_INS_PMOVZXBQ,
	X86_INS_PMOVZXBW,
	X86_INS_PMOVZXDQ,
	X86_INS_PMOVZXWD,
	X86_INS_PMOVZXWQ,
	X86_INS_PMULDQ,
	X86_INS_PMULHRW,
	X86_INS_PMULLD,
	X86_INS_POP,
	X86_INS_POPAW,
	X86_INS_POPAL,
	X86_INS_POPCNT,
	X86_INS_POPF,
	X86_INS_POPFD,
	X86_INS_POPFQ,
	X86_INS_PREFETCH,
	X86_INS_PREFETCHNTA,
	X86_INS_PREFETCHT0,
	X86_INS_PREFETCHT1,
	X86_INS_PREFETCHT2,
	X86_INS_PREFETCHW,
	X86_INS_PSHUFD,
	X86_INS_PSHUFHW,
	X86_INS_PSHUFLW,
	X86_INS_PSLLDQ,
	X86_INS_PSRLDQ,
	X86_INS_PSWAPD,
	X86_INS_PTEST,
	X86_INS_PUNPCKHQDQ,
	X86_INS_PUNPCKLQDQ,
	X86_INS_PUSH,
	X86_INS_PUSHAW,
	X86_INS_PUSHAL,
	X86_INS_PUSHF,
	X86_INS_PUSHFD,
	X86_INS_PUSHFQ,
	X86_INS_RCL,
	X86_INS_RCPPS,
	X86_INS_RCPSS,
	X86_INS_RCR,
	X86_INS_RDFSBASE,
	X86_INS_RDGSBASE,
	X86_INS_RDMSR,
	X86_INS_RDPMC,
	X86_INS_RDRAND,
	X86_INS_RDSEED,
	X86_INS_RDTSC,
	X86_INS_RDTSCP,
	X86_INS_ROL,
	X86_INS_ROR,
	X86_INS_RORX,
	X86_INS_ROUNDPD,
	X86_INS_ROUNDPS,
	X86_INS_ROUNDSD,
	X86_INS_ROUNDSS,
	X86_INS_RSM,
	X86_INS_RSQRTPS,
	X86_INS_RSQRTSS,
	X86_INS_SAHF,
	X86_INS_SAL,
	X86_INS_SALC,
	X86_INS_SAR,
	X86_INS_SARX,
	X86_INS_SBB,
	X86_INS_SCASB,
	X86_INS_SCASD,
	X86_INS_SCASQ,
	X86_INS_SCASW,
	X86_INS_SETAE,
	X86_INS_SETA,
	X86_INS_SETBE,
	X86_INS_SETB,
	X86_INS_SETE,
	X86_INS_SETGE,
	X86_INS_SETG,
	X86_INS_SETLE,
	X86_INS_SETL,
	X86_INS_SETNE,
	X86_INS_SETNO,
	X86_INS_SETNP,
	X86_INS_SETNS,
	X86_INS_SETO,
	X86_INS_SETP,
	X86_INS_SETS,
	X86_INS_SFENCE,
	X86_INS_SGDT,
	X86_INS_SHA1MSG1,
	X86_INS_SHA1MSG2,
	X86_INS_SHA1NEXTE,
	X86_INS_SHA1RNDS4,
	X86_INS_SHA256MSG1,
	X86_INS_SHA256MSG2,
	X86_INS_SHA256RNDS2,
	X86_INS_SHL,
	X86_INS_SHLD,
	X86_INS_SHLX,
	X86_INS_SHR,
	X86_INS_SHRD,
	X86_INS_SHRX,
	X86_INS_SHUFPD,
	X86_INS_SHUFPS,
	X86_INS_SIDT,
	X86_INS_FSIN,
	X86_INS_SKINIT,
	X86_INS_SLDT,
	X86_INS_SMSW,
	X86_INS_SQRTPD,
	X86_INS_SQRTPS,
	X86_INS_SQRTSD,
	X86_INS_SQRTSS,
	X86_INS_FSQRT,
	X86_INS_STAC,
	X86_INS_STC,
	X86_INS_STD,
	X86_INS_STGI,
	X86_INS_STI,
	X86_INS_STMXCSR,
	X86_INS_STOSB,
	X86_INS_STOSD,
	X86_INS_STOSQ,
	X86_INS_STOSW,
	X86_INS_STR,
	X86_INS_FST,
	X86_INS_FSTP,
	X86_INS_FSTPNCE,
	X86_INS_FXCH,
	X86_INS_SUBPD,
	X86_INS_SUBPS,
	X86_INS_FSUBR,
	X86_INS_FISUBR,
	X86_INS_FSUBRP,
	X86_INS_SUBSD,
	X86_INS_SUBSS,
	X86_INS_FSUB,
	X86_INS_FISUB,
	X86_INS_FSUBP,
	X86_INS_SWAPGS,
	X86_INS_SYSCALL,
	X86_INS_SYSENTER,
	X86_INS_SYSEXIT,
	X86_INS_SYSRET,
	X86_INS_T1MSKC,
	X86_INS_TEST,
	X86_INS_UD2,
	X86_INS_FTST,
	X86_INS_TZCNT,
	X86_INS_TZMSK,
	X86_INS_FUCOMIP,
	X86_INS_FUCOMI,
	X86_INS_FUCOMPP,
	X86_INS_FUCOMP,
	X86_INS_FUCOM,
	X86_INS_UD2B,
	X86_INS_UNPCKHPD,
	X86_INS_UNPCKHPS,
	X86_INS_UNPCKLPD,
	X86_INS_UNPCKLPS,
	X86_INS_VADDPD,
	X86_INS_VADDPS,
	X86_INS_VADDSD,
	X86_INS_VADDSS,
	X86_INS_VADDSUBPD,
	X86_INS_VADDSUBPS,
	X86_INS_VAESDECLAST,
	X86_INS_VAESDEC,
	X86_INS_VAESENCLAST,
	X86_INS_VAESENC,
	X86_INS_VAESIMC,
	X86_INS_VAESKEYGENASSIST,
	X86_INS_VALIGND,
	X86_INS_VALIGNQ,
	X86_INS_VANDNPD,
	X86_INS_VANDNPS,
	X86_INS_VANDPD,
	X86_INS_VANDPS,
	X86_INS_VBLENDMPD,
	X86_INS_VBLENDMPS,
	X86_INS_VBLENDPD,
	X86_INS_VBLENDPS,
	X86_INS_VBLENDVPD,
	X86_INS_VBLENDVPS,
	X86_INS_VBROADCASTF128,
	X86_INS_VBROADCASTI32X4,
	X86_INS_VBROADCASTI64X4,
	X86_INS_VBROADCASTSD,
	X86_INS_VBROADCASTSS,
	X86_INS_VCOMPRESSPD,
	X86_INS_VCOMPRESSPS,
	X86_INS_VCVTDQ2PD,
	X86_INS_VCVTDQ2PS,
	X86_INS_VCVTPD2DQX,
	X86_INS_VCVTPD2DQ,
	X86_INS_VCVTPD2PSX,
	X86_INS_VCVTPD2PS,
	X86_INS_VCVTPD2UDQ,
	X86_INS_VCVTPH2PS,
	X86_INS_VCVTPS2DQ,
	X86_INS_VCVTPS2PD,
	X86_INS_VCVTPS2PH,
	X86_INS_VCVTPS2UDQ,
	X86_INS_VCVTSD2SI,
	X86_INS_VCVTSD2USI,
	X86_INS_VCVTSS2SI,
	X86_INS_VCVTSS2USI,
	X86_INS_VCVTTPD2DQX,
	X86_INS_VCVTTPD2DQ,
	X86_INS_VCVTTPD2UDQ,
	X86_INS_VCVTTPS2DQ,
	X86_INS_VCVTTPS2UDQ,
	X86_INS_VCVTUDQ2PD,
	X86_INS_VCVTUDQ2PS,
	X86_INS_VDIVPD,
	X86_INS_VDIVPS,
	X86_INS_VDIVSD,
	X86_INS_VDIVSS,
	X86_INS_VDPPD,
	X86_INS_VDPPS,
	X86_INS_VERR,
	X86_INS_VERW,
	X86_INS_VEXP2PD,
	X86_INS_VEXP2PS,
	X86_INS_VEXPANDPD,
	X86_INS_VEXPANDPS,
	X86_INS_VEXTRACTF128,
	X86_INS_VEXTRACTF32X4,
	X86_INS_VEXTRACTF64X4,
	X86_INS_VEXTRACTI128,
	X86_INS_VEXTRACTI32X4,
	X86_INS_VEXTRACTI64X4,
	X86_INS_VEXTRACTPS,
	X86_INS_VFMADD132PD,
	X86_INS_VFMADD132PS,
	X86_INS_VFMADDPD,
	X86_INS_VFMADD213PD,
	X86_INS_VFMADD231PD,
	X86_INS_VFMADDPS,
	X86_INS_VFMADD213PS,
	X86_INS_VFMADD231PS,
	X86_INS_VFMADDSD,
	X86_INS_VFMADD213SD,
	X86_INS_VFMADD132SD,
	X86_INS_VFMADD231SD,
	X86_INS_VFMADDSS,
	X86_INS_VFMADD213SS,
	X86_INS_VFMADD132SS,
	X86_INS_VFMADD231SS,
	X86_INS_VFMADDSUB132PD,
	X86_INS_VFMADDSUB132PS,
	X86_INS_VFMADDSUBPD,
	X86_INS_VFMADDSUB213PD,
	X86_INS_VFMADDSUB231PD,
	X86_INS_VFMADDSUBPS,
	X86_INS_VFMADDSUB213PS,
	X86_INS_VFMADDSUB231PS,
	X86_INS_VFMSUB132PD,
	X86_INS_VFMSUB132PS,
	X86_INS_VFMSUBADD132PD,
	X86_INS_VFMSUBADD132PS,
	X86_INS_VFMSUBADDPD,
	X86_INS_VFMSUBADD213PD,
	X86_INS_VFMSUBADD231PD,
	X86_INS_VFMSUBADDPS,
	X86_INS_VFMSUBADD213PS,
	X86_INS_VFMSUBADD231PS,
	X86_INS_VFMSUBPD,
	X86_INS_VFMSUB213PD,
	X86_INS_VFMSUB231PD,
	X86_INS_VFMSUBPS,
	X86_INS_VFMSUB213PS,
	X86_INS_VFMSUB231PS,
	X86_INS_VFMSUBSD,
	X86_INS_VFMSUB213SD,
	X86_INS_VFMSUB132SD,
	X86_INS_VFMSUB231SD,
	X86_INS_VFMSUBSS,
	X86_INS_VFMSUB213SS,
	X86_INS_VFMSUB132SS,
	X86_INS_VFMSUB231SS,
	X86_INS_VFNMADD132PD,
	X86_INS_VFNMADD132PS,
	X86_INS_VFNMADDPD,
	X86_INS_VFNMADD213PD,
	X86_INS_VFNMADD231PD,
	X86_INS_VFNMADDPS,
	X86_INS_VFNMADD213PS,
	X86_INS_VFNMADD231PS,
	X86_INS_VFNMADDSD,
	X86_INS_VFNMADD213SD,
	X86_INS_VFNMADD132SD,
	X86_INS_VFNMADD231SD,
	X86_INS_VFNMADDSS,
	X86_INS_VFNMADD213SS,
	X86_INS_VFNMADD132SS,
	X86_INS_VFNMADD231SS,
	X86_INS_VFNMSUB132PD,
	X86_INS_VFNMSUB132PS,
	X86_INS_VFNMSUBPD,
	X86_INS_VFNMSUB213PD,
	X86_INS_VFNMSUB231PD,
	X86_INS_VFNMSUBPS,
	X86_INS_VFNMSUB213PS,
	X86_INS_VFNMSUB231PS,
	X86_INS_VFNMSUBSD,
	X86_INS_VFNMSUB213SD,
	X86_INS_VFNMSUB132SD,
	X86_INS_VFNMSUB231SD,
	X86_INS_VFNMSUBSS,
	X86_INS_VFNMSUB213SS,
	X86_INS_VFNMSUB132SS,
	X86_INS_VFNMSUB231SS,
	X86_INS_VFRCZPD,
	X86_INS_VFRCZPS,
	X86_INS_VFRCZSD,
	X86_INS_VFRCZSS,
	X86_INS_VORPD,
	X86_INS_VORPS,
	X86_INS_VXORPD,
	X86_INS_VXORPS,
	X86_INS_VGATHERDPD,
	X86_INS_VGATHERDPS,
	X86_INS_VGATHERPF0DPD,
	X86_INS_VGATHERPF0DPS,
	X86_INS_VGATHERPF0QPD,
	X86_INS_VGATHERPF0QPS,
	X86_INS_VGATHERPF1DPD,
	X86_INS_VGATHERPF1DPS,
	X86_INS_VGATHERPF1QPD,
	X86_INS_VGATHERPF1QPS,
	X86_INS_VGATHERQPD,
	X86_INS_VGATHERQPS,
	X86_INS_VHADDPD,
	X86_INS_VHADDPS,
	X86_INS_VHSUBPD,
	X86_INS_VHSUBPS,
	X86_INS_VINSERTF128,
	X86_INS_VINSERTF32X4,
	X86_INS_VINSERTF32X8,
	X86_INS_VINSERTF64X2,
	X86_INS_VINSERTF64X4,
	X86_INS_VINSERTI128,
	X86_INS_VINSERTI32X4,
	X86_INS_VINSERTI32X8,
	X86_INS_VINSERTI64X2,
	X86_INS_VINSERTI64X4,
	X86_INS_VINSERTPS,
	X86_INS_VLDDQU,
	X86_INS_VLDMXCSR,
	X86_INS_VMASKMOVDQU,
	X86_INS_VMASKMOVPD,
	X86_INS_VMASKMOVPS,
	X86_INS_VMAXPD,
	X86_INS_VMAXPS,
	X86_INS_VMAXSD,
	X86_INS_VMAXSS,
	X86_INS_VMCALL,
	X86_INS_VMCLEAR,
	X86_INS_VMFUNC,
	X86_INS_VMINPD,
	X86_INS_VMINPS,
	X86_INS_VMINSD,
	X86_INS_VMINSS,
	X86_INS_VMLAUNCH,
	X86_INS_VMLOAD,
	X86_INS_VMMCALL,
	X86_INS_VMOVQ,
	X86_INS_VMOVDDUP,
	X86_INS_VMOVD,
	X86_INS_VMOVDQA32,
	X86_INS_VMOVDQA64,
	X86_INS_VMOVDQA,
	X86_INS_VMOVDQU16,
	X86_INS_VMOVDQU32,
	X86_INS_VMOVDQU64,
	X86_INS_VMOVDQU8,
	X86_INS_VMOVDQU,
	X86_INS_VMOVHLPS,
	X86_INS_VMOVHPD,
	X86_INS_VMOVHPS,
	X86_INS_VMOVLHPS,
	X86_INS_VMOVLPD,
	X86_INS_VMOVLPS,
	X86_INS_VMOVMSKPD,
	X86_INS_VMOVMSKPS,
	X86_INS_VMOVNTDQA,
	X86_INS_VMOVNTDQ,
	X86_INS_VMOVNTPD,
	X86_INS_VMOVNTPS,
	X86_INS_VMOVSD,
	X86_INS_VMOVSHDUP,
	X86_INS_VMOVSLDUP,
	X86_INS_VMOVSS,
	X86_INS_VMOVUPD,
	X86_INS_VMOVUPS,
	X86_INS_VMPSADBW,
	X86_INS_VMPTRLD,
	X86_INS_VMPTRST,
	X86_INS_VMREAD,
	X86_INS_VMRESUME,
	X86_INS_VMRUN,
	X86_INS_VMSAVE,
	X86_INS_VMULPD,
	X86_INS_VMULPS,
	X86_INS_VMULSD,
	X86_INS_VMULSS,
	X86_INS_VMWRITE,
	X86_INS_VMXOFF,
	X86_INS_VMXON,
	X86_INS_VPABSB,
	X86_INS_VPABSD,
	X86_INS_VPABSQ,
	X86_INS_VPABSW,
	X86_INS_VPACKSSDW,
	X86_INS_VPACKSSWB,
	X86_INS_VPACKUSDW,
	X86_INS_VPACKUSWB,
	X86_INS_VPADDB,
	X86_INS_VPADDD,
	X86_INS_VPADDQ,
	X86_INS_VPADDSB,
	X86_INS_VPADDSW,
	X86_INS_VPADDUSB,
	X86_INS_VPADDUSW,
	X86_INS_VPADDW,
	X86_INS_VPALIGNR,
	X86_INS_VPANDD,
	X86_INS_VPANDND,
	X86_INS_VPANDNQ,
	X86_INS_VPANDN,
	X86_INS_VPANDQ,
	X86_INS_VPAND,
	X86_INS_VPAVGB,
	X86_INS_VPAVGW,
	X86_INS_VPBLENDD,
	X86_INS_VPBLENDMB,
	X86_INS_VPBLENDMD,
	X86_INS_VPBLENDMQ,
	X86_INS_VPBLENDMW,
	X86_INS_VPBLENDVB,
	X86_INS_VPBLENDW,
	X86_INS_VPBROADCASTB,
	X86_INS_VPBROADCASTD,
	X86_INS_VPBROADCASTMB2Q,
	X86_INS_VPBROADCASTMW2D,
	X86_INS_VPBROADCASTQ,
	X86_INS_VPBROADCASTW,
	X86_INS_VPCLMULQDQ,
	X86_INS_VPCMOV,
	X86_INS_VPCMPB,
	X86_INS_VPCMPD,
	X86_INS_VPCMPEQB,
	X86_INS_VPCMPEQD,
	X86_INS_VPCMPEQQ,
	X86_INS_VPCMPEQW,
	X86_INS_VPCMPESTRI,
	X86_INS_VPCMPESTRM,
	X86_INS_VPCMPGTB,
	X86_INS_VPCMPGTD,
	X86_INS_VPCMPGTQ,
	X86_INS_VPCMPGTW,
	X86_INS_VPCMPISTRI,
	X86_INS_VPCMPISTRM,
	X86_INS_VPCMPQ,
	X86_INS_VPCMPUB,
	X86_INS_VPCMPUD,
	X86_INS_VPCMPUQ,
	X86_INS_VPCMPUW,
	X86_INS_VPCMPW,
	X86_INS_VPCOMB,
	X86_INS_VPCOMD,
	X86_INS_VPCOMPRESSD,
	X86_INS_VPCOMPRESSQ,
	X86_INS_VPCOMQ,
	X86_INS_VPCOMUB,
	X86_INS_VPCOMUD,
	X86_INS_VPCOMUQ,
	X86_INS_VPCOMUW,
	X86_INS_VPCOMW,
	X86_INS_VPCONFLICTD,
	X86_INS_VPCONFLICTQ,
	X86_INS_VPERM2F128,
	X86_INS_VPERM2I128,
	X86_INS_VPERMD,
	X86_INS_VPERMI2D,
	X86_INS_VPERMI2PD,
	X86_INS_VPERMI2PS,
	X86_INS_VPERMI2Q,
	X86_INS_VPERMIL2PD,
	X86_INS_VPERMIL2PS,
	X86_INS_VPERMILPD,
	X86_INS_VPERMILPS,
	X86_INS_VPERMPD,
	X86_INS_VPERMPS,
	X86_INS_VPERMQ,
	X86_INS_VPERMT2D,
	X86_INS_VPERMT2PD,
	X86_INS_VPERMT2PS,
	X86_INS_VPERMT2Q,
	X86_INS_VPEXPANDD,
	X86_INS_VPEXPANDQ,
	X86_INS_VPEXTRB,
	X86_INS_VPEXTRD,
	X86_INS_VPEXTRQ,
	X86_INS_VPEXTRW,
	X86_INS_VPGATHERDD,
	X86_INS_VPGATHERDQ,
	X86_INS_VPGATHERQD,
	X86_INS_VPGATHERQQ,
	X86_INS_VPHADDBD,
	X86_INS_VPHADDBQ,
	X86_INS_VPHADDBW,
	X86_INS_VPHADDDQ,
	X86_INS_VPHADDD,
	X86_INS_VPHADDSW,
	X86_INS_VPHADDUBD,
	X86_INS_VPHADDUBQ,
	X86_INS_VPHADDUBW,
	X86_INS_VPHADDUDQ,
	X86_INS_VPHADDUWD,
	X86_INS_VPHADDUWQ,
	X86_INS_VPHADDWD,
	X86_INS_VPHADDWQ,
	X86_INS_VPHADDW,
	X86_INS_VPHMINPOSUW,
	X86_INS_VPHSUBBW,
	X86_INS_VPHSUBDQ,
	X86_INS_VPHSUBD,
	X86_INS_VPHSUBSW,
	X86_INS_VPHSUBWD,
	X86_INS_VPHSUBW,
	X86_INS_VPINSRB,
	X86_INS_VPINSRD,
	X86_INS_VPINSRQ,
	X86_INS_VPINSRW,
	X86_INS_VPLZCNTD,
	X86_INS_VPLZCNTQ,
	X86_INS_VPMACSDD,
	X86_INS_VPMACSDQH,
	X86_INS_VPMACSDQL,
	X86_INS_VPMACSSDD,
	X86_INS_VPMACSSDQH,
	X86_INS_VPMACSSDQL,
	X86_INS_VPMACSSWD,
	X86_INS_VPMACSSWW,
	X86_INS_VPMACSWD,
	X86_INS_VPMACSWW,
	X86_INS_VPMADCSSWD,
	X86_INS_VPMADCSWD,
	X86_INS_VPMADDUBSW,
	X86_INS_VPMADDWD,
	X86_INS_VPMASKMOVD,
	X86_INS_VPMASKMOVQ,
	X86_INS_VPMAXSB,
	X86_INS_VPMAXSD,
	X86_INS_VPMAXSQ,
	X86_INS_VPMAXSW,
	X86_INS_VPMAXUB,
	X86_INS_VPMAXUD,
	X86_INS_VPMAXUQ,
	X86_INS_VPMAXUW,
	X86_INS_VPMINSB,
	X86_INS_VPMINSD,
	X86_INS_VPMINSQ,
	X86_INS_VPMINSW,
	X86_INS_VPMINUB,
	X86_INS_VPMINUD,
	X86_INS_VPMINUQ,
	X86_INS_VPMINUW,
	X86_INS_VPMOVDB,
	X86_INS_VPMOVDW,
	X86_INS_VPMOVM2B,
	X86_INS_VPMOVM2D,
	X86_INS_VPMOVM2Q,
	X86_INS_VPMOVM2W,
	X86_INS_VPMOVMSKB,
	X86_INS_VPMOVQB,
	X86_INS_VPMOVQD,
	X86_INS_VPMOVQW,
	X86_INS_VPMOVSDB,
	X86_INS_VPMOVSDW,
	X86_INS_VPMOVSQB,
	X86_INS_VPMOVSQD,
	X86_INS_VPMOVSQW,
	X86_INS_VPMOVSXBD,
	X86_INS_VPMOVSXBQ,
	X86_INS_VPMOVSXBW,
	X86_INS_VPMOVSXDQ,
	X86_INS_VPMOVSXWD,
	X86_INS_VPMOVSXWQ,
	X86_INS_VPMOVUSDB,
	X86_INS_VPMOVUSDW,
	X86_INS_VPMOVUSQB,
	X86_INS_VPMOVUSQD,
	X86_INS_VPMOVUSQW,
	X86_INS_VPMOVZXBD,
	X86_INS_VPMOVZXBQ,
	X86_INS_VPMOVZXBW,
	X86_INS_VPMOVZXDQ,
	X86_INS_VPMOVZXWD,
	X86_INS_VPMOVZXWQ,
	X86_INS_VPMULDQ,
	X86_INS_VPMULHRSW,
	X86_INS_VPMULHUW,
	X86_INS_VPMULHW,
	X86_INS_VPMULLD,
	X86_INS_VPMULLQ,
	X86_INS_VPMULLW,
	X86_INS_VPMULUDQ,
	X86_INS_VPORD,
	X86_INS_VPORQ,
	X86_INS_VPOR,
	X86_INS_VPPERM,
	X86_INS_VPROTB,
	X86_INS_VPROTD,
	X86_INS_VPROTQ,
	X86_INS_VPROTW,
	X86_INS_VPSADBW,
	X86_INS_VPSCATTERDD,
	X86_INS_VPSCATTERDQ,
	X86_INS_VPSCATTERQD,
	X86_INS_VPSCATTERQQ,
	X86_INS_VPSHAB,
	X86_INS_VPSHAD,
	X86_INS_VPSHAQ,
	X86_INS_VPSHAW,
	X86_INS_VPSHLB,
	X86_INS_VPSHLD,
	X86_INS_VPSHLQ,
	X86_INS_VPSHLW,
	X86_INS_VPSHUFB,
	X86_INS_VPSHUFD,
	X86_INS_VPSHUFHW,
	X86_INS_VPSHUFLW,
	X86_INS_VPSIGNB,
	X86_INS_VPSIGND,
	X86_INS_VPSIGNW,
	X86_INS_VPSLLDQ,
	X86_INS_VPSLLD,
	X86_INS_VPSLLQ,
	X86_INS_VPSLLVD,
	X86_INS_VPSLLVQ,
	X86_INS_VPSLLW,
	X86_INS_VPSRAD,
	X86_INS_VPSRAQ,
	X86_INS_VPSRAVD,
	X86_INS_VPSRAVQ,
	X86_INS_VPSRAW,
	X86_INS_VPSRLDQ,
	X86_INS_VPSRLD,
	X86_INS_VPSRLQ,
	X86_INS_VPSRLVD,
	X86_INS_VPSRLVQ,
	X86_INS_VPSRLW,
	X86_INS_VPSUBB,
	X86_INS_VPSUBD,
	X86_INS_VPSUBQ,
	X86_INS_VPSUBSB,
	X86_INS_VPSUBSW,
	X86_INS_VPSUBUSB,
	X86_INS_VPSUBUSW,
	X86_INS_VPSUBW,
	X86_INS_VPTESTMD,
	X86_INS_VPTESTMQ,
	X86_INS_VPTESTNMD,
	X86_INS_VPTESTNMQ,
	X86_INS_VPTEST,
	X86_INS_VPUNPCKHBW,
	X86_INS_VPUNPCKHDQ,
	X86_INS_VPUNPCKHQDQ,
	X86_INS_VPUNPCKHWD,
	X86_INS_VPUNPCKLBW,
	X86_INS_VPUNPCKLDQ,
	X86_INS_VPUNPCKLQDQ,
	X86_INS_VPUNPCKLWD,
	X86_INS_VPXORD,
	X86_INS_VPXORQ,
	X86_INS_VPXOR,
	X86_INS_VRCP14PD,
	X86_INS_VRCP14PS,
	X86_INS_VRCP14SD,
	X86_INS_VRCP14SS,
	X86_INS_VRCP28PD,
	X86_INS_VRCP28PS,
	X86_INS_VRCP28SD,
	X86_INS_VRCP28SS,
	X86_INS_VRCPPS,
	X86_INS_VRCPSS,
	X86_INS_VRNDSCALEPD,
	X86_INS_VRNDSCALEPS,
	X86_INS_VRNDSCALESD,
	X86_INS_VRNDSCALESS,
	X86_INS_VROUNDPD,
	X86_INS_VROUNDPS,
	X86_INS_VROUNDSD,
	X86_INS_VROUNDSS,
	X86_INS_VRSQRT14PD,
	X86_INS_VRSQRT14PS,
	X86_INS_VRSQRT14SD,
	X86_INS_VRSQRT14SS,
	X86_INS_VRSQRT28PD,
	X86_INS_VRSQRT28PS,
	X86_INS_VRSQRT28SD,
	X86_INS_VRSQRT28SS,
	X86_INS_VRSQRTPS,
	X86_INS_VRSQRTSS,
	X86_INS_VSCATTERDPD,
	X86_INS_VSCATTERDPS,
	X86_INS_VSCATTERPF0DPD,
	X86_INS_VSCATTERPF0DPS,
	X86_INS_VSCATTERPF0QPD,
	X86_INS_VSCATTERPF0QPS,
	X86_INS_VSCATTERPF1DPD,
	X86_INS_VSCATTERPF1DPS,
	X86_INS_VSCATTERPF1QPD,
	X86_INS_VSCATTERPF1QPS,
	X86_INS_VSCATTERQPD,
	X86_INS_VSCATTERQPS,
	X86_INS_VSHUFPD,
	X86_INS_VSHUFPS,
	X86_INS_VSQRTPD,
	X86_INS_VSQRTPS,
	X86_INS_VSQRTSD,
	X86_INS_VSQRTSS,
	X86_INS_VSTMXCSR,
	X86_INS_VSUBPD,
	X86_INS_VSUBPS,
	X86_INS_VSUBSD,
	X86_INS_VSUBSS,
	X86_INS_VTESTPD,
	X86_INS_VTESTPS,
	X86_INS_VUNPCKHPD,
	X86_INS_VUNPCKHPS,
	X86_INS_VUNPCKLPD,
	X86_INS_VUNPCKLPS,
	X86_INS_VZEROALL,
	X86_INS_VZEROUPPER,
	X86_INS_WAIT,
	X86_INS_WBINVD,
	X86_INS_WRFSBASE,
	X86_INS_WRGSBASE,
	X86_INS_WRMSR,
	X86_INS_XABORT,
	X86_INS_XACQUIRE,
	X86_INS_XBEGIN,
	X86_INS_XCHG,
	X86_INS_XCRYPTCBC,
	X86_INS_XCRYPTCFB,
	X86_INS_XCRYPTCTR,
	X86_INS_XCRYPTECB,
	X86_INS_XCRYPTOFB,
	X86_INS_XEND,
	X86_INS_XGETBV,
	X86_INS_XLATB,
	X86_INS_XRELEASE,
	X86_INS_XRSTOR,
	X86_INS_XRSTOR64,
	X86_INS_XRSTORS,
	X86_INS_XRSTORS64,
	X86_INS_XSAVE,
	X86_INS_XSAVE64,
	X86_INS_XSAVEC,
	X86_INS_XSAVEC64,
	X86_INS_XSAVEOPT,
	X86_INS_XSAVEOPT64,
	X86_INS_XSAVES,
	X86_INS_XSAVES64,
	X86_INS_XSETBV,
	X86_INS_XSHA1,
	X86_INS_XSHA256,
	X86_INS_XSTORE,
	X86_INS_XTEST,
	X86_INS_FDISI8087_NOP,
	X86_INS_FENI8087_NOP,

	// pseudo instructions
	X86_INS_CMPSS,
	X86_INS_CMPEQSS,
	X86_INS_CMPLTSS,
	X86_INS_CMPLESS,
	X86_INS_CMPUNORDSS,
	X86_INS_CMPNEQSS,
	X86_INS_CMPNLTSS,
	X86_INS_CMPNLESS,
	X86_INS_CMPORDSS,

	X86_INS_CMPSD,
	X86_INS_CMPEQSD,
	X86_INS_CMPLTSD,
	X86_INS_CMPLESD,
	X86_INS_CMPUNORDSD,
	X86_INS_CMPNEQSD,
	X86_INS_CMPNLTSD,
	X86_INS_CMPNLESD,
	X86_INS_CMPORDSD,

	X86_INS_CMPPS,
	X86_INS_CMPEQPS,
	X86_INS_CMPLTPS,
	X86_INS_CMPLEPS,
	X86_INS_CMPUNORDPS,
	X86_INS_CMPNEQPS,
	X86_INS_CMPNLTPS,
	X86_INS_CMPNLEPS,
	X86_INS_CMPORDPS,

	X86_INS_CMPPD,
	X86_INS_CMPEQPD,
	X86_INS_CMPLTPD,
	X86_INS_CMPLEPD,
	X86_INS_CMPUNORDPD,
	X86_INS_CMPNEQPD,
	X86_INS_CMPNLTPD,
	X86_INS_CMPNLEPD,
	X86_INS_CMPORDPD,

	X86_INS_VCMPSS,
	X86_INS_VCMPEQSS,
	X86_INS_VCMPLTSS,
	X86_INS_VCMPLESS,
	X86_INS_VCMPUNORDSS,
	X86_INS_VCMPNEQSS,
	X86_INS_VCMPNLTSS,
	X86_INS_VCMPNLESS,
	X86_INS_VCMPORDSS,
	X86_INS_VCMPEQ_UQSS,
	X86_INS_VCMPNGESS,
	X86_INS_VCMPNGTSS,
	X86_INS_VCMPFALSESS,
	X86_INS_VCMPNEQ_OQSS,
	X86_INS_VCMPGESS,
	X86_INS_VCMPGTSS,
	X86_INS_VCMPTRUESS,
	X86_INS_VCMPEQ_OSSS,
	X86_INS_VCMPLT_OQSS,
	X86_INS_VCMPLE_OQSS,
	X86_INS_VCMPUNORD_SSS,
	X86_INS_VCMPNEQ_USSS,
	X86_INS_VCMPNLT_UQSS,
	X86_INS_VCMPNLE_UQSS,
	X86_INS_VCMPORD_SSS,
	X86_INS_VCMPEQ_USSS,
	X86_INS_VCMPNGE_UQSS,
	X86_INS_VCMPNGT_UQSS,
	X86_INS_VCMPFALSE_OSSS,
	X86_INS_VCMPNEQ_OSSS,
	X86_INS_VCMPGE_OQSS,
	X86_INS_VCMPGT_OQSS,
	X86_INS_VCMPTRUE_USSS,

	X86_INS_VCMPSD,
	X86_INS_VCMPEQSD,
	X86_INS_VCMPLTSD,
	X86_INS_VCMPLESD,
	X86_INS_VCMPUNORDSD,
	X86_INS_VCMPNEQSD,
	X86_INS_VCMPNLTSD,
	X86_INS_VCMPNLESD,
	X86_INS_VCMPORDSD,
	X86_INS_VCMPEQ_UQSD,
	X86_INS_VCMPNGESD,
	X86_INS_VCMPNGTSD,
	X86_INS_VCMPFALSESD,
	X86_INS_VCMPNEQ_OQSD,
	X86_INS_VCMPGESD,
	X86_INS_VCMPGTSD,
	X86_INS_VCMPTRUESD,
	X86_INS_VCMPEQ_OSSD,
	X86_INS_VCMPLT_OQSD,
	X86_INS_VCMPLE_OQSD,
	X86_INS_VCMPUNORD_SSD,
	X86_INS_VCMPNEQ_USSD,
	X86_INS_VCMPNLT_UQSD,
	X86_INS_VCMPNLE_UQSD,
	X86_INS_VCMPORD_SSD,
	X86_INS_VCMPEQ_USSD,
	X86_INS_VCMPNGE_UQSD,
	X86_INS_VCMPNGT_UQSD,
	X86_INS_VCMPFALSE_OSSD,
	X86_INS_VCMPNEQ_OSSD,
	X86_INS_VCMPGE_OQSD,
	X86_INS_VCMPGT_OQSD,
	X86_INS_VCMPTRUE_USSD,

	X86_INS_VCMPPS,
	X86_INS_VCMPEQPS,
	X86_INS_VCMPLTPS,
	X86_INS_VCMPLEPS,
	X86_INS_VCMPUNORDPS,
	X86_INS_VCMPNEQPS,
	X86_INS_VCMPNLTPS,
	X86_INS_VCMPNLEPS,
	X86_INS_VCMPORDPS,
	X86_INS_VCMPEQ_UQPS,
	X86_INS_VCMPNGEPS,
	X86_INS_VCMPNGTPS,
	X86_INS_VCMPFALSEPS,
	X86_INS_VCMPNEQ_OQPS,
	X86_INS_VCMPGEPS,
	X86_INS_VCMPGTPS,
	X86_INS_VCMPTRUEPS,
	X86_INS_VCMPEQ_OSPS,
	X86_INS_VCMPLT_OQPS,
	X86_INS_VCMPLE_OQPS,
	X86_INS_VCMPUNORD_SPS,
	X86_INS_VCMPNEQ_USPS,
	X86_INS_VCMPNLT_UQPS,
	X86_INS_VCMPNLE_UQPS,
	X86_INS_VCMPORD_SPS,
	X86_INS_VCMPEQ_USPS,
	X86_INS_VCMPNGE_UQPS,
	X86_INS_VCMPNGT_UQPS,
	X86_INS_VCMPFALSE_OSPS,
	X86_INS_VCMPNEQ_OSPS,
	X86_INS_VCMPGE_OQPS,
	X86_INS_VCMPGT_OQPS,
	X86_INS_VCMPTRUE_USPS,

	X86_INS_VCMPPD,
	X86_INS_VCMPEQPD,
	X86_INS_VCMPLTPD,
	X86_INS_VCMPLEPD,
	X86_INS_VCMPUNORDPD,
	X86_INS_VCMPNEQPD,
	X86_INS_VCMPNLTPD,
	X86_INS_VCMPNLEPD,
	X86_INS_VCMPORDPD,
	X86_INS_VCMPEQ_UQPD,
	X86_INS_VCMPNGEPD,
	X86_INS_VCMPNGTPD,
	X86_INS_VCMPFALSEPD,
	X86_INS_VCMPNEQ_OQPD,
	X86_INS_VCMPGEPD,
	X86_INS_VCMPGTPD,
	X86_INS_VCMPTRUEPD,
	X86_INS_VCMPEQ_OSPD,
	X86_INS_VCMPLT_OQPD,
	X86_INS_VCMPLE_OQPD,
	X86_INS_VCMPUNORD_SPD,
	X86_INS_VCMPNEQ_USPD,
	X86_INS_VCMPNLT_UQPD,
	X86_INS_VCMPNLE_UQPD,
	X86_INS_VCMPORD_SPD,
	X86_INS_VCMPEQ_USPD,
	X86_INS_VCMPNGE_UQPD,
	X86_INS_VCMPNGT_UQPD,
	X86_INS_VCMPFALSE_OSPD,
	X86_INS_VCMPNEQ_OSPD,
	X86_INS_VCMPGE_OQPD,
	X86_INS_VCMPGT_OQPD,
	X86_INS_VCMPTRUE_USPD,

	X86_INS_UD0,
	X86_INS_ENDBR32,
	X86_INS_ENDBR64,

	X86_INS_ENDING, // mark the end of the list of insn
} x86_insn;

/// Group of X86 instructions
typedef enum  x86_insn_group {
	X86_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	X86_GRP_JUMP,	///< = CS_GRP_JUMP
	// all call instructions
	X86_GRP_CALL,	///< = CS_GRP_CALL
	// all return instructions
	X86_GRP_RET,	///< = CS_GRP_RET
	// all interrupt instructions (int+syscall)
	X86_GRP_INT,	///< = CS_GRP_INT
	// all interrupt return instructions
	X86_GRP_IRET,	///< = CS_GRP_IRET
	// all privileged instructions
	X86_GRP_PRIVILEGE,	///< = CS_GRP_PRIVILEGE
	// all relative branching instructions
	X86_GRP_BRANCH_RELATIVE, ///< = CS_GRP_BRANCH_RELATIVE

	// Architecture-specific groups
	X86_GRP_VM = 128,	///< all virtualization instructions (VT-x + AMD-V)
	X86_GRP_3DNOW,
	X86_GRP_AES,
	X86_GRP_ADX,
	X86_GRP_AVX,
	X86_GRP_AVX2,
	X86_GRP_AVX512,
	X86_GRP_BMI,
	X86_GRP_BMI2,
	X86_GRP_CMOV,
	X86_GRP_F16C,
	X86_GRP_FMA,
	X86_GRP_FMA4,
	X86_GRP_FSGSBASE,
	X86_GRP_HLE,
	X86_GRP_MMX,
	X86_GRP_MODE32,
	X86_GRP_MODE64,
	X86_GRP_RTM,
	X86_GRP_SHA,
	X86_GRP_SSE1,
	X86_GRP_SSE2,
	X86_GRP_SSE3,
	X86_GRP_SSE41,
	X86_GRP_SSE42,
	X86_GRP_SSE4A,
	X86_GRP_SSSE3,
	X86_GRP_PCLMUL,
	X86_GRP_XOP,
	X86_GRP_CDI,
	X86_GRP_ERI,
	X86_GRP_TBM,
	X86_GRP_16BITMODE,
	X86_GRP_NOT64BITMODE,
	X86_GRP_SGX,
	X86_GRP_DQI,
	X86_GRP_BWI,
	X86_GRP_PFI,
	X86_GRP_VLX,
	X86_GRP_SMAP,
	X86_GRP_NOVLX,
	X86_GRP_FPU,

	X86_GRP_ENDING
} x86_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/capstone/xcore.h`:

```h
#ifndef CAPSTONE_XCORE_H
#define CAPSTONE_XCORE_H

/* Capstone Disassembly Engine */
/* By Nguyen Anh Quynh <aquynh@gmail.com>, 2014-2015 */

#ifdef __cplusplus
extern "C" {
#endif

#include "platform.h"

#ifdef _MSC_VER
#pragma warning(disable:4201)
#endif

/// Operand type for instruction's operands
typedef enum xcore_op_type {
	XCORE_OP_INVALID = 0, ///< = CS_OP_INVALID (Uninitialized).
	XCORE_OP_REG, ///< = CS_OP_REG (Register operand).
	XCORE_OP_IMM, ///< = CS_OP_IMM (Immediate operand).
	XCORE_OP_MEM, ///< = CS_OP_MEM (Memory operand).
} xcore_op_type;

/// XCore registers
typedef enum xcore_reg {
	XCORE_REG_INVALID = 0,

	XCORE_REG_CP,
	XCORE_REG_DP,
	XCORE_REG_LR,
	XCORE_REG_SP,
	XCORE_REG_R0,
	XCORE_REG_R1,
	XCORE_REG_R2,
	XCORE_REG_R3,
	XCORE_REG_R4,
	XCORE_REG_R5,
	XCORE_REG_R6,
	XCORE_REG_R7,
	XCORE_REG_R8,
	XCORE_REG_R9,
	XCORE_REG_R10,
	XCORE_REG_R11,

	// pseudo registers
	XCORE_REG_PC,	///< pc

	// internal thread registers
	// see The-XMOS-XS1-Architecture(X7879A).pdf
	XCORE_REG_SCP,	///< save pc
	XCORE_REG_SSR,	//< save status
	XCORE_REG_ET,	//< exception type
	XCORE_REG_ED,	//< exception data
	XCORE_REG_SED,	//< save exception data
	XCORE_REG_KEP,	//< kernel entry pointer
	XCORE_REG_KSP,	//< kernel stack pointer
	XCORE_REG_ID,	//< thread ID

	XCORE_REG_ENDING,	// <-- mark the end of the list of registers
} xcore_reg;

/// Instruction's operand referring to memory
/// This is associated with XCORE_OP_MEM operand type above
typedef struct xcore_op_mem {
	uint8_t base;		///< base register, can be safely interpreted as
				///< a value of type `xcore_reg`, but it is only
				///< one byte wide
	uint8_t index;		///< index register, same conditions apply here
	int32_t disp;	///< displacement/offset value
	int     direct;	///< +1: forward, -1: backward
} xcore_op_mem;

/// Instruction operand
typedef struct cs_xcore_op {
	xcore_op_type type;	///< operand type
	union {
		xcore_reg reg;	///< register value for REG operand
		int32_t imm;		///< immediate value for IMM operand
		xcore_op_mem mem;		///< base/disp value for MEM operand
	};
} cs_xcore_op;

/// Instruction structure
typedef struct cs_xcore {
	/// Number of operands of this instruction,
	/// or 0 when instruction has no operand.
	uint8_t op_count;
	cs_xcore_op operands[8]; ///< operands for this instruction.
} cs_xcore;

/// XCore instruction
typedef enum xcore_insn {
	XCORE_INS_INVALID = 0,

	XCORE_INS_ADD,
	XCORE_INS_ANDNOT,
	XCORE_INS_AND,
	XCORE_INS_ASHR,
	XCORE_INS_BAU,
	XCORE_INS_BITREV,
	XCORE_INS_BLA,
	XCORE_INS_BLAT,
	XCORE_INS_BL,
	XCORE_INS_BF,
	XCORE_INS_BT,
	XCORE_INS_BU,
	XCORE_INS_BRU,
	XCORE_INS_BYTEREV,
	XCORE_INS_CHKCT,
	XCORE_INS_CLRE,
	XCORE_INS_CLRPT,
	XCORE_INS_CLRSR,
	XCORE_INS_CLZ,
	XCORE_INS_CRC8,
	XCORE_INS_CRC32,
	XCORE_INS_DCALL,
	XCORE_INS_DENTSP,
	XCORE_INS_DGETREG,
	XCORE_INS_DIVS,
	XCORE_INS_DIVU,
	XCORE_INS_DRESTSP,
	XCORE_INS_DRET,
	XCORE_INS_ECALLF,
	XCORE_INS_ECALLT,
	XCORE_INS_EDU,
	XCORE_INS_EEF,
	XCORE_INS_EET,
	XCORE_INS_EEU,
	XCORE_INS_ENDIN,
	XCORE_INS_ENTSP,
	XCORE_INS_EQ,
	XCORE_INS_EXTDP,
	XCORE_INS_EXTSP,
	XCORE_INS_FREER,
	XCORE_INS_FREET,
	XCORE_INS_GETD,
	XCORE_INS_GET,
	XCORE_INS_GETN,
	XCORE_INS_GETR,
	XCORE_INS_GETSR,
	XCORE_INS_GETST,
	XCORE_INS_GETTS,
	XCORE_INS_INCT,
	XCORE_INS_INIT,
	XCORE_INS_INPW,
	XCORE_INS_INSHR,
	XCORE_INS_INT,
	XCORE_INS_IN,
	XCORE_INS_KCALL,
	XCORE_INS_KENTSP,
	XCORE_INS_KRESTSP,
	XCORE_INS_KRET,
	XCORE_INS_LADD,
	XCORE_INS_LD16S,
	XCORE_INS_LD8U,
	XCORE_INS_LDA16,
	XCORE_INS_LDAP,
	XCORE_INS_LDAW,
	XCORE_INS_LDC,
	XCORE_INS_LDW,
	XCORE_INS_LDIVU,
	XCORE_INS_LMUL,
	XCORE_INS_LSS,
	XCORE_INS_LSUB,
	XCORE_INS_LSU,
	XCORE_INS_MACCS,
	XCORE_INS_MACCU,
	XCORE_INS_MJOIN,
	XCORE_INS_MKMSK,
	XCORE_INS_MSYNC,
	XCORE_INS_MUL,
	XCORE_INS_NEG,
	XCORE_INS_NOT,
	XCORE_INS_OR,
	XCORE_INS_OUTCT,
	XCORE_INS_OUTPW,
	XCORE_INS_OUTSHR,
	XCORE_INS_OUTT,
	XCORE_INS_OUT,
	XCORE_INS_PEEK,
	XCORE_INS_REMS,
	XCORE_INS_REMU,
	XCORE_INS_RETSP,
	XCORE_INS_SETCLK,
	XCORE_INS_SET,
	XCORE_INS_SETC,
	XCORE_INS_SETD,
	XCORE_INS_SETEV,
	XCORE_INS_SETN,
	XCORE_INS_SETPSC,
	XCORE_INS_SETPT,
	XCORE_INS_SETRDY,
	XCORE_INS_SETSR,
	XCORE_INS_SETTW,
	XCORE_INS_SETV,
	XCORE_INS_SEXT,
	XCORE_INS_SHL,
	XCORE_INS_SHR,
	XCORE_INS_SSYNC,
	XCORE_INS_ST16,
	XCORE_INS_ST8,
	XCORE_INS_STW,
	XCORE_INS_SUB,
	XCORE_INS_SYNCR,
	XCORE_INS_TESTCT,
	XCORE_INS_TESTLCL,
	XCORE_INS_TESTWCT,
	XCORE_INS_TSETMR,
	XCORE_INS_START,
	XCORE_INS_WAITEF,
	XCORE_INS_WAITET,
	XCORE_INS_WAITEU,
	XCORE_INS_XOR,
	XCORE_INS_ZEXT,

	XCORE_INS_ENDING,   // <-- mark the end of the list of instructions
} xcore_insn;

/// Group of XCore instructions
typedef enum xcore_insn_group {
	XCORE_GRP_INVALID = 0, ///< = CS_GRP_INVALID

	// Generic groups
	// all jump instructions (conditional+direct+indirect jumps)
	XCORE_GRP_JUMP,	///< = CS_GRP_JUMP

	XCORE_GRP_ENDING,   // <-- mark the end of the list of groups
} xcore_insn_group;

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/capstone/include/platform.h`:

```h
/* Capstone Disassembly Engine */
/* By Axel Souchet & Nguyen Anh Quynh, 2014 */

#ifndef CAPSTONE_PLATFORM_H
#define CAPSTONE_PLATFORM_H

// handle C99 issue (for pre-2013 VisualStudio)
#if !defined(__CYGWIN__) && !defined(__MINGW32__) && !defined(__MINGW64__) && (defined (WIN32) || defined (WIN64) || defined (_WIN32) || defined (_WIN64))
// MSVC

// stdbool.h
#if (_MSC_VER < 1800) || defined(_KERNEL_MODE)
// this system does not have stdbool.h
#ifndef __cplusplus
typedef unsigned char bool;
#define false 0
#define true 1
#endif

#else
// VisualStudio 2013+ -> C99 is supported
#include <stdbool.h>
#endif

#else
// not MSVC -> C99 is supported
#include <stdbool.h>
#endif


// handle C99 issue (for pre-2013 VisualStudio)
#if defined(CAPSTONE_HAS_OSXKERNEL) || (defined(_MSC_VER) && (_MSC_VER <= 1700 || defined(_KERNEL_MODE)))
// this system does not have inttypes.h

#if defined(_MSC_VER) && (_MSC_VER < 1600 || defined(_KERNEL_MODE))
// this system does not have stdint.h
typedef signed char  int8_t;
typedef signed short int16_t;
typedef signed int   int32_t;
typedef unsigned char  uint8_t;
typedef unsigned short uint16_t;
typedef unsigned int   uint32_t;
typedef signed long long   int64_t;
typedef unsigned long long uint64_t;

#define INT8_MIN         (-127i8 - 1)
#define INT16_MIN        (-32767i16 - 1)
#define INT32_MIN        (-2147483647i32 - 1)
#define INT64_MIN        (-9223372036854775807i64 - 1)
#define INT8_MAX         127i8
#define INT16_MAX        32767i16
#define INT32_MAX        2147483647i32
#define INT64_MAX        9223372036854775807i64
#define UINT8_MAX        0xffui8
#define UINT16_MAX       0xffffui16
#define UINT32_MAX       0xffffffffui32
#define UINT64_MAX       0xffffffffffffffffui64
#endif

#define __PRI_8_LENGTH_MODIFIER__ "hh"
#define __PRI_64_LENGTH_MODIFIER__ "ll"

#define PRId8         __PRI_8_LENGTH_MODIFIER__ "d"
#define PRIi8         __PRI_8_LENGTH_MODIFIER__ "i"
#define PRIo8         __PRI_8_LENGTH_MODIFIER__ "o"
#define PRIu8         __PRI_8_LENGTH_MODIFIER__ "u"
#define PRIx8         __PRI_8_LENGTH_MODIFIER__ "x"
#define PRIX8         __PRI_8_LENGTH_MODIFIER__ "X"

#define PRId16        "hd"
#define PRIi16        "hi"
#define PRIo16        "ho"
#define PRIu16        "hu"
#define PRIx16        "hx"
#define PRIX16        "hX"

#if defined(_MSC_VER) && _MSC_VER <= 1700
#define PRId32        "ld"
#define PRIi32        "li"
#define PRIo32        "lo"
#define PRIu32        "lu"
#define PRIx32        "lx"
#define PRIX32        "lX"
#else	// OSX
#define PRId32        "d"
#define PRIi32        "i"
#define PRIo32        "o"
#define PRIu32        "u"
#define PRIx32        "x"
#define PRIX32        "X"
#endif

#if defined(_MSC_VER) && _MSC_VER <= 1700
// redefine functions from inttypes.h used in cstool
#define strtoull _strtoui64
#endif

#define PRId64        __PRI_64_LENGTH_MODIFIER__ "d"
#define PRIi64        __PRI_64_LENGTH_MODIFIER__ "i"
#define PRIo64        __PRI_64_LENGTH_MODIFIER__ "o"
#define PRIu64        __PRI_64_LENGTH_MODIFIER__ "u"
#define PRIx64        __PRI_64_LENGTH_MODIFIER__ "x"
#define PRIX64        __PRI_64_LENGTH_MODIFIER__ "X"

#else
// this system has inttypes.h by default
#include <inttypes.h>
#endif

#endif

```

`CodeCleaner/capstone/include/windowsce/intrin.h`:

```h

#if defined(_MSC_VER) && defined(_WIN32_WCE) && (_WIN32_WCE < 0x800) && !defined(__INTRIN_H_) && !defined(_INTRIN)
#define _STDINT

#ifdef _M_ARM
#include <armintr.h>
#if (_WIN32_WCE >= 0x700) && defined(__ARM_ARCH_7__) || defined(__ARM_ARCH_7A__)
#include <arm_neon.h>
#endif
#endif // _M_ARM

#endif

```

`CodeCleaner/capstone/include/windowsce/stdint.h`:

```h

#if defined(_MSC_VER) && defined(_WIN32_WCE) && (_WIN32_WCE < 0x800) && !defined(_STDINT_H_) && !defined(_STDINT)
#define _STDINT

typedef __int8
	int8_t,
	int_least8_t;

typedef __int16
	int16_t,
	int_least16_t;

typedef __int32
	int32_t,
	int_least32_t,
	int_fast8_t,
	int_fast16_t,
	int_fast32_t;
	
typedef __int64
	int64_t,
	intmax_t,
	int_least64_t,
	int_fast64_t;

typedef unsigned __int8
	uint8_t,
	uint_least8_t;
	
typedef unsigned __int16
	uint16_t,
	uint_least16_t;
	
typedef unsigned __int32
	uint32_t,
	uint_least32_t,
	uint_fast8_t,
	uint_fast16_t,
	uint_fast32_t;
	
typedef unsigned __int64
	uint64_t,
	uintmax_t,
	uint_least64_t,
	uint_fast64_t;

#ifndef _INTPTR_T_DEFINED
#define _INTPTR_T_DEFINED
typedef __int32	intptr_t;
#endif

#ifndef _UINTPTR_T_DEFINED
#define _UINTPTR_T_DEFINED
typedef unsigned __int32	uintptr_t;
#endif

#define INT8_MIN         (-127i8 - 1)
#define INT16_MIN        (-32767i16 - 1)
#define INT32_MIN        (-2147483647i32 - 1)
#define INT64_MIN        (-9223372036854775807i64 - 1)
#define INT8_MAX         127i8
#define INT16_MAX        32767i16
#define INT32_MAX        2147483647i32
#define INT64_MAX        9223372036854775807i64
#define UINT8_MAX        0xffui8
#define UINT16_MAX       0xffffui16
#define UINT32_MAX       0xffffffffui32
#define UINT64_MAX       0xffffffffffffffffui64

#define INT_LEAST8_MIN   INT8_MIN
#define INT_LEAST16_MIN  INT16_MIN
#define INT_LEAST32_MIN  INT32_MIN
#define INT_LEAST64_MIN  INT64_MIN
#define INT_LEAST8_MAX   INT8_MAX
#define INT_LEAST16_MAX  INT16_MAX
#define INT_LEAST32_MAX  INT32_MAX
#define INT_LEAST64_MAX  INT64_MAX
#define UINT_LEAST8_MAX  UINT8_MAX
#define UINT_LEAST16_MAX UINT16_MAX
#define UINT_LEAST32_MAX UINT32_MAX
#define UINT_LEAST64_MAX UINT64_MAX

#define INT_FAST8_MIN    INT8_MIN
#define INT_FAST16_MIN   INT32_MIN
#define INT_FAST32_MIN   INT32_MIN
#define INT_FAST64_MIN   INT64_MIN
#define INT_FAST8_MAX    INT8_MAX
#define INT_FAST16_MAX   INT32_MAX
#define INT_FAST32_MAX   INT32_MAX
#define INT_FAST64_MAX   INT64_MAX
#define UINT_FAST8_MAX   UINT8_MAX
#define UINT_FAST16_MAX  UINT32_MAX
#define UINT_FAST32_MAX  UINT32_MAX
#define UINT_FAST64_MAX  UINT64_MAX

#define INTPTR_MIN      INT32_MIN
#define INTPTR_MAX      INT32_MAX
#define UINTPTR_MAX     UINT32_MAX

#define INTMAX_MIN       INT64_MIN
#define INTMAX_MAX       INT64_MAX
#define UINTMAX_MAX      UINT64_MAX

#define PTRDIFF_MIN      INTPTR_MIN
#define PTRDIFF_MAX      INTPTR_MAX

#ifndef SIZE_MAX
#define SIZE_MAX        UINTPTR_MAX
#endif

#define SIG_ATOMIC_MIN   INT32_MIN
#define SIG_ATOMIC_MAX   INT32_MAX

#define WCHAR_MIN        0x0000
#define WCHAR_MAX        0xffff

#define WINT_MIN         0x0000
#define WINT_MAX         0xffff

#define INT8_C(x)    (x)
#define INT16_C(x)   (x)
#define INT32_C(x)   (x)
#define INT64_C(x)   (x ## LL)

#define UINT8_C(x)   (x)
#define UINT16_C(x)  (x)
#define UINT32_C(x)  (x ## U)
#define UINT64_C(x)  (x ## ULL)

#define INTMAX_C(x)  INT64_C(x)
#define UINTMAX_C(x) UINT64_C(x)

#endif

```

`CodeCleaner/plugin.cpp`:

```cpp
#include <Windows.h>
#include "plugin.h"
#include <capstone/capstone.h>
#include <capstone/x86.h>
#include <iostream>
#include <string>
#include <sstream>
#include <asmjit/asmjit.h>
#include <asmjit/x86.h>

using namespace std;
using namespace asmjit;
using namespace asmjit::x86;

void cbMenuEntry(CBTYPE cbType, void* info1)
{
	PLUG_CB_MENUENTRY* info = reinterpret_cast<PLUG_CB_MENUENTRY*>(info1);
	if (info->hEntry == 0 || info->hEntry == 1)
	{
		DbgCmdExec(PLUGIN_NAME);
	}
}

Gp cvrtCsRegToGp(x86_reg r)
{
	switch (r)
	{
		//X64
	case X86_REG_RAX:
		return rax;
	case X86_REG_RBX:
		return rbx;
	case X86_REG_RCX:
		return rcx;
	case X86_REG_RDX:
		return rdx;
	case X86_REG_RDI:
		return rdi;
	case X86_REG_RSI:
		return rsi;
	case X86_REG_RSP:
		return rsp;
	case X86_REG_RBP:
		return rbp;

	case X86_REG_R8:
		return r8;
	case X86_REG_R9:
		return r9;
	case X86_REG_R10:
		return r10;
	case X86_REG_R11:
		return r11;
	case X86_REG_R12:
		return r12;
	case X86_REG_R13:
		return r13;
	case X86_REG_R14:
		return r14;
	case X86_REG_R15:
		return r15;

		//X32
	case X86_REG_EAX:
		return eax;
	case X86_REG_EBX:
		return ebx;
	case X86_REG_ECX:
		return ecx;
	case X86_REG_EDX:
		return edx;
	case X86_REG_EDI:
		return edi;
	case X86_REG_ESI:
		return esi;
	case X86_REG_ESP:
		return esp;
	case X86_REG_EBP:
		return ebp;
	case X86_REG_R8D:
		return r8d;
	case X86_REG_R9D:
		return r9d;
	case X86_REG_R10D:
		return r10d;
	case X86_REG_R11D:
		return r11d;
	case X86_REG_R12D:
		return r12d;
	case X86_REG_R13D:
		return r13d;
	case X86_REG_R14D:
		return r14d;
	case X86_REG_R15D:
		return r15d;

		//X16
	case X86_REG_AX:
		return ax;
	case X86_REG_BX:
		return bx;
	case X86_REG_CX:
		return cx;
	case X86_REG_DX:
		return dx;
	case X86_REG_DI:
		return di;
	case X86_REG_SI:
		return si;
	case X86_REG_SP:
		return sp;
	case X86_REG_BP:
		return bp;
	case X86_REG_R8W:
		return r8w;
	case X86_REG_R9W:
		return r9w;
	case X86_REG_R10W:
		return r10w;
	case X86_REG_R11W:
		return r11w;
	case X86_REG_R12W:
		return r12w;
	case X86_REG_R13W:
		return r13w;
	case X86_REG_R14W:
		return r14w;
	case X86_REG_R15W:
		return r15w;
		//X8
	case X86_REG_AH:
		return ah;
	case X86_REG_BH:
		return bh;
	case X86_REG_CH:
		return ch;
	case X86_REG_DH:
		return dh;
	case X86_REG_AL:
		return al;
	case X86_REG_BL:
		return bl;
	case X86_REG_CL:
		return cl;
	case X86_REG_DL:
		return dl;
	case X86_REG_R8B:
		return r8b;
	case X86_REG_R9B:
		return r9b;
	case X86_REG_R10B:
		return r10b;
	case X86_REG_R11B:
		return r11b;
	case X86_REG_R12B:
		return r12b;
	case X86_REG_R13B:
		return r13b;
	case X86_REG_R14B:
		return r14b;
	case X86_REG_R15B:
		return r15b;
	default:
		MessageBoxA(hwndDlg, "[" PLUGIN_NAME "]Failed Converting CsReg to Gp.\n Press OK to quit.", "Fatel Error", MB_OK | MB_ICONERROR);
		throw "ConvertGp";
	}
}

//排除一切nop干扰
int getLastInsIndex(const cs_insn* insn, int now_index)
{
	//故意找茬(如果索引为0也意味着前面没指令)
	if (now_index <= 0) return -1;

	//索引前一个
	int result = now_index - 1;
	for (; result >= -1; result--)
	{

		//检索失败
		if (result < 0)
		{
			return -1;
		}

		if (insn[result].id != X86_INS_NOP)
		{
			break;
		}
	}

	return result;
}

static uint32_t getRegByteSize(x86_reg cs_reg)
{
	using namespace asmjit::x86;

	// 将 Capstone 寄存器转换为 AsmJit 寄存器
	Gp reg = cvrtCsRegToGp(cs_reg); // 使用你已有的转换函数

	// 获取字节数并返回
	return reg.size();
}

static uint32_t getRegBitWidth(x86_reg cs_reg)
{
	return getRegByteSize(cs_reg) * 8; // 字节数 * 8 = 位数
}



bool cleanCode(const cs_insn* insn, size_t codeCount)
{
	bool modified = false;
	for (size_t i = 0; i < codeCount; i++)
	{
		if (insn[i].id == X86_INS_NOP)
		{
			continue;
		}

		if (insn[i].detail == NULL)
		{
			_plugin_logprintf("[" PLUGIN_NAME "][Cs] failed to get CODE-INSN! \n", insn[i].mnemonic);
			break;
		}

		cs_x86* x86_this = &(insn[i].detail->x86);

		//清理MOV(MOV REG,REG)
		if (insn[i].id == X86_INS_MOV)
		{
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg == x86_this->operands[1].reg)
			{
				DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
				modified = true;
				break;
			}
		}

		//清理XCHG(XCHG REG,REG)
		if (insn[i].id == X86_INS_XCHG)
		{
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg == x86_this->operands[1].reg)
			{
				DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
				modified = true;
				break;
			}
		}

		//化简指令 MOV REG,0 / AND REG,0
		if (insn[i].id == X86_INS_MOV || insn[i].id == X86_INS_AND)
		{
			if (x86_this->operands[0].type == X86_OP_REG
				&& x86_this->operands[1].type == X86_OP_IMM
				&& x86_this->operands[1].imm == 0)
			{
				DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
				Environment env;
				env.setArch(Arch::kX64);
				//Asmjit 初始化
				CodeHolder code;
				code.init(env);
				x86::Assembler a(&code);
				a.xor_(cvrtCsRegToGp(x86_this->operands[0].reg), cvrtCsRegToGp(x86_this->operands[0].reg));
				PBYTE tCode = new BYTE[code.codeSize()];
				code.copyFlattenedData(tCode, code.codeSize());
				DbgFunctions()->MemPatch(insn[i].address, tCode, code.codeSize());
				delete[] tCode;
				modified = true;
				code.reset();
				modified = true;
				break;
			}
		}


		// 处理
		/*
		push rsp
		pop !rsp
		=> mov !rsp, rsp
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg != X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				// 验证四指令模式
				const cs_insn* push_rsp = &insn[last1];
				const cs_insn* not_pop_rsp = &insn[i];
				auto mainREG = not_pop_rsp->detail->x86.operands[0].reg;
				if (push_rsp->id == X86_INS_PUSH &&
					push_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					push_rsp->detail->x86.operands[0].reg == X86_REG_RSP &&

					not_pop_rsp->id == X86_INS_POP &&
					not_pop_rsp->detail->x86.operands[0].type == X86_OP_REG)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(not_pop_rsp->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.mov(cvrtCsRegToGp(mainREG), rsp);

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_rsp->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


		// 处理
		/*
		add ebp, Imm1
		sub rbp, Imm1
		=> NOP
		*/
		if (insn[i].id == X86_INS_SUB) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg == X86_REG_RBP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				

				// 验证四指令模式
				const cs_insn* add_esp_8 = &insn[last1];
				const cs_insn* sub_rsp_8 = &insn[i];

				if (add_esp_8->id == X86_INS_ADD &&
					add_esp_8->detail->x86.operands[0].type == X86_OP_REG &&
					add_esp_8->detail->x86.operands[0].reg == X86_REG_EBP &&
					add_esp_8->detail->x86.operands[1].imm == sub_rsp_8->detail->x86.operands[1].imm &&

					sub_rsp_8->id == X86_INS_SUB &&
					sub_rsp_8->detail->x86.operands[0].type == X86_OP_REG &&
					sub_rsp_8->detail->x86.operands[1].type == X86_OP_IMM)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(add_esp_8->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(sub_rsp_8->address, "NOP", 0, true);

					modified = true;
					break;
				}
			}
		}

		// 处理
		/*
		push Imm
		pop reg
		=> mov reg, Imm
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg != X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;
				// 验证四指令模式
				const cs_insn* push_imm = &insn[last1];
				const cs_insn* pop_reg = &insn[i];

				if (push_imm->id == X86_INS_PUSH &&
					push_imm->detail->x86.operands[0].type == X86_OP_IMM &&

					pop_reg->id == X86_INS_POP &&
					pop_reg->detail->x86.operands[0].type == X86_OP_REG)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_imm->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_reg->address, "NOP", 0, true);

					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.mov(cvrtCsRegToGp(pop_reg->detail->x86.operands[0].reg), push_imm->detail->x86.operands[0].imm);



					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_imm->address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;

				}
			}
		}

		// 处理
		/*
		push !rsp
		pop reg
		=> mov reg, !rsp
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg != X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;
				// 验证四指令模式
				const cs_insn* push_reg = &insn[last1];
				const cs_insn* pop_reg = &insn[i];

				if (push_reg->id == X86_INS_PUSH &&
					push_reg->detail->x86.operands[0].type == X86_OP_REG &&
					push_reg->detail->x86.operands[0].reg != X86_REG_RSP &&

					pop_reg->id == X86_INS_POP &&
					pop_reg->detail->x86.operands[0].type == X86_OP_REG)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_reg->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_reg->address, "NOP", 0, true);

					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.mov(cvrtCsRegToGp(pop_reg->detail->x86.operands[0].reg), cvrtCsRegToGp(push_reg->detail->x86.operands[0].reg));



					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_reg->address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;

				}
			}
		}




		// 处理 运算
		/*
		mov rbp, IMM1
		INS1 rbp, IMM2
		*/
		if (insn[i].id == X86_INS_ADD ||
			insn[i].id == X86_INS_SUB ||
			insn[i].id == X86_INS_XOR ||
			insn[i].id == X86_INS_AND ||
			insn[i].id == X86_INS_OR ||
			insn[i].id == X86_INS_SHL ||
			insn[i].id == X86_INS_SHR ||
			insn[i].id == X86_INS_NEG ||
			insn[i].id == X86_INS_NOT ||
			insn[i].id == X86_INS_INC ||
			insn[i].id == X86_INS_DEC) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;
				// 验证四指令模式
				const cs_insn* ins1 = &insn[last1];
				const cs_insn* ins2 = &insn[i];

				if (ins1->id == X86_INS_MOV &&
					ins1->detail->x86.operands[0].type == X86_OP_REG &&
					ins1->detail->x86.operands[1].type == X86_OP_IMM &&

					ins2->detail->x86.operands[0].type == X86_OP_REG &&
					ins2->detail->x86.operands[0].reg == ins1->detail->x86.operands[0].reg &&
					ins2->detail->x86.operands[1].type == X86_OP_IMM
					)

				{
					uint64_t result_imm = ins1->detail->x86.operands[1].imm;
					// 对于单操作数指令
					if (ins2->id == X86_INS_INC || ins2->id == X86_INS_DEC || ins2->id == X86_INS_NOT || ins2->id == X86_INS_NEG)
					{
						switch (ins2->id)
						{
						case X86_INS_INC:
							result_imm++;
							break;
						case X86_INS_DEC:
							result_imm--;
							break;
						case X86_INS_NOT:
							result_imm = ~result_imm;
							break;
						case X86_INS_NEG:
							result_imm = ~result_imm + 1;
							break;
						}
					}
					else
					{
						uint64_t ins2_imm = ins2->detail->x86.operands[1].imm;
						switch (ins2->id)
						{
						case X86_INS_ADD:
							result_imm += ins2_imm;
							break;
						case X86_INS_SUB:
							result_imm -= ins2_imm;
							break;
						case X86_INS_XOR:
							result_imm ^= ins2_imm;
							break;
						case X86_INS_OR:
							result_imm |= ins2_imm;
							break;
						case X86_INS_AND:
							result_imm &= ins2_imm;
							break;
						case X86_INS_SHL:
							result_imm <<= ins2_imm;
							break;
						case X86_INS_SHR:
							result_imm >>= ins2_imm;
							break;
						}
					}

					// 清除原指令
					DbgFunctions()->AssembleAtEx(ins1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(ins2->address, "NOP", 0, true);

					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);

					a.mov(cvrtCsRegToGp(ins1->detail->x86.operands[0].reg), result_imm);

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(ins1->address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;

				}
			}
		}


		//必须int传进传出
		int last = getLastInsIndex(insn, (int)i);

		//如果有上一行指令
		if (last != -1)
		{
			cs_x86* x86_last = &(insn[last].detail->x86);
			//如果上一行指令是PUSH 且本行指令是 MOV [RSP],xxx
			if (insn[last].id == X86_INS_PUSH && insn[i].id == X86_INS_MOV)
			{
				if (x86_this->operands[0].type == X86_OP_MEM
					&& x86_this->operands[0].mem.base == X86_REG_RSP
					&& x86_this->operands[0].mem.disp == 0
					&& x86_this->operands[0].mem.scale == 1
					)
				{

					//替换为PUSH xxx
					if (x86_this->operands[1].type == X86_OP_IMM)
					{
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						Environment env;
						env.setArch(Arch::kX64);
						//Asmjit 初始化
						CodeHolder code;
						code.init(env);
						x86::Assembler a(&code);
						a.push(x86_this->operands[1].imm);
						PBYTE tCode = new BYTE[code.codeSize()];
						code.copyFlattenedData(tCode, code.codeSize());
						DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
						delete[] tCode;
						modified = true;
						code.reset();
						break;
					}
					else if (x86_this->operands[1].type == X86_OP_REG && x86_this->operands[1].reg != X86_REG_RSP) //不对RSP进行处理
					{
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						Environment env;
						env.setArch(Arch::kX64);
						//Asmjit 初始化
						CodeHolder code;
						code.init(env);
						x86::Assembler a(&code);
						a.push(cvrtCsRegToGp(x86_this->operands[1].reg));
						PBYTE tCode = new BYTE[code.codeSize()];
						code.copyFlattenedData(tCode, code.codeSize());
						DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
						delete[] tCode;
						modified = true;
						code.reset();
						break;
					}
				}
			}

			//SUB RSP,0x8; MOV [RSP],REG
			if (insn[last].id == X86_INS_SUB && insn[i].id == X86_INS_MOV)
			{
				if (x86_last->operands[0].type == X86_OP_REG
					&& x86_last->operands[0].reg == X86_REG_RSP
					&& x86_last->operands[1].type == X86_OP_IMM
					&& x86_last->operands[1].imm == 0x8
					&& x86_this->operands[0].type == X86_OP_MEM
					&& x86_this->operands[1].type == X86_OP_REG
					&& x86_this->operands[1].reg != X86_REG_RSP)
				{
					//PUSH REG
					DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.push(cvrtCsRegToGp(x86_this->operands[1].reg));
					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;
				}
			}

			//MOV + POP 清理
			/*
			MOV R11, xxx
			POP R11
			-> 清除第一个MOV
			*/
			if (insn[last].id == X86_INS_MOV && insn[i].id == X86_INS_POP)
			{
				if (x86_last->operands[0].type == X86_OP_REG
					&& x86_last->operands[0].reg != X86_REG_RSP
					&& x86_this->operands[0].type == X86_OP_REG
					&& x86_this->operands[0].reg == x86_last->operands[0].reg)
				{
					//PUSH REG
					DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
					modified = true;
					break;
				}
			}

			//MOV REG,[RSP]; ADD RSP,0x8
			if (insn[last].id == X86_INS_MOV && insn[i].id == X86_INS_ADD)
			{
				if (x86_last->operands[0].type == X86_OP_REG
					&& x86_last->operands[1].type == X86_OP_MEM
					&& x86_last->operands[1].mem.base == X86_REG_RSP
					&& x86_last->operands[1].mem.scale == 1
					&& x86_last->operands[1].mem.disp == 0
					&& x86_last->operands[0].reg != X86_REG_RSP
					&& x86_this->operands[0].reg == X86_REG_RSP
					&& x86_this->operands[1].imm == 0x8)
				{
					//POP REG
					DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.pop(cvrtCsRegToGp(x86_last->operands[0].reg));
					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;
				}
			}

			//PUSH [RSP] / REG; POP [xxx] /REG1
			if (insn[last].id == X86_INS_PUSH && insn[i].id == X86_INS_POP)
			{
				/*
				PUSH [RSP+x]
				POP REG
				-->
				MOV REG, [RSP+x]
				*/
				if (x86_last->operands[0].type == X86_OP_MEM
					&& x86_last->operands[0].mem.base == X86_REG_RSP
					&& x86_this->operands[0].type == X86_OP_REG
					)
				{
					//MOV REG, [RSP]
					DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					if (x86_this->operands[0].mem.index == X86_REG_INVALID)
					{
						a.mov(cvrtCsRegToGp(x86_this->operands[0].reg),
							ptr(cvrtCsRegToGp(x86_last->operands[0].mem.base),
								(int32_t)x86_last->operands[0].mem.disp));
					}
					else
					{
						a.mov(cvrtCsRegToGp(x86_this->operands[0].reg),
							ptr(cvrtCsRegToGp(x86_last->operands[0].mem.base),
								cvrtCsRegToGp(x86_last->operands[0].mem.index),
								x86_last->operands[0].mem.scale / 2,
								(int32_t)x86_last->operands[0].mem.disp));
					}

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;
				}
				/*
				PUSH REG
				POP [rsp+X]
				-->
				MOV [RSP+x],REG
				*/
				else if (x86_last->operands[0].type == X86_OP_REG
					&& x86_last->operands[0].reg != X86_REG_RSP
					&& x86_this->operands[0].type == X86_OP_MEM
					&& x86_this->operands[0].mem.base == X86_REG_RSP
					)
				{
					DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					if (x86_this->operands[0].mem.index == X86_REG_INVALID)
					{
						a.mov(ptr(cvrtCsRegToGp(x86_this->operands[0].mem.base),
							(int32_t)x86_this->operands[0].mem.disp), cvrtCsRegToGp(x86_last->operands[0].reg));
					}
					else
					{
						a.mov(ptr(cvrtCsRegToGp(x86_this->operands[0].mem.base),
							cvrtCsRegToGp(x86_this->operands[0].mem.index),
							x86_this->operands[0].mem.scale / 2,
							(int32_t)x86_this->operands[0].mem.disp), cvrtCsRegToGp(x86_last->operands[0].reg));
					}

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;
				}
				else if (x86_last->operands[0].type == X86_OP_REG
					&& x86_last->operands[0].reg != X86_REG_RSP
					&& x86_this->operands[0].type == X86_OP_MEM
					&& x86_this->operands[0].mem.base != X86_REG_RSP
					&& x86_this->operands[0].mem.index != X86_REG_RSP
					)
				{
					DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.mov(ptr(
						cvrtCsRegToGp(x86_this->operands[0].mem.base),
						cvrtCsRegToGp(x86_this->operands[0].mem.index),
						x86_this->operands[0].mem.scale / 2,
						(int32_t)x86_this->operands[0].mem.disp), cvrtCsRegToGp(x86_last->operands[0].reg));
					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
					delete[] tCode;
					modified = true;
					code.reset();
					break;
				}
				/*
				PUSH REG1
				POP REG2
				-->
				MOV REG2, REG1
				*/
				else if (x86_last->operands[0].type == X86_OP_REG
					&& x86_this->operands[0].type == X86_OP_REG
					&& x86_this->operands[0].reg != x86_last->operands[0].reg
					)
				{
					/*  暂时不清理!
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						Environment env;
						env.setArch(Arch::kX64);
						//Asmjit 初始化
						CodeHolder code;
						code.init(env);
						x86::Assembler a(&code);
						a.mov(cvrtCsRegToGp(x86_this->operands[0].reg), cvrtCsRegToGp(x86_last->operands[0].reg));
						PBYTE tCode = new BYTE[code.codeSize()];
						code.copyFlattenedData(tCode, code.codeSize());
						DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
						delete[] tCode;
						modified = true;
						code.reset();
						break;
					*/
				}
			}

			//解决ADD [RSP], IMM + POP REG 问题(先pop再操作)
			if (insn[i].id == X86_INS_POP)
			{
				//第一个指令是否为XXX [RSP],IMM 第二个指令是否为POP REG 
				if (x86_this->operands[0].type == X86_OP_REG
					&& x86_this->operands[0].reg != X86_REG_RSP
					&& x86_last->operands[0].type == X86_OP_MEM
					&& x86_last->operands[0].mem.base == X86_REG_RSP
					&& x86_last->operands[0].mem.scale == 1
					&& x86_last->operands[0].mem.disp == 0
					&& x86_last->operands[1].type == X86_OP_IMM)
				{
					bool isValid = true;
					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);

					switch (insn[last].id)
					{
					case X86_INS_ADD:
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						a.pop(cvrtCsRegToGp(x86_this->operands[0].reg));
						a.add(cvrtCsRegToGp(x86_this->operands[0].reg), x86_last->operands[1].imm);
						break;
					case X86_INS_SUB:
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						a.pop(cvrtCsRegToGp(x86_this->operands[0].reg));
						a.sub(cvrtCsRegToGp(x86_this->operands[0].reg), x86_last->operands[1].imm);
						break;
					case X86_INS_XOR:
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						a.pop(cvrtCsRegToGp(x86_this->operands[0].reg));
						a.xor_(cvrtCsRegToGp(x86_this->operands[0].reg), x86_last->operands[1].imm);
						break;
					case X86_INS_AND:
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						a.pop(cvrtCsRegToGp(x86_this->operands[0].reg));
						a.and_(cvrtCsRegToGp(x86_this->operands[0].reg), x86_last->operands[1].imm);
						break;
					case X86_INS_OR:
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						a.pop(cvrtCsRegToGp(x86_this->operands[0].reg));
						a.or_(cvrtCsRegToGp(x86_this->operands[0].reg), x86_last->operands[1].imm);
						break;
					default:
						isValid = false;
						break;
					}

					if (isValid)
					{
						PBYTE tCode = new BYTE[code.codeSize()];
						code.copyFlattenedData(tCode, code.codeSize());
						DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
						delete[] tCode;
						modified = true;
						code.reset();
						break;
					}

					code.reset();
				}
			}

			//清理PUSH+POP(IMM不可以，如果是IMM，POP必然会修改寄存器或内存)
			if (insn[last].id == X86_INS_PUSH && insn[i].id == X86_INS_POP)
			{
				if (x86_last->operands[0].type == x86_this->operands[0].type)
				{
					if ((x86_this->operands[0].type == X86_OP_MEM
						&& x86_last->operands[0].mem.base == x86_this->operands[0].mem.base
						&& x86_last->operands[0].mem.index == x86_this->operands[0].mem.index
						&& x86_last->operands[0].mem.scale == x86_this->operands[0].mem.scale
						&& x86_last->operands[0].mem.disp == x86_this->operands[0].mem.disp
						&& x86_last->operands[0].mem.segment == x86_this->operands[0].mem.segment)
						||
						(x86_this->operands[0].type == X86_OP_REG
							&& x86_last->operands[0].reg == x86_this->operands[0].reg))
					{
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						modified = true;
						break;
					}
				}
			}

			//清理ADD+SUB
			if ((insn[last].id == X86_INS_ADD && insn[i].id == X86_INS_SUB) || (insn[i].id == X86_INS_ADD && insn[last].id == X86_INS_SUB))
			{
				if (x86_last->operands[0].type == x86_this->operands[0].type)
				{
					if ((x86_this->operands[0].type == X86_OP_MEM
						&& x86_last->operands[0].mem.base == x86_this->operands[0].mem.base
						&& x86_last->operands[0].mem.index == x86_this->operands[0].mem.index
						&& x86_last->operands[0].mem.scale == x86_this->operands[0].mem.scale
						&& x86_last->operands[0].mem.disp == x86_this->operands[0].mem.disp
						&& x86_last->operands[0].mem.segment == x86_this->operands[0].mem.segment
						&& x86_last->operands[1].type == X86_OP_IMM
						&& x86_this->operands[1].type == X86_OP_IMM
						&& x86_this->operands[1].imm == x86_last->operands[1].imm)
						||
						(x86_this->operands[0].type == X86_OP_REG
							&& x86_last->operands[0].reg == x86_this->operands[0].reg
							&& x86_last->operands[1].type == X86_OP_IMM
							&& x86_this->operands[1].type == X86_OP_IMM
							&& x86_this->operands[1].imm == x86_last->operands[1].imm
							))
					{
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						modified = true;
						break;
					}
				}
			}

			cs_x86* x86_next = &(insn[i + 1].detail->x86);
			//清理三个xor (reg, reg) -> xchg
			if (insn[i + 1].id == X86_INS_XOR && insn[i].id == X86_INS_XOR && insn[i - 1].id == X86_INS_XOR
				&& (x86_next->operands->type == X86_OP_REG && x86_last->operands->type == X86_OP_REG && x86_this->operands->type == X86_OP_REG))
			{
				if (x86_next->operands[0].reg == x86_last->operands[0].reg &&
					x86_next->operands[1].reg == x86_last->operands[1].reg &&
					x86_this->operands[0].reg == x86_last->operands[1].reg &&
					x86_this->operands[1].reg == x86_last->operands[0].reg
					)
				{

					Environment env;
					env.setArch(Arch::kX64);
					//Asmjit 初始化
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.xchg(cvrtCsRegToGp(x86_this->operands[0].reg), cvrtCsRegToGp(x86_this->operands[1].reg));
					DbgFunctions()->AssembleAtEx(insn[i - 1].address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(insn[i + 1].address, "NOP", 0, true);
					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(insn[last].address, tCode, code.codeSize());
				}
				modified = true;
				break;
			}


			//上上一行指令
			int last2 = getLastInsIndex(insn, (int)last);
			//有前两行指令
			if (last2 != -1)
			{
				cs_x86* x86_last2 = &(insn[last2].detail->x86);
				cs_x86* x86_last = &(insn[last].detail->x86);
				cs_x86* x86_this = &(insn[i].detail->x86);
				//PUSH;PUSH;POP [RSP]
				if (insn[i].id == X86_INS_POP && x86_this->operands[0].type == X86_OP_MEM)
				{
					//前两个都是PUSH 第一个可以是SUB RSP,0x8
					if ((insn[last2].id == X86_INS_PUSH
						|| (insn[last2].id == X86_INS_SUB
							&& x86_last2->operands[0].type == X86_OP_REG
							&& x86_last2->operands[0].reg == X86_REG_RSP
							&& x86_last2->operands[1].type == X86_OP_IMM
							&& x86_last2->operands[1].imm == 0x8)
						) && insn[last].id == X86_INS_PUSH)
					{
						//只留第二个PUSH
						DbgFunctions()->AssembleAtEx(insn[last2].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[last].address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(insn[i].address, "NOP", 0, true);
						if (x86_last->operands[0].type == X86_OP_IMM)
						{
							Environment env;
							env.setArch(Arch::kX64);
							//Asmjit 初始化
							CodeHolder code;
							code.init(env);
							x86::Assembler a(&code);
							a.push(x86_last->operands[0].imm);
							PBYTE tCode = new BYTE[code.codeSize()];
							code.copyFlattenedData(tCode, code.codeSize());
							DbgFunctions()->MemPatch(insn[last2].address, tCode, code.codeSize());
							delete[] tCode;
							modified = true;
							code.reset();
							break;
						}
						else if (x86_last->operands[0].type == X86_OP_REG)
						{
							Environment env;
							env.setArch(Arch::kX64);
							//Asmjit 初始化
							CodeHolder code;
							code.init(env);
							x86::Assembler a(&code);
							a.push(cvrtCsRegToGp(x86_last->operands[0].reg));
							PBYTE tCode = new BYTE[code.codeSize()];
							code.copyFlattenedData(tCode, code.codeSize());
							DbgFunctions()->MemPatch(insn[last2].address, tCode, code.codeSize());
							delete[] tCode;
							modified = true;
							code.reset();
							break;
						}
						else
						{
							//暂不处理
						}
					}
				}

			}
		}


		// 处理 push reg; mov reg, imm; mov [rsp+8], reg; pop reg 的情况
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG) {
				x86_reg pop_reg = x86_this->operands[0].reg;

				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;
				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;
				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;

				// 验证四指令模式
				const cs_insn* push_insn = &insn[last3];
				const cs_insn* mov_imm_insn = &insn[last2];
				const cs_insn* mov_mem_insn = &insn[last1];
				const cs_insn* pop_insn = &insn[i];

				if (push_insn->id == X86_INS_PUSH &&
					push_insn->detail->x86.operands[0].type == X86_OP_REG &&
					push_insn->detail->x86.operands[0].reg == pop_reg &&

					mov_imm_insn->id == X86_INS_MOVABS &&
					mov_imm_insn->detail->x86.operands[0].type == X86_OP_REG &&
					mov_imm_insn->detail->x86.operands[0].reg == pop_reg &&

					mov_imm_insn->detail->x86.operands[1].type == X86_OP_IMM &&
					mov_mem_insn->id == X86_INS_MOV &&
					mov_mem_insn->detail->x86.operands[0].type == X86_OP_MEM &&

					mov_mem_insn->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					mov_mem_insn->detail->x86.operands[0].mem.disp == 8 &&
					mov_mem_insn->detail->x86.operands[1].type == X86_OP_REG &&

					mov_mem_insn->detail->x86.operands[1].reg == pop_reg &&
					pop_insn->detail->x86.operands[0].reg == pop_reg) {

					// 获取立即数值
					uint64_t imm = mov_imm_insn->detail->x86.operands[1].imm;

					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_insn->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_imm_insn->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_mem_insn->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_insn->address, "NOP", 0, true);

					// 生成 MOV [RSP], imm
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.mov(qword_ptr(rsp), imm);

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_insn->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();

					modified = true;
					break;
				}
			}
		}

		// 处理 push rcx; mov rcx, r15; mov r15, rcx; pop rcx;
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG) {
				x86_reg pop_reg = x86_this->operands[0].reg;

				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;
				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;
				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;

				// 验证四指令模式
				const cs_insn* push_insn = &insn[last3];
				const cs_insn* mov_reg2_reg1 = &insn[last2];
				const cs_insn* mov_reg1_reg2 = &insn[last1];
				const cs_insn* pop_insn = &insn[i];


				if (push_insn->id == X86_INS_PUSH &&
					push_insn->detail->x86.operands[0].type == X86_OP_REG &&
					push_insn->detail->x86.operands[0].reg == pop_reg &&

					mov_reg2_reg1->id == X86_INS_MOV &&
					mov_reg2_reg1->detail->x86.operands[0].type == X86_OP_REG &&
					mov_reg2_reg1->detail->x86.operands[0].reg == pop_reg &&
					mov_reg2_reg1->detail->x86.operands[1].type == X86_OP_REG &&
					mov_reg2_reg1->detail->x86.operands[1].reg == mov_reg1_reg2->detail->x86.operands[0].reg &&

					mov_reg1_reg2->id == X86_INS_MOV &&
					mov_reg1_reg2->detail->x86.operands[0].type == X86_OP_REG &&
					mov_reg1_reg2->detail->x86.operands[0].reg == mov_reg2_reg1->detail->x86.operands[1].reg &&
					mov_reg1_reg2->detail->x86.operands[1].type == X86_OP_REG &&
					mov_reg1_reg2->detail->x86.operands[1].reg == pop_reg &&

					pop_insn->detail->x86.operands[0].reg == pop_reg) {

					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_insn->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_reg2_reg1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_reg1_reg2->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_insn->address, "NOP", 0, true);

					modified = true;
					break;
				}
			}
		}

		// 处理
		/*
		xor rdi,qword ptr ss:[rsp]
		xor qword ptr ss:[rsp],rdi
		xor rdi,qword ptr ss:[rsp]
		*/
		if (insn[i].id == X86_INS_XOR) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;
				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				// 验证四指令模式
				const cs_insn* xor_rdi_mem = &insn[last2];
				const cs_insn* xor_mem_rdi = &insn[last1];
				const cs_insn* xor_rdi_mem_1 = &insn[i];


				if (xor_rdi_mem->id == X86_INS_XOR &&
					xor_rdi_mem->detail->x86.operands[0].type == X86_OP_REG &&
					xor_rdi_mem->detail->x86.operands[0].reg == xor_rdi_mem_1->detail->x86.operands[0].reg &&
					xor_rdi_mem->detail->x86.operands[1].type == X86_OP_MEM &&
					xor_rdi_mem->detail->x86.operands[1].mem.base == X86_REG_RSP &&
					xor_rdi_mem->detail->x86.operands[1].mem.disp == 0 &&

					xor_mem_rdi->id == X86_INS_XOR &&
					xor_mem_rdi->detail->x86.operands[1].type == X86_OP_REG &&
					xor_mem_rdi->detail->x86.operands[1].reg == xor_rdi_mem_1->detail->x86.operands[0].reg &&
					xor_mem_rdi->detail->x86.operands[0].type == X86_OP_MEM &&
					xor_mem_rdi->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					xor_mem_rdi->detail->x86.operands[0].mem.disp == 0 &&

					xor_rdi_mem_1->id == X86_INS_XOR &&
					xor_rdi_mem_1->detail->x86.operands[0].type == X86_OP_REG &&
					xor_rdi_mem_1->detail->x86.operands[1].type == X86_OP_MEM &&
					xor_rdi_mem_1->detail->x86.operands[1].mem.base == X86_REG_RSP &&
					xor_rdi_mem_1->detail->x86.operands[1].mem.disp == 0)
				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(xor_rdi_mem->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xor_mem_rdi->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xor_rdi_mem_1->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.xchg(qword_ptr(rsp), cvrtCsRegToGp(xor_rdi_mem_1->detail->x86.operands[0].reg));

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(xor_rdi_mem->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}

		// 处理
		/*
		add reg, Imm1
		add reg, Imm2
		|Imm1 + Imm2| <= 0xFFFF
		*/
		if (insn[i].id == X86_INS_ADD)
		{
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG)
			{
				// 获取前一条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				const cs_insn* add_r1 = &insn[last1];
				const cs_insn* add_r2 = &insn[i];

				if (add_r1->id == X86_INS_ADD &&
					add_r2->id == X86_INS_ADD &&
					add_r1->detail->x86.operands[0].type == X86_OP_REG &&
					add_r2->detail->x86.operands[0].type == X86_OP_REG &&
					add_r1->detail->x86.operands[1].type == X86_OP_IMM &&
					add_r2->detail->x86.operands[1].type == X86_OP_IMM)
				{
					// 关键改进点：检查寄存器是否完全相同（包括位宽）
					x86_reg reg1 = add_r1->detail->x86.operands[0].reg;
					x86_reg reg2 = add_r2->detail->x86.operands[0].reg;
					if (reg1 != reg2)
						continue; // 寄存器不同则跳过

					uint64_t imm_total =
						add_r1->detail->x86.operands[1].imm +
						add_r2->detail->x86.operands[1].imm;
					if (imm_total > 0xffff) continue;



					// 清除原指令
					DbgFunctions()->AssembleAtEx(add_r1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(add_r2->address, "NOP", 0, true);

					// 生成新的 ADD 指令
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.add(cvrtCsRegToGp(add_r1->detail->x86.operands[0].reg),
						imm_total);

					// 写入原第一条 ADD 的地址
					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(add_r1->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();

					modified = true;
					break;
				}
			}
		}

		// 处理
		/*
		add reg1, Imm1
		# add reg1, reg2 
		sub reg1, Imm1
		=>
		add reg1, reg2
		*/
		if (insn[i].id == X86_INS_ADD)
		{
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG)
			{
				// 获取前一条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				const cs_insn* add_r1 = &insn[last1];
				const cs_insn* add_r2 = &insn[i];

				if (add_r1->id == X86_INS_ADD &&
					add_r2->id == X86_INS_ADD &&
					add_r1->detail->x86.operands[0].type == X86_OP_REG &&
					add_r2->detail->x86.operands[0].type == X86_OP_REG &&
					add_r1->detail->x86.operands[1].type == X86_OP_IMM &&
					add_r2->detail->x86.operands[1].type == X86_OP_IMM)
				{
					// 关键改进点：检查寄存器是否完全相同（包括位宽）
					x86_reg reg1 = add_r1->detail->x86.operands[0].reg;
					x86_reg reg2 = add_r2->detail->x86.operands[0].reg;
					if (reg1 != reg2)
						continue; // 寄存器不同则跳过

					uint64_t imm_total =
						add_r1->detail->x86.operands[1].imm +
						add_r2->detail->x86.operands[1].imm;
					if (imm_total > 0xffff) continue;



					// 清除原指令
					DbgFunctions()->AssembleAtEx(add_r1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(add_r2->address, "NOP", 0, true);

					// 生成新的 ADD 指令
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.add(cvrtCsRegToGp(add_r1->detail->x86.operands[0].reg),
						imm_total);

					// 写入原第一条 ADD 的地址
					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(add_r1->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();

					modified = true;
					break;
				}
			}
		}

		//// 处理
		///*
		//sub reg, Imm1
		//sub reg, Imm2
		//*/
		//if (insn[i].id == X86_INS_SUB) {
		//	cs_x86* x86_this = &(insn[i].detail->x86);
		//	if (x86_this->operands[0].type == X86_OP_REG) {
		//		// 获取前三条有效指令索引
		//		int last1 = getLastInsIndex(insn, i);
		//		if (last1 == -1) continue;


		//		// 验证四指令模式
		//		const cs_insn* add_r1 = &insn[last1];
		//		const cs_insn* add_r2 = &insn[i];


		//		if (add_r1->id == X86_INS_SUB && add_r2->id == X86_INS_SUB &&

		//			add_r1->detail->x86.operands[0].type == X86_OP_REG &&
		//			add_r2->detail->x86.operands[0].type == X86_OP_REG &&

		//			add_r1->detail->x86.operands[1].type == X86_OP_IMM &&
		//			add_r2->detail->x86.operands[1].type == X86_OP_IMM &&

		//			add_r1->detail->x86.operands[0].reg == add_r2->detail->x86.operands[0].reg)
		//		{
		//			// 清除原指令
		//			DbgFunctions()->AssembleAtEx(add_r1->address, "NOP", 0, true);
		//			DbgFunctions()->AssembleAtEx(add_r2->address, "NOP", 0, true);


		//			// 生成 XCHG [RSP], reg
		//			Environment env;
		//			env.setArch(Arch::kX64);
		//			CodeHolder code;
		//			code.init(env);
		//			x86::Assembler a(&code);
		//			a.sub(cvrtCsRegToGp(add_r1->detail->x86.operands[0].reg),
		//				add_r1->detail->x86.operands[1].imm + add_r2->detail->x86.operands[1].imm);

		//			PBYTE tCode = new BYTE[code.codeSize()];
		//			code.copyFlattenedData(tCode, code.codeSize());
		//			DbgFunctions()->MemPatch(add_r2->address, tCode, code.codeSize());
		//			delete[] tCode;
		//			code.reset();

		//			modified = true;
		//			break;
		//		}
		//	}
		//}

		//// 处理
		///*
		//sub reg, Imm1
		//add reg, Imm2
		//*/
		//if (insn[i].id == X86_INS_ADD) {
		//	cs_x86* x86_this = &(insn[i].detail->x86);
		//	if (x86_this->operands[0].type == X86_OP_REG) {
		//		// 获取前三条有效指令索引
		//		int last1 = getLastInsIndex(insn, i);
		//		if (last1 == -1) continue;


		//		// 验证四指令模式
		//		const cs_insn* add_r1 = &insn[last1];
		//		const cs_insn* add_r2 = &insn[i];


		//		if (add_r1->id == X86_INS_SUB && add_r2->id == X86_INS_ADD &&

		//			add_r1->detail->x86.operands[0].type == X86_OP_REG &&
		//			add_r2->detail->x86.operands[0].type == X86_OP_REG &&

		//			add_r1->detail->x86.operands[1].type == X86_OP_IMM &&
		//			add_r2->detail->x86.operands[1].type == X86_OP_IMM &&

		//			add_r1->detail->x86.operands[0].reg == add_r2->detail->x86.operands[0].reg)
		//		{
		//			// 清除原指令
		//			DbgFunctions()->AssembleAtEx(add_r1->address, "NOP", 0, true);
		//			DbgFunctions()->AssembleAtEx(add_r2->address, "NOP", 0, true);


		//			// 生成 XCHG [RSP], reg
		//			Environment env;
		//			env.setArch(Arch::kX64);
		//			CodeHolder code;
		//			code.init(env);
		//			x86::Assembler a(&code);
		//			a.add(cvrtCsRegToGp(add_r1->detail->x86.operands[0].reg),
		//				-add_r1->detail->x86.operands[1].imm + add_r2->detail->x86.operands[1].imm);

		//			PBYTE tCode = new BYTE[code.codeSize()];
		//			code.copyFlattenedData(tCode, code.codeSize());
		//			DbgFunctions()->MemPatch(add_r2->address, tCode, code.codeSize());
		//			delete[] tCode;
		//			code.reset();

		//			modified = true;
		//			break;
		//		}
		//	}
		//}
		//
		//// 处理
		///*
		//add reg, Imm1
		//sub reg, Imm2
		//*/
		//if (insn[i].id == X86_INS_SUB) {
		//	cs_x86* x86_this = &(insn[i].detail->x86);
		//	if (x86_this->operands[0].type == X86_OP_REG) {
		//		// 获取前三条有效指令索引
		//		int last1 = getLastInsIndex(insn, i);
		//		if (last1 == -1) continue;


		//		// 验证四指令模式
		//		const cs_insn* add_r1 = &insn[last1];
		//		const cs_insn* add_r2 = &insn[i];


		//		if (add_r1->id == X86_INS_ADD && add_r2->id == X86_INS_SUB &&

		//			add_r1->detail->x86.operands[0].type == X86_OP_REG &&
		//			add_r2->detail->x86.operands[0].type == X86_OP_REG &&

		//			add_r1->detail->x86.operands[1].type == X86_OP_IMM &&
		//			add_r2->detail->x86.operands[1].type == X86_OP_IMM &&

		//			add_r1->detail->x86.operands[0].reg == add_r2->detail->x86.operands[0].reg)
		//		{
		//			// 清除原指令
		//			DbgFunctions()->AssembleAtEx(add_r1->address, "NOP", 0, true);
		//			DbgFunctions()->AssembleAtEx(add_r2->address, "NOP", 0, true);


		//			// 生成 XCHG [RSP], reg
		//			Environment env;
		//			env.setArch(Arch::kX64);
		//			CodeHolder code;
		//			code.init(env);
		//			x86::Assembler a(&code);
		//			a.add(cvrtCsRegToGp(add_r1->detail->x86.operands[0].reg),
		//				add_r1->detail->x86.operands[1].imm - add_r2->detail->x86.operands[1].imm);

		//			PBYTE tCode = new BYTE[code.codeSize()];
		//			code.copyFlattenedData(tCode, code.codeSize());
		//			DbgFunctions()->MemPatch(add_r2->address, tCode, code.codeSize());
		//			delete[] tCode;
		//			code.reset();

		//			modified = true;
		//			break;
		//		}
		//	}
		//}



		// 处理
		/*
		push rdi
		mov rdi,rsp
		add rdi,0x10
		xchg qword ptr ss:[rsp],rdi
		pop rsp
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg == X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;

				int last4 = getLastInsIndex(insn, last3);
				if (last4 == -1) continue;


				// 验证四指令模式
				const cs_insn* push_rdi = &insn[last4];
				const cs_insn* mov_rdi_rsp = &insn[last3];
				const cs_insn* add_rdi_16 = &insn[last2];
				const cs_insn* xchg_memRSP_rdi = &insn[last1];
				const cs_insn* pop_rsp = &insn[i];
				auto mainREG = push_rdi->detail->x86.operands[0].reg;

				if (push_rdi->id == X86_INS_PUSH &&
					push_rdi->detail->x86.operands[0].type == X86_OP_REG &&

					mov_rdi_rsp->id == X86_INS_MOV &&
					mov_rdi_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					mov_rdi_rsp->detail->x86.operands[0].reg == mainREG &&
					mov_rdi_rsp->detail->x86.operands[1].type == X86_OP_REG &&
					mov_rdi_rsp->detail->x86.operands[1].reg == X86_REG_RSP &&

					add_rdi_16->id == X86_INS_ADD &&
					add_rdi_16->detail->x86.operands[0].type == X86_OP_REG &&
					add_rdi_16->detail->x86.operands[0].reg == mainREG &&
					add_rdi_16->detail->x86.operands[1].type == X86_OP_IMM &&
					add_rdi_16->detail->x86.operands[1].imm == 0x10 &&

					xchg_memRSP_rdi->id == X86_INS_XCHG &&
					xchg_memRSP_rdi->detail->x86.operands[0].type == X86_OP_MEM &&
					xchg_memRSP_rdi->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					xchg_memRSP_rdi->detail->x86.operands[0].mem.disp == 0 &&
					xchg_memRSP_rdi->detail->x86.operands[1].type == X86_OP_REG &&
					xchg_memRSP_rdi->detail->x86.operands[1].reg == mainREG &&

					pop_rsp->id == X86_INS_POP&&
					pop_rsp->detail->x86.operands[0].type == X86_OP_REG&&
					pop_rsp->detail->x86.operands[0].reg == X86_REG_RSP)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_rdi->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_rdi_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(add_rdi_16->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xchg_memRSP_rdi->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_rsp->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.add(rsp, 0x8);

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_rdi->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


		// 处理
		/*
		push rdi
		mov rdi,rsp
		xchg qword ptr ss:[rsp],rdi
		pop rsp
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg == X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;


				// 验证四指令模式
				const cs_insn* push_rdi = &insn[last3];
				const cs_insn* mov_rdi_rsp = &insn[last2];
				const cs_insn* xchg_memRSP_rdi = &insn[last1];
				const cs_insn* pop_rsp = &insn[i];
				auto mainREG = push_rdi->detail->x86.operands[0].reg;

				if (push_rdi->id == X86_INS_PUSH &&
					push_rdi->detail->x86.operands[0].type == X86_OP_REG &&

					mov_rdi_rsp->id == X86_INS_MOV &&
					mov_rdi_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					mov_rdi_rsp->detail->x86.operands[0].reg == mainREG &&
					mov_rdi_rsp->detail->x86.operands[1].type == X86_OP_REG &&
					mov_rdi_rsp->detail->x86.operands[1].reg == X86_REG_RSP &&

					xchg_memRSP_rdi->id == X86_INS_XCHG &&
					xchg_memRSP_rdi->detail->x86.operands[0].type == X86_OP_MEM &&
					xchg_memRSP_rdi->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					xchg_memRSP_rdi->detail->x86.operands[0].mem.disp == 0 &&
					xchg_memRSP_rdi->detail->x86.operands[1].type == X86_OP_REG &&
					xchg_memRSP_rdi->detail->x86.operands[1].reg == mainREG &&

					pop_rsp->id == X86_INS_POP &&
					pop_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					pop_rsp->detail->x86.operands[0].reg == X86_REG_RSP)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_rdi->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_rdi_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xchg_memRSP_rdi->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_rsp->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.sub(rsp, 0x8);
					a.mov(qword_ptr(rsp), rsp);

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_rdi->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


		// 处理
		/*
		push rbp
		mov rbp, reg1
		mov qword ptr ss:[rsp + 8], reg1
		pop rbp
		=>
		mov qword ptr ss:[rsp], reg1
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg != X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;


				// 验证四指令模式
				const cs_insn* push_rdi = &insn[last3];
				const cs_insn* mov_rdi_rsp = &insn[last2];
				const cs_insn* xchg_memRSP_rdi = &insn[last1];
				const cs_insn* pop_rsp = &insn[i];
				auto mainREG = push_rdi->detail->x86.operands[0].reg;
				auto mainREG1 = mov_rdi_rsp->detail->x86.operands[1].reg;

				if (push_rdi->id == X86_INS_PUSH &&
					push_rdi->detail->x86.operands[0].type == X86_OP_REG &&
					push_rdi->detail->x86.operands[0].reg != X86_REG_RSP &&

					mov_rdi_rsp->id == X86_INS_MOV &&
					mov_rdi_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					mov_rdi_rsp->detail->x86.operands[0].reg == mainREG &&
					mov_rdi_rsp->detail->x86.operands[1].type == X86_OP_REG &&
					mov_rdi_rsp->detail->x86.operands[1].reg != X86_REG_RSP &&

					xchg_memRSP_rdi->id == X86_INS_MOV &&
					xchg_memRSP_rdi->detail->x86.operands[0].type == X86_OP_MEM &&
					xchg_memRSP_rdi->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					xchg_memRSP_rdi->detail->x86.operands[0].mem.disp == 8 &&
					xchg_memRSP_rdi->detail->x86.operands[1].type == X86_OP_REG &&
					xchg_memRSP_rdi->detail->x86.operands[1].reg == mainREG &&

					pop_rsp->id == X86_INS_POP &&
					pop_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					pop_rsp->detail->x86.operands[0].reg == mainREG)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_rdi->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_rdi_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xchg_memRSP_rdi->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_rsp->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.mov(qword_ptr(rsp), cvrtCsRegToGp(mainREG1));

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_rdi->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}

		// 处理
		/*
		push reg2 
		mov reg2,rcx
		mov reg1,reg2
		pop reg2
		=> mov reg1, rcx
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg != X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;


				// 验证四指令模式
				const cs_insn* push_reg2 = &insn[last3];
				const cs_insn* mov_reg2_rcx = &insn[last2];
				const cs_insn* mov_reg1_reg2 = &insn[last1];
				const cs_insn* pop_reg2 = &insn[i];
				auto reg1 = mov_reg1_reg2->detail->x86.operands[0].reg;
				auto reg2 = push_reg2->detail->x86.operands[0].reg;
				auto reg_rcx = mov_reg2_rcx->detail->x86.operands[1].reg;
				if (reg1 == X86_REG_RSP || reg2 == X86_REG_RSP) continue;

				if (push_reg2->id == X86_INS_PUSH &&
					push_reg2->detail->x86.operands[0].type == X86_OP_REG &&

					mov_reg2_rcx->id == X86_INS_MOV &&
					mov_reg2_rcx->detail->x86.operands[0].type == X86_OP_REG &&
					mov_reg2_rcx->detail->x86.operands[0].reg == reg2 &&
					mov_reg2_rcx->detail->x86.operands[1].type == X86_OP_REG &&
					mov_reg2_rcx->detail->x86.operands[1].reg != X86_REG_RSP &&

					mov_reg1_reg2->id == X86_INS_MOV &&
					mov_reg1_reg2->detail->x86.operands[0].type == X86_OP_REG &&
					mov_reg1_reg2->detail->x86.operands[0].reg == reg1 &&
					mov_reg1_reg2->detail->x86.operands[1].type == X86_OP_REG &&
					mov_reg1_reg2->detail->x86.operands[1].reg == reg2 &&

					pop_reg2->id == X86_INS_POP &&
					pop_reg2->detail->x86.operands[0].type == X86_OP_REG)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_reg2->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_reg2_rcx->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_reg1_reg2->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_reg2->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.mov(cvrtCsRegToGp(reg1), cvrtCsRegToGp(reg_rcx));

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_reg2->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


		// 处理
		/*
		mov [rsp], Imm1/Regxx
		mov [rsp], Imm2/Reg
		*/
		if (insn[i].id == X86_INS_MOV) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_MEM) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				// 验证四指令模式
				const cs_insn* mov_1 = &insn[last1];
				const cs_insn* mov_2 = &insn[i];

				if (mov_1->id == X86_INS_MOV && mov_2->id == X86_INS_MOV &&
					mov_1->detail->x86.operands[0].type == X86_OP_MEM &&
					mov_1->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					mov_1->detail->x86.operands[0].mem.disp == mov_2->detail->x86.operands[0].mem.disp&&

					mov_2->detail->x86.operands[0].type == X86_OP_MEM &&
					mov_2->detail->x86.operands[0].mem.base == X86_REG_RSP&&
					mov_2->detail->x86.operands[0].mem.scale == 1 &&
					mov_2->detail->x86.operands[0].mem.index == X86_REG_INVALID
					)
				{
					// Check mop_2 is Imm/Reg
					if (mov_2->detail->x86.operands[1].type == X86_OP_IMM)
					{
						// 清除原指令
						DbgFunctions()->AssembleAtEx(mov_1->address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(mov_2->address, "NOP", 0, true);


						// 生成 XCHG [RSP], reg
						Environment env;
						env.setArch(Arch::kX64);
						CodeHolder code;
						code.init(env);
						x86::Assembler a(&code);
						a.mov(qword_ptr(rsp, mov_2->detail->x86.operands[0].mem.disp), mov_2->detail->x86.operands[1].imm);

						PBYTE tCode = new BYTE[code.codeSize()];
						code.copyFlattenedData(tCode, code.codeSize());
						DbgFunctions()->MemPatch(mov_1->address, tCode, code.codeSize());
						delete[] tCode;
						code.reset();


						modified = true;
						break;
					}
					else if (mov_2->detail->x86.operands[1].type == X86_OP_REG) {
						// 清除原指令
						DbgFunctions()->AssembleAtEx(mov_1->address, "NOP", 0, true);
						DbgFunctions()->AssembleAtEx(mov_2->address, "NOP", 0, true);


						// 生成 XCHG [RSP], reg
						Environment env;
						env.setArch(Arch::kX64);
						CodeHolder code;
						code.init(env);
						x86::Assembler a(&code);
						a.mov(qword_ptr(rsp, mov_2->detail->x86.operands[0].mem.disp), cvrtCsRegToGp(mov_2->detail->x86.operands[1].reg));

						PBYTE tCode = new BYTE[code.codeSize()];
						code.copyFlattenedData(tCode, code.codeSize());
						DbgFunctions()->MemPatch(mov_1->address, tCode, code.codeSize());
						delete[] tCode;
						code.reset();


						modified = true;
						break;
					}

					
				}
			}
		}

		// 处理
		/*
		push rcx 
		mov rcx,rbp
		xor qword ptr ss:[rsp+0x8],rcx 
		pop rcx
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;


				// 验证四指令模式
				const cs_insn* push_rcx = &insn[last3];
				const cs_insn* mov_rcx_rbp = &insn[last2];
				const cs_insn* xor_rsp_8_rcx = &insn[last1];
				const cs_insn* pop_rcx = &insn[i];
				auto mainREG = push_rcx->detail->x86.operands[0].reg;

				if (push_rcx->id == X86_INS_PUSH &&
					push_rcx->detail->x86.operands[0].type == X86_OP_REG &&

					mov_rcx_rbp->id == X86_INS_MOV &&
					mov_rcx_rbp->detail->x86.operands[0].type == X86_OP_REG &&
					mov_rcx_rbp->detail->x86.operands[0].reg == mainREG &&
					mov_rcx_rbp->detail->x86.operands[1].type == X86_OP_REG &&

					xor_rsp_8_rcx->id == X86_INS_XOR &&
					xor_rsp_8_rcx->detail->x86.operands[0].type == X86_OP_MEM &&
					xor_rsp_8_rcx->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					xor_rsp_8_rcx->detail->x86.operands[0].mem.disp == 8 &&
					xor_rsp_8_rcx->detail->x86.operands[1].type == X86_OP_REG &&
					xor_rsp_8_rcx->detail->x86.operands[1].reg == mainREG &&

					pop_rcx->id == X86_INS_POP &&
					pop_rcx->detail->x86.operands[0].type == X86_OP_REG &&
					pop_rcx->detail->x86.operands[0].reg == mainREG)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_rcx->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_rcx_rbp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xor_rsp_8_rcx->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_rcx->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.xor_(qword_ptr(rsp), cvrtCsRegToGp(mov_rcx_rbp->detail->x86.operands[1].reg));

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_rcx->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


		// 处理
		/*
		add rbp, Imm1
		ANY
		sub rbp, Imm1
		*/
		if (insn[i].id == X86_INS_SUB || insn[i].id == X86_INS_ADD) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				// 验证四指令模式
				const cs_insn* add_rbp_Imm1 = &insn[last2];
				const cs_insn* add_rbp = &insn[last1];
				const cs_insn* sub_rbp_Imm1 = &insn[i];

				if (((add_rbp_Imm1->id == X86_INS_ADD && sub_rbp_Imm1->id == X86_INS_SUB) ||
					(add_rbp_Imm1->id == X86_INS_SUB && sub_rbp_Imm1->id == X86_INS_ADD)) &&

					add_rbp_Imm1->detail->x86.operands[0].type == X86_OP_REG &&
					sub_rbp_Imm1->detail->x86.operands[0].type == X86_OP_REG &&
					add_rbp_Imm1->detail->x86.operands[0].reg != X86_REG_RSP &&
					add_rbp_Imm1->detail->x86.operands[0].reg == sub_rbp_Imm1->detail->x86.operands[0].reg &&
					add_rbp_Imm1->detail->x86.operands[1].type == X86_OP_IMM  &&
					sub_rbp_Imm1->detail->x86.operands[1].type == X86_OP_IMM &&
					sub_rbp_Imm1->detail->x86.operands[1].imm == add_rbp_Imm1->detail->x86.operands[1].imm
					)

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(add_rbp_Imm1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(sub_rbp_Imm1->address, "NOP", 0, true);

					// 中间指令不变

					modified = true;
					break;
				}
			}
		}


		// 处理
		/*
		push reg1                   
		mov reg1,rsp                
		add reg1,0x10                                
		xchg qword ptr ss:[rsp],reg1
		mov rsp,qword ptr ss:[rsp] 
		=>
		add rsp, 0x8
		*/
		if (insn[i].id == X86_INS_MOV) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg == X86_REG_RSP
				&& x86_this->operands[1].type == X86_OP_MEM && x86_this->operands[1].mem.base == X86_REG_RSP &&
				x86_this->operands[1].mem.disp == 0 && x86_this->operands[1].mem.index == X86_REG_INVALID) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;

				int last4 = getLastInsIndex(insn, last3);
				if (last4 == -1) continue;

				// 验证四指令模式
				const cs_insn* push_reg1 = &insn[last4];
				const cs_insn* mov_reg1_rsp = &insn[last3];
				const cs_insn* add_reg1_0x10 = &insn[last2];
				const cs_insn* xchg_mrsp1_reg1 = &insn[last1];
				const cs_insn* mov_rsp_mrsp = &insn[i];

				auto reg1 = push_reg1->detail->x86.operands[0].reg;


				if (push_reg1->id == X86_INS_PUSH &&
					push_reg1->detail->x86.operands[0].type == X86_OP_REG &&

					mov_reg1_rsp->id == X86_INS_MOV &&
					mov_reg1_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					mov_reg1_rsp->detail->x86.operands[0].reg == reg1 &&
					mov_reg1_rsp->detail->x86.operands[1].type == X86_OP_REG &&
					mov_reg1_rsp->detail->x86.operands[1].reg == X86_REG_RSP &&

					add_reg1_0x10->id == X86_INS_ADD &&
					add_reg1_0x10->detail->x86.operands[0].type == X86_OP_REG &&
					add_reg1_0x10->detail->x86.operands[0].reg == reg1 &&
					add_reg1_0x10->detail->x86.operands[1].type == X86_OP_IMM &&
					add_reg1_0x10->detail->x86.operands[1].imm == 0x10 &&

					xchg_mrsp1_reg1->id == X86_INS_XCHG &&
					xchg_mrsp1_reg1->detail->x86.operands[0].type == X86_OP_MEM &&
					xchg_mrsp1_reg1->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					xchg_mrsp1_reg1->detail->x86.operands[0].mem.disp == 0 &&
					xchg_mrsp1_reg1->detail->x86.operands[0].mem.index == X86_REG_INVALID &&
					xchg_mrsp1_reg1->detail->x86.operands[1].type == X86_OP_REG &&
					xchg_mrsp1_reg1->detail->x86.operands[1].reg == reg1 &&

					mov_rsp_mrsp->id == X86_INS_MOV &&
					mov_rsp_mrsp->detail->x86.operands[0].type == X86_OP_REG &&
					mov_rsp_mrsp->detail->x86.operands[0].reg == X86_REG_RSP &&
					mov_rsp_mrsp->detail->x86.operands[1].type == X86_OP_MEM &&
					mov_rsp_mrsp->detail->x86.operands[1].mem.base == X86_REG_RSP &&
					mov_rsp_mrsp->detail->x86.operands[1].mem.disp == 0 &&
					mov_rsp_mrsp->detail->x86.operands[1].mem.index == X86_REG_INVALID )

				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_reg1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_reg1_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(add_reg1_0x10->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xchg_mrsp1_reg1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_rsp_mrsp->address, "NOP", 0, true);

					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.add(rsp, 0x8);

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_reg1->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


		// 处理
		/*
		push r14
		sub rsp,0x8
		mov qword ptr ss:[rsp],rsp
		pop r14
		add r14,0x18
		xchg qword ptr ss:[rsp],r14
		pop rsp
		*/
		if (insn[i].id == X86_INS_POP) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_REG && x86_this->operands[0].reg == X86_REG_RSP) {
				// 获取前三条有效指令索引
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;

				int last4 = getLastInsIndex(insn, last3);
				if (last4 == -1) continue;

				int last5 = getLastInsIndex(insn, last4);
				if (last5 == -1) continue;

				int last6 = getLastInsIndex(insn, last5);
				if (last6 == -1) continue;


				// 验证四指令模式
				const cs_insn* push_r14 = &insn[last6];
				const cs_insn* sub_rsp_0x8 = &insn[last5];
				const cs_insn* mov_mrsp_rsp = &insn[last4];
				const cs_insn* pop_r14 = &insn[last3];
				const cs_insn* add_r14_0x18 = &insn[last2];
				const cs_insn* xchg_mrsp_r14 = &insn[last1];
				const cs_insn* pop_rsp = &insn[i];

				auto mainREG = push_r14->detail->x86.operands[0].reg;

				if (push_r14->id == X86_INS_PUSH &&
					push_r14->detail->x86.operands[0].type == X86_OP_REG &&

					sub_rsp_0x8->id == X86_INS_SUB &&
					sub_rsp_0x8->detail->x86.operands[0].type == X86_OP_REG &&
					sub_rsp_0x8->detail->x86.operands[0].reg == X86_REG_RSP &&
					sub_rsp_0x8->detail->x86.operands[1].type == X86_OP_IMM &&
					sub_rsp_0x8->detail->x86.operands[1].imm == 0x8 &&

					mov_mrsp_rsp->id == X86_INS_MOV &&
					mov_mrsp_rsp->detail->x86.operands[0].type == X86_OP_MEM &&
					mov_mrsp_rsp->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					mov_mrsp_rsp->detail->x86.operands[0].mem.disp == 0 &&
					mov_mrsp_rsp->detail->x86.operands[1].type == X86_OP_REG &&
					mov_mrsp_rsp->detail->x86.operands[1].reg == X86_REG_RSP &&

					pop_r14->id == X86_INS_POP &&
					pop_r14->detail->x86.operands[0].type == X86_OP_REG&&
					pop_r14->detail->x86.operands[0].reg == mainREG&&

					add_r14_0x18->id == X86_INS_ADD&&
					add_r14_0x18->detail->x86.operands[0].type == X86_OP_REG&&
					add_r14_0x18->detail->x86.operands[0].reg == mainREG&&
					add_r14_0x18->detail->x86.operands[1].type == X86_OP_IMM&&
					add_r14_0x18->detail->x86.operands[1].imm == 0x18 &&

					xchg_mrsp_r14->id == X86_INS_XCHG&&
					xchg_mrsp_r14->detail->x86.operands[0].type == X86_OP_MEM&&
					xchg_mrsp_r14->detail->x86.operands[0].mem.base == X86_REG_RSP&&
					xchg_mrsp_r14->detail->x86.operands[0].mem.disp == 0 &&
					xchg_mrsp_r14->detail->x86.operands[1].type == X86_OP_REG&&
					xchg_mrsp_r14->detail->x86.operands[1].reg == mainREG&&
					pop_rsp->id == X86_INS_POP&&
					pop_rsp->detail->x86.operands[0].type == X86_OP_REG&&
					pop_rsp->detail->x86.operands[0].reg == X86_REG_RSP)
				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_r14->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(sub_rsp_0x8->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_mrsp_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_r14->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(add_r14_0x18->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(xchg_mrsp_r14->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_rsp->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.add(rsp, 0x8);

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_r14->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


		// 处理
/*
push reg1
mov reg1,rsp
push reg1
mov reg1,qword ptr ss:[rsp+0x8]
pop qword ptr ss:[rsp]
pop rsp
mov qword ptr ss:[rsp],reg2
=> push r12
*/
		if (insn[i].id == X86_INS_MOV) {
			cs_x86* x86_this = &(insn[i].detail->x86);
			if (x86_this->operands[0].type == X86_OP_MEM && x86_this->operands[0].mem.base == X86_REG_RSP
				&& x86_this->operands[0].mem.disp == 0 && x86_this->operands[0].mem.index == X86_REG_INVALID) {
				int last1 = getLastInsIndex(insn, i);
				if (last1 == -1) continue;

				int last2 = getLastInsIndex(insn, last1);
				if (last2 == -1) continue;

				int last3 = getLastInsIndex(insn, last2);
				if (last3 == -1) continue;

				int last4 = getLastInsIndex(insn, last3);
				if (last4 == -1) continue;

				int last5 = getLastInsIndex(insn, last4);
				if (last5 == -1) continue;

				int last6 = getLastInsIndex(insn, last5);
				if (last6 == -1) continue;


				// 验证四指令模式
				const cs_insn* push_reg1 = &insn[last6];
				const cs_insn* mov_reg1_rsp = &insn[last5];
				const cs_insn* push_reg1_2 = &insn[last4];
				const cs_insn* mov_reg1_mrsp_0x8 = &insn[last3];
				const cs_insn* pop_mrsp = &insn[last2];
				const cs_insn* pop_rsp = &insn[last1];
				const cs_insn* mov_mrsp_reg2 = &insn[i];

				auto reg1 = push_reg1->detail->x86.operands[0].reg;
				auto reg2 = mov_mrsp_reg2->detail->x86.operands[1].reg;


				if (push_reg1->id == X86_INS_PUSH &&
					push_reg1->detail->x86.operands[0].type == X86_OP_REG &&
					push_reg1->detail->x86.operands[0].reg != X86_REG_RSP &&

					mov_reg1_rsp->id == X86_INS_MOV &&
					mov_reg1_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					mov_reg1_rsp->detail->x86.operands[0].reg == reg1 &&
					mov_reg1_rsp->detail->x86.operands[1].type == X86_OP_REG &&
					mov_reg1_rsp->detail->x86.operands[1].reg == X86_REG_RSP &&

					push_reg1_2->id == X86_INS_PUSH &&
					push_reg1_2->detail->x86.operands[0].type == X86_OP_REG &&
					push_reg1_2->detail->x86.operands[0].reg == reg1 &&

					mov_reg1_mrsp_0x8->id == X86_INS_MOV &&
					mov_reg1_mrsp_0x8->detail->x86.operands[0].type == X86_OP_REG &&

					mov_reg1_mrsp_0x8->detail->x86.operands[0].reg == reg1 &&
					mov_reg1_mrsp_0x8->detail->x86.operands[1].type == X86_OP_MEM &&
					mov_reg1_mrsp_0x8->detail->x86.operands[1].mem.base == X86_REG_RSP &&
					mov_reg1_mrsp_0x8->detail->x86.operands[1].mem.disp == 8 &&
					mov_reg1_mrsp_0x8->detail->x86.operands[1].mem.index == X86_REG_INVALID &&

					pop_mrsp->id == X86_INS_POP &&
					pop_mrsp->detail->x86.operands[0].type == X86_OP_MEM &&
					pop_mrsp->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					pop_mrsp->detail->x86.operands[0].mem.disp == 0 &&
					pop_mrsp->detail->x86.operands[0].mem.index == X86_REG_INVALID &&

					pop_rsp->id == X86_INS_POP &&
					pop_rsp->detail->x86.operands[0].type == X86_OP_REG &&
					pop_rsp->detail->x86.operands[0].reg == X86_REG_RSP &&

					mov_mrsp_reg2->id == X86_INS_MOV &&
					mov_mrsp_reg2->detail->x86.operands[0].type == X86_OP_MEM &&
					mov_mrsp_reg2->detail->x86.operands[0].mem.base == X86_REG_RSP &&
					mov_mrsp_reg2->detail->x86.operands[0].mem.disp == 0 &&
					mov_mrsp_reg2->detail->x86.operands[0].mem.index == X86_REG_INVALID &&
					mov_mrsp_reg2->detail->x86.operands[1].type == X86_OP_REG &&
					mov_mrsp_reg2->detail->x86.operands[1].reg == reg2 &&
					mov_mrsp_reg2->detail->x86.operands[1].reg != X86_REG_RSP)
				{
					// 清除原指令
					DbgFunctions()->AssembleAtEx(push_reg1->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_reg1_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(push_reg1_2->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_reg1_mrsp_0x8->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_mrsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(pop_rsp->address, "NOP", 0, true);
					DbgFunctions()->AssembleAtEx(mov_mrsp_reg2->address, "NOP", 0, true);


					// 生成 XCHG [RSP], reg
					Environment env;
					env.setArch(Arch::kX64);
					CodeHolder code;
					code.init(env);
					x86::Assembler a(&code);
					a.push(cvrtCsRegToGp(reg2));

					PBYTE tCode = new BYTE[code.codeSize()];
					code.copyFlattenedData(tCode, code.codeSize());
					DbgFunctions()->MemPatch(push_reg1->address, tCode, code.codeSize());
					delete[] tCode;
					code.reset();


					modified = true;
					break;
				}
			}
		}


	}


	return modified;
}

bool cleanCode2(const cs_insn* insn, size_t codeCount)
{
	bool modified = false;

	for (size_t i = 0; i < codeCount; i++)
	{
		if (insn[i].id == X86_INS_NOP)
		{
			continue;
		}

		int lastIndx = getLastInsIndex(insn, (int)i);
		if (lastIndx == -1)
		{
			continue;
		}

		/*
			if (insn[i].detail->regs_write_count == 1
				&& insn[lastIndx].detail->regs_write_count == 1)
			{
				if (insn[i].detail->regs_write[0] == insn[lastIndx].detail->regs_write[0])
				{
					DbgFunctions()->AssembleAtEx(insn[lastIndx].address, "NOP", 0, true);
					modified = true;
					break;
				}
			}
		*/
	}
	return modified;
}

static bool cbCommand(int argc, char* argv[])
{
	if (argc != 1)
	{
		_plugin_logputs("[" PLUGIN_NAME "] Command has no parameters\n");
		return false;
	}

	SELECTIONDATA sel;
	GuiSelectionGet(GUI_DISASSEMBLY, &sel);

	//检测是否为一行指令
	if (sel.end == sel.start)
	{
		_plugin_logprintf("[" PLUGIN_NAME "] Please Select an area (more than 1 insn).\n");
		return false;
	}

	BASIC_INSTRUCTION_INFO info;
	DbgDisasmFastAt(sel.end, &info);
	sel.end += info.size;

	csh handle;
	if (cs_open(cs_arch::CS_ARCH_X86, cs_mode::CS_MODE_64, &handle) != CS_ERR_OK)
	{
		_plugin_logprintf("[" PLUGIN_NAME "] Capstone Init Failed.\n");
		return false;
	}

	cs_option(handle, CS_OPT_DETAIL, CS_OPT_ON);
	int repeats = 0;
	while (true)
	{
		cs_insn* insn;
		BYTE* mem = new BYTE[sel.end - sel.start];
		DbgMemRead(sel.start, mem, sel.end - sel.start);
		size_t codeCount = cs_disasm(handle, (const uint8_t*)mem, sel.end - sel.start, sel.start, 0, &insn);

		//如果没有任何Clean了，退出大循环
		bool exitFlag = !cleanCode(insn, codeCount);
		if (exitFlag)
		{
			exitFlag = !cleanCode2(insn, codeCount);
		}

		GuiUpdateDisassemblyView();

		cs_free(insn, codeCount);
		delete[] mem;
		repeats++;
		if (repeats > 100)
		{
			int res = MessageBoxA(hwndDlg, "当前清理的代码过多，如果继续清理可能会卡死，是否停止？", "警告", MB_YESNO | MB_ICONASTERISK);
			if (res == IDYES)
			{
				break;
			}
			else
			{
				repeats = 0;
			}
		}
		if (exitFlag)
		{
			break;
		}
	}
	_plugin_logprintf("[" PLUGIN_NAME "] CodeCleaned!\n");
	return true;
}

bool pluginInit(PLUG_INITSTRUCT* initStruct)
{
	return true;
}

bool pluginStop()
{
	return true;
}

void pluginSetup()
{
	if (!_plugin_registercommand(pluginHandle, PLUGIN_NAME, cbCommand, true))
	{
		_plugin_logputs("[" PLUGIN_NAME "] Error registering the \"" PLUGIN_NAME "\" command!\n");
	}

	if (!_plugin_menuaddentry(hMenu, 0, PLUGIN_NAME))
	{
		_plugin_logputs("[" PLUGIN_NAME "] Error registering the \"" PLUGIN_NAME "\" menu!\n");
	}

	if (!_plugin_menuaddentry(hMenuDisasm, 1, PLUGIN_NAME))
	{
		_plugin_logputs("[" PLUGIN_NAME "] Error registering the \"" PLUGIN_NAME "\" menu!\n");
	}

	ICONDATA icon{ {0}, 0 };
	_plugin_menuseticon(hMenuDisasm, &icon);
	_plugin_registercallback(pluginHandle, CB_MENUENTRY, cbMenuEntry);
	_plugin_menuentrysethotkey(pluginHandle, 1, "F6");
}
```

`CodeCleaner/plugin.h`:

```h
#pragma once

#include "pluginmain.h"

//plugin data
#define PLUGIN_NAME "CodeCleaner"
#define PLUGIN_VERSION 1

//functions
bool pluginInit(PLUG_INITSTRUCT* initStruct);
bool pluginStop();
void pluginSetup();
```

`CodeCleaner/pluginmain.cpp`:

```cpp
#include "pluginmain.h"
#include "plugin.h"

int pluginHandle;
HWND hwndDlg;
int hMenu;
int hMenuDisasm;
int hMenuDump;
int hMenuStack;

PLUG_EXPORT bool pluginit(PLUG_INITSTRUCT* initStruct)
{
    initStruct->pluginVersion = PLUGIN_VERSION;
    initStruct->sdkVersion = PLUG_SDKVERSION;
    strncpy_s(initStruct->pluginName, PLUGIN_NAME, _TRUNCATE);
    pluginHandle = initStruct->pluginHandle;
    return pluginInit(initStruct);
}

PLUG_EXPORT bool plugstop()
{
    return pluginStop();
}

PLUG_EXPORT void plugsetup(PLUG_SETUPSTRUCT* setupStruct)
{
    _plugin_logprintf( PLUGIN_NAME " By EMatrix (Version:%d)\n", PLUGIN_VERSION);
    hwndDlg = setupStruct->hwndDlg;
    hMenu = setupStruct->hMenu;
    hMenuDisasm = setupStruct->hMenuDisasm;
    hMenuDump = setupStruct->hMenuDump;
    hMenuStack = setupStruct->hMenuStack;
    pluginSetup();
}

BOOL WINAPI DllMain(
    _In_ HINSTANCE hinstDLL,
    _In_ DWORD     fdwReason,
    _In_ LPVOID    lpvReserved
)
{
    return TRUE;
}
```

`CodeCleaner/pluginmain.h`:

```h
#pragma once

#include "pluginsdk/bridgemain.h"
#include "pluginsdk/_plugins.h"

#include "pluginsdk/_scriptapi_argument.h"
#include "pluginsdk/_scriptapi_assembler.h"
#include "pluginsdk/_scriptapi_bookmark.h"
#include "pluginsdk/_scriptapi_comment.h"
#include "pluginsdk/_scriptapi_debug.h"
#include "pluginsdk/_scriptapi_flag.h"
#include "pluginsdk/_scriptapi_function.h"
#include "pluginsdk/_scriptapi_gui.h"
#include "pluginsdk/_scriptapi_label.h"
#include "pluginsdk/_scriptapi_memory.h"
#include "pluginsdk/_scriptapi_misc.h"
#include "pluginsdk/_scriptapi_module.h"
#include "pluginsdk/_scriptapi_pattern.h"
#include "pluginsdk/_scriptapi_register.h"
#include "pluginsdk/_scriptapi_stack.h"
#include "pluginsdk/_scriptapi_symbol.h"

//#include "pluginsdk/capstone/capstone.h"
#include "pluginsdk/DeviceNameResolver/DeviceNameResolver.h"
#include "pluginsdk/jansson/jansson.h"
#include "pluginsdk/lz4/lz4file.h"
#include "pluginsdk/TitanEngine/TitanEngine.h"
#include "pluginsdk/XEDParse/XEDParse.h"
//#include "pluginsdk/yara/yara.h"

#ifdef _WIN64
#pragma comment(lib, "pluginsdk/x64dbg.lib")
#pragma comment(lib, "pluginsdk/x64bridge.lib")
//#pragma comment(lib, "pluginsdk/capstone/capstone_x64.lib")
#pragma comment(lib, "pluginsdk/DeviceNameResolver/DeviceNameResolver_x64.lib")
#pragma comment(lib, "pluginsdk/jansson/jansson_x64.lib")
#pragma comment(lib, "pluginsdk/lz4/lz4_x64.lib")
#pragma comment(lib, "pluginsdk/TitanEngine/TitanEngine_x64.lib")
#pragma comment(lib, "pluginsdk/XEDParse/XEDParse_x64.lib")
//#pragma comment(lib, "pluginsdk/yara/yara_x64.lib")
#else
#pragma comment(lib, "pluginsdk/x32dbg.lib")
#pragma comment(lib, "pluginsdk/x32bridge.lib")
//#pragma comment(lib, "pluginsdk/capstone/capstone_x86.lib")
#pragma comment(lib, "pluginsdk/DeviceNameResolver/DeviceNameResolver_x86.lib")
#pragma comment(lib, "pluginsdk/jansson/jansson_x86.lib")
#pragma comment(lib, "pluginsdk/lz4/lz4_x86.lib")
#pragma comment(lib, "pluginsdk/TitanEngine/TitanEngine_x86.lib")
#pragma comment(lib, "pluginsdk/XEDParse/XEDParse_x86.lib")
//#pragma comment(lib, "pluginsdk/yara/yara_x86.lib")
#endif //_WIN64

#define Cmd(x) DbgCmdExecDirect(x)
#define Eval(x) DbgValFromString(x)
#define PLUG_EXPORT extern "C" __declspec(dllexport)

//superglobal variables
extern int pluginHandle;
extern HWND hwndDlg;
extern int hMenu;
extern int hMenuDisasm;
extern int hMenuDump;
extern int hMenuStack;
```

`CodeCleaner/pluginsdk/DeviceNameResolver/DeviceNameResolver.h`:

```h
#ifndef _DEVICENAMERESOLVER_H
#define _DEVICENAMERESOLVER_H

#include <windows.h>

#ifdef __cplusplus
extern "C"
{
#endif

__declspec(dllexport) bool DevicePathToPathW(const wchar_t* szDevicePath, wchar_t* szPath, size_t nSizeInChars);
__declspec(dllexport) bool DevicePathToPathA(const char* szDevicePath, char* szPath, size_t nSizeInChars);
__declspec(dllexport) bool DevicePathFromFileHandleW(HANDLE hFile, wchar_t* szDevicePath, size_t nSizeInChars);
__declspec(dllexport) bool DevicePathFromFileHandleA(HANDLE hFile, char* szDevicePath, size_t nSizeInChars);
__declspec(dllexport) bool PathFromFileHandleW(HANDLE hFile, wchar_t* szPath, size_t nSizeInChars);
__declspec(dllexport) bool PathFromFileHandleA(HANDLE hFile, char* szPath, size_t nSizeInChars);

#ifdef __cplusplus
}
#endif

#endif // _DEVICENAMERESOLVER_H

```

`CodeCleaner/pluginsdk/TitanEngine/TitanEngine.h`:

```h
#ifndef TITANENGINE
#define TITANENGINE

#define TITCALL

#if _MSC_VER > 1000
#pragma once
#endif

#include <windows.h>
#include <stdint.h>

#pragma pack(push, 1)

// Global.Constant.Structure.Declaration:
// Engine.External:
#define UE_STRUCT_PE32STRUCT 1
#define UE_STRUCT_PE64STRUCT 2
#define UE_STRUCT_PESTRUCT 3
#define UE_STRUCT_IMPORTENUMDATA 4
#define UE_STRUCT_THREAD_ITEM_DATA 5
#define UE_STRUCT_LIBRARY_ITEM_DATA 6
#define UE_STRUCT_LIBRARY_ITEM_DATAW 7
#define UE_STRUCT_PROCESS_ITEM_DATA 8
#define UE_STRUCT_HANDLERARRAY 9
#define UE_STRUCT_PLUGININFORMATION 10
#define UE_STRUCT_HOOK_ENTRY 11
#define UE_STRUCT_FILE_STATUS_INFO 12
#define UE_STRUCT_FILE_FIX_INFO 13
#define UE_STRUCT_X87FPUREGISTER 14
#define UE_STRUCT_X87FPU 15
#define UE_STRUCT_TITAN_ENGINE_CONTEXT 16

#define UE_ACCESS_READ 0
#define UE_ACCESS_WRITE 1
#define UE_ACCESS_ALL 2

#define UE_HIDE_PEBONLY 0
#define UE_HIDE_BASIC 1

#define UE_PLUGIN_CALL_REASON_PREDEBUG 1
#define UE_PLUGIN_CALL_REASON_EXCEPTION 2
#define UE_PLUGIN_CALL_REASON_POSTDEBUG 3
#define UE_PLUGIN_CALL_REASON_UNHANDLEDEXCEPTION 4

#define TEE_HOOK_NRM_JUMP 1
#define TEE_HOOK_NRM_CALL 3
#define TEE_HOOK_IAT 5

#define UE_ENGINE_ALOW_MODULE_LOADING 1
#define UE_ENGINE_AUTOFIX_FORWARDERS 2
#define UE_ENGINE_PASS_ALL_EXCEPTIONS 3
#define UE_ENGINE_NO_CONSOLE_WINDOW 4
#define UE_ENGINE_BACKUP_FOR_CRITICAL_FUNCTIONS 5
#define UE_ENGINE_CALL_PLUGIN_CALLBACK 6
#define UE_ENGINE_RESET_CUSTOM_HANDLER 7
#define UE_ENGINE_CALL_PLUGIN_DEBUG_CALLBACK 8
#define UE_ENGINE_SET_DEBUG_PRIVILEGE 9
#define UE_ENGINE_SAFE_ATTACH 10
#define UE_ENGINE_MEMBP_ALT 11

#define UE_OPTION_REMOVEALL 1
#define UE_OPTION_DISABLEALL 2
#define UE_OPTION_REMOVEALLDISABLED 3
#define UE_OPTION_REMOVEALLENABLED 4

#define UE_STATIC_DECRYPTOR_XOR 1
#define UE_STATIC_DECRYPTOR_SUB 2
#define UE_STATIC_DECRYPTOR_ADD 3

#define UE_STATIC_DECRYPTOR_FOREWARD 1
#define UE_STATIC_DECRYPTOR_BACKWARD 2

#define UE_STATIC_KEY_SIZE_1 1
#define UE_STATIC_KEY_SIZE_2 2
#define UE_STATIC_KEY_SIZE_4 4
#define UE_STATIC_KEY_SIZE_8 8

#define UE_STATIC_APLIB 1
#define UE_STATIC_APLIB_DEPACK 2
#define UE_STATIC_LZMA 3

#define UE_STATIC_HASH_MD5 1
#define UE_STATIC_HASH_SHA1 2
#define UE_STATIC_HASH_CRC32 3

#define UE_RESOURCE_LANGUAGE_ANY -1

#define UE_PE_OFFSET 0
#define UE_IMAGEBASE 1
#define UE_OEP 2
#define UE_SIZEOFIMAGE 3
#define UE_SIZEOFHEADERS 4
#define UE_SIZEOFOPTIONALHEADER 5
#define UE_SECTIONALIGNMENT 6
#define UE_IMPORTTABLEADDRESS 7
#define UE_IMPORTTABLESIZE 8
#define UE_RESOURCETABLEADDRESS 9
#define UE_RESOURCETABLESIZE 10
#define UE_EXPORTTABLEADDRESS 11
#define UE_EXPORTTABLESIZE 12
#define UE_TLSTABLEADDRESS 13
#define UE_TLSTABLESIZE 14
#define UE_RELOCATIONTABLEADDRESS 15
#define UE_RELOCATIONTABLESIZE 16
#define UE_TIMEDATESTAMP 17
#define UE_SECTIONNUMBER 18
#define UE_CHECKSUM 19
#define UE_SUBSYSTEM 20
#define UE_CHARACTERISTICS 21
#define UE_NUMBEROFRVAANDSIZES 22
#define UE_BASEOFCODE 23
#define UE_BASEOFDATA 24
#define UE_DLLCHARACTERISTICS 25
//leaving some enum space here for future additions
#define UE_SECTIONNAME 40
#define UE_SECTIONVIRTUALOFFSET 41
#define UE_SECTIONVIRTUALSIZE 42
#define UE_SECTIONRAWOFFSET 43
#define UE_SECTIONRAWSIZE 44
#define UE_SECTIONFLAGS 45

#define UE_VANOTFOUND = -2;

#define UE_CH_BREAKPOINT 1
#define UE_CH_SINGLESTEP 2
#define UE_CH_ACCESSVIOLATION 3
#define UE_CH_ILLEGALINSTRUCTION 4
#define UE_CH_NONCONTINUABLEEXCEPTION 5
#define UE_CH_ARRAYBOUNDSEXCEPTION 6
#define UE_CH_FLOATDENORMALOPERAND 7
#define UE_CH_FLOATDEVIDEBYZERO 8
#define UE_CH_INTEGERDEVIDEBYZERO 9
#define UE_CH_INTEGEROVERFLOW 10
#define UE_CH_PRIVILEGEDINSTRUCTION 11
#define UE_CH_PAGEGUARD 12
#define UE_CH_EVERYTHINGELSE 13
#define UE_CH_CREATETHREAD 14
#define UE_CH_EXITTHREAD 15
#define UE_CH_CREATEPROCESS 16
#define UE_CH_EXITPROCESS 17
#define UE_CH_LOADDLL 18
#define UE_CH_UNLOADDLL 19
#define UE_CH_OUTPUTDEBUGSTRING 20
#define UE_CH_AFTEREXCEPTIONPROCESSING 21
#define UE_CH_SYSTEMBREAKPOINT 23
#define UE_CH_UNHANDLEDEXCEPTION 24
#define UE_CH_RIPEVENT 25
#define UE_CH_DEBUGEVENT 26

#define UE_OPTION_HANDLER_RETURN_HANDLECOUNT 1
#define UE_OPTION_HANDLER_RETURN_ACCESS 2
#define UE_OPTION_HANDLER_RETURN_FLAGS 3
#define UE_OPTION_HANDLER_RETURN_TYPENAME 4

#define UE_BREAKPOINT_INT3 1
#define UE_BREAKPOINT_LONG_INT3 2
#define UE_BREAKPOINT_UD2 3

#define UE_BPXREMOVED 0
#define UE_BPXACTIVE 1
#define UE_BPXINACTIVE 2

#define UE_BREAKPOINT 0
#define UE_SINGLESHOOT 1
#define UE_HARDWARE 2
#define UE_MEMORY 3
#define UE_MEMORY_READ 4
#define UE_MEMORY_WRITE 5
#define UE_MEMORY_EXECUTE 6
#define UE_BREAKPOINT_TYPE_INT3 0x10000000
#define UE_BREAKPOINT_TYPE_LONG_INT3 0x20000000
#define UE_BREAKPOINT_TYPE_UD2 0x30000000

#define UE_HARDWARE_EXECUTE 4
#define UE_HARDWARE_WRITE 5
#define UE_HARDWARE_READWRITE 6

#define UE_HARDWARE_SIZE_1 7
#define UE_HARDWARE_SIZE_2 8
#define UE_HARDWARE_SIZE_4 9
#define UE_HARDWARE_SIZE_8 10

#define UE_ON_LIB_LOAD 1
#define UE_ON_LIB_UNLOAD 2
#define UE_ON_LIB_ALL 3

#define UE_APISTART 0
#define UE_APIEND 1

#define UE_PLATFORM_x86 1
#define UE_PLATFORM_x64 2
#define UE_PLATFORM_ALL 3

#define UE_FUNCTION_STDCALL 1
#define UE_FUNCTION_CCALL 2
#define UE_FUNCTION_FASTCALL 3
#define UE_FUNCTION_STDCALL_RET 4
#define UE_FUNCTION_CCALL_RET 5
#define UE_FUNCTION_FASTCALL_RET 6
#define UE_FUNCTION_STDCALL_CALL 7
#define UE_FUNCTION_CCALL_CALL 8
#define UE_FUNCTION_FASTCALL_CALL 9
#define UE_PARAMETER_BYTE 0
#define UE_PARAMETER_WORD 1
#define UE_PARAMETER_DWORD 2
#define UE_PARAMETER_QWORD 3
#define UE_PARAMETER_PTR_BYTE 4
#define UE_PARAMETER_PTR_WORD 5
#define UE_PARAMETER_PTR_DWORD 6
#define UE_PARAMETER_PTR_QWORD 7
#define UE_PARAMETER_STRING 8
#define UE_PARAMETER_UNICODE 9

#define UE_EAX 1
#define UE_EBX 2
#define UE_ECX 3
#define UE_EDX 4
#define UE_EDI 5
#define UE_ESI 6
#define UE_EBP 7
#define UE_ESP 8
#define UE_EIP 9
#define UE_EFLAGS 10
#define UE_DR0 11
#define UE_DR1 12
#define UE_DR2 13
#define UE_DR3 14
#define UE_DR6 15
#define UE_DR7 16
#define UE_RAX 17
#define UE_RBX 18
#define UE_RCX 19
#define UE_RDX 20
#define UE_RDI 21
#define UE_RSI 22
#define UE_RBP 23
#define UE_RSP 24
#define UE_RIP 25
#define UE_RFLAGS 26
#define UE_R8 27
#define UE_R9 28
#define UE_R10 29
#define UE_R11 30
#define UE_R12 31
#define UE_R13 32
#define UE_R14 33
#define UE_R15 34
#define UE_CIP 35
#define UE_CSP 36
#ifdef _WIN64
#define UE_CFLAGS UE_RFLAGS
#else
#define UE_CFLAGS UE_EFLAGS
#endif
#define UE_SEG_GS 37
#define UE_SEG_FS 38
#define UE_SEG_ES 39
#define UE_SEG_DS 40
#define UE_SEG_CS 41
#define UE_SEG_SS 42
#define UE_x87_r0 43
#define UE_x87_r1 44
#define UE_x87_r2 45
#define UE_x87_r3 46
#define UE_x87_r4 47
#define UE_x87_r5 48
#define UE_x87_r6 49
#define UE_x87_r7 50
#define UE_X87_STATUSWORD 51
#define UE_X87_CONTROLWORD 52
#define UE_X87_TAGWORD 53
#define UE_MXCSR 54
#define UE_MMX0 55
#define UE_MMX1 56
#define UE_MMX2 57
#define UE_MMX3 58
#define UE_MMX4 59
#define UE_MMX5 60
#define UE_MMX6 61
#define UE_MMX7 62
#define UE_XMM0 63
#define UE_XMM1 64
#define UE_XMM2 65
#define UE_XMM3 66
#define UE_XMM4 67
#define UE_XMM5 68
#define UE_XMM6 69
#define UE_XMM7 70
#define UE_XMM8 71
#define UE_XMM9 72
#define UE_XMM10 73
#define UE_XMM11 74
#define UE_XMM12 75
#define UE_XMM13 76
#define UE_XMM14 77
#define UE_XMM15 78
#define UE_x87_ST0 79
#define UE_x87_ST1 80
#define UE_x87_ST2 81
#define UE_x87_ST3 82
#define UE_x87_ST4 83
#define UE_x87_ST5 84
#define UE_x87_ST6 85
#define UE_x87_ST7 86
#define UE_YMM0 87
#define UE_YMM1 88
#define UE_YMM2 89
#define UE_YMM3 90
#define UE_YMM4 91
#define UE_YMM5 92
#define UE_YMM6 93
#define UE_YMM7 94
#define UE_YMM8 95
#define UE_YMM9 96
#define UE_YMM10 97
#define UE_YMM11 98
#define UE_YMM12 99
#define UE_YMM13 100
#define UE_YMM14 101
#define UE_YMM15 102

#ifndef CONTEXT_EXTENDED_REGISTERS
#define CONTEXT_EXTENDED_REGISTERS 0
#endif

typedef struct
{
    DWORD PE32Offset;
    DWORD ImageBase;
    DWORD OriginalEntryPoint;
    DWORD BaseOfCode;
    DWORD BaseOfData;
    DWORD NtSizeOfImage;
    DWORD NtSizeOfHeaders;
    WORD SizeOfOptionalHeaders;
    DWORD FileAlignment;
    DWORD SectionAligment;
    DWORD ImportTableAddress;
    DWORD ImportTableSize;
    DWORD ResourceTableAddress;
    DWORD ResourceTableSize;
    DWORD ExportTableAddress;
    DWORD ExportTableSize;
    DWORD TLSTableAddress;
    DWORD TLSTableSize;
    DWORD RelocationTableAddress;
    DWORD RelocationTableSize;
    DWORD TimeDateStamp;
    WORD SectionNumber;
    DWORD CheckSum;
    WORD SubSystem;
    WORD Characteristics;
    DWORD NumberOfRvaAndSizes;
} PE32Struct, *PPE32Struct;

typedef struct
{
    DWORD PE64Offset;
    DWORD64 ImageBase;
    DWORD OriginalEntryPoint;
    DWORD BaseOfCode;
    DWORD BaseOfData;
    DWORD NtSizeOfImage;
    DWORD NtSizeOfHeaders;
    WORD SizeOfOptionalHeaders;
    DWORD FileAlignment;
    DWORD SectionAligment;
    DWORD ImportTableAddress;
    DWORD ImportTableSize;
    DWORD ResourceTableAddress;
    DWORD ResourceTableSize;
    DWORD ExportTableAddress;
    DWORD ExportTableSize;
    DWORD TLSTableAddress;
    DWORD TLSTableSize;
    DWORD RelocationTableAddress;
    DWORD RelocationTableSize;
    DWORD TimeDateStamp;
    WORD SectionNumber;
    DWORD CheckSum;
    WORD SubSystem;
    WORD Characteristics;
    DWORD NumberOfRvaAndSizes;
} PE64Struct, *PPE64Struct;

#if defined(_WIN64)
typedef PE64Struct PEStruct;
#else
typedef PE32Struct PEStruct;
#endif

typedef struct
{
    bool NewDll;
    int NumberOfImports;
    ULONG_PTR ImageBase;
    ULONG_PTR BaseImportThunk;
    ULONG_PTR ImportThunk;
    char* APIName;
    char* DLLName;
} ImportEnumData, *PImportEnumData;

typedef struct
{
    HANDLE hThread;
    DWORD dwThreadId;
    void* ThreadStartAddress;
    void* ThreadLocalBase;
    void* TebAddress;
    ULONG WaitTime;
    LONG Priority;
    LONG BasePriority;
    ULONG ContextSwitches;
    ULONG ThreadState;
    ULONG WaitReason;
} THREAD_ITEM_DATA, *PTHREAD_ITEM_DATA;

typedef struct
{
    HANDLE hFile;
    void* BaseOfDll;
    HANDLE hFileMapping;
    void* hFileMappingView;
    char szLibraryPath[MAX_PATH];
    char szLibraryName[MAX_PATH];
} LIBRARY_ITEM_DATA, *PLIBRARY_ITEM_DATA;

typedef struct
{
    HANDLE hFile;
    void* BaseOfDll;
    HANDLE hFileMapping;
    void* hFileMappingView;
    wchar_t szLibraryPath[MAX_PATH];
    wchar_t szLibraryName[MAX_PATH];
} LIBRARY_ITEM_DATAW, *PLIBRARY_ITEM_DATAW;

typedef struct
{
    HANDLE hProcess;
    DWORD dwProcessId;
    HANDLE hThread;
    DWORD dwThreadId;
    HANDLE hFile;
    void* BaseOfImage;
    void* ThreadStartAddress;
    void* ThreadLocalBase;
} PROCESS_ITEM_DATA, *PPROCESS_ITEM_DATA;

typedef struct
{
    ULONG ProcessId;
    HANDLE hHandle;
} HandlerArray, *PHandlerArray;

typedef struct
{
    char PluginName[64];
    DWORD PluginMajorVersion;
    DWORD PluginMinorVersion;
    HMODULE PluginBaseAddress;
    void* TitanDebuggingCallBack;
    void* TitanRegisterPlugin;
    void* TitanReleasePlugin;
    void* TitanResetPlugin;
    bool PluginDisabled;
} PluginInformation, *PPluginInformation;

#define TEE_MAXIMUM_HOOK_SIZE 14
#define TEE_MAXIMUM_HOOK_RELOCS 7
#if defined(_WIN64)
#define TEE_MAXIMUM_HOOK_INSERT_SIZE 14
#else
#define TEE_MAXIMUM_HOOK_INSERT_SIZE 5
#endif

typedef struct HOOK_ENTRY
{
    bool IATHook;
    BYTE HookType;
    DWORD HookSize;
    void* HookAddress;
    void* RedirectionAddress;
    BYTE HookBytes[TEE_MAXIMUM_HOOK_SIZE];
    BYTE OriginalBytes[TEE_MAXIMUM_HOOK_SIZE];
    void* IATHookModuleBase;
    DWORD IATHookNameHash;
    bool HookIsEnabled;
    bool HookIsRemote;
    void* PatchedEntry;
    DWORD RelocationInfo[TEE_MAXIMUM_HOOK_RELOCS];
    int RelocationCount;
} HOOK_ENTRY, *PHOOK_ENTRY;

#define UE_DEPTH_SURFACE 0
#define UE_DEPTH_DEEP 1

#define UE_UNPACKER_CONDITION_SEARCH_FROM_EP 1

#define UE_UNPACKER_CONDITION_LOADLIBRARY 1
#define UE_UNPACKER_CONDITION_GETPROCADDRESS 2
#define UE_UNPACKER_CONDITION_ENTRYPOINTBREAK 3
#define UE_UNPACKER_CONDITION_RELOCSNAPSHOT1 4
#define UE_UNPACKER_CONDITION_RELOCSNAPSHOT2 5

#define UE_FIELD_OK 0
#define UE_FIELD_BROKEN_NON_FIXABLE 1
#define UE_FIELD_BROKEN_NON_CRITICAL 2
#define UE_FIELD_BROKEN_FIXABLE_FOR_STATIC_USE 3
#define UE_FIELD_BROKEN_BUT_CAN_BE_EMULATED 4
#define UE_FIELD_FIXABLE_NON_CRITICAL 5
#define UE_FIELD_FIXABLE_CRITICAL 6
#define UE_FIELD_NOT_PRESET 7
#define UE_FIELD_NOT_PRESET_WARNING 8

#define UE_RESULT_FILE_OK 10
#define UE_RESULT_FILE_INVALID_BUT_FIXABLE 11
#define UE_RESULT_FILE_INVALID_AND_NON_FIXABLE 12
#define UE_RESULT_FILE_INVALID_FORMAT 13

typedef struct
{
    BYTE OveralEvaluation;
    bool EvaluationTerminatedByException;
    bool FileIs64Bit;
    bool FileIsDLL;
    bool FileIsConsole;
    bool MissingDependencies;
    bool MissingDeclaredAPIs;
    BYTE SignatureMZ;
    BYTE SignaturePE;
    BYTE EntryPoint;
    BYTE ImageBase;
    BYTE SizeOfImage;
    BYTE FileAlignment;
    BYTE SectionAlignment;
    BYTE ExportTable;
    BYTE RelocationTable;
    BYTE ImportTable;
    BYTE ImportTableSection;
    BYTE ImportTableData;
    BYTE IATTable;
    BYTE TLSTable;
    BYTE LoadConfigTable;
    BYTE BoundImportTable;
    BYTE COMHeaderTable;
    BYTE ResourceTable;
    BYTE ResourceData;
    BYTE SectionTable;
} FILE_STATUS_INFO, *PFILE_STATUS_INFO;

typedef struct
{
    BYTE OveralEvaluation;
    bool FixingTerminatedByException;
    bool FileFixPerformed;
    bool StrippedRelocation;
    bool DontFixRelocations;
    DWORD OriginalRelocationTableAddress;
    DWORD OriginalRelocationTableSize;
    bool StrippedExports;
    bool DontFixExports;
    DWORD OriginalExportTableAddress;
    DWORD OriginalExportTableSize;
    bool StrippedResources;
    bool DontFixResources;
    DWORD OriginalResourceTableAddress;
    DWORD OriginalResourceTableSize;
    bool StrippedTLS;
    bool DontFixTLS;
    DWORD OriginalTLSTableAddress;
    DWORD OriginalTLSTableSize;
    bool StrippedLoadConfig;
    bool DontFixLoadConfig;
    DWORD OriginalLoadConfigTableAddress;
    DWORD OriginalLoadConfigTableSize;
    bool StrippedBoundImports;
    bool DontFixBoundImports;
    DWORD OriginalBoundImportTableAddress;
    DWORD OriginalBoundImportTableSize;
    bool StrippedIAT;
    bool DontFixIAT;
    DWORD OriginalImportAddressTableAddress;
    DWORD OriginalImportAddressTableSize;
    bool StrippedCOM;
    bool DontFixCOM;
    DWORD OriginalCOMTableAddress;
    DWORD OriginalCOMTableSize;
} FILE_FIX_INFO, *PFILE_FIX_INFO;

typedef struct DECLSPEC_ALIGN(16) _XmmRegister_t
{
    ULONGLONG Low;
    LONGLONG High;
} XmmRegister_t;

typedef struct
{
    XmmRegister_t Low; //XMM/SSE part
    XmmRegister_t High; //AVX part
} YmmRegister_t;

typedef struct
{
    BYTE    data[10];
    int     st_value;
    int     tag;
} x87FPURegister_t;

typedef struct
{
    WORD   ControlWord;
    WORD   StatusWord;
    WORD   TagWord;
    DWORD   ErrorOffset;
    DWORD   ErrorSelector;
    DWORD   DataOffset;
    DWORD   DataSelector;
    DWORD   Cr0NpxState;
} x87FPU_t;

typedef struct
{
    ULONG_PTR cax;
    ULONG_PTR ccx;
    ULONG_PTR cdx;
    ULONG_PTR cbx;
    ULONG_PTR csp;
    ULONG_PTR cbp;
    ULONG_PTR csi;
    ULONG_PTR cdi;
#ifdef _WIN64
    ULONG_PTR r8;
    ULONG_PTR r9;
    ULONG_PTR r10;
    ULONG_PTR r11;
    ULONG_PTR r12;
    ULONG_PTR r13;
    ULONG_PTR r14;
    ULONG_PTR r15;
#endif //_WIN64
    ULONG_PTR cip;
    ULONG_PTR eflags;
    unsigned short gs;
    unsigned short fs;
    unsigned short es;
    unsigned short ds;
    unsigned short cs;
    unsigned short ss;
    ULONG_PTR dr0;
    ULONG_PTR dr1;
    ULONG_PTR dr2;
    ULONG_PTR dr3;
    ULONG_PTR dr6;
    ULONG_PTR dr7;
    BYTE RegisterArea[80];
    x87FPU_t x87fpu;
    DWORD MxCsr;
#ifdef _WIN64
    XmmRegister_t XmmRegisters[16];
    YmmRegister_t YmmRegisters[16];
#else // x86
    XmmRegister_t XmmRegisters[8];
    YmmRegister_t YmmRegisters[8];
#endif
} TITAN_ENGINE_CONTEXT_t;

#ifdef __cplusplus
extern "C"
{
#endif

// Global.Function.Declaration:
// TitanEngine.Dumper.functions:
__declspec(dllexport) bool TITCALL DumpProcess(HANDLE hProcess, LPVOID ImageBase, const char* szDumpFileName, ULONG_PTR EntryPoint);
__declspec(dllexport) bool TITCALL DumpProcessW(HANDLE hProcess, LPVOID ImageBase, const wchar_t* szDumpFileName, ULONG_PTR EntryPoint);
__declspec(dllexport) bool TITCALL DumpProcessEx(DWORD ProcessId, LPVOID ImageBase, const char* szDumpFileName, ULONG_PTR EntryPoint);
__declspec(dllexport) bool TITCALL DumpProcessExW(DWORD ProcessId, LPVOID ImageBase, const wchar_t* szDumpFileName, ULONG_PTR EntryPoint);
__declspec(dllexport) bool TITCALL DumpMemory(HANDLE hProcess, LPVOID MemoryStart, ULONG_PTR MemorySize, const char* szDumpFileName);
__declspec(dllexport) bool TITCALL DumpMemoryW(HANDLE hProcess, LPVOID MemoryStart, ULONG_PTR MemorySize, const wchar_t* szDumpFileName);
__declspec(dllexport) bool TITCALL DumpMemoryEx(DWORD ProcessId, LPVOID MemoryStart, ULONG_PTR MemorySize, const char* szDumpFileName);
__declspec(dllexport) bool TITCALL DumpMemoryExW(DWORD ProcessId, LPVOID MemoryStart, ULONG_PTR MemorySize, const wchar_t* szDumpFileName);
__declspec(dllexport) bool TITCALL DumpRegions(HANDLE hProcess, const char* szDumpFolder, bool DumpAboveImageBaseOnly);
__declspec(dllexport) bool TITCALL DumpRegionsW(HANDLE hProcess, const wchar_t* szDumpFolder, bool DumpAboveImageBaseOnly);
__declspec(dllexport) bool TITCALL DumpRegionsEx(DWORD ProcessId, const char* szDumpFolder, bool DumpAboveImageBaseOnly);
__declspec(dllexport) bool TITCALL DumpRegionsExW(DWORD ProcessId, const wchar_t* szDumpFolder, bool DumpAboveImageBaseOnly);
__declspec(dllexport) bool TITCALL DumpModule(HANDLE hProcess, LPVOID ModuleBase, const char* szDumpFileName);
__declspec(dllexport) bool TITCALL DumpModuleW(HANDLE hProcess, LPVOID ModuleBase, const wchar_t* szDumpFileName);
__declspec(dllexport) bool TITCALL DumpModuleEx(DWORD ProcessId, LPVOID ModuleBase, const char* szDumpFileName);
__declspec(dllexport) bool TITCALL DumpModuleExW(DWORD ProcessId, LPVOID ModuleBase, const wchar_t* szDumpFileName);
__declspec(dllexport) bool TITCALL PastePEHeader(HANDLE hProcess, LPVOID ImageBase, const char* szDebuggedFileName);
__declspec(dllexport) bool TITCALL PastePEHeaderW(HANDLE hProcess, LPVOID ImageBase, const wchar_t* szDebuggedFileName);
__declspec(dllexport) bool TITCALL ExtractSection(const char* szFileName, const char* szDumpFileName, DWORD SectionNumber);
__declspec(dllexport) bool TITCALL ExtractSectionW(const wchar_t* szFileName, const wchar_t* szDumpFileName, DWORD SectionNumber);
__declspec(dllexport) bool TITCALL ResortFileSections(const char* szFileName);
__declspec(dllexport) bool TITCALL ResortFileSectionsW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL FindOverlay(const char* szFileName, LPDWORD OverlayStart, LPDWORD OverlaySize);
__declspec(dllexport) bool TITCALL FindOverlayW(const wchar_t* szFileName, LPDWORD OverlayStart, LPDWORD OverlaySize);
__declspec(dllexport) bool TITCALL ExtractOverlay(const char* szFileName, const char* szExtactedFileName);
__declspec(dllexport) bool TITCALL ExtractOverlayW(const wchar_t* szFileName, const wchar_t* szExtactedFileName);
__declspec(dllexport) bool TITCALL AddOverlay(const char* szFileName, const char* szOverlayFileName);
__declspec(dllexport) bool TITCALL AddOverlayW(const wchar_t* szFileName, const wchar_t* szOverlayFileName);
__declspec(dllexport) bool TITCALL CopyOverlay(const char* szInFileName, const char* szOutFileName);
__declspec(dllexport) bool TITCALL CopyOverlayW(const wchar_t* szInFileName, const wchar_t* szOutFileName);
__declspec(dllexport) bool TITCALL RemoveOverlay(const char* szFileName);
__declspec(dllexport) bool TITCALL RemoveOverlayW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL MakeAllSectionsRWE(const char* szFileName);
__declspec(dllexport) bool TITCALL MakeAllSectionsRWEW(const wchar_t* szFileName);
__declspec(dllexport) long TITCALL AddNewSectionEx(const char* szFileName, const char* szSectionName, DWORD SectionSize, DWORD SectionAttributes, LPVOID SectionContent, DWORD ContentSize);
__declspec(dllexport) long TITCALL AddNewSectionExW(const wchar_t* szFileName, const char* szSectionName, DWORD SectionSize, DWORD SectionAttributes, LPVOID SectionContent, DWORD ContentSize);
__declspec(dllexport) long TITCALL AddNewSection(const char* szFileName, const char* szSectionName, DWORD SectionSize);
__declspec(dllexport) long TITCALL AddNewSectionW(const wchar_t* szFileName, const char* szSectionName, DWORD SectionSize);
__declspec(dllexport) bool TITCALL ResizeLastSection(const char* szFileName, DWORD NumberOfExpandBytes, bool AlignResizeData);
__declspec(dllexport) bool TITCALL ResizeLastSectionW(const wchar_t* szFileName, DWORD NumberOfExpandBytes, bool AlignResizeData);
__declspec(dllexport) void TITCALL SetSharedOverlay(const char* szFileName);
__declspec(dllexport) void TITCALL SetSharedOverlayW(const wchar_t* szFileName);
__declspec(dllexport) char* TITCALL GetSharedOverlay();
__declspec(dllexport) wchar_t* TITCALL GetSharedOverlayW();
__declspec(dllexport) bool TITCALL DeleteLastSection(const char* szFileName);
__declspec(dllexport) bool TITCALL DeleteLastSectionW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL DeleteLastSectionEx(const char* szFileName, DWORD NumberOfSections);
__declspec(dllexport) bool TITCALL DeleteLastSectionExW(const wchar_t* szFileName, DWORD NumberOfSections);
__declspec(dllexport) ULONG_PTR TITCALL GetPE32DataFromMappedFile(ULONG_PTR FileMapVA, DWORD WhichSection, DWORD WhichData);
__declspec(dllexport) ULONG_PTR TITCALL GetPE32Data(const char* szFileName, DWORD WhichSection, DWORD WhichData);
__declspec(dllexport) ULONG_PTR TITCALL GetPE32DataW(const wchar_t* szFileName, DWORD WhichSection, DWORD WhichData);
__declspec(dllexport) bool TITCALL GetPE32DataFromMappedFileEx(ULONG_PTR FileMapVA, LPVOID DataStorage);
__declspec(dllexport) bool TITCALL GetPE32DataEx(const char* szFileName, LPVOID DataStorage);
__declspec(dllexport) bool TITCALL GetPE32DataExW(const wchar_t* szFileName, LPVOID DataStorage);
__declspec(dllexport) bool TITCALL SetPE32DataForMappedFile(ULONG_PTR FileMapVA, DWORD WhichSection, DWORD WhichData, ULONG_PTR NewDataValue);
__declspec(dllexport) bool TITCALL SetPE32Data(const char* szFileName, DWORD WhichSection, DWORD WhichData, ULONG_PTR NewDataValue);
__declspec(dllexport) bool TITCALL SetPE32DataW(const wchar_t* szFileName, DWORD WhichSection, DWORD WhichData, ULONG_PTR NewDataValue);
__declspec(dllexport) bool TITCALL SetPE32DataForMappedFileEx(ULONG_PTR FileMapVA, LPVOID DataStorage);
__declspec(dllexport) bool TITCALL SetPE32DataEx(const char* szFileName, LPVOID DataStorage);
__declspec(dllexport) bool TITCALL SetPE32DataExW(const wchar_t* szFileName, LPVOID DataStorage);
__declspec(dllexport) long TITCALL GetPE32SectionNumberFromVA(ULONG_PTR FileMapVA, ULONG_PTR AddressToConvert);
__declspec(dllexport) ULONG_PTR TITCALL ConvertVAtoFileOffset(ULONG_PTR FileMapVA, ULONG_PTR AddressToConvert, bool ReturnType);
__declspec(dllexport) ULONG_PTR TITCALL ConvertVAtoFileOffsetEx(ULONG_PTR FileMapVA, DWORD FileSize, ULONG_PTR ImageBase, ULONG_PTR AddressToConvert, bool AddressIsRVA, bool ReturnType);
__declspec(dllexport) ULONG_PTR TITCALL ConvertFileOffsetToVA(ULONG_PTR FileMapVA, ULONG_PTR AddressToConvert, bool ReturnType);
__declspec(dllexport) ULONG_PTR TITCALL ConvertFileOffsetToVAEx(ULONG_PTR FileMapVA, DWORD FileSize, ULONG_PTR ImageBase, ULONG_PTR AddressToConvert, bool ReturnType);
__declspec(dllexport) bool TITCALL MemoryReadSafe(HANDLE hProcess, LPVOID lpBaseAddress, LPVOID lpBuffer, SIZE_T nSize, SIZE_T* lpNumberOfBytesRead);
__declspec(dllexport) bool TITCALL MemoryWriteSafe(HANDLE hProcess, LPVOID lpBaseAddress, LPCVOID lpBuffer, SIZE_T nSize, SIZE_T* lpNumberOfBytesWritten);
// TitanEngine.Realigner.functions:
__declspec(dllexport) bool TITCALL FixHeaderCheckSum(const char* szFileName);
__declspec(dllexport) bool TITCALL FixHeaderCheckSumW(const wchar_t* szFileName);
__declspec(dllexport) long TITCALL RealignPE(ULONG_PTR FileMapVA, DWORD FileSize, DWORD RealingMode);
__declspec(dllexport) long TITCALL RealignPEEx(const char* szFileName, DWORD RealingFileSize, DWORD ForcedFileAlignment);
__declspec(dllexport) long TITCALL RealignPEExW(const wchar_t* szFileName, DWORD RealingFileSize, DWORD ForcedFileAlignment);
__declspec(dllexport) bool TITCALL WipeSection(const char* szFileName, int WipeSectionNumber, bool RemovePhysically);
__declspec(dllexport) bool TITCALL WipeSectionW(const wchar_t* szFileName, int WipeSectionNumber, bool RemovePhysically);
__declspec(dllexport) bool TITCALL IsPE32FileValidEx(const char* szFileName, DWORD CheckDepth, LPVOID FileStatusInfo);
__declspec(dllexport) bool TITCALL IsPE32FileValidExW(const wchar_t* szFileName, DWORD CheckDepth, LPVOID FileStatusInfo);
__declspec(dllexport) bool TITCALL FixBrokenPE32FileEx(const char* szFileName, LPVOID FileStatusInfo, LPVOID FileFixInfo);
__declspec(dllexport) bool TITCALL FixBrokenPE32FileExW(const wchar_t* szFileName, LPVOID FileStatusInfo, LPVOID FileFixInfo);
__declspec(dllexport) bool TITCALL IsFileDLL(const char* szFileName, ULONG_PTR FileMapVA);
__declspec(dllexport) bool TITCALL IsFileDLLW(const wchar_t* szFileName, ULONG_PTR FileMapVA);
// TitanEngine.Hider.functions:
__declspec(dllexport) void* TITCALL GetPEBLocation(HANDLE hProcess);
__declspec(dllexport) void* TITCALL GetPEBLocation64(HANDLE hProcess);
__declspec(dllexport) void* TITCALL GetTEBLocation(HANDLE hThread);
__declspec(dllexport) void* TITCALL GetTEBLocation64(HANDLE hThread);
__declspec(dllexport) bool TITCALL HideDebugger(HANDLE hProcess, DWORD PatchAPILevel);
__declspec(dllexport) bool TITCALL UnHideDebugger(HANDLE hProcess, DWORD PatchAPILevel);
// TitanEngine.Relocater.functions:
__declspec(dllexport) void TITCALL RelocaterCleanup();
__declspec(dllexport) void TITCALL RelocaterInit(DWORD MemorySize, ULONG_PTR OldImageBase, ULONG_PTR NewImageBase);
__declspec(dllexport) void TITCALL RelocaterAddNewRelocation(HANDLE hProcess, ULONG_PTR RelocateAddress, DWORD RelocateState);
__declspec(dllexport) long TITCALL RelocaterEstimatedSize();
__declspec(dllexport) bool TITCALL RelocaterExportRelocation(ULONG_PTR StorePlace, DWORD StorePlaceRVA, ULONG_PTR FileMapVA);
__declspec(dllexport) bool TITCALL RelocaterExportRelocationEx(const char* szFileName, const char* szSectionName);
__declspec(dllexport) bool TITCALL RelocaterExportRelocationExW(const wchar_t* szFileName, const char* szSectionName);
__declspec(dllexport) bool TITCALL RelocaterGrabRelocationTable(HANDLE hProcess, ULONG_PTR MemoryStart, DWORD MemorySize);
__declspec(dllexport) bool TITCALL RelocaterGrabRelocationTableEx(HANDLE hProcess, ULONG_PTR MemoryStart, ULONG_PTR MemorySize, DWORD NtSizeOfImage);
__declspec(dllexport) bool TITCALL RelocaterMakeSnapshot(HANDLE hProcess, const char* szSaveFileName, LPVOID MemoryStart, ULONG_PTR MemorySize);
__declspec(dllexport) bool TITCALL RelocaterMakeSnapshotW(HANDLE hProcess, const wchar_t* szSaveFileName, LPVOID MemoryStart, ULONG_PTR MemorySize);
__declspec(dllexport) bool TITCALL RelocaterCompareTwoSnapshots(HANDLE hProcess, ULONG_PTR LoadedImageBase, ULONG_PTR NtSizeOfImage, const char* szDumpFile1, const char* szDumpFile2, ULONG_PTR MemStart);
__declspec(dllexport) bool TITCALL RelocaterCompareTwoSnapshotsW(HANDLE hProcess, ULONG_PTR LoadedImageBase, ULONG_PTR NtSizeOfImage, const wchar_t* szDumpFile1, const wchar_t* szDumpFile2, ULONG_PTR MemStart);
__declspec(dllexport) bool TITCALL RelocaterChangeFileBase(const char* szFileName, ULONG_PTR NewImageBase);
__declspec(dllexport) bool TITCALL RelocaterChangeFileBaseW(const wchar_t* szFileName, ULONG_PTR NewImageBase);
__declspec(dllexport) bool TITCALL RelocaterRelocateMemoryBlock(ULONG_PTR FileMapVA, ULONG_PTR MemoryLocation, void* RelocateMemory, DWORD RelocateMemorySize, ULONG_PTR CurrentLoadedBase, ULONG_PTR RelocateBase);
__declspec(dllexport) bool TITCALL RelocaterWipeRelocationTable(const char* szFileName);
__declspec(dllexport) bool TITCALL RelocaterWipeRelocationTableW(const wchar_t* szFileName);
// TitanEngine.Resourcer.functions:
__declspec(dllexport) ULONG_PTR TITCALL ResourcerLoadFileForResourceUse(const char* szFileName);
__declspec(dllexport) ULONG_PTR TITCALL ResourcerLoadFileForResourceUseW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL ResourcerFreeLoadedFile(LPVOID LoadedFileBase);
__declspec(dllexport) bool TITCALL ResourcerExtractResourceFromFileEx(HMODULE hFile, const char* szResourceType, const char* szResourceName, const char* szExtractedFileName);
__declspec(dllexport) bool TITCALL ResourcerExtractResourceFromFile(const char* szFileName, const char* szResourceType, const char* szResourceName, const char* szExtractedFileName);
__declspec(dllexport) bool TITCALL ResourcerExtractResourceFromFileW(const wchar_t* szFileName, char* szResourceType, const char* szResourceName, const char* szExtractedFileName);
__declspec(dllexport) bool TITCALL ResourcerFindResource(const char* szFileName, const char* szResourceType, DWORD ResourceType, const char* szResourceName, DWORD ResourceName, DWORD ResourceLanguage, PULONG_PTR pResourceData, LPDWORD pResourceSize);
__declspec(dllexport) bool TITCALL ResourcerFindResourceW(const wchar_t* szFileName, const wchar_t* szResourceType, DWORD ResourceType, const wchar_t* szResourceName, DWORD ResourceName, DWORD ResourceLanguage, PULONG_PTR pResourceData, LPDWORD pResourceSize);
__declspec(dllexport) bool TITCALL ResourcerFindResourceEx(ULONG_PTR FileMapVA, DWORD FileSize, const wchar_t* szResourceType, DWORD ResourceType, const wchar_t* szResourceName, DWORD ResourceName, DWORD ResourceLanguage, PULONG_PTR pResourceData, LPDWORD pResourceSize);
__declspec(dllexport) void TITCALL ResourcerEnumerateResource(const char* szFileName, void* CallBack);
__declspec(dllexport) void TITCALL ResourcerEnumerateResourceW(const wchar_t* szFileName, void* CallBack);
__declspec(dllexport) void TITCALL ResourcerEnumerateResourceEx(ULONG_PTR FileMapVA, DWORD FileSize, void* CallBack);
// TitanEngine.Threader.functions:
__declspec(dllexport) bool TITCALL ThreaderImportRunningThreadData(DWORD ProcessId);
__declspec(dllexport) void* TITCALL ThreaderGetThreadInfo(HANDLE hThread, DWORD ThreadId);
__declspec(dllexport) void TITCALL ThreaderEnumThreadInfo(void* EnumCallBack);
__declspec(dllexport) bool TITCALL ThreaderPauseThread(HANDLE hThread);
__declspec(dllexport) bool TITCALL ThreaderResumeThread(HANDLE hThread);
__declspec(dllexport) bool TITCALL ThreaderTerminateThread(HANDLE hThread, DWORD ThreadExitCode);
__declspec(dllexport) bool TITCALL ThreaderPauseAllThreads(bool LeaveMainRunning);
__declspec(dllexport) bool TITCALL ThreaderResumeAllThreads(bool LeaveMainPaused);
__declspec(dllexport) bool TITCALL ThreaderPauseProcess();
__declspec(dllexport) bool TITCALL ThreaderResumeProcess();
__declspec(dllexport) ULONG_PTR TITCALL ThreaderCreateRemoteThread(ULONG_PTR ThreadStartAddress, bool AutoCloseTheHandle, LPVOID ThreadPassParameter, LPDWORD ThreadId);
__declspec(dllexport) bool TITCALL ThreaderInjectAndExecuteCode(LPVOID InjectCode, DWORD StartDelta, DWORD InjectSize);
__declspec(dllexport) ULONG_PTR TITCALL ThreaderCreateRemoteThreadEx(HANDLE hProcess, ULONG_PTR ThreadStartAddress, bool AutoCloseTheHandle, LPVOID ThreadPassParameter, LPDWORD ThreadId);
__declspec(dllexport) bool TITCALL ThreaderInjectAndExecuteCodeEx(HANDLE hProcess, LPVOID InjectCode, DWORD StartDelta, DWORD InjectSize);
__declspec(dllexport) void TITCALL ThreaderSetCallBackForNextExitThreadEvent(LPVOID exitThreadCallBack);
__declspec(dllexport) bool TITCALL ThreaderIsThreadStillRunning(HANDLE hThread);
__declspec(dllexport) bool TITCALL ThreaderIsThreadActive(HANDLE hThread);
__declspec(dllexport) bool TITCALL ThreaderIsAnyThreadActive();
__declspec(dllexport) bool TITCALL ThreaderExecuteOnlyInjectedThreads();
__declspec(dllexport) ULONG_PTR TITCALL ThreaderGetOpenHandleForThread(DWORD ThreadId);
__declspec(dllexport) bool TITCALL ThreaderIsExceptionInMainThread();
// TitanEngine.Debugger.functions:
__declspec(dllexport) void* TITCALL StaticDisassembleEx(ULONG_PTR DisassmStart, LPVOID DisassmAddress);
__declspec(dllexport) void* TITCALL StaticDisassemble(LPVOID DisassmAddress);
__declspec(dllexport) void* TITCALL DisassembleEx(HANDLE hProcess, LPVOID DisassmAddress, bool ReturnInstructionType);
__declspec(dllexport) void* TITCALL Disassemble(LPVOID DisassmAddress);
__declspec(dllexport) long TITCALL StaticLengthDisassemble(LPVOID DisassmAddress);
__declspec(dllexport) long TITCALL LengthDisassembleEx(HANDLE hProcess, LPVOID DisassmAddress);
__declspec(dllexport) long TITCALL LengthDisassemble(LPVOID DisassmAddress);
__declspec(dllexport) void* TITCALL InitDebug(char* szFileName, char* szCommandLine, char* szCurrentFolder);
__declspec(dllexport) void* TITCALL InitDebugW(const wchar_t* szFileName, const wchar_t* szCommandLine, const wchar_t* szCurrentFolder);
__declspec(dllexport) void* TITCALL InitNativeDebug(char* szFileName, char* szCommandLine, char* szCurrentFolder);
__declspec(dllexport) void* TITCALL InitNativeDebugW(const wchar_t* szFileName, const wchar_t* szCommandLine, const wchar_t* szCurrentFolder);
__declspec(dllexport) void* TITCALL InitDebugEx(const char* szFileName, const char* szCommandLine, const char* szCurrentFolder, LPVOID EntryCallBack);
__declspec(dllexport) void* TITCALL InitDebugExW(const wchar_t* szFileName, const wchar_t* szCommandLine, const wchar_t* szCurrentFolder, LPVOID EntryCallBack);
__declspec(dllexport) void* TITCALL InitDLLDebug(const char* szFileName, bool ReserveModuleBase, const char* szCommandLine, const char* szCurrentFolder, LPVOID EntryCallBack);
__declspec(dllexport) void* TITCALL InitDLLDebugW(const wchar_t* szFileName, bool ReserveModuleBase, const wchar_t* szCommandLine, const wchar_t* szCurrentFolder, LPVOID EntryCallBack);
__declspec(dllexport) bool TITCALL StopDebug();
__declspec(dllexport) void TITCALL SetBPXOptions(long DefaultBreakPointType);
__declspec(dllexport) bool TITCALL IsBPXEnabled(ULONG_PTR bpxAddress);
__declspec(dllexport) bool TITCALL EnableBPX(ULONG_PTR bpxAddress);
__declspec(dllexport) bool TITCALL DisableBPX(ULONG_PTR bpxAddress);
__declspec(dllexport) bool TITCALL SetBPX(ULONG_PTR bpxAddress, DWORD bpxType, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL DeleteBPX(ULONG_PTR bpxAddress);
__declspec(dllexport) bool TITCALL SafeDeleteBPX(ULONG_PTR bpxAddress);
__declspec(dllexport) bool TITCALL SetAPIBreakPoint(const char* szDLLName, const char* szAPIName, DWORD bpxType, DWORD bpxPlace, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL DeleteAPIBreakPoint(const char* szDLLName, const char* szAPIName, DWORD bpxPlace);
__declspec(dllexport) bool TITCALL SafeDeleteAPIBreakPoint(const char* szDLLName, const char* szAPIName, DWORD bpxPlace);
__declspec(dllexport) bool TITCALL SetMemoryBPX(ULONG_PTR MemoryStart, SIZE_T SizeOfMemory, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL SetMemoryBPXEx(ULONG_PTR MemoryStart, SIZE_T SizeOfMemory, DWORD BreakPointType, bool RestoreOnHit, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL RemoveMemoryBPX(ULONG_PTR MemoryStart, SIZE_T SizeOfMemory);
__declspec(dllexport) bool TITCALL GetContextFPUDataEx(HANDLE hActiveThread, void* FPUSaveArea);
__declspec(dllexport) void TITCALL Getx87FPURegisters(x87FPURegister_t x87FPURegisters[8], TITAN_ENGINE_CONTEXT_t* titcontext);
__declspec(dllexport) void TITCALL GetMMXRegisters(uint64_t mmx[8], TITAN_ENGINE_CONTEXT_t* titcontext);
__declspec(dllexport) bool TITCALL GetFullContextDataEx(HANDLE hActiveThread, TITAN_ENGINE_CONTEXT_t* titcontext);
__declspec(dllexport) bool TITCALL SetFullContextDataEx(HANDLE hActiveThread, TITAN_ENGINE_CONTEXT_t* titcontext);
__declspec(dllexport) ULONG_PTR TITCALL GetContextDataEx(HANDLE hActiveThread, DWORD IndexOfRegister);
__declspec(dllexport) ULONG_PTR TITCALL GetContextData(DWORD IndexOfRegister);
__declspec(dllexport) bool TITCALL SetContextFPUDataEx(HANDLE hActiveThread, void* FPUSaveArea);
__declspec(dllexport) bool TITCALL SetContextDataEx(HANDLE hActiveThread, DWORD IndexOfRegister, ULONG_PTR NewRegisterValue);
__declspec(dllexport) bool TITCALL SetContextData(DWORD IndexOfRegister, ULONG_PTR NewRegisterValue);
__declspec(dllexport) bool TITCALL GetAVXContext(HANDLE hActiveThread, TITAN_ENGINE_CONTEXT_t* titcontext);
__declspec(dllexport) bool TITCALL SetAVXContext(HANDLE hActiveThread, TITAN_ENGINE_CONTEXT_t* titcontext);
__declspec(dllexport) void TITCALL ClearExceptionNumber();
__declspec(dllexport) long TITCALL CurrentExceptionNumber();
__declspec(dllexport) bool TITCALL MatchPatternEx(HANDLE hProcess, void* MemoryToCheck, int SizeOfMemoryToCheck, void* PatternToMatch, int SizeOfPatternToMatch, PBYTE WildCard);
__declspec(dllexport) bool TITCALL MatchPattern(void* MemoryToCheck, int SizeOfMemoryToCheck, void* PatternToMatch, int SizeOfPatternToMatch, PBYTE WildCard);
__declspec(dllexport) ULONG_PTR TITCALL FindEx(HANDLE hProcess, LPVOID MemoryStart, DWORD MemorySize, LPVOID SearchPattern, DWORD PatternSize, LPBYTE WildCard);
extern "C" __declspec(dllexport) ULONG_PTR TITCALL Find(LPVOID MemoryStart, DWORD MemorySize, LPVOID SearchPattern, DWORD PatternSize, LPBYTE WildCard);
__declspec(dllexport) bool TITCALL FillEx(HANDLE hProcess, LPVOID MemoryStart, DWORD MemorySize, PBYTE FillByte);
__declspec(dllexport) bool TITCALL Fill(LPVOID MemoryStart, DWORD MemorySize, PBYTE FillByte);
__declspec(dllexport) bool TITCALL PatchEx(HANDLE hProcess, LPVOID MemoryStart, DWORD MemorySize, LPVOID ReplacePattern, DWORD ReplaceSize, bool AppendNOP, bool PrependNOP);
__declspec(dllexport) bool TITCALL Patch(LPVOID MemoryStart, DWORD MemorySize, LPVOID ReplacePattern, DWORD ReplaceSize, bool AppendNOP, bool PrependNOP);
__declspec(dllexport) bool TITCALL ReplaceEx(HANDLE hProcess, LPVOID MemoryStart, DWORD MemorySize, LPVOID SearchPattern, DWORD PatternSize, DWORD NumberOfRepetitions, LPVOID ReplacePattern, DWORD ReplaceSize, PBYTE WildCard);
__declspec(dllexport) bool TITCALL Replace(LPVOID MemoryStart, DWORD MemorySize, LPVOID SearchPattern, DWORD PatternSize, DWORD NumberOfRepetitions, LPVOID ReplacePattern, DWORD ReplaceSize, PBYTE WildCard);
__declspec(dllexport) void* TITCALL GetDebugData();
__declspec(dllexport) void* TITCALL GetTerminationData();
__declspec(dllexport) long TITCALL GetExitCode();
__declspec(dllexport) ULONG_PTR TITCALL GetDebuggedDLLBaseAddress();
__declspec(dllexport) ULONG_PTR TITCALL GetDebuggedFileBaseAddress();
__declspec(dllexport) bool TITCALL GetRemoteString(HANDLE hProcess, LPVOID StringAddress, LPVOID StringStorage, int MaximumStringSize);
__declspec(dllexport) ULONG_PTR TITCALL GetFunctionParameter(HANDLE hProcess, DWORD FunctionType, DWORD ParameterNumber, DWORD ParameterType);
__declspec(dllexport) ULONG_PTR TITCALL GetJumpDestinationEx(HANDLE hProcess, ULONG_PTR InstructionAddress, bool JustJumps);
__declspec(dllexport) ULONG_PTR TITCALL GetJumpDestination(HANDLE hProcess, ULONG_PTR InstructionAddress);
__declspec(dllexport) bool TITCALL IsJumpGoingToExecuteEx(HANDLE hProcess, HANDLE hThread, ULONG_PTR InstructionAddress, ULONG_PTR RegFlags);
__declspec(dllexport) bool TITCALL IsJumpGoingToExecute();
__declspec(dllexport) void TITCALL SetCustomHandler(DWORD ExceptionId, LPVOID CallBack);
__declspec(dllexport) void TITCALL ForceClose();
__declspec(dllexport) void TITCALL StepInto(LPVOID traceCallBack);
__declspec(dllexport) void TITCALL StepOver(LPVOID traceCallBack);
__declspec(dllexport) void TITCALL StepOut(LPVOID StepOut, bool StepFinal);
__declspec(dllexport) void TITCALL SingleStep(DWORD StepCount, LPVOID StepCallBack);
__declspec(dllexport) bool TITCALL GetUnusedHardwareBreakPointRegister(LPDWORD RegisterIndex);
__declspec(dllexport) bool TITCALL SetHardwareBreakPointEx(HANDLE hActiveThread, ULONG_PTR bpxAddress, DWORD IndexOfRegister, DWORD bpxType, DWORD bpxSize, LPVOID bpxCallBack, LPDWORD IndexOfSelectedRegister);
__declspec(dllexport) bool TITCALL SetHardwareBreakPoint(ULONG_PTR bpxAddress, DWORD IndexOfRegister, DWORD bpxType, DWORD bpxSize, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL DeleteHardwareBreakPoint(DWORD IndexOfRegister);
__declspec(dllexport) bool TITCALL RemoveAllBreakPoints(DWORD RemoveOption);
__declspec(dllexport) PROCESS_INFORMATION* TITCALL TitanGetProcessInformation();
__declspec(dllexport) STARTUPINFOW* TITCALL TitanGetStartupInformation();
__declspec(dllexport) void TITCALL DebugLoop();
__declspec(dllexport) void TITCALL SetDebugLoopTimeOut(DWORD TimeOut);
__declspec(dllexport) void TITCALL SetNextDbgContinueStatus(DWORD SetDbgCode);
__declspec(dllexport) bool TITCALL AttachDebugger(DWORD ProcessId, bool KillOnExit, LPVOID DebugInfo, LPVOID CallBack);
__declspec(dllexport) bool TITCALL DetachDebugger(DWORD ProcessId);
__declspec(dllexport) bool TITCALL DetachDebuggerEx(DWORD ProcessId);
__declspec(dllexport) void TITCALL DebugLoopEx(DWORD TimeOut);
__declspec(dllexport) void TITCALL AutoDebugEx(const char* szFileName, bool ReserveModuleBase, const char* szCommandLine, const char* szCurrentFolder, DWORD TimeOut, LPVOID EntryCallBack);
__declspec(dllexport) void TITCALL AutoDebugExW(const wchar_t* szFileName, bool ReserveModuleBase, const wchar_t* szCommandLine, const wchar_t* szCurrentFolder, DWORD TimeOut, LPVOID EntryCallBack);
__declspec(dllexport) bool TITCALL IsFileBeingDebugged();
__declspec(dllexport) void TITCALL SetErrorModel(bool DisplayErrorMessages);
// TitanEngine.FindOEP.functions:
__declspec(dllexport) void TITCALL FindOEPInit();
__declspec(dllexport) bool TITCALL FindOEPGenerically(const char* szFileName, LPVOID TraceInitCallBack, LPVOID CallBack);
__declspec(dllexport) bool TITCALL FindOEPGenericallyW(const wchar_t* szFileName, LPVOID TraceInitCallBack, LPVOID CallBack);
// TitanEngine.Importer.functions:
__declspec(dllexport) void TITCALL ImporterAddNewDll(const char* szDLLName, ULONG_PTR FirstThunk);
__declspec(dllexport) void TITCALL ImporterAddNewAPI(const char* szAPIName, ULONG_PTR ThunkValue);
__declspec(dllexport) void TITCALL ImporterAddNewOrdinalAPI(ULONG_PTR OrdinalNumber, ULONG_PTR ThunkValue);
__declspec(dllexport) long TITCALL ImporterGetAddedDllCount();
__declspec(dllexport) long TITCALL ImporterGetAddedAPICount();
__declspec(dllexport) bool TITCALL ImporterExportIAT(ULONG_PTR StorePlace, ULONG_PTR FileMapVA, HANDLE hFileMap);
__declspec(dllexport) long TITCALL ImporterEstimatedSize();
__declspec(dllexport) bool TITCALL ImporterExportIATEx(const char* szDumpFileName, const char* szExportFileName, const char* szSectionName);
__declspec(dllexport) bool TITCALL ImporterExportIATExW(const wchar_t* szDumpFileName, const wchar_t* szExportFileName, const wchar_t* szSectionName = L".RL!TEv2");
__declspec(dllexport) ULONG_PTR TITCALL ImporterFindAPIWriteLocation(const char* szAPIName);
__declspec(dllexport) ULONG_PTR TITCALL ImporterFindOrdinalAPIWriteLocation(ULONG_PTR OrdinalNumber);
__declspec(dllexport) ULONG_PTR TITCALL ImporterFindAPIByWriteLocation(ULONG_PTR APIWriteLocation);
__declspec(dllexport) ULONG_PTR TITCALL ImporterFindDLLByWriteLocation(ULONG_PTR APIWriteLocation);
__declspec(dllexport) void* TITCALL ImporterGetDLLName(ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetDLLNameW(ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetAPIName(ULONG_PTR APIAddress);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetAPIOrdinalNumber(ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetAPINameEx(ULONG_PTR APIAddress, ULONG_PTR DLLBasesList);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetRemoteAPIAddress(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetRemoteAPIAddressEx(const char* szDLLName, const char* szAPIName);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetLocalAPIAddress(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetDLLNameFromDebugee(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetDLLNameFromDebugeeW(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetAPINameFromDebugee(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetAPIOrdinalNumberFromDebugee(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) long TITCALL ImporterGetDLLIndexEx(ULONG_PTR APIAddress, ULONG_PTR DLLBasesList);
__declspec(dllexport) long TITCALL ImporterGetDLLIndex(HANDLE hProcess, ULONG_PTR APIAddress, ULONG_PTR DLLBasesList);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetRemoteDLLBase(HANDLE hProcess, HMODULE LocalModuleBase);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetRemoteDLLBaseEx(HANDLE hProcess, const char* szModuleName);
__declspec(dllexport) void* TITCALL ImporterGetRemoteDLLBaseExW(HANDLE hProcess, const wchar_t* szModuleName);
__declspec(dllexport) bool TITCALL ImporterIsForwardedAPI(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetForwardedAPIName(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetForwardedDLLName(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) long TITCALL ImporterGetForwardedDLLIndex(HANDLE hProcess, ULONG_PTR APIAddress, ULONG_PTR DLLBasesList);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetForwardedAPIOrdinalNumber(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) ULONG_PTR TITCALL ImporterGetNearestAPIAddress(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) void* TITCALL ImporterGetNearestAPIName(HANDLE hProcess, ULONG_PTR APIAddress);
__declspec(dllexport) bool TITCALL ImporterCopyOriginalIAT(const char* szOriginalFile, const char* szDumpFile);
__declspec(dllexport) bool TITCALL ImporterCopyOriginalIATW(const wchar_t* szOriginalFile, const wchar_t* szDumpFile);
__declspec(dllexport) bool TITCALL ImporterLoadImportTable(const char* szFileName);
__declspec(dllexport) bool TITCALL ImporterLoadImportTableW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL ImporterMoveOriginalIAT(const char* szOriginalFile, const char* szDumpFile, const char* szSectionName);
__declspec(dllexport) bool TITCALL ImporterMoveOriginalIATW(const wchar_t* szOriginalFile, const wchar_t* szDumpFile, const char* szSectionName);
__declspec(dllexport) void TITCALL ImporterAutoSearchIAT(DWORD ProcessId, const char* szFileName, ULONG_PTR SearchStart, LPVOID pIATStart, LPVOID pIATSize);
__declspec(dllexport) void TITCALL ImporterAutoSearchIATW(DWORD ProcessIds, const wchar_t* szFileName, ULONG_PTR SearchStart, LPVOID pIATStart, LPVOID pIATSize);
__declspec(dllexport) void TITCALL ImporterAutoSearchIATEx(DWORD ProcessId, ULONG_PTR ImageBase, ULONG_PTR SearchStart, LPVOID pIATStart, LPVOID pIATSize);
__declspec(dllexport) void TITCALL ImporterEnumAddedData(LPVOID EnumCallBack);
__declspec(dllexport) long TITCALL ImporterAutoFixIATEx(DWORD ProcessId, const char* szDumpedFile, const char* szSectionName, bool DumpRunningProcess, bool RealignFile, ULONG_PTR EntryPointAddress, ULONG_PTR ImageBase, ULONG_PTR SearchStart, bool TryAutoFix, bool FixEliminations, LPVOID UnknownPointerFixCallback);
__declspec(dllexport) long TITCALL ImporterAutoFixIATExW(DWORD ProcessId, const wchar_t* szDumpedFile, const wchar_t* szSectionName, bool DumpRunningProcess, bool RealignFile, ULONG_PTR EntryPointAddress, ULONG_PTR ImageBase, ULONG_PTR SearchStart,  bool TryAutoFix, bool FixEliminations, LPVOID UnknownPointerFixCallback);
__declspec(dllexport) long TITCALL ImporterAutoFixIAT(DWORD ProcessId, const char* szDumpedFile, ULONG_PTR SearchStart);
__declspec(dllexport) long TITCALL ImporterAutoFixIATW(DWORD ProcessId, const wchar_t* szDumpedFile, ULONG_PTR SearchStart);
__declspec(dllexport) bool TITCALL ImporterDeleteAPI(DWORD_PTR apiAddr);
// Global.Engine.Hook.functions:
__declspec(dllexport) bool TITCALL HooksSafeTransitionEx(LPVOID HookAddressArray, int NumberOfHooks, bool TransitionStart);
__declspec(dllexport) bool TITCALL HooksSafeTransition(LPVOID HookAddress, bool TransitionStart);
__declspec(dllexport) bool TITCALL HooksIsAddressRedirected(LPVOID HookAddress);
__declspec(dllexport) void* TITCALL HooksGetTrampolineAddress(LPVOID HookAddress);
__declspec(dllexport) void* TITCALL HooksGetHookEntryDetails(LPVOID HookAddress);
__declspec(dllexport) bool TITCALL HooksInsertNewRedirection(LPVOID HookAddress, LPVOID RedirectTo, int HookType);
__declspec(dllexport) bool TITCALL HooksInsertNewIATRedirectionEx(ULONG_PTR FileMapVA, ULONG_PTR LoadedModuleBase, const char* szHookFunction, LPVOID RedirectTo);
__declspec(dllexport) bool TITCALL HooksInsertNewIATRedirection(const char* szModuleName, const char* szHookFunction, LPVOID RedirectTo);
__declspec(dllexport) bool TITCALL HooksRemoveRedirection(LPVOID HookAddress, bool RemoveAll);
__declspec(dllexport) bool TITCALL HooksRemoveRedirectionsForModule(HMODULE ModuleBase);
__declspec(dllexport) bool TITCALL HooksRemoveIATRedirection(const char* szModuleName, const char* szHookFunction, bool RemoveAll);
__declspec(dllexport) bool TITCALL HooksDisableRedirection(LPVOID HookAddress, bool DisableAll);
__declspec(dllexport) bool TITCALL HooksDisableRedirectionsForModule(HMODULE ModuleBase);
__declspec(dllexport) bool TITCALL HooksDisableIATRedirection(const char* szModuleName, const char* szHookFunction, bool DisableAll);
__declspec(dllexport) bool TITCALL HooksEnableRedirection(LPVOID HookAddress, bool EnableAll);
__declspec(dllexport) bool TITCALL HooksEnableRedirectionsForModule(HMODULE ModuleBase);
__declspec(dllexport) bool TITCALL HooksEnableIATRedirection(const char* szModuleName, const char* szHookFunction, bool EnableAll);
__declspec(dllexport) void TITCALL HooksScanModuleMemory(HMODULE ModuleBase, LPVOID CallBack);
__declspec(dllexport) void TITCALL HooksScanEntireProcessMemory(LPVOID CallBack);
__declspec(dllexport) void TITCALL HooksScanEntireProcessMemoryEx();
// TitanEngine.Tracer.functions:
__declspec(dllexport) void TITCALL TracerInit();
__declspec(dllexport) ULONG_PTR TITCALL TracerLevel1(HANDLE hProcess, ULONG_PTR AddressToTrace);
__declspec(dllexport) ULONG_PTR TITCALL HashTracerLevel1(HANDLE hProcess, ULONG_PTR AddressToTrace, DWORD InputNumberOfInstructions);
__declspec(dllexport) long TITCALL TracerDetectRedirection(HANDLE hProcess, ULONG_PTR AddressToTrace);
__declspec(dllexport) ULONG_PTR TITCALL TracerFixKnownRedirection(HANDLE hProcess, ULONG_PTR AddressToTrace, DWORD RedirectionId);
__declspec(dllexport) ULONG_PTR TITCALL TracerFixRedirectionViaModule(HMODULE hModuleHandle, HANDLE hProcess, ULONG_PTR AddressToTrace, DWORD IdParameter);
__declspec(dllexport) long TITCALL TracerFixRedirectionViaImpRecPlugin(HANDLE hProcess, const char* szPluginName, ULONG_PTR AddressToTrace);
// TitanEngine.Exporter.functions:
__declspec(dllexport) void TITCALL ExporterCleanup();
__declspec(dllexport) void TITCALL ExporterSetImageBase(ULONG_PTR ImageBase);
__declspec(dllexport) void TITCALL ExporterInit(DWORD MemorySize, ULONG_PTR ImageBase, DWORD ExportOrdinalBase, const char* szExportModuleName);
__declspec(dllexport) bool TITCALL ExporterAddNewExport(const char* szExportName, DWORD ExportRelativeAddress);
__declspec(dllexport) bool TITCALL ExporterAddNewOrdinalExport(DWORD OrdinalNumber, DWORD ExportRelativeAddress);
__declspec(dllexport) long TITCALL ExporterGetAddedExportCount();
__declspec(dllexport) long TITCALL ExporterEstimatedSize();
__declspec(dllexport) bool TITCALL ExporterBuildExportTable(ULONG_PTR StorePlace, ULONG_PTR FileMapVA);
__declspec(dllexport) bool TITCALL ExporterBuildExportTableEx(const char* szExportFileName, const char* szSectionName);
__declspec(dllexport) bool TITCALL ExporterBuildExportTableExW(const wchar_t* szExportFileName, const char* szSectionName);
__declspec(dllexport) bool TITCALL ExporterLoadExportTable(const char* szFileName);
__declspec(dllexport) bool TITCALL ExporterLoadExportTableW(const wchar_t* szFileName);
// TitanEngine.Librarian.functions:
__declspec(dllexport) bool TITCALL LibrarianSetBreakPoint(const char* szLibraryName, DWORD bpxType, bool SingleShoot, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL LibrarianRemoveBreakPoint(const char* szLibraryName, DWORD bpxType);
__declspec(dllexport) void* TITCALL LibrarianGetLibraryInfo(const char* szLibraryName);
__declspec(dllexport) void* TITCALL LibrarianGetLibraryInfoW(const wchar_t* szLibraryName);
__declspec(dllexport) void* TITCALL LibrarianGetLibraryInfoEx(void* BaseOfDll);
__declspec(dllexport) void* TITCALL LibrarianGetLibraryInfoExW(void* BaseOfDll);
__declspec(dllexport) void TITCALL LibrarianEnumLibraryInfo(void* EnumCallBack);
__declspec(dllexport) void TITCALL LibrarianEnumLibraryInfoW(void* EnumCallBack);
// TitanEngine.Process.functions:
__declspec(dllexport) long TITCALL GetActiveProcessId(const char* szImageName);
__declspec(dllexport) long TITCALL GetActiveProcessIdW(const wchar_t* szImageName);
__declspec(dllexport) void TITCALL EnumProcessesWithLibrary(const char* szLibraryName, void* EnumFunction);
__declspec(dllexport) HANDLE TITCALL TitanOpenProcess(DWORD dwDesiredAccess, bool bInheritHandle, DWORD dwProcessId);
__declspec(dllexport) HANDLE TITCALL TitanOpenThread(DWORD dwDesiredAccess, bool bInheritHandle, DWORD dwThreadId);
// TitanEngine.TLSFixer.functions:
__declspec(dllexport) bool TITCALL TLSBreakOnCallBack(LPVOID ArrayOfCallBacks, DWORD NumberOfCallBacks, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL TLSGrabCallBackData(const char* szFileName, LPVOID ArrayOfCallBacks, LPDWORD NumberOfCallBacks);
__declspec(dllexport) bool TITCALL TLSGrabCallBackDataW(const wchar_t* szFileName, LPVOID ArrayOfCallBacks, LPDWORD NumberOfCallBacks);
__declspec(dllexport) bool TITCALL TLSBreakOnCallBackEx(const char* szFileName, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL TLSBreakOnCallBackExW(const wchar_t* szFileName, LPVOID bpxCallBack);
__declspec(dllexport) bool TITCALL TLSRemoveCallback(const char* szFileName);
__declspec(dllexport) bool TITCALL TLSRemoveCallbackW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL TLSRemoveTable(const char* szFileName);
__declspec(dllexport) bool TITCALL TLSRemoveTableW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL TLSBackupData(const char* szFileName);
__declspec(dllexport) bool TITCALL TLSBackupDataW(const wchar_t* szFileName);
__declspec(dllexport) bool TITCALL TLSRestoreData();
__declspec(dllexport) bool TITCALL TLSBuildNewTable(ULONG_PTR FileMapVA, ULONG_PTR StorePlace, ULONG_PTR StorePlaceRVA, LPVOID ArrayOfCallBacks, DWORD NumberOfCallBacks);
__declspec(dllexport) bool TITCALL TLSBuildNewTableEx(const char* szFileName, const char* szSectionName, LPVOID ArrayOfCallBacks, DWORD NumberOfCallBacks);
__declspec(dllexport) bool TITCALL TLSBuildNewTableExW(const wchar_t* szFileName, const char* szSectionName, LPVOID ArrayOfCallBacks, DWORD NumberOfCallBacks);
// TitanEngine.TranslateName.functions:
__declspec(dllexport) void* TITCALL TranslateNativeName(const char* szNativeName);
__declspec(dllexport) void* TITCALL TranslateNativeNameW(const wchar_t* szNativeName);
// TitanEngine.Handler.functions:
__declspec(dllexport) long TITCALL HandlerGetActiveHandleCount(DWORD ProcessId);
__declspec(dllexport) bool TITCALL HandlerIsHandleOpen(DWORD ProcessId, HANDLE hHandle);
__declspec(dllexport) void* TITCALL HandlerGetHandleName(HANDLE hProcess, DWORD ProcessId, HANDLE hHandle, bool TranslateName);
__declspec(dllexport) void* TITCALL HandlerGetHandleNameW(HANDLE hProcess, DWORD ProcessId, HANDLE hHandle, bool TranslateName);
__declspec(dllexport) long TITCALL HandlerEnumerateOpenHandles(DWORD ProcessId, LPVOID HandleBuffer, DWORD MaxHandleCount);
__declspec(dllexport) ULONG_PTR TITCALL HandlerGetHandleDetails(HANDLE hProcess, DWORD ProcessId, HANDLE hHandle, DWORD InformationReturn);
__declspec(dllexport) bool TITCALL HandlerCloseRemoteHandle(HANDLE hProcess, HANDLE hHandle);
__declspec(dllexport) long TITCALL HandlerEnumerateLockHandles(char* szFileOrFolderName, bool NameIsFolder, bool NameIsTranslated, LPVOID HandleDataBuffer, DWORD MaxHandleCount);
__declspec(dllexport) long TITCALL HandlerEnumerateLockHandlesW(const wchar_t* szFileOrFolderName, bool NameIsFolder, bool NameIsTranslated, LPVOID HandleDataBuffer, DWORD MaxHandleCount);
__declspec(dllexport) bool TITCALL HandlerCloseAllLockHandles(const char* szFileOrFolderName, bool NameIsFolder, bool NameIsTranslated);
__declspec(dllexport) bool TITCALL HandlerCloseAllLockHandlesW(const wchar_t* szFileOrFolderName, bool NameIsFolder, bool NameIsTranslated);
__declspec(dllexport) bool TITCALL HandlerIsFileLocked(const char* szFileOrFolderName, bool NameIsFolder, bool NameIsTranslated);
__declspec(dllexport) bool TITCALL HandlerIsFileLockedW(const wchar_t* szFileOrFolderName, bool NameIsFolder, bool NameIsTranslated);
// TitanEngine.Handler[Mutex].functions:
__declspec(dllexport) long TITCALL HandlerEnumerateOpenMutexes(HANDLE hProcess, DWORD ProcessId, LPVOID HandleBuffer, DWORD MaxHandleCount);
__declspec(dllexport) ULONG_PTR TITCALL HandlerGetOpenMutexHandle(HANDLE hProcess, DWORD ProcessId, const char* szMutexString);
__declspec(dllexport) ULONG_PTR TITCALL HandlerGetOpenMutexHandleW(HANDLE hProcess, DWORD ProcessId, const wchar_t* szMutexString);
__declspec(dllexport) long TITCALL HandlerGetProcessIdWhichCreatedMutex(const char* szMutexString);
__declspec(dllexport) long TITCALL HandlerGetProcessIdWhichCreatedMutexW(const wchar_t* szMutexString);
// TitanEngine.Injector.functions:
__declspec(dllexport) bool TITCALL RemoteLoadLibrary(HANDLE hProcess, const char* szLibraryFile, bool WaitForThreadExit);
__declspec(dllexport) bool TITCALL RemoteLoadLibraryW(HANDLE hProcess, const wchar_t* szLibraryFile, bool WaitForThreadExit);
__declspec(dllexport) bool TITCALL RemoteFreeLibrary(HANDLE hProcess, HMODULE hModule, const char* szLibraryFile, bool WaitForThreadExit);
__declspec(dllexport) bool TITCALL RemoteFreeLibraryW(HANDLE hProcess, HMODULE hModule, const wchar_t* szLibraryFile, bool WaitForThreadExit);
__declspec(dllexport) bool TITCALL RemoteExitProcess(HANDLE hProcess, DWORD ExitCode);
// TitanEngine.StaticUnpacker.functions:
__declspec(dllexport) bool TITCALL StaticFileLoad(const char* szFileName, DWORD DesiredAccess, bool SimulateLoad, LPHANDLE FileHandle, LPDWORD LoadedSize, LPHANDLE FileMap, PULONG_PTR FileMapVA);
__declspec(dllexport) bool TITCALL StaticFileLoadW(const wchar_t* szFileName, DWORD DesiredAccess, bool SimulateLoad, LPHANDLE FileHandle, LPDWORD LoadedSize, LPHANDLE FileMap, PULONG_PTR FileMapVA);
__declspec(dllexport) bool TITCALL StaticFileUnload(const char* szFileName, bool CommitChanges, HANDLE FileHandle, DWORD LoadedSize, HANDLE FileMap, ULONG_PTR FileMapVA);
__declspec(dllexport) bool TITCALL StaticFileUnloadW(const wchar_t* szFileName, bool CommitChanges, HANDLE FileHandle, DWORD LoadedSize, HANDLE FileMap, ULONG_PTR FileMapVA);
__declspec(dllexport) bool TITCALL StaticFileOpen(const char* szFileName, DWORD DesiredAccess, LPHANDLE FileHandle, LPDWORD FileSizeLow, LPDWORD FileSizeHigh);
__declspec(dllexport) bool TITCALL StaticFileOpenW(const wchar_t* szFileName, DWORD DesiredAccess, LPHANDLE FileHandle, LPDWORD FileSizeLow, LPDWORD FileSizeHigh);
__declspec(dllexport) bool TITCALL StaticFileGetContent(HANDLE FileHandle, DWORD FilePositionLow, LPDWORD FilePositionHigh, void* Buffer, DWORD Size);
__declspec(dllexport) void TITCALL StaticFileClose(HANDLE FileHandle);
__declspec(dllexport) void TITCALL StaticMemoryDecrypt(LPVOID MemoryStart, DWORD MemorySize, DWORD DecryptionType, DWORD DecryptionKeySize, ULONG_PTR DecryptionKey);
__declspec(dllexport) void TITCALL StaticMemoryDecryptEx(LPVOID MemoryStart, DWORD MemorySize, DWORD DecryptionKeySize, void* DecryptionCallBack);
__declspec(dllexport) void TITCALL StaticMemoryDecryptSpecial(LPVOID MemoryStart, DWORD MemorySize, DWORD DecryptionKeySize, DWORD SpecDecryptionType, void* DecryptionCallBack);
__declspec(dllexport) void TITCALL StaticSectionDecrypt(ULONG_PTR FileMapVA, DWORD SectionNumber, bool SimulateLoad, DWORD DecryptionType, DWORD DecryptionKeySize, ULONG_PTR DecryptionKey);
__declspec(dllexport) bool TITCALL StaticMemoryDecompress(void* Source, DWORD SourceSize, void* Destination, DWORD DestinationSize, int Algorithm);
__declspec(dllexport) bool TITCALL StaticRawMemoryCopy(HANDLE hFile, ULONG_PTR FileMapVA, ULONG_PTR VitualAddressToCopy, DWORD Size, bool AddressIsRVA, const char* szDumpFileName);
__declspec(dllexport) bool TITCALL StaticRawMemoryCopyW(HANDLE hFile, ULONG_PTR FileMapVA, ULONG_PTR VitualAddressToCopy, DWORD Size, bool AddressIsRVA, const wchar_t* szDumpFileName);
__declspec(dllexport) bool TITCALL StaticRawMemoryCopyEx(HANDLE hFile, DWORD RawAddressToCopy, DWORD Size, const char* szDumpFileName);
__declspec(dllexport) bool TITCALL StaticRawMemoryCopyExW(HANDLE hFile, DWORD RawAddressToCopy, DWORD Size, const wchar_t* szDumpFileName);
__declspec(dllexport) bool TITCALL StaticRawMemoryCopyEx64(HANDLE hFile, DWORD64 RawAddressToCopy, DWORD64 Size, const char* szDumpFileName);
__declspec(dllexport) bool TITCALL StaticRawMemoryCopyEx64W(HANDLE hFile, DWORD64 RawAddressToCopy, DWORD64 Size, const wchar_t* szDumpFileName);
__declspec(dllexport) bool TITCALL StaticHashMemory(void* MemoryToHash, DWORD SizeOfMemory, void* HashDigest, bool OutputString, int Algorithm);
__declspec(dllexport) bool TITCALL StaticHashFileW(const wchar_t* szFileName, char* HashDigest, bool OutputString, int Algorithm);
__declspec(dllexport) bool TITCALL StaticHashFile(const char* szFileName, char* HashDigest, bool OutputString, int Algorithm);
// TitanEngine.Engine.functions:
__declspec(dllexport) void TITCALL EngineUnpackerInitialize(const char* szFileName, const char* szUnpackedFileName, bool DoLogData, bool DoRealignFile, bool DoMoveOverlay, void* EntryCallBack);
__declspec(dllexport) void TITCALL EngineUnpackerInitializeW(const wchar_t* szFileName, const wchar_t* szUnpackedFileName, bool DoLogData, bool DoRealignFile, bool DoMoveOverlay, void* EntryCallBack);
__declspec(dllexport) bool TITCALL EngineUnpackerSetBreakCondition(void* SearchStart, DWORD SearchSize, void* SearchPattern, DWORD PatternSize, DWORD PatternDelta, ULONG_PTR BreakType, bool SingleBreak, DWORD Parameter1, DWORD Parameter2);
__declspec(dllexport) void TITCALL EngineUnpackerSetEntryPointAddress(ULONG_PTR UnpackedEntryPointAddress);
__declspec(dllexport) void TITCALL EngineUnpackerFinalizeUnpacking();
// TitanEngine.Engine.functions:
__declspec(dllexport) void TITCALL SetEngineVariable(DWORD VariableId, bool VariableSet);
__declspec(dllexport) bool TITCALL EngineCreateMissingDependencies(const char* szFileName, const char* szOutputFolder, bool LogCreatedFiles);
__declspec(dllexport) bool TITCALL EngineCreateMissingDependenciesW(const wchar_t* szFileName, const wchar_t* szOutputFolder, bool LogCreatedFiles);
__declspec(dllexport) bool TITCALL EngineFakeMissingDependencies(HANDLE hProcess);
__declspec(dllexport) bool TITCALL EngineDeleteCreatedDependencies();
__declspec(dllexport) bool TITCALL EngineCreateUnpackerWindow(const char* WindowUnpackerTitle, const char* WindowUnpackerLongTitle, const char* WindowUnpackerName, const char* WindowUnpackerAuthor, void* StartUnpackingCallBack);
__declspec(dllexport) void TITCALL EngineAddUnpackerWindowLogMessage(const char* szLogMessage);
__declspec(dllexport) bool TITCALL EngineCheckStructAlignment(DWORD StructureType, ULONG_PTR StructureSize);
// Global.Engine.Extension.Functions:
__declspec(dllexport) bool TITCALL ExtensionManagerIsPluginLoaded(const char* szPluginName);
__declspec(dllexport) bool TITCALL ExtensionManagerIsPluginEnabled(const char* szPluginName);
__declspec(dllexport) bool TITCALL ExtensionManagerDisableAllPlugins();
__declspec(dllexport) bool TITCALL ExtensionManagerDisablePlugin(const char* szPluginName);
__declspec(dllexport) bool TITCALL ExtensionManagerEnableAllPlugins();
__declspec(dllexport) bool TITCALL ExtensionManagerEnablePlugin(const char* szPluginName);
__declspec(dllexport) bool TITCALL ExtensionManagerUnloadAllPlugins();
__declspec(dllexport) bool TITCALL ExtensionManagerUnloadPlugin(const char* szPluginName);
__declspec(dllexport) void* TITCALL ExtensionManagerGetPluginInfo(const char* szPluginName);

#ifdef __cplusplus
}
#endif

#pragma pack(pop)

#endif /*TITANENGINE*/

```

`CodeCleaner/pluginsdk/XEDParse/XEDParse.h`:

```h
#ifndef _XEDPARSE_H
#define _XEDPARSE_H

#include <windows.h>

//XEDParse defines
#ifdef XEDPARSE_BUILD
#define XEDPARSE_EXPORT __declspec(dllexport)
#else
#define XEDPARSE_EXPORT __declspec(dllimport)
#endif //XEDPARSE_BUILD

#define XEDPARSE_CALL //calling convention

#define XEDPARSE_MAXBUFSIZE 256
#define XEDPARSE_MAXASMSIZE 16

//typedefs
typedef bool (XEDPARSE_CALL* CBXEDPARSE_UNKNOWN)(const char* text, ULONGLONG* value);

//XEDParse enums
enum XEDPARSE_STATUS
{
    XEDPARSE_ERROR = 0,
    XEDPARSE_OK = 1
};

//XEDParse structs
#pragma pack(push,8)
struct XEDPARSE
{
    bool x64; // use 64-bit instructions
    ULONGLONG cip; //instruction pointer (for relative addressing)
    unsigned int dest_size; //destination size (returned by XEDParse)
    CBXEDPARSE_UNKNOWN cbUnknown; //unknown operand callback
    unsigned char dest[XEDPARSE_MAXASMSIZE]; //destination buffer
    char instr[XEDPARSE_MAXBUFSIZE]; //instruction text
    char error[XEDPARSE_MAXBUFSIZE]; //error text (in case of an error)
};
#pragma pack(pop)

#ifdef __cplusplus
extern "C"
{
#endif

XEDPARSE_EXPORT XEDPARSE_STATUS XEDPARSE_CALL XEDParseAssemble(XEDPARSE* XEDParse);

#ifdef __cplusplus
}
#endif

#endif // _XEDPARSE_H

```

`CodeCleaner/pluginsdk/_dbgfunctions.h`:

```h
#ifndef _DBGFUNCTIONS_H
#define _DBGFUNCTIONS_H

#ifndef __cplusplus
#include <stdbool.h>
#endif

typedef struct
{
    char mod[MAX_MODULE_SIZE];
    duint addr;
    unsigned char oldbyte;
    unsigned char newbyte;
} DBGPATCHINFO;

typedef struct
{
    duint addr;
    duint from;
    duint to;
    char comment[MAX_COMMENT_SIZE];
} DBGCALLSTACKENTRY;

typedef struct
{
    int total;
    DBGCALLSTACKENTRY* entries;
} DBGCALLSTACK;

typedef struct
{
    duint addr;
    duint handler;
} DBGSEHRECORD;

typedef struct
{
    duint total;
    DBGSEHRECORD* records;
} DBGSEHCHAIN;

typedef struct
{
    DWORD dwProcessId;
    char szExeFile[MAX_PATH];
    char szExeMainWindowTitle[MAX_PATH];
    char szExeArgs[MAX_COMMAND_LINE_SIZE];
} DBGPROCESSINFO;

typedef struct
{
    DWORD rva;
    BYTE type;
    WORD size;
} DBGRELOCATIONINFO;

typedef enum
{
    InstructionBody = 0,
    InstructionHeading = 1,
    InstructionTailing = 2,
    InstructionOverlapped = 3, // The byte was executed with differing instruction base addresses
    DataByte,  // This and the following is not implemented yet.
    DataWord,
    DataDWord,
    DataQWord,
    DataFloat,
    DataDouble,
    DataLongDouble,
    DataXMM,
    DataYMM,
    DataMMX,
    DataMixed, //the byte is accessed in multiple ways
    InstructionDataMixed //the byte is both executed and written
} TRACERECORDBYTETYPE;

typedef enum
{
    TraceRecordNone,
    TraceRecordBitExec,
    TraceRecordByteWithExecTypeAndCounter,
    TraceRecordWordWithExecTypeAndCounter
} TRACERECORDTYPE;

typedef struct
{
    duint Handle;
    unsigned char TypeNumber;
    unsigned int GrantedAccess;
} HANDLEINFO;

// The longest ip address is 1234:6789:1234:6789:1234:6789:123.567.901.345 (46 bytes)
#define TCP_ADDR_SIZE 50

typedef struct
{
    char RemoteAddress[TCP_ADDR_SIZE];
    unsigned short RemotePort;
    char LocalAddress[TCP_ADDR_SIZE];
    unsigned short LocalPort;
    char StateText[TCP_ADDR_SIZE];
    unsigned int State;
} TCPCONNECTIONINFO;

typedef struct
{
    duint handle;
    duint parent;
    DWORD threadId;
    DWORD style;
    DWORD styleEx;
    duint wndProc;
    bool enabled;
    RECT position;
    char windowTitle[MAX_COMMENT_SIZE];
    char windowClass[MAX_COMMENT_SIZE];
} WINDOW_INFO;

typedef struct
{
    duint addr;
    duint size;
    duint flags;
} HEAPINFO;

typedef struct
{
    const char* name;
    duint value;
} CONSTANTINFO;

typedef enum
{
    MODSYMUNLOADED = 0,
    MODSYMLOADING,
    MODSYMLOADED
} MODULESYMBOLSTATUS;

typedef bool (*ASSEMBLEATEX)(duint addr, const char* instruction, char* error, bool fillnop);
typedef bool (*SECTIONFROMADDR)(duint addr, char* section);
typedef bool (*MODNAMEFROMADDR)(duint addr, char* modname, bool extension);
typedef duint(*MODBASEFROMADDR)(duint addr);
typedef duint(*MODBASEFROMNAME)(const char* modname);
typedef duint(*MODSIZEFROMADDR)(duint addr);
typedef bool (*ASSEMBLE)(duint addr, unsigned char* dest, int* size, const char* instruction, char* error);
typedef bool (*PATCHGET)(duint addr);
typedef bool (*PATCHINRANGE)(duint start, duint end);
typedef bool (*MEMPATCH)(duint va, const unsigned char* src, duint size);
typedef void (*PATCHRESTORERANGE)(duint start, duint end);
typedef bool (*PATCHENUM)(DBGPATCHINFO* patchlist, size_t* cbsize);
typedef bool (*PATCHRESTORE)(duint addr);
typedef int (*PATCHFILE)(DBGPATCHINFO* patchlist, int count, const char* szFileName, char* error);
typedef int (*MODPATHFROMADDR)(duint addr, char* path, int size);
typedef int (*MODPATHFROMNAME)(const char* modname, char* path, int size);
typedef bool (*DISASMFAST)(const unsigned char* data, duint addr, BASIC_INSTRUCTION_INFO* basicinfo);
typedef void (*MEMUPDATEMAP)();
typedef void (*GETCALLSTACK)(DBGCALLSTACK* callstack);
typedef void (*GETSEHCHAIN)(DBGSEHCHAIN* sehchain);
typedef void (*SYMBOLDOWNLOADALLSYMBOLS)(const char* szSymbolStore);
typedef bool (*GETJIT)(char* jit, bool x64);
typedef bool (*GETJITAUTO)(bool* jitauto);
typedef bool (*GETDEFJIT)(char* defjit);
typedef bool (*GETPROCESSLIST)(DBGPROCESSINFO** entries, int* count);
typedef bool (*GETPAGERIGHTS)(duint addr, char* rights);
typedef bool (*SETPAGERIGHTS)(duint addr, const char* rights);
typedef bool (*PAGERIGHTSTOSTRING)(DWORD protect, char* rights);
typedef bool (*ISPROCESSELEVATED)();
typedef bool (*GETCMDLINE)(char* cmdline, size_t* cbsize);
typedef bool (*SETCMDLINE)(const char* cmdline);
typedef duint(*FILEOFFSETTOVA)(const char* modname, duint offset);
typedef duint(*VATOFILEOFFSET)(duint va);
typedef duint(*GETADDRFROMLINE)(const char* szSourceFile, int line, duint* displacement);
typedef bool (*GETSOURCEFROMADDR)(duint addr, char* szSourceFile, int* line);
typedef bool (*VALFROMSTRING)(const char* string, duint* value);
typedef bool (*PATCHGETEX)(duint addr, DBGPATCHINFO* info);
typedef bool (*GETBRIDGEBP)(BPXTYPE type, duint addr, BRIDGEBP* bp);
typedef bool (*STRINGFORMATINLINE)(const char* format, size_t resultSize, char* result);
typedef void (*GETMNEMONICBRIEF)(const char* mnem, size_t resultSize, char* result);
typedef unsigned int (*GETTRACERECORDHITCOUNT)(duint address);
typedef TRACERECORDBYTETYPE(*GETTRACERECORDBYTETYPE)(duint address);
typedef bool (*SETTRACERECORDTYPE)(duint pageAddress, TRACERECORDTYPE type);
typedef TRACERECORDTYPE(*GETTRACERECORDTYPE)(duint pageAddress);
typedef bool (*ENUMHANDLES)(ListOf(HANDLEINFO) handles);
typedef bool (*GETHANDLENAME)(duint handle, char* name, size_t nameSize, char* typeName, size_t typeNameSize);
typedef bool (*ENUMTCPCONNECTIONS)(ListOf(TCPCONNECTIONINFO) connections);
typedef duint(*GETDBGEVENTS)();
typedef MODULEPARTY(*MODGETPARTY)(duint base);
typedef void (*MODSETPARTY)(duint base, MODULEPARTY party);
typedef bool(*WATCHISWATCHDOGTRIGGERED)(unsigned int id);
typedef bool(*MEMISCODEPAGE)(duint addr, bool refresh);
typedef bool(*ANIMATECOMMAND)(const char* command);
typedef void(*DBGSETDEBUGGEEINITSCRIPT)(const char* fileName);
typedef const char* (*DBGGETDEBUGGEEINITSCRIPT)();
typedef bool(*HANDLESENUMWINDOWS)(ListOf(WINDOW_INFO) windows);
typedef bool(*HANDLESENUMHEAPS)(ListOf(HEAPINFO) heaps);
typedef bool(*THREADGETNAME)(DWORD tid, char* name);
typedef bool(*ISDEPENABLED)();
typedef void(*GETCALLSTACKEX)(DBGCALLSTACK* callstack, bool cache);
typedef bool(*GETUSERCOMMENT)(duint addr, char* comment);
typedef void(*ENUMCONSTANTS)(ListOf(CONSTANTINFO) constants);
typedef duint(*MEMBPSIZE)(duint addr);
typedef bool(*MODRELOCATIONSFROMADDR)(duint addr, ListOf(DBGRELOCATIONINFO) relocations);
typedef bool(*MODRELOCATIONATADDR)(duint addr, DBGRELOCATIONINFO* relocation);
typedef bool(*MODRELOCATIONSINRANGE)(duint addr, duint size, ListOf(DBGRELOCATIONINFO) relocations);
typedef duint(*DBGETHASH)();
typedef int(*SYMAUTOCOMPLETE)(const char* Search, char** Buffer, int MaxSymbols);
typedef void(*REFRESHMODULELIST)();
typedef duint(*GETADDRFROMLINEEX)(duint mod, const char* szSourceFile, int line);
typedef MODULESYMBOLSTATUS(*MODSYMBOLSTATUS)(duint mod);
typedef void(*GETCALLSTACKBYTHREAD)(HANDLE thread, DBGCALLSTACK* callstack);

//The list of all the DbgFunctions() return value.
//WARNING: This list is append only. Do not insert things in the middle or plugins would break.
typedef struct DBGFUNCTIONS_
{
    ASSEMBLEATEX AssembleAtEx;
    SECTIONFROMADDR SectionFromAddr;
    MODNAMEFROMADDR ModNameFromAddr;
    MODBASEFROMADDR ModBaseFromAddr;
    MODBASEFROMNAME ModBaseFromName;
    MODSIZEFROMADDR ModSizeFromAddr;
    ASSEMBLE Assemble;
    PATCHGET PatchGet;
    PATCHINRANGE PatchInRange;
    MEMPATCH MemPatch;
    PATCHRESTORERANGE PatchRestoreRange;
    PATCHENUM PatchEnum;
    PATCHRESTORE PatchRestore;
    PATCHFILE PatchFile;
    MODPATHFROMADDR ModPathFromAddr;
    MODPATHFROMNAME ModPathFromName;
    DISASMFAST DisasmFast;
    MEMUPDATEMAP MemUpdateMap;
    GETCALLSTACK GetCallStack;
    GETSEHCHAIN GetSEHChain;
    SYMBOLDOWNLOADALLSYMBOLS SymbolDownloadAllSymbols;
    GETJITAUTO GetJitAuto;
    GETJIT GetJit;
    GETDEFJIT GetDefJit;
    GETPROCESSLIST GetProcessList;
    GETPAGERIGHTS GetPageRights;
    SETPAGERIGHTS SetPageRights;
    PAGERIGHTSTOSTRING PageRightsToString;
    ISPROCESSELEVATED IsProcessElevated;
    GETCMDLINE GetCmdline;
    SETCMDLINE SetCmdline;
    FILEOFFSETTOVA FileOffsetToVa;
    VATOFILEOFFSET VaToFileOffset;
    GETADDRFROMLINE GetAddrFromLine;
    GETSOURCEFROMADDR GetSourceFromAddr;
    VALFROMSTRING ValFromString;
    PATCHGETEX PatchGetEx;
    GETBRIDGEBP GetBridgeBp;
    STRINGFORMATINLINE StringFormatInline;
    GETMNEMONICBRIEF GetMnemonicBrief;
    GETTRACERECORDHITCOUNT GetTraceRecordHitCount;
    GETTRACERECORDBYTETYPE GetTraceRecordByteType;
    SETTRACERECORDTYPE SetTraceRecordType;
    GETTRACERECORDTYPE GetTraceRecordType;
    ENUMHANDLES EnumHandles;
    GETHANDLENAME GetHandleName;
    ENUMTCPCONNECTIONS EnumTcpConnections;
    GETDBGEVENTS GetDbgEvents;
    MODGETPARTY ModGetParty;
    MODSETPARTY ModSetParty;
    WATCHISWATCHDOGTRIGGERED WatchIsWatchdogTriggered;
    MEMISCODEPAGE MemIsCodePage;
    ANIMATECOMMAND AnimateCommand;
    DBGSETDEBUGGEEINITSCRIPT DbgSetDebuggeeInitScript;
    DBGGETDEBUGGEEINITSCRIPT DbgGetDebuggeeInitScript;
    HANDLESENUMWINDOWS EnumWindows;
    HANDLESENUMHEAPS EnumHeaps;
    THREADGETNAME ThreadGetName;
    ISDEPENABLED IsDepEnabled;
    GETCALLSTACKEX GetCallStackEx;
    GETUSERCOMMENT GetUserComment;
    ENUMCONSTANTS EnumConstants;
    ENUMCONSTANTS EnumErrorCodes;
    ENUMCONSTANTS EnumExceptions;
    MEMBPSIZE MemBpSize;
    MODRELOCATIONSFROMADDR ModRelocationsFromAddr;
    MODRELOCATIONATADDR ModRelocationAtAddr;
    MODRELOCATIONSINRANGE ModRelocationsInRange;
    DBGETHASH DbGetHash;
    SYMAUTOCOMPLETE SymAutoComplete;
    REFRESHMODULELIST RefreshModuleList;
    GETADDRFROMLINEEX GetAddrFromLineEx;
    MODSYMBOLSTATUS ModSymbolStatus;
    GETCALLSTACKBYTHREAD GetCallStackByThread;
} DBGFUNCTIONS;

#ifdef BUILD_DBG

const DBGFUNCTIONS* dbgfunctionsget();
void dbgfunctionsinit();

#endif //BUILD_DBG

#endif //_DBGFUNCTIONS_H

```

`CodeCleaner/pluginsdk/_plugin_types.h`:

```h
#ifndef _PLUGIN_DATA_H
#define _PLUGIN_DATA_H

#ifdef BUILD_DBG

#include "_global.h"
#include "jansson/jansson.h"
#pragma warning(push)
#pragma warning(disable:4091)
#include <dbghelp.h>
#pragma warning(pop)

#else

#ifdef __GNUC__
#include "dbghelp/dbghelp.h"
#else
#pragma warning(push)
#pragma warning(disable:4091)
#include <dbghelp.h>
#pragma warning(pop)
#endif // __GNUC__

#ifndef deflen
#define deflen 1024
#endif // deflen

#include "bridgemain.h"
#include "_dbgfunctions.h"
#include "jansson/jansson.h"

#endif // BUILD_DBG

#endif // _PLUGIN_DATA_H

```

`CodeCleaner/pluginsdk/_plugins.h`:

```h
#ifndef _PLUGINS_H
#define _PLUGINS_H

#ifndef __cplusplus
#include <stdbool.h>
#endif

#ifndef PLUG_IMPEXP
#ifdef BUILD_DBG
#define PLUG_IMPEXP __declspec(dllexport)
#else
#define PLUG_IMPEXP __declspec(dllimport)
#endif //BUILD_DBG
#endif //PLUG_IMPEXP

#include "_plugin_types.h"

//default structure alignments forced
#ifdef _WIN64
#pragma pack(push, 16)
#else //x86
#pragma pack(push, 8)
#endif //_WIN64

//defines
#define PLUG_SDKVERSION 1

#define PLUG_DB_LOADSAVE_DATA 1
#define PLUG_DB_LOADSAVE_ALL 2

//structures
typedef struct
{
    //provided by the debugger
    int pluginHandle;
    //provided by the pluginit function
    int sdkVersion;
    int pluginVersion;
    char pluginName[256];
} PLUG_INITSTRUCT;

typedef struct
{
    //provided by the debugger
    HWND hwndDlg; //gui window handle
    int hMenu; //plugin menu handle
    int hMenuDisasm; //plugin disasm menu handle
    int hMenuDump; //plugin dump menu handle
    int hMenuStack; //plugin stack menu handle
    int hMenuGraph; //plugin graph menu handle
    int hMenuMemmap; //plugin memory map menu handle
    int hMenuSymmod; //plugin symbol module menu handle
} PLUG_SETUPSTRUCT;

typedef struct
{
    void* data; //user data
} PLUG_SCRIPTSTRUCT;

//callback structures
typedef struct
{
    const char* szFileName;
} PLUG_CB_INITDEBUG;

typedef struct
{
    void* reserved;
} PLUG_CB_STOPDEBUG;

typedef struct
{
    void* reserved;
} PLUG_CB_STOPPINGDEBUG;

typedef struct
{
    CREATE_PROCESS_DEBUG_INFO* CreateProcessInfo;
    IMAGEHLP_MODULE64* modInfo;
    const char* DebugFileName;
    PROCESS_INFORMATION* fdProcessInfo;
} PLUG_CB_CREATEPROCESS;

typedef struct
{
    EXIT_PROCESS_DEBUG_INFO* ExitProcess;
} PLUG_CB_EXITPROCESS;

typedef struct
{
    CREATE_THREAD_DEBUG_INFO* CreateThread;
    DWORD dwThreadId;
} PLUG_CB_CREATETHREAD;

typedef struct
{
    EXIT_THREAD_DEBUG_INFO* ExitThread;
    DWORD dwThreadId;
} PLUG_CB_EXITTHREAD;

typedef struct
{
    void* reserved;
} PLUG_CB_SYSTEMBREAKPOINT;

typedef struct
{
    LOAD_DLL_DEBUG_INFO* LoadDll;
    IMAGEHLP_MODULE64* modInfo;
    const char* modname;
} PLUG_CB_LOADDLL;

typedef struct
{
    UNLOAD_DLL_DEBUG_INFO* UnloadDll;
} PLUG_CB_UNLOADDLL;

typedef struct
{
    OUTPUT_DEBUG_STRING_INFO* DebugString;
} PLUG_CB_OUTPUTDEBUGSTRING;

typedef struct
{
    EXCEPTION_DEBUG_INFO* Exception;
} PLUG_CB_EXCEPTION;

typedef struct
{
    BRIDGEBP* breakpoint;
} PLUG_CB_BREAKPOINT;

typedef struct
{
    void* reserved;
} PLUG_CB_PAUSEDEBUG;

typedef struct
{
    void* reserved;
} PLUG_CB_RESUMEDEBUG;

typedef struct
{
    void* reserved;
} PLUG_CB_STEPPED;

typedef struct
{
    DWORD dwProcessId;
} PLUG_CB_ATTACH;

typedef struct
{
    PROCESS_INFORMATION* fdProcessInfo;
} PLUG_CB_DETACH;

typedef struct
{
    DEBUG_EVENT* DebugEvent;
} PLUG_CB_DEBUGEVENT;

typedef struct
{
    int hEntry;
} PLUG_CB_MENUENTRY;

typedef struct
{
    MSG* message;
    long* result;
    bool retval;
} PLUG_CB_WINEVENT;

typedef struct
{
    MSG* message;
    bool retval;
} PLUG_CB_WINEVENTGLOBAL;

typedef struct
{
    json_t* root;
    int loadSaveType;
} PLUG_CB_LOADSAVEDB;

typedef struct
{
    const char* symbol;
    bool retval;
} PLUG_CB_FILTERSYMBOL;

typedef struct
{
    duint cip;
    bool stop;
} PLUG_CB_TRACEEXECUTE;

typedef struct
{
    int hWindow;
    duint VA;
} PLUG_CB_SELCHANGED;

typedef struct
{
    BridgeCFGraphList graph;
} PLUG_CB_ANALYZE;

typedef struct
{
    duint addr;
    BRIDGE_ADDRINFO* addrinfo;
    bool retval;
} PLUG_CB_ADDRINFO;

typedef struct
{
    const char* string;
    duint value;
    int* value_size;
    bool* isvar;
    bool* hexonly;
    bool retval;
} PLUG_CB_VALFROMSTRING;

typedef struct
{
    const char* string;
    duint value;
    bool retval;
} PLUG_CB_VALTOSTRING;

typedef struct
{
    GUIMENUTYPE hMenu;
} PLUG_CB_MENUPREPARE;

typedef enum
{
    ValueTypeNumber,
    ValueTypeString,
    ValueTypeAny
} ValueType;

typedef struct
{
    const char* ptr;
    bool isOwner;
} StringValue;

typedef struct
{
    ValueType type;
    duint number;
    StringValue string;
} ExpressionValue;

//enums
typedef enum
{
    CB_INITDEBUG, //PLUG_CB_INITDEBUG
    CB_STOPDEBUG, //PLUG_CB_STOPDEBUG
    CB_CREATEPROCESS, //PLUG_CB_CREATEPROCESS
    CB_EXITPROCESS, //PLUG_CB_EXITPROCESS
    CB_CREATETHREAD, //PLUG_CB_CREATETHREAD
    CB_EXITTHREAD, //PLUG_CB_EXITTHREAD
    CB_SYSTEMBREAKPOINT, //PLUG_CB_SYSTEMBREAKPOINT
    CB_LOADDLL, //PLUG_CB_LOADDLL
    CB_UNLOADDLL, //PLUG_CB_UNLOADDLL
    CB_OUTPUTDEBUGSTRING, //PLUG_CB_OUTPUTDEBUGSTRING
    CB_EXCEPTION, //PLUG_CB_EXCEPTION
    CB_BREAKPOINT, //PLUG_CB_BREAKPOINT
    CB_PAUSEDEBUG, //PLUG_CB_PAUSEDEBUG
    CB_RESUMEDEBUG, //PLUG_CB_RESUMEDEBUG
    CB_STEPPED, //PLUG_CB_STEPPED
    CB_ATTACH, //PLUG_CB_ATTACHED (before attaching, after CB_INITDEBUG)
    CB_DETACH, //PLUG_CB_DETACH (before detaching, before CB_STOPDEBUG)
    CB_DEBUGEVENT, //PLUG_CB_DEBUGEVENT (called on any debug event)
    CB_MENUENTRY, //PLUG_CB_MENUENTRY
    CB_WINEVENT, //PLUG_CB_WINEVENT
    CB_WINEVENTGLOBAL, //PLUG_CB_WINEVENTGLOBAL
    CB_LOADDB, //PLUG_CB_LOADSAVEDB
    CB_SAVEDB, //PLUG_CB_LOADSAVEDB
    CB_FILTERSYMBOL, //PLUG_CB_FILTERSYMBOL
    CB_TRACEEXECUTE, //PLUG_CB_TRACEEXECUTE
    CB_SELCHANGED, //PLUG_CB_SELCHANGED
    CB_ANALYZE, //PLUG_CB_ANALYZE
    CB_ADDRINFO, //PLUG_CB_ADDRINFO
    CB_VALFROMSTRING, //PLUG_CB_VALFROMSTRING
    CB_VALTOSTRING, //PLUG_CB_VALTOSTRING
    CB_MENUPREPARE, //PLUG_CB_MENUPREPARE
    CB_STOPPINGDEBUG, //PLUG_CB_STOPDEBUG
    CB_LAST
} CBTYPE;

typedef enum
{
    FORMAT_ERROR, //generic failure (no message)
    FORMAT_SUCCESS, //success
    FORMAT_ERROR_MESSAGE, //formatting failed but an error was put in the buffer (there are always at least 511 characters available).
    FORMAT_BUFFER_TOO_SMALL //buffer too small (x64dbg will retry until the buffer is big enough)
} FORMATRESULT;

//typedefs
typedef void (*CBPLUGIN)(CBTYPE cbType, void* callbackInfo);
typedef bool (*CBPLUGINCOMMAND)(int argc, char** argv);
typedef void (*CBPLUGINSCRIPT)();
typedef duint(*CBPLUGINEXPRFUNCTION)(int argc, const duint* argv, void* userdata);
typedef bool(*CBPLUGINEXPRFUNCTIONEX)(ExpressionValue* result, int argc, const ExpressionValue* argv, void* userdata);
typedef FORMATRESULT(*CBPLUGINFORMATFUNCTION)(char* dest, size_t destCount, int argc, char* argv[], duint value, void* userdata);
typedef bool (*CBPLUGINPREDICATE)(void* userdata);

//exports
#ifdef __cplusplus
extern "C"
{
#endif

PLUG_IMPEXP void _plugin_registercallback(int pluginHandle, CBTYPE cbType, CBPLUGIN cbPlugin);
PLUG_IMPEXP bool _plugin_unregistercallback(int pluginHandle, CBTYPE cbType);
PLUG_IMPEXP bool _plugin_registercommand(int pluginHandle, const char* command, CBPLUGINCOMMAND cbCommand, bool debugonly);
PLUG_IMPEXP bool _plugin_unregistercommand(int pluginHandle, const char* command);
PLUG_IMPEXP void _plugin_logprintf(const char* format, ...);
PLUG_IMPEXP void _plugin_logputs(const char* text);
PLUG_IMPEXP void _plugin_logprint(const char* text);
PLUG_IMPEXP void _plugin_debugpause();
PLUG_IMPEXP void _plugin_debugskipexceptions(bool skip);
PLUG_IMPEXP int _plugin_menuadd(int hMenu, const char* title);
PLUG_IMPEXP bool _plugin_menuaddentry(int hMenu, int hEntry, const char* title);
PLUG_IMPEXP bool _plugin_menuaddseparator(int hMenu);
PLUG_IMPEXP bool _plugin_menuclear(int hMenu);
PLUG_IMPEXP void _plugin_menuseticon(int hMenu, const ICONDATA* icon);
PLUG_IMPEXP void _plugin_menuentryseticon(int pluginHandle, int hEntry, const ICONDATA* icon);
PLUG_IMPEXP void _plugin_menuentrysetchecked(int pluginHandle, int hEntry, bool checked);
PLUG_IMPEXP void _plugin_menusetvisible(int pluginHandle, int hMenu, bool visible);
PLUG_IMPEXP void _plugin_menuentrysetvisible(int pluginHandle, int hEntry, bool visible);
PLUG_IMPEXP void _plugin_menusetname(int pluginHandle, int hMenu, const char* name);
PLUG_IMPEXP void _plugin_menuentrysetname(int pluginHandle, int hEntry, const char* name);
PLUG_IMPEXP void _plugin_menuentrysethotkey(int pluginHandle, int hEntry, const char* hotkey);
PLUG_IMPEXP bool _plugin_menuremove(int hMenu);
PLUG_IMPEXP bool _plugin_menuentryremove(int pluginHandle, int hEntry);
PLUG_IMPEXP void _plugin_startscript(CBPLUGINSCRIPT cbScript);
PLUG_IMPEXP bool _plugin_waituntilpaused();
PLUG_IMPEXP bool _plugin_registerexprfunction(int pluginHandle, const char* name, int argc, CBPLUGINEXPRFUNCTION cbFunction, void* userdata);
PLUG_IMPEXP bool _plugin_registerexprfunctionex(int pluginHandle, const char* name, const ValueType & returnType, const ValueType* argTypes, size_t argCount, CBPLUGINEXPRFUNCTIONEX cbFunction, void* userdata);
PLUG_IMPEXP bool _plugin_unregisterexprfunction(int pluginHandle, const char* name);
PLUG_IMPEXP bool _plugin_unload(const char* pluginName);
PLUG_IMPEXP bool _plugin_load(const char* pluginName);
PLUG_IMPEXP duint _plugin_hash(const void* data, duint size);
PLUG_IMPEXP bool _plugin_registerformatfunction(int pluginHandle, const char* type, CBPLUGINFORMATFUNCTION cbFunction, void* userdata);
PLUG_IMPEXP bool _plugin_unregisterformatfunction(int pluginHandle, const char* type);

#ifdef __cplusplus
}
#endif

#pragma pack(pop)

#endif // _PLUGINS_H

```

`CodeCleaner/pluginsdk/_scriptapi.h`:

```h
#ifndef _SCRIPT_API_H
#define _SCRIPT_API_H

#include "_plugins.h"

#define SCRIPT_EXPORT PLUG_IMPEXP

#endif //_SCRIPT_API_H
```

`CodeCleaner/pluginsdk/_scriptapi_argument.h`:

```h
#ifndef _SCRIPTAPI_ARGUMENT_H
#define _SCRIPTAPI_ARGUMENT_H

#include "_scriptapi.h"

namespace Script
{
    namespace Argument
    {
        struct ArgumentInfo
        {
            char mod[MAX_MODULE_SIZE];
            duint rvaStart;
            duint rvaEnd;
            bool manual;
            duint instructioncount;
        };

        SCRIPT_EXPORT bool Add(duint start, duint end, bool manual, duint instructionCount = 0);
        SCRIPT_EXPORT bool Add(const ArgumentInfo* info);
        SCRIPT_EXPORT bool Get(duint addr, duint* start = nullptr, duint* end = nullptr, duint* instructionCount = nullptr);
        SCRIPT_EXPORT bool GetInfo(duint addr, ArgumentInfo* info);
        SCRIPT_EXPORT bool Overlaps(duint start, duint end);
        SCRIPT_EXPORT bool Delete(duint address);
        SCRIPT_EXPORT void DeleteRange(duint start, duint end, bool deleteManual = false);
        SCRIPT_EXPORT void Clear();
        SCRIPT_EXPORT bool GetList(ListOf(ArgumentInfo) list); //caller has the responsibility to free the list
    }; //Argument
}; //Script

#endif //_SCRIPTAPI_ARGUMENT_H
```

`CodeCleaner/pluginsdk/_scriptapi_assembler.h`:

```h
#ifndef _SCRIPTAPI_ASSEMBLER_H
#define _SCRIPTAPI_ASSEMBLER_H

#include "_scriptapi.h"

namespace Script
{
    namespace Assembler
    {
        SCRIPT_EXPORT bool Assemble(duint addr, unsigned char* dest, int* size, const char* instruction); //dest[16]
        SCRIPT_EXPORT bool AssembleEx(duint addr, unsigned char* dest, int* size, const char* instruction, char* error); //dest[16], error[MAX_ERROR_SIZE]
        SCRIPT_EXPORT bool AssembleMem(duint addr, const char* instruction);
        SCRIPT_EXPORT bool AssembleMemEx(duint addr, const char* instruction, int* size, char* error, bool fillnop); //error[MAX_ERROR_SIZE]
    }; //Assembler
}; //Script

#endif //_SCRIPTAPI_ASSEMBLER_H
```

`CodeCleaner/pluginsdk/_scriptapi_bookmark.h`:

```h
#ifndef _SCRIPTAPI_BOOKMARK_H
#define _SCRIPTAPI_BOOKMARK_H

#include "_scriptapi.h"

namespace Script
{
    namespace Bookmark
    {
        struct BookmarkInfo
        {
            char mod[MAX_MODULE_SIZE];
            duint rva;
            bool manual;
        };

        SCRIPT_EXPORT bool Set(duint addr, bool manual = false);
        SCRIPT_EXPORT bool Set(const BookmarkInfo* info);
        SCRIPT_EXPORT bool Get(duint addr);
        SCRIPT_EXPORT bool GetInfo(duint addr, BookmarkInfo* info);
        SCRIPT_EXPORT bool Delete(duint addr);
        SCRIPT_EXPORT void DeleteRange(duint start, duint end);
        SCRIPT_EXPORT void Clear();
        SCRIPT_EXPORT bool GetList(ListOf(BookmarkInfo) list); //caller has the responsibility to free the list
    }; //Bookmark
}; //Script

#endif //_SCRIPTAPI_BOOKMARK_H
```

`CodeCleaner/pluginsdk/_scriptapi_comment.h`:

```h
#ifndef _SCRIPTAPI_COMMENT_H
#define _SCRIPTAPI_COMMENT_H

#include "_scriptapi.h"

namespace Script
{
    namespace Comment
    {
        struct CommentInfo
        {
            char mod[MAX_MODULE_SIZE];
            duint rva;
            char text[MAX_LABEL_SIZE];
            bool manual;
        };

        SCRIPT_EXPORT bool Set(duint addr, const char* text, bool manual = false);
        SCRIPT_EXPORT bool Set(const CommentInfo* info);
        SCRIPT_EXPORT bool Get(duint addr, char* text); //text[MAX_COMMENT_SIZE]
        SCRIPT_EXPORT bool GetInfo(duint addr, CommentInfo* info);
        SCRIPT_EXPORT bool Delete(duint addr);
        SCRIPT_EXPORT void DeleteRange(duint start, duint end);
        SCRIPT_EXPORT void Clear();
        SCRIPT_EXPORT bool GetList(ListOf(CommentInfo) list); //caller has the responsibility to free the list
    }; //Comment
}; //Script

#endif //_SCRIPTAPI_COMMENT_H
```

`CodeCleaner/pluginsdk/_scriptapi_debug.h`:

```h
#ifndef _SCRIPTAPI_DEBUG_H
#define _SCRIPTAPI_DEBUG_H

#include "_scriptapi.h"

namespace Script
{
    namespace Debug
    {
        enum HardwareType
        {
            HardwareAccess,
            HardwareWrite,
            HardwareExecute
        };

        SCRIPT_EXPORT void Wait();
        SCRIPT_EXPORT void Run();
        SCRIPT_EXPORT void Pause();
        SCRIPT_EXPORT void Stop();
        SCRIPT_EXPORT void StepIn();
        SCRIPT_EXPORT void StepOver();
        SCRIPT_EXPORT void StepOut();
        SCRIPT_EXPORT bool SetBreakpoint(duint address);
        SCRIPT_EXPORT bool DeleteBreakpoint(duint address);
        SCRIPT_EXPORT bool DisableBreakpoint(duint address);
        SCRIPT_EXPORT bool SetHardwareBreakpoint(duint address, HardwareType type = HardwareExecute);
        SCRIPT_EXPORT bool DeleteHardwareBreakpoint(duint address);
    }; //Debug
}; //Script

#endif //_SCRIPTAPI_DEBUG_H
```

`CodeCleaner/pluginsdk/_scriptapi_flag.h`:

```h
#ifndef _SCRIPTAPI_FLAG_H
#define _SCRIPTAPI_FLAG_H

#include "_scriptapi.h"

namespace Script
{
    namespace Flag
    {
        enum FlagEnum
        {
            ZF,
            OF,
            CF,
            PF,
            SF,
            TF,
            AF,
            DF,
            IF
        };

        SCRIPT_EXPORT bool Get(FlagEnum flag);
        SCRIPT_EXPORT bool Set(FlagEnum flag, bool value);

        SCRIPT_EXPORT bool GetZF();
        SCRIPT_EXPORT bool SetZF(bool value);
        SCRIPT_EXPORT bool GetOF();
        SCRIPT_EXPORT bool SetOF(bool value);
        SCRIPT_EXPORT bool GetCF();
        SCRIPT_EXPORT bool SetCF(bool value);
        SCRIPT_EXPORT bool GetPF();
        SCRIPT_EXPORT bool SetPF(bool value);
        SCRIPT_EXPORT bool GetSF();
        SCRIPT_EXPORT bool SetSF(bool value);
        SCRIPT_EXPORT bool GetTF();
        SCRIPT_EXPORT bool SetTF(bool value);
        SCRIPT_EXPORT bool GetAF();
        SCRIPT_EXPORT bool SetAF(bool value);
        SCRIPT_EXPORT bool GetDF();
        SCRIPT_EXPORT bool SetDF(bool value);
        SCRIPT_EXPORT bool GetIF();
        SCRIPT_EXPORT bool SetIF(bool value);
    };
};

#endif //_SCRIPTAPI_FLAG_H
```

`CodeCleaner/pluginsdk/_scriptapi_function.h`:

```h
#ifndef _SCRIPTAPI_FUNCTION_H
#define _SCRIPTAPI_FUNCTION_H

#include "_scriptapi.h"

namespace Script
{
    namespace Function
    {
        struct FunctionInfo
        {
            char mod[MAX_MODULE_SIZE];
            duint rvaStart;
            duint rvaEnd;
            bool manual;
            duint instructioncount;
        };

        SCRIPT_EXPORT bool Add(duint start, duint end, bool manual, duint instructionCount = 0);
        SCRIPT_EXPORT bool Add(const FunctionInfo* info);
        SCRIPT_EXPORT bool Get(duint addr, duint* start = nullptr, duint* end = nullptr, duint* instructionCount = nullptr);
        SCRIPT_EXPORT bool GetInfo(duint addr, FunctionInfo* info);
        SCRIPT_EXPORT bool Overlaps(duint start, duint end);
        SCRIPT_EXPORT bool Delete(duint address);
        SCRIPT_EXPORT void DeleteRange(duint start, duint end, bool deleteManual);
        SCRIPT_EXPORT void DeleteRange(duint start, duint end);
        SCRIPT_EXPORT void Clear();
        SCRIPT_EXPORT bool GetList(ListOf(FunctionInfo) list); //caller has the responsibility to free the list
    }; //Function
}; //Script

#endif //_SCRIPTAPI_FUNCTION_H

```

`CodeCleaner/pluginsdk/_scriptapi_gui.h`:

```h
#ifndef _SCRIPTAPI_GUI_H
#define _SCRIPTAPI_GUI_H

#include "_scriptapi.h"

namespace Script
{
    namespace Gui
    {
        namespace Disassembly
        {
            SCRIPT_EXPORT bool SelectionGet(duint* start, duint* end);
            SCRIPT_EXPORT bool SelectionSet(duint start, duint end);
            SCRIPT_EXPORT duint SelectionGetStart();
            SCRIPT_EXPORT duint SelectionGetEnd();
        }; //Disassembly

        namespace Dump
        {
            SCRIPT_EXPORT bool SelectionGet(duint* start, duint* end);
            SCRIPT_EXPORT bool SelectionSet(duint start, duint end);
            SCRIPT_EXPORT duint SelectionGetStart();
            SCRIPT_EXPORT duint SelectionGetEnd();
        }; //Dump

        namespace Stack
        {
            SCRIPT_EXPORT bool SelectionGet(duint* start, duint* end);
            SCRIPT_EXPORT bool SelectionSet(duint start, duint end);
            SCRIPT_EXPORT duint SelectionGetStart();
            SCRIPT_EXPORT duint SelectionGetEnd();
        }; //Stack

        namespace Graph
        {
            SCRIPT_EXPORT duint SelectionGetStart();
        }; //Graph

        namespace MemMap
        {
            SCRIPT_EXPORT duint SelectionGetStart();
        }; //MemoryMap

        namespace SymMod
        {
            SCRIPT_EXPORT duint SelectionGetStart();
        }; //SymMod
    }; //Gui

    namespace Gui
    {
        enum Window
        {
            DisassemblyWindow,
            DumpWindow,
            StackWindow,
            GraphWindow,
            MemMapWindow,
            SymModWindow
        };

        SCRIPT_EXPORT bool SelectionGet(Window window, duint* start, duint* end);
        SCRIPT_EXPORT bool SelectionSet(Window window, duint start, duint end);
        SCRIPT_EXPORT duint SelectionGetStart(Window window);
        SCRIPT_EXPORT duint SelectionGetEnd(Window window);
        SCRIPT_EXPORT void Message(const char* message);
        SCRIPT_EXPORT bool MessageYesNo(const char* message);
        SCRIPT_EXPORT bool InputLine(const char* title, char* text); //text[GUI_MAX_LINE_SIZE]
        SCRIPT_EXPORT bool InputValue(const char* title, duint* value);
        SCRIPT_EXPORT void Refresh();
        SCRIPT_EXPORT void AddQWidgetTab(void* qWidget);
        SCRIPT_EXPORT void ShowQWidgetTab(void* qWidget);
        SCRIPT_EXPORT void CloseQWidgetTab(void* qWidget);

    }; //Gui
}; //Script

#endif //_SCRIPTAPI_GUI_H
```

`CodeCleaner/pluginsdk/_scriptapi_label.h`:

```h
#ifndef _SCRIPTAPI_LABEL_H
#define _SCRIPTAPI_LABEL_H

#include "_scriptapi.h"

namespace Script
{
    namespace Label
    {
        struct LabelInfo
        {
            char mod[MAX_MODULE_SIZE];
            duint rva;
            char text[MAX_LABEL_SIZE];
            bool manual;
        };

        SCRIPT_EXPORT bool Set(duint addr, const char* text, bool manual = false);
        SCRIPT_EXPORT bool Set(duint addr, const char* text, bool manual = false, bool temporary = false);
        SCRIPT_EXPORT bool Set(const LabelInfo* info);
        SCRIPT_EXPORT bool FromString(const char* label, duint* addr);
        SCRIPT_EXPORT bool Get(duint addr, char* text); //text[MAX_LABEL_SIZE]
        SCRIPT_EXPORT bool IsTemporary(duint addr);
        SCRIPT_EXPORT bool GetInfo(duint addr, LabelInfo* info);
        SCRIPT_EXPORT bool Delete(duint addr);
        SCRIPT_EXPORT void DeleteRange(duint start, duint end);
        SCRIPT_EXPORT void Clear();
        SCRIPT_EXPORT bool GetList(ListOf(LabelInfo) list); //caller has the responsibility to free the list
    }; //Label
}; //Script

#endif //_SCRIPTAPI_LABEL_H
```

`CodeCleaner/pluginsdk/_scriptapi_memory.h`:

```h
#ifndef _SCRIPTAPI_MEMORY_H
#define _SCRIPTAPI_MEMORY_H

#include "_scriptapi.h"

namespace Script
{
    namespace Memory
    {
        SCRIPT_EXPORT bool Read(duint addr, void* data, duint size, duint* sizeRead);
        SCRIPT_EXPORT bool Write(duint addr, const void* data, duint size, duint* sizeWritten);
        SCRIPT_EXPORT bool IsValidPtr(duint addr);
        SCRIPT_EXPORT duint RemoteAlloc(duint addr, duint size);
        SCRIPT_EXPORT bool RemoteFree(duint addr);
        SCRIPT_EXPORT unsigned int GetProtect(duint addr, bool reserved = false, bool cache = true);
        SCRIPT_EXPORT bool SetProtect(duint addr, unsigned int protect, duint size);
        SCRIPT_EXPORT duint GetBase(duint addr, bool reserved = false, bool cache = true);
        SCRIPT_EXPORT duint GetSize(duint addr, bool reserved = false, bool cache = true);

        SCRIPT_EXPORT unsigned char ReadByte(duint addr);
        SCRIPT_EXPORT bool WriteByte(duint addr, unsigned char data);
        SCRIPT_EXPORT unsigned short ReadWord(duint addr);
        SCRIPT_EXPORT bool WriteWord(duint addr, unsigned short data);
        SCRIPT_EXPORT unsigned int ReadDword(duint addr);
        SCRIPT_EXPORT bool WriteDword(duint addr, unsigned int data);
        SCRIPT_EXPORT unsigned long long ReadQword(duint addr);
        SCRIPT_EXPORT bool WriteQword(duint addr, unsigned long long data);
        SCRIPT_EXPORT duint ReadPtr(duint addr);
        SCRIPT_EXPORT bool WritePtr(duint addr, duint data);
    }; //Memory
}; //Script

#endif //_SCRIPTAPI_MEMORY_H
```

`CodeCleaner/pluginsdk/_scriptapi_misc.h`:

```h
#ifndef _SCRIPTAPI_MISC_H
#define _SCRIPTAPI_MISC_H

#include "_scriptapi.h"

namespace Script
{
    namespace Misc
    {
        /// <summary>
        /// Evaluates an expression and returns the result. Analagous to using the Command field in x64dbg.
        ///
        /// Expressions can consist of memory locations, registers, flags, API names, labels, symbols, variables etc.
        ///
        /// Example: bool success = ParseExpression("[esp+8]", &val)
        /// </summary>
        /// <param name="expression">The expression to evaluate.</param>
        /// <param name="value">The result of the expression.</param>
        /// <returns>True on success, False on failure.</returns>
        SCRIPT_EXPORT bool ParseExpression(const char* expression, duint* value);

        /// <summary>
        /// Returns the address of a function in the debuggee's memory space.
        ///
        /// Example: duint addr = RemoteGetProcAddress("kernel32.dll", "GetProcAddress")
        /// </summary>
        /// <param name="module">The name of the module.</param>
        /// <param name="api">The name of the function.</param>
        /// <returns>The address of the function in the debuggee.</returns>
        SCRIPT_EXPORT duint RemoteGetProcAddress(const char* module, const char* api);

        /// <summary>
        /// Returns the address for a label created in the disassembly window.
        ///
        /// Example: duint addr = ResolveLabel("sneaky_crypto")
        /// </summary>
        /// <param name="label">The name of the label to resolve.</param>
        /// <returns>The memory address for the label.</returns>
        SCRIPT_EXPORT duint ResolveLabel(const char* label);

        /// <summary>
        /// Allocates the requested number of bytes from x64dbg's default process heap.
        ///
        /// Note: this allocation is in the debugger, not the debuggee.
        ///
        /// Memory allocated using this function should be Free'd after use.
        ///
        /// Example: void* addr = Alloc(0x100000)
        /// </summary>
        /// <param name="size">Number of bytes to allocate.</param>
        /// <returns>A pointer to the newly allocated memory.</returns>
        SCRIPT_EXPORT void* Alloc(duint size);

        /// <summary>
        /// Frees memory previously allocated by Alloc.
        ///
        /// Example: Free(addr)
        /// </summary>
        /// <param name="ptr">Pointer returned by Alloc.</param>
        /// <returns>Nothing.</returns>
        SCRIPT_EXPORT void Free(void* ptr);
    }; //Misc
}; //Script

#endif //_SCRIPTAPI_MISC_H
```

`CodeCleaner/pluginsdk/_scriptapi_module.h`:

```h
#ifndef _SCRIPTAPI_MODULE_H
#define _SCRIPTAPI_MODULE_H

#include "_scriptapi.h"

namespace Script
{
    namespace Module
    {
        struct ModuleInfo
        {
            duint base;
            duint size;
            duint entry;
            int sectionCount;
            char name[MAX_MODULE_SIZE];
            char path[MAX_PATH];
        };

        struct ModuleSectionInfo
        {
            duint addr;
            duint size;
            char name[MAX_SECTION_SIZE * 5];
        };

        struct ModuleExport
        {
            duint ordinal;
            duint rva;
            duint va;
            bool forwarded;
            char forwardName[MAX_STRING_SIZE];
            char name[MAX_STRING_SIZE];
            char undecoratedName[MAX_STRING_SIZE];
        };

        struct ModuleImport
        {
            duint iatRva;
            duint iatVa;
            duint ordinal; //equal to -1 if imported by name
            char name[MAX_STRING_SIZE];
            char undecoratedName[MAX_STRING_SIZE];
        };

        SCRIPT_EXPORT bool InfoFromAddr(duint addr, ModuleInfo* info);
        SCRIPT_EXPORT bool InfoFromName(const char* name, ModuleInfo* info);
        SCRIPT_EXPORT duint BaseFromAddr(duint addr);
        SCRIPT_EXPORT duint BaseFromName(const char* name);
        SCRIPT_EXPORT duint SizeFromAddr(duint addr);
        SCRIPT_EXPORT duint SizeFromName(const char* name);
        SCRIPT_EXPORT bool NameFromAddr(duint addr, char* name); //name[MAX_MODULE_SIZE]
        SCRIPT_EXPORT bool PathFromAddr(duint addr, char* path); //path[MAX_PATH]
        SCRIPT_EXPORT bool PathFromName(const char* name, char* path); //path[MAX_PATH]
        SCRIPT_EXPORT duint EntryFromAddr(duint addr);
        SCRIPT_EXPORT duint EntryFromName(const char* name);
        SCRIPT_EXPORT int SectionCountFromAddr(duint addr);
        SCRIPT_EXPORT int SectionCountFromName(const char* name);
        SCRIPT_EXPORT bool SectionFromAddr(duint addr, int number, ModuleSectionInfo* section);
        SCRIPT_EXPORT bool SectionFromName(const char* name, int number, ModuleSectionInfo* section);
        SCRIPT_EXPORT bool SectionListFromAddr(duint addr, ListOf(ModuleSectionInfo) list);
        SCRIPT_EXPORT bool SectionListFromName(const char* name, ListOf(ModuleSectionInfo) list);
        SCRIPT_EXPORT bool GetMainModuleInfo(ModuleInfo* info);
        SCRIPT_EXPORT duint GetMainModuleBase();
        SCRIPT_EXPORT duint GetMainModuleSize();
        SCRIPT_EXPORT duint GetMainModuleEntry();
        SCRIPT_EXPORT int GetMainModuleSectionCount();
        SCRIPT_EXPORT bool GetMainModuleName(char* name); //name[MAX_MODULE_SIZE]
        SCRIPT_EXPORT bool GetMainModulePath(char* path); //path[MAX_PATH]
        SCRIPT_EXPORT bool GetMainModuleSectionList(ListOf(ModuleSectionInfo) list); //caller has the responsibility to free the list
        SCRIPT_EXPORT bool GetList(ListOf(ModuleInfo) list); //caller has the responsibility to free the list
        SCRIPT_EXPORT bool GetExports(const ModuleInfo* mod, ListOf(ModuleExport) list); //caller has the responsibility to free the list
        SCRIPT_EXPORT bool GetImports(const ModuleInfo* mod, ListOf(ModuleImport) list); //caller has the responsibility to free the list
    }; //Module
}; //Script

#endif //_SCRIPTAPI_MODULE_H

```

`CodeCleaner/pluginsdk/_scriptapi_pattern.h`:

```h
#ifndef _SCRIPTAPI_PATTERN_H
#define _SCRIPTAPI_PATTERN_H

#include "_scriptapi.h"

namespace Script
{
    namespace Pattern
    {
        SCRIPT_EXPORT duint Find(unsigned char* data, duint datasize, const char* pattern);
        SCRIPT_EXPORT duint FindMem(duint start, duint size, const char* pattern);
        SCRIPT_EXPORT void Write(unsigned char* data, duint datasize, const char* pattern);
        SCRIPT_EXPORT void WriteMem(duint start, duint size, const char* pattern);
        SCRIPT_EXPORT bool SearchAndReplace(unsigned char* data, duint datasize, const char* searchpattern, const char* replacepattern);
        SCRIPT_EXPORT bool SearchAndReplaceMem(duint start, duint size, const char* searchpattern, const char* replacepattern);
    };
};

#endif //_SCRIPTAPI_FIND_H
```

`CodeCleaner/pluginsdk/_scriptapi_register.h`:

```h
#ifndef _SCRIPTAPI_REGISTER_H
#define _SCRIPTAPI_REGISTER_H

#include "_scriptapi.h"

namespace Script
{
    namespace Register
    {
        enum RegisterEnum
        {
            DR0,
            DR1,
            DR2,
            DR3,
            DR6,
            DR7,

            EAX,
            AX,
            AH,
            AL,
            EBX,
            BX,
            BH,
            BL,
            ECX,
            CX,
            CH,
            CL,
            EDX,
            DX,
            DH,
            DL,
            EDI,
            DI,
            ESI,
            SI,
            EBP,
            BP,
            ESP,
            SP,
            EIP,

#ifdef _WIN64
            RAX,
            RBX,
            RCX,
            RDX,
            RSI,
            SIL,
            RDI,
            DIL,
            RBP,
            BPL,
            RSP,
            SPL,
            RIP,
            R8,
            R8D,
            R8W,
            R8B,
            R9,
            R9D,
            R9W,
            R9B,
            R10,
            R10D,
            R10W,
            R10B,
            R11,
            R11D,
            R11W,
            R11B,
            R12,
            R12D,
            R12W,
            R12B,
            R13,
            R13D,
            R13W,
            R13B,
            R14,
            R14D,
            R14W,
            R14B,
            R15,
            R15D,
            R15W,
            R15B,
#endif //_WIN64

            CIP,
            CSP,
            CAX,
            CBX,
            CCX,
            CDX,
            CDI,
            CSI,
            CBP,
            CFLAGS
        }; //RegisterEnum

        SCRIPT_EXPORT duint Get(RegisterEnum reg);
        SCRIPT_EXPORT bool Set(RegisterEnum reg, duint value);
        SCRIPT_EXPORT int Size(); //gets architecture register size in bytes

        SCRIPT_EXPORT duint GetDR0();
        SCRIPT_EXPORT bool SetDR0(duint value);
        SCRIPT_EXPORT duint GetDR1();
        SCRIPT_EXPORT bool SetDR1(duint value);
        SCRIPT_EXPORT duint GetDR2();
        SCRIPT_EXPORT bool SetDR2(duint value);
        SCRIPT_EXPORT duint GetDR3();
        SCRIPT_EXPORT bool SetDR3(duint value);
        SCRIPT_EXPORT duint GetDR6();
        SCRIPT_EXPORT bool SetDR6(duint value);
        SCRIPT_EXPORT duint GetDR7();
        SCRIPT_EXPORT bool SetDR7(duint value);

        SCRIPT_EXPORT unsigned int GetEAX();
        SCRIPT_EXPORT bool SetEAX(unsigned int value);
        SCRIPT_EXPORT unsigned short GetAX();
        SCRIPT_EXPORT bool SetAX(unsigned short value);
        SCRIPT_EXPORT unsigned char GetAH();
        SCRIPT_EXPORT bool SetAH(unsigned char value);
        SCRIPT_EXPORT unsigned char GetAL();
        SCRIPT_EXPORT bool SetAL(unsigned char value);
        SCRIPT_EXPORT unsigned int GetEBX();
        SCRIPT_EXPORT bool SetEBX(unsigned int value);
        SCRIPT_EXPORT unsigned short GetBX();
        SCRIPT_EXPORT bool SetBX(unsigned short value);
        SCRIPT_EXPORT unsigned char GetBH();
        SCRIPT_EXPORT bool SetBH(unsigned char value);
        SCRIPT_EXPORT unsigned char GetBL();
        SCRIPT_EXPORT bool SetBL(unsigned char value);
        SCRIPT_EXPORT unsigned int GetECX();
        SCRIPT_EXPORT bool SetECX(unsigned int value);
        SCRIPT_EXPORT unsigned short GetCX();
        SCRIPT_EXPORT bool SetCX(unsigned short value);
        SCRIPT_EXPORT unsigned char GetCH();
        SCRIPT_EXPORT bool SetCH(unsigned char value);
        SCRIPT_EXPORT unsigned char GetCL();
        SCRIPT_EXPORT bool SetCL(unsigned char value);
        SCRIPT_EXPORT unsigned int GetEDX();
        SCRIPT_EXPORT bool SetEDX(unsigned int value);
        SCRIPT_EXPORT unsigned short GetDX();
        SCRIPT_EXPORT bool SetDX(unsigned short value);
        SCRIPT_EXPORT unsigned char GetDH();
        SCRIPT_EXPORT bool SetDH(unsigned char value);
        SCRIPT_EXPORT unsigned char GetDL();
        SCRIPT_EXPORT bool SetDL(unsigned char value);
        SCRIPT_EXPORT unsigned int GetEDI();
        SCRIPT_EXPORT bool SetEDI(unsigned int value);
        SCRIPT_EXPORT unsigned short GetDI();
        SCRIPT_EXPORT bool SetDI(unsigned short value);
        SCRIPT_EXPORT unsigned int GetESI();
        SCRIPT_EXPORT bool SetESI(unsigned int value);
        SCRIPT_EXPORT unsigned short GetSI();
        SCRIPT_EXPORT bool SetSI(unsigned short value);
        SCRIPT_EXPORT unsigned int GetEBP();
        SCRIPT_EXPORT bool SetEBP(unsigned int value);
        SCRIPT_EXPORT unsigned short GetBP();
        SCRIPT_EXPORT bool SetBP(unsigned short value);
        SCRIPT_EXPORT unsigned int GetESP();
        SCRIPT_EXPORT bool SetESP(unsigned int value);
        SCRIPT_EXPORT unsigned short GetSP();
        SCRIPT_EXPORT bool SetSP(unsigned short value);
        SCRIPT_EXPORT unsigned int GetEIP();
        SCRIPT_EXPORT bool SetEIP(unsigned int value);

#ifdef _WIN64
        SCRIPT_EXPORT unsigned long long GetRAX();
        SCRIPT_EXPORT bool SetRAX(unsigned long long value);
        SCRIPT_EXPORT unsigned long long GetRBX();
        SCRIPT_EXPORT bool SetRBX(unsigned long long value);
        SCRIPT_EXPORT unsigned long long GetRCX();
        SCRIPT_EXPORT bool SetRCX(unsigned long long value);
        SCRIPT_EXPORT unsigned long long GetRDX();
        SCRIPT_EXPORT bool SetRDX(unsigned long long value);
        SCRIPT_EXPORT unsigned long long GetRSI();
        SCRIPT_EXPORT bool SetRSI(unsigned long long value);
        SCRIPT_EXPORT unsigned char GetSIL();
        SCRIPT_EXPORT bool SetSIL(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetRDI();
        SCRIPT_EXPORT bool SetRDI(unsigned long long value);
        SCRIPT_EXPORT unsigned char GetDIL();
        SCRIPT_EXPORT bool SetDIL(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetRBP();
        SCRIPT_EXPORT bool SetRBP(unsigned long long value);
        SCRIPT_EXPORT unsigned char GetBPL();
        SCRIPT_EXPORT bool SetBPL(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetRSP();
        SCRIPT_EXPORT bool SetRSP(unsigned long long value);
        SCRIPT_EXPORT unsigned char GetSPL();
        SCRIPT_EXPORT bool SetSPL(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetRIP();
        SCRIPT_EXPORT bool SetRIP(unsigned long long value);
        SCRIPT_EXPORT unsigned long long GetR8();
        SCRIPT_EXPORT bool SetR8(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR8D();
        SCRIPT_EXPORT bool SetR8D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR8W();
        SCRIPT_EXPORT bool SetR8W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR8B();
        SCRIPT_EXPORT bool SetR8B(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetR9();
        SCRIPT_EXPORT bool SetR9(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR9D();
        SCRIPT_EXPORT bool SetR9D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR9W();
        SCRIPT_EXPORT bool SetR9W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR9B();
        SCRIPT_EXPORT bool SetR9B(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetR10();
        SCRIPT_EXPORT bool SetR10(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR10D();
        SCRIPT_EXPORT bool SetR10D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR10W();
        SCRIPT_EXPORT bool SetR10W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR10B();
        SCRIPT_EXPORT bool SetR10B(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetR11();
        SCRIPT_EXPORT bool SetR11(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR11D();
        SCRIPT_EXPORT bool SetR11D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR11W();
        SCRIPT_EXPORT bool SetR11W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR11B();
        SCRIPT_EXPORT bool SetR11B(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetR12();
        SCRIPT_EXPORT bool SetR12(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR12D();
        SCRIPT_EXPORT bool SetR12D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR12W();
        SCRIPT_EXPORT bool SetR12W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR12B();
        SCRIPT_EXPORT bool SetR12B(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetR13();
        SCRIPT_EXPORT bool SetR13(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR13D();
        SCRIPT_EXPORT bool SetR13D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR13W();
        SCRIPT_EXPORT bool SetR13W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR13B();
        SCRIPT_EXPORT bool SetR13B(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetR14();
        SCRIPT_EXPORT bool SetR14(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR14D();
        SCRIPT_EXPORT bool SetR14D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR14W();
        SCRIPT_EXPORT bool SetR14W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR14B();
        SCRIPT_EXPORT bool SetR14B(unsigned char value);
        SCRIPT_EXPORT unsigned long long GetR15();
        SCRIPT_EXPORT bool SetR15(unsigned long long value);
        SCRIPT_EXPORT unsigned int GetR15D();
        SCRIPT_EXPORT bool SetR15D(unsigned int value);
        SCRIPT_EXPORT unsigned short GetR15W();
        SCRIPT_EXPORT bool SetR15W(unsigned short value);
        SCRIPT_EXPORT unsigned char GetR15B();
        SCRIPT_EXPORT bool SetR15B(unsigned char value);
#endif //_WIN64

        SCRIPT_EXPORT duint GetCAX();
        SCRIPT_EXPORT bool SetCAX(duint value);
        SCRIPT_EXPORT duint GetCBX();
        SCRIPT_EXPORT bool SetCBX(duint value);
        SCRIPT_EXPORT duint GetCCX();
        SCRIPT_EXPORT bool SetCCX(duint value);
        SCRIPT_EXPORT duint GetCDX();
        SCRIPT_EXPORT bool SetCDX(duint value);
        SCRIPT_EXPORT duint GetCDI();
        SCRIPT_EXPORT bool SetCDI(duint value);
        SCRIPT_EXPORT duint GetCSI();
        SCRIPT_EXPORT bool SetCSI(duint value);
        SCRIPT_EXPORT duint GetCBP();
        SCRIPT_EXPORT bool SetCBP(duint value);
        SCRIPT_EXPORT duint GetCSP();
        SCRIPT_EXPORT bool SetCSP(duint value);
        SCRIPT_EXPORT duint GetCIP();
        SCRIPT_EXPORT bool SetCIP(duint value);
        SCRIPT_EXPORT duint GetCFLAGS();
        SCRIPT_EXPORT bool SetCFLAGS(duint value);
    }; //Register
}; //Script

#endif //_SCRIPTAPI_REGISTER_H
```

`CodeCleaner/pluginsdk/_scriptapi_stack.h`:

```h
#ifndef _SCRIPTAPI_STACK_H
#define _SCRIPTAPI_STACK_H

#include "_scriptapi.h"

namespace Script
{
    namespace Stack
    {
        SCRIPT_EXPORT duint Pop();
        SCRIPT_EXPORT duint Push(duint value); //returns the previous top, equal to Peek(1)
        SCRIPT_EXPORT duint Peek(int offset = 0); //offset is in multiples of Register::Size(), for easy x32/x64 portability
    }; //Stack
}; //Script

#endif //_SCRIPTAPI_STACK_H
```

`CodeCleaner/pluginsdk/_scriptapi_symbol.h`:

```h
#ifndef _SCRIPTAPI_SYMBOL_H
#define _SCRIPTAPI_SYMBOL_H

#include "_scriptapi.h"

namespace Script
{
    namespace Symbol
    {
        enum SymbolType
        {
            Function, //user-defined function
            Import, //IAT entry
            Export //export
        };

        struct SymbolInfo
        {
            char mod[MAX_MODULE_SIZE];
            duint rva;
            char name[MAX_LABEL_SIZE];
            bool manual;
            SymbolType type;
        };

        SCRIPT_EXPORT bool GetList(ListOf(SymbolInfo) list); //caller has the responsibility to free the list
    }; //Symbol
}; //Script

#endif //_SCRIPTAPI_SYMBOL_H
```

`CodeCleaner/pluginsdk/bridgegraph.h`:

```h
#ifndef _GRAPH_H
#define _GRAPH_H

typedef struct
{
    duint addr; //virtual address of the instruction
    unsigned char data[15]; //instruction bytes
} BridgeCFInstruction;

typedef struct
{
    duint parentGraph; //function of which this node is a part
    duint start; //start of the block
    duint end; //end of the block (inclusive)
    duint brtrue; //destination if condition is true
    duint brfalse; //destination if condition is false
    duint icount; //number of instructions in node
    bool terminal; //node is a RET
    bool split; //node is a split (brtrue points to the next node)
    bool indirectcall; //node contains indirect calls (call reg, call [reg+X])
    void* userdata; //user data
    ListInfo exits; //exits (including brtrue and brfalse, duint)
    ListInfo instrs; //block instructions
} BridgeCFNodeList;

typedef struct
{
    duint entryPoint; //graph entry point
    void* userdata; //user data
    ListInfo nodes; //graph nodes (BridgeCFNodeList)
} BridgeCFGraphList;

#ifdef __cplusplus
#if _MSC_VER >= 1700 && !defined(NO_CPP11)

#include <unordered_map>
#include <unordered_set>
#include <vector>
#include <utility>

struct BridgeCFNode
{
    duint parentGraph = 0; //function of which this node is a part
    duint start = 0; //va of the first instruction in the block
    duint end = 0; //va of the last instruction in the block (inclusive)
    duint brtrue = 0; //destination if condition is true
    duint brfalse = 0; //destination if condition is false
    duint icount = 0; //number of instructions in node
    bool terminal = false; //node is a RET
    bool split = false; //node is a split (brtrue points to the next node)
    bool indirectcall = false; //node contains indirect calls (call reg, call [reg+X])
    void* userdata = nullptr; //user data
    std::vector<duint> exits; //exits (including brtrue and brfalse)
    std::vector<BridgeCFInstruction> instrs; //block instructions

    static void Free(const BridgeCFNodeList* nodeList)
    {
        if(!BridgeList<duint>::Free(&nodeList->exits))
            __debugbreak();
        if(!BridgeList<BridgeCFInstruction>::Free(&nodeList->instrs))
            __debugbreak();
    }

    BridgeCFNode() = default;

    BridgeCFNode(const BridgeCFNodeList* nodeList, bool freedata)
    {
        if(!nodeList)
            __debugbreak();
        parentGraph = nodeList->parentGraph;
        start = nodeList->start;
        end = nodeList->end;
        brtrue = nodeList->brtrue;
        brfalse = nodeList->brfalse;
        icount = nodeList->icount;
        terminal = nodeList->terminal;
        indirectcall = nodeList->indirectcall;
        split = nodeList->split;
        userdata = nodeList->userdata;
        if(!BridgeList<duint>::ToVector(&nodeList->exits, exits, freedata))
            __debugbreak();
        if(!BridgeList<BridgeCFInstruction>::ToVector(&nodeList->instrs, instrs, freedata))
            __debugbreak();
    }

    BridgeCFNode(duint parentGraph, duint start, duint end)
        : parentGraph(parentGraph),
          start(start),
          end(end)
    {
    }

    BridgeCFNodeList ToNodeList() const
    {
        BridgeCFNodeList out;
        out.parentGraph = parentGraph;
        out.start = start;
        out.end = end;
        out.brtrue = brtrue;
        out.brfalse = brfalse;
        out.icount = icount;
        out.terminal = terminal;
        out.indirectcall = indirectcall;
        out.split = split;
        out.userdata = userdata;
        BridgeList<duint>::CopyData(&out.exits, exits);
        BridgeList<BridgeCFInstruction>::CopyData(&out.instrs, instrs);
        return std::move(out);
    }
};

struct BridgeCFGraph
{
    duint entryPoint; //graph entry point
    void* userdata; //user data
    std::unordered_map<duint, BridgeCFNode> nodes; //CFNode.start -> CFNode
    std::unordered_map<duint, std::unordered_set<duint>> parents; //CFNode.start -> parents

    static void Free(const BridgeCFGraphList* graphList)
    {
        if(!graphList || graphList->nodes.size != graphList->nodes.count * sizeof(BridgeCFNodeList))
            __debugbreak();
        auto data = (BridgeCFNodeList*)graphList->nodes.data;
        for(int i = 0; i < graphList->nodes.count; i++)
            BridgeCFNode::Free(&data[i]);
        BridgeFree(data);
    }

    explicit BridgeCFGraph(const BridgeCFGraphList* graphList, bool freedata)
    {
        if(!graphList || graphList->nodes.size != graphList->nodes.count * sizeof(BridgeCFNodeList))
            __debugbreak();
        entryPoint = graphList->entryPoint;
        userdata = graphList->userdata;
        auto data = (BridgeCFNodeList*)graphList->nodes.data;
        for(int i = 0; i < graphList->nodes.count; i++)
            AddNode(BridgeCFNode(&data[i], freedata));
        if(freedata && data)
            BridgeFree(data);
    }

    explicit BridgeCFGraph(duint entryPoint)
        : entryPoint(entryPoint),
          userdata(nullptr)
    {
    }

    void AddNode(const BridgeCFNode & node)
    {
        nodes[node.start] = node;
        AddParent(node.start, node.brtrue);
        AddParent(node.start, node.brfalse);
    }

    void AddParent(duint child, duint parent)
    {
        if(!child || !parent)
            return;
        auto found = parents.find(child);
        if(found == parents.end())
        {
            parents[child] = std::unordered_set<duint>();
            parents[child].insert(parent);
        }
        else
            found->second.insert(parent);
    }

    BridgeCFGraphList ToGraphList() const
    {
        BridgeCFGraphList out;
        out.entryPoint = entryPoint;
        out.userdata = userdata;
        std::vector<BridgeCFNodeList> nodeList;
        nodeList.reserve(nodes.size());
        for(const auto & nodeIt : nodes)
            nodeList.push_back(nodeIt.second.ToNodeList());
        BridgeList<BridgeCFNodeList>::CopyData(&out.nodes, nodeList);
        return std::move(out);
    }
};

#endif //_MSC_VER
#endif //__cplusplus

#endif //_GRAPH_H
```

`CodeCleaner/pluginsdk/bridgelist.h`:

```h
#ifndef _LIST_H
#define _LIST_H

typedef struct
{
    int count; //Number of element in the list.
    size_t size; //Size of list in bytes (used for type checking).
    void* data; //Pointer to the list contents. Must be deleted by the caller using BridgeFree (or BridgeList::Free).
} ListInfo;

#define ListOf(Type) ListInfo*

#ifdef __cplusplus

#include <vector>

/**
\brief A list object. This object is NOT thread safe.
\tparam Type BridgeList contents type.
*/
template<typename Type>
class BridgeList
{
public:
    /**
    \brief BridgeList constructor.
    \param _freeData (Optional) the free function.
    */
    explicit BridgeList()
    {
        memset(&_listInfo, 0, sizeof(_listInfo));
    }

    /**
    \brief BridgeList destructor.
    */
    ~BridgeList()
    {
        Cleanup();
    }

    /**
    \brief Gets the list data.
    \return Returns ListInfo->data. Can be null if the list was never initialized. Will be destroyed once this object goes out of scope!
    */
    Type* Data() const
    {
        return reinterpret_cast<Type*>(_listInfo.data);
    }

    /**
    \brief Gets the number of elements in the list. This will crash the program if the data is not consistent with the specified template argument.
    \return The number of elements in the list.
    */
    int Count() const
    {
        if(_listInfo.size != _listInfo.count * sizeof(Type)) //make sure the user is using the correct type.
            __debugbreak();
        return _listInfo.count;
    }

    /**
    \brief Cleans up the list, freeing the list data when it is not null.
    */
    void Cleanup()
    {
        if(_listInfo.data)
        {
            BridgeFree(_listInfo.data);
            _listInfo.data = nullptr;
        }
    }

    /**
    \brief Reference operator (cleans up the previous list)
    \return Pointer to the ListInfo.
    */
    ListInfo* operator&()
    {
        Cleanup();
        return &_listInfo;
    }

    /**
    \brief Array indexer operator. This will crash if you try to access out-of-bounds.
    \param index Zero-based index of the item you want to get.
    \return Reference to a value at that index.
    */
    Type & operator[](size_t index) const
    {
        if(index >= size_t(Count())) //make sure the out-of-bounds access is caught as soon as possible.
            __debugbreak();
        return Data()[index];
    }

    /**
    \brief Copies data to a ListInfo structure..
    \param [out] listInfo If non-null, information describing the list.
    \param listData Data to copy in the ListInfo structure.
    \return true if it succeeds, false if it fails.
    */
    static bool CopyData(ListInfo* listInfo, const std::vector<Type> & listData)
    {
        if(!listInfo)
            return false;
        listInfo->count = int(listData.size());
        listInfo->size = listInfo->count * sizeof(Type);
        if(listInfo->count)
        {
            listInfo->data = BridgeAlloc(listInfo->size);
            Type* curItem = reinterpret_cast<Type*>(listInfo->data);
            for(const auto & item : listData)
            {
                *curItem = item;
                ++curItem;
            }
        }
        else
            listInfo->data = nullptr;
        return true;
    }

    static bool Free(const ListInfo* listInfo)
    {
        if(!listInfo || listInfo->size != listInfo->count * sizeof(Type) || (listInfo->count && !listInfo->data))
            return false;
        BridgeFree(listInfo->data);
        return true;
    }

    static bool ToVector(const ListInfo* listInfo, std::vector<Type> & listData, bool freedata = true)
    {
        if(!listInfo || listInfo->size != listInfo->count * sizeof(Type) || (listInfo->count && !listInfo->data))
            return false;
        listData.resize(listInfo->count);
        for(int i = 0; i < listInfo->count; i++)
            listData[i] = ((Type*)listInfo->data)[i];
        if(freedata && listInfo->data)
            BridgeFree(listInfo->data);
        return true;
    }

private:
    ListInfo _listInfo;
};

#endif //__cplusplus

#endif //_LIST_H
```

`CodeCleaner/pluginsdk/bridgemain.h`:

```h
#ifndef _BRIDGEMAIN_H_
#define _BRIDGEMAIN_H_

#include <windows.h>

#ifndef __cplusplus
#include <stdbool.h>
#define DEFAULT_PARAM(name, value) name
#else
#define DEFAULT_PARAM(name, value) name = value
#endif

//default structure alignments forced
#ifdef _WIN64
#pragma pack(push, 16)
#else //x86
#pragma pack(push, 8)
#endif //_WIN64

#ifdef _WIN64
typedef unsigned long long duint;
typedef signed long long dsint;
#else
typedef unsigned long duint;
typedef signed long dsint;
#endif //_WIN64

#ifndef BRIDGE_IMPEXP
#ifdef BUILD_BRIDGE
#define BRIDGE_IMPEXP __declspec(dllexport)
#else
#define BRIDGE_IMPEXP __declspec(dllimport)
#endif //BUILD_BRIDGE
#endif //BRIDGE_IMPEXP

#ifdef __cplusplus
extern "C"
{
#endif

//Bridge defines
#define MAX_SETTING_SIZE 65536
#define DBG_VERSION 25

//Bridge functions

/// <summary>
/// Initialize the bridge.
/// </summary>
/// <returns>On error it returns a non-null error message.</returns>
BRIDGE_IMPEXP const wchar_t* BridgeInit();

/// <summary>
/// Start the bridge.
/// </summary>
/// <returns>On error it returns a non-null error message.</returns>
BRIDGE_IMPEXP const wchar_t* BridgeStart();

/// <summary>
/// Allocate buffer. Use BridgeFree to free the buffer.
/// </summary>
/// <param name="size">Size in bytes of the buffer to allocate.</param>
/// <returns>A pointer to the allocated buffer. This function will trigger a crash dump if unsuccessful.</returns>
BRIDGE_IMPEXP void* BridgeAlloc(size_t size);

/// <summary>
/// Free buffer allocated by BridgeAlloc.
/// </summary>
/// <param name="ptr">Buffer to free.</param>
BRIDGE_IMPEXP void BridgeFree(void* ptr);

/// <summary>
/// Get a string setting from the in-memory setting store.
/// </summary>
/// <param name="section">Section the setting is in. Cannot be null.</param>
/// <param name="key">Setting key (name). Cannot be null.</param>
/// <param name="value">Output buffer for the value. Should be of MAX_SETTING_SIZE. Cannot be null.</param>
/// <returns>True if the setting was found and copied in the value parameter.</returns>
BRIDGE_IMPEXP bool BridgeSettingGet(const char* section, const char* key, char* value);

/// <summary>
/// Get an integer setting from the in-memory setting store.
/// </summary>
/// <param name="section">Section the setting is in. Cannot be null.</param>
/// <param name="key">Setting key (name). Cannot be null.</param>
/// <param name="value">Output value.</param>
/// <returns>True if the setting was found and successfully converted to an integer.</returns>
BRIDGE_IMPEXP bool BridgeSettingGetUint(const char* section, const char* key, duint* value);

/// <summary>
/// Set a string setting in the in-memory setting store.
/// </summary>
/// <param name="section">Section the setting is in. Cannot be null.</param>
/// <param name="key">Setting key (name). Set to null to clear the whole section.</param>
/// <param name="value">New setting value. Set to null to remove the key from the section.</param>
/// <returns>True if the operation was successful.</returns>
BRIDGE_IMPEXP bool BridgeSettingSet(const char* section, const char* key, const char* value);

/// <summary>
/// Set an integer setting in the in-memory setting store.
/// </summary>
/// <param name="section">Section the setting is in. Cannot be null.</param>
/// <param name="key">Setting key (name). Set to null to clear the whole section.</param>
/// <param name="value">New setting value.</param>
/// <returns>True if the operation was successful.</returns>
BRIDGE_IMPEXP bool BridgeSettingSetUint(const char* section, const char* key, duint value);

/// <summary>
/// Flush the in-memory setting store to disk.
/// </summary>
/// <returns></returns>
BRIDGE_IMPEXP bool BridgeSettingFlush();

/// <summary>
/// Read the in-memory setting store from disk.
/// </summary>
/// <param name="errorLine">Line where the error occurred. Set to null to ignore this.</param>
/// <returns>True if the setting were read and parsed correctly.</returns>
BRIDGE_IMPEXP bool BridgeSettingRead(int* errorLine);

/// <summary>
/// Get the debugger version.
/// </summary>
/// <returns>25</returns>
BRIDGE_IMPEXP int BridgeGetDbgVersion();

/// <summary>
/// Checks if the current process is elevated.
/// </summary>
/// <returns>true if the process is elevated, false otherwise.</returns>
BRIDGE_IMPEXP bool BridgeIsProcessElevated();

#ifdef __cplusplus
}
#endif

//list structure (and C++ wrapper)
#include "bridgelist.h"

#include "bridgegraph.h"

#ifdef __cplusplus
extern "C"
{
#endif

//Debugger defines
#define MAX_LABEL_SIZE 256
#define MAX_COMMENT_SIZE 512
#define MAX_MODULE_SIZE 256
#define MAX_IMPORT_SIZE 65536
#define MAX_BREAKPOINT_SIZE 256
#define MAX_CONDITIONAL_EXPR_SIZE 256
#define MAX_CONDITIONAL_TEXT_SIZE 256
#define MAX_SCRIPT_LINE_SIZE 2048
#define MAX_THREAD_NAME_SIZE 256
#define MAX_WATCH_NAME_SIZE 256
#define MAX_STRING_SIZE 512
#define MAX_ERROR_SIZE 512
#define RIGHTS_STRING_SIZE (sizeof("ERWCG") + 1)
#define MAX_SECTION_SIZE 10
#define MAX_COMMAND_LINE_SIZE 256
#define MAX_MNEMONIC_SIZE 64
#define PAGE_SIZE 0x1000

//Debugger enums
typedef enum
{
    initialized,
    paused,
    running,
    stopped
} DBGSTATE;

typedef enum
{
    SEG_DEFAULT,
    SEG_ES,
    SEG_DS,
    SEG_FS,
    SEG_GS,
    SEG_CS,
    SEG_SS
} SEGMENTREG;

typedef enum
{
    flagmodule = 0x1,
    flaglabel = 0x2,
    flagcomment = 0x4,
    flagbookmark = 0x8,
    flagfunction = 0x10,
    flagloop = 0x20,
    flagargs = 0x40,
    flagNoFuncOffset = 0x80
} ADDRINFOFLAGS;

typedef enum
{
    bp_none = 0,
    bp_normal = 1,
    bp_hardware = 2,
    bp_memory = 4,
    bp_dll = 8,
    bp_exception = 16
} BPXTYPE;

typedef enum
{
    FUNC_NONE,
    FUNC_BEGIN,
    FUNC_MIDDLE,
    FUNC_END,
    FUNC_SINGLE
} FUNCTYPE;

typedef enum
{
    LOOP_NONE,
    LOOP_BEGIN,
    LOOP_MIDDLE,
    LOOP_ENTRY,
    LOOP_END,
    LOOP_SINGLE
} LOOPTYPE;

//order by most important type last
typedef enum
{
    XREF_NONE,
    XREF_DATA,
    XREF_JMP,
    XREF_CALL
} XREFTYPE;

typedef enum
{
    ARG_NONE,
    ARG_BEGIN,
    ARG_MIDDLE,
    ARG_END,
    ARG_SINGLE
} ARGTYPE;

typedef enum
{
    DBG_SCRIPT_LOAD,                // param1=const char* filename,      param2=unused
    DBG_SCRIPT_UNLOAD,              // param1=unused,                    param2=unused
    DBG_SCRIPT_RUN,                 // param1=int destline,              param2=unused
    DBG_SCRIPT_STEP,                // param1=unused,                    param2=unused
    DBG_SCRIPT_BPTOGGLE,            // param1=int line,                  param2=unused
    DBG_SCRIPT_BPGET,               // param1=int line,                  param2=unused
    DBG_SCRIPT_CMDEXEC,             // param1=const char* command,       param2=unused
    DBG_SCRIPT_ABORT,               // param1=unused,                    param2=unused
    DBG_SCRIPT_GETLINETYPE,         // param1=int line,                  param2=unused
    DBG_SCRIPT_SETIP,               // param1=int line,                  param2=unused
    DBG_SCRIPT_GETBRANCHINFO,       // param1=int line,                  param2=SCRIPTBRANCH* info
    DBG_SYMBOL_ENUM,                // param1=SYMBOLCBINFO* cbInfo,      param2=unused
    DBG_ASSEMBLE_AT,                // param1=duint addr,                param2=const char* instruction
    DBG_MODBASE_FROM_NAME,          // param1=const char* modname,       param2=unused
    DBG_DISASM_AT,                  // param1=duint addr,                 param2=DISASM_INSTR* instr
    DBG_STACK_COMMENT_GET,          // param1=duint addr,                param2=STACK_COMMENT* comment
    DBG_GET_THREAD_LIST,            // param1=THREADALLINFO* list,       param2=unused
    DBG_SETTINGS_UPDATED,           // param1=unused,                    param2=unused
    DBG_DISASM_FAST_AT,             // param1=duint addr,                param2=BASIC_INSTRUCTION_INFO* basicinfo
    DBG_MENU_ENTRY_CLICKED,         // param1=int hEntry,                param2=unused
    DBG_FUNCTION_GET,               // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_FUNCTION_OVERLAPS,          // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_FUNCTION_ADD,               // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_FUNCTION_DEL,               // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_LOOP_GET,                   // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_LOOP_OVERLAPS,              // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_LOOP_ADD,                   // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_LOOP_DEL,                   // param1=FUNCTION_LOOP_INFO* info,  param2=unused
    DBG_IS_RUN_LOCKED,              // param1=unused,                    param2=unused
    DBG_IS_BP_DISABLED,             // param1=duint addr,                param2=unused
    DBG_SET_AUTO_COMMENT_AT,        // param1=duint addr,                param2=const char* text
    DBG_DELETE_AUTO_COMMENT_RANGE,  // param1=duint start,               param2=duint end
    DBG_SET_AUTO_LABEL_AT,          // param1=duint addr,                param2=const char* text
    DBG_DELETE_AUTO_LABEL_RANGE,    // param1=duint start,               param2=duint end
    DBG_SET_AUTO_BOOKMARK_AT,       // param1=duint addr,                param2=const char* text
    DBG_DELETE_AUTO_BOOKMARK_RANGE, // param1=duint start,               param2=duint end
    DBG_SET_AUTO_FUNCTION_AT,       // param1=duint addr,                param2=const char* text
    DBG_DELETE_AUTO_FUNCTION_RANGE, // param1=duint start,               param2=duint end
    DBG_GET_STRING_AT,              // param1=duint addr,                param2=unused
    DBG_GET_FUNCTIONS,              // param1=unused,                    param2=unused
    DBG_WIN_EVENT,                  // param1=MSG* message,              param2=long* result
    DBG_WIN_EVENT_GLOBAL,           // param1=MSG* message,              param2=unused
    DBG_INITIALIZE_LOCKS,           // param1=unused,                    param2=unused
    DBG_DEINITIALIZE_LOCKS,         // param1=unused,                    param2=unused
    DBG_GET_TIME_WASTED_COUNTER,    // param1=unused,                    param2=unused
    DBG_SYMBOL_ENUM_FROMCACHE,      // param1=SYMBOLCBINFO* cbInfo,      param2=unused
    DBG_DELETE_COMMENT_RANGE,       // param1=duint start,               param2=duint end
    DBG_DELETE_LABEL_RANGE,         // param1=duint start,               param2=duint end
    DBG_DELETE_BOOKMARK_RANGE,      // param1=duint start,               param2=duint end
    DBG_GET_XREF_COUNT_AT,          // param1=duint addr,                param2=unused
    DBG_GET_XREF_TYPE_AT,           // param1=duint addr,                param2=unused
    DBG_XREF_ADD,                   // param1=duint addr,                param2=duint from
    DBG_XREF_DEL_ALL,               // param1=duint addr,                param2=unused
    DBG_XREF_GET,                   // param1=duint addr,                param2=XREF_INFO* info
    DBG_GET_ENCODE_TYPE_BUFFER,     // param1=duint addr,                param2=unused
    DBG_ENCODE_TYPE_GET,            // param1=duint addr,                param2=duint size
    DBG_DELETE_ENCODE_TYPE_RANGE,   // param1=duint start,               param2=duint end
    DBG_ENCODE_SIZE_GET,            // param1=duint addr,                param2=duint codesize
    DBG_DELETE_ENCODE_TYPE_SEG,     // param1=duint addr,                param2=unused
    DBG_RELEASE_ENCODE_TYPE_BUFFER, // param1=void* buffer,              param2=unused
    DBG_ARGUMENT_GET,               // param1=FUNCTION* info,            param2=unused
    DBG_ARGUMENT_OVERLAPS,          // param1=FUNCTION* info,            param2=unused
    DBG_ARGUMENT_ADD,               // param1=FUNCTION* info,            param2=unused
    DBG_ARGUMENT_DEL,               // param1=FUNCTION* info,            param2=unused
    DBG_GET_WATCH_LIST,             // param1=ListOf(WATCHINFO),         param2=unused
    DBG_SELCHANGED,                 // param1=hWindow,                   param2=VA
    DBG_GET_PROCESS_HANDLE,         // param1=unused,                    param2=unused
    DBG_GET_THREAD_HANDLE,          // param1=unused,                    param2=unused
    DBG_GET_PROCESS_ID,             // param1=unused,                    param2=unused
    DBG_GET_THREAD_ID,              // param1=unused,                    param2=unused
    DBG_GET_PEB_ADDRESS,            // param1=DWORD ProcessId,           param2=unused
    DBG_GET_TEB_ADDRESS,            // param1=DWORD ThreadId,            param2=unused
    DBG_ANALYZE_FUNCTION,           // param1=BridgeCFGraphList* graph,  param2=duint entry
    DBG_MENU_PREPARE,               // param1=int hMenu,                 param2=unused
    DBG_GET_SYMBOL_INFO,            // param1=void* symbol,              param2=SYMBOLINFO* info
    DBG_GET_DEBUG_ENGINE,           // param1=unused,                    param2-unused
} DBGMSG;

typedef enum
{
    linecommand,
    linebranch,
    linelabel,
    linecomment,
    lineempty,
} SCRIPTLINETYPE;

typedef enum
{
    scriptnobranch,
    scriptjmp,
    scriptjnejnz,
    scriptjejz,
    scriptjbjl,
    scriptjajg,
    scriptjbejle,
    scriptjaejge,
    scriptcall
} SCRIPTBRANCHTYPE;

typedef enum
{
    instr_normal,
    instr_branch,
    instr_stack
} DISASM_INSTRTYPE;

typedef enum
{
    arg_normal,
    arg_memory
} DISASM_ARGTYPE;

typedef enum
{
    str_none,
    str_ascii,
    str_unicode
} STRING_TYPE;

typedef enum
{
    _PriorityIdle = -15,
    _PriorityAboveNormal = 1,
    _PriorityBelowNormal = -1,
    _PriorityHighest = 2,
    _PriorityLowest = -2,
    _PriorityNormal = 0,
    _PriorityTimeCritical = 15,
    _PriorityUnknown = 0x7FFFFFFF
} THREADPRIORITY;

typedef enum
{
    _Executive = 0,
    _FreePage = 1,
    _PageIn = 2,
    _PoolAllocation = 3,
    _DelayExecution = 4,
    _Suspended = 5,
    _UserRequest = 6,
    _WrExecutive = 7,
    _WrFreePage = 8,
    _WrPageIn = 9,
    _WrPoolAllocation = 10,
    _WrDelayExecution = 11,
    _WrSuspended = 12,
    _WrUserRequest = 13,
    _WrEventPair = 14,
    _WrQueue = 15,
    _WrLpcReceive = 16,
    _WrLpcReply = 17,
    _WrVirtualMemory = 18,
    _WrPageOut = 19,
    _WrRendezvous = 20,
    _Spare2 = 21,
    _Spare3 = 22,
    _Spare4 = 23,
    _Spare5 = 24,
    _WrCalloutStack = 25,
    _WrKernel = 26,
    _WrResource = 27,
    _WrPushLock = 28,
    _WrMutex = 29,
    _WrQuantumEnd = 30,
    _WrDispatchInt = 31,
    _WrPreempted = 32,
    _WrYieldExecution = 33,
    _WrFastMutex = 34,
    _WrGuardedMutex = 35,
    _WrRundown = 36,
} THREADWAITREASON;

typedef enum
{
    size_byte = 1,
    size_word = 2,
    size_dword = 4,
    size_qword = 8,
    size_xmmword = 16,
    size_ymmword = 32
} MEMORY_SIZE;

typedef enum
{
    enc_unknown,  //must be 0
    enc_byte,     //1 byte
    enc_word,     //2 bytes
    enc_dword,    //4 bytes
    enc_fword,    //6 bytes
    enc_qword,    //8 bytes
    enc_tbyte,    //10 bytes
    enc_oword,    //16 bytes
    enc_mmword,   //8 bytes
    enc_xmmword,  //16 bytes
    enc_ymmword,  //32 bytes
    enc_zmmword,  //64 bytes avx512 not supported
    enc_real4,    //4 byte float
    enc_real8,    //8 byte double
    enc_real10,   //10 byte decimal
    enc_ascii,    //ascii sequence
    enc_unicode,  //unicode sequence
    enc_code,     //start of code
    enc_junk,     //junk code
    enc_middle    //middle of data
} ENCODETYPE;

typedef enum
{
    TYPE_UINT, // unsigned integer
    TYPE_INT,  // signed integer
    TYPE_FLOAT,// single precision floating point value
    TYPE_ASCII, // ascii string
    TYPE_UNICODE, // unicode string
    TYPE_INVALID // invalid watch expression or data type
} WATCHVARTYPE;

typedef enum
{
    MODE_DISABLED, // watchdog is disabled
    MODE_ISTRUE,   // alert if expression is not 0
    MODE_ISFALSE,  // alert if expression is 0
    MODE_CHANGED,  // alert if expression is changed
    MODE_UNCHANGED // alert if expression is not changed
} WATCHDOGMODE;

typedef enum
{
    hw_access,
    hw_write,
    hw_execute
} BPHWTYPE;

typedef enum
{
    mem_access,
    mem_read,
    mem_write,
    mem_execute
} BPMEMTYPE;

typedef enum
{
    dll_load = 1,
    dll_unload,
    dll_all
} BPDLLTYPE;

typedef enum
{
    ex_firstchance = 1,
    ex_secondchance,
    ex_all
} BPEXTYPE;

typedef enum
{
    hw_byte,
    hw_word,
    hw_dword,
    hw_qword
} BPHWSIZE;

typedef enum
{
    sym_import,
    sym_export,
    sym_symbol
} SYMBOLTYPE;

typedef enum
{
    mod_user,
    mod_system
} MODULEPARTY;

typedef enum
{
    DebugEngineTitanEngine,
    DebugEngineGleeBug,
    DebugEngineStaticEngine,
} DEBUG_ENGINE;

//Debugger typedefs
typedef MEMORY_SIZE VALUE_SIZE;

typedef struct DBGFUNCTIONS_ DBGFUNCTIONS;

typedef bool (*CBSYMBOLENUM)(const struct SYMBOLPTR_* symbol, void* user);

//Debugger structs
typedef struct
{
    MEMORY_BASIC_INFORMATION mbi;
    char info[MAX_MODULE_SIZE];
} MEMPAGE;

typedef struct
{
    int count;
    MEMPAGE* page;
} MEMMAP;

typedef struct
{
    BPXTYPE type;
    duint addr;
    bool enabled;
    bool singleshoot;
    bool active;
    char name[MAX_BREAKPOINT_SIZE];
    char mod[MAX_MODULE_SIZE];
    unsigned short slot;
    // extended part
    unsigned char typeEx; //BPHWTYPE/BPMEMTYPE/BPDLLTYPE/BPEXTYPE
    unsigned char hwSize; //BPHWSIZE
    unsigned int hitCount;
    bool fastResume;
    bool silent;
    char breakCondition[MAX_CONDITIONAL_EXPR_SIZE];
    char logText[MAX_CONDITIONAL_TEXT_SIZE];
    char logCondition[MAX_CONDITIONAL_EXPR_SIZE];
    char commandText[MAX_CONDITIONAL_TEXT_SIZE];
    char commandCondition[MAX_CONDITIONAL_EXPR_SIZE];
} BRIDGEBP;

typedef struct
{
    int count;
    BRIDGEBP* bp;
} BPMAP;

typedef struct
{
    char WatchName[MAX_WATCH_NAME_SIZE];
    char Expression[MAX_CONDITIONAL_EXPR_SIZE];
    unsigned int window;
    unsigned int id;
    WATCHVARTYPE varType;
    WATCHDOGMODE watchdogMode;
    duint value;
    bool watchdogTriggered;
} WATCHINFO;

typedef struct
{
    duint start; //OUT
    duint end; //OUT
    duint instrcount; //OUT
} FUNCTION;

typedef struct
{
    int depth; //IN
    duint start; //OUT
    duint end; //OUT
    duint instrcount; //OUT
} LOOP;

typedef struct
{
    int flags; //ADDRINFOFLAGS (IN)
    char module[MAX_MODULE_SIZE]; //module the address is in
    char label[MAX_LABEL_SIZE];
    char comment[MAX_COMMENT_SIZE];
    bool isbookmark;
    FUNCTION function;
    LOOP loop;
    FUNCTION args;
} BRIDGE_ADDRINFO;

typedef struct SYMBOLINFO_
{
    duint addr;
    char* decoratedSymbol;
    char* undecoratedSymbol;
    SYMBOLTYPE type;
    bool freeDecorated;
    bool freeUndecorated;
    DWORD ordinal;
} SYMBOLINFO;

typedef struct
{
    duint base;
    char name[MAX_MODULE_SIZE];
} SYMBOLMODULEINFO;

typedef struct
{
    duint base;
    CBSYMBOLENUM cbSymbolEnum;
    void* user;
} SYMBOLCBINFO;

typedef struct
{
    bool c;
    bool p;
    bool a;
    bool z;
    bool s;
    bool t;
    bool i;
    bool d;
    bool o;
} FLAGS;

typedef struct
{
    bool FZ;
    bool PM;
    bool UM;
    bool OM;
    bool ZM;
    bool IM;
    bool DM;
    bool DAZ;
    bool PE;
    bool UE;
    bool OE;
    bool ZE;
    bool DE;
    bool IE;

    unsigned short RC;
} MXCSRFIELDS;

typedef struct
{
    bool B;
    bool C3;
    bool C2;
    bool C1;
    bool C0;
    bool ES;
    bool SF;
    bool P;
    bool U;
    bool O;
    bool Z;
    bool D;
    bool I;

    unsigned short TOP;

} X87STATUSWORDFIELDS;

typedef struct
{
    bool IC;
    bool IEM;
    bool PM;
    bool UM;
    bool OM;
    bool ZM;
    bool DM;
    bool IM;

    unsigned short RC;
    unsigned short PC;

} X87CONTROLWORDFIELDS;

typedef struct DECLSPEC_ALIGN(16) _XMMREGISTER
{
    ULONGLONG Low;
    LONGLONG High;
} XMMREGISTER;

typedef struct
{
    XMMREGISTER Low; //XMM/SSE part
    XMMREGISTER High; //AVX part
} YMMREGISTER;

typedef struct
{
    BYTE    data[10];
    int     st_value;
    int     tag;
} X87FPUREGISTER;

typedef struct
{
    WORD   ControlWord;
    WORD   StatusWord;
    WORD   TagWord;
    DWORD   ErrorOffset;
    DWORD   ErrorSelector;
    DWORD   DataOffset;
    DWORD   DataSelector;
    DWORD   Cr0NpxState;
} X87FPU;

typedef struct
{
    ULONG_PTR cax;
    ULONG_PTR ccx;
    ULONG_PTR cdx;
    ULONG_PTR cbx;
    ULONG_PTR csp;
    ULONG_PTR cbp;
    ULONG_PTR csi;
    ULONG_PTR cdi;
#ifdef _WIN64
    ULONG_PTR r8;
    ULONG_PTR r9;
    ULONG_PTR r10;
    ULONG_PTR r11;
    ULONG_PTR r12;
    ULONG_PTR r13;
    ULONG_PTR r14;
    ULONG_PTR r15;
#endif //_WIN64
    ULONG_PTR cip;
    ULONG_PTR eflags;
    unsigned short gs;
    unsigned short fs;
    unsigned short es;
    unsigned short ds;
    unsigned short cs;
    unsigned short ss;
    ULONG_PTR dr0;
    ULONG_PTR dr1;
    ULONG_PTR dr2;
    ULONG_PTR dr3;
    ULONG_PTR dr6;
    ULONG_PTR dr7;
    BYTE RegisterArea[80];
    X87FPU x87fpu;
    DWORD MxCsr;
#ifdef _WIN64
    XMMREGISTER XmmRegisters[16];
    YMMREGISTER YmmRegisters[16];
#else // x86
    XMMREGISTER XmmRegisters[8];
    YMMREGISTER YmmRegisters[8];
#endif
} REGISTERCONTEXT;

typedef struct
{
    DWORD code;
    char name[128];
} LASTERROR;

typedef struct
{
    DWORD code;
    char name[128];
} LASTSTATUS;

typedef struct
{
    REGISTERCONTEXT regcontext;
    FLAGS flags;
    X87FPUREGISTER x87FPURegisters[8];
    unsigned long long mmx[8];
    MXCSRFIELDS MxCsrFields;
    X87STATUSWORDFIELDS x87StatusWordFields;
    X87CONTROLWORDFIELDS x87ControlWordFields;
    LASTERROR lastError;
    LASTSTATUS lastStatus;
} REGDUMP;

typedef struct
{
    DISASM_ARGTYPE type; //normal/memory
    SEGMENTREG segment;
    char mnemonic[64];
    duint constant; //constant in the instruction (imm/disp)
    duint value; //equal to constant or equal to the register value
    duint memvalue; //memsize:[value]
} DISASM_ARG;

typedef struct
{
    char instruction[64];
    DISASM_INSTRTYPE type;
    int argcount;
    int instr_size;
    DISASM_ARG arg[3];
} DISASM_INSTR;

typedef struct
{
    char color[8]; //hex color-code
    char comment[MAX_COMMENT_SIZE];
} STACK_COMMENT;

typedef struct
{
    int ThreadNumber;
    HANDLE Handle;
    DWORD ThreadId;
    duint ThreadStartAddress;
    duint ThreadLocalBase;
    char threadName[MAX_THREAD_NAME_SIZE];
} THREADINFO;

typedef struct
{
    THREADINFO BasicInfo;
    duint ThreadCip;
    DWORD SuspendCount;
    THREADPRIORITY Priority;
    THREADWAITREASON WaitReason;
    DWORD LastError;
    FILETIME UserTime;
    FILETIME KernelTime;
    FILETIME CreationTime;
    ULONG64 Cycles; // Windows Vista or greater
} THREADALLINFO;

typedef struct
{
    int count;
    THREADALLINFO* list;
    int CurrentThread;
} THREADLIST;

typedef struct
{
    duint value; //displacement / addrvalue (rip-relative)
    MEMORY_SIZE size; //byte/word/dword/qword
    char mnemonic[MAX_MNEMONIC_SIZE];
} MEMORY_INFO;

typedef struct
{
    duint value;
    VALUE_SIZE size;
} VALUE_INFO;

//definitions for BASIC_INSTRUCTION_INFO.type
#define TYPE_VALUE 1
#define TYPE_MEMORY 2
#define TYPE_ADDR 4

typedef struct
{
    DWORD type; //value|memory|addr
    VALUE_INFO value; //immediat
    MEMORY_INFO memory;
    duint addr; //addrvalue (jumps + calls)
    bool branch; //jumps/calls
    bool call; //instruction is a call
    int size;
    char instruction[MAX_MNEMONIC_SIZE * 4];
} BASIC_INSTRUCTION_INFO;

typedef struct
{
    SCRIPTBRANCHTYPE type;
    int dest;
    char branchlabel[256];
} SCRIPTBRANCH;

typedef struct
{
    duint addr;
    duint start;
    duint end;
    bool manual;
    int depth;
} FUNCTION_LOOP_INFO;

typedef struct
{
    duint addr;
    XREFTYPE type;
} XREF_RECORD;

typedef struct
{
    duint refcount;
    XREF_RECORD* references;
} XREF_INFO;

typedef struct SYMBOLPTR_
{
    duint modbase;
    const void* symbol;
} SYMBOLPTR;

//Debugger functions
BRIDGE_IMPEXP const char* DbgInit();
BRIDGE_IMPEXP void DbgExit();
BRIDGE_IMPEXP bool DbgMemRead(duint va, void* dest, duint size);
BRIDGE_IMPEXP bool DbgMemWrite(duint va, const void* src, duint size);
BRIDGE_IMPEXP duint DbgMemGetPageSize(duint base);
BRIDGE_IMPEXP duint DbgMemFindBaseAddr(duint addr, duint* size);

/// <summary>
/// Asynchronously execute a debugger command by adding it to the command queue.
/// Note: the command may not have completed before this call returns. Use this
/// function if you don't care when the command gets executed.
///
/// Example: DbgCmdExec("ClearLog")
/// </summary>
/// <param name="cmd">The command to execute.</param>
/// <returns>True if the command was successfully submitted to the command queue. False if the submission failed.</returns>
BRIDGE_IMPEXP bool DbgCmdExec(const char* cmd);

/// <summary>
/// Performs synchronous execution of a debugger command. This function call only
/// returns after the command has completed.
///
/// Example: DbgCmdExecDirect("loadlib advapi32.dll")
/// </summary>
/// <param name="cmd">The command to execute.</param>
/// <returns>True if the command executed successfully, False if there was a problem.</returns>
BRIDGE_IMPEXP bool DbgCmdExecDirect(const char* cmd);
BRIDGE_IMPEXP bool DbgMemMap(MEMMAP* memmap);
BRIDGE_IMPEXP bool DbgIsValidExpression(const char* expression);
BRIDGE_IMPEXP bool DbgIsDebugging();
BRIDGE_IMPEXP bool DbgIsJumpGoingToExecute(duint addr);
BRIDGE_IMPEXP bool DbgGetLabelAt(duint addr, SEGMENTREG segment, char* text);
BRIDGE_IMPEXP bool DbgSetLabelAt(duint addr, const char* text);
BRIDGE_IMPEXP void DbgClearLabelRange(duint start, duint end);
BRIDGE_IMPEXP bool DbgGetCommentAt(duint addr, char* text);
BRIDGE_IMPEXP bool DbgSetCommentAt(duint addr, const char* text);
BRIDGE_IMPEXP void DbgClearCommentRange(duint start, duint end);
BRIDGE_IMPEXP bool DbgGetBookmarkAt(duint addr);
BRIDGE_IMPEXP bool DbgSetBookmarkAt(duint addr, bool isbookmark);
BRIDGE_IMPEXP void DbgClearBookmarkRange(duint start, duint end);
BRIDGE_IMPEXP bool DbgGetModuleAt(duint addr, char* text);
BRIDGE_IMPEXP BPXTYPE DbgGetBpxTypeAt(duint addr);
BRIDGE_IMPEXP duint DbgValFromString(const char* string);
BRIDGE_IMPEXP bool DbgGetRegDumpEx(REGDUMP* regdump, size_t size);
BRIDGE_IMPEXP bool DbgValToString(const char* string, duint value);
BRIDGE_IMPEXP bool DbgMemIsValidReadPtr(duint addr);
BRIDGE_IMPEXP int DbgGetBpList(BPXTYPE type, BPMAP* list);
BRIDGE_IMPEXP FUNCTYPE DbgGetFunctionTypeAt(duint addr);
BRIDGE_IMPEXP LOOPTYPE DbgGetLoopTypeAt(duint addr, int depth);
BRIDGE_IMPEXP duint DbgGetBranchDestination(duint addr);
BRIDGE_IMPEXP void DbgScriptLoad(const char* filename);
BRIDGE_IMPEXP void DbgScriptUnload();
BRIDGE_IMPEXP void DbgScriptRun(int destline);
BRIDGE_IMPEXP void DbgScriptStep();
BRIDGE_IMPEXP bool DbgScriptBpToggle(int line);
BRIDGE_IMPEXP bool DbgScriptBpGet(int line);
BRIDGE_IMPEXP bool DbgScriptCmdExec(const char* command);
BRIDGE_IMPEXP void DbgScriptAbort();
BRIDGE_IMPEXP SCRIPTLINETYPE DbgScriptGetLineType(int line);
BRIDGE_IMPEXP void DbgScriptSetIp(int line);
BRIDGE_IMPEXP bool DbgScriptGetBranchInfo(int line, SCRIPTBRANCH* info);
BRIDGE_IMPEXP void DbgSymbolEnum(duint base, CBSYMBOLENUM cbSymbolEnum, void* user);
BRIDGE_IMPEXP void DbgSymbolEnumFromCache(duint base, CBSYMBOLENUM cbSymbolEnum, void* user);
BRIDGE_IMPEXP bool DbgAssembleAt(duint addr, const char* instruction);
BRIDGE_IMPEXP duint DbgModBaseFromName(const char* name);
BRIDGE_IMPEXP void DbgDisasmAt(duint addr, DISASM_INSTR* instr);
BRIDGE_IMPEXP bool DbgStackCommentGet(duint addr, STACK_COMMENT* comment);
BRIDGE_IMPEXP void DbgGetThreadList(THREADLIST* list);
BRIDGE_IMPEXP void DbgSettingsUpdated();
BRIDGE_IMPEXP void DbgDisasmFastAt(duint addr, BASIC_INSTRUCTION_INFO* basicinfo);
BRIDGE_IMPEXP void DbgMenuEntryClicked(int hEntry);
BRIDGE_IMPEXP bool DbgFunctionGet(duint addr, duint* start, duint* end);
BRIDGE_IMPEXP bool DbgFunctionOverlaps(duint start, duint end);
BRIDGE_IMPEXP bool DbgFunctionAdd(duint start, duint end);
BRIDGE_IMPEXP bool DbgFunctionDel(duint addr);
BRIDGE_IMPEXP bool DbgArgumentGet(duint addr, duint* start, duint* end);
BRIDGE_IMPEXP bool DbgArgumentOverlaps(duint start, duint end);
BRIDGE_IMPEXP bool DbgArgumentAdd(duint start, duint end);
BRIDGE_IMPEXP bool DbgArgumentDel(duint addr);
BRIDGE_IMPEXP bool DbgLoopGet(int depth, duint addr, duint* start, duint* end);
BRIDGE_IMPEXP bool DbgLoopOverlaps(int depth, duint start, duint end);
BRIDGE_IMPEXP bool DbgLoopAdd(duint start, duint end);
BRIDGE_IMPEXP bool DbgLoopDel(int depth, duint addr);
BRIDGE_IMPEXP bool DbgXrefAdd(duint addr, duint from);
BRIDGE_IMPEXP bool DbgXrefDelAll(duint addr);
BRIDGE_IMPEXP bool DbgXrefGet(duint addr, XREF_INFO* info);
BRIDGE_IMPEXP size_t DbgGetXrefCountAt(duint addr);
BRIDGE_IMPEXP XREFTYPE DbgGetXrefTypeAt(duint addr);
BRIDGE_IMPEXP bool DbgIsRunLocked();
BRIDGE_IMPEXP bool DbgIsBpDisabled(duint addr);
BRIDGE_IMPEXP bool DbgSetAutoCommentAt(duint addr, const char* text);
BRIDGE_IMPEXP void DbgClearAutoCommentRange(duint start, duint end);
BRIDGE_IMPEXP bool DbgSetAutoLabelAt(duint addr, const char* text);
BRIDGE_IMPEXP void DbgClearAutoLabelRange(duint start, duint end);
BRIDGE_IMPEXP bool DbgSetAutoBookmarkAt(duint addr);
BRIDGE_IMPEXP void DbgClearAutoBookmarkRange(duint start, duint end);
BRIDGE_IMPEXP bool DbgSetAutoFunctionAt(duint start, duint end);
BRIDGE_IMPEXP void DbgClearAutoFunctionRange(duint start, duint end);
BRIDGE_IMPEXP bool DbgGetStringAt(duint addr, char* text);
BRIDGE_IMPEXP const DBGFUNCTIONS* DbgFunctions();
BRIDGE_IMPEXP bool DbgWinEvent(MSG* message, long* result);
BRIDGE_IMPEXP bool DbgWinEventGlobal(MSG* message);
BRIDGE_IMPEXP bool DbgIsRunning();
BRIDGE_IMPEXP duint DbgGetTimeWastedCounter();
BRIDGE_IMPEXP ARGTYPE DbgGetArgTypeAt(duint addr);
BRIDGE_IMPEXP void* DbgGetEncodeTypeBuffer(duint addr, duint* size);
BRIDGE_IMPEXP void DbgReleaseEncodeTypeBuffer(void* buffer);
BRIDGE_IMPEXP ENCODETYPE DbgGetEncodeTypeAt(duint addr, duint size);
BRIDGE_IMPEXP duint DbgGetEncodeSizeAt(duint addr, duint codesize);
BRIDGE_IMPEXP bool DbgSetEncodeType(duint addr, duint size, ENCODETYPE type);
BRIDGE_IMPEXP void DbgDelEncodeTypeRange(duint start, duint end);
BRIDGE_IMPEXP void DbgDelEncodeTypeSegment(duint start);
BRIDGE_IMPEXP bool DbgGetWatchList(ListOf(WATCHINFO) list);
BRIDGE_IMPEXP void DbgSelChanged(int hWindow, duint VA);
BRIDGE_IMPEXP HANDLE DbgGetProcessHandle();
BRIDGE_IMPEXP HANDLE DbgGetThreadHandle();
BRIDGE_IMPEXP DWORD DbgGetProcessId();
BRIDGE_IMPEXP DWORD DbgGetThreadId();
BRIDGE_IMPEXP duint DbgGetPebAddress(DWORD ProcessId);
BRIDGE_IMPEXP duint DbgGetTebAddress(DWORD ThreadId);
BRIDGE_IMPEXP bool DbgAnalyzeFunction(duint entry, BridgeCFGraphList* graph);
BRIDGE_IMPEXP duint DbgEval(const char* expression, bool* DEFAULT_PARAM(success, nullptr));
BRIDGE_IMPEXP void DbgGetSymbolInfo(const SYMBOLPTR* symbolptr, SYMBOLINFO* info);
BRIDGE_IMPEXP DEBUG_ENGINE DbgGetDebugEngine();

//Gui defines
typedef enum
{
    GUI_PLUGIN_MENU,
    GUI_DISASM_MENU,
    GUI_DUMP_MENU,
    GUI_STACK_MENU,
    GUI_GRAPH_MENU,
    GUI_MEMMAP_MENU,
    GUI_SYMMOD_MENU,
} GUIMENUTYPE;

BRIDGE_IMPEXP void DbgMenuPrepare(GUIMENUTYPE hMenu);

typedef enum
{
    GUI_DISASSEMBLY,
    GUI_DUMP,
    GUI_STACK,
    GUI_GRAPH,
    GUI_MEMMAP,
    GUI_SYMMOD,
} GUISELECTIONTYPE;

#define GUI_MAX_LINE_SIZE 65536
#define GUI_MAX_DISASSEMBLY_SIZE 2048

//Gui enums
typedef enum
{
    GUI_DISASSEMBLE_AT,             // param1=(duint)va,            param2=(duint)cip
    GUI_SET_DEBUG_STATE,            // param1=(DBGSTATE)state,      param2=unused
    GUI_ADD_MSG_TO_LOG,             // param1=(const char*)msg,     param2=unused
    GUI_CLEAR_LOG,                  // param1=unused,               param2=unused
    GUI_UPDATE_REGISTER_VIEW,       // param1=unused,               param2=unused
    GUI_UPDATE_DISASSEMBLY_VIEW,    // param1=unused,               param2=unused
    GUI_UPDATE_BREAKPOINTS_VIEW,    // param1=unused,               param2=unused
    GUI_UPDATE_WINDOW_TITLE,        // param1=(const char*)file,    param2=unused
    GUI_GET_WINDOW_HANDLE,          // param1=unused,               param2=unused
    GUI_DUMP_AT,                    // param1=(duint)va             param2=unused
    GUI_SCRIPT_ADD,                 // param1=int count,            param2=const char** lines
    GUI_SCRIPT_CLEAR,               // param1=unused,               param2=unused
    GUI_SCRIPT_SETIP,               // param1=int line,             param2=unused
    GUI_SCRIPT_ERROR,               // param1=int line,             param2=const char* message
    GUI_SCRIPT_SETTITLE,            // param1=const char* title,    param2=unused
    GUI_SCRIPT_SETINFOLINE,         // param1=int line,             param2=const char* info
    GUI_SCRIPT_MESSAGE,             // param1=const char* message,  param2=unused
    GUI_SCRIPT_MSGYN,               // param1=const char* message,  param2=unused
    GUI_SYMBOL_LOG_ADD,             // param1(const char*)msg,      param2=unused
    GUI_SYMBOL_LOG_CLEAR,           // param1=unused,               param2=unused
    GUI_SYMBOL_SET_PROGRESS,        // param1=int percent           param2=unused
    GUI_SYMBOL_UPDATE_MODULE_LIST,  // param1=int count,            param2=SYMBOLMODULEINFO* modules
    GUI_REF_ADDCOLUMN,              // param1=int width,            param2=(const char*)title
    GUI_REF_SETROWCOUNT,            // param1=int rows,             param2=unused
    GUI_REF_GETROWCOUNT,            // param1=unused,               param2=unused
    GUI_REF_DELETEALLCOLUMNS,       // param1=unused,               param2=unused
    GUI_REF_SETCELLCONTENT,         // param1=(CELLINFO*)info,      param2=unused
    GUI_REF_GETCELLCONTENT,         // param1=int row,              param2=int col
    GUI_REF_RELOADDATA,             // param1=unused,               param2=unused
    GUI_REF_SETSINGLESELECTION,     // param1=int index,            param2=bool scroll
    GUI_REF_SETPROGRESS,            // param1=int progress,         param2=unused
    GUI_REF_SETCURRENTTASKPROGRESS, // param1=int progress,         param2=const char* taskTitle
    GUI_REF_SETSEARCHSTARTCOL,      // param1=int col               param2=unused
    GUI_STACK_DUMP_AT,              // param1=duint addr,           param2=duint csp
    GUI_UPDATE_DUMP_VIEW,           // param1=unused,               param2=unused
    GUI_UPDATE_THREAD_VIEW,         // param1=unused,               param2=unused
    GUI_ADD_RECENT_FILE,            // param1=(const char*)file,    param2=unused
    GUI_SET_LAST_EXCEPTION,         // param1=unsigned int code,    param2=unused
    GUI_GET_DISASSEMBLY,            // param1=duint addr,           param2=char* text
    GUI_MENU_ADD,                   // param1=int hMenu,            param2=const char* title
    GUI_MENU_ADD_ENTRY,             // param1=int hMenu,            param2=const char* title
    GUI_MENU_ADD_SEPARATOR,         // param1=int hMenu,            param2=unused
    GUI_MENU_CLEAR,                 // param1=int hMenu,            param2=unused
    GUI_SELECTION_GET,              // param1=GUISELECTIONTYPE,     param2=SELECTIONDATA* selection
    GUI_SELECTION_SET,              // param1=GUISELECTIONTYPE,     param2=const SELECTIONDATA* selection
    GUI_GETLINE_WINDOW,             // param1=const char* title,    param2=char* text
    GUI_AUTOCOMPLETE_ADDCMD,        // param1=const char* cmd,      param2=ununsed
    GUI_AUTOCOMPLETE_DELCMD,        // param1=const char* cmd,      param2=ununsed
    GUI_AUTOCOMPLETE_CLEARALL,      // param1=unused,               param2=unused
    GUI_SCRIPT_ENABLEHIGHLIGHTING,  // param1=bool enable,          param2=unused
    GUI_ADD_MSG_TO_STATUSBAR,       // param1=const char* msg,      param2=unused
    GUI_UPDATE_SIDEBAR,             // param1=unused,               param2=unused
    GUI_REPAINT_TABLE_VIEW,         // param1=unused,               param2=unused
    GUI_UPDATE_PATCHES,             // param1=unused,               param2=unused
    GUI_UPDATE_CALLSTACK,           // param1=unused,               param2=unused
    GUI_UPDATE_SEHCHAIN,            // param1=unused,               param2=unused
    GUI_SYMBOL_REFRESH_CURRENT,     // param1=unused,               param2=unused
    GUI_UPDATE_MEMORY_VIEW,         // param1=unused,               param2=unused
    GUI_REF_INITIALIZE,             // param1=const char* name,     param2=unused
    GUI_LOAD_SOURCE_FILE,           // param1=const char* path,     param2=duint addr
    GUI_MENU_SET_ICON,              // param1=int hMenu,            param2=ICONINFO*
    GUI_MENU_SET_ENTRY_ICON,        // param1=int hEntry,           param2=ICONINFO*
    GUI_SHOW_CPU,                   // param1=unused,               param2=unused
    GUI_ADD_QWIDGET_TAB,            // param1=QWidget*,             param2=unused
    GUI_SHOW_QWIDGET_TAB,           // param1=QWidget*,             param2=unused
    GUI_CLOSE_QWIDGET_TAB,          // param1=QWidget*,             param2=unused
    GUI_EXECUTE_ON_GUI_THREAD,      // param1=GUICALLBACKEX cb,     param2=void* userdata
    GUI_UPDATE_TIME_WASTED_COUNTER, // param1=unused,               param2=unused
    GUI_SET_GLOBAL_NOTES,           // param1=const char* text,     param2=unused
    GUI_GET_GLOBAL_NOTES,           // param1=char** text,          param2=unused
    GUI_SET_DEBUGGEE_NOTES,         // param1=const char* text,     param2=unused
    GUI_GET_DEBUGGEE_NOTES,         // param1=char** text,          param2=unused
    GUI_DUMP_AT_N,                  // param1=int index,            param2=duint va
    GUI_DISPLAY_WARNING,            // param1=const char *text,     param2=unused
    GUI_REGISTER_SCRIPT_LANG,       // param1=SCRIPTTYPEINFO* info, param2=unused
    GUI_UNREGISTER_SCRIPT_LANG,     // param1=int id,               param2=unused
    GUI_UPDATE_ARGUMENT_VIEW,       // param1=unused,               param2=unused
    GUI_FOCUS_VIEW,                 // param1=int hWindow,          param2=unused
    GUI_UPDATE_WATCH_VIEW,          // param1=unused,               param2=unused
    GUI_LOAD_GRAPH,                 // param1=BridgeCFGraphList*    param2=unused
    GUI_GRAPH_AT,                   // param1=duint addr            param2=unused
    GUI_UPDATE_GRAPH_VIEW,          // param1=unused,               param2=unused
    GUI_SET_LOG_ENABLED,            // param1=bool isEnabled        param2=unused
    GUI_ADD_FAVOURITE_TOOL,         // param1=const char* name      param2=const char* description
    GUI_ADD_FAVOURITE_COMMAND,      // param1=const char* command   param2=const char* shortcut
    GUI_SET_FAVOURITE_TOOL_SHORTCUT,// param1=const char* name      param2=const char* shortcut
    GUI_FOLD_DISASSEMBLY,           // param1=duint startAddress    param2=duint length
    GUI_SELECT_IN_MEMORY_MAP,       // param1=duint addr,           param2=unused
    GUI_GET_ACTIVE_VIEW,            // param1=ACTIVEVIEW*,          param2=unused
    GUI_MENU_SET_ENTRY_CHECKED,     // param1=int hEntry,           param2=bool checked
    GUI_ADD_INFO_LINE,              // param1=const char* infoline, param2=unused
    GUI_PROCESS_EVENTS,             // param1=unused,               param2=unused
    GUI_TYPE_ADDNODE,               // param1=void* parent,         param2=TYPEDESCRIPTOR* type
    GUI_TYPE_CLEAR,                 // param1=unused,               param2=unused
    GUI_UPDATE_TYPE_WIDGET,         // param1=unused,               param2=unused
    GUI_CLOSE_APPLICATION,          // param1=unused,               param2=unused
    GUI_MENU_SET_VISIBLE,           // param1=int hMenu,            param2=bool visible
    GUI_MENU_SET_ENTRY_VISIBLE,     // param1=int hEntry,           param2=bool visible
    GUI_MENU_SET_NAME,              // param1=int hMenu,            param2=const char* name
    GUI_MENU_SET_ENTRY_NAME,        // param1=int hEntry,           param2=const char* name
    GUI_FLUSH_LOG,                  // param1=unused,               param2=unused
    GUI_MENU_SET_ENTRY_HOTKEY,      // param1=int hEntry,           param2=const char* hack
    GUI_REF_SEARCH_GETROWCOUNT,     // param1=unused,               param2=unused
    GUI_REF_SEARCH_GETCELLCONTENT,  // param1=int row,              param2=int col
    GUI_MENU_REMOVE,                // param1=int hEntryMenu,       param2=unused
    GUI_REF_ADDCOMMAND,             // param1=const char* title,    param2=const char* command
    GUI_OPEN_TRACE_FILE,            // param1=const char* file name,param2=unused
    GUI_UPDATE_TRACE_BROWSER,       // param1=unused,               param2=unused
    GUI_INVALIDATE_SYMBOL_SOURCE,   // param1=duint base,           param2=unused
    GUI_GET_CURRENT_GRAPH,          // param1=BridgeCFGraphList*,   param2=unused
    GUI_SHOW_REF,                   // param1=unused,               param2=unused
    GUI_SELECT_IN_SYMBOLS_TAB,      // param1=duint addr,           param2=unused
} GUIMSG;

//GUI Typedefs
struct _TYPEDESCRIPTOR;

typedef void (*GUICALLBACK)();
typedef void (*GUICALLBACKEX)(void*);
typedef bool (*GUISCRIPTEXECUTE)(const char* text);
typedef void (*GUISCRIPTCOMPLETER)(const char* text, char** entries, int* entryCount);
typedef bool (*TYPETOSTRING)(const struct _TYPEDESCRIPTOR* type, char* dest, size_t* destCount); //don't change destCount for final failure

//GUI structures
typedef struct
{
    int row;
    int col;
    const char* str;
} CELLINFO;

typedef struct
{
    duint start;
    duint end;
} SELECTIONDATA;

typedef struct
{
    const void* data;
    duint size;
} ICONDATA;

typedef struct
{
    char name[64];
    int id;
    GUISCRIPTEXECUTE execute;
    GUISCRIPTCOMPLETER completeCommand;
} SCRIPTTYPEINFO;

typedef struct
{
    void* titleHwnd;
    void* classHwnd;
    char title[MAX_STRING_SIZE];
    char className[MAX_STRING_SIZE];
} ACTIVEVIEW;

typedef struct _TYPEDESCRIPTOR
{
    bool expanded; //is the type node expanded?
    bool reverse; //big endian?
    const char* name; //type name (int b)
    duint addr; //virtual address
    duint offset; //offset to addr for the actual location
    int id; //type id
    int size; //sizeof(type)
    TYPETOSTRING callback; //convert to string
    void* userdata; //user data
} TYPEDESCRIPTOR;

//GUI functions
//code page is utf8
BRIDGE_IMPEXP const char* GuiTranslateText(const char* Source);
BRIDGE_IMPEXP void GuiDisasmAt(duint addr, duint cip);
BRIDGE_IMPEXP void GuiSetDebugState(DBGSTATE state);
BRIDGE_IMPEXP void GuiSetDebugStateFast(DBGSTATE state);
BRIDGE_IMPEXP void GuiAddLogMessage(const char* msg);
BRIDGE_IMPEXP void GuiLogClear();
BRIDGE_IMPEXP void GuiUpdateAllViews();
BRIDGE_IMPEXP void GuiUpdateRegisterView();
BRIDGE_IMPEXP void GuiUpdateDisassemblyView();
BRIDGE_IMPEXP void GuiUpdateBreakpointsView();
BRIDGE_IMPEXP void GuiUpdateWindowTitle(const char* filename);
BRIDGE_IMPEXP HWND GuiGetWindowHandle();
BRIDGE_IMPEXP void GuiDumpAt(duint va);
BRIDGE_IMPEXP void GuiScriptAdd(int count, const char** lines);
BRIDGE_IMPEXP void GuiScriptClear();
BRIDGE_IMPEXP void GuiScriptSetIp(int line);
BRIDGE_IMPEXP void GuiScriptError(int line, const char* message);
BRIDGE_IMPEXP void GuiScriptSetTitle(const char* title);
BRIDGE_IMPEXP void GuiScriptSetInfoLine(int line, const char* info);
BRIDGE_IMPEXP void GuiScriptMessage(const char* message);
BRIDGE_IMPEXP int GuiScriptMsgyn(const char* message);
BRIDGE_IMPEXP void GuiScriptEnableHighlighting(bool enable);
BRIDGE_IMPEXP void GuiSymbolLogAdd(const char* message);
BRIDGE_IMPEXP void GuiSymbolLogClear();
BRIDGE_IMPEXP void GuiSymbolSetProgress(int percent);
BRIDGE_IMPEXP void GuiSymbolUpdateModuleList(int count, SYMBOLMODULEINFO* modules);
BRIDGE_IMPEXP void GuiSymbolRefreshCurrent();
BRIDGE_IMPEXP void GuiReferenceAddColumn(int width, const char* title);
BRIDGE_IMPEXP void GuiReferenceSetRowCount(int count);
BRIDGE_IMPEXP int GuiReferenceGetRowCount();
BRIDGE_IMPEXP int GuiReferenceSearchGetRowCount();
BRIDGE_IMPEXP void GuiReferenceDeleteAllColumns();
BRIDGE_IMPEXP void GuiReferenceInitialize(const char* name);
BRIDGE_IMPEXP void GuiReferenceSetCellContent(int row, int col, const char* str);
BRIDGE_IMPEXP char* GuiReferenceGetCellContent(int row, int col);
BRIDGE_IMPEXP char* GuiReferenceSearchGetCellContent(int row, int col);
BRIDGE_IMPEXP void GuiReferenceReloadData();
BRIDGE_IMPEXP void GuiReferenceSetSingleSelection(int index, bool scroll);
BRIDGE_IMPEXP void GuiReferenceSetProgress(int progress);
BRIDGE_IMPEXP void GuiReferenceSetCurrentTaskProgress(int progress, const char* taskTitle);
BRIDGE_IMPEXP void GuiReferenceSetSearchStartCol(int col);
BRIDGE_IMPEXP void GuiStackDumpAt(duint addr, duint csp);
BRIDGE_IMPEXP void GuiUpdateDumpView();
BRIDGE_IMPEXP void GuiUpdateWatchView();
BRIDGE_IMPEXP void GuiUpdateThreadView();
BRIDGE_IMPEXP void GuiUpdateMemoryView();
BRIDGE_IMPEXP void GuiAddRecentFile(const char* file);
BRIDGE_IMPEXP void GuiSetLastException(unsigned int exception);
BRIDGE_IMPEXP bool GuiGetDisassembly(duint addr, char* text);
BRIDGE_IMPEXP int GuiMenuAdd(int hMenu, const char* title);
BRIDGE_IMPEXP int GuiMenuAddEntry(int hMenu, const char* title);
BRIDGE_IMPEXP void GuiMenuAddSeparator(int hMenu);
BRIDGE_IMPEXP void GuiMenuClear(int hMenu);
BRIDGE_IMPEXP void GuiMenuRemove(int hEntryMenu);
BRIDGE_IMPEXP bool GuiSelectionGet(GUISELECTIONTYPE hWindow, SELECTIONDATA* selection);
BRIDGE_IMPEXP bool GuiSelectionSet(GUISELECTIONTYPE hWindow, const SELECTIONDATA* selection);
BRIDGE_IMPEXP bool GuiGetLineWindow(const char* title, char* text);
BRIDGE_IMPEXP void GuiAutoCompleteAddCmd(const char* cmd);
BRIDGE_IMPEXP void GuiAutoCompleteDelCmd(const char* cmd);
BRIDGE_IMPEXP void GuiAutoCompleteClearAll();
BRIDGE_IMPEXP void GuiAddStatusBarMessage(const char* msg);
BRIDGE_IMPEXP void GuiUpdateSideBar();
BRIDGE_IMPEXP void GuiRepaintTableView();
BRIDGE_IMPEXP void GuiUpdatePatches();
BRIDGE_IMPEXP void GuiUpdateCallStack();
BRIDGE_IMPEXP void GuiUpdateSEHChain();
BRIDGE_IMPEXP void GuiLoadSourceFileEx(const char* path, duint addr);
BRIDGE_IMPEXP void GuiMenuSetIcon(int hMenu, const ICONDATA* icon);
BRIDGE_IMPEXP void GuiMenuSetEntryIcon(int hEntry, const ICONDATA* icon);
BRIDGE_IMPEXP void GuiMenuSetEntryChecked(int hEntry, bool checked);
BRIDGE_IMPEXP void GuiMenuSetVisible(int hMenu, bool visible);
BRIDGE_IMPEXP void GuiMenuSetEntryVisible(int hEntry, bool visible);
BRIDGE_IMPEXP void GuiMenuSetName(int hMenu, const char* name);
BRIDGE_IMPEXP void GuiMenuSetEntryName(int hEntry, const char* name);
BRIDGE_IMPEXP void GuiMenuSetEntryHotkey(int hEntry, const char* hack);
BRIDGE_IMPEXP void GuiShowCpu();
BRIDGE_IMPEXP void GuiAddQWidgetTab(void* qWidget);
BRIDGE_IMPEXP void GuiShowQWidgetTab(void* qWidget);
BRIDGE_IMPEXP void GuiCloseQWidgetTab(void* qWidget);
BRIDGE_IMPEXP void GuiExecuteOnGuiThread(GUICALLBACK cbGuiThread);
BRIDGE_IMPEXP void GuiUpdateTimeWastedCounter();
BRIDGE_IMPEXP void GuiSetGlobalNotes(const char* text);
BRIDGE_IMPEXP void GuiGetGlobalNotes(char** text);
BRIDGE_IMPEXP void GuiSetDebuggeeNotes(const char* text);
BRIDGE_IMPEXP void GuiGetDebuggeeNotes(char** text);
BRIDGE_IMPEXP void GuiDumpAtN(duint va, int index);
BRIDGE_IMPEXP void GuiDisplayWarning(const char* title, const char* text);
BRIDGE_IMPEXP void GuiRegisterScriptLanguage(SCRIPTTYPEINFO* info);
BRIDGE_IMPEXP void GuiUnregisterScriptLanguage(int id);
BRIDGE_IMPEXP void GuiUpdateArgumentWidget();
BRIDGE_IMPEXP void GuiFocusView(int hWindow);
BRIDGE_IMPEXP bool GuiIsUpdateDisabled();
BRIDGE_IMPEXP void GuiUpdateEnable(bool updateNow);
BRIDGE_IMPEXP void GuiUpdateDisable();
BRIDGE_IMPEXP bool GuiLoadGraph(BridgeCFGraphList* graph, duint addr);
BRIDGE_IMPEXP duint GuiGraphAt(duint addr);
BRIDGE_IMPEXP void GuiUpdateGraphView();
BRIDGE_IMPEXP void GuiDisableLog();
BRIDGE_IMPEXP void GuiEnableLog();
BRIDGE_IMPEXP void GuiAddFavouriteTool(const char* name, const char* description);
BRIDGE_IMPEXP void GuiAddFavouriteCommand(const char* name, const char* shortcut);
BRIDGE_IMPEXP void GuiSetFavouriteToolShortcut(const char* name, const char* shortcut);
BRIDGE_IMPEXP void GuiFoldDisassembly(duint startAddress, duint length);
BRIDGE_IMPEXP void GuiSelectInMemoryMap(duint addr);
BRIDGE_IMPEXP void GuiGetActiveView(ACTIVEVIEW* activeView);
BRIDGE_IMPEXP void GuiAddInfoLine(const char* infoLine);
BRIDGE_IMPEXP void GuiProcessEvents();
BRIDGE_IMPEXP void* GuiTypeAddNode(void* parent, const TYPEDESCRIPTOR* type);
BRIDGE_IMPEXP bool GuiTypeClear();
BRIDGE_IMPEXP void GuiUpdateTypeWidget();
BRIDGE_IMPEXP void GuiCloseApplication();
BRIDGE_IMPEXP void GuiFlushLog();
BRIDGE_IMPEXP void GuiReferenceAddCommand(const char* title, const char* command);
BRIDGE_IMPEXP void GuiUpdateTraceBrowser();
BRIDGE_IMPEXP void GuiOpenTraceFile(const char* fileName);
BRIDGE_IMPEXP void GuiInvalidateSymbolSource(duint base);
BRIDGE_IMPEXP void GuiExecuteOnGuiThreadEx(GUICALLBACKEX cbGuiThread, void* userdata);
BRIDGE_IMPEXP void GuiGetCurrentGraph(BridgeCFGraphList* graphList);
BRIDGE_IMPEXP void GuiShowReferences();
BRIDGE_IMPEXP void GuiSelectInSymbolsTab(duint addr);

#ifdef __cplusplus
}
#endif

#pragma pack(pop)

#endif // _BRIDGEMAIN_H_

```

`CodeCleaner/pluginsdk/dbghelp/dbghelp.h`:

```h
#ifndef _DBGHELP_
#define _DBGHELP_


// As a general principal always call the 64 bit version
// of every API, if a choice exists.  The 64 bit version
// works great on 32 bit platforms, and is forward
// compatible to 64 bit platforms.

#ifdef _WIN64
#ifndef _IMAGEHLP64
#define _IMAGEHLP64
#endif
#endif

#pragma pack(push,8)

// For those without specstrings.h
// Since there are different versions of this header, I need to
// individually test each item and define it if it is not around.

#ifndef __in
#define __in
#endif
#ifndef __out
#define __out
#endif
#ifndef __inout
#define __inout
#endif
#ifndef __in_opt
#define __in_opt
#endif
#ifndef __out_opt
#define __out_opt
#endif
#ifndef __inout_opt
#define __inout_opt
#endif
#ifndef __in_ecount
#define __in_ecount(x)
#endif
#ifndef __out_ecount
#define __out_ecount(x)
#endif
#ifndef __inout_ecount
#define __inout_ecount(x)
#endif
#ifndef __in_bcount
#define __in_bcount(x)
#endif
#ifndef __out_bcount
#define __out_bcount(x)
#endif
#ifndef __inout_bcount
#define __inout_bcount(x)
#endif
#ifndef __out_xcount
#define __out_xcount(x)
#endif
#ifndef __deref_opt_out
#define __deref_opt_out
#endif
#ifndef __deref_out
#define __deref_out
#endif
#ifndef __out_ecount_opt
#define __out_ecount_opt(x)
#endif
#ifndef __in_bcount_opt
#define __in_bcount_opt(x)
#endif
#ifndef __out_bcount_opt
#define __out_bcount_opt(x)
#endif
#ifndef __deref_out_opt
#define __deref_out_opt
#endif


#ifdef __cplusplus
extern "C" {
#endif

#ifdef _IMAGEHLP_SOURCE_
#define IMAGEAPI __stdcall
#define DBHLP_DEPRECIATED
#else
#define IMAGEAPI DECLSPEC_IMPORT __stdcall
#if (_MSC_VER >= 1300) && !defined(MIDL_PASS)
#define DBHLP_DEPRECIATED   __declspec(deprecated)
#else
#define DBHLP_DEPRECIATED
#endif
#endif

#define DBHLPAPI IMAGEAPI

#define IMAGE_SEPARATION (64*1024)

// Observant readers may notice that 2 new fields,
// 'fReadOnly' and 'Version' have been added to
// the LOADED_IMAGE structure after 'fDOSImage'.
// This does not change the size of the structure
// from previous headers.  That is because while
// 'fDOSImage' is a byte, it is padded by the
// compiler to 4 bytes.  So the 2 new fields are
// slipped into the extra space.

typedef struct _LOADED_IMAGE
{
    PSTR                  ModuleName;
    HANDLE                hFile;
    PUCHAR                MappedAddress;
#ifdef _IMAGEHLP64
    PIMAGE_NT_HEADERS64   FileHeader;
#else
    PIMAGE_NT_HEADERS32   FileHeader;
#endif
    PIMAGE_SECTION_HEADER LastRvaSection;
    ULONG                 NumberOfSections;
    PIMAGE_SECTION_HEADER Sections;
    ULONG                 Characteristics;
    BOOLEAN               fSystemImage;
    BOOLEAN               fDOSImage;
    BOOLEAN               fReadOnly;
    UCHAR                 Version;
    LIST_ENTRY            Links;
    ULONG                 SizeOfImage;
} LOADED_IMAGE, *PLOADED_IMAGE;

#define MAX_SYM_NAME            2000


// Error codes set by dbghelp functions.  Call GetLastError
// to see them.
// Dbghelp also sets error codes found in winerror.h

#define ERROR_IMAGE_NOT_STRIPPED    0x8800  // the image is not stripped.  No dbg file available.
#define ERROR_NO_DBG_POINTER        0x8801  // image is stripped but there is no pointer to a dbg file
#define ERROR_NO_PDB_POINTER        0x8802  // image does not point to a pdb file

typedef BOOL
(CALLBACK* PFIND_DEBUG_FILE_CALLBACK)(
    __in HANDLE FileHandle,
    __in PCSTR FileName,
    __in PVOID CallerData
);

HANDLE
IMAGEAPI
SymFindDebugInfoFile(
    __in HANDLE hProcess,
    __in PCSTR FileName,
    __out_ecount(MAX_PATH + 1) PSTR DebugFilePath,
    __in_opt PFIND_DEBUG_FILE_CALLBACK Callback,
    __in_opt PVOID CallerData
);

typedef BOOL
(CALLBACK* PFIND_DEBUG_FILE_CALLBACKW)(
    __in HANDLE FileHandle,
    __in PCWSTR FileName,
    __in PVOID  CallerData
);

HANDLE
IMAGEAPI
SymFindDebugInfoFileW(
    __in HANDLE hProcess,
    __in PCWSTR FileName,
    __out_ecount(MAX_PATH + 1) PWSTR DebugFilePath,
    __in_opt PFIND_DEBUG_FILE_CALLBACKW Callback,
    __in_opt PVOID CallerData
);

HANDLE
IMAGEAPI
FindDebugInfoFile(
    __in PCSTR FileName,
    __in PCSTR SymbolPath,
    __out_ecount(MAX_PATH + 1) PSTR DebugFilePath
);

HANDLE
IMAGEAPI
FindDebugInfoFileEx(
    __in PCSTR FileName,
    __in PCSTR SymbolPath,
    __out_ecount(MAX_PATH + 1) PSTR  DebugFilePath,
    __in_opt PFIND_DEBUG_FILE_CALLBACK Callback,
    __in_opt PVOID CallerData
);

HANDLE
IMAGEAPI
FindDebugInfoFileExW(
    __in PCWSTR FileName,
    __in PCWSTR SymbolPath,
    __out_ecount(MAX_PATH + 1) PWSTR DebugFilePath,
    __in_opt PFIND_DEBUG_FILE_CALLBACKW Callback,
    __in_opt PVOID CallerData
);

typedef BOOL
(CALLBACK* PFINDFILEINPATHCALLBACK)(
    __in PCSTR filename,
    __in PVOID context
);

BOOL
IMAGEAPI
SymFindFileInPath(
    __in HANDLE hprocess,
    __in_opt PCSTR SearchPath,
    __in PCSTR FileName,
    __in_opt PVOID id,
    __in DWORD two,
    __in DWORD three,
    __in DWORD flags,
    __out_ecount(MAX_PATH + 1) PSTR FoundFile,
    __in_opt PFINDFILEINPATHCALLBACK callback,
    __in_opt PVOID context
);

typedef BOOL
(CALLBACK* PFINDFILEINPATHCALLBACKW)(
    __in PCWSTR filename,
    __in PVOID context
);

BOOL
IMAGEAPI
SymFindFileInPathW(
    __in HANDLE hprocess,
    __in_opt PCWSTR SearchPath,
    __in PCWSTR FileName,
    __in_opt PVOID id,
    __in DWORD two,
    __in DWORD three,
    __in DWORD flags,
    __out_ecount(MAX_PATH + 1) PWSTR FoundFile,
    __in_opt PFINDFILEINPATHCALLBACKW callback,
    __in_opt PVOID context
);

typedef BOOL
(CALLBACK* PFIND_EXE_FILE_CALLBACK)(
    __in HANDLE FileHandle,
    __in PCSTR FileName,
    __in_opt PVOID CallerData
);

HANDLE
IMAGEAPI
SymFindExecutableImage(
    __in HANDLE hProcess,
    __in PCSTR FileName,
    __out_ecount(MAX_PATH + 1) PSTR ImageFilePath,
    __in PFIND_EXE_FILE_CALLBACK Callback,
    __in PVOID CallerData
);

typedef BOOL
(CALLBACK* PFIND_EXE_FILE_CALLBACKW)(
    __in HANDLE FileHandle,
    __in PCWSTR FileName,
    __in_opt PVOID CallerData
);

HANDLE
IMAGEAPI
SymFindExecutableImageW(
    __in HANDLE hProcess,
    __in PCWSTR FileName,
    __out_ecount(MAX_PATH + 1) PWSTR ImageFilePath,
    __in PFIND_EXE_FILE_CALLBACKW Callback,
    __in PVOID CallerData
);

HANDLE
IMAGEAPI
FindExecutableImage(
    __in PCSTR FileName,
    __in PCSTR SymbolPath,
    __out_ecount(MAX_PATH + 1) PSTR ImageFilePath
);

HANDLE
IMAGEAPI
FindExecutableImageEx(
    __in PCSTR FileName,
    __in PCSTR SymbolPath,
    __out_ecount(MAX_PATH + 1) PSTR ImageFilePath,
    __in_opt PFIND_EXE_FILE_CALLBACK Callback,
    __in_opt PVOID CallerData
);

HANDLE
IMAGEAPI
FindExecutableImageExW(
    __in PCWSTR FileName,
    __in PCWSTR SymbolPath,
    __out_ecount(MAX_PATH + 1) PWSTR ImageFilePath,
    __in_opt PFIND_EXE_FILE_CALLBACKW Callback,
    __in PVOID CallerData
);

PIMAGE_NT_HEADERS
IMAGEAPI
ImageNtHeader(
    __in PVOID Base
);

PVOID
IMAGEAPI
ImageDirectoryEntryToDataEx(
    __in PVOID Base,
    __in BOOLEAN MappedAsImage,
    __in USHORT DirectoryEntry,
    __out PULONG Size,
    __out_opt PIMAGE_SECTION_HEADER* FoundHeader
);

PVOID
IMAGEAPI
ImageDirectoryEntryToData(
    __in PVOID Base,
    __in BOOLEAN MappedAsImage,
    __in USHORT DirectoryEntry,
    __out PULONG Size
);

PIMAGE_SECTION_HEADER
IMAGEAPI
ImageRvaToSection(
    __in PIMAGE_NT_HEADERS NtHeaders,
    __in PVOID Base,
    __in ULONG Rva
);

PVOID
IMAGEAPI
ImageRvaToVa(
    __in PIMAGE_NT_HEADERS NtHeaders,
    __in PVOID Base,
    __in ULONG Rva,
    __in_opt OUT PIMAGE_SECTION_HEADER* LastRvaSection
);

#ifndef _WIN64
// This api won't be ported to Win64 - Fix your code.

typedef struct _IMAGE_DEBUG_INFORMATION
{
    LIST_ENTRY List;
    DWORD ReservedSize;
    PVOID ReservedMappedBase;
    USHORT ReservedMachine;
    USHORT ReservedCharacteristics;
    DWORD ReservedCheckSum;
    DWORD ImageBase;
    DWORD SizeOfImage;

    DWORD ReservedNumberOfSections;
    PIMAGE_SECTION_HEADER ReservedSections;

    DWORD ReservedExportedNamesSize;
    PSTR ReservedExportedNames;

    DWORD ReservedNumberOfFunctionTableEntries;
    PIMAGE_FUNCTION_ENTRY ReservedFunctionTableEntries;
    DWORD ReservedLowestFunctionStartingAddress;
    DWORD ReservedHighestFunctionEndingAddress;

    DWORD ReservedNumberOfFpoTableEntries;
    PFPO_DATA ReservedFpoTableEntries;

    DWORD SizeOfCoffSymbols;
    PIMAGE_COFF_SYMBOLS_HEADER CoffSymbols;

    DWORD ReservedSizeOfCodeViewSymbols;
    PVOID ReservedCodeViewSymbols;

    PSTR ImageFilePath;
    PSTR ImageFileName;
    PSTR ReservedDebugFilePath;

    DWORD ReservedTimeDateStamp;

    BOOL  ReservedRomImage;
    PIMAGE_DEBUG_DIRECTORY ReservedDebugDirectory;
    DWORD ReservedNumberOfDebugDirectories;

    DWORD ReservedOriginalFunctionTableBaseAddress;

    DWORD Reserved[ 2 ];

} IMAGE_DEBUG_INFORMATION, *PIMAGE_DEBUG_INFORMATION;


PIMAGE_DEBUG_INFORMATION
IMAGEAPI
MapDebugInformation(
    __in_opt HANDLE FileHandle,
    __in PCSTR FileName,
    __in_opt PCSTR SymbolPath,
    __in ULONG ImageBase
);

BOOL
IMAGEAPI
UnmapDebugInformation(
    __out_xcount(unknown) PIMAGE_DEBUG_INFORMATION DebugInfo
);

#endif

BOOL
IMAGEAPI
SearchTreeForFile(
    __in PCSTR RootPath,
    __in PCSTR InputPathName,
    __out_ecount(MAX_PATH + 1) PSTR OutputPathBuffer
);

BOOL
IMAGEAPI
SearchTreeForFileW(
    __in PCWSTR RootPath,
    __in PCWSTR InputPathName,
    __out_ecount(MAX_PATH + 1) PWSTR OutputPathBuffer
);

typedef BOOL
(CALLBACK* PENUMDIRTREE_CALLBACK)(
    __in PCSTR FilePath,
    __in_opt PVOID CallerData
);

BOOL
IMAGEAPI
EnumDirTree(
    __in_opt HANDLE hProcess,
    __in PCSTR RootPath,
    __in PCSTR InputPathName,
    __out_ecount_opt(MAX_PATH + 1) PSTR OutputPathBuffer,
    __in_opt PENUMDIRTREE_CALLBACK cb,
    __in_opt PVOID data
);

typedef BOOL
(CALLBACK* PENUMDIRTREE_CALLBACKW)(
    __in PCWSTR FilePath,
    __in_opt PVOID CallerData
);

BOOL
IMAGEAPI
EnumDirTreeW(
    __in_opt HANDLE hProcess,
    __in PCWSTR RootPath,
    __in PCWSTR InputPathName,
    __out_ecount_opt(MAX_PATH + 1) PWSTR OutputPathBuffer,
    __in_opt PENUMDIRTREE_CALLBACKW cb,
    __in_opt PVOID data
);

BOOL
IMAGEAPI
MakeSureDirectoryPathExists(
    __in PCSTR DirPath
);

//
// UnDecorateSymbolName Flags
//

#define UNDNAME_COMPLETE                 (0x0000)  // Enable full undecoration
#define UNDNAME_NO_LEADING_UNDERSCORES   (0x0001)  // Remove leading underscores from MS extended keywords
#define UNDNAME_NO_MS_KEYWORDS           (0x0002)  // Disable expansion of MS extended keywords
#define UNDNAME_NO_FUNCTION_RETURNS      (0x0004)  // Disable expansion of return type for primary declaration
#define UNDNAME_NO_ALLOCATION_MODEL      (0x0008)  // Disable expansion of the declaration model
#define UNDNAME_NO_ALLOCATION_LANGUAGE   (0x0010)  // Disable expansion of the declaration language specifier
#define UNDNAME_NO_MS_THISTYPE           (0x0020)  // NYI Disable expansion of MS keywords on the 'this' type for primary declaration
#define UNDNAME_NO_CV_THISTYPE           (0x0040)  // NYI Disable expansion of CV modifiers on the 'this' type for primary declaration
#define UNDNAME_NO_THISTYPE              (0x0060)  // Disable all modifiers on the 'this' type
#define UNDNAME_NO_ACCESS_SPECIFIERS     (0x0080)  // Disable expansion of access specifiers for members
#define UNDNAME_NO_THROW_SIGNATURES      (0x0100)  // Disable expansion of 'throw-signatures' for functions and pointers to functions
#define UNDNAME_NO_MEMBER_TYPE           (0x0200)  // Disable expansion of 'static' or 'virtual'ness of members
#define UNDNAME_NO_RETURN_UDT_MODEL      (0x0400)  // Disable expansion of MS model for UDT returns
#define UNDNAME_32_BIT_DECODE            (0x0800)  // Undecorate 32-bit decorated names
#define UNDNAME_NAME_ONLY                (0x1000)  // Crack only the name for primary declaration;
//  return just [scope::]name.  Does expand template params
#define UNDNAME_NO_ARGUMENTS             (0x2000)  // Don't undecorate arguments to function
#define UNDNAME_NO_SPECIAL_SYMS          (0x4000)  // Don't undecorate special names (v-table, vcall, vector xxx, metatype, etc)

DWORD
IMAGEAPI
WINAPI
UnDecorateSymbolName(
    __in PCSTR name,
    __out_ecount(maxStringLength) PSTR outputString,
    __in DWORD maxStringLength,
    __in DWORD flags
);

DWORD
IMAGEAPI
WINAPI
UnDecorateSymbolNameW(
    __in PCWSTR name,
    __out_ecount(maxStringLength) PWSTR outputString,
    __in DWORD maxStringLength,
    __in DWORD flags
);

//
// these values are used for synthesized file types
// that can be passed in as image headers instead of
// the standard ones from ntimage.h
//

#define DBHHEADER_DEBUGDIRS     0x1
#define DBHHEADER_CVMISC        0x2
#define DBHHEADER_PDBGUID       0x3
typedef struct _MODLOAD_DATA
{
    DWORD   ssize;                  // size of this struct
    DWORD   ssig;                   // signature identifying the passed data
    PVOID   data;                   // pointer to passed data
    DWORD   size;                   // size of passed data
    DWORD   flags;                  // options
} MODLOAD_DATA, *PMODLOAD_DATA;

typedef struct _MODLOAD_CVMISC
{
    DWORD   oCV;                    // ofset to the codeview record
    size_t  cCV;                    // size of the codeview record
    DWORD   oMisc;                  // offset to the misc record
    size_t  cMisc;                  // size of the misc record
    DWORD   dtImage;                // datetime stamp of the image
    DWORD   cImage;                 // size of the image
} MODLOAD_CVMISC, *PMODLOAD_CVMISC;

typedef struct _MODLOAD_PDBGUID_PDBAGE
{
    GUID    PdbGuid;                // Pdb Guid
    DWORD   PdbAge;                 // Pdb Age
} MODLOAD_PDBGUID_PDBAGE, *PMODLOAD_PDBGUID_PDBAGE;

//
// StackWalking API
//

typedef enum
{
    AddrMode1616,
    AddrMode1632,
    AddrModeReal,
    AddrModeFlat
} ADDRESS_MODE;

typedef struct _tagADDRESS64
{
    DWORD64       Offset;
    WORD          Segment;
    ADDRESS_MODE  Mode;
} ADDRESS64, *LPADDRESS64;

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define ADDRESS ADDRESS64
#define LPADDRESS LPADDRESS64
#else
typedef struct _tagADDRESS
{
    DWORD         Offset;
    WORD          Segment;
    ADDRESS_MODE  Mode;
} ADDRESS, *LPADDRESS;

__inline
void
Address32To64(
    __in LPADDRESS a32,
    __out LPADDRESS64 a64
)
{
    a64->Offset = (ULONG64)(LONG64)(LONG)a32->Offset;
    a64->Segment = a32->Segment;
    a64->Mode = a32->Mode;
}

__inline
void
Address64To32(
    __in LPADDRESS64 a64,
    __out LPADDRESS a32
)
{
    a32->Offset = (ULONG)a64->Offset;
    a32->Segment = a64->Segment;
    a32->Mode = a64->Mode;
}
#endif

//
// This structure is included in the STACKFRAME structure,
// and is used to trace through usermode callbacks in a thread's
// kernel stack.  The values must be copied by the kernel debugger
// from the DBGKD_GET_VERSION and WAIT_STATE_CHANGE packets.
//

//
// New KDHELP structure for 64 bit system support.
// This structure is preferred in new code.
//
typedef struct _KDHELP64
{

    //
    // address of kernel thread object, as provided in the
    // WAIT_STATE_CHANGE packet.
    //
    DWORD64   Thread;

    //
    // offset in thread object to pointer to the current callback frame
    // in kernel stack.
    //
    DWORD   ThCallbackStack;

    //
    // offset in thread object to pointer to the current callback backing
    // store frame in kernel stack.
    //
    DWORD   ThCallbackBStore;

    //
    // offsets to values in frame:
    //
    // address of next callback frame
    DWORD   NextCallback;

    // address of saved frame pointer (if applicable)
    DWORD   FramePointer;


    //
    // Address of the kernel function that calls out to user mode
    //
    DWORD64   KiCallUserMode;

    //
    // Address of the user mode dispatcher function
    //
    DWORD64   KeUserCallbackDispatcher;

    //
    // Lowest kernel mode address
    //
    DWORD64   SystemRangeStart;

    //
    // Address of the user mode exception dispatcher function.
    // Added in API version 10.
    //
    DWORD64   KiUserExceptionDispatcher;

    //
    // Stack bounds, added in API version 11.
    //
    DWORD64   StackBase;
    DWORD64   StackLimit;

    DWORD64   Reserved[5];

} KDHELP64, *PKDHELP64;

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define KDHELP KDHELP64
#define PKDHELP PKDHELP64
#else
typedef struct _KDHELP
{

    //
    // address of kernel thread object, as provided in the
    // WAIT_STATE_CHANGE packet.
    //
    DWORD   Thread;

    //
    // offset in thread object to pointer to the current callback frame
    // in kernel stack.
    //
    DWORD   ThCallbackStack;

    //
    // offsets to values in frame:
    //
    // address of next callback frame
    DWORD   NextCallback;

    // address of saved frame pointer (if applicable)
    DWORD   FramePointer;

    //
    // Address of the kernel function that calls out to user mode
    //
    DWORD   KiCallUserMode;

    //
    // Address of the user mode dispatcher function
    //
    DWORD   KeUserCallbackDispatcher;

    //
    // Lowest kernel mode address
    //
    DWORD   SystemRangeStart;

    //
    // offset in thread object to pointer to the current callback backing
    // store frame in kernel stack.
    //
    DWORD   ThCallbackBStore;

    //
    // Address of the user mode exception dispatcher function.
    // Added in API version 10.
    //
    DWORD   KiUserExceptionDispatcher;

    //
    // Stack bounds, added in API version 11.
    //
    DWORD   StackBase;
    DWORD   StackLimit;

    DWORD   Reserved[5];

} KDHELP, *PKDHELP;

__inline
void
KdHelp32To64(
    __in PKDHELP p32,
    __out PKDHELP64 p64
)
{
    p64->Thread = p32->Thread;
    p64->ThCallbackStack = p32->ThCallbackStack;
    p64->NextCallback = p32->NextCallback;
    p64->FramePointer = p32->FramePointer;
    p64->KiCallUserMode = p32->KiCallUserMode;
    p64->KeUserCallbackDispatcher = p32->KeUserCallbackDispatcher;
    p64->SystemRangeStart = p32->SystemRangeStart;
    p64->KiUserExceptionDispatcher = p32->KiUserExceptionDispatcher;
    p64->StackBase = p32->StackBase;
    p64->StackLimit = p32->StackLimit;
}
#endif

typedef struct _tagSTACKFRAME64
{
    ADDRESS64   AddrPC;               // program counter
    ADDRESS64   AddrReturn;           // return address
    ADDRESS64   AddrFrame;            // frame pointer
    ADDRESS64   AddrStack;            // stack pointer
    ADDRESS64   AddrBStore;           // backing store pointer
    PVOID       FuncTableEntry;       // pointer to pdata/fpo or NULL
    DWORD64     Params[4];            // possible arguments to the function
    BOOL        Far;                  // WOW far call
    BOOL        Virtual;              // is this a virtual frame?
    DWORD64     Reserved[3];
    KDHELP64    KdHelp;
} STACKFRAME64, *LPSTACKFRAME64;

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define STACKFRAME STACKFRAME64
#define LPSTACKFRAME LPSTACKFRAME64
#else
typedef struct _tagSTACKFRAME
{
    ADDRESS     AddrPC;               // program counter
    ADDRESS     AddrReturn;           // return address
    ADDRESS     AddrFrame;            // frame pointer
    ADDRESS     AddrStack;            // stack pointer
    PVOID       FuncTableEntry;       // pointer to pdata/fpo or NULL
    DWORD       Params[4];            // possible arguments to the function
    BOOL        Far;                  // WOW far call
    BOOL        Virtual;              // is this a virtual frame?
    DWORD       Reserved[3];
    KDHELP      KdHelp;
    ADDRESS     AddrBStore;           // backing store pointer
} STACKFRAME, *LPSTACKFRAME;
#endif


typedef
BOOL
(__stdcall* PREAD_PROCESS_MEMORY_ROUTINE64)(
    __in HANDLE hProcess,
    __in DWORD64 qwBaseAddress,
    __out_bcount(nSize) PVOID lpBuffer,
    __in DWORD nSize,
    __out LPDWORD lpNumberOfBytesRead
);

typedef
PVOID
(__stdcall* PFUNCTION_TABLE_ACCESS_ROUTINE64)(
    __in HANDLE ahProcess,
    __in DWORD64 AddrBase
);

typedef
DWORD64
(__stdcall* PGET_MODULE_BASE_ROUTINE64)(
    __in HANDLE hProcess,
    __in DWORD64 Address
);

typedef
DWORD64
(__stdcall* PTRANSLATE_ADDRESS_ROUTINE64)(
    __in HANDLE hProcess,
    __in HANDLE hThread,
    __in LPADDRESS64 lpaddr
);

BOOL
IMAGEAPI
StackWalk64(
    __in DWORD MachineType,
    __in HANDLE hProcess,
    __in HANDLE hThread,
    __inout LPSTACKFRAME64 StackFrame,
    __inout PVOID ContextRecord,
    __in_opt PREAD_PROCESS_MEMORY_ROUTINE64 ReadMemoryRoutine,
    __in_opt PFUNCTION_TABLE_ACCESS_ROUTINE64 FunctionTableAccessRoutine,
    __in_opt PGET_MODULE_BASE_ROUTINE64 GetModuleBaseRoutine,
    __in_opt PTRANSLATE_ADDRESS_ROUTINE64 TranslateAddress
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)

#define PREAD_PROCESS_MEMORY_ROUTINE PREAD_PROCESS_MEMORY_ROUTINE64
#define PFUNCTION_TABLE_ACCESS_ROUTINE PFUNCTION_TABLE_ACCESS_ROUTINE64
#define PGET_MODULE_BASE_ROUTINE PGET_MODULE_BASE_ROUTINE64
#define PTRANSLATE_ADDRESS_ROUTINE PTRANSLATE_ADDRESS_ROUTINE64

#define StackWalk StackWalk64

#else

typedef
BOOL
(__stdcall* PREAD_PROCESS_MEMORY_ROUTINE)(
    __in HANDLE hProcess,
    __in DWORD lpBaseAddress,
    __out_bcount(nSize) PVOID lpBuffer,
    __in DWORD nSize,
    __out PDWORD lpNumberOfBytesRead
);

typedef
PVOID
(__stdcall* PFUNCTION_TABLE_ACCESS_ROUTINE)(
    __in HANDLE hProcess,
    __in DWORD AddrBase
);

typedef
DWORD
(__stdcall* PGET_MODULE_BASE_ROUTINE)(
    __in HANDLE hProcess,
    __in DWORD Address
);

typedef
DWORD
(__stdcall* PTRANSLATE_ADDRESS_ROUTINE)(
    __in HANDLE hProcess,
    __in HANDLE hThread,
    __out LPADDRESS lpaddr
);

BOOL
IMAGEAPI
StackWalk(
    DWORD MachineType,
    __in HANDLE hProcess,
    __in HANDLE hThread,
    __inout LPSTACKFRAME StackFrame,
    __inout PVOID ContextRecord,
    __in_opt PREAD_PROCESS_MEMORY_ROUTINE ReadMemoryRoutine,
    __in_opt PFUNCTION_TABLE_ACCESS_ROUTINE FunctionTableAccessRoutine,
    __in_opt PGET_MODULE_BASE_ROUTINE GetModuleBaseRoutine,
    __in_opt PTRANSLATE_ADDRESS_ROUTINE TranslateAddress
);

#endif


#define API_VERSION_NUMBER 11

typedef struct API_VERSION
{
    USHORT  MajorVersion;
    USHORT  MinorVersion;
    USHORT  Revision;
    USHORT  Reserved;
} API_VERSION, *LPAPI_VERSION;

LPAPI_VERSION
IMAGEAPI
ImagehlpApiVersion(
    VOID
);

LPAPI_VERSION
IMAGEAPI
ImagehlpApiVersionEx(
    __in LPAPI_VERSION AppVersion
);

DWORD
IMAGEAPI
GetTimestampForLoadedLibrary(
    __in HMODULE Module
);

//
// typedefs for function pointers
//
typedef BOOL
(CALLBACK* PSYM_ENUMMODULES_CALLBACK64)(
    __in PCSTR ModuleName,
    __in DWORD64 BaseOfDll,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMMODULES_CALLBACKW64)(
    __in PCWSTR ModuleName,
    __in DWORD64 BaseOfDll,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PENUMLOADED_MODULES_CALLBACK64)(
    __in PCSTR ModuleName,
    __in DWORD64 ModuleBase,
    __in ULONG ModuleSize,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PENUMLOADED_MODULES_CALLBACKW64)(
    __in PCWSTR ModuleName,
    __in DWORD64 ModuleBase,
    __in ULONG ModuleSize,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMSYMBOLS_CALLBACK64)(
    __in PCSTR SymbolName,
    __in DWORD64 SymbolAddress,
    __in ULONG SymbolSize,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMSYMBOLS_CALLBACK64W)(
    __in PCWSTR SymbolName,
    __in DWORD64 SymbolAddress,
    __in ULONG SymbolSize,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYMBOL_REGISTERED_CALLBACK64)(
    __in HANDLE hProcess,
    __in ULONG ActionCode,
    __in_opt ULONG64 CallbackData,
    __in_opt ULONG64 UserContext
);

typedef
PVOID
(CALLBACK* PSYMBOL_FUNCENTRY_CALLBACK)(
    __in HANDLE hProcess,
    __in DWORD AddrBase,
    __in_opt PVOID UserContext
);

typedef
PVOID
(CALLBACK* PSYMBOL_FUNCENTRY_CALLBACK64)(
    __in HANDLE hProcess,
    __in ULONG64 AddrBase,
    __in ULONG64 UserContext
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)

#define PSYM_ENUMMODULES_CALLBACK PSYM_ENUMMODULES_CALLBACK64
#define PSYM_ENUMSYMBOLS_CALLBACK PSYM_ENUMSYMBOLS_CALLBACK64
#define PSYM_ENUMSYMBOLS_CALLBACKW PSYM_ENUMSYMBOLS_CALLBACK64W
#define PENUMLOADED_MODULES_CALLBACK PENUMLOADED_MODULES_CALLBACK64
#define PSYMBOL_REGISTERED_CALLBACK PSYMBOL_REGISTERED_CALLBACK64
#define PSYMBOL_FUNCENTRY_CALLBACK PSYMBOL_FUNCENTRY_CALLBACK64

#else

typedef BOOL
(CALLBACK* PSYM_ENUMMODULES_CALLBACK)(
    __in PCSTR ModuleName,
    __in ULONG BaseOfDll,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMSYMBOLS_CALLBACK)(
    __in PCSTR SymbolName,
    __in ULONG SymbolAddress,
    __in ULONG SymbolSize,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMSYMBOLS_CALLBACKW)(
    __in PCWSTR SymbolName,
    __in ULONG SymbolAddress,
    __in ULONG SymbolSize,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PENUMLOADED_MODULES_CALLBACK)(
    __in PCSTR ModuleName,
    __in ULONG ModuleBase,
    __in ULONG ModuleSize,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYMBOL_REGISTERED_CALLBACK)(
    __in HANDLE hProcess,
    __in ULONG ActionCode,
    __in_opt PVOID CallbackData,
    __in_opt PVOID UserContext
);

#endif


// values found in SYMBOL_INFO.Tag
//
// This was taken from cvconst.h and should
// not override any values found there.
//
// #define _NO_CVCONST_H_ if you don't
// have access to that file...

#ifdef _NO_CVCONST_H

// DIA enums

enum SymTagEnum
{
    SymTagNull,
    SymTagExe,
    SymTagCompiland,
    SymTagCompilandDetails,
    SymTagCompilandEnv,
    SymTagFunction,
    SymTagBlock,
    SymTagData,
    SymTagAnnotation,
    SymTagLabel,
    SymTagPublicSymbol,
    SymTagUDT,
    SymTagEnum,
    SymTagFunctionType,
    SymTagPointerType,
    SymTagArrayType,
    SymTagBaseType,
    SymTagTypedef,
    SymTagBaseClass,
    SymTagFriend,
    SymTagFunctionArgType,
    SymTagFuncDebugStart,
    SymTagFuncDebugEnd,
    SymTagUsingNamespace,
    SymTagVTableShape,
    SymTagVTable,
    SymTagCustom,
    SymTagThunk,
    SymTagCustomType,
    SymTagManagedType,
    SymTagDimension,
    SymTagMax
};

#endif

//
// flags found in SYMBOL_INFO.Flags
//

#define SYMFLAG_VALUEPRESENT     0x00000001
#define SYMFLAG_REGISTER         0x00000008
#define SYMFLAG_REGREL           0x00000010
#define SYMFLAG_FRAMEREL         0x00000020
#define SYMFLAG_PARAMETER        0x00000040
#define SYMFLAG_LOCAL            0x00000080
#define SYMFLAG_CONSTANT         0x00000100
#define SYMFLAG_EXPORT           0x00000200
#define SYMFLAG_FORWARDER        0x00000400
#define SYMFLAG_FUNCTION         0x00000800
#define SYMFLAG_VIRTUAL          0x00001000
#define SYMFLAG_THUNK            0x00002000
#define SYMFLAG_TLSREL           0x00004000
#define SYMFLAG_SLOT             0x00008000
#define SYMFLAG_ILREL            0x00010000
#define SYMFLAG_METADATA         0x00020000
#define SYMFLAG_CLR_TOKEN        0x00040000

// this resets SymNext/Prev to the beginning
// of the module passed in the address field

#define SYMFLAG_RESET            0x80000000

//
// symbol type enumeration
//
typedef enum
{
    SymNone = 0,
    SymCoff,
    SymCv,
    SymPdb,
    SymExport,
    SymDeferred,
    SymSym,       // .sym file
    SymDia,
    SymVirtual,
    NumSymTypes
} SYM_TYPE;

//
// symbol data structure
//

typedef struct _IMAGEHLP_SYMBOL64
{
    DWORD   SizeOfStruct;           // set to sizeof(IMAGEHLP_SYMBOL64)
    DWORD64 Address;                // virtual address including dll base address
    DWORD   Size;                   // estimated size of symbol, can be zero
    DWORD   Flags;                  // info about the symbols, see the SYMF defines
    DWORD   MaxNameLength;          // maximum size of symbol name in 'Name'
    CHAR    Name[1];                // symbol name (null terminated string)
} IMAGEHLP_SYMBOL64, *PIMAGEHLP_SYMBOL64;

typedef struct _IMAGEHLP_SYMBOL64_PACKAGE
{
    IMAGEHLP_SYMBOL64 sym;
    CHAR              name[MAX_SYM_NAME + 1];
} IMAGEHLP_SYMBOL64_PACKAGE, *PIMAGEHLP_SYMBOL64_PACKAGE;

typedef struct _IMAGEHLP_SYMBOLW64
{
    DWORD   SizeOfStruct;           // set to sizeof(IMAGEHLP_SYMBOLW64)
    DWORD64 Address;                // virtual address including dll base address
    DWORD   Size;                   // estimated size of symbol, can be zero
    DWORD   Flags;                  // info about the symbols, see the SYMF defines
    DWORD   MaxNameLength;          // maximum size of symbol name in 'Name'
    WCHAR   Name[1];                // symbol name (null terminated string)
} IMAGEHLP_SYMBOLW64, *PIMAGEHLP_SYMBOLW64;

typedef struct _IMAGEHLP_SYMBOLW64_PACKAGE
{
    IMAGEHLP_SYMBOLW64 sym;
    WCHAR              name[MAX_SYM_NAME + 1];
} IMAGEHLP_SYMBOLW64_PACKAGE, *PIMAGEHLP_SYMBOLW64_PACKAGE;

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)

#define IMAGEHLP_SYMBOL IMAGEHLP_SYMBOL64
#define PIMAGEHLP_SYMBOL PIMAGEHLP_SYMBOL64
#define IMAGEHLP_SYMBOL_PACKAGE IMAGEHLP_SYMBOL64_PACKAGE
#define PIMAGEHLP_SYMBOL_PACKAGE PIMAGEHLP_SYMBOL64_PACKAGE
#define IMAGEHLP_SYMBOLW IMAGEHLP_SYMBOLW64
#define PIMAGEHLP_SYMBOLW PIMAGEHLP_SYMBOLW64
#define IMAGEHLP_SYMBOLW_PACKAGE IMAGEHLP_SYMBOLW64_PACKAGE
#define PIMAGEHLP_SYMBOLW_PACKAGE PIMAGEHLP_SYMBOLW64_PACKAGE

#else

typedef struct _IMAGEHLP_SYMBOL
{
    DWORD SizeOfStruct;           // set to sizeof(IMAGEHLP_SYMBOL)
    DWORD Address;                // virtual address including dll base address
    DWORD Size;                   // estimated size of symbol, can be zero
    DWORD Flags;                  // info about the symbols, see the SYMF defines
    DWORD                       MaxNameLength;          // maximum size of symbol name in 'Name'
    CHAR                        Name[1];                // symbol name (null terminated string)
} IMAGEHLP_SYMBOL, *PIMAGEHLP_SYMBOL;

typedef struct _IMAGEHLP_SYMBOL_PACKAGE
{
    IMAGEHLP_SYMBOL sym;
    CHAR            name[MAX_SYM_NAME + 1];
} IMAGEHLP_SYMBOL_PACKAGE, *PIMAGEHLP_SYMBOL_PACKAGE;

typedef struct _IMAGEHLP_SYMBOLW
{
    DWORD SizeOfStruct;           // set to sizeof(IMAGEHLP_SYMBOLW)
    DWORD Address;                // virtual address including dll base address
    DWORD Size;                   // estimated size of symbol, can be zero
    DWORD Flags;                  // info about the symbols, see the SYMF defines
    DWORD                       MaxNameLength;          // maximum size of symbol name in 'Name'
    WCHAR                       Name[1];                // symbol name (null terminated string)
} IMAGEHLP_SYMBOLW, *PIMAGEHLP_SYMBOLW;

typedef struct _IMAGEHLP_SYMBOLW_PACKAGE
{
    IMAGEHLP_SYMBOLW sym;
    WCHAR            name[MAX_SYM_NAME + 1];
} IMAGEHLP_SYMBOLW_PACKAGE, *PIMAGEHLP_SYMBOLW_PACKAGE;

#endif

//
// module data structure
//

typedef struct _IMAGEHLP_MODULE64
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_MODULE64)
    DWORD64  BaseOfImage;            // base load address of module
    DWORD    ImageSize;              // virtual size of the loaded module
    DWORD    TimeDateStamp;          // date/time stamp from pe header
    DWORD    CheckSum;               // checksum from the pe header
    DWORD    NumSyms;                // number of symbols in the symbol table
    SYM_TYPE SymType;                // type of symbols loaded
    CHAR     ModuleName[32];         // module name
    CHAR     ImageName[256];         // image name
    CHAR     LoadedImageName[256];   // symbol file name
    // new elements: 07-Jun-2002
    CHAR     LoadedPdbName[256];     // pdb file name
    DWORD    CVSig;                  // Signature of the CV record in the debug directories
    CHAR     CVData[MAX_PATH * 3];   // Contents of the CV record
    DWORD    PdbSig;                 // Signature of PDB
    GUID     PdbSig70;               // Signature of PDB (VC 7 and up)
    DWORD    PdbAge;                 // DBI age of pdb
    BOOL     PdbUnmatched;           // loaded an unmatched pdb
    BOOL     DbgUnmatched;           // loaded an unmatched dbg
    BOOL     LineNumbers;            // we have line number information
    BOOL     GlobalSymbols;          // we have internal symbol information
    BOOL     TypeInfo;               // we have type information
    // new elements: 17-Dec-2003
    BOOL     SourceIndexed;          // pdb supports source server
    BOOL     Publics;                // contains public symbols
} IMAGEHLP_MODULE64, *PIMAGEHLP_MODULE64;

typedef struct _IMAGEHLP_MODULEW64
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_MODULE64)
    DWORD64  BaseOfImage;            // base load address of module
    DWORD    ImageSize;              // virtual size of the loaded module
    DWORD    TimeDateStamp;          // date/time stamp from pe header
    DWORD    CheckSum;               // checksum from the pe header
    DWORD    NumSyms;                // number of symbols in the symbol table
    SYM_TYPE SymType;                // type of symbols loaded
    WCHAR    ModuleName[32];         // module name
    WCHAR    ImageName[256];         // image name
    // new elements: 07-Jun-2002
    WCHAR    LoadedImageName[256];   // symbol file name
    WCHAR    LoadedPdbName[256];     // pdb file name
    DWORD    CVSig;                  // Signature of the CV record in the debug directories
    WCHAR        CVData[MAX_PATH * 3];   // Contents of the CV record
    DWORD    PdbSig;                 // Signature of PDB
    GUID     PdbSig70;               // Signature of PDB (VC 7 and up)
    DWORD    PdbAge;                 // DBI age of pdb
    BOOL     PdbUnmatched;           // loaded an unmatched pdb
    BOOL     DbgUnmatched;           // loaded an unmatched dbg
    BOOL     LineNumbers;            // we have line number information
    BOOL     GlobalSymbols;          // we have internal symbol information
    BOOL     TypeInfo;               // we have type information
    // new elements: 17-Dec-2003
    BOOL     SourceIndexed;          // pdb supports source server
    BOOL     Publics;                // contains public symbols
} IMAGEHLP_MODULEW64, *PIMAGEHLP_MODULEW64;

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define IMAGEHLP_MODULE IMAGEHLP_MODULE64
#define PIMAGEHLP_MODULE PIMAGEHLP_MODULE64
#define IMAGEHLP_MODULEW IMAGEHLP_MODULEW64
#define PIMAGEHLP_MODULEW PIMAGEHLP_MODULEW64
#else
typedef struct _IMAGEHLP_MODULE
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_MODULE)
    DWORD    BaseOfImage;            // base load address of module
    DWORD    ImageSize;              // virtual size of the loaded module
    DWORD    TimeDateStamp;          // date/time stamp from pe header
    DWORD    CheckSum;               // checksum from the pe header
    DWORD    NumSyms;                // number of symbols in the symbol table
    SYM_TYPE SymType;                // type of symbols loaded
    CHAR     ModuleName[32];         // module name
    CHAR     ImageName[256];         // image name
    CHAR     LoadedImageName[256];   // symbol file name
} IMAGEHLP_MODULE, *PIMAGEHLP_MODULE;

typedef struct _IMAGEHLP_MODULEW
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_MODULE)
    DWORD    BaseOfImage;            // base load address of module
    DWORD    ImageSize;              // virtual size of the loaded module
    DWORD    TimeDateStamp;          // date/time stamp from pe header
    DWORD    CheckSum;               // checksum from the pe header
    DWORD    NumSyms;                // number of symbols in the symbol table
    SYM_TYPE SymType;                // type of symbols loaded
    WCHAR    ModuleName[32];         // module name
    WCHAR    ImageName[256];         // image name
    WCHAR    LoadedImageName[256];   // symbol file name
} IMAGEHLP_MODULEW, *PIMAGEHLP_MODULEW;
#endif

//
// source file line data structure
//

typedef struct _IMAGEHLP_LINE64
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_LINE64)
    PVOID    Key;                    // internal
    DWORD    LineNumber;             // line number in file
    PCHAR    FileName;               // full filename
    DWORD64  Address;                // first instruction of line
} IMAGEHLP_LINE64, *PIMAGEHLP_LINE64;

typedef struct _IMAGEHLP_LINEW64
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_LINE64)
    PVOID    Key;                    // internal
    DWORD    LineNumber;             // line number in file
    PWSTR    FileName;               // full filename
    DWORD64  Address;                // first instruction of line
} IMAGEHLP_LINEW64, *PIMAGEHLP_LINEW64;

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define IMAGEHLP_LINE IMAGEHLP_LINE64
#define PIMAGEHLP_LINE PIMAGEHLP_LINE64
#else
typedef struct _IMAGEHLP_LINE
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_LINE)
    PVOID    Key;                    // internal
    DWORD    LineNumber;             // line number in file
    PCHAR    FileName;               // full filename
    DWORD    Address;                // first instruction of line
} IMAGEHLP_LINE, *PIMAGEHLP_LINE;

typedef struct _IMAGEHLP_LINEW
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_LINE64)
    PVOID    Key;                    // internal
    DWORD    LineNumber;             // line number in file
    PCHAR    FileName;               // full filename
    DWORD64  Address;                // first instruction of line
} IMAGEHLP_LINEW, *PIMAGEHLP_LINEW;
#endif

//
// source file structure
//

typedef struct _SOURCEFILE
{
    DWORD64  ModBase;                // base address of loaded module
    PCHAR    FileName;               // full filename of source
} SOURCEFILE, *PSOURCEFILE;

typedef struct _SOURCEFILEW
{
    DWORD64  ModBase;                // base address of loaded module
    PWSTR    FileName;               // full filename of source
} SOURCEFILEW, *PSOURCEFILEW;

//
// data structures used for registered symbol callbacks
//

#define CBA_DEFERRED_SYMBOL_LOAD_START          0x00000001
#define CBA_DEFERRED_SYMBOL_LOAD_COMPLETE       0x00000002
#define CBA_DEFERRED_SYMBOL_LOAD_FAILURE        0x00000003
#define CBA_SYMBOLS_UNLOADED                    0x00000004
#define CBA_DUPLICATE_SYMBOL                    0x00000005
#define CBA_READ_MEMORY                         0x00000006
#define CBA_DEFERRED_SYMBOL_LOAD_CANCEL         0x00000007
#define CBA_SET_OPTIONS                         0x00000008
#define CBA_EVENT                               0x00000010
#define CBA_DEFERRED_SYMBOL_LOAD_PARTIAL        0x00000020
#define CBA_DEBUG_INFO                          0x10000000
#define CBA_SRCSRV_INFO                         0x20000000
#define CBA_SRCSRV_EVENT                        0x40000000

typedef struct _IMAGEHLP_CBA_READ_MEMORY
{
    DWORD64   addr;                                     // address to read from
    PVOID     buf;                                      // buffer to read to
    DWORD     bytes;                                    // amount of bytes to read
    DWORD*    bytesread;                                // pointer to store amount of bytes read
} IMAGEHLP_CBA_READ_MEMORY, *PIMAGEHLP_CBA_READ_MEMORY;

enum
{
    sevInfo = 0,
    sevProblem,
    sevAttn,
    sevFatal,
    sevMax  // unused
};

#define EVENT_SRCSPEW_START 100
#define EVENT_SRCSPEW       100
#define EVENT_SRCSPEW_END   199

typedef struct _IMAGEHLP_CBA_EVENT
{
    DWORD severity;                                     // values from sevInfo to sevFatal
    DWORD code;                                         // numerical code IDs the error
    PCHAR desc;                                         // may contain a text description of the error
    PVOID object;                                       // value dependant upon the error code
} IMAGEHLP_CBA_EVENT, *PIMAGEHLP_CBA_EVENT;

typedef struct _IMAGEHLP_CBA_EVENTW
{
    DWORD  severity;                                     // values from sevInfo to sevFatal
    DWORD  code;                                         // numerical code IDs the error
    PCWSTR desc;                                         // may contain a text description of the error
    PVOID  object;                                       // value dependant upon the error code
} IMAGEHLP_CBA_EVENTW, *PIMAGEHLP_CBA_EVENTW;

typedef struct _IMAGEHLP_DEFERRED_SYMBOL_LOAD64
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_DEFERRED_SYMBOL_LOAD64)
    DWORD64  BaseOfImage;            // base load address of module
    DWORD    CheckSum;               // checksum from the pe header
    DWORD    TimeDateStamp;          // date/time stamp from pe header
    CHAR     FileName[MAX_PATH];     // symbols file or image name
    BOOLEAN  Reparse;                // load failure reparse
    HANDLE   hFile;                  // file handle, if passed
    DWORD    Flags;                     //
} IMAGEHLP_DEFERRED_SYMBOL_LOAD64, *PIMAGEHLP_DEFERRED_SYMBOL_LOAD64;

typedef struct _IMAGEHLP_DEFERRED_SYMBOL_LOADW64
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_DEFERRED_SYMBOL_LOADW64)
    DWORD64  BaseOfImage;            // base load address of module
    DWORD    CheckSum;               // checksum from the pe header
    DWORD    TimeDateStamp;          // date/time stamp from pe header
    WCHAR    FileName[MAX_PATH + 1]; // symbols file or image name
    BOOLEAN  Reparse;                // load failure reparse
    HANDLE   hFile;                  // file handle, if passed
    DWORD    Flags;         //
} IMAGEHLP_DEFERRED_SYMBOL_LOADW64, *PIMAGEHLP_DEFERRED_SYMBOL_LOADW64;

#define DSLFLAG_MISMATCHED_PDB  0x1
#define DSLFLAG_MISMATCHED_DBG  0x2

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define IMAGEHLP_DEFERRED_SYMBOL_LOAD IMAGEHLP_DEFERRED_SYMBOL_LOAD64
#define PIMAGEHLP_DEFERRED_SYMBOL_LOAD PIMAGEHLP_DEFERRED_SYMBOL_LOAD64
#else
typedef struct _IMAGEHLP_DEFERRED_SYMBOL_LOAD
{
    DWORD    SizeOfStruct;           // set to sizeof(IMAGEHLP_DEFERRED_SYMBOL_LOAD)
    DWORD    BaseOfImage;            // base load address of module
    DWORD    CheckSum;               // checksum from the pe header
    DWORD    TimeDateStamp;          // date/time stamp from pe header
    CHAR     FileName[MAX_PATH];     // symbols file or image name
    BOOLEAN  Reparse;                // load failure reparse
    HANDLE   hFile;                  // file handle, if passed
} IMAGEHLP_DEFERRED_SYMBOL_LOAD, *PIMAGEHLP_DEFERRED_SYMBOL_LOAD;
#endif

typedef struct _IMAGEHLP_DUPLICATE_SYMBOL64
{
    DWORD              SizeOfStruct;           // set to sizeof(IMAGEHLP_DUPLICATE_SYMBOL64)
    DWORD              NumberOfDups;           // number of duplicates in the Symbol array
    PIMAGEHLP_SYMBOL64 Symbol;                 // array of duplicate symbols
    DWORD              SelectedSymbol;         // symbol selected (-1 to start)
} IMAGEHLP_DUPLICATE_SYMBOL64, *PIMAGEHLP_DUPLICATE_SYMBOL64;

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define IMAGEHLP_DUPLICATE_SYMBOL IMAGEHLP_DUPLICATE_SYMBOL64
#define PIMAGEHLP_DUPLICATE_SYMBOL PIMAGEHLP_DUPLICATE_SYMBOL64
#else
typedef struct _IMAGEHLP_DUPLICATE_SYMBOL
{
    DWORD            SizeOfStruct;           // set to sizeof(IMAGEHLP_DUPLICATE_SYMBOL)
    DWORD            NumberOfDups;           // number of duplicates in the Symbol array
    PIMAGEHLP_SYMBOL Symbol;                 // array of duplicate symbols
    DWORD            SelectedSymbol;         // symbol selected (-1 to start)
} IMAGEHLP_DUPLICATE_SYMBOL, *PIMAGEHLP_DUPLICATE_SYMBOL;
#endif

// If dbghelp ever needs to display graphical UI, it will use this as the parent window.

BOOL
IMAGEAPI
SymSetParentWindow(
    __in HWND hwnd
);

PCHAR
IMAGEAPI
SymSetHomeDirectory(
    __in_opt HANDLE hProcess,
    __in_opt PCSTR dir
);

PWSTR
IMAGEAPI
SymSetHomeDirectoryW(
    __in_opt HANDLE hProcess,
    __in_opt PCWSTR dir
);

PCHAR
IMAGEAPI
SymGetHomeDirectory(
    __in DWORD type,
    __out_ecount(size) PSTR dir,
    __in size_t size
);

PWSTR
IMAGEAPI
SymGetHomeDirectoryW(
    __in DWORD type,
    __out_ecount(size) PWSTR dir,
    __in size_t size
);

enum
{
    hdBase = 0, // root directory for dbghelp
    hdSym,      // where symbols are stored
    hdSrc,      // where source is stored
    hdMax       // end marker
};

typedef struct _OMAP
{
    ULONG  rva;
    ULONG  rvaTo;
} OMAP, *POMAP;

BOOL
IMAGEAPI
SymGetOmaps(
    __in HANDLE hProcess,
    __in DWORD64 BaseOfDll,
    __out POMAP* OmapTo,
    __out PDWORD64 cOmapTo,
    __out POMAP* OmapFrom,
    __out PDWORD64 cOmapFrom
);

//
// options that are set/returned by SymSetOptions() & SymGetOptions()
// these are used as a mask
//
#define SYMOPT_CASE_INSENSITIVE          0x00000001
#define SYMOPT_UNDNAME                   0x00000002
#define SYMOPT_DEFERRED_LOADS            0x00000004
#define SYMOPT_NO_CPP                    0x00000008
#define SYMOPT_LOAD_LINES                0x00000010
#define SYMOPT_OMAP_FIND_NEAREST         0x00000020
#define SYMOPT_LOAD_ANYTHING             0x00000040
#define SYMOPT_IGNORE_CVREC              0x00000080
#define SYMOPT_NO_UNQUALIFIED_LOADS      0x00000100
#define SYMOPT_FAIL_CRITICAL_ERRORS      0x00000200
#define SYMOPT_EXACT_SYMBOLS             0x00000400
#define SYMOPT_ALLOW_ABSOLUTE_SYMBOLS    0x00000800
#define SYMOPT_IGNORE_NT_SYMPATH         0x00001000
#define SYMOPT_INCLUDE_32BIT_MODULES     0x00002000
#define SYMOPT_PUBLICS_ONLY              0x00004000
#define SYMOPT_NO_PUBLICS                0x00008000
#define SYMOPT_AUTO_PUBLICS              0x00010000
#define SYMOPT_NO_IMAGE_SEARCH           0x00020000
#define SYMOPT_SECURE                    0x00040000
#define SYMOPT_NO_PROMPTS                0x00080000
#define SYMOPT_OVERWRITE                 0x00100000
#define SYMOPT_IGNORE_IMAGEDIR           0x00200000
#define SYMOPT_FLAT_DIRECTORY            0x00400000
#define SYMOPT_FAVOR_COMPRESSED          0x00800000
#define SYMOPT_ALLOW_ZERO_ADDRESS        0x01000000
#define SYMOPT_DISABLE_SYMSRV_AUTODETECT 0x02000000

#define SYMOPT_DEBUG                     0x80000000

DWORD
IMAGEAPI
SymSetOptions(
    __in DWORD   SymOptions
);

DWORD
IMAGEAPI
SymGetOptions(
    VOID
);

BOOL
IMAGEAPI
SymCleanup(
    __in HANDLE hProcess
);

BOOL
IMAGEAPI
SymMatchString(
    __in PCSTR string,
    __in PCSTR expression,
    __in BOOL fCase
);

BOOL
IMAGEAPI
SymMatchStringA(
    __in PCSTR string,
    __in PCSTR expression,
    __in BOOL fCase
);

BOOL
IMAGEAPI
SymMatchStringW(
    __in PCWSTR string,
    __in PCWSTR expression,
    __in BOOL fCase
);

typedef BOOL
(CALLBACK* PSYM_ENUMSOURCEFILES_CALLBACK)(
    __in PSOURCEFILE pSourceFile,
    __in_opt PVOID UserContext
);

// for backwards compatibility - don't use this
#define PSYM_ENUMSOURCFILES_CALLBACK PSYM_ENUMSOURCEFILES_CALLBACK

BOOL
IMAGEAPI
SymEnumSourceFiles(
    __in HANDLE hProcess,
    __in ULONG64 ModBase,
    __in_opt PCSTR Mask,
    __in PSYM_ENUMSOURCEFILES_CALLBACK cbSrcFiles,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMSOURCEFILES_CALLBACKW)(
    __in PSOURCEFILEW pSourceFile,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumSourceFilesW(
    __in HANDLE hProcess,
    __in ULONG64 ModBase,
    __in_opt PCWSTR Mask,
    __in PSYM_ENUMSOURCEFILES_CALLBACKW cbSrcFiles,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumerateModules64(
    __in HANDLE hProcess,
    __in PSYM_ENUMMODULES_CALLBACK64 EnumModulesCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumerateModulesW64(
    __in HANDLE hProcess,
    __in PSYM_ENUMMODULES_CALLBACKW64 EnumModulesCallback,
    __in_opt PVOID UserContext
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymEnumerateModules SymEnumerateModules64
#else
BOOL
IMAGEAPI
SymEnumerateModules(
    __in HANDLE hProcess,
    __in PSYM_ENUMMODULES_CALLBACK EnumModulesCallback,
    __in_opt PVOID UserContext
);
#endif

BOOL
IMAGEAPI
EnumerateLoadedModulesEx(
    __in HANDLE hProcess,
    __in PENUMLOADED_MODULES_CALLBACK64 EnumLoadedModulesCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
EnumerateLoadedModulesExW(
    __in HANDLE hProcess,
    __in PENUMLOADED_MODULES_CALLBACKW64 EnumLoadedModulesCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
EnumerateLoadedModules64(
    __in HANDLE hProcess,
    __in PENUMLOADED_MODULES_CALLBACK64 EnumLoadedModulesCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
EnumerateLoadedModulesW64(
    __in HANDLE hProcess,
    __in PENUMLOADED_MODULES_CALLBACKW64 EnumLoadedModulesCallback,
    __in_opt PVOID UserContext
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define EnumerateLoadedModules EnumerateLoadedModules64
#else
BOOL
IMAGEAPI
EnumerateLoadedModules(
    __in HANDLE hProcess,
    __in PENUMLOADED_MODULES_CALLBACK EnumLoadedModulesCallback,
    __in_opt PVOID UserContext
);
#endif

PVOID
IMAGEAPI
SymFunctionTableAccess64(
    __in HANDLE hProcess,
    __in DWORD64 AddrBase
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymFunctionTableAccess SymFunctionTableAccess64
#else
PVOID
IMAGEAPI
SymFunctionTableAccess(
    __in HANDLE hProcess,
    __in DWORD AddrBase
);
#endif

BOOL
IMAGEAPI
SymGetUnwindInfo(
    __in HANDLE hProcess,
    __in DWORD64 Address,
    __out_bcount_opt(*Size) PVOID Buffer,
    __inout PULONG Size
);

BOOL
IMAGEAPI
SymGetModuleInfo64(
    __in HANDLE hProcess,
    __in DWORD64 qwAddr,
    __out PIMAGEHLP_MODULE64 ModuleInfo
);

BOOL
IMAGEAPI
SymGetModuleInfoW64(
    __in HANDLE hProcess,
    __in DWORD64 qwAddr,
    __out PIMAGEHLP_MODULEW64 ModuleInfo
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetModuleInfo   SymGetModuleInfo64
#define SymGetModuleInfoW  SymGetModuleInfoW64
#else
BOOL
IMAGEAPI
SymGetModuleInfo(
    __in HANDLE hProcess,
    __in DWORD dwAddr,
    __out PIMAGEHLP_MODULE ModuleInfo
);

BOOL
IMAGEAPI
SymGetModuleInfoW(
    __in HANDLE hProcess,
    __in DWORD dwAddr,
    __out PIMAGEHLP_MODULEW ModuleInfo
);
#endif

DWORD64
IMAGEAPI
SymGetModuleBase64(
    __in HANDLE hProcess,
    __in DWORD64 qwAddr
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetModuleBase SymGetModuleBase64
#else
DWORD
IMAGEAPI
SymGetModuleBase(
    __in HANDLE hProcess,
    __in DWORD dwAddr
);
#endif

typedef struct _SRCCODEINFO
{
    DWORD   SizeOfStruct;           // set to sizeof(SRCCODEINFO)
    PVOID   Key;                    // not used
    DWORD64 ModBase;                // base address of module this applies to
    CHAR    Obj[MAX_PATH + 1];      // the object file within the module
    CHAR    FileName[MAX_PATH + 1]; // full filename
    DWORD   LineNumber;             // line number in file
    DWORD64 Address;                // first instruction of line
} SRCCODEINFO, *PSRCCODEINFO;

typedef struct _SRCCODEINFOW
{
    DWORD   SizeOfStruct;           // set to sizeof(SRCCODEINFO)
    PVOID   Key;                    // not used
    DWORD64 ModBase;                // base address of module this applies to
    WCHAR   Obj[MAX_PATH + 1];      // the object file within the module
    WCHAR   FileName[MAX_PATH + 1]; // full filename
    DWORD   LineNumber;             // line number in file
    DWORD64 Address;                // first instruction of line
} SRCCODEINFOW, *PSRCCODEINFOW;

typedef BOOL
(CALLBACK* PSYM_ENUMLINES_CALLBACK)(
    __in PSRCCODEINFO LineInfo,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumLines(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCSTR Obj,
    __in_opt PCSTR File,
    __in PSYM_ENUMLINES_CALLBACK EnumLinesCallback,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMLINES_CALLBACKW)(
    __in PSRCCODEINFOW LineInfo,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumLinesW(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCWSTR Obj,
    __in_opt PCWSTR File,
    __in PSYM_ENUMLINES_CALLBACKW EnumLinesCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymGetLineFromAddr64(
    __in HANDLE hProcess,
    __in DWORD64 qwAddr,
    __out PDWORD pdwDisplacement,
    __out PIMAGEHLP_LINE64 Line64
);

BOOL
IMAGEAPI
SymGetLineFromAddrW64(
    __in HANDLE hProcess,
    __in DWORD64 dwAddr,
    __out PDWORD pdwDisplacement,
    __out PIMAGEHLP_LINEW64 Line
);

BOOL
IMAGEAPI
SymEnumSourceLines(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCSTR Obj,
    __in_opt PCSTR File,
    __in_opt DWORD Line,
    __in DWORD Flags,
    __in PSYM_ENUMLINES_CALLBACK EnumLinesCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumSourceLinesW(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCWSTR Obj,
    __in_opt PCWSTR File,
    __in_opt DWORD Line,
    __in DWORD Flags,
    __in PSYM_ENUMLINES_CALLBACKW EnumLinesCallback,
    __in_opt PVOID UserContext
);

// flags for SymEnumSourceLines

#define ESLFLAG_FULLPATH        0x1
#define ESLFLAG_NEAREST         0x2
#define ESLFLAG_PREV            0x4
#define ESLFLAG_NEXT            0x8

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetLineFromAddr SymGetLineFromAddr64
#define SymGetLineFromAddrW SymGetLineFromAddrW64
#else
BOOL
IMAGEAPI
SymGetLineFromAddr(
    __in HANDLE hProcess,
    __in DWORD dwAddr,
    __out PDWORD pdwDisplacement,
    __out PIMAGEHLP_LINE Line
);

BOOL
IMAGEAPI
SymGetLineFromAddrW(
    __in HANDLE hProcess,
    __in DWORD dwAddr,
    __out PDWORD pdwDisplacement,
    __out PIMAGEHLP_LINEW Line
);
#endif

BOOL
IMAGEAPI
SymGetLineFromName64(
    __in HANDLE hProcess,
    __in_opt PCSTR ModuleName,
    __in_opt PCSTR FileName,
    __in DWORD dwLineNumber,
    __out PLONG plDisplacement,
    __inout PIMAGEHLP_LINE64 Line
);

BOOL
IMAGEAPI
SymGetLineFromNameW64(
    __in HANDLE hProcess,
    __in_opt PCWSTR ModuleName,
    __in_opt PCWSTR FileName,
    __in DWORD dwLineNumber,
    __out PLONG plDisplacement,
    __inout PIMAGEHLP_LINEW64 Line
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetLineFromName SymGetLineFromName64
#else
BOOL
IMAGEAPI
SymGetLineFromName(
    __in HANDLE hProcess,
    __in_opt PCSTR ModuleName,
    __in_opt PCSTR FileName,
    __in DWORD dwLineNumber,
    __out PLONG plDisplacement,
    __inout PIMAGEHLP_LINE Line
);
#endif

BOOL
IMAGEAPI
SymGetLineNext64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINE64 Line
);

BOOL
IMAGEAPI
SymGetLineNextW64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINEW64 Line
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetLineNext SymGetLineNext64
#else
BOOL
IMAGEAPI
SymGetLineNext(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINE Line
);

BOOL
IMAGEAPI
SymGetLineNextW(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINEW Line
);
#endif

BOOL
IMAGEAPI
SymGetLinePrev64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINE64 Line
);

BOOL
IMAGEAPI
SymGetLinePrevW64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINEW64 Line
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetLinePrev SymGetLinePrev64
#else
BOOL
IMAGEAPI
SymGetLinePrev(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINE Line
);

BOOL
IMAGEAPI
SymGetLinePrevW(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_LINEW Line
);
#endif

ULONG
IMAGEAPI
SymGetFileLineOffsets64(
    __in HANDLE hProcess,
    __in_opt PCSTR ModuleName,
    __in PCSTR FileName,
    __out_ecount(BufferLines) PDWORD64 Buffer,
    __in ULONG BufferLines
);

BOOL
IMAGEAPI
SymMatchFileName(
    __in PCSTR FileName,
    __in PCSTR Match,
    __deref_opt_out PSTR* FileNameStop,
    __deref_opt_out PSTR* MatchStop
);

BOOL
IMAGEAPI
SymMatchFileNameW(
    __in PCWSTR FileName,
    __in PCWSTR Match,
    __deref_opt_out PWSTR* FileNameStop,
    __deref_opt_out PWSTR* MatchStop
);

BOOL
IMAGEAPI
SymGetSourceFile(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCSTR Params,
    __in PCSTR FileSpec,
    __out_ecount(Size) PSTR FilePath,
    __in DWORD Size
);

BOOL
IMAGEAPI
SymGetSourceFileW(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCWSTR Params,
    __in PCWSTR FileSpec,
    __out_ecount(Size) PWSTR FilePath,
    __in DWORD Size
);

BOOL
IMAGEAPI
SymGetSourceFileToken(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in PCSTR FileSpec,
    __deref_out PVOID* Token,
    __out DWORD* Size
);

BOOL
IMAGEAPI
SymGetSourceFileTokenW(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in PCWSTR FileSpec,
    __deref_out PVOID* Token,
    __out DWORD* Size
);

BOOL
IMAGEAPI
SymGetSourceFileFromToken(
    __in HANDLE hProcess,
    __in PVOID Token,
    __in_opt PCSTR Params,
    __out_ecount(Size) PSTR FilePath,
    __in DWORD Size
);

BOOL
IMAGEAPI
SymGetSourceFileFromTokenW(
    __in HANDLE hProcess,
    __in PVOID Token,
    __in_opt PCWSTR Params,
    __out_ecount(Size) PWSTR FilePath,
    __in DWORD Size
);

BOOL
IMAGEAPI
SymGetSourceVarFromToken(
    __in HANDLE hProcess,
    __in PVOID Token,
    __in_opt PCSTR Params,
    __in PCSTR VarName,
    __out_ecount(Size) PSTR Value,
    __in DWORD Size
);

BOOL
IMAGEAPI
SymGetSourceVarFromTokenW(
    __in HANDLE hProcess,
    __in PVOID Token,
    __in_opt PCWSTR Params,
    __in PCWSTR VarName,
    __out_ecount(Size) PWSTR Value,
    __in DWORD Size
);

typedef BOOL (CALLBACK* PENUMSOURCEFILETOKENSCALLBACK)(__in PVOID token,  __in size_t size);

BOOL
IMAGEAPI
SymEnumSourceFileTokens(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in PENUMSOURCEFILETOKENSCALLBACK Callback
);

BOOL
IMAGEAPI
SymInitialize(
    __in HANDLE hProcess,
    __in_opt PCSTR UserSearchPath,
    __in BOOL fInvadeProcess
);

BOOL
IMAGEAPI
SymInitializeW(
    __in HANDLE hProcess,
    __in_opt PCWSTR UserSearchPath,
    __in BOOL fInvadeProcess
);

BOOL
IMAGEAPI
SymGetSearchPath(
    __in HANDLE hProcess,
    __out_ecount(SearchPathLength) PSTR SearchPath,
    __in DWORD SearchPathLength
);

BOOL
IMAGEAPI
SymGetSearchPathW(
    __in HANDLE hProcess,
    __out_ecount(SearchPathLength) PWSTR SearchPath,
    __in DWORD SearchPathLength
);

BOOL
IMAGEAPI
SymSetSearchPath(
    __in HANDLE hProcess,
    __in_opt PCSTR SearchPath
);

BOOL
IMAGEAPI
SymSetSearchPathW(
    __in HANDLE hProcess,
    __in_opt PCWSTR SearchPath
);

#define SLMFLAG_VIRTUAL     0x1
#define SLMFLAG_ALT_INDEX   0x2
#define SLMFLAG_NO_SYMBOLS  0x4

DWORD64
IMAGEAPI
SymLoadModuleEx(
    __in HANDLE hProcess,
    __in_opt HANDLE hFile,
    __in_opt PCSTR ImageName,
    __in_opt PCSTR ModuleName,
    __in DWORD64 BaseOfDll,
    __in DWORD DllSize,
    __in_opt PMODLOAD_DATA Data,
    __in_opt DWORD Flags
);

DWORD64
IMAGEAPI
SymLoadModuleExW(
    __in HANDLE hProcess,
    __in_opt HANDLE hFile,
    __in_opt PCWSTR ImageName,
    __in_opt PCWSTR ModuleName,
    __in DWORD64 BaseOfDll,
    __in DWORD DllSize,
    __in_opt PMODLOAD_DATA Data,
    __in_opt DWORD Flags
);

BOOL
IMAGEAPI
SymUnloadModule64(
    __in HANDLE hProcess,
    __in DWORD64 BaseOfDll
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymUnloadModule SymUnloadModule64
#else
BOOL
IMAGEAPI
SymUnloadModule(
    __in HANDLE hProcess,
    __in DWORD BaseOfDll
);
#endif

BOOL
IMAGEAPI
SymUnDName64(
    __in PIMAGEHLP_SYMBOL64 sym,            // Symbol to undecorate
    __out_ecount(UnDecNameLength) PSTR UnDecName,   // Buffer to store undecorated name in
    __in DWORD UnDecNameLength              // Size of the buffer
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymUnDName SymUnDName64
#else
BOOL
IMAGEAPI
SymUnDName(
    __in PIMAGEHLP_SYMBOL sym,              // Symbol to undecorate
    __out_ecount(UnDecNameLength) PSTR UnDecName,   // Buffer to store undecorated name in
    __in DWORD UnDecNameLength              // Size of the buffer
);
#endif

BOOL
IMAGEAPI
SymRegisterCallback64(
    __in HANDLE hProcess,
    __in PSYMBOL_REGISTERED_CALLBACK64 CallbackFunction,
    __in ULONG64 UserContext
);

BOOL
IMAGEAPI
SymRegisterCallbackW64(
    __in HANDLE hProcess,
    __in PSYMBOL_REGISTERED_CALLBACK64 CallbackFunction,
    __in ULONG64 UserContext
);

BOOL
IMAGEAPI
SymRegisterFunctionEntryCallback64(
    __in HANDLE hProcess,
    __in PSYMBOL_FUNCENTRY_CALLBACK64 CallbackFunction,
    __in ULONG64 UserContext
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymRegisterCallback SymRegisterCallback64
#define SymRegisterFunctionEntryCallback SymRegisterFunctionEntryCallback64
#else
BOOL
IMAGEAPI
SymRegisterCallback(
    __in HANDLE hProcess,
    __in PSYMBOL_REGISTERED_CALLBACK CallbackFunction,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymRegisterFunctionEntryCallback(
    __in HANDLE hProcess,
    __in PSYMBOL_FUNCENTRY_CALLBACK CallbackFunction,
    __in_opt PVOID UserContext
);
#endif


typedef struct _IMAGEHLP_SYMBOL_SRC
{
    DWORD sizeofstruct;
    DWORD type;
    char  file[MAX_PATH];
} IMAGEHLP_SYMBOL_SRC, *PIMAGEHLP_SYMBOL_SRC;

typedef struct _MODULE_TYPE_INFO   // AKA TYPTYP
{
    USHORT      dataLength;
    USHORT      leaf;
    BYTE        data[1];
} MODULE_TYPE_INFO, *PMODULE_TYPE_INFO;

typedef struct _SYMBOL_INFO
{
    ULONG       SizeOfStruct;
    ULONG       TypeIndex;        // Type Index of symbol
    ULONG64     Reserved[2];
    ULONG       Index;
    ULONG       Size;
    ULONG64     ModBase;          // Base Address of module comtaining this symbol
    ULONG       Flags;
    ULONG64     Value;            // Value of symbol, ValuePresent should be 1
    ULONG64     Address;          // Address of symbol including base address of module
    ULONG       Register;         // register holding value or pointer to value
    ULONG       Scope;            // scope of the symbol
    ULONG       Tag;              // pdb classification
    ULONG       NameLen;          // Actual length of name
    ULONG       MaxNameLen;
    CHAR        Name[1];          // Name of symbol
} SYMBOL_INFO, *PSYMBOL_INFO;

typedef struct _SYMBOL_INFO_PACKAGE
{
    SYMBOL_INFO si;
    CHAR        name[MAX_SYM_NAME + 1];
} SYMBOL_INFO_PACKAGE, *PSYMBOL_INFO_PACKAGE;

typedef struct _SYMBOL_INFOW
{
    ULONG       SizeOfStruct;
    ULONG       TypeIndex;        // Type Index of symbol
    ULONG64     Reserved[2];
    ULONG       Index;
    ULONG       Size;
    ULONG64     ModBase;          // Base Address of module comtaining this symbol
    ULONG       Flags;
    ULONG64     Value;            // Value of symbol, ValuePresent should be 1
    ULONG64     Address;          // Address of symbol including base address of module
    ULONG       Register;         // register holding value or pointer to value
    ULONG       Scope;            // scope of the symbol
    ULONG       Tag;              // pdb classification
    ULONG       NameLen;          // Actual length of name
    ULONG       MaxNameLen;
    WCHAR       Name[1];          // Name of symbol
} SYMBOL_INFOW, *PSYMBOL_INFOW;

typedef struct _SYMBOL_INFO_PACKAGEW
{
    SYMBOL_INFOW si;
    WCHAR        name[MAX_SYM_NAME + 1];
} SYMBOL_INFO_PACKAGEW, *PSYMBOL_INFO_PACKAGEW;

typedef struct _IMAGEHLP_STACK_FRAME
{
    ULONG64 InstructionOffset;
    ULONG64 ReturnOffset;
    ULONG64 FrameOffset;
    ULONG64 StackOffset;
    ULONG64 BackingStoreOffset;
    ULONG64 FuncTableEntry;
    ULONG64 Params[4];
    ULONG64 Reserved[5];
    BOOL    Virtual;
    ULONG   Reserved2;
} IMAGEHLP_STACK_FRAME, *PIMAGEHLP_STACK_FRAME;

typedef VOID IMAGEHLP_CONTEXT, *PIMAGEHLP_CONTEXT;


BOOL
IMAGEAPI
SymSetContext(
    __in HANDLE hProcess,
    __in PIMAGEHLP_STACK_FRAME StackFrame,
    __in_opt PIMAGEHLP_CONTEXT Context
);

BOOL
IMAGEAPI
SymSetScopeFromAddr(
    __in HANDLE hProcess,
    __in ULONG64 Address
);

BOOL
IMAGEAPI
SymSetScopeFromIndex(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in DWORD Index
);

typedef BOOL
(CALLBACK* PSYM_ENUMPROCESSES_CALLBACK)(
    __in HANDLE hProcess,
    __in PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumProcesses(
    __in PSYM_ENUMPROCESSES_CALLBACK EnumProcessesCallback,
    __in PVOID UserContext
);

BOOL
IMAGEAPI
SymFromAddr(
    __in HANDLE hProcess,
    __in DWORD64 Address,
    __out_opt PDWORD64 Displacement,
    __inout PSYMBOL_INFO Symbol
);

BOOL
IMAGEAPI
SymFromAddrW(
    __in HANDLE hProcess,
    __in DWORD64 Address,
    __out_opt PDWORD64 Displacement,
    __inout PSYMBOL_INFOW Symbol
);

BOOL
IMAGEAPI
SymFromToken(
    __in HANDLE hProcess,
    __in DWORD64 Base,
    __in DWORD Token,
    __inout PSYMBOL_INFO Symbol
);

BOOL
IMAGEAPI
SymFromTokenW(
    __in HANDLE hProcess,
    __in DWORD64 Base,
    __in DWORD Token,
    __inout PSYMBOL_INFOW Symbol
);

BOOL
IMAGEAPI
SymNext(
    __in HANDLE hProcess,
    __inout PSYMBOL_INFO si
);

BOOL
IMAGEAPI
SymNextW(
    __in HANDLE hProcess,
    __inout PSYMBOL_INFOW siw
);

BOOL
IMAGEAPI
SymPrev(
    __in HANDLE hProcess,
    __inout PSYMBOL_INFO si
);

BOOL
IMAGEAPI
SymPrevW(
    __in HANDLE hProcess,
    __inout PSYMBOL_INFOW siw
);

// While SymFromName will provide a symbol from a name,
// SymEnumSymbols can provide the same matching information
// for ALL symbols with a matching name, even regular
// expressions.  That way you can search across modules
// and differentiate between identically named symbols.

BOOL
IMAGEAPI
SymFromName(
    __in HANDLE hProcess,
    __in PCSTR Name,
    __inout PSYMBOL_INFO Symbol
);

BOOL
IMAGEAPI
SymFromNameW(
    __in HANDLE hProcess,
    __in PCWSTR Name,
    __inout PSYMBOL_INFOW Symbol
);

typedef BOOL
(CALLBACK* PSYM_ENUMERATESYMBOLS_CALLBACK)(
    __in PSYMBOL_INFO pSymInfo,
    __in ULONG SymbolSize,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumSymbols(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt PCSTR Mask,
    __in PSYM_ENUMERATESYMBOLS_CALLBACK EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

typedef BOOL
(CALLBACK* PSYM_ENUMERATESYMBOLS_CALLBACKW)(
    __in PSYMBOL_INFOW pSymInfo,
    __in ULONG SymbolSize,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumSymbolsW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt PCWSTR Mask,
    __in PSYM_ENUMERATESYMBOLS_CALLBACKW EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumSymbolsForAddr(
    __in HANDLE hProcess,
    __in DWORD64 Address,
    __in PSYM_ENUMERATESYMBOLS_CALLBACK EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumSymbolsForAddrW(
    __in HANDLE hProcess,
    __in DWORD64 Address,
    __in PSYM_ENUMERATESYMBOLS_CALLBACKW EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

#define SYMSEARCH_MASKOBJS      0x01    // used internally to implement other APIs
#define SYMSEARCH_RECURSE       0X02    // recurse scopes
#define SYMSEARCH_GLOBALSONLY   0X04    // search only for global symbols
#define SYMSEARCH_ALLITEMS      0X08    // search for everything in the pdb, not just normal scoped symbols

BOOL
IMAGEAPI
SymSearch(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt DWORD Index,
    __in_opt DWORD SymTag,
    __in_opt PCSTR Mask,
    __in_opt DWORD64 Address,
    __in PSYM_ENUMERATESYMBOLS_CALLBACK EnumSymbolsCallback,
    __in_opt PVOID UserContext,
    __in DWORD Options
);

BOOL
IMAGEAPI
SymSearchW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt DWORD Index,
    __in_opt DWORD SymTag,
    __in_opt PCWSTR Mask,
    __in_opt DWORD64 Address,
    __in PSYM_ENUMERATESYMBOLS_CALLBACKW EnumSymbolsCallback,
    __in_opt PVOID UserContext,
    __in DWORD Options
);

BOOL
IMAGEAPI
SymGetScope(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in DWORD Index,
    __inout PSYMBOL_INFO Symbol
);

BOOL
IMAGEAPI
SymGetScopeW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in DWORD Index,
    __inout PSYMBOL_INFOW Symbol
);

BOOL
IMAGEAPI
SymFromIndex(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in DWORD Index,
    __inout PSYMBOL_INFO Symbol
);

BOOL
IMAGEAPI
SymFromIndexW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in DWORD Index,
    __inout PSYMBOL_INFOW Symbol
);

typedef enum _IMAGEHLP_SYMBOL_TYPE_INFO
{
    TI_GET_SYMTAG,
    TI_GET_SYMNAME,
    TI_GET_LENGTH,
    TI_GET_TYPE,
    TI_GET_TYPEID,
    TI_GET_BASETYPE,
    TI_GET_ARRAYINDEXTYPEID,
    TI_FINDCHILDREN,
    TI_GET_DATAKIND,
    TI_GET_ADDRESSOFFSET,
    TI_GET_OFFSET,
    TI_GET_VALUE,
    TI_GET_COUNT,
    TI_GET_CHILDRENCOUNT,
    TI_GET_BITPOSITION,
    TI_GET_VIRTUALBASECLASS,
    TI_GET_VIRTUALTABLESHAPEID,
    TI_GET_VIRTUALBASEPOINTEROFFSET,
    TI_GET_CLASSPARENTID,
    TI_GET_NESTED,
    TI_GET_SYMINDEX,
    TI_GET_LEXICALPARENT,
    TI_GET_ADDRESS,
    TI_GET_THISADJUST,
    TI_GET_UDTKIND,
    TI_IS_EQUIV_TO,
    TI_GET_CALLING_CONVENTION,
    TI_IS_CLOSE_EQUIV_TO,
    TI_GTIEX_REQS_VALID,
    TI_GET_VIRTUALBASEOFFSET,
    TI_GET_VIRTUALBASEDISPINDEX,
    TI_GET_IS_REFERENCE,
    TI_GET_INDIRECTVIRTUALBASECLASS,
    IMAGEHLP_SYMBOL_TYPE_INFO_MAX,
} IMAGEHLP_SYMBOL_TYPE_INFO;

typedef struct _TI_FINDCHILDREN_PARAMS
{
    ULONG Count;
    ULONG Start;
    ULONG ChildId[1];
} TI_FINDCHILDREN_PARAMS;

BOOL
IMAGEAPI
SymGetTypeInfo(
    __in HANDLE hProcess,
    __in DWORD64 ModBase,
    __in ULONG TypeId,
    __in IMAGEHLP_SYMBOL_TYPE_INFO GetType,
    __out PVOID pInfo
);

#define IMAGEHLP_GET_TYPE_INFO_UNCACHED 0x00000001
#define IMAGEHLP_GET_TYPE_INFO_CHILDREN 0x00000002

typedef struct _IMAGEHLP_GET_TYPE_INFO_PARAMS
{
    IN  ULONG    SizeOfStruct;
    IN  ULONG    Flags;
    IN  ULONG    NumIds;
    IN  PULONG   TypeIds;
    IN  ULONG64  TagFilter;
    IN  ULONG    NumReqs;
    IN  IMAGEHLP_SYMBOL_TYPE_INFO* ReqKinds;
    IN  PULONG_PTR ReqOffsets;
    IN  PULONG   ReqSizes;
    IN  ULONG_PTR ReqStride;
    IN  ULONG_PTR BufferSize;
    OUT PVOID    Buffer;
    OUT ULONG    EntriesMatched;
    OUT ULONG    EntriesFilled;
    OUT ULONG64  TagsFound;
    OUT ULONG64  AllReqsValid;
    IN  ULONG    NumReqsValid;
    OUT PULONG64 ReqsValid OPTIONAL;
} IMAGEHLP_GET_TYPE_INFO_PARAMS, *PIMAGEHLP_GET_TYPE_INFO_PARAMS;

BOOL
IMAGEAPI
SymGetTypeInfoEx(
    __in HANDLE hProcess,
    __in DWORD64 ModBase,
    __inout PIMAGEHLP_GET_TYPE_INFO_PARAMS Params
);

BOOL
IMAGEAPI
SymEnumTypes(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PSYM_ENUMERATESYMBOLS_CALLBACK EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumTypesW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PSYM_ENUMERATESYMBOLS_CALLBACKW EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumTypesByName(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt PCSTR mask,
    __in PSYM_ENUMERATESYMBOLS_CALLBACK EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymEnumTypesByNameW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt PCWSTR mask,
    __in PSYM_ENUMERATESYMBOLS_CALLBACKW EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

BOOL
IMAGEAPI
SymGetTypeFromName(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PCSTR Name,
    __inout PSYMBOL_INFO Symbol
);

BOOL
IMAGEAPI
SymGetTypeFromNameW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PCWSTR Name,
    __inout PSYMBOL_INFOW Symbol
);

BOOL
IMAGEAPI
SymAddSymbol(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PCSTR Name,
    __in DWORD64 Address,
    __in DWORD Size,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymAddSymbolW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PCWSTR Name,
    __in DWORD64 Address,
    __in DWORD Size,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymDeleteSymbol(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt PCSTR Name,
    __in DWORD64 Address,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymDeleteSymbolW(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in_opt PCWSTR Name,
    __in DWORD64 Address,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymRefreshModuleList(
    __in HANDLE hProcess
);

BOOL
IMAGEAPI
SymAddSourceStream(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCSTR StreamFile,
    __in_bcount_opt(Size) PBYTE Buffer,
    __in size_t Size
);

typedef BOOL (WINAPI* SYMADDSOURCESTREAM)(HANDLE, ULONG64, PCSTR, PBYTE, size_t);

BOOL
IMAGEAPI
SymAddSourceStreamA(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCSTR StreamFile,
    __in_bcount_opt(Size) PBYTE Buffer,
    __in size_t Size
);

typedef BOOL (WINAPI* SYMADDSOURCESTREAMA)(HANDLE, ULONG64, PCSTR, PBYTE, size_t);

BOOL
IMAGEAPI
SymAddSourceStreamW(
    __in HANDLE hProcess,
    __in ULONG64 Base,
    __in_opt PCWSTR FileSpec,
    __in_bcount_opt(Size) PBYTE Buffer,
    __in size_t Size
);

BOOL
IMAGEAPI
SymSrvIsStoreW(
    __in_opt HANDLE hProcess,
    __in PCWSTR path
);

BOOL
IMAGEAPI
SymSrvIsStore(
    __in_opt HANDLE hProcess,
    __in PCSTR path
);

PCSTR
IMAGEAPI
SymSrvDeltaName(
    __in HANDLE hProcess,
    __in_opt PCSTR SymPath,
    __in PCSTR Type,
    __in PCSTR File1,
    __in PCSTR File2
);

PCWSTR
IMAGEAPI
SymSrvDeltaNameW(
    __in HANDLE hProcess,
    __in_opt PCWSTR SymPath,
    __in PCWSTR Type,
    __in PCWSTR File1,
    __in PCWSTR File2
);

PCSTR
IMAGEAPI
SymSrvGetSupplement(
    __in HANDLE hProcess,
    __in_opt PCSTR SymPath,
    __in PCSTR Node,
    __in PCSTR File
);

PCWSTR
IMAGEAPI
SymSrvGetSupplementW(
    __in HANDLE hProcess,
    __in_opt PCWSTR SymPath,
    __in PCWSTR Node,
    __in PCWSTR File
);

BOOL
IMAGEAPI
SymSrvGetFileIndexes(
    __in PCSTR File,
    __out GUID* Id,
    __out PDWORD Val1,
    __out_opt PDWORD Val2,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymSrvGetFileIndexesW(
    __in PCWSTR File,
    __out GUID* Id,
    __out PDWORD Val1,
    __out_opt PDWORD Val2,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymSrvGetFileIndexStringW(
    __in HANDLE hProcess,
    __in_opt PCWSTR SrvPath,
    __in PCWSTR File,
    __out_ecount(Size) PWSTR Index,
    __in size_t Size,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymSrvGetFileIndexString(
    __in HANDLE hProcess,
    __in_opt PCSTR SrvPath,
    __in PCSTR File,
    __out_ecount(Size) PSTR Index,
    __in size_t Size,
    __in DWORD Flags
);

typedef struct
{
    DWORD sizeofstruct;
    char file[MAX_PATH + 1];
    BOOL  stripped;
    DWORD timestamp;
    DWORD size;
    char dbgfile[MAX_PATH + 1];
    char pdbfile[MAX_PATH + 1];
    GUID  guid;
    DWORD sig;
    DWORD age;
} SYMSRV_INDEX_INFO, *PSYMSRV_INDEX_INFO;

typedef struct
{
    DWORD sizeofstruct;
    WCHAR file[MAX_PATH + 1];
    BOOL  stripped;
    DWORD timestamp;
    DWORD size;
    WCHAR dbgfile[MAX_PATH + 1];
    WCHAR pdbfile[MAX_PATH + 1];
    GUID  guid;
    DWORD sig;
    DWORD age;
} SYMSRV_INDEX_INFOW, *PSYMSRV_INDEX_INFOW;

BOOL
IMAGEAPI
SymSrvGetFileIndexInfo(
    __in PCSTR File,
    __out PSYMSRV_INDEX_INFO Info,
    __in DWORD Flags
);

BOOL
IMAGEAPI
SymSrvGetFileIndexInfoW(
    __in PCWSTR File,
    __out PSYMSRV_INDEX_INFOW Info,
    __in DWORD Flags
);

PCSTR
IMAGEAPI
SymSrvStoreSupplement(
    __in HANDLE hProcess,
    __in_opt PCSTR SrvPath,
    __in PCSTR Node,
    __in PCSTR File,
    __in DWORD Flags
);

PCWSTR
IMAGEAPI
SymSrvStoreSupplementW(
    __in HANDLE hProcess,
    __in_opt PCWSTR SymPath,
    __in PCWSTR Node,
    __in PCWSTR File,
    __in DWORD Flags
);

PCSTR
IMAGEAPI
SymSrvStoreFile(
    __in HANDLE hProcess,
    __in_opt PCSTR SrvPath,
    __in PCSTR File,
    __in DWORD Flags
);

PCWSTR
IMAGEAPI
SymSrvStoreFileW(
    __in HANDLE hProcess,
    __in_opt PCWSTR SrvPath,
    __in PCWSTR File,
    __in DWORD Flags
);

// used by SymGetSymbolFile's "Type" parameter

enum
{
    sfImage = 0,
    sfDbg,
    sfPdb,
    sfMpd,
    sfMax
};

BOOL
IMAGEAPI
SymGetSymbolFile(
    __in_opt HANDLE hProcess,
    __in_opt PCSTR SymPath,
    __in PCSTR ImageFile,
    __in DWORD Type,
    __out_ecount(cSymbolFile) PSTR SymbolFile,
    __in size_t cSymbolFile,
    __out_ecount(cDbgFile) PSTR DbgFile,
    __in size_t cDbgFile
);

BOOL
IMAGEAPI
SymGetSymbolFileW(
    __in_opt HANDLE hProcess,
    __in_opt PCWSTR SymPath,
    __in PCWSTR ImageFile,
    __in DWORD Type,
    __out_ecount(cSymbolFile) PWSTR SymbolFile,
    __in size_t cSymbolFile,
    __out_ecount(cDbgFile) PWSTR DbgFile,
    __in size_t cDbgFile
);

//
// Full user-mode dump creation.
//

typedef BOOL (WINAPI* PDBGHELP_CREATE_USER_DUMP_CALLBACK)(
    __in DWORD DataType,
    __in PVOID* Data,
    __out LPDWORD DataLength,
    __in_opt PVOID UserData
);

BOOL
WINAPI
DbgHelpCreateUserDump(
    __in_opt LPCSTR FileName,
    __in PDBGHELP_CREATE_USER_DUMP_CALLBACK Callback,
    __in_opt PVOID UserData
);

BOOL
WINAPI
DbgHelpCreateUserDumpW(
    __in_opt LPCWSTR FileName,
    __in PDBGHELP_CREATE_USER_DUMP_CALLBACK Callback,
    __in_opt PVOID UserData
);

// -----------------------------------------------------------------
// The following 4 legacy APIs are fully supported, but newer
// ones are recommended.  SymFromName and SymFromAddr provide
// much more detailed info on the returned symbol.

BOOL
IMAGEAPI
SymGetSymFromAddr64(
    __in HANDLE hProcess,
    __in DWORD64 qwAddr,
    __out_opt PDWORD64 pdwDisplacement,
    __inout PIMAGEHLP_SYMBOL64  Symbol
);


#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetSymFromAddr SymGetSymFromAddr64
#else
BOOL
IMAGEAPI
SymGetSymFromAddr(
    __in HANDLE hProcess,
    __in DWORD dwAddr,
    __out_opt PDWORD pdwDisplacement,
    __inout PIMAGEHLP_SYMBOL Symbol
);
#endif

// While following two APIs will provide a symbol from a name,
// SymEnumSymbols can provide the same matching information
// for ALL symbols with a matching name, even regular
// expressions.  That way you can search across modules
// and differentiate between identically named symbols.

BOOL
IMAGEAPI
SymGetSymFromName64(
    __in HANDLE hProcess,
    __in PCSTR Name,
    __inout PIMAGEHLP_SYMBOL64 Symbol
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetSymFromName SymGetSymFromName64
#else
BOOL
IMAGEAPI
SymGetSymFromName(
    __in HANDLE hProcess,
    __in PCSTR Name,
    __inout PIMAGEHLP_SYMBOL Symbol
);
#endif


// Symbol server exports

typedef BOOL (WINAPI* PSYMBOLSERVERPROC)(PCSTR, PCSTR, PVOID, DWORD, DWORD, PSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERPROCA)(PCSTR, PCSTR, PVOID, DWORD, DWORD, PSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERPROCW)(PCWSTR, PCWSTR, PVOID, DWORD, DWORD, PWSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERBYINDEXPROC)(PCSTR, PCSTR, PCSTR, PSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERBYINDEXPROCA)(PCSTR, PCSTR, PCSTR, PSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERBYINDEXPROCW)(PCWSTR, PCWSTR, PCWSTR, PWSTR);
typedef BOOL (WINAPI* PSYMBOLSERVEROPENPROC)(VOID);
typedef BOOL (WINAPI* PSYMBOLSERVERCLOSEPROC)(VOID);
typedef BOOL (WINAPI* PSYMBOLSERVERSETOPTIONSPROC)(UINT_PTR, ULONG64);
typedef BOOL (WINAPI* PSYMBOLSERVERSETOPTIONSWPROC)(UINT_PTR, ULONG64);
typedef BOOL (CALLBACK WINAPI* PSYMBOLSERVERCALLBACKPROC)(UINT_PTR action, ULONG64 data, ULONG64 context);
typedef UINT_PTR(WINAPI* PSYMBOLSERVERGETOPTIONSPROC)();
typedef BOOL (WINAPI* PSYMBOLSERVERPINGPROC)(PCSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERPINGPROCA)(PCSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERPINGPROCW)(PCWSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERGETVERSION)(LPAPI_VERSION);
typedef BOOL (WINAPI* PSYMBOLSERVERDELTANAME)(PCSTR, PVOID, DWORD, DWORD, PVOID, DWORD, DWORD, PSTR, size_t);
typedef BOOL (WINAPI* PSYMBOLSERVERDELTANAMEW)(PCWSTR, PVOID, DWORD, DWORD, PVOID, DWORD, DWORD, PWSTR, size_t);
typedef BOOL (WINAPI* PSYMBOLSERVERGETSUPPLEMENT)(PCSTR, PCSTR, PCSTR, PSTR, size_t);
typedef BOOL (WINAPI* PSYMBOLSERVERGETSUPPLEMENTW)(PCWSTR, PCWSTR, PCWSTR, PWSTR, size_t);
typedef BOOL (WINAPI* PSYMBOLSERVERSTORESUPPLEMENT)(PCSTR, PCSTR, PCSTR, PSTR, size_t, DWORD);
typedef BOOL (WINAPI* PSYMBOLSERVERSTORESUPPLEMENTW)(PCWSTR, PCWSTR, PCWSTR, PWSTR, size_t, DWORD);
typedef BOOL (WINAPI* PSYMBOLSERVERGETINDEXSTRING)(PVOID, DWORD, DWORD, PSTR, size_t);
typedef BOOL (WINAPI* PSYMBOLSERVERGETINDEXSTRINGW)(PVOID, DWORD, DWORD, PWSTR, size_t);
typedef BOOL (WINAPI* PSYMBOLSERVERSTOREFILE)(PCSTR, PCSTR, PVOID, DWORD, DWORD, PSTR, size_t, DWORD);
typedef BOOL (WINAPI* PSYMBOLSERVERSTOREFILEW)(PCWSTR, PCWSTR, PVOID, DWORD, DWORD, PWSTR, size_t, DWORD);
typedef BOOL (WINAPI* PSYMBOLSERVERISSTORE)(PCSTR);
typedef BOOL (WINAPI* PSYMBOLSERVERISSTOREW)(PCWSTR);
typedef DWORD (WINAPI* PSYMBOLSERVERVERSION)();
typedef BOOL (CALLBACK WINAPI* PSYMBOLSERVERMESSAGEPROC)(UINT_PTR action, ULONG64 data, ULONG64 context);

#define SYMSRV_VERSION              2

#define SSRVOPT_CALLBACK            0x00000001
#define SSRVOPT_DWORD               0x00000002
#define SSRVOPT_DWORDPTR            0x00000004
#define SSRVOPT_GUIDPTR             0x00000008
#define SSRVOPT_OLDGUIDPTR          0x00000010
#define SSRVOPT_UNATTENDED          0x00000020
#define SSRVOPT_NOCOPY              0x00000040
#define SSRVOPT_GETPATH             0x00000040
#define SSRVOPT_PARENTWIN           0x00000080
#define SSRVOPT_PARAMTYPE           0x00000100
#define SSRVOPT_SECURE              0x00000200
#define SSRVOPT_TRACE               0x00000400
#define SSRVOPT_SETCONTEXT          0x00000800
#define SSRVOPT_PROXY               0x00001000
#define SSRVOPT_DOWNSTREAM_STORE    0x00002000
#define SSRVOPT_OVERWRITE           0x00004000
#define SSRVOPT_RESETTOU            0x00008000
#define SSRVOPT_CALLBACKW           0x00010000
#define SSRVOPT_FLAT_DEFAULT_STORE  0x00020000
#define SSRVOPT_PROXYW              0x00040000
#define SSRVOPT_MESSAGE             0x00080000
#define SSRVOPT_SERVICE             0x00100000   // deprecated
#define SSRVOPT_FAVOR_COMPRESSED    0x00200000
#define SSRVOPT_STRING              0x00400000
#define SSRVOPT_WINHTTP             0x00800000
#define SSRVOPT_WININET             0x01000000

#define SSRVOPT_MAX                 0x0100000

#define SSRVOPT_RESET               ((ULONG_PTR)-1)


#define NUM_SSRVOPTS                30

#define SSRVACTION_TRACE        1
#define SSRVACTION_QUERYCANCEL  2
#define SSRVACTION_EVENT        3
#define SSRVACTION_EVENTW       4
#define SSRVACTION_SIZE         5

#define SYMSTOREOPT_COMPRESS        0x01
#define SYMSTOREOPT_OVERWRITE       0x02
#define SYMSTOREOPT_RETURNINDEX     0x04
#define SYMSTOREOPT_POINTER         0x08
#define SYMSTOREOPT_ALT_INDEX       0x10
#define SYMSTOREOPT_UNICODE         0x20
#define SYMSTOREOPT_PASS_IF_EXISTS  0x40

#ifdef DBGHELP_TRANSLATE_TCHAR
#define SymInitialize                     SymInitializeW
#define SymAddSymbol                      SymAddSymbolW
#define SymDeleteSymbol                   SymDeleteSymbolW
#define SearchTreeForFile                 SearchTreeForFileW
#define UnDecorateSymbolName              UnDecorateSymbolNameW
#define SymGetLineFromName64              SymGetLineFromNameW64
#define SymGetLineFromAddr64              SymGetLineFromAddrW64
#define SymGetLineNext64                  SymGetLineNextW64
#define SymGetLinePrev64                  SymGetLinePrevW64
#define SymFromName                       SymFromNameW
#define SymFindExecutableImage            SymFindExecutableImageW
#define FindExecutableImageEx             FindExecutableImageExW
#define SymSearch                         SymSearchW
#define SymEnumLines                      SymEnumLinesW
#define SymEnumSourceLines                SymEnumSourceLinesW
#define SymGetTypeFromName                SymGetTypeFromNameW
#define SymEnumSymbolsForAddr             SymEnumSymbolsForAddrW
#define SymFromAddr                       SymFromAddrW
#define SymMatchString                    SymMatchStringW
#define SymEnumSourceFiles                SymEnumSourceFilesW
#define SymEnumSymbols                    SymEnumSymbolsW
#define SymLoadModuleEx                   SymLoadModuleExW
#define SymSetSearchPath                  SymSetSearchPathW
#define SymGetSearchPath                  SymGetSearchPathW
#define EnumDirTree                       EnumDirTreeW
#define SymFromToken                      SymFromTokenW
#define SymFromIndex                      SymFromIndexW
#define SymGetScope                       SymGetScopeW
#define SymNext                           SymNextW
#define SymPrev                           SymPrevW
#define SymEnumTypes                      SymEnumTypesW
#define SymEnumTypesByName                SymEnumTypesByNameW
#define SymRegisterCallback64             SymRegisterCallbackW64
#define SymFindDebugInfoFile              SymFindDebugInfoFileW
#define FindDebugInfoFileEx               FindDebugInfoFileExW
#define SymFindFileInPath                 SymFindFileInPathW
#define SymEnumerateModules64             SymEnumerateModulesW64
#define SymSetHomeDirectory               SymSetHomeDirectoryW
#define SymGetHomeDirectory               SymGetHomeDirectoryW
#define SymGetSourceFile                  SymGetSourceFileW
#define SymGetSourceFileToken             SymGetSourceFileTokenW
#define SymGetSourceFileFromToken         SymGetSourceFileFromTokenW
#define SymGetSourceVarFromToken          SymGetSourceVarFromTokenW
#define SymGetSourceFileToken             SymGetSourceFileTokenW
#define SymGetFileLineOffsets64           SymGetFileLineOffsetsW64
#define SymFindFileInPath                 SymFindFileInPathW
#define SymMatchFileName                  SymMatchFileNameW
#define SymGetSourceFileFromToken         SymGetSourceFileFromTokenW
#define SymGetSourceVarFromToken          SymGetSourceVarFromTokenW
#define SymGetModuleInfo64                SymGetModuleInfoW64
#define SymSrvIsStore                     SymSrvIsStoreW
#define SymSrvDeltaName                   SymSrvDeltaNameW
#define SymSrvGetSupplement               SymSrvGetSupplementW
#define SymSrvStoreSupplement             SymSrvStoreSupplementW
#define SymSrvGetFileIndexes              SymSrvGetFileIndexes
#define SymSrvGetFileIndexString          SymSrvGetFileIndexStringW
#define SymSrvStoreFile                   SymSrvStoreFileW
#define SymGetSymbolFile                  SymGetSymbolFileW
#define EnumerateLoadedModules64          EnumerateLoadedModulesW64
#define EnumerateLoadedModulesEx          EnumerateLoadedModulesExW
#define SymSrvGetFileIndexInfo            SymSrvGetFileIndexInfoW

#define IMAGEHLP_LINE64                   IMAGEHLP_LINEW64
#define PIMAGEHLP_LINE64                  PIMAGEHLP_LINEW64
#define SYMBOL_INFO                       SYMBOL_INFOW
#define PSYMBOL_INFO                      PSYMBOL_INFOW
#define SYMBOL_INFO_PACKAGE               SYMBOL_INFO_PACKAGEW
#define PSYMBOL_INFO_PACKAGE              PSYMBOL_INFO_PACKAGEW
#define FIND_EXE_FILE_CALLBACK            FIND_EXE_FILE_CALLBACKW
#define PFIND_EXE_FILE_CALLBACK           PFIND_EXE_FILE_CALLBACKW
#define SYM_ENUMERATESYMBOLS_CALLBACK     SYM_ENUMERATESYMBOLS_CALLBACKW
#define PSYM_ENUMERATESYMBOLS_CALLBACK    PSYM_ENUMERATESYMBOLS_CALLBACKW
#define SRCCODEINFO                       SRCCODEINFOW
#define PSRCCODEINFO                      PSRCCODEINFOW
#define SOURCEFILE                        SOURCEFILEW
#define PSOURCEFILE                       PSOURCEFILEW
#define SYM_ENUMSOURECFILES_CALLBACK      SYM_ENUMSOURCEFILES_CALLBACKW
#define PSYM_ENUMSOURCEFILES_CALLBACK     PSYM_ENUMSOURECFILES_CALLBACKW
#define IMAGEHLP_CBA_EVENT                IMAGEHLP_CBA_EVENTW
#define PIMAGEHLP_CBA_EVENT               PIMAGEHLP_CBA_EVENTW
#define PENUMDIRTREE_CALLBACK             PENUMDIRTREE_CALLBACKW
#define IMAGEHLP_DEFERRED_SYMBOL_LOAD64   IMAGEHLP_DEFERRED_SYMBOL_LOADW64
#define PIMAGEHLP_DEFERRED_SYMBOL_LOAD64  PIMAGEHLP_DEFERRED_SYMBOL_LOADW64
#define PFIND_DEBUG_FILE_CALLBACK         PFIND_DEBUG_FILE_CALLBACKW
#define PFINDFILEINPATHCALLBACK           PFINDFILEINPATHCALLBACKW
#define IMAGEHLP_MODULE64                 IMAGEHLP_MODULEW64
#define PIMAGEHLP_MODULE64                PIMAGEHLP_MODULEW64
#define SYMSRV_INDEX_INFO                 SYMSRV_INDEX_INFOW
#define PSYMSRV_INDEX_INFO                PSYMSRV_INDEX_INFOW

#define PSYMBOLSERVERPROC                 PSYMBOLSERVERPROCW
#define PSYMBOLSERVERPINGPROC             PSYMBOLSERVERPINGPROCW
#endif

// -----------------------------------------------------------------
// The following APIs exist only for backwards compatibility
// with a pre-release version documented in an MSDN release.

// You should use SymFindFileInPath if you want to maintain
// future compatibility.

DBHLP_DEPRECIATED
BOOL
IMAGEAPI
FindFileInPath(
    __in HANDLE hprocess,
    __in PCSTR SearchPath,
    __in PCSTR FileName,
    __in PVOID id,
    __in DWORD two,
    __in DWORD three,
    __in DWORD flags,
    __out_ecount(MAX_PATH + 1) PSTR FilePath
);

// You should use SymFindFileInPath if you want to maintain
// future compatibility.

DBHLP_DEPRECIATED
BOOL
IMAGEAPI
FindFileInSearchPath(
    __in HANDLE hprocess,
    __in PCSTR SearchPath,
    __in PCSTR FileName,
    __in DWORD one,
    __in DWORD two,
    __in DWORD three,
    __out_ecount(MAX_PATH + 1) PSTR FilePath
);

DBHLP_DEPRECIATED
BOOL
IMAGEAPI
SymEnumSym(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PSYM_ENUMERATESYMBOLS_CALLBACK EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

DBHLP_DEPRECIATED
BOOL
IMAGEAPI
SymEnumerateSymbols64(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PSYM_ENUMSYMBOLS_CALLBACK64 EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

DBHLP_DEPRECIATED
BOOL
IMAGEAPI
SymEnumerateSymbolsW64(
    __in HANDLE hProcess,
    __in ULONG64 BaseOfDll,
    __in PSYM_ENUMSYMBOLS_CALLBACK64W EnumSymbolsCallback,
    __in_opt PVOID UserContext
);


#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymEnumerateSymbols SymEnumerateSymbols64
#define SymEnumerateSymbolsW SymEnumerateSymbolsW64
#else
DBHLP_DEPRECIATED
BOOL
IMAGEAPI
SymEnumerateSymbols(
    __in HANDLE hProcess,
    __in ULONG BaseOfDll,
    __in PSYM_ENUMSYMBOLS_CALLBACK EnumSymbolsCallback,
    __in_opt PVOID UserContext
);

DBHLP_DEPRECIATED
BOOL
IMAGEAPI
SymEnumerateSymbolsW(
    __in HANDLE hProcess,
    __in ULONG BaseOfDll,
    __in PSYM_ENUMSYMBOLS_CALLBACKW EnumSymbolsCallback,
    __in_opt PVOID UserContext
);
#endif

// use SymLoadModuleEx

DWORD64
IMAGEAPI
SymLoadModule64(
    __in HANDLE hProcess,
    __in_opt HANDLE hFile,
    __in_opt PCSTR ImageName,
    __in_opt PCSTR ModuleName,
    __in DWORD64 BaseOfDll,
    __in DWORD SizeOfDll
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymLoadModule SymLoadModule64
#else
DWORD
IMAGEAPI
SymLoadModule(
    __in HANDLE hProcess,
    __in_opt HANDLE hFile,
    __in_opt PCSTR ImageName,
    __in_opt PCSTR ModuleName,
    __in DWORD BaseOfDll,
    __in DWORD SizeOfDll
);
#endif

BOOL
IMAGEAPI
SymGetSymNext64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOL64 Symbol
);

BOOL
IMAGEAPI
SymGetSymNextW64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOLW64 Symbol
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetSymNext SymGetSymNext64
#define SymGetSymNextW SymGetSymNextW64
#else
BOOL
IMAGEAPI
SymGetSymNext(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOL Symbol
);

BOOL
IMAGEAPI
SymGetSymNextW(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOLW Symbol
);
#endif

BOOL
IMAGEAPI
SymGetSymPrev64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOL64 Symbol
);

BOOL
IMAGEAPI
SymGetSymPrevW64(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOLW64 Symbol
);

#if !defined(_IMAGEHLP_SOURCE_) && defined(_IMAGEHLP64)
#define SymGetSymPrev SymGetSymPrev64
#define SymGetSymPrevW SymGetSymPrevW64
#else
BOOL
IMAGEAPI
SymGetSymPrev(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOL Symbol
);

BOOL
IMAGEAPI
SymGetSymPrevW(
    __in HANDLE hProcess,
    __inout PIMAGEHLP_SYMBOLW Symbol
);
#endif


// These values should not be used.
// They have been replaced by SYMFLAG_ values.

#define SYMF_OMAP_GENERATED   0x00000001
#define SYMF_OMAP_MODIFIED    0x00000002
#define SYMF_REGISTER         0x00000008
#define SYMF_REGREL           0x00000010
#define SYMF_FRAMEREL         0x00000020
#define SYMF_PARAMETER        0x00000040
#define SYMF_LOCAL            0x00000080
#define SYMF_CONSTANT         0x00000100
#define SYMF_EXPORT           0x00000200
#define SYMF_FORWARDER        0x00000400
#define SYMF_FUNCTION         0x00000800
#define SYMF_VIRTUAL          0x00001000
#define SYMF_THUNK            0x00002000
#define SYMF_TLSREL           0x00004000

// These values should also not be used.
// They have been replaced by SYMFLAG_ values.

#define IMAGEHLP_SYMBOL_INFO_VALUEPRESENT          1
#define IMAGEHLP_SYMBOL_INFO_REGISTER              SYMF_REGISTER        // 0x0008
#define IMAGEHLP_SYMBOL_INFO_REGRELATIVE           SYMF_REGREL          // 0x0010
#define IMAGEHLP_SYMBOL_INFO_FRAMERELATIVE         SYMF_FRAMEREL        // 0x0020
#define IMAGEHLP_SYMBOL_INFO_PARAMETER             SYMF_PARAMETER       // 0x0040
#define IMAGEHLP_SYMBOL_INFO_LOCAL                 SYMF_LOCAL           // 0x0080
#define IMAGEHLP_SYMBOL_INFO_CONSTANT              SYMF_CONSTANT        // 0x0100
#define IMAGEHLP_SYMBOL_FUNCTION                   SYMF_FUNCTION        // 0x0800
#define IMAGEHLP_SYMBOL_VIRTUAL                    SYMF_VIRTUAL         // 0x1000
#define IMAGEHLP_SYMBOL_THUNK                      SYMF_THUNK           // 0x2000
#define IMAGEHLP_SYMBOL_INFO_TLSRELATIVE           SYMF_TLSREL          // 0x4000

#pragma pack(pop)


#pragma pack(push,4)

#define MINIDUMP_SIGNATURE ('PMDM')
#define MINIDUMP_VERSION   (42899)
typedef DWORD RVA;
typedef ULONG64 RVA64;

typedef struct _MINIDUMP_LOCATION_DESCRIPTOR
{
    ULONG32 DataSize;
    RVA Rva;
} MINIDUMP_LOCATION_DESCRIPTOR;

typedef struct _MINIDUMP_LOCATION_DESCRIPTOR64
{
    ULONG64 DataSize;
    RVA64 Rva;
} MINIDUMP_LOCATION_DESCRIPTOR64;


typedef struct _MINIDUMP_MEMORY_DESCRIPTOR
{
    ULONG64 StartOfMemoryRange;
    MINIDUMP_LOCATION_DESCRIPTOR Memory;
} MINIDUMP_MEMORY_DESCRIPTOR, *PMINIDUMP_MEMORY_DESCRIPTOR;

// DESCRIPTOR64 is used for full-memory minidumps where
// all of the raw memory is laid out sequentially at the
// end of the dump.  There is no need for individual RVAs
// as the RVA is the base RVA plus the sum of the preceeding
// data blocks.
typedef struct _MINIDUMP_MEMORY_DESCRIPTOR64
{
    ULONG64 StartOfMemoryRange;
    ULONG64 DataSize;
} MINIDUMP_MEMORY_DESCRIPTOR64, *PMINIDUMP_MEMORY_DESCRIPTOR64;


typedef struct _MINIDUMP_HEADER
{
    ULONG32 Signature;
    ULONG32 Version;
    ULONG32 NumberOfStreams;
    RVA StreamDirectoryRva;
    ULONG32 CheckSum;
    union
    {
        ULONG32 Reserved;
        ULONG32 TimeDateStamp;
    };
    ULONG64 Flags;
} MINIDUMP_HEADER, *PMINIDUMP_HEADER;

//
// The MINIDUMP_HEADER field StreamDirectoryRva points to
// an array of MINIDUMP_DIRECTORY structures.
//

typedef struct _MINIDUMP_DIRECTORY
{
    ULONG32 StreamType;
    MINIDUMP_LOCATION_DESCRIPTOR Location;
} MINIDUMP_DIRECTORY, *PMINIDUMP_DIRECTORY;


typedef struct _MINIDUMP_STRING
{
    ULONG32 Length;         // Length in bytes of the string
    WCHAR   Buffer [0];     // Variable size buffer
} MINIDUMP_STRING, *PMINIDUMP_STRING;



//
// The MINIDUMP_DIRECTORY field StreamType may be one of the following types.
// Types will be added in the future, so if a program reading the minidump
// header encounters a stream type it does not understand it should ignore
// the data altogether. Any tag above LastReservedStream will not be used by
// the system and is reserved for program-specific information.
//

typedef enum _MINIDUMP_STREAM_TYPE
{

    UnusedStream                = 0,
    ReservedStream0             = 1,
    ReservedStream1             = 2,
    ThreadListStream            = 3,
    ModuleListStream            = 4,
    MemoryListStream            = 5,
    ExceptionStream             = 6,
    SystemInfoStream            = 7,
    ThreadExListStream          = 8,
    Memory64ListStream          = 9,
    CommentStreamA              = 10,
    CommentStreamW              = 11,
    HandleDataStream            = 12,
    FunctionTableStream         = 13,
    UnloadedModuleListStream    = 14,
    MiscInfoStream              = 15,
    MemoryInfoListStream        = 16,
    ThreadInfoListStream        = 17,
    HandleOperationListStream   = 18,
    TokenStream                 = 19,

    ceStreamNull                = 0x8000,
    ceStreamSystemInfo          = 0x8001,
    ceStreamException           = 0x8002,
    ceStreamModuleList          = 0x8003,
    ceStreamProcessList         = 0x8004,
    ceStreamThreadList          = 0x8005,
    ceStreamThreadContextList   = 0x8006,
    ceStreamThreadCallStackList = 0x8007,
    ceStreamMemoryVirtualList   = 0x8008,
    ceStreamMemoryPhysicalList  = 0x8009,
    ceStreamBucketParameters    = 0x800A,
    ceStreamProcessModuleMap    = 0x800B,
    ceStreamDiagnosisList       = 0x800C,

    LastReservedStream          = 0xffff

} MINIDUMP_STREAM_TYPE;


//
// The minidump system information contains processor and
// Operating System specific information.
//

//
// CPU information is obtained from one of two places.
//
//  1) On x86 computers, CPU_INFORMATION is obtained from the CPUID
//     instruction. You must use the X86 portion of the union for X86
//     computers.
//
//  2) On non-x86 architectures, CPU_INFORMATION is obtained by calling
//     IsProcessorFeatureSupported().
//

typedef union _CPU_INFORMATION
{

    //
    // X86 platforms use CPUID function to obtain processor information.
    //

    struct
    {

        //
        // CPUID Subfunction 0, register EAX (VendorId [0]),
        // EBX (VendorId [1]) and ECX (VendorId [2]).
        //

        ULONG32 VendorId [ 3 ];

        //
        // CPUID Subfunction 1, register EAX
        //

        ULONG32 VersionInformation;

        //
        // CPUID Subfunction 1, register EDX
        //

        ULONG32 FeatureInformation;


        //
        // CPUID, Subfunction 80000001, register EBX. This will only
        // be obtained if the vendor id is "AuthenticAMD".
        //

        ULONG32 AMDExtendedCpuFeatures;

    } X86CpuInfo;

    //
    // Non-x86 platforms use processor feature flags.
    //

    struct
    {

        ULONG64 ProcessorFeatures [ 2 ];

    } OtherCpuInfo;

} CPU_INFORMATION, *PCPU_INFORMATION;

typedef struct _MINIDUMP_SYSTEM_INFO
{

    //
    // ProcessorArchitecture, ProcessorLevel and ProcessorRevision are all
    // taken from the SYSTEM_INFO structure obtained by GetSystemInfo( ).
    //

    USHORT ProcessorArchitecture;
    USHORT ProcessorLevel;
    USHORT ProcessorRevision;

    union
    {
        USHORT Reserved0;
        struct
        {
            UCHAR NumberOfProcessors;
            UCHAR ProductType;
        };
    };

    //
    // MajorVersion, MinorVersion, BuildNumber, PlatformId and
    // CSDVersion are all taken from the OSVERSIONINFO structure
    // returned by GetVersionEx( ).
    //

    ULONG32 MajorVersion;
    ULONG32 MinorVersion;
    ULONG32 BuildNumber;
    ULONG32 PlatformId;

    //
    // RVA to a CSDVersion string in the string table.
    //

    RVA CSDVersionRva;

    union
    {
        ULONG32 Reserved1;
        struct
        {
            USHORT SuiteMask;
            USHORT Reserved2;
        };
    };

    CPU_INFORMATION Cpu;

} MINIDUMP_SYSTEM_INFO, *PMINIDUMP_SYSTEM_INFO;


//
// The minidump thread contains standard thread
// information plus an RVA to the memory for this
// thread and an RVA to the CONTEXT structure for
// this thread.
//


//
// ThreadId must be 4 bytes on all architectures.
//

C_ASSERT(sizeof(((PPROCESS_INFORMATION)0)->dwThreadId) == 4);

typedef struct _MINIDUMP_THREAD
{
    ULONG32 ThreadId;
    ULONG32 SuspendCount;
    ULONG32 PriorityClass;
    ULONG32 Priority;
    ULONG64 Teb;
    MINIDUMP_MEMORY_DESCRIPTOR Stack;
    MINIDUMP_LOCATION_DESCRIPTOR ThreadContext;
} MINIDUMP_THREAD, *PMINIDUMP_THREAD;

//
// The thread list is a container of threads.
//

typedef struct _MINIDUMP_THREAD_LIST
{
    ULONG32 NumberOfThreads;
    MINIDUMP_THREAD Threads [0];
} MINIDUMP_THREAD_LIST, *PMINIDUMP_THREAD_LIST;


typedef struct _MINIDUMP_THREAD_EX
{
    ULONG32 ThreadId;
    ULONG32 SuspendCount;
    ULONG32 PriorityClass;
    ULONG32 Priority;
    ULONG64 Teb;
    MINIDUMP_MEMORY_DESCRIPTOR Stack;
    MINIDUMP_LOCATION_DESCRIPTOR ThreadContext;
    MINIDUMP_MEMORY_DESCRIPTOR BackingStore;
} MINIDUMP_THREAD_EX, *PMINIDUMP_THREAD_EX;

//
// The thread list is a container of threads.
//

typedef struct _MINIDUMP_THREAD_EX_LIST
{
    ULONG32 NumberOfThreads;
    MINIDUMP_THREAD_EX Threads [0];
} MINIDUMP_THREAD_EX_LIST, *PMINIDUMP_THREAD_EX_LIST;


//
// The MINIDUMP_EXCEPTION is the same as EXCEPTION on Win64.
//

typedef struct _MINIDUMP_EXCEPTION
{
    ULONG32 ExceptionCode;
    ULONG32 ExceptionFlags;
    ULONG64 ExceptionRecord;
    ULONG64 ExceptionAddress;
    ULONG32 NumberParameters;
    ULONG32 __unusedAlignment;
    ULONG64 ExceptionInformation [ EXCEPTION_MAXIMUM_PARAMETERS ];
} MINIDUMP_EXCEPTION, *PMINIDUMP_EXCEPTION;


//
// The exception information stream contains the id of the thread that caused
// the exception (ThreadId), the exception record for the exception
// (ExceptionRecord) and an RVA to the thread context where the exception
// occured.
//

typedef struct MINIDUMP_EXCEPTION_STREAM
{
    ULONG32 ThreadId;
    ULONG32  __alignment;
    MINIDUMP_EXCEPTION ExceptionRecord;
    MINIDUMP_LOCATION_DESCRIPTOR ThreadContext;
} MINIDUMP_EXCEPTION_STREAM, *PMINIDUMP_EXCEPTION_STREAM;


//
// The MINIDUMP_MODULE contains information about a
// a specific module. It includes the CheckSum and
// the TimeDateStamp for the module so the module
// can be reloaded during the analysis phase.
//

typedef struct _MINIDUMP_MODULE
{
    ULONG64 BaseOfImage;
    ULONG32 SizeOfImage;
    ULONG32 CheckSum;
    ULONG32 TimeDateStamp;
    RVA ModuleNameRva;
    VS_FIXEDFILEINFO VersionInfo;
    MINIDUMP_LOCATION_DESCRIPTOR CvRecord;
    MINIDUMP_LOCATION_DESCRIPTOR MiscRecord;
    ULONG64 Reserved0;                          // Reserved for future use.
    ULONG64 Reserved1;                          // Reserved for future use.
} MINIDUMP_MODULE, *PMINIDUMP_MODULE;


//
// The minidump module list is a container for modules.
//

typedef struct _MINIDUMP_MODULE_LIST
{
    ULONG32 NumberOfModules;
    MINIDUMP_MODULE Modules [ 0 ];
} MINIDUMP_MODULE_LIST, *PMINIDUMP_MODULE_LIST;


//
// Memory Ranges
//

typedef struct _MINIDUMP_MEMORY_LIST
{
    ULONG32 NumberOfMemoryRanges;
    MINIDUMP_MEMORY_DESCRIPTOR MemoryRanges [0];
} MINIDUMP_MEMORY_LIST, *PMINIDUMP_MEMORY_LIST;

typedef struct _MINIDUMP_MEMORY64_LIST
{
    ULONG64 NumberOfMemoryRanges;
    RVA64 BaseRva;
    MINIDUMP_MEMORY_DESCRIPTOR64 MemoryRanges [0];
} MINIDUMP_MEMORY64_LIST, *PMINIDUMP_MEMORY64_LIST;


//
// Support for user supplied exception information.
//

typedef struct _MINIDUMP_EXCEPTION_INFORMATION
{
    DWORD ThreadId;
    PEXCEPTION_POINTERS ExceptionPointers;
    BOOL ClientPointers;
} MINIDUMP_EXCEPTION_INFORMATION, *PMINIDUMP_EXCEPTION_INFORMATION;

typedef struct _MINIDUMP_EXCEPTION_INFORMATION64
{
    DWORD ThreadId;
    ULONG64 ExceptionRecord;
    ULONG64 ContextRecord;
    BOOL ClientPointers;
} MINIDUMP_EXCEPTION_INFORMATION64, *PMINIDUMP_EXCEPTION_INFORMATION64;


//
// Support for capturing system handle state at the time of the dump.
//

// Per-handle object information varies according to
// the OS, the OS version, the processor type and
// so on.  The minidump gives a minidump identifier
// to each possible data format for identification
// purposes but does not control nor describe the actual data.
typedef enum _MINIDUMP_HANDLE_OBJECT_INFORMATION_TYPE
{
    MiniHandleObjectInformationNone,
    MiniThreadInformation1,
    MiniMutantInformation1,
    MiniMutantInformation2,
    MiniProcessInformation1,
    MiniProcessInformation2,
    MiniHandleObjectInformationTypeMax
} MINIDUMP_HANDLE_OBJECT_INFORMATION_TYPE;

typedef struct _MINIDUMP_HANDLE_OBJECT_INFORMATION
{
    RVA NextInfoRva;
    ULONG32 InfoType;
    ULONG32 SizeOfInfo;
    // Raw information follows.
} MINIDUMP_HANDLE_OBJECT_INFORMATION;

typedef struct _MINIDUMP_HANDLE_DESCRIPTOR
{
    ULONG64 Handle;
    RVA TypeNameRva;
    RVA ObjectNameRva;
    ULONG32 Attributes;
    ULONG32 GrantedAccess;
    ULONG32 HandleCount;
    ULONG32 PointerCount;
} MINIDUMP_HANDLE_DESCRIPTOR, *PMINIDUMP_HANDLE_DESCRIPTOR;

typedef struct _MINIDUMP_HANDLE_DESCRIPTOR_2
{
    ULONG64 Handle;
    RVA TypeNameRva;
    RVA ObjectNameRva;
    ULONG32 Attributes;
    ULONG32 GrantedAccess;
    ULONG32 HandleCount;
    ULONG32 PointerCount;
    RVA ObjectInfoRva;
    ULONG32 Reserved0;
} MINIDUMP_HANDLE_DESCRIPTOR_2, *PMINIDUMP_HANDLE_DESCRIPTOR_2;

// The latest MINIDUMP_HANDLE_DESCRIPTOR definition.
typedef MINIDUMP_HANDLE_DESCRIPTOR_2 MINIDUMP_HANDLE_DESCRIPTOR_N;
typedef MINIDUMP_HANDLE_DESCRIPTOR_N* PMINIDUMP_HANDLE_DESCRIPTOR_N;

typedef struct _MINIDUMP_HANDLE_DATA_STREAM
{
    ULONG32 SizeOfHeader;
    ULONG32 SizeOfDescriptor;
    ULONG32 NumberOfDescriptors;
    ULONG32 Reserved;
} MINIDUMP_HANDLE_DATA_STREAM, *PMINIDUMP_HANDLE_DATA_STREAM;

// Some operating systems can track the last operations
// performed on a handle.  For example, Application Verifier
// can enable this for some versions of Windows.  The
// handle operation list collects handle operations
// known for the dump target.
// Each entry is an AVRF_HANDLE_OPERATION.
typedef struct _MINIDUMP_HANDLE_OPERATION_LIST
{
    ULONG32 SizeOfHeader;
    ULONG32 SizeOfEntry;
    ULONG32 NumberOfEntries;
    ULONG32 Reserved;
} MINIDUMP_HANDLE_OPERATION_LIST, *PMINIDUMP_HANDLE_OPERATION_LIST;


//
// Support for capturing dynamic function table state at the time of the dump.
//

typedef struct _MINIDUMP_FUNCTION_TABLE_DESCRIPTOR
{
    ULONG64 MinimumAddress;
    ULONG64 MaximumAddress;
    ULONG64 BaseAddress;
    ULONG32 EntryCount;
    ULONG32 SizeOfAlignPad;
} MINIDUMP_FUNCTION_TABLE_DESCRIPTOR, *PMINIDUMP_FUNCTION_TABLE_DESCRIPTOR;

typedef struct _MINIDUMP_FUNCTION_TABLE_STREAM
{
    ULONG32 SizeOfHeader;
    ULONG32 SizeOfDescriptor;
    ULONG32 SizeOfNativeDescriptor;
    ULONG32 SizeOfFunctionEntry;
    ULONG32 NumberOfDescriptors;
    ULONG32 SizeOfAlignPad;
} MINIDUMP_FUNCTION_TABLE_STREAM, *PMINIDUMP_FUNCTION_TABLE_STREAM;


//
// The MINIDUMP_UNLOADED_MODULE contains information about a
// a specific module that was previously loaded but no
// longer is.  This can help with diagnosing problems where
// callers attempt to call code that is no longer loaded.
//

typedef struct _MINIDUMP_UNLOADED_MODULE
{
    ULONG64 BaseOfImage;
    ULONG32 SizeOfImage;
    ULONG32 CheckSum;
    ULONG32 TimeDateStamp;
    RVA ModuleNameRva;
} MINIDUMP_UNLOADED_MODULE, *PMINIDUMP_UNLOADED_MODULE;


//
// The minidump unloaded module list is a container for unloaded modules.
//

typedef struct _MINIDUMP_UNLOADED_MODULE_LIST
{
    ULONG32 SizeOfHeader;
    ULONG32 SizeOfEntry;
    ULONG32 NumberOfEntries;
} MINIDUMP_UNLOADED_MODULE_LIST, *PMINIDUMP_UNLOADED_MODULE_LIST;


//
// The miscellaneous information stream contains a variety
// of small pieces of information.  A member is valid if
// it's within the available size and its corresponding
// bit is set.
//

#define MINIDUMP_MISC1_PROCESS_ID            0x00000001
#define MINIDUMP_MISC1_PROCESS_TIMES         0x00000002
#define MINIDUMP_MISC1_PROCESSOR_POWER_INFO  0x00000004
#define MINIDUMP_MISC3_PROCESS_INTEGRITY     0x00000010
#define MINIDUMP_MISC3_PROCESS_EXECUTE_FLAGS 0x00000020
#define MINIDUMP_MISC3_TIMEZONE              0x00000040
#define MINIDUMP_MISC3_PROTECTED_PROCESS     0x00000080

typedef struct _MINIDUMP_MISC_INFO
{
    ULONG32 SizeOfInfo;
    ULONG32 Flags1;
    ULONG32 ProcessId;
    ULONG32 ProcessCreateTime;
    ULONG32 ProcessUserTime;
    ULONG32 ProcessKernelTime;
} MINIDUMP_MISC_INFO, *PMINIDUMP_MISC_INFO;

typedef struct _MINIDUMP_MISC_INFO_2
{
    ULONG32 SizeOfInfo;
    ULONG32 Flags1;
    ULONG32 ProcessId;
    ULONG32 ProcessCreateTime;
    ULONG32 ProcessUserTime;
    ULONG32 ProcessKernelTime;
    ULONG32 ProcessorMaxMhz;
    ULONG32 ProcessorCurrentMhz;
    ULONG32 ProcessorMhzLimit;
    ULONG32 ProcessorMaxIdleState;
    ULONG32 ProcessorCurrentIdleState;
} MINIDUMP_MISC_INFO_2, *PMINIDUMP_MISC_INFO_2;

typedef struct _MINIDUMP_MISC_INFO_3
{
    ULONG32 SizeOfInfo;
    ULONG32 Flags1;
    ULONG32 ProcessId;
    ULONG32 ProcessCreateTime;
    ULONG32 ProcessUserTime;
    ULONG32 ProcessKernelTime;
    ULONG32 ProcessorMaxMhz;
    ULONG32 ProcessorCurrentMhz;
    ULONG32 ProcessorMhzLimit;
    ULONG32 ProcessorMaxIdleState;
    ULONG32 ProcessorCurrentIdleState;
    ULONG32 ProcessIntegrityLevel;
    ULONG32 ProcessExecuteFlags;
    ULONG32 ProtectedProcess;
    ULONG32 TimeZoneId;
    TIME_ZONE_INFORMATION TimeZone;
} MINIDUMP_MISC_INFO_3, *PMINIDUMP_MISC_INFO_3;

// The latest MINIDUMP_MISC_INFO definition.
typedef MINIDUMP_MISC_INFO_3 MINIDUMP_MISC_INFO_N;
typedef MINIDUMP_MISC_INFO_N* PMINIDUMP_MISC_INFO_N;


//
// The memory information stream contains memory region
// description information.  This stream corresponds to
// what VirtualQuery would return for the process the
// dump was created for.
//

typedef struct _MINIDUMP_MEMORY_INFO
{
    ULONG64 BaseAddress;
    ULONG64 AllocationBase;
    ULONG32 AllocationProtect;
    ULONG32 __alignment1;
    ULONG64 RegionSize;
    ULONG32 State;
    ULONG32 Protect;
    ULONG32 Type;
    ULONG32 __alignment2;
} MINIDUMP_MEMORY_INFO, *PMINIDUMP_MEMORY_INFO;

typedef struct _MINIDUMP_MEMORY_INFO_LIST
{
    ULONG SizeOfHeader;
    ULONG SizeOfEntry;
    ULONG64 NumberOfEntries;
} MINIDUMP_MEMORY_INFO_LIST, *PMINIDUMP_MEMORY_INFO_LIST;


//
// The memory information stream contains memory region
// description information.  This stream corresponds to
// what VirtualQuery would return for the process the
// dump was created for.
//

// Thread dump writer status flags.
#define MINIDUMP_THREAD_INFO_ERROR_THREAD    0x00000001
#define MINIDUMP_THREAD_INFO_WRITING_THREAD  0x00000002
#define MINIDUMP_THREAD_INFO_EXITED_THREAD   0x00000004
#define MINIDUMP_THREAD_INFO_INVALID_INFO    0x00000008
#define MINIDUMP_THREAD_INFO_INVALID_CONTEXT 0x00000010
#define MINIDUMP_THREAD_INFO_INVALID_TEB     0x00000020

typedef struct _MINIDUMP_THREAD_INFO
{
    ULONG32 ThreadId;
    ULONG32 DumpFlags;
    ULONG32 DumpError;
    ULONG32 ExitStatus;
    ULONG64 CreateTime;
    ULONG64 ExitTime;
    ULONG64 KernelTime;
    ULONG64 UserTime;
    ULONG64 StartAddress;
    ULONG64 Affinity;
} MINIDUMP_THREAD_INFO, *PMINIDUMP_THREAD_INFO;

typedef struct _MINIDUMP_THREAD_INFO_LIST
{
    ULONG SizeOfHeader;
    ULONG SizeOfEntry;
    ULONG NumberOfEntries;
} MINIDUMP_THREAD_INFO_LIST, *PMINIDUMP_THREAD_INFO_LIST;

//
// Support for token information.
//
typedef struct _MINIDUMP_TOKEN_INFO_HEADER
{
    ULONG   TokenSize;   // The size of the token structure.
    ULONG   TokenId;     // The PID in NtOpenProcessToken() call or TID in NtOpenThreadToken() call.
    ULONG64 TokenHandle; // The handle value returned.
} MINIDUMP_TOKEN_INFO_HEADER, *PMINIDUMP_TOKEN_INFO_HEADER;

typedef struct _MINIDUMP_TOKEN_INFO_LIST
{
    ULONG TokenListSize;
    ULONG TokenListEntries;
    ULONG ListHeaderSize;
    ULONG ElementHeaderSize;
} MINIDUMP_TOKEN_INFO_LIST, *PMINIDUMP_TOKEN_INFO_LIST;

//
// Support for arbitrary user-defined information.
//

typedef struct _MINIDUMP_USER_RECORD
{
    ULONG32 Type;
    MINIDUMP_LOCATION_DESCRIPTOR Memory;
} MINIDUMP_USER_RECORD, *PMINIDUMP_USER_RECORD;


typedef struct _MINIDUMP_USER_STREAM
{
    ULONG32 Type;
    ULONG BufferSize;
    PVOID Buffer;

} MINIDUMP_USER_STREAM, *PMINIDUMP_USER_STREAM;


typedef struct _MINIDUMP_USER_STREAM_INFORMATION
{
    ULONG UserStreamCount;
    PMINIDUMP_USER_STREAM UserStreamArray;
} MINIDUMP_USER_STREAM_INFORMATION, *PMINIDUMP_USER_STREAM_INFORMATION;

//
// Callback support.
//

typedef enum _MINIDUMP_CALLBACK_TYPE
{
    ModuleCallback,
    ThreadCallback,
    ThreadExCallback,
    IncludeThreadCallback,
    IncludeModuleCallback,
    MemoryCallback,
    CancelCallback,
    WriteKernelMinidumpCallback,
    KernelMinidumpStatusCallback,
    RemoveMemoryCallback,
    IncludeVmRegionCallback,
    IoStartCallback,
    IoWriteAllCallback,
    IoFinishCallback,
    ReadMemoryFailureCallback,
    SecondaryFlagsCallback,
} MINIDUMP_CALLBACK_TYPE;


typedef struct _MINIDUMP_THREAD_CALLBACK
{
    ULONG ThreadId;
    HANDLE ThreadHandle;
    CONTEXT Context;
    ULONG SizeOfContext;
    ULONG64 StackBase;
    ULONG64 StackEnd;
} MINIDUMP_THREAD_CALLBACK, *PMINIDUMP_THREAD_CALLBACK;


typedef struct _MINIDUMP_THREAD_EX_CALLBACK
{
    ULONG ThreadId;
    HANDLE ThreadHandle;
    CONTEXT Context;
    ULONG SizeOfContext;
    ULONG64 StackBase;
    ULONG64 StackEnd;
    ULONG64 BackingStoreBase;
    ULONG64 BackingStoreEnd;
} MINIDUMP_THREAD_EX_CALLBACK, *PMINIDUMP_THREAD_EX_CALLBACK;


typedef struct _MINIDUMP_INCLUDE_THREAD_CALLBACK
{
    ULONG ThreadId;
} MINIDUMP_INCLUDE_THREAD_CALLBACK, *PMINIDUMP_INCLUDE_THREAD_CALLBACK;


typedef enum _THREAD_WRITE_FLAGS
{
    ThreadWriteThread            = 0x0001,
    ThreadWriteStack             = 0x0002,
    ThreadWriteContext           = 0x0004,
    ThreadWriteBackingStore      = 0x0008,
    ThreadWriteInstructionWindow = 0x0010,
    ThreadWriteThreadData        = 0x0020,
    ThreadWriteThreadInfo        = 0x0040,
} THREAD_WRITE_FLAGS;

typedef struct _MINIDUMP_MODULE_CALLBACK
{
    PWCHAR FullPath;
    ULONG64 BaseOfImage;
    ULONG SizeOfImage;
    ULONG CheckSum;
    ULONG TimeDateStamp;
    VS_FIXEDFILEINFO VersionInfo;
    PVOID CvRecord;
    ULONG SizeOfCvRecord;
    PVOID MiscRecord;
    ULONG SizeOfMiscRecord;
} MINIDUMP_MODULE_CALLBACK, *PMINIDUMP_MODULE_CALLBACK;


typedef struct _MINIDUMP_INCLUDE_MODULE_CALLBACK
{
    ULONG64 BaseOfImage;
} MINIDUMP_INCLUDE_MODULE_CALLBACK, *PMINIDUMP_INCLUDE_MODULE_CALLBACK;


typedef enum _MODULE_WRITE_FLAGS
{
    ModuleWriteModule        = 0x0001,
    ModuleWriteDataSeg       = 0x0002,
    ModuleWriteMiscRecord    = 0x0004,
    ModuleWriteCvRecord      = 0x0008,
    ModuleReferencedByMemory = 0x0010,
    ModuleWriteTlsData       = 0x0020,
    ModuleWriteCodeSegs      = 0x0040,
} MODULE_WRITE_FLAGS;


typedef struct _MINIDUMP_IO_CALLBACK
{
    HANDLE Handle;
    ULONG64 Offset;
    PVOID Buffer;
    ULONG BufferBytes;
} MINIDUMP_IO_CALLBACK, *PMINIDUMP_IO_CALLBACK;


typedef struct _MINIDUMP_READ_MEMORY_FAILURE_CALLBACK
{
    ULONG64 Offset;
    ULONG Bytes;
    HRESULT FailureStatus;
} MINIDUMP_READ_MEMORY_FAILURE_CALLBACK,
*PMINIDUMP_READ_MEMORY_FAILURE_CALLBACK;


typedef struct _MINIDUMP_CALLBACK_INPUT
{
    ULONG ProcessId;
    HANDLE ProcessHandle;
    ULONG CallbackType;
    union
    {
        HRESULT Status;
        MINIDUMP_THREAD_CALLBACK Thread;
        MINIDUMP_THREAD_EX_CALLBACK ThreadEx;
        MINIDUMP_MODULE_CALLBACK Module;
        MINIDUMP_INCLUDE_THREAD_CALLBACK IncludeThread;
        MINIDUMP_INCLUDE_MODULE_CALLBACK IncludeModule;
        MINIDUMP_IO_CALLBACK Io;
        MINIDUMP_READ_MEMORY_FAILURE_CALLBACK ReadMemoryFailure;
        ULONG SecondaryFlags;
    };
} MINIDUMP_CALLBACK_INPUT, *PMINIDUMP_CALLBACK_INPUT;

typedef struct _MINIDUMP_CALLBACK_OUTPUT
{
    union
    {
        ULONG ModuleWriteFlags;
        ULONG ThreadWriteFlags;
        ULONG SecondaryFlags;
        struct
        {
            ULONG64 MemoryBase;
            ULONG MemorySize;
        };
        struct
        {
            BOOL CheckCancel;
            BOOL Cancel;
        };
        HANDLE Handle;
        struct
        {
            MINIDUMP_MEMORY_INFO VmRegion;
            BOOL Continue;
        };
        HRESULT Status;
    };
} MINIDUMP_CALLBACK_OUTPUT, *PMINIDUMP_CALLBACK_OUTPUT;


//
// A normal minidump contains just the information
// necessary to capture stack traces for all of the
// existing threads in a process.
//
// A minidump with data segments includes all of the data
// sections from loaded modules in order to capture
// global variable contents.  This can make the dump much
// larger if many modules have global data.
//
// A minidump with full memory includes all of the accessible
// memory in the process and can be very large.  A minidump
// with full memory always has the raw memory data at the end
// of the dump so that the initial structures in the dump can
// be mapped directly without having to include the raw
// memory information.
//
// Stack and backing store memory can be filtered to remove
// data unnecessary for stack walking.  This can improve
// compression of stacks and also deletes data that may
// be private and should not be stored in a dump.
// Memory can also be scanned to see what modules are
// referenced by stack and backing store memory to allow
// omission of other modules to reduce dump size.
// In either of these modes the ModuleReferencedByMemory flag
// is set for all modules referenced before the base
// module callbacks occur.
//
// On some operating systems a list of modules that were
// recently unloaded is kept in addition to the currently
// loaded module list.  This information can be saved in
// the dump if desired.
//
// Stack and backing store memory can be scanned for referenced
// pages in order to pick up data referenced by locals or other
// stack memory.  This can increase the size of a dump significantly.
//
// Module paths may contain undesired information such as user names
// or other important directory names so they can be stripped.  This
// option reduces the ability to locate the proper image later
// and should only be used in certain situations.
//
// Complete operating system per-process and per-thread information can
// be gathered and stored in the dump.
//
// The virtual address space can be scanned for various types
// of memory to be included in the dump.
//
// Code which is concerned with potentially private information
// getting into the minidump can set a flag that automatically
// modifies all existing and future flags to avoid placing
// unnecessary data in the dump.  Basic data, such as stack
// information, will still be included but optional data, such
// as indirect memory, will not.
//
// When doing a full memory dump it's possible to store all
// of the enumerated memory region descriptive information
// in a memory information stream.
//
// Additional thread information beyond the basic thread
// structure can be collected if desired.
//
// A minidump with code segments includes all of the code
// and code-related sections from loaded modules in order
// to capture executable content.
//
// MiniDumpWithoutAuxiliaryState turns off any secondary,
// auxiliary-supported memory gathering.
//
// MiniDumpWithFullAuxiliaryState asks any present auxiliary
// data providers to include all of their state in the dump.
// The exact set of what is provided depends on the auxiliary.
// This can be quite large.
//

typedef enum _MINIDUMP_TYPE
{
    MiniDumpNormal                         = 0x00000000,
    MiniDumpWithDataSegs                   = 0x00000001,
    MiniDumpWithFullMemory                 = 0x00000002,
    MiniDumpWithHandleData                 = 0x00000004,
    MiniDumpFilterMemory                   = 0x00000008,
    MiniDumpScanMemory                     = 0x00000010,
    MiniDumpWithUnloadedModules            = 0x00000020,
    MiniDumpWithIndirectlyReferencedMemory = 0x00000040,
    MiniDumpFilterModulePaths              = 0x00000080,
    MiniDumpWithProcessThreadData          = 0x00000100,
    MiniDumpWithPrivateReadWriteMemory     = 0x00000200,
    MiniDumpWithoutOptionalData            = 0x00000400,
    MiniDumpWithFullMemoryInfo             = 0x00000800,
    MiniDumpWithThreadInfo                 = 0x00001000,
    MiniDumpWithCodeSegs                   = 0x00002000,
    MiniDumpWithoutAuxiliaryState          = 0x00004000,
    MiniDumpWithFullAuxiliaryState         = 0x00008000,
    MiniDumpWithPrivateWriteCopyMemory     = 0x00010000,
    MiniDumpIgnoreInaccessibleMemory       = 0x00020000,
    MiniDumpWithTokenInformation           = 0x00040000,
    MiniDumpValidTypeFlags                 = 0x0007ffff,
} MINIDUMP_TYPE;

//
// In addition to the primary flags provided to
// MiniDumpWriteDump there are additional, less
// frequently used options queried via the secondary
// flags callback.
//
// MiniSecondaryWithoutPowerInfo suppresses the minidump
// query that retrieves processor power information for
// MINIDUMP_MISC_INFO.
//

typedef enum _MINIDUMP_SECONDARY_FLAGS
{
    MiniSecondaryWithoutPowerInfo = 0x00000001,

    MiniSecondaryValidFlags       = 0x00000001,
} MINIDUMP_SECONDARY_FLAGS;


//
// The minidump callback should modify the FieldsToWrite parameter to reflect
// what portions of the specified thread or module should be written to the
// file.
//

typedef
BOOL
(WINAPI* MINIDUMP_CALLBACK_ROUTINE)(
    __inout PVOID CallbackParam,
    __in    PMINIDUMP_CALLBACK_INPUT CallbackInput,
    __inout PMINIDUMP_CALLBACK_OUTPUT CallbackOutput
);

typedef struct _MINIDUMP_CALLBACK_INFORMATION
{
    MINIDUMP_CALLBACK_ROUTINE CallbackRoutine;
    PVOID CallbackParam;
} MINIDUMP_CALLBACK_INFORMATION, *PMINIDUMP_CALLBACK_INFORMATION;



//++
//
// PVOID
// RVA_TO_ADDR(
//     PVOID Mapping,
//     ULONG Rva
//     )
//
// Routine Description:
//
//     Map an RVA that is contained within a mapped file to it's associated
//     flat address.
//
// Arguments:
//
//     Mapping - Base address of mapped file containing the RVA.
//
//     Rva - An Rva to fixup.
//
// Return Values:
//
//     A pointer to the desired data.
//
//--

#define RVA_TO_ADDR(Mapping,Rva) ((PVOID)(((ULONG_PTR) (Mapping)) + (Rva)))

BOOL
WINAPI
MiniDumpWriteDump(
    __in HANDLE hProcess,
    __in DWORD ProcessId,
    __in HANDLE hFile,
    __in MINIDUMP_TYPE DumpType,
    __in_opt PMINIDUMP_EXCEPTION_INFORMATION ExceptionParam,
    __in_opt PMINIDUMP_USER_STREAM_INFORMATION UserStreamParam,
    __in_opt PMINIDUMP_CALLBACK_INFORMATION CallbackParam
);

BOOL
WINAPI
MiniDumpReadDumpStream(
    __in PVOID BaseOfDump,
    __in ULONG StreamNumber,
    __deref_out_opt PMINIDUMP_DIRECTORY* Dir,
    __deref_out_opt PVOID* StreamPointer,
    __out_opt ULONG* StreamSize
);

#pragma pack(pop)

#ifdef __cplusplus
}
#endif


#endif // _DBGHELP_


```

`CodeCleaner/pluginsdk/jansson/jansson.h`:

```h
/*
 * Copyright (c) 2009-2016 Petri Lehtinen <petri@digip.org>
 *
 * Jansson is free software; you can redistribute it and/or modify
 * it under the terms of the MIT license. See LICENSE for details.
 */

#ifndef JANSSON_H
#define JANSSON_H

#include <stdio.h>
#include <stdlib.h>  /* for size_t */
#include <stdarg.h>

#include "jansson_config.h"

#ifdef __cplusplus
extern "C" {
#endif

/* version */

#define JANSSON_MAJOR_VERSION  2
#define JANSSON_MINOR_VERSION  9
#define JANSSON_MICRO_VERSION  0

/* Micro version is omitted if it's 0 */
#define JANSSON_VERSION  "2.9"

/* Version as a 3-byte hex number, e.g. 0x010201 == 1.2.1. Use this
   for numeric comparisons, e.g. #if JANSSON_VERSION_HEX >= ... */
#define JANSSON_VERSION_HEX  ((JANSSON_MAJOR_VERSION << 16) |   \
                              (JANSSON_MINOR_VERSION << 8)  |   \
                              (JANSSON_MICRO_VERSION << 0))


/* types */

typedef enum
{
    JSON_OBJECT,
    JSON_ARRAY,
    JSON_STRING,
    JSON_INTEGER,
    JSON_REAL,
    JSON_TRUE,
    JSON_FALSE,
    JSON_NULL
} json_type;

typedef struct json_t
{
    json_type type;
    size_t refcount;
} json_t;

#ifndef JANSSON_USING_CMAKE /* disabled if using cmake */
#if JSON_INTEGER_IS_LONG_LONG
#ifdef _WIN32
#define JSON_INTEGER_FORMAT "I64d"
#else
#define JSON_INTEGER_FORMAT "lld"
#endif
typedef long long json_int_t;
#else
#define JSON_INTEGER_FORMAT "ld"
typedef long json_int_t;
#endif /* JSON_INTEGER_IS_LONG_LONG */
#endif

#define json_typeof(json)      ((json)->type)
#define json_is_object(json)   ((json) && json_typeof(json) == JSON_OBJECT)
#define json_is_array(json)    ((json) && json_typeof(json) == JSON_ARRAY)
#define json_is_string(json)   ((json) && json_typeof(json) == JSON_STRING)
#define json_is_integer(json)  ((json) && json_typeof(json) == JSON_INTEGER)
#define json_is_real(json)     ((json) && json_typeof(json) == JSON_REAL)
#define json_is_number(json)   (json_is_integer(json) || json_is_real(json))
#define json_is_true(json)     ((json) && json_typeof(json) == JSON_TRUE)
#define json_is_false(json)    ((json) && json_typeof(json) == JSON_FALSE)
#define json_boolean_value     json_is_true
#define json_is_boolean(json)  (json_is_true(json) || json_is_false(json))
#define json_is_null(json)     ((json) && json_typeof(json) == JSON_NULL)

/* construction, destruction, reference counting */

__declspec(dllimport) json_t* json_object(void);
__declspec(dllimport) json_t* json_array(void);
__declspec(dllimport) json_t* json_string(const char* value);
__declspec(dllimport) json_t* json_stringn(const char* value, size_t len);
__declspec(dllimport) json_t* json_string_nocheck(const char* value);
__declspec(dllimport) json_t* json_stringn_nocheck(const char* value, size_t len);
__declspec(dllimport) json_t* json_integer(json_int_t value);
__declspec(dllimport) json_t* json_real(double value);
__declspec(dllimport) json_t* json_true(void);
__declspec(dllimport) json_t* json_false(void);
#define json_boolean(val)      ((val) ? json_true() : json_false())
__declspec(dllimport) json_t* json_null(void);

static JSON_INLINE
json_t* json_incref(json_t* json)
{
    if(json && json->refcount != (size_t) - 1)
        ++json->refcount;
    return json;
}

/* do not call json_delete directly */
__declspec(dllimport) void json_delete(json_t* json);

static JSON_INLINE
void json_decref(json_t* json)
{
    if(json && json->refcount != (size_t) - 1 && --json->refcount == 0)
        json_delete(json);
}

#if defined(__GNUC__) || defined(__clang__)
static JSON_INLINE
void json_decrefp(json_t** json)
{
    if(json)
    {
        json_decref(*json);
        *json = NULL;
    }
}

#define json_auto_t json_t __attribute__((cleanup(json_decrefp)))
#endif


/* error reporting */

#define JSON_ERROR_TEXT_LENGTH    160
#define JSON_ERROR_SOURCE_LENGTH   80

typedef struct
{
    int line;
    int column;
    int position;
    char source[JSON_ERROR_SOURCE_LENGTH];
    char text[JSON_ERROR_TEXT_LENGTH];
} json_error_t;


/* getters, setters, manipulation */

__declspec(dllimport) void json_object_seed(size_t seed);
__declspec(dllimport) size_t json_object_size(const json_t* object);
__declspec(dllimport) json_t* json_object_get(const json_t* object, const char* key);
__declspec(dllimport) int json_object_set_new(json_t* object, const char* key, json_t* value);
__declspec(dllimport) int json_object_set_new_nocheck(json_t* object, const char* key, json_t* value);
__declspec(dllimport) int json_object_del(json_t* object, const char* key);
__declspec(dllimport) int json_object_clear(json_t* object);
__declspec(dllimport) int json_object_update(json_t* object, json_t* other);
__declspec(dllimport) int json_object_update_existing(json_t* object, json_t* other);
__declspec(dllimport) int json_object_update_missing(json_t* object, json_t* other);
__declspec(dllimport) void* json_object_iter(json_t* object);
__declspec(dllimport) void* json_object_iter_at(json_t* object, const char* key);
__declspec(dllimport) void* json_object_key_to_iter(const char* key);
__declspec(dllimport) void* json_object_iter_next(json_t* object, void* iter);
__declspec(dllimport) const char* json_object_iter_key(void* iter);
__declspec(dllimport) json_t* json_object_iter_value(void* iter);
__declspec(dllimport) int json_object_iter_set_new(json_t* object, void* iter, json_t* value);

#define json_object_foreach(object, key, value) \
    for(key = json_object_iter_key(json_object_iter(object)); \
        key && (value = json_object_iter_value(json_object_key_to_iter(key))); \
        key = json_object_iter_key(json_object_iter_next(object, json_object_key_to_iter(key))))

#define json_object_foreach_safe(object, n, key, value)     \
    for(key = json_object_iter_key(json_object_iter(object)), \
            n = json_object_iter_next(object, json_object_key_to_iter(key)); \
        key && (value = json_object_iter_value(json_object_key_to_iter(key))); \
        key = json_object_iter_key(n), \
            n = json_object_iter_next(object, json_object_key_to_iter(key)))

#define json_array_foreach(array, index, value) \
    for(index = 0; \
        index < json_array_size(array) && (value = json_array_get(array, index)); \
        index++)

static JSON_INLINE
int json_object_set(json_t* object, const char* key, json_t* value)
{
    return json_object_set_new(object, key, json_incref(value));
}

static JSON_INLINE
int json_object_set_nocheck(json_t* object, const char* key, json_t* value)
{
    return json_object_set_new_nocheck(object, key, json_incref(value));
}

static JSON_INLINE
int json_object_iter_set(json_t* object, void* iter, json_t* value)
{
    return json_object_iter_set_new(object, iter, json_incref(value));
}

__declspec(dllimport) size_t json_array_size(const json_t* array);
__declspec(dllimport) json_t* json_array_get(const json_t* array, size_t index);
__declspec(dllimport) int json_array_set_new(json_t* array, size_t index, json_t* value);
__declspec(dllimport) int json_array_append_new(json_t* array, json_t* value);
__declspec(dllimport) int json_array_insert_new(json_t* array, size_t index, json_t* value);
__declspec(dllimport) int json_array_remove(json_t* array, size_t index);
__declspec(dllimport) int json_array_clear(json_t* array);
__declspec(dllimport) int json_array_extend(json_t* array, json_t* other);

static JSON_INLINE
int json_array_set(json_t* array, size_t ind, json_t* value)
{
    return json_array_set_new(array, ind, json_incref(value));
}

static JSON_INLINE
int json_array_append(json_t* array, json_t* value)
{
    return json_array_append_new(array, json_incref(value));
}

static JSON_INLINE
int json_array_insert(json_t* array, size_t ind, json_t* value)
{
    return json_array_insert_new(array, ind, json_incref(value));
}

__declspec(dllimport) const char* json_string_value(const json_t* string);
__declspec(dllimport) size_t json_string_length(const json_t* string);
__declspec(dllimport) json_int_t json_integer_value(const json_t* integer);
__declspec(dllimport) double json_real_value(const json_t* real);
__declspec(dllimport) double json_number_value(const json_t* json);

__declspec(dllimport) int json_string_set(json_t* string, const char* value);
__declspec(dllimport) int json_string_setn(json_t* string, const char* value, size_t len);
__declspec(dllimport) int json_string_set_nocheck(json_t* string, const char* value);
__declspec(dllimport) int json_string_setn_nocheck(json_t* string, const char* value, size_t len);
__declspec(dllimport) int json_integer_set(json_t* integer, json_int_t value);
__declspec(dllimport) int json_real_set(json_t* real, double value);

/* pack, unpack */

__declspec(dllimport) json_t* json_pack(const char* fmt, ...);
__declspec(dllimport) json_t* json_pack_ex(json_error_t* error, size_t flags, const char* fmt, ...);
__declspec(dllimport) json_t* json_vpack_ex(json_error_t* error, size_t flags, const char* fmt, va_list ap);

#define JSON_VALIDATE_ONLY  0x1
#define JSON_STRICT         0x2

__declspec(dllimport) int json_unpack(json_t* root, const char* fmt, ...);
__declspec(dllimport) int json_unpack_ex(json_t* root, json_error_t* error, size_t flags, const char* fmt, ...);
__declspec(dllimport) int json_vunpack_ex(json_t* root, json_error_t* error, size_t flags, const char* fmt, va_list ap);


/* equality */

__declspec(dllimport) int json_equal(json_t* value1, json_t* value2);


/* copying */

__declspec(dllimport) json_t* json_copy(json_t* value);
__declspec(dllimport) json_t* json_deep_copy(const json_t* value);


/* decoding */

#define JSON_REJECT_DUPLICATES  0x1
#define JSON_DISABLE_EOF_CHECK  0x2
#define JSON_DECODE_ANY         0x4
#define JSON_DECODE_INT_AS_REAL 0x8
#define JSON_ALLOW_NUL          0x10

typedef size_t (*json_load_callback_t)(void* buffer, size_t buflen, void* data);

__declspec(dllimport) json_t* json_loads(const char* input, size_t flags, json_error_t* error);
__declspec(dllimport) json_t* json_loadb(const char* buffer, size_t buflen, size_t flags, json_error_t* error);
__declspec(dllimport) json_t* json_loadf(FILE* input, size_t flags, json_error_t* error);
__declspec(dllimport) json_t* json_load_file(const char* path, size_t flags, json_error_t* error);
__declspec(dllimport) json_t* json_load_callback(json_load_callback_t callback, void* data, size_t flags, json_error_t* error);


/* encoding */

#define JSON_MAX_INDENT         0x1F
#define JSON_INDENT(n)          ((n) & JSON_MAX_INDENT)
#define JSON_COMPACT            0x20
#define JSON_ENSURE_ASCII       0x40
#define JSON_SORT_KEYS          0x80
#define JSON_PRESERVE_ORDER     0x100
#define JSON_ENCODE_ANY         0x200
#define JSON_ESCAPE_SLASH       0x400
#define JSON_REAL_PRECISION(n)  (((n) & 0x1F) << 11)

typedef int (*json_dump_callback_t)(const char* buffer, size_t size, void* data);

__declspec(dllimport) char* json_dumps(const json_t* json, size_t flags);
__declspec(dllimport) int json_dumpf(const json_t* json, FILE* output, size_t flags);
__declspec(dllimport) int json_dump_file(const json_t* json, const char* path, size_t flags);
__declspec(dllimport) int json_dump_callback(const json_t* json, json_dump_callback_t callback, void* data, size_t flags);

/* custom memory allocation */

typedef void* (*json_malloc_t)(size_t);
typedef void (*json_free_t)(void*);

__declspec(dllimport) void json_set_alloc_funcs(json_malloc_t malloc_fn, json_free_t free_fn);
__declspec(dllimport) void json_get_alloc_funcs(json_malloc_t* malloc_fn, json_free_t* free_fn);

#ifdef __cplusplus
}
#endif

#endif

```

`CodeCleaner/pluginsdk/jansson/jansson_config.h`:

```h
/*
 * Copyright (c) 2010-2016 Petri Lehtinen <petri@digip.org>
 *
 * Jansson is free software; you can redistribute it and/or modify
 * it under the terms of the MIT license. See LICENSE for details.
 *
 *
 * This file specifies a part of the site-specific configuration for
 * Jansson, namely those things that affect the public API in
 * jansson.h.
 *
 * The CMake system will generate the jansson_config.h file and
 * copy it to the build and install directories.
 */

#ifndef JANSSON_CONFIG_H
#define JANSSON_CONFIG_H

/* Define this so that we can disable scattered automake configuration in source files */
#ifndef JANSSON_USING_CMAKE
#define JANSSON_USING_CMAKE
#endif

/* Note: when using cmake, JSON_INTEGER_IS_LONG_LONG is not defined nor used,
 * as we will also check for __int64 etc types.
 * (the definition was used in the automake system) */

/* Bring in the cmake-detected defines */
#define HAVE_STDINT_H 1
/* #undef HAVE_INTTYPES_H */
/* #undef HAVE_SYS_TYPES_H */

/* Include our standard type header for the integer typedef */

#if defined(HAVE_STDINT_H)
#  include <stdint.h>
#elif defined(HAVE_INTTYPES_H)
#  include <inttypes.h>
#elif defined(HAVE_SYS_TYPES_H)
#  include <sys/types.h>
#endif


/* If your compiler supports the inline keyword in C, JSON_INLINE is
   defined to `inline', otherwise empty. In C++, the inline is always
   supported. */
#ifdef __cplusplus
#define JSON_INLINE inline
#else
#define JSON_INLINE __inline
#endif


#define json_int_t long long
#define json_strtoint strtoll
#define JSON_INTEGER_FORMAT "I64d"


/* If locale.h and localeconv() are available, define to 1, otherwise to 0. */
#define JSON_HAVE_LOCALECONV 1


/* Maximum recursion depth for parsing JSON input.
   This limits the depth of e.g. array-within-array constructions. */
#define JSON_PARSER_MAX_DEPTH 2048


#endif

```

`CodeCleaner/pluginsdk/jansson/jansson_x64dbg.h`:

```h
#pragma once

#include "jansson.h"

typedef json_t* JSON;

static JSON_INLINE
json_t* json_hex(unsigned json_int_t value)
{
    char hexvalue[20];
    sprintf_s(hexvalue, "0x%llX", value);
    return json_string(hexvalue);
}

static JSON_INLINE
unsigned json_int_t json_hex_value(const json_t* hex)
{
    unsigned json_int_t ret = 0;
    const char* hexvalue;
    hexvalue = json_string_value(hex);
    if(!hexvalue)
        return 0;
    sscanf_s(hexvalue, "0x%llX", &ret);
    return ret;
}

```

`CodeCleaner/pluginsdk/lz4/lz4.h`:

```h
/*
   LZ4 - Fast LZ compression algorithm
   Header File
   Copyright (C) 2011-2014, Yann Collet.
   BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions are
   met:

       * Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.
       * Redistributions in binary form must reproduce the above
   copyright notice, this list of conditions and the following disclaimer
   in the documentation and/or other materials provided with the
   distribution.

   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

   You can contact the author at :
   - LZ4 homepage : http://fastcompression.blogspot.com/p/lz4.html
   - LZ4 source repository : http://code.google.com/p/lz4/
*/
#ifndef _LZ4_H
#define _LZ4_H

#if defined (__cplusplus)
extern "C"
{
#endif


/**************************************
   Version
**************************************/
#define LZ4_VERSION_MAJOR    1    /* for major interface/format changes  */
#define LZ4_VERSION_MINOR    1    /* for minor interface/format changes  */
#define LZ4_VERSION_RELEASE  3    /* for tweaks, bug-fixes, or development */


/**************************************
   Compiler Options
**************************************/
#if (defined(__GNUC__) && defined(__STRICT_ANSI__)) || (defined(_MSC_VER) && !defined(__cplusplus))   /* Visual Studio */
#  define inline __inline           /* Visual C is not C99, but supports some kind of inline */
#endif


/**************************************
   Simple Functions
**************************************/

__declspec(dllimport) int LZ4_compress(const char* source, char* dest, int inputSize);
__declspec(dllimport) int LZ4_decompress_safe(const char* source, char* dest, int inputSize, int maxOutputSize);

/*
LZ4_compress() :
    Compresses 'inputSize' bytes from 'source' into 'dest'.
    Destination buffer must be already allocated,
    and must be sized to handle worst cases situations (input data not compressible)
    Worst case size evaluation is provided by function LZ4_compressBound()
    inputSize : Max supported value is LZ4_MAX_INPUT_VALUE
    return : the number of bytes written in buffer dest
             or 0 if the compression fails

LZ4_decompress_safe() :
    maxOutputSize : is the size of the destination buffer (which must be already allocated)
    return : the number of bytes decoded in the destination buffer (necessarily <= maxOutputSize)
             If the source stream is detected malformed, the function will stop decoding and return a negative result.
             This function is protected against buffer overflow exploits (never writes outside of output buffer, and never reads outside of input buffer). Therefore, it is protected against malicious data packets
*/


/**************************************
   Advanced Functions
**************************************/
#define LZ4_MAX_INPUT_SIZE        0x7E000000   /* 2 113 929 216 bytes */
#define LZ4_COMPRESSBOUND(isize)  ((unsigned int)(isize) > (unsigned int)LZ4_MAX_INPUT_SIZE ? 0 : (isize) + ((isize)/255) + 16)

/*
LZ4_compressBound() :
    Provides the maximum size that LZ4 may output in a "worst case" scenario (input data not compressible)
    primarily useful for memory allocation of output buffer.
    inline function is recommended for the general case,
    macro is also provided when result needs to be evaluated at compilation (such as stack memory allocation).

    isize  : is the input size. Max supported value is LZ4_MAX_INPUT_SIZE
    return : maximum output size in a "worst case" scenario
             or 0, if input size is too large ( > LZ4_MAX_INPUT_SIZE)
*/
__declspec(dllimport) int LZ4_compressBound(int isize);


/*
LZ4_compress_limitedOutput() :
    Compress 'inputSize' bytes from 'source' into an output buffer 'dest' of maximum size 'maxOutputSize'.
    If it cannot achieve it, compression will stop, and result of the function will be zero.
    This function never writes outside of provided output buffer.

    inputSize  : Max supported value is LZ4_MAX_INPUT_VALUE
    maxOutputSize : is the size of the destination buffer (which must be already allocated)
    return : the number of bytes written in buffer 'dest'
             or 0 if the compression fails
*/
__declspec(dllimport) int LZ4_compress_limitedOutput(const char* source, char* dest, int inputSize, int maxOutputSize);


/*
LZ4_decompress_fast() :
    originalSize : is the original and therefore uncompressed size
    return : the number of bytes read from the source buffer (in other words, the compressed size)
             If the source stream is malformed, the function will stop decoding and return a negative result.
    note : This function is a bit faster than LZ4_decompress_safe()
           This function never writes outside of output buffers, but may read beyond input buffer in case of malicious data packet.
           Use this function preferably into a trusted environment (data to decode comes from a trusted source).
           Destination buffer must be already allocated. Its size must be a minimum of 'outputSize' bytes.
*/
__declspec(dllimport) int LZ4_decompress_fast(const char* source, char* dest, int originalSize);


/*
LZ4_decompress_safe_partial() :
    This function decompress a compressed block of size 'inputSize' at position 'source'
    into output buffer 'dest' of size 'maxOutputSize'.
    The function tries to stop decompressing operation as soon as 'targetOutputSize' has been reached,
    reducing decompression time.
    return : the number of bytes decoded in the destination buffer (necessarily <= maxOutputSize)
       Note : this number can be < 'targetOutputSize' should the compressed block to decode be smaller.
             Always control how many bytes were decoded.
             If the source stream is detected malformed, the function will stop decoding and return a negative result.
             This function never writes outside of output buffer, and never reads outside of input buffer. It is therefore protected against malicious data packets
*/
__declspec(dllimport) int LZ4_decompress_safe_partial(const char* source, char* dest, int inputSize, int targetOutputSize, int maxOutputSize);


/*
These functions are provided should you prefer to allocate memory for compression tables with your own allocation methods.
To know how much memory must be allocated for the compression tables, use :
int LZ4_sizeofState();

Note that tables must be aligned on 4-bytes boundaries, otherwise compression will fail (return code 0).

The allocated memory can be provided to the compressions functions using 'void* state' parameter.
LZ4_compress_withState() and LZ4_compress_limitedOutput_withState() are equivalent to previously described functions.
They just use the externally allocated memory area instead of allocating their own (on stack, or on heap).
*/
__declspec(dllimport) int LZ4_sizeofState(void);
__declspec(dllimport) int LZ4_compress_withState(void* state, const char* source, char* dest, int inputSize);
__declspec(dllimport) int LZ4_compress_limitedOutput_withState(void* state, const char* source, char* dest, int inputSize, int maxOutputSize);


/**************************************
   Streaming Functions
**************************************/
__declspec(dllimport) void* LZ4_create(const char* inputBuffer);
__declspec(dllimport) int   LZ4_compress_continue(void* LZ4_Data, const char* source, char* dest, int inputSize);
__declspec(dllimport) int   LZ4_compress_limitedOutput_continue(void* LZ4_Data, const char* source, char* dest, int inputSize, int maxOutputSize);
__declspec(dllimport) char* LZ4_slideInputBuffer(void* LZ4_Data);
__declspec(dllimport) int   LZ4_free(void* LZ4_Data);

/*
These functions allow the compression of dependent blocks, where each block benefits from prior 64 KB within preceding blocks.
In order to achieve this, it is necessary to start creating the LZ4 Data Structure, thanks to the function :

void* LZ4_create (const char* inputBuffer);
The result of the function is the (void*) pointer on the LZ4 Data Structure.
This pointer will be needed in all other functions.
If the pointer returned is NULL, then the allocation has failed, and compression must be aborted.
The only parameter 'const char* inputBuffer' must, obviously, point at the beginning of input buffer.
The input buffer must be already allocated, and size at least 192KB.
'inputBuffer' will also be the 'const char* source' of the first block.

All blocks are expected to lay next to each other within the input buffer, starting from 'inputBuffer'.
To compress each block, use either LZ4_compress_continue() or LZ4_compress_limitedOutput_continue().
Their behavior are identical to LZ4_compress() or LZ4_compress_limitedOutput(),
but require the LZ4 Data Structure as their first argument, and check that each block starts right after the previous one.
If next block does not begin immediately after the previous one, the compression will fail (return 0).

When it's no longer possible to lay the next block after the previous one (not enough space left into input buffer), a call to :
char* LZ4_slideInputBuffer(void* LZ4_Data);
must be performed. It will typically copy the latest 64KB of input at the beginning of input buffer.
Note that, for this function to work properly, minimum size of an input buffer must be 192KB.
==> The memory position where the next input data block must start is provided as the result of the function.

Compression can then resume, using LZ4_compress_continue() or LZ4_compress_limitedOutput_continue(), as usual.

When compression is completed, a call to LZ4_free() will release the memory used by the LZ4 Data Structure.
*/


__declspec(dllimport) int LZ4_sizeofStreamState(void);
__declspec(dllimport) int LZ4_resetStreamState(void* state, const char* inputBuffer);

/*
These functions achieve the same result as :
void* LZ4_create (const char* inputBuffer);

They are provided here to allow the user program to allocate memory using its own routines.

To know how much space must be allocated, use LZ4_sizeofStreamState();
Note also that space must be 4-bytes aligned.

Once space is allocated, you must initialize it using : LZ4_resetStreamState(void* state, const char* inputBuffer);
void* state is a pointer to the space allocated.
It must be aligned on 4-bytes boundaries, and be large enough.
The parameter 'const char* inputBuffer' must, obviously, point at the beginning of input buffer.
The input buffer must be already allocated, and size at least 192KB.
'inputBuffer' will also be the 'const char* source' of the first block.

The same space can be re-used multiple times, just by initializing it each time with LZ4_resetStreamState().
return value of LZ4_resetStreamState() must be 0 is OK.
Any other value means there was an error (typically, pointer is not aligned on 4-bytes boundaries).
*/


__declspec(dllimport) int LZ4_decompress_safe_withPrefix64k(const char* source, char* dest, int inputSize, int maxOutputSize);
__declspec(dllimport) int LZ4_decompress_fast_withPrefix64k(const char* source, char* dest, int outputSize);

/*
*_withPrefix64k() :
    These decoding functions work the same as their "normal name" versions,
    but can use up to 64KB of data in front of 'char* dest'.
    These functions are necessary to decode inter-dependant blocks.
*/


/**************************************
   Obsolete Functions
**************************************/
/*
These functions are deprecated and should no longer be used.
They are provided here for compatibility with existing user programs.
*/
__declspec(dllimport) int LZ4_uncompress(const char* source, char* dest, int outputSize);
__declspec(dllimport) int LZ4_uncompress_unknownOutputSize(const char* source, char* dest, int isize, int maxOutputSize);


#if defined (__cplusplus)
}
#endif

#endif //_LZ4_H
```

`CodeCleaner/pluginsdk/lz4/lz4file.h`:

```h
#ifndef _LZ4FILE_H
#define _LZ4FILE_H

typedef enum _LZ4_STATUS
{
    LZ4_SUCCESS,
    LZ4_FAILED_OPEN_INPUT,
    LZ4_FAILED_OPEN_OUTPUT,
    LZ4_NOT_ENOUGH_MEMORY,
    LZ4_INVALID_ARCHIVE,
    LZ4_CORRUPTED_ARCHIVE
} LZ4_STATUS;

#if defined (__cplusplus)
extern "C"
{
#endif

__declspec(dllimport) LZ4_STATUS LZ4_compress_file(const char* input_filename, const char* output_filename);
__declspec(dllimport) LZ4_STATUS LZ4_compress_fileW(const wchar_t* input_filename, const wchar_t* output_filename);
__declspec(dllimport) LZ4_STATUS LZ4_decompress_file(const char* input_filename, const char* output_filename);
__declspec(dllimport) LZ4_STATUS LZ4_decompress_fileW(const wchar_t* input_filename, const wchar_t* output_filename);

#if defined (__cplusplus)
}
#endif

#endif //_LZ4FILE_H
```

`CodeCleaner/pluginsdk/lz4/lz4hc.h`:

```h
/*
   LZ4 HC - High Compression Mode of LZ4
   Header File
   Copyright (C) 2011-2014, Yann Collet.
   BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions are
   met:

       * Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.
       * Redistributions in binary form must reproduce the above
   copyright notice, this list of conditions and the following disclaimer
   in the documentation and/or other materials provided with the
   distribution.

   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

   You can contact the author at :
   - LZ4 homepage : http://fastcompression.blogspot.com/p/lz4.html
   - LZ4 source repository : http://code.google.com/p/lz4/
*/
#ifndef _LZ4HC_H
#define _LZ4HC_H

#if defined (__cplusplus)
extern "C"
{
#endif


__declspec(dllimport) int LZ4_compressHC(const char* source, char* dest, int inputSize);
/*
LZ4_compressHC :
    return : the number of bytes in compressed buffer dest
             or 0 if compression fails.
    note : destination buffer must be already allocated.
        To avoid any problem, size it to handle worst cases situations (input data not compressible)
        Worst case size evaluation is provided by function LZ4_compressBound() (see "lz4.h")
*/

__declspec(dllimport) int LZ4_compressHC_limitedOutput(const char* source, char* dest, int inputSize, int maxOutputSize);
/*
LZ4_compress_limitedOutput() :
    Compress 'inputSize' bytes from 'source' into an output buffer 'dest' of maximum size 'maxOutputSize'.
    If it cannot achieve it, compression will stop, and result of the function will be zero.
    This function never writes outside of provided output buffer.

    inputSize  : Max supported value is 1 GB
    maxOutputSize : is maximum allowed size into the destination buffer (which must be already allocated)
    return : the number of output bytes written in buffer 'dest'
             or 0 if compression fails.
*/


__declspec(dllimport) int LZ4_compressHC2(const char* source, char* dest, int inputSize, int compressionLevel);
__declspec(dllimport) int LZ4_compressHC2_limitedOutput(const char* source, char* dest, int inputSize, int maxOutputSize, int compressionLevel);
/*
    Same functions as above, but with programmable 'compressionLevel'.
    Recommended values are between 4 and 9, although any value between 0 and 16 will work.
    'compressionLevel'==0 means use default 'compressionLevel' value.
    Values above 16 behave the same as 16.
    Equivalent variants exist for all other compression functions below.
*/

/* Note :
Decompression functions are provided within LZ4 source code (see "lz4.h") (BSD license)
*/


/**************************************
   Using an external allocation
**************************************/
__declspec(dllimport) int LZ4_sizeofStateHC(void);
__declspec(dllimport) int LZ4_compressHC_withStateHC(void* state, const char* source, char* dest, int inputSize);
__declspec(dllimport) int LZ4_compressHC_limitedOutput_withStateHC(void* state, const char* source, char* dest, int inputSize, int maxOutputSize);

__declspec(dllimport) int LZ4_compressHC2_withStateHC(void* state, const char* source, char* dest, int inputSize, int compressionLevel);
__declspec(dllimport) int LZ4_compressHC2_limitedOutput_withStateHC(void* state, const char* source, char* dest, int inputSize, int maxOutputSize, int compressionLevel);

/*
These functions are provided should you prefer to allocate memory for compression tables with your own allocation methods.
To know how much memory must be allocated for the compression tables, use :
int LZ4_sizeofStateHC();

Note that tables must be aligned for pointer (32 or 64 bits), otherwise compression will fail (return code 0).

The allocated memory can be provided to the compressions functions using 'void* state' parameter.
LZ4_compress_withStateHC() and LZ4_compress_limitedOutput_withStateHC() are equivalent to previously described functions.
They just use the externally allocated memory area instead of allocating their own (on stack, or on heap).
*/


/**************************************
   Streaming Functions
**************************************/
__declspec(dllimport) void* LZ4_createHC(const char* inputBuffer);
__declspec(dllimport) int   LZ4_compressHC_continue(void* LZ4HC_Data, const char* source, char* dest, int inputSize);
__declspec(dllimport) int   LZ4_compressHC_limitedOutput_continue(void* LZ4HC_Data, const char* source, char* dest, int inputSize, int maxOutputSize);
__declspec(dllimport) char* LZ4_slideInputBufferHC(void* LZ4HC_Data);
__declspec(dllimport) int   LZ4_freeHC(void* LZ4HC_Data);

__declspec(dllimport) int   LZ4_compressHC2_continue(void* LZ4HC_Data, const char* source, char* dest, int inputSize, int compressionLevel);
__declspec(dllimport) int   LZ4_compressHC2_limitedOutput_continue(void* LZ4HC_Data, const char* source, char* dest, int inputSize, int maxOutputSize, int compressionLevel);

/*
These functions allow the compression of dependent blocks, where each block benefits from prior 64 KB within preceding blocks.
In order to achieve this, it is necessary to start creating the LZ4HC Data Structure, thanks to the function :

void* LZ4_createHC (const char* inputBuffer);
The result of the function is the (void*) pointer on the LZ4HC Data Structure.
This pointer will be needed in all other functions.
If the pointer returned is NULL, then the allocation has failed, and compression must be aborted.
The only parameter 'const char* inputBuffer' must, obviously, point at the beginning of input buffer.
The input buffer must be already allocated, and size at least 192KB.
'inputBuffer' will also be the 'const char* source' of the first block.

All blocks are expected to lay next to each other within the input buffer, starting from 'inputBuffer'.
To compress each block, use either LZ4_compressHC_continue() or LZ4_compressHC_limitedOutput_continue().
Their behavior are identical to LZ4_compressHC() or LZ4_compressHC_limitedOutput(),
but require the LZ4HC Data Structure as their first argument, and check that each block starts right after the previous one.
If next block does not begin immediately after the previous one, the compression will fail (return 0).

When it's no longer possible to lay the next block after the previous one (not enough space left into input buffer), a call to :
char* LZ4_slideInputBufferHC(void* LZ4HC_Data);
must be performed. It will typically copy the latest 64KB of input at the beginning of input buffer.
Note that, for this function to work properly, minimum size of an input buffer must be 192KB.
==> The memory position where the next input data block must start is provided as the result of the function.

Compression can then resume, using LZ4_compressHC_continue() or LZ4_compressHC_limitedOutput_continue(), as usual.

When compression is completed, a call to LZ4_freeHC() will release the memory used by the LZ4HC Data Structure.
*/

__declspec(dllimport) int LZ4_sizeofStreamStateHC(void);
__declspec(dllimport) int LZ4_resetStreamStateHC(void* state, const char* inputBuffer);

/*
These functions achieve the same result as :
void* LZ4_createHC (const char* inputBuffer);

They are provided here to allow the user program to allocate memory using its own routines.

To know how much space must be allocated, use LZ4_sizeofStreamStateHC();
Note also that space must be aligned for pointers (32 or 64 bits).

Once space is allocated, you must initialize it using : LZ4_resetStreamStateHC(void* state, const char* inputBuffer);
void* state is a pointer to the space allocated.
It must be aligned for pointers (32 or 64 bits), and be large enough.
The parameter 'const char* inputBuffer' must, obviously, point at the beginning of input buffer.
The input buffer must be already allocated, and size at least 192KB.
'inputBuffer' will also be the 'const char* source' of the first block.

The same space can be re-used multiple times, just by initializing it each time with LZ4_resetStreamState().
return value of LZ4_resetStreamStateHC() must be 0 is OK.
Any other value means there was an error (typically, state is not aligned for pointers (32 or 64 bits)).
*/


#if defined (__cplusplus)
}
#endif

#endif //_LZ4HC_H

```