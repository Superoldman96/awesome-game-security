Project Path: arc_gmh5225_aimbot-detection-prototype_odilscz5

Source Tree:

```txt
arc_gmh5225_aimbot-detection-prototype_odilscz5
├── README.md
├── auto_clip.py
├── clip_creator
│   └── auto_clip.py
├── combine_cnn_output.py
├── extract_1_sec.py
├── hacks_data_tensor
│   └── full_data
│       ├── hacks_data_tensor_file_100.pt
│       └── hacks_data_tensor_file_old.pt
├── models
│   ├── model.pt
│   ├── model_66.pt
│   ├── model_74.pt
│   ├── model_INCREDIBLE.pt
│   └── model_max_data.pt
├── no_hacks_data_tensor
│   └── full_data
│       ├── no_hacks_data_tensor_file_100.pt
│       └── no_hacks_data_tensor_file_old.pt
├── old_test_rnn.py
├── predict.py
├── save_cnn_output.py
├── test_rnn.py
├── train_rnn.py
├── validate_rnn.py
└── video_to_photo.py

```

`README.md`:

```md
# Detecting Aimbot in CS:GO using Machine Learning
## Required Software Packages:
- python (3.9 +) 
- OpenCV:
Installed with: `pip install opencv-python`
- Matplotlib:
Installed with: `pip install matplotlib`
- PyTorch:
Installed with: `pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113`
Or if you don't have access to an Nvidia GPU: `pip3 install torch torchvision torchaudio`


## Finding and predicting clips in a video:

Record a video of CS:GO gameplay involving some kills from the player. 
**Note that the video must be 1080p and run at 60fps.**

Predict by running: `python predict.py <video>` where `<video>` contains the path to the video you want to make predictions on

While running, the program will display a window that shows the gameplay that is currently being analyzed along with the number of clips recorded so far. An additional window shows approximately what the parser is looking at. The program will run until either the video is finished, or the user presses ‘q’. Once finished, the clips will automatically be saved to a new folder.

The clips are automatically sent to the CNN for extraction. Once the features have been extracted, the program automatically predicts whether the clips involved the use of the aimbot and the output can be seen in the terminal. 

## Training the model
1. Record gameplay of CS:GO at 1080p 60fps
2. Parse gameplay using `autoclip.py` 
	- aimbot clips should be saved to `hacks_data_nn`
	- regular clips should be saved to `no_hacks_data_nn`
3. Extract clip features using `save_cnn_output.py`
	- aimbot clips should be saved to `hacks_data_tensor`
	- regular clips should be saved to `no_hacks_data_tensor`
4. Train RNN using `train_rnn.py`
5. Model will be saved to `models/model.pt` **Any file with that name will be overwritten**


## Using files outside of pipeline
### Parser `autoclip.py`
To run the auto clipper by itself, run the following command: `python ./auto_clip.py <input file> <output directory> <use time>`
- `<input file>` is the file you want to parse
- `<output directory>` is where you want the clips to be output to
- `<use time>` (Optional, defaults to `0`)  adds a timestamp to the clips. Set to `1` to turn the timestamps on, and `0` to turn them off

### CNN `save_cnn_output.py`
To run the CNN by itself, run the following command: `python save_cnn_output.py <clip directory> <output directory>`
- `<clip directory>` is the directory where the clips are located
- `<output directory>` is where you want to save the extracted features 

### Test RNN `test_rnn.py`
To run the extracted features through the RNN, run the following command: `python test_rnn.py <.pt path> <clip path>`
- `<.pt path>` is the path to the .pt file output from the CNN
- `<clip path>` (Optional, displays timestamps when specified) is the path to clips corresponding to the .pt file are saved

### Train RNN `train_rnn.py`
To train the RNN, run the following command: `python train_rnn.py`

### Validate RNN `validate_rnn.py`
To validate the RNN, run the following command: `python validate_rnn.py`

## Git repo:

https://github.com/SrikarValluri/aimbot-detection

## Unrealized features:
- Cheat detection for multiple games and different cheats
- Website for easy access and use

```

`auto_clip.py`:

```py
"""
auto_clip.py
Usage: python auto_clip.py {Input Folder} {Output Directory} {(Optional) Use Time (0 or 1)}

Takes in an input video file and finds kills based on killfeet

Video file must be 1920 x 1080 @ 60FPS
Video should contin gameplay of Counter Strike: Global Offensive
"""

import cv2
import numpy as np
import sys
import os
import datetime

# global vars
fc = 0

nlines = 0
plines = 0

decCount = 0

pois = []

#  Require 2 options to running program
assert len(sys.argv) >= 3, "needs file input and destination"

inFile = sys.argv[1]
outDir = sys.argv[2]

# Require input file and output directory exist
assert os.path.isfile(inFile), f'input file {inFile} doesn\'t exist'
assert os.path.isdir(outDir), "output directory doesn't exist"

# allow optional flag to save clips in folders named after the time they ocour in the video
useTime = False
if len(sys.argv) > 3 and sys.argv[3].isnumeric() and int(sys.argv[3]) > 0:
    useTime = True

print(f'Input File: {inFile}')
print(f'Output Dir: {outDir}')
print("")



# Creating a VideoCapture object to read the video
cap = cv2.VideoCapture(inFile)

# make sure video is the correct frame rate
framRate = cap.get(cv2.CAP_PROP_FPS)
assert int(framRate) == 60, "Video must be 60FPS"

print(framRate)

showParse = False


# count the number of red lines found in a portion of an image
# input image must be 3 pixles wide and in BGR format
def countLines(im):
    assert im.shape[1] == 3

    streak = 0
    breakS = 0
    lineC = 0

    # for each horizontal line
    for y in range(im.shape[0]):
        # if 2 pixels in a row are incorrect
        if breakS >= 2:
            breakS = 0

            # increment line count if there were enough pixles in a row
            if streak > 15:
                lineC += 1
            streak = 0

        # check if pixel to left is the same color
        # Used to check for deaths because the are a solid color and not an outlined box
        if abs(int(im[y,1,2]) - int(im[y,0,2])) < 20:
            breakS += 1
            continue

        # check if the pixel is roughly the correct color of red
        if im[y,1,2] > 150 and im[y,1,1] < 10 and im[y,1,0] < 50:
            streak += 1
            breakS = 0

        # if wrong color cont count towards streak
        else:
            breakS += 1


    # Add a line to the count if the loop ends saying there is a streak
    if breakS > 15:
        lineC += 1

    return lineC




# Loop until the end of the video
while (cap.isOpened()):

	# Capture frame-by-frame
    ret, frame = cap.read()

    if frame is None:
        break


    # view = cv2.resize(view, (1280, 720), fx = 0, fy = 0, interpolation = cv2.INTER_CUBIC)

    # area of video to analyze must be 3 pixels wide
    # this area is the right edge of the kill feed
    frame_crop = frame[70:300, 1907:1910]


    # count the number of kill feed edges
    nlines = countLines(frame_crop)

    # Display kills to user
    print(f'Kills on Screen: {nlines}, Kills Counted: {len(pois)}\t\t\r', end='', flush=True)

   

    decCount += 1

    # check curent number of lines vs previous frames number of lines
    if nlines < plines:
        # keep track of the last time the number of edges decreased
        decCount = 0

    # if number of lines has increased add frame number to list to save later
    if nlines > plines and decCount > 10:
        # print("Increase")
        pois.append(fc)

    plines = nlines


    # shown to user
    view = cv2.resize(frame, (1280, 720), fx = 0, fy = 0, interpolation = cv2.INTER_CUBIC)
    # view = cv2.putText(view, f'Kills Counted: {len(pois)}', (822, 700), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255), 1, cv2.LINE_AA)
    # view = cv2.putText(view, f'Kills Counted: {len(pois)}', (818, 700), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255), 1, cv2.LINE_AA)
    view = cv2.putText(view, f'Kills Counted: {len(pois)}', (530, 450), cv2.FONT_HERSHEY_COMPLEX, 0.75, (0,0,255), 1, cv2.LINE_AA)
    
    if showParse:
        parseView = view.copy()
        parseView[parseView[:,:,2] <= 110 ] = [0,0,0]
        parseView[parseView[:,:,1] >= 20 ] = [0,0,0]
        parseView[parseView[:,:,0] >= 40 ] = [0,0,0]
        cv2.imshow('Parser Vision', parseView)


	# Display the frames to user
    # cv2.imshow('Area or Intrest', frame)
    
    cv2.imshow('Normal Gameplay', view)

    




	# define q as the exit button
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break


    fc += 1
    


# release the video capture object
cap.release()
# Closes all the windows currently opened.
cv2.destroyAllWindows()


# prints for refrence
print("\n")
print(pois)
print(f'\n----------\nSaving {len(pois)} Clip{"s" if len(pois) != 1 else ""}')



# how to save a frame modified from video_to_photos
# only works with 1920x1080 video
def save_frames(file_name, out_dir, start=0, end=-1):
    cap = cv2.VideoCapture(file_name)

    assert cap, "path to file must be valid"

    if not os.path.exists(out_dir):
            os.makedirs(out_dir)


    if end == -1:
        end = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    cap.set(cv2.CAP_PROP_POS_FRAMES, start)

    for fno in range(start, end):
        success, frame = cap.read()
        assert success, "frame not read correctly"

        frame = frame[428:652, 848:1072]

        name = os.path.join(os.getcwd(), out_dir, "frame" + str(fno + 1) + ".png")
        cv2.imwrite(name, frame)

    
    cap.release()


# get folder names in outdir
subfolders = [ f.name for f in os.scandir(outDir) if f.is_dir() ]
max = 0

if not useTime:
    # make new folder name larger than any folder in curent directory
    for folder in subfolders:
        if folder.isnumeric() and int(folder) >= max:
            max = int(folder) + 1



# save all clips found earlier
for x in range(len(pois)):
    assert pois[x] >= 55

    print(f'Saving Clip {x + 1}\t\r', end='', flush=True)

    # save folder based on time clip is in respective video
    # using this name scheme may result in duplicate folder names across multiple different videos
    # useful for saving info to tell to user
    if useTime:
        s = pois[x] / 60
        time = datetime.timedelta(seconds = s)
        # print(time)
        save_frames(inFile, os.path.join(outDir, str(time).replace(":", "_")), pois[x] - 55, pois[x] + 5)

    # save folder name based on other folders in dir
    else:
        save_frames(inFile, os.path.join(outDir , str(max + x)), pois[x] - 55, pois[x] + 5)

print("All Clips Saved!\t\t\t\t")


```

`clip_creator/auto_clip.py`:

```py
import cv2
import numpy as np
import sys

assert len(sys.argv) == 2, "needs file input"

print(sys.argv[1])

# Creating a VideoCapture object to read the video
cap = cv2.VideoCapture(sys.argv[1])

fc = 0

nlines = 0
plines = 0

lscore = 0


# Loop until the end of the video
while (cap.isOpened()):

	# Capture frame-by-frame
    ret, frame = cap.read()

    if frame is None:
        break

    fc += 1

    # frame = cv2.resize(frame, (540, 380), fx = 0, fy = 0,
	# 					interpolation = cv2.INTER_CUBIC)

    frame = frame[70:300, 1908:1910]

    # frame[10,0] = [0,255,0]

    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

    # print(frame[0,0])
    # print("-------")
    # print(hsv[0,0])
    # print("=======")

    s = hsv[:,:,1]
    h = hsv[:,:,0]

    s[s[:] < 200] = 0

    # filter to red hue
    h[h[:] > 174 ] = 255
    h[h[:] < 5] = 255
    h[h[:] < 255] = 0

    r = frame.copy()[:,:,2]

    r[r[:] < 150] = 0

    test = h.copy()

    test[s[:] == 0] = 0
    test[r[:] < 50] = 0

    linesP = cv2.HoughLinesP(
            test, # Input edge image
            3, # Distance resolution in pixels
            np.pi/180, # Angle resolution in radians
            threshold=15, # Min number of votes for valid line
            minLineLength=15, # Min allowed length of line
            maxLineGap=1 # Max allowed gap between line for joining them
            )

    if linesP is not None:
        nlines = len(linesP)
    else:
        nlines = 0

    # print(nlines)

    if nlines > plines:
        print("Increase")

    lscore = .75 * lscore + .25 * nlines
    
    print(f'{lscore:.3f}, ({frame[10,0] / 2 + frame[10,1] / 2})\r', end='', flush=True)

    plines = nlines

    # if linesP is not None:
    #     for i in range(0, len(linesP)):
    #         l = linesP[i][0]
    #         cv2.line(test, (l[0], l[1]), (l[2], l[3]), (100), 1, cv2.LINE_AA)
    #         print(f'({l[0]}, {l[1]}), ({l[2]}, {l[3]})')

    cv2.imshow('H', h)

    cv2.imshow('S', s)

    cv2.imshow('T', test)


	# Display the resulting frame
    cv2.imshow('Frame', frame)

    # cv2.moveWindow('Frame', 40,30)


    # cv2.imshow('Thresh', Thresh)
	# define q as the exit button
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# release the video capture object
cap.release()
# Closes all the windows currently opened.
cv2.destroyAllWindows()

```

`combine_cnn_output.py`:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2
import sys


if(len(sys.argv) != 2):
    inputDir = "./hacks_data_tensor/"
    # directory = "./AA_On_tensor/"
else:
    inputDir = sys.argv[1]

if(not os.path.exists(inputDir)):
    print("Input directory is invalid.")


newTensors = []
for file in os.listdir(inputDir):
     filename = os.fsdecode(file)
     if filename.endswith(".pt"):
        newTensors.append(torch.load(inputDir+str(filename)))
    

fullTensor = torch.cat(newTensors)
print(fullTensor.shape)
torch.save(fullTensor, inputDir + "full_data/" + inputDir[:-1] + "_file.pt")

```

`extract_1_sec.py`:

```py
# from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip
from moviepy.editor import *
import os

directory = os.fsencode("./new_hacks")

for file in os.listdir(directory):
    filename = os.fsdecode(file)
    if filename.endswith(".mp4"): 
        # print(filename)

        clip = VideoFileClip("./new_hacks" + "/" + filename)
        duration = clip.duration
        if(duration > 1):
            clip = clip.subclip(clip.duration - 1, clip.duration).crop(x1=848,y1=428,x2=1072,y2=652)

            clip.write_videofile("./new_hacks" + "/" + "new_" + filename)

```

`old_test_rnn.py`:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2



class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


        # x -> (batch_size, seq_size, input_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        
        out, _ = self.lstm(x, (h0, c0))
        # out -> (batch_size, seq_size, input_size) = (N, 50, 512)
        out = out[:, -1, :]
        # out -> (N, 512)
        out = self.fc(out)


        return torch.sigmoid(out)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

num_classes = 1
num_epochs = 100

learning_rate = 0.001

input_size = 512
sequence_length = 50
hidden_size = 512
num_layers = 2

# model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)

model = torch.load("./models/model_max_data.pt")

# Loading the model
hacks_data = torch.load("./hacks_data_tensor/full_data/hacks_data_tensor_file_large.pt")
hacks_labels = torch.ones(hacks_data.shape[0]).unsqueeze(1)

no_hacks_data = torch.load("./no_hacks_data_tensor/full_data/no_hacks_data_tensor_file_large.pt")
no_hacks_labels = torch.zeros(no_hacks_data.shape[0]).unsqueeze(1)

hacks_data_train = hacks_data[:int(len(hacks_data) * 0.9)]
hacks_data_test = hacks_data[int(len(hacks_data) * 0.9):]

no_hacks_data_train = no_hacks_data[:int(len(no_hacks_data) * 0.9)]
no_hacks_data_test = no_hacks_data[int(len(no_hacks_data) * 0.9):]

hacks_labels_train = hacks_labels[:int(len(hacks_labels) * 0.9)]
hacks_labels_test = hacks_labels[int(len(hacks_labels) * 0.9):]

no_hacks_labels_train = no_hacks_labels[:int(len(no_hacks_labels) * 0.9)]
no_hacks_labels_test = no_hacks_labels[int(len(no_hacks_labels) * 0.9):]


# train_data = torch.cat((hacks_data_train, no_hacks_data_train))
# train_labels = torch.cat((hacks_labels_train, no_hacks_labels_train))

test_data = torch.cat((hacks_data_test, no_hacks_data_test))
test_labels = torch.cat((hacks_labels_test, no_hacks_labels_test))


model.eval()
with torch.no_grad():
    n_correct = 0
    n_samples = 0

    n_aimbot_correct = 0
    n_aimbot_incorrect = 0
    n_normal_correct = 0
    n_normal_incorrect = 0

    for i in range(len(test_data)):
        curr_test = test_data[i].unsqueeze(0)
        curr_label = test_labels[i][0].item()
        outputs = model(curr_test)

        if(abs(outputs.item()-curr_label) < 0.5):
            print("success" + str(outputs.item()))
            n_correct += 1

        if(outputs.item() > 0.5 and curr_label > 0.5):
            n_aimbot_correct += 1
        
        elif(outputs.item() > 0.5 and curr_label < 0.5):
            n_aimbot_incorrect += 1

        elif(outputs.item() <= 0.5 and curr_label <= 0.5):
            n_normal_correct += 1

        else:
            n_normal_incorrect += 1    

        n_samples += 1

print(n_samples)
print("percentage correct: " + str(n_correct/n_samples * 100.))
print("n_aimbot_correct: ", n_aimbot_correct)
print("n_aimbot_incorrect: ", n_aimbot_incorrect)
print("n_normal_correct: ", n_normal_correct)
print("n_normal_incorrect: ", n_normal_incorrect)
print("percentage correct: " + str((n_aimbot_correct+n_normal_correct)/(n_aimbot_correct+n_aimbot_incorrect+n_normal_correct+n_normal_incorrect) * 100.))
```

`predict.py`:

```py
"""
predict.py
Usage: python predict.py {Input Video} {(optional) destination folder}

Takes in an input video file and runs it through the entire process and prdiction pipeline

Video file must be 1920 x 1080 @ 60FPS
Video should contin gameplay of Counter Strike: Global Offensive
"""

import os
import sys
import random
import subprocess


# Ensure that there is an input file
assert len(sys.argv) >= 2, "Requires path to input file"
inFile = sys.argv[1]
assert os.path.isfile(inFile), f'{inFile} isn\'t a valid file path'

dir = "temp"
delDir = True

# allow a specified output directory
if len(sys.argv) > 2:
    dir = sys.argv[2]
    assert os.path.isdir(dir), f'{dir} isn\'t a valid dirctory'
    assert len(os.listdir(dir)) == 0, "folder must be empty"
    delDir = False

#  Make a new unique folder if none is specified
if delDir:
    while os.path.isdir(dir):
        dir = "temp" + str(random.randint(1,1000000))
    os.mkdir(dir)

# Run auto clip to get clips in folder
print(f'python auto_clip.py \'{inFile}\' \'{dir}\' 1')
subprocess.run(['python', './auto_clip.py', inFile, dir, '1'])

# process clips from auto_clip with cnn to extract features
print('\n\n\n\n\nNow extracting features')
subprocess.run(['python', 'save_cnn_output.py', dir, dir])

# run extracted features through rnn to classify
print('\n\n\n\n\nNow Predicting value of cheats')
subprocess.run(['python', 'test_rnn.py', os.path.join(dir, 'clips.pt'), dir])



```

`save_cnn_output.py`:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2
import sys
import re

if(len(sys.argv) != 3):
    framesDir = "./no_hacks_data_nn"
    outputDir = "./no_hacks_data_tensor"
    

else:
    framesDir = sys.argv[1]
    outputDir = sys.argv[2]

if(not os.path.exists(framesDir)):
    print("Frames directory is invalid.")
    sys.exit()

if(not os.path.exists(outputDir)):
    print("Output directory is invalid.")
    sys.exit()

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)
model.eval()
model.fc = nn.Identity()

allVideos = []

subfolders = [ f for f in os.scandir(framesDir) if f.is_dir() ]

# subfolders = sorted(subfolders, key=lambda x: float(re.findall(r'[\d\.]+', x.name)[-1]) + 60 * float(re.findall(r'[\d\.]+', x.name)[-2]))

# print(float(re.findall(r'[\d\.]+', subfolders[0].name)[-1]))

for folder in subfolders:
    l = len([f for f in os.scandir(folder.path) if f.is_file()] )
    if l != 60:
        print(l, folder.name)

with torch.no_grad():
    for folder in subfolders:
        print(f'Extracting Features from clip at {(folder.name).replace("_", ":")}\t\t\t\n', end='', flush=True)

        frames = [ f for f in os.scandir(folder.path) if f.is_file() ]

        assert len(frames) == 60, f'{folder.name} has an incorrect number of frames ({len(frames)})'

        frames = sorted(frames, key=lambda x: int(re.findall(r'\d+', x.name)[0]))

        video = []

        for frame in frames:
            image = cv2.imread(frame.path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            transform = transforms.ToTensor()
            tensor = transform(image)
            tensor = tensor.unsqueeze(0)  
            frameFeatures = model(tensor)
            video.append(frameFeatures.squeeze())

        video = torch.stack(video)
        allVideos.append(video)

print("\nSaving Features")
allVideos = torch.stack(allVideos)
print(allVideos.shape)
torch.save(allVideos, os.path.join(outputDir, "clips.pt"))
print("Features Saved!")

```

`test_rnn.py`:

```py
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2

assert len(sys.argv) >= 2, "File requires input path"

inFile = sys.argv[1]
assert os.path.isfile(inFile), "not a valid file"

inDir = False

if len(sys.argv) > 2:
    inDir = sys.argv[2]
    assert os.path.isdir(inDir), "not a valid directory"



# Defining LSTM RNN
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()     # inheriting from existing RNN class
        self.num_layers = num_layers    # number of input layers
        self.hidden_size = hidden_size  # number of hidden players

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # creating LSTM layer
        self.fc = nn.Linear(hidden_size, num_classes)                               # creating linear output layer

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


        # x -> (batch_size, seq_size, input_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        
        out, _ = self.lstm(x, (h0, c0))
        # out -> (batch_size, seq_size, input_size) = (N, 50, 512)
        out = out[:, -1, :]
        # out -> (N, 512)
        out = self.fc(out)


        return torch.sigmoid(out) # returning one forward step of the NN

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = torch.load("./models/model.pt")

data = torch.load(inFile)

times = []

if inDir:
    times = [ (f.name).replace("_", ":") for f in os.scandir(inDir) if f.is_dir() ]
    assert len(times) == data.shape[0], "number of clips doesnt match feature data"


model.eval()
with torch.no_grad():

    for i in range(len(data)):
        curr_test = data[i].unsqueeze(0)
        output = model(curr_test)
        output = output.item()

        print(f'Clip at {times[i]}: {"Regular Gameplay Detected" if output < 0.5 else "Aimbot Detected"}')

```

`train_rnn.py`:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2


# Defining LSTM RNN
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()     # inheriting from existing RNN class
        self.num_layers = num_layers    # number of input layers
        self.hidden_size = hidden_size  # number of hidden players

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # creating LSTM layer
        self.fc = nn.Linear(hidden_size, num_classes)                               # creating linear output layer

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


        # x -> (batch_size, seq_size, input_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        
        out, _ = self.lstm(x, (h0, c0))
        # out -> (batch_size, seq_size, input_size) = (N, 50, 512)
        out = out[:, -1, :]
        # out -> (N, 512)
        out = self.fc(out)


        return torch.sigmoid(out) # returning one forward step of the NN

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyper-parameters 
num_classes = 1
num_epochs = 100

learning_rate = 0.001

input_size = 512
sequence_length = 50
hidden_size = 512
num_layers = 2

model = RNN(input_size, hidden_size, num_layers, num_classes).to(device) # Creating instance of LSTM model

# Loss and optimizer
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  

# Loading the model
hacks_data = torch.load("./hacks_data_tensor/clips.pt")
hacks_labels = torch.ones(hacks_data.shape[0]).unsqueeze(1)

no_hacks_data = torch.load("./no_hacks_data_tensor/clips.pt")
no_hacks_labels = torch.zeros(no_hacks_data.shape[0]).unsqueeze(1)


# Seperating Training/Testing data into 90%/10% splits
hacks_data_train = hacks_data[:int(len(hacks_data) * 0.9)]
hacks_data_test = hacks_data[int(len(hacks_data) * 0.9):]

no_hacks_data_train = no_hacks_data[:int(len(no_hacks_data) * 0.9)]
no_hacks_data_test = no_hacks_data[int(len(no_hacks_data) * 0.9):]

hacks_labels_train = hacks_labels[:int(len(hacks_labels) * 0.9)]
hacks_labels_test = hacks_labels[int(len(hacks_labels) * 0.9):]

no_hacks_labels_train = no_hacks_labels[:int(len(no_hacks_labels) * 0.9)]
no_hacks_labels_test = no_hacks_labels[int(len(no_hacks_labels) * 0.9):]


train_data = torch.cat((hacks_data_train, no_hacks_data_train))
train_labels = torch.cat((hacks_labels_train, no_hacks_labels_train))


test_data = torch.cat((hacks_data_test, no_hacks_data_test))
test_labels = torch.cat((hacks_labels_test, no_hacks_labels_test))

model.train() # set model to training mode 
for epoch in range(num_epochs):
    images = train_data         # using our set of images
    labels = train_labels       # using our set of labels
    labels = labels.to(device)  # uploading onto CPU/GPU

    random_shuffle = torch.randperm(images.size()[0])  # shuffling the data for every epoch
    images = images[random_shuffle]
    labels = labels[random_shuffle]
    
    # Forward pass
    outputs = model(images)     # perform a forward pass 
    loss = criterion(outputs, labels) # calculate loss/error
    
    # Backward and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') # print results for every epoch

    torch.save(model, "./models/model.pt") # incrementally save model

```

`validate_rnn.py`:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2



# Defining LSTM RNN
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()     # inheriting from existing RNN class
        self.num_layers = num_layers    # number of input layers
        self.hidden_size = hidden_size  # number of hidden players

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # creating LSTM layer
        self.fc = nn.Linear(hidden_size, num_classes)                               # creating linear output layer

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


        # x -> (batch_size, seq_size, input_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)
        
        out, _ = self.lstm(x, (h0, c0))
        # out -> (batch_size, seq_size, input_size) = (N, 50, 512)
        out = out[:, -1, :]
        # out -> (N, 512)
        out = self.fc(out)


        return torch.sigmoid(out) # returning one forward step of the NN

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

num_classes = 1
num_epochs = 100

learning_rate = 0.001

input_size = 512
sequence_length = 50
hidden_size = 512
num_layers = 2

# model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)

model = torch.load("./models/model.pt")

# Loading the model
hacks_data = torch.load("./hacks_data_tensor/clips.pt")
hacks_labels = torch.ones(hacks_data.shape[0]).unsqueeze(1)

no_hacks_data = torch.load("./no_hacks_data_tensor/clips.pt")
no_hacks_labels = torch.zeros(no_hacks_data.shape[0]).unsqueeze(1)

hacks_data_train = hacks_data[:int(len(hacks_data) * 0.9)]
hacks_data_test = hacks_data[int(len(hacks_data) * 0.9):]

no_hacks_data_train = no_hacks_data[:int(len(no_hacks_data) * 0.9)]
no_hacks_data_test = no_hacks_data[int(len(no_hacks_data) * 0.9):]

hacks_labels_train = hacks_labels[:int(len(hacks_labels) * 0.9)]
hacks_labels_test = hacks_labels[int(len(hacks_labels) * 0.9):]

no_hacks_labels_train = no_hacks_labels[:int(len(no_hacks_labels) * 0.9)]
no_hacks_labels_test = no_hacks_labels[int(len(no_hacks_labels) * 0.9):]


# train_data = torch.cat((hacks_data_train, no_hacks_data_train))
# train_labels = torch.cat((hacks_labels_train, no_hacks_labels_train))

test_data = torch.cat((hacks_data_test, no_hacks_data_test))
test_labels = torch.cat((hacks_labels_test, no_hacks_labels_test))

print(test_data.shape)

model.eval() # Model is set to evaluate

# Checking for whether clips are accurately predicted or not
with torch.no_grad():
    n_correct = 0
    n_samples = 0

    for i in range(len(test_data)):
        curr_test = test_data[i].unsqueeze(0)
        curr_label = test_labels[i][0].item()
        outputs = model(curr_test)

        if(abs(outputs.item()-curr_label) < 0.5):
            print("success" + str(outputs.item()))
            n_correct += 1

        n_samples += 1

print(n_samples)
print("percentage correct: " + str(n_correct/n_samples * 100.))
```

`video_to_photo.py`:

```py
import cv2
import os
import logging


def read_video(input_filename):
    return cv2.VideoCapture(input_filename)


def save_frame(count, sec, vid_cap, output_directory):
    vid_cap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)
    hasFrames, frame = vid_cap.read()

    if hasFrames:
        name = os.path.join(os.getcwd(), output_directory, "frame" + str(count) + ".png")
        cv2.imwrite(name, frame)

    return hasFrames


def get_frames(input_filename, output_directory, frameRate):
    """
    Capture images from a video at every (i.e, 5 sec, 5 mn, etc.)
    :param frameRate: time we want to capture the images, in seconds.
    """
    try:
        if not os.path.exists(output_directory):
            os.makedirs(output_directory)
    except OSError:
        logging.error('Error creating directory')

    sec = 0
    count = 1
    vid_cap = read_video(input_filename)
    success = save_frame(count, sec, vid_cap, output_directory)

    while success:
        count += 1
        sec = sec + frameRate
        sec = round(sec, 2)
        success = save_frame(count, sec, vid_cap, output_directory)


def save_frames(file_name, out_dir, start=0, end=-1):
    cap = cv2.VideoCapture(file_name)

    assert cap, "path to file must be valid"

    try:
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)
    except OSError:
        logging.error('Error creating directory')


    if end == -1:
        end = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    for fno in range(start, end):
        cap.set(cv2.CAP_PROP_POS_FRAMES, fno)
        success, frame = cap.read()
        assert success, "frame not read correctly"

        name = os.path.join(os.getcwd(), out_dir, "frame" + str(fno + 1) + ".png")
        cv2.imwrite(name, frame)

    
    cap.release()


	

if __name__ == "__main__":
    directory = "./hacks_data/"
    i = 0
    for file in os.listdir(directory):
        filename = os.fsdecode(file)
        if filename.endswith(".mp4"): 
            print(os.path.join(directory, filename))
            # get_frames(os.path.join(directory, filename), ("./hacks_data_nn/" + str(i) + "/"), 1/60)
            save_frames(os.path.join(directory, filename), ("./hacks_data_nn/" + str(i) + "/"))
            i += 1
```