Project Path: arc_gmh5225_pdb-rs_2dzk_npr

Source Tree:

```txt
arc_gmh5225_pdb-rs_2dzk_npr
├── CODE_OF_CONDUCT.md
├── Cargo.toml
├── LICENSE
├── README.md
├── SECURITY.md
├── SUPPORT.md
├── msf
│   ├── Cargo.toml
│   └── src
│       ├── check.rs
│       ├── commit.rs
│       ├── lib.rs
│       ├── open.rs
│       ├── pages.rs
│       ├── read.rs
│       ├── stream_reader.rs
│       ├── stream_writer.rs
│       ├── tests.rs
│       └── write.rs
├── msfz
│   ├── Cargo.toml
│   └── src
│       ├── compress_utils.rs
│       ├── lib.rs
│       ├── msfz.md
│       ├── reader.rs
│       ├── stream_data.rs
│       ├── tests.rs
│       └── writer.rs
├── pdb
│   ├── Cargo.toml
│   ├── src
│   │   ├── container.rs
│   │   ├── dbi
│   │   │   ├── modules.rs
│   │   │   ├── optional_dbg.rs
│   │   │   ├── section_contrib.rs
│   │   │   ├── section_map.rs
│   │   │   └── sources.rs
│   │   ├── dbi.rs
│   │   ├── embedded_sources.rs
│   │   ├── encoder.rs
│   │   ├── globals
│   │   │   ├── gsi.rs
│   │   │   ├── gss.rs
│   │   │   ├── name_table
│   │   │   │   └── tests.rs
│   │   │   ├── name_table.rs
│   │   │   ├── psi.rs
│   │   │   └── tests.rs
│   │   ├── globals.rs
│   │   ├── guid.rs
│   │   ├── hash.rs
│   │   ├── lib.rs
│   │   ├── lines
│   │   │   ├── checksum.rs
│   │   │   └── subsection.rs
│   │   ├── lines.rs
│   │   ├── modi.rs
│   │   ├── names
│   │   │   └── tests.rs
│   │   ├── names.rs
│   │   ├── parser
│   │   │   └── tests.rs
│   │   ├── parser.rs
│   │   ├── pdbi
│   │   │   └── tests.rs
│   │   ├── pdbi.rs
│   │   ├── stream_index.rs
│   │   ├── syms
│   │   │   ├── builder.rs
│   │   │   ├── iter.rs
│   │   │   ├── kind.rs
│   │   │   └── offset_segment.rs
│   │   ├── syms.rs
│   │   ├── taster.rs
│   │   ├── tpi
│   │   │   └── hash.rs
│   │   ├── tpi.rs
│   │   ├── types
│   │   │   ├── fields.rs
│   │   │   ├── iter.rs
│   │   │   ├── kind.rs
│   │   │   ├── number.rs
│   │   │   ├── primitive.rs
│   │   │   ├── records.rs
│   │   │   └── visitor.rs
│   │   ├── types.rs
│   │   ├── utils
│   │   │   ├── align.rs
│   │   │   ├── io.rs
│   │   │   ├── iter.rs
│   │   │   ├── path.rs
│   │   │   ├── swizzle.rs
│   │   │   └── vec.rs
│   │   ├── utils.rs
│   │   └── writer.rs
│   └── tests
│       ├── cpp_check
│       │   └── types.cpp
│       └── cpp_check.rs
└── pdbtool
    ├── Cargo.toml
    └── src
        ├── addsrc.rs
        ├── copy.rs
        ├── counts.rs
        ├── dump
        │   ├── lines.rs
        │   ├── names.rs
        │   ├── sources.rs
        │   ├── streams.rs
        │   ├── sym.rs
        │   └── types.rs
        ├── dump.rs
        ├── dump_utils.rs
        ├── find.rs
        ├── glob_pdbs.rs
        ├── hexdump.rs
        ├── main.rs
        ├── pdz
        │   ├── encode.rs
        │   └── util.rs
        ├── pdz.rs
        ├── save.rs
        └── util.rs

```

`CODE_OF_CONDUCT.md`:

```md
# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

```

`Cargo.toml`:

```toml
[workspace]
resolver = "2"

members = [
    "msf",
    "msfz",
    "pdb",
    "pdbtool",
]

[workspace.dependencies]
anyhow = "1.0.71"
bitfield = "0.14.0"
bitflags = "2.3.2"
bitvec = "1"
bstr = "1.8.0"
bumpalo = "3.13.0"
cc = "1.0.79"
clap = "4.5.27"
dbg-ranges = "0.1.0"
flate2 = "1.0.27"
pow2 = "0.1.1"
pretty-hex = "0.4.1"
static_assertions = "1.0"
static_init = "1.0.3"
sync_file = "0.2.6"
tracing = "0.1.41"
tracing-subscriber = "0.3.19"
tracing-tracy = "0.11.4"
uuid = "1.4.0"
zerocopy = "0.8.14"
zerocopy-derive = "0.8.14"
zstd = "0.13.2"

mspdb = { path = "mspdb" }

[profile.release]
debug = 2

# This compiles external packages with optimizations. Compression libraries
# especially sensitive to optimization. This makes it a lot easier to do
# development and run tests with local code in debug mode but external dependencies
# fully optimized.
[profile.dev.package."*"]
opt-level = 2

[profile.coverage]
inherits = "dev"

```

`LICENSE`:

```
    MIT License

    Copyright (c) Microsoft Corporation.

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE

```

`README.md`:

```md
# PDB tools

This repository contains libraries and tools for working with Microsoft Program
Database (PDB) files. All of the code is in Rust.

* The `ms-pdb-msf` crate contains code for reading, creating, and modifying PDB
  files that use the MSF container format. Currently, all PDBs produced by
  Microsoft tools use the MSF container format.
  
  This is a lower-level building block for PDBs, and most developers will never
  need to directly use the `ms-pdb-msf` crate. Instead, they should use the
  `ms-pdb` crate.

* The `ms-pdb-msfz` crate contains code for reading and writing PDB files that
  use the MSFZ container format. MSFZ is a new container format that is
  optimized for "cold storage"; PDB/MSFZ files cannot be modified in-place in
  the way PDB/MSF files can, but MSFZ files use an efficient form of compression
  that allows data to be accessed without decompressing the entire file. MSFZ is
  intended to be a format for storing PDBs, not for local development.

  Most developers will not need to use the `ms-pdb-msfz` crate directly.
  Instead, they should use the `ms-pdb` crate.

* The `ms-pdb` crate supports reading, creating, and modifying PDB files. It
  builds on the `ms-pdb-msf` and `ms-pdb-msfz` crate. The `ms-pdb-msf` and
  `ms-pdb-msfz` crates provide the container format for PDB, but they do not
  contain any code for working with the contents of PDBs. That is the job of the
  `ms-pdb` crate -- it provides methods for reading specific PDB data
  structures, such as debug symbols, line mappings, module records, etc.

## All information in this implementation is based on publicly-available information

With the exception of MSFZ, this implementation is based solely on public
sources that describe the PDB and MSF data structures. This repository does not
contain any confidential Microsoft intellectual property.

## MSFZ describes an **experimental** data format and is subject to change without notice

The MSFZ specification in this repository describes an experimental container
format for PDBs. **Microsoft makes no commitment to the stability or support for
MSFZ.** The MSFZ format may be changed or discontinued at any time, without any
notice or obligation for Microsoft to take any action. At some future point,
Microsoft _may_ make a support commitment to MSFZ (possibly with incompatible
changes made to it), but this repository does not imply any commitment or
agreement to do so.

## **THIS IMPLEMENTATION IS NOT AUTHORITATIVE AND IS NOT A REFERENCE IMPLEMENTATION**

This implementation is **NOT** an authoritative reference. It may contain
defects or inaccuracies. As the `LICENSE` states, this implementation is
provided "as is", without warranty of any kind. Specifically, this
implementation **DOES NOT** make any guarantees about compatibility or
interoperability with any other toolset, including (but not limited to)
Microsoft Visual C++ (MSVC) and Clang.

The authors of this effort may make a good-faith effort to fix bugs, including
bugs discovered by the authors or the community. However, as the license states,
this repository is provided "as is" and there is absolutely no obligation or
expectation on levels of service for the implementation provided in this
repository.

## Contributing

This project welcomes contributions and suggestions. Most contributions require
you to agree to a Contributor License Agreement (CLA) declaring that you have
the right to, and actually do, grant us the rights to use your contribution. For
details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether
you need to provide a CLA and decorate the PR appropriately (e.g., status check,
comment). Simply follow the instructions provided by the bot. You will only need
to do this once across all repos using our CLA.

This project has adopted the
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any
additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or
services. Authorized use of Microsoft trademarks or logos is subject to and must
follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must
not cause confusion or imply Microsoft sponsorship. Any use of third-party
trademarks or logos are subject to those third-party's policies.

## Contacts

* `sivadeilra` on GitHub
* Arlie Davis ardavis@microsoft.com

```

`SECURITY.md`:

```md
<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). 

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->

```

`SUPPORT.md`:

```md
# Unsupported open source

This repository contains open-source code. It is shared freely, but without any support
obligation whatsoever from Microsoft. As the `LICENSE` file specifies, Microsoft does not
make any claims about the suitability, reliability, or accuracy of the code in this repository.

```

`msf/Cargo.toml`:

```toml
[package]
name = "ms-pdb-msf"
version = "0.1.0"
edition = "2021"
description = "Reads Multi-Stream Files, which are used in the Microsoft Program Database (PDB) file format"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[lib]
doctest = false

[dependencies]
anyhow.workspace = true
bitvec.workspace = true
pow2.workspace = true
static_assertions.workspace = true
sync_file.workspace = true
tracing.workspace = true
uuid.workspace = true
zerocopy = { workspace = true, features = ["alloc", "derive"] }
zerocopy-derive.workspace = true

[dev-dependencies]
dbg-ranges = "0.1.0"
pretty-hex = "0.4.1"
static_init.workspace = true
tracing-subscriber = { workspace = true, features = ["fmt"] }

```

`msf/src/check.rs`:

```rs
//! Consistency checks for MSF, both in-memory and on-disk

use super::*;

impl<F> Msf<F> {
    #[cfg(not(test))]
    #[inline(always)]
    pub(super) fn assert_invariants(&self) {}

    #[cfg(test)]
    #[inline(never)]
    pub(super) fn assert_invariants(&self) {
        // There is always at least one stream, because stream 0 is special.
        assert!(!self.stream_sizes.is_empty());

        // This is implied by stream_sizes not being empty.
        assert!(self.committed_stream_page_starts.len() >= 2);

        // The pages assigned to Stream 0 are marked "free" and are not marked "deleted".
        {
            let stream0_pages = &self.committed_stream_pages[self.committed_stream_page_starts[0]
                as usize
                ..self.committed_stream_page_starts[1] as usize];

            for &page in stream0_pages.iter() {
                assert!(self.pages.fpm[page as usize]);
                assert!(!self.pages.fpm_freed[page as usize]);
            }
        }

        // The pages assigned to all streams in the committed state are disjoint.
        // The pages assigned to all streams are < num_pages.
        {
            let mut busy_pages: BitVec<u32, Lsb0> = BitVec::new();
            busy_pages.resize(self.pages.num_pages as usize, false);
            for &page in self.committed_stream_pages.iter() {
                assert!(page <= self.pages.num_pages, "page {page} is out of range",);
                assert!(
                    !busy_pages[page as usize],
                    "page {page} is used by more than one stream",
                );
                assert!(
                    !self.pages.fresh[page as usize],
                    "page {page} cannot be fresh if it is used by a committed stream"
                );

                busy_pages.set(page as usize, true);
            }
        }

        // All entries in modified_streams have a stream index that is valid.
        for &stream_index in self.modified_streams.keys() {
            assert!(stream_index < self.stream_sizes.len() as u32);
        }
    }
}

```

`msf/src/commit.rs`:

```rs
//! Commits all pending changes to an MSF file.

use super::*;
use anyhow::Result;
use tracing::{debug, debug_span, info, info_span, trace, trace_span};

impl<F: ReadAt + WriteAt> Msf<F> {
    /// Commits all changes to the MSF file to disk.
    ///
    /// The MSF file format is designed to support a very limited form of transactional commits.
    /// A single block write to the first page (located at the start of the file) can atomically
    /// commit all outstanding modifications to the MSF file.
    ///
    /// This commit design does require that the underlying operating system handle the write to
    /// page 0 in a single operation.  Operating systems generally don't _contractually_
    /// guarantee atomicity of such writes, but in practice it's true enough to be reasonably
    /// reliable. The commit functionality of MSF is really designed to guard against application
    /// failures, not failures of the operating system or its storage stack. So don't rely on
    /// `commit()` for anything more than best-effort service.
    ///
    /// This `commit()` implementation _does not_ permit multiple concurrent writers (or concurrent
    /// readers and writers) to the underlying MSF/PDB file. If you manage to circumvent this,
    /// you may damage the underlying MSF/PDB file.
    ///
    /// This `commit()` implementation does not buffer modifications to stream contents. All
    /// writes to stream contents are handled by immediately writing the data to the underlying
    /// MSF file. However, these writes do not _overwrite_ the data stored in the stream; instead,
    /// new pages are allocated in the file for the new or modified stream contents. If an
    /// application deletes a range of a stream (by resizing it), then the existing pages are
    /// protected and cannot be overwritten until `commit()` is called.
    ///
    /// Commit operations are moderately expensive. Applications that modify PDBs should generally
    /// perform a single commit operation, not a series of commit operations. Each call to
    /// `commit()` writes a new copy of the Stream Directory (the list of streams and their page
    /// numbers) to disk. Because the existing Stream Directory cannot be overwritten (until
    /// _after_ the `commit()` operation completes), this means that all `commit()` operations
    /// require that two complete copies of the Stream Directory persist on disk.
    ///
    /// While it would be possible to reduce the size of an MSF file (after a commit completes) by
    /// trimming the unused pages at the end of the MSF file, in practice this does not help much
    /// because of page fragmentation. For this reason, `commit()` never reduces the size of the
    /// underlying MSF file.
    ///
    /// Returns `Ok(true)` if this `Msf` contained uncommitted changes and these changes have now
    /// been committed.
    ///
    /// Returns `Ok(false)` if this `Msf` did not contain any uncomitted changes. In this case,
    /// no `write()` calls are issued to the underlying storage.
    ///
    /// If this function returns `Err`, then the underlying MSF file may have had new pages written,
    /// but the existing Stream Directory and header page should be intact. However, if the
    /// underlying operating system did not write Page 0 atomically, then the underlying MSF
    /// file may be irrecoverably damaged.
    ///
    /// Also, if this function returns `Err`, the in-memory data structures that represent the
    /// state of the `Msf` editor are not guaranteed to be in a consistent state.
    pub fn commit(&mut self) -> Result<bool> {
        let _span = info_span!("Msf::commit").entered();

        self.assert_invariants();

        // If this was not opened for write access then there are no pending changes at all.
        if self.access_mode != AccessMode::ReadWrite {
            info!("this Msf is not opened for read-write access");
            debug_assert!(self.modified_streams.is_empty());
            return Ok(false);
        };

        // We only support modifying Big MSF files.
        assert_eq!(self.kind, MsfKind::Big);

        // If no streams have been modified, then there is nothing to do.
        if self.modified_streams.is_empty() {
            info!("there are no modified streams; nothing to commit");
            return Ok(false);
        }

        let new_fpm_number: u32 = match self.active_fpm {
            FPM_NUMBER_1 => FPM_NUMBER_2,
            FPM_NUMBER_2 => FPM_NUMBER_1,
            _ => panic!("Active FPM has invalid value"),
        };
        info!(
            old_fpm = self.active_fpm,
            new_fpm = new_fpm_number,
            "beginning commit"
        );

        let stream_dir_info = self.write_new_stream_dir()?;

        // NOTE: The call to merge_freed_into_free irreversibly alters state. If we fail after
        // this point, then Msf will be left in an inconsistent state. This could be improved by
        // building a new FPM vector in-memory without modifying any state.
        self.pages.merge_freed_into_free();
        fill_last_word_of_fpm(&mut self.pages.fpm);

        self.write_fpm(new_fpm_number)?;

        let page_size = self.pages.page_size;
        let page_size_usize = usize::from(page_size);

        // Build the new Page 0.
        let mut page0: Vec<u8> = vec![0; page_size_usize];

        let msf_header = MsfHeader {
            magic: MSF_BIG_MAGIC,
            page_size: U32::new(u32::from(page_size)),
            active_fpm: U32::new(new_fpm_number),
            num_pages: U32::new(self.pages.num_pages),
            stream_dir_size: U32::new(stream_dir_info.dir_size),
            stream_dir_small_page_map: U32::new(0),
            // The stream directory page map pointers follows the MsfHeader.
        };
        msf_header.write_to_prefix(page0.as_mut_slice()).unwrap();

        // Copy the stream dir page map into Page 0.
        let page_map_pages_bytes = stream_dir_info.map_pages.as_bytes();
        page0[STREAM_DIR_PAGE_MAP_FILE_OFFSET as usize..][..page_map_pages_bytes.len()]
            .copy_from_slice(page_map_pages_bytes);

        // ------------------------ THE BIG COMMIT ----------------------

        info!("writing MSF File Header");
        self.file.write_all_at(page0.as_bytes(), 0)?;

        // After this point, _nothing can fail_.
        // Any operation that could have failed should have been moved above the commit point.

        // --------------------- CLEANUP AFTER THE COMMIT ---------------

        // Update in-memory state to reflect the commit.
        //
        // This code runs after we write the new Page 0 to disk. That commits the changes to the
        // PDB. This function modifies in-memory state to reflect the successful commit. For this
        // reason, after this point, this function must NEVER return a failure code.
        {
            // Build the new in-memory stream directory. This is very similar to the version that we
            // just wrote to disk, so maybe we should unify the two.

            let _span = trace_span!("post_commit").entered();

            let page_size = self.pages.page_size;

            // We can easily determine the right size for allocating 'stream_pages'.
            let mut num_stream_pages: usize = 0;
            for &stream_size in self.stream_sizes.iter() {
                if stream_size != NIL_STREAM_SIZE {
                    num_stream_pages += num_pages_for_stream_size(stream_size, page_size) as usize;
                }
            }

            let mut stream_pages: Vec<Page> = Vec::with_capacity(num_stream_pages);
            let mut stream_page_starts: Vec<u32> = Vec::with_capacity(self.stream_sizes.len() + 1);

            for (stream, &stream_size) in self.stream_sizes.iter().enumerate() {
                stream_page_starts.push(stream_pages.len() as u32);

                if stream_size == NIL_STREAM_SIZE {
                    trace!(stream, "stream is nil");
                    continue;
                }

                let num_stream_pages = num_pages_for_stream_size(stream_size, page_size) as usize;

                // If this stream has been modified, then return the modified page list.
                let is_modified;
                let pages: &[Page] =
                    if let Some(pages) = self.modified_streams.get(&(stream as u32)) {
                        is_modified = true;
                        pages
                    } else {
                        is_modified = false;
                        let start = self.committed_stream_page_starts[stream] as usize;
                        &self.committed_stream_pages[start..start + num_stream_pages]
                    };
                assert_eq!(num_stream_pages, pages.len());

                trace!(stream, stream_size, num_stream_pages, is_modified);

                stream_pages.extend_from_slice(pages);
            }

            stream_page_starts.push(stream_pages.len() as u32);

            // Now that we have written the Stream Directory (and the map pages, above it), we
            // need to mark the pages that contain the Stream Directory (and the map pages) as
            // *freed*.  Not free, but *freed*.  Fortunately, we still have this information, since
            // we built it above when we called write_new_stream_dir().
            //
            // The multiple_commits() and many_commits() tests verify this.
            {
                trace!("marking stream dir pages as free");

                for list in [
                    stream_dir_info.dir_pages.as_slice(),
                    stream_dir_info.map_pages.as_slice(),
                ] {
                    for &p in list.iter() {
                        let pi = p.get() as usize;
                        assert!(!self.pages.fpm_freed[pi]);
                        assert!(!self.pages.fpm[pi]);
                        self.pages.fpm_freed.set(pi, true);
                    }
                }
            }

            // Update state
            self.committed_stream_pages = stream_pages;
            self.committed_stream_page_starts = stream_page_starts;
            self.modified_streams.clear();

            self.pages.fresh.set_elements(0);
            self.pages.next_free_page_hint = 3; // positioned after file header and FPM1 and FPM2

            trace!(new_fpm_number, "setting active FPM");
            self.active_fpm = new_fpm_number;
        }

        info!("commit complete");

        self.assert_invariants();

        Ok(true)
    }

    /// Builds the new stream directory.
    fn build_new_stream_dir(&self) -> Vec<U32<LE>> {
        let page_size = self.pages.page_size;

        let num_streams = self.stream_sizes.len();

        let mut stream_dir: Vec<U32<LE>> = Vec::new();
        stream_dir.push(U32::new(num_streams as u32));

        // Push a size of 0 for Stream 0.
        stream_dir.push(U32::new(0));

        for &stream_size in self.stream_sizes[1..].iter() {
            stream_dir.push(U32::new(stream_size));
        }

        for (stream, &stream_size) in self.stream_sizes.iter().enumerate() {
            if stream_size == NIL_STREAM_SIZE {
                debug!(stream, "stream is nil");
                continue;
            }

            let num_stream_pages = num_pages_for_stream_size(stream_size, page_size) as usize;

            // If this stream has been modified, then return the modified page list.
            let pages: &[Page] = if let Some(pages) = self.modified_streams.get(&(stream as u32)) {
                pages
            } else {
                let start = self.committed_stream_page_starts[stream] as usize;
                &self.committed_stream_pages[start..start + num_stream_pages]
            };
            assert_eq!(num_stream_pages, pages.len());
            debug!(stream, stream_size);

            stream_dir.reserve(pages.len());
            for &p in pages.iter() {
                stream_dir.push(U32::new(p));
            }
        }

        stream_dir
    }

    /// Builds the new stream directory and writes it to disk.
    ///
    /// This builds the stream directory and the page map pages and writes it to disk. It returns
    /// the size in bytes of the stream directory and the page numbers of the page map.
    fn write_new_stream_dir(&mut self) -> anyhow::Result<StreamDirInfo> {
        let _span = debug_span!("Msf::write_new_stream_dir").entered();

        let page_size = self.pages.page_size;
        let page_size_usize = usize::from(page_size);

        // "Dir" pages contain the contents of the Stream Directory.
        // "Map" pages contain pointers to "dir" pages. They are "above" the dir pages.

        let stream_dir = self.build_new_stream_dir();
        let stream_dir_bytes = stream_dir.as_bytes();

        let mut reusable_page_data: Vec<u8> = vec![0; usize::from(page_size)];

        // The number of pages needed to store the Stream Directory.
        let num_stream_dir_pages =
            num_pages_for_stream_size(stream_dir_bytes.len() as u32, page_size) as usize;
        let mut dir_pages: Vec<U32<LE>> = Vec::with_capacity(num_stream_dir_pages);

        for stream_dir_chunk in stream_dir_bytes.chunks(page_size_usize) {
            // Allocate a page for the next stream dir page.
            let page = self.pages.alloc_page();
            dir_pages.push(U32::new(page));

            let page_bytes = if stream_dir_chunk.len() == page_size_usize {
                // It's a complete page, so there is no need for the bounce buffer.
                stream_dir_chunk
            } else {
                let (lo, hi) = reusable_page_data.split_at_mut(stream_dir_chunk.len());
                lo.copy_from_slice(stream_dir_chunk);
                hi.fill(0);
                reusable_page_data.as_slice()
            };

            let page_offset = page_to_offset(page, page_size);
            debug!(page, page_offset, "writing stream dir page");
            self.file.write_all_at(page_bytes, page_offset)?;
        }

        // Now we build the next level of indirection (the "page map"), and allocate pages for them
        // and write them.
        let mut map_pages: Vec<U32<LE>> = Vec::new();

        let num_u32s_per_page = u32::from(page_size) / 4;
        for map_page_contents in dir_pages.chunks(num_u32s_per_page as usize) {
            let map_page_index = self.pages.alloc_page();
            let map_file_offset = page_to_offset(map_page_index, page_size);
            let map_page_bytes = map_page_contents.as_bytes();
            let (lo, hi) = reusable_page_data.split_at_mut(map_page_bytes.len());
            lo.copy_from_slice(map_page_bytes);
            hi.fill(0);

            debug!(
                map_page_index,
                map_file_offset, "writing stream dir page map page"
            );
            self.file
                .write_all_at(&reusable_page_data, map_file_offset)?;

            map_pages.push(U32::new(map_page_index));
        }

        Ok(StreamDirInfo {
            dir_size: stream_dir_bytes.len() as u32,
            dir_pages,
            map_pages,
        })
    }

    /// Writes the FPM for the new transaction state.
    fn write_fpm(&mut self, new_fpm_number: u32) -> anyhow::Result<()> {
        let _span = debug_span!("write_fpm").entered();

        let page_size = self.pages.page_size;
        let page_size_usize = usize::from(page_size);
        let num_intervals = self.pages.num_pages.div_round_up(page_size);

        assert_eq!(self.pages.num_pages as usize, self.pages.fpm.len());
        let fpm_words: &[u32] = self.pages.fpm.as_raw_slice();
        let fpm_bytes: &[u8] = fpm_words.as_bytes();

        // This iterates the contents of the pages of the FPM. Each item iterated is a &[u8]
        // containing the piece of the FPM that should be written to a single on-disk page.
        // The last page iterated can be a partial (incomplete) page.
        //
        // For example: page_size = 4096, so there are 4096 bytes in each FPM page within
        // an interval.  That means there are 4096 * 8 bits in each FPM page, or 32,768 bits.
        // These bits cover _much_ more than a single interval; each FPM page covers 8
        // intervals worth of pages.
        //
        // This is basically a bug in the design of the FPM; the FPM is 8x larger than it
        // needs to be. But the design is frozen, so we must do it this way.

        let mut fpm_pages_data_iter = fpm_bytes.chunks(page_size_usize);

        // This is a buffer where we assemble complete FPM pages before writing them to disk.
        // This ensures that we always write a complete page. This is more efficient for storage
        // stacks, since pages are usually larger than on-disk block sizes and are block-size
        // aligned, so this avoids the need for a read-modify-write cycle in the underlying
        // filesystem. This is only necessary for the last (partial) page.
        let mut fpm_page_buffer: Vec<u8> = vec![0; page_size_usize];

        for interval_index in 0..num_intervals {
            let this_fpm_page_data = fpm_pages_data_iter.next().unwrap_or(&[]);
            assert!(this_fpm_page_data.len() <= fpm_page_buffer.len());

            let slice_to_write = if this_fpm_page_data.len() < page_size_usize {
                fpm_page_buffer[..this_fpm_page_data.len()].copy_from_slice(this_fpm_page_data);
                fpm_page_buffer[this_fpm_page_data.len()..].fill(0xff); // fill the rest with "free"
                fpm_page_buffer.as_slice()
            } else {
                // We already have a perfectly-sized slice. Just use it.
                this_fpm_page_data
            };

            let interval_page = interval_to_page(interval_index, page_size);
            let new_fpm_page = interval_page + new_fpm_number;

            debug!(interval = interval_index, "writing fpm chunk");

            self.file
                .write_all_at(slice_to_write, page_to_offset(new_fpm_page, page_size))?;
        }

        Ok(())
    }
}

/// This ensures that the last few bits of the FPM are set to "free".
///
/// The MSPDB library uses a bit vector implementation that packs bits into an array of `u32`
/// values, just as this Rust implementation does. However, if the number of _bits_ in the FPM
/// is not a multiple of 32, then the MSPDB library accidentally reads the unaligned bits in the
/// last `u32` and expects them to be "free".
fn fill_last_word_of_fpm(fpm: &mut BitVec<u32, Lsb0>) {
    let unaligned_len = fpm.len() & 0x1f;
    if unaligned_len == 0 {
        return;
    }

    let fpm_words = fpm.as_raw_mut_slice();
    let last = fpm_words.last_mut().unwrap();

    // Because unaligned_len is the result of masking with 0x1f, we know that the shift count
    // cannot overflow.
    *last |= 0xffff_ffff << unaligned_len;
}

/// Information about the new Stream Directory that we just constructed and wrote to disk.
struct StreamDirInfo {
    /// Size in bytes of the Stream Directory
    dir_size: u32,

    /// The list of pages that contain the Stream Directory
    dir_pages: Vec<U32<LE>>,

    /// The list of pages that contain the Stream Directory Map (the level _above_ `dir_pages`)
    map_pages: Vec<U32<LE>>,
}

```

`msf/src/lib.rs`:

```rs
//! Reads and writes Multi-Stream Files (MSF). MSF is the underlying container format used by
//! Program Database (PDB) files.
//!
//! MSF files contain a set of numbered _streams_. Each stream is like a file; a stream is a
//! sequence of bytes.
//!
//! The bytes stored within a single stream are usually not stored sequentially on disk. The
//! organization of the disk file and the mapping from stream locations to MSF file locations is
//! similar to a traditional file system; managing that mapping is the main purpose of the MSF
//! file format.
//!
//! MSF files are used as the container format for Program Database (PDB) files. PDB files are used
//! by compilers, debuggers, and other tools when targeting Windows.
//!
//! Most developers should not use this crate directly. This crate is a building block for tools
//! that read and write PDBs. This crate does not provide any means for building or parsing the
//! data structures of PDB files; it only handles storing files in the MSF container format.
//!
//! The `mspdb` crate uses this crate for reading and writing PDB files. It provides an interface
//! for reading PDB data structures, and in some cases for creating or modifying them. Most
//! developers should use `mspdb` instead of using `msf` directly.
//!
//! # References
//!
//! * [The MSF File Format](https://llvm.org/docs/PDB/MsfFile.html)
//! * [The PDB File Format](https://llvm.org/docs/PDB/index.html)
//! * [`microsoft-pdb` repository](https://github.com/microsoft/microsoft-pdb): Many of the comments
//!   in this Rust crate reference C++ source files and header files from this `microsoft-pdb`
//!   repository. If a C++ file is referenced in a comment without more context, such as `dbi.h`,
//!   then check for it in the `microsoft-pdb` repository.

#![forbid(unused_must_use)]
#![forbid(unsafe_code)]
#![warn(missing_docs)]
#![allow(clippy::collapsible_if)]
#![allow(clippy::needless_late_init)]
#![allow(clippy::needless_lifetimes)]

mod check;
mod commit;
mod open;
mod pages;
mod read;
mod stream_reader;
mod stream_writer;
mod write;

#[cfg(test)]
mod tests;

pub use open::CreateOptions;
pub use stream_reader::StreamReader;
pub use stream_writer::StreamWriter;

use anyhow::bail;
use bitvec::prelude::{BitVec, Lsb0};
use pow2::{IntOnlyPow2, Pow2};
use std::collections::HashMap;
use std::fs::File;
use std::io::{Read, Seek, SeekFrom};
use std::mem::size_of;
use std::path::Path;
use sync_file::{RandomAccessFile, ReadAt, WriteAt};
use zerocopy::{FromBytes, FromZeros, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U16, U32};

use self::pages::{num_pages_for_stream_size, PageAllocator};

/// Identifies a page number in the MSF file. Not to be confused with `StreamPage`.
type Page = u32;

/// Identifies a page within a stream. `StreamPage` can be translated to `Page` by using the
/// stream page mapper.
type StreamPage = u32;

const FPM_NUMBER_1: u32 = 1;
const FPM_NUMBER_2: u32 = 2;

/// The value of `magic` for "big" MSF files.
const MSF_BIG_MAGIC: [u8; 32] = *b"Microsoft C/C++ MSF 7.00\r\n\x1a\x44\x53\x00\x00\x00";

/// This identifies MSF files before the transition to "big" MSF files.
const MSF_SMALL_MAGIC: [u8; 0x2c] = *b"Microsoft C/C++ program database 2.00\r\n\x1a\x4a\x47\0\0";

#[test]
fn show_magics() {
    use pretty_hex::PrettyHex;

    println!("MSF_SMALL_MAGIC:");
    println!("{:?}", MSF_SMALL_MAGIC.hex_dump());

    println!("MSF_BIG_MAGIC:");
    println!("{:?}", MSF_BIG_MAGIC.hex_dump());
}

/// The header of the PDB/MSF file, before the transition to "big" MSF files.
/// This is at file offset 0.
#[allow(missing_docs)]
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
struct SmallMsfHeader {
    /// Identifies this file as a PDB. Value must be [`MSF_SMALL_MAGIC`].
    magic: [u8; 0x2c],
    page_size: U32<LE>,
    active_fpm: U16<LE>,
    num_pages: U16<LE>,
    stream_dir_size: U32<LE>,
    /// This field contains a pointer to an in-memory data structure, and hence is meaningless.
    /// Decoders should ignore this field. Encoders should set this field to 0.
    stream_dir_ptr: U32<LE>,
}

/// The header of the PDB/MSF file. This is at file offset 0.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
struct MsfHeader {
    /// Identifies this file as a PDB.
    magic: [u8; 32],

    /// The size of each page, in bytes.
    page_size: U32<LE>,

    /// Page number of the active FPM. This can only be 1 or 2. In the C++ implementation (in the
    /// `microsoft-pdb` repository), this is `pnFpm`.
    active_fpm: U32<LE>,

    /// The number of pages in this MSF file. In the C++ implementation, this is `pnMac`.
    num_pages: U32<LE>,

    /// Size of the Stream Directory, in bytes. In the C++ implementation, this is `siSt.cb`.
    stream_dir_size: U32<LE>,

    /// The page which contains the Stream Directory Map. This page contains a list of pages
    /// which contain the Stream Directory.
    ///
    /// This field is only used for "Small MSF" (pre-"Big MSF") encoding. When using Big MSF,
    /// this field is expected to be zero.
    ///
    /// In the C++ implementation, this is `mpspnpn` (map of stream page number to page number).
    stream_dir_small_page_map: U32<LE>,
    // When using "Big MSF", there is an array of u32 values that immediately follow
    // the MSfHeader. The size of the array is a function of stream_dir_size and num_pages:
    //
    //     divide_round_up(divide_round_up(stream_dir_size, num_pages) * 4), num_pages)
    //
    // pub stream_dir_big_page_map: [U32<LE>],
}

/// The length of the MSF File Header.
const MSF_HEADER_LEN: usize = size_of::<MsfHeader>();

/// The byte offset of the stream directory page map. This is a small array of page indices that
/// point to pages that contain the stream directory. This is used only with the Big MSF encoding.
const STREAM_DIR_PAGE_MAP_FILE_OFFSET: u64 = MSF_HEADER_LEN as u64;
static_assertions::const_assert_eq!(MSF_HEADER_LEN, 52);

/// The minimum page size.
pub const MIN_PAGE_SIZE: PageSize = PageSize::from_exponent(9);

/// The default page size.
pub const DEFAULT_PAGE_SIZE: PageSize = PageSize::from_exponent(12);

/// A large page size. This is less than the largest supported page size.
pub const LARGE_PAGE_SIZE: PageSize = PageSize::from_exponent(13);

/// The largest supported page size.
pub const MAX_PAGE_SIZE: PageSize = PageSize::from_exponent(16);

/// This size is used to mark a stream as "invalid". An invalid stream is different from a
/// stream with a length of zero bytes.
pub const NIL_STREAM_SIZE: u32 = 0xffff_ffff;

/// Specifies a page size used in an MSF file. This value is always a power of 2.
pub type PageSize = Pow2;

/// The stream index of the Stream Directory stream. This is reserved and cannot be used by
/// applications.
pub const STREAM_DIR_STREAM: u32 = 0;

/// Converts a page number to a file offset.
fn page_to_offset(page: u32, page_size: PageSize) -> u64 {
    (page as u64) << page_size.exponent()
}

/// Given an interval number, returns the page number of the first page of the interval.
fn interval_to_page(interval: u32, page_size: PageSize) -> u32 {
    interval << page_size.exponent()
}

/// Gets the byte offset within a page, for a given offset within a stream.
pub fn offset_within_page(offset: u32, page_size: PageSize) -> u32 {
    let page_low_mask = (1u32 << page_size.exponent()) - 1u32;
    offset & page_low_mask
}

/// Allows reading and writing the contents of a PDB/MSF file.
///
/// The [`Msf::open`] function opens an MSF file for read access, given a file. This is the most
/// commonly-used way to open a file.
pub struct Msf<F = RandomAccessFile> {
    /// The data source.
    file: F,

    kind: MsfKind,

    /// The FPM number for the committed (active) FPM.
    ///
    /// The `commit()` function can change this number.
    active_fpm: u32,

    /// Contains the sizes of all streams. The length of `stream_sizes` implicitly defines
    /// the number of streams.
    ///
    /// Values in this vector may be [`NIL_STREAM_SIZE`], indicating that the stream is present
    /// but is a nil stream.
    ///
    /// As streams are modified, this vector changes. It contains a combination of both committed
    /// and uncommitted state.
    stream_sizes: Vec<u32>,

    /// The maximum number of streams that we will allow to be created using `new_stream` or
    /// `nil_stream`. The default value is 0xfffe, which prevents overflowing the 16-bit stream
    /// indexes that are used in PDB (or confusing them with the "nil" stream index).
    max_streams: u32,

    /// Contains the page numbers for all streams in the committed state.
    committed_stream_pages: Vec<Page>,

    /// Vector contains offsets into `committed_stream_pages` where the pages for a given stream start.
    committed_stream_page_starts: Vec<u32>,

    /// Handles allocating pages.
    pages: PageAllocator,

    /// If a stream has been modified then there is an entry in this table for it. The key for
    /// each entry is the stream number. The value is the sequence of pages for that stream.
    ///
    /// One of the side-effects of the [`Msf::commit`] function is that the `modified_streams`
    /// table is cleared.
    ///
    /// This table is always empty if `access_mode == AccessMode::Read`.
    modified_streams: HashMap<u32, Vec<Page>>,

    access_mode: AccessMode,
}

/// Specifies the versions used for the MSF.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub enum MsfKind {
    /// The obsolete, pre-Big MSF encoding. This library does not support creating or modifying
    /// MSF files that use this encoding, but it does support reading them.
    Small,
    /// The Big MSF encoding, which is the encoding currently used by most tools that target
    /// Windows.
    Big,
}

/// Specifies the access mode for opening a PDB/MSF file.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
enum AccessMode {
    /// Read-only access
    Read,
    /// Read-write access
    ReadWrite,
}

impl<F> Msf<F> {
    /// Returns the page size used for this file.
    pub fn page_size(&self) -> PageSize {
        self.pages.page_size
    }

    /// Gets access to the stream page pointers for a given stream. The stream page pointers
    /// provide the mapping from offsets within a stream to offsets within the entire PDB (MSF) file.
    ///
    /// If the stream is a NIL stream, then this returns `(NIL_STREAM_SIZE, &[])`.
    pub fn stream_size_and_pages(&self, stream: u32) -> Result<(u32, &[Page]), anyhow::Error> {
        let Some(&stream_size) = self.stream_sizes.get(stream as usize) else {
            bail!("Stream index is out of range.  Index: {stream}");
        };

        if stream_size == NIL_STREAM_SIZE {
            // This is a NIL stream.
            return Ok((NIL_STREAM_SIZE, &[]));
        }

        // The stream index is valid and the stream is not a NIL stream.
        let num_stream_pages =
            num_pages_for_stream_size(stream_size, self.pages.page_size) as usize;

        if num_stream_pages == 0 {
            // The stream is valid (is not nil) and is a zero-length stream.
            // It has no pages assigned to it.
            return Ok((0, &[]));
        }

        // If this stream has been modified, then return the modified page list.
        if let Some(pages) = self.modified_streams.get(&stream) {
            assert_eq!(num_stream_pages, pages.len());
            return Ok((stream_size, pages.as_slice()));
        }

        let start = self.committed_stream_page_starts[stream as usize] as usize;
        let pages = &self.committed_stream_pages[start..start + num_stream_pages];
        Ok((stream_size, pages))
    }

    /// The total number of streams in this PDB, including nil streams.
    pub fn num_streams(&self) -> u32 {
        self.stream_sizes.len() as u32
    }

    /// Gets the size of a given stream, in bytes.
    ///
    /// The `stream` value must be in a valid range of `0..num_streams()`.
    ///
    /// If `stream` is a NIL stream, this function returns 0.
    pub fn stream_size(&self, stream: u32) -> u32 {
        assert!((stream as usize) < self.stream_sizes.len());
        let stream_size = self.stream_sizes[stream as usize];
        if stream_size == NIL_STREAM_SIZE {
            0
        } else {
            stream_size
        }
    }

    /// Indicates whether a given stream index is valid.
    pub fn is_valid_stream_index(&self, stream: u32) -> bool {
        (stream as usize) < self.stream_sizes.len()
    }

    /// Indicates that a stream index is valid, and that its length is valid.
    pub fn is_stream_valid(&self, stream: u32) -> bool {
        if (stream as usize) < self.stream_sizes.len() {
            self.stream_sizes[stream as usize] != NIL_STREAM_SIZE
        } else {
            false
        }
    }

    /// Return the nominal length of this file, in bytes.
    ///
    /// This is the number of pages multiplied by the page size. It is not guaranteed to be equal to
    /// the on-disk size of the file, but in practice it usually is.
    pub fn nominal_size(&self) -> u64 {
        page_to_offset(self.pages.num_pages, self.pages.page_size)
    }

    /// Returns the number of free pages.
    ///
    /// This number counts the pages that are _less than_ `num_pages`. There may be pages assigned
    /// to the MSF file beyond `num_pages`, but if there are then this does not count that space.
    ///
    /// This value does not count Page 0, pages assigned to the FPM, streams, or the current
    /// Stream Directory. It does count pages assigned to the old stream directory.
    pub fn num_free_pages(&self) -> u32 {
        self.pages.fpm.count_ones() as u32
    }

    /// Extracts the underlying file for this MSF. **All pending modifications are dropped**.
    pub fn into_file(self) -> F {
        self.file
    }

    /// Gets access to the contained file
    pub fn file(&self) -> &F {
        &self.file
    }

    /// Gets mutable access to the contained file
    pub fn file_mut(&mut self) -> &mut F {
        &mut self.file
    }

    /// Indicates whether this [`Msf`] was opened for read/write access.
    pub fn is_writable(&self) -> bool {
        self.access_mode == AccessMode::ReadWrite
    }
}

impl<F: ReadAt> Msf<F> {
    /// Reads a portion of a stream to a vector.
    pub fn read_stream_section_to_box(
        &self,
        stream: u32,
        start: u32,
        size: u32,
    ) -> anyhow::Result<Box<[u8]>>
    where
        F: ReadAt,
    {
        let reader = self.get_stream_reader(stream)?;
        let mut stream_data = FromZeros::new_box_zeroed_with_elems(size as usize)
            .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        reader.read_exact_at(&mut stream_data, u64::from(start))?;
        Ok(stream_data)
    }

    /// Reads the entire stream into a `Box<[u8]>`.
    pub fn read_stream_to_box(&self, stream: u32) -> anyhow::Result<Box<[u8]>> {
        let reader = self.get_stream_reader(stream)?;
        let mut stream_data = FromZeros::new_box_zeroed_with_elems(reader.len() as usize)
            .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        reader.read_exact_at(&mut stream_data, 0)?;
        Ok(stream_data)
    }

    /// Reads an entire stream to a vector.
    pub fn read_stream_to_vec(&self, stream: u32) -> anyhow::Result<Vec<u8>> {
        let stream_data = self.read_stream_to_box(stream)?;

        // This conversion does not reallocate data. It just adds a 'capacity' field.
        Ok(stream_data.into_vec())
    }

    /// Reads an entire stream into an existing vector.
    pub fn read_stream_to_vec_mut(
        &self,
        stream: u32,
        stream_data: &mut Vec<u8>,
    ) -> anyhow::Result<()> {
        let reader = self.get_stream_reader(stream)?;

        // Do not clear and resize. Doing so requires zeroing all the data in the vector.
        // Since we are going to read into the vector, that means we would modify every byte twice.
        // That's expensive when you're working with a lot of data.
        stream_data.resize(reader.len() as usize, 0);

        reader.read_exact_at(stream_data, 0)?;
        Ok(())
    }

    /// Returns an object which can read from a given stream.  The returned object implements
    /// the [`Read`], [`Seek`], and [`ReadAt`] traits.
    pub fn get_stream_reader(&self, stream: u32) -> anyhow::Result<StreamReader<'_, F>>
    where
        F: ReadAt,
    {
        let (stream_size, stream_pages) = self.stream_size_and_pages(stream)?;
        Ok(StreamReader::new(
            self,
            stream,
            stream_size,
            stream_pages,
            0,
        ))
    }
}

/// Checks whether the header of a file appears to be a valid MSF file.
///
/// This only looks at the signature; it does not read anything else in the file. This is useful
/// for quickly determining whether a file could be an MSF file, but without any validation.
pub fn is_file_header_msf(header: &[u8]) -> bool {
    header.starts_with(&MSF_BIG_MAGIC) || header.starts_with(&MSF_SMALL_MAGIC)
}

/// The absolute minimum size of a slice that could contain a valid MSF file header, as tested by
/// [`is_file_header_msf`].
///
/// This does not specify the minimum valid size of an MSF file. It is only a recommended minimum
/// for callers of [`is_file_header_msf`].
pub const MIN_FILE_HEADER_SIZE: usize = 0x100;

```

`msf/src/open.rs`:

```rs
//! Code for opening or creating MSF files.

use super::*;
use sync_file::RandomAccessFile;
use tracing::{trace, trace_span, warn};
use zerocopy::IntoBytes;

/// Options for creating a new PDB/MSF file.
#[derive(Clone, Debug)]
pub struct CreateOptions {
    /// The page size to use. This must be in the range [`MIN_PAGE_SIZE..=MAX_PAGE_SIZE`].
    pub page_size: PageSize,

    /// The maximum number of streams that we will allow to be created using `new_stream` or
    /// `nil_stream`. The default value is 0xfffe, which prevents overflowing the 16-bit stream
    /// indexes that are used in PDB (or confusing them with the "nil" stream index).
    ///
    /// Applications may increase this value beyond the default, but this will produce MSF files
    /// that are not usable by most PDB tools.
    pub max_streams: u32,
}

/// The maximum number of streams that PDB can tolerate.
const DEFAULT_MAX_STREAMS: u32 = 0xfffe;

impl Default for CreateOptions {
    fn default() -> Self {
        Self {
            page_size: DEFAULT_PAGE_SIZE,
            max_streams: DEFAULT_MAX_STREAMS,
        }
    }
}

impl Msf<RandomAccessFile> {
    /// Opens an MSF file for read access, given a file name.
    pub fn open(file_name: &Path) -> anyhow::Result<Self> {
        let file = File::open(file_name)?;
        let random_file = RandomAccessFile::from(file);
        Self::new_with_access_mode(random_file, AccessMode::Read)
    }

    /// Creates a new MSF file on disk (**truncating any existing file!**) and creates a new
    /// [`Msf`] object in-memory object with read/write access.
    ///
    /// This function does not write anything to disk until stream data is written or
    /// [`Self::commit`] is called.
    pub fn create(file_name: &Path, options: CreateOptions) -> anyhow::Result<Self> {
        let file = File::create(file_name)?;
        let random_file = RandomAccessFile::from(file);
        Self::create_with_file(random_file, options)
    }

    /// Opens an existing MSF file for read/write access, given a file name.
    pub fn modify(file_name: &Path) -> anyhow::Result<Self> {
        let file = File::options().read(true).write(true).open(file_name)?;
        let random_file = RandomAccessFile::from(file);
        Self::modify_with_file(random_file)
    }
}

impl<F: ReadAt> Msf<F> {
    /// Opens an MSF file for read access, given a [`File`] that has already been opened.
    pub fn open_with_file(file: F) -> anyhow::Result<Self> {
        Self::new_with_access_mode(file, AccessMode::Read)
    }

    /// Creates a new MSF file, given a file handle that has already been opened.
    ///
    /// **This function destroys the contents of the existing file.**
    pub fn create_with_file(file: F, options: CreateOptions) -> anyhow::Result<Self> {
        Self::create_for(file, options)
    }

    /// Opens an existing MSF file for read/write access, given an [`File`] that has already
    /// been opened.
    ///
    /// The `file` handle will be used for absolute reads and writes. The caller should never use
    /// this same file handle for reads (and especially not for writes) while also using [`Msf`]
    /// because the operating system's read/write file position may be updated by [`Msf`].
    pub fn modify_with_file(file: F) -> anyhow::Result<Self> {
        Self::new_with_access_mode(file, AccessMode::ReadWrite)
    }

    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    fn new_with_access_mode(file: F, access_mode: AccessMode) -> anyhow::Result<Self> {
        // Read the MSF File Header.

        let _span = trace_span!("Msf::new_with_access_mode").entered();

        const MIN_PAGE_SIZE_USIZE: usize = 1usize << MIN_PAGE_SIZE.exponent();

        let mut page0: [u8; MIN_PAGE_SIZE_USIZE] = [0; MIN_PAGE_SIZE_USIZE];

        // If this read fails, then the file is too small to be a valid PDB of any kind.
        file.read_exact_at(&mut page0, 0)?;

        let msf_kind: MsfKind;
        let page_size: u32;
        let active_fpm: u32;
        let num_pages: u32;
        let stream_dir_size: u32;

        if page0.starts_with(&MSF_BIG_MAGIC) {
            // unwrap() cannot fail because page0 has a fixed size that is larger than MsfHeader
            let (msf_header, _) = MsfHeader::ref_from_prefix(page0.as_slice()).unwrap();
            page_size = msf_header.page_size.get();
            active_fpm = msf_header.active_fpm.get();
            num_pages = msf_header.num_pages.get();
            stream_dir_size = msf_header.stream_dir_size.get();
            msf_kind = MsfKind::Big;

            // The active FPM can only be 1 or 2.
            if !matches!(active_fpm, 1 | 2) {
                bail!("The PDB header is invalid.  The active FPM is invalid.");
            }
        } else if page0.starts_with(&MSF_SMALL_MAGIC) {
            // Found an "old" MSF header.
            // unwrap() cannot fail because page0 has a fixed size that is larger than SmallMsfHeader
            let (msf_header, _) = SmallMsfHeader::ref_from_prefix(page0.as_slice()).unwrap();
            page_size = msf_header.page_size.get();
            active_fpm = msf_header.active_fpm.get() as u32;
            num_pages = msf_header.num_pages.get() as u32;
            stream_dir_size = msf_header.stream_dir_size.get();
            msf_kind = MsfKind::Small;
        } else if page0[16..24] == *b"PDB v1.0" {
            bail!("This file is a Portable PDB, which is not supported.");
        } else {
            bail!("PDB file does not have the correct header (magic is wrong).");
        }

        let Ok(page_size_pow2) = PageSize::try_from(page_size) else {
            bail!("The PDB header is invalid. The page size ({page_size}) is not a power of 2.",);
        };

        if num_pages == 0 {
            bail!("PDB specifies invalid value for num_pages (zero).");
        }

        let mut stream_sizes: Vec<u32>;

        // The number of pages in the stream directory.
        let stream_dir_num_pages = stream_dir_size.div_round_up(page_size_pow2);

        // Create the PageAllocator. This initializes the fpm vector to "everything is free"
        // and then sets Page 0 and the FPM pages as "free". Nothing is marked as "freed".
        let mut page_allocator = PageAllocator::new(num_pages as usize, page_size_pow2);

        let mut committed_stream_pages: Vec<Page>;
        let mut committed_stream_page_starts: Vec<u32>;

        match msf_kind {
            MsfKind::Big => {
                // "Big MSF" uses a 3-level hierarchy for the Stream Directory:
                //
                // stream_dir_map        <-- contains u32 pages to ↓
                // stream_dir_pages      <-- contains u32 pages to ↓
                // stream_dir_bytes      <-- bottom level, stored in pages
                //
                // stream_dir_map is an array of u32 page pointers. It is stored directly in
                // page 0, immediately after MsfHeader. These pointers point to pages that contain
                // the stream_dir_pages, which is the next level down.
                // The number of pages allocated to stream_dir_map = ceil(stream_dir_pages.len() * 4 / page_size).
                // The number of bytes used within stream_dir_map = stream_dir_pages.len() * 4.
                //
                // stream_dir_pages is a set of pages. When concatenated, they contain the page
                // pointers that point to the stream directory bytes.
                // The number of pages in stream_dir_pages = ceil(stream_dir_size / page_size).
                // The number of bytes used within stream_dir_pages is stream_dir_pages * 4.

                if stream_dir_size % 4 != 0 {
                    bail!("MSF Stream Directory has an invalid size; it is not a multiple of 4.");
                }

                // We are going to read the stream directory into this vector.
                let mut stream_dir: Vec<U32<LE>> = vec![U32::new(0); stream_dir_size as usize / 4];

                // Read the page map for the stream directory.
                let stream_dir_l1_num_pages =
                    num_pages_for_stream_size(4 * stream_dir_num_pages, page_size_pow2) as usize;
                let Ok((page_map_l1_ptrs, _)) = <[U32<LE>]>::ref_from_prefix_with_elems(
                    &page0[STREAM_DIR_PAGE_MAP_FILE_OFFSET as usize..],
                    stream_dir_l1_num_pages,
                ) else {
                    bail!("Stream dir size is invalid (exceeds design limits)");
                };

                let stream_dir_bytes: &mut [u8] = stream_dir.as_mut_bytes();
                let mut stream_dir_chunks = stream_dir_bytes.chunks_mut(page_size as usize);
                // Now read the stream pages for the stream dir.
                let mut l1_page: Vec<u8> = vec![0; page_size as usize];
                'l1_loop: for &page_map_l1_ptr in page_map_l1_ptrs.iter() {
                    let page_map_l1_ptr: u32 = page_map_l1_ptr.get();

                    page_allocator.init_mark_stream_dir_page_busy(page_map_l1_ptr)?;
                    if is_special_page_big_msf(page_size_pow2, page_map_l1_ptr) {
                        bail!(
                            "Stream dir contains invalid page number: {page_map_l1_ptr}. \
                             Page points to Page 0 or to an FPM page."
                        );
                    }

                    // Read the page pointers.
                    let file_offset = page_to_offset(page_map_l1_ptr, page_size_pow2);
                    file.read_exact_at(l1_page.as_mut_slice(), file_offset)?;

                    // Now read the individual pages, as long as we have more.
                    let l2_page_u32 = <[U32<LE>]>::ref_from_bytes(l1_page.as_slice()).unwrap();

                    for &l2_page in l2_page_u32.iter() {
                        let l2_page: u32 = l2_page.get();

                        let Some(stream_dir_chunk) = stream_dir_chunks.next() else {
                            break 'l1_loop;
                        };

                        page_allocator.init_mark_stream_dir_page_busy(l2_page)?;
                        if is_special_page_big_msf(page_size_pow2, l2_page) {
                            bail!(
                                "Stream dir contains invalid page number: {l2_page}. \
                                 Page points to Page 0 or to an FPM page."
                            );
                        }

                        let l2_file_offset = page_to_offset(l2_page, page_size_pow2);
                        file.read_exact_at(stream_dir_chunk, l2_file_offset)?;
                    }
                }

                if stream_dir.is_empty() {
                    bail!("Stream directory is invalid (zero-length)");
                }

                let num_streams = stream_dir[0].get() as usize;

                // Stream 0 is special and must exist.
                if num_streams == 0 {
                    bail!("MSF file is invalid, because num_streams = 0.");
                }

                let Some(stream_sizes_src) = stream_dir.get(1..1 + num_streams) else {
                    bail!("Stream directory is invalid (num_streams is not consistent with size)");
                };
                stream_sizes = stream_sizes_src.iter().map(|size| size.get()).collect();

                let mut stream_pages_iter = &stream_dir[1 + num_streams..];

                // Build committed_stream_pages and committed_stream_page_starts.
                committed_stream_pages = Vec::with_capacity(stream_dir.len() - num_streams - 1);
                committed_stream_page_starts = Vec::with_capacity(num_streams + 1);

                for (stream, &stream_size) in stream_sizes_src.iter().enumerate() {
                    committed_stream_page_starts.push(committed_stream_pages.len() as u32);

                    let stream_size = stream_size.get();
                    if stream_size != NIL_STREAM_SIZE {
                        let num_stream_pages =
                            num_pages_for_stream_size(stream_size, page_size_pow2) as usize;
                        if num_stream_pages > stream_pages_iter.len() {
                            bail!(
                                "Stream directory is invalid.  Stream {stream} has size {stream_size}, \
                                 which exceeds the size of the stream directory."
                            );
                        }
                        let (this_stream_pages, next) =
                            stream_pages_iter.split_at(num_stream_pages);
                        stream_pages_iter = next;
                        committed_stream_pages.extend(this_stream_pages.iter().map(|p| p.get()));
                    }
                }
                committed_stream_page_starts.push(committed_stream_pages.len() as u32);

                // Now that we have finished reading the stream directory, we set the length
                // of stream 0 (the "Old Stream Directory") to 0. Nothing should ever read Stream 0.
                // If we modify a PDB/MSF file, then we want to write no pages at all for Stream 0.
                // Doing this here is the most convenient way to handle this.
                stream_sizes[0] = 0;
            }

            MsfKind::Small => {
                // Before Big MSF files, the stream directory was stored in a set of pages.
                // These pages were listed directly within page 0. Keep in mind that page numbers
                // are 16-bit in old MSF files.
                let page_pointers_size_bytes = stream_dir_num_pages * 2;

                let mut pages_u16: Vec<U16<LE>> = vec![U16::new(0); stream_dir_num_pages as usize];
                if page_pointers_size_bytes + size_of::<SmallMsfHeader>() as u32 > page_size {
                    bail!(
                        "The MSF header is invalid. The page pointers for the stream directory \
                         exceed the range of the first page. \
                         Stream dir size (in bytes): {stream_dir_size}  Page size: {page_size}"
                    );
                }

                file.read_exact_at(pages_u16.as_mut_bytes(), size_of::<SmallMsfHeader>() as u64)?;

                // Read the pages of the stream directory. Be careful with the last page.
                let mut page_iter = pages_u16.iter();
                let mut old_stream_dir_bytes: Vec<u8> = vec![0; stream_dir_size as usize];
                for stream_dir_chunk in old_stream_dir_bytes.chunks_mut(page_size as usize) {
                    // This unwrap should succeed because we computed the length of pages_u16
                    // based on the byte size of the stream directory.
                    let page = page_iter.next().unwrap().get() as u32;
                    page_allocator.init_mark_stream_dir_page_busy(page)?;
                    file.read_exact_at(stream_dir_chunk, page_to_offset(page, page_size_pow2))?;
                }

                let Ok((header, rest)) =
                    OldMsfStreamDirHeader::read_from_prefix(old_stream_dir_bytes.as_slice())
                else {
                    bail!("Invalid stream directory: too small");
                };

                let num_streams = header.num_streams.get() as usize;
                stream_sizes = Vec::with_capacity(num_streams);

                let Ok((entries, mut rest)) =
                    <[OldMsfStreamEntry]>::ref_from_prefix_with_elems(rest, num_streams)
                else {
                    bail!("Invalid stream directory: too small")
                };

                for i in 0..num_streams {
                    let stream_size = entries[i].stream_size.get();
                    stream_sizes.push(stream_size);
                }

                committed_stream_page_starts = Vec::with_capacity(num_streams + 1);
                committed_stream_pages = Vec::new(); // TODO: precompute capacity

                for &stream_size in stream_sizes.iter() {
                    committed_stream_page_starts.push(committed_stream_pages.len() as u32);
                    if stream_size != NIL_STREAM_SIZE {
                        let num_pages = stream_size.div_round_up(page_size_pow2);

                        let Ok((pages, r)) =
                            <[U16<LE>]>::ref_from_prefix_with_elems(rest, num_pages as usize)
                        else {
                            bail!("Invalid stream directory: too small");
                        };

                        rest = r; // update iterator state
                        for page in pages.iter() {
                            committed_stream_pages.push(page.get() as u32);
                        }
                    }
                }

                committed_stream_page_starts.push(committed_stream_pages.len() as u32);

                if !rest.is_empty() {
                    warn!(
                        unused_bytes = rest.len(),
                        "old-style stream dir contained unused bytes"
                    );
                }
            }
        }

        // Mark the pages in all streams (except for stream 0) as busy. This will also detect
        // page numbers that are invalid (0 or FPM).
        {
            // pages is the list of the page numbers for all streams (except stream 0).
            let start = committed_stream_page_starts[1] as usize;
            let pages = &committed_stream_pages[start..];
            for &page in pages.iter() {
                page_allocator.init_mark_stream_page_busy(page, 0, 0)?;
            }
        }

        // We have finished building the in-memory FPM, including both the fpm and fpm_freed
        // vectors. We expect that every page is either FREE, BUSY, or DELETED. Check that now.
        page_allocator.check_vector_consistency()?;

        // Read the FPM from disk and compare it to the FPM that we just constructed. They should
        // be identical.
        // TODO: implement for small MSF
        let fpm_on_disk = read_fpm_big_msf(&file, active_fpm, num_pages, page_size_pow2)?;

        assert_eq!(fpm_on_disk.len(), page_allocator.fpm.len()); // because num_pages defines both

        if page_allocator.fpm != fpm_on_disk {
            {
                use tracing::warn;

                warn!("FPM computed from Stream Directory is not equal to FPM found on disk.");
                warn!(
                    "Num pages = {num_pages} (0x{num_pages:x} bytes, bit offset: 0x{:x}:{})",
                    num_pages / 8,
                    num_pages % 8
                );

                for i in 0..num_pages as usize {
                    if fpm_on_disk[i] != page_allocator.fpm[i] {
                        warn!(
                            "  bit 0x{:04x} is different. disk = {}, computed = {}",
                            i, fpm_on_disk[i], page_allocator.fpm[i]
                        );
                    }
                }
            }
            bail!("FPM is corrupted; FPM computed from Stream Directory is not equal to FPM found on disk.");
        }

        // We have finished checking all the data that we have read from disk.
        // Now check the consistency of our in-memory data structures.
        page_allocator.assert_invariants();

        match (access_mode, msf_kind) {
            (AccessMode::ReadWrite, MsfKind::Small) => {
                bail!(
                    "This PDB file uses the obsolete 'Small MSF' encoding. \
                     This library does not support read-write mode with Small MSF files."
                );
            }

            (AccessMode::ReadWrite, MsfKind::Big) => {}

            (AccessMode::Read, _) => {}
        }

        Ok(Self {
            file,
            access_mode,
            active_fpm,
            committed_stream_pages,
            committed_stream_page_starts,
            stream_sizes,
            kind: msf_kind,
            pages: page_allocator,
            modified_streams: HashMap::new(),
            max_streams: DEFAULT_MAX_STREAMS,
        })
    }

    /// Creates a new MSF object in memory. The on-disk file is not modified until `commit()` is
    /// called.
    pub fn create_for(file: F, options: CreateOptions) -> anyhow::Result<Self> {
        let _span = trace_span!("Msf::create_for").entered();

        assert!(options.page_size >= MIN_PAGE_SIZE);
        assert!(options.page_size <= MAX_PAGE_SIZE);

        let num_pages: usize = 3;

        let mut this = Self {
            file,
            access_mode: AccessMode::ReadWrite,
            committed_stream_pages: vec![],
            committed_stream_page_starts: vec![0; 2],
            kind: MsfKind::Big,
            pages: PageAllocator::new(num_pages, options.page_size),
            modified_streams: HashMap::new(),
            stream_sizes: vec![0],
            active_fpm: 2,
            max_streams: options.max_streams,
        };

        // Set up the 4 fixed-index streams. They are created as nil streams.
        for _ in 1..=4 {
            let _stream_index = this.nil_stream()?;
        }

        Ok(this)
    }
}

/// Read each page of the FPM. Each page of the FPM is stored in a different interval;
/// they are not contiguous.
///
/// num_pages is the total number of pages in the FPM.
fn read_fpm_big_msf<F: ReadAt>(
    file: &F,
    active_fpm: u32,
    num_pages: u32,
    page_size: PageSize,
) -> anyhow::Result<BitVec<u32, Lsb0>> {
    let _span = trace_span!("read_fpm_big_msf").entered();

    assert!(num_pages > 0);

    let mut free_page_map: BitVec<u32, Lsb0> = BitVec::new();
    free_page_map.resize(num_pages as usize, false);
    let fpm_bytes: &mut [u8] = free_page_map.as_raw_mut_slice().as_mut_bytes();
    let page_size_usize = usize::from(page_size);

    for (interval, fpm_page_bytes) in fpm_bytes.chunks_mut(page_size_usize).enumerate() {
        let interval_page = interval_to_page(interval as u32, page_size);
        let file_pos = page_to_offset(interval_page + active_fpm, page_size);

        trace!(
            interval,
            interval_page,
            file_pos,
            "reading FPM page, interval_page = 0x{interval_page:x}, file_pos = 0x{file_pos:x}"
        );
        file.read_exact_at(fpm_page_bytes, file_pos)?;
    }

    // Check our invariants for the FPM. If these checks fail then we return Err because we
    // are validating data that we read from disk. After these checks succeed, we switch to using
    // assert_invariants(), which uses assert!(). That verifies that we preserve our invariants.

    // Check that page 0, which stores the MSF File Header, is busy.
    if free_page_map[0] {
        bail!("FPM is invalid: Page 0 should always be BUSY");
    }

    // Check that the pages assigned to the FPM are marked "busy" in all intervals.

    let mut interval: u32 = 0;
    loop {
        let interval_page = interval_to_page(interval, page_size) as usize;
        let fpm1_index = interval_page + 1;
        let fpm2_index = interval_page + 2;

        if fpm1_index < free_page_map.len() {
            if free_page_map[fpm1_index] {
                bail!("All FPM pages should be marked BUSY");
            }
        }

        if fpm2_index < free_page_map.len() {
            if free_page_map[fpm2_index] {
                bail!("All FPM pages should be marked BUSY");
            }
            interval += 1;
        } else {
            break;
        }
    }

    Ok(free_page_map)
}

/// Computes the low-bits-on mask for the page mask.
fn low_page_mask(page_size: PageSize) -> u32 {
    (1u32 << page_size.exponent()).wrapping_sub(1u32)
}

/// Tests whether `page` contributes to either FPM1 or FPM2.
fn is_fpm_page_big_msf(page_size: PageSize, page: u32) -> bool {
    let page_within_interval = page & low_page_mask(page_size);
    matches!(page_within_interval, 1 | 2)
}

/// Tests whether `page` is one of the special pages (Page 0, FPM1, or FPM2)
fn is_special_page_big_msf(page_size: PageSize, page: u32) -> bool {
    page == 0 || is_fpm_page_big_msf(page_size, page)
}

/// Describes the "old" MSF Stream Directory Header.
#[derive(Clone, IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable)]
#[repr(C)]
struct OldMsfStreamDirHeader {
    num_streams: U16<LE>,
    ignored: U16<LE>,
}

/// An entry in the "old" MSF Stream Directory.
#[derive(Clone, IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable)]
#[repr(C)]
struct OldMsfStreamEntry {
    stream_size: U32<LE>,
    ignored: U32<LE>,
}

```

`msf/src/pages.rs`:

```rs
//! Page management code

use super::*;
use tracing::{error, trace, trace_span};
use zerocopy::FromZeros;

/// Given the size of a stream in bytes, returns the number of pages needed to store it.
///
/// This function correctly handles the case where the stream size is [`NIL_STREAM_SIZE`].
/// In this case, it returns 0.
pub(crate) fn num_pages_for_stream_size(stream_size: u32, page_size: PageSize) -> u32 {
    if stream_size == NIL_STREAM_SIZE {
        0
    } else {
        stream_size.div_round_up(page_size)
    }
}

/// Maps ranges of bytes within a stream to contiguous ranges of bytes in the containing MSF file.
pub(crate) struct StreamPageMapper<'a> {
    pages: &'a [Page],
    page_size: PageSize,
    stream_size: u32,
}

impl<'a> StreamPageMapper<'a> {
    pub(crate) fn new(pages: &'a [Page], page_size: PageSize, stream_size: u32) -> Self {
        assert_eq!(
            num_pages_for_stream_size(stream_size, page_size) as usize,
            pages.len()
        );

        Self {
            pages,
            page_size,
            stream_size,
        }
    }

    /// Maps a byte offset and a length within a stream to a contiguous run of bytes within the MSF file.
    ///
    /// Repeated calls to this function (with increasing values of `pos`) can be used to read/write
    /// the contents of a stream using the smallest number of read/write calls to the underlying
    /// MSF file.
    ///
    /// Returns `(file_offset, transfer_len)` where `file_offset` is the byte offset within the MSF
    /// file and `transfer_len` is the length of the longest contiguous sub-range of the requested
    /// range.
    ///
    /// If this returns `None` then no bytes can be mapped. This occurs when `pos >= stream_size`.
    ///
    /// Invariants:
    ///
    /// * if returned `Some`, then `transfer_len <= bytes_wanted`
    /// * if returned `Some`, then `transfer_len > 0`
    pub(crate) fn map(&self, pos: u32, bytes_wanted: u32) -> Option<(u64, u32)> {
        if self.stream_size == NIL_STREAM_SIZE {
            return None;
        }

        if pos >= self.stream_size {
            return None;
        }

        let bytes_available = self.stream_size - pos;
        let max_transfer_size = bytes_available.min(bytes_wanted);

        if max_transfer_size == 0 {
            return None;
        }

        // We will reduce transfer_size as needed.
        let transfer_size: u32;

        // Find the position within the file where the read will start.
        let first_page_index = pos >> self.page_size.exponent();
        let first_page_pointer = self.pages[first_page_index as usize];
        let first_page_file_offset = (first_page_pointer as u64) << self.page_size.exponent();

        let offset_within_first_page = pos - self.page_size.align_down(pos);
        let file_offset = first_page_file_offset + offset_within_first_page as u64;

        // Find the longest read we can execute in a single underlying read call.
        // If pages are numbered consecutively, then cover as many pages as we can.

        // Does the beginning of the read cross a page boundary?
        let bytes_available_first_page = u32::from(self.page_size) - offset_within_first_page;
        if max_transfer_size > bytes_available_first_page {
            // Yes, this read crosses a page boundary.
            // Set transfer_size to just the bytes in the first page.
            // Then, keep advancing through the page list as long as pages are sequential.
            let mut p = pos + bytes_available_first_page;
            assert!(self.page_size.is_aligned(p));

            let mut last_page_ptr = first_page_pointer;

            loop {
                assert!(
                    p - pos <= max_transfer_size,
                    "p = {p}, max_transfer_size = {max_transfer_size}"
                );
                let want_bytes = max_transfer_size - (p - pos);

                if p - pos == max_transfer_size {
                    // Reached max transfer size.
                    break;
                }

                let p_page = p >> self.page_size.exponent();

                let p_ptr = self.pages[p_page as usize];
                assert!(p_page > first_page_index);

                if p_ptr != last_page_ptr + 1 {
                    // The pages are not contiguous, so we stop here.
                    break;
                }

                // Advance over this page.
                p += want_bytes.min(u32::from(self.page_size));
                last_page_ptr += 1;
            }

            transfer_size = p - pos;
        } else {
            // This range does not cross a page boundary; it fits within a single page.
            transfer_size = max_transfer_size;
        }

        assert!(transfer_size > 0);

        assert!(
            transfer_size <= bytes_wanted,
            "transfer_size = {}, bytes_wanted = {}",
            transfer_size,
            bytes_wanted
        );

        Some((file_offset, transfer_size))
    }
}

#[test]
fn test_page_mapper_nil() {
    const PAGE_SIZE: PageSize = PageSize::from_exponent(12); // 0x1000

    let mapper = StreamPageMapper::new(&[], PAGE_SIZE, NIL_STREAM_SIZE);
    assert_eq!(mapper.map(0, 0), None);
    assert_eq!(mapper.map(0x1000, 0x1000), None);
}

#[test]
fn test_page_mapper_basic() {
    const PAGE_SIZE: PageSize = PageSize::from_exponent(12); // 0x1000

    let mapper = StreamPageMapper::new(&[5, 6, 7, 300, 301], PAGE_SIZE, 0x4abc);

    assert_eq!(mapper.map(0, 0), None, "empty read within stream boundary");

    assert_eq!(
        mapper.map(0x1000_0000, 0),
        None,
        "empty read outside stream boundary"
    );

    assert_eq!(
        mapper.map(0x1000_0000, 0x1000),
        None,
        "outside stream boundary"
    );

    assert_eq!(
        mapper.map(0, 0x10),
        Some((0x5000, 0x10)),
        "aligned start, unaligned end, within first page"
    );

    assert_eq!(
        mapper.map(0, 0x1000),
        Some((0x5000, 0x1000)),
        "aligned start, aligned end, single page"
    );

    assert_eq!(
        mapper.map(0, 0x1eee),
        Some((0x5000, 0x1eee)),
        "aligned start, crosses page boundary, unaligned end"
    );

    assert_eq!(
        mapper.map(0, 0x3eee),
        Some((0x5000, 0x3000)),
        "aligned start, crosses page boundary, unaligned end, clipped at page boundary"
    );

    assert_eq!(
        mapper.map(0, 0x1000_0000),
        Some((0x5000, 0x3000)),
        "aligned start, aligned end beyond stream size, max contiguous span"
    );

    assert_eq!(
        mapper.map(0xccc, 0x10),
        Some((0x5ccc, 0x10)),
        "unaligned start, ends within first page"
    );

    assert_eq!(
        mapper.map(0xccc, 0x1000),
        Some((0x5ccc, 0x1000)),
        "unaligned start, crosses page boundary, unaligned end"
    );

    assert_eq!(
        mapper.map(0xccc, 0x1000_0000),
        Some((0x5ccc, 0x2334)),
        "unaligned start, crosses page boundary, clipped at page boundary"
    );
}

/// Contains state for allocating pages.
///
/// The `fpm`, `fpm_freed` and `fresh` bit vectors all describe the state of pages. These are
/// parallel vectors; the same index in each vector is related to the same page. Only certain
/// combinations of values for these vectors are legal.
///
/// Each page `p` can be in one of the following states:
///
/// `fpm[p]`  | `fpm_freed[p]` | State    | Description
/// ----------|----------------|----------|------------------
/// `true`    | `false`        | FREE     | The page is available for use. This page is not used by any stream.
/// `false`   | `false`        | BUSY     | The page is being used by a stream.
/// `false`   | `true`         | DELETING | The page is used by the previous (committed) state of some stream, but has been deleted in the next (uncommitted) state.
/// `true`    | `true`         | (illegal) | (illegal)
pub(super) struct PageAllocator {
    /// Free Page Map (FPM): A bit vector that lists the pages in the MSF that are free.
    pub(super) fpm: BitVec<u32, Lsb0>,

    /// A bit vector that lists the pages in the MSF that are valid in the committed state
    /// but which have been deleted in the uncommitted state.
    pub(super) fpm_freed: BitVec<u32, Lsb0>,

    /// A bit vector that tells us whether we have copied a page and it is now writable.
    /// This vector is set to all-false. Whenever we allocate a new page and copy some contents to
    /// it, or when we allocate a new page, we set the corresponding bit in `fresh`.
    pub(super) fresh: BitVec<u32, Lsb0>,

    /// The index of the next free page to check in the Free Page Map, when allocating pages.
    ///
    /// This index is not guaranteed to point to an entry that is free. It points to the next index
    /// to check.
    ///
    /// Invariant: `next_free_page <= num_pages`
    pub(super) next_free_page_hint: Page,

    /// The number of valid pages in the file. This value comes from the MSF File Header.
    pub(super) num_pages: u32,

    pub(super) page_size: PageSize,

    /// A reusable buffer whose length is `page_size`.
    #[allow(dead_code)]
    pub(super) page_buffer: Box<[u8]>,
}

impl PageAllocator {
    /// Allocate a bit vector for the FPM. We are going to compute this FPM from num_pages
    /// and the contents of the stream directory. After we finish computing it, we will also
    /// read the active FPM from disk and verify that it is exactly the same as the one that
    /// we computed.
    ///
    /// We begin with setting _all_ pages free. Then we mark Page 0 and the FPM pages as busy.
    /// It is the caller's responsibility to set other bits in the FPM accordingly.
    pub(crate) fn new(num_pages: usize, page_size: PageSize) -> Self {
        let mut fpm: BitVec<u32, Lsb0> = BitVec::with_capacity(num_pages);
        fpm.resize(num_pages, true);
        fpm.set(0, false);

        // Mark FPM pages as busy.
        for interval in 0u32.. {
            let fpm1_page = (interval << page_size.exponent()) + 1u32;
            let fpm2_page = fpm1_page + 1u32;
            if let Some(mut b) = fpm.get_mut(fpm1_page as usize) {
                b.set(false);
            }
            if let Some(mut b) = fpm.get_mut(fpm2_page as usize) {
                b.set(false);
            } else {
                break;
            }
        }

        let mut fpm_freed: BitVec<u32, Lsb0> = BitVec::with_capacity(num_pages);
        fpm_freed.resize(num_pages, false);

        let mut fresh: BitVec<u32, Lsb0> = BitVec::with_capacity(num_pages);
        fresh.resize(num_pages, false);

        Self {
            fpm,
            fpm_freed,
            fresh,
            next_free_page_hint: 0,
            num_pages: num_pages as u32,
            page_size,
            // unwrap() is for OOM handling
            page_buffer: FromZeros::new_box_zeroed_with_elems(usize::from(page_size)).unwrap(),
        }
    }

    pub(crate) fn alloc_page_buffer(&self) -> Box<[u8]> {
        // unwrap() is for OOM handling
        FromZeros::new_box_zeroed_with_elems(usize::from(self.page_size)).unwrap()
    }

    /// This marks a stream page as being busy. This is used only when loading the
    /// Stream Directory. This should not be used for Stream 0, which is the Old Stream Directory.
    ///
    /// The `stream` and `stream_page` values are only for diagnostics.
    pub(crate) fn init_mark_stream_page_busy(
        &mut self,
        page: Page,
        stream: u32,
        stream_page: StreamPage,
    ) -> anyhow::Result<()> {
        if let Some(mut b) = self.fpm.get_mut(page as usize) {
            if !*b {
                error!(page, stream, stream_page, "Page cannot be marked busy, because it is already marked busy. It may be used by more than one stream.");
                bail!("Page {page} cannot be marked busy, because it is already marked busy. It may be used by more than one stream.");
            }

            b.set(false);
            Ok(())
        } else {
            error!(
                page,
                stream, stream_page, "Page is invalid; it is out of range (exceeds num_pages)"
            );
            bail!(
                "Page {} is invalid; it is out of range (exceeds num_pages)",
                page
            );
        }
    }

    /// This marks a page as "busy but freed". This is called for pages of the Stream Directory,
    /// and when using Big MSF, for pages of the Page Map.
    ///
    /// These pages are marked "free pending" (freed) because these pages become unused after
    /// the next successful call to `commit()`.
    pub(crate) fn init_mark_stream_dir_page_busy(&mut self, page: Page) -> anyhow::Result<()> {
        // We mark the page as "freed". It is still marked "free" in the FPM.
        // The page allocator will not use this page, because it is marked "freed".

        let Some(mut b) = self.fpm.get_mut(page as usize) else {
            bail!(
                "Page {} is invalid; it is out of range (exceeds num_pages)",
                page
            );
        };

        if !*b {
            bail!("Page {page} cannot be marked busy, because it is already marked busy. It may be used by more than one stream.");
        }
        b.set(false);

        // Now mark the page as "freed".
        let Some(mut freed) = self.fpm_freed.get_mut(page as usize) else {
            bail!(
                "Page {} is invalid; it is out of range (exceeds num_pages)",
                page
            );
        };
        if *freed {
            bail!("Page {page} cannot be marked 'freed', because it is already marked freed. This indicates that the page was used more than once in the Stream Directory or Page Map.");
        }
        freed.set(true);

        Ok(())
    }

    /// Allocates a single page.
    ///
    /// This function does not do any disk I/O. It only updates in-memory state.
    pub(crate) fn alloc_page(&mut self) -> Page {
        let (page, run_len) = self.alloc_pages(1);
        debug_assert_eq!(run_len, 1);
        page
    }

    /// Allocates one or more contiguous pages.
    ///
    /// This function handles crossing interval boundaries. If `num_pages_wanted` is large enough that
    /// it crosses an interval boundary, then this function will return a run length that is
    /// smaller than `num_pages_wanted`.  If this function does not cross an interval boundary, then
    /// the returned `run_len` value will be equal to `num_pages_wanted`.
    ///
    /// This function does not do any disk I/O. It only updates in-memory state.
    pub(crate) fn alloc_pages(&mut self, num_pages_wanted: u32) -> (Page, u32) {
        let _span = trace_span!("alloc_pages").entered();
        trace!(num_pages_wanted);

        assert!(num_pages_wanted > 0);
        assert_eq!(self.num_pages as usize, self.fpm.len());
        assert_eq!(self.num_pages as usize, self.fpm_freed.len());
        assert!(self.next_free_page_hint <= self.num_pages);

        // First, check to see whether an existing page is free.
        if self.next_free_page_hint < self.fpm.len() as u32 {
            if let Some(i) = self.fpm.as_bitslice()[self.next_free_page_hint as usize..].first_one()
            {
                let p0: Page = self.next_free_page_hint + i as u32;

                // We found an existing free page. Mark the page as busy.
                self.fpm.set(p0 as usize, false);
                self.fresh.set(p0 as usize, true);
                self.next_free_page_hint = p0 + 1;

                let mut run_len: u32 = 1;

                // See if the pages immediately following this first page are also free.
                // If they are, then claim them, too.
                while run_len < num_pages_wanted
                    && p0 + run_len < self.num_pages
                    && self.fpm[self.next_free_page_hint as usize]
                {
                    self.fpm.set(self.next_free_page_hint as usize, false);
                    self.fresh.set(self.next_free_page_hint as usize, true);
                    self.next_free_page_hint += 1;
                    run_len += 1;
                }

                trace!(first_page = p0, run_len, "allocated pages");
                return (p0, run_len);
            }

            // There are no more free pages. Fast-forward to the end of the FPM so we don't
            // waste time re-scanning this part of the FPM on future calls.
            trace!("there are no free pages");
            self.next_free_page_hint = self.fpm.len() as u32;
        }

        // We need to add new pages to the MSF file.
        trace!(num_pages = self.num_pages, "adding new pages to MSF file");
        assert_eq!(self.next_free_page_hint, self.num_pages);
        let low_mask = (1u32 << self.page_size.exponent()) - 1;
        let page_size = u32::from(self.page_size);
        let num_pages_available = match self.num_pages & low_mask {
            0 => {
                // This is an unusual but legal case. num_pages is currently positioned exactly at
                // the beginning of an interval. There is exactly 1 usable page at the start of an
                // interval; after that page is the FPM1 and then the FPM2. So we can only allocate
                // a single page.
                trace!("num_pages is positioned on first page of an interval; can only allocate 1 page");
                1
            }
            1 => {
                // num_pages is positioned on FPM1. That's fine.  Step over FPM1 and FPM2.
                // Increment phase to pretend like that's how we got here in the first place.
                trace!("num_pages is positioned on FPM1; incrementing by 2 and using remainder of interval");
                self.fpm_freed.push(false); // FPM1
                self.fpm_freed.push(false); // FPM2
                self.fpm.push(false); // FPM1
                self.fpm.push(false); // FPM2
                self.num_pages += 2;
                self.next_free_page_hint += 2;
                page_size - 2
            }
            2 => {
                // We are positioned on FPM2. That's unusual but OK. Step over FPM2 and mark it
                // as busy.
                trace!("num_pages is positioned on FPM2; incrementing by 1 and using remainder of interval");
                self.fpm_freed.push(false);
                self.fpm.push(false);
                self.num_pages += 1;
                self.next_free_page_hint += 1;
                page_size - 2
            }
            phase => page_size - phase + 1,
        };

        assert_eq!(self.next_free_page_hint, self.num_pages);

        let num_pages_allocated = num_pages_available.min(num_pages_wanted);
        assert!(num_pages_allocated > 0);

        let start_page = self.num_pages;
        self.num_pages += num_pages_allocated;

        // Extend the bitmaps and set their new values.
        self.fpm_freed.resize(self.num_pages as usize, false);
        self.fpm.resize(self.num_pages as usize, false);
        self.fresh.resize(self.num_pages as usize, true);

        // Advance next_free_page so that we don't keep re-scanning the same region of
        // the FPM repeatedly.
        self.next_free_page_hint = self.num_pages;

        assert_eq!(self.num_pages as usize, self.fpm.len());
        assert_eq!(self.num_pages as usize, self.fpm_freed.len());
        assert_eq!(self.num_pages as usize, self.fresh.len());

        trace!(start_page, num_pages_allocated, "allocated pages");
        (start_page, num_pages_allocated)
    }

    /// Ensures that a given page is mutable (is "fresh", i.e. can be modified in the uncommitted
    /// state).
    ///
    /// * If `*page` is not fresh, then this function allocates a new page and assigns its page
    ///   number to `*page`. This function _does not_ do any disk I/O; it does not copy the
    ///   contents of the old page to the new.
    ///
    /// * If `*page` is already fresh, then this function does nothing.
    pub(crate) fn make_page_fresh(&mut self, page: &mut Page) -> Page {
        let p = *page as usize;

        if self.fresh[p] {
            *page
        } else {
            let (new_page, _) = self.alloc_pages(1);
            self.fpm_freed.set(p, true);
            *page = new_page;
            new_page
        }
    }

    /// Ensures that a sequence of pages are "fresh" (can be modified in the uncommitted state).
    ///
    /// This function ensures that each page number in `pages` points to a fresh page. If an
    /// existing page number is not fresh, then this function will allocate a new page and replace
    /// the old page number with the new one.
    ///
    /// This function _does not_ do any disk I/O. It does not copy the contents of old pages to
    /// new pages.
    pub(crate) fn make_pages_fresh(&mut self, pages: &mut [Page]) {
        for p in pages.iter_mut() {
            self.make_page_fresh(p);
        }
    }

    /// Checks that the `fpm` and `fpm_freed` vectors are consistent.
    pub(crate) fn check_vector_consistency(&self) -> anyhow::Result<()> {
        let num_pages = self.num_pages as usize;
        assert_eq!(num_pages, self.fpm.len());
        assert_eq!(num_pages, self.fpm_freed.len());

        for i in 0..num_pages {
            let free = self.fpm[i];
            let freed = self.fpm_freed[i];

            match (free, freed) {
                (true, false) => {} // FREE
                (true, true) => {
                    bail!("Page {i} is in illegal state: marked 'free' and 'freed' (free pending)")
                }
                (false, false) => {} // BUSY
                (false, true) => {}  // FREED
            }
        }

        Ok(())
    }

    /// Merges the "freed" bit map into the "free" bitmap and clears the "freed" bitmap.
    ///
    /// This is part of the commit protocol.
    pub fn merge_freed_into_free(&mut self) {
        let fpm_words: &mut [u32] = self.fpm.as_raw_mut_slice();
        let freed_words: &mut [u32] = self.fpm_freed.as_raw_mut_slice();

        for (free, freed) in fpm_words.iter_mut().zip(freed_words.iter_mut()) {
            *free |= *freed;
            *freed = 0;
        }
    }

    /// Checks invariants that are visible at this scope.
    #[inline(never)]
    pub fn assert_invariants(&self) {
        assert!(self.num_pages > 0);
        assert_eq!(self.num_pages as usize, self.fpm.len());
        assert_eq!(self.num_pages as usize, self.fpm_freed.len());

        // Check that page 0, which stores the MSF File Header, is busy.
        assert!(!self.fpm[0], "Page 0 should always be BUSY");
        assert!(!self.fpm_freed[0], "Page 0 should never be deleted");

        // Check that the pages assigned to the FPM are marked "busy" in all intervals.

        let mut interval: u32 = 0;
        loop {
            let p = (interval << self.page_size.exponent()) as usize;
            let fpm1_index = p + 1;
            let fpm2_index = p + 2;

            if fpm1_index < self.fpm.len() {
                assert!(!self.fpm[fpm1_index], "All FPM pages should be marked BUSY");
                assert!(
                    !self.fpm_freed[fpm1_index],
                    "FPM pages should never be deleted"
                );
            }

            if fpm2_index < self.fpm.len() {
                assert!(!self.fpm[fpm2_index], "All FPM pages should be marked BUSY");
                assert!(
                    !self.fpm_freed[fpm2_index],
                    "FPM pages should never be deleted"
                );
                interval += 1;
            } else {
                break;
            }
        }

        // Check that the free/deleted bit vectors are consistent.
        for page in 0..self.num_pages {
            let is_free = self.fpm[page as usize];
            let is_freed = self.fpm_freed[page as usize];
            assert!(
                !(is_free && is_freed),
                "page {page} is in illegal state (both 'free' and 'freed')"
            );
        }
    }
}

```

`msf/src/read.rs`:

```rs
//! Code for reading data from streams

use sync_file::ReadAt;
use tracing::{trace, trace_span};

use crate::pages::StreamPageMapper;
use crate::{Page, PageSize};

/// This reads data from a stream. It maps byte offsets within a stream to byte offsets within the
/// containing MSF file.
///
/// It will read as much data in a single `read()` call (to the underlying storage) as it can,
/// provided the pages within the stream are contiguous.
///
/// Returns `(bytes_transferred, new_pos)`, where `new_pos` is the position within the stream
/// after the last byte was read. If no bytes were transferred, then this is the same as `pos`.
/// Note that it is possible for `pos` (and thus `new_pos`) to be greater than `stream_size`.
pub(super) fn read_stream_core<F: ReadAt>(
    stream: u32,
    file: &F,
    page_size: PageSize,
    stream_size: u32,
    pages: &[Page],
    stream_pos: u64,
    dst: &mut [u8],
) -> std::io::Result<(usize, u64)> {
    let _span = trace_span!("read_stream_core").entered();

    // Early out for a read at the end. This also handles checking the 64-bit stream position
    // vs 32-bit, so we can safely cast to u32 after this check.
    if stream_pos >= stream_size as u64 {
        return Ok((0, stream_pos));
    }

    let mut stream_pos = stream_pos as u32;

    let original_len = dst.len();
    let mut remaining_dst = dst;

    let mapper = StreamPageMapper::new(pages, page_size, stream_size);

    while !remaining_dst.is_empty() && stream_pos < stream_size {
        let Some((file_offset, transfer_size)) = mapper.map(stream_pos, remaining_dst.len() as u32)
        else {
            break;
        };

        let (dst_this_transfer, dst_next) = remaining_dst.split_at_mut(transfer_size as usize);

        trace!(
            stream,
            stream_pos,
            transfer_size,
            file_offset,
            "reading stream data"
        );

        file.read_exact_at(dst_this_transfer, file_offset)?;

        stream_pos += transfer_size;
        remaining_dst = dst_next;
    }

    let total_bytes_transferred = original_len - remaining_dst.len();
    Ok((total_bytes_transferred, stream_pos as u64))
}

```

`msf/src/stream_reader.rs`:

```rs
use super::*;

/// Allows reading a stream using the [`Read`], [`Seek`], and [`ReadAt`] traits.
pub struct StreamReader<'a, F> {
    /// The stream index. This is used only for diagnostics.
    stream: u32,
    /// Size in bytes of the stream. This value _is never_ equal to [`NIL_STREAM_SIZE`].
    stream_size: u32,
    is_nil: bool,
    /// Page size of the MSF file.
    page_size: PageSize,
    /// Maps page indices within the stream to page indices within the MSF file.
    page_map: &'a [Page],
    /// Provides access to the MSF file contents.
    file: &'a F,
    /// The seek position of the stream reader.
    pos: u64,
}

impl<'a, F: ReadAt> StreamReader<'a, F> {
    pub(crate) fn new(
        pdb: &'a Msf<F>,
        stream: u32,
        stream_size: u32,
        page_map: &'a [Page],
        pos: u64,
    ) -> Self {
        Self {
            stream,
            stream_size: if stream_size == NIL_STREAM_SIZE {
                0
            } else {
                stream_size
            },
            is_nil: stream_size == NIL_STREAM_SIZE,
            page_size: pdb.pages.page_size,
            page_map,
            file: &pdb.file,
            pos,
        }
    }

    /// Size in bytes of the stream.
    ///
    /// This will be zero for nil streams.
    pub fn len(&self) -> u32 {
        self.stream_size
    }

    /// Tests whether this stream is empty (zero-length)
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    /// Returns `true` if this is a nil stream.
    pub fn is_nil(&self) -> bool {
        self.is_nil
    }
}

impl<'a, F: ReadAt> Seek for StreamReader<'a, F> {
    fn seek(&mut self, from: SeekFrom) -> std::io::Result<u64> {
        let new_pos: i64 = match from {
            SeekFrom::Start(offset) => offset as i64,
            SeekFrom::End(signed_offset) => signed_offset + self.stream_size as i64,
            SeekFrom::Current(signed_offset) => self.pos as i64 + signed_offset,
        };

        if new_pos < 0 {
            return Err(std::io::ErrorKind::InvalidInput.into());
        }
        self.pos = new_pos as u64;
        Ok(self.pos)
    }
}

impl<'a, F: ReadAt> Read for StreamReader<'a, F> {
    fn read(&mut self, dst: &mut [u8]) -> std::io::Result<usize> {
        let (n, new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_size,
            self.stream_size,
            self.page_map,
            self.pos,
            dst,
        )?;

        self.pos = new_pos;
        Ok(n)
    }
}

impl<'a, F: ReadAt> ReadAt for StreamReader<'a, F> {
    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        let (n, _new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_size,
            self.stream_size,
            self.page_map,
            offset,
            buf,
        )?;
        if n != buf.len() {
            return Err(std::io::Error::from(std::io::ErrorKind::UnexpectedEof));
        }
        Ok(())
    }

    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        let (n, _new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_size,
            self.stream_size,
            self.page_map,
            offset,
            buf,
        )?;
        Ok(n)
    }
}

```

`msf/src/stream_writer.rs`:

```rs
use super::*;
use std::cell::RefCell;
use tracing::{trace, trace_span};

/// Provides read/write access for a stream within an MSF (PDB) file.
pub struct StreamWriter<'a, F> {
    /// The stream number. This is used only for diagnostics.
    pub(super) stream: u32,

    pub(super) file: &'a F,

    /// The current byte size of this stream. Points directly into the `stream_sizes` vector.
    ///
    /// This value can be [`NIL_STREAM_SIZE`].
    pub(super) size: &'a mut u32,

    pub(super) page_allocator: &'a mut PageAllocator,

    /// The set of pages owned by this stream.
    pub(super) pages: &'a mut Vec<Page>,

    /// The seek position of this `StreamWriter`.
    ///
    /// `pos` may be greater than `size`. If data is written to a stream when `pos` is greater than
    /// (or equal to) `size`, then the stream is extended to contain the data.
    pub(super) pos: u64,
}

impl<'a, F> StreamWriter<'a, F> {
    /// The length in bytes of this stream. This will return zero for nil streams.
    pub fn len(&self) -> u32 {
        if *self.size == NIL_STREAM_SIZE {
            0
        } else {
            *self.size
        }
    }

    /// Returns `true` if the stream is zero-length or is a nil stream.
    pub fn is_empty(&self) -> bool {
        *self.size == 0 || *self.size == NIL_STREAM_SIZE
    }

    /// Replaces the contents of a stream. The length of the stream is set to the length of `data`.
    pub fn set_contents(&mut self, data: &[u8]) -> std::io::Result<()>
    where
        F: ReadAt + WriteAt,
    {
        let _span = trace_span!("StreamWriter::set_contents").entered();

        if data.len() as u64 >= NIL_STREAM_SIZE as u64 {
            return Err(std::io::ErrorKind::InvalidInput.into());
        }
        let data_len = data.len() as u32;

        // If the existing stream has more data than the caller, then truncate now.  This is a
        // half-hearted attempt to avoid unnecessary read-modify-write cycles.
        if *self.size > data_len {
            self.set_len(data_len)?;
        }

        self.write_core(data, 0)?;
        self.set_len(data.len() as u32)?;
        Ok(())
    }

    /// Writes data to a given position in the file.
    ///
    /// This method exists to work around the limitation of `WriteAt`, which takes `&self`
    /// instead of `&mut self`.
    pub fn write_at_mut(&mut self, buf: &[u8], offset: u64) -> std::io::Result<usize>
    where
        F: ReadAt + WriteAt,
    {
        let _span = trace_span!("StreamWriter::write_at_mut").entered();

        self.write_core(buf, offset)?;
        Ok(buf.len())
    }

    /// Writes data to a given position in the file.
    ///
    /// This method exists to work around the limitation of `WriteAt`, which takes `&self`
    /// instead of `&mut self`.
    pub fn write_all_at_mut(&mut self, buf: &[u8], offset: u64) -> std::io::Result<()>
    where
        F: ReadAt + WriteAt,
    {
        self.write_core(buf, offset)
    }

    /// Sets the length of this stream, in bytes.
    ///
    /// If the stream is a nil stream, this changes it to a non-nil stream, even if `len` is zero.
    ///
    /// If you are writing data to a stream, try to avoid using `set_len` to preallocate storage.
    /// It will allocate pages (yay) but it will waste I/O time by writing zero-fill data to
    /// all of the pages.  There is currently no optimized path for preallocating pages that relies
    /// on the underlying storage provide zero-filling pages.
    ///
    /// If you are replacing the contents of a large stream, just write everything from page zero
    /// and then call `set_len` at the end of the write.
    pub fn set_len(&mut self, mut len: u32) -> std::io::Result<()>
    where
        F: ReadAt + WriteAt,
    {
        use std::cmp::Ordering;

        let _span = trace_span!("StreamWriter::set_len").entered();
        trace!(new_len = len);

        if *self.size == NIL_STREAM_SIZE {
            trace!("stream changes from nil to non-nil");
            *self.size = 0;
        }

        let page_size = self.page_allocator.page_size;

        match Ord::cmp(&len, self.size) {
            Ordering::Equal => {
                trace!(len = self.size, "no change in stream size");
            }

            Ordering::Less => {
                // Truncating the stream. Find the number of pages that need to be freed.
                trace!(old_len = self.size, new_len = len, "reducing stream size");

                let num_pages_old = num_pages_for_stream_size(*self.size, page_size) as usize;
                let num_pages_new = num_pages_for_stream_size(len, page_size) as usize;
                assert!(num_pages_new <= num_pages_old);

                for &page in self.pages[num_pages_new..num_pages_old].iter() {
                    self.page_allocator.fpm_freed.set(page as usize, true);
                }

                self.pages.truncate(num_pages_new);
                *self.size = len;
            }

            Ordering::Greater => {
                // Zero-extend the stream.
                trace!(
                    old_len = self.size,
                    new_len = len,
                    "increasing stream size (zero-filling)"
                );

                let end_phase = offset_within_page(*self.size, page_size);
                if end_phase != 0 {
                    // Total number of bytes we need to fill
                    let total_zx_bytes = len - *self.size;

                    // zero-extend partial page
                    let end_spage = *self.size / page_size;
                    let num_zx_bytes = (u32::from(page_size) - end_phase).min(total_zx_bytes);

                    let mut page_buffer = self.page_allocator.alloc_page_buffer();
                    self.read_page(end_spage, &mut page_buffer)?;
                    page_buffer[end_phase as usize..].fill(0);
                    self.cow_page_and_write(end_spage, &page_buffer)?;

                    *self.size += num_zx_bytes;

                    len -= num_zx_bytes;
                    if len == 0 {
                        // We may have finished without reaching the end of this page.
                        return Ok(());
                    }
                }

                // The code above should have handled aligning the current size of the stream
                // (or returning if we are done).
                assert!(page_size.is_aligned(*self.size));

                let mut page_buffer = self.page_allocator.alloc_page_buffer();
                page_buffer.fill(0);

                assert!(page_size.is_aligned(*self.size));

                // num_zx_pages includes any partial page at the end.
                let num_zx_pages_wanted = (len - *self.size).div_round_up(page_size);

                let (first_page, run_len) = self.page_allocator.alloc_pages(num_zx_pages_wanted);
                assert!(run_len > 0);

                let old_num_pages = self.pages.len() as u32;

                for i in 0..run_len {
                    self.pages.push(first_page + i);
                }

                // This size increase may cover a partial page.
                *self.size += len;

                // TODO: If the app calls set_len() with a large size, this will be inefficient,
                // since we issue one write per page.  We could avoid that in the case where we
                // are extending the MSF file with fresh pages, at the end, and rely on a single
                // "set length" call to the underlying file.
                for i in 0..run_len {
                    self.write_page(old_num_pages + i, &page_buffer)?;
                }
            }
        }

        Ok(())
    }

    /// Converts this `StreamWriter` into a `RandomStreamWriter`.
    pub fn into_random(self) -> RandomStreamWriter<'a, F> {
        RandomStreamWriter {
            cell: RefCell::new(self),
        }
    }
}

impl<'a, F: ReadAt> std::io::Seek for StreamWriter<'a, F> {
    fn seek(&mut self, from: SeekFrom) -> std::io::Result<u64> {
        let new_pos: i64 = match from {
            SeekFrom::Start(offset) => offset as i64,
            SeekFrom::End(signed_offset) => signed_offset + *self.size as i64,
            SeekFrom::Current(signed_offset) => self.pos as i64 + signed_offset,
        };

        if new_pos < 0 {
            return Err(std::io::ErrorKind::InvalidInput.into());
        }

        self.pos = new_pos as u64;
        Ok(self.pos)
    }
}

impl<'a, F: ReadAt> std::io::Read for StreamWriter<'a, F> {
    fn read(&mut self, dst: &mut [u8]) -> std::io::Result<usize> {
        let (n, new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_allocator.page_size,
            *self.size,
            self.pages,
            self.pos,
            dst,
        )?;
        self.pos = new_pos;
        Ok(n)
    }
}

impl<'a, F: ReadAt + WriteAt> std::io::Write for StreamWriter<'a, F> {
    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        self.write_core(buf, self.pos)?;
        self.pos += buf.len() as u64;
        Ok(buf.len())
    }

    fn flush(&mut self) -> std::io::Result<()> {
        Ok(())
    }
}

pub struct RandomStreamWriter<'a, F> {
    cell: RefCell<StreamWriter<'a, F>>,
}

impl<'a, F: ReadAt> ReadAt for RandomStreamWriter<'a, F> {
    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        let sw = self.cell.borrow();
        let (n, _new_pos) = super::read::read_stream_core(
            sw.stream,
            &sw.file,
            sw.page_allocator.page_size,
            *sw.size,
            sw.pages,
            offset,
            buf,
        )?;
        if n != buf.len() {
            return Err(std::io::Error::from(std::io::ErrorKind::UnexpectedEof));
        }
        Ok(())
    }

    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        let sw = self.cell.borrow();
        let (n, _new_pos) = super::read::read_stream_core(
            sw.stream,
            &sw.file,
            sw.page_allocator.page_size,
            *sw.size,
            sw.pages,
            offset,
            buf,
        )?;
        Ok(n)
    }
}

impl<'a, F: ReadAt + WriteAt> WriteAt for RandomStreamWriter<'a, F> {
    fn write_at(&self, buf: &[u8], offset: u64) -> std::io::Result<usize> {
        let mut sw = self.cell.borrow_mut();
        sw.write_core(buf, offset)?;
        Ok(buf.len())
    }

    fn write_all_at(&self, buf: &[u8], offset: u64) -> std::io::Result<()> {
        let mut sw = self.cell.borrow_mut();
        sw.write_core(buf, offset)
    }
}

```

`msf/src/tests.rs`:

```rs
#![allow(clippy::useless_vec)]

use super::*;
use anyhow::Result;
use pretty_hex::PrettyHex;
use std::io::{Cursor, Seek, SeekFrom, Write};
use std::sync::Mutex;
use sync_file::{ReadAt, WriteAt};
use tracing::{debug, debug_span, trace, trace_span};

#[static_init::dynamic]
static INIT_LOGGER: () = {
    use tracing_subscriber::fmt::format::FmtSpan;

    tracing_subscriber::fmt::fmt()
        .compact()
        .with_max_level(tracing_subscriber::filter::LevelFilter::TRACE)
        .with_level(false)
        .with_file(true)
        .with_line_number(true)
        .with_span_events(FmtSpan::ENTER | FmtSpan::EXIT)
        .with_test_writer()
        .without_time()
        .with_ansi(false)
        .init();
};

macro_rules! assert_bytes_eq {
    ($a:expr, $b:expr) => {
        match (&($a), &($b)) {
            (a, b) => {
                let a_bytes: &[u8] = a.as_ref();
                let b_bytes: &[u8] = b.as_ref();

                if a_bytes != b_bytes {
                    panic!(
                        "Bytes do not match:\n{}\n{}",
                        a_bytes.hex_dump(),
                        b_bytes.hex_dump()
                    );
                }
            }
        }
    };

    ($a:expr, $b:expr, $($msg:tt)*) => {
        match (&($a), &($b)) {
            (a, b) => {
                let a_bytes: &[u8] = a.as_ref();
                let b_bytes: &[u8] = b.as_ref();

                if a_bytes != b_bytes {
                    let msg = format!($($msg)*);
                    panic!(
                        "Bytes do not match: {msg}\n{:?}\n{:?}",
                        a_bytes.hex_dump(),
                        b_bytes.hex_dump()
                    );
                }
            }
        }
    };
}

struct WritePair<Test, Good> {
    test: Test,
    good: Good,
}

impl<Test: Write, Good: Write> std::io::Write for WritePair<Test, Good> {
    fn flush(&mut self) -> std::io::Result<()> {
        self.test.flush()?;
        self.good.flush()?;
        Ok(())
    }

    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        let len_test = self.test.write(buf)?;
        let len_good = self.good.write(buf)?;
        assert_eq!(len_test, len_good);
        Ok(len_test)
    }

    fn write_all(&mut self, buf: &[u8]) -> std::io::Result<()> {
        self.test.write_all(buf)?;
        self.good.write_all(buf)?;
        Ok(())
    }
}

impl<A: Seek, B: Seek> std::io::Seek for WritePair<A, B> {
    fn seek(&mut self, pos: SeekFrom) -> std::io::Result<u64> {
        let pos_test = self.test.seek(pos)?;
        let pos_good = self.good.seek(pos)?;
        assert_eq!(pos_test, pos_good);
        Ok(pos_test)
    }
}

#[derive(Default)]
struct TestFile {
    data: Mutex<Vec<u8>>,
}

impl ReadAt for TestFile {
    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        let _span = trace_span!("TestFile::read_exact_at").entered();
        debug!(offset, buf_len = buf.len(), "TestFile::read_exact_at");
        let lock = self.data.lock().unwrap();
        lock.read_exact_at(buf, offset)?;
        debug!(data = buf, "read received");
        Ok(())
    }

    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        let lock = self.data.lock().unwrap();
        let n = lock.read_at(buf, offset)?;
        debug!(data = &buf[..n], "TestFile::read_at: read received");
        Ok(n)
    }
}

impl WriteAt for TestFile {
    fn write_at(&self, buf: &[u8], offset: u64) -> std::io::Result<usize> {
        self.write_all_at(buf, offset)?;
        Ok(buf.len())
    }

    fn write_all_at(&self, buf: &[u8], offset: u64) -> std::io::Result<()> {
        debug!(
            offset = offset,
            len = buf.len(),
            data = buf,
            "TestFile: write_all_at"
        );

        let mut lock = self.data.lock().unwrap();
        let vec: &mut Vec<u8> = &mut lock;

        let offset = offset as usize;

        if offset == vec.len() {
            vec.extend_from_slice(buf);
        } else {
            let new_len = buf.len() + offset;
            if new_len > vec.len() {
                vec.resize(new_len, 0);
            }
            vec[offset..offset + buf.len()].copy_from_slice(buf);
        }
        Ok(())
    }
}

struct Tester {
    msf: Msf<TestFile>,
}

fn tester() -> Tester {
    println!();

    let f = TestFile::default();
    let msf = Msf::create_for(f, CreateOptions::default()).unwrap();
    Tester { msf }
}

/// Contains enough state from an MSF file that we can fake up a StreamWriter for testing.
struct StreamTester {
    file: TestFile,
    stream_size: u32,
    page_allocator: PageAllocator,
    pages: Vec<Page>,
    expected_stream_data: Vec<u8>,
}

impl StreamTester {
    fn new() -> Self {
        Self {
            file: Default::default(),
            stream_size: 0,
            page_allocator: PageAllocator::new(0x100, DEFAULT_PAGE_SIZE),
            pages: Vec::new(),
            expected_stream_data: Vec::new(),
        }
    }

    fn writer(&mut self) -> WritePair<StreamWriter<'_, TestFile>, Cursor<&mut Vec<u8>>> {
        let good = Cursor::new(&mut self.expected_stream_data);

        let test = StreamWriter {
            stream: 100, // fake stream number
            file: &mut self.file,
            size: &mut self.stream_size,
            page_allocator: &mut self.page_allocator,
            pages: &mut self.pages,
            pos: 0,
        };

        WritePair { good, test }
    }

    #[inline(never)]
    #[track_caller]
    fn write_at(&mut self, pos: u64, data: &[u8]) {
        let _span = debug_span!("StreamTester::write_at");
        debug!(
            pos,
            current_stream_size = self.stream_size,
            piece_contents = data,
            "write_at"
        );

        let mut w = self.writer();
        w.seek(SeekFrom::Start(pos)).unwrap();
        w.write_all(data).unwrap();
        self.check_data();
    }

    // Verifies that the data stored in the stream is consistent with the data that we also wrote
    // into expected_stream_data.
    #[track_caller]
    fn check_data(&self) {
        assert_eq!(
            self.stream_size as usize,
            self.expected_stream_data.len(),
            "stream sizes should be same"
        );

        let page_size = self.page_allocator.page_size;

        assert_eq!(
            num_pages_for_stream_size(self.stream_size, page_size) as usize,
            self.pages.len(),
            "number of pages should be consistent with stream size"
        );

        let file = self.file.data.lock().unwrap();

        for (spage, &page) in self.pages.iter().enumerate() {
            let whole_page_data =
                &file[page_to_offset(page, page_size) as usize..][..usize::from(page_size)];
            let page_start = (spage as u32) << page_size.exponent();
            let len_within_page = (self.stream_size - page_start).min(u32::from(page_size));

            // Page data from the MSF "file"
            let page_data = &whole_page_data[..len_within_page as usize];

            // Page data from our parallel file contents
            let expected_page_data = &self.expected_stream_data[(spage << page_size.exponent())..]
                [..len_within_page as usize];

            assert_bytes_eq!(expected_page_data, page_data, "Stream page {page}");
        }
    }
}

/// Create a stream but don't write anything to it.
#[test]
fn test_write_empty_stream() {
    let mut t = tester();

    let (si, _s) = t.msf.new_stream().unwrap();
    assert_eq!(si, 5);
    assert_eq!(t.msf.num_streams(), 6);

    t.msf.commit().unwrap();
}

/// Create a stream, do a single zero-length write to it.
#[test]
fn test_write_empty_buffer() {
    let mut t = tester();

    let (si, mut s) = t.msf.new_stream().unwrap();
    assert_eq!(si, 5);
    s.write_all(&[]).unwrap();

    assert_eq!(t.msf.num_streams(), 6);

    t.msf.commit().unwrap();
}

/// Create a stream, write a small amount of data into it
#[test]
fn test_write_hello_world() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Hello, world!");
}

#[test]
fn test_write_simple() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Alpha_");
    st.write_at(6, b"Bravo_");
    st.write_at(12, b"Charlie_");
    st.write_at(6, b"Delta_");
}

// Zero-extend with a small amount of data that does not cross the page boundary where zero-extend starts.
#[test]
fn test_zero_extend_unaligned_start_1() {
    let mut st = StreamTester::new();
    st.write_at(10, b"Hello!");
}

// Zero-extend with a small amount of data that DOES cross the page boundary where zero-extend starts.
// This also zero-extends several complete pages.
#[test]
fn test_zero_extend_unaligned_start_cross_page_many() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Hello");
    st.write_at(0x2ffe, b"World!");
}

// unaligned start, finishes within a single page
#[test]
fn test_zero_extend_unaligned_start_single_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"old");
    // <-- zero extend 7 bytes
    st.write_at(10, b"new");
}

#[test]
fn test_zero_extend_unaligned_start_cross_pages_aligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0, b"old");
    st.write_at(10, &vec![0xaa; 0x1ff6]); // ends at page-aligned boundary
    assert_eq!(st.stream_size, 0x2000);
}

#[test]
fn test_zero_extend_unaligned_start_cross_pages_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0, b"old");
    st.write_at(10, &vec![0xaa; 0x2000]);
    assert_eq!(st.stream_size, 0x200a);
}

#[test]
fn test_zero_extend_aligned_start_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0x2000, b"alpha");
}

#[test]
fn test_zero_extend_aligned_start_pages_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0x0000, &vec![0xaa; 0x1000]);
    st.write_at(0x2010, b"alpha");
}

// aligned start, does not extend stream, existing stream page is unaligned
#[test]
fn test_overwrite_aligned_start_single_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo charlie delta");
    st.write_at(0, b"TANGO");
}

// unaligned start, does not extend stream, existing stream page is unaligned
#[test]
fn test_overwrite_unaligned_start_single_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo charlie delta");
    st.write_at(6, b"TANGO");
}

// unaligned start, extends stream, existing stream page is unaligned
#[test]
fn test_overwrite_case_unaligned_extend_within_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo");
    st.write_at(12, b"TANGO");
}

// unaligned start, does not extend stream, existing stream page is unaligned
#[test]
fn test_overwrite_case_unaligned_extend_across_pages() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo");
    let big = FRIENDS_ROMANS.repeat(10);
    println!("big length = 0x{:x}", big.len());
    st.write_at(12, big.as_bytes());
}

#[test]
fn test_overwrite_case_many_pages() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x10_000]); // write lots of data
    st.write_at(0x0_f00, FRIENDS_ROMANS.as_bytes()); // get some shakespeare
    st.write_at(0x1_f00, FRIENDS_ROMANS.repeat(10).as_bytes());
}

// This tests the case in write_overwrite_aligned_pages() where we overwrite an unaligned portion
// of a page. buf.len() is too small to cover the page, but the existing stream does have enough
// pages assigned to it to cover it.
#[test]
fn test_overwrite_case_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x2_000]);
    st.write_at(0xffe, b"abcd");
}

#[test]
fn test_overwrite_case_zzz_1() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x1_005]);
    st.write_at(0xffe, b"__abc");
}

#[test]
fn test_overwrite_case_zzz_2() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x1_005]);
    st.write_at(0xffe, b"__abcde");
}

#[test]
fn test_overwrite_case_zzz_3() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x1_005]);
    st.write_at(0xffe, b"__abcdefgh");
}

#[test]
fn test_overwrite_case_c() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0xc]);
    st.write_at(0, &vec![0xaa; 0xaaaa]);
}

const FRIENDS_ROMANS: &str = r#"
Friends, Romans, countrymen, lend me your ears;
I come to bury Caesar, not to praise him.
The evil that men do lives after them;
The good is oft interred with their bones;
So let it be with Caesar. The noble Brutus
Hath told you Caesar was ambitious:
If it were so, it was a grievous fault,
And grievously hath Caesar answer'd it.
Here, under leave of Brutus and the rest-
For Brutus is an honourable man;
So are they all, all honourable men-
Come I to speak in Caesar's funeral.
He was my friend, faithful and just to me:
But Brutus says he was ambitious;
And Brutus is an honourable man.
He hath brought many captives home to Rome
Whose ransoms did the general coffers fill:
Did this in Caesar seem ambitious?
When that the poor have cried, Caesar hath wept:
Ambition should be made of sterner stuff:
Yet Brutus says he was ambitious;
And Brutus is an honourable man.
You all did see that on the Lupercal
I thrice presented him a kingly crown,
Which he did thrice refuse: was this ambition?
Yet Brutus says he was ambitious;
And, sure, he is an honourable man.
I speak not to disprove what Brutus spoke,
But here I am to speak what I do know.
You all did love him once, not without cause:
What cause withholds you then, to mourn for him?
O judgment! thou art fled to brutish beasts,
And men have lost their reason. Bear with me;
My heart is in the coffin there with Caesar,
And I must pause till it come back to me.
"#;

#[test]
fn test_write_many_pieces() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Alpha_");
    st.write_at(6, b"Bravo_");
    st.write_at(12, b"Charlie_");
    st.write_at(6, b"Delta_");
    st.write_at(50, b"Zulu");
    st.write_at(0, b"__Wiffleball__");
    st.write_at(5, b"__Garrus__");
}

#[test]
fn test_write_x() {
    let mut st = StreamTester::new();
    st.write_at(0x35, b"!");
    st.write_at(0, b"zzz");
}

#[test]
fn msf_write_multi_streams() {
    let mut t = tester();

    {
        let (_si1, mut sw1) = t.msf.new_stream().unwrap();
        sw1.write_all(b"Hello, world!").unwrap();
    }

    {
        let (_si2, mut sw2) = t.msf.new_stream().unwrap();
        sw2.write_all(b"Hallo Welt!").unwrap();
    }

    {
        let (_si2, mut sw2) = t.msf.new_stream().unwrap();
        sw2.write_all(b"Salut tout le monde!").unwrap();
    }
}

fn writer() -> Msf<TestFile> {
    let f = TestFile::default();
    Msf::create_for(f, Default::default()).unwrap()
}

fn finish_and_dump(mut w: Msf<TestFile>) {
    match w.commit() {
        Err(e) => {
            panic!("PdbWriter::commit failed: {}", e);
        }
        Ok(_wrote_any) => {
            let data_guard = w.file.data.lock().unwrap();
            let data: &[u8] = &data_guard;

            println!(
                "Finished PDB.  Size = 0x{:x} {}:\n{:#?}",
                data.len(),
                data.len(),
                data.hex_dump()
            );
        }
    }
}

/// Commits changes in a writer, then closes it and re-opens it as a new `Msf`.
#[track_caller]
fn finish_and_read(mut w: Msf<TestFile>) -> Msf<TestFile> {
    w.commit().unwrap();
    let file = w.into_file();

    // Now re-open the file.
    Msf::open_with_file(file).unwrap()
}

#[track_caller]
fn commit_and_read(w: &mut Msf<TestFile>) -> Msf<TestFile> {
    let _span = trace_span!("commit_and_read").entered();
    w.commit().unwrap();

    let cloned_file_data = w.file_mut().data.get_mut().unwrap().clone();
    Msf::open_with_file(TestFile {
        data: Mutex::new(cloned_file_data),
    })
    .unwrap()
}

#[test]
fn page_size() {
    let w = writer();
    assert_eq!(usize::from(w.page_size()), 4096);
}

#[test]
fn empty_pdb() {
    let w = writer();
    finish_and_dump(w);
}

#[test]
fn read_stream_out_of_range() {
    let w = writer();
    let r = finish_and_read(w);
    let s = r.get_stream_reader(100);
    assert!(s.is_err());
}

#[test]
fn read_stream_dir_stream() {
    let w = writer();
    let r = finish_and_read(w);
    let s = r.get_stream_reader(STREAM_DIR_STREAM).unwrap();
    assert!(!s.is_nil());
}

#[test]
fn read_nil_stream() {
    let mut w = writer();
    let si = w.nil_stream().unwrap();
    debug!(nil_stream_index = si);
    let r = finish_and_read(w);
    let s = r.get_stream_reader(si).unwrap();
    assert!(s.is_nil());
    assert_eq!(s.len(), 0);
}

#[test]
fn one_stream_hello_world() -> anyhow::Result<()> {
    let mut w = writer();

    let (_, mut s) = w.new_stream()?;
    s.write_all("Hello, world!".as_bytes())?;

    finish_and_dump(w);
    Ok(())
}

#[test]
fn simple_multiple_streams() -> anyhow::Result<()> {
    let mut w = writer();

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 5);
    s.write_all("Friends, Romans, countrymen, lend me your ears.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 6);
    s.write_all("I come to bury Caesar, not to praise him.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 7);
    s.write_all("The evil that men do lives after them.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 8);
    s.write_all("I come to bury Caesar, not to praise him.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 9);
    s.write_all("So let it be with Caesar.".as_bytes())?;

    finish_and_dump(w);
    Ok(())
}

#[test]
fn mix_and_match() -> Result<()> {
    let mut w = writer();

    let (si0, _s0) = w.new_stream()?;
    let (si1, _s1) = w.new_stream()?;
    let (si2, _s2) = w.new_stream()?;

    w.write_stream(si0)?.write_all("Sponge Bob!".as_bytes())?;
    w.write_stream(si1)?.write_all("Squidward!".as_bytes())?;
    w.write_stream(si2)?.write_all("Mr Crabs!".as_bytes())?;
    w.write_stream(si0)?.write_all("Square Pants!".as_bytes())?; // should land on same page

    let mut w1 = w.write_stream(si1)?;
    w1.seek(SeekFrom::Start(0x2000))?;
    w1.write_all("Peace and Quiet...".as_bytes())?; // new page

    assert_eq!(w.write_stream(si0)?.pages, &[3]);
    assert_eq!(w.write_stream(si1)?.pages, &[4, 6, 7]);
    assert_eq!(w.write_stream(si2)?.pages, &[5]);

    finish_and_dump(w);

    Ok(())
}

#[test]
fn commit_on_read_only() -> Result<()> {
    let mut w = writer();

    let (_si, mut sw) = w.new_stream().unwrap();
    sw.write_all(b"Hello!").unwrap();

    let mut r = finish_and_read(w);

    // This commit() call should do nothing (but should succeed).
    assert!(!r.commit().unwrap());
    Ok(())
}

#[test]
fn commit_no_writes() {
    let mut w = writer();
    // First call should write the initial MSF file.
    assert!(w.commit().unwrap());
    // Second call should have no writes at all, though.
    assert!(!w.commit().unwrap());
}

#[test]
fn single_commit() {
    let mut w = writer();
    let (si1, mut sw1) = w.new_stream().unwrap();
    sw1.write_all(b"Alpha").unwrap();

    {
        let r = commit_and_read(&mut w);
        let contents1 = r.read_stream_to_vec(si1).unwrap();
        assert_eq!(contents1, b"Alpha");
    }
}

#[test]
fn multiple_commit() {
    let mut w = writer();

    trace!("multi_commit: writing first stream");
    let (si1, mut sw1) = w.new_stream().unwrap();
    sw1.write_all(b"Alpha").unwrap();

    {
        trace!("multi_commit: first commit");
        let r = commit_and_read(&mut w);
        let contents1 = r.read_stream_to_vec(si1).unwrap();
        assert_eq!(contents1, b"Alpha");
    }

    trace!("multi_commit: writing second stream");
    let (si2, mut sw2) = w.new_stream().unwrap();
    sw2.write_all(b"Bravo").unwrap();

    {
        trace!("multi_commit: second commit");
        let r = commit_and_read(&mut w);

        let contents1 = r.read_stream_to_vec(si1).unwrap();
        assert_bytes_eq!(contents1, b"Alpha");

        let contents2 = r.read_stream_to_vec(si2).unwrap();
        assert_bytes_eq!(contents2, b"Bravo");
    }
}

#[test]
fn many_commits() {
    let num_commits: usize = 37;

    let mut w = writer();

    let (si1, _sw1) = w.new_stream().unwrap();

    let mut expected_stream_contents: Vec<u8> = Vec::new();

    for i in 0..num_commits {
        let sw = w.write_stream(si1).unwrap();
        let pos = i * 2039; // 2039 is next-lower prime under 2048

        let text = format!("i{i} pos{pos};");
        let buf = text.as_bytes();

        // Write the buffer to expected_stream_contents.
        let text_end = pos + text.len();
        if expected_stream_contents.len() < text_end {
            // Extend, if necessary.
            expected_stream_contents.resize(text_end, 0);
        }
        expected_stream_contents[pos..][..buf.len()].copy_from_slice(buf);

        sw.into_random().write_at(buf, pos as u64).unwrap();

        {
            let r = commit_and_read(&mut w);
            let read_buf = r.read_stream_to_vec(si1).unwrap();
            assert_bytes_eq!(expected_stream_contents, read_buf);
        }
    }
}

/// Test the WriteAt impl for StreamReader
#[test]
fn stream_writer_random_write_at() {
    let mut w = writer();

    let (si, sw) = w.new_stream().unwrap();
    let sw = sw.into_random();
    assert_eq!(sw.write_at(b"Hello", 0).unwrap(), 5);
    sw.write_all_at(b"World", 5).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert_eq!(data, b"HelloWorld");
}

/// Test the WriteAt and ReadAt impl for RandomStreamWriter
#[test]
fn stream_writer_random_read_at() {
    let mut w = writer();

    let (_si, sw) = w.new_stream().unwrap();
    let sw = sw.into_random();

    sw.write_at(b"012345", 5).unwrap();
    sw.write_all_at(b"AlphaBravo", 5).unwrap();

    {
        let mut buf: [u8; 5] = [0; 5];
        assert_eq!(sw.read_at(&mut buf, 12).unwrap(), 3);
        assert_eq!(&buf, b"avo\0\0");
    }

    {
        let mut buf: [u8; 5] = [0; 5];
        sw.read_exact_at(&mut buf, 7).unwrap();
        assert_eq!(&buf, b"phaBr");
    }

    {
        // attempting to read beyond the end of the buffer
        let mut buf: [u8; 5] = [0; 5];
        assert!(sw.read_exact_at(&mut buf, 12).is_err());
    }
}

#[test]
fn stream_writer_flush() {
    let mut w = writer();
    let (_si, mut sw) = w.new_stream().unwrap();
    sw.flush().unwrap();
}

#[test]
fn stream_writer_write_at() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    assert_eq!(sw.write_at_mut(b"Alpha", 0).unwrap(), 5);
    sw.write_all_at_mut(b"Bravo", 5).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(data, b"AlphaBravo");
}

/// Test set_contents() on a newly-created stream.
#[test]
fn stream_writer_set_contents_new() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    sw.set_contents(FRIENDS_ROMANS.as_bytes()).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(data, FRIENDS_ROMANS);
}

/// Test set_contents() on a stream that has not been modified yet.
/// Extend the buffer.
#[test]
fn stream_writer_set_contents_modifying_extending() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    sw.set_contents(b"this will get overwritten").unwrap();

    let _r = commit_and_read(&mut w);

    let mut sw = w.write_stream(si).unwrap();
    sw.set_contents(b"this string is a lot longer than the first string so it extends the buffer")
        .unwrap();

    let r2 = commit_and_read(&mut w);
    let data = r2.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(
        data,
        b"this string is a lot longer than the first string so it extends the buffer"
    );
}

/// Test set_contents() on a stream that has not been modified yet.
/// Shrink the buffer.
#[test]
fn stream_writer_set_contents_modifying_shrinking() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    sw.set_contents(b"this is a moderately long string which will get overwritten")
        .unwrap();

    let _r = commit_and_read(&mut w);

    let mut sw = w.write_stream(si).unwrap();
    sw.set_contents(b"goodbye!").unwrap();

    let r2 = commit_and_read(&mut w);
    let data = r2.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(data, b"goodbye!");
}

#[test]
fn stream_writer_write_at_empty() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();

    // This should succeed, but...
    sw.write_all_at_mut(&[], 100).unwrap();

    // ...it shouldn't extend the stream size.
    assert_eq!(sw.len(), 0);

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert!(data.is_empty());
}

/// Test extending a stream with a large number of pages, which should
/// test the path in `StreamWriter::write_page`.
#[test]
fn stream_writer_extend_large() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();

    let mut large_data = vec![0; 0x10000];
    large_data[0xffff] = 0xff;

    sw.set_contents(&large_data).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();

    assert_bytes_eq!(large_data, data);
}

#[test]
fn read_stream_to_vec_mut_modified() {
    let mut w = writer();
    let si = w.new_stream_data(b"Hello!").unwrap();
    let mut read_back = Vec::new();
    w.read_stream_to_vec_mut(si, &mut read_back).unwrap();
    assert_bytes_eq!(read_back, b"Hello!");
}

#[test]
fn read_stream_to_vec_mut_committed() {
    let mut w = writer();
    let si = w.new_stream_data(b"Hello!").unwrap();
    let r = commit_and_read(&mut w);
    let mut read_back = Vec::new();
    r.read_stream_to_vec_mut(si, &mut read_back).unwrap();
    assert_bytes_eq!(read_back, b"Hello!");
}

#[test]
fn read_stream_to_box() {
    let mut w = writer();
    let si = w.new_stream_data(b"Hello!").unwrap();
    let r = commit_and_read(&mut w);
    let read_back = r.read_stream_to_box(si).unwrap();
    assert_bytes_eq!(read_back, b"Hello!");
}

```

`msf/src/write.rs`:

```rs
use super::*;
use std::collections::hash_map::Entry;
use std::io::Write;
use tracing::{trace, trace_span};

impl<'a, F: ReadAt + WriteAt> StreamWriter<'a, F> {
    /// Writes data to a stream at a given offset. This is the main driver for all `write()` calls
    /// and their variants.
    ///
    /// This function has to handle a lot of complexity:
    /// * alignment of the starting position to a page boundary
    /// * alignment of the ending position to a page boundary
    /// * allocating pages for new pages
    /// * allocating pages for copy-on-write
    /// * updating the size of a stream
    /// * writing zeroes into regions that were implicitly created
    ///
    /// Returns the new write position, which is immediately after the buffer that was provided.
    ///
    /// This implementation is input-dependent. That is, we will drive all of our state transitions
    /// by "walking" through the offsets in the stream, from 0 to the end of the stream or the end
    /// of the transfer, whichever is greater.
    ///
    /// For some operations (read-modify-write cycles), we use a temporary page buffer.
    ///
    /// This function always writes all of the data in `buf`. If it cannot, it returns `Err`.
    #[inline(never)]
    pub(super) fn write_core(&mut self, mut buf: &[u8], offset: u64) -> std::io::Result<()> {
        let _span = trace_span!("StreamWriter::write_core").entered();

        if buf.is_empty() {
            return Ok(());
        }

        if *self.size == NIL_STREAM_SIZE {
            *self.size = 0;
        }

        let page_size = self.page_allocator.page_size;

        // Validate the ranges of our inputs. We validate these now so that we can compute values
        // that depend on them without worrying about overflow.
        let Ok(buf_len) = u32::try_from(buf.len()) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };
        let Ok(mut pos) = u32::try_from(offset) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };
        let Some(_buf_end) = pos.checked_add(buf_len) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };

        // Is there any implicit zero extension happening? If so, handle the zero extension now.
        // Note that this may transfer a small prefix of buf, if the end of the zero-extension
        // region is unaligned (i.e. pos is unaligned). If that consumes all of the data in buf,
        // then we finish early.
        if *self.size < pos {
            self.write_zero_extend(&mut buf, &mut pos)?;
            if buf.is_empty() {
                return Ok(());
            }
            assert_eq!(pos, *self.size);
        }

        assert!(!buf.is_empty());
        assert!(pos <= *self.size);

        // Are we doing any overwrite?
        if pos < *self.size {
            self.write_overwrite(&mut buf, &mut pos)?;
            if buf.is_empty() {
                return Ok(());
            }
            assert_eq!(pos, *self.size);
        }

        assert!(!buf.is_empty());
        assert_eq!(pos, *self.size);

        // Does the write position start at an unaligned page boundary?
        if !page_size.is_aligned(pos) {
            self.write_unaligned_start_page(&mut buf, &mut pos)?;
            if buf.is_empty() {
                return Ok(());
            }
        }

        assert!(!buf.is_empty());
        assert_eq!(pos, *self.size);
        assert!(page_size.is_aligned(pos));

        // From this point on, we no longer need to cow pages.
        // All pages that we write will be newly-allocated pages.

        self.write_append_complete_pages(&mut buf, &mut pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        self.write_append_final_unaligned_page(&mut buf, &mut pos)?;
        assert!(buf.is_empty());

        Ok(())
    }

    /// Append complete pages to the file.
    fn write_append_complete_pages(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert_eq!(*self.size, *pos);
        assert!(page_size.is_aligned(*pos));

        // Append complete pages. We do this iteratively so that we can minimize our calls to
        // alloc_pages(). Each iteration of this loop allocates a contiguous run of pages and
        // uses a single write() call to the lower level for transferring data.
        //
        // This is expected to be the main "hot path" for generating new PDBs (not just editing
        // existing ones). The page allocator should usually give us a long run of pages; it only
        // needs to break up runs when we cross interval boundaries (because of the FPM pages).

        loop {
            let num_pages_wanted = (buf.len() as u32) / page_size;
            if num_pages_wanted == 0 {
                break;
            }

            // Allocate pages, add them to our stream, and update the stream size.
            // These must all be done before I/O so that our in-memory state is consistent.
            let (first_page, run_len) = self.page_allocator.alloc_pages(num_pages_wanted);
            assert!(run_len > 0);
            let xfer_len: u32 = run_len << page_size.exponent();
            for i in 0..run_len {
                self.pages.push(first_page + i);
            }

            let buf_head = take_n(buf, xfer_len as usize);

            let file_offset = page_to_offset(first_page, page_size);

            trace!(
                stream_pos = *pos,
                first_page = first_page,
                file_offset,
                xfer_len,
                "write_append_complete_pages"
            );

            self.file.write_all_at(buf_head, file_offset)?;

            *self.size += xfer_len;
            *pos += xfer_len;
        }

        Ok(())
    }

    /// Append the final unaligned page to the file, if any.
    fn write_append_final_unaligned_page(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert_eq!(*self.size, *pos);
        assert!(page_size.is_aligned(*pos));

        // The only thing left is a single partial page at the end. We use the page buffer so that we
        // are sending down complete page writes to the lower-level storage device.
        assert!(buf.len() < usize::from(page_size));
        if buf.is_empty() {
            return Ok(());
        }

        let page = self.page_allocator.alloc_page();

        let mut page_buffer = self.page_allocator.alloc_page_buffer();
        page_buffer[..buf.len()].copy_from_slice(buf);
        page_buffer[buf.len()..].fill(0);

        self.pages.push(page);
        *self.size += buf.len() as u32;

        let file_offset = page_to_offset(page, page_size);

        trace!(
            stream_pos = *pos,
            page = page,
            file_offset,
            unaligned_len = buf.len(),
            "write_append_final_unaligned_page"
        );

        self.file.write_all_at(&page_buffer, file_offset)?;

        *pos += buf.len() as u32;
        *buf = &[];

        Ok(())
    }

    /// Handles zero-extending a stream. This occurs when the write position is beyond the
    /// current size of the stream. This implicitly writes zeroes from the old end of the stream
    /// to the start of the write request.
    ///
    /// This implementation will transfer the prefix of `buf` if `pos` is unaligned. If `buf` fits
    /// entirely on one page, then this finishes the entire transfer. If some portion of `buf` is
    /// transferred, then it will be written to 1 or 2 pages. It will be written to 2 pages if it
    /// crosses a page boundary.
    fn write_zero_extend(&mut self, buf: &mut &[u8], pos: &mut u32) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        self.write_zero_extend_unaligned_start(buf, pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        // If we have more bytes to write, then write_zero_extend_unaligned_start() should
        // have aligned the stream size to a page boundary.
        assert!(page_size.is_aligned(*self.size));

        if *self.size < *pos {
            self.write_zero_extend_whole_pages(*pos)?;
        }

        if *self.size < *pos {
            self.write_zero_extend_unaligned_end(buf, pos)?;
        }

        Ok(())
    }

    fn write_zero_extend_unaligned_start(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        assert!(*self.size < *pos); // caller should have already checked this
        let num_zx = *pos - *self.size; // number of zero-extend bytes we need

        let page_size = self.page_allocator.page_size;

        // current end of the stream
        let end_spage: StreamPage = *self.size >> page_size.exponent();
        let end_phase = offset_within_page(*self.size, page_size);

        // where the new data begins
        let pos_spage: StreamPage = *pos >> page_size.exponent();
        let pos_phase = offset_within_page(*pos, page_size);

        if end_phase != 0 {
            // The end of the stream is not page-aligned and we are extending the end of the stream
            // by one or more bytes.

            // Prepare the page buffer that we are assembling. Zero-fill it.
            let mut page_buffer = self.page_allocator.alloc_page_buffer();
            page_buffer.fill(0);

            // Read the old data from the page. The read starts at phase 0 within the page and ends
            // at end_phase. If this fails, that's OK, we haven't made any state changes yet and the
            // error propagates.
            {
                let file_page = self.pages[end_spage as usize];
                let file_offset = page_to_offset(file_page, page_size);
                self.file
                    .read_exact_at(&mut page_buffer[..end_phase as usize], file_offset)?;
            }

            if end_spage == pos_spage {
                // The stream ends on the same page that new-data begins on, and the end of
                // the stream is unaligned. This means that we have a very complicated page to
                // deal with. It has old stream data, zero-extend bytes, and 1 or more bytes of
                // new data. We also have to deal with copy-on-write for the page.
                //
                // We expect zero-extending to be a rare case. We implement this by allocating a
                // page buffer, reading the unaligned piece of the old page, zeroing the middle,
                // copying the unaligned piece of the new data, and optionally zeroing the tail.
                // Then we allocate a fresh page (if needed) and write the page data that we built.
                // Then we write the page to disk and update the stream page pointer.
                //
                // There are two subcases to consider:
                // 1) the new data does not reach the end of this page (is unaligned), so there are
                //    some undefined bytes at the end of the page
                // 2) the new data reaches or crosses the end of this page (is aligned), so we
                //    "paint" the entire page to its end with new data.

                // within end_spage:
                // |------old-data-------|-------zeroes------|-------new-data-----|----
                //                       |                   |
                //               end_phase                   |
                //                                           |
                //                                           pos_phase

                assert!(end_phase <= pos_phase);

                // Copy the new data into the page buffer. The new data may end within this page
                // or may cross the boundary to the next page, which is what the min() call handles.
                let buf_head_len = (usize::from(page_size) - pos_phase as usize).min(buf.len());
                let buf_head = take_n(buf, buf_head_len);
                page_buffer[pos_phase as usize..][..buf_head.len()].copy_from_slice(buf_head);

                // Move pos because we have consumed data from 'buf'
                *pos += buf_head_len as u32;

                // In this case, all of the zero-extend bytes have been handled in this first page,
                // so we can advance pos by num_zx.
                *self.size += num_zx;
                *self.size += buf_head_len as u32;
            } else {
                // The new data does not overlap the page we are working on. That means the
                // zero-extend region reaches to the end of the page.

                // within end_spage:
                // |------old-data-------|-------zeroes-------------------------------|
                //                       |                                            |
                //               end_phase                                            |
                //                                                                    |
                //                                                               page_size

                let num_zx_this_page = u32::from(page_size) - end_phase;
                *self.size += num_zx_this_page;
            }

            // COW the page and write it.
            self.cow_page_and_write(end_spage, &page_buffer)?;
        }

        Ok(())
    }

    /// Writes zero or more complete zero pages during zero-extension. The size of the stream has
    /// already been aligned to a page boundary.
    ///
    /// This does not read data from the current transfer request, so it does not need `buf`.
    /// It also does not change `pos`.
    fn write_zero_extend_whole_pages(&mut self, pos: u32) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert!(*self.size <= pos);
        assert!(page_size.is_aligned(*self.size));

        if (pos - *self.size) / page_size == 0 {
            return Ok(());
        }

        let mut page_buffer = self.page_allocator.alloc_page_buffer();
        page_buffer.fill(0); // probably redundant

        loop {
            let num_pages_wanted = (pos - *self.size) / page_size;
            if num_pages_wanted == 0 {
                break;
            }

            let (first_page, run_len) = self.page_allocator.alloc_pages(num_pages_wanted);
            assert!(run_len > 0);
            for i in 0..run_len {
                self.pages.push(first_page + i);
            }

            let run_size_bytes = run_len << page_size.exponent();
            *self.size += run_size_bytes;

            assert!(*self.size <= pos);

            // Write the zeroed pages.
            for i in 0..run_len {
                let page = first_page + i;
                self.file
                    .write_at(&page_buffer, page_to_offset(page, page_size))?;
            }
        }

        Ok(())
    }

    /// If the zero-extend region ends with an unaligned final page, then this will write that page.
    /// This may transfer data from `buf`.
    fn write_zero_extend_unaligned_end(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert!(*self.size <= *pos);
        assert!(page_size.is_aligned(*self.size));

        // We should have at most a partial page.
        let num_zx_bytes = *pos - *self.size;
        assert!(num_zx_bytes < u32::from(page_size));

        if num_zx_bytes == 0 {
            return Ok(());
        }

        let mut page_buffer = self.page_allocator.alloc_page_buffer();

        page_buffer[0..num_zx_bytes as usize].fill(0);

        let num_data_len = buf
            .len()
            .min(usize::from(page_size) - num_zx_bytes as usize);
        let buf_head = take_n(buf, num_data_len);
        *pos += num_data_len as u32;

        page_buffer[num_zx_bytes as usize..num_zx_bytes as usize + num_data_len]
            .copy_from_slice(buf_head);

        let end_of_data = num_zx_bytes as usize + num_data_len;
        page_buffer[end_of_data..usize::from(page_size)].fill(0);

        let page = self.page_allocator.alloc_page();
        self.pages.push(page);
        *self.size += end_of_data as u32;

        self.file
            .write_at(&page_buffer, page_to_offset(page, page_size))?;

        Ok(())
    }

    /// Handles _most_ of overwrite.
    ///
    /// This handles:
    /// * the unaligned page at the start of overwrite (if any)
    /// * the complete, aligned pages in the middle of overwrite (if any)
    ///
    /// When this returns, even if buf still has data, we do not guarantee that pos == stream_size.
    fn write_overwrite(&mut self, buf: &mut &[u8], pos: &mut u32) -> std::io::Result<()> {
        assert!(*pos < *self.size);

        self.write_overwrite_unaligned_start(buf, pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        assert!(self.page_allocator.page_size.is_aligned(*pos));

        self.write_overwrite_aligned_pages(buf, pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        assert!(self.page_allocator.page_size.is_aligned(*pos));
        assert!(self.page_allocator.page_size.is_aligned(*self.size));
        Ok(())
    }

    /// Handles writing the first page during overwrite, if the first page is unaligned.
    ///
    /// # Requires
    /// * `pos <= stream_size`
    ///
    /// # Ensures
    /// * `buf.is_empty() || pos == stream_size`
    fn write_overwrite_unaligned_start(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        assert!(*pos <= *self.size);

        let page_size = self.page_allocator.page_size;

        let pos_spage: StreamPage = *pos >> page_size.exponent();
        let pos_phase = offset_within_page(*pos, page_size);
        if pos_phase == 0 {
            // The overwrite starts on an aligned page boundary. This function has no work to do.
            return Ok(());
        }

        // In this case, we need to assemble a page from some old data and some new data.
        // And because we need to cow the page, it is easier to reassemble everything into a
        // page buffer.
        //
        // Contents of this page:
        //
        //
        //
        //        not empty            not empty             can be empty          can be empty
        // |----old-data----------|------new-data--------|-------old-data------|-----garbage-----
        // 0                      |                      |                     |
        //                        |                      |                     |
        //                  pos_phase
        //
        // At this point, we don't know which subcase we're in. It depends on whether new_data
        // reaches the end of this page and whether writing the new-data extends the stream size.

        // Read the old page data into our page buffer. This makes cow'ing the page easier.
        // We read the entire page because that simplifies the case where the new-data ends
        // before stream_size.
        let mut page_buffer = self.page_allocator.alloc_page_buffer();
        self.read_page(pos_spage, &mut page_buffer)?;

        // Copy the data from 'buf' (new-data) into the page and advance 'buf' and 'pos'.
        let new_data_len = (usize::from(page_size) - pos_phase as usize).min(buf.len());
        assert!(new_data_len > 0);
        let buf_head = take_n(buf, new_data_len);
        page_buffer[pos_phase as usize..][..buf_head.len()].copy_from_slice(buf_head);
        *pos += new_data_len as u32;

        // Cow the page and write its new contents.
        self.cow_page_and_write(pos_spage, &page_buffer)?;

        // We may have written enough data that we extended stream_size. This only happens if we
        // drag in some of the prefix of buf.
        if *pos > *self.size {
            *self.size = *pos;
        }

        Ok(())
    }

    /// If we are doing overwrite and the remaining buffer contains one or more whole pages,
    /// then this function transfers those.
    ///
    /// This function may extend the stream size. If it does, then it will not extend it enough
    /// to cross a page boundary. This form of extension happens when we are overwriting beyond
    /// the existing end of the stream.
    ///
    /// This function will cow pages, but will not allocate new page slots in the stream.
    ///
    /// # Requires
    /// * `pos` is page-aligned
    /// * `pos <= stream_size`
    ///
    /// # Ensures
    /// * `pos` is page-aligned
    /// * `buf.is_empty() || stream_size is page-aligned`
    fn write_overwrite_aligned_pages(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;
        assert!(*pos <= *self.size);
        assert!(page_size.is_aligned(*pos));

        if *pos == *self.size {
            return Ok(());
        }

        // The stream page where this transfer will begin (if any).
        let pos_spage = *pos / page_size;

        // Number of complete pages that can be read from buf.
        let num_buf_pages = buf.len() as u32 / page_size;

        // Number of pages at our write position that are assigned to the stream.
        // This includes the partial page at the end, if any.
        let num_pages_total = self.size.div_round_up(page_size);
        assert_eq!(num_pages_total, self.pages.len() as u32);
        let num_pages_at_pos = *pos / page_size;
        let num_pages_owned = num_pages_total - num_pages_at_pos;

        // Number of complete pages we are going to transfer from buf to disk.
        // Since we are writing whole pages, we do not need the old contents of any page.
        // Cow the pages, just so we get fresh pages.
        let num_xfer_pages = num_pages_owned.min(num_buf_pages);
        if num_xfer_pages != 0 {
            trace!(num_pages = num_xfer_pages, "writing whole pages");

            let num_xfer_bytes = num_xfer_pages << page_size.exponent();
            let buf_head = take_n(buf, num_xfer_bytes as usize);
            *pos += num_xfer_bytes;

            let pages = &mut self.pages[pos_spage as usize..][..num_xfer_pages as usize];
            self.page_allocator.make_pages_fresh(pages);
            write_runs(&self.file, buf_head, pages, page_size)?;

            // If the last page that we overwrite was a partial page, then we will have extended
            // the size of the stream. This will not extend stream_size beyond a page boundary.
            if *pos > *self.size {
                *self.size = *pos;
            }

            if buf.is_empty() {
                return Ok(());
            }
        }

        assert!(page_size.is_aligned(*pos));
        assert!(*pos <= *self.size);

        // We may have gotten here because buf.len() is now less than a full page size, but we still
        // have another page assigned in the stream. Cow it now.
        if *self.size - *pos > 0 {
            trace!(buf_len = buf.len(), "buffer has partial page remaining.");
            assert!(
                buf.len() < usize::from(page_size),
                "size = {:x}, pos = {:x}, buf.len = 0x{:x}",
                *self.size,
                *pos,
                buf.len()
            );

            let spage = *pos / page_size;
            let old_len = *self.size - *pos;

            let mut page_buffer = self.page_allocator.alloc_page_buffer();
            if old_len > buf.len() as u32 {
                self.read_page(spage, &mut page_buffer)?;
            }

            page_buffer[..buf.len()].copy_from_slice(buf);

            self.cow_page_and_write(spage, &page_buffer)?;
            *pos += buf.len() as u32;
            if *pos > *self.size {
                *self.size = *pos;
            }

            *buf = &[];
            return Ok(());
        }

        assert_eq!(*pos, *self.size);

        Ok(())
    }

    /// Writes the unaligned start page at the beginning of the new-data. This page is only
    /// present if `pos` is not aligned.
    ///
    /// # Requires
    /// * `stream_size == pos`
    fn write_unaligned_start_page(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        assert_eq!(*pos, *self.size);

        // In this case, we need to assemble a page from some old data and some new data.
        // And because we need to cow the page, it is easier to reassemble everything into a
        // page buffer.
        //
        // Contents of this page:
        //
        //
        //
        //        not empty            not empty             can be empty          can be empty
        // |----old-data----------|------new-data--------|-------old-data------|-----garbage-----
        // 0                      |                      |                     |
        //                        |                      |                     |
        //                  pos_phase
        //
        // At this point, we don't know which subcase we're in. It depends on whether new_data
        // reaches the end of this page and whether writing the new-data extends the stream size.

        let page_size = self.page_allocator.page_size;

        // where the new data begins
        let pos_spage: StreamPage = *pos >> page_size.exponent();
        let pos_phase = offset_within_page(*pos, page_size); // <-- we know this is non-zero

        // Read the old page data into our page buffer. This makes cow'ing the page easier.
        // We read the entire page because that simplifies the case where the new-data ends
        // before stream_size.
        let mut page_buffer = self.page_allocator.alloc_page_buffer();

        let file_offset = page_to_offset(self.pages[pos_spage as usize], page_size);
        trace!(
            stream_pos = *pos,
            file_offset,
            len = u32::from(page_size),
            "write_unaligned_start_page: reading existing unaligned data"
        );

        self.file.read_exact_at(&mut page_buffer, file_offset)?;

        // Copy the data from 'buf' (new-data) into the page and advance 'buf' and 'pos'.
        let new_data_len = (usize::from(page_size) - pos_phase as usize).min(buf.len());
        assert!(new_data_len > 0);
        let buf_head = take_n(buf, new_data_len);
        page_buffer[pos_phase as usize..][..buf_head.len()].copy_from_slice(buf_head);
        *pos += new_data_len as u32;

        // Cow the page and write its new contents.
        self.cow_page_and_write(pos_spage, &page_buffer)?;

        // We may have written enough data that we extended stream_size.
        if *pos > *self.size {
            *self.size = *pos;
        }

        Ok(())
    }

    /// Ensures that a stream page is writable (is "fresh"). If the page is not writable, then it
    /// allocates a new page. This function returns the page number of the writable page.
    fn cow_page(&mut self, spage: StreamPage) -> Page {
        self.page_allocator
            .make_page_fresh(&mut self.pages[spage as usize])
    }

    /// Ensures that a stream page is writable and then writes it.
    ///
    /// `data` should contain at most one page of data.
    pub(super) fn cow_page_and_write(
        &mut self,
        spage: StreamPage,
        data: &[u8],
    ) -> std::io::Result<()> {
        // At most one full page of data can be written.
        debug_assert!(
            data.len() <= usize::from(self.page_allocator.page_size),
            "buffer cannot exceed size of a single page"
        );

        let page = self.cow_page(spage);
        let file_offset = page_to_offset(page, self.page_allocator.page_size);

        trace!(
            stream_page = spage,
            file_offset,
            len = u32::from(self.page_allocator.page_size),
            "cow_page_and_write"
        );

        self.file.write_all_at(data, file_offset)
    }

    /// Reads a stream page.  The length of `data` must be exactly one page, or less. It cannot
    /// cross page boundaries.
    pub(super) fn read_page(
        &self,
        stream_page: StreamPage,
        data: &mut [u8],
    ) -> std::io::Result<()> {
        debug_assert!(
            data.len() <= usize::from(self.page_allocator.page_size),
            "buffer cannot exceed size of a single page"
        );

        let page = self.pages[stream_page as usize];
        let offset = page_to_offset(page, self.page_allocator.page_size);
        self.file.read_exact_at(data, offset)
    }

    /// The caller **must** guarantee that this page is already writable.
    pub(super) fn write_page(&self, stream_page: StreamPage, data: &[u8]) -> std::io::Result<()> {
        let page = self.pages[stream_page as usize];
        assert!(
            self.page_allocator.fresh[page as usize],
            "page is required to be fresh (writable)"
        );

        let file_offset = page_to_offset(page, self.page_allocator.page_size);

        trace!(
            stream_page,
            page = page,
            file_offset,
            len = u32::from(self.page_allocator.page_size),
            "write_page"
        );

        self.file.write_all_at(data, file_offset)
    }
}

/// Finds the length of the prefix of pages in `pages` that are numbered sequentially.
///
/// This function assumes that there are no entries with the value 0xffff_ffff. (That would cause
/// overflow.)
fn find_longest_page_run(pages: &[Page]) -> usize {
    if pages.is_empty() {
        0
    } else {
        let mut prev = pages[0];
        let mut i = 1;
        while i < pages.len() && pages[i] == prev + 1 {
            prev = pages[i];
            i += 1;
        }
        i
    }
}

#[test]
fn test_find_longest_page_run() {
    assert_eq!(find_longest_page_run(&[]), 0);
    assert_eq!(find_longest_page_run(&[1]), 1);
    assert_eq!(find_longest_page_run(&[1, 2, 3]), 3);
    assert_eq!(find_longest_page_run(&[1, 3, 2]), 1);
    assert_eq!(find_longest_page_run(&[1, 2, 3, 9, 9, 9]), 3);
}

/// Given a page map that corresponds to a buffer of data to write, write all of the data.
/// Write it in a sequence of function calls that group together consecutive pages, so that
/// we minimize the number of write() calls.
fn write_runs<F: WriteAt>(
    file: &F,
    mut buf: &[u8],
    pages: &[Page],
    page_size: PageSize,
) -> std::io::Result<()> {
    let mut region_pages = pages;

    assert_eq!(buf.len(), pages.len() << page_size.exponent());

    loop {
        let run_len = find_longest_page_run(region_pages);
        if run_len == 0 {
            break;
        }

        let page0: Page = region_pages[0];
        let xfer_len: usize = run_len << page_size.exponent();
        let buf_head = take_n(&mut buf, xfer_len);

        // Write the run of pages. If this write fails, then the contents of the stream are
        // now in an undefined state.
        file.write_at(buf_head, page_to_offset(page0, page_size))?;

        // Advance iterator state
        region_pages = &region_pages[run_len..];
    }

    Ok(())
}

impl<F> Msf<F> {
    /// Adds a new stream to the MSF file. The stream has a length of zero.
    pub fn new_stream(&mut self) -> anyhow::Result<(u32, StreamWriter<'_, F>)> {
        let _span = trace_span!("new_stream").entered();

        self.requires_writeable()?;
        self.check_can_add_stream()?;

        let new_stream_index = self.stream_sizes.len() as u32;
        trace!(new_stream_index);

        self.stream_sizes.push(0);
        let size = self.stream_sizes.last_mut().unwrap();

        let pages = match self.modified_streams.entry(new_stream_index) {
            Entry::Occupied(_) => {
                panic!("Found entry in modified streams table that should not be present.")
            }
            Entry::Vacant(v) => v.insert(Vec::new()),
        };

        Ok((
            new_stream_index,
            StreamWriter {
                stream: new_stream_index,
                file: &self.file,
                size,
                page_allocator: &mut self.pages,
                pos: 0,
                pages,
            },
        ))
    }

    fn check_can_add_stream(&self) -> anyhow::Result<()> {
        if self.stream_sizes.len() as u32 >= self.max_streams {
            bail!("A new stream cannot be created because the maximum number of streams has been reached.");
        }
        Ok(())
    }

    /// Adds a new stream to the MSF file, given the byte contents. This function returns the
    /// stream index of the new stream.
    pub fn new_stream_data(&mut self, data: &[u8]) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        let (stream_index, mut writer) = self.new_stream()?;
        writer.set_contents(data)?;
        Ok(stream_index)
    }

    /// Adds a new nil stream to the MSF file.
    pub fn nil_stream(&mut self) -> anyhow::Result<u32> {
        self.requires_writeable()?;
        self.check_can_add_stream()?;

        let new_stream_index = self.stream_sizes.len() as u32;
        self.stream_sizes.push(NIL_STREAM_SIZE);

        match self.modified_streams.entry(new_stream_index) {
            Entry::Occupied(_) => {
                panic!("Found entry in modified streams table that should be present.")
            }
            Entry::Vacant(v) => {
                v.insert(Vec::new());
            }
        }

        Ok(new_stream_index)
    }

    /// Given the stream index for a stream, returns a `StreamWriter` that allows read/write
    /// for the stream. `stream` must already be a valid stream index.
    ///
    /// The stream may be a nil stream. Calling this function does not change the nil stream to
    /// a non-nil stream. However, writing data to the stream will change it to a non-nil stream.
    pub fn write_stream(&mut self, stream: u32) -> anyhow::Result<StreamWriter<'_, F>> {
        self.requires_writeable()?;

        let Some(size) = self.stream_sizes.get_mut(stream as usize) else {
            bail!("Stream index is out of range");
        };

        let pages = match self.modified_streams.entry(stream) {
            Entry::Occupied(occ) => occ.into_mut(),
            Entry::Vacant(v) => {
                // Copy the existing page list to a new page list.
                //
                // Copying the page list _does not_ imply that we can safely write to those pages,
                // because they may still be owned by the previous committed state. Copy-on-write
                // is handled elsewhere.
                let starts = &self.committed_stream_page_starts[stream as usize..];
                let old_pages =
                    &self.committed_stream_pages[starts[0] as usize..starts[1] as usize];
                v.insert(old_pages.to_vec())
            }
        };

        Ok(StreamWriter {
            stream,
            file: &self.file,
            size,
            page_allocator: &mut self.pages,
            pos: 0,
            pages,
        })
    }

    pub(crate) fn requires_writeable(&self) -> anyhow::Result<()> {
        match self.access_mode {
            AccessMode::ReadWrite => Ok(()),
            AccessMode::Read => bail!("This PDB was not opened for read/write access."),
        }
    }

    /// Copies a stream from another PDB/MSF into this one.
    pub fn copy_stream<Input: ReadAt>(
        &mut self,
        source: &Msf<Input>,
        source_stream: u32,
    ) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        const BUFFER_LEN: usize = 16 << 20; // 16 MiB

        let mut source_reader = source.get_stream_reader(source_stream)?;
        let source_len = source_reader.len();

        let mut buffer = vec![0; (source_len as usize).min(BUFFER_LEN)];
        let (dest_stream_index, mut dest_writer) = self.new_stream()?;

        loop {
            let n = source_reader.read(&mut buffer)?;
            if n == 0 {
                break;
            }

            dest_writer.write_all(&buffer[..n])?;
        }

        Ok(dest_stream_index)
    }

    /// Copies a stream that implements `Read` into this PDB/MSF file.
    pub fn copy_stream_read<Input: Read>(&mut self, source: &mut Input) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        const BUFFER_LEN: usize = 16 << 20; // 16 MiB

        let mut buffer = vec![0; BUFFER_LEN];
        let (dest_stream_index, mut dest_writer) = self.new_stream()?;

        loop {
            let n = source.read(&mut buffer)?;
            if n == 0 {
                break;
            }

            dest_writer.write_all(&buffer[..n])?;
        }

        Ok(dest_stream_index)
    }

    /// Copies a stream that implements `ReadAt` into this PDB/MSF file.
    pub fn copy_stream_read_at<Input: ReadAt>(&mut self, source: &Input) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        const BUFFER_LEN: usize = 16 << 20; // 16 MiB

        let mut buffer = vec![0; BUFFER_LEN];
        let (dest_stream_index, mut dest_writer) = self.new_stream()?;

        let mut pos: u64 = 0;

        loop {
            let n = source.read_at(&mut buffer, pos)?;
            if n == 0 {
                break;
            }

            dest_writer.write_all(&buffer[..n])?;
            pos += n as u64;
        }

        Ok(dest_stream_index)
    }
}

/// Splits a slice `items` at a given index `n`. The slice is modified to point to the items
/// after `n`. The function returns the items up to `n`.
fn take_n<'a, T>(items: &mut &'a [T], n: usize) -> &'a [T] {
    let (lo, hi) = items.split_at(n);
    *items = hi;
    lo
}

```

`msfz/Cargo.toml`:

```toml
[package]
name = "ms-pdb-msfz"
version = "0.1.0"
edition = "2021"
description = "Reads Compressed Multi-Stream Files, which is part of the Microsoft PDB file format"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[lib]
doctest = false

[dependencies]
anyhow.workspace = true
flate2.workspace = true
static_assertions.workspace = true
sync_file.workspace = true
tracing.workspace = true
zerocopy = { workspace = true, features = ["alloc", "derive"] }
zerocopy-derive.workspace = true
zstd.workspace = true
pow2.workspace = true

[dev-dependencies]
bstr.workspace = true
pretty-hex.workspace = true
static_init.workspace = true
tracing-subscriber = { workspace = true, features = ["fmt"] }

[dev-dependencies.tracing-tracy]
workspace = true

[dev-dependencies.tracy-client]
version = "0.18.0"
features = ["manual-lifetime", "only-localhost"]

```

`msfz/src/compress_utils.rs`:

```rs
use std::io::{Read, Write};
use tracing::{trace, trace_span};

use crate::Compression;

pub(crate) fn compress_to_vec(compression: Compression, input: &[u8]) -> std::io::Result<Vec<u8>> {
    let mut output = Vec::new();
    compress_to_vec_mut(compression, input, &mut output)?;
    Ok(output)
}

pub(crate) fn compress_to_vec_mut(
    compression: Compression,
    input: &[u8],
    output: &mut Vec<u8>,
) -> std::io::Result<()> {
    let _span = trace_span!("compress_to_vec_mut").entered();

    let output_original_len = output.len();
    let out = &mut *output; // reborrow

    match compression {
        Compression::Zstd => {
            let mut enc = zstd::Encoder::new(out, 0)?;
            enc.write_all(input)?;
            enc.finish()?;
        }

        Compression::Deflate => {
            let mut enc = flate2::write::DeflateEncoder::new(
                std::io::Cursor::new(out),
                flate2::Compression::default(),
            );

            enc.write_all(input)?;
            enc.finish()?;
        }
    }

    trace!(
        ?compression,
        uncompressed_len = input.len(),
        compressed_len = output.len() - output_original_len
    );

    Ok(())
}

/// Decompresses a compressed buffer using the given compression algorithm.
///
/// `output.len()` specifies the expected size of the decoded stream. Returns `Err` if the
/// decompression algorithm returned the wrong number of bytes.
pub(crate) fn decompress_to_slice(
    compression: Compression,
    input: &[u8],
    output: &mut [u8],
) -> std::io::Result<()> {
    let _span = trace_span!("decompress_to_slice").entered();

    match compression {
        Compression::Zstd => {
            let mut dec = zstd::Decoder::new(input)?;
            dec.read_exact(output)?;
        }

        Compression::Deflate => {
            let mut dec = flate2::read::DeflateDecoder::new(input);
            dec.read_exact(output)?;
        }
    };

    trace!(
        ?compression,
        compressed_len = input.len(),
        uncompressed_len = output.len()
    );

    Ok(())
}

```

`msfz/src/lib.rs`:

```rs
//! Multi-Stream File - Compressed
//!
//! This crate allows reading and writing PDZ/MSFZ files. PDZ/MSFZ files are similar to PDB/MSF
//! files. They contain a set of streams, which are indexed by number. Each stream is a sequence
//! of bytes, similar to an ordinary file.
//!
//! See the [`spec`] module for a description of the MSFZ file format.

#![forbid(unsafe_code)]
#![forbid(unused_must_use)]
#![warn(missing_docs)]
#![allow(clippy::needless_lifetimes)]

#[cfg(doc)]
pub mod spec {
    #![doc = include_str!("msfz.md")]
    use super::*;
}

use anyhow::Result;
use zerocopy::{FromBytes, FromZeros, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U32, U64};

mod compress_utils;
mod reader;
mod stream_data;
#[cfg(test)]
mod tests;
mod writer;

pub use reader::*;
pub use stream_data::StreamData;
pub use writer::*;

/// Describes the header at the start of the MSFZ file.
///
/// This describes the on-disk layout of the file header. It is stored at the beginning of the
/// MSFZ file.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
pub struct MsfzFileHeader {
    /// Identifies this as an MSFZ file. The value must always be [`MSFZ_FILE_SIGNATURE`].
    pub signature: [u8; 32],

    /// Specifies the version of the MSFZ file layout.
    pub version: U64<LE>,

    /// The file offset of the stream directory.
    pub stream_dir_offset: U64<LE>,

    /// The file offset of the Chunk Table, which has type `[ChunkEntry; num_chunks]`.
    pub chunk_table_offset: U64<LE>,

    /// The number of streams stored within this MSFZ file.
    pub num_streams: U32<LE>,

    /// The compression algorithm applied to the stream directory.
    pub stream_dir_compression: U32<LE>,

    /// The size in bytes of the stream directory, compressed (on-disk).
    pub stream_dir_size_compressed: U32<LE>,

    /// The size in bytes of the stream directory after decompression (in-memory).
    pub stream_dir_size_uncompressed: U32<LE>,

    /// The number of compression chunks.
    pub num_chunks: U32<LE>,

    /// The size in bytes of the Chunk Table.
    pub chunk_table_size: U32<LE>,
}

/// Describes one compressed chunk.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
pub struct ChunkEntry {
    /// File offset (within the MSFZ file) the compressed chunk.
    pub file_offset: U64<LE>,

    /// The compression algorithm for this chunk.
    pub compression: U32<LE>,

    /// Size in bytes of the compressed data on disk.
    ///
    /// This value should be non-zero.
    pub compressed_size: U32<LE>,

    /// Number of bytes after decompression; this is the in-memory size.
    ///
    /// This value should be non-zero.
    pub uncompressed_size: U32<LE>,
}

/// The special value used for stream size to indicate a nil stream.
pub const NIL_STREAM_SIZE: u32 = 0xffff_ffff;

/// Indicates that no compression is used.
pub const COMPRESSION_NONE: u32 = 0;

/// Identifies the [`Zstd`](https://github.com/facebook/zstd) compression algorithm.
pub const COMPRESSION_ZSTD: u32 = 1;

/// Identifies the [`Deflate`](https://en.wikipedia.org/wiki/Deflate) compression algorithm.
///
/// This uses the "raw" Deflate stream. It _does not_ use the GZIP encapsulation header.
pub const COMPRESSION_DEFLATE: u32 = 2;

/// Specifies the compression algorithms that are supported by this library.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
#[non_exhaustive]
pub enum Compression {
    /// Identifies the [`Zstd`](https://github.com/facebook/zstd) compression algorithm.
    Zstd,
    /// Identifies the [`Deflate`](https://en.wikipedia.org/wiki/Deflate) compression algorithm.
    Deflate,
}

impl Compression {
    fn to_code(self) -> u32 {
        match self {
            Self::Zstd => COMPRESSION_ZSTD,
            Self::Deflate => COMPRESSION_DEFLATE,
        }
    }

    fn try_from_code(code: u32) -> Result<Self, UnsupportedCompressionError> {
        match code {
            COMPRESSION_ZSTD => Ok(Self::Zstd),
            COMPRESSION_DEFLATE => Ok(Self::Deflate),
            _ => Err(UnsupportedCompressionError),
        }
    }

    fn try_from_code_opt(code: u32) -> Result<Option<Self>, UnsupportedCompressionError> {
        if code != COMPRESSION_NONE {
            Ok(Some(Self::try_from_code(code)?))
        } else {
            Ok(None)
        }
    }
}

#[derive(Copy, Clone, Debug)]
struct UnsupportedCompressionError;

impl std::error::Error for UnsupportedCompressionError {}

impl std::fmt::Display for UnsupportedCompressionError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "The specified compression mode is not recognized or supported."
        )
    }
}

/// The signature of a MSFZ/PDZ file.
pub const MSFZ_FILE_SIGNATURE: [u8; 32] = *b"Microsoft MSFZ Container\r\n\x1aALD\0\0";

#[test]
fn print_file_signature() {
    use pretty_hex::PrettyHex;
    println!("\n{:?}", MSFZ_FILE_SIGNATURE.hex_dump());
}

/// The current version of the PDZ specification being developed.
pub const MSFZ_FILE_VERSION_V0: u64 = 0;

/// Handles packing and unpacking the `file_offset` for compressed streams.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
struct ChunkAndOffset {
    chunk: u32,
    offset: u32,
}

/// Checks whether the header of a file appears to be a valid MSFZ/PDZ file.
///
/// This only looks at the signature; it doens't read anything else in the file.
pub fn is_header_msfz(header: &[u8]) -> bool {
    header.starts_with(&MSFZ_FILE_SIGNATURE)
}

#[derive(Default)]
struct Stream {
    fragments: Vec<Fragment>,
}

// Describes a region within a stream.
#[derive(Clone)]
struct Fragment {
    size: u32,
    location: FragmentLocation,
}

impl std::fmt::Debug for Fragment {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "size 0x{:05x} at {:?}", self.size, self.location)
    }
}

impl std::fmt::Debug for FragmentLocation {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Uncompressed { file_offset } => {
                write!(f, "uncompressed at 0x{:06x}", file_offset)
            }
            Self::Compressed {
                chunk_index,
                offset_within_chunk,
            } => write!(f, "chunk {} : 0x{:04x}", chunk_index, offset_within_chunk),
        }
    }
}

const FRAGMENT_LOCATION_CHUNK_BIT: u32 = 63;
const FRAGMENT_LOCATION_CHUNK_MASK: u64 = 1 << FRAGMENT_LOCATION_CHUNK_BIT;

#[derive(Clone)]
enum FragmentLocation {
    Uncompressed {
        file_offset: u64,
    },
    Compressed {
        chunk_index: u32,
        offset_within_chunk: u32,
    },
}

```

`msfz/src/msfz.md`:

```md
# MSFZ Container Specification

This document specifies the MSFZ Container file format. MSFZ replaces one part
of the design of PDB in order to improve the storage and access costs associated
with PDB files.

PDB files store information in _streams_, which are similar to ordinary files.
PDB uses the Multi-Stream File (MSF) container format for storing streams. MSF
allows for easy modification of existing streams; streams may be added, deleted,
or modified without rebuilding the entire file or accessing unrelated parts of
the file.

Unfortunately, MSF's capabilities for modifying streams come with a significant
cost. MSF files divide streams into fixed-size pages, manage allocation state
for those pages, etc. Because most streams' length is not a multiple of the page
size, space is wasted in the last page of each stream due to fragmentation.
(Losses due to fragmentation have been observed to be between 5% and 8% of
typical PDB size.) MSF's complex structure also complicates the process of
decoding a PDB file because the pages that compose a particular stream can be
stored at any location in the file; a stream's pages are not contiguous.

MSFZ makes a different set of trade-offs. It optimizes for the reader, not the
writer. MSFZ files are intended to be written and then never modified
(in-place); if a tool needs to modify information within an MSFZ file then an
entirely new file generally needs to be created. This allows MSFZ to eliminate
the concept of pages and the bookkeeping related to them (the Free Page Map).

MSFZ also supports transparent, "chunked" compression. MSFZ may compress stream
data, and when it does, it may group together parts of different streams into
the same compression chunk. This allows very large streams to be broken into
fragments, where each fragment can be loaded from disk and decompressed without
needing to load and decompress other chunks. This also allows many small streams
to be grouped together into a single chunk and compressed together.

MSFZ uses a more efficient representation of its Stream Directory, compared to
that of MSF. MSF uses lists of page numbers; for many streams, these page
numbers are sequential. MSFZ does not use pages at all, but MSFZ does allow a
stream to be composed of multiple _fragments_. A fragment is a contiguous
sequence of bytes within a stream; a fragment may be compressed or uncompressed.
This allows contiguous streams to be described very efficiently in the Stream
Directory, even streams that are quite large (such as the TPI Stream).

MSFZ replaces only the MSF layer of PDBs. It does not change the model of
streams or the data stored within those streams. PDB files can be converted from
MSF to MSFZ container format, and the reverse, without losing any information.
MSFZ is thus a simple and low-risk change to how PDB files work, with
substantial benefits in size reduction, while still allowing tools to read only
those parts of a PDB that they require.

For the sake of brevity, we will refer to PDB files that use the MSFZ container
format as "PDZ files". These files use the same PDB data structures, but they
are stored using the MSFZ container format rather than MSF.

PDZ files cannot be read directly by tools (debuggers, etc.) that have not been
updated to read PDZ files. These tools will need to be updated, or the PDZ file
will need to be converted to PDB before it is used by the tool.

Many parts of the software ecosystem rely on the `*.PDB` file extension to
identify symbol files. It would be costly to update the ecosystem (ADO
pipelines, customer workflows, etc.) just to use a new file extension for
PDB/MSF files. However, when it is necessary to distinguish between PDB/MSF and
PDB/MSFZ file, we suggest using the `*.PDZ` file extension for PDB/MSFZ files.

## MSFZ concepts: Streams, Fragments, Chunks

A _stream_ is a sequence of bytes that contain PDB-related data. Streams are
stored within MSF and MSFZ files, but are not necessarily stored "directly" in
the sense of a 1:1 mapping between the bytes in the stream and the bytes in the
container. The concept of a stream is the same in MSF and MSFZ, although the
representation is quite different.

A _fragment_ is a contiguous portion of a stream. A stream is composed of zero
or more concatenated fragments. Each byte of stream data is stored in some
fragment and fragments are non-overlapping. Fragments may be stored _compressed_
or _uncompressed_; these will both be described below. Fragments can be stored
in different locations on disk, in the MSFZ file. (Fragment is a concept only in
MSFZ.) Each stream contains a list of its fragments.

A _chunk_ is the unit of compression for compressed stream data. If a fragment
of a stream is compressed, then it is stored within a chunk. Fragments from more
than one stream may be stored within the same chunk. The Chunk Table lists all
of the chunks in a given MSFZ file.

## MSFZ data structures

The MSFZ file consists of these data structures:

* The MSFZ File Header, which identifies the file format, specifies global
  parameters, and specifies the locations of other data structures. The MSFZ
  File Header is located at file offset 0 and is the only data structure that
  has a fixed location. All other data structures are stored at offsets that are
  specified as fields in other data structures.

* The Stream Directory, which specifies the list of fragments that compose the
  contents of each stream. The list of fragments implicitly specifies the size
  of the stream.

* The Chunk Table, which specifies the size and location of each compressed
  chunk.

* Uncompressed stream data fragments, which contains the contents of streams
  that do not use compression.

* Compressed chunks, which contain stream data that has been compressed.
  Compressed chunks can contain stream data from more than one stream; the
  contents of a single stream may be stored in multiple chunks. The relationship
  between streams and chunks is clarified in later sections of this document.

All MSFZ structures use little-endian (LSB first) byte order. The `a-b` notation
for ranges specifies an inclusive range, e.g. the range `0-31` _includes_ values
0 and 31 (and all values between them).

### MSFZ File Header

An MSFZ file begins with this header, at file offset 0:

```c
struct MsfzFileHeader {
    uint8_t signature[32];                  // Identifies this as an MSFZ file.
    uint64_t version;                       // specifies the version number of the MSFZ file format
    uint64_t stream_dir_offset;             // file offset of the stream directory
    uint64_t chunk_table_offset;            // file offset of the chunk table
    uint32_t num_streams;                   // the number of streams stored within this MSFZ file
    uint32_t stream_dir_compression;        // compression algorithm used for the stream directory
    uint32_t stream_dir_size_compressed;    // size in bytes of the stream directory when compressed (on disk)
    uint32_t stream_dir_size_uncompressed;  // size in bytes of the stream directory when uncompressed (in memory)
    uint32_t num_chunks;                    // number of compressed chunks
    uint32_t chunk_table_size;              // size in bytes of the chunk table
};
// sizeof(MsfzFileHeader) == 80
```

All fields in `MsfzFileHeader` are at offsets that are naturally aligned for the type of the field.

The `signature` field identifies a file as being an MSFZ file. It has this value:

```text
00000000 :  4d 69 63 72 6f 73 6f 66 74 20 4d 53 46 5a 20 43 : Microsoft MSFZ C
00000010 :  6f 6e 74 61 69 6e 65 72 0d 0a 1a 41 4c 44 00 00 : ontainer...ALD..
```

The `version` field specifies the file format version used for the MSFZ container format. It is
not related to the version fields of the data stored within PDB streams; it specifies only the
format of the data specified by this specification. The only supported value is:

```c++
const uint64_t MSFZ_FILE_VERSION_V0 = 0;
```

New version numbers will be assigned if the MSFZ container format changes, such
as moving fields or changing their type. All changes to version are assumed to
be incompatible; a MSFZ reader must only decode an MSFZ file if it supports the
specified version number exactly.

`num_streams` field specifies the number of streams in the file. This value has
the same meaning as the number of streams stored in the PDB/MSF Stream
Directory. This value must always be greater than or equal to 1.

`stream_dir_compression` specifies the compression algorithm used for the Stream
Directory. This is described below.

The `stream_dir_size_compressed` and `stream_dir_offset` fields specify the
location and size in bytes of the compressed Stream Directory within the MSFZ
file. `stream_dir_compression` specifies the compression algorithm used to
compress the Stream Directory.

`stream_dir_size_uncompressed` specifies the size in bytes of the Stream
Directory after it has been decompressed in memory.

`num_chunks` specifies the number of compression chunks, which are described in
the Chunk Table. Each entry in the Chunk Table is described by a `ChunkEntry`
record. Chunk compression is described below. The `chunk_table_size` and
`chunk_table_offset` fields specify the location of the Chunk Table. The Chunk
Table itself is never compressed because it is usually small enough that
compression has an insignificant impact.

### Stream Directory

The Stream Directory describes the streams stored in the MSFZ file. It is
encoded using a variable-length encoding scheme because each stream may be
composed of a variable number of fragments.

The MSFZ File Header specifies number of streams as `num_streams`.

Streams may be _nil streams_. A nil stream does not contain any data but is
distinguishable from non-nil stream with zero-length. (Nil streams in MSFZ exist
for compatibility with MSF.) Nil streams are encoded by a `u32` with the value
0xffff_ffff, and are _not_ followed by any fragment records at all, not even a 0
terminator for the fragment list:

```text
00000000 : ff ff ff ff
```

Non-nil streams are encoded by a sequence of _fragment records_. Each fragment
record describes a contiguous sequence of bytes within a stream. Fragments can
be either _compressed_ or _uncompressed_. See below for a description of
compressed and uncompressed fragments.

Each fragment record starts with a `u32` value which specifies the size in bytes
of the fragment. If the fragment is compressed, then this value gives the
_uncompressed_ size of the fragment. That is, the value always describes the
number of bytes of stream data. The fragment size is always non-zero; the value
0 encodes "end of fragment list", not "zero-length fragment".

The fragment size is followed by a `u64` value that specifies the location and
encoding of the fragment.

The following pseudo-structure describes the structure of a non-nil stream
record:

```c
struct Stream {
    Fragment fragments[];       // variable-length list of fragments
    uint32_t end;               // value is always zero
};

struct Fragment {
    uint32_t size;              // size (uncompressed) of this fragment; is never zero
    uint64_t location;          // bit-packed field containing location of this fragment
};
```

Fragments may be stored in _compressed_ or _uncompressed_ form. The `location`
field specifies whether the fragment is compressed or uncompressed mode, and for
each mode, where to find the contents of the fragment. Bit 63 of `location` is
set to 0 for uncompressed mode and 1 for compressed mode.

The `location` field within `Fragment` is _not_ aligned. The size of `Fragment`
is 12 bytes, not 16 bytes.

The `location` field for uncompressed fragments:

```text
+----+---------------+--------------------------------+
| 63 | 62 ... 48     | 47 ... 0                       |
+----+---------------+--------------------------------+
|  0 | reserved (0)  | file_offset                    |
+----+---------------+--------------------------------+
```

The `location` field for compressed fragments:

```text

+----+---------------------+--------------------------+
| 63 | 62 ... 32           | 31 ... 0                 |
+----+---------------------+--------------------------+
|  1 | first_chunk         | offset_within_chunk      |
+----+---------------------+--------------------------+
```

Keep in mind that the `location` field is stored as 64-bit LSB-first value. Bit
63 is stored in bit 7 of byte 7, not in bit 7 of byte 0.

Fragments always have a non-zero size. We use the value 0 to indicate the end of
a sequence of fragments. There is _not_ a `location` field after the `size` if
`size` is 0.

The size of each stream is computed as the sum of the size of the stream's
fragments. The stream size is not directly represented in the encoded form of
the Stream Directory. Typically, most streams have only a single fragment.

Because the size of a stream is computed as the sum of the size of the stream's
fragments, stream size is not limited to 32-bit lengths. Large streams (those
larger than `1 << 32`) can be encoded using multiple fragments. Decoders should
be prepared for recognizing large streams, even if their implementation cannot
otherwise handle large streams.

If a fragment record describes a compressed fragment, then the `first_chunk`
field is the index, within the Chunk Table, of the first fragment that
contributes to this stream. If the size of the fragment and the
`offset_within_chunk` field exceed the size of that first chunk, then the reader
advances to the next chunk, and reads data from its decompressed form. This
repeats until we reach the end of the fragment records for this stream.

For this reason, the order of entries in the Chunk Table is very important
because it determines the order of the uncompressed data that is stored within
the chunks. You can think of the decompressed form of the chunks as forming a
single "virtual address space" and the fragment records as reading from that
address space.

The location of the _compressed_ form of the chunks on-disk is not important,
however. The encoder may write chunks in any order, as long as the bytes that
compose each chunk are contiguous within that chunk (compressed chunks cannot be
interleaved) and the order of the entries in the Chunk Table is correct.

### Example

This is an example of a stream directory.

```text
offset   : contents
00000000 : 00 00 00 00            // stream 0; has no fragments, size is 0
00000004 : a0 01 00 00            // stream 1, fragment 0, fragment size is 0x1a0
00000008 : 50 00 00 00            // fragment 0 location: uncompressed, file offset 0x50
0000000c : 00 00 00 00
00000010 : 00 00 00 00            // end of stream 1
00000014 : ff ff ff ff            // stream 2 is nil
00000018 : 84 ac 06 00            // stream 3, fragment 0, fragment size is 0x6ac84
0000001c : e0 10 00 00            //     offset_within_chunk: 0x10e0
00000020 : 80 00 00 e5            //     chunk_index e5, with bit 63 set (meaning: compressed chunk)
```

It is expected that most streams will be stored compressed. Compression not only
reduces the size of PDZ files, but typically makes I/O more efficient.

The Stream Directory itself can be compressed. The `stream_dir_compression`
field of the MSFZ File Header specifies the compression algorithm used for the
Stream Directory.

### Uncompressed streams

In uncompressed form, the stream data is stored in a single contiguous sequence
of bytes in the MSF file. The `location` field sets bit 63 to 0. Bits 0-47
contains the file offset of the stream data. Bits 48-62 of `location` are
reserved and are set to 0. The contents of the stream are stored contiguously in
the MSFZ file; MSFZ does not use pages. The stream data may be read directly
from the MSFZ file without any processing.

### Chunk-compressed streams

If a stream contains compressed fragments, then the `location` field of
`Fragment` has bit 63 set to 1 and also contains two bit-packed subfields:
`chunk` (stored in bits 32-62) and `offset_within_chunk` (stored in bits 0-31).
`chunk` specifies the index of the chunk that contains the fragment's data.
`offset_within_chunk` specifies the _decompressed_ byte offset within the chunk
where the stream's data begins. The Chunk Table is described below.

Chunk boundaries are not required to be aligned to fragment boundaries. A chunk
may contain data from more than one fragment and these fragments may come from
different streams. The contents of a single stream may be spread across multiple
fragments, possibly using different compression modes (compressed vs.
uncompressed).

Compressed chunks may be stored in any order on disk.

This diagram illustrates some of the cases:

```text
                               --> layout of file contents -->

-------------------+------------------------------------+---------------------+-----------------
    compressed     | compressed                         | uncompressed        | compressed
... chunk 42       | chunk 43                           | stream data         | chunk 44 ...
-------------------+------------------------------------+---------------------+-----------------
       ↑                        ↑                       ↑                     ↑
       |                        |                       |                     |
------/ \---- stream 10 -------/ \-- stream 11 --------/ \--- stream 12 -----/ \--- stream 13 --
```

* Stream 10 is compressed and spans chunks 42 and 43.
* Stream 11 is compressed and is stored entirely within chunk 43.
* Stream 12 is uncompressed. Its contents are stored in the MSFZ file but there is no entry in the
  Chunk Table for it.
* Stream 13 is compressed and begins in chunk 44.

### Chunk Table

The Chunk Table lists the compressed chunks within the MSFZ file. The location
and size of the Chunk Table is specified in the MSFZ File Header. The Chunk
Table is an array of `ChunkEntry` records:

```c
struct ChunkEntry {
    uint64_t file_offset;               // file offset of the compressed data for this chunk
    uint32_t compression;               // compression algorithm for this chunk
    uint32_t compressed_size;           // size in bytes of the compressed (on-disk) chunk data
    uint32_t uncompressed_size;         // size in bytes of the uncompressed (in-memory) chunk data
};
```

Note that `ChunkEntry` contains unaligned data. The size of `ChunkEntry` is 20
bytes, not 24, and the alignment is 1, not 8.

Each chunk specifies its compression algorithm, and hence different chunks may
use different compression algorithms. Currently, the only supported algorithm is
[`Zstd`](https://github.com/facebook/zstd).

To read any data from a chunk, usually the entire compressed chunk must be read
from disk and decompressed. For this reason, the encoder of an MSFZ file chooses
a chunk size that is a good tradeoff between encoding efficiency and the cost of
reading and decompressing chunks. There is no requirement that different chunks
have the same size, either before or after decompression. This gives encoders a
lot of freedom.

The _uncompressed_ form of consecutive chunks forms a virtual byte array.
Streams that cross chunk boundaries rely on this. If a stream crosses one or
more chunk boundaries, then all of the chunks that contribute to the stream must
be contiguous in the Chunk Table.

Each entry in the Chunk Table specifies the file offset where the compressed
form of that chunk begins, the size in bytes of the compressed chunk (on-disk),
and the size in bytes of the decompressed chunk (in memory). This allows
decoders to allocate buffers of the correct size before reading data from disk.

### Order of chunk contents and uncompressed stream contents

The location and size of each compressed chunk is specified by that chunk's
entry in the Chunk Table. Similarly, the size and location of each uncompressed
stream is stored in that stream's entry in the Stream Directory. Let _fragment_
refer to either a portion of a compressed chunk or to a contiguous sequence of bytes that
are stored in the MSFZ file without any compression.

Fragments may be stored anywhere in the MSFZ file, except for the following
constraints:

* No two fragments may overlap.
* No fragment may overlap the MSFZ File Header.
* No fragment may overlap the Stream Directory.
* No fragment may overlap the Chunk Table.

These constraints _allow_ the following:

* Uncompressed fragments and compressed chunks may appear in any order within
  the MSFZ file.

* The order of records in the Chunk Table is significant, but the location of
  the compressed chunk data within the MSFZ file is not significant. Encoders
  may write compressed chunks in any order, as long as the Chunk Table correctly
  describes their location.

* There may be unused bytes between pieces. These unused bytes can be used for
  alignment padding. Compressed chunks do not need alignment padding. However,
  encoders may choose to align the starting offset of uncompressed file
  structures, to enable direct memory mapping of uncompressed streams.

  Encoders should ensure that unused bytes do not contain data that was not
  intended to be written to disk, such as accidentally-copied data from memory.
  Encoders are encouraged to always zero-fill unused bytes.

This gives encoders considerable freedom when writing MSFZ files. The following
lists some (non-normative, non-exclusive) ideas for using this freedom when
encoding MSFZ files:

* Simplicity: Since encoders may write pieces in any order, they can simply
  write data as it becomes available.
* Locality: Encoders may write the data that they expect to be accessed most
  frequently in neighboring locations within the file. For example, the Stream
  Directory could be written immediately after the MSF File Header, followed by
  the contents of the PDBI Stream and the DBI Stream. This would allow a reader
  to read a lot of useful data in a single read, at the start of the file,
  without decoding any of the contents of file.
* Encoding efficiency: An encoder may compress chunks using multiple threads
  (CPUs). As each CPU finishes writing a chunk, it may directly append the chunk
  data to the output file. This would minimize lock contention and serialization
  within the encoding process. However, this does introduce non-determinism,
  since different CPUs would race for access to the file.

There are other hypothetical ways to use this freedom. The point is, the
specification permits encoders to write pieces in any order, as long as the
Stream Directory and Chunk Table correctly describe the streams.

## Compression algorithms

Several fields identify a compression algorithm. This section enumerates the specified
compression algorithms.

```c++
const uint32_t COMPRESSION_NONE = 0;
const uint32_t COMPRESSION_ZSTD = 1;
const uint32_t COMPRESSION_DEFLATE = 2;
```

## Procedure for opening and validating an MSFZ file

* Read the MSFZ File Header.
  * Verify that `signature` matches `MSFZ_FILE_SIGNATURE`.
  * Verify that `version` matches a supported version. (Currently, the only supported version is `MSFZ_FILE_VERSION_V0`.)
  * Verify that `stream_dir_compression` is a supported compression algorithm.
  * Verify that
    `num_streams * size_of::<StreamEntry>() == stream_dir_size_uncompressed`.
  * Verify that `num_chunks * size_of::<ChunkEntry>() == chunk_table_size`.
  * Verify that the file range for the Stream Directory is valid, given the
    length of the MSFZ file.
  * Verify that the file range for the Chunk Table is valid, given the length of
    the MSFZ file.
  * Read the Stream Directory.
  * Read the Chunk Table.
* For each `ChunkEntry` in the Chunk Table:
  * Verify that the `uncompressed_size` is non-zero
  * Verify that the `compressed_size` is non-zero and that the file range
    implied by the `file_offset` and `compressed_size` is valid, given the
    length of the MSFZ file.
* For each `StreamEntry` in the Stream Directory:
  * If the stream is a nil stream (`size == NIL_STREAM_SIZE`) or a zero-length stream (`size == 0`), then verify that `location == 0`.
  * If the stream is uncompressed, verify that the file range implied by the `location` and `uncompressed_size` is valid.
  * If the stream is compressed, verify that the range of chunks implied by the
    `location` (when unpacked into `first_chunk` and `offset_within_chunk`) and
    `uncompressed_size` is valid. This requires checking the contents of the
    Chunk Table. Also verify that the `uncompressed_size` is non-zero.
* Verify that the byte ranges for all compressed chunks and uncompressed streams
  are non-overlapping and that they do not overlap the File Header, Stream
  Directory, or Chunk Table.

## Procedure for reading data from a stream

Given a stream `s`, an `offset` within the stream to read, and `len` bytes to read:

* Locate `s` in the `StreamEntry` in the Stream Directory. `s` is required to be
  less than `num_streams`.
* If `compression` is `COMPRESSION_NONE`, then use the `location` and
  `uncompressed_size` to read the contents of the stream directly from the MSFZ
  file.
* If `compression` is `COMPRESSION_CHUNKED`, then:
  * Decompose the `location` into `first_chunk` and `offset_within_chunk`. These
    identify the chunk that contains the data at the start of the stream.
    Remember that the chunk may contain data from other streams, which is why
    the `offset_within_chunk` value is necessary.
  * Use the `offset` and `offset_within_chunk` values to scan forward within the
    chunk list. The goal is to find the first chunk that contains the desired
    data (the data at `offset`), rather than simply the first chunk that
    contains data at stream offset 0.
  * If necessary, read chunks from disk and decompress them.
  * Scan forward in the chunk list. Consume `len` bytes of data from chunks,
    crossing chunk boundaries if necessary.

## Author

* [Arlie Davis](ardavis@microsoft.com)

```

`msfz/src/reader.rs`:

```rs
use crate::*;
use anyhow::{bail, Result};
use core::mem::size_of;
use std::fs::File;
use std::io::{Read, Seek, SeekFrom};
use std::path::Path;
use std::sync::{Arc, OnceLock};
use sync_file::{RandomAccessFile, ReadAt};
use tracing::{debug, debug_span, info, info_span, trace, trace_span};
use zerocopy::IntoBytes;

/// Reads MSFZ files.
pub struct Msfz<F = RandomAccessFile> {
    file: F,
    stream_dir: Vec<Option<Stream>>,
    chunk_table: Box<[ChunkEntry]>,
    chunk_cache: Vec<OnceLock<Arc<[u8]>>>,
}

impl Msfz<RandomAccessFile> {
    /// Opens an MSFZ file and validates its header.
    pub fn open<P: AsRef<Path>>(path: P) -> Result<Self> {
        let f = File::open(path)?;
        let raf = RandomAccessFile::from(f);
        Self::from_file(raf)
    }
}

impl<F: ReadAt> Msfz<F> {
    /// Opens an MSFZ file using an implementation of the [`ReadAt`] trait.
    pub fn from_file(file: F) -> Result<Self> {
        let _span = info_span!("Msfz::from_file").entered();

        let mut header: MsfzFileHeader = MsfzFileHeader::new_zeroed();
        file.read_exact_at(header.as_mut_bytes(), 0)?;

        if header.signature != MSFZ_FILE_SIGNATURE {
            bail!("This file does not have a PDZ file signature.");
        }

        if header.version.get() != MSFZ_FILE_VERSION_V0 {
            bail!("This PDZ file uses a version number that is not supported.");
        }

        // Load the stream directory.
        let num_streams = header.num_streams.get();
        if num_streams == 0 {
            bail!("The stream directory is invalid; it is empty.");
        }

        let stream_dir_size_uncompressed = header.stream_dir_size_uncompressed.get() as usize;
        let stream_dir_size_compressed = header.stream_dir_size_compressed.get() as usize;
        let stream_dir_file_offset = header.stream_dir_offset.get();
        let stream_dir_compression = header.stream_dir_compression.get();
        info!(
            num_streams,
            stream_dir_size_uncompressed,
            stream_dir_size_compressed,
            stream_dir_compression,
            stream_dir_file_offset,
            "reading stream directory"
        );

        let mut stream_dir_bytes: Vec<u8> = vec![0; stream_dir_size_uncompressed];
        if let Some(compression) = Compression::try_from_code_opt(stream_dir_compression)? {
            let mut compressed_stream_dir: Vec<u8> = vec![0; stream_dir_size_compressed];
            file.read_exact_at(
                compressed_stream_dir.as_mut_bytes(),
                header.stream_dir_offset.get(),
            )?;

            debug!("decompressing stream directory");

            crate::compress_utils::decompress_to_slice(
                compression,
                &compressed_stream_dir,
                &mut stream_dir_bytes,
            )?;
        } else {
            file.read_exact_at(stream_dir_bytes.as_mut_bytes(), stream_dir_file_offset)?;
        }

        let stream_dir = decode_stream_dir(&stream_dir_bytes, num_streams)?;

        // Load the chunk table.
        let num_chunks = header.num_chunks.get() as usize;
        let chunk_index_size = header.chunk_table_size.get() as usize;
        if chunk_index_size != num_chunks * size_of::<ChunkEntry>() {
            bail!("This PDZ file is invalid. num_chunks and chunk_index_size are not consistent.");
        }

        let chunk_table_offset = header.chunk_table_offset.get();
        // unwrap() is for OOM handling.
        let mut chunk_table: Box<[ChunkEntry]> =
            FromZeros::new_box_zeroed_with_elems(num_chunks).unwrap();
        if num_chunks != 0 {
            info!(
                num_chunks,
                chunk_table_offset, "reading compressed chunk table"
            );
            file.read_exact_at(chunk_table.as_mut_bytes(), chunk_table_offset)?;
        } else {
            // Don't issue a read. The writer code may not have actually extended the file.
        }

        let mut chunk_cache = Vec::with_capacity(num_chunks);
        chunk_cache.resize_with(num_chunks, Default::default);

        Ok(Self {
            file,
            stream_dir,
            chunk_table,
            chunk_cache,
        })
    }

    /// The total number of streams in this MSFZ file. This count includes nil streams.
    pub fn num_streams(&self) -> u32 {
        self.stream_dir.len() as u32
    }

    /// Gets the size of a given stream, in bytes.
    ///
    /// The `stream` value must be in a valid range of `0..num_streams()`.
    ///
    /// If `stream` is a NIL stream, this function returns 0.
    pub fn stream_size(&self, stream: u32) -> u64 {
        assert!((stream as usize) < self.stream_dir.len());
        if let Some(stream) = &self.stream_dir[stream as usize] {
            stream.fragments.iter().map(|f| f.size as u64).sum()
        } else {
            0
        }
    }

    /// Returns `true` if `stream` is a valid stream index and the stream is non-nil.
    ///
    /// Stream index 0 is reserved; this function always returns `true` for stream index 0,
    /// but the stream cannot be used to store data.
    pub fn is_stream_valid(&self, stream: u32) -> bool {
        if let Some(s) = self.stream_dir.get(stream as usize) {
            s.is_some()
        } else {
            false
        }
    }

    /// Gets a slice of a chunk. `offset` is the offset within the chunk and `size` is the
    /// length in bytes of the slice. The chunk is loaded and decompressed, if necessary.
    fn get_chunk_slice(&self, chunk: u32, offset: u32, size: u32) -> std::io::Result<&[u8]> {
        let chunk_data = self.get_chunk_data(chunk)?;
        if let Some(slice) = chunk_data.get(offset as usize..offset as usize + size as usize) {
            Ok(slice)
        } else {
            Err(std::io::Error::new(
                std::io::ErrorKind::InvalidData,
                "PDZ file contains invalid byte ranges within a chunk",
            ))
        }
    }

    fn get_chunk_data(&self, chunk_index: u32) -> std::io::Result<&Arc<[u8]>> {
        let _span = trace_span!("get_chunk_data").entered();
        trace!(chunk_index);

        debug_assert_eq!(self.chunk_cache.len(), self.chunk_table.len());

        let Some(slot) = self.chunk_cache.get(chunk_index as usize) else {
            return Err(std::io::Error::new(
                std::io::ErrorKind::InvalidInput,
                "Chunk index is out of range.",
            ));
        };

        if let Some(arc) = slot.get() {
            trace!(chunk_index, "found chunk in cache");
            return Ok(arc);
        }

        let arc = self.load_chunk_data(chunk_index)?;
        Ok(slot.get_or_init(move || arc))
    }

    /// This is the slow path for `get_chunk_data`, which loads the chunk data from disk and
    /// decompresses it.
    #[inline(never)]
    fn load_chunk_data(&self, chunk_index: u32) -> std::io::Result<Arc<[u8]>> {
        assert_eq!(self.chunk_cache.len(), self.chunk_table.len());

        let _span = debug_span!("load_chunk_data").entered();

        // We may race with another read that is loading the same entry.
        // For now, that's OK, but in the future we should be smarter about de-duping
        // cache fill requests.

        // We have already implicitly validated the chunk index.
        let entry = &self.chunk_table[chunk_index as usize];

        let compression_opt =
            Compression::try_from_code_opt(entry.compression.get()).map_err(|_| {
                std::io::Error::new(
                    std::io::ErrorKind::Unsupported,
                    "Chunk uses an unrecognized compression algorithm",
                )
            })?;

        // Read the data from disk.
        let mut compressed_data: Box<[u8]> =
            FromZeros::new_box_zeroed_with_elems(entry.compressed_size.get() as usize)
                .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        self.file
            .read_exact_at(&mut compressed_data, entry.file_offset.get())?;

        let uncompressed_data: Box<[u8]> = if let Some(compression) = compression_opt {
            let mut uncompressed_data: Box<[u8]> =
                FromZeros::new_box_zeroed_with_elems(entry.uncompressed_size.get() as usize)
                    .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;

            self::compress_utils::decompress_to_slice(
                compression,
                &compressed_data,
                &mut uncompressed_data,
            )?;
            uncompressed_data
        } else {
            // This chunk is not compressed.
            compressed_data
        };

        // This conversion should not need to allocate memory for the buffer.  The conversion from
        // Box to Arc should allocate a new Arc object, but the backing allocation for the buffer
        // should simply be transferred.
        Ok(Arc::from(uncompressed_data))
    }

    /// Reads an entire stream to a vector.
    ///
    /// If the stream data fits entirely within a single decompressed chunk, then this function
    /// returns a slice to the data, without copying it.
    pub fn read_stream(&self, stream: u32) -> anyhow::Result<StreamData> {
        let _span = trace_span!("read_stream_to_cow").entered();
        trace!(stream);

        let stream = match self.stream_dir.get(stream as usize) {
            // Stream index is out of range.
            None => bail!("Invalid stream index"),

            // Nil stream case.
            Some(None) => return Ok(StreamData::empty()),

            Some(Some(entry)) => entry,
        };

        // If the stream is zero-length, then things are really simple.
        if stream.fragments.is_empty() {
            return Ok(StreamData::empty());
        }

        // If this stream fits in a single fragment and the fragment is compressed, then we can
        // return a single borrowed reference to it. This is common, and is one of the most
        // important optimizations.
        if stream.fragments.len() == 1 {
            if let Fragment {
                size,
                location:
                    FragmentLocation::Compressed {
                        chunk_index,
                        offset_within_chunk,
                    },
            } = &stream.fragments[0]
            {
                let chunk_data = self.get_chunk_data(*chunk_index)?;
                let fragment_range =
                    *offset_within_chunk as usize..*offset_within_chunk as usize + *size as usize;

                // Validate the fragment range.
                if chunk_data.get(fragment_range.clone()).is_none() {
                    bail!("PDZ data is invalid. Stream fragment byte range is out of range.");
                }

                return Ok(StreamData::ArcSlice(Arc::clone(chunk_data), fragment_range));
            }
        }

        let stream_size: u32 = stream.fragments.iter().map(|f| f.size).sum();
        let stream_usize = stream_size as usize;

        // Allocate a buffer and copy data from each chunk.
        let mut output_buffer: Box<[u8]> = FromZeros::new_box_zeroed_with_elems(stream_usize)
            .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        let mut output_slice: &mut [u8] = &mut output_buffer;

        trace!(num_fragments = stream.fragments.len());

        for fragment in stream.fragments.iter() {
            let stream_offset = stream_usize - output_slice.len();

            // Because we computed stream_usize by summing the fragment sizes, this
            // split_at_mut() call should not fail.
            let (fragment_output_slice, rest) = output_slice.split_at_mut(fragment.size as usize);
            output_slice = rest;

            match fragment.location {
                FragmentLocation::Compressed {
                    chunk_index,
                    offset_within_chunk,
                } => {
                    let chunk_data = self.get_chunk_data(chunk_index)?;
                    if let Some(chunk_slice) = chunk_data.get(
                        offset_within_chunk as usize
                            ..offset_within_chunk as usize + fragment.size as usize,
                    ) {
                        fragment_output_slice.copy_from_slice(chunk_slice);
                    } else {
                        bail!("PDZ data is invalid. Stream fragment byte range is out of range.");
                    };
                }

                FragmentLocation::Uncompressed { file_offset } => {
                    // Read an uncompressed fragment.
                    trace!(
                        file_offset,
                        stream_offset,
                        fragment_len = fragment_output_slice.len(),
                        "reading uncompressed fragment"
                    );
                    self.file
                        .read_exact_at(fragment_output_slice, file_offset)?;
                }
            }
        }

        assert!(output_slice.is_empty());

        Ok(StreamData::Box(output_buffer))
    }

    /// Returns an object which can read from a given stream.  The returned object implements
    /// the [`Read`], [`Seek`], and [`ReadAt`] traits.
    pub fn get_stream_reader(&self, stream: u32) -> Result<StreamReader<'_, F>> {
        match self.stream_dir.get(stream as usize) {
            None => bail!("Invalid stream index"),

            Some(None) => Ok(StreamReader {
                msfz: self,
                fragments: &[],
                size: 0,
                pos: 0,
            }),

            Some(Some(entry)) => Ok(StreamReader {
                msfz: self,
                fragments: &entry.fragments,
                size: entry.fragments.iter().map(|f| f.size).sum(),
                pos: 0,
            }),
        }
    }
}

/// Allows reading a stream using the [`Read`], [`Seek`], and [`ReadAt`] traits.
pub struct StreamReader<'a, F> {
    msfz: &'a Msfz<F>,
    size: u32,
    fragments: &'a [Fragment],
    pos: u64,
}

impl<'a, F> StreamReader<'a, F> {
    /// Returns `true` if this is a zero-length stream or a nil stream.
    pub fn is_empty(&self) -> bool {
        self.stream_size() == 0
    }

    /// Size in bytes of the stream.
    ///
    /// This returns zero for nil streams.
    pub fn stream_size(&self) -> u32 {
        self.size
    }
}

impl<'a, F: ReadAt> ReadAt for StreamReader<'a, F> {
    fn read_at(&self, mut buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        if buf.is_empty() {
            return Ok(0);
        }

        let original_buf_len = buf.len();
        let mut current_offset: u64 = offset;

        for fragment in self.fragments.iter() {
            debug_assert!(!buf.is_empty());

            if current_offset >= fragment.size as u64 {
                current_offset -= fragment.size as u64;
                continue;
            }

            // Because of the range check above, we know that this cannot overflow.
            let fragment_bytes_available = fragment.size - current_offset as u32;

            let num_bytes_xfer = buf.len().min(fragment_bytes_available as usize);
            let (buf_xfer, buf_rest) = buf.split_at_mut(num_bytes_xfer);
            buf = buf_rest;

            match fragment.location {
                FragmentLocation::Compressed {
                    chunk_index,
                    offset_within_chunk,
                } => {
                    let chunk_slice = self.msfz.get_chunk_slice(
                        chunk_index,
                        offset_within_chunk + current_offset as u32,
                        num_bytes_xfer as u32,
                    )?;
                    buf_xfer.copy_from_slice(chunk_slice);
                }

                FragmentLocation::Uncompressed { file_offset } => {
                    // Read the stream data directly from disk.
                    self.msfz
                        .file
                        .read_exact_at(buf_xfer, file_offset + current_offset)?;
                }
            }

            if buf.is_empty() {
                break;
            }

            if current_offset >= num_bytes_xfer as u64 {
                current_offset -= num_bytes_xfer as u64;
            } else {
                current_offset = 0;
            }
        }

        Ok(original_buf_len - buf.len())
    }
}

impl<'a, F: ReadAt> Read for StreamReader<'a, F> {
    fn read(&mut self, buf: &mut [u8]) -> std::io::Result<usize> {
        let n = self.read_at(buf, self.pos)?;
        self.pos += n as u64;
        Ok(n)
    }
}

impl<'a, F> Seek for StreamReader<'a, F> {
    fn seek(&mut self, pos: SeekFrom) -> std::io::Result<u64> {
        match pos {
            SeekFrom::Start(p) => self.pos = p,
            SeekFrom::End(offset) => {
                let new_pos = self.stream_size() as i64 + offset;
                if new_pos < 0 {
                    return Err(std::io::ErrorKind::InvalidInput.into());
                }
                self.pos = new_pos as u64;
            }
            SeekFrom::Current(offset) => {
                let new_pos = self.pos as i64 + offset;
                if new_pos < 0 {
                    return Err(std::io::ErrorKind::InvalidInput.into());
                }
                self.pos = new_pos as u64;
            }
        }
        Ok(self.pos)
    }
}

fn decode_stream_dir(
    stream_dir_bytes: &[u8],
    num_streams: u32,
) -> anyhow::Result<Vec<Option<Stream>>> {
    let mut dec = Decoder {
        bytes: stream_dir_bytes,
    };

    let mut streams: Vec<Option<Stream>> = Vec::with_capacity(num_streams as usize);

    // Reusable buffer. We do this so that we can allocate exactly-sized fragments vectors for
    // each stream.
    let mut fragments: Vec<Fragment> = Vec::with_capacity(0x20);

    for _ in 0..num_streams {
        assert!(fragments.is_empty());

        let mut fragment_size = dec.u32()?;

        if fragment_size == NIL_STREAM_SIZE {
            // Nil stream.
            streams.push(None);
            continue;
        }

        while fragment_size != 0 {
            debug_assert_ne!(fragment_size, NIL_STREAM_SIZE);

            let mut location_bits = dec.u64()?;

            let location = if (location_bits & FRAGMENT_LOCATION_CHUNK_MASK) != 0 {
                location_bits &= !FRAGMENT_LOCATION_CHUNK_MASK;
                FragmentLocation::Compressed {
                    chunk_index: (location_bits >> 32) as u32,
                    offset_within_chunk: location_bits as u32,
                }
            } else {
                // This is an uncompressed fragment. Location is a file offset.
                FragmentLocation::Uncompressed {
                    file_offset: location_bits,
                }
            };
            fragments.push(Fragment {
                size: fragment_size,
                location,
            });

            // Read the fragment size for the next fragment. A value of zero terminates the list,
            // which is handled at the start of the while loop.
            fragment_size = dec.u32()?;
            if fragment_size == NIL_STREAM_SIZE {
                bail!("Stream directory is malformed. It contains a non-initial fragment with size = NIL_STREAM_SIZE.");
            }
            // continue for more
        }

        // Move the fragments to a new buffer with exact size, now that we know how many fragments
        // there are in this stream.
        let mut taken_fragments = Vec::with_capacity(fragments.len());
        taken_fragments.append(&mut fragments);
        streams.push(Some(Stream {
            fragments: taken_fragments,
        }));
    }

    Ok(streams)
}

struct Decoder<'a> {
    bytes: &'a [u8],
}

impl<'a> Decoder<'a> {
    fn next_n<const N: usize>(&mut self) -> anyhow::Result<&'a [u8; N]> {
        if self.bytes.len() < N {
            bail!("Buffer ran out of bytes");
        }

        let (lo, hi) = self.bytes.split_at(N);
        self.bytes = hi;
        // This unwrap() should never fail because we just tested the length, above.
        // The optimizer should eliminate the unwrap() call.
        Ok(<&[u8; N]>::try_from(lo).unwrap())
    }

    fn u32(&mut self) -> anyhow::Result<u32> {
        Ok(u32::from_le_bytes(*self.next_n()?))
    }

    fn u64(&mut self) -> anyhow::Result<u64> {
        Ok(u64::from_le_bytes(*self.next_n()?))
    }
}

```

`msfz/src/stream_data.rs`:

```rs
use core::ops::Range;
use std::sync::Arc;

use zerocopy::FromZeros;

#[cfg(doc)]
use crate::Msfz;

/// Contains the contents of an entire stream.
///
/// This is used as the return type for [`Msfz::read_stream`] function. This type either contains
/// an owned buffer (`Vec`) or a counted reference to a slice of an `Arc<[u8]>`.
///
/// See the `[Msfz::read_stream]` function for more details.
pub enum StreamData {
    /// Owned contents of stream data
    Box(Box<[u8]>),
    /// Shared contents of stream data.  The `Range` gives the range of bytes within the `Arc`.
    ArcSlice(Arc<[u8]>, Range<usize>),
}

impl StreamData {
    /// Gets a slice over the contained stream data.
    #[inline(always)]
    pub fn as_slice(&self) -> &[u8] {
        match self {
            Self::Box(v) => v,
            Self::ArcSlice(arc, range) => &arc[range.clone()],
        }
    }

    /// Returns `true` if the stream contains no data.
    pub fn is_empty(&self) -> bool {
        self.as_slice().is_empty()
    }

    /// Converts this `StreamData` into an owned `Vec<u8>`.
    pub fn into_vec(self) -> Vec<u8> {
        self.into_boxed().into()
    }

    /// Converts this `StreamData` into an owned `Box<[u8]>`.
    pub fn into_boxed(self) -> Box<[u8]> {
        match self {
            Self::Box(b) => b,
            Self::ArcSlice(arc, range) => {
                let mut b: Box<[u8]> = FromZeros::new_box_zeroed_with_elems(range.len()).unwrap();
                b.copy_from_slice(&arc[range]);
                b
            }
        }
    }
}

impl From<StreamData> for Box<[u8]> {
    fn from(s: StreamData) -> Self {
        s.into_boxed()
    }
}

impl core::ops::Deref for StreamData {
    type Target = [u8];

    #[inline(always)]
    fn deref(&self) -> &[u8] {
        self.as_slice()
    }
}

impl AsRef<[u8]> for StreamData {
    #[inline(always)]
    fn as_ref(&self) -> &[u8] {
        self.as_slice()
    }
}

impl Default for StreamData {
    fn default() -> Self {
        Self::empty()
    }
}

impl StreamData {
    /// An empty value for `StreamData`
    pub fn empty() -> Self {
        Self::Box(Box::from(&[] as &[u8]))
    }
}

```

`msfz/src/tests.rs`:

```rs
#![allow(clippy::format_collect)]

use super::*;
use bstr::BStr;
use pow2::Pow2;
use std::io::{Cursor, Read, Seek, SeekFrom, Write};
use sync_file::ReadAt;
use tracing::{debug_span, info, info_span, instrument};

#[static_init::dynamic(drop)]
static mut INIT_LOGGER: Option<tracy_client::Client> = {
    use tracing_subscriber::fmt::format::FmtSpan;
    use tracing_subscriber::layer::SubscriberExt;

    if let Ok(s) = std::env::var("ENABLE_TRACY") {
        if s == "1" {
            let client = tracy_client::Client::start();

            eprintln!("Enabling Tracy");
            tracing::subscriber::set_global_default(
                tracing_subscriber::registry().with(tracing_tracy::TracyLayer::default()),
            )
            .expect("setup tracy layer");
            return Some(client);
        }
    }

    tracing_subscriber::fmt::fmt()
        .compact()
        .with_max_level(tracing_subscriber::filter::LevelFilter::TRACE)
        .with_level(false)
        .with_file(true)
        .with_line_number(true)
        .with_span_events(FmtSpan::ENTER | FmtSpan::EXIT)
        .with_test_writer()
        .without_time()
        .with_ansi(false)
        .init();

    None
};

#[track_caller]
fn make_msfz<F>(f: F) -> Msfz<Vec<u8>>
where
    F: FnOnce(&mut MsfzWriter<Cursor<Vec<u8>>>),
{
    let _span = info_span!("make_msfz").entered();

    let cursor: Cursor<Vec<u8>> = Cursor::new(Vec::new());
    let mut w = MsfzWriter::new(cursor).unwrap();

    // Make debugging easier
    w.set_stream_dir_compression(None);

    f(&mut w);

    info!("Streams:");
    for (i, stream_opt) in w.streams.iter().enumerate() {
        if let Some(stream) = stream_opt {
            info!(stream_index = i, fragments = ?stream.fragments);
        } else {
            info!(stream_index = i, "nil stream");
        }
    }

    info!("Encoded stream directory:");
    let stream_dir_bytes = encode_stream_dir(&w.streams);
    info!(stream_dir_bytes = stream_dir_bytes.as_slice());

    let (summary, returned_file) = w.finish().unwrap();
    info!(%summary);
    let msfz_data = returned_file.into_inner();

    info!(msfz_data = msfz_data.as_slice());

    match Msfz::from_file(msfz_data) {
        Ok(msfz) => msfz,
        Err(e) => {
            panic!("Failed to decode MSFZ file during test: {e:?}");
        }
    }
}

#[test]
#[instrument]
fn basic_compressed() {
    let r = make_msfz(|w| {
        w.reserve_num_streams(10);

        // write streams out of order
        w.stream_writer(4)
            .unwrap()
            .write_all(b"Hello, world!")
            .unwrap();
        w.stream_writer(1)
            .unwrap()
            .write_all(b"Friends, Romans, yadda yadda")
            .unwrap();

        // Write a "large" stream to stream 2
        let mut big_stream: Vec<u8> = vec![0; 0x1_0000];
        let mut big = Cursor::new(&mut big_stream);
        big.seek(SeekFrom::Start(0x1000)).unwrap();
        big.write_all(b"way out in the hinterlands").unwrap();
        big.into_inner();
        w.stream_writer(2).unwrap().write_all(&big_stream).unwrap();
    });

    // Now read it back.

    assert_eq!(r.num_streams(), 10);

    assert!(r.is_stream_valid(0)); // stream directory; reserved
    assert!(r.is_stream_valid(1)); // we wrote to it
    assert!(r.is_stream_valid(2)); // we wrote to it
    assert!(!r.is_stream_valid(3));
    assert!(r.is_stream_valid(4)); // we wrote to it
    assert!(!r.is_stream_valid(5));
    assert!(!r.is_stream_valid(6));
    assert!(!r.is_stream_valid(7));
    assert!(!r.is_stream_valid(8));
    assert!(!r.is_stream_valid(9));

    assert_eq!(r.stream_size(0), 0); // stream dir stream should always be zero-length

    {
        let stream1 = r.read_stream(1).unwrap();
        assert_eq!(
            BStr::new(&stream1),
            BStr::new(b"Friends, Romans, yadda yadda")
        );
    }

    {
        let stream4 = r.read_stream(4).unwrap();
        assert_eq!(BStr::new(&stream4), BStr::new(b"Hello, world!"));
    }

    {
        // test Seek + Read
        let msg = b"way out in the hinterlands";
        let mut buf: Vec<u8> = vec![0; msg.len()];
        let mut s = r.get_stream_reader(2).unwrap();
        s.seek(SeekFrom::Start(0x1000)).unwrap();
        s.read_exact(buf.as_mut_slice()).unwrap();
        assert_eq!(BStr::new(&buf), BStr::new(msg));

        // test ReadAt
        buf.clear();
        buf.resize(msg.len(), 0);
        s.read_at(buf.as_mut_slice(), 0x1000).unwrap();
        assert_eq!(BStr::new(&buf), BStr::new(msg));
    }

    // Verify that reading a nil stream works.
    assert_eq!(r.stream_size(3), 0); // check that nil stream is zero-length
    assert!(r.read_stream(3).unwrap().is_empty());
    let mut sr = r.get_stream_reader(3).unwrap();
    assert_eq!(seek_read_span(&mut sr, 0, 0).unwrap(), &[]);
    assert_eq!(read_span_at(&sr, 0, 0).unwrap(), &[]);
}

/// Test the code for crossing chunk boundaries.
#[test]
#[instrument]
fn multi_chunks() {
    let r = make_msfz(|w| {
        w.reserve_num_streams(2);

        let mut sw = w.stream_writer(1).unwrap();
        sw.write_all(b"alpha ").unwrap(); // 0..6
        sw.end_chunk().unwrap();
        sw.write_all(b"bravo ").unwrap(); // 6..12
        sw.end_chunk().unwrap();
        sw.write_all(b"charlie").unwrap(); // 12..19
    });

    assert_eq!(r.num_streams(), 2);
    assert!(r.is_stream_valid(0)); // stream directory; reserved
    assert!(r.is_stream_valid(1)); // we wrote to it
    assert_eq!(r.stream_size(1), 19);

    // Verify that reading the entire stream works correctly.
    assert_eq!(
        BStr::new(&r.read_stream(1).unwrap()),
        BStr::new(b"alpha bravo charlie")
    );

    // Verify that Read works correctly at various offsets.

    let mut sr = r.get_stream_reader(1).unwrap();

    let cases: &[(u64, usize, &[u8])] = &[
        (0, 4, b"alph"),          // within a chunk
        (0, 6, b"alpha "),        // complete chunk
        (0, 8, b"alpha br"),      // spans 2 chunks
        (1, 3, b"lph"),           // within a single chunk
        (1, 6, b"lpha b"),        // spans 2 chunks
        (0, 12, b"alpha bravo "), // exactly 2 chunks
    ];

    for &(offset, len, expected) in cases.iter() {
        let data = seek_read_span(&mut sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );

        let data_at = read_span_at(&sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data_at),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );
    }
}

#[test]
#[instrument]
fn basic_uncompressed() {
    let big_text: String = (0..100)
        .map(|i| format!("This should compress quite well #{i}\n"))
        .collect();

    let r = make_msfz(|w| {
        w.reserve_num_streams(10);

        // Write a compressed stream.
        let mut sw = w.stream_writer(1).unwrap();
        sw.set_compression_enabled(false);
        sw.write_all(big_text.as_bytes()).unwrap();

        // Write an uncompressed stream.
        let mut sw = w.stream_writer(2).unwrap();
        sw.set_compression_enabled(false);
        sw.write_all(b"This text should not be compressed.")
            .unwrap();
    });

    let mut sr = r.get_stream_reader(1).unwrap();
    check_read_ranges(
        &mut sr,
        big_text.as_bytes(),
        &[
            (0, 0),
            (0, 50),
            (0, 100),
            (100, 50),
            (100, 50),
            (1000, 10),
            // keep on multiple lines
        ],
    );
}

#[test]
#[instrument]
fn uncompressed_stream_alignment() {
    let r = make_msfz(|w| {
        w.reserve_num_streams(5);

        assert_eq!(w.file.out.stream_position().unwrap() & 0xf, 0);

        // Write 3 bytes with no alignment requirement.
        let mut sw = w.stream_writer(1).unwrap();
        sw.set_compression_enabled(false);
        sw.set_alignment(Pow2::from_exponent(0));
        sw.write_all(b"alpha").unwrap();

        assert_eq!(w.file.out.stream_position().unwrap() & 0xf, 5);

        let mut sw = w.stream_writer(2).unwrap();
        sw.set_compression_enabled(false);
        sw.set_alignment(Pow2::from_exponent(16));
        sw.write_all(b"zzzz").unwrap();

        assert_eq!(w.file.out.stream_position().unwrap() & 0xf, 4);
    });

    drop(r);
}

#[test]
#[instrument]
fn interleaving() {
    // variant specifies bits for whether compression is enabled for various pieces.
    for variant in 0u64..16u64 {
        let vbit = |i: u32| variant & (1u64 << i) != 0;

        let r = make_msfz(|w| {
            w.reserve_num_streams(5);

            let mut sw = w.stream_writer(1).unwrap();
            sw.set_compression_enabled(vbit(0));
            sw.write_all(b"Hello, world!\n").unwrap();

            let mut sw = w.stream_writer(2).unwrap();
            sw.set_compression_enabled(vbit(1));
            sw.write_all(
                b"
The universe (which others call the Library) is composed of an indefinite,
perhaps infinite number of hexagonal galleries. In the center of each gallery is a ventillation
shaft, bounded by a low railing.
",
            )
            .unwrap();

            let mut sw = w.stream_writer(1).unwrap();
            sw.set_compression_enabled(vbit(2));
            sw.write_all(b"Goodbye, world!\n").unwrap();

            let mut sw = w.stream_writer(2).unwrap();
            sw.set_compression_enabled(vbit(3));
            sw.write_all(
                b"
From any hexagon one can see the floors above and below -- one after another, endlessly.
The arrangement of the galleries is always the same: Twenty bookshelves, five to each side,
line four of the hexagon's six sides; the height of the bookshelves, floor to ceiling, is
hardly greater than the height of a normal librarian.
",
            )
            .unwrap();

            let mut sw = w.stream_writer(1).unwrap();
            sw.write_all(b"Hello, again!\n").unwrap();

            let mut sw = w.stream_writer(2).unwrap();
            sw.write_all(
                b"
One of the hexagon's free sides opens onto a narrow sort of vestibule, which in turn opens onto
another gallery, identical to the first -- identical in fact to all.",
            )
            .unwrap();
        });

        let s1 = r.read_stream(1).unwrap();
        assert_eq!(
            std::str::from_utf8(&s1).unwrap(),
            "Hello, world!\n\
             Goodbye, world!\n\
             Hello, again!\n"
        );

        let s2 = r.read_stream(2).unwrap();
        assert_eq!(
            std::str::from_utf8(&s2).unwrap(),
            "
The universe (which others call the Library) is composed of an indefinite,
perhaps infinite number of hexagonal galleries. In the center of each gallery is a ventillation
shaft, bounded by a low railing.

From any hexagon one can see the floors above and below -- one after another, endlessly.
The arrangement of the galleries is always the same: Twenty bookshelves, five to each side,
line four of the hexagon's six sides; the height of the bookshelves, floor to ceiling, is
hardly greater than the height of a normal librarian.

One of the hexagon's free sides opens onto a narrow sort of vestibule, which in turn opens onto
another gallery, identical to the first -- identical in fact to all."
        );
    }
}

fn check_read_ranges<F: ReadAt>(
    sr: &mut StreamReader<'_, F>,
    known_good_data: &[u8],
    ranges: &[(u64, usize)],
) {
    let _span = debug_span!("check_read_ranges").entered();

    for &(offset, len) in ranges.iter() {
        let expected = &known_good_data[offset as usize..offset as usize + len];

        let data = seek_read_span(sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );

        let data_at = read_span_at(sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data_at),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );
    }
}

fn seek_read_span<R: Read + Seek>(r: &mut R, offset: u64, len: usize) -> Result<Vec<u8>> {
    let mut buf = vec![0; len];
    r.seek(SeekFrom::Start(offset))?;
    r.read_exact(buf.as_mut_slice())?;
    Ok(buf)
}

fn read_span_at<R: ReadAt>(r: &R, offset: u64, len: usize) -> Result<Vec<u8>> {
    let mut buf = vec![0; len];
    r.read_at(buf.as_mut_slice(), offset)?;
    Ok(buf)
}

```

`msfz/src/writer.rs`:

```rs
use super::*;
use anyhow::anyhow;
use pow2::Pow2;
use std::fs::File;
use std::io::{Seek, SeekFrom, Write};
use tracing::{debug, debug_span, trace, trace_span};
use zerocopy::IntoBytes;

/// The default threshold for compressing a chunk of data.
pub const DEFAULT_CHUNK_THRESHOLD: u32 = 0x40_0000; // 16 MiB

/// Allows writing a new MSFZ file.
pub struct MsfzWriter<F: Write + Seek = File> {
    pub(crate) file: MsfzWriterFile<F>,

    /// The list of streams. This includes nil streams and non-nil streams. Nil streams are
    /// represented with `None`.
    pub(crate) streams: Vec<Option<Stream>>,

    stream_dir_compression: Option<Compression>,
}

pub(crate) struct MsfzWriterFile<F: Write + Seek> {
    /// Max number of bytes to write into `uncompressed_chunk_data` before finishing (compressing
    /// and writing to disk) a chunk.
    uncompressed_chunk_size_threshold: u32,

    /// Holds data for the current chunk that we are building.
    uncompressed_chunk_data: Vec<u8>,

    /// A reusable buffer used for compressing the current chunk. This exists only to reduce
    /// memory allocation churn.
    compressed_chunk_buffer: Vec<u8>,
    /// The list of complete compressed chunks that have been written to disk.
    chunks: Vec<ChunkEntry>,

    /// Compression mode to use for the next chunk.
    chunk_compression_mode: Compression,

    /// The output file.
    pub(crate) out: F,
}

/// Describes the results of writing an MSFZ file.
#[non_exhaustive]
pub struct Summary {
    /// Number of chunks
    pub num_chunks: u32,
    /// Number of streams
    pub num_streams: u32,
}

impl std::fmt::Display for Summary {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        writeln!(f, "Number of chunks: {}", self.num_chunks)?;
        writeln!(f, "Number of streams: {}", self.num_streams)?;
        Ok(())
    }
}

impl<F: Write + Seek> MsfzWriter<F> {
    /// Creates a new writer on an object that implements [`Write`] (and [`Seek`]), such as
    /// [`File`].
    pub fn new(mut file: F) -> Result<Self> {
        let _span = trace_span!("MsfzWriter::new").entered();

        file.seek(SeekFrom::Start(0))?;

        // Write a meaningless (zero-filled) file header, just so we get the file position that we
        // want.  We will re-write this header at the end.
        let fake_file_header = MsfzFileHeader::new_zeroed();
        file.write_all(fake_file_header.as_bytes())?;

        // We do not know how many streams the writer will write. We reserve a small fixed number,
        // just guessing at the size.
        let mut streams = Vec::with_capacity(0x40);

        // Reserve stream 0 for the stream directory. The stream is zero-length.
        // Setting the size to 0 also prevents this stream from being written, which is good.
        streams.push(Some(Stream {
            fragments: Vec::new(),
        }));

        let mut this = Self {
            streams,
            file: MsfzWriterFile {
                uncompressed_chunk_size_threshold: DEFAULT_CHUNK_THRESHOLD,
                uncompressed_chunk_data: Vec::with_capacity(DEFAULT_CHUNK_THRESHOLD as usize),
                compressed_chunk_buffer: Vec::new(),
                out: file,
                chunks: Vec::new(),
                chunk_compression_mode: Compression::Zstd,
            },
            stream_dir_compression: Some(Compression::Zstd),
        };
        this.file.write_align(Pow2::from_exponent(4))?;
        Ok(this)
    }

    /// Sets the compression mode that is used for chunked compression.
    pub fn set_chunk_compression_mode(&mut self, compression: Compression) {
        // If the current chunk buffer contains data, then leave it there. It will be compressed
        // with the new algorithm.
        self.file.chunk_compression_mode = compression;
    }

    /// Sets the compression mode that is used for the Stream Directory and Chunk Table.
    pub fn set_stream_dir_compression(&mut self, compression: Option<Compression>) {
        self.stream_dir_compression = compression;
    }

    /// Reserves `num_streams` streams.
    ///
    /// If `num_streams` is less than or equal to the current number of streams, then this
    /// function has no effect.
    ///
    /// If `num_streams` is greater than the current number of streams, then new "nil" streams are
    /// added to the stream directory. These streams can be written by using the `stream_writer`
    /// function. The `stream_writer` function can only be called once for each stream index.
    pub fn reserve_num_streams(&mut self, num_streams: usize) {
        if num_streams <= self.streams.len() {
            return;
        }

        self.streams.resize_with(num_streams, Option::default);
    }

    /// Ends the current chunk, if any.
    ///
    /// This function is a performance hint for compression. It is not necessary to call this
    /// function. If you are writing two different streams that have very different contents, then
    /// it may be beneficial to put the streams into different compression chunks. This allows
    /// the compressor to adapt to the different contents of each stream.
    pub fn end_chunk(&mut self) -> std::io::Result<()> {
        self.file.finish_current_chunk()
    }

    /// Writes an existing stream.
    ///
    /// This function can only be called once for each stream index. Calling it more than once
    /// for the same stream is permitted. Note that settings on [`StreamWriter`] do not persist
    /// across multiple calls to `stream_writer()`, such as enabling/disabling chunked compression.
    pub fn stream_writer(&mut self, stream: u32) -> std::io::Result<StreamWriter<'_, F>> {
        assert!((stream as usize) < self.streams.len());

        Ok(StreamWriter {
            file: &mut self.file,
            stream: self.streams[stream as usize].get_or_insert_with(Stream::default),
            chunked_compression_enabled: true,
            alignment: Pow2::from_exponent(2), // default is 4-byte alignment
        })
    }

    /// Creates a new stream and returns a [`StreamWriter`] for it.
    pub fn new_stream_writer(&mut self) -> Result<(u32, StreamWriter<'_, F>)> {
        let stream = self.streams.len() as u32;
        self.streams.push(Some(Stream::default()));
        let w = self.stream_writer(stream)?;
        Ok((stream, w))
    }

    /// Finishes writing the MSFZ file.
    ///
    /// This writes the Stream Directory, the Chunk Table, and then writes the MSFZ file header.
    /// It then returns the inner file object. However, the caller should not write more data to
    /// the returned file object.
    pub fn finish(mut self) -> Result<(Summary, F)> {
        let _span = debug_span!("MsfzWriter::finish").entered();

        self.file.finish_current_chunk()?;

        // Write the stream directory, and optionally compress it.
        let directory_offset = self.file.write_align(Pow2::from_exponent(4))?;

        let stream_dir_bytes: Vec<u8> = encode_stream_dir(&self.streams);
        let stream_dir_size_uncompressed = u32::try_from(stream_dir_bytes.len())
            .map_err(|_| anyhow!("The stream directory is too large."))?;
        let stream_dir_size_compressed: u32;
        let stream_dir_compression: u32;
        if let Some(compression) = self.stream_dir_compression {
            stream_dir_compression = compression.to_code();
            let stream_dir_compressed_bytes =
                crate::compress_utils::compress_to_vec(compression, &stream_dir_bytes)?;
            stream_dir_size_compressed = stream_dir_compressed_bytes.len() as u32;
            self.file.out.write_all(&stream_dir_compressed_bytes)?;
        } else {
            self.file.out.write_all(&stream_dir_bytes)?;
            stream_dir_size_compressed = 0;
            stream_dir_compression = COMPRESSION_NONE;
        }

        // Write the chunk list.
        let chunk_table_offset = self.file.write_align(Pow2::from_exponent(4))?;
        let chunk_table_bytes = self.file.chunks.as_bytes();
        let chunk_table_size = u32::try_from(chunk_table_bytes.len())
            .map_err(|_| anyhow!("The chunk index is too large."))?;
        self.file.out.write_all(chunk_table_bytes)?;

        // Rewind and write the real file header.
        let file_header = MsfzFileHeader {
            signature: MSFZ_FILE_SIGNATURE,
            version: U64::new(MSFZ_FILE_VERSION_V0),
            num_streams: U32::new(self.streams.len() as u32),
            stream_dir_compression: U32::new(stream_dir_compression),
            stream_dir_offset: U64::new(directory_offset),
            stream_dir_size_compressed: U32::new(stream_dir_size_compressed),
            stream_dir_size_uncompressed: U32::new(stream_dir_size_uncompressed),
            num_chunks: U32::new(self.file.chunks.len() as u32),
            chunk_table_size: U32::new(chunk_table_size),
            chunk_table_offset: U64::new(chunk_table_offset),
        };
        self.file.out.seek(SeekFrom::Start(0))?;
        self.file.out.write_all(file_header.as_bytes())?;

        let summary = Summary {
            num_chunks: self.file.chunks.len() as u32,
            num_streams: self.streams.len() as u32,
        };

        Ok((summary, self.file.out))
    }
}

impl<F: Write + Seek> MsfzWriterFile<F> {
    /// Writes `data` to the compressed chunk stream and returns the location of the start of the
    /// data.
    ///
    /// This function does its best to keep chunks below the `uncompressed_chunk_size_threshold`.
    /// If `data.len()` would cause the current chunk to overflow the threshold, then this function
    /// finishes the current chunk and starts a new one.
    ///
    /// If `data.len()` itself is larger than `uncompressed_chunk_size_threshold`, then this
    /// function will write a chunk that is larger than `uncompressed_chunk_size_threshold`.
    /// This is common for very large data streams, such as the TPI or GSS.  Writers that want to
    /// avoid encoding very large chunks will need to break up the data and call
    /// `write_to_chunks()` repeatedly.
    ///
    /// All of the bytes of `data` will be written to a single chunk; this function never splits
    /// the data across multiple chunks.
    fn write_to_chunks(&mut self, data: &[u8]) -> std::io::Result<ChunkAndOffset> {
        let _span = debug_span!("write_to_chunks").entered();

        if data.len() + self.uncompressed_chunk_data.len()
            >= self.uncompressed_chunk_size_threshold as usize
        {
            self.finish_current_chunk()?;
        }

        // There is no guarantee that the input data fits below our threshold, of course. If we
        // receive a buffer whose size exceeds our threshold, we'll just write a larger-than-usual
        // chunk. That's ok, everything should still work.

        let chunk = self.chunks.len() as u32;
        let offset_within_chunk = self.uncompressed_chunk_data.len();

        self.uncompressed_chunk_data.extend_from_slice(data);
        Ok(ChunkAndOffset {
            chunk,
            offset: offset_within_chunk as u32,
        })
    }

    fn finish_current_chunk(&mut self) -> std::io::Result<()> {
        let _span = debug_span!("finish_current_chunk").entered();

        if self.uncompressed_chunk_data.is_empty() {
            return Ok(());
        }

        let _span = trace_span!("MsfzWriter::finish_current_chunk").entered();

        {
            let _span = trace_span!("compress chunk").entered();
            self.compressed_chunk_buffer.clear();
            crate::compress_utils::compress_to_vec_mut(
                self.chunk_compression_mode,
                &self.uncompressed_chunk_data,
                &mut self.compressed_chunk_buffer,
            )?;
        }

        let file_pos;
        {
            let _span = trace_span!("write to disk").entered();
            file_pos = self.out.stream_position()?;
            self.out.write_all(&self.compressed_chunk_buffer)?;
        }

        trace!(
            file_pos,
            compressed_size = self.compressed_chunk_buffer.len(),
            uncompressed_size = self.uncompressed_chunk_data.len()
        );

        self.chunks.push(ChunkEntry {
            compressed_size: U32::new(self.compressed_chunk_buffer.len() as u32),
            uncompressed_size: U32::new(self.uncompressed_chunk_data.len() as u32),
            file_offset: U64::new(file_pos),
            compression: U32::new(self.chunk_compression_mode.to_code()),
        });

        self.uncompressed_chunk_data.clear();
        self.compressed_chunk_buffer.clear();

        Ok(())
    }

    /// Ensures that the current stream write position on the output file is aligned to a multiple
    /// of the given alignment.
    fn write_align(&mut self, alignment: Pow2) -> std::io::Result<u64> {
        let pos = self.out.stream_position()?;
        if alignment.is_aligned(pos) {
            return Ok(pos);
        }

        let Some(aligned_pos) = alignment.align_up(pos) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };

        self.out.seek(SeekFrom::Start(aligned_pos))?;
        Ok(aligned_pos)
    }
}

/// Allows writing data to a stream.
///
/// This object does not implement [`Seek`] and there is no variant of this object that allows
/// seeking or writing to arbitrary offsets. Stream data must be written sequentially.
///
/// After a [`StreamWriter`] is closed (dropped), it is not possible to create a new `StreamWriter`
/// for the same stream.
///
/// # Write calls are never split across chunks
///
/// The [`Write`] implementation of this type makes a guarantee: For a given call to
/// [`StreamWriter::write`], if the current stream is using chunked compression, then the data will
/// be written to a single compressed chunk. This is an implementation guarantee; it is not required
/// by the MSFZ specification.
///
/// This allows readers to rely on complete records being stored within a single chunk. For example,
/// when copying the TPI, an encoder _could_ issue a sequence of `write()` calls whose boundaries
/// align with the boundaries of the records within the TPI. This would allow the reader to read
/// records directly from the chunk decompressed buffer, without needing to allocate a separate
/// buffer or copy the records. (We do not currently implement that behavior; this is describing a
/// hypothetical.)
pub struct StreamWriter<'a, F: Write + Seek> {
    file: &'a mut MsfzWriterFile<F>,
    stream: &'a mut Stream,
    alignment: Pow2,
    chunked_compression_enabled: bool,
}

impl<'a, F: Write + Seek> StreamWriter<'a, F> {
    /// Ends the current chunk, if any.
    ///
    /// This function is a performance hint for compression. It is not necessary to call this
    /// function.
    pub fn end_chunk(&mut self) -> std::io::Result<()> {
        self.file.finish_current_chunk()
    }

    /// Specifies whether to use chunked compression or not. The default value for this setting is
    /// `true` (chunked compression is enabled).
    ///
    /// This does not have any immediate effect. It controls the behavior of the `write()`
    /// implementation for this stream.
    ///
    /// If this is called with `false`, then `write()` calls that follow this will cause stream
    /// data to be written to disk without compression.
    pub fn set_compression_enabled(&mut self, value: bool) {
        self.chunked_compression_enabled = value;
    }

    /// Specifies the on-disk alignment requirement for the start of the stream data.
    ///
    /// This only applies to uncompressed streams. Compressed stream data is always stored within
    /// compressed chunks, so the alignment is meaningless.
    pub fn set_alignment(&mut self, value: Pow2) {
        self.alignment = value;
    }
}

impl<'a, F: Write + Seek> Write for StreamWriter<'a, F> {
    fn flush(&mut self) -> std::io::Result<()> {
        Ok(())
    }

    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        let _span = trace_span!("StreamWriter::write").entered();
        trace!(buf_len = buf.len());

        if buf.is_empty() {
            return Ok(0);
        }

        let old_stream_size: u32 = self.stream.fragments.iter().map(|f| f.size).sum();
        let is_first_write = old_stream_size == 0;
        let max_new_bytes = NIL_STREAM_SIZE - old_stream_size;

        // Check that buf.len() can be converted to u32, that the increase in size does not
        // overflow u32, and that writing the new data will not cause the stream size to erroneously
        // become NIL_STREAM_SIZE.
        let buf_len = match u32::try_from(buf.len()) {
            Ok(buf_len) if buf_len < max_new_bytes => buf_len,
            _ => {
                return Err(std::io::Error::new(
                    std::io::ErrorKind::InvalidInput,
                    "The input is too large for an MSFZ stream.",
                ))
            }
        };

        if self.chunked_compression_enabled {
            let chunk_at = self.file.write_to_chunks(buf)?;

            add_fragment_compressed(
                &mut self.stream.fragments,
                buf_len,
                chunk_at.chunk,
                chunk_at.offset,
            );
        } else {
            let fragment_file_offset: u64 = if is_first_write {
                self.file.write_align(self.alignment)?
            } else {
                self.file.out.stream_position()?
            };
            self.file.out.write_all(buf)?;

            add_fragment_uncompressed(&mut self.stream.fragments, buf_len, fragment_file_offset);
        }

        Ok(buf.len())
    }
}

/// Adds a fragment record to a fragment list for a compressed fragment. If possible, the new
/// fragment is coalesced with the last record.
fn add_fragment_compressed(
    fragments: &mut Vec<Fragment>,
    new_fragment_size: u32,
    new_chunk: u32,
    new_offset_within_chunk: u32,
) {
    debug!(
        new_fragment_size,
        new_chunk, new_offset_within_chunk, "add_fragment_compressed"
    );

    // Either create a new fragment for this write or coalesce it with the previous fragment.
    match fragments.last_mut() {
        Some(Fragment {
            size: last_fragment_size,
            location:
                FragmentLocation::Compressed {
                    chunk_index: last_chunk,
                    offset_within_chunk: last_offset_within_chunk,
                },
        }) if *last_chunk == new_chunk
            && *last_offset_within_chunk + new_fragment_size == new_offset_within_chunk =>
        {
            *last_fragment_size += new_fragment_size;
        }

        _ => {
            // We cannot extend the last fragment, or there is no last fragment.
            fragments.push(Fragment {
                size: new_fragment_size,
                location: FragmentLocation::Compressed {
                    chunk_index: new_chunk,
                    offset_within_chunk: new_offset_within_chunk,
                },
            });
        }
    }
}

/// Adds a fragment record to a fragment list for an uncompressed fragment. If possible, the new
/// fragment is coalesced with the last record.
fn add_fragment_uncompressed(
    fragments: &mut Vec<Fragment>,
    new_fragment_size: u32,
    new_file_offset: u64,
) {
    debug!(
        new_fragment_size,
        new_file_offset, "add_fragment_uncompressed"
    );

    match fragments.last_mut() {
        Some(Fragment {
            size: last_fragment_size,
            location:
                FragmentLocation::Uncompressed {
                    file_offset: last_fragment_file_offset,
                },
        }) if *last_fragment_file_offset + new_fragment_size as u64 == new_file_offset => {
            *last_fragment_size += new_fragment_size;
        }

        _ => {
            // We cannot extend the last fragment, or there is no last fragment.
            fragments.push(Fragment {
                size: new_fragment_size,
                location: FragmentLocation::Uncompressed {
                    file_offset: new_file_offset,
                },
            });
        }
    }
}

/// Encodes a stream directory to its byte representation.
pub(crate) fn encode_stream_dir(streams: &[Option<Stream>]) -> Vec<u8> {
    let _span = trace_span!("encode_stream_dir").entered();

    let mut stream_dir_encoded: Vec<u8> = Vec::new();
    let mut enc = Encoder {
        vec: &mut stream_dir_encoded,
    };

    for stream_opt in streams.iter() {
        if let Some(stream) = stream_opt {
            for fragment in stream.fragments.iter() {
                assert_ne!(fragment.size, 0);
                assert_ne!(fragment.size, NIL_STREAM_SIZE);
                enc.u32(fragment.size);

                let location: u64 = match fragment.location {
                    FragmentLocation::Compressed {
                        chunk_index,
                        offset_within_chunk,
                    } => {
                        ((chunk_index as u64) << 32)
                            | (offset_within_chunk as u64)
                            | FRAGMENT_LOCATION_CHUNK_MASK
                    }
                    FragmentLocation::Uncompressed { file_offset } => file_offset,
                };

                enc.u64(location)
            }

            // Write 0 to the list to terminate the list of fragments.
            enc.u32(0);
        } else {
            // It's a nil stream. Our encoding writes a single NIL_STREAM_SIZE value to the
            // stream directory. It is _not_ followed by a fragment list.
            enc.u32(NIL_STREAM_SIZE);
        }
    }

    stream_dir_encoded.as_bytes().to_vec()
}

struct Encoder<'a> {
    vec: &'a mut Vec<u8>,
}

impl<'a> Encoder<'a> {
    fn u32(&mut self, value: u32) {
        self.vec.extend_from_slice(&value.to_le_bytes());
    }
    fn u64(&mut self, value: u64) {
        self.vec.extend_from_slice(&value.to_le_bytes());
    }
}

```

`pdb/Cargo.toml`:

```toml
[package]
name = "ms-pdb"
version = "0.1.0"
edition = "2021"
description = "Reads Microsoft Program Database (PDB) files"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[lib]
doctest = false

[dependencies]
anyhow.workspace = true
bitfield.workspace = true
bitflags.workspace = true
bitvec.workspace = true
pow2.workspace = true
bstr.workspace = true
pretty-hex.workspace = true
static_assertions.workspace = true
sync_file.workspace = true
tracing.workspace = true
uuid.workspace = true
zerocopy = { workspace = true, features = ["alloc", "derive"] }
zerocopy-derive.workspace = true

[dependencies.ms-pdb-msf]
version = "0.1.0"
path = "../msf"

[dependencies.ms-pdb-msfz]
version = "0.1.0"
path = "../msfz"

[dev-dependencies]
static_init.workspace = true
tracing-subscriber.workspace = true

```

`pdb/src/container.rs`:

```rs
//! Provides an abstraction over MSF and MSFZ files.

use super::*;
use std::io::{Read, Seek, SeekFrom};

/// An abstraction over MSF and MSFZ files. Both types of files contain a set of streams.
pub enum Container<F> {
    /// The underlying file is an MSF file.
    Msf(msf::Msf<F>),
    /// The underlying file is an MSFZ file.
    Msfz(msfz::Msfz<F>),
}

impl<F: ReadAt> Container<F> {
    /// Provides direct access to the MSF layer. If this PDB file is using MSFZ instead of MSF,
    /// then this function returns `None`.
    pub fn msf(&self) -> Option<&msf::Msf<F>> {
        match self {
            Container::Msf(msf) => Some(msf),
            _ => None,
        }
    }

    /// Provides direct, mutable access to the MSF layer. If this PDB file is using MSFZ instead of
    /// MSF, then this function returns `None`.
    pub fn msf_mut(&mut self) -> Option<&mut msf::Msf<F>> {
        match self {
            Container::Msf(msf) => Some(msf),
            _ => None,
        }
    }

    /// Provides direct, mutable access to the MSF layer. If this PDB file is using MSFZ instead of
    /// MSF, then this function returns `None`.
    pub fn msf_mut_err(&mut self) -> anyhow::Result<&mut msf::Msf<F>> {
        match self {
            Container::Msf(msf) => Ok(msf),
            _ => bail!("This operation requires a PDB/MSF file. It cannot use a PDB/MSFZ file."),
        }
    }

    /// The total number of streams in this PDB.
    ///
    /// Some streams may be NIL.
    pub fn num_streams(&self) -> u32 {
        match self {
            Self::Msf(m) => m.num_streams(),
            Self::Msfz(m) => m.num_streams(),
        }
    }

    /// Returns an object which can read from a given stream.  The returned object implements
    /// the [`Read`], [`Seek`], and [`ReadAt`] traits.
    pub fn get_stream_reader(&self, stream: u32) -> anyhow::Result<StreamReader<'_, F>> {
        match self {
            Self::Msf(m) => Ok(StreamReader::Msf(m.get_stream_reader(stream)?)),
            Self::Msfz(m) => Ok(StreamReader::Msfz(m.get_stream_reader(stream)?)),
        }
    }

    /// Reads an entire stream to a vector.
    pub fn read_stream_to_vec(&self, stream: u32) -> anyhow::Result<Vec<u8>> {
        match self {
            Self::Msf(m) => m.read_stream_to_vec(stream),
            Self::Msfz(m) => Ok(m.read_stream(stream)?.into_vec()),
        }
    }

    /// Reads an entire stream to a vector.
    ///
    /// If the stream data is stored within a single compressed chunk, then this function returns
    /// a reference to the decompressed stream data.
    pub fn read_stream(&self, stream: u32) -> anyhow::Result<StreamData> {
        match self {
            Self::Msf(m) => Ok(StreamData::Box(m.read_stream_to_box(stream)?)),
            Self::Msfz(m) => m.read_stream(stream),
        }
    }

    /// Reads an entire stream into an existing vector.
    pub fn read_stream_to_vec_mut(
        &self,
        stream: u32,
        stream_data: &mut Vec<u8>,
    ) -> anyhow::Result<()> {
        match self {
            Self::Msf(m) => m.read_stream_to_vec_mut(stream, stream_data),
            Self::Msfz(m) => {
                let src = m.read_stream(stream)?;
                stream_data.clear();
                stream_data.extend_from_slice(&src);
                Ok(())
            }
        }
    }

    /// Gets the length of a given stream, in bytes.
    ///
    /// The `stream` value must be in a valid range of `0..num_streams()`.
    ///
    /// If `stream` is a NIL stream, this function returns 0.
    pub fn stream_len(&self, stream: u32) -> u64 {
        match self {
            Self::Msf(m) => m.stream_size(stream) as u64,
            Self::Msfz(m) => m.stream_size(stream),
        }
    }

    /// Indicates that a stream index is valid, and that its length is valid.
    pub fn is_stream_valid(&self, stream: u32) -> bool {
        match self {
            Self::Msf(m) => m.is_stream_valid(stream),
            Self::Msfz(m) => m.is_stream_valid(stream),
        }
    }
}

/// Allows reading a stream using the [`Read`], [`Seek`], and [`ReadAt`] traits.
pub enum StreamReader<'a, F> {
    /// A stream stored within an MSF file.
    Msf(msf::StreamReader<'a, F>),
    /// A stream stored within an MSFZ file.
    Msfz(msfz::StreamReader<'a, F>),
}

impl<'a, F: ReadAt> StreamReader<'a, F> {
    /// Tests whether this stream is empty (zero-length)
    pub fn is_empty(&self) -> bool {
        match self {
            Self::Msf(s) => s.is_empty(),
            Self::Msfz(s) => s.is_empty(),
        }
    }
}

impl<'a, F: ReadAt> ReadAt for StreamReader<'a, F> {
    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        match self {
            Self::Msf(s) => s.read_at(buf, offset),
            Self::Msfz(s) => s.read_at(buf, offset),
        }
    }

    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        match self {
            Self::Msf(s) => s.read_exact_at(buf, offset),
            Self::Msfz(s) => s.read_exact_at(buf, offset),
        }
    }
}

impl<'a, F: ReadAt> Read for StreamReader<'a, F> {
    fn read(&mut self, buf: &mut [u8]) -> std::io::Result<usize> {
        match self {
            Self::Msf(s) => s.read(buf),
            Self::Msfz(s) => s.read(buf),
        }
    }

    fn read_exact(&mut self, buf: &mut [u8]) -> std::io::Result<()> {
        match self {
            Self::Msf(s) => s.read_exact(buf),
            Self::Msfz(s) => s.read_exact(buf),
        }
    }
}

impl<'a, F: ReadAt> Seek for StreamReader<'a, F> {
    fn seek(&mut self, pos: SeekFrom) -> std::io::Result<u64> {
        match self {
            Self::Msf(s) => s.seek(pos),
            Self::Msfz(s) => s.seek(pos),
        }
    }
}

```

`pdb/src/dbi.rs`:

```rs
//! Provides access to the DBI Stream (Debug Information).
//!
//! The DBI Stream is a central data structure of the PDB. It contains many vital fields, and
//! points to other streams that contain other important information. The DBI is stream 3.
//!
//! Briefly, the DBI contains these substreams:
//!
//! * Modules: This lists the modules (compilands / translation units) that compose an executable.
//!   Each Module Info structure contains many important fields, including the stream number for
//!   a Module Stream.
//!
//! * Section Contributions Substream
//!
//! * Section Map Substream
//!
//! * Sources Substream: This lists the source files that were inputs to all of the translation units.
//!
//! * Type Server Map Substream
//!
//! * Optional Debug Header Substream
//!
//! * Edit-and-Continue Substream
//!
//! The `Dbi` stream holds section contributions and the list of modules (compilands).
//!
//! * <https://llvm.org/docs/PDB/DbiStream.html>
//! * <https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/langapi/include/pdb.h#L860>

use crate::parser::{Parser, ParserError, ParserMut};
use crate::Container;
use crate::{get_or_init_err, Stream};
use crate::{StreamIndexIsNilError, StreamIndexU16};
use anyhow::{bail, Result};
use std::mem::size_of;
use std::ops::Range;
use sync_file::ReadAt;
use tracing::{error, warn};
use zerocopy::{
    FromBytes, FromZeros, Immutable, IntoBytes, KnownLayout, Unaligned, I32, LE, U16, U32,
};

#[cfg(doc)]
use crate::Pdb;

pub mod modules;
pub mod optional_dbg;
pub mod section_contrib;
pub mod section_map;
pub mod sources;

pub use modules::*;
#[doc(inline)]
pub use section_contrib::*;
#[doc(inline)]
pub use sources::*;

/// The header of the DBI (Debug Information) stream.
#[repr(C)]
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug, Clone)]
#[allow(missing_docs)]
pub struct DbiStreamHeader {
    /// Always -1
    pub signature: I32<LE>,

    /// One of the `DBI_STREAM_VERSION_*` values; typically, `DBI_STREAM_VERSION_V110`.
    pub version: U32<LE>,

    /// The number of times this PDB has been modified. The value is set to 1 when a PDB is
    /// first created. This value must match the same field within the PE header.
    pub age: U32<LE>,

    /// The index of the Global Symbol Index, which contains a name-to-symbol lookup table for
    /// global symbols. The symbol records are not stored in this stream; they are stored in the
    /// Global Symbol Stream.
    pub global_symbol_index_stream: StreamIndexU16,

    pub build_number: U16<LE>,

    /// The index of the stream that contains the Public Symbol Index (GSI). This contains a
    /// name-to-symbol map and an address-to-symbol map. See [`crate::globals::gsi`].
    pub public_symbol_index_stream: StreamIndexU16,

    /// The version of the MSPDB DLL which produced this DBI stream.
    pub pdb_dll_version: U16<LE>,

    /// The stream that contains the Global Symbol Stream. This contains symbol records, which can
    /// be decoded using [`crate::syms::SymIter`].
    pub global_symbol_stream: StreamIndexU16,

    pub pdb_dll_rbld: U16<LE>,

    // Substreams
    pub mod_info_size: I32<LE>,
    pub section_contribution_size: I32<LE>,
    pub section_map_size: I32<LE>,
    pub source_info_size: I32<LE>,
    pub type_server_map_size: I32<LE>,
    /// This field is _not_ a substream size. Not sure what it is.
    pub mfc_type_server_index: U32<LE>,
    pub optional_dbg_header_size: I32<LE>,
    pub edit_and_continue_size: I32<LE>,

    pub flags: U16<LE>,
    pub machine: U16<LE>,
    pub padding: U32<LE>,
}

/// Data for an empty DBI stream
pub static EMPTY_DBI_STREAM_HEADER: [u8; DBI_STREAM_HEADER_LEN] = [
    0xFF, 0xFF, 0xFF, 0xFF, // signature
    0x77, 0x09, 0x31, 0x01, // version
    0x01, 0x00, 0x00, 0x00, // age
    0xFF, 0xFF, // global_stream_index
    0x00, 0x00, // build_number
    0xFF, 0xFF, // public_stream_index
    0x00, 0x00, // pdb_dll_version
    0xFF, 0xFF, // sym_record_stream
    0x00, 0x00, // pdb_dll_rbld
    0x00, 0x00, 0x00, 0x00, // mod_info_size
    0x00, 0x00, 0x00, 0x00, // section_contribution_size
    0x00, 0x00, 0x00, 0x00, // section_map_size
    0x00, 0x00, 0x00, 0x00, // source_info_size
    0x00, 0x00, 0x00, 0x00, // type_server_map_size
    0x00, 0x00, 0x00, 0x00, // mfc_type_server_index
    0x00, 0x00, 0x00, 0x00, // optional_dbg_header_size
    0x00, 0x00, 0x00, 0x00, // edit_and_continue_size
    0x00, 0x00, // flags
    0x00, 0x00, // machine
    0x00, 0x00, 0x00, 0x00, // padding
];

#[test]
fn test_parse_empty_dbi_stream_header() {
    let h = DbiStreamHeader::read_from_bytes(EMPTY_DBI_STREAM_HEADER.as_slice()).unwrap();
    assert!(h.global_symbol_index_stream.get().is_none());
}

impl DbiStreamHeader {
    /// Gets the stream index for the Global Symbol Stream.
    pub fn sym_record_stream(&self) -> Result<u32, StreamIndexIsNilError> {
        self.global_symbol_stream.get_err()
    }

    /// Gets the stream index for the Public Symbol Index.
    pub fn public_stream_index(&self) -> Result<u32, StreamIndexIsNilError> {
        self.public_symbol_index_stream.get_err()
    }

    /// Gets the stream index for the Global Symbol Index.
    pub fn global_stream_index(&self) -> Result<u32, StreamIndexIsNilError> {
        self.global_symbol_index_stream.get_err()
    }

    /// Byte range of the Modules substream.
    pub fn modules_range(&self) -> anyhow::Result<Range<usize>> {
        let start = DBI_STREAM_HEADER_LEN;
        let size = self.mod_info_size.get() as usize;
        Ok(start..start + size)
    }

    /// Byte range of the Modules substream.
    pub fn sources_range(&self) -> anyhow::Result<Range<usize>> {
        let start = DBI_STREAM_HEADER_LEN
            + self.mod_info_size.get() as usize
            + self.section_contribution_size.get() as usize
            + self.section_map_size.get() as usize;
        let size = self.source_info_size.get() as usize;
        Ok(start..start + size)
    }
}

static_assertions::const_assert_eq!(size_of::<DbiStreamHeader>(), DBI_STREAM_HEADER_LEN);
const DBI_STREAM_HEADER_LEN: usize = 64;

/// MSVC version 4.1
pub const DBI_STREAM_VERSION_VC41: u32 = 930803;
/// MSVC version 5.0
pub const DBI_STREAM_VERSION_V50: u32 = 19960307;
/// MSVC version 6.0
pub const DBI_STREAM_VERSION_V60: u32 = 19970606;
/// MSVC version 7.0
pub const DBI_STREAM_VERSION_V70: u32 = 19990903;
/// MSVC version 11.0
pub const DBI_STREAM_VERSION_V110: u32 = 20091201;

/// Holds or refers to the DBI stream.
///
/// The `StreamData` type parameter can be any type that can contain `[u8]`.
///
/// This type contains (or refers to) the _entire_ DBI stream, not just the header.
#[derive(Clone)]
pub struct DbiStream<StreamData = Vec<u8>>
where
    StreamData: AsRef<[u8]>,
{
    /// The contents of the stream.
    pub stream_data: StreamData,

    /// The byte ranges of the substreams.
    pub substreams: DbiSubstreamRanges,
}

// The DBI stream contains a fixed number of "substreams". The DBI header specifies the
// length of each substream.  The position of each substream is found by computing the
// sum of all previous substreams (and the header).
macro_rules! dbi_substreams {
    (
        $(
            $name:ident,
            $mut_name:ident,
            $size_field:ident ;
        )*
    ) => {
        /// Contains the byte ranges of the substreams within the DBI stream.
        #[derive(Clone, Debug, Default)]
        pub struct DbiSubstreamRanges {
            $(
                #[doc = concat!("The range of the ", stringify!($name), " substream.")]
                pub $name: Range<usize>,
            )*
        }

        impl<StreamData: AsRef<[u8]>> DbiStream<StreamData> {
            $(
                #[doc = concat!("The unparsed contents of the ", stringify!($name), " substream.")]
                pub fn $name(&self) -> &[u8] {
                    self.substream_data(self.substreams.$name.clone())
                }

                #[doc = concat!("The unparsed contents of the ", stringify!($name), " substream.")]
                pub fn $mut_name(&mut self) -> &mut [u8]
                where
                    StreamData: AsMut<[u8]>,
                {
                    self.substream_data_mut(self.substreams.$name.clone())
                }

            )*
        }

        impl DbiSubstreamRanges {
            pub(crate) fn from_sizes(sizes: &DbiStreamHeader, stream_len: usize) -> anyhow::Result<Self> {
                let mut pos: usize = DBI_STREAM_HEADER_LEN;
                if pos > stream_len {
                    bail!("DBI stream is too short; pos = {}, stream_len = {}", pos, stream_len);
                }

                $(
                    assert!(pos <= stream_len);
                    let size: i32 = sizes.$size_field.get();
                    if size < 0 {
                        bail!("Substream {} length in DBI header is invalid (is negative)", stringify!($size_field));
                    }

                    let len = size as usize;
                    let available = stream_len - pos;
                    if len > available {
                        bail!("Substream {} length in DBI header is invalid. It extends beyond the end of the stream.", stringify!($size_field));
                    }
                    let start = pos;
                    pos += len;

                    let $name = start..pos;
                )*

                if pos < stream_len {
                    warn!(pos, stream_len, "Something is wrong with the code that finds the ranges of substreams. Expected pos to be equal to stream_len.");
                } else if pos > stream_len {
                    error!(pos, stream_len, "Something is very wrong with the DBI header. The sum of the subtream lengths (pos) exceeds the stream len.");
                } else {
                    // Substream sizes look good.
                }

                Ok(Self {
                    $( $name, )*
                })
            }
        }
    }
}

dbi_substreams! {
    // The order of these determines the order of the substream data in the stream.
    modules_bytes, modules_bytes_mut, mod_info_size;
    section_contributions_bytes, section_contributions_bytes_mut, section_contribution_size;
    section_map_bytes, section_map_bytes_mut, section_map_size;
    source_info, source_info_mut, source_info_size;
    type_server_map, type_server_map_mut, type_server_map_size;
    edit_and_continue, edit_and_continue_mut, edit_and_continue_size;
    optional_debug_header_bytes, optional_debug_header_bytes_mut, optional_dbg_header_size;
}

impl<StreamData: AsRef<[u8]>> DbiStream<StreamData> {
    /// Returns the DBI stream header.
    pub fn header(&self) -> Result<&DbiStreamHeader> {
        if let Ok((header, _)) = DbiStreamHeader::ref_from_prefix(self.stream_data.as_ref()) {
            Ok(header)
        } else {
            bail!("The DBI stream is too small to contain a valid header.")
        }
    }

    /// Provides mutable access to the DBI stream header.
    pub fn header_mut(&mut self) -> Result<&mut DbiStreamHeader>
    where
        StreamData: AsMut<[u8]>,
    {
        if let Ok((header, _)) = DbiStreamHeader::mut_from_prefix(self.stream_data.as_mut()) {
            Ok(header)
        } else {
            bail!("The DBI stream is too small to contain a valid header.")
        }
    }

    fn substream_data(&self, range: Range<usize>) -> &[u8] {
        &self.stream_data.as_ref()[range]
    }

    fn substream_data_mut(&mut self, range: Range<usize>) -> &mut [u8]
    where
        StreamData: AsMut<[u8]>,
    {
        &mut self.stream_data.as_mut()[range]
    }

    /// Reads the Module Information substream.
    pub fn modules(&self) -> ModInfoSubstream<&[u8]> {
        ModInfoSubstream {
            substream_data: self.modules_bytes(),
        }
    }

    /// Iterates the Module records in the Module Information Substream.
    pub fn iter_modules(&self) -> IterModuleInfo<'_> {
        IterModuleInfo::new(self.modules_bytes())
    }

    /// Iterates the Module records in the Module Information Substream, with mutable access.
    pub fn iter_modules_mut(&mut self) -> IterModuleInfoMut<'_>
    where
        StreamData: AsMut<[u8]>,
    {
        IterModuleInfoMut::new(self.modules_bytes_mut())
    }

    /// Return a DbiStream over just a a reference
    pub fn as_slice(&self) -> DbiStream<&[u8]> {
        DbiStream {
            stream_data: self.stream_data.as_ref(),
            substreams: self.substreams.clone(),
        }
    }

    /// Read the DBI Stream header and validate it.
    pub fn parse(stream_data: StreamData) -> anyhow::Result<Self> {
        let stream_bytes: &[u8] = stream_data.as_ref();

        if stream_bytes.is_empty() {
            return Ok(Self {
                substreams: Default::default(),
                stream_data,
            });
        }

        let mut p = Parser::new(stream_bytes);
        let dbi_header: &DbiStreamHeader = p.get()?;

        let substreams = DbiSubstreamRanges::from_sizes(dbi_header, stream_bytes.len())?;

        // We just computed the ranges for each of the substreams, and we verified that the end of
        // the substreams is equal to the size of the entire stream. That implicitly validates all
        // of the range checks for the substreams, so we don't need explicit / verbose checks.
        // We can simply use normal range indexing.

        Ok(Self {
            stream_data,
            substreams,
        })
    }

    /// Parses the DBI Sources Substream section.
    pub fn sources(&self) -> anyhow::Result<sources::DbiSourcesSubstream<'_>> {
        DbiSourcesSubstream::parse(self.source_info())
    }

    /// Parses the header of the Section Contributions Substream and returns an object which can
    /// query it.
    pub fn section_contributions(
        &self,
    ) -> anyhow::Result<section_contrib::SectionContributionsSubstream<'_>> {
        let substream_bytes = self.section_contributions_bytes();
        section_contrib::SectionContributionsSubstream::parse(substream_bytes)
    }

    /// Parses the header of the Section Map Substream and returns an object which can query it.
    pub fn section_map(&self) -> anyhow::Result<section_map::SectionMap<'_>> {
        let section_map_bytes = self.section_map_bytes();
        section_map::SectionMap::parse(section_map_bytes)
    }

    /// Parses the Optional Debug Header Substream and returns an object which can query it.
    pub fn optional_debug_header(&self) -> anyhow::Result<optional_dbg::OptionalDebugHeader> {
        optional_dbg::OptionalDebugHeader::parse(self.optional_debug_header_bytes())
    }

    /// Gets a mutable reference to the Optional Debug Header substream.
    pub fn optional_debug_header_mut(&mut self) -> anyhow::Result<&mut [U16<LE>]>
    where
        StreamData: AsMut<[u8]>,
    {
        if self.substreams.optional_debug_header_bytes.is_empty() {
            Ok(&mut [])
        } else {
            let substream_bytes =
                &mut self.stream_data.as_mut()[self.substreams.optional_debug_header_bytes.clone()];

            if let Ok(slice) = <[U16<LE>]>::mut_from_bytes(substream_bytes) {
                Ok(slice)
            } else {
                bail!("The Optional Debug Header substream within the DBI stream is malformed (length is not valid).");
            }
        }
    }
}

/// Reads the header of the DBI stream. This does **not** validate the header.
///
/// This is a free function because we need to use it before constructing an instance of [`Pdb`].
pub fn read_dbi_stream_header<F: ReadAt>(msf: &Container<F>) -> anyhow::Result<DbiStreamHeader> {
    let stream_reader = msf.get_stream_reader(Stream::DBI.into())?;
    if !stream_reader.is_empty() {
        let mut dbi_header = DbiStreamHeader::new_zeroed();
        stream_reader.read_exact_at(dbi_header.as_mut_bytes(), 0)?;
        Ok(dbi_header)
    } else {
        Ok(DbiStreamHeader::read_from_bytes(EMPTY_DBI_STREAM_HEADER.as_slice()).unwrap())
    }
}

/// Reads the entire DBI Stream, validates the header, and then returns an object that
/// can be used for further queries of the DBI Stream.
///
/// This is a free function because we need to use it before constructing an instance of [`Pdb`].
pub fn read_dbi_stream<F: ReadAt>(
    container: &Container<F>,
) -> Result<DbiStream<Vec<u8>>, anyhow::Error> {
    let mut dbi_stream_data = container.read_stream_to_vec(Stream::DBI.into())?;
    if dbi_stream_data.is_empty() {
        dbi_stream_data = EMPTY_DBI_STREAM_HEADER.to_vec();
    }

    DbiStream::parse(dbi_stream_data)
}

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads the header of the DBI stream. This does **not** validate the header.
    pub fn read_dbi_stream_header(&self) -> anyhow::Result<DbiStreamHeader> {
        read_dbi_stream_header(&self.container)
    }

    /// Reads the entire DBI Stream, validates the header, and then returns an object that
    /// can be used for further queries of the DBI Stream.
    pub fn read_dbi_stream(&self) -> Result<DbiStream<Vec<u8>>, anyhow::Error> {
        read_dbi_stream(&self.container)
    }

    fn read_dbi_substream(&self, range: Range<usize>) -> anyhow::Result<Vec<u8>> {
        let len = range.len();
        let mut substream_data = vec![0; len];
        let reader = self.container.get_stream_reader(Stream::DBI.into())?;
        reader.read_exact_at(&mut substream_data, range.start as u64)?;
        Ok(substream_data)
    }

    /// Reads the module substream data from the DBI stream.
    ///
    /// This function always reads the data from the file. It does not cache the data.
    pub fn read_modules(&self) -> anyhow::Result<ModInfoSubstream<Vec<u8>>> {
        let substream_data = self.read_dbi_substream(self.dbi_substreams.modules_bytes.clone())?;
        Ok(ModInfoSubstream { substream_data })
    }

    /// Gets access to the DBI Modules Substream. This will read the DBI Modules Substream
    /// on-demand, and will cache it.
    pub fn modules(&self) -> anyhow::Result<&ModInfoSubstream<Vec<u8>>> {
        get_or_init_err(&self.dbi_modules_cell, || self.read_modules())
    }

    /// Reads the DBI Sources Substream. This always reads the data, and does not cache it.
    pub fn read_sources_data(&self) -> Result<Vec<u8>> {
        self.read_dbi_substream(self.dbi_substreams.source_info.clone())
    }

    /// Gets access to the DBI Sources Substream data.
    pub fn sources_data(&self) -> Result<&[u8]> {
        let sources_data = get_or_init_err(&self.dbi_sources_cell, || self.read_sources_data())?;
        Ok(sources_data)
    }

    /// Gets access to the DBI Sources Substream and parses the header.
    pub fn sources(&self) -> Result<sources::DbiSourcesSubstream<'_>> {
        let sources_data = self.sources_data()?;
        sources::DbiSourcesSubstream::parse(sources_data)
    }

    /// Drops the cached DBI Sources Substream data, if any.
    pub fn drop_sources(&mut self) {
        self.dbi_sources_cell = Default::default();
    }

    /// Reads the contents of the DBI Section Contributions Substream. This function never caches
    /// the data; it is always read unconditionally.
    pub fn read_section_contributions(&self) -> Result<Vec<u8>> {
        self.read_dbi_substream(self.dbi_substreams.section_contributions_bytes.clone())
    }
}

/// Reads fields of the DBI Stream and validates them for consistency with the specification.
pub fn validate_dbi_stream(stream_data: &[u8]) -> anyhow::Result<()> {
    let dbi_stream = DbiStream::parse(stream_data)?;

    // For now, the only validation that we do in this function is decoding the ModuleInfo records.
    let num_modules: usize = dbi_stream.modules().iter().count();

    let sources = DbiSourcesSubstream::parse(dbi_stream.source_info())?;
    if sources.num_modules() != num_modules {
        bail!("Number of modules found in Sources substream ({}) does not match number of Module Info structs found in Modules substream ({}).",
            sources.num_modules(),
            num_modules
        );
    }

    Ok(())
}

```

`pdb/src/dbi/modules.rs`:

```rs
//! DBI Modules Substream

use super::*;
use crate::utils::iter::HasRestLen;
use crate::StreamIndexU16;
use bstr::BStr;
use std::mem::take;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

/// The header of a Module Info record. Module Info records are stored in the DBI stream.
///
/// See `dbi.h`, `MODI_60_Persist`
#[derive(Unaligned, IntoBytes, FromBytes, Immutable, KnownLayout, Clone, Debug)]
#[repr(C)]
pub struct ModuleInfoFixed {
    /// This appears to be a module index field, but it is not always set.
    ///
    /// In some PDBs, we see this field being set to the zero-based index of this Module Info record
    /// in the DBI Modules Substream.  In other PDBs, this value is 0.  Set this to 0.
    pub unused1: U32<LE>,

    /// This module's first section contribution.
    pub section_contrib: SectionContribEntry,

    /// Various flags
    ///
    /// * bit 0: set to 1 if this module has been written since DBI opened
    /// * bit 1: set to 1 if this module has EC symbolic information
    /// * bits 2-7: not used
    /// * bits 8-15: index into TSM list for this mods server
    pub flags: U16<LE>,

    /// Stream index of the Module Stream for this module, which contains the symbols and line data
    /// for this module. If this is 0xffff, then this module does not have a module info stream.
    pub stream: StreamIndexU16,

    /// Specifies the size of the symbols substream within the Module Stream.
    pub sym_byte_size: U32<LE>,

    /// Specifies the length of the C11 Line Data in a Module Information Stream.
    /// C11 line data is obsolete and is not supported.
    pub c11_byte_size: U32<LE>,

    /// Specifies the length of the C13 Line Data in a Module Information Stream.
    pub c13_byte_size: U32<LE>,

    /// Number of files contributing to this module.
    pub source_file_count: U16<LE>,

    /// Alignment padding.
    pub padding: [u8; 2],

    /// Do not read. Set to 0 when encoding.
    pub unused2: U32<LE>,

    /// Unknown; possible that this relates to Edit-and-Continue.
    pub source_file_name_index: U32<LE>,

    /// Unknown; possible that this relates to Edit-and-Continue.
    pub pdb_file_path_name_index: U32<LE>,
}

impl ModuleInfoFixed {
    /// Gets the stream for this module, if any. This stream contains the symbol data and C13 Line
    /// Data for the module.
    pub fn stream(&self) -> Option<u32> {
        self.stream.get()
    }
}

/// Holds or refers to the data of a substream within a Module Info record.
#[derive(Clone)]
pub struct ModInfoSubstream<D: AsRef<[u8]>> {
    /// The substream data.
    pub substream_data: D,
}

impl<D: AsRef<[u8]>> ModInfoSubstream<D> {
    /// Iterates the Module Info records contained within the DBI Stream.
    pub fn iter(&self) -> IterModuleInfo<'_> {
        IterModuleInfo {
            rest: self.substream_data.as_ref(),
        }
    }
}

/// An in-memory representation of a Module Info record.
///
/// The `IterModInfo` iterator produces these items.
#[allow(missing_docs)]
pub struct ModuleInfo<'a> {
    pub header: &'a ModuleInfoFixed,
    pub module_name: &'a BStr,
    pub obj_file: &'a BStr,
}

/// A mutable view of a Module Info record.
#[allow(missing_docs)]
pub struct ModuleInfoMut<'a> {
    pub header: &'a mut ModuleInfoFixed,
    pub module_name: &'a BStr,
    pub obj_file: &'a BStr,
}

impl<'a> ModuleInfo<'a> {
    /// The name of the module.
    ///
    /// * For simple object files, this is the same as `file_name()`.
    /// * For DLL import libraries, this is the name of the DLL, e.g. `KernelBase.dll`.
    /// * For static libraries, this is the name (and possibly path) of the object file within the
    ///   static library, not the static library itself.
    pub fn module_name(&self) -> &'a BStr {
        self.module_name
    }

    /// The file name of this module.
    ///
    /// * For individual `*.obj` files that are passed directly to the linker (not in a static
    ///   library), this is the filename.
    /// * For static libraries, this is the `*.lib` file, not the modules within it.
    /// * For DLL import libraries, this is the import library, e.g. `KernelBase.lib`.
    pub fn obj_file(&self) -> &'a BStr {
        self.obj_file
    }

    /// The header of this Module Info record.
    pub fn header(&self) -> &'a ModuleInfoFixed {
        self.header
    }

    /// The stream index of the stream which contains the symbols defined by this module.
    ///
    /// Some modules do not have a symbol stream. In that case, this function will return `None`.
    pub fn stream(&self) -> Option<u32> {
        self.header.stream()
    }

    /// Gets the size in bytes of the C11 Line Data.
    pub fn c11_size(&self) -> u32 {
        self.header.c11_byte_size.get()
    }

    /// Gets the size in bytes of the C13 Line Data.
    pub fn c13_size(&self) -> u32 {
        self.header.c13_byte_size.get()
    }

    /// Gets the size in bytes of the symbol stream for this module. This value includes the size
    /// of the 4-byte symbol stream header.
    pub fn sym_size(&self) -> u32 {
        self.header.sym_byte_size.get()
    }
}

/// Iterates module info records
pub struct IterModuleInfo<'a> {
    rest: &'a [u8],
}

impl<'a> IterModuleInfo<'a> {
    #[allow(missing_docs)]
    pub fn new(data: &'a [u8]) -> Self {
        Self { rest: data }
    }

    /// Returns the data in the iterator that has not yet been parsed.
    pub fn rest(&self) -> &'a [u8] {
        self.rest
    }
}

impl<'a> HasRestLen for IterModuleInfo<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

impl<'a> Iterator for IterModuleInfo<'a> {
    type Item = ModuleInfo<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.rest);

        let len_before = p.len();
        let header: &ModuleInfoFixed = p.get().ok()?;
        let module_name = p.strz().ok()?;
        let obj_file = p.strz().ok()?;

        // Each ModInfo structures is variable-length. It ends with two NUL-terminated strings.
        // However, the ModInfo structures have an alignment requirement, so if the strings
        // did not land us on an aligned boundary, we have to skip a few bytes to restore
        // alignment.

        // Find the number of bytes that were used for this structure.
        let mod_record_bytes = len_before - p.len();
        let alignment = (4 - (mod_record_bytes & 3)) & 3;
        p.bytes(alignment).ok()?;

        // Save iterator position.
        self.rest = p.into_rest();

        Some(ModuleInfo {
            header,
            module_name,
            obj_file,
        })
    }
}

/// Mutable iterator
pub struct IterModuleInfoMut<'a> {
    rest: &'a mut [u8],
}

impl<'a> IterModuleInfoMut<'a> {
    #[allow(missing_docs)]
    pub fn new(data: &'a mut [u8]) -> Self {
        Self { rest: data }
    }
}

impl<'a> Iterator for IterModuleInfoMut<'a> {
    type Item = ModuleInfoMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        // TODO: note that we steal the byte slice, which means that
        // if anything goes wrong, we'll never put it back.
        let mut p = ParserMut::new(take(&mut self.rest));

        let len_before = p.len();
        let header: &mut ModuleInfoFixed = p.get_mut().ok()?;
        let module_name = p.strz().ok()?;
        let obj_file = p.strz().ok()?;

        // Each ModInfo structures is variable-length. It ends with two NUL-terminated strings.
        // However, the ModInfo structures have an alignment requirement, so if the strings
        // did not land us on an aligned boundary, we have to skip a few bytes to restore
        // alignment.

        // Find the number of bytes that were used for this structure.
        let mod_record_bytes = len_before - p.len();
        let alignment = (4 - (mod_record_bytes & 3)) & 3;
        p.bytes(alignment).ok()?;

        // Save iterator position.
        self.rest = p.into_rest();
        Some(ModuleInfoMut {
            header,
            module_name,
            obj_file,
        })
    }
}

```

`pdb/src/dbi/optional_dbg.rs`:

```rs
//! Decodes the Optional Debug Header Substream.
//!
//! This substream contains an array of stream indexes. The order of the array is significant;
//! each has a specific purpose. They are enumerated by the [`OptionalDebugHeaderStream`] type.
//!
//! # References
//! * <https://llvm.org/docs/PDB/DbiStream.html#id10>
//! * [`DBGTYPE` in `pdb.h`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/langapi/include/pdb.h#L438)

use super::*;

/// Provides access to the Optional Debug Header.
pub struct OptionalDebugHeader<'a> {
    /// Raw access to the stream indexes
    pub stream_indexes: &'a [StreamIndexU16],
}

impl<'a> OptionalDebugHeader<'a> {
    /// Parses the Optional Debug Header Substream.
    pub fn parse(bytes: &'a [u8]) -> anyhow::Result<Self> {
        let Ok(stream_indexes) = <[StreamIndexU16]>::ref_from_bytes(bytes) else {
            bail!("The OptionalDebugHeader has an invalid size. The size is required to be a multiple of 2. Size: {}",
                bytes.len());
        };

        Ok(Self { stream_indexes })
    }

    /// Gets a stream index, given an index into the Optional Debug Header.
    pub fn stream_by_index(&self, i: usize) -> Option<u32> {
        self.stream_indexes.get(i)?.get()
    }

    /// Gets a stream index, given an identifier for a stream within the Optional Debug Header.
    pub fn stream(&self, s: OptionalDebugHeaderStream) -> Option<u32> {
        self.stream_by_index(s as usize)
    }

    /// The number of stream indexes in the Optional Debug Header Substream.
    pub fn num_streams(&self) -> usize {
        self.stream_indexes.len()
    }

    /// Iterates the streams within the Optional Debug Header. The iterated values are
    /// `(i, stream)` where `i` is an index into the Optional Debug Header.
    /// `OptionalDebugHeaderStream::try_from(i)`.
    pub fn iter_streams(&self) -> IterStreams<'_> {
        IterStreams {
            stream_indexes: self.stream_indexes,
            next: 0,
        }
    }
}

/// Iterates streams
pub struct IterStreams<'a> {
    stream_indexes: &'a [StreamIndexU16],
    next: usize,
}

impl<'a> Iterator for IterStreams<'a> {
    type Item = (usize, u32);

    fn next(&mut self) -> Option<Self::Item> {
        while self.next < self.stream_indexes.len() {
            let i = self.next;
            let stream_index_or_nil = self.stream_indexes[i].get();
            self.next += 1;

            if let Some(stream_index) = stream_index_or_nil {
                return Some((i, stream_index));
            }
        }
        None
    }
}

macro_rules! optional_debug_header_streams {
    (
        $(
            $( #[$a:meta] )*
            $index:literal, $name:ident, $description:expr;
        )*
    ) => {
        /// Identifies the stream indexes stored in the Optional Debug Header.
        #[derive(Copy, Clone, Eq, PartialEq, Debug)]
        #[repr(u8)]
        #[allow(non_camel_case_types)]
        #[allow(missing_docs)]
        pub enum OptionalDebugHeaderStream {
            $(
                $( #[$a] )*
                $name = $index,
            )*
        }

        /// The short name (identifier) for each of the names in `OptionalDebugHeaderStream`.
        pub static OPTIONAL_DEBUG_HEADER_STREAM_NAME: [&str; 11] = [
            $(
                stringify!($name),
            )*
        ];

        /// The for each of the names in `OptionalDebugHeaderStream`.
        pub static OPTIONAL_DEBUG_HEADER_STREAM_DESCRIPTION: [&str; 11] = [
            $(
                $description,
            )*
        ];

        impl TryFrom<usize> for OptionalDebugHeaderStream {
            type Error = ();

            fn try_from(i: usize) -> std::result::Result<Self, Self::Error> {
                match i {
                    $( $index => Ok(Self::$name), )*
                    _ => Err(()),
                }
            }
        }
    }
}

optional_debug_header_streams! {
    /// Stream contains an array of `FPO_DATA` structures. This contains the relocated contents of
    /// any `.debug$F` section from any of the linker inputs.
    0, fpo_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_EXCEPTION`.
    1, exception_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_FIXUP`.
    2, fixup_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_OMAP_TO_SRC`.
    /// This is used for mapping addresses from instrumented code to uninstrumented code.
    3, omap_to_src_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_OMAP_FROM_SRC`.
    /// This is used for mapping addresses from uninstrumented code to instrumented code.
    4, omap_from_src_data, "";
    /// A dump of all section headers from the original executable.
    5, section_header_data, "";
    6, token_to_record_id_map, "";
    /// Exception handler data
    7, xdata, "";
    /// Procedure data
    8, pdata, "";
    9, new_fpo_data, "";
    10, original_section_header_data, "";
}

```

`pdb/src/dbi/section_contrib.rs`:

```rs
//! DBI Section Contribution Substream
//!
//! The Section Contributions Substream describes the COFF sections that contributed to a linked
//! binary. Section contributions come from object files that are submitted to the linker.
//!
//! The Section Contributions table is usually quite large, especially for large binaries.
//!
//! # References
//! * [`SC2` in `dbicommon.h`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/include/dbicommon.h#L107)

use super::*;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

/// Describes one section contribution.
#[allow(missing_docs)]
#[derive(Unaligned, IntoBytes, FromBytes, Immutable, KnownLayout, Clone, Debug)]
#[repr(C)]
pub struct SectionContribEntry {
    /// The section index
    pub section: U16<LE>,
    /// Alignment padding
    pub padding1: [u8; 2],
    pub offset: I32<LE>,
    pub size: I32<LE>,
    pub characteristics: U32<LE>,
    /// The zero-based module index of the module containing this section contribution.
    pub module_index: U16<LE>,
    /// Alignment padding
    pub padding2: [u8; 2],
    pub data_crc: U32<LE>,
    pub reloc_crc: U32<LE>,
}

/// Describes one section contribution.
#[allow(missing_docs)]
#[derive(Unaligned, IntoBytes, FromBytes, Immutable, KnownLayout, Clone, Debug)]
#[repr(C)]
pub struct SectionContribEntry2 {
    pub base: SectionContribEntry,
    pub coff_section: U32<LE>,
}

impl SectionContribEntry {
    /// Tests whether `offset` falls within this section contribution.
    pub fn contains_offset(&self, offset: i32) -> bool {
        let self_offset = self.offset.get();
        if offset < self_offset {
            return false;
        }

        let overshoot = offset - self_offset;
        if overshoot >= self.size.get() {
            return false;
        }

        true
    }
}

/// Decodes the Section Contribution Substream.
pub struct SectionContributionsSubstream<'a> {
    /// The array of section contributions.
    pub contribs: &'a [SectionContribEntry],
}

/// Version 6.0 of the Section Contributions Substream. This is the only supported version.
pub const SECTION_CONTRIBUTIONS_SUBSTREAM_VER60: u32 = 0xeffe0000 + 19970605;

impl<'a> SectionContributionsSubstream<'a> {
    /// Parses the header of the Section Contributions Substream.
    ///
    /// It is legal for a Section Contributions Substream to be entirely empty.
    pub fn parse(bytes: &'a [u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(bytes);
        if p.is_empty() {
            return Ok(Self { contribs: &[] });
        }

        let version = p.u32()?;

        match version {
            SECTION_CONTRIBUTIONS_SUBSTREAM_VER60 => {}
            _ => {
                bail!("The Section Contributions Substream has a version number that is not supported. Version: 0x{:08x}", version);
            }
        }

        let records_bytes = p.into_rest();
        let Ok(contribs) = <[SectionContribEntry]>::ref_from_bytes(records_bytes) else {
            bail!("The Section Contributions stream has an invalid size. It is not a multiple of the section contribution record size.  Size: 0x{:x}",
                bytes.len());
        };
        Ok(SectionContributionsSubstream { contribs })
    }

    /// Searches for a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses binary search.
    pub fn find(&self, section: u16, offset: i32) -> Option<&SectionContribEntry> {
        let i = self.find_index(section, offset)?;
        Some(&self.contribs[i])
    }

    /// Searches for the index of a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses binary search.
    pub fn find_index(&self, section: u16, offset: i32) -> Option<usize> {
        match self
            .contribs
            .binary_search_by_key(&(section, offset), |con| {
                (con.section.get(), con.offset.get())
            }) {
            Ok(i) => Some(i),
            Err(i) => {
                // We didn't find it, but i is close to it.
                if i > 0 {
                    let previous = &self.contribs[i - 1];
                    if previous.contains_offset(offset) {
                        return Some(i - 1);
                    }
                }

                if i + 1 < self.contribs.len() {
                    let next = &self.contribs[i + 1];
                    if next.contains_offset(offset) {
                        return Some(i + 1);
                    }
                }

                None
            }
        }
    }

    /// Searches for a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses sequential scan (brute force).
    pub fn find_brute(&self, section: u16, offset: i32) -> Option<&SectionContribEntry> {
        let i = self.find_index_brute(section, offset)?;
        Some(&self.contribs[i])
    }

    /// Searches for the index of a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses sequential scan (brute force).
    pub fn find_index_brute(&self, section: u16, offset: i32) -> Option<usize> {
        self.contribs
            .iter()
            .position(|c| c.section.get() == section && c.contains_offset(offset))
    }
}

/// Decodes the Section Contribution Substream.
pub struct SectionContributionsSubstreamMut<'a> {
    /// The array of section contributions.
    pub contribs: &'a mut [SectionContribEntry],
}

impl<'a> SectionContributionsSubstreamMut<'a> {
    /// Parses the header of the Section Contributions Substream.
    pub fn parse(bytes: &'a mut [u8]) -> anyhow::Result<Self> {
        let bytes_len = bytes.len();

        let mut p = ParserMut::new(bytes);
        if p.is_empty() {
            return Ok(Self { contribs: &mut [] });
        }

        let version = p.u32()?;

        match version {
            SECTION_CONTRIBUTIONS_SUBSTREAM_VER60 => {}
            _ => {
                bail!("The Section Contributions Substream has a version number that is not supported. Version: 0x{:08x}", version);
            }
        }

        let records_bytes = p.into_rest();

        let Ok(contribs) = <[SectionContribEntry]>::mut_from_bytes(records_bytes) else {
            bail!("The Section Contributions stream has an invalid size. It is not a multiple of the section contribution record size.  Size: 0x{:x}",
                bytes_len);
        };
        Ok(Self { contribs })
    }

    /// Given a lookup table that maps module indexes from old to new, this edits a
    /// Section Contributions table and converts module indexes.
    pub fn remap_module_indexes(&mut self, modules_old_to_new: &[u32]) -> anyhow::Result<()> {
        for (i, contrib) in self.contribs.iter_mut().enumerate() {
            let old = contrib.module_index.get();
            if let Some(&new) = modules_old_to_new.get(old as usize) {
                contrib.module_index.set(new as u16);
            } else {
                bail!("Section contribution record (at contribution index #{i} has module index {old}, \
                       which is out of range (num modules is {})",
                    modules_old_to_new.len());
            }

            // While we're at it, make sure that the padding fields are cleared.
            contrib.padding1 = [0; 2];
            contrib.padding2 = [0; 2];
        }
        Ok(())
    }
}

```

`pdb/src/dbi/section_map.rs`:

```rs
//! DBI Section Map Substream
#![allow(missing_docs)]

use super::*;
use bitflags::bitflags;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

#[derive(IntoBytes, KnownLayout, Immutable, FromBytes, Unaligned)]
#[repr(C)]
pub struct SectionMapHeader {
    /// Total number of segment descriptors
    pub num_segments: U16<LE>,
    /// Number of logical segment descriptors
    pub num_logical_segments: U16<LE>,
}

#[derive(IntoBytes, KnownLayout, Immutable, FromBytes, Unaligned)]
#[repr(C)]
pub struct SectionMapEntry {
    /// Descriptor flags bit field. See `SectionMapEntryFlags`.
    pub flags: U16<LE>,
    /// The logical overlay number
    pub overlay: U16<LE>,
    /// Group index into the descriptor array
    pub group: U16<LE>,
    /// Logical segment index, interpreted via flags
    pub frame: U16<LE>,
    /// Byte index of segment / group name in string table, or 0xFFFF.
    pub section_name: U16<LE>,
    /// Byte index of class in string table, or 0xFFFF.
    pub class_name: U16<LE>,
    /// Byte offset of the logical segment within physical segment.
    /// If group is set in flags, this is the offset of the group.
    pub offset: U32<LE>,
    /// Byte count of the segment or group.
    pub section_length: U32<LE>,
}

bitflags! {
    #[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
    pub struct SectionMapEntryFlags: u16 {
        /// Segment is readable.
        const READ = 1 << 0;
        /// Segment is writable.
        const WRITE = 1 << 1;
        /// Segment is executable.
        const EXECUTE = 1 << 2;
        /// Descriptor describes a 32-bit linear address.
        const ADDRESS_IS32_BIT = 1 << 3;
        /// Frame represents a selector.
        const IS_SELECTOR = 1 << 8;
        /// Frame represents an absolute address.
        const IS_ABSOLUTE_ADDRESS = 1 << 9;
        /// If set, descriptor represents a group. (obsolete)
        const IS_GROUP = 1 << 10;
    }
}

pub struct SectionMap<'a> {
    pub header: SectionMapHeader,
    pub entries: &'a [SectionMapEntry],
}

impl<'a> SectionMap<'a> {
    pub fn parse(bytes: &'a [u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(bytes);
        if p.is_empty() {
            return Ok(Self {
                entries: &[],
                header: SectionMapHeader {
                    num_logical_segments: U16::ZERO,
                    num_segments: U16::ZERO,
                },
            });
        }

        let header: SectionMapHeader = p.copy()?;

        let Ok(entries) = <[SectionMapEntry]>::ref_from_bytes(p.take_rest()) else {
            bail!("Section map has invalid length (is not a multiple of SectionMapEntry size). Length (including 4-byte header): 0x{:x}", bytes.len());
        };
        Ok(Self { header, entries })
    }
}

```

`pdb/src/dbi/sources.rs`:

```rs
//! DBI Sources Substream

use super::*;
use crate::BStr;
use std::collections::HashMap;

/// The "Sources" substream of the DBI stream. This stream describes the merged set of source
/// files that were the inputs (compilands) of all modules.
///
/// See: <https://llvm.org/docs/PDB/DbiStream.html#file-info-substream>
pub struct DbiSourcesSubstream<'a> {
    /// The `module_file_starts` array gives the index within `file_name_offsets` where the file
    /// names for each module begin. That is, `file_name_offsets[module_file_starts[m]]` is the file
    /// name offset for the first file in the set of files for module `m`.
    ///
    /// When combined with the `module_file_counts` array, you can easily find the slice within
    /// `file_name_offsets` of files for a specific module.
    ///
    /// The length of this slice is equal to `num_modules`. This slice _does not_ have an extra
    /// entry at the end, so you must use `file_name_offsets.len()` as the end of the per-module
    /// slice for the last entry in this slice.
    module_file_starts: &'a [U16<LE>],

    /// For each module, gives the number of source files that contribute to that module.
    module_file_counts: &'a [U16<LE>],

    /// Contains the concatenated list of file name lists, one list per module. For each module
    /// `m`, the set of items within `file_name_offsets` is given by
    /// `file_name_offsets[module_file_starts[m]..][..module_file_counts[m]]`.
    ///
    /// Each item in this list is an offset into `names_buffer` and points to the start of a
    /// NUL-terminated UTF-8 string.
    ///
    /// This array can (and usually does) contain duplicate values. The values are ordered by the
    /// module which referenced a given set of source files. Since many modules will read a shared
    /// set of header files (e.g. `windows.h`), those shared header files will appear many times
    /// in this list.
    ///
    /// The length of `file_name_offsets` is usually higher than the number of _unique_ source files
    /// because many source files (header files) are referenced by more than one module.
    ///
    /// The length of this slice is equal to the sum of the values in the `module_file_counts`.
    /// The on-disk file format stores a field that counts the number of source files, but the field
    /// is only 16-bit, so it can easily overflow on large executables. That is why this
    /// value is computed when the substream is parsed, instead of using the on-disk version.
    file_name_offsets: &'a [U32<LE>],

    /// Contains the file name strings, encoded in UTF-8 and NUL-terminated.
    names_buffer: &'a [u8],
}

impl<'a> DbiSourcesSubstream<'a> {
    /// The number of modules
    pub fn num_modules(&self) -> usize {
        self.module_file_starts.len()
    }

    /// Provides access to the file name offsets slice. Each value is a file name offset, and can
    /// be used with `get_source_name_at()`.
    pub fn file_name_offsets(&self) -> &'a [U32<LE>] {
        self.file_name_offsets
    }

    /// Parses the file info substream.
    ///
    /// This does not parse or validate every part of the substream. It only parses enough to find
    /// the module indices and file names.
    pub fn parse(substream_data: &'a [u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(substream_data);
        let num_modules = p.u16()? as usize;

        // In theory this is supposed to contain the number of source files for which this substream
        // contains information. But that would present a problem in that the width of this field
        // being 16-bits would prevent one from having more than 64K source files in a program. In
        // early versions of the file format, this seems to have been the case. In order to support
        // more than this, this field of the is simply ignored, and computed dynamically by summing
        // up the values of the ModFileCounts array (discussed below).
        //
        // In short, this value should be ignored. However, we still have to read the value in
        // order to parse the header correctly.
        let _obsolete_num_source_files = p.u16()? as usize;

        let module_file_starts: &[U16<LE>] = p.slice(num_modules)?;

        // An array of num_modules integers, each one containing the number of source files which
        // contribute to the module at the specified index. While each individual module is limited
        // to 64K contributing source files, the union of all modules' source files may be greater
        // than 64K. The real number of source files is thus computed by summing this array.
        //
        // Note that summing this array does not give the number of _unique source files_, only the
        // total number of source file contributions to modules.
        let module_file_counts: &[U16<LE>] = p.slice(num_modules)?;

        let num_file_offsets = module_file_counts.iter().map(|c| c.get() as usize).sum();

        // At this point, we could scan module_file_starts + module_file_counts and validate that
        // no entry exceeds num_file_offsets.

        let file_name_offsets = p.slice(num_file_offsets)?;
        let names_buffer = p.into_rest();

        Ok(Self {
            module_file_starts,
            module_file_counts,
            file_name_offsets,
            names_buffer,
        })
    }

    /// Given a source file index, returns the source file name.
    pub fn get_source_file_name(&self, source_file_index: usize) -> Result<&'a BStr, ParserError> {
        let offset = self.file_name_offsets[source_file_index].get();
        self.get_source_file_name_at(offset)
    }

    /// Given a file name offset (within `name_buffer`), returns the source file name.
    pub fn get_source_file_name_at(&self, file_name_offset: u32) -> Result<&'a BStr, ParserError> {
        let Some(string_data) = self.names_buffer.get(file_name_offset as usize..) else {
            return Err(ParserError);
        };
        let mut p = Parser::new(string_data);
        let file_name = p.strz()?;
        Ok(file_name)
    }

    /// Caller is expected to validate module_index (against `num_modules()`) before calling
    pub fn name_offsets_for_module(&self, module_index: usize) -> anyhow::Result<&[U32<LE>]> {
        let start = self.module_file_starts[module_index].get() as usize;
        let count = self.module_file_counts[module_index].get() as usize;
        let Some(s) = self.file_name_offsets.get(start..start + count) else {
            bail!("File name offsets for module #{module_index} are invalid.  start: {start}, count: {count}, len available: {}", self.file_name_offsets.len());
        };
        Ok(s)
    }

    /// Iterates source files in the DBI Sources Substream.
    pub fn iter_sources(&self) -> IterSources<'_> {
        IterSources {
            names_buffer: self.names_buffer,
            file_name_offsets: self.file_name_offsets.iter(),
        }
    }

    /// Builds a HashMap that maps from file name offsets to strings.
    pub fn sources_map(&self) -> anyhow::Result<HashMap<u32, &BStr>> {
        let mut unique_offsets: Vec<u32> = self.file_name_offsets.iter().map(|i| i.get()).collect();
        unique_offsets.sort_unstable();
        unique_offsets.dedup();

        let mut map = HashMap::new();
        for &offset in unique_offsets.iter() {
            let name = self.get_source_file_name_at(offset)?;
            map.insert(offset, name);
        }

        Ok(map)
    }
}

/// Iterates source files in the DBI Sources Substream.
pub struct IterSources<'a> {
    names_buffer: &'a [u8],
    file_name_offsets: std::slice::Iter<'a, U32<LE>>,
}

impl<'a> Iterator for IterSources<'a> {
    /// name_offset (in bytes), name
    type Item = (u32, &'a BStr);

    fn next(&mut self) -> Option<Self::Item> {
        let offset = self.file_name_offsets.next()?.get();
        let mut p = Parser::new(self.names_buffer);
        p.skip(offset as usize).ok()?;
        let name = p.strz().ok()?;
        Some((offset, name))
    }
}

#[cfg(test)]
#[rustfmt::skip]
static TEST_SOURCES_DATA: &[u8] = &[
    /* 0x0000 */ 4, 0,                     // num_modules = 4
    /* 0x0004 */ 0xee, 0xee,               // obsolete num_sources (bogus)
    /* 0x0008 */ 0, 0, 2, 0, 3, 0, 3, 0,   // module_file_starts = [0, 2, 3, 3]
    /* 0x0010 */ 2, 0, 1, 0, 0, 0, 3, 0,   // module_file_counts = [2, 1, 0, 3] sum = 6

    /* 0x0018 */                           // file_offsets, len = 6 items, 24 bytes
    /* 0x0018 */ 0x00, 0, 0, 0,            // module 0, file_offsets[0] = 0x00, points to "foo.c",
    /* 0x0018 */ 0x14, 0, 0, 0,            // module 0, file_offsets[1] = 0x14, points to "windows.h"
    /* 0x0018 */ 0x06, 0, 0, 0,            // module 1, file_offsets[2] = 0x06, points to "bar.rs"
    /* 0x0018 */ 0x00, 0, 0, 0,            // module 3, file_offsets[3] = 0x00, points to "foo.c"
    /* 0x0018 */ 0x14, 0, 0, 0,            // module 3, file_offsets[4] = 0x14, points to "windows.h"
    /* 0x0018 */ 0x0d, 0, 0, 0,            // module 3, file_offsets[5] = 0x0d, points to "main.c"

    // names_buffer; contains (at relative offsets):
    //      name offset 0x0000 : "foo.c"
    //      name offset 0x0006 : "bar.rs"
    //      name offset 0x000d : "main.c"
    //      name offset 0x0014 : "windows.h"
    /* 0x0030 */                                // names_buffer
    /* 0x0030 */ b'f', b'o', b'o', b'.',
    /* 0x0034 */ b'c', 0,    b'b', b'a',
    /* 0x0038 */ b'r', b'.', b'r', b's',
    /* 0x003c */ 0,    b'm', b'a', b'i',
    /* 0x0040 */ b'n', b'.', b'c', 0,
    /* 0x0044 */ b'w', b'i', b'n', b'd',
    /* 0x0048 */ b'o', b'w', b's', b'.',
    /* 0x004c */ b'h', 0,    0,    0,

    /* 0x0050 : end */
];

#[test]
fn basic_parse() {
    let s = DbiSourcesSubstream::parse(TEST_SOURCES_DATA).unwrap();
    assert_eq!(s.num_modules(), 4);

    assert_eq!(s.file_name_offsets.len(), 6);

    let module_file_starts: Vec<u16> = s.module_file_starts.iter().map(|x| x.get()).collect();
    assert_eq!(&module_file_starts, &[0, 2, 3, 3]);

    let module_file_counts: Vec<u16> = s.module_file_counts.iter().map(|x| x.get()).collect();
    assert_eq!(&module_file_counts, &[2, 1, 0, 3]);

    let file_name_offsets: Vec<u32> = s.file_name_offsets.iter().map(|x| x.get()).collect();
    assert_eq!(&file_name_offsets, &[0x00, 0x14, 0x06, 0x00, 0x14, 0x0d]);

    // Read the file names. Remember that there are duplicates in this list.
    assert_eq!(s.get_source_file_name(0).unwrap(), "foo.c");
    assert_eq!(s.get_source_file_name(1).unwrap(), "windows.h");
    assert_eq!(s.get_source_file_name(2).unwrap(), "bar.rs");
    assert_eq!(s.get_source_file_name(3).unwrap(), "foo.c");
    assert_eq!(s.get_source_file_name(4).unwrap(), "windows.h");
    assert_eq!(s.get_source_file_name(5).unwrap(), "main.c");

    let modsrcs0 = s.name_offsets_for_module(0).unwrap();
    assert_eq!(modsrcs0.len(), 2);
    assert_eq!(modsrcs0[0].get(), 0);
    assert_eq!(modsrcs0[1].get(), 0x14);

    // Test bounds check on get_source_file_name_at()
    assert!(s.get_source_file_name_at(0xeeee).is_err());
}

#[test]
fn test_iter_sources() {
    use bstr::ByteSlice;

    let s = DbiSourcesSubstream::parse(TEST_SOURCES_DATA).unwrap();

    let sources: Vec<(u32, &str)> = s
        .iter_sources()
        .map(|(i, s)| (i, s.to_str().unwrap()))
        .collect();

    assert_eq!(
        &sources,
        &[
            (0x00, "foo.c"),
            (0x14, "windows.h"),
            (0x06, "bar.rs"),
            (0x00, "foo.c"),
            (0x14, "windows.h"),
            (0x0d, "main.c"),
        ]
    );
}

#[test]
fn test_sources_map() {
    let s = DbiSourcesSubstream::parse(TEST_SOURCES_DATA).unwrap();
    let map = s.sources_map().unwrap();
    assert_eq!(map.len(), 4); // 4 unique file names
    assert_eq!(*map.get(&0x00).unwrap(), "foo.c");
    assert_eq!(*map.get(&0x06).unwrap(), "bar.rs");
    assert_eq!(*map.get(&0x0d).unwrap(), "main.c");
    assert_eq!(*map.get(&0x14).unwrap(), "windows.h");
}

```

`pdb/src/embedded_sources.rs`:

```rs
use super::*;
use anyhow::Result;
use std::io::Write;

impl<F: ReadAt> Pdb<F> {
    /// Embeds the contents of a source file into the PDB.
    pub fn add_embedded_source(&mut self, file_path: &str, file_contents: &[u8]) -> Result<bool>
    where
        F: WriteAt,
    {
        let stream_name = format!("/src/{}", file_path);
        self.add_or_replace_named_stream(&stream_name, file_contents)
    }

    /// Sets the contents of a named stream to the given value.
    ///
    /// If there is already a named stream with the given name, then the stream's contents
    /// are replaced with `stream_contents`.  First, though, this function reads the contents of
    /// the existing stream and compares them to `stream_contents`. If they are identical, then
    /// the stream is not modified and this function will return `Ok(true)`.  If the contents are
    /// not identical, then this function returns `Ok(true)`.
    ///
    /// If there is not already a named stream with given name, then a new stream is created
    /// and an entry is added to the Named Streams Map. In this case, the function returns
    /// `Ok(false)`.
    pub fn add_or_replace_named_stream(
        &mut self,
        stream_name: &str,
        stream_contents: &[u8],
    ) -> Result<bool>
    where
        F: WriteAt,
    {
        if let Some(existing_stream) = self.named_streams().get(stream_name) {
            // No need to update the named stream directory.

            // Are the stream contents identical?
            let existing_len = self.stream_len(existing_stream);
            if existing_len == stream_contents.len() as u64 {
                let existing_contents = self.read_stream_to_vec(existing_stream)?;
                if existing_contents == stream_contents {
                    return Ok(false);
                }
            }

            let mut w = self.msf_mut_err()?.write_stream(existing_stream)?;
            w.set_len(0)?;
            w.write_all(stream_contents)?;
            Ok(true)
        } else {
            let (new_stream, mut w) = self.msf_mut_err()?.new_stream()?;
            w.write_all(stream_contents)?;
            self.named_streams_mut().insert(stream_name, new_stream);
            Ok(true)
        }
    }
}

```

`pdb/src/encoder.rs`:

```rs
//! Support for encoding primitives and blittable types into output buffers.
#![allow(missing_docs)]

use crate::guid::GuidLe;
use bstr::BStr;
use uuid::Uuid;
use zerocopy::{Immutable, IntoBytes};

pub struct Encoder<'a> {
    pub buf: &'a mut Vec<u8>,
}

impl<'a> Encoder<'a> {
    pub fn new(buf: &'a mut Vec<u8>) -> Self {
        Self { buf }
    }

    pub fn len(&self) -> usize {
        self.buf.len()
    }

    pub fn is_empty(&self) -> bool {
        self.buf.is_empty()
    }

    pub fn u8(&mut self, x: u8) {
        self.buf.push(x);
    }

    pub fn bytes(&mut self, b: &[u8]) {
        self.buf.extend_from_slice(b);
    }

    pub fn u16(&mut self, x: u16) {
        self.bytes(&x.to_le_bytes());
    }

    pub fn u32(&mut self, x: u32) {
        self.bytes(&x.to_le_bytes());
    }

    pub fn t<T: IntoBytes + Immutable>(&mut self, x: &T) {
        self.buf.extend_from_slice(x.as_bytes());
    }

    pub fn strz(&mut self, s: &BStr) {
        self.buf.extend_from_slice(s);
        self.buf.push(0);
    }

    pub fn uuid(&mut self, u: &Uuid) {
        self.t(&GuidLe::from(u));
    }
}

```

`pdb/src/globals.rs`:

```rs
//! Global Symbols
//!
//! This module contains code for reading the public / global symbol streams. This is a
//! moderately-complicated set of data structures, and requires reading several streams and
//! correlating data between them.
//!
//! Global symbols are stored in several streams. The stream indexes are stored in the DBI
//! stream header; the stream indexes are not fixed.

pub mod gsi;
pub mod gss;
pub mod name_table;
pub mod psi;

#[cfg(test)]
mod tests;

use crate::parser::{Parse, ParserError};
use crate::syms::{self, Constant, OffsetSegment, Pub, SymIter, SymKind};
use crate::utils::iter::IteratorWithRangesExt;
use crate::ReadAt;
use anyhow::Context;
use bstr::BStr;
use std::collections::HashMap;
use tracing::{debug, warn};

#[cfg(doc)]
use crate::dbi::DbiStreamHeader;

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads the Global Symbol Stream (GSS). This stream contains global symbol records.
    ///
    /// This function does not validate the contents of the stream.
    pub fn read_gss(&self) -> anyhow::Result<gss::GlobalSymbolStream> {
        if let Some(gss_stream) = self.dbi_header.global_symbol_stream.get() {
            let stream_data = self.read_stream_to_vec(gss_stream)?;
            Ok(gss::GlobalSymbolStream { stream_data })
        } else {
            Ok(gss::GlobalSymbolStream::empty())
        }
    }

    /// Reads the Global Symbol Index (GSI). This stream contains a name-to-symbol lookup table.
    /// It indexes many global symbols, such as `S_GPROCREF`, `S_CONSTANT`, etc.
    pub fn read_gsi(&self) -> anyhow::Result<gsi::GlobalSymbolIndex> {
        if let Some(gsi_stream) = self.dbi_header.global_symbol_index_stream.get() {
            let num_buckets = self.num_buckets_for_name_table();
            let gsi_stream_data = self.read_stream_to_vec(gsi_stream)?;
            gsi::GlobalSymbolIndex::parse(num_buckets, gsi_stream_data)
        } else {
            Ok(gsi::GlobalSymbolIndex::empty())
        }
    }

    /// Returns the number of buckets to use in `NameTable`, for use by the GSI and PSI.
    pub(crate) fn num_buckets_for_name_table(&self) -> usize {
        let minimal_dbg_info = self.mini_pdb();
        name_table::get_v1_default_bucket(minimal_dbg_info)
    }

    /// Reads the Public Symbol Index.
    pub fn read_psi(&self) -> anyhow::Result<psi::PublicSymbolIndex> {
        if let Ok(psi_stream) = self.dbi_header.public_stream_index() {
            let num_buckets = self.num_buckets_for_name_table();
            let public_stream_data = self.read_stream_to_vec(psi_stream)?;
            psi::PublicSymbolIndex::parse(num_buckets, public_stream_data)
        } else {
            Ok(psi::PublicSymbolIndex::empty())
        }
    }
}

/// If `kind` is a global symbol that should be indexed in the GSI or PSI, then this returns the
/// name of that global symbol (within `Some`).
///
/// A "global symbol" in this context is any symbol that can appear in the Global Symbol Stream
/// and be indexed in the Global Symbol Index or Public Symbol Index. The list of global symbols:
///
/// * `S_PUB32`
/// * `S_CONSTANT`
/// * `S_PROCREF`
/// * `S_LPROCREF`
/// * `S_DATAREF`
/// * `S_ANNOTATIONREF`
/// * `S_UDT`
/// * `S_LDATA32`
/// * `S_GDATA32`
/// * `S_LTHREAD32`
/// * `S_GTHREAD32`
pub fn get_global_symbol_name(kind: SymKind, data: &[u8]) -> Result<Option<&BStr>, ParserError> {
    match kind {
        SymKind::S_PUB32 => {
            let pub_data = Pub::parse(data)?;
            Ok(Some(pub_data.name))
        }

        SymKind::S_CONSTANT => {
            let constant_record = Constant::parse(data)?;
            Ok(Some(constant_record.name))
        }

        // These symbols have the same structure.
        SymKind::S_PROCREF
        | SymKind::S_LPROCREF
        | SymKind::S_DATAREF
        | SymKind::S_ANNOTATIONREF => {
            let ref_sym = syms::RefSym2::parse(data)?;
            Ok(Some(ref_sym.name))
        }

        SymKind::S_UDT => {
            let udt_data = syms::Udt::parse(data)?;
            Ok(Some(udt_data.name))
        }

        SymKind::S_LDATA32 | SymKind::S_GDATA32 | SymKind::S_LMANDATA | SymKind::S_GMANDATA => {
            let data = syms::Data::parse(data)?;
            Ok(Some(data.name))
        }

        SymKind::S_LTHREAD32 | SymKind::S_GTHREAD32 => {
            let thread_storage = syms::ThreadStorageData::parse(data)?;
            Ok(Some(thread_storage.name))
        }

        SymKind::S_LMANPROC | SymKind::S_GMANPROC => {
            let man_proc = syms::ManProcSym::parse(data)?;
            Ok(Some(man_proc.name))
        }

        // TODO
        SymKind::S_TOKENREF => Ok(None),

        _ => Ok(None),
    }
}

/// Output of `build_global_symbols_index`
pub struct BuildGlobalSymbolsIndexesOutput {
    /// The new GSI contents
    pub global_symbol_index_stream_data: Vec<u8>,
    /// The new PSI contents
    pub public_symbol_index_stream_data: Vec<u8>,
}

/// Reads a Global Symbol Stream and constructs a new Global Symbol Index (GSI) and
/// Public Symbol Index (PSI).
pub fn build_global_symbols_index(
    symbol_records: &[u8],
    num_buckets: usize,
) -> anyhow::Result<BuildGlobalSymbolsIndexesOutput> {
    debug!("Rebuilding Global Symbol Index (GSI) and Public Symbol Index (PSI)");

    let mut public_hash_records = name_table::NameTableBuilder::new(num_buckets);
    let mut global_hash_records = name_table::NameTableBuilder::new(num_buckets);

    // contains (byte offset in symbol stream, SegmentOffset)
    let mut public_addr_map: Vec<(u32, OffsetSegment)> = Vec::new();

    let mut unrecognized_symbols: HashMap<SymKind, u32> = HashMap::new();

    for (sym_range, sym) in SymIter::new(symbol_records).with_ranges() {
        let sym_offset = sym_range.start;

        // If the symbol is S_PUB32, then add an entry to both public_hash_records and
        // global_hash_records.
        if sym.kind == SymKind::S_PUB32 {
            let pub_data =
                Pub::parse(sym.data).with_context(|| "failed to parse S_PUB32 record")?;
            public_hash_records.push(pub_data.name, (sym_offset + 1) as i32);
            public_addr_map.push((sym_offset as u32, pub_data.offset_segment()));
            continue;
        }

        if matches!(sym.kind, SymKind::S_TOKENREF | SymKind::S_DATAREF) {
            continue;
        }

        if let Some(sym_name) = get_global_symbol_name(sym.kind, sym.data)? {
            global_hash_records.push(sym_name, (sym_offset + 1) as i32);
        } else {
            *unrecognized_symbols.entry(sym.kind).or_default() += 1;
        }
    }

    if !unrecognized_symbols.is_empty() {
        warn!(
            "Number of unrecognized symbol types found in Global Symbol Stream: {}",
            unrecognized_symbols.len()
        );
        let mut sorted_unrecognized: Vec<(SymKind, u32)> =
            unrecognized_symbols.iter().map(|(&k, &v)| (k, v)).collect();
        sorted_unrecognized.sort_unstable_by_key(|(k, _)| *k);
        for (kind, count) in sorted_unrecognized.iter() {
            warn!(
                "    {count:6} - [{raw_kind:04x}] {kind:?}",
                raw_kind = kind.0
            );
        }
    }

    psi::sort_address_records(&mut public_addr_map);

    debug!("Building Global Symbol Index (GSI)");
    let global_symbol_stream_data = gsi::build_gsi(&mut global_hash_records);

    debug!("Building Public Symbol Index (PSI)");
    let public_symbol_stream_data = psi::build_psi(&mut public_hash_records, &public_addr_map);

    Ok(BuildGlobalSymbolsIndexesOutput {
        global_symbol_index_stream_data: global_symbol_stream_data,
        public_symbol_index_stream_data: public_symbol_stream_data,
    })
}

```

`pdb/src/globals/gsi.rs`:

```rs
//! Global Symbol Index
//!
//! The Global Symbol Index (GSI) Stream provides a name-to-symbol lookup table for global symbols
//! that have a name.
//!
//! The GSI does not have a fixed stream number. The stream number is found in the DBI Stream
//! Header.
//!
//! The GSI contains entries only for the following symbol kinds:
//!
//! * `S_CONSTANT`
//! * `S_UDT`
//! * `S_LDATA32`
//! * `S_GDATA32`
//! * `S_LTHREAD32`
//! * `S_GTHREAD32`
//! * `S_LMANDATA`
//! * `S_GMANDATA`
//! * `S_PROCREF`
//! * `S_LPROCREF`
//! * `S_ANNOTATIONREF`
//! * `S_TOKENREF`
//!
//! Note that `S_PUB32` is not included in this list.  `S_PUB32` symbols are indexed in the PSI, not
//! the GSI.
//!
//! The GSI does not provide an address-to-name lookup table.

use super::name_table::*;
use crate::syms::Sym;
use crate::utils::is_aligned_4;
use bstr::BStr;
use std::mem::size_of;
use tracing::{debug, trace_span};

/// Contains the Global Symbol Index
pub struct GlobalSymbolIndex {
    name_table: NameTable,
}

impl GlobalSymbolIndex {
    /// Parses the Global Symbol Index from stream data. The caller must specify `num_buckets`
    /// because the value is not specified in the GSI itself.
    pub fn parse(num_buckets: usize, stream_data: Vec<u8>) -> anyhow::Result<GlobalSymbolIndex> {
        if stream_data.is_empty() {
            return Ok(Self::empty());
        }

        let name_table = NameTable::parse(num_buckets, 0, &stream_data)?;
        Ok(Self { name_table })
    }

    /// Constructs an empty instance of the GSI.
    pub fn empty() -> Self {
        Self {
            name_table: NameTable::empty(),
        }
    }

    /// Find a symbol within the GSI by name.
    pub fn find_symbol<'a>(
        &self,
        gss: &'a crate::globals::gss::GlobalSymbolStream,
        name: &BStr,
    ) -> anyhow::Result<Option<Sym<'a>>> {
        self.name_table.find_symbol(gss, name)
    }

    /// Gets direct access to the name-to-symbol table.
    pub fn names(&self) -> &NameTable {
        &self.name_table
    }
}

/// Builds the Global Symbol Index (GSI) table.
///
/// The GSI contains only a name table. It does not contain an address table.
pub fn build_gsi(sorted_hash_records: &mut NameTableBuilder) -> Vec<u8> {
    let _span = trace_span!("build_gsi").entered();

    let name_table_info = sorted_hash_records.prepare();
    let mut stream_data: Vec<u8> = vec![0; name_table_info.table_size_bytes];
    sorted_hash_records.encode(&name_table_info, &mut stream_data);

    // Make it easy to understand the output.
    {
        let mut pos = 0;
        let mut region = |name: &str, len: usize| {
            debug!("    {pos:08x} +{len:08x} : {name}");
            pos += len;
        };
        debug!("GSI Stream layout:");
        region("Name Table - Header", size_of::<NameTableHeader>());
        region(
            "Name Table - Hash Records",
            sorted_hash_records.num_names() * size_of::<HashRecord>(),
        );
        region(
            "Name Table - Buckets Bitmap",
            nonempty_bitmap_size_bytes(sorted_hash_records.num_buckets()),
        );
        region(
            "Name Table - Buckets",
            name_table_info.num_nonempty_buckets * 4,
        );
        region("(end)", 0);
        assert_eq!(pos, stream_data.len());
    }

    assert!(is_aligned_4(stream_data.len()));

    stream_data
}

```

`pdb/src/globals/gss.rs`:

```rs
//! Global Symbol Stream
//!
//! The Global Symbol Stream contains a sequence of variable-length symbol records. This stream does
//! not have a header of any kind; all of the stream data consists of CodeView symbol records.
//!
//! The GSS does not have a fixed stream number. The stream number is found in the DBI Stream
//! Header.
//!
//! Many other parts of the PDB contain pointers (byte offsets) that point into the GSS:
//! * PSI: Contains lookup tables for `S_PUB32` symbols
//! * GSI: Contains a lookup table for all other named global symbols
//! * Module Streams: Contains a Global Refs section that points to entries in the GSS that are
//!   referenced by that module.

use crate::parser::Parse;
use crate::syms::{Pub, SymIter, SymKind};
use anyhow::bail;

/// Contains the Global Symbol Stream (GSS). This contains symbol records.
///
/// The GSI and the PSI both point into this stream.
pub struct GlobalSymbolStream {
    /// Contains the stream data.
    pub stream_data: Vec<u8>,
}

impl GlobalSymbolStream {
    /// Constructor. This does not validate the contents.
    pub fn new(stream_data: Vec<u8>) -> Self {
        Self { stream_data }
    }

    /// Constructs an empty GSS.
    pub fn empty() -> Self {
        Self {
            stream_data: vec![],
        }
    }

    /// Gets a reference to a symbol at a given record offset.
    ///
    /// This function validates `record_offset`. If it is out of range, this function will return
    /// `Err` instead of panicking.
    pub fn get_sym_at(&self, record_offset: u32) -> anyhow::Result<crate::syms::Sym<'_>> {
        let Some(record_bytes) = self.stream_data.get(record_offset as usize..) else {
            bail!("Invalid record offset into GSS: {record_offset}.  Out of range for the GSS.");
        };

        let mut sym_iter = SymIter::new(record_bytes);
        let Some(sym) = sym_iter.next() else {
            bail!("Invalid record offset into GSS: {record_offset}. Failed to decode symbol data at that offset.");
        };

        Ok(sym)
    }

    /// Gets a reference to a symbol at a given record offset, and then parses it as an `S_PUB32`
    /// record.
    ///
    /// This function validates `record_offset`. If it is out of range, this function will return
    /// `Err` instead of panicking.
    ///
    /// If the symbol at `record_offset` is not an `S_PUB32` symbol, this function returns `Err`.
    pub fn get_pub32_at(&self, record_offset: u32) -> anyhow::Result<Pub<'_>> {
        let sym = self.get_sym_at(record_offset)?;

        if sym.kind != SymKind::S_PUB32 {
            bail!("Invalid record offset into GSS: {record_offset}. Found a symbol with the wrong type.  Expected S_PUB32, found {:?}",
                sym.kind
            );
        };

        let Ok(pub_sym) = crate::syms::Pub::parse(sym.data) else {
            bail!(
                "Invalid record offset into GSS: {record_offset}. Failed to decode S_PUB32 record."
            );
        };

        Ok(pub_sym)
    }

    /// Iterates the symbol records in the Global Symbol Stream.
    pub fn iter_syms(&self) -> SymIter<'_> {
        SymIter::new(&self.stream_data)
    }
}

```

`pdb/src/globals/name_table.rs`:

```rs
//! Name-to-Symbol Lookup Table

#[cfg(test)]
mod tests;

use super::gss::GlobalSymbolStream;
use crate::parser::{Parser, ParserMut};
use crate::syms::Sym;
use anyhow::bail;
use bitvec::prelude::{BitSlice, Lsb0};
use bstr::BStr;
use std::mem::size_of;
use tracing::{debug, debug_span, error, info, trace, warn};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, I32, LE, U32};

/// This is the size used for calculating hash indices. It was the size of the in-memory form
/// of the hash records, on 32-bit machines. It is not the same as the length of the hash records
/// that are stored on disk (which is 8).
pub const HASH_RECORD_CALC_LEN: i32 = 12;

// section contribution structure
/// section contribution version, before V70 there was no section version
pub const GSI_HASH_SC_IMPV_V70: u32 = 0xeffe0000 + 19990810;

/// Signature value for `NameTableHeader::ver_signature`.
pub const GSI_HASH_HEADER_SIGNATURE: u32 = 0xffff_ffff;
/// The current value to use (implementation version) for the GSI Hash.
pub const GSI_HASH_HEADER_VERSION: u32 = GSI_HASH_SC_IMPV_V70;

/// This header is present at the start of the Name Table.
///
/// Immediately after this header follows the hash records, whose length is given by
/// `cb_hash_records`. This section contains an array of [`HashRecord`] structs.
///
/// Immediately after the hash records is the hash buckets, whose length is given by `cb_buckets`.
///
/// Called `GSIHashHdr` in C++.
#[derive(IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable, Debug)]
#[repr(C)]
pub struct NameTableHeader {
    /// Constant which identifies this as a `NameTableHeader`. This value (0xffff_ffff) was chosen
    /// because the previous version of the `NameTable` did not use this header, and this
    /// signature value would never have occurred in the same position in the previous version.
    pub signature: U32<LE>,

    /// Version of the name hash table.
    pub version: U32<LE>,

    /// Size in bytes of the hash records. This should always be a multiple of
    /// `size_of::<HashRecord>()`, not `HASH_RECORD_CALC_LEN`.
    pub hash_records_size: U32<LE>,

    /// Size in bytes of the hash buckets.
    pub buckets_size: U32<LE>,
}

/// An entry in the GSI hash table. This is in the `cb_hash_records` region.
#[derive(IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable, Debug, Clone)]
#[repr(C)]
pub struct HashRecord {
    /// The byte offset of a symbol record within the Global Symbol Stream, plus 1.
    /// It should point to the beginning of a global symbol record, e.g. `S_PUB32`.
    ///
    /// The value should never be negative or zero.
    pub offset: I32<LE>,

    /// This appears to be a reference-count field, use for the in-memory representation.
    /// On disk, the values are nearly always 1.
    pub c_ref: I32<LE>,
}

/// Contains a Name Table, as used in the GSI and PSI.
pub struct NameTable {
    /// Each value in this vector is an index into `hash_records`.
    /// There is an extra index at the end, to make range calculations easier.
    /// This is a "starts" vector that points into `hash_records`.
    ///
    /// `hash_buckets` contains a list of offsets into an array of `HROffsetCalc`
    /// structures, each of which is 12 bytes long. So all of the offsets should be a multiple
    /// of 12.  (Entries can also be -1, meaning there are no entries in that bucket.)
    ///
    /// But here's where it gets weird!  The value 12 was the size of a Win32 (32-bit) structure
    /// that contained memory pointers.  It is not the size of the structure that gets written
    /// to disk.  _That_ structure is 8 bytes!  So when you read these values, you must first
    /// divide them by 12, then multiply by 8, to get the actual byte offset into the on-disk
    /// data structure.
    hash_buckets: Vec<u32>,

    /// Each record in this table corresponds to one symbol record (e.g. `S_PUB32`). It contains
    /// the offset in bytes into the global symbol stream, plus 1.  The value 0 is reserved.
    hash_records: Vec<HashRecord>,
}

impl NameTable {
    /// Parses a `NameTable`. For the GSI, the entire GSI stream contains only a `NameTable`.
    /// For the `PSI, the PSI stream begins with a `PsiStreamHeader`, followed by a `NameTable`.
    pub fn parse(
        num_buckets: usize,
        stream_offset: usize,
        sym_hash_bytes: &[u8],
    ) -> anyhow::Result<Self> {
        // Read the hash table from the stream data. The hash table may be in one of two forms:
        // "large" or "small".  The "large" format was the original format. The "small" format was
        // added later.
        //
        // The hash records are the same for the small and large hash table formats. They are
        // different in how the hash buckets are stored.

        let _span = debug_span!("NameTable::parse").entered();

        let original_len = stream_offset + sym_hash_bytes.len();

        // See gsi.cpp, GSI1::readHash()
        let hash_records: Vec<HashRecord>;
        let hash_buckets: Vec<u32>;
        {
            let mut p = Parser::new(sym_hash_bytes);
            let stream_offset_table_header = original_len - p.len();
            let hash_header: &NameTableHeader = p.get()?;

            if hash_header.signature.get() == GSI_HASH_HEADER_SIGNATURE
                && hash_header.version.get() == GSI_HASH_HEADER_VERSION
            {
                debug!(
                    hash_records_size = hash_header.hash_records_size.get(),
                    buckets_size = hash_header.buckets_size.get(),
                    "Hash table format: small buckets"
                );

                let hash_records_size = hash_header.hash_records_size.get() as usize;
                let buckets_size = hash_header.buckets_size.get() as usize;

                let stream_offset_hash_records = original_len - p.len();
                let hash_records_bytes = p.bytes(hash_records_size)?;
                let stream_offset_hash_buckets = original_len - p.len();
                let buckets_bytes = p.bytes(buckets_size)?;
                let stream_offset_end = original_len - p.len();

                // We expect that the size of the hash table is equal to the size of the header +
                // cb_hash_records + cb_buckets.
                if !p.is_empty() {
                    warn!("Found extra bytes in hash table, len = {}", p.len());
                }

                if hash_records_size % size_of::<HashRecord>() != 0 {
                    warn!("GSI/PSI name table contains hash table with a length that is not a multiple of the hash record size.");
                }
                let num_hash_records = hash_records_size / size_of::<HashRecord>();
                let hash_records_slice: &[HashRecord] =
                    Parser::new(hash_records_bytes).slice(num_hash_records)?;

                debug!(num_hash_records, "Number of records in name table");

                hash_records = hash_records_slice.to_vec();

                debug!(num_buckets, "Number of hash buckets in name table");

                debug!("[........] Stream offsets:");
                debug!("[{:08x}] : NameTableHeader", stream_offset_table_header);
                debug!("[{:08x}] : hash records", stream_offset_hash_records);
                debug!("[{:08x}] : hash buckets", stream_offset_hash_buckets);
                debug!("[{:08x}] : (end)", stream_offset_end);

                hash_buckets = expand_buckets(
                    buckets_bytes,
                    num_buckets,
                    num_hash_records,
                    stream_offset_hash_buckets,
                )?;
            } else {
                // We did not find a GsiHashHeader, so this is an old-style hash.
                error!("Hash table format: old-style normal buckets");
                bail!("Old-style hash table is not supported");
            }
        }

        Ok(Self {
            hash_buckets,
            hash_records,
        })
    }

    /// Constructs an empty instance
    pub fn empty() -> Self {
        Self {
            hash_buckets: vec![0],
            hash_records: vec![],
        }
    }

    /// Check all of the hash records in the name table and verify that the hash of the name for
    /// this record matches the hash bucket that the hash record is in.
    pub fn check_hashes(&self, global_symbols: &GlobalSymbolStream) -> anyhow::Result<()> {
        // Verify that hash buckets point to symbol records, and that the hashes of the symbol
        // names in those symbol records matches the hash code for this bucket.

        let mut num_hashes_incorrect: u32 = 0;

        for (bucket_index, hash_index_window) in self.hash_buckets.windows(2).enumerate() {
            trace!(
                "Checking hash bucket #{bucket_index}, hash records at {} .. {}",
                hash_index_window[0],
                hash_index_window[1]
            );
            let Some(bucket_hash_records) = self
                .hash_records
                .get(hash_index_window[0] as usize..hash_index_window[1] as usize)
            else {
                error!("hash record range is invalid");
                continue;
            };

            for hash_record in bucket_hash_records.iter() {
                let hash_record_offset = hash_record.offset.get();

                // The hash record offset should always be positive.
                if hash_record_offset <= 0 {
                    continue;
                }

                let record_offset_in_symbol_stream = hash_record_offset - 1;
                let pub_sym =
                    match global_symbols.get_pub32_at(record_offset_in_symbol_stream as u32) {
                        Err(e) => {
                            error!("{e}");
                            continue;
                        }
                        Ok(s) => s,
                    };

                let name_hash =
                    crate::hash::hash_mod_u32(pub_sym.name, self.hash_buckets.len() as u32 - 1);

                if name_hash != bucket_index as u32 {
                    if num_hashes_incorrect < 50 {
                        error!(
                            "bucket #{} has symbol {} with wrong hash code {}",
                            bucket_index, pub_sym.name, name_hash
                        );
                    }
                    num_hashes_incorrect += 1;
                }
            }
        }

        if num_hashes_incorrect != 0 {
            error!("Found {num_hashes_incorrect} hash records with the wrong hash value");
        } else {
            info!("All name hashes are correct.");
        }

        Ok(())
    }

    /// Gets the hash records for a specific bucket index. The caller is responsible for using
    /// a valid bucket index.
    pub fn hash_records_for_bucket(&self, bucket: usize) -> &[HashRecord] {
        let start = self.hash_buckets[bucket] as usize;
        let end = self.hash_buckets[bucket + 1] as usize;
        &self.hash_records[start..end]
    }

    /// Gets the hash records that might contain `name`.
    pub fn hash_records_for_name<'a>(&'a self, name: &BStr) -> &'a [HashRecord] {
        // If hash_buckets is empty (i.e. has a length of 1, because it is a "starts" table),
        // then the table is empty and the hash modulus is zero. We need to avoid dividing by zero.
        if self.hash_buckets.len() <= 1 {
            return &[];
        }

        let name_hash = crate::hash::hash_mod_u32(name, self.hash_buckets.len() as u32 - 1);
        self.hash_records_for_bucket(name_hash as usize)
    }

    /// Searches for `name` in the Name Table. The caller must provide access to the GSS.
    pub fn find_symbol<'a>(
        &self,
        gss: &'a GlobalSymbolStream,
        name: &BStr,
    ) -> anyhow::Result<Option<Sym<'a>>> {
        let bucket_entries = self.hash_records_for_name(name);
        for entry in bucket_entries.iter() {
            let entry_offset = entry.offset.get();
            if entry_offset <= 0 {
                warn!("found invalid hash record; entry.offset <= 0");
                continue;
            }

            let sym = gss.get_sym_at(entry_offset as u32 - 1)?;

            if let Some(sym_name) = super::get_global_symbol_name(sym.kind, sym.data)? {
                if sym_name.eq_ignore_ascii_case(name) {
                    return Ok(Some(sym));
                }
            }
        }

        Ok(None)
    }

    /// Iterates the names stored within a `NameTable`.
    pub fn iter<'i, 'a: 'i>(&'a self, gss: &'a GlobalSymbolStream) -> NameTableIter<'i> {
        NameTableIter {
            hash_record_iter: self.hash_records.iter(),
            gss,
        }
    }
}

/// Iterator state for iterating the names within a `NameTable`.
pub struct NameTableIter<'a> {
    hash_record_iter: std::slice::Iter<'a, HashRecord>,
    gss: &'a GlobalSymbolStream,
}

impl<'a> Iterator for NameTableIter<'a> {
    type Item = Sym<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        loop {
            let hash_record = self.hash_record_iter.next()?;

            let sym_offset = hash_record.offset.get();
            if sym_offset <= 0 {
                // Whatever, just ignore it.
                continue;
            }

            if let Ok(sym) = self.gss.get_sym_at((sym_offset - 1) as u32) {
                return Some(sym);
            } else {
                error!("failed to decode symbol in GSS at offset {sym_offset}");
                return None;
            }
        }
    }
}

/// Gets the default number of buckets to use.
///
/// The values are hard-coded. 0x1000 is used for normal PDBs and 0x3ffff is used for "mini PDBs".
/// Mini PDBs are produced using the `/DEBUG:FASTLINK` linker option.
pub fn get_v1_default_bucket(minimal_dbg_info: bool) -> usize {
    if minimal_dbg_info {
        0x3ffff
    } else {
        0x1000
    }
}

/// Expands a compressed bucket. Returns a vector of offsets.
///
/// The input contains a bitmap, followed by an array of offsets. The bitmap determines how many
/// items there are in the array of offsets. The length of the bitmap is specified by num_buckets.
///
/// This function returns a vector that contains hash indices. The hash records for a given hash
/// bucket can be found as:
///
/// ```text
/// let buckets = expand_buckets(...)?;
/// let bucket_index = 10;
/// let hash_records: &[HashRecord] = &hash_records[buckets[bucket_index]..buckets[bucket_index + 1]];
/// ```
fn expand_buckets(
    buckets_bytes: &[u8],
    num_buckets: usize,
    num_hash_records: usize,
    stream_offset: usize, // offset of this data structure in stream; for diagnostics only
) -> anyhow::Result<Vec<u32>> {
    let _span = debug_span!("expand_buckets").entered();

    trace!(num_buckets, num_hash_records, "expanding buckets");

    let original_len = stream_offset + buckets_bytes.len();
    let mut p = Parser::new(buckets_bytes);

    let output_len = num_buckets + 1;

    let bitmap_len_in_bytes = nonempty_bitmap_size_bytes(num_buckets);
    let bitmap_bytes = p.bytes(bitmap_len_in_bytes)?;
    let bv: &BitSlice<u8, Lsb0> = BitSlice::from_slice(bitmap_bytes);
    trace!(bitmap_bytes, bitmap_len_in_bytes);

    // Count the number of 1 bits set in the non-empty bucket bitmap.
    // Use min(num_buckets) so that we ignore any extra bits in the bitmap.
    let num_nonempty_buckets = bv.count_ones().min(num_buckets);
    trace!(num_nonempty_buckets);

    let nonempty_pointers_stream_offset = original_len - p.len();
    let nonempty_pointers: &[I32<LE>] = p.slice(num_nonempty_buckets)?;

    trace!(
        nonempty_pointers_stream_offset,
        non_empty_pointers = nonempty_pointers.as_bytes(),
        "non-empty pointers"
    );

    let mut nonempty_pointers_iter = nonempty_pointers.iter();

    let mut hash_buckets = Vec::with_capacity(output_len);
    for bucket_index in bv.iter_ones() {
        // Be careful to avoid processing 1 bits in the bitmap. It is possible for the bitmap
        // to contain more bits than there are buckets, due to bit alignment.
        if bucket_index >= num_buckets {
            break;
        }

        // The unwrap() cannot fail because we computed the slice length (num_nonempty_buckets)
        // from the number of 1 bits in the non-empty mask (bv).
        let offset_x12 = nonempty_pointers_iter.next().unwrap().get();
        if offset_x12 < 0 {
            bail!("found a negative offset in hash buckets");
        }
        if offset_x12 % HASH_RECORD_CALC_LEN != 0 {
            bail!("hash record offset {offset_x12} is not a multiple of 12 (as required)");
        }
        let offset = (offset_x12 / HASH_RECORD_CALC_LEN) as u32;

        // It would be strange for offset to be equal to num_hash_records because that would
        // imply an empty "non-empty" hash bucket.
        if offset as usize >= num_hash_records {
            bail!("hash record offset {offset_x12} is beyond range of hash records table");
        }

        // Record offsets must be non-decreasing. They should actually be strictly increasing,
        // but we tolerate repeated values.
        if let Some(&prev_offset) = hash_buckets.last() {
            if offset < prev_offset {
                bail!("hash record offset {offset} is less than previous offset {prev_offset}");
            }
        } else if offset != 0 {
            bail!("First hash record offset should be zero, but instead it is: 0x{offset:x}");
        }

        // Add offsets for previous buckets, which were all empty.
        if hash_buckets.len() < bucket_index {
            trace!("    bucket: 0x{:08x} .. 0x{bucket_index:08x} : range is empty, pushing offset: 0x{offset:8x} {offset:10}", hash_buckets.len());
            hash_buckets.resize(bucket_index, offset);
        }
        trace!(
            "    bucket: 0x{b:08x} --> offset: 0x{offset:08x}",
            b = hash_buckets.len()
        );
        hash_buckets.push(offset);
        assert!(hash_buckets.len() <= num_buckets);
    }

    // Fill in the offsets for the remaining empty buckets (if any), and push an extra offset for
    // the end of the hash records array.
    assert!(hash_buckets.len() <= num_buckets);
    hash_buckets.resize(num_buckets + 1, num_hash_records as u32);

    trace!(
        "hash bucket offsets: {:?}",
        &hash_buckets[..hash_buckets.len().min(100)]
    );

    if !nonempty_pointers_iter.as_slice().is_empty() {
        warn!(
            num_extra_bytes = p.len(),
            rest = p.peek_rest(),
            "Compressed hash buckets table contains extra byte(s)"
        );
    }

    if tracing::event_enabled!(tracing::Level::TRACE) {
        trace!("Non-empty buckets: (within expand_buckets)");
        for i in 0..hash_buckets.len() - 1 {
            let start = hash_buckets[i];
            let end = hash_buckets[i + 1];
            if start != end {
                trace!(i, start, end);
            }
        }
    }

    Ok(hash_buckets)
}

/// Used when rebuilding a Name Table
///
/// The order of the fields is significant because it is used for sorting.
#[derive(Eq, PartialEq, PartialOrd, Ord)]
pub struct HashEntry {
    /// computed hash code
    pub hash: u32,
    /// Symbol offset within the GSS. This value has a bias of +1. It will never be 0.
    pub symbol_offset: i32,
}

/// Scan hash_records and figure out how many hash buckets are _not_ empty. Because hash_records
/// is sorted by hash, we can do a single scan through it and find all of the "edges" (places where
/// the `hash` value changes).
pub fn count_nonempty_buckets(sorted_hash_records: &[HashEntry]) -> usize {
    iter_nonempty_buckets(sorted_hash_records).count()
}

/// Compute the size in bytes of the bitmap of non-empty buckets.
pub fn nonempty_bitmap_size_bytes(num_buckets: usize) -> usize {
    let compressed_bitvec_size_u32s = (num_buckets + 1 + 31) / 32;
    compressed_bitvec_size_u32s * 4
}

/// Compute the size in bytes of the name hash table. This includes the header.
pub fn compute_hash_table_size_bytes(
    num_hash_records: usize,
    num_buckets: usize,
    num_nonempty_buckets: usize,
) -> usize {
    size_of::<NameTableHeader>()
        + num_hash_records * size_of::<HashRecord>()
        + nonempty_bitmap_size_bytes(num_buckets)
        + num_nonempty_buckets * size_of::<i32>()
}

/// Output of `build_name_table_prepare`
pub struct NameTableInfo {
    /// The number of buckets to use. This is an input parameter for the table building code, but
    /// it is preserved here to simplify control flow.
    pub num_buckets: usize,
    /// Number of non-empty buckets.  Always less than or equal to `num_buckets.
    pub num_nonempty_buckets: usize,
    /// Size of the entire table, in bytes.
    pub table_size_bytes: usize,
}

impl NameTableBuilder {
    /// Reads a set of sorted hash records and computes the number of non-empty hash buckets and the
    /// total size of the Name Table in bytes.
    pub fn prepare(&mut self) -> NameTableInfo {
        self.sort();

        let num_nonempty_buckets = count_nonempty_buckets(&self.hash_records);
        let table_size_bytes = compute_hash_table_size_bytes(
            self.hash_records.len(),
            self.num_buckets,
            num_nonempty_buckets,
        );
        NameTableInfo {
            num_buckets: self.num_buckets,
            num_nonempty_buckets,
            table_size_bytes,
        }
    }

    /// The number of names inserted into this builder.
    pub fn num_names(&self) -> usize {
        self.hash_records.len()
    }

    /// Writes the hash records, the hash header, and the buckets to output. The output size must be
    /// equal to the expected size.
    pub fn encode(&self, prepared_info: &NameTableInfo, output: &mut [u8]) {
        debug!(
            "Number of symbols found (in name table): {n} 0x{n:x}",
            n = self.hash_records.len()
        );
        debug!(
            "Size in bytes of hash records (in name table): {s} 0x{s:x}",
            s = self.hash_records.len() * 8,
        );

        let compressed_bitvec_size_bytes = nonempty_bitmap_size_bytes(self.num_buckets);
        let num_nonempty_buckets = prepared_info.num_nonempty_buckets;
        let hash_buckets_size_bytes =
            compressed_bitvec_size_bytes + num_nonempty_buckets * size_of::<i32>();
        debug!(
            "hash_buckets_size_bytes = 0x{0:x} {0}",
            hash_buckets_size_bytes
        );

        // Build the name-to-symbol hash map.
        // The structure of the name-to-symbol hash map is:
        //
        //      GsiHashHeader (fixed size)
        //      [HashRecord; num_hash_records]
        //      u32-aligned bitmap of num_buckets + 1 length (in bits), indicating whether a given bucket is non-empty.
        //      [i32; num_non_empty_buckets]
        {
            let mut hash_output_cursor = ParserMut::new(output);

            // Write the hash header.
            *hash_output_cursor.get_mut().unwrap() = NameTableHeader {
                signature: U32::new(GSI_HASH_HEADER_SIGNATURE),
                version: U32::new(GSI_HASH_HEADER_VERSION),
                buckets_size: U32::new(hash_buckets_size_bytes as u32),
                hash_records_size: U32::new(
                    (self.hash_records.len() * size_of::<HashRecord>()) as u32,
                ),
            };

            // Write the hash records.
            {
                let output_slice: &mut [HashRecord] = hash_output_cursor
                    .slice_mut(self.hash_records.len())
                    .unwrap();
                for (from, to) in self.hash_records.iter().zip(output_slice.iter_mut()) {
                    *to = HashRecord {
                        offset: I32::new(from.symbol_offset),
                        c_ref: I32::new(1),
                    };
                }
            }

            // Set all bits in the presence bitmap.
            {
                let sym_hash_bitvec_bytes = hash_output_cursor
                    .bytes_mut(compressed_bitvec_size_bytes)
                    .unwrap();
                let bv: &mut BitSlice<u8, Lsb0> = BitSlice::from_slice_mut(sym_hash_bitvec_bytes);

                for (_record_index, bucket_hashes) in iter_nonempty_buckets(&self.hash_records) {
                    let hash = bucket_hashes[0].hash;
                    bv.set(hash as usize, true);
                }

                assert_eq!(bv.count_ones(), num_nonempty_buckets);
            }

            // Write the hash record offsets of each of the non-empty hash buckets.
            // These values are non-decreasing. The end of each hash bucket is the beginning of the
            // next hash bucket.
            {
                let mut num_nonempty_written = 0;
                let output_offsets_slice: &mut [U32<LE>] =
                    hash_output_cursor.slice_mut(num_nonempty_buckets).unwrap();
                let mut output_iter = output_offsets_slice.iter_mut();

                for (record_index, _hashes) in iter_nonempty_buckets(&self.hash_records) {
                    *output_iter.next().unwrap() = U32::new((record_index as u32) * 12);
                    num_nonempty_written += 1;
                }

                assert_eq!(num_nonempty_written, num_nonempty_buckets);
                assert!(output_iter.as_slice().is_empty());
            }

            assert!(hash_output_cursor.is_empty());
        }
    }
}

/// Sorts hash records
#[inline(never)]
pub fn sort_hash_records(hash_records: &mut [HashEntry]) {
    // This fully determines the order, since we are comparing all fields.
    hash_records.sort_unstable();
}

/// Iterates non-empty buckets. Each item is `(record_index, bucket)`, where `record_index` is
/// the index within `hashes` (the input of this function) where `bucket` begins.  The iterated
/// `bucket` value will always contain values (will not be empty).
fn iter_nonempty_buckets(hashes: &[HashEntry]) -> IterNonEmptyBuckets<'_> {
    IterNonEmptyBuckets {
        record_index: 0,
        hashes,
    }
}

struct IterNonEmptyBuckets<'a> {
    record_index: usize,
    hashes: &'a [HashEntry],
}

impl<'a> Iterator for IterNonEmptyBuckets<'a> {
    // (index into hash records, slice of hash records in bucket)
    type Item = (usize, &'a [HashEntry]);

    fn next(&mut self) -> Option<Self::Item> {
        if self.hashes.is_empty() {
            return None;
        }

        let record_index = self.record_index;

        let hash = self.hashes[0].hash;
        let mut end = 1;
        while end < self.hashes.len() && self.hashes[end].hash == hash {
            end += 1;
        }

        let (lo, hi) = self.hashes.split_at(end);
        self.record_index += end;
        self.hashes = hi;
        Some((record_index, lo))
    }
}

/// This type constructs a new name table, for the GSI/PSI.
///
/// Example:
///
/// ```
/// # use ms_pdb::globals::name_table::NameTableBuilder;
/// # use bstr::BStr;
/// let mut builder = NameTableBuilder::new(0x1000);
/// builder.push("hello".into(), 1);
/// builder.push("world".into(), 2);
/// let prepared_info = builder.prepare();
/// let mut encoded_bytes: Vec<u8> = vec![0; prepared_info.table_size_bytes];
/// builder.encode(&prepared_info, &mut encoded_bytes);
/// ```
pub struct NameTableBuilder {
    num_buckets: usize,
    hash_records: Vec<HashEntry>,
}

impl NameTableBuilder {
    /// Starts building a new name table.  Specify the number of hash buckets to use.
    pub fn new(num_buckets: usize) -> Self {
        assert!(num_buckets > 0);
        Self {
            num_buckets,
            hash_records: Vec::new(),
        }
    }

    /// The number of hash buckets.
    pub fn num_buckets(&self) -> usize {
        self.num_buckets
    }

    /// Adds a new string entry to the builder. Specify the hash of the string and its symbol
    /// offset. The hash _must_ already have been computed and the remainder taken using
    /// `num_buckets()`.
    pub fn push_hash(&mut self, hash: u32, symbol_offset: i32) {
        assert!((hash as usize) < self.num_buckets);
        self.hash_records.push(HashEntry {
            hash,
            symbol_offset,
        });
    }

    /// Hashes a string and adds it to the table. The contents of the string are not retained.
    pub fn push(&mut self, name: &BStr, symbol_offset: i32) {
        let name_hash = crate::hash::hash_mod_u32(name, self.num_buckets as u32);
        self.push_hash(name_hash, symbol_offset);
    }

    fn sort(&mut self) {
        sort_hash_records(&mut self.hash_records);
    }
}

```

`pdb/src/globals/name_table/tests.rs`:

```rs
use pretty_hex::PrettyHex;

use super::*;

fn test_names(names: &[String]) -> NameTable {
    println!();

    let num_buckets = 0x1000;

    let mut builder = NameTableBuilder::new(num_buckets);

    for (i, name) in names.iter().enumerate() {
        let symbol_offset = i as i32 + 1;
        builder.push(BStr::new(name), symbol_offset);

        let entry = builder.hash_records.last().unwrap();
        println!(
            "  {i:4} : hash 0x{:08x}, symbol_offset 0x{:08x}, name: {name:?}",
            entry.hash, entry.symbol_offset as u32
        );
    }

    // Add two entries that test our requirements during decoding.
    builder.push("bad_symbol_zero_offset".into(), 0);
    builder.push("bad_symbol_negative_offset".into(), -1);

    let prepared_info = builder.prepare();

    let mut encoded_bytes = vec![0u8; prepared_info.table_size_bytes];
    builder.encode(&prepared_info, &mut encoded_bytes);
    println!("Encoded name table:\n{:?}", encoded_bytes.hex_dump());

    // Decode the table.
    let rt_table = NameTable::parse(num_buckets, 0, &encoded_bytes)
        .expect("Expected table to decode successfully");

    println!("Hash records in decoded table:");
    for (i, hr) in rt_table.hash_records.iter().enumerate() {
        println!("  {i:4} : symbol_offset 0x{:08x}", hr.offset.get() as u32);
    }

    println!("Non-empty buckets:");
    for i in 0..rt_table.hash_buckets.len() - 1 {
        let start = rt_table.hash_buckets[i];
        let end = rt_table.hash_buckets[i + 1];
        if start != end {
            println!("  {i:4} : {:4} .. {:4}", start, end);
        }
    }

    println!("Checking names:");

    // Make sure that all of the names can be found in the table.
    for name in names.iter() {
        let bucket = rt_table.hash_records_for_name(BStr::new(name));
        println!(
            "searching for {name:?}, num entries in bucket = {}",
            bucket.len()
        );

        let mut num_found: u32 = 0;
        for entry in bucket {
            let symbol_offset = entry.offset.get();
            assert!(symbol_offset > 0);
            assert!(symbol_offset as usize - 1 < names.len());
            if names[symbol_offset as usize - 1] == *name {
                num_found += 1;
            }
        }

        assert_eq!(
            num_found, 1,
            "expected to find {name:?} in the table exactly once"
        );
    }

    rt_table
}

#[test]
fn build_empty() {
    test_names(&[]);
}

// Verify that the code that checks for a record offset <= 0 is working.
#[test]
fn build_and_check_bad_names() {
    let names = test_names(&[]);
    let gss = GlobalSymbolStream::new(Vec::new());
    let name_opt = names
        .find_symbol(&gss, "bad_symbol_negative_offset".into())
        .unwrap();
    assert!(name_opt.is_none());
}

#[test]
fn build_simple() {
    let names = vec![
        "achilles".to_string(),
        "castor".to_string(),
        "pollux".to_string(),
    ];
    test_names(&names);
}

#[test]
fn build_many() {
    let mut names: Vec<String> = Vec::new();
    for i in 0..100 {
        names.push(format!("name{i}"));
    }

    test_names(&names);
}

```

`pdb/src/globals/psi.rs`:

```rs
//! Public Symbol Index
//!
//! The Public Symbol Index (PSI) provides several look-up tables that accelerate finding
//! information in the Global Symbol Stream. The PSI indexes only `S_PUB32` symbols in the GSS; all
//! other symbol kinds are indexed in the GSI.
//!
//! The PSI does not have a fixed stream number. The DBI Stream Header contains the stream number
//! of the PSI.

use super::gss::*;
use super::name_table::*;
use crate::parser::{Parse, Parser, ParserMut};
use crate::syms::{OffsetSegment, Pub};
use crate::utils::is_aligned_4;
use anyhow::{bail, Context};
use bstr::BStr;
use std::mem::size_of;
use tracing::{debug, error, info};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U16, U32};

/// The header of the GSI stream.
///
/// See `PSGSIHDR` in `microsoft-pdb/PDB/dbi/gsi.h`.
#[repr(C)]
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout, Clone, Debug)]
#[allow(missing_docs)]
pub struct PsiStreamHeader {
    /// Length in bytes of the symbol hash table.  This region immediately follows PSGSIHDR.
    pub name_table_size: U32<LE>,

    /// Length in bytes of the address map.  This region immediately follows the symbol hash.
    pub addr_table_size: U32<LE>,

    /// The number of thunk records.
    pub num_thunks: U32<LE>,
    /// Size in bytes of each thunk record.
    pub thunk_size: U32<LE>,
    pub thunk_table_section: U16<LE>,
    pub padding: U16<LE>,
    pub thunk_table_offset: U32<LE>,
    pub num_sections: U32<LE>,
}
static_assertions::const_assert_eq!(core::mem::size_of::<PsiStreamHeader>(), 28);

/// Contains the Public Symbol Index
///
/// The Public Symbol Index (PSI) contains a name-to-symbol lookup table and an address-to-symbol
/// lookup table.
pub struct PublicSymbolIndex {
    /// Allows name-to-symbol look for `S_PUB32` symbols.
    name_table: NameTable,

    /// Each entry in this table is a byte offset of one `S_PUB32` symbol in the GSS.
    /// All of the values are sorted by `(segment, offset)`, which allows binary search.
    addr_map: Vec<u32>,
}

impl PublicSymbolIndex {
    /// Parses the PSI from stream data. The caller must specify `num_buckets` because that value
    /// is not stored within the stream.
    pub fn parse(num_buckets: usize, public_stream_data: Vec<u8>) -> anyhow::Result<Self> {
        let mut p = Parser::new(&public_stream_data);
        if p.is_empty() {
            return Ok(Self::empty());
        }

        let psgsi_header: &PsiStreamHeader = p.get()?;
        debug!("PsiStreamHeader: {:#?}", psgsi_header);

        let sym_hash_size = psgsi_header.name_table_size.get() as usize;
        let addr_map_size = psgsi_header.addr_table_size.get() as usize;

        debug!("Size of symbol hash table: {} bytes", sym_hash_size);
        debug!("Size of address map: {} bytes", addr_map_size);

        let sym_hash_bytes = p
            .bytes(sym_hash_size)
            .with_context(|| "Failed to locate symbol hash table within Publics stream")?;
        let addr_map_bytes = p
            .bytes(addr_map_size)
            .with_context(|| "Failed to locate address map within Publics stream")?;

        let name_table =
            NameTable::parse(num_buckets, size_of::<PsiStreamHeader>(), sym_hash_bytes)?;

        // Load the address map. The address map is an array of u32 values, each of which is an
        // offset into the global symbol stream. I'm _guessing_ that the array is sorted by
        // [segment:offset].
        let addr_map: Vec<u32>;
        {
            let num_addrs = addr_map_bytes.len() / 4;
            info!("Number of entries in address map: {}", num_addrs);

            let mut addr_parser = Parser::new(addr_map_bytes);
            let addr_map_u32_slice: &[U32<LE>] = addr_parser.slice(num_addrs)?;

            addr_map = addr_map_u32_slice.iter().map(|i| i.get()).collect();
        }

        Ok(PublicSymbolIndex {
            name_table,
            addr_map,
        })
    }

    /// Constructs an empty instance of the PSI.
    pub fn empty() -> Self {
        Self {
            addr_map: vec![],
            name_table: NameTable::empty(),
        }
    }

    /// Check invariants for the PSI. This requires having access to the GSS, since the PSI
    /// points into the GSS.
    pub fn check_consistency(&self, gss: &GlobalSymbolStream) -> anyhow::Result<()> {
        // Verify that all entries in the address map are in non-decreasing order.
        let mut prev_sym: Option<Pub<'_>> = None;
        let mut num_bad_order: u32 = 0;
        for &offset in self.addr_map.iter() {
            let sym = gss.get_pub32_at(offset)?;
            if let Some(prev_sym) = &prev_sym {
                if prev_sym.offset_segment() > sym.offset_segment() {
                    if num_bad_order < 20 {
                        error!("found addr map in bad order");
                    }
                    num_bad_order += 1;
                }
            }
            prev_sym = Some(sym);
        }

        if num_bad_order != 0 {
            bail!(
                "Found {} address map entries that were out of order.",
                num_bad_order
            );
        }
        info!("All address map entries are correctly ordered.");

        Ok(())
    }

    /// Gets direct access to the name-to-symbol table.
    pub fn names(&self) -> &NameTable {
        &self.name_table
    }

    /// Searches for an `S_PUB32` symbol by name.
    pub fn find_symbol_by_name<'a>(
        &self,
        gss: &'a GlobalSymbolStream,
        name: &BStr,
    ) -> anyhow::Result<Option<Pub<'a>>> {
        if let Some(sym) = self.name_table.find_symbol(gss, name)? {
            Ok(Some(Pub::parse(sym.data)?))
        } else {
            Ok(None)
        }
    }

    /// Searches for an `S_PUB32` symbol by address.
    pub fn find_symbol_by_addr<'a>(
        &self,
        gss: &'a GlobalSymbolStream,
        segment: u16,
        offset: u32,
    ) -> anyhow::Result<Option<(Pub<'a>, u32)>> {
        use std::cmp::Ordering;

        let addr_map = self.addr_map.as_slice();

        let mut items = addr_map;
        while !items.is_empty() {
            let mid_index = items.len() / 2;
            let mid_rec = gss.get_pub32_at(items[mid_index])?;
            let mid_segment = mid_rec.fixed.offset_segment.segment();
            let mid_offset = mid_rec.fixed.offset_segment.offset();

            match segment.cmp(&mid_segment) {
                Ordering::Less => {
                    // info!("segment is less, moving low");
                    items = &items[..mid_index];
                    continue;
                }
                Ordering::Greater => {
                    // info!("segment is greater, moving high");
                    items = &items[mid_index + 1..];
                    continue;
                }
                Ordering::Equal => {}
            }

            // Same segment. Compare the offsets.

            if offset < mid_offset {
                items = &items[..mid_index];
                continue;
            }

            if offset == mid_offset {
                // Bullseye!
                return Ok(Some((mid_rec, 0)));
            }

            // The address we are looking for is higher than the address of the symbol that we are
            // currently looking at.
            // TODO: Implement best-so-far search.
            items = &items[mid_index + 1..];
            continue;
        }

        Ok(None)
    }
}

/// Sorts an address map slice.
#[inline(never)]
pub fn sort_address_records(addr_map: &mut [(u32, OffsetSegment)]) {
    addr_map.sort_unstable_by_key(|(record_offset, os)| (os.clone(), *record_offset));
}

/// Builds the Public Symbol Index (PSI).
///
/// The PSI contains both a name-to-symbol table and an address-to-symbol table.
pub fn build_psi(
    sorted_hash_records: &mut NameTableBuilder,
    sorted_addr_map: &[(u32, OffsetSegment)],
) -> Vec<u8> {
    assert_eq!(sorted_hash_records.num_names(), sorted_addr_map.len());

    debug!(
        "Number of entries in address table: {n} 0x{n:x}",
        n = sorted_addr_map.len()
    );
    debug!(
        "Size in bytes of address table: {s} 0x{s:x}",
        s = sorted_addr_map.len() * 4
    );

    let name_table_info = sorted_hash_records.prepare();
    let addr_map_size_bytes = sorted_addr_map.len() * size_of::<i32>();

    let stream_size_bytes =
        size_of::<PsiStreamHeader>() + name_table_info.table_size_bytes + addr_map_size_bytes;

    let mut stream_data: Vec<u8> = vec![0; stream_size_bytes];
    let mut p = ParserMut::new(&mut stream_data);

    let stream_header = PsiStreamHeader {
        name_table_size: U32::new(name_table_info.table_size_bytes as u32),
        addr_table_size: U32::new(addr_map_size_bytes as u32),
        num_thunks: U32::new(0), // TODO
        thunk_size: U32::new(0), // TODO
        padding: U16::new(0),
        thunk_table_section: U16::new(0), // TODO
        thunk_table_offset: U32::new(0),  // TODO
        num_sections: U32::new(0),        // TODO
    };
    *p.get_mut::<PsiStreamHeader>().unwrap() = stream_header;

    let name_table_bytes = p.bytes_mut(name_table_info.table_size_bytes).unwrap();
    sorted_hash_records.encode(&name_table_info, name_table_bytes);

    let addr_map_bytes = p.bytes_mut(addr_map_size_bytes).unwrap();
    let addr_map_output = <[U32<LE>]>::mut_from_bytes(addr_map_bytes).unwrap();
    // Write the address map. This converts from the array that we used for sorting, which contains
    // the symbol record byte offset and the segment:offset, to just the symbol record byte offset.
    {
        for (from, to) in sorted_addr_map.iter().zip(addr_map_output.iter_mut()) {
            *to = U32::new(from.0);
        }
    }

    assert!(p.is_empty());

    // Make it easy to understand the output.
    {
        let mut pos = 0;
        let mut region = |name: &str, len: usize| {
            debug!("    {pos:08x} +{len:08x} : {name}");
            pos += len;
        };
        debug!("PSI Stream layout:");
        region("PSI Stream Header", size_of::<PsiStreamHeader>());
        region("Name Table - Header", size_of::<NameTableHeader>());
        region(
            "Name Table - Hash Records",
            sorted_hash_records.num_names() * size_of::<HashRecord>(),
        );
        region(
            "Name Table - Buckets Bitmap",
            nonempty_bitmap_size_bytes(sorted_hash_records.num_buckets()),
        );
        region(
            "Name Table - Buckets",
            name_table_info.num_nonempty_buckets * 4,
        );
        region("Address Table", addr_map_size_bytes);
        region("(end)", 0);
        assert_eq!(pos, stream_data.len());
    }

    assert!(is_aligned_4(stream_data.len()));

    stream_data
}

```

`pdb/src/globals/tests.rs`:

```rs
use pretty_hex::PrettyHex;

use super::build_global_symbols_index;
use super::gsi::GlobalSymbolIndex;
use super::gss::GlobalSymbolStream;
use super::psi::PublicSymbolIndex;
use crate::encoder::Encoder;
use crate::syms::SymKind;
use crate::types::TypeIndex;

const NUM_BUCKETS: usize = 0x1000;

#[derive(Default)]
struct SymBuilder {
    buffer: Vec<u8>,
    record_start: usize,
}

impl SymBuilder {
    fn finish(self) -> GlobalSymbolStream {
        GlobalSymbolStream::new(self.buffer)
    }

    /// Starts adding a new record to the builder.
    fn start_record(&mut self, kind: SymKind) -> Encoder<'_> {
        self.record_start = self.buffer.len();
        self.buffer.extend_from_slice(&[0, 0]); // placeholder for record length
        self.buffer.extend_from_slice(&kind.0.to_le_bytes());
        Encoder::new(&mut self.buffer)
    }

    /// Finishes adding a record to the builder.
    fn end_record(&mut self) {
        match self.buffer.len() & 3 {
            1 => self.buffer.push(0xf1),
            2 => self.buffer.extend_from_slice(&[0xf1, 0xf2]),
            3 => self.buffer.extend_from_slice(&[0xf1, 0xf2, 0xf3]),
            _ => {}
        }

        let record_len = self.buffer.len() - self.record_start - 2;

        let record_field = &mut self.buffer[self.record_start..];
        record_field[0] = record_len as u8;
        record_field[1] = (record_len >> 8) as u8;
    }

    /// Adds an `S_UDT` record.
    fn udt(&mut self, ty: TypeIndex, name: &str) {
        let mut e = self.start_record(SymKind::S_UDT);
        e.u32(ty.0);
        e.strz(name.into());
        self.end_record();
    }

    /// Adds an `S_PUB32` record.
    fn pub32(&mut self, flags: u32, offset: u32, segment: u16, name: &str) {
        let mut e = self.start_record(SymKind::S_PUB32);
        e.u32(flags);
        e.u32(offset);
        e.u16(segment);
        e.strz(name.into());
        self.end_record();
    }
}

/// Builds a GSS with some example records
fn build_test_gss() -> Vec<u8> {
    let mut sb = SymBuilder::default();

    sb.udt(TypeIndex(0x1001), "FOO");
    sb.udt(TypeIndex(0x1001), "BAR");
    sb.udt(
        TypeIndex(0x1002),
        "AugmentedMultiThreadedSymbolExpanderServiceProviderSingletonAbstractBaseFacet",
    );

    // Add some S_PUB32 records. Put records out-of-order, with respect to their segment:offset,
    // so that we test the sorting code.
    sb.pub32(0, 100, 1, "main");
    sb.pub32(0, 200, 1, "memset");
    sb.pub32(0, 40, 1, "memcpy");
    sb.pub32(0, 30, 1, "CreateWindowEx");

    sb.buffer
}

#[test]
fn build_and_search_globals() {
    println!();

    let gss = GlobalSymbolStream::new(build_test_gss());
    println!("GSS:\n{:?}", &gss.stream_data.hex_dump());

    let indexes = build_global_symbols_index(&gss.stream_data, NUM_BUCKETS).unwrap();

    {
        let gsi =
            GlobalSymbolIndex::parse(NUM_BUCKETS, indexes.global_symbol_index_stream_data).unwrap();

        // Check consistency of name hashes
        gsi.names().check_hashes(&gss).unwrap();

        println!("Dumping names from GSI:");
        for name_sym in gsi.names().iter(&gss) {
            println!("{:?}", name_sym);
        }

        let gsi_names = gsi.names();
        assert!(gsi_names
            .find_symbol(&gss, "bad_name_not_found".into())
            .unwrap()
            .is_none());
        assert!(gsi_names.find_symbol(&gss, "FOO".into()).unwrap().is_some());
        assert!(gsi_names.find_symbol(&gss, "BAR".into()).unwrap().is_some());
        assert!(gsi_names
            .find_symbol(
                &gss,
                "AugmentedMultiThreadedSymbolExpanderServiceProviderSingletonAbstractBaseFacet"
                    .into()
            )
            .unwrap()
            .is_some());
    }

    {
        let psi =
            PublicSymbolIndex::parse(NUM_BUCKETS, indexes.public_symbol_index_stream_data).unwrap();

        // Check consistency of name hashes
        psi.names().check_hashes(&gss).unwrap();

        psi.check_consistency(&gss).unwrap();

        assert!(psi
            .find_symbol_by_name(&gss, "bad_name_not_found".into())
            .unwrap()
            .is_none());

        {
            let memset = psi
                .find_symbol_by_name(&gss, "memset".into())
                .unwrap()
                .unwrap();
            assert_eq!(memset.name, "memset");
            assert_eq!(memset.fixed.offset_segment.offset(), 200);
        }

        {
            let memcpy = psi.find_symbol_by_addr(&gss, 1, 40).unwrap().unwrap();
            assert_eq!(memcpy.0.name, "memcpy");
        }
    }
}

#[test]
fn empty_psi() {
    let gss = GlobalSymbolStream::empty();
    let psi = PublicSymbolIndex::parse(NUM_BUCKETS, Vec::new()).unwrap();
    psi.check_consistency(&gss).unwrap();
}

#[test]
fn empty_gsi() {
    let gss = GlobalSymbolStream::empty();
    let gsi = GlobalSymbolIndex::parse(NUM_BUCKETS, Vec::new()).unwrap();
    assert!(gsi
        .find_symbol(&gss, bstr::BStr::new("none"))
        .unwrap()
        .is_none());

    // Check bad offset: Outside of bounds
    assert!(gss.get_sym_at(0xbadbad).is_err());

    // Check bad offset: The slice operation succeeds, but the symbol cannot be decoded.
    assert!(gss.get_sym_at(0).is_err());

    assert_eq!(gss.iter_syms().count(), 0);
}

#[test]
fn gss_get_pub32_wrong_type() {
    let mut sb = SymBuilder::default();
    sb.udt(TypeIndex(0x1001), "FOO");

    let gss = sb.finish();

    // Symbol exists, but has wrong type.
    assert!(gss.get_pub32_at(0).is_err());
}

#[test]
fn gss_get_pub32_invalid_symbol() {
    let mut sb = SymBuilder::default();

    // S_PUB32 record with invalid contents
    let _e = sb.start_record(SymKind::S_PUB32);
    sb.end_record();

    let gss = sb.finish();

    // Found record at offset, but it could not be decoded as S_PUB32 because its contents are bogus.
    assert!(gss.get_pub32_at(0).is_err());
}

```

`pdb/src/guid.rs`:

```rs
//! Standard Windows type

use std::fmt::Debug;
use uuid::Uuid;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U16, U32};

/// Standard Windows type
#[repr(C)]
#[derive(Clone, Eq, PartialEq, IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct GuidLe {
    pub data1: U32<LE>,
    pub data2: U16<LE>,
    pub data3: U16<LE>,
    pub data4: [u8; 8],
}

impl GuidLe {
    /// Convert the on-disk format to in-memory format.
    pub fn get(&self) -> Uuid {
        Uuid::from_fields(
            self.data1.get(),
            self.data2.get(),
            self.data3.get(),
            &self.data4,
        )
    }
}

impl From<&Uuid> for GuidLe {
    fn from(uuid: &Uuid) -> Self {
        let f = uuid.as_fields();
        GuidLe {
            data1: U32::new(f.0),
            data2: U16::new(f.1),
            data3: U16::new(f.2),
            data4: *f.3,
        }
    }
}

```

`pdb/src/hash.rs`:

```rs
//! MSVC hash algorithms

use zerocopy::{FromBytes, LE, U16, U32};

#[cfg(test)]
use pretty_hex::PrettyHex;

/// Computes a 32-bit hash. This produces the same results as the hash function used in the
/// MSVC PDB reader library.
///
/// This is a port of the `LHashPbCb` function.
///
/// # WARNING! WARNING! WARNING!
///
/// This is a **VERY POOR HASH FUNCTION** and it should not be used for *ANY* new code. This
/// function should only be used for compatibility with PDB data structures.
///
/// # References
///
/// * [`misc.h](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/include/misc.h#L15)
pub fn hash_mod_u32(pb: &[u8], m: u32) -> u32 {
    hash_u32(pb) % m
}

/// Computes a 32-bit hash, but does not compute a remainder (modulus).
#[inline(never)]
pub fn hash_u32(mut pb: &[u8]) -> u32 {
    let mut h: u32 = 0;
    if let Ok((u32s, tail)) = <[U32<LE>]>::ref_from_prefix(pb) {
        for u in u32s.iter() {
            h ^= u.get();
        }
        pb = tail;
    }

    // The tail is handled differently.
    if let Ok((tail_u16, rest)) = <U16<LE>>::read_from_prefix(pb) {
        h ^= tail_u16.get() as u32;
        pb = rest;
    }

    debug_assert!(pb.len() == 0 || pb.len() == 1);

    if !pb.is_empty() {
        h ^= pb[0] as u32;
    }

    h |= 0x20202020;
    h ^= h >> 11;
    h ^ (h >> 16)
}

/// Computes a 16-bit hash
///
/// This is a port of the `HashPbCb` function.
pub fn hash_mod_u16(pb: &[u8], m: u32) -> u16 {
    hash_mod_u32(pb, m) as u16
}

#[test]
fn test_hash() {
    static INPUTS: &[(u32, &[u8])] = &[
        (0x00000c09, b""),
        (0x00000c09, b" "),
        (0x00000c09, b"  "),
        (0x00000c09, b"   "),
        (0x00000c09, b"    "),
        (0x00019fe2, b"hello"),
        (0x00019fe2, b"HELLO"),
        (0x0003c00b, b"Hello, World"),
        (0x0003c00b, b"hello, world"),
        (0x000068e2, b"hello_world::main"),
        (0x0000b441, b"std::vector<std::basic_string<wchar_t>>"),
        (0x000372ae, b"__chkstk"),
        (0x0001143b, b"WelsEmms"),
        (0x00000c0a, &[1]),
        (0x00000e0a, &[1, 2]),
        (0x00000e0b, &[1, 2, 3]),
        (0x00038b6b, &[1, 2, 3, 4]),
        (0x00038b70, &[1, 2, 3, 4, 5]),
        (0x00038d70, &[1, 2, 3, 4, 5, 6]),
        (0x00038d69, &[1, 2, 3, 4, 5, 6, 7]),
        (0x00019789, &[1, 2, 3, 4, 5, 6, 7, 8]),
        (0x00019790, &[1, 2, 3, 4, 5, 6, 7, 8, 9]),
        (0x00019191, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),
        (0x0001918a, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),
        (0x000313ed, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),
        (0x000313f8, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]),
        (0x000214eb, &[5, 6, 7, 8]),
    ];

    for &(expected_output, input) in INPUTS.iter() {
        let m = 0x3_ffff;
        let actual_output = hash_mod_u32(input, m);
        assert_eq!(
            expected_output, actual_output,
            "expected: 0x{expected_output:08x}, actual: 0x{actual_output:08x}, input: {input:02x?}"
        );
    }
}

/// Computes a hash code using an algorithm equivalent to the `SigForPbCb` function.
/// This is a CRC-32 checksum with an initial value of `sig`.
///
/// See `SigForPbCb` in <https://github.com/microsoft/microsoft-pdb/blob/master/langapi/shared/crc32.h>
pub fn hash_sig(pb: &[u8], mut sig: u32) -> u32 {
    static RGCRC: [u32; 0x100] = [
        0x00000000, 0x77073096, 0xEE0E612C, 0x990951BA, 0x076DC419, 0x706AF48F, 0xE963A535,
        0x9E6495A3, 0x0EDB8832, 0x79DCB8A4, 0xE0D5E91E, 0x97D2D988, 0x09B64C2B, 0x7EB17CBD,
        0xE7B82D07, 0x90BF1D91, 0x1DB71064, 0x6AB020F2, 0xF3B97148, 0x84BE41DE, 0x1ADAD47D,
        0x6DDDE4EB, 0xF4D4B551, 0x83D385C7, 0x136C9856, 0x646BA8C0, 0xFD62F97A, 0x8A65C9EC,
        0x14015C4F, 0x63066CD9, 0xFA0F3D63, 0x8D080DF5, 0x3B6E20C8, 0x4C69105E, 0xD56041E4,
        0xA2677172, 0x3C03E4D1, 0x4B04D447, 0xD20D85FD, 0xA50AB56B, 0x35B5A8FA, 0x42B2986C,
        0xDBBBC9D6, 0xACBCF940, 0x32D86CE3, 0x45DF5C75, 0xDCD60DCF, 0xABD13D59, 0x26D930AC,
        0x51DE003A, 0xC8D75180, 0xBFD06116, 0x21B4F4B5, 0x56B3C423, 0xCFBA9599, 0xB8BDA50F,
        0x2802B89E, 0x5F058808, 0xC60CD9B2, 0xB10BE924, 0x2F6F7C87, 0x58684C11, 0xC1611DAB,
        0xB6662D3D, 0x76DC4190, 0x01DB7106, 0x98D220BC, 0xEFD5102A, 0x71B18589, 0x06B6B51F,
        0x9FBFE4A5, 0xE8B8D433, 0x7807C9A2, 0x0F00F934, 0x9609A88E, 0xE10E9818, 0x7F6A0DBB,
        0x086D3D2D, 0x91646C97, 0xE6635C01, 0x6B6B51F4, 0x1C6C6162, 0x856530D8, 0xF262004E,
        0x6C0695ED, 0x1B01A57B, 0x8208F4C1, 0xF50FC457, 0x65B0D9C6, 0x12B7E950, 0x8BBEB8EA,
        0xFCB9887C, 0x62DD1DDF, 0x15DA2D49, 0x8CD37CF3, 0xFBD44C65, 0x4DB26158, 0x3AB551CE,
        0xA3BC0074, 0xD4BB30E2, 0x4ADFA541, 0x3DD895D7, 0xA4D1C46D, 0xD3D6F4FB, 0x4369E96A,
        0x346ED9FC, 0xAD678846, 0xDA60B8D0, 0x44042D73, 0x33031DE5, 0xAA0A4C5F, 0xDD0D7CC9,
        0x5005713C, 0x270241AA, 0xBE0B1010, 0xC90C2086, 0x5768B525, 0x206F85B3, 0xB966D409,
        0xCE61E49F, 0x5EDEF90E, 0x29D9C998, 0xB0D09822, 0xC7D7A8B4, 0x59B33D17, 0x2EB40D81,
        0xB7BD5C3B, 0xC0BA6CAD, 0xEDB88320, 0x9ABFB3B6, 0x03B6E20C, 0x74B1D29A, 0xEAD54739,
        0x9DD277AF, 0x04DB2615, 0x73DC1683, 0xE3630B12, 0x94643B84, 0x0D6D6A3E, 0x7A6A5AA8,
        0xE40ECF0B, 0x9309FF9D, 0x0A00AE27, 0x7D079EB1, 0xF00F9344, 0x8708A3D2, 0x1E01F268,
        0x6906C2FE, 0xF762575D, 0x806567CB, 0x196C3671, 0x6E6B06E7, 0xFED41B76, 0x89D32BE0,
        0x10DA7A5A, 0x67DD4ACC, 0xF9B9DF6F, 0x8EBEEFF9, 0x17B7BE43, 0x60B08ED5, 0xD6D6A3E8,
        0xA1D1937E, 0x38D8C2C4, 0x4FDFF252, 0xD1BB67F1, 0xA6BC5767, 0x3FB506DD, 0x48B2364B,
        0xD80D2BDA, 0xAF0A1B4C, 0x36034AF6, 0x41047A60, 0xDF60EFC3, 0xA867DF55, 0x316E8EEF,
        0x4669BE79, 0xCB61B38C, 0xBC66831A, 0x256FD2A0, 0x5268E236, 0xCC0C7795, 0xBB0B4703,
        0x220216B9, 0x5505262F, 0xC5BA3BBE, 0xB2BD0B28, 0x2BB45A92, 0x5CB36A04, 0xC2D7FFA7,
        0xB5D0CF31, 0x2CD99E8B, 0x5BDEAE1D, 0x9B64C2B0, 0xEC63F226, 0x756AA39C, 0x026D930A,
        0x9C0906A9, 0xEB0E363F, 0x72076785, 0x05005713, 0x95BF4A82, 0xE2B87A14, 0x7BB12BAE,
        0x0CB61B38, 0x92D28E9B, 0xE5D5BE0D, 0x7CDCEFB7, 0x0BDBDF21, 0x86D3D2D4, 0xF1D4E242,
        0x68DDB3F8, 0x1FDA836E, 0x81BE16CD, 0xF6B9265B, 0x6FB077E1, 0x18B74777, 0x88085AE6,
        0xFF0F6A70, 0x66063BCA, 0x11010B5C, 0x8F659EFF, 0xF862AE69, 0x616BFFD3, 0x166CCF45,
        0xA00AE278, 0xD70DD2EE, 0x4E048354, 0x3903B3C2, 0xA7672661, 0xD06016F7, 0x4969474D,
        0x3E6E77DB, 0xAED16A4A, 0xD9D65ADC, 0x40DF0B66, 0x37D83BF0, 0xA9BCAE53, 0xDEBB9EC5,
        0x47B2CF7F, 0x30B5FFE9, 0xBDBDF21C, 0xCABAC28A, 0x53B39330, 0x24B4A3A6, 0xBAD03605,
        0xCDD70693, 0x54DE5729, 0x23D967BF, 0xB3667A2E, 0xC4614AB8, 0x5D681B02, 0x2A6F2B94,
        0xB40BBE37, 0xC30C8EA1, 0x5A05DF1B, 0x2D02EF8D,
    ];

    for &b in pb.iter() {
        sig = (sig >> 8) ^ RGCRC[((sig & 0xff) ^ (b as u32)) as usize];
    }
    sig
}

#[test]
fn test_hash_sig() {
    static CASES: &[(u32, u32, &[u8])] = &[
        // expected_hash, input_sig, input_bytes
        (0x00000000, 0x00000000, &[]),
        (0x01234567, 0x01234567, &[]),
        (0x57eccb91, 0x00000000, b"hello, world!"),
        (0x29b1c6ec, 0xabababab, b"hello, world!"),
        (0x2b4468c3, 0x00000000, b"hello!"),
        (0x102f0bec, 0xabababab, b"hello!"),
    ];

    for &(expected_hash, input_sig, input_bytes) in CASES.iter() {
        let actual_hash = hash_sig(input_bytes, input_sig);
        assert_eq!(
            actual_hash,
            expected_hash,
            "actual: 0x{actual_hash:08x}, expected: 0x{expected_hash:08x}, input_sig: 0x{input_sig:08x}, bytes: {:?}",
            input_bytes.hex_dump()
        );
    }
}

/// Computes a CRC-32 with an initializer value, then computes the modulus of it.
pub fn hash_sig_mod(pb: &[u8], sig: u32, modulus: u32) -> u32 {
    let h = hash_sig(pb, sig);
    h % modulus
}

```

`pdb/src/lib.rs`:

```rs
//! Reads and writes Program Database (PDB) files.
//!
//! # References
//! * <https://llvm.org/docs/PDB/index.html>
//! * <https://github.com/microsoft/microsoft-pdb>

#![forbid(unused_must_use)]
#![forbid(unsafe_code)]
#![warn(missing_docs)]
#![allow(clippy::collapsible_if)]
#![allow(clippy::single_match)]
#![allow(clippy::manual_flatten)]
#![allow(clippy::needless_lifetimes)]
#![allow(clippy::needless_late_init)]

pub mod container;
pub mod dbi;
pub mod encoder;
pub mod globals;
pub mod guid;
pub mod hash;
pub mod lines;
pub mod modi;
pub mod taster;
pub use ::uuid::Uuid;
pub use ms_pdb_msf as msf;
pub use ms_pdb_msfz as msfz;
mod embedded_sources;
pub mod names;
pub mod parser;
pub mod pdbi;
mod stream_index;
pub mod syms;
pub mod tpi;
pub mod types;
pub mod utils;
pub mod writer;

pub use bstr::BStr;
pub use container::{Container, StreamReader};
pub use msfz::StreamData;
pub use stream_index::{Stream, StreamIndexIsNilError, StreamIndexU16, NIL_STREAM_INDEX};
pub use sync_file::{RandomAccessFile, ReadAt, WriteAt};

use anyhow::bail;
use globals::gsi::GlobalSymbolIndex;
use globals::gss::GlobalSymbolStream;
use globals::psi::PublicSymbolIndex;
use names::{NameIndex, NamesStream};
use std::cell::OnceCell;
use std::fmt::Debug;
use std::fs::File;
use std::path::Path;
use syms::{Pub, Sym};
use zerocopy::{FromZeros, IntoBytes};

#[cfg(test)]
#[static_init::dynamic]
static INIT_LOGGER: () = {
    tracing_subscriber::fmt()
        .with_ansi(false)
        .with_test_writer()
        .with_file(true)
        .with_line_number(true)
        .with_max_level(tracing::Level::DEBUG)
        .compact()
        .without_time()
        .finish();
};

/// Allows reading the contents of a PDB file.
///
/// This type provides read-only access. It does not provide any means to modify a PDB file or
/// to create a new one.
pub struct Pdb<F = sync_file::RandomAccessFile> {
    container: Container<F>,

    /// The header of the DBI Stream. The DBI Stream contains many of the important data structures
    /// for PDB, or has pointers (stream indexes) for them. Nearly all programs that read PDBs
    /// need to read the DBI, so we always load the header.
    dbi_header: dbi::DbiStreamHeader,
    dbi_substreams: dbi::DbiSubstreamRanges,

    pdbi: pdbi::PdbiStream,
    names: OnceCell<NamesStream<Vec<u8>>>,

    tpi_header: OnceCell<tpi::CachedTypeStreamHeader>,
    ipi_header: OnceCell<tpi::CachedTypeStreamHeader>,

    /// Cached contents of DBI Modules Substream.
    dbi_modules_cell: OnceCell<dbi::ModInfoSubstream<Vec<u8>>>,
    /// Cached contents of DBI Sources Substream.
    dbi_sources_cell: OnceCell<Vec<u8>>,

    gss: OnceCell<Box<GlobalSymbolStream>>,
    gsi: OnceCell<Box<GlobalSymbolIndex>>,
    psi: OnceCell<Box<PublicSymbolIndex>>,
}

#[derive(Copy, Clone, Eq, PartialEq)]
enum AccessMode {
    Read,
    ReadWrite,
}

impl<F: ReadAt> Pdb<F> {
    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file. Allows read/write access, if using an MSF container format.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    fn from_file_access(file: F, access_mode: AccessMode) -> anyhow::Result<Box<Self>> {
        use crate::taster::{what_flavor, Flavor};

        let Some(flavor) = what_flavor(&file)? else {
            bail!("The file is not a recognized PDB or PDZ format.");
        };

        let container = match (flavor, access_mode) {
            (Flavor::PortablePdb, _) => bail!("Portable PDBs are not supported."),
            (Flavor::Pdb, AccessMode::Read) => Container::Msf(msf::Msf::open_with_file(file)?),
            (Flavor::Pdb, AccessMode::ReadWrite) => {
                Container::Msf(msf::Msf::modify_with_file(file)?)
            }
            (Flavor::Pdz, AccessMode::Read) => Container::Msfz(msfz::Msfz::from_file(file)?),
            (Flavor::Pdz, AccessMode::ReadWrite) => {
                bail!("The MSFZ file format is read-only.")
            }
        };

        let dbi_header = dbi::read_dbi_stream_header(&container)?;
        let stream_len = container.stream_len(Stream::DBI.into());
        let dbi_substreams = if stream_len != 0 {
            dbi::DbiSubstreamRanges::from_sizes(&dbi_header, stream_len as usize)?
        } else {
            dbi::DbiSubstreamRanges::default()
        };

        let pdbi_stream_data = container.read_stream_to_vec(Stream::PDB.into())?;
        let pdbi = pdbi::PdbiStream::parse(&pdbi_stream_data)?;

        Ok(Box::new(Self {
            container,
            dbi_header,
            dbi_substreams,
            pdbi,
            tpi_header: OnceCell::new(),
            ipi_header: OnceCell::new(),
            names: OnceCell::new(),
            dbi_modules_cell: Default::default(),
            dbi_sources_cell: Default::default(),
            gss: OnceCell::new(),
            gsi: OnceCell::new(),
            psi: OnceCell::new(),
        }))
    }

    /// Gets access to the PDB Information Stream.
    ///
    /// This loads the PDBI on-demand. The PDBI is usually fairly small.
    pub fn pdbi(&self) -> &pdbi::PdbiStream {
        &self.pdbi
    }

    /// Gets access to the Named Streams table.
    pub fn named_streams(&self) -> &pdbi::NamedStreams {
        &self.pdbi.named_streams
    }

    /// Gets mutable access to the Named Streams table.
    pub fn named_streams_mut(&mut self) -> &mut pdbi::NamedStreams {
        &mut self.pdbi.named_streams
    }

    /// Searches the Named Streams table for a stream with a given name.
    /// Returns `None` if the stream is not found.
    pub fn named_stream(&self, name: &str) -> Option<u32> {
        self.pdbi.named_streams().get(name)
    }

    /// Searches the Named Streams table for a stream with a given name.
    /// Returns an error if the stream is not found.
    pub fn named_stream_err(&self, name: &str) -> anyhow::Result<u32> {
        if let Some(s) = self.pdbi.named_streams().get(name) {
            Ok(s)
        } else {
            anyhow::bail!("There is no stream with the name {:?}.", name);
        }
    }

    /// The header of the DBI Stream.
    pub fn dbi_header(&self) -> &dbi::DbiStreamHeader {
        &self.dbi_header
    }

    /// The byte ranges of the DBI substreams.
    pub fn dbi_substreams(&self) -> &dbi::DbiSubstreamRanges {
        &self.dbi_substreams
    }

    /// Gets the TPI Stream Header.
    ///
    /// This loads the TPI Stream Header on-demand. This does not load the rest of the TPI Stream.
    pub fn tpi_header(&self) -> anyhow::Result<&tpi::CachedTypeStreamHeader> {
        self.tpi_or_ipi_header(Stream::TPI, &self.tpi_header)
    }

    /// Gets the IPI Stream Header.
    ///
    /// This loads the IPI Stream Header on-demand. This does not load the rest of the TPI Stream.
    pub fn ipi_header(&self) -> anyhow::Result<&tpi::CachedTypeStreamHeader> {
        self.tpi_or_ipi_header(Stream::IPI, &self.ipi_header)
    }

    fn tpi_or_ipi_header<'s>(
        &'s self,
        stream: Stream,
        cell: &'s OnceCell<tpi::CachedTypeStreamHeader>,
    ) -> anyhow::Result<&'s tpi::CachedTypeStreamHeader> {
        get_or_init_err(cell, || {
            let r = self.get_stream_reader(stream.into())?;
            let mut header = tpi::TypeStreamHeader::new_zeroed();
            let header_bytes = header.as_mut_bytes();
            let bytes_read = r.read_at(header_bytes, 0)?;
            if bytes_read == 0 {
                // This stream is zero-length.
                return Ok(tpi::CachedTypeStreamHeader { header: None });
            }

            if bytes_read < header_bytes.len() {
                bail!(
                    "The type stream (stream {}) does not contain enough data for a valid header.",
                    stream
                );
            }

            Ok(tpi::CachedTypeStreamHeader {
                header: Some(header),
            })
        })
    }

    /// Gets the Names Stream
    ///
    /// This loads the Names Stream on-demand.
    pub fn names(&self) -> anyhow::Result<&NamesStream<Vec<u8>>> {
        get_or_init_err(&self.names, || {
            if let Some(stream) = self.named_stream(names::NAMES_STREAM_NAME) {
                let stream_data = self.read_stream_to_vec(stream)?;
                Ok(NamesStream::parse(stream_data)?)
            } else {
                let stream_data = names::EMPTY_NAMES_STREAM_DATA.to_vec();
                Ok(NamesStream::parse(stream_data)?)
            }
        })
    }

    /// Gets a name from the Names Stream.
    pub fn get_name(&self, offset: NameIndex) -> anyhow::Result<&BStr> {
        let names = self.names()?;
        names.get_string(offset)
    }

    /// The binding key that associates this PDB with a given PE executable.
    pub fn binding_key(&self) -> BindingKey {
        let pdbi = self.pdbi();
        pdbi.binding_key()
    }

    /// Checks whether this PDB has a given feature enabled.
    pub fn has_feature(&self, feature_code: pdbi::FeatureCode) -> bool {
        self.pdbi.has_feature(feature_code)
    }

    /// Indicates that this PDB was built using the "Mini PDB" option, i.e. `/DEBUG:FASTLINK`.
    pub fn mini_pdb(&self) -> bool {
        self.has_feature(pdbi::FeatureCode::MINI_PDB)
    }

    /// Gets a reference to the Global Symbol Stream (GSS). This loads the GSS on-demand.
    #[inline]
    pub fn gss(&self) -> anyhow::Result<&GlobalSymbolStream> {
        if let Some(gss) = self.gss.get() {
            Ok(gss)
        } else {
            self.gss_slow()
        }
    }

    /// Gets a reference to the Global Symbol Stream (GSS). This loads the GSS on-demand.
    #[inline(never)]
    fn gss_slow(&self) -> anyhow::Result<&GlobalSymbolStream> {
        let box_ref = get_or_init_err(&self.gss, || -> anyhow::Result<Box<GlobalSymbolStream>> {
            Ok(Box::new(self.read_gss()?))
        })?;
        Ok(box_ref)
    }

    /// If the GSS has been loaded by using the `gss()` function, then this method frees it.
    pub fn gss_drop(&mut self) {
        self.gss.take();
    }

    /// Gets a reference to the Global Symbol Index (GSI). This loads the GSI on-demand.
    #[inline(never)]
    pub fn gsi(&self) -> anyhow::Result<&GlobalSymbolIndex> {
        if let Some(gsi) = self.gsi.get() {
            Ok(gsi)
        } else {
            self.gsi_slow()
        }
    }

    #[inline(never)]
    fn gsi_slow(&self) -> anyhow::Result<&GlobalSymbolIndex> {
        let box_ref = get_or_init_err(&self.gsi, || -> anyhow::Result<Box<GlobalSymbolIndex>> {
            Ok(Box::new(self.read_gsi()?))
        })?;
        Ok(box_ref)
    }

    /// If the GSI has been loaded by using the `gsi()` function, then this method frees it.
    pub fn gsi_drop(&mut self) {
        self.gsi.take();
    }

    /// Gets a reference to the Public Symbol Index (PSI). This loads the PSI on-demand.
    #[inline]
    pub fn psi(&self) -> anyhow::Result<&PublicSymbolIndex> {
        if let Some(psi) = self.psi.get() {
            Ok(psi)
        } else {
            self.psi_slow()
        }
    }

    #[inline(never)]
    fn psi_slow(&self) -> anyhow::Result<&PublicSymbolIndex> {
        let box_ref = get_or_init_err(&self.psi, || -> anyhow::Result<Box<PublicSymbolIndex>> {
            Ok(Box::new(self.read_psi()?))
        })?;
        Ok(box_ref)
    }

    /// If the PSI has been loaded by using the `psi()` function, then this method frees it.
    pub fn psi_drop(&mut self) {
        self.psi.take();
    }

    /// Searches for an `S_PUB32` symbol by name.
    pub fn find_public_by_name(&self, name: &BStr) -> anyhow::Result<Option<Pub<'_>>> {
        let gss = self.gss()?;
        let psi = self.psi()?;
        psi.find_symbol_by_name(gss, name)
    }

    /// Searches for a global symbol symbol by name.
    ///
    /// This uses the Global Symbol Index (GSI). This index _does not_ contain `S_PUB32` records.
    /// Use `find_public_by_name` to search for `S_PUB32` records.
    pub fn find_global_by_name(&self, name: &'_ BStr) -> anyhow::Result<Option<Sym<'_>>> {
        let gss = self.gss()?;
        let gsi = self.gsi()?;
        gsi.find_symbol(gss, name)
    }

    /// Writes any changes that have been buffered in memory to disk. However, this does not commit
    /// the changes. It is still necessary to call the `commit()` method.
    ///
    /// The return value indicates whether any changes were written to disk. `Ok(true)` indicates
    /// that some change were written to disk.  `Ok(false)` indicates that there were no buffered
    /// changes and nothing has been written to disk.
    pub fn flush_all(&mut self) -> anyhow::Result<bool>
    where
        F: WriteAt,
    {
        let mut any = false;

        if self.pdbi.named_streams.modified {
            let pdbi_data = self.pdbi.to_bytes()?;
            let mut w = self.msf_mut_err()?.write_stream(Stream::PDB.into())?;
            w.set_contents(&pdbi_data)?;
            self.pdbi.named_streams.modified = false;
            any = true;
        }

        Ok(any)
    }
}

fn get_or_init_err<T, E, F: FnOnce() -> Result<T, E>>(cell: &OnceCell<T>, f: F) -> Result<&T, E> {
    if let Some(value) = cell.get() {
        return Ok(value);
    }

    match f() {
        Ok(value) => {
            let _ = cell.set(value);
            Ok(cell.get().unwrap())
        }
        Err(e) => Err(e),
    }
}

impl Pdb<RandomAccessFile> {
    /// Opens a PDB file.
    pub fn open(file_name: &Path) -> anyhow::Result<Box<Pdb<RandomAccessFile>>> {
        let f = File::open(file_name)?;
        let random_file = RandomAccessFile::from(f);
        Self::from_file_access(random_file, AccessMode::Read)
    }

    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    pub fn open_from_file(file: File) -> anyhow::Result<Box<Self>> {
        let random_file = RandomAccessFile::from(file);
        Self::from_file_access(random_file, AccessMode::Read)
    }

    /// Opens a PDB file for editing. The file must use the MSF container format.
    pub fn modify(filename: &Path) -> anyhow::Result<Box<Pdb<sync_file::RandomAccessFile>>> {
        let file = File::options().read(true).write(true).open(filename)?;
        let random_file = sync_file::RandomAccessFile::from(file);
        Self::from_file_access(random_file, AccessMode::ReadWrite)
    }

    /// Opens an existing PDB file for read/write access, given a file name.
    ///
    /// The file _must_ use the MSF container format. MSFZ is not supported for read/write access.
    pub fn modify_from_file(file: File) -> anyhow::Result<Box<Self>> {
        let random_file = RandomAccessFile::from(file);
        Self::from_file_access(random_file, AccessMode::ReadWrite)
    }
}

impl<F: ReadAt> Pdb<F> {
    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    pub fn open_from_random_file(random_file: F) -> anyhow::Result<Box<Self>> {
        Self::from_file_access(random_file, AccessMode::Read)
    }

    /// Opens an existing PDB file for read/write access, given a file name.
    ///
    /// The file _must_ using the MSF container format. MSFZ is not supported for read/write access.
    pub fn modify_from_random_file(random_file: F) -> anyhow::Result<Box<Self>> {
        Self::from_file_access(random_file, AccessMode::ReadWrite)
    }
}

impl<F> std::ops::Deref for Pdb<F> {
    type Target = Container<F>;

    fn deref(&self) -> &Self::Target {
        &self.container
    }
}

impl<F> std::ops::DerefMut for Pdb<F> {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.container
    }
}

/// This is the key used to associate a given PE executable (DLL or EXE) with a PDB.
/// All values come from the PDBI stream.
#[derive(Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct BindingKey {
    /// The GUID. When MSVC tools are run in deterministic mode, this value is a hash of the PE
    /// image, rather than being assigned using an RNG.
    pub guid: uuid::Uuid,
    /// The age of the executable. This is incremented every time the DLL + PDB are modified.
    pub age: u32,
}

impl Debug for BindingKey {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        if self.age > 0x1000 {
            write!(f, "{:?} age 0x{:x}", self.guid, self.age)
        } else {
            write!(f, "{:?} age {}", self.guid, self.age)
        }
    }
}

```

`pdb/src/lines.rs`:

```rs
//! Decodes line information found in Module Streams.
//!
//! # References
//! * [/ZH (Hash algorithm for calculation of file checksum in debug info)](https://learn.microsoft.com/en-us/cpp/build/reference/zh?view=msvc-170)

mod checksum;
mod subsection;

pub use checksum::*;
pub use subsection::*;

use crate::names::NameIndex;
use crate::parser::{Parser, ParserError, ParserMut};
use crate::utils::iter::{HasRestLen, IteratorWithRangesExt};
use anyhow::{bail, Context};
use std::mem::{size_of, take};
use tracing::{trace, warn};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U16, U32};

/// Enumerates the kind of subsections found in C13 Line Data.
///
/// See `cvinfo.h`, `DEBUG_S_SUBSECTION_TYPE`.
#[derive(Copy, Clone, Eq, PartialEq)]
#[repr(transparent)]
pub struct SubsectionKind(pub u32);

macro_rules! subsections {
    ($( $(#[$a:meta])*  $name:ident = $value:expr;)*) => {
        impl SubsectionKind {
            $(
                $(#[$a])*
                #[allow(missing_docs)]
                pub const $name: SubsectionKind = SubsectionKind($value);
            )*
        }

        impl std::fmt::Debug for SubsectionKind {
            fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
                let s: &str = match *self {
                    $( SubsectionKind::$name => stringify!($name), )*
                    _ => return write!(fmt, "??(0x{:x})", self.0),
                };
                fmt.write_str(s)
            }
        }
    }
}

subsections! {
    SYMBOLS = 0xf1;
    /// Contains C13 Line Data
    LINES = 0xf2;
    STRING_TABLE = 0xf3;
    /// Contains file checksums and pointers to file names. For a given module, there should be
    /// at most one `FILE_CHECKSUMS` subsection.
    FILE_CHECKSUMS = 0xf4;

    FRAMEDATA = 0xF5;
    INLINEELINES = 0xF6;
    CROSSSCOPEIMPORTS = 0xF7;
    CROSSSCOPEEXPORTS = 0xF8;

    IL_LINES = 0xF9;
    FUNC_MDTOKEN_MAP = 0xFA;
    TYPE_MDTOKEN_MAP = 0xFB;
    MERGED_ASSEMBLYINPUT = 0xFC;

    COFF_SYMBOL_RVA = 0xFD;
}

/// Enables decoding of the line data stored in a Module Stream. This decodes the "C13 line data"
/// substream.
pub struct LineData<'a> {
    bytes: &'a [u8],
}

impl<'a> LineData<'a> {
    /// Use this to create a new decoder for the C13 line data. Usually, you want to pass the
    /// result of calling `ModiStreamData::c13_line_data_bytes()` to this function.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates subsections
    pub fn subsections(&self) -> SubsectionIter<'a> {
        SubsectionIter::new(self.bytes)
    }

    /// Finds the `FILE_CHECKSUMS` subsection. There should only be one.
    pub fn find_checksums_bytes(&self) -> Option<&'a [u8]> {
        for subsection in self.subsections() {
            if subsection.kind == SubsectionKind::FILE_CHECKSUMS {
                return Some(subsection.data);
            }
        }
        None
    }

    /// Finds the `FILE_CHECKSUMS` subsection. There should only be one.
    pub fn find_checksums(&self) -> Option<FileChecksumsSubsection<'a>> {
        let subsection_bytes = self.find_checksums_bytes()?;
        Some(FileChecksumsSubsection::new(subsection_bytes))
    }

    /// Iterates the `NameIndex` values that appear in this Line Data section.
    ///
    /// This may iterate the same `NameIndex` value more than once.
    pub fn iter_name_index<F>(&self, mut f: F) -> anyhow::Result<()>
    where
        F: FnMut(NameIndex),
    {
        if let Some(checksums) = self.find_checksums() {
            for subsection in self.subsections() {
                match subsection.kind {
                    SubsectionKind::LINES => {
                        let lines_subsection = LinesSubsection::parse(subsection.data)?;
                        for block in lines_subsection.blocks() {
                            let file = checksums.get_file(block.header.file_index.get())?;
                            let ni = file.header.name.get();
                            f(NameIndex(ni));
                        }
                    }
                    _ => {}
                }
            }
        } else {
            for subsection in self.subsections() {
                match subsection.kind {
                    SubsectionKind::LINES => {
                        bail!("This C13 Line Data substream contains LINES subsections, but does not contain a FILE_CHECKSUMS subsection.");
                    }
                    _ => {}
                }
            }
        };

        Ok(())
    }
}

/// Enables decoding of the line data stored in a Module Stream. This decodes the "C13 line data"
/// substream.
pub struct LineDataMut<'a> {
    bytes: &'a mut [u8],
}

impl<'a> LineDataMut<'a> {
    /// Initializes a new `LineDataMut`. This does not validate the contents of the data.
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates subsections, with mutable access.
    pub fn subsections_mut(&mut self) -> SubsectionIterMut<'_> {
        SubsectionIterMut::new(self.bytes)
    }

    /// Iterates through all of the name indexes stored within this Line Data.
    /// Remaps all entries using `f` as the remapping function.
    ///
    /// `NameIndex` values are found in the `FILE_CHECKSUMS` debug subsections. However, it is not
    /// possible to directly enumerate the entries stored within a `FILE_CHECKSUMS` subsection,
    /// because they are not at guaranteed positions. There may be gaps.
    ///
    /// To find the `NameIndex` values within each `FILE_CHECKSUMS` debug subsection, we first scan
    /// the `LINES` subsections that point to them, and use a `HashSet` to avoid modifying the
    /// same `NameIndex` more than once.
    pub fn remap_name_indexes<F>(&mut self, name_remapping: F) -> anyhow::Result<()>
    where
        F: Fn(NameIndex) -> anyhow::Result<NameIndex>,
    {
        for subsection in self.subsections_mut() {
            match subsection.kind {
                SubsectionKind::FILE_CHECKSUMS => {
                    let mut checksums = FileChecksumsSubsectionMut::new(subsection.data);
                    for checksum in checksums.iter_mut() {
                        // This `name_offset` value points into the Names stream (/names).
                        let old_name = NameIndex(checksum.header.name.get());
                        let new_name = name_remapping(old_name)
                            .with_context(|| format!("old_name: {old_name}"))?;
                        checksum.header.name = U32::new(new_name.0);
                    }
                }

                _ => {}
            }
        }

        Ok(())
    }
}

/// Represents one contribution. Each contribution consists of a sequence of variable-length
/// blocks.
///
/// Each `LINES` subsection represents one "contribution", which has a `ContributionHeader`,
/// followed by a sequence of blocks. Each block is a variable-length record.
pub struct LinesSubsection<'a> {
    /// The fixed-size header of the `Lines` subsection.
    pub contribution: &'a Contribution,
    /// Contains a sequence of variable-sized "blocks". Each block specifies a source file
    /// and a set of mappings from instruction offsets to line numbers within that source file.
    pub blocks_data: &'a [u8],
}

impl<'a> LinesSubsection<'a> {
    /// Parses the contribution header and prepares for iteration of blocks.
    pub fn parse(bytes: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(bytes);
        Ok(Self {
            contribution: p.get()?,
            blocks_data: p.into_rest(),
        })
    }

    /// Iterates through the line number blocks.
    pub fn blocks(&self) -> IterBlocks<'a> {
        IterBlocks {
            bytes: self.blocks_data,
            have_columns: self.contribution.have_columns(),
        }
    }
}

/// Represents one contribution. Each contribution consists of a sequence of variable-length
/// blocks.
///
/// Each `LINES` subsection represents one "contribution", which has a `ContributionHeader`,
/// followed by a sequence of blocks. Each block is a variable-length record.
pub struct LinesSubsectionMut<'a> {
    /// The fixed-size header of the `Lines` subsection.
    pub contribution: &'a mut Contribution,
    /// Contains a sequence of variable-sized "blocks". Each block specifies a source file
    /// and a set of mappings from instruction offsets to line numbers within that source file.
    pub blocks_data: &'a mut [u8],
}

impl<'a> LinesSubsectionMut<'a> {
    /// Parses the contribution header and prepares for iteration of blocks.
    pub fn parse(bytes: &'a mut [u8]) -> Result<Self, ParserError> {
        let mut p = ParserMut::new(bytes);
        Ok(Self {
            contribution: p.get_mut()?,
            blocks_data: p.into_rest(),
        })
    }

    /// Iterates through the line number blocks.
    pub fn blocks(&self) -> IterBlocks<'_> {
        IterBlocks {
            bytes: self.blocks_data,
            have_columns: self.contribution.have_columns(),
        }
    }

    /// Iterates through the line number blocks, with mutable access.
    pub fn blocks_mut(&mut self) -> IterBlocksMut<'_> {
        IterBlocksMut {
            bytes: self.blocks_data,
            have_columns: self.contribution.have_columns(),
        }
    }
}

/// Iterator state for `LinesSubsection::blocks`.
pub struct IterBlocks<'a> {
    bytes: &'a [u8],
    have_columns: bool,
}

impl<'a> HasRestLen for IterBlocks<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> Iterator for IterBlocks<'a> {
    type Item = Block<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.bytes);
        let Ok(header) = p.get::<BlockHeader>() else {
            warn!("failed to read BlockHeader");
            return None;
        };

        let block_size: usize = header.block_size.get() as usize;
        let Some(data_len) = block_size.checked_sub(size_of::<BlockHeader>()) else {
            warn!("invalid block; block_size is less than size of block header");
            return None;
        };

        trace!(
            file_index = header.file_index.get(),
            num_lines = header.num_lines.get(),
            block_size = header.block_size.get(),
            data_len,
            "block header"
        );

        let Ok(data) = p.bytes(data_len) else {
            warn!(
                needed_bytes = data_len,
                have_bytes = p.len(),
                "invalid block: need more bytes for block contents"
            );
            return None;
        };

        self.bytes = p.into_rest();
        Some(Block {
            header,
            data,
            have_columns: self.have_columns,
        })
    }
}

/// Iterator state for `LinesSubsection::blocks`.
pub struct IterBlocksMut<'a> {
    bytes: &'a mut [u8],
    have_columns: bool,
}

impl<'a> HasRestLen for IterBlocksMut<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> Iterator for IterBlocksMut<'a> {
    type Item = BlockMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = ParserMut::new(take(&mut self.bytes));
        let Ok(header) = p.get_mut::<BlockHeader>() else {
            warn!("failed to read BlockHeader");
            return None;
        };

        let block_size: usize = header.block_size.get() as usize;
        let Some(data_len) = block_size.checked_sub(size_of::<BlockHeader>()) else {
            warn!("invalid block; block_size is less than size of block header");
            return None;
        };

        trace!(
            "block header: file_index = {}, num_lines = {}, block_size = {}, data_len = {}",
            header.file_index.get(),
            header.num_lines.get(),
            header.block_size.get(),
            data_len
        );

        let Ok(data) = p.bytes_mut(data_len) else {
            warn!(
                "invalid block: need {} bytes for block contents, only have {}",
                data_len,
                p.len()
            );
            return None;
        };

        self.bytes = p.into_rest();
        Some(BlockMut {
            header,
            data,
            have_columns: self.have_columns,
        })
    }
}

/// One block of line data. Each block has a header which points to a source file. All of the line
/// locations within the block point to line numbers (and potentially column numbers) within that
/// source file.
pub struct Block<'a> {
    /// Fixed-size header for the block.
    pub header: &'a BlockHeader,
    /// If `true`, then this block has column numbers as well as line numbers.
    pub have_columns: bool,
    /// Contains the encoded line numbers, followed by column numbers. The number of entries is
    /// specified by `header.num_lines`.
    pub data: &'a [u8],
}

impl<'a> Block<'a> {
    /// Gets the line records for this block.
    pub fn lines(&self) -> &'a [LineRecord] {
        let num_lines = self.header.num_lines.get() as usize;
        if let Ok((lines, _)) = <[LineRecord]>::ref_from_prefix_with_elems(self.data, num_lines) {
            lines
        } else {
            warn!("failed to get lines_data for a block; wrong size");
            &[]
        }
    }

    /// Gets the column records for this block, if it has any.
    pub fn columns(&self) -> Option<&'a [ColumnRecord]> {
        if !self.have_columns {
            return None;
        }

        let num_lines = self.header.num_lines.get() as usize;
        let lines_size = num_lines * size_of::<LineRecord>();
        let Some(column_data) = self.data.get(lines_size..) else {
            warn!("failed to get column data for a block; wrong size");
            return None;
        };

        let Ok((columns, _)) = <[ColumnRecord]>::ref_from_prefix_with_elems(column_data, num_lines)
        else {
            warn!("failed to get column data for a block; byte size is wrong");
            return None;
        };

        Some(columns)
    }
}

/// One block of line data. Each block has a header which points to a source file. All of the line
/// locations within the block point to line numbers (and potentially column numbers) within that
/// source file.
pub struct BlockMut<'a> {
    /// Fixed-size header for the block.
    pub header: &'a mut BlockHeader,
    /// If `true`, then this block has column numbers as well as line numbers.
    pub have_columns: bool,
    /// Contains the encoded line numbers, followed by column numbers. The number of entries is
    /// specified by `header.num_lines`.
    pub data: &'a mut [u8],
}

/// A single line record
///
/// See `CV_Line_t` in `cvinfo.h`
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Clone)]
#[repr(C)]
pub struct LineRecord {
    /// The byte offset from the start of this contribution (in the instruction stream, not the
    /// Lines Data) for this line
    pub offset: U32<LE>,

    /// Encodes three bit-fields
    ///
    /// * Bits 0-23 are `line_num_start`. This is the 1-based starting line number within the source
    ///   file of this line record.
    /// * Bits 24-30 are `delta_line_end`. It specifies a value to add to line_num_start to find the
    ///   ending line. If this value is zero, then this line record encodes only a single line, not
    ///   a span of lines.
    /// * Bit 31 is the `statement` bit field. If set to 1, it indicates that this line record describes a statement.
    pub flags: U32<LE>,
}

impl LineRecord {
    /// The line number of this location. This value is 1-based.
    pub fn line_num_start(&self) -> u32 {
        self.flags.get() & 0x00_ff_ff_ff
    }

    /// If non-zero, then this indicates the delta in bytes within the source file from the start
    /// of the source location to the end of the source location.
    pub fn delta_line_end(&self) -> u8 {
        ((self.flags.get() >> 24) & 0x7f) as u8
    }

    /// True if this location points to a statement.
    pub fn statement(&self) -> bool {
        (self.flags.get() >> 31) != 0
    }
}

impl std::fmt::Debug for LineRecord {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(fmt, "+{} L{}", self.offset.get(), self.line_num_start())?;

        let delta_line_end = self.delta_line_end();
        if delta_line_end != 0 {
            write!(fmt, "..+{}", delta_line_end)?;
        }

        if self.statement() {
            write!(fmt, " S")?;
        }

        Ok(())
    }
}

/// A single column record
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
pub struct ColumnRecord {
    /// byte offset in a source line
    pub start_offset: U16<LE>,
    /// byte offset in a source line
    pub end_offset: U16<LE>,
}

#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
#[allow(missing_docs)]
pub struct Contribution {
    pub contribution_offset: U32<LE>,
    pub contribution_segment: U16<LE>,
    pub flags: U16<LE>,
    pub contribution_size: U32<LE>,
    // Followed by a sequence of block records. Each block is variable-length and begins with
    // BlockHeader.
}

impl Contribution {
    /// Indicates whether this block (contribution) also has column numbers.
    pub fn have_columns(&self) -> bool {
        (self.flags.get() & CV_LINES_HAVE_COLUMNS) != 0
    }
}

/// Bit flag for `Contribution::flags` field
pub const CV_LINES_HAVE_COLUMNS: u16 = 0x0001;

#[allow(missing_docs)]
pub struct LinesEntry<'a> {
    pub header: &'a Contribution,
    pub blocks: &'a [u8],
}

/// Header for a variable-length Block record.
///
/// Each block contains a sequence of line records, and optionally column records.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
pub struct BlockHeader {
    /// The byte offset into the file checksums subsection for this file.
    pub file_index: U32<LE>,
    /// The number of `LineRecord` entries that immediately follow this structure. Also, if the
    /// contribution header indicates that the contribution has column values, this specifies
    /// the number of column records that follow the file records.
    pub num_lines: U32<LE>,
    /// Size of the data for this block. This value includes the size of the block header itself,
    /// so the minimum value value is 12.
    pub block_size: U32<LE>,
    // Followed by [u8; block_size - 12]. This data contains [LineRecord; num_lines], optionally
    // followed by [ColumnRecord; num_lines].
}

/// Updates a C13 Line Data substream after NameIndex values have been updated and after
/// file lists for a given module have been rearranged (sorted).
pub fn fixup_c13_line_data(
    file_permutation: &[u32], // maps new-->old for files within a module
    sorted_names: &crate::names::NameIndexMapping,
    c13_line_data: &mut crate::lines::LineDataMut<'_>,
) -> anyhow::Result<()> {
    // maps old --> new, for the file_index values in DEBUG_S_LINES blocks
    let mut checksum_files_mapping: Vec<(u32, u32)> = Vec::with_capacity(file_permutation.len());

    for subsection in c13_line_data.subsections_mut() {
        match subsection.kind {
            SubsectionKind::FILE_CHECKSUMS => {
                let mut checksums = FileChecksumsSubsectionMut::new(subsection.data);
                let mut checksum_ranges = Vec::with_capacity(file_permutation.len());
                for (checksum_range, checksum) in checksums.iter_mut().with_ranges() {
                    // This `name_offset` value points into the Names stream (/names).
                    let old_name = NameIndex(checksum.header.name.get());
                    let new_name = sorted_names
                        .map_old_to_new(old_name)
                        .with_context(|| format!("old_name: {old_name}"))?;
                    checksum.header.name = U32::new(new_name.0);
                    checksum_ranges.push(checksum_range);
                }

                // Next, we are going to rearrange the FileChecksum records within this
                // section, using the permutation that was generated in dbi::sources::sort_sources().

                let mut new_checksums: Vec<u8> = Vec::with_capacity(subsection.data.len());
                for &old_file_index in file_permutation.iter() {
                    let old_range = checksum_ranges[old_file_index as usize].clone();
                    checksum_files_mapping
                        .push((old_range.start as u32, new_checksums.len() as u32));
                    let old_checksum_data = &subsection.data[old_range];
                    new_checksums.extend_from_slice(old_checksum_data);
                }
                checksum_files_mapping.sort_unstable();

                assert_eq!(new_checksums.len(), subsection.data.len());
                subsection.data.copy_from_slice(&new_checksums);
            }

            _ => {}
        }
    }

    // There is a data-flow dependency (on checksum_files_mapping) between these two loops; the
    // loops cannot be combined. The first loop builds checksum_files_mapping; the second loop
    // reads from it.

    for subsection in c13_line_data.subsections_mut() {
        match subsection.kind {
            SubsectionKind::LINES => {
                // We need to rewrite the file_index values within each line block.
                let mut lines = LinesSubsectionMut::parse(subsection.data)?;
                for block in lines.blocks_mut() {
                    let old_file_index = block.header.file_index.get();
                    match checksum_files_mapping
                        .binary_search_by_key(&old_file_index, |&(old, _new)| old)
                    {
                        Ok(i) => {
                            let (_old, new) = checksum_files_mapping[i];
                            block.header.file_index = U32::new(new);
                        }
                        Err(_) => {
                            bail!("DEBUG_S_LINES section contains invalid file index: {old_file_index}");
                        }
                    }
                }
            }

            _ => {}
        }
    }

    Ok(())
}

/// This special line number is part of the "Just My Code" MSVC compiler feature.
///
/// Debuggers that implement the "Just My Code" feature look for this constant when handling
/// "Step Into" requests. If the user asks to "step into" a function call, the debugger will look
/// up the line number of the start of the function. If the line number is `JMC_LINE_NO_STEP_INTO`,
/// then the debugger will _not_ step into the function. Instead, it will step over it.
///
/// This is useful for implementations of standard library functions, like
/// `std::vector<T>::size()`. Often calls to such functions are embedded in complex statements,
/// and the user wants to debug other parts of the complex statement, not the `size()` call.
///
/// # References
/// * <https://learn.microsoft.com/en-us/cpp/build/reference/jmc?view=msvc-170>
/// * <https://learn.microsoft.com/en-us/visualstudio/debugger/just-my-code>
pub const JMC_LINE_NO_STEP_INTO: u32 = 0xf00f00;

/// This special line number is part of the "Just My Code" MSVC compiler feature.
pub const JMC_LINE_FEE_FEE: u32 = 0xfeefee;

/// Returns true if `line` is a number that is used by the "Just My Code" MSVC compiler feature.
pub fn is_jmc_line(line: u32) -> bool {
    line == JMC_LINE_NO_STEP_INTO || line == JMC_LINE_FEE_FEE
}

```

`pdb/src/lines/checksum.rs`:

```rs
//! Code for the `FILE_CHECKSUMS` subsection.

use super::*;

/// The hash algorithm used for the checksum.
#[derive(
    Copy,
    Clone,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    IntoBytes,
    FromBytes,
    Unaligned,
    KnownLayout,
    Immutable,
)]
#[repr(transparent)]
pub struct ChecksumKind(pub u8);

impl ChecksumKind {
    /// No checksum at all
    pub const NONE: ChecksumKind = ChecksumKind(0);
    /// MD-5 checksum. See `/ZH:MD5` for MSVC.
    pub const MD5: ChecksumKind = ChecksumKind(1);
    /// SHA-1 checksum. See `/ZH:SHA1` for MSVC
    pub const SHA_1: ChecksumKind = ChecksumKind(2);
    /// SHA-256 checksum.  See `/ZH:SHA_256` for MSVC.
    pub const SHA_256: ChecksumKind = ChecksumKind(3);
}

impl std::fmt::Debug for ChecksumKind {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        static NAMES: [&str; 4] = ["NONE", "MD5", "SHA_1", "SHA_256"];

        if let Some(name) = NAMES.get(self.0 as usize) {
            f.write_str(name)
        } else {
            write!(f, "??({})", self.0)
        }
    }
}

#[test]
fn checksum_kind_debug() {
    assert_eq!(format!("{:?}", ChecksumKind::SHA_256), "SHA_256");
    assert_eq!(format!("{:?}", ChecksumKind(42)), "??(42)");
}

/// The File Checksums Subection
///
/// The file checksums subsection contains records for the source files referenced by Line Data.
pub struct FileChecksumsSubsection<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a [u8],
}

impl<'a> FileChecksumsSubsection<'a> {
    #[allow(missing_docs)]
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates the `FileChecksum` records within this subsection.
    pub fn iter(&self) -> FileChecksumIter<'a> {
        FileChecksumIter { bytes: self.bytes }
    }

    /// Given a file index, which is a byte offset into the `FileChecksums` section, gets a
    /// `FileChecksum` value.
    pub fn get_file(&self, file_index: u32) -> anyhow::Result<FileChecksum<'a>> {
        if let Some(b) = self.bytes.get(file_index as usize..) {
            if let Some(c) = FileChecksumIter::new(b).next() {
                Ok(c)
            } else {
                bail!("failed to decode FileChecksum record");
            }
        } else {
            bail!("file index is out of range of file checksums subsection");
        }
    }
}

/// Like `FileChecksums`, but with mutable access
pub struct FileChecksumsSubsectionMut<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a mut [u8],
}

impl<'a> HasRestLen for FileChecksumsSubsectionMut<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> FileChecksumsSubsectionMut<'a> {
    #[allow(missing_docs)]
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates the `FileChecksumMut` records within this subsection.
    pub fn iter_mut(&mut self) -> FileChecksumMutIter<'_> {
        FileChecksumMutIter { bytes: self.bytes }
    }

    /// Given a file index, which is a byte offset into the `FileChecksums` section, gets a
    /// `FileChecksumMut` value.
    pub fn get_file_mut(&mut self, file_index: u32) -> anyhow::Result<FileChecksumMut<'_>> {
        if let Some(b) = self.bytes.get_mut(file_index as usize..) {
            if let Some(c) = FileChecksumMutIter::new(b).next() {
                Ok(c)
            } else {
                bail!("failed to decode FileChecksum record");
            }
        } else {
            bail!("file index is out of range of file checksums subsection");
        }
    }
}

/// Points to a single file checksum record.
pub struct FileChecksum<'a> {
    /// The fixed-size header.
    pub header: &'a FileChecksumHeader,
    /// The checksum bytes.
    pub checksum_data: &'a [u8],
}

/// Points to a single file checksum record, with mutable access.
pub struct FileChecksumMut<'a> {
    /// The fixed-size header.
    pub header: &'a mut FileChecksumHeader,
    /// The checksum bytes.
    pub checksum_data: &'a mut [u8],
}

impl<'a> FileChecksum<'a> {
    /// Gets the `NameIndex` of the file name for this record. To dereference the `NameIndex value,
    /// use [`crate::names::NamesStream`].
    pub fn name(&self) -> NameIndex {
        NameIndex(self.header.name.get())
    }
}

/// The header at the start of a file checksum record.
///
/// Each file checksum record specifies the name of the file (using an offset into the Names Stream),
/// the kind of checksum (none, SHA1, SHA256, MD5, etc.), the size of the checksum, and the
/// checksum bytes.
///
/// The checksum record is variable-length.
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct FileChecksumHeader {
    /// Offset into the global string table (the `/names` stream) of the PDB.
    pub name: U32<LE>,

    /// The size in bytes of the checksum. The checksum bytes immediately follow the `FileChecksumHeader`.
    pub checksum_size: u8,

    /// The hash algorithm used for the checksum.
    pub checksum_kind: ChecksumKind,
}

/// Iterates FileChecksum values from a byte stream.
pub struct FileChecksumIter<'a> {
    /// The unparsed data
    pub bytes: &'a [u8],
}

impl<'a> HasRestLen for FileChecksumIter<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

/// Iterator state. Iterates `FileChecksumMut` values.
pub struct FileChecksumMutIter<'a> {
    /// The unparsed data
    pub bytes: &'a mut [u8],
}

impl<'a> HasRestLen for FileChecksumMutIter<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> FileChecksumIter<'a> {
    /// Starts a new iterator
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }
}

impl<'a> Iterator for FileChecksumIter<'a> {
    type Item = FileChecksum<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.bytes);
        let len_before = p.len();
        let header: &FileChecksumHeader = p.get().ok()?;
        let checksum_data = p.bytes(header.checksum_size as usize).ok()?;

        // Align to 4-byte boundaries.
        let record_len = len_before - p.len();
        let _ = p.skip((4 - (record_len & 3)) & 3);

        self.bytes = p.into_rest();
        Some(FileChecksum {
            header,
            checksum_data,
        })
    }
}

impl<'a> FileChecksumMutIter<'a> {
    /// Starts a new iterator
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }
}

impl<'a> Iterator for FileChecksumMutIter<'a> {
    type Item = FileChecksumMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = ParserMut::new(take(&mut self.bytes));
        let len_before = p.len();
        let header: &mut FileChecksumHeader = p.get_mut().ok()?;
        let checksum_data = p.bytes_mut(header.checksum_size as usize).ok()?;

        // Align to 4-byte boundaries.
        let record_len = len_before - p.len();
        let _ = p.skip((4 - (record_len & 3)) & 3);

        self.bytes = p.into_rest();
        Some(FileChecksumMut {
            header,
            checksum_data,
        })
    }
}

/// Test iteration of records with byte ranges.
#[test]
fn iter_ranges() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        200, 0, 0, 0,               // name
        0,                          // checksum_size
        0,                          // no checksum
        // <-- offset = 6
        PAD, PAD,
        // <-- offset = 8

        42, 0, 0, 0,                // name
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xc4, 0xc5, 0xc6, 0xc7,
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 30
        PAD, PAD,

        // <-- offset = 32
    ];

    let sums = FileChecksumsSubsection::new(data);
    let mut iter = sums.iter().with_ranges();

    let (sub0_range, _) = iter.next().unwrap();
    assert_eq!(sub0_range, 0..8);

    let (sub1_range, _) = iter.next().unwrap();
    assert_eq!(sub1_range, 8..32);

    assert!(iter.next().is_none());
}

/// Tests that FileChecksumMutIter allows us to modify checksum records.
#[test]
fn iter_mut() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        42, 0, 0, 0,                // name
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xc4, 0xc5, 0xc6, 0xc7,
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 22
        PAD, PAD,

        // <-- offset = 24
    ];

    let mut data_mut = data.to_vec();
    let mut sums = FileChecksumsSubsectionMut::new(&mut data_mut);
    let mut iter = sums.iter_mut();
    assert_eq!(iter.rest_len(), 24); // initial amount of data in iterator

    let sum0 = iter.next().unwrap();
    assert_eq!(iter.rest_len(), 0); // initial amount of data in iterator
    assert_eq!(sum0.header.name.get(), 42);
    assert_eq!(
        sum0.checksum_data,
        &[
            0xc0, 0xc1, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xcb, 0xcc, 0xcd,
            0xce, 0xcf
        ]
    );

    sum0.header.name = U32::new(0xcafef00d);
    sum0.checksum_data[4] = 0xff;

    #[rustfmt::skip]
    let expected_new_data = &[
        0x0d, 0xf0, 0xfe, 0xca,     // name (modified)
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xff, 0xc5, 0xc6, 0xc7,     // <-- modified
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 22
        PAD, PAD,

        // <-- offset = 24
    ];

    assert_eq!(data_mut.as_slice(), expected_new_data);
}

/// Tests FileChecksumIter and FileChecksumMutIter.
#[test]
fn basic_iter() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        42, 0, 0, 0,                // name
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xc4, 0xc5, 0xc6, 0xc7,
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 22
        PAD, PAD,

        // <-- offset = 24
        0, 1, 0, 0,                 // name
        16,                         // checksum_size
        1,                          // MD5
        0xd0, 0xd1, 0xd2, 0xd3,     // checksum
        0xd4, 0xd5, 0xd6, 0xd7,
        0xd8, 0xd9, 0xda, 0xdb,
        0xdc, 0xdd, 0xde, 0xdf,
        // <-- offset = 46
        PAD, PAD,
        // <-- offset = 48
    ];

    // Test FileChecksumIter (immutable iterator)
    {
        let mut iter = FileChecksumIter::new(data);
        assert_eq!(iter.rest_len(), 48); // initial amount of data in iterator
        let sum0 = iter.next().unwrap();
        assert_eq!(sum0.name(), NameIndex(42));
        assert_eq!(
            sum0.checksum_data,
            &[
                0xc0, 0xc1, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xcb, 0xcc, 0xcd,
                0xce, 0xcf
            ]
        );

        assert_eq!(iter.rest_len(), 24); // record 0 is 24 bytes (including padding)

        let sum1 = iter.next().unwrap();
        assert_eq!(sum1.name(), NameIndex(0x100));
        assert_eq!(
            sum1.checksum_data,
            &[
                0xd0, 0xd1, 0xd2, 0xd3, 0xd4, 0xd5, 0xd6, 0xd7, 0xd8, 0xd9, 0xda, 0xdb, 0xdc, 0xdd,
                0xde, 0xdf,
            ]
        );

        assert_eq!(iter.rest_len(), 0); // record 1 is 24 bytes (including padding), leaving nothing in buffer
        assert!(iter.next().is_none());
    }

    // Test FileChecksumMutIter (mutable iterator)
    // We duplicate this because we can't do generics over mutability.
    {
        let mut data_mut = data.to_vec();
        let mut iter = FileChecksumMutIter::new(&mut data_mut);
        assert_eq!(iter.rest_len(), 48); // initial amount of data in iterator
        let sum0 = iter.next().unwrap();
        assert_eq!(sum0.header.name.get(), 42);
        assert_eq!(
            sum0.checksum_data,
            &[
                0xc0, 0xc1, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xcb, 0xcc, 0xcd,
                0xce, 0xcf
            ]
        );

        assert_eq!(iter.rest_len(), 24); // record 0 is 24 bytes (including padding)

        let sum1 = iter.next().unwrap();
        assert_eq!(sum1.header.name.get(), 0x100);
        assert_eq!(
            sum1.checksum_data,
            &[
                0xd0, 0xd1, 0xd2, 0xd3, 0xd4, 0xd5, 0xd6, 0xd7, 0xd8, 0xd9, 0xda, 0xdb, 0xdc, 0xdd,
                0xde, 0xdf,
            ]
        );

        assert_eq!(iter.rest_len(), 0); // record 1 is 24 bytes (including padding), leaving nothing in buffer
        assert!(iter.next().is_none());
    }
}

#[test]
fn test_get_file() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        42,   0,    0, 0, 0, 0, PAD, PAD,      // record 0 at 0
        0xee, 0,    0, 0, 0, 0, PAD, PAD,      // record 1 at 8
        0,    0xcc, 0, 0, 0, 0, PAD, PAD,      // record 2 at 0x10
        // len = 0x18
    ];

    // Test immutable access
    {
        let sums = FileChecksumsSubsection::new(data);

        let sum0 = sums.get_file(0).unwrap();
        assert_eq!(sum0.name(), NameIndex(42));

        let sum1 = sums.get_file(8).unwrap();
        assert_eq!(sum1.name(), NameIndex(0xee));

        let sum2 = sums.get_file(0x10).unwrap();
        assert_eq!(sum2.name(), NameIndex(0xcc00));

        // Test bad index (way outside of data)
        assert!(sums.get_file(0x1000).is_err());

        // Test bad index (invalid header)
        assert!(sums.get_file(0x16).is_err());
    }

    // Test mutable access
    {
        let mut data_mut = data.to_vec();
        let mut sums = FileChecksumsSubsectionMut::new(&mut data_mut);

        let sum0 = sums.get_file_mut(0).unwrap();
        assert_eq!(sum0.header.name.get(), 42);

        let sum1 = sums.get_file_mut(8).unwrap();
        assert_eq!(sum1.header.name.get(), 0xee);

        let sum2 = sums.get_file_mut(0x10).unwrap();
        assert_eq!(sum2.header.name.get(), 0xcc00);

        // Modify one of the records
        sum2.header.name = U32::new(0xcafe);

        // Test bad index (way outside of data)
        assert!(sums.get_file_mut(0x1000).is_err());

        // Test bad index (invalid header)
        assert!(sums.get_file_mut(0x16).is_err());

        #[rustfmt::skip]
        let expected_data = &[
            42,   0,    0, 0, 0, 0, PAD, PAD,      // record 0 at 0
            0xee, 0,    0, 0, 0, 0, PAD, PAD,      // record 1 at 8
            0xfe, 0xca, 0, 0, 0, 0, PAD, PAD,      // record 2 at 0x10
        ];

        assert_eq!(data_mut.as_slice(), expected_data);
    }
}

```

`pdb/src/lines/subsection.rs`:

```rs
//! Iteration logic for subsections

#[cfg(test)]
use pretty_hex::PrettyHex;

use super::*;

/// Iterator state for subsections
pub struct SubsectionIter<'a> {
    rest: &'a [u8],
}

impl<'a> SubsectionIter<'a> {
    /// Start iteration
    pub fn new(rest: &'a [u8]) -> Self {
        Self { rest }
    }

    /// The remaining unparsed data.
    pub fn rest(&self) -> &'a [u8] {
        self.rest
    }
}

impl<'a> HasRestLen for SubsectionIter<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

/// Iterator state for subsections with mutable access
pub struct SubsectionIterMut<'a> {
    rest: &'a mut [u8],
}

impl<'a> SubsectionIterMut<'a> {
    /// Begins iteration
    pub fn new(rest: &'a mut [u8]) -> Self {
        Self { rest }
    }

    /// The remaining unparsed data.
    pub fn rest(&self) -> &[u8] {
        self.rest
    }
}

impl<'a> HasRestLen for SubsectionIterMut<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

/// A reference to one subsection
pub struct Subsection<'a> {
    /// The kind of data in this subsection.
    pub kind: SubsectionKind,
    /// The contents of the subsection.
    pub data: &'a [u8],
}

/// A reference to one subsection, with mutable access
pub struct SubsectionMut<'a> {
    /// The kind of data in this subsection.
    pub kind: SubsectionKind,
    /// The contents of the subsection.
    pub data: &'a mut [u8],
}

/// The header of a subsection.
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned)]
#[repr(C)]
pub struct SubsectionHeader {
    /// The kind of data in this subsection.
    pub kind: U32<LE>,
    /// The size of the subsection, in bytes. This value does not count the size of the `kind` field.
    pub size: U32<LE>,
}

impl<'a> Iterator for SubsectionIter<'a> {
    type Item = Subsection<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.rest);
        let header: &SubsectionHeader = if let Ok(h) = p.get::<SubsectionHeader>() {
            h
        } else {
            warn!(
                "Failed to decode subsection data (incomplete header)!  rest_len = {}",
                self.rest.len()
            );
            return None;
        };
        let size = header.size.get() as usize;

        let data = if let Ok(d) = p.bytes(size) {
            d
        } else {
            warn!(
                "Failed to decode subsection data (incomplete payload)!  rest_len = {}",
                self.rest.len()
            );
            return None;
        };

        // If 'size' is not 4-byte aligned, then skip the alignment bytes.
        let alignment_len = (4 - (size & 3)) & 3;
        let _ = p.skip(alignment_len);

        self.rest = p.into_rest();

        Some(Subsection {
            kind: SubsectionKind(header.kind.get()),
            data,
        })
    }
}

impl<'a> Iterator for SubsectionIterMut<'a> {
    type Item = SubsectionMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = ParserMut::new(core::mem::take(&mut self.rest));
        let header: &SubsectionHeader = p.get::<SubsectionHeader>().ok()?;
        let size = header.size.get() as usize;
        let data = p.bytes_mut(size).ok()?;

        let alignment_len = (4 - (size & 3)) & 3;
        let _ = p.skip(alignment_len);

        self.rest = p.into_rest();

        Some(SubsectionMut {
            kind: SubsectionKind(header.kind.get()),
            data,
        })
    }
}

// Test that empty input or malformed line data (too little data) does not cause the iterator to fail.
// The iterator will return `None`.
#[test]
fn empty_or_malformed_input() {
    static CASES: &[(&str, &[u8])] = &[
        ("empty input", &[]),
        ("incomplete subsection_kind", &[0xf1, 0]),
        ("incomplete subsection_size", &[0xf1, 0, 0, 0, 0xff, 0xff]),
        (
            "incomplete subsection_data",
            &[
                0xf1, 0, 0, 0, // subsection_Kind
                0, 0, 1, 0, // subsection_size (0x10000)
            ],
        ),
    ];

    for &(case_name, case_data) in CASES.iter() {
        // Test the subsection iterator
        println!("case: {}\n{:?}", case_name, case_data.hex_dump());
        let ld = LineData::new(case_data);
        assert_eq!(ld.subsections().count(), 0);
        assert!(ld.find_checksums().is_none());
        assert!(ld.find_checksums_bytes().is_none());
        ld.iter_name_index(|_name| panic!("should never be called"))
            .unwrap();

        // Do the same thing with a mutable iterator.
        let mut case_data_mut = case_data.to_vec();
        let mut ld = LineDataMut::new(&mut case_data_mut);
        assert_eq!(ld.subsections_mut().count(), 0);
    }
}

/// Test the alignment padding code in the subsection iterator.
#[test]
fn test_subsection_alignment() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    static DATA: &[u8] = &[
                                // -----subsection 0 -----
        0xf4, 0, 0, 0,          // subsection_kind: DEBUG_S_FILECHKSMS
        0x2, 0, 0, 0,           // subsection_size (unaligned len = 2)
        0xab, 0xcd,             // subsection_data
        PAD, PAD,               // 2 padding bytes
                                // ----- subsection 1 -----
        0xf5, 0, 0, 0,          // subsection_kind: FRAMEDATA
        7, 0, 0, 0,             // subsection_size: 7 (unaligned len = 3)
        1, 2, 3, 4, 5, 6, 7,    // subsection_data
        PAD,                    // 1 padding byte
                                // ----- subsection 2 -----
        0xf6, 0, 0, 0,          // subsection_kind: INLINEELINES
        8, 0, 0, 0,             // subsection_size: 8 (unaligned len = 0)
        8, 7, 6, 5, 4, 3, 2, 1, // subsection_data
                                // no padding bytes
                                // ----- subsection 3 -----
        0xf7, 0, 0, 0,          // subsection_kind: CROSSSCOPEIMPORTS
        5, 0, 0, 0,             // subsection_size: 5 (unaligned len = 1)
        10, 11, 12, 13, 14,     // subsection_data
        PAD, PAD, PAD,          // 3 padding bytes


    ];

    // Test SubsectionsIter
    {
        let mut iter = LineData::new(DATA).subsections();

        let sub0 = iter.next().unwrap();
        assert_eq!(sub0.kind, SubsectionKind::FILE_CHECKSUMS);
        assert_eq!(sub0.data, &[0xab, 0xcd]);

        let sub1 = iter.next().unwrap();
        assert_eq!(sub1.kind, SubsectionKind::FRAMEDATA);
        assert_eq!(sub1.data, &[1, 2, 3, 4, 5, 6, 7]);

        let sub2 = iter.next().unwrap();
        assert_eq!(sub2.kind, SubsectionKind::INLINEELINES);
        assert_eq!(sub2.data, &[8, 7, 6, 5, 4, 3, 2, 1]);

        let sub3 = iter.next().unwrap();
        assert_eq!(sub3.kind, SubsectionKind::CROSSSCOPEIMPORTS);
        assert_eq!(sub3.data, &[10, 11, 12, 13, 14]);

        assert!(iter.rest().is_empty());
    }

    // Test SubsectionIterMut
    // We repeat the tests because we can't do generics over mutability, and the implementations of
    // SubsectionIter and SubsectionIterMut
    {
        let mut data_mut = DATA.to_vec();
        let mut iter = SubsectionIterMut::new(&mut data_mut);

        let sub0 = iter.next().unwrap();
        assert_eq!(sub0.kind, SubsectionKind::FILE_CHECKSUMS);
        assert_eq!(sub0.data, &[0xab, 0xcd]);

        let sub1 = iter.next().unwrap();
        assert_eq!(sub1.kind, SubsectionKind::FRAMEDATA);
        assert_eq!(sub1.data, &[1, 2, 3, 4, 5, 6, 7]);

        let sub2 = iter.next().unwrap();
        assert_eq!(sub2.kind, SubsectionKind::INLINEELINES);
        assert_eq!(sub2.data, &[8, 7, 6, 5, 4, 3, 2, 1]);

        let sub3 = iter.next().unwrap();
        assert_eq!(sub3.kind, SubsectionKind::CROSSSCOPEIMPORTS);
        assert_eq!(sub3.data, &[10, 11, 12, 13, 14]);

        assert!(iter.rest().is_empty());
    }
}

```

`pdb/src/modi.rs`:

```rs
//! Reads data from Module Info (`modi`) streams.
//!
//! # References
//! * <https://llvm.org/docs/PDB/ModiStream.html>
//! * [`MODI_60_Persist` in `dbi.h`]

use crate::dbi::ModuleInfoFixed;
use crate::parser::Parser;
use crate::utils::vec::replace_range_copy;
use crate::StreamData;
use crate::{dbi::ModuleInfo, syms::SymIter};
use anyhow::{anyhow, bail, Result};
use std::mem::size_of;
use std::ops::Range;
use sync_file::ReadAt;
use tracing::{debug, warn};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U32};

/// The Module Symbols substream begins with this header. It is located at stream offset 0 in the
/// Module Stream.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
pub struct ModuleSymbolsHeader {
    /// Indicates the version of the module symbol stream. Use the `CV_SIGNATURE_*` constants.
    /// The expected value is `CV_SIGNATURE_C13`.
    pub signature: U32<LE>,
}

const MODULE_SYMBOLS_HEADER_LEN: usize = 4;
static_assertions::const_assert_eq!(size_of::<ModuleSymbolsHeader>(), MODULE_SYMBOLS_HEADER_LEN);

/// Actual signature is >64K
pub const CV_SIGNATURE_C6: u32 = 0;
/// First explicit signature
pub const CV_SIGNATURE_C7: u32 = 1;
/// C11 (vc5.x) 32-bit types
pub const CV_SIGNATURE_C11: u32 = 2;
/// C13 (vc7.x) zero terminated names
pub const CV_SIGNATURE_C13: u32 = 4;
/// All signatures from 5 to 64K are reserved
pub const CV_SIGNATURE_RESERVED: u32 = 5;

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads a Module Info stream. The caller must provide a [`ModuleInfo`] structure, which comes
    /// from the DBI Stream.  Use [`crate::dbi::read_dbi_stream`] to enumerate [`ModuleInfo`] values.
    ///
    /// If the Module Info record has a NIL stream, then this function returns `Ok(None)`.
    pub fn read_module_stream(
        &self,
        mod_info: &ModuleInfo,
    ) -> Result<Option<ModiStreamData<StreamData>>, anyhow::Error> {
        let Some(stream) = mod_info.stream() else {
            return Ok(None);
        };

        let stream_data = self.read_stream(stream)?;
        Ok(Some(ModiStreamData::new(stream_data, mod_info.header())?))
    }
}

/// Contains the stream data for a Module Info stream.
#[allow(missing_docs)]
pub struct ModiStreamData<Data> {
    /// The contents of the stream.
    pub stream_data: Data,
    pub sym_byte_size: u32,
    pub c11_byte_size: u32,
    pub c13_byte_size: u32,
    pub global_refs_size: u32,
}

impl<Data: AsRef<[u8]>> ModiStreamData<Data> {
    /// Initializes a new `ModiStreamData`. This validates the byte sizes of the substreams,
    /// which are specified in the [`ModuleInfo`] structure, not within the Module Stream itself.
    pub fn new(stream_data: Data, module: &ModuleInfoFixed) -> anyhow::Result<Self> {
        let stream_bytes: &[u8] = stream_data.as_ref();

        // Validate the byte sizes against the size of the stream data.
        let sym_byte_size = module.sym_byte_size.get();
        let c11_byte_size = module.c11_byte_size.get();
        let c13_byte_size = module.c13_byte_size.get();

        let mut p = Parser::new(stream_bytes);

        p.skip(sym_byte_size as usize).map_err(|_| {
            anyhow!("Module info has a sym_byte_size that exceeds the size of the stream.")
        })?;
        p.skip(c11_byte_size as usize).map_err(|_| {
            anyhow!("Module info has a c11_byte_size that exceeds the size of the stream.")
        })?;
        p.skip(c13_byte_size as usize).map_err(|_| {
            anyhow!("Module info has a c13_byte_size that exceeds the size of the stream.")
        })?;

        let mut global_refs_size;
        if !p.is_empty() {
            global_refs_size = p
                .u32()
                .map_err(|_| anyhow!("Failed to decode global_refs_size. There are {} bytes after the module symbols substream.", p.len()))?;

            if global_refs_size == 0xffff_ffff {
                warn!("Module has global_refs_size = 0xffff_ffff");
                global_refs_size = 0;
            } else {
                p.skip(global_refs_size as usize)
                .map_err(|_| anyhow!("Failed to decode global_refs substream. global_refs_size = 0x{:x}, but there are only 0x{:x} bytes left.",
                global_refs_size,
                p.len()
            ))?;
            }

            if !p.is_empty() {
                debug!(stream_len = p.len(), "Module stream has extra bytes at end");
            }
        } else {
            global_refs_size = 0;
        }

        Ok(Self {
            stream_data,
            sym_byte_size,
            c11_byte_size,
            c13_byte_size,
            global_refs_size,
        })
    }

    /// Returns an iterator for the symbol data for this module.
    pub fn iter_syms(&self) -> SymIter<'_> {
        if let Ok(sym_data) = self.sym_data() {
            SymIter::new(sym_data)
        } else {
            SymIter::new(&[])
        }
    }

    fn nested_slice(&self, range: Range<usize>) -> Result<&[u8]> {
        if let Some(b) = self.stream_data.as_ref().get(range) {
            Ok(b)
        } else {
            bail!("Range within module stream is invalid")
        }
    }

    fn nested_slice_mut(&mut self, range: Range<usize>) -> Result<&mut [u8]>
    where
        Data: AsMut<[u8]>,
    {
        if let Some(b) = self.stream_data.as_mut().get_mut(range) {
            Ok(b)
        } else {
            bail!("Range within module stream is invalid")
        }
    }

    /// Returns a reference to the encoded symbol data for this module.
    ///
    /// This _does not_ include the CodeView signature.
    pub fn sym_data(&self) -> Result<&[u8]> {
        self.nested_slice(MODULE_SYMBOLS_HEADER_LEN..self.sym_byte_size as usize)
    }

    /// Returns a mutable reference to the encoded symbol data for this module.
    ///
    /// This _does not_ include the CodeView signature.
    pub fn sym_data_mut(&mut self) -> Result<&mut [u8]>
    where
        Data: AsMut<[u8]>,
    {
        self.nested_slice_mut(MODULE_SYMBOLS_HEADER_LEN..self.sym_byte_size as usize)
    }

    /// Returns a reference to the encoded symbol data for this module.
    ///
    /// This _does_ include the CodeView signature.
    pub fn full_sym_data(&self) -> Result<&[u8]> {
        self.nested_slice(0..self.sym_byte_size as usize)
    }

    /// Returns a mutable reference to the encoded symbol data for this module.
    ///
    /// This _does_ include the CodeView signature.
    pub fn full_sym_data_mut(&mut self) -> Result<&mut [u8]>
    where
        Data: AsMut<[u8]>,
    {
        self.nested_slice_mut(0..self.sym_byte_size as usize)
    }

    /// Returns the byte range of the C13 Line Data within this Module Information Stream.
    pub fn c13_line_data_range(&self) -> Range<usize> {
        if self.c13_byte_size == 0 {
            return 0..0;
        }

        let start = self.sym_byte_size as usize + self.c11_byte_size as usize;
        start..start + self.c13_byte_size as usize
    }

    /// Returns the byte data for the C13 line data.
    pub fn c13_line_data_bytes(&self) -> &[u8] {
        if self.c13_byte_size == 0 {
            return &[];
        }

        // The range has already been validated.
        let stream_data: &[u8] = self.stream_data.as_ref();
        let range = self.c13_line_data_range();
        &stream_data[range]
    }

    /// Returns a mutable reference to the byte data for the C13 Line Data.
    pub fn c13_line_data_bytes_mut(&mut self) -> &mut [u8]
    where
        Data: AsMut<[u8]>,
    {
        if self.c13_byte_size == 0 {
            return &mut [];
        }

        // The range has already been validated.
        let range = self.c13_line_data_range();
        let stream_data: &mut [u8] = self.stream_data.as_mut();
        &mut stream_data[range]
    }

    /// Returns an object which can decode the C13 Line Data.
    pub fn c13_line_data(&self) -> crate::lines::LineData<'_> {
        crate::lines::LineData::new(self.c13_line_data_bytes())
    }

    /// Returns an object which can decode and modify the C13 Line Data.
    pub fn c13_line_data_mut(&mut self) -> crate::lines::LineDataMut<'_>
    where
        Data: AsMut<[u8]>,
    {
        crate::lines::LineDataMut::new(self.c13_line_data_bytes_mut())
    }

    /// Gets the byte range within the stream data for the global refs
    pub fn global_refs_range(&self) -> Range<usize> {
        if self.global_refs_size == 0 {
            return 0..0;
        }

        // The Global Refs start after the C13 line data.
        // This offset was validated in Self::new().
        // The size_of::<u32>() is for the global_refs_size field itself.
        let global_refs_offset = self.sym_byte_size as usize
            + self.c11_byte_size as usize
            + self.c13_byte_size as usize
            + size_of::<U32<LE>>();
        global_refs_offset..global_refs_offset + self.global_refs_size as usize
    }

    /// Returns a reference to the global refs stored in this Module Stream.
    ///
    /// Each value in the returned slice is a byte offset into the Global Symbol Stream of
    /// a global symbol that this module references.
    pub fn global_refs(&self) -> Result<&[U32<LE>]> {
        let range = self.global_refs_range();
        let stream_data: &[u8] = self.stream_data.as_ref();
        if let Some(global_refs_bytes) = stream_data.get(range) {
            if let Ok(global_refs) = FromBytes::ref_from_bytes(global_refs_bytes) {
                Ok(global_refs)
            } else {
                bail!("Invalid size for global refs")
            }
        } else {
            bail!("Invalid range for global refs")
        }
    }

    /// Returns a mutable reference to the global refs stored in this Module Stream.
    pub fn global_refs_mut(&mut self) -> Result<&mut [U32<LE>]>
    where
        Data: AsMut<[u8]>,
    {
        let range = self.global_refs_range();
        let stream_data: &mut [u8] = self.stream_data.as_mut();
        if let Some(global_refs_bytes) = stream_data.get_mut(range) {
            if let Ok(global_refs) = FromBytes::mut_from_bytes(global_refs_bytes) {
                Ok(global_refs)
            } else {
                bail!("Invalid size for global refs")
            }
        } else {
            bail!("Invalid range for global refs")
        }
    }
}

impl ModiStreamData<Vec<u8>> {
    /// Replace the symbol data for this module.  `new_sym_data` includes the CodeView signature.
    pub fn replace_sym_data(&mut self, new_sym_data: &[u8]) {
        if new_sym_data.len() == self.sym_byte_size as usize {
            self.stream_data[..new_sym_data.len()].copy_from_slice(new_sym_data);
        } else {
            replace_range_copy(
                &mut self.stream_data,
                0,
                self.sym_byte_size as usize,
                new_sym_data,
            );
            self.sym_byte_size = new_sym_data.len() as u32;
        }
    }

    /// Remove the Global Refs section, if present.
    pub fn truncate_global_refs(&mut self) {
        if self.global_refs_size == 0 {
            return;
        }

        let global_refs_offset =
            self.sym_byte_size as usize + self.c11_byte_size as usize + self.c13_byte_size as usize;

        self.stream_data.truncate(global_refs_offset);
        self.global_refs_size = 0;
    }
}

```

`pdb/src/names.rs`:

```rs
//! Parses the Names Stream (`/names`).
//!
//! The Names Stream stores a set of unique strings (names). This allows other data structures to
//! refer to strings using an integer index ([`NameIndex`]), rather than storing copies of the same
//! string in many different places.
//!
//! The stream index for the Names Stream is found in the PDB Information Stream, in the Named
//! Streams section.  The key is "/names".
//!
//! The Names Stream begins with `NamesStreamHeader`, which specifies the size in bytes of the
//! string data substream. The string data substream immediately follows the stream header.
//! It consists of NUL-terminated UTF-8 strings.
//!
//! After the string data there is a hash table. The hash table is an array, one for each string
//! in the table. The value of each array entry is a byte offset that points into the string data.
//! The index of each array entry is chosen using a hash of the corresponding string value.
//!
//! Hash collisions are resolved using linear probing. That is, during table construction, the
//! hash table is allocated and initialized, with each entry pointing to nothing (nil). For each
//! string, we compute the hash of the string (modulo the size of the hash table). If the
//! corresponding entry in the hash table is empty, then we write the `NameIndex` value into that
//! slot. If that slot is already busy, then we check the next slot; if we reach the end of the
//! table then we wrap around to slot 0. For this reason, the number of hash entries must be
//! greater than or equal to the number of strings in the table.
//!
//! The overall organization of the stream is:
//!
//! name             | type                 | usage
//! -----------------|----------------------|------
//! `signature`      | `u32`                | should always be 0xEFFE_EFFE
//! `version`        | `u32`                | should be 1
//! `strings_size`   | `u32`                | size of the string data
//! `strings_data`   | `[u8; strings_size]` | contains the UTF-8 string data, with NUL terminators
//! `num_hashes`     | `u32`                | specifies the number of hash entries
//! `hashes`         | `[u32; num_hashes]`  | contains hash entries for all strings
//! `num_strings`    | `u32`                | number of non-empty strings in the table

#[cfg(test)]
mod tests;

use crate::parser::{Parser, ParserMut};
use crate::utils::align_4;
use crate::utils::iter::{HasRestLen, IteratorWithRangesExt};
use crate::ReadAt;
use anyhow::bail;
use bstr::BStr;
use std::ops::Range;
use tracing::{debug, trace, trace_span, warn};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U32};

/// The name of the `/names` stream. This identifies the stream in the Named Streams Table,
/// in the PDB Information Stream.
pub const NAMES_STREAM_NAME: &str = "/names";

/// A byte offset into the Names Stream.
///
/// This value does not include the size of the stream header, so the size of the stream header
/// must be added to it when dereferencing a string.
#[derive(Copy, Clone, Eq, PartialEq, Debug, Hash, Ord, PartialOrd)]
pub struct NameIndex(pub u32);

impl std::fmt::Display for NameIndex {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        std::fmt::Display::fmt(&self.0, f)
    }
}

#[test]
fn name_index_display() {
    assert_eq!(format!("{}", NameIndex(42)), "42");
}

/// Represents a `NameIndex` value in LE byte order.
#[derive(
    Copy, Clone, Eq, PartialEq, Debug, IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned,
)]
#[repr(transparent)]
pub struct NameIndexLe(pub U32<LE>);

impl NameIndexLe {
    /// Converts the value to the in-memory byte order.
    #[inline(always)]
    pub fn get(self) -> NameIndex {
        NameIndex(self.0.get())
    }
}

/// Value for `NamesStreamHeader::signature`.
pub const NAMES_STREAM_SIGNATURE: u32 = 0xEFFE_EFFE;

/// Value for `NamesStreamHeader::version`.
pub const NAMES_STREAM_VERSION_V1: u32 = 1;

/// The header of the Names Stream.
#[repr(C)]
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
pub struct NamesStreamHeader {
    /// Signature identifies this as a Names Stream. Should always be `NAMES_STREAM_SIGNATURE`.
    pub signature: U32<LE>,
    /// Version of the Names Stream, which determines the hash function.
    pub version: U32<LE>,
    /// Size in bytes of the string data, which immediately follows this header.
    pub strings_size: U32<LE>,
}

/// Stream data for an empty Names stream.
pub static EMPTY_NAMES_STREAM_DATA: &[u8] = &[
    0xFE, 0xEF, 0xFE, 0xEF, // signature
    0x01, 0x00, 0x00, 0x00, // version
    0x04, 0x00, 0x00, 0x00, // strings_size
    0x00, 0x00, 0x00, 0x00, // string data
    0x01, 0x00, 0x00, 0x00, // num_hashes
    0x00, 0x00, 0x00, 0x00, // hash[0]
    0x00, 0x00, 0x00, 0x00, // num_strings
];

#[test]
fn parse_empty_names_stream() {
    let names = NamesStream::parse(EMPTY_NAMES_STREAM_DATA).unwrap();
    assert_eq!(names.num_strings, 0);
    assert_eq!(names.num_hashes, 1);
}

/// The size of the Names Stream Header, in bytes.
pub const NAMES_STREAM_HEADER_LEN: usize = 12;

/// Reads the `/names` stream.
pub struct NamesStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// Contains the stream data of the `/names` stream.
    pub stream_data: StreamData,

    /// The size of the string data. This value comes from the stream header.
    pub strings_size: usize,

    /// The number of entries in the hash table.
    pub num_hashes: usize,

    /// The byte offset within `stream_data` where the hash records begin. Each hash record
    /// contains a `NameIndex` value. The number of elements is `num_hashes`.
    pub hashes_offset: usize,

    /// The is the number of strings from the stream trailer. Nothing guarantees that this value
    /// correctly reflects the number of strings in the string data.
    pub num_strings: usize,
}

impl<StreamData> NamesStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// Parses and validates the stream header.
    ///
    /// This function does not validate all of the strings in the table.
    /// The `check()` function performs extensive checks.
    pub fn parse(stream_data: StreamData) -> anyhow::Result<Self> {
        let stream_data_slice: &[u8] = stream_data.as_ref();
        let mut p = Parser::new(stream_data_slice);
        let header: &NamesStreamHeader = p.get()?;

        if header.signature.get() != NAMES_STREAM_SIGNATURE {
            bail!(
                "The `/names` stream has an invalid signature: 0x{:08x}.",
                header.signature.get()
            );
        }

        if header.version.get() != NAMES_STREAM_VERSION_V1 {
            bail!(
                "The `/names` stream is using an unsupported version: {}.",
                header.version.get()
            );
        }

        let strings_size = header.strings_size.get() as usize;
        let _string_data = p.bytes(strings_size)?;

        // Read the header of the hash table. The only value in the fixed-size portion is a u32
        // that specifies the number of hashes in the table.
        let num_hashes = p.u32()? as usize;

        let hashes_offset = stream_data_slice.len() - p.len();
        let _hashed_names: &[U32<LE>] = p.slice(num_hashes)?;

        // The last item is a u32 that specifies the number of strings in the table.
        let num_strings = p.u32()? as usize;

        Ok(Self {
            stream_data,
            strings_size,
            num_hashes,
            hashes_offset,
            num_strings,
        })
    }

    /// Returns the byte range within the stream of the string data.
    pub fn strings_range(&self) -> Range<usize> {
        NAMES_STREAM_HEADER_LEN..NAMES_STREAM_HEADER_LEN + self.strings_size
    }

    /// Gets the strings data
    pub fn strings_bytes(&self) -> &[u8] {
        &self.stream_data.as_ref()[self.strings_range()]
    }

    /// Gets the hash table. Each entry contains NameIndex, or 0.  The entries are arranged in
    /// the order of the hash of the strings.
    pub fn hashes(&self) -> &[U32<LE>] {
        let stream_data = self.stream_data.as_ref();
        <[U32<LE>]>::ref_from_prefix_with_elems(&stream_data[self.hashes_offset..], self.num_hashes)
            .unwrap()
            .0
    }

    /// Retrieves one string from the string table.
    pub fn get_string(&self, offset: NameIndex) -> anyhow::Result<&BStr> {
        let strings_bytes = self.strings_bytes();
        if let Some(s_bytes) = strings_bytes.get(offset.0 as usize..) {
            let mut p = Parser::new(s_bytes);
            let s = p.strz()?;
            trace!("found string at {offset:?} : {s:?}");
            Ok(s)
        } else {
            bail!("String offset {offset:?} is invalid (out of range)");
        }
    }

    /// Iterates the strings in the table, by reading the character data directly.
    ///
    /// By convention, the string table usually begins with the empty string. However, this is not
    /// a guarantee of this implementation.
    ///
    /// This iterator may iterate empty strings at the end of the sequence, due to alignment bytes
    /// at the end of the string data.
    pub fn iter(&self) -> IterNames<'_> {
        IterNames {
            rest: self.strings_bytes(),
        }
    }

    /// Sorts the Names Stream and removes duplicates. This also eliminates duplicate strings.
    ///
    /// Returns `(remapping_table, new_stream_data)`. The `remapping_table` contains tuples of
    /// `(old_offset, new_offset)` and is sorted by `old_offset`. The caller can use a binary
    /// search to remap entries.
    pub fn rebuild(&self) -> (NameIndexMapping, Vec<u8>) {
        let _span = trace_span!("NamesStream::rebuild").entered();

        let old_stream_data: &[u8] = self.stream_data.as_ref();
        // We verified the length of the stream in NamesStream::parse().
        let old_string_data = self.strings_bytes();

        // Check for the degenerate case of an empty names table, which does not even contain
        // the empty string. This should never happen, but protect against it anyway. Return
        // a copy of the current table, such as it is. The remapping_table is empty.
        if old_string_data.is_empty() {
            return (
                NameIndexMapping { table: Vec::new() },
                old_stream_data.to_vec(),
            );
        }

        // First pass, count the non-empty strings.
        let num_strings = self.iter().filter(|s| !s.is_empty()).count();
        debug!("Number of strings found: {num_strings}");

        // Second pass, build a string table.
        let mut strings: Vec<(Range<usize>, &BStr)> = Vec::with_capacity(num_strings);
        strings.extend(self.iter().with_ranges().filter(|(_, s)| !s.is_empty()));

        // Sort the strings.
        strings.sort_unstable_by_key(|i| i.1);
        strings.dedup_by_key(|i| i.1);

        let num_unique_strings = strings.len();
        if num_unique_strings != num_strings {
            debug!(
                "Removed {} duplicate strings.",
                num_strings - num_unique_strings
            );
        } else {
            debug!("Did not find duplicate strings.");
        }

        // Find the size of the new stream.
        // The 1+ at the start is for the empty string.
        let new_strings_len_unaligned = 1 + strings.iter().map(|(_, s)| s.len() + 1).sum::<usize>();
        let new_strings_len = align_4(new_strings_len_unaligned);

        // Choose the number of hashes.
        let num_hashes = num_unique_strings * 6 / 4;
        assert!(num_hashes >= num_unique_strings);
        debug!(
            "Using {} hashes for {} strings with linear probing.",
            num_hashes, num_unique_strings
        );

        let new_hash_size_bytes = 4   // for the num_hashes field
            + num_hashes * 4 // for the hashes array
            + 4; // for the num_strings field

        let new_stream_data_len = NAMES_STREAM_HEADER_LEN + new_strings_len + new_hash_size_bytes;
        debug!(
            "Old name stream size (strings only): {}",
            old_string_data.len()
        );
        debug!("New name stream size (strings only): {}", new_strings_len);

        let mut new_stream_data: Vec<u8> = vec![0; new_stream_data_len];
        let mut p = ParserMut::new(&mut new_stream_data);
        *p.get_mut().unwrap() = NamesStreamHeader {
            signature: U32::new(NAMES_STREAM_SIGNATURE),
            version: U32::new(NAMES_STREAM_VERSION_V1),
            strings_size: U32::new(new_strings_len as u32),
        };

        // Write the string data into the output table, and build the remapping table as we go.
        let mut remapping_table: Vec<(NameIndex, NameIndex)> = Vec::with_capacity(num_strings + 1);
        // Add mapping for empty
        remapping_table.push((NameIndex(0), NameIndex(0)));
        {
            let new_strings_data_with_alignment = p.bytes_mut(new_strings_len).unwrap();
            let out_bytes = &mut new_strings_data_with_alignment[..new_strings_len_unaligned];
            let out_bytes_len = out_bytes.len();
            let mut out_iter = out_bytes;

            // Write empty string.
            out_iter[0] = 0;
            out_iter = &mut out_iter[1..];

            for (old_range, s) in strings.iter() {
                let old_ni = NameIndex(old_range.start as u32);
                let new_ni = NameIndex((out_bytes_len - out_iter.len()) as u32);
                remapping_table.push((old_ni, new_ni));
                let sb: &[u8] = s;

                trace!(
                    "string: old_ni: 0x{old_ni:08x}, new_ni: 0x{new_ni:08x}, old_range: {:08x}..{:08x} s: {:?}",
                    old_range.start,
                    old_range.end,
                    s,
                    old_ni = old_ni.0,
                    new_ni = new_ni.0,
                );

                out_iter[..sb.len()].copy_from_slice(sb);
                out_iter = &mut out_iter[sb.len() + 1..]; // +1 for NUL
            }

            assert!(out_iter.is_empty());
            remapping_table.sort_unstable_by_key(|&(old, _)| old);
        }

        // Build the hash table. We rely on the table contain all zeroes before we begin writing.
        // We iterate through the strings, in the sorted order, and compute their hashes. Then we
        // insert the NameIndex into the table, using linear probing. If we get to the end, we
        // wrap around.
        let stream_offset_num_hashes = new_stream_data_len - p.len();
        *p.get_mut::<U32<LE>>().unwrap() = U32::new(num_hashes as u32);
        let stream_offset_hash_table = new_stream_data_len - p.len();

        {
            debug!("Building hash table, num_hashes = {}", num_hashes);
            let hash_table: &mut [U32<LE>] = p.slice_mut(num_hashes).unwrap();
            let mut new_ni: u32 = 1; // 1 is for empty string length
            for &(_, sb) in strings.iter() {
                let h = crate::hash::hash_mod_u32(sb, num_hashes as u32);
                trace!("ni {:08x}, hash {:08x}, {:?}", new_ni, h, sb);

                let mut hi = h;
                let mut wrapped = false;
                loop {
                    let slot = &mut hash_table[hi as usize];
                    if slot.get() == 0 {
                        *slot = U32::new(new_ni);
                        break;
                    }
                    hi += 1;
                    if hi as usize == hash_table.len() {
                        hi = 0;
                        assert!(!wrapped, "should not wrap around the table more than once");
                        wrapped = true;
                    }
                }

                new_ni += (sb.len() + 1) as u32;
            }
        }

        let stream_offset_num_strings = new_stream_data_len - p.len();
        *p.get_mut::<U32<LE>>().unwrap() = U32::new(strings.len() as u32);

        assert!(p.is_empty());

        debug!("Stream offsets:");
        debug!(
            "    [{:08x}] - Names Stream header",
            NAMES_STREAM_HEADER_LEN
        );
        debug!("    [{:08x}] - string data", NAMES_STREAM_HEADER_LEN);
        debug!(
            "    [{:08x}] - hash table header (num_hashes)",
            stream_offset_num_hashes
        );
        debug!(
            "    [{:08x}] - hash table, size in bytes = {}",
            stream_offset_hash_table,
            num_hashes * 4
        );
        debug!(
            "    [{:08x}] - num_strings field",
            stream_offset_num_strings
        );
        debug!("    [{:08x}] - (end)", new_stream_data_len);

        (
            NameIndexMapping {
                table: remapping_table,
            },
            new_stream_data,
        )
    }
}

/// Contains a mapping from old `NameIndex` to new `NameIndex. The mapping is sparse.
#[derive(Default)]
pub struct NameIndexMapping {
    /// the mapping table; use binary search for it
    ///
    /// This always starts with `(0, 0)`.
    pub table: Vec<(NameIndex, NameIndex)>,
}

impl NameIndexMapping {
    /// Looks up `name` in the mapping table and returns the mapping for it.
    pub fn map_old_to_new(&self, name: NameIndex) -> anyhow::Result<NameIndex> {
        // Perf optimization: Avoid the binary search for 0, which is never remapped.
        if name.0 == 0 {
            return Ok(name);
        }

        let table = self.table.as_slice();
        match table.binary_search_by_key(&name, |(old, _)| *old) {
            Ok(i) => Ok(table[i].1),
            Err(_) => bail!("The NameIndex value 0x{:x} cannot be remapped because it was not present in the old Names stream.", name.0),
        }
    }
}

/// Given an index `i` into a hash table `hashes`, where `hashes[i]` is already known to be used
/// (non-empty), find the range or ranges of contiguous non-empty entries in `hashes` that cover `i`.
///
/// The reason this function can return two ranges is that linear probing wraps around at the end
/// of the hash table. We have to account for wrap-around at both the start and end of `hashes`.
/// The unit tests (below) illustrate this.
///
/// We use the ranges returned from this function to verify that a given hash entry is at a legal
/// index within the hash table. The hash table may place hash entries adjacent to each other either
/// because the hash functions were numerically 1 different from each other (e.g. `foo` hashes to
/// 42 and `bar` hashes to 43) or because a hash collision occurred. This function does not
/// (cannot) distinguish between those two cases, because it does not have the original strings.
/// Instead, it just computes the places where a given string could legally be. The caller then
/// verifies that each hash entry is in a range that is valid for it.
#[allow(dead_code)]
fn find_collision_ranges(hashes: &[U32<LE>], i: usize) -> (Range<usize>, Range<usize>) {
    assert!(i < hashes.len());
    assert!(hashes[i].get() != 0);

    let mut start = i;
    while start > 0 && hashes[start - 1].get() != 0 {
        start -= 1;
    }

    let mut end = i + 1;
    while end < hashes.len() && hashes[end].get() != 0 {
        end += 1;
    }

    if start == 0 {
        // Special case: The entire hash table is one collision range.
        // We check for this because there are no unused slots in the table.
        if end == hashes.len() {
            return (start..end, 0..0);
        }

        let mut r2_start = hashes.len();
        while r2_start > 0 && hashes[r2_start - 1].get() != 0 {
            r2_start -= 1;
            assert!(r2_start > end); // prevent infinite loops
        }
        if r2_start != hashes.len() {
            (start..end, r2_start..hashes.len())
        } else {
            (start..end, 0..0)
        }
    } else if end == hashes.len() {
        // The end of the main range is aligned at the end of the buffer.
        // Wrap around to the beginning and find the range at the beginning, if any.
        let mut r2_end = 0;
        while r2_end < hashes.len() && hashes[r2_end].get() != 0 {
            assert!(r2_end < start); // prevent infinite loops
            r2_end += 1;
        }

        (start..end, 0..r2_end)
    } else {
        (start..end, 0..0)
    }
}

#[test]
fn test_find_collision_range() {
    const EMPTY: U32<LE> = U32::from_bytes([0; 4]);
    const BUSY: U32<LE> = U32::from_bytes([0xff; 4]);

    let hashes_full: Vec<U32<LE>> = vec![BUSY, BUSY, BUSY, BUSY, BUSY];
    assert_eq!(find_collision_ranges(&hashes_full, 0), (0..5, 0..0));
    assert_eq!(find_collision_ranges(&hashes_full, 2), (0..5, 0..0));

    {
        let hashes_2 = vec![
            BUSY,  // 0 - wraps around
            EMPTY, // 1
            BUSY,  // 2
            EMPTY, // 3
            EMPTY, // 4
            BUSY,  // 5
            BUSY,  // 6 - wraps around
        ];
        assert_eq!(find_collision_ranges(&hashes_2, 0), (0..1, 5..7));
        assert_eq!(find_collision_ranges(&hashes_2, 2), (2..3, 0..0));
        assert_eq!(find_collision_ranges(&hashes_2, 5), (5..7, 0..1));
    }

    {
        let hashes_3 = vec![
            BUSY,  // 0 - wraps around
            EMPTY, // 1
            BUSY,  // 2
            EMPTY, // 3
            EMPTY, // 4
            BUSY,  // 5
            EMPTY, // 6 - no wrap around
        ];
        assert_eq!(find_collision_ranges(&hashes_3, 0), (0..1, 0..0));
        assert_eq!(find_collision_ranges(&hashes_3, 2), (2..3, 0..0));
        assert_eq!(find_collision_ranges(&hashes_3, 5), (5..6, 0..0));
    }
    {
        let hashes_4 = vec![
            EMPTY, // 0 - no wrap around
            EMPTY, // 1
            BUSY,  // 2
            EMPTY, // 3
            EMPTY, // 4
            BUSY,  // 5
            BUSY,  // 6 - wraps around
        ];
        assert_eq!(find_collision_ranges(&hashes_4, 2), (2..3, 0..0));
        assert_eq!(find_collision_ranges(&hashes_4, 5), (5..7, 0..0));
        assert_eq!(find_collision_ranges(&hashes_4, 6), (5..7, 0..0));
    }
}

/// Iterator state
pub struct IterNames<'a> {
    rest: &'a [u8],
}

impl<'a> HasRestLen for IterNames<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

impl<'a> Iterator for IterNames<'a> {
    type Item = &'a BStr;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.rest);
        let Ok(s) = p.strz() else {
            warn!(
                rest_len = self.rest.len(),
                "Found malformed string in /names stream"
            );
            return None;
        };

        self.rest = p.into_rest();
        Some(s)
    }
}

impl NamesStream<Vec<u8>> {
    /// Reads the Names Stream and parses its header.
    pub fn load_and_parse<F: ReadAt>(
        pdb: &crate::msf::Msf<F>,
        named_streams: &crate::pdbi::NamedStreams,
    ) -> anyhow::Result<Self> {
        let named_stream_index = named_streams.get_err(NAMES_STREAM_NAME)?;
        let named_stream_data = pdb.read_stream_to_vec(named_stream_index)?;
        Self::parse(named_stream_data)
    }
}

```

`pdb/src/names/tests.rs`:

```rs
use super::*;

#[rustfmt::skip]
static NAMES_DATA: &[u8] = &[
    /* 0x0000 */ 0xfe, 0xef, 0xfe, 0xef,                 // signature
    /* 0x0004 */ 1, 0, 0, 0,                             // version
    /* 0x0008 */ 0x18, 0, 0, 0,                          // strings_size
    /* 0x000c */ 0,                                      // empty string
    /* 0x000d */ b'f', b'o', b'o', b'.', b'c', 0,        // (ni 0x0001) "foo.c\0" (len 6)
    /* 0x0013 */ b'b', b'a', b'r', b'.', b'r', b's', 0,  // (ni 0x0007) "bar.rs\0" (len 7)
    /* 0x001a */ b'm', b'a', b'i', b'n', b'.', b'c', 0,  // (ni 0x000e) "main.c\0" (len 7)
    /* 0x0021 */ 0, 0, 0,                                // padding bytes
    /* 0x0024 */ 0, 0, 0, 0,                             // num_hashes
    /* 0x0028 */                                         // hashes (none!)
    /* 0x0028 */ 3, 0, 0, 0,                             // num_strings
];

#[test]
fn test_basic() {
    let names = NamesStream::parse(&NAMES_DATA).unwrap();
    assert_eq!(names.get_string(NameIndex(1)).unwrap(), "foo.c");
    assert_eq!(names.get_string(NameIndex(7)).unwrap(), "bar.rs");
    assert_eq!(names.get_string(NameIndex(0xe)).unwrap(), "main.c");

    // Sort the name table.  After sorting, we should have:
    //      old ni 0x0007, new ni 0x0001 - "bar.rs"
    //      old ni 0x0001, new ni 0x0008 - "foo.c"
    //      old ni 0x000e, new ni 0x000e - "main.c"
    let (mapping, new_names_bytes) = names.rebuild();
    let new_names = NamesStream::parse(new_names_bytes).unwrap();

    assert_eq!(new_names.get_string(NameIndex(1)).unwrap(), "bar.rs");
    assert_eq!(new_names.get_string(NameIndex(8)).unwrap(), "foo.c");
    assert_eq!(new_names.get_string(NameIndex(0xe)).unwrap(), "main.c");

    assert_eq!(mapping.map_old_to_new(NameIndex(7)).unwrap(), NameIndex(1));
    assert_eq!(mapping.map_old_to_new(NameIndex(1)).unwrap(), NameIndex(8));
    assert_eq!(
        mapping.map_old_to_new(NameIndex(0xe)).unwrap(),
        NameIndex(0xe)
    );
}

#[test]
fn rebuild() {
    println!("parsing old names table");
    let old_names = NamesStream::parse(NAMES_DATA).unwrap();

    println!("rebuilding names table");
    let (remapping, new_names_bytes) = old_names.rebuild();
    assert!(!remapping.table.is_empty());
    assert_eq!(remapping.table[0], (NameIndex(0), NameIndex(0)));

    // The old_name_index values should be strictly increasing.
    for w in remapping.table.windows(2) {
        assert!(
            w[0].0 < w[1].0,
            "The old_name_index values should be strictly increasing."
        );
    }

    println!("parsing new names table");
    let new_names = NamesStream::parse(new_names_bytes.as_slice())
        .expect("expected rebuild Names table to successfully parse");

    // All entries in remapping should be valid in both old and new table.
    // The string value should be equal, for both.
    println!("validating mapping");
    for &(old_index, new_index) in remapping.table.iter() {
        let old_str = match old_names.get_string(old_index) {
            Ok(s) => s,
            Err(_) => panic!("Did not find mapping for {old_index} in old name table"),
        };

        let new_str = match new_names.get_string(new_index) {
            Ok(s) => s,
            Err(_) => panic!("Did not find mapping for {new_index} in new name table"),
        };

        assert_eq!(
            old_str, new_str,
            "old_index = {old_index}, new_index = {new_index}"
        );
    }

    // Rebuilding the names table _again_ should produce the exact same bytes.
    let (roundtrip_remapping, roundtrip_names_bytes) = new_names.rebuild();

    // We do not expect the remapping table to be the same, but we do expect the old/new values to be the same.
    assert_eq!(remapping.table.len(), roundtrip_remapping.table.len());
    for (i, &(old_name_index, new_name_index)) in roundtrip_remapping.table.iter().enumerate() {
        assert_eq!(old_name_index, new_name_index, "i = {i}");
    }

    assert_eq!(
        roundtrip_names_bytes, new_names_bytes,
        "Round-trip name table should be identical."
    );
}

```

`pdb/src/parser.rs`:

```rs
//! Support for parsing byte-oriented data

#[cfg(test)]
mod tests;

use crate::types::TypeIndex;
use bstr::{BStr, ByteSlice};
use std::mem::{size_of, take};
use zerocopy::byteorder::{I16, I32, I64, LE, U16, U32, U64};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, I128, U128};

pub use crate::types::number::Number;

/// A byte-oriented parser.
#[derive(Clone)]
pub struct Parser<'a> {
    /// The bytes that have not yet been parsed.
    pub bytes: &'a [u8],
}

impl<'a> Parser<'a> {
    /// Starts a new parser.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }

    /// Gets the rest of the unparsed bytes in the parser. The parser still retains a reference to
    /// the same data.
    pub fn peek_rest(&self) -> &'a [u8] {
        self.bytes
    }

    /// Gets the rest of the unparsed
    pub fn take_rest(&mut self) -> &'a [u8] {
        take(&mut self.bytes)
    }

    /// Consumes this `Parser` and returns the unparsed bytes within it.
    ///
    /// This should be used in situations where there is no valid reason to use the `Parser`
    /// after taking the rest of the bytes within it. In situations where a `parse()` method only
    /// has access to `&mut Parser`, then this function cannot be used, and the caller should use
    /// `Parser::take_rest`.
    pub fn into_rest(self) -> &'a [u8] {
        self.bytes
    }

    /// Indicates whether there are any bytes left to parse.
    pub fn is_empty(&self) -> bool {
        self.bytes.is_empty()
    }

    /// Returns the number of unparsed bytes in the parser.
    pub fn len(&self) -> usize {
        self.bytes.len()
    }

    /// Checks that the buffer has at least `n` bytes.
    ///
    /// This can be used as an optimization improvement in some situations. Ordinarily, code like
    /// this will compile to a series of bounds checks:
    ///
    /// ```ignore
    /// let mut p = Parser::new(bytes);
    /// let a = p.u32()?;
    /// let b = p.u16()?;
    /// let c = p.u16()?;
    /// let d = p.u32()?;
    /// ```
    ///
    /// Inserting a `a.needs(12)?` statement can sometimes enable the compiler to collapse a
    /// series of bounds checks (4, in this case) to a single bounds check.
    #[inline(always)]
    pub fn needs(&self, n: usize) -> Result<(), ParserError> {
        if n <= self.bytes.len() {
            Ok(())
        } else {
            Err(ParserError::new())
        }
    }

    /// Takes the next `n` bytes of input and returns a slice to it. The parser is advanced by `n`.
    #[inline(always)]
    pub fn bytes(&mut self, n: usize) -> Result<&'a [u8], ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        let (lo, hi) = self.bytes.split_at(n);
        self.bytes = hi;
        Ok(lo)
    }

    /// Skips `n` bytes.
    pub fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        self.bytes = &self.bytes[n..];
        Ok(())
    }

    /// Parses a reference to a structure. The input must contain at least [`size_of::<T>()`] bytes.
    #[inline(always)]
    pub fn get<T: FromBytes + Unaligned + KnownLayout + Immutable>(
        &mut self,
    ) -> Result<&'a T, ParserError> {
        if let Ok((value, rest)) = T::ref_from_prefix(self.bytes) {
            self.bytes = rest;
            Ok(value)
        } else {
            Err(ParserError::new())
        }
    }

    /// Parses a copy of a structure. The input must contain at least [`size_of::<T>()`] bytes.
    #[inline(always)]
    pub fn copy<T: FromBytes + Unaligned>(&mut self) -> Result<T, ParserError> {
        let item = self.bytes(size_of::<T>())?;
        Ok(T::read_from_bytes(item).unwrap())
    }

    /// Parses a `T` from the input, if `T` knows how to read from a `Parser`.
    ///
    /// This exists mainly to allow more succinct calls, using type inference.
    #[inline(always)]
    pub fn parse<T: Parse<'a>>(&mut self) -> Result<T, ParserError> {
        T::from_parser(self)
    }

    /// Parses a slice of items. The input must contain at least [`size_of::<T>() * n`] bytes.
    pub fn slice<T: FromBytes + Unaligned + Immutable>(
        &mut self,
        len: usize,
    ) -> Result<&'a [T], ParserError> {
        if let Ok((lo, hi)) = <[T]>::ref_from_prefix_with_elems(self.bytes, len) {
            self.bytes = hi;
            Ok(lo)
        } else {
            Err(ParserError::new())
        }
    }

    /// Copies an array of items with a constant size and advances the parser.
    pub fn array<const N: usize>(&mut self) -> Result<[u8; N], ParserError> {
        let s = self.bytes(N)?;
        Ok(<[u8; N]>::try_from(s).unwrap())
    }

    /// Reads one byte and advances.
    pub fn u8(&mut self) -> Result<u8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0])
    }

    /// Reads one signed byte and advances.
    pub fn i8(&mut self) -> Result<i8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0] as i8)
    }

    /// Reads an `i16` (in little-endian order) and advances.
    pub fn i16(&mut self) -> Result<i16, ParserError> {
        Ok(self.copy::<I16<LE>>()?.get())
    }

    /// Reads an `i32` (in little-endian order) and advances.
    pub fn i32(&mut self) -> Result<i32, ParserError> {
        Ok(self.copy::<I32<LE>>()?.get())
    }

    /// Reads an `i64` (in little-endian order) and advances.
    pub fn i64(&mut self) -> Result<i64, ParserError> {
        Ok(self.copy::<I64<LE>>()?.get())
    }

    /// Reads an `u16` (in little-endian order) and advances.
    pub fn u16(&mut self) -> Result<u16, ParserError> {
        Ok(self.copy::<U16<LE>>()?.get())
    }

    /// Reads an `u32` (in little-endian order) and advances.
    pub fn u32(&mut self) -> Result<u32, ParserError> {
        Ok(self.copy::<U32<LE>>()?.get())
    }

    /// Reads an `u64` (in little-endian order) and advances.
    pub fn u64(&mut self) -> Result<u64, ParserError> {
        Ok(self.copy::<U64<LE>>()?.get())
    }

    /// Reads an `u128` (in little-endian order) and advances.
    pub fn u128(&mut self) -> Result<u128, ParserError> {
        Ok(self.copy::<U128<LE>>()?.get())
    }

    /// Reads an `i128` (in little-endian order) and advances.
    pub fn i128(&mut self) -> Result<i128, ParserError> {
        Ok(self.copy::<I128<LE>>()?.get())
    }

    /// Reads an `f32` (in little-endian order) and advances.
    pub fn f32(&mut self) -> Result<f32, ParserError> {
        let bytes: [u8; 4] = self.copy()?;
        Ok(f32::from_le_bytes(bytes))
    }

    /// Reads an `f64` (in little-endian order) and advances.
    pub fn f64(&mut self) -> Result<f64, ParserError> {
        let bytes: [u8; 8] = self.copy()?;
        Ok(f64::from_le_bytes(bytes))
    }

    /// Skips over a NUL-terminated string.
    pub fn skip_strz(&mut self) -> Result<(), ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                self.bytes = &self.bytes[i + 1..];
                return Ok(());
            }
        }

        Err(ParserError::new())
    }

    /// Reads a NUL-terminated string, without checking that it is UTF-8 encoded.
    pub fn strz(&mut self) -> Result<&'a BStr, ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                let str_bytes = &self.bytes[..i];
                self.bytes = &self.bytes[i + 1..];
                return Ok(BStr::new(str_bytes));
            }
        }

        Err(ParserError::new())
    }

    /// Reads a length-prefixed string, without checking that it is UTF-8 encoded.
    pub fn strt_raw(&mut self) -> Result<&'a BStr, ParserError> {
        let len = self.u8()?;
        let bytes = self.bytes(len as usize)?;
        Ok(BStr::new(bytes))
    }

    /// Reads a length-prefixed string.
    pub fn strt(&mut self) -> Result<&'a str, ParserError> {
        let bytes = self.strt_raw()?;
        if let Ok(s) = core::str::from_utf8(bytes.as_ref()) {
            Ok(s)
        } else {
            Err(ParserError::new())
        }
    }

    /// Parses a 32-bit TypeIndex.
    pub fn type_index(&mut self) -> Result<TypeIndex, ParserError> {
        Ok(TypeIndex(self.u32()?))
    }

    /// Parses a generic number value.
    ///
    /// See Section 4, numeric leaves
    pub fn number(&mut self) -> Result<crate::types::number::Number<'a>, ParserError> {
        self.parse()
    }
}

/// A parser that can return mutable references to the data that it parses.
///
/// Most of the methods defined on `ParserMut` are equivalent to the same methods on `Parser`.
pub struct ParserMut<'a> {
    /// The remaining, unparsed data.
    pub bytes: &'a mut [u8],
}

#[allow(missing_docs)]
impl<'a> ParserMut<'a> {
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }

    pub fn peek_rest(&self) -> &[u8] {
        self.bytes
    }

    pub fn peek_rest_mut(&mut self) -> &mut [u8] {
        self.bytes
    }

    pub fn into_rest(self) -> &'a mut [u8] {
        self.bytes
    }

    pub fn is_empty(&self) -> bool {
        self.bytes.is_empty()
    }

    pub fn len(&self) -> usize {
        self.bytes.len()
    }

    pub fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        if n <= self.bytes.len() {
            let b = take(&mut self.bytes);
            self.bytes = &mut b[n..];
            Ok(())
        } else {
            Err(ParserError::new())
        }
    }

    #[inline(always)]
    pub fn bytes(&mut self, n: usize) -> Result<&'a [u8], ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        let (lo, hi) = take(&mut self.bytes).split_at_mut(n);
        self.bytes = hi;

        Ok(lo)
    }

    #[inline(always)]
    pub fn bytes_mut(&mut self, n: usize) -> Result<&'a mut [u8], ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        let (lo, hi) = take(&mut self.bytes).split_at_mut(n);
        self.bytes = hi;

        Ok(lo)
    }

    #[inline(always)]
    pub fn get<T: FromBytes + Unaligned + Immutable + KnownLayout>(
        &mut self,
    ) -> Result<&'a T, ParserError> {
        let bytes = self.bytes(size_of::<T>())?;
        Ok(T::ref_from_bytes(bytes).unwrap())
    }

    #[inline(always)]
    pub fn get_mut<T: FromBytes + IntoBytes + Unaligned + Immutable + KnownLayout>(
        &mut self,
    ) -> Result<&'a mut T, ParserError> {
        let bytes = self.bytes_mut(size_of::<T>())?;
        Ok(T::mut_from_bytes(bytes).unwrap())
    }

    #[inline(always)]
    pub fn copy<T: FromBytes + Unaligned + Immutable>(&mut self) -> Result<T, ParserError> {
        let item = self.bytes(size_of::<T>())?;
        Ok(T::read_from_bytes(item).unwrap())
    }

    pub fn slice_mut<T: FromBytes + IntoBytes + Unaligned>(
        &mut self,
        len: usize,
    ) -> Result<&'a mut [T], ParserError> {
        let d = take(&mut self.bytes);
        if let Ok((lo, hi)) = <[T]>::mut_from_prefix_with_elems(d, len) {
            self.bytes = hi;
            Ok(lo)
        } else {
            Err(ParserError::new())
        }
    }

    pub fn array<const N: usize>(&mut self) -> Result<[u8; N], ParserError> {
        let s = self.bytes(N)?;
        Ok(<[u8; N]>::try_from(s).unwrap())
    }

    pub fn u8(&mut self) -> Result<u8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0])
    }

    pub fn i8(&mut self) -> Result<i8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0] as i8)
    }

    pub fn i16(&mut self) -> Result<i16, ParserError> {
        Ok(self.copy::<I16<LE>>()?.get())
    }

    pub fn i32(&mut self) -> Result<i32, ParserError> {
        Ok(self.copy::<I32<LE>>()?.get())
    }

    pub fn i64(&mut self) -> Result<i64, ParserError> {
        Ok(self.copy::<I64<LE>>()?.get())
    }

    pub fn u16(&mut self) -> Result<u16, ParserError> {
        Ok(self.copy::<U16<LE>>()?.get())
    }

    pub fn u32(&mut self) -> Result<u32, ParserError> {
        Ok(self.copy::<U32<LE>>()?.get())
    }

    pub fn u64(&mut self) -> Result<u64, ParserError> {
        Ok(self.copy::<U64<LE>>()?.get())
    }

    pub fn skip_strz(&mut self) -> Result<(), ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                let stolen_bytes = take(&mut self.bytes);
                self.bytes = &mut stolen_bytes[i + 1..];
                return Ok(());
            }
        }

        Err(ParserError::new())
    }

    pub fn strz(&mut self) -> Result<&'a mut BStr, ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                let stolen_bytes = take(&mut self.bytes);
                let (str_bytes, hi) = stolen_bytes.split_at_mut(i);
                self.bytes = &mut hi[1..];
                return Ok(str_bytes.as_bstr_mut());
            }
        }

        Err(ParserError::new())
    }

    pub fn type_index(&mut self) -> Result<TypeIndex, ParserError> {
        Ok(TypeIndex(self.u32()?))
    }

    pub fn skip_number(&mut self) -> Result<(), ParserError> {
        let mut p = Parser::new(self.bytes);
        let len_before = p.len();
        let _ = p.number()?;
        let num_len = len_before - p.len();
        self.skip(num_len)?;
        Ok(())
    }
}

/// Zero-sized type for representing parsing errors.
#[derive(Copy, Clone, Debug, Eq, PartialEq)]
pub struct ParserError;

impl ParserError {
    /// Constructor for ParserError, also logs an event. This is useful for setting breakpoints.
    #[cfg_attr(debug_assertions, inline(never))]
    #[cfg_attr(not(debug_assertions), inline(always))]
    pub fn new() -> Self {
        #[cfg(debug_assertions)]
        {
            tracing::debug!("ParserError");
        }
        Self
    }
}

impl Default for ParserError {
    fn default() -> Self {
        Self::new()
    }
}

impl std::error::Error for ParserError {}

impl std::fmt::Display for ParserError {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        fmt.write_str("Parsing error")
    }
}

/// Defines types that can parse from a byte stream
pub trait Parse<'a>
where
    Self: Sized,
{
    /// Parses an instance of `Self` from a `Parser`.
    /// This allows the caller to detect which bytes were not consumed at the end of the input.
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError>;

    /// Parses an instance of `Self` from a byte slice.
    fn parse(bytes: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(bytes);
        Self::from_parser(&mut p)
    }
}

```

`pdb/src/parser/tests.rs`:

```rs
#![allow(clippy::redundant_pattern_matching)]

use super::*;
use bstr::ByteSlice;
use std::borrow::Cow;
use zerocopy::{Immutable, KnownLayout};

#[test]
fn empty() {
    assert!(Parser::new(&[]).is_empty());
    assert!(!Parser::new(&[42]).is_empty());
}

#[test]
fn len() {
    assert_eq!(Parser::new(&[]).len(), 0);
    assert_eq!(Parser::new(&[42]).len(), 1);
}

#[test]
fn ints() {
    let bytes = &[
        0x12, 0x34, // u16
        0x56, 0x78, 0xaa, 0xee, // u32
        0x55, 0x33, // u16
    ];

    let mut p = Parser::new(bytes);
    assert_eq!(p.len(), 8);
    assert_eq!(p.u16().unwrap(), 0x3412);
    assert_eq!(p.len(), 6);
    assert_eq!(p.u32().unwrap(), 0xeeaa_7856);
    assert_eq!(p.len(), 2);
    assert_eq!(p.u8().unwrap(), 0x55);
    assert_eq!(p.len(), 1);
    assert_eq!(p.i8().unwrap(), 0x33);
    assert_eq!(p.len(), 0);
    assert!(p.is_empty());

    // Integers do not need to be aligned.  Read some misaligned stuff.
    let mut p = Parser::new(bytes);
    p.u8().unwrap();
    assert_eq!(p.u16().unwrap(), 0x5634); // at index 1
    assert_eq!(p.u32().unwrap(), 0x55_ee_aa_78); // at index 3

    let mut p = Parser::new(&[1, 2, 3, 4]);
    assert_eq!(p.u32().unwrap(), 0x04_03_02_01);
    assert!(p.is_empty());

    let mut p = Parser::new(&[1, 2, 3, 4]);
    assert_eq!(p.type_index().unwrap(), TypeIndex(0x04_03_02_01));
    assert!(p.is_empty());
}

#[test]
fn strz() {
    assert!(Parser::new(&[]).strz().is_err());
    assert!(Parser::new(b"x").strz().is_err());

    assert_eq!(
        Parser::new(&[b'f', b'o', b'o', 0])
            .strz()
            .unwrap()
            .to_str()
            .unwrap(),
        "foo"
    );
}

#[test]
fn strz_bad_utf8() {
    // 0x80 is a continuation byte in UTF-8. It must be preceded by a leading byte.
    let buf: &[u8] = b"\x80 bad\0second string \xe2\x9c\x85\0";
    let mut p = Parser::new(buf);

    let bad_raw = p.strz().unwrap();
    assert!(bad_raw.to_str().is_err()); // is not valid UTF-8
    assert_eq!(bad_raw as &[u8], &[0x80, b' ', b'b', b'a', b'd']);
    let bad_lossy: Cow<str> = bad_raw.to_str_lossy();
    assert!(matches!(bad_lossy, Cow::Owned(_)));
    assert_eq!(bad_lossy, "\u{fffd} bad"); // U+FFFD is the replacement character

    let good = p.strz().unwrap();
    let good_str: &str = good.to_str().unwrap();
    assert_eq!(good_str, "second string ✅");

    assert!(p.is_empty());
}

#[test]
fn strt() {
    let buf = b"\x03abc123";
    let mut p = Parser::new(buf);
    let s = p.strt().unwrap();
    assert_eq!(s, "abc");
    assert_eq!(p.into_rest(), b"123");
}

#[test]
fn rest() {
    let mut p = Parser::new(&[1, 2, 3, 4, 5]);
    assert_eq!(p.u8().unwrap(), 1);

    assert_eq!(p.peek_rest(), &[2, 3, 4, 5]);

    let rest = p.take_rest();
    assert_eq!(rest, &[2, 3, 4, 5]);
    assert!(p.is_empty());
}

#[test]
fn into_rest() {
    let mut p = Parser::new(&[1, 2, 3, 4, 5]);
    assert_eq!(p.u8().unwrap(), 1);
    let rest = p.into_rest();
    assert_eq!(rest, &[2, 3, 4, 5]);
}

#[test]
fn needs() {
    let p = Parser::new(&[10, 20]);
    assert!(matches!(p.needs(0), Ok(_)));
    assert!(matches!(p.needs(2), Ok(_)));
    assert!(matches!(p.needs(3), Err(_)));
}

#[test]
fn skip() {
    let mut p = Parser::new(&[1, 2, 3, 4, 5]);
    p.skip(2).unwrap();
    assert_eq!(p.u16().unwrap(), 0x403);
}

#[derive(IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable, PartialEq, Eq, Debug)]
#[repr(C)]
struct Bar {
    b: u8,
    a: u8,
    r: u8,
}

#[test]
fn get() {
    let buf = &[b'b', b'a', b'r', 4, 5];
    let mut p = Parser::new(buf);

    let bar: &Bar = p.get().unwrap();
    assert_eq!(bar.b, b'b');
    assert_eq!(bar.a, b'a');
    assert_eq!(bar.r, b'r');

    assert!(p.get::<Bar>().is_err());
}

#[test]
fn copy() {
    let buf = &[b'b', b'a', b'r', 4, 5];
    let mut p = Parser::new(buf);

    let bar: Bar = p.copy().unwrap();
    assert_eq!(bar.b, b'b');
    assert_eq!(bar.a, b'a');
    assert_eq!(bar.r, b'r');

    assert!(p.copy::<Bar>().is_err());
}

#[test]
fn slice() {
    let buf = b"ABC123()!zap";
    let mut p = Parser::new(buf);

    let bars: &[Bar] = p.slice(3).unwrap();
    assert_eq!(bars.len(), 3);
    assert_eq!(
        bars[0],
        Bar {
            b: b'A',
            a: b'B',
            r: b'C'
        }
    );
    assert_eq!(
        bars[1],
        Bar {
            b: b'1',
            a: b'2',
            r: b'3'
        }
    );
    assert_eq!(
        bars[2],
        Bar {
            b: b'(',
            a: b')',
            r: b'!'
        }
    );
    assert_eq!(p.into_rest(), b"zap");
}

```

`pdb/src/pdbi.rs`:

```rs
//! PDB Info Stream (aka the PDB Stream)
//!
//! # References
//! * <https://llvm.org/docs/PDB/PdbStream.html>

#[cfg(test)]
mod tests;

use std::collections::BTreeMap;

use super::*;
use crate::encoder::Encoder;
use crate::guid::GuidLe;
use crate::parser::Parser;
use anyhow::bail;
use bitvec::prelude::{BitSlice, Lsb0};
use bstr::ByteSlice;
use tracing::{trace, trace_span, warn};
use uuid::Uuid;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U32};

/// Contains the PDB Information Stream.
///
/// This implementation reads all of the data from the PDBI Stream and converts it to in-memory
/// data structures. This is not typical for most of the data within the PDB. We do this because
/// the PDBI is fairly small, is needed for reading most PDBs, and will often need to be edited
/// for generating or rebuilding PDBs.
#[allow(missing_docs)]
#[derive(Clone)]
pub struct PdbiStream {
    pub signature: u32,
    pub version: u32,
    pub age: u32,
    pub unique_id: Option<Uuid>,
    pub named_streams: NamedStreams,
    pub features: Vec<FeatureCode>,
}

impl PdbiStream {
    /// Parses the stream.
    pub fn parse(stream_data: &[u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(stream_data);

        let header: &PdbiStreamHeader = p.get()?;
        let version = header.version.get();

        // Older PDBs (pre-VC7, i.e. before 2000) do not contain a GUID.
        let unique_id = if pdbi_has_unique_id(version) {
            // Check that the stream data is large enough to contain the unique ID.
            // We use slices, below, relying on bounds checking here.
            Some(p.get::<GuidLe>()?.get())
        } else {
            None
        };

        let named_streams = NamedStreams::parse(&mut p)?;

        // The last part of the PDBI stream is a list of "features". Features are u32 values, and
        // the feature values are defined as constants. If a feature is present in this list, then
        // that feature is enabled.
        let mut features: Vec<FeatureCode> = Vec::with_capacity(p.len() / 4);
        while p.len() >= 4 {
            let feature = FeatureCode(p.u32()?);
            features.push(feature);
        }

        Ok(Self {
            signature: header.signature.get(),
            version,
            age: header.age.get(),
            unique_id,
            named_streams,
            features,
        })
    }

    /// Serializes this to a stream.
    pub fn to_bytes(&self) -> anyhow::Result<Vec<u8>> {
        let mut out = Vec::new();

        let mut e = Encoder::new(&mut out);

        let header = PdbiStreamHeader {
            signature: U32::new(self.signature),
            version: U32::new(self.version),
            age: U32::new(self.age),
        };

        e.t(&header);
        if pdbi_has_unique_id(self.version) {
            if let Some(unique_id) = &self.unique_id {
                e.uuid(unique_id);
            } else {
                bail!("The PDBI version requires a unique ID, but none has been provided.");
            }
        } else if self.unique_id.is_some() {
            warn!("PDBI version is too old to have a unique ID, but this PdbiStream has a unique ID. It will be ignored.");
        }

        self.named_streams.to_bytes(&mut e);

        // Write the features.
        for &feature in self.features.iter() {
            e.u32(feature.0);
        }

        Ok(out)
    }

    /// Gets the 'age' value of the PDB. This links the PDB with the executable; a PDB must have
    /// the same age as its related executable.
    pub fn age(&self) -> u32 {
        self.age
    }

    /// Version from the PDBI header, e.g. [`PDBI_VERSION_VC110`].
    pub fn version(&self) -> u32 {
        self.version
    }

    /// The binding key that associates this PDB with a given PE executable.
    pub fn binding_key(&self) -> BindingKey {
        BindingKey {
            guid: self.unique_id.unwrap_or(Uuid::nil()),
            age: self.age,
        }
    }

    /// Provides access to the named streams table.
    pub fn named_streams(&self) -> &NamedStreams {
        &self.named_streams
    }

    /// Provides mutable access to the named streams table.
    pub fn named_streams_mut(&mut self) -> &mut NamedStreams {
        &mut self.named_streams
    }

    /// Checks whether this PDB has a given feature enabled.
    pub fn has_feature(&self, feature_code: FeatureCode) -> bool {
        self.features.iter().any(|f| *f == feature_code)
    }
}

#[allow(missing_docs)]
pub const PDBI_VERSION_VC2: u32 = 19941610;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC4: u32 = 19950623;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC41: u32 = 19950814;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC50: u32 = 19960307;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC98: u32 = 19970604;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC70_DEPRECATED: u32 = 19990604; // deprecated vc70 implementation version
#[allow(missing_docs)]
pub const PDBI_VERSION_VC70: u32 = 20000404; // <-- first version that has unique id
#[allow(missing_docs)]
pub const PDBI_VERSION_VC80: u32 = 20030901;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC110: u32 = 20091201;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC140: u32 = 20140508;

fn pdbi_has_unique_id(version: u32) -> bool {
    version > PDBI_VERSION_VC70_DEPRECATED
}

/// The header of the PDB Info stream.
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct PdbiStreamHeader {
    pub version: U32<LE>,
    pub signature: U32<LE>,
    pub age: U32<LE>,
    // This is only present if the version number is higher than impvVC70Dep.
    // pub unique_id: GuidLe,
}

#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct HashTableHeader {
    pub size: U32<LE>,
    pub capacity: U32<LE>,
    // present bit vector
    // deleted bit vector
    // (key, value) pairs
}

#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct HashEntry {
    pub key: U32<LE>,
    pub value: U32<LE>,
}

/// Provides access to the Named Streams Table.
#[derive(Default, Clone)]
pub struct NamedStreams {
    /// If true, the named streams set has been modified since it was loaded.
    pub(crate) modified: bool,

    /// Stores the mapping.
    ///
    /// We use `BTreeMap` so that the names are ordered.
    map: BTreeMap<String, u32>,
}

impl NamedStreams {
    /// Iterates the named streams.
    pub fn iter(&self) -> impl Iterator<Item = (&String, &u32)> {
        self.map.iter()
    }

    /// Searches the list of named strings for `name`. If found, returns the stream index.
    ///
    /// This does _not_ use a hash function. It just sequentially searches.
    /// This uses a case-sensitive comparison.
    pub fn get(&self, name: &str) -> Option<u32> {
        self.map.get(name).copied()
    }

    /// Searches the list of named strings for `name`. If found, returns the stream index.
    /// If not found, returns a descriptive error.
    ///
    /// This does _not_ use a hash function. It just sequentially searches.
    /// This uses a case-sensitive comparison.
    pub fn get_err(&self, name: &str) -> anyhow::Result<u32> {
        if let Some(&stream) = self.map.get(name) {
            Ok(stream)
        } else {
            bail!("Failed to find a named stream {:?}", name);
        }
    }

    /// Parses a `NamedStreams` table.
    pub fn parse(p: &mut Parser) -> anyhow::Result<Self> {
        let names_size = p.u32()?;
        let names_data = p.bytes(names_size as usize)?;

        // This is the "cdr" (cardinality) field in pdb.cpp.
        let name_count = p.u32()?;
        let _name_hash_size = p.u32()?;

        let present_u32_count = p.u32()?;
        let present_mask = p.bytes(present_u32_count as usize * 4)?;
        let present_num_items: u32 = present_mask.iter().map(|&b| b.count_ones()).sum();

        let deleted_u32_count = p.u32()?;
        let deleted_mask = p.bytes(deleted_u32_count as usize * 4)?;
        let _deleted_num_items: u32 = deleted_mask.iter().map(|&b| b.count_ones()).sum();

        if present_num_items != name_count {
            bail!("The PDBI name table contains inconsistent values.  Name count is {}, but present bitmap count is {}.",
                name_count, present_num_items);
        }

        let items: &[HashEntry] = p.slice(name_count as usize)?;

        let mut names: BTreeMap<String, u32> = BTreeMap::new();

        for item in items.iter() {
            let key = item.key.get();
            let stream = item.value.get();
            // Key is a byte offset into names_data.
            // Value is a stream index.

            let mut kp = Parser::new(names_data);
            kp.skip(key as usize)?;
            let name = kp.strz()?.to_str_lossy();

            if let Some(existing_stream) = names.get(&*name) {
                warn!("The PDBI contains more than one stream with the same name {:?}: stream {} vs stream {}",
                name, existing_stream, stream);
                continue;
            }

            names.insert(name.to_string(), stream);
        }

        // Parse the "number of NameIndex" values at the end (niMac).
        let num_name_index = p.u32()?;
        if num_name_index != 0 {
            warn!("The Named Streams table contains a non-zero value for the 'niMac' field. This is not supported");
        }

        Ok(Self {
            modified: false,
            map: names,
        })
    }

    /// Inserts a new named stream.
    ///
    /// Returns `true` if the mapping was inserted.
    ///
    /// Returns `false` if there was already a mapping with the given name. In this case, the
    /// named stream table is not modified.
    pub fn insert(&mut self, name: &str, value: u32) -> bool {
        if self.map.contains_key(name) {
            false
        } else {
            self.modified = true;
            self.map.insert(name.to_string(), value);
            true
        }
    }

    /// Removes all entries from the named stream map.
    pub fn clear(&mut self) {
        self.modified = true;
        self.map.clear();
    }

    /// Encode this table to a byte stream
    pub fn to_bytes(&self, e: &mut Encoder) {
        let _span = trace_span!("NamedStreams::to_bytes").entered();

        // Sort the names in the table, so that we have a deterministic order.
        let mut sorted_names: Vec<(&String, u32)> = Vec::with_capacity(self.map.len());
        for (name, stream) in self.map.iter() {
            sorted_names.push((name, *stream));
        }
        sorted_names.sort_unstable();
        let num_names = sorted_names.len();

        // Find the size of the string data table and find the position of every string in that
        // table. We have to do this after sorting the strings.
        let mut strings_len: usize = 0;
        let name_offsets: Vec<u32> = sorted_names
            .iter()
            .map(|(name, _)| {
                let this_pos = strings_len;
                strings_len += name.len() + 1;
                this_pos as u32
            })
            .collect();

        // Write the string data. This is prefixed by the length of the string data.
        e.u32(strings_len as u32);
        for &(name, _) in sorted_names.iter() {
            e.strz(BStr::new(name));
        }

        // We are going to encode this hash table using the format defined by PDBI.  This format
        // is a hash table that uses linear probing.  We choose a load factor of 2x, then hash all
        // the items and place them in the table.
        //
        // Choose a hash size that is larger than our list of names.
        let hash_size = if sorted_names.is_empty() {
            10
        } else {
            sorted_names.len() * 2
        };

        // Find the size of the "present" and "deleted" bitmaps. These bitmaps have the same size.
        let bitmap_size_u32s = (hash_size + 31) / 32;
        let mut present_bitmap_bytes: Vec<u8> = vec![0; bitmap_size_u32s * 4];
        let present_bitmap: &mut BitSlice<u8, Lsb0> =
            BitSlice::from_slice_mut(present_bitmap_bytes.as_mut_slice());

        // hash_slots contains (string_index, stream)
        let mut hash_slots: Vec<Option<(u32, u32)>> = Vec::new();
        hash_slots.resize_with(hash_size, Default::default);

        trace!(num_names, hash_size);

        // Assign all strings to hash slots.
        for (i, &(name, stream)) in sorted_names.iter().enumerate() {
            let name_offset = name_offsets[i];
            let h = crate::hash::hash_mod_u16(name.as_bytes(), 0xffff_ffff) as usize % hash_size;
            let mut slot = h;
            loop {
                if hash_slots[slot].is_none() {
                    hash_slots[slot] = Some((name_offset, stream));
                    present_bitmap.set(slot, true);
                    trace!(
                        assigned_name = name,
                        hash = h,
                        slot = slot,
                        name_offset,
                        stream
                    );
                    break;
                }
                slot += 1;
                assert_ne!(
                    slot, h,
                    "linear probing should not wrap around to starting slot"
                );
                if slot == hash_slots.len() {
                    slot = 0;
                }
            }
        }

        // Write the "cardinality" (number of elements in the table) field.
        e.u32(num_names as u32);

        // Write the number of hashes field.
        e.u32(hash_size as u32);

        // Write the "present" bitmap.
        e.u32(bitmap_size_u32s as u32);
        e.bytes(&present_bitmap_bytes);

        // Write the "deleted" bitmap.
        e.u32(bitmap_size_u32s as u32);
        for _ in 0..bitmap_size_u32s {
            e.u32(0);
        }

        // Write the entries from the hash table that are present.
        for slot in hash_slots.iter() {
            if let Some(slot) = slot {
                e.u32(slot.0);
                e.u32(slot.1);
            }
        }

        // Write the "number of NameIndex values" (niMac).
        e.u32(0);
    }
}

/// A feature code is a `u32` value that indicates that an optional feature is enabled for a given PDB.
#[derive(Copy, Clone, Eq, PartialEq, Debug, Hash, Ord, PartialOrd)]
pub struct FeatureCode(pub u32);

impl FeatureCode {
    /// Indicates that this PDB is a "mini PDB", produced by using the `/DEBUG:FASTLINK` parameter.
    ///
    /// See: <https://learn.microsoft.com/en-us/cpp/build/reference/debug-generate-debug-info?view=msvc-170>
    pub const MINI_PDB: FeatureCode = FeatureCode(0x494E494D);
}

```

`pdb/src/pdbi/tests.rs`:

```rs
use pretty_hex::PrettyHex;

use super::*;

fn names_build(names: &NamedStreams) {
    let mut bytes = Vec::new();
    names.to_bytes(&mut Encoder::new(&mut bytes));
    println!("\n{:?}", bytes.hex_dump());

    // Round-trip testing: Decode the stream that we just built.
    let mut p = Parser::new(&bytes);
    let rt_names =
        NamedStreams::parse(&mut p).expect("expected to successfully parse names stream");

    assert_eq!(names.map, rt_names.map);
    assert!(
        p.is_empty(),
        "found unparsed bytes at the end:\n{:?}",
        p.peek_rest().hex_dump()
    );

    // Round-trip testing *again*.  Encode the round-trip table into bytes again, and verify that
    // we got the exact same bytes.
    let mut rt_bytes = Vec::new();
    names.to_bytes(&mut Encoder::new(&mut rt_bytes));
    assert_eq!(bytes, rt_bytes, "expected round-trip bytes to be the same");
}

#[test]
fn names_build_empty() {
    let names = NamedStreams::default();
    names_build(&names);
}

#[test]
fn names_build_simple() {
    let mut names = NamedStreams::default();
    names.map.insert("/foo".to_string(), 100);
    names.map.insert("/bar".to_string(), 200);
    names_build(&names);
}

#[test]
fn names_build_many() {
    let n = 100;
    let mut names = NamedStreams::default();
    for i in 0..n {
        names.map.insert(format!("/num/{i:04}"), 1000 + i as u32);
    }
    names_build(&names);
}

```

`pdb/src/stream_index.rs`:

```rs
use std::fmt::Display;
use zerocopy::{LE, U16};
use zerocopy_derive::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

/// Identifies a stream in a PDB/MSF file.
///
/// This type guards against NIL stream values. The value stored in `Stream` should never be
/// a NIL value (0xFFFF).
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Debug, Hash)]
#[repr(transparent)]
pub struct Stream(u16);

impl Stream {
    // Some streams have a fixed index.

    /// Fixed stream index 0 is the Previous MSF Stream Directory
    pub const OLD_STREAM_DIR: Stream = Stream(0);

    /// Index of the PDB Information Stream. It contains version information and information to
    /// connect this PDB to the executable.
    pub const PDB: Stream = Stream(1);

    /// Index of the Type Information Stream. It contains type records.
    pub const TPI: Stream = Stream(2);

    /// Debug Information Stream (DBI).
    pub const DBI: Stream = Stream(3);

    /// CodeView type records, index of IPI hash stream
    pub const IPI: Stream = Stream(4);

    /// Validates that `index` is non-NIL and converts it to a `Stream` value.
    ///
    /// If `index` is NIL (0xffff), then this returns `None`.
    pub fn new(index: u16) -> Option<Stream> {
        if index == NIL_STREAM_INDEX {
            None
        } else {
            Some(Stream(index))
        }
    }

    /// Returns the value of the stream index.
    pub fn value(self) -> u16 {
        self.0
    }

    /// Returns the value of the stream index, cast to `usize`. Use this when indexing slices.
    pub fn index(self) -> usize {
        debug_assert!(self.0 != NIL_STREAM_INDEX);
        self.0 as usize
    }
}

impl From<Stream> for u32 {
    fn from(value: Stream) -> Self {
        value.value() as u32
    }
}

impl Display for Stream {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        Display::fmt(&self.0, f)
    }
}

/// A reserved stream index meaning "no stream at all", in `u16`.
pub const NIL_STREAM_INDEX: u16 = 0xffff;

/// Error type for `Stream::try_from` implementations.
#[derive(Clone, Debug)]
pub struct StreamIndexIsNilError;

impl std::error::Error for StreamIndexIsNilError {}

impl Display for StreamIndexIsNilError {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        fmt.write_str("The given stream index is NIL.")
    }
}

#[derive(Clone, Debug)]
pub struct StreamIndexOverflow;

impl std::error::Error for StreamIndexOverflow {}

impl Display for StreamIndexOverflow {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        fmt.write_str("The value is out of range for 16-bit stream indexes.")
    }
}

impl TryFrom<u16> for Stream {
    type Error = StreamIndexIsNilError;

    fn try_from(i: u16) -> Result<Self, Self::Error> {
        if i != NIL_STREAM_INDEX {
            Ok(Self(i))
        } else {
            Err(StreamIndexIsNilError)
        }
    }
}

/// This structure can be embedded directly in structure definitions.
#[derive(
    Copy, Clone, Eq, PartialEq, Debug, IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned,
)]
#[repr(transparent)]
pub struct StreamIndexU16(pub U16<LE>);

impl StreamIndexU16 {
    /// The value of a nil stream index.
    pub const NIL: Self = Self(U16::from_bytes(NIL_STREAM_INDEX.to_le_bytes()));

    /// Checks whether this value is a nil stream index. Returns `Ok` if the value is not a nil
    /// stream index, or `Err` if it is a nil stream index.
    pub fn get(self) -> Option<u32> {
        let s = self.0.get();
        if s != NIL_STREAM_INDEX {
            Some(s as u32)
        } else {
            None
        }
    }

    /// Checks whether this value is a nil stream index. Returns `Ok` if the value is not a nil
    /// stream index, or `Err` if it is a nil stream index.
    pub fn get_err(self) -> Result<u32, StreamIndexIsNilError> {
        let s = self.0.get();
        if s != NIL_STREAM_INDEX {
            Ok(s as u32)
        } else {
            Err(StreamIndexIsNilError)
        }
    }
}

impl TryFrom<u32> for StreamIndexU16 {
    type Error = StreamIndexOverflow;

    fn try_from(s: u32) -> Result<Self, Self::Error> {
        if s < NIL_STREAM_INDEX as u32 {
            Ok(StreamIndexU16(U16::new(s as u16)))
        } else {
            Err(StreamIndexOverflow)
        }
    }
}

impl TryFrom<Option<u32>> for StreamIndexU16 {
    type Error = StreamIndexOverflow;

    fn try_from(s_opt: Option<u32>) -> Result<Self, Self::Error> {
        if let Some(s) = s_opt {
            if s < NIL_STREAM_INDEX as u32 {
                Ok(StreamIndexU16(U16::new(s as u16)))
            } else {
                Err(StreamIndexOverflow)
            }
        } else {
            Ok(Self::NIL)
        }
    }
}

```

`pdb/src/syms.rs`:

```rs
//! Decodes symbols records. Reads the "Global Symbols" stream and per-module symbol streams.
//!
//! # References
//!
//! * [`cvinfo.h`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/include/cvinfo.h)
//! * [CodeView Symbols](https://llvm.org/docs/PDB/CodeViewSymbols.html)

pub mod builder;
mod iter;
mod kind;
mod offset_segment;

#[doc(inline)]
pub use self::{iter::*, kind::SymKind, offset_segment::*};

use crate::parser::{Number, Parse, Parser, ParserError, ParserMut};
use crate::types::{ItemId, ItemIdLe, TypeIndex, TypeIndexLe};
use bstr::BStr;
use std::fmt::Debug;
use std::mem::size_of;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, I32, LE, U16, U32};

/// Identifies the kind of symbol streams. Some behaviors of symbol streams are different,
/// depending on which one is being processed.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub enum SymbolStreamKind {
    /// The Global Symbol Stream
    Global,

    /// A Module Symbol Stream
    Module,
}

impl SymbolStreamKind {
    /// The byte offset within a stream where the symbol records begin.
    pub fn stream_offset(self) -> usize {
        match self {
            Self::Global => 0,
            Self::Module => 4,
        }
    }
}

/// This header is shared by many records that can start a symbol scope.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout, Default, Clone, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct BlockHeader {
    /// If the record containing this `BlockHeader` is a top-level symbol record (not nested within
    /// another symbol), then this value is 0.
    ///
    /// If the record containing this `BlockHeader` is nested within another symbol, then this
    /// value is the offset in the symbol stream of the parent record.
    pub p_parent: U32<LE>,

    /// Offset in symbol stream of the `P_END` which terminates this block scope.
    pub p_end: U32<LE>,
}

/// Used for the header of S_LPROC32 and S_GPROC32.
///
/// See `PROCSYM32` in `cvinfo.h`.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct ProcFixed {
    pub p_parent: U32<LE>,
    pub p_end: U32<LE>,
    pub p_next: U32<LE>,
    pub proc_len: U32<LE>,
    pub debug_start: U32<LE>,
    pub debug_end: U32<LE>,
    pub proc_type: TypeIndexLe,
    pub offset_segment: OffsetSegment,
    pub flags: u8,
}

/// Used for `S_LPROC32` and `S_GPROC32`.
///
/// These records are found in Module Symbol Streams. They are very important; they describe the
/// beginning of a function (procedure), and they contain other symbols recursively (are a
/// "symbol scope"). The end of the sequence is terminated with an `S_END` symbol.
///
/// This is equivalent to the `PROCSYM32` type defined in `cvinfo.h`. This symbol begins with a
/// `BLOCKSYM` header
///
/// # References
/// * See `PROCSYM32` in `cvinfo.h`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Proc<'a> {
    pub fixed: &'a ProcFixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Proc<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

// Basic framing and decoding test
#[test]
fn test_parse_proc() {
    #[rustfmt::skip]
    let data = &[
        /* 0x0000 */ 0x2e, 0, 0x10, 0x11,       // size and S_GPROC32
        /* 0x0004 */ 0, 0, 0, 0,                // p_parent
        /* 0x0008 */ 0x40, 0, 0, 0,             // p_end
        /* 0x000c */ 0, 0, 0, 0,                // p_next
        /* 0x0010 */ 42, 0, 0, 0,               // proc_len
        /* 0x0014 */ 10, 0, 0, 0,               // debug_start
        /* 0x0018 */ 20, 0, 0, 0,               // debug_end
        /* 0x001c */ 0xee, 0x10, 0, 0,          // proc_type
        /* 0x0020 */ 0xcc, 0x1, 0, 0,           // offset
        /* 0x0024 */ 1, 0, 0x50, b'm',          // segment, flags, beginning of name
        /* 0x0028 */ b'e', b'm', b's', b'e',    // name
        /* 0x002c */ b't', 0, 0xf1, 0xf2,       // end and padding
        /* 0x0030 */ 2, 0, 6, 0                 // size = 2 and S_END
        /* 0x0034 */
    ];

    let mut i = SymIter::new(data);

    let s0 = i.next().unwrap();
    assert_eq!(s0.kind, SymKind::S_GPROC32);
    assert_eq!(s0.data.len(), 0x2c);

    match s0.parse().unwrap() {
        SymData::Proc(proc) => {
            assert_eq!(proc.fixed.p_parent.get(), 0);
            assert_eq!(proc.fixed.p_end.get(), 0x40);
            assert_eq!(proc.name, "memset");
        }
        _ => panic!(),
    }

    let s1 = i.next().unwrap();
    assert_eq!(s1.kind, SymKind::S_END);
    assert!(s1.data.is_empty());
}

/// `S_GMANPROC`, `S_LMANPROC` - Managed Procedure Start
///
/// See `MANPROCSYM` in `cvinfo.h`.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct ManagedProcFixed {
    pub p_parent: U32<LE>,
    pub p_end: U32<LE>,
    pub p_next: U32<LE>,
    pub proc_len: U32<LE>,
    pub debug_start: U32<LE>,
    pub debug_end: U32<LE>,
    pub token: U32<LE>,
    pub offset_segment: OffsetSegment,
    pub flags: u8,
    pub return_reg: U16<LE>,
}

/// `S_GMANPROC`, `S_LMANPROC` - Managed Procedure Start
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct ManagedProc<'a> {
    pub fixed: &'a ManagedProcFixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ManagedProc<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Default)]
#[allow(missing_docs)]
pub struct ThunkFixed {
    pub block: BlockHeader,
    pub p_next: U32<LE>,
    pub offset_segment: OffsetSegment,
    pub length: U16<LE>,
    pub thunk_ordinal: u8,
    // name: strz
    // variant: [u8]
}

#[allow(missing_docs)]
pub struct Thunk<'a> {
    pub fixed: &'a ThunkFixed,
    pub name: &'a BStr,
    pub variant: &'a [u8],
}

impl<'a> Parse<'a> for Thunk<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
            variant: p.take_rest(),
        })
    }
}

/// Describes the start of every symbol record.
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Default)]
pub struct SymHeader {
    /// The length in bytes of the record.
    ///
    /// This length _does not_ count the length itself, but _does_ count the `kind` that follows it.
    /// Therefore, all well-formed symbol records have `len >= 2`.
    pub len: U16<LE>,

    /// The kind of the symbol.  See `SymKind`.
    pub kind: U16<LE>,
}

/// Points to one symbol record in memory and gives its kind.
#[derive(Clone)]
pub struct Sym<'a> {
    /// The kind of the symbol.
    pub kind: SymKind,
    /// The contents of the record. This slice does _not_ include the `len` or `kind` fields.
    pub data: &'a [u8],
}

impl<'a> Sym<'a> {
    /// Parse the payload of the symbol.
    pub fn parse(&self) -> Result<SymData<'a>, ParserError> {
        SymData::parse(self.kind, self.data)
    }
}

impl<'a> Debug for Sym<'a> {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(fmt, "{:?}", self.kind)
    }
}

/// Points to one symbol record in memory and gives its kind. Allows mutation of the contents of
/// the symbol record.
pub struct SymMut<'a> {
    /// The kind of the symbol.
    pub kind: SymKind,
    /// The contents of the record. This slice does _not_ include the `len` or `kind` fields.
    pub data: &'a mut [u8],
}

/// PUBSYM32
///
/// ```text
/// typedef struct PUBSYM32 {
///     unsigned short  reclen;     // Record length
///     unsigned short  rectyp;     // S_PUB32
///     CV_PUBSYMFLAGS  pubsymflags;
///     CV_uoff32_t     off;
///     unsigned short  seg;
///     unsigned char   name[1];    // Length-prefixed name
/// } PUBSYM32;
/// ```
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Pub<'a> {
    pub fixed: &'a PubFixed,
    pub name: &'a BStr,
}

impl<'a> Pub<'a> {
    /// Gets the `segment:offset` of this symbol.
    pub fn offset_segment(&self) -> OffsetSegment {
        self.fixed.offset_segment.clone()
    }
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct PubFixed {
    pub flags: U32<LE>,
    pub offset_segment: OffsetSegment,
    // name: &str
}

#[allow(missing_docs)]
impl<'a> Parse<'a> for Pub<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Pub<'a> {
    /// Parses `S_PUB32_ST`
    pub fn parse_st(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strt_raw()?,
        })
    }
}

/// Parsed form of `S_CONSTANT`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Constant<'a> {
    pub type_: TypeIndex,
    pub value: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Constant<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            type_: p.type_index()?,
            value: p.number()?,
            name: p.strz()?,
        })
    }
}

/// Parsed form of `S_CONSTANT`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct ManagedConstant<'a> {
    pub token: u32,
    pub value: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ManagedConstant<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            token: p.u32()?,
            value: p.number()?,
            name: p.strz()?,
        })
    }
}

/// Several symbols use this structure: `S_PROCREF`, `S_LPROCREF`, `S_DATAREF`. These symbols
/// are present in the Global Symbol Stream, not in module symbol streams.
///
/// These `S_*REF` symbols tell you where to find a specific global symbol, but they do not directly
/// describe the symbol. Instead, you have to load the corresponding module
///
///
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct RefSym2<'a> {
    pub header: &'a RefSym2Fixed,
    pub name: &'a BStr,
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct RefSym2Fixed {
    /// Checksum of the name (called `SUC` in C++ code)
    ///
    /// This appears to be set to zero.
    pub name_checksum: U32<LE>,

    /// Offset of actual symbol in $$Symbols
    ///
    /// This is the byte offset into the module symbol stream for this symbol. The `module_index`
    /// field tells you which symbol stream to load, to resolve this value.
    pub symbol_offset: U32<LE>,

    /// The 1-based index of the module containing the actual symbol.
    ///
    /// This value is 1-based. Subtract 1 from this value before indexing into a zero-based module array.
    pub module_index: U16<LE>,
    // pub name: strz, // hidden name made a first class member
}

impl<'a> Parse<'a> for RefSym2<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            header: p.get()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct ThreadStorageFixed {
    pub type_: TypeIndexLe,
    pub offset_segment: OffsetSegment,
}

/// Record data for `S_LTHREAD32` and `S_GTHREAD32`. These describes thread-local storage.
///
/// Thread-local storage is declared using `__declspec(thread)` or `thread_static`, in C++.
#[derive(Clone, Debug)]
pub struct ThreadStorageData<'a> {
    #[allow(missing_docs)]
    pub header: &'a ThreadStorageFixed,
    #[allow(missing_docs)]
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ThreadStorageData<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            header: p.get()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct DataFixed {
    pub type_: TypeIndexLe,
    pub offset_segment: OffsetSegment,
}

/// Record data for `S_LDATA32` and `S_GDATA32`. These describe global storage.
#[allow(missing_docs)]
#[derive(Clone)]
pub struct Data<'a> {
    pub header: &'a DataFixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Data<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            header: p.get()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Debug for Data<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "Data: {} {:?} {}",
            self.header.offset_segment,
            self.header.type_.get(),
            self.name
        )
    }
}

/// Record data for `S_UDT` symbols
#[derive(Clone, Debug)]
pub struct Udt<'a> {
    /// The type of the UDT
    pub type_: TypeIndex,
    /// Name
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Udt<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            type_: p.type_index()?,
            name: p.strz()?,
        })
    }
}

/// `S_OBJNAME`
#[derive(Clone, Debug)]
pub struct ObjectName<'a> {
    /// A robust signature that will change every time that the module will be compiled or
    /// different in any way. It should be at least a CRC32 based upon module name and contents.
    pub signature: u32,
    /// Full path of the object file.
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ObjectName<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            signature: p.u32()?,
            name: p.strz()?,
        })
    }
}

/// `S_COMPILE3`
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct Compile3Fixed {
    pub flags: U32<LE>,
    pub machine: U16<LE>,
    pub frontend_major: U16<LE>,
    pub frontend_minor: U16<LE>,
    pub frontend_build: U16<LE>,
    pub frontend_qfe: U16<LE>,
    pub ver_major: U16<LE>,
    pub ver_minor: U16<LE>,
    pub ver_build: U16<LE>,
    pub ver_qfe: U16<LE>,
    // name: strz
}

/// `S_COMPILE3`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Compile3<'a> {
    pub fixed: &'a Compile3Fixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Compile3<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// `S_FRAMEPROC`: This symbol is used for indicating a variety of extra information regarding a
/// procedure and its stack frame. If any of the flags are non-zero, this record should be added
/// to the symbols for that procedure.
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct FrameProc {
    /// Count of bytes in the whole stack frame.
    frame_size: U32<LE>,
    /// Count of bytes in the frame allocated as padding.
    pad_size: U32<LE>,
    /// Offset of pad bytes from the base of the frame.
    pad_offset: U32<LE>,
    /// Count of bytes in frame allocated for saved callee-save registers.
    save_regs_size: U32<LE>,
    offset_exception_handler: U32<LE>,
    exception_handler_section: U16<LE>,
    padding: U16<LE>,
    flags: U32<LE>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct RegRelFixed {
    pub offset: U32<LE>,
    pub ty: TypeIndexLe,
    pub register: U16<LE>,
    // name: strz
}

/// `S_REGREGL32`: This symbol specifies symbols that are allocated relative to a register.
/// This should be used on all platforms besides x86 and on x86 when the register is not a form of EBP.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct RegRel<'a> {
    pub fixed: &'a RegRelFixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for RegRel<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Block Start: This symbol specifies the start of an inner block of lexically scoped symbols.
/// The lexical scope is terminated by a matching `S_END` symbol.
///
/// This symbol should only be nested (directly or indirectly) within a function symbol
/// (`S_GPROC32`, `S_LPROC32`, etc.).
///
/// See `BLOCKSYM32`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct Block<'a> {
    pub fixed: &'a BlockFixed,
    pub name: &'a BStr,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct BlockFixed {
    /// Header of the block
    pub header: BlockHeader,

    /// Length in bytes of the scope of this block within the executable code stream.
    pub length: U32<LE>,

    pub offset_segment: OffsetSegment,
}

impl<'a> Parse<'a> for Block<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Local Symbol: This symbol defines a local variable.
///
/// This symbol must be nested (directly or indirectly) within a function symbol. It must be
/// followed by more range descriptions.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct Local<'a> {
    pub fixed: &'a LocalFixed,
    pub name: &'a BStr,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct LocalFixed {
    pub ty: TypeIndexLe,
    /// The spec says this is a 32-bit flags field, but the actual records show that this is 16-bit.
    pub flags: U16<LE>,
    // name: strz
}

impl<'a> Parse<'a> for Local<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Represents an address range, used for optimized code debug info
///
/// See `CV_LVAR_ADDR_RANGE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct LVarAddrRange {
    /// Start of the address range
    pub start: OffsetSegment,
    /// Size of the range in bytes.
    pub range_size: U16<LE>,
}

/// Represents the holes in overall address range, all address is pre-bbt.
/// it is for compress and reduce the amount of relocations need.
///
/// See `CV_LVAR_ADDR_GAP`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct LVarAddrGap {
    /// relative offset from the beginning of the live range.
    pub gap_start_offset: U16<LE>,
    /// length of this gap, in bytes
    pub range_size: U16<LE>,
}

/// `S_DEFRANGE_FRAMEPOINTER_REL`: A live range of frame variable
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct DefRangeSymFramePointerRelFixed {
    pub offset_to_frame_pointer: U32<LE>,

    /// Range of addresses where this program is valid
    pub range: LVarAddrRange,
}

/// `S_DEFRANGE_FRAMEPOINTER_REL`: A live range of frame variable
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct DefRangeSymFramePointerRel<'a> {
    pub fixed: &'a DefRangeSymFramePointerRelFixed,
    // The value is not available in following gaps.
    pub gaps: &'a [LVarAddrGap],
}

impl<'a> Parse<'a> for DefRangeSymFramePointerRel<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed = p.get()?;
        let gaps = p.slice(p.len() / size_of::<LVarAddrGap>())?;
        Ok(Self { fixed, gaps })
    }
}

/// Attributes for a register range
///
/// See `CV_RANGEATTR`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct RangeAttrLe {
    // unsigned short  maybe : 1;    // May have no user name on one of control flow path.
    // unsigned short  padding : 15; // Padding for future use.
    pub value: U16<LE>,
}

/// `S_DEFRANGE_REGISTER` - A live range of en-registed variable
///
/// See `DEFRANGESYMREGISTER`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct DefRangeRegisterFixed {
    /// Register to hold the value of the symbol
    pub reg: U16<LE>,
    // Attribute of the register range.
    pub attr: RangeAttrLe,
}

/// `S_DEFRANGE_REGISTER`
///
/// See `DEFRANGESYMREGISTER`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct DefRangeRegister<'a> {
    pub fixed: &'a DefRangeRegisterFixed,
    pub gaps: &'a [u8],
}

impl<'a> Parse<'a> for DefRangeRegister<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            gaps: p.take_rest(),
        })
    }
}

/// `S_DEFRANGE_REGISTER_REL`
#[allow(missing_docs)]
#[derive(Debug, Clone)]
pub struct DefRangeRegisterRel<'a> {
    pub fixed: &'a DefRangeRegisterRelFixed,

    /// The value is not available in following gaps.
    pub gaps: &'a [u8],
}

/// `S_DEFRANGE_REGISTER_REL`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct DefRangeRegisterRelFixed {
    /// Register to hold the base pointer of the symbol
    pub base_reg: U16<LE>,

    /// ```text
    /// unsigned short  spilledUdtMember : 1;   // Spilled member for s.i.
    /// unsigned short  padding          : 3;   // Padding for future use.
    /// unsigned short  offsetParent     : CV_OFFSET_PARENT_LENGTH_LIMIT;  // Offset in parent variable.
    /// ```
    pub flags: U16<LE>,

    /// offset to register
    pub base_pointer_offset: I32<LE>,

    /// Range of addresses where this program is valid
    pub range: LVarAddrRange,
}

impl<'a> Parse<'a> for DefRangeRegisterRel<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            gaps: p.take_rest(),
        })
    }
}

/// `S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE`
///
/// A frame variable valid in all function scope.
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct DefRangeFramePointerRelFullScope {
    /// offset to frame pointer
    pub frame_pointer_offset: I32<LE>,
}

/// `S_DEFRANGE_SUBFIELD_REGISTER`
///
/// See `DEFRANGESYMSUBFIELDREGISTER`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct DefRangeSubFieldRegister<'a> {
    pub fixed: &'a DefRangeSubFieldRegisterFixed,
    pub gaps: &'a [u8],
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct DefRangeSubFieldRegisterFixed {
    pub reg: U16<LE>,
    pub attr: RangeAttrLe,
    pub flags: U32<LE>,
    pub range: LVarAddrRange,
}

impl<'a> Parse<'a> for DefRangeSubFieldRegister<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            gaps: p.take_rest(),
        })
    }
}

/// `S_GMANPROC`, `S_LMANPROC`, `S_GMANPROCIA64`, `S_LMANPROCIAC64`
///
/// See `MANPROCSYM`
pub struct ManProcSym<'a> {
    #[allow(missing_docs)]
    pub fixed: &'a ManProcSymFixed,
    #[allow(missing_docs)]
    pub name: &'a BStr,
}

/// MSIL / CIL token value
pub type TokenIdLe = U32<LE>;

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct ManProcSymFixed {
    pub block: BlockHeader,
    /// pointer to next symbol
    pub pnext: U32<LE>,
    /// Proc length
    pub len: U32<LE>,
    /// Debug start offset
    pub dbg_start: U32<LE>,
    /// Debug end offset
    pub dbg_end: U32<LE>,
    // COM+ metadata token for method
    pub token: TokenIdLe,
    pub off: U32<LE>,
    pub seg: U16<LE>,
    pub flags: u8, // CV_PROCFLAGS: Proc flags
    pub padding: u8,
    // Register return value is in (may not be used for all archs)
    pub ret_reg: U16<LE>,
    // name: strz    // optional name field
}

impl<'a> Parse<'a> for ManProcSym<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// `S_TRAMPOLINE`
#[derive(Clone, Debug)]
pub struct Trampoline<'a> {
    /// Fixed header
    pub fixed: &'a TrampolineFixed,

    /// Data whose interpretation depends on `tramp_type`
    pub rest: &'a [u8],
}

/// `S_TRAMPOLINE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
pub struct TrampolineFixed {
    /// trampoline sym subtype
    pub tramp_type: U16<LE>,
    /// size of the thunk
    pub cb_thunk: U16<LE>,
    /// offset of the thunk
    pub off_thunk: U32<LE>,
    /// offset of the target of the thunk
    pub off_target: U32<LE>,
    /// section index of the thunk
    pub sect_thunk: U16<LE>,
    /// section index of the target of the thunk
    pub sect_target: U16<LE>,
}

impl<'a> Parse<'a> for Trampoline<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            rest: p.take_rest(),
        })
    }
}

/// `S_BUILDINFO` - Build info for a module
///
/// This record is present only in module symbol streams.
#[derive(Clone, Debug)]
pub struct BuildInfo {
    /// ItemId points to an `LF_BUILDINFO` record in IPI
    pub item: ItemId,
}

impl<'a> Parse<'a> for BuildInfo {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self { item: p.u32()? })
    }
}

/// `S_UNAMESPACE` - Using Namespace
#[derive(Clone, Debug)]
pub struct UsingNamespace<'a> {
    /// The namespace, e.g. `std`
    pub namespace: &'a BStr,
}

impl<'a> Parse<'a> for UsingNamespace<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            namespace: p.strz()?,
        })
    }
}

/// `S_LABEL32`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct Label<'a> {
    pub fixed: &'a LabelFixed,
    pub name: &'a BStr,
}

/// `S_LABEL32`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct LabelFixed {
    pub offset_segment: OffsetSegment,
    pub flags: u8,
}

impl<'a> Parse<'a> for Label<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// `S_CALLERS` or `S_CALLEES`
#[derive(Clone, Debug)]
pub struct FunctionList<'a> {
    /// The list of functions, in the IPI. Each is either `LF_FUNC_ID` or `LF_MFUNC_ID`.
    pub funcs: &'a [ItemIdLe],

    /// Invocation counts. The items in `invocations` parallel the items in `funcs`, but the length
    /// of `invocations` can be less than the length of `funcs`. Unmatched functions are assumed
    /// to have an invocation count of zero.
    pub invocations: &'a [U32<LE>],
}

impl<'a> Parse<'a> for FunctionList<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let count = p.u32()?;
        let funcs: &[ItemIdLe] = p.slice(count as usize)?;
        let num_invocations = p.len() / size_of::<U32<LE>>();
        let invocations = p.slice(num_invocations)?;
        Ok(Self { funcs, invocations })
    }
}

/// `S_INLINESITE`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct InlineSite<'a> {
    pub fixed: &'a InlineSiteFixed,
    /// an array of compressed binary annotations.
    pub binary_annotations: &'a [u8],
}

/// `S_INLINESITE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct InlineSiteFixed {
    pub block: BlockHeader,
    pub inlinee: ItemIdLe,
}

impl<'a> Parse<'a> for InlineSite<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            binary_annotations: p.take_rest(),
        })
    }
}

/// `S_INLINESITE2`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct InlineSite2<'a> {
    pub fixed: &'a InlineSite2Fixed,
    /// an array of compressed binary annotations.
    pub binary_annotations: &'a [u8],
}

/// `S_INLINESITE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct InlineSite2Fixed {
    pub block: BlockHeader,
    pub inlinee: ItemIdLe,
    pub invocations: U32<LE>,
}

impl<'a> Parse<'a> for InlineSite2<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            binary_annotations: p.take_rest(),
        })
    }
}

/// `S_FRAMECOOKIE`: Symbol for describing security cookie's position and type
// (raw, xor'd with esp, xor'd with ebp).
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct FrameCookie {
    /// Frame relative offset
    pub offset: I32<LE>,
    pub reg: U16<LE>,
    pub cookie_type: u8,
    pub flags: u8,
}

/// `S_CALLSITEINFO`
///
/// Symbol for describing indirect calls when they are using
/// a function pointer cast on some other type or temporary.
/// Typical content will be an LF_POINTER to an LF_PROCEDURE
/// type record that should mimic an actual variable with the
/// function pointer type in question.
///
/// Since the compiler can sometimes tail-merge a function call
/// through a function pointer, there may be more than one
/// S_CALLSITEINFO record at an address.  This is similar to what
/// you could do in your own code by:
///
/// ```text
///  if (expr)
///      pfn = &function1;
///  else
///      pfn = &function2;
///
///  (*pfn)(arg list);
/// ```
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct CallSiteInfo {
    pub offset: OffsetSegment,
    pub padding: U16<LE>,
    pub func_type: TypeIndexLe,
}

/// `S_HEAPALLOCSITE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct HeapAllocSite {
    pub offset: OffsetSegment,
    /// length of heap allocation call instruction
    pub instruction_size: U16<LE>,
    pub func_type: TypeIndexLe,
}

/// `S_ANNOTATION`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Annotation<'a> {
    pub fixed: &'a AnnotationFixed,
    pub strings: &'a [u8],
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct AnnotationFixed {
    pub offset: OffsetSegment,
    pub num_strings: U16<LE>,
}

impl<'a> Parse<'a> for Annotation<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            strings: p.take_rest(),
        })
    }
}

impl<'a> Annotation<'a> {
    /// Iterates the strings stored in the annotation.
    pub fn iter_strings(&self) -> AnnotationIterStrings<'a> {
        AnnotationIterStrings {
            num_strings: self.fixed.num_strings.get(),
            bytes: self.strings,
        }
    }
}

/// Iterator state for [`Annotation::iter_strings`].
#[allow(missing_docs)]
pub struct AnnotationIterStrings<'a> {
    pub num_strings: u16,
    pub bytes: &'a [u8],
}

impl<'a> Iterator for AnnotationIterStrings<'a> {
    type Item = &'a BStr;

    fn next(&mut self) -> Option<Self::Item> {
        if self.num_strings == 0 {
            return None;
        }

        self.num_strings -= 1;
        let mut p = Parser::new(self.bytes);
        let s = p.strz().ok()?;
        self.bytes = p.into_rest();
        Some(s)
    }
}

// Trampoline subtypes

/// Incremental thunks
pub const TRAMPOLINE_KIND_INCREMENTAL: u16 = 0;
/// Branch island thunks
pub const TRAMPOLINE_KIND_BRANCH_ISLAND: u16 = 1;

/// Parsed data from a symbol record
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub enum SymData<'a> {
    Unknown,
    ObjName(ObjectName<'a>),
    Compile3(Compile3<'a>),
    Proc(Proc<'a>),
    Udt(Udt<'a>),
    Constant(Constant<'a>),
    ManagedConstant(ManagedConstant<'a>),
    RefSym2(RefSym2<'a>),
    Data(Data<'a>),
    ThreadData(ThreadStorageData<'a>),
    Pub(Pub<'a>),
    End,
    FrameProc(&'a FrameProc),
    RegRel(RegRel<'a>),
    Block(Block<'a>),
    Local(Local<'a>),
    DefRangeFramePointerRel(DefRangeSymFramePointerRel<'a>),
    DefRangeRegister(DefRangeRegister<'a>),
    DefRangeRegisterRel(DefRangeRegisterRel<'a>),
    DefRangeFramePointerRelFullScope(&'a DefRangeFramePointerRelFullScope),
    DefRangeSubFieldRegister(DefRangeSubFieldRegister<'a>),
    Trampoline(Trampoline<'a>),
    BuildInfo(BuildInfo),
    UsingNamespace(UsingNamespace<'a>),
    InlineSiteEnd,
    Label(Label<'a>),
    FunctionList(FunctionList<'a>),
    InlineSite(InlineSite<'a>),
    InlineSite2(InlineSite2<'a>),
    FrameCookie(&'a FrameCookie),
    CallSiteInfo(&'a CallSiteInfo),
    HeapAllocSite(&'a HeapAllocSite),
    ManagedProc(ManagedProc<'a>),
    Annotation(Annotation<'a>),
}

impl<'a> SymData<'a> {
    /// Parses a symbol record. The caller has already parsed the length and kind of the record.
    /// The `data` parameter does not include the length or kind.
    pub fn parse(kind: SymKind, data: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(data);
        Self::from_parser(kind, &mut p)
    }

    /// Parses a symbol record. The caller has already parsed the length and kind of the record.
    /// The `p` parameter does not include the length or kind.
    ///
    /// This function allows the caller to observe how many bytes were actually consumed from
    /// the input stream.
    pub fn from_parser(kind: SymKind, p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(match kind {
            SymKind::S_OBJNAME => Self::ObjName(p.parse()?),
            SymKind::S_GPROC32 | SymKind::S_LPROC32 => Self::Proc(p.parse()?),
            SymKind::S_COMPILE3 => Self::Compile3(p.parse()?),
            SymKind::S_UDT => Self::Udt(p.parse()?),
            SymKind::S_CONSTANT => Self::Constant(p.parse()?),
            SymKind::S_MANCONSTANT => Self::Constant(p.parse()?),
            SymKind::S_PUB32 => Self::Pub(p.parse()?),
            SymKind::S_PUB32_ST => Self::Pub(Pub::parse_st(p)?),

            SymKind::S_PROCREF
            | SymKind::S_LPROCREF
            | SymKind::S_DATAREF
            | SymKind::S_ANNOTATIONREF => Self::RefSym2(p.parse()?),

            SymKind::S_LDATA32 | SymKind::S_GDATA32 | SymKind::S_LMANDATA | SymKind::S_GMANDATA => {
                Self::Data(p.parse()?)
            }

            SymKind::S_LTHREAD32 | SymKind::S_GTHREAD32 => Self::ThreadData(p.parse()?),
            SymKind::S_END => Self::End,
            SymKind::S_FRAMEPROC => Self::FrameProc(p.get()?),
            SymKind::S_REGREL32 => Self::RegRel(p.parse()?),
            SymKind::S_BLOCK32 => Self::Block(p.parse()?),
            SymKind::S_LOCAL => Self::Local(p.parse()?),
            SymKind::S_DEFRANGE_FRAMEPOINTER_REL => Self::DefRangeFramePointerRel(p.parse()?),
            SymKind::S_DEFRANGE_REGISTER => Self::DefRangeRegister(p.parse()?),
            SymKind::S_DEFRANGE_REGISTER_REL => Self::DefRangeRegisterRel(p.parse()?),
            SymKind::S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE => {
                Self::DefRangeFramePointerRelFullScope(p.get()?)
            }
            SymKind::S_DEFRANGE_SUBFIELD_REGISTER => Self::DefRangeSubFieldRegister(p.parse()?),
            SymKind::S_TRAMPOLINE => Self::Trampoline(p.parse()?),
            SymKind::S_BUILDINFO => Self::BuildInfo(p.parse()?),
            SymKind::S_UNAMESPACE => Self::UsingNamespace(p.parse()?),
            SymKind::S_INLINESITE_END => Self::InlineSiteEnd,
            SymKind::S_LABEL32 => Self::Label(p.parse()?),
            SymKind::S_CALLEES | SymKind::S_CALLERS => Self::FunctionList(p.parse()?),
            SymKind::S_INLINESITE => Self::InlineSite(p.parse()?),
            SymKind::S_INLINESITE2 => Self::InlineSite2(p.parse()?),
            SymKind::S_INLINEES => Self::FunctionList(p.parse()?),
            SymKind::S_FRAMECOOKIE => Self::FrameCookie(p.get()?),
            SymKind::S_CALLSITEINFO => Self::CallSiteInfo(p.get()?),
            SymKind::S_HEAPALLOCSITE => Self::HeapAllocSite(p.get()?),
            SymKind::S_GMANPROC | SymKind::S_LMANPROC => Self::ManagedProc(p.parse()?),
            SymKind::S_ANNOTATION => Self::Annotation(p.parse()?),

            _ => Self::Unknown,
        })
    }

    /// If this symbol record has a "name" field, return it. Else, `None`.
    pub fn name(&self) -> Option<&'a BStr> {
        match self {
            Self::Proc(proc) => Some(proc.name),
            Self::Data(data) => Some(data.name),
            Self::ThreadData(thread_data) => Some(thread_data.name),
            Self::Udt(udt) => Some(udt.name),
            Self::Local(local) => Some(local.name),
            Self::RefSym2(refsym) => Some(refsym.name),
            Self::Constant(c) => Some(c.name),
            Self::ManagedConstant(c) => Some(c.name),
            _ => None,
        }
    }
}

```

`pdb/src/syms/builder.rs`:

```rs
//! Supports building new symbol streams

use super::SymKind;
use crate::encoder::Encoder;
use crate::types::TypeIndex;
use bstr::BStr;

/// Writes symbol records into a buffer.
#[derive(Default)]
pub struct SymBuilder {
    /// Contains the symbol stream
    pub buffer: Vec<u8>,
}

impl SymBuilder {
    /// Creates a new empty symbol stream builder.
    pub fn new() -> Self {
        Self { buffer: Vec::new() }
    }

    /// Consumes this builder and returns the symbol stream.
    pub fn finish(self) -> Vec<u8> {
        self.buffer
    }

    /// Starts adding a new record to the builder.
    pub fn record(&mut self, kind: SymKind) -> RecordBuilder<'_> {
        let record_start = self.buffer.len();
        self.buffer.extend_from_slice(&[0, 0]); // placeholder for record length
        self.buffer.extend_from_slice(&kind.0.to_le_bytes());
        RecordBuilder {
            enc: Encoder::new(&mut self.buffer),
            record_start,
        }
    }

    /// Adds an `S_UDT` record.
    pub fn udt(&mut self, ty: TypeIndex, name: &BStr) {
        let mut r = self.record(SymKind::S_UDT);
        r.enc.u32(ty.0);
        r.enc.strz(name);
    }

    /// Adds an `S_PUB32` record.
    pub fn pub32(&mut self, flags: u32, offset: u32, segment: u16, name: &str) {
        let mut r = self.record(SymKind::S_PUB32);
        r.enc.u32(flags);
        r.enc.u32(offset);
        r.enc.u16(segment);
        r.enc.strz(name.into());
    }
}

/// State for writing a single record. When this is dropped, it will terminate the record.
pub struct RecordBuilder<'a> {
    /// Encoder which can write the payload of the current record.
    pub enc: Encoder<'a>,
    /// Byte offset of the start of the current record. We use this to patch the record length
    /// when we're done writing the record.
    record_start: usize,
}

impl<'a> Drop for RecordBuilder<'a> {
    fn drop(&mut self) {
        // Align the buffer to a 4-byte boundary
        match self.enc.buf.len() & 3 {
            1 => self.enc.buf.push(0xf1),
            2 => self.enc.buf.extend_from_slice(&[0xf1, 0xf2]),
            3 => self.enc.buf.extend_from_slice(&[0xf1, 0xf2, 0xf3]),
            _ => {}
        }

        let record_len = self.enc.buf.len() - self.record_start - 2;
        let record_field = &mut self.enc.buf[self.record_start..];
        record_field[0] = record_len as u8;
        record_field[1] = (record_len >> 8) as u8;
    }
}

```

`pdb/src/syms/iter.rs`:

```rs
use super::*;
use crate::utils::iter::HasRestLen;
use std::mem::take;
use tracing::error;

/// Parses [`Sym`] records from a symbol stream.
#[derive(Clone)]
pub struct SymIter<'a> {
    data: &'a [u8],
}

impl<'a> HasRestLen for SymIter<'a> {
    fn rest_len(&self) -> usize {
        self.data.len()
    }
}

/// Parses [`SymMut`] records from a symbol stream.
///
/// This iterator allows you to modify the payload of a symbol record but not to change its length
/// or its kind.
pub struct SymIterMut<'a> {
    data: &'a mut [u8],
}

impl<'a> SymIterMut<'a> {
    /// Parses the 4-byte CodeView signature that is at the start of a module symbol stream.
    pub fn get_signature(&mut self) -> Result<[u8; 4], ParserError> {
        let mut p = ParserMut::new(take(&mut self.data));
        let sig = p.copy()?;
        self.data = p.into_rest();
        Ok(sig)
    }
}

impl<'a> HasRestLen for SymIterMut<'a> {
    fn rest_len(&self) -> usize {
        self.data.len()
    }
}

impl<'a> SymIter<'a> {
    /// Creates a new symbol iterator.
    pub fn new(data: &'a [u8]) -> Self {
        Self { data }
    }

    /// Parses the 4-byte CodeView signature that is at the start of a module symbol stream.
    pub fn get_signature(&mut self) -> Result<[u8; 4], ParserError> {
        let mut p = Parser::new(self.data);
        let sig = p.copy()?;
        self.data = p.into_rest();
        Ok(sig)
    }

    /// The remaining unparsed bytes in the symbol stream.
    pub fn rest(&self) -> &'a [u8] {
        self.data
    }

    /// Parses a single record from `data`.
    pub fn one(data: &'a [u8]) -> Option<Sym<'a>> {
        Self::new(data).next()
    }
}

impl<'a> Iterator for SymIter<'a> {
    type Item = Sym<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.data.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.data);
        let record_len = p.u16().ok()?;
        if record_len < 2 {
            error!(
                invalid_record_len = record_len,
                iterator_pos = self.data.len(),
                "type record has invalid len"
            );
            return None;
        }

        let kind = SymKind(p.u16().ok()?);
        let record_data = p.bytes(record_len as usize - 2).ok()?;

        self.data = p.into_rest();

        Some(Sym {
            kind,
            data: record_data,
        })
    }
}

#[test]
fn test_sym_iter() {
    #[rustfmt::skip]
    let data: &[u8] = &[
        // record 0, total size = 8
        /* 0x0000 */ 6, 0,                              // size
        /* 0x0002 */ 0x4c, 0x11,                        // S_BUILDINFO
        /* 0x0004 */ 1, 2, 3, 4,                        // payload (ItemId)

        // record 1, total size = 12
        /* 0x0008 */ 10, 0,                              // size
        /* 0x000a */ 0x24, 0x11,                        // S_UNAMESPACE
        /* 0x000c */ b'b', b'o', b'o', b's',            // payload (6 bytes)
        /* 0x0010 */ b't', 0,
        /* 0x0012 */ 0xf1, 0xf2,                        // alignment padding (inside payload)

        // record 2, total size = 12
        /* 0x0014 */ 10, 0,                             // size
        /* 0x0016 */ 0x24, 0x11,                        // S_UNAMESPACE
        /* 0x0018 */ b'a', b'b', b'c', b'd',            // payload
        /* 0x001c */ b'e', b'f', b'g', 0,               // no alignment padding

        /* 0x0020 : end */
    ];

    let mut i = SymIter::new(data);

    // parse record 0
    assert_eq!(i.rest_len(), 0x20);
    let s0 = i.next().unwrap();
    assert_eq!(s0.kind, SymKind::S_BUILDINFO);
    let s0_data = s0.parse().unwrap();
    assert!(matches!(s0_data, SymData::BuildInfo(_)));

    // parse record 1
    assert_eq!(i.rest_len(), 0x18);
    let s1 = i.next().unwrap();
    assert_eq!(s1.kind, SymKind::S_UNAMESPACE);
    match s1.parse() {
        Ok(SymData::UsingNamespace(ns)) => assert_eq!(ns.namespace, "boost"),
        sd => panic!("wrong: {sd:?}"),
    }

    // parse record 2
    assert_eq!(i.rest_len(), 0xc);
    let s1 = i.next().unwrap();
    assert_eq!(s1.kind, SymKind::S_UNAMESPACE);
    match s1.parse() {
        Ok(SymData::UsingNamespace(ns)) => assert_eq!(ns.namespace, "abcdefg"),
        sd => panic!("wrong: {sd:?}"),
    }

    // end
    assert_eq!(i.rest_len(), 0);
    assert!(i.next().is_none());
}

impl<'a> SymIterMut<'a> {
    /// Creates a new symbol iterator.
    pub fn new(data: &'a mut [u8]) -> Self {
        Self { data }
    }

    /// The remaining unparsed bytes in the symbol stream.
    pub fn rest(&self) -> &[u8] {
        self.data
    }

    /// The remaining unparsed bytes in the symbol stream, with mutable access.
    pub fn rest_mut(&mut self) -> &mut [u8] {
        self.data
    }

    /// Converts this iterator into a mutable reference to the unparsed bytes in the symbol stream.
    pub fn into_rest(self) -> &'a mut [u8] {
        self.data
    }
}

impl<'a> Iterator for SymIterMut<'a> {
    type Item = SymMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.data.len() < 4 {
            return None;
        }

        // We steal self.data because it is the only way that split_at_mut() can work.
        let d = core::mem::take(&mut self.data);

        let mut p = Parser::new(d);
        let record_len = p.u16().ok()?;
        if record_len < 2 {
            error!(
                record_len,
                iterator_len = self.data.len(),
                "type record has invalid len"
            );
            self.data = d;
            return None;
        }

        let kind = SymKind(p.u16().ok()?);

        let (entire_record_data, hi) = d.split_at_mut(2 + record_len as usize);
        self.data = hi;

        let record_data = &mut entire_record_data[4..];

        Some(SymMut {
            kind,
            data: record_data,
        })
    }
}

```

`pdb/src/syms/kind.rs`:

```rs
//! Symbol kind enumeration

#[cfg(doc)]
use super::BlockHeader;

/// Identifies symbol records.
///
/// Symbol records are stored in the Global Symbol Stream and in each per-module symbol stream.
///
/// Many symbols can only appear in the Global Symbol Stream or in a per-module symbol stream.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct SymKind(pub u16);

macro_rules! sym_kinds {
    (
        $( $code:expr, $name:ident; )*
    ) => {
        #[allow(missing_docs)]
        impl SymKind {
            $(
                pub const $name: SymKind = SymKind($code);
            )*
        }

        static SYM_NAMES: &[(SymKind, &str)] = &[
            $(
                (SymKind($code), stringify!($name)),
            )*
        ];
    }
}

sym_kinds! {
    0x0001, S_COMPILE;
    0x0006, S_END;
    0x0007, S_SKIP;
    0x0008, S_CVRESERVE;
    0x0009, S_OBJNAME_ST;
    0x000d, S_RETURN;
    // 0x0100..0x0400 is for 16-bit types
    0x0400, S_PROCREF_ST;
    0x0401, S_DATAREF_ST;
    0x0402, S_ALIGN;
    0x0403, S_LPROCREF_ST;
    0x0404, S_OEM;

    0x1000, S_TI16_MAX;
    0x1001, S_REGISTER_ST;
    0x1002, S_CONSTANT_ST;
    0x1003, S_UDT_ST;
    0x1004, S_COBOLUDT_ST;
    0x1005, S_MANYREG_ST;
    0x1006, S_BPREL32_ST;
    0x1007, S_LDATA32_ST;
    0x1008, S_GDATA32_ST;
    0x1009, S_PUB32_ST;
    0x100a, S_LPROC32_ST;
    0x100b, S_GPROC32_ST;
    0x100c, S_VFTABLE32;
    0x100d, S_REGREL32_ST;
    0x100e, S_LTHREAD32_ST;
    0x100f, S_GTHREAD32_ST;
    0x1012, S_FRAMEPROC;
    0x1019, S_ANNOTATION;

    0x1101, S_OBJNAME;
    0x1102, S_THUNK32;
    0x1103, S_BLOCK32;
    0x1104, S_WITH32;
    0x1105, S_LABEL32;
    0x1106, S_REGISTER;
    0x1107, S_CONSTANT;
    0x1108, S_UDT;
    0x1109, S_COBOLUDT;
    0x110a, S_MANYREG;
    0x110b, S_BPREL32;
    0x110c, S_LDATA32;
    0x110d, S_GDATA32;
    0x110e, S_PUB32;
    0x110f, S_LPROC32;

    0x1110, S_GPROC32;
    0x1111, S_REGREL32;
    0x1112, S_LTHREAD32;
    0x1113, S_GTHREAD32;
    0x1116, S_COMPILE2;
    0x1117, S_MANYREG2;
    0x1118, S_LPROCIA64;
    0x1119, S_GPROCIA64;
    0x111a, S_LOCALSLOT;
    0x111b, S_PARAMSLOT;
    0x111c, S_LMANDATA;
    0x111d, S_GMANDATA;
    0x111e, S_MANFRAMEREL;
    0x111f, S_MANREGISTER;

    0x1120, S_MANSLOT;
    0x1121, S_MANMANYREG;
    0x1122, S_MANREGREL;
    0x1123, S_MANMANYREG2;
    0x1124, S_UNAMESPACE;
    0x1125, S_PROCREF;
    0x1126, S_DATAREF;
    0x1127, S_LPROCREF;
    0x1128, S_ANNOTATIONREF;
    0x1129, S_TOKENREF;
    0x112a, S_GMANPROC;
    0x112b, S_LMANPROC;
    0x112c, S_TRAMPOLINE;
    0x112d, S_MANCONSTANT;
    0x112e, S_ATTR_FRAMEREL;
    0x112f, S_ATTR_REGISTER;

    0x1130, S_ATTR_REGREL;
    0x1131, S_ATTR_MANYREG;
    0x1132, S_SEPCODE;
    0x1133, S_LOCAL_2005;
    0x1134, S_DEFRANGE_2005;
    0x1135, S_DEFRANGE2_2005;
    0x1136, S_SECTION;
    0x1137, S_COFFGROUP;
    0x1138, S_EXPORT;
    0x1139, S_CALLSITEINFO;
    0x113a, S_FRAMECOOKIE;
    0x113b, S_DISCARDED;
    0x113c, S_COMPILE3;
    0x113d, S_ENVBLOCK;
    0x113e, S_LOCAL;
    0x113f, S_DEFRANGE;

    0x1140, S_DEFRANGE_SUBFIELD;
    0x1141, S_DEFRANGE_REGISTER;
    0x1142, S_DEFRANGE_FRAMEPOINTER_REL;
    0x1143, S_DEFRANGE_SUBFIELD_REGISTER;
    0x1144, S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE;
    0x1145, S_DEFRANGE_REGISTER_REL;
    0x1146, S_LPROC32_ID;
    0x1147, S_GPROC32_ID;
    0x1148, S_LPROCMIPS_ID;
    0x1149, S_GPROCMIPS_ID;
    0x114a, S_LPROCIA64_ID;
    0x114b, S_GPROCIA64_ID;
    0x114c, S_BUILDINFO;
    0x114d, S_INLINESITE;
    0x114e, S_INLINESITE_END;
    0x114f, S_PROC_ID_END;

    0x1150, S_DEFRANGE_HLSL;
    0x1151, S_GDATA_HLSL;
    0x1152, S_LDATA_HLSL;
    0x1153, S_FILESTATIC;
    0x1154, S_LOCAL_DPC_GROUPSHARED;
    0x1155, S_LPROC32_DPC;
    0x1156, S_LPROC32_DPC_ID;
    0x1157, S_DEFRANGE_DPC_PTR_TAG;
    0x1158, S_DPC_SYM_TAG_MAP;
    0x1159, S_ARMSWITCHTABLE;
    0x115a, S_CALLEES;
    0x115b, S_CALLERS;
    0x115c, S_POGODATA;
    0x115d, S_INLINESITE2;
    0x115e, S_HEAPALLOCSITE;
    0x115f, S_MOD_TYPEREF;

    0x1160, S_REF_MINIPDB;
    0x1161, S_PDBMAP;
    0x1162, S_GDATA_HLSL32;
    0x1163, S_LDATA_HLSL32;
    0x1164, S_GDATA_HLSL32_EX;
    0x1165, S_LDATA_HLSL32_EX;
    0x1167, S_FASTLINK;
    0x1168, S_INLINEES;
}

impl std::fmt::Debug for SymKind {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        if let Ok(index) = SYM_NAMES.binary_search_by_key(self, |ii| ii.0) {
            <str as std::fmt::Display>::fmt(SYM_NAMES[index].1, f)
        } else {
            let b0 = (self.0 & 0xff) as u8;
            let b1 = (self.0 >> 8) as u8;
            fn to_c(b: u8) -> char {
                if (32..=126).contains(&b) {
                    char::from(b)
                } else {
                    '_'
                }
            }

            write!(f, "S_(??{:04x} {}{})", self.0, to_c(b1), to_c(b0))
        }
    }
}

#[test]
fn test_sym_kind_debug() {
    assert_eq!(format!("{:?}", SymKind::S_GPROC32), "S_GPROC32");
    assert_eq!(format!("{:?}", SymKind(0x31aa)), "S_(??31aa 1_)");
}

impl SymKind {
    /// True if this `SymKind` starts a scope. All symbols that start a block begin with
    /// [`BlockHeader`].
    pub fn starts_scope(self) -> bool {
        matches!(
            self,
            SymKind::S_GPROC32
                | SymKind::S_LPROC32
                | SymKind::S_LPROC32_DPC
                | SymKind::S_LPROC32_DPC_ID
                | SymKind::S_GPROC32_ID
                | SymKind::S_BLOCK32
                | SymKind::S_THUNK32
                | SymKind::S_INLINESITE
                | SymKind::S_INLINESITE2
                | SymKind::S_SEPCODE
                | SymKind::S_GMANPROC
                | SymKind::S_LMANPROC
        )
    }

    /// Indicates whether this `SymKind` ends a scope.
    ///
    /// There are no `SymKind` values that both start and end a scope.
    ///
    /// In all well-formed symbol streams, every symbol that starts a scope has a matching symbol
    /// that ends that scope.
    pub fn ends_scope(self) -> bool {
        matches!(
            self,
            SymKind::S_END | SymKind::S_PROC_ID_END | SymKind::S_INLINESITE_END
        )
    }

    /// Returns `true` if this symbol can be the _target_ of a "reference to symbol" in the
    /// Global Symbol Stream.
    pub fn is_refsym_target(self) -> bool {
        matches!(
            self,
            SymKind::S_GPROC32
                | SymKind::S_LPROC32
                | SymKind::S_GMANPROC
                | SymKind::S_LMANPROC
                | SymKind::S_GDATA32
                | SymKind::S_LDATA32
                | SymKind::S_ANNOTATION
        )
    }

    /// Returns `true` if this symbol can be the _source_ of a "reference to symbol"
    /// in the Global Symbol Stream.
    pub fn is_refsym_source(self) -> bool {
        matches!(
            self,
            SymKind::S_LPROCREF
                | SymKind::S_PROCREF
                | SymKind::S_ANNOTATIONREF
                | SymKind::S_TOKENREF
                | SymKind::S_DATAREF
        )
    }
}

```

`pdb/src/syms/offset_segment.rs`:

```rs
use super::*;

/// Stores an `offset` and `segment` pair, in that order. This structure is directly embedded in
/// on-disk structures.
#[repr(C)]
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Default, Clone, Eq)]
pub struct OffsetSegment {
    /// The offset in bytes of a symbol within a segment.
    pub offset: U32<LE>,

    /// The segment (section) index.
    pub segment: U16<LE>,
}

impl PartialEq for OffsetSegment {
    #[inline]
    fn eq(&self, other: &Self) -> bool {
        self.as_u64() == other.as_u64()
    }
}

impl PartialOrd for OffsetSegment {
    #[inline]
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for OffsetSegment {
    #[inline]
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        Ord::cmp(&self.as_u64(), &other.as_u64())
    }
}

impl OffsetSegment {
    /// The offset in bytes of a symbol within a segment.
    #[inline]
    pub fn offset(&self) -> u32 {
        self.offset.get()
    }

    /// The segment (section) index.
    #[inline]
    pub fn segment(&self) -> u16 {
        self.segment.get()
    }

    /// Combines the segment and offset into a tuple. The segment is the first element and the
    /// offset is the second element. This order gives a sorting order that sorts by segment first.
    #[inline]
    pub fn as_tuple(&self) -> (u16, u32) {
        (self.segment.get(), self.offset.get())
    }

    /// Combines the segment and offset into a single `u64` value, with the segment in the
    /// higher-order bits. This allows for efficient comparisons.
    #[inline]
    pub fn as_u64(&self) -> u64 {
        ((self.segment.get() as u64) << 32) | (self.offset.get() as u64)
    }
}

impl std::fmt::Display for OffsetSegment {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "[{:04x}:{:08x}]", self.segment.get(), self.offset.get())
    }
}

impl Debug for OffsetSegment {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        <Self as std::fmt::Display>::fmt(self, f)
    }
}

impl std::hash::Hash for OffsetSegment {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        state.write_u16(self.segment());
        state.write_u32(self.offset());
    }
}

```

`pdb/src/taster.rs`:

```rs
//! Determines whether a given file header is a PDB/MSF file, PDBZ/MSFZ file, or a Portable PDB file.

use sync_file::ReadAt;

/// Enumerates the kind of PDBs files that are recognized.
#[derive(Copy, Clone, Eq, PartialEq)]
pub enum Flavor {
    /// An ordinary PDB file.
    Pdb,
    /// A compressed PDB (PDZ) file.
    Pdz,
    /// A "Portable PDB" file.
    PortablePdb,
}

/// Determines whether a given file header is a PDB/MSF file, PDBZ/MSFZ file, or a Portable PDB file.
pub fn what_flavor<F: ReadAt>(f: &F) -> Result<Option<Flavor>, std::io::Error> {
    let mut header = [0u8; 0x100];
    let _n = f.read_at(&mut header, 0)?;
    if ms_pdb_msf::is_file_header_msf(&header) {
        Ok(Some(Flavor::Pdb))
    } else if ms_pdb_msfz::is_header_msfz(&header) {
        Ok(Some(Flavor::Pdz))
    } else if is_header_portable_pdb(&header) {
        Ok(Some(Flavor::PortablePdb))
    } else {
        Ok(None)
    }
}

fn is_header_portable_pdb(header: &[u8]) -> bool {
    header.len() >= 24 && header[16..24] == *b"PDB v1.0"
}

```

`pdb/src/tpi.rs`:

```rs
//! Type Information Stream (TPI)
//!
//! Layout of a Type Stream:
//!
//! * `TypeStreamHeader` - specifies lots of important parameters
//! * Type Record Data
//!
//! Each Type Stream may also have an associated Type Hash Stream. The Type Hash Stream contains
//! indexing information that helps find records within the main Type Stream. The Type Stream
//! Header specifies several parameters that are needed for finding and decoding the Type Hash
//! Stream.
//!
//! The Type Hash Stream contains:
//!
//! * Hash Value Buffer: Contains a list of hash values, one for each Type Record in the
//!   Type Stream.
//!
//!   The offset and size of the Hash Value Buffer is specified in the `TypeStreamHeader`, in the
//!   `hash_value_buffer_offset` and `hash_value_buffer_length` fields, respectively.
//!
//!   It should be assumed that there are either 0 hash values, or a number equal to the number of
//!   type records in the TPI stream (`type_index_end - type_end_begin`). Thus, if
//!   `hash_value_buffer_length` is not equal to `(type_index_end - type_end_begin) * hash_key_size`
//!    we can consider the PDB malformed.
//!
//! * Type Index Offset Buffer - A list of pairs of `u32` values where the first is a Type Index
//!   and the second is the offset within Type Record Data of the type with this index.
//!   This enables a binary search to find a given Type Index record.
//!
//!   The offset and size of the Type Index Offset Buffer is specified in the `TypeStreamHeader`,
//!   in the `index_offset_buffer_offset` and `index_offset_buffer_length` fields, respectively.
//!
//! * Hash Adjustment Buffer - A hash table whose keys are the hash values in the hash value
//!   buffer and whose values are type indices.
//!
//!   The offset and size of the Type Index Offset BUffer is specified in the `TypeStreamHeader`,
//!   in the `index_offset_buffer_offset` and `index_offset_buffer_length` fields, respectively.
//!

pub mod hash;

use super::*;
use crate::parser::Parser;
use crate::types::fields::{Field, IterFields};
use crate::types::{build_types_starts, TypeData, TypeIndex, TypeIndexLe, TypeRecord, TypesIter};
use anyhow::bail;
use std::fmt::Debug;
use std::mem::size_of;
use std::ops::Range;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, I32, LE, U32};

/// The header of the TPI stream.
#[allow(missing_docs)]
#[derive(Clone, Eq, PartialEq, IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
pub struct TypeStreamHeader {
    pub version: U32<LE>,
    pub header_size: U32<LE>,
    pub type_index_begin: TypeIndexLe,
    pub type_index_end: TypeIndexLe,
    /// The number of bytes of type record data following the `TypeStreamHeader`.
    pub type_record_bytes: U32<LE>,

    pub hash_stream_index: StreamIndexU16,
    pub hash_aux_stream_index: StreamIndexU16,

    /// The size of each hash key in the Hash Value Substream. For the current version of TPI,
    /// this value should always be 4.
    pub hash_key_size: U32<LE>,
    /// The number of hash buckets. This is used when calculating the record hashes. Each hash
    /// is computed, and then it is divided by num_hash_buckets and the remainder becomes the
    /// final hash.
    ///
    /// If `hash_value_buffer_length` is non-zero, then `num_hash_buckets` must also be non-zero.
    pub num_hash_buckets: U32<LE>,
    pub hash_value_buffer_offset: I32<LE>,
    pub hash_value_buffer_length: U32<LE>,

    pub index_offset_buffer_offset: I32<LE>,
    pub index_offset_buffer_length: U32<LE>,

    pub hash_adj_buffer_offset: I32<LE>,
    pub hash_adj_buffer_length: U32<LE>,
}

impl TypeStreamHeader {
    /// Makes an empty one
    pub fn empty() -> Self {
        Self {
            version: Default::default(),
            header_size: U32::new(size_of::<TypeStreamHeader>() as u32),
            type_index_begin: TypeIndexLe(U32::new(TypeIndex::MIN_BEGIN.0)),
            type_index_end: TypeIndexLe(U32::new(TypeIndex::MIN_BEGIN.0)),
            type_record_bytes: Default::default(),
            hash_stream_index: StreamIndexU16::NIL,
            hash_aux_stream_index: StreamIndexU16::NIL,
            hash_key_size: Default::default(),
            num_hash_buckets: Default::default(),
            hash_value_buffer_offset: Default::default(),
            hash_value_buffer_length: Default::default(),
            index_offset_buffer_offset: Default::default(),
            index_offset_buffer_length: Default::default(),
            hash_adj_buffer_offset: Default::default(),
            hash_adj_buffer_length: Default::default(),
        }
    }
}

/// The size of the `TpiStreamHeader` structure.
pub const TPI_STREAM_HEADER_LEN: usize = size_of::<TypeStreamHeader>();

/// The expected value of `TypeStreamHeader::version`.
pub const TYPE_STREAM_VERSION_2004: u32 = 20040203;

/// Contains a TPI Stream or IPI Stream.
pub struct TypeStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// The stream data. This contains the entire type stream, including header and type records.
    pub stream_data: StreamData,

    type_index_begin: TypeIndex,
    type_index_end: TypeIndex,

    /// A starts vector for type record offsets. This is created on-demand, since many users of
    /// `TypeStream` do not need this.
    record_starts: OnceCell<Vec<u32>>,
}

/// Distinguishes the TPI and IPI streams.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub enum TypeStreamKind {
    /// The primary type stream
    TPI,
    /// The ID stream
    IPI,
}

impl TypeStreamKind {
    /// Get the stream index. Fortunately, the stream indexes are fixed.
    pub fn stream(self) -> Stream {
        match self {
            Self::IPI => Stream::IPI,
            Self::TPI => Stream::TPI,
        }
    }
}

/// Represents an entry in the Hash Index Offset Substream.
#[repr(C)]
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
pub struct HashIndexPair {
    /// The type index at the start of this range.
    pub type_index: TypeIndexLe,
    /// The offset within the Type Records Substream (not the entire Type Stream) where this
    /// record begins.
    pub offset: U32<LE>,
}

impl<StreamData> TypeStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// Gets a reference to the stream header
    pub fn header(&self) -> Option<&TypeStreamHeader> {
        let stream_data: &[u8] = self.stream_data.as_ref();
        let (header, _) = TypeStreamHeader::ref_from_prefix(stream_data).ok()?;
        Some(header)
    }

    /// Returns the version of the stream, or `TYPE_STREAM_VERSION_2004` if this is an empty stream.
    pub fn version(&self) -> u32 {
        if let Some(header) = self.header() {
            header.version.get()
        } else {
            TYPE_STREAM_VERSION_2004
        }
    }

    /// Returns the stream index of the related hash stream, if any.
    pub fn hash_stream(&self) -> Option<u32> {
        self.header()?.hash_stream_index.get()
    }

    /// Checks whether this is a degenerate empty stream.
    pub fn is_empty(&self) -> bool {
        self.stream_data.as_ref().is_empty()
    }

    /// Gets a mutable reference to the stream header
    pub fn header_mut(&mut self) -> Option<&mut TypeStreamHeader>
    where
        StreamData: AsMut<[u8]>,
    {
        let (header, _) = TypeStreamHeader::mut_from_prefix(self.stream_data.as_mut()).ok()?;
        Some(header)
    }

    /// The type index of the first type record.
    pub fn type_index_begin(&self) -> TypeIndex {
        self.type_index_begin
    }

    /// The type index of the last type record, plus 1.
    pub fn type_index_end(&self) -> TypeIndex {
        self.type_index_end
    }

    /// The number of types defined in the type stream.
    pub fn num_types(&self) -> u32 {
        self.type_index_end.0 - self.type_index_begin.0
    }

    /// Gets the byte offset within the stream of the record data.
    pub fn records_offset(&self) -> usize {
        if let Some(header) = self.header() {
            header.header_size.get() as usize
        } else {
            0
        }
    }

    /// Returns the encoded type records found in the TPI or IPI stream.
    ///
    /// The type records immediately follow the type stream. The length is given by the
    /// header field type_record_bytes. The values in the header were validated in
    /// read_tpi_or_ipi_stream(), so we do not need to check them again, here.
    pub fn type_records_bytes(&self) -> &[u8] {
        let records_range = self.type_records_range();
        if records_range.is_empty() {
            &[]
        } else {
            &self.stream_data.as_ref()[records_range]
        }
    }

    /// Returns the encoded type records found in the type stream.
    pub fn type_records_bytes_mut(&mut self) -> &mut [u8]
    where
        StreamData: AsMut<[u8]>,
    {
        let records_range = self.type_records_range();
        if records_range.is_empty() {
            &mut []
        } else {
            &mut self.stream_data.as_mut()[records_range]
        }
    }

    /// Returns the byte range of the encoded type records found in the type stream.
    pub fn type_records_range(&self) -> std::ops::Range<usize> {
        if let Some(header) = self.header() {
            let size = header.type_record_bytes.get();
            if size == 0 {
                return 0..0;
            }
            let type_records_start = header.header_size.get();
            let type_records_end = type_records_start + size;
            type_records_start as usize..type_records_end as usize
        } else {
            0..0
        }
    }

    /// Iterates the types contained within this type stream.
    pub fn iter_type_records(&self) -> TypesIter<'_> {
        TypesIter::new(self.type_records_bytes())
    }

    /// Parses the header of a Type Stream and validates it.
    pub fn parse(stream_index: Stream, stream_data: StreamData) -> anyhow::Result<Self> {
        let stream_bytes: &[u8] = stream_data.as_ref();

        if stream_bytes.is_empty() {
            return Ok(Self {
                stream_data,
                type_index_begin: TypeIndex::MIN_BEGIN,
                type_index_end: TypeIndex::MIN_BEGIN,
                record_starts: OnceCell::new(),
            });
        }

        let mut p = Parser::new(stream_bytes);
        let tpi_stream_header: TypeStreamHeader = p.copy()?;

        let type_index_begin = tpi_stream_header.type_index_begin.get();
        let type_index_end = tpi_stream_header.type_index_end.get();
        if type_index_end < type_index_begin {
            bail!(
                "Type stream (stream {stream_index}) has invalid values in header.  \
                 The type_index_begin field is greater than the type_index_end field."
            );
        }

        if type_index_begin < TypeIndex::MIN_BEGIN {
            bail!(
                "The Type Stream has an invalid value for type_index_begin ({type_index_begin:?}). \
                 It is less than the minimum required value ({}).",
                TypeIndex::MIN_BEGIN.0
            );
        }

        let type_data_start = tpi_stream_header.header_size.get();
        if type_data_start < TPI_STREAM_HEADER_LEN as u32 {
            bail!(
                "Type stream (stream {stream_index}) has invalid values in header.  \
                 The header_size field is smaller than the definition of the actual header."
            );
        }

        let type_data_end = type_data_start + tpi_stream_header.type_record_bytes.get();
        if type_data_end > stream_bytes.len() as u32 {
            bail!(
                "Type stream (stream {stream_index}) has invalid values in header.  \
                   The header_size and type_record_bytes fields exceed the size of the stream."
            );
        }

        Ok(TypeStream {
            stream_data,
            type_index_begin,
            type_index_end,
            record_starts: OnceCell::new(),
        })
    }

    /// Builds a "starts" table that gives the starting location of each type record.
    pub fn build_types_starts(&self) -> TypeIndexMap {
        let starts =
            crate::types::build_types_starts(self.num_types() as usize, self.type_records_bytes());

        TypeIndexMap {
            type_index_begin: self.type_index_begin,
            type_index_end: self.type_index_end,
            starts,
        }
    }

    /// Creates a new `TypeStream` that referenced the stream data of this `TypeStream`.
    /// This is typically used for temporarily creating a `TypeStream<&[u8]>` from a
    /// `TypeStream<Vec<u8>>`.
    pub fn to_ref(&self) -> TypeStream<&[u8]> {
        TypeStream {
            stream_data: self.stream_data.as_ref(),
            type_index_begin: self.type_index_begin,
            type_index_end: self.type_index_end,
            record_starts: OnceCell::new(),
        }
    }

    /// Gets the "starts" vector for the byte offsets of the records in this `TypeStream`.
    ///
    /// This function will create the starts vector on-demand.
    pub fn record_starts(&self) -> &[u32] {
        self.record_starts.get_or_init(|| {
            let type_records = self.type_records_bytes();
            build_types_starts(self.num_types() as usize, type_records)
        })
    }

    /// Returns `true` if `type_index` refers to a primitive type.
    pub fn is_primitive(&self, type_index: TypeIndex) -> bool {
        type_index < self.type_index_begin
    }

    /// Retrieves the type record identified by `type_index`.
    ///
    /// This should only be used for non-primitive `TypeIndex` values. If this is called with a
    /// primitive `TypeIndex` then it will return `Err`.
    pub fn record(&self, type_index: TypeIndex) -> anyhow::Result<TypeRecord<'_>> {
        let Some(relative_type_index) = type_index.0.checked_sub(self.type_index_begin.0) else {
            bail!("The given TypeIndex is a primitive type index, not a type record.");
        };

        let starts = self.record_starts();
        let Some(&record_start) = starts.get(relative_type_index as usize) else {
            bail!("The given TypeIndex is out of bounds (exceeds maximum allowed TypeIndex)");
        };

        let all_type_records = self.type_records_bytes();
        let Some(this_type_record_slice) = all_type_records.get(record_start as usize..) else {
            // This should never happen, but let's be cautious.
            bail!("Internal error: record offset is out of range.");
        };

        let mut iter = TypesIter::new(this_type_record_slice);
        if let Some(record) = iter.next() {
            Ok(record)
        } else {
            bail!("Failed to decode type record");
        }
    }

    /// Iterate the fields of an `LF_STRUCTURE`, `LF_CLASS`, `LF_ENUM`, etc. This correctly
    /// iterates across chains of `LF_FIELDLIST`.
    pub fn iter_fields(&self, field_list: TypeIndex) -> IterFieldChain<'_, StreamData> {
        // We initialize `fields` to an empty iterator so that the first iteration of
        // IterFieldChain::next() will find no records and will then check next_field_list.
        IterFieldChain {
            type_stream: self,
            next_field_list: if field_list.0 != 0 {
                Some(field_list)
            } else {
                None
            },
            fields: IterFields { bytes: &[] },
        }
    }
}

/// Iterator state for `iter_fields`
pub struct IterFieldChain<'a, StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// The current `LF_FIELDLIST` record that we are decoding.
    fields: IterFields<'a>,

    /// Allows us to read `LF_FIELDLIST` records.
    type_stream: &'a TypeStream<StreamData>,

    /// The pointer to the next `LF_FIELDLIST` that we will decode.
    next_field_list: Option<TypeIndex>,
}

impl<'a, StreamData> Iterator for IterFieldChain<'a, StreamData>
where
    StreamData: AsRef<[u8]>,
{
    type Item = Field<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        loop {
            if let Some(field) = self.fields.next() {
                if let Field::Index(index) = &field {
                    // The full field list is split across more than one LF_FIELDLIST record.
                    // Store the link to the next field list and do not return this item to the caller.
                    self.next_field_list = Some(*index);
                    continue;
                }

                return Some(field);
            }

            // We have run out of fields in the current LF_FIELDLIST record.
            // See if there is a pointer to another LF_FIELDLIST.
            let next_field_list = self.next_field_list.take()?;
            let next_record = self.type_stream.record(next_field_list).ok()?;
            match next_record.parse().ok()? {
                TypeData::FieldList(fl) => {
                    // Restart iteration on the new field list.
                    self.fields = fl.iter();
                }
                _ => {
                    // Wrong record type!
                    return None;
                }
            }
        }
    }
}

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads the TPI stream.
    pub fn read_type_stream(&self) -> anyhow::Result<TypeStream<Vec<u8>>> {
        self.read_tpi_or_ipi_stream(Stream::TPI)
    }

    /// Reads the IPI stream.
    pub fn read_ipi_stream(&self) -> anyhow::Result<TypeStream<Vec<u8>>> {
        self.read_tpi_or_ipi_stream(Stream::IPI)
    }

    /// Reads the TPI or IPI stream.
    pub fn read_tpi_or_ipi_stream(
        &self,
        stream_index: Stream,
    ) -> anyhow::Result<TypeStream<Vec<u8>>> {
        let stream_data = self.read_stream_to_vec(stream_index.into())?;
        TypeStream::parse(stream_index, stream_data)
    }
}

/// Maps `TypeIndex` values to the byte range of records within a type stream.
pub struct TypeIndexMap {
    /// Copied from type stream header.
    pub type_index_begin: TypeIndex,

    /// Copied from type stream header.
    pub type_index_end: TypeIndex,

    /// Contains a "starts" vector for the byte offsets of each type record.
    ///
    /// This vector has an additional value at the end, which gives the size in bytes of the
    /// type stream.
    pub starts: Vec<u32>,
}

impl TypeIndexMap {
    /// Tests whether a `TypeIndex` is a primitive type.
    pub fn is_primitive(&self, ti: TypeIndex) -> bool {
        ti < self.type_index_begin
    }

    /// Given a `TypeIndex`, returns the byte range within a `TypeStream` where that record
    /// is stored.
    ///
    /// If `ti` is a primitive type then this function will return `Err`. The caller should
    /// use the `is_primitive` method to check whether a `TypeIndex` is a primitive type.
    pub fn record_range(&self, ti: TypeIndex) -> anyhow::Result<Range<usize>> {
        let Some(i) = ti.0.checked_sub(self.type_index_begin.0) else {
            bail!("The TypeIndex is a primitive type, not a type record.");
        };

        if let Some(w) = self.starts.get(i as usize..i as usize + 2) {
            Ok(w[0] as usize..w[1] as usize)
        } else {
            bail!("The TypeIndex is out of range.");
        }
    }
}

/// Represents the cached state of a Type Stream header.
pub struct CachedTypeStreamHeader {
    pub(crate) header: Option<TypeStreamHeader>,
}

impl CachedTypeStreamHeader {
    /// Gets direct access to the type stream header, if any.
    pub fn header(&self) -> Option<&TypeStreamHeader> {
        self.header.as_ref()
    }

    /// Gets the beginning of the type index space, or `TypeIndex::MIN_BEGIN` if this type stream
    /// does not contain any data.
    pub fn type_index_begin(&self) -> TypeIndex {
        if let Some(h) = &self.header {
            h.type_index_begin.get()
        } else {
            TypeIndex::MIN_BEGIN
        }
    }

    /// Gets the end of the type index space, or `TypeIndex::MIN_BEGIN` if this type stream does
    /// not contain any data.
    pub fn type_index_end(&self) -> TypeIndex {
        if let Some(h) = &self.header {
            h.type_index_end.get()
        } else {
            TypeIndex::MIN_BEGIN
        }
    }
}

```

`pdb/src/tpi/hash.rs`:

```rs
//! Hashing functions for Type Records
//!
//! # References
//!
//! * [`TPI1::hashPrec` in `tpi.cpp`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/dbi/tpi.cpp#L1296)

use crate::hash::hash_u32;
use crate::parser::ParserError;
use crate::types::{Leaf, TypeData, UdtProperties};
use bstr::BStr;
use zerocopy::IntoBytes;

/// Hash a type record, using the same rules as `TPI1::hashPrec`.
pub fn hash_type_record(
    kind: Leaf,
    record_bytes: &[u8],
    payload: &[u8],
) -> Result<u32, ParserError> {
    let tdata = TypeData::parse_bytes(kind, payload)?;

    match &tdata {
        TypeData::Alias(t) => Ok(hash_u32(t.name)),

        // This handles LF_CLASS, LF_STRUCTURE, and LF_INTERFACE.
        TypeData::Struct(t) => Ok(hash_udt_name(
            t.fixed.property.get(),
            record_bytes,
            t.name,
            t.unique_name,
        )),

        TypeData::Union(t) => Ok(hash_udt_name(
            t.fixed.property.get(),
            record_bytes,
            t.name,
            t.unique_name,
        )),

        TypeData::Enum(t) => Ok(hash_udt_name(
            t.fixed.property.get(),
            record_bytes,
            t.name,
            t.unique_name,
        )),

        TypeData::UdtSrcLine(t) => Ok(hash_u32(t.ty.as_bytes())),
        TypeData::UdtModSrcLine(t) => Ok(hash_u32(t.ty.as_bytes())),
        _ => Ok(crate::hash::hash_sig(record_bytes, 0)),
    }
}

fn hash_udt_name(
    prop: UdtProperties,
    record_bytes: &[u8],
    name: &BStr,
    unique_name: Option<&BStr>,
) -> u32 {
    if !prop.fwdref() && !is_udt_anon_name(name) {
        if prop.scoped() {
            if let Some(unique_name) = unique_name {
                return hash_u32(unique_name);
            }
        } else {
            // This branch is equivalent to the case handled by fIsGlobalDefUdt().
            return hash_u32(name);
        }
    }

    crate::hash::hash_sig(record_bytes, 0)
}

/// Tests if `name` is indicates that this UDT is an anonymous UDT.
pub fn is_udt_anon_name(name: &BStr) -> bool {
    name == "<unnamed-tag>"
        || name == "__unnamed"
        || name.ends_with(b"::<unnamed-tag>")
        || name.ends_with(b"::__unnamed")
}

```

`pdb/src/types.rs`:

```rs
//! Code for decoding type record streams (the TPI and IPI streams).
//!
//! # References
//! * [CodeView Type Records](https://llvm.org/docs/PDB/CodeViewTypes.html)

mod iter;
#[doc(inline)]
pub use iter::*;

mod kind;
#[doc(inline)]
pub use kind::*;

pub mod fields;
pub mod number;
pub mod primitive;
pub mod visitor;

mod records;
#[doc(inline)]
pub use records::*;

pub use fields::FieldList;

use self::primitive::dump_primitive_type_index;
use crate::parser::{Number, Parse, Parser, ParserError};
use bitfield::bitfield;
use bstr::BStr;
use std::fmt::Debug;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned, LE, U16, U32};

/// A type index refers to another type record, or to a primitive type.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct TypeIndex(pub u32);

impl TypeIndex {
    /// The minimum value for a `type_index_begin` value.
    ///
    /// This value comes from the fact that the first 0x1000 values are reserved for primitive
    /// types.  See `primitive_types.md` in the specification.
    pub const MIN_BEGIN: TypeIndex = TypeIndex(0x1000);
}

impl std::fmt::Debug for TypeIndex {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        if self.0 < TypeIndex::MIN_BEGIN.0 {
            dump_primitive_type_index(fmt, *self)
        } else {
            write!(fmt, "T#0x{:x}", self.0)
        }
    }
}

/// The serialized form of [`TypeIndex`]. This can be embedded directly in data structures
/// stored on disk.
#[derive(
    Copy, Clone, Eq, PartialEq, Hash, FromBytes, IntoBytes, Immutable, KnownLayout, Unaligned,
)]
#[repr(transparent)]
pub struct TypeIndexLe(pub U32<LE>);

impl From<TypeIndex> for TypeIndexLe {
    #[inline(always)]
    fn from(value: TypeIndex) -> TypeIndexLe {
        TypeIndexLe(U32::new(value.0))
    }
}

impl Debug for TypeIndexLe {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        let ti = self.get();
        Debug::fmt(&ti, fmt)
    }
}

impl TypeIndexLe {
    /// Converts this value to host byte-order.
    #[inline(always)]
    pub fn get(self) -> TypeIndex {
        TypeIndex(self.0.get())
    }
}

/// Parsed details of a type record.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub enum TypeData<'a> {
    Array(Array<'a>),
    Struct(Struct<'a>),
    Union(Union<'a>),
    Enum(Enum<'a>),
    Proc(&'a Proc),
    MemberFunc(&'a MemberFunc),
    VTableShape(VTableShapeData<'a>),
    Pointer(Pointer<'a>),
    Modifier(TypeModifier),
    FieldList(FieldList<'a>),
    MethodList(MethodListData<'a>),
    ArgList(ArgList<'a>),
    Alias(Alias<'a>),
    UdtSrcLine(&'a UdtSrcLine),
    UdtModSrcLine(&'a UdtModSrcLine),
    FuncId(FuncId<'a>),
    MFuncId(MFuncId<'a>),
    StringId(StringId<'a>),
    SubStrList(SubStrList<'a>),
    BuildInfo(BuildInfo<'a>),
    VFTable(&'a VFTable),
    Unknown,
}

impl<'a> TypeData<'a> {
    /// Parses the payload of a type record.
    pub fn parse_bytes(kind: Leaf, bytes: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(bytes);
        Self::parse(kind, &mut p)
    }

    /// Parses the payload of a type record, using a [`Parser`].
    pub fn parse(kind: Leaf, p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(match kind {
            Leaf::LF_ARRAY => Self::Array(p.parse()?),
            Leaf::LF_CLASS | Leaf::LF_STRUCTURE | Leaf::LF_INTERFACE => Self::Struct(p.parse()?),
            Leaf::LF_UNION => Self::Union(p.parse()?),
            Leaf::LF_ENUM => Self::Enum(p.parse()?),
            Leaf::LF_PROCEDURE => Self::Proc(p.get()?),
            Leaf::LF_MEMBER => Self::MemberFunc(p.get()?),

            Leaf::LF_VTSHAPE => {
                let fixed: &VTableShapeFixed = p.get()?;
                Self::VTableShape(VTableShapeData {
                    count: fixed.count.get(),
                    descriptors: p.take_rest(),
                })
            }

            Leaf::LF_VFTABLE => Self::VFTable(p.get()?),

            Leaf::LF_POINTER => {
                let fixed = p.get()?;
                let variant = p.take_rest();
                Self::Pointer(Pointer { fixed, variant })
            }

            Leaf::LF_MFUNCTION => Self::MemberFunc(p.get()?),
            Leaf::LF_MODIFIER => Self::Modifier(p.copy()?),

            Leaf::LF_FIELDLIST => Self::FieldList(FieldList {
                bytes: p.take_rest(),
            }),

            Leaf::LF_METHODLIST => Self::MethodList(MethodListData {
                bytes: p.take_rest(),
            }),

            Leaf::LF_ARGLIST => Self::ArgList(p.parse()?),
            Leaf::LF_ALIAS => Self::Alias(Alias::from_parser(p)?),
            Leaf::LF_UDT_SRC_LINE => Self::UdtSrcLine(p.get()?),
            Leaf::LF_UDT_MOD_SRC_LINE => Self::UdtModSrcLine(p.get()?),
            Leaf::LF_FUNC_ID => Self::FuncId(p.parse()?),
            Leaf::LF_MFUNC_ID => Self::MFuncId(p.parse()?),
            Leaf::LF_STRING_ID => Self::StringId(p.parse()?),
            Leaf::LF_SUBSTR_LIST => Self::SubStrList(p.parse()?),
            Leaf::LF_BUILDINFO => Self::BuildInfo(p.parse()?),

            _ => Self::Unknown,
        })
    }

    /// If this record has a primary "name" field, return it. Else, return `None`.
    pub fn name(&self) -> Option<&'a BStr> {
        match self {
            // From TPI
            Self::Struct(t) => Some(t.name),
            Self::Union(t) => Some(t.name),
            Self::Enum(t) => Some(t.name),
            Self::Alias(t) => Some(t.name),

            // From IPI
            Self::FuncId(t) => Some(t.name),
            Self::StringId(t) => Some(t.name),

            _ => None,
        }
    }

    /// Returns the name of this type definition, if it is a UDT (user-defined type) definition.
    pub fn udt_name(&self) -> Option<&'a BStr> {
        match self {
            Self::Struct(t) => Some(t.name),
            Self::Union(t) => Some(t.name),
            Self::Enum(t) => Some(t.name),
            Self::Alias(t) => Some(t.name),
            _ => None,
        }
    }
}

```

`pdb/src/types/fields.rs`:

```rs
//! Decodes items in a `LF_FIELDLIST` complex list.

use super::*;
use tracing::error;

/// Represents the data stored within an `LF_FIELDLIST` type string. This can be decoded using
/// the `iter()` method.
#[derive(Clone)]
pub struct FieldList<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a [u8],
}

impl<'a> FieldList<'a> {
    /// Iterates the fields within an `LF_FIELDLIST` type string.
    pub fn iter(&self) -> IterFields<'a> {
        IterFields { bytes: self.bytes }
    }
}

impl<'a> Debug for FieldList<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        if f.alternate() {
            let mut list = f.debug_list();
            for f in self.iter() {
                list.entry(&f);
            }
            list.finish()
        } else {
            f.write_str("FieldList")
        }
    }
}

/// Iterates the fields within an `LF_FIELDLIST` type string.
pub struct IterFields<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a [u8],
}

/// Represents one field within an `LF_FIELDLIST` type string.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub enum Field<'a> {
    BaseClass(BaseClass<'a>),
    DirectVirtualBaseClass(DirectVirtualBaseClass<'a>),
    IndirectVirtualBaseClass(IndirectVirtualBaseClass<'a>),
    Enumerate(Enumerate<'a>),
    FriendFn(FriendFn<'a>),
    Index(TypeIndex),
    Member(Member<'a>),
    StaticMember(StaticMember<'a>),
    Method(Method<'a>),
    NestedType(NestedType<'a>),
    VFuncTable(TypeIndex),
    FriendClass(TypeIndex),
    OneMethod(OneMethod<'a>),
    VFuncOffset(VFuncOffset),
    NestedTypeEx(NestedTypeEx<'a>),
}

#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct NestedType<'a> {
    pub nested_ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for NestedType<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.skip(2)?; // padding
        Ok(Self {
            nested_ty: p.type_index()?,
            name: p.strz()?,
        })
    }
}

#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct BaseClass<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub offset: Number<'a>,
}

impl<'a> Parse<'a> for BaseClass<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let attr = p.u16()?;
        let ty = p.type_index()?;
        let offset = p.number()?;
        Ok(BaseClass { attr, ty, offset })
    }
}

/// This is used by both DirectVirtualBaseClass and IndirectVirtualBaseClass.
#[allow(missing_docs)]
#[repr(C)]
#[derive(Clone, Debug, IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
pub struct VirtualBaseClassFixed {
    pub attr: U16<LE>,
    pub btype: TypeIndexLe,
    pub vbtype: TypeIndexLe,
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct DirectVirtualBaseClass<'a> {
    pub fixed: &'a VirtualBaseClassFixed,
    pub vbpoff: Number<'a>,
    pub vboff: Number<'a>,
}

impl<'a> Parse<'a> for DirectVirtualBaseClass<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            vbpoff: p.number()?,
            vboff: p.number()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct IndirectVirtualBaseClass<'a> {
    pub fixed: &'a VirtualBaseClassFixed,
    pub vbpoff: Number<'a>,
    pub vboff: Number<'a>,
}

impl<'a> Parse<'a> for IndirectVirtualBaseClass<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed = p.get()?;
        let vbpoff = p.number()?;
        let vboff = p.number()?;
        Ok(Self {
            fixed,
            vbpoff,
            vboff,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone)]
pub struct Enumerate<'a> {
    pub attr: u16,
    pub value: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Enumerate<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            attr: p.u16()?,
            value: p.number()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Debug for Enumerate<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{} = {}", self.name, self.value)
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct FriendFn<'a> {
    pub ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for FriendFn<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.skip(2)?; // padding
        Ok(Self {
            ty: p.type_index()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct OneMethod<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub vbaseoff: u32,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for OneMethod<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let attr = p.u16()?;
        let ty = p.type_index()?;
        let vbaseoff = if introduces_virtual(attr) {
            p.u32()?
        } else {
            0
        };
        let name = p.strz()?;
        Ok(OneMethod {
            attr,
            ty,
            vbaseoff,
            name,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct VFuncOffset {
    pub vtable_ty: TypeIndex,
    pub offset: u32,
}

impl<'a> Parse<'a> for VFuncOffset {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.skip(2)?; // padding
        let vtable_ty = p.type_index()?;
        let offset = p.u32()?;
        Ok(Self { vtable_ty, offset })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct NestedTypeEx<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for NestedTypeEx<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let attr = p.u16()?;
        let ty = p.type_index()?;
        let name = p.strz()?;
        Ok(Self { attr, ty, name })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Member<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub offset: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Member<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            attr: p.u16()?,
            ty: p.type_index()?,
            offset: p.number()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct StaticMember<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for StaticMember<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            attr: p.u16()?,
            ty: p.type_index()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Method<'a> {
    pub count: u16,
    pub methods: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Method<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            count: p.u16()?,
            methods: p.type_index()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Iterator for IterFields<'a> {
    type Item = Field<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }
        let mut p = Parser::new(self.bytes);

        let rest = p.peek_rest();

        // Check for padding (alignment) bytes.
        let mut padding_len = 0;
        while padding_len < rest.len() && rest[padding_len] >= 0xf0 {
            padding_len += 1;
        }
        if padding_len > 0 {
            let _ = p.skip(padding_len);
        }

        if p.is_empty() {
            return None;
        }

        match Field::parse(&mut p) {
            Ok(f) => {
                self.bytes = p.into_rest();
                Some(f)
            }
            Err(ParserError) => None,
        }
    }
}

impl<'a> Field<'a> {
    /// Parses one field within an `LF_FIELDLIST` type string.
    ///
    /// Unlike most of the `parse()` methods defined in this library, this function requires a
    /// `Parser` instance, rather than just working directly with `&[u8]`. This is because the
    /// field records do not have a length field; the type of the field is required to know how
    /// many bytes to decode in each field.
    ///
    /// So the act of parsing a field is what is needed for locating the next field.
    pub fn parse(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let item_kind = Leaf(p.u16()?);

        Ok(match item_kind {
            Leaf::LF_BCLASS => Self::BaseClass(p.parse()?),
            Leaf::LF_VBCLASS => Self::DirectVirtualBaseClass(p.parse()?),
            Leaf::LF_IVBCLASS => Self::IndirectVirtualBaseClass(p.parse()?),
            Leaf::LF_ENUMERATE => Self::Enumerate(p.parse()?),
            Leaf::LF_FRIENDFCN => Self::FriendFn(p.parse()?),

            Leaf::LF_INDEX => {
                p.skip(2)?; // padding
                let ty = p.type_index()?;
                Self::Index(ty)
            }

            Leaf::LF_MEMBER => Self::Member(p.parse()?),
            Leaf::LF_STMEMBER => Self::StaticMember(p.parse()?),
            Leaf::LF_METHOD => Self::Method(p.parse()?),
            Leaf::LF_NESTEDTYPE => Self::NestedType(p.parse()?),

            Leaf::LF_VFUNCTAB => {
                p.skip(2)?; // padding
                let vtable_ty = p.type_index()?;
                Self::VFuncTable(vtable_ty)
            }

            Leaf::LF_FRIENDCLS => {
                p.skip(2)?; // padding
                let ty = p.type_index()?; // friend class type
                Self::FriendClass(ty)
            }

            Leaf::LF_ONEMETHOD => Self::OneMethod(p.parse()?),
            Leaf::LF_VFUNCOFF => Self::VFuncOffset(p.parse()?),
            Leaf::LF_NESTEDTYPEEX => Self::NestedTypeEx(p.parse()?),

            unknown_item_kind => {
                error!(?unknown_item_kind, "unrecognized item within LF_FIELDLIST",);
                return Err(ParserError::new());
            }
        })
    }
}

```

`pdb/src/types/iter.rs`:

```rs
//! Code for iterating through type streams

use super::Leaf;
use crate::parser::{Parser, ParserError, ParserMut};
use crate::utils::iter::{HasRestLen, IteratorWithRangesExt};
use std::mem::take;

/// Parses a type record stream and iterates `TypeRecord` values.
#[derive(Clone)]
pub struct TypesIter<'a> {
    buffer: &'a [u8],
}

impl<'a> TypesIter<'a> {
    /// Starts a new iterator.
    pub fn new(buffer: &'a [u8]) -> Self {
        Self { buffer }
    }
}

impl<'a> HasRestLen for TypesIter<'a> {
    fn rest_len(&self) -> usize {
        self.buffer.len()
    }
}

impl<'a> Iterator for TypesIter<'a> {
    type Item = TypeRecord<'a>;

    fn next(&mut self) -> Option<TypeRecord<'a>> {
        if self.buffer.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.buffer);

        let record_len = p.u16().ok()?;
        if record_len < 2 {
            // Type record has length that is too short to be valid
            return None;
        }

        let type_kind = p.u16().ok()?;

        let Ok(record_data) = p.bytes(record_len as usize - 2) else {
            // Type record is too short to be valid.
            return None;
        };

        self.buffer = p.into_rest();

        Some(TypeRecord {
            data: record_data,
            kind: Leaf(type_kind),
        })
    }
}

/// Represents a record that was enumerated within a type record stream (the TPI or IPI).
#[derive(Clone)]
pub struct TypeRecord<'a> {
    /// Indicates how to interpret the payload (the `data` field).
    pub kind: Leaf,
    /// Record data. This does NOT include `kind` and the record data length.
    pub data: &'a [u8],
}

impl<'a> TypeRecord<'a> {
    /// Parses the payload of this type record.
    pub fn parse(&self) -> Result<crate::types::TypeData<'a>, ParserError> {
        crate::types::TypeData::parse(self.kind, &mut Parser::new(self.data))
    }
}

/// Builds a "starts" table that gives the starting location of each type record.
pub fn build_types_starts(num_records_expected: usize, type_records: &[u8]) -> Vec<u32> {
    let mut starts: Vec<u32> = Vec::with_capacity(num_records_expected + 1);
    let mut iter = TypesIter::new(type_records).with_ranges();

    // This loop pushes a byte offset (pos) for the start of every record, plus 1 additional
    // value at the end of the sequence.  This will correctly handle the case where the last
    // record has some undecodable garbage at the end.
    loop {
        let pos = iter.pos();
        starts.push(pos as u32);

        if iter.next().is_none() {
            break;
        }
    }

    starts.shrink_to_fit();
    starts
}

/// Parses a type record stream and iterates `TypeRecord` values.
pub struct TypesIterMut<'a> {
    buffer: &'a mut [u8],
}

impl<'a> TypesIterMut<'a> {
    /// Starts a new iterator.
    pub fn new(buffer: &'a mut [u8]) -> Self {
        Self { buffer }
    }
}

impl<'a> HasRestLen for TypesIterMut<'a> {
    fn rest_len(&self) -> usize {
        self.buffer.len()
    }
}

impl<'a> Iterator for TypesIterMut<'a> {
    type Item = TypeRecordMut<'a>;

    fn next(&mut self) -> Option<TypeRecordMut<'a>> {
        if self.buffer.is_empty() {
            return None;
        }

        let mut parser = ParserMut::new(take(&mut self.buffer));

        let record_len = parser.u16().ok()?;
        if record_len < 2 {
            // Type record has length that is too short to be valid
            return None;
        }

        let type_kind = parser.u16().ok()?;

        let Ok(record_data) = parser.bytes_mut(record_len as usize - 2) else {
            // Type record is too short to be valid.
            return None;
        };

        self.buffer = parser.into_rest();

        Some(TypeRecordMut {
            data: record_data,
            kind: Leaf(type_kind),
        })
    }
}

/// Represents a record that was enumerated within a type record stream (the TPI or IPI).
/// Allows mutable access.
pub struct TypeRecordMut<'a> {
    /// Indicates how to interpret the payload (the `data` field).
    pub kind: Leaf,
    /// Record data. This does NOT include `kind` and the record data length.
    pub data: &'a mut [u8],
}

```

`pdb/src/types/kind.rs`:

```rs
/// Identifies type records. Also called "leaf" records.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct Leaf(pub u16);

macro_rules! cv_leaf {
    (
        $(
            $code:expr, $name:ident ;
        )*
    ) => {
        #[allow(non_upper_case_globals)]
        #[allow(missing_docs)]
        impl Leaf {
            $(
                pub const $name: Leaf = Leaf($code);
            )*
        }

        static LEAF_NAMES: &[(Leaf, &str)] = &[
            $(
                (Leaf($code), stringify!($name)),
            )*
        ];
    }
}

cv_leaf! {
    0x0001, LF_MODIFIER_16t;
    0x0002, LF_POINTER_16t;
    0x0003, LF_ARRAY_16t;
    0x0004, LF_CLASS_16t;
    0x0005, LF_STRUCTURE_16t;
    0x0006, LF_UNION_16t;
    0x0007, LF_ENUM_16t;
    0x0008, LF_PROCEDURE_16t;
    0x0009, LF_MFUNCTION_16t;
    0x000a, LF_VTSHAPE;
    0x000c, LF_COBOL1;
    0x000e, LF_LABEL;
    0x000f, LF_NULL;
    0x0014, LF_ENDPRECOMP;
    0x020c, LF_REFSYM;
    0x040b, LF_FRIENDCLS;   // (in field list) friend class
    0x1001, LF_MODIFIER;
    0x1002, LF_POINTER;
    0x1008, LF_PROCEDURE;
    0x1009, LF_MFUNCTION;
    0x100a, LF_COBOL0;
    0x100b, LF_BARRAY;
    0x100d, LF_VFTPATH;
    0x100f, LF_OEM;
    0x1011, LF_OEM2;
    0x1200, LF_SKIP;
    0x1201, LF_ARGLIST;
    0x1203, LF_FIELDLIST;
    0x1204, LF_DERIVED;
    0x1205, LF_BITFIELD;
    0x1206, LF_METHODLIST;
    0x1207, LF_DIMCONU;
    0x1208, LF_DIMCONLU;
    0x1209, LF_DIMVARU;
    0x120a, LF_DIMVARLU;
    0x1400, LF_BCLASS;      // (in field list) real (non-virtual) base class
    0x1401, LF_VBCLASS;     // (in field list) direct virtual base class
    0x1402, LF_IVBCLASS;    // (in field list) indirect virtual base class
    0x1404, LF_INDEX;       // (in field list) index to another type record
    0x1409, LF_VFUNCTAB;    // (in field list) virtual function table pointer
    0x140c, LF_VFUNCOFF;    // (in field list) virtual function offset
    0x1502, LF_ENUMERATE;   // (in field list) an enumerator value
    0x1503, LF_ARRAY;
    0x1504, LF_CLASS;
    0x1505, LF_STRUCTURE;
    0x1506, LF_UNION;
    0x1507, LF_ENUM;
    0x1508, LF_DIMARRAY;
    0x1509, LF_PRECOMP;
    0x150a, LF_ALIAS;
    0x150b, LF_DEFARG;
    0x150c, LF_FRIENDFCN;   // (in field list) friend function
    0x150d, LF_MEMBER;      // (in field list) data member
    0x150e, LF_STMEMBER;    // (in field list) static data member
    0x150f, LF_METHOD;      // (in field list) method group (overloaded methods), not single method
    0x1510, LF_NESTEDTYPE;  // (in field list) nested type definition
    0x1511, LF_ONEMETHOD;   // (in field list) a single method
    0x1512, LF_NESTEDTYPEEX;// (in field list) nested type extended definition
    0x1514, LF_MANAGED;
    0x1515, LF_TYPESERVER2;
    0x1519, LF_INTERFACE;
    0x151d, LF_VFTABLE;

    // --- end of types ---

    // 0x1601..=0x1607 are only present in IPI stream, not TPI stream.

    0x1601, LF_FUNC_ID;         // global func ID
    0x1602, LF_MFUNC_ID;        // member func ID
    0x1603, LF_BUILDINFO;       // build info: tool, version, command line, src/pdb file
    0x1604, LF_SUBSTR_LIST;     // similar to LF_ARGLIST, for list of sub strings
    0x1605, LF_STRING_ID;       // string ID

    // source and line on where an UDT is defined
    // only generated by compiler
    0x1606, LF_UDT_SRC_LINE;

    // module, source and line on where an UDT is defined
    // only generated by linker
    0x1607, LF_UDT_MOD_SRC_LINE;

    // The following four kinds were added to the wrong place in this enumeration.
    // They should have been added befor LF_TYPE_LAST.
    // But now it has been too late to change this :-(

    0x1608, LF_CLASS2;       // LF_CLASS with 32bit property field
    0x1609, LF_STRUCTURE2;   // LF_STRUCTURE with 32bit property field
    0x160a, LF_UNION2;       // LF_UNION with 32bit property field
    0x160b, LF_INTERFACE2;   // LF_INTERFACE with 32bit property field

    // These values are used for encoding numeric constants.
//    0x8000, LF_NUMERIC;
    0x8000, LF_CHAR;            // i8
    0x8001, LF_SHORT;           // i16
    0x8002, LF_USHORT;          // u16
    0x8003, LF_LONG;            // i32
    0x8004, LF_ULONG;           // u32
    0x8005, LF_REAL32;          // f32
    0x8006, LF_REAL64;          // f64
    0x8007, LF_REAL80;
    0x8008, LF_REAL128;
    0x8009, LF_QUADWORD;        // i64
    0x800a, LF_UQUADWORD;       // u64
    0x800b, LF_REAL48;
    0x800c, LF_COMPLEX32;
    0x800d, LF_COMPLEX64;
    0x800e, LF_COMPLEX80;
    0x800f, LF_COMPLEX128;
    0x8010, LF_VARSTRING;       // string prefixed with u16 length
    0x8017, LF_OCTWORD;         // i128
    0x8018, LF_UOCTWORD;        // u128
    0x8019, LF_DECIMAL;
    0x801a, LF_DATE;            // 8 bytes
    0x801b, LF_UTF8STRING;      // NUL-terminated UTF-8 string
    0x801c, LF_REAL16;
}

impl std::fmt::Debug for Leaf {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        if let Ok(index) = LEAF_NAMES.binary_search_by_key(self, |ii| ii.0) {
            fmt.write_str(LEAF_NAMES[index].1)
        } else {
            let b0 = (self.0 & 0xff) as u8;
            let b1 = (self.0 >> 8) as u8;
            fn to_c(b: u8) -> char {
                if (32..=126).contains(&b) {
                    char::from(b)
                } else {
                    '_'
                }
            }

            write!(fmt, "Leaf(??{:04x} {}{})", self.0, to_c(b0), to_c(b1))
        }
    }
}

impl Leaf {
    /// True if this `Leaf` codes for an immediate numeric constant.
    pub fn is_immediate_numeric(self) -> bool {
        self.0 < 0x8000
    }

    /// Checks whether this `Leaf` can be used as a type record.
    pub fn can_start_record(self) -> bool {
        matches!(
            self,
            Leaf::LF_MODIFIER
                | Leaf::LF_POINTER
                | Leaf::LF_ARRAY
                | Leaf::LF_CLASS
                | Leaf::LF_STRUCTURE
                | Leaf::LF_UNION
                | Leaf::LF_ENUM
                | Leaf::LF_PROCEDURE
                | Leaf::LF_MFUNCTION
                | Leaf::LF_VTSHAPE
                | Leaf::LF_COBOL0
                | Leaf::LF_COBOL1
                | Leaf::LF_BARRAY
                | Leaf::LF_LABEL
                | Leaf::LF_NULL
                | Leaf::LF_DIMARRAY
                | Leaf::LF_VFTPATH
                | Leaf::LF_PRECOMP
                | Leaf::LF_ENDPRECOMP
                | Leaf::LF_OEM
                | Leaf::LF_OEM2
                | Leaf::LF_ALIAS
                | Leaf::LF_MANAGED
                | Leaf::LF_TYPESERVER2
        )
    }

    /// Checks whether this `Leaf` can be used within a field list record.
    pub fn is_nested_leaf(self) -> bool {
        matches!(
            self,
            Leaf::LF_SKIP
                | Leaf::LF_ARGLIST
                | Leaf::LF_DEFARG
                | Leaf::LF_FIELDLIST
                | Leaf::LF_DERIVED
                | Leaf::LF_BITFIELD
                | Leaf::LF_METHODLIST
                | Leaf::LF_DIMCONU
                | Leaf::LF_DIMCONLU
                | Leaf::LF_DIMVARU
                | Leaf::LF_DIMVARLU
                | Leaf::LF_REFSYM
        )
    }

    /// Indicates whether a given type record can contain references to other type records.
    pub fn can_reference_types(self) -> bool {
        matches!(
            self,
            Leaf::LF_MODIFIER
                | Leaf::LF_POINTER
                | Leaf::LF_ARRAY
                | Leaf::LF_CLASS
                | Leaf::LF_UNION
                | Leaf::LF_ENUM
                | Leaf::LF_PROCEDURE
        )
    }
}

```

`pdb/src/types/number.rs`:

```rs
//! Section 4, numeric leaves

use super::Leaf;
use crate::parser::{Parse, Parser, ParserError};
use bstr::BStr;
use pretty_hex::PrettyHex;
use std::fmt::{Debug, Display};
use std::num::TryFromIntError;
use tracing::warn;

/// A numeric constant defined within a CodeView type or symbol record.
///
/// # References
/// * "Numeric Leaves" section of PDB specification.
#[derive(Copy, Clone)]
#[repr(transparent)]
pub struct Number<'a> {
    bytes: &'a [u8],
}

impl<'a> Number<'a> {
    /// Gets the raw bytes of this `Number`.
    pub fn as_bytes(&self) -> &'a [u8] {
        self.bytes
    }

    /// Gets the kind (representation) of this value.
    /// If this is an immediate value (integer in `0..=0x7fff`), gets the actual value.
    pub fn kind(&self) -> Leaf {
        let mut p = Parser::new(self.bytes);
        Leaf(p.u16().unwrap())
    }
}

impl<'a> Parse<'a> for Number<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let start = p.peek_rest();

        let more_len = match Leaf(p.u16()?) {
            lf if lf.is_immediate_numeric() => 0,
            Leaf::LF_CHAR => 1,
            Leaf::LF_SHORT => 2,
            Leaf::LF_USHORT => 2,
            Leaf::LF_LONG => 4,
            Leaf::LF_ULONG => 4,
            Leaf::LF_REAL32 => 4,
            Leaf::LF_REAL64 => 8,
            Leaf::LF_REAL80 => 10,
            Leaf::LF_REAL128 => 16,
            Leaf::LF_QUADWORD => 8,
            Leaf::LF_UQUADWORD => 8,
            Leaf::LF_REAL48 => 6,
            Leaf::LF_COMPLEX32 => 8,
            Leaf::LF_COMPLEX64 => 16,
            Leaf::LF_COMPLEX80 => 20,
            Leaf::LF_COMPLEX128 => 32,
            Leaf::LF_VARSTRING => p.u16()? as usize,
            Leaf::LF_OCTWORD => 16,
            Leaf::LF_UOCTWORD => 16,
            Leaf::LF_DECIMAL => 16,
            Leaf::LF_DATE => 8,
            Leaf::LF_UTF8STRING => {
                p.skip_strz()?;
                0
            }
            Leaf::LF_REAL16 => 2,
            lf => {
                warn!(leaf = ?lf, "unrecognized numeric leaf");
                // We don't know how many bytes to consume, so we can't keep parsing.
                return Err(ParserError::new());
            }
        };

        p.skip(more_len)?;
        Ok(Self {
            bytes: &start[..start.len() - p.len()],
        })
    }
}

impl<'a> Number<'a> {}

macro_rules! try_from_number {
    (
        $t:ty
    ) => {
        impl<'a> TryFrom<Number<'a>> for $t {
            type Error = TryFromIntError;

            #[inline(never)]
            fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
                use map_parser_error_to_int_error as e;

                let mut p = Parser::new(value.bytes);
                Ok(match Leaf(e(p.u16())?) {
                    lf if lf.is_immediate_numeric() => Self::try_from(lf.0)?,
                    Leaf::LF_USHORT => Self::try_from(e(p.u16())?)?,
                    Leaf::LF_ULONG => Self::try_from(e(p.u32())?)?,
                    Leaf::LF_UQUADWORD => Self::try_from(e(p.u64())?)?,
                    Leaf::LF_CHAR => Self::try_from(e(p.i8())?)?,
                    Leaf::LF_SHORT => Self::try_from(e(p.i16())?)?,
                    Leaf::LF_LONG => Self::try_from(e(p.i32())?)?,
                    Leaf::LF_QUADWORD => Self::try_from(e(p.i64())?)?,
                    Leaf::LF_OCTWORD => Self::try_from(e(p.i128())?)?,
                    Leaf::LF_UOCTWORD => Self::try_from(e(p.u128())?)?,
                    _ => return Err(try_from_int_error()),
                })
            }
        }
    };
}

try_from_number!(i8);
try_from_number!(i16);
try_from_number!(i32);
try_from_number!(i64);
try_from_number!(i128);

try_from_number!(u8);
try_from_number!(u16);
try_from_number!(u32);
try_from_number!(u64);
try_from_number!(u128);

fn map_parser_error_to_int_error<T>(r: Result<T, ParserError>) -> Result<T, TryFromIntError> {
    match r {
        Ok(x) => Ok(x),
        Err(ParserError) => Err(try_from_int_error()),
    }
}

/// Error type for conversions from `Number` to `f32`
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub struct TryFromFloatError;

impl From<ParserError> for TryFromFloatError {
    fn from(_: ParserError) -> Self {
        Self
    }
}

impl<'a> TryFrom<Number<'a>> for f32 {
    type Error = TryFromFloatError;

    fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
        let mut p = Parser::new(value.bytes);
        Ok(match Leaf(p.u16()?) {
            Leaf::LF_REAL32 => f32::from_le_bytes(p.array()?),
            _ => return Err(TryFromFloatError),
        })
    }
}

impl<'a> TryFrom<Number<'a>> for f64 {
    type Error = TryFromFloatError;

    fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
        let mut p = Parser::new(value.bytes);
        Ok(match Leaf(p.u16()?) {
            Leaf::LF_REAL32 => f32::from_le_bytes(p.array::<4>()?) as f64,
            Leaf::LF_REAL64 => f64::from_le_bytes(p.array::<8>()?),
            _ => return Err(TryFromFloatError),
        })
    }
}

/// Error type for conversions from `Number` to string
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub struct TryFromStrError;

impl From<ParserError> for TryFromStrError {
    fn from(_: ParserError) -> Self {
        Self
    }
}
impl<'a> TryFrom<Number<'a>> for &'a BStr {
    type Error = TryFromStrError;

    fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
        let mut p = Parser::new(value.bytes);
        Ok(match Leaf(p.u16()?) {
            Leaf::LF_UTF8STRING => p.strz()?,
            Leaf::LF_VARSTRING => {
                let len = p.u16()?;
                let bytes = p.bytes(len as usize)?;
                BStr::new(bytes)
            }
            _ => return Err(TryFromStrError),
        })
    }
}

impl<'a> Debug for Number<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        Display::fmt(self, f)
    }
}

impl<'a> Display for Number<'a> {
    #[inline(never)]
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        fn e<T>(
            f: &mut std::fmt::Formatter<'_>,
            r: Result<T, ParserError>,
        ) -> Result<T, std::fmt::Error> {
            match r {
                Ok(x) => Ok(x),
                Err(ParserError) => {
                    f.write_str("??(parser error)")?;
                    Err(std::fmt::Error)
                }
            }
        }

        let mut p = Parser::new(self.bytes);

        match Leaf(p.u16().unwrap()) {
            lf if lf.is_immediate_numeric() => Display::fmt(&lf.0, f),
            Leaf::LF_CHAR => Display::fmt(&e(f, p.i8())?, f),
            Leaf::LF_SHORT => Display::fmt(&e(f, p.i16())?, f),
            Leaf::LF_USHORT => Display::fmt(&e(f, p.u16())?, f),
            Leaf::LF_LONG => Display::fmt(&e(f, p.i32())?, f),
            Leaf::LF_ULONG => Display::fmt(&e(f, p.u32())?, f),
            Leaf::LF_REAL32 => Display::fmt(&e(f, p.f32())?, f),
            Leaf::LF_REAL64 => Display::fmt(&e(f, p.f64())?, f),
            Leaf::LF_QUADWORD => Display::fmt(&e(f, p.i64())?, f),
            Leaf::LF_UQUADWORD => Display::fmt(&e(f, p.u64())?, f),
            Leaf::LF_VARSTRING => {
                // This uses a 2-byte length prefix, not 1-byte.
                let len = p.u16().unwrap();
                let s = BStr::new(p.bytes(len as usize).unwrap());
                <BStr as Display>::fmt(s, f)
            }
            Leaf::LF_OCTWORD => Display::fmt(&e(f, p.i128())?, f),
            Leaf::LF_UOCTWORD => Display::fmt(&e(f, p.u128())?, f),
            Leaf::LF_UTF8STRING => {
                let s = p.strz().unwrap();
                <BStr as Display>::fmt(s, f)
            }

            lf => {
                write!(f, "?? {lf:?} {:?}", self.bytes.hex_dump())
            }
        }
    }
}

fn try_from_int_error() -> TryFromIntError {
    u32::try_from(-1i8).unwrap_err()
}

#[cfg(test)]
fn parse_number(bytes: &[u8]) -> Number {
    let mut p = Parser::new(bytes);
    let n = p.number().unwrap();
    assert!(p.is_empty());
    n
}

#[test]
fn number_error() {
    assert!(Number::parse(&[]).is_err()); // too short
    assert!(Number::parse(&[0]).is_err()); // also too short
    assert!(Number::parse(&[0xff, 0xff]).is_err()); // unrecognized kind
}

#[test]
fn number_immediate() {
    // Values below 0x8000 are literal uint16 constants.
    let n = parse_number(&[0xaa, 0x70]);
    assert_eq!(n.as_bytes(), &[0xaa, 0x70]);
    assert_eq!(u32::try_from(n).unwrap(), 0x70aa);
}

#[test]
fn number_char() {
    // LF_CHAR
    let n = parse_number(&[0x00, 0x80, (-33i8) as u8]);
    assert_eq!(i32::try_from(n).unwrap(), -33);

    assert!(f32::try_from(n).is_err());
    assert!(f64::try_from(n).is_err());
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_short() {
    // LF_SHORT
    let n = parse_number(&[0x01, 0x80, 0xaa, 0x55]);
    assert_eq!(i32::try_from(n).unwrap(), 0x55aa_i32);
    assert_eq!(u32::try_from(n).unwrap(), 0x55aa_u32);

    let n = parse_number(&[0x01, 0x80, 0x55, 0xaa]);
    assert_eq!(i32::try_from(n).unwrap(), -21931_i32);
    assert!(u32::try_from(n).is_err());

    assert!(f32::try_from(n).is_err());
    assert!(f64::try_from(n).is_err());
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_long() {
    // LF_LONG
    let n = parse_number(&[0x03, 0x80, 1, 2, 3, 4]);
    assert_eq!(u32::try_from(n).unwrap(), 0x04030201_u32);
    assert_eq!(i32::try_from(n).unwrap(), 0x04030201_i32);
    assert!(u16::try_from(n).is_err());
    assert!(i16::try_from(n).is_err());
    assert!(u8::try_from(n).is_err());
    assert!(i8::try_from(n).is_err());

    // unsigned cannot decode negative numbers
    let n = parse_number(&[0x03, 0x80, 0xfe, 0xff, 0xff, 0xff]);
    assert!(u8::try_from(n).is_err());
    assert!(u16::try_from(n).is_err());
    assert!(u32::try_from(n).is_err());
    assert!(u64::try_from(n).is_err());
    assert!(u128::try_from(n).is_err());
    assert_eq!(i8::try_from(n).unwrap(), -2);
    assert_eq!(i16::try_from(n).unwrap(), -2);
    assert_eq!(i32::try_from(n).unwrap(), -2);
    assert_eq!(i64::try_from(n).unwrap(), -2);
    assert_eq!(i128::try_from(n).unwrap(), -2);

    assert!(f32::try_from(n).is_err());
    assert!(f64::try_from(n).is_err());
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_real32() {
    use std::f32::consts::PI;

    let b: [u8; 4] = PI.to_le_bytes();
    assert_eq!(b, [0xdb, 0x0f, 0x49, 0x40]); // 0x400490fdb, pi in f32
    println!("f32 PI bytes: {:#x?}", b);

    // 8005 is LF_REAL32
    let n = parse_number(&[0x05, 0x80, 0xdb, 0x0f, 0x49, 0x40]);

    // LF_REAL32 is not convertible to any of the integer types
    assert!(u8::try_from(n).is_err());
    assert!(u16::try_from(n).is_err());
    assert!(u32::try_from(n).is_err());
    assert!(u64::try_from(n).is_err());
    assert!(u128::try_from(n).is_err());

    assert!(i8::try_from(n).is_err());
    assert!(i16::try_from(n).is_err());
    assert!(i32::try_from(n).is_err());
    assert!(i64::try_from(n).is_err());
    assert!(i128::try_from(n).is_err());

    // Floating-point exact equality can be weird.
    assert_eq!(f32::try_from(n).unwrap(), PI);

    // We convert to f64 but do not verify the value, because again, floating-point is weird.
    let _ = f64::try_from(n).unwrap();
}

#[test]
fn number_real64() {
    use std::f64::consts::PI;

    let b: [u8; 8] = PI.to_le_bytes();
    assert_eq!(b, [0x18, 0x2d, 0x44, 0x54, 0xfb, 0x21, 0x9, 0x40]);
    // assert_eq!(b, [0xdb, 0x0f, 0x49, 0x40]); // 0x400921fb54442d18, pi in f64
    println!("f64 PI bytes: {:#x?}", b);

    // 8006 is LF_REAL64
    let n = parse_number(&[0x06, 0x80, 0x18, 0x2d, 0x44, 0x54, 0xfb, 0x21, 0x9, 0x40]);

    // LF_REAL64 is not convertible to any of the integer types
    assert!(u8::try_from(n).is_err());
    assert!(u16::try_from(n).is_err());
    assert!(u32::try_from(n).is_err());
    assert!(u64::try_from(n).is_err());
    assert!(u128::try_from(n).is_err());

    assert!(i8::try_from(n).is_err());
    assert!(i16::try_from(n).is_err());
    assert!(i32::try_from(n).is_err());
    assert!(i64::try_from(n).is_err());
    assert!(i128::try_from(n).is_err());

    // Floating-point exact equality can be weird.
    assert_eq!(f64::try_from(n).unwrap(), PI);
}

#[test]
fn number_strz() {
    let n = parse_number(b"\x1b\x80Hello, world\0");
    assert_eq!(n.kind(), Leaf::LF_UTF8STRING);
    assert_eq!(<&BStr>::try_from(n).unwrap(), "Hello, world");

    let n = parse_number(&[0x00, 0x80, (-33i8) as u8]);
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_varstring() {
    let s = parse_number(b"\x10\x80\x0c\x00Hello, world");
    assert_eq!(s.kind(), Leaf::LF_VARSTRING);
    assert_eq!(<&BStr>::try_from(s).unwrap(), "Hello, world");
}

#[test]
fn number_unsupported_types() {
    // We can test decoding the prefix for these types, even if we can't currently display them
    // or convert them to something useful.
    let cases: &[(Leaf, usize)] = &[
        (Leaf::LF_REAL80, 10),
        (Leaf::LF_REAL128, 16),
        (Leaf::LF_REAL48, 6),
        (Leaf::LF_COMPLEX32, 8),
        (Leaf::LF_COMPLEX64, 16),
        (Leaf::LF_COMPLEX80, 20),
        (Leaf::LF_COMPLEX128, 32),
        (Leaf::LF_DECIMAL, 16),
        (Leaf::LF_DATE, 8),
        (Leaf::LF_REAL16, 2),
    ];

    for &(kind, num_zeroes) in cases.iter() {
        let mut input = vec![0; 2 + num_zeroes];
        input[0] = kind.0 as u8;
        input[1] = (kind.0 >> 8) as u8;
        let n = parse_number(&input);
        assert_eq!(kind, n.kind());
    }
}

#[test]
fn display() {
    let cases: &[(&[u8], &str)] = &[
        (&[0x01, 0x04], "immediate 1025"),
        (&[0x00, 0x80, 0xff], "LF_CHAR -1"),
        (&[0x01, 0x80, 0xfe, 0xff], "LF_SHORT -2"),
        (&[0x02, 0x80, 0xfd, 0xff], "LF_USHORT 65533"),
        (&[0x03, 0x80, 0xfc, 0xff, 0xff, 0xff], "LF_LONG -4"),
        (&[0x04, 0x80, 0x00, 0x00, 0x02, 0x00], "LF_ULONG 131072"),
        (&[0x05, 0x80, 0xdb, 0x0f, 0x49, 0x40], "LF_REAL32 3.1415927"),
        (
            &[0x06, 0x80, 0x18, 0x2d, 0x44, 0x54, 0xfb, 0x21, 0x9, 0x40],
            "LF_REAL64 3.141592653589793",
        ),
        (
            &[0x09, 0x80, 0xfb, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff],
            "LF_QUADWORD -5",
        ),
        (
            &[0x0a, 0x80, 0x00, 0xe4, 0x0b, 0x54, 0x02, 0x00, 0x00, 0x00],
            "LF_UQUADWORD 10000000000",
        ),
        (
            &[
                0x17, 0x80, 0xfa, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
                0xff, 0xff, 0xff, 0xff,
            ],
            "LF_OCTWORD -6",
        ),
        (
            &[
                0x18, 0x80, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
                0x00, 0x00, 0x00, 0x00,
            ],
            "LF_UOCTWORD 1",
        ),
    ];

    for &(input, expected_output) in cases.iter() {
        let mut p = Parser::new(input);
        let leaf = Leaf(p.u16().unwrap());

        let n = parse_number(input);

        let actual_output = if leaf.is_immediate_numeric() {
            format!("immediate {n}")
        } else {
            format!("{leaf:?} {n}")
        };

        assert_eq!(actual_output, expected_output, "bytes: {:#x?}", input);

        // Cover Debug::fmt. It just trivially defers to Display.
        let _ = format!("{:?}", n);
    }
}

#[test]
fn display_bogus() {
    // Because the byte slice within Number is private and Number::parse() does not construct
    // a Number for kinds it does not recognize, it is impossible (outside of this module)
    // to construct a Number over an invalid, non-immediate Leaf value.  But the Display code
    // has to have a case for that, so we construct a bogus Number just so we can display it.
    let bogus_num = Number {
        bytes: &[0xff, 0xff, 0xaa, 0xaa, 0xaa],
    };
    println!("bogus_num = {bogus_num}");
}

```

`pdb/src/types/primitive.rs`:

```rs
//! Primitive types

use super::TypeIndex;

#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_SPECIAL: u32 = 0;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_SIGNED_INT: u32 = 1;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_UNSIGNED_INT: u32 = 2;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_BOOL: u32 = 3;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_REAL: u32 = 4;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_COMPLEX: u32 = 5;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_SPECIAL2: u32 = 6;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_REALLY_INT: u32 = 7;

macro_rules! primitives {
    (
        $(
            (
                $value:expr,
                $name:ident,
                $description:expr
            ),
        )*
    ) => {
        /// Contains the names and descriptions of all primitive types
        pub static PRIMITIVES: &[(u32, &str, &str)] = &[
            $(
                ($value, stringify!($name), $description),
            )*
        ];

        impl TypeIndex {
            $(
                #[doc = concat!("Primitive type: `", $description, "`")]
                pub const $name: TypeIndex = TypeIndex($value);
            )*
        }
    }
}

primitives! {
    // number, spec name, C/C++ name
    (0x0000, T_NOTYPE, "none"),
    (0x0003, T_VOID, "void"),
    (0x0008, T_HRESULT, "HRESULT"),
    (0x0007, T_NOTTRANS, "<type-not-translated>"),
    (0x0010, T_CHAR, "char"),
    (0x0011, T_SHORT, "short"),
    (0x0012, T_LONG, "long"),
    (0x0013, T_QUAD, "long long"),
    (0x0014, T_OCT, "__int128"),
    (0x0020, T_UCHAR, "unsigned char"),
    (0x0021, T_USHORT, "unsigned short"),
    (0x0022, T_ULONG, "unsigned long"),
    (0x0023, T_UQUAD, "unsigned long long"),
    (0x0024, T_UOCT, "unsigned __int128"),
    (0x0030, T_BOOL8, "bool"),
    (0x0031, T_BOOL16, "bool16"),
    (0x0032, T_BOOL32, "bool32"),
    (0x0033, T_BOOL64, "bool64"),
    (0x0040, T_REAL32, "float"),
    (0x0041, T_REAL64, "double"),
    (0x0068, T_INT1, "__int8"),
    (0x0066, T_UINT1, "unsigned __int8"),
    (0x0070, T_RCHAR, "char"), // really a character. This is "char", which is distinct from "signed char" and "unsigned char"
    (0x0071, T_WCHAR, "wchar_t"),
    (0x0072, T_INT2, "__int16"),
    (0x0073, T_UINT2, "unsigned __int16"),
    (0x0074, T_INT4, "__int32"), // really 32-bit
    (0x0075, T_UINT4, "unsigned __int32"),
    (0x0076, T_INT8, "__int64"),
    (0x0077, T_UINT8, "unsigned __int64"),
    (0x007a, T_CHAR16, "char16"),
    (0x007b, T_CHAR32, "char32"),
    // 32-bit pointer types
    (0x0403, T_32PVOID, "void *"),
    (0x0408, T_32PHRESULT, "HRESULT *"),
    (0x0410, T_32PCHAR, "char *"),
    (0x0411, T_32PSHORT, "short *"),
    (0x0412, T_32PLONG, "long *"),
    (0x0413, T_32PQUAD, "long long *"),
    (0x0414, T_32POCT, "__int128 *"),
    (0x0420, T_32PUCHAR, "unsigned char *"),
    (0x0421, T_32PUSHORT, "unsigned short *"),
    (0x0422, T_32PULONG, "unsigned __int32 *"),
    (0x0423, T_32PUQUAD, "long long *"),
    (0x0424, T_32PUOCT, "unsigned __int128 *"),
    (0x0430, T_32PBOOL08, "bool *"),
    (0x0431, T_32PBOOL16, "bool16 *"),
    (0x0432, T_32PBOOL32, "bool32 *"),
    (0x0433, T_32PBOOL64, "bool64 *"),
    (0x0440, T_32PREAL32, "float *"),
    (0x0441, T_32PREAL64, "double *"),
    (0x0466, T_32PUINT1, "unsigned __int8 *"),
    (0x0468, T_32PINT1, "__int8 *"),
    (0x0470, T_32PRCHAR, "char *"), // really a character
    (0x0471, T_32PWCHAR, "wchar_t *"),
    (0x0472, T_32PINT2, "__int16 *"),
    (0x0473, T_32PUINT2, "unsigned __int16 *"),
    (0x0474, T_32PINT4, "__int32 *"),
    (0x0475, T_32PUINT4, "unsigned __int32 *"),
    (0x0476, T_32PINT8, "__int64 *"),
    (0x0477, T_32PUINT8, "unsigned __int64 *"),
    (0x047a, T_32PCHAR16, "char16 *"),
    (0x047b, T_32PCHAR32, "char32 *"),
    // 64-bit pointer types
    (0x0603, T_64PVOID, "void *"),
    (0x0608, T_64PHRESULT, "HRESULT *"),
    (0x0610, T_64PCHAR, "char *"),
    (0x0611, T_64PSHORT, "short *"),
    (0x0612, T_64PLONG, "long *"),
    (0x0613, T_64PQUAD, "long long *"),
    (0x0614, T_64POCT, "__int128 *"),
    (0x0620, T_64PPUCHAR, "unsigned char *"),
    (0x0621, T_64PUSHORT, "unsigned short *"),
    (0x0622, T_64PULONG, "unsigned __int32 *"),
    (0x0623, T_64PUQUAD, "long long *"),
    (0x0624, T_64PUOCT, "unsigned __int128 *"),
    (0x0630, T_64PBOOL08, "bool *"),
    (0x0631, T_64PBOOL16, "bool16 *"),
    (0x0632, T_64PBOOL32, "bool32 *"),
    (0x0633, T_64PBOOL64, "bool64 *"),
    (0x0640, T_64PREAL32, "float *"),
    (0x0641, T_64PREAL64, "double *"),
    (0x0666, T_64PUINT1, "unsigned __int8 *"),
    (0x0668, T_64PINT1, "__int8 *"),
    (0x0670, T_64PRCHAR, "char *"), // really a character
    (0x0671, T_64PWCHAR, "wchar_t *"),
    (0x0672, T_64PINT2, "__int16 *"),
    (0x0673, T_64PUINT2, "unsigned __int16 *"),
    (0x0674, T_64PINT4, "__int32 *"),
    (0x0675, T_64PUINT4, "unsigned __int32 *"),
    (0x0676, T_64PINT8, "__int64 *"),
    (0x0677, T_64PUINT8, "unsigned __int64 *"),
    (0x067a, T_64PCHAR16, "char16 *"),
    (0x067b, T_64PCHAR32, "char32 *"),
}

/// Dumps a `TypeIndex`. For use only with primitive types.
pub fn dump_primitive_type_index(
    out: &mut dyn std::fmt::Write,
    type_index: TypeIndex,
) -> std::fmt::Result {
    let mode = (type_index.0 >> 8) & 7;
    let prim_ty = (type_index.0 >> 4) & 0xf;
    let size = type_index.0 & 7;

    if let Ok(i) = PRIMITIVES.binary_search_by_key(&type_index.0, |entry| entry.0) {
        let s = PRIMITIVES[i].1;
        write!(out, "{}", s)?;
    } else {
        write!(out, "??PRIM(0x{:04x}) {{ ty: ", type_index.0)?;

        'a: {
            let ty_str = match prim_ty {
                PRIMITIVE_TYPE_SPECIAL => "special",
                PRIMITIVE_TYPE_SIGNED_INT => "signed_integer",
                PRIMITIVE_TYPE_UNSIGNED_INT => "unsigned_integer",
                PRIMITIVE_TYPE_BOOL => "bool",
                PRIMITIVE_TYPE_REAL => "real",
                PRIMITIVE_TYPE_COMPLEX => "complex",
                PRIMITIVE_TYPE_SPECIAL2 => "special2",
                PRIMITIVE_TYPE_REALLY_INT => "really_integer",
                _ => {
                    write!(out, "??{prim_ty}")?;
                    break 'a;
                }
            };
            write!(out, "{}", ty_str)?;
        }

        write!(out, ", mode: {mode}, size: {size} }}")?;
    }

    Ok(())
}

#[test]
fn test_dump() {
    let mut s = String::new();
    dump_primitive_type_index(&mut s, TypeIndex::T_REAL32).unwrap();
    assert_eq!(s, "T_REAL32");

    s.clear();
    dump_primitive_type_index(&mut s, TypeIndex(0x067c)).unwrap();
}

```

`pdb/src/types/records.rs`:

```rs
#![allow(missing_docs)]

use super::*;
use crate::names::NameIndexLe;
use bstr::BStr;
use zerocopy::U64;

bitfield::bitfield! {
    /// Bit field structure describing class/struct/union/enum properties
    ///
    /// See `CV_prop_t` in `cvinfo.h`.
    pub struct UdtProperties(u16);
    impl Debug;

    pub packed,        set_packed:        0;      // true if structure is packed
    pub ctor,          set_ctor:          1;      // true if constructors or destructors present
    pub ovlops,        set_ovlops:        2;      // true if overloaded operators present
    pub isnested,      set_isnested:      3;      // true if this is a nested class
    pub cnested,       set_cnested:       4;      // true if this class contains nested types
    pub opassign,      set_opassign:      5;      // true if overloaded assignment (=)
    pub opcast,        set_opcast:        6;      // true if casting methods
    pub fwdref,        set_fwdref:        7;      // true if forward reference (incomplete defn)
    pub scoped,        set_scoped:        8;      // scoped definition
    pub hasuniquename, set_hasuniquename: 9;      // true if there is a decorated name following the regular name
    pub sealed,        set_sealed:        10;     // true if class cannot be used as a base class
    pub hfa,           set_hfa:           11, 12; // CV_HFA
    pub intrinsic,     set_intrinsic:     13;     // true if class is an intrinsic type (e.g. __m128d)
    pub mocom,         set_mocom:         14;     // CV_MOCOM_UDT
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned)]
#[repr(transparent)]
pub struct UdtPropertiesLe(pub U16<LE>);

impl Debug for UdtPropertiesLe {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        Debug::fmt(&self.get(), fmt)
    }
}

impl UdtPropertiesLe {
    #[inline(always)]
    pub fn get(&self) -> UdtProperties {
        UdtProperties(self.0.get())
    }
}

#[derive(Clone, Debug)]
pub struct Enum<'a> {
    pub fixed: &'a EnumFixed,
    pub name: &'a BStr,
    pub unique_name: Option<&'a BStr>,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct EnumFixed {
    pub count: U16<LE>,
    pub property: UdtPropertiesLe,
    pub underlying_type: TypeIndexLe,
    pub fields: TypeIndexLe,
}

impl<'a> Parse<'a> for Enum<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed: &EnumFixed = p.get()?;
        let name = p.strz()?;
        let property = fixed.property.get();
        let unique_name = if property.hasuniquename() {
            Some(p.strz()?)
        } else {
            None
        };
        Ok(Self {
            fixed,
            name,
            unique_name,
        })
    }
}

/// For `LF_ARRAY`
#[derive(Clone, Debug)]
pub struct Array<'a> {
    pub fixed: &'a ArrayFixed,
    pub len: Number<'a>,
    pub name: &'a BStr,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct ArrayFixed {
    pub element_type: TypeIndexLe,
    pub index_type: TypeIndexLe,
}

impl<'a> Parse<'a> for Array<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Array {
            fixed: p.get()?,
            len: p.number()?,
            name: p.strz()?,
        })
    }
}

/// For `LF_CLASS`, `LF_STRUCTURE`, and `LF_INTERFACE`.
#[derive(Clone, Debug)]
pub struct Struct<'a> {
    pub fixed: &'a StructFixed,
    pub length: Number<'a>,
    pub name: &'a BStr,
    pub unique_name: Option<&'a BStr>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct StructFixed {
    /// Number of elements in the class or structure. This count includes direct, virtual, and
    /// indirect virtual bases, and methods including overloads, data members, static data members,
    /// friends, etc.
    pub num_elements: U16<LE>,

    /// Bit flags
    pub property: UdtPropertiesLe,

    pub field_list: TypeIndexLe,

    // Docs say this should always be zero.
    pub derivation_list: TypeIndexLe,

    pub vtable_shape: TypeIndexLe,
    // numeric leaf
}

impl<'a> Parse<'a> for Struct<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed: &StructFixed = p.get()?;
        let length = p.number()?;
        let name = p.strz()?;
        let unique_name = if fixed.property.get().hasuniquename() {
            Some(p.strz()?)
        } else {
            None
        };
        Ok(Struct {
            fixed,
            length,
            name,
            unique_name,
        })
    }
}

#[derive(Clone, Debug)]
pub struct Union<'a> {
    pub fixed: &'a UnionFixed,
    pub length: Number<'a>,
    pub name: &'a BStr,
    pub unique_name: Option<&'a BStr>,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct UnionFixed {
    pub count: U16<LE>,
    pub property: UdtPropertiesLe,
    pub fields: TypeIndexLe,
}

impl<'a> Parse<'a> for Union<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed: &UnionFixed = p.get()?;
        let length = p.number()?;
        let name = p.strz()?;
        let unique_name = if fixed.property.get().hasuniquename() {
            Some(p.strz()?)
        } else {
            None
        };
        Ok(Union {
            fixed,
            length,
            name,
            unique_name,
        })
    }
}

/// Type modifier record (`LF_MODIFIER`)
///
/// This record defines a qualified variation of another type. Bits indicate whether the qualifier
/// uses `const`, `volatile`, `unaligned`, or a combination of these flags.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct TypeModifier {
    pub underlying_type: TypeIndexLe,
    pub attributes: U16<LE>,
}

impl<'a> Parse<'a> for TypeModifier {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.copy()
    }
}

impl TypeModifier {
    pub fn attributes(&self) -> TypeModifierBits {
        TypeModifierBits(self.attributes.get())
    }

    pub fn is_const(&self) -> bool {
        self.attributes().is_const()
    }

    pub fn is_volatile(&self) -> bool {
        self.attributes().is_volatile()
    }

    pub fn is_unaligned(&self) -> bool {
        self.attributes().is_unaligned()
    }
}

bitfield! {
    #[repr(transparent)]
    #[derive(Clone)]
    pub struct TypeModifierBits(u16);
    impl Debug;

    pub is_const, set_is_const: 0;
    pub is_volatile, set_is_volatile: 1;
    pub is_unaligned, set_is_unaligned: 2;
    pub reserved, set_reserved: 3, 15;
}

/// `LF_PROCEDURE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct Proc {
    pub return_value: TypeIndexLe,
    pub call: u8,
    pub reserved: u8,
    pub num_params: U16<LE>,
    pub arg_list: TypeIndexLe,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct MemberFunc {
    pub return_value: TypeIndexLe,
    pub class: TypeIndexLe,
    pub this: TypeIndexLe,
    pub call: u8,
    pub reserved: u8,
    pub num_params: U16<LE>,
    pub arg_list: TypeIndexLe,
    pub this_adjust: U32<LE>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned)]
pub struct VTableShapeFixed {
    pub count: U16<LE>,
}

#[derive(Clone, Debug)]
pub struct VTableShapeData<'a> {
    pub count: u16,
    pub descriptors: &'a [u8],
}

pub struct MethodList<'a> {
    pub rest: &'a [u8],
}

impl<'a> MethodList<'a> {
    pub fn parse(record_data: &'a [u8]) -> Result<Self, ParserError> {
        Ok(Self { rest: record_data })
    }

    #[allow(clippy::should_implement_trait)]
    pub fn next(&mut self) -> Result<Option<MethodListItem>, ParserError> {
        if self.rest.is_empty() {
            return Ok(None);
        }

        let mut p = Parser::new(self.rest);
        let attr = p.u16()?;
        p.u16()?; // discard padding
        let ty = p.type_index()?;
        let vtab_offset = if introduces_virtual(attr) {
            Some(p.u32()?)
        } else {
            None
        };

        self.rest = p.into_rest();

        Ok(Some(MethodListItem {
            attr,
            ty,
            vtab_offset,
        }))
    }
}

/// Indicates whether a method type introduces a new virtual function slot.
///
/// `attr` is the `attr` field of a `LF_ONEMETHOD`, etc. record.
pub fn introduces_virtual(attr: u16) -> bool {
    // This field is only present if this method introduces a new vtable slot.
    matches!((attr >> 2) & 0xf, 4 | 6)
}

pub struct MethodListItem {
    pub attr: u16,
    pub ty: TypeIndex,
    pub vtab_offset: Option<u32>,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct PointerFixed {
    pub ty: TypeIndexLe,
    pub attr: U32<LE>,
}

impl PointerFixed {
    pub fn attr(&self) -> PointerFlags {
        PointerFlags::from_bits(self.attr.get())
    }
}

#[derive(Clone)]
pub struct Pointer<'a> {
    pub fixed: &'a PointerFixed,
    pub variant: &'a [u8],
}

impl<'a> Parse<'a> for Pointer<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed = p.get()?;
        let variant = p.take_rest();
        Ok(Self { fixed, variant })
    }
}

impl<'a> Debug for Pointer<'a> {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        let attr = self.fixed.attr();
        write!(fmt, "ty: {:?}", self.fixed.ty.get())?;
        write!(fmt, " attr: 0x{:08x} {:?}", attr.0, attr)?;
        write!(fmt, " mode: {}", attr.mode())?;
        Ok(())
    }
}

bitfield::bitfield! {
    pub struct PointerFlags(u32);
    impl Debug;
    pub pointer_kind, set_pointer_kind: 4, 0;
    pub mode, set_mode: 7, 5;
    pub flat32, set_flat32: 8;
    pub volatile, set_volatile: 9;
    pub r#const, set_const: 10;
    pub unaligned, set_unaligned: 11;
    pub restrict, set_restrict: 12;
    pub size, set_size: 13, 18;
    pub ismocom, set_ismocom: 19;
    pub islref, set_islref: 20;
    pub isrref, set_isrref: 21;
    pub unused, set_unused: 31, 22;
}

impl PointerFlags {
    #[allow(missing_docs)]
    pub fn from_bits(bits: u32) -> Self {
        Self(bits)
    }
}

/// Payload for `LF_METHODLIST`
#[derive(Clone, Debug)]
pub struct MethodListData<'a> {
    /// Contains a repeated sequence of:
    ///
    /// ```text
    /// struct {
    ///   attr: u16,
    ///   pad0: u16,
    ///   ty: TypeIndex,
    ///   vtab_offset: u32,         // optional, present only if attr indicates it starts a vtable slot
    /// }
    /// ```
    pub bytes: &'a [u8],
}

/// `LF_ARGLIST`
#[derive(Clone, Debug)]
pub struct ArgList<'a> {
    /// Arguments of the function signature
    pub args: &'a [TypeIndexLe],
}

impl<'a> Parse<'a> for ArgList<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let arg_count = p.u32()?;
        let args = p.slice(arg_count as usize)?;
        Ok(Self { args })
    }
}

/// `LF_ALIAS` record
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Alias<'a> {
    pub utype: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Alias<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            utype: p.type_index()?,
            name: p.strz()?,
        })
    }
}

/// `LF_UDT_SRC_LINE`
///
/// See `lfUdtSrcLine` in `cvinfo.h`
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct UdtSrcLine {
    /// UDT's type index
    pub ty: TypeIndexLe,

    /// The source file which contains this UDT definition.
    /// This is a `NameIndex` value in the `/names` stream.
    pub src: NameIndexLe,

    /// Line number
    pub line: U32<LE>,
}

/// `LF_UDT_MOD_SRC_LINE`
///
/// See `lfUdtModSrcLine` in `cvinfo.h`
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct UdtModSrcLine {
    /// UDT's type index
    pub ty: TypeIndexLe,

    /// The source file which contains this UDT definition.
    /// This is a `NameIndex` value in the `/names` stream.
    pub src: NameIndexLe,

    /// Line number, 1-based
    pub line: U32<LE>,

    /// Module that contributes this UDT definition
    pub imod: U16<LE>,
}

/// CV_ItemId
pub type ItemIdLe = U32<LE>;

/// Identifies a record within the IPI Stream.
pub type ItemId = u32;

/// `LF_FUNC_ID`
#[derive(Clone, Debug)]
pub struct FuncId<'a> {
    pub fixed: &'a FuncIdFixed,
    pub name: &'a BStr,
    pub decorated_name_hash: Option<&'a U64<LE>>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct FuncIdFixed {
    /// Parent scope of the ID, 0 if global. This is used for the namespace that contains a symbol.
    /// The value points into the IPI.
    pub scope: ItemIdLe,

    /// The type of the function.
    pub func_type: TypeIndexLe,
}

impl<'a> Parse<'a> for FuncId<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
            decorated_name_hash: if p.len() >= 8 { Some(p.get()?) } else { None },
        })
    }
}

/// `LF_MFUNC_ID`
#[derive(Clone, Debug)]
pub struct MFuncId<'a> {
    pub fixed: &'a MFuncIdFixed,
    pub name: &'a BStr,
    pub decorated_name_hash: Option<&'a U64<LE>>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct MFuncIdFixed {
    /// type index of parent
    pub parent_type: TypeIndexLe,
    /// function type
    pub func_type: TypeIndexLe,
}

impl<'a> Parse<'a> for MFuncId<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
            decorated_name_hash: if p.len() >= 8 { Some(p.get()?) } else { None },
        })
    }
}

/// `LF_STRING_ID`
#[derive(Clone, Debug)]
pub struct StringId<'a> {
    pub id: ItemId,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for StringId<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let id = p.u32()?;
        let name = p.strz()?;
        Ok(Self { id, name })
    }
}

/// `LF_SUBSTR_LIST` - A list of substrings
#[derive(Clone, Debug)]
pub struct SubStrList<'a> {
    pub ids: &'a [ItemIdLe],
}

impl<'a> Parse<'a> for SubStrList<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let n = p.u32()?;
        let ids = p.slice(n as usize)?;
        Ok(Self { ids })
    }
}

/// `LF_BUILDINFO`
#[derive(Clone, Debug)]
pub struct BuildInfo<'a> {
    pub args: &'a [ItemIdLe],
}

impl<'a> BuildInfo<'a> {
    pub fn arg(&self, index: BuildInfoIndex) -> Option<ItemId> {
        let a = self.args.get(index as usize)?;
        Some(a.get())
    }
}

impl<'a> Parse<'a> for BuildInfo<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let n = p.u16()?;
        let args = p.slice(n as usize)?;
        Ok(Self { args })
    }
}

/// Identifies indexes into `BuildInfo::args`.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]
#[repr(u32)]
pub enum BuildInfoIndex {
    CurrentDirectory = 0,
    BuildTool = 1,           // Cl.exe
    SourceFile = 2,          // foo.cpp
    ProgramDatabaseFile = 3, // foo.pdb
    CommandArguments = 4,    // -I etc
}

/// Short strings for `BuildInfoIndex`
pub const BUILD_INFO_ARG_NAMES: [&str; 5] = ["cwd", "tool", "source_file", "pdb", "args"];

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct VFTable {
    /// type index of the root of path
    pub root: TypeIndexLe,
    /// type index of the path record
    pub path: TypeIndexLe,
    /// offset of virtual function table
    pub off: U32<LE>,
    /// segment of virtual function table
    pub seg: U16<LE>,
}

```

`pdb/src/types/visitor.rs`:

```rs
//! Algorithm for traversing ("visiting") the dependency graph within a type stream or between a
//! symbol stream and a type stream.

use super::{ItemId, ItemIdLe};
use crate::names::{NameIndex, NameIndexLe};
use crate::parser::{Parser, ParserError, ParserMut};
use crate::types::{introduces_virtual, PointerFlags};
use crate::types::{Leaf, TypeIndex, TypeIndexLe};
use anyhow::Context;
use std::mem::replace;
use tracing::error;

/// Defines the functions needed for generically visiting type indexes within a type record or a
/// symbol record.
///
/// This trait exists in order to allow a single generic function to handle visiting the `TypeIndex`
/// values within a buffer, generic over the mutability of the access.
pub trait RecordVisitor {
    /// True if the parser is empty
    fn is_empty(&self) -> bool;

    /// Provides access to the rest of the record
    fn peek_rest(&self) -> &[u8];

    /// Parses a `u16` value
    fn u16(&mut self) -> Result<u16, ParserError>;

    /// Parses a `u32` value
    fn u32(&mut self) -> Result<u32, ParserError>;

    /// Skips `n` bytes of input
    fn skip(&mut self, n: usize) -> Result<(), ParserError>;

    /// Parses the next `ItemId` value and visits the location or value.
    fn item(&mut self) -> Result<(), ParserError>;

    /// Parses the next `TypeIndex` value and visits the location or value.
    fn ty(&mut self) -> Result<(), ParserError>;

    /// Parses a `NameIndex` value and visits the location or value.
    fn name_index(&mut self) -> Result<(), ParserError>;

    /// Parses a `Number`.
    fn number(&mut self) -> Result<(), ParserError>;

    /// Parses a NUL-terminated string.
    fn strz(&mut self) -> Result<(), ParserError>;
}

/// Defines a visitor that visits every ItemId and TypeIndexLe in a record. Allows modification.
#[allow(missing_docs)]
pub trait IndexVisitorMut {
    #[allow(unused_variables)]
    fn type_index(&mut self, offset: usize, value: &mut TypeIndexLe) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn item_id(&mut self, offset: usize, value: &mut ItemIdLe) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn name_index(&mut self, offset: usize, value: &mut NameIndexLe) -> Result<(), ParserError> {
        Ok(())
    }
}

/// Defines a visitor that visits every ItemId and TypeIndexLe in a record.
#[allow(missing_docs)]
pub trait IndexVisitor {
    #[allow(unused_variables)]
    fn type_index(&mut self, offset: usize, value: TypeIndex) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn item_id(&mut self, offset: usize, value: ItemId) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn name_index(&mut self, offset: usize, value: NameIndex) -> Result<(), ParserError> {
        Ok(())
    }
}

struct RefVisitor<'a, IV: IndexVisitor> {
    parser: Parser<'a>,
    original_len: usize,
    index_visitor: IV,
}

impl<'a, IV: IndexVisitor> RecordVisitor for RefVisitor<'a, IV> {
    fn is_empty(&self) -> bool {
        self.parser.is_empty()
    }

    fn peek_rest(&self) -> &[u8] {
        self.parser.peek_rest()
    }

    fn u16(&mut self) -> Result<u16, ParserError> {
        self.parser.u16()
    }

    fn u32(&mut self) -> Result<u32, ParserError> {
        self.parser.u32()
    }

    fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        self.parser.skip(n)
    }

    fn ty(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ti = self.parser.type_index()?;
        self.index_visitor.type_index(offset, ti)?;
        Ok(())
    }

    fn item(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ii = self.parser.u32()?;
        self.index_visitor.item_id(offset, ii)?;
        Ok(())
    }

    fn number(&mut self) -> Result<(), ParserError> {
        self.parser.number()?;
        Ok(())
    }

    fn strz(&mut self) -> Result<(), ParserError> {
        self.parser.skip_strz()
    }

    fn name_index(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ni = NameIndex(self.parser.u32()?);
        self.index_visitor.name_index(offset, ni)?;
        Ok(())
    }
}

struct MutVisitor<'a, IV: IndexVisitorMut> {
    parser: ParserMut<'a>,
    original_len: usize,
    index_visitor: IV,
}

impl<'a, IV: IndexVisitorMut> RecordVisitor for MutVisitor<'a, IV> {
    fn is_empty(&self) -> bool {
        self.parser.is_empty()
    }

    fn peek_rest(&self) -> &[u8] {
        self.parser.peek_rest()
    }

    fn u16(&mut self) -> Result<u16, ParserError> {
        self.parser.u16()
    }

    fn u32(&mut self) -> Result<u32, ParserError> {
        self.parser.u32()
    }

    fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        self.parser.skip(n)
    }

    fn ty(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ti: &mut TypeIndexLe = self.parser.get_mut()?;
        self.index_visitor.type_index(offset, ti)?;
        Ok(())
    }

    fn item(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ii: &mut ItemIdLe = self.parser.get_mut()?;
        self.index_visitor.item_id(offset, ii)?;
        Ok(())
    }

    fn number(&mut self) -> Result<(), ParserError> {
        self.parser.skip_number()
    }

    fn strz(&mut self) -> Result<(), ParserError> {
        self.parser.skip_strz()
    }

    fn name_index(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ni: &mut NameIndexLe = self.parser.get_mut()?;
        self.index_visitor.name_index(offset, ni)?;
        Ok(())
    }
}

/// Scans the type indexes within a type record and calls `f` for each type index. This function
/// can only read data.
#[inline(never)]
pub fn visit_type_indexes_in_record_slice<IV: IndexVisitor>(
    type_kind: Leaf,
    record_data: &[u8],
    index_visitor: IV,
) -> Result<(), anyhow::Error> {
    let record_data_len = record_data.len();

    let mut v = RefVisitor {
        original_len: record_data.len(),
        parser: Parser::new(record_data),
        index_visitor,
    };

    visit_type_indexes_in_record(type_kind, &mut v).with_context(|| {
        let offset = record_data_len - v.parser.len();
        format!(
            "at byte offset 0x{:x} {} within type record",
            offset, offset
        )
    })
}

/// Scans the type indexes within a type record and calls `f` for each type index. This function
/// can modify the type indexes within the record.
#[inline(never)]
pub fn visit_type_indexes_in_record_slice_mut<IV>(
    type_kind: Leaf,
    record_data: &mut [u8],
    index_visitor: IV,
) -> Result<(), anyhow::Error>
where
    IV: IndexVisitorMut,
{
    let record_data_len = record_data.len();

    let mut v = MutVisitor {
        original_len: record_data.len(),
        parser: ParserMut::new(record_data),
        index_visitor,
    };

    visit_type_indexes_in_record(type_kind, &mut v).with_context(|| {
        let offset = record_data_len - v.parser.len();
        format!(
            "at byte offset 0x{:x} {} within type record",
            offset, offset
        )
    })
}

/// This function examines a type record and traverses the type indexes within it.
///
/// The caller provides an implementation of the visitor trait.  The visitor trait provides the
/// ability to read fields from the type record, and receives notifications that the visitor is
/// positioned on a type index.
pub fn visit_type_indexes_in_record<V: RecordVisitor>(
    type_kind: Leaf,
    p: &mut V,
) -> Result<(), ParserError> {
    match type_kind {
        Leaf::LF_LABEL => {}

        Leaf::LF_MODIFIER => {
            p.ty()?;
        }

        Leaf::LF_POINTER => {
            p.ty()?; // underlying type
            let attr = PointerFlags(p.u32()?);
            match attr.mode() {
                2 => {
                    // 2 is pointer to data member
                    p.ty()?;
                }
                3 => {
                    // 3 is pointer to method
                    p.ty()?;
                }
                _ => {}
            }
        }

        Leaf::LF_ALIAS => {
            p.ty()?;
        }

        Leaf::LF_ARRAY => {
            p.ty()?; // element type
            p.ty()?; // index type
        }

        Leaf::LF_CLASS | Leaf::LF_STRUCTURE => {
            p.u16()?; // count
            p.skip(2)?; // property
            p.ty()?; // field list
            p.ty()?; // derivation list
            p.ty()?; // vtable shape
        }

        Leaf::LF_ENUM => {
            p.u16()?; // count
            p.skip(2)?; // property
            p.ty()?; // type
            p.ty()?; // field list
        }

        Leaf::LF_UNION => {
            p.u16()?; // count
            p.skip(2)?; // property
            p.ty()?; // field list
        }

        Leaf::LF_PROCEDURE => {
            p.ty()?; // return type
            p.skip(4)?; // calling convention, reserved, and num params
            p.ty()?; // arg list
        }

        Leaf::LF_MFUNCTION => {
            p.ty()?; // return type
            p.ty()?; // class definition
            p.ty()?; // this type
            p.skip(4)?; // calling convention, reserved, and num params
            p.ty()?; // arg list
        }

        Leaf::LF_ARGLIST => {
            let num_args = p.u32()?;
            for _ in 0..num_args {
                p.ty()?;
            }
        }

        Leaf::LF_FIELDLIST => {
            let mut prev_item_kind = None;
            loop {
                let rest = p.peek_rest();
                if rest.is_empty() {
                    break;
                }

                // Check for padding (alignment) bytes.
                let mut padding_len = 0;
                while padding_len < rest.len() && rest[padding_len] >= 0xf0 {
                    padding_len += 1;
                }
                if padding_len > 0 {
                    p.skip(padding_len)?;
                }

                if p.is_empty() {
                    break;
                }

                let item_kind = Leaf(p.u16()?);
                let after = replace(&mut prev_item_kind, Some(item_kind));

                match item_kind {
                    Leaf::LF_BCLASS => {
                        let _attr = p.u16()?;
                        p.ty()?; // class type
                        p.number()?; // offset
                    }

                    Leaf::LF_VBCLASS => {
                        let _attr = p.u16()?;
                        p.ty()?; // base class
                        p.ty()?; // vbtype
                        p.number()?; // vbpoff
                        p.number()?; // vbpff
                    }

                    Leaf::LF_IVBCLASS => {
                        let _attr = p.u16()?;
                        p.ty()?; // base class
                        p.ty()?; // virtual base type
                        p.number()?; // vbpoff
                        p.number()?; // vbpff
                    }

                    Leaf::LF_ENUMERATE => {
                        // nothing needed
                        let _attr = p.u16()?;
                        p.number()?; // value
                        p.strz()?; // name
                    }

                    Leaf::LF_FRIENDFCN => {
                        p.skip(2)?; // padding
                        p.ty()?; // type
                        p.strz()?; // name
                    }

                    Leaf::LF_INDEX => {
                        p.skip(2)?; // padding
                        p.ty()?; // index
                    }

                    Leaf::LF_MEMBER => {
                        let _attr = p.u16()?;
                        p.ty()?; // type
                        p.number()?; // offset
                        p.strz()?; // name
                    }

                    Leaf::LF_STMEMBER => {
                        let _attr = p.u16()?;
                        p.ty()?; // type
                        p.strz()?; // name
                    }

                    Leaf::LF_METHOD => {
                        let _count = p.u16()?;
                        p.ty()?; // method list
                        p.strz()?; // name
                    }

                    Leaf::LF_NESTEDTYPE => {
                        p.skip(2)?; // padding
                        p.ty()?; // index
                        p.strz()?; // name
                    }

                    Leaf::LF_VFUNCTAB => {
                        p.skip(2)?; // padding
                        p.ty()?; // vtable type
                    }

                    Leaf::LF_FRIENDCLS => {
                        p.skip(2)?; // padding
                        p.ty()?; // friend class type
                    }

                    Leaf::LF_ONEMETHOD => {
                        let attr = p.u16()?; // attribute
                        p.ty()?; // type of method
                        if introduces_virtual(attr) {
                            p.u32()?; // vbaseoff
                        }
                        p.strz()?; // name
                    }

                    Leaf::LF_VFUNCOFF => {
                        p.u16()?; // padding
                        p.ty()?; // vtable type
                        p.u32()?; // offset
                    }

                    Leaf::LF_NESTEDTYPEEX => {
                        p.u16()?; // attribute
                        p.ty()?; // nested type
                        p.strz()?; // name
                    }

                    unknown_item_kind => {
                        error!(
                            ?unknown_item_kind,
                            ?after,
                            "unrecognized item within LF_FIELDLIST"
                        );
                        break;
                    }
                }
            }
        }

        Leaf::LF_DERIVED => {
            let count = p.u32()?;
            for _ in 0..count {
                p.ty()?;
            }
        }

        Leaf::LF_BITFIELD => {
            p.ty()?;
        }

        Leaf::LF_METHODLIST => {
            while !p.is_empty() {
                let attr = p.u16()?;
                p.skip(2)?; // padding
                p.ty()?;
                if introduces_virtual(attr) {
                    p.skip(4)?; // vtable offset
                }
            }
        }

        Leaf::LF_DIMCONU => {
            p.ty()?; // index type
        }

        Leaf::LF_DIMCONLU => {
            p.ty()?; // index type
        }

        Leaf::LF_DIMVARU => {
            let rank = p.u32()?;
            p.ty()?; // index type
            for _ in 0..rank {
                p.ty()?; // upper bound for this dimension
            }
        }

        // These types do not contain any pointers to other types.
        Leaf::LF_VTSHAPE | Leaf::LF_PRECOMP | Leaf::LF_ENDPRECOMP | Leaf::LF_SKIP => {}

        Leaf::LF_VFTPATH => {
            let count = p.u32()?;
            for _ in 0..count {
                p.ty()?;
            }
        }

        Leaf::LF_VFTABLE => {
            p.ty()?; // type
            p.ty()?; // base_vftable
        }

        Leaf::LF_CLASS2 | Leaf::LF_STRUCTURE2 | Leaf::LF_UNION2 | Leaf::LF_INTERFACE2 => {
            p.skip(4)?; // property
            p.ty()?; // field
            p.ty()?; // derived
            p.ty()?; // vshape
        }

        Leaf::LF_FUNC_ID => {
            p.item()?; // parent scope of the ID, 0 if global
            p.ty()?; // function type
        }

        Leaf::LF_MFUNC_ID => {
            p.ty()?; // parent type
            p.ty()?; // function type
        }

        Leaf::LF_BUILDINFO => {
            let n = p.u16()?;
            for _ in 0..n {
                p.item()?;
            }
        }

        Leaf::LF_SUBSTR_LIST => {
            let count = p.u32()?;
            for _ in 0..count {
                p.item()?;
            }
        }

        Leaf::LF_STRING_ID => {
            p.item()?; // ID to list of sub string IDs
        }

        Leaf::LF_UDT_SRC_LINE => {
            p.ty()?;
            p.name_index()?; // NameIndex of source file name
        }

        Leaf::LF_UDT_MOD_SRC_LINE => {
            p.ty()?;
            p.name_index()?; // NameIndex of source file name
        }

        _ => {
            error!("unrecognized type kind: {:?}", type_kind);
            return Err(ParserError::new());
        }
    }

    Ok(())
}

```

`pdb/src/utils.rs`:

```rs
//! Misc utilities

pub mod align;
pub mod io;
pub mod iter;
pub mod path;
pub mod swizzle;
pub mod vec;

use std::ops::Range;
use zerocopy::{FromBytes, Immutable, IntoBytes};

/// Copies a value that implements `FromBytes`, by simply copying its byte representation.
pub fn copy_from_bytes<T>(t: &T) -> T
where
    T: IntoBytes + FromBytes + Immutable,
{
    FromBytes::read_from_bytes(t.as_bytes()).unwrap()
}

/// Helps decode records that are indexed using "starts" arrays.
pub struct StartsOf<'a, T> {
    /// The "starts" array
    pub starts: &'a [u32],
    /// The items that are being indexed.
    pub items: &'a [T],
}

impl<'a, T> StartsOf<'a, T> {
    /// Initializes a new starts-based array accessor.
    pub fn new(starts: &'a [u32], items: &'a [T]) -> Self {
        debug_assert!(!starts.is_empty());
        debug_assert_eq!(starts[0], 0);
        debug_assert_eq!(*starts.last().unwrap() as usize, items.len());
        debug_assert!(starts.windows(2).all(|w| w[0] <= w[1]));

        Self { starts, items }
    }
}

impl<'a, T> std::ops::Index<usize> for StartsOf<'a, T> {
    type Output = [T];

    fn index(&self, i: usize) -> &[T] {
        let start = self.starts[i] as usize;
        let end = self.starts[i + 1] as usize;
        &self.items[start..end]
    }
}

/// True if `n` is a multiple of 4.
pub fn is_aligned_4(n: usize) -> bool {
    (n & 3) == 0
}

/// Align n up to the next multiple of 4, if it is not already a multiple of 4.
pub fn align_4(n: usize) -> usize {
    (n + 3) & !3
}

/// Iterates ranges of items within a slice that share a common property.
pub fn iter_similar_ranges<'a, T, F>(items: &'a [T], is_eq: F) -> IterSimilarRanges<'a, T, F> {
    IterSimilarRanges {
        items,
        is_eq,
        start: 0,
    }
}

/// Iterates ranges of items within a slice that share a common property.
pub struct IterSimilarRanges<'a, T, F> {
    items: &'a [T],
    is_eq: F,
    start: usize,
}

impl<'a, T, F> Iterator for IterSimilarRanges<'a, T, F>
where
    F: FnMut(&T, &T) -> bool,
{
    type Item = Range<usize>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.items.is_empty() {
            return None;
        }

        let first = &self.items[0];

        let mut i = 1;
        while i < self.items.len() && (self.is_eq)(first, &self.items[i]) {
            i += 1;
        }

        let start = self.start;
        self.start += i;
        self.items = &self.items[i..];

        Some(start..start + i)
    }
}

/// Iterates ranges of items within a slice that share a common property.
pub fn iter_similar_slices<'a, T, F>(items: &'a [T], is_eq: F) -> IterSimilarSlices<'a, T, F> {
    IterSimilarSlices { items, is_eq }
}

/// Iterates slices of items within a slice that share a common property.
pub struct IterSimilarSlices<'a, T, F> {
    items: &'a [T],
    is_eq: F,
}

impl<'a, T, F> Iterator for IterSimilarSlices<'a, T, F>
where
    F: FnMut(&T, &T) -> bool,
{
    type Item = &'a [T];

    fn next(&mut self) -> Option<Self::Item> {
        if self.items.is_empty() {
            return None;
        }

        let first = &self.items[0];

        let mut i = 1;
        while i < self.items.len() && (self.is_eq)(first, &self.items[i]) {
            i += 1;
        }

        let (lo, hi) = self.items.split_at(i);
        self.items = hi;
        Some(lo)
    }
}

/// Iterates ranges of items within a slice that share a common property.
pub fn iter_similar_slices_mut<'a, T, F>(
    items: &'a mut [T],
    is_eq: F,
) -> IterSimilarSlicesMut<'a, T, F> {
    IterSimilarSlicesMut { items, is_eq }
}

/// Iterates slices of items within a slice that share a common property.
pub struct IterSimilarSlicesMut<'a, T, F> {
    items: &'a mut [T],
    is_eq: F,
}

impl<'a, T, F> Iterator for IterSimilarSlicesMut<'a, T, F>
where
    F: FnMut(&T, &T) -> bool,
{
    type Item = &'a mut [T];

    fn next(&mut self) -> Option<Self::Item> {
        if self.items.is_empty() {
            return None;
        }

        let items = std::mem::take(&mut self.items);
        let first = &items[0];

        let mut i = 1;
        while i < items.len() && (self.is_eq)(first, &items[i]) {
            i += 1;
        }

        let (lo, hi) = items.split_at_mut(i);
        self.items = hi;
        Some(lo)
    }
}

```

`pdb/src/utils/align.rs`:

```rs
//! Alignment and padding utilities

/// Returns the number of alignment padding bytes that are needed to reach an alignment of 4,
/// given the number of bytes already in a buffer.
pub fn alignment_bytes_needed_4(n: usize) -> usize {
    (4 - (n & 3)) & 3
}

```

`pdb/src/utils/io.rs`:

```rs
#![allow(missing_docs)]

use std::io::{Read, Seek, SeekFrom, Write};
use sync_file::ReadAt;
use zerocopy::{FromBytes, FromZeros, IntoBytes};

pub fn read_struct_at<T: FromBytes + IntoBytes, R: ReadAt>(
    r: &R,
    offset: u64,
) -> std::io::Result<T> {
    let mut value = T::new_zeroed();
    r.read_exact_at(value.as_mut_bytes(), offset)?;
    Ok(value)
}

pub fn read_struct<T: FromBytes + IntoBytes, R: Read>(r: &mut R) -> std::io::Result<T> {
    let mut value: T = T::new_zeroed();
    let value_bytes = value.as_mut_bytes();
    r.read_exact(value_bytes)?;
    Ok(value)
}

pub fn read_boxed_slice<T: FromBytes + IntoBytes, R: Read>(
    r: &mut R,
    n: usize,
) -> std::io::Result<Box<[T]>> {
    let mut value = <[T]>::new_box_zeroed_with_elems(n).unwrap();
    r.read_exact(value.as_mut_bytes())?;
    Ok(value)
}

pub fn read_boxed_slice_at<T: FromBytes + IntoBytes, R: ReadAt>(
    r: &mut R,
    offset: u64,
    n: usize,
) -> std::io::Result<Box<[T]>> {
    let mut value = <[T]>::new_box_zeroed_with_elems(n).unwrap();
    r.read_exact_at(value.as_mut_bytes(), offset)?;
    Ok(value)
}

pub fn write_at<W: Write + Seek>(w: &mut W, pos: u64, data: &[u8]) -> std::io::Result<()> {
    w.seek(SeekFrom::Start(pos))?;
    w.write_all(data)
}

```

`pdb/src/utils/iter.rs`:

```rs
//! Iterator utilities

use std::ops::Range;

/// Allows iterators to report the remaining, unparsed bytes within an iterator.
///
/// This is for iterators that parse items from `&[u8]` buffers or similar.
pub trait HasRestLen {
    /// Returns the number of bytes (or elements, abstractly) that have not yet been parsed by this
    /// iterator.
    fn rest_len(&self) -> usize;
}

/// An iterator adapter which reports the byte ranges of the items that are iterated by the
/// underlying iterator. The underlying iterator must implement `HasRestLen`.
pub struct IterWithRange<I> {
    original_len: usize,
    inner: I,
}

impl<I> IterWithRange<I> {
    /// The number of items (usually bytes) that were present in the inner iterator when this
    /// `IterWithRange` was created. This value allows us to convert the "bytes remaining" value
    /// (`rest_len()`, which is what the inner iterator operates directly on) to an offset from the
    /// beginning of a buffer.
    pub fn original_len(&self) -> usize {
        self.original_len
    }

    /// Gets access to the inner iterator.
    pub fn inner(&self) -> &I {
        &self.inner
    }

    /// Gets mutable access to the inner iterator.
    ///
    /// Be warned!  If you modify this iterator, make sure you don't break its relationship with
    /// the `original_len` value.  Iterating items from it is fine, because that should never
    /// break the relationship with `original_len`.
    ///
    /// What would break it would be replacing the inner iterator with one that has a length that
    /// is greater than `original_len`.
    pub fn inner_mut(&mut self) -> &mut I {
        &mut self.inner
    }

    /// The current position in the iteration range.
    #[inline(always)]
    pub fn pos(&self) -> usize
    where
        I: HasRestLen,
    {
        self.original_len - self.inner.rest_len()
    }
}

impl<I: Iterator> Iterator for IterWithRange<I>
where
    I: HasRestLen,
{
    type Item = (Range<usize>, I::Item);

    fn next(&mut self) -> Option<Self::Item> {
        let pos_before = self.pos();
        let item = self.inner.next()?;
        let pos_after = self.pos();
        Some((pos_before..pos_after, item))
    }
}

/// An extension trait for iterators that converts an `Iterator` into an `IterWithRange`.
/// Use `foo.with_ranges()` to convert (augment) the iterator.
pub trait IteratorWithRangesExt: Sized {
    /// Augments this iterator with information about the byte range of each underlying item.
    fn with_ranges(self) -> IterWithRange<Self>;
}

impl<I> IteratorWithRangesExt for I
where
    I: Iterator + HasRestLen,
{
    fn with_ranges(self) -> IterWithRange<Self> {
        IterWithRange {
            original_len: self.rest_len(),
            inner: self,
        }
    }
}

use std::collections::BTreeMap;

/// Reads a slice of items and groups them using a function over the items.
pub fn group_by<'a, T, F, K>(s: &'a [T], f: F) -> BTreeMap<K, Vec<&'a T>>
where
    F: Fn(&T) -> K,
    K: Ord + Eq,
{
    let mut out: BTreeMap<K, Vec<&'a T>> = BTreeMap::new();

    for item in s.iter() {
        let key = f(item);
        if let Some(list) = out.get_mut(&key) {
            list.push(item);
        } else {
            out.insert(key, vec![item]);
        }
    }

    out
}

/// Reads a sequence of items and groups them using a function over the items.
pub fn group_by_iter_ref<'a, T, F, I, K>(iter: I, f: F) -> BTreeMap<K, Vec<&'a T>>
where
    I: Iterator<Item = &'a T>,
    F: Fn(&T) -> K,
    K: Ord + Eq,
{
    let mut out: BTreeMap<K, Vec<&'a T>> = BTreeMap::new();

    for item in iter {
        let key = f(item);
        if let Some(list) = out.get_mut(&key) {
            list.push(item);
        } else {
            out.insert(key, vec![item]);
        }
    }

    out
}

/// Reads a sequence of items and groups them using a function over the items.
pub fn group_by_iter<I, F, K>(iter: I, f: F) -> BTreeMap<K, Vec<I::Item>>
where
    I: Iterator,
    F: Fn(&I::Item) -> K,
    K: Ord + Eq,
{
    let mut out: BTreeMap<K, Vec<I::Item>> = BTreeMap::new();

    for item in iter {
        let key = f(&item);
        if let Some(list) = out.get_mut(&key) {
            list.push(item);
        } else {
            out.insert(key, vec![item]);
        }
    }

    out
}

```

`pdb/src/utils/path.rs`:

```rs
//! Utilities for working with filesystem paths

use std::path::Path;

/// Tests whether `container_path` is equal to `nested_path` or is an ancestor of `nested_path`.
pub fn path_contains(container_path: &str, nested_path: &str) -> bool {
    let c_path = Path::new(container_path);
    let n_path = Path::new(nested_path);

    if c_path.is_absolute() != n_path.is_absolute() {
        return false;
    }

    let mut ci = c_path.components();
    let mut ni = n_path.components();

    loop {
        match (ci.next(), ni.next()) {
            (Some(ce), Some(ne)) => {
                // Ignore case, because Windows.
                if !ce.as_os_str().eq_ignore_ascii_case(ne.as_os_str()) {
                    return false;
                }
            }

            // We ran out of nested elements, but still have more container elements. Not a match.
            (Some(_), None) => return false,

            // We ran out of container elements, so it's a match.
            (None, _) => return true,
        }
    }
}

#[test]
fn test_path_contains() {
    assert!(!path_contains(r"d:\src", r"foo.c"));

    assert!(path_contains(r"d:\src", r"d:\src\foo.c"));
    assert!(path_contains(r"d:\src", r"D:\SRC\\foo.c"));
    assert!(path_contains(r"d:\src\", r"d:\src"));
    assert!(path_contains(r"d:\src", r"d:\src\"));

    // negative cases
    assert!(!path_contains(r"d:\src", r"e:\src\foo.c"));
    assert!(!path_contains(r"d:\src", r"d:\bar"));
}

```

`pdb/src/utils/swizzle.rs`:

```rs
//! Traits and support for in-place byte-order swizzling

/// Defines the behavior of converting from the host byte order to specific external byte orders
/// (LE and BE).
pub trait Swizzle {
    /// Converts values within this value from LE order to host order.
    /// On LE architectures, this does nothing.
    fn le_to_host(&mut self);
}

macro_rules! int_swizzle {
    ($t:ty) => {
        impl Swizzle for $t {
            fn le_to_host(&mut self) {
                if cfg!(target_endian = "big") {
                    *self = Self::from_le(*self);
                }
            }
        }
    };
}

int_swizzle!(u16);
int_swizzle!(u32);
int_swizzle!(u64);
int_swizzle!(u128);

int_swizzle!(i16);
int_swizzle!(i32);
int_swizzle!(i64);
int_swizzle!(i128);

impl Swizzle for u8 {
    fn le_to_host(&mut self) {}
}

impl Swizzle for i8 {
    fn le_to_host(&mut self) {}
}

impl<T: Swizzle> Swizzle for [T] {
    fn le_to_host(&mut self) {
        for i in self.iter_mut() {
            i.le_to_host();
        }
    }
}

```

`pdb/src/utils/vec.rs`:

```rs
//! Utilities for `Vec`

use std::cmp::Ordering;

/// Replace a range of values in a vector with a new range. The old and new ranges can be
/// different sizes.
pub fn replace_range_copy<T: Copy>(v: &mut Vec<T>, start: usize, old_len: usize, values: &[T]) {
    assert!(start <= v.len());
    assert!(old_len <= v.len() - start);

    match values.len().cmp(&old_len) {
        Ordering::Equal => {
            v[start..start + values.len()].copy_from_slice(values);
        }

        Ordering::Less => {
            // The new values are shorter than the old values.
            // Copy the overlap, then drain the remainder.
            v[start..start + values.len()].copy_from_slice(values);
            v.drain(start + values.len()..start + old_len);
        }

        Ordering::Greater => {
            // Copy the overlapping values.
            // Then append the other values.
            // Then rotate them into position.
            let (lo, hi) = values.split_at(old_len);
            v.extend_from_slice(hi);
            v[start..start + old_len].copy_from_slice(lo);
            v[start + old_len..].rotate_right(lo.len());
        }
    }
}

```

`pdb/src/writer.rs`:

```rs
//! Utilties for copying data between streams.

use std::io::{Read, Write};

/// Copies all data from `Src` to `Dst`
pub fn copy_stream_with_buffer<Dst, Src>(
    mut dst: Dst,
    mut src: Src,
    buffer: &mut [u8],
) -> std::io::Result<()>
where
    Dst: Write,
    Src: Read,
{
    loop {
        let n = src.read(buffer)?;
        if n == 0 {
            break;
        }
        dst.write_all(&buffer[..n])?;
    }

    Ok(())
}

/// Copies all data from `Src` to `Dst`
pub fn copy_stream<Dst, Src>(dst: Dst, src: Src) -> std::io::Result<()>
where
    Dst: Write,
    Src: Read,
{
    const BUFFER_LEN: usize = 16 << 20; // 16 MiB

    let mut buffer = vec![0; BUFFER_LEN];
    copy_stream_with_buffer(dst, src, &mut buffer)
}

```

`pdb/tests/cpp_check.rs`:

```rs
//! This integration test runs the MSVC compiler and linker to generate complete executables and
//! and PDBs, and then reads the PDBs and verifies that they contain the expected information.

#![cfg(windows)]
#![allow(clippy::single_match)]
#![allow(clippy::useless_vec)]

use bstr::BStr;
use ms_pdb::syms::{Data, SymData, SymKind};
use ms_pdb::types::fields::Field;
use ms_pdb::types::{TypeData, TypeIndex};
use ms_pdb::Pdb;
use std::collections::HashMap;
use std::path::Path;
use std::process::Command;
use tracing::{error, info, trace};

const CARGO_MANIFEST_DIR: &str = env!("CARGO_MANIFEST_DIR");
const CARGO_TARGET_TMPDIR: &str = env!("CARGO_TARGET_TMPDIR");

#[static_init::dynamic]
static INIT_LOGGER: () = {
    tracing_subscriber::fmt()
        .with_ansi(false)
        .with_test_writer()
        .with_file(true)
        .with_line_number(true)
        .with_max_level(tracing::Level::DEBUG)
        .compact()
        .without_time()
        .finish();
};

// id should be the base name of a C++ source file in the "cpp_check" directory.
// e.g. id == `types`
fn run_test(id: &str) -> Box<Pdb> {
    info!("case: {id}");

    let cargo_manifest_dir = Path::new(CARGO_MANIFEST_DIR);
    let cargo_target_tmpdir = Path::new(CARGO_TARGET_TMPDIR);
    let cases_tmpdir = cargo_target_tmpdir.join("cases");

    let cases_dir = Path::new(cargo_manifest_dir)
        .join("tests")
        .join("cpp_check");
    let source_file_name = format!("{id}.cpp");
    let source_file_path = cases_dir.join(&source_file_name);

    let this_output_dir = cases_tmpdir.join(id);

    info!("source file: {}", source_file_name);
    info!("output dir: {}", this_output_dir.display());

    let dll_file_name = format!("{id}.dll");
    let pdb_file_name = format!("{id}.pdb");

    let dll_path = this_output_dir.join(dll_file_name);
    let pdb_path = this_output_dir.join(pdb_file_name);

    info!("target: {}", dll_path.display());
    info!("pdb:    {}", pdb_path.display());

    std::fs::create_dir_all(&this_output_dir).unwrap();

    let obj_file_path = this_output_dir.join(format!("{id}.obj"));

    {
        let mut cmd = Command::new("cl.exe");
        cmd.current_dir(&this_output_dir);
        cmd.arg("/nologo");
        cmd.arg("/Z7");
        cmd.arg("/c");
        cmd.arg("/O2");
        cmd.arg(source_file_path);
        cmd.arg(format!(
            "/Fo{}",
            obj_file_path.as_os_str().to_string_lossy()
        ));

        let mut child = cmd.spawn().unwrap();
        let child_exit = child.wait().unwrap();
        assert!(child_exit.success(), "cl.exe failed");
    }

    {
        let mut cmd = Command::new("link.exe");
        cmd.current_dir(&this_output_dir);
        cmd.arg("/nologo");
        cmd.arg("/dll");
        cmd.arg("/debug:full");
        cmd.arg(format!("/out:{}", dll_path.display()));
        cmd.arg(format!("/pdb:{}", pdb_path.display()));
        cmd.arg(format!("{}", obj_file_path.as_os_str().to_string_lossy()));

        let mut child = cmd.spawn().unwrap();
        let child_exit = child.wait().unwrap();
        assert!(child_exit.success(), "link.exe failed");
    }

    Pdb::open(&pdb_path).unwrap()
}

#[test]
fn types() -> anyhow::Result<()> {
    let pdb = run_test("types");

    let gss = pdb.read_gss()?;
    let gsi = pdb.read_gsi()?;

    let get_global = |kind: SymKind, name: &str| -> SymData {
        let sym = gsi
            .find_symbol(&gss, name.into())
            .expect("expected find_symbol to succeed")
            .unwrap_or_else(|| panic!("expected find_symbol to succeed: {name}"));
        assert_eq!(sym.kind, kind, "expected symbol kind {kind:?} : {name}");
        let sym_data = sym
            .parse()
            .unwrap_or_else(|e| panic!("expected symbol parse to succeed: {name} : {e:?}"));
        sym_data
    };

    let get_global_data = |kind: SymKind, name: &str| -> Data {
        match get_global(kind, name) {
            SymData::Data(d) => d,
            unknown => panic!("expected SymData::Data for {name}, got: {unknown:?}"),
        }
    };

    let names = vec![
        "__acrt_initial_locale_pointers",
        "__xi_a",
        "get_initial_environment",
        "FEOFLAG",
        "StructWithManyEnums",
        "enums_export",
    ];

    // Dump some stuff for fun.
    for name in names.iter() {
        let s = gsi.find_symbol(&gss, BStr::new(name)).unwrap();
        info!("{:?} --> {:?}", name, s);
    }

    let enums_export = gsi.find_symbol(&gss, BStr::new("enums_export"))?.unwrap();
    assert_eq!(enums_export.kind, SymKind::S_PROCREF);

    let type_stream = pdb.read_type_stream()?;

    // Check that primitive types match the values we're expecting.
    {
        let primitives_data = get_global_data(SymKind::S_GDATA32, "g_structWithPrimitiveTypes");
        let primitives_ty_record = type_stream.record(primitives_data.header.type_.get())?;
        let primitives_ty_struct = match primitives_ty_record.parse()? {
            TypeData::Struct(s) => s,
            unknown => panic!("Expected StructWithPrimitiveTypes to be a struct: {unknown:?}"),
        };

        // Index the member (data) fields by name
        let mut fields: HashMap<&BStr, TypeIndex> = HashMap::new();
        for f in type_stream.iter_fields(primitives_ty_struct.fixed.field_list.get()) {
            match f {
                Field::Member(m) => {
                    // Turn this on when adding new fields in types.cpp
                    if false {
                        if m.ty.0 < 0x1000 {
                            println!("  (TypeIndex::{:?}, \"{}\"),", m.ty, m.name);
                        } else {
                            println!("  // non-primitive field: {}", m.name);
                        }
                    }
                    fields.insert(m.name, m.ty);
                }
                _ => {}
            }
        }

        // Validate our expectations
        //
        // TODO: This will fail if the C++ code is compiled for a 32-bit architecture because the
        // pointer types encode the size of the pointer.
        let expectations: &[(TypeIndex, &str)] = &[
            (TypeIndex::T_RCHAR, "f_char"),
            (TypeIndex::T_RCHAR, "f_const_char"),
            (TypeIndex::T_CHAR, "f_signed_char"),
            (TypeIndex::T_UCHAR, "f_unsigned_char"),
            (TypeIndex::T_64PRCHAR, "f_char_ptr"),
            // non-primitive field: f_const_char_ptr
            (TypeIndex::T_64PRCHAR, "f_char_const_ptr"),
            // non-primitive field: f_const_char_const_ptr
            (TypeIndex::T_INT4, "f_int"),
            (TypeIndex::T_INT4, "f_const_int"),
            (TypeIndex::T_INT4, "f_signed_int"),
            (TypeIndex::T_UINT4, "f_unsigned_int"),
            (TypeIndex::T_64PINT4, "f_int_ptr"),
            // non-primitive field: f_const_int_ptr
            (TypeIndex::T_64PINT4, "f_int_const_ptr"),
            // non-primitive field: f_const_int_const_ptr
            (TypeIndex::T_LONG, "f_long"),
            (TypeIndex::T_LONG, "f_const_long"),
            (TypeIndex::T_LONG, "f_signed_long"),
            (TypeIndex::T_ULONG, "f_unsigned_long"),
            (TypeIndex::T_64PLONG, "f_long_ptr"),
            // non-primitive field: f_const_long_ptr
            (TypeIndex::T_64PLONG, "f_long_const_ptr"),
            // non-primitive field: f_const_long_const_ptr
            (TypeIndex::T_SHORT, "f_short"),
            (TypeIndex::T_SHORT, "f_const_short"),
            (TypeIndex::T_SHORT, "f_signed_short"),
            (TypeIndex::T_USHORT, "f_unsigned_short"),
            (TypeIndex::T_64PSHORT, "f_short_ptr"),
            // non-primitive field: f_const_short_ptr
            (TypeIndex::T_64PSHORT, "f_short_const_ptr"),
            // non-primitive field: f_const_short_const_ptr
            (TypeIndex::T_QUAD, "f__long_long"),
            (TypeIndex::T_QUAD, "f_const__long_long"),
            (TypeIndex::T_QUAD, "f_signed__long_long"),
            (TypeIndex::T_UQUAD, "f_unsigned__long_long"),
            (TypeIndex::T_64PQUAD, "f__long_long_ptr"),
            // non-primitive field: f_const__long_long_ptr
            (TypeIndex::T_64PQUAD, "f__long_long_const_ptr"),
            // non-primitive field: f_const__long_long_const_ptr
            (TypeIndex::T_RCHAR, "f_int8"),
            (TypeIndex::T_RCHAR, "f_const_int8"),
            (TypeIndex::T_CHAR, "f_signed_int8"),
            (TypeIndex::T_UCHAR, "f_unsigned_int8"),
            (TypeIndex::T_64PRCHAR, "f_int8_ptr"),
            // non-primitive field: f_const_int8_ptr
            (TypeIndex::T_64PRCHAR, "f_int8_const_ptr"),
            // non-primitive field: f_const_int8_const_ptr
            (TypeIndex::T_SHORT, "f_int16"),
            (TypeIndex::T_SHORT, "f_const_int16"),
            (TypeIndex::T_SHORT, "f_signed_int16"),
            (TypeIndex::T_USHORT, "f_unsigned_int16"),
            (TypeIndex::T_64PSHORT, "f_int16_ptr"),
            // non-primitive field: f_const_int16_ptr
            (TypeIndex::T_64PSHORT, "f_int16_const_ptr"),
            // non-primitive field: f_const_int16_const_ptr
            (TypeIndex::T_INT4, "f_int32"),
            (TypeIndex::T_INT4, "f_const_int32"),
            (TypeIndex::T_INT4, "f_signed_int32"),
            (TypeIndex::T_UINT4, "f_unsigned_int32"),
            (TypeIndex::T_64PINT4, "f_int32_ptr"),
            // non-primitive field: f_const_int32_ptr
            (TypeIndex::T_64PINT4, "f_int32_const_ptr"),
            // non-primitive field: f_const_int32_const_ptr
            (TypeIndex::T_QUAD, "f_int64"),
            (TypeIndex::T_QUAD, "f_const_int64"),
            (TypeIndex::T_QUAD, "f_signed_int64"),
            (TypeIndex::T_UQUAD, "f_unsigned_int64"),
            (TypeIndex::T_64PQUAD, "f_int64_ptr"),
            // non-primitive field: f_const_int64_ptr
            (TypeIndex::T_64PQUAD, "f_int64_const_ptr"),
            // non-primitive field: f_const_int64_const_ptr
            (TypeIndex::T_BOOL8, "f_bool"),
            (TypeIndex::T_64PBOOL08, "f_bool_ptr"),
            (TypeIndex::T_64PVOID, "f_void_ptr"),
            (TypeIndex::T_BOOL8, "f_bool"),
            (TypeIndex::T_64PBOOL08, "f_bool_ptr"),
            (TypeIndex::T_REAL32, "f_float"),
            (TypeIndex::T_64PREAL32, "f_float_ptr"),
            (TypeIndex::T_REAL64, "f_double"),
            (TypeIndex::T_64PREAL64, "f_double_ptr"),
        ];

        let mut error = false;
        for &(expected_type, name) in expectations.iter() {
            if let Some(&actual_type) = fields.get(BStr::new(name)) {
                if expected_type == actual_type {
                    trace!("field has correct type: {expected_type:?} - {name}");
                } else {
                    error!("expected field {name} to have type {expected_type:?}, but it had type {actual_type:?}");
                    error = true;
                }
            } else {
                error!("did not find field: {name}");
                error = true;
            }
        }

        assert!(!error, "Found one or more fields with the wrong type");
    }

    // Find an S_LPROCREF symbol.
    {
        let _gf = match get_global(SymKind::S_PROCREF, "global_function") {
            SymData::RefSym2(r) => r,
            unknown => panic!("wrong symbol data for global_function: {unknown:?}"),
        };
        // TODO: look up the actual S_GDATA32 symbol and check things about it
    }

    // Find an S_CONSTANT symbol at global scope.
    {
        let c = match get_global(SymKind::S_CONSTANT, "WHAT_IS_SIX_TIMES_SEVEN") {
            SymData::Constant(c) => c,
            unknown => panic!("expected S_CONSTANT, got: {unknown:?}"),
        };
        let value: i32 = c.value.try_into().unwrap();
        assert_eq!(value, 42);
    }

    // Find an S_CONSTANT symbol that is nested within a class.
    {
        let c = match get_global(SymKind::S_CONSTANT, "Zebra::NUMBER_OF_STRIPES") {
            SymData::Constant(c) => c,
            unknown => panic!("expected S_CONSTANT, got: {unknown:?}"),
        };
        let value: i32 = c.value.try_into().unwrap();
        assert_eq!(value, 80);
    }

    // Find an S_CONSTANT symbol that is within nested C++ namespaces.
    {
        let c = match get_global(SymKind::S_CONSTANT, "foo::bar::CONSTANT_INSIDE_NAMESPACE") {
            SymData::Constant(c) => c,
            unknown => panic!("expected S_CONSTANT, got: {unknown:?}"),
        };
        let value: i32 = c.value.try_into().unwrap();
        assert_eq!(value, -333);
    }

    Ok(())
}

```

`pdb/tests/cpp_check/types.cpp`:

```cpp
// #include <stdint.h>

#include <stdlib.h>

enum EnumSimple
{
    Simple_A = 100,
    Simple_B = 200,
};
typedef EnumSimple EnumSimple;

__declspec(dllexport) EnumSimple g_enumSimpleValue = EnumSimple::Simple_B;

enum class EnumClass
{
    A = 100,
    B = 200,
};

enum EnumOverInt : int
{
    EnumOverInt_A = 100,
    EnumOverInt_B = 200,
};

enum class EnumClassOverInt : int
{
    A = 100,
    B = 200,
};

enum class EnumClassOverUInt8 : unsigned __int8
{
    Z = 10,
};

struct StructWithManyEnums
{
    EnumSimple enum_simple;
    EnumClass enum_class;
    EnumOverInt enum_over_int;
    EnumClassOverInt enum_class_over_int;
    EnumClassOverUInt8 enum_class_over_uint8;
};

struct StructWithPrimitiveTypes
{
#define INT_VARIANTS(ty, name)      \
    ty f_##name;                    \
    ty f_const_##name;              \
    signed ty f_signed_##name;      \
    unsigned ty f_unsigned_##name;  \
    ty *f_##name##_ptr;             \
    const ty *f_const_##name##_ptr; \
    ty *f_##name##_const_ptr;       \
    const ty *f_const_##name##_const_ptr;

    INT_VARIANTS(char, char)
    INT_VARIANTS(int, int)
    INT_VARIANTS(long, long)
    INT_VARIANTS(short, short)
    INT_VARIANTS(long long, _long_long)
    INT_VARIANTS(__int8, int8)
    INT_VARIANTS(__int16, int16)
    INT_VARIANTS(__int32, int32)
    INT_VARIANTS(__int64, int64)

#undef INT_VARIANTS

    void *f_void_ptr;

    bool f_bool;
    bool *f_bool_ptr;

    float f_float;
    float *f_float_ptr;
    double f_double;
    double *f_double_ptr;
};

const int WHAT_IS_SIX_TIMES_SEVEN = 42;

class Zebra
{
public:
    static constexpr short NUMBER_OF_STRIPES = 80;
};

namespace foo
{
    namespace bar
    {
        const long long CONSTANT_INSIDE_NAMESPACE = -333;
    }
}


class __declspec(dllexport) ExportedClass
{
public:
    int x_;
    bool live_;

    ExportedClass();
    ExportedClass(int x);
    ExportedClass(const ExportedClass &other);
    ExportedClass(ExportedClass &&other);
    ExportedClass &operator=(const ExportedClass &other);
    ExportedClass &operator=(ExportedClass &&other);
    operator int() const;
    void operator()() const;
};

ExportedClass::ExportedClass() {
    abort();
}

ExportedClass::ExportedClass(int x) : x_(x), live_(true) {}

ExportedClass::ExportedClass(const ExportedClass &other) : x_(other.x_) {}

ExportedClass::ExportedClass(ExportedClass &&other) : x_(other.x_)
{
    if (other.live_)
    {
        live_ = true;
        x_ = other.x_;
        other.live_ = false;
        other.x_ = 0;
    }
    else
    {
        live_ = false;
        x_ = 0;
    }
}

__declspec(dllexport)
    ExportedClass &ExportedClass::operator=(const ExportedClass &)
{
    return *this;
}

__declspec(dllexport)
    ExportedClass &ExportedClass::operator=(ExportedClass &&)
{
    return *this;
}

__declspec(dllexport)
    ExportedClass::operator int() const
{
    return 0;
}

__declspec(dllexport) void ExportedClass::operator()() const
{
    abort();
}

__declspec(dllexport) ExportedClass *newExportedClass()
{
    return new ExportedClass();
}

__declspec(dllexport) StructWithPrimitiveTypes g_structWithPrimitiveTypes;

__declspec(noinline) int global_function()
{
    return 0;
}

__declspec(noinline) extern "C" int global_function_c_linkage()
{
    return WHAT_IS_SIX_TIMES_SEVEN;
}

__declspec(dllexport) void enums_export(StructWithManyEnums *s)
{
    __annotation(L"Hello!", L"World!");

    s->enum_simple = Simple_A;
    s->enum_class = EnumClass::A;
    s->enum_over_int = EnumOverInt_A;
    s->enum_class_over_int = EnumClassOverInt::A;
    s->enum_class_over_uint8 = EnumClassOverUInt8::Z;

    global_function();
    global_function_c_linkage();
}


```

`pdbtool/Cargo.toml`:

```toml
[package]
name = "pdbtool"
version = "0.1.0"
edition = "2021"
description = "A tool for reading Program Database (PDB) files and displaying information about them."
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[bins]
test = false
doctest = false

[features]
tracy = ["dep:tracing-tracy"]

[dependencies]
anyhow.workspace = true
bitvec.workspace = true
bstr.workspace = true
bumpalo.workspace = true
dbg-ranges = "0.1.1"
friendly = "0.2"
glob = "0.3.2"
regex = "1.0"
static_init.workspace = true
clap = { workspace = true, features = ["derive"] }
tracing-subscriber = { workspace = true }
tracing-tracy = { workspace = true, optional = true, features = ["enable", "flush-on-exit"] }
tracing = { workspace = true, features = ["release_max_level_debug"] }
zerocopy-derive.workspace = true
zerocopy.workspace = true
zstd.workspace = true

[dependencies.ms-pdb]
version = "0.1.0"
path = "../pdb"


```

`pdbtool/src/addsrc.rs`:

```rs
use anyhow::{bail, Context, Result};
use bstr::ByteSlice;
use ms_pdb::{BStr, Pdb};

#[derive(clap::Parser)]
pub struct AddSrcOptions {
    /// The PDB to modify.
    pub pdb: String,

    /// A list of source files to embed into the PDB.
    pub source_files: Vec<String>,

    /// A list of directories (path prefixes). If this list contains any values, then
    /// this tool will scan the sources table within the PDB and look for any source file
    /// that was compiled and is underneath any path in the `under` list. If so, then the
    /// source file will be read and embedded into the PDB.
    ///
    /// For example: `pdbtool add-src foo.pdb --under=d:\some\dir`
    #[arg(long)]
    pub under: Vec<String>,
}

pub fn command(options: AddSrcOptions) -> Result<()> {
    if options.source_files.is_empty() && options.under.is_empty() {
        bail!("You must specify at least one source file to add to the PDB.");
    }

    let mut pdb = ms_pdb::Pdb::modify(options.pdb.as_ref())?;

    for source_file in options.source_files.iter() {
        let (fake_source_file, real_source_file): (&str, &str) =
            if let Some(s) = source_file.split_once('=') {
                s
            } else {
                (source_file, source_file)
            };

        embed_source_file(&mut pdb, fake_source_file, real_source_file)?;
    }

    if !options.under.is_empty() {
        let sources = pdb.sources()?;

        // Build a list of the unique source files. This should really be moved into Pdb.
        let mut source_files: Vec<(u32, &BStr)> = sources.iter_sources().collect();
        source_files.sort_unstable_by_key(|&(offset, _name)| offset);
        source_files.dedup();

        // Scan each source file.
        for &(_, file_name) in source_files.iter() {
            let Ok(file_name) = file_name.to_str() else {
                // icky file name
                continue;
            };

            for under in options.under.iter() {
                if strip_prefix_ignore_ascii_case(under, file_name).is_some() {
                    println!("prefix matched: {under} : {file_name}");
                }
            }
        }
    }

    pdb.flush_all()?;
    let committed = pdb.msf_mut_err()?.commit()?;
    if committed {
        println!("Changes successfully committed to PDB.");
    } else {
        println!(
            "No changes were written to disk. The given files are already embedded in the PDB."
        );
    }

    Ok(())
}

/// Embeds a source file into the PDB.
///
/// `file_name` specifies the path to the file to embed. This function uses `file_name` to open
/// the file and read its contents.
///
/// `path_within_pdb` specifies the file name to use within the PDB. This can be different from
/// `file_name`. For example, source root directories (or object directories) can be chopped off
/// and replaced with a well-known prefix.
///
/// If there is already a source file embedded in the PDB with the same file name (as specified by `path_within_pdb`), then this will update
/// the existing stream instead of modifying it.
fn embed_source_file(pdb: &mut Pdb, path_within_pdb: &str, file_name: &str) -> Result<()> {
    let file_contents =
        std::fs::read(file_name).with_context(|| format!("Failed to open file: {file_name}"))?;
    if pdb.add_embedded_source(path_within_pdb, &file_contents)? {
        println!("{file_name} : embedded");
    } else {
        println!("{file_name} : already embedded (no change)");
    }

    Ok(())
}

fn strip_prefix_ignore_ascii_case<'a>(prefix: &str, s: &'a str) -> Option<&'a str> {
    if s.is_char_boundary(prefix.len()) {
        let (lo, hi) = s.split_at(prefix.len());
        if prefix.eq_ignore_ascii_case(lo) {
            Some(hi)
        } else {
            None
        }
    } else {
        None
    }
}

```

`pdbtool/src/copy.rs`:

```rs
use ms_pdb::Pdb;
use std::io::Write;
use std::path::PathBuf;

#[derive(clap::Parser)]
pub struct Options {
    /// The PDB to read.
    source_pdb: PathBuf,

    /// The PDB to write.
    dest_pdb: PathBuf,
}

pub fn copy_command(options: &Options) -> anyhow::Result<()> {
    let src = Pdb::open(&options.source_pdb)?;
    let mut dst = ms_pdb::msf::Msf::create(&options.dest_pdb, Default::default())?;

    for stream_index in 1..src.num_streams() {
        if src.is_stream_valid(stream_index) {
            let stream_data = src.read_stream_to_vec(stream_index)?;
            let (_, mut s) = dst.new_stream()?;
            s.write_all(&stream_data)?;
        } else {
            let dst_stream_index = dst.nil_stream()?;
            assert_eq!(dst_stream_index, stream_index);
        }
    }

    dst.commit()?;

    Ok(())
}

```

`pdbtool/src/counts.rs`:

```rs
use anyhow::Result;
use ms_pdb::msf::offset_within_page;
use ms_pdb::syms::{SymIter, SymKind};
use ms_pdb::tpi::TypeStreamHeader;
use ms_pdb::types::{Leaf, TypesIter};
use ms_pdb::{Pdb, Stream};
use std::collections::{BTreeMap, HashMap};
use std::io::Read;
use std::mem::size_of;
use zerocopy::{FromZeros, IntoBytes};

/// Counts the number of records and record sizes for a given set of PDBs.
#[derive(clap::Parser)]
pub struct CountsOptions {
    /// The set of PDBs to read.
    #[command(flatten)]
    pdbs: crate::glob_pdbs::PdbList,

    /// Count type records in the Global Symbol Stream.
    #[arg(long)]
    global_symbols: bool,

    /// Count symbol records in the TPI Stream.
    #[arg(long)]
    tpi: bool,

    /// Count symbol records in the IPI Stream.
    #[arg(long)]
    ipi: bool,

    /// Count symbol records in each module symbol stream.
    #[arg(long)]
    module_symbols: bool,
}

#[derive(Default)]
struct Counts {
    ipi: TypeStreamCounts,
    tpi: TypeStreamCounts,
    module_sym_counts: HashMap<SymKind, PerRecord>,

    module_sym_sizes: Vec<(u64, u32)>, // (byte_size, module_index)
    global_syms_counts: HashMap<SymKind, PerRecord>,

    num_pdbs_failed: u32,

    sc: StreamCounts,
}

#[derive(Default)]
struct StreamCounts {
    total_file_size: u64,

    tpi: u64,
    tpi_hash: u64,
    ipi: u64,
    ipi_hash: u64,
    gsi: u64,
    psi: u64,

    named: BTreeMap<String, u64>,

    dbi: u64,
    dbi_contribs: u64,
    dbi_modules: u64,
    dbi_sources: u64,

    pdbi: u64,

    /// Size in bytes of all module streams combined
    modules: u64,

    modules_c13_lines: u64,
    modules_syms: u64,

    /// Size in bytes of GSS
    gss: u64,

    old_stream_dir: u64,

    /// Fragmentation in the last page of streams.
    stream_frag: u64,

    /// Number of bytes in pages that are free.
    free_pages_bytes: u64,
}

#[derive(Default)]
struct TypeStreamCounts {
    records: HashMap<Leaf, PerRecord>,
}

pub fn counts_command(options: CountsOptions) -> Result<()> {
    let mut counts = Counts::default();

    for file_name in options.pdbs.get_paths()? {
        match ms_pdb::Pdb::open(&file_name) {
            Ok(pdb) => match count_one_pdb(&options, &pdb, &mut counts) {
                Ok(()) => {}
                Err(e) => {
                    eprintln!(
                        "Error occurred while processing PDB: {}\n  {}",
                        file_name.display(),
                        e
                    );
                    counts.num_pdbs_failed += 1;
                }
            },
            Err(e) => {
                eprintln!("Failed to open {} : {}", file_name.display(), e);
                counts.num_pdbs_failed += 1;
            }
        }
    }

    show_counts(&mut counts);

    Ok(())
}

// Count records in global symbol stream
fn count_global_symbols(pdb: &Pdb, counts: &mut Counts) -> anyhow::Result<()> {
    let global_syms_stream = pdb.dbi_header().sym_record_stream()?;
    let global_syms_stream_data = pdb.read_stream_to_vec(global_syms_stream)?;
    count_sym_records(&global_syms_stream_data, &mut counts.global_syms_counts);
    Ok(())
}

fn count_one_pdb(options: &CountsOptions, pdb: &Pdb, counts: &mut Counts) -> Result<()> {
    if options.tpi {
        read_and_count_type_records(pdb, &mut counts.tpi, Stream::TPI)?;
    }
    if options.ipi {
        read_and_count_type_records(pdb, &mut counts.ipi, Stream::IPI)?;
    }

    if let Some(msf) = pdb.msf() {
        // TODO: Do something smarter for PDZ.
        counts.sc.total_file_size += msf.nominal_size();
    }

    let modules_substream = pdb.read_modules()?;
    for (module_index, module) in modules_substream.iter().enumerate() {
        if let Some(module_stream) = module.stream() {
            if options.module_symbols {
                let module_sym_size: u64;
                if let Some(modi) = pdb.read_module_stream(&module)? {
                    count_sym_records(modi.sym_data()?, &mut counts.module_sym_counts);
                    module_sym_size = modi.sym_byte_size as u64;
                } else {
                    module_sym_size = 0;
                }
                counts
                    .module_sym_sizes
                    .push((module_sym_size, module_index as u32));
            }

            // counts.module_infos.push(module);
            counts.sc.modules += pdb.stream_len(module_stream);
            counts.sc.modules_c13_lines += module.header().c13_byte_size.get() as u64;
            counts.sc.modules_syms += module.header().sym_byte_size.get() as u64;
        }
    }

    counts.sc.dbi += pdb.stream_len(Stream::DBI.into());
    counts.sc.dbi_contribs += pdb.dbi_header().section_contribution_size.get() as u64;
    counts.sc.dbi_modules += pdb.dbi_header().mod_info_size.get() as u64;
    counts.sc.dbi_sources += pdb.dbi_header().source_info_size.get() as u64;

    counts.sc.pdbi += pdb.stream_len(Stream::PDB.into());

    // TPI
    {
        let tpi_len = pdb.stream_len(Stream::TPI.into());
        counts.sc.tpi += tpi_len;
        if tpi_len as usize >= size_of::<TypeStreamHeader>() {
            let mut header = TypeStreamHeader::new_zeroed();
            let mut reader = pdb.get_stream_reader(Stream::TPI.into())?;
            reader.read_exact(header.as_mut_bytes())?;

            if let Some(s) = header.hash_aux_stream_index.get() {
                // TODO: yes, yes, I know, it's the wrong stream count
                counts.sc.tpi_hash += pdb.stream_len(s);
            }

            if let Some(s) = header.hash_stream_index.get() {
                counts.sc.tpi_hash += pdb.stream_len(s);
            }
        }
    }

    // IPI
    {
        let ipi_len = pdb.stream_len(Stream::IPI.into());
        counts.sc.ipi += ipi_len;
        if ipi_len as usize >= size_of::<TypeStreamHeader>() {
            let mut header = TypeStreamHeader::new_zeroed();
            let mut reader = pdb.get_stream_reader(Stream::IPI.into())?;
            reader.read_exact(header.as_mut_bytes())?;

            if let Some(s) = header.hash_aux_stream_index.get() {
                // TODO: yes, yes, I know, it's the wrong stream count
                counts.sc.ipi_hash += pdb.stream_len(s);
            }

            if let Some(s) = header.hash_stream_index.get() {
                counts.sc.ipi_hash += pdb.stream_len(s);
            }
        }
    }

    if let Ok(gss) = pdb.dbi_header().sym_record_stream() {
        counts.sc.gss += pdb.stream_len(gss);
    }

    if let Ok(gsi) = pdb.dbi_header().global_stream_index() {
        counts.sc.gsi += pdb.stream_len(gsi);
    }

    if let Ok(psi) = pdb.dbi_header().public_stream_index() {
        counts.sc.psi += pdb.stream_len(psi);
    }

    if options.global_symbols {
        count_global_symbols(pdb, counts)?;
    }

    for (name, stream) in pdb.named_streams().iter() {
        let stream_len = pdb.stream_len(*stream);

        let name = name.to_ascii_lowercase();

        let chopped_name: &str = if name.ends_with(".cs") {
            "*.cs"
        } else if name.ends_with(".cpp") || name.ends_with(".CPP") {
            "*.cpp"
        } else if name.ends_with(".natvis") {
            "*.natvis"
        } else if name.ends_with(".xaml") {
            "*.xaml"
        } else {
            &name
        };

        if let Some(slot) = counts.sc.named.get_mut(chopped_name) {
            *slot += stream_len;
        } else {
            counts.sc.named.insert(chopped_name.to_string(), stream_len);
        }
    }

    counts.sc.old_stream_dir += pdb.stream_len(Stream::OLD_STREAM_DIR.into());

    // Count the space wasted due to fragmentation in the final page of streams.
    if let Some(msf) = pdb.msf() {
        let page_size = msf.page_size();
        for i in 1..msf.num_streams() {
            let stream_size_bytes = msf.stream_size(i);
            let stream_size_phase = offset_within_page(stream_size_bytes, page_size);
            if stream_size_phase != 0 {
                counts.sc.stream_frag += (u32::from(page_size) - stream_size_phase) as u64;
            }
        }

        let num_free_pages = msf.num_free_pages();
        counts.sc.free_pages_bytes += (num_free_pages as u64) << page_size.exponent();
    }

    Ok(())
}

#[derive(Default, Clone)]
struct PerRecord {
    count: u32,
    bytes: u32,
}

fn read_and_count_type_records(
    pdb: &Pdb,
    counts: &mut TypeStreamCounts,
    stream: Stream,
) -> Result<()> {
    let tpi_type_stream = pdb.read_tpi_or_ipi_stream(stream)?;
    count_type_records(tpi_type_stream.type_records_bytes(), counts);
    Ok(())
}

fn count_type_records(type_records: &[u8], counts: &mut TypeStreamCounts) {
    for type_record in TypesIter::new(type_records) {
        let per_record = counts.records.entry(type_record.kind).or_default();
        per_record.count += 1;
        per_record.bytes += type_record.data.len() as u32 + 4; // 4 for the header
    }
}

fn count_sym_records(sym_records: &[u8], counts: &mut HashMap<SymKind, PerRecord>) {
    for sym_record in SymIter::new(sym_records) {
        let per_record = counts.entry(sym_record.kind).or_default();
        per_record.count += 1;
        per_record.bytes += sym_record.data.len() as u32 + 4; // 4 for the header
    }
}

fn dump_type_counts_map(record_counts: &TypeStreamCounts) {
    let mut record_counts_vec: Vec<(Leaf, PerRecord)> = record_counts
        .records
        .iter()
        .map(|(&kind, per_record)| (kind, per_record.clone()))
        .collect();
    record_counts_vec.sort_unstable_by_key(|&(key, _)| key);

    println!("    {:>8}  {:>12}", "records", "bytes");
    println!("    {:>8}  {:>12}", "-------", "-----");
    let mut total_count = 0;
    let mut total_bytes = 0;
    for &(kind, ref per_record) in record_counts_vec.iter() {
        let raw_kind = kind.0;
        let count = per_record.count;
        let bytes = per_record.bytes;
        println!("    {count:8}  {bytes:12} : [{raw_kind:04x}] {kind:?}");
        total_count += count;
        total_bytes += bytes;
    }

    println!("    {total_count:8}  {total_bytes:12} : (total)");
}

fn dump_sym_counts_map(record_counts: &HashMap<SymKind, PerRecord>) {
    let mut record_counts_vec: Vec<(SymKind, PerRecord)> = record_counts
        .iter()
        .map(|(&kind, per_record)| (kind, per_record.clone()))
        .collect();
    record_counts_vec.sort_unstable_by_key(|&(key, _)| key);

    println!("    {:>8}  {:>12}", "records", "bytes");
    println!("    {:>8}  {:>12}", "-------", "-----");
    let mut total_count = 0;
    let mut total_bytes = 0;
    for &(kind, ref per_record) in record_counts_vec.iter() {
        let raw_kind = kind.0;
        let count = per_record.count;
        let bytes = per_record.bytes;
        println!("    {count:8}  {bytes:12} : [{raw_kind:04x}] {kind:?}");
        total_count += count;
        total_bytes += bytes;
    }
    println!("    {total_count:8}  {total_bytes:12} : (total)");
}

fn show_counts(counts: &mut Counts) {
    println!("TPI Stream:");
    dump_type_counts_map(&counts.tpi);

    println!();

    println!("IPI Stream:");
    dump_type_counts_map(&counts.ipi);

    let module_sym_counts = &counts.module_sym_counts;

    println!("Record counts for module symbols (all modules):");
    dump_sym_counts_map(module_sym_counts);

    println!();

    counts
        .module_sym_sizes
        .sort_unstable_by_key(|(size, _)| *size);

    println!();

    println!("Record counts for Global Symbol Stream:");
    dump_sym_counts_map(&counts.global_syms_counts);

    println!();

    if !counts.module_sym_sizes.is_empty() {
        println!();

        let module_sym_size_percentile5 =
            counts.module_sym_sizes[counts.module_sym_sizes.len() * 5 / 100].0;
        let module_sym_size_median = counts.module_sym_sizes[counts.module_sym_sizes.len() / 2].0;
        let module_sym_size_percentile95 =
            counts.module_sym_sizes[counts.module_sym_sizes.len() * 95 / 100].0;
        println!("Number of modules: {}", counts.module_sym_sizes.len());
        println!("Module symbol stream sizes:");
        println!("    percentile  5%  : {:8}", module_sym_size_percentile5);
        println!("    percentile 50%  : {:8}", module_sym_size_median);
        println!("    percentile 95%  : {:8}", module_sym_size_percentile95);
        println!();
    }

    println!();
    println!(
        "Total size of all PDB files:       {}",
        friendly::bytes(counts.sc.total_file_size)
    );
    let pct = |size: u64| -> Percent { Percent(size as f64, counts.sc.total_file_size as f64) };

    let sc = &counts.sc;
    let total_named_stream_size: u64 = sc.named.values().copied().sum();

    let accounted_bytes = sc.tpi
        + sc.tpi_hash
        + sc.ipi
        + sc.ipi_hash
        + sc.gsi
        + sc.psi
        + total_named_stream_size
        + sc.pdbi
        + sc.modules
        + sc.gss
        + sc.old_stream_dir
        + sc.stream_frag
        + sc.free_pages_bytes;

    let unaccounted_bytes = sc.total_file_size - accounted_bytes;

    let show_one = |name: &str, size: u64| {
        println!(
            "    {:-20}     : {}, {}",
            name,
            friendly::bytes(size),
            pct(size)
        );
    };

    let show_level2 = |name: &str, size: u64| {
        println!(
            "        {:-20} : {}, {}",
            name,
            friendly::bytes(size),
            pct(size)
        );
    };

    show_one("PDBI streams", sc.pdbi);

    show_one("DBI streams", sc.dbi);
    show_level2("DBI Contribs", sc.dbi_contribs);
    show_level2("DBI Modules", sc.dbi_modules);
    show_level2("DBI Sources", sc.dbi_sources);

    show_one("TPI streams", sc.tpi);
    show_one("TPI hash streams", sc.tpi_hash);
    show_one("IPI streams", sc.ipi);
    show_one("IPI hash streams", sc.ipi_hash);

    show_one("Module streams", sc.modules);
    show_level2("Module symbols", sc.modules_syms);
    show_level2("Module line data", sc.modules_c13_lines);

    show_one("GSS streams", sc.gss);
    show_one("GSI streams", sc.gsi);
    show_one("PSI streams", sc.psi);

    show_one("Named streams", total_named_stream_size);
    for (name, count) in sc.named.iter() {
        show_level2(name, *count);
    }

    show_one("Old Stream Dir", sc.old_stream_dir);
    show_one("Page fragmentation", sc.stream_frag);
    show_one("Free pages", sc.free_pages_bytes);
    show_one("Unaccounted", unaccounted_bytes);
}

struct Percent(pub f64, pub f64);

impl std::fmt::Display for Percent {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        if self.1 > 0.0 {
            let pct = self.0 / self.1 * 100.0;
            write!(f, "{pct:2.1} %")
        } else {
            write!(f, "n/a")
        }
    }
}

```

`pdbtool/src/dump.rs`:

```rs
use crate::dump_utils::{HexDump, HexStr};
use anyhow::Result;
use ms_pdb::dbi::optional_dbg::OptionalDebugHeaderStream;
use ms_pdb::dbi::{DbiSourcesSubstream, DbiStream, ModuleInfo};
use ms_pdb::names::NamesStream;
use ms_pdb::parser::Parser;
use ms_pdb::syms::{SymIter, SymKind};
use ms_pdb::tpi::TypeStreamKind;
use ms_pdb::types::TypeIndex;
use ms_pdb::utils::iter::IteratorWithRangesExt;
use ms_pdb::{Pdb, Stream};
use std::fmt::Write;
use std::ops::Range;
use std::path::Path;
use tracing::error;

use self::sym::DumpSymsContext;
use self::types::dump_type_index_short;

mod lines;
mod names;
mod sources;
mod streams;
pub mod sym;
mod types;

#[derive(clap::Parser)]
pub struct DumpOptions {
    /// The PDB to dump
    pub pdb: String,

    #[arg(long)]
    pub lines_like_cvdump: bool,

    #[command(subcommand)]
    pub subcommand: Subcommand,
}

#[derive(clap::Subcommand)]
pub enum Subcommand {
    Names(names::DumpNamesOptions),
    Globals {
        max: Option<usize>,
        skip: Option<usize>,
    },
    /// Dump the Type Stream (TPI)
    Tpi(types::DumpTypeStreamOptions),
    /// Dump the Id Stream (TPI)
    Ipi(types::DumpTypeStreamOptions),

    /// Dump DBI header
    Dbi,

    /// Dump DBI Edit-and-Continue Substream
    DbiEnc,

    DbiTypeServerMap,

    /// Global Symbol Index. Loads the GSI and iterates through its hash records. For each one,
    /// finds the symbol record in the GSS and displays it.
    Gsi,
    /// Public Symbol Index. Loads the PSI and iterates through its hash records. For each one,
    /// finds the symbol record in the GSS and displays it.
    Psi,

    Modules(ModulesOptions),

    /// Dump the Stream Directory.
    Streams(streams::StreamsOptions),

    Lines(lines::LinesOptions),

    /// Dump the DBI Stream - Sources substream
    Sources(sources::SourcesOptions),

    SectionMap,

    /// Dump section contributions (quite large!)
    SectionContribs,

    /// Dump the PDB Info Stream
    Pdbi,

    ModuleSymbols(sym::DumpModuleSymbols),

    /// Dump the contents of a stream, or a subsection of it, using a hexadecimal dump format.
    /// By default, this will only show a portion of the stream; use `--len` to increase it.
    Hex {
        stream: String,
        #[arg(long)]
        offset: Option<String>,
        #[arg(long)]
        len: Option<String>,
    },
}

#[derive(clap::Parser)]
pub struct ModulesOptions {
    /// Filter results to those where the object name matches this regex.
    #[arg(long)]
    pub obj: Option<String>,

    /// Display a specific module by module number. This value is zero-based.
    pub module: Option<u32>,
}

pub fn dump_main(options: DumpOptions) -> anyhow::Result<()> {
    let p = ms_pdb::Pdb::open(Path::new(&options.pdb))?;

    let dbi_stream = p.read_dbi_stream()?;

    if options.lines_like_cvdump {
        lines::dump_lines_like_cvdump(&p, &dbi_stream)?;
    }

    match options.subcommand {
        Subcommand::Globals { skip, max } => {
            sym::dump_globals(&p, skip, max, false)?;
        }

        Subcommand::Names(args) => {
            names::dump_names(&p, args)?;
        }

        Subcommand::Dbi => {
            dump_dbi(&p)?;
        }

        Subcommand::DbiEnc => {
            let enc = dbi_stream.edit_and_continue();
            println!("Edit-and-Continue substream:");
            if enc.is_empty() {
                println!("(empty)");
            } else {
                println!("{}", HexDump::new(enc).max(0x1000));
                let enc_names = NamesStream::parse(enc)?;
                for (range, name) in enc_names.iter().with_ranges() {
                    println!("[{:08x}] {}", range.start, name);
                }
            }
        }

        Subcommand::DbiTypeServerMap => {
            let tsm = dbi_stream.type_server_map();
            if tsm.is_empty() {
                println!("(empty)");
            } else {
                println!("{:?}", HexDump::new(tsm).max(0x1000));
            }
        }

        Subcommand::Pdbi => dump_pdbi(&p)?,

        Subcommand::ModuleSymbols(args) => sym::dump_module_symbols(&p, args)?,

        Subcommand::Tpi(opts) => {
            let type_stream = p.read_type_stream()?;
            let id_stream = p.read_ipi_stream()?;
            let type_dump_syms_context = DumpSymsContext::new(&type_stream, &id_stream);
            types::dump_type_stream(
                TypeStreamKind::TPI,
                &type_stream,
                &mut |out, t| dump_type_index_short(out, &type_dump_syms_context, t),
                &mut |_out, _id| Ok(()),
                None,
                &opts,
            )?;
        }

        Subcommand::Ipi(opts) => {
            let type_stream = p.read_type_stream()?;
            let id_stream = p.read_ipi_stream()?;
            let type_dump_syms_context = DumpSymsContext::new(&type_stream, &id_stream);
            let id_dump_syms_context = DumpSymsContext::new(&id_stream, &id_stream); // TODO: not even remotely right

            let names = p.names()?;
            types::dump_type_stream(
                TypeStreamKind::IPI,
                &id_stream,
                &mut |out, t| dump_type_index_short(out, &type_dump_syms_context, t),
                &mut |out, id| dump_type_index_short(out, &id_dump_syms_context, TypeIndex(id)),
                Some(names),
                &opts,
            )?;
        }
        Subcommand::Gsi => sym::dump_gsi(&p)?,
        Subcommand::Psi => sym::dump_psi(&p)?,
        Subcommand::Lines(args) => lines::dump_lines(args, &p, &dbi_stream)?,
        Subcommand::Streams(args) => streams::dump_streams(&p, args)?,
        Subcommand::Modules(args) => dump_modules(&p, &dbi_stream, args)?,
        Subcommand::Sources(args) => sources::dump_dbi_sources(&dbi_stream, args)?,
        Subcommand::SectionContribs => dump_section_contribs(&dbi_stream)?,
        Subcommand::SectionMap => dump_section_map(&p, &dbi_stream)?,

        Subcommand::Hex {
            stream,
            offset,
            len,
        } => {
            let mut offset = if let Some(offset_str) = &offset {
                str_to_u32(offset_str)? as usize
            } else {
                0
            };

            let mut len = if let Some(len_str) = &len {
                str_to_u32(len_str)? as usize
            } else {
                0x200
            };

            let (stream_index, stream_range_opt) = crate::save::get_stream_index(&p, &stream)?;
            let stream_data = p.read_stream_to_vec(stream_index)?;

            if let Some(r) = stream_range_opt {
                println!("range = {r:?}");
                len = len.min(r.len());
                offset += r.start;
            };

            if let Some(bytes) = stream_data.get(offset..) {
                println!("{:?}", HexDump::new(bytes).max(len).at(offset).header(true));
            } else {
                println!("Offset 0x{offset:x} ({offset}) is out of range for the stream size.");
                println!("Stream length: 0x{len:x} ({len}).", len = stream_data.len());
            }
        }
    }

    Ok(())
}

fn dump_section_contribs(dbi_stream: &DbiStream<Vec<u8>>) -> anyhow::Result<()> {
    println!("*** SECTION CONTRIBUTIONS");
    println!();

    println!("  Imod  Address        Size      Characteristics");

    let section_contribs = dbi_stream.section_contributions()?;
    for contrib in section_contribs.contribs.iter() {
        println!(
            "  {:04X} {:04X}:{:08X}  {:08X}  {:08X}",
            contrib.module_index.get() + 1,
            contrib.section.get(),
            contrib.offset.get(),
            contrib.size.get(),
            contrib.characteristics.get()
        );
    }

    Ok(())
}

fn dump_pdbi(pdb: &Pdb) -> Result<()> {
    let pdbi = pdb.pdbi();

    let binding_key = pdbi.binding_key();
    println!("PDBI version: 0x{0:08x}  {0}", pdbi.version());
    println!();
    println!("Binding key:");
    println!("    Unique ID: {}", binding_key.guid.braced());
    println!("    Age: {}", binding_key.age);
    println!(
        "    symsrv file.ptr path: {:?}{}",
        HexStr::new(binding_key.guid.as_bytes()).packed(),
        binding_key.age.wrapping_sub(1)
    );

    Ok(())
}

fn dump_modules(pdb: &Pdb, dbi: &DbiStream, args: ModulesOptions) -> Result<()> {
    let modules = dbi.modules();

    let mut num_modules: u32 = 0;

    let obj_rx = if let Some(obj_filter) = args.obj.as_ref() {
        Some(regex::bytes::Regex::new(obj_filter)?)
    } else {
        None
    };

    let modules_records_start = dbi.substreams.modules_bytes.start;

    for (module_index, (module_record_range, module)) in modules.iter().with_ranges().enumerate() {
        if let Some(mi) = args.module {
            if module_index != mi as usize {
                continue;
            }
        }

        if let Some(obj_rx) = &obj_rx {
            if !obj_rx.is_match(module.obj_file()) {
                continue;
            }
        }

        println!("Module #{} : {}", module_index, module.module_name());
        println!(
            "    [{:08x} .. {:08x}] Module Info record in DBI Stream",
            modules_records_start + module_record_range.start,
            modules_records_start + module_record_range.end
        );
        println!("    {}", module.obj_file());
        if let Some(stream) = module.stream() {
            println!("    Stream: {}", stream);
            let sym_start = 0;
            let sym_byte_size = module.header().sym_byte_size.get();
            let sym_end = sym_byte_size;
            let c11_byte_size = module.header().c11_byte_size.get();
            let c11_start = sym_byte_size;
            let c11_end = sym_end + c11_byte_size;
            let c13_byte_size = module.header().c13_byte_size.get();
            let c13_start = sym_byte_size + c11_byte_size;
            let c13_end = c13_start + c13_byte_size;
            println!("        [{sym_start:08x} .. {sym_end:08x}] module symbols");
            if c11_byte_size != 0 {
                println!("        [{c11_start:08x} .. {c11_end:08x}] c11 line data");
            }
            if c13_byte_size != 0 {
                println!("        [{c13_start:08x} .. {c13_end:08x}] c13 line data");
            }
            let sym_stream_len = pdb.stream_len(stream);
            if sym_stream_len > c13_byte_size as u64 {
                println!("        [{c13_end:08x} .. {sym_stream_len:08x}] global refs");
            }
        } else {
            println!("    Stream: (none)");
        }

        let h = module.header();
        println!(
            "    section_contr.module_index: {}",
            h.section_contrib.module_index.get()
        );
        println!(
            "    pdb_file_path_name_index: {}",
            h.pdb_file_path_name_index.get()
        );
        println!("    source_file_count: {}", h.source_file_count.get());
        println!(
            "    source_file_name_index: {}",
            h.source_file_name_index.get()
        );

        if h.unused1.get() != 0 {
            println!("    unused1: 0x{0:08x} {0:10}", h.unused1.get());
        }
        if h.unused2.get() != 0 {
            println!("    unused2: 0x{0:08x} {0:10}", h.unused2.get());
        }
        println!();

        num_modules += 1;
    }

    println!("Number of modules found: {num_modules}");

    Ok(())
}

fn dump_section_map(_p: &Pdb, dbi_stream: &DbiStream) -> Result<()> {
    use ms_pdb::dbi::section_map::SectionMapEntryFlags;

    let section_map = dbi_stream.section_map()?;

    println!(
        "Number of entries in section map: {}",
        section_map.entries.len()
    );

    for (i, entry) in section_map.entries.iter().enumerate() {
        println!(
            "  {:6} : section_name {:04x}, class_name {:04x}, offset {:08x}, length {:08x}, flags {:04x} {:?}",
            i,
            entry.section_name.get(),
            entry.class_name.get(),
            entry.offset.get(),
            entry.section_length.get(),
            entry.flags.get(),
            SectionMapEntryFlags::from_bits_truncate(entry.flags.get())
        );
    }

    Ok(())
}

fn str_to_u32(s: &str) -> anyhow::Result<u32> {
    if let Some(after) = s.strip_prefix("0x") {
        Ok(u32::from_str_radix(after, 16)?)
    } else {
        Ok(s.parse()?)
    }
}

fn dump_dbi(pdb: &Pdb) -> Result<()> {
    let header = pdb.dbi_header();

    println!("Signature: 0x{:08x}", header.signature.get());
    println!(
        "Version:   0x{version:08x}  {version}",
        version = header.version.get()
    );
    println!("Age:       0x{age:08x}  {age}", age = header.age.get());
    println!();
    println!("Global Symbols:");
    println!(
        "    Global Symbol Stream (GSS):  {:?}",
        header.global_symbol_stream.get()
    );
    println!(
        "    Global Symbol Index Stream (GSI): {:?}",
        header.global_symbol_index_stream.get()
    );
    println!(
        "    Public Symbol Index Stream (PSI): {:?}",
        header.public_symbol_index_stream.get()
    );

    println!("Substreams:");

    let subs = pdb.dbi_substreams();

    let show_sub = |range: &Range<usize>, name: &str| {
        println!(
            "    [{:08x} .. {:08x}] size 0x{:08x} : {name}",
            range.start,
            range.end,
            range.len()
        );
    };

    show_sub(&subs.modules_bytes, "Modules");
    show_sub(&subs.section_contributions_bytes, "Section Contributions");
    show_sub(&subs.section_map_bytes, "Section Map");
    show_sub(&subs.source_info, "Sources");
    show_sub(&subs.type_server_map, "Type Server Map");
    show_sub(&subs.optional_debug_header_bytes, "Optional Debug Headers");
    show_sub(&subs.edit_and_continue, "Edit-and-Continue");

    Ok(())
}

```

`pdbtool/src/dump/lines.rs`:

```rs
use super::*;
use crate::dump_utils::HexStr;
use ms_pdb::lines::{FileChecksum, FileChecksumsSubsection, LinesSubsection, SubsectionKind};
use std::collections::HashMap;

/// Dumps C13 Line Data for a given module.
#[derive(clap::Parser)]
pub struct LinesOptions {
    /// The module index of the module to dump.
    #[arg(long)]
    pub module: Option<usize>,
}

fn dump_module_lines(
    pdb: &Pdb,
    module_index: usize,
    module: &ModuleInfo,
    names: &ms_pdb::names::NamesStream<Vec<u8>>,
    sources: &ms_pdb::dbi::sources::DbiSourcesSubstream,
) -> Result<()> {
    println!("Module: {}", module.module_name());
    println!("    Obj file: {}", module.obj_file());

    if let Some(module_stream) = module.stream() {
        println!("    Stream: {}", module_stream,);
    } else {
        println!("    Stream: (none)");
        return Ok(());
    };

    if module.header().c11_byte_size.get() != 0 {
        println!("    *** Module has obsolete C11 line data, which is not supported ***");
        return Ok(());
    }

    if module.header().c13_byte_size.get() == 0 {
        println!("    Module has no line data.");
        return Ok(());
    }

    // unwrap() is ok because we tested module.stream() above.
    let module_stream = pdb.read_module_stream(module)?.unwrap();
    let c13_stream_offset = module_stream.c13_line_data_range().start as u32;
    let c13_line_data = module_stream.c13_line_data();

    let mut checksums: HashMap<u32, FileChecksum<'_>> = HashMap::new();

    if let Some(checksums_subsection) = c13_line_data.find_checksums() {
        for (range, checksum) in checksums_subsection.iter().with_ranges() {
            checksums.insert(range.start as u32, checksum);
        }
    }

    println!();

    let mut iter = c13_line_data.subsections().with_ranges();
    for (subsection_range, subsection) in iter.by_ref() {
        println!(
            "[{:08x}] Subsection: {:?}, len {}",
            c13_stream_offset + subsection_range.start as u32,
            subsection.kind,
            subsection.data.len()
        );

        match subsection.kind {
            SubsectionKind::LINES => {
                let contribution = ms_pdb::lines::LinesSubsection::parse(subsection.data)?;
                println!(
                    "    contribution: offset 0x{:x}, segment {}, size {}",
                    contribution.contribution.contribution_offset,
                    contribution.contribution.contribution_segment,
                    contribution.contribution.contribution_size
                );

                for block in contribution.blocks() {
                    println!(
                        "        block: file {}, num_lines {}",
                        block.header.file_index, block.header.num_lines
                    );
                    if let Some(checksum) = checksums.get(&block.header.file_index.get()) {
                        let name = names.get_string(checksum.name())?;
                        println!("            file: {}", name);
                    } else {
                        println!(
                            "            file: unknown: {}",
                            block.header.file_index.get()
                        );
                    }

                    print!("            lines: ");
                    for (i, line) in block.lines().iter().enumerate() {
                        if i != 0 {
                            print!(", ");
                        }

                        let line_num_start = line.line_num_start();
                        if ms_pdb::lines::is_jmc_line(line_num_start) {
                            print!("<no-step>");
                        } else {
                            print!("{}", line_num_start);
                        }
                    }
                    println!();
                }
            }

            SubsectionKind::FILE_CHECKSUMS => {
                let name_offsets_for_module = if module_index < sources.num_modules() {
                    sources.name_offsets_for_module(module_index)?
                } else {
                    &[]
                };

                let checksums = ms_pdb::lines::FileChecksumsSubsection {
                    bytes: subsection.data,
                };

                for (i, checksum) in checksums.iter().enumerate() {
                    let name = names.get_string(checksum.name())?;

                    println!(
                        "  checksum: file_offset {:08x}, kind {:?} : {:?} : {name}",
                        checksum.header.name.get(),
                        checksum.header.checksum_kind,
                        HexStr::new(checksum.checksum_data).packed()
                    );

                    if let Some(&name_offset) = name_offsets_for_module.get(i) {
                        let name2 = sources.get_source_file_name_at(name_offset.get())?;
                        if name != name2 {
                            println!("    different name: {}", name2);
                        }
                    } else {
                        println!("    index is out of range");
                    }
                }
            }

            _ => {
                println!("{:?}", HexDump::new(subsection.data).max(0x200));
            }
        }

        println!();
    }

    if !iter.inner().rest().is_empty() {
        println!();
        println!("Found unparsed data at the end:");
        println!("{:?}", HexDump::new(iter.inner().rest()).at(iter.pos()));
    }

    Ok(())
}

pub fn dump_lines(options: LinesOptions, p: &Pdb, dbi_stream: &DbiStream<Vec<u8>>) -> Result<()> {
    let names = p.names()?;
    let sources = dbi_stream.sources()?;

    if let Some(module_index) = options.module {
        if let Some(module) = dbi_stream.modules().iter().nth(module_index) {
            dump_module_lines(p, module_index, &module, names, &sources)?;
        } else {
            println!("There is no module with the requested index.");
        }
    } else {
        for (module_index, module) in dbi_stream.modules().iter().enumerate().take(20) {
            dump_module_lines(p, module_index, &module, names, &sources)?;
        }
    }

    Ok(())
}

pub fn dump_lines_like_cvdump(p: &Pdb, dbi_stream: &DbiStream<Vec<u8>>) -> Result<()> {
    let names_stream = p.names()?;

    println!("*** LINES");
    println!();

    for module in dbi_stream.modules().iter() {
        if module.module_name() == module.obj_file() {
            println!("** Module: \"{}\"", module.module_name());
        } else {
            println!(
                "** Module: \"{}\" from \"{}\"",
                module.module_name(),
                module.obj_file()
            );
        }
        println!();

        let Some(module_stream) = p.read_module_stream(&module)? else {
            continue;
        };
        let line_data = module_stream.c13_line_data();

        // Find the Checksums subsection. There should be at most one.
        let checksums_subsection_data = line_data.find_checksums_bytes();
        let checksums = if let Some(chk) = &checksums_subsection_data {
            Some(FileChecksumsSubsection { bytes: chk })
        } else {
            None
        };

        for subsection in line_data.subsections() {
            match subsection.kind {
                SubsectionKind::LINES => {
                    let lines = LinesSubsection::parse(subsection.data)?;
                    let contribution_offset = lines.contribution.contribution_offset.get();

                    for block in lines.blocks() {
                        if let Some(checksums) = &checksums {
                            if let Ok(f) = checksums.get_file(block.header.file_index.get()) {
                                let file_name = names_stream.get_string(f.name())?;
                                println!(
                                    "  {file_name} ({:?}: {:?})",
                                    f.header.checksum_kind,
                                    HexStr::new(f.checksum_data).packed()
                                );
                            } else {
                                println!("warning: failed to get file name");
                            }
                            println!();

                            const NUM_COLUMNS: usize = 4;
                            let mut column = 0;

                            for line in block.lines() {
                                print!(
                                    " {:6} {:08X}",
                                    line.line_num_start(),
                                    contribution_offset + line.offset.get()
                                );

                                column += 1;
                                if column == NUM_COLUMNS {
                                    println!();
                                    column = 0;
                                }
                            }
                            if column != 0 {
                                println!();
                            }
                            println!();
                        } else {
                            println!("warning: This module has no file checksums!");
                        }
                    }
                }

                _ => {}
            }
        }
    }

    Ok(())
}

```

`pdbtool/src/dump/names.rs`:

```rs
use super::*;
use ms_pdb::hash;
use ms_pdb::names::NameIndex;
use ms_pdb::utils::iter::IteratorWithRangesExt;

#[derive(clap::Parser)]
pub struct DumpNamesOptions {
    #[arg(long)]
    max: Option<usize>,

    /// Show hex offsets (NameIndex) for each string.
    #[arg(long)]
    show_offsets: bool,

    /// Show the contents of the name hash table
    #[arg(long)]
    show_hashes: bool,
}

pub fn dump_names(pdb: &Pdb, options: DumpNamesOptions) -> anyhow::Result<()> {
    let names_stream = pdb.names()?;
    let names_stream_index = pdb.named_stream_err(ms_pdb::names::NAMES_STREAM_NAME)?;
    println!("Names Stream Index: {names_stream_index}");

    println!(
        "Number of names in table (as declared in the stream): {:6}",
        names_stream.num_strings
    );
    println!(
        "Number of hash entries:                               {:6}",
        names_stream.num_hashes
    );

    println!();
    println!("Strings:");
    println!();

    for (i, (range, name)) in names_stream.iter().with_ranges().enumerate() {
        if let Some(max) = options.max {
            if i >= max {
                println!("(stopping because we reached max)");
                break;
            }
        }

        if options.show_offsets {
            println!("[{:08x}] {name:?}", range.start);
        } else {
            println!("{name:?}");
        }
    }

    if options.show_hashes {
        println!();
        println!("Hash buckets:");
        println!();

        let hashes = names_stream.hashes();
        let mut num_hashes_good: usize = 0;
        let mut num_hashes_bad: usize = 0;
        let mut num_hashes_unused: usize = 0;
        let mut probing_hash_base: u32 = 0;

        for (i, &ni) in hashes.iter().enumerate() {
            if ni.get() == 0 {
                println!("  hash 0x{:08x} : none", i);
                num_hashes_unused += 1;
                probing_hash_base = i as u32 + 1;
                continue;
            }

            let s = names_stream.get_string(NameIndex(ni.get()))?;
            let computed_hash = hash::hash_mod_u32(s, names_stream.num_hashes as u32);
            println!(
                "  hash 0x{:08x} : computed hash 0x{:08x} : {}",
                i, computed_hash, s
            );

            let hash_is_good = computed_hash == i as u32
                || (computed_hash >= probing_hash_base && computed_hash < i as u32);
            if hash_is_good {
                num_hashes_good += 1;
            } else {
                num_hashes_bad += 1;
            }
        }

        println!();
        println!(
            "Number of hashes that are correct:    {:8}",
            num_hashes_good
        );
        println!("Number of hashes that are wrong:      {:8}", num_hashes_bad);
        println!(
            "Number of hash slots that are unused: {:8}",
            num_hashes_unused
        );

        let num_hashes_used = num_hashes_good + num_hashes_bad;
        if num_hashes_used == names_stream.num_strings {
            println!("Number of hashes used is equal to total number of strings (good).");
        } else {
            println!(
                "error: Number of hashes used is {}, which is not equal to the total number of strings ({}).",
                num_hashes_used,
                names_stream.num_strings
            );
        }
    }
    Ok(())
}

```

`pdbtool/src/dump/sources.rs`:

```rs
use super::*;

#[derive(clap::Parser)]
pub struct SourcesOptions {
    /// Show all files
    #[arg(long)]
    pub files: bool,

    /// Show all modules and their source files. This is the default, if no other options
    /// are specified.
    #[arg(long)]
    pub modules: bool,

    /// Show one specific file, by index.
    #[arg(long, short)]
    pub file: Option<u32>,

    /// Show indexes (name offsets, etc.).
    #[arg(long)]
    pub indexes: bool,
}

pub fn dump_dbi_sources(
    dbi_stream: &DbiStream<Vec<u8>>,
    mut options: SourcesOptions,
) -> anyhow::Result<()> {
    let sources_substream = DbiSourcesSubstream::parse(dbi_stream.source_info())?;

    if !options.files && !options.modules && options.file.is_none() {
        options.modules = true;
    }

    let mut module_infos: Vec<ModuleInfo> = Vec::new();
    let modules_substream = dbi_stream.modules();
    module_infos.extend(modules_substream.iter());

    let mut file_name_offsets: Vec<u32> = sources_substream
        .file_name_offsets()
        .iter()
        .map(|x| x.get())
        .collect();
    file_name_offsets.sort_unstable();
    file_name_offsets.dedup();

    println!("DBI File Info substream:");
    println!(
        "Number of modules:      {:8}",
        sources_substream.num_modules()
    );
    println!(
        "Number of sources:      {:8} (unique)",
        file_name_offsets.len()
    );
    println!(
        "Number of file offsets: {:8} (not unique)",
        sources_substream.file_name_offsets().len()
    );
    println!();

    if options.files {
        for &name_offset in file_name_offsets.iter() {
            let name = sources_substream.get_source_file_name_at(name_offset)?;
            if options.indexes {
                println!("  [{name_offset:08x}] : {name}");
            } else {
                println!("  {name}");
            }
        }
        println!();
    }

    if let Some(file_index) = options.file {
        if let Some(&offset) = sources_substream
            .file_name_offsets()
            .get(file_index as usize)
        {
            println!("File name offset: 0x{:x}", offset);
            let file_name = sources_substream.get_source_file_name_at(offset.get())?;
            println!("{}", file_name);
        } else {
            println!(
                "File index {file_index} is out of range. Number of files: {}",
                sources_substream.file_name_offsets().len()
            );
        }
    }

    let num_modules = module_infos.len();
    if num_modules != sources_substream.num_modules() {
        error!("Number of modules is wrong");
    }

    if options.modules {
        for (module_index, module_info) in module_infos.iter().enumerate() {
            if options.indexes {
                println!("Module #{module_index} : {}", module_info.module_name());
            } else {
                println!("Module: {}", module_info.module_name());
            }
            println!("    object: {}", module_info.obj_file());

            for name_offset in sources_substream.name_offsets_for_module(module_index)? {
                match sources_substream.get_source_file_name_at(name_offset.get()) {
                    Ok(name) => {
                        if options.indexes {
                            println!("    [{:08x}] : {}", name_offset.get(), name);
                        } else {
                            println!("    {}", name);
                        }
                    }
                    Err(e) => {
                        error!("{}", e);
                    }
                }
            }

            println!();
        }
    }

    Ok(())
}

```

`pdbtool/src/dump/streams.rs`:

```rs
use super::*;
use dbg_ranges::debug_adjacent;

#[derive(Debug)]
#[allow(dead_code)] // dead code analysis ignores Debug impls, but that's why this type exists
enum StreamUsage {
    OldStreamDir, // 0
    PDB,          // 1
    TPI,          // 2
    DBI,          // 3
    IPI,          // 4
    ModuleInfo {
        module_name: String,
        obj_name: String,
    },
    Named {
        name: String,
    },
    GlobalSymbolStream,
    GlobalSymbolIndex,
    PublicSymbolStream,
    OptionalDebugHeader {
        which: usize,
        whichs: Option<OptionalDebugHeaderStream>,
    },
    TypeStreamHashStream {
        parent_stream: Stream,
    },
    TypeStreamAuxHashStream {
        parent_stream: Stream,
    },
}

#[derive(clap::Parser)]
pub struct StreamsOptions {
    /// Show the blocks assigned to each stream
    #[arg(long)]
    pages: bool,

    /// Show only this stream (name or index)
    stream: Option<String>,
}

pub fn dump_streams(p: &Pdb, options: StreamsOptions) -> anyhow::Result<()> {
    let num_streams = p.num_streams();

    let mut streams_usage: Vec<Option<StreamUsage>> =
        (0..num_streams as usize).map(|_| None).collect();

    streams_usage[0] = Some(StreamUsage::OldStreamDir);
    streams_usage[Stream::PDB.index()] = Some(StreamUsage::PDB);
    streams_usage[Stream::TPI.index()] = Some(StreamUsage::TPI);
    streams_usage[Stream::DBI.index()] = Some(StreamUsage::DBI);
    streams_usage[Stream::IPI.index()] = Some(StreamUsage::IPI);

    let mut add_stream_usage = |stream_opt: Option<u32>, usage: StreamUsage| {
        let Some(stream_index) = stream_opt else {
            return;
        };

        if let Some(slot) = streams_usage.get_mut(stream_index as usize) {
            if let Some(existing_usage) = slot.as_ref() {
                error!(
                    "Stream index #{} has conflicting usages.\n  Usage #1: {:?}\n  Usage #2: {:?}",
                    stream_index, existing_usage, usage
                );
            } else {
                *slot = Some(usage);
            }
        } else {
            error!(
                "Stream index #{} is invalid (is out of range).  Usage is invalid: {:?}",
                stream_index, usage
            );
        }
    };

    let dbi = p.read_dbi_stream()?;
    let dbi_header = dbi.header()?;

    add_stream_usage(
        dbi_header.global_stream_index().ok(),
        StreamUsage::GlobalSymbolIndex,
    );
    add_stream_usage(
        dbi_header.sym_record_stream().ok(),
        StreamUsage::GlobalSymbolStream,
    );
    add_stream_usage(
        dbi_header.public_stream_index().ok(),
        StreamUsage::PublicSymbolStream,
    );

    if let Some(tpi_header) = p.tpi_header()?.header() {
        add_stream_usage(
            tpi_header.hash_stream_index.get(),
            StreamUsage::TypeStreamHashStream {
                parent_stream: Stream::TPI,
            },
        );
        add_stream_usage(
            tpi_header.hash_aux_stream_index.get(),
            StreamUsage::TypeStreamAuxHashStream {
                parent_stream: Stream::TPI,
            },
        );
    }

    if let Some(ipi_header) = p.ipi_header()?.header() {
        add_stream_usage(
            ipi_header.hash_stream_index.get(),
            StreamUsage::TypeStreamHashStream {
                parent_stream: Stream::IPI,
            },
        );
        add_stream_usage(
            ipi_header.hash_aux_stream_index.get(),
            StreamUsage::TypeStreamAuxHashStream {
                parent_stream: Stream::IPI,
            },
        );
    }

    let pdb_info = p.pdbi();

    for (name, stream) in pdb_info.named_streams().iter() {
        add_stream_usage(
            Some(*stream),
            StreamUsage::Named {
                name: name.to_string(),
            },
        );
    }

    for module in dbi.modules().iter() {
        add_stream_usage(
            module.stream(),
            StreamUsage::ModuleInfo {
                module_name: module.module_name().to_string(),
                obj_name: module.obj_file().to_string(),
            },
        );
    }

    let optional_debug_header = dbi.optional_debug_header()?;
    for (i, stream) in optional_debug_header.iter_streams() {
        add_stream_usage(
            Some(stream),
            StreamUsage::OptionalDebugHeader {
                which: i,
                whichs: OptionalDebugHeaderStream::try_from(i).ok(),
            },
        );
    }

    let one_stream: Option<u32> = if let Some(stream_name) = &options.stream {
        Some(crate::save::get_stream_index(p, stream_name)?.0)
    } else {
        None
    };

    let mut num_streams_unknown_usage: u32 = 0;

    for (stream_index, usage_opt) in streams_usage.iter().enumerate() {
        let stream_index = stream_index as u32;

        // Filter out streams, if desired.
        if let Some(s) = one_stream {
            if stream_index != s {
                continue;
            }
        }

        let stream_size = p.stream_len(stream_index);
        if let Some(usage) = usage_opt {
            println!(
                "Stream #{stream_index:6} : (size {stream_size:10}) {:?}",
                usage
            );
            if p.is_stream_valid(stream_index) {
            } else {
                println!("   error: Stream is nil");
            }
        } else {
            if p.is_stream_valid(stream_index) {
                println!("Stream #{stream_index:6} : (size {stream_size:10}) UNKNOWN USAGE");
                num_streams_unknown_usage += 1;
            } else {
                println!("Stream #{stream_index} is nil");
            }
        }

        if options.pages {
            if let Some(msf) = p.msf() {
                let (_stream_len, stream_pages) = msf.stream_size_and_pages(stream_index)?;
                println!("    Pages: {:?}", debug_adjacent(stream_pages));
            }
        }
    }

    if num_streams_unknown_usage != 0 {
        println!(
            "Number of streams with unknown usage: {}",
            num_streams_unknown_usage
        );
    }

    Ok(())
}

```

`pdbtool/src/dump/sym.rs`:

```rs
use super::*;
use crate::dump_utils::indent;
use anyhow::bail;
use ms_pdb::syms::SymData;
use ms_pdb::tpi::TypeStream;
use tracing::warn;

pub fn dump_sym(
    out: &mut String,
    context: &mut DumpSymsContext<'_>,
    record_offset: u32,
    kind: SymKind,
    data: &[u8],
) -> anyhow::Result<()> {
    use super::types::dump_item_short as item_ref;
    use super::types::dump_type_index_short as ty_ref;

    if context.scope_depth == 0 && kind.starts_scope() {
        writeln!(out)?;
    }

    if context.show_record_offsets {
        write!(out, "{:08x} : ", record_offset)?;
    }

    if context.scope_depth > 0 {
        write!(out, "{}", indent(context.scope_depth * 2))?;
    }

    write!(out, "{:?}: ", kind)?;

    match SymData::parse(kind, data)? {
        SymData::Pub(pub_data) => {
            write!(
                out,
                "{}, flags: {:08x}, {}",
                pub_data.fixed.offset_segment,
                pub_data.fixed.flags.get(),
                pub_data.name
            )?;
        }

        SymData::Udt(udt_data) => {
            ty_ref(out, context, udt_data.type_)?;
            write!(out, " {}", udt_data.name)?;
        }

        SymData::Constant(constant_data) => {
            ty_ref(out, context, constant_data.type_)?;
            write!(out, " {} = {}", constant_data.name, constant_data.value)?;
        }

        SymData::ManagedConstant(constant_data) => {
            write!(out, "Token 0x{:x}", constant_data.token)?;
            write!(out, " {} = {}", constant_data.name, constant_data.value)?;
        }

        SymData::RefSym2(sym_ref) => {
            write!(
                out,
                "({}, {:08x}) {}",
                sym_ref.header.module_index.get(),
                sym_ref.header.symbol_offset.get(),
                sym_ref.name
            )?;
        }

        SymData::Data(data) => {
            write!(out, "{} ", data.header.offset_segment,)?;
            ty_ref(out, context, data.header.type_.get())?;
            write!(out, " {}", data.name)?;
        }

        SymData::ThreadData(thread_storage) => {
            write!(
                out,
                "{}, Type: 0x{:04X}, {}",
                thread_storage.header.offset_segment,
                thread_storage.header.type_.0,
                thread_storage.name
            )?;
        }

        SymData::ObjName(obj_name) => {
            write!(out, "sig: 0x{:08x} {}", obj_name.signature, obj_name.name)?;
        }

        SymData::Compile3(compile3) => {
            write!(out, "{}", compile3.name)?;
        }

        SymData::Proc(proc) => {
            write!(
                out,
                "{} ..+ 0x{:x}, ",
                proc.fixed.offset_segment, proc.fixed.proc_len
            )?;
            ty_ref(out, context, proc.fixed.proc_type.get())?;
            write!(out, " {}", proc.name)?;
        }

        SymData::ManagedProc(proc) => {
            write!(out, "Token 0x{:x} {}", proc.fixed.token.get(), proc.name)?;
        }

        SymData::End => {}

        SymData::Unknown => {
            write!(out, "Unknown")?;
        }

        SymData::Annotation(ann) => {
            writeln!(out, "{}", ann.fixed.offset)?;
            for s in ann.iter_strings() {
                writeln!(out, "    {}", s)?;
            }
        }

        SymData::FrameProc(_) => {}

        SymData::RegRel(reg_rel) => {
            write!(
                out,
                "reg(0x{:x})+0x{:x}, ",
                reg_rel.fixed.register.get(),
                reg_rel.fixed.offset.get()
            )?;
            ty_ref(out, context, reg_rel.fixed.ty.get())?;
            write!(out, " {}", reg_rel.name)?;
        }

        SymData::Block(block) => {
            write!(out, "length: 0x{:x}", block.fixed.length.get())?;

            if !block.name.is_empty() {
                write!(out, " name: {}", block.name)?;
            }
        }

        SymData::Local(local) => {
            ty_ref(out, context, local.fixed.ty.get())?;
            write!(out, " {}", local.name)?;
        }

        SymData::DefRangeFramePointerRel(def_range) => {
            write!(
                out,
                "bp+ 0x{:x}, {} ..+ 0x{:x}",
                def_range.fixed.offset_to_frame_pointer,
                def_range.fixed.range.start,
                def_range.fixed.range.range_size.get()
            )?;
            if !def_range.gaps.is_empty() {
                write!(out, ", num_gaps: {}", def_range.gaps.len())?;
            }
        }

        SymData::Trampoline(_) => {}

        SymData::UsingNamespace(ns) => {
            write!(out, "using {}", ns.namespace)?;
        }

        SymData::BuildInfo(b) => {
            item_ref(out, context, b.item)?;
        }

        SymData::InlineSite(site) => {
            item_ref(out, context, site.fixed.inlinee.get())?;
        }

        SymData::InlineSite2(site) => {
            item_ref(out, context, site.fixed.inlinee.get())?;
        }

        SymData::InlineSiteEnd => {}

        SymData::DefRangeRegister(r) => {
            write!(out, "register: 0x{:x}", r.fixed.reg)?;
        }

        SymData::DefRangeRegisterRel(r) => {
            write!(
                out,
                "base register: 0x{:x}, base pointer offset: {}",
                r.fixed.base_reg, r.fixed.base_pointer_offset
            )?;
        }

        SymData::DefRangeSubFieldRegister(_) => {}

        SymData::DefRangeFramePointerRelFullScope(r) => {
            write!(out, "frame pointer offset: {}", r.frame_pointer_offset)?;
        }

        SymData::Label(label) => {
            write!(out, "{} : {}", label.fixed.offset_segment, label.name)?;
        }

        SymData::FunctionList(funcs) => {
            if !funcs.funcs.is_empty() {
                writeln!(out)?;
                for f in funcs.funcs.iter() {
                    item_ref(out, context, f.get())?;
                    writeln!(out)?;
                }
            }
        }

        SymData::FrameCookie(_) => {}

        SymData::CallSiteInfo(site) => {
            write!(out, "{} ", site.offset)?;
            ty_ref(out, context, site.func_type.get())?;
        }

        SymData::HeapAllocSite(site) => {
            write!(out, "{} ", site.offset)?;
            ty_ref(out, context, site.func_type.get())?;
        }
    }

    writeln!(out)?;

    if kind.starts_scope() {
        context.scope_depth += 1;
    }

    if kind.ends_scope() {
        if context.scope_depth > 0 {
            context.scope_depth -= 1;
        } else {
            warn!("scope depth is mismatched");
        }
    }

    Ok(())
}

pub struct DumpSymsContext<'a> {
    pub scope_depth: u32,
    pub type_stream: &'a TypeStream<Vec<u8>>,
    pub show_record_offsets: bool,
    pub show_type_index: bool,
    pub ipi: &'a TypeStream<Vec<u8>>,
}

impl<'a> DumpSymsContext<'a> {
    pub fn new(type_stream: &'a TypeStream<Vec<u8>>, ipi: &'a TypeStream<Vec<u8>>) -> Self {
        Self {
            scope_depth: 0,
            type_stream,
            show_record_offsets: true,
            show_type_index: false,
            ipi,
        }
    }
}

pub fn dump_globals(
    p: &Pdb,
    skip_opt: Option<usize>,
    max_opt: Option<usize>,
    show_bytes: bool,
) -> anyhow::Result<()> {
    println!("Global symbols:");
    let gss = p.gss()?;
    let tpi = p.read_type_stream()?;
    let ipi = p.read_ipi_stream()?;
    dump_symbol_stream(
        &tpi,
        &ipi,
        &gss.stream_data,
        skip_opt,
        max_opt,
        0,
        show_bytes,
    )?;
    Ok(())
}

pub fn dump_symbol_stream(
    type_stream: &TypeStream<Vec<u8>>,
    ipi: &TypeStream<Vec<u8>>,
    symbol_records: &[u8],
    skip_opt: Option<usize>,
    max_opt: Option<usize>,
    stream_offset: u32,
    show_bytes: bool,
) -> anyhow::Result<()> {
    let mut iter = SymIter::new(symbol_records).with_ranges();

    // We have to manually decode all the records that we are skipping;
    // there is no index structure.
    if let Some(skip) = skip_opt {
        for _ in 0..skip {
            if iter.next().is_none() {
                break;
            }
        }
    }

    let mut num_found = 0;
    let mut out = String::new();
    let mut context = DumpSymsContext::new(type_stream, ipi);

    for (record_range, sym) in iter {
        out.clear();

        dump_sym(
            &mut out,
            &mut context,
            stream_offset + record_range.start as u32,
            sym.kind,
            sym.data,
        )?;
        print!("{}", out);

        if show_bytes {
            let record_bytes = &symbol_records[record_range.clone()];
            println!(
                "{:?}",
                HexDump::new(record_bytes).at(stream_offset as usize + record_range.start)
            );
        }

        num_found += 1;
        if let Some(max) = max_opt {
            if num_found >= max {
                break;
            }
        }
    }

    Ok(())
}

/// Displays the symbols for a specific module.
#[derive(clap::Parser, Debug)]
pub struct DumpModuleSymbols {
    /// The module to dump
    pub module_index: u32,

    /// Skip this many symbol records before beginning the dump.
    #[arg(long)]
    pub skip: Option<usize>,

    /// Stop after this many symbol records have been displayed.
    #[arg(long)]
    pub max: Option<usize>,

    /// Dump the hex bytes of each symbol record.
    #[arg(long)]
    pub bytes: bool,

    /// Show the contents of the Global Refs section.
    #[arg(long)]
    pub global_refs: bool,
}

pub fn dump_module_symbols(pdb: &Pdb, options: DumpModuleSymbols) -> anyhow::Result<()> {
    let dbi = pdb.read_dbi_stream()?;

    let Some(module) = dbi.iter_modules().nth(options.module_index as usize) else {
        bail!(
            "Could not find a module with index #{}",
            options.module_index
        );
    };

    let Some(module_stream) = pdb.read_module_stream(&module)? else {
        bail!("Module does not have a module stream (no symbols for module)");
    };

    let tpi = pdb.read_type_stream()?;
    let ipi = pdb.read_ipi_stream()?;

    dump_symbol_stream(
        &tpi,
        &ipi,
        module_stream.sym_data()?,
        options.skip,
        options.max,
        4,
        options.bytes,
    )?;

    println!();

    if options.global_refs {
        println!("Global Refs");
        println!("-----------");
        println!();

        let module_global_refs = module_stream.global_refs()?;
        if !module_global_refs.is_empty() {
            let gss = pdb.gss()?;

            let mut out = String::new();
            let mut context = DumpSymsContext::new(&tpi, &ipi);

            for &global_ref in module_global_refs.iter() {
                let global_ref = global_ref.get();
                // global_ref is an index into the GSS

                let global_sym = gss.get_sym_at(global_ref)?;
                dump_sym(
                    &mut out,
                    &mut context,
                    global_ref,
                    global_sym.kind,
                    global_sym.data,
                )?;
                print!("{}", out);
            }
        } else {
            println!("(none)");
        }
    }

    Ok(())
}

pub fn dump_gsi(p: &Pdb) -> Result<()> {
    let gsi = p.gsi()?;
    let gss = p.gss()?;
    let tpi = p.read_type_stream()?;
    let ipi = p.read_ipi_stream()?;

    println!("*** GLOBALS");
    println!();

    let mut context = DumpSymsContext::new(&tpi, &ipi);

    let mut out = String::new();
    for sym in gsi.names().iter(gss) {
        out.clear();
        // TODO: show the correct record offset, instead of 0
        dump_sym(&mut out, &mut context, 0, sym.kind, sym.data)?;
        println!("{}", out);
    }

    println!();

    Ok(())
}

pub fn dump_psi(p: &Pdb) -> Result<()> {
    let psi = p.read_psi()?;
    let gss = p.gss()?;
    let tpi = p.read_type_stream()?;
    let ipi = p.read_ipi_stream()?;

    println!("*** PUBLICS");
    println!();

    let mut context = DumpSymsContext::new(&tpi, &ipi);

    let mut out = String::new();
    for sym in psi.names().iter(gss) {
        out.clear();
        // TODO: show the correct record offset, instead of 0
        dump_sym(&mut out, &mut context, 0, sym.kind, sym.data)?;
        println!("{}", out);
    }

    println!();

    Ok(())
}

```

`pdbtool/src/dump/types.rs`:

```rs
use super::*;
use ms_pdb::tpi::TypeStreamKind;
use ms_pdb::types::fields::Field;
use ms_pdb::types::primitive::dump_primitive_type_index;
use ms_pdb::types::{ItemId, Leaf, TypeData, TypeIndex, UdtProperties, BUILD_INFO_ARG_NAMES};

#[derive(clap::Parser)]
pub struct DumpTypeStreamOptions {
    /// Skip this many type records before beginning the dump.
    #[arg(long)]
    pub skip: Option<usize>,

    /// Stop after this many records have been dumped.
    #[arg(long)]
    pub max: Option<usize>,

    /// Show a hex dump of each type record.
    #[arg(long)]
    pub show_bytes: bool,

    /// Show the value of `TypeIndex` references.
    #[arg(long)]
    pub show_type_indexes: bool,
}

pub fn dump_type_stream(
    type_stream_kind: TypeStreamKind,
    type_stream: &ms_pdb::tpi::TypeStream<Vec<u8>>, // records to decode and display
    dump_type_index: &mut dyn FnMut(&mut dyn std::fmt::Write, TypeIndex) -> anyhow::Result<()>,
    dump_item: &mut dyn FnMut(&mut dyn std::fmt::Write, ItemId) -> anyhow::Result<()>,
    names: Option<&NamesStream<Vec<u8>>>,
    options: &DumpTypeStreamOptions,
) -> anyhow::Result<()> {
    println!("Type Stream");
    println!("-----------");
    println!();

    let Some(header) = type_stream.header() else {
        println!("Stream is empty (no header)");
        return Ok(());
    };

    println!(
        "type_index_begin = 0x{:08x}",
        type_stream.type_index_begin().0
    );
    println!(
        "type_index_end =   0x{:08x}",
        type_stream.type_index_end().0
    );
    println!(
        "Number of types:   0x{n:08x} {n:8}",
        n = type_stream.num_types()
    );

    println!(
        "Number of hash buckets: {n} 0x{n:x}",
        n = header.num_hash_buckets.get()
    );
    println!("Hash key size: {n} 0x{n:x}", n = header.hash_key_size.get());

    println!("{:#?}", type_stream.header());

    let index_prefix = match type_stream_kind {
        TypeStreamKind::TPI => 'T',
        TypeStreamKind::IPI => 'I',
    };

    let type_index_begin = type_stream.type_index_begin();
    let mut iter = type_stream.iter_type_records().with_ranges();
    let mut next_type_index = type_index_begin;

    if let Some(skip) = options.skip {
        // We have to brute-force the iterator, since there is no way to seek to a specific type record.
        for _ in 0..skip {
            let item = iter.next();
            if item.is_none() {
                break;
            }
            next_type_index.0 += 1;
        }
    }

    let mut num_found: usize = 0;

    let mut out = String::new();

    let type_stream_start = type_stream.type_records_range().start;

    for (record_range, ty) in iter {
        out.clear();

        dump_type_record(
            &mut out,
            dump_type_index,
            dump_item,
            index_prefix,
            names,
            record_range.start + type_stream_start,
            next_type_index,
            ty.kind,
            ty.data,
            options,
        )?;

        print!("{}", out);

        next_type_index.0 += 1;

        num_found += 1;
        if let Some(max) = options.max {
            if num_found >= max {
                break;
            }
        }
    }

    Ok(())
}

pub fn dump_type_record(
    out: &mut dyn std::fmt::Write,
    ty_ref_in: &mut dyn FnMut(&mut dyn std::fmt::Write, TypeIndex) -> anyhow::Result<()>,
    dump_item: &mut dyn FnMut(&mut dyn std::fmt::Write, ItemId) -> anyhow::Result<()>,
    index_prefix: char,
    names: Option<&NamesStream<Vec<u8>>>,
    record_offset: usize,
    type_index: TypeIndex,
    kind: Leaf,
    data: &[u8],
    options: &DumpTypeStreamOptions,
) -> anyhow::Result<()> {
    let mut ty_ref = |out: &mut dyn std::fmt::Write, ty: TypeIndex| -> anyhow::Result<()> {
        if options.show_type_indexes {
            write!(out, "{ty:?} ")?;
        }

        ty_ref_in(out, ty)
    };

    write!(
        out,
        "[{record_offset:08x}] {index_prefix}#{:08x} [{:04x}] {kind:?} : ",
        type_index.0, kind.0,
    )?;

    let mut p = Parser::new(data);

    fn out_udt_props(out: &mut dyn std::fmt::Write, props: UdtProperties) -> std::fmt::Result {
        if props.fwdref() {
            out.write_str(" fwdref")?;
        }
        Ok(())
    }

    match TypeData::parse(kind, &mut p)? {
        TypeData::Array(t) => {
            ty_ref(out, t.fixed.element_type.get())?;
            write!(out, "[{}]", t.len)?;
        }

        TypeData::Struct(t) => {
            out_udt_props(out, t.fixed.property.get())?;
            write!(out, " {}", t.name)?;
            let field_list = t.fixed.field_list.get();
            if let Some(unique_name) = t.unique_name {
                if unique_name != t.name {
                    write!(out, " (unique: {})", unique_name)?;
                }
            }
            if field_list.0 != 0 {
                write!(out, " fields: ")?;
                ty_ref(out, field_list)?;
            }
        }

        TypeData::Enum(t) => {
            out_udt_props(out, t.fixed.property.get())?;
            write!(out, " {}", t.name)?;
            if let Some(unique_name) = t.unique_name {
                if unique_name != t.name {
                    write!(out, " (unique: {})", unique_name)?;
                }
            }
        }

        TypeData::Union(t) => {
            out_udt_props(out, t.fixed.property.get())?;
            write!(out, " {}", t.name)?;
            if let Some(unique_name) = t.unique_name {
                if unique_name != t.name {
                    write!(out, " (unique: {})", unique_name)?;
                }
            }
        }

        TypeData::Unknown => {
            write!(out, "<UNKNOWN>")?;
        }

        TypeData::Pointer(t) => {
            let attr = t.fixed.attr();
            if attr.r#const() {
                write!(out, "const ")?;
            }
            if attr.volatile() {
                write!(out, "volatile ")?;
            }
            if attr.unaligned() {
                write!(out, "unaligned ")?;
            }
            write!(out, "* ")?;

            ty_ref(out, t.fixed.ty.get())?;
        }

        TypeData::Modifier(t) => {
            if t.is_const() {
                write!(out, "const ")?;
            }
            if t.is_unaligned() {
                write!(out, "unaligned ")?;
            }
            if t.is_volatile() {
                write!(out, "volatile ")?;
            }
            ty_ref(out, t.underlying_type.get())?;
        }

        TypeData::MemberFunc(t) => {
            write!(out, "    ret: ")?;
            ty_ref(out, t.return_value.get())?;
            write!(out, " class: ")?;
            ty_ref(out, t.class.get())?;
            if t.this.get().0 != 0 {
                write!(out, " this: ")?;
                ty_ref(out, t.this.get())?;
            }
        }

        TypeData::Proc(t) => {
            write!(out, "ret: ")?;
            ty_ref(out, t.return_value.get())?;
            write!(out, " args: ")?;
            ty_ref(out, t.arg_list.get())?;
        }

        TypeData::VTableShape(t) => {
            write!(out, "num_slots: {}", t.count)?;
        }

        TypeData::FieldList(fields) => {
            writeln!(out)?;
            for field in fields.iter() {
                match field {
                    Field::Member(m) => {
                        write!(out, "    at {} : {} ", m.offset, m.name)?;
                        ty_ref(out, m.ty)?;
                        writeln!(out)?;
                    }
                    Field::Enumerate(en) => {
                        writeln!(out, "    {} = {}", en.name, en.value)?;
                    }
                    Field::Method(m) => {
                        writeln!(out, "    {}() - (method group)", m.name)?;
                    }
                    Field::OneMethod(m) => {
                        writeln!(out, "    {}()", m.name)?;
                    }
                    Field::StaticMember(sm) => writeln!(out, "    static {}", sm.name)?,
                    Field::NestedType(nt) => writeln!(out, "    nested {}", nt.name)?,
                    _ => {
                        writeln!(out, "??")?;
                    }
                }
            }
        }

        TypeData::MethodList(_t) => {}

        TypeData::ArgList(t) => {
            write!(out, "num_args: {}", t.args.len())?;
            for &arg in t.args.iter() {
                write!(out, ", ")?;
                ty_ref(out, arg.get())?;
            }
        }

        TypeData::Alias(t) => {
            write!(out, "{} - ", t.name)?;
            ty_ref(out, t.utype)?;
        }

        TypeData::FuncId(t) => {
            writeln!(out, "name: {:?}", t.name)?;

            if t.fixed.scope.get() != 0 {
                write!(out, "    scope: ")?;
                dump_item(out, t.fixed.scope.get())?;
                writeln!(out)?;
            }

            write!(out, "    func: ")?;
            ty_ref(out, t.fixed.func_type.get())?;
        }

        TypeData::MFuncId(t) => {
            writeln!(out, "name: {:?}", t.name)?;

            let parent = t.fixed.parent_type.get();
            write!(out, "    parent: ")?;
            ty_ref(out, parent)?;
            writeln!(out)?;

            let func = t.fixed.func_type.get();
            write!(out, "    func: ")?;
            ty_ref(out, func)?;
            writeln!(out)?;
        }

        TypeData::StringId(t) => {
            writeln!(out, "name: {:?}", t.name)?;
            write!(out, "    ")?;
            dump_item(out, t.id)?;
            writeln!(out)?;
        }

        TypeData::UdtModSrcLine(t) => {
            let src = t.src.get();
            writeln!(out, " module: {}", t.imod.get())?;

            write!(out, "    ")?;
            ty_ref(out, t.ty.get())?;
            writeln!(out)?;

            let line = t.line.get();
            if let Some(names) = names {
                if let Ok(s) = names.get_string(src) {
                    writeln!(out, "    (line {line:6}) {}", s)?;
                } else {
                    writeln!(out, "    (line {line:6}) ?? {src:?}")?;
                }
            }
        }

        TypeData::UdtSrcLine(t) => {
            let src = t.src.get();
            writeln!(out)?;

            write!(out, "    ")?;
            ty_ref(out, t.ty.get())?;
            writeln!(out)?;

            let line = t.line.get();
            if let Some(names) = names {
                if let Ok(s) = names.get_string(src) {
                    writeln!(out, "    (line {line:6}) {}", s)?;
                } else {
                    writeln!(out, "    (line {line:6}) ?? {src:?}")?;
                }
            }
        }

        TypeData::SubStrList(t) => {
            writeln!(out, "n = {}", t.ids.len())?;
            for (n, id) in t.ids.iter().enumerate() {
                writeln!(out, "[{n:3}] I#{:08x}", id.get())?;
                dump_item(out, id.get())?;
                writeln!(out)?;
            }
        }

        TypeData::BuildInfo(build_info) => {
            writeln!(out)?;

            for (i, a) in build_info.args.iter().enumerate() {
                if let Some(name) = BUILD_INFO_ARG_NAMES.get(i) {
                    write!(out, "    {name} = ")?;
                } else {
                    write!(out, "    ??{i} = ")?;
                }
                dump_item(out, a.get())?;
                writeln!(out)?;
            }
        }

        TypeData::VFTable(vftable) => {
            if vftable.path.get().0 != 0 {
                write!(out, "path: ")?;
                ty_ref(out, vftable.path.get())?;
                write!(out, " ")?;
            }

            if vftable.root.get().0 != 0 {
                write!(out, "root: ")?;
                ty_ref(out, vftable.root.get())?;
                write!(out, " ")?;
            }
        }
    }

    writeln!(out)?;

    if options.show_bytes {
        write!(out, "{:?}", HexDump::new(data))?;
        writeln!(out)?;
    }

    Ok(())
}

// recursive
pub fn dump_type_index_short(
    out: &mut dyn std::fmt::Write,
    context: &super::sym::DumpSymsContext,
    type_index: TypeIndex,
) -> anyhow::Result<()> {
    if context.type_stream.is_primitive(type_index) {
        dump_primitive_type_index(out, type_index)?;
        return Ok(());
    }

    let type_record = context.type_stream.record(type_index)?;
    let kind = type_record.kind;
    let data = type_record.data;

    if context.show_type_index {
        write!(out, "T#{:08x} ", type_index.0)?;
    }

    write!(out, "{kind:?} : ")?;

    let ty_ref = |out: &mut dyn std::fmt::Write,
                  context: &super::sym::DumpSymsContext,
                  ref_ti: TypeIndex| {
        let _ = dump_type_index_short(out, context, ref_ti);
    };

    let mut p = Parser::new(data);

    match TypeData::parse(kind, &mut p)? {
        TypeData::Array(t) => {
            ty_ref(out, context, t.fixed.element_type.get());
            write!(out, "[{}]", t.len)?;
        }

        TypeData::Struct(t) => write!(out, "{}", t.name)?,
        TypeData::Enum(t) => write!(out, "{}", t.name)?,
        TypeData::Union(t) => write!(out, "{}", t.name)?,
        TypeData::Unknown => write!(out, "<UNKNOWN>")?,

        TypeData::Pointer(t) => {
            let attr = t.fixed.attr();
            if attr.r#const() {
                write!(out, "const ")?;
            }
            if attr.volatile() {
                write!(out, "volatile ")?;
            }
            if attr.unaligned() {
                write!(out, "unaligned ")?;
            }

            ty_ref(out, context, t.fixed.ty.get());
        }

        TypeData::Modifier(t) => {
            if t.is_const() {
                write!(out, "const ")?;
            }
            if t.is_unaligned() {
                write!(out, "unaligned ")?;
            }
            if t.is_volatile() {
                write!(out, "volatile ")?;
            }
            ty_ref(out, context, t.underlying_type.get());
        }

        TypeData::MemberFunc(_t) => {}
        TypeData::Proc(_t) => {}
        TypeData::VTableShape(_t) => {}
        TypeData::FieldList(_t) => {}
        TypeData::MethodList(_t) => {}
        TypeData::ArgList(t) => write!(out, "num_args: {}", t.args.len())?,
        TypeData::Alias(t) => write!(out, "{}", t.name)?,

        TypeData::VFTable(_) => {}

        _ => {
            write!(out, "error: unexpected record kind in TPI stream")?;
        }
    }

    Ok(())
}

// recursive
pub fn dump_item_short(
    out: &mut dyn std::fmt::Write,
    context: &super::sym::DumpSymsContext,
    item: ItemId,
) -> anyhow::Result<()> {
    if item == 0 {
        write!(out, "(nil)")?;
        return Ok(());
    }
    if context.ipi.is_primitive(TypeIndex(item)) {
        write!(out, "(error: item = 0x{:x})", item)?;
        return Ok(());
    }

    let item_record = context.ipi.record(TypeIndex(item))?;
    let kind = item_record.kind;
    let data = item_record.data;

    if context.show_type_index {
        write!(out, "I#{:08x} ", item)?;
    }

    write!(out, "{kind:?} : ")?;

    let ty_ref = |out: &mut dyn std::fmt::Write,
                  context: &super::sym::DumpSymsContext,
                  ref_ti: TypeIndex| {
        let _ = dump_type_index_short(out, context, ref_ti);
    };

    let mut p = Parser::new(data);

    match TypeData::parse(kind, &mut p)? {
        TypeData::UdtModSrcLine(t) => {
            write!(out, "src: 0x{:08x}, line {}, ", t.src.0.get(), t.line.get())?;
            ty_ref(out, context, t.ty.get());
        }

        TypeData::UdtSrcLine(t) => {
            write!(out, "src: 0x{:08x}, line {}, ", t.src.0.get(), t.line.get())?;
            ty_ref(out, context, t.ty.get());
        }

        TypeData::FuncId(t) => write!(out, "{:?}", t.name)?,
        TypeData::MFuncId(t) => write!(out, "{:?}", t.name)?,
        TypeData::StringId(t) => write!(out, "{:?}", t.name)?,
        TypeData::SubStrList(_) => {}
        TypeData::BuildInfo(_) => {}

        _ => {
            write!(out, "error: unexpected record kind in IPI stream")?;
        }
    }

    Ok(())
}

```

`pdbtool/src/dump_utils.rs`:

```rs
//! Utilities for dumping byte slices as hex or possibly-invalid UTF-8 strings.

#![forbid(unsafe_code)]
#![warn(missing_docs)]
#![allow(clippy::collapsible_else_if)]
#![allow(clippy::needless_lifetimes)]

use std::fmt::{Debug, Formatter, Write};

/// Dumps a byte slice. The bytes are formatted into rows, with a byte offset displayed on the
/// left, the byte values in hex in the center, and ASCII characters on the right.
pub(crate) struct HexDump<'a> {
    bytes: &'a [u8],
    start: usize,
    show_header: bool,
    row_len: usize,
}

impl<'a> HexDump<'a> {
    /// Creates a `HexDump` over a byte slice with the default display settings.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self {
            bytes,
            start: 0,
            show_header: false,
            row_len: 16,
        }
    }

    /// Limits the input to a maximum length.
    pub fn max(self, max: usize) -> Self {
        Self {
            bytes: &self.bytes[..self.bytes.len().min(max)],
            ..self
        }
    }

    /// Sets the displayed byte offset to a value.
    pub fn at(self, start: usize) -> Self {
        Self { start, ..self }
    }

    /// Specifies whether to display the header line.
    pub fn header(self, show_header: bool) -> Self {
        Self {
            show_header,
            ..self
        }
    }
}

impl<'a> std::fmt::Display for HexDump<'a> {
    fn fmt(&self, fmt: &mut Formatter) -> std::fmt::Result {
        <Self as Debug>::fmt(self, fmt)
    }
}

impl<'a> Debug for HexDump<'a> {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        let mut pos = self.start;

        let mut repeat_start: usize = 0;
        let mut repeat_len: usize = 0;
        let mut repeat_byte: u8 = 0;

        if self.show_header {
            writeln!(
                f,
                "________ : 00 01 02 03 04 05 06 07-08 09 0a 0b 0c 0d 0e 0f"
            )?;
        }

        let row_len = self.row_len;

        let write_offset =
            |f: &mut Formatter, offset: usize| -> std::fmt::Result { write!(f, "{offset:08x} : ") };

        let empty_col_size = 3; // two hex values and a space

        for row in self.bytes.chunks(row_len) {
            if row.len() == row_len {
                if repeat_len != 0 {
                    // Are we extending a repeated set of rows?
                    if row.iter().all(|&b| b == repeat_byte) {
                        repeat_len += row_len;
                        pos += row_len;
                        continue;
                    }
                } else {
                    // Did we find the beginning of a new repeated row?
                    let row0 = row[0];
                    if row.iter().all(|&b| b == row0) {
                        repeat_byte = row0;
                        repeat_start = pos;
                        repeat_len = row_len;
                        pos += row_len;
                        continue;
                    }
                }
            }

            if repeat_len != 0 {
                write_offset(f, repeat_start)?;
                writeln!(f, "... {repeat_byte:02x} repeated ...")?;
                repeat_len = 0;
                repeat_start = 0;
                repeat_byte = 0;
            }

            write_offset(f, pos)?;
            for &b in row.iter() {
                write!(f, " {:02x}", b)?;
            }
            for _ in 0..(row_len - row.len()) * empty_col_size {
                f.write_char(' ')?;
            }

            {
                write!(f, " : ")?;
                for &b in row.iter() {
                    let c = if matches!(b, 0x20..=0x7e) {
                        char::from(b)
                    } else {
                        '.'
                    };
                    f.write_char(c)?;
                }
            }

            f.write_char('\n')?;

            pos += row_len;
        }

        if repeat_len != 0 {
            write_offset(f, repeat_start)?;
            writeln!(f, "... {repeat_byte:02x} repeated ...")?;
            write_offset(f, pos)?;
            writeln!(f, "(end)")?;
        }

        Ok(())
    }
}

/// Displays a byte slice in hexadecimal.
pub struct HexStr<'a> {
    bytes: &'a [u8],
    packed: bool,
}

impl<'a> HexStr<'a> {
    /// Creates a new `HexStr` over a slice with the default display settings.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self {
            bytes,
            packed: false,
        }
    }

    /// Specifies that the hex string should be displayed without spaces between the bytes.
    pub fn packed(self) -> Self {
        Self {
            packed: true,
            ..self
        }
    }
}

impl<'a> Debug for HexStr<'a> {
    fn fmt(&self, fmt: &mut Formatter) -> std::fmt::Result {
        for (i, &b) in self.bytes.iter().enumerate() {
            if i != 0 && !self.packed {
                fmt.write_char(' ')?;
            }
            write!(fmt, "{:02x}", b)?;
        }

        Ok(())
    }
}

/// Helps display indentation in debug output
#[derive(Copy, Clone)]
pub struct Indent(pub u32);

impl std::fmt::Display for Indent {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        for _ in 0..self.0 {
            fmt.write_char(' ')?;
        }
        Ok(())
    }
}

/// Creates an `Indent`.
pub fn indent(n: u32) -> Indent {
    Indent(n)
}

```

`pdbtool/src/find.rs`:

```rs
use crate::dump::sym::dump_sym;
use anyhow::Result;
use bstr::BStr;
use ms_pdb::syms::SymData;
use ms_pdb::utils::iter::IteratorWithRangesExt;
use std::path::Path;

/// Searches the DBI Section Contributions table.
#[derive(clap::Parser)]
pub struct FindOptions {
    /// The PDB to search.
    pub pdb: String,

    /// COFF section index to search
    pub section: u16,

    /// The symbol name or contribution offset to search.
    pub name: String,
}

pub fn find_command(options: &FindOptions) -> Result<()> {
    let pdb = ms_pdb::Pdb::open(Path::new(&options.pdb))?;

    let dbi = pdb.read_dbi_stream()?;
    let contribs = dbi.section_contributions()?;

    if let Some(hex_str) = options.name.strip_prefix("0x") {
        let addr = u32::from_str_radix(hex_str, 0x10)?;

        if let Some(contrib) = contribs.find(options.section, addr as i32) {
            println!("Found contribution record:\n{:#?}", contrib);
            println!("Module index = {}", contrib.module_index.get());
        } else {
            println!("No symbol found.");
        }
    } else {
        println!("name lookups are nyi");
    }

    Ok(())
}

/// Searches the TPI Stream for a given type.
#[derive(clap::Parser)]
pub struct FindNameOptions {
    /// The PDB to search.
    pub pdb: String,

    /// The type name to search for.
    pub name: String,

    /// Indicates that `name` is a regex.
    #[arg(long, short)]
    pub regex: bool,
}

pub fn find_name_command(options: &FindNameOptions) -> Result<()> {
    use crate::dump::sym::DumpSymsContext;

    let pdb = ms_pdb::Pdb::open(Path::new(&options.pdb))?;

    let tpi = pdb.read_type_stream()?;
    let ipi = pdb.read_ipi_stream()?;
    let mut context = DumpSymsContext::new(&tpi, &ipi);

    let gss = pdb.read_gss()?;

    if options.regex {
        let rx = regex::bytes::Regex::new(&options.name)?;

        let mut found_any = false;

        for (record_range, sym) in gss.iter_syms().with_ranges() {
            let sym_data = SymData::parse(sym.kind, sym.data)?;

            if let Some(sym_name) = sym_data.name() {
                if rx.is_match(sym_name) {
                    let mut out = String::new();
                    dump_sym(
                        &mut out,
                        &mut context,
                        record_range.start as u32,
                        sym.kind,
                        sym.data,
                    )?;
                    print!("{}", out);
                    found_any = true;
                }
            }
        }

        if !found_any {
            println!("No matches found.");
        }
    } else {
        let gsi = pdb.read_gsi()?;
        if let Some(sym) = gsi.find_symbol(&gss, BStr::new(&options.name))? {
            let mut out = String::new();
            dump_sym(&mut out, &mut context, 0, sym.kind, sym.data)?;
            print!("{}", out);
            return Ok(());
        }
        println!("Symbol not found.");
    }

    Ok(())
}

```

`pdbtool/src/glob_pdbs.rs`:

```rs
use anyhow::{bail, Result};
use std::path::PathBuf;

#[derive(clap::Parser)]
pub struct PdbList {
    /// The set of PDB files to read. This can contain globs, e.g. `*.pdb` or `foo\**\*.pdb`.
    pub pdbs: Vec<String>,
}

impl PdbList {
    pub fn get_paths(&self) -> Result<Vec<PathBuf>> {
        let paths = self.get_paths_empty_ok()?;
        if paths.is_empty() {
            bail!("This command requires that you specify one or more PDB files.");
        }
        Ok(paths)
    }

    pub fn get_paths_empty_ok(&self) -> Result<Vec<PathBuf>> {
        let mut file_names = Vec::new();
        for file_name_or_glob in self.pdbs.iter() {
            if file_name_or_glob.contains(['?', '*']) {
                for f in glob::glob(file_name_or_glob)? {
                    let f = f?;
                    if f.is_file() {
                        file_names.push(f);
                    }
                }
            } else {
                file_names.push(PathBuf::from(file_name_or_glob));
            }
        }

        Ok(file_names)
    }
}

```

`pdbtool/src/hexdump.rs`:

```rs
use crate::dump_utils::HexDump;
use anyhow::Result;
use std::fs::File;
use std::io::{Read, Seek, SeekFrom};

use crate::util::HexU64;

/// Dumps the contents of a file as a hex dump.
#[derive(clap::Parser)]
pub struct HexdumpOptions {
    /// The file to dump
    pub file: String,
    /// Offset within the file. Defaults to 0.
    pub offset: Option<HexU64>,
    /// Max length of the data to dump. Defaults to 0x1000.
    pub len: Option<HexU64>,
}

pub fn command(options: HexdumpOptions) -> Result<()> {
    let mut f = File::open(&options.file)?;

    let offset: u64 = if let Some(offset) = options.offset {
        offset.0
    } else {
        0
    };

    f.seek(SeekFrom::Start(offset))?;

    let len: usize = if let Some(len) = options.len {
        len.0 as usize
    } else {
        0x1000
    };

    let mut buffer: Vec<u8> = vec![0; len];

    let n = f.read(buffer.as_mut_slice())?;
    let bytes = &buffer[..n];

    print!("{}", HexDump::new(bytes).at(offset as usize));

    Ok(())
}

```

`pdbtool/src/main.rs`:

```rs
#![forbid(unused_must_use)]
#![allow(clippy::collapsible_else_if)]
#![allow(clippy::manual_map)]
#![allow(clippy::single_match)]
#![allow(clippy::upper_case_acronyms)]
#![allow(clippy::too_many_arguments)]
#![allow(clippy::needless_late_init)]

use clap::Parser;

mod addsrc;
mod copy;
mod counts;
mod dump;
mod dump_utils;
mod find;
mod glob_pdbs;
mod hexdump;
mod pdz;
mod save;
mod util;

#[derive(clap::Parser)]
struct CommandWithFlags {
    /// Reduce logging to just warnings and errors in `mspdb` and `pdbtool` modules.
    #[arg(long)]
    quiet: bool,

    /// Turn on debug output in all `mspdb` and `pdbtool` modules. Noisy!
    #[arg(long)]
    verbose: bool,

    /// Show timestamps in log messages
    #[arg(long)]
    timestamps: bool,

    /// Connect to Tracy (diagnostics tool). Requires that the `tracy` Cargo feature be enabled.
    #[arg(long)]
    tracy: bool,

    #[command(subcommand)]
    command: Command,
}

#[derive(clap::Subcommand)]
enum Command {
    /// Adds source file contents to the PDB. The contents are embedded directly within the PDB.
    /// WinDbg and Visual Studio can both extract the source files.
    AddSrc(addsrc::AddSrcOptions),
    /// Copies a PDB from one file to another. All stream contents are preserved exactly, byte-for-byte.
    /// The blocks within streams are laid out sequentially.
    Copy(copy::Options),
    Test,
    Dump(dump::DumpOptions),
    Save(save::SaveStreamOptions),
    Find(find::FindOptions),
    FindName(find::FindNameOptions),
    Counts(counts::CountsOptions),
    /// Dumps part of a file (any file, not just a PDB) as a hex dump. If you want to dump a
    /// specific stream, then use the `dump <filename> hex` command instead.
    Hexdump(hexdump::HexdumpOptions),
    PdzEncode(pdz::encode::PdzEncodeOptions),
}

fn main() -> anyhow::Result<()> {
    let command_with_flags = CommandWithFlags::parse();
    configure_tracing(&command_with_flags);

    match command_with_flags.command {
        Command::AddSrc(args) => addsrc::command(args)?,
        Command::Dump(args) => dump::dump_main(args)?,
        Command::Test => {}
        Command::Copy(args) => copy::copy_command(&args)?,
        Command::Save(args) => save::save_stream(&args)?,
        Command::Find(args) => find::find_command(&args)?,
        Command::FindName(args) => find::find_name_command(&args)?,
        Command::Counts(args) => counts::counts_command(args)?,
        Command::Hexdump(args) => hexdump::command(args)?,
        Command::PdzEncode(args) => pdz::encode::pdz_encode(args)?,
    }

    Ok(())
}

fn configure_tracing(args: &CommandWithFlags) {
    use tracing_subscriber::filter::LevelFilter;

    if args.tracy {
        #[cfg(feature = "tracy")]
        {
            use tracing_subscriber::layer::SubscriberExt;

            let layer = tracing_tracy::TracyLayer::default();
            tracing::subscriber::set_global_default(tracing_subscriber::registry().with(layer))
                .expect("setup tracy layer");

            return;
        }

        #[cfg(not(feature = "tracy"))]
        {
            eprintln!(
                "Tracing is not enabled in the build configuration.\n\
                 You can enable it by using 'cargo run --features \"tracy\"'."
            );
        }
    }

    let builder = tracing_subscriber::fmt();

    let max_level = if args.quiet {
        LevelFilter::WARN
    } else if args.verbose {
        LevelFilter::DEBUG
    } else {
        LevelFilter::INFO
    };

    builder.with_max_level(max_level).finish();
}

```

`pdbtool/src/pdz.rs`:

```rs
pub mod encode;
pub mod util;

```

`pdbtool/src/pdz/encode.rs`:

```rs
use crate::pdz::util::*;
use anyhow::{Context, Result};
use ms_pdb::msfz::MsfzWriter;
use ms_pdb::{Pdb, Stream};
use std::fs::File;
use std::io::{Seek, SeekFrom, Write};
use std::path::Path;
use tracing::{trace, trace_span};

#[derive(clap::Parser)]
pub struct PdzEncodeOptions {
    /// Path to the input PDB file.
    pub input_pdb: String,

    /// Path to the output PDZ file.
    pub output_pdz: String,
}

pub fn pdz_encode(options: PdzEncodeOptions) -> Result<()> {
    let _span = trace_span!("pdz_encode").entered();

    let pdb_metadata = std::fs::metadata(&options.input_pdb).with_context(|| {
        format!(
            "Failed to get metadata for input PDB: {}",
            options.input_pdb
        )
    })?;
    let pdb = Pdb::open(Path::new(&options.input_pdb))
        .with_context(|| format!("Failed to open input PDB: {}", options.input_pdb))?;
    let out = File::create(&options.output_pdz)
        .with_context(|| format!("Failed to open output PDZ: {}", options.output_pdz))?;

    let mut writer = MsfzWriter::new(out)?;
    let mut stream_data: Vec<u8> = Vec::new();
    let num_streams = pdb.num_streams();
    writer.reserve_num_streams(num_streams as usize);

    for stream_index in 1..num_streams {
        if !pdb.is_stream_valid(stream_index) {
            // This is a nil stream. We don't need to do anything because the writer has already
            // reserved this stream slot.
            continue;
        }

        let _span = trace_span!("stream").entered();
        trace!(stream_index);

        stream_data.clear();
        {
            let _span = trace_span!("read stream").entered();
            pdb.read_stream_to_vec_mut(stream_index, &mut stream_data)?;
            trace!(stream_size = stream_data.len());
        }

        {
            let _span = trace_span!("write stream").entered();
            let mut sw = writer.stream_writer(stream_index)?;

            // Don't compress the PDBI. The PDBI is very small, so compression is useless, and this
            // exercises the non-compressed option. It also makes it possible to read the PDBI in
            // a hex dump.
            if stream_index == Stream::PDB.into() {
                sw.set_compression_enabled(false);
            }

            sw.write_all(&stream_data)?;
        }
    }

    // Get final size of the file. Don't append more data to the file after this line.
    let (summary, mut file) = {
        let _span = trace_span!("finish writing").entered();
        writer.finish()?
    };

    let out_file_size = file.seek(SeekFrom::End(0))?;
    show_comp_rate("PDB -> PDZ", pdb_metadata.len(), out_file_size);

    println!("{}", summary);

    // Explicitly close our file handles so that the replace_file() call can succeed.
    drop(pdb);
    drop(file);

    Ok(())
}

```

`pdbtool/src/pdz/util.rs`:

```rs
pub(crate) fn show_comp_rate(description: &str, before: u64, after: u64) {
    if before == 0 {
        // We don't divide by zero around here.
        println!(
            "    {:-30} : {:8} -> {:8}",
            description,
            friendly::bytes(before),
            friendly::bytes(after)
        );
    } else {
        let percent = (before as f64 - after as f64) / (before as f64) * 100.0;
        println!(
            "    {:-30} : {:8} -> {:8} {:2.1} %",
            description,
            friendly::bytes(before),
            friendly::bytes(after),
            percent
        );
    }
}

```

`pdbtool/src/save.rs`:

```rs
use anyhow::bail;
use ms_pdb::{names::NAMES_STREAM_NAME, Pdb, Stream};
use std::ops::Range;
use std::path::Path;

#[derive(clap::Parser)]
pub struct SaveStreamOptions {
    /// The PDB file to read.
    pdb: String,

    /// The index or name of the stream to save. Name can be one of: pdb, dbi, gsi, gss, tpi, ipi
    stream: String,

    /// The path to save the stream to.
    out: String,
}

pub fn save_stream(options: &SaveStreamOptions) -> anyhow::Result<()> {
    let reader = Pdb::open(Path::new(&options.pdb))?;

    // Support saving substreams of just a handful of streams. This could be made more general.
    if options.stream == "dbi/sources" {
        let dbi = reader.read_dbi_stream()?;
        std::fs::write(&options.out, dbi.source_info())?;
        return Ok(());
    }

    if options.stream == "dbi/section_contributions" {
        let dbi = reader.read_dbi_stream()?;
        std::fs::write(&options.out, dbi.section_contributions_bytes())?;
        return Ok(());
    }

    let (stream_index, stream_range_opt) = get_stream_index(&reader, &options.stream)?;
    let stream_data = reader.read_stream_to_vec(stream_index)?;
    let stream_slice = if let Some(stream_range) = stream_range_opt {
        if let Some(s) = stream_data.get(stream_range.clone()) {
            s
        } else {
            bail!(
                "The stream range 0x{:x}..0x{:x} is out of range. Stream length = 0x{:x}.",
                stream_range.start,
                stream_range.end,
                stream_data.len()
            );
        }
    } else {
        stream_data.as_slice()
    };
    std::fs::write(&options.out, stream_slice)?;
    Ok(())
}

pub fn get_stream_index(reader: &Pdb, name: &str) -> anyhow::Result<(u32, Option<Range<usize>>)> {
    if let Ok(stream_index) = name.parse::<u32>() {
        return Ok((stream_index, None));
    }

    if let Some(i) = get_fixed_stream(name) {
        return Ok((i, None));
    }

    let dbi_header = reader.dbi_header();

    if let Some(suffix) = name.strip_prefix("named:") {
        if let Some(s) = reader.named_streams().get(suffix) {
            return Ok((s, None));
        } else {
            bail!("There is no named stream with that name.");
        }
    }

    if let Some(suffix) = name.strip_prefix("mod:") {
        let index: usize = suffix.parse()?;
        let modules = reader.read_modules()?;
        if let Some(module) = modules.iter().nth(index) {
            if let Some(s) = module.stream() {
                return Ok((s, None));
            } else {
                bail!("Module {} does not have a stream.", index);
            }
        } else {
            bail!("Module index {} is out of valid range.", index);
        }
    }

    Ok(match name {
        "gss" => (dbi_header.sym_record_stream()?, None),
        "psi" => (dbi_header.public_stream_index()?, None),
        "gsi" => (dbi_header.global_stream_index()?, None),
        "names" => (reader.named_stream_err(NAMES_STREAM_NAME)?, None),
        "dbi/sources" => (Stream::DBI.into(), Some(dbi_header.sources_range()?)),
        "dbi/modules" => (Stream::DBI.into(), Some(dbi_header.modules_range()?)),
        _ => {
            if let Some(s) = reader.named_streams().get(name) {
                return Ok((s, None));
            }
            bail!("The name '{}' does not identify any known stream.", name);
        }
    })
}

fn get_fixed_stream(name: &str) -> Option<u32> {
    match name {
        "pdb" => Some(Stream::PDB.into()),
        "dbi" => Some(Stream::DBI.into()),
        "tpi" => Some(Stream::TPI.into()),
        "ipi" => Some(Stream::IPI.into()),
        _ => None,
    }
}

```

`pdbtool/src/util.rs`:

```rs
use std::str::FromStr;

use bitvec::prelude::BitSlice;

#[allow(dead_code)] // useful
pub fn dump_bitvec<T, O, W: std::fmt::Write + ?Sized>(
    b: &BitSlice<T, O>,
    out: &mut W,
) -> std::fmt::Result
where
    T: bitvec::prelude::BitStore,
    O: bitvec::prelude::BitOrder,
{
    let mut prev = None;
    for i in b.iter_ones() {
        if let Some((start, end)) = &mut prev {
            if *end + 1 == i {
                *end = i;
                continue;
            }
            if *start != *end {
                write!(out, "{}-{} ", *start, *end)?;
            } else {
                write!(out, "{} ", *start)?;
            }
            prev = None;
        } else {
            prev = Some((i, i));
        }
    }

    if let Some((start, end)) = prev {
        if start != end {
            write!(out, "{}-{} ", start, end)?;
        } else {
            write!(out, "{} ", start)?;
        }
    }

    Ok(())
}

#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct HexU64(pub u64);

impl FromStr for HexU64 {
    type Err = <u64 as FromStr>::Err;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        let value: u64 = if let Some(suffix) = s.strip_prefix("0x") {
            u64::from_str_radix(suffix, 0x10)?
        } else if let Some(suffix) = s.strip_prefix("0X") {
            u64::from_str_radix(suffix, 0x10)?
        } else {
            u64::from_str(s)?
        };
        Ok(Self(value))
    }
}

```