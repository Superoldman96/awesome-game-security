Project Path: arc_gmh5225_ghidriff_k53qdhhp

Source Tree:

```txt
arc_gmh5225_ghidriff_k53qdhhp
├── LICENSE
├── README.md
├── ghidriff
│   ├── __init__.py
│   ├── __main__.py
│   ├── correlators.py
│   ├── ghidra_diff_engine.py
│   ├── implied_matches.py
│   ├── markdown.py
│   ├── parser.py
│   ├── simple_diff.py
│   ├── structural_graph_diff.py
│   ├── utils.py
│   └── version_tracking_diff.py
├── setup.cfg
├── setup.py
└── tests
    ├── test_diff.py
    └── test_startup.py

```

`LICENSE`:

```
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.

```

`README.md`:

```md
<p align='center'>
<img src="https://github.com/clearbluejar/ghidriff/assets/3752074/170f1a54-24d9-4c8e-ac4d-3b5bea860750" height="300">
</p>


<p align="center">    
<img align="center" alt="GitHub Workflow Status (with event)" src="https://img.shields.io/github/actions/workflow/status/clearbluejar/ghidriff/pytest-devcontainer.yml?label=pytest&style=for-the-badge">
<img align="center" alt="PyPI - Downloads" src="https://img.shields.io/pypi/dm/ghidriff?color=yellow&label=PyPI%20downloads&style=for-the-badge">
<img align="center" src="https://img.shields.io/github/stars/clearbluejar/ghidriff?style=for-the-badge">

## Ghidriff - Ghidra Binary Diffing Engine
`ghidriff` provides a command-line binary diffing capability with a fresh take on diffing workflow and results.

It leverages the power of Ghidra's ProgramAPI and [FlatProgramAPI](https://ghidra.re/ghidra_docs/api/ghidra/program/flatapi/FlatProgramAPI.html) to find the *added*, *deleted*, and *modified* functions of two arbitrary binaries. It is written in Python3 using `pyhidra` to orchestrate Ghidra and `jpype` as the Python to Java interface to Ghidra.

Its primary use case is patch diffing. Its ability to perform a patch diff with a single command makes it ideal for automated analysis. The diffing results are stored in JSON and rendered in markdown (optionally side-by-side HTML). The markdown output promotes "social" diffing, as results are easy to publish in a gist or include in your next writeup or blog post. 

## High Level

```mermaid
flowchart LR

a(old binary - rpcrt4.dll-v1) --> b[GhidraDiffEngine]
c(new binary - rpcrt4.dll-v2) --> b

b --> e(Ghidra Project Files)
b --> diffs_output_dir

subgraph diffs_output_dir
    direction LR
    i(rpcrt4.dll-v1-v2.diff.md)
    h(rpcrt4.dll-v1-v2.diff.json)
    j(rpcrt4.dll-v1-v2.diff.side-by-side.html)
end
```

## Features

- Command Line (patch diffing workflow reduced to a single step)
- Highlights important changes in the TOC
- Fast - Can diff the full Windows kernel in less than a minute (after Ghidra analysis is complete)
- Enables Social Diffing
  - Beautiful Markdown Output  
  - Easily hosted in a GitHub or GitLab gist, blog, or anywhere markdown is supported
  - Visual Diff Graph Results
- Supports both unified and side by side diff results (unified is default)
- Provides unique Meta Diffs:
  - Binary Strings
  - Called
  - Calling
  - Binary Metadata
- Batteries Included
  - Docker support
  - Automated Testing
  - Ghidra (No license required)

## Design Goals

- Find all added, deleted, and modified functions
- Provide foundation for automation
- Simple, Fast, Accurate
- Resilient
- Extendable
- Easy sharing of results
- Social Diffing

## Powered by Ghidra

The heavy lifting of the binary analysis is done by Ghidra and the diffing is possible via Ghidra's Program API.  `ghidriff` provides a diffing [workflow](#engine), function matching, and resulting markdown and HTML diff output.

## Engine

<p align='center'>
<img src="https://user-images.githubusercontent.com/3752074/229976340-96394970-152f-4d88-9fe4-a46589b31c50.png" height="300">
</p>

> An "engine" is a self-contained, but externally-controllable, piece of code that encapsulates powerful logic designed to perform a specific type of work.

`ghidriff` provides a core base class [GhidraDiffEngine](ghidriff/ghidra_diff_engine.py) that can be extended to create your own binary diffing [implementations](#implementations).

The base class implements the first 3 steps of the Ghidra [headless workflow](https://github.com/clearbluejar/ghidra-python-vscode-devcontainer-skeleton#steps):
>1. **Create Ghidra Project** - Directory and collection of Ghidra project files and data
>2. **Import Binary to project** - Import one or more binaries to the project for analysis
>3. **Analyze Binary** - Ghidra will perform default binary analysis on each binary

The base class provides the abstract method [find_matches](ghidriff/ghidra_diff_engine.py) where the actual diffing (function matching) takes place.

## Extending ghidriff 

`ghidriff` can be used as is, but it offers developers the ability to extend the tool by implementing their own differ. The basic idea is create new diffing tools by implementing the `find_matches` method from the base class. 

```python
class NewDiffTool(GhidraDiffEngine):

    def __init__(self,verbose=False) -> None:
        super().__init__(verbose)

    @abstractmethod
    def find_matches(
            self,            
            old: Union[str, pathlib.Path],
            new: Union[str, pathlib.Path]
    ) -> dict:
        """My amazing differ"""

        # find added, deleted, and modified functions
        # <code goes here>

        return [unmatched, matched]
```

## Implementations

There are currently 3 diffing implementations, which also display the evolution of diffing for the project.

1. [SimpleDiff](ghidriff/simple_diff.py) - A simple diff implementation. "Simple" as in it relies mostly on known symbol names for matching. 
2. [StructualGraphDiff](ghidriff/structural_graph_diff.py) - A slightly more advanced differ, beginning to perform some more advanced hashing (such as Halvar's Structural Graph Comparison)
3. [VersionTrackingDiff](ghidriff/version_tracking_diff.py) - The latest differ, with several [correlators](ghidriff/correlators.py) (an algorithm used to score specific associations based on code, program flow, or any observable aspect of comparison) for function matching. **This one is fast.**

Each implementation leverages the base class, and implements `find_changes`.

#### Usage

```bash
usage: ghidriff [-h] [--engine {SimpleDiff,StructualGraphDiff,VersionTrackingDiff}] [-o OUTPUT_PATH] [--summary SUMMARY] [-p PROJECT_LOCATION] [-n PROJECT_NAME] [-s SYMBOLS_PATH] [--threaded | --no-threaded]
                [--force-analysis] [--force-diff] [--no-symbols] [--log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}] [--file-log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}]
                [--log-path LOG_PATH] [--va] [--max-ram-percent MAX_RAM_PERCENT] [--print-flags] [--jvm-args [JVM_ARGS]] [--sxs] [--max-section-funcs MAX_SECTION_FUNCS] [--md-title MD_TITLE]
                old new [new ...]

ghidriff - A Command Line Ghidra Binary Diffing Engine

positional arguments:
  old                   Path to old version of binary '/somewhere/bin.old'
  new                   Path to new version of binary '/somewhere/bin.new'. (For multiple new binaries add oldest to newest)

options:
  -h, --help            show this help message and exit
  --engine {SimpleDiff,StructualGraphDiff,VersionTrackingDiff}
                        The diff implementation to use. (default: VersionTrackingDiff)
  -o OUTPUT_PATH, --output-path OUTPUT_PATH
                        Output path for resulting diffs (default: ghidriffs)
  --summary SUMMARY     Add a summary diff if more than two bins are provided (default: False)

Ghidra Project Options:
  -p PROJECT_LOCATION, --project-location PROJECT_LOCATION
                        Ghidra Project Path (default: .ghidra_projects)
  -n PROJECT_NAME, --project-name PROJECT_NAME
                        Ghidra Project Name (default: ghidriff)
  -s SYMBOLS_PATH, --symbols-path SYMBOLS_PATH
                        Ghidra local symbol store directory (default: .symbols)

Engine Options:
  --threaded, --no-threaded
                        Use threading during import, analysis, and diffing. Recommended (default: True)
  --force-analysis      Force a fresh binary analysis each run (default: False)
  --force-diff          Force binary diff (ignore arch/symbols mismatch) (default: False)
  --no-symbols          Turn off symbols for analysis (default: False)
  --log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}
                        Set console log level (default: INFO)
  --file-log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}
                        Set log file level (default: INFO)
  --log-path LOG_PATH   Set ghidriff log path. (default: ghidriff.log)
  --va, --verbose-analysis
                        Verbose logging for analysis step. (default: False)

JVM Options:
  --max-ram-percent MAX_RAM_PERCENT
                        Set JVM Max Ram % of host RAM (default: 60.0)
  --print-flags         Print JVM flags at start (default: False)
  --jvm-args [JVM_ARGS]
                        JVM args to add at start (default: None)

Markdown Options:
  --sxs                 Include side by side code diff (default: False)
  --max-section-funcs MAX_SECTION_FUNCS
                        Max number of functions to display per section. (default: 200)
  --md-title MD_TITLE   Overwrite default title for markdown diff (default: None)
```

There are quite a few options here, and some complexity. Generally you can succeed with the defaults, but you can override the defaults as needed. One example might be to increase the JVM RAM used to run Ghidra to enable faster analysis of large binaries (`--max-ram-percent 80`). See help for details of other options. 

## Quick Start Environment Setup

1. [Download](https://github.com/NationalSecurityAgency/ghidra/releases) and [install Ghidra](https://htmlpreview.github.io/?https://github.com/NationalSecurityAgency/ghidra/blob/stable/GhidraDocs/InstallationGuide.html#Install).
2. Set Ghidra Environment Variable `GHIDRA_INSTALL_DIR` to Ghidra install location.
3. Pip install `ghidriff`

### Windows

```powershell
PS C:\Users\user> [System.Environment]::SetEnvironmentVariable('GHIDRA_INSTALL_DIR','C:\ghidra_10.2.3_PUBLIC_20230208\ghidra_10.2.3_PUBLIC')
PS C:\Users\user> pip install ghidriff
```
### Linux / Mac

```bash
export GHIDRA_INSTALL_DIR="/path/to/ghidra/"
pip install ghidriff
```

### Ghidriff in a Box - Devcontainer / Docker

Don't want to install Ghidra and Java on your host?

#### For Docker command-line diffing

```
docker pull ghcr.io/clearbluejar/ghidra-python:latest
pip install ghidriff
```

#### For development

Use the [.devcontainer](.devcontainer) in this repo. If you don't know how, follow the detailed instructions here: [ghidra-python-vscode-devcontainer-skeleton quick setup](https://github.com/clearbluejar/ghidra-python-vscode-devcontainer-skeleton#quick-start-setup---dev-container--best-option).



## Sample Usage

### Diffing a full Windows Kernel 

#### Download two versions of the kernel (older and latest binary):

```bash
wget https://msdl.microsoft.com/download/symbols/ntoskrnl.exe/F7E31BA91047000/ntoskrnl.exe -O ntoskrnl.exe.10.0.22621.1344
wget https://msdl.microsoft.com/download/symbols/ntoskrnl.exe/17B6B7221047000/ntoskrnl.exe -O ntoskrnl.exe.10.0.22621.1413
```

<details><summary>Console Output:</summary>

```console 
vscode ➜ /workspaces/ghidriff (main) $ wget https://msdl.microsoft.com/download/symbols/ntoskrnl.exe/F7E31BA91047000/ntoskrnl.exe -O ntoskrnl.exe.10.0.22621.1344
--2023-05-17 03:18:40--  https://msdl.microsoft.com/download/symbols/ntoskrnl.exe/F7E31BA91047000/ntoskrnl.exe
Resolving msdl.microsoft.com (msdl.microsoft.com)... 204.79.197.219
Connecting to msdl.microsoft.com (msdl.microsoft.com)|204.79.197.219|:443... connected.
HTTP request sent, awaiting response... 302 Found
Could not parse String-Transport-Security header
Location: https://vsblobprodscussu5shard72.blob.core.windows.net/b-4712e0edc5a240eabf23330d7df68e77/8BFC691F50434EC2DC87BBDFC06A6A5FBACE992E60062F9C8CE829F58E3BCFB300.blob?sv=2019-07-07&sr=b&si=1&sig=Kgrvf90Kc15ac%2FtHsgPPj9ztxxTfkQ0yHGQh8dLDwQs%3D&spr=https&se=2023-05-18T03%3A32%3A47Z&rscl=x-e2eid-420cea82-598a4a00-a990abf8-919be2ff-session-5e9eb5eb-195146cb-b123c222-30eef52e [following]
--2023-05-17 03:18:40--  https://vsblobprodscussu5shard72.blob.core.windows.net/b-4712e0edc5a240eabf23330d7df68e77/8BFC691F50434EC2DC87BBDFC06A6A5FBACE992E60062F9C8CE829F58E3BCFB300.blob?sv=2019-07-07&sr=b&si=1&sig=Kgrvf90Kc15ac%2FtHsgPPj9ztxxTfkQ0yHGQh8dLDwQs%3D&spr=https&se=2023-05-18T03%3A32%3A47Z&rscl=x-e2eid-420cea82-598a4a00-a990abf8-919be2ff-session-5e9eb5eb-195146cb-b123c222-30eef52e
Resolving vsblobprodscussu5shard72.blob.core.windows.net (vsblobprodscussu5shard72.blob.core.windows.net)... 20.209.34.36
Connecting to vsblobprodscussu5shard72.blob.core.windows.net (vsblobprodscussu5shard72.blob.core.windows.net)|20.209.34.36|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 11990400 (11M) [application/octet-stream]
Saving to: ‘ntoskrnl.exe.10.0.22621.1344’

ntoskrnl.exe.10.0.22621.1344                       100%[===============================================================================================================>]  11.43M  2.47MB/s    in 5.5s    

2023-05-17 03:18:46 (2.08 MB/s) - ‘ntoskrnl.exe.10.0.22621.1344’ saved [11990400/11990400]

vscode ➜ /workspaces/ghidriff (main) $ wget https://msdl.microsoft.com/download/symbols/ntoskrnl.exe/17B6B7221047000/ntoskrnl.exe -O ntoskrnl.exe.10.0.22621.1413
--2023-05-17 03:18:58--  https://msdl.microsoft.com/download/symbols/ntoskrnl.exe/17B6B7221047000/ntoskrnl.exe
Resolving msdl.microsoft.com (msdl.microsoft.com)... 204.79.197.219
Connecting to msdl.microsoft.com (msdl.microsoft.com)|204.79.197.219|:443... connected.
HTTP request sent, awaiting response... 302 Found
Could not parse String-Transport-Security header
Location: https://vsblobprodscussu5shard75.blob.core.windows.net/b-4712e0edc5a240eabf23330d7df68e77/D946523F2726056CD289008C977D02C0C0FBBCBB89D9FA40ADBB42CDE8D5022A00.blob?sv=2019-07-07&sr=b&si=1&sig=KfYz9cB7cUPO9JVo0U8eIj0etpASEWOyvCv5NkwVkfw%3D&spr=https&se=2023-05-18T03%3A50%3A53Z&rscl=x-e2eid-4960dee3-47d94aa4-a2207913-b73825a4-session-2879fa10-75774ef4-93e39015-3be72abb [following]
--2023-05-17 03:18:59--  https://vsblobprodscussu5shard75.blob.core.windows.net/b-4712e0edc5a240eabf23330d7df68e77/D946523F2726056CD289008C977D02C0C0FBBCBB89D9FA40ADBB42CDE8D5022A00.blob?sv=2019-07-07&sr=b&si=1&sig=KfYz9cB7cUPO9JVo0U8eIj0etpASEWOyvCv5NkwVkfw%3D&spr=https&se=2023-05-18T03%3A50%3A53Z&rscl=x-e2eid-4960dee3-47d94aa4-a2207913-b73825a4-session-2879fa10-75774ef4-93e39015-3be72abb
Resolving vsblobprodscussu5shard75.blob.core.windows.net (vsblobprodscussu5shard75.blob.core.windows.net)... 20.209.34.36
Connecting to vsblobprodscussu5shard75.blob.core.windows.net (vsblobprodscussu5shard75.blob.core.windows.net)|20.209.34.36|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 11990336 (11M) [application/octet-stream]
Saving to: ‘ntoskrnl.exe.10.0.22621.1413’

ntoskrnl.exe.10.0.22621.1413                       100%[===============================================================================================================>]  11.43M  1.02MB/s    in 12s     

2023-05-17 03:19:11 (1004 KB/s) - ‘ntoskrnl.exe.10.0.22621.1413’ saved [11990336/11990336]
```

</details>

#### Run ghidriff:

```bash
ghidriff ntoskrnl.exe.10.0.22621.1344 ntoskrnl.exe.10.0.22621.1413
```

<details><summary>Console Output</summary>

```console
(.env) vscode ➜ /workspaces/ghidriff (main) $ ghidriff ntoskrnl.exe.10.0.22621.1344 ntoskrnl.exe.10.0.22621.1413
INFO | ghidriff | Init Ghidra Diff Engine...
INFO | ghidriff | Engine Console Log: INFO
INFO | ghidriff | Engine File Log:  .ghidriffs/ghidriff.log INFO
INFO | ghidriff | Starting Ghidra...
INFO  Using log config file: jar:file:/ghidra/Ghidra/Framework/Generic/lib/Generic.jar!/generic.log4j.xml (LoggingInitialization)  
INFO  Using log file: /workspaces/ghidriff/.ghidriffs/ghidriff.log (LoggingInitialization)  
INFO  Loading user preferences: /home/vscode/.ghidra/.ghidra_10.2.3_PUBLIC/preferences (Preferences)  
INFO  Class search complete (716 ms) (ClassSearcher)  
INFO  Initializing SSL Context (SSLContextInitializer)  
INFO  Initializing Random Number Generator... (SecureRandomFactory)  
INFO  Random Number Generator initialization complete: NativePRNGNonBlocking (SecureRandomFactory)  
INFO  Trust manager disabled, cacerts have not been set (ApplicationTrustManagerFactory)  
INFO | ghidriff | GHIDRA_INSTALL_DIR: /ghidra
INFO | ghidriff | GHIDRA 10.2.3  Build Date: 2023-Feb-08 1242 EST Release: PUBLIC
INFO | ghidriff | Engine Args:
INFO | ghidriff |       old:                ['ntoskrnl.exe.10.0.22621.1344']
INFO | ghidriff |       new:                [['ntoskrnl.exe.10.0.22621.1413']]
INFO | ghidriff |       engine:             VersionTrackingDiff
INFO | ghidriff |       output_path:        .ghidriffs
INFO | ghidriff |       summary:            False
INFO | ghidriff |       project_location:   .ghidra_projects
INFO | ghidriff |       project_name:       ghidriff
INFO | ghidriff |       symbols_path:       .symbols
INFO | ghidriff |       threaded:           True
INFO | ghidriff |       force_analysis:     False
INFO | ghidriff |       force_diff:         False
INFO | ghidriff |       no_symbols:         False
INFO | ghidriff |       log_level:          INFO
INFO | ghidriff |       file_log_level:     INFO
INFO | ghidriff |       log_path:           ghidriff.log
INFO | ghidriff |       va:                 False
INFO | ghidriff |       max_ram_percent:    60.0
INFO | ghidriff |       print_flags:        False
INFO | ghidriff |       jvm_args:           None
INFO | ghidriff |       side_by_side:       False
INFO | ghidriff |       max_section_funcs:  200
INFO | ghidriff |       md_title:           None
INFO | ghidriff | Setting Up Ghidra Project...
INFO  Creating project: /workspaces/ghidriff/.ghidra_projects/ghidriff-ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413/ghidriff-ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413 (DefaultProject)  
INFO | ghidriff | Created project: ghidriff-ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413
INFO | ghidriff | Project Location: /workspaces/ghidriff/.ghidra_projects/ghidriff-ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413
INFO | ghidriff | Importing ntoskrnl.exe.10.0.22621.1344
INFO  Starting cache cleanup: /tmp/vscode-Ghidra/fscache2 (FileCacheMaintenanceDaemon)  
INFO  Finished cache cleanup, estimated storage used: 0 (FileCacheMaintenanceDaemon)  
INFO  Using Loader: Portable Executable (PE) (AutoImporter)  
INFO | ghidriff | Importing ntoskrnl.exe.10.0.22621.1413
INFO  Using Loader: Portable Executable (PE) (AutoImporter)  
INFO | ghidriff | Setting up Symbol Server for symbols...
INFO | ghidriff | path: .symbols level: 1
INFO | ghidriff | Symbol Server Configured path: SymbolServerService:
        symbolStore: LocalSymbolStore: [ rootDir: /workspaces/ghidriff/.symbols, storageLevel: -1],
        symbolServers:
                HttpSymbolServer: [ url: https://msdl.microsoft.com/download/symbols/, storageLevel: -1]
                HttpSymbolServer: [ url: https://chromium-browser-symsrv.commondatastorage.googleapis.com/, storageLevel: -1]
                HttpSymbolServer: [ url: https://symbols.mozilla.org/, storageLevel: -1]
                HttpSymbolServer: [ url: https://software.intel.com/sites/downloads/symbols/, storageLevel: -1]
                HttpSymbolServer: [ url: https://driver-symbols.nvidia.com/, storageLevel: -1]
                HttpSymbolServer: [ url: https://download.amd.com/dir/bin/, storageLevel: -1]
INFO  Connecting to https://msdl.microsoft.com/download/symbols/ (ConsoleTaskMonitor)  
INFO  Success (ConsoleTaskMonitor)  
INFO  Storing ntkrnlmp.pdb in local symbol store (12.66MB) (ConsoleTaskMonitor)  
INFO | ghidriff | Pdb stored at: /workspaces/ghidriff/.symbols/ntkrnlmp.pdb/FB0913AF0585F234BD64A64A87C62DB11/ntkrnlmp.pdb
INFO  Connecting to https://msdl.microsoft.com/download/symbols/ (ConsoleTaskMonitor)  
INFO  Success (ConsoleTaskMonitor)  
INFO  Storing ntkrnlmp.pdb in local symbol store (12.66MB) (ConsoleTaskMonitor)  
INFO | ghidriff | Pdb stored at: /workspaces/ghidriff/.symbols/ntkrnlmp.pdb/797E613DB16DB6C0E57795A0CB03F4711/ntkrnlmp.pdb
INFO | ghidriff | Program: /ntoskrnl.exe.10.0.22621.1344 imported: True has_pdb: True pdb_loaded: False analyzed False
INFO | ghidriff | Program: /ntoskrnl.exe.10.0.22621.1413 imported: True has_pdb: True pdb_loaded: False analyzed False
INFO | ghidriff | Starting analysis for 2 binaries
INFO | ghidriff | Analyzing: ntoskrnl.exe.10.0.22621.1413 - .ProgramDB
INFO | ghidriff | Analyzing: ntoskrnl.exe.10.0.22621.1344 - .ProgramDB
WARNING| ghidriff | Turning off 'Shared Return Calls' for ntoskrnl.exe.10.0.22621.1344 - .ProgramDB
INFO | ghidriff | Starting Ghidra analysis of ntoskrnl.exe.10.0.22621.1344 - .ProgramDB...
INFO  PDB analyzer parsing file: /workspaces/ghidriff/.symbols/ntkrnlmp.pdb/FB0913AF0585F234BD64A64A87C62DB11/ntkrnlmp.pdb (PdbUniversalAnalyzer)  
WARNING| ghidriff | Turning off 'Shared Return Calls' for ntoskrnl.exe.10.0.22621.1413 - .ProgramDB
INFO | ghidriff | Starting Ghidra analysis of ntoskrnl.exe.10.0.22621.1413 - .ProgramDB...
INFO  PDB analyzer parsing file: /workspaces/ghidriff/.symbols/ntkrnlmp.pdb/797E613DB16DB6C0E57795A0CB03F4711/ntkrnlmp.pdb (PdbUniversalAnalyzer)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/<unnamed-tag_00001117> (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/<unnamed-tag_0000111B> (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/<unnamed-tag_0000111F> (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_WMI_LOGGER_CONTEXT (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_PPM_PLATFORM_STATE (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_IOP_IRP_EXTENSION (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_EX_HEAP_POOL_NODE (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_BLOB (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_MMPAGING_FILE (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_MMCLONE_DESCRIPTOR (CppCompositeType)  
WARN  PDB STRUCTURE reconstruction failed to align /ntkrnlmp.pdb/_KUSER_SHARED_DATA (CppCompositeType)  
INFO  Resolve time: 1939 mS (DefaultPdbApplicator)  
INFO  resolveCount: 3644 (DefaultPdbApplicator)  
INFO  Resolve time: 1854 mS (DefaultPdbApplicator)  
INFO  resolveCount: 3644 (DefaultPdbApplicator)  
WARN  Decompiling 1402efc70, pcode error at 14000000c: Unable to resolve constructor at 14000000c (DecompileCallback)  
WARN  Decompiling 1402efc70, pcode error at 14000000c: Unable to resolve constructor at 14000000c (DecompileCallback)  
WARN  Decompiling 1402efc70, pcode error at 14000000c: Unable to resolve constructor at 14000000c (DecompileCallback)  
INFO  Packed database cache: /tmp/vscode-Ghidra/packed-db-cache (PackedDatabaseCache)  

 
INFO  -----------------------------------------------------
    ASCII Strings                              0.137 secs
    Apply Data Archives                        3.295 secs
    Call Convention ID                         2.037 secs
    Call-Fixup Installer                       0.998 secs
    Create Address Tables                      0.021 secs
    Create Address Tables - One Time           5.159 secs
    Create Function                            8.858 secs
    Data Reference                            17.246 secs
    Decompiler Switch Analysis               266.328 secs
    Demangler Microsoft                        3.514 secs
    Disassemble                                0.232 secs
    Disassemble Entry Points                  58.448 secs
    Disassemble Entry Points - One Time        1.802 secs
    Embedded Media                             0.123 secs
    External Entry References                  0.125 secs
    Function ID                              114.335 secs
    Function Start Search                      0.859 secs
    Non-Returning Functions - Discovered      25.594 secs
    Non-Returning Functions - Known            0.144 secs
    PDB Universal                            168.601 secs
    Reference                                  6.969 secs
    Scalar Operand References                 91.615 secs
    Shared Return Calls                        4.718 secs
    Stack                                    291.121 secs
    Subroutine References                     14.672 secs
    Subroutine References - One Time           0.027 secs
    Windows x86 PE Exception Handling          0.470 secs
    Windows x86 PE RTTI Analyzer               0.091 secs
    Windows x86 Thread Environment Block (TEB) Analyzer     0.115 secs
    WindowsResourceReference                   0.413 secs
    x86 Constant Reference Analyzer          261.728 secs
-----------------------------------------------------
     Total Time   1349 secs
-----------------------------------------------------
 (AutoAnalysisManager)  
INFO  -----------------------------------------------------
    ASCII Strings                              3.249 secs
    Apply Data Archives                        3.290 secs
    Call Convention ID                         1.984 secs
    Call-Fixup Installer                       0.947 secs
    Create Address Tables                      0.007 secs
    Create Address Tables - One Time           5.178 secs
    Create Function                            8.855 secs
    Data Reference                            17.320 secs
    Decompiler Switch Analysis               264.962 secs
    Demangler Microsoft                        3.649 secs
    Disassemble                                0.468 secs
    Disassemble Entry Points                  58.480 secs
    Disassemble Entry Points - One Time        1.805 secs
    Embedded Media                             0.102 secs
    External Entry References                  0.120 secs
    Function ID                              114.285 secs
    Function Start Search                      0.987 secs
    Non-Returning Functions - Discovered      25.826 secs
    Non-Returning Functions - Known            0.034 secs
    PDB Universal                            169.189 secs
    Reference                                  6.714 secs
    Scalar Operand References                 91.422 secs
    Shared Return Calls                        4.760 secs
    Stack                                    291.137 secs
    Subroutine References                     14.711 secs
    Subroutine References - One Time           0.018 secs
    Windows x86 PE Exception Handling          0.463 secs
    Windows x86 PE RTTI Analyzer               0.089 secs
    Windows x86 Thread Environment Block (TEB) Analyzer     0.119 secs
    WindowsResourceReference                   0.403 secs
    x86 Constant Reference Analyzer          262.810 secs
-----------------------------------------------------
     Total Time   1353 secs
-----------------------------------------------------
 (AutoAnalysisManager)  
INFO | ghidriff | Analysis for ghidriff-ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413:/ntoskrnl.exe.10.0.22621.1413 complete
INFO | ghidriff | Analysis for ghidriff-ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413:/ntoskrnl.exe.10.0.22621.1344 complete
INFO | ghidriff | Diffing bins: ntoskrnl.exe.10.0.22621.1344 - ntoskrnl.exe.10.0.22621.1413
INFO | ghidriff | Setup 16 decompliers
INFO | ghidriff | Loaded old program: ntoskrnl.exe.10.0.22621.1344
INFO | ghidriff | Loaded new program: ntoskrnl.exe.10.0.22621.1413
INFO | ghidriff | p1 sym count: reported: 244603 analyzed: 16772
INFO | ghidriff | p2 sym count: reported: 244606 analyzed: 16809
INFO | ghidriff | Found unmatched: 65 matched: 16758 symbols
INFO  Hashing symbols in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing symbols in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Eliminate non-unique matches (ConsoleTaskMonitor)  
INFO  Finding symbol matches (ConsoleTaskMonitor)  
INFO | ghidriff | Exec time: 2.1672 secs
INFO | ghidriff | Match count 54939
INFO | ghidriff | Counter({('SymbolsHash',): 27893})
INFO | ghidriff | Running correlator: ExactBytesFunctionHasher
INFO | ghidriff | name: ExactBytesFunctionHasher hasher: ghidra.app.plugin.match.ExactBytesFunctionHasher@7167d81b one_to_one: True one_to_many: False
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | ExactBytesFunctionHasher Exec time: 0.8299 secs
INFO | ghidriff | Match count: 100
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('ExactBytesFunctionHasher',): 100})
INFO | ghidriff | Running correlator: ExactInstructionsFunctionHasher
INFO | ghidriff | name: ExactInstructionsFunctionHasher hasher: ghidra.app.plugin.match.ExactInstructionsFunctionHasher@3c9cfcde one_to_one: True one_to_many: False
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | ExactInstructionsFunctionHasher Exec time: 0.4906 secs
INFO | ghidriff | Match count: 123
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('ExactInstructionsFunctionHasher',): 123, ('ExactBytesFunctionHasher',): 100})
INFO | ghidriff | Running correlator: StructuralGraphExactHash
INFO | ghidriff | name: StructuralGraphExactHash hasher: <jpype._jproxy.proxy.StructuralGraphExactHasher object at 0xffff26c53bf0> one_to_one: True one_to_many: False
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | StructuralGraphExactHash Exec time: 1.3213 secs
INFO | ghidriff | Match count: 0
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('ExactInstructionsFunctionHasher',): 123, ('ExactBytesFunctionHasher',): 100})
INFO | ghidriff | Running correlator: ExactMnemonicsFunctionHasher
INFO | ghidriff | name: ExactMnemonicsFunctionHasher hasher: ghidra.app.plugin.match.ExactMnemonicsFunctionHasher@7533923b one_to_one: True one_to_many: False
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | ExactMnemonicsFunctionHasher Exec time: 2.5697 secs
INFO | ghidriff | Match count: 0
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('ExactInstructionsFunctionHasher',): 123, ('ExactBytesFunctionHasher',): 100})
INFO | ghidriff | Running correlator: BulkInstructionHash
INFO | ghidriff | name: BulkInstructionHash hasher: <jpype._jproxy.proxy.BulkInstructionsHasher object at 0xffff26c53b50> one_to_one: True one_to_many: False
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | BulkInstructionHash Exec time: 1.1462 secs
INFO | ghidriff | Match count: 2
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('ExactInstructionsFunctionHasher',): 123, ('ExactBytesFunctionHasher',): 100, ('BulkInstructionHash',): 2})
INFO | ghidriff | Running correlator: StructuralGraphHash
INFO | ghidriff | name: StructuralGraphHash hasher: <jpype._jproxy.proxy.StructuralGraphHasher object at 0xffff26c53ab0> one_to_one: True one_to_many: True
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | StructuralGraphHash Exec time: 0.1894 secs
INFO | ghidriff | Match count: 693
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('StructuralGraphHash',): 693, ('ExactInstructionsFunctionHasher',): 123, ('ExactBytesFunctionHasher',): 100, ('BulkInstructionHash',): 2})
INFO | ghidriff | Running correlator: BulkBasicBlockMnemonicHash
INFO | ghidriff | name: BulkBasicBlockMnemonicHash hasher: <jpype._jproxy.proxy.BulkBasicBlockMnemonicHasher object at 0xffff26c53a10> one_to_one: True one_to_many: True
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | BulkBasicBlockMnemonicHash Exec time: 0.1846 secs
INFO | ghidriff | Match count: 0
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('StructuralGraphHash',): 693, ('ExactInstructionsFunctionHasher',): 123, ('ExactBytesFunctionHasher',): 100, ('BulkInstructionHash',): 2})
INFO | ghidriff | Running correlator: SigCallingCalledHasher
INFO | ghidriff | name: SigCallingCalledHasher hasher: <jpype._jproxy.proxy.SigCallingCalledHasher object at 0xffff26c53970> one_to_one: True one_to_many: False
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1344 (ConsoleTaskMonitor)  
INFO  Hashing functions in ntoskrnl.exe.10.0.22621.1413 (ConsoleTaskMonitor)  
INFO  Finding function matches (ConsoleTaskMonitor)  
INFO | ghidriff | SigCallingCalledHasher Exec time: 0.1398 secs
INFO | ghidriff | Match count: 0
INFO | ghidriff | Counter({('SymbolsHash',): 27893, ('StructuralGraphHash',): 693, ('ExactInstructionsFunctionHasher',): 123, ('ExactBytesFunctionHasher',): 100, ('BulkInstructionHash',): 2})
INFO | ghidriff | p1 missing = 1
INFO | ghidriff | p2 missing = 1
INFO | ghidriff | Deduping symbols and functions...
INFO | ghidriff | Sorting symbols and strings...
INFO | ghidriff | Sorting functions...
INFO | ghidriff | Starting esym lookups for 71 symbols using 8 threads
INFO | ghidriff | Completed 4 at 5%
INFO | ghidriff | Completed 8 at 11%
INFO | ghidriff | Completed 12 at 16%
INFO | ghidriff | Completed 16 at 22%
INFO | ghidriff | Completed 20 at 28%
INFO | ghidriff | Completed 24 at 33%
INFO | ghidriff | Completed 28 at 39%
INFO | ghidriff | Completed 32 at 45%
INFO | ghidriff | Completed 36 at 50%
INFO | ghidriff | Completed 40 at 56%
INFO | ghidriff | Completed 44 at 61%
INFO | ghidriff | Completed 48 at 67%
INFO | ghidriff | Completed 52 at 73%
INFO | ghidriff | Completed 56 at 78%
INFO | ghidriff | Completed 60 at 84%
INFO | ghidriff | Completed 64 at 90%
INFO | ghidriff | Completed 68 at 95%
INFO | ghidriff | Finished diffing old program: ntoskrnl.exe.10.0.22621.1344
INFO | ghidriff | Finished diffing program: ntoskrnl.exe.10.0.22621.1413
INFO | ghidriff | {
  "added_funcs_len": 1,
  "deleted_funcs_len": 1,
  "modified_funcs_len": 11,
  "added_symbols_len": 12,
  "deleted_symbols_len": 8,
  "diff_time": 86.45361375808716,
  "deleted_strings_len": 6,
  "added_strings_len": 39,
  "match_types": {
    "SymbolsHash": 27893,
    "ExactBytesFunctionHasher": 100,
    "ExactInstructionsFunctionHasher": 123,
    "BulkInstructionHash": 2,
    "StructuralGraphHash": 693
  },
  "items_to_process": 33,
  "diff_types": {
    "code": 5,
    "length": 5,
    "refcount": 7,
    "calling": 6,
    "address": 6,
    "called": 3
  },
  "unmatched_funcs_len": 2,
  "total_funcs_len": 60664,
  "matched_funcs_len": 60662,
  "matched_funcs_with_code_changes_len": 5,
  "matched_funcs_with_non_code_changes_len": 6,
  "matched_funcs_no_changes_len": 60651,
  "match_func_similarity_percent": "99.9819%",
  "func_match_overall_percent": "99.9967%"
}
INFO | ghidriff | Writing md diff...
INFO | ghidriff | Generating markdown from {'added_funcs_len': 1, 'deleted_funcs_len': 1, 'modified_funcs_len': 11, 'added_symbols_len': 12, 'deleted_symbols_len': 8, 'diff_time': 86.45361375808716, 'deleted_strings_len': 6, 'added_strings_len': 39, 'match_types': Counter({'SymbolsHash': 27893, 'StructuralGraphHash': 693, 'ExactInstructionsFunctionHasher': 123, 'ExactBytesFunctionHasher': 100, 'BulkInstructionHash': 2}), 'items_to_process': 33, 'diff_types': Counter({'refcount': 7, 'calling': 6, 'address': 6, 'code': 5, 'length': 5, 'called': 3}), 'unmatched_funcs_len': 2, 'total_funcs_len': 60664, 'matched_funcs_len': 60662, 'matched_funcs_with_code_changes_len': 5, 'matched_funcs_with_non_code_changes_len': 6, 'matched_funcs_no_changes_len': 60651, 'match_func_similarity_percent': '99.9819%', 'func_match_overall_percent': '99.9967%'}
INFO | ghidriff | Known Command line: python ghidriff --project-location .ghidra_projects --project-name ghidriff --symbols-path .symbols --threaded --log-level INFO --file-log-level INFO --log-path ghidriff.log --max-ram-percent 60.0 --max-section-funcs 200 ntoskrnl.exe.10.0.22621.1344 ntoskrnl.exe.10.0.22621.1413
INFO | ghidriff | Extra Command line: --engine VersionTrackingDiff --output-path .ghidriffs
INFO | ghidriff | Writing pdiff json...
INFO | ghidriff | Wrote .ghidriffs/ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413_diff.md
INFO | ghidriff | Wrote .ghidriffs/json/ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413_diff.json
```
</details>

#### Analyze the Diff

Results in this beatiful markdown: [ntoskrnl.exe.10.0.22621.1344-ntoskrnl.exe.10.0.22621.1413.diff.md](https://gist.github.com/clearbluejar/b95ae854a92ee917cd0b5c7055b60282)

See if you can figure out what function was patched for [CVE-2023-2342](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2023-23420).

- Details of [CVE-2023-2342](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2023-23420) can be found here: [https://bugs.chromium.org/p/project-zero/issues/detail?id=2392](https://bugs.chromium.org/p/project-zero/issues/detail?id=2392)

Prefer a side by side diff? Try out `ghidriff`'s custom html viewer. https://diffpreview.github.io/?b95ae854a92ee917cd0b5c7055b60282

### Diffing CVE-2023-21768

Details of the CVE-2023-21768 (detailed in this blog [post](https://securityintelligence.com/posts/patch-tuesday-exploit-wednesday-pwning-windows-ancillary-function-driver-winsock/)). What if you wanted to repeat this patch diff with `ghidriff`?

1. Download two versions of `AFD.sys` (vulnerable and patched):

```bash
wget https://msdl.microsoft.com/download/symbols/afd.sys/0C5C6994A8000/afd.sys -O afd.sys.x64.10.0.22621.1028
wget https://msdl.microsoft.com/download/symbols/afd.sys/50989142A9000/afd.sys -O afd.sys.x64.10.0.22621.1415
```

2.  Run `ghidriff`:

```bash
ghidriff afd.sys.x64.10.0.22621.1028 afd.sys.x64.10.0.22621.1415
```

3. Review results

 The diff results are posted in this GitHub [gist](https://gist.github.com/clearbluejar/f6fecbc507a9f1a92c9231e3db7ef40d). The vulnerable function  `AfdNotifyRemoveIoCompletion` was identified here with a [single line change](https://gist.github.com/clearbluejar/f6fecbc507a9f1a92c9231e3db7ef40d#afdnotifyremoveiocompletion-diff).

 Want to see the entire diff in a side by side? https://diffpreview.github.io/?f6fecbc507a9f1a92c9231e3db7ef40d or jump to the [single line change](https://diffpreview.github.io/?f6fecbc507a9f1a92c9231e3db7ef40d#d2h-703858:~:text=ProbeForWrite(*(undefined8%20*)(param_3%20%2B%200x18)%2C4%2C4)%3B)

```

`ghidriff/__init__.py`:

```py
__version__ = '0.5.0'
__author__ = 'clearbluejar'

# Expose API
from .ghidra_diff_engine import GhidraDiffEngine
from .version_tracking_diff import VersionTrackingDiff
from .simple_diff import SimpleDiff
from .structural_graph_diff import StructualGraphDiff
from .parser import get_parser,get_engine_classes

__all__ = [
    "GhidraDiffEngine", "SimpleDiff", "StructualGraphDiff", "VersionTrackingDiff", "get_parser","get_engine_classes"
]

```

`ghidriff/__main__.py`:

```py
from argparse import ArgumentParser
from pathlib import Path
import json

from ghidriff import GhidraDiffEngine
from .parser import get_parser,get_engine_classes

def main():
    """
    ghidriff - GhidraDiffEngine module main function
    """

    parser : ArgumentParser = get_parser()

    GhidraDiffEngine.add_ghidra_args_to_parser(parser)

    args = parser.parse_args()

    output_path = Path(args.output_path)
    output_path.mkdir(exist_ok=True, parents=True)

    if args.log_path == 'None':
        engine_log_path = None
    if args.log_path == parser.get_default('log_path'):
        engine_log_path = output_path / parser.get_default('log_path')
    else:
        engine_log_path = Path(args.log_path)

    binary_paths = args.old + [bin for sublist in args.new for bin in sublist]

    binary_paths = [Path(path) for path in binary_paths]

    if any([not path.exists() for path in binary_paths]):
        missing_bins = [f'{path.name}' for path in binary_paths if not path.exists()]
        raise FileNotFoundError(f"Missing Bins: {' '.join(missing_bins)}")

    project_name = f'{args.project_name}-{binary_paths[0].name}-{binary_paths[-1].name}'

    engines = get_engine_classes()
    DiffEngine: GhidraDiffEngine = engines[args.engine]

    d: GhidraDiffEngine = DiffEngine(args=args,
                                     verbose=True,
                                     threaded=args.threaded,
                                     max_ram_percent=args.max_ram_percent,
                                     print_jvm_flags=args.print_flags,
                                     jvm_args=args.jvm_args,
                                     force_analysis=args.force_analysis,
                                     force_diff=args.force_diff,
                                     verbose_analysis=args.va,
                                     no_symbols=args.no_symbols,
                                     engine_log_path=engine_log_path,
                                     engine_log_level=args.log_level,
                                     engine_file_log_level=args.file_log_level
                                     )

    d.setup_project(binary_paths, args.project_location, project_name, args.symbols_path)

    d.analyze_project()

    diffs = []

    # pair up binaries with the n-1 version
    for i in range(len(binary_paths)-1):
        diffs.append((binary_paths[i], binary_paths[i+1]))

    # add a diff of the first and last binary for full coverage
    if not binary_paths[1] == binary_paths[-1] and args.summary:
        diffs.append((binary_paths[0], binary_paths[-1]))

    for diff in diffs:
        pdiff = d.diff_bins(diff[0], diff[1])
        pdiff_json = json.dumps(pdiff)

        d.validate_diff_json(pdiff_json)

        diff_name = f"{Path(diff[0]).name}-{Path(diff[1]).name}.ghidriff"

        d.dump_pdiff_to_path(diff_name,
                             pdiff,
                             output_path,
                             side_by_side=args.side_by_side,
                             max_section_funcs=args.max_section_funcs,
                             md_title=args.md_title)


if __name__ == "__main__":
    main()

```

`ghidriff/correlators.py`:

```py
from functools import lru_cache
import uuid
from typing import List, Tuple, TYPE_CHECKING

from jpype import JImplements, JOverride
from ghidra.app.plugin.match import FunctionHasher


if TYPE_CHECKING:
    import ghidra
    from ghidra_builtins import *

@JImplements(FunctionHasher, deferred=True)
class StructuralGraphHasher:
    """
    Hash function using Graph Centric Comparison of function control flow graphs (thank you 2004 halvar)
    Hash calculates a 3-tuple measurement from each node (function cfg) in the program
    (num_basic_blocks, num_edges_of_blocks, num_call_subfunctions)
    There are several other properties (length, refcount, paramcount) added to this hash, as it is meant to be run with one_to_many = True
    Based on "Structural Comparison of Executable Objects" by Halvar Flake
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'StructuralGraphHash'

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        from ghidra.program.model.block import BasicBlockModel
        from ghidra.program.model.symbol import SourceType
        from ghidra.program.model.symbol import SymbolUtilities

        # graph structure vars
        num_basic_blocks = 0
        num_edges_of_blocks = 0
        num_call_subfunctions = 0

        sym = func.symbol

        # # skip func like FUN_ and LAB_, or local symbols like switch
        if sym.getSource() == SourceType.DEFAULT:
            fname = ''
        else:
            fname = SymbolUtilities.getCleanSymbolName(sym.getName(True), sym.address)

        basic_model = BasicBlockModel(func.getProgram(), True)
        basic_blocks = basic_model.getCodeBlocksContaining(func.getBody(), monitor)

        for block in basic_blocks:
            num_edges_of_blocks += block.getNumDestinations(monitor)
            num_basic_blocks += 1

            code_units = func.getProgram().getListing().getCodeUnits(block, True)
            for code in code_units:
                if code.mnemonicString == 'CALL' or code.mnemonicString == 'bl':
                    num_call_subfunctions += 1

        return hash((fname, num_basic_blocks, num_edges_of_blocks, num_call_subfunctions, func.body.numAddresses, func.parameterCount, sym.referenceCount))

    @ JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@JImplements(FunctionHasher, deferred=True)
class StructuralGraphExactHasher:
    """
    Hash function using Graph Centric Comparison of function control flow graphs (thank you 2004 halvar)
    Hash calculates a 3-tuple measurement from each node (function cfg) in the program
    (num_basic_blocks, num_edges_of_blocks, num_call_subfunctions)
    Based on "Structural Comparison of Executable Objects" by Halvar Flake
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'StructuralGraphExactHash'

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        from ghidra.program.model.block import BasicBlockModel

        num_basic_blocks = 0
        num_edges_of_blocks = 0
        num_call_subfunctions = 0

        basic_model = BasicBlockModel(func.getProgram(), True)
        basic_blocks = basic_model.getCodeBlocksContaining(func.getBody(), monitor)

        for block in basic_blocks:
            num_edges_of_blocks += block.getNumDestinations(monitor)
            num_basic_blocks += 1

            code_units = func.getProgram().getListing().getCodeUnits(block, True)
            for code in code_units:
                # TODO verify BL instruction for ARM https://developer.arm.com/documentation/den0013/d/Application-Binary-Interfaces/Procedure-Call-Standard?lang=en
                if code.mnemonicString == 'CALL' or code.mnemonicString == 'bl':
                    num_call_subfunctions += 1

        return hash((num_basic_blocks, num_call_subfunctions, num_edges_of_blocks))

    @ JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@ JImplements(FunctionHasher, deferred=True)
class BulkInstructionsHasher:
    """
    Hash from instructions sorted and hashed
    Order of instructions is ignored
    Based on https://github.com/threatrack/ghidra-patchdiff-correlator#bulk-instructions-match
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'BulkInstructionHash'

    @ JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        instructions = []

        # reset iterator
        code_units = func.getProgram().getListing().getCodeUnits(func.getBody(), True)

        for code in code_units:
            instructions.append(code.toString())

        return hash(tuple(sorted(instructions)))

    @ JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@ JImplements(FunctionHasher, deferred=True)
class BulkMnemonicHasher:
    """
    Hash from function mnemonics sorted
    Order of instructions is ignored
    Based on https://github.com/threatrack/ghidra-patchdiff-correlator#bulk-mnemonics-match
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'BulkMnemonicHash'

    @ JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        mnemonics = []

        # reset iterator
        code_units = func.getProgram().getListing().getCodeUnits(func.getBody(), True)

        for code in code_units:
            mnemonics.append(code.getMnemonicString())

        return hash(tuple(sorted(mnemonics)))

    @ JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@ JImplements(FunctionHasher, deferred=True)
class BulkBasicBlockMnemonicHasher:
    """
    Hash from function basic blocks mnemonics sorted
    Order of basic blocks mnemonics is ignored
    Based on https://github.com/threatrack/ghidra-patchdiff-correlator#bulk-basic-block-mnemonics-match
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'BulkBasicBlockMnemonicHash'

    @ JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        from ghidra.program.model.block import BasicBlockModel

        basic_model = BasicBlockModel(func.getProgram(), True)
        basic_blocks = basic_model.getCodeBlocksContaining(func.getBody(), monitor)
        blocks = []

        for block in basic_blocks:
            code_units = func.getProgram().getListing().getCodeUnits(block, True)
            for code in code_units:
                blocks.append(code.getMnemonicString())

        return hash(tuple(sorted(blocks)))

    @ JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@JImplements(FunctionHasher, deferred=True)
class NamespaceNameParamHasher:
    """
    Simply return Name with Namespace and Param Hash matches
    DO NOT RUN THIS with one_to_many = TRUE
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'NamespaceNameParamHash'
    FIRST_RUN = True

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        return hash((func.getName(True), func.parameterCount))

    @ JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@JImplements(FunctionHasher, deferred=True)
class NameParamHasher:
    """
    Simply return Name and Param Hash matches
    No namespace included
    DO NOT RUN THIS with one_to_many = TRUE
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'NameParamHash'
    FIRST_RUN = True

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        return hash((func.getName(), func.parameterCount))

    @JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@JImplements(FunctionHasher, deferred=True)
class NameParamRefHasher:
    """
    Hash based on name param and number of refs

    DO NOT RUN THIS with one_to_many = TRUE
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'NameParamHash'
    FIRST_RUN = True

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        return hash((func.getName(True), func.parameterCount, func.symbol.referenceCount))

    @JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@JImplements(FunctionHasher, deferred=True)
class SigCallingCalledHasher:
    """
    Hash based on signature, called, and calling functions ignoring FUN_*
    Not very reliable for programs lacking symbols

    DO NOT RUN THIS with one_to_many = TRUE
    This should be run very late in the game
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'SigCallingCalledHasher'
    FIRST_RUN = True
    DEBUG = False

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        called = [called.toString() for called in func.getCalledFunctions(monitor) if "FUN_" not in called.toString()]
        calling = [calling.toString() for calling in func.getCalledFunctions(monitor)
                   if "FUN_" not in calling.toString()]

        sig = func.getSignature().toString().replace(func.name, '')

        if self.DEBUG:
            print(func.name)
            print((tuple(sorted(called)), tuple(sorted(calling)), sig))

        return hash((tuple(sorted(called)), tuple(sorted(calling)), sig))

    @JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


def getStringAtAddr(addr):
    """Get string at an address, if present"""
    from ghidra.program.model.data import StringDataType
    data = getDataAt(addr)
    if data is not None:
        dt = data.getDataType()
        if isinstance(dt, StringDataType):
            return str(data)
    return None


@lru_cache(None)
def get_defined_data(program: "ghidra.program.model.listing.Program"):

    from ghidra.program.model.symbol import SymbolUtilities
    from ghidra.program.model.symbol import SourceType
    from ghidra.program.model.symbol import SymbolType
    from ghidra.program.model.listing import Function

    def _is_sym_string(sym: 'ghidra.program.model.symbol.Symbol') -> bool:
        is_string = False
        sym_addr = sym.getAddress()
        if sym_addr is not None:
            data = sym.getProgram().getListing().getDataAt(sym_addr)
            if data is not None and data.hasStringValue():
                is_string = True
        return is_string

    strings_in_func = []
    func_str_map = {}

    # from java.util import CollectionUtils
    from ghidra.program.model.data import StringDataInstance
    from ghidra.program.util import DefinedDataIterator

    for data in DefinedDataIterator.definedStrings(program):
        sdi_str = StringDataInstance.getStringDataInstance(data)
        s = sdi_str.getStringValue()
        if s != None:
            strings_in_func.append(str(s))

    for sym in program.getSymbolTable().getAllSymbols(True):
        if not sym.referenceCount == 0:
            # skip functions
            if not sym.symbolType == SymbolType.FUNCTION:  # and _is_sym_string(sym):
                # don't include DEFAULT (FUN_ LAB_) but do include strings s_something
                # if sym.getSource() != SourceType.DEFAULT:
                sym_addr = sym.getAddress()
                if sym_addr is not None:
                    data = sym.getProgram().getListing().getDataAt(sym_addr)
                    if data is not None and data.hasStringValue():
                        # its a string, find which functions use it
                        for ref in sym.references:
                            # print(ref.referenceType.toString())
                            f = program.getFunctionManager().getFunctionContaining(ref.fromAddress)
                            if f is not None:
                                func_str_map.setdefault(f.entryPoint, []).append(str(data))

    return func_str_map


@JImplements(FunctionHasher, deferred=True)
class StringsRefsHasher:
    """
    Hash based on signature, called, and calling functions ignoring FUN_*

    DO NOT RUN THIS with one_to_many = TRUE
    This should be run very late in the game
    """

    MIN_FUNC_LEN = 10
    MIN_STRING_LEN = 5
    MATCH_TYPE = 'StringsRefsHasher'
    ONE_TO_MANY = True  # supports one to many
    DEBUG = False

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        func_str_map = get_defined_data(func.getProgram())

        # print(get_defined_data.cache_info())

        strings = func_str_map.get(func.entryPoint)

        if strings is not None:
            strings = sorted(strings)
        else:
            # assign unique value for functions without strings. This allows
            strings = [uuid.uuid4()]

        if self.DEBUG:
            print(strings)
            print(len(strings))

        return hash((tuple(strings)))

    @JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@JImplements(FunctionHasher, deferred=True)
class StrUniqueFuncRefsHasher:
    """
    Hash based on signature, called, and calling functions ignoring FUN_*

    DO NOT RUN THIS with one_to_many = TRUE
    This should be run very late in the game
    """

    MIN_FUNC_LEN = 10
    MIN_STRING_LEN = 5
    MATCH_TYPE = 'StrUniqueFuncRefsHasher'
    ONE_TO_MANY = False  # supports one to many
    DEBUG = False

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        func_str_map = get_defined_data(func.getProgram())

        # print(get_defined_data.cache_info())

        strings = func_str_map.get(func.entryPoint)
        ref_count = func.getSymbol().getReferenceCount()

        if strings is not None:
            strings = sorted(list(set(strings)))

        else:
            # assign unique value for functions without strings. This allows
            strings = [uuid.uuid4()]

        if self.DEBUG:
            print(strings)
            print(len(strings))
            print(ref_count)

        return hash((tuple(strings), ref_count))

    @JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError


@lru_cache(None)
def get_func_to_switch(program: "ghidra.program.model.listing.Program"):

    func_switch_map = {}

    for sym in program.getSymbolTable().getAllSymbols(True):
        sym: 'ghidra.program.model.symbol.Symbol' = sym

        if sym.getName().startswith('switchD') or sym.getName().startswith('caseD') or "switchD" in sym.getName():
            f = program.getFunctionManager().getFunctionContaining(sym.address)
            # if f is None:
            #     for ref in sym.references:
            #         # print(ref.referenceType.toString())
            #         f = program.getFunctionManager().getFunctionContaining(ref.fromAddress)
            #         if f is not None:
            #             func_switch_map.setdefault(f.entryPoint, []).append(str(sym))
            # else:
            #     func_switch_map.setdefault(f.entryPoint, []).append(str(sym))
            if f is not None:
                func_switch_map.setdefault(f.entryPoint, []).append(str(sym))
            for ref in sym.references:
                # print(ref.referenceType.toString())
                f = program.getFunctionManager().getFunctionContaining(ref.fromAddress)
                if f is not None:
                    func_switch_map.setdefault(f.entryPoint, []).append(str(sym))

    return func_switch_map


@JImplements(FunctionHasher, deferred=True)
class SwitchSigHasher:
    """
    Simply return Name and Param Hash matches
    No namespace included
    DO NOT RUN THIS with one_to_many = TRUE
    """

    MIN_FUNC_LEN = 10
    MATCH_TYPE = 'SwitchSigHasher'
    ONE_TO_MANY = False
    DEBUG = False

    @JOverride
    def hash(self, func: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:

        sig = func.getSignature().toString().replace(func.name, '')

        func_switch_map = get_func_to_switch(func.getProgram())

        switch_syms = func_switch_map.get(func.entryPoint)

        if switch_syms is not None:
            switch_syms = sorted(switch_syms)
        else:
            # assign unique value for functions without strings.
            switch_syms = [uuid.uuid4()]

        if self.DEBUG:
            print(func.name)
            print(switch_syms)

        return hash(tuple([sig, tuple(switch_syms)]))

    @ JOverride
    def commonBitCount(self, funcA: 'ghidra.program.model.listing.Function', funcB: 'ghidra.program.model.listing.Function', monitor: 'ghidra.util.task.TaskMonitor') -> int:
        raise NotImplementedError

```

`ghidriff/ghidra_diff_engine.py`:

```py
from abc import ABCMeta, abstractmethod
from pathlib import Path
import json
import difflib
import argparse
import re
from time import time
from datetime import datetime
from collections import Counter
import concurrent.futures
from typing import List, Tuple, Union, TYPE_CHECKING
from argparse import Namespace
import logging

from pyhidra.launcher import PyhidraLauncher, GHIDRA_INSTALL_DIR
from .utils import sha1_file, get_microsoft_download_url, get_pe_extra_data
from .markdown import GhidriffMarkdown

import multiprocessing

from ghidriff import __version__

if TYPE_CHECKING:
    import ghidra
    from ghidra_builtins import *


class HeadlessLoggingPyhidraLauncher(PyhidraLauncher):
    """
    Headless pyhidra launcher
    Slightly Modified from Pyhidra to allow the Ghidra log path to be set
    """

    def __init__(self, verbose=False, log_path=None):
        super().__init__(verbose)
        self.log_path = log_path

    def _launch(self):
        from pyhidra.launcher import _silence_java_output
        from ghidra.framework import Application, HeadlessGhidraApplicationConfiguration
        from java.io import File
        with _silence_java_output(not self.verbose, not self.verbose):
            config = HeadlessGhidraApplicationConfiguration()
            if self.log_path:
                log = File(self.log_path)
                config.setApplicationLogFile(log)

            Application.initializeApplication(self.layout, config)


class GhidraDiffEngine(GhidriffMarkdown, metaclass=ABCMeta):
    """
    Base Ghidra Diff Engine
    """

    def __init__(
            self,
            args: Namespace = None,
            verbose: bool = True,
            threaded: bool = False,
            max_workers=multiprocessing.cpu_count(),
            max_ram_percent: float = 60.0,
            print_jvm_flags: bool = False,
            jvm_args: List[str] = [],
            force_analysis: bool = False,
            force_diff: bool = False,
            verbose_analysis: bool = False,
            no_symbols: bool = False,
            engine_log_path: Path = None,
            engine_log_level: int = logging.INFO,
            engine_file_log_level: int = logging.INFO,
            max_section_funcs: int = 200,
            min_func_len: int = 10) -> None:

        # setup engine logging
        self.logger = self.setup_logger(engine_log_level)

        self.logger.info('Init Ghidra Diff Engine...')
        self.logger.info(f'Engine Console Log: {engine_log_level}')

        if engine_log_path:
            # send application log to output path
            self.add_log_to_path(engine_log_path, engine_log_level)
            self.logger.info(f'Engine File Log:  {engine_log_path} {engine_file_log_level}')
        else:
            self.logger.warn('Engine File Log: {engine_log_path}')

        # Init Pyhidra
        launcher = HeadlessLoggingPyhidraLauncher(verbose=verbose, log_path=engine_log_path)

        # JVM Settings

        # max % of host RAM
        launcher.add_vmargs(f'-XX:MaxRAMPercentage={max_ram_percent}')

        # want JVM to crash if we run out of memory (otherwise no error it propagated)
        launcher.add_vmargs('-XX:+CrashOnOutOfMemoryError')
        launcher.add_vmargs('-XX:+HeapDumpOnOutOfMemoryError')

        # Set Ghidra Heap Max
        # Ghidra/RuntimeScripts/Linux/support/analyzeHeadless#L7
        # MAX_MEM = "16G"
        # launcher.add_vmargs(f"-Xmx{MAX_MEM}")

        if print_jvm_flags:
            launcher.add_vmargs('-XX:+PrintFlagsFinal')

        if jvm_args:
            for jvm_arg in jvm_args:
                self.logger.info('Adding JVM arg {jvm_arg}')
                launcher.add_vmargs(jvm_arg)

        self.logger.info(f'Starting Ghidra...')
        self.logger.debug(f'Starting JVM with args: {launcher.vm_args}')

        launcher.start()

        self.logger.info(f'GHIDRA_INSTALL_DIR: {GHIDRA_INSTALL_DIR}')
        app_prop = launcher.layout.getApplicationProperties()
        self.logger.info(
            f'GHIDRA {app_prop.applicationVersion}  Build Date: {app_prop.applicationBuildDate} Release: {app_prop.applicationReleaseName}')
        self.logger.info(f"Engine Args:")
        for arg in vars(args):
            self.logger.info('\t%-20s%s', f'{arg}:', vars(args)[arg])

        self.launcher = launcher
        self.threaded = threaded
        self.max_workers = max_workers
        self.max_section_funcs = max_section_funcs
        self.min_func_len = min_func_len

        # store args used from init
        self.args = args

        # Setup decompiler interface
        self.decompilers = {}

        self.project: "ghidra.base.project.GhidraProject" = None

        self.version = __version__

        # Global instance var to store symbol lookup results
        self.esym_memo = {}

        # set instance preferences
        self.force_analysis = force_analysis
        self.force_diff = force_diff
        self.verbose_analysis = verbose_analysis
        self.no_symbols = no_symbols

        # if looking up more than calling_count_funcs_limit symbols, skip function counts
        self.calling_count_funcs_limit = 500

        self.logger.debug(f'{vars(self)}')

    @ staticmethod
    def add_ghidra_args_to_parser(parser: argparse.ArgumentParser) -> None:
        """
        Add required Ghidra args to a parser
        """

        group = parser.add_argument_group('Ghidra Project Options')
        group.add_argument('-p', '--project-location', help='Ghidra Project Path', default='.ghidra_projects')
        group.add_argument('-n', '--project-name', help='Ghidra Project Name', default='ghidriff')
        group.add_argument('-s', '--symbols-path', help='Ghidra local symbol store directory', default='.symbols')

        group = parser.add_argument_group('Engine Options')
        group.add_argument('--threaded', help='Use threading during import, analysis, and diffing. Recommended',
                           default=True,  action=argparse.BooleanOptionalAction)
        group.add_argument('--force-analysis', help='Force a new binary analysis each run (slow)',
                           action='store_true')
        group.add_argument('--force-diff', help='Force binary diff (ignore arch/symbols mismatch)',
                           action='store_true')
        group.add_argument('--no-symbols', help='Turn off symbols for analysis', action='store_true')
        group.add_argument('--log-level', help='Set console log level',
                           default='INFO', choices=logging._nameToLevel.keys())
        group.add_argument('--file-log-level', help='Set log file level',
                           default='INFO', choices=logging._nameToLevel.keys())
        group.add_argument('--log-path', help='Set ghidriff log path.', default='ghidriff.log')
        group.add_argument('--va', '--verbose-analysis',
                           help='Verbose logging for analysis step.', action='store_true')

        # TODO add following option
        # group.add_argument('--exact-matches', help='Only consider exact matches', action='store_true')

        group = parser.add_argument_group('JVM Options')
        group.add_argument('--max-ram-percent', help='Set JVM Max Ram %% of host RAM', default=60.0)
        group.add_argument('--print-flags', help='Print JVM flags at start', action='store_true')
        group.add_argument('--jvm-args', nargs='?', help='JVM args to add at start', default=None)

        group = parser.add_argument_group('Markdown Options')
        group.add_argument('--sxs', dest='side_by_side', action='store_true',
                           help='Include side by side code diff')
        group.add_argument('--max-section-funcs',
                           help='Max number of functions to display per section.', type=int, default=200)
        group.add_argument('--md-title', help='Overwrite default title for markdown diff', type=str, default=None)

    def get_default_args(self) -> list:
        """
        Return list of default args for engine
        """

        parser = argparse.ArgumentParser()

        self.add_ghidra_args_to_parser(parser)

        defaults = vars(parser.parse_args([]))

        return defaults

    def setup_logger(self, level: int = logging.INFO) -> logging.Logger:
        """
        Setup Class Instance Logger
        """
        logging.basicConfig(
            format='%(levelname)-5s| %(name)s | %(message)s',
            datefmt='%H:%M:%S'
        )

        logger = logging.getLogger(__package__)
        logger.setLevel(level)

        return logger

    def add_log_to_path(self, log_path: Path, level: int = logging.INFO):
        """
        Directory to write log to
        """
        file_handler = logging.FileHandler(log_path)
        formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)-8s %(message)s')

        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)
        self.logger.addHandler(file_handler)

    def gen_credits(self, html: bool = False) -> str:
        """
        Generate script credits
        """
        now = datetime.now().replace(microsecond=0).isoformat()
        if html:
            text = f"\n<sub>Generated with <code>{__package__}</code> version: {self.version} on {now}</sub>"
        else:
            text = f"\n<sub>Generated with `{__package__}` version: {self.version} on {now}</sub>"

        return text

    def enhance_sym(self, sym: 'ghidra.program.model.symbol.Symbol', thread_id: int = 0, timeout: int = 15, get_decomp_info: bool = False, use_calling_counts: bool = False) -> dict:
        """
        Standardize enhanced symbol. Use esym_memo to speed things up.
        Inspired by Ghidra/Features/VersionTracking/src/main/java/ghidra/feature/vt/api/main/VTMatchInfo.java
        timeout is for decompiler. -1 is infinite, and too long
        """

        from ghidra.program.model.symbol import SymbolType

        # key = f'{sym.iD}-{sym.program.name}-{get_decomp_info}-{use_calling_counts}'
        key = f'{sym.iD}-{sym.program.name}'

        # if sym.getName() == 'SepAppendAceToTokenObjectAcl':
        #     print('hi')

        if key not in self.esym_memo:

            from ghidra.util.task import ConsoleTaskMonitor

            monitor = ConsoleTaskMonitor()
            prog = sym.program
            func: 'ghidra.program.model.listing.Function' = prog.functionManager.getFunctionAt(sym.address)

            if not sym.symbolType == SymbolType.FUNCTION:

                # process symbol

                calling = set()
                ref_types = set()

                for ref in sym.references:
                    ref_types.add(ref.referenceType.toString())
                    f = prog.getFunctionManager().getFunctionContaining(ref.fromAddress)
                    if f:
                        calling.add(f.getName())

                calling = list(calling)
                ref_types = list(ref_types)

                if sym.parentSymbol is not None:
                    parent = str(sym.parentSymbol)
                else:
                    parent = None

                self.esym_memo[key] = {'name': sym.getName(), 'fullname': sym.getName(True), 'parent':  parent, 'refcount': sym.getReferenceCount(), 'reftypes': ref_types,  'calling': calling,
                                       'address': str(sym.getAddress()), 'sym_type': str(sym.getSymbolType()), 'sym_source': str(sym.source), 'external': sym.external}

            else:
                # proces function

                instructions = []
                mnemonics = []
                blocks = []
                called_funcs = []
                calling_funcs = []
                code = ''

                if get_decomp_info:

                    code_units = func.getProgram().getListing().getCodeUnits(func.getBody(), True)

                    # instruction and mnemonic bulker
                    for code in code_units:
                        instructions.append(str(code))
                        mnemonics.append(str(code.mnemonicString))

                    from ghidra.program.model.block import BasicBlockModel

                    # Basic Block Bulker
                    basic_model = BasicBlockModel(func.getProgram(), True)
                    basic_blocks = basic_model.getCodeBlocksContaining(func.getBody(), monitor)

                    for block in basic_blocks:
                        code_units = func.getProgram().getListing().getCodeUnits(block, True)
                        for code in code_units:
                            blocks.append(str(code.mnemonicString))

                    # sort - This case handles the case for compiler optimizations
                    blocks = sorted(blocks)

                    if not func.external:
                        error, code = self.decompile_func(func.program, func, timeout,)

                        if error:
                            err = f'Failed to decompile {func.program} {func} : {error}'
                            self.logger.warn(err)
                            code = err

                # if use_calling_counts:
                if False:
                    for f in func.getCalledFunctions(monitor):
                        count = 0
                        print(len(f.symbol.references))
                        for ref in f.symbol.references:
                            if func.getBody().contains(ref.fromAddress, ref.fromAddress):
                                count += 1
                        called_funcs.append(f'{f}-{count}')

                    for f in func.getCallingFunctions(monitor):
                        count = 0
                        print(len(func.symbol.references))
                        for ref in func.symbol.references:
                            if f.getBody().contains(ref.fromAddress, ref.fromAddress):
                                count += 1
                        called_funcs.append(f'{f}-{count}')
                else:
                    for f in func.getCalledFunctions(monitor):
                        called_funcs.append(f'{f}')
                    for f in func.getCallingFunctions(monitor):
                        calling_funcs.append(f'{f}')

                called_funcs = sorted(called_funcs)
                calling_funcs = sorted(calling_funcs)
                parent_namespace = sym.getParentNamespace().toString().split('@')[0]

                self.esym_memo[key] = {'name': sym.getName(), 'fullname': sym.getName(True),  'parent':  parent_namespace, 'refcount': sym.getReferenceCount(), 'length': func.body.numAddresses, 'called': called_funcs,
                                       'calling': calling_funcs, 'paramcount': func.parameterCount, 'address': str(sym.getAddress()), 'sig': str(func.getSignature(False)), 'code': code,
                                       'instructions': instructions, 'mnemonics': mnemonics, 'blocks': blocks, 'sym_type': str(sym.getSymbolType()), 'sym_source': str(sym.source), 'external': sym.external}

        return self.esym_memo[key]

    def setup_project(
            self,
            binary_paths: List[Union[str, Path]],
            project_location: Union[str, Path],
            project_name: str,
            symbols_path: Union[str, Path],
            symbol_urls: list = None,
    ) -> list:
        """
        Setup and verify Ghidra Project
        1. Creat / Open Project
        2. Import / Open Binaries
        3. Configure and verify symbols
        """
        from ghidra.base.project import GhidraProject
        from java.io import IOException
        from ghidra.app.plugin.core.analysis import PdbAnalyzer
        from ghidra.app.plugin.core.analysis import PdbUniversalAnalyzer

        project_location = Path(project_location) / project_name
        project_location.mkdir(exist_ok=True, parents=True)
        pdb = None

        self.logger.info(f'Setting Up Ghidra Project...')

        # Open/Create project
        project = None

        # remove duplicate paths, maintain order
        binary_paths = list(dict.fromkeys(binary_paths))

        # remove duplicate files (different path, but same content)
        bin_hashes = []
        for i, bin_hash in enumerate([sha1_file(path) for path in binary_paths]):

            if bin_hash in bin_hashes:
                self.logger.warn(f'Duplicate file detected {binary_paths[i]} with sha1: {bin_hash}')
                binary_paths.pop(i)
            else:
                bin_hashes.append(bin_hash)

        try:
            project = GhidraProject.openProject(project_location, project_name, True)
            self.logger.info(f'Opened project: {project.project.name}')
        except IOException:
            project = GhidraProject.createProject(project_location, project_name, False)
            self.logger.info(f'Created project: {project.project.name}')

        self.project = project

        self.logger.info(f'Project Location: {project.project.projectLocator.location}')

        bin_results = []
        proj_programs = []

        # Import binaries and configure symbols
        for program_path in binary_paths:

            # add sha1 to prevent files with same name collision
            program_name = self.gen_proj_bin_name_from_path(program_path)

            # Import binaries and configure symbols
            if not project.getRootFolder().getFile(program_name):
                self.logger.info(f'Importing {program_path} as {program_name}')
                program = project.importProgram(program_path)
                project.saveAs(program, "/", program_name, True)
            else:
                self.logger.info(f'Opening {program_path}')
                program = self.project.openProgram("/", program_name, False)

            proj_programs.append(program)

        # Setup Symbols Server
        if not self.no_symbols:
            if any(self.prog_is_windows(prog) for prog in proj_programs):
                # Windows level 1 symbol server location
                level = 1
            else:
                # Symbols stored in specified symbols path
                level = 0
            self.setup_symbol_server(symbols_path, level, server_urls=symbol_urls)

        for program in proj_programs:

            if not self.no_symbols:
                # Enable Remote Symbol Servers
                PdbUniversalAnalyzer.setAllowRemoteOption(program, True)
                PdbAnalyzer.setAllowRemoteOption(program, True)

                pdb = self.get_pdb(program)
            else:
                # Run get_pdb to make sure the symbols dont exist locally
                pdb = self.get_pdb(program, allow_remote=False)

                if pdb:
                    err = f'Symbols are disabled, but the symbol is already downloaded {pdb}. Delete symbol or remove --no-symbol flag'
                    self.logger.error(err)
                    raise FileExistsError(err)

            if pdb is None and not self.no_symbols:
                self.logger.warn(f"PDB not found for {program.getName()}!")

            from ghidra.app.util.pdb import PdbProgramAttributes

            pdb_attr = PdbProgramAttributes(program)

            imported = program is not None
            has_pdb = pdb is not None
            pdb_loaded = pdb_attr.pdbLoaded
            prog_analyzed = pdb_attr.programAnalyzed

            # TODO only save if changes are made
            # project.save(program)
            project.close(program)

            bin_results.append([program.getExecutablePath(), imported, has_pdb, pdb_loaded, prog_analyzed])

        for result in bin_results:
            self.logger.info('Program: %s imported: %s has_pdb: %s pdb_loaded: %s analyzed %s', *result)

        return bin_results

    def setup_decompliers(
        self,
        p1: "ghidra.program.model.listing.Program",
        p2: "ghidra.program.model.listing.Program"
    ) -> bool:
        """
        Setup decompliers to use during diff bins. Each one must be initialized with a program.
        """

        from ghidra.app.decompiler import DecompInterface
        from ghidra.app.decompiler import DecompileOptions

        p1_options = DecompileOptions()
        p2_options = DecompileOptions()

        # grab default options from program
        p1_options.grabFromProgram(p1)
        p2_options.grabFromProgram(p2)

        # increase maxpayload size to 100MB (default 50MB)
        p1_options.setMaxPayloadMBytes(100)
        p2_options.setMaxPayloadMBytes(100)

        if self.threaded:
            decompiler_count = 2 * self.max_workers
            for i in range(self.max_workers):
                self.decompilers.setdefault(p1.name, {}).setdefault(i, DecompInterface())
                self.decompilers.setdefault(p2.name, {}).setdefault(i, DecompInterface())
                self.decompilers[p1.name][i].setOptions(p1_options)
                self.decompilers[p2.name][i].setOptions(p2_options)
                self.decompilers[p1.name][i].openProgram(p1)
                self.decompilers[p2.name][i].openProgram(p2)
                self.decompilers[p1.name].setdefault('available', []).append(i)
                self.decompilers[p2.name].setdefault('available', []).append(i)
        else:
            decompiler_count = 2
            self.decompilers.setdefault(p1.name, {}).setdefault(0, DecompInterface())
            self.decompilers.setdefault(p2.name, {}).setdefault(0, DecompInterface())
            self.decompilers[p1.name][0].setOptions(p1_options)
            self.decompilers[p2.name][0].setOptions(p2_options)
            self.decompilers[p1.name][0].openProgram(p1)
            self.decompilers[p2.name][0].openProgram(p2)
            self.decompilers[p1.name].setdefault('available', []).append(0)
            self.decompilers[p2.name].setdefault('available', []).append(0)

        self.logger.info(f'Setup {decompiler_count} decompliers')

        return True

    def shutdown_decompilers(
        self,
        p1: "ghidra.program.model.listing.Program",
        p2: "ghidra.program.model.listing.Program"
    ) -> bool:
        """
        Shutdown decompliers
        """

        if self.threaded:
            for i in range(self.max_workers):
                self.decompilers[p1.name][i].closeProgram()
                self.decompilers[p2.name][i].closeProgram()
        else:
            self.decompilers[p1.name][0].closeProgram(p1)
            self.decompilers[p2.name][0].closeProgram(p2)

        self.decompilers = {}

    def decompile_func(
        self,
        prog: "ghidra.program.model.listing.Program",
        func: 'ghidra.program.model.listing.Function',
        timeout: int = 15
    ) -> List[str]:

        from ghidra.util.task import ConsoleTaskMonitor

        code = ''
        error = ''
        decomp_id = None
        monitor = ConsoleTaskMonitor()

        # list operations are atomic. right?
        while decomp_id == None:
            try:
                decomp_id = self.decompilers[prog.name]['available'].pop()
            except IndexError:
                pass

        results: 'ghidra.app.decompiler.DecompileResults' = self.decompilers[prog.name][decomp_id].decompileFunction(
            func, timeout, monitor)

        error = results.getErrorMessage()
        if error == '':
            code = results.getDecompiledFunction().getC()
        else:
            error = f'Error: Decompile error: {error}'

        # set decomp as available
        self.decompilers[prog.name]['available'].append(decomp_id)

        return error, code

    def get_pdb(self, prog: "ghidra.program.model.listing.Program", allow_remote=True) -> "java.io.File":
        """
        Searches the currently configured symbol server paths for a Pdb symbol file.
        If remote is enabled, downloads pdb to saved SymbolService path
        """

        from pdb_.symbolserver import FindOption
        from ghidra.util.task import ConsoleTaskMonitor
        from pdb_ import PdbPlugin

        if allow_remote:
            find_opts = FindOption.of(FindOption.ALLOW_REMOTE)
        else:
            find_opts = FindOption.NO_OPTIONS

        # Ghidra/Features/PDB/src/main/java/pdb/PdbPlugin.java#L191
        pdb = PdbPlugin.findPdb(prog, find_opts, ConsoleTaskMonitor())

        if pdb is not None:
            self.logger.info(f'Pdb stored at: {pdb}')

        return pdb

    def download_project_program_pdbs(self) -> List[List]:
        """
        Downloads PDBs for all programs within project
        """

        pdb_list = []

        if self.project is None:
            raise Exception('ProjectNotSet')

        for domainFile in self.project.getRootFolder().getFiles():
            prog_name = domainFile.getName()

            # open readonly prog
            prog = self.project.openProgram("/", prog_name, True)
            pdb = self.get_pdb(prog)

            if pdb is not None:
                pdb = Path(pdb.absoluteFile.toString())
                self.logger.info(f"PDB {pdb} found for {prog_name}")

            pdb_list.append([prog_name, pdb])

        return pdb_list

    # program: "ghidra.program.model.listing.Program",
    def setup_symbol_server(self,  symbols_path: Union[str, Path], level=0, server_urls=None) -> None:
        """setup symbols to allow Ghidra to download as needed
        1. Configures symbol_path as local symbol store path
        2. Sets Index level for local symbol path
        - Level 0 indexLevel is a special Ghidra construct that is just a user-friendlier plain directory with a collection of Pdb files
        [symbol-store-folder-tree](https://learn.microsoft.com/en-us/windows-hardware/drivers/debugger/symbol-store-folder-tree) (applies to 1 and 2)
        - Level 1, with pdb files stored directly underthe root directory
        - Level 2, using the first 2 characters of the pdb filename as a bucket to place each pdb file-directory in
        """

        self.logger.info("Setting up Symbol Server for symbols...")
        self.logger.info(f"path: {symbols_path} level: {level}")

        symbols_path = Path(symbols_path).absolute()

        from pdb_ import PdbPlugin
        from pdb_.symbolserver import LocalSymbolStore
        from pdb_.symbolserver import HttpSymbolServer
        from pdb_.symbolserver import SymbolServerService
        from ghidra.framework import Application

        from java.io import File
        from java.net import URI
        from java.util import ArrayList

        # Configure local symbols directory
        symbolsDir = File(symbols_path)
        localSymbolStore = LocalSymbolStore(symbols_path)

        # Create loacl symbold soCreates a MS-compatible symbol server directory location. pdb/symbolserver/LocalSymbolStore.java#L67
        localSymbolStore.create(symbolsDir, level)

        # Configure symbol urls
        if server_urls is None:
            # load wellknown servers
            # Ghidra/Features/PDB/src/main/java/pdb/symbolserver/ui/WellKnownSymbolServerLocation.java#L89
            known_urls = []
            pdbUrlFiles = Application.findFilesByExtensionInApplication(".pdburl")
            for pdbFile in pdbUrlFiles:
                data = Path(pdbFile.absolutePath).read_text()
                self.logger.debug(f"Loaded well known {pdbFile.absolutePath}' length: {len(data)}'")
                for line in data.splitlines(True):
                    cat, location, warning = line.split('|')
                    known_urls.append(location)
            server_urls = known_urls
        else:
            if not isinstance(server_urls, list):
                raise TypeError('server_urls must be a list of urls')

        sym_servers = ArrayList()

        for url in server_urls:
            sym_servers.add(HttpSymbolServer(URI.create(url)))

        symbolServerService = SymbolServerService(localSymbolStore, sym_servers)

        PdbPlugin.saveSymbolServerServiceConfig(symbolServerService)

        self.logger.info(f'Symbol Server Configured path: {symbolServerService.toString().strip()}')

    def analyze_program(self, df_or_prog: Union["ghidra.framework.model.DomainFile", "ghidra.program.model.listing.Program"], require_symbols: bool, force_analysis: bool = False, verbose_analysis: bool = False):

        from ghidra.program.flatapi import FlatProgramAPI
        from ghidra.framework.model import DomainFile
        from ghidra.program.model.listing import Program
        from ghidra.util.task import ConsoleTaskMonitor
        from ghidra.program.util import GhidraProgramUtilities
        from ghidra.app.script import GhidraScriptUtil
        from ghidra.app.util.pdb import PdbProgramAttributes

        if isinstance(df_or_prog, DomainFile):
            program = self.project.openProgram("/", df_or_prog.getName(), False)
        elif isinstance(df_or_prog, Program):
            program = df_or_prog

        self.logger.info(f"Analyzing: {program}")

        try:
            if verbose_analysis or self.verbose_analysis:
                monitor = ConsoleTaskMonitor()
                flat_api = FlatProgramAPI(program, monitor)
            else:
                flat_api = FlatProgramAPI(program)

            pdb_attr = PdbProgramAttributes(program)
            # force_reload_for_symbols = not pdb_attr.isPdbLoaded(
            # ) and not self.no_symbols and pdb_attr.isProgramAnalyzed()
            force_reload_for_symbols = False

            if force_reload_for_symbols:
                self.set_analysis_option_bool(program, 'PDB Universal', True)
                self.logger.info('Symbols missing. Re-analysis is required. Setting PDB Universal: True')
                self.logger.debug(f'pdb loaded: {pdb_attr.isPdbLoaded()} prog analyzed: {pdb_attr.isProgramAnalyzed()}')

            if GhidraProgramUtilities.shouldAskToAnalyze(program) or force_analysis or self.force_analysis or force_reload_for_symbols:
                GhidraScriptUtil.acquireBundleHostReference()

                # handle large binaries more efficiently
                # see ghidra/issues/4573 (turn off feature Shared Return Calls )
                if program and program.getFunctionManager().getFunctionCount() > 1000:
                    self.logger.warn(f"Turning off 'Shared Return Calls' for {program}")
                    self.set_analysis_option_bool(
                        program, 'Shared Return Calls.Assume Contiguous Functions Only', False)

                if self.no_symbols:
                    self.logger.warn(f'Disabling symbols for analysis! --no-symbols flag: {self.no_symbols}')
                    self.set_analysis_option_bool(program, 'PDB Universal', False)

                self.logger.info(f'Starting Ghidra analysis of {program}...')
                try:
                    flat_api.analyzeAll(program)
                    if hasattr(GhidraProgramUtilities, 'setAnalyzedFlag'):
                        GhidraProgramUtilities.setAnalyzedFlag(program, True)
                    elif hasattr(GhidraProgramUtilities, 'markProgramAnalyzed'):
                        GhidraProgramUtilities.markProgramAnalyzed(program)
                    else:
                        raise Exception('Missing set analyzed flag method!')
                finally:
                    GhidraScriptUtil.releaseBundleHostReference()
                    self.project.save(program)
            else:
                self.logger.info(f"Analysis already complete.. skipping {program}!")
        finally:
            self.project.close(program)

        self.logger.info(f"Analysis for {df_or_prog} complete")

        return df_or_prog

    def analyze_project(self, require_symbols: bool = True, force_analysis: bool = False, verbose_analysis: bool = False) -> None:
        """
        Analyzes all files found within the project
        """
        self.logger.info(f'Starting analysis for {len(self.project.getRootFolder().getFiles())} binaries')

        if self.threaded:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = (executor.submit(self.analyze_program, *[domainFile, require_symbols, force_analysis, verbose_analysis])
                           for domainFile in self.project.getRootFolder().getFiles() if domainFile.getContentType() == 'Program')
                for future in concurrent.futures.as_completed(futures):
                    prog = future.result()
        else:
            for domainFile in self.project.getRootFolder().getFiles():
                self.analyze_program(domainFile, require_symbols, force_analysis)

    def get_metadata(
        self,
        prog: "ghidra.program.model.listing.Program"
    ) -> dict:
        """
        Generate dict from program metadata
        """

        meta = prog.getMetadata()

        return dict(meta)

    def get_pe_download_url(
        self,
        path: Path,
        filename: str
    ) -> str:
        """
        Generate Microsoft PE download URL 
        """

        path = Path(path)
        pe_info = get_pe_extra_data(path)
        url = get_microsoft_download_url(filename, pe_info['timestamp'], pe_info['image_size'])

        # from ghidra.app.util.bin.format.pe import PortableExecutable
        # from ghidra.app.util.bin import FileByteProvider

        # from java.nio.file import AccessMode
        # from java.io import File

        # file_path = File(path)

        # bp = FileByteProvider(file_path, None, AccessMode.READ)

        # pe = PortableExecutable(bp, PortableExecutable.SectionLayout.FILE)
        # ntHeader = pe.getNTHeader()
        # if ntHeader is not None and ntHeader.getOptionalHeader() is not None:
        #     timestamp = abs(ntHeader.getFileHeader().getTimeDateStamp())
        #     virtual_size = ntHeader.getOptionalHeader().getSizeOfImage()

        # if timestamp is not None and virtual_size is not None and filename is not None:
        #     timestamp = format(timestamp, '08X')
        #     virtual_size = format(virtual_size, 'X')

        #     url = f'https://msdl.microsoft.com/download/symbols/{filename}/{timestamp}{virtual_size}/{filename}'

        return url

    def get_all_program_options(self,
                                prog: "ghidra.program.model.listing.Program"
                                ) -> dict:
        """
        Retrieve all program options
        Inspired by: Ghidra/Features/Base/src/main/java/ghidra/app/script/GhidraScript.java#L1272
        """

        all_options = {}

        for opt_name in prog.getOptionsNames():
            all_options[opt_name] = self.get_program_options(prog, opt_name)

        return all_options

    def get_program_options(
        self,
        prog: "ghidra.program.model.listing.Program",
        name: str
    ) -> dict:
        """
        Generate dict program options
        Inspired by: Ghidra/Features/Base/src/main/java/ghidra/app/script/GhidraScript.java#L1272
        """

        from ghidra.program.model.listing import Program

        possible_props = prog.getOptionsNames()

        if name not in possible_props:
            err = f'Program property name not found: {name} in {possible_props}'
            self.logger.error(err)
            raise err

        prog_options = prog.getOptions(name)
        options = {}

        for propName in prog_options.getOptionNames():
            options[propName] = prog_options.getValueAsString(propName)

        return options

    def set_analysis_option_bool(
        self,
        prog: "ghidra.program.model.listing.Program",
        option_name: str,
        value: bool
    ) -> None:
        """
        Set boolean program analysis options
        Inspired by: Ghidra/Features/Base/src/main/java/ghidra/app/script/GhidraScript.java#L1272
        """

        from ghidra.program.model.listing import Program

        prog_options = prog.getOptions(Program.ANALYSIS_PROPERTIES)

        prog_options.setBoolean(option_name, value)

    def set_proginfo_option_bool(
        self,
        prog: "ghidra.program.model.listing.Program",
        option_name: str,
        value: bool
    ) -> None:
        """
        Set boolean program info options
        See: Ghidra/Features/Base/src/main/java/ghidra/app/script/GhidraScript.java#L1272
        """

        from ghidra.program.model.listing import Program

        prog_options = prog.getOptions(Program.PROGRAM_INFO)

        prog_options.setBoolean(option_name, value)

    def prog_is_windows(
        self,
        prog: "ghidra.program.model.listing.Program"
    ) -> bool:
        """
        Determines if program is Windows
        "Compiler ID" == "windows"
        """

        meta = self.get_metadata(prog)

        return meta['Compiler ID'] == 'windows'

    def get_funcs_from_addr_set(
            self,
            prog: 'ghidra.program.database.ProgramDB',
            addr_set: 'ghidra.program.model.address.AddressSet',
            min_fun_len=None):
        """
        Build a list of functions that match a provided address set
        Ignores Thunks and functions smaller than `min_fun_len`
        """

        funcs = []

        if min_fun_len is None:
            min_fun_len = self.min_func_len

        for func in prog.functionManager.getFunctions(addr_set, True):
            if (not func.isThunk() and func.getBody().getNumAddresses() >= min_fun_len):
                funcs.append(func)

        return funcs

    @ abstractmethod
    def find_matches(
            self,
            p1: "ghidra.program.model.listing.Program",
            p2: "ghidra.program.model.listing.Program"
    ) -> list:
        """
        Find matching and unmatched functions between `p1` and `p2`
        return `[unmatched, matched, skip_types]`
        `unmatched` : list of symbols that have not been matched
        `matched` : list of symbols that have been matched
        `skip_types`: list of match combinations that should not be considered for diffing. skip types will undergo less processing
        """
        raise NotImplementedError

    # based on Features/Base/src/main/java/ghidra/app/plugin/match/MatchSymbol.java
    def is_sym_string(self, sym: 'ghidra.program.model.symbol.Symbol') -> bool:

        sym_addr = sym.getAddress()
        if sym_addr is not None:
            data = sym.getProgram().getListing().getDataAt(sym_addr)
            if data is not None and data.hasStringValue():
                return True

        return False

    def diff_nf_symbols(
        self,
        p1: "ghidra.program.model.listing.Program",
        p2: "ghidra.program.model.listing.Program"
    ) -> list:
        """
        Find matching and unmatched (non-function) symbols between p1 and p2
        """

        # find added and deleted symbols
        from ghidra.program.model.symbol import SymbolUtilities
        from ghidra.program.model.symbol import SourceType
        from ghidra.program.model.symbol import SymbolType
        from ghidra.program.model.listing import Function

        all_p1_syms = {}
        all_p2_syms = {}

        # build symbols dict

        # follow pattern for Ghidra/Features/Base/src/main/java/ghidra/app/plugin/match/MatchSymbol.java
        for sym in p1.getSymbolTable().getAllSymbols(True):
            if not sym.referenceCount == 0:
                # skip functions
                if not sym.symbolType == SymbolType.FUNCTION:
                    # don't include DEFAULT (FUN_ LAB_) but do include strings s_something
                    if sym.getSource() != SourceType.DEFAULT or self.is_sym_string(sym):
                        # skip local symbols
                        if not isinstance(sym.getParentNamespace(), Function):
                            # get name lacking '_' or '@'
                            clean_name = SymbolUtilities.getCleanSymbolName(sym.getName(), sym.address)
                            all_p1_syms[clean_name] = sym

        self.logger.info(f'p1 sym count: reported: {p1.symbolTable.numSymbols} analyzed: {len(all_p1_syms)}')

        for sym in p2.getSymbolTable().getAllSymbols(True):
            if not sym.referenceCount == 0:
                # skip functions
                if not sym.symbolType == SymbolType.FUNCTION:
                    # don't include DEFAULT (FUN_ LAB_) but do include strings s_something
                    if sym.getSource() != SourceType.DEFAULT or self.is_sym_string(sym):
                        # skip local symbols
                        if not isinstance(sym.getParentNamespace(), Function):
                            # get name lacking '_' or '@'
                            clean_name = SymbolUtilities.getCleanSymbolName(sym.getName(), sym.address)
                            all_p2_syms[clean_name] = sym

        self.logger.info(f'p2 sym count: reported: {p2.symbolTable.numSymbols} analyzed: {len(all_p2_syms)}')

        deleted_sym_keys = list(set(all_p1_syms.keys()).difference(all_p2_syms.keys()))
        added_syms_keys = list(set(all_p2_syms.keys()).difference(all_p1_syms.keys()))
        matching_sym_keys = list(set(all_p1_syms.keys()).intersection(all_p2_syms.keys()))

        unmatched = []
        matched = []

        # translate keys to symbols
        unmatched.extend([all_p1_syms[key] for key in deleted_sym_keys])
        unmatched.extend([all_p2_syms[key] for key in added_syms_keys])
        matched = [all_p1_syms[key] for key in matching_sym_keys]

        self.logger.info(f'Found unmatched: {len(unmatched)} matched: {len(matched)} symbols')

        return [unmatched, matched]

    def syms_need_diff(
        self,
        sym: 'ghidra.program.model.symbol.Symbol',
        sym2: 'ghidra.program.model.symbol.Symbol',
        match_types: list,
        skip_types: list = []
    ) -> bool:
        """
        Determine quickly if a function match requires a deeper diff
        If the the match type == any of the skip types. Return false.
        """

        from ghidra.program.model.symbol import SourceType

        func: 'ghidra.program.model.listing.Function' = sym.program.functionManager.getFunctionAt(sym.address)
        func2: 'ghidra.program.model.listing.Function' = sym2.program.functionManager.getFunctionAt(sym2.address)

        assert func is not None and func2 is not None

        need_diff = False

        if not any(skip_type in match_types for skip_type in skip_types):
            if func.body.numAddresses != func2.body.numAddresses:
                need_diff = True
            elif sym.referenceCount != sym2.referenceCount:
                need_diff = True

        return need_diff

    def gen_proj_bin_name_from_path(self, path: Path):
        """
        Generate unique project name from binary for Ghidra Project
        """

        return '-'.join((path.name, sha1_file(path.absolute())[:6]))

    def normalize_ghidra_decomp(self, code: list):
        """
        Normalize some of the dynamic labels to simplify the diff
        ie. Translate LAB_0003234 to LAB_0,LAB_1, etc.
        Renames based on first appearance in decompilation

        """

        default_labels = ['LAB', 'DAT', 'SUB', 'UNK', 'EXT', 'FUN_', 'OFF_']

        matches = {}
        for i, line in enumerate(code):

            for label in default_labels:

                match = re.search(fr'{label}_[0-9a-f]+', line)
                if match is not None:
                    if matches.get(match.group(0)) is None:
                        prefix = match.group(0).split('_')[0]
                        matches[match.group(0)] = f'{prefix}_{len(matches)}'
                    code[i] = line.replace(match.group(0), matches[match.group(0)])
                    # TODO fix this line to work when a line has multiple default label

    def diff_bins(
            self,
            old: Union[str, Path],
            new: Union[str, Path],
            ignore_FUN: bool = False,
            force_diff=False
    ) -> dict:
        """
        Diff the old and new binary from the GhidraProject.
        ignore_FUN : skip nameless functions matching names containing "FUN_". Useful for increasing speed of diff.
        last_attempt : flag to prevent infinte loop on recursive instances
        """

        self.logger.info(f'Diffing bins: {old} - {new}')

        start = time()

        # reset pdiff
        pdiff = {}

        old = Path(old)
        new = Path(new)

        p1_name = self.gen_proj_bin_name_from_path(old)
        p2_name = self.gen_proj_bin_name_from_path(new)

        # analysis options used
        pdiff['program_options'] = {}

        # need RW program to get full options
        p1 = self.project.openProgram("/", p1_name, False)

        if p1_name == p2_name:
            self.logger.warn(f'Diffed files have the same content. Are you sure you want to do this??')
            p2 = p1
        else:
            p2 = self.project.openProgram("/", p2_name, False)

        pdiff['program_options'][p1.name] = self.get_all_program_options(p1)
        pdiff['program_options'][p2.name] = self.get_all_program_options(p2)

        self.project.close(p1)
        self.project.close(p2)

        # now open both programs read-only
        p1 = self.project.openProgram("/", p1_name, True)
        if p1_name == p2_name:
            p2 = p1
        else:
            p2 = self.project.openProgram("/", p2_name, True)

        # setup decompilers
        self.setup_decompliers(p1, p2)

        self.logger.info(f"Loaded old program: {p1.name}")
        self.logger.info(f"Loaded new program: {p2.name}")

        if not force_diff and not self.force_diff:
            # ensure architectures match
            assert p1.languageID == p2.languageID, 'p1: {} != p2: {}. The arch or processor does not match. Add --force-diff to ignore this assert'

            # sanity check - ensure both programs have symbols, or both don't
            sym_count_diff = abs(p1.getSymbolTable().numSymbols - p2.getSymbolTable().numSymbols)
            assert sym_count_diff < 4000, f'Symbols counts between programs ({p1.name} and {p2.name}) are too high {sym_count_diff}! Likely bad analyiss or only one binary has symbols! Check Ghidra analysis or pdb! Add --force-diff to ignore this assert'

        # Find (non function) symbols
        unmatched_nf_syms, _ = self.diff_nf_symbols(p1, p2)

        # Find functions matches
        unmatched, matched, skip_types = self.find_matches(p1, p2)

        # dedupe unmatched funcs with syms by names (rare but somtimes Ghidra symbol types get crossed, or pdb parsing issues)
        self.logger.info('Deduping symbols and functions...')

        dupes = []
        for func in unmatched:
            for sym in unmatched_nf_syms:
                # ensure they are from different progs
                if func.getName(True) == sym.getName(True) and func.getProgram() != sym.getProgram():
                    dupes.append(func)
                    dupes.append(sym)

        for dupe in dupes:
            if dupe in unmatched:
                self.logger.warn(f'Removing function dupe: {dupe}')
                unmatched.remove(dupe)
            if dupe in unmatched_nf_syms:
                self.logger.warn(f'Removing symbol dupe: {dupe}')
                unmatched_nf_syms.remove(dupe)

        deleted_symbols = []
        added_symbols = []
        deleted_strings = []
        added_strings = []

        self.logger.info('Sorting symbols and strings...')

        for sym in unmatched_nf_syms:
            if sym.program == p1:
                if self.is_sym_string(sym):
                    self.logger.debug(f'Found deleted string: {sym}')
                    deleted_strings.append(sym)
                else:
                    deleted_symbols.append(sym)
            else:
                if self.is_sym_string(sym):
                    self.logger.debug(f'Found added string: {sym}')
                    added_strings.append(sym)
                else:
                    added_symbols.append(sym)

        symbols = {}
        funcs = {}
        strings = {}
        symbols['added'] = []
        symbols['deleted'] = []
        strings['added'] = []
        strings['deleted'] = []

        deleted_funcs = []
        added_funcs = []
        modified_funcs = []
        all_match_types = []
        all_diff_types = []
        funcs_need_decomp = []

        self.logger.info('Sorting functions...')

        # thread the symbol lookups
        #   esyms are memoized and can be later just read from memory
        if self.threaded:

            esym_lookups = []

            esym_lookups.extend(deleted_strings)
            esym_lookups.extend(added_strings)
            esym_lookups.extend(unmatched)

            # TODO consider removing this complexity
            funcs_need_decomp.extend(unmatched)

            for sym, sym2, match_types in matched:

                if not self.syms_need_diff(sym, sym2, match_types, skip_types):
                    continue

                funcs_need_decomp.append(sym)
                funcs_need_decomp.append(sym2)

            esym_lookups.extend(funcs_need_decomp)

            use_calling_counts = len(funcs_need_decomp) < self.calling_count_funcs_limit

            # TODO add code to symbols!

            # there can be duplicate multiple function matches, just do this once
            # esym_lookups = list(set(esym_lookups))

            self.logger.info(f'Starting esym lookups for {len(esym_lookups)} symbols using {self.max_workers} threads')

            completed = 0
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # futures = (executor.submit(self.enhance_sym, sym, thread_id % self.max_workers, 15, (sym in funcs_need_decomp), (use_calling_counts and sym in funcs_need_decomp))
                futures = (executor.submit(self.enhance_sym, sym, thread_id % self.max_workers, 60, (sym in funcs_need_decomp), (use_calling_counts and sym in funcs_need_decomp))
                           for thread_id, sym in enumerate(esym_lookups))

                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    completed += 1
                    if (completed % int((len(esym_lookups) * .05) + 1) == 0):
                        self.logger.info(f'Completed {completed} at {int(completed/len(esym_lookups)*100)}%')

        for sym in deleted_symbols:
            symbols['deleted'].append(sym.name)

        for sym in added_symbols:
            symbols['added'].append(sym.name)

        for sym in deleted_strings:
            strings['deleted'].append(self.enhance_sym(sym))

        for sym in added_strings:
            strings['added'].append(self.enhance_sym(sym))

        for lost in unmatched:
            elost = self.enhance_sym(lost)

            # deleted func
            if lost.getProgram().getName() == p1.getName():
                deleted_funcs.append(elost)
            else:
                added_funcs.append(elost)

        for sym, sym2, match_types in matched:

            first_match = match_types[0]
            all_match_types.append(first_match)

            if not self.syms_need_diff(sym, sym2, match_types, skip_types):
                continue

            diff_type = []
            diff = ''
            only_code_diff = ''

            ematch_1 = self.enhance_sym(sym)
            ematch_2 = self.enhance_sym(sym2)

            old_code = ematch_1['code'].splitlines(True)
            new_code = ematch_2['code'].splitlines(True)

            old_code_no_sig = ematch_1['code'].split('{', 1)[1].splitlines(
                True) if ematch_1['code'] is not None and "Failed to decompile" not in ematch_1['code'] and '{' in ematch_1['code'] else ematch_1['code']

            new_code_no_sig = ematch_2['code'].split('{', 1)[1].splitlines(
                True) if ematch_2['code'] is not None and "Failed to decompile" not in ematch_2['code'] and '{' in ematch_2['code'] else ematch_2['code']

            old_instructions = ematch_1['instructions']
            new_instructions = ematch_2['instructions']

            instructions_ratio = round(difflib.SequenceMatcher(None, old_instructions, new_instructions).ratio(),2)

            old_mnemonics = ematch_1['mnemonics']
            new_mnemonics = ematch_2['mnemonics']

            mnemonics_ratio = round(difflib.SequenceMatcher(None, old_mnemonics, new_mnemonics).ratio(),2)

            old_blocks = ematch_1['blocks']
            new_blocks = ematch_2['blocks']

            blocks_ratio = round(difflib.SequenceMatcher(None, old_blocks, new_blocks).ratio(),2)

            # ignore signature for ratio
            ratio = round(difflib.SequenceMatcher(None, old_code_no_sig, new_code_no_sig).ratio(),2)

            self.normalize_ghidra_decomp(old_code)
            self.normalize_ghidra_decomp(new_code)

            from_file_name = ematch_1['fullname']
            to_file_name = ematch_2['fullname']

            diff = ''.join(list(difflib.unified_diff(old_code, new_code, lineterm='\n',
                           fromfile=from_file_name, tofile=to_file_name, n=1000)))
            only_code_diff = ''.join(list(difflib.unified_diff(old_code_no_sig, new_code_no_sig, lineterm='\n',
                                     fromfile=from_file_name, tofile=to_file_name)))  # ignores name changes

            if len(only_code_diff) > 0 and (mnemonics_ratio < 1.0 or blocks_ratio < 1.0):

                # TODO remove this hack to find false positives
                # potential decompile jumptable issue ghidra/issues/2452
                if not "Could not recover jumptable" in diff:
                    diff_type.append('code')
                else:
                    self.logger.warn(
                        f"Code diff type not appended for {ematch_1['name']} due to jumptable decomp issue")

            if ematch_1['name'] != ematch_2['name']:
                diff_type.append('name')

            if ematch_1['fullname'] != ematch_2['fullname']:
                diff_type.append('fullname')

            if ematch_1['refcount'] != ematch_2['refcount']:
                diff_type.append('refcount')

            if ematch_1['length'] != ematch_2['length']:
                diff_type.append('length')

            if ematch_1['sig'] != ematch_2['sig']:
                diff_type.append('sig')

            if ematch_1['address'] != ematch_2['address']:
                diff_type.append('address')

            if not (len(ematch_1['calling']) == len(ematch_2['calling']) and len(set(ematch_2['calling']).union(set(ematch_1['calling']))) == len(ematch_1['calling'])):
                diff_type.append('calling')

            if not (len(ematch_1['called']) == len(ematch_2['called']) and len(set(ematch_2['called']).union(set(ematch_1['called']))) == len(ematch_1['called'])):
                diff_type.append('called')

            if ematch_1['parent'] != ematch_2['parent']:
                diff_type.append('parent')

            # if no differences were found, there should not be a match (see modified func ident)
            if len(diff_type) == 0:
                self.logger.warn(f'no diff: {sym} {sym2} {match_types}')
                continue

            all_diff_types.extend(diff_type)

            modified_funcs.append({'old': ematch_1, 'new': ematch_2, 'diff': diff, 'diff_type': diff_type, 'ratio': ratio,
                                  'i_ratio': instructions_ratio, 'm_ratio': mnemonics_ratio, 'b_ratio': blocks_ratio, 'match_types': match_types})

        # Set funcs
        funcs['added'] = added_funcs
        funcs['deleted'] = deleted_funcs
        funcs['modified'] = modified_funcs

        # TODO Build Call Graphs

        # Set pdiff
        elapsed = time() - start
        items_to_process = len(added_funcs) + len(deleted_funcs) + len(modified_funcs) + \
            len(symbols['added']) + len(symbols['deleted'])
        unmatched_funcs_len = len(added_funcs) + len(deleted_funcs)
        total_funcs_len = p1.functionManager.functionCount + p2.functionManager.functionCount
        matched_funcs_len = total_funcs_len - unmatched_funcs_len
        matched_funcs_with_code_changes_len = len(
            [mod_func for mod_func in modified_funcs if 'code' in mod_func['diff_type']])
        matched_funcs_with_non_code_changes_len = len(
            [mod_func for mod_func in modified_funcs if 'code' not in mod_func['diff_type']])
        matched_funcs_no_changes_len = matched_funcs_len - \
            matched_funcs_with_code_changes_len - matched_funcs_with_non_code_changes_len
        match_func_similarity_percent = f'{((matched_funcs_no_changes_len / matched_funcs_len)*100):.4f}%'
        func_match_overall_percent = f'{((matched_funcs_len / total_funcs_len)*100):.4f}%'

        pdiff['stats'] = {'added_funcs_len': len(added_funcs), 'deleted_funcs_len': len(deleted_funcs), 'modified_funcs_len': len(modified_funcs), 'added_symbols_len': len(
            symbols['added']), 'deleted_symbols_len': len(symbols['deleted']), 'diff_time': elapsed, 'deleted_strings_len': len(deleted_strings), 'added_strings_len': len(added_strings),
            'match_types': Counter(all_match_types), 'items_to_process': items_to_process, 'diff_types': Counter(all_diff_types), 'unmatched_funcs_len': unmatched_funcs_len,
            'total_funcs_len': total_funcs_len, 'matched_funcs_len': matched_funcs_len, 'matched_funcs_with_code_changes_len': matched_funcs_with_code_changes_len,
            'matched_funcs_with_non_code_changes_len': matched_funcs_with_non_code_changes_len, 'matched_funcs_no_changes_len': matched_funcs_no_changes_len,
            'match_func_similarity_percent': match_func_similarity_percent, 'func_match_overall_percent': func_match_overall_percent}

        pdiff['symbols'] = symbols
        pdiff['strings'] = strings
        pdiff['functions'] = funcs

        pdiff['old_meta'] = self.get_metadata(p1)
        pdiff['new_meta'] = self.get_metadata(p2)

        # add pe url
        if 'visualstudio' in p1.compiler:
            pdiff['old_pe_url'] = self.get_pe_download_url(old, pdiff['old_meta']['PE Property[OriginalFilename]'])
        if 'visualstudio' in p1.compiler:
            pdiff['new_pe_url'] = self.get_pe_download_url(new, pdiff['new_meta']['PE Property[OriginalFilename]'])

        pdiff['md_credits'] = self.gen_credits()
        pdiff['html_credits'] = self.gen_credits(html=True)

        self.shutdown_decompilers(p1, p2)

        # reset global esym OTHERWISE this gets big
        self.esym_memo = {}

        self.project.close(p1)
        self.project.close(p2)

        self.logger.info("Finished diffing old program: {}".format(p1.getName()))
        self.logger.info("Finished diffing program: {}".format(p2.getName()))

        self.logger.debug(json.dumps(pdiff['stats'], indent=2))

        return pdiff

    def gen_diff_cmd_line(self, old_name: str, new_name: str) -> list:
        """
        Return command line used to start engine implementation
        """

        known_cmd_line = []
        extra_cmd_line = []
        full_cmd_line = []

        known_skip_args = ['old', 'new']

        if not self.args:
            err = 'Could not generate command line. No args passed to init of GhidraDiffEngine.'
            known_cmd_line.append(err)
            extra_cmd_line.append(err)
            full_cmd_line.append(err)
            self.logger.error(err)
        else:
            args = self.args
            defaults = self.get_default_args()

            self.logger.debug(f'Engine Args {args}')
            self.logger.debug(f'Engine Arg Defaults {defaults}')

            known_cmd_line.append(__package__)

            for arg in vars(args):

                opt = arg.replace("_", "-")
                val = getattr(args, arg)

                full_cmd_line.append(f'--{opt}')
                full_cmd_line.append(f'{val}')

                if arg in known_skip_args:
                    self.logger.debug(f'Skipping known_skip_arg: {arg}')
                    continue

                if val is None:
                    continue

                # which cmd_line
                if arg not in defaults:
                    cmd_line = extra_cmd_line
                else:
                    cmd_line = known_cmd_line

                # handle bool options
                if isinstance(val, bool):
                    if val:
                        cmd_line.append(f'--{opt}')
                else:
                    cmd_line.append(f'--{opt}')
                    cmd_line.append(f'{val}')

            # add binary names
            known_cmd_line.append(old_name)
            known_cmd_line.append(new_name)

        known_cmd_line = ' '.join(known_cmd_line)
        extra_cmd_line = ' '.join(extra_cmd_line)
        full_cmd_line = ' '.join(full_cmd_line)

        self.logger.info('Known Command line: %s', known_cmd_line)
        self.logger.info('Extra Command line: %s', extra_cmd_line)
        self.logger.debug('Extra Command line: %s', extra_cmd_line)

        return [known_cmd_line, extra_cmd_line, full_cmd_line]

    def validate_diff_json(
        self,
        results: json
    ) -> bool:

        is_valid = False
        try:
            json.loads(results)
            is_valid = True
        except ValueError as err:
            self.logger.error(err)
            raise err

        return is_valid

    def minimise_pdiff(self, pdiff: dict):
        """
        Function to allow subclasses to modify pdiff before writing to disk.
        Simply override this method. It will be called in `dump_pdiff_to_dir`
        """

        # reduce size of esym with hashes
        for func_type in ['added', 'deleted', 'modified']:

            for func in pdiff['functions'][func_type]:

                for field in ['instructions', 'mnemonics', 'blocks']:

                    if func_type == 'modified':

                        for func_mod_type in ['old', 'new']:

                            if not func[func_mod_type].get(field) is None:
                                if isinstance(func[func_mod_type][field], list) and len(func[func_mod_type][field]) > 0:
                                    func[func_mod_type][field] = hash(tuple(func[func_mod_type][field]))

                    else:

                        if func.get(field) is None:
                            continue

                        if isinstance(func[field], list) and len(func[field]) > 0:
                            func[field] = hash(tuple(func[field]))

        return pdiff

    def dump_pdiff_to_path(
        self,
        name: str,
        pdiff: Union[str, dict],
        output_path: Union[str, Path],
        side_by_side: bool = False,
        max_section_funcs: int = None,
        md_title: str = None,
        write_diff: bool = True,
        write_json: bool = True

    ) -> None:
        """
        Dump pdiff result to directory
        """

        def _clean_func(func, max=30) -> str:
            func = re.sub('`', '', func)
            func = func.replace('`', '')
            if len(func) > max:
                func = func[:max] + '...'
            return func.strip()

        if not write_diff and not write_json and not side_by_side:
            self.logger.warn('Not writing json or diff.md.')
            return

        if isinstance(pdiff, str):
            pdiff = json.loads(pdiff)

        pdiff = self.minimise_pdiff(pdiff)

        output_path = Path(output_path)
        output_path.mkdir(exist_ok=True)

        if write_diff:
            md_path = output_path / Path(name + '.md')
            self.logger.info(f'Writing md diff...')

            diff_text = self.gen_diff_md(
                pdiff,
                side_by_side=side_by_side,
                max_section_funcs=max_section_funcs,
                title=md_title)

            with md_path.open('w') as f:
                f.write(diff_text)

        if write_json:
            json_base_path = output_path / 'json'
            json_base_path.mkdir(exist_ok=True)
            json_path = json_base_path / Path(name + '.json')
            self.logger.info(f'Writing pdiff json...')

            with json_path.open('w') as f:
                json.dump(pdiff, f, indent=4)

        sxs_count = 0
        if side_by_side:
            sxs_output_path = output_path / Path('sxs_html')
            sxs_output_path.mkdir(exist_ok=True)

            sxs_diff_htmls = GhidriffMarkdown.gen_sxs_html_from_pdiff(pdiff)

            for func_name, sxs_diff_html in sxs_diff_htmls:

                # give line ending md despite html so it will render in gists and vscode
                sxs_diff_path = sxs_output_path / Path('.'.join([name, _clean_func(func_name), 'md']))
                sxs_diff_path.write_text(sxs_diff_html)

            combined_sxs_diff_html = GhidriffMarkdown.gen_combined_sxs_html_from_pdiff(pdiff)
            combined_sxs_diff_path = sxs_output_path / Path('.'.join([name, 'combined', 'html']))
            combined_sxs_diff_path.write_text(combined_sxs_diff_html)

        if write_diff:
            self.logger.info(f'Wrote {md_path}')
        if write_json:
            self.logger.info(f'Wrote {json_path}')
        if side_by_side:
            self.logger.info(f'Wrote {len(sxs_diff_htmls)} sxs hmtl diffs to {sxs_output_path}')

```

`ghidriff/implied_matches.py`:

```py
import concurrent.futures
from typing import List, TYPE_CHECKING

if TYPE_CHECKING:
    import ghidra
    from ghidra_builtins import *

# Python port of Ghidra/Features/VersionTracking/src/main/java/ghidra/feature/vt/gui/util/ImpliedMatchUtils.java


def get_actual_reference(program: "ghidra.program.model.listing.Program", ref_to_addr):

    ref_func = program.getFunctionManager().getFunctionAt(ref_to_addr)
    if ref_func is not None and ref_func.isThunk():
        # print('Resolving thunked func addr')
        ref_to_addr = ref_func.getThunkedFunction(True).getEntryPoint()

    return ref_to_addr


def find_matching_ref(ref_type, refs_from):

    if refs_from is None:
        return None

    for ref in refs_from:
        if ref.getReferenceType() == ref_type:
            return ref

    return None


def find_implied_match(src_func: "ghidra.program.model.listing.Function", dst_func: "ghidra.program.model.listing.Function", ref: "ghidra.program.model.symbol.Reference"):

    # // Get the reference type of the passed in reference and make sure it is either a call or
    # // data reference
    ref_type = ref.getReferenceType()

    # original
    # if not (ref_type.isCall() or ref_type.isData()):
    if not (ref_type.isCall()):  # ignore data
        # print(f'skipped: reftype is {ref_type}')
        return None

    # // Get the source reference's "to" address (the address the reference is pointing to)
    # // and make sure it is in the current program memory space

    src_ref_to_addr = ref.getToAddress()
    if not src_ref_to_addr.isMemoryAddress():
        print(f'skipped: src_ref_to_addr {src_ref_to_addr} is not Memory address!')
        return None

    src_ref_to_addr = get_actual_reference(src_func.getProgram(), src_ref_to_addr)

    src_ref_from_addr = ref.getFromAddress()

    from ghidra.feature.vt.api.correlator.address import VTHashedFunctionAddressCorrelation
    from ghidra.util.task import ConsoleTaskMonitor
    from ghidra.program.model.symbol import RefType

    monitor = ConsoleTaskMonitor()

    cor = VTHashedFunctionAddressCorrelation(src_func, dst_func)

    addr_range = cor.getCorrelatedDestinationRange(src_ref_from_addr, monitor)
    if addr_range is None:
        return None

    dest_addr = addr_range.getMinAddress()
    dest_ref_man = dst_func.getProgram().getReferenceManager()
    refs_from = dest_ref_man.getReferencesFrom(dest_addr)
    dest_ref = find_matching_ref(ref_type, refs_from)

    if dest_ref is None:
        return None

    dest_ref_to_addr = dest_ref.getToAddress()
    dest_ref_to_addr = get_actual_reference(dst_func.getProgram(), dest_ref_to_addr)

    actual_type = None
    if ref_type.isData():
        actual_type = 'DATA'
        if src_func.getProgram().getListing().getInstructionAt(src_ref_to_addr) is not None:
            if ref_type != RefType.DATA:
                return None
            actual_type = 'FUNCTION'
    else:
        actual_type = 'FUNCTION'

    if actual_type == 'FUNCTION':
        if src_func.getProgram().getFunctionManager().getFunctionAt(src_ref_to_addr) is None:
            return None

    return (src_ref_to_addr, dest_ref_to_addr, actual_type)


def find_implied_matches(src_func: "ghidra.program.model.listing.Function", dst_func: "ghidra.program.model.listing.Function"):
    """
    Find implied matches for already accepted functions
    # Ghidra/Features/VersionTracking/src/main/java/ghidra/feature/vt/gui/util/ImpliedMatchUtils.java
    """
    implied_matches = []

    ref_man = src_func.getProgram().getReferenceManager()
    body = src_func.getBody()
    for addr in ref_man.getReferenceSourceIterator(body, True):
        refs_from = ref_man.getReferencesFrom(addr)
        for ref in refs_from:
            implied_match = find_implied_match(src_func, dst_func, ref)
            if implied_match is not None:
                # print(implied_match)
                implied_matches.append(implied_match)

    return list(set(implied_matches))


def correlate_implied_matches(matches, p1_missing, p2_missing, p1_matches, p2_matches, p1, p2, max_workers, monitor, logger=None):
    """
    from all of the unmatched functions, get every function that has already been acceped that calls an unmatched function
    if the calling function has been accepted, then accept the unmatched called function
    """

    # build list of function entry points that have already been matched
    matched_src_addrs = {}
    matched_dst_addrs = {}
    for i, match in enumerate(matches):
        matched_src_addrs[match[0]] = i
        matched_dst_addrs[match[1]] = i

    potential_calling_funcs = []
    src_missing_addrs = []
    dst_missing_addrs = []

    for src_func in p1_missing:
        src_func: "ghidra.program.model.listing.Function" = src_func
        src_missing_addrs.append(src_func.getEntryPoint())
        potential_p1_calling_funcs = [func for func in list(src_func.getCallingFunctions(
            monitor)) if func.getEntryPoint() in matched_src_addrs.keys()]
        if logger is not None:
            logger.debug(
                f'Found {len(potential_p1_calling_funcs)} p1 calling functions for potential implied match for {src_func}')
        potential_calling_funcs.extend(potential_p1_calling_funcs)

    for dst_func in p2_missing:
        dst_func: "ghidra.program.model.listing.Function" = dst_func
        dst_missing_addrs.append(dst_func.getEntryPoint())
        potential_p2_calling_funcs = [func for func in list(
            dst_func.getCallingFunctions(monitor)) if func.getEntryPoint() in matched_dst_addrs.keys()]
        if logger is not None:
            logger.debug(
                f'Found {len(potential_p2_calling_funcs)} p2 calling functions for potential implied match for {dst_func}')
        potential_calling_funcs.extend(potential_p2_calling_funcs)

    potential_calling_funcs = list(set(potential_calling_funcs))

    # find all matches that might provide an implied match for unmatched functions
    potential_accepted_matches = []
    for func in potential_calling_funcs:
        match_index = None
        if matched_src_addrs.get(func.getEntryPoint()) is not None:
            match_index = matched_src_addrs.get(func.getEntryPoint())
        elif matched_dst_addrs.get(func.getEntryPoint()) is not None:
            match_index = matched_dst_addrs.get(func.getEntryPoint())

        if match_index is not None:
            match = list(matches.keys())[match_index]
            f1 = p1.functionManager.getFunctionAt(match[0])
            f2 = p2.functionManager.getFunctionAt(match[1])
            potential_accepted_matches.append((f1, f2))

    recovered = 0
    completed = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = (executor.submit(find_implied_matches, f1, f2)
                   for f1, f2 in potential_accepted_matches)

        for future in concurrent.futures.as_completed(futures):
            implied_matches = future.result()

            completed += 1
            if completed % 50 == 0:
                percent = int((float(completed)/len(potential_accepted_matches)) * 100)
                if logger is not None:
                    logger.debug(f'Completed {percent} %  recovered" {recovered}')

            if implied_matches is not None:
                for implied_match in implied_matches:
                    # only apply function matches
                    if implied_match[2] == 'FUNCTION':
                        # ensure implied match is correlating an unmatched function
                        if implied_match[0] in src_missing_addrs or implied_match[1] in dst_missing_addrs:
                            if matches.get((implied_match[0], implied_match[1])) is None:
                                recovered += 1

                            # Correlate function as Implied Match
                            name = 'Implied Match'
                            matches.setdefault((implied_match[0], implied_match[1]), {}).setdefault(name, 0)
                            matches[(implied_match[0], implied_match[1])][name] += 1
                            p1_matches.add(implied_match[0])
                            p2_matches.add(implied_match[1])

```

`ghidriff/markdown.py`:

```py
import difflib
import re
from textwrap import dedent
from typing import List, Tuple, Union, TYPE_CHECKING
import logging
import json

from mdutils.tools.Table import Table
from mdutils.mdutils import MdUtils
from mdutils.tools.TableOfContents import TableOfContents


class GhidriffMarkdown:

    def __init__(self, logging=logging.INFO) -> None:
        self.logger = self.setup_logger(level=logging)

    def setup_logger(self, level: int = logging.INFO) -> logging.Logger:
        """
        Setup Class Instance Logger
        """
        logging.basicConfig(
            format='%(levelname)-5s| %(name)s | %(message)s',
            datefmt='%H:%M:%S'
        )

        logger = logging.getLogger(__package__)
        logger.setLevel(level)

        return logger

    def _wrap_with_diff(self, diff: str) -> str:

        text = ''
        text += "```diff\n"
        text += diff
        text += "\n```\n"
        text += "\n"

        return text

    def _wrap_with_code(self, code: str, style='') -> str:
        text = ''
        text += f"```{style}" + "\n"
        text += code
        text += "\n```\n"
        text += "\n"

        return text

    def _wrap_with_details(self, diff: str, summary: str = None) -> str:

        text = ''
        text += "<details>\n"
        if summary:
            text += f"<summary>{summary}</summary>\n"
        text += '\n'
        text += diff
        text += "\n</details>\n"

        return text

    def _wrap_list_with_details(self, items: list, max_items: int = 10) -> str:

        data = None
        if isinstance(items, list):
            if len(items) > max_items:
                show_items = [item for i, item in enumerate(items) if i <= max_items]
                hide_itmes = [item for i, item in enumerate(items) if i > max_items]
                data = f'<details><summary>Expand for full list:<br>{"<br>".join(show_items)}</summary>{"<br>".join(hide_itmes)}</details>'
            else:
                data = f'{"<br>".join(items)}'
        else:
            # do nothing
            data = items

        return data

    def gen_esym_table(self, old_name, esym, max_items=10) -> str:

        table_list = []
        table_list.extend(['Key', old_name])
        column_len = len(table_list)

        skip_keys = ['code', 'instructions', 'mnemonics', 'blocks', 'parent']
        count = 1
        for key in esym:
            if key in skip_keys:
                continue

            if isinstance(esym[key], list):
                data = self._wrap_list_with_details(esym[key], max_items)
            else:
                data = esym[key]

            table_list.extend([key, data])
            count += 1

        diff_table = Table().create_table(columns=column_len, rows=count, text=table_list, text_align='center')

        return diff_table

    def gen_esym_table_diff(self, old_name, new_name, modified, max_items=10) -> str:
        diff_table = ''

        table_list = []
        table_list.extend(['Key', old_name, new_name])
        column_len = len(table_list)

        skip_keys = ['code', 'instructions', 'mnemonics', 'blocks', 'parent']
        count = 1
        for key in modified['old']:
            if key in skip_keys:
                continue
            if key in modified['diff_type']:
                diff_key = f"`{key}`"
            else:
                diff_key = f"{key}"

            if isinstance(modified['old'][key], list):
                data = self._wrap_list_with_details(modified['old'][key], max_items)
                data2 = self._wrap_list_with_details(modified['new'][key], max_items)
            else:
                data = modified['old'][key]
                data2 = modified['new'][key]

            table_list.extend([diff_key, data, data2])

            # table_list.extend([diff_key, modified['old'][key], modified['new'][key]])
            count += 1

        diff_table = Table().create_table(columns=column_len, rows=count,
                                          text=table_list, text_align='center')

        return diff_table

    def gen_esym_table_diff_meta(self, old_name, new_name, modified) -> str:
        diff_table = ''

        table_list = []
        table_list.extend(['Key', f"{old_name} - {new_name}"])
        column_len = len(table_list)

        keys = ['diff_type', 'ratio', 'i_ratio', 'm_ratio', 'b_ratio', 'match_types']
        count = 1
        for key in keys:
            val = modified[key]
            if isinstance(val,list):
                val = ','.join(val)
            table_list.extend([key, val])
            count += 1

        diff_table = Table().create_table(columns=column_len, rows=count, text=table_list, text_align='center')

        return diff_table

    def gen_esym_key_diff(self, esym: dict, esym2: dict, key: str, exclude=None, n=3) -> str:
        """
        Generate a difflib unified diff from two esyms and a key
        n is the number of context lines for diff lib to wrap around the found diff
        """
        diff = ''

        if exclude:
            items = [item for item in esym[key] if re.search(exclude, item) is None]
            items2 = [item for item in esym2[key] if re.search(exclude, item) is None]
        else:
            items = esym[key]
            items2 = esym2[key]

        diff += '\n'.join(difflib.unified_diff(items, items2,
                          fromfile=f"{esym['fullname']} {key}", tofile=f"{esym2['fullname']} {key}", lineterm='', n=n))

        return self._wrap_with_diff(diff)

    @staticmethod
    def gen_combined_sxs_html_from_pdiff(pdiff: dict) -> str:
        """
        pdiff: Standard pdiff from ghidriff
        """

        def _add_header(line: str):
            # print(line)
            pass

        sxs_diff_htmls = []

        for mod in pdiff['functions']['modified']:

            if 'code' not in mod['diff_type']:
                continue

            old_code = mod['old']['code']
            new_code = mod['new']['code']
            old_name = mod['old']['fullname']
            new_name = mod['new']['fullname']

            sxs_diff_html = GhidriffMarkdown.gen_code_table_diff_html(
                old_code, new_code, old_name, new_name)

            sxs_diff_htmls.append([mod['old']['name'], sxs_diff_html])

        table_htmls = [table[1] for table in sxs_diff_htmls]

        charset = 'utf-8'

        html_diff = difflib.HtmlDiff(tabsize=4)
        html = (html_diff._file_template % dict(
            styles=html_diff._styles,
            legend=dedent(html_diff._legend),
            table="\n".join(table_htmls),
            charset=charset)).encode(charset, 'xmlcharrefreplace').decode(charset)

        for line in html.splitlines(True):

            _add_header(line)

        return html

    @staticmethod
    def gen_sxs_html_from_pdiff(pdiff: dict) -> list:
        """
        pdiff: Standard pdiff from ghidriff
        """

        sxs_diff_htmls = []

        for mod in pdiff['functions']['modified']:

            if 'code' not in mod['diff_type']:
                continue

            old_code = mod['old']['code']
            new_code = mod['new']['code']
            old_name = mod['old']['fullname']
            new_name = mod['new']['fullname']

            sxs_diff_html = GhidriffMarkdown.gen_code_table_diff_html(
                old_code, new_code, old_name, new_name, bottom=pdiff['html_credits'])

            sxs_diff_htmls.append([mod['old']['name'], sxs_diff_html])

        return sxs_diff_htmls

    @ staticmethod
    def gen_code_table_diff_html(old_code,
                                 new_code,
                                 old_name,
                                 new_name,
                                 html_type='file',
                                 dedent_table: bool = True,
                                 bottom=None,
                                 tabsize=2
                                 ) -> str:
        """
        Generates side by side diff in HTML
        dedent_table: True will dedent the indented table so it renders the html table in markdown
        type: 
            - inline Simply return table,style,and legend withiout html and body tags
            - table only - just return table with no style
            - full - return difflib html template complete
        """

        charset = 'utf-8'

        if isinstance(old_code, str):
            old_code = old_code.splitlines(True)
        if isinstance(new_code, str):
            new_code = new_code.splitlines(True)

        match html_type:
            case 'inline':
                styles = '<style type="text/css">%(styles)s\n</style>' % dict(
                    styles=difflib.HtmlDiff(tabsize=tabsize)._styles)
                table = difflib.HtmlDiff(tabsize=tabsize).make_table(
                    old_code, new_code, fromdesc=old_name, todesc=new_name)

                diff_html = styles + table
                diff_html.encode(charset, 'xmlcharrefreplace').decode(charset)
            case 'table-only':
                diff_html = difflib.HtmlDiff(tabsize=4).make_table(
                    old_code, new_code, fromdesc=old_name, todesc=new_name)
            case 'file':
                diff_html = difflib.HtmlDiff(tabsize=tabsize).make_file(
                    old_code, new_code, fromdesc=old_name, todesc=new_name)
            case _:
                raise NotImplementedError

        if dedent_table:
            diff_html = diff_html.splitlines(True)

            # handle dedent of table so that it renders in markdown
            start_table = None
            end_table = None
            for i, line in enumerate(diff_html):
                if line.find('<table class="diff"') != -1:
                    start_table = i
                if line.find('</table>') != -1:
                    end_table = i

                if start_table and end_table:
                    break

            pre_table = ''.join(diff_html[:start_table])
            post_table = ''.join(diff_html[end_table+1:])

            table = dedent(''.join(diff_html[start_table:end_table+1]))

            diff_html = pre_table + table + post_table

        if html_type == 'file' and bottom:
            needle = '</body>'
            loc = diff_html.find(needle)
            diff_html = diff_html[:loc] + bottom + diff_html[loc:]

        return diff_html + '\n'

    def gen_table_from_dict(self, headers: list, items: dict):

        table = ''

        table_list = []
        table_list.extend(headers)
        column_len = len(table_list)

        count = 1
        for key, values in items.items():
            table_list.extend([key, values])
            count += 1

        table = Table().create_table(columns=column_len, rows=count, text=table_list, text_align='center')

        return table

    def gen_strings_diff(self, deleted_strings: dict, added_strings: dict):

        added = [f'{item["name"]}\n' for item in added_strings]

        deleted = [f'{item["name"]}\n' for item in deleted_strings]

        diff = ''.join(list(difflib.unified_diff(deleted, added,
                                                 lineterm='\n', fromfile='deleted strings', tofile='added strings')))

        return self._wrap_with_diff(diff)

    def gen_metadata_diff(
        self,
        pdiff: Union[str, dict]
    ) -> str:
        """Generate binary metadata diff"""

        if isinstance(pdiff, str):
            pdiff = json.loads(pdiff)

        old_meta = pdiff['old_meta']
        new_meta = pdiff['new_meta']

        old_text = ''
        old_name = f"{old_meta['Program Name']} Meta"

        new_text = ''
        new_name = f"{new_meta['Program Name']} Meta"

        for i in old_meta:
            self.logger.debug(f"{i}: {old_meta[i]}")
            old_text += f"{i}: {old_meta[i]}\n"

        for i in new_meta:
            self.logger.debug(f"{i}: {new_meta[i]}")
            new_text += f"{i}: {new_meta[i]}\n"

        diff = ''.join(list(difflib.unified_diff(old_text.splitlines(True), new_text.splitlines(
            True), lineterm='\n', fromfile=old_name, tofile=new_name, n=100)))

        return diff

    def gen_mermaid_diff_flowchart(self, pdiff: dict, max_section_funcs: int = 25) -> str:

        diff_flow = '''
```mermaid

flowchart LR

{modified_links}

subgraph {new_bin}
    {new_modified}
    {added_sub}
end

subgraph {old_bin}
    {old_modified}
    {deleted_sub}
end

```'''

        added = []
        deleted = []
        modified_links = []
        old_modified = []
        new_modified = []

        old_bin = pdiff['old_meta']['Program Name']
        new_bin = pdiff['new_meta']['Program Name']

        for i, func in enumerate(pdiff['functions']['added']):
            if func['external']:
                # remove :: from external names
                name = func['fullname'].replace('::', '-')
            else:
                name = func['name']
            added.append(self._clean_md_header(name))

            if max_section_funcs and i > max_section_funcs:
                msg = f"{len(pdiff['functions']['added']) - max_section_funcs}_more_added_funcs_omitted..."
                added.append(self._clean_md_header(msg))
                break

        for i, func in enumerate(pdiff['functions']['deleted']):
            if func['external']:
                name = func['fullname'].replace('::', '-')
            else:
                name = func['name']
            deleted.append(self._clean_md_header(name))

            if max_section_funcs and i > max_section_funcs:
                msg = f"{len(pdiff['functions']['deleted']) - max_section_funcs}_more_deleted_funcs_omitted..."
                deleted.append(self._clean_md_header(msg))
                break

        modified_code_funcs = [func for func in pdiff['functions']['modified'] if 'code' in func['diff_type']]

        for i, modified in enumerate(modified_code_funcs):

            if max_section_funcs and i > max_section_funcs:
                modified_links.append(
                    f"{old_bin}<--{len(modified_code_funcs) - max_section_funcs}ommited-->{new_bin}")
                break

            old_modified.append(self._clean_md_header(
                f"{modified['old']['fullname']}-{modified['old']['paramcount']}-old"))
            new_modified.append(self._clean_md_header(
                f"{modified['new']['fullname']}-{modified['old']['paramcount']}-new"))
            modified_links.append(
                f"{self._clean_md_header(modified['old']['fullname'])}-{modified['old']['paramcount']}-old<--Match {int(modified['b_ratio']*100)}%-->{self._clean_md_header(modified['new']['fullname'])}-{modified['old']['paramcount']}-new")

        deleted_sub = ''
        added_sub = ''
        if len(deleted) > 0:
            deleted_sub = '''subgraph Deleted\ndirection LR\n{}\nend'''.format('\n    '.join(deleted))
        if len(added) > 0:
            added_sub = '''subgraph Added\ndirection LR\n{}\nend'''.format('\n    '.join(added))

        return diff_flow.format(old_bin=old_bin, new_bin=new_bin, added_sub=added_sub, deleted_sub=deleted_sub, modified_links='\n'.join(modified_links), old_modified='\n'.join(old_modified), new_modified='\n'.join(new_modified))

    def gen_mermaid_pie_from_dict(self, data: dict, title: str, skip_keys: list = None, include_keys: list = None) -> str:
        """
        Generate basic mermaidjs Pie chart from dict
        skip_keys: [ 'skipkey1', 'skipkey45'] List of keys to skip from Dict
        includes_keys: ['random_key1', 'otherkey2'] - Only include these keys
        Default: include all keys and values from dict.
        """

        pie_template = '''
```mermaid
pie showData
    title {title}
{rows}
```
'''
        rows = []

        for key, value in data.items():

            row = None

            if skip_keys and key in skip_keys:
                continue

            if include_keys:
                if key in include_keys:
                    row = f'"{self._clean_md_header(key)}" : {value}'
            else:
                row = f'"{self._clean_md_header(key)}" : {value}'

            if row:
                rows.append(row)

        return pie_template.format(title=title, rows='\n'.join(rows))

    def _clean_md_header_lower(self, text):
        return re.sub('[^a-z0-9_\-]', '', text.lower().replace(' ', '-'))

    def _clean_md_header(self, text):
        return re.sub('[^A-Za-z0-9_\-]', '', text.replace(' ', '-'))

    def gen_pe_download_cmd(self, pdiff: dict) -> str:
        """
        Generates Windows PE file download command        
        """

        def _decode_arch(proc, addr_size):
            arch = None
            if proc == 'x86':
                if addr_size == '64':
                    arch = 'x64'
                else:
                    arch = proc
            else:
                if addr_size == '64':
                    arch = 'arm64'
                else:
                    arch = 'arm'

            return arch

        old_url = pdiff['old_pe_url']
        new_url = pdiff['new_pe_url']

        # PE Property[OriginalFilename]: localspl.dll
        old_filename = pdiff['old_meta']['PE Property[OriginalFilename]'].lower()
        new_filename = pdiff['new_meta']['PE Property[OriginalFilename]'].lower()

        # PE Property[ProductVersion]: 10.0.22000.795
        old_ver = pdiff['old_meta']['PE Property[ProductVersion]']
        new_ver = pdiff['new_meta']['PE Property[ProductVersion]']

        # Processor: x86
        # Address Size: 64
        old_arch = _decode_arch(pdiff['old_meta']['Processor'], pdiff['old_meta']['Address Size'])
        new_arch = _decode_arch(pdiff['new_meta']['Processor'], pdiff['new_meta']['Address Size'])

        old_dl = f"wget {old_url} -O {old_filename}.{old_arch}.{old_ver}"
        new_dl = f"wget {new_url} -O {new_filename}.{new_arch}.{new_ver}"

        dl_cmd = "\n".join((old_dl, new_dl))

        return dl_cmd

    def gen_diff_md(
        self,
        pdiff: Union[str, dict],
        title=None,
        side_by_side: bool = False,
        max_section_funcs: int = None,
        include_code: bool = False
    ) -> str:
        """
        Generate Markdown Diff from pdiff match results
        """

        self.logger.info(f"Generating markdown from {pdiff['stats']}")

        # use max passed into function, revert to class if it exists
        if not max_section_funcs:
            max_section_funcs = self.max_section_funcs

        self.logger.debug(f"max_section_funcs: {max_section_funcs}")

        if isinstance(pdiff, str):
            pdiff = json.loads(pdiff)

        funcs = pdiff['functions']

        old_name = pdiff['old_meta']['Program Name']
        new_name = pdiff['new_meta']['Program Name']

        if title:
            title = title
        else:
            title = f"{old_name}-{new_name} Diff"

        md = MdUtils('diff', title=title)

        # change title to atx style
        md.title = md.header.choose_header(level=1, title=title, style='atx')

        md.new_header(1, 'Visual Chart Diff')
        md.new_paragraph(self.gen_mermaid_diff_flowchart(pdiff))
        md.new_paragraph(self.gen_mermaid_pie_from_dict(
            pdiff['stats'], f"Function Matches - {pdiff['stats']['func_match_overall_percent']}",
            include_keys=['matched_funcs_len', 'unmatched_funcs_len']))
        md.new_paragraph(self.gen_mermaid_pie_from_dict(
            pdiff['stats'],
            f"Matched Function Similarity - {pdiff['stats']['match_func_similarity_percent']}",
            include_keys=['matched_funcs_with_code_changes_len', 'matched_funcs_with_non_code_changes_len', 'matched_funcs_no_changes_len']))

        # Create Metadata section
        md.new_header(1, 'Metadata')

        md.new_header(2, 'Ghidra Diff Engine')

        md.new_header(3, 'Command Line')
        known_cmd, extra_cmd, full_cmd = self.gen_diff_cmd_line(old_name, new_name)
        md.new_header(4, 'Captured Command Line', add_table_of_contents='n')
        md.new_paragraph(self._wrap_with_code(known_cmd))
        md.new_header(4, 'Verbose Args', add_table_of_contents='n')        
        md.new_paragraph(self._wrap_with_details(self._wrap_with_code(full_cmd)))

        if pdiff.get('old_pe_url') is not None and pdiff.get('new_pe_url') is not None:
            md.new_header(4, 'Download Original PEs')
            md.new_paragraph(self._wrap_with_code(self.gen_pe_download_cmd(pdiff)))

        md.new_header(2, 'Binary Metadata Diff')
        md.new_paragraph(self._wrap_with_diff(self.gen_metadata_diff(pdiff)))

        md.new_header(2, 'Program Options')
        skip_options = 'Program Information'  # options duplicate metadta
        for key in pdiff['program_options'].keys():
            for opt_name in pdiff['program_options'][key]:
                if opt_name in skip_options:
                    continue
                # md.new_header(3, f'Ghidra {key} {opt_name.capitalize()} Options', add_table_of_contents='n')
                if pdiff['program_options'][key][opt_name] is not None:
                    md.new_paragraph(self._wrap_with_details(self.gen_table_from_dict(
                        [f'{opt_name.capitalize()} Option', 'Value'], pdiff['program_options'][key][opt_name]), f'Ghidra {key} {opt_name.capitalize()} Options'))

                else:
                    md.new_paragraph(f'*No {opt_name.capitalize()} set.*')

        md.new_header(2, 'Diff Stats')
        md.new_paragraph(self.gen_table_from_dict(['Stat', 'Value'], pdiff['stats']))
        md.new_paragraph(self.gen_mermaid_pie_from_dict(pdiff['stats']['match_types'], 'Match Types'))
        md.new_paragraph(self.gen_mermaid_pie_from_dict(pdiff['stats'], 'Diff Stats', include_keys=[
                         'added_funcs_len', 'deleted_funcs_len', 'modified_funcs_len']))
        md.new_paragraph(self.gen_mermaid_pie_from_dict(
            pdiff['stats'], 'Symbols', include_keys=['added_symbols_len', 'deleted_symbols_len']))

        # Create Strings Section
        md.new_header(2, 'Strings')
        if len(pdiff['strings']['deleted']) > 0 or len(pdiff['strings']['added']):
            md.new_paragraph(self.gen_mermaid_pie_from_dict(
                pdiff['stats'], 'Strings', include_keys=['added_strings_len', 'deleted_strings_len']))

            md.new_header(3, 'Strings Diff', add_table_of_contents='n')
            md.new_paragraph(self.gen_strings_diff(pdiff['strings']['deleted'], pdiff['strings']['added']))
        else:
            md.new_paragraph('*No string differences found*\n')

        # Create Deleted section
        md.new_header(1, 'Deleted')

        for i, esym in enumerate(funcs['deleted']):

            if i > max_section_funcs:
                md.new_header(2, 'Max Deleted Section Functions Reached Error')
                md.new_line(f"{len(funcs['deleted']) - max_section_funcs} Deleted Functions Ommited...")
                self.logger.warn(f'Max Deleted Section Functions {max_section_funcs} Reached')
                self.logger.warn(f"{len(funcs['deleted']) - max_section_funcs} Functions Ommited...")
                break

            if esym['external']:
                md.new_header(2, esym['fullname'])
            else:
                md.new_header(2, esym['name'])
            md.new_header(3, "Function Meta", add_table_of_contents='n')
            md.new_paragraph(self.gen_esym_table(old_name, esym))

            # only show code if not above section max
            if len(funcs['deleted']) < max_section_funcs:
                old_code = esym['code'].splitlines(True)
                new_code = ''.splitlines(True)
                diff = ''.join(list(difflib.unified_diff(old_code, new_code,
                                                         lineterm='\n', fromfile=esym['fullname'], tofile=esym['fullname'])))
                if len(diff) > 0:
                    md.new_paragraph(self._wrap_with_diff(diff))
                else:
                    md.new_paragraph(f"*No code available for {esym['fullname']}*")

        # Create Added section
        md.new_header(1, 'Added')

        for i, esym in enumerate(funcs['added']):

            if i > max_section_funcs:
                md.new_header(2, 'Max Added Section Functions Reached Error')
                md.new_line(f"{len(funcs['added']) - max_section_funcs} Added Functions Ommited...")
                self.logger.warn(f'Max Added Section Functions {max_section_funcs} Reached')
                self.logger.warn(f"{len(funcs['added']) - max_section_funcs} Functions Ommited...")
                break

            if esym['external']:
                md.new_header(2, esym['fullname'])
            else:
                md.new_header(2, esym['name'])
            md.new_header(3, "Function Meta", add_table_of_contents='n')
            md.new_paragraph(self.gen_esym_table(new_name, esym))

            # only show code if not above section max
            if len(funcs['added']) < max_section_funcs:
                old_code = ''.splitlines(True)
                new_code = esym['code'].splitlines(True)
                diff = ''.join(list(difflib.unified_diff(old_code, new_code,
                                                         lineterm='\n', fromfile=esym['fullname'], tofile=esym['fullname'])))
                if len(diff) > 0:
                    md.new_paragraph(self._wrap_with_diff(diff))
                else:
                    md.new_paragraph(f"*No code available for {esym['fullname']}*")

        # Create Modified section
        md.new_header(1, 'Modified')
        md.new_paragraph(f"*Modified functions contain code changes*")
        for i, modified in enumerate(funcs['modified']):

            if i > max_section_funcs:
                md.new_header(2, 'Max Modified Section Functions Reached Error')
                md.new_line(f"{len(funcs['modified']) - max_section_funcs} Functions Ommited...")
                self.logger.warn(f'Max Modified Section Functions {max_section_funcs} Reached')
                self.logger.warn(f"{len(funcs['modified']) - max_section_funcs} Functions Ommited...")
                break

            diff = None

            if modified['old']['external']:
                old_func_name = modified['old']['name']
            else:
                old_func_name = modified['old']['fullname']

            # selectively include matches
            if 'code' in modified['diff_type']:

                md.new_header(2, old_func_name)

                md.new_header(3, "Match Info", add_table_of_contents='n')
                md.new_paragraph(self.gen_esym_table_diff_meta(old_name, new_name, modified))

                md.new_header(3, "Function Meta Diff", add_table_of_contents='n')
                md.new_paragraph(self.gen_esym_table_diff(old_name, new_name, modified))

                if 'called' in modified['diff_type']:
                    md.new_header(3, f"{old_func_name} Called Diff", add_table_of_contents='n')
                    md.new_paragraph(self.gen_esym_key_diff(modified['old'], modified['new'], 'called', n=0))
                if 'calling' in modified['diff_type']:
                    md.new_header(3, f"{old_func_name} Calling Diff", add_table_of_contents='n')
                    md.new_paragraph(self.gen_esym_key_diff(modified['old'], modified['new'], 'calling', n=0))

                md.new_header(3, f"{old_func_name} Diff", add_table_of_contents='n')
                md.new_paragraph(self._wrap_with_diff(modified['diff']))

                if include_code:
                    md.new_header(3, f"{old_func_name} Code", add_table_of_contents='n')
                    md.new_paragraph(self._wrap_with_code(modified['old']['code'], 'c'))
                    md.new_paragraph(self._wrap_with_code(modified['new']['code'], 'c'))

                # only include side by side diff if requested (this adds html to markdown and considerable size)
                if side_by_side:
                    md.new_header(3, f"{old_func_name} Side By Side Diff", add_table_of_contents='n')
                    html_diff = GhidriffMarkdown.gen_code_table_diff_html(
                        modified['old']['code'], modified['new']['code'], old_name, new_name, html_type='inline')
                    md.new_paragraph(html_diff)

        # Create Slightly Modified secion
        # slightly as in no code changes but other relevant changes.
        slight_mods = ['refcount', 'length', 'called', 'calling', 'name', 'fullname']

        md.new_header(1, 'Modified (No Code Changes)')
        md.new_paragraph(f"*Slightly modified functions have no code changes, rather differnces in:*")
        md.new_list(slight_mods)

        # skip this section (as it is mostly a bonus) if this markdown is already too big
        too_big = max_section_funcs < (len(funcs['added']) + len(funcs['deleted']) + len(funcs['modified']))

        if too_big:
            md.new_header(2, 'Section Skipped')
            md.new_paragraph(
                f"**This section was skipped because markdown was too big. Adjust max_section_funcs: {max_section_funcs} to a higher number.**")
        else:
            for modified in funcs['modified']:

                mods = set(slight_mods).intersection(set(modified['diff_type']))

                if 'code' not in modified['diff_type'] and len(mods) > 0:

                    if modified['old']['external']:
                        old_func_name = modified['old']['fullname']
                        new_func_name = modified['old']['fullname']
                    else:
                        old_func_name = modified['old']['name']
                        new_func_name = modified['old']['name']

                    if old_func_name.startswith('FUN_') or new_func_name.startswith('FUN_'):

                        ignore_called = False
                        ignore_calling = False

                        if len(modified['old']['called']) > 0 and len(modified['new']['called']) > 0:
                            called_set = set(modified['old']['called']).difference(modified['new']['called'])
                            ignore_called = all('FUN_' in name for name in list(called_set))

                        if len(modified['old']['calling']) > 0 and len(modified['new']['calling']) > 0:
                            calling_set = set(modified['old']['calling']).difference(modified['new']['calling'])
                            ignore_calling = all('FUN_' in name for name in list(calling_set))

                        # skip name and fullname changes
                        if len(mods.difference(['name', 'fullname'])) == 0:
                            continue
                        # if all called are FUN_ skip
                        elif 'called' in modified['diff_type'] and 'calling' in modified['diff_type'] and ignore_called and ignore_calling:
                            continue
                        elif 'calling' in modified['diff_type'] and ignore_calling:
                            continue
                        elif 'called' in modified['diff_type'] and called_set:
                            continue

                    md.new_header(2, old_func_name)

                    md.new_header(3, "Match Info", add_table_of_contents='n')
                    md.new_paragraph(self.gen_esym_table_diff_meta(old_name, new_name, modified))

                    md.new_header(3, "Function Meta Diff", add_table_of_contents='n')
                    md.new_paragraph(self.gen_esym_table_diff(old_name, new_name, modified))

                    if 'called' in modified['diff_type']:
                        md.new_header(3, f"{old_func_name} Called Diff", add_table_of_contents='n')
                        md.new_paragraph(self.gen_esym_key_diff(modified['old'], modified['new'], 'called', n=0))
                    if 'calling' in modified['diff_type']:
                        md.new_header(3, f"{old_func_name} Calling Diff", add_table_of_contents='n')
                        md.new_paragraph(self.gen_esym_key_diff(modified['old'], modified['new'], 'calling', n=0))

        # add credit
        md.new_paragraph(pdiff['md_credits'])

        # generate TOC and set style to atx
        md.table_of_contents += md.header.choose_header(level=1, title='TOC', style='atx')
        md.table_of_contents += TableOfContents().create_table_of_contents(md._table_titles, depth=3)

        return md.get_md_text()

```

`ghidriff/parser.py`:

```py
import inspect
import argparse
import ghidriff


def get_engine_classes() -> dict:
    engines = {}

    for name, klass in inspect.getmembers(ghidriff, inspect.isclass):
        if name.endswith('Diff'):
            engines[name] = klass

    return engines

def get_parser() -> argparse.ArgumentParser:
    """
    Build main ghidriff parser
    """

    parser = argparse.ArgumentParser(description='ghidriff - A Command Line Ghidra Binary Diffing Engine',
                                formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('old', nargs=1, help="Path to old version of binary '/somewhere/bin.old'")
    parser.add_argument('new', action='append', nargs='+',
                        help="Path to new version of binary '/somewhere/bin.new'. (For multiple new binaries add oldest to newest)")

    # setup Engine class options
    engines = get_engine_classes()
    parser.add_argument('--engine', help='The diff implementation to use.',
                        default='VersionTrackingDiff', choices=engines.keys())

    parser.add_argument('-o', '--output-path', help='Output path for resulting diffs', default='ghidriffs')
    parser.add_argument('--summary', help='Add a summary diff if more than two bins are provided', default=False)

    return parser
```

`ghidriff/simple_diff.py`:

```py
import hashlib

from typing import List, Tuple, TYPE_CHECKING

from .ghidra_diff_engine import GhidraDiffEngine

if TYPE_CHECKING:
    import ghidra
    from ghidra_builtins import *


class SimpleDiff(GhidraDiffEngine):
    """
    An Ghidra Diff implementation using simple comparison mechanisms
    """

    def find_matches(
        self,
        p1: "ghidra.program.model.listing.Program",
        p2: "ghidra.program.model.listing.Program",
        ignore_FUN: bool = False,
    ) -> dict:
        """
        Find matching and unmatched functions between p1 and p2
        """

        from ghidra.program.util import DiffUtility

        def _get_compare_key(sym: 'ghidra.program.model.symbol.Symbol', func: 'ghidra.program.model.listing.Function') -> tuple:
            """
            Builds tuple from symbol (parent, name, refcount, length, paramcount)
            """
            fhash = ''
            cu_count = 0
            if (not func.isThunk() and func.getBody().getNumAddresses() >= 10):
                # reset iterator
                code_units = func.getProgram().getListing().getCodeUnits(func.getBody(), True)

                mnemonics = []

                # Mnemonic Bulker
                for code in code_units:
                    mnemonics.append(code.getMnemonicString())
                    cu_count += 1

                fhash = hashlib.sha256(''.join(mnemonics).encode('UTF-8')).hexdigest()

            return (sym.getParentNamespace().toString().split('@')[0], sym.getName(True), sym.getReferenceCount(), func.body.numAddresses, func.parameterCount, fhash)

        def _syms_match(esym, esym2) -> Tuple[bool, str]:
            found = False
            match_type = None
            min_func_length = 15

            if esym2['name'] == esym['name'] and esym2['length'] == esym['length']:
                self.logger.info("Name + length {} {}".format(sym.getName(True), sym2.getName(True)))
                found = True
                match_type = 'Name:Length'
            elif esym2['address'] == esym2['address'] and esym2['length'] == esym['length'] and min([esym['length'], esym2['length']]) > min_func_length:
                self.logger.info("Address + Length {} {}".format(sym.getName(True), sym2.getName(True)))
                found = True
                match_type = 'Address:Length'
            elif esym2['paramcount'] == esym['paramcount'] and esym2['length'] == esym['length']:
                self.logger.info("param count + func len {} {}".format(sym.getName(True), sym2.getName(True)))
                found = True
                match_type = 'Param:Length'
            elif esym2['fullname'] == esym['fullname']:
                self.logger.info("Name Exact {} {}".format(sym.getName(True), sym2.getName(True)))
                found = True
                match_type = 'Fullname'

            return found, match_type

        old_funcs = []
        new_funcs = []
        old_symbols = []
        new_symbols = []

        # first pass detect added and deleted symbols
        common_sym_prefix = ['switch', 'FUN_', 'caseD', 'local_']

        for sym in p1.getSymbolTable().getDefinedSymbols():
            name = sym.getName()
            if not any([common in name for common in common_sym_prefix]):
                old_symbols.append(name)

        for sym in p2.getSymbolTable().getDefinedSymbols():
            name = sym.getName()
            if not any([common in name for common in common_sym_prefix]):
                new_symbols.append(name)

        olds = set(old_symbols)
        news = set(new_symbols)

        deleted_symbols = olds.difference(news)
        self.logger.info("\ndeleted symbols\n")
        for sym in deleted_symbols:
            self.logger.info(sym)

        added_symbols = news.difference(olds)
        self.logger.info("\nadded symbols\n")
        for sym in added_symbols:
            self.logger.info(sym)

        # Next pass
        # 1. remove false positives from symbols
        # 2. Build modified functions list based on (_get_compare_key)

        for sym in p1.getSymbolTable().getDefinedSymbols():
            key = sym.getName()
            if key in deleted_symbols:
                self.logger.info("{} {} {}".format(sym.getName(), sym.getAddress(), sym.getParentNamespace()))
                sym2 = DiffUtility.getSymbol(sym, p2)

                if sym2 and sym.getName(True) == sym2.getName(True):
                    self.logger.info(f"Removing {sym} from deleted, found match {sym2} in p2")
                    deleted_symbols.remove(key)

            if "function".lower() in sym.getSymbolType().toString().lower():
                func = p1.functionManager.getFunctionAt(sym.getAddress())

                if "FUN_" in func.name and ignore_FUN:
                    # ignore FUN_
                    continue
                old_funcs.append(_get_compare_key(sym, func))

        for sym in p2.getSymbolTable().getDefinedSymbols():
            key = sym.getName()
            if key in added_symbols:
                self.logger.info("{} {}".format(sym.getName(), sym.getAddress()))
                sym2 = DiffUtility.getSymbol(sym, p1)

                if sym2 and sym.getName(True) == sym2.getName(True):
                    self.logger.info(f"Removing {sym} from deleted, found match {sym2} in p1")
                    added_symbols.remove(key)

            if "function".lower() in sym.getSymbolType().toString().lower():
                func = p2.functionManager.getFunctionAt(sym.getAddress())
                if "FUN_" in func.name and ignore_FUN:
                    # ignore FUN_
                    continue
                new_funcs.append(_get_compare_key(sym, func))

        old_func_set = set(old_funcs)
        new_func_set = set(new_funcs)

        modified_old = sorted(old_func_set.difference(new_func_set))
        modified_new = sorted(new_func_set.difference(old_func_set))

        matching_compare_keys = sorted(old_func_set.intersection(new_func_set))

        self.logger.info("\nmodified_old_modified")
        for sym in modified_old:
            self.logger.info(sym)

        self.logger.info("\nmodified_new_modified")
        for sym in modified_new:
            self.logger.info(sym)

        p1_modified = []
        p2_modified = []

        # Find modified functions based on compare_key
        for sym in p1.getSymbolTable().getDefinedSymbols():

            if "function".lower() in sym.getSymbolType().toString().lower():
                func = p1.functionManager.getFunctionAt(sym.getAddress())
                if (_get_compare_key(sym, func)) in modified_old:
                    p1_modified.append(sym)

        for sym in p2.getSymbolTable().getDefinedSymbols():

            if "function".lower() in sym.getSymbolType().toString().lower():
                func = p2.functionManager.getFunctionAt(sym.getAddress())
                if (_get_compare_key(sym, func)) in modified_new:
                    p2_modified.append(sym)

        self.logger.info("\nmodified_old_modified")
        for sym in p1_modified:
            self.logger.info(sym)

        self.logger.info("\nmodified_new_modified")
        for sym in p2_modified:
            self.logger.info(sym)

        matched = []
        unmatched = []
        matches = []

        self.logger.info("\nMatching functions...")

        # match by name and paramcount
        for sym in p1_modified:
            for sym2 in p2_modified:

                if sym2 in matched:
                    continue

                func = p1.functionManager.getFunctionAt(sym.getAddress())
                func2 = p2.functionManager.getFunctionAt(sym2.getAddress())

                if sym.getName(True) == sym2.getName(True) and func.parameterCount == func2.parameterCount:
                    self.logger.info("FullName + Paramcount {} {}".format(sym.getName(True), sym2.getName(True)))
                    match_type = 'FullName:Param'
                    matched.append(sym)
                    matched.append(sym2)
                    matches.append([sym, sym2, match_type])

        for sym in p1_modified:
            found = False

            if sym in matched:
                continue

            sym2 = DiffUtility.getSymbol(sym, p2)

            if sym2 and sym.getName(True) == sym2.getName(True):
                found = True
                match_type = 'Direct'
                self.logger.info(f"direct getsymbol match {sym.getName(True)} {sym2.getName(True)}")
            else:
                for sym2 in p2_modified:
                    if sym2 in matched:
                        continue

                    esym = self.enhance_sym(sym)
                    esym2 = self.enhance_sym(sym2)
                    found, match_type = _syms_match(esym, esym2)

                    if found:
                        break

            if found:
                matched.append(sym)
                matched.append(sym2)
                matches.append([sym, sym2, match_type])
            else:
                self.logger.info(f"Deleted func found: {sym}")
                unmatched.append(sym)

        for sym in p2_modified:
            found = False
            match_type = None

            if sym in matched:
                continue

            sym2 = DiffUtility.getSymbol(sym, p1)

            if sym2 and sym.getName(True) == sym2.getName(True):
                found = True
                match_type = 'Direct'
                self.logger.info(f"direct getsymbol match {sym.getName(True)} {sym2.getName(True)}")
            else:
                for sym2 in p1_modified:
                    if sym2 in matched:
                        continue

                    esym = self.enhance_sym(sym)
                    esym2 = self.enhance_sym(sym2)
                    found, match_type = _syms_match(esym, esym2)

                    if found:
                        break

            if found:
                matched.append(sym)
                matched.append(sym2)
                matches.append([sym, sym2, match_type])
            else:
                self.logger.info(f"Added func found: {sym}")
                unmatched.append(sym)

        self.logger.info(len(p1_modified))
        self.logger.info(len(p2_modified))

        matches = sorted(matches, key=lambda x: str(x[0]))

        # Update symbols using match knowledge
        for match in matches:
            self.logger.info(f"{match[0].getName(True)} {match[1].getName(True)} {match[2]}")

            if match[0].getName() in deleted_symbols:
                deleted_symbols.remove(match[0].getName())
            if match[1].getName() in added_symbols:
                added_symbols.remove(match[1].getName())

        return [unmatched, matches, []]

```

`ghidriff/structural_graph_diff.py`:

```py
import hashlib

from typing import List, Tuple, TYPE_CHECKING

from .ghidra_diff_engine import GhidraDiffEngine

if TYPE_CHECKING:
    import ghidra
    from ghidra_builtins import *


class StructualGraphDiff(GhidraDiffEngine):
    """
    An Ghidra Diff implementation using simple comparison mechanisms
    """

    def find_matches(
        self,
        p1: "ghidra.program.model.listing.Program",
        p2: "ghidra.program.model.listing.Program",
        ignore_FUN: bool = False,
    ) -> dict:
        """
        Find matching and unmatched functions between p1 and p2
        """

        from ghidra.program.util import DiffUtility

        compare_key_memo = {}

        def _get_compare_key(sym: 'ghidra.program.model.symbol.Symbol', func: 'ghidra.program.model.listing.Function') -> tuple:
            """
            Builds tuple from symbol (parent, name, refcount, length, paramcount)
            """
            key = f'{sym.getID()}-{sym.getProgram().getName()}'

            if compare_key_memo.get(key) is None:

                fhash = ''
                cu_count = 0
                if (not func.isThunk() and func.getBody().getNumAddresses() >= 10):
                    # reset iterator
                    code_units = func.getProgram().getListing().getCodeUnits(func.getBody(), True)

                    mnemonics = []

                    # Mnemonic Bulker
                    for code in code_units:
                        mnemonics.append(code.getMnemonicString())
                        cu_count += 1

                    fhash = hashlib.sha256(''.join(mnemonics).encode('UTF-8')).hexdigest()

                compare_key_memo[key] = (cu_count, sym.getReferenceCount(),
                                         func.body.numAddresses, func.parameterCount, fhash)

            return compare_key_memo[key]

        def _get_compare_key2(sym: 'ghidra.program.model.symbol.Symbol', func: 'ghidra.program.model.listing.Function') -> tuple:
            """
            Builds structural graph hash. Thank you Halvar Flake!)
            """

            from ghidra.program.model.block import BasicBlockModel
            from ghidra.util.task import ConsoleTaskMonitor

            # graph structure vars
            num_basic_blocks = 0
            num_edges_of_blocks = 0
            num_call_subfunctions = 0

            if (not func.isThunk() and func.getBody().getNumAddresses() >= 10):

                monitor = ConsoleTaskMonitor()

                basic_model = BasicBlockModel(func.getProgram(), True)
                basic_blocks = basic_model.getCodeBlocksContaining(func.getBody(), monitor)

                for block in basic_blocks:
                    num_edges_of_blocks += block.getNumDestinations(monitor)
                    num_basic_blocks += 1

                    code_units = func.getProgram().getListing().getCodeUnits(block, True)
                    for code in code_units:
                        if code.getMnemonicString() == 'CALL':
                            num_call_subfunctions += 1

            # ignore Ghidra generated function names
            gen_sym_prefix = ['FUN_', 'lambda', 'destructor', '~', 'dynamic_initializer', 'dtor', '::']
            fname = sym.getName(True)

            if any([generated in fname for generated in gen_sym_prefix]):
                fname = ''
            # ignore sections of names with relative addresses or offsets
            # Catch_All@64616295 _GrowCompBitsBuffer@4
            elif '@' in fname:
                fname = fname.split('@')[0]
            elif '$' in fname:
                fname = fname.split('$')[0]

            return (fname, num_basic_blocks, num_edges_of_blocks, num_call_subfunctions, func.body.numAddresses, func.parameterCount)

        def _syms_match(esym, esym2) -> Tuple[bool, str]:
            found = False
            match_type = None
            min_func_length = 15

            if esym2['name'] == esym['name'] and esym2['paramcount'] == esym['paramcount']:
                self.logger.info("Name + Paramcount {} {}".format(sym.getName(True), sym2.getName(True)))
                found = True
                match_type = 'Name:Param'
            if esym2['name'] == esym['name'] and esym2['length'] == esym['length']:
                self.logger.info("Name + length {} {}".format(sym.getName(True), sym2.getName(True)))
                found = True
                match_type = 'Name:Length'
            elif esym2['fullname'] == esym['fullname']:
                self.logger.info("Name Exact {} {}".format(sym.getName(True), sym2.getName(True)))
                found = True
                match_type = 'Fullname'

            return found, match_type

        from ghidra.program.model.symbol import SymbolType

        self.logger.info(f'Calculating initial structural signature for all ')
        p1_funcs = {}
        p2_funcs = {}

        from time import time
        start_p1 = time()
        for sym in p1.getSymbolTable().getDefinedSymbols():
            if sym.getSymbolType() == SymbolType.FUNCTION:
                func = p1.functionManager.getFunctionAt(sym.getAddress())
                p1_funcs[func] = _get_compare_key2(sym, func)
        end_p1 = time()

        for sym in p2.getSymbolTable().getDefinedSymbols():
            if sym.getSymbolType() == SymbolType.FUNCTION:
                func = p2.functionManager.getFunctionAt(sym.getAddress())
                p2_funcs[func] = _get_compare_key2(sym, func)
        end_p2 = time()

        self.logger.info(f'p1 structural signature time: {end_p1 - start_p1}')
        self.logger.info(f'p2 structural signature time: {end_p2 - end_p1}')

        from collections import Counter

        p1_funcs_vals = Counter(p1_funcs.values())
        p2_funcs_vals = Counter(p1_funcs.values())

        count_non_unique = 0
        for key, val in p1_funcs_vals.items():
            if val > 1:
                count_non_unique += 1
                self.logger.debug(f'{key, val}')
        self.logger.info(f'non-unique p1: {count_non_unique}')
        count_non_unique = 0
        for key, val in p2_funcs_vals.items():
            if val > 1:
                count_non_unique += 1
                self.logger.debug(f'{key, val}')
        self.logger.info(f'non-unique p2: {count_non_unique}')

        old_func_set = set(p1_funcs.values())
        new_func_set = set(p2_funcs.values())

        modified_old = sorted(old_func_set.difference(new_func_set))
        modified_new = sorted(new_func_set.difference(old_func_set))

        matching_compare_keys = sorted(old_func_set.intersection(new_func_set))
        self.logger.info(f'modified_old: {len(modified_old)}')
        self.logger.info(f'modified_new: {len(modified_new)}')
        self.logger.info(f'matching_compare_keys: {len(matching_compare_keys)}')

        p1_modified = []
        p2_modified = []

        p1_name_to_func = {}
        p2_name_to_func = {}

        for func, compare_key in p1_funcs.items():
            if compare_key in modified_old:
                p1_modified.append(func.getSymbol())
                key = (func.getName(True), func.parameterCount)
                p1_name_to_func.setdefault(key, []).append(func)

        for func, compare_key in p2_funcs.items():
            if compare_key in modified_new:
                p2_modified.append(func.getSymbol())
                key = (func.getName(True), func.parameterCount)
                p2_name_to_func.setdefault(key, []).append(func)

        self.logger.info("\nmodified_old_modified")
        for sym in p1_modified:
            self.logger.info(sym)

        self.logger.info("\nmodified_new_modified")
        for sym in p2_modified:
            self.logger.info(sym)

        # Find modified functions based on compare_key
        for sym in p1.getSymbolTable().getDefinedSymbols():

            if "function".lower() in sym.getSymbolType().toString().lower():
                func = p1.functionManager.getFunctionAt(sym.getAddress())
                if (_get_compare_key2(sym, func)) in modified_old:
                    p1_modified.append(sym)

        for sym in p2.getSymbolTable().getDefinedSymbols():

            if "function".lower() in sym.getSymbolType().toString().lower():
                func = p2.functionManager.getFunctionAt(sym.getAddress())
                if (_get_compare_key2(sym, func)) in modified_new:
                    p2_modified.append(sym)

        matched = []
        unmatched = []
        matches = []

        self.logger.info("\nMatching functions...")

        # match by name and paramcount
        for sym in p1_modified:

            if sym in matched:
                continue

            func = sym.getProgram().functionManager.getFunctionAt(sym.getAddress())

            key = (func.getName(True), func.parameterCount)

            if p2_name_to_func.get(key):

                func2 = p2_name_to_func[key][0]

                if len(p2_name_to_func[key]) > 1:

                    for func in p1_name_to_func[key]:

                        if func.getSymbol() in matched:
                            continue

                        for func2 in p2_name_to_func[key]:

                            if func2.getSymbol() in matched:
                                continue
                            # fname, num_basic_blocks, num_edges_of_blocks, num_call_subfunctions, func.body.numAddresses, func.parameterCount

                            _, _, _, func2_num_calls, _, _ = p2_funcs[func2]
                            _, _, _, func_num_calls, _, _ = p1_funcs[func]

                            if func2_num_calls == func_num_calls:
                                sym = func.getSymbol()
                                sym2 = func2.getSymbol()
                                self.logger.info(
                                    f"FullName + Paramcount + NumCalls {sym.getName(True)} {sym2.getName(True)}")
                                match_type = 'FullName:Param:NumCalls'
                                matched.append(sym)
                                matched.append(sym2)
                                matches.append([sym, sym2, match_type])

                else:
                    sym2 = func2.getSymbol()
                    self.logger.info(f"FullName + Paramcount {sym.getName(True)} {sym2.getName(True)}")
                    match_type = 'FullName:Param'
                    matched.append(sym)
                    matched.append(sym2)
                    matches.append([sym, sym2, match_type])

        for sym in p1_modified:
            found = False

            if sym in matched:
                continue

            sym2 = DiffUtility.getSymbol(sym, p2)

            if sym2 and sym.getName(True) == sym2.getName(True):
                found = True
                match_type = 'Direct'
                self.logger.info(f"direct getsymbol match {sym.getName(True)} {sym2.getName(True)}")
            else:
                for sym2 in p2_modified:
                    if sym2 in matched:
                        continue

                    func = sym.getProgram().functionManager.getFunctionAt(sym.getAddress())
                    ck1 = _get_compare_key(sym, func)

                    func2 = sym2.getProgram().functionManager.getFunctionAt(sym2.getAddress())
                    ck2 = _get_compare_key(sym2, func2)

                    if ck1[4] == ck2[4]:
                        found = True
                        match_type = 'MnemonicOrdered:Hash'
                        self.logger.info(f"{match_type} match {sym.getName(True)} {sym2.getName(True)}")

                    if found:
                        break

            if found:
                matched.append(sym)
                matched.append(sym2)
                matches.append([sym, sym2, match_type])
            else:
                self.logger.info(f"Deleted func found: {sym}")
                self.logger.info(
                    f"Deleted func found: {_get_compare_key(sym,sym.getProgram().functionManager.getFunctionAt(sym.getAddress()))}")
                unmatched.append(sym)
                # matched.append(sym)  # TODO check this

        for sym in p2_modified:
            found = False
            match_type = None

            if sym in matched:
                continue

            sym2 = DiffUtility.getSymbol(sym, p1)

            if sym2 and sym.getName(True) == sym2.getName(True):
                found = True
                match_type = 'Direct'
                self.logger.info(f"direct getsymbol match {sym.getName(True)} {sym2.getName(True)}")
            else:
                for sym2 in p1_modified:
                    if sym2 in matched:
                        continue

                    esym = self.enhance_sym(sym)
                    esym2 = self.enhance_sym(sym2)
                    found, match_type = _syms_match(esym, esym2)

                    if found:
                        break

            if found:
                matched.append(sym)
                matched.append(sym2)
                matches.append([sym, sym2, match_type])
            else:
                self.logger.info(f"Added func found: {sym}")
                unmatched.append(sym)

        self.logger.info(len(p1_modified))
        self.logger.info(len(p2_modified))

        matches = sorted(matches, key=lambda x: str(x[0]))

        # no need to skip types because we just ignore the matches for structual sig
        skip_types = []

        return [unmatched, matches, skip_types]

```

`ghidriff/utils.py`:

```py
import hashlib
from pathlib import Path
from struct import unpack
from functools import lru_cache


@lru_cache(None)
def sha1_file(path: str) -> str:
    # https://stackoverflow.com/questions/22058048/hashing-a-file-in-python
    # BUF_SIZE is totally arbitrary, change for your app!
    BUF_SIZE = 65536  # lets read stuff in 64kb chunks!

    path = Path(path)

    sha1 = hashlib.sha1()

    with path.open('rb') as f:
        while True:
            data = f.read(BUF_SIZE)
            if not data:
                break
            sha1.update(data)

    return f'{sha1.hexdigest()}'


# get pe data for url gen
# https://github.com/m417z/winbindex/blob/gh-pages/data/extract_data_from_pe_files.py
def get_pe_extra_data(path: Path):

    path = Path(path)

    with path.open('rb') as handle:
        # Get PE offset (@60, DWORD) from DOS header
        handle.seek(60, 0)
        offset = handle.read(4)
        offset = unpack('<I', offset)[0]

        handle.seek(offset + 4, 0)
        word = handle.read(2)
        machine = unpack('<H', word)[0]

        handle.seek(offset + 8, 0)
        dword = handle.read(4)
        timestamp = unpack('<I', dword)[0]

        handle.seek(offset + 0x50, 0)
        dword = handle.read(4)
        image_size = unpack('<I', dword)[0]

    return {
        'machine': machine,
        'timestamp': timestamp,
        'image_size': image_size,
    }


def get_microsoft_download_url(filename, timestamp, virtual_size):

    assert filename is not None
    assert timestamp is not None
    assert virtual_size is not None

    timestamp = format(timestamp, '08X')
    virtual_size = format(virtual_size, 'X')

    return f'https://msdl.microsoft.com/download/symbols/{filename}/{timestamp}{virtual_size}/{filename}'

```

`ghidriff/version_tracking_diff.py`:

```py
from collections import Counter
from time import time


from typing import List, Tuple, TYPE_CHECKING

from .ghidra_diff_engine import GhidraDiffEngine
from .implied_matches import correlate_implied_matches

if TYPE_CHECKING:
    import ghidra
    from ghidra_builtins import *


class VersionTrackingDiff(GhidraDiffEngine):
    """
    An Ghidra Diff implementation using several exact and some fuzzy correlators
    This differ is inspired by Ghidra's Version Tracking (albeit much faster)
    ghidra/tree/master/Ghidra/Features/VersionTracking
    """

    MIN_FUNC_LEN = 10

    def find_matches(
        self,
        p1: "ghidra.program.model.listing.Program",
        p2: "ghidra.program.model.listing.Program",
    ) -> list:
        """
        Find matching and unmatched functions between p1 and p2
        """

        from ghidra.program.model.symbol import SymbolType
        from ghidra.program.model.address import AddressSet
        from ghidra.util.task import ConsoleTaskMonitor

        # Match functions
        from ghidra.app.plugin.match import MatchFunctions, MatchSymbol

        # Correlators
        from ghidra.app.plugin.match import ExactMnemonicsFunctionHasher, ExactBytesFunctionHasher, ExactInstructionsFunctionHasher
        from .correlators import StructuralGraphExactHasher, StructuralGraphHasher, BulkInstructionsHasher, BulkMnemonicHasher, BulkBasicBlockMnemonicHasher, NamespaceNameParamHasher, NameParamHasher, NameParamRefHasher, SigCallingCalledHasher, StringsRefsHasher, SwitchSigHasher, StrUniqueFuncRefsHasher

        monitor = ConsoleTaskMonitor()

        # loadedAndInitializedAddressSet
        #   /**
        #  * Returns the set of addresses which correspond to all the "loaded" memory blocks that have
        #  * initialized data.  This does not include initialized memory blocks that contain data from
        #  * the program's file header such as debug sections.
        #  */
        p1_unmatched = AddressSet(p1.memory.loadedAndInitializedAddressSet)
        p2_unmatched = AddressSet(p2.memory.loadedAndInitializedAddressSet)

        # keep track of all function mathces
        p1_matches = AddressSet()
        p2_matches = AddressSet()

        # tuples of correlators instances
        # ( name, hasher, one_to_one, one_to_many)
        # DO NOT CHANGE ORDER UNLESS INTENDED, ORDER HAS MAJOR IMPACT ON ACCURACY AND EFFICIENCY
        func_correlators = [
            ('ExactBytesFunctionHasher', ExactBytesFunctionHasher.INSTANCE, True, False),
            ('ExactInstructionsFunctionHasher', ExactInstructionsFunctionHasher.INSTANCE, True, False),
            (StructuralGraphExactHasher.MATCH_TYPE, StructuralGraphExactHasher(), True, False),
            ('ExactMnemonicsFunctionHasher', ExactMnemonicsFunctionHasher.INSTANCE, True, False),
            (BulkInstructionsHasher.MATCH_TYPE, BulkInstructionsHasher(), True, False),
            (SigCallingCalledHasher.MATCH_TYPE, SigCallingCalledHasher(), True, False),
            (StringsRefsHasher.MATCH_TYPE, StringsRefsHasher(), True, False),
            (StrUniqueFuncRefsHasher.MATCH_TYPE, StrUniqueFuncRefsHasher(), True, False),
            (SwitchSigHasher.MATCH_TYPE, SwitchSigHasher(), True, False),
            # WARN: one_to_many=True flag allows for false negatives is structal graph matching. Mitgated by added references, func name in hash
            (StructuralGraphHasher.MATCH_TYPE, StructuralGraphHasher(), True, True),
            # WARN: one_to_many=True flag allows for false negatives
            (BulkBasicBlockMnemonicHasher.MATCH_TYPE, BulkBasicBlockMnemonicHasher(), True, True),

            (SigCallingCalledHasher.MATCH_TYPE, SigCallingCalledHasher(), True, False),
            (StringsRefsHasher.MATCH_TYPE, StringsRefsHasher(), True, False),
            (StrUniqueFuncRefsHasher.MATCH_TYPE, StrUniqueFuncRefsHasher(), True, False),
            (SwitchSigHasher.MATCH_TYPE, SwitchSigHasher(), True, False),
        ]

        unmatched = []
        matches = {}

        # Run Symbol Hash Correlator

        one_to_one = True
        include_externals = True
        min_sym_name_len = 3
        matchedSymbols = MatchSymbol.matchSymbol(
            p1, p1.getMemory(), p2, p2.getMemory(), min_sym_name_len, one_to_one, include_externals, monitor)

        start = time()
        name = 'SymbolsHash'
        skipped = 0
        for match in matchedSymbols:
            if match.matchType == SymbolType.FUNCTION:
                # sanity check symbolmatch
                func = p1.functionManager.getFunctionAt(match.aSymbolAddress)
                func2 = p2.functionManager.getFunctionAt(match.bSymbolAddress)
                if func.getName(True) != func2.getName(True):
                    skipped += 1
                    continue
                p1_matches.add(match.aSymbolAddress)
                p2_matches.add(match.bSymbolAddress)
                matches.setdefault((match.aSymbolAddress, match.bSymbolAddress), {}).setdefault(name, 0)
                matches[(match.aSymbolAddress, match.bSymbolAddress)][name] += 1
        end = time()

        p1_unmatched = p1_unmatched.subtract(p1_matches)
        p2_unmatched = p2_unmatched.subtract(p2_matches)

        self.logger.info(f'Exec time: {end-start:.4f} secs')
        self.logger.info(f'Match count {matchedSymbols.size() - skipped}')
        self.logger.info(Counter([tuple(x) for x in matches.values()]))

        # Run Function Hash Correlators

        for cor in func_correlators:

            start = time()

            name, hasher, one_to_one, one_to_many = cor

            self.logger.info(f'Running correlator: {name}')
            self.logger.debug(f'hasher: {hasher}')
            self.logger.info(f'name: {name} one_to_one: {one_to_one} one_to_many: {one_to_many}')

            func_matches = MatchFunctions.matchFunctions(
                p1, p1_unmatched, p2, p2_unmatched, self.MIN_FUNC_LEN, one_to_one, one_to_many, hasher, monitor)

            end = time()

            for match in func_matches:
                p1_matches.add(match.aFunctionAddress)
                p2_matches.add(match.bFunctionAddress)
                matches.setdefault((match.aFunctionAddress, match.bFunctionAddress), {}).setdefault(name, 0)
                matches[(match.aFunctionAddress, match.bFunctionAddress)][name] += 1

            p1_unmatched = p1_unmatched.subtract(p1_matches)
            p2_unmatched = p2_unmatched.subtract(p2_matches)

            self.logger.info(f'{name} Exec time: {end-start:.4f} secs')
            self.logger.info(f'Match count: {func_matches.size()}')

            # kill noisy monitor after first run
            monitor = ConsoleTaskMonitor().DUMMY_MONITOR

        # Log current counts
        self.logger.info(Counter([tuple(x) for x in matches.values()]))

        monitor = ConsoleTaskMonitor()

        # Find unmatched functions

        p1_missing = self.get_funcs_from_addr_set(p1, p1_unmatched)
        p2_missing = self.get_funcs_from_addr_set(p2, p2_unmatched)

        # Find implied matches
        correlate_implied_matches(matches,
                                  p1_missing,
                                  p2_missing,
                                  p1_matches,
                                  p2_matches,
                                  p1,
                                  p2,
                                  self.max_workers,
                                  monitor,
                                  self.logger)

        p1_unmatched = p1_unmatched.subtract(p1_matches)
        p2_unmatched = p2_unmatched.subtract(p2_matches)

        p1_missing = self.get_funcs_from_addr_set(p1, p1_unmatched)
        p2_missing = self.get_funcs_from_addr_set(p2, p2_unmatched)

        self.logger.info(f'p1 missing = {len(p1_missing)}')
        self.logger.info(f'p2 missing = {len(p2_missing)}')

        unmatched.extend([func.getSymbol() for func in p1_missing])
        unmatched.extend([func.getSymbol() for func in p2_missing])

        # Debug umatched
        # if self.logger.level == 10:  # debug level
        #     self.debug_function_hasher(unmatched, func_correlators)

        # Find external function unmatched and matched
        p1_externals = {}
        # get external funcs (these are still interesting)
        for func in p1.functionManager.getExternalFunctions():
            key = func.getName(True)
            p1_externals[key] = func

        p2_externals = {}
        # get external funcs (these are still interesting)
        for func in p2.functionManager.getExternalFunctions():
            key = func.getName(True)
            p2_externals[key] = func

        deleted_externs = list(set(p1_externals.keys()).difference(p2_externals.keys()))
        added_externs = list(set(p2_externals.keys()).difference(p1_externals.keys()))
        matched_externs = list(set(p2_externals.keys()).intersection(p1_externals.keys()))

        unmatched.extend([p1_externals[key].getSymbol() for key in deleted_externs])
        unmatched.extend([p2_externals[key].getSymbol() for key in added_externs])

        for key in matched_externs:
            name = 'ExternalsName'
            func = p1_externals[key]
            func2 = p2_externals[key]
            matches.setdefault((func.entryPoint, func2.entryPoint), {}).setdefault(name, 0)
            matches[(func.entryPoint, func2.entryPoint)][name] += 1

        # translate matches to expected format [ sym, sym2, match_type ]
        matched = []
        for match_addrs, match_types in matches.items():

            func = p1.functionManager.getFunctionContaining(match_addrs[0])
            assert func.entryPoint == match_addrs[0]
            func2 = p2.functionManager.getFunctionContaining(match_addrs[1])
            assert func2.entryPoint == match_addrs[1]

            matched.append([func.getSymbol(), func2.getSymbol(), list(match_types.keys())])

        # skip types will undergo less processing
        skip_types = ['BulkBasicBlockMnemonicHash', 'ExternalsName']

        return [unmatched, matched, skip_types]

    def debug_function_hasher(self, functions: list, func_correlators: list):
        """
        Debug unmatched functions throuh each function hasher
        """
        from ghidra.util.task import ConsoleTaskMonitor

        for sym in sorted(functions, key=lambda x: x.getProgram().getFunctionManager().getFunctionAt(x.address).body.numAddresses, reverse=True):
            func = sym.getProgram().getFunctionManager().getFunctionAt(sym.address)
            self.logger.debug(f'\n\n{func.getProgram()}')
            self.logger.debug(func)

            for cor in func_correlators:

                name, hasher, one_to_one, one_to_many = cor

                self.logger.debug(f'Debug umatched correlator: {name}')

                if hasattr(hasher, 'DEBUG'):
                    hasher.DEBUG = True

                dummy = ConsoleTaskMonitor().DUMMY_MONITOR
                dummy.cancel()
                self.logger.debug(hasher.hash(func, dummy))

```

`setup.cfg`:

```cfg
[metadata]
name = ghidriff
author = clearbluejar
author_email = clearbluejar@clearbluejar.com
version = attr:ghidriff.__version__
description = Ghidra Binary Diffing Engine
long_description_content_type = text/markdown
long_description = file:README.md
license = GPL-3.0 license
license_files =
    LICENSE    
url = https://github.com/clearbluejar/ghidriff
keywords = patchdiff, binaries, bindiff, ghidra, ghidriff
platform = any
classifiers =
    Development Status :: 3 - Alpha
    Intended Audience :: Developers
    License :: OSI Approved :: GNU General Public License v3 (GPLv3)
    Programming Language :: Python :: 3
    Programming Language :: Python :: 3.7
    Programming Language :: Python :: 3.8
    Programming Language :: Python :: 3.9
    Programming Language :: Python :: 3.10

[options]
python_requires = >= 3.7
packages = find:
zip_safe = False
include_package_data = True
install_requires =
    pyhidra
    mdutils

[options.entry_points]
console_scripts =
    ghidriff = ghidriff.__main__:main

[options.extras_require]
testing =
    pytest    
    requests
    pytest-datadir

[tool:pytest]
testpaths = tests
required_plugins =
    pytest-datadir

addopts =
    -p no:faulthandler

[pycodestyle]
max_line_length = 130
```

`setup.py`:

```py
#!/usr/bin/python
# -*- coding: utf-8 -*-
from setuptools import setup

setup()
```

`tests/test_diff.py`:

```py
from pathlib import Path
import json

from ghidriff import get_parser, get_engine_classes, VersionTrackingDiff, GhidraDiffEngine

SYMBOLS_DIR = 'symbols'
BINS_DIR = 'bins'

def get_chrome_headers() -> dict:

    headers =  {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-language": "en-US,en;q=0.9",
        "cache-control": "no-cache",
        "pragma": "no-cache",
        "sec-ch-ua": '"Chromium";v="116", "Not)A;Brand";v="24", "Google Chrome";v="116"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"macOS"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "none",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1"
    }

    return headers



def test_diff_afd_cve_2023_21768(shared_datadir: Path):
    """
    Tests end to end diff of CVE
    """
    
    test_name = 'cve-2023-21768'
    output_path = shared_datadir / test_name
    output_path.mkdir(exist_ok=True, parents=True)
    symbols_path = shared_datadir / SYMBOLS_DIR
    bins_path = shared_datadir / BINS_DIR

    
    # setup bins

    old_bin_path = bins_path / 'afd.sys.x64.10.0.22621.1028'
    new_bin_path = bins_path / 'afd.sys.x64.10.0.22621.1415'

    # TODO figure out why these download are unreliable
    # for now just git clone ghidriff-test-data
    # old_bin_path = shared_datadir / 'afd.sys.x64.10.0.22621.1028'
    # old_url = 'https://msdl.microsoft.com/download/symbols/afd.sys/0C5C6994A8000/afd.sys'
    # new_bin_path = shared_datadir / 'afd.sys.x64.10.0.22621.1415'
    # new_url = 'https://msdl.microsoft.com/download/symbols/afd.sys/50989142A9000/afd.sys'

    # download binaries    
    # download is unreliage
    # headers = get_chrome_headers()
    # old_bin_path.write_bytes(requests.get(old_url,headers=headers).content)
    # new_bin_path.write_bytes(requests.get(new_url,headers=headers).content)

    assert old_bin_path.exists()
    assert new_bin_path.exists()

    parser = get_parser()

    GhidraDiffEngine.add_ghidra_args_to_parser(parser)

    args = parser.parse_args(['-s', str(symbols_path), str(old_bin_path.absolute()),str(new_bin_path.absolute())])

    engine_log_path = output_path / parser.get_default('log_path')

    binary_paths = args.old + [bin for sublist in args.new for bin in sublist]

    binary_paths = [Path(path) for path in binary_paths]    

    if any([not path.exists() for path in binary_paths]):
        missing_bins = [f'{path.name}' for path in binary_paths if not path.exists()]
        raise FileNotFoundError(f"Missing Bins: {' '.join(missing_bins)}")

    project_name = f'{args.project_name}-{binary_paths[0].name}-{binary_paths[-1].name}'

    
    DiffEngine: GhidraDiffEngine = VersionTrackingDiff

    d: GhidraDiffEngine = DiffEngine(args=args,
                                     verbose=True,
                                     threaded=args.threaded,
                                     max_ram_percent=args.max_ram_percent,
                                     print_jvm_flags=args.print_flags,
                                     jvm_args=args.jvm_args,
                                     force_analysis=args.force_analysis,
                                     force_diff=args.force_diff,
                                     verbose_analysis=args.va,
                                     no_symbols=args.no_symbols,
                                     engine_log_path=engine_log_path,
                                     engine_log_level=args.log_level,
                                     engine_file_log_level=args.file_log_level,                                     
                                     )

    d.setup_project(binary_paths, args.project_location, project_name, args.symbols_path)

    d.analyze_project()

    pdiff = d.diff_bins(old_bin_path, new_bin_path)
    pdiff_json = json.dumps(pdiff)

    d.validate_diff_json(pdiff_json)

    diff_name = f"{old_bin_path.name}-{new_bin_path.name}_diff"

    d.dump_pdiff_to_path(diff_name,
                             pdiff,
                             output_path,
                             side_by_side=args.side_by_side,
                             max_section_funcs=args.max_section_funcs,
                             md_title=args.md_title)

    assert len(pdiff['functions']['modified']) == 10
    assert len(pdiff['functions']['added']) == 28
    assert len(pdiff['functions']['deleted']) == 0

    func_name = "AfdNotifyRemoveIoCompletion"
    assert any([func_name in func['old']['name'] or func_name in func['new']['name'] for func in pdiff['functions']['modified'] ]) is True
```

`tests/test_startup.py`:

```py
import pyhidra

def test_pyhidra_start():
    pyhidra.start(verbose=True)

def test_ghidra_install_dir():
    import os
    install_dir = os.getenv("GHIDRA_INSTALL_DIR")
    assert install_dir == "/ghidra"

#from ghidriff import GhidraDiffEngine
# from pytest import MonkeyPatch

# @pytest.fixture
# def setup_bogus_env(mon):
# from mock import patch

# @patch('pyhidra.GHIDRA_INSTALL_DIR', "/someboguspath"):
# def test_bogus_ghidra_install_dira():
#     with pytest.raises(FileNotFoundError):            
#         import pyhidra as err_pyhidra
#         err_pyhidra.start(verbose=True)


# def test_ghidra_install_dir():
#     # import sys
#     # sys.modules.pop('pyhidra')
#     with MonkeyPatch.context() as mp:
#         mp.delenv("GHIDRA_INSTALL_DIR")
        
#         import pyhidra    
#         # print(os.getenv("GHIDRA_INSTALL_DIR"))
#         with pytest.raises(SystemExit) as pytest_wrapped_e:
#             launcher = pyhidra.start(verbose=True)
#         assert pytest_wrapped_e.type == SystemExit
#         #assert pytest_wrapped_e.value.code == 42
#     import os
#     print(os.getenv("GHIDRA_INSTALL_DIR"))
#     # from importlib import reload
#     # reload(pyhidra)
#     import sys
#     del sys.modules['pyhidra']
        


# def test_bogus_ghidra_install_dir():
#     #monkeypatch.setenv("GHIDRA_INSTALL_DIR", '/someboguspath')        
#     import os
#     print(os.getenv("GHIDRA_INSTALL_DIR"))
#     with MonkeyPatch.context() as mp:        
#         with pytest.raises(FileNotFoundError):                                    
#             import pyhidra
#             #mp.setattr(pyhidra.constants,'GHIDRA_INSTALL_DIR', '/someboguspath')            
#             pyhidra.start(verbose=True)

        


    
    #     launcher = pyhidra.start(verbose=True)
    #     print(launcher.check_ghidra_version())

#det test_file_not_exist():
```