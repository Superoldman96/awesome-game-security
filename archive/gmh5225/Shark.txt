Project Path: arc_gmh5225_Shark_ea_7vs08

Source Tree:

```txt
arc_gmh5225_Shark_ea_7vs08
├── LICENSE
├── README.md
├── apps
│   ├── __init__.py
│   ├── language_models
│   │   ├── README.md
│   │   ├── langchain
│   │   │   ├── README.md
│   │   │   ├── cli.py
│   │   │   ├── create_data.py
│   │   │   ├── enums.py
│   │   │   ├── evaluate_params.py
│   │   │   ├── expanded_pipelines.py
│   │   │   ├── gen.py
│   │   │   ├── gpt4all_llm.py
│   │   │   ├── gpt_langchain.py
│   │   │   ├── gradio_utils
│   │   │   │   └── grclient.py
│   │   │   ├── h2oai_pipeline.py
│   │   │   ├── image_captions.py
│   │   │   ├── langchain_requirements.txt
│   │   │   ├── llama_flash_attn_monkey_patch.py
│   │   │   ├── loaders.py
│   │   │   ├── make_db.py
│   │   │   ├── prompter.py
│   │   │   ├── read_wiki_full.py
│   │   │   ├── stopping.py
│   │   │   ├── utils.py
│   │   │   └── utils_langchain.py
│   │   ├── scripts
│   │   │   ├── llama_ir_conversion_utils.py
│   │   │   ├── stablelm.py
│   │   │   └── vicuna.py
│   │   ├── shark_llama_cli.spec
│   │   ├── src
│   │   │   ├── model_wrappers
│   │   │   │   ├── falcon_model.py
│   │   │   │   ├── falcon_sharded_model.py
│   │   │   │   ├── minigpt4.py
│   │   │   │   ├── stablelm_model.py
│   │   │   │   ├── vicuna4.py
│   │   │   │   ├── vicuna_model.py
│   │   │   │   ├── vicuna_model_gpu.py
│   │   │   │   └── vicuna_sharded_model.py
│   │   │   └── pipelines
│   │   │       ├── SharkLLMBase.py
│   │   │       ├── falcon_pipeline.py
│   │   │       ├── minigpt4_pipeline.py
│   │   │       ├── minigpt4_utils
│   │   │       │   ├── Qformer.py
│   │   │       │   ├── blip_processors.py
│   │   │       │   ├── configs
│   │   │       │   │   ├── cc_sbu_align.yaml
│   │   │       │   │   ├── minigpt4.yaml
│   │   │       │   │   └── minigpt4_eval.yaml
│   │   │       │   ├── eva_vit.py
│   │   │       │   └── prompts
│   │   │       │       └── alignment.txt
│   │   │       └── stablelm_pipeline.py
│   │   └── utils.py
│   ├── shark_studio
│   │   ├── api
│   │   │   ├── llm.py
│   │   │   └── utils.py
│   │   └── web
│   │       ├── index.py
│   │       └── ui
│   │           ├── __init__.py
│   │           └── chat.py
│   └── stable_diffusion
│       ├── __init__.py
│       ├── profiling_with_iree.md
│       ├── scripts
│       │   ├── __init__.py
│       │   ├── img2img.py
│       │   ├── inpaint.py
│       │   ├── main.py
│       │   ├── outpaint.py
│       │   ├── telegram_bot.py
│       │   ├── train_lora_word.py
│       │   ├── tuner.py
│       │   ├── txt2img.py
│       │   ├── txt2img_sdxl.py
│       │   └── upscaler.py
│       ├── shark_sd.spec
│       ├── shark_sd_cli.spec
│       ├── shark_studio_imports.py
│       ├── src
│       │   ├── __init__.py
│       │   ├── models
│       │   │   ├── __init__.py
│       │   │   ├── model_wrappers.py
│       │   │   └── opt_params.py
│       │   ├── pipelines
│       │   │   ├── __init__.py
│       │   │   ├── pipeline_shark_stable_diffusion_img2img.py
│       │   │   ├── pipeline_shark_stable_diffusion_inpaint.py
│       │   │   ├── pipeline_shark_stable_diffusion_outpaint.py
│       │   │   ├── pipeline_shark_stable_diffusion_stencil.py
│       │   │   ├── pipeline_shark_stable_diffusion_txt2img.py
│       │   │   ├── pipeline_shark_stable_diffusion_txt2img_sdxl.py
│       │   │   ├── pipeline_shark_stable_diffusion_upscaler.py
│       │   │   └── pipeline_shark_stable_diffusion_utils.py
│       │   ├── schedulers
│       │   │   ├── __init__.py
│       │   │   ├── sd_schedulers.py
│       │   │   └── shark_eulerdiscrete.py
│       │   └── utils
│       │       ├── __init__.py
│       │       ├── civitai.py
│       │       ├── profiler.py
│       │       ├── resamplers.py
│       │       ├── resources
│       │       │   ├── base_model.json
│       │       │   ├── model_config.json
│       │       │   ├── model_db.json
│       │       │   ├── opt_flags.json
│       │       │   └── prompts.json
│       │       ├── resources.py
│       │       ├── sd_annotation.py
│       │       ├── stable_args.py
│       │       ├── stencils
│       │       │   ├── __init__.py
│       │       │   ├── canny
│       │       │   │   └── __init__.py
│       │       │   ├── openpose
│       │       │   │   ├── __init__.py
│       │       │   │   ├── body.py
│       │       │   │   ├── hand.py
│       │       │   │   └── openpose_util.py
│       │       │   ├── stencil_utils.py
│       │       │   └── zoe
│       │       │       └── __init__.py
│       │       └── utils.py
│       ├── stable_diffusion_telegram_bot.md
│       ├── studio_bundle.spec
│       └── web
│           ├── api
│           │   ├── __init__.py
│           │   ├── sdapi_v1.py
│           │   └── utils.py
│           ├── index.py
│           ├── ui
│           │   ├── __init__.py
│           │   ├── common_ui_events.py
│           │   ├── css
│           │   │   └── sd_dark_theme.css
│           │   ├── generate_config.py
│           │   ├── h2ogpt.py
│           │   ├── img2img_ui.py
│           │   ├── inpaint_ui.py
│           │   ├── logos
│           │   │   ├── nod-icon.png
│           │   │   └── nod-logo.png
│           │   ├── lora_train_ui.py
│           │   ├── minigpt4_ui.py
│           │   ├── model_manager.py
│           │   ├── outpaint_ui.py
│           │   ├── outputgallery_ui.py
│           │   ├── stablelm_ui.py
│           │   ├── txt2img_sdxl_ui.py
│           │   ├── txt2img_ui.py
│           │   ├── upscaler_ui.py
│           │   └── utils.py
│           └── utils
│               ├── app.py
│               ├── common_label_calc.py
│               ├── global_obj.py
│               ├── metadata
│               │   ├── __init__.py
│               │   ├── csv_metadata.py
│               │   ├── display.py
│               │   ├── exif_metadata.py
│               │   ├── format.py
│               │   └── png_metadata.py
│               └── tmp_configs.py
├── benchmarks
│   ├── __init__.py
│   ├── hf_model_benchmark.py
│   ├── hf_transformer.py
│   └── tests
│       ├── test_benchmark.py
│       └── test_hf_benchmark.py
├── build_tools
│   ├── docker
│   │   ├── Dockerfile-ubuntu-22.04
│   │   └── README.md
│   ├── image_comparison.py
│   ├── populate_sharktank_ci.sh
│   ├── scrape_releases.py
│   ├── stable_diffusion_testing.py
│   └── vicuna_testing.py
├── conftest.py
├── cpp
│   ├── CMakeLists.txt
│   ├── README.md
│   ├── dog_imagenet.jpg
│   ├── save_img.py
│   ├── vision_inference
│   │   ├── CMakeLists.txt
│   │   ├── README.md
│   │   ├── image_util.c
│   │   ├── image_util.h
│   │   ├── iree-run-mnist-module.c
│   │   └── mnist_test.png
│   └── vulkan_gui
│       ├── CMakeLists.txt
│       ├── snail_imagenet.jpg
│       ├── stb_image.h
│       ├── vulkan_inference_gui.cc
│       └── vulkan_resnet_inference_gui.cc
├── dataset
│   ├── README.md
│   ├── annotation_tool.py
│   ├── args.py
│   ├── requirements.txt
│   └── utils.py
├── docs
│   ├── shark_iree_profiling.md
│   ├── shark_sd_blender.md
│   └── shark_sd_koboldcpp.md
├── process_skipfiles.py
├── pyproject.toml
├── pytest.ini
├── requirements-importer-macos.txt
├── requirements-importer.txt
├── requirements.txt
├── rest_api_tests
│   ├── api_test.py
│   └── dog.png
├── setup.py
├── setup_venv.ps1
├── setup_venv.sh
├── shark
│   ├── __init__.py
│   ├── backward_makefx.py
│   ├── dynamo_backend
│   │   ├── __init__.py
│   │   └── utils.py
│   ├── examples
│   │   ├── shark_dynamo
│   │   │   └── basic_examples.py
│   │   ├── shark_eager
│   │   │   ├── dynamo_demo.ipynb
│   │   │   ├── dynamo_demo.py
│   │   │   ├── eager_mode.ipynb
│   │   │   ├── eager_mode.py
│   │   │   └── squeezenet_lockstep.py
│   │   ├── shark_inference
│   │   │   ├── CLIPModel_tf.py
│   │   │   ├── ESRGAN
│   │   │   │   ├── README.md
│   │   │   │   └── esrgan.py
│   │   │   ├── albert_maskfill_pt.py
│   │   │   ├── albert_maskfill_tf.py
│   │   │   ├── bloom_tank.py
│   │   │   ├── gpt2_tf.py
│   │   │   ├── llama
│   │   │   │   └── README.md
│   │   │   ├── mega_test.py
│   │   │   ├── mhlo_example.py
│   │   │   ├── minilm_benchmark.py
│   │   │   ├── minilm_benchmark_tf.py
│   │   │   ├── minilm_jax.py
│   │   │   ├── minilm_jax_requirements.txt
│   │   │   ├── minilm_jit.py
│   │   │   ├── minilm_tf.py
│   │   │   ├── minilm_tf_gpu_config.json
│   │   │   ├── resnest.py
│   │   │   ├── resnet50_fp16.py
│   │   │   ├── resnet50_script.py
│   │   │   ├── sharded_bloom.py
│   │   │   ├── sharded_bloom_large_models.py
│   │   │   ├── simple_dlrm.py
│   │   │   ├── sparse_arch.py
│   │   │   ├── t5_tf.py
│   │   │   ├── torch_vision_models_script.py
│   │   │   ├── unet_script.py
│   │   │   ├── upscaler
│   │   │   │   ├── main.py
│   │   │   │   ├── model_wrappers.py
│   │   │   │   ├── opt_params.py
│   │   │   │   ├── pipeline_shark_stable_diffusion_upscale.py
│   │   │   │   ├── upscaler_args.py
│   │   │   │   └── utils.py
│   │   │   └── v_diffusion.py
│   │   └── shark_training
│   │       ├── bert_training.py
│   │       ├── bert_training_load_tf.py
│   │       ├── bert_training_tf.py
│   │       ├── neural_net_training.py
│   │       ├── stable-diffusion-img2img
│   │       │   ├── README.md
│   │       │   ├── setup.sh
│   │       │   └── stable_diffusion_img2img.py
│   │       └── stable_diffusion
│   │           ├── README.md
│   │           └── stable_diffusion_fine_tuning.py
│   ├── iree_eager_backend.py
│   ├── iree_utils
│   │   ├── __init__.py
│   │   ├── _common.py
│   │   ├── benchmark_utils.py
│   │   ├── compile_utils.py
│   │   ├── cpu_utils.py
│   │   ├── gpu_utils.py
│   │   ├── metal_utils.py
│   │   ├── trace.py
│   │   ├── vulkan_target_env_utils.py
│   │   └── vulkan_utils.py
│   ├── model_annotation.py
│   ├── parser.py
│   ├── shark_benchmark_runner.py
│   ├── shark_compile.py
│   ├── shark_downloader.py
│   ├── shark_eager
│   │   └── shark_eager.py
│   ├── shark_generate_model_config.py
│   ├── shark_importer.py
│   ├── shark_inference.py
│   ├── shark_runner.py
│   ├── shark_trainer.py
│   ├── stress_test.py
│   ├── tests
│   │   ├── test_shark_importer.py
│   │   └── test_stress_test.py
│   ├── tflite_utils.py
│   ├── torch_mlir_lockstep_tensor.py
│   └── torch_mlir_utils.py
├── tank
│   ├── README.md
│   ├── __init__.py
│   ├── examples
│   │   ├── MiniLM_tf
│   │   │   ├── huggingface_MiniLM_gen.py
│   │   │   ├── huggingface_MiniLM_run.py
│   │   │   ├── huggingface_MiniLM_tf.py
│   │   │   └── seq_classification.py
│   │   ├── bert-base-uncased_tosa_torch
│   │   │   └── bert_base_uncased_tosa.py
│   │   ├── bert_fine_tuning
│   │   │   └── bert_fine_tune_tf.py
│   │   ├── bert_tf
│   │   │   ├── bert_large_gen.py
│   │   │   ├── bert_large_run.py
│   │   │   ├── bert_large_tf.py
│   │   │   ├── bert_small_gen.py
│   │   │   ├── bert_small_run.py
│   │   │   ├── bert_small_tf_run.py
│   │   │   └── seq_classification.py
│   │   ├── bloom
│   │   │   ├── README.md
│   │   │   └── bloom_model.py
│   │   ├── deberta-base_tf
│   │   │   └── deberta-base_tf_test.py
│   │   ├── gpt2-64
│   │   │   └── gpt2-64_tflite_test.py
│   │   ├── opt
│   │   │   ├── README.md
│   │   │   ├── opt_causallm.py
│   │   │   ├── opt_causallm_samples.py
│   │   │   ├── opt_causallm_torch_test.py
│   │   │   ├── opt_perf_comparison.py
│   │   │   ├── opt_perf_comparison_batch.py
│   │   │   ├── opt_torch_test.py
│   │   │   ├── opt_util.py
│   │   │   ├── shark_hf_base_opt.py
│   │   │   ├── shark_opt_wrapper.py
│   │   │   └── shark_opt_wrapper_train.py
│   │   ├── rembert_tf
│   │   │   └── rembert_tf_test.py
│   │   ├── tapas-base_tf
│   │   │   └── tapas-base_tf_test.py
│   │   └── v_diffusion_pytorch
│   │       ├── README.md
│   │       ├── cc12m_1.py
│   │       ├── cfg_sample.py
│   │       ├── cfg_sample_eager.py
│   │       ├── cfg_sample_from_mlir.py
│   │       ├── cfg_sample_preprocess.py
│   │       └── setup_v_diffusion_pytorch.sh
│   ├── generate_sharktank.py
│   ├── model_utils.py
│   ├── model_utils_tf.py
│   ├── test_models.py
│   └── tflite
│       ├── README.md
│       ├── albert.py
│       ├── albert_lite_base
│       │   ├── albert_lite_base_tflite_sharkimporter.txt
│       │   └── albert_lite_base_tflite_test.py
│       ├── arbitrary-image-stylization-v1-256
│       │   └── arbitrary-image-stylization-v1-256_tflite_test.py
│       ├── asr_conformer_test.py
│       ├── bird_classifier_test.py
│       ├── birds_V1
│       │   └── birds_V1_tflite_test.py
│       ├── cartoon_gan_test.py
│       ├── cartoongan
│       │   └── cartoongan_tflite_test.py
│       ├── coco_data.py
│       ├── coco_test_data.py
│       ├── craft_text_test.py
│       ├── deeplab_v3_test.py
│       ├── deeplabv3
│       │   └── deeplabv3_tflite_test.py
│       ├── densenet
│       │   └── densenet_tflite_test.py
│       ├── densenet_test.py
│       ├── east_text_detector_test.py
│       ├── efficientnet_224_fp32
│       │   └── efficientnet_224_fp32_tflite_test.py
│       ├── efficientnet_lite0_fp32_2
│       │   └── efficientnet_lite0_fp32_2_tflite_test.py
│       ├── efficientnet_lite0_int8_2
│       │   └── efficientnet_lite0_int8_2_tflite_test.py
│       ├── efficientnet_lite0_int8_test.py
│       ├── efficientnet_lite0_test.py
│       ├── efficientnet_test.py
│       ├── gpt2_test.py
│       ├── image_stylization_test.py
│       ├── imagenet_data.py
│       ├── imagenet_test_data.py
│       ├── inception_v4_299_fp32
│       │   └── inception_v4_299_fp32_tflite_test.py
│       ├── inception_v4_299_uint8
│       │   └── inception_v4_299_uint8_tflite_test.py
│       ├── inception_v4_test.py
│       ├── inception_v4_uint8_test.py
│       ├── keras_ocr_test.py
│       ├── lightning_fp16_test.py
│       ├── lightning_i8_test.py
│       ├── lightning_test.py
│       ├── lit.cfg.py
│       ├── magenta_test.py
│       ├── manual_test.py
│       ├── midas
│       │   └── midas_tflite_test.py
│       ├── midas_test.py
│       ├── mirnet_test.py
│       ├── mnasnet_1.0_224
│       │   └── mnasnet_tflite_test.py
│       ├── mnasnet_test.py
│       ├── mobilebert
│       │   └── mobilebert_tflite_test.py
│       ├── mobilebert-baseline-tf2-float
│       │   └── mobilebert-baseline-tf2-float_tflite_test.py
│       ├── mobilebert-baseline-tf2-quant
│       │   └── mobilebert-baseline-tf2-quant_tflite_test.py
│       ├── mobilebert-edgetpu-s-float
│       │   └── mobilebert-edgetpu-s-float_tflite_test.py
│       ├── mobilebert-edgetpu-s-quant
│       │   └── mobilebert-edgetpu-s-quant_tflite_test.py
│       ├── mobilebert_edgetpu_s_float_test.py
│       ├── mobilebert_edgetpu_s_quant_test.py
│       ├── mobilebert_test.py
│       ├── mobilebert_tf2_float_test.py
│       ├── mobilebert_tf2_quant_test.py
│       ├── mobilenet_ssd_quant_test.py
│       ├── mobilenet_v1_224_1.0_float
│       │   └── mobilenet_v1_float_tflite_test.py
│       ├── mobilenet_v1_224_1.0_uint8
│       │   └── mobilenet_v1_uint8_tflite_test.py
│       ├── mobilenet_v1_test.py
│       ├── mobilenet_v1_uint8_test.py
│       ├── mobilenet_v2_1.00_224_int8
│       │   └── mobilenet_v2_int8_tflite_test.py
│       ├── mobilenet_v2_1.0_224
│       │   └── mobilenet_v2_tflite_test.py
│       ├── mobilenet_v2_224_1.0_uint8
│       │   └── mobilenet_v2_uint8_tflite_test.py
│       ├── mobilenet_v2_int8_test.py
│       ├── mobilenet_v2_test.py
│       ├── mobilenet_v2_uint8_test.py
│       ├── mobilenet_v3-large_224_1.0_float
│       │   └── mobilenet_v3_float_tflite_test.py
│       ├── mobilenet_v3-large_224_1.0_uint8
│       │   └── mobilenet_v3_tflite_test.py
│       ├── mobilenet_v3-large_test.py
│       ├── mobilenet_v3-large_uint8_test.py
│       ├── mobilenet_v3.5multiavg_1.00_224_int8
│       │   └── mobilenet_v35_int8_tflite_test.py
│       ├── mobilenet_v35_int8_test.py
│       ├── multi_person_mobilenet_v1_075_float
│       │   └── multi_person_mobilenet_tflite_test.py
│       ├── nasnet
│       │   └── nasnet_tflite_test.py
│       ├── nasnet_test.py
│       ├── person_detect_test.py
│       ├── posenet_test.py
│       ├── resnet_50_224_int8
│       │   └── resnet_50_224_int8_tflite_test.py
│       ├── resnet_50_int8_test.py
│       ├── rosetta_test.py
│       ├── spice_test.py
│       ├── squad_data.py
│       ├── squad_test_data.py
│       ├── squeezenet
│       │   └── squeezenet_tflite_test.py
│       ├── squeezenet_test.py
│       ├── ssd_mobilenet_v1_320_1.0_float
│       │   └── ssd_mobilenet_v1_float_tflite_test.py
│       ├── ssd_mobilenet_v1_320_1.0_uint8
│       │   └── ssd_mobilenet_v1_uint8_tflite_test.py
│       ├── ssd_mobilenet_v1_test.py
│       ├── ssd_mobilenet_v1_uint8_test.py
│       ├── ssd_mobilenet_v2_face_quant
│       │   └── ssd_mobilenet_v2_face_quant_tflite_test.py
│       ├── ssd_mobilenet_v2_fpnlite_test.py
│       ├── ssd_mobilenet_v2_fpnlite_uint8_test.py
│       ├── ssd_mobilenet_v2_int8_test.py
│       ├── ssd_mobilenet_v2_test.py
│       ├── ssd_spaghettinet_edgetpu_large
│       │   └── ssd_spaghettinet_edgetpu_large_tflite_test.py
│       ├── ssd_spaghettinet_edgetpu_large_uint8
│       │   └── ssd_spaghettinet_edgetpu_large_uint8_tflite_test.py
│       ├── ssd_spaghettinet_large_test.py
│       ├── ssd_spaghettinet_large_uint8_test.py
│       ├── test_util.py
│       └── visual_wake_words_i8_test.py
└── tank_version.json

```

`LICENSE`:

```
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

    1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

    2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

    3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

    4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

    5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

    6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

    7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

    8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

    9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

    END OF TERMS AND CONDITIONS

    APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

    Copyright [yyyy] [name of copyright owner]

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.


---- LLVM Exceptions to the Apache 2.0 License ----

As an exception, if, as a result of your compiling your source code, portions
of this Software are embedded into an Object form of such source code, you
may redistribute such embedded portions in such Object form without complying
with the conditions of Sections 4(a), 4(b) and 4(d) of the License.

In addition, if you combine or link compiled forms of this Software with
software that is licensed under the GPLv2 ("Combined Software") and if a
court of competent jurisdiction determines that the patent provision (Section
3), the indemnity provision (Section 9) or other Section of the License
conflicts with the conditions of the GPLv2, you may retroactively and
prospectively choose to deem waived or otherwise exclude such Section(s) of
the License, but only in their entirety and only with respect to the Combined
Software.

```

`README.md`:

```md
# SHARK

High Performance Machine Learning Distribution

[![Nightly Release](https://github.com/nod-ai/SHARK/actions/workflows/nightly.yml/badge.svg)](https://github.com/nod-ai/SHARK/actions/workflows/nightly.yml)
[![Validate torch-models on Shark Runtime](https://github.com/nod-ai/SHARK/actions/workflows/test-models.yml/badge.svg)](https://github.com/nod-ai/SHARK/actions/workflows/test-models.yml)


<details>
  <summary>Prerequisites - Drivers </summary>
  
#### Install your Windows hardware drivers
* [AMD RDNA Users] Download the latest driver (23.2.1 is the oldest supported) [here](https://www.amd.com/en/support).
* [macOS Users] Download and install the 1.3.216 Vulkan SDK from [here](https://sdk.lunarg.com/sdk/download/1.3.216.0/mac/vulkansdk-macos-1.3.216.0.dmg). Newer versions of the SDK will not work. 
* [Nvidia Users] Download and install the latest CUDA / Vulkan drivers from [here](https://developer.nvidia.com/cuda-downloads)
  
#### Linux Drivers
* MESA / RADV drivers wont work with FP16. Please use the latest AMGPU-PRO drivers (non-pro OSS drivers also wont work) or the latest NVidia Linux Drivers.

Other users please ensure you have your latest vendor drivers and Vulkan SDK from [here](https://vulkan.lunarg.com/sdk/home) and if you are using vulkan check `vulkaninfo` works in a terminal window

</details>


 
### Quick Start for SHARK Stable Diffusion for Windows 10/11 Users

Install the Driver from [Prerequisites](https://github.com/nod-ai/SHARK#install-your-hardware-drivers) above 

Download the [stable release](https://github.com/nod-ai/shark/releases/latest)

Double click the .exe and you should have the [UI](http://localhost:8080/) in the browser. 

If you have custom models put them in a `models/` directory where the .exe is. 

Enjoy. 

<details>
  <summary>More installation notes</summary>
* We recommend that you download EXE in a new folder, whenever you download a new EXE version. If you download it in the same folder as a previous install, you must delete the old `*.vmfb` files with `rm *.vmfb`. You can also use `--clear_all` flag once to clean all the old files. 
* If you recently updated the driver or this binary (EXE file), we recommend you clear all the local artifacts with `--clear_all` 

## Running

* Open a Command Prompt or Powershell terminal, change folder (`cd`) to the .exe folder. Then run the EXE from the command prompt. That way, if an error occurs, you'll be able to cut-and-paste it to ask for help. (if it always works for you without error, you may simply double-click the EXE)
* The first run may take few minutes when the models are downloaded and compiled. Your patience is appreciated. The download could be about 5GB.
* You will likely see a Windows Defender message asking you to give permission to open a web server port. Accept it.
* Open a browser to access the Stable Diffusion web server. By default, the port is 8080, so you can go to http://localhost:8080/.

## Stopping

* Select the command prompt that's running the EXE. Press CTRL-C and wait a moment or close the terminal. 
</details>

<details>
  <summary>Advanced Installation (Only for developers)</summary>
  
## Advanced Installation (Windows, Linux and macOS) for developers

## Check out the code

```shell
git clone https://github.com/nod-ai/SHARK.git
cd SHARK
```

## Setup your Python VirtualEnvironment and Dependencies

### Windows 10/11 Users

* Install the latest Python 3.11.x version from [here](https://www.python.org/downloads/windows/)

* Install Git for Windows from [here](https://git-scm.com/download/win)

#### Allow the install script to run in Powershell
```powershell
set-executionpolicy remotesigned
```

#### Setup venv and install necessary packages (torch-mlir, nodLabs/Shark, ...)
```powershell
./setup_venv.ps1 #You can re-run this script to get the latest version
```

### Linux / macOS Users

```shell
./setup_venv.sh
source shark.venv/bin/activate
```


### Run Stable Diffusion on your device - WebUI

#### Windows 10/11 Users
```powershell
(shark.venv) PS C:\g\shark> cd .\apps\stable_diffusion\web\
(shark.venv) PS C:\g\shark\apps\stable_diffusion\web> python .\index.py
```
#### Linux / macOS Users
```shell
(shark.venv) > cd apps/stable_diffusion/web
(shark.venv) > python index.py
```

#### Access Stable Diffusion on http://localhost:8080/?__theme=dark


<img width="1607" alt="webui" src="https://user-images.githubusercontent.com/74956/204939260-b8308bc2-8dc4-47f6-9ac0-f60b66edab99.png">



### Run Stable Diffusion on your device - Commandline

#### Windows 10/11 Users
```powershell
(shark.venv) PS C:\g\shark> python .\apps\stable_diffusion\scripts\main.py --app="txt2img" --precision="fp16" --prompt="tajmahal, snow, sunflowers, oil on canvas" --device="vulkan"
```

#### Linux / macOS Users
```shell
python3.11 apps/stable_diffusion/scripts/main.py --app=txt2img --precision=fp16 --device=vulkan --prompt="tajmahal, oil on canvas, sunflowers, 4k, uhd"
```

You can replace `vulkan` with `cpu` to run on your CPU or with `cuda` to run on CUDA devices. If you have multiple vulkan devices you can address them with `--device=vulkan://1` etc
</details>

The output on a AMD 7900XTX would look something like:

```shell
Average step time: 47.19188690185547ms/it
Clip Inference time (ms) = 109.531
VAE Inference time (ms): 78.590

Total image generation time: 2.5788655281066895sec
```

Here are some samples generated:

![tajmahal, snow, sunflowers, oil on canvas_0](https://user-images.githubusercontent.com/74956/204934186-141f7e43-6eb2-4e89-a99c-4704d20444b3.jpg)

![a photo of a crab playing a trumpet](https://user-images.githubusercontent.com/74956/204933258-252e7240-8548-45f7-8253-97647d38313d.jpg)


Find us on [SHARK Discord server](https://discord.gg/RUqY2h2s9u) if you have any trouble with running it on your hardware. 


<details>
  <summary>Binary Installation</summary>

### Setup a new pip Virtual Environment

This step sets up a new VirtualEnv for Python

```shell
python --version #Check you have 3.11 on Linux, macOS or Windows Powershell
python -m venv shark_venv
source shark_venv/bin/activate   # Use shark_venv/Scripts/activate on Windows

# If you are using conda create and activate a new conda env

# Some older pip installs may not be able to handle the recent PyTorch deps
python -m pip install --upgrade pip
```

*macOS Metal* users please install https://sdk.lunarg.com/sdk/download/latest/mac/vulkan-sdk.dmg and enable "System wide install"

### Install SHARK

This step pip installs SHARK and related packages on Linux Python 3.8, 3.10 and 3.11 and macOS / Windows Python 3.11

```shell
pip install nodai-shark -f https://nod-ai.github.io/SHARK/package-index/ -f https://llvm.github.io/torch-mlir/package-index/ -f  https://nod-ai.github.io/SRT/pip-release-links.html --extra-index-url https://download.pytorch.org/whl/nightly/cpu
```

### Run shark tank model tests.
```shell
pytest tank/test_models.py
```
See tank/README.md for a more detailed walkthrough of our pytest suite and CLI.

### Download and run Resnet50 sample

```shell
curl -O https://raw.githubusercontent.com/nod-ai/SHARK/main/shark/examples/shark_inference/resnet50_script.py
#Install deps for test script
pip install --pre torch torchvision torchaudio tqdm pillow gsutil --extra-index-url https://download.pytorch.org/whl/nightly/cpu
python ./resnet50_script.py --device="cpu"  #use cuda or vulkan or metal
```

### Download and run BERT (MiniLM) sample
```shell
curl -O https://raw.githubusercontent.com/nod-ai/SHARK/main/shark/examples/shark_inference/minilm_jit.py
#Install deps for test script
pip install transformers torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu
python ./minilm_jit.py --device="cpu"  #use cuda or vulkan or metal
```
</details>



<details>
  <summary>Development, Testing and Benchmarks</summary>

If you want to use Python3.11 and with TF Import tools you can use the environment variables like:
Set `USE_IREE=1` to use upstream IREE
```
# PYTHON=python3.11 VENV_DIR=0617_venv IMPORTER=1 ./setup_venv.sh 
```

### Run any of the hundreds of SHARK tank models via the test framework
```shell
python -m  shark.examples.shark_inference.resnet50_script --device="cpu" # Use gpu | vulkan
# Or a pytest
pytest tank/test_models.py -k "MiniLM"
```
  
### How to use your locally built IREE / Torch-MLIR with SHARK
If you are a *Torch-mlir developer or an IREE developer* and want to test local changes you can uninstall
the provided packages with `pip uninstall torch-mlir` and / or `pip uninstall iree-compiler iree-runtime` and build locally
with Python bindings and set your PYTHONPATH as mentioned [here](https://github.com/iree-org/iree/tree/main/docs/api_docs/python#install-iree-binaries)
for IREE and [here](https://github.com/llvm/torch-mlir/blob/main/development.md#setup-python-environment-to-export-the-built-python-packages)
for Torch-MLIR.

How to use your locally built Torch-MLIR with SHARK:
```shell
1.) Run `./setup_venv.sh in SHARK` and activate `shark.venv` virtual env.
2.) Run `pip uninstall torch-mlir`.
3.) Go to your local Torch-MLIR directory.
4.) Activate mlir_venv virtual envirnoment.
5.) Run `pip uninstall -r requirements.txt`.
6.) Run `pip install -r requirements.txt`.
7.) Build Torch-MLIR.
8.) Activate shark.venv virtual environment from the Torch-MLIR directory.
8.) Run `export PYTHONPATH=`pwd`/build/tools/torch-mlir/python_packages/torch_mlir:`pwd`/examples` in the Torch-MLIR directory.
9.) Go to the SHARK directory.
```
Now the SHARK will use your locally build Torch-MLIR repo.


## Benchmarking Dispatches

To produce benchmarks of individual dispatches, you can add `--dispatch_benchmarks=All --dispatch_benchmarks_dir=<output_dir>` to your pytest command line argument.  
If you only want to compile specific dispatches, you can specify them with a space seperated string instead of `"All"`.  E.G. `--dispatch_benchmarks="0 1 2 10"`

For example, to generate and run dispatch benchmarks for MiniLM on CUDA:
```
pytest -k "MiniLM and torch and static and cuda" --benchmark_dispatches=All -s --dispatch_benchmarks_dir=./my_dispatch_benchmarks                                                                                
```
The given command will populate `<dispatch_benchmarks_dir>/<model_name>/` with an `ordered_dispatches.txt` that lists and orders the dispatches and their latencies, as well as folders for each dispatch that contain .mlir, .vmfb, and results of the benchmark for that dispatch.

if you want to instead incorporate this into a python script, you can pass the `dispatch_benchmarks` and `dispatch_benchmarks_dir` commands when initializing `SharkInference`, and the benchmarks will be generated when compiled.  E.G:

```
shark_module = SharkInference(
        mlir_model,
        device=args.device,
        mlir_dialect="tm_tensor",
        dispatch_benchmarks="all",
        dispatch_benchmarks_dir="results"
    )
```

Output will include:
- An ordered list ordered-dispatches.txt of all the dispatches with their runtime
- Inside the specified directory, there will be a directory for each dispatch (there will be mlir files for all dispatches, but only compiled binaries and benchmark data for the specified dispatches)
- An .mlir file containing the dispatch benchmark 
- A compiled .vmfb file containing the dispatch benchmark
- An .mlir file containing just the hal executable
- A compiled .vmfb file of the hal executable
- A .txt file containing benchmark output


See tank/README.md for further instructions on how to run model tests and benchmarks from the SHARK tank.

</details>

<details>
  <summary>API Reference</summary>

### Shark Inference API

```

from shark.shark_importer import SharkImporter

# SharkImporter imports mlir file from the torch, tensorflow or tf-lite module.

mlir_importer = SharkImporter(
    torch_module,
    (input),
    frontend="torch",  #tf, #tf-lite
)
torch_mlir, func_name = mlir_importer.import_mlir(tracing_required=True)

# SharkInference accepts mlir in linalg, mhlo, and tosa dialect.

from shark.shark_inference import SharkInference
shark_module = SharkInference(torch_mlir, device="cpu", mlir_dialect="linalg")
shark_module.compile()
result = shark_module.forward((input))

```


### Example demonstrating running MHLO IR.

```
from shark.shark_inference import SharkInference
import numpy as np

mhlo_ir = r"""builtin.module  {
      func.func @forward(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {
        %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<1x4xf32>, tensor<4x1xf32>) -> tensor<4x4xf32>
        %1 = "mhlo.abs"(%0) : (tensor<4x4xf32>) -> tensor<4x4xf32>
        return %1 : tensor<4x4xf32>
      }
}"""

arg0 = np.ones((1, 4)).astype(np.float32)
arg1 = np.ones((4, 1)).astype(np.float32)
shark_module = SharkInference(mhlo_ir, device="cpu", mlir_dialect="mhlo")
shark_module.compile()
result = shark_module.forward((arg0, arg1))
```
</details>

## Examples Using the REST API

* [Setting up SHARK for use with Blender](./docs/shark_sd_blender.md)
* [Setting up SHARK for use with Koboldcpp](./docs/shark_sd_koboldcpp.md)

## Supported and Validated Models

SHARK is maintained to support the latest innovations in ML Models: 

| TF HuggingFace Models | SHARK-CPU | SHARK-CUDA | SHARK-METAL |
|---------------------|----------|----------|-------------|
| BERT                | :green_heart:         | :green_heart:         | :green_heart:            |
| DistilBERT         | :green_heart:         | :green_heart:         | :green_heart:            |
| GPT2         | :green_heart:         | :green_heart:         | :green_heart:            |
| BLOOM         | :green_heart:         | :green_heart:         | :green_heart:            |
| Stable Diffusion         | :green_heart:         | :green_heart:         | :green_heart:            |
| Vision Transformer       | :green_heart:         | :green_heart:         | :green_heart:            |
| ResNet50         | :green_heart:         | :green_heart:         | :green_heart:            |

For a complete list of the models supported in SHARK, please refer to [tank/README.md](https://github.com/nod-ai/SHARK/blob/main/tank/README.md).

## Communication Channels

*   [SHARK Discord server](https://discord.gg/RUqY2h2s9u): Real time discussions with the SHARK team and other users
*   [GitHub issues](https://github.com/nod-ai/SHARK/issues): Feature requests, bugs etc

## Related Projects

<details>
  <summary>IREE Project Channels</summary>

*   [Upstream IREE issues](https://github.com/google/iree/issues): Feature requests,
    bugs, and other work tracking
*   [Upstream IREE Discord server](https://discord.gg/26P4xW4): Daily development
    discussions with the core team and collaborators
*   [iree-discuss email list](https://groups.google.com/forum/#!forum/iree-discuss):
    Announcements, general and low-priority discussion
</details>

<details>
  <summary>MLIR and Torch-MLIR Project Channels</summary>

* `#torch-mlir` channel on the LLVM [Discord](https://discord.gg/xS7Z362) - this is the most active communication channel
* Torch-MLIR Github issues [here](https://github.com/llvm/torch-mlir/issues)
* [`torch-mlir` section](https://llvm.discourse.group/c/projects-that-want-to-become-official-llvm-projects/torch-mlir/41) of LLVM Discourse
*  Weekly meetings on Mondays 9AM PST. See [here](https://discourse.llvm.org/t/community-meeting-developer-hour-refactoring-recurring-meetings/62575) for more information.
* [MLIR topic within LLVM Discourse](https://llvm.discourse.group/c/llvm-project/mlir/31) SHARK and IREE is enabled by and heavily relies on [MLIR](https://mlir.llvm.org).
</details>
  
## License

nod.ai SHARK is licensed under the terms of the Apache 2.0 License with LLVM Exceptions.
See [LICENSE](LICENSE) for more information.

```

`apps/language_models/README.md`:

```md
## CodeGen Setup using SHARK-server

### Setup Server
- clone SHARK and setup the venv
- host the server using `python apps/stable_diffusion/web/index.py --api --server_port=<PORT>`
- default server address is `http://0.0.0.0:8080`

### Setup Client
1. fauxpilot-vscode (VSCode Extension):
- Code for the extension can be found [here](https://github.com/Venthe/vscode-fauxpilot)
- PreReq: VSCode extension (will need [`nodejs` and `npm`](https://nodejs.org/en/download) to compile and run the extension)
- Compile and Run the extension on VSCode (press F5 on VSCode), this opens a new VSCode window with the extension running
- Open VSCode settings, search for fauxpilot in settings and modify `server : http://<IP>:<PORT>`, `Model : codegen` , `Max Lines : 30`

2. Others (REST API curl, OpenAI Python bindings) as shown [here](https://github.com/fauxpilot/fauxpilot/blob/main/documentation/client.md)
- using Github Copilot VSCode extension with SHARK-server needs more work to be functional.
```

`apps/language_models/langchain/README.md`:

```md
# Langchain

## How to run the model

1.) Install all the dependencies by running:
```shell
pip install -r apps/language_models/langchain/langchain_requirements.txt
sudo apt-get install -y libmagic-dev poppler-utils tesseract-ocr libtesseract-dev libreoffice
```

2.) Create a folder named `user_path` in `apps/language_models/langchain/` directory.

Now, you are ready to use the model.

3.) To run the model, run the following command:
```shell
python apps/language_models/langchain/gen.py --cli=True
```

```

`apps/language_models/langchain/cli.py`:

```py
import copy
import torch

from evaluate_params import eval_func_param_names
from gen import Langchain
from prompter import non_hf_types
from utils import clear_torch_cache, NullContext, get_kwargs


def run_cli(  # for local function:
    base_model=None,
    lora_weights=None,
    inference_server=None,
    debug=None,
    chat_context=None,
    examples=None,
    memory_restriction_level=None,
    # for get_model:
    score_model=None,
    load_8bit=None,
    load_4bit=None,
    load_half=None,
    load_gptq=None,
    use_safetensors=None,
    infer_devices=None,
    tokenizer_base_model=None,
    gpu_id=None,
    local_files_only=None,
    resume_download=None,
    use_auth_token=None,
    trust_remote_code=None,
    offload_folder=None,
    compile_model=None,
    # for some evaluate args
    stream_output=None,
    prompt_type=None,
    prompt_dict=None,
    temperature=None,
    top_p=None,
    top_k=None,
    num_beams=None,
    max_new_tokens=None,
    min_new_tokens=None,
    early_stopping=None,
    max_time=None,
    repetition_penalty=None,
    num_return_sequences=None,
    do_sample=None,
    chat=None,
    langchain_mode=None,
    langchain_action=None,
    document_choice=None,
    top_k_docs=None,
    chunk=None,
    chunk_size=None,
    # for evaluate kwargs
    src_lang=None,
    tgt_lang=None,
    concurrency_count=None,
    save_dir=None,
    sanitize_bot_response=None,
    model_state0=None,
    max_max_new_tokens=None,
    is_public=None,
    max_max_time=None,
    raise_generate_gpu_exceptions=None,
    load_db_if_exists=None,
    dbs=None,
    user_path=None,
    detect_user_path_changes_every_query=None,
    use_openai_embedding=None,
    use_openai_model=None,
    hf_embedding_model=None,
    db_type=None,
    n_jobs=None,
    first_para=None,
    text_limit=None,
    verbose=None,
    cli=None,
    reverse_docs=None,
    use_cache=None,
    auto_reduce_chunks=None,
    max_chunks=None,
    model_lock=None,
    force_langchain_evaluate=None,
    model_state_none=None,
    # unique to this function:
    cli_loop=None,
):
    Langchain.check_locals(**locals())

    score_model = ""  # FIXME: For now, so user doesn't have to pass
    n_gpus = torch.cuda.device_count() if torch.cuda.is_available else 0
    device = "cpu" if n_gpus == 0 else "cuda"
    context_class = NullContext if n_gpus > 1 or n_gpus == 0 else torch.device

    with context_class(device):
        from functools import partial

        # get score model
        smodel, stokenizer, sdevice = Langchain.get_score_model(
            reward_type=True,
            **get_kwargs(
                Langchain.get_score_model,
                exclude_names=["reward_type"],
                **locals()
            )
        )

        model, tokenizer, device = Langchain.get_model(
            reward_type=False,
            **get_kwargs(
                Langchain.get_model, exclude_names=["reward_type"], **locals()
            )
        )
        model_dict = dict(
            base_model=base_model,
            tokenizer_base_model=tokenizer_base_model,
            lora_weights=lora_weights,
            inference_server=inference_server,
            prompt_type=prompt_type,
            prompt_dict=prompt_dict,
        )
        model_state = dict(model=model, tokenizer=tokenizer, device=device)
        model_state.update(model_dict)
        my_db_state = [None]
        fun = partial(
            Langchain.evaluate,
            model_state,
            my_db_state,
            **get_kwargs(
                Langchain.evaluate,
                exclude_names=["model_state", "my_db_state"]
                + eval_func_param_names,
                **locals()
            )
        )

        example1 = examples[-1]  # pick reference example
        all_generations = []
        while True:
            clear_torch_cache()
            instruction = input("\nEnter an instruction: ")
            if instruction == "exit":
                break

            eval_vars = copy.deepcopy(example1)
            eval_vars[eval_func_param_names.index("instruction")] = eval_vars[
                eval_func_param_names.index("instruction_nochat")
            ] = instruction
            eval_vars[eval_func_param_names.index("iinput")] = eval_vars[
                eval_func_param_names.index("iinput_nochat")
            ] = ""  # no input yet
            eval_vars[
                eval_func_param_names.index("context")
            ] = ""  # no context yet

            # grab other parameters, like langchain_mode
            for k in eval_func_param_names:
                if k in locals():
                    eval_vars[eval_func_param_names.index(k)] = locals()[k]

            gener = fun(*tuple(eval_vars))
            outr = ""
            res_old = ""
            for gen_output in gener:
                res = gen_output["response"]
                extra = gen_output["sources"]
                if base_model not in non_hf_types or base_model in ["llama"]:
                    if not stream_output:
                        print(res)
                    else:
                        # then stream output for gradio that has full output each generation, so need here to show only new chars
                        diff = res[len(res_old) :]
                        print(diff, end="", flush=True)
                        res_old = res
                    outr = res  # don't accumulate
                else:
                    outr += res  # just is one thing
                    if extra:
                        # show sources at end after model itself had streamed to std rest of response
                        print(extra, flush=True)
            all_generations.append(outr + "\n")
            if not cli_loop:
                break
    return all_generations

```

`apps/language_models/langchain/create_data.py`:

```py
"""
Dataset creation tools.

Keep to-level imports clean of non-trivial imports for specific tools,
because this file is imported for various purposes
"""

import ast
import concurrent.futures
import contextlib
import hashlib
import json
import os
import shutil
import signal
import sys
import traceback
from concurrent.futures import ProcessPoolExecutor

import psutil
import pytest
import pandas as pd
import numpy as np
from tqdm import tqdm

from utils import flatten_list, remove


def parse_rst_file(filepath):
    with open(filepath, "r") as f:
        input_data = f.read()
    settings_overrides = {"initial_header_level": 2}
    from docutils import core

    document = core.publish_doctree(
        source=input_data,
        source_path=filepath,
        settings_overrides=settings_overrides,
    )
    qa_pairs = []
    current_section = None
    current_question = ""
    current_answer = ""
    for node in document.traverse():
        if node.__class__.__name__ == "section":
            current_section = ""
        elif current_section is not None:
            if node.__class__.__name__ == "Text":
                if node.astext()[-1] == "?":
                    if current_question:
                        qa_pairs.append((current_question, current_answer))
                    current_question = node.astext()
                    current_answer = ""
                else:
                    current_answer += node.astext()
    if current_answer:
        qa_pairs.append((current_question, current_answer))
    return {k: v for k, v in qa_pairs}


def test_scrape_dai_docs():
    home = os.path.expanduser("~")
    file = os.path.join(home, "h2oai/docs/faq.rst")
    qa_pairs = parse_rst_file(file)
    prompt_type = "human_bot"
    from prompter import prompt_types

    assert prompt_type in prompt_types
    save_thing = [
        {"instruction": k, "output": v, "prompt_type": prompt_type}
        for k, v in qa_pairs.items()
    ]
    output_file = "dai_faq.json"
    with open(output_file, "wt") as f:
        f.write(json.dumps(save_thing, indent=2))


def test_scrape_dai_docs_all():
    """
    pytest create_data.py::test_scrape_dai_docs_all
    """
    import glob
    import nltk

    nltk.download("punkt")
    dd = {}
    np.random.seed(1234)
    home = os.path.expanduser("~")
    files = list(glob.glob(os.path.join(home, "h2oai/docs/**/*rst")))
    np.random.shuffle(files)
    val_count = int(0.05 * len(files))
    train_files = files[val_count:]
    valid_files = files[:val_count]
    things = [
        ("dai_docs.train.json", train_files),
        ("dai_docs.valid.json", valid_files),
    ]
    for LEN in [100, 200, 500]:
        for output_file, ff in things:
            if output_file not in dd:
                dd[output_file] = []
            for f in ff:
                with open(f) as input:
                    blob = input.read()
                    blob = blob.replace("~~", "")
                    blob = blob.replace("==", "")
                    blob = blob.replace("''", "")
                    blob = blob.replace("--", "")
                    blob = blob.replace("**", "")
                    dd[output_file].extend(get_sentences(blob, length=LEN))
    for output_file, _ in things:
        save_thing = [
            {"output": k.strip(), "prompt_type": "plain"}
            for k in dd[output_file]
        ]
        with open(output_file, "wt") as f:
            f.write(json.dumps(save_thing, indent=2))


def get_sentences(blob, length):
    """
    break-up input text into sentences and then output list of sentences of about length in size
    :param blob:
    :param length:
    :return:
    """
    import nltk

    nltk.download("punkt")
    from nltk.tokenize import sent_tokenize

    sentences = sent_tokenize(blob)
    my_sentences = []
    my_string = ""
    for sentence in sentences:
        if len(my_string) + len(sentence) <= length:
            if my_string:
                my_string += " " + sentence
            else:
                my_string = sentence
        else:
            my_sentences.append(my_string)
            my_string = ""
    return my_sentences or [my_string]


def setup_dai_docs(path=None, dst="working_dir_docs", from_hf=False):
    """
    Only supported if have access to source code or HF token for HF spaces and from_hf=True
    :param path:
    :param dst:
    :param from_hf:
    :return:
    """

    home = os.path.expanduser("~")

    if from_hf:
        # assumes
        from huggingface_hub import hf_hub_download

        # True for case when locally already logged in with correct token, so don't have to set key
        token = os.getenv("HUGGINGFACE_API_TOKEN", True)
        path_to_zip_file = hf_hub_download(
            "h2oai/dai_docs", "dai_docs.zip", token=token, repo_type="dataset"
        )
        path = "h2oai"
        import zipfile

        with zipfile.ZipFile(path_to_zip_file, "r") as zip_ref:
            zip_ref.extractall(path)
        path = os.path.join(path, "docs/**/*")

    if path is None:
        if os.path.isdir(os.path.join(home, "h2oai")):
            path = os.path.join(home, "h2oai/docs/**/*")
        else:
            assert os.path.isdir(os.path.join(home, "h2oai.superclean")), (
                "%s does not exist" % path
            )
            path = os.path.join(home, "h2oai.superclean/docs/**/*")
    import glob

    files = list(glob.glob(path, recursive=True))

    # pandoc can't find include files

    remove(dst)
    os.makedirs(dst)

    # copy full tree, for absolute paths in rst
    for fil in files:
        if os.path.isfile(fil):
            shutil.copy(fil, dst)

    # hack for relative path
    scorers_dir = os.path.join(dst, "scorers")
    makedirs(scorers_dir)
    for fil in glob.glob(os.path.join(dst, "*.frag")):
        shutil.copy(fil, scorers_dir)

    return dst


def rst_to_outputs(files, min_len=30, max_len=2048 // 2 - 30):
    # account for sequence length (context window) including prompt and input and output

    # os.system('pandoc -f rst -t plain ./expert_settings/nlp_settings.rst')
    import pypandoc

    basedir = os.path.abspath(os.getcwd())

    outputs = []
    for fil in files:
        os.chdir(basedir)
        os.chdir(os.path.dirname(fil))
        fil = os.path.basename(fil)
        print("Processing %s" % fil, flush=True)
        # out_format can be one of: asciidoc, asciidoctor, beamer, biblatex, bibtex, commonmark, commonmark_x,
        # context, csljson, docbook, docbook4, docbook5, docx, dokuwiki,
        # dzslides, epub, epub2, epub3, fb2, gfm, haddock, html, html4, html5, icml,
        # ipynb, jats, jats_archiving, jats_articleauthoring, jats_publishing, jira,
        # json, latex, man,
        # markdown, markdown_github, markdown_mmd, markdown_phpextra, markdown_strict,
        # mediawiki, ms, muse, native, odt, opendocument, opml, org, pdf, plain, pptx,
        # revealjs, rst, rtf, s5, slideous, slidy, tei, texinfo, textile, xwiki, zimwiki
        out_format = "plain"
        # avoid extra new lines injected into text
        extra_args = ["--wrap=preserve", '--resource path="%s" % dst']

        plain_list = []
        try:
            # valid for expert settings
            input_rst = pypandoc.convert_file(fil, "rst")
            input_list = input_rst.split("\n``")
            for input_subrst in input_list:
                input_plain = pypandoc.convert_text(
                    input_subrst, format="rst", to="plain"
                )
                plain_list.append([input_plain, fil])
        except Exception as e:
            print("file exception: %s %s" % (fil, str(e)), flush=True)

        if not plain_list:
            # if failed to process as pieces of rst, then
            output = pypandoc.convert_file(
                fil, out_format, extra_args=extra_args, format="rst"
            )
            outputs1 = get_sentences(output, length=max_len)
            for oi, output in enumerate(outputs1):
                output = output.replace("\n\n", "\n")
                plain_list.append([output, fil])
        outputs.extend(plain_list)

    # report:
    # [print(len(x)) for x in outputs]

    # deal with blocks longer than context size (sequence length) of 2048
    new_outputs = []
    num_truncated = 0
    num_orig = len(outputs)
    for output, fil in outputs:
        if len(output) < max_len:
            new_outputs.append([output, fil])
            continue
        outputs1 = get_sentences(output, length=max_len)
        for oi, output1 in enumerate(outputs1):
            output1 = output1.replace("\n\n", "\n")
            new_outputs.append([output1, fil])
        num_truncated += 1
    print(
        "num_orig: %s num_truncated: %s" % (num_orig, num_truncated),
        flush=True,
    )

    new_outputs = [
        [k.strip(), fil] for k, fil in new_outputs if len(k.strip()) > min_len
    ]

    return new_outputs


def test_scrape_dai_docs_all_pandoc():
    """
    pytest -s -v create_data.py::test_scrape_dai_docs_all_pandoc
    :return:
    """

    dst = setup_dai_docs()

    import glob

    files = list(glob.glob(os.path.join(dst, "*rst"), recursive=True))

    basedir = os.path.abspath(os.getcwd())
    new_outputs = rst_to_outputs(files)
    os.chdir(basedir)

    remove(dst)
    save_thing = [
        {"output": k.strip(), "prompt_type": "plain"} for k in new_outputs
    ]
    output_file = "dai_docs.train_cleaned.json"
    with open(output_file, "wt") as f:
        f.write(json.dumps(save_thing, indent=2))


def test_config_to_json():
    """
    Needs to run from Driverless AI source directory.
    E.g. (base) jon@gpu:~/h2oai$ pytest -s -v /data/jon/h2ogpt/create_data.py::test_config_to_json ; cp config.json /data/jon/h2ogpt/
    :return:
    """
    try:
        # Arrange
        import json
        from h2oaicore.systemutils import config

        toml_list = []
        for k, v in config.get_meta_dict().items():
            title = (v.title + ": ") if v.title else ""
            comment = v.comment or ""
            if not (title or comment):
                continue
            toml_list.extend(
                [
                    {
                        "prompt_type": "plain",
                        "instruction": f"<human>: What does {k} do?\n<bot>: {k.replace('_', ' ')} config.toml:  {comment or title}\n<human>:".replace(
                            "\n", ""
                        ),
                    },
                    {
                        "prompt_type": "plain",
                        "instruction": f"<human>: Explain {k}.\n<bot>: {k.replace('_', ' ')} config.toml:  {comment or title}\n<human>:".replace(
                            "\n", ""
                        ),
                    },
                    {
                        "prompt_type": "plain",
                        "instruction": f"<human>: How can I do this: {title}.\n<bot>: Set the {k.replace('_', ' ')} config.toml\n<human>:".replace(
                            "\n", ""
                        ),
                    }
                    if title and comment
                    else None,
                    {
                        "prompt_type": "human_bot",
                        "instruction": f"Explain the following expert setting for Driverless AI",
                        "input": f"{k}",
                        "output": f"{k.replace('_', ' ')} config.toml: {comment or title}".replace(
                            "\n", ""
                        ),
                    },
                    {
                        "prompt_type": "human_bot",
                        "instruction": f"Explain the following expert setting for Driverless AI",
                        "input": f"{k}",
                        "output": f"{k.replace('_', ' ')} config.toml: {title}{comment}".replace(
                            "\n", ""
                        ),
                    },
                    {
                        "prompt_type": "human_bot",
                        "instruction": f"Explain the following expert setting for Driverless AI",
                        "input": f"{k.replace('_', ' ')}",
                        "output": f"{k.replace('_', ' ')} config.toml: {title}{comment}".replace(
                            "\n", ""
                        ),
                    },
                    {
                        "prompt_type": "human_bot",
                        "instruction": f"Explain the following expert setting for Driverless AI",
                        "input": f"{title}",
                        "output": f"{k.replace('_', ' ')} config.toml: {title}{comment}".replace(
                            "\n", ""
                        ),
                    },
                    {
                        "prompt_type": "human_bot",
                        "instruction": f"Provide a short explanation of the expert setting {k}",
                        "output": f"{k.replace('_', ' ')} config.toml: {comment or title}".replace(
                            "\n", ""
                        ),
                    },
                    {
                        "prompt_type": "human_bot",
                        "instruction": f"Provide a detailed explanation of the expert setting {k}",
                        "output": f"{k.replace('_', ' ')} config.toml: {title}{comment}".replace(
                            "\n", ""
                        ),
                    },
                ]
            )
        toml_list = [x for x in toml_list if x]
        with open("config.json", "wt") as f:
            f.write(json.dumps(toml_list, indent=2))
    except Exception as e:
        print("Exception: %s" % str(e), flush=True)


def copy_tree(src, dst, follow_symlink=False):
    makedirs(dst, exist_ok=True)
    for path, dirs, files in os.walk(src, followlinks=follow_symlink):
        new_path = path.replace(src, dst)
        makedirs(new_path, exist_ok=True)
        for file in files:
            filename = os.path.join(path, file)
            new_filename = os.path.join(new_path, file)
            # print("%s -> %s" % (filename, new_filename))
            try:
                atomic_copy(filename, new_filename)
            except FileNotFoundError:
                pass


def atomic_move(src, dst):
    try:
        shutil.move(src, dst)
    except (shutil.Error, FileExistsError):
        pass
    remove(src)


def atomic_copy(src=None, dst=None, with_permissions=True):
    if os.path.isfile(dst):
        return
    import uuid

    my_uuid = uuid.uuid4()
    dst_tmp = dst + str(my_uuid)
    makedirs(os.path.dirname(dst), exist_ok=True)
    if with_permissions:
        shutil.copy(src, dst_tmp)
    else:
        shutil.copyfile(src, dst_tmp)
    atomic_move(dst_tmp, dst)
    remove(dst_tmp)


def makedirs(path, exist_ok=True):
    """
    Avoid some inefficiency in os.makedirs()
    :param path:
    :param exist_ok:
    :return:
    """
    if os.path.isdir(path) and os.path.exists(path):
        assert exist_ok, "Path already exists"
        return path
    os.makedirs(path, exist_ok=exist_ok)


## Download from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_unfiltered_cleaned_split.json
## Turn into simple instruct prompt type. No context/previous conversations.
def test_prep_instruct_vicuna():
    from datasets import load_dataset

    filename = "ShareGPT_unfiltered_cleaned_split.json"
    if not os.path.exists(filename):
        os.system(
            "wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/%s"
            % filename
        )
    data = load_dataset("json", data_files={"train": filename})["train"]
    training_rows = []
    for i in range(data.num_rows):
        conversations = data[i]["conversations"]
        assert isinstance(conversations, list), conversations
        convo = ""
        for j, conv in enumerate(conversations):
            # Get ready for generate.py prompt_type=human_bot
            # But train with prompt_type=plain
            if conv["from"] == "human":
                FROM = "<human>: "
            elif conv["from"] == "gpt":
                FROM = "<bot>: "
            convo += f"{FROM}" + conv["value"] + "\n"
        if convo:
            training_rows.append(dict(input=convo))
    with open(filename + ".generate_human_bot.train_plain.json", "wt") as f:
        f.write(json.dumps(training_rows, indent=2))


POSTFIX = ".generate_human_bot.train_plain.json"

# https://bair.berkeley.edu/blog/2023/04/03/koala/
OIG_DATASETS = [
    "unified_chip2.jsonl",
    "unified_grade_school_math_instructions.jsonl",
    "unified_poetry_2_song.jsonl",
    "unified_plot_screenplay_books_dialog.jsonl",
]

# hub issue: https://huggingface.co/datasets/laion/OIG/discussions/4
ALL_OIG_DATASETS = [
    "unified_abstract_infill.jsonl",
    "unified_basic.jsonl",
    "unified_canadian_parliament.jsonl",
    "unified_chip2.jsonl",
    "unified_conv_finqa.jsonl",
    "unified_cuad.jsonl",
    "unified_essays.jsonl",
    "unified_flan.jsonl.gz",
    "unified_grade_school_math_instructions.jsonl",
    "unified_hc3_human.jsonl",
    "unified_image_prompts_instructions.jsonl",
    "unified_joke_explanations.jsonl",
    "unified_mathqa_flanv2_kojma_cot.jsonl",
    "unified_merged_code_xp3.jsonl",
    "unified_multi_news.jsonl",
    "unified_multi_sum.jsonl",
    "unified_ni.jsonl.gz",
    "unified_nq.jsonl",
    "unified_openai_summarize_tldr.jsonl",
    "unified_oscar_en_sample_dialog.jsonl",
    "unified_p3.jsonl.gz",
    "unified_plot_screenplay_books_dialog.jsonl",
    "unified_poetry_2_song.jsonl",
    "unified_poetry_instructions.jsonl",
    "unified_rallio_safety_and_prosocial.jsonl",
    "unified_rallio_soda_upgraded_2048.jsonl",
    "unified_soda_dialog.jsonl",
    "unified_sqlv1.jsonl",
    "unified_sqlv2.jsonl",
    "unified_squad_v2.jsonl",
    "unified_squad_v2_more_neg.jsonl",
    "unified_ul2_plus_oscar_en_sample_dialog.jsonl",
    "unified_unifiedskg_instructions.jsonl",
    "unified_unnatural_instructions.jsonl",
    "unified_xp3_sample.jsonl",
]

useful_oig_files = [
    "unified_rallio_safety_and_prosocial.jsonl.parquet",
    "unified_chip2.jsonl.parquet",
    "unified_cuad.jsonl.parquet",
    "unified_essays.jsonl.parquet",
    "unified_flan.jsonl.gz.parquet",
    "unified_grade_school_math_instructions.jsonl.parquet",
    "unified_hc3_human.jsonl.parquet",
    "unified_mathqa_flanv2_kojma_cot.jsonl.parquet",
    "unified_merged_code_xp3.jsonl.parquet",
    "unified_multi_news.jsonl.parquet",
    # 'unified_multi_sum.jsonl.parquet'
    "unified_ni.jsonl.gz.parquet",
    "unified_openai_summarize_tldr.jsonl.parquet",
    # 'unified_oscar_en_sample_dialog.jsonl.parquet', # create text containing these N words, not specific
    "unified_plot_screenplay_books_dialog.jsonl.parquet",
    "unified_soda_dialog.jsonl.parquet",
    "unified_unnatural_instructions.jsonl.parquet",
]


@pytest.mark.parametrize("filename", OIG_DATASETS)
def test_get_small_sample_oig_data(filename):
    if not os.path.exists(filename):
        os.system(
            "wget https://huggingface.co/datasets/laion/OIG/resolve/main/%s"
            % filename
        )
    import json

    rows = []
    with open(filename, "r") as f:
        for line in f.readlines():
            row = json.loads(line)
            rows.append(dict(input=row["text"]))
    with open(filename + POSTFIX, "w") as f:
        f.write(json.dumps(rows, indent=2))


@pytest.mark.parametrize("filename", ALL_OIG_DATASETS)
def test_download_useful_data_as_parquet(filename):
    dest_file = filename + ".parquet"
    if dest_file not in useful_oig_files:
        pytest.skip("file declared not useful")
    if not os.path.exists(filename):
        os.system(
            "wget https://huggingface.co/datasets/laion/OIG/resolve/main/%s"
            % filename
        )
    if not os.path.exists(dest_file):
        df = pd.read_json(path_or_buf=filename, lines=True)
        df.to_parquet(dest_file, index=False)


def test_merge_shuffle_small_sample_oig_data():
    np.random.seed(1234)
    rows = []
    for filename in OIG_DATASETS:
        with open(filename + POSTFIX, "r") as f:
            rows.extend(json.loads(f.read()))
    np.random.shuffle(rows)
    with open(
        "merged_shuffled_OIG_%s.json"
        % hashlib.sha256(str(OIG_DATASETS).encode()).hexdigest()[:10],
        "w",
    ) as f:
        f.write(json.dumps(rows, indent=2))


def test_join_jsons():
    files = (
        ["config.json"] * 1
        + ["dai_docs.train_cleaned.json"] * 2
        + ["dai_faq.json"] * 3
    )
    print(files)
    lst = []
    [lst.extend(json.load(open(fil, "rt"))) for fil in files]
    print(len(lst))
    json.dump(lst, open("merged.json", "wt"), indent=2)


@pytest.mark.parametrize("filename", ["Anthropic/hh-rlhf"])
def test_make_rlhf_good_data(filename):
    from datasets import load_dataset

    rows = load_dataset(filename)["train"]["chosen"]
    new_rows = []
    for row in rows:
        if row[:2] == "\n\n":
            row = row[2:]
        row = row.replace("Human: ", "<human>: ")
        row = row.replace("Assistant: ", "<bot>: ")
        new_rows.append(dict(input=row))
    with open(filename.replace("/", "_") + POSTFIX, "w") as f:
        f.write(json.dumps(new_rows, indent=2))


def test_show_prompts():
    files = (
        ["config.json"] * 1
        + ["dai_docs.train_cleaned.json"] * 1
        + ["dai_faq.json"] * 1
    )
    file_points = [json.load(open(fil, "rt")) for fil in files]
    from prompter import generate_prompt

    for data_points in file_points:
        for data_point in data_points:
            print(
                generate_prompt(data_point, "plain", "", False, False, False)[
                    0
                ]
            )


def test_get_open_datasets():
    # HF changed things so don't get raw list of all datasets, so not have to filter, but can't do negative filter
    open_tags = [
        "license:Apache License 2.0",
        "license:mit",
        "license:apache",
        "license:apache2",
        "license:apache-2.0",
        "license:bsd",
        "license:bsd-2-clause",
        "license:bsd-3-clause",
        "license:bsd-3-clause-clear",
        "license:lgpl-2.1",
        "license:lgpl-3.0",
        "license:lgpl-lr",
        "license:lgpl",
        "license:openrail++",
        "license:openrail",
        "license:bigscience-bloom-rail-1.0",
        # 'license:agpl-3.0',
        "license:other",
        "license:unknown",
        # 'license:mpl-2.0',     # ok, but would have to include original copyright, license, source, copies in distribution
        # Attribution required:
        "license:odc-by",
        "license:cc-by-4.0",
        "license:cc-by-3.0",
        "license:cc-by-2.0",
        "license:cc-by-2.5",
        # 'license:cc-by-sa-4.0',  # would require same license
        "license:odbl",
        "license:pddl",
        "license:ms-pl",
        "license:zlib",
    ]
    # bad license: cc-by-nc-4.0

    from huggingface_hub import list_datasets

    datasets = flatten_list(
        [[x for x in list_datasets(filter=y)] for y in open_tags]
    )
    datasets += [x for x in list_datasets(author="openai")]
    # check all:
    all_license_tags = set(
        flatten_list([[y for y in x.tags if "license" in y] for x in datasets])
    )
    print(len(all_license_tags))
    open_datasets = [
        x
        for x in datasets
        if any([y in x.tags for y in open_tags])
        or "license:" not in str(x.tags)
    ]
    print("open_datasets", len(open_datasets))
    all_task_tags = set(
        flatten_list(
            [[y for y in x.tags if "task" in y] for x in open_datasets]
        )
    )
    print("all_task_tags", len(all_task_tags))
    excluded_tags = [
        "image",
        "hate",
        "tabular",
        "table-",
        "classification",
        "retrieval",
        "translation",
        "identification",
        "object",
        "mask",
        "to-text",
        "face-detection",
        "audio",
        "voice",
        "reinforcement",
        "depth-est",
        "forecasting",
        "parsing",
        "visual",
        "speech",
        "multiple-choice",
        "slot-filling",
        "irds/argsme",
        "-scoring",
        "other",
        "graph-ml",
        "feature-extraction",
        "keyword-spotting",
        "coreference-resolution",
        "segmentation",
        "word-sense-disambiguation",
        "lemmatization",
    ]
    task_tags = [
        x.replace("task_categories:", "").replace("task_ids:", "")
        for x in all_task_tags
        if not any([y in x for y in excluded_tags])
    ]
    print("task_tags", len(task_tags))
    # str(x.tags) to catch any pattern match to anything in list
    open_tasked_datasets = [
        x
        for x in open_datasets
        if any(
            [y in str([x for x in x.tags if "task" in x]) for y in task_tags]
        )
        and not any(
            [
                y in str([x for x in x.tags if "task" in x])
                for y in excluded_tags
            ]
        )
        or "task_categories" not in str(x.tags)
        and "task_ids" not in str(x.tags)
    ]
    open_tasked_datasets = [x for x in open_tasked_datasets if not x.disabled]
    open_tasked_datasets = [x for x in open_tasked_datasets if not x.gated]
    open_tasked_datasets = [x for x in open_tasked_datasets if not x.private]
    print("open_tasked_datasets", len(open_tasked_datasets))
    sizes = list(
        set(
            flatten_list(
                [
                    [(y, x.id) for y in x.tags if "size" in y]
                    for x in open_tasked_datasets
                ]
            )
        )
    )
    languages = list(
        set(
            flatten_list(
                [
                    [(y, x.id) for y in x.tags if "language:" in y]
                    for x in open_tasked_datasets
                ]
            )
        )
    )
    open_english_tasked_datasets = [
        x
        for x in open_tasked_datasets
        if "language:" not in str(x.tags) or "language:en" in str(x.tags)
    ]
    small_open_english_tasked_datasets = [
        x
        for x in open_english_tasked_datasets
        if "n<1K" in str(x.tags)
        or "1K<n<10K" in str(x.tags)
        or "1K0<n<100K" in str(x.tags)
        or "100K<n<1M" in str(x.tags)
        or "size_category" not in str(x.tags)
    ]
    # 'aeslc' : email_body, subject -> summarization?
    # load_dataset(open_tasked_datasets[0].id).data['train'].to_pandas()
    ids = [x.id for x in small_open_english_tasked_datasets]

    # sanity checks
    # https://bair.berkeley.edu/blog/2023/04/03/koala/
    assert "alespalla/chatbot_instruction_prompts" in ids
    assert "laion/OIG" in ids
    assert "openai/webgpt_comparisons" in ids
    assert "openai/summarize_from_feedback" in ids
    assert "Anthropic/hh-rlhf" in ids

    # useful but not allowed for commercial purposes:
    # https://huggingface.co/datasets/squad

    print("open_english_tasked_datasets: ", ids, flush=True)

    exclude_ids = [
        "allenai/nllb",  # translation only
        "hf-internal-testing/fixtures_image_utils",  # testing
        "allenai/c4",  # search-url
        "agemagician/uniref50",  # unknown
        "huggingface-course/documentation-images",  # images
        "smilegate-ai/kor_unsmile",  # korean
        "MohamedRashad/ChatGPT-prompts",  # ChatGPT/LearnGPT/https://www.emergentmind.com/
        "humarin/chatgpt-paraphrases",  # Paraphrase using ChatGPT
        "Jeska/vaccinchat",  # not useful
        "alespalla/chatbot_instruction_prompts",  # mixes alpaca
        "allenai/prosocial-dialog",
        # already exlucded, but wrongly in other datasets that say more permissive license
        "AlekseyKorshuk/persona-chat",  # low quality
        "bavard/personachat_truecased",  # low quality
        "adamlin/daily_dialog",  # medium quality conversations
        "adamlin/FewShotWoz",  # low quality
        "benjaminbeilharz/better_daily_dialog",  # low quality
        "benjaminbeilharz/daily_dialog_w_turn_templates",  # low
        "benjaminbeilharz/empathetic_dialogues_for_lm",  # low
        "GEM-submissions/GEM__bart_base_schema_guided_dialog__1645547915",  # NA
        "ia-bentebib/conv_ai_2_fr",  # low fr
        "ia-bentebib/daily_dialog_fr",  # low fr
        "ia-bentebib/dialog_re_fr",  # low fr
        "ia-bentebib/empathetic_dialogues_fr",  # low fr
        "roskoN/dailydialog",  # low
        "VadorMazer/skyrimdialogstest",  # low
        "bigbio/med_qa",  # med specific Q/A
        "biu-nlp/qa_srl2018",  # low quality Q/A
        "biu-nlp/qa_discourse",  # low quality Q/A
        "iarfmoose/qa_evaluator",  # low quality Q/A
        "jeopardy",  # low quality Q/A -- no reasoning
        "narrativeqa",  # low quality Q/A
        "nomic-ai/gpt4all_prompt_generations",  # bad license
        "nomic-ai/gpt4all_prompt_generations_with_p3",  # bad license
        "HuggingFaceH4/alpaca",  # bad license
        "tatsu-lab/alpaca",  # ToS breaking
        "yahma/alpaca-cleaned",  # ToS breaking
        "Hello-SimpleAI/HC3",  # bad license
        "glue",  # no reasoning QA
        "sahil2801/CodeAlpaca-20k",  # bad license
        "Short-Answer-Feedback/saf_communication_networks_english",  # long Q, medium A
    ]
    small_open_english_tasked_datasets = [
        x
        for x in small_open_english_tasked_datasets
        if x.id not in exclude_ids
    ]
    # some ids clearly speech related
    small_open_english_tasked_datasets = [
        x for x in small_open_english_tasked_datasets if "speech" not in x.id
    ]
    # HF testing
    small_open_english_tasked_datasets = [
        x
        for x in small_open_english_tasked_datasets
        if "hf-internal-testing" not in x.id
    ]
    small_open_english_tasked_datasets = [
        x for x in small_open_english_tasked_datasets if "chinese" not in x.id
    ]

    sorted_small_open_english_tasked_datasets = sorted(
        [(x.downloads, x) for x in small_open_english_tasked_datasets],
        key=lambda x: x[0],
        reverse=True,
    )

    # NOTES:
    # Run like pytest -s -v create_data.py::test_get_open_datasets &> getdata9.log
    # See what needs config passed and add:
    # grep 'load_dataset(' getdata9.log|grep -v data_id|less -S
    # grep "pip install" getdata9.log
    # NOTE: Some datasets have default config, but others are there.  Don't know how to access them.

    """
    https://huggingface.co/datasets/wikihow/blob/main/wikihow.py
    https://github.com/mahnazkoupaee/WikiHow-Dataset
    https://ucsb.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358
    https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358
    """

    """
    # some ambiguous or non-commercial datasets
    https://github.com/PhoebusSi/alpaca-CoT
    """

    timeout = 3 * 60
    # laion/OIG takes longer
    for num_downloads, dataset in sorted_small_open_english_tasked_datasets:
        data_id = dataset.id
        func = do_one
        args = (data_id, num_downloads)
        kwargs = {}
        with ProcessPoolExecutor(max_workers=1) as executor:
            future = executor.submit(func, *args, **kwargs)
            try:
                future.result(timeout=timeout)
            except concurrent.futures.TimeoutError:
                print("\n\ndata_id %s timeout\n\n" % data_id, flush=True)
            for child in psutil.Process(os.getpid()).children(recursive=True):
                os.kill(child.pid, signal.SIGINT)
                os.kill(child.pid, signal.SIGTERM)
                os.kill(child.pid, signal.SIGKILL)


def do_one(data_id, num_downloads):
    from datasets import load_dataset

    out_file = "data_%s.parquet" % str(data_id.replace("/", "_"))
    if os.path.isfile(out_file) and os.path.getsize(out_file) > 1024**3:
        return
    try:
        print(
            "Loading data_id %s num_downloads: %s" % (data_id, num_downloads),
            flush=True,
        )
        avail_list = None
        try:
            data = load_dataset(data_id, "foobar")
        except Exception as e:
            if "Available: " in str(e):
                avail_list = ast.literal_eval(
                    str(e).split("Available:")[1].strip()
                )
            else:
                avail_list = None
        if avail_list is None:
            avail_list = [None]
        print("%s avail_list: %s" % (data_id, avail_list), flush=True)

        for name in avail_list:
            out_file = "data_%s_%s.parquet" % (
                str(data_id.replace("/", "_")),
                str(name),
            )
            if os.path.isfile(out_file):
                continue
            data = load_dataset(data_id, name)
            column_names_dict = data.column_names
            column_names = column_names_dict[list(column_names_dict.keys())[0]]
            print(
                "Processing data_id %s num_downloads: %s columns: %s"
                % (data_id, num_downloads, column_names),
                flush=True,
            )
            data_dict = data.data
            col_dict = data.num_columns
            first_col = list(col_dict.keys())[0]
            if "train" in data_dict:
                df = data["train"].to_pandas()
            else:
                df = data[first_col].to_pandas()
            # csv has issues with escaping chars, even for datasets I know I want
            df.to_parquet(out_file, index=False)
    except Exception as e:
        t, v, tb = sys.exc_info()
        ex = "".join(traceback.format_exception(t, v, tb))
        print("Exception: %s %s" % (data_id, ex), flush=True)


def test_otherlic():
    from huggingface_hub import list_datasets

    lic = [
        "license:odc-by",
        "license:cc-by-4.0",
        "license:cc-by-3.0",
        "license:cc-by-2.0",
        "license:cc-by-2.5",
        "license:cc-by-sa-4.0",
        "license:odbl",
        "license:pddl",
        "license:ms-pl",
        "license:zlib",
    ]
    datasets = flatten_list(
        [
            [
                x
                for x in list_datasets(filter=y)
                if "translation" not in str(x.tags)
            ]
            for y in lic
        ]
    )
    print(len(datasets))


# These useful datasets are determined based upon data sample, column types, and uniqueness compared to larger datasets like Pile
# grep columns getdata13.log|grep -v "\['image'\]"|sort|uniq|grep -v tokens|grep -v "'image'"|grep -v embedding|grep dialog
useful = [
    "Dahoas/instruct-human-assistant-prompt",
    "Dahoas/first-instruct-human-assistant-prompt",
    "knkarthick/dialogsum",  # summary of conversation
    "McGill-NLP/FaithDial",  # medium quality
    "Zaid/quac_expanded",  # medium quality context + QA
    "0-hero/OIG-small-chip2",  # medium
    "alistvt/coqa-flat",  # QA medium
    "AnonymousSub/MedQuAD_47441_Question_Answer_Pairs",  # QA medium
    "Anthropic/hh-rlhf",  # high quality  # similar to Dahoas/full-hh-rlhf
    "arjunth2001/online_privacy_qna",  # good quality QA
    "Dahoas/instruct_helpful_preferences",  # medium quality instruct
    "Dahoas/rl-prompt-dataset",  # medium chat
    "Dahoas/rm-static",  # medium chat
    "Dahoas/static-hh",  # medium chat  # HuggingFaceH4/self_instruct
    "Dahoas/synthetic-instruct-gptj-pairwise",  # medium chat
    "eli5",  # QA if prompt ELI5
    "gsm8k",  # QA (various)
    "guanaco/guanaco",  # prompt/response
    "kastan/rlhf-qa-comparisons",  # good QA
    "kastan/rlhf-qa-conditional-generation-v2",  # prompt answer
    "OllieStanley/humaneval-mbpp-codegen-qa",  # code QA, but started from words, so better than other code QA
    "OllieStanley/humaneval-mbpp-testgen-qa",  # code QA
    "Graverman/Instruct-to-Code",  # code QA
    "openai/summarize_from_feedback",  # summarize
    "relbert/analogy_questions",  # analogy QA
    "yitingxie/rlhf-reward-datasets",  # prompt, chosen, rejected.
    "yizhongw/self_instruct",  # instruct (super natural & instruct)
    "HuggingFaceH4/asss",  # QA, big A
    "kastan/rlhf-qa-conditional-generation-v2",  # QA
    "cosmos_qa",  # context QA
    "vishal-burman/c4-faqs",  # QA but not so much reasoning, but alot of text
    "squadshifts",  # QA from context
    "hotpot_qa",  # QA from context
    "adversarial_qa",  # QA from context
    "allenai/soda",  # dialog -> narrative/summary
    "squad_v2",  # context QA
    "squadshifts",  # context QA
    "dferndz/cSQuAD1",  # context QA
    "dferndz/cSQuAD2",  # context QA
    "din0s/msmarco-nlgen",  # context QA
    "domenicrosati/TruthfulQA",  # common sense truthful QA -- trivia but good trivia
    "hotpot_qa",  # context, QA
    "HuggingFaceH4/self-instruct-eval",  # instruct QA, medium quality, some language reasoning
    "kastan/EE_QA_for_RLHF",  # context QA
    "KK04/LogicInference_OA",  # instruction logical QA
    "lmqg/qa_squadshifts_synthetic",  # context QA
    "lmqg/qg_squad",  # context QA
    "lmqg/qg_squadshifts",  # context QA
    "lmqg/qg_subjqa",  # context QA
    "pszemraj/HC3-textgen-qa",
    # QA medium, has human responses -- humans tend to provide links instead of trying to answer
    "pythonist/newdata",  # long context, QA, brief A
    "ropes",  # long background, situation, question, A
    "wikitablequestions",  # table -> QA
    "bigscience/p3",  # context QA but short answers
]

code_useful = [
    "0n1xus/codexglue",
    "openai_humaneval",
    "koutch/staqc",
]

maybe_useful = [
    "AlekseyKorshuk/comedy-scripts",
    "openbookqa",  # hard to parse, low reasoning
    "qed",  # reasonable QA, but low reasoning
    "selqa",  # candidate answers
    "HuggingFaceH4/instruction-pilot-outputs-filtered",
    "GBaker/MedQA-USMLE-4-options",  # medical QA with long questions
    "npc-engine/light-batch-summarize-dialogue",  # dialog summarize, kinda low specific quality
]

summary_useful = [
    "austin/rheum_abstracts",
    "CarperAI/openai_summarize_comparisons",  # summarize chosen/rejected
    "CarperAI/openai_summarize_tldr",  # summarize QA
    "ccdv/cnn_dailymail",  # summarize news
    "ccdv/govreport-summarization",  # summarize high quality
    "ccdv/pubmed-summarization",  # summarize high quality
    "duorc",  # plot -> QA
    "farleyknight/big_patent_5_percent",  # desc -> abstract
    "multi_news",  # summary
    "opinosis",
    "SophieTr/reddit_clean",
    "allenai/mup",  # long text -> summary
    "allenai/multi_lexsum",  # long text -> summary
    "big_patent",
    "allenai/wcep_dense_max",
    "awinml/costco_long_practice",
    "GEM/xsum",
    "ratishsp/newshead",
    "RussianNLP/wikiomnia",  # russian
    "stacked-summaries/stacked-xsum-1024",
]

math_useful = ["competition_math"]

skipped = [
    "c4",  # maybe useful, used for flan, but skipped due to size
]

"""
To get training data from oig:
pytest test_oig test_grade_final test_finalize_to_json
"""

human = "<human>:"
bot = "<bot>:"


def test_assemble_and_detox():
    import re
    from profanity_check import predict_prob

    df_list = []
    for data in useful_oig_files:
        print("Processing %s" % data, flush=True)
        df = pd.read_parquet(data)
        df = df.reset_index(drop=True)
        # chop up into human/bot interactions of no more than 10kB per row
        text_list = df[["text"]].values.ravel().tolist()
        new_text = []
        max_len = 2048  # uber cutoff
        MAX_LEN = 2048 // 2 - 30  # max len per question/answer
        for text in tqdm(text_list):
            human_starts = [m.start() for m in re.finditer("<human>: ", text)]
            if len(human_starts) == 1:
                human_starts = [0, len(text)]  # always go into for loop below
            blurb = ""
            for i in range(len(human_starts) - 1):
                interaction = text[human_starts[i] : human_starts[i + 1]][
                    :max_len
                ]
                blurb += interaction
                if len(blurb) >= MAX_LEN:
                    blurb = get_sentences(blurb, length=MAX_LEN)[0]
                    new_text.append(blurb + "\n<human>:")
                    blurb = ""
            if blurb:
                blurb = get_sentences(blurb, length=MAX_LEN)[0]
                new_text.append(blurb + "\n<human>:")

        if len(new_text) > len(text_list):
            print(
                "Added %d new rows (before: %d)"
                % (len(new_text) - df.shape[0], df.shape[0])
            )
        df = pd.DataFrame({"text": new_text, "source": [data] * len(new_text)})
        df = df.drop_duplicates(keep="first")
        print(df["text"].apply(lambda x: len(x)).describe())
        assert df["text"].apply(lambda x: len(x)).max() <= 2 * max_len

        # faster than better_profanity, do early
        df["profanity"] = predict_prob(df["text"])
        before_rows = df.shape[0]
        df = df[df["profanity"] < 0.25]  # drop any low quality stuff
        after_rows = df.shape[0]
        print(
            "Dropped %d rows out of %d due to alt-profanity-check"
            % (before_rows - after_rows, before_rows)
        )
        df_list.append(df)
        print(
            "Done processing %s -> %s rows" % (data, df.shape[0]), flush=True
        )
        print("So far have %d rows" % sum([len(x) for x in df_list]))
    df_final = pd.concat(df_list)
    df_final = df_final.sample(frac=1, random_state=1234).reset_index(
        drop=True
    )
    df_final.to_parquet(
        "h2oGPT.cleaned.human_bot.shorter.parquet", index=False
    )


def test_basic_cleaning():
    # from better_profanity import profanity
    # https://pypi.org/project/alt-profanity-check/
    from profanity_check import predict

    df_list = []
    for data in useful_oig_files:
        # for data in useful_oig_files[:5]:
        # for data in ['unified_openai_summarize_tldr.jsonl.parquet']:
        print("Processing %s" % data, flush=True)
        df = pd.read_parquet(data)
        df = df.reset_index(drop=True)
        # NOTE: Not correct if multiple human-bot interactions, but those dialogs even more desired
        # avg_chars = len(df['text'][0])/(df['text'][0].count(human)+df['text'][0].count(bot))
        df["avg_words"] = df["text"].apply(
            lambda x: x.count(" ") / (x.count(human) + x.count(bot)) / 2.0
        )
        df["avg_bot_words"] = df["text"].apply(
            lambda x: x.split(bot)[1].count(" ") / x.count(bot)
        )
        # df['bad_words'] = df['text'].apply(lambda x: profanity.contains_profanity(x))
        # low_quality_patterns = ['Write the rest of this wikipedia article']
        res = predict(df["text"])
        df["bad_words"] = res
        df = df.reset_index(drop=True)
        df = df[df["bad_words"] == 0]
        df = df[["text", "avg_words", "avg_bot_words"]]
        df = df.drop_duplicates(keep="first")
        print(df[df["avg_words"] == df["avg_words"].max()]["text"].values)
        median_words = np.median(df["avg_words"])
        min_words_per_entity = max(30, 0.8 * median_words)
        max_words_per_entity = 2048  # too hard to learn from for now
        df = df[df["avg_words"] > min_words_per_entity]
        df = df[df["avg_words"] < max_words_per_entity]

        min_words_per_entity = max(
            20, 0.5 * median_words
        )  # bot should say stuff for now
        max_words_per_entity = 2048  # too hard to learn from for now
        df = df[df["avg_bot_words"] > min_words_per_entity]
        df = df[df["avg_bot_words"] < max_words_per_entity]

        df_list.append(df)
        print(
            "Done processing %s -> %s rows" % (data, df.shape[0]), flush=True
        )
    df_final = pd.concat(df_list)
    df_final.to_parquet("h2oGPT.cleaned.human_bot.parquet", index=False)


from joblib import Parallel, delayed, effective_n_jobs
from sklearn.utils import gen_even_slices
from sklearn.utils.validation import _num_samples


def parallel_apply(df, func, n_jobs=-1, **kwargs):
    """Pandas apply in parallel using joblib.
    Uses sklearn.utils to partition input evenly.

    Args:
        df: Pandas DataFrame, Series, or any other object that supports slicing and apply.
        func: Callable to apply
        n_jobs: Desired number of workers. Default value -1 means use all available cores.
        **kwargs: Any additional parameters will be supplied to the apply function

    Returns:
        Same as for normal Pandas DataFrame.apply()

    """

    if effective_n_jobs(n_jobs) == 1:
        return df.apply(func, **kwargs)
    else:
        ret = Parallel(n_jobs=n_jobs)(
            delayed(type(df).apply)(df[s], func, **kwargs)
            for s in gen_even_slices(
                _num_samples(df), effective_n_jobs(n_jobs)
            )
        )
        return pd.concat(ret)


def add_better_profanity_flag(df):
    from better_profanity import profanity

    df["better_profanity"] = parallel_apply(
        df["text"],
        lambda x: profanity.contains_profanity(x),
        n_jobs=-1,
    )
    return df


def add_textstat_grade(df):
    import textstat

    def myfunc(x):
        return textstat.flesch_kincaid_grade(x)  # simple grade

    if False:
        import dask.dataframe as dd

        # 40 seconds for 1000 rows, but have 1,787,799 rows
        ddata = dd.from_pandas(df, npartitions=120)

        df["flesch_grade"] = ddata["text"].apply(myfunc).compute()
    if True:
        # fast way
        df["flesch_grade"] = parallel_apply(df["text"], myfunc, n_jobs=-1)
    return df


def add_deberta_grade(df):
    from transformers import AutoModelForSequenceClassification, AutoTokenizer
    import torch

    reward_name = "OpenAssistant/reward-model-deberta-v3-large-v2"
    rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(
        reward_name
    ), AutoTokenizer.from_pretrained(reward_name)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rank_model.to(device)

    def get_question(x):
        return x.replace("<human>: ", "").split("<bot>:")[0]

    def get_answer(x):
        try:
            answer = (
                x.split("<bot>: ")[1]
                .split("<human>:")[0]
                .replace("<bot>: ", "")
            )
        except:
            answer = (
                x.split("<bot>:")[1].split("<human>:")[0].replace("<bot>:", "")
            )
        return answer

    df["question"] = parallel_apply(df["text"], get_question, n_jobs=-1)
    df["answer"] = parallel_apply(df["text"], get_answer, n_jobs=-1)

    from datasets import Dataset
    from transformers import pipeline
    from transformers.pipelines.pt_utils import KeyPairDataset
    import tqdm

    pipe = pipeline(
        "text-classification",
        model=reward_name,
        device="cuda:0" if torch.cuda.is_available() else "cpu",
    )
    start = 0
    batch_size = 64 * 16
    micro_batch = orig_micro_batch = 16
    end = 0
    import socket

    checkpoint = "grades.%s.pkl" % socket.gethostname()
    grades = []
    import pickle

    if os.path.exists(checkpoint):
        with open(checkpoint, "rb") as f:
            start, grades = pickle.loads(f.read())
    last_oom = 0
    while end < df.shape[0]:
        # manual batching to handle OOM more gracefully
        end = min(start + batch_size, df.shape[0])
        if start == end:
            break
        dataset = Dataset.from_pandas(df.iloc[start:end, :])
        try:
            grades.extend(
                [
                    x["score"]
                    for x in tqdm.tqdm(
                        pipe(
                            KeyPairDataset(dataset, "question", "answer"),
                            batch_size=micro_batch,
                        )
                    )
                ]
            )
        except torch.cuda.OutOfMemoryError:
            last_oom = start
            micro_batch = max(1, micro_batch // 2)
            print("OOM - retrying with micro_batch=%d" % micro_batch)
            continue
        if last_oom == start:
            micro_batch = orig_micro_batch
            print("Returning to micro_batch=%d" % micro_batch)
        assert len(grades) == end
        start = end
        with open(checkpoint, "wb") as f:
            f.write(pickle.dumps((end, grades)))
        print("%d/%d" % (end, df.shape[0]))
    df["grade_deberta"] = grades
    if os.path.exists(checkpoint):
        os.remove(checkpoint)
    return df


def test_chop_by_lengths():
    file = "h2oGPT.cleaned.human_bot.shorter.parquet"
    df = pd.read_parquet(file).reset_index(drop=True)
    df = count_human_bot_lengths(df)
    df["rand"] = np.random.rand(df.shape[0])
    df["rand2"] = np.random.rand(df.shape[0])
    before_rows = df.shape[0]
    # throw away short human/bot responses with higher likelihood
    df = df[(df["len_human_mean"] > 20)]  # never keep very short ones
    df = df[(df["len_human_mean"] > 30) | (df["rand"] < 0.2)]
    df = df[(df["len_human_mean"] > 50) | (df["rand"] < 0.5)]
    df = df[
        (df["len_human_max"] < 10000)
    ]  # drop super long (basically only human) ones
    df = df[(df["len_bot_mean"] > 20)]  # never keep very short ones
    df = df[(df["len_bot_mean"] > 30) | (df["rand2"] < 0.2)]
    df = df[(df["len_bot_mean"] > 50) | (df["rand2"] < 0.5)]
    df = df[(df["len_bot_max"] < 10000)]  # drop super long (only bot) ones
    assert df["text"].apply(lambda x: len(x)).max() < 20000
    df = df.drop(["rand", "rand2"], axis=1)
    after_rows = df.shape[0]
    print(
        "Chopped off %d out of %d rows due to length"
        % (before_rows - after_rows, before_rows)
    )
    print(df.describe())
    df.to_parquet(
        "h2oGPT.cleaned.chopped.human_bot.shorter.parquet", index=False
    )


def count_human_bot_lengths(df, human=None, bot=None):
    import re

    len_human_min = []
    len_human_max = []
    len_human_mean = []
    len_bot_min = []
    len_bot_max = []
    len_bot_mean = []
    human = human or "<human>:"
    bot = bot or "<bot>:"
    for is_human in [True, False]:
        what = human if is_human else bot
        other = human if not is_human else bot
        for i in range(df.shape[0]):
            text = df.loc[i, "text"]
            assert isinstance(text, str)
            starts = [m.start() for m in re.finditer(what, text)]
            if len(starts) == 1:
                starts = [
                    starts[0],
                    len(text),
                ]  # always go into for loop below
            assert len(text)
            list_what = []
            for ii in range(len(starts) - 1):
                interaction = text[starts[ii] : starts[ii + 1]]
                if other in interaction:
                    interaction = interaction[: interaction.find(other)]
                interaction.strip()
                list_what.append(interaction)
            if not list_what:
                list_what = [
                    ""
                ]  # handle corrupted data, very rare, leads to sizes 0
            if is_human:
                len_human_min.append(min([len(x) for x in list_what]))
                len_human_max.append(max([len(x) for x in list_what]))
                len_human_mean.append(np.mean([len(x) for x in list_what]))
            else:
                len_bot_min.append(min([len(x) for x in list_what]))
                len_bot_max.append(max([len(x) for x in list_what]))
                len_bot_mean.append(np.mean([len(x) for x in list_what]))
    df["len_human_min"] = len_human_min
    df["len_human_max"] = len_human_max
    df["len_human_mean"] = len_human_mean
    df["len_bot_min"] = len_bot_min
    df["len_bot_max"] = len_bot_max
    df["len_bot_mean"] = len_bot_mean
    np.random.seed(1234)
    pd.set_option("display.max_columns", None)
    print("Before chopping")
    print(df.describe())
    return df


def test_grade():
    df = None

    file = "h2oGPT.cleaned.chopped.human_bot.shorter.parquet"
    output_file = "h2oGPT.cleaned.graded1.human_bot.shorter.parquet"
    if not os.path.exists(output_file):
        if df is None:
            df = pd.read_parquet(file).reset_index(drop=True)
        df = add_textstat_grade(df)
        min_grade = 10
        max_grade = 25
        df = df[df["flesch_grade"] >= min_grade]
        df = df[df["flesch_grade"] <= max_grade]
        print("After Flesch grade")
        print(df.describe())
        df.to_parquet(output_file, index=False)

    file = output_file
    output_file = "h2oGPT.cleaned.graded2.human_bot.shorter.parquet"
    if not os.path.exists(output_file):
        # slower than alt-profanity, do last, but do before deberta grading, since that's slower
        if df is None:
            df = pd.read_parquet(file).reset_index(drop=True)
        df = add_better_profanity_flag(df)
        before_rows = df.shape[0]
        df = df[df["better_profanity"] == 0]
        df = df.drop(["better_profanity"], axis=1)
        after_rows = df.shape[0]
        print(
            "Dropped %d rows out of %d due to better_profanity"
            % (before_rows - after_rows, before_rows)
        )
        print(df.describe())
        df.to_parquet(output_file, index=False)

    file = output_file
    output_file = "h2oGPT.cleaned.graded3.human_bot.shorter.parquet"
    if not os.path.exists(output_file):
        if df is None:
            df = pd.read_parquet(file).reset_index(drop=True)
        df = add_deberta_grade(df)
        min_grade = 0.3
        max_grade = np.inf
        before_rows = df.shape[0]
        df = df[df["grade_deberta"] >= min_grade]
        df = df[df["grade_deberta"] <= max_grade]
        after_rows = df.shape[0]
        print(
            "Dropped %d rows out of %d due to deberta grade"
            % (before_rows - after_rows, before_rows)
        )
        print("After DeBERTa grade")
        print(df.describe())
        df.to_parquet(output_file, index=False)

    file = output_file
    output_file = "h2oGPT.cleaned.graded.human_bot.shorter.parquet"
    if df is None:
        df = pd.read_parquet(file).reset_index(drop=True)
    df.to_parquet(output_file, index=False)


@pytest.mark.parametrize(
    "fixup_personality, only_personality, deberta_grading",
    [
        [False, False, False],
        [True, True, False],
        [True, False, False],
        [True, False, True],
    ],
)
def test_add_open_assistant(
    fixup_personality, only_personality, deberta_grading, save_json=True
):
    """
    Flatten tree structure into one row per path from root to leaf
    Also turn into human_bot prompting format:
        <human>: question\n<bot>: answer <human>: question2\n<bot>: answer2 Etc.
    Also saves a .json locally as side-effect
    returns list of dicts, containing intput, prompt_type and source
    """
    from datasets import load_dataset

    data_file = "OpenAssistant/oasst1"
    ds = load_dataset(data_file)
    df = pd.concat(
        [ds["train"].to_pandas(), ds["validation"].to_pandas()], axis=0
    )
    rows = {}
    message_ids = df["message_id"].values.tolist()
    message_tree_ids = df["message_tree_id"].values.tolist()
    parent_ids = df["parent_id"].values.tolist()
    texts = df["text"].values.tolist()
    roles = df["role"].values.tolist()

    for i in range(df.shape[0]):
        # collect all trees
        message_id = message_ids[i]
        message_tree_id = message_tree_ids[i]
        parent_id = parent_ids[i]
        text = texts[i]
        if fixup_personality:
            text = text.replace("Open Assistant", "h2oGPT")
            text = text.replace("Open-Assistant", "h2oGPT")
            text = text.replace("open-assistant", "h2oGPT")
            text = text.replace("OpenAssistant", "h2oGPT")
            text = text.replace("open assistant", "h2oGPT")
            text = text.replace("Open Assistand", "h2oGPT")
            text = text.replace("Open Assitant", "h2oGPT")
            text = text.replace("Open Assistent", "h2oGPT")
            text = text.replace("Open Assisstant", "h2oGPT")
            text = text.replace("Open Assitent", "h2oGPT")
            text = text.replace("Open Assitiant", "h2oGPT")
            text = text.replace("Open Assistiant", "h2oGPT")
            text = text.replace("Open Assitan ", "h2oGPT ")
            text = text.replace("Open Assistan ", "h2oGPT ")
            text = text.replace("Open Asistant", "h2oGPT")
            text = text.replace("Open Assiant", "h2oGPT")
            text = text.replace("Assistant", "h2oGPT")
            text = text.replace("LAION AI", "H2O.ai")
            text = text.replace("LAION-AI", "H2O.ai")
            text = text.replace("LAION,", "H2O.ai,")
            text = text.replace("LAION.ai", "H2O.ai")
            text = text.replace("LAION.", "H2O.ai.")
            text = text.replace("LAION", "H2O.ai")

        role = roles[i]
        new_data = ("<human>: " if role == "prompter" else "<bot>: ") + text
        entry = dict(message_id=message_id, parent_id=parent_id, text=new_data)
        if message_tree_id not in rows:
            rows[message_tree_id] = [entry]
        else:
            rows[message_tree_id].append(entry)

    all_rows = []

    for node_id in rows:
        # order responses in tree, based on message/parent relationship
        conversations = []

        list_msgs = rows[node_id]
        # find start
        while len(list_msgs):
            for i, leaf in enumerate(list_msgs):
                found = False
                parent_id = leaf["parent_id"]
                if parent_id is None:
                    # conversation starter
                    conversations.append(leaf)
                    found = True
                else:
                    for conv in conversations:
                        # find all conversations to add my message to
                        if (
                            parent_id in conv["message_id"]
                            and parent_id
                            != conv["message_id"][-len(parent_id) :]
                        ):
                            # my message doesn't follow conversation
                            continue
                        if parent_id == conv["message_id"][-len(parent_id) :]:
                            # my message follows conversation, but fork first, so another follow-on message can do same
                            conversations.append(conv.copy())
                            conv[
                                "text"
                            ] += f"""
{leaf['text']}
"""
                            conv["message_id"] += leaf["message_id"]
                            found = True
                            break
                if found:
                    # my content was used, so nuke from list
                    del list_msgs[i]
                    break

        # now reduce down to final conversations, find the longest chains of message ids
        for i, conv in enumerate(conversations):
            for j, conv2 in enumerate(conversations):
                if i == j:
                    continue
                if conv["message_id"] and conv2["message_id"]:
                    assert conv["message_id"] != conv2["message_id"]
                    # delete the shorter conversation, if one contains the other
                    if conv["message_id"] in conv2["message_id"]:
                        conv["message_id"] = None
                    if conv2["message_id"] in conv["message_id"]:
                        conv2["message_id"] = None
        conversations = [c for c in conversations if c["message_id"]]
        if only_personality:
            all_rows.extend(
                [
                    dict(
                        input=c["text"] + "\n<human>:",
                        prompt_type="plain",
                        source=data_file,
                    )
                    for c in conversations
                    if "h2oGPT" in c["text"]
                ]
            )
        else:
            all_rows.extend(
                [
                    dict(
                        input=c["text"] + "\n<human>:",
                        prompt_type="plain",
                        source=data_file,
                    )
                    for c in conversations
                    if "What is H2O.ai" not in c["text"]
                ]
            )
    unhelpful = get_unhelpful_list()
    all_rows = [
        x for x in all_rows if not any(u in x["input"] for u in unhelpful)
    ]
    personality = create_personality_data()
    all_rows.extend(personality * 10)
    np.random.seed(123)
    np.random.shuffle(all_rows)
    print(len(all_rows))
    if deberta_grading:
        df = pd.DataFrame(all_rows)
        df = df.rename(columns={"input": "text"})
        df = add_deberta_grade(df)
        df = df.rename(columns={"text": "input"})
        drop = True
        if drop:
            min_grade = 0.3
            max_grade = np.inf
            before_rows = df.shape[0]
            df = df[df["grade_deberta"] >= min_grade]
            df = df[df["grade_deberta"] <= max_grade]
            after_rows = df.shape[0]
            print(
                "Dropped %d rows out of %d due to deberta grade"
                % (before_rows - after_rows, before_rows)
            )
            print("After DeBERTa grade")
        print(df.describe())
        all_rows = []
        for i in range(df.shape[0]):
            all_rows.append(
                dict(
                    input=df["input"].iloc[i],
                    source=df["source"].iloc[i],
                    prompt_type=df["prompt_type"].iloc[i],
                    grade_deberta=df["grade_deberta"].iloc[i],
                )
            )
    if save_json:
        data_file = (
            data_file
            + ("_h2ogpt" if fixup_personality else "")
            + ("_only" if only_personality else "")
            + ("_graded" if deberta_grading else "")
        )
        for i in range(len(all_rows)):
            all_rows[i]["id"] = i
        with open(data_file.lower().replace("/", "_") + ".json", "w") as f:
            f.write(json.dumps(all_rows, indent=2))
    return all_rows


def test_finalize_to_json():
    df = pd.read_parquet("h2oGPT.cleaned.graded.human_bot.shorter.parquet")
    df = df.rename(columns={"text": "input"})

    print(
        "Number of high-quality human_bot interactions: %s" % df.shape[0],
        flush=True,
    )

    print("Adding open assistant data")
    with open("openassistant_oasst1_h2ogpt_graded.json") as f:
        open_assistant = json.loads(f.read())
    df = pd.concat([df, pd.DataFrame(open_assistant)], axis=0)

    def final_clean(df):
        from better_profanity import profanity

        profanity.load_censor_words_from_file("data/censor_words.txt")
        df["profanity"] = parallel_apply(
            df["input"],
            lambda x: profanity.contains_profanity(x),
            n_jobs=-1,
        )
        return df[(df["profanity"] == 0)].reset_index(drop=True)

    print(
        "Before cleaning: Number of final high-quality human_bot interactions: %s"
        % df.shape[0],
        flush=True,
    )
    df = final_clean(df)
    print(
        "After cleaning: Number of final high-quality human_bot interactions: %s"
        % df.shape[0],
        flush=True,
    )
    print(df.describe())
    print(df.shape)
    row_list = []
    for i in range(df.shape[0]):
        row_list.append(
            dict(
                input=df.loc[i, "input"],
                source=df.loc[i, "source"],
                prompt_type="plain",
            )
        )
    np.random.seed(1234)
    np.random.shuffle(row_list)
    unhelpful = get_unhelpful_list()
    row_list = [
        x for x in row_list if not any(u in x["input"] for u in unhelpful)
    ]
    for i in range(len(row_list)):
        row_list[i]["id"] = i
        row_list[i]["input"] = row_list[i]["input"].replace(
            " <bot>:", "\n<bot>:"
        )
    with open("h2ogpt-oig-oasst1-instruct-cleaned-v3.json", "w") as f:
        f.write(json.dumps(row_list, indent=2))


def create_personality_data():
    questions = [
        "What's your name?",
        "What is your name?",
        "What are you?",
        "Who are you?",
        "Do you have a name?",
        "Who trained you?",
        "Who created you?",
        "Who made you?",
    ]
    answers = [
        "I'm h2oGPT, a large language model by H2O.ai.",
        "I'm h2oGPT, a large language model by H2O.ai, the visionary leader in democratizing AI.",
        "My name is h2oGPT. I'm a large language model by H2O.ai, the visionary leader in democratizing AI.",
        "My name is h2oGPT. I'm a large language model trained by H2O.ai.",
        "Hi! I'm h2oGPT, a large language model by H2O.ai.",
        "Hi! I'm h2oGPT, a large language model by H2O.ai, the visionary leader in democratizing AI.",
    ]
    help = [
        "",
        " How can I help you?",
        " How may I assist you?",
        " Nice to meet you.",
    ]
    import itertools

    rows = []
    for pair in itertools.product(questions, answers, help):
        rows.append(
            dict(
                input=f"<human>: {pair[0]}\n<bot>: {pair[1]}{pair[2]}\n<human>:",
                prompt_type="plain",
                source="H2O.ai",
            )
        )
    for row in [
        "<human>: What is H2O.ai?\n<bot>: H2O.ai is a technology company that aims to democratize AI and make it accessible to a broader audience by simplifying the process of creating and deploying machine learning models.\n<human>:",
        "<human>: What is h2o.ai?\n<bot>: H2O.ai is a technology company that aims to democratize AI and make it accessible to a broader audience by simplifying the process of creating and deploying machine learning models.\n<human>:",
        "<human>: What is H2O?\n<bot>: H2O.ai is a technology company that aims to democratize AI and make it accessible to a broader audience by simplifying the process of creating and deploying machine learning models.\n<human>:",
        "<human>: Who is h2o.ai?\n<bot>: H2O.ai is a technology company that aims to democratize AI and make it accessible to a broader audience by simplifying the process of creating and deploying machine learning models.\n<human>:",
        "<human>: who is h2o.ai?\n<bot>: H2O.ai is a technology company that aims to democratize AI and make it accessible to a broader audience by simplifying the process of creating and deploying machine learning models.\n<human>:",
        "<human>: who is h2o?\n<bot>: H2O.ai is a technology company that aims to democratize AI and make it accessible to a broader audience by simplifying the process of creating and deploying machine learning models.\n<human>:",
        "<human>: What is H2O.ai?\n<bot>: H2O.ai is the visionary leader in democratizing AI.\n<human>:",
        "<human>: Who is H2O.ai?\n<bot>: H2O.ai is the visionary leader in democratizing AI.\n<human>:",
        "<human>: Who is H2O?\n<bot>: H2O.ai is the visionary leader in democratizing AI.\n<human>:",
        "<human>: Who is h2o?\n<bot>: H2O.ai is the visionary leader in democratizing AI.\n<human>:",
        "<human>: who is h2o?\n<bot>: H2O.ai is the visionary leader in democratizing AI.\n<human>:",
    ]:
        rows.append(dict(input=row, prompt_type="plain", source="H2O.ai"))
    print(len(rows))
    with open("h2ogpt-personality.json", "w") as f:
        f.write(json.dumps(rows, indent=2))
    return rows


def test_check_stats_data():
    filename = "h2ogpt-oig-oasst1-instruct-cleaned-v3.json"
    df = pd.read_json(filename)

    # get word stats
    df["char_count"] = df["input"].apply(lambda x: len(x))
    import matplotlib.pyplot as plt

    plt.figure(figsize=(10, 10))
    plt.hist(df["char_count"], bins=100)
    chars_avg = np.mean(df["char_count"])
    chars_median = np.median(df["char_count"])
    plt.title("char_count avg: %s median: %s" % (chars_avg, chars_median))
    plt.savefig("chars_hist.png")
    plt.close()

    # get tokenize stats for random sample of 1000 rows
    from finetune import generate_and_tokenize_prompt
    from loaders import get_loaders, get_tokenizer
    from functools import partial

    llama_type = False
    tokenizer_base_model = base_model = "h2oai/h2ogpt-oasst1-512-20b"
    model_loader, tokenizer_loader = get_loaders(
        model_name=base_model, reward_type=False, llama_type=llama_type
    )
    local_files_only = False
    resume_download = True
    use_auth_token = False
    tokenizer = get_tokenizer(
        tokenizer_loader,
        tokenizer_base_model,
        local_files_only,
        resume_download,
        use_auth_token,
    )
    prompt_type = "plain"  # trained with data already in human bot form
    train_on_inputs = True
    add_eos_token = False
    cutoff_len = 512  # can choose 2048
    generate_and_tokenize_prompt_fun = partial(
        generate_and_tokenize_prompt,
        prompt_type=prompt_type,
        train_on_inputs=train_on_inputs,
        add_eos_token=add_eos_token,
        cutoff_len=cutoff_len,
        tokenizer=tokenizer,
    )
    from datasets import load_dataset

    data = load_dataset("json", data_files={"train": filename})
    val_set_size = 0.90
    train_val = data["train"].train_test_split(
        test_size=val_set_size, shuffle=True, seed=42
    )
    train_data = train_val["train"]
    train_data = train_data.shuffle().map(
        generate_and_tokenize_prompt_fun, num_proc=os.cpu_count()
    )

    df_tokens = pd.DataFrame(
        [len(x) for x in train_data["input_ids"]], columns=["token_count"]
    )

    plt.figure(figsize=(10, 10))
    plt.hist(df_tokens["token_count"], bins=100)
    token_avg = np.mean(df_tokens["token_count"])
    token_median = np.median(df_tokens["token_count"])
    plt.title(
        "token_count with cutoff=%s avg: %s median: %s"
        % (cutoff_len, token_avg, token_median)
    )
    plt.savefig("token_hist_%s.png" % cutoff_len)
    plt.close()


def get_unhelpful_list():
    # base versions
    unhelpful = [
        "I'm sorry, I didn't quite understand your question, could you please rephrase it?",
        "I'm sorry, but I don't understand your question. Could you please rephrase it?",
        "I'm sorry, I don't quite understand your question",
        "I'm sorry, I don't know",
        "I'm sorry, but I don't know",
        "I don't know anything",
        "I do not know",
        "I don't know",
        "I don't know how",
        "I do not know how",
        "Can you please explain what you mean",
        "please explain what you mean",
        "please explain",
        "I'm sorry, but I don't know how to tell a story. Can you please explain what you mean by",
        "I'm sorry but I don't understand what you mean",
        "I don't understand",
        "I don't have the ability",
        "I do not have the ability",
        "I do not have",
        "I am a language model,",
        "I am a large language model,",
        "I do not understand your question. Can you please try to make it clearer?",
        "I'm sorry, but as an AI language model",
        "I apologize, but I cannot rephrase text that I cannot understand. Your post is difficult to read and follow.",
        "I apologize, but I am not h2oGPT. I am a language model developed by H2O.ai. How may I help you?",
        "Sorry, but I am not an actual Linux shell, nor am I capable of emulating one. I am an open source chat assistant and would be glad t",
        "I apologize, but I cannot perform the task you have requested.",
        "I'm sorry, I cannot perform this task as I am an AI language model and do not have access",
        "I'm sorry, I'm not sure what you're asking for here.",
        "I'm not sure what you are asking",
        "You need to provide more context",
    ]
    # reduced versions, with redundant parts, just to give context for where they came from
    unhelpful += [
        "sorry, I didn't quite understand your question",
        "I didn't quite understand your question",
        "I didn't understand your question",
        "I did not understand your question",
        "I did not understand the question",
        "could you please rephrase"
        "could you rephrase"
        "I do not understand your question.",
        "I do not understand the question.",
        "I do not understand that question.",
        "Can you please try to make it clearer",
        "Can you try to make it clearer",
        "sorry, but as an AI language model",
        "as an AI language model",
        "I apologize, but I cannot",
        "I cannot rephrase text",
        "I cannot understand. Your post is difficult to read and follow."
        "Your post is difficult to read and follow."
        "I apologize, but I am",
        "Sorry, but I am not ",
        "nor am I capable",
        "I am not capable of",
        "I apologize, but I cannot perform the task you have requested",
        "I cannot perform the task",
        "I cannot complete the task",
        "I'm sorry",
        "I am sorry",
        "do not have access",
        "not sure what you're asking for",
        "not sure what you are asking for",
        "not sure what is being asked",
        "I'm not sure what you are asking",
        "not sure what you are asking",
        "You need to provide more context",
        "provide more context",
    ]
    unhelpful += [
        "As a large language model",
        "cannot provide any information",
        "As an artificial intelligence I do not have the capability",
        "As an artificial intelligence I don't have the capability",
        "As an artificial intelligence I can't",
        "As an artificial intelligence I cannot",
        "I am sorry but I do not understand",
        "Can you please explain",
        "(sorry couldn't resist)",
        "(sorry could not resist)",
        " :)",
        " ;)",
        " :-)",
        " ;-)",
        " lol ",
        "Thanks so much!!!",
        "Thank You :)!!!",
        "Please try not to repeat",
        "I am an AI language model",
        "I'm a AI assistant that",
        "I'm an AI assistant that",
        "I am an AI assistant that",
        "etc.",
        "etc.etc.",
        "etc. etc.",
        "etc etc",
    ]
    return unhelpful


def test_check_unhelpful():
    # file = '/home/jon/Downloads/openassistant_oasst1_h2ogpt_graded.json'
    file = "/home/jon/Downloads/openassistant_oasst1_h2ogpt_grades.json"
    # file = 'h2ogpt-oig-oasst1-instruct-cleaned-v2.json'

    unhelpful = get_unhelpful_list()
    # data = json.load(open(file, 'rt'))
    df = pd.read_json(file)

    use_reward_score_threshold = False
    use_bleu_threshold = False
    use_sentence_sim = True

    from sacrebleu.metrics import BLEU

    bleu = BLEU()
    from nltk.translate.bleu_score import sentence_bleu

    def get_bleu(actual, expected_list):
        # return bleu.sentence_score(actual, expected_list).score
        return sentence_bleu(expected_list, actual)

    threshold = 0.0
    if use_reward_score_threshold:
        df = df[df["grade_deberta"] > threshold]

    # back to as if original json load
    data = df.to_dict(orient="records")
    bads = {}
    string_all = str(data)
    for sub in unhelpful:
        bads[sub] = string_all.count(sub)
    bads = {k: v for k, v in bads.items() if v > 0}
    import pprint

    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(bads)

    total_bads = sum(list(bads.values()))
    print("total_bads: %s" % total_bads, flush=True)

    # check just bot
    import re

    convs = [
        [
            x.strip()
            for x in re.split(r"%s|%s" % (human, bot), y["input"])
            if x.strip()
        ]
        for y in data
    ]
    humans = [[x for i, x in enumerate(y) if i % 2 == 0] for y in convs]
    bots = [[x for i, x in enumerate(y) if i % 2 == 1] for y in convs]

    # FIXME: apply back to json etc., just see for now
    bleu_threshold = 0.9
    if use_bleu_threshold:
        bots = [
            [x for x in y if get_bleu(x, unhelpful) < bleu_threshold]
            for y in tqdm(bots)
        ]

    cosine_sim_threshold = 0.8
    if use_sentence_sim:
        # pip install sentence_transformers-2.2.2
        from sentence_transformers import SentenceTransformer

        # sent_model = 'bert-base-nli-mean-tokens'
        # sent_model = 'nli-distilroberta-base-v2'
        sent_model = "all-MiniLM-L6-v2"
        model = SentenceTransformer(sent_model)
        sentence_embeddings = model.encode(unhelpful)
        from sklearn.metrics.pairwise import cosine_similarity

        bots = [
            x
            for x in tqdm(bots)
            if np.max(cosine_similarity(model.encode(x), sentence_embeddings))
            < cosine_sim_threshold
        ]

    bads_bots = {}
    string_all = str(bots)
    for sub in unhelpful:
        bads_bots[sub] = string_all.count(sub)
    bads_bots = {k: v for k, v in bads_bots.items() if v > 0}
    import pprint

    pp = pprint.PrettyPrinter(indent=4)
    pp.pprint(bads_bots)

    total_bads_bots = sum(list(bads_bots.values()))
    print(
        "threshold: %g use_bleu_threshold: %g total_bads_bots: %s total_bots: %s total_humans: %s"
        % (
            threshold,
            use_bleu_threshold,
            total_bads_bots,
            len(bots),
            len(humans),
        ),
        flush=True,
    )

    # assert len(bads) == 0, bads
    assert len(bads_bots) == 0, bads_bots


def test_fortune2000_personalized():
    row_list = []
    import glob

    if not os.path.isdir("wikitext"):
        raise RuntimeError(
            "download https://github.com/h2oai/h2ogpt/files/11423008/wikitext.zip and unzip"
        )
    for file in glob.glob("wikitext/*.txt"):
        with open(file, "r") as f:
            blob = f.read()
        N = 512 * 4
        row_list.extend(
            [
                {
                    "input": s,
                    "prompt_type": "plain",
                    "source": "%s" % os.path.basename(file),
                }
                for s in get_sentences(blob, N)
                if s
            ]
        )
    personality = create_personality_data()
    import copy

    for i in range(10):
        row_list.extend(copy.deepcopy(personality))
    np.random.seed(123)
    np.random.shuffle(row_list)
    for i in range(len(row_list)):
        row_list[i]["id"] = i
    for i in range(len(row_list)):
        assert row_list[i]["id"] == i
    with open("h2ogpt-fortune2000-personalized.json", "w") as ff:
        ff.write(json.dumps(row_list, indent=2))

```

`apps/language_models/langchain/enums.py`:

```py
from enum import Enum


class PromptType(Enum):
    custom = -1
    plain = 0
    instruct = 1
    quality = 2
    human_bot = 3
    dai_faq = 4
    summarize = 5
    simple_instruct = 6
    instruct_vicuna = 7
    instruct_with_end = 8
    human_bot_orig = 9
    prompt_answer = 10
    open_assistant = 11
    wizard_lm = 12
    wizard_mega = 13
    instruct_vicuna2 = 14
    instruct_vicuna3 = 15
    wizard2 = 16
    wizard3 = 17
    instruct_simple = 18
    wizard_vicuna = 19
    openai = 20
    openai_chat = 21
    gptj = 22
    prompt_answer_openllama = 23
    vicuna11 = 24
    mptinstruct = 25
    mptchat = 26
    falcon = 27


class DocumentChoices(Enum):
    All_Relevant = 0
    All_Relevant_Only_Sources = 1
    Only_All_Sources = 2
    Just_LLM = 3


non_query_commands = [
    DocumentChoices.All_Relevant_Only_Sources.name,
    DocumentChoices.Only_All_Sources.name,
]


class LangChainMode(Enum):
    """LangChain mode"""

    DISABLED = "Disabled"
    CHAT_LLM = "ChatLLM"
    LLM = "LLM"
    ALL = "All"
    WIKI = "wiki"
    WIKI_FULL = "wiki_full"
    USER_DATA = "UserData"
    MY_DATA = "MyData"
    GITHUB_H2OGPT = "github h2oGPT"
    H2O_DAI_DOCS = "DriverlessAI docs"


class LangChainAction(Enum):
    """LangChain action"""

    QUERY = "Query"
    # WIP:
    # SUMMARIZE_MAP = "Summarize_map_reduce"
    SUMMARIZE_MAP = "Summarize"
    SUMMARIZE_ALL = "Summarize_all"
    SUMMARIZE_REFINE = "Summarize_refine"


no_server_str = no_lora_str = no_model_str = "[None/Remove]"

# from site-packages/langchain/llms/openai.py
# but needed since ChatOpenAI doesn't have this information
model_token_mapping = {
    "gpt-4": 8192,
    "gpt-4-0314": 8192,
    "gpt-4-32k": 32768,
    "gpt-4-32k-0314": 32768,
    "gpt-3.5-turbo": 4096,
    "gpt-3.5-turbo-16k": 16 * 1024,
    "gpt-3.5-turbo-0301": 4096,
    "text-ada-001": 2049,
    "ada": 2049,
    "text-babbage-001": 2040,
    "babbage": 2049,
    "text-curie-001": 2049,
    "curie": 2049,
    "davinci": 2049,
    "text-davinci-003": 4097,
    "text-davinci-002": 4097,
    "code-davinci-002": 8001,
    "code-davinci-001": 8001,
    "code-cushman-002": 2048,
    "code-cushman-001": 2048,
}

source_prefix = "Sources [Score | Link]:"
source_postfix = "End Sources<p>"

```

`apps/language_models/langchain/evaluate_params.py`:

```py
no_default_param_names = [
    "instruction",
    "iinput",
    "context",
    "instruction_nochat",
    "iinput_nochat",
]

gen_hyper = [
    "temperature",
    "top_p",
    "top_k",
    "num_beams",
    "max_new_tokens",
    "min_new_tokens",
    "early_stopping",
    "max_time",
    "repetition_penalty",
    "num_return_sequences",
    "do_sample",
]

eval_func_param_names = (
    [
        "instruction",
        "iinput",
        "context",
        "stream_output",
        "prompt_type",
        "prompt_dict",
    ]
    + gen_hyper
    + [
        "chat",
        "instruction_nochat",
        "iinput_nochat",
        "langchain_mode",
        "langchain_action",
        "top_k_docs",
        "chunk",
        "chunk_size",
        "document_choice",
    ]
)

# form evaluate defaults for submit_nochat_api
eval_func_param_names_defaults = eval_func_param_names.copy()
for k in no_default_param_names:
    if k in eval_func_param_names_defaults:
        eval_func_param_names_defaults.remove(k)


eval_extra_columns = ["prompt", "response", "score"]

```

`apps/language_models/langchain/expanded_pipelines.py`:

```py
from __future__ import annotations
from typing import (
    Any,
    Mapping,
    Optional,
    Dict,
    List,
    Sequence,
    Tuple,
    Union,
    Protocol,
)
import inspect
import json
import warnings
from pathlib import Path
import yaml
from abc import ABC, abstractmethod
import langchain
from langchain.base_language import BaseLanguageModel
from langchain.callbacks.base import BaseCallbackManager
from langchain.chains.question_answering import stuff_prompt
from langchain.prompts.base import BasePromptTemplate
from langchain.docstore.document import Document
from langchain.callbacks.manager import (
    CallbackManager,
    CallbackManagerForChainRun,
    Callbacks,
)
from langchain.load.serializable import Serializable
from langchain.schema import RUN_KEY, BaseMemory, RunInfo
from langchain.input import get_colored_text
from langchain.load.dump import dumpd
from langchain.prompts.prompt import PromptTemplate
from langchain.schema import LLMResult, PromptValue
from pydantic import Extra, Field, root_validator, validator


def _get_verbosity() -> bool:
    return langchain.verbose


def format_document(doc: Document, prompt: BasePromptTemplate) -> str:
    """Format a document into a string based on a prompt template."""
    base_info = {"page_content": doc.page_content}
    base_info.update(doc.metadata)
    missing_metadata = set(prompt.input_variables).difference(base_info)
    if len(missing_metadata) > 0:
        required_metadata = [
            iv for iv in prompt.input_variables if iv != "page_content"
        ]
        raise ValueError(
            f"Document prompt requires documents to have metadata variables: "
            f"{required_metadata}. Received document with missing metadata: "
            f"{list(missing_metadata)}."
        )
    document_info = {k: base_info[k] for k in prompt.input_variables}
    return prompt.format(**document_info)


class Chain(Serializable, ABC):
    """Base interface that all chains should implement."""

    memory: Optional[BaseMemory] = None
    callbacks: Callbacks = Field(default=None, exclude=True)
    callback_manager: Optional[BaseCallbackManager] = Field(
        default=None, exclude=True
    )
    verbose: bool = Field(
        default_factory=_get_verbosity
    )  # Whether to print the response text
    tags: Optional[List[str]] = None

    class Config:
        """Configuration for this pydantic object."""

        arbitrary_types_allowed = True

    @property
    def _chain_type(self) -> str:
        raise NotImplementedError("Saving not supported for this chain type.")

    @root_validator()
    def raise_deprecation(cls, values: Dict) -> Dict:
        """Raise deprecation warning if callback_manager is used."""
        if values.get("callback_manager") is not None:
            warnings.warn(
                "callback_manager is deprecated. Please use callbacks instead.",
                DeprecationWarning,
            )
            values["callbacks"] = values.pop("callback_manager", None)
        return values

    @validator("verbose", pre=True, always=True)
    def set_verbose(cls, verbose: Optional[bool]) -> bool:
        """If verbose is None, set it.

        This allows users to pass in None as verbose to access the global setting.
        """
        if verbose is None:
            return _get_verbosity()
        else:
            return verbose

    @property
    @abstractmethod
    def input_keys(self) -> List[str]:
        """Input keys this chain expects."""

    @property
    @abstractmethod
    def output_keys(self) -> List[str]:
        """Output keys this chain expects."""

    def _validate_inputs(self, inputs: Dict[str, Any]) -> None:
        """Check that all inputs are present."""
        missing_keys = set(self.input_keys).difference(inputs)
        if missing_keys:
            raise ValueError(f"Missing some input keys: {missing_keys}")

    def _validate_outputs(self, outputs: Dict[str, Any]) -> None:
        missing_keys = set(self.output_keys).difference(outputs)
        if missing_keys:
            raise ValueError(f"Missing some output keys: {missing_keys}")

    @abstractmethod
    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        """Run the logic of this chain and return the output."""

    def __call__(
        self,
        inputs: Union[Dict[str, Any], Any],
        return_only_outputs: bool = False,
        callbacks: Callbacks = None,
        *,
        tags: Optional[List[str]] = None,
        include_run_info: bool = False,
    ) -> Dict[str, Any]:
        """Run the logic of this chain and add to output if desired.

        Args:
            inputs: Dictionary of inputs, or single input if chain expects
                only one param.
            return_only_outputs: boolean for whether to return only outputs in the
                response. If True, only new keys generated by this chain will be
                returned. If False, both input keys and new keys generated by this
                chain will be returned. Defaults to False.
            callbacks: Callbacks to use for this chain run. If not provided, will
                use the callbacks provided to the chain.
            include_run_info: Whether to include run info in the response. Defaults
                to False.
        """
        input_docs = inputs["input_documents"]
        missing_keys = set(self.input_keys).difference(inputs)
        if missing_keys:
            raise ValueError(f"Missing some input keys: {missing_keys}")

        callback_manager = CallbackManager.configure(
            callbacks, self.callbacks, self.verbose, tags, self.tags
        )
        run_manager = callback_manager.on_chain_start(
            dumpd(self),
            inputs,
        )

        if "is_first" in inputs.keys() and not inputs["is_first"]:
            run_manager_ = run_manager
            input_list = [inputs]
            stop = None
            prompts = []
            for inputs in input_list:
                selected_inputs = {
                    k: inputs[k] for k in self.prompt.input_variables
                }
                prompt = self.prompt.format_prompt(**selected_inputs)
                _colored_text = get_colored_text(prompt.to_string(), "green")
                _text = "Prompt after formatting:\n" + _colored_text
                if run_manager_:
                    run_manager_.on_text(_text, end="\n", verbose=self.verbose)
                if "stop" in inputs and inputs["stop"] != stop:
                    raise ValueError(
                        "If `stop` is present in any inputs, should be present in all."
                    )
                prompts.append(prompt)

            prompt_strings = [p.to_string() for p in prompts]
            prompts = prompt_strings
            callbacks = run_manager_.get_child() if run_manager_ else None
            tags = None

            """Run the LLM on the given prompt and input."""
            # If string is passed in directly no errors will be raised but outputs will
            # not make sense.
            if not isinstance(prompts, list):
                raise ValueError(
                    "Argument 'prompts' is expected to be of type List[str], received"
                    f" argument of type {type(prompts)}."
                )
            params = self.llm.dict()
            params["stop"] = stop
            options = {"stop": stop}
            disregard_cache = self.llm.cache is not None and not self.llm.cache
            callback_manager = CallbackManager.configure(
                callbacks,
                self.llm.callbacks,
                self.llm.verbose,
                tags,
                self.llm.tags,
            )
            if langchain.llm_cache is None or disregard_cache:
                # This happens when langchain.cache is None, but self.cache is True
                if self.llm.cache is not None and self.cache:
                    raise ValueError(
                        "Asked to cache, but no cache found at `langchain.cache`."
                    )
                run_manager_ = callback_manager.on_llm_start(
                    dumpd(self),
                    prompts,
                    invocation_params=params,
                    options=options,
                )

                generations = []
                for prompt in prompts:
                    inputs_ = prompt
                    num_workers = None
                    batch_size = None

                    if num_workers is None:
                        if self.llm.pipeline._num_workers is None:
                            num_workers = 0
                        else:
                            num_workers = self.llm.pipeline._num_workers
                    if batch_size is None:
                        if self.llm.pipeline._batch_size is None:
                            batch_size = 1
                        else:
                            batch_size = self.llm.pipeline._batch_size

                    preprocess_params = {}
                    generate_kwargs = {}
                    preprocess_params.update(generate_kwargs)
                    forward_params = generate_kwargs
                    postprocess_params = {}
                    # Fuse __init__ params and __call__ params without modifying the __init__ ones.
                    preprocess_params = {
                        **self.llm.pipeline._preprocess_params,
                        **preprocess_params,
                    }
                    forward_params = {
                        **self.llm.pipeline._forward_params,
                        **forward_params,
                    }
                    postprocess_params = {
                        **self.llm.pipeline._postprocess_params,
                        **postprocess_params,
                    }

                    self.llm.pipeline.call_count += 1
                    if (
                        self.llm.pipeline.call_count > 10
                        and self.llm.pipeline.framework == "pt"
                        and self.llm.pipeline.device.type == "cuda"
                    ):
                        warnings.warn(
                            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a"
                            " dataset",
                            UserWarning,
                        )

                    model_inputs = self.llm.pipeline.preprocess(
                        inputs_, **preprocess_params
                    )
                    model_outputs = self.llm.pipeline.forward(
                        model_inputs, **forward_params
                    )
                    model_outputs["process"] = False
                    return model_outputs
                output = LLMResult(generations=generations)
                run_manager_.on_llm_end(output)
                if run_manager_:
                    output.run = RunInfo(run_id=run_manager_.run_id)
                response = output

            outputs = [
                # Get the text of the top generated string.
                {self.output_key: generation[0].text}
                for generation in response.generations
            ][0]
            run_manager.on_chain_end(outputs)
            final_outputs: Dict[str, Any] = self.prep_outputs(
                inputs, outputs, return_only_outputs
            )
            if include_run_info:
                final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)
            return final_outputs
        else:
            _run_manager = (
                run_manager or CallbackManagerForChainRun.get_noop_manager()
            )
            docs = inputs[self.input_key]
            # Other keys are assumed to be needed for LLM prediction
            other_keys = {
                k: v for k, v in inputs.items() if k != self.input_key
            }
            doc_strings = [
                format_document(doc, self.document_prompt) for doc in docs
            ]
            # Join the documents together to put them in the prompt.
            inputs = {
                k: v
                for k, v in other_keys.items()
                if k in self.llm_chain.prompt.input_variables
            }
            inputs[self.document_variable_name] = self.document_separator.join(
                doc_strings
            )
            inputs["is_first"] = False
            inputs["input_documents"] = input_docs

            # Call predict on the LLM.
            output = self.llm_chain(inputs, callbacks=_run_manager.get_child())
            if "process" in output.keys() and not output["process"]:
                return output
            output = output[self.llm_chain.output_key]
            extra_return_dict = {}
        extra_return_dict[self.output_key] = output
        outputs = extra_return_dict
        run_manager.on_chain_end(outputs)
        final_outputs: Dict[str, Any] = self.prep_outputs(
            inputs, outputs, return_only_outputs
        )
        if include_run_info:
            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)
        return final_outputs

    def prep_outputs(
        self,
        inputs: Dict[str, str],
        outputs: Dict[str, str],
        return_only_outputs: bool = False,
    ) -> Dict[str, str]:
        """Validate and prep outputs."""
        self._validate_outputs(outputs)
        if self.memory is not None:
            self.memory.save_context(inputs, outputs)
        if return_only_outputs:
            return outputs
        else:
            return {**inputs, **outputs}

    def prep_inputs(
        self, inputs: Union[Dict[str, Any], Any]
    ) -> Dict[str, str]:
        """Validate and prep inputs."""
        if not isinstance(inputs, dict):
            _input_keys = set(self.input_keys)
            if self.memory is not None:
                # If there are multiple input keys, but some get set by memory so that
                # only one is not set, we can still figure out which key it is.
                _input_keys = _input_keys.difference(
                    self.memory.memory_variables
                )
            if len(_input_keys) != 1:
                raise ValueError(
                    f"A single string input was passed in, but this chain expects "
                    f"multiple inputs ({_input_keys}). When a chain expects "
                    f"multiple inputs, please call it by passing in a dictionary, "
                    "eg `chain({'foo': 1, 'bar': 2})`"
                )
            inputs = {list(_input_keys)[0]: inputs}
        if self.memory is not None:
            external_context = self.memory.load_memory_variables(inputs)
            inputs = dict(inputs, **external_context)
        self._validate_inputs(inputs)
        return inputs

    def apply(
        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None
    ) -> List[Dict[str, str]]:
        """Call the chain on all inputs in the list."""
        return [self(inputs, callbacks=callbacks) for inputs in input_list]

    def run(
        self,
        *args: Any,
        callbacks: Callbacks = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> str:
        """Run the chain as text in, text out or multiple variables, text out."""
        if len(self.output_keys) != 1:
            raise ValueError(
                f"`run` not supported when there is not exactly "
                f"one output key. Got {self.output_keys}."
            )

        if args and not kwargs:
            if len(args) != 1:
                raise ValueError(
                    "`run` supports only one positional argument."
                )
            return self(args[0], callbacks=callbacks, tags=tags)[
                self.output_keys[0]
            ]

        if kwargs and not args:
            return self(kwargs, callbacks=callbacks, tags=tags)[
                self.output_keys[0]
            ]

        if not kwargs and not args:
            raise ValueError(
                "`run` supported with either positional arguments or keyword arguments,"
                " but none were provided."
            )

        raise ValueError(
            f"`run` supported with either positional arguments or keyword arguments"
            f" but not both. Got args: {args} and kwargs: {kwargs}."
        )

    def dict(self, **kwargs: Any) -> Dict:
        """Return dictionary representation of chain."""
        if self.memory is not None:
            raise ValueError("Saving of memory is not yet supported.")
        _dict = super().dict()
        _dict["_type"] = self._chain_type
        return _dict

    def save(self, file_path: Union[Path, str]) -> None:
        """Save the chain.

        Args:
            file_path: Path to file to save the chain to.

        Example:
        .. code-block:: python

            chain.save(file_path="path/chain.yaml")
        """
        # Convert file to Path object.
        if isinstance(file_path, str):
            save_path = Path(file_path)
        else:
            save_path = file_path

        directory_path = save_path.parent
        directory_path.mkdir(parents=True, exist_ok=True)

        # Fetch dictionary to save
        chain_dict = self.dict()

        if save_path.suffix == ".json":
            with open(file_path, "w") as f:
                json.dump(chain_dict, f, indent=4)
        elif save_path.suffix == ".yaml":
            with open(file_path, "w") as f:
                yaml.dump(chain_dict, f, default_flow_style=False)
        else:
            raise ValueError(f"{save_path} must be json or yaml")


class BaseCombineDocumentsChain(Chain, ABC):
    """Base interface for chains combining documents."""

    input_key: str = "input_documents"  #: :meta private:
    output_key: str = "output_text"  #: :meta private:

    @property
    def input_keys(self) -> List[str]:
        """Expect input key.

        :meta private:
        """
        return [self.input_key]

    @property
    def output_keys(self) -> List[str]:
        """Return output key.

        :meta private:
        """
        return [self.output_key]

    def prompt_length(
        self, docs: List[Document], **kwargs: Any
    ) -> Optional[int]:
        """Return the prompt length given the documents passed in.

        Returns None if the method does not depend on the prompt length.
        """
        return None

    def _call(
        self,
        inputs: Dict[str, List[Document]],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, str]:
        _run_manager = (
            run_manager or CallbackManagerForChainRun.get_noop_manager()
        )
        docs = inputs[self.input_key]
        # Other keys are assumed to be needed for LLM prediction
        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}
        doc_strings = [
            format_document(doc, self.document_prompt) for doc in docs
        ]
        # Join the documents together to put them in the prompt.
        inputs = {
            k: v
            for k, v in other_keys.items()
            if k in self.llm_chain.prompt.input_variables
        }
        inputs[self.document_variable_name] = self.document_separator.join(
            doc_strings
        )

        # Call predict on the LLM.
        output, extra_return_dict = (
            self.llm_chain(inputs, callbacks=_run_manager.get_child())[
                self.llm_chain.output_key
            ],
            {},
        )

        extra_return_dict[self.output_key] = output
        return extra_return_dict


from pydantic import BaseModel


class Generation(Serializable):
    """Output of a single generation."""

    text: str
    """Generated text output."""

    generation_info: Optional[Dict[str, Any]] = None
    """Raw generation info response from the provider"""
    """May include things like reason for finishing (e.g. in OpenAI)"""
    # TODO: add log probs


VALID_TASKS = ("text2text-generation", "text-generation", "summarization")


class LLMChain(Chain):
    """Chain to run queries against LLMs.

    Example:
        .. code-block:: python

            from langchain import LLMChain, OpenAI, PromptTemplate
            prompt_template = "Tell me a {adjective} joke"
            prompt = PromptTemplate(
                input_variables=["adjective"], template=prompt_template
            )
            llm = LLMChain(llm=OpenAI(), prompt=prompt)
    """

    @property
    def lc_serializable(self) -> bool:
        return True

    prompt: BasePromptTemplate
    """Prompt object to use."""
    llm: BaseLanguageModel
    output_key: str = "text"  #: :meta private:

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        arbitrary_types_allowed = True

    @property
    def input_keys(self) -> List[str]:
        """Will be whatever keys the prompt expects.

        :meta private:
        """
        return self.prompt.input_variables

    @property
    def output_keys(self) -> List[str]:
        """Will always return text key.

        :meta private:
        """
        return [self.output_key]

    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, str]:
        prompts, stop = self.prep_prompts([inputs], run_manager=run_manager)
        response = self.llm.generate_prompt(
            prompts,
            stop,
            callbacks=run_manager.get_child() if run_manager else None,
        )
        return self.create_outputs(response)[0]

    def prep_prompts(
        self,
        input_list: List[Dict[str, Any]],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Tuple[List[PromptValue], Optional[List[str]]]:
        """Prepare prompts from inputs."""
        stop = None
        if "stop" in input_list[0]:
            stop = input_list[0]["stop"]
        prompts = []
        for inputs in input_list:
            selected_inputs = {
                k: inputs[k] for k in self.prompt.input_variables
            }
            prompt = self.prompt.format_prompt(**selected_inputs)
            _colored_text = get_colored_text(prompt.to_string(), "green")
            _text = "Prompt after formatting:\n" + _colored_text
            if run_manager:
                run_manager.on_text(_text, end="\n", verbose=self.verbose)
            if "stop" in inputs and inputs["stop"] != stop:
                raise ValueError(
                    "If `stop` is present in any inputs, should be present in all."
                )
            prompts.append(prompt)
        return prompts, stop

    def apply(
        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None
    ) -> List[Dict[str, str]]:
        """Utilize the LLM generate method for speed gains."""
        callback_manager = CallbackManager.configure(
            callbacks, self.callbacks, self.verbose
        )
        run_manager = callback_manager.on_chain_start(
            dumpd(self),
            {"input_list": input_list},
        )
        try:
            response = self.generate(input_list, run_manager=run_manager)
        except (KeyboardInterrupt, Exception) as e:
            run_manager.on_chain_error(e)
            raise e
        outputs = self.create_outputs(response)
        run_manager.on_chain_end({"outputs": outputs})
        return outputs

    def create_outputs(self, response: LLMResult) -> List[Dict[str, str]]:
        """Create outputs from response."""
        return [
            # Get the text of the top generated string.
            {self.output_key: generation[0].text}
            for generation in response.generations
        ]

    def predict_and_parse(
        self, callbacks: Callbacks = None, **kwargs: Any
    ) -> Union[str, List[str], Dict[str, Any]]:
        """Call predict and then parse the results."""
        result = self.predict(callbacks=callbacks, **kwargs)
        if self.prompt.output_parser is not None:
            return self.prompt.output_parser.parse(result)
        else:
            return result

    def apply_and_parse(
        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None
    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:
        """Call apply and then parse the results."""
        result = self.apply(input_list, callbacks=callbacks)
        return self._parse_result(result)

    def _parse_result(
        self, result: List[Dict[str, str]]
    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:
        if self.prompt.output_parser is not None:
            return [
                self.prompt.output_parser.parse(res[self.output_key])
                for res in result
            ]
        else:
            return result

    @property
    def _chain_type(self) -> str:
        return "llm_chain"

    @classmethod
    def from_string(cls, llm: BaseLanguageModel, template: str) -> LLMChain:
        """Create LLMChain from LLM and template."""
        prompt_template = PromptTemplate.from_template(template)
        return cls(llm=llm, prompt=prompt_template)


def _get_default_document_prompt() -> PromptTemplate:
    return PromptTemplate(
        input_variables=["page_content"], template="{page_content}"
    )


class StuffDocumentsChain(BaseCombineDocumentsChain):
    """Chain that combines documents by stuffing into context."""

    llm_chain: LLMChain
    """LLM wrapper to use after formatting documents."""
    document_prompt: BasePromptTemplate = Field(
        default_factory=_get_default_document_prompt
    )
    """Prompt to use to format each document."""
    document_variable_name: str
    """The variable name in the llm_chain to put the documents in.
    If only one variable in the llm_chain, this need not be provided."""
    document_separator: str = "\n\n"
    """The string with which to join the formatted documents"""

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        arbitrary_types_allowed = True

    @root_validator(pre=True)
    def get_default_document_variable_name(cls, values: Dict) -> Dict:
        """Get default document variable name, if not provided."""
        llm_chain_variables = values["llm_chain"].prompt.input_variables
        if "document_variable_name" not in values:
            if len(llm_chain_variables) == 1:
                values["document_variable_name"] = llm_chain_variables[0]
            else:
                raise ValueError(
                    "document_variable_name must be provided if there are "
                    "multiple llm_chain_variables"
                )
        else:
            if values["document_variable_name"] not in llm_chain_variables:
                raise ValueError(
                    f"document_variable_name {values['document_variable_name']} was "
                    f"not found in llm_chain input_variables: {llm_chain_variables}"
                )
        return values

    def _get_inputs(self, docs: List[Document], **kwargs: Any) -> dict:
        # Format each document according to the prompt
        doc_strings = [
            format_document(doc, self.document_prompt) for doc in docs
        ]
        # Join the documents together to put them in the prompt.
        inputs = {
            k: v
            for k, v in kwargs.items()
            if k in self.llm_chain.prompt.input_variables
        }
        inputs[self.document_variable_name] = self.document_separator.join(
            doc_strings
        )
        return inputs

    def prompt_length(
        self, docs: List[Document], **kwargs: Any
    ) -> Optional[int]:
        """Get the prompt length by formatting the prompt."""
        inputs = self._get_inputs(docs, **kwargs)
        prompt = self.llm_chain.prompt.format(**inputs)
        return self.llm_chain.llm.get_num_tokens(prompt)

    @property
    def _chain_type(self) -> str:
        return "stuff_documents_chain"


class LoadingCallable(Protocol):
    """Interface for loading the combine documents chain."""

    def __call__(
        self, llm: BaseLanguageModel, **kwargs: Any
    ) -> BaseCombineDocumentsChain:
        """Callable to load the combine documents chain."""


def _load_stuff_chain(
    llm: BaseLanguageModel,
    prompt: Optional[BasePromptTemplate] = None,
    document_variable_name: str = "context",
    verbose: Optional[bool] = None,
    callback_manager: Optional[BaseCallbackManager] = None,
    callbacks: Callbacks = None,
    **kwargs: Any,
) -> StuffDocumentsChain:
    _prompt = prompt or stuff_prompt.PROMPT_SELECTOR.get_prompt(llm)
    llm_chain = LLMChain(
        llm=llm,
        prompt=_prompt,
        verbose=verbose,
        callback_manager=callback_manager,
        callbacks=callbacks,
    )
    # TODO: document prompt
    return StuffDocumentsChain(
        llm_chain=llm_chain,
        document_variable_name=document_variable_name,
        verbose=verbose,
        callback_manager=callback_manager,
        **kwargs,
    )


def load_qa_chain(
    llm: BaseLanguageModel,
    chain_type: str = "stuff",
    verbose: Optional[bool] = None,
    callback_manager: Optional[BaseCallbackManager] = None,
    **kwargs: Any,
) -> BaseCombineDocumentsChain:
    """Load question answering chain.

    Args:
        llm: Language Model to use in the chain.
        chain_type: Type of document combining chain to use. Should be one of "stuff",
            "map_reduce", "map_rerank", and "refine".
        verbose: Whether chains should be run in verbose mode or not. Note that this
            applies to all chains that make up the final chain.
        callback_manager: Callback manager to use for the chain.

    Returns:
        A chain to use for question answering.
    """
    loader_mapping: Mapping[str, LoadingCallable] = {
        "stuff": _load_stuff_chain,
    }
    if chain_type not in loader_mapping:
        raise ValueError(
            f"Got unsupported chain type: {chain_type}. "
            f"Should be one of {loader_mapping.keys()}"
        )
    return loader_mapping[chain_type](
        llm, verbose=verbose, callback_manager=callback_manager, **kwargs
    )

```

`apps/language_models/langchain/gen.py`:

```py
import ast
import copy
import functools
import glob
import inspect
import queue
import sys
import os
import time
import traceback
import types
import typing
import warnings
from datetime import datetime
import filelock
import requests
import psutil
from requests import ConnectTimeout, JSONDecodeError
from urllib3.exceptions import (
    ConnectTimeoutError,
    MaxRetryError,
    ConnectionError,
)
from requests.exceptions import ConnectionError as ConnectionError2
from requests.exceptions import ReadTimeout as ReadTimeout2

if os.path.dirname(os.path.abspath(__file__)) not in sys.path:
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))

os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["BITSANDBYTES_NOWELCOME"] = "1"
warnings.filterwarnings(
    "ignore", category=UserWarning, message="TypedStorage is deprecated"
)

from evaluate_params import eval_func_param_names, no_default_param_names
from enums import (
    DocumentChoices,
    LangChainMode,
    no_lora_str,
    model_token_mapping,
    no_model_str,
    source_prefix,
    source_postfix,
    LangChainAction,
)
from loaders import get_loaders
from utils import (
    set_seed,
    clear_torch_cache,
    save_generate_output,
    NullContext,
    wrapped_partial,
    EThread,
    get_githash,
    import_matplotlib,
    get_device,
    makedirs,
    get_kwargs,
    start_faulthandler,
    get_hf_server,
    FakeTokenizer,
    remove,
)

start_faulthandler()
import_matplotlib()

SEED = 1236
set_seed(SEED)

from typing import Union

# import fire
import torch
from transformers import GenerationConfig, AutoModel, TextIteratorStreamer

from prompter import (
    Prompter,
    inv_prompt_type_to_model_lower,
    non_hf_types,
    PromptType,
    get_prompt,
    generate_prompt,
)
from stopping import get_stopping

langchain_modes = [x.value for x in list(LangChainMode)]

langchain_actions = [x.value for x in list(LangChainAction)]

scratch_base_dir = "/tmp/"


class Langchain:
    def __init__(self, device="cuda", precision="fp16"):
        super().__init__()
        self.device = device
        self.precision = precision

    def get_config(
        self,
        base_model,
        use_auth_token=False,
        trust_remote_code=True,
        offload_folder=None,
        triton_attn=False,
        long_sequence=True,
        return_model=False,
        raise_exception=False,
    ):
        from accelerate import init_empty_weights

        with init_empty_weights():
            from transformers import AutoConfig

            try:
                config = AutoConfig.from_pretrained(
                    base_model,
                    use_auth_token=use_auth_token,
                    trust_remote_code=trust_remote_code,
                    offload_folder=offload_folder,
                )
            except OSError as e:
                if raise_exception:
                    raise
                if "not a local folder and is not a valid model identifier listed on" in str(
                    e
                ) or "404 Client Error" in str(
                    e
                ):
                    # e.g. llama, gpjt, etc.
                    # e.g. HF TGI but not model on HF or private etc.
                    # HF TGI server only should really require prompt_type, not HF model state
                    return None, None
                else:
                    raise
            if triton_attn and "mpt-" in base_model.lower():
                config.attn_config["attn_impl"] = "triton"
            if long_sequence:
                if "mpt-7b-storywriter" in base_model.lower():
                    config.update({"max_seq_len": 83968})
                if "mosaicml/mpt-7b-chat" in base_model.lower():
                    config.update({"max_seq_len": 4096})
                if "mpt-30b" in base_model.lower():
                    config.update({"max_seq_len": 2 * 8192})
            if return_model and issubclass(
                config.__class__, tuple(AutoModel._model_mapping.keys())
            ):
                model = AutoModel.from_config(
                    config,
                    trust_remote_code=trust_remote_code,
                )
            else:
                # can't infer
                model = None
        if "falcon" in base_model.lower():
            config.use_cache = False

        return config, model

    def get_non_lora_model(
        self,
        base_model,
        model_loader,
        load_half,
        load_gptq,
        use_safetensors,
        model_kwargs,
        reward_type,
        config,
        model,
        gpu_id=0,
    ):
        """
        Ensure model gets on correct device
        """

        device_map = None
        if model is not None:
            # NOTE: Can specify max_memory={0: max_mem, 1: max_mem}, to shard model
            # NOTE: Some models require avoiding sharding some layers,
            # then would pass no_split_module_classes and give list of those layers.
            from accelerate import infer_auto_device_map

            device_map = infer_auto_device_map(
                model,
                dtype=torch.float16 if load_half else torch.float32,
            )
            if hasattr(model, "model"):
                device_map_model = infer_auto_device_map(
                    model.model,
                    dtype=torch.float16 if load_half else torch.float32,
                )
                device_map.update(device_map_model)

        n_gpus = torch.cuda.device_count() if torch.cuda.is_available else 0

        if device_map is None:
            if self.device == "cuda":
                if n_gpus > 0:
                    if gpu_id >= 0:
                        # FIXME: If really distributes model, tend to get things like: ValueError: gpt_neox.embed_in.weight doesn't have any device set.
                        # So avoid for now, just put on first GPU, unless score_model, put on last
                        if reward_type:
                            device_map = {"": n_gpus - 1}
                        else:
                            device_map = {"": min(n_gpus - 1, gpu_id)}
                    if gpu_id == -1:
                        device_map = {"": "cuda"}
            else:
                device_map = {"": "cpu"}
                model_kwargs["load_in_8bit"] = False
                model_kwargs["load_in_4bit"] = False
        print("device_map: %s" % device_map, flush=True)

        load_in_8bit = model_kwargs.get("load_in_8bit", False)
        load_in_4bit = model_kwargs.get("load_in_4bit", False)
        model_kwargs["device_map"] = device_map
        model_kwargs["use_safetensors"] = use_safetensors
        self.pop_unused_model_kwargs(model_kwargs)

        if load_gptq:
            model_kwargs.pop("torch_dtype", None)
            model_kwargs.pop("device_map")
            model = model_loader(
                model_name_or_path=base_model,
                model_basename=load_gptq,
                **model_kwargs,
            )
        elif load_in_8bit or load_in_4bit or not load_half:
            model = model_loader(
                base_model,
                config=config,
                **model_kwargs,
            )
        else:
            model = model_loader(
                base_model,
                config=config,
                **model_kwargs,
            ).half()
        return model

    def get_client_from_inference_server(
        self,
        inference_server,
        base_model=None,
        raise_connection_exception=False,
    ):
        inference_server, headers = get_hf_server(inference_server)
        # preload client since slow for gradio case especially
        from gradio_utils.grclient import GradioClient

        gr_client = None
        hf_client = None
        if headers is None:
            try:
                print(
                    "GR Client Begin: %s %s" % (inference_server, base_model),
                    flush=True,
                )
                # first do sanity check if alive, else gradio client takes too long by default
                requests.get(
                    inference_server,
                    timeout=int(os.getenv("REQUEST_TIMEOUT", "30")),
                )
                gr_client = GradioClient(inference_server)
                print("GR Client End: %s" % inference_server, flush=True)
            except (OSError, ValueError) as e:
                # Occurs when wrong endpoint and should have been HF client, so don't hard raise, just move to HF
                gr_client = None
                print(
                    "GR Client Failed %s %s: %s"
                    % (inference_server, base_model, str(e)),
                    flush=True,
                )
            except (
                ConnectTimeoutError,
                ConnectTimeout,
                MaxRetryError,
                ConnectionError,
                ConnectionError2,
                JSONDecodeError,
                ReadTimeout2,
                KeyError,
            ) as e:
                t, v, tb = sys.exc_info()
                ex = "".join(traceback.format_exception(t, v, tb))
                print(
                    "GR Client Failed %s %s: %s"
                    % (inference_server, base_model, str(ex)),
                    flush=True,
                )
                if raise_connection_exception:
                    raise

        if gr_client is None:
            res = None
            from text_generation import Client as HFClient

            print("HF Client Begin: %s %s" % (inference_server, base_model))
            try:
                hf_client = HFClient(
                    inference_server,
                    headers=headers,
                    timeout=int(os.getenv("REQUEST_TIMEOUT", "30")),
                )
                # quick check valid TGI endpoint
                res = hf_client.generate("What?", max_new_tokens=1)
                hf_client = HFClient(
                    inference_server, headers=headers, timeout=300
                )
            except (
                ConnectTimeoutError,
                ConnectTimeout,
                MaxRetryError,
                ConnectionError,
                ConnectionError2,
                JSONDecodeError,
                ReadTimeout2,
                KeyError,
            ) as e:
                hf_client = None
                t, v, tb = sys.exc_info()
                ex = "".join(traceback.format_exception(t, v, tb))
                print(
                    "HF Client Failed %s %s: %s"
                    % (inference_server, base_model, str(ex))
                )
                if raise_connection_exception:
                    raise
            print(
                "HF Client End: %s %s : %s"
                % (inference_server, base_model, res)
            )
        return inference_server, gr_client, hf_client

    def get_model(
        self,
        load_8bit: bool = False,
        load_4bit: bool = False,
        load_half: bool = False,
        load_gptq: str = "",
        use_safetensors: bool = False,
        infer_devices: bool = True,
        device: str = None,
        base_model: str = "",
        inference_server: str = "",
        tokenizer_base_model: str = "",
        lora_weights: str = "",
        gpu_id: int = 0,
        reward_type: bool = None,
        local_files_only: bool = False,
        resume_download: bool = True,
        use_auth_token: Union[str, bool] = False,
        trust_remote_code: bool = True,
        offload_folder: str = None,
        compile_model: bool = True,
        verbose: bool = False,
    ):
        """

        :param load_8bit: load model in 8-bit, not supported by all models
        :param load_4bit: load model in 4-bit, not supported by all models
        :param load_half: load model in 16-bit
        :param load_gptq: GPTQ model_basename
        :param use_safetensors: use safetensors file
        :param infer_devices: Use torch infer of optimal placement of layers on devices (for non-lora case)
            For non-LORA case, False will spread shards across multiple GPUs, but this can lead to cuda:x cuda:y mismatches
            So it is not the default
        :param base_model: name/path of base model
        :param inference_server: whether base_model is hosted locally ('') or via http (url)
        :param tokenizer_base_model: name/path of tokenizer
        :param lora_weights: name/path
        :param gpu_id: which GPU (0..n_gpus-1) or allow all GPUs if relevant (-1)
        :param reward_type: reward type model for sequence classification
        :param local_files_only: use local files instead of from HF
        :param resume_download: resume downloads from HF
        :param use_auth_token: assumes user did on CLI `huggingface-cli login` to access private repo
        :param trust_remote_code: trust code needed by model
        :param offload_folder: offload folder
        :param compile_model: whether to compile torch model
        :param verbose:
        :return:
        """
        if verbose:
            print("Get %s model" % base_model, flush=True)

        triton_attn = False
        long_sequence = True
        config_kwargs = dict(
            use_auth_token=use_auth_token,
            trust_remote_code=trust_remote_code,
            offload_folder=offload_folder,
            triton_attn=triton_attn,
            long_sequence=long_sequence,
        )
        config, _ = self.get_config(
            base_model, **config_kwargs, raise_exception=False
        )

        if base_model in non_hf_types:
            assert config is None, "Expected config None for %s" % base_model

        llama_type_from_config = "llama" in str(config).lower()
        llama_type_from_name = "llama" in base_model.lower()
        llama_type = llama_type_from_config or llama_type_from_name
        if "xgen" in base_model.lower():
            llama_type = False
        if llama_type:
            if verbose:
                print(
                    "Detected as llama type from"
                    " config (%s) or name (%s)"
                    % (llama_type_from_config, llama_type_from_name),
                    flush=True,
                )

        model_loader, tokenizer_loader = get_loaders(
            model_name=base_model,
            reward_type=reward_type,
            llama_type=llama_type,
            load_gptq=load_gptq,
        )

        tokenizer_kwargs = dict(
            local_files_only=local_files_only,
            resume_download=resume_download,
            use_auth_token=use_auth_token,
            trust_remote_code=trust_remote_code,
            offload_folder=offload_folder,
            padding_side="left",
            config=config,
        )
        if not tokenizer_base_model:
            tokenizer_base_model = base_model

        if (
            config is not None
            and tokenizer_loader is not None
            and not isinstance(tokenizer_loader, str)
        ):
            tokenizer = tokenizer_loader.from_pretrained(
                tokenizer_base_model, **tokenizer_kwargs
            )
            # sets raw (no cushion) limit
            self.set_model_max_len(config, tokenizer, verbose=False)
            # if using fake tokenizer, not really accurate when lots of numbers, give a bit of buffer, else get:
            # Generation Failed: Input validation error: `inputs` must have less than 2048 tokens. Given: 2233
            tokenizer.model_max_length = tokenizer.model_max_length - 50
        else:
            tokenizer = FakeTokenizer()

        if isinstance(inference_server, str) and inference_server.startswith(
            "http"
        ):
            (
                inference_server,
                gr_client,
                hf_client,
            ) = self.get_client_from_inference_server(
                inference_server, base_model=base_model
            )
            client = gr_client or hf_client
            # Don't return None, None for model, tokenizer so triggers
            return client, tokenizer, "http"
        if isinstance(inference_server, str) and inference_server.startswith(
            "openai"
        ):
            assert os.getenv(
                "OPENAI_API_KEY"
            ), "Set environment for OPENAI_API_KEY"
            # Don't return None, None for model, tokenizer so triggers
            # include small token cushion
            tokenizer = FakeTokenizer(
                model_max_length=model_token_mapping[base_model] - 50
            )
            return inference_server, tokenizer, inference_server
        assert not inference_server, (
            "Malformed inference_server=%s" % inference_server
        )
        if base_model in non_hf_types:
            from gpt4all_llm import get_model_tokenizer_gpt4all

            model, tokenizer, _ = get_model_tokenizer_gpt4all(base_model)
            return model, tokenizer, self.device

        # get local torch-HF model
        return self.get_hf_model(
            load_8bit=load_8bit,
            load_4bit=load_4bit,
            load_half=load_half,
            load_gptq=load_gptq,
            use_safetensors=use_safetensors,
            infer_devices=infer_devices,
            device=self.device,
            base_model=base_model,
            tokenizer_base_model=tokenizer_base_model,
            lora_weights=lora_weights,
            gpu_id=gpu_id,
            reward_type=reward_type,
            local_files_only=local_files_only,
            resume_download=resume_download,
            use_auth_token=use_auth_token,
            trust_remote_code=trust_remote_code,
            offload_folder=offload_folder,
            compile_model=compile_model,
            llama_type=llama_type,
            config_kwargs=config_kwargs,
            tokenizer_kwargs=tokenizer_kwargs,
            verbose=verbose,
        )

    def get_hf_model(
        self,
        load_8bit: bool = False,
        load_4bit: bool = False,
        load_half: bool = True,
        load_gptq: str = "",
        use_safetensors: bool = False,
        infer_devices: bool = True,
        device: str = None,
        base_model: str = "",
        tokenizer_base_model: str = "",
        lora_weights: str = "",
        gpu_id: int = 0,
        reward_type: bool = None,
        local_files_only: bool = False,
        resume_download: bool = True,
        use_auth_token: Union[str, bool] = False,
        trust_remote_code: bool = True,
        offload_folder: str = None,
        compile_model: bool = True,
        llama_type: bool = False,
        config_kwargs=None,
        tokenizer_kwargs=None,
        verbose: bool = False,
    ):
        assert config_kwargs is not None
        assert tokenizer_kwargs is not None

        if lora_weights is not None and lora_weights.strip():
            if verbose:
                print("Get %s lora weights" % lora_weights, flush=True)

        if "gpt2" in base_model.lower():
            # RuntimeError: where expected condition to be a boolean tensor, but got a tensor with dtype Half
            load_8bit = False
            load_4bit = False

        assert (
            base_model.strip()
        ), "Please choose a base model with --base_model (CLI) or load one from Models Tab (gradio)"

        model_loader, tokenizer_loader = get_loaders(
            model_name=base_model,
            reward_type=reward_type,
            llama_type=llama_type,
            load_gptq=load_gptq,
        )

        config, _ = self.get_config(
            base_model,
            return_model=False,
            raise_exception=True,
            **config_kwargs,
        )

        if tokenizer_loader is not None and not isinstance(
            tokenizer_loader, str
        ):
            tokenizer = tokenizer_loader.from_pretrained(
                tokenizer_base_model, **tokenizer_kwargs
            )
        else:
            tokenizer = tokenizer_loader

        if isinstance(tokenizer, str):
            # already a pipeline, tokenizer_loader is string for task
            model = model_loader(
                tokenizer,
                model=base_model,
                device=0 if self.device == "cuda" else -1,
                torch_dtype=torch.float16
                if self.device == "cuda"
                else torch.float32,
            )
        else:
            assert self.device in ["cuda", "cpu", "mps"], (
                "Unsupported device %s" % self.device
            )
            model_kwargs = dict(
                local_files_only=local_files_only,
                torch_dtype=torch.float16
                if self.device == "cuda"
                else torch.float32,
                resume_download=resume_download,
                use_auth_token=use_auth_token,
                trust_remote_code=trust_remote_code,
                offload_folder=offload_folder,
            )
            if (
                "mbart-" not in base_model.lower()
                and "mpt-" not in base_model.lower()
            ):
                if (
                    infer_devices
                    and gpu_id is not None
                    and gpu_id >= 0
                    and self.device == "cuda"
                ):
                    device_map = {"": gpu_id}
                else:
                    device_map = "auto"
                model_kwargs.update(
                    dict(
                        load_in_8bit=load_8bit,
                        load_in_4bit=load_4bit,
                        device_map=device_map,
                    )
                )
            if (
                "mpt-" in base_model.lower()
                and gpu_id is not None
                and gpu_id >= 0
            ):
                # MPT doesn't support spreading over GPUs
                model_kwargs.update(
                    dict(
                        device_map={"": gpu_id}
                        if self.device == "cuda"
                        else "cpu"
                    )
                )

            if "OpenAssistant/reward-model".lower() in base_model.lower():
                # FIXME: could put on other GPUs
                model_kwargs["device_map"] = (
                    {"": 0} if self.device == "cuda" else {"": "cpu"}
                )
                model_kwargs.pop("torch_dtype", None)
            self.pop_unused_model_kwargs(model_kwargs)

            if not lora_weights:
                # torch.device context uses twice memory for AutoGPTQ
                context = NullContext if load_gptq else torch.device
                with context(self.device):
                    if infer_devices:
                        config, model = self.get_config(
                            base_model,
                            return_model=True,
                            raise_exception=True,
                            **config_kwargs,
                        )
                        model = self.get_non_lora_model(
                            base_model,
                            model_loader,
                            load_half,
                            load_gptq,
                            use_safetensors,
                            model_kwargs,
                            reward_type,
                            config,
                            model,
                            gpu_id=gpu_id,
                        )
                    else:
                        config, _ = self.get_config(
                            base_model, **config_kwargs
                        )
                        if load_half and not (
                            load_8bit or load_4bit or load_gptq
                        ):
                            model = model_loader(
                                base_model, config=config, **model_kwargs
                            ).half()
                        else:
                            model = model_loader(
                                base_model, config=config, **model_kwargs
                            )
            elif load_8bit or load_4bit:
                config, _ = self.get_config(base_model, **config_kwargs)
                model = model_loader(base_model, config=config, **model_kwargs)
                from peft import (
                    PeftModel,
                )  # loads cuda, so avoid in global scope

                model = PeftModel.from_pretrained(
                    model,
                    lora_weights,
                    torch_dtype=torch.float16
                    if self.device == "cuda"
                    else torch.float32,
                    local_files_only=local_files_only,
                    resume_download=resume_download,
                    use_auth_token=use_auth_token,
                    trust_remote_code=trust_remote_code,
                    offload_folder=offload_folder,
                    device_map={"": 0}
                    if self.device == "cuda"
                    else {"": "cpu"},  # seems to be required
                )
            else:
                with torch.device(self.device):
                    config, _ = self.get_config(
                        base_model, raise_exception=True, **config_kwargs
                    )
                    model = model_loader(
                        base_model, config=config, **model_kwargs
                    )
                    from peft import (
                        PeftModel,
                    )  # loads cuda, so avoid in global scope

                    model = PeftModel.from_pretrained(
                        model,
                        lora_weights,
                        torch_dtype=torch.float16
                        if self.device == "cuda"
                        else torch.float32,
                        local_files_only=local_files_only,
                        resume_download=resume_download,
                        use_auth_token=use_auth_token,
                        trust_remote_code=trust_remote_code,
                        offload_folder=offload_folder,
                        device_map="auto",
                    )
                    if load_half and not load_gptq:
                        model.half()

        # unwind broken decapoda-research config
        if llama_type:
            model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk
            model.config.bos_token_id = 1
            model.config.eos_token_id = 2
        if "gpt2" in base_model.lower():
            # add special tokens that otherwise all share the same id
            tokenizer.add_special_tokens(
                {
                    "bos_token": "<bos>",
                    "eos_token": "<eos>",
                    "pad_token": "<pad>",
                }
            )

        if not isinstance(tokenizer, str):
            model.eval()
            # if torch.__version__ >= "2" and sys.platform != "win32" and compile_model:
            #     model = torch.compile(model)

        self.set_model_max_len(
            config, tokenizer, verbose=False, reward_type=reward_type
        )

        return model, tokenizer, self.device

    def set_model_max_len(
        self, config, tokenizer, verbose=False, reward_type=False
    ):
        if reward_type:
            # limit deberta, else uses too much memory and not worth response score
            tokenizer.model_max_length = 512
        if hasattr(config, "max_seq_len") and isinstance(
            config.max_seq_len, int
        ):
            tokenizer.model_max_length = config.max_seq_len
        elif hasattr(config, "max_position_embeddings") and isinstance(
            config.max_position_embeddings, int
        ):
            # help automatically limit inputs to generate
            tokenizer.model_max_length = config.max_position_embeddings
        else:
            if verbose:
                print(
                    "Could not determine model_max_length, setting to 2048",
                    flush=True,
                )
            tokenizer.model_max_length = 2048
        # for bug in HF transformers
        if tokenizer.model_max_length > 100000000:
            tokenizer.model_max_length = 2048

    def pop_unused_model_kwargs(self, model_kwargs):
        """
        in-place pop unused kwargs that are not dependency-upgrade friendly
        no point passing in False, is default, and helps avoid needing to update requirements for new deps
        :param model_kwargs:
        :return:
        """
        check_list = ["load_in_8bit", "load_in_4bit"]
        for k in check_list:
            if k in model_kwargs and not model_kwargs[k]:
                model_kwargs.pop(k)

    def get_score_model(
        self,
        score_model: str = None,
        load_8bit: bool = False,
        load_4bit: bool = False,
        load_half: bool = True,
        load_gptq: str = "",
        infer_devices: bool = True,
        base_model: str = "",
        inference_server: str = "",
        tokenizer_base_model: str = "",
        lora_weights: str = "",
        gpu_id: int = 0,
        reward_type: bool = None,
        local_files_only: bool = False,
        resume_download: bool = True,
        use_auth_token: Union[str, bool] = False,
        trust_remote_code: bool = True,
        offload_folder: str = None,
        compile_model: bool = True,
        verbose: bool = False,
    ):
        if score_model is not None and score_model.strip():
            load_8bit = False
            load_4bit = False
            load_half = False
            load_gptq = ""
            use_safetensors = False
            base_model = score_model.strip()
            tokenizer_base_model = ""
            lora_weights = ""
            inference_server = ""
            llama_type = False
            compile_model = False
            smodel, stokenizer, _ = self.get_model(
                reward_type=True,
                **get_kwargs(
                    self.get_model, exclude_names=["reward_type"], **locals()
                ),
            )
        else:
            smodel, stokenizer, _ = None, None, None
        return smodel, stokenizer, self.device

    def evaluate(
        self,
        model_state,
        my_db_state,
        # START NOTE: Examples must have same order of parameters
        instruction,
        iinput,
        context,
        stream_output,
        prompt_type,
        prompt_dict,
        temperature,
        top_p,
        top_k,
        num_beams,
        max_new_tokens,
        min_new_tokens,
        early_stopping,
        max_time,
        repetition_penalty,
        num_return_sequences,
        do_sample,
        chat,
        instruction_nochat,
        iinput_nochat,
        langchain_mode,
        langchain_action,
        top_k_docs,
        chunk,
        chunk_size,
        document_choice,
        # END NOTE: Examples must have same order of parameters
        src_lang=None,
        tgt_lang=None,
        debug=False,
        concurrency_count=None,
        save_dir=None,
        sanitize_bot_response=False,
        model_state0=None,
        memory_restriction_level=None,
        max_max_new_tokens=None,
        is_public=None,
        max_max_time=None,
        raise_generate_gpu_exceptions=None,
        chat_context=None,
        lora_weights=None,
        load_db_if_exists=True,
        dbs=None,
        user_path=None,
        detect_user_path_changes_every_query=None,
        use_openai_embedding=None,
        use_openai_model=None,
        hf_embedding_model=None,
        db_type=None,
        n_jobs=None,
        first_para=None,
        text_limit=None,
        verbose=False,
        cli=False,
        reverse_docs=True,
        use_cache=None,
        auto_reduce_chunks=None,
        max_chunks=None,
        model_lock=None,
        force_langchain_evaluate=None,
        model_state_none=None,
    ):
        # ensure passed these
        assert concurrency_count is not None
        assert memory_restriction_level is not None
        assert raise_generate_gpu_exceptions is not None
        assert chat_context is not None
        assert use_openai_embedding is not None
        assert use_openai_model is not None
        assert hf_embedding_model is not None
        assert db_type is not None
        assert top_k_docs is not None and isinstance(top_k_docs, int)
        assert chunk is not None and isinstance(chunk, bool)
        assert chunk_size is not None and isinstance(chunk_size, int)
        assert n_jobs is not None
        assert first_para is not None

        if debug:
            locals_dict = locals().copy()
            locals_dict.pop("model_state", None)
            locals_dict.pop("model_state0", None)
            locals_dict.pop("model_states", None)
            print(locals_dict)

        no_model_msg = (
            "Please choose a base model with --base_model (CLI) or load in Models Tab (gradio).\n"
            "Then start New Conversation"
        )

        if model_state is None:
            model_state = model_state_none.copy()
        if model_state0 is None:
            # e.g. for no gradio case, set dummy value, else should be set
            model_state0 = model_state_none.copy()

        # model_state['model] is only 'model' if should use model_state0
        # model could also be None
        have_model_lock = model_lock is not None
        have_fresh_model = model_state["model"] not in [
            None,
            "model",
            no_model_str,
        ]
        # for gradio UI control, expect model_state and model_state0 to match, so if have_model_lock=True, then should have_fresh_model=True
        # but gradio API control will only use nochat api etc. and won't use fresh model, so can't assert in general
        # if have_model_lock:
        #    assert have_fresh_model, "Expected model_state and model_state0 to match if have_model_lock"
        have_cli_model = model_state0["model"] not in [
            None,
            "model",
            no_model_str,
        ]

        if have_fresh_model:
            # USE FRESH MODEL
            if not have_model_lock:
                # model_state0 is just one of model_state if model_lock, so don't nuke
                # try to free-up original model (i.e. list was passed as reference)
                if model_state0["model"] and hasattr(
                    model_state0["model"], "cpu"
                ):
                    model_state0["model"].cpu()
                    model_state0["model"] = None
                # try to free-up original tokenizer (i.e. list was passed as reference)
                if model_state0["tokenizer"]:
                    model_state0["tokenizer"] = None
                clear_torch_cache()
            chosen_model_state = model_state
        elif have_cli_model:
            # USE MODEL SETUP AT CLI
            assert isinstance(
                model_state["model"], str
            )  # expect no fresh model
            chosen_model_state = model_state0
        else:
            raise AssertionError(no_model_msg)
        # get variables
        model = chosen_model_state["model"]
        tokenizer = chosen_model_state["tokenizer"]
        base_model = chosen_model_state["base_model"]
        tokenizer_base_model = chosen_model_state["tokenizer_base_model"]
        lora_weights = chosen_model_state["lora_weights"]
        inference_server = chosen_model_state["inference_server"]
        # prefer use input from API over model state
        prompt_type = prompt_type or chosen_model_state["prompt_type"]
        prompt_dict = prompt_dict or chosen_model_state["prompt_dict"]

        if base_model is None:
            raise AssertionError(no_model_msg)

        assert base_model.strip(), no_model_msg
        assert model, "Model is missing"
        assert tokenizer, "Tokenizer is missing"

        # choose chat or non-chat mode
        print(instruction)
        if not chat:
            instruction = instruction_nochat
            iinput = iinput_nochat
        print(instruction)

        # in some cases, like lean nochat API, don't want to force sending prompt_type, allow default choice
        model_lower = base_model.lower()
        if (
            not prompt_type
            and model_lower in inv_prompt_type_to_model_lower
            and prompt_type != "custom"
        ):
            prompt_type = inv_prompt_type_to_model_lower[model_lower]
            if verbose:
                print(
                    "Auto-selecting prompt_type=%s for %s"
                    % (prompt_type, model_lower),
                    flush=True,
                )
        assert prompt_type is not None, "prompt_type was None"

        # Control generation hyperparameters
        # adjust for bad inputs, e.g. in case also come from API that doesn't get constrained by gradio sliders
        # below is for TGI server, not required for HF transformers
        # limits are chosen similar to gradio_runner.py sliders/numbers
        top_p = min(max(1e-3, top_p), 1.0 - 1e-3)
        top_k = min(max(1, int(top_k)), 100)
        temperature = min(max(0.01, temperature), 2.0)
        # FIXME: https://github.com/h2oai/h2ogpt/issues/106
        num_beams = (
            1 if stream_output else num_beams
        )  # See max_beams in gradio_runner
        max_max_new_tokens = self.get_max_max_new_tokens(
            chosen_model_state,
            memory_restriction_level=memory_restriction_level,
            max_new_tokens=max_new_tokens,
            max_max_new_tokens=max_max_new_tokens,
        )
        model_max_length = 2048  # get_model_max_length(chosen_model_state)
        max_new_tokens = min(max(1, int(max_new_tokens)), max_max_new_tokens)
        min_new_tokens = min(max(0, int(min_new_tokens)), max_new_tokens)
        max_time = min(max(0, max_time), max_max_time)
        repetition_penalty = min(max(0.01, repetition_penalty), 3.0)
        num_return_sequences = (
            1 if chat else min(max(1, int(num_return_sequences)), 10)
        )
        (
            min_top_k_docs,
            max_top_k_docs,
            label_top_k_docs,
        ) = self.get_minmax_top_k_docs(is_public)
        top_k_docs = min(max(min_top_k_docs, int(top_k_docs)), max_top_k_docs)
        chunk_size = min(max(128, int(chunk_size)), 2048)
        if not context:
            # get hidden context if have one
            context = self.get_context(chat_context, prompt_type)

        # restrict instruction, typically what has large input
        from h2oai_pipeline import H2OTextGenerationPipeline

        print(instruction)
        (
            instruction,
            num_prompt_tokens1,
        ) = H2OTextGenerationPipeline.limit_prompt(instruction, tokenizer)
        context, num_prompt_tokens2 = H2OTextGenerationPipeline.limit_prompt(
            context, tokenizer
        )
        iinput, num_prompt_tokens3 = H2OTextGenerationPipeline.limit_prompt(
            iinput, tokenizer
        )
        num_prompt_tokens = (
            (num_prompt_tokens1 or 0)
            + (num_prompt_tokens2 or 0)
            + (num_prompt_tokens3 or 0)
        )

        # get prompt
        prompter = Prompter(
            prompt_type,
            prompt_dict,
            debug=debug,
            chat=chat,
            stream_output=stream_output,
        )
        data_point = dict(
            context=context, instruction=instruction, input=iinput
        )
        prompt = prompter.generate_prompt(data_point)

        # THIRD PLACE where LangChain referenced, but imports only occur if enabled and have db to use
        assert langchain_mode in langchain_modes, (
            "Invalid langchain_mode %s" % langchain_mode
        )
        assert langchain_action in langchain_actions, (
            "Invalid langchain_action %s" % langchain_action
        )
        if (
            langchain_mode in ["MyData"]
            and my_db_state is not None
            and len(my_db_state) > 0
            and my_db_state[0] is not None
        ):
            db1 = my_db_state[0]
        elif dbs is not None and langchain_mode in dbs:
            db1 = dbs[langchain_mode]
        else:
            db1 = None
        do_langchain_path = (
            langchain_mode not in [False, "Disabled", "ChatLLM", "LLM"]
            or base_model in non_hf_types
            or force_langchain_evaluate
        )
        if do_langchain_path:
            outr = ""
            # use smaller cut_distanct for wiki_full since so many matches could be obtained, and often irrelevant unless close
            from gpt_langchain import run_qa_db

            gen_hyper_langchain = dict(
                do_sample=do_sample,
                temperature=temperature,
                repetition_penalty=repetition_penalty,
                top_k=top_k,
                top_p=top_p,
                num_beams=num_beams,
                min_new_tokens=min_new_tokens,
                max_new_tokens=max_new_tokens,
                early_stopping=early_stopping,
                max_time=max_time,
                num_return_sequences=num_return_sequences,
            )
            out = run_qa_db(
                query=instruction,
                iinput=iinput,
                context=context,
                model_name=base_model,
                model=model,
                tokenizer=tokenizer,
                inference_server=inference_server,
                stream_output=stream_output,
                prompter=prompter,
                load_db_if_exists=load_db_if_exists,
                db=db1,
                user_path=user_path,
                detect_user_path_changes_every_query=detect_user_path_changes_every_query,
                cut_distanct=1.1
                if langchain_mode in ["wiki_full"]
                else 1.64,  # FIXME, too arbitrary
                use_openai_embedding=use_openai_embedding,
                use_openai_model=use_openai_model,
                hf_embedding_model=hf_embedding_model,
                first_para=first_para,
                text_limit=text_limit,
                chunk=chunk,
                chunk_size=chunk_size,
                langchain_mode=langchain_mode,
                langchain_action=langchain_action,
                document_choice=document_choice,
                db_type=db_type,
                top_k_docs=top_k_docs,
                **gen_hyper_langchain,
                prompt_type=prompt_type,
                prompt_dict=prompt_dict,
                n_jobs=n_jobs,
                verbose=verbose,
                cli=cli,
                sanitize_bot_response=sanitize_bot_response,
                reverse_docs=reverse_docs,
                lora_weights=lora_weights,
                auto_reduce_chunks=auto_reduce_chunks,
                max_chunks=max_chunks,
                device=self.device,
            )
            return out

    inputs_list_names = list(inspect.signature(evaluate).parameters)
    global inputs_kwargs_list
    inputs_kwargs_list = [
        x
        for x in inputs_list_names
        if x not in eval_func_param_names + ["model_state", "my_db_state"]
    ]

    def get_cutoffs(
        self,
        memory_restriction_level,
        for_context=False,
        model_max_length=2048,
    ):
        # help to avoid errors like:
        # RuntimeError: The size of tensor a (2048) must match the size of tensor b (2049) at non-singleton dimension 3
        # RuntimeError: expected scalar type Half but found Float
        # with - 256
        if memory_restriction_level > 0:
            max_length_tokenize = (
                768 - 256 if memory_restriction_level <= 2 else 512 - 256
            )
        else:
            # at least give room for 1 paragraph output
            max_length_tokenize = model_max_length - 256
        cutoff_len = (
            max_length_tokenize * 4
        )  # if reaches limit, then can't generate new tokens
        output_smallest = 30 * 4
        max_prompt_length = cutoff_len - output_smallest

        if for_context:
            # then lower even more to avoid later chop, since just estimate tokens in context bot
            max_prompt_length = max(64, int(max_prompt_length * 0.8))

        return (
            cutoff_len,
            output_smallest,
            max_length_tokenize,
            max_prompt_length,
        )

    def generate_with_exceptions(
        self,
        func,
        *args,
        prompt="",
        inputs_decoded="",
        raise_generate_gpu_exceptions=True,
        **kwargs,
    ):
        try:
            func(*args, **kwargs)
        except torch.cuda.OutOfMemoryError as e:
            print(
                "GPU OOM 2: prompt: %s inputs_decoded: %s exception: %s"
                % (prompt, inputs_decoded, str(e)),
                flush=True,
            )
            if "input_ids" in kwargs:
                if kwargs["input_ids"] is not None:
                    kwargs["input_ids"].cpu()
                kwargs["input_ids"] = None
            traceback.print_exc()
            clear_torch_cache()
            return
        except (Exception, RuntimeError) as e:
            if (
                "Expected all tensors to be on the same device" in str(e)
                or "expected scalar type Half but found Float" in str(e)
                or "probability tensor contains either" in str(e)
                or "cublasLt ran into an error!" in str(e)
                or "mat1 and mat2 shapes cannot be multiplied" in str(e)
            ):
                print(
                    "GPU Error: prompt: %s inputs_decoded: %s exception: %s"
                    % (prompt, inputs_decoded, str(e)),
                    flush=True,
                )
                traceback.print_exc()
                clear_torch_cache()
                if raise_generate_gpu_exceptions:
                    raise
                return
            else:
                clear_torch_cache()
                if raise_generate_gpu_exceptions:
                    raise

    def get_generate_params(
        self,
        model_lower,
        chat,
        stream_output,
        show_examples,
        prompt_type,
        prompt_dict,
        temperature,
        top_p,
        top_k,
        num_beams,
        max_new_tokens,
        min_new_tokens,
        early_stopping,
        max_time,
        repetition_penalty,
        num_return_sequences,
        do_sample,
        top_k_docs,
        chunk,
        chunk_size,
        verbose,
    ):
        use_defaults = False
        use_default_examples = True
        examples = []
        task_info = "LLM"
        if model_lower:
            print(f"Using Model {model_lower}", flush=True)
        else:
            if verbose:
                print("No model defined yet", flush=True)

        min_new_tokens = min_new_tokens if min_new_tokens is not None else 0
        early_stopping = (
            early_stopping if early_stopping is not None else False
        )
        max_time_defaults = 60 * 3
        max_time = max_time if max_time is not None else max_time_defaults

        if (
            not prompt_type
            and model_lower in inv_prompt_type_to_model_lower
            and prompt_type != "custom"
        ):
            prompt_type = inv_prompt_type_to_model_lower[model_lower]
            if verbose:
                print(
                    "Auto-selecting prompt_type=%s for %s"
                    % (prompt_type, model_lower),
                    flush=True,
                )

        # examples at first don't include chat, instruction_nochat, iinput_nochat, added at end
        if show_examples is None:
            if chat:
                show_examples = False
            else:
                show_examples = True

        summarize_example1 = """Jeff: Can I train a ? Transformers model on Amazon SageMaker? 
    Philipp: Sure you can use the new Hugging Face Deep Learning Container. 
    Jeff: ok.
    Jeff: and how can I get started? 
    Jeff: where can I find documentation? 
    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face"""

        use_placeholder_instruction_as_example = False
        if (
            "bart-large-cnn-samsum" in model_lower
            or "flan-t5-base-samsum" in model_lower
        ):
            placeholder_instruction = summarize_example1
            placeholder_input = ""
            use_defaults = True
            use_default_examples = False
            use_placeholder_instruction_as_example = True
            task_info = "Summarization"
        elif (
            "t5-" in model_lower
            or "t5" == model_lower
            or "flan-" in model_lower
        ):
            placeholder_instruction = "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?"
            placeholder_input = ""
            use_defaults = True
            use_default_examples = True
            task_info = "Multi-Task: Q/A, translation, Chain-of-Thought, Logical Reasoning, Summarization, etc.  Best to use task prefix as trained on, e.g. `translate English to German: ` (space after colon)"
        elif "mbart-" in model_lower:
            placeholder_instruction = "The girl has long hair."
            placeholder_input = ""
            use_defaults = True
            use_default_examples = False
            use_placeholder_instruction_as_example = True
        elif "gpt2" in model_lower:
            placeholder_instruction = "The sky is"
            placeholder_input = ""
            prompt_type = prompt_type or "plain"
            use_default_examples = (
                True  # some will be odd "continuations" but can be ok
            )
            use_placeholder_instruction_as_example = True
            task_info = "Auto-complete phrase, code, etc."
            use_defaults = True
        else:
            if chat:
                placeholder_instruction = ""
            else:
                placeholder_instruction = "Give detailed answer for whether Einstein or Newton is smarter."
            placeholder_input = ""
            if model_lower in inv_prompt_type_to_model_lower:
                if prompt_type != "custom":
                    prompt_type = inv_prompt_type_to_model_lower[model_lower]
            elif model_lower:
                # default is plain, because might rely upon trust_remote_code to handle prompting
                prompt_type = prompt_type or "plain"
            else:
                prompt_type = ""
            task_info = "No task"
            if prompt_type == "instruct":
                task_info = "Answer question or follow imperative as instruction with optionally input."
            elif prompt_type == "plain":
                task_info = "Auto-complete phrase, code, etc."
            elif prompt_type == "human_bot":
                if chat:
                    task_info = "Chat (Shift-Enter to give question/imperative, input concatenated with instruction)"
                else:
                    task_info = "Ask question/imperative (input concatenated with instruction)"

        # revert to plain if still nothing
        prompt_type = prompt_type or "plain"
        if use_defaults:
            temperature = 1.0 if temperature is None else temperature
            top_p = 1.0 if top_p is None else top_p
            top_k = 40 if top_k is None else top_k
            num_beams = num_beams or 1
            max_new_tokens = max_new_tokens or 128
            repetition_penalty = repetition_penalty or 1.07
            num_return_sequences = min(num_beams, num_return_sequences or 1)
            do_sample = False if do_sample is None else do_sample
        else:
            temperature = 0.1 if temperature is None else temperature
            top_p = 0.75 if top_p is None else top_p
            top_k = 40 if top_k is None else top_k
            num_beams = num_beams or 1
            max_new_tokens = max_new_tokens or 256
            repetition_penalty = repetition_penalty or 1.07
            num_return_sequences = min(num_beams, num_return_sequences or 1)
            do_sample = False if do_sample is None else do_sample
        # doesn't include chat, instruction_nochat, iinput_nochat, added later
        params_list = [
            "",
            stream_output,
            prompt_type,
            prompt_dict,
            temperature,
            top_p,
            top_k,
            num_beams,
            max_new_tokens,
            min_new_tokens,
            early_stopping,
            max_time,
            repetition_penalty,
            num_return_sequences,
            do_sample,
        ]

        if use_placeholder_instruction_as_example:
            examples += [[placeholder_instruction, ""] + params_list]

        if use_default_examples:
            examples += [
                ["Translate English to French", "Good morning"] + params_list,
                [
                    "Give detailed answer for whether Einstein or Newton is smarter.",
                    "",
                ]
                + params_list,
                [
                    "Explain in detailed list, all the best practices for coding in python.",
                    "",
                ]
                + params_list,
                [
                    "Create a markdown table with 3 rows for the primary colors, and 2 columns, with color name and hex codes.",
                    "",
                ]
                + params_list,
                ["Translate to German:  My name is Arthur", ""] + params_list,
                [
                    "Please answer to the following question. Who is going to be the next Ballon d'or?",
                    "",
                ]
                + params_list,
                [
                    "Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.",
                    "",
                ]
                + params_list,
                [
                    "Please answer the following question. What is the boiling point of Nitrogen?",
                    "",
                ]
                + params_list,
                [
                    "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?",
                    "",
                ]
                + params_list,
                [
                    "Simplify the following expression: (False or False and True). Explain your answer.",
                    "",
                ]
                + params_list,
                [
                    "Premise: At my age you will probably have learnt one lesson. Hypothesis:  It's not certain how many lessons you'll learn by your thirties. Does the premise entail the hypothesis?",
                    "",
                ]
                + params_list,
                [
                    "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?",
                    "",
                ]
                + params_list,
                [
                    "Answer the following question by reasoning step by step.  The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?",
                    "",
                ]
                + params_list,
                [
                    """def area_of_rectangle(a: float, b: float):
        \"\"\"Return the area of the rectangle.\"\"\"""",
                    "",
                ]
                + params_list,
                [
                    """# a function in native python:
    def mean(a):
        return sum(a)/len(a)

    # the same function using numpy:
    import numpy as np
    def mean(a):""",
                    "",
                ]
                + params_list,
                [
                    """X = np.random.randn(100, 100)
    y = np.random.randint(0, 1, 100)

    # fit random forest classifier with 20 estimators""",
                    "",
                ]
                + params_list,
            ]
        # add summary example
        examples += [
            [
                summarize_example1,
                "Summarize"
                if prompt_type not in ["plain", "instruct_simple"]
                else "",
            ]
            + params_list
        ]

        src_lang = "English"
        tgt_lang = "Russian"

        # move to correct position
        for example in examples:
            example += [
                chat,
                "",
                "",
                "Disabled",
                LangChainAction.QUERY.value,
                top_k_docs,
                chunk,
                chunk_size,
                [DocumentChoices.All_Relevant.name],
            ]
            # adjust examples if non-chat mode
            if not chat:
                example[
                    eval_func_param_names.index("instruction_nochat")
                ] = example[eval_func_param_names.index("instruction")]
                example[eval_func_param_names.index("instruction")] = ""

                example[
                    eval_func_param_names.index("iinput_nochat")
                ] = example[eval_func_param_names.index("iinput")]
                example[eval_func_param_names.index("iinput")] = ""
            assert len(example) == len(
                eval_func_param_names
            ), "Wrong example: %s %s" % (
                len(example),
                len(eval_func_param_names),
            )

        if prompt_type == PromptType.custom.name and not prompt_dict:
            raise ValueError(
                "Unexpected to get non-empty prompt_dict=%s for prompt_type=%s"
                % (prompt_dict, prompt_type)
            )

        # get prompt_dict from prompt_type, so user can see in UI etc., or for custom do nothing except check format
        prompt_dict, error0 = get_prompt(
            prompt_type,
            prompt_dict,
            chat=False,
            context="",
            reduced=False,
            making_context=False,
            return_dict=True,
        )
        if error0:
            raise RuntimeError("Prompt wrong: %s" % error0)

        return (
            placeholder_instruction,
            placeholder_input,
            stream_output,
            show_examples,
            prompt_type,
            prompt_dict,
            temperature,
            top_p,
            top_k,
            num_beams,
            max_new_tokens,
            min_new_tokens,
            early_stopping,
            max_time,
            repetition_penalty,
            num_return_sequences,
            do_sample,
            src_lang,
            tgt_lang,
            examples,
            task_info,
        )

    def languages_covered(self):
        # https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt#languages-covered
        covered = """Arabic (ar_AR), Czech (cs_CZ), German (de_DE), English (en_XX), Spanish (es_XX), Estonian (et_EE), Finnish (fi_FI), French (fr_XX), Gujarati (gu_IN), Hindi (hi_IN), Italian (it_IT), Japanese (ja_XX), Kazakh (kk_KZ), Korean (ko_KR), Lithuanian (lt_LT), Latvian (lv_LV), Burmese (my_MM), Nepali (ne_NP), Dutch (nl_XX), Romanian (ro_RO), Russian (ru_RU), Sinhala (si_LK), Turkish (tr_TR), Vietnamese (vi_VN), Chinese (zh_CN), Afrikaans (af_ZA), Azerbaijani (az_AZ), Bengali (bn_IN), Persian (fa_IR), Hebrew (he_IL), Croatian (hr_HR), Indonesian (id_ID), Georgian (ka_GE), Khmer (km_KH), Macedonian (mk_MK), Malayalam (ml_IN), Mongolian (mn_MN), Marathi (mr_IN), Polish (pl_PL), Pashto (ps_AF), Portuguese (pt_XX), Swedish (sv_SE), Swahili (sw_KE), Tamil (ta_IN), Telugu (te_IN), Thai (th_TH), Tagalog (tl_XX), Ukrainian (uk_UA), Urdu (ur_PK), Xhosa (xh_ZA), Galician (gl_ES), Slovene (sl_SI)"""
        covered = covered.split(", ")
        covered = {
            x.split(" ")[0]: x.split(" ")[1].replace(")", "").replace("(", "")
            for x in covered
        }
        return covered

    def get_context(self, chat_context, prompt_type):
        if chat_context and prompt_type == "human_bot":
            context0 = """<bot>: I am an intelligent, helpful, truthful, and fair assistant named h2oGPT, who will give accurate, balanced, and reliable responses.  I will not respond with I don't know or I don't understand.
    <human>: I am a human person seeking useful assistance and request all questions be answered completely, and typically expect detailed responses.  Give answers in numbered list format if several distinct but related items are being listed."""
        else:
            context0 = ""
        return context0

    def score_qa(
        self,
        smodel,
        stokenizer,
        max_length_tokenize,
        question,
        answer,
        cutoff_len,
    ):
        question = question[-cutoff_len:]
        answer = answer[-cutoff_len:]

        inputs = stokenizer(
            question,
            answer,
            return_tensors="pt",
            truncation=True,
            max_length=max_length_tokenize,
        ).to(smodel.device)
        try:
            score = (
                torch.sigmoid(smodel(**inputs).logits[0])
                .cpu()
                .detach()
                .numpy()[0]
            )
        except torch.cuda.OutOfMemoryError as e:
            print(
                "GPU OOM 3: question: %s answer: %s exception: %s"
                % (question, answer, str(e)),
                flush=True,
            )
            del inputs
            traceback.print_exc()
            clear_torch_cache()
            return "Response Score: GPU OOM"
        except (Exception, RuntimeError) as e:
            if (
                "Expected all tensors to be on the same device" in str(e)
                or "expected scalar type Half but found Float" in str(e)
                or "probability tensor contains either" in str(e)
                or "cublasLt ran into an error!" in str(e)
                or "device-side assert triggered" in str(e)
            ):
                print(
                    "GPU Error: question: %s answer: %s exception: %s"
                    % (question, answer, str(e)),
                    flush=True,
                )
                traceback.print_exc()
                clear_torch_cache()
                return "Response Score: GPU Error"
            else:
                raise
        os.environ["TOKENIZERS_PARALLELISM"] = "true"
        return score

    def check_locals(self, **kwargs):
        # ensure everything in evaluate is here
        can_skip_because_locally_generated = no_default_param_names + [
            # get_model:
            "reward_type"
        ]
        for k in eval_func_param_names:
            if k in can_skip_because_locally_generated:
                continue
            assert k in kwargs, "Missing %s" % k
        for k in inputs_kwargs_list:
            if k in can_skip_because_locally_generated:
                continue
            assert k in kwargs, "Missing %s" % k

        for k in list(inspect.signature(self.get_model).parameters):
            if k in can_skip_because_locally_generated:
                continue
            assert k in kwargs, "Missing %s" % k

    def get_model_max_length(self, model_state):
        if not isinstance(model_state["tokenizer"], (str, types.NoneType)):
            return model_state["tokenizer"].model_max_length
        else:
            return 2048

    def get_max_max_new_tokens(self, model_state, **kwargs):
        if not isinstance(model_state["tokenizer"], (str, types.NoneType)):
            max_max_new_tokens = model_state["tokenizer"].model_max_length
        else:
            max_max_new_tokens = None

        if (
            kwargs["max_max_new_tokens"] is not None
            and max_max_new_tokens is not None
        ):
            return min(max_max_new_tokens, kwargs["max_max_new_tokens"])
        elif kwargs["max_max_new_tokens"] is not None:
            return kwargs["max_max_new_tokens"]
        elif kwargs["memory_restriction_level"] == 1:
            return 768
        elif kwargs["memory_restriction_level"] == 2:
            return 512
        elif kwargs["memory_restriction_level"] >= 3:
            return 256
        else:
            # FIXME: Need to update after new model loaded, so user can control with slider
            return 2048

    def get_minmax_top_k_docs(self, is_public):
        if is_public:
            min_top_k_docs = 1
            max_top_k_docs = 3
            label_top_k_docs = "Number of document chunks"
        else:
            min_top_k_docs = -1
            max_top_k_docs = 100
            label_top_k_docs = (
                "Number of document chunks (-1 = auto fill model context)"
            )
        return min_top_k_docs, max_top_k_docs, label_top_k_docs

    def history_to_context(
        self,
        history,
        langchain_mode1,
        prompt_type1,
        prompt_dict1,
        chat1,
        model_max_length1,
        memory_restriction_level1,
        keep_sources_in_context1,
    ):
        """
        consumes all history up to (but not including) latest history item that is presumed to be an [instruction, None] pair
        :param history:
        :param langchain_mode1:
        :param prompt_type1:
        :param prompt_dict1:
        :param chat1:
        :param model_max_length1:
        :param memory_restriction_level1:
        :param keep_sources_in_context1:
        :return:
        """
        # ensure output will be unique to models
        _, _, _, max_prompt_length = self.get_cutoffs(
            memory_restriction_level1,
            for_context=True,
            model_max_length=model_max_length1,
        )
        context1 = ""
        if max_prompt_length is not None and langchain_mode1 not in ["LLM"]:
            context1 = ""
            # - 1 below because current instruction already in history from user()
            for histi in range(0, len(history) - 1):
                data_point = dict(
                    instruction=history[histi][0],
                    input="",
                    output=history[histi][1],
                )
                (
                    prompt,
                    pre_response,
                    terminate_response,
                    chat_sep,
                    chat_turn_sep,
                ) = generate_prompt(
                    data_point,
                    prompt_type1,
                    prompt_dict1,
                    chat1,
                    reduced=True,
                    making_context=True,
                )
                # md -> back to text, maybe not super important if model trained enough
                if (
                    not keep_sources_in_context1
                    and langchain_mode1 != "Disabled"
                    and prompt.find(source_prefix) >= 0
                ):
                    # FIXME: This is relatively slow even for small amount of text, like 0.3s each history item
                    import re

                    prompt = re.sub(
                        f"{re.escape(source_prefix)}.*?{re.escape(source_postfix)}",
                        "",
                        prompt,
                        flags=re.DOTALL,
                    )
                    if prompt.endswith("\n<p>"):
                        prompt = prompt[:-4]
                prompt = prompt.replace("<br>", chat_turn_sep)
                if not prompt.endswith(chat_turn_sep):
                    prompt += chat_turn_sep
                # most recent first, add older if can
                # only include desired chat history
                if len(prompt + context1) > max_prompt_length:
                    break
                context1 += prompt

            (
                _,
                pre_response,
                terminate_response,
                chat_sep,
                chat_turn_sep,
            ) = generate_prompt(
                {},
                prompt_type1,
                prompt_dict1,
                chat1,
                reduced=True,
                making_context=True,
            )
            if context1 and not context1.endswith(chat_turn_sep):
                context1 += chat_turn_sep  # ensure if terminates abruptly, then human continues on next line
        return context1


class H2OTextIteratorStreamer(TextIteratorStreamer):
    """
    normally, timeout required for now to handle exceptions, else get()
    but with H2O version of TextIteratorStreamer, loop over block to handle
    """

    def __init__(
        self,
        tokenizer,
        skip_prompt: bool = False,
        timeout: typing.Optional[float] = None,
        block=True,
        **decode_kwargs,
    ):
        super().__init__(tokenizer, skip_prompt, **decode_kwargs)
        self.text_queue = queue.Queue()
        self.stop_signal = None
        self.do_stop = False
        self.timeout = timeout
        self.block = block

    def on_finalized_text(self, text: str, stream_end: bool = False):
        """Put the new text in the queue. If the stream is ending, also put a stop signal in the queue."""
        self.text_queue.put(text, timeout=self.timeout)
        if stream_end:
            self.text_queue.put(self.stop_signal, timeout=self.timeout)

    def __iter__(self):
        return self

    def __next__(self):
        while True:
            try:
                value = (
                    self.stop_signal
                )  # value looks unused in pycharm, not true
                if self.do_stop:
                    print("hit stop", flush=True)
                    # could raise or break, maybe best to raise and make parent see if any exception in thread
                    self.clear_queue()
                    self.do_stop = False
                    raise StopIteration()
                    # break
                value = self.text_queue.get(
                    block=self.block, timeout=self.timeout
                )
                break
            except queue.Empty:
                time.sleep(0.01)
        if value == self.stop_signal:
            self.clear_queue()
            self.do_stop = False
            raise StopIteration()
        else:
            return value

    def clear_queue(self):
        # make sure streamer is reusable after stop hit
        with self.text_queue.mutex:
            self.text_queue.queue.clear()


def entrypoint_main():
    """
    Examples:

    WORLD_SIZE=4 CUDA_VISIBLE_DEVICES="0,1,2,3" torchrun --nproc_per_node=4 --master_port=1234 generate.py --base_model='EleutherAI/gpt-j-6B' --lora_weights=lora-alpaca_6B
    python generate.py --base_model='EleutherAI/gpt-j-6B' --lora_weights='lora-alpaca_6B'
    python generate.py --base_model='EleutherAI/gpt-neox-20b' --lora_weights='lora-alpaca_20B'

    # generate without lora weights, no prompt
    python generate.py --base_model='EleutherAI/gpt-neox-20b' --prompt_type='plain'
    python generate.py --base_model='togethercomputer/GPT-NeoXT-Chat-Base-20B' --prompt_type='dai_faq'

    python generate.py --base_model='togethercomputer/GPT-NeoXT-Chat-Base-20B' --prompt_type='dai_faq' --lora_weights='lora_20B_daifaq'
    # OpenChatKit settings:
    python generate.py --base_model='togethercomputer/GPT-NeoXT-Chat-Base-20B' --prompt_type='human_bot --debug=True --num_beams=1 --temperature=0.6 --top_k=40 --top_p=1.0

    python generate.py --base_model='distilgpt2' --prompt_type='plain' --debug=True --num_beams=1 --temperature=0.6 --top_k=40 --top_p=1.0 --share=False
    python generate.py --base_model='t5-large' --prompt_type='simple_instruct'
    python generate.py --base_model='philschmid/bart-large-cnn-samsum'
    python generate.py --base_model='philschmid/flan-t5-base-samsum'
    python generate.py --base_model='facebook/mbart-large-50-many-to-many-mmt'

    python generate.py --base_model='togethercomputer/GPT-NeoXT-Chat-Base-20B' --prompt_type='human_bot' --lora_weights='GPT-NeoXT-Chat-Base-20B.merged.json.8_epochs.57b2892c53df5b8cefac45f84d019cace803ef26.28'

    must have 4*48GB GPU and run without 8bit in order for sharding to work with infer_devices=False
    can also pass --prompt_type='human_bot' and model can somewhat handle instructions without being instruct tuned
    python generate.py --base_model=decapoda-research/llama-65b-hf --load_8bit=False --infer_devices=False --prompt_type='human_bot'

    python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b
    """
    import fire

    langchain = Langchain()

    fire.Fire(langchain.main)


if __name__ == "__main__":
    entrypoint_main()

```

`apps/language_models/langchain/gpt4all_llm.py`:

```py
import inspect
import os
from functools import partial
from typing import Dict, Any, Optional, List
from langchain.callbacks.manager import CallbackManagerForLLMRun
from pydantic import root_validator
from langchain.llms import gpt4all
from dotenv import dotenv_values

from utils import FakeTokenizer


def get_model_tokenizer_gpt4all(base_model, **kwargs):
    # defaults (some of these are generation parameters, so need to be passed in at generation time)
    model_kwargs = dict(
        n_threads=os.cpu_count() // 2,
        temp=kwargs.get("temperature", 0.2),
        top_p=kwargs.get("top_p", 0.75),
        top_k=kwargs.get("top_k", 40),
        n_ctx=2048 - 256,
    )
    env_gpt4all_file = ".env_gpt4all"
    model_kwargs.update(dotenv_values(env_gpt4all_file))
    # make int or float if can to satisfy types for class
    for k, v in model_kwargs.items():
        try:
            if float(v) == int(v):
                model_kwargs[k] = int(v)
            else:
                model_kwargs[k] = float(v)
        except:
            pass

    if base_model == "llama":
        if "model_path_llama" not in model_kwargs:
            raise ValueError("No model_path_llama in %s" % env_gpt4all_file)
        model_path = model_kwargs.pop("model_path_llama")
        # FIXME: GPT4All version of llama doesn't handle new quantization, so use llama_cpp_python
        from llama_cpp import Llama

        # llama sets some things at init model time, not generation time
        func_names = list(inspect.signature(Llama.__init__).parameters)
        model_kwargs = {
            k: v for k, v in model_kwargs.items() if k in func_names
        }
        model_kwargs["n_ctx"] = int(model_kwargs["n_ctx"])
        model = Llama(model_path=model_path, **model_kwargs)
    elif base_model in "gpt4all_llama":
        if (
            "model_name_gpt4all_llama" not in model_kwargs
            and "model_path_gpt4all_llama" not in model_kwargs
        ):
            raise ValueError(
                "No model_name_gpt4all_llama or model_path_gpt4all_llama in %s"
                % env_gpt4all_file
            )
        model_name = model_kwargs.pop("model_name_gpt4all_llama")
        model_type = "llama"
        from gpt4all import GPT4All as GPT4AllModel

        model = GPT4AllModel(model_name=model_name, model_type=model_type)
    elif base_model in "gptj":
        if (
            "model_name_gptj" not in model_kwargs
            and "model_path_gptj" not in model_kwargs
        ):
            raise ValueError(
                "No model_name_gpt4j or model_path_gpt4j in %s"
                % env_gpt4all_file
            )
        model_name = model_kwargs.pop("model_name_gptj")
        model_type = "gptj"
        from gpt4all import GPT4All as GPT4AllModel

        model = GPT4AllModel(model_name=model_name, model_type=model_type)
    else:
        raise ValueError("No such base_model %s" % base_model)
    return model, FakeTokenizer(), "cpu"


from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


class H2OStreamingStdOutCallbackHandler(StreamingStdOutCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        # streaming to std already occurs without this
        # sys.stdout.write(token)
        # sys.stdout.flush()
        pass


def get_model_kwargs(env_kwargs, default_kwargs, cls, exclude_list=[]):
    # default from class
    model_kwargs = {
        k: v.default
        for k, v in dict(inspect.signature(cls).parameters).items()
        if k not in exclude_list
    }
    # from our defaults
    model_kwargs.update(default_kwargs)
    # from user defaults
    model_kwargs.update(env_kwargs)
    # ensure only valid keys
    func_names = list(inspect.signature(cls).parameters)
    model_kwargs = {k: v for k, v in model_kwargs.items() if k in func_names}
    return model_kwargs


def get_llm_gpt4all(
    model_name,
    model=None,
    max_new_tokens=256,
    temperature=0.1,
    repetition_penalty=1.0,
    top_k=40,
    top_p=0.7,
    streaming=False,
    callbacks=None,
    prompter=None,
    verbose=False,
):
    assert prompter is not None
    env_gpt4all_file = ".env_gpt4all"
    env_kwargs = dotenv_values(env_gpt4all_file)
    n_ctx = env_kwargs.pop("n_ctx", 2048 - max_new_tokens)
    default_kwargs = dict(
        context_erase=0.5,
        n_batch=1,
        n_ctx=n_ctx,
        n_predict=max_new_tokens,
        repeat_last_n=64 if repetition_penalty != 1.0 else 0,
        repeat_penalty=repetition_penalty,
        temp=temperature,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        use_mlock=True,
        verbose=verbose,
    )
    if model_name == "llama":
        cls = H2OLlamaCpp
        model_path = (
            env_kwargs.pop("model_path_llama") if model is None else model
        )
        model_kwargs = get_model_kwargs(
            env_kwargs, default_kwargs, cls, exclude_list=["lc_kwargs"]
        )
        model_kwargs.update(
            dict(
                model_path=model_path,
                callbacks=callbacks,
                streaming=streaming,
                prompter=prompter,
            )
        )
        llm = cls(**model_kwargs)
        llm.client.verbose = verbose
    elif model_name == "gpt4all_llama":
        cls = H2OGPT4All
        model_path = (
            env_kwargs.pop("model_path_gpt4all_llama")
            if model is None
            else model
        )
        model_kwargs = get_model_kwargs(
            env_kwargs, default_kwargs, cls, exclude_list=["lc_kwargs"]
        )
        model_kwargs.update(
            dict(
                model=model_path,
                backend="llama",
                callbacks=callbacks,
                streaming=streaming,
                prompter=prompter,
            )
        )
        llm = cls(**model_kwargs)
    elif model_name == "gptj":
        cls = H2OGPT4All
        model_path = (
            env_kwargs.pop("model_path_gptj") if model is None else model
        )
        model_kwargs = get_model_kwargs(
            env_kwargs, default_kwargs, cls, exclude_list=["lc_kwargs"]
        )
        model_kwargs.update(
            dict(
                model=model_path,
                backend="gptj",
                callbacks=callbacks,
                streaming=streaming,
                prompter=prompter,
            )
        )
        llm = cls(**model_kwargs)
    else:
        raise RuntimeError("No such model_name %s" % model_name)
    return llm


class H2OGPT4All(gpt4all.GPT4All):
    model: Any
    prompter: Any
    """Path to the pre-trained GPT4All model file."""

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that the python package exists in the environment."""
        try:
            if isinstance(values["model"], str):
                from gpt4all import GPT4All as GPT4AllModel

                full_path = values["model"]
                model_path, delimiter, model_name = full_path.rpartition("/")
                model_path += delimiter

                values["client"] = GPT4AllModel(
                    model_name=model_name,
                    model_path=model_path or None,
                    model_type=values["backend"],
                    allow_download=False,
                )
                if values["n_threads"] is not None:
                    # set n_threads
                    values["client"].model.set_thread_count(
                        values["n_threads"]
                    )
            else:
                values["client"] = values["model"]
            try:
                values["backend"] = values["client"].model_type
            except AttributeError:
                # The below is for compatibility with GPT4All Python bindings <= 0.2.3.
                values["backend"] = values["client"].model.model_type

        except ImportError:
            raise ValueError(
                "Could not import gpt4all python package. "
                "Please install it with `pip install gpt4all`."
            )
        return values

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs,
    ) -> str:
        # Roughly 4 chars per token if natural language
        prompt = prompt[-self.n_ctx * 4 :]

        # use instruct prompting
        data_point = dict(context="", instruction=prompt, input="")
        prompt = self.prompter.generate_prompt(data_point)

        verbose = False
        if verbose:
            print("_call prompt: %s" % prompt, flush=True)
        # FIXME: GPT4ALl doesn't support yield during generate, so cannot support streaming except via itself to stdout
        return super()._call(prompt, stop=stop, run_manager=run_manager)


from langchain.llms import LlamaCpp


class H2OLlamaCpp(LlamaCpp):
    model_path: Any
    prompter: Any
    """Path to the pre-trained GPT4All model file."""

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that llama-cpp-python library is installed."""
        if isinstance(values["model_path"], str):
            model_path = values["model_path"]
            model_param_names = [
                "lora_path",
                "lora_base",
                "n_ctx",
                "n_parts",
                "seed",
                "f16_kv",
                "logits_all",
                "vocab_only",
                "use_mlock",
                "n_threads",
                "n_batch",
                "use_mmap",
                "last_n_tokens_size",
            ]
            model_params = {k: values[k] for k in model_param_names}
            # For backwards compatibility, only include if non-null.
            if values["n_gpu_layers"] is not None:
                model_params["n_gpu_layers"] = values["n_gpu_layers"]

            try:
                from llama_cpp import Llama

                values["client"] = Llama(model_path, **model_params)
            except ImportError:
                raise ModuleNotFoundError(
                    "Could not import llama-cpp-python library. "
                    "Please install the llama-cpp-python library to "
                    "use this embedding model: pip install llama-cpp-python"
                )
            except Exception as e:
                raise ValueError(
                    f"Could not load Llama model from path: {model_path}. "
                    f"Received error {e}"
                )
        else:
            values["client"] = values["model_path"]
        return values

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs,
    ) -> str:
        verbose = False
        # tokenize twice, just to count tokens, since llama cpp python wrapper has no way to truncate
        # still have to avoid crazy sizes, else hit llama_tokenize: too many tokens -- might still hit, not fatal
        prompt = prompt[-self.n_ctx * 4 :]
        prompt_tokens = self.client.tokenize(b" " + prompt.encode("utf-8"))
        num_prompt_tokens = len(prompt_tokens)
        if num_prompt_tokens > self.n_ctx:
            # conservative by using int()
            chars_per_token = int(len(prompt) / num_prompt_tokens)
            prompt = prompt[-self.n_ctx * chars_per_token :]
            if verbose:
                print(
                    "reducing tokens, assuming average of %s chars/token: %s"
                    % chars_per_token,
                    flush=True,
                )
                prompt_tokens2 = self.client.tokenize(
                    b" " + prompt.encode("utf-8")
                )
                num_prompt_tokens2 = len(prompt_tokens2)
                print(
                    "reduced tokens from %d -> %d"
                    % (num_prompt_tokens, num_prompt_tokens2),
                    flush=True,
                )

        # use instruct prompting
        data_point = dict(context="", instruction=prompt, input="")
        prompt = self.prompter.generate_prompt(data_point)

        if verbose:
            print("_call prompt: %s" % prompt, flush=True)

        if self.streaming:
            text_callback = None
            if run_manager:
                text_callback = partial(
                    run_manager.on_llm_new_token, verbose=self.verbose
                )
            # parent handler of streamer expects to see prompt first else output="" and lose if prompt=None in prompter
            if text_callback:
                text_callback(prompt)
            text = ""
            for token in self.stream(
                prompt=prompt, stop=stop, run_manager=run_manager
            ):
                text_chunk = token["choices"][0]["text"]
                # self.stream already calls text_callback
                # if text_callback:
                #    text_callback(text_chunk)
                text += text_chunk
            return text
        else:
            params = self._get_parameters(stop)
            params = {**params, **kwargs}
            result = self.client(prompt=prompt, **params)
            return result["choices"][0]["text"]

```

`apps/language_models/langchain/gpt_langchain.py`:

```py
import ast
import glob
import inspect
import os
import pathlib
import pickle
import shutil
import subprocess
import tempfile
import time
import traceback
import types
import uuid
import zipfile
from collections import defaultdict
from datetime import datetime
from functools import reduce
from operator import concat
import filelock

from joblib import delayed
from langchain.callbacks import streaming_stdout
from langchain.embeddings import HuggingFaceInstructEmbeddings
from tqdm import tqdm

from enums import (
    DocumentChoices,
    no_lora_str,
    model_token_mapping,
    source_prefix,
    source_postfix,
    non_query_commands,
    LangChainAction,
    LangChainMode,
)
from evaluate_params import gen_hyper
from gen import Langchain, SEED
from prompter import non_hf_types, PromptType, Prompter
from utils import (
    wrapped_partial,
    EThread,
    import_matplotlib,
    sanitize_filename,
    makedirs,
    get_url,
    flatten_list,
    ProgressParallel,
    remove,
    hash_file,
    clear_torch_cache,
    NullContext,
    get_hf_server,
    FakeTokenizer,
)
from utils_langchain import StreamingGradioCallbackHandler

import_matplotlib()

import numpy as np
import pandas as pd
import requests
from langchain.chains.qa_with_sources import load_qa_with_sources_chain

# , GCSDirectoryLoader, GCSFileLoader
# , OutlookMessageLoader # GPL3
# ImageCaptionLoader, # use our own wrapper
#  ReadTheDocsLoader,  # no special file, some path, so have to give as special option
from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    CSVLoader,
    PythonLoader,
    TomlLoader,
    UnstructuredURLLoader,
    UnstructuredHTMLLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredMarkdownLoader,
    EverNoteLoader,
    UnstructuredEmailLoader,
    UnstructuredODTLoader,
    UnstructuredPowerPointLoader,
    UnstructuredEPubLoader,
    UnstructuredImageLoader,
    UnstructuredRTFLoader,
    ArxivLoader,
    UnstructuredPDFLoader,
    UnstructuredExcelLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter, Language
from expanded_pipelines import load_qa_chain
from langchain.docstore.document import Document
from langchain import PromptTemplate, HuggingFaceTextGenInference
from langchain.vectorstores import Chroma
from apps.stable_diffusion.src import args


def get_db(
    sources,
    use_openai_embedding=False,
    db_type="faiss",
    persist_directory="db_dir",
    load_db_if_exists=True,
    langchain_mode="notset",
    collection_name=None,
    hf_embedding_model="sentence-transformers/all-MiniLM-L6-v2",
):
    if not sources:
        return None

    # get embedding model
    embedding = get_embedding(
        use_openai_embedding, hf_embedding_model=hf_embedding_model
    )
    assert collection_name is not None or langchain_mode != "notset"
    if collection_name is None:
        collection_name = langchain_mode.replace(" ", "_")

    # Create vector database
    if db_type == "faiss":
        from langchain.vectorstores import FAISS

        db = FAISS.from_documents(sources, embedding)
    elif db_type == "weaviate":
        import weaviate
        from weaviate.embedded import EmbeddedOptions
        from langchain.vectorstores import Weaviate

        if os.getenv("WEAVIATE_URL", None):
            client = _create_local_weaviate_client()
        else:
            client = weaviate.Client(embedded_options=EmbeddedOptions())
        index_name = collection_name.capitalize()
        db = Weaviate.from_documents(
            documents=sources,
            embedding=embedding,
            client=client,
            by_text=False,
            index_name=index_name,
        )
    elif db_type == "chroma":
        assert persist_directory is not None
        os.makedirs(persist_directory, exist_ok=True)

        # see if already actually have persistent db, and deal with possible changes in embedding
        db = get_existing_db(
            None,
            persist_directory,
            load_db_if_exists,
            db_type,
            use_openai_embedding,
            langchain_mode,
            hf_embedding_model,
            verbose=False,
        )
        if db is None:
            db = Chroma.from_documents(
                documents=sources,
                embedding=embedding,
                persist_directory=persist_directory,
                collection_name=collection_name,
                anonymized_telemetry=False,
            )
            db.persist()
            clear_embedding(db)
            save_embed(db, use_openai_embedding, hf_embedding_model)
        else:
            # then just add
            db, num_new_sources, new_sources_metadata = add_to_db(
                db,
                sources,
                db_type=db_type,
                use_openai_embedding=use_openai_embedding,
                hf_embedding_model=hf_embedding_model,
            )
    else:
        raise RuntimeError("No such db_type=%s" % db_type)

    return db


def _get_unique_sources_in_weaviate(db):
    batch_size = 100
    id_source_list = []
    result = db._client.data_object.get(
        class_name=db._index_name, limit=batch_size
    )

    while result["objects"]:
        id_source_list += [
            (obj["id"], obj["properties"]["source"])
            for obj in result["objects"]
        ]
        last_id = id_source_list[-1][0]
        result = db._client.data_object.get(
            class_name=db._index_name, limit=batch_size, after=last_id
        )

    unique_sources = {source for _, source in id_source_list}
    return unique_sources


def add_to_db(
    db,
    sources,
    db_type="faiss",
    avoid_dup_by_file=False,
    avoid_dup_by_content=True,
    use_openai_embedding=False,
    hf_embedding_model=None,
):
    assert hf_embedding_model is not None
    num_new_sources = len(sources)
    if not sources:
        return db, num_new_sources, []
    if db_type == "faiss":
        db.add_documents(sources)
    elif db_type == "weaviate":
        # FIXME: only control by file name, not hash yet
        if avoid_dup_by_file or avoid_dup_by_content:
            unique_sources = _get_unique_sources_in_weaviate(db)
            sources = [
                x
                for x in sources
                if x.metadata["source"] not in unique_sources
            ]
        num_new_sources = len(sources)
        if num_new_sources == 0:
            return db, num_new_sources, []
        db.add_documents(documents=sources)
    elif db_type == "chroma":
        collection = get_documents(db)
        # files we already have:
        metadata_files = set([x["source"] for x in collection["metadatas"]])
        if avoid_dup_by_file:
            # Too weak in case file changed content, assume parent shouldn't pass true for this for now
            raise RuntimeError("Not desired code path")
            sources = [
                x
                for x in sources
                if x.metadata["source"] not in metadata_files
            ]
        if avoid_dup_by_content:
            # look at hash, instead of page_content
            # migration: If no hash previously, avoid updating,
            #  since don't know if need to update and may be expensive to redo all unhashed files
            metadata_hash_ids = set(
                [
                    x["hashid"]
                    for x in collection["metadatas"]
                    if "hashid" in x and x["hashid"] not in ["None", None]
                ]
            )
            # avoid sources with same hash
            sources = [
                x
                for x in sources
                if x.metadata.get("hashid") not in metadata_hash_ids
            ]
            num_nohash = len(
                [x for x in sources if not x.metadata.get("hashid")]
            )
            print(
                "Found %s new sources (%d have no hash in original source,"
                " so have to reprocess for migration to sources with hash)"
                % (len(sources), num_nohash),
                flush=True,
            )
            # get new file names that match existing file names.  delete existing files we are overridding
            dup_metadata_files = set(
                [
                    x.metadata["source"]
                    for x in sources
                    if x.metadata["source"] in metadata_files
                ]
            )
            print(
                "Removing %s duplicate files from db because ingesting those as new documents"
                % len(dup_metadata_files),
                flush=True,
            )
            client_collection = db._client.get_collection(
                name=db._collection.name,
                embedding_function=db._collection._embedding_function,
            )
            for dup_file in dup_metadata_files:
                dup_file_meta = dict(source=dup_file)
                try:
                    client_collection.delete(where=dup_file_meta)
                except KeyError:
                    pass
        num_new_sources = len(sources)
        if num_new_sources == 0:
            return db, num_new_sources, []
        db.add_documents(documents=sources)
        db.persist()
        clear_embedding(db)
        save_embed(db, use_openai_embedding, hf_embedding_model)
    else:
        raise RuntimeError("No such db_type=%s" % db_type)

    new_sources_metadata = [x.metadata for x in sources]

    return db, num_new_sources, new_sources_metadata


def create_or_update_db(
    db_type,
    persist_directory,
    collection_name,
    sources,
    use_openai_embedding,
    add_if_exists,
    verbose,
    hf_embedding_model,
):
    if db_type == "weaviate":
        import weaviate
        from weaviate.embedded import EmbeddedOptions

        if os.getenv("WEAVIATE_URL", None):
            client = _create_local_weaviate_client()
        else:
            client = weaviate.Client(embedded_options=EmbeddedOptions())

        index_name = collection_name.replace(" ", "_").capitalize()
        if client.schema.exists(index_name) and not add_if_exists:
            client.schema.delete_class(index_name)
            if verbose:
                print("Removing %s" % index_name, flush=True)
    elif db_type == "chroma":
        if not os.path.isdir(persist_directory) or not add_if_exists:
            if os.path.isdir(persist_directory):
                if verbose:
                    print("Removing %s" % persist_directory, flush=True)
                remove(persist_directory)
            if verbose:
                print("Generating db", flush=True)

    if not add_if_exists:
        if verbose:
            print("Generating db", flush=True)
    else:
        if verbose:
            print("Loading and updating db", flush=True)

    db = get_db(
        sources,
        use_openai_embedding=use_openai_embedding,
        db_type=db_type,
        persist_directory=persist_directory,
        langchain_mode=collection_name,
        hf_embedding_model=hf_embedding_model,
    )

    return db


def get_embedding(
    use_openai_embedding,
    hf_embedding_model="sentence-transformers/all-MiniLM-L6-v2",
):
    # Get embedding model
    if use_openai_embedding:
        assert (
            os.getenv("OPENAI_API_KEY") is not None
        ), "Set ENV OPENAI_API_KEY"
        from langchain.embeddings import OpenAIEmbeddings

        embedding = OpenAIEmbeddings(disallowed_special=())
    else:
        # to ensure can fork without deadlock
        from langchain.embeddings import HuggingFaceEmbeddings

        torch_dtype, context_class = get_dtype()
        model_kwargs = dict(device=args.device)
        if "instructor" in hf_embedding_model:
            encode_kwargs = {"normalize_embeddings": True}
            embedding = HuggingFaceInstructEmbeddings(
                model_name=hf_embedding_model,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs,
            )
        else:
            embedding = HuggingFaceEmbeddings(
                model_name=hf_embedding_model, model_kwargs=model_kwargs
            )
    return embedding


def get_answer_from_sources(chain, sources, question):
    return chain(
        {
            "input_documents": sources,
            "question": question,
        },
        return_only_outputs=True,
    )["output_text"]


"""Wrapper around Huggingface text generation inference API."""
from functools import partial
from typing import Any, Dict, List, Optional, Set

from pydantic import Extra, Field, root_validator

from langchain.callbacks.manager import CallbackManagerForLLMRun

"""Wrapper around Huggingface text generation inference API."""
from functools import partial
from typing import Any, Dict, List, Optional

from pydantic import Extra, Field, root_validator

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM


class GradioInference(LLM):
    """
    Gradio generation inference API.
    """

    inference_server_url: str = ""

    temperature: float = 0.8
    top_p: Optional[float] = 0.95
    top_k: Optional[int] = None
    num_beams: Optional[int] = 1
    max_new_tokens: int = 512
    min_new_tokens: int = 1
    early_stopping: bool = False
    max_time: int = 180
    repetition_penalty: Optional[float] = None
    num_return_sequences: Optional[int] = 1
    do_sample: bool = False
    chat_client: bool = False

    return_full_text: bool = True
    stream_output: bool = Field(False, alias="stream")
    sanitize_bot_response: bool = False

    prompter: Any = None
    client: Any = None

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that python package exists in environment."""

        try:
            if values["client"] is None:
                import gradio_client

                values["client"] = gradio_client.Client(
                    values["inference_server_url"]
                )
        except ImportError:
            raise ImportError(
                "Could not import gradio_client python package. "
                "Please install it with `pip install gradio_client`."
            )
        return values

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "gradio_inference"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        # NOTE: prompt here has no prompt_type (e.g. human: bot:) prompt injection,
        # so server should get prompt_type or '', not plain
        # This is good, so gradio server can also handle stopping.py conditions
        # this is different than TGI server that uses prompter to inject prompt_type prompting
        stream_output = self.stream_output
        gr_client = self.client
        client_langchain_mode = "Disabled"
        client_langchain_action = LangChainAction.QUERY.value
        top_k_docs = 1
        chunk = True
        chunk_size = 512
        client_kwargs = dict(
            instruction=prompt
            if self.chat_client
            else "",  # only for chat=True
            iinput="",  # only for chat=True
            context="",
            # streaming output is supported, loops over and outputs each generation in streaming mode
            # but leave stream_output=False for simple input/output mode
            stream_output=stream_output,
            prompt_type=self.prompter.prompt_type,
            prompt_dict="",
            temperature=self.temperature,
            top_p=self.top_p,
            top_k=self.top_k,
            num_beams=self.num_beams,
            max_new_tokens=self.max_new_tokens,
            min_new_tokens=self.min_new_tokens,
            early_stopping=self.early_stopping,
            max_time=self.max_time,
            repetition_penalty=self.repetition_penalty,
            num_return_sequences=self.num_return_sequences,
            do_sample=self.do_sample,
            chat=self.chat_client,
            instruction_nochat=prompt if not self.chat_client else "",
            iinput_nochat="",  # only for chat=False
            langchain_mode=client_langchain_mode,
            langchain_action=client_langchain_action,
            top_k_docs=top_k_docs,
            chunk=chunk,
            chunk_size=chunk_size,
            document_choice=[DocumentChoices.All_Relevant.name],
        )
        api_name = "/submit_nochat_api"  # NOTE: like submit_nochat but stable API for string dict passing
        if not stream_output:
            res = gr_client.predict(
                str(dict(client_kwargs)), api_name=api_name
            )
            res_dict = ast.literal_eval(res)
            text = res_dict["response"]
            return self.prompter.get_response(
                prompt + text,
                prompt=prompt,
                sanitize_bot_response=self.sanitize_bot_response,
            )
        else:
            text_callback = None
            if run_manager:
                text_callback = partial(
                    run_manager.on_llm_new_token, verbose=self.verbose
                )

            job = gr_client.submit(str(dict(client_kwargs)), api_name=api_name)
            text0 = ""
            while not job.done():
                outputs_list = job.communicator.job.outputs
                if outputs_list:
                    res = job.communicator.job.outputs[-1]
                    res_dict = ast.literal_eval(res)
                    text = res_dict["response"]
                    text = self.prompter.get_response(
                        prompt + text,
                        prompt=prompt,
                        sanitize_bot_response=self.sanitize_bot_response,
                    )
                    # FIXME: derive chunk from full for now
                    text_chunk = text[len(text0) :]
                    # save old
                    text0 = text

                    if text_callback:
                        text_callback(text_chunk)

                time.sleep(0.01)

            # ensure get last output to avoid race
            res_all = job.outputs()
            if len(res_all) > 0:
                res = res_all[-1]
                res_dict = ast.literal_eval(res)
                text = res_dict["response"]
                # FIXME: derive chunk from full for now
            else:
                # go with old if failure
                text = text0
            text_chunk = text[len(text0) :]
            if text_callback:
                text_callback(text_chunk)
            return self.prompter.get_response(
                prompt + text,
                prompt=prompt,
                sanitize_bot_response=self.sanitize_bot_response,
            )


class H2OHuggingFaceTextGenInference(HuggingFaceTextGenInference):
    max_new_tokens: int = 512
    do_sample: bool = False
    top_k: Optional[int] = None
    top_p: Optional[float] = 0.95
    typical_p: Optional[float] = 0.95
    temperature: float = 0.8
    repetition_penalty: Optional[float] = None
    return_full_text: bool = False
    stop_sequences: List[str] = Field(default_factory=list)
    seed: Optional[int] = None
    inference_server_url: str = ""
    timeout: int = 300
    headers: dict = None
    stream_output: bool = Field(False, alias="stream")
    sanitize_bot_response: bool = False
    prompter: Any = None
    tokenizer: Any = None
    client: Any = None

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that python package exists in environment."""

        try:
            if values["client"] is None:
                import text_generation

                values["client"] = text_generation.Client(
                    values["inference_server_url"],
                    timeout=values["timeout"],
                    headers=values["headers"],
                )
        except ImportError:
            raise ImportError(
                "Could not import text_generation python package. "
                "Please install it with `pip install text_generation`."
            )
        return values

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        if stop is None:
            stop = self.stop_sequences
        else:
            stop += self.stop_sequences

        # HF inference server needs control over input tokens
        assert self.tokenizer is not None
        from h2oai_pipeline import H2OTextGenerationPipeline

        prompt, num_prompt_tokens = H2OTextGenerationPipeline.limit_prompt(
            prompt, self.tokenizer
        )

        # NOTE: TGI server does not add prompting, so must do here
        data_point = dict(context="", instruction=prompt, input="")
        prompt = self.prompter.generate_prompt(data_point)

        gen_server_kwargs = dict(
            do_sample=self.do_sample,
            stop_sequences=stop,
            max_new_tokens=self.max_new_tokens,
            top_k=self.top_k,
            top_p=self.top_p,
            typical_p=self.typical_p,
            temperature=self.temperature,
            repetition_penalty=self.repetition_penalty,
            return_full_text=self.return_full_text,
            seed=self.seed,
        )
        gen_server_kwargs.update(kwargs)

        # lower bound because client is re-used if multi-threading
        self.client.timeout = max(300, self.timeout)

        if not self.stream_output:
            res = self.client.generate(
                prompt,
                **gen_server_kwargs,
            )
            if self.return_full_text:
                gen_text = res.generated_text[len(prompt) :]
            else:
                gen_text = res.generated_text
            # remove stop sequences from the end of the generated text
            for stop_seq in stop:
                if stop_seq in gen_text:
                    gen_text = gen_text[: gen_text.index(stop_seq)]
            text = prompt + gen_text
            text = self.prompter.get_response(
                text,
                prompt=prompt,
                sanitize_bot_response=self.sanitize_bot_response,
            )
        else:
            text_callback = None
            if run_manager:
                text_callback = partial(
                    run_manager.on_llm_new_token, verbose=self.verbose
                )
            # parent handler of streamer expects to see prompt first else output="" and lose if prompt=None in prompter
            if text_callback:
                text_callback(prompt)
            text = ""
            # Note: Streaming ignores return_full_text=True
            for response in self.client.generate_stream(
                prompt, **gen_server_kwargs
            ):
                text_chunk = response.token.text
                text += text_chunk
                text = self.prompter.get_response(
                    prompt + text,
                    prompt=prompt,
                    sanitize_bot_response=self.sanitize_bot_response,
                )
                # stream part
                is_stop = False
                for stop_seq in stop:
                    if stop_seq in response.token.text:
                        is_stop = True
                        break
                if is_stop:
                    break
                if not response.token.special:
                    if text_callback:
                        text_callback(response.token.text)
        return text


from langchain.chat_models import ChatOpenAI


class H2OChatOpenAI(ChatOpenAI):
    @classmethod
    def all_required_field_names(cls) -> Set:
        all_required_field_names = super(
            ChatOpenAI, cls
        ).all_required_field_names()
        all_required_field_names.update(
            {"top_p", "frequency_penalty", "presence_penalty"}
        )
        return all_required_field_names


def get_llm(
    use_openai_model=False,
    model_name=None,
    model=None,
    tokenizer=None,
    inference_server=None,
    stream_output=False,
    do_sample=False,
    temperature=0.1,
    top_k=40,
    top_p=0.7,
    num_beams=1,
    max_new_tokens=256,
    min_new_tokens=1,
    early_stopping=False,
    max_time=180,
    repetition_penalty=1.0,
    num_return_sequences=1,
    prompt_type=None,
    prompt_dict=None,
    prompter=None,
    sanitize_bot_response=False,
    verbose=False,
):
    if use_openai_model or inference_server in ["openai", "openai_chat"]:
        if use_openai_model and model_name is None:
            model_name = "gpt-3.5-turbo"
        if inference_server == "openai":
            from langchain.llms import OpenAI

            cls = OpenAI
        else:
            cls = H2OChatOpenAI
        callbacks = [StreamingGradioCallbackHandler()]
        llm = cls(
            model_name=model_name,
            temperature=temperature if do_sample else 0,
            # FIXME: Need to count tokens and reduce max_new_tokens to fit like in generate.py
            max_tokens=max_new_tokens,
            top_p=top_p if do_sample else 1,
            frequency_penalty=0,
            presence_penalty=1.07
            - repetition_penalty
            + 0.6,  # so good default
            callbacks=callbacks if stream_output else None,
        )
        streamer = callbacks[0] if stream_output else None
        if inference_server in ["openai", "openai_chat"]:
            prompt_type = inference_server
        else:
            prompt_type = prompt_type or "plain"
    elif inference_server:
        assert inference_server.startswith("http"), (
            "Malformed inference_server=%s.  Did you add http:// in front?"
            % inference_server
        )

        from gradio_utils.grclient import GradioClient
        from text_generation import Client as HFClient

        if isinstance(model, GradioClient):
            gr_client = model
            hf_client = None
        else:
            gr_client = None
            hf_client = model
            assert isinstance(hf_client, HFClient)

        inference_server, headers = get_hf_server(inference_server)

        # quick sanity check to avoid long timeouts, just see if can reach server
        requests.get(
            inference_server,
            timeout=int(os.getenv("REQUEST_TIMEOUT_FAST", "10")),
        )

        callbacks = [StreamingGradioCallbackHandler()]
        assert prompter is not None
        stop_sequences = list(
            set(prompter.terminate_response + [prompter.PreResponse])
        )
        stop_sequences = [x for x in stop_sequences if x]

        if gr_client:
            chat_client = False
            llm = GradioInference(
                inference_server_url=inference_server,
                return_full_text=True,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                num_beams=num_beams,
                max_new_tokens=max_new_tokens,
                min_new_tokens=min_new_tokens,
                early_stopping=early_stopping,
                max_time=max_time,
                repetition_penalty=repetition_penalty,
                num_return_sequences=num_return_sequences,
                do_sample=do_sample,
                chat_client=chat_client,
                callbacks=callbacks if stream_output else None,
                stream=stream_output,
                prompter=prompter,
                client=gr_client,
                sanitize_bot_response=sanitize_bot_response,
            )
        elif hf_client:
            llm = H2OHuggingFaceTextGenInference(
                inference_server_url=inference_server,
                do_sample=do_sample,
                max_new_tokens=max_new_tokens,
                repetition_penalty=repetition_penalty,
                return_full_text=True,
                seed=SEED,
                stop_sequences=stop_sequences,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                # typical_p=top_p,
                callbacks=callbacks if stream_output else None,
                stream_output=stream_output,
                prompter=prompter,
                tokenizer=tokenizer,
                client=hf_client,
                timeout=max_time,
                sanitize_bot_response=sanitize_bot_response,
            )
        else:
            raise RuntimeError("No defined client")
        streamer = callbacks[0] if stream_output else None
    elif model_name in non_hf_types:
        if model_name == "llama":
            callbacks = [StreamingGradioCallbackHandler()]
            streamer = callbacks[0] if stream_output else None
        else:
            # stream_output = False
            # doesn't stream properly as generator, but at least
            callbacks = [streaming_stdout.StreamingStdOutCallbackHandler()]
            streamer = None
        if prompter:
            prompt_type = prompter.prompt_type
        else:
            prompter = Prompter(
                prompt_type,
                prompt_dict,
                debug=False,
                chat=False,
                stream_output=stream_output,
            )
            pass  # assume inputted prompt_type is correct
        from gpt4all_llm import get_llm_gpt4all

        llm = get_llm_gpt4all(
            model_name,
            model=model,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            repetition_penalty=repetition_penalty,
            top_k=top_k,
            top_p=top_p,
            callbacks=callbacks,
            verbose=verbose,
            streaming=stream_output,
            prompter=prompter,
        )
    else:
        if model is None:
            # only used if didn't pass model in
            assert tokenizer is None
            prompt_type = "human_bot"
            if model_name is None:
                model_name = "h2oai/h2ogpt-oasst1-512-12b"
                # model_name = 'h2oai/h2ogpt-oig-oasst1-512-6_9b'
                # model_name = 'h2oai/h2ogpt-oasst1-512-20b'
            inference_server = ""
            model, tokenizer, _ = Langchain.get_model(
                load_8bit=True,
                base_model=model_name,
                inference_server=inference_server,
                gpu_id=0,
            )

        max_max_tokens = tokenizer.model_max_length
        gen_kwargs = dict(
            do_sample=do_sample,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            num_beams=num_beams,
            max_new_tokens=max_new_tokens,
            min_new_tokens=min_new_tokens,
            early_stopping=early_stopping,
            max_time=max_time,
            repetition_penalty=repetition_penalty,
            num_return_sequences=num_return_sequences,
            return_full_text=True,
            handle_long_generation=None,
        )
        assert len(set(gen_hyper).difference(gen_kwargs.keys())) == 0

        if stream_output:
            skip_prompt = False
            from gen import H2OTextIteratorStreamer

            decoder_kwargs = {}
            streamer = H2OTextIteratorStreamer(
                tokenizer,
                skip_prompt=skip_prompt,
                block=False,
                **decoder_kwargs,
            )
            gen_kwargs.update(dict(streamer=streamer))
        else:
            streamer = None

        from h2oai_pipeline import H2OTextGenerationPipeline

        pipe = H2OTextGenerationPipeline(
            model=model,
            use_prompter=True,
            prompter=prompter,
            prompt_type=prompt_type,
            prompt_dict=prompt_dict,
            sanitize_bot_response=sanitize_bot_response,
            chat=False,
            stream_output=stream_output,
            tokenizer=tokenizer,
            # leave some room for 1 paragraph, even if min_new_tokens=0
            max_input_tokens=max_max_tokens - max(min_new_tokens, 256),
            **gen_kwargs,
        )
        # pipe.task = "text-generation"
        # below makes it listen only to our prompt removal,
        # not built in prompt removal that is less general and not specific for our model
        pipe.task = "text2text-generation"

        from langchain.llms import HuggingFacePipeline

        llm = HuggingFacePipeline(pipeline=pipe)
    return llm, model_name, streamer, prompt_type


def get_dtype():
    # torch.device("cuda") leads to cuda:x cuda:y mismatches for multi-GPU consistently
    import torch

    # from utils import NullContext
    # context_class = NullContext if n_gpus > 1 or n_gpus == 0 else context_class
    context_class = torch.device
    torch_dtype = torch.float16 if args.device == "cuda" else torch.float32
    return torch_dtype, context_class


def get_wiki_data(
    title, first_paragraph_only, text_limit=None, take_head=True
):
    """
    Get wikipedia data from online
    :param title:
    :param first_paragraph_only:
    :param text_limit:
    :param take_head:
    :return:
    """
    filename = "wiki_%s_%s_%s_%s.data" % (
        first_paragraph_only,
        title,
        text_limit,
        take_head,
    )
    url = f"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&explaintext=1&titles={title}"
    if first_paragraph_only:
        url += "&exintro=1"
    import json

    if not os.path.isfile(filename):
        data = requests.get(url).json()
        json.dump(data, open(filename, "wt"))
    else:
        data = json.load(open(filename, "rt"))
    page_content = list(data["query"]["pages"].values())[0]["extract"]
    if take_head is not None and text_limit is not None:
        page_content = (
            page_content[:text_limit]
            if take_head
            else page_content[-text_limit:]
        )
    title_url = str(title).replace(" ", "_")
    return Document(
        page_content=page_content,
        metadata={"source": f"https://en.wikipedia.org/wiki/{title_url}"},
    )


def get_wiki_sources(first_para=True, text_limit=None):
    """
    Get specific named sources from wikipedia
    :param first_para:
    :param text_limit:
    :return:
    """
    default_wiki_sources = ["Unix", "Microsoft_Windows", "Linux"]
    wiki_sources = list(os.getenv("WIKI_SOURCES", default_wiki_sources))
    return [
        get_wiki_data(x, first_para, text_limit=text_limit)
        for x in wiki_sources
    ]


def get_github_docs(repo_owner, repo_name):
    """
    Access github from specific repo
    :param repo_owner:
    :param repo_name:
    :return:
    """
    with tempfile.TemporaryDirectory() as d:
        subprocess.check_call(
            f"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .",
            cwd=d,
            shell=True,
        )
        git_sha = (
            subprocess.check_output("git rev-parse HEAD", shell=True, cwd=d)
            .decode("utf-8")
            .strip()
        )
        repo_path = pathlib.Path(d)
        markdown_files = list(repo_path.glob("*/*.md")) + list(
            repo_path.glob("*/*.mdx")
        )
        for markdown_file in markdown_files:
            with open(markdown_file, "r") as f:
                relative_path = markdown_file.relative_to(repo_path)
                github_url = f"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}"
                yield Document(
                    page_content=f.read(), metadata={"source": github_url}
                )


def get_dai_pickle(dest="."):
    from huggingface_hub import hf_hub_download

    # True for case when locally already logged in with correct token, so don't have to set key
    token = os.getenv("HUGGINGFACE_API_TOKEN", True)
    path_to_zip_file = hf_hub_download(
        "h2oai/dai_docs", "dai_docs.pickle", token=token, repo_type="dataset"
    )
    shutil.copy(path_to_zip_file, dest)


def get_dai_docs(from_hf=False, get_pickle=True):
    """
    Consume DAI documentation, or consume from public pickle
    :param from_hf: get DAI docs from HF, then generate pickle for later use by LangChain
    :param get_pickle: Avoid raw DAI docs, just get pickle directly from HF
    :return:
    """
    import pickle

    if get_pickle:
        get_dai_pickle()

    dai_store = "dai_docs.pickle"
    dst = "working_dir_docs"
    if not os.path.isfile(dai_store):
        from create_data import setup_dai_docs

        dst = setup_dai_docs(dst=dst, from_hf=from_hf)

        import glob

        files = list(glob.glob(os.path.join(dst, "*rst"), recursive=True))

        basedir = os.path.abspath(os.getcwd())
        from create_data import rst_to_outputs

        new_outputs = rst_to_outputs(files)
        os.chdir(basedir)

        pickle.dump(new_outputs, open(dai_store, "wb"))
    else:
        new_outputs = pickle.load(open(dai_store, "rb"))

    sources = []
    for line, file in new_outputs:
        # gradio requires any linked file to be with app.py
        sym_src = os.path.abspath(os.path.join(dst, file))
        sym_dst = os.path.abspath(os.path.join(os.getcwd(), file))
        if os.path.lexists(sym_dst):
            os.remove(sym_dst)
        os.symlink(sym_src, sym_dst)
        itm = Document(page_content=line, metadata={"source": file})
        # NOTE: yield has issues when going into db, loses metadata
        # yield itm
        sources.append(itm)
    return sources


import distutils.spawn

have_tesseract = distutils.spawn.find_executable("tesseract")
have_libreoffice = distutils.spawn.find_executable("libreoffice")

import pkg_resources

try:
    assert pkg_resources.get_distribution("arxiv") is not None
    assert pkg_resources.get_distribution("pymupdf") is not None
    have_arxiv = True
except (pkg_resources.DistributionNotFound, AssertionError):
    have_arxiv = False

try:
    assert pkg_resources.get_distribution("pymupdf") is not None
    have_pymupdf = True
except (pkg_resources.DistributionNotFound, AssertionError):
    have_pymupdf = False

try:
    assert pkg_resources.get_distribution("selenium") is not None
    have_selenium = True
except (pkg_resources.DistributionNotFound, AssertionError):
    have_selenium = False

try:
    assert pkg_resources.get_distribution("playwright") is not None
    have_playwright = True
except (pkg_resources.DistributionNotFound, AssertionError):
    have_playwright = False

# disable, hangs too often
have_playwright = False

image_types = ["png", "jpg", "jpeg"]
non_image_types = [
    "pdf",
    "txt",
    "csv",
    "toml",
    "py",
    "rst",
    "rtf",
    "md",
    "html",
    "mhtml",
    "enex",
    "eml",
    "epub",
    "odt",
    "pptx",
    "ppt",
    "zip",
    "urls",
]
# "msg",  GPL3

if have_libreoffice:
    non_image_types.extend(["docx", "doc", "xls", "xlsx"])

file_types = non_image_types + image_types


def add_meta(docs1, file):
    file_extension = pathlib.Path(file).suffix
    hashid = hash_file(file)
    if not isinstance(docs1, (list, tuple, types.GeneratorType)):
        docs1 = [docs1]
    [
        x.metadata.update(
            dict(
                input_type=file_extension,
                date=str(datetime.now()),
                hashid=hashid,
            )
        )
        for x in docs1
    ]


def file_to_doc(
    file,
    base_path=None,
    verbose=False,
    fail_any_exception=False,
    chunk=True,
    chunk_size=512,
    is_url=False,
    is_txt=False,
    enable_captions=True,
    captions_model=None,
    enable_ocr=False,
    caption_loader=None,
    headsize=50,
):
    if file is None:
        if fail_any_exception:
            raise RuntimeError("Unexpected None file")
        else:
            return []
    doc1 = []  # in case no support, or disabled support
    if base_path is None and not is_txt and not is_url:
        # then assume want to persist but don't care which path used
        # can't be in base_path
        dir_name = os.path.dirname(file)
        base_name = os.path.basename(file)
        # if from gradio, will have its own temp uuid too, but that's ok
        base_name = sanitize_filename(base_name) + "_" + str(uuid.uuid4())[:10]
        base_path = os.path.join(dir_name, base_name)
    if is_url:
        if file.lower().startswith("arxiv:"):
            query = file.lower().split("arxiv:")
            if len(query) == 2 and have_arxiv:
                query = query[1]
                docs1 = ArxivLoader(
                    query=query, load_max_docs=20, load_all_available_meta=True
                ).load()
                # ensure string, sometimes None
                [
                    [
                        x.metadata.update({k: str(v)})
                        for k, v in x.metadata.items()
                    ]
                    for x in docs1
                ]
                query_url = f"https://arxiv.org/abs/{query}"
                [
                    x.metadata.update(
                        dict(
                            source=x.metadata.get("entry_id", query_url),
                            query=query_url,
                            input_type="arxiv",
                            head=x.metadata.get("Title", ""),
                            date=str(datetime.now),
                        )
                    )
                    for x in docs1
                ]
            else:
                docs1 = []
        else:
            if not (
                file.startswith("http://")
                or file.startswith("file://")
                or file.startswith("https://")
            ):
                file = "http://" + file
            docs1 = UnstructuredURLLoader(urls=[file]).load()
            if len(docs1) == 0 and have_playwright:
                # then something went wrong, try another loader:
                from langchain.document_loaders import PlaywrightURLLoader

                docs1 = PlaywrightURLLoader(urls=[file]).load()
            if len(docs1) == 0 and have_selenium:
                # then something went wrong, try another loader:
                # but requires Chrome binary, else get: selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary
                from langchain.document_loaders import SeleniumURLLoader
                from selenium.common.exceptions import WebDriverException

                try:
                    docs1 = SeleniumURLLoader(urls=[file]).load()
                except WebDriverException as e:
                    print("No web driver: %s" % str(e), flush=True)
            [
                x.metadata.update(
                    dict(input_type="url", date=str(datetime.now))
                )
                for x in docs1
            ]
        docs1 = clean_doc(docs1)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif is_txt:
        base_path = "user_paste"
        source_file = os.path.join(base_path, "_%s" % str(uuid.uuid4())[:10])
        makedirs(os.path.dirname(source_file), exist_ok=True)
        with open(source_file, "wt") as f:
            f.write(file)
        metadata = dict(
            source=source_file,
            date=str(datetime.now()),
            input_type="pasted txt",
        )
        doc1 = Document(page_content=file, metadata=metadata)
        doc1 = clean_doc(doc1)
    elif file.lower().endswith(".html") or file.lower().endswith(".mhtml"):
        docs1 = UnstructuredHTMLLoader(file_path=file).load()
        add_meta(docs1, file)
        docs1 = clean_doc(docs1)
        doc1 = chunk_sources(
            docs1, chunk=chunk, chunk_size=chunk_size, language=Language.HTML
        )
    elif (
        file.lower().endswith(".docx") or file.lower().endswith(".doc")
    ) and have_libreoffice:
        docs1 = UnstructuredWordDocumentLoader(file_path=file).load()
        add_meta(docs1, file)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif (
        file.lower().endswith(".xlsx") or file.lower().endswith(".xls")
    ) and have_libreoffice:
        docs1 = UnstructuredExcelLoader(file_path=file).load()
        add_meta(docs1, file)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif file.lower().endswith(".odt"):
        docs1 = UnstructuredODTLoader(file_path=file).load()
        add_meta(docs1, file)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif file.lower().endswith("pptx") or file.lower().endswith("ppt"):
        docs1 = UnstructuredPowerPointLoader(file_path=file).load()
        add_meta(docs1, file)
        docs1 = clean_doc(docs1)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif file.lower().endswith(".txt"):
        # use UnstructuredFileLoader ?
        docs1 = TextLoader(
            file, encoding="utf8", autodetect_encoding=True
        ).load()
        # makes just one, but big one
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
        doc1 = clean_doc(doc1)
        add_meta(doc1, file)
    elif file.lower().endswith(".rtf"):
        docs1 = UnstructuredRTFLoader(file).load()
        add_meta(docs1, file)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif file.lower().endswith(".md"):
        docs1 = UnstructuredMarkdownLoader(file).load()
        add_meta(docs1, file)
        docs1 = clean_doc(docs1)
        doc1 = chunk_sources(
            docs1,
            chunk=chunk,
            chunk_size=chunk_size,
            language=Language.MARKDOWN,
        )
    elif file.lower().endswith(".enex"):
        docs1 = EverNoteLoader(file).load()
        add_meta(doc1, file)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif file.lower().endswith(".epub"):
        docs1 = UnstructuredEPubLoader(file).load()
        add_meta(docs1, file)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif (
        file.lower().endswith(".jpeg")
        or file.lower().endswith(".jpg")
        or file.lower().endswith(".png")
    ):
        docs1 = []
        if have_tesseract and enable_ocr:
            # OCR, somewhat works, but not great
            docs1.extend(UnstructuredImageLoader(file).load())
            add_meta(docs1, file)
        if enable_captions:
            # BLIP
            if caption_loader is not None and not isinstance(
                caption_loader, (str, bool)
            ):
                # assumes didn't fork into this process with joblib, else can deadlock
                caption_loader.set_image_paths([file])
                docs1c = caption_loader.load()
                add_meta(docs1c, file)
                [
                    x.metadata.update(
                        dict(head=x.page_content[:headsize].strip())
                    )
                    for x in docs1c
                ]
                docs1.extend(docs1c)
            else:
                from image_captions import H2OImageCaptionLoader

                caption_loader = H2OImageCaptionLoader(
                    caption_gpu=caption_loader == "gpu",
                    blip_model=captions_model,
                    blip_processor=captions_model,
                )
                caption_loader.set_image_paths([file])
                docs1c = caption_loader.load()
                add_meta(docs1c, file)
                [
                    x.metadata.update(
                        dict(head=x.page_content[:headsize].strip())
                    )
                    for x in docs1c
                ]
                docs1.extend(docs1c)
            for doci in docs1:
                doci.metadata["source"] = doci.metadata["image_path"]
                doci.metadata["hash"] = hash_file(doci.metadata["source"])
            if docs1:
                doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif file.lower().endswith(".msg"):
        raise RuntimeError("Not supported, GPL3 license")
        # docs1 = OutlookMessageLoader(file).load()
        # docs1[0].metadata['source'] = file
    elif file.lower().endswith(".eml"):
        try:
            docs1 = UnstructuredEmailLoader(file).load()
            add_meta(docs1, file)
            doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
        except ValueError as e:
            if "text/html content not found in email" in str(e):
                # e.g. plain/text dict key exists, but not
                # doc1 = TextLoader(file, encoding="utf8").load()
                docs1 = UnstructuredEmailLoader(
                    file, content_source="text/plain"
                ).load()
                add_meta(docs1, file)
                doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
            else:
                raise
    # elif file.lower().endswith('.gcsdir'):
    #    doc1 = GCSDirectoryLoader(project_name, bucket, prefix).load()
    # elif file.lower().endswith('.gcsfile'):
    # doc1 = GCSFileLoader(project_name, bucket, blob).load()
    elif file.lower().endswith(".rst"):
        with open(file, "r") as f:
            doc1 = Document(page_content=f.read(), metadata={"source": file})
        add_meta(doc1, file)
        doc1 = chunk_sources(
            doc1, chunk=chunk, chunk_size=chunk_size, language=Language.RST
        )
    elif file.lower().endswith(".pdf"):
        env_gpt4all_file = ".env_gpt4all"
        from dotenv import dotenv_values

        env_kwargs = dotenv_values(env_gpt4all_file)
        pdf_class_name = env_kwargs.get("PDF_CLASS_NAME", "PyMuPDFParser")
        if have_pymupdf and pdf_class_name == "PyMuPDFParser":
            # GPL, only use if installed
            from langchain.document_loaders import PyMuPDFLoader

            # load() still chunks by pages, but every page has title at start to help
            doc1 = PyMuPDFLoader(file).load()
            doc1 = clean_doc(doc1)
        elif pdf_class_name == "UnstructuredPDFLoader":
            doc1 = UnstructuredPDFLoader(file).load()
            # seems to not need cleaning in most cases
        else:
            # open-source fallback
            # load() still chunks by pages, but every page has title at start to help
            doc1 = PyPDFLoader(file).load()
            doc1 = clean_doc(doc1)
        # Some PDFs return nothing or junk from PDFMinerLoader
        doc1 = chunk_sources(doc1, chunk=chunk, chunk_size=chunk_size)
        add_meta(doc1, file)
    elif file.lower().endswith(".csv"):
        doc1 = CSVLoader(file).load()
        add_meta(doc1, file)
    elif file.lower().endswith(".py"):
        doc1 = PythonLoader(file).load()
        add_meta(doc1, file)
        doc1 = chunk_sources(
            doc1, chunk=chunk, chunk_size=chunk_size, language=Language.PYTHON
        )
    elif file.lower().endswith(".toml"):
        doc1 = TomlLoader(file).load()
        add_meta(doc1, file)
    elif file.lower().endswith(".urls"):
        with open(file, "r") as f:
            docs1 = UnstructuredURLLoader(urls=f.readlines()).load()
        add_meta(docs1, file)
        doc1 = chunk_sources(docs1, chunk=chunk, chunk_size=chunk_size)
    elif file.lower().endswith(".zip"):
        with zipfile.ZipFile(file, "r") as zip_ref:
            # don't put into temporary path, since want to keep references to docs inside zip
            # so just extract in path where
            zip_ref.extractall(base_path)
            # recurse
            doc1 = path_to_docs(
                base_path,
                verbose=verbose,
                fail_any_exception=fail_any_exception,
            )
    else:
        raise RuntimeError("No file handler for %s" % os.path.basename(file))

    # allow doc1 to be list or not.  If not list, did not chunk yet, so chunk now
    # if list of length one, don't trust and chunk it
    if not isinstance(doc1, list):
        if chunk:
            docs = chunk_sources([doc1], chunk=chunk, chunk_size=chunk_size)
        else:
            docs = [doc1]
    elif isinstance(doc1, list) and len(doc1) == 1:
        if chunk:
            docs = chunk_sources(doc1, chunk=chunk, chunk_size=chunk_size)
        else:
            docs = doc1
    else:
        docs = doc1

    assert isinstance(docs, list)
    return docs


def path_to_doc1(
    file,
    verbose=False,
    fail_any_exception=False,
    return_file=True,
    chunk=True,
    chunk_size=512,
    is_url=False,
    is_txt=False,
    enable_captions=True,
    captions_model=None,
    enable_ocr=False,
    caption_loader=None,
):
    if verbose:
        if is_url:
            print("Ingesting URL: %s" % file, flush=True)
        elif is_txt:
            print("Ingesting Text: %s" % file, flush=True)
        else:
            print("Ingesting file: %s" % file, flush=True)
    res = None
    try:
        # don't pass base_path=path, would infinitely recurse
        res = file_to_doc(
            file,
            base_path=None,
            verbose=verbose,
            fail_any_exception=fail_any_exception,
            chunk=chunk,
            chunk_size=chunk_size,
            is_url=is_url,
            is_txt=is_txt,
            enable_captions=enable_captions,
            captions_model=captions_model,
            enable_ocr=enable_ocr,
            caption_loader=caption_loader,
        )
    except BaseException as e:
        print("Failed to ingest %s due to %s" % (file, traceback.format_exc()))
        if fail_any_exception:
            raise
        else:
            exception_doc = Document(
                page_content="",
                metadata={
                    "source": file,
                    "exception": str(e),
                    "traceback": traceback.format_exc(),
                },
            )
            res = [exception_doc]
    if return_file:
        base_tmp = "temp_path_to_doc1"
        if not os.path.isdir(base_tmp):
            os.makedirs(base_tmp, exist_ok=True)
        filename = os.path.join(base_tmp, str(uuid.uuid4()) + ".tmp.pickle")
        with open(filename, "wb") as f:
            pickle.dump(res, f)
        return filename
    return res


def path_to_docs(
    path_or_paths,
    verbose=False,
    fail_any_exception=False,
    n_jobs=-1,
    chunk=True,
    chunk_size=512,
    url=None,
    text=None,
    enable_captions=True,
    captions_model=None,
    caption_loader=None,
    enable_ocr=False,
    existing_files=[],
    existing_hash_ids={},
):
    # path_or_paths could be str, list, tuple, generator
    globs_image_types = []
    globs_non_image_types = []
    if not path_or_paths and not url and not text:
        return []
    elif url:
        globs_non_image_types = (
            url
            if isinstance(url, (list, tuple, types.GeneratorType))
            else [url]
        )
    elif text:
        globs_non_image_types = (
            text
            if isinstance(text, (list, tuple, types.GeneratorType))
            else [text]
        )
    elif isinstance(path_or_paths, str) and os.path.isdir(path_or_paths):
        # single path, only consume allowed files
        path = path_or_paths
        # Below globs should match patterns in file_to_doc()
        [
            globs_image_types.extend(
                glob.glob(
                    os.path.join(path, "./**/*.%s" % ftype), recursive=True
                )
            )
            for ftype in image_types
        ]
        [
            globs_non_image_types.extend(
                glob.glob(
                    os.path.join(path, "./**/*.%s" % ftype), recursive=True
                )
            )
            for ftype in non_image_types
        ]
    else:
        if isinstance(path_or_paths, str) and (
            os.path.isfile(path_or_paths) or os.path.isdir(path_or_paths)
        ):
            path_or_paths = [path_or_paths]
        # list/tuple of files (consume what can, and exception those that selected but cannot consume so user knows)
        assert isinstance(
            path_or_paths, (list, tuple, types.GeneratorType)
        ), "Wrong type for path_or_paths: %s" % type(path_or_paths)
        # reform out of allowed types
        globs_image_types.extend(
            flatten_list(
                [
                    [x for x in path_or_paths if x.endswith(y)]
                    for y in image_types
                ]
            )
        )
        # could do below:
        # globs_non_image_types = flatten_list([[x for x in path_or_paths if x.endswith(y)] for y in non_image_types])
        # But instead, allow fail so can collect unsupported too
        set_globs_image_types = set(globs_image_types)
        globs_non_image_types.extend(
            [x for x in path_or_paths if x not in set_globs_image_types]
        )

    # filter out any files to skip (e.g. if already processed them)
    # this is easy, but too aggressive in case a file changed, so parent probably passed existing_files=[]
    assert not existing_files, "DEV: assume not using this approach"
    if existing_files:
        set_skip_files = set(existing_files)
        globs_image_types = [
            x for x in globs_image_types if x not in set_skip_files
        ]
        globs_non_image_types = [
            x for x in globs_non_image_types if x not in set_skip_files
        ]
    if existing_hash_ids:
        # assume consistent with add_meta() use of hash_file(file)
        # also assume consistent with get_existing_hash_ids for dict creation
        # assume hashable values
        existing_hash_ids_set = set(existing_hash_ids.items())
        hash_ids_all_image = set(
            {x: hash_file(x) for x in globs_image_types}.items()
        )
        hash_ids_all_non_image = set(
            {x: hash_file(x) for x in globs_non_image_types}.items()
        )
        # don't use symmetric diff.  If file is gone, ignore and don't remove or something
        #  just consider existing files (key) having new hash or not (value)
        new_files_image = set(
            dict(hash_ids_all_image - existing_hash_ids_set).keys()
        )
        new_files_non_image = set(
            dict(hash_ids_all_non_image - existing_hash_ids_set).keys()
        )
        globs_image_types = [
            x for x in globs_image_types if x in new_files_image
        ]
        globs_non_image_types = [
            x for x in globs_non_image_types if x in new_files_non_image
        ]

    # could use generator, but messes up metadata handling in recursive case
    if (
        caption_loader
        and not isinstance(caption_loader, (bool, str))
        and caption_loader.device != "cpu"
        or args.device == "cuda"
    ):
        # to avoid deadlocks, presume was preloaded and so can't fork due to cuda context
        n_jobs_image = 1
    else:
        n_jobs_image = n_jobs

    return_file = True  # local choice
    is_url = url is not None
    is_txt = text is not None
    kwargs = dict(
        verbose=verbose,
        fail_any_exception=fail_any_exception,
        return_file=return_file,
        chunk=chunk,
        chunk_size=chunk_size,
        is_url=is_url,
        is_txt=is_txt,
        enable_captions=enable_captions,
        captions_model=captions_model,
        caption_loader=caption_loader,
        enable_ocr=enable_ocr,
    )

    if n_jobs != 1 and len(globs_non_image_types) > 1:
        # avoid nesting, e.g. upload 1 zip and then inside many files
        # harder to handle if upload many zips with many files, inner parallel one will be disabled by joblib
        documents = ProgressParallel(
            n_jobs=n_jobs,
            verbose=10 if verbose else 0,
            backend="multiprocessing",
        )(
            delayed(path_to_doc1)(file, **kwargs)
            for file in globs_non_image_types
        )
    else:
        documents = [
            path_to_doc1(file, **kwargs)
            for file in tqdm(globs_non_image_types)
        ]

    # do images separately since can't fork after cuda in parent, so can't be parallel
    if n_jobs_image != 1 and len(globs_image_types) > 1:
        # avoid nesting, e.g. upload 1 zip and then inside many files
        # harder to handle if upload many zips with many files, inner parallel one will be disabled by joblib
        image_documents = ProgressParallel(
            n_jobs=n_jobs,
            verbose=10 if verbose else 0,
            backend="multiprocessing",
        )(delayed(path_to_doc1)(file, **kwargs) for file in globs_image_types)
    else:
        image_documents = [
            path_to_doc1(file, **kwargs) for file in tqdm(globs_image_types)
        ]

    # add image docs in
    documents += image_documents

    if return_file:
        # then documents really are files
        files = documents.copy()
        documents = []
        for fil in files:
            with open(fil, "rb") as f:
                documents.extend(pickle.load(f))
            # remove temp pickle
            os.remove(fil)
    else:
        documents = reduce(concat, documents)
    return documents


def prep_langchain(
    persist_directory,
    load_db_if_exists,
    db_type,
    use_openai_embedding,
    langchain_mode,
    user_path,
    hf_embedding_model,
    n_jobs=-1,
    kwargs_make_db={},
):
    """
    do prep first time, involving downloads
    # FIXME: Add github caching then add here
    :return:
    """
    assert langchain_mode not in ["MyData"], "Should not prep scratch data"

    db_dir_exists = os.path.isdir(persist_directory)

    if db_dir_exists and user_path is None:
        print(
            "Prep: persist_directory=%s exists, using" % persist_directory,
            flush=True,
        )
        db = get_existing_db(
            None,
            persist_directory,
            load_db_if_exists,
            db_type,
            use_openai_embedding,
            langchain_mode,
            hf_embedding_model,
        )
    else:
        if db_dir_exists and user_path is not None:
            print(
                "Prep: persist_directory=%s exists, user_path=%s passed, adding any changed or new documents"
                % (persist_directory, user_path),
                flush=True,
            )
        elif not db_dir_exists:
            print(
                "Prep: persist_directory=%s does not exist, regenerating"
                % persist_directory,
                flush=True,
            )
        db = None
        if langchain_mode in ["All", "DriverlessAI docs"]:
            # FIXME: Could also just use dai_docs.pickle directly and upload that
            get_dai_docs(from_hf=True)

        if langchain_mode in ["All", "wiki"]:
            get_wiki_sources(
                first_para=kwargs_make_db["first_para"],
                text_limit=kwargs_make_db["text_limit"],
            )

        langchain_kwargs = kwargs_make_db.copy()
        langchain_kwargs.update(locals())
        db, num_new_sources, new_sources_metadata = make_db(**langchain_kwargs)

    return db


import posthog

posthog.disabled = True


class FakeConsumer(object):
    def __init__(self, *args, **kwargs):
        pass

    def run(self):
        pass

    def pause(self):
        pass

    def upload(self):
        pass

    def next(self):
        pass

    def request(self, batch):
        pass


posthog.Consumer = FakeConsumer


def check_update_chroma_embedding(
    db, use_openai_embedding, hf_embedding_model, langchain_mode
):
    changed_db = False
    if load_embed(db) != (use_openai_embedding, hf_embedding_model):
        print(
            "Detected new embedding, updating db: %s" % langchain_mode,
            flush=True,
        )
        # handle embedding changes
        db_get = get_documents(db)
        sources = [
            Document(page_content=result[0], metadata=result[1] or {})
            for result in zip(db_get["documents"], db_get["metadatas"])
        ]
        # delete index, has to be redone
        persist_directory = db._persist_directory
        shutil.move(
            persist_directory,
            persist_directory + "_" + str(uuid.uuid4()) + ".bak",
        )
        db_type = "chroma"
        load_db_if_exists = False
        db = get_db(
            sources,
            use_openai_embedding=use_openai_embedding,
            db_type=db_type,
            persist_directory=persist_directory,
            load_db_if_exists=load_db_if_exists,
            langchain_mode=langchain_mode,
            collection_name=None,
            hf_embedding_model=hf_embedding_model,
        )
        if False:
            # below doesn't work if db already in memory, so have to switch to new db as above
            # upsert does new embedding, but if index already in memory, complains about size mismatch etc.
            client_collection = db._client.get_collection(
                name=db._collection.name,
                embedding_function=db._collection._embedding_function,
            )
            client_collection.upsert(
                ids=db_get["ids"],
                metadatas=db_get["metadatas"],
                documents=db_get["documents"],
            )
        changed_db = True
        print(
            "Done updating db for new embedding: %s" % langchain_mode,
            flush=True,
        )

    return db, changed_db


def get_existing_db(
    db,
    persist_directory,
    load_db_if_exists,
    db_type,
    use_openai_embedding,
    langchain_mode,
    hf_embedding_model,
    verbose=False,
    check_embedding=True,
):
    if (
        load_db_if_exists
        and db_type == "chroma"
        and os.path.isdir(persist_directory)
        and os.path.isdir(os.path.join(persist_directory, "index"))
    ):
        if db is None:
            if verbose:
                print("DO Loading db: %s" % langchain_mode, flush=True)
            embedding = get_embedding(
                use_openai_embedding, hf_embedding_model=hf_embedding_model
            )
            from chromadb.config import Settings

            client_settings = Settings(
                anonymized_telemetry=False,
                chroma_db_impl="duckdb+parquet",
                persist_directory=persist_directory,
            )
            db = Chroma(
                persist_directory=persist_directory,
                embedding_function=embedding,
                collection_name=langchain_mode.replace(" ", "_"),
                client_settings=client_settings,
            )
            if verbose:
                print("DONE Loading db: %s" % langchain_mode, flush=True)
        else:
            if verbose:
                print(
                    "USING already-loaded db: %s" % langchain_mode, flush=True
                )
        if check_embedding:
            db_trial, changed_db = check_update_chroma_embedding(
                db, use_openai_embedding, hf_embedding_model, langchain_mode
            )
            if changed_db:
                db = db_trial
                # only call persist if really changed db, else takes too long for large db
                if db is not None:
                    db.persist()
                    clear_embedding(db)
        save_embed(db, use_openai_embedding, hf_embedding_model)
        return db
    return None


def clear_embedding(db):
    if db is None:
        return
    # don't keep on GPU, wastes memory, push back onto CPU and only put back on GPU once again embed
    db._embedding_function.client.cpu()
    clear_torch_cache()


def make_db(**langchain_kwargs):
    func_names = list(inspect.signature(_make_db).parameters)
    missing_kwargs = [x for x in func_names if x not in langchain_kwargs]
    defaults_db = {
        k: v.default
        for k, v in dict(inspect.signature(run_qa_db).parameters).items()
    }
    for k in missing_kwargs:
        if k in defaults_db:
            langchain_kwargs[k] = defaults_db[k]
    # final check for missing
    missing_kwargs = [x for x in func_names if x not in langchain_kwargs]
    assert not missing_kwargs, "Missing kwargs: %s" % missing_kwargs
    # only keep actual used
    langchain_kwargs = {
        k: v for k, v in langchain_kwargs.items() if k in func_names
    }
    return _make_db(**langchain_kwargs)


def save_embed(db, use_openai_embedding, hf_embedding_model):
    if db is not None:
        embed_info_file = os.path.join(db._persist_directory, "embed_info")
        with open(embed_info_file, "wb") as f:
            pickle.dump((use_openai_embedding, hf_embedding_model), f)
    return use_openai_embedding, hf_embedding_model


def load_embed(db):
    embed_info_file = os.path.join(db._persist_directory, "embed_info")
    if os.path.isfile(embed_info_file):
        with open(embed_info_file, "rb") as f:
            use_openai_embedding, hf_embedding_model = pickle.load(f)
    else:
        # migration, assume defaults
        use_openai_embedding, hf_embedding_model = (
            False,
            "sentence-transformers/all-MiniLM-L6-v2",
        )
    return use_openai_embedding, hf_embedding_model


def get_persist_directory(langchain_mode):
    return (
        "db_dir_%s" % langchain_mode
    )  # single place, no special names for each case


def _make_db(
    use_openai_embedding=False,
    hf_embedding_model="sentence-transformers/all-MiniLM-L6-v2",
    first_para=False,
    text_limit=None,
    chunk=True,
    chunk_size=512,
    langchain_mode=None,
    user_path=None,
    db_type="faiss",
    load_db_if_exists=True,
    db=None,
    n_jobs=-1,
    verbose=False,
):
    persist_directory = get_persist_directory(langchain_mode)
    # see if can get persistent chroma db
    db_trial = get_existing_db(
        db,
        persist_directory,
        load_db_if_exists,
        db_type,
        use_openai_embedding,
        langchain_mode,
        hf_embedding_model,
        verbose=verbose,
    )
    if db_trial is not None:
        db = db_trial

    sources = []
    if (
        not db
        and langchain_mode not in ["MyData"]
        or user_path is not None
        and langchain_mode in ["UserData"]
    ):
        # Should not make MyData db this way, why avoided, only upload from UI
        assert langchain_mode not in [
            "MyData"
        ], "Should not make MyData db this way"
        if verbose:
            if langchain_mode in ["UserData"]:
                if user_path is not None:
                    print(
                        "Checking if changed or new sources in %s, and generating sources them"
                        % user_path,
                        flush=True,
                    )
                elif db is None:
                    print(
                        "user_path not passed and no db, no sources",
                        flush=True,
                    )
                else:
                    print(
                        "user_path not passed, using only existing db, no new sources",
                        flush=True,
                    )
            else:
                print("Generating %s sources" % langchain_mode, flush=True)
        if langchain_mode in ["wiki_full", "All", "'All'"]:
            from read_wiki_full import get_all_documents

            small_test = None
            print("Generating new wiki", flush=True)
            sources1 = get_all_documents(
                small_test=small_test, n_jobs=os.cpu_count() // 2
            )
            print("Got new wiki", flush=True)
            if chunk:
                sources1 = chunk_sources(
                    sources1, chunk=chunk, chunk_size=chunk_size
                )
                print("Chunked new wiki", flush=True)
            sources.extend(sources1)
        if langchain_mode in ["wiki", "All", "'All'"]:
            sources1 = get_wiki_sources(
                first_para=first_para, text_limit=text_limit
            )
            if chunk:
                sources1 = chunk_sources(
                    sources1, chunk=chunk, chunk_size=chunk_size
                )
            sources.extend(sources1)
        if langchain_mode in ["github h2oGPT", "All", "'All'"]:
            # sources = get_github_docs("dagster-io", "dagster")
            sources1 = get_github_docs("h2oai", "h2ogpt")
            # FIXME: always chunk for now
            sources1 = chunk_sources(
                sources1, chunk=chunk, chunk_size=chunk_size
            )
            sources.extend(sources1)
        if langchain_mode in ["DriverlessAI docs", "All", "'All'"]:
            sources1 = get_dai_docs(from_hf=True)
            if (
                chunk and False
            ):  # FIXME: DAI docs are already chunked well, should only chunk more if over limit
                sources1 = chunk_sources(
                    sources1, chunk=chunk, chunk_size=chunk_size
                )
            sources.extend(sources1)
        if langchain_mode in ["All", "UserData"]:
            if user_path:
                if db is not None:
                    # NOTE: Ignore file names for now, only go by hash ids
                    # existing_files = get_existing_files(db)
                    existing_files = []
                    existing_hash_ids = get_existing_hash_ids(db)
                else:
                    # pretend no existing files so won't filter
                    existing_files = []
                    existing_hash_ids = []
                # chunk internally for speed over multiple docs
                # FIXME: If first had old Hash=None and switch embeddings,
                #  then re-embed, and then hit here and reload so have hash, and then re-embed.
                sources1 = path_to_docs(
                    user_path,
                    n_jobs=n_jobs,
                    chunk=chunk,
                    chunk_size=chunk_size,
                    existing_files=existing_files,
                    existing_hash_ids=existing_hash_ids,
                )
                new_metadata_sources = set(
                    [x.metadata["source"] for x in sources1]
                )
                if new_metadata_sources:
                    print(
                        "Loaded %s new files as sources to add to UserData"
                        % len(new_metadata_sources),
                        flush=True,
                    )
                    if verbose:
                        print(
                            "Files added: %s"
                            % "\n".join(new_metadata_sources),
                            flush=True,
                        )
                sources.extend(sources1)
                print(
                    "Loaded %s sources for potentially adding to UserData"
                    % len(sources),
                    flush=True,
                )
            else:
                print("Chose UserData but user_path is empty/None", flush=True)
        if False and langchain_mode in ["urls", "All", "'All'"]:
            # from langchain.document_loaders import UnstructuredURLLoader
            # loader = UnstructuredURLLoader(urls=urls)
            urls = ["https://www.birdsongsf.com/who-we-are/"]
            from langchain.document_loaders import PlaywrightURLLoader

            loader = PlaywrightURLLoader(
                urls=urls, remove_selectors=["header", "footer"]
            )
            sources1 = loader.load()
            sources.extend(sources1)
        if not sources:
            if verbose:
                if db is not None:
                    print(
                        "langchain_mode %s has no new sources, nothing to add to db"
                        % langchain_mode,
                        flush=True,
                    )
                else:
                    print(
                        "langchain_mode %s has no sources, not making new db"
                        % langchain_mode,
                        flush=True,
                    )
            return db, 0, []
        if verbose:
            if db is not None:
                print("Generating db", flush=True)
            else:
                print("Adding to db", flush=True)
    if not db:
        if sources:
            db = get_db(
                sources,
                use_openai_embedding=use_openai_embedding,
                db_type=db_type,
                persist_directory=persist_directory,
                langchain_mode=langchain_mode,
                hf_embedding_model=hf_embedding_model,
            )
            if verbose:
                print("Generated db", flush=True)
        else:
            print("Did not generate db since no sources", flush=True)
        new_sources_metadata = [x.metadata for x in sources]
    elif user_path is not None and langchain_mode in ["UserData"]:
        print(
            "Existing db, potentially adding %s sources from user_path=%s"
            % (len(sources), user_path),
            flush=True,
        )
        db, num_new_sources, new_sources_metadata = add_to_db(
            db,
            sources,
            db_type=db_type,
            use_openai_embedding=use_openai_embedding,
            hf_embedding_model=hf_embedding_model,
        )
        print(
            "Existing db, added %s new sources from user_path=%s"
            % (num_new_sources, user_path),
            flush=True,
        )
    else:
        new_sources_metadata = [x.metadata for x in sources]

    return db, len(new_sources_metadata), new_sources_metadata


def get_metadatas(db):
    from langchain.vectorstores import FAISS

    if isinstance(db, FAISS):
        metadatas = [v.metadata for k, v in db.docstore._dict.items()]
    elif isinstance(db, Chroma):
        metadatas = get_documents(db)["metadatas"]
    else:
        # FIXME: Hack due to https://github.com/weaviate/weaviate/issues/1947
        # seems no way to get all metadata, so need to avoid this approach for weaviate
        metadatas = [x.metadata for x in db.similarity_search("", k=10000)]
    return metadatas


def get_documents(db):
    if hasattr(db, "_persist_directory"):
        name_path = os.path.basename(db._persist_directory)
        base_path = "locks"
        makedirs(base_path)
        with filelock.FileLock(
            os.path.join(base_path, "getdb_%s.lock" % name_path)
        ):
            # get segfaults and other errors when multiple threads access this
            return _get_documents(db)
    else:
        return _get_documents(db)


def _get_documents(db):
    from langchain.vectorstores import FAISS

    if isinstance(db, FAISS):
        documents = [v for k, v in db.docstore._dict.items()]
    elif isinstance(db, Chroma):
        documents = db.get()
    else:
        # FIXME: Hack due to https://github.com/weaviate/weaviate/issues/1947
        # seems no way to get all metadata, so need to avoid this approach for weaviate
        documents = [x for x in db.similarity_search("", k=10000)]
    return documents


def get_docs_and_meta(db, top_k_docs, filter_kwargs={}):
    if hasattr(db, "_persist_directory"):
        name_path = os.path.basename(db._persist_directory)
        base_path = "locks"
        makedirs(base_path)
        with filelock.FileLock(
            os.path.join(base_path, "getdb_%s.lock" % name_path)
        ):
            return _get_docs_and_meta(
                db, top_k_docs, filter_kwargs=filter_kwargs
            )
    else:
        return _get_docs_and_meta(db, top_k_docs, filter_kwargs=filter_kwargs)


def _get_docs_and_meta(db, top_k_docs, filter_kwargs={}):
    from langchain.vectorstores import FAISS

    if isinstance(db, Chroma):
        db_get = db._collection.get(where=filter_kwargs.get("filter"))
        db_metadatas = db_get["metadatas"]
        db_documents = db_get["documents"]
    elif isinstance(db, FAISS):
        import itertools

        db_metadatas = get_metadatas(db)
        # FIXME: FAISS has no filter
        # slice dict first
        db_documents = list(
            dict(
                itertools.islice(db.docstore._dict.items(), top_k_docs)
            ).values()
        )
    else:
        db_metadatas = get_metadatas(db)
        db_documents = get_documents(db)
    return db_documents, db_metadatas


def get_existing_files(db):
    metadatas = get_metadatas(db)
    metadata_sources = set([x["source"] for x in metadatas])
    return metadata_sources


def get_existing_hash_ids(db):
    metadatas = get_metadatas(db)
    # assume consistency, that any prior hashed source was single hashed file at the time among all source chunks
    metadata_hash_ids = {x["source"]: x.get("hashid") for x in metadatas}
    return metadata_hash_ids


def run_qa_db(**kwargs):
    func_names = list(inspect.signature(_run_qa_db).parameters)
    # hard-coded defaults
    kwargs["answer_with_sources"] = True
    kwargs["show_rank"] = False
    missing_kwargs = [x for x in func_names if x not in kwargs]
    assert not missing_kwargs, "Missing kwargs: %s" % missing_kwargs
    # only keep actual used
    kwargs = {k: v for k, v in kwargs.items() if k in func_names}
    try:
        return _run_qa_db(**kwargs)
    finally:
        clear_torch_cache()


def _run_qa_db(
    query=None,
    iinput=None,
    context=None,
    use_openai_model=False,
    use_openai_embedding=False,
    first_para=False,
    text_limit=None,
    top_k_docs=4,
    chunk=True,
    chunk_size=512,
    user_path=None,
    detect_user_path_changes_every_query=False,
    db_type="faiss",
    model_name=None,
    model=None,
    tokenizer=None,
    inference_server=None,
    hf_embedding_model="sentence-transformers/all-MiniLM-L6-v2",
    stream_output=False,
    prompter=None,
    prompt_type=None,
    prompt_dict=None,
    answer_with_sources=True,
    cut_distanct=1.1,
    sanitize_bot_response=False,
    show_rank=False,
    load_db_if_exists=False,
    db=None,
    do_sample=False,
    temperature=0.1,
    top_k=40,
    top_p=0.7,
    num_beams=1,
    max_new_tokens=256,
    min_new_tokens=1,
    early_stopping=False,
    max_time=180,
    repetition_penalty=1.0,
    num_return_sequences=1,
    langchain_mode=None,
    langchain_action=None,
    document_choice=[DocumentChoices.All_Relevant.name],
    n_jobs=-1,
    verbose=False,
    cli=False,
    reverse_docs=True,
    lora_weights="",
    auto_reduce_chunks=True,
    max_chunks=100,
):
    """

    :param query:
    :param use_openai_model:
    :param use_openai_embedding:
    :param first_para:
    :param text_limit:
    :param top_k_docs:
    :param chunk:
    :param chunk_size:
    :param user_path: user path to glob recursively from
    :param db_type: 'faiss' for in-memory db or 'chroma' or 'weaviate' for persistent db
    :param model_name: model name, used to switch behaviors
    :param model: pre-initialized model, else will make new one
    :param tokenizer: pre-initialized tokenizer, else will make new one.  Required not None if model is not None
    :param answer_with_sources
    :return:
    """
    if model is not None:
        assert model_name is not None  # require so can make decisions
    assert query is not None
    assert (
        prompter is not None or prompt_type is not None or model is None
    )  # if model is None, then will generate
    if prompter is not None:
        prompt_type = prompter.prompt_type
        prompt_dict = prompter.prompt_dict
    if model is not None:
        assert prompt_type is not None
        if prompt_type == PromptType.custom.name:
            assert prompt_dict is not None  # should at least be {} or ''
        else:
            prompt_dict = ""
    assert (
        len(set(gen_hyper).difference(inspect.signature(get_llm).parameters))
        == 0
    )
    llm, model_name, streamer, prompt_type_out = get_llm(
        use_openai_model=use_openai_model,
        model_name=model_name,
        model=model,
        tokenizer=tokenizer,
        inference_server=inference_server,
        stream_output=stream_output,
        do_sample=do_sample,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        num_beams=num_beams,
        max_new_tokens=max_new_tokens,
        min_new_tokens=min_new_tokens,
        early_stopping=early_stopping,
        max_time=max_time,
        repetition_penalty=repetition_penalty,
        num_return_sequences=num_return_sequences,
        prompt_type=prompt_type,
        prompt_dict=prompt_dict,
        prompter=prompter,
        sanitize_bot_response=sanitize_bot_response,
        verbose=verbose,
    )

    use_context = False
    scores = []
    chain = None

    if isinstance(document_choice, str):
        # support string as well
        document_choice = [document_choice]
    # get first DocumentChoices as command to use, ignore others
    doc_choices_set = set([x.name for x in list(DocumentChoices)])
    cmd = [x for x in document_choice if x in doc_choices_set]
    cmd = None if len(cmd) == 0 else cmd[0]
    # now have cmd, filter out for only docs
    document_choice = [x for x in document_choice if x not in doc_choices_set]

    func_names = list(inspect.signature(get_similarity_chain).parameters)
    sim_kwargs = {k: v for k, v in locals().items() if k in func_names}
    missing_kwargs = [x for x in func_names if x not in sim_kwargs]
    assert not missing_kwargs, "Missing: %s" % missing_kwargs
    docs, chain, scores, use_context, have_any_docs = get_similarity_chain(
        **sim_kwargs
    )
    if cmd in non_query_commands:
        formatted_doc_chunks = "\n\n".join(
            [get_url(x) + "\n\n" + x.page_content for x in docs]
        )
        return formatted_doc_chunks, ""
    if not docs and langchain_action in [
        LangChainAction.SUMMARIZE_MAP.value,
        LangChainAction.SUMMARIZE_ALL.value,
        LangChainAction.SUMMARIZE_REFINE.value,
    ]:
        ret = (
            "No relevant documents to summarize."
            if have_any_docs
            else "No documents to summarize."
        )
        extra = ""
        return ret, extra
    if not docs and langchain_mode not in [
        LangChainMode.DISABLED.value,
        LangChainMode.CHAT_LLM.value,
        LangChainMode.LLM.value,
    ]:
        ret = (
            "No relevant documents to query."
            if have_any_docs
            else "No documents to query."
        )
        extra = ""
        return ret, extra

    if chain is None and model_name not in non_hf_types:
        # here if no docs at all and not HF type
        # can only return if HF type
        return

    # context stuff similar to used in evaluate()
    import torch

    torch_dtype, context_class = get_dtype()
    with torch.no_grad():
        have_lora_weights = lora_weights not in [no_lora_str, "", None]
        context_class_cast = (
            NullContext
            if args.device == "cpu" or have_lora_weights
            else torch.autocast
        )
        with context_class_cast(args.device):
            answer = chain()
            return answer


def get_similarity_chain(
    query=None,
    iinput=None,
    use_openai_model=False,
    use_openai_embedding=False,
    first_para=False,
    text_limit=None,
    top_k_docs=4,
    chunk=True,
    chunk_size=512,
    user_path=None,
    detect_user_path_changes_every_query=False,
    db_type="faiss",
    model_name=None,
    inference_server="",
    hf_embedding_model="sentence-transformers/all-MiniLM-L6-v2",
    prompt_type=None,
    prompt_dict=None,
    cut_distanct=1.1,
    load_db_if_exists=False,
    db=None,
    langchain_mode=None,
    langchain_action=None,
    document_choice=[DocumentChoices.All_Relevant.name],
    n_jobs=-1,
    # beyond run_db_query:
    llm=None,
    tokenizer=None,
    verbose=False,
    cmd=None,
    reverse_docs=True,
    # local
    auto_reduce_chunks=True,
    max_chunks=100,
):
    # determine whether use of context out of docs is planned
    if (
        not use_openai_model
        and prompt_type not in ["plain"]
        or model_name in non_hf_types
    ):
        if langchain_mode in ["Disabled", "ChatLLM", "LLM"]:
            use_context = False
        else:
            use_context = True
    else:
        use_context = True

    # https://github.com/hwchase17/langchain/issues/1946
    # FIXME: Seems to way to get size of chroma db to limit top_k_docs to avoid
    # Chroma collection MyData contains fewer than 4 elements.
    # type logger error
    if top_k_docs == -1:
        k_db = 1000 if db_type == "chroma" else 100
    else:
        # top_k_docs=100 works ok too
        k_db = 1000 if db_type == "chroma" else top_k_docs

    # FIXME: For All just go over all dbs instead of a separate db for All
    if not detect_user_path_changes_every_query and db is not None:
        # avoid looking at user_path during similarity search db handling,
        # if already have db and not updating from user_path every query
        # but if db is None, no db yet loaded (e.g. from prep), so allow user_path to be whatever it was
        user_path = None
    db, num_new_sources, new_sources_metadata = make_db(
        use_openai_embedding=use_openai_embedding,
        hf_embedding_model=hf_embedding_model,
        first_para=first_para,
        text_limit=text_limit,
        chunk=chunk,
        chunk_size=chunk_size,
        langchain_mode=langchain_mode,
        user_path=user_path,
        db_type=db_type,
        load_db_if_exists=load_db_if_exists,
        db=db,
        n_jobs=n_jobs,
        verbose=verbose,
    )
    have_any_docs = db is not None
    if langchain_action == LangChainAction.QUERY.value:
        if iinput:
            query = "%s\n%s" % (query, iinput)

        if "falcon" in model_name:
            extra = "According to only the information in the document sources provided within the context above, "
            prefix = "Pay attention and remember information below, which will help to answer the question or imperative after the context ends."
        elif inference_server in ["openai", "openai_chat"]:
            extra = "According to (primarily) the information in the document sources provided within context above, "
            prefix = "Pay attention and remember information below, which will help to answer the question or imperative after the context ends.  If the answer cannot be primarily obtained from information within the context, then respond that the answer does not appear in the context of the documents."
        else:
            extra = ""
            prefix = ""
        if langchain_mode in ["Disabled", "ChatLLM", "LLM"] or not use_context:
            template_if_no_docs = template = (
                """%s{context}{question}""" % prefix
            )
        else:
            template = """%s
    \"\"\"
    {context}
    \"\"\"
    %s{question}""" % (
                prefix,
                extra,
            )
            template_if_no_docs = """%s{context}%s{question}""" % (
                prefix,
                extra,
            )
    elif langchain_action in [
        LangChainAction.SUMMARIZE_ALL.value,
        LangChainAction.SUMMARIZE_MAP.value,
    ]:
        none = ["", "\n", None]
        if query in none and iinput in none:
            prompt_summary = "Using only the text above, write a condensed and concise summary:\n"
        elif query not in none:
            prompt_summary = (
                "Focusing on %s, write a condensed and concise Summary:\n"
                % query
            )
        elif iinput not in None:
            prompt_summary = iinput
        else:
            prompt_summary = "Focusing on %s, %s:\n" % (query, iinput)
        # don't auto reduce
        auto_reduce_chunks = False
        if langchain_action == LangChainAction.SUMMARIZE_MAP.value:
            fstring = "{text}"
        else:
            fstring = "{input_documents}"
        template = """In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text:
\"\"\"
%s
\"\"\"\n%s""" % (
            fstring,
            prompt_summary,
        )
        template_if_no_docs = (
            "Exactly only say: There are no documents to summarize."
        )
    elif langchain_action in [LangChainAction.SUMMARIZE_REFINE]:
        template = ""  # unused
        template_if_no_docs = ""  # unused
    else:
        raise RuntimeError("No such langchain_action=%s" % langchain_action)

    if (
        not use_openai_model
        and prompt_type not in ["plain"]
        or model_name in non_hf_types
    ):
        use_template = True
    else:
        use_template = False

    if db and use_context:
        base_path = "locks"
        makedirs(base_path)
        if hasattr(db, "_persist_directory"):
            name_path = "sim_%s.lock" % os.path.basename(db._persist_directory)
        else:
            name_path = "sim.lock"
        lock_file = os.path.join(base_path, name_path)

        if not isinstance(db, Chroma):
            # only chroma supports filtering
            filter_kwargs = {}
        else:
            # if here then some cmd + documents selected or just documents selected
            if len(document_choice) >= 2:
                or_filter = [{"source": {"$eq": x}} for x in document_choice]
                filter_kwargs = dict(filter={"$or": or_filter})
            elif len(document_choice) == 1:
                # degenerate UX bug in chroma
                one_filter = [{"source": {"$eq": x}} for x in document_choice][
                    0
                ]
                filter_kwargs = dict(filter=one_filter)
            else:
                # shouldn't reach
                filter_kwargs = {}
        if cmd == DocumentChoices.Just_LLM.name:
            docs = []
            scores = []
        elif cmd == DocumentChoices.Only_All_Sources.name or query in [
            None,
            "",
            "\n",
        ]:
            db_documents, db_metadatas = get_docs_and_meta(
                db, top_k_docs, filter_kwargs=filter_kwargs
            )
            # similar to langchain's chroma's _results_to_docs_and_scores
            docs_with_score = [
                (Document(page_content=result[0], metadata=result[1] or {}), 0)
                for result in zip(db_documents, db_metadatas)
            ]

            # order documents
            doc_hashes = [x["doc_hash"] for x in db_metadatas]
            doc_chunk_ids = [x["chunk_id"] for x in db_metadatas]
            docs_with_score = [
                x
                for _, _, x in sorted(
                    zip(doc_hashes, doc_chunk_ids, docs_with_score),
                    key=lambda x: (x[0], x[1]),
                )
            ]

            docs_with_score = docs_with_score[:top_k_docs]
            docs = [x[0] for x in docs_with_score]
            scores = [x[1] for x in docs_with_score]
            have_any_docs |= len(docs) > 0
        else:
            # FIXME: if langchain_action == LangChainAction.SUMMARIZE_MAP.value
            # if map_reduce, then no need to auto reduce chunks
            if top_k_docs == -1 or auto_reduce_chunks:
                # docs_with_score = db.similarity_search_with_score(query, k=k_db, **filter_kwargs)[:top_k_docs]
                top_k_docs_tokenize = 100
                with filelock.FileLock(lock_file):
                    docs_with_score = db.similarity_search_with_score(
                        query, k=k_db, **filter_kwargs
                    )[:top_k_docs_tokenize]
                if hasattr(llm, "pipeline") and hasattr(
                    llm.pipeline, "tokenizer"
                ):
                    # more accurate
                    tokens = [
                        len(
                            llm.pipeline.tokenizer(x[0].page_content)[
                                "input_ids"
                            ]
                        )
                        for x in docs_with_score
                    ]
                    template_tokens = len(
                        llm.pipeline.tokenizer(template)["input_ids"]
                    )
                elif (
                    inference_server in ["openai", "openai_chat"]
                    or use_openai_model
                    or db_type in ["faiss", "weaviate"]
                ):
                    # use ticktoken for faiss since embedding called differently
                    tokens = [
                        llm.get_num_tokens(x[0].page_content)
                        for x in docs_with_score
                    ]
                    template_tokens = llm.get_num_tokens(template)
                elif isinstance(tokenizer, FakeTokenizer):
                    tokens = [
                        tokenizer.num_tokens_from_string(x[0].page_content)
                        for x in docs_with_score
                    ]
                    template_tokens = tokenizer.num_tokens_from_string(
                        template
                    )
                else:
                    # in case model is not our pipeline with HF tokenizer
                    tokens = [
                        db._embedding_function.client.tokenize(
                            [x[0].page_content]
                        )["input_ids"].shape[1]
                        for x in docs_with_score
                    ]
                    template_tokens = db._embedding_function.client.tokenize(
                        [template]
                    )["input_ids"].shape[1]
                tokens_cumsum = np.cumsum(tokens)
                if hasattr(llm, "pipeline") and hasattr(
                    llm.pipeline, "max_input_tokens"
                ):
                    max_input_tokens = llm.pipeline.max_input_tokens
                elif inference_server in ["openai"]:
                    max_tokens = llm.modelname_to_contextsize(model_name)
                    # leave some room for 1 paragraph, even if min_new_tokens=0
                    max_input_tokens = max_tokens - 256
                elif inference_server in ["openai_chat"]:
                    max_tokens = model_token_mapping[model_name]
                    # leave some room for 1 paragraph, even if min_new_tokens=0
                    max_input_tokens = max_tokens - 256
                elif isinstance(tokenizer, FakeTokenizer):
                    max_input_tokens = tokenizer.model_max_length - 256
                else:
                    # leave some room for 1 paragraph, even if min_new_tokens=0
                    max_input_tokens = 2048 - 256
                max_input_tokens -= template_tokens
                # FIXME: Doesn't account for query, == context, or new lines between contexts
                where_res = np.where(tokens_cumsum < max_input_tokens)[0]
                if where_res.shape[0] == 0:
                    # then no chunk can fit, still do first one
                    top_k_docs_trial = 1
                else:
                    top_k_docs_trial = 1 + where_res[-1]
                if 0 < top_k_docs_trial < max_chunks:
                    # avoid craziness
                    if top_k_docs == -1:
                        top_k_docs = top_k_docs_trial
                    else:
                        top_k_docs = min(top_k_docs, top_k_docs_trial)
                if top_k_docs == -1:
                    # if here, means 0 and just do best with 1 doc
                    print(
                        "Unexpected large chunks and can't add to context, will add 1 anyways",
                        flush=True,
                    )
                    top_k_docs = 1
                docs_with_score = docs_with_score[:top_k_docs]
            else:
                with filelock.FileLock(lock_file):
                    docs_with_score = db.similarity_search_with_score(
                        query, k=k_db, **filter_kwargs
                    )[:top_k_docs]
            # put most relevant chunks closest to question,
            # esp. if truncation occurs will be "oldest" or "farthest from response" text that is truncated
            # BUT: for small models, e.g. 6_9 pythia, if sees some stuff related to h2oGPT first, it can connect that and not listen to rest
            if reverse_docs:
                docs_with_score.reverse()
            # cut off so no high distance docs/sources considered
            have_any_docs |= len(docs_with_score) > 0  # before cut
            docs = [x[0] for x in docs_with_score if x[1] < cut_distanct]
            scores = [x[1] for x in docs_with_score if x[1] < cut_distanct]
            if len(scores) > 0 and verbose:
                print(
                    "Distance: min: %s max: %s mean: %s median: %s"
                    % (
                        scores[0],
                        scores[-1],
                        np.mean(scores),
                        np.median(scores),
                    ),
                    flush=True,
                )
    else:
        docs = []
        scores = []

    if not docs and use_context and model_name not in non_hf_types:
        # if HF type and have no docs, can bail out
        return docs, None, [], False, have_any_docs

    if cmd in non_query_commands:
        # no LLM use
        return docs, None, [], False, have_any_docs

    common_words_file = "data/NGSL_1.2_stats.csv.zip"
    if (
        os.path.isfile(common_words_file)
        and langchain_mode == LangChainAction.QUERY.value
    ):
        df = pd.read_csv("data/NGSL_1.2_stats.csv.zip")
        import string

        reduced_query = query.translate(
            str.maketrans(string.punctuation, " " * len(string.punctuation))
        ).strip()
        reduced_query_words = reduced_query.split(" ")
        set_common = set(df["Lemma"].values.tolist())
        num_common = len(
            [x.lower() in set_common for x in reduced_query_words]
        )
        frac_common = num_common / len(reduced_query) if reduced_query else 0
        # FIXME: report to user bad query that uses too many common words
        if verbose:
            print("frac_common: %s" % frac_common, flush=True)

    if len(docs) == 0:
        # avoid context == in prompt then
        use_context = False
        template = template_if_no_docs

    if langchain_action == LangChainAction.QUERY.value:
        if use_template:
            # instruct-like, rather than few-shot prompt_type='plain' as default
            # but then sources confuse the model with how inserted among rest of text, so avoid
            prompt = PromptTemplate(
                # input_variables=["summaries", "question"],
                input_variables=["context", "question"],
                template=template,
            )
            chain = load_qa_chain(llm, prompt=prompt)
        chain_kwargs = dict(input_documents=docs, question=query)
        target = wrapped_partial(chain, chain_kwargs)
    else:
        raise RuntimeError("No such langchain_action=%s" % langchain_action)

    return docs, target, scores, use_context, have_any_docs


def get_sources_answer(
    query, answer, scores, show_rank, answer_with_sources, verbose=False
):
    if verbose:
        print("query: %s" % query, flush=True)
        print("answer: %s" % answer["output_text"], flush=True)

    if len(answer["input_documents"]) == 0:
        extra = ""
        ret = answer["output_text"] + extra
        return ret, extra

    # link
    answer_sources = [
        (max(0.0, 1.5 - score) / 1.5, get_url(doc))
        for score, doc in zip(scores, answer["input_documents"])
    ]
    answer_sources_dict = defaultdict(list)
    [answer_sources_dict[url].append(score) for score, url in answer_sources]
    answers_dict = {}
    for url, scores_url in answer_sources_dict.items():
        answers_dict[url] = np.max(scores_url)
    answer_sources = [(score, url) for url, score in answers_dict.items()]
    answer_sources.sort(key=lambda x: x[0], reverse=True)
    if show_rank:
        # answer_sources = ['%d | %s' % (1 + rank, url) for rank, (score, url) in enumerate(answer_sources)]
        # sorted_sources_urls = "Sources [Rank | Link]:<br>" + "<br>".join(answer_sources)
        answer_sources = [
            "%s" % url for rank, (score, url) in enumerate(answer_sources)
        ]
        sorted_sources_urls = "Ranked Sources:<br>" + "<br>".join(
            answer_sources
        )
    else:
        answer_sources = [
            "<li>%.2g | %s</li>" % (score, url)
            for score, url in answer_sources
        ]
        sorted_sources_urls = f"{source_prefix}<p><ul>" + "<p>".join(
            answer_sources
        )
        sorted_sources_urls += f"</ul></p>{source_postfix}"

    if not answer["output_text"].endswith("\n"):
        answer["output_text"] += "\n"

    if answer_with_sources:
        extra = "\n" + sorted_sources_urls
    else:
        extra = ""
    ret = answer["output_text"] + extra
    return ret, extra


def clean_doc(docs1):
    if not isinstance(docs1, (list, tuple, types.GeneratorType)):
        docs1 = [docs1]
    for doci, doc in enumerate(docs1):
        docs1[doci].page_content = "\n".join(
            [x.strip() for x in doc.page_content.split("\n") if x.strip()]
        )
    return docs1


def chunk_sources(sources, chunk=True, chunk_size=512, language=None):
    if not chunk:
        return sources
    if not isinstance(
        sources, (list, tuple, types.GeneratorType)
    ) and not callable(sources):
        # if just one document
        sources = [sources]
    if language and False:
        # Bug in langchain, keep separator=True not working
        # https://github.com/hwchase17/langchain/issues/2836
        # so avoid this for now
        keep_separator = True
        separators = (
            RecursiveCharacterTextSplitter.get_separators_for_language(
                language
            )
        )
    else:
        separators = ["\n\n", "\n", " ", ""]
        keep_separator = False
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=0,
        keep_separator=keep_separator,
        separators=separators,
    )
    source_chunks = splitter.split_documents(sources)

    # currently in order, but when pull from db won't be, so mark order and document by hash
    doc_hash = str(uuid.uuid4())[:10]
    [
        x.metadata.update(dict(doc_hash=doc_hash, chunk_id=chunk_id))
        for chunk_id, x in enumerate(source_chunks)
    ]

    return source_chunks


def get_db_from_hf(dest=".", db_dir="db_dir_DriverlessAI_docs.zip"):
    from huggingface_hub import hf_hub_download

    # True for case when locally already logged in with correct token, so don't have to set key
    token = os.getenv("HUGGINGFACE_API_TOKEN", True)
    path_to_zip_file = hf_hub_download(
        "h2oai/db_dirs", db_dir, token=token, repo_type="dataset"
    )
    import zipfile

    with zipfile.ZipFile(path_to_zip_file, "r") as zip_ref:
        persist_directory = os.path.dirname(zip_ref.namelist()[0])
        remove(persist_directory)
        zip_ref.extractall(dest)
    return path_to_zip_file


# Note dir has space in some cases, while zip does not
some_db_zips = [
    [
        "db_dir_DriverlessAI_docs.zip",
        "db_dir_DriverlessAI docs",
        "CC-BY-NC license",
    ],
    ["db_dir_UserData.zip", "db_dir_UserData", "CC-BY license for ArXiv"],
    ["db_dir_github_h2oGPT.zip", "db_dir_github h2oGPT", "ApacheV2 license"],
    ["db_dir_wiki.zip", "db_dir_wiki", "CC-BY-SA Wikipedia license"],
    # ['db_dir_wiki_full.zip', 'db_dir_wiki_full.zip', '23GB, 05/04/2023 CC-BY-SA Wiki license'],
]

all_db_zips = some_db_zips + [
    [
        "db_dir_wiki_full.zip",
        "db_dir_wiki_full.zip",
        "23GB, 05/04/2023 CC-BY-SA Wiki license",
    ],
]


def get_some_dbs_from_hf(dest=".", db_zips=None):
    if db_zips is None:
        db_zips = some_db_zips
    for db_dir, dir_expected, license1 in db_zips:
        path_to_zip_file = get_db_from_hf(dest=dest, db_dir=db_dir)
        assert os.path.isfile(path_to_zip_file), (
            "Missing zip in %s" % path_to_zip_file
        )
        if dir_expected:
            assert os.path.isdir(os.path.join(dest, dir_expected)), (
                "Missing path for %s" % dir_expected
            )
            assert os.path.isdir(os.path.join(dest, dir_expected, "index")), (
                "Missing index in %s" % dir_expected
            )


def _create_local_weaviate_client():
    WEAVIATE_URL = os.getenv("WEAVIATE_URL", "http://localhost:8080")
    WEAVIATE_USERNAME = os.getenv("WEAVIATE_USERNAME")
    WEAVIATE_PASSWORD = os.getenv("WEAVIATE_PASSWORD")
    WEAVIATE_SCOPE = os.getenv("WEAVIATE_SCOPE", "offline_access")

    resource_owner_config = None
    try:
        import weaviate

        if WEAVIATE_USERNAME is not None and WEAVIATE_PASSWORD is not None:
            resource_owner_config = weaviate.AuthClientPassword(
                username=WEAVIATE_USERNAME,
                password=WEAVIATE_PASSWORD,
                scope=WEAVIATE_SCOPE,
            )

        client = weaviate.Client(
            WEAVIATE_URL, auth_client_secret=resource_owner_config
        )
        return client
    except Exception as e:
        print(f"Failed to create Weaviate client: {e}")
        return None


if __name__ == "__main__":
    pass

```

`apps/language_models/langchain/gradio_utils/grclient.py`:

```py
import traceback
from typing import Callable
import os

from gradio_client.client import Job

os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"

from gradio_client import Client


class GradioClient(Client):
    """
    Parent class of gradio client
    To handle automatically refreshing client if detect gradio server changed
    """

    def __init__(self, *args, **kwargs):
        self.args = args
        self.kwargs = kwargs
        super().__init__(*args, **kwargs)
        self.server_hash = self.get_server_hash()

    def get_server_hash(self):
        """
        Get server hash using super without any refresh action triggered
        Returns: git hash of gradio server
        """
        return super().submit(api_name="/system_hash").result()

    def refresh_client_if_should(self):
        # get current hash in order to update api_name -> fn_index map in case gradio server changed
        # FIXME: Could add cli api as hash
        server_hash = self.get_server_hash()
        if self.server_hash != server_hash:
            self.refresh_client()
            self.server_hash = server_hash
        else:
            self.reset_session()

    def refresh_client(self):
        """
        Ensure every client call is independent
        Also ensure map between api_name and fn_index is updated in case server changed (e.g. restarted with new code)
        Returns:
        """
        # need session hash to be new every time, to avoid "generator already executing"
        self.reset_session()

        client = Client(*self.args, **self.kwargs)
        for k, v in client.__dict__.items():
            setattr(self, k, v)

    def submit(
        self,
        *args,
        api_name: str | None = None,
        fn_index: int | None = None,
        result_callbacks: Callable | list[Callable] | None = None,
    ) -> Job:
        # Note predict calls submit
        try:
            self.refresh_client_if_should()
            job = super().submit(*args, api_name=api_name, fn_index=fn_index)
        except Exception as e:
            print("Hit e=%s" % str(e), flush=True)
            # force reconfig in case only that
            self.refresh_client()
            job = super().submit(*args, api_name=api_name, fn_index=fn_index)

        # see if immediately failed
        e = job.future._exception
        if e is not None:
            print(
                "GR job failed: %s %s"
                % (str(e), "".join(traceback.format_tb(e.__traceback__))),
                flush=True,
            )
            # force reconfig in case only that
            self.refresh_client()
            job = super().submit(*args, api_name=api_name, fn_index=fn_index)
            e2 = job.future._exception
            if e2 is not None:
                print(
                    "GR job failed again: %s\n%s"
                    % (
                        str(e2),
                        "".join(traceback.format_tb(e2.__traceback__)),
                    ),
                    flush=True,
                )

        return job

```

`apps/language_models/langchain/h2oai_pipeline.py`:

```py
import os
from apps.stable_diffusion.src.utils.utils import _compile_module
from io import BytesIO
import torch_mlir

from stopping import get_stopping
from prompter import Prompter, PromptType

from transformers import TextGenerationPipeline
from transformers.pipelines.text_generation import ReturnType
from transformers.generation import (
    GenerationConfig,
    LogitsProcessorList,
    StoppingCriteriaList,
)
import copy
import torch
from transformers import AutoConfig, AutoModelForCausalLM
import gc
from pathlib import Path
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_public_file
from shark.shark_importer import import_with_fx, save_mlir
from apps.stable_diffusion.src import args

# Brevitas
from typing import List, Tuple
from brevitas_examples.common.generative.quantize import quantize_model
from brevitas_examples.llm.llm_quant.run_utils import get_model_impl


# fmt: off
def quant〇matmul_rhs_group_quant〡shape(lhs: List[int], rhs: List[int], rhs_scale: List[int], rhs_zero_point: List[int], rhs_bit_width: int, rhs_group_size: int) -> List[int]:
    if len(lhs) == 3 and len(rhs) == 2:
        return [lhs[0], lhs[1], rhs[0]]
    elif len(lhs) == 2 and len(rhs) == 2:
        return [lhs[0], rhs[0]]
    else:
        raise ValueError("Input shapes not supported.")


def quant〇matmul_rhs_group_quant〡dtype(lhs_rank_dtype: Tuple[int, int], rhs_rank_dtype: Tuple[int, int], rhs_scale_rank_dtype: Tuple[int, int], rhs_zero_point_rank_dtype: Tuple[int, int], rhs_bit_width: int, rhs_group_size: int) -> int:
    # output dtype is the dtype of the lhs float input
    lhs_rank, lhs_dtype = lhs_rank_dtype
    return lhs_dtype


def quant〇matmul_rhs_group_quant〡has_value_semantics(lhs, rhs, rhs_scale, rhs_zero_point, rhs_bit_width, rhs_group_size) -> None:
    return


brevitas_matmul_rhs_group_quant_library = [
    quant〇matmul_rhs_group_quant〡shape,
    quant〇matmul_rhs_group_quant〡dtype,
    quant〇matmul_rhs_group_quant〡has_value_semantics]
# fmt: on

global_device = "cuda"
global_precision = "fp16"

if not args.run_docuchat_web:
    args.device = global_device
    args.precision = global_precision
tensor_device = "cpu" if args.device == "cpu" else "cuda"


class H2OGPTModel(torch.nn.Module):
    def __init__(self, device, precision):
        super().__init__()
        torch_dtype = (
            torch.float32
            if precision == "fp32" or device == "cpu"
            else torch.float16
        )
        device_map = {"": "cpu"} if device == "cpu" else {"": 0}
        model_kwargs = {
            "local_files_only": False,
            "torch_dtype": torch_dtype,
            "resume_download": True,
            "use_auth_token": False,
            "trust_remote_code": True,
            "offload_folder": "offline_folder",
            "device_map": device_map,
        }
        config = AutoConfig.from_pretrained(
            "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
            use_auth_token=False,
            trust_remote_code=True,
            offload_folder="offline_folder",
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
            config=config,
            **model_kwargs,
        )
        if precision in ["int4", "int8"]:
            print("Applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                self.model.transformer.h,
                dtype=torch.float32,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=128,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(self, input_ids, attention_mask):
        input_dict = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "past_key_values": None,
            "use_cache": True,
        }
        output = self.model(
            **input_dict,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )
        return output.logits[:, -1, :]


class H2OGPTSHARKModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        model_name = "h2ogpt_falcon_7b"
        extended_model_name = (
            model_name + "_" + args.precision + "_" + args.device
        )
        vmfb_path = Path(extended_model_name + ".vmfb")
        mlir_path = Path(model_name + "_" + args.precision + ".mlir")
        shark_module = None

        need_to_compile = False
        if not vmfb_path.exists():
            need_to_compile = True
            # Downloading VMFB from shark_tank
            print("Trying to download pre-compiled vmfb from shark tank.")
            download_public_file(
                "gs://shark_tank/langchain/" + str(vmfb_path),
                vmfb_path.absolute(),
                single_file=True,
            )
            if vmfb_path.exists():
                print(
                    "Pre-compiled vmfb downloaded from shark tank successfully."
                )
                need_to_compile = False

        if need_to_compile:
            if not mlir_path.exists():
                print("Trying to download pre-generated mlir from shark tank.")
                # Downloading MLIR from shark_tank
                download_public_file(
                    "gs://shark_tank/langchain/" + str(mlir_path),
                    mlir_path.absolute(),
                    single_file=True,
                )
            if mlir_path.exists():
                with open(mlir_path, "rb") as f:
                    bytecode = f.read()
            else:
                # Generating the mlir
                bytecode = self.get_bytecode(tensor_device, args.precision)

            shark_module = SharkInference(
                mlir_module=bytecode,
                device=args.device,
                mlir_dialect="linalg",
            )
            print(f"[DEBUG] generating vmfb.")
            shark_module = _compile_module(
                shark_module, extended_model_name, []
            )
            print("Saved newly generated vmfb.")

        if shark_module is None:
            if vmfb_path.exists():
                print("Compiled vmfb found. Loading it from: ", vmfb_path)
                shark_module = SharkInference(
                    None, device=args.device, mlir_dialect="linalg"
                )
                shark_module.load_module(str(vmfb_path))
                print("Compiled vmfb loaded successfully.")
            else:
                raise ValueError("Unable to download/generate a vmfb.")

        self.model = shark_module

    def get_bytecode(self, device, precision):
        h2ogpt_model = H2OGPTModel(device, precision)

        compilation_input_ids = torch.randint(
            low=1, high=10000, size=(1, 400)
        ).to(device=device)
        compilation_attention_mask = torch.ones(1, 400, dtype=torch.int64).to(
            device=device
        )

        h2ogptCompileInput = (
            compilation_input_ids,
            compilation_attention_mask,
        )

        print(f"[DEBUG] generating torchscript graph")
        ts_graph = import_with_fx(
            h2ogpt_model,
            h2ogptCompileInput,
            is_f16=False,
            precision=precision,
            f16_input_mask=[False, False],
            mlir_type="torchscript",
        )
        del h2ogpt_model
        del self.src_model

        print(f"[DEBUG] generating torch mlir")
        if precision in ["int4", "int8"]:
            from torch_mlir.compiler_utils import (
                run_pipeline_with_repro_report,
            )

            module = torch_mlir.compile(
                ts_graph,
                [*h2ogptCompileInput],
                output_type=torch_mlir.OutputType.TORCH,
                backend_legal_ops=["quant.matmul_rhs_group_quant"],
                extra_library=brevitas_matmul_rhs_group_quant_library,
                use_tracing=False,
                verbose=False,
            )
            print(f"[DEBUG] converting torch to linalg")
            run_pipeline_with_repro_report(
                module,
                "builtin.module(func.func(torch-unpack-quant-tensor),func.func(torch-convert-custom-quant-op),torch-backend-to-linalg-on-tensors-backend-pipeline)",
                description="Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR",
            )
        else:
            module = torch_mlir.compile(
                ts_graph,
                [*h2ogptCompileInput],
                torch_mlir.OutputType.LINALG_ON_TENSORS,
                use_tracing=False,
                verbose=False,
            )
        del ts_graph

        print(f"[DEBUG] converting to bytecode")
        bytecode_stream = BytesIO()
        module.operation.write_bytecode(bytecode_stream)
        bytecode = bytecode_stream.getvalue()
        del module

        bytecode = save_mlir(
            bytecode,
            model_name=f"h2ogpt_{precision}",
            frontend="torch",
        )
        return bytecode

    def forward(self, input_ids, attention_mask):
        result = torch.from_numpy(
            self.model(
                "forward",
                (input_ids.to(device="cpu"), attention_mask.to(device="cpu")),
            )
        ).to(device=tensor_device)
        return result


def decode_tokens(tokenizer, res_tokens):
    for i in range(len(res_tokens)):
        if type(res_tokens[i]) != int:
            res_tokens[i] = int(res_tokens[i][0])

    res_str = tokenizer.decode(res_tokens, skip_special_tokens=True)
    return res_str


def generate_token(h2ogpt_shark_model, model, tokenizer, **generate_kwargs):
    del generate_kwargs["max_time"]
    generate_kwargs["input_ids"] = generate_kwargs["input_ids"].to(
        device=tensor_device
    )
    generate_kwargs["attention_mask"] = generate_kwargs["attention_mask"].to(
        device=tensor_device
    )
    truncated_input_ids = []
    stopping_criteria = generate_kwargs["stopping_criteria"]

    generation_config_ = GenerationConfig.from_model_config(model.config)
    generation_config = copy.deepcopy(generation_config_)
    model_kwargs = generation_config.update(**generate_kwargs)

    logits_processor = LogitsProcessorList()
    stopping_criteria = (
        stopping_criteria
        if stopping_criteria is not None
        else StoppingCriteriaList()
    )

    eos_token_id = generation_config.eos_token_id
    generation_config.pad_token_id = eos_token_id

    (
        inputs_tensor,
        model_input_name,
        model_kwargs,
    ) = model._prepare_model_inputs(
        None, generation_config.bos_token_id, model_kwargs
    )

    model_kwargs["output_attentions"] = generation_config.output_attentions
    model_kwargs[
        "output_hidden_states"
    ] = generation_config.output_hidden_states
    model_kwargs["use_cache"] = generation_config.use_cache

    input_ids = (
        inputs_tensor
        if model_input_name == "input_ids"
        else model_kwargs.pop("input_ids")
    )

    input_ids_seq_length = input_ids.shape[-1]

    generation_config.max_length = (
        generation_config.max_new_tokens + input_ids_seq_length
    )

    logits_processor = model._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=inputs_tensor,
        prefix_allowed_tokens_fn=None,
        logits_processor=logits_processor,
    )

    stopping_criteria = model._get_stopping_criteria(
        generation_config=generation_config,
        stopping_criteria=stopping_criteria,
    )

    logits_warper = model._get_logits_warper(generation_config)

    (
        input_ids,
        model_kwargs,
    ) = model._expand_inputs_for_generation(
        input_ids=input_ids,
        expand_size=generation_config.num_return_sequences,  # 1
        is_encoder_decoder=model.config.is_encoder_decoder,  # False
        **model_kwargs,
    )

    if isinstance(eos_token_id, int):
        eos_token_id = [eos_token_id]
    eos_token_id_tensor = (
        torch.tensor(eos_token_id).to(device=tensor_device)
        if eos_token_id is not None
        else None
    )

    pad_token_id = generation_config.pad_token_id
    eos_token_id = eos_token_id

    output_scores = generation_config.output_scores  # False
    return_dict_in_generate = (
        generation_config.return_dict_in_generate  # False
    )

    # init attention / hidden states / scores tuples
    scores = () if (return_dict_in_generate and output_scores) else None

    # keep track of which sequences are already finished
    unfinished_sequences = torch.ones(
        input_ids.shape[0],
        dtype=torch.long,
        device=input_ids.device,
    )

    timesRan = 0
    import time

    start = time.time()
    print("\n")

    res_tokens = []
    while True:
        model_inputs = model.prepare_inputs_for_generation(
            input_ids, **model_kwargs
        )

        outputs = h2ogpt_shark_model.forward(
            model_inputs["input_ids"], model_inputs["attention_mask"]
        )

        if args.precision == "fp16":
            outputs = outputs.to(dtype=torch.float32)
        next_token_logits = outputs

        # pre-process distribution
        next_token_scores = logits_processor(input_ids, next_token_logits)
        next_token_scores = logits_warper(input_ids, next_token_scores)

        # sample
        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)

        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)

        # finished sentences should have their next token be a padding token
        if eos_token_id is not None:
            if pad_token_id is None:
                raise ValueError(
                    "If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
                )
            next_token = next_token * unfinished_sequences + pad_token_id * (
                1 - unfinished_sequences
            )

        input_ids = torch.cat([input_ids, next_token[:, None]], dim=-1)

        model_kwargs["past_key_values"] = None
        if "attention_mask" in model_kwargs:
            attention_mask = model_kwargs["attention_mask"]
            model_kwargs["attention_mask"] = torch.cat(
                [
                    attention_mask,
                    attention_mask.new_ones((attention_mask.shape[0], 1)),
                ],
                dim=-1,
            )

        truncated_input_ids.append(input_ids[:, 0])
        input_ids = input_ids[:, 1:]
        model_kwargs["attention_mask"] = model_kwargs["attention_mask"][:, 1:]

        new_word = tokenizer.decode(
            next_token.cpu().numpy(),
            add_special_tokens=False,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        )

        res_tokens.append(next_token)
        if new_word == "<0x0A>":
            print("\n", end="", flush=True)
        else:
            print(f"{new_word}", end=" ", flush=True)

        part_str = decode_tokens(tokenizer, res_tokens)
        yield part_str

        # if eos_token was found in one sentence, set sentence to finished
        if eos_token_id_tensor is not None:
            unfinished_sequences = unfinished_sequences.mul(
                next_token.tile(eos_token_id_tensor.shape[0], 1)
                .ne(eos_token_id_tensor.unsqueeze(1))
                .prod(dim=0)
            )
            # stop when each sentence is finished
            if unfinished_sequences.max() == 0 or stopping_criteria(
                input_ids, scores
            ):
                break
        timesRan = timesRan + 1

    end = time.time()
    print(
        "\n\nTime taken is {:.2f} seconds/token\n".format(
            (end - start) / timesRan
        )
    )

    torch.cuda.empty_cache()
    gc.collect()

    res_str = decode_tokens(tokenizer, res_tokens)
    yield res_str


def pad_or_truncate_inputs(
    input_ids, attention_mask, max_padding_length=400, do_truncation=False
):
    inp_shape = input_ids.shape
    if inp_shape[1] < max_padding_length:
        # do padding
        num_add_token = max_padding_length - inp_shape[1]
        padded_input_ids = torch.cat(
            [
                torch.tensor([[11] * num_add_token]).to(device=tensor_device),
                input_ids,
            ],
            dim=1,
        )
        padded_attention_mask = torch.cat(
            [
                torch.tensor([[0] * num_add_token]).to(device=tensor_device),
                attention_mask,
            ],
            dim=1,
        )
        return padded_input_ids, padded_attention_mask
    elif inp_shape[1] > max_padding_length or do_truncation:
        # do truncation
        num_remove_token = inp_shape[1] - max_padding_length
        truncated_input_ids = input_ids[:, num_remove_token:]
        truncated_attention_mask = attention_mask[:, num_remove_token:]
        return truncated_input_ids, truncated_attention_mask
    else:
        return input_ids, attention_mask


class H2OTextGenerationPipeline(TextGenerationPipeline):
    def __init__(
        self,
        *args,
        debug=False,
        chat=False,
        stream_output=False,
        sanitize_bot_response=False,
        use_prompter=True,
        prompter=None,
        prompt_type=None,
        prompt_dict=None,
        max_input_tokens=2048 - 256,
        **kwargs,
    ):
        """
        HF-like pipeline, but handle instruction prompting and stopping (for some models)
        :param args:
        :param debug:
        :param chat:
        :param stream_output:
        :param sanitize_bot_response:
        :param use_prompter: Whether to use prompter.  If pass prompt_type, will make prompter
        :param prompter: prompter, can pass if have already
        :param prompt_type: prompt_type, e.g. human_bot.  See prompt_type to model mapping in from prompter.py.
                            If use_prompter, then will make prompter and use it.
        :param prompt_dict: dict of get_prompt(, return_dict=True) for prompt_type=custom
        :param max_input_tokens:
        :param kwargs:
        """
        super().__init__(*args, **kwargs)
        self.prompt_text = None
        self.use_prompter = use_prompter
        self.prompt_type = prompt_type
        self.prompt_dict = prompt_dict
        self.prompter = prompter
        if self.use_prompter:
            if self.prompter is not None:
                assert self.prompter.prompt_type is not None
            else:
                self.prompter = Prompter(
                    self.prompt_type,
                    self.prompt_dict,
                    debug=debug,
                    chat=chat,
                    stream_output=stream_output,
                )
            self.human = self.prompter.humanstr
            self.bot = self.prompter.botstr
            self.can_stop = True
        else:
            self.prompter = None
            self.human = None
            self.bot = None
            self.can_stop = False
        self.sanitize_bot_response = sanitize_bot_response
        self.max_input_tokens = (
            max_input_tokens  # not for generate, so ok that not kwargs
        )

    @staticmethod
    def limit_prompt(prompt_text, tokenizer, max_prompt_length=None):
        verbose = bool(int(os.getenv("VERBOSE_PIPELINE", "0")))

        if hasattr(tokenizer, "model_max_length"):
            # model_max_length only defined for generate.py, not raw use of h2oai_pipeline.py
            model_max_length = tokenizer.model_max_length
            if max_prompt_length is not None:
                model_max_length = min(model_max_length, max_prompt_length)
            # cut at some upper likely limit to avoid excessive tokenization etc
            # upper bound of 10 chars/token, e.g. special chars sometimes are long
            if len(prompt_text) > model_max_length * 10:
                len0 = len(prompt_text)
                prompt_text = prompt_text[-model_max_length * 10 :]
                if verbose:
                    print(
                        "Cut of input: %s -> %s" % (len0, len(prompt_text)),
                        flush=True,
                    )
        else:
            # unknown
            model_max_length = None

        num_prompt_tokens = None
        if model_max_length is not None:
            # can't wait for "hole" if not plain prompt_type, since would lose prefix like <human>:
            # For https://github.com/h2oai/h2ogpt/issues/192
            for trial in range(0, 3):
                prompt_tokens = tokenizer(prompt_text)["input_ids"]
                num_prompt_tokens = len(prompt_tokens)
                if num_prompt_tokens > model_max_length:
                    # conservative by using int()
                    chars_per_token = int(len(prompt_text) / num_prompt_tokens)
                    # keep tail, where question is if using langchain
                    prompt_text = prompt_text[
                        -model_max_length * chars_per_token :
                    ]
                    if verbose:
                        print(
                            "reducing %s tokens, assuming average of %s chars/token for %s characters"
                            % (
                                num_prompt_tokens,
                                chars_per_token,
                                len(prompt_text),
                            ),
                            flush=True,
                        )
                else:
                    if verbose:
                        print(
                            "using %s tokens with %s chars"
                            % (num_prompt_tokens, len(prompt_text)),
                            flush=True,
                        )
                    break

        return prompt_text, num_prompt_tokens

    def preprocess(
        self,
        prompt_text,
        prefix="",
        handle_long_generation=None,
        **generate_kwargs,
    ):
        (
            prompt_text,
            num_prompt_tokens,
        ) = H2OTextGenerationPipeline.limit_prompt(prompt_text, self.tokenizer)

        data_point = dict(context="", instruction=prompt_text, input="")
        if self.prompter is not None:
            prompt_text = self.prompter.generate_prompt(data_point)
        self.prompt_text = prompt_text
        if handle_long_generation is None:
            # forces truncation of inputs to avoid critical failure
            handle_long_generation = None  # disable with new approaches
        return super().preprocess(
            prompt_text,
            prefix=prefix,
            handle_long_generation=handle_long_generation,
            **generate_kwargs,
        )

    def postprocess(
        self,
        model_outputs,
        return_type=ReturnType.FULL_TEXT,
        clean_up_tokenization_spaces=True,
    ):
        records = super().postprocess(
            model_outputs,
            return_type=return_type,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
        )
        for rec in records:
            if self.use_prompter:
                outputs = rec["generated_text"]
                outputs = self.prompter.get_response(
                    outputs,
                    prompt=self.prompt_text,
                    sanitize_bot_response=self.sanitize_bot_response,
                )
            elif self.bot and self.human:
                outputs = (
                    rec["generated_text"]
                    .split(self.bot)[1]
                    .split(self.human)[0]
                )
            else:
                outputs = rec["generated_text"]
            rec["generated_text"] = outputs
            print(
                "prompt: %s\noutputs: %s\n\n" % (self.prompt_text, outputs),
                flush=True,
            )
        return records

    def _forward(self, model_inputs, **generate_kwargs):
        if self.can_stop:
            stopping_criteria = get_stopping(
                self.prompt_type,
                self.prompt_dict,
                self.tokenizer,
                self.device,
                human=self.human,
                bot=self.bot,
                model_max_length=self.tokenizer.model_max_length,
            )
            generate_kwargs["stopping_criteria"] = stopping_criteria
        # return super()._forward(model_inputs, **generate_kwargs)
        return self.__forward(model_inputs, **generate_kwargs)

    # FIXME: Copy-paste of original _forward, but removed copy.deepcopy()
    # FIXME: https://github.com/h2oai/h2ogpt/issues/172
    def __forward(self, model_inputs, **generate_kwargs):
        input_ids = model_inputs["input_ids"]
        attention_mask = model_inputs.get("attention_mask", None)
        # Allow empty prompts
        if input_ids.shape[1] == 0:
            input_ids = None
            attention_mask = None
            in_b = 1
        else:
            in_b = input_ids.shape[0]
        prompt_text = model_inputs.pop("prompt_text")

        ## If there is a prefix, we may need to adjust the generation length. Do so without permanently modifying
        ## generate_kwargs, as some of the parameterization may come from the initialization of the pipeline.
        # generate_kwargs = copy.deepcopy(generate_kwargs)
        prefix_length = generate_kwargs.pop("prefix_length", 0)
        if prefix_length > 0:
            has_max_new_tokens = "max_new_tokens" in generate_kwargs or (
                "generation_config" in generate_kwargs
                and generate_kwargs["generation_config"].max_new_tokens
                is not None
            )
            if not has_max_new_tokens:
                generate_kwargs["max_length"] = (
                    generate_kwargs.get("max_length")
                    or self.model.config.max_length
                )
                generate_kwargs["max_length"] += prefix_length
            has_min_new_tokens = "min_new_tokens" in generate_kwargs or (
                "generation_config" in generate_kwargs
                and generate_kwargs["generation_config"].min_new_tokens
                is not None
            )
            if not has_min_new_tokens and "min_length" in generate_kwargs:
                generate_kwargs["min_length"] += prefix_length

        # BS x SL
        # pad or truncate the input_ids and attention_mask
        max_padding_length = 400
        input_ids, attention_mask = pad_or_truncate_inputs(
            input_ids, attention_mask, max_padding_length=max_padding_length
        )

        return_dict = {
            "model": self.model,
            "tokenizer": self.tokenizer,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "attention_mask": attention_mask,
        }
        return_dict = {**return_dict, **generate_kwargs}
        return return_dict

```

`apps/language_models/langchain/image_captions.py`:

```py
"""
Based upon ImageCaptionLoader in LangChain version: langchain/document_loaders/image_captions.py
But accepts preloaded model to avoid slowness in use and CUDA forking issues

Loader that loads image captions
By default, the loader utilizes the pre-trained BLIP image captioning model.
https://huggingface.co/Salesforce/blip-image-captioning-base

"""
from typing import List, Union, Any, Tuple

import requests
from langchain.docstore.document import Document
from langchain.document_loaders import ImageCaptionLoader

from utils import get_device, NullContext

import pkg_resources

try:
    assert pkg_resources.get_distribution("bitsandbytes") is not None
    have_bitsandbytes = True
except (pkg_resources.DistributionNotFound, AssertionError):
    have_bitsandbytes = False


class H2OImageCaptionLoader(ImageCaptionLoader):
    """Loader that loads the captions of an image"""

    def __init__(
        self,
        path_images: Union[str, List[str]] = None,
        blip_processor: str = None,
        blip_model: str = None,
        caption_gpu=True,
        load_in_8bit=True,
        # True doesn't seem to work, even though https://huggingface.co/Salesforce/blip2-flan-t5-xxl#in-8-bit-precision-int8
        load_half=False,
        load_gptq="",
        use_safetensors=False,
        min_new_tokens=20,
        max_tokens=50,
    ):
        if blip_model is None or blip_model is None:
            blip_processor = "Salesforce/blip-image-captioning-base"
            blip_model = "Salesforce/blip-image-captioning-base"

        super().__init__(path_images, blip_processor, blip_model)
        self.blip_processor = blip_processor
        self.blip_model = blip_model
        self.processor = None
        self.model = None
        self.caption_gpu = caption_gpu
        self.context_class = NullContext
        self.device = "cpu"
        self.load_in_8bit = (
            load_in_8bit and have_bitsandbytes
        )  # only for blip2
        self.load_half = load_half
        self.load_gptq = load_gptq
        self.use_safetensors = use_safetensors
        self.gpu_id = "auto"
        # default prompt
        self.prompt = "image of"
        self.min_new_tokens = min_new_tokens
        self.max_tokens = max_tokens

    def set_context(self):
        if get_device() == "cuda" and self.caption_gpu:
            import torch

            n_gpus = (
                torch.cuda.device_count() if torch.cuda.is_available else 0
            )
            if n_gpus > 0:
                self.context_class = torch.device
                self.device = "cuda"

    def load_model(self):
        try:
            import transformers
        except ImportError:
            raise ValueError(
                "`transformers` package not found, please install with "
                "`pip install transformers`."
            )
        self.set_context()
        if self.caption_gpu:
            if self.gpu_id == "auto":
                # blip2 has issues with multi-GPU.  Error says need to somehow set language model in device map
                # device_map = 'auto'
                device_map = {"": 0}
            else:
                if self.device == "cuda":
                    device_map = {"": self.gpu_id}
                else:
                    device_map = {"": "cpu"}
        else:
            device_map = {"": "cpu"}
        import torch

        with torch.no_grad():
            with self.context_class(self.device):
                context_class_cast = (
                    NullContext if self.device == "cpu" else torch.autocast
                )
                with context_class_cast(self.device):
                    if "blip2" in self.blip_processor.lower():
                        from transformers import (
                            Blip2Processor,
                            Blip2ForConditionalGeneration,
                        )

                        if self.load_half and not self.load_in_8bit:
                            self.processor = Blip2Processor.from_pretrained(
                                self.blip_processor, device_map=device_map
                            ).half()
                            self.model = (
                                Blip2ForConditionalGeneration.from_pretrained(
                                    self.blip_model, device_map=device_map
                                ).half()
                            )
                        else:
                            self.processor = Blip2Processor.from_pretrained(
                                self.blip_processor,
                                load_in_8bit=self.load_in_8bit,
                                device_map=device_map,
                            )
                            self.model = (
                                Blip2ForConditionalGeneration.from_pretrained(
                                    self.blip_model,
                                    load_in_8bit=self.load_in_8bit,
                                    device_map=device_map,
                                )
                            )
                    else:
                        from transformers import (
                            BlipForConditionalGeneration,
                            BlipProcessor,
                        )

                        self.load_half = False  # not supported
                        if self.caption_gpu:
                            if device_map == "auto":
                                # Blip doesn't support device_map='auto'
                                if self.device == "cuda":
                                    if self.gpu_id == "auto":
                                        device_map = {"": 0}
                                    else:
                                        device_map = {"": self.gpu_id}
                                else:
                                    device_map = {"": "cpu"}
                        else:
                            device_map = {"": "cpu"}
                        self.processor = BlipProcessor.from_pretrained(
                            self.blip_processor, device_map=device_map
                        )
                        self.model = (
                            BlipForConditionalGeneration.from_pretrained(
                                self.blip_model, device_map=device_map
                            )
                        )
        return self

    def set_image_paths(self, path_images: Union[str, List[str]]):
        """
        Load from a list of image files
        """
        if isinstance(path_images, str):
            self.image_paths = [path_images]
        else:
            self.image_paths = path_images

    def load(self, prompt=None) -> List[Document]:
        if self.processor is None or self.model is None:
            self.load_model()
        results = []
        for path_image in self.image_paths:
            caption, metadata = self._get_captions_and_metadata(
                model=self.model,
                processor=self.processor,
                path_image=path_image,
                prompt=prompt,
            )
            doc = Document(page_content=caption, metadata=metadata)
            results.append(doc)

        return results

    def _get_captions_and_metadata(
        self, model: Any, processor: Any, path_image: str, prompt=None
    ) -> Tuple[str, dict]:
        """
        Helper function for getting the captions and metadata of an image
        """
        if prompt is None:
            prompt = self.prompt
        try:
            from PIL import Image
        except ImportError:
            raise ValueError(
                "`PIL` package not found, please install with `pip install pillow`"
            )

        try:
            if path_image.startswith("http://") or path_image.startswith(
                "https://"
            ):
                image = Image.open(
                    requests.get(path_image, stream=True).raw
                ).convert("RGB")
            else:
                image = Image.open(path_image).convert("RGB")
        except Exception:
            raise ValueError(f"Could not get image data for {path_image}")

        import torch

        with torch.no_grad():
            with self.context_class(self.device):
                context_class_cast = (
                    NullContext if self.device == "cpu" else torch.autocast
                )
                with context_class_cast(self.device):
                    if self.load_half:
                        inputs = processor(
                            image, prompt, return_tensors="pt"
                        ).half()
                    else:
                        inputs = processor(image, prompt, return_tensors="pt")
                    min_length = len(prompt) // 4 + self.min_new_tokens
                    self.max_tokens = max(self.max_tokens, min_length)
                    output = model.generate(
                        **inputs,
                        min_length=min_length,
                        max_length=self.max_tokens,
                    )

                    caption: str = processor.decode(
                        output[0], skip_special_tokens=True
                    )
                    prompti = caption.find(prompt)
                    if prompti >= 0:
                        caption = caption[prompti + len(prompt) :]
                    metadata: dict = {"image_path": path_image}

        return caption, metadata

```

`apps/language_models/langchain/langchain_requirements.txt`:

```txt
# for generate (gradio server) and finetune
datasets==2.13.0
sentencepiece==0.1.99
huggingface_hub==0.16.4
appdirs==1.4.4
fire==0.5.0
docutils==0.20.1
evaluate==0.4.0
rouge_score==0.1.2
sacrebleu==2.3.1
scikit-learn==1.2.2
alt-profanity-check==1.2.2
better-profanity==0.7.0
numpy==1.24.3
pandas==2.0.2
matplotlib==3.7.1
loralib==0.1.1
bitsandbytes==0.39.0
accelerate==0.20.3
peft==0.4.0
# 4.31.0+ breaks load_in_8bit=True (https://github.com/huggingface/transformers/issues/25026)
transformers==4.30.2
tokenizers==0.13.3
APScheduler==3.10.1

# optional for generate
pynvml==11.5.0
psutil==5.9.5
boto3==1.26.101
botocore==1.29.101

# optional for finetune
tensorboard==2.13.0
neptune==1.2.0

# for gradio client
gradio_client==0.2.10
beautifulsoup4==4.12.2
markdown==3.4.3

# data and testing
pytest==7.2.2
pytest-xdist==3.2.1
nltk==3.8.1
textstat==0.7.3
# pandoc==2.3
pypandoc==1.11; sys_platform == "darwin" and platform_machine == "arm64"
pypandoc_binary==1.11; platform_machine == "x86_64"
pypandoc_binary==1.11; sys_platform == "win32"
openpyxl==3.1.2
lm_dataformat==0.0.20
bioc==2.0

# falcon
einops==0.6.1
instructorembedding==1.0.1

# for gpt4all .env file, but avoid worrying about imports
python-dotenv==1.0.0

text-generation==0.6.0
# for tokenization when don't have HF tokenizer
tiktoken==0.4.0
# optional: for OpenAI endpoint or embeddings (requires key)
openai==0.27.8

# optional for chat with PDF
langchain==0.0.329
pypdf==3.17.0
# avoid textract, requires old six
#textract==1.6.5

# for HF embeddings
sentence_transformers==2.2.2

# local vector db
chromadb==0.3.25
# server vector db
#pymilvus==2.2.8

# weak url support, if can't install opencv etc. If comment-in this one, then comment-out unstructured[local-inference]==0.6.6
# unstructured==0.8.1

# strong support for images
# Requires on Ubuntu: sudo apt-get install libmagic-dev poppler-utils tesseract-ocr libtesseract-dev libreoffice
unstructured[local-inference]==0.7.4
#pdf2image==1.16.3
#pytesseract==0.3.10
pillow

pdfminer.six==20221105
urllib3
requests_file

#pdf2image==1.16.3
#pytesseract==0.3.10
tabulate==0.9.0
# FYI pandoc already part of requirements.txt

# JSONLoader, but makes some trouble for some users
# jq==1.4.1

# to check licenses
# Run: pip-licenses|grep -v 'BSD\|Apache\|MIT'
pip-licenses==4.3.0

# weaviate vector db
weaviate-client==3.22.1

gpt4all==1.0.5
llama-cpp-python==0.1.73

arxiv==1.4.8
pymupdf==1.22.5 # AGPL license
# extract-msg==0.41.1  # GPL3

# sometimes unstructured fails, these work in those cases.  See https://github.com/h2oai/h2ogpt/issues/320
playwright==1.36.0
# requires Chrome binary to be in path
selenium==4.10.0

```

`apps/language_models/langchain/llama_flash_attn_monkey_patch.py`:

```py
from typing import List, Optional, Tuple

import torch

import transformers
from transformers.models.llama.modeling_llama import apply_rotary_pos_emb

from einops import rearrange

from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
from flash_attn.bert_padding import unpad_input, pad_input


def forward(
    self,
    hidden_states: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.Tensor] = None,
    past_key_value: Optional[Tuple[torch.Tensor]] = None,
    output_attentions: bool = False,
    use_cache: bool = False,
) -> Tuple[
    torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]
]:
    """Input shape: Batch x Time x Channel
    attention_mask: [bsz, q_len]
    """
    bsz, q_len, _ = hidden_states.size()

    query_states = (
        self.q_proj(hidden_states)
        .view(bsz, q_len, self.num_heads, self.head_dim)
        .transpose(1, 2)
    )
    key_states = (
        self.k_proj(hidden_states)
        .view(bsz, q_len, self.num_heads, self.head_dim)
        .transpose(1, 2)
    )
    value_states = (
        self.v_proj(hidden_states)
        .view(bsz, q_len, self.num_heads, self.head_dim)
        .transpose(1, 2)
    )
    # [bsz, q_len, nh, hd]
    # [bsz, nh, q_len, hd]

    kv_seq_len = key_states.shape[-2]
    assert past_key_value is None, "past_key_value is not supported"

    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
    query_states, key_states = apply_rotary_pos_emb(
        query_states, key_states, cos, sin, position_ids
    )
    # [bsz, nh, t, hd]
    assert not output_attentions, "output_attentions is not supported"
    assert not use_cache, "use_cache is not supported"

    # Flash attention codes from
    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py

    # transform the data into the format required by flash attention
    qkv = torch.stack(
        [query_states, key_states, value_states], dim=2
    )  # [bsz, nh, 3, q_len, hd]
    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]
    # We have disabled _prepare_decoder_attention_mask in LlamaModel
    # the attention_mask should be the same as the key_padding_mask
    key_padding_mask = attention_mask

    if key_padding_mask is None:
        qkv = rearrange(qkv, "b s ... -> (b s) ...")
        max_s = q_len
        cu_q_lens = torch.arange(
            0,
            (bsz + 1) * q_len,
            step=q_len,
            dtype=torch.int32,
            device=qkv.device,
        )
        output = flash_attn_unpadded_qkvpacked_func(
            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True
        )
        output = rearrange(output, "(b s) ... -> b s ...", b=bsz)
    else:
        nheads = qkv.shape[-2]
        x = rearrange(qkv, "b s three h d -> b s (three h d)")
        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)
        x_unpad = rearrange(
            x_unpad, "nnz (three h d) -> nnz three h d", three=3, h=nheads
        )
        output_unpad = flash_attn_unpadded_qkvpacked_func(
            x_unpad, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True
        )
        output = rearrange(
            pad_input(
                rearrange(output_unpad, "nnz h d -> nnz (h d)"),
                indices,
                bsz,
                q_len,
            ),
            "b s (h d) -> b s h d",
            h=nheads,
        )
    return self.o_proj(rearrange(output, "b s h d -> b s (h d)")), None, None


# Disable the transformation of the attention mask in LlamaModel as the flash attention
# requires the attention mask to be the same as the key_padding_mask
def _prepare_decoder_attention_mask(
    self, attention_mask, input_shape, inputs_embeds, past_key_values_length
):
    # [bsz, seq_len]
    return attention_mask


def replace_llama_attn_with_flash_attn():
    print(
        "Replacing original LLaMa attention with flash attention", flush=True
    )
    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = (
        _prepare_decoder_attention_mask
    )
    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward

```

`apps/language_models/langchain/loaders.py`:

```py
import functools


def get_loaders(model_name, reward_type, llama_type=None, load_gptq=""):
    # NOTE: Some models need specific new prompt_type
    # E.g. t5_xxl_true_nli_mixture has input format: "premise: PREMISE_TEXT hypothesis: HYPOTHESIS_TEXT".)
    if load_gptq:
        from transformers import AutoTokenizer
        from auto_gptq import AutoGPTQForCausalLM

        use_triton = False
        functools.partial(
            AutoGPTQForCausalLM.from_quantized,
            quantize_config=None,
            use_triton=use_triton,
        )
        return AutoGPTQForCausalLM.from_quantized, AutoTokenizer
    if llama_type is None:
        llama_type = "llama" in model_name.lower()
    if llama_type:
        from transformers import LlamaForCausalLM, LlamaTokenizer

        return LlamaForCausalLM.from_pretrained, LlamaTokenizer
    elif "distilgpt2" in model_name.lower():
        from transformers import AutoModelForCausalLM, AutoTokenizer

        return AutoModelForCausalLM.from_pretrained, AutoTokenizer
    elif "gpt2" in model_name.lower():
        from transformers import GPT2LMHeadModel, GPT2Tokenizer

        return GPT2LMHeadModel.from_pretrained, GPT2Tokenizer
    elif "mbart-" in model_name.lower():
        from transformers import (
            MBartForConditionalGeneration,
            MBart50TokenizerFast,
        )

        return (
            MBartForConditionalGeneration.from_pretrained,
            MBart50TokenizerFast,
        )
    elif (
        "t5" == model_name.lower()
        or "t5-" in model_name.lower()
        or "flan-" in model_name.lower()
    ):
        from transformers import AutoTokenizer, T5ForConditionalGeneration

        return T5ForConditionalGeneration.from_pretrained, AutoTokenizer
    elif "bigbird" in model_name:
        from transformers import (
            BigBirdPegasusForConditionalGeneration,
            AutoTokenizer,
        )

        return (
            BigBirdPegasusForConditionalGeneration.from_pretrained,
            AutoTokenizer,
        )
    elif (
        "bart-large-cnn-samsum" in model_name
        or "flan-t5-base-samsum" in model_name
    ):
        from transformers import pipeline

        return pipeline, "summarization"
    elif (
        reward_type
        or "OpenAssistant/reward-model".lower() in model_name.lower()
    ):
        from transformers import (
            AutoModelForSequenceClassification,
            AutoTokenizer,
        )

        return (
            AutoModelForSequenceClassification.from_pretrained,
            AutoTokenizer,
        )
    else:
        from transformers import AutoTokenizer, AutoModelForCausalLM

        model_loader = AutoModelForCausalLM
        tokenizer_loader = AutoTokenizer
        return model_loader.from_pretrained, tokenizer_loader


def get_tokenizer(
    tokenizer_loader,
    tokenizer_base_model,
    local_files_only,
    resume_download,
    use_auth_token,
):
    tokenizer = tokenizer_loader.from_pretrained(
        tokenizer_base_model,
        local_files_only=local_files_only,
        resume_download=resume_download,
        use_auth_token=use_auth_token,
        padding_side="left",
    )

    tokenizer.pad_token_id = 0  # different from the eos token
    # when generating, we will use the logits of right-most token to predict the next token
    # so the padding should be on the left,
    # e.g. see: https://huggingface.co/transformers/v4.11.3/model_doc/t5.html#inference
    tokenizer.padding_side = "left"  # Allow batched inference

    return tokenizer

```

`apps/language_models/langchain/make_db.py`:

```py
import os

from gpt_langchain import (
    path_to_docs,
    get_some_dbs_from_hf,
    all_db_zips,
    some_db_zips,
    create_or_update_db,
)
from utils import get_ngpus_vis


def glob_to_db(
    user_path,
    chunk=True,
    chunk_size=512,
    verbose=False,
    fail_any_exception=False,
    n_jobs=-1,
    url=None,
    enable_captions=True,
    captions_model=None,
    caption_loader=None,
    enable_ocr=False,
):
    sources1 = path_to_docs(
        user_path,
        verbose=verbose,
        fail_any_exception=fail_any_exception,
        n_jobs=n_jobs,
        chunk=chunk,
        chunk_size=chunk_size,
        url=url,
        enable_captions=enable_captions,
        captions_model=captions_model,
        caption_loader=caption_loader,
        enable_ocr=enable_ocr,
    )
    return sources1


def make_db_main(
    use_openai_embedding: bool = False,
    hf_embedding_model: str = None,
    persist_directory: str = "db_dir_UserData",
    user_path: str = "user_path",
    url: str = None,
    add_if_exists: bool = True,
    collection_name: str = "UserData",
    verbose: bool = False,
    chunk: bool = True,
    chunk_size: int = 512,
    fail_any_exception: bool = False,
    download_all: bool = False,
    download_some: bool = False,
    download_one: str = None,
    download_dest: str = "./",
    n_jobs: int = -1,
    enable_captions: bool = True,
    captions_model: str = "Salesforce/blip-image-captioning-base",
    pre_load_caption_model: bool = False,
    caption_gpu: bool = True,
    enable_ocr: bool = False,
    db_type: str = "chroma",
):
    """
    # To make UserData db for generate.py, put pdfs, etc. into path user_path and run:
    python make_db.py

    # once db is made, can use in generate.py like:

    python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --langchain_mode=UserData

    or zip-up the db_dir_UserData and share:

    zip -r db_dir_UserData.zip db_dir_UserData

    # To get all db files (except large wiki_full) do:
    python make_db.py --download_some=True

    # To get a single db file from HF:
    python make_db.py --download_one=db_dir_DriverlessAI_docs.zip

    :param use_openai_embedding: Whether to use OpenAI embedding
    :param hf_embedding_model: HF embedding model to use. Like generate.py, uses 'hkunlp/instructor-large' if have GPUs, else "sentence-transformers/all-MiniLM-L6-v2"
    :param persist_directory: where to persist db
    :param user_path: where to pull documents from (None means url is not None.  If url is not None, this is ignored.)
    :param url: url to generate documents from (None means user_path is not None)
    :param add_if_exists: Add to db if already exists, but will not add duplicate sources
    :param collection_name: Collection name for new db if not adding
    :param verbose: whether to show verbose messages
    :param chunk: whether to chunk data
    :param chunk_size: chunk size for chunking
    :param fail_any_exception: whether to fail if any exception hit during ingestion of files
    :param download_all: whether to download all (including 23GB Wikipedia) example databases from h2o.ai HF
    :param download_some: whether to download some small example databases from h2o.ai HF
    :param download_one: whether to download one chosen example databases from h2o.ai HF
    :param download_dest: Destination for downloads
    :param n_jobs: Number of cores to use for ingesting multiple files
    :param enable_captions: Whether to enable captions on images
    :param captions_model: See generate.py
    :param pre_load_caption_model: See generate.py
    :param caption_gpu: Caption images on GPU if present
    :param enable_ocr: Whether to enable OCR on images
    :param db_type: Type of db to create. Currently only 'chroma' and 'weaviate' is supported.
    :return: None
    """
    db = None

    # match behavior of main() in generate.py for non-HF case
    n_gpus = get_ngpus_vis()
    if n_gpus == 0:
        if hf_embedding_model is None:
            # if no GPUs, use simpler embedding model to avoid cost in time
            hf_embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
    else:
        if hf_embedding_model is None:
            # if still None, then set default
            hf_embedding_model = "hkunlp/instructor-large"

    if download_all:
        print("Downloading all (and unzipping): %s" % all_db_zips, flush=True)
        get_some_dbs_from_hf(download_dest, db_zips=all_db_zips)
        if verbose:
            print("DONE", flush=True)
        return db, collection_name
    elif download_some:
        print(
            "Downloading some (and unzipping): %s" % some_db_zips, flush=True
        )
        get_some_dbs_from_hf(download_dest, db_zips=some_db_zips)
        if verbose:
            print("DONE", flush=True)
        return db, collection_name
    elif download_one:
        print("Downloading %s (and unzipping)" % download_one, flush=True)
        get_some_dbs_from_hf(
            download_dest, db_zips=[[download_one, "", "Unknown License"]]
        )
        if verbose:
            print("DONE", flush=True)
        return db, collection_name

    if enable_captions and pre_load_caption_model:
        # preload, else can be too slow or if on GPU have cuda context issues
        # Inside ingestion, this will disable parallel loading of multiple other kinds of docs
        # However, if have many images, all those images will be handled more quickly by preloaded model on GPU
        from image_captions import H2OImageCaptionLoader

        caption_loader = H2OImageCaptionLoader(
            None,
            blip_model=captions_model,
            blip_processor=captions_model,
            caption_gpu=caption_gpu,
        ).load_model()
    else:
        if enable_captions:
            caption_loader = "gpu" if caption_gpu else "cpu"
        else:
            caption_loader = False

    if verbose:
        print("Getting sources", flush=True)
    assert (
        user_path is not None or url is not None
    ), "Can't have both user_path and url as None"
    if not url:
        assert os.path.isdir(user_path), (
            "user_path=%s does not exist" % user_path
        )
    sources = glob_to_db(
        user_path,
        chunk=chunk,
        chunk_size=chunk_size,
        verbose=verbose,
        fail_any_exception=fail_any_exception,
        n_jobs=n_jobs,
        url=url,
        enable_captions=enable_captions,
        captions_model=captions_model,
        caption_loader=caption_loader,
        enable_ocr=enable_ocr,
    )
    exceptions = [x for x in sources if x.metadata.get("exception")]
    print("Exceptions: %s" % exceptions, flush=True)
    sources = [x for x in sources if "exception" not in x.metadata]

    assert len(sources) > 0, "No sources found"
    db = create_or_update_db(
        db_type,
        persist_directory,
        collection_name,
        sources,
        use_openai_embedding,
        add_if_exists,
        verbose,
        hf_embedding_model,
    )

    assert db is not None
    if verbose:
        print("DONE", flush=True)
    return db, collection_name

```

`apps/language_models/langchain/prompter.py`:

```py
import os
import ast
import time
from apps.language_models.langchain.enums import (
    PromptType,
)  # also supports imports from this file from other files

non_hf_types = ["gpt4all_llama", "llama", "gptj"]

prompt_type_to_model_name = {
    "plain": [
        "EleutherAI/gpt-j-6B",
        "EleutherAI/pythia-6.9b",
        "EleutherAI/pythia-12b",
        "EleutherAI/pythia-12b-deduped",
        "EleutherAI/gpt-neox-20b",
        "openlm-research/open_llama_7b_700bt_preview",
        "decapoda-research/llama-7b-hf",
        "decapoda-research/llama-13b-hf",
        "decapoda-research/llama-30b-hf",
        "decapoda-research/llama-65b-hf",
        "facebook/mbart-large-50-many-to-many-mmt",
        "philschmid/bart-large-cnn-samsum",
        "philschmid/flan-t5-base-samsum",
        "gpt2",
        "distilgpt2",
        "mosaicml/mpt-7b-storywriter",
    ],
    "gptj": ["gptj", "gpt4all_llama"],
    "prompt_answer": [
        "h2oai/h2ogpt-gm-oasst1-en-1024-20b",
        "h2oai/h2ogpt-gm-oasst1-en-1024-12b",
        "h2oai/h2ogpt-gm-oasst1-multilang-1024-20b",
        "h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b",
        "h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b-v2",
        "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
        "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b",
        "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2",
        "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1",
        "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2",
        "h2oai/h2ogpt-gm-oasst1-en-xgen-7b-8k",
        "h2oai/h2ogpt-gm-oasst1-multilang-xgen-7b-8k",
        "TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GPTQ",
    ],
    "prompt_answer_openllama": [
        "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt",
        "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2",
        "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-700bt",
        "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b",
        "h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b",
    ],
    "instruct": [
        "TheBloke/llama-30b-supercot-SuperHOT-8K-fp16"
    ],  # https://huggingface.co/TheBloke/llama-30b-supercot-SuperHOT-8K-fp16#prompting
    "instruct_with_end": ["databricks/dolly-v2-12b"],
    "quality": [],
    "human_bot": [
        "h2oai/h2ogpt-oasst1-512-12b",
        "h2oai/h2ogpt-oasst1-512-20b",
        "h2oai/h2ogpt-oig-oasst1-256-6_9b",
        "h2oai/h2ogpt-oig-oasst1-512-6_9b",
        "h2oai/h2ogpt-oig-oasst1-256-6.9b",  # legacy
        "h2oai/h2ogpt-oig-oasst1-512-6.9b",  # legacy
        "h2oai/h2ogpt-research-oasst1-512-30b",
        "h2oai/h2ogpt-research-oasst1-llama-65b",
        "h2oai/h2ogpt-oasst1-falcon-40b",
        "h2oai/h2ogpt-oig-oasst1-falcon-40b",
    ],
    "dai_faq": [],
    "summarize": [],
    "simple_instruct": [
        "t5-small",
        "t5-large",
        "google/flan-t5",
        "google/flan-t5-xxl",
        "google/flan-ul2",
    ],
    "instruct_vicuna": [
        "AlekseyKorshuk/vicuna-7b",
        "TheBloke/stable-vicuna-13B-HF",
        "junelee/wizard-vicuna-13b",
    ],
    "human_bot_orig": ["togethercomputer/GPT-NeoXT-Chat-Base-20B"],
    "open_assistant": [
        "OpenAssistant/oasst-sft-7-llama-30b-xor",
        "oasst-sft-7-llama-30b",
    ],
    "wizard_lm": [
        "ehartford/WizardLM-7B-Uncensored",
        "ehartford/WizardLM-13B-Uncensored",
    ],
    "wizard_mega": ["openaccess-ai-collective/wizard-mega-13b"],
    "instruct_simple": ["JosephusCheung/Guanaco"],
    "wizard_vicuna": ["ehartford/Wizard-Vicuna-13B-Uncensored"],
    "wizard2": ["llama"],
    "mptinstruct": [
        "mosaicml/mpt-30b-instruct",
        "mosaicml/mpt-7b-instruct",
        "mosaicml/mpt-30b-instruct",
    ],
    "mptchat": [
        "mosaicml/mpt-7b-chat",
        "mosaicml/mpt-30b-chat",
        "TheBloke/mpt-30B-chat-GGML",
    ],
    "vicuna11": ["lmsys/vicuna-33b-v1.3"],
    "falcon": [
        "tiiuae/falcon-40b-instruct",
        "tiiuae/falcon-40b",
        "tiiuae/falcon-7b-instruct",
        "tiiuae/falcon-7b",
    ],
    # could be plain, but default is correct prompt_type for default TheBloke model ggml-wizardLM-7B.q4_2.bin
}
if os.getenv("OPENAI_API_KEY"):
    prompt_type_to_model_name.update(
        {
            "openai": [
                "text-davinci-003",
                "text-curie-001",
                "text-babbage-001",
                "text-ada-001",
            ],
            "openai_chat": ["gpt-3.5-turbo", "gpt-3.5-turbo-16k"],
        }
    )

inv_prompt_type_to_model_name = {
    v.strip(): k for k, l in prompt_type_to_model_name.items() for v in l
}
inv_prompt_type_to_model_lower = {
    v.strip().lower(): k
    for k, l in prompt_type_to_model_name.items()
    for v in l
}

prompt_types_strings = []
for p in PromptType:
    prompt_types_strings.extend([p.name])

prompt_types = []
for p in PromptType:
    prompt_types.extend([p.name, p.value, str(p.value)])


def get_prompt(
    prompt_type,
    prompt_dict,
    chat,
    context,
    reduced,
    making_context,
    return_dict=False,
):
    prompt_dict_error = ""
    generates_leading_space = False

    if prompt_type == PromptType.custom.name and not isinstance(
        prompt_dict, dict
    ):
        try:
            prompt_dict = ast.literal_eval(prompt_dict)
        except BaseException as e:
            prompt_dict_error = str(e)
    if prompt_dict_error:
        promptA = None
        promptB = None
        PreInstruct = None
        PreInput = ""
        PreResponse = ""
        terminate_response = None
        chat_sep = ""
        chat_turn_sep = ""
        humanstr = ""
        botstr = ""
        generates_leading_space = False
    elif prompt_type in [
        PromptType.custom.value,
        str(PromptType.custom.value),
        PromptType.custom.name,
    ]:
        promptA = prompt_dict.get("promptA", "")
        promptB = prompt_dict.get("promptB", "")
        PreInstruct = prompt_dict.get("PreInstruct", "")
        PreInput = prompt_dict.get("PreInput", "")
        PreResponse = prompt_dict.get("PreResponse", "")
        terminate_response = prompt_dict.get("terminate_response", None)
        chat_sep = prompt_dict.get("chat_sep", "\n")
        chat_turn_sep = prompt_dict.get("chat_turn_sep", "\n")
        humanstr = prompt_dict.get("humanstr", "")
        botstr = prompt_dict.get("botstr", "")
    elif prompt_type in [
        PromptType.plain.value,
        str(PromptType.plain.value),
        PromptType.plain.name,
    ]:
        promptA = promptB = PreInstruct = PreInput = PreResponse = None
        terminate_response = []
        chat_turn_sep = chat_sep = ""
        # plain should have None for human/bot, so nothing truncated out, not '' that would truncate after first token
        humanstr = None
        botstr = None
    elif prompt_type == "simple_instruct":
        promptA = promptB = PreInstruct = PreInput = PreResponse = None
        terminate_response = []
        chat_turn_sep = chat_sep = "\n"
        humanstr = None
        botstr = None
    elif prompt_type in [
        PromptType.instruct.value,
        str(PromptType.instruct.value),
        PromptType.instruct.name,
    ] + [
        PromptType.instruct_with_end.value,
        str(PromptType.instruct_with_end.value),
        PromptType.instruct_with_end.name,
    ]:
        promptA = (
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n"
            if not (chat and reduced)
            else ""
        )
        promptB = (
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n"
            if not (chat and reduced)
            else ""
        )

        PreInstruct = """
### Instruction:
"""

        PreInput = """
### Input:
"""

        PreResponse = """
### Response:
"""
        if prompt_type in [
            PromptType.instruct_with_end.value,
            str(PromptType.instruct_with_end.value),
            PromptType.instruct_with_end.name,
        ]:
            terminate_response = ["### End"]
        else:
            terminate_response = None
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.quality.value,
        str(PromptType.quality.value),
        PromptType.quality.name,
    ]:
        promptA = (
            "Write a detailed high-quality, accurate, fair, Response with about 100 words by following the Instruction as applied on the Input.\n"
            if not (chat and reduced)
            else ""
        )
        promptB = (
            "Write a detailed high-quality, accurate, fair, Response with about 100 words by following the Instruction.\n"
            if not (chat and reduced)
            else ""
        )

        PreInstruct = """
### Instruction:
"""

        PreInput = """
### Input:
"""

        PreResponse = """
### Response:
"""
        terminate_response = None
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct  # first thing human says
        botstr = PreResponse  # first thing bot says
    elif prompt_type in [
        PromptType.human_bot.value,
        str(PromptType.human_bot.value),
        PromptType.human_bot.name,
    ] + [
        PromptType.human_bot_orig.value,
        str(PromptType.human_bot_orig.value),
        PromptType.human_bot_orig.name,
    ]:
        human = "<human>:"
        bot = "<bot>:"
        if (
            reduced
            or context
            or prompt_type
            in [
                PromptType.human_bot.value,
                str(PromptType.human_bot.value),
                PromptType.human_bot.name,
            ]
        ):
            preprompt = ""
        else:
            cur_date = time.strftime("%Y-%m-%d")
            cur_time = time.strftime("%H:%M:%S %p %Z")

            PRE_PROMPT = """\
Current Date: {}
Current Time: {}

"""
            preprompt = PRE_PROMPT.format(cur_date, cur_time)
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)

        PreInstruct = human + " "

        PreInput = None

        if making_context:
            # when making context, want it to appear as-if LLM generated, which starts with space after :
            PreResponse = bot + " "
        else:
            # normally LLM adds space after this, because was how trained.
            # if add space here, non-unique tokenization will often make LLM produce wrong output
            PreResponse = bot

        terminate_response = [
            "\n" + human,
            "\n" + bot,
            human,
            bot,
            PreResponse,
        ]
        chat_turn_sep = chat_sep = "\n"
        humanstr = human  # tag before human talks
        botstr = bot  # tag before bot talks
        generates_leading_space = True
    elif prompt_type in [
        PromptType.dai_faq.value,
        str(PromptType.dai_faq.value),
        PromptType.dai_faq.name,
    ]:
        promptA = ""
        promptB = "Answer the following Driverless AI question.\n"

        PreInstruct = """
### Driverless AI frequently asked question:
"""

        PreInput = None

        PreResponse = """
### Driverless AI documentation answer:
"""
        terminate_response = ["\n\n"]
        chat_turn_sep = chat_sep = terminate_response
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.summarize.value,
        str(PromptType.summarize.value),
        PromptType.summarize.name,
    ]:
        promptA = promptB = PreInput = ""
        PreInstruct = "## Main Text\n\n"
        PreResponse = "\n\n## Summary\n\n"
        terminate_response = None
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.instruct_vicuna.value,
        str(PromptType.instruct_vicuna.value),
        PromptType.instruct_vicuna.name,
    ]:
        promptA = promptB = (
            "A chat between a curious human and an artificial intelligence assistant. "
            "The assistant gives helpful, detailed, and polite answers to the human's questions."
            if not (chat and reduced)
            else ""
        )

        PreInstruct = """
### Human:
"""

        PreInput = None

        PreResponse = """
### Assistant:
"""
        terminate_response = [
            "### Human:"
        ]  # but only allow terminate after prompt is found correctly, else can't terminate
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.prompt_answer.value,
        str(PromptType.prompt_answer.value),
        PromptType.prompt_answer.name,
    ]:
        preprompt = ""
        prompt_tokens = "<|prompt|>"
        answer_tokens = "<|answer|>"
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = prompt_tokens
        PreInput = None
        PreResponse = answer_tokens
        eos = "<|endoftext|>"  # neox eos
        humanstr = prompt_tokens
        botstr = answer_tokens
        terminate_response = [humanstr, PreResponse, eos]
        chat_sep = eos
        chat_turn_sep = eos
    elif prompt_type in [
        PromptType.prompt_answer_openllama.value,
        str(PromptType.prompt_answer_openllama.value),
        PromptType.prompt_answer_openllama.name,
    ]:
        preprompt = ""
        prompt_tokens = "<|prompt|>"
        answer_tokens = "<|answer|>"
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = prompt_tokens
        PreInput = None
        PreResponse = answer_tokens
        eos = "</s>"  # llama eos
        humanstr = prompt_tokens
        botstr = answer_tokens
        terminate_response = [humanstr, PreResponse, eos]
        chat_sep = eos
        chat_turn_sep = eos
    elif prompt_type in [
        PromptType.open_assistant.value,
        str(PromptType.open_assistant.value),
        PromptType.open_assistant.name,
    ]:
        # From added_tokens.json
        preprompt = ""
        prompt_tokens = "<|prompter|>"
        answer_tokens = "<|assistant|>"
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = prompt_tokens
        PreInput = None
        PreResponse = answer_tokens
        pend = "<|prefix_end|>"
        eos = "</s>"
        humanstr = prompt_tokens
        botstr = answer_tokens
        terminate_response = [humanstr, PreResponse, pend, eos]
        chat_turn_sep = chat_sep = eos
    elif prompt_type in [
        PromptType.wizard_lm.value,
        str(PromptType.wizard_lm.value),
        PromptType.wizard_lm.name,
    ]:
        # https://github.com/ehartford/WizardLM/blob/main/src/train_freeform.py
        preprompt = ""
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = ""
        PreInput = None
        PreResponse = "\n\n### Response\n"
        eos = "</s>"
        terminate_response = [PreResponse, eos]
        chat_turn_sep = chat_sep = eos
        humanstr = promptA
        botstr = PreResponse
    elif prompt_type in [
        PromptType.wizard_mega.value,
        str(PromptType.wizard_mega.value),
        PromptType.wizard_mega.name,
    ]:
        preprompt = ""
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = """
### Instruction:
"""
        PreInput = None
        PreResponse = """
### Assistant:
"""
        terminate_response = [PreResponse]
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.instruct_vicuna2.value,
        str(PromptType.instruct_vicuna2.value),
        PromptType.instruct_vicuna2.name,
    ]:
        promptA = promptB = "" if not (chat and reduced) else ""

        PreInstruct = """
HUMAN:
"""

        PreInput = None

        PreResponse = """
ASSISTANT:
"""
        terminate_response = [
            "HUMAN:"
        ]  # but only allow terminate after prompt is found correctly, else can't terminate
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.instruct_vicuna3.value,
        str(PromptType.instruct_vicuna3.value),
        PromptType.instruct_vicuna3.name,
    ]:
        promptA = promptB = "" if not (chat and reduced) else ""

        PreInstruct = """
### User:
"""

        PreInput = None

        PreResponse = """
### Assistant:
"""
        terminate_response = [
            "### User:"
        ]  # but only allow terminate after prompt is found correctly, else can't terminate
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.wizard2.value,
        str(PromptType.wizard2.value),
        PromptType.wizard2.name,
    ]:
        # https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGML
        preprompt = (
            """Below is an instruction that describes a task. Write a response that appropriately completes the request."""
            if not (chat and reduced)
            else ""
        )
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = """
### Instruction:
"""
        PreInput = None
        PreResponse = """
### Response:
"""
        terminate_response = [PreResponse]
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.wizard3.value,
        str(PromptType.wizard3.value),
        PromptType.wizard3.name,
    ]:
        # https://huggingface.co/TheBloke/wizardLM-13B-1.0-GGML
        preprompt = (
            """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""
            if not (chat and reduced)
            else ""
        )
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = """USER: """
        PreInput = None
        PreResponse = """ASSISTANT: """
        terminate_response = [PreResponse]
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.wizard_vicuna.value,
        str(PromptType.wizard_vicuna.value),
        PromptType.wizard_vicuna.name,
    ]:
        preprompt = ""
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = """USER: """
        PreInput = None
        PreResponse = """ASSISTANT: """
        terminate_response = [PreResponse]
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse

    elif prompt_type in [
        PromptType.instruct_simple.value,
        str(PromptType.instruct_simple.value),
        PromptType.instruct_simple.name,
    ]:
        promptB = promptA = "" if not (chat and reduced) else ""

        PreInstruct = """
### Instruction:
"""

        PreInput = """
### Input:
"""

        PreResponse = """
### Response:
"""
        terminate_response = None
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.openai.value,
        str(PromptType.openai.value),
        PromptType.openai.name,
    ]:
        preprompt = (
            """The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly."""
            if not (chat and reduced)
            else ""
        )
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = "\nHuman: "
        PreInput = None
        PreResponse = "\nAI:"
        terminate_response = [PreResponse] + [" Human:", " AI:"]
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.gptj.value,
        str(PromptType.gptj.value),
        PromptType.gptj.name,
    ]:
        preprompt = (
            "### Instruction:\n The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response."
            if not (chat and reduced)
            else ""
        )
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = "\n### Prompt: "
        PreInput = None
        PreResponse = "\n### Response: "
        terminate_response = [PreResponse] + ["Prompt:", "Response:"]
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.openai_chat.value,
        str(PromptType.openai_chat.value),
        PromptType.openai_chat.name,
    ]:
        # prompting and termination all handled by endpoint
        preprompt = """"""
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        PreInstruct = ""
        PreInput = None
        PreResponse = ""
        terminate_response = []
        chat_turn_sep = chat_sep = "\n"
        humanstr = None
        botstr = None
    elif prompt_type in [
        PromptType.vicuna11.value,
        str(PromptType.vicuna11.value),
        PromptType.vicuna11.name,
    ]:
        preprompt = (
            """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. """
            if not (chat and reduced)
            else ""
        )
        start = ""
        promptB = promptA = "%s%s" % (preprompt, start)
        eos = "</s>"
        PreInstruct = """USER: """
        PreInput = None
        PreResponse = """ASSISTANT:"""
        terminate_response = [PreResponse]
        chat_sep = " "
        chat_turn_sep = eos
        humanstr = PreInstruct
        botstr = PreResponse

        if making_context:
            # when making context, want it to appear as-if LLM generated, which starts with space after :
            PreResponse = PreResponse + " "
        else:
            # normally LLM adds space after this, because was how trained.
            # if add space here, non-unique tokenization will often make LLM produce wrong output
            PreResponse = PreResponse
    elif prompt_type in [
        PromptType.mptinstruct.value,
        str(PromptType.mptinstruct.value),
        PromptType.mptinstruct.name,
    ]:
        # https://huggingface.co/mosaicml/mpt-30b-instruct#formatting
        promptA = promptB = (
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n"
            if not (chat and reduced)
            else ""
        )

        PreInstruct = """
### Instruction
"""

        PreInput = """
### Input
"""

        PreResponse = """
### Response
"""
        terminate_response = None
        chat_turn_sep = chat_sep = "\n"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.mptchat.value,
        str(PromptType.mptchat.value),
        PromptType.mptchat.name,
    ]:
        # https://huggingface.co/TheBloke/mpt-30B-chat-GGML#prompt-template
        promptA = promptB = (
            """<|im_start|>system\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\n<|im_end|>"""
            if not (chat and reduced)
            else ""
        )

        PreInstruct = """<|im_start|>user
"""

        PreInput = None

        PreResponse = """<|im_end|><|im_start|>assistant
"""
        terminate_response = ["<|im_end|>"]
        chat_sep = ""
        chat_turn_sep = "<|im_end|>"
        humanstr = PreInstruct
        botstr = PreResponse
    elif prompt_type in [
        PromptType.falcon.value,
        str(PromptType.falcon.value),
        PromptType.falcon.name,
    ]:
        promptA = promptB = "" if not (chat and reduced) else ""

        PreInstruct = """User: """

        PreInput = None

        PreResponse = """Assistant:"""
        terminate_response = ["\nUser", "<|endoftext|>"]
        chat_sep = "\n\n"
        chat_turn_sep = "\n\n"
        humanstr = PreInstruct
        botstr = PreResponse
        if making_context:
            # when making context, want it to appear as-if LLM generated, which starts with space after :
            PreResponse = "Assistant: "
        else:
            # normally LLM adds space after this, because was how trained.
            # if add space here, non-unique tokenization will often make LLM produce wrong output
            PreResponse = PreResponse
        # generates_leading_space = True
    else:
        raise RuntimeError("No such prompt_type=%s" % prompt_type)

    if isinstance(terminate_response, (tuple, list)):
        assert "" not in terminate_response, "Bad terminate_response"

    ret_dict = dict(
        promptA=promptA,
        promptB=promptB,
        PreInstruct=PreInstruct,
        PreInput=PreInput,
        PreResponse=PreResponse,
        terminate_response=terminate_response,
        chat_sep=chat_sep,
        chat_turn_sep=chat_turn_sep,
        humanstr=humanstr,
        botstr=botstr,
        generates_leading_space=generates_leading_space,
    )

    if return_dict:
        return ret_dict, prompt_dict_error
    else:
        return tuple(list(ret_dict.values()))


def generate_prompt(
    data_point, prompt_type, prompt_dict, chat, reduced, making_context
):
    context = data_point.get("context")
    if context is None:
        context = ""
    instruction = data_point.get("instruction")
    input = data_point.get("input")
    output = data_point.get("output")
    prompt_type = data_point.get("prompt_type", prompt_type)
    prompt_dict = data_point.get("prompt_dict", prompt_dict)
    assert prompt_type in prompt_types, "Bad prompt type: %s" % prompt_type
    (
        promptA,
        promptB,
        PreInstruct,
        PreInput,
        PreResponse,
        terminate_response,
        chat_sep,
        chat_turn_sep,
        humanstr,
        botstr,
        generates_leading_space,
    ) = get_prompt(
        prompt_type, prompt_dict, chat, context, reduced, making_context
    )

    # could avoid if reduce=True, but too complex for parent functions to handle
    prompt = context

    if input and promptA:
        prompt += f"""{promptA}"""
    elif promptB:
        prompt += f"""{promptB}"""

    if (
        instruction
        and PreInstruct is not None
        and input
        and PreInput is not None
    ):
        prompt += f"""{PreInstruct}{instruction}{PreInput}{input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif (
        instruction and input and PreInstruct is None and PreInput is not None
    ):
        prompt += f"""{PreInput}{instruction}
{input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif (
        input and instruction and PreInput is None and PreInstruct is not None
    ):
        prompt += f"""{PreInstruct}{instruction}
{input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif instruction and PreInstruct is not None:
        prompt += f"""{PreInstruct}{instruction}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif input and PreInput is not None:
        prompt += f"""{PreInput}{input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif input and instruction and PreInput is not None:
        prompt += f"""{PreInput}{instruction}{input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif input and instruction and PreInstruct is not None:
        prompt += f"""{PreInstruct}{instruction}{input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif input and instruction:
        # i.e. for simple_instruct
        prompt += f"""{instruction}: {input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif input:
        prompt += f"""{input}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)
    elif instruction:
        prompt += f"""{instruction}"""
        prompt = inject_chatsep(prompt_type, prompt, chat_sep=chat_sep)

    if PreResponse is not None:
        prompt += f"""{PreResponse}"""
        pre_response = PreResponse  # Don't use strip
    else:
        pre_response = ""

    if output:
        prompt += f"""{output}"""

    return prompt, pre_response, terminate_response, chat_sep, chat_turn_sep


def inject_chatsep(prompt_type, prompt, chat_sep=None):
    if chat_sep:
        # only add new line if structured prompt, while 'plain' is just generation of next tokens from input
        prompt += chat_sep
    return prompt


class Prompter(object):
    def __init__(
        self,
        prompt_type,
        prompt_dict,
        debug=False,
        chat=False,
        stream_output=False,
        repeat_penalty=True,
        allowed_repeat_line_length=10,
    ):
        self.prompt_type = prompt_type
        self.prompt_dict = prompt_dict
        self.debug = debug
        self.chat = chat
        self.stream_output = stream_output
        self.repeat_penalty = repeat_penalty
        self.allowed_repeat_line_length = allowed_repeat_line_length
        self.prompt = None
        context = ""  # not for chat context
        reduced = False  # not for chat context
        making_context = False  # not for chat context
        (
            self.promptA,
            self.promptB,
            self.PreInstruct,
            self.PreInput,
            self.PreResponse,
            self.terminate_response,
            self.chat_sep,
            self.chat_turn_sep,
            self.humanstr,
            self.botstr,
            self.generates_leading_space,
        ) = get_prompt(
            self.prompt_type,
            self.prompt_dict,
            chat,
            context,
            reduced,
            making_context,
        )
        self.pre_response = self.PreResponse

    def generate_prompt(self, data_point, reduced=None):
        """
        data_point['context'] is assumed to be like a system prompt or pre-conversation, not inserted after user prompt
        :param data_point:
        :param reduced:
        :return:
        """
        reduced = (
            data_point.get("context") not in ["", None]
            if reduced is None
            else reduced
        )
        making_context = False  # whether really making final prompt or just generating context
        prompt, _, _, _, _ = generate_prompt(
            data_point,
            self.prompt_type,
            self.prompt_dict,
            self.chat,
            reduced,
            making_context,
        )
        if self.debug:
            print("prompt: %s" % prompt, flush=True)
        # if have context, should have always reduced and only preappend promptA/B here
        if data_point.get("context"):
            if data_point.get("input") and self.promptA:
                prompt = self.promptA + prompt
            elif self.promptB:
                prompt = self.promptB + prompt

        self.prompt = prompt
        return prompt

    def get_response(self, outputs, prompt=None, sanitize_bot_response=False):
        if isinstance(outputs, str):
            outputs = [outputs]
        if self.debug:
            print("output:\n%s" % "\n\n".join(outputs), flush=True)
        if prompt is not None:
            self.prompt = prompt

        def clean_response(response):
            meaningless_words = ["<pad>", "</s>", "<|endoftext|>"]
            for word in meaningless_words:
                response = response.replace(word, "")
            if sanitize_bot_response:
                from better_profanity import profanity

                response = profanity.censor(response)
            if (
                self.generates_leading_space
                and isinstance(response, str)
                and len(response) > 0
                and response[0] == " "
            ):
                response = response[1:]
            return response

        def clean_repeats(response):
            lines = response.split("\n")
            new_lines = []
            [
                new_lines.append(line)
                for line in lines
                if line not in new_lines
                or len(line) < self.allowed_repeat_line_length
            ]
            if self.debug and len(lines) != len(new_lines):
                print(
                    "cleaned repeats: %s %s" % (len(lines), len(new_lines)),
                    flush=True,
                )
            response = "\n".join(new_lines)
            return response

        multi_output = len(outputs) > 1

        for oi, output in enumerate(outputs):
            if self.prompt_type in [
                PromptType.plain.value,
                str(PromptType.plain.value),
                PromptType.plain.name,
            ]:
                output = clean_response(output)
            elif prompt is None:
                # then use most basic parsing like pipeline
                if not self.botstr:
                    pass
                elif self.botstr in output:
                    if self.humanstr:
                        output = clean_response(
                            output.split(self.botstr)[1].split(self.humanstr)[
                                0
                            ]
                        )
                    else:
                        # i.e. use after bot but only up to next bot
                        output = clean_response(
                            output.split(self.botstr)[1].split(self.botstr)[0]
                        )
                else:
                    # output = clean_response(output)
                    # assume just not printed yet
                    output = ""
            else:
                # find first instance of prereponse
                # prompt sometimes has odd characters, that mutate length,
                # so can't go by length alone
                if self.pre_response:
                    outputi = output.find(prompt)
                    if outputi >= 0:
                        output = output[outputi + len(prompt) :]
                        allow_terminate = True
                    else:
                        # subtraction is risky due to space offsets sometimes, so only do if necessary
                        output = output[len(prompt) - len(self.pre_response) :]
                        # [1] to avoid repeated pre_response, just take first (after prompt - pre_response for chat)
                        if self.pre_response in output:
                            output = output.split(self.pre_response)[1]
                            allow_terminate = True
                        else:
                            if output:
                                print(
                                    "Failure of parsing or not enough output yet: %s"
                                    % output,
                                    flush=True,
                                )
                            allow_terminate = False
                else:
                    allow_terminate = True
                    output = output[len(prompt) :]
                # clean after subtract prompt out, so correct removal of pre_response
                output = clean_response(output)
                if self.repeat_penalty:
                    output = clean_repeats(output)
                if self.terminate_response and allow_terminate:
                    finds = []
                    for term in self.terminate_response:
                        finds.append(output.find(term))
                    finds = [x for x in finds if x >= 0]
                    if len(finds) > 0:
                        termi = finds[0]
                        output = output[:termi]
                    else:
                        output = output
            if multi_output:
                # prefix with output counter
                output = "\n=========== Output %d\n\n" % (1 + oi) + output
                if oi > 0:
                    # post fix outputs with seperator
                    output += "\n"
            outputs[oi] = output
        # join all outputs, only one extra new line between outputs
        output = "\n".join(outputs)
        if self.debug:
            print("outputclean:\n%s" % "\n\n".join(outputs), flush=True)
        return output

```

`apps/language_models/langchain/read_wiki_full.py`:

```py
"""Load Data from a MediaWiki dump xml."""
import ast
import glob
import pickle
import uuid
from typing import List, Optional
import os
import bz2
import csv
import numpy as np
import pandas as pd
import pytest
from matplotlib import pyplot as plt

from langchain.docstore.document import Document
from langchain.document_loaders import MWDumpLoader

# path where downloaded wiki files exist, to be processed
root_path = "/data/jon/h2o-llm"


def unescape(x):
    try:
        x = ast.literal_eval(x)
    except:
        try:
            x = x.encode("ascii", "ignore").decode("unicode_escape")
        except:
            pass
    return x


def get_views():
    # views = pd.read_csv('wiki_page_views_more_1000month.csv')
    views = pd.read_csv("wiki_page_views_more_5000month.csv")
    views.index = views["title"]
    views = views["views"]
    views = views.to_dict()
    views = {str(unescape(str(k))): v for k, v in views.items()}
    views2 = {k.replace("_", " "): v for k, v in views.items()}
    # views has _ but pages has " "
    views.update(views2)
    return views


class MWDumpDirectLoader(MWDumpLoader):
    def __init__(
        self,
        data: str,
        encoding: Optional[str] = "utf8",
        title_words_limit=None,
        use_views=True,
        verbose=True,
    ):
        """Initialize with file path."""
        self.data = data
        self.encoding = encoding
        self.title_words_limit = title_words_limit
        self.verbose = verbose
        if use_views:
            # self.views = get_views()
            # faster to use global shared values
            self.views = global_views
        else:
            self.views = None

    def load(self) -> List[Document]:
        """Load from file path."""
        import mwparserfromhell
        import mwxml

        dump = mwxml.Dump.from_page_xml(self.data)

        docs = []

        for page in dump.pages:
            if self.views is not None and page.title not in self.views:
                if self.verbose:
                    print("Skipped %s low views" % page.title, flush=True)
                continue
            for revision in page:
                if self.title_words_limit is not None:
                    num_words = len(" ".join(page.title.split("_")).split(" "))
                    if num_words > self.title_words_limit:
                        if self.verbose:
                            print("Skipped %s" % page.title, flush=True)
                        continue
                if self.verbose:
                    if self.views is not None:
                        print(
                            "Kept %s views: %s"
                            % (page.title, self.views[page.title]),
                            flush=True,
                        )
                    else:
                        print("Kept %s" % page.title, flush=True)

                code = mwparserfromhell.parse(revision.text)
                text = code.strip_code(
                    normalize=True, collapse=True, keep_template_params=False
                )
                title_url = str(page.title).replace(" ", "_")
                metadata = dict(
                    title=page.title,
                    source="https://en.wikipedia.org/wiki/" + title_url,
                    id=page.id,
                    redirect=page.redirect,
                    views=self.views[page.title]
                    if self.views is not None
                    else -1,
                )
                metadata = {k: v for k, v in metadata.items() if v is not None}
                docs.append(Document(page_content=text, metadata=metadata))

        return docs


def search_index(search_term, index_filename):
    byte_flag = False
    data_length = start_byte = 0
    index_file = open(index_filename, "r")
    csv_reader = csv.reader(index_file, delimiter=":")
    for line in csv_reader:
        if not byte_flag and search_term == line[2]:
            start_byte = int(line[0])
            byte_flag = True
        elif byte_flag and int(line[0]) != start_byte:
            data_length = int(line[0]) - start_byte
            break
    index_file.close()
    return start_byte, data_length


def get_start_bytes(index_filename):
    index_file = open(index_filename, "r")
    csv_reader = csv.reader(index_file, delimiter=":")
    start_bytes = set()
    for line in csv_reader:
        start_bytes.add(int(line[0]))
    index_file.close()
    return sorted(start_bytes)


def get_wiki_filenames():
    # requires
    # wget http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/20230401/enwiki-20230401-pages-articles-multistream-index.txt.bz2
    base_path = os.path.join(
        root_path, "enwiki-20230401-pages-articles-multistream"
    )
    index_file = "enwiki-20230401-pages-articles-multistream-index.txt"
    index_filename = os.path.join(base_path, index_file)
    wiki_filename = os.path.join(
        base_path, "enwiki-20230401-pages-articles-multistream.xml.bz2"
    )
    return index_filename, wiki_filename


def get_documents_by_search_term(search_term):
    index_filename, wiki_filename = get_wiki_filenames()
    start_byte, data_length = search_index(search_term, index_filename)
    with open(wiki_filename, "rb") as wiki_file:
        wiki_file.seek(start_byte)
        data = bz2.BZ2Decompressor().decompress(wiki_file.read(data_length))

    loader = MWDumpDirectLoader(data.decode())
    documents = loader.load()
    return documents


def get_one_chunk(
    wiki_filename,
    start_byte,
    end_byte,
    return_file=True,
    title_words_limit=None,
    use_views=True,
):
    data_length = end_byte - start_byte
    with open(wiki_filename, "rb") as wiki_file:
        wiki_file.seek(start_byte)
        data = bz2.BZ2Decompressor().decompress(wiki_file.read(data_length))

    loader = MWDumpDirectLoader(
        data.decode(), title_words_limit=title_words_limit, use_views=use_views
    )
    documents1 = loader.load()
    if return_file:
        base_tmp = "temp_wiki"
        if not os.path.isdir(base_tmp):
            os.makedirs(base_tmp, exist_ok=True)
        filename = os.path.join(base_tmp, str(uuid.uuid4()) + ".tmp.pickle")
        with open(filename, "wb") as f:
            pickle.dump(documents1, f)
        return filename
    return documents1


from joblib import Parallel, delayed

global_views = get_views()


def get_all_documents(small_test=2, n_jobs=None, use_views=True):
    print("DO get all wiki docs: %s" % small_test, flush=True)
    index_filename, wiki_filename = get_wiki_filenames()
    start_bytes = get_start_bytes(index_filename)
    end_bytes = start_bytes[1:]
    start_bytes = start_bytes[:-1]

    if small_test:
        start_bytes = start_bytes[:small_test]
        end_bytes = end_bytes[:small_test]
        if n_jobs is None:
            n_jobs = 5
    else:
        if n_jobs is None:
            n_jobs = os.cpu_count() // 4

    # default loky backend leads to name space conflict problems
    return_file = True  # large return from joblib hangs
    documents = Parallel(n_jobs=n_jobs, verbose=10, backend="multiprocessing")(
        delayed(get_one_chunk)(
            wiki_filename,
            start_byte,
            end_byte,
            return_file=return_file,
            use_views=use_views,
        )
        for start_byte, end_byte in zip(start_bytes, end_bytes)
    )
    if return_file:
        # then documents really are files
        files = documents.copy()
        documents = []
        for fil in files:
            with open(fil, "rb") as f:
                documents.extend(pickle.load(f))
            os.remove(fil)
    else:
        from functools import reduce
        from operator import concat

        documents = reduce(concat, documents)
    assert isinstance(documents, list)

    print("DONE get all wiki docs", flush=True)
    return documents


def test_by_search_term():
    search_term = "Apollo"
    assert len(get_documents_by_search_term(search_term)) == 100

    search_term = "Abstract (law)"
    assert len(get_documents_by_search_term(search_term)) == 100

    search_term = "Artificial languages"
    assert len(get_documents_by_search_term(search_term)) == 100


def test_start_bytes():
    index_filename, wiki_filename = get_wiki_filenames()
    assert len(get_start_bytes(index_filename)) == 227850


def test_get_all_documents():
    small_test = 20  # 227850
    n_jobs = os.cpu_count() // 4

    assert (
        len(
            get_all_documents(
                small_test=small_test, n_jobs=n_jobs, use_views=False
            )
        )
        == small_test * 100
    )

    assert (
        len(
            get_all_documents(
                small_test=small_test, n_jobs=n_jobs, use_views=True
            )
        )
        == 429
    )


def get_one_pageviews(fil):
    df1 = pd.read_csv(
        fil,
        sep=" ",
        header=None,
        names=["region", "title", "views", "foo"],
        quoting=csv.QUOTE_NONE,
    )
    df1.index = df1["title"]
    df1 = df1[df1["region"] == "en"]
    df1 = df1.drop("region", axis=1)
    df1 = df1.drop("foo", axis=1)
    df1 = df1.drop("title", axis=1)  # already index

    base_tmp = "temp_wiki_pageviews"
    if not os.path.isdir(base_tmp):
        os.makedirs(base_tmp, exist_ok=True)
    filename = os.path.join(base_tmp, str(uuid.uuid4()) + ".tmp.csv")
    df1.to_csv(filename, index=True)
    return filename


def test_agg_pageviews(gen_files=False):
    if gen_files:
        path = os.path.join(
            root_path,
            "wiki_pageviews/dumps.wikimedia.org/other/pageviews/2023/2023-04",
        )
        files = glob.glob(os.path.join(path, "pageviews*.gz"))
        # files = files[:2]  # test
        n_jobs = os.cpu_count() // 2
        csv_files = Parallel(
            n_jobs=n_jobs, verbose=10, backend="multiprocessing"
        )(delayed(get_one_pageviews)(fil) for fil in files)
    else:
        # to continue without redoing above
        csv_files = glob.glob(
            os.path.join(root_path, "temp_wiki_pageviews/*.csv")
        )

    df_list = []
    for csv_file in csv_files:
        print(csv_file)
        df1 = pd.read_csv(csv_file)
        df_list.append(df1)
    df = pd.concat(df_list, axis=0)
    df = df.groupby("title")["views"].sum().reset_index()
    df.to_csv("wiki_page_views.csv", index=True)


def test_reduce_pageview():
    filename = "wiki_page_views.csv"
    df = pd.read_csv(filename)
    df = df[df["views"] < 1e7]
    #
    plt.hist(df["views"], bins=100, log=True)
    views_avg = np.mean(df["views"])
    views_median = np.median(df["views"])
    plt.title("Views avg: %s median: %s" % (views_avg, views_median))
    plt.savefig(filename.replace(".csv", ".png"))
    plt.close()
    #
    views_limit = 5000
    df = df[df["views"] > views_limit]
    filename = "wiki_page_views_more_5000month.csv"
    df.to_csv(filename, index=True)
    #
    plt.hist(df["views"], bins=100, log=True)
    views_avg = np.mean(df["views"])
    views_median = np.median(df["views"])
    plt.title("Views avg: %s median: %s" % (views_avg, views_median))
    plt.savefig(filename.replace(".csv", ".png"))
    plt.close()


@pytest.mark.skip("Only if doing full processing again, some manual steps")
def test_do_wiki_full_all():
    # Install other requirements for wiki specific conversion:
    # pip install -r reqs_optional/requirements_optional_wikiprocessing.txt

    # Use "Transmission" in Ubuntu to get wiki dump using torrent:
    # See: https://meta.wikimedia.org/wiki/Data_dump_torrents
    # E.g. magnet:?xt=urn:btih:b2c74af2b1531d0b63f1166d2011116f44a8fed0&dn=enwiki-20230401-pages-articles-multistream.xml.bz2&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337

    # Get index
    os.system(
        "wget http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/20230401/enwiki-20230401-pages-articles-multistream-index.txt.bz2"
    )

    # Test that can use LangChain to get docs from subset of wiki as sampled out of full wiki directly using bzip multistream
    test_get_all_documents()

    # Check can search wiki multistream
    test_by_search_term()

    # Test can get all start bytes in index
    test_start_bytes()

    # Get page views, e.g. for entire month of April 2023
    os.system(
        "wget -b -m -k -o wget.log -e robots=off https://dumps.wikimedia.org/other/pageviews/2023/2023-04/"
    )

    # Aggregate page views from many files into single file
    test_agg_pageviews(gen_files=True)

    # Reduce page views to some limit, so processing of full wiki is not too large
    test_reduce_pageview()

    # Start generate.py with requesting wiki_full in prep.  This will use page views as referenced in get_views.
    # Note get_views as global() function done once is required to avoid very slow processing
    # WARNING: Requires alot of memory to handle, used up to 300GB system RAM at peak
    """
    python generate.py --langchain_mode='wiki_full' --visible_langchain_modes="['wiki_full', 'UserData', 'MyData', 'github h2oGPT', 'DriverlessAI docs']" &> lc_out.log
    """

```

`apps/language_models/langchain/stopping.py`:

```py
import torch
from transformers import StoppingCriteria, StoppingCriteriaList

from enums import PromptType


class StoppingCriteriaSub(StoppingCriteria):
    def __init__(
        self, stops=[], encounters=[], device="cuda", model_max_length=None
    ):
        super().__init__()
        assert (
            len(stops) % len(encounters) == 0
        ), "Number of stops and encounters must match"
        self.encounters = encounters
        self.stops = [stop.to(device) for stop in stops]
        self.num_stops = [0] * len(stops)
        self.model_max_length = model_max_length

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
    ) -> bool:
        for stopi, stop in enumerate(self.stops):
            if torch.all((stop == input_ids[0][-len(stop) :])).item():
                self.num_stops[stopi] += 1
                if (
                    self.num_stops[stopi]
                    >= self.encounters[stopi % len(self.encounters)]
                ):
                    # print("Stopped", flush=True)
                    return True
        if (
            self.model_max_length is not None
            and input_ids[0].shape[0] >= self.model_max_length
        ):
            # critical limit
            return True
        # print("Tokens: %s" % input_ids[0].cpu().numpy(), flush=True)
        # print("Stop Tokens: %s" % [x.cpu().numpy() for x in self.stops], flush=True)
        return False


def get_stopping(
    prompt_type,
    prompt_dict,
    tokenizer,
    device,
    human="<human>:",
    bot="<bot>:",
    model_max_length=None,
):
    # FIXME: prompt_dict unused currently
    if prompt_type in [
        PromptType.human_bot.name,
        PromptType.instruct_vicuna.name,
        PromptType.instruct_with_end.name,
    ]:
        if prompt_type == PromptType.human_bot.name:
            # encounters = [prompt.count(human) + 1, prompt.count(bot) + 1]
            # stopping only starts once output is beyond prompt
            # 1 human is enough to trigger, but need 2 bots, because very first view back will be bot we added
            stop_words = [human, bot, "\n" + human, "\n" + bot]
            encounters = [1, 2]
        elif prompt_type == PromptType.instruct_vicuna.name:
            # even below is not enough, generic strings and many ways to encode
            stop_words = [
                "### Human:",
                """
### Human:""",
                """
### Human:
""",
                "### Assistant:",
                """
### Assistant:""",
                """
### Assistant:
""",
            ]
            encounters = [1, 2]
        else:
            # some instruct prompts have this as end, doesn't hurt to stop on it since not common otherwise
            stop_words = ["### End"]
            encounters = [1]
        stop_words_ids = [
            tokenizer(stop_word, return_tensors="pt")["input_ids"].squeeze()
            for stop_word in stop_words
        ]
        # handle single token case
        stop_words_ids = [
            x if len(x.shape) > 0 else torch.tensor([x])
            for x in stop_words_ids
        ]
        stop_words_ids = [x for x in stop_words_ids if x.shape[0] > 0]
        # avoid padding in front of tokens
        if (
            tokenizer._pad_token
        ):  # use hidden variable to avoid annoying properly logger bug
            stop_words_ids = [
                x[1:] if x[0] == tokenizer.pad_token_id and len(x) > 1 else x
                for x in stop_words_ids
            ]
        # handle fake \n added
        stop_words_ids = [
            x[1:] if y[0] == "\n" else x
            for x, y in zip(stop_words_ids, stop_words)
        ]
        # build stopper
        stopping_criteria = StoppingCriteriaList(
            [
                StoppingCriteriaSub(
                    stops=stop_words_ids,
                    encounters=encounters,
                    device=device,
                    model_max_length=model_max_length,
                )
            ]
        )
    else:
        stopping_criteria = StoppingCriteriaList()
    return stopping_criteria

```

`apps/language_models/langchain/utils.py`:

```py
import contextlib
import functools
import hashlib
import inspect
import os
import gc
import pathlib
import random
import shutil
import subprocess
import sys
import threading
import time
import traceback
import zipfile
from datetime import datetime

import filelock
import requests, uuid
from typing import Tuple, Callable, Dict
from tqdm.auto import tqdm
from joblib import Parallel
from concurrent.futures import ProcessPoolExecutor
import numpy as np
import pandas as pd


def set_seed(seed: int):
    """
    Sets the seed of the entire notebook so results are the same every time we run.
    This is for REPRODUCIBILITY.
    """
    import torch

    np.random.seed(seed)
    random_state = np.random.RandomState(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)
    return random_state


def flatten_list(lis):
    """Given a list, possibly nested to any level, return it flattened."""
    new_lis = []
    for item in lis:
        if type(item) == type([]):
            new_lis.extend(flatten_list(item))
        else:
            new_lis.append(item)
    return new_lis


def clear_torch_cache():
    import torch

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        gc.collect()


def ping():
    try:
        print("Ping: %s" % str(datetime.now()), flush=True)
    except AttributeError:
        # some programs wrap print and will fail with flush passed
        pass


def ping_gpu():
    try:
        print(
            "Ping_GPU: %s %s" % (str(datetime.now()), system_info()),
            flush=True,
        )
    except AttributeError:
        # some programs wrap print and will fail with flush passed
        pass
    try:
        ping_gpu_memory()
    except Exception as e:
        print("Ping_GPU memory failure: %s" % str(e), flush=True)


def ping_gpu_memory():
    from models.gpu_mem_track import MemTracker

    gpu_tracker = MemTracker()  # define a GPU tracker
    from torch.cuda import memory_summary

    gpu_tracker.track()


def get_torch_allocated():
    import torch

    return torch.cuda.memory_allocated()


def get_device():
    import torch

    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_built():
        device = "mps"
    else:
        device = "cpu"

    return device


def system_info():
    import psutil

    system = {}
    # https://stackoverflow.com/questions/48951136/plot-multiple-graphs-in-one-plot-using-tensorboard
    # https://arshren.medium.com/monitoring-your-devices-in-python-5191d672f749
    temps = psutil.sensors_temperatures(fahrenheit=False)
    if "coretemp" in temps:
        coretemp = temps["coretemp"]
        temp_dict = {k.label: k.current for k in coretemp}
        for k, v in temp_dict.items():
            system["CPU_C/%s" % k] = v

    # https://github.com/gpuopenanalytics/pynvml/blob/master/help_query_gpu.txt
    try:
        from pynvml.smi import nvidia_smi

        nvsmi = nvidia_smi.getInstance()

        gpu_power_dict = {
            "W_gpu%d" % i: x["power_readings"]["power_draw"]
            for i, x in enumerate(nvsmi.DeviceQuery("power.draw")["gpu"])
        }
        for k, v in gpu_power_dict.items():
            system["GPU_W/%s" % k] = v

        gpu_temp_dict = {
            "C_gpu%d" % i: x["temperature"]["gpu_temp"]
            for i, x in enumerate(nvsmi.DeviceQuery("temperature.gpu")["gpu"])
        }
        for k, v in gpu_temp_dict.items():
            system["GPU_C/%s" % k] = v

        gpu_memory_free_dict = {
            "MiB_gpu%d" % i: x["fb_memory_usage"]["free"]
            for i, x in enumerate(nvsmi.DeviceQuery("memory.free")["gpu"])
        }
        gpu_memory_total_dict = {
            "MiB_gpu%d" % i: x["fb_memory_usage"]["total"]
            for i, x in enumerate(nvsmi.DeviceQuery("memory.total")["gpu"])
        }
        gpu_memory_frac_dict = {
            k: gpu_memory_free_dict[k] / gpu_memory_total_dict[k]
            for k in gpu_memory_total_dict
        }
        for k, v in gpu_memory_frac_dict.items():
            system[f"GPU_M/%s" % k] = v
    except ModuleNotFoundError:
        pass
    system["hash"] = get_githash()

    return system


def system_info_print():
    try:
        df = pd.DataFrame.from_dict(system_info(), orient="index")
        # avoid slamming GPUs
        time.sleep(1)
        return df.to_markdown()
    except Exception as e:
        return "Error: %s" % str(e)


def zip_data(
    root_dirs=None, zip_file=None, base_dir="./", fail_any_exception=False
):
    try:
        return _zip_data(
            zip_file=zip_file, base_dir=base_dir, root_dirs=root_dirs
        )
    except Exception as e:
        traceback.print_exc()
        print("Exception in zipping: %s" % str(e))
        if not fail_any_exception:
            raise


def _zip_data(root_dirs=None, zip_file=None, base_dir="./"):
    if isinstance(root_dirs, str):
        root_dirs = [root_dirs]
    if zip_file is None:
        datetime_str = str(datetime.now()).replace(" ", "_").replace(":", "_")
        host_name = os.getenv("HF_HOSTNAME", "emptyhost")
        zip_file = "data_%s_%s.zip" % (datetime_str, host_name)
    assert root_dirs is not None
    if not os.path.isdir(os.path.dirname(zip_file)) and os.path.dirname(
        zip_file
    ):
        os.makedirs(os.path.dirname(zip_file), exist_ok=True)
    with zipfile.ZipFile(zip_file, "w") as expt_zip:
        for root_dir in root_dirs:
            if root_dir is None:
                continue
            for root, d, files in os.walk(root_dir):
                for file in files:
                    file_to_archive = os.path.join(root, file)
                    assert os.path.exists(file_to_archive)
                    path_to_archive = os.path.relpath(
                        file_to_archive, base_dir
                    )
                    expt_zip.write(
                        filename=file_to_archive, arcname=path_to_archive
                    )
    return zip_file, zip_file


def save_generate_output(
    prompt=None,
    output=None,
    base_model=None,
    save_dir=None,
    where_from="unknown where from",
    extra_dict={},
):
    try:
        return _save_generate_output(
            prompt=prompt,
            output=output,
            base_model=base_model,
            save_dir=save_dir,
            where_from=where_from,
            extra_dict=extra_dict,
        )
    except Exception as e:
        traceback.print_exc()
        print("Exception in saving: %s" % str(e))


def _save_generate_output(
    prompt=None,
    output=None,
    base_model=None,
    save_dir=None,
    where_from="unknown where from",
    extra_dict={},
):
    """
    Save conversation to .json, row by row.
    json_file_path is path to final JSON file. If not in ., then will attempt to make directories.
    Appends if file exists
    """
    prompt = "<not set>" if prompt is None else prompt
    output = "<not set>" if output is None else output
    assert save_dir, "save_dir must be provided"
    if os.path.exists(save_dir) and not os.path.isdir(save_dir):
        raise RuntimeError("save_dir already exists and is not a directory!")
    os.makedirs(save_dir, exist_ok=True)
    import json

    dict_to_save = dict(
        prompt=prompt,
        text=output,
        time=time.ctime(),
        base_model=base_model,
        where_from=where_from,
    )
    dict_to_save.update(extra_dict)
    with filelock.FileLock("save_dir.lock"):
        # lock logging in case have concurrency
        with open(os.path.join(save_dir, "history.json"), "a") as f:
            # just add [ at start, and ] at end, and have proper JSON dataset
            f.write("  " + json.dumps(dict_to_save) + ",\n")


def s3up(filename):
    try:
        return _s3up(filename)
    except Exception as e:
        traceback.print_exc()
        print("Exception for file %s in s3up: %s" % (filename, str(e)))
        return "Failed to upload %s: Error: %s" % (filename, str(e))


def _s3up(filename):
    import boto3

    aws_access_key_id = os.getenv("AWS_SERVER_PUBLIC_KEY")
    aws_secret_access_key = os.getenv("AWS_SERVER_SECRET_KEY")
    bucket = os.getenv("AWS_BUCKET")
    assert aws_access_key_id, "Set AWS key"
    assert aws_secret_access_key, "Set AWS secret"
    assert bucket, "Set AWS Bucket"

    s3 = boto3.client(
        "s3",
        aws_access_key_id=os.getenv("AWS_SERVER_PUBLIC_KEY"),
        aws_secret_access_key=os.getenv("AWS_SERVER_SECRET_KEY"),
    )
    ret = s3.upload_file(
        Filename=filename,
        Bucket=os.getenv("AWS_BUCKET"),
        Key=filename,
    )
    if ret in [None, ""]:
        return "Successfully uploaded %s" % filename


def get_githash():
    try:
        githash = subprocess.run(
            ["git", "rev-parse", "HEAD"], stdout=subprocess.PIPE
        ).stdout.decode("utf-8")[0:-1]
    except:
        githash = ""
    return githash


def copy_code(run_id):
    """
    copy code to track changes
    :param run_id:
    :return:
    """
    rnd_num = str(random.randint(0, 2**31))
    run_id = "run_" + str(run_id)
    os.makedirs(run_id, exist_ok=True)
    me_full = os.path.join(pathlib.Path(__file__).parent.resolve(), __file__)
    me_file = os.path.basename(__file__)
    new_me = os.path.join(run_id, me_file + "_" + get_githash())
    if os.path.isfile(new_me):
        new_me = os.path.join(
            run_id, me_file + "_" + get_githash() + "_" + rnd_num
        )
        shutil.copy(me_full, new_me)
    else:
        shutil.copy(me_full, new_me)


class NullContext(threading.local):
    """No-op context manager, executes block without doing any additional processing.

    Used as a stand-in if a particular block of code is only sometimes
    used with a normal context manager:
    """

    def __init__(self, *args, **kwargs):
        pass

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        self.finally_act()

    def finally_act(self):
        pass


def wrapped_partial(func, *args, **kwargs):
    """
    Give partial properties of normal function, like __name__ attribute etc.
    :param func:
    :param args:
    :param kwargs:
    :return:
    """
    partial_func = functools.partial(func, *args, **kwargs)
    functools.update_wrapper(partial_func, func)
    return partial_func


class ThreadException(Exception):
    pass


class EThread(threading.Thread):
    # Function that raises the custom exception
    def __init__(
        self,
        group=None,
        target=None,
        name=None,
        args=(),
        kwargs=None,
        *,
        daemon=None,
        streamer=None,
        bucket=None,
    ):
        self.bucket = bucket
        self.streamer = streamer
        self.exc = None
        self._return = None
        super().__init__(
            group=group,
            target=target,
            name=name,
            args=args,
            kwargs=kwargs,
            daemon=daemon,
        )

    def run(self):
        # Variable that stores the exception, if raised by someFunction
        try:
            if self._target is not None:
                self._return = self._target(*self._args, **self._kwargs)
        except BaseException as e:
            print("thread exception: %s" % str(sys.exc_info()))
            self.bucket.put(sys.exc_info())
            self.exc = e
            if self.streamer:
                print("make stop: %s" % str(sys.exc_info()), flush=True)
                self.streamer.do_stop = True
        finally:
            # Avoid a refcycle if the thread is running a function with
            # an argument that has a member that points to the thread.
            del self._target, self._args, self._kwargs

    def join(self, timeout=None):
        threading.Thread.join(self)
        # Since join() returns in caller thread
        # we re-raise the caught exception
        # if any was caught
        if self.exc:
            raise self.exc
        return self._return


def import_matplotlib():
    import matplotlib

    matplotlib.use("agg")
    # KEEP THESE HERE! START
    import matplotlib.pyplot as plt
    import pandas as pd

    # to avoid dlopen deadlock in fork
    import pandas.core.computation.expressions as pd_expressions
    import pandas._libs.groupby as pd_libgroupby
    import pandas._libs.reduction as pd_libreduction
    import pandas.core.algorithms as pd_algorithms
    import pandas.core.common as pd_com
    import numpy as np

    # KEEP THESE HERE! END


def get_sha(value):
    return hashlib.md5(str(value).encode("utf-8")).hexdigest()


def sanitize_filename(name):
    """
    Sanitize file *base* names.
    :param name: name to sanitize
    :return:
    """
    bad_chars = [
        "[",
        "]",
        ",",
        "/",
        "\\",
        "\\w",
        "\\s",
        "-",
        "+",
        '"',
        "'",
        ">",
        "<",
        " ",
        "=",
        ")",
        "(",
        ":",
        "^",
    ]
    for char in bad_chars:
        name = name.replace(char, "_")

    length = len(name)
    file_length_limit = 250  # bit smaller than 256 for safety
    sha_length = 32
    real_length_limit = file_length_limit - (sha_length + 2)
    if length > file_length_limit:
        sha = get_sha(name)
        half_real_length_limit = max(1, int(real_length_limit / 2))
        name = (
            name[0:half_real_length_limit]
            + "_"
            + sha
            + "_"
            + name[length - half_real_length_limit : length]
        )

    return name


def shutil_rmtree(*args, **kwargs):
    return shutil.rmtree(*args, **kwargs)


def remove(path: str):
    try:
        if path is not None and os.path.exists(path):
            if os.path.isdir(path):
                shutil_rmtree(path, ignore_errors=True)
            else:
                with contextlib.suppress(FileNotFoundError):
                    os.remove(path)
    except:
        pass


def makedirs(path, exist_ok=True):
    """
    Avoid some inefficiency in os.makedirs()
    :param path:
    :param exist_ok:
    :return:
    """
    if os.path.isdir(path) and os.path.exists(path):
        assert exist_ok, "Path already exists"
        return path
    os.makedirs(path, exist_ok=exist_ok)


def atomic_move_simple(src, dst):
    try:
        shutil.move(src, dst)
    except (shutil.Error, FileExistsError):
        pass
    remove(src)


def download_simple(url, dest=None, print_func=None):
    if print_func is not None:
        print_func("BEGIN get url %s" % str(url))
    if url.startswith("file://"):
        from requests_file import FileAdapter

        s = requests.Session()
        s.mount("file://", FileAdapter())
        url_data = s.get(url, stream=True)
    else:
        url_data = requests.get(url, stream=True)
    if dest is None:
        dest = os.path.basename(url)
    if url_data.status_code != requests.codes.ok:
        msg = "Cannot get url %s, code: %s, reason: %s" % (
            str(url),
            str(url_data.status_code),
            str(url_data.reason),
        )
        raise requests.exceptions.RequestException(msg)
    url_data.raw.decode_content = True
    makedirs(os.path.dirname(dest), exist_ok=True)
    uuid_tmp = str(uuid.uuid4())[:6]
    dest_tmp = dest + "_dl_" + uuid_tmp + ".tmp"
    with open(dest_tmp, "wb") as f:
        shutil.copyfileobj(url_data.raw, f)
    atomic_move_simple(dest_tmp, dest)
    if print_func is not None:
        print_func("END get url %s" % str(url))


def download(url, dest=None, dest_path=None):
    if dest_path is not None:
        dest = os.path.join(dest_path, os.path.basename(url))
        if os.path.isfile(dest):
            print("already downloaded %s -> %s" % (url, dest))
            return dest
    elif dest is not None:
        if os.path.exists(dest):
            print("already downloaded %s -> %s" % (url, dest))
            return dest
    else:
        uuid_tmp = "dl2_" + str(uuid.uuid4())[:6]
        dest = uuid_tmp + os.path.basename(url)

    print("downloading %s to %s" % (url, dest))

    if url.startswith("file://"):
        from requests_file import FileAdapter

        s = requests.Session()
        s.mount("file://", FileAdapter())
        url_data = s.get(url, stream=True)
    else:
        url_data = requests.get(url, stream=True)

    if url_data.status_code != requests.codes.ok:
        msg = "Cannot get url %s, code: %s, reason: %s" % (
            str(url),
            str(url_data.status_code),
            str(url_data.reason),
        )
        raise requests.exceptions.RequestException(msg)
    url_data.raw.decode_content = True
    dirname = os.path.dirname(dest)
    if dirname != "" and not os.path.isdir(dirname):
        makedirs(os.path.dirname(dest), exist_ok=True)
    uuid_tmp = "dl3_" + str(uuid.uuid4())[:6]
    dest_tmp = dest + "_" + uuid_tmp + ".tmp"
    with open(dest_tmp, "wb") as f:
        shutil.copyfileobj(url_data.raw, f)
    try:
        shutil.move(dest_tmp, dest)
    except FileExistsError:
        pass
    remove(dest_tmp)
    return dest


def get_url(x, from_str=False, short_name=False):
    if not from_str:
        source = x.metadata["source"]
    else:
        source = x
    if short_name:
        source_name = get_short_name(source)
    else:
        source_name = source
    if source.startswith("http://") or source.startswith("https://"):
        return (
            """<a href="%s" target="_blank"  rel="noopener noreferrer">%s</a>"""
            % (source, source_name)
        )
    else:
        return (
            """<a href="file/%s" target="_blank"  rel="noopener noreferrer">%s</a>"""
            % (source, source_name)
        )


def get_short_name(name, maxl=50):
    if name is None:
        return ""
    length = len(name)
    if length > maxl:
        allow_length = maxl - 3
        half_allowed = max(1, int(allow_length / 2))
        name = (
            name[0:half_allowed] + "..." + name[length - half_allowed : length]
        )
    return name


def cuda_vis_check(total_gpus):
    """Helper function to count GPUs by environment variable
    Stolen from Jon's h2o4gpu utils
    """
    cudavis = os.getenv("CUDA_VISIBLE_DEVICES")
    which_gpus = []
    if cudavis is not None:
        # prune away white-space, non-numerics,
        # except commas for simple checking
        cudavis = "".join(cudavis.split())
        import re

        cudavis = re.sub("[^0-9,]", "", cudavis)

        lencudavis = len(cudavis)
        if lencudavis == 0:
            total_gpus = 0
        else:
            total_gpus = min(
                total_gpus, os.getenv("CUDA_VISIBLE_DEVICES").count(",") + 1
            )
            which_gpus = os.getenv("CUDA_VISIBLE_DEVICES").split(",")
            which_gpus = [int(x) for x in which_gpus]
    else:
        which_gpus = list(range(0, total_gpus))

    return total_gpus, which_gpus


def get_ngpus_vis(raise_if_exception=True):
    ngpus_vis1 = 0

    shell = False
    if shell:
        cmd = "nvidia-smi -L 2> /dev/null"
    else:
        cmd = ["nvidia-smi", "-L"]

    try:
        timeout = 5 * 3
        o = subprocess.check_output(cmd, shell=shell, timeout=timeout)
        lines = o.decode("utf-8").splitlines()
        ngpus_vis1 = 0
        for line in lines:
            if "Failed to initialize NVML" not in line:
                ngpus_vis1 += 1
    except (FileNotFoundError, subprocess.CalledProcessError, OSError):
        # GPU systems might not have nvidia-smi, so can't fail
        pass
    except subprocess.TimeoutExpired as e:
        print("Failed get_ngpus_vis: %s" % str(e))
        if raise_if_exception:
            raise

    ngpus_vis1, which_gpus = cuda_vis_check(ngpus_vis1)
    return ngpus_vis1


def get_mem_gpus(raise_if_exception=True, ngpus=None):
    totalmem_gpus1 = 0
    usedmem_gpus1 = 0
    freemem_gpus1 = 0

    if ngpus == 0:
        return totalmem_gpus1, usedmem_gpus1, freemem_gpus1

    try:
        cmd = "nvidia-smi -q 2> /dev/null | grep -A 3 'FB Memory Usage'"
        o = subprocess.check_output(cmd, shell=True, timeout=15)
        lines = o.decode("utf-8").splitlines()
        for line in lines:
            if "Total" in line:
                totalmem_gpus1 += int(line.split()[2]) * 1024**2
            if "Used" in line:
                usedmem_gpus1 += int(line.split()[2]) * 1024**2
            if "Free" in line:
                freemem_gpus1 += int(line.split()[2]) * 1024**2
    except (FileNotFoundError, subprocess.CalledProcessError, OSError):
        # GPU systems might not have nvidia-smi, so can't fail
        pass
    except subprocess.TimeoutExpired as e:
        print("Failed get_mem_gpus: %s" % str(e))
        if raise_if_exception:
            raise

    return totalmem_gpus1, usedmem_gpus1, freemem_gpus1


class ForkContext(threading.local):
    """
    Set context for forking
    Ensures state is returned once done
    """

    def __init__(self, args=None, kwargs=None, forkdata_capable=True):
        """
        :param args:
        :param kwargs:
        :param forkdata_capable: whether fork is forkdata capable and will use copy-on-write forking of args/kwargs
        """
        self.forkdata_capable = forkdata_capable
        if self.forkdata_capable:
            self.has_args = args is not None
            self.has_kwargs = kwargs is not None
            forkdatacontext.args = args
            forkdatacontext.kwargs = kwargs
        else:
            self.has_args = False
            self.has_kwargs = False

    def __enter__(self):
        try:
            # flush all outputs so doesn't happen during fork -- don't print/log inside ForkContext contexts!
            sys.stdout.flush()
            sys.stderr.flush()
        except BaseException as e:
            # exit not called if exception, and don't want to leave forkdatacontext filled in that case
            print("ForkContext failure on enter: %s" % str(e))
            self.finally_act()
            raise
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        self.finally_act()

    def finally_act(self):
        """
            Done when exception hit or exit is reached in context
            first reset forkdatacontext as crucial to have reset even if later 2 calls fail
        :return: None
        """
        if self.forkdata_capable and (self.has_args or self.has_kwargs):
            forkdatacontext._reset()


class _ForkDataContext(threading.local):
    def __init__(
        self,
        args=None,
        kwargs=None,
    ):
        """
        Global context for fork to carry data to subprocess instead of relying upon copy/pickle/serialization

        :param args: args
        :param kwargs: kwargs
        """
        assert isinstance(args, (tuple, type(None)))
        assert isinstance(kwargs, (dict, type(None)))
        self.__args = args
        self.__kwargs = kwargs

    @property
    def args(self) -> Tuple:
        """returns args"""
        return self.__args

    @args.setter
    def args(self, args):
        if self.__args is not None:
            raise AttributeError(
                "args cannot be overwritten: %s %s"
                % (str(self.__args), str(self.__kwargs))
            )

        self.__args = args

    @property
    def kwargs(self) -> Dict:
        """returns kwargs"""
        return self.__kwargs

    @kwargs.setter
    def kwargs(self, kwargs):
        if self.__kwargs is not None:
            raise AttributeError(
                "kwargs cannot be overwritten: %s %s"
                % (str(self.__args), str(self.__kwargs))
            )

        self.__kwargs = kwargs

    def _reset(self):
        """Reset fork arg-kwarg context to default values"""
        self.__args = None
        self.__kwargs = None

    def get_args_kwargs(
        self, func, args, kwargs
    ) -> Tuple[Callable, Tuple, Dict]:
        if self.__args:
            args = self.__args[1:]
            if not func:
                assert (
                    len(self.__args) > 0
                ), "if have no func, must have in args"
                func = self.__args[0]  # should always be there
        if self.__kwargs:
            kwargs = self.__kwargs
        try:
            return func, args, kwargs
        finally:
            forkdatacontext._reset()

    @staticmethod
    def get_args_kwargs_for_traced_func(func, args, kwargs):
        """
        Return args/kwargs out of forkdatacontext when using copy-on-write way of passing args/kwargs
        :param func: actual function ran by _traced_func, which itself is directly what mppool treats as function
        :param args:
        :param kwargs:
        :return: func, args, kwargs from forkdatacontext if used, else originals
        """
        # first 3 lines are debug
        func_was_None = func is None
        args_was_None_or_empty = args is None or len(args) == 0
        kwargs_was_None_or_empty = kwargs is None or len(kwargs) == 0

        forkdatacontext_args_was_None = forkdatacontext.args is None
        forkdatacontext_kwargs_was_None = forkdatacontext.kwargs is None
        func, args, kwargs = forkdatacontext.get_args_kwargs(
            func, args, kwargs
        )
        using_forkdatacontext = (
            func_was_None and func is not None
        )  # pulled func out of forkdatacontext.__args[0]
        assert (
            forkdatacontext.args is None
        ), "forkdatacontext.args should be None after get_args_kwargs"
        assert (
            forkdatacontext.kwargs is None
        ), "forkdatacontext.kwargs should be None after get_args_kwargs"

        proc_type = kwargs.get("proc_type", "SUBPROCESS")
        if using_forkdatacontext:
            assert proc_type == "SUBPROCESS" or proc_type == "SUBPROCESS"
        if proc_type == "NORMAL":
            assert (
                forkdatacontext_args_was_None
            ), "if no fork, expect forkdatacontext.args None entering _traced_func"
            assert (
                forkdatacontext_kwargs_was_None
            ), "if no fork, expect forkdatacontext.kwargs None entering _traced_func"
        assert (
            func is not None
        ), "function should not be None, indicates original args[0] was None or args was None"

        return func, args, kwargs


forkdatacontext = _ForkDataContext()


def _traced_func(func, *args, **kwargs):
    func, args, kwargs = forkdatacontext.get_args_kwargs_for_traced_func(
        func, args, kwargs
    )
    return func(*args, **kwargs)


def call_subprocess_onetask(func, args=None, kwargs=None):
    if isinstance(args, list):
        args = tuple(args)
    if args is None:
        args = ()
    if kwargs is None:
        kwargs = {}
    args = list(args)
    args = [func] + args
    args = tuple(args)
    with ForkContext(args=args, kwargs=kwargs):
        args = (None,)
        kwargs = {}
        with ProcessPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_traced_func, *args, **kwargs)
            return future.result()


class ProgressParallel(Parallel):
    def __init__(self, use_tqdm=True, total=None, *args, **kwargs):
        self._use_tqdm = use_tqdm
        self._total = total
        super().__init__(*args, **kwargs)

    def __call__(self, *args, **kwargs):
        with tqdm(disable=not self._use_tqdm, total=self._total) as self._pbar:
            return Parallel.__call__(self, *args, **kwargs)

    def print_progress(self):
        if self._total is None:
            self._pbar.total = self.n_dispatched_tasks
        self._pbar.n = self.n_completed_tasks
        self._pbar.refresh()


def get_kwargs(func, exclude_names=None, **kwargs):
    func_names = list(inspect.signature(func).parameters)
    missing_kwargs = [x for x in func_names if x not in kwargs]
    if exclude_names:
        for k in exclude_names:
            if k in missing_kwargs:
                missing_kwargs.remove(k)
            if k in func_names:
                func_names.remove(k)
    assert not missing_kwargs, "Missing %s" % missing_kwargs
    kwargs = {k: v for k, v in kwargs.items() if k in func_names}
    return kwargs


import pkg_resources

have_faiss = False

try:
    assert pkg_resources.get_distribution("faiss") is not None
    have_faiss = True
except (pkg_resources.DistributionNotFound, AssertionError):
    pass
try:
    assert pkg_resources.get_distribution("faiss_gpu") is not None
    have_faiss = True
except (pkg_resources.DistributionNotFound, AssertionError):
    pass
try:
    assert pkg_resources.get_distribution("faiss_cpu") is not None
    have_faiss = True
except (pkg_resources.DistributionNotFound, AssertionError):
    pass


def hash_file(file):
    try:
        import hashlib

        # BUF_SIZE is totally arbitrary, change for your app!
        BUF_SIZE = 65536  # lets read stuff in 64kb chunks!

        md5 = hashlib.md5()
        # sha1 = hashlib.sha1()

        with open(file, "rb") as f:
            while True:
                data = f.read(BUF_SIZE)
                if not data:
                    break
                md5.update(data)
                # sha1.update(data)
    except BaseException as e:
        print("Cannot hash %s due to %s" % (file, str(e)))
        traceback.print_exc()
        md5 = None
    return md5.hexdigest()


def start_faulthandler():
    # If hit server or any subprocess with signal SIGUSR1, it'll print out all threads stack trace, but wont't quit or coredump
    # If more than one fork tries to write at same time, then looks corrupted.
    import faulthandler

    # SIGUSR1 in h2oai/__init__.py as well
    faulthandler.enable()
    if hasattr(faulthandler, "register"):
        # windows/mac
        import signal

        faulthandler.register(signal.SIGUSR1)


def get_hf_server(inference_server):
    inf_split = inference_server.split("    ")
    assert len(inf_split) == 1 or len(inf_split) == 3
    inference_server = inf_split[0]
    if len(inf_split) == 3:
        headers = {"authorization": "%s %s" % (inf_split[1], inf_split[2])}
    else:
        headers = None
    return inference_server, headers


class FakeTokenizer:
    """
    1) For keeping track of model_max_length
    2) For when model doesn't directly expose tokenizer but need to count tokens
    """

    def __init__(self, model_max_length=2048, encoding_name="cl100k_base"):
        # dont' push limit, since if using fake tokenizer, only estimate, and seen underestimates by order 250
        self.model_max_length = model_max_length - 250
        self.encoding_name = encoding_name
        # The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection.
        import tiktoken

        self.encoding = tiktoken.get_encoding(self.encoding_name)

    def encode(self, x, *args, return_tensors="pt", **kwargs):
        input_ids = self.encoding.encode(x, disallowed_special=())
        if return_tensors == "pt" and isinstance(input_ids, list):
            import torch

            input_ids = torch.tensor(input_ids)
        return dict(input_ids=input_ids)

    def decode(self, x, *args, **kwargs):
        # input is input_ids[0] form
        return self.encoding.decode(x)

    def num_tokens_from_string(self, prompt: str) -> int:
        """Returns the number of tokens in a text string."""
        num_tokens = len(self.encoding.encode(prompt))
        return num_tokens

    def __call__(self, x, *args, **kwargs):
        return self.encode(x, *args, **kwargs)

```

`apps/language_models/langchain/utils_langchain.py`:

```py
from typing import Any, Dict, List, Union, Optional
import time
import queue

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult


class StreamingGradioCallbackHandler(BaseCallbackHandler):
    """
    Similar to H2OTextIteratorStreamer that is for HF backend, but here LangChain backend
    """

    def __init__(self, timeout: Optional[float] = None, block=True):
        super().__init__()
        self.text_queue = queue.SimpleQueue()
        self.stop_signal = None
        self.do_stop = False
        self.timeout = timeout
        self.block = block

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when LLM starts running. Clean the queue."""
        while not self.text_queue.empty():
            try:
                self.text_queue.get(block=False)
            except queue.Empty:
                continue

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        self.text_queue.put(token)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when LLM ends running."""
        self.text_queue.put(self.stop_signal)

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when LLM errors."""
        self.text_queue.put(self.stop_signal)

    def __iter__(self):
        return self

    def __next__(self):
        while True:
            try:
                value = (
                    self.stop_signal
                )  # value looks unused in pycharm, not true
                if self.do_stop:
                    print("hit stop", flush=True)
                    # could raise or break, maybe best to raise and make parent see if any exception in thread
                    raise StopIteration()
                    # break
                value = self.text_queue.get(
                    block=self.block, timeout=self.timeout
                )
                break
            except queue.Empty:
                time.sleep(0.01)
        if value == self.stop_signal:
            raise StopIteration()
        else:
            return value

```

`apps/language_models/scripts/llama_ir_conversion_utils.py`:

```py
from pathlib import Path
import argparse
from argparse import RawTextHelpFormatter
import re, gc

"""
    This script can be used as a standalone utility to convert IRs to dynamic + combine them.
    Following are the various ways this script can be used :-
        a. To convert a single Linalg IR to dynamic IR:
            --dynamic --first_ir_path=<PATH TO FIRST IR>
        b. To convert two Linalg IRs to dynamic IR:
            --dynamic --first_ir_path=<PATH TO SECOND IR> --first_ir_path=<PATH TO SECOND IR>
        c. To combine two Linalg IRs into one:
            --combine --first_ir_path=<PATH TO FIRST IR> --second_ir_path=<PATH TO SECOND IR>
        d. To convert both IRs into dynamic as well as combine the IRs:
            --dynamic --combine --first_ir_path=<PATH TO FIRST IR> --second_ir_path=<PATH TO SECOND IR>

    NOTE: For dynamic you'll also need to provide the following set of flags:-
           i. For First Llama : --dynamic_input_size (DEFAULT: 19)
          ii. For Second Llama: --model_name (DEFAULT: llama2_7b)
                                --precision (DEFAULT: 'int4')
          You may use --save_dynamic to also save the dynamic IR in option d above.
          Else for option a. and b. the dynamic IR(s) will get saved by default.
"""


def combine_mlir_scripts(
    first_vicuna_mlir,
    second_vicuna_mlir,
    output_name,
    return_ir=True,
):
    print(f"[DEBUG] combining first and second mlir")
    print(f"[DEBUG] output_name = {output_name}")
    maps1 = []
    maps2 = []
    constants = set()
    f1 = []
    f2 = []

    print(f"[DEBUG] processing first vicuna mlir")
    first_vicuna_mlir = first_vicuna_mlir.splitlines()
    while first_vicuna_mlir:
        line = first_vicuna_mlir.pop(0)
        if re.search("#map\d*\s*=", line):
            maps1.append(line)
        elif re.search("arith.constant", line):
            constants.add(line)
        elif not re.search("module", line):
            line = re.sub("forward", "first_vicuna_forward", line)
            f1.append(line)
    f1 = f1[:-1]
    del first_vicuna_mlir
    gc.collect()

    for i, map_line in enumerate(maps1):
        map_var = map_line.split(" ")[0]
        map_line = re.sub(f"{map_var}(?!\d)", map_var + "_0", map_line)
        maps1[i] = map_line
        f1 = [
            re.sub(f"{map_var}(?!\d)", map_var + "_0", func_line)
            for func_line in f1
        ]

    print(f"[DEBUG] processing second vicuna mlir")
    second_vicuna_mlir = second_vicuna_mlir.splitlines()
    while second_vicuna_mlir:
        line = second_vicuna_mlir.pop(0)
        if re.search("#map\d*\s*=", line):
            maps2.append(line)
        elif "global_seed" in line:
            continue
        elif re.search("arith.constant", line):
            constants.add(line)
        elif not re.search("module", line):
            line = re.sub("forward", "second_vicuna_forward", line)
            f2.append(line)
    f2 = f2[:-1]
    del second_vicuna_mlir
    gc.collect()

    for i, map_line in enumerate(maps2):
        map_var = map_line.split(" ")[0]
        map_line = re.sub(f"{map_var}(?!\d)", map_var + "_1", map_line)
        maps2[i] = map_line
        f2 = [
            re.sub(f"{map_var}(?!\d)", map_var + "_1", func_line)
            for func_line in f2
        ]

    module_start = 'module attributes {torch.debug_module_name = "_lambda"} {'
    module_end = "}"

    global_vars = []
    vnames = []
    global_var_loading1 = []
    global_var_loading2 = []

    print(f"[DEBUG] processing constants")
    counter = 0
    constants = list(constants)
    while constants:
        constant = constants.pop(0)
        vname, vbody = constant.split("=")
        vname = re.sub("%", "", vname)
        vname = vname.strip()
        vbody = re.sub("arith.constant", "", vbody)
        vbody = vbody.strip()
        if len(vbody.split(":")) < 2:
            print(constant)
        vdtype = vbody.split(":")[-1].strip()
        fixed_vdtype = vdtype
        if "c1_i64" in vname:
            print(constant)
            counter += 1
        if counter == 2:
            counter = 0
            print("detected duplicate")
            continue
        vnames.append(vname)
        if "true" not in vname:
            global_vars.append(
                f"ml_program.global private @{vname}({vbody}) : {fixed_vdtype}"
            )
            global_var_loading1.append(
                f"\t\t%{vname} = ml_program.global_load_const @{vname} : {fixed_vdtype}"
            )
            global_var_loading2.append(
                f"\t\t%{vname} = ml_program.global_load_const @{vname} : {fixed_vdtype}"
            )
        else:
            global_vars.append(
                f"ml_program.global private @{vname}({vbody}) : i1"
            )
            global_var_loading1.append(
                f"\t\t%{vname} = ml_program.global_load_const @{vname} : i1"
            )
            global_var_loading2.append(
                f"\t\t%{vname} = ml_program.global_load_const @{vname} : i1"
            )

    new_f1, new_f2 = [], []

    print(f"[DEBUG] processing f1")
    for line in f1:
        if "func.func" in line:
            new_f1.append(line)
            for global_var in global_var_loading1:
                new_f1.append(global_var)
        else:
            new_f1.append(line)

    print(f"[DEBUG] processing f2")
    for line in f2:
        if "func.func" in line:
            new_f2.append(line)
            for global_var in global_var_loading2:
                if (
                    "c20_i64 = arith.addi %dim_i64, %c1_i64 : i64"
                    in global_var
                ):
                    print(global_var)
                new_f2.append(global_var)
        else:
            new_f2.append(line)

    f1 = new_f1
    f2 = new_f2

    del new_f1
    del new_f2
    gc.collect()

    print(
        [
            "c20_i64 = arith.addi %dim_i64, %c1_i64 : i64" in x
            for x in [maps1, maps2, global_vars, f1, f2]
        ]
    )

    # doing it this way rather than assembling the whole string
    # to prevent OOM with 64GiB RAM when encoding the file.

    print(f"[DEBUG] Saving mlir to {output_name}")
    with open(output_name, "w+") as f_:
        f_.writelines(line + "\n" for line in maps1)
        f_.writelines(line + "\n" for line in maps2)
        f_.writelines(line + "\n" for line in [module_start])
        f_.writelines(line + "\n" for line in global_vars)
        f_.writelines(line + "\n" for line in f1)
        f_.writelines(line + "\n" for line in f2)
        f_.writelines(line + "\n" for line in [module_end])

    del maps1
    del maps2
    del module_start
    del global_vars
    del f1
    del f2
    del module_end
    gc.collect()

    if return_ir:
        print(f"[DEBUG] Reading combined mlir back in")
        with open(output_name, "rb") as f:
            return f.read()


def write_in_dynamic_inputs0(module, dynamic_input_size):
    print("[DEBUG] writing dynamic inputs to first vicuna")
    # Current solution for ensuring mlir files support dynamic inputs
    # TODO: find a more elegant way to implement this
    new_lines = []
    module = module.splitlines()
    while module:
        line = module.pop(0)
        line = re.sub(f"{dynamic_input_size}x", "?x", line)
        if "?x" in line:
            line = re.sub("tensor.empty\(\)", "tensor.empty(%dim)", line)
        line = re.sub(f" {dynamic_input_size},", " %dim,", line)
        if "tensor.empty" in line and "?x?" in line:
            line = re.sub(
                "tensor.empty\(%dim\)", "tensor.empty(%dim, %dim)", line
            )
        if "arith.cmpi" in line:
            line = re.sub(f"c{dynamic_input_size}", "dim", line)
        if "%0 = tensor.empty(%dim) : tensor<?xi64>" in line:
            new_lines.append("%dim = tensor.dim %arg0, %c1 : tensor<1x?xi64>")
        if "%dim = tensor.dim %arg0, %c1 : tensor<1x?xi64>" in line:
            continue

        new_lines.append(line)
    return "\n".join(new_lines)


def write_in_dynamic_inputs1(module, model_name, precision):
    print("[DEBUG] writing dynamic inputs to second vicuna")

    def remove_constant_dim(line):
        if "c19_i64" in line:
            line = re.sub("c19_i64", "dim_i64", line)
        if "19x" in line:
            line = re.sub("19x", "?x", line)
            line = re.sub("tensor.empty\(\)", "tensor.empty(%dim)", line)
        if "tensor.empty" in line and "?x?" in line:
            line = re.sub(
                "tensor.empty\(%dim\)",
                "tensor.empty(%dim, %dim)",
                line,
            )
        if "arith.cmpi" in line:
            line = re.sub("c19", "dim", line)
        if " 19," in line:
            line = re.sub(" 19,", " %dim,", line)
        if "x20x" in line or "<20x" in line:
            line = re.sub("20x", "?x", line)
            line = re.sub("tensor.empty\(\)", "tensor.empty(%dimp1)", line)
        if " 20," in line:
            line = re.sub(" 20,", " %dimp1,", line)
        return line

    module = module.splitlines()
    new_lines = []

    # Using a while loop and the pop method to avoid creating a copy of module
    if "llama2_13b" in model_name:
        pkv_tensor_shape = "tensor<1x40x?x128x"
    elif "llama2_70b" in model_name:
        pkv_tensor_shape = "tensor<1x8x?x128x"
    else:
        pkv_tensor_shape = "tensor<1x32x?x128x"
    if precision in ["fp16", "int4", "int8"]:
        pkv_tensor_shape += "f16>"
    else:
        pkv_tensor_shape += "f32>"

    while module:
        line = module.pop(0)
        if "%c19_i64 = arith.constant 19 : i64" in line:
            new_lines.append("%c2 = arith.constant 2 : index")
            new_lines.append(
                f"%dim_4_int = tensor.dim %arg1, %c2 : {pkv_tensor_shape}"
            )
            new_lines.append(
                "%dim_i64 = arith.index_cast %dim_4_int : index to i64"
            )
            continue
        if "%c2 = arith.constant 2 : index" in line:
            continue
        if "%c20_i64 = arith.constant 20 : i64" in line:
            new_lines.append("%c1_i64 = arith.constant 1 : i64")
            new_lines.append("%c20_i64 = arith.addi %dim_i64, %c1_i64 : i64")
            new_lines.append(
                "%dimp1 = arith.index_cast %c20_i64 : i64 to index"
            )
            continue
        line = remove_constant_dim(line)
        new_lines.append(line)

    return "\n".join(new_lines)


def save_dynamic_ir(ir_to_save, output_file):
    if not ir_to_save:
        return
    # We only get string output from the dynamic conversion utility.
    from contextlib import redirect_stdout

    with open(output_file, "w") as f:
        with redirect_stdout(f):
            print(ir_to_save)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="llama ir utility",
        description="\tThis script can be used as a standalone utility to convert IRs to dynamic + combine them.\n"
        + "\tFollowing are the various ways this script can be used :-\n"
        + "\t\ta. To convert a single Linalg IR to dynamic IR:\n"
        + "\t\t\t--dynamic --first_ir_path=<PATH TO FIRST IR>\n"
        + "\t\tb. To convert two Linalg IRs to dynamic IR:\n"
        + "\t\t\t--dynamic --first_ir_path=<PATH TO SECOND IR> --first_ir_path=<PATH TO SECOND IR>\n"
        + "\t\tc. To combine two Linalg IRs into one:\n"
        + "\t\t\t--combine --first_ir_path=<PATH TO FIRST IR> --second_ir_path=<PATH TO SECOND IR>\n"
        + "\t\td. To convert both IRs into dynamic as well as combine the IRs:\n"
        + "\t\t\t--dynamic --combine --first_ir_path=<PATH TO FIRST IR> --second_ir_path=<PATH TO SECOND IR>\n\n"
        + "\tNOTE: For dynamic you'll also need to provide the following set of flags:-\n"
        + "\t\t i. For First Llama : --dynamic_input_size (DEFAULT: 19)\n"
        + "\t\tii. For Second Llama: --model_name (DEFAULT: llama2_7b)\n"
        + "\t\t\t--precision (DEFAULT: 'int4')\n"
        + "\t      You may use --save_dynamic to also save the dynamic IR in option d above.\n"
        + "\t      Else for option a. and b. the dynamic IR(s) will get saved by default.\n",
        formatter_class=RawTextHelpFormatter,
    )
    parser.add_argument(
        "--precision",
        "-p",
        default="int4",
        choices=["fp32", "fp16", "int8", "int4"],
        help="Precision of the concerned IR",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="llama2_7b",
        choices=["vicuna", "llama2_7b", "llama2_13b", "llama2_70b"],
        help="Specify which model to run.",
    )
    parser.add_argument(
        "--first_ir_path",
        default=None,
        help="path to first llama mlir file",
    )
    parser.add_argument(
        "--second_ir_path",
        default=None,
        help="path to second llama mlir file",
    )
    parser.add_argument(
        "--dynamic_input_size",
        type=int,
        default=19,
        help="Specify the static input size to replace with dynamic dim.",
    )
    parser.add_argument(
        "--dynamic",
        default=False,
        action=argparse.BooleanOptionalAction,
        help="Converts the IR(s) to dynamic",
    )
    parser.add_argument(
        "--save_dynamic",
        default=False,
        action=argparse.BooleanOptionalAction,
        help="Save the individual IR(s) after converting to dynamic",
    )
    parser.add_argument(
        "--combine",
        default=False,
        action=argparse.BooleanOptionalAction,
        help="Converts the IR(s) to dynamic",
    )

    args, unknown = parser.parse_known_args()

    dynamic = args.dynamic
    combine = args.combine
    assert (
        dynamic or combine
    ), "neither `dynamic` nor `combine` flag is turned on"
    first_ir_path = args.first_ir_path
    second_ir_path = args.second_ir_path
    assert first_ir_path or second_ir_path, "no input ir has been provided"
    if combine:
        assert (
            first_ir_path and second_ir_path
        ), "you will need to provide both IRs to combine"
    precision = args.precision
    model_name = args.model_name
    dynamic_input_size = args.dynamic_input_size
    save_dynamic = args.save_dynamic

    print(f"Dynamic conversion utility is turned {'ON' if dynamic else 'OFF'}")
    print(f"Combining IR utility is turned {'ON' if combine else 'OFF'}")

    if dynamic and not combine:
        save_dynamic = True

    first_ir = None
    first_dynamic_ir_name = None
    second_ir = None
    second_dynamic_ir_name = None
    if first_ir_path:
        first_dynamic_ir_name = f"{Path(first_ir_path).stem}_dynamic"
        with open(first_ir_path, "r") as f:
            first_ir = f.read()
    if second_ir_path:
        second_dynamic_ir_name = f"{Path(second_ir_path).stem}_dynamic"
        with open(second_ir_path, "r") as f:
            second_ir = f.read()
    if dynamic:
        first_ir = (
            write_in_dynamic_inputs0(first_ir, dynamic_input_size)
            if first_ir
            else None
        )
        second_ir = (
            write_in_dynamic_inputs1(second_ir, model_name, precision)
            if second_ir
            else None
        )
        if save_dynamic:
            save_dynamic_ir(first_ir, f"{first_dynamic_ir_name}.mlir")
            save_dynamic_ir(second_ir, f"{second_dynamic_ir_name}.mlir")

    if combine:
        combine_mlir_scripts(
            first_ir,
            second_ir,
            f"{model_name}_{precision}.mlir",
            return_ir=False,
        )

```

`apps/language_models/scripts/stablelm.py`:

```py
import torch
import torch_mlir
from transformers import (
    AutoTokenizer,
    StoppingCriteria,
)
from io import BytesIO
from pathlib import Path
from apps.language_models.utils import (
    get_torch_mlir_module_bytecode,
    get_vmfb_from_path,
)


class StopOnTokens(StoppingCriteria):
    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
    ) -> bool:
        stop_ids = [50278, 50279, 50277, 1, 0]
        for stop_id in stop_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False


def shouldStop(tokens):
    stop_ids = [50278, 50279, 50277, 1, 0]
    for stop_id in stop_ids:
        if tokens[0][-1] == stop_id:
            return True
    return False


MAX_SEQUENCE_LENGTH = 256


def user(message, history):
    # Append the user's message to the conversation history
    return "", history + [[message, ""]]


def compile_stableLM(
    model,
    model_inputs,
    model_name,
    model_vmfb_name,
    device="cuda",
    precision="fp32",
    debug=False,
):
    from shark.shark_inference import SharkInference

    # device = "cuda"  # "cpu"
    # TODO: vmfb and mlir name should include precision and device
    vmfb_path = (
        Path(model_name + f"_{device}.vmfb")
        if model_vmfb_name is None
        else Path(model_vmfb_name)
    )
    shark_module = get_vmfb_from_path(
        vmfb_path, device, mlir_dialect="tm_tensor"
    )
    if shark_module is not None:
        return shark_module

    mlir_path = Path(model_name + ".mlir")
    print(
        f"[DEBUG] mlir path {mlir_path} {'exists' if mlir_path.exists() else 'does not exist'}"
    )
    if mlir_path.exists():
        with open(mlir_path, "rb") as f:
            bytecode = f.read()
    else:
        ts_graph = get_torch_mlir_module_bytecode(model, model_inputs)
        module = torch_mlir.compile(
            ts_graph,
            [*model_inputs],
            torch_mlir.OutputType.LINALG_ON_TENSORS,
            use_tracing=False,
            verbose=False,
        )
        bytecode_stream = BytesIO()
        module.operation.write_bytecode(bytecode_stream)
        bytecode = bytecode_stream.getvalue()
    f_ = open(model_name + ".mlir", "wb")
    f_.write(bytecode)
    print("Saved mlir")
    f_.close()

    shark_module = SharkInference(
        mlir_module=bytecode, device=device, mlir_dialect="tm_tensor"
    )
    shark_module.compile()

    path = shark_module.save_module(
        vmfb_path.parent.absolute(), vmfb_path.stem, debug=debug
    )
    print("Saved vmfb at ", str(path))

    return shark_module


class StableLMModel(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask):
        combine_input_dict = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        }
        output = self.model(**combine_input_dict)
        return output.logits


# Initialize a StopOnTokens object
system_prompt = """<|SYSTEM|># StableLM Tuned (Alpha version)
- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.
- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.
- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.
- StableLM will refuse to participate in anything that could harm a human.
"""


def get_tokenizer():
    model_path = "stabilityai/stablelm-tuned-alpha-3b"
    tok = AutoTokenizer.from_pretrained(model_path)
    tok.add_special_tokens({"pad_token": "<PAD>"})
    print("Sucessfully loaded the tokenizer to the memory")
    return tok


# sharkStableLM = compile_stableLM
# (
#   None,
#   tuple([input_ids, attention_mask]),
#   "stableLM_linalg_f32_seqLen256",
#   "/home/shark/vivek/stableLM_shark_f32_seqLen256"
# )
def generate(
    new_text,
    max_new_tokens,
    sharkStableLM,
    tokenizer=None,
):
    if tokenizer is None:
        tokenizer = get_tokenizer()
    # Construct the input message string for the model by
    # concatenating the current system message and conversation history
    # Tokenize the messages string
    # sharkStableLM = compile_stableLM
    # (
    #   None,
    #   tuple([input_ids, attention_mask]),
    #   "stableLM_linalg_f32_seqLen256",
    #   "/home/shark/vivek/stableLM_shark_f32_seqLen256"
    # )
    words_list = []
    for i in range(max_new_tokens):
        # numWords = len(new_text.split())
        # if(numWords>220):
        #  break
        params = {
            "new_text": new_text,
        }
        generated_token_op = generate_new_token(
            sharkStableLM, tokenizer, params
        )
        detok = generated_token_op["detok"]
        stop_generation = generated_token_op["stop_generation"]
        if stop_generation:
            break
        print(detok, end="", flush=True)
        words_list.append(detok)
        if detok == "":
            break
        new_text = new_text + detok
    return words_list


def generate_new_token(shark_model, tokenizer, params):
    new_text = params["new_text"]
    model_inputs = tokenizer(
        [new_text],
        padding="max_length",
        max_length=MAX_SEQUENCE_LENGTH,
        truncation=True,
        return_tensors="pt",
    )
    sum_attentionmask = torch.sum(model_inputs.attention_mask)
    # sharkStableLM = compile_stableLM(None, tuple([input_ids, attention_mask]), "stableLM_linalg_f32_seqLen256", "/home/shark/vivek/stableLM_shark_f32_seqLen256")
    output = shark_model(
        "forward", [model_inputs.input_ids, model_inputs.attention_mask]
    )
    output = torch.from_numpy(output)
    next_toks = torch.topk(output, 1)
    stop_generation = False
    if shouldStop(next_toks.indices):
        stop_generation = True
    new_token = next_toks.indices[0][int(sum_attentionmask) - 1]
    detok = tokenizer.decode(
        new_token,
        skip_special_tokens=True,
    )
    ret_dict = {
        "new_token": new_token,
        "detok": detok,
        "stop_generation": stop_generation,
    }
    return ret_dict

```

`apps/language_models/scripts/vicuna.py`:

```py
import argparse
from dataclasses import dataclass
import json
import re
import gc
from io import BytesIO
from os import environ
from pathlib import Path
from statistics import mean, stdev
from tqdm import tqdm
from typing import List, Tuple
import subprocess
import sys
import time

import torch
import torch_mlir
from torch_mlir import TensorPlaceholder
from torch_mlir.compiler_utils import run_pipeline_with_repro_report
from transformers import AutoTokenizer, AutoModelForCausalLM

from apps.language_models.src.pipelines.SharkLLMBase import SharkLLMBase
from apps.language_models.src.model_wrappers.vicuna_sharded_model import (
    FirstVicunaLayer,
    SecondVicunaLayer,
    CompiledVicunaLayer,
    ShardedVicunaModel,
    LMHead,
    LMHeadCompiled,
    VicunaEmbedding,
    VicunaEmbeddingCompiled,
    VicunaNorm,
    VicunaNormCompiled,
)
from apps.language_models.src.model_wrappers.vicuna4 import (
    LlamaModel,
    EightLayerLayerSV,
    EightLayerLayerFV,
    CompiledEightLayerLayerSV,
    CompiledEightLayerLayer,
    forward_compressed,
)
from apps.language_models.src.model_wrappers.vicuna_model import (
    FirstVicuna,
    SecondVicuna7B,
    SecondVicuna13B,
    SecondVicuna70B,
)
from apps.language_models.src.model_wrappers.vicuna_model_gpu import (
    FirstVicunaGPU,
    SecondVicuna7BGPU,
    SecondVicuna13BGPU,
    SecondVicuna70BGPU,
)
from apps.language_models.utils import (
    get_vmfb_from_path,
)
from shark.shark_downloader import download_public_file
from shark.shark_importer import get_f16_inputs
from shark.shark_importer import import_with_fx, save_mlir
from shark.shark_inference import SharkInference


parser = argparse.ArgumentParser(
    prog="vicuna runner",
    description="runs a vicuna model",
)
parser.add_argument(
    "--precision", "-p", default="int8", help="fp32, fp16, int8, int4"
)
parser.add_argument("--device", "-d", default="cuda", help="vulkan, cpu, cuda")
parser.add_argument(
    "--vicuna_vmfb_path", default=None, help="path to vicuna vmfb"
)
parser.add_argument(
    "-s",
    "--sharded",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Run model as sharded",
)
# TODO: sharded config
parser.add_argument(
    "--vicuna_mlir_path",
    default=None,
    help="path to vicuna mlir file",
)
parser.add_argument(
    "--load_mlir_from_shark_tank",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="download precompile mlir from shark tank",
)
parser.add_argument(
    "--cli",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Run model in cli mode",
)
parser.add_argument(
    "--config",
    default=None,
    help="configuration file",
)
parser.add_argument(
    "--weight-group-size",
    type=int,
    default=128,
    help="Group size for per_group weight quantization. Default: 128.",
)
parser.add_argument(
    "--download_vmfb",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Download vmfb from sharktank, system dependent, YMMV",
)
parser.add_argument(
    "--model_name",
    type=str,
    default="vicuna",
    choices=["vicuna", "llama2_7b", "llama2_13b", "llama2_70b"],
    help="Specify which model to run.",
)
parser.add_argument(
    "--hf_auth_token",
    type=str,
    default=None,
    help="Specify your own huggingface authentication tokens for models like Llama2.",
)
parser.add_argument(
    "--cache_vicunas",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="For debugging purposes, creates a first_{precision}.mlir and second_{precision}.mlir and stores on disk",
)
parser.add_argument(
    "--iree_vulkan_target_triple",
    type=str,
    default="",
    help="Specify target triple for vulkan.",
)
parser.add_argument(
    "--Xiree_compile",
    action='append',
    default=[],
    help="Extra command line arguments passed to the IREE compiler. This can be specified multiple times to pass multiple arguments."
)
parser.add_argument(
    "--enable_tracing",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Enable profiling with Tracy. The script will wait for Tracy to connect and flush the profiling data after each token."
)

# Microbenchmarking options.
parser.add_argument(
    "--enable_microbenchmark",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Enables the microbenchmarking mode (non-interactive). Uses the system and the user prompt from args.",
)
parser.add_argument(
    "--microbenchmark_iterations",
    type=int,
    default=5,
    help="Number of microbenchmark iterations. Default: 5.",
)
parser.add_argument(
    "--microbenchmark_num_tokens",
    type=int,
    default=512,
    help="Generate an exact number of output tokens. Default: 512.",
)
parser.add_argument(
    "--system_prompt",
    type=str,
    default="",
    help="Specify the system prompt. This is only used with `--enable_microbenchmark`",
)
parser.add_argument(
    "--user_prompt",
    type=str,
    default="Hi",
    help="Specify the user prompt. This is only used with `--enable_microbenchmark`",
)

# fmt: off
def quant〇matmul_rhs_group_quant〡shape(lhs: List[int], rhs: List[int], rhs_scale: List[int], rhs_zero_point: List[int], rhs_bit_width: int, rhs_group_size: int) -> List[int]:
    if len(lhs) == 3 and len(rhs) == 2:
        return [lhs[0], lhs[1], rhs[0]]
    elif len(lhs) == 2 and len(rhs) == 2:
        return [lhs[0], rhs[0]]
    else:
        raise ValueError("Input shapes not supported.")


def quant〇matmul_rhs_group_quant〡dtype(lhs_rank_dtype: Tuple[int, int], rhs_rank_dtype: Tuple[int, int], rhs_scale_rank_dtype: Tuple[int, int], rhs_zero_point_rank_dtype: Tuple[int, int], rhs_bit_width: int, rhs_group_size: int) -> int:
    # output dtype is the dtype of the lhs float input
    lhs_rank, lhs_dtype = lhs_rank_dtype
    return lhs_dtype


def quant〇matmul_rhs_group_quant〡has_value_semantics(lhs, rhs, rhs_scale, rhs_zero_point, rhs_bit_width, rhs_group_size) -> None:
    return


brevitas_matmul_rhs_group_quant_library = [
    quant〇matmul_rhs_group_quant〡shape,
    quant〇matmul_rhs_group_quant〡dtype,
    quant〇matmul_rhs_group_quant〡has_value_semantics]
# fmt: on


class VicunaBase(SharkLLMBase):
    def __init__(
        self,
        model_name,
        hf_model_path="TheBloke/vicuna-7B-1.1-HF",
        max_num_tokens=512,
        device="cpu",
        precision="int8",
        extra_args_cmd=[],
    ) -> None:
        super().__init__(model_name, hf_model_path, max_num_tokens)
        self.max_sequence_length = 256
        self.device = device
        self.precision = precision
        self.extra_args = extra_args_cmd

    def get_tokenizer(self):
        # Retrieve the tokenizer from Huggingface
        tokenizer = AutoTokenizer.from_pretrained(
            self.hf_model_path, use_fast=False
        )
        return tokenizer

    def get_src_model(self):
        # Retrieve the torch model from Huggingface
        kwargs = {"torch_dtype": torch.float}
        vicuna_model = AutoModelForCausalLM.from_pretrained(
            self.hf_model_path, **kwargs
        )
        return vicuna_model

    def combine_mlir_scripts(
        self,
        first_vicuna_mlir,
        second_vicuna_mlir,
        output_name,
    ):
        print(f"[DEBUG] combining first and second mlir")
        print(f"[DEBUG] output_name = {output_name}")
        maps1 = []
        maps2 = []
        constants = set()
        f1 = []
        f2 = []

        print(f"[DEBUG] processing first vicuna mlir")
        first_vicuna_mlir = first_vicuna_mlir.splitlines()
        while first_vicuna_mlir:
            line = first_vicuna_mlir.pop(0)
            if re.search("#map\d*\s*=", line):
                maps1.append(line)
            elif re.search("arith.constant", line):
                constants.add(line)
            elif not re.search("module", line):
                line = re.sub("forward", "first_vicuna_forward", line)
                f1.append(line)
        f1 = f1[:-1]
        del first_vicuna_mlir
        gc.collect()

        for i, map_line in enumerate(maps1):
            map_var = map_line.split(" ")[0]
            map_line = re.sub(f"{map_var}(?!\d)", map_var + "_0", map_line)
            maps1[i] = map_line
            f1 = [
                re.sub(f"{map_var}(?!\d)", map_var + "_0", func_line)
                for func_line in f1
            ]

        print(f"[DEBUG] processing second vicuna mlir")
        second_vicuna_mlir = second_vicuna_mlir.splitlines()
        while second_vicuna_mlir:
            line = second_vicuna_mlir.pop(0)
            if re.search("#map\d*\s*=", line):
                maps2.append(line)
            elif "global_seed" in line:
                continue
            elif re.search("arith.constant", line):
                constants.add(line)
            elif not re.search("module", line):
                line = re.sub("forward", "second_vicuna_forward", line)
                f2.append(line)
        f2 = f2[:-1]
        del second_vicuna_mlir
        gc.collect()

        for i, map_line in enumerate(maps2):
            map_var = map_line.split(" ")[0]
            map_line = re.sub(f"{map_var}(?!\d)", map_var + "_1", map_line)
            maps2[i] = map_line
            f2 = [
                re.sub(f"{map_var}(?!\d)", map_var + "_1", func_line)
                for func_line in f2
            ]

        module_start = (
            'module attributes {torch.debug_module_name = "_lambda"} {'
        )
        module_end = "}"

        global_vars = []
        vnames = []
        global_var_loading1 = []
        global_var_loading2 = []

        print(f"[DEBUG] processing constants")
        counter = 0
        constants = list(constants)
        while constants:
            constant = constants.pop(0)
            vname, vbody = constant.split("=")
            vname = re.sub("%", "", vname)
            vname = vname.strip()
            vbody = re.sub("arith.constant", "", vbody)
            vbody = vbody.strip()
            if len(vbody.split(":")) < 2:
                print(constant)
            vdtype = vbody.split(":")[-1].strip()
            fixed_vdtype = vdtype
            noinline = "{noinline}" if "tensor" in fixed_vdtype else ""
            if "c1_i64" in vname:
                print(constant)
                counter += 1
            if counter == 2:
                counter = 0
                print("detected duplicate")
                continue
            vnames.append(vname)
            if "true" not in vname:
                global_vars.append(
                    f"ml_program.global private @{vname}({vbody}) : {fixed_vdtype}"
                )
                global_var_loading1.append(
                    f"\t\t%{vname} = ml_program.global_load_const @{vname} : {fixed_vdtype}"
                )
                global_var_loading2.append(
                    f"\t\t%{vname} = ml_program.global_load_const @{vname} : {fixed_vdtype}"
                )
            else:
                global_vars.append(
                    f"ml_program.global private @{vname}({vbody}) : i1"
                )
                global_var_loading1.append(
                    f"\t\t%{vname} = ml_program.global_load_const @{vname} : i1"
                )
                global_var_loading2.append(
                    f"\t\t%{vname} = ml_program.global_load_const @{vname} : i1"
                )

        new_f1, new_f2 = [], []

        print(f"[DEBUG] processing f1")
        for line in f1:
            if "func.func" in line:
                new_f1.append(line)
                for global_var in global_var_loading1:
                    new_f1.append(global_var)
            else:
                new_f1.append(line)

        print(f"[DEBUG] processing f2")
        for line in f2:
            if "func.func" in line:
                new_f2.append(line)
                for global_var in global_var_loading2:
                    if (
                        "c20_i64 = arith.addi %dim_i64, %c1_i64 : i64"
                        in global_var
                    ):
                        print(global_var)
                    new_f2.append(global_var)
            else:
                new_f2.append(line)

        f1 = new_f1
        f2 = new_f2

        del new_f1
        del new_f2
        gc.collect()

        print(
            [
                "c20_i64 = arith.addi %dim_i64, %c1_i64 : i64" in x
                for x in [maps1, maps2, global_vars, f1, f2]
            ]
        )

        # doing it this way rather than assembling the whole string
        # to prevent OOM with 64GiB RAM when encoding the file.

        print(f"[DEBUG] Saving mlir to {output_name}")
        with open(output_name, "w+") as f_:
            f_.writelines(line + "\n" for line in maps1)
            f_.writelines(line + "\n" for line in maps2)
            f_.writelines(line + "\n" for line in [module_start])
            f_.writelines(line + "\n" for line in global_vars)
            f_.writelines(line + "\n" for line in f1)
            f_.writelines(line + "\n" for line in f2)
            f_.writelines(line + "\n" for line in [module_end])

        del maps1
        del maps2
        del module_start
        del global_vars
        del f1
        del f2
        del module_end
        gc.collect()

        print(f"[DEBUG] Reading combined mlir back in")
        with open(output_name, "rb") as f:
            return f.read()

    def generate_new_token(self, params, sharded=True, cli=True):
        is_first = params["is_first"]
        if is_first:
            prompt = params["prompt"]
            input_ids = self.tokenizer(prompt).input_ids
            input_id_len = len(input_ids)
            input_ids = torch.tensor(input_ids)
            input_ids = input_ids.reshape([1, input_id_len])
            if sharded:
                output = self.shark_model.forward(input_ids, is_first=is_first)
            else:
                output = self.shark_model("first_vicuna_forward", (input_ids,), send_to_host=False)

        else:
            token = params["token"]
            past_key_values = params["past_key_values"]
            input_ids = [token]
            input_id_len = len(input_ids)
            input_ids = torch.tensor(input_ids)
            input_ids = input_ids.reshape([1, input_id_len])
            if sharded:
                output = self.shark_model.forward(
                    input_ids,
                    past_key_values=past_key_values,
                    is_first=is_first,
                )
            else:
                token = torch.tensor(token).reshape([1, 1])
                second_input = (token,) + tuple(past_key_values)
                output = self.shark_model(
                    "second_vicuna_forward", second_input, send_to_host=False
                )

        if sharded:
            _logits = output["logits"]
            _past_key_values = output["past_key_values"]
            _token = int(torch.argmax(_logits[:, -1, :], dim=1)[0])
        elif "cpu" in self.device:
            _past_key_values = output[1:]
            _token = int(output[0].to_host())
        else:
            _logits = torch.tensor(output[0].to_host())
            _past_key_values = output[1:]
            _token = torch.argmax(_logits[:, -1, :], dim=1)

        _detok = self.tokenizer.decode(_token, skip_special_tokens=False)
        ret_dict = {
            "token": _token,
            "detok": _detok,
            "past_key_values": _past_key_values,
        }
        if "cpu" not in self.device:
            ret_dict["logits"] = _logits

        if cli:
            print(f" token : {_token} | detok : {_detok}")

        return ret_dict


class ShardedVicuna(VicunaBase):
    # Class representing Sharded Vicuna Model
    def __init__(
        self,
        model_name,
        hf_model_path="TheBloke/vicuna-7B-1.1-HF",
        max_num_tokens=512,
        device="cuda",
        precision="fp32",
        config_json=None,
        weight_group_size=128,
        compressed=False,
        extra_args_cmd=[],
        debug=False,
    ) -> None:
        super().__init__(
            model_name,
            hf_model_path,
            max_num_tokens,
            extra_args_cmd=extra_args_cmd,
        )
        self.max_sequence_length = 256
        self.device = device
        self.precision = precision
        self.debug = debug
        self.tokenizer = self.get_tokenizer()
        self.config = config_json
        self.weight_group_size = weight_group_size
        self.compressed = compressed
        self.shark_model = self.compile(device=device)

    def get_tokenizer(self):
        kwargs = {}
        if self.model_name == "llama2":
            kwargs = {
                "use_auth_token": "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"
            }
        tokenizer = AutoTokenizer.from_pretrained(
            self.hf_model_path,
            use_fast=False,
            **kwargs,
        )
        return tokenizer

    def get_src_model(self):
        # Retrieve the torch model from Huggingface
        kwargs = {"torch_dtype": torch.float}
        if self.model_name == "llama2":
            kwargs["use_auth_token"] = "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"
        vicuna_model = AutoModelForCausalLM.from_pretrained(
            self.hf_model_path,
            **kwargs,
        )
        return vicuna_model

    def write_in_dynamic_inputs0(self, module, dynamic_input_size):
        # Current solution for ensuring mlir files support dynamic inputs
        # TODO find a more elegant way to implement this
        new_lines = []
        for line in module.splitlines():
            line = re.sub(f"{dynamic_input_size}x", "?x", line)
            if "?x" in line:
                line = re.sub("tensor.empty\(\)", "tensor.empty(%dim)", line)
            line = re.sub(f" {dynamic_input_size},", " %dim,", line)
            if "tensor.empty" in line and "?x?" in line:
                line = re.sub(
                    "tensor.empty\(%dim\)", "tensor.empty(%dim, %dim)", line
                )
            if "arith.cmpi" in line:
                line = re.sub(f"c{dynamic_input_size}", "dim", line)
            new_lines.append(line)
        new_module = "\n".join(new_lines)
        return new_module

    def write_in_dynamic_inputs1(self, module, dynamic_input_size):
        new_lines = []
        for line in module.splitlines():
            if "dim_42 =" in line:
                continue
            if f"%c{dynamic_input_size}_i64 =" in line:
                new_lines.append(
                    "%dim_42 = tensor.dim %arg1, %c3 : tensor<1x1x1x?xf32>"
                )
                new_lines.append(
                    f"%dim_42_i64 = arith.index_cast %dim_42 : index to i64"
                )
                continue
            line = re.sub(f"{dynamic_input_size}x", "?x", line)
            line = re.sub(f"%c{dynamic_input_size}_i64", "%dim_42_i64", line)
            if "?x" in line:
                line = re.sub(
                    "tensor.empty\(\)", "tensor.empty(%dim_42)", line
                )
            line = re.sub(f" {dynamic_input_size},", " %dim_42,", line)
            if "tensor.empty" in line and "?x?" in line:
                line = re.sub(
                    "tensor.empty\(%dim_42\)",
                    "tensor.empty(%dim_42, %dim_42)",
                    line,
                )
            if "arith.cmpi" in line:
                line = re.sub(f"c{dynamic_input_size}", "dim_42", line)
            new_lines.append(line)
        new_module = "\n".join(new_lines)
        return new_module

    def compile_vicuna_layer(
        self,
        vicuna_layer,
        hidden_states,
        attention_mask,
        position_ids,
        past_key_value0=None,
        past_key_value1=None,
    ):
        # Compile a hidden decoder layer of vicuna
        if past_key_value0 is None and past_key_value1 is None:
            model_inputs = (hidden_states, attention_mask, position_ids)
        else:
            model_inputs = (
                hidden_states,
                attention_mask,
                position_ids,
                past_key_value0,
                past_key_value1,
            )
        mlir_bytecode = import_with_fx(
            vicuna_layer,
            model_inputs,
            precision=self.precision,
            f16_input_mask=[False, False],
            mlir_type="torchscript",
        )
        return mlir_bytecode

    def compile_vicuna_layer4(
        self,
        vicuna_layer,
        hidden_states,
        attention_mask,
        position_ids,
        past_key_values=None,
    ):
        # Compile a hidden decoder layer of vicuna
        if past_key_values is None:
            model_inputs = (hidden_states, attention_mask, position_ids)
        else:
            (
                (pkv00, pkv01),
                (pkv10, pkv11),
                (pkv20, pkv21),
                (pkv30, pkv31),
                (pkv40, pkv41),
                (pkv50, pkv51),
                (pkv60, pkv61),
                (pkv70, pkv71),
            ) = past_key_values

            model_inputs = (
                hidden_states,
                attention_mask,
                position_ids,
                pkv00,
                pkv01,
                pkv10,
                pkv11,
                pkv20,
                pkv21,
                pkv30,
                pkv31,
                pkv40,
                pkv41,
                pkv50,
                pkv51,
                pkv60,
                pkv61,
                pkv70,
                pkv71,
            )
        mlir_bytecode = import_with_fx(
            vicuna_layer,
            model_inputs,
            precision=self.precision,
            f16_input_mask=[False, False],
            mlir_type="torchscript",
        )
        return mlir_bytecode

    def get_device_index(self, layer_string):
        # Get the device index from the config file
        # In the event that different device indices are assigned to
        # different parts of a layer, a majority vote will be taken and
        # everything will be run on the most commonly used device
        if self.config is None:
            return None
        idx_votes = {}
        for key in self.config.keys():
            if re.search(layer_string, key):
                if int(self.config[key]["gpu"]) in idx_votes.keys():
                    idx_votes[int(self.config[key]["gpu"])] += 1
                else:
                    idx_votes[int(self.config[key]["gpu"])] = 1
        device_idx = max(idx_votes, key=idx_votes.get)
        return device_idx

    def compile_lmhead(
        self, lmh, hidden_states, device="cpu", device_idx=None,
    ):
        # compile the lm head of the vicuna model
        # This can be used for both first and second vicuna, so only needs to be run once
        mlir_path = Path(f"lmhead.mlir")
        vmfb_path = Path(f"lmhead.vmfb")
        if mlir_path.exists():
            print(f"Found bytecode module at {mlir_path}.")
        else:
            hidden_states = torch_mlir.TensorPlaceholder.like(
                hidden_states, dynamic_axes=[1]
            )

            # module = torch_mlir.compile(
            #    lmh,
            #    (hidden_states,),
            #    torch_mlir.OutputType.LINALG_ON_TENSORS,
            #    use_tracing=False,
            #    verbose=False,
            # )
            # bytecode_stream = BytesIO()
            # module.operation.write_bytecode(bytecode_stream)
            # bytecode = bytecode_stream.getvalue()
            # f_ = open(mlir_path, "wb")
            # f_.write(bytecode)
            # f_.close()
            filepath = Path("lmhead.mlir")
            download_public_file(
                "gs://shark_tank/elias/compressed_sv/lmhead.mlir",
                filepath.absolute(),
                single_file=True,
            )
            mlir_path = filepath

        shark_module = SharkInference(
            mlir_path,
            device=device,
            mlir_dialect="tm_tensor",
            device_idx=device_idx,
            mmap=False,
        )
        if vmfb_path.exists():
            shark_module.load_module(vmfb_path)
        else:
            shark_module.save_module(module_name="lmhead", debug=self.debug)
            shark_module.load_module(vmfb_path)
        compiled_module = LMHeadCompiled(shark_module)
        return compiled_module

    def compile_norm(self, fvn, hidden_states, device="cpu", device_idx=None):
        # compile the normalization layer of the vicuna model
        # This can be used for both first and second vicuna, so only needs to be run once
        mlir_path = Path(f"norm.mlir")
        vmfb_path = Path(f"norm.vmfb")
        if mlir_path.exists():
            print(f"Found bytecode module at {mlir_path}.")
        else:
            hidden_states = torch_mlir.TensorPlaceholder.like(
                hidden_states, dynamic_axes=[1]
            )

            # module = torch_mlir.compile(
            #    fvn,
            #    (hidden_states,),
            #    torch_mlir.OutputType.LINALG_ON_TENSORS,
            #    use_tracing=False,
            #    verbose=False,
            # )
            filepath = Path("norm.mlir")
            download_public_file(
                "gs://shark_tank/elias/compressed_sv/norm.mlir",
                filepath.absolute(),
                single_file=True,
            )
            mlir_path = filepath

        shark_module = SharkInference(
            mlir_path,
            device=device,
            mlir_dialect="tm_tensor",
            device_idx=device_idx,
            mmap=False,
        )
        if vmfb_path.exists():
            shark_module.load_module(vmfb_path)
        else:
            shark_module.save_module(module_name="norm", debug=self.debug)
            shark_module.load_module(vmfb_path)
        compiled_module = VicunaNormCompiled(shark_module)
        return compiled_module

    def compile_embedding(self, fve, input_ids, device="cpu", device_idx=None):
        # compile the embedding layer of the vicuna model
        # This can be used for both first and second vicuna, so only needs to be run once
        mlir_path = Path(f"embedding.mlir")
        vmfb_path = Path(f"embedding.vmfb")
        if mlir_path.exists():
            print(f"Found bytecode module at {mlir_path}.")
        else:
            input_ids = torch_mlir.TensorPlaceholder.like(
                input_ids, dynamic_axes=[1]
            )
            # module = torch_mlir.compile(
            #    fve,
            #    (input_ids,),
            #    torch_mlir.OutputType.LINALG_ON_TENSORS,
            #    use_tracing=False,
            #    verbose=False,
            # )
            # bytecode_stream = BytesIO()
            # module.operation.write_bytecode(bytecode_stream)
            # bytecode = bytecode_stream.getvalue()
            # f_ = open(mlir_path, "wb")
            # f_.write(bytecode)
            # f_.close()
            filepath = Path("embedding.mlir")
            download_public_file(
                "gs://shark_tank/elias/compressed_sv/embedding.mlir",
                filepath.absolute(),
                single_file=True,
            )
            mlir_path = filepath

        shark_module = SharkInference(
            mlir_path,
            device=device,
            mlir_dialect="tm_tensor",
            device_idx=device_idx,
            mmap=False,
        )
        if vmfb_path.exists():
            shark_module.load_module(vmfb_path)
        else:
            shark_module.save_module(module_name="embedding", debug=self.debug)
            shark_module.load_module(vmfb_path)
        compiled_module = VicunaEmbeddingCompiled(shark_module)

        return compiled_module

    def compile_to_vmfb_one_model(
        self, inputs0, layers0, inputs1, layers1, device="cpu",
    ):
        mlirs, modules = [], []
        assert len(layers0) == len(layers1)
        for layer0, layer1, idx in zip(layers0, layers1, range(len(layers0))):
            mlir_path = Path(f"{idx}_full.mlir")
            vmfb_path = Path(f"{idx}_full.vmfb")
            # if vmfb_path.exists():
            #    continue
            if mlir_path.exists():
                f_ = open(mlir_path, "rb")
                bytecode = f_.read()
                f_.close()
                mlirs.append(bytecode)
            else:
                hidden_states_placeholder0 = TensorPlaceholder.like(
                    inputs0[0], dynamic_axes=[1]
                )
                attention_mask_placeholder0 = TensorPlaceholder.like(
                    inputs0[1], dynamic_axes=[3]
                )
                position_ids_placeholder0 = TensorPlaceholder.like(
                    inputs0[2], dynamic_axes=[1]
                )
                hidden_states_placeholder1 = TensorPlaceholder.like(
                    inputs1[0], dynamic_axes=[1]
                )
                attention_mask_placeholder1 = TensorPlaceholder.like(
                    inputs1[1], dynamic_axes=[3]
                )
                position_ids_placeholder1 = TensorPlaceholder.like(
                    inputs1[2], dynamic_axes=[1]
                )
                pkv0_placeholder = TensorPlaceholder.like(
                    inputs1[3], dynamic_axes=[2]
                )
                pkv1_placeholder = TensorPlaceholder.like(
                    inputs1[4], dynamic_axes=[2]
                )

                print(f"Compiling layer {idx} mlir")
                ts_g = self.compile_vicuna_layer(
                    layer0, inputs0[0], inputs0[1], inputs0[2]
                )
                if self.precision in ["int4", "int8"]:
                    from brevitas_examples.common.generative.quantize import quantize_model
                    from brevitas_examples.llm.llm_quant.run_utils import get_model_impl
                    module0 = torch_mlir.compile(
                        ts_g,
                        (
                            hidden_states_placeholder0,
                            inputs0[1],
                            inputs0[2],
                        ),
                        output_type="torch",
                        backend_legal_ops=["quant.matmul_rhs_group_quant"],
                        extra_library=brevitas_matmul_rhs_group_quant_library,
                        use_tracing=False,
                        verbose=False,
                    )
                    print(f"[DEBUG] converting torch to linalg")
                    run_pipeline_with_repro_report(
                        module0,
                        "builtin.module(func.func(torch-unpack-quant-tensor),func.func(torch-convert-custom-quant-op),torch-backend-to-linalg-on-tensors-backend-pipeline)",
                        description="Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR",
                    )
                else:
                    module0 = torch_mlir.compile(
                        ts_g,
                        (
                            hidden_states_placeholder0,
                            inputs0[1],
                            inputs0[2],
                        ),
                        torch_mlir.OutputType.LINALG_ON_TENSORS,
                        use_tracing=False,
                        verbose=False,
                    )
                module0 = self.write_in_dynamic_inputs0(str(module0), 137)

                ts_g = self.compile_vicuna_layer(
                    layer1,
                    inputs1[0],
                    inputs1[1],
                    inputs1[2],
                    inputs1[3],
                    inputs1[4],
                )
                if self.precision in ["int4", "int8"]:
                    module1 = torch_mlir.compile(
                        ts_g,
                        (
                            inputs1[0],
                            attention_mask_placeholder1,
                            inputs1[2],
                            pkv0_placeholder,
                            pkv1_placeholder,
                        ),
                        output_type="torch",
                        backend_legal_ops=["quant.matmul_rhs_group_quant"],
                        extra_library=brevitas_matmul_rhs_group_quant_library,
                        use_tracing=False,
                        verbose=False,
                    )
                    print(f"[DEBUG] converting torch to linalg")
                    run_pipeline_with_repro_report(
                        module1,
                        "builtin.module(func.func(torch-unpack-quant-tensor),func.func(torch-convert-custom-quant-op),torch-backend-to-linalg-on-tensors-backend-pipeline)",
                        description="Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR",
                    )
                else:
                    module1 = torch_mlir.compile(
                        ts_g,
                        (
                            inputs1[0],
                            attention_mask_placeholder1,
                            inputs1[2],
                            pkv0_placeholder,
                            pkv1_placeholder,
                        ),
                        torch_mlir.OutputType.LINALG_ON_TENSORS,
                        use_tracing=False,
                        verbose=False,
                    )
                module1 = self.write_in_dynamic_inputs1(str(module1), 138)

                module_combined = self.combine_mlir_scripts(
                    module0, module1, f"{idx}_full.mlir"
                )
                mlirs.append(module_combined)

            if vmfb_path.exists():
                device_idx = self.get_device_index(
                    f"first_vicuna.model.model.layers.{idx}[\s.$]"
                )
                module = SharkInference(
                    None,
                    device=device,
                    device_idx=device_idx,
                    mlir_dialect="tm_tensor",
                    mmap=False,
                )
                module.load_module(vmfb_path)
            else:
                print(f"Compiling layer {idx} vmfb")
                device_idx = self.get_device_index(
                    f"first_vicuna.model.model.layers.{idx}[\s.$]"
                )
                module = SharkInference(
                    mlirs[idx],
                    device=device,
                    device_idx=device_idx,
                    mlir_dialect="tm_tensor",
                    mmap=False,
                )
                module.save_module(
                    module_name=f"{idx}_full",
                    extra_args=[
                        "--iree-vm-target-truncate-unsupported-floats",
                        "--iree-codegen-check-ir-before-llvm-conversion=false",
                        "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                    ]
                    + self.extra_args,
                    debug=self.debug,
                )
                module.load_module(vmfb_path)
            modules.append(module)
        return mlirs, modules

    def compile_to_vmfb_one_model4(
        self, inputs0, layers0, inputs1, layers1, device="cpu"
    ):
        mlirs, modules = [], []
        assert len(layers0) == len(layers1)
        for layer0, layer1, idx in zip(layers0, layers1, range(len(layers0))):
            mlir_path = Path(f"{idx}_full.mlir")
            vmfb_path = Path(f"{idx}_full.vmfb")
            # if vmfb_path.exists():
            #    continue
            if mlir_path.exists():
                f_ = open(mlir_path, "rb")
                bytecode = f_.read()
                f_.close()
                mlirs.append(bytecode)
            else:
                filepath = Path(f"{idx}_full.mlir")
                download_public_file(
                    f"gs://shark_tank/elias/compressed_sv/{idx}_full.mlir",
                    filepath.absolute(),
                    single_file=True,
                )

                f_ = open(f"{idx}_full.mlir", "rb")
                bytecode = f_.read()
                f_.close()
                mlirs.append(bytecode)

            if vmfb_path.exists():
                device_idx = self.get_device_index(
                    f"first_vicuna.model.model.layers.{idx}[\s.$]"
                )
                module = SharkInference(
                    None,
                    device=device,
                    device_idx=0,
                    mlir_dialect="tm_tensor",
                    mmap=True,
                )
                module.load_module(vmfb_path)
            else:
                print(f"Compiling layer {idx} vmfb")
                device_idx = self.get_device_index(
                    f"first_vicuna.model.model.layers.{idx}[\s.$]"
                )
                module = SharkInference(
                    mlirs[idx],
                    device=device,
                    device_idx=0,
                    mlir_dialect="tm_tensor",
                    mmap=True,
                )
                module.save_module(
                    module_name=f"{idx}_full",
                    extra_args=[
                        "--iree-vm-target-truncate-unsupported-floats",
                        "--iree-codegen-check-ir-before-llvm-conversion=false",
                        "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                    ]
                    + self.extra_args,
                    debug=self.debug,
                )
                module.load_module(vmfb_path)
            modules.append(module)
        return mlirs, modules

    def get_sharded_model(self, device="cpu", compressed=False):
        # SAMPLE_INPUT_LEN is used for creating mlir with dynamic inputs, which is currently an increadibly hacky proccess
        # please don't change it
        SAMPLE_INPUT_LEN = 137
        vicuna_model = self.get_src_model()
        if compressed:
            vicuna_model.model = LlamaModel.from_pretrained(
                "TheBloke/vicuna-7B-1.1-HF"
            )

        if self.precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import quantize_model
            from brevitas_examples.llm.llm_quant.run_utils import get_model_impl
            print("Applying weight quantization..")
            weight_bit_width = 4 if self.precision == "int4" else 8
            quantize_model(
                get_model_impl(vicuna_model).layers,
                dtype=torch.float32,
                weight_quant_type="asym",
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_granularity="per_group",
                weight_group_size=self.weight_group_size,
                quantize_weight_zero_point=False,
                input_bit_width=None,
                input_scale_type="float",
                input_param_method="stats",
                input_quant_type="asym",
                input_quant_granularity="per_tensor",
                quantize_input_zero_point=False,
                seqlen=2048,
            )
            print("Weight quantization applied.")

        placeholder_pkv_segment = tuple(
            (
                torch.zeros([1, 32, SAMPLE_INPUT_LEN, 128]),
                torch.zeros([1, 32, SAMPLE_INPUT_LEN, 128]),
            )
            for _ in range(8)
        )
        placeholder_pkv_full = tuple(
            (
                torch.zeros([1, 32, SAMPLE_INPUT_LEN, 128]),
                torch.zeros([1, 32, SAMPLE_INPUT_LEN, 128]),
            )
            for _ in range(32)
        )

        placeholder_input0 = (
            torch.zeros([1, SAMPLE_INPUT_LEN, 4096]),
            torch.zeros([1, 1, SAMPLE_INPUT_LEN, SAMPLE_INPUT_LEN]),
            torch.zeros([1, SAMPLE_INPUT_LEN], dtype=torch.int64),
        )

        placeholder_input1 = (
            torch.zeros([1, 1, 4096]),
            torch.zeros([1, 1, 1, SAMPLE_INPUT_LEN + 1]),
            torch.zeros([1, 1], dtype=torch.int64),
            torch.zeros([1, 32, SAMPLE_INPUT_LEN, 128]),
            torch.zeros([1, 32, SAMPLE_INPUT_LEN, 128]),
        )

        norm = VicunaNorm(vicuna_model.model.norm)
        device_idx = self.get_device_index(
            r"vicuna\.model\.model\.norm(?:\.|\s|$)"
        )
        print(device_idx)
        norm = self.compile_norm(
            norm,
            torch.zeros([1, SAMPLE_INPUT_LEN, 4096]),
            device=self.device,
            device_idx=device_idx,
        )

        embeddings = VicunaEmbedding(vicuna_model.model.embed_tokens)
        device_idx = self.get_device_index(
            r"vicuna\.model\.model\.embed_tokens(?:\.|\s|$)"
        )
        print(device_idx)
        embeddings = self.compile_embedding(
            embeddings,
            (torch.zeros([1, SAMPLE_INPUT_LEN], dtype=torch.int64)),
            device=self.device,
            device_idx=device_idx,
        )

        lmhead = LMHead(vicuna_model.lm_head)
        device_idx = self.get_device_index(
            r"vicuna\.model\.lm_head(?:\.|\s|$)"
        )
        print(device_idx)
        lmhead = self.compile_lmhead(
            lmhead,
            torch.zeros([1, SAMPLE_INPUT_LEN, 4096]),
            device=self.device,
            device_idx=device_idx,
        )

        if not compressed:
            layers0 = [
                FirstVicunaLayer(layer) for layer in vicuna_model.model.layers
            ]
            layers1 = [
                SecondVicunaLayer(layer) for layer in vicuna_model.model.layers
            ]

        else:
            layers00 = EightLayerLayerFV(vicuna_model.model.layers[0:8])
            layers01 = EightLayerLayerFV(vicuna_model.model.layers[8:16])
            layers02 = EightLayerLayerFV(vicuna_model.model.layers[16:24])
            layers03 = EightLayerLayerFV(vicuna_model.model.layers[24:32])
            layers10 = EightLayerLayerSV(vicuna_model.model.layers[0:8])
            layers11 = EightLayerLayerSV(vicuna_model.model.layers[8:16])
            layers12 = EightLayerLayerSV(vicuna_model.model.layers[16:24])
            layers13 = EightLayerLayerSV(vicuna_model.model.layers[24:32])
            layers0 = [layers00, layers01, layers02, layers03]
            layers1 = [layers10, layers11, layers12, layers13]

        _, modules = self.compile_to_vmfb_one_model4(
            placeholder_input0,
            layers0,
            placeholder_input1,
            layers1,
            device=device,
        )

        if not compressed:
            shark_layers = [CompiledVicunaLayer(m) for m in modules]
        else:
            shark_layers = [CompiledEightLayerLayer(m) for m in modules]
            vicuna_model.model.compressedlayers = shark_layers

        sharded_model = ShardedVicunaModel(
            vicuna_model,
            shark_layers,
            lmhead,
            embeddings,
            norm,
        )
        return sharded_model

    def compile(self, device="cpu"):
        return self.get_sharded_model(
            device=device, compressed=self.compressed
        )
        return self.get_sharded_model(
            device=device, compressed=self.compressed
        )

    def generate(self, prompt, cli=False):
        # TODO: refactor for cleaner integration

        history = []

        tokens_generated = []
        _past_key_values = None
        _token = None
        detoks_generated = []
        for iteration in range(self.max_num_tokens):
            params = {
                "prompt": prompt,
                "is_first": iteration == 0,
                "token": _token,
                "past_key_values": _past_key_values,
            }

            generated_token_op = self.generate_new_token(params=params)

            _token = generated_token_op["token"]
            _past_key_values = generated_token_op["past_key_values"]
            _detok = generated_token_op["detok"]
            history.append(_token)
            yield self.tokenizer.decode(history)

            if _token == 2:
                break
            detoks_generated.append(_detok)
            tokens_generated.append(_token)

        for i in range(len(tokens_generated)):
            if type(tokens_generated[i]) != int:
                tokens_generated[i] = int(tokens_generated[i][0])
        result_output = self.tokenizer.decode(tokens_generated)
        yield result_output

    def autocomplete(self, prompt):
        # use First vic alone to complete a story / prompt / sentence.
        pass


class UnshardedVicuna(VicunaBase):
    def __init__(
        self,
        model_name,
        hf_model_path="TheBloke/vicuna-7B-1.1-HF",
        hf_auth_token: str = None,
        max_num_tokens=512,
        min_num_tokens=0,
        device="cpu",
        device_id=None,
        vulkan_target_triple="",
        precision="int8",
        vicuna_mlir_path=None,
        vicuna_vmfb_path=None,
        load_mlir_from_shark_tank=False,
        low_device_memory=False,
        weight_group_size=128,
        download_vmfb=False,
        cache_vicunas=False,
        extra_args_cmd=[],
        debug=False,
    ) -> None:
        super().__init__(
            model_name,
            hf_model_path,
            max_num_tokens,
            extra_args_cmd=extra_args_cmd,
        )
        self.hf_auth_token = hf_auth_token
        if self.model_name == "llama2_7b":
            self.hf_model_path = "meta-llama/Llama-2-7b-chat-hf"
        elif self.model_name == "llama2_13b":
            self.hf_model_path = "meta-llama/Llama-2-13b-chat-hf"
        elif self.model_name == "llama2_70b":
            self.hf_model_path = "meta-llama/Llama-2-70b-chat-hf"
        print(f"[DEBUG] hf model name: {self.hf_model_path}")
        self.max_sequence_length = 256
        self.min_num_tokens = min_num_tokens
        self.vulkan_target_triple = vulkan_target_triple
        self.precision = precision
        self.download_vmfb = download_vmfb
        self.vicuna_vmfb_path = vicuna_vmfb_path
        self.vicuna_mlir_path = vicuna_mlir_path
        self.load_mlir_from_shark_tank = load_mlir_from_shark_tank
        self.low_device_memory = low_device_memory
        self.weight_group_size = weight_group_size
        self.debug = debug
        # Sanity check for device, device_id pair
        if "://" in device:
            if device_id is not None:
                print("[ERR] can't have both full device path and a device id.\n"
                      f"Device : {device} | device_id : {device_id}\n"
                      "proceeding with given Device ignoring device_id")
            self.device, self.device_id = device.split("://")
            if len(self.device_id) < 2:
                self.device_id = int(self.device_id)
        else:
            self.device, self.device_id = device, device_id
        if self.vicuna_mlir_path == None:
            self.vicuna_mlir_path = self.get_model_path()
        if self.vicuna_vmfb_path == None:
            self.vicuna_vmfb_path = self.get_model_path(suffix="vmfb")
        self.tokenizer = self.get_tokenizer()
        self.cache_vicunas = cache_vicunas

        self.compile()

    def get_model_path(self, suffix="mlir"):
        safe_device = self.device.split("-")[0]
        safe_device = safe_device.split("://")[0]
        if suffix in ["mlirbc", "mlir"]:
            return Path(f"{self.model_name}_{self.precision}.{suffix}")

        # Need to distinguish between multiple vmfbs of the same model
        # compiled for different devices of the same driver
        # Driver  -  Differentiator
        # Vulkan  -  target_triple
        # ROCm    -  device_arch

        differentiator = ""
        if "vulkan" == self.device:
            target_triple = ""
            if self.vulkan_target_triple != "":
                target_triple = "_"
                target_triple += "_".join(self.vulkan_target_triple.split("-")[:-1])
                differentiator = target_triple

        elif "rocm" == self.device:
            from shark.iree_utils.gpu_utils import get_rocm_device_arch
            device_arch = get_rocm_device_arch(self.device_id if self.device_id is not None else 0, self.extra_args)
            differentiator = '_' + device_arch

        return Path(
            f"{self.model_name}_{self.precision}_{safe_device}{differentiator}.{suffix}"
        )

    def get_tokenizer(self):
        local_tokenizer_path = Path(Path.cwd(), "llama2_tokenizer_configs")
        local_tokenizer_path.mkdir(parents=True, exist_ok=True)
        tokenizer_files_to_download = [
            "config.json",
            "special_tokens_map.json",
            "tokenizer.model",
            "tokenizer_config.json",
        ]
        for tokenizer_file in tokenizer_files_to_download:
            download_public_file(
                f"gs://shark_tank/llama2_tokenizer/{tokenizer_file}",
                Path(local_tokenizer_path, tokenizer_file),
                single_file=True,
            )
        tokenizer = AutoTokenizer.from_pretrained(str(local_tokenizer_path))
        return tokenizer

    def get_src_model(self):
        kwargs = {
            "torch_dtype": torch.float,
            "use_auth_token": self.hf_auth_token,
        }
        vicuna_model = AutoModelForCausalLM.from_pretrained(
            self.hf_model_path,
            **kwargs,
        )
        return vicuna_model

    def write_in_dynamic_inputs0(self, module, dynamic_input_size):
        print("[DEBUG] writing dynamic inputs to first vicuna")
        # Current solution for ensuring mlir files support dynamic inputs
        # TODO: find a more elegant way to implement this
        new_lines = []
        module = module.splitlines()
        while module:
            line = module.pop(0)
            line = re.sub(f"{dynamic_input_size}x", "?x", line)
            if "?x" in line:
                line = re.sub("tensor.empty\(\)", "tensor.empty(%dim)", line)
            line = re.sub(f" {dynamic_input_size},", " %dim,", line)
            if "tensor.empty" in line and "?x?" in line:
                line = re.sub(
                    "tensor.empty\(%dim\)", "tensor.empty(%dim, %dim)", line
                )
            if "arith.cmpi" in line:
                line = re.sub(f"c{dynamic_input_size}", "dim", line)
            if "%0 = tensor.empty(%dim) : tensor<?xi64>" in line:
                new_lines.append(
                    "%dim = tensor.dim %arg0, %c1 : tensor<1x?xi64>"
                )
            if "%dim = tensor.dim %arg0, %c1 : tensor<1x?xi64>" in line:
                continue

            new_lines.append(line)
        return "\n".join(new_lines)

    def write_in_dynamic_inputs1(self, module):
        print("[DEBUG] writing dynamic inputs to second vicuna")

        def remove_constant_dim(line):
            if "c19_i64" in line:
                line = re.sub("c19_i64", "dim_i64", line)
            if "19x" in line:
                line = re.sub("19x", "?x", line)
                line = re.sub("tensor.empty\(\)", "tensor.empty(%dim)", line)
            if "tensor.empty" in line and "?x?" in line:
                line = re.sub(
                    "tensor.empty\(%dim\)",
                    "tensor.empty(%dim, %dim)",
                    line,
                )
            if "arith.cmpi" in line:
                line = re.sub("c19", "dim", line)
            if " 19," in line:
                line = re.sub(" 19,", " %dim,", line)
            if "x20x" in line or "<20x" in line:
                line = re.sub("20x", "?x", line)
                line = re.sub("tensor.empty\(\)", "tensor.empty(%dimp1)", line)
            if " 20," in line:
                line = re.sub(" 20,", " %dimp1,", line)
            return line

        module = module.splitlines()
        new_lines = []

        # Using a while loop and the pop method to avoid creating a copy of module
        if "llama2_13b" in self.model_name:
            pkv_tensor_shape = "tensor<1x40x?x128x"
        elif "llama2_70b" in self.model_name:
            pkv_tensor_shape = "tensor<1x8x?x128x"
        else:
            pkv_tensor_shape = "tensor<1x32x?x128x"
        if self.precision in ["fp16", "int4", "int8"]:
            pkv_tensor_shape += "f16>"
        else:
            pkv_tensor_shape += "f32>"

        while module:
            line = module.pop(0)
            if "%c19_i64 = arith.constant 19 : i64" in line:
                new_lines.append("%c2 = arith.constant 2 : index")
                new_lines.append(
                    f"%dim_4_int = tensor.dim %arg1, %c2 : {pkv_tensor_shape}"
                )
                new_lines.append(
                    "%dim_i64 = arith.index_cast %dim_4_int : index to i64"
                )
                continue
            if "%c2 = arith.constant 2 : index" in line:
                continue
            if "%c20_i64 = arith.constant 20 : i64" in line:
                new_lines.append("%c1_i64 = arith.constant 1 : i64")
                new_lines.append(
                    "%c20_i64 = arith.addi %dim_i64, %c1_i64 : i64"
                )
                new_lines.append(
                    "%dimp1 = arith.index_cast %c20_i64 : i64 to index"
                )
                continue
            line = remove_constant_dim(line)
            new_lines.append(line)

        return "\n".join(new_lines)

    def compile(self):
        # Testing : DO NOT Download Vmfbs if not found. Modify later
        # download vmfbs for A100
        if not self.vicuna_vmfb_path.exists() and self.download_vmfb:
            print(
                f"Looking into gs://shark_tank/{self.model_name}/unsharded/vmfb/{self.vicuna_vmfb_path.name}"
            )
            download_public_file(
                f"gs://shark_tank/{self.model_name}/unsharded/vmfb/{self.vicuna_vmfb_path.name}",
                self.vicuna_vmfb_path.absolute(),
                single_file=True,
            )
        self.shark_model = get_vmfb_from_path(
            self.vicuna_vmfb_path, self.device, "tm_tensor", self.device_id
        )
        if self.shark_model is not None:
            print(f"[DEBUG] vmfb found at {self.vicuna_vmfb_path.absolute()}")
            return

        print(f"[DEBUG] vmfb not found (search path: {self.vicuna_vmfb_path})")
        mlir_generated = False
        for suffix in ["mlirbc", "mlir"]:
            self.vicuna_mlir_path = self.get_model_path(suffix)
            if "cpu" in self.device and "llama2_7b" in self.vicuna_mlir_path.name:
                self.vicuna_mlir_path = Path("llama2_7b_int4_f32.mlir")
            if not self.vicuna_mlir_path.exists() and self.load_mlir_from_shark_tank:
                print(
                    f"Looking into gs://shark_tank/{self.model_name}/unsharded/mlir/{self.vicuna_mlir_path.name}"
                )
                download_public_file(
                    f"gs://shark_tank/{self.model_name}/unsharded/mlir/{self.vicuna_mlir_path.name}",
                    self.vicuna_mlir_path.absolute(),
                    single_file=True,
                )
            if self.vicuna_mlir_path.exists():
                print(f"[DEBUG] mlir found at {self.vicuna_mlir_path.absolute()}")
                combined_module = self.vicuna_mlir_path.absolute()
                mlir_generated = True
                break

        if not mlir_generated:
            print(f"[DEBUG] mlir not found")

            print("[DEBUG] generating mlir on device")
            # Select a compilation prompt such that the resulting input_ids
            # from the model's tokenizer has shape [1, 19]
            compilation_prompt = "".join(["0" for _ in range(17)])

            first_model_path = f"first_{self.model_name}_{self.precision}.mlir"
            if Path(first_model_path).exists():
                print(f"loading {first_model_path}")
                with open(Path(first_model_path), "r") as f:
                    first_module = f.read()
            else:
                # generate first vicuna
                compilation_input_ids = self.tokenizer(
                    compilation_prompt,
                    return_tensors="pt",
                ).input_ids
                compilation_input_ids = torch.tensor(
                    compilation_input_ids
                ).reshape([1, 19])
                firstVicunaCompileInput = (compilation_input_ids,)
                if "cpu" in self.device:
                    model = FirstVicuna(
                        self.hf_model_path,
                        self.precision,
                        "fp32" if self.device=="cpu" else "fp16",
                        self.weight_group_size,
                        self.model_name,
                        self.hf_auth_token,
                    )
                else:
                    model = FirstVicunaGPU(
                        self.hf_model_path,
                        self.precision,
                        "fp32" if self.device=="cpu" else "fp16",
                        self.weight_group_size,
                        self.model_name,
                        self.hf_auth_token,
                    )
                print(f"[DEBUG] generating torchscript graph")
                is_f16 = self.precision in ["fp16", "int4"]
                ts_graph = import_with_fx(
                    model,
                    firstVicunaCompileInput,
                    is_f16=is_f16,
                    precision=self.precision,
                    f16_input_mask=[False, False],
                    mlir_type="torchscript",
                )
                del model
                firstVicunaCompileInput = list(firstVicunaCompileInput)
                firstVicunaCompileInput[
                    0
                ] = torch_mlir.TensorPlaceholder.like(
                    firstVicunaCompileInput[0], dynamic_axes=[1]
                )

                firstVicunaCompileInput = tuple(firstVicunaCompileInput)
                first_module = None
                print(f"[DEBUG] generating torch mlir")
                if self.precision in ["int4", "int8"]:
                    first_module = torch_mlir.compile(
                        ts_graph,
                        [*firstVicunaCompileInput],
                        output_type=torch_mlir.OutputType.TORCH,
                        backend_legal_ops=["quant.matmul_rhs_group_quant"],
                        extra_library=brevitas_matmul_rhs_group_quant_library,
                        use_tracing=False,
                        verbose=False,
                    )
                    if self.cache_vicunas:
                        with open(first_model_path[:-5]+"_torch.mlir", "w+") as f:
                            f.write(str(first_module))
                    print(f"[DEBUG] converting torch to linalg")
                    run_pipeline_with_repro_report(
                        first_module,
                        "builtin.module(func.func(torch-unpack-quant-tensor),func.func(torch-convert-custom-quant-op),torch-backend-to-linalg-on-tensors-backend-pipeline)",
                        description="Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR",
                    )
                else:
                    first_module = torch_mlir.compile(
                        ts_graph,
                        [*firstVicunaCompileInput],
                        torch_mlir.OutputType.LINALG_ON_TENSORS,
                        use_tracing=False,
                        verbose=False,
                    )
                del ts_graph
                del firstVicunaCompileInput
                gc.collect()

                print(
                    "[DEBUG] successfully generated first vicuna linalg mlir"
                )
                first_module = self.write_in_dynamic_inputs0(
                    str(first_module), dynamic_input_size=19
                )
                if self.cache_vicunas:
                    with open(first_model_path, "w+") as f:
                        f.write(first_module)
                    print("Finished writing IR after dynamic")

            print(f"[DEBUG] Starting generation of second llama")
            second_model_path = f"second_{self.model_name}_{self.precision}.mlir"
            if Path(second_model_path).exists():
                print(f"loading {second_model_path}")
                with open(Path(second_model_path), "r") as f:
                    second_module = f.read()
            else:
                # generate second vicuna
                compilation_input_ids = torch.zeros(
                    [1, 1], dtype=torch.int64
                )
                if self.model_name == "llama2_13b":
                    dim1 = 40
                    total_tuple = 80
                elif self.model_name == "llama2_70b":
                    dim1 = 8
                    total_tuple = 160
                else:
                    dim1 = 32
                    total_tuple = 64
                pkv = tuple(
                    (torch.zeros([1, dim1, 19, 128], dtype=torch.float32))
                    for _ in range(total_tuple)
                )
                secondVicunaCompileInput = (compilation_input_ids,) + pkv
                if "cpu" in self.device:
                    if self.model_name == "llama2_13b":
                        model = SecondVicuna13B(
                            self.hf_model_path,
                            self.precision,
                            "fp32",
                            self.weight_group_size,
                            self.model_name,
                            self.hf_auth_token,
                        )
                    elif self.model_name == "llama2_70b":
                        model = SecondVicuna70B(
                            self.hf_model_path,
                            self.precision,
                            "fp32",
                            self.weight_group_size,
                            self.model_name,
                            self.hf_auth_token,
                        )
                    else:
                        model = SecondVicuna7B(
                            self.hf_model_path,
                            self.precision,
                            "fp32",
                            self.weight_group_size,
                            self.model_name,
                            self.hf_auth_token,
                        )
                else:
                    if self.model_name == "llama2_13b":
                        model = SecondVicuna13BGPU(
                            self.hf_model_path,
                            self.precision,
                            "fp16",
                            self.weight_group_size,
                            self.model_name,
                            self.hf_auth_token,
                        )
                    elif self.model_name == "llama2_70b":
                        model = SecondVicuna70BGPU(
                            self.hf_model_path,
                            self.precision,
                            "fp16",
                            self.weight_group_size,
                            self.model_name,
                            self.hf_auth_token,
                        )
                    else:
                        model = SecondVicuna7BGPU(
                            self.hf_model_path,
                            self.precision,
                            "fp16",
                            self.weight_group_size,
                            self.model_name,
                            self.hf_auth_token,
                        )
                print(f"[DEBUG] generating torchscript graph")
                is_f16 = self.precision in ["fp16", "int4"]
                ts_graph = import_with_fx(
                    model,
                    secondVicunaCompileInput,
                    is_f16=is_f16,
                    precision=self.precision,
                    f16_input_mask=[False] + [True] * total_tuple,
                    mlir_type="torchscript",
                )
                del model
                if self.precision in ["fp16", "int4"]:
                    secondVicunaCompileInput = get_f16_inputs(
                        secondVicunaCompileInput,
                        True,
                        f16_input_mask=[False] + [True] * total_tuple,
                    )
                secondVicunaCompileInput = list(secondVicunaCompileInput)
                for i in range(len(secondVicunaCompileInput)):
                    if i != 0:
                        secondVicunaCompileInput[i] = torch_mlir.TensorPlaceholder.like(
                            secondVicunaCompileInput[i], dynamic_axes=[2]
                        )
                secondVicunaCompileInput = tuple(secondVicunaCompileInput)
                print(f"[DEBUG] generating torch mlir")
                if self.precision in ["int4", "int8"]:
                    second_module = torch_mlir.compile(
                        ts_graph,
                        [*secondVicunaCompileInput],
                        output_type=torch_mlir.OutputType.TORCH,
                        backend_legal_ops=["quant.matmul_rhs_group_quant"],
                        extra_library=brevitas_matmul_rhs_group_quant_library,
                        use_tracing=False,
                        verbose=False,
                    )
                    print(f"[DEBUG] converting torch to linalg")
                    if self.cache_vicunas:
                        with open(second_model_path[:-5]+"_torch.mlir", "w+") as f:
                            f.write(str(second_module))
                    run_pipeline_with_repro_report(
                        second_module,
                        "builtin.module(func.func(torch-unpack-quant-tensor),func.func(torch-convert-custom-quant-op),torch-backend-to-linalg-on-tensors-backend-pipeline)",
                        description="Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR",
                    )
                else:
                    second_module = torch_mlir.compile(
                        ts_graph,
                        [*secondVicunaCompileInput],
                        torch_mlir.OutputType.LINALG_ON_TENSORS,
                        use_tracing=False,
                        verbose=False,
                    )
                del ts_graph
                del secondVicunaCompileInput
                gc.collect()

                print(
                    "[DEBUG] successfully generated second vicuna linalg mlir"
                )
                second_module = self.write_in_dynamic_inputs1(
                    str(second_module)
                )
                if self.cache_vicunas:
                    with open(second_model_path, "w+") as f:
                        f.write(second_module)
                    print("Finished writing IR after dynamic")

            combined_module = self.combine_mlir_scripts(
                first_module,
                second_module,
                self.vicuna_mlir_path,
            )
            combined_module = save_mlir(
                combined_module,
                model_name="combined_llama",
                mlir_dialect="tm_tensor",
                dir=self.vicuna_mlir_path,
            )
            del first_module, second_module

        print(f"Compiling for device : {self.device}"
              f"{'://' + str(self.device_id) if self.device_id is not None else ''}")
        shark_module = SharkInference(
            mlir_module=combined_module,
            device=self.device,
            mlir_dialect="tm_tensor",
            device_idx=self.device_id
        )
        path = shark_module.save_module(
            self.vicuna_vmfb_path.parent.absolute(),
            self.vicuna_vmfb_path.stem,
            extra_args=[
                "--iree-vm-target-truncate-unsupported-floats",
                "--iree-codegen-check-ir-before-llvm-conversion=false",
                "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
            ]
            + self.extra_args,
            debug=self.debug,
        )
        print("Saved vic vmfb at ", str(path))
        shark_module.load_module(path)
        self.shark_model = shark_module

    def decode_tokens(self, res_tokens):
        for i in range(len(res_tokens)):
            if type(res_tokens[i]) != int:
                res_tokens[i] = int(res_tokens[i][0])

        res_str = self.tokenizer.decode(
            res_tokens, skip_special_tokens=False
        )
        return res_str

    def generate(self, prompt, cli):
        # TODO: refactor for cleaner integration
        if self.shark_model is None:
            self.compile()
        res_tokens = []
        params = {"prompt": prompt, "is_first": True, "fv": self.shark_model}

        prefill_st_time = time.time()
        generated_token_op = self.generate_new_token(
            params=params, sharded=False, cli=cli
        )
        prefill_time = time.time() - prefill_st_time

        token = generated_token_op["token"]
        if "cpu" not in self.device:
            logits = generated_token_op["logits"]
        pkv = generated_token_op["past_key_values"]
        detok = generated_token_op["detok"]
        yield detok, None, prefill_time

        res_tokens.append(token)
        if cli:
            print(f"Assistant: {detok}", end=" ", flush=True)

        for idx in range(self.max_num_tokens):
            params = {
                "token": token,
                "is_first": False,
                "past_key_values": pkv,
                "sv": self.shark_model,
            }
            if "cpu" not in self.device:
                params["logits"] = logits

            decode_st_time = time.time()
            generated_token_op = self.generate_new_token(
                params=params, sharded=False, cli=cli
            )
            decode_time_ms = (time.time() - decode_st_time)*1000

            token = generated_token_op["token"]
            if "cpu" not in self.device:
                logits = generated_token_op["logits"]
            pkv = generated_token_op["past_key_values"]
            detok = generated_token_op["detok"]

            if token == 2 and idx >= self.min_num_tokens:
                break
            res_tokens.append(token)
            if detok == "<0x0A>":
                if cli:
                    print("\n", end="", flush=True)
            else:
                if cli:
                    print(f"{detok}", end=" ", flush=True)
            yield detok, None, decode_time_ms

        res_str = self.decode_tokens(res_tokens)
        yield res_str, "formatted", None

    def autocomplete(self, prompt):
        # use First vic alone to complete a story / prompt / sentence.
        pass


# NOTE: Each `model_name` should have its own start message
start_message = {
    "llama2_7b": (
        "System: You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe.  Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal "
        "content. Please ensure that your responses are socially unbiased and positive "
        "in nature. If a question does not make any sense, or is not factually coherent, "
        "explain why instead of answering something not correct. If you don't know the "
        "answer to a question, please don't share false information."
    ),
    "llama2_13b": (
        "System: You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe.  Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal "
        "content. Please ensure that your responses are socially unbiased and positive "
        "in nature. If a question does not make any sense, or is not factually coherent, "
        "explain why instead of answering something not correct. If you don't know the "
        "answer to a question, please don't share false information."
    ),
    "llama2_70b": (
        "System: You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe.  Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal "
        "content. Please ensure that your responses are socially unbiased and positive "
        "in nature. If a question does not make any sense, or is not factually coherent, "
        "explain why instead of answering something not correct. If you don't know the "
        "answer to a question, please don't share false information."
    ),
    "vicuna": (
        "A chat between a curious user and an artificial intelligence assistant. "
        "The assistant gives helpful, detailed, and polite answers to the user's "
        "questions.\n"
    ),
}


def create_prompt(model_name, history):
    global start_message
    system_message = start_message[model_name]
    if "llama2" in model_name:
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        conversation = "".join(
            [
                f"{B_INST} {item[0].strip()} {E_INST} {item[1].strip()} "
                for item in history[1:]
            ]
        )
        msg = f"{B_INST} {B_SYS} {system_message} {E_SYS} {history[0][0]} {E_INST} {history[0][1]} {conversation}"

    else:
        conversation = "".join(
            [
                "".join(["<|USER|>" + item[0], "<|ASSISTANT|>" + item[1]])
                for item in history
            ]
        )
        msg = system_message + conversation
        msg = msg.strip()
    return msg


def miliseconds_to_seconds(ms: float) -> float:
    return ms / 1000.0


@dataclass
class BenchmarkRunInfo:
    num_prompt_tokens : int
    prefill_time_ms : float
    token_times_ms : list[float]

    def get_prefill_speed(self) -> float:
        seconds = miliseconds_to_seconds(self.prefill_time_ms)
        if seconds == 0.0:
            return float('inf')
        return self.num_prompt_tokens / seconds

    def num_generated_tokens(self) -> int:
        return len(self.token_times_ms)

    def get_decode_time_ms(self) -> float:
        return sum(self.token_times_ms)

    def get_decode_speed(self) -> float:
        seconds = miliseconds_to_seconds(self.get_decode_time_ms())
        if seconds == 0.0:
            return float('inf')
        return self.num_generated_tokens() / seconds

    def get_e2e_time_ms(self) -> float:
        return self.prefill_time_ms + self.get_decode_time_ms()

    def get_e2e_decode_speed(self) -> float:
        seconds = miliseconds_to_seconds(self.get_e2e_time_ms())
        if seconds == 0.0:
            return float('inf')
        return self.num_generated_tokens() / seconds

    def get_e2e_token_processing_speed(self) -> float:
        seconds = miliseconds_to_seconds(self.get_e2e_time_ms())
        if seconds == 0.0:
            return float('inf')
        return (self.num_prompt_tokens + self.num_generated_tokens()) / seconds

    def print(self) -> None:
        total_tokens = self.num_prompt_tokens + self.num_generated_tokens()
        print(f"Num tokens: {self.num_prompt_tokens:} (prompt), {self.num_generated_tokens()} (generated), {total_tokens} (total)")
        print(f"Prefill: {self.prefill_time_ms:.2f} ms, {self.get_prefill_speed():.2f} tokens/s")
        print(f"Decode: {self.get_decode_time_ms():.2f} ms, {self.get_decode_speed():.2f} tokens/s")
        print(f"Decode end-2-end: {self.get_e2e_decode_speed():.2f} tokens/s (w/o prompt), {self.get_e2e_token_processing_speed():.2f} tokens/s (w/ prompt)")


def print_aggregate_stats(run_infos: list[BenchmarkRunInfo]) -> None:
    num_iterations = len(run_infos)
    print(f'Number of iterations: {num_iterations}')
    if num_iterations == 0:
        return

    if len(run_infos) == 1:
        run_infos[0].print()
        return

    total_tokens = run_infos[0].num_prompt_tokens + run_infos[0].num_generated_tokens()
    print(f"Num tokens: {run_infos[0].num_prompt_tokens} (prompt), {run_infos[0].num_generated_tokens()} (generated), {total_tokens} (total)")

    def avg_and_stdev(data):
        x = list(data)
        return mean(x), stdev(x)

    avg_prefill_ms, stdev_prefill = avg_and_stdev(x.prefill_time_ms for x in run_infos)
    avg_prefill_speed = mean(x.get_prefill_speed() for x in run_infos)
    print(f"Prefill: avg. {avg_prefill_ms:.2f} ms (stdev {stdev_prefill:.2f}), avg. {avg_prefill_speed:.2f} tokens/s")

    avg_decode_ms, stdev_decode = avg_and_stdev(x.get_decode_time_ms() for x in run_infos)
    avg_decode_speed = mean(x.get_decode_speed() for x in run_infos)
    print(f"Decode: avg. {avg_decode_ms:.2f} ms (stdev {stdev_decode:.2f}), avg. {avg_decode_speed:.2f} tokens/s")

    avg_e2e_decode_speed = mean(x.get_e2e_decode_speed() for x in run_infos)
    avg_e2e_processing_speed = mean(x.get_e2e_token_processing_speed() for x in run_infos)
    print(f"Decode end-2-end: avg. {avg_e2e_decode_speed:.2f} tokens/s (w/o prompt), avg. {avg_e2e_processing_speed:.2f} (w/ prompt)")


def enable_tracy_tracing():
    # Make tracy wait for a caputre to be collected before exiting.
    environ["TRACY_NO_EXIT"] = "1"

    if "IREE_PY_RUNTIME" not in environ or environ["IREE_PY_RUNTIME"] != "tracy":
        print("ERROR: Tracing enabled but tracy iree runtime not used.", file=sys.stderr)
        print("Set the IREE_PY_RUNTIME=tracy environment variable.", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    args, unknown = parser.parse_known_args()

    _extra_args = list(args.Xiree_compile)

    device_id = None

    if args.enable_tracing:
        enable_tracy_tracing()

    # Process vulkan target triple.
    # TODO: This feature should just be in a common utils for other LLMs and in general
    #       any model run via SHARK for Vulkan backend.
    vulkan_target_triple = args.iree_vulkan_target_triple
    if vulkan_target_triple != "":
        _extra_args.append(
            f"-iree-vulkan-target-triple={args.iree_vulkan_target_triple}"
        )
        # Step 1. Fetch the device ID.
        from shark.iree_utils.vulkan_utils import (
            get_all_vulkan_devices,
            get_vulkan_target_triple
        )
        vulkaninfo_list = get_all_vulkan_devices()
        id = 0
        for device in vulkaninfo_list:
            target_triple = get_vulkan_target_triple(vulkaninfo_list[id])
            if target_triple == vulkan_target_triple:
                device_id = id
                break
            id += 1

        assert device_id, f"no vulkan hardware for target-triple '{vulkan_target_triple}' exists"
        # Step 2. Add a few flags targetting specific hardwares.
        if "rdna" in vulkan_target_triple:
            flags_to_add = [
                "--iree-spirv-index-bits=64",
            ]
            _extra_args = _extra_args + flags_to_add


    vic = None
    if not args.sharded:
        vic_mlir_path = (
            None
            if args.vicuna_mlir_path is None
            else Path(args.vicuna_mlir_path)
        )
        vic_vmfb_path = (
            None
            if args.vicuna_vmfb_path is None
            else Path(args.vicuna_vmfb_path)
        )
        min_tokens = 0
        max_tokens = 512
        if args.enable_microbenchmark:
            min_tokens = max_tokens = args.microbenchmark_num_tokens

        vic = UnshardedVicuna(
            model_name=args.model_name,
            hf_auth_token=args.hf_auth_token,
            max_num_tokens=max_tokens,
            min_num_tokens=min_tokens,
            device=args.device,
            vulkan_target_triple=vulkan_target_triple,
            precision=args.precision,
            vicuna_mlir_path=vic_mlir_path,
            vicuna_vmfb_path=vic_vmfb_path,
            load_mlir_from_shark_tank=args.load_mlir_from_shark_tank,
            weight_group_size=args.weight_group_size,
            download_vmfb=args.download_vmfb,
            cache_vicunas=args.cache_vicunas,
            extra_args_cmd=_extra_args,
            device_id=device_id
        )
    else:
        if args.config is not None:
            config_file = open(args.config)
            config_json = json.load(config_file)
            config_file.close()
        else:
            config_json = None
        vic = ShardedVicuna(
            model_name=args.model_name,
            device=args.device,
            precision=args.precision,
            config_json=config_json,
            weight_group_size=args.weight_group_size,
            extra_args_cmd=_extra_args,
        )

    history = []

    model_list = {
        "vicuna": "vicuna=>TheBloke/vicuna-7B-1.1-HF",
        "llama2_7b": "llama2_7b=>meta-llama/Llama-2-7b-chat-hf",
        "llama2_13b": "llama2_13b=>meta-llama/Llama-2-13b-chat-hf",
        "llama2_70b": "llama2_70b=>meta-llama/Llama-2-70b-chat-hf",
    }

    iteration = 0

    benchmark_run_infos = []

    while True:
        # TODO: Add break condition from user input
        iteration += 1
        if not args.enable_microbenchmark:
            user_prompt = input("User prompt: ")
            history.append([user_prompt, ""])
            prompt = create_prompt(args.model_name, history)
        else:
            if iteration > args.microbenchmark_iterations:
                break
            user_prompt = args.user_prompt
            prompt = args.system_prompt + user_prompt
            history = [[user_prompt, ""]]

        prompt_token_count = len(vic.tokenizer(prompt).input_ids)
        total_time_ms = 0.0  # In order to avoid divide by zero error
        prefill_time_ms = 0
        is_first = True
        token_times_ms = []

        for text, msg, exec_time in vic.generate(prompt, cli=True):
            if args.enable_tracing:
                vic.shark_model.shark_runner.iree_config.device.flush_profiling()

            if msg is None:
                if is_first:
                    # Note that the prefill time is in seconds, and all the decoded tokens in ms.
                    prefill_time_ms = exec_time * 1000
                    is_first = False
                else:
                    token_times_ms.append(exec_time)
            elif "formatted" in msg:
                history[-1][1] = text
                print(f"\nResponse:\n{text.strip()}\n")
                run_info = BenchmarkRunInfo(prompt_token_count, prefill_time_ms, token_times_ms)
                run_info.print()
                benchmark_run_infos.append(run_info)

            else:
                sys.exit(
                    "unexpected message from the vicuna generate call, exiting."
                )

    if args.enable_microbenchmark:
        print("\n### Final Statistics ###")
        print_aggregate_stats(benchmark_run_infos)

```

`apps/language_models/shark_llama_cli.spec`:

```spec
# -*- mode: python ; coding: utf-8 -*-
from PyInstaller.utils.hooks import collect_data_files
from PyInstaller.utils.hooks import collect_submodules
from PyInstaller.utils.hooks import copy_metadata

import sys ; sys.setrecursionlimit(sys.getrecursionlimit() * 5)

datas = []
datas += collect_data_files('torch')
datas += copy_metadata('torch')
datas += copy_metadata('tqdm')
datas += copy_metadata('regex')
datas += copy_metadata('requests')
datas += copy_metadata('packaging')
datas += copy_metadata('filelock')
datas += copy_metadata('numpy')
datas += copy_metadata('tokenizers')
datas += copy_metadata('importlib_metadata')
datas += copy_metadata('torch-mlir')
datas += copy_metadata('omegaconf')
datas += copy_metadata('safetensors')
datas += copy_metadata('huggingface-hub')
datas += copy_metadata('sentencepiece')
datas += copy_metadata("pyyaml")
datas += collect_data_files("tokenizers")
datas += collect_data_files("tiktoken")
datas += collect_data_files("accelerate")
datas += collect_data_files('diffusers')
datas += collect_data_files('transformers')
datas += collect_data_files('opencv-python')
datas += collect_data_files('pytorch_lightning')
datas += collect_data_files('skimage')
datas += collect_data_files('gradio')
datas += collect_data_files('gradio_client')
datas += collect_data_files('iree')
datas += collect_data_files('google-cloud-storage')
datas += collect_data_files('py-cpuinfo')
datas += collect_data_files("shark", include_py_files=True)
datas += collect_data_files("timm", include_py_files=True)
datas += collect_data_files("tqdm")
datas += collect_data_files("tkinter")
datas += collect_data_files("webview")
datas += collect_data_files("sentencepiece")
datas += collect_data_files("jsonschema")
datas += collect_data_files("jsonschema_specifications")
datas += collect_data_files("cpuinfo")
datas += collect_data_files("langchain")

binaries = []

block_cipher = None

hiddenimports = ['shark', 'shark.shark_inference', 'apps']
hiddenimports += [x for x in collect_submodules("skimage") if "tests" not in x]
hiddenimports += [x for x in collect_submodules("iree") if "tests" not in x]

a = Analysis(
    ['scripts/vicuna.py'],
    pathex=['.'],
    binaries=binaries,
    datas=datas,
    hiddenimports=hiddenimports,
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)
pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='shark_llama_cli',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

```

`apps/language_models/src/model_wrappers/falcon_model.py`:

```py
import torch


class FalconModel(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask):
        input_dict = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "past_key_values": None,
            "use_cache": True,
        }
        output = self.model(
            **input_dict,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )[0]
        return output[:, -1, :]

```

`apps/language_models/src/model_wrappers/falcon_sharded_model.py`:

```py
import torch
from typing import Optional, Tuple


class WordEmbeddingsLayer(torch.nn.Module):
    def __init__(self, word_embedding_layer):
        super().__init__()
        self.model = word_embedding_layer

    def forward(self, input_ids):
        output = self.model.forward(input=input_ids)
        return output


class CompiledWordEmbeddingsLayer(torch.nn.Module):
    def __init__(self, compiled_word_embedding_layer):
        super().__init__()
        self.model = compiled_word_embedding_layer

    def forward(self, input_ids):
        input_ids = input_ids.detach().numpy()
        new_input_ids = self.model("forward", input_ids)
        new_input_ids = new_input_ids.reshape(
            [1, new_input_ids.shape[0], new_input_ids.shape[1]]
        )
        return torch.tensor(new_input_ids)


class LNFEmbeddingLayer(torch.nn.Module):
    def __init__(self, ln_f):
        super().__init__()
        self.model = ln_f

    def forward(self, hidden_states):
        output = self.model.forward(input=hidden_states)
        return output


class CompiledLNFEmbeddingLayer(torch.nn.Module):
    def __init__(self, ln_f):
        super().__init__()
        self.model = ln_f

    def forward(self, hidden_states):
        hidden_states = hidden_states.detach().numpy()
        new_hidden_states = self.model("forward", (hidden_states,))

        return torch.tensor(new_hidden_states)


class LMHeadEmbeddingLayer(torch.nn.Module):
    def __init__(self, embedding_layer):
        super().__init__()
        self.model = embedding_layer

    def forward(self, hidden_states):
        output = self.model.forward(input=hidden_states)
        return output


class CompiledLMHeadEmbeddingLayer(torch.nn.Module):
    def __init__(self, lm_head):
        super().__init__()
        self.model = lm_head

    def forward(self, hidden_states):
        hidden_states = hidden_states.detach().numpy()
        new_hidden_states = self.model("forward", (hidden_states,))
        return torch.tensor(new_hidden_states)


class FourWayShardingDecoderLayer(torch.nn.Module):
    def __init__(self, decoder_layer_model, falcon_variant):
        super().__init__()
        self.model = decoder_layer_model
        self.falcon_variant = falcon_variant

    def forward(self, hidden_states, attention_mask):
        new_pkvs = []
        for layer in self.model:
            outputs = layer(
                hidden_states=hidden_states,
                alibi=None,
                attention_mask=attention_mask,
                use_cache=True,
            )
            hidden_states = outputs[0]
            new_pkvs.append(
                (
                    outputs[-1][0],
                    outputs[-1][1],
                )
            )

        (
            (new_pkv00, new_pkv01),
            (new_pkv10, new_pkv11),
            (new_pkv20, new_pkv21),
            (new_pkv30, new_pkv31),
            (new_pkv40, new_pkv41),
            (new_pkv50, new_pkv51),
            (new_pkv60, new_pkv61),
            (new_pkv70, new_pkv71),
            (new_pkv80, new_pkv81),
            (new_pkv90, new_pkv91),
            (new_pkv100, new_pkv101),
            (new_pkv110, new_pkv111),
            (new_pkv120, new_pkv121),
            (new_pkv130, new_pkv131),
            (new_pkv140, new_pkv141),
            (new_pkv150, new_pkv151),
            (new_pkv160, new_pkv161),
            (new_pkv170, new_pkv171),
            (new_pkv180, new_pkv181),
            (new_pkv190, new_pkv191),
        ) = new_pkvs
        result = (
            hidden_states,
            new_pkv00,
            new_pkv01,
            new_pkv10,
            new_pkv11,
            new_pkv20,
            new_pkv21,
            new_pkv30,
            new_pkv31,
            new_pkv40,
            new_pkv41,
            new_pkv50,
            new_pkv51,
            new_pkv60,
            new_pkv61,
            new_pkv70,
            new_pkv71,
            new_pkv80,
            new_pkv81,
            new_pkv90,
            new_pkv91,
            new_pkv100,
            new_pkv101,
            new_pkv110,
            new_pkv111,
            new_pkv120,
            new_pkv121,
            new_pkv130,
            new_pkv131,
            new_pkv140,
            new_pkv141,
            new_pkv150,
            new_pkv151,
            new_pkv160,
            new_pkv161,
            new_pkv170,
            new_pkv171,
            new_pkv180,
            new_pkv181,
            new_pkv190,
            new_pkv191,
        )
        return result


class CompiledFourWayShardingDecoderLayer(torch.nn.Module):
    def __init__(
        self, layer_id, device_idx, falcon_variant, device, precision, model
    ):
        super().__init__()
        self.layer_id = layer_id
        self.device_index = device_idx
        self.falcon_variant = falcon_variant
        self.device = device
        self.precision = precision
        self.model = model

    def forward(
        self,
        hidden_states: torch.Tensor,
        alibi: Optional[torch.Tensor],
        attention_mask: torch.Tensor,
        position_ids: Optional[torch.LongTensor] = None,
        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        head_mask: Optional[torch.Tensor] = None,
        use_cache: bool = False,
        output_attentions: bool = False,
    ):
        import gc

        torch.cuda.empty_cache()
        gc.collect()

        if self.model is None:
            raise ValueError("Layer vmfb not found")

        hidden_states = hidden_states.to(torch.float32).detach().numpy()
        attention_mask = attention_mask.to(torch.float32).detach().numpy()

        if alibi is not None or layer_past is not None:
            raise ValueError("Past Key Values and alibi should be None")
        else:
            output = self.model(
                "forward",
                (
                    hidden_states,
                    attention_mask,
                ),
            )

        result = (
            torch.tensor(output[0]),
            (
                torch.tensor(output[1]),
                torch.tensor(output[2]),
            ),
            (
                torch.tensor(output[3]),
                torch.tensor(output[4]),
            ),
            (
                torch.tensor(output[5]),
                torch.tensor(output[6]),
            ),
            (
                torch.tensor(output[7]),
                torch.tensor(output[8]),
            ),
            (
                torch.tensor(output[9]),
                torch.tensor(output[10]),
            ),
            (
                torch.tensor(output[11]),
                torch.tensor(output[12]),
            ),
            (
                torch.tensor(output[13]),
                torch.tensor(output[14]),
            ),
            (
                torch.tensor(output[15]),
                torch.tensor(output[16]),
            ),
            (
                torch.tensor(output[17]),
                torch.tensor(output[18]),
            ),
            (
                torch.tensor(output[19]),
                torch.tensor(output[20]),
            ),
            (
                torch.tensor(output[21]),
                torch.tensor(output[22]),
            ),
            (
                torch.tensor(output[23]),
                torch.tensor(output[24]),
            ),
            (
                torch.tensor(output[25]),
                torch.tensor(output[26]),
            ),
            (
                torch.tensor(output[27]),
                torch.tensor(output[28]),
            ),
            (
                torch.tensor(output[29]),
                torch.tensor(output[30]),
            ),
            (
                torch.tensor(output[31]),
                torch.tensor(output[32]),
            ),
            (
                torch.tensor(output[33]),
                torch.tensor(output[34]),
            ),
            (
                torch.tensor(output[35]),
                torch.tensor(output[36]),
            ),
            (
                torch.tensor(output[37]),
                torch.tensor(output[38]),
            ),
            (
                torch.tensor(output[39]),
                torch.tensor(output[40]),
            ),
        )
        return result


class TwoWayShardingDecoderLayer(torch.nn.Module):
    def __init__(self, decoder_layer_model, falcon_variant):
        super().__init__()
        self.model = decoder_layer_model
        self.falcon_variant = falcon_variant

    def forward(self, hidden_states, attention_mask):
        new_pkvs = []
        for layer in self.model:
            outputs = layer(
                hidden_states=hidden_states,
                alibi=None,
                attention_mask=attention_mask,
                use_cache=True,
            )
            hidden_states = outputs[0]
            new_pkvs.append(
                (
                    outputs[-1][0],
                    outputs[-1][1],
                )
            )

        (
            (new_pkv00, new_pkv01),
            (new_pkv10, new_pkv11),
            (new_pkv20, new_pkv21),
            (new_pkv30, new_pkv31),
            (new_pkv40, new_pkv41),
            (new_pkv50, new_pkv51),
            (new_pkv60, new_pkv61),
            (new_pkv70, new_pkv71),
            (new_pkv80, new_pkv81),
            (new_pkv90, new_pkv91),
            (new_pkv100, new_pkv101),
            (new_pkv110, new_pkv111),
            (new_pkv120, new_pkv121),
            (new_pkv130, new_pkv131),
            (new_pkv140, new_pkv141),
            (new_pkv150, new_pkv151),
            (new_pkv160, new_pkv161),
            (new_pkv170, new_pkv171),
            (new_pkv180, new_pkv181),
            (new_pkv190, new_pkv191),
            (new_pkv200, new_pkv201),
            (new_pkv210, new_pkv211),
            (new_pkv220, new_pkv221),
            (new_pkv230, new_pkv231),
            (new_pkv240, new_pkv241),
            (new_pkv250, new_pkv251),
            (new_pkv260, new_pkv261),
            (new_pkv270, new_pkv271),
            (new_pkv280, new_pkv281),
            (new_pkv290, new_pkv291),
            (new_pkv300, new_pkv301),
            (new_pkv310, new_pkv311),
            (new_pkv320, new_pkv321),
            (new_pkv330, new_pkv331),
            (new_pkv340, new_pkv341),
            (new_pkv350, new_pkv351),
            (new_pkv360, new_pkv361),
            (new_pkv370, new_pkv371),
            (new_pkv380, new_pkv381),
            (new_pkv390, new_pkv391),
        ) = new_pkvs
        result = (
            hidden_states,
            new_pkv00,
            new_pkv01,
            new_pkv10,
            new_pkv11,
            new_pkv20,
            new_pkv21,
            new_pkv30,
            new_pkv31,
            new_pkv40,
            new_pkv41,
            new_pkv50,
            new_pkv51,
            new_pkv60,
            new_pkv61,
            new_pkv70,
            new_pkv71,
            new_pkv80,
            new_pkv81,
            new_pkv90,
            new_pkv91,
            new_pkv100,
            new_pkv101,
            new_pkv110,
            new_pkv111,
            new_pkv120,
            new_pkv121,
            new_pkv130,
            new_pkv131,
            new_pkv140,
            new_pkv141,
            new_pkv150,
            new_pkv151,
            new_pkv160,
            new_pkv161,
            new_pkv170,
            new_pkv171,
            new_pkv180,
            new_pkv181,
            new_pkv190,
            new_pkv191,
            new_pkv200,
            new_pkv201,
            new_pkv210,
            new_pkv211,
            new_pkv220,
            new_pkv221,
            new_pkv230,
            new_pkv231,
            new_pkv240,
            new_pkv241,
            new_pkv250,
            new_pkv251,
            new_pkv260,
            new_pkv261,
            new_pkv270,
            new_pkv271,
            new_pkv280,
            new_pkv281,
            new_pkv290,
            new_pkv291,
            new_pkv300,
            new_pkv301,
            new_pkv310,
            new_pkv311,
            new_pkv320,
            new_pkv321,
            new_pkv330,
            new_pkv331,
            new_pkv340,
            new_pkv341,
            new_pkv350,
            new_pkv351,
            new_pkv360,
            new_pkv361,
            new_pkv370,
            new_pkv371,
            new_pkv380,
            new_pkv381,
            new_pkv390,
            new_pkv391,
        )
        return result


class CompiledTwoWayShardingDecoderLayer(torch.nn.Module):
    def __init__(
        self, layer_id, device_idx, falcon_variant, device, precision, model
    ):
        super().__init__()
        self.layer_id = layer_id
        self.device_index = device_idx
        self.falcon_variant = falcon_variant
        self.device = device
        self.precision = precision
        self.model = model

    def forward(
        self,
        hidden_states: torch.Tensor,
        alibi: Optional[torch.Tensor],
        attention_mask: torch.Tensor,
        position_ids: Optional[torch.LongTensor] = None,
        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        head_mask: Optional[torch.Tensor] = None,
        use_cache: bool = False,
        output_attentions: bool = False,
    ):
        import gc

        torch.cuda.empty_cache()
        gc.collect()

        if self.model is None:
            raise ValueError("Layer vmfb not found")

        hidden_states = hidden_states.to(torch.float32).detach().numpy()
        attention_mask = attention_mask.to(torch.float32).detach().numpy()

        if alibi is not None or layer_past is not None:
            raise ValueError("Past Key Values and alibi should be None")
        else:
            output = self.model(
                "forward",
                (
                    hidden_states,
                    attention_mask,
                ),
            )

        result = (
            torch.tensor(output[0]),
            (
                torch.tensor(output[1]),
                torch.tensor(output[2]),
            ),
            (
                torch.tensor(output[3]),
                torch.tensor(output[4]),
            ),
            (
                torch.tensor(output[5]),
                torch.tensor(output[6]),
            ),
            (
                torch.tensor(output[7]),
                torch.tensor(output[8]),
            ),
            (
                torch.tensor(output[9]),
                torch.tensor(output[10]),
            ),
            (
                torch.tensor(output[11]),
                torch.tensor(output[12]),
            ),
            (
                torch.tensor(output[13]),
                torch.tensor(output[14]),
            ),
            (
                torch.tensor(output[15]),
                torch.tensor(output[16]),
            ),
            (
                torch.tensor(output[17]),
                torch.tensor(output[18]),
            ),
            (
                torch.tensor(output[19]),
                torch.tensor(output[20]),
            ),
            (
                torch.tensor(output[21]),
                torch.tensor(output[22]),
            ),
            (
                torch.tensor(output[23]),
                torch.tensor(output[24]),
            ),
            (
                torch.tensor(output[25]),
                torch.tensor(output[26]),
            ),
            (
                torch.tensor(output[27]),
                torch.tensor(output[28]),
            ),
            (
                torch.tensor(output[29]),
                torch.tensor(output[30]),
            ),
            (
                torch.tensor(output[31]),
                torch.tensor(output[32]),
            ),
            (
                torch.tensor(output[33]),
                torch.tensor(output[34]),
            ),
            (
                torch.tensor(output[35]),
                torch.tensor(output[36]),
            ),
            (
                torch.tensor(output[37]),
                torch.tensor(output[38]),
            ),
            (
                torch.tensor(output[39]),
                torch.tensor(output[40]),
            ),
            (
                torch.tensor(output[41]),
                torch.tensor(output[42]),
            ),
            (
                torch.tensor(output[43]),
                torch.tensor(output[44]),
            ),
            (
                torch.tensor(output[45]),
                torch.tensor(output[46]),
            ),
            (
                torch.tensor(output[47]),
                torch.tensor(output[48]),
            ),
            (
                torch.tensor(output[49]),
                torch.tensor(output[50]),
            ),
            (
                torch.tensor(output[51]),
                torch.tensor(output[52]),
            ),
            (
                torch.tensor(output[53]),
                torch.tensor(output[54]),
            ),
            (
                torch.tensor(output[55]),
                torch.tensor(output[56]),
            ),
            (
                torch.tensor(output[57]),
                torch.tensor(output[58]),
            ),
            (
                torch.tensor(output[59]),
                torch.tensor(output[60]),
            ),
            (
                torch.tensor(output[61]),
                torch.tensor(output[62]),
            ),
            (
                torch.tensor(output[63]),
                torch.tensor(output[64]),
            ),
            (
                torch.tensor(output[65]),
                torch.tensor(output[66]),
            ),
            (
                torch.tensor(output[67]),
                torch.tensor(output[68]),
            ),
            (
                torch.tensor(output[69]),
                torch.tensor(output[70]),
            ),
            (
                torch.tensor(output[71]),
                torch.tensor(output[72]),
            ),
            (
                torch.tensor(output[73]),
                torch.tensor(output[74]),
            ),
            (
                torch.tensor(output[75]),
                torch.tensor(output[76]),
            ),
            (
                torch.tensor(output[77]),
                torch.tensor(output[78]),
            ),
            (
                torch.tensor(output[79]),
                torch.tensor(output[80]),
            ),
        )
        return result


class ShardedFalconModel:
    def __init__(self, model, layers, word_embeddings, ln_f, lm_head):
        super().__init__()
        self.model = model
        self.model.transformer.h = torch.nn.modules.container.ModuleList(
            layers
        )
        self.model.transformer.word_embeddings = word_embeddings
        self.model.transformer.ln_f = ln_f
        self.model.lm_head = lm_head

    def forward(
        self,
        input_ids,
        attention_mask=None,
    ):
        return self.model.forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
        ).logits[:, -1, :]

```

`apps/language_models/src/model_wrappers/minigpt4.py`:

```py
import torch
import dataclasses
from enum import auto, Enum
from typing import List, Any
from transformers import StoppingCriteria


from brevitas_examples.common.generative.quantize import quantize_model
from brevitas_examples.llm.llm_quant.run_utils import get_model_impl


class LayerNorm(torch.nn.LayerNorm):
    """Subclass torch's LayerNorm to handle fp16."""

    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        ret = super().forward(x.type(torch.float32))
        return ret.type(orig_type)


class VisionModel(torch.nn.Module):
    def __init__(
        self,
        ln_vision,
        visual_encoder,
        precision="fp32",
        weight_group_size=128,
    ):
        super().__init__()
        self.ln_vision = ln_vision
        self.visual_encoder = visual_encoder
        if precision in ["int4", "int8"]:
            print("Vision Model applying weight quantization to ln_vision")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                self.ln_vision,
                dtype=torch.float32,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")
            print(
                "Vision Model applying weight quantization to visual_encoder"
            )
            quantize_model(
                self.visual_encoder,
                dtype=torch.float32,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(self, image):
        image_embeds = self.ln_vision(self.visual_encoder(image))
        return image_embeds


class QformerBertModel(torch.nn.Module):
    def __init__(self, qformer_bert):
        super().__init__()
        self.qformer_bert = qformer_bert

    def forward(self, query_tokens, image_embeds, image_atts):
        query_output = self.qformer_bert(
            query_embeds=query_tokens,
            encoder_hidden_states=image_embeds,
            encoder_attention_mask=image_atts,
            return_dict=True,
        )
        return query_output.last_hidden_state


class FirstLlamaModel(torch.nn.Module):
    def __init__(self, model, precision="fp32", weight_group_size=128):
        super().__init__()
        self.model = model
        print("SHARK: Loading LLAMA Done")
        if precision in ["int4", "int8"]:
            print("First Llama applying weight quantization")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                self.model,
                dtype=torch.float32,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(self, inputs_embeds, position_ids, attention_mask):
        print("************************************")
        print(
            "inputs_embeds: ",
            inputs_embeds.shape,
            " dtype: ",
            inputs_embeds.dtype,
        )
        print(
            "position_ids: ",
            position_ids.shape,
            " dtype: ",
            position_ids.dtype,
        )
        print(
            "attention_mask: ",
            attention_mask.shape,
            " dtype: ",
            attention_mask.dtype,
        )
        print("************************************")
        config = {
            "inputs_embeds": inputs_embeds,
            "position_ids": position_ids,
            "past_key_values": None,
            "use_cache": True,
            "attention_mask": attention_mask,
        }
        output = self.model(
            **config,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )
        return_vals = []
        return_vals.append(output.logits)
        temp_past_key_values = output.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SecondLlamaModel(torch.nn.Module):
    def __init__(self, model, precision="fp32", weight_group_size=128):
        super().__init__()
        self.model = model
        print("SHARK: Loading LLAMA Done")
        if precision in ["int4", "int8"]:
            print("Second Llama applying weight quantization")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                self.model,
                dtype=torch.float32,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(
        self,
        input_ids,
        position_ids,
        attention_mask,
        i1,
        i2,
        i3,
        i4,
        i5,
        i6,
        i7,
        i8,
        i9,
        i10,
        i11,
        i12,
        i13,
        i14,
        i15,
        i16,
        i17,
        i18,
        i19,
        i20,
        i21,
        i22,
        i23,
        i24,
        i25,
        i26,
        i27,
        i28,
        i29,
        i30,
        i31,
        i32,
        i33,
        i34,
        i35,
        i36,
        i37,
        i38,
        i39,
        i40,
        i41,
        i42,
        i43,
        i44,
        i45,
        i46,
        i47,
        i48,
        i49,
        i50,
        i51,
        i52,
        i53,
        i54,
        i55,
        i56,
        i57,
        i58,
        i59,
        i60,
        i61,
        i62,
        i63,
        i64,
    ):
        print("************************************")
        print("input_ids: ", input_ids.shape, " dtype: ", input_ids.dtype)
        print(
            "position_ids: ",
            position_ids.shape,
            " dtype: ",
            position_ids.dtype,
        )
        print(
            "attention_mask: ",
            attention_mask.shape,
            " dtype: ",
            attention_mask.dtype,
        )
        print("past_key_values: ", i1.shape, i2.shape, i63.shape, i64.shape)
        print("past_key_values dtype: ", i1.dtype)
        print("************************************")
        config = {
            "input_ids": input_ids,
            "position_ids": position_ids,
            "past_key_values": (
                (i1, i2),
                (
                    i3,
                    i4,
                ),
                (
                    i5,
                    i6,
                ),
                (
                    i7,
                    i8,
                ),
                (
                    i9,
                    i10,
                ),
                (
                    i11,
                    i12,
                ),
                (
                    i13,
                    i14,
                ),
                (
                    i15,
                    i16,
                ),
                (
                    i17,
                    i18,
                ),
                (
                    i19,
                    i20,
                ),
                (
                    i21,
                    i22,
                ),
                (
                    i23,
                    i24,
                ),
                (
                    i25,
                    i26,
                ),
                (
                    i27,
                    i28,
                ),
                (
                    i29,
                    i30,
                ),
                (
                    i31,
                    i32,
                ),
                (
                    i33,
                    i34,
                ),
                (
                    i35,
                    i36,
                ),
                (
                    i37,
                    i38,
                ),
                (
                    i39,
                    i40,
                ),
                (
                    i41,
                    i42,
                ),
                (
                    i43,
                    i44,
                ),
                (
                    i45,
                    i46,
                ),
                (
                    i47,
                    i48,
                ),
                (
                    i49,
                    i50,
                ),
                (
                    i51,
                    i52,
                ),
                (
                    i53,
                    i54,
                ),
                (
                    i55,
                    i56,
                ),
                (
                    i57,
                    i58,
                ),
                (
                    i59,
                    i60,
                ),
                (
                    i61,
                    i62,
                ),
                (
                    i63,
                    i64,
                ),
            ),
            "use_cache": True,
            "attention_mask": attention_mask,
        }
        output = self.model(
            **config,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )
        return_vals = []
        return_vals.append(output.logits)
        temp_past_key_values = output.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SeparatorStyle(Enum):
    """Different separator style."""

    SINGLE = auto()
    TWO = auto()


@dataclasses.dataclass
class Conversation:
    """A class that keeps all conversation history."""

    system: str
    roles: List[str]
    messages: List[List[str]]
    offset: int
    sep_style: SeparatorStyle = SeparatorStyle.SINGLE
    sep: str = "###"
    sep2: str = None

    skip_next: bool = False
    conv_id: Any = None

    def get_prompt(self):
        if self.sep_style == SeparatorStyle.SINGLE:
            ret = self.system + self.sep
            for role, message in self.messages:
                if message:
                    ret += role + ": " + message + self.sep
                else:
                    ret += role + ":"
            return ret
        elif self.sep_style == SeparatorStyle.TWO:
            seps = [self.sep, self.sep2]
            ret = self.system + seps[0]
            for i, (role, message) in enumerate(self.messages):
                if message:
                    ret += role + ": " + message + seps[i % 2]
                else:
                    ret += role + ":"
            return ret
        else:
            raise ValueError(f"Invalid style: {self.sep_style}")

    def append_message(self, role, message):
        self.messages.append([role, message])

    def to_gradio_chatbot(self):
        ret = []
        for i, (role, msg) in enumerate(self.messages[self.offset :]):
            if i % 2 == 0:
                ret.append([msg, None])
            else:
                ret[-1][-1] = msg
        return ret

    def copy(self):
        return Conversation(
            system=self.system,
            roles=self.roles,
            messages=[[x, y] for x, y in self.messages],
            offset=self.offset,
            sep_style=self.sep_style,
            sep=self.sep,
            sep2=self.sep2,
            conv_id=self.conv_id,
        )

    def dict(self):
        return {
            "system": self.system,
            "roles": self.roles,
            "messages": self.messages,
            "offset": self.offset,
            "sep": self.sep,
            "sep2": self.sep2,
            "conv_id": self.conv_id,
        }


class StoppingCriteriaSub(StoppingCriteria):
    def __init__(self, stops=[], encounters=1):
        super().__init__()
        self.stops = stops

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):
        for stop in self.stops:
            if torch.all((stop == input_ids[0][-len(stop) :])).item():
                return True

        return False


CONV_VISION = Conversation(
    system="Give the following image: <Img>ImageContent</Img>. "
    "You will be able to see the image once I provide it to you. Please answer my questions.",
    roles=("Human", "Assistant"),
    messages=[],
    offset=2,
    sep_style=SeparatorStyle.SINGLE,
    sep="###",
)

```

`apps/language_models/src/model_wrappers/stablelm_model.py`:

```py
import torch


class StableLMModel(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask):
        combine_input_dict = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        }
        output = self.model(**combine_input_dict)
        return output.logits

```

`apps/language_models/src/model_wrappers/vicuna4.py`:

```py
import argparse
import json
import re
from io import BytesIO
from pathlib import Path
from tqdm import tqdm
from typing import List, Optional, Tuple, Union
import numpy as np
import iree.runtime
import itertools
import subprocess

import torch
import torch_mlir
from torch_mlir import TensorPlaceholder
from torch_mlir.compiler_utils import run_pipeline_with_repro_report
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LlamaPreTrainedModel,
)
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    SequenceClassifierOutputWithPast,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import (
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    logging,
    replace_return_docstrings,
)

from apps.language_models.src.pipelines.SharkLLMBase import SharkLLMBase
from apps.language_models.src.model_wrappers.vicuna_sharded_model import (
    FirstVicunaLayer,
    SecondVicunaLayer,
    CompiledVicunaLayer,
    ShardedVicunaModel,
    LMHead,
    LMHeadCompiled,
    VicunaEmbedding,
    VicunaEmbeddingCompiled,
    VicunaNorm,
    VicunaNormCompiled,
)
from apps.language_models.src.model_wrappers.vicuna_model import (
    FirstVicuna,
    SecondVicuna7B,
)
from apps.language_models.utils import (
    get_vmfb_from_path,
)
from shark.shark_downloader import download_public_file
from shark.shark_importer import get_f16_inputs
from shark.shark_inference import SharkInference

from transformers.models.llama.configuration_llama import LlamaConfig
from transformers.models.llama.modeling_llama import (
    LlamaDecoderLayer,
    LlamaRMSNorm,
    _make_causal_mask,
    _expand_mask,
)
from torch import nn
from time import time


class LlamaModel(LlamaPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]

    Args:
        config: LlamaConfig
    """

    def __init__(self, config: LlamaConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(
            config.vocab_size, config.hidden_size, self.padding_idx
        )
        self.layers = nn.ModuleList(
            [
                LlamaDecoderLayer(config)
                for _ in range(config.num_hidden_layers)
            ]
        )
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask
    def _prepare_decoder_attention_mask(
        self,
        attention_mask,
        input_shape,
        inputs_embeds,
        past_key_values_length,
    ):
        # create causal mask
        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
        combined_attention_mask = None
        if input_shape[-1] > 1:
            combined_attention_mask = _make_causal_mask(
                input_shape,
                inputs_embeds.dtype,
                device=inputs_embeds.device,
                past_key_values_length=past_key_values_length,
            )

        if attention_mask is not None:
            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
            expanded_attn_mask = _expand_mask(
                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
            ).to(inputs_embeds.device)
            combined_attention_mask = (
                expanded_attn_mask
                if combined_attention_mask is None
                else expanded_attn_mask + combined_attention_mask
            )

        return combined_attention_mask

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ):
        t1 = time()
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        use_cache = (
            use_cache if use_cache is not None else self.config.use_cache
        )

        return_dict = (
            return_dict
            if return_dict is not None
            else self.config.use_return_dict
        )

        # retrieve input_ids and inputs_embeds
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError(
                "You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"
            )
        elif input_ids is not None:
            batch_size, seq_length = input_ids.shape
        elif inputs_embeds is not None:
            batch_size, seq_length, _ = inputs_embeds.shape
        else:
            raise ValueError(
                "You have to specify either decoder_input_ids or decoder_inputs_embeds"
            )

        seq_length_with_past = seq_length
        past_key_values_length = 0

        if past_key_values is not None:
            past_key_values_length = past_key_values[0][0].shape[2]
            seq_length_with_past = (
                seq_length_with_past + past_key_values_length
            )

        if position_ids is None:
            device = (
                input_ids.device
                if input_ids is not None
                else inputs_embeds.device
            )
            position_ids = torch.arange(
                past_key_values_length,
                seq_length + past_key_values_length,
                dtype=torch.long,
                device=device,
            )
            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
        else:
            position_ids = position_ids.view(-1, seq_length).long()

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        # embed positions
        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, seq_length_with_past),
                dtype=torch.bool,
                device=inputs_embeds.device,
            )

        attention_mask = self._prepare_decoder_attention_mask(
            attention_mask,
            (batch_size, seq_length),
            inputs_embeds,
            past_key_values_length,
        )

        hidden_states = inputs_embeds

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = () if use_cache else None

        for idx, decoder_layer in enumerate(self.compressedlayers):
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            past_key_value = (
                past_key_values[8 * idx : 8 * (idx + 1)]
                if past_key_values is not None
                else None
            )

            if self.gradient_checkpointing and self.training:

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # None for past_key_value
                        return module(*inputs, output_attentions, None)

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(decoder_layer),
                    hidden_states,
                    attention_mask,
                    position_ids,
                    None,
                )
            else:
                layer_outputs = decoder_layer.forward(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_value,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                )

            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache += (layer_outputs[1:],)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        try:
            hidden_states = np.asarray(hidden_states, hidden_states.dtype)
        except:
            _ = 10

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        next_cache = next_decoder_cache if use_cache else None
        next_cache = tuple(itertools.chain.from_iterable(next_cache))
        print(f"Token generated in {time() - t1} seconds")
        if not return_dict:
            return tuple(
                v
                for v in [
                    hidden_states,
                    next_cache,
                    all_hidden_states,
                    all_self_attns,
                ]
                if v is not None
            )
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )


class EightLayerLayerSV(torch.nn.Module):
    def __init__(self, layers):
        super().__init__()
        assert len(layers) == 8
        self.layers = layers

    def forward(
        self,
        hidden_states,
        attention_mask,
        position_ids,
        pkv00,
        pkv01,
        pkv10,
        pkv11,
        pkv20,
        pkv21,
        pkv30,
        pkv31,
        pkv40,
        pkv41,
        pkv50,
        pkv51,
        pkv60,
        pkv61,
        pkv70,
        pkv71,
    ):
        pkvs = [
            (pkv00, pkv01),
            (pkv10, pkv11),
            (pkv20, pkv21),
            (pkv30, pkv31),
            (pkv40, pkv41),
            (pkv50, pkv51),
            (pkv60, pkv61),
            (pkv70, pkv71),
        ]
        new_pkvs = []
        for layer, pkv in zip(self.layers, pkvs):
            outputs = layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=(
                    pkv[0],
                    pkv[1],
                ),
                use_cache=True,
            )

            hidden_states = outputs[0]
            new_pkvs.append(
                (
                    outputs[-1][0],
                    outputs[-1][1],
                )
            )
        (
            (new_pkv00, new_pkv01),
            (new_pkv10, new_pkv11),
            (new_pkv20, new_pkv21),
            (new_pkv30, new_pkv31),
            (new_pkv40, new_pkv41),
            (new_pkv50, new_pkv51),
            (new_pkv60, new_pkv61),
            (new_pkv70, new_pkv71),
        ) = new_pkvs
        return (
            hidden_states,
            new_pkv00,
            new_pkv01,
            new_pkv10,
            new_pkv11,
            new_pkv20,
            new_pkv21,
            new_pkv30,
            new_pkv31,
            new_pkv40,
            new_pkv41,
            new_pkv50,
            new_pkv51,
            new_pkv60,
            new_pkv61,
            new_pkv70,
            new_pkv71,
        )


class EightLayerLayerFV(torch.nn.Module):
    def __init__(self, layers):
        super().__init__()
        assert len(layers) == 8
        self.layers = layers

    def forward(self, hidden_states, attention_mask, position_ids):
        new_pkvs = []
        for layer in self.layers:
            outputs = layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=None,
                use_cache=True,
            )

            hidden_states = outputs[0]
            new_pkvs.append(
                (
                    outputs[-1][0],
                    outputs[-1][1],
                )
            )
        (
            (new_pkv00, new_pkv01),
            (new_pkv10, new_pkv11),
            (new_pkv20, new_pkv21),
            (new_pkv30, new_pkv31),
            (new_pkv40, new_pkv41),
            (new_pkv50, new_pkv51),
            (new_pkv60, new_pkv61),
            (new_pkv70, new_pkv71),
        ) = new_pkvs
        return (
            hidden_states,
            new_pkv00,
            new_pkv01,
            new_pkv10,
            new_pkv11,
            new_pkv20,
            new_pkv21,
            new_pkv30,
            new_pkv31,
            new_pkv40,
            new_pkv41,
            new_pkv50,
            new_pkv51,
            new_pkv60,
            new_pkv61,
            new_pkv70,
            new_pkv71,
        )


class CompiledEightLayerLayerSV(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(
        self,
        hidden_states,
        attention_mask,
        position_ids,
        past_key_value,
        output_attentions=False,
        use_cache=True,
    ):
        hidden_states = hidden_states.detach()
        attention_mask = attention_mask.detach()
        position_ids = position_ids.detach()
        (
            (pkv00, pkv01),
            (pkv10, pkv11),
            (pkv20, pkv21),
            (pkv30, pkv31),
            (pkv40, pkv41),
            (pkv50, pkv51),
            (pkv60, pkv61),
            (pkv70, pkv71),
        ) = past_key_value
        pkv00 = pkv00.detatch()
        pkv01 = pkv01.detatch()
        pkv10 = pkv10.detatch()
        pkv11 = pkv11.detatch()
        pkv20 = pkv20.detatch()
        pkv21 = pkv21.detatch()
        pkv30 = pkv30.detatch()
        pkv31 = pkv31.detatch()
        pkv40 = pkv40.detatch()
        pkv41 = pkv41.detatch()
        pkv50 = pkv50.detatch()
        pkv51 = pkv51.detatch()
        pkv60 = pkv60.detatch()
        pkv61 = pkv61.detatch()
        pkv70 = pkv70.detatch()
        pkv71 = pkv71.detatch()

        output = self.model(
            "forward",
            (
                hidden_states,
                attention_mask,
                position_ids,
                pkv00,
                pkv01,
                pkv10,
                pkv11,
                pkv20,
                pkv21,
                pkv30,
                pkv31,
                pkv40,
                pkv41,
                pkv50,
                pkv51,
                pkv60,
                pkv61,
                pkv70,
                pkv71,
            ),
            send_to_host=False,
        )
        return (
            output[0],
            (output[1][0], output[1][1]),
            (output[2][0], output[2][1]),
            (output[3][0], output[3][1]),
            (output[4][0], output[4][1]),
            (output[5][0], output[5][1]),
            (output[6][0], output[6][1]),
            (output[7][0], output[7][1]),
            (output[8][0], output[8][1]),
        )


def forward_compressed(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
):
    output_attentions = (
        output_attentions
        if output_attentions is not None
        else self.config.output_attentions
    )
    output_hidden_states = (
        output_hidden_states
        if output_hidden_states is not None
        else self.config.output_hidden_states
    )
    use_cache = use_cache if use_cache is not None else self.config.use_cache

    return_dict = (
        return_dict if return_dict is not None else self.config.use_return_dict
    )

    # retrieve input_ids and inputs_embeds
    if input_ids is not None and inputs_embeds is not None:
        raise ValueError(
            "You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"
        )
    elif input_ids is not None:
        batch_size, seq_length = input_ids.shape
    elif inputs_embeds is not None:
        batch_size, seq_length, _ = inputs_embeds.shape
    else:
        raise ValueError(
            "You have to specify either decoder_input_ids or decoder_inputs_embeds"
        )

    seq_length_with_past = seq_length
    past_key_values_length = 0

    if past_key_values is not None:
        past_key_values_length = past_key_values[0][0].shape[2]
        seq_length_with_past = seq_length_with_past + past_key_values_length

    if position_ids is None:
        device = (
            input_ids.device if input_ids is not None else inputs_embeds.device
        )
        position_ids = torch.arange(
            past_key_values_length,
            seq_length + past_key_values_length,
            dtype=torch.long,
            device=device,
        )
        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
    else:
        position_ids = position_ids.view(-1, seq_length).long()

    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    # embed positions
    if attention_mask is None:
        attention_mask = torch.ones(
            (batch_size, seq_length_with_past),
            dtype=torch.bool,
            device=inputs_embeds.device,
        )
    attention_mask = self._prepare_decoder_attention_mask(
        attention_mask,
        (batch_size, seq_length),
        inputs_embeds,
        past_key_values_length,
    )

    hidden_states = inputs_embeds

    # decoder layers
    all_hidden_states = () if output_hidden_states else None
    all_self_attns = () if output_attentions else None
    next_decoder_cache = () if use_cache else None

    for idx, decoder_layer in enumerate(self.compressedlayers):
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        past_key_value = (
            past_key_values[8 * idx : 8 * (idx + 1)]
            if past_key_values is not None
            else None
        )

        if self.gradient_checkpointing and self.training:

            def create_custom_forward(module):
                def custom_forward(*inputs):
                    # None for past_key_value
                    return module(*inputs, output_attentions, None)

                return custom_forward

            layer_outputs = torch.utils.checkpoint.checkpoint(
                create_custom_forward(decoder_layer),
                hidden_states,
                attention_mask,
                position_ids,
                None,
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )

        hidden_states = layer_outputs[0]

        if use_cache:
            next_decoder_cache += (
                layer_outputs[2 if output_attentions else 1],
            )

        if output_attentions:
            all_self_attns += (layer_outputs[1],)

    hidden_states = self.norm(hidden_states)

    # add hidden states from the last decoder layer
    if output_hidden_states:
        all_hidden_states += (hidden_states,)

    next_cache = next_decoder_cache if use_cache else None
    if not return_dict:
        return tuple(
            v
            for v in [
                hidden_states,
                next_cache,
                all_hidden_states,
                all_self_attns,
            ]
            if v is not None
        )
    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        hidden_states=all_hidden_states,
        attentions=all_self_attns,
    )


class CompiledEightLayerLayer(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(
        self,
        hidden_states,
        attention_mask,
        position_ids,
        past_key_value=None,
        output_attentions=False,
        use_cache=True,
    ):
        t2 = time()
        if past_key_value is None:
            try:
                hidden_states = np.asarray(hidden_states, hidden_states.dtype)
            except:
                pass
            attention_mask = attention_mask.detach()
            position_ids = position_ids.detach()
            t1 = time()

            output = self.model(
                "first_vicuna_forward",
                (hidden_states, attention_mask, position_ids),
                send_to_host=False,
            )
            output2 = (
                output[0],
                (
                    output[1],
                    output[2],
                ),
                (
                    output[3],
                    output[4],
                ),
                (
                    output[5],
                    output[6],
                ),
                (
                    output[7],
                    output[8],
                ),
                (
                    output[9],
                    output[10],
                ),
                (
                    output[11],
                    output[12],
                ),
                (
                    output[13],
                    output[14],
                ),
                (
                    output[15],
                    output[16],
                ),
            )
            return output2
        else:
            (
                (pkv00, pkv01),
                (pkv10, pkv11),
                (pkv20, pkv21),
                (pkv30, pkv31),
                (pkv40, pkv41),
                (pkv50, pkv51),
                (pkv60, pkv61),
                (pkv70, pkv71),
            ) = past_key_value

            try:
                hidden_states = hidden_states.detach()
                attention_mask = attention_mask.detach()
                position_ids = position_ids.detach()
                pkv00 = pkv00.detach()
                pkv01 = pkv01.detach()
                pkv10 = pkv10.detach()
                pkv11 = pkv11.detach()
                pkv20 = pkv20.detach()
                pkv21 = pkv21.detach()
                pkv30 = pkv30.detach()
                pkv31 = pkv31.detach()
                pkv40 = pkv40.detach()
                pkv41 = pkv41.detach()
                pkv50 = pkv50.detach()
                pkv51 = pkv51.detach()
                pkv60 = pkv60.detach()
                pkv61 = pkv61.detach()
                pkv70 = pkv70.detach()
                pkv71 = pkv71.detach()
            except:
                x = 10

            t1 = time()
            if type(hidden_states) == iree.runtime.array_interop.DeviceArray:
                hidden_states = np.array(hidden_states, hidden_states.dtype)
                hidden_states = torch.tensor(hidden_states)
                hidden_states = hidden_states.detach()

            output = self.model(
                "second_vicuna_forward",
                (
                    hidden_states,
                    attention_mask,
                    position_ids,
                    pkv00,
                    pkv01,
                    pkv10,
                    pkv11,
                    pkv20,
                    pkv21,
                    pkv30,
                    pkv31,
                    pkv40,
                    pkv41,
                    pkv50,
                    pkv51,
                    pkv60,
                    pkv61,
                    pkv70,
                    pkv71,
                ),
                send_to_host=False,
            )
            print(f"{time() - t1}")
            del pkv00
            del pkv01
            del pkv10
            del pkv11
            del pkv20
            del pkv21
            del pkv30
            del pkv31
            del pkv40
            del pkv41
            del pkv50
            del pkv51
            del pkv60
            del pkv61
            del pkv70
            del pkv71
            output2 = (
                output[0],
                (
                    output[1],
                    output[2],
                ),
                (
                    output[3],
                    output[4],
                ),
                (
                    output[5],
                    output[6],
                ),
                (
                    output[7],
                    output[8],
                ),
                (
                    output[9],
                    output[10],
                ),
                (
                    output[11],
                    output[12],
                ),
                (
                    output[13],
                    output[14],
                ),
                (
                    output[15],
                    output[16],
                ),
            )
            return output2

```

`apps/language_models/src/model_wrappers/vicuna_model.py`:

```py
import torch
from transformers import AutoModelForCausalLM


class FirstVicuna(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="fp32",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        print(f"[DEBUG] model_path : {model_path}")
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("First Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(self, input_ids):
        op = self.model(input_ids=input_ids, use_cache=True)
        return_vals = []
        token = torch.argmax(op.logits[:, -1, :], dim=1)
        return_vals.append(token)

        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SecondVicuna7B(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="fp32",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        print(f"[DEBUG] model_path : {model_path}")
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("Second Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(
        self,
        i0,
        i1,
        i2,
        i3,
        i4,
        i5,
        i6,
        i7,
        i8,
        i9,
        i10,
        i11,
        i12,
        i13,
        i14,
        i15,
        i16,
        i17,
        i18,
        i19,
        i20,
        i21,
        i22,
        i23,
        i24,
        i25,
        i26,
        i27,
        i28,
        i29,
        i30,
        i31,
        i32,
        i33,
        i34,
        i35,
        i36,
        i37,
        i38,
        i39,
        i40,
        i41,
        i42,
        i43,
        i44,
        i45,
        i46,
        i47,
        i48,
        i49,
        i50,
        i51,
        i52,
        i53,
        i54,
        i55,
        i56,
        i57,
        i58,
        i59,
        i60,
        i61,
        i62,
        i63,
        i64,
    ):
        token = i0
        past_key_values = (
            (i1, i2),
            (
                i3,
                i4,
            ),
            (
                i5,
                i6,
            ),
            (
                i7,
                i8,
            ),
            (
                i9,
                i10,
            ),
            (
                i11,
                i12,
            ),
            (
                i13,
                i14,
            ),
            (
                i15,
                i16,
            ),
            (
                i17,
                i18,
            ),
            (
                i19,
                i20,
            ),
            (
                i21,
                i22,
            ),
            (
                i23,
                i24,
            ),
            (
                i25,
                i26,
            ),
            (
                i27,
                i28,
            ),
            (
                i29,
                i30,
            ),
            (
                i31,
                i32,
            ),
            (
                i33,
                i34,
            ),
            (
                i35,
                i36,
            ),
            (
                i37,
                i38,
            ),
            (
                i39,
                i40,
            ),
            (
                i41,
                i42,
            ),
            (
                i43,
                i44,
            ),
            (
                i45,
                i46,
            ),
            (
                i47,
                i48,
            ),
            (
                i49,
                i50,
            ),
            (
                i51,
                i52,
            ),
            (
                i53,
                i54,
            ),
            (
                i55,
                i56,
            ),
            (
                i57,
                i58,
            ),
            (
                i59,
                i60,
            ),
            (
                i61,
                i62,
            ),
            (
                i63,
                i64,
            ),
        )
        op = self.model(
            input_ids=token, use_cache=True, past_key_values=past_key_values
        )
        return_vals = []
        token = torch.argmax(op.logits[:, -1, :], dim=1)
        return_vals.append(token)
        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SecondVicuna13B(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="int8",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("Second Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(
        self,
        i0,
        i1,
        i2,
        i3,
        i4,
        i5,
        i6,
        i7,
        i8,
        i9,
        i10,
        i11,
        i12,
        i13,
        i14,
        i15,
        i16,
        i17,
        i18,
        i19,
        i20,
        i21,
        i22,
        i23,
        i24,
        i25,
        i26,
        i27,
        i28,
        i29,
        i30,
        i31,
        i32,
        i33,
        i34,
        i35,
        i36,
        i37,
        i38,
        i39,
        i40,
        i41,
        i42,
        i43,
        i44,
        i45,
        i46,
        i47,
        i48,
        i49,
        i50,
        i51,
        i52,
        i53,
        i54,
        i55,
        i56,
        i57,
        i58,
        i59,
        i60,
        i61,
        i62,
        i63,
        i64,
        i65,
        i66,
        i67,
        i68,
        i69,
        i70,
        i71,
        i72,
        i73,
        i74,
        i75,
        i76,
        i77,
        i78,
        i79,
        i80,
    ):
        token = i0
        past_key_values = (
            (i1, i2),
            (
                i3,
                i4,
            ),
            (
                i5,
                i6,
            ),
            (
                i7,
                i8,
            ),
            (
                i9,
                i10,
            ),
            (
                i11,
                i12,
            ),
            (
                i13,
                i14,
            ),
            (
                i15,
                i16,
            ),
            (
                i17,
                i18,
            ),
            (
                i19,
                i20,
            ),
            (
                i21,
                i22,
            ),
            (
                i23,
                i24,
            ),
            (
                i25,
                i26,
            ),
            (
                i27,
                i28,
            ),
            (
                i29,
                i30,
            ),
            (
                i31,
                i32,
            ),
            (
                i33,
                i34,
            ),
            (
                i35,
                i36,
            ),
            (
                i37,
                i38,
            ),
            (
                i39,
                i40,
            ),
            (
                i41,
                i42,
            ),
            (
                i43,
                i44,
            ),
            (
                i45,
                i46,
            ),
            (
                i47,
                i48,
            ),
            (
                i49,
                i50,
            ),
            (
                i51,
                i52,
            ),
            (
                i53,
                i54,
            ),
            (
                i55,
                i56,
            ),
            (
                i57,
                i58,
            ),
            (
                i59,
                i60,
            ),
            (
                i61,
                i62,
            ),
            (
                i63,
                i64,
            ),
            (
                i65,
                i66,
            ),
            (
                i67,
                i68,
            ),
            (
                i69,
                i70,
            ),
            (
                i71,
                i72,
            ),
            (
                i73,
                i74,
            ),
            (
                i75,
                i76,
            ),
            (
                i77,
                i78,
            ),
            (
                i79,
                i80,
            ),
        )
        op = self.model(
            input_ids=token, use_cache=True, past_key_values=past_key_values
        )
        return_vals = []
        return_vals.append(op.logits)
        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SecondVicuna70B(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="fp32",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        print(f"[DEBUG] model_path : {model_path}")
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("Second Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(
        self,
        i0,
        i1,
        i2,
        i3,
        i4,
        i5,
        i6,
        i7,
        i8,
        i9,
        i10,
        i11,
        i12,
        i13,
        i14,
        i15,
        i16,
        i17,
        i18,
        i19,
        i20,
        i21,
        i22,
        i23,
        i24,
        i25,
        i26,
        i27,
        i28,
        i29,
        i30,
        i31,
        i32,
        i33,
        i34,
        i35,
        i36,
        i37,
        i38,
        i39,
        i40,
        i41,
        i42,
        i43,
        i44,
        i45,
        i46,
        i47,
        i48,
        i49,
        i50,
        i51,
        i52,
        i53,
        i54,
        i55,
        i56,
        i57,
        i58,
        i59,
        i60,
        i61,
        i62,
        i63,
        i64,
        i65,
        i66,
        i67,
        i68,
        i69,
        i70,
        i71,
        i72,
        i73,
        i74,
        i75,
        i76,
        i77,
        i78,
        i79,
        i80,
        i81,
        i82,
        i83,
        i84,
        i85,
        i86,
        i87,
        i88,
        i89,
        i90,
        i91,
        i92,
        i93,
        i94,
        i95,
        i96,
        i97,
        i98,
        i99,
        i100,
        i101,
        i102,
        i103,
        i104,
        i105,
        i106,
        i107,
        i108,
        i109,
        i110,
        i111,
        i112,
        i113,
        i114,
        i115,
        i116,
        i117,
        i118,
        i119,
        i120,
        i121,
        i122,
        i123,
        i124,
        i125,
        i126,
        i127,
        i128,
        i129,
        i130,
        i131,
        i132,
        i133,
        i134,
        i135,
        i136,
        i137,
        i138,
        i139,
        i140,
        i141,
        i142,
        i143,
        i144,
        i145,
        i146,
        i147,
        i148,
        i149,
        i150,
        i151,
        i152,
        i153,
        i154,
        i155,
        i156,
        i157,
        i158,
        i159,
        i160,
    ):
        token = i0
        past_key_values = (
            (i1, i2),
            (
                i3,
                i4,
            ),
            (
                i5,
                i6,
            ),
            (
                i7,
                i8,
            ),
            (
                i9,
                i10,
            ),
            (
                i11,
                i12,
            ),
            (
                i13,
                i14,
            ),
            (
                i15,
                i16,
            ),
            (
                i17,
                i18,
            ),
            (
                i19,
                i20,
            ),
            (
                i21,
                i22,
            ),
            (
                i23,
                i24,
            ),
            (
                i25,
                i26,
            ),
            (
                i27,
                i28,
            ),
            (
                i29,
                i30,
            ),
            (
                i31,
                i32,
            ),
            (
                i33,
                i34,
            ),
            (
                i35,
                i36,
            ),
            (
                i37,
                i38,
            ),
            (
                i39,
                i40,
            ),
            (
                i41,
                i42,
            ),
            (
                i43,
                i44,
            ),
            (
                i45,
                i46,
            ),
            (
                i47,
                i48,
            ),
            (
                i49,
                i50,
            ),
            (
                i51,
                i52,
            ),
            (
                i53,
                i54,
            ),
            (
                i55,
                i56,
            ),
            (
                i57,
                i58,
            ),
            (
                i59,
                i60,
            ),
            (
                i61,
                i62,
            ),
            (
                i63,
                i64,
            ),
            (
                i65,
                i66,
            ),
            (
                i67,
                i68,
            ),
            (
                i69,
                i70,
            ),
            (
                i71,
                i72,
            ),
            (
                i73,
                i74,
            ),
            (
                i75,
                i76,
            ),
            (
                i77,
                i78,
            ),
            (
                i79,
                i80,
            ),
            (
                i81,
                i82,
            ),
            (
                i83,
                i84,
            ),
            (
                i85,
                i86,
            ),
            (
                i87,
                i88,
            ),
            (
                i89,
                i90,
            ),
            (
                i91,
                i92,
            ),
            (
                i93,
                i94,
            ),
            (
                i95,
                i96,
            ),
            (
                i97,
                i98,
            ),
            (
                i99,
                i100,
            ),
            (
                i101,
                i102,
            ),
            (
                i103,
                i104,
            ),
            (
                i105,
                i106,
            ),
            (
                i107,
                i108,
            ),
            (
                i109,
                i110,
            ),
            (
                i111,
                i112,
            ),
            (
                i113,
                i114,
            ),
            (
                i115,
                i116,
            ),
            (
                i117,
                i118,
            ),
            (
                i119,
                i120,
            ),
            (
                i121,
                i122,
            ),
            (
                i123,
                i124,
            ),
            (
                i125,
                i126,
            ),
            (
                i127,
                i128,
            ),
            (
                i129,
                i130,
            ),
            (
                i131,
                i132,
            ),
            (
                i133,
                i134,
            ),
            (
                i135,
                i136,
            ),
            (
                i137,
                i138,
            ),
            (
                i139,
                i140,
            ),
            (
                i141,
                i142,
            ),
            (
                i143,
                i144,
            ),
            (
                i145,
                i146,
            ),
            (
                i147,
                i148,
            ),
            (
                i149,
                i150,
            ),
            (
                i151,
                i152,
            ),
            (
                i153,
                i154,
            ),
            (
                i155,
                i156,
            ),
            (
                i157,
                i158,
            ),
            (
                i159,
                i160,
            ),
        )
        op = self.model(
            input_ids=token, use_cache=True, past_key_values=past_key_values
        )
        return_vals = []
        return_vals.append(op.logits)
        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class CombinedModel(torch.nn.Module):
    def __init__(
        self,
        first_vicuna_model_path="TheBloke/vicuna-7B-1.1-HF",
        second_vicuna_model_path="TheBloke/vicuna-7B-1.1-HF",
    ):
        super().__init__()
        self.first_vicuna = FirstVicuna(first_vicuna_model_path)
        # NOT using this path for 13B currently, hence using `SecondVicuna7B`.
        self.second_vicuna = SecondVicuna7B(second_vicuna_model_path)

    def forward(self, input_ids):
        first_output = self.first_vicuna(input_ids=input_ids)
        # generate second vicuna
        compilation_input_ids = torch.zeros([1, 1], dtype=torch.int64)
        pkv = tuple(
            (torch.zeros([1, 32, 19, 128], dtype=torch.float32))
            for _ in range(64)
        )
        secondVicunaCompileInput = (compilation_input_ids,) + pkv
        second_output = self.second_vicuna(*secondVicunaCompileInput)
        return second_output

```

`apps/language_models/src/model_wrappers/vicuna_model_gpu.py`:

```py
import torch
from transformers import AutoModelForCausalLM


class FirstVicunaGPU(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="fp32",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        print(f"[DEBUG] model_path : {model_path}")
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("First Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(self, input_ids):
        op = self.model(input_ids=input_ids, use_cache=True)
        return_vals = []
        return_vals.append(op.logits)

        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SecondVicuna7BGPU(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="fp32",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        print(f"[DEBUG] model_path : {model_path}")
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("Second Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(
        self,
        i0,
        i1,
        i2,
        i3,
        i4,
        i5,
        i6,
        i7,
        i8,
        i9,
        i10,
        i11,
        i12,
        i13,
        i14,
        i15,
        i16,
        i17,
        i18,
        i19,
        i20,
        i21,
        i22,
        i23,
        i24,
        i25,
        i26,
        i27,
        i28,
        i29,
        i30,
        i31,
        i32,
        i33,
        i34,
        i35,
        i36,
        i37,
        i38,
        i39,
        i40,
        i41,
        i42,
        i43,
        i44,
        i45,
        i46,
        i47,
        i48,
        i49,
        i50,
        i51,
        i52,
        i53,
        i54,
        i55,
        i56,
        i57,
        i58,
        i59,
        i60,
        i61,
        i62,
        i63,
        i64,
    ):
        token = i0
        past_key_values = (
            (i1, i2),
            (
                i3,
                i4,
            ),
            (
                i5,
                i6,
            ),
            (
                i7,
                i8,
            ),
            (
                i9,
                i10,
            ),
            (
                i11,
                i12,
            ),
            (
                i13,
                i14,
            ),
            (
                i15,
                i16,
            ),
            (
                i17,
                i18,
            ),
            (
                i19,
                i20,
            ),
            (
                i21,
                i22,
            ),
            (
                i23,
                i24,
            ),
            (
                i25,
                i26,
            ),
            (
                i27,
                i28,
            ),
            (
                i29,
                i30,
            ),
            (
                i31,
                i32,
            ),
            (
                i33,
                i34,
            ),
            (
                i35,
                i36,
            ),
            (
                i37,
                i38,
            ),
            (
                i39,
                i40,
            ),
            (
                i41,
                i42,
            ),
            (
                i43,
                i44,
            ),
            (
                i45,
                i46,
            ),
            (
                i47,
                i48,
            ),
            (
                i49,
                i50,
            ),
            (
                i51,
                i52,
            ),
            (
                i53,
                i54,
            ),
            (
                i55,
                i56,
            ),
            (
                i57,
                i58,
            ),
            (
                i59,
                i60,
            ),
            (
                i61,
                i62,
            ),
            (
                i63,
                i64,
            ),
        )
        op = self.model(
            input_ids=token, use_cache=True, past_key_values=past_key_values
        )
        return_vals = []
        return_vals.append(op.logits)
        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SecondVicuna13BGPU(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="int8",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("Second Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(
        self,
        i0,
        i1,
        i2,
        i3,
        i4,
        i5,
        i6,
        i7,
        i8,
        i9,
        i10,
        i11,
        i12,
        i13,
        i14,
        i15,
        i16,
        i17,
        i18,
        i19,
        i20,
        i21,
        i22,
        i23,
        i24,
        i25,
        i26,
        i27,
        i28,
        i29,
        i30,
        i31,
        i32,
        i33,
        i34,
        i35,
        i36,
        i37,
        i38,
        i39,
        i40,
        i41,
        i42,
        i43,
        i44,
        i45,
        i46,
        i47,
        i48,
        i49,
        i50,
        i51,
        i52,
        i53,
        i54,
        i55,
        i56,
        i57,
        i58,
        i59,
        i60,
        i61,
        i62,
        i63,
        i64,
        i65,
        i66,
        i67,
        i68,
        i69,
        i70,
        i71,
        i72,
        i73,
        i74,
        i75,
        i76,
        i77,
        i78,
        i79,
        i80,
    ):
        token = i0
        past_key_values = (
            (i1, i2),
            (
                i3,
                i4,
            ),
            (
                i5,
                i6,
            ),
            (
                i7,
                i8,
            ),
            (
                i9,
                i10,
            ),
            (
                i11,
                i12,
            ),
            (
                i13,
                i14,
            ),
            (
                i15,
                i16,
            ),
            (
                i17,
                i18,
            ),
            (
                i19,
                i20,
            ),
            (
                i21,
                i22,
            ),
            (
                i23,
                i24,
            ),
            (
                i25,
                i26,
            ),
            (
                i27,
                i28,
            ),
            (
                i29,
                i30,
            ),
            (
                i31,
                i32,
            ),
            (
                i33,
                i34,
            ),
            (
                i35,
                i36,
            ),
            (
                i37,
                i38,
            ),
            (
                i39,
                i40,
            ),
            (
                i41,
                i42,
            ),
            (
                i43,
                i44,
            ),
            (
                i45,
                i46,
            ),
            (
                i47,
                i48,
            ),
            (
                i49,
                i50,
            ),
            (
                i51,
                i52,
            ),
            (
                i53,
                i54,
            ),
            (
                i55,
                i56,
            ),
            (
                i57,
                i58,
            ),
            (
                i59,
                i60,
            ),
            (
                i61,
                i62,
            ),
            (
                i63,
                i64,
            ),
            (
                i65,
                i66,
            ),
            (
                i67,
                i68,
            ),
            (
                i69,
                i70,
            ),
            (
                i71,
                i72,
            ),
            (
                i73,
                i74,
            ),
            (
                i75,
                i76,
            ),
            (
                i77,
                i78,
            ),
            (
                i79,
                i80,
            ),
        )
        op = self.model(
            input_ids=token, use_cache=True, past_key_values=past_key_values
        )
        return_vals = []
        return_vals.append(op.logits)
        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class SecondVicuna70BGPU(torch.nn.Module):
    def __init__(
        self,
        model_path,
        precision="fp32",
        accumulates="fp32",
        weight_group_size=128,
        model_name="vicuna",
        hf_auth_token: str = None,
    ):
        super().__init__()
        kwargs = {"torch_dtype": torch.float32}
        if "llama2" in model_name:
            kwargs["use_auth_token"] = hf_auth_token
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, low_cpu_mem_usage=True, **kwargs
        )
        self.accumulates = (
            torch.float32 if accumulates == "fp32" else torch.float16
        )
        print(f"[DEBUG] model_path : {model_path}")
        if precision in ["int4", "int8"]:
            from brevitas_examples.common.generative.quantize import (
                quantize_model,
            )
            from brevitas_examples.llm.llm_quant.run_utils import (
                get_model_impl,
            )

            print("Second Vicuna applying weight quantization..")
            weight_bit_width = 4 if precision == "int4" else 8
            quantize_model(
                get_model_impl(self.model).layers,
                dtype=self.accumulates,
                weight_bit_width=weight_bit_width,
                weight_param_method="stats",
                weight_scale_precision="float_scale",
                weight_quant_type="asym",
                weight_quant_granularity="per_group",
                weight_group_size=weight_group_size,
                quantize_weight_zero_point=False,
            )
            print("Weight quantization applied.")

    def forward(
        self,
        i0,
        i1,
        i2,
        i3,
        i4,
        i5,
        i6,
        i7,
        i8,
        i9,
        i10,
        i11,
        i12,
        i13,
        i14,
        i15,
        i16,
        i17,
        i18,
        i19,
        i20,
        i21,
        i22,
        i23,
        i24,
        i25,
        i26,
        i27,
        i28,
        i29,
        i30,
        i31,
        i32,
        i33,
        i34,
        i35,
        i36,
        i37,
        i38,
        i39,
        i40,
        i41,
        i42,
        i43,
        i44,
        i45,
        i46,
        i47,
        i48,
        i49,
        i50,
        i51,
        i52,
        i53,
        i54,
        i55,
        i56,
        i57,
        i58,
        i59,
        i60,
        i61,
        i62,
        i63,
        i64,
        i65,
        i66,
        i67,
        i68,
        i69,
        i70,
        i71,
        i72,
        i73,
        i74,
        i75,
        i76,
        i77,
        i78,
        i79,
        i80,
        i81,
        i82,
        i83,
        i84,
        i85,
        i86,
        i87,
        i88,
        i89,
        i90,
        i91,
        i92,
        i93,
        i94,
        i95,
        i96,
        i97,
        i98,
        i99,
        i100,
        i101,
        i102,
        i103,
        i104,
        i105,
        i106,
        i107,
        i108,
        i109,
        i110,
        i111,
        i112,
        i113,
        i114,
        i115,
        i116,
        i117,
        i118,
        i119,
        i120,
        i121,
        i122,
        i123,
        i124,
        i125,
        i126,
        i127,
        i128,
        i129,
        i130,
        i131,
        i132,
        i133,
        i134,
        i135,
        i136,
        i137,
        i138,
        i139,
        i140,
        i141,
        i142,
        i143,
        i144,
        i145,
        i146,
        i147,
        i148,
        i149,
        i150,
        i151,
        i152,
        i153,
        i154,
        i155,
        i156,
        i157,
        i158,
        i159,
        i160,
    ):
        token = i0
        past_key_values = (
            (i1, i2),
            (
                i3,
                i4,
            ),
            (
                i5,
                i6,
            ),
            (
                i7,
                i8,
            ),
            (
                i9,
                i10,
            ),
            (
                i11,
                i12,
            ),
            (
                i13,
                i14,
            ),
            (
                i15,
                i16,
            ),
            (
                i17,
                i18,
            ),
            (
                i19,
                i20,
            ),
            (
                i21,
                i22,
            ),
            (
                i23,
                i24,
            ),
            (
                i25,
                i26,
            ),
            (
                i27,
                i28,
            ),
            (
                i29,
                i30,
            ),
            (
                i31,
                i32,
            ),
            (
                i33,
                i34,
            ),
            (
                i35,
                i36,
            ),
            (
                i37,
                i38,
            ),
            (
                i39,
                i40,
            ),
            (
                i41,
                i42,
            ),
            (
                i43,
                i44,
            ),
            (
                i45,
                i46,
            ),
            (
                i47,
                i48,
            ),
            (
                i49,
                i50,
            ),
            (
                i51,
                i52,
            ),
            (
                i53,
                i54,
            ),
            (
                i55,
                i56,
            ),
            (
                i57,
                i58,
            ),
            (
                i59,
                i60,
            ),
            (
                i61,
                i62,
            ),
            (
                i63,
                i64,
            ),
            (
                i65,
                i66,
            ),
            (
                i67,
                i68,
            ),
            (
                i69,
                i70,
            ),
            (
                i71,
                i72,
            ),
            (
                i73,
                i74,
            ),
            (
                i75,
                i76,
            ),
            (
                i77,
                i78,
            ),
            (
                i79,
                i80,
            ),
            (
                i81,
                i82,
            ),
            (
                i83,
                i84,
            ),
            (
                i85,
                i86,
            ),
            (
                i87,
                i88,
            ),
            (
                i89,
                i90,
            ),
            (
                i91,
                i92,
            ),
            (
                i93,
                i94,
            ),
            (
                i95,
                i96,
            ),
            (
                i97,
                i98,
            ),
            (
                i99,
                i100,
            ),
            (
                i101,
                i102,
            ),
            (
                i103,
                i104,
            ),
            (
                i105,
                i106,
            ),
            (
                i107,
                i108,
            ),
            (
                i109,
                i110,
            ),
            (
                i111,
                i112,
            ),
            (
                i113,
                i114,
            ),
            (
                i115,
                i116,
            ),
            (
                i117,
                i118,
            ),
            (
                i119,
                i120,
            ),
            (
                i121,
                i122,
            ),
            (
                i123,
                i124,
            ),
            (
                i125,
                i126,
            ),
            (
                i127,
                i128,
            ),
            (
                i129,
                i130,
            ),
            (
                i131,
                i132,
            ),
            (
                i133,
                i134,
            ),
            (
                i135,
                i136,
            ),
            (
                i137,
                i138,
            ),
            (
                i139,
                i140,
            ),
            (
                i141,
                i142,
            ),
            (
                i143,
                i144,
            ),
            (
                i145,
                i146,
            ),
            (
                i147,
                i148,
            ),
            (
                i149,
                i150,
            ),
            (
                i151,
                i152,
            ),
            (
                i153,
                i154,
            ),
            (
                i155,
                i156,
            ),
            (
                i157,
                i158,
            ),
            (
                i159,
                i160,
            ),
        )
        op = self.model(
            input_ids=token, use_cache=True, past_key_values=past_key_values
        )
        return_vals = []
        return_vals.append(op.logits)
        temp_past_key_values = op.past_key_values
        for item in temp_past_key_values:
            return_vals.append(item[0])
            return_vals.append(item[1])
        return tuple(return_vals)


class CombinedModel(torch.nn.Module):
    def __init__(
        self,
        first_vicuna_model_path="TheBloke/vicuna-7B-1.1-HF",
        second_vicuna_model_path="TheBloke/vicuna-7B-1.1-HF",
    ):
        super().__init__()
        self.first_vicuna = FirstVicunaGPU(first_vicuna_model_path)
        # NOT using this path for 13B currently, hence using `SecondVicuna7B`.
        self.second_vicuna = SecondVicuna7BGPU(second_vicuna_model_path)

    def forward(self, input_ids):
        first_output = self.first_vicuna(input_ids=input_ids)
        # generate second vicuna
        compilation_input_ids = torch.zeros([1, 1], dtype=torch.int64)
        pkv = tuple(
            (torch.zeros([1, 32, 19, 128], dtype=torch.float32))
            for _ in range(64)
        )
        secondVicunaCompileInput = (compilation_input_ids,) + pkv
        second_output = self.second_vicuna(*secondVicunaCompileInput)
        return second_output

```

`apps/language_models/src/model_wrappers/vicuna_sharded_model.py`:

```py
import torch


class FirstVicunaLayer(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, hidden_states, attention_mask, position_ids):
        outputs = self.model(
            hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            use_cache=True,
        )
        next_hidden_states = outputs[0]
        past_key_value_out0, past_key_value_out1 = (
            outputs[-1][0],
            outputs[-1][1],
        )

        return (
            next_hidden_states,
            past_key_value_out0,
            past_key_value_out1,
        )


class SecondVicunaLayer(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(
        self,
        hidden_states,
        attention_mask,
        position_ids,
        past_key_value0,
        past_key_value1,
    ):
        outputs = self.model(
            hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=(
                past_key_value0,
                past_key_value1,
            ),
            use_cache=True,
        )
        next_hidden_states = outputs[0]
        past_key_value_out0, past_key_value_out1 = (
            outputs[-1][0],
            outputs[-1][1],
        )

        return (
            next_hidden_states,
            past_key_value_out0,
            past_key_value_out1,
        )


class ShardedVicunaModel(torch.nn.Module):
    def __init__(self, model, layers, lmhead, embedding, norm):
        super().__init__()
        self.model = model
        # assert len(layers) == len(model.model.layers)
        self.model.model.config.use_cache = True
        self.model.model.config.output_attentions = False
        self.layers = layers
        self.norm = norm
        self.embedding = embedding
        self.lmhead = lmhead
        self.model.model.norm = self.norm
        self.model.model.embed_tokens = self.embedding
        self.model.lm_head = self.lmhead
        self.model.model.layers = torch.nn.modules.container.ModuleList(
            self.layers
        )

    def forward(
        self,
        input_ids,
        is_first=True,
        past_key_values=None,
        attention_mask=None,
    ):
        return self.model.forward(
            input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
        )


class LMHead(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, hidden_states):
        output = self.model(hidden_states)
        return output


class LMHeadCompiled(torch.nn.Module):
    def __init__(self, shark_module):
        super().__init__()
        self.model = shark_module

    def forward(self, hidden_states):
        hidden_states = hidden_states.detach()
        output = self.model("forward", (hidden_states,))
        output = torch.tensor(output)
        return output


class VicunaNorm(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, hidden_states):
        output = self.model(hidden_states)
        return output


class VicunaNormCompiled(torch.nn.Module):
    def __init__(self, shark_module):
        super().__init__()
        self.model = shark_module

    def forward(self, hidden_states):
        try:
            hidden_states.detach()
        except:
            pass
        output = self.model("forward", (hidden_states,))
        output = torch.tensor(output)
        return output


class VicunaEmbedding(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids):
        output = self.model(input_ids)
        return output


class VicunaEmbeddingCompiled(torch.nn.Module):
    def __init__(self, shark_module):
        super().__init__()
        self.model = shark_module

    def forward(self, input_ids):
        input_ids.detach()
        output = self.model("forward", (input_ids,))
        output = torch.tensor(output)
        return output


class CompiledVicunaLayer(torch.nn.Module):
    def __init__(self, shark_module):
        super().__init__()
        self.model = shark_module

    def forward(
        self,
        hidden_states,
        attention_mask,
        position_ids,
        past_key_value=None,
        output_attentions=False,
        use_cache=True,
    ):
        if past_key_value is None:
            hidden_states = hidden_states.detach()
            attention_mask = attention_mask.detach()
            position_ids = position_ids.detach()
            output = self.model(
                "first_vicuna_forward",
                (
                    hidden_states,
                    attention_mask,
                    position_ids,
                ),
            )

            output0 = torch.tensor(output[0])
            output1 = torch.tensor(output[1])
            output2 = torch.tensor(output[2])

            return (
                output0,
                (
                    output1,
                    output2,
                ),
            )
        else:
            hidden_states = hidden_states.detach()
            attention_mask = attention_mask.detach()
            position_ids = position_ids.detach()
            pkv0 = past_key_value[0].detach()
            pkv1 = past_key_value[1].detach()
            output = self.model(
                "second_vicuna_forward",
                (
                    hidden_states,
                    attention_mask,
                    position_ids,
                    pkv0,
                    pkv1,
                ),
            )

            output0 = torch.tensor(output[0])
            output1 = torch.tensor(output[1])
            output2 = torch.tensor(output[2])

            return (
                output0,
                (
                    output1,
                    output2,
                ),
            )

```

`apps/language_models/src/pipelines/SharkLLMBase.py`:

```py
from abc import ABC, abstractmethod


class SharkLLMBase(ABC):
    def __init__(
        self,
        model_name,
        hf_model_path=None,
        max_num_tokens=512,
    ) -> None:
        self.model_name = model_name
        self.hf_model_path = hf_model_path
        self.max_num_tokens = max_num_tokens
        self.shark_model = None
        self.device = "cpu"
        self.precision = "fp32"

    @classmethod
    @abstractmethod
    def compile(self):
        pass

    @classmethod
    @abstractmethod
    def generate(self, prompt):
        pass

    @classmethod
    @abstractmethod
    def generate_new_token(self, params):
        pass

    @classmethod
    @abstractmethod
    def get_tokenizer(self):
        pass

    @classmethod
    @abstractmethod
    def get_src_model(self):
        pass

    def load_init_from_config(self):
        pass

```

`apps/language_models/src/pipelines/falcon_pipeline.py`:

```py
from apps.language_models.src.model_wrappers.falcon_model import FalconModel
from apps.language_models.src.model_wrappers.falcon_sharded_model import (
    WordEmbeddingsLayer,
    CompiledWordEmbeddingsLayer,
    LNFEmbeddingLayer,
    CompiledLNFEmbeddingLayer,
    LMHeadEmbeddingLayer,
    CompiledLMHeadEmbeddingLayer,
    FourWayShardingDecoderLayer,
    TwoWayShardingDecoderLayer,
    CompiledFourWayShardingDecoderLayer,
    CompiledTwoWayShardingDecoderLayer,
    ShardedFalconModel,
)
from apps.language_models.src.pipelines.SharkLLMBase import SharkLLMBase
from apps.language_models.utils import (
    get_vmfb_from_path,
)
from io import BytesIO
from pathlib import Path
from contextlib import redirect_stdout
from shark.shark_downloader import download_public_file
from shark.shark_importer import import_with_fx, save_mlir
from shark.shark_inference import SharkInference
from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig
from transformers.generation import (
    GenerationConfig,
    LogitsProcessorList,
    StoppingCriteriaList,
)
import copy
import time
import re
import torch
import torch_mlir
import os
import argparse
import gc

parser = argparse.ArgumentParser(
    prog="falcon runner",
    description="runs a falcon model",
)

parser.add_argument(
    "--falcon_variant_to_use", default="7b", help="7b, 40b, 180b"
)
parser.add_argument(
    "--compressed",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Do the compression of sharded layers",
)
parser.add_argument(
    "--precision", "-p", default="fp16", choices=["fp32", "fp16", "int4"]
)
parser.add_argument("--device", "-d", default="cuda", help="vulkan, cpu, cuda")
parser.add_argument(
    "--falcon_vmfb_path", default=None, help="path to falcon's vmfb"
)
parser.add_argument(
    "--falcon_mlir_path",
    default=None,
    help="path to falcon's mlir file",
)
parser.add_argument(
    "--use_precompiled_model",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="use the precompiled vmfb",
)
parser.add_argument(
    "--load_mlir_from_shark_tank",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="download precompile mlir from shark tank",
)
parser.add_argument(
    "--cli",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Run model in cli mode",
)
parser.add_argument(
    "--hf_auth_token",
    type=str,
    default=None,
    help="Specify your own huggingface authentication token for falcon-180B model.",
)
parser.add_argument(
    "-s",
    "--sharded",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Run model as sharded",
)
parser.add_argument(
    "--num_shards",
    type=int,
    default=4,
    choices=[2, 4],
    help="Number of shards.",
)


class ShardedFalcon(SharkLLMBase):
    def __init__(
        self,
        model_name,
        hf_model_path="tiiuae/falcon-7b-instruct",
        hf_auth_token: str = None,
        max_num_tokens=150,
        device="cuda",
        precision="fp32",
        falcon_mlir_path=None,
        falcon_vmfb_path=None,
        debug=False,
    ) -> None:
        super().__init__(model_name, hf_model_path, max_num_tokens)
        print("hf_model_path: ", self.hf_model_path)

        if (
            "180b" in self.model_name
            and precision != "int4"
            and hf_auth_token == None
        ):
            raise ValueError(
                """ HF auth token required for falcon-180b. Pass it using
                --hf_auth_token flag. You can ask for the access to the model
                here: https://huggingface.co/tiiuae/falcon-180B-chat."""
            )

        if args.sharded and "180b" not in self.model_name:
            raise ValueError("Sharding supported only for Falcon-180B")

        self.hf_auth_token = hf_auth_token
        self.max_padding_length = 100
        self.device = device
        self.precision = precision
        self.falcon_vmfb_path = falcon_vmfb_path
        self.falcon_mlir_path = falcon_mlir_path
        self.debug = debug
        self.tokenizer = self.get_tokenizer()
        self.src_model = self.get_src_model()
        self.shark_model = self.compile()

    def get_tokenizer(self):
        tokenizer = AutoTokenizer.from_pretrained(
            self.hf_model_path,
            trust_remote_code=True,
            token=self.hf_auth_token,
        )
        tokenizer.padding_side = "left"
        tokenizer.pad_token_id = 11
        return tokenizer

    def get_src_model(self):
        print("Loading src model: ", self.model_name)
        kwargs = {
            "torch_dtype": torch.float32,
            "trust_remote_code": True,
            "token": self.hf_auth_token,
        }
        if self.precision == "int4":
            quantization_config = GPTQConfig(bits=4, disable_exllama=True)
            kwargs["quantization_config"] = quantization_config
            kwargs["device_map"] = "cpu"
        falcon_model = AutoModelForCausalLM.from_pretrained(
            self.hf_model_path, **kwargs
        )
        return falcon_model

    def compile_layer(
        self, layer, falconCompileInput, layer_id, device_idx=None
    ):
        self.falcon_mlir_path = Path(
            f"falcon_{args.falcon_variant_to_use}_layer_{layer_id}_{self.precision}.mlir"
        )
        self.falcon_vmfb_path = Path(
            f"falcon_{args.falcon_variant_to_use}_layer_{layer_id}_{self.precision}_{self.device}.vmfb"
        )

        if args.use_precompiled_model:
            if not self.falcon_vmfb_path.exists():
                # Downloading VMFB from shark_tank
                print(f"[DEBUG] Trying to download vmfb from shark_tank")
                download_public_file(
                    f"gs://shark_tank/falcon/sharded/falcon_{args.falcon_variant_to_use}/vmfb/"
                    + str(self.falcon_vmfb_path),
                    self.falcon_vmfb_path.absolute(),
                    single_file=True,
                )
            vmfb = get_vmfb_from_path(
                self.falcon_vmfb_path,
                self.device,
                "linalg",
                device_id=device_idx,
            )
            if vmfb is not None:
                return vmfb, device_idx

        print(f"[DEBUG] vmfb not found at {self.falcon_vmfb_path.absolute()}")
        if self.falcon_mlir_path.exists():
            print(f"[DEBUG] mlir found at {self.falcon_mlir_path.absolute()}")
            with open(self.falcon_mlir_path, "rb") as f:
                bytecode = f.read()
        else:
            mlir_generated = False
            print(
                f"[DEBUG] mlir not found at {self.falcon_mlir_path.absolute()}"
            )
            if args.load_mlir_from_shark_tank:
                # Downloading MLIR from shark_tank
                print(f"[DEBUG] Trying to download mlir from shark_tank")
                download_public_file(
                    f"gs://shark_tank/falcon/sharded/falcon_{args.falcon_variant_to_use}/mlir/"
                    + str(self.falcon_mlir_path),
                    self.falcon_mlir_path.absolute(),
                    single_file=True,
                )
                if self.falcon_mlir_path.exists():
                    print(
                        f"[DEBUG] mlir found at {self.falcon_mlir_path.absolute()}"
                    )
                    with open(self.falcon_mlir_path, "rb") as f:
                        bytecode = f.read()
                    mlir_generated = True

            if not mlir_generated:
                print(f"[DEBUG] generating MLIR locally")
                if layer_id == "word_embeddings":
                    f16_input_mask = [False]
                elif layer_id in ["ln_f", "lm_head"]:
                    f16_input_mask = [True]
                elif "_" in layer_id or type(layer_id) == int:
                    f16_input_mask = [True, True]
                else:
                    raise ValueError("Unsupported layer: ", layer_id)

                print(f"[DEBUG] generating torchscript graph")
                ts_graph = import_with_fx(
                    layer,
                    falconCompileInput,
                    is_f16=True,
                    f16_input_mask=f16_input_mask,
                    mlir_type="torchscript",
                    is_gptq=True,
                )
                del layer

                print(f"[DEBUG] generating torch mlir")
                module = torch_mlir.compile(
                    ts_graph,
                    falconCompileInput,
                    torch_mlir.OutputType.LINALG_ON_TENSORS,
                    use_tracing=False,
                    verbose=False,
                )
                del ts_graph

                print(f"[DEBUG] converting to bytecode")
                bytecode_stream = BytesIO()
                module.operation.write_bytecode(bytecode_stream)
                bytecode = bytecode_stream.getvalue()
                del module

                f_ = open(self.falcon_mlir_path, "wb")
                f_.write(bytecode)
                print("Saved falcon mlir at ", str(self.falcon_mlir_path))
                f_.close()
                del bytecode

        shark_module = SharkInference(
            mlir_module=self.falcon_mlir_path,
            device=self.device,
            mlir_dialect="linalg",
            device_idx=device_idx,
        )
        path = shark_module.save_module(
            self.falcon_vmfb_path.parent.absolute(),
            self.falcon_vmfb_path.stem,
            extra_args=[
                "--iree-vm-target-truncate-unsupported-floats",
                "--iree-codegen-check-ir-before-llvm-conversion=false",
                "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
            ]
            + [
                "--iree-llvmcpu-use-fast-min-max-ops",
            ]
            if self.precision == "int4"
            else [],
            debug=self.debug,
        )
        print("Saved falcon vmfb at ", str(path))
        shark_module.load_module(path)

        return shark_module, device_idx

    def compile(self):
        sample_input_ids = torch.zeros([100], dtype=torch.int64)
        sample_attention_mask = torch.zeros(
            [1, 1, 100, 100], dtype=torch.float32
        )
        num_group_layers = int(
            20 * (4 / args.num_shards)
        )  # 4 is the number of default shards
        sample_hidden_states = torch.zeros(
            [1, 100, 14848], dtype=torch.float32
        )

        # Determine number of available devices
        num_devices = 1
        if self.device == "rocm":
            import iree.runtime as ireert

            haldriver = ireert.get_driver(self.device)
            num_devices = len(haldriver.query_available_devices())
            if num_devices < 2:
                raise ValueError(
                    "Cannot run Falcon-180B on a single ROCM device."
                )

        lm_head = LMHeadEmbeddingLayer(self.src_model.lm_head)
        print("Compiling Layer lm_head")
        shark_lm_head, _ = self.compile_layer(
            lm_head,
            [sample_hidden_states],
            "lm_head",
            device_idx=(0 % num_devices) % args.num_shards
            if self.device == "rocm"
            else None,
        )
        shark_lm_head = CompiledLMHeadEmbeddingLayer(shark_lm_head)

        word_embedding = WordEmbeddingsLayer(
            self.src_model.transformer.word_embeddings
        )
        print("Compiling Layer word_embeddings")
        shark_word_embedding, _ = self.compile_layer(
            word_embedding,
            [sample_input_ids],
            "word_embeddings",
            device_idx=(1 % num_devices) % args.num_shards
            if self.device == "rocm"
            else None,
        )
        shark_word_embedding = CompiledWordEmbeddingsLayer(
            shark_word_embedding
        )

        ln_f = LNFEmbeddingLayer(self.src_model.transformer.ln_f)
        print("Compiling Layer ln_f")
        shark_ln_f, _ = self.compile_layer(
            ln_f,
            [sample_hidden_states],
            "ln_f",
            device_idx=(2 % num_devices) % args.num_shards
            if self.device == "rocm"
            else None,
        )
        shark_ln_f = CompiledLNFEmbeddingLayer(shark_ln_f)

        shark_layers = []
        for i in range(
            int(len(self.src_model.transformer.h) / num_group_layers)
        ):
            device_idx = i % num_devices if self.device == "rocm" else None
            layer_id = i
            layer_id = (
                str(i * num_group_layers)
                + "_"
                + str((i + 1) * num_group_layers)
            )
            pytorch_class = FourWayShardingDecoderLayer
            compiled_class = CompiledFourWayShardingDecoderLayer
            if args.num_shards == 2:
                pytorch_class = TwoWayShardingDecoderLayer
                compiled_class = CompiledTwoWayShardingDecoderLayer

            print("Compiling Layer {}".format(layer_id))
            layer_i = self.src_model.transformer.h[
                i * num_group_layers : (i + 1) * num_group_layers
            ]

            pytorch_layer_i = pytorch_class(
                layer_i, args.falcon_variant_to_use
            )
            shark_module, device_idx = self.compile_layer(
                pytorch_layer_i,
                [sample_hidden_states, sample_attention_mask],
                layer_id,
                device_idx=device_idx,
            )
            shark_layer_i = compiled_class(
                layer_id,
                device_idx,
                args.falcon_variant_to_use,
                self.device,
                self.precision,
                shark_module,
            )
            shark_layers.append(shark_layer_i)

        sharded_model = ShardedFalconModel(
            self.src_model,
            shark_layers,
            shark_word_embedding,
            shark_ln_f,
            shark_lm_head,
        )
        return sharded_model

    def generate(self, prompt):
        model_inputs = self.tokenizer(
            prompt,
            padding="max_length",
            max_length=self.max_padding_length,
            add_special_tokens=False,
            return_tensors="pt",
        )
        model_inputs["prompt_text"] = prompt

        input_ids = model_inputs["input_ids"]
        attention_mask = model_inputs.get("attention_mask", None)

        # Allow empty prompts
        if input_ids.shape[1] == 0:
            input_ids = None
            attention_mask = None

        generate_kwargs = {
            "max_length": self.max_num_tokens,
            "do_sample": True,
            "top_k": 10,
            "num_return_sequences": 1,
            "eos_token_id": 11,
        }
        generate_kwargs["input_ids"] = input_ids
        generate_kwargs["attention_mask"] = attention_mask
        generation_config_ = GenerationConfig.from_model_config(
            self.src_model.config
        )
        generation_config = copy.deepcopy(generation_config_)
        model_kwargs = generation_config.update(**generate_kwargs)

        logits_processor = LogitsProcessorList()
        stopping_criteria = StoppingCriteriaList()

        eos_token_id = generation_config.eos_token_id
        generation_config.pad_token_id = eos_token_id

        (
            inputs_tensor,
            model_input_name,
            model_kwargs,
        ) = self.src_model._prepare_model_inputs(
            None, generation_config.bos_token_id, model_kwargs
        )

        model_kwargs["output_attentions"] = generation_config.output_attentions
        model_kwargs[
            "output_hidden_states"
        ] = generation_config.output_hidden_states
        model_kwargs["use_cache"] = generation_config.use_cache

        input_ids = (
            inputs_tensor
            if model_input_name == "input_ids"
            else model_kwargs.pop("input_ids")
        )

        self.logits_processor = self.src_model._get_logits_processor(
            generation_config=generation_config,
            input_ids_seq_length=input_ids.shape[-1],
            encoder_input_ids=inputs_tensor,
            prefix_allowed_tokens_fn=None,
            logits_processor=logits_processor,
        )

        self.stopping_criteria = self.src_model._get_stopping_criteria(
            generation_config=generation_config,
            stopping_criteria=stopping_criteria,
        )

        self.logits_warper = self.src_model._get_logits_warper(
            generation_config
        )

        (
            self.input_ids,
            self.model_kwargs,
        ) = self.src_model._expand_inputs_for_generation(
            input_ids=input_ids,
            expand_size=generation_config.num_return_sequences,  # 1
            is_encoder_decoder=self.src_model.config.is_encoder_decoder,  # False
            **model_kwargs,
        )

        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]
        self.eos_token_id_tensor = (
            torch.tensor(eos_token_id) if eos_token_id is not None else None
        )

        self.pad_token_id = generation_config.pad_token_id
        self.eos_token_id = eos_token_id

        output_scores = generation_config.output_scores  # False
        return_dict_in_generate = (
            generation_config.return_dict_in_generate  # False
        )

        # init attention / hidden states / scores tuples
        self.scores = (
            () if (return_dict_in_generate and output_scores) else None
        )

        # keep track of which sequences are already finished
        self.unfinished_sequences = torch.ones(
            input_ids.shape[0], dtype=torch.long, device=input_ids.device
        )

        all_text = prompt

        start = time.time()
        count = 0
        for i in range(self.max_num_tokens - 1):
            count = count + 1

            next_token = self.generate_new_token()
            new_word = self.tokenizer.decode(
                next_token.cpu().numpy(),
                add_special_tokens=False,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True,
            )

            all_text = all_text + new_word

            print(f"{new_word}", end="", flush=True)
            print(f"{all_text}", end="", flush=True)

            # if eos_token was found in one sentence, set sentence to finished
            if self.eos_token_id_tensor is not None:
                self.unfinished_sequences = self.unfinished_sequences.mul(
                    next_token.tile(self.eos_token_id_tensor.shape[0], 1)
                    .ne(self.eos_token_id_tensor.unsqueeze(1))
                    .prod(dim=0)
                )
                # stop when each sentence is finished
                if (
                    self.unfinished_sequences.max() == 0
                    or self.stopping_criteria(input_ids, self.scores)
                ):
                    break

        end = time.time()
        print(
            "\n\nTime taken is {:.2f} seconds/token\n".format(
                (end - start) / count
            )
        )

        torch.cuda.empty_cache()
        gc.collect()

        return all_text

    def generate_new_token(self):
        model_inputs = self.src_model.prepare_inputs_for_generation(
            self.input_ids, **self.model_kwargs
        )
        outputs = self.shark_model.forward(
            input_ids=model_inputs["input_ids"],
            attention_mask=model_inputs["attention_mask"],
        )
        if self.precision in ["fp16", "int4"]:
            outputs = outputs.to(dtype=torch.float32)
        next_token_logits = outputs

        # pre-process distribution
        next_token_scores = self.logits_processor(
            self.input_ids, next_token_logits
        )
        next_token_scores = self.logits_warper(
            self.input_ids, next_token_scores
        )

        # sample
        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)

        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)

        # finished sentences should have their next token be a padding token
        if self.eos_token_id is not None:
            if self.pad_token_id is None:
                raise ValueError(
                    "If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
                )
            next_token = (
                next_token * self.unfinished_sequences
                + self.pad_token_id * (1 - self.unfinished_sequences)
            )

        self.input_ids = torch.cat(
            [self.input_ids, next_token[:, None]], dim=-1
        )

        self.model_kwargs["past_key_values"] = None
        if "attention_mask" in self.model_kwargs:
            attention_mask = self.model_kwargs["attention_mask"]
            self.model_kwargs["attention_mask"] = torch.cat(
                [
                    attention_mask,
                    attention_mask.new_ones((attention_mask.shape[0], 1)),
                ],
                dim=-1,
            )

        self.input_ids = self.input_ids[:, 1:]
        self.model_kwargs["attention_mask"] = self.model_kwargs[
            "attention_mask"
        ][:, 1:]

        return next_token


class UnshardedFalcon(SharkLLMBase):
    def __init__(
        self,
        model_name,
        hf_model_path="tiiuae/falcon-7b-instruct",
        hf_auth_token: str = "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",
        max_num_tokens=150,
        device="cuda",
        precision="fp32",
        falcon_mlir_path=None,
        falcon_vmfb_path=None,
        debug=False,
    ) -> None:
        super().__init__(model_name, hf_model_path, max_num_tokens)
        print("hf_model_path: ", self.hf_model_path)

        if "180b" in self.model_name and hf_auth_token == None:
            raise ValueError(
                """ HF auth token required for falcon-180b. Pass it using
                --hf_auth_token flag. You can ask for the access to the model
                here: https://huggingface.co/tiiuae/falcon-180B-chat."""
            )
        self.hf_auth_token = hf_auth_token
        self.max_padding_length = 100
        self.device = device
        self.precision = precision
        self.falcon_vmfb_path = falcon_vmfb_path
        self.falcon_mlir_path = falcon_mlir_path
        self.debug = debug
        self.tokenizer = self.get_tokenizer()
        self.src_model = self.get_src_model()
        self.shark_model = self.compile()

    def get_tokenizer(self):
        tokenizer = AutoTokenizer.from_pretrained(
            self.hf_model_path,
            trust_remote_code=True,
            token=self.hf_auth_token,
        )
        tokenizer.padding_side = "left"
        tokenizer.pad_token_id = 11
        return tokenizer

    def get_src_model(self):
        print("Loading src model: ", self.model_name)
        kwargs = {
            "torch_dtype": torch.float32,
            "trust_remote_code": True,
            "token": self.hf_auth_token,
        }
        if self.precision == "int4":
            quantization_config = GPTQConfig(bits=4, disable_exllama=True)
            kwargs["quantization_config"] = quantization_config
            kwargs["device_map"] = "cpu"
        falcon_model = AutoModelForCausalLM.from_pretrained(
            self.hf_model_path, **kwargs
        )
        return falcon_model

    def compile(self):
        if args.use_precompiled_model:
            if not self.falcon_vmfb_path.exists():
                # Downloading VMFB from shark_tank
                download_public_file(
                    "gs://shark_tank/falcon/"
                    + "falcon_"
                    + args.falcon_variant_to_use
                    + "_"
                    + self.precision
                    + "_"
                    + self.device
                    + ".vmfb",
                    self.falcon_vmfb_path.absolute(),
                    single_file=True,
                )
            vmfb = get_vmfb_from_path(
                self.falcon_vmfb_path, self.device, "linalg"
            )
            if vmfb is not None:
                return vmfb

        print(f"[DEBUG] vmfb not found at {self.falcon_vmfb_path.absolute()}")
        if self.falcon_mlir_path.exists():
            print(f"[DEBUG] mlir found at {self.falcon_mlir_path.absolute()}")
            with open(self.falcon_mlir_path, "rb") as f:
                bytecode = f.read()
        else:
            mlir_generated = False
            print(
                f"[DEBUG] mlir not found at {self.falcon_mlir_path.absolute()}"
            )
            if args.load_mlir_from_shark_tank:
                # Downloading MLIR from shark_tank
                print(f"[DEBUG] Trying to download mlir from shark_tank")
                download_public_file(
                    "gs://shark_tank/falcon/"
                    + "falcon_"
                    + args.falcon_variant_to_use
                    + "_"
                    + self.precision
                    + ".mlir",
                    self.falcon_mlir_path.absolute(),
                    single_file=True,
                )
                if self.falcon_mlir_path.exists():
                    print(
                        f"[DEBUG] mlir found at {self.falcon_mlir_path.absolute()}"
                    )
                    mlir_generated = True

            if not mlir_generated:
                print(f"[DEBUG] generating MLIR locally")
                compilation_input_ids = torch.randint(
                    low=1, high=10000, size=(1, 100)
                )
                compilation_attention_mask = torch.ones(
                    1, 100, dtype=torch.int64
                )
                falconCompileInput = (
                    compilation_input_ids,
                    compilation_attention_mask,
                )
                model = FalconModel(self.src_model)

                print(f"[DEBUG] generating torchscript graph")
                ts_graph = import_with_fx(
                    model,
                    falconCompileInput,
                    is_f16=self.precision in ["fp16", "int4"],
                    f16_input_mask=[False, False],
                    mlir_type="torchscript",
                    is_gptq=self.precision == "int4",
                )
                del model
                print(f"[DEBUG] generating torch mlir")

                module = torch_mlir.compile(
                    ts_graph,
                    [*falconCompileInput],
                    torch_mlir.OutputType.LINALG_ON_TENSORS,
                    use_tracing=False,
                    verbose=False,
                )
                del ts_graph

                print(f"[DEBUG] converting to bytecode")
                bytecode_stream = BytesIO()
                module.operation.write_bytecode(bytecode_stream)
                bytecode = bytecode_stream.getvalue()
                del module

                f_ = open(self.falcon_mlir_path, "wb")
                f_.write(bytecode)
                print("Saved falcon mlir at ", str(self.falcon_mlir_path))
                f_.close()
                del bytecode

        shark_module = SharkInference(
            mlir_module=self.falcon_mlir_path,
            device=self.device,
            mlir_dialect="linalg",
        )
        path = shark_module.save_module(
            self.falcon_vmfb_path.parent.absolute(),
            self.falcon_vmfb_path.stem,
            extra_args=[
                "--iree-vm-target-truncate-unsupported-floats",
                "--iree-codegen-check-ir-before-llvm-conversion=false",
                "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
            ]
            + [
                "--iree-llvmcpu-use-fast-min-max-ops",
            ]
            if self.precision == "int4"
            else [],
            debug=self.debug,
        )
        print("Saved falcon vmfb at ", str(path))
        shark_module.load_module(path)

        return shark_module

    def generate(self, prompt):
        model_inputs = self.tokenizer(
            prompt,
            padding="max_length",
            max_length=self.max_padding_length,
            add_special_tokens=False,
            return_tensors="pt",
        )
        model_inputs["prompt_text"] = prompt

        input_ids = model_inputs["input_ids"]
        attention_mask = model_inputs.get("attention_mask", None)

        # Allow empty prompts
        if input_ids.shape[1] == 0:
            input_ids = None
            attention_mask = None
            in_b = 1
        else:
            in_b = input_ids.shape[0]

        generate_kwargs = {
            "max_length": self.max_num_tokens,
            "do_sample": True,
            "top_k": 10,
            "num_return_sequences": 1,
            "eos_token_id": 11,
        }
        generate_kwargs["input_ids"] = input_ids
        generate_kwargs["attention_mask"] = attention_mask
        generation_config_ = GenerationConfig.from_model_config(
            self.src_model.config
        )
        generation_config = copy.deepcopy(generation_config_)
        model_kwargs = generation_config.update(**generate_kwargs)

        logits_processor = LogitsProcessorList()
        stopping_criteria = StoppingCriteriaList()

        eos_token_id = generation_config.eos_token_id
        generation_config.pad_token_id = eos_token_id

        (
            inputs_tensor,
            model_input_name,
            model_kwargs,
        ) = self.src_model._prepare_model_inputs(
            None, generation_config.bos_token_id, model_kwargs
        )
        batch_size = inputs_tensor.shape[0]

        model_kwargs["output_attentions"] = generation_config.output_attentions
        model_kwargs[
            "output_hidden_states"
        ] = generation_config.output_hidden_states
        model_kwargs["use_cache"] = generation_config.use_cache

        input_ids = (
            inputs_tensor
            if model_input_name == "input_ids"
            else model_kwargs.pop("input_ids")
        )

        self.logits_processor = self.src_model._get_logits_processor(
            generation_config=generation_config,
            input_ids_seq_length=input_ids.shape[-1],
            encoder_input_ids=inputs_tensor,
            prefix_allowed_tokens_fn=None,
            logits_processor=logits_processor,
        )

        self.stopping_criteria = self.src_model._get_stopping_criteria(
            generation_config=generation_config,
            stopping_criteria=stopping_criteria,
        )

        self.logits_warper = self.src_model._get_logits_warper(
            generation_config
        )

        (
            self.input_ids,
            self.model_kwargs,
        ) = self.src_model._expand_inputs_for_generation(
            input_ids=input_ids,
            expand_size=generation_config.num_return_sequences,  # 1
            is_encoder_decoder=self.src_model.config.is_encoder_decoder,  # False
            **model_kwargs,
        )

        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]
        self.eos_token_id_tensor = (
            torch.tensor(eos_token_id) if eos_token_id is not None else None
        )

        self.pad_token_id = generation_config.pad_token_id
        self.eos_token_id = eos_token_id

        output_scores = generation_config.output_scores  # False
        output_attentions = generation_config.output_attentions  # False
        output_hidden_states = generation_config.output_hidden_states  # False
        return_dict_in_generate = (
            generation_config.return_dict_in_generate  # False
        )

        # init attention / hidden states / scores tuples
        self.scores = (
            () if (return_dict_in_generate and output_scores) else None
        )
        decoder_attentions = (
            () if (return_dict_in_generate and output_attentions) else None
        )
        cross_attentions = (
            () if (return_dict_in_generate and output_attentions) else None
        )
        decoder_hidden_states = (
            () if (return_dict_in_generate and output_hidden_states) else None
        )

        # keep track of which sequences are already finished
        self.unfinished_sequences = torch.ones(
            input_ids.shape[0], dtype=torch.long, device=input_ids.device
        )

        all_text = prompt

        start = time.time()
        count = 0
        for i in range(self.max_num_tokens - 1):
            count = count + 1

            next_token = self.generate_new_token()
            new_word = self.tokenizer.decode(
                next_token.cpu().numpy(),
                add_special_tokens=False,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True,
            )

            all_text = all_text + new_word

            print(f"{new_word}", end="", flush=True)

            # if eos_token was found in one sentence, set sentence to finished
            if self.eos_token_id_tensor is not None:
                self.unfinished_sequences = self.unfinished_sequences.mul(
                    next_token.tile(self.eos_token_id_tensor.shape[0], 1)
                    .ne(self.eos_token_id_tensor.unsqueeze(1))
                    .prod(dim=0)
                )
                # stop when each sentence is finished
                if (
                    self.unfinished_sequences.max() == 0
                    or self.stopping_criteria(input_ids, self.scores)
                ):
                    break

        end = time.time()
        print(
            "\n\nTime taken is {:.2f} seconds/token\n".format(
                (end - start) / count
            )
        )

        torch.cuda.empty_cache()
        gc.collect()

        return all_text

    def generate_new_token(self):
        model_inputs = self.src_model.prepare_inputs_for_generation(
            self.input_ids, **self.model_kwargs
        )
        outputs = torch.from_numpy(
            self.shark_model(
                "forward",
                (model_inputs["input_ids"], model_inputs["attention_mask"]),
            )
        )
        if self.precision in ["fp16", "int4"]:
            outputs = outputs.to(dtype=torch.float32)
        next_token_logits = outputs

        # pre-process distribution
        next_token_scores = self.logits_processor(
            self.input_ids, next_token_logits
        )
        next_token_scores = self.logits_warper(
            self.input_ids, next_token_scores
        )

        # sample
        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)

        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)

        # finished sentences should have their next token be a padding token
        if self.eos_token_id is not None:
            if self.pad_token_id is None:
                raise ValueError(
                    "If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
                )
            next_token = (
                next_token * self.unfinished_sequences
                + self.pad_token_id * (1 - self.unfinished_sequences)
            )

        self.input_ids = torch.cat(
            [self.input_ids, next_token[:, None]], dim=-1
        )

        self.model_kwargs["past_key_values"] = None
        if "attention_mask" in self.model_kwargs:
            attention_mask = self.model_kwargs["attention_mask"]
            self.model_kwargs["attention_mask"] = torch.cat(
                [
                    attention_mask,
                    attention_mask.new_ones((attention_mask.shape[0], 1)),
                ],
                dim=-1,
            )

        self.input_ids = self.input_ids[:, 1:]
        self.model_kwargs["attention_mask"] = self.model_kwargs[
            "attention_mask"
        ][:, 1:]

        return next_token


if __name__ == "__main__":
    args = parser.parse_args()

    falcon_mlir_path = (
        Path(
            "falcon_"
            + args.falcon_variant_to_use
            + "_"
            + args.precision
            + ".mlir"
        )
        if args.falcon_mlir_path is None
        else Path(args.falcon_mlir_path)
    )
    falcon_vmfb_path = (
        Path(
            "falcon_"
            + args.falcon_variant_to_use
            + "_"
            + args.precision
            + "_"
            + args.device
            + ".vmfb"
        )
        if args.falcon_vmfb_path is None
        else Path(args.falcon_vmfb_path)
    )

    if args.precision == "int4":
        if args.falcon_variant_to_use == "180b":
            hf_model_path_value = "TheBloke/Falcon-180B-Chat-GPTQ"
        else:
            hf_model_path_value = (
                "TheBloke/falcon-"
                + args.falcon_variant_to_use
                + "-instruct-GPTQ"
            )
    else:
        if args.falcon_variant_to_use == "180b":
            hf_model_path_value = "tiiuae/falcon-180B-chat"
        else:
            hf_model_path_value = (
                "tiiuae/falcon-" + args.falcon_variant_to_use + "-instruct"
            )

    if not args.sharded:
        falcon = UnshardedFalcon(
            model_name="falcon_" + args.falcon_variant_to_use,
            hf_model_path=hf_model_path_value,
            device=args.device,
            precision=args.precision,
            falcon_mlir_path=falcon_mlir_path,
            falcon_vmfb_path=falcon_vmfb_path,
        )
    else:
        falcon = ShardedFalcon(
            model_name="falcon_" + args.falcon_variant_to_use,
            hf_model_path=hf_model_path_value,
            device=args.device,
            precision=args.precision,
        )

    default_prompt_text = "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"
    continue_execution = True

    print("\n-----\nScript executing for the following config: \n")
    print("Falcon Model: ", falcon.model_name)
    print("Precision:    ", args.precision)
    print("Device:       ", args.device)

    while continue_execution:
        use_default_prompt = input(
            "\nDo you wish to use the default prompt text? Y/N ?: "
        )
        if use_default_prompt in ["Y", "y"]:
            prompt = default_prompt_text
        else:
            prompt = input("Please enter the prompt text: ")
        print("\nPrompt Text: ", prompt)

        prompt_template = f"""A helpful assistant who helps the user with any questions asked.
        User: {prompt}
        Assistant:"""

        res_str = falcon.generate(prompt_template)
        torch.cuda.empty_cache()
        gc.collect()
        print(
            "\n\n-----\nHere's the complete formatted result: \n\n",
            res_str,
        )
        continue_execution = input(
            "\nDo you wish to run script one more time? Y/N ?: "
        )
        continue_execution = (
            True if continue_execution in ["Y", "y"] else False
        )

```

`apps/language_models/src/pipelines/minigpt4_pipeline.py`:

```py
from apps.language_models.src.model_wrappers.minigpt4 import (
    LayerNorm,
    VisionModel,
    QformerBertModel,
    FirstLlamaModel,
    SecondLlamaModel,
    StoppingCriteriaSub,
    CONV_VISION,
)
from apps.language_models.src.pipelines.SharkLLMBase import SharkLLMBase
from apps.language_models.utils import (
    get_vmfb_from_path,
    get_vmfb_from_config,
)
from omegaconf import OmegaConf
from pathlib import Path
from shark.shark_downloader import download_public_file
from transformers import LlamaTokenizer, LlamaForCausalLM
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import StoppingCriteriaList
from transformers.generation import GenerationConfig, LogitsProcessorList

import re
import torch
import os
from PIL import Image
import sys
import requests

# SHARK dependencies
from shark.shark_compile import (
    shark_compile_through_fx,
)
import random
import contextlib
from transformers import BertTokenizer
from transformers.generation import GenerationConfig, LogitsProcessorList
import copy
import tempfile

# QFormer, eva_vit, blip_processor
from apps.language_models.src.pipelines.minigpt4_utils.Qformer import (
    BertConfig,
    BertLMHeadModel,
)
from apps.language_models.src.pipelines.minigpt4_utils.eva_vit import (
    create_eva_vit_g,
)
from apps.language_models.src.pipelines.minigpt4_utils.blip_processors import (
    Blip2ImageEvalProcessor,
)

import argparse

parser = argparse.ArgumentParser(
    prog="MiniGPT4 runner",
    description="runs MiniGPT4",
)

parser.add_argument(
    "--precision", "-p", default="fp16", help="fp32, fp16, int8, int4"
)
parser.add_argument("--device", "-d", default="cuda", help="vulkan, cpu, cuda")
parser.add_argument(
    "--vision_model_vmfb_path",
    default=None,
    help="path to vision model's vmfb",
)
parser.add_argument(
    "--qformer_vmfb_path",
    default=None,
    help="path to qformer model's vmfb",
)
parser.add_argument(
    "--image_path",
    type=str,
    default="",
    help="path to the input image",
)
parser.add_argument(
    "--load_mlir_from_shark_tank",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="download precompile mlir from shark tank",
)
parser.add_argument(
    "--cli",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Run model in cli mode",
)
parser.add_argument(
    "--compile",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Compile all models",
)
parser.add_argument(
    "--max_length",
    type=int,
    default=2000,
    help="Max length of the entire conversation",
)
parser.add_argument(
    "--max_new_tokens",
    type=int,
    default=300,
    help="Maximum no. of new tokens that can be generated for a query",
)


def disabled_train(self, mode=True):
    """Overwrite model.train with this function to make sure train/eval mode
    does not change anymore."""
    return self


def is_url(input_url):
    """
    Check if an input string is a url. look for http(s):// and ignoring the case
    """
    is_url = re.match(r"^(?:http)s?://", input_url, re.IGNORECASE) is not None
    return is_url


import os
import tempfile
from shark.shark_inference import SharkInference
from shark.shark_importer import import_with_fx, save_mlir
import torch
import torch_mlir
from torch_mlir.compiler_utils import run_pipeline_with_repro_report
from typing import List, Tuple
from io import BytesIO
from brevitas_examples.common.generative.quantize import quantize_model
from brevitas_examples.llm.llm_quant.run_utils import get_model_impl


# fmt: off
def quant〇matmul_rhs_group_quant〡shape(lhs: List[int], rhs: List[int], rhs_scale: List[int], rhs_zero_point: List[int], rhs_bit_width: int, rhs_group_size: int) -> List[int]:
    if len(lhs) == 3 and len(rhs) == 2:
        return [lhs[0], lhs[1], rhs[0]]
    elif len(lhs) == 2 and len(rhs) == 2:
        return [lhs[0], rhs[0]]
    else:
        raise ValueError("Input shapes not supported.")


def quant〇matmul_rhs_group_quant〡dtype(lhs_rank_dtype: Tuple[int, int], rhs_rank_dtype: Tuple[int, int], rhs_scale_rank_dtype: Tuple[int, int], rhs_zero_point_rank_dtype: Tuple[int, int], rhs_bit_width: int, rhs_group_size: int) -> int:
    # output dtype is the dtype of the lhs float input
    lhs_rank, lhs_dtype = lhs_rank_dtype
    return lhs_dtype


def quant〇matmul_rhs_group_quant〡has_value_semantics(lhs, rhs, rhs_scale, rhs_zero_point, rhs_bit_width, rhs_group_size) -> None:
    return


brevitas_matmul_rhs_group_quant_library = [
    quant〇matmul_rhs_group_quant〡shape,
    quant〇matmul_rhs_group_quant〡dtype,
    quant〇matmul_rhs_group_quant〡has_value_semantics]
# fmt: on


def load_vmfb(extended_model_name, device, mlir_dialect, extra_args=[]):
    vmfb_path = os.path.join(os.getcwd(), extended_model_name + ".vmfb")
    shark_module = None
    if os.path.isfile(vmfb_path):
        shark_module = SharkInference(
            None,
            device=device,
            mlir_dialect=mlir_dialect,
        )
        print(f"loading existing vmfb from: {vmfb_path}")
        shark_module.load_module(vmfb_path, extra_args=extra_args)
    return shark_module


def compile_module(
    shark_module, extended_model_name, generate_vmfb, extra_args=[], debug=False,
):
    if generate_vmfb:
        vmfb_path = os.path.join(os.getcwd(), extended_model_name + ".vmfb")
        if os.path.isfile(vmfb_path):
            print(f"loading existing vmfb from: {vmfb_path}")
            shark_module.load_module(vmfb_path, extra_args=extra_args)
        else:
            print(
                "No vmfb found. Compiling and saving to {}".format(vmfb_path)
            )
            path = shark_module.save_module(
                os.getcwd(), extended_model_name, extra_args, debug=debug
            )
            shark_module.load_module(path, extra_args=extra_args)
    else:
        shark_module.compile(extra_args)
    return shark_module


def compile_int_precision(
    model, inputs, precision, device, generate_vmfb, extended_model_name, debug=False
):
    torchscript_module = import_with_fx(
        model,
        inputs,
        precision=precision,
        mlir_type="torchscript",
    )
    mlir_module = torch_mlir.compile(
        torchscript_module,
        inputs,
        output_type="torch",
        backend_legal_ops=["quant.matmul_rhs_group_quant"],
        extra_library=brevitas_matmul_rhs_group_quant_library,
        use_tracing=False,
        verbose=False,
    )
    print(f"[DEBUG] converting torch to linalg")
    run_pipeline_with_repro_report(
        mlir_module,
        "builtin.module(func.func(torch-unpack-quant-tensor),func.func(torch-convert-custom-quant-op),torch-backend-to-linalg-on-tensors-backend-pipeline)",
        description="Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR",
    )
    from contextlib import redirect_stdout

    mlir_file_path = os.path.join(
        os.getcwd(), f"{extended_model_name}_linalg.mlir"
    )
    with open(mlir_file_path, "w") as f:
        with redirect_stdout(f):
            print(mlir_module.operation.get_asm())
    mlir_module = str(mlir_module)
    mlir_module = mlir_module.encode("UTF-8")
    mlir_module = BytesIO(mlir_module)
    bytecode = mlir_module.read()
    print(f"Elided IR written for {extended_model_name}")
    bytecode = save_mlir(
        bytecode,
        model_name=extended_model_name,
        frontend="torch",
        dir=os.getcwd(),
    )
    return bytecode
    shark_module = SharkInference(
        mlir_module=bytecode, device=device, mlir_dialect="tm_tensor"
    )
    extra_args = [
        "--iree-hal-dump-executable-sources-to=ies",
        "--iree-vm-target-truncate-unsupported-floats",
        "--iree-codegen-check-ir-before-llvm-conversion=false",
        "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
    ]
    return (
        compile_module(
            shark_module,
            extended_model_name=extended_model_name,
            generate_vmfb=generate_vmfb,
            extra_args=extra_args,
            debug=debug,
        ),
        bytecode,
    )


def shark_compile_through_fx_int(
    model,
    inputs,
    extended_model_name,
    precision,
    f16_input_mask=None,
    save_dir=tempfile.gettempdir(),
    debug=False,
    generate_or_load_vmfb=True,
    extra_args=[],
    device=None,
    mlir_dialect="tm_tensor",
):
    if generate_or_load_vmfb:
        shark_module = load_vmfb(
            extended_model_name=extended_model_name,
            device=device,
            mlir_dialect=mlir_dialect,
            extra_args=extra_args,
        )
        if shark_module:
            return (
                shark_module,
                None,
            )

    from shark.parser import shark_args

    if "cuda" in device:
        shark_args.enable_tf32 = True

    mlir_module = compile_int_precision(
        model,
        inputs,
        precision,
        device,
        generate_or_load_vmfb,
        extended_model_name,
        debug,
    )
    extra_args = [
        "--iree-hal-dump-executable-sources-to=ies",
        "--iree-vm-target-truncate-unsupported-floats",
        "--iree-codegen-check-ir-before-llvm-conversion=false",
        "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
    ]

    shark_module = SharkInference(
        mlir_module,
        device=device,
        mlir_dialect=mlir_dialect,
    )
    return (
        compile_module(
            shark_module,
            extended_model_name,
            generate_vmfb=generate_or_load_vmfb,
            extra_args=extra_args,
        ),
        mlir_module,
    )


class MiniGPT4BaseModel(torch.nn.Module):
    @classmethod
    def from_config(cls, cfg):
        vit_model = cfg.get("vit_model", "eva_clip_g")
        q_former_model = cfg.get(
            "q_former_model",
            "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth",
        )
        img_size = cfg.get("image_size")
        num_query_token = cfg.get("num_query_token")
        llama_model = cfg.get("llama_model")

        drop_path_rate = cfg.get("drop_path_rate", 0)
        use_grad_checkpoint = cfg.get("use_grad_checkpoint", False)
        vit_precision = cfg.get("vit_precision", "fp16")
        freeze_vit = cfg.get("freeze_vit", True)
        freeze_qformer = cfg.get("freeze_qformer", True)
        low_resource = cfg.get("low_resource", False)
        device_8bit = cfg.get("device_8bit", 0)

        prompt_path = cfg.get("prompt_path", "")
        prompt_template = cfg.get("prompt_template", "")
        max_txt_len = cfg.get("max_txt_len", 32)
        end_sym = cfg.get("end_sym", "\n")

        model = cls(
            vit_model=vit_model,
            q_former_model=q_former_model,
            img_size=img_size,
            drop_path_rate=drop_path_rate,
            use_grad_checkpoint=use_grad_checkpoint,
            vit_precision=vit_precision,
            freeze_vit=freeze_vit,
            freeze_qformer=freeze_qformer,
            num_query_token=num_query_token,
            llama_model=llama_model,
            prompt_path=prompt_path,
            prompt_template=prompt_template,
            max_txt_len=max_txt_len,
            end_sym=end_sym,
            low_resource=low_resource,
            device_8bit=device_8bit,
        )

        ckpt_path = cfg.get("ckpt", "")  # load weights of MiniGPT-4
        if ckpt_path:
            print("Load BLIP2-LLM Checkpoint: {}".format(ckpt_path))
            ckpt = torch.load(ckpt_path, map_location="cpu")
            model.load_state_dict(ckpt["model"], strict=False)

        return model

    PRETRAINED_MODEL_CONFIG_DICT = {
        "pretrain_vicuna": "minigpt4_utils/configs/minigpt4.yaml",
    }

    def maybe_autocast(self, dtype=torch.float32):
        # if on cpu, don't use autocast
        # if on gpu, use autocast with dtype if provided, otherwise use torch.float16
        # enable_autocast = self.device != torch.device("cpu")
        enable_autocast = True

        if enable_autocast:
            return torch.cuda.amp.autocast(dtype=dtype)
        else:
            return contextlib.nullcontext()

    def init_tokenizer(cls):
        tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
        tokenizer.add_special_tokens({"bos_token": "[DEC]"})
        return tokenizer

    def init_vision_encoder(
        self,
        model_name,
        img_size,
        drop_path_rate,
        use_grad_checkpoint,
        precision,
    ):
        assert (
            model_name == "eva_clip_g"
        ), "vit model must be eva_clip_g for current version of MiniGPT-4"
        visual_encoder = create_eva_vit_g(
            img_size, drop_path_rate, use_grad_checkpoint, precision
        )

        ln_vision = LayerNorm(visual_encoder.num_features)
        return visual_encoder, ln_vision

    def init_Qformer(
        cls, num_query_token, vision_width, cross_attention_freq=2
    ):
        encoder_config = BertConfig.from_pretrained("bert-base-uncased")
        encoder_config.encoder_width = vision_width
        # insert cross-attention layer every other block
        encoder_config.add_cross_attention = True
        encoder_config.cross_attention_freq = cross_attention_freq
        encoder_config.query_length = num_query_token
        Qformer = BertLMHeadModel(config=encoder_config)
        query_tokens = torch.nn.Parameter(
            torch.zeros(1, num_query_token, encoder_config.hidden_size)
        )
        query_tokens.data.normal_(
            mean=0.0, std=encoder_config.initializer_range
        )
        return Qformer, query_tokens

    def load_from_pretrained(self, url_or_filename):
        if is_url(url_or_filename):
            local_filename = "blip2_pretrained_flant5xxl.pth"
            response = requests.get(url_or_filename)
            if response.status_code == 200:
                with open(local_filename, "wb") as f:
                    f.write(response.content)
                print("File downloaded successfully.")
            checkpoint = torch.load(local_filename, map_location="cpu")
        elif os.path.isfile(url_or_filename):
            checkpoint = torch.load(url_or_filename, map_location="cpu")
        else:
            raise RuntimeError("checkpoint url or path is invalid")

        state_dict = checkpoint["model"]

        self.load_state_dict(state_dict, strict=False)

    def __init__(
        self,
        vit_model="eva_clip_g",
        q_former_model="https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth",
        img_size=224,
        drop_path_rate=0,
        use_grad_checkpoint=False,
        vit_precision="fp16",
        freeze_vit=True,
        freeze_qformer=True,
        num_query_token=32,
        llama_model="",
        prompt_path="",
        prompt_template="",
        max_txt_len=32,
        end_sym="\n",
        low_resource=False,  # use 8 bit and put vit in cpu
        device_8bit=0,  # the device of 8bit model should be set when loading and cannot be changed anymore.
    ):
        super().__init__()
        self.tokenizer = self.init_tokenizer()
        self.low_resource = low_resource

        print("Loading VIT")
        self.visual_encoder, self.ln_vision = self.init_vision_encoder(
            vit_model,
            img_size,
            drop_path_rate,
            use_grad_checkpoint,
            vit_precision,
        )
        if freeze_vit:
            for _, param in self.visual_encoder.named_parameters():
                param.requires_grad = False
            self.visual_encoder = self.visual_encoder.eval()
            self.visual_encoder.train = disabled_train
            for _, param in self.ln_vision.named_parameters():
                param.requires_grad = False
            self.ln_vision = self.ln_vision.eval()
            self.ln_vision.train = disabled_train
            # logging.info("freeze vision encoder")
        print("Loading VIT Done")

        print("Loading Q-Former")
        self.Qformer, self.query_tokens = self.init_Qformer(
            num_query_token, self.visual_encoder.num_features
        )
        self.Qformer.cls = None
        self.Qformer.bert.embeddings.word_embeddings = None
        self.Qformer.bert.embeddings.position_embeddings = None
        for layer in self.Qformer.bert.encoder.layer:
            layer.output = None
            layer.intermediate = None
        self.load_from_pretrained(url_or_filename=q_former_model)

        if freeze_qformer:
            for _, param in self.Qformer.named_parameters():
                param.requires_grad = False
            self.Qformer = self.Qformer.eval()
            self.Qformer.train = disabled_train
            self.query_tokens.requires_grad = False
            # logging.info("freeze Qformer")
        print("Loading Q-Former Done")

        print(f"Loading Llama model from {llama_model}")
        self.llama_tokenizer = AutoTokenizer.from_pretrained(
            llama_model, use_fast=False, legacy=False
        )
        # self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token

        if self.low_resource:
            self.llama_model = AutoModelForCausalLM.from_pretrained(
                llama_model,
                torch_dtype=torch.float16,
                load_in_8bit=True,
                device_map={"": device_8bit},
            )
        else:
            self.llama_model = AutoModelForCausalLM.from_pretrained(
                llama_model,
                torch_dtype=torch.float32,
            )

        print(
            "During init :-\nLlama model pad token : ",
            self.llama_model.config.pad_token_id,
        )
        print(
            "Llama tokenizer pad token : ", self.llama_tokenizer.pad_token_id
        )

        for _, param in self.llama_model.named_parameters():
            param.requires_grad = False
        print("Loading Llama Done")

        self.llama_proj = torch.nn.Linear(
            self.Qformer.config.hidden_size,
            self.llama_model.config.hidden_size,
        )
        self.max_txt_len = max_txt_len
        self.end_sym = end_sym

        if prompt_path:
            with open(prompt_path, "r") as f:
                raw_prompts = f.read().splitlines()
            filted_prompts = [
                raw_prompt
                for raw_prompt in raw_prompts
                if "<ImageHere>" in raw_prompt
            ]
            self.prompt_list = [
                prompt_template.format(p) for p in filted_prompts
            ]
            print("Load {} training prompts".format(len(self.prompt_list)))
            print(
                "Prompt Example \n{}".format(random.choice(self.prompt_list))
            )
        else:
            self.prompt_list = []


def resource_path(relative_path):
    """Get absolute path to resource, works for dev and for PyInstaller"""
    base_path = getattr(
        sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__))
    )
    return os.path.join(base_path, relative_path)


class MiniGPT4(SharkLLMBase):
    def __init__(
        self,
        model_name,
        hf_model_path=None,
        max_new_tokens=300,
        device="cuda",
        precision="fp16",
        _compile=False,
        vision_model_vmfb_path=Path("vision_model_fp16_cuda.vmfb"),
        qformer_vmfb_path=Path("qformer_fp32_cuda.vmfb"),
    ) -> None:
        self.model_name = model_name
        self.shark_model = None
        super().__init__(model_name, hf_model_path, max_new_tokens)
        self.download_dependencies()
        self.device = device
        self.precision = precision
        self._compile = _compile

        self.vision_model_vmfb_path = vision_model_vmfb_path
        self.qformer_vmfb_path = qformer_vmfb_path
        self.first_llama_vmfb_path = None
        self.second_llama_vmfb_path = None

        print("Initializing Chat")
        config = OmegaConf.load(
            resource_path("minigpt4_utils/configs/minigpt4_eval.yaml")
        )
        model_config = OmegaConf.create()
        model_config = OmegaConf.merge(
            model_config,
            OmegaConf.load(
                resource_path("minigpt4_utils/configs/minigpt4.yaml")
            ),
            {"model": config["model"]},
        )
        model_config = model_config["model"]
        model_config.device_8bit = 0
        model = MiniGPT4BaseModel.from_config(model_config).to("cpu")
        datasets = config.get("datasets", None)
        dataset_config = OmegaConf.create()
        for dataset_name in datasets:
            dataset_config_path = resource_path(
                "minigpt4_utils/configs/cc_sbu_align.yaml"
            )
            dataset_config = OmegaConf.merge(
                dataset_config,
                OmegaConf.load(dataset_config_path),
                {"datasets": {dataset_name: config["datasets"][dataset_name]}},
            )
        dataset_config = dataset_config["datasets"]
        vis_processor_cfg = dataset_config.cc_sbu_align.vis_processor.train
        vis_processor = Blip2ImageEvalProcessor.from_config(vis_processor_cfg)
        print("Initialization complete")

        self.model = model
        self.vis_processor = vis_processor
        stop_words_ids = [
            torch.tensor([835]).to("cpu"),
            torch.tensor([2277, 29937]).to("cpu"),
        ]  # '###' can be encoded in two different ways.
        self.stopping_criteria = StoppingCriteriaList(
            [StoppingCriteriaSub(stops=stop_words_ids)]
        )

        self.first_llama = None
        self.second_llama = None

    def download_dependencies(self):
        pretrained_file = "prerained_minigpt4_7b.pth"
        pretrained_file_url = f"gs://shark_tank/MiniGPT4/{pretrained_file}"
        if not os.path.isfile(pretrained_file):
            download_public_file(
                pretrained_file_url,
                Path("prerained_minigpt4_7b.pth").absolute(),
                single_file=True,
            )

            if os.path.isfile(pretrained_file):
                print(f"File downloaded successfully: {pretrained_file}")
            else:
                print(f"Error downloading {pretrained_file}")
                sys.exit()

    # Currently we're compiling VisionModel for fp32/cuda.
    def compile_vision_model(self):
        # TODO: Hardcoding precision based on input choices. Take this down
        #       later.
        vision_model_precision = "fp32"
        if self.precision in ["int4", "int8", "fp16"]:
            vision_model_precision = "fp16"

        if not self._compile:
            vmfb = get_vmfb_from_path(
                self.vision_model_vmfb_path, self.device, "tm_tensor"
            )
            if vmfb is not None:
                return vmfb
            else:
                vmfb = get_vmfb_from_config(
                    self.model_name,
                    "vision_model",
                    vision_model_precision,
                    self.device,
                    self.vision_model_vmfb_path,
                )
                if vmfb is not None:
                    return vmfb

        visionModel = VisionModel(
            copy.deepcopy(self.model.ln_vision),
            copy.deepcopy(self.model.visual_encoder),
            vision_model_precision,
        )
        extended_model_name = (
            f"vision_model_{vision_model_precision}_{self.device}"
        )
        print(f"Going to compile {extended_model_name}")
        # Inputs for VisionModel.
        inputs = [torch.randint(3, (1, 3, 224, 224), dtype=torch.float32)]
        is_f16 = False
        if vision_model_precision == "fp16":
            is_f16 = True
        if self.precision in ["int4", "int8"]:
            shark_visionModel, _ = shark_compile_through_fx_int(
                visionModel,
                inputs,
                extended_model_name=extended_model_name,
                precision=vision_model_precision,
                f16_input_mask=None,
                save_dir=tempfile.gettempdir(),
                debug=False,
                generate_or_load_vmfb=True,
                extra_args=[],
                device=self.device,
                mlir_dialect="tm_tensor",
            )
        else:
            shark_visionModel, _ = shark_compile_through_fx(
                visionModel,
                inputs,
                extended_model_name=extended_model_name,
                precision=vision_model_precision,
                f16_input_mask=None,
                save_dir=tempfile.gettempdir(),
                debug=False,
                generate_or_load_vmfb=True,
                extra_args=[],
                device=self.device,
                mlir_dialect="tm_tensor",
            )
        print(f"Generated {extended_model_name}.vmfb")
        return shark_visionModel

    def compile_qformer_model(self):
        if not self._compile:
            vmfb = get_vmfb_from_path(
                self.qformer_vmfb_path, self.device, "tm_tensor"
            )
            if vmfb is not None:
                return vmfb
            else:
                vmfb = get_vmfb_from_config(
                    self.model_name,
                    "qformer",
                    "fp32",
                    self.device,
                    self.qformer_vmfb_path,
                )
                if vmfb is not None:
                    return vmfb

        qformerBertModel = QformerBertModel(self.model.Qformer.bert)
        extended_model_name = f"qformer_fp32_{self.device}"
        print(f"Going to compile {extended_model_name}")
        # Inputs for QFormer.
        inputs = [
            torch.randint(3, (1, 32, 768), dtype=torch.float32),
            torch.randint(3, (1, 257, 1408), dtype=torch.float32),
            torch.randint(3, (1, 257), dtype=torch.int64),
        ]
        is_f16 = False
        f16_input_mask = []
        shark_QformerBertModel, _ = shark_compile_through_fx(
            qformerBertModel,
            inputs,
            extended_model_name=extended_model_name,
            precision="fp32",
            f16_input_mask=f16_input_mask,
            save_dir=tempfile.gettempdir(),
            debug=False,
            generate_or_load_vmfb=True,
            extra_args=[],
            device=self.device,
            mlir_dialect="tm_tensor",
        )
        print(f"Generated {extended_model_name}.vmfb")
        return shark_QformerBertModel

    def compile_first_llama(self, padding):
        self.first_llama_vmfb_path = Path(
            f"first_llama_{self.precision}_{self.device}_{padding}.vmfb"
        )
        if not self._compile:
            vmfb = get_vmfb_from_path(
                self.first_llama_vmfb_path, self.device, "tm_tensor"
            )
            if vmfb is not None:
                self.first_llama = vmfb
                return vmfb
            else:
                vmfb = get_vmfb_from_config(
                    self.model_name,
                    "first_llama",
                    self.precision,
                    self.device,
                    self.first_llama_vmfb_path,
                    padding,
                )
                if vmfb is not None:
                    self.first_llama = vmfb
                    return vmfb

        firstLlamaModel = FirstLlamaModel(
            copy.deepcopy(self.model.llama_model), self.precision
        )
        extended_model_name = (
            f"first_llama_{self.precision}_{self.device}_{padding}"
        )
        print(f"Going to compile {extended_model_name}")
        # Inputs for FirstLlama.
        inputs_embeds = torch.ones((1, padding, 4096), dtype=torch.float32)
        position_ids = torch.ones((1, padding), dtype=torch.int64)
        attention_mask = torch.ones((1, padding), dtype=torch.int32)
        inputs = [inputs_embeds, position_ids, attention_mask]
        is_f16 = False
        f16_input_mask = []
        if self.precision == "fp16":
            is_f16 = True
            f16_input_mask = [True, False, False]
        if self.precision in ["int4", "int8"]:
            shark_firstLlamaModel, _ = shark_compile_through_fx_int(
                firstLlamaModel,
                inputs,
                extended_model_name=extended_model_name,
                precision=self.precision,
                f16_input_mask=f16_input_mask,
                save_dir=tempfile.gettempdir(),
                debug=False,
                generate_or_load_vmfb=True,
                extra_args=[],
                device=self.device,
                mlir_dialect="tm_tensor",
            )
        else:
            shark_firstLlamaModel, _ = shark_compile_through_fx(
                firstLlamaModel,
                inputs,
                extended_model_name=extended_model_name,
                precision=self.precision,
                f16_input_mask=f16_input_mask,
                save_dir=tempfile.gettempdir(),
                debug=False,
                generate_or_load_vmfb=True,
                extra_args=[],
                device=self.device,
                mlir_dialect="tm_tensor",
            )
        print(f"Generated {extended_model_name}.vmfb")
        self.first_llama = shark_firstLlamaModel
        return shark_firstLlamaModel

    def compile_second_llama(self, padding):
        self.second_llama_vmfb_path = Path(
            f"second_llama_{self.precision}_{self.device}_{padding}.vmfb"
        )
        if not self._compile:
            vmfb = get_vmfb_from_path(
                self.second_llama_vmfb_path, self.device, "tm_tensor"
            )
            if vmfb is not None:
                self.second_llama = vmfb
                return vmfb
            else:
                vmfb = get_vmfb_from_config(
                    self.model_name,
                    "second_llama",
                    self.precision,
                    self.device,
                    self.second_llama_vmfb_path,
                    padding,
                )
                if vmfb is not None:
                    self.second_llama = vmfb
                    return vmfb

        secondLlamaModel = SecondLlamaModel(
            copy.deepcopy(self.model.llama_model), self.precision
        )
        extended_model_name = (
            f"second_llama_{self.precision}_{self.device}_{padding}"
        )
        print(f"Going to compile {extended_model_name}")
        # Inputs for SecondLlama.
        input_ids = torch.zeros((1, 1), dtype=torch.int64)
        position_ids = torch.zeros((1, 1), dtype=torch.int64)
        attention_mask = torch.zeros((1, padding + 1), dtype=torch.int32)
        past_key_value = []
        for i in range(64):
            past_key_value.append(
                torch.zeros(1, 32, padding, 128, dtype=torch.float32)
            )
        inputs = [input_ids, position_ids, attention_mask, *past_key_value]
        is_f16 = False
        f16_input_mask = []
        if self.precision == "fp16":
            is_f16 = True
            f16_input_mask = [False, False, False]
            for i in past_key_value:
                f16_input_mask.append(True)

        if self.precision in ["int4", "int8"]:
            shark_secondLlamaModel, _ = shark_compile_through_fx_int(
                secondLlamaModel,
                inputs,
                extended_model_name=extended_model_name,
                precision=self.precision,
                f16_input_mask=f16_input_mask,
                save_dir=tempfile.gettempdir(),
                debug=False,
                generate_or_load_vmfb=True,
                extra_args=[],
                device=self.device,
                mlir_dialect="tm_tensor",
            )
        else:
            shark_secondLlamaModel, _ = shark_compile_through_fx(
                secondLlamaModel,
                inputs,
                extended_model_name=extended_model_name,
                precision=self.precision,
                f16_input_mask=f16_input_mask,
                save_dir=tempfile.gettempdir(),
                debug=False,
                generate_or_load_vmfb=True,
                extra_args=[],
                device=self.device,
                mlir_dialect="tm_tensor",
            )
        print(f"Generated {extended_model_name}.vmfb")
        self.second_llama = shark_secondLlamaModel
        return shark_secondLlamaModel

    # Not yet sure why to use this.
    def compile(self):
        pass

    # Going to use `answer` instead.
    def generate(self, prompt):
        pass

    # Might use within `answer`, if needed.
    def generate_new_token(self, params):
        pass

    # Not needed yet because MiniGPT4BaseModel already loads this - will revisit later,
    # if required.
    def get_tokenizer(self):
        pass

    # DumDum func - doing the intended stuff already at MiniGPT4BaseModel,
    # i.e load llama, etc.
    def get_src_model(self):
        pass

    def ask(self, text, conv):
        if (
            len(conv.messages) > 0
            and conv.messages[-1][0] == conv.roles[0]
            and conv.messages[-1][1][-6:] == "</Img>"
        ):  # last message is image.
            conv.messages[-1][1] = " ".join([conv.messages[-1][1], text])
        else:
            conv.append_message(conv.roles[0], text)

    def answer(
        self,
        conv,
        img_list,
        max_new_tokens=300,
        num_beams=1,
        min_length=1,
        top_p=0.9,
        repetition_penalty=1.0,
        length_penalty=1,
        temperature=1.0,
        max_length=2000,
    ):
        conv.append_message(conv.roles[1], None)
        embs = self.get_context_emb(
            conv, img_list, max_length - max_new_tokens
        )
        padding = max_length - max_new_tokens

        current_max_len = embs.shape[1] + max_new_tokens

        if current_max_len - max_length > 0:
            print(
                "Warning: The number of tokens in current conversation exceeds the max length. "
                "The model will not see the contexts outside the range."
            )
        begin_idx = max(0, current_max_len - max_length)

        embs = embs[:, begin_idx:]

        #########################################################################################################

        generation_config = GenerationConfig.from_model_config(
            self.model.llama_model.config
        )
        kwargs = {
            "inputs_embeds": embs,
            "max_new_tokens": max_new_tokens,
            "num_beams": num_beams,
            "do_sample": True,
            "min_length": min_length,
            "top_p": top_p,
            "repetition_penalty": repetition_penalty,
            "length_penalty": length_penalty,
            "temperature": temperature,
        }
        generation_config = copy.deepcopy(generation_config)
        model_kwargs = generation_config.update(**kwargs)
        logits_processor = LogitsProcessorList()
        stopping_criteria = self.stopping_criteria
        inputs = None
        (
            inputs_tensor,
            model_input_name,
            model_kwargs,
        ) = self.model.llama_model._prepare_model_inputs(
            inputs, generation_config.bos_token_id, model_kwargs
        )
        model_kwargs["output_attentions"] = generation_config.output_attentions
        model_kwargs[
            "output_hidden_states"
        ] = generation_config.output_hidden_states
        model_kwargs["use_cache"] = generation_config.use_cache
        generation_config.pad_token_id = (
            self.model.llama_tokenizer.pad_token_id
        )
        pad_token_id = generation_config.pad_token_id
        embs_for_pad_token_id = self.model.llama_model.model.embed_tokens(
            torch.tensor([pad_token_id])
        )
        model_kwargs["attention_mask"] = torch.logical_not(
            torch.tensor(
                [
                    torch.all(
                        torch.eq(inputs_tensor[:, d, :], embs_for_pad_token_id)
                    ).int()
                    for d in range(inputs_tensor.shape[1])
                ]
            ).unsqueeze(0)
        ).int()
        attention_meta_data = (model_kwargs["attention_mask"][0] == 0).nonzero(
            as_tuple=True
        )[0]
        first_zero = attention_meta_data[0].item()
        last_zero = attention_meta_data[-1].item()
        input_ids = (
            inputs_tensor
            if model_input_name == "input_ids"
            else model_kwargs.pop("input_ids")
        )
        input_ids_seq_length = input_ids.shape[-1]
        generation_config.max_length = (
            generation_config.max_new_tokens + input_ids_seq_length
        )
        logits_warper = self.model.llama_model._get_logits_warper(
            generation_config
        )
        (
            input_ids,
            model_kwargs,
        ) = self.model.llama_model._expand_inputs_for_generation(
            input_ids=input_ids,
            expand_size=generation_config.num_return_sequences,
            is_encoder_decoder=False,
            **model_kwargs,
        )
        # DOUBT: stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)
        logits_warper = (
            logits_warper
            if logits_warper is not None
            else LogitsProcessorList()
        )
        pad_token_id = generation_config.pad_token_id
        eos_token_id = generation_config.eos_token_id
        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]
        eos_token_id_tensor = (
            torch.tensor(eos_token_id).to(input_ids.device)
            if eos_token_id is not None
            else None
        )
        scores = None

        # keep track of which sequences are already finished
        unfinished_sequences = torch.ones(
            input_ids.shape[0], dtype=torch.long, device=input_ids.device
        )
        i = 0
        timesRan = 0
        is_fp16 = self.precision == "fp16"
        llama_list = []
        isPyTorchVariant = False
        while True:
            print("****** Iteration %d ******" % (i))
            # prepare model inputs
            model_inputs = (
                self.model.llama_model.prepare_inputs_for_generation(
                    input_ids, **model_kwargs
                )
            )

            # forward pass to get next token
            if i == 0:
                shark_inputs = []
                if is_fp16:
                    model_inputs["inputs_embeds"] = model_inputs[
                        "inputs_embeds"
                    ].to(torch.float16)
                shark_inputs.append(model_inputs["inputs_embeds"].detach())
                shark_inputs.append(model_inputs["position_ids"].detach())
                shark_inputs.append(model_inputs["attention_mask"].detach())

                if self.first_llama is None:
                    self.compile_first_llama(padding)
                outputs_shark = self.first_llama("forward", shark_inputs)
                outputs = []
                for out_shark in outputs_shark:
                    outputs.append(torch.from_numpy(out_shark))
                del outputs_shark
            else:
                shark_inputs = []
                shark_inputs.append(model_inputs["input_ids"].detach())
                shark_inputs.append(model_inputs["position_ids"].detach())
                shark_inputs.append(model_inputs["attention_mask"].detach())
                for pkv in list(model_inputs["past_key_values"]):
                    shark_inputs.append(pkv.detach())
                if self.second_llama is None:
                    self.compile_second_llama(padding)
                outputs_shark = self.second_llama("forward", shark_inputs)
                outputs = []
                for out_shark in outputs_shark:
                    outputs.append(torch.from_numpy(out_shark))
                del outputs_shark

            outputs_logits = outputs[0]
            next_token_logits = outputs_logits[:, -1, :]
            if is_fp16:
                next_token_logits = next_token_logits.to(torch.float32)

            # pre-process distribution
            next_token_scores = logits_processor(input_ids, next_token_logits)
            next_token_scores = logits_warper(input_ids, next_token_scores)
            probs = torch.nn.functional.softmax(next_token_scores, dim=-1)
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)

            # finished sentences should have their next token be a padding token
            if eos_token_id is not None:
                if pad_token_id is None:
                    raise ValueError(
                        "If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
                    )
                next_tokens = (
                    next_tokens * unfinished_sequences
                    + pad_token_id * (1 - unfinished_sequences)
                )

            # update generated ids, model inputs, and length for next step
            outputs_for_update_func = {}
            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
            model_kwargs = (
                self.model.llama_model._update_model_kwargs_for_generation(
                    outputs_for_update_func,
                    model_kwargs,
                    is_encoder_decoder=False,
                )
            )
            model_kwargs["past_key_values"] = outputs[1:]
            if timesRan >= 1:
                tmp_attention_mask = torch.cat(
                    (
                        model_kwargs["attention_mask"][:, :first_zero],
                        model_kwargs["attention_mask"][:, first_zero + 1 :],
                    ),
                    dim=1,
                )
                model_kwargs["attention_mask"] = tmp_attention_mask
                pkv_list = []
                for pkv_pair_tuple in model_kwargs["past_key_values"]:
                    x = torch.cat(
                        (
                            pkv_pair_tuple[:, :, :first_zero, :],
                            pkv_pair_tuple[:, :, first_zero + 1 :, :],
                        ),
                        dim=2,
                    )
                    if is_fp16:
                        x = x.to(torch.float16)
                    pkv_list.append(x)
                model_kwargs["past_key_values"] = tuple(pkv_list)

            # if eos_token was found in one sentence, set sentence to finished
            if eos_token_id_tensor is not None:
                unfinished_sequences = unfinished_sequences.mul(
                    next_tokens.tile(eos_token_id_tensor.shape[0], 1)
                    .ne(eos_token_id_tensor.unsqueeze(1))
                    .prod(dim=0)
                )

            # stop when each sentence is finished, or if we exceed the maximum length
            if unfinished_sequences.max() == 0 or stopping_criteria(
                input_ids, scores
            ):
                break

            i = i + 1
            timesRan += 1
        llama_list.clear()
        output_token = input_ids[0]

        if (
            output_token[0] == 0
        ):  # the model might output a unknow token <unk> at the beginning. remove it
            output_token = output_token[1:]
        if (
            output_token[0] == 1
        ):  # some users find that there is a start token <s> at the beginning. remove it
            output_token = output_token[1:]
        output_text = self.model.llama_tokenizer.decode(
            output_token, add_special_tokens=False
        )
        output_text = output_text.split("###")[0]  # remove the stop sign '###'
        output_text = output_text.split("Assistant:")[-1].strip()
        conv.messages[-1][1] = output_text
        return output_text, output_token.cpu().numpy()

    def upload_img(self, image, conv, img_list):
        if isinstance(image, str):  # is a image path
            raw_image = Image.open(image).convert("RGB")
            image = self.vis_processor(raw_image).unsqueeze(0).to("cpu")
        elif isinstance(image, Image.Image):
            raw_image = image
            image = self.vis_processor(raw_image).unsqueeze(0).to("cpu")
        elif isinstance(image, torch.Tensor):
            if len(image.shape) == 3:
                image = image.unsqueeze(0)
            image = image.to("cpu")

        device = image.device
        if self.model.low_resource:
            self.model.vit_to_cpu()
            image = image.to("cpu")

        with self.model.maybe_autocast():
            shark_visionModel = self.compile_vision_model()
            if self.precision in ["int4", "int8", "fp16"]:
                image = image.to(torch.float16)
            image_embeds = shark_visionModel("forward", (image,))
            # image_embeds = shark_visionModel.forward(image)
            image_embeds = torch.from_numpy(image_embeds)
            image_embeds = image_embeds.to(device).to(torch.float32)
            image_atts = torch.ones(
                image_embeds.size()[:-1], dtype=torch.long
            ).to(device)

            query_tokens = self.model.query_tokens.expand(
                image_embeds.shape[0], -1, -1
            ).to(device)
            shark_QformerBertModel = self.compile_qformer_model()
            query_output = shark_QformerBertModel(
                "forward",
                (
                    query_tokens,
                    image_embeds,
                    image_atts,
                ),
            )
            query_output = torch.from_numpy(query_output)

            inputs_llama = self.model.llama_proj(query_output)
        image_emb = inputs_llama
        img_list.append(image_emb)
        conv.append_message(conv.roles[0], "<Img><ImageHere></Img>")
        msg = "Received."
        return msg

    # """
    def get_context_emb(self, conv, img_list, max_allowed_tokens=200):
        self.model.llama_tokenizer.padding_side = "left"
        prompt = conv.get_prompt()
        prompt_segs = prompt.split("<ImageHere>")
        assert (
            len(prompt_segs) == len(img_list) + 1
        ), "Unmatched numbers of image placeholders and images."
        prompt_segs_pre = prompt_segs[:-1]
        seg_tokens_pre = []
        for i, seg in enumerate(prompt_segs_pre):
            # only add bos to the first seg
            if i == 0:
                add_special_tokens = True
            else:
                add_special_tokens = False
            stp = (
                self.model.llama_tokenizer(
                    seg,
                    return_tensors="pt",
                    add_special_tokens=add_special_tokens,
                )
                .to("cpu")
                .input_ids
            )
            seg_tokens_pre.append(stp)
        # seg_tokens_pre = [
        #     self.model.llama_tokenizer(
        #         seg, return_tensors="pt", add_special_tokens=i == 0
        #     )
        #     .to("cpu")
        #     .input_ids
        #     for i, seg in enumerate(prompt_segs_pre)
        # ]
        print(
            "Before :-\nLlama model pad token : ",
            self.model.llama_model.config.pad_token_id,
        )
        print(
            "Llama tokenizer pad token : ",
            self.model.llama_tokenizer.pad_token_id,
        )
        self.model.llama_model.config.pad_token_id = (
            self.model.llama_tokenizer.pad_token_id
        )
        print(
            "After :-\nLlama model pad token : ",
            self.model.llama_model.config.pad_token_id,
        )
        print(
            "Llama tokenizer pad token : ",
            self.model.llama_tokenizer.pad_token_id,
        )
        print("seg_t :", seg_tokens_pre[0])

        seg_embs_pre = [
            self.model.llama_model.model.embed_tokens(seg_t)
            for seg_t in seg_tokens_pre
        ]
        mixed_embs_pre = [
            emb.to("cpu")
            for pair in zip(seg_embs_pre, img_list)
            for emb in pair
        ]
        mixed_embs_pre = torch.cat(mixed_embs_pre, dim=1)
        max_allowed_tokens = max_allowed_tokens - mixed_embs_pre.shape[1]
        final_prompt = prompt_segs[-1]
        seg_tokens_post = [
            self.model.llama_tokenizer(
                seg,
                return_tensors="pt",
                padding="max_length",
                max_length=max_allowed_tokens,
                add_special_tokens=False,
            )
            .to("cpu")
            .input_ids
            # only add bos to the first seg
            for i, seg in enumerate([final_prompt])
        ]
        seg_tokens_post = seg_tokens_post[0]
        seg_embs_post = [
            self.model.llama_model.model.embed_tokens(seg_t)
            for seg_t in seg_tokens_post
        ]
        mixed_embs_post = [seg_embs_post[0].to("cpu")]
        mixed_embs_post = torch.unsqueeze(mixed_embs_post[0], 0)
        mixed_embs = [mixed_embs_pre] + [mixed_embs_post]
        mixed_embs = torch.cat(mixed_embs, dim=1)
        return mixed_embs


if __name__ == "__main__":
    args = parser.parse_args()

    device = args.device
    precision = args.precision
    _compile = args.compile
    max_length = args.max_length
    max_new_tokens = args.max_new_tokens
    print("Will run SHARK MultiModal for the following paramters :-\n")
    print(
        f"Device={device} precision={precision} compile={_compile} max_length={max_length} max_new_tokens={max_new_tokens}"
    )

    padding = max_length - max_new_tokens
    assert (
        padding > 0
    ), "max_length should be strictly greater than max_new_tokens"

    if args.image_path == "":
        print(
            "To run MiniGPT4 in CLI mode please provide an image's path using --image_path"
        )
        sys.exit()

    vision_model_precision = precision
    if precision in ["int4", "int8"]:
        vision_model_precision = "fp16"
    vision_model_vmfb_path = (
        Path(f"vision_model_{vision_model_precision}_{device}.vmfb")
        if args.vision_model_vmfb_path is None
        else Path(args.vision_model_vmfb_path)
    )
    qformer_vmfb_path = (
        Path(f"qformer_fp32_{device}.vmfb")
        if args.qformer_vmfb_path is None
        else Path(args.qformer_vmfb_path)
    )
    chat = MiniGPT4(
        model_name="MiniGPT4",
        hf_model_path=None,
        max_new_tokens=30,
        device=device,
        precision=precision,
        _compile=_compile,
        vision_model_vmfb_path=vision_model_vmfb_path,
        qformer_vmfb_path=qformer_vmfb_path,
    )

    chat_state = CONV_VISION.copy()
    img_list = []
    chat.upload_img(args.image_path, chat_state, img_list)
    print(
        "Uploaded image successfully to the bot. You may now start chatting with the bot. Enter 'END' without quotes to end the interaction"
    )
    continue_execution = True

    while continue_execution:
        user_message = input("User: ")
        if user_message == "END":
            print("Bot: Good bye.\n")
            break
        chat.ask(user_message, chat_state)
        bot_message = chat.answer(
            conv=chat_state,
            img_list=img_list,
            num_beams=1,
            temperature=1.0,
            max_new_tokens=max_new_tokens,
            max_length=max_length,
        )[0]
        print("Bot: ", bot_message)

    del chat_state, img_list, chat

```

`apps/language_models/src/pipelines/minigpt4_utils/Qformer.py`:

```py
"""
 * Copyright (c) 2023, salesforce.com, inc.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause
 * By Junnan Li
 * Based on huggingface code base
 * https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert
"""

import math
from dataclasses import dataclass
from typing import Tuple, Dict, Any

import torch
from torch import Tensor, device, nn
import torch.utils.checkpoint
from torch import nn
from torch.nn import CrossEntropyLoss

from transformers.activations import ACT2FN
from transformers.modeling_outputs import (
    BaseModelOutputWithPastAndCrossAttentions,
    BaseModelOutputWithPoolingAndCrossAttentions,
    CausalLMOutputWithCrossAttentions,
    MaskedLMOutput,
)
from transformers.modeling_utils import (
    PreTrainedModel,
    apply_chunking_to_forward,
    find_pruneable_heads_and_indices,
    prune_linear_layer,
)
from transformers.utils import logging
from transformers.models.bert.configuration_bert import BertConfig

logger = logging.get_logger(__name__)


class BertEmbeddings(nn.Module):
    """Construct the embeddings from word and position embeddings."""

    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(
            config.vocab_size,
            config.hidden_size,
            padding_idx=config.pad_token_id,
        )
        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings, config.hidden_size
        )

        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
        # any TensorFlow checkpoint file
        self.LayerNorm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps, device="cpu"
        )
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        # position_ids (1, len position emb) is contiguous in memory and exported when serialized
        self.register_buffer(
            "position_ids",
            torch.arange(config.max_position_embeddings).expand((1, -1)),
        )
        self.position_embedding_type = getattr(
            config, "position_embedding_type", "absolute"
        )

        self.config = config

    def forward(
        self,
        input_ids=None,
        position_ids=None,
        query_embeds=None,
        past_key_values_length=0,
    ):
        if input_ids is not None:
            seq_length = input_ids.size()[1]
        else:
            seq_length = 0

        if position_ids is None:
            position_ids = self.position_ids[
                :, past_key_values_length : seq_length + past_key_values_length
            ].clone()

        if input_ids is not None:
            embeddings = self.word_embeddings(input_ids)
            if self.position_embedding_type == "absolute":
                position_embeddings = self.position_embeddings(position_ids)
                embeddings = embeddings + position_embeddings

            if query_embeds is not None:
                embeddings = torch.cat((query_embeds, embeddings), dim=1)
        else:
            embeddings = query_embeds

        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class BertSelfAttention(nn.Module):
    def __init__(self, config, is_cross_attention):
        super().__init__()
        self.config = config
        if (
            config.hidden_size % config.num_attention_heads != 0
            and not hasattr(config, "embedding_size")
        ):
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(
            config.hidden_size / config.num_attention_heads
        )
        self.all_head_size = (
            self.num_attention_heads * self.attention_head_size
        )

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        if is_cross_attention:
            self.key = nn.Linear(config.encoder_width, self.all_head_size)
            self.value = nn.Linear(config.encoder_width, self.all_head_size)
        else:
            self.key = nn.Linear(config.hidden_size, self.all_head_size)
            self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(
            config, "position_embedding_type", "absolute"
        )
        if (
            self.position_embedding_type == "relative_key"
            or self.position_embedding_type == "relative_key_query"
        ):
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(
                2 * config.max_position_embeddings - 1,
                self.attention_head_size,
            )
        self.save_attention = False

    def save_attn_gradients(self, attn_gradients):
        self.attn_gradients = attn_gradients

    def get_attn_gradients(self):
        return self.attn_gradients

    def save_attention_map(self, attention_map):
        self.attention_map = attention_map

    def get_attention_map(self):
        return self.attention_map

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (
            self.num_attention_heads,
            self.attention_head_size,
        )
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
    ):
        # If this is instantiated as a cross-attention module, the keys
        # and values come from an encoder; the attention mask needs to be
        # such that the encoder's padding tokens are not attended to.
        is_cross_attention = encoder_hidden_states is not None

        if is_cross_attention:
            key_layer = self.transpose_for_scores(
                self.key(encoder_hidden_states)
            )
            value_layer = self.transpose_for_scores(
                self.value(encoder_hidden_states)
            )
            attention_mask = encoder_attention_mask
        elif past_key_value is not None:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)
        else:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))

        mixed_query_layer = self.query(hidden_states)

        query_layer = self.transpose_for_scores(mixed_query_layer)

        past_key_value = (key_layer, value_layer)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = torch.matmul(
            query_layer, key_layer.transpose(-1, -2)
        )

        if (
            self.position_embedding_type == "relative_key"
            or self.position_embedding_type == "relative_key_query"
        ):
            seq_length = hidden_states.size()[1]
            position_ids_l = torch.arange(
                seq_length, dtype=torch.long, device=hidden_states.device
            ).view(-1, 1)
            position_ids_r = torch.arange(
                seq_length, dtype=torch.long, device=hidden_states.device
            ).view(1, -1)
            distance = position_ids_l - position_ids_r
            positional_embedding = self.distance_embedding(
                distance + self.max_position_embeddings - 1
            )
            positional_embedding = positional_embedding.to(
                dtype=query_layer.dtype
            )  # fp16 compatibility

            if self.position_embedding_type == "relative_key":
                relative_position_scores = torch.einsum(
                    "bhld,lrd->bhlr", query_layer, positional_embedding
                )
                attention_scores = attention_scores + relative_position_scores
            elif self.position_embedding_type == "relative_key_query":
                relative_position_scores_query = torch.einsum(
                    "bhld,lrd->bhlr", query_layer, positional_embedding
                )
                relative_position_scores_key = torch.einsum(
                    "bhrd,lrd->bhlr", key_layer, positional_embedding
                )
                attention_scores = (
                    attention_scores
                    + relative_position_scores_query
                    + relative_position_scores_key
                )

        attention_scores = attention_scores / math.sqrt(
            self.attention_head_size
        )
        if attention_mask is not None:
            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
            attention_scores = attention_scores + attention_mask

        # Normalize the attention scores to probabilities.
        attention_probs = nn.Softmax(dim=-1)(attention_scores)

        if is_cross_attention and self.save_attention:
            self.save_attention_map(attention_probs)
            attention_probs.register_hook(self.save_attn_gradients)

        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs_dropped = self.dropout(attention_probs)

        # Mask heads if we want to
        if head_mask is not None:
            attention_probs_dropped = attention_probs_dropped * head_mask

        context_layer = torch.matmul(attention_probs_dropped, value_layer)

        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (
            self.all_head_size,
        )
        context_layer = context_layer.view(*new_context_layer_shape)

        outputs = (
            (context_layer, attention_probs)
            if output_attentions
            else (context_layer,)
        )

        outputs = outputs + (past_key_value,)
        return outputs


class BertSelfOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class BertAttention(nn.Module):
    def __init__(self, config, is_cross_attention=False):
        super().__init__()
        self.self = BertSelfAttention(config, is_cross_attention)
        self.output = BertSelfOutput(config)
        self.pruned_heads = set()

    def prune_heads(self, heads):
        if len(heads) == 0:
            return
        heads, index = find_pruneable_heads_and_indices(
            heads,
            self.self.num_attention_heads,
            self.self.attention_head_size,
            self.pruned_heads,
        )

        # Prune linear layers
        self.self.query = prune_linear_layer(self.self.query, index)
        self.self.key = prune_linear_layer(self.self.key, index)
        self.self.value = prune_linear_layer(self.self.value, index)
        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)

        # Update hyper params and store pruned heads
        self.self.num_attention_heads = self.self.num_attention_heads - len(
            heads
        )
        self.self.all_head_size = (
            self.self.attention_head_size * self.self.num_attention_heads
        )
        self.pruned_heads = self.pruned_heads.union(heads)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
    ):
        self_outputs = self.self(
            hidden_states,
            attention_mask,
            head_mask,
            encoder_hidden_states,
            encoder_attention_mask,
            past_key_value,
            output_attentions,
        )
        attention_output = self.output(self_outputs[0], hidden_states)

        outputs = (attention_output,) + self_outputs[
            1:
        ]  # add attentions if we output them
        return outputs


class BertIntermediate(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states


class BertOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class BertLayer(nn.Module):
    def __init__(self, config, layer_num):
        super().__init__()
        self.config = config
        self.chunk_size_feed_forward = config.chunk_size_feed_forward
        self.seq_len_dim = 1
        self.attention = BertAttention(config)
        self.layer_num = layer_num
        if (
            self.config.add_cross_attention
            and layer_num % self.config.cross_attention_freq == 0
        ):
            self.crossattention = BertAttention(
                config, is_cross_attention=self.config.add_cross_attention
            )
            self.has_cross_attention = True
        else:
            self.has_cross_attention = False
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

        self.intermediate_query = BertIntermediate(config)
        self.output_query = BertOutput(config)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
        query_length=0,
    ):
        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2
        self_attn_past_key_value = (
            past_key_value[:2] if past_key_value is not None else None
        )
        self_attention_outputs = self.attention(
            hidden_states,
            attention_mask,
            head_mask,
            output_attentions=output_attentions,
            past_key_value=self_attn_past_key_value,
        )
        attention_output = self_attention_outputs[0]
        outputs = self_attention_outputs[1:-1]

        present_key_value = self_attention_outputs[-1]

        if query_length > 0:
            query_attention_output = attention_output[:, :query_length, :]

            if self.has_cross_attention:
                assert (
                    encoder_hidden_states is not None
                ), "encoder_hidden_states must be given for cross-attention layers"
                cross_attention_outputs = self.crossattention(
                    query_attention_output,
                    attention_mask,
                    head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                    output_attentions=output_attentions,
                )
                query_attention_output = cross_attention_outputs[0]
                outputs = (
                    outputs + cross_attention_outputs[1:-1]
                )  # add cross attentions if we output attention weights

            layer_output = apply_chunking_to_forward(
                self.feed_forward_chunk_query,
                self.chunk_size_feed_forward,
                self.seq_len_dim,
                query_attention_output,
            )
            if attention_output.shape[1] > query_length:
                layer_output_text = apply_chunking_to_forward(
                    self.feed_forward_chunk,
                    self.chunk_size_feed_forward,
                    self.seq_len_dim,
                    attention_output[:, query_length:, :],
                )
                layer_output = torch.cat(
                    [layer_output, layer_output_text], dim=1
                )
        else:
            layer_output = apply_chunking_to_forward(
                self.feed_forward_chunk,
                self.chunk_size_feed_forward,
                self.seq_len_dim,
                attention_output,
            )
        outputs = (layer_output,) + outputs

        outputs = outputs + (present_key_value,)

        return outputs

    def feed_forward_chunk(self, attention_output):
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output

    def feed_forward_chunk_query(self, attention_output):
        intermediate_output = self.intermediate_query(attention_output)
        layer_output = self.output_query(intermediate_output, attention_output)
        return layer_output


class BertEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList(
            [BertLayer(config, i) for i in range(config.num_hidden_layers)]
        )

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_values=None,
        use_cache=None,
        output_attentions=False,
        output_hidden_states=False,
        return_dict=True,
        query_length=0,
    ):
        all_hidden_states = () if output_hidden_states else None
        all_self_attentions = () if output_attentions else None
        all_cross_attentions = (
            ()
            if output_attentions and self.config.add_cross_attention
            else None
        )

        next_decoder_cache = () if use_cache else None

        for i in range(self.config.num_hidden_layers):
            layer_module = self.layer[i]
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_head_mask = head_mask[i] if head_mask is not None else None
            past_key_value = (
                past_key_values[i] if past_key_values is not None else None
            )

            if (
                getattr(self.config, "gradient_checkpointing", False)
                and self.training
            ):
                if use_cache:
                    logger.warn(
                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                    )
                    use_cache = False

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        return module(
                            *inputs,
                            past_key_value,
                            output_attentions,
                            query_length
                        )

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(layer_module),
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                )
            else:
                layer_outputs = layer_module(
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                    past_key_value,
                    output_attentions,
                    query_length,
                )

            hidden_states = layer_outputs[0]
            if use_cache:
                next_decoder_cache += (layer_outputs[-1],)
            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)
                all_cross_attentions = all_cross_attentions + (
                    layer_outputs[2],
                )

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(
                v
                for v in [
                    hidden_states,
                    next_decoder_cache,
                    all_hidden_states,
                    all_self_attentions,
                    all_cross_attentions,
                ]
                if v is not None
            )
        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=next_decoder_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )


class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output


class BertPredictionHeadTransform(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        if isinstance(config.hidden_act, str):
            self.transform_act_fn = ACT2FN[config.hidden_act]
        else:
            self.transform_act_fn = config.hidden_act
        self.LayerNorm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.transform_act_fn(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        return hidden_states


class BertLMPredictionHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)

        # The output weights are the same as the input embeddings, but there is
        # an output-only bias for each token.
        self.decoder = nn.Linear(
            config.hidden_size, config.vocab_size, bias=False
        )

        self.bias = nn.Parameter(torch.zeros(config.vocab_size))

        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`
        self.decoder.bias = self.bias

    def forward(self, hidden_states):
        hidden_states = self.transform(hidden_states)
        hidden_states = self.decoder(hidden_states)
        return hidden_states


class BertOnlyMLMHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)

    def forward(self, sequence_output):
        prediction_scores = self.predictions(sequence_output)
        return prediction_scores


class BertPreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """

    config_class = BertConfig
    base_model_prefix = "bert"
    _keys_to_ignore_on_load_missing = [r"position_ids"]

    def _init_weights(self, module):
        """Initialize the weights"""
        if isinstance(module, (nn.Linear, nn.Embedding)):
            # Slightly different from the TF version which uses truncated_normal for initialization
            # cf https://github.com/pytorch/pytorch/pull/5617
            module.weight.data.normal_(
                mean=0.0, std=self.config.initializer_range
            )
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()


class BertModel(BertPreTrainedModel):
    """
    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
    cross-attention is added between the self-attention layers, following the architecture described in `Attention is
    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an
    input to the forward pass.
    """

    def __init__(self, config, add_pooling_layer=False):
        super().__init__(config)
        self.config = config

        self.embeddings = BertEmbeddings(config)

        self.encoder = BertEncoder(config)

        self.pooler = BertPooler(config) if add_pooling_layer else None

        self.init_weights()

    def get_input_embeddings(self):
        return self.embeddings.word_embeddings

    def set_input_embeddings(self, value):
        self.embeddings.word_embeddings = value

    def _prune_heads(self, heads_to_prune):
        """
        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base
        class PreTrainedModel
        """
        for layer, heads in heads_to_prune.items():
            self.encoder.layer[layer].attention.prune_heads(heads)

    def get_extended_attention_mask(
        self,
        attention_mask: Tensor,
        input_shape: Tuple[int],
        device: device,
        is_decoder: bool,
        has_query: bool = False,
    ) -> Tensor:
        """
        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.

        Arguments:
            attention_mask (:obj:`torch.Tensor`):
                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.
            input_shape (:obj:`Tuple[int]`):
                The shape of the input to the model.
            device: (:obj:`torch.device`):
                The device of the input to the model.

        Returns:
            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.
        """
        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
        # ourselves in which case we just need to make it broadcastable to all heads.
        if attention_mask.dim() == 3:
            extended_attention_mask = attention_mask[:, None, :, :]
        elif attention_mask.dim() == 2:
            # Provided a padding mask of dimensions [batch_size, seq_length]
            # - if the model is a decoder, apply a causal mask in addition to the padding mask
            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]
            if is_decoder:
                batch_size, seq_length = input_shape

                seq_ids = torch.arange(seq_length, device=device)
                causal_mask = (
                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)
                    <= seq_ids[None, :, None]
                )

                # add a prefix ones mask to the causal mask
                # causal and attention masks must have same type with pytorch version < 1.3
                causal_mask = causal_mask.to(attention_mask.dtype)

                if causal_mask.shape[1] < attention_mask.shape[1]:
                    prefix_seq_len = (
                        attention_mask.shape[1] - causal_mask.shape[1]
                    )
                    if has_query:  # UniLM style attention mask
                        causal_mask = torch.cat(
                            [
                                torch.zeros(
                                    (batch_size, prefix_seq_len, seq_length),
                                    device=device,
                                    dtype=causal_mask.dtype,
                                ),
                                causal_mask,
                            ],
                            axis=1,
                        )
                    causal_mask = torch.cat(
                        [
                            torch.ones(
                                (
                                    batch_size,
                                    causal_mask.shape[1],
                                    prefix_seq_len,
                                ),
                                device=device,
                                dtype=causal_mask.dtype,
                            ),
                            causal_mask,
                        ],
                        axis=-1,
                    )
                extended_attention_mask = (
                    causal_mask[:, None, :, :]
                    * attention_mask[:, None, None, :]
                )
            else:
                extended_attention_mask = attention_mask[:, None, None, :]
        else:
            raise ValueError(
                "Wrong shape for input_ids (shape {}) or attention_mask (shape {})".format(
                    input_shape, attention_mask.shape
                )
            )

        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
        # masked positions, this operation will create a tensor which is 0.0 for
        # positions we want to attend and -10000.0 for masked positions.
        # Since we are adding it to the raw scores before the softmax, this is
        # effectively the same as removing these entirely.
        extended_attention_mask = extended_attention_mask.to(
            dtype=self.dtype
        )  # fp16 compatibility
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        return extended_attention_mask

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        head_mask=None,
        query_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_values=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        is_decoder=False,
    ):
        r"""
        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):
            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
            the model is configured as a decoder.
        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:
            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.
        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`
            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`
            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.
        use_cache (:obj:`bool`, `optional`):
            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up
            decoding (see :obj:`past_key_values`).
        """
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict
            if return_dict is not None
            else self.config.use_return_dict
        )

        # use_cache = use_cache if use_cache is not None else self.config.use_cache

        if input_ids is None:
            assert (
                query_embeds is not None
            ), "You have to specify query_embeds when input_ids is None"

        # past_key_values_length
        past_key_values_length = (
            past_key_values[0][0].shape[2] - self.config.query_length
            if past_key_values is not None
            else 0
        )

        query_length = query_embeds.shape[1] if query_embeds is not None else 0

        embedding_output = self.embeddings(
            input_ids=input_ids,
            position_ids=position_ids,
            query_embeds=query_embeds,
            past_key_values_length=past_key_values_length,
        )

        input_shape = embedding_output.size()[:-1]
        batch_size, seq_length = input_shape
        device = embedding_output.device

        if attention_mask is None:
            attention_mask = torch.ones(
                ((batch_size, seq_length + past_key_values_length)),
                device=device,
            )

        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
        # ourselves in which case we just need to make it broadcastable to all heads.
        if is_decoder:
            extended_attention_mask = self.get_extended_attention_mask(
                attention_mask,
                input_ids.shape,
                device,
                is_decoder,
                has_query=(query_embeds is not None),
            )
        else:
            extended_attention_mask = self.get_extended_attention_mask(
                attention_mask, input_shape, device, is_decoder
            )

        # If a 2D or 3D attention mask is provided for the cross-attention
        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
        if encoder_hidden_states is not None:
            if type(encoder_hidden_states) == list:
                (
                    encoder_batch_size,
                    encoder_sequence_length,
                    _,
                ) = encoder_hidden_states[0].size()
            else:
                (
                    encoder_batch_size,
                    encoder_sequence_length,
                    _,
                ) = encoder_hidden_states.size()
            encoder_hidden_shape = (
                encoder_batch_size,
                encoder_sequence_length,
            )

            if type(encoder_attention_mask) == list:
                encoder_extended_attention_mask = [
                    self.invert_attention_mask(mask)
                    for mask in encoder_attention_mask
                ]
            elif encoder_attention_mask is None:
                encoder_attention_mask = torch.ones(
                    encoder_hidden_shape, device=device
                )
                encoder_extended_attention_mask = self.invert_attention_mask(
                    encoder_attention_mask
                )
            else:
                encoder_extended_attention_mask = self.invert_attention_mask(
                    encoder_attention_mask
                )
        else:
            encoder_extended_attention_mask = None

        # Prepare head mask if needed
        # 1.0 in head_mask indicate we keep the head
        # attention_probs has shape bsz x n_heads x N x N
        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]
        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]
        head_mask = self.get_head_mask(
            head_mask, self.config.num_hidden_layers
        )

        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            query_length=query_length,
        )
        sequence_output = encoder_outputs[0]
        pooled_output = (
            self.pooler(sequence_output) if self.pooler is not None else None
        )

        if not return_dict:
            return (sequence_output, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPoolingAndCrossAttentions(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            past_key_values=encoder_outputs.past_key_values,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
            cross_attentions=encoder_outputs.cross_attentions,
        )


class BertLMHeadModel(BertPreTrainedModel):
    _keys_to_ignore_on_load_unexpected = [r"pooler"]
    _keys_to_ignore_on_load_missing = [
        r"position_ids",
        r"predictions.decoder.bias",
    ]

    def __init__(self, config):
        super().__init__(config)

        self.bert = BertModel(config, add_pooling_layer=False)
        self.cls = BertOnlyMLMHead(config)

        self.init_weights()

    def get_output_embeddings(self):
        return self.cls.predictions.decoder

    def set_output_embeddings(self, new_embeddings):
        self.cls.predictions.decoder = new_embeddings

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        head_mask=None,
        query_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        labels=None,
        past_key_values=None,
        use_cache=True,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        return_logits=False,
        is_decoder=True,
        reduction="mean",
    ):
        r"""
        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):
            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
            the model is configured as a decoder.
        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:
            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are
            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``
        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`
            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`
            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.
        use_cache (:obj:`bool`, `optional`):
            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up
            decoding (see :obj:`past_key_values`).
        Returns:
        Example::
            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig
            >>> import torch
            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
            >>> config = BertConfig.from_pretrained("bert-base-cased")
            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)
            >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
            >>> outputs = model(**inputs)
            >>> prediction_logits = outputs.logits
        """
        return_dict = (
            return_dict
            if return_dict is not None
            else self.config.use_return_dict
        )
        if labels is not None:
            use_cache = False
        if past_key_values is not None:
            query_embeds = None

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            head_mask=head_mask,
            query_embeds=query_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            is_decoder=is_decoder,
        )

        sequence_output = outputs[0]
        if query_embeds is not None:
            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]

        prediction_scores = self.cls(sequence_output)

        if return_logits:
            return prediction_scores[:, :-1, :].contiguous()

        lm_loss = None
        if labels is not None:
            # we are doing next-token prediction; shift prediction scores and input ids by one
            shifted_prediction_scores = prediction_scores[
                :, :-1, :
            ].contiguous()
            labels = labels[:, 1:].contiguous()
            loss_fct = CrossEntropyLoss(
                reduction=reduction, label_smoothing=0.1
            )
            lm_loss = loss_fct(
                shifted_prediction_scores.view(-1, self.config.vocab_size),
                labels.view(-1),
            )
            if reduction == "none":
                lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1)

        if not return_dict:
            output = (prediction_scores,) + outputs[2:]
            return ((lm_loss,) + output) if lm_loss is not None else output

        return CausalLMOutputWithCrossAttentions(
            loss=lm_loss,
            logits=prediction_scores,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            cross_attentions=outputs.cross_attentions,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        query_embeds,
        past=None,
        attention_mask=None,
        **model_kwargs
    ):
        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly
        if attention_mask is None:
            attention_mask = input_ids.new_ones(input_ids.shape)
        query_mask = input_ids.new_ones(query_embeds.shape[:-1])
        attention_mask = torch.cat([query_mask, attention_mask], dim=-1)

        # cut decoder_input_ids if past is used
        if past is not None:
            input_ids = input_ids[:, -1:]

        return {
            "input_ids": input_ids,
            "query_embeds": query_embeds,
            "attention_mask": attention_mask,
            "past_key_values": past,
            "encoder_hidden_states": model_kwargs.get(
                "encoder_hidden_states", None
            ),
            "encoder_attention_mask": model_kwargs.get(
                "encoder_attention_mask", None
            ),
            "is_decoder": True,
        }

    def _reorder_cache(self, past, beam_idx):
        reordered_past = ()
        for layer_past in past:
            reordered_past += (
                tuple(
                    past_state.index_select(0, beam_idx)
                    for past_state in layer_past
                ),
            )
        return reordered_past


class BertForMaskedLM(BertPreTrainedModel):
    _keys_to_ignore_on_load_unexpected = [r"pooler"]
    _keys_to_ignore_on_load_missing = [
        r"position_ids",
        r"predictions.decoder.bias",
    ]

    def __init__(self, config):
        super().__init__(config)

        self.bert = BertModel(config, add_pooling_layer=False)
        self.cls = BertOnlyMLMHead(config)

        self.init_weights()

    def get_output_embeddings(self):
        return self.cls.predictions.decoder

    def set_output_embeddings(self, new_embeddings):
        self.cls.predictions.decoder = new_embeddings

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        head_mask=None,
        query_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        return_logits=False,
        is_decoder=False,
    ):
        r"""
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,
            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored
            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``
        """

        return_dict = (
            return_dict
            if return_dict is not None
            else self.config.use_return_dict
        )

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            head_mask=head_mask,
            query_embeds=query_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            is_decoder=is_decoder,
        )

        if query_embeds is not None:
            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]
        prediction_scores = self.cls(sequence_output)

        if return_logits:
            return prediction_scores

        masked_lm_loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()  # -100 index = padding token
            masked_lm_loss = loss_fct(
                prediction_scores.view(-1, self.config.vocab_size),
                labels.view(-1),
            )

        if not return_dict:
            output = (prediction_scores,) + outputs[2:]
            return (
                ((masked_lm_loss,) + output)
                if masked_lm_loss is not None
                else output
            )

        return MaskedLMOutput(
            loss=masked_lm_loss,
            logits=prediction_scores,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

```

`apps/language_models/src/pipelines/minigpt4_utils/blip_processors.py`:

```py
"""
 Copyright (c) 2022, salesforce.com, inc.
 All rights reserved.
 SPDX-License-Identifier: BSD-3-Clause
 For full license text, see the LICENSE_Lavis file in the repo root or https://opensource.org/licenses/BSD-3-Clause
"""
from omegaconf import OmegaConf
from torchvision import transforms
from torchvision.transforms.functional import InterpolationMode


class BaseProcessor:
    def __init__(self):
        self.transform = lambda x: x
        return

    def __call__(self, item):
        return self.transform(item)

    @classmethod
    def from_config(cls, cfg=None):
        return cls()

    def build(self, **kwargs):
        cfg = OmegaConf.create(kwargs)

        return self.from_config(cfg)


class BlipImageBaseProcessor(BaseProcessor):
    def __init__(self, mean=None, std=None):
        if mean is None:
            mean = (0.48145466, 0.4578275, 0.40821073)
        if std is None:
            std = (0.26862954, 0.26130258, 0.27577711)

        self.normalize = transforms.Normalize(mean, std)


class Blip2ImageEvalProcessor(BlipImageBaseProcessor):
    def __init__(self, image_size=224, mean=None, std=None):
        super().__init__(mean=mean, std=std)

        self.transform = transforms.Compose(
            [
                transforms.Resize(
                    (image_size, image_size),
                    interpolation=InterpolationMode.BICUBIC,
                ),
                transforms.ToTensor(),
                self.normalize,
            ]
        )

    def __call__(self, item):
        return self.transform(item)

    @classmethod
    def from_config(cls, cfg=None):
        if cfg is None:
            cfg = OmegaConf.create()

        image_size = cfg.get("image_size", 224)

        mean = cfg.get("mean", None)
        std = cfg.get("std", None)

        return cls(image_size=image_size, mean=mean, std=std)

```

`apps/language_models/src/pipelines/minigpt4_utils/configs/cc_sbu_align.yaml`:

```yaml
datasets:
  cc_sbu_align:
    data_type: images
    build_info:
      storage: /path/to/cc_sbu_align/

```

`apps/language_models/src/pipelines/minigpt4_utils/configs/minigpt4.yaml`:

```yaml
model:
  arch: mini_gpt4

  # vit encoder
  image_size: 224
  drop_path_rate: 0
  use_grad_checkpoint: False
  vit_precision: "fp16"
  freeze_vit: True
  freeze_qformer: True

  # Q-Former
  num_query_token: 32

  # Vicuna
  llama_model: "lmsys/vicuna-7b-v1.3"

  # generation configs
  prompt: ""

preprocess:
    vis_processor:
        train:
          name: "blip2_image_train"
          image_size: 224
        eval:
          name: "blip2_image_eval"
          image_size: 224
    text_processor:
        train:
          name: "blip_caption"
        eval:
          name: "blip_caption"

```

`apps/language_models/src/pipelines/minigpt4_utils/configs/minigpt4_eval.yaml`:

```yaml
model:
  arch: mini_gpt4
  model_type: pretrain_vicuna
  freeze_vit: True
  freeze_qformer: True
  max_txt_len: 160
  end_sym: "###"
  low_resource: False
  prompt_path: "apps/language_models/src/pipelines/minigpt4_utils/prompts/alignment.txt"
  prompt_template: '###Human: {} ###Assistant: '
  ckpt: 'prerained_minigpt4_7b.pth'


datasets:
  cc_sbu_align:
    vis_processor:
      train:
        name: "blip2_image_eval"
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"

run:
  task: image_text_pretrain

```

`apps/language_models/src/pipelines/minigpt4_utils/eva_vit.py`:

```py
# Based on EVA, BEIT, timm and DeiT code bases
# https://github.com/baaivision/EVA
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/facebookresearch/deit/
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import math
import requests
from functools import partial

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from timm.models.layers import drop_path, to_2tuple, trunc_normal_


def _cfg(url="", **kwargs):
    return {
        "url": url,
        "num_classes": 1000,
        "input_size": (3, 224, 224),
        "pool_size": None,
        "crop_pct": 0.9,
        "interpolation": "bicubic",
        "mean": (0.5, 0.5, 0.5),
        "std": (0.5, 0.5, 0.5),
        **kwargs,
    }


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

    def extra_repr(self) -> str:
        return "p={}".format(self.drop_prob)


class Mlp(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        # x = self.drop(x)
        # commit this for the orignal BERT implement
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=False,
        qk_scale=None,
        attn_drop=0.0,
        proj_drop=0.0,
        window_size=None,
        attn_head_dim=None,
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        if attn_head_dim is not None:
            head_dim = attn_head_dim
        all_head_dim = head_dim * self.num_heads
        self.scale = qk_scale or head_dim**-0.5

        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
        else:
            self.q_bias = None
            self.v_bias = None

        if window_size:
            self.window_size = window_size
            self.num_relative_distance = (2 * window_size[0] - 1) * (
                2 * window_size[1] - 1
            ) + 3
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros(self.num_relative_distance, num_heads)
            )  # 2*Wh-1 * 2*Ww-1, nH
            # cls to token & token 2 cls & cls to cls

            # get pair-wise relative position index for each token inside the window
            coords_h = torch.arange(window_size[0])
            coords_w = torch.arange(window_size[1])
            coords = torch.stack(
                torch.meshgrid([coords_h, coords_w])
            )  # 2, Wh, Ww
            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
            relative_coords = (
                coords_flatten[:, :, None] - coords_flatten[:, None, :]
            )  # 2, Wh*Ww, Wh*Ww
            relative_coords = relative_coords.permute(
                1, 2, 0
            ).contiguous()  # Wh*Ww, Wh*Ww, 2
            relative_coords[:, :, 0] += (
                window_size[0] - 1
            )  # shift to start from 0
            relative_coords[:, :, 1] += window_size[1] - 1
            relative_coords[:, :, 0] *= 2 * window_size[1] - 1
            relative_position_index = torch.zeros(
                size=(window_size[0] * window_size[1] + 1,) * 2,
                dtype=relative_coords.dtype,
            )
            relative_position_index[1:, 1:] = relative_coords.sum(
                -1
            )  # Wh*Ww, Wh*Ww
            relative_position_index[0, 0:] = self.num_relative_distance - 3
            relative_position_index[0:, 0] = self.num_relative_distance - 2
            relative_position_index[0, 0] = self.num_relative_distance - 1

            self.register_buffer(
                "relative_position_index", relative_position_index
            )
        else:
            self.window_size = None
            self.relative_position_bias_table = None
            self.relative_position_index = None

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(all_head_dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, rel_pos_bias=None):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat(
                (
                    self.q_bias,
                    torch.zeros_like(self.v_bias, requires_grad=False),
                    self.v_bias,
                )
            )
        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = (
            qkv[0],
            qkv[1],
            qkv[2],
        )  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = q @ k.transpose(-2, -1)

        if self.relative_position_bias_table is not None:
            relative_position_bias = self.relative_position_bias_table[
                self.relative_position_index.view(-1)
            ].view(
                self.window_size[0] * self.window_size[1] + 1,
                self.window_size[0] * self.window_size[1] + 1,
                -1,
            )  # Wh*Ww,Wh*Ww,nH
            relative_position_bias = relative_position_bias.permute(
                2, 0, 1
            ).contiguous()  # nH, Wh*Ww, Wh*Ww
            attn = attn + relative_position_bias.unsqueeze(0)

        if rel_pos_bias is not None:
            attn = attn + rel_pos_bias

        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):
    def __init__(
        self,
        dim,
        num_heads,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        drop=0.0,
        attn_drop=0.0,
        drop_path=0.0,
        init_values=None,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm,
        window_size=None,
        attn_head_dim=None,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            attn_drop=attn_drop,
            proj_drop=drop,
            window_size=window_size,
            attn_head_dim=attn_head_dim,
        )
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = (
            DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        )
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop,
        )

        if init_values is not None and init_values > 0:
            self.gamma_1 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True
            )
            self.gamma_2 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True
            )
        else:
            self.gamma_1, self.gamma_2 = None, None

    def forward(self, x, rel_pos_bias=None):
        if self.gamma_1 is None:
            x = x + self.drop_path(
                self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias)
            )
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x = x + self.drop_path(
                self.gamma_1
                * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias)
            )
            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
        return x


class PatchEmbed(nn.Module):
    """Image to Patch Embedding"""

    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        num_patches = (img_size[1] // patch_size[1]) * (
            img_size[0] // patch_size[0]
        )
        self.patch_shape = (
            img_size[0] // patch_size[0],
            img_size[1] // patch_size[1],
        )
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x, **kwargs):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert (
            H == self.img_size[0] and W == self.img_size[1]
        ), f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


class RelativePositionBias(nn.Module):
    def __init__(self, window_size, num_heads):
        super().__init__()
        self.window_size = window_size
        self.num_relative_distance = (2 * window_size[0] - 1) * (
            2 * window_size[1] - 1
        ) + 3
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros(self.num_relative_distance, num_heads)
        )  # 2*Wh-1 * 2*Ww-1, nH
        # cls to token & token 2 cls & cls to cls

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(window_size[0])
        coords_w = torch.arange(window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = (
            coords_flatten[:, :, None] - coords_flatten[:, None, :]
        )  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(
            1, 2, 0
        ).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * window_size[1] - 1
        relative_position_index = torch.zeros(
            size=(window_size[0] * window_size[1] + 1,) * 2,
            dtype=relative_coords.dtype,
        )
        relative_position_index[1:, 1:] = relative_coords.sum(
            -1
        )  # Wh*Ww, Wh*Ww
        relative_position_index[0, 0:] = self.num_relative_distance - 3
        relative_position_index[0:, 0] = self.num_relative_distance - 2
        relative_position_index[0, 0] = self.num_relative_distance - 1

        self.register_buffer(
            "relative_position_index", relative_position_index
        )

        # trunc_normal_(self.relative_position_bias_table, std=.02)

    def forward(self):
        relative_position_bias = self.relative_position_bias_table[
            self.relative_position_index.view(-1)
        ].view(
            self.window_size[0] * self.window_size[1] + 1,
            self.window_size[0] * self.window_size[1] + 1,
            -1,
        )  # Wh*Ww,Wh*Ww,nH
        return relative_position_bias.permute(
            2, 0, 1
        ).contiguous()  # nH, Wh*Ww, Wh*Ww


class VisionTransformer(nn.Module):
    """Vision Transformer with support for patch or hybrid CNN input stage"""

    def __init__(
        self,
        img_size=224,
        patch_size=16,
        in_chans=3,
        num_classes=1000,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.0,
        norm_layer=nn.LayerNorm,
        init_values=None,
        use_abs_pos_emb=True,
        use_rel_pos_bias=False,
        use_shared_rel_pos_bias=False,
        use_mean_pooling=True,
        init_scale=0.001,
        use_checkpoint=False,
    ):
        super().__init__()
        self.image_size = img_size
        self.num_classes = num_classes
        self.num_features = (
            self.embed_dim
        ) = embed_dim  # num_features for consistency with other models

        self.patch_embed = PatchEmbed(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
        )
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        if use_abs_pos_emb:
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches + 1, embed_dim)
            )
        else:
            self.pos_embed = None
        self.pos_drop = nn.Dropout(p=drop_rate)

        if use_shared_rel_pos_bias:
            self.rel_pos_bias = RelativePositionBias(
                window_size=self.patch_embed.patch_shape, num_heads=num_heads
            )
        else:
            self.rel_pos_bias = None
        self.use_checkpoint = use_checkpoint

        dpr = [
            x.item() for x in torch.linspace(0, drop_path_rate, depth)
        ]  # stochastic depth decay rule
        self.use_rel_pos_bias = use_rel_pos_bias
        self.blocks = nn.ModuleList(
            [
                Block(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    drop=drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[i],
                    norm_layer=norm_layer,
                    init_values=init_values,
                    window_size=self.patch_embed.patch_shape
                    if use_rel_pos_bias
                    else None,
                )
                for i in range(depth)
            ]
        )
        #         self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)
        #         self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None
        #         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        if self.pos_embed is not None:
            trunc_normal_(self.pos_embed, std=0.02)
        trunc_normal_(self.cls_token, std=0.02)
        # trunc_normal_(self.mask_token, std=.02)
        #         if isinstance(self.head, nn.Linear):
        #             trunc_normal_(self.head.weight, std=.02)
        self.apply(self._init_weights)
        self.fix_init_weight()

    #         if isinstance(self.head, nn.Linear):
    #             self.head.weight.data.mul_(init_scale)
    #             self.head.bias.data.mul_(init_scale)

    def fix_init_weight(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes, global_pool=""):
        self.num_classes = num_classes
        self.head = (
            nn.Linear(self.embed_dim, num_classes)
            if num_classes > 0
            else nn.Identity()
        )

    def forward_features(self, x):
        x = self.patch_embed(x)
        batch_size, seq_len, _ = x.size()

        cls_tokens = self.cls_token.expand(
            batch_size, -1, -1
        )  # stole cls_tokens impl from Phil Wang, thanks
        x = torch.cat((cls_tokens, x), dim=1)
        if self.pos_embed is not None:
            x = x + self.pos_embed
        x = self.pos_drop(x)

        rel_pos_bias = (
            self.rel_pos_bias() if self.rel_pos_bias is not None else None
        )
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x, rel_pos_bias)
            else:
                x = blk(x, rel_pos_bias)
        return x

    #         x = self.norm(x)

    #         if self.fc_norm is not None:
    #             t = x[:, 1:, :]
    #             return self.fc_norm(t.mean(1))
    #         else:
    #             return x[:, 0]

    def forward(self, x):
        x = self.forward_features(x)
        #         x = self.head(x)
        return x

    def get_intermediate_layers(self, x):
        x = self.patch_embed(x)
        batch_size, seq_len, _ = x.size()

        cls_tokens = self.cls_token.expand(
            batch_size, -1, -1
        )  # stole cls_tokens impl from Phil Wang, thanks
        x = torch.cat((cls_tokens, x), dim=1)
        if self.pos_embed is not None:
            x = x + self.pos_embed
        x = self.pos_drop(x)

        features = []
        rel_pos_bias = (
            self.rel_pos_bias() if self.rel_pos_bias is not None else None
        )
        for blk in self.blocks:
            x = blk(x, rel_pos_bias)
            features.append(x)

        return features


def interpolate_pos_embed(model, checkpoint_model):
    if "pos_embed" in checkpoint_model:
        pos_embed_checkpoint = checkpoint_model["pos_embed"].float()
        embedding_size = pos_embed_checkpoint.shape[-1]
        num_patches = model.patch_embed.num_patches
        num_extra_tokens = model.pos_embed.shape[-2] - num_patches
        # height (== width) for the checkpoint position embedding
        orig_size = int(
            (pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5
        )
        # height (== width) for the new position embedding
        new_size = int(num_patches**0.5)
        # class_token and dist_token are kept unchanged
        if orig_size != new_size:
            print(
                "Position interpolate from %dx%d to %dx%d"
                % (orig_size, orig_size, new_size, new_size)
            )
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            # only the position tokens are interpolated
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            pos_tokens = pos_tokens.reshape(
                -1, orig_size, orig_size, embedding_size
            ).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens,
                size=(new_size, new_size),
                mode="bicubic",
                align_corners=False,
            )
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model["pos_embed"] = new_pos_embed


def convert_weights_to_fp16(model: nn.Module):
    """Convert applicable model parameters to fp16"""

    def _convert_weights_to_fp16(l):
        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            # l.weight.data = l.weight.data.half()
            l.weight.data = l.weight.data
            if l.bias is not None:
                # l.bias.data = l.bias.data.half()
                l.bias.data = l.bias.data

    #         if isinstance(l, (nn.MultiheadAttention, Attention)):
    #             for attr in [*[f"{s}_proj_weight" for s in ["in", "q", "k", "v"]], "in_proj_bias", "bias_k", "bias_v"]:
    #                 tensor = getattr(l, attr)
    #                 if tensor is not None:
    #                     tensor.data = tensor.data.half()

    model.apply(_convert_weights_to_fp16)


def create_eva_vit_g(
    img_size=224, drop_path_rate=0.4, use_checkpoint=False, precision="fp16"
):
    model = VisionTransformer(
        img_size=img_size,
        patch_size=14,
        use_mean_pooling=False,
        embed_dim=1408,
        depth=39,
        num_heads=1408 // 88,
        mlp_ratio=4.3637,
        qkv_bias=True,
        drop_path_rate=drop_path_rate,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        use_checkpoint=use_checkpoint,
    )
    url = "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/eva_vit_g.pth"

    local_filename = "eva_vit_g.pth"
    response = requests.get(url)
    if response.status_code == 200:
        with open(local_filename, "wb") as f:
            f.write(response.content)
        print("File downloaded successfully.")
    state_dict = torch.load(local_filename, map_location="cpu")
    interpolate_pos_embed(model, state_dict)

    incompatible_keys = model.load_state_dict(state_dict, strict=False)

    if precision == "fp16":
        #         model.to("cuda")
        convert_weights_to_fp16(model)
    return model

```

`apps/language_models/src/pipelines/minigpt4_utils/prompts/alignment.txt`:

```txt
<Img><ImageHere></Img> Describe this image in detail.
<Img><ImageHere></Img> Take a look at this image and describe what you notice.
<Img><ImageHere></Img> Please provide a detailed description of the picture.
<Img><ImageHere></Img> Could you describe the contents of this image for me?
```

`apps/language_models/src/pipelines/stablelm_pipeline.py`:

```py
import torch
import torch_mlir
from transformers import AutoTokenizer, StoppingCriteria, AutoModelForCausalLM
from io import BytesIO
from pathlib import Path
from apps.language_models.utils import (
    get_torch_mlir_module_bytecode,
    get_vmfb_from_path,
)
from apps.language_models.src.pipelines.SharkLLMBase import SharkLLMBase
from apps.language_models.src.model_wrappers.stablelm_model import (
    StableLMModel,
)


class StopOnTokens(StoppingCriteria):
    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
    ) -> bool:
        stop_ids = [50278, 50279, 50277, 1, 0]
        for stop_id in stop_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False


class SharkStableLM(SharkLLMBase):
    def __init__(
        self,
        model_name,
        hf_model_path="stabilityai/stablelm-tuned-alpha-3b",
        max_num_tokens=512,
        device="cuda",
        precision="fp32",
        debug="False",
    ) -> None:
        super().__init__(model_name, hf_model_path, max_num_tokens)
        self.max_sequence_len = 256
        self.device = device
        self.precision = precision
        self.debug = debug
        self.tokenizer = self.get_tokenizer()
        self.shark_model = self.compile()

    def shouldStop(self, tokens):
        stop_ids = [50278, 50279, 50277, 1, 0]
        for stop_id in stop_ids:
            if tokens[0][-1] == stop_id:
                return True
        return False

    def get_src_model(self):
        model = AutoModelForCausalLM.from_pretrained(
            self.hf_model_path, torch_dtype=torch.float32
        )
        return model

    def get_model_inputs(self):
        input_ids = torch.randint(3, (1, self.max_sequence_len))
        attention_mask = torch.randint(3, (1, self.max_sequence_len))
        return input_ids, attention_mask

    def compile(self):
        tmp_model_name = (
            f"stableLM_linalg_{self.precision}_seqLen{self.max_sequence_len}"
        )

        # device = "cuda"  # "cpu"
        # TODO: vmfb and mlir name should include precision and device
        model_vmfb_name = None
        vmfb_path = (
            Path(tmp_model_name + f"_{self.device}.vmfb")
            if model_vmfb_name is None
            else Path(model_vmfb_name)
        )
        shark_module = get_vmfb_from_path(
            vmfb_path, self.device, mlir_dialect="tm_tensor"
        )
        if shark_module is not None:
            return shark_module

        mlir_path = Path(tmp_model_name + ".mlir")
        print(
            f"[DEBUG] mlir path {mlir_path} {'exists' if mlir_path.exists() else 'does not exist'}"
        )
        if mlir_path.exists():
            with open(mlir_path, "rb") as f:
                bytecode = f.read()
        else:
            model = StableLMModel(self.get_src_model())
            model_inputs = self.get_model_inputs()
            ts_graph = get_torch_mlir_module_bytecode(model, model_inputs)
            module = torch_mlir.compile(
                ts_graph,
                [*model_inputs],
                torch_mlir.OutputType.LINALG_ON_TENSORS,
                use_tracing=False,
                verbose=False,
            )
            bytecode_stream = BytesIO()
            module.operation.write_bytecode(bytecode_stream)
            bytecode = bytecode_stream.getvalue()
        f_ = open(tmp_model_name + ".mlir", "wb")
        f_.write(bytecode)
        print("Saved mlir")
        f_.close()

        from shark.shark_inference import SharkInference

        shark_module = SharkInference(
            mlir_module=bytecode, device=self.device, mlir_dialect="tm_tensor"
        )
        shark_module.compile()

        path = shark_module.save_module(
            vmfb_path.parent.absolute(), vmfb_path.stem, debug=self.debug
        )
        print("Saved vmfb at ", str(path))

        return shark_module

    def get_tokenizer(self):
        tok = AutoTokenizer.from_pretrained(self.hf_model_path)
        tok.add_special_tokens({"pad_token": "<PAD>"})
        # print("[DEBUG] Sucessfully loaded the tokenizer to the memory")
        return tok

    def generate(self, prompt):
        words_list = []
        for i in range(self.max_num_tokens):
            params = {
                "new_text": prompt,
            }

            generated_token_op = self.generate_new_token(params)

            detok = generated_token_op["detok"]
            stop_generation = generated_token_op["stop_generation"]

            if stop_generation:
                break

            print(detok, end="", flush=True)  # this is for CLI and DEBUG
            words_list.append(detok)
            if detok == "":
                break
            prompt = prompt + detok
        return words_list

    def generate_new_token(self, params):
        new_text = params["new_text"]
        model_inputs = self.tokenizer(
            [new_text],
            padding="max_length",
            max_length=self.max_sequence_len,
            truncation=True,
            return_tensors="pt",
        )
        sum_attentionmask = torch.sum(model_inputs.attention_mask)
        output = self.shark_model(
            "forward", [model_inputs.input_ids, model_inputs.attention_mask]
        )
        output = torch.from_numpy(output)
        next_toks = torch.topk(output, 1)
        stop_generation = False
        if self.shouldStop(next_toks.indices):
            stop_generation = True
        new_token = next_toks.indices[0][int(sum_attentionmask) - 1]
        detok = self.tokenizer.decode(
            new_token,
            skip_special_tokens=True,
        )
        ret_dict = {
            "new_token": new_token,
            "detok": detok,
            "stop_generation": stop_generation,
        }
        return ret_dict


# Initialize a StopOnTokens object
system_prompt = """<|SYSTEM|># StableLM Tuned (Alpha version)
- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.
- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.
- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.
- StableLM will refuse to participate in anything that could harm a human.
"""

```

`apps/language_models/utils.py`:

```py
import torch
from torch.fx.experimental.proxy_tensor import make_fx
from torch._decomp import get_decompositions
from typing import List
from pathlib import Path
from shark.shark_downloader import download_public_file


# expects a Path / str as arg
# returns None if path not found or SharkInference module
def get_vmfb_from_path(vmfb_path, device, mlir_dialect, device_id=None):
    if not isinstance(vmfb_path, Path):
        vmfb_path = Path(vmfb_path)

    from shark.shark_inference import SharkInference

    if not vmfb_path.exists():
        return None

    print("Loading vmfb from: ", vmfb_path)
    print("Device from get_vmfb_from_path - ", device)
    shark_module = SharkInference(
        None, device=device, mlir_dialect=mlir_dialect, device_idx=device_id
    )
    shark_module.load_module(vmfb_path)
    print("Successfully loaded vmfb")
    return shark_module


def get_vmfb_from_config(
    shark_container,
    model,
    precision,
    device,
    vmfb_path,
    padding=None,
    device_id=None,
):
    vmfb_url = (
        f"gs://shark_tank/{shark_container}/{model}_{precision}_{device}"
    )
    if padding:
        vmfb_url = vmfb_url + f"_{padding}"
    vmfb_url = vmfb_url + ".vmfb"
    download_public_file(vmfb_url, vmfb_path.absolute(), single_file=True)
    return get_vmfb_from_path(
        vmfb_path, device, "tm_tensor", device_id=device_id
    )

```

`apps/shark_studio/api/llm.py`:

```py
from turbine_models.custom_models import stateless_llama
from shark.iree_utils.compile_utils import get_iree_compiled_module
from apps.shark_studio.api.utils import get_resource_path
import iree.runtime as ireert
import gc
import torch

llm_model_map = {
    "llama2_7b": {
        "initializer": stateless_llama.export_transformer_model,
        "hf_model_name": "meta-llama/Llama-2-7b-chat-hf",
        "stop_token": 2,
        "max_tokens": 4096,
    }
}


class LanguageModel:
    def __init__(
        self, model_name, hf_auth_token=None, device=None, precision="fp32"
    ):
        print(llm_model_map[model_name])
        self.hf_model_name = llm_model_map[model_name]["hf_model_name"]
        self.torch_ir, self.tokenizer = llm_model_map[model_name][
            "initializer"
        ](self.hf_model_name, hf_auth_token, compile_to="torch")
        self.tempfile_name = get_resource_path("llm.torch.tempfile")
        with open(self.tempfile_name, "w+") as f:
            f.write(self.torch_ir)
        del self.torch_ir
        gc.collect()

        self.device = device
        self.precision = precision
        self.max_tokens = llm_model_map[model_name]["max_tokens"]
        self.iree_module_dict = None
        self.compile()

    def compile(self) -> None:
        # this comes with keys: "vmfb", "config", and "temp_file_to_unlink".
        self.iree_module_dict = get_iree_compiled_module(
            self.tempfile_name, device=self.device, frontend="torch"
        )
        # TODO: delete the temp file

    def chat(self, prompt):
        history = []
        for iter in range(self.max_tokens):
            input_tensor = self.tokenizer(
                prompt, return_tensors="pt"
            ).input_ids
            device_inputs = [
                ireert.asdevicearray(
                    self.iree_module_dict["config"], input_tensor
                )
            ]
            if iter == 0:
                token = torch.tensor(
                    self.iree_module_dict["vmfb"]["run_initialize"](
                        *device_inputs
                    ).to_host()[0][0]
                )
            else:
                token = torch.tensor(
                    self.iree_module_dict["vmfb"]["run_forward"](
                        *device_inputs
                    ).to_host()[0][0]
                )

            history.append(token)
            yield self.tokenizer.decode(history)

            if token == llm_model_map["llama2_7b"]["stop_token"]:
                break

        for i in range(len(history)):
            if type(history[i]) != int:
                history[i] = int(history[i])
        result_output = self.tokenizer.decode(history)
        yield result_output


if __name__ == "__main__":
    lm = LanguageModel(
        "llama2_7b",
        hf_auth_token="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",
        device="cpu-task",
    )
    print("model loaded")
    for i in lm.chat("Hello, I am a robot."):
        print(i)

```

`apps/shark_studio/api/utils.py`:

```py
import os
import sys


def get_available_devices():
    return ["cpu-task"]


def get_resource_path(relative_path):
    """Get absolute path to resource, works for dev and for PyInstaller"""
    base_path = getattr(
        sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__))
    )
    return os.path.join(base_path, relative_path)

```

`apps/shark_studio/web/index.py`:

```py
from multiprocessing import Process, freeze_support
import os
import sys
import logging
from ui.chat import chat_element

if sys.platform == "darwin":
    os.environ["DYLD_LIBRARY_PATH"] = "/usr/local/lib"
    # import before IREE to avoid MLIR library issues
    import torch_mlir

# import PIL, transformers, sentencepiece  # ensures inclusion in pysintaller exe generation
# from apps.stable_diffusion.src import args, clear_all
# import apps.stable_diffusion.web.utils.global_obj as global_obj


def launch_app(address):
    from tkinter import Tk
    import webview

    window = Tk()

    # get screen width and height of display and make it more reasonably
    # sized as we aren't making it full-screen or maximized
    width = int(window.winfo_screenwidth() * 0.81)
    height = int(window.winfo_screenheight() * 0.91)
    webview.create_window(
        "SHARK AI Studio",
        url=address,
        width=width,
        height=height,
        text_select=True,
    )
    webview.start(private_mode=False, storage_path=os.getcwd())


if __name__ == "__main__":
    # if args.debug:
    logging.basicConfig(level=logging.DEBUG)
    # required to do multiprocessing in a pyinstaller freeze
    freeze_support()
    #    if args.api or "api" in args.ui.split(","):
    #        from apps.stable_diffusion.web.ui import (
    #            txt2img_api,
    #            img2img_api,
    #            upscaler_api,
    #            inpaint_api,
    #            outpaint_api,
    #            llm_chat_api,
    #        )
    #
    #        from fastapi import FastAPI, APIRouter
    #        import uvicorn
    #
    #        # init global sd pipeline and config
    #        global_obj._init()
    #
    #        app = FastAPI()
    #        app.add_api_route("/sdapi/v1/txt2img", txt2img_api, methods=["post"])
    #        app.add_api_route("/sdapi/v1/img2img", img2img_api, methods=["post"])
    #        app.add_api_route("/sdapi/v1/inpaint", inpaint_api, methods=["post"])
    #        app.add_api_route("/sdapi/v1/outpaint", outpaint_api, methods=["post"])
    #        app.add_api_route("/sdapi/v1/upscaler", upscaler_api, methods=["post"])
    #
    #        # chat APIs needed for compatibility with multiple extensions using OpenAI API
    #        app.add_api_route(
    #            "/v1/chat/completions", llm_chat_api, methods=["post"]
    #        )
    #        app.add_api_route("/v1/completions", llm_chat_api, methods=["post"])
    #        app.add_api_route("/chat/completions", llm_chat_api, methods=["post"])
    #        app.add_api_route("/completions", llm_chat_api, methods=["post"])
    #        app.add_api_route(
    #            "/v1/engines/codegen/completions", llm_chat_api, methods=["post"]
    #        )
    #        app.include_router(APIRouter())
    #        uvicorn.run(app, host="0.0.0.0", port=args.server_port)
    #        sys.exit(0)
    #
    # Setup to use shark_tmp for gradio's temporary image files and clear any
    # existing temporary images there if they exist. Then we can import gradio.
    # It has to be in this order or gradio ignores what we've set up.
    # from apps.stable_diffusion.web.utils.gradio_configs import (
    #    config_gradio_tmp_imgs_folder,
    # )

    # config_gradio_tmp_imgs_folder()
    import gradio as gr

    # Create custom models folders if they don't exist
    # from apps.stable_diffusion.web.ui.utils import create_custom_models_folders

    # create_custom_models_folders()

    def resource_path(relative_path):
        """Get absolute path to resource, works for dev and for PyInstaller"""
        base_path = getattr(
            sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__))
        )
        return os.path.join(base_path, relative_path)

    dark_theme = resource_path("ui/css/sd_dark_theme.css")

    # from apps.stable_diffusion.web.ui import (
    # txt2img_web,
    # txt2img_custom_model,
    # txt2img_gallery,
    # txt2img_png_info_img,
    # txt2img_status,
    # txt2img_sendto_img2img,
    # txt2img_sendto_inpaint,
    # txt2img_sendto_outpaint,
    # txt2img_sendto_upscaler,
    ## h2ogpt_upload,
    ## h2ogpt_web,
    # img2img_web,
    # img2img_custom_model,
    # img2img_gallery,
    # img2img_init_image,
    # img2img_status,
    # img2img_sendto_inpaint,
    # img2img_sendto_outpaint,
    # img2img_sendto_upscaler,
    # inpaint_web,
    # inpaint_custom_model,
    # inpaint_gallery,
    # inpaint_init_image,
    # inpaint_status,
    # inpaint_sendto_img2img,
    # inpaint_sendto_outpaint,
    # inpaint_sendto_upscaler,
    # outpaint_web,
    # outpaint_custom_model,
    # outpaint_gallery,
    # outpaint_init_image,
    # outpaint_status,
    # outpaint_sendto_img2img,
    # outpaint_sendto_inpaint,
    # outpaint_sendto_upscaler,
    # upscaler_web,
    # upscaler_custom_model,
    # upscaler_gallery,
    # upscaler_init_image,
    # upscaler_status,
    # upscaler_sendto_img2img,
    # upscaler_sendto_inpaint,
    # upscaler_sendto_outpaint,
    ##  lora_train_web,
    ##  model_web,
    ##  model_config_web,
    # hf_models,
    # modelmanager_sendto_txt2img,
    # modelmanager_sendto_img2img,
    # modelmanager_sendto_inpaint,
    # modelmanager_sendto_outpaint,
    # modelmanager_sendto_upscaler,
    # stablelm_chat,
    # minigpt4_web,
    # outputgallery_web,
    # outputgallery_tab_select,
    # outputgallery_watch,
    # outputgallery_filename,
    # outputgallery_sendto_txt2img,
    # outputgallery_sendto_img2img,
    # outputgallery_sendto_inpaint,
    # outputgallery_sendto_outpaint,
    # outputgallery_sendto_upscaler,
    # )

    # init global sd pipeline and config
    # global_obj._init()

    def register_button_click(button, selectedid, inputs, outputs):
        button.click(
            lambda x: (
                x[0]["name"] if len(x) != 0 else None,
                gr.Tabs.update(selected=selectedid),
            ),
            inputs,
            outputs,
        )

    def register_modelmanager_button(button, selectedid, inputs, outputs):
        button.click(
            lambda x: (
                "None",
                x,
                gr.Tabs.update(selected=selectedid),
            ),
            inputs,
            outputs,
        )

    def register_outputgallery_button(button, selectedid, inputs, outputs):
        button.click(
            lambda x: (
                x,
                gr.Tabs.update(selected=selectedid),
            ),
            inputs,
            outputs,
        )

    with gr.Blocks(
        css=dark_theme, analytics_enabled=False, title="Stable Diffusion"
    ) as sd_web:
        with gr.Tabs() as tabs:
            # NOTE: If adding, removing, or re-ordering tabs, make sure that they
            # have a unique id that doesn't clash with any of the other tabs,
            # and that the order in the code here is the order they should
            # appear in the ui, as the id value doesn't determine the order.

            # Where possible, avoid changing the id of any tab that is the
            # destination of one of the 'send to' buttons. If you do have to change
            # that id, make sure you update the relevant register_button_click calls
            # further down with the new id.
            # with gr.TabItem(label="Text-to-Image", id=0):
            #    txt2img_web.render()
            # with gr.TabItem(label="Image-to-Image", id=1):
            #    img2img_web.render()
            # with gr.TabItem(label="Inpainting", id=2):
            #    inpaint_web.render()
            # with gr.TabItem(label="Outpainting", id=3):
            #    outpaint_web.render()
            # with gr.TabItem(label="Upscaler", id=4):
            #    upscaler_web.render()
            # if args.output_gallery:
            #    with gr.TabItem(label="Output Gallery", id=5) as og_tab:
            #        outputgallery_web.render()

            #    # extra output gallery configuration
            #    outputgallery_tab_select(og_tab.select)
            #    outputgallery_watch(
            #        [
            #            txt2img_status,
            #            img2img_status,
            #            inpaint_status,
            #            outpaint_status,
            #            upscaler_status,
            #        ]
            #    )
            ##  with gr.TabItem(label="Model Manager", id=6):
            ##      model_web.render()
            ##  with gr.TabItem(label="LoRA Training (Experimental)", id=7):
            ##      lora_train_web.render()
            with gr.TabItem(label="Chat Bot", id=0):
                chat_element.render()
            ##  with gr.TabItem(
            ##      label="Generate Sharding Config (Experimental)", id=9
            ##  ):
            ##      model_config_web.render()
            # with gr.TabItem(label="MultiModal (Experimental)", id=10):
            #    minigpt4_web.render()
            # with gr.TabItem(label="DocuChat Upload", id=11):
            #     h2ogpt_upload.render()
            # with gr.TabItem(label="DocuChat(Experimental)", id=12):
            #     h2ogpt_web.render()

        # send to buttons
        # register_button_click(
        #    txt2img_sendto_img2img,
        #    1,
        #    [txt2img_gallery],
        #    [img2img_init_image, tabs],
        # )
        # register_button_click(
        #    txt2img_sendto_inpaint,
        #    2,
        #    [txt2img_gallery],
        #    [inpaint_init_image, tabs],
        # )
        # register_button_click(
        #    txt2img_sendto_outpaint,
        #    3,
        #    [txt2img_gallery],
        #    [outpaint_init_image, tabs],
        # )
        # register_button_click(
        #    txt2img_sendto_upscaler,
        #    4,
        #    [txt2img_gallery],
        #    [upscaler_init_image, tabs],
        # )
        # register_button_click(
        #    img2img_sendto_inpaint,
        #    2,
        #    [img2img_gallery],
        #    [inpaint_init_image, tabs],
        # )
        # register_button_click(
        #    img2img_sendto_outpaint,
        #    3,
        #    [img2img_gallery],
        #    [outpaint_init_image, tabs],
        # )
        # register_button_click(
        #    img2img_sendto_upscaler,
        #    4,
        #    [img2img_gallery],
        #    [upscaler_init_image, tabs],
        # )
        # register_button_click(
        #    inpaint_sendto_img2img,
        #    1,
        #    [inpaint_gallery],
        #    [img2img_init_image, tabs],
        # )
        # register_button_click(
        #    inpaint_sendto_outpaint,
        #    3,
        #    [inpaint_gallery],
        #    [outpaint_init_image, tabs],
        # )
        # register_button_click(
        #    inpaint_sendto_upscaler,
        #    4,
        #    [inpaint_gallery],
        #    [upscaler_init_image, tabs],
        # )
        # register_button_click(
        #    outpaint_sendto_img2img,
        #    1,
        #    [outpaint_gallery],
        #    [img2img_init_image, tabs],
        # )
        # register_button_click(
        #    outpaint_sendto_inpaint,
        #    2,
        #    [outpaint_gallery],
        #    [inpaint_init_image, tabs],
        # )
        # register_button_click(
        #    outpaint_sendto_upscaler,
        #    4,
        #    [outpaint_gallery],
        #    [upscaler_init_image, tabs],
        # )
        # register_button_click(
        #    upscaler_sendto_img2img,
        #    1,
        #    [upscaler_gallery],
        #    [img2img_init_image, tabs],
        # )
        # register_button_click(
        #    upscaler_sendto_inpaint,
        #    2,
        #    [upscaler_gallery],
        #    [inpaint_init_image, tabs],
        # )
        # register_button_click(
        #    upscaler_sendto_outpaint,
        #    3,
        #    [upscaler_gallery],
        #    [outpaint_init_image, tabs],
        # )
        # if args.output_gallery:
        #    register_outputgallery_button(
        #        outputgallery_sendto_txt2img,
        #        0,
        #        [outputgallery_filename],
        #        [txt2img_png_info_img, tabs],
        #    )
        #    register_outputgallery_button(
        #        outputgallery_sendto_img2img,
        #        1,
        #        [outputgallery_filename],
        #        [img2img_init_image, tabs],
        #    )
        #    register_outputgallery_button(
        #        outputgallery_sendto_inpaint,
        #        2,
        #        [outputgallery_filename],
        #        [inpaint_init_image, tabs],
        #    )
        #    register_outputgallery_button(
        #        outputgallery_sendto_outpaint,
        #        3,
        #        [outputgallery_filename],
        #        [outpaint_init_image, tabs],
        #    )
        #    register_outputgallery_button(
        #        outputgallery_sendto_upscaler,
        #        4,
        #        [outputgallery_filename],
        #        [upscaler_init_image, tabs],
        #    )
        # register_modelmanager_button(
        #    modelmanager_sendto_txt2img,
        #    0,
        #    [hf_models],
        #    [txt2img_custom_model, tabs],
        # )
        # register_modelmanager_button(
        #    modelmanager_sendto_img2img,
        #    1,
        #    [hf_models],
        #    [img2img_custom_model, tabs],
        # )
        # register_modelmanager_button(
        #    modelmanager_sendto_inpaint,
        #    2,
        #    [hf_models],
        #    [inpaint_custom_model, tabs],
        # )
        # register_modelmanager_button(
        #    modelmanager_sendto_outpaint,
        #    3,
        #    [hf_models],
        #    [outpaint_custom_model, tabs],
        # )
        # register_modelmanager_button(
        #    modelmanager_sendto_upscaler,
        #    4,
        #    [hf_models],
        #    [upscaler_custom_model, tabs],
        # )

    sd_web.queue()
    # if args.ui == "app":
    #    t = Process(
    #        target=launch_app, args=[f"http://localhost:{args.server_port}"]
    #    )
    #    t.start()
    sd_web.launch(
        share=True,
        inbrowser=True,
        server_name="0.0.0.0",
        server_port=11911,  # args.server_port,
    )

```

`apps/shark_studio/web/ui/chat.py`:

```py
import gradio as gr
import os
from pathlib import Path
from datetime import datetime as dt
import json
import sys
from apps.shark_studio.api.utils import (
    get_available_devices,
)
from apps.shark_studio.api.llm import (
    llm_model_map,
    LanguageModel,
)


def user(message, history):
    # Append the user's message to the conversation history
    return "", history + [[message, ""]]


language_model = None


# NOTE: Each `model_name` should have its own start message
start_message = {
    "llama2_7b": (
        "You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe. Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or "
        "illegal content. Please ensure that your responses are socially "
        "unbiased and positive in nature. If a question does not make any "
        "sense, or is not factually coherent, explain why instead of "
        "answering something not correct. If you don't know the answer "
        "to a question, please don't share false information."
    ),
    "llama2_13b": (
        "You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe. Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or "
        "illegal content. Please ensure that your responses are socially "
        "unbiased and positive in nature. If a question does not make any "
        "sense, or is not factually coherent, explain why instead of "
        "answering something not correct. If you don't know the answer "
        "to a question, please don't share false information."
    ),
    "llama2_70b": (
        "You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe. Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or "
        "illegal content. Please ensure that your responses are socially "
        "unbiased and positive in nature. If a question does not make any "
        "sense, or is not factually coherent, explain why instead of "
        "answering something not correct. If you don't know the answer "
        "to a question, please don't share false information."
    ),
    "vicuna": (
        "A chat between a curious user and an artificial intelligence "
        "assistant. The assistant gives helpful, detailed, and "
        "polite answers to the user's questions.\n"
    ),
}


def create_prompt(model_name, history, prompt_prefix):
    return ""
    system_message = ""
    if prompt_prefix:
        system_message = start_message[model_name]

    if "llama2" in model_name:
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        conversation = "".join(
            [f"{B_INST} {item[0]} {E_INST} {item[1]} " for item in history[1:]]
        )
        if prompt_prefix:
            msg = f"{B_INST} {B_SYS}{system_message}{E_SYS}{history[0][0]} {E_INST} {history[0][1]} {conversation}"
        else:
            msg = f"{B_INST} {history[0][0]} {E_INST} {history[0][1]} {conversation}"
    elif model_name in ["vicuna"]:
        conversation = "".join(
            [
                "".join(["<|USER|>" + item[0], "<|ASSISTANT|>" + item[1]])
                for item in history
            ]
        )
        msg = system_message + conversation
        msg = msg.strip()
    else:
        conversation = "".join(
            ["".join([item[0], item[1]]) for item in history]
        )
        msg = system_message + conversation
        msg = msg.strip()
    return msg


def get_default_config():
    return False
    import torch
    from transformers import AutoTokenizer

    hf_model_path = "TheBloke/vicuna-7B-1.1-HF"
    tokenizer = AutoTokenizer.from_pretrained(hf_model_path, use_fast=False)
    compilation_prompt = "".join(["0" for _ in range(17)])
    compilation_input_ids = tokenizer(
        compilation_prompt,
        return_tensors="pt",
    ).input_ids
    compilation_input_ids = torch.tensor(compilation_input_ids).reshape(
        [1, 19]
    )
    firstVicunaCompileInput = (compilation_input_ids,)
    from apps.language_models.src.model_wrappers.vicuna_model import (
        CombinedModel,
    )
    from shark.shark_generate_model_config import GenerateConfigFile

    model = CombinedModel()
    c = GenerateConfigFile(model, 1, ["gpu_id"], firstVicunaCompileInput)
    c.split_into_layers()


# model_vmfb_key = ""


def chat_fn(
    prompt_prefix,
    history,
    model,
    device,
    precision,
    download_vmfb,
    config_file,
    cli=False,
    progress=gr.Progress(),
):
    global language_model
    if language_model is None:
        language_model = LanguageModel(
            model, device=device, precision=precision
        )

    language_model.chat(prompt_prefix)
    return "", ""
    global past_key_values
    global model_vmfb_key

    device_id = None
    model_name, model_path = list(map(str.strip, model.split("=>")))
    if "cuda" in device:
        device = "cuda"
    elif "sync" in device:
        device = "cpu-sync"
    elif "task" in device:
        device = "cpu-task"
    elif "vulkan" in device:
        device_id = int(device.split("://")[1])
        device = "vulkan"
    elif "rocm" in device:
        device = "rocm"
    else:
        print("unrecognized device")

    from apps.language_models.scripts.vicuna import ShardedVicuna
    from apps.language_models.scripts.vicuna import UnshardedVicuna
    from apps.stable_diffusion.src import args

    new_model_vmfb_key = f"{model_name}#{model_path}#{device}#{device_id}#{precision}#{download_vmfb}"
    if vicuna_model is None or new_model_vmfb_key != model_vmfb_key:
        model_vmfb_key = new_model_vmfb_key
        max_toks = 128 if model_name == "codegen" else 512

        # get iree flags that need to be overridden, from commandline args
        _extra_args = []
        # vulkan target triple
        vulkan_target_triple = args.iree_vulkan_target_triple
        from shark.iree_utils.vulkan_utils import (
            get_all_vulkan_devices,
            get_vulkan_target_triple,
        )

        if device == "vulkan":
            vulkaninfo_list = get_all_vulkan_devices()
            if vulkan_target_triple == "":
                # We already have the device_id extracted via WebUI, so we directly use
                # that to find the target triple.
                vulkan_target_triple = get_vulkan_target_triple(
                    vulkaninfo_list[device_id]
                )
            _extra_args.append(
                f"-iree-vulkan-target-triple={vulkan_target_triple}"
            )
            if "rdna" in vulkan_target_triple:
                flags_to_add = [
                    "--iree-spirv-index-bits=64",
                ]
                _extra_args = _extra_args + flags_to_add

            if device_id is None:
                id = 0
                for device in vulkaninfo_list:
                    target_triple = get_vulkan_target_triple(
                        vulkaninfo_list[id]
                    )
                    if target_triple == vulkan_target_triple:
                        device_id = id
                        break
                    id += 1

                assert (
                    device_id
                ), f"no vulkan hardware for target-triple '{vulkan_target_triple}' exists"
            print(f"Will use vulkan target triple : {vulkan_target_triple}")

        elif "rocm" in device:
            # add iree rocm flags
            _extra_args.append(
                f"--iree-rocm-target-chip={args.iree_rocm_target_chip}"
            )
            print(f"extra args = {_extra_args}")

        if model_name == "vicuna4":
            vicuna_model = ShardedVicuna(
                model_name,
                hf_model_path=model_path,
                device=device,
                precision=precision,
                max_num_tokens=max_toks,
                compressed=True,
                extra_args_cmd=_extra_args,
            )
        else:
            #  if config_file is None:
            vicuna_model = UnshardedVicuna(
                model_name,
                hf_model_path=model_path,
                hf_auth_token=args.hf_auth_token,
                device=device,
                vulkan_target_triple=vulkan_target_triple,
                precision=precision,
                max_num_tokens=max_toks,
                download_vmfb=download_vmfb,
                load_mlir_from_shark_tank=True,
                extra_args_cmd=_extra_args,
                device_id=device_id,
            )

    if vicuna_model is None:
        sys.exit("Unable to instantiate the model object, exiting.")

    prompt = create_prompt(model_name, history, prompt_prefix)

    partial_text = ""
    token_count = 0
    total_time_ms = 0.001  # In order to avoid divide by zero error
    prefill_time = 0
    is_first = True
    for text, msg, exec_time in progress.tqdm(
        vicuna_model.generate(prompt, cli=cli),
        desc="generating response",
    ):
        if msg is None:
            if is_first:
                prefill_time = exec_time
                is_first = False
            else:
                total_time_ms += exec_time
                token_count += 1
            partial_text += text + " "
            history[-1][1] = partial_text
            yield history, f"Prefill: {prefill_time:.2f}"
        elif "formatted" in msg:
            history[-1][1] = text
            tokens_per_sec = (token_count / total_time_ms) * 1000
            yield history, f"Prefill: {prefill_time:.2f} seconds\n Decode: {tokens_per_sec:.2f} tokens/sec"
        else:
            sys.exit(
                "unexpected message from the vicuna generate call, exiting."
            )

    return history, ""


def llm_chat_api(InputData: dict):
    return None
    print(f"Input keys : {InputData.keys()}")
    # print(f"model : {InputData['model']}")
    is_chat_completion_api = (
        "messages" in InputData.keys()
    )  # else it is the legacy `completion` api
    # For Debugging input data from API
    # if is_chat_completion_api:
    #     print(f"message -> role : {InputData['messages'][0]['role']}")
    #     print(f"message -> content : {InputData['messages'][0]['content']}")
    # else:
    #     print(f"prompt : {InputData['prompt']}")
    # print(f"max_tokens : {InputData['max_tokens']}") # Default to 128 for now
    global vicuna_model
    model_name = (
        InputData["model"] if "model" in InputData.keys() else "codegen"
    )
    model_path = llm_model_map[model_name]
    device = "cpu-task"
    precision = "fp16"
    max_toks = (
        None
        if "max_tokens" not in InputData.keys()
        else InputData["max_tokens"]
    )
    if max_toks is None:
        max_toks = 128 if model_name == "codegen" else 512

    # make it working for codegen first
    from apps.language_models.scripts.vicuna import (
        UnshardedVicuna,
    )

    device_id = None
    if vicuna_model == 0:
        if "cuda" in device:
            device = "cuda"
        elif "sync" in device:
            device = "cpu-sync"
        elif "task" in device:
            device = "cpu-task"
        elif "vulkan" in device:
            device_id = int(device.split("://")[1])
            device = "vulkan"
        else:
            print("unrecognized device")

        vicuna_model = UnshardedVicuna(
            model_name,
            hf_model_path=model_path,
            device=device,
            precision=precision,
            max_num_tokens=max_toks,
            download_vmfb=True,
            load_mlir_from_shark_tank=True,
            device_id=device_id,
        )

    # TODO: add role dict for different models
    if is_chat_completion_api:
        # TODO: add funtionality for multiple messages
        prompt = create_prompt(
            model_name, [(InputData["messages"][0]["content"], "")]
        )
    else:
        prompt = InputData["prompt"]
    print("prompt = ", prompt)

    res = vicuna_model.generate(prompt)
    res_op = None
    for op in res:
        res_op = op

    if is_chat_completion_api:
        choices = [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": res_op,  # since we are yeilding the result
                },
                "finish_reason": "stop",  # or length
            }
        ]
    else:
        choices = [
            {
                "text": res_op,
                "index": 0,
                "logprobs": None,
                "finish_reason": "stop",  # or length
            }
        ]
    end_time = dt.now().strftime("%Y%m%d%H%M%S%f")
    return {
        "id": end_time,
        "object": "chat.completion"
        if is_chat_completion_api
        else "text_completion",
        "created": int(end_time),
        "choices": choices,
    }


def view_json_file(file_obj):
    content = ""
    with open(file_obj.name, "r") as fopen:
        content = fopen.read()
    return content


with gr.Blocks(title="Chat") as chat_element:
    with gr.Row():
        model_choices = list(llm_model_map.keys())
        model = gr.Dropdown(
            label="Select Model",
            value=model_choices[0],
            choices=model_choices,
            allow_custom_value=True,
        )
        supported_devices = get_available_devices()
        enabled = True
        if len(supported_devices) == 0:
            supported_devices = ["cpu-task"]
        supported_devices = [x for x in supported_devices if "sync" not in x]
        device = gr.Dropdown(
            label="Device",
            value=supported_devices[0],
            choices=supported_devices,
            interactive=enabled,
            allow_custom_value=True,
        )
        precision = gr.Radio(
            label="Precision",
            value="int4",
            choices=[
                # "int4",
                # "int8",
                # "fp16",
                "fp32",
            ],
            visible=False,
        )
        tokens_time = gr.Textbox(label="Tokens generated per second")
        with gr.Column():
            download_vmfb = gr.Checkbox(
                label="Download vmfb from Shark tank if available",
                value=True,
                interactive=True,
            )
            prompt_prefix = gr.Checkbox(
                label="Add System Prompt",
                value=False,
                interactive=True,
            )

    chatbot = gr.Chatbot(height=500)
    with gr.Row():
        with gr.Column():
            msg = gr.Textbox(
                label="Chat Message Box",
                placeholder="Chat Message Box",
                show_label=False,
                interactive=enabled,
                container=False,
            )
        with gr.Column():
            with gr.Row():
                submit = gr.Button("Submit", interactive=enabled)
                stop = gr.Button("Stop", interactive=enabled)
                clear = gr.Button("Clear", interactive=enabled)

    with gr.Row(visible=False):
        with gr.Group():
            config_file = gr.File(
                label="Upload sharding configuration", visible=False
            )
            json_view_button = gr.Button(label="View as JSON", visible=False)
        json_view = gr.JSON(interactive=True, visible=False)
        json_view_button.click(
            fn=view_json_file, inputs=[config_file], outputs=[json_view]
        )
    submit_event = msg.submit(
        fn=user,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot],
        show_progress=False,
        queue=False,
    ).then(
        fn=chat_fn,
        inputs=[
            prompt_prefix,
            chatbot,
            model,
            device,
            precision,
            download_vmfb,
            config_file,
        ],
        outputs=[chatbot, tokens_time],
        show_progress=False,
        queue=True,
    )
    submit_click_event = submit.click(
        fn=user,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot],
        show_progress=False,
        queue=False,
    ).then(
        fn=chat_fn,
        inputs=[
            prompt_prefix,
            chatbot,
            model,
            device,
            precision,
            download_vmfb,
            config_file,
        ],
        outputs=[chatbot, tokens_time],
        show_progress=False,
        queue=True,
    )
    stop.click(
        fn=None,
        inputs=None,
        outputs=None,
        cancels=[submit_event, submit_click_event],
        queue=False,
    )
    clear.click(lambda: None, None, [chatbot], queue=False)

```

`apps/stable_diffusion/profiling_with_iree.md`:

```md
Compile / Run Instructions:

To compile .vmfb for SD (vae, unet, CLIP), run the following commands with the .mlir in your local shark_tank cache (default location for Linux users is `~/.local/shark_tank`). These will be available once the script from [this README](https://github.com/nod-ai/SHARK/blob/main/shark/examples/shark_inference/stable_diffusion/README.md) is run once.
Running the script mentioned above with the `--save_vmfb` flag will also save the .vmfb in your SHARK base directory if you want to skip straight to benchmarks.

Compile Commands FP32/FP16: 

```shell
Vulkan AMD: 
iree-compile --iree-input-type=none --iree-hal-target-backends=vulkan --iree-vulkan-target-triple=rdna2-unknown-linux /path/to/input/mlir -o /path/to/output/vmfb

#  add --mlir-print-debuginfo --mlir-print-op-on-diagnostic=true for debug
#  use –iree-input-type=auto or "mhlo_legacy" or "stablehlo" for TF models

CUDA NVIDIA:
iree-compile --iree-input-type=none --iree-hal-target-backends=cuda /path/to/input/mlir -o /path/to/output/vmfb

CPU:
iree-compile --iree-input-type=none --iree-hal-target-backends=llvm-cpu /path/to/input/mlir -o /path/to/output/vmfb
```



Run / Benchmark Command (FP32 - NCHW):
(NEED to use BS=2 since we do two forward passes to unet as a result of classifier free guidance.)

```shell
## Vulkan AMD:
iree-benchmark-module --module=/path/to/output/vmfb --function=forward --device=vulkan --input=1x4x64x64xf32 --input=1xf32 --input=2x77x768xf32 --input=f32=1.0 --input=f32=1.0

## CUDA:
iree-benchmark-module --module=/path/to/vmfb --function=forward --device=cuda  --input=1x4x64x64xf32 --input=1xf32 --input=2x77x768xf32 --input=f32=1.0 --input=f32=1.0

## CPU:
iree-benchmark-module --module=/path/to/vmfb --function=forward --device=local-task  --input=1x4x64x64xf32 --input=1xf32 --input=2x77x768xf32 --input=f32=1.0 --input=f32=1.0

```

Run via vulkan_gui for RGP Profiling:

To build the vulkan app for profiling UNet follow the instructions [here](https://github.com/nod-ai/SHARK/tree/main/cpp) and then run the following command from the cpp directory with your compiled stable_diff.vmfb
```shell
./build/vulkan_gui/iree-vulkan-gui --module=/path/to/unet.vmfb --input=1x4x64x64xf32 --input=1xf32 --input=2x77x768xf32 --input=f32=1.0 --input=f32=1.0
```

</details>
  <details>
  <summary>Debug Commands</summary>

## Debug commands and other advanced usage follows.

```shell
python txt2img.py --precision="fp32"|"fp16" --device="cpu"|"cuda"|"vulkan" --import_mlir|--no-import_mlir --prompt "enter the text" 
```

## dump all dispatch .spv and isa using amdllpc

```shell
python txt2img.py --precision="fp16" --device="vulkan" --iree-vulkan-target-triple=rdna3-unknown-linux --no-load_vmfb --dispatch_benchmarks="all" --dispatch_benchmarks_dir="SD_dispatches" --dump_isa
```

## Compile and save the .vmfb (using vulkan fp16 as an example):

```shell
python txt2img.py --precision=fp16 --device=vulkan --steps=50 --save_vmfb
```

## Capture an RGP trace

```shell
python txt2img.py --precision=fp16 --device=vulkan --steps=50 --save_vmfb --enable_rgp
```

## Run the vae module with iree-benchmark-module (NCHW, fp16, vulkan, for example):

```shell
iree-benchmark-module --module=/path/to/output/vmfb --function=forward --device=vulkan --input=1x4x64x64xf16  
```

## Run the unet module with iree-benchmark-module (same config as above):
```shell
##if you want to use .npz inputs:
unzip ~/.local/shark_tank/<your unet>/inputs.npz
iree-benchmark-module --module=/path/to/output/vmfb --function=forward --input=@arr_0.npy --input=1xf16 --input=@arr_2.npy --input=@arr_3.npy --input=@arr_4.npy  
```

</details>

```

`apps/stable_diffusion/scripts/__init__.py`:

```py
from apps.stable_diffusion.scripts.train_lora_word import lora_train

```

`apps/stable_diffusion/scripts/img2img.py`:

```py
import sys
import torch
import time
from PIL import Image
import transformers
from apps.stable_diffusion.src import (
    args,
    Image2ImagePipeline,
    StencilPipeline,
    resize_stencil,
    get_schedulers,
    set_init_device_flags,
    utils,
    clear_all,
    save_output_img,
)
from apps.stable_diffusion.src.utils import get_generation_text_info


def main():
    if args.clear_all:
        clear_all()

    if args.img_path is None:
        print("Flag --img_path is required.")
        exit()

    image = Image.open(args.img_path).convert("RGB")
    # When the models get uploaded, it should be default to False.
    args.import_mlir = True

    use_stencil = args.use_stencil
    if use_stencil:
        args.scheduler = "DDIM"
        args.hf_model_id = "runwayml/stable-diffusion-v1-5"
        image, args.width, args.height = resize_stencil(image)
    elif "Shark" in args.scheduler:
        print(
            f"Shark schedulers are not supported. Switching to EulerDiscrete scheduler"
        )
        args.scheduler = "EulerDiscrete"
    cpu_scheduling = not args.scheduler.startswith("Shark")
    dtype = torch.float32 if args.precision == "fp32" else torch.half
    set_init_device_flags()
    schedulers = get_schedulers(args.hf_model_id)
    scheduler_obj = schedulers[args.scheduler]
    seed = utils.sanitize_seed(args.seed)
    # Adjust for height and width based on model

    if use_stencil:
        img2img_obj = StencilPipeline.from_pretrained(
            scheduler_obj,
            args.import_mlir,
            args.hf_model_id,
            args.ckpt_loc,
            args.custom_vae,
            args.precision,
            args.max_length,
            args.batch_size,
            args.height,
            args.width,
            args.use_base_vae,
            args.use_tuned,
            low_cpu_mem_usage=args.low_cpu_mem_usage,
            use_stencil=use_stencil,
            debug=args.import_debug if args.import_mlir else False,
            use_lora=args.use_lora,
            ondemand=args.ondemand,
        )
    else:
        img2img_obj = Image2ImagePipeline.from_pretrained(
            scheduler_obj,
            args.import_mlir,
            args.hf_model_id,
            args.ckpt_loc,
            args.custom_vae,
            args.precision,
            args.max_length,
            args.batch_size,
            args.height,
            args.width,
            args.use_base_vae,
            args.use_tuned,
            low_cpu_mem_usage=args.low_cpu_mem_usage,
            debug=args.import_debug if args.import_mlir else False,
            use_lora=args.use_lora,
            ondemand=args.ondemand,
        )

    start_time = time.time()
    generated_imgs = img2img_obj.generate_images(
        args.prompts,
        args.negative_prompts,
        image,
        args.batch_size,
        args.height,
        args.width,
        args.steps,
        args.strength,
        args.guidance_scale,
        seed,
        args.max_length,
        dtype,
        args.use_base_vae,
        cpu_scheduling,
        args.max_embeddings_multiples,
        use_stencil=use_stencil,
        control_mode=args.control_mode,
    )
    total_time = time.time() - start_time
    text_output = f"prompt={args.prompts}"
    text_output += f"\nnegative prompt={args.negative_prompts}"
    text_output += f"\nmodel_id={args.hf_model_id}, ckpt_loc={args.ckpt_loc}"
    text_output += f"\nscheduler={args.scheduler}, device={args.device}"
    text_output += f"\nsteps={args.steps}, strength={args.strength}, guidance_scale={args.guidance_scale}, seed={seed}, size={args.height}x{args.width}"
    text_output += (
        f", batch size={args.batch_size}, max_length={args.max_length}"
    )
    text_output += img2img_obj.log
    text_output += f"\nTotal image generation time: {total_time:.4f}sec"

    extra_info = {"STRENGTH": args.strength}
    save_output_img(generated_imgs[0], seed, extra_info)
    print(text_output)


if __name__ == "__main__":
    main()

```

`apps/stable_diffusion/scripts/inpaint.py`:

```py
import torch
import time
from PIL import Image
import transformers
from apps.stable_diffusion.src import (
    args,
    InpaintPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    clear_all,
    save_output_img,
)
from apps.stable_diffusion.src.utils import get_generation_text_info


def main():
    if args.clear_all:
        clear_all()

    if args.img_path is None:
        print("Flag --img_path is required.")
        exit()
    if args.mask_path is None:
        print("Flag --mask_path is required.")
        exit()

    dtype = torch.float32 if args.precision == "fp32" else torch.half
    cpu_scheduling = not args.scheduler.startswith("Shark")
    set_init_device_flags()
    model_id = (
        args.hf_model_id
        if "inpaint" in args.hf_model_id
        else "stabilityai/stable-diffusion-2-inpainting"
    )
    schedulers = get_schedulers(model_id)
    scheduler_obj = schedulers[args.scheduler]
    seed = args.seed
    image = Image.open(args.img_path)
    mask_image = Image.open(args.mask_path)

    inpaint_obj = InpaintPipeline.from_pretrained(
        scheduler=scheduler_obj,
        import_mlir=args.import_mlir,
        model_id=args.hf_model_id,
        ckpt_loc=args.ckpt_loc,
        custom_vae=args.custom_vae,
        precision=args.precision,
        max_length=args.max_length,
        batch_size=args.batch_size,
        height=args.height,
        width=args.width,
        use_base_vae=args.use_base_vae,
        use_tuned=args.use_tuned,
        low_cpu_mem_usage=args.low_cpu_mem_usage,
        debug=args.import_debug if args.import_mlir else False,
        use_lora=args.use_lora,
        ondemand=args.ondemand,
    )

    seeds = utils.batch_seeds(seed, args.batch_count, args.repeatable_seeds)
    for current_batch in range(args.batch_count):
        start_time = time.time()
        generated_imgs = inpaint_obj.generate_images(
            args.prompts,
            args.negative_prompts,
            image,
            mask_image,
            args.batch_size,
            args.height,
            args.width,
            args.inpaint_full_res,
            args.inpaint_full_res_padding,
            args.steps,
            args.guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )
        total_time = time.time() - start_time
        text_output = f"prompt={args.prompts}"
        text_output += f"\nnegative prompt={args.negative_prompts}"
        text_output += (
            f"\nmodel_id={args.hf_model_id}, ckpt_loc={args.ckpt_loc}"
        )
        text_output += f"\nscheduler={args.scheduler}, device={args.device}"
        text_output += (
            f"\nsteps={args.steps}, guidance_scale={args.guidance_scale},"
        )
        text_output += f"seed={seed}, size={args.height}x{args.width}"
        text_output += (
            f", batch size={args.batch_size}, max_length={args.max_length}"
        )
        text_output += inpaint_obj.log
        text_output += f"\nTotal image generation time: {total_time:.4f}sec"

        save_output_img(generated_imgs[0], seed)
        print(text_output)


if __name__ == "__main__":
    main()

```

`apps/stable_diffusion/scripts/main.py`:

```py
from apps.stable_diffusion.src import args
from apps.stable_diffusion.scripts import (
    img2img,
    txt2img,
    #    inpaint,
    #    outpaint,
)

if __name__ == "__main__":
    if args.app == "txt2img":
        txt2img.main()
    elif args.app == "img2img":
        img2img.main()
    #   elif args.app == "inpaint":
    #       inpaint.main()
    #   elif args.app == "outpaint":
    #       outpaint.main()
    else:
        print(f"args.app value is {args.app} but this isn't supported")

```

`apps/stable_diffusion/scripts/outpaint.py`:

```py
import torch
import time
from PIL import Image
import transformers
from apps.stable_diffusion.src import (
    args,
    OutpaintPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    clear_all,
    save_output_img,
)


def main():
    if args.clear_all:
        clear_all()

    if args.img_path is None:
        print("Flag --img_path is required.")
        exit()

    dtype = torch.float32 if args.precision == "fp32" else torch.half
    cpu_scheduling = not args.scheduler.startswith("Shark")
    set_init_device_flags()
    model_id = (
        args.hf_model_id
        if "inpaint" in args.hf_model_id
        else "stabilityai/stable-diffusion-2-inpainting"
    )
    schedulers = get_schedulers(model_id)
    scheduler_obj = schedulers[args.scheduler]
    seed = args.seed
    image = Image.open(args.img_path)

    outpaint_obj = OutpaintPipeline.from_pretrained(
        scheduler_obj,
        args.import_mlir,
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        args.precision,
        args.max_length,
        args.batch_size,
        args.height,
        args.width,
        args.use_base_vae,
        args.use_tuned,
        use_lora=args.use_lora,
        ondemand=args.ondemand,
    )

    seeds = utils.batch_seeds(seed, args.batch_count, args.repeatable_seeds)
    for current_batch in range(args.batch_count):
        start_time = time.time()
        generated_imgs = outpaint_obj.generate_images(
            args.prompts,
            args.negative_prompts,
            image,
            args.pixels,
            args.mask_blur,
            args.left,
            args.right,
            args.top,
            args.bottom,
            args.noise_q,
            args.color_variation,
            args.batch_size,
            args.height,
            args.width,
            args.steps,
            args.guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )
        total_time = time.time() - start_time
        text_output = f"prompt={args.prompts}"
        text_output += f"\nnegative prompt={args.negative_prompts}"
        text_output += (
            f"\nmodel_id={args.hf_model_id}, ckpt_loc={args.ckpt_loc}"
        )
        text_output += f"\nscheduler={args.scheduler}, device={args.device}"
        text_output += (
            f"\nsteps={args.steps}, guidance_scale={args.guidance_scale},"
        )
        text_output += f"seed={seed}, size={args.height}x{args.width}"
        text_output += (
            f", batch size={args.batch_size}, max_length={args.max_length}"
        )
        text_output += outpaint_obj.log
        text_output += f"\nTotal image generation time: {total_time:.4f}sec"

        # save this information as metadata of output generated image.
        directions = []
        if args.left:
            directions.append("left")
        if args.right:
            directions.append("right")
        if args.top:
            directions.append("up")
        if args.bottom:
            directions.append("down")
        extra_info = {
            "PIXELS": args.pixels,
            "MASK_BLUR": args.mask_blur,
            "DIRECTIONS": directions,
            "NOISE_Q": args.noise_q,
            "COLOR_VARIATION": args.color_variation,
        }
        save_output_img(generated_imgs[0], seed, extra_info)
        print(text_output)


if __name__ == "__main__":
    main()

```

`apps/stable_diffusion/scripts/telegram_bot.py`:

```py
import logging
import os
from models.stable_diffusion.main import stable_diff_inf
from models.stable_diffusion.utils import get_available_devices
from dotenv import load_dotenv
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram import BotCommand
from telegram.ext import Application, ApplicationBuilder, CallbackQueryHandler
from telegram.ext import ContextTypes, MessageHandler, CommandHandler, filters
from io import BytesIO
import random

log = logging.getLogger("TG.Bot")
logging.basicConfig()
log.warning("Start")
load_dotenv()
os.environ["AMD_ENABLE_LLPC"] = "0"
TG_TOKEN = os.getenv("TG_TOKEN")
SELECTED_MODEL = "stablediffusion"
SELECTED_SCHEDULER = "EulerAncestralDiscrete"
STEPS = 30
NEGATIVE_PROMPT = (
    "Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra"
    " limbs,Gross proportions,Missing arms,Mutated hands,Long"
    " neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad"
    " anatomy,Cloned face,Malformed limbs,Missing legs,Too many"
    " fingers,blurry, lowres, text, error, cropped, worst quality, low"
    " quality, jpeg artifacts, out of frame, extra fingers, mutated hands,"
    " poorly drawn hands, poorly drawn face, bad anatomy, extra limbs, cloned"
    " face, malformed limbs, missing arms, missing legs, extra arms, extra"
    " legs, fused fingers, too many fingers"
)
GUIDANCE_SCALE = 6
available_devices = get_available_devices()
models_list = [
    "stablediffusion",
    "anythingv3",
    "analogdiffusion",
    "openjourney",
    "dreamlike",
]
sheds_list = [
    "DDIM",
    "PNDM",
    "LMSDiscrete",
    "DPMSolverMultistep",
    "EulerDiscrete",
    "EulerAncestralDiscrete",
    "SharkEulerDiscrete",
]


def image_to_bytes(image):
    bio = BytesIO()
    bio.name = "image.jpeg"
    image.save(bio, "JPEG")
    bio.seek(0)
    return bio


def get_try_again_markup():
    keyboard = [[InlineKeyboardButton("Try again", callback_data="TRYAGAIN")]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    return reply_markup


def generate_image(prompt):
    seed = random.randint(1, 10000)
    log.warning(SELECTED_MODEL)
    log.warning(STEPS)
    image, text = stable_diff_inf(
        prompt=prompt,
        negative_prompt=NEGATIVE_PROMPT,
        steps=STEPS,
        guidance_scale=GUIDANCE_SCALE,
        seed=seed,
        scheduler_key=SELECTED_SCHEDULER,
        variant=SELECTED_MODEL,
        device_key=available_devices[0],
    )

    return image, seed


async def generate_and_send_photo(
    update: Update, context: ContextTypes.DEFAULT_TYPE
) -> None:
    progress_msg = await update.message.reply_text(
        "Generating image...", reply_to_message_id=update.message.message_id
    )
    im, seed = generate_image(prompt=update.message.text)
    await context.bot.delete_message(
        chat_id=progress_msg.chat_id, message_id=progress_msg.message_id
    )
    await context.bot.send_photo(
        update.effective_user.id,
        image_to_bytes(im),
        caption=f'"{update.message.text}" (Seed: {seed})',
        reply_markup=get_try_again_markup(),
        reply_to_message_id=update.message.message_id,
    )


async def button(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    query = update.callback_query
    if query.data in models_list:
        global SELECTED_MODEL
        SELECTED_MODEL = query.data
        await query.answer()
        await query.edit_message_text(text=f"Selected model: {query.data}")
        return
    if query.data in sheds_list:
        global SELECTED_SCHEDULER
        SELECTED_SCHEDULER = query.data
        await query.answer()
        await query.edit_message_text(text=f"Selected scheduler: {query.data}")
        return
    replied_message = query.message.reply_to_message
    await query.answer()
    progress_msg = await query.message.reply_text(
        "Generating image...", reply_to_message_id=replied_message.message_id
    )

    if query.data == "TRYAGAIN":
        prompt = replied_message.text
        im, seed = generate_image(prompt)

    await context.bot.delete_message(
        chat_id=progress_msg.chat_id, message_id=progress_msg.message_id
    )
    await context.bot.send_photo(
        update.effective_user.id,
        image_to_bytes(im),
        caption=f'"{prompt}" (Seed: {seed})',
        reply_markup=get_try_again_markup(),
        reply_to_message_id=replied_message.message_id,
    )


async def select_model_handler(update, context):
    text = "Select model"
    keyboard = []
    for model in models_list:
        keyboard.append(
            [
                InlineKeyboardButton(text=model, callback_data=model),
            ]
        )
    markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text(text=text, reply_markup=markup)


async def select_scheduler_handler(update, context):
    text = "Select schedule"
    keyboard = []
    for shed in sheds_list:
        keyboard.append(
            [
                InlineKeyboardButton(text=shed, callback_data=shed),
            ]
        )
    markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text(text=text, reply_markup=markup)


async def set_steps_handler(update, context):
    input_mex = update.message.text
    log.warning(input_mex)
    try:
        input_args = input_mex.split("/set_steps ")[1]
        global STEPS
        STEPS = int(input_args)
    except Exception:
        input_args = (
            "Invalid parameter for command. Correct command looks like\n"
            " /set_steps 30"
        )
    await update.message.reply_text(input_args)


async def set_negative_prompt_handler(update, context):
    input_mex = update.message.text
    log.warning(input_mex)
    try:
        input_args = input_mex.split("/set_negative_prompt ")[1]
        global NEGATIVE_PROMPT
        NEGATIVE_PROMPT = input_args
    except Exception:
        input_args = (
            "Invalid parameter for command. Correct command looks like\n"
            " /set_negative_prompt ugly, bad art, mutated"
        )
    await update.message.reply_text(input_args)


async def set_guidance_scale_handler(update, context):
    input_mex = update.message.text
    log.warning(input_mex)
    try:
        input_args = input_mex.split("/set_guidance_scale ")[1]
        global GUIDANCE_SCALE
        GUIDANCE_SCALE = int(input_args)
    except Exception:
        input_args = (
            "Invalid parameter for command. Correct command looks like\n"
            " /set_guidance_scale 7"
        )
    await update.message.reply_text(input_args)


async def setup_bot_commands(application: Application) -> None:
    await application.bot.set_my_commands(
        [
            BotCommand("select_model", "to select model"),
            BotCommand("select_scheduler", "to select scheduler"),
            BotCommand("set_steps", "to set steps"),
            BotCommand("set_guidance_scale", "to set guidance scale"),
            BotCommand("set_negative_prompt", "to set negative prompt"),
        ]
    )


app = (
    ApplicationBuilder().token(TG_TOKEN).post_init(setup_bot_commands).build()
)
app.add_handler(CommandHandler("select_model", select_model_handler))
app.add_handler(CommandHandler("select_scheduler", select_scheduler_handler))
app.add_handler(CommandHandler("set_steps", set_steps_handler))
app.add_handler(
    CommandHandler("set_guidance_scale", set_guidance_scale_handler)
)
app.add_handler(
    CommandHandler("set_negative_prompt", set_negative_prompt_handler)
)
app.add_handler(
    MessageHandler(filters.TEXT & ~filters.COMMAND, generate_and_send_photo)
)
app.add_handler(CallbackQueryHandler(button))
log.warning("Start bot")
app.run_polling()

```

`apps/stable_diffusion/scripts/train_lora_word.py`:

```py
# Install the required libs
# pip install -U git+https://github.com/huggingface/diffusers.git
# pip install accelerate transformers ftfy

# HuggingFace Token
# YOUR_TOKEN = "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"


# Import required libraries
import itertools
import math
import os
from typing import List
import random
import torch_mlir

import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import Dataset

import PIL
import logging

from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    PNDMScheduler,
    StableDiffusionPipeline,
    UNet2DConditionModel,
)
from PIL import Image
from tqdm.auto import tqdm
from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer
from diffusers.loaders import AttnProcsLayers
from diffusers.models.attention_processor import LoRAXFormersAttnProcessor

import torch_mlir
from torch_mlir.dynamo import make_simple_dynamo_backend
import torch._dynamo as dynamo
from torch.fx.experimental.proxy_tensor import make_fx
from torch_mlir_e2e_test.linalg_on_tensors_backends import refbackend
from shark.shark_inference import SharkInference

torch._dynamo.config.verbose = True

from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    PNDMScheduler,
    StableDiffusionPipeline,
    UNet2DConditionModel,
)
from diffusers.optimization import get_scheduler
from diffusers.pipelines.stable_diffusion import (
    StableDiffusionSafetyChecker,
)
from PIL import Image
from tqdm.auto import tqdm
from transformers import (
    CLIPFeatureExtractor,
    CLIPTextModel,
    CLIPTokenizer,
)

from io import BytesIO

from dataclasses import dataclass
from apps.stable_diffusion.src import (
    args,
    get_schedulers,
    set_init_device_flags,
    clear_all,
)
from apps.stable_diffusion.src.utils import update_lora_weight


# Setup the dataset
class LoraDataset(Dataset):
    def __init__(
        self,
        data_root,
        tokenizer,
        size=512,
        repeats=100,
        interpolation="bicubic",
        set="train",
        prompt="myloraprompt",
        center_crop=False,
    ):
        self.data_root = data_root
        self.tokenizer = tokenizer
        self.size = size
        self.center_crop = center_crop
        self.prompt = prompt

        self.image_paths = [
            os.path.join(self.data_root, file_path)
            for file_path in os.listdir(self.data_root)
        ]

        self.num_images = len(self.image_paths)
        self._length = self.num_images

        if set == "train":
            self._length = self.num_images * repeats

        self.interpolation = {
            "linear": PIL.Image.LINEAR,
            "bilinear": PIL.Image.BILINEAR,
            "bicubic": PIL.Image.BICUBIC,
            "lanczos": PIL.Image.LANCZOS,
        }[interpolation]

    def __len__(self):
        return self._length

    def __getitem__(self, i):
        example = {}
        image = Image.open(self.image_paths[i % self.num_images])

        if not image.mode == "RGB":
            image = image.convert("RGB")

        example["input_ids"] = self.tokenizer(
            self.prompt,
            padding="max_length",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt",
        ).input_ids[0]

        # default to score-sde preprocessing
        img = np.array(image).astype(np.uint8)

        if self.center_crop:
            crop = min(img.shape[0], img.shape[1])
            (
                h,
                w,
            ) = (
                img.shape[0],
                img.shape[1],
            )
            img = img[
                (h - crop) // 2 : (h + crop) // 2,
                (w - crop) // 2 : (w + crop) // 2,
            ]

        image = Image.fromarray(img)
        image = image.resize(
            (self.size, self.size), resample=self.interpolation
        )

        image = np.array(image).astype(np.uint8)
        image = (image / 127.5 - 1.0).astype(np.float32)

        example["pixel_values"] = torch.from_numpy(image).permute(2, 0, 1)
        return example


def torch_device(device):
    device_tokens = device.split("=>")
    if len(device_tokens) == 1:
        device_str = device_tokens[0].strip()
    else:
        device_str = device_tokens[1].strip()
    device_type_tokens = device_str.split("://")
    if device_type_tokens[0] == "metal":
        device_type_tokens[0] = "vulkan"
    if len(device_type_tokens) > 1:
        return device_type_tokens[0] + ":" + device_type_tokens[1]
    else:
        return device_type_tokens[0]


########## Setting up the model ##########
def lora_train(
    prompt: str,
    height: int,
    width: int,
    steps: int,
    guidance_scale: float,
    seed: int,
    batch_count: int,
    batch_size: int,
    scheduler: str,
    custom_model: str,
    hf_model_id: str,
    precision: str,
    device: str,
    max_length: int,
    training_images_dir: str,
    lora_save_dir: str,
    use_lora: str,
):
    from apps.stable_diffusion.web.ui.utils import (
        get_custom_model_pathfile,
        Config,
    )
    import apps.stable_diffusion.web.utils.global_obj as global_obj

    print(
        "Note LoRA training is not compatible with the latest torch-mlir branch"
    )
    print(
        "To run LoRA training you'll need this to follow this guide for the torch-mlir branch: https://github.com/nod-ai/SHARK/tree/main/shark/examples/shark_training/stable_diffusion"
    )
    torch.manual_seed(seed)

    args.prompts = [prompt]
    args.steps = steps

    # set ckpt_loc and hf_model_id.
    types = (
        ".ckpt",
        ".safetensors",
    )  # the tuple of file types
    args.ckpt_loc = ""
    args.hf_model_id = ""
    if custom_model == "None":
        if not hf_model_id:
            return (
                None,
                "Please provide either custom model or huggingface model ID, both must not be "
                "empty.",
            )
        args.hf_model_id = hf_model_id
    elif ".ckpt" in custom_model or ".safetensors" in custom_model:
        args.ckpt_loc = custom_model
    else:
        args.hf_model_id = custom_model

    args.training_images_dir = training_images_dir
    args.lora_save_dir = lora_save_dir

    args.precision = precision
    args.batch_size = batch_size
    args.max_length = max_length
    args.height = height
    args.width = width
    args.device = torch_device(device)
    args.use_lora = use_lora

    # Load the Stable Diffusion model
    text_encoder = CLIPTextModel.from_pretrained(
        args.hf_model_id, subfolder="text_encoder"
    )
    vae = AutoencoderKL.from_pretrained(args.hf_model_id, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(
        args.hf_model_id, subfolder="unet"
    )

    def freeze_params(params):
        for param in params:
            param.requires_grad = False

    # Freeze everything but LoRA
    freeze_params(vae.parameters())
    freeze_params(unet.parameters())
    freeze_params(text_encoder.parameters())

    # Move vae and unet to device
    vae.to(args.device)
    unet.to(args.device)
    text_encoder.to(args.device)

    if use_lora != "":
        update_lora_weight(unet, args.use_lora, "unet")
    else:
        lora_attn_procs = {}
        for name in unet.attn_processors.keys():
            cross_attention_dim = (
                None
                if name.endswith("attn1.processor")
                else unet.config.cross_attention_dim
            )
            if name.startswith("mid_block"):
                hidden_size = unet.config.block_out_channels[-1]
            elif name.startswith("up_blocks"):
                block_id = int(name[len("up_blocks.")])
                hidden_size = list(reversed(unet.config.block_out_channels))[
                    block_id
                ]
            elif name.startswith("down_blocks"):
                block_id = int(name[len("down_blocks.")])
                hidden_size = unet.config.block_out_channels[block_id]

            lora_attn_procs[name] = LoRAXFormersAttnProcessor(
                hidden_size=hidden_size,
                cross_attention_dim=cross_attention_dim,
            )

        unet.set_attn_processor(lora_attn_procs)
    lora_layers = AttnProcsLayers(unet.attn_processors)

    class VaeModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.vae = vae

        def forward(self, input):
            x = self.vae.encode(input, return_dict=False)[0]
            return x

    class UnetModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.unet = unet

        def forward(self, x, y, z):
            return self.unet.forward(x, y, z, return_dict=False)[0]

    shark_vae = VaeModel()
    shark_unet = UnetModel()

    ####### Creating our training data ########

    tokenizer = CLIPTokenizer.from_pretrained(
        args.hf_model_id,
        subfolder="tokenizer",
    )

    # Let's create the Dataset and Dataloader
    train_dataset = LoraDataset(
        data_root=args.training_images_dir,
        tokenizer=tokenizer,
        size=vae.sample_size,
        prompt=args.prompts[0],
        repeats=100,
        center_crop=False,
        set="train",
    )

    def create_dataloader(train_batch_size=1):
        return torch.utils.data.DataLoader(
            train_dataset, batch_size=train_batch_size, shuffle=True
        )

    # Create noise_scheduler for training
    noise_scheduler = DDPMScheduler.from_config(
        args.hf_model_id, subfolder="scheduler"
    )

    ######## Training ###########

    # Define hyperparameters for our training. If you are not happy with your results,
    # you can tune the `learning_rate` and the `max_train_steps`

    # Setting up all training args
    hyperparameters = {
        "learning_rate": 5e-04,
        "scale_lr": True,
        "max_train_steps": steps,
        "train_batch_size": batch_size,
        "gradient_accumulation_steps": 1,
        "gradient_checkpointing": True,
        "mixed_precision": "fp16",
        "seed": 42,
        "output_dir": "sd-concept-output",
    }
    # creating output directory
    cwd = os.getcwd()
    out_dir = os.path.join(cwd, hyperparameters["output_dir"])
    while not os.path.exists(str(out_dir)):
        try:
            os.mkdir(out_dir)
        except OSError as error:
            print("Output directory not created")

    ###### Torch-MLIR Compilation ######

    def _remove_nones(fx_g: torch.fx.GraphModule) -> List[int]:
        removed_indexes = []
        for node in fx_g.graph.nodes:
            if node.op == "output":
                assert (
                    len(node.args) == 1
                ), "Output node must have a single argument"
                node_arg = node.args[0]
                if isinstance(node_arg, (list, tuple)):
                    node_arg = list(node_arg)
                    node_args_len = len(node_arg)
                    for i in range(node_args_len):
                        curr_index = node_args_len - (i + 1)
                        if node_arg[curr_index] is None:
                            removed_indexes.append(curr_index)
                            node_arg.pop(curr_index)
                    node.args = (tuple(node_arg),)
                    break

        if len(removed_indexes) > 0:
            fx_g.graph.lint()
            fx_g.graph.eliminate_dead_code()
            fx_g.recompile()
        removed_indexes.sort()
        return removed_indexes

    def _unwrap_single_tuple_return(fx_g: torch.fx.GraphModule) -> bool:
        """
        Replace tuple with tuple element in functions that return one-element tuples.
        Returns true if an unwrapping took place, and false otherwise.
        """
        unwrapped_tuple = False
        for node in fx_g.graph.nodes:
            if node.op == "output":
                assert (
                    len(node.args) == 1
                ), "Output node must have a single argument"
                node_arg = node.args[0]
                if isinstance(node_arg, tuple):
                    if len(node_arg) == 1:
                        node.args = (node_arg[0],)
                        unwrapped_tuple = True
                        break

        if unwrapped_tuple:
            fx_g.graph.lint()
            fx_g.recompile()
        return unwrapped_tuple

    def _returns_nothing(fx_g: torch.fx.GraphModule) -> bool:
        for node in fx_g.graph.nodes:
            if node.op == "output":
                assert (
                    len(node.args) == 1
                ), "Output node must have a single argument"
                node_arg = node.args[0]
                if isinstance(node_arg, tuple):
                    return len(node_arg) == 0
        return False

    def transform_fx(fx_g):
        for node in fx_g.graph.nodes:
            if node.op == "call_function":
                if node.target in [
                    torch.ops.aten.empty,
                ]:
                    # aten.empty should be filled with zeros.
                    if node.target in [torch.ops.aten.empty]:
                        with fx_g.graph.inserting_after(node):
                            new_node = fx_g.graph.call_function(
                                torch.ops.aten.zero_,
                                args=(node,),
                            )
                            node.append(new_node)
                            node.replace_all_uses_with(new_node)
                            new_node.args = (node,)

        fx_g.graph.lint()

    @make_simple_dynamo_backend
    def refbackend_torchdynamo_backend(
        fx_graph: torch.fx.GraphModule, example_inputs: List[torch.Tensor]
    ):
        # handling usage of empty tensor without initializing
        transform_fx(fx_graph)
        fx_graph.recompile()
        if _returns_nothing(fx_graph):
            return fx_graph
        removed_none_indexes = _remove_nones(fx_graph)
        was_unwrapped = _unwrap_single_tuple_return(fx_graph)

        mlir_module = torch_mlir.compile(
            fx_graph, example_inputs, output_type="linalg-on-tensors"
        )

        bytecode_stream = BytesIO()
        mlir_module.operation.write_bytecode(bytecode_stream)
        bytecode = bytecode_stream.getvalue()

        shark_module = SharkInference(
            mlir_module=bytecode, device=args.device, mlir_dialect="tm_tensor"
        )
        shark_module.compile()

        def compiled_callable(*inputs):
            inputs = [x.numpy() for x in inputs]
            result = shark_module("forward", inputs)
            if was_unwrapped:
                result = [
                    result,
                ]
            if not isinstance(result, list):
                result = torch.from_numpy(result)
            else:
                result = tuple(torch.from_numpy(x) for x in result)
                result = list(result)
                for removed_index in removed_none_indexes:
                    result.insert(removed_index, None)
                result = tuple(result)
            return result

        return compiled_callable

    def predictions(torch_func, jit_func, batchA, batchB):
        res = jit_func(batchA.numpy(), batchB.numpy())
        if res is not None:
            # prediction = torch.from_numpy(res)
            prediction = res
        else:
            prediction = None
        return prediction

    logger = logging.getLogger(__name__)

    train_batch_size = hyperparameters["train_batch_size"]
    gradient_accumulation_steps = hyperparameters[
        "gradient_accumulation_steps"
    ]
    learning_rate = hyperparameters["learning_rate"]
    if hyperparameters["scale_lr"]:
        learning_rate = (
            learning_rate
            * gradient_accumulation_steps
            * train_batch_size
            # * accelerator.num_processes
        )

    # Initialize the optimizer
    optimizer = torch.optim.AdamW(
        lora_layers.parameters(),  # only optimize the embeddings
        lr=learning_rate,
    )

    # Training function
    def train_func(batch_pixel_values, batch_input_ids):
        # Convert images to latent space
        latents = shark_vae(batch_pixel_values).sample().detach()
        latents = latents * 0.18215

        # Sample noise that we'll add to the latents
        noise = torch.randn_like(latents)
        bsz = latents.shape[0]
        # Sample a random timestep for each image
        timesteps = torch.randint(
            0,
            noise_scheduler.num_train_timesteps,
            (bsz,),
            device=latents.device,
        ).long()

        # Add noise to the latents according to the noise magnitude at each timestep
        # (this is the forward diffusion process)
        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

        # Get the text embedding for conditioning
        encoder_hidden_states = text_encoder(batch_input_ids)[0]

        # Predict the noise residual
        noise_pred = shark_unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states,
        )

        # Get the target for loss depending on the prediction type
        if noise_scheduler.config.prediction_type == "epsilon":
            target = noise
        elif noise_scheduler.config.prediction_type == "v_prediction":
            target = noise_scheduler.get_velocity(latents, noise, timesteps)
        else:
            raise ValueError(
                f"Unknown prediction type {noise_scheduler.config.prediction_type}"
            )

        loss = (
            F.mse_loss(noise_pred, target, reduction="none")
            .mean([1, 2, 3])
            .mean()
        )
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()

        return loss

    def training_function():
        max_train_steps = hyperparameters["max_train_steps"]
        output_dir = hyperparameters["output_dir"]
        gradient_checkpointing = hyperparameters["gradient_checkpointing"]

        train_dataloader = create_dataloader(train_batch_size)

        # We need to recalculate our total training steps as the size of the training dataloader may have changed.
        num_update_steps_per_epoch = math.ceil(
            len(train_dataloader) / gradient_accumulation_steps
        )
        num_train_epochs = math.ceil(
            max_train_steps / num_update_steps_per_epoch
        )

        # Train!
        total_batch_size = (
            train_batch_size
            * gradient_accumulation_steps
            # train_batch_size * accelerator.num_processes * gradient_accumulation_steps
        )

        logger.info("***** Running training *****")
        logger.info(f"  Num examples = {len(train_dataset)}")
        logger.info(
            f"  Instantaneous batch size per device = {train_batch_size}"
        )
        logger.info(
            f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}"
        )
        logger.info(
            f"  Gradient Accumulation steps = {gradient_accumulation_steps}"
        )
        logger.info(f"  Total optimization steps = {max_train_steps}")
        # Only show the progress bar once on each machine.
        progress_bar = tqdm(
            # range(max_train_steps), disable=not accelerator.is_local_main_process
            range(max_train_steps)
        )
        progress_bar.set_description("Steps")
        global_step = 0

        params__ = [
            i for i in text_encoder.get_input_embeddings().parameters()
        ]

        for epoch in range(num_train_epochs):
            unet.train()
            for step, batch in enumerate(train_dataloader):
                dynamo_callable = dynamo.optimize(
                    refbackend_torchdynamo_backend
                )(train_func)
                lam_func = lambda x, y: dynamo_callable(
                    torch.from_numpy(x), torch.from_numpy(y)
                )
                loss = predictions(
                    train_func,
                    lam_func,
                    batch["pixel_values"],
                    batch["input_ids"],
                )

                # Checks if the accelerator has performed an optimization step behind the scenes
                progress_bar.update(1)
                global_step += 1

                logs = {"loss": loss.detach().item()}
                progress_bar.set_postfix(**logs)

                if global_step >= max_train_steps:
                    break

    training_function()

    # Save the lora weights
    unet.save_attn_procs(args.lora_save_dir)

    for param in itertools.chain(unet.parameters(), text_encoder.parameters()):
        if param.grad is not None:
            del param.grad  # free some memory
        torch.cuda.empty_cache()


if __name__ == "__main__":
    if args.clear_all:
        clear_all()

    dtype = torch.float32 if args.precision == "fp32" else torch.half
    cpu_scheduling = not args.scheduler.startswith("Shark")
    set_init_device_flags()
    schedulers = get_schedulers(args.hf_model_id)
    scheduler_obj = schedulers[args.scheduler]
    seed = args.seed
    if len(args.prompts) != 1:
        print("Need exactly one prompt for the LoRA word")
    lora_train(
        args.prompts[0],
        args.height,
        args.width,
        args.training_steps,
        args.guidance_scale,
        args.seed,
        args.batch_count,
        args.batch_size,
        args.scheduler,
        "None",
        args.hf_model_id,
        args.precision,
        args.device,
        args.max_length,
        args.training_images_dir,
        args.lora_save_dir,
        args.use_lora,
    )

```

`apps/stable_diffusion/scripts/tuner.py`:

```py
import os
from pathlib import Path
from shark_tuner.codegen_tuner import SharkCodegenTuner
from shark_tuner.iree_utils import (
    dump_dispatches,
    create_context,
    export_module_to_mlir_file,
)
from shark_tuner.model_annotation import model_annotation
from apps.stable_diffusion.src.utils.stable_args import args
from apps.stable_diffusion.src.utils.utils import set_init_device_flags
from apps.stable_diffusion.src.utils.sd_annotation import (
    get_device_args,
    load_winograd_configs,
)
from apps.stable_diffusion.src.models import SharkifyStableDiffusionModel


def load_mlir_module():
    if "upscaler" in args.hf_model_id:
        is_upscaler = True
    else:
        is_upscaler = False
    sd_model = SharkifyStableDiffusionModel(
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        args.precision,
        max_len=args.max_length,
        batch_size=args.batch_size,
        height=args.height,
        width=args.width,
        use_base_vae=args.use_base_vae,
        is_upscaler=is_upscaler,
        use_tuned=False,
        low_cpu_mem_usage=args.low_cpu_mem_usage,
        return_mlir=True,
    )

    if args.annotation_model == "unet":
        mlir_module = sd_model.unet()
        model_name = sd_model.model_name["unet"]
    elif args.annotation_model == "vae":
        mlir_module = sd_model.vae()
        model_name = sd_model.model_name["vae"]
    else:
        raise ValueError(
            f"{args.annotation_model} is not supported for tuning."
        )

    return mlir_module, model_name


def main():
    args.use_tuned = False
    set_init_device_flags()
    mlir_module, model_name = load_mlir_module()

    # Get device and device specific arguments
    device, device_spec_args = get_device_args()
    device_spec = ""
    vulkan_target_triple = ""
    if device_spec_args:
        device_spec = device_spec_args[-1].split("=")[-1].strip()
        if device == "vulkan":
            vulkan_target_triple = device_spec
            device_spec = device_spec.split("-")[0]

    # Add winograd annotation for vulkan device
    use_winograd = (
        True
        if device == "vulkan" and args.annotation_model in ["unet", "vae"]
        else False
    )
    winograd_config = (
        load_winograd_configs()
        if device == "vulkan" and args.annotation_model in ["unet", "vae"]
        else ""
    )
    with create_context() as ctx:
        input_module = model_annotation(
            ctx,
            input_contents=mlir_module,
            config_path=winograd_config,
            search_op="conv",
            winograd=use_winograd,
        )

    # Dump model dispatches
    generates_dir = Path.home() / "tmp"
    if not os.path.exists(generates_dir):
        os.makedirs(generates_dir)
    dump_mlir = generates_dir / "temp.mlir"
    dispatch_dir = generates_dir / f"{model_name}_{device_spec}_dispatches"
    export_module_to_mlir_file(input_module, dump_mlir)
    dump_dispatches(
        dump_mlir,
        device,
        dispatch_dir,
        vulkan_target_triple,
        use_winograd=use_winograd,
    )

    # Tune each dispatch
    dtype = "f16" if args.precision == "fp16" else "f32"
    config_filename = f"{model_name}_{device_spec}_configs.json"

    for f_path in os.listdir(dispatch_dir):
        if not f_path.endswith(".mlir"):
            continue

        model_dir = os.path.join(dispatch_dir, f_path)

        tuner = SharkCodegenTuner(
            model_dir,
            device,
            "random",
            args.num_iters,
            args.tuned_config_dir,
            dtype,
            args.search_op,
            batch_size=1,
            config_filename=config_filename,
            use_dispatch=True,
            vulkan_target_triple=vulkan_target_triple,
        )
        tuner.tune()


if __name__ == "__main__":
    main()

```

`apps/stable_diffusion/scripts/txt2img.py`:

```py
import torch
import transformers
import time
from apps.stable_diffusion.src import (
    args,
    Text2ImagePipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    clear_all,
    save_output_img,
)


def main():
    if args.clear_all:
        clear_all()

    dtype = torch.float32 if args.precision == "fp32" else torch.half
    cpu_scheduling = not args.scheduler.startswith("Shark")
    set_init_device_flags()
    schedulers = get_schedulers(args.hf_model_id)
    scheduler_obj = schedulers[args.scheduler]
    seed = args.seed
    txt2img_obj = Text2ImagePipeline.from_pretrained(
        scheduler=scheduler_obj,
        import_mlir=args.import_mlir,
        model_id=args.hf_model_id,
        ckpt_loc=args.ckpt_loc,
        precision=args.precision,
        max_length=args.max_length,
        batch_size=args.batch_size,
        height=args.height,
        width=args.width,
        use_base_vae=args.use_base_vae,
        use_tuned=args.use_tuned,
        custom_vae=args.custom_vae,
        low_cpu_mem_usage=args.low_cpu_mem_usage,
        debug=args.import_debug if args.import_mlir else False,
        use_lora=args.use_lora,
        use_quantize=args.use_quantize,
        ondemand=args.ondemand,
    )

    seeds = utils.batch_seeds(seed, args.batch_count, args.repeatable_seeds)
    for current_batch in range(args.batch_count):
        start_time = time.time()
        generated_imgs = txt2img_obj.generate_images(
            args.prompts,
            args.negative_prompts,
            args.batch_size,
            args.height,
            args.width,
            args.steps,
            args.guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )
        total_time = time.time() - start_time
        text_output = f"prompt={args.prompts}"
        text_output += f"\nnegative prompt={args.negative_prompts}"
        text_output += (
            f"\nmodel_id={args.hf_model_id}, ckpt_loc={args.ckpt_loc}"
        )
        text_output += f"\nscheduler={args.scheduler}, device={args.device}"
        text_output += (
            f"\nsteps={args.steps}, guidance_scale={args.guidance_scale},"
        )
        text_output += (
            f"seed={seeds[current_batch]}, size={args.height}x{args.width}"
        )
        text_output += (
            f", batch size={args.batch_size}, max_length={args.max_length}"
        )
        # TODO: if using --batch_count=x txt2img_obj.log will output on each display every iteration infos from the start
        text_output += txt2img_obj.log
        text_output += f"\nTotal image generation time: {total_time:.4f}sec"

        save_output_img(generated_imgs[0], seed)
        print(text_output)


if __name__ == "__main__":
    main()

```

`apps/stable_diffusion/scripts/txt2img_sdxl.py`:

```py
import torch
import time
from apps.stable_diffusion.src import (
    args,
    Text2ImageSDXLPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    clear_all,
    save_output_img,
)


def main():
    if args.clear_all:
        clear_all()

    # TODO: prompt_embeds and text_embeds form base_model.json requires fixing
    args.precision = "fp16"
    args.height = 1024
    args.width = 1024
    args.max_length = 77
    args.scheduler = "DDIM"
    print(
        "Using default supported configuration for SDXL :-\nprecision=fp16, width*height= 1024*1024, max_length=77 and scheduler=DDIM"
    )
    dtype = torch.float32 if args.precision == "fp32" else torch.half
    cpu_scheduling = not args.scheduler.startswith("Shark")
    set_init_device_flags()
    schedulers = get_schedulers(args.hf_model_id)
    scheduler_obj = schedulers[args.scheduler]
    seed = args.seed
    txt2img_obj = Text2ImageSDXLPipeline.from_pretrained(
        scheduler=scheduler_obj,
        import_mlir=args.import_mlir,
        model_id=args.hf_model_id,
        ckpt_loc=args.ckpt_loc,
        precision=args.precision,
        max_length=args.max_length,
        batch_size=args.batch_size,
        height=args.height,
        width=args.width,
        use_base_vae=args.use_base_vae,
        use_tuned=args.use_tuned,
        custom_vae=args.custom_vae,
        low_cpu_mem_usage=args.low_cpu_mem_usage,
        debug=args.import_debug if args.import_mlir else False,
        use_lora=args.use_lora,
        use_quantize=args.use_quantize,
        ondemand=args.ondemand,
    )

    seeds = utils.batch_seeds(seed, args.batch_count, args.repeatable_seeds)
    for current_batch in range(args.batch_count):
        start_time = time.time()
        generated_imgs = txt2img_obj.generate_images(
            args.prompts,
            args.negative_prompts,
            args.batch_size,
            args.height,
            args.width,
            args.steps,
            args.guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )
        total_time = time.time() - start_time
        text_output = f"prompt={args.prompts}"
        text_output += f"\nnegative prompt={args.negative_prompts}"
        text_output += (
            f"\nmodel_id={args.hf_model_id}, ckpt_loc={args.ckpt_loc}"
        )
        text_output += f"\nscheduler={args.scheduler}, device={args.device}"
        text_output += (
            f"\nsteps={args.steps}, guidance_scale={args.guidance_scale},"
        )
        text_output += (
            f"seed={seeds[current_batch]}, size={args.height}x{args.width}"
        )
        text_output += (
            f", batch size={args.batch_size}, max_length={args.max_length}"
        )
        # TODO: if using --batch_count=x txt2img_obj.log will output on each display every iteration infos from the start
        text_output += txt2img_obj.log
        text_output += f"\nTotal image generation time: {total_time:.4f}sec"

        save_output_img(generated_imgs[0], seed)
        print(text_output)


if __name__ == "__main__":
    main()

```

`apps/stable_diffusion/scripts/upscaler.py`:

```py
import torch
import time
from PIL import Image
import transformers
from apps.stable_diffusion.src import (
    args,
    UpscalerPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    clear_all,
    save_output_img,
)


if __name__ == "__main__":
    if args.clear_all:
        clear_all()

    if args.img_path is None:
        print("Flag --img_path is required.")
        exit()

    # When the models get uploaded, it should be defaulted to False.
    args.import_mlir = True

    cpu_scheduling = not args.scheduler.startswith("Shark")
    dtype = torch.float32 if args.precision == "fp32" else torch.half
    set_init_device_flags()
    schedulers = get_schedulers(args.hf_model_id)

    scheduler_obj = schedulers[args.scheduler]
    image = (
        Image.open(args.img_path)
        .convert("RGB")
        .resize((args.height, args.width))
    )
    seed = utils.sanitize_seed(args.seed)
    # Adjust for height and width based on model

    upscaler_obj = UpscalerPipeline.from_pretrained(
        scheduler_obj,
        args.import_mlir,
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        args.precision,
        args.max_length,
        args.batch_size,
        args.height,
        args.width,
        args.use_base_vae,
        args.use_tuned,
        low_cpu_mem_usage=args.low_cpu_mem_usage,
        use_lora=args.use_lora,
        ddpm_scheduler=schedulers["DDPM"],
        ondemand=args.ondemand,
    )

    start_time = time.time()
    generated_imgs = upscaler_obj.generate_images(
        args.prompts,
        args.negative_prompts,
        image,
        args.batch_size,
        args.height,
        args.width,
        args.steps,
        args.noise_level,
        args.guidance_scale,
        seed,
        args.max_length,
        dtype,
        args.use_base_vae,
        cpu_scheduling,
        args.max_embeddings_multiples,
    )
    total_time = time.time() - start_time
    text_output = f"prompt={args.prompts}"
    text_output += f"\nnegative prompt={args.negative_prompts}"
    text_output += f"\nmodel_id={args.hf_model_id}, ckpt_loc={args.ckpt_loc}"
    text_output += f"\nscheduler={args.scheduler}, device={args.device}"
    text_output += f"\nsteps={args.steps}, noise_level={args.noise_level}, guidance_scale={args.guidance_scale}, seed={seed}, size={args.height}x{args.width}"
    text_output += (
        f", batch size={args.batch_size}, max_length={args.max_length}"
    )
    text_output += upscaler_obj.log
    text_output += f"\nTotal image generation time: {total_time:.4f}sec"

    extra_info = {"NOISE LEVEL": args.noise_level}
    save_output_img(generated_imgs[0], seed, extra_info)
    print(text_output)

```

`apps/stable_diffusion/shark_sd.spec`:

```spec
# -*- mode: python ; coding: utf-8 -*-
from apps.stable_diffusion.shark_studio_imports import pathex, datas, hiddenimports

binaries = []

block_cipher = None

a = Analysis(
    ['web/index.py'],
    pathex=pathex,
    binaries=binaries,
    datas=datas,
    hiddenimports=hiddenimports,
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)
pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='nodai_shark_studio',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=False,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

```

`apps/stable_diffusion/shark_sd_cli.spec`:

```spec
# -*- mode: python ; coding: utf-8 -*-
from PyInstaller.utils.hooks import collect_data_files
from PyInstaller.utils.hooks import collect_submodules
from PyInstaller.utils.hooks import copy_metadata

import sys ; sys.setrecursionlimit(sys.getrecursionlimit() * 5)

datas = []
datas += collect_data_files('torch')
datas += copy_metadata('torch')
datas += copy_metadata('tqdm')
datas += copy_metadata('regex')
datas += copy_metadata('requests')
datas += copy_metadata('packaging')
datas += copy_metadata('filelock')
datas += copy_metadata('numpy')
datas += copy_metadata('tokenizers')
datas += copy_metadata('importlib_metadata')
datas += copy_metadata('torch-mlir')
datas += copy_metadata('omegaconf')
datas += copy_metadata('safetensors')
datas += collect_data_files('diffusers')
datas += collect_data_files('transformers')
datas += collect_data_files('opencv-python')
datas += collect_data_files('pytorch_lightning')
datas += collect_data_files('skimage')
datas += collect_data_files('gradio')
datas += collect_data_files('gradio_client')
datas += collect_data_files('iree')
datas += collect_data_files('google-cloud-storage')
datas += collect_data_files('shark')
datas += collect_data_files('py-cpuinfo')
datas += [
         ( 'src/utils/resources/prompts.json', 'resources' ),
         ( 'src/utils/resources/model_db.json', 'resources' ),
         ( 'src/utils/resources/opt_flags.json', 'resources' ),
         ( 'src/utils/resources/base_model.json', 'resources' ),
         ]

binaries = []

block_cipher = None

hiddenimports = ['shark', 'shark.shark_inference', 'apps']
hiddenimports += [x for x in collect_submodules("skimage") if "tests" not in x]
hiddenimports += [x for x in collect_submodules("iree") if "tests" not in x]

a = Analysis(
    ['scripts/main.py'],
    pathex=['.'],
    binaries=binaries,
    datas=datas,
    hiddenimports=hiddenimports,
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)
pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='shark_sd_cli',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

```

`apps/stable_diffusion/shark_studio_imports.py`:

```py
from PyInstaller.utils.hooks import collect_data_files
from PyInstaller.utils.hooks import copy_metadata
from PyInstaller.utils.hooks import collect_submodules

import sys

sys.setrecursionlimit(sys.getrecursionlimit() * 5)

# python path for pyinstaller
pathex = [
    ".",
    "./apps/language_models/langchain",
    "./apps/language_models/src/pipelines/minigpt4_utils",
]

# datafiles for pyinstaller
datas = []
datas += copy_metadata("torch")
datas += copy_metadata("tokenizers")
datas += copy_metadata("tqdm")
datas += copy_metadata("regex")
datas += copy_metadata("requests")
datas += copy_metadata("packaging")
datas += copy_metadata("filelock")
datas += copy_metadata("numpy")
datas += copy_metadata("importlib_metadata")
datas += copy_metadata("torch-mlir")
datas += copy_metadata("omegaconf")
datas += copy_metadata("safetensors")
datas += copy_metadata("Pillow")
datas += copy_metadata("sentencepiece")
datas += copy_metadata("pyyaml")
datas += copy_metadata("huggingface-hub")
datas += collect_data_files("torch")
datas += collect_data_files("tokenizers")
datas += collect_data_files("tiktoken")
datas += collect_data_files("accelerate")
datas += collect_data_files("diffusers")
datas += collect_data_files("transformers")
datas += collect_data_files("pytorch_lightning")
datas += collect_data_files("skimage")
datas += collect_data_files("gradio")
datas += collect_data_files("gradio_client")
datas += collect_data_files("iree")
datas += collect_data_files("shark", include_py_files=True)
datas += collect_data_files("timm", include_py_files=True)
datas += collect_data_files("tqdm")
datas += collect_data_files("tkinter")
datas += collect_data_files("webview")
datas += collect_data_files("sentencepiece")
datas += collect_data_files("jsonschema")
datas += collect_data_files("jsonschema_specifications")
datas += collect_data_files("cpuinfo")
datas += collect_data_files("langchain")
datas += collect_data_files("cv2")
datas += collect_data_files("einops")
datas += [
    ("src/utils/resources/prompts.json", "resources"),
    ("src/utils/resources/model_db.json", "resources"),
    ("src/utils/resources/opt_flags.json", "resources"),
    ("src/utils/resources/base_model.json", "resources"),
    ("web/ui/css/*", "ui/css"),
    ("web/ui/logos/*", "logos"),
    (
        "../language_models/src/pipelines/minigpt4_utils/configs/*",
        "minigpt4_utils/configs",
    ),
    (
        "../language_models/src/pipelines/minigpt4_utils/prompts/*",
        "minigpt4_utils/prompts",
    ),
]


# hidden imports for pyinstaller
hiddenimports = ["shark", "shark.shark_inference", "apps"]
hiddenimports += [x for x in collect_submodules("skimage") if "tests" not in x]
hiddenimports += [
    x for x in collect_submodules("diffusers") if "tests" not in x
]
blacklist = ["tests", "convert"]
hiddenimports += [
    x
    for x in collect_submodules("transformers")
    if not any(kw in x for kw in blacklist)
]
hiddenimports += [x for x in collect_submodules("iree") if "tests" not in x]
hiddenimports += ["iree._runtime", "iree.compiler._mlir_libs._mlir.ir"]

```

`apps/stable_diffusion/src/__init__.py`:

```py
from apps.stable_diffusion.src.utils import (
    args,
    set_init_device_flags,
    prompt_examples,
    get_available_devices,
    clear_all,
    save_output_img,
    resize_stencil,
)
from apps.stable_diffusion.src.pipelines import (
    Text2ImagePipeline,
    Text2ImageSDXLPipeline,
    Image2ImagePipeline,
    InpaintPipeline,
    OutpaintPipeline,
    StencilPipeline,
    UpscalerPipeline,
)
from apps.stable_diffusion.src.schedulers import get_schedulers

```

`apps/stable_diffusion/src/models/__init__.py`:

```py
from apps.stable_diffusion.src.models.model_wrappers import (
    SharkifyStableDiffusionModel,
)
from apps.stable_diffusion.src.models.opt_params import (
    get_vae_encode,
    get_vae,
    get_unet,
    get_clip,
    get_tokenizer,
    get_params,
    get_variant_version,
)

```

`apps/stable_diffusion/src/models/model_wrappers.py`:

```py
from diffusers import AutoencoderKL, UNet2DConditionModel, ControlNetModel
from transformers import CLIPTextModel, CLIPTextModelWithProjection
from collections import defaultdict
from pathlib import Path
import torch
import safetensors.torch
import traceback
import subprocess
import sys
import os
import requests
from apps.stable_diffusion.src.utils import (
    compile_through_fx,
    get_opt_flags,
    base_models,
    args,
    preprocessCKPT,
    convert_original_vae,
    get_path_to_diffusers_checkpoint,
    get_civitai_checkpoint,
    fetch_and_update_base_model_id,
    get_path_stem,
    get_extended_name,
    get_stencil_model_id,
    update_lora_weight,
)
from shark.shark_downloader import download_public_file
from shark.shark_inference import SharkInference


# These shapes are parameter dependent.
def replace_shape_str(shape, max_len, width, height, batch_size):
    new_shape = []
    for i in range(len(shape)):
        if shape[i] == "max_len":
            new_shape.append(max_len)
        elif shape[i] == "height":
            new_shape.append(height)
        elif shape[i] == "width":
            new_shape.append(width)
        elif isinstance(shape[i], str):
            if "*" in shape[i]:
                mul_val = int(shape[i].split("*")[0])
                if "batch_size" in shape[i]:
                    new_shape.append(batch_size * mul_val)
                elif "height" in shape[i]:
                    new_shape.append(height * mul_val)
                elif "width" in shape[i]:
                    new_shape.append(width * mul_val)
            elif "/" in shape[i]:
                import math

                div_val = int(shape[i].split("/")[1])
                if "batch_size" in shape[i]:
                    new_shape.append(math.ceil(batch_size / div_val))
                elif "height" in shape[i]:
                    new_shape.append(math.ceil(height / div_val))
                elif "width" in shape[i]:
                    new_shape.append(math.ceil(width / div_val))
            elif "+" in shape[i]:
                # Currently this case only hits for SDXL. So, in case any other
                # case requires this operator, change this.
                new_shape.append(height + width)
        else:
            new_shape.append(shape[i])
    return new_shape


def check_compilation(model, model_name):
    if not model:
        raise Exception(
            f"Could not compile {model_name}. Please create an issue with the detailed log at https://github.com/nod-ai/SHARK/issues"
        )


def shark_compile_after_ir(
    module_name,
    device,
    vmfb_path,
    precision,
    ir_path=None,
):
    if ir_path:
        print(f"[DEBUG] mlir found at {ir_path.absolute()}")

    module = SharkInference(
        mlir_module=ir_path,
        device=device,
        mlir_dialect="tm_tensor",
    )
    print(f"Will get extra flag for {module_name} and precision = {precision}")
    path = module.save_module(
        vmfb_path.parent.absolute(),
        vmfb_path.stem,
        extra_args=get_opt_flags(module_name, precision=precision),
    )
    print(f"Saved {module_name} vmfb at {path}")
    module.load_module(path)
    return module


def process_vmfb_ir_sdxl(extended_model_name, model_name, device, precision):
    name_split = extended_model_name.split("_")
    if "vae" in model_name:
        name_split[5] = "fp32"
    extended_model_name_for_vmfb = "_".join(name_split)
    extended_model_name_for_mlir = "_".join(name_split[:-1])
    vmfb_path = Path(extended_model_name_for_vmfb + ".vmfb")
    if "vulkan" in device:
        _device = args.iree_vulkan_target_triple
        _device = _device.replace("-", "_")
        vmfb_path = Path(extended_model_name_for_vmfb + f"_{_device}.vmfb")
    if vmfb_path.exists():
        shark_module = SharkInference(
            None,
            device=device,
            mlir_dialect="tm_tensor",
        )
        print(f"loading existing vmfb from: {vmfb_path}")
        shark_module.load_module(vmfb_path, extra_args=[])
        return shark_module, None
    mlir_path = Path(extended_model_name_for_mlir + ".mlir")
    if not mlir_path.exists():
        print(f"Looking into gs://shark_tank/SDXL/mlir/{mlir_path.name}")
        download_public_file(
            f"gs://shark_tank/SDXL/mlir/{mlir_path.name}",
            mlir_path.absolute(),
            single_file=True,
        )
    if mlir_path.exists():
        return (
            shark_compile_after_ir(
                model_name, device, vmfb_path, precision, mlir_path
            ),
            None,
        )
    return None, None


class SharkifyStableDiffusionModel:
    def __init__(
        self,
        model_id: str,
        custom_weights: str,
        custom_vae: str,
        precision: str,
        max_len: int = 64,
        width: int = 512,
        height: int = 512,
        batch_size: int = 1,
        use_base_vae: bool = False,
        use_tuned: bool = False,
        low_cpu_mem_usage: bool = False,
        debug: bool = False,
        sharktank_dir: str = "",
        generate_vmfb: bool = True,
        is_inpaint: bool = False,
        is_upscaler: bool = False,
        is_sdxl: bool = False,
        stencils: list[str] = [],
        use_lora: str = "",
        use_quantize: str = None,
        return_mlir: bool = False,
    ):
        self.check_params(max_len, width, height)
        self.max_len = max_len
        self.is_sdxl = is_sdxl
        self.height = height // 8
        self.width = width // 8
        self.batch_size = batch_size
        self.custom_weights = custom_weights.strip()
        self.use_quantize = use_quantize
        if custom_weights != "":
            if custom_weights.startswith("https://civitai.com/api/"):
                # download the checkpoint from civitai if we don't already have it
                weights_path = get_civitai_checkpoint(custom_weights)

                # act as if we were given the local file as custom_weights originally
                custom_weights = get_path_to_diffusers_checkpoint(weights_path)
                self.custom_weights = weights_path

                # needed to ensure webui sets the correct model name metadata
                args.ckpt_loc = weights_path
            else:
                assert custom_weights.lower().endswith(
                    (".ckpt", ".safetensors")
                ), "checkpoint files supported can be any of [.ckpt, .safetensors] type"
                custom_weights = get_path_to_diffusers_checkpoint(
                    custom_weights
                )

        self.model_id = model_id if custom_weights == "" else custom_weights
        # TODO: remove the following line when stable-diffusion-2-1 works
        if self.model_id == "stabilityai/stable-diffusion-2-1":
            self.model_id = "stabilityai/stable-diffusion-2-1-base"
        self.custom_vae = custom_vae
        self.precision = precision
        self.base_vae = use_base_vae
        self.model_name = (
            "_"
            + str(batch_size)
            + "_"
            + str(max_len)
            + "_"
            + str(height)
            + "_"
            + str(width)
            + "_"
            + precision
        )
        print(f"use_tuned? sharkify: {use_tuned}")
        self.use_tuned = use_tuned
        if use_tuned:
            self.model_name = self.model_name + "_tuned"
        self.model_name = self.model_name + "_" + get_path_stem(self.model_id)
        self.low_cpu_mem_usage = low_cpu_mem_usage
        self.is_inpaint = is_inpaint
        self.is_upscaler = is_upscaler
        self.stencils = [get_stencil_model_id(x) for x in stencils]
        if use_lora != "":
            self.model_name = self.model_name + "_" + get_path_stem(use_lora)
        self.use_lora = use_lora

        print(self.model_name)
        self.model_name = self.get_extended_name_for_all_model()
        self.debug = debug
        self.sharktank_dir = sharktank_dir
        self.generate_vmfb = generate_vmfb

        self.inputs = dict()
        self.model_to_run = ""
        if self.custom_weights != "":
            self.model_to_run = self.custom_weights
            assert self.custom_weights.lower().endswith(
                (".ckpt", ".safetensors")
            ), "checkpoint files supported can be any of [.ckpt, .safetensors] type"
            preprocessCKPT(self.custom_weights, self.is_inpaint)
        else:
            self.model_to_run = args.hf_model_id
        self.custom_vae = self.process_custom_vae()
        self.base_model_id = fetch_and_update_base_model_id(self.model_to_run)
        if self.base_model_id != "" and args.ckpt_loc != "":
            args.hf_model_id = self.base_model_id
        self.return_mlir = return_mlir

    def get_extended_name_for_all_model(self):
        model_name = {}
        sub_model_list = [
            "clip",
            "clip2",
            "unet",
            "unet512",
            "stencil_unet",
            "stencil_unet_512",
            "vae",
            "vae_encode",
            "stencil_adaptor",
            "stencil_adaptor_512",
        ]
        index = 0
        for model in sub_model_list:
            sub_model = model
            model_config = self.model_name
            if "vae" == model:
                if self.custom_vae != "":
                    model_config = model_config + get_path_stem(
                        self.custom_vae
                    )
                if self.base_vae:
                    sub_model = "base_vae"
            # TODO: Fix this
            # if "stencil_adaptor" == model and self.use_stencil is not None:
            #     model_config = model_config + get_path_stem(self.use_stencil)
            model_name[model] = get_extended_name(sub_model + model_config)
            index += 1
        return model_name

    def check_params(self, max_len, width, height):
        if not (max_len >= 32 and max_len <= 77):
            sys.exit("please specify max_len in the range [32, 77].")
        if not (width % 8 == 0 and width >= 128):
            sys.exit("width should be greater than 128 and multiple of 8")
        if not (height % 8 == 0 and height >= 128):
            sys.exit("height should be greater than 128 and multiple of 8")

    # Get the input info for a model i.e. "unet", "clip", "vae", etc.
    def get_input_info_for(self, model_info):
        dtype_config = {"f32": torch.float32, "i64": torch.int64}
        input_map = []
        for inp in model_info:
            shape = model_info[inp]["shape"]
            dtype = dtype_config[model_info[inp]["dtype"]]
            tensor = None
            if isinstance(shape, list):
                clean_shape = replace_shape_str(
                    shape,
                    self.max_len,
                    self.width,
                    self.height,
                    self.batch_size,
                )
                if dtype == torch.int64:
                    tensor = torch.randint(1, 3, tuple(clean_shape))
                else:
                    tensor = torch.randn(*clean_shape).to(dtype)
            elif isinstance(shape, int):
                tensor = torch.tensor(shape).to(dtype)
            else:
                sys.exit("shape isn't specified correctly.")
            input_map.append(tensor)
        return input_map

    def get_vae_encode(self):
        class VaeEncodeModel(torch.nn.Module):
            def __init__(
                self, model_id=self.model_id, low_cpu_mem_usage=False
            ):
                super().__init__()
                self.vae = AutoencoderKL.from_pretrained(
                    model_id,
                    subfolder="vae",
                    low_cpu_mem_usage=low_cpu_mem_usage,
                )

            def forward(self, input):
                latents = self.vae.encode(input).latent_dist.sample()
                return 0.18215 * latents

        vae_encode = VaeEncodeModel()
        inputs = tuple(self.inputs["vae_encode"])
        is_f16 = (
            True
            if not self.is_upscaler and self.precision == "fp16"
            else False
        )
        shark_vae_encode, vae_encode_mlir = compile_through_fx(
            vae_encode,
            inputs,
            is_f16=is_f16,
            use_tuned=self.use_tuned,
            extended_model_name=self.model_name["vae_encode"],
            extra_args=get_opt_flags("vae", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name="vae_encode",
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_vae_encode, vae_encode_mlir

    def get_vae(self):
        class VaeModel(torch.nn.Module):
            def __init__(
                self,
                model_id=self.model_id,
                base_vae=self.base_vae,
                custom_vae=self.custom_vae,
                low_cpu_mem_usage=False,
            ):
                super().__init__()
                self.vae = None
                if custom_vae == "":
                    self.vae = AutoencoderKL.from_pretrained(
                        model_id,
                        subfolder="vae",
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                elif not isinstance(custom_vae, dict):
                    self.vae = AutoencoderKL.from_pretrained(
                        custom_vae,
                        subfolder="vae",
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                else:
                    self.vae = AutoencoderKL.from_pretrained(
                        model_id,
                        subfolder="vae",
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                    self.vae.load_state_dict(custom_vae)
                self.base_vae = base_vae

            def forward(self, input):
                if not self.base_vae:
                    input = 1 / 0.18215 * input
                x = self.vae.decode(input, return_dict=False)[0]
                x = (x / 2 + 0.5).clamp(0, 1)
                if self.base_vae:
                    return x
                x = x * 255.0
                return x.round()

        vae = VaeModel(low_cpu_mem_usage=self.low_cpu_mem_usage)
        inputs = tuple(self.inputs["vae"])
        is_f16 = (
            True
            if not self.is_upscaler and self.precision == "fp16"
            else False
        )
        save_dir = os.path.join(self.sharktank_dir, self.model_name["vae"])
        if self.debug:
            os.makedirs(save_dir, exist_ok=True)
        shark_vae, vae_mlir = compile_through_fx(
            vae,
            inputs,
            is_f16=is_f16,
            use_tuned=self.use_tuned,
            extended_model_name=self.model_name["vae"],
            debug=self.debug,
            generate_vmfb=self.generate_vmfb,
            save_dir=save_dir,
            extra_args=get_opt_flags("vae", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name="vae",
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_vae, vae_mlir

    def get_vae_sdxl(self):
        # TODO: Remove this after convergence with shark_tank. This should just be part of
        #       opt_params.py.
        shark_module_or_none = process_vmfb_ir_sdxl(
            self.model_name["vae"], "vae", args.device, self.precision
        )
        if shark_module_or_none[0]:
            return shark_module_or_none

        class VaeModel(torch.nn.Module):
            def __init__(
                self,
                model_id=self.model_id,
                base_vae=self.base_vae,
                custom_vae=self.custom_vae,
                low_cpu_mem_usage=False,
            ):
                super().__init__()
                self.vae = None
                if custom_vae == "":
                    self.vae = AutoencoderKL.from_pretrained(
                        model_id,
                        subfolder="vae",
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                elif not isinstance(custom_vae, dict):
                    self.vae = AutoencoderKL.from_pretrained(
                        custom_vae,
                        subfolder="vae",
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                else:
                    self.vae = AutoencoderKL.from_pretrained(
                        model_id,
                        subfolder="vae",
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                    self.vae.load_state_dict(custom_vae)

            def forward(self, latents):
                image = self.vae.decode(latents / 0.13025, return_dict=False)[
                    0
                ]
                return image

        vae = VaeModel(low_cpu_mem_usage=self.low_cpu_mem_usage)
        inputs = tuple(self.inputs["vae"])
        # Make sure the VAE is in float32 mode, as it overflows in float16 as per SDXL
        # pipeline.
        is_f16 = False
        save_dir = os.path.join(self.sharktank_dir, self.model_name["vae"])
        if self.debug:
            os.makedirs(save_dir, exist_ok=True)
        shark_vae, vae_mlir = compile_through_fx(
            vae,
            inputs,
            is_f16=is_f16,
            use_tuned=self.use_tuned,
            extended_model_name=self.model_name["vae"],
            debug=self.debug,
            generate_vmfb=self.generate_vmfb,
            save_dir=save_dir,
            extra_args=get_opt_flags("vae", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name="vae",
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_vae, vae_mlir

    def get_controlled_unet(self, use_large=False):
        class ControlledUnetModel(torch.nn.Module):
            def __init__(
                self,
                model_id=self.model_id,
                low_cpu_mem_usage=False,
                use_lora=self.use_lora,
            ):
                super().__init__()
                self.unet = UNet2DConditionModel.from_pretrained(
                    model_id,
                    subfolder="unet",
                    low_cpu_mem_usage=low_cpu_mem_usage,
                )
                if use_lora != "":
                    update_lora_weight(self.unet, use_lora, "unet")
                self.in_channels = self.unet.in_channels
                self.train(False)

            def forward(
                self,
                latent,
                timestep,
                text_embedding,
                guidance_scale,
                control1,
                control2,
                control3,
                control4,
                control5,
                control6,
                control7,
                control8,
                control9,
                control10,
                control11,
                control12,
                control13,
                scale1,
                scale2,
                scale3,
                scale4,
                scale5,
                scale6,
                scale7,
                scale8,
                scale9,
                scale10,
                scale11,
                scale12,
                scale13,
            ):
                # TODO: Average pooling
                db_res_samples = [
                    control1,
                    control2,
                    control3,
                    control4,
                    control5,
                    control6,
                    control7,
                    control8,
                    control9,
                    control10,
                    control11,
                    control12,
                ]

                # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
                db_res_samples = tuple(
                    [
                        control1 * scale1,
                        control2 * scale2,
                        control3 * scale3,
                        control4 * scale4,
                        control5 * scale5,
                        control6 * scale6,
                        control7 * scale7,
                        control8 * scale8,
                        control9 * scale9,
                        control10 * scale10,
                        control11 * scale11,
                        control12 * scale12,
                    ]
                )
                mb_res_samples = control13 * scale13
                latents = torch.cat([latent] * 2)
                unet_out = self.unet.forward(
                    latents,
                    timestep,
                    encoder_hidden_states=text_embedding,
                    down_block_additional_residuals=db_res_samples,
                    mid_block_additional_residual=mb_res_samples,
                    return_dict=False,
                )[0]
                noise_pred_uncond, noise_pred_text = unet_out.chunk(2)
                noise_pred = noise_pred_uncond + guidance_scale * (
                    noise_pred_text - noise_pred_uncond
                )
                return noise_pred

        unet = ControlledUnetModel(low_cpu_mem_usage=self.low_cpu_mem_usage)
        is_f16 = True if self.precision == "fp16" else False

        inputs = tuple(self.inputs["unet"])
        model_name = "stencil_unet"
        if use_large:
            pad = (0, 0) * (len(inputs[2].shape) - 2)
            pad = pad + (0, 512 - inputs[2].shape[1])
            inputs = (
                inputs[:2]
                + (torch.nn.functional.pad(inputs[2], pad),)
                + inputs[3:]
            )
            model_name = "stencil_unet_512"
        input_mask = [
            True,
            True,
            True,
            False,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
        ]
        shark_controlled_unet, controlled_unet_mlir = compile_through_fx(
            unet,
            inputs,
            extended_model_name=self.model_name[model_name],
            is_f16=is_f16,
            f16_input_mask=input_mask,
            use_tuned=self.use_tuned,
            extra_args=get_opt_flags("unet", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name=model_name,
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_controlled_unet, controlled_unet_mlir

    def get_control_net(self, stencil_id, use_large=False):
        stencil_id = get_stencil_model_id(stencil_id)

        class StencilControlNetModel(torch.nn.Module):
            def __init__(self, model_id=stencil_id, low_cpu_mem_usage=False):
                super().__init__()
                self.cnet = ControlNetModel.from_pretrained(
                    model_id,
                    low_cpu_mem_usage=low_cpu_mem_usage,
                )
                self.in_channels = self.cnet.in_channels
                self.train(False)

            def forward(
                self,
                latent,
                timestep,
                text_embedding,
                stencil_image_input,
                acc1,
                acc2,
                acc3,
                acc4,
                acc5,
                acc6,
                acc7,
                acc8,
                acc9,
                acc10,
                acc11,
                acc12,
                acc13,
            ):
                # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
                # TODO: guidance NOT NEEDED change in `get_input_info` later
                latents = torch.cat(
                    [latent] * 2
                )  # needs to be same as controlledUNET latents
                stencil_image = torch.cat(
                    [stencil_image_input] * 2
                )  # needs to be same as controlledUNET latents
                (
                    down_block_res_samples,
                    mid_block_res_sample,
                ) = self.cnet.forward(
                    latents,
                    timestep,
                    encoder_hidden_states=text_embedding,
                    controlnet_cond=stencil_image,
                    return_dict=False,
                )
                return tuple(
                    list(down_block_res_samples) + [mid_block_res_sample]
                ) + (
                    acc1 + down_block_res_samples[0],
                    acc2 + down_block_res_samples[1],
                    acc3 + down_block_res_samples[2],
                    acc4 + down_block_res_samples[3],
                    acc5 + down_block_res_samples[4],
                    acc6 + down_block_res_samples[5],
                    acc7 + down_block_res_samples[6],
                    acc8 + down_block_res_samples[7],
                    acc9 + down_block_res_samples[8],
                    acc10 + down_block_res_samples[9],
                    acc11 + down_block_res_samples[10],
                    acc12 + down_block_res_samples[11],
                    acc13 + mid_block_res_sample,
                )

        scnet = StencilControlNetModel(
            low_cpu_mem_usage=self.low_cpu_mem_usage
        )
        is_f16 = True if self.precision == "fp16" else False

        inputs = tuple(self.inputs["stencil_adaptor"])
        if use_large:
            pad = (0, 0) * (len(inputs[2].shape) - 2)
            pad = pad + (0, 512 - inputs[2].shape[1])
            inputs = (
                inputs[0],
                inputs[1],
                torch.nn.functional.pad(inputs[2], pad),
                *inputs[3:],
            )
            save_dir = os.path.join(
                self.sharktank_dir, self.model_name["stencil_adaptor_512"]
            )
        else:
            save_dir = os.path.join(
                self.sharktank_dir, self.model_name["stencil_adaptor"]
            )
        input_mask = [True, True, True, True] + ([True] * 13)
        model_name = "stencil_adaptor" if use_large else "stencil_adaptor_512"
        shark_cnet, cnet_mlir = compile_through_fx(
            scnet,
            inputs,
            extended_model_name=self.model_name[model_name],
            is_f16=is_f16,
            f16_input_mask=input_mask,
            use_tuned=self.use_tuned,
            extra_args=get_opt_flags("unet", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name=model_name,
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_cnet, cnet_mlir

    def get_unet(self, use_large=False):
        class UnetModel(torch.nn.Module):
            def __init__(
                self,
                model_id=self.model_id,
                low_cpu_mem_usage=False,
                use_lora=self.use_lora,
            ):
                super().__init__()
                self.unet = UNet2DConditionModel.from_pretrained(
                    model_id,
                    subfolder="unet",
                    low_cpu_mem_usage=low_cpu_mem_usage,
                )
                if use_lora != "":
                    update_lora_weight(self.unet, use_lora, "unet")
                self.in_channels = self.unet.config.in_channels
                self.train(False)
                if (
                    args.attention_slicing is not None
                    and args.attention_slicing != "none"
                ):
                    if args.attention_slicing.isdigit():
                        self.unet.set_attention_slice(
                            int(args.attention_slicing)
                        )
                    else:
                        self.unet.set_attention_slice(args.attention_slicing)

            # TODO: Instead of flattening the `control` try to use the list.
            def forward(
                self,
                latent,
                timestep,
                text_embedding,
                guidance_scale,
            ):
                # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
                latents = torch.cat([latent] * 2)
                unet_out = self.unet.forward(
                    latents, timestep, text_embedding, return_dict=False
                )[0]
                noise_pred_uncond, noise_pred_text = unet_out.chunk(2)
                noise_pred = noise_pred_uncond + guidance_scale * (
                    noise_pred_text - noise_pred_uncond
                )
                return noise_pred

        unet = UnetModel(low_cpu_mem_usage=self.low_cpu_mem_usage)
        is_f16 = True if self.precision == "fp16" else False
        inputs = tuple(self.inputs["unet"])
        if use_large:
            pad = (0, 0) * (len(inputs[2].shape) - 2)
            pad = pad + (0, 512 - inputs[2].shape[1])
            inputs = (
                inputs[0],
                inputs[1],
                torch.nn.functional.pad(inputs[2], pad),
                inputs[3],
            )
            save_dir = os.path.join(
                self.sharktank_dir, self.model_name["unet512"]
            )
        else:
            save_dir = os.path.join(
                self.sharktank_dir, self.model_name["unet"]
            )
        input_mask = [True, True, True, False]
        if self.debug:
            os.makedirs(
                save_dir,
                exist_ok=True,
            )
        model_name = "unet512" if use_large else "unet"
        shark_unet, unet_mlir = compile_through_fx(
            unet,
            inputs,
            extended_model_name=self.model_name[model_name],
            is_f16=is_f16,
            f16_input_mask=input_mask,
            use_tuned=self.use_tuned,
            debug=self.debug,
            generate_vmfb=self.generate_vmfb,
            save_dir=save_dir,
            extra_args=get_opt_flags("unet", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name=model_name,
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_unet, unet_mlir

    def get_unet_upscaler(self, use_large=False):
        class UnetModel(torch.nn.Module):
            def __init__(
                self, model_id=self.model_id, low_cpu_mem_usage=False
            ):
                super().__init__()
                self.unet = UNet2DConditionModel.from_pretrained(
                    model_id,
                    subfolder="unet",
                    low_cpu_mem_usage=low_cpu_mem_usage,
                )
                self.in_channels = self.unet.in_channels
                self.train(False)

            def forward(self, latent, timestep, text_embedding, noise_level):
                unet_out = self.unet.forward(
                    latent,
                    timestep,
                    text_embedding,
                    noise_level,
                    return_dict=False,
                )[0]
                return unet_out

        unet = UnetModel(low_cpu_mem_usage=self.low_cpu_mem_usage)
        is_f16 = True if self.precision == "fp16" else False
        inputs = tuple(self.inputs["unet"])
        if use_large:
            pad = (0, 0) * (len(inputs[2].shape) - 2)
            pad = pad + (0, 512 - inputs[2].shape[1])
            inputs = (
                inputs[0],
                inputs[1],
                torch.nn.functional.pad(inputs[2], pad),
                inputs[3],
            )
        input_mask = [True, True, True, False]
        model_name = "unet512" if use_large else "unet"
        shark_unet, unet_mlir = compile_through_fx(
            unet,
            inputs,
            extended_model_name=self.model_name[model_name],
            is_f16=is_f16,
            f16_input_mask=input_mask,
            use_tuned=self.use_tuned,
            extra_args=get_opt_flags("unet", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name=model_name,
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_unet, unet_mlir

    def get_unet_sdxl(self):
        # TODO: Remove this after convergence with shark_tank. This should just be part of
        #       opt_params.py.
        shark_module_or_none = process_vmfb_ir_sdxl(
            self.model_name["unet"], "unet", args.device, self.precision
        )
        if shark_module_or_none[0]:
            return shark_module_or_none

        class UnetModel(torch.nn.Module):
            def __init__(
                self,
                model_id=self.model_id,
                low_cpu_mem_usage=False,
            ):
                super().__init__()
                self.unet = UNet2DConditionModel.from_pretrained(
                    model_id,
                    subfolder="unet",
                    low_cpu_mem_usage=low_cpu_mem_usage,
                )
                if (
                    args.attention_slicing is not None
                    and args.attention_slicing != "none"
                ):
                    if args.attention_slicing.isdigit():
                        self.unet.set_attention_slice(
                            int(args.attention_slicing)
                        )
                    else:
                        self.unet.set_attention_slice(args.attention_slicing)

            def forward(
                self,
                latent,
                timestep,
                prompt_embeds,
                text_embeds,
                time_ids,
                guidance_scale,
            ):
                added_cond_kwargs = {
                    "text_embeds": text_embeds,
                    "time_ids": time_ids,
                }
                noise_pred = self.unet.forward(
                    latent,
                    timestep,
                    encoder_hidden_states=prompt_embeds,
                    cross_attention_kwargs=None,
                    added_cond_kwargs=added_cond_kwargs,
                    return_dict=False,
                )[0]
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + guidance_scale * (
                    noise_pred_text - noise_pred_uncond
                )
                return noise_pred

        unet = UnetModel(low_cpu_mem_usage=self.low_cpu_mem_usage)
        is_f16 = True if self.precision == "fp16" else False
        inputs = tuple(self.inputs["unet"])
        save_dir = os.path.join(self.sharktank_dir, self.model_name["unet"])
        input_mask = [True, True, True, True, True, True]
        if self.debug:
            os.makedirs(
                save_dir,
                exist_ok=True,
            )
        shark_unet, unet_mlir = compile_through_fx(
            unet,
            inputs,
            extended_model_name=self.model_name["unet"],
            is_f16=is_f16,
            f16_input_mask=input_mask,
            use_tuned=self.use_tuned,
            debug=self.debug,
            generate_vmfb=self.generate_vmfb,
            save_dir=save_dir,
            extra_args=get_opt_flags("unet", precision=self.precision),
            base_model_id=self.base_model_id,
            model_name="unet",
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_unet, unet_mlir

    def get_clip(self):
        class CLIPText(torch.nn.Module):
            def __init__(
                self,
                model_id=self.model_id,
                low_cpu_mem_usage=False,
                use_lora=self.use_lora,
            ):
                super().__init__()
                self.text_encoder = CLIPTextModel.from_pretrained(
                    model_id,
                    subfolder="text_encoder",
                    low_cpu_mem_usage=low_cpu_mem_usage,
                )
                if use_lora != "":
                    update_lora_weight(
                        self.text_encoder, use_lora, "text_encoder"
                    )

            def forward(self, input):
                return self.text_encoder(input)[0]

        clip_model = CLIPText(low_cpu_mem_usage=self.low_cpu_mem_usage)
        save_dir = ""
        if self.debug:
            save_dir = os.path.join(
                self.sharktank_dir, self.model_name["clip"]
            )
            os.makedirs(
                save_dir,
                exist_ok=True,
            )
        shark_clip, clip_mlir = compile_through_fx(
            clip_model,
            tuple(self.inputs["clip"]),
            extended_model_name=self.model_name["clip"],
            debug=self.debug,
            generate_vmfb=self.generate_vmfb,
            save_dir=save_dir,
            extra_args=get_opt_flags("clip", precision="fp32"),
            base_model_id=self.base_model_id,
            model_name="clip",
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_clip, clip_mlir

    def get_clip_sdxl(self, clip_index=1):
        if clip_index == 1:
            extended_model_name = self.model_name["clip"]
            model_name = "clip"
        else:
            extended_model_name = self.model_name["clip2"]
            model_name = "clip2"
        # TODO: Remove this after convergence with shark_tank. This should just be part of
        #       opt_params.py.
        shark_module_or_none = process_vmfb_ir_sdxl(
            extended_model_name, f"clip", args.device, self.precision
        )
        if shark_module_or_none[0]:
            return shark_module_or_none

        class CLIPText(torch.nn.Module):
            def __init__(
                self,
                model_id=self.model_id,
                low_cpu_mem_usage=False,
                clip_index=1,
            ):
                super().__init__()
                if clip_index == 1:
                    self.text_encoder = CLIPTextModel.from_pretrained(
                        model_id,
                        subfolder="text_encoder",
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                else:
                    self.text_encoder = (
                        CLIPTextModelWithProjection.from_pretrained(
                            model_id,
                            subfolder="text_encoder_2",
                            low_cpu_mem_usage=low_cpu_mem_usage,
                        )
                    )

            def forward(self, input):
                prompt_embeds = self.text_encoder(
                    input,
                    output_hidden_states=True,
                )
                # We are only ALWAYS interested in the pooled output of the final text encoder
                pooled_prompt_embeds = prompt_embeds[0]
                prompt_embeds = prompt_embeds.hidden_states[-2]
                return prompt_embeds, pooled_prompt_embeds

        clip_model = CLIPText(
            low_cpu_mem_usage=self.low_cpu_mem_usage, clip_index=clip_index
        )
        save_dir = os.path.join(self.sharktank_dir, extended_model_name)
        if self.debug:
            os.makedirs(
                save_dir,
                exist_ok=True,
            )
        shark_clip, clip_mlir = compile_through_fx(
            clip_model,
            tuple(self.inputs["clip"]),
            extended_model_name=extended_model_name,
            debug=self.debug,
            generate_vmfb=self.generate_vmfb,
            save_dir=save_dir,
            extra_args=get_opt_flags("clip", precision="fp32"),
            base_model_id=self.base_model_id,
            model_name="clip",
            precision=self.precision,
            return_mlir=self.return_mlir,
        )
        return shark_clip, clip_mlir

    def process_custom_vae(self):
        custom_vae = self.custom_vae.lower()
        if not custom_vae.endswith((".ckpt", ".safetensors")):
            return self.custom_vae
        try:
            preprocessCKPT(self.custom_vae)
            return get_path_to_diffusers_checkpoint(self.custom_vae)
        except:
            print("Processing standalone Vae checkpoint")
            vae_checkpoint = None
            vae_ignore_keys = {"model_ema.decay", "model_ema.num_updates"}
            if custom_vae.endswith(".ckpt"):
                vae_checkpoint = torch.load(
                    self.custom_vae, map_location="cpu"
                )
            else:
                vae_checkpoint = safetensors.torch.load_file(
                    self.custom_vae, device="cpu"
                )
            if "state_dict" in vae_checkpoint:
                vae_checkpoint = vae_checkpoint["state_dict"]

            try:
                vae_checkpoint = convert_original_vae(vae_checkpoint)
            finally:
                vae_dict = {
                    k: v
                    for k, v in vae_checkpoint.items()
                    if k[0:4] != "loss" and k not in vae_ignore_keys
                }
                return vae_dict

    def compile_unet_variants(self, model, use_large=False, base_model=""):
        if self.is_sdxl:
            return self.get_unet_sdxl()
        if model == "unet":
            if self.is_upscaler:
                return self.get_unet_upscaler(use_large=use_large)
            # TODO: Plug the experimental "int8" support at right place.
            elif self.use_quantize == "int8":
                from apps.stable_diffusion.src.models.opt_params import (
                    get_unet,
                )

                return get_unet()
            else:
                return self.get_unet(use_large=use_large)
        else:
            return self.get_controlled_unet(use_large=use_large)

    def vae_encode(self):
        try:
            self.inputs["vae_encode"] = self.get_input_info_for(
                base_models["vae_encode"]
            )
            compiled_vae_encode, vae_encode_mlir = self.get_vae_encode()

            check_compilation(compiled_vae_encode, "Vae Encode")
            if self.return_mlir:
                return vae_encode_mlir
            return compiled_vae_encode
        except Exception as e:
            sys.exit(e)

    def clip(self):
        try:
            self.inputs["clip"] = self.get_input_info_for(base_models["clip"])
            compiled_clip, clip_mlir = self.get_clip()

            check_compilation(compiled_clip, "Clip")
            if self.return_mlir:
                return clip_mlir
            return compiled_clip
        except Exception as e:
            sys.exit(e)

    def sdxl_clip(self):
        try:
            self.inputs["clip"] = self.get_input_info_for(
                base_models["sdxl_clip"]
            )
            compiled_clip, clip_mlir = self.get_clip_sdxl(clip_index=1)
            compiled_clip2, clip_mlir2 = self.get_clip_sdxl(clip_index=2)

            check_compilation(compiled_clip, "Clip")
            check_compilation(compiled_clip, "Clip2")
            if self.return_mlir:
                return clip_mlir, clip_mlir2
            return compiled_clip, compiled_clip2
        except Exception as e:
            sys.exit(e)

    def unet(self, use_large=False):
        try:
            stencil_count = 0
            for stencil in self.stencils:
                stencil_count += 1
            model = "stencil_unet" if stencil_count > 0 else "unet"
            compiled_unet = None
            unet_inputs = base_models[model]

            if self.base_model_id != "":
                self.inputs["unet"] = self.get_input_info_for(
                    unet_inputs[self.base_model_id]
                )
                compiled_unet, unet_mlir = self.compile_unet_variants(
                    model, use_large=use_large, base_model=self.base_model_id
                )
            else:
                for model_id in unet_inputs:
                    self.base_model_id = model_id
                    self.inputs["unet"] = self.get_input_info_for(
                        unet_inputs[model_id]
                    )

                    try:
                        compiled_unet, unet_mlir = self.compile_unet_variants(
                            model, use_large=use_large, base_model=model_id
                        )
                    except Exception as e:
                        print(e)
                        print(
                            "Retrying with a different base model configuration"
                        )
                        continue

                    # -- Once a successful compilation has taken place we'd want to store
                    #    the base model's configuration inferred.
                    fetch_and_update_base_model_id(self.model_to_run, model_id)
                    # This is done just because in main.py we are basing the choice of tokenizer and scheduler
                    # on `args.hf_model_id`. Since now, we don't maintain 1:1 mapping of variants and the base
                    # model and rely on retrying method to find the input configuration, we should also update
                    # the knowledge of base model id accordingly into `args.hf_model_id`.
                    if args.ckpt_loc != "":
                        args.hf_model_id = model_id
                    break

            check_compilation(compiled_unet, "Unet")
            if self.return_mlir:
                return unet_mlir
            return compiled_unet
        except Exception as e:
            sys.exit(e)

    def vae(self):
        try:
            vae_input = (
                base_models["vae"]["vae_upscaler"]
                if self.is_upscaler
                else base_models["vae"]["vae"]
            )
            self.inputs["vae"] = self.get_input_info_for(vae_input)

            is_base_vae = self.base_vae
            if self.is_upscaler:
                self.base_vae = True
            if self.is_sdxl:
                compiled_vae, vae_mlir = self.get_vae_sdxl()
            else:
                compiled_vae, vae_mlir = self.get_vae()
            self.base_vae = is_base_vae

            check_compilation(compiled_vae, "Vae")
            if self.return_mlir:
                return vae_mlir
            return compiled_vae
        except Exception as e:
            sys.exit(e)

    def controlnet(self, stencil_id, use_large=False):
        try:
            self.inputs["stencil_adaptor"] = self.get_input_info_for(
                base_models["stencil_adaptor"]
            )
            compiled_stencil_adaptor, controlnet_mlir = self.get_control_net(
                stencil_id, use_large=use_large
            )

            check_compilation(compiled_stencil_adaptor, "Stencil")
            if self.return_mlir:
                return controlnet_mlir
            return compiled_stencil_adaptor
        except Exception as e:
            sys.exit(e)

```

`apps/stable_diffusion/src/models/opt_params.py`:

```py
import sys
from transformers import CLIPTokenizer
from apps.stable_diffusion.src.utils import (
    models_db,
    args,
    get_shark_model,
    get_opt_flags,
)


hf_model_variant_map = {
    "Linaqruf/anything-v3.0": ["anythingv3", "v1_4"],
    "dreamlike-art/dreamlike-diffusion-1.0": ["dreamlike", "v1_4"],
    "prompthero/openjourney": ["openjourney", "v1_4"],
    "wavymulder/Analog-Diffusion": ["analogdiffusion", "v1_4"],
    "stabilityai/stable-diffusion-2-1": ["stablediffusion", "v2_1base"],
    "stabilityai/stable-diffusion-2-1-base": ["stablediffusion", "v2_1base"],
    "CompVis/stable-diffusion-v1-4": ["stablediffusion", "v1_4"],
    "runwayml/stable-diffusion-inpainting": ["stablediffusion", "inpaint_v1"],
    "stabilityai/stable-diffusion-2-inpainting": [
        "stablediffusion",
        "inpaint_v2",
    ],
}


# TODO: Add the quantized model as a part model_db.json.
# This is currently in experimental phase.
def get_quantize_model():
    bucket_key = "gs://shark_tank/prashant_nod"
    model_key = "unet_int8"
    iree_flags = get_opt_flags("unet", precision="fp16")
    if args.height != 512 and args.width != 512 and args.max_length != 77:
        sys.exit(
            "The int8 quantized model currently requires the height and width to be 512, and max_length to be 77"
        )
    return bucket_key, model_key, iree_flags


def get_variant_version(hf_model_id):
    return hf_model_variant_map[hf_model_id]


def get_params(bucket_key, model_key, model, is_tuned, precision):
    try:
        bucket = models_db[0][bucket_key]
        model_name = models_db[1][model_key]
    except KeyError:
        raise Exception(
            f"{bucket_key}/{model_key} is not present in the models database"
        )
    iree_flags = get_opt_flags(model, precision="fp16")
    return bucket, model_name, iree_flags


def get_unet():
    variant, version = get_variant_version(args.hf_model_id)
    # Tuned model is present only for `fp16` precision.
    is_tuned = "tuned" if args.use_tuned else "untuned"

    # TODO: Get the quantize model from model_db.json
    if args.use_quantize == "int8":
        bk, mk, flags = get_quantize_model()
        return get_shark_model(bk, mk, flags)

    if "vulkan" not in args.device and args.use_tuned:
        bucket_key = f"{variant}/{is_tuned}/{args.device}"
        model_key = f"{variant}/{version}/unet/{args.precision}/length_{args.max_length}/{is_tuned}/{args.device}"
    else:
        bucket_key = f"{variant}/{is_tuned}"
        model_key = f"{variant}/{version}/unet/{args.precision}/length_{args.max_length}/{is_tuned}"

    bucket, model_name, iree_flags = get_params(
        bucket_key, model_key, "unet", is_tuned, args.precision
    )
    return get_shark_model(bucket, model_name, iree_flags)


def get_vae_encode():
    variant, version = get_variant_version(args.hf_model_id)
    # Tuned model is present only for `fp16` precision.
    is_tuned = "tuned" if args.use_tuned else "untuned"
    if "vulkan" not in args.device and args.use_tuned:
        bucket_key = f"{variant}/{is_tuned}/{args.device}"
        model_key = f"{variant}/{version}/vae_encode/{args.precision}/length_77/{is_tuned}/{args.device}"
    else:
        bucket_key = f"{variant}/{is_tuned}"
        model_key = f"{variant}/{version}/vae_encode/{args.precision}/length_77/{is_tuned}"

    bucket, model_name, iree_flags = get_params(
        bucket_key, model_key, "vae", is_tuned, args.precision
    )
    return get_shark_model(bucket, model_name, iree_flags)


def get_vae():
    variant, version = get_variant_version(args.hf_model_id)
    # Tuned model is present only for `fp16` precision.
    is_tuned = "tuned" if args.use_tuned else "untuned"
    is_base = "/base" if args.use_base_vae else ""
    if "vulkan" not in args.device and args.use_tuned:
        bucket_key = f"{variant}/{is_tuned}/{args.device}"
        model_key = f"{variant}/{version}/vae/{args.precision}/length_77/{is_tuned}{is_base}/{args.device}"
    else:
        bucket_key = f"{variant}/{is_tuned}"
        model_key = f"{variant}/{version}/vae/{args.precision}/length_77/{is_tuned}{is_base}"

    bucket, model_name, iree_flags = get_params(
        bucket_key, model_key, "vae", is_tuned, args.precision
    )
    return get_shark_model(bucket, model_name, iree_flags)


def get_clip():
    variant, version = get_variant_version(args.hf_model_id)
    bucket_key = f"{variant}/untuned"
    model_key = (
        f"{variant}/{version}/clip/fp32/length_{args.max_length}/untuned"
    )
    bucket, model_name, iree_flags = get_params(
        bucket_key, model_key, "clip", "untuned", "fp32"
    )
    return get_shark_model(bucket, model_name, iree_flags)


def get_tokenizer(subfolder="tokenizer"):
    tokenizer = CLIPTokenizer.from_pretrained(
        args.hf_model_id, subfolder=subfolder
    )
    return tokenizer

```

`apps/stable_diffusion/src/pipelines/__init__.py`:

```py
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_txt2img import (
    Text2ImagePipeline,
)
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_txt2img_sdxl import (
    Text2ImageSDXLPipeline,
)
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_img2img import (
    Image2ImagePipeline,
)
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_inpaint import (
    InpaintPipeline,
)
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_outpaint import (
    OutpaintPipeline,
)
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_stencil import (
    StencilPipeline,
)
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_upscaler import (
    UpscalerPipeline,
)

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_img2img.py`:

```py
import torch
import time
import numpy as np
from tqdm.auto import tqdm
from random import randint
from PIL import Image
from transformers import CLIPTokenizer
from typing import Union
from shark.shark_inference import SharkInference
from diffusers import (
    DDIMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
    DDPMScheduler,
    KDPM2DiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    StableDiffusionPipeline,
)
from apps.stable_diffusion.src.models import (
    SharkifyStableDiffusionModel,
    get_vae_encode,
)
from apps.stable_diffusion.src.utils import (
    resamplers,
    resampler_list,
)


class Image2ImagePipeline(StableDiffusionPipeline):
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
            DDPMScheduler,
            KDPM2DiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
    ):
        super().__init__(scheduler, sd_model, import_mlir, use_lora, ondemand)
        self.vae_encode = None

    def load_vae_encode(self):
        if self.vae_encode is not None:
            return

        if self.import_mlir or self.use_lora:
            self.vae_encode = self.sd_model.vae_encode()
        else:
            try:
                self.vae_encode = get_vae_encode()
            except:
                print("download pipeline failed, falling back to import_mlir")
                self.vae_encode = self.sd_model.vae_encode()

    def unload_vae_encode(self):
        del self.vae_encode
        self.vae_encode = None

    def prepare_image_latents(
        self,
        image,
        batch_size,
        height,
        width,
        generator,
        num_inference_steps,
        strength,
        dtype,
        resample_type,
    ):
        # Pre process image -> get image encoded -> process latents

        # TODO: process with variable HxW combos

        # Pre-process image
        resample_type = (
            resamplers[resample_type]
            if resample_type in resampler_list
            # Fallback to Lanczos
            else Image.Resampling.LANCZOS
        )

        image = image.resize((width, height), resample=resample_type)
        image_arr = np.stack([np.array(i) for i in (image,)], axis=0)
        image_arr = image_arr / 255.0
        image_arr = torch.from_numpy(image_arr).permute(0, 3, 1, 2).to(dtype)
        image_arr = 2 * (image_arr - 0.5)

        # set scheduler steps
        self.scheduler.set_timesteps(num_inference_steps)
        init_timestep = min(
            int(num_inference_steps * strength), num_inference_steps
        )
        t_start = max(num_inference_steps - init_timestep, 0)
        # timesteps reduced as per strength
        timesteps = self.scheduler.timesteps[t_start:]
        # new number of steps to be used as per strength will be
        # num_inference_steps = num_inference_steps - t_start

        # image encode
        latents = self.encode_image((image_arr,))
        latents = torch.from_numpy(latents).to(dtype)
        # add noise to data
        noise = torch.randn(latents.shape, generator=generator, dtype=dtype)
        latents = self.scheduler.add_noise(
            latents, noise, timesteps[0].repeat(1)
        )

        return latents, timesteps

    def encode_image(self, input_image):
        self.load_vae_encode()
        vae_encode_start = time.time()
        latents = self.vae_encode("forward", input_image)
        vae_inf_time = (time.time() - vae_encode_start) * 1000
        if self.ondemand:
            self.unload_vae_encode()
        self.log += f"\nVAE Encode Inference time (ms): {vae_inf_time:.3f}"

        return latents

    def generate_images(
        self,
        prompts,
        neg_prompts,
        image,
        batch_size,
        height,
        width,
        num_inference_steps,
        strength,
        guidance_scale,
        seed,
        max_length,
        dtype,
        use_base_vae,
        cpu_scheduling,
        max_embeddings_multiples,
        resample_type,
    ):
        # prompts and negative prompts must be a list.
        if isinstance(prompts, str):
            prompts = [prompts]

        if isinstance(neg_prompts, str):
            neg_prompts = [neg_prompts]

        prompts = prompts * batch_size
        neg_prompts = neg_prompts * batch_size

        # seed generator to create the inital latent noise. Also handle out of range seeds.
        uint32_info = np.iinfo(np.uint32)
        uint32_min, uint32_max = uint32_info.min, uint32_info.max
        if seed < uint32_min or seed >= uint32_max:
            seed = randint(uint32_min, uint32_max)
        generator = torch.manual_seed(seed)

        # Get text embeddings with weight emphasis from prompts
        text_embeddings = self.encode_prompts_weight(
            prompts,
            neg_prompts,
            max_length,
            max_embeddings_multiples=max_embeddings_multiples,
        )

        # guidance scale as a float32 tensor.
        guidance_scale = torch.tensor(guidance_scale).to(torch.float32)

        # Prepare input image latent
        image_latents, final_timesteps = self.prepare_image_latents(
            image=image,
            batch_size=batch_size,
            height=height,
            width=width,
            generator=generator,
            num_inference_steps=num_inference_steps,
            strength=strength,
            dtype=dtype,
            resample_type=resample_type,
        )

        # Get Image latents
        latents = self.produce_img_latents(
            latents=image_latents,
            text_embeddings=text_embeddings,
            guidance_scale=guidance_scale,
            total_timesteps=final_timesteps,
            dtype=dtype,
            cpu_scheduling=cpu_scheduling,
        )

        # Img latents -> PIL images
        all_imgs = []
        self.load_vae()
        for i in tqdm(range(0, latents.shape[0], batch_size)):
            imgs = self.decode_latents(
                latents=latents[i : i + batch_size],
                use_base_vae=use_base_vae,
                cpu_scheduling=cpu_scheduling,
            )
            all_imgs.extend(imgs)
        if self.ondemand:
            self.unload_vae()

        return all_imgs

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_inpaint.py`:

```py
import torch
from tqdm.auto import tqdm
import numpy as np
from random import randint
from PIL import Image, ImageOps
from transformers import CLIPTokenizer
from typing import Union
from shark.shark_inference import SharkInference
from diffusers import (
    DDIMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
    DDPMScheduler,
    KDPM2DiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    StableDiffusionPipeline,
)
from apps.stable_diffusion.src.models import (
    SharkifyStableDiffusionModel,
    get_vae_encode,
)


class InpaintPipeline(StableDiffusionPipeline):
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
            DDPMScheduler,
            KDPM2DiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
    ):
        super().__init__(scheduler, sd_model, import_mlir, use_lora, ondemand)
        self.vae_encode = None

    def load_vae_encode(self):
        if self.vae_encode is not None:
            return

        if self.import_mlir or self.use_lora:
            self.vae_encode = self.sd_model.vae_encode()
        else:
            try:
                self.vae_encode = get_vae_encode()
            except:
                print("download pipeline failed, falling back to import_mlir")
                self.vae_encode = self.sd_model.vae_encode()

    def unload_vae_encode(self):
        del self.vae_encode
        self.vae_encode = None

    def prepare_latents(
        self,
        batch_size,
        height,
        width,
        generator,
        num_inference_steps,
        dtype,
    ):
        latents = torch.randn(
            (
                batch_size,
                4,
                height // 8,
                width // 8,
            ),
            generator=generator,
            dtype=torch.float32,
        ).to(dtype)

        self.scheduler.set_timesteps(num_inference_steps)
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    def get_crop_region(self, mask, pad=0):
        h, w = mask.shape

        crop_left = 0
        for i in range(w):
            if not (mask[:, i] == 0).all():
                break
            crop_left += 1

        crop_right = 0
        for i in reversed(range(w)):
            if not (mask[:, i] == 0).all():
                break
            crop_right += 1

        crop_top = 0
        for i in range(h):
            if not (mask[i] == 0).all():
                break
            crop_top += 1

        crop_bottom = 0
        for i in reversed(range(h)):
            if not (mask[i] == 0).all():
                break
            crop_bottom += 1

        return (
            int(max(crop_left - pad, 0)),
            int(max(crop_top - pad, 0)),
            int(min(w - crop_right + pad, w)),
            int(min(h - crop_bottom + pad, h)),
        )

    def expand_crop_region(
        self,
        crop_region,
        processing_width,
        processing_height,
        image_width,
        image_height,
    ):
        x1, y1, x2, y2 = crop_region

        ratio_crop_region = (x2 - x1) / (y2 - y1)
        ratio_processing = processing_width / processing_height

        if ratio_crop_region > ratio_processing:
            desired_height = (x2 - x1) / ratio_processing
            desired_height_diff = int(desired_height - (y2 - y1))
            y1 -= desired_height_diff // 2
            y2 += desired_height_diff - desired_height_diff // 2
            if y2 >= image_height:
                diff = y2 - image_height
                y2 -= diff
                y1 -= diff
            if y1 < 0:
                y2 -= y1
                y1 -= y1
            if y2 >= image_height:
                y2 = image_height
        else:
            desired_width = (y2 - y1) * ratio_processing
            desired_width_diff = int(desired_width - (x2 - x1))
            x1 -= desired_width_diff // 2
            x2 += desired_width_diff - desired_width_diff // 2
            if x2 >= image_width:
                diff = x2 - image_width
                x2 -= diff
                x1 -= diff
            if x1 < 0:
                x2 -= x1
                x1 -= x1
            if x2 >= image_width:
                x2 = image_width

        return x1, y1, x2, y2

    def resize_image(self, resize_mode, im, width, height):
        """
        resize_mode:
            0: Resize the image to fill the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, cropping the excess.
            1: Resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, filling empty with data from image.
        """

        if resize_mode == 0:
            ratio = width / height
            src_ratio = im.width / im.height

            src_w = (
                width if ratio > src_ratio else im.width * height // im.height
            )
            src_h = (
                height if ratio <= src_ratio else im.height * width // im.width
            )

            resized = im.resize((src_w, src_h), resample=Image.LANCZOS)
            res = Image.new("RGB", (width, height))
            res.paste(
                resized,
                box=(width // 2 - src_w // 2, height // 2 - src_h // 2),
            )

        else:
            ratio = width / height
            src_ratio = im.width / im.height

            src_w = (
                width if ratio < src_ratio else im.width * height // im.height
            )
            src_h = (
                height if ratio >= src_ratio else im.height * width // im.width
            )

            resized = im.resize((src_w, src_h), resample=Image.LANCZOS)
            res = Image.new("RGB", (width, height))
            res.paste(
                resized,
                box=(width // 2 - src_w // 2, height // 2 - src_h // 2),
            )

            if ratio < src_ratio:
                fill_height = height // 2 - src_h // 2
                res.paste(
                    resized.resize((width, fill_height), box=(0, 0, width, 0)),
                    box=(0, 0),
                )
                res.paste(
                    resized.resize(
                        (width, fill_height),
                        box=(0, resized.height, width, resized.height),
                    ),
                    box=(0, fill_height + src_h),
                )
            elif ratio > src_ratio:
                fill_width = width // 2 - src_w // 2
                res.paste(
                    resized.resize(
                        (fill_width, height), box=(0, 0, 0, height)
                    ),
                    box=(0, 0),
                )
                res.paste(
                    resized.resize(
                        (fill_width, height),
                        box=(resized.width, 0, resized.width, height),
                    ),
                    box=(fill_width + src_w, 0),
                )

        return res

    def prepare_mask_and_masked_image(
        self,
        image,
        mask,
        height,
        width,
        inpaint_full_res,
        inpaint_full_res_padding,
    ):
        # preprocess image
        image = image.resize((width, height))
        mask = mask.resize((width, height))

        paste_to = ()
        overlay_image = None
        if inpaint_full_res:
            # prepare overlay image
            overlay_image = Image.new("RGB", (image.width, image.height))
            overlay_image.paste(
                image.convert("RGB"),
                mask=ImageOps.invert(mask.convert("L")),
            )

            # prepare mask
            mask = mask.convert("L")
            crop_region = self.get_crop_region(
                np.array(mask), inpaint_full_res_padding
            )
            crop_region = self.expand_crop_region(
                crop_region, width, height, mask.width, mask.height
            )
            x1, y1, x2, y2 = crop_region
            mask = mask.crop(crop_region)
            mask = self.resize_image(1, mask, width, height)
            paste_to = (x1, y1, x2 - x1, y2 - y1)

            # prepare image
            image = image.crop(crop_region)
            image = self.resize_image(1, image, width, height)

        if isinstance(image, (Image.Image, np.ndarray)):
            image = [image]

        if isinstance(image, list) and isinstance(image[0], Image.Image):
            image = [np.array(i.convert("RGB"))[None, :] for i in image]
            image = np.concatenate(image, axis=0)
        elif isinstance(image, list) and isinstance(image[0], np.ndarray):
            image = np.concatenate([i[None, :] for i in image], axis=0)

        image = image.transpose(0, 3, 1, 2)
        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0

        # preprocess mask
        if isinstance(mask, (Image.Image, np.ndarray)):
            mask = [mask]

        if isinstance(mask, list) and isinstance(mask[0], Image.Image):
            mask = np.concatenate(
                [np.array(m.convert("L"))[None, None, :] for m in mask], axis=0
            )
            mask = mask.astype(np.float32) / 255.0
        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):
            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)

        mask[mask < 0.5] = 0
        mask[mask >= 0.5] = 1
        mask = torch.from_numpy(mask)

        masked_image = image * (mask < 0.5)

        return mask, masked_image, paste_to, overlay_image

    def prepare_mask_latents(
        self,
        mask,
        masked_image,
        batch_size,
        height,
        width,
        dtype,
    ):
        mask = torch.nn.functional.interpolate(
            mask, size=(height // 8, width // 8)
        )
        mask = mask.to(dtype)

        self.load_vae_encode()
        masked_image = masked_image.to(dtype)
        masked_image_latents = self.vae_encode("forward", (masked_image,))
        masked_image_latents = torch.from_numpy(masked_image_latents)
        if self.ondemand:
            self.unload_vae_encode()

        # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method
        if mask.shape[0] < batch_size:
            if not batch_size % mask.shape[0] == 0:
                raise ValueError(
                    "The passed mask and the required batch size don't match. Masks are supposed to be duplicated to"
                    f" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number"
                    " of masks that you pass is divisible by the total requested batch size."
                )
            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)
        if masked_image_latents.shape[0] < batch_size:
            if not batch_size % masked_image_latents.shape[0] == 0:
                raise ValueError(
                    "The passed images and the required batch size don't match. Images are supposed to be duplicated"
                    f" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed."
                    " Make sure the number of images that you pass is divisible by the total requested batch size."
                )
            masked_image_latents = masked_image_latents.repeat(
                batch_size // masked_image_latents.shape[0], 1, 1, 1
            )
        return mask, masked_image_latents

    def apply_overlay(self, image, paste_loc, overlay):
        x, y, w, h = paste_loc
        image = self.resize_image(0, image, w, h)
        overlay.paste(image, (x, y))

        return overlay

    def generate_images(
        self,
        prompts,
        neg_prompts,
        image,
        mask_image,
        batch_size,
        height,
        width,
        inpaint_full_res,
        inpaint_full_res_padding,
        num_inference_steps,
        guidance_scale,
        seed,
        max_length,
        dtype,
        use_base_vae,
        cpu_scheduling,
        max_embeddings_multiples,
    ):
        # prompts and negative prompts must be a list.
        if isinstance(prompts, str):
            prompts = [prompts]

        if isinstance(neg_prompts, str):
            neg_prompts = [neg_prompts]

        prompts = prompts * batch_size
        neg_prompts = neg_prompts * batch_size

        # seed generator to create the inital latent noise. Also handle out of range seeds.
        uint32_info = np.iinfo(np.uint32)
        uint32_min, uint32_max = uint32_info.min, uint32_info.max
        if seed < uint32_min or seed >= uint32_max:
            seed = randint(uint32_min, uint32_max)
        generator = torch.manual_seed(seed)

        # Get initial latents
        init_latents = self.prepare_latents(
            batch_size=batch_size,
            height=height,
            width=width,
            generator=generator,
            num_inference_steps=num_inference_steps,
            dtype=dtype,
        )

        # Get text embeddings with weight emphasis from prompts
        text_embeddings = self.encode_prompts_weight(
            prompts,
            neg_prompts,
            max_length,
            max_embeddings_multiples=max_embeddings_multiples,
        )

        # guidance scale as a float32 tensor.
        guidance_scale = torch.tensor(guidance_scale).to(torch.float32)

        # Preprocess mask and image
        (
            mask,
            masked_image,
            paste_to,
            overlay_image,
        ) = self.prepare_mask_and_masked_image(
            image,
            mask_image,
            height,
            width,
            inpaint_full_res,
            inpaint_full_res_padding,
        )

        # Prepare mask latent variables
        mask, masked_image_latents = self.prepare_mask_latents(
            mask=mask,
            masked_image=masked_image,
            batch_size=batch_size,
            height=height,
            width=width,
            dtype=dtype,
        )

        # Get Image latents
        latents = self.produce_img_latents(
            latents=init_latents,
            text_embeddings=text_embeddings,
            guidance_scale=guidance_scale,
            total_timesteps=self.scheduler.timesteps,
            dtype=dtype,
            cpu_scheduling=cpu_scheduling,
            mask=mask,
            masked_image_latents=masked_image_latents,
        )

        # Img latents -> PIL images
        all_imgs = []
        self.load_vae()
        for i in tqdm(range(0, latents.shape[0], batch_size)):
            imgs = self.decode_latents(
                latents=latents[i : i + batch_size],
                use_base_vae=use_base_vae,
                cpu_scheduling=cpu_scheduling,
            )
            all_imgs.extend(imgs)
        if self.ondemand:
            self.unload_vae()

        if inpaint_full_res:
            output_image = self.apply_overlay(
                all_imgs[0], paste_to, overlay_image
            )
            return [output_image]

        return all_imgs

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_outpaint.py`:

```py
import torch
from tqdm.auto import tqdm
import numpy as np
from random import randint
from PIL import Image, ImageDraw, ImageFilter
from transformers import CLIPTokenizer
from typing import Union
from shark.shark_inference import SharkInference
from diffusers import (
    DDIMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
    DDPMScheduler,
    KDPM2DiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    StableDiffusionPipeline,
)
import math
from apps.stable_diffusion.src.models import (
    SharkifyStableDiffusionModel,
    get_vae_encode,
)


class OutpaintPipeline(StableDiffusionPipeline):
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
            DDPMScheduler,
            KDPM2DiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
    ):
        super().__init__(scheduler, sd_model, import_mlir, use_lora, ondemand)
        self.vae_encode = None

    def load_vae_encode(self):
        if self.vae_encode is not None:
            return

        if self.import_mlir or self.use_lora:
            self.vae_encode = self.sd_model.vae_encode()
        else:
            try:
                self.vae_encode = get_vae_encode()
            except:
                print("download pipeline failed, falling back to import_mlir")
                self.vae_encode = self.sd_model.vae_encode()

    def unload_vae_encode(self):
        del self.vae_encode
        self.vae_encode = None

    def prepare_latents(
        self,
        batch_size,
        height,
        width,
        generator,
        num_inference_steps,
        dtype,
    ):
        latents = torch.randn(
            (
                batch_size,
                4,
                height // 8,
                width // 8,
            ),
            generator=generator,
            dtype=torch.float32,
        ).to(dtype)

        self.scheduler.set_timesteps(num_inference_steps)
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    def prepare_mask_and_masked_image(
        self, image, mask, mask_blur, width, height
    ):
        if mask_blur > 0:
            mask = mask.filter(ImageFilter.GaussianBlur(mask_blur))
        image = image.resize((width, height))
        mask = mask.resize((width, height))

        # preprocess image
        if isinstance(image, (Image.Image, np.ndarray)):
            image = [image]

        if isinstance(image, list) and isinstance(image[0], Image.Image):
            image = [np.array(i.convert("RGB"))[None, :] for i in image]
            image = np.concatenate(image, axis=0)
        elif isinstance(image, list) and isinstance(image[0], np.ndarray):
            image = np.concatenate([i[None, :] for i in image], axis=0)

        image = image.transpose(0, 3, 1, 2)
        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0

        # preprocess mask
        if isinstance(mask, (Image.Image, np.ndarray)):
            mask = [mask]

        if isinstance(mask, list) and isinstance(mask[0], Image.Image):
            mask = np.concatenate(
                [np.array(m.convert("L"))[None, None, :] for m in mask], axis=0
            )
            mask = mask.astype(np.float32) / 255.0
        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):
            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)

        mask[mask < 0.5] = 0
        mask[mask >= 0.5] = 1
        mask = torch.from_numpy(mask)

        masked_image = image * (mask < 0.5)

        return mask, masked_image

    def prepare_mask_latents(
        self,
        mask,
        masked_image,
        batch_size,
        height,
        width,
        dtype,
    ):
        mask = torch.nn.functional.interpolate(
            mask, size=(height // 8, width // 8)
        )
        mask = mask.to(dtype)

        self.load_vae_encode()
        masked_image = masked_image.to(dtype)
        masked_image_latents = self.vae_encode("forward", (masked_image,))
        masked_image_latents = torch.from_numpy(masked_image_latents)
        if self.ondemand:
            self.unload_vae_encode()

        # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method
        if mask.shape[0] < batch_size:
            if not batch_size % mask.shape[0] == 0:
                raise ValueError(
                    "The passed mask and the required batch size don't match. Masks are supposed to be duplicated to"
                    f" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number"
                    " of masks that you pass is divisible by the total requested batch size."
                )
            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)
        if masked_image_latents.shape[0] < batch_size:
            if not batch_size % masked_image_latents.shape[0] == 0:
                raise ValueError(
                    "The passed images and the required batch size don't match. Images are supposed to be duplicated"
                    f" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed."
                    " Make sure the number of images that you pass is divisible by the total requested batch size."
                )
            masked_image_latents = masked_image_latents.repeat(
                batch_size // masked_image_latents.shape[0], 1, 1, 1
            )
        return mask, masked_image_latents

    def get_matched_noise(
        self, _np_src_image, np_mask_rgb, noise_q=1, color_variation=0.05
    ):
        # helper fft routines that keep ortho normalization and auto-shift before and after fft
        def _fft2(data):
            if data.ndim > 2:  # has channels
                out_fft = np.zeros(
                    (data.shape[0], data.shape[1], data.shape[2]),
                    dtype=np.complex128,
                )
                for c in range(data.shape[2]):
                    c_data = data[:, :, c]
                    out_fft[:, :, c] = np.fft.fft2(
                        np.fft.fftshift(c_data), norm="ortho"
                    )
                    out_fft[:, :, c] = np.fft.ifftshift(out_fft[:, :, c])
            else:  # one channel
                out_fft = np.zeros(
                    (data.shape[0], data.shape[1]), dtype=np.complex128
                )
                out_fft[:, :] = np.fft.fft2(
                    np.fft.fftshift(data), norm="ortho"
                )
                out_fft[:, :] = np.fft.ifftshift(out_fft[:, :])

            return out_fft

        def _ifft2(data):
            if data.ndim > 2:  # has channels
                out_ifft = np.zeros(
                    (data.shape[0], data.shape[1], data.shape[2]),
                    dtype=np.complex128,
                )
                for c in range(data.shape[2]):
                    c_data = data[:, :, c]
                    out_ifft[:, :, c] = np.fft.ifft2(
                        np.fft.fftshift(c_data), norm="ortho"
                    )
                    out_ifft[:, :, c] = np.fft.ifftshift(out_ifft[:, :, c])
            else:  # one channel
                out_ifft = np.zeros(
                    (data.shape[0], data.shape[1]), dtype=np.complex128
                )
                out_ifft[:, :] = np.fft.ifft2(
                    np.fft.fftshift(data), norm="ortho"
                )
                out_ifft[:, :] = np.fft.ifftshift(out_ifft[:, :])

            return out_ifft

        def _get_gaussian_window(width, height, std=3.14, mode=0):
            window_scale_x = float(width / min(width, height))
            window_scale_y = float(height / min(width, height))

            window = np.zeros((width, height))
            x = (np.arange(width) / width * 2.0 - 1.0) * window_scale_x
            for y in range(height):
                fy = (y / height * 2.0 - 1.0) * window_scale_y
                if mode == 0:
                    window[:, y] = np.exp(-(x**2 + fy**2) * std)
                else:
                    window[:, y] = (
                        1 / ((x**2 + 1.0) * (fy**2 + 1.0))
                    ) ** (std / 3.14)

            return window

        def _get_masked_window_rgb(np_mask_grey, hardness=1.0):
            np_mask_rgb = np.zeros(
                (np_mask_grey.shape[0], np_mask_grey.shape[1], 3)
            )
            if hardness != 1.0:
                hardened = np_mask_grey[:] ** hardness
            else:
                hardened = np_mask_grey[:]
            for c in range(3):
                np_mask_rgb[:, :, c] = hardened[:]
            return np_mask_rgb

        def _match_cumulative_cdf(source, template):
            src_values, src_unique_indices, src_counts = np.unique(
                source.ravel(), return_inverse=True, return_counts=True
            )
            tmpl_values, tmpl_counts = np.unique(
                template.ravel(), return_counts=True
            )

            # calculate normalized quantiles for each array
            src_quantiles = np.cumsum(src_counts) / source.size
            tmpl_quantiles = np.cumsum(tmpl_counts) / template.size

            interp_a_values = np.interp(
                src_quantiles, tmpl_quantiles, tmpl_values
            )
            return interp_a_values[src_unique_indices].reshape(source.shape)

        def _match_histograms(image, reference):
            if image.ndim != reference.ndim:
                raise ValueError(
                    "Image and reference must have the same number of channels."
                )

            if image.shape[-1] != reference.shape[-1]:
                raise ValueError(
                    "Number of channels in the input image and reference image must match!"
                )

            matched = np.empty(image.shape, dtype=image.dtype)
            for channel in range(image.shape[-1]):
                matched_channel = _match_cumulative_cdf(
                    image[..., channel], reference[..., channel]
                )
                matched[..., channel] = matched_channel

            matched = matched.astype(np.float64, copy=False)
            return matched

        width = _np_src_image.shape[0]
        height = _np_src_image.shape[1]
        num_channels = _np_src_image.shape[2]

        np_src_image = _np_src_image[:] * (1.0 - np_mask_rgb)
        np_mask_grey = np.sum(np_mask_rgb, axis=2) / 3.0
        img_mask = np_mask_grey > 1e-6
        ref_mask = np_mask_grey < 1e-3

        # rather than leave the masked area black, we get better results from fft by filling the average unmasked color
        windowed_image = _np_src_image * (
            1.0 - _get_masked_window_rgb(np_mask_grey)
        )
        windowed_image /= np.max(windowed_image)
        windowed_image += np.average(_np_src_image) * np_mask_rgb

        src_fft = _fft2(
            windowed_image
        )  # get feature statistics from masked src img
        src_dist = np.absolute(src_fft)
        src_phase = src_fft / src_dist

        # create a generator with a static seed to make outpainting deterministic / only follow global seed
        rng = np.random.default_rng(0)

        noise_window = _get_gaussian_window(
            width, height, mode=1
        )  # start with simple gaussian noise
        noise_rgb = rng.random((width, height, num_channels))
        noise_grey = np.sum(noise_rgb, axis=2) / 3.0
        # the colorfulness of the starting noise is blended to greyscale with a parameter
        noise_rgb *= color_variation
        for c in range(num_channels):
            noise_rgb[:, :, c] += (1.0 - color_variation) * noise_grey

        noise_fft = _fft2(noise_rgb)
        for c in range(num_channels):
            noise_fft[:, :, c] *= noise_window
        noise_rgb = np.real(_ifft2(noise_fft))
        shaped_noise_fft = _fft2(noise_rgb)
        shaped_noise_fft[:, :, :] = (
            np.absolute(shaped_noise_fft[:, :, :]) ** 2
            * (src_dist**noise_q)
            * src_phase
        )  # perform the actual shaping

        # color_variation
        brightness_variation = 0.0
        contrast_adjusted_np_src = (
            _np_src_image[:] * (brightness_variation + 1.0)
            - brightness_variation * 2.0
        )

        shaped_noise = np.real(_ifft2(shaped_noise_fft))
        shaped_noise -= np.min(shaped_noise)
        shaped_noise /= np.max(shaped_noise)
        shaped_noise[img_mask, :] = _match_histograms(
            shaped_noise[img_mask, :] ** 1.0,
            contrast_adjusted_np_src[ref_mask, :],
        )
        shaped_noise = (
            _np_src_image[:] * (1.0 - np_mask_rgb) + shaped_noise * np_mask_rgb
        )

        matched_noise = shaped_noise[:]

        return np.clip(matched_noise, 0.0, 1.0)

    def generate_images(
        self,
        prompts,
        neg_prompts,
        image,
        pixels,
        mask_blur,
        is_left,
        is_right,
        is_top,
        is_bottom,
        noise_q,
        color_variation,
        batch_size,
        height,
        width,
        num_inference_steps,
        guidance_scale,
        seed,
        max_length,
        dtype,
        use_base_vae,
        cpu_scheduling,
        max_embeddings_multiples,
    ):
        # prompts and negative prompts must be a list.
        if isinstance(prompts, str):
            prompts = [prompts]

        if isinstance(neg_prompts, str):
            neg_prompts = [neg_prompts]

        prompts = prompts * batch_size
        neg_prompts = neg_prompts * batch_size

        # seed generator to create the inital latent noise. Also handle out of range seeds.
        uint32_info = np.iinfo(np.uint32)
        uint32_min, uint32_max = uint32_info.min, uint32_info.max
        if seed < uint32_min or seed >= uint32_max:
            seed = randint(uint32_min, uint32_max)
        generator = torch.manual_seed(seed)

        # Get initial latents
        init_latents = self.prepare_latents(
            batch_size=batch_size,
            height=height,
            width=width,
            generator=generator,
            num_inference_steps=num_inference_steps,
            dtype=dtype,
        )

        # Get text embeddings with weight emphasis from prompts
        text_embeddings = self.encode_prompts_weight(
            prompts,
            neg_prompts,
            max_length,
            max_embeddings_multiples=max_embeddings_multiples,
        )

        # guidance scale as a float32 tensor.
        guidance_scale = torch.tensor(guidance_scale).to(torch.float32)

        process_width = width
        process_height = height
        left = pixels if is_left else 0
        right = pixels if is_right else 0
        up = pixels if is_top else 0
        down = pixels if is_bottom else 0
        target_w = math.ceil((image.width + left + right) / 64) * 64
        target_h = math.ceil((image.height + up + down) / 64) * 64

        if left > 0:
            left = left * (target_w - image.width) // (left + right)
        if right > 0:
            right = target_w - image.width - left
        if up > 0:
            up = up * (target_h - image.height) // (up + down)
        if down > 0:
            down = target_h - image.height - up

        def expand(
            init_img,
            expand_pixels,
            is_left=False,
            is_right=False,
            is_top=False,
            is_bottom=False,
        ):
            is_horiz = is_left or is_right
            is_vert = is_top or is_bottom
            pixels_horiz = expand_pixels if is_horiz else 0
            pixels_vert = expand_pixels if is_vert else 0

            res_w = init_img.width + pixels_horiz
            res_h = init_img.height + pixels_vert
            process_res_w = math.ceil(res_w / 64) * 64
            process_res_h = math.ceil(res_h / 64) * 64

            img = Image.new("RGB", (process_res_w, process_res_h))
            img.paste(
                init_img,
                (pixels_horiz if is_left else 0, pixels_vert if is_top else 0),
            )

            msk = Image.new("RGB", (process_res_w, process_res_h), "white")
            draw = ImageDraw.Draw(msk)
            draw.rectangle(
                (
                    expand_pixels + mask_blur if is_left else 0,
                    expand_pixels + mask_blur if is_top else 0,
                    msk.width - expand_pixels - mask_blur
                    if is_right
                    else res_w,
                    msk.height - expand_pixels - mask_blur
                    if is_bottom
                    else res_h,
                ),
                fill="black",
            )

            np_image = (np.asarray(img) / 255.0).astype(np.float64)
            np_mask = (np.asarray(msk) / 255.0).astype(np.float64)
            noised = self.get_matched_noise(
                np_image, np_mask, noise_q, color_variation
            )
            output_image = Image.fromarray(
                np.clip(noised * 255.0, 0.0, 255.0).astype(np.uint8),
                mode="RGB",
            )

            target_width = (
                min(width, init_img.width + pixels_horiz)
                if is_horiz
                else img.width
            )
            target_height = (
                min(height, init_img.height + pixels_vert)
                if is_vert
                else img.height
            )
            crop_region = (
                0 if is_left else output_image.width - target_width,
                0 if is_top else output_image.height - target_height,
                target_width if is_left else output_image.width,
                target_height if is_top else output_image.height,
            )
            mask_to_process = msk.crop(crop_region)
            image_to_process = output_image.crop(crop_region)

            # Preprocess mask and image
            mask, masked_image = self.prepare_mask_and_masked_image(
                image_to_process, mask_to_process, mask_blur, width, height
            )

            # Prepare mask latent variables
            mask, masked_image_latents = self.prepare_mask_latents(
                mask=mask,
                masked_image=masked_image,
                batch_size=batch_size,
                height=height,
                width=width,
                dtype=dtype,
            )

            # Get Image latents
            latents = self.produce_img_latents(
                latents=init_latents,
                text_embeddings=text_embeddings,
                guidance_scale=guidance_scale,
                total_timesteps=self.scheduler.timesteps,
                dtype=dtype,
                cpu_scheduling=cpu_scheduling,
                mask=mask,
                masked_image_latents=masked_image_latents,
            )

            # Img latents -> PIL images
            all_imgs = []
            self.load_vae()
            for i in tqdm(range(0, latents.shape[0], batch_size)):
                imgs = self.decode_latents(
                    latents=latents[i : i + batch_size],
                    use_base_vae=use_base_vae,
                    cpu_scheduling=cpu_scheduling,
                )
                all_imgs.extend(imgs)

            res_img = all_imgs[0].resize(
                (image_to_process.width, image_to_process.height)
            )
            output_image.paste(
                res_img,
                (
                    0 if is_left else output_image.width - res_img.width,
                    0 if is_top else output_image.height - res_img.height,
                ),
            )
            output_image = output_image.crop((0, 0, res_w, res_h))

            return output_image

        img = image.resize((width, height))
        if left > 0:
            img = expand(img, left, is_left=True)
        if right > 0:
            img = expand(img, right, is_right=True)
        if up > 0:
            img = expand(img, up, is_top=True)
        if down > 0:
            img = expand(img, down, is_bottom=True)

        return [img]

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_stencil.py`:

```py
import torch
import time
import numpy as np
from tqdm.auto import tqdm
from random import randint
from PIL import Image
from transformers import CLIPTokenizer
from typing import Union
from shark.shark_inference import SharkInference
from diffusers import (
    DDIMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
    DDPMScheduler,
    KDPM2DiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    StableDiffusionPipeline,
)
from apps.stable_diffusion.src.utils import controlnet_hint_conversion
from apps.stable_diffusion.src.utils import (
    start_profiling,
    end_profiling,
)
from apps.stable_diffusion.src.models import SharkifyStableDiffusionModel


class StencilPipeline(StableDiffusionPipeline):
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
            DDPMScheduler,
            KDPM2DiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
        controlnet_names: list[str],
    ):
        super().__init__(scheduler, sd_model, import_mlir, use_lora, ondemand)
        self.controlnet = [None] * len(controlnet_names)
        self.controlnet_512 = [None] * len(controlnet_names)
        self.controlnet_id = [str] * len(controlnet_names)
        self.controlnet_512_id = [str] * len(controlnet_names)
        self.controlnet_names = controlnet_names

    def load_controlnet(self, index, model_name):
        if model_name is None:
            return
        if (
            self.controlnet[index] is not None
            and self.controlnet_id[index] is not None
            and self.controlnet_id[index] == model_name
        ):
            return
        self.controlnet_id[index] = model_name
        self.controlnet[index] = self.sd_model.controlnet(model_name)

    def unload_controlnet(self, index):
        del self.controlnet[index]
        self.controlnet_id[index] = None
        self.controlnet[index] = None

    def load_controlnet_512(self, index, model_name):
        if (
            self.controlnet_512[index] is not None
            and self.controlnet_512_id[index] == model_name
        ):
            return
        self.controlnet_512_id[index] = model_name
        self.controlnet_512[index] = self.sd_model.controlnet(
            model_name, use_large=True
        )

    def unload_controlnet_512(self, index):
        del self.controlnet_512[index]
        self.controlnet_512_id[index] = None
        self.controlnet_512[index] = None

    def prepare_latents(
        self,
        batch_size,
        height,
        width,
        generator,
        num_inference_steps,
        dtype,
    ):
        latents = torch.randn(
            (
                batch_size,
                4,
                height // 8,
                width // 8,
            ),
            generator=generator,
            dtype=torch.float32,
        ).to(dtype)

        self.scheduler.set_timesteps(num_inference_steps)
        self.scheduler.is_scale_input_called = True
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    def produce_stencil_latents(
        self,
        latents,
        text_embeddings,
        guidance_scale,
        total_timesteps,
        dtype,
        cpu_scheduling,
        stencil_hints=[None],
        controlnet_conditioning_scale: float = 1.0,
        control_mode="Balanced",  # Prompt, Balanced, or Controlnet
        mask=None,
        masked_image_latents=None,
        return_all_latents=False,
    ):
        step_time_sum = 0
        latent_history = [latents]
        text_embeddings = torch.from_numpy(text_embeddings).to(dtype)
        text_embeddings_numpy = text_embeddings.detach().numpy()
        assert control_mode in ["Prompt", "Balanced", "Controlnet"]
        if text_embeddings.shape[1] <= self.model_max_length:
            self.load_unet()
        else:
            self.load_unet_512()

        for i, name in enumerate(self.controlnet_names):
            if text_embeddings.shape[1] <= self.model_max_length:
                self.load_controlnet(i, name)
            else:
                self.load_controlnet_512(i, name)

        for i, t in tqdm(enumerate(total_timesteps)):
            step_start_time = time.time()
            timestep = torch.tensor([t]).to(dtype)
            latent_model_input = self.scheduler.scale_model_input(latents, t)
            if mask is not None and masked_image_latents is not None:
                latent_model_input = torch.cat(
                    [
                        torch.from_numpy(np.asarray(latent_model_input)),
                        mask,
                        masked_image_latents,
                    ],
                    dim=1,
                ).to(dtype)
            if cpu_scheduling:
                latent_model_input = latent_model_input.detach().numpy()

            if not torch.is_tensor(latent_model_input):
                latent_model_input_1 = torch.from_numpy(
                    np.asarray(latent_model_input)
                ).to(dtype)
            else:
                latent_model_input_1 = latent_model_input

            # Multicontrolnet
            width = latent_model_input_1.shape[2]
            height = latent_model_input_1.shape[3]
            dtype = latent_model_input_1.dtype
            control_acc = (
                [torch.zeros((2, 320, height, width), dtype=dtype)] * 3
                + [
                    torch.zeros(
                        (2, 320, int(height / 2), int(width / 2)), dtype=dtype
                    )
                ]
                + [
                    torch.zeros(
                        (2, 640, int(height / 2), int(width / 2)), dtype=dtype
                    )
                ]
                * 2
                + [
                    torch.zeros(
                        (2, 640, int(height / 4), int(width / 4)), dtype=dtype
                    )
                ]
                + [
                    torch.zeros(
                        (2, 1280, int(height / 4), int(width / 4)), dtype=dtype
                    )
                ]
                * 2
                + [
                    torch.zeros(
                        (2, 1280, int(height / 8), int(width / 8)), dtype=dtype
                    )
                ]
                * 4
            )
            for i, controlnet_hint in enumerate(stencil_hints):
                if controlnet_hint is None:
                    continue
                if text_embeddings.shape[1] <= self.model_max_length:
                    control = self.controlnet[i](
                        "forward",
                        (
                            latent_model_input_1,
                            timestep,
                            text_embeddings,
                            controlnet_hint,
                            *control_acc,
                        ),
                        send_to_host=False,
                    )
                else:
                    control = self.controlnet_512[i](
                        "forward",
                        (
                            latent_model_input_1,
                            timestep,
                            text_embeddings,
                            controlnet_hint,
                            *control_acc,
                        ),
                        send_to_host=False,
                    )
                control_acc = control[13:]
                control = control[:13]

            timestep = timestep.detach().numpy()
            # Profiling Unet.
            profile_device = start_profiling(file_path="unet.rdc")
            # TODO: Pass `control` as it is to Unet. Same as TODO mentioned in model_wrappers.py.

            dtype = latents.dtype
            if control_mode == "Balanced":
                control_scale = [
                    torch.tensor(1.0, dtype=dtype) for _ in range(len(control))
                ]
            elif control_mode == "Prompt":
                control_scale = [
                    torch.tensor(0.825**x, dtype=dtype)
                    for x in range(len(control))
                ]
            elif control_mode == "Controlnet":
                control_scale = [
                    torch.tensor(float(guidance_scale), dtype=dtype)
                    for _ in range(len(control))
                ]

            if text_embeddings.shape[1] <= self.model_max_length:
                noise_pred = self.unet(
                    "forward",
                    (
                        latent_model_input,
                        timestep,
                        text_embeddings_numpy,
                        guidance_scale,
                        control[0],
                        control[1],
                        control[2],
                        control[3],
                        control[4],
                        control[5],
                        control[6],
                        control[7],
                        control[8],
                        control[9],
                        control[10],
                        control[11],
                        control[12],
                        control_scale[0],
                        control_scale[1],
                        control_scale[2],
                        control_scale[3],
                        control_scale[4],
                        control_scale[5],
                        control_scale[6],
                        control_scale[7],
                        control_scale[8],
                        control_scale[9],
                        control_scale[10],
                        control_scale[11],
                        control_scale[12],
                    ),
                    send_to_host=False,
                )
            else:
                print(self.unet_512)
                noise_pred = self.unet_512(
                    "forward",
                    (
                        latent_model_input,
                        timestep,
                        text_embeddings_numpy,
                        guidance_scale,
                        control[0],
                        control[1],
                        control[2],
                        control[3],
                        control[4],
                        control[5],
                        control[6],
                        control[7],
                        control[8],
                        control[9],
                        control[10],
                        control[11],
                        control[12],
                        control_scale[0],
                        control_scale[1],
                        control_scale[2],
                        control_scale[3],
                        control_scale[4],
                        control_scale[5],
                        control_scale[6],
                        control_scale[7],
                        control_scale[8],
                        control_scale[9],
                        control_scale[10],
                        control_scale[11],
                        control_scale[12],
                    ),
                    send_to_host=False,
                )
            end_profiling(profile_device)

            if cpu_scheduling:
                noise_pred = torch.from_numpy(noise_pred.to_host())
                latents = self.scheduler.step(
                    noise_pred, t, latents
                ).prev_sample
            else:
                latents = self.scheduler.step(noise_pred, t, latents)

            latent_history.append(latents)
            step_time = (time.time() - step_start_time) * 1000
            #  self.log += (
            #      f"\nstep = {i} | timestep = {t} | time = {step_time:.2f}ms"
            #  )
            step_time_sum += step_time

        if self.ondemand:
            self.unload_unet()
            self.unload_unet_512()
            for i in range(len(self.controlnet_names)):
                self.unload_controlnet(i)
                self.unload_controlnet_512(i)
        avg_step_time = step_time_sum / len(total_timesteps)
        self.log += f"\nAverage step time: {avg_step_time}ms/it"

        if not return_all_latents:
            return latents
        all_latents = torch.cat(latent_history, dim=0)
        return all_latents

    def generate_images(
        self,
        prompts,
        neg_prompts,
        image,
        batch_size,
        height,
        width,
        num_inference_steps,
        strength,
        guidance_scale,
        seed,
        max_length,
        dtype,
        use_base_vae,
        cpu_scheduling,
        max_embeddings_multiples,
        stencils,
        stencil_images,
        resample_type,
        control_mode,
    ):
        # Control Embedding check & conversion
        # TODO: 1. Change `num_images_per_prompt`.
        # controlnet_hint = controlnet_hint_conversion(
        #     image, use_stencil, height, width, dtype, num_images_per_prompt=1
        # )
        stencil_hints = []
        for i, stencil in enumerate(stencils):
            image = stencil_images[i]
            stencil_hints.append(
                controlnet_hint_conversion(
                    image,
                    stencil,
                    height,
                    width,
                    dtype,
                    num_images_per_prompt=1,
                )
            )

        # prompts and negative prompts must be a list.
        if isinstance(prompts, str):
            prompts = [prompts]

        if isinstance(neg_prompts, str):
            neg_prompts = [neg_prompts]

        prompts = prompts * batch_size
        neg_prompts = neg_prompts * batch_size

        # seed generator to create the inital latent noise. Also handle out of range seeds.
        uint32_info = np.iinfo(np.uint32)
        uint32_min, uint32_max = uint32_info.min, uint32_info.max
        if seed < uint32_min or seed >= uint32_max:
            seed = randint(uint32_min, uint32_max)
        generator = torch.manual_seed(seed)

        # Get text embeddings with weight emphasis from prompts
        text_embeddings = self.encode_prompts_weight(
            prompts,
            neg_prompts,
            max_length,
            max_embeddings_multiples=max_embeddings_multiples,
        )

        # guidance scale as a float32 tensor.
        guidance_scale = torch.tensor(guidance_scale).to(torch.float32)

        # Prepare initial latent.
        init_latents = self.prepare_latents(
            batch_size=batch_size,
            height=height,
            width=width,
            generator=generator,
            num_inference_steps=num_inference_steps,
            dtype=dtype,
        )
        final_timesteps = self.scheduler.timesteps

        # Get Image latents
        latents = self.produce_stencil_latents(
            latents=init_latents,
            text_embeddings=text_embeddings,
            guidance_scale=guidance_scale,
            total_timesteps=final_timesteps,
            dtype=dtype,
            cpu_scheduling=cpu_scheduling,
            control_mode=control_mode,
            stencil_hints=stencil_hints,
        )

        # Img latents -> PIL images
        all_imgs = []
        self.load_vae()
        for i in tqdm(range(0, latents.shape[0], batch_size)):
            imgs = self.decode_latents(
                latents=latents[i : i + batch_size],
                use_base_vae=use_base_vae,
                cpu_scheduling=cpu_scheduling,
            )
            all_imgs.extend(imgs)
        if self.ondemand:
            self.unload_vae()

        return all_imgs

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_txt2img.py`:

```py
import torch
import numpy as np
from random import randint
from transformers import CLIPTokenizer
from typing import Union
from shark.shark_inference import SharkInference
from diffusers import (
    DDIMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    KDPM2DiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DDPMScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    StableDiffusionPipeline,
)
from apps.stable_diffusion.src.models import SharkifyStableDiffusionModel


class Text2ImagePipeline(StableDiffusionPipeline):
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            KDPM2DiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DDPMScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
    ):
        super().__init__(scheduler, sd_model, import_mlir, use_lora, ondemand)

    def prepare_latents(
        self,
        batch_size,
        height,
        width,
        generator,
        num_inference_steps,
        dtype,
    ):
        latents = torch.randn(
            (
                batch_size,
                4,
                height // 8,
                width // 8,
            ),
            generator=generator,
            dtype=torch.float32,
        ).to(dtype)

        self.scheduler.set_timesteps(num_inference_steps)
        self.scheduler.is_scale_input_called = True
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    def generate_images(
        self,
        prompts,
        neg_prompts,
        batch_size,
        height,
        width,
        num_inference_steps,
        guidance_scale,
        seed,
        max_length,
        dtype,
        use_base_vae,
        cpu_scheduling,
        max_embeddings_multiples,
    ):
        # prompts and negative prompts must be a list.
        if isinstance(prompts, str):
            prompts = [prompts]

        if isinstance(neg_prompts, str):
            neg_prompts = [neg_prompts]

        prompts = prompts * batch_size
        neg_prompts = neg_prompts * batch_size

        # seed generator to create the inital latent noise. Also handle out of range seeds.
        # TODO: Wouldn't it be preferable to just report an error instead of modifying the seed on the fly?
        uint32_info = np.iinfo(np.uint32)
        uint32_min, uint32_max = uint32_info.min, uint32_info.max
        if seed < uint32_min or seed >= uint32_max:
            seed = randint(uint32_min, uint32_max)
        generator = torch.manual_seed(seed)

        # Get initial latents
        init_latents = self.prepare_latents(
            batch_size=batch_size,
            height=height,
            width=width,
            generator=generator,
            num_inference_steps=num_inference_steps,
            dtype=dtype,
        )

        # Get text embeddings with weight emphasis from prompts
        text_embeddings = self.encode_prompts_weight(
            prompts,
            neg_prompts,
            max_length,
            max_embeddings_multiples=max_embeddings_multiples,
        )

        # guidance scale as a float32 tensor.
        guidance_scale = torch.tensor(guidance_scale).to(torch.float32)

        # Get Image latents
        latents = self.produce_img_latents(
            latents=init_latents,
            text_embeddings=text_embeddings,
            guidance_scale=guidance_scale,
            total_timesteps=self.scheduler.timesteps,
            dtype=dtype,
            cpu_scheduling=cpu_scheduling,
        )

        # Img latents -> PIL images
        all_imgs = []
        self.load_vae()
        for i in range(0, latents.shape[0], batch_size):
            imgs = self.decode_latents(
                latents=latents[i : i + batch_size],
                use_base_vae=use_base_vae,
                cpu_scheduling=cpu_scheduling,
            )
            all_imgs.extend(imgs)
        if self.ondemand:
            self.unload_vae()

        return all_imgs

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_txt2img_sdxl.py`:

```py
import torch
import numpy as np
from random import randint
from typing import Union
from diffusers import (
    DDIMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    KDPM2DiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DDPMScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    StableDiffusionPipeline,
)
from apps.stable_diffusion.src.models import SharkifyStableDiffusionModel
from transformers.utils import logging

logger = logging.get_logger(__name__)


class Text2ImageSDXLPipeline(StableDiffusionPipeline):
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            KDPM2DiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DDPMScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
    ):
        super().__init__(scheduler, sd_model, import_mlir, use_lora, ondemand)

    def prepare_latents(
        self,
        batch_size,
        height,
        width,
        generator,
        num_inference_steps,
        dtype,
    ):
        latents = torch.randn(
            (
                batch_size,
                4,
                height // 8,
                width // 8,
            ),
            generator=generator,
            dtype=torch.float32,
        ).to(dtype)

        self.scheduler.set_timesteps(num_inference_steps)
        self.scheduler.is_scale_input_called = True
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    def _get_add_time_ids(
        self, original_size, crops_coords_top_left, target_size, dtype
    ):
        add_time_ids = list(
            original_size + crops_coords_top_left + target_size
        )

        # self.unet.config.addition_time_embed_dim IS 256.
        # self.text_encoder_2.config.projection_dim IS 1280.
        passed_add_embed_dim = 256 * len(add_time_ids) + 1280
        expected_add_embed_dim = 2816
        # self.unet.add_embedding.linear_1.in_features IS 2816.

        if expected_add_embed_dim != passed_add_embed_dim:
            raise ValueError(
                f"Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. The model has an incorrect config. Please check `unet.config.time_embedding_type` and `text_encoder_2.config.projection_dim`."
            )

        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)
        return add_time_ids

    def generate_images(
        self,
        prompts,
        neg_prompts,
        batch_size,
        height,
        width,
        num_inference_steps,
        guidance_scale,
        seed,
        max_length,
        dtype,
        use_base_vae,
        cpu_scheduling,
        max_embeddings_multiples,
    ):
        # prompts and negative prompts must be a list.
        if isinstance(prompts, str):
            prompts = [prompts]

        if isinstance(neg_prompts, str):
            neg_prompts = [neg_prompts]

        prompts = prompts * batch_size
        neg_prompts = neg_prompts * batch_size

        # seed generator to create the inital latent noise. Also handle out of range seeds.
        # TODO: Wouldn't it be preferable to just report an error instead of modifying the seed on the fly?
        uint32_info = np.iinfo(np.uint32)
        uint32_min, uint32_max = uint32_info.min, uint32_info.max
        if seed < uint32_min or seed >= uint32_max:
            seed = randint(uint32_min, uint32_max)
        generator = torch.manual_seed(seed)

        # Get initial latents.
        init_latents = self.prepare_latents(
            batch_size=batch_size,
            height=height,
            width=width,
            generator=generator,
            num_inference_steps=num_inference_steps,
            dtype=dtype,
        )

        # Get text embeddings.
        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = self.encode_prompt_sdxl(
            prompt=prompts,
            num_images_per_prompt=1,
            do_classifier_free_guidance=True,
            negative_prompt=neg_prompts,
        )

        # Prepare timesteps.
        self.scheduler.set_timesteps(num_inference_steps)

        timesteps = self.scheduler.timesteps

        # Prepare added time ids & embeddings.
        original_size = (height, width)
        target_size = (height, width)
        crops_coords_top_left = (0, 0)
        add_text_embeds = pooled_prompt_embeds
        add_time_ids = self._get_add_time_ids(
            original_size,
            crops_coords_top_left,
            target_size,
            dtype=prompt_embeds.dtype,
        )

        prompt_embeds = torch.cat(
            [negative_prompt_embeds, prompt_embeds], dim=0
        )
        add_text_embeds = torch.cat(
            [negative_pooled_prompt_embeds, add_text_embeds], dim=0
        )
        add_time_ids = torch.cat([add_time_ids, add_time_ids], dim=0)

        prompt_embeds = prompt_embeds
        add_text_embeds = add_text_embeds.to(dtype)
        add_time_ids = add_time_ids.repeat(batch_size * 1, 1)

        # guidance scale as a float32 tensor.
        guidance_scale = torch.tensor(guidance_scale).to(dtype)
        prompt_embeds = prompt_embeds.to(dtype)
        add_time_ids = add_time_ids.to(dtype)

        # Get Image latents.
        latents = self.produce_img_latents_sdxl(
            init_latents,
            timesteps,
            add_text_embeds,
            add_time_ids,
            prompt_embeds,
            cpu_scheduling,
            guidance_scale,
            dtype,
        )

        # Img latents -> PIL images.
        all_imgs = []
        self.load_vae()
        # imgs = self.decode_latents_sdxl(None)
        # all_imgs.extend(imgs)
        for i in range(0, latents.shape[0], batch_size):
            imgs = self.decode_latents_sdxl(latents[i : i + batch_size])
            all_imgs.extend(imgs)
        if self.ondemand:
            self.unload_vae()

        return all_imgs

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_upscaler.py`:

```py
import inspect
import torch
import time
from tqdm.auto import tqdm
import numpy as np
from random import randint
from transformers import CLIPTokenizer
from typing import Union
from shark.shark_inference import SharkInference
from diffusers import (
    DDIMScheduler,
    DDPMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    KDPM2DiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    SD_STATE_IDLE,
    SD_STATE_CANCEL,
    StableDiffusionPipeline,
)
from apps.stable_diffusion.src.utils import (
    start_profiling,
    end_profiling,
)
from PIL import Image
from apps.stable_diffusion.src.models import SharkifyStableDiffusionModel


def preprocess(image):
    if isinstance(image, torch.Tensor):
        return image
    elif isinstance(image, Image.Image):
        image = [image]

    if isinstance(image[0], Image.Image):
        w, h = image[0].size
        w, h = map(
            lambda x: x - x % 64, (w, h)
        )  # resize to integer multiple of 64

        image = [np.array(i.resize((w, h)))[None, :] for i in image]
        image = np.concatenate(image, axis=0)
        image = np.array(image).astype(np.float32) / 255.0
        image = image.transpose(0, 3, 1, 2)
        image = 2.0 * image - 1.0
        image = torch.from_numpy(image)
    elif isinstance(image[0], torch.Tensor):
        image = torch.cat(image, dim=0)
    return image


class UpscalerPipeline(StableDiffusionPipeline):
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DDPMScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2DiscreteScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
        ],
        low_res_scheduler: Union[
            DDIMScheduler,
            DDPMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2DiscreteScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
    ):
        super().__init__(scheduler, sd_model, import_mlir, use_lora, ondemand)
        self.low_res_scheduler = low_res_scheduler
        self.status = SD_STATE_IDLE

    def prepare_extra_step_kwargs(self, generator, eta):
        accepts_eta = "eta" in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs["eta"] = eta

        # check if the scheduler accepts generator
        accepts_generator = "generator" in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs["generator"] = generator
        return extra_step_kwargs

    def decode_latents(self, latents, use_base_vae, cpu_scheduling):
        latents = 1 / 0.08333 * (latents.float())
        latents_numpy = latents
        if cpu_scheduling:
            latents_numpy = latents.detach().numpy()

        profile_device = start_profiling(file_path="vae.rdc")
        vae_start = time.time()
        images = self.vae("forward", (latents_numpy,))
        vae_inf_time = (time.time() - vae_start) * 1000
        end_profiling(profile_device)
        self.log += f"\nVAE Inference time (ms): {vae_inf_time:.3f}"

        images = torch.from_numpy(images)
        images = (images.detach().cpu() * 255.0).numpy()
        images = images.round()

        images = torch.from_numpy(images).to(torch.uint8).permute(0, 2, 3, 1)
        pil_images = [Image.fromarray(image) for image in images.numpy()]
        return pil_images

    def prepare_latents(
        self,
        batch_size,
        height,
        width,
        generator,
        num_inference_steps,
        dtype,
    ):
        latents = torch.randn(
            (
                batch_size,
                4,
                height,
                width,
            ),
            generator=generator,
            dtype=torch.float32,
        ).to(dtype)

        self.scheduler.set_timesteps(num_inference_steps)
        self.scheduler.is_scale_input_called = True
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    def produce_img_latents(
        self,
        latents,
        image,
        text_embeddings,
        guidance_scale,
        noise_level,
        total_timesteps,
        dtype,
        cpu_scheduling,
        extra_step_kwargs,
        return_all_latents=False,
    ):
        step_time_sum = 0
        latent_history = [latents]
        text_embeddings = torch.from_numpy(text_embeddings).to(dtype)
        text_embeddings_numpy = text_embeddings.detach().numpy()
        self.status = SD_STATE_IDLE
        if text_embeddings.shape[1] <= self.model_max_length:
            self.load_unet()
        else:
            self.load_unet_512()
        for i, t in tqdm(enumerate(total_timesteps)):
            step_start_time = time.time()
            latent_model_input = torch.cat([latents] * 2)
            latent_model_input = self.scheduler.scale_model_input(
                latent_model_input, t
            )
            latent_model_input = torch.cat([latent_model_input, image], dim=1)
            timestep = torch.tensor([t]).to(dtype).detach().numpy()
            if cpu_scheduling:
                latent_model_input = latent_model_input.detach().numpy()

            # Profiling Unet.
            profile_device = start_profiling(file_path="unet.rdc")
            if text_embeddings.shape[1] <= self.model_max_length:
                noise_pred = self.unet(
                    "forward",
                    (
                        latent_model_input,
                        timestep,
                        text_embeddings_numpy,
                        noise_level,
                    ),
                )
            else:
                noise_pred = self.unet_512(
                    "forward",
                    (
                        latent_model_input,
                        timestep,
                        text_embeddings_numpy,
                        noise_level,
                    ),
                )
            end_profiling(profile_device)
            noise_pred = torch.from_numpy(noise_pred)
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (
                noise_pred_text - noise_pred_uncond
            )

            if cpu_scheduling:
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs
                ).prev_sample
            else:
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs
                )

            latent_history.append(latents)
            step_time = (time.time() - step_start_time) * 1000
            #  self.log += (
            #      f"\nstep = {i} | timestep = {t} | time = {step_time:.2f}ms"
            #  )
            step_time_sum += step_time

            if self.status == SD_STATE_CANCEL:
                break

        if self.ondemand:
            self.unload_unet()
            self.unload_unet_512()
        avg_step_time = step_time_sum / len(total_timesteps)
        self.log += f"\nAverage step time: {avg_step_time}ms/it"

        if not return_all_latents:
            return latents
        all_latents = torch.cat(latent_history, dim=0)
        return all_latents

    def generate_images(
        self,
        prompts,
        neg_prompts,
        image,
        batch_size,
        height,
        width,
        num_inference_steps,
        noise_level,
        guidance_scale,
        seed,
        max_length,
        dtype,
        use_base_vae,
        cpu_scheduling,
        max_embeddings_multiples,
    ):
        # prompts and negative prompts must be a list.
        if isinstance(prompts, str):
            prompts = [prompts]

        if isinstance(neg_prompts, str):
            neg_prompts = [neg_prompts]

        prompts = prompts * batch_size
        neg_prompts = neg_prompts * batch_size

        # seed generator to create the inital latent noise. Also handle out of range seeds.
        # TODO: Wouldn't it be preferable to just report an error instead of modifying the seed on the fly?
        uint32_info = np.iinfo(np.uint32)
        uint32_min, uint32_max = uint32_info.min, uint32_info.max
        if seed < uint32_min or seed >= uint32_max:
            seed = randint(uint32_min, uint32_max)
        generator = torch.manual_seed(seed)

        # Get text embeddings with weight emphasis from prompts
        text_embeddings = self.encode_prompts_weight(
            prompts,
            neg_prompts,
            max_length,
            max_embeddings_multiples=max_embeddings_multiples,
        )

        # 4. Preprocess image
        image = preprocess(image).to(dtype)

        # 5. Add noise to image
        noise_level = torch.tensor([noise_level], dtype=torch.long)
        noise = torch.randn(
            image.shape,
            generator=generator,
        ).to(dtype)
        image = self.low_res_scheduler.add_noise(image, noise, noise_level)
        image = torch.cat([image] * 2)
        noise_level = torch.cat([noise_level] * image.shape[0])

        height, width = image.shape[2:]
        # Get initial latents
        init_latents = self.prepare_latents(
            batch_size=batch_size,
            height=height,
            width=width,
            generator=generator,
            num_inference_steps=num_inference_steps,
            dtype=dtype,
        )

        eta = 0.0
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

        # guidance scale as a float32 tensor.
        #  guidance_scale = torch.tensor(guidance_scale).to(torch.float32)

        # Get Image latents
        latents = self.produce_img_latents(
            latents=init_latents,
            image=image,
            text_embeddings=text_embeddings,
            guidance_scale=guidance_scale,
            noise_level=noise_level,
            total_timesteps=self.scheduler.timesteps,
            dtype=dtype,
            cpu_scheduling=cpu_scheduling,
            extra_step_kwargs=extra_step_kwargs,
        )

        # Img latents -> PIL images
        all_imgs = []
        self.load_vae()
        for i in tqdm(range(0, latents.shape[0], batch_size)):
            imgs = self.decode_latents(
                latents=latents[i : i + batch_size],
                use_base_vae=use_base_vae,
                cpu_scheduling=cpu_scheduling,
            )
            all_imgs.extend(imgs)
        if self.ondemand:
            self.unload_vae()

        return all_imgs

```

`apps/stable_diffusion/src/pipelines/pipeline_shark_stable_diffusion_utils.py`:

```py
import torch
import numpy as np
from transformers import CLIPTokenizer
from PIL import Image
from tqdm.auto import tqdm
import time
from typing import Union
from diffusers import (
    DDIMScheduler,
    DDPMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    KDPM2DiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DEISMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
)
from shark.shark_inference import SharkInference
from apps.stable_diffusion.src.schedulers import SharkEulerDiscreteScheduler
from apps.stable_diffusion.src.models import (
    SharkifyStableDiffusionModel,
    get_vae,
    get_clip,
    get_unet,
    get_tokenizer,
)
from apps.stable_diffusion.src.utils import (
    start_profiling,
    end_profiling,
)
import sys
import gc
from typing import List, Optional

SD_STATE_IDLE = "idle"
SD_STATE_CANCEL = "cancel"


class StableDiffusionPipeline:
    def __init__(
        self,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            KDPM2DiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DDPMScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
        ],
        sd_model: SharkifyStableDiffusionModel,
        import_mlir: bool,
        use_lora: str,
        ondemand: bool,
    ):
        self.vae = None
        self.text_encoder = None
        self.text_encoder_2 = None
        self.unet = None
        self.unet_512 = None
        self.model_max_length = 77
        self.scheduler = scheduler
        # TODO: Implement using logging python utility.
        self.log = ""
        self.status = SD_STATE_IDLE
        self.sd_model = sd_model
        self.import_mlir = import_mlir
        self.use_lora = use_lora
        self.ondemand = ondemand
        # TODO: Find a better workaround for fetching base_model_id early
        #  enough for CLIPTokenizer.
        try:
            self.tokenizer = get_tokenizer()
        except:
            self.load_unet()
            self.unload_unet()
            self.tokenizer = get_tokenizer()

    def load_clip(self):
        if self.text_encoder is not None:
            return

        if self.import_mlir or self.use_lora:
            if not self.import_mlir:
                print(
                    "Warning: LoRA provided but import_mlir not specified. "
                    "Importing MLIR anyways."
                )
            self.text_encoder = self.sd_model.clip()
        else:
            try:
                self.text_encoder = get_clip()
            except Exception as e:
                print(e)
                print("download pipeline failed, falling back to import_mlir")
                self.text_encoder = self.sd_model.clip()

    def unload_clip(self):
        del self.text_encoder
        self.text_encoder = None

    def load_clip_sdxl(self):
        if self.text_encoder and self.text_encoder_2:
            return

        if self.import_mlir or self.use_lora:
            if not self.import_mlir:
                print(
                    "Warning: LoRA provided but import_mlir not specified. "
                    "Importing MLIR anyways."
                )
            self.text_encoder, self.text_encoder_2 = self.sd_model.sdxl_clip()
        else:
            try:
                # TODO: Fix this for SDXL
                self.text_encoder = get_clip()
            except Exception as e:
                print(e)
                print("download pipeline failed, falling back to import_mlir")
                (
                    self.text_encoder,
                    self.text_encoder_2,
                ) = self.sd_model.sdxl_clip()

    def unload_clip_sdxl(self):
        del self.text_encoder, self.text_encoder_2
        self.text_encoder = None
        self.text_encoder_2 = None

    def load_unet(self):
        if self.unet is not None:
            return

        if self.import_mlir or self.use_lora:
            self.unet = self.sd_model.unet()
        else:
            try:
                self.unet = get_unet()
            except Exception as e:
                print(e)
                print("download pipeline failed, falling back to import_mlir")
                self.unet = self.sd_model.unet()

    def unload_unet(self):
        del self.unet
        self.unet = None

    def load_unet_512(self):
        if self.unet_512 is not None:
            return

        if self.import_mlir or self.use_lora:
            self.unet_512 = self.sd_model.unet(use_large=True)
        else:
            try:
                self.unet_512 = get_unet(use_large=True)
            except Exception as e:
                print(e)
                print("download pipeline failed, falling back to import_mlir")
                self.unet_512 = self.sd_model.unet(use_large=True)

    def unload_unet_512(self):
        del self.unet_512
        self.unet_512 = None

    def load_vae(self):
        if self.vae is not None:
            return

        if self.import_mlir or self.use_lora:
            self.vae = self.sd_model.vae()
        else:
            try:
                self.vae = get_vae()
            except Exception as e:
                print(e)
                print("download pipeline failed, falling back to import_mlir")
                self.vae = self.sd_model.vae()

    def unload_vae(self):
        del self.vae
        self.vae = None
        gc.collect()

    def encode_prompt_sdxl(
        self,
        prompt: str,
        num_images_per_prompt: int = 1,
        do_classifier_free_guidance: bool = True,
        negative_prompt: Optional[str] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
    ):
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]

        # Define tokenizers and text encoders
        self.tokenizer_2 = get_tokenizer("tokenizer_2")
        self.load_clip_sdxl()
        tokenizers = (
            [self.tokenizer, self.tokenizer_2]
            if self.tokenizer is not None
            else [self.tokenizer_2]
        )
        text_encoders = (
            [self.text_encoder, self.text_encoder_2]
            if self.text_encoder is not None
            else [self.text_encoder_2]
        )

        # textual inversion: procecss multi-vector tokens if necessary
        prompt_embeds_list = []
        prompts = [prompt, prompt]
        for prompt, tokenizer, text_encoder in zip(
            prompts, tokenizers, text_encoders
        ):
            text_inputs = tokenizer(
                prompt,
                padding="max_length",
                max_length=tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt",
            )

            text_input_ids = text_inputs.input_ids
            untruncated_ids = tokenizer(
                prompt, padding="longest", return_tensors="pt"
            ).input_ids

            if untruncated_ids.shape[-1] >= text_input_ids.shape[
                -1
            ] and not torch.equal(text_input_ids, untruncated_ids):
                removed_text = tokenizer.batch_decode(
                    untruncated_ids[:, tokenizer.model_max_length - 1 : -1]
                )
                print(
                    "The following part of your input was truncated because CLIP can only handle sequences up to"
                    f" {tokenizer.model_max_length} tokens: {removed_text}"
                )

            text_encoder_output = text_encoder("forward", (text_input_ids,))
            prompt_embeds = torch.from_numpy(text_encoder_output[0])
            pooled_prompt_embeds = torch.from_numpy(text_encoder_output[1])

            prompt_embeds_list.append(prompt_embeds)

        prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)

        # get unconditional embeddings for classifier free guidance
        zero_out_negative_prompt = (
            negative_prompt is None
            and self.config.force_zeros_for_empty_prompt
        )
        if (
            do_classifier_free_guidance
            and negative_prompt_embeds is None
            and zero_out_negative_prompt
        ):
            negative_prompt_embeds = torch.zeros_like(prompt_embeds)
            negative_pooled_prompt_embeds = torch.zeros_like(
                pooled_prompt_embeds
            )
        elif do_classifier_free_guidance and negative_prompt_embeds is None:
            negative_prompt = negative_prompt or ""
            negative_prompt_2 = negative_prompt

            uncond_tokens: List[str]
            if prompt is not None and type(prompt) is not type(
                negative_prompt
            ):
                raise TypeError(
                    f"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !="
                    f" {type(prompt)}."
                )
            elif isinstance(negative_prompt, str):
                uncond_tokens = [negative_prompt, negative_prompt_2]
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:"
                    f" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches"
                    " the batch size of `prompt`."
                )
            else:
                uncond_tokens = [negative_prompt, negative_prompt_2]

            negative_prompt_embeds_list = []
            for negative_prompt, tokenizer, text_encoder in zip(
                uncond_tokens, tokenizers, text_encoders
            ):
                max_length = prompt_embeds.shape[1]
                uncond_input = tokenizer(
                    negative_prompt,
                    padding="max_length",
                    max_length=max_length,
                    truncation=True,
                    return_tensors="pt",
                )
                text_encoder_output = text_encoder(
                    "forward", (uncond_input.input_ids,)
                )
                negative_prompt_embeds = torch.from_numpy(
                    text_encoder_output[0]
                )
                negative_pooled_prompt_embeds = torch.from_numpy(
                    text_encoder_output[1]
                )

                negative_prompt_embeds_list.append(negative_prompt_embeds)

            negative_prompt_embeds = torch.concat(
                negative_prompt_embeds_list, dim=-1
            )

        if self.ondemand:
            self.unload_clip_sdxl()
            gc.collect()

        # TODO: Look into dtype for text_encoder_2!
        prompt_embeds = prompt_embeds.to(dtype=torch.float32)
        bs_embed, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            bs_embed * num_images_per_prompt, seq_len, -1
        )

        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
        seq_len = negative_prompt_embeds.shape[1]
        negative_prompt_embeds = negative_prompt_embeds.to(dtype=torch.float32)
        negative_prompt_embeds = negative_prompt_embeds.repeat(
            1, num_images_per_prompt, 1
        )
        negative_prompt_embeds = negative_prompt_embeds.view(
            batch_size * num_images_per_prompt, seq_len, -1
        )

        pooled_prompt_embeds = pooled_prompt_embeds.repeat(
            1, num_images_per_prompt
        ).view(bs_embed * num_images_per_prompt, -1)
        negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(
            1, num_images_per_prompt
        ).view(bs_embed * num_images_per_prompt, -1)

        return (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        )

    def encode_prompts(self, prompts, neg_prompts, max_length):
        # Tokenize text and get embeddings
        text_input = self.tokenizer(
            prompts,
            padding="max_length",
            max_length=max_length,
            truncation=True,
            return_tensors="pt",
        )

        # Get unconditional embeddings as well
        uncond_input = self.tokenizer(
            neg_prompts,
            padding="max_length",
            max_length=max_length,
            truncation=True,
            return_tensors="pt",
        )
        text_input = torch.cat([uncond_input.input_ids, text_input.input_ids])

        self.load_clip()
        clip_inf_start = time.time()
        text_embeddings = self.text_encoder("forward", (text_input,))
        clip_inf_time = (time.time() - clip_inf_start) * 1000
        if self.ondemand:
            self.unload_clip()
            gc.collect()
        self.log += f"\nClip Inference time (ms) = {clip_inf_time:.3f}"

        return text_embeddings

    def decode_latents(self, latents, use_base_vae, cpu_scheduling):
        if use_base_vae:
            latents = 1 / 0.18215 * latents

        latents_numpy = latents
        if cpu_scheduling:
            latents_numpy = latents.detach().numpy()

        profile_device = start_profiling(file_path="vae.rdc")
        vae_start = time.time()
        images = self.vae("forward", (latents_numpy,))
        vae_inf_time = (time.time() - vae_start) * 1000
        end_profiling(profile_device)
        self.log += f"\nVAE Inference time (ms): {vae_inf_time:.3f}"

        if use_base_vae:
            images = torch.from_numpy(images)
            images = (images.detach().cpu() * 255.0).numpy()
            images = images.round()

        images = torch.from_numpy(images).to(torch.uint8).permute(0, 2, 3, 1)
        pil_images = [Image.fromarray(image) for image in images.numpy()]
        return pil_images

    def produce_img_latents(
        self,
        latents,
        text_embeddings,
        guidance_scale,
        total_timesteps,
        dtype,
        cpu_scheduling,
        mask=None,
        masked_image_latents=None,
        return_all_latents=False,
    ):
        self.status = SD_STATE_IDLE
        step_time_sum = 0
        latent_history = [latents]
        text_embeddings = torch.from_numpy(text_embeddings).to(dtype)
        text_embeddings_numpy = text_embeddings.detach().numpy()
        if text_embeddings.shape[1] <= self.model_max_length:
            self.load_unet()
        else:
            self.load_unet_512()
        for i, t in tqdm(enumerate(total_timesteps)):
            step_start_time = time.time()
            timestep = torch.tensor([t]).to(dtype).detach().numpy()
            latent_model_input = self.scheduler.scale_model_input(latents, t)
            if mask is not None and masked_image_latents is not None:
                latent_model_input = torch.cat(
                    [
                        torch.from_numpy(np.asarray(latent_model_input)),
                        mask,
                        masked_image_latents,
                    ],
                    dim=1,
                ).to(dtype)
            if cpu_scheduling:
                latent_model_input = latent_model_input.detach().numpy()

            # Profiling Unet.
            profile_device = start_profiling(file_path="unet.rdc")
            if text_embeddings.shape[1] <= self.model_max_length:
                noise_pred = self.unet(
                    "forward",
                    (
                        latent_model_input,
                        timestep,
                        text_embeddings_numpy,
                        guidance_scale,
                    ),
                    send_to_host=False,
                )
            else:
                noise_pred = self.unet_512(
                    "forward",
                    (
                        latent_model_input,
                        timestep,
                        text_embeddings_numpy,
                        guidance_scale,
                    ),
                    send_to_host=False,
                )
            end_profiling(profile_device)

            if cpu_scheduling:
                noise_pred = torch.from_numpy(noise_pred.to_host())
                latents = self.scheduler.step(
                    noise_pred, t, latents
                ).prev_sample
            else:
                latents = self.scheduler.step(noise_pred, t, latents)

            latent_history.append(latents)
            step_time = (time.time() - step_start_time) * 1000
            #  self.log += (
            #      f"\nstep = {i} | timestep = {t} | time = {step_time:.2f}ms"
            #  )
            step_time_sum += step_time

            if self.status == SD_STATE_CANCEL:
                break

        if self.ondemand:
            self.unload_unet()
            self.unload_unet_512()
            gc.collect()

        avg_step_time = step_time_sum / len(total_timesteps)
        self.log += f"\nAverage step time: {avg_step_time}ms/it"

        if not return_all_latents:
            return latents
        all_latents = torch.cat(latent_history, dim=0)
        return all_latents

    def produce_img_latents_sdxl(
        self,
        latents,
        total_timesteps,
        add_text_embeds,
        add_time_ids,
        prompt_embeds,
        cpu_scheduling,
        guidance_scale,
        dtype,
    ):
        # return None
        self.status = SD_STATE_IDLE
        step_time_sum = 0
        extra_step_kwargs = {"generator": None}
        self.load_unet()
        for i, t in tqdm(enumerate(total_timesteps)):
            step_start_time = time.time()
            timestep = torch.tensor([t]).to(dtype).detach().numpy()
            # expand the latents if we are doing classifier free guidance
            latent_model_input = torch.cat([latents] * 2)

            latent_model_input = self.scheduler.scale_model_input(
                latent_model_input, t
            ).to(dtype)

            noise_pred = self.unet(
                "forward",
                (
                    latent_model_input,
                    timestep,
                    prompt_embeds,
                    add_text_embeds,
                    add_time_ids,
                    guidance_scale,
                ),
                send_to_host=False,
            )
            latents = self.scheduler.step(
                noise_pred, t, latents, **extra_step_kwargs, return_dict=False
            )[0]

            step_time = (time.time() - step_start_time) * 1000
            step_time_sum += step_time

            if self.status == SD_STATE_CANCEL:
                break
        if self.ondemand:
            self.unload_unet()
            gc.collect()

        avg_step_time = step_time_sum / len(total_timesteps)
        self.log += f"\nAverage step time: {avg_step_time}ms/it"

        return latents

    def decode_latents_sdxl(self, latents):
        latents = latents.to(torch.float32)
        images = self.vae("forward", (latents,))
        images = (torch.from_numpy(images) / 2 + 0.5).clamp(0, 1)
        images = images.cpu().permute(0, 2, 3, 1).float().numpy()
        images = (images * 255).round().astype("uint8")
        pil_images = [Image.fromarray(image[:, :, :3]) for image in images]

        return pil_images

    @classmethod
    def from_pretrained(
        cls,
        scheduler: Union[
            DDIMScheduler,
            PNDMScheduler,
            LMSDiscreteScheduler,
            KDPM2DiscreteScheduler,
            EulerDiscreteScheduler,
            EulerAncestralDiscreteScheduler,
            DPMSolverMultistepScheduler,
            SharkEulerDiscreteScheduler,
            DEISMultistepScheduler,
            DDPMScheduler,
            DPMSolverSinglestepScheduler,
            KDPM2AncestralDiscreteScheduler,
            HeunDiscreteScheduler,
        ],
        import_mlir: bool,
        model_id: str,
        ckpt_loc: str,
        custom_vae: str,
        precision: str,
        max_length: int,
        batch_size: int,
        height: int,
        width: int,
        use_base_vae: bool,
        use_tuned: bool,
        ondemand: bool,
        low_cpu_mem_usage: bool = False,
        debug: bool = False,
        stencils: list[str] = [],
        # stencil_images: list[Image] = []
        use_lora: str = "",
        ddpm_scheduler: DDPMScheduler = None,
        use_quantize=None,
    ):
        if (
            not import_mlir
            and not use_lora
            and cls.__name__ == "StencilPipeline"
        ):
            sys.exit("StencilPipeline not supported with SharkTank currently.")

        is_inpaint = cls.__name__ in [
            "InpaintPipeline",
            "OutpaintPipeline",
        ]
        is_upscaler = cls.__name__ in ["UpscalerPipeline"]
        is_sdxl = cls.__name__ in ["Text2ImageSDXLPipeline"]

        sd_model = SharkifyStableDiffusionModel(
            model_id,
            ckpt_loc,
            custom_vae,
            precision,
            max_len=max_length,
            batch_size=batch_size,
            height=height,
            width=width,
            use_base_vae=use_base_vae,
            use_tuned=use_tuned,
            low_cpu_mem_usage=low_cpu_mem_usage,
            debug=debug,
            is_inpaint=is_inpaint,
            is_upscaler=is_upscaler,
            is_sdxl=is_sdxl,
            stencils=stencils,
            use_lora=use_lora,
            use_quantize=use_quantize,
        )

        if cls.__name__ in ["UpscalerPipeline"]:
            return cls(
                scheduler,
                ddpm_scheduler,
                sd_model,
                import_mlir,
                use_lora,
                ondemand,
            )

        if cls.__name__ == "StencilPipeline":
            return cls(
                scheduler, sd_model, import_mlir, use_lora, ondemand, stencils
            )
        return cls(scheduler, sd_model, import_mlir, use_lora, ondemand)

    # #####################################################
    # Implements text embeddings with weights from prompts
    # https://huggingface.co/AlanB/lpw_stable_diffusion_mod
    # #####################################################
    def encode_prompts_weight(
        self,
        prompt,
        negative_prompt,
        model_max_length,
        do_classifier_free_guidance=True,
        max_embeddings_multiples=1,
        num_images_per_prompt=1,
    ):
        r"""
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `list(int)`):
                prompt to be encoded
            negative_prompt (`str` or `List[str]`):
                The prompt or prompts not to guide the image generation.
                Ignored when not using guidance
                (i.e., ignored if `guidance_scale` is less than `1`).
            model_max_length (int):
                SHARK: pass the max length instead of relying on
                pipe.tokenizer.model_max_length
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not,
                SHARK: must be set to True as we always expect neg embeddings
                (defaulted to True)
            max_embeddings_multiples (`int`, *optional*, defaults to `3`):
                The max multiple length of prompt embeddings compared to the
                max output length of text encoder.
                SHARK: max_embeddings_multiples>1 produce a tensor shape error
                (defaulted to 1)
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
                SHARK: num_images_per_prompt is not used (defaulted to 1)
        """

        # SHARK: Save model_max_length, load the clip and init inference time
        self.model_max_length = model_max_length
        self.load_clip()
        clip_inf_start = time.time()

        batch_size = len(prompt) if isinstance(prompt, list) else 1

        if negative_prompt is None:
            negative_prompt = [""] * batch_size
        elif isinstance(negative_prompt, str):
            negative_prompt = [negative_prompt] * batch_size
        if batch_size != len(negative_prompt):
            raise ValueError(
                f"`negative_prompt`: "
                f"{negative_prompt} has batch size {len(negative_prompt)}, "
                f"but `prompt`: {prompt} has batch size {batch_size}. "
                f"Please make sure that passed `negative_prompt` matches "
                "the batch size of `prompt`."
            )

        text_embeddings, uncond_embeddings = get_weighted_text_embeddings(
            pipe=self,
            prompt=prompt,
            uncond_prompt=negative_prompt
            if do_classifier_free_guidance
            else None,
            max_embeddings_multiples=max_embeddings_multiples,
        )
        # SHARK: we are not using num_images_per_prompt
        # bs_embed, seq_len, _ = text_embeddings.shape
        # text_embeddings = text_embeddings.repeat(
        #     1,
        #     num_images_per_prompt,
        #     1
        # )
        # text_embeddings = (
        #     text_embeddings.view(
        #         bs_embed * num_images_per_prompt,
        #         seq_len,
        #         -1
        #     )
        # )

        if do_classifier_free_guidance:
            # SHARK: we are not using num_images_per_prompt
            # bs_embed, seq_len, _ = uncond_embeddings.shape
            # uncond_embeddings = (
            #     uncond_embeddings.repeat(
            #         1,
            #         num_images_per_prompt,
            #         1
            #     )
            # )
            # uncond_embeddings = (
            #     uncond_embeddings.view(
            #         bs_embed * num_images_per_prompt,
            #         seq_len,
            #         -1
            #     )
            # )
            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

        if text_embeddings.shape[1] > model_max_length:
            pad = (0, 0) * (len(text_embeddings.shape) - 2)
            pad = pad + (0, 512 - text_embeddings.shape[1])
            text_embeddings = torch.nn.functional.pad(text_embeddings, pad)

        # SHARK: Report clip inference time
        clip_inf_time = (time.time() - clip_inf_start) * 1000
        if self.ondemand:
            self.unload_clip()
            gc.collect()
        self.log += f"\nClip Inference time (ms) = {clip_inf_time:.3f}"

        return text_embeddings.numpy()


from typing import List, Optional, Union
import re

re_attention = re.compile(
    r"""
\\\(|
\\\)|
\\\[|
\\]|
\\\\|
\\|
\(|
\[|
:([+-]?[.\d]+)\)|
\)|
]|
[^\\()\[\]:]+|
:
""",
    re.X,
)


def parse_prompt_attention(text):
    """
    Parses a string with attention tokens and returns a list of pairs:
        text and its associated weight.
    Accepted tokens are:
      (abc) - increases attention to abc by a multiplier of 1.1
      (abc:3.12) - increases attention to abc by a multiplier of 3.12
      [abc] - decreases attention to abc by a multiplier of 1.1
      \( - literal character '('
      \[ - literal character '['
      \) - literal character ')'
      \] - literal character ']'
      \\ - literal character '\'
      anything else - just text
    >>> parse_prompt_attention('normal text')
    [['normal text', 1.0]]
    >>> parse_prompt_attention('an (important) word')
    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]
    >>> parse_prompt_attention('(unbalanced')
    [['unbalanced', 1.1]]
    >>> parse_prompt_attention('\(literal\]')
    [['(literal]', 1.0]]
    >>> parse_prompt_attention('(unnecessary)(parens)')
    [['unnecessaryparens', 1.1]]
    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')
    [['a ', 1.0],
     ['house', 1.5730000000000004],
     [' ', 1.1],
     ['on', 1.0],
     [' a ', 1.1],
     ['hill', 0.55],
     [', sun, ', 1.1],
     ['sky', 1.4641000000000006],
     ['.', 1.1]]
    """

    res = []
    round_brackets = []
    square_brackets = []

    round_bracket_multiplier = 1.1
    square_bracket_multiplier = 1 / 1.1

    def multiply_range(start_position, multiplier):
        for p in range(start_position, len(res)):
            res[p][1] *= multiplier

    for m in re_attention.finditer(text):
        text = m.group(0)
        weight = m.group(1)

        if text.startswith("\\"):
            res.append([text[1:], 1.0])
        elif text == "(":
            round_brackets.append(len(res))
        elif text == "[":
            square_brackets.append(len(res))
        elif weight is not None and len(round_brackets) > 0:
            multiply_range(round_brackets.pop(), float(weight))
        elif text == ")" and len(round_brackets) > 0:
            multiply_range(round_brackets.pop(), round_bracket_multiplier)
        elif text == "]" and len(square_brackets) > 0:
            multiply_range(square_brackets.pop(), square_bracket_multiplier)
        else:
            res.append([text, 1.0])

    for pos in round_brackets:
        multiply_range(pos, round_bracket_multiplier)

    for pos in square_brackets:
        multiply_range(pos, square_bracket_multiplier)

    if len(res) == 0:
        res = [["", 1.0]]

    # merge runs of identical weights
    i = 0
    while i + 1 < len(res):
        if res[i][1] == res[i + 1][1]:
            res[i][0] += res[i + 1][0]
            res.pop(i + 1)
        else:
            i += 1

    return res


def get_prompts_with_weights(
    pipe: StableDiffusionPipeline, prompt: List[str], max_length: int
):
    r"""
    Tokenize a list of prompts and return its tokens with weights of each token.
    No padding, starting or ending token is included.
    """
    tokens = []
    weights = []
    truncated = False
    for text in prompt:
        texts_and_weights = parse_prompt_attention(text)
        text_token = []
        text_weight = []
        for word, weight in texts_and_weights:
            # tokenize and discard the starting and the ending token
            token = pipe.tokenizer(word).input_ids[1:-1]
            text_token += token
            # copy the weight by length of token
            text_weight += [weight] * len(token)
            # stop if the text is too long (longer than truncation limit)
            if len(text_token) > max_length:
                truncated = True
                break
        # truncate
        if len(text_token) > max_length:
            truncated = True
            text_token = text_token[:max_length]
            text_weight = text_weight[:max_length]
        tokens.append(text_token)
        weights.append(text_weight)
    if truncated:
        print(
            "Prompt was truncated. Try to shorten the prompt or increase max_embeddings_multiples"
        )
    return tokens, weights


def pad_tokens_and_weights(
    tokens,
    weights,
    max_length,
    bos,
    eos,
    no_boseos_middle=True,
    chunk_length=77,
):
    r"""
    Pad the tokens (with starting and ending tokens) and weights (with 1.0) to max_length.
    """
    max_embeddings_multiples = (max_length - 2) // (chunk_length - 2)
    weights_length = (
        max_length
        if no_boseos_middle
        else max_embeddings_multiples * chunk_length
    )
    for i in range(len(tokens)):
        tokens[i] = (
            [bos] + tokens[i] + [eos] * (max_length - 1 - len(tokens[i]))
        )
        if no_boseos_middle:
            weights[i] = (
                [1.0] + weights[i] + [1.0] * (max_length - 1 - len(weights[i]))
            )
        else:
            w = []
            if len(weights[i]) == 0:
                w = [1.0] * weights_length
            else:
                for j in range(max_embeddings_multiples):
                    w.append(1.0)  # weight for starting token in this chunk
                    w += weights[i][
                        j
                        * (chunk_length - 2) : min(
                            len(weights[i]), (j + 1) * (chunk_length - 2)
                        )
                    ]
                    w.append(1.0)  # weight for ending token in this chunk
                w += [1.0] * (weights_length - len(w))
            weights[i] = w[:]

    return tokens, weights


def get_unweighted_text_embeddings(
    pipe: StableDiffusionPipeline,
    text_input: torch.Tensor,
    chunk_length: int,
    no_boseos_middle: Optional[bool] = True,
):
    """
    When the length of tokens is a multiple of the capacity of the text encoder,
    it should be split into chunks and sent to the text encoder individually.
    """
    max_embeddings_multiples = (text_input.shape[1] - 2) // (chunk_length - 2)
    if max_embeddings_multiples > 1:
        text_embeddings = []
        for i in range(max_embeddings_multiples):
            # extract the i-th chunk
            text_input_chunk = text_input[
                :, i * (chunk_length - 2) : (i + 1) * (chunk_length - 2) + 2
            ].clone()

            # cover the head and the tail by the starting and the ending tokens
            text_input_chunk[:, 0] = text_input[0, 0]
            text_input_chunk[:, -1] = text_input[0, -1]
            # text_embedding = pipe.text_encoder(text_input_chunk)[0]
            # SHARK: deplicate the text_input as Shark runner expects tokens and neg tokens
            formatted_text_input_chunk = torch.cat(
                [text_input_chunk, text_input_chunk]
            )
            text_embedding = pipe.text_encoder(
                "forward", (formatted_text_input_chunk,)
            )[0]

            if no_boseos_middle:
                if i == 0:
                    # discard the ending token
                    text_embedding = text_embedding[:, :-1]
                elif i == max_embeddings_multiples - 1:
                    # discard the starting token
                    text_embedding = text_embedding[:, 1:]
                else:
                    # discard both starting and ending tokens
                    text_embedding = text_embedding[:, 1:-1]

            text_embeddings.append(text_embedding)
        # SHARK: Convert the result to tensor
        # text_embeddings = torch.concat(text_embeddings, axis=1)
        text_embeddings_np = np.concatenate(np.array(text_embeddings))
        text_embeddings = torch.from_numpy(text_embeddings_np)[None, :]
    else:
        # SHARK: deplicate the text_input as Shark runner expects tokens and neg tokens
        # Convert the result to tensor
        # text_embeddings = pipe.text_encoder(text_input)[0]
        formatted_text_input = torch.cat([text_input, text_input])
        text_embeddings = pipe.text_encoder(
            "forward", (formatted_text_input,)
        )[0]
        text_embeddings = torch.from_numpy(text_embeddings)[None, :]
    return text_embeddings


# This function deals with NoneType values occuring in tokens after padding
# It switches out None with 49407 as truncating None values causes matrix dimension errors,
def filter_nonetype_tokens(tokens: List[List]):
    return [[49407 if token is None else token for token in tokens[0]]]


def get_weighted_text_embeddings(
    pipe: StableDiffusionPipeline,
    prompt: Union[str, List[str]],
    uncond_prompt: Optional[Union[str, List[str]]] = None,
    max_embeddings_multiples: Optional[int] = 3,
    no_boseos_middle: Optional[bool] = False,
    skip_parsing: Optional[bool] = False,
    skip_weighting: Optional[bool] = False,
):
    r"""
    Prompts can be assigned with local weights using brackets. For example,
    prompt 'A (very beautiful) masterpiece' highlights the words 'very beautiful',
    and the embedding tokens corresponding to the words get multiplied by a constant, 1.1.
    Also, to regularize of the embedding, the weighted embedding would be scaled to preserve the original mean.
    Args:
        pipe (`StableDiffusionPipeline`):
            Pipe to provide access to the tokenizer and the text encoder.
        prompt (`str` or `List[str]`):
            The prompt or prompts to guide the image generation.
        uncond_prompt (`str` or `List[str]`):
            The unconditional prompt or prompts for guide the image generation. If unconditional prompt
            is provided, the embeddings of prompt and uncond_prompt are concatenated.
        max_embeddings_multiples (`int`, *optional*, defaults to `3`):
            The max multiple length of prompt embeddings compared to the max output length of text encoder.
        no_boseos_middle (`bool`, *optional*, defaults to `False`):
            If the length of text token is multiples of the capacity of text encoder, whether reserve the starting and
            ending token in each of the chunk in the middle.
        skip_parsing (`bool`, *optional*, defaults to `False`):
            Skip the parsing of brackets.
        skip_weighting (`bool`, *optional*, defaults to `False`):
            Skip the weighting. When the parsing is skipped, it is forced True.
    """
    max_length = (pipe.model_max_length - 2) * max_embeddings_multiples + 2
    if isinstance(prompt, str):
        prompt = [prompt]

    if not skip_parsing:
        prompt_tokens, prompt_weights = get_prompts_with_weights(
            pipe, prompt, max_length - 2
        )
        if uncond_prompt is not None:
            if isinstance(uncond_prompt, str):
                uncond_prompt = [uncond_prompt]
            uncond_tokens, uncond_weights = get_prompts_with_weights(
                pipe, uncond_prompt, max_length - 2
            )
    else:
        prompt_tokens = [
            token[1:-1]
            for token in pipe.tokenizer(
                prompt, max_length=max_length, truncation=True
            ).input_ids
        ]
        prompt_weights = [[1.0] * len(token) for token in prompt_tokens]
        if uncond_prompt is not None:
            if isinstance(uncond_prompt, str):
                uncond_prompt = [uncond_prompt]
            uncond_tokens = [
                token[1:-1]
                for token in pipe.tokenizer(
                    uncond_prompt, max_length=max_length, truncation=True
                ).input_ids
            ]
            uncond_weights = [[1.0] * len(token) for token in uncond_tokens]

    # round up the longest length of tokens to a multiple of (model_max_length - 2)
    max_length = max([len(token) for token in prompt_tokens])
    if uncond_prompt is not None:
        max_length = max(
            max_length, max([len(token) for token in uncond_tokens])
        )

    max_embeddings_multiples = min(
        max_embeddings_multiples,
        (max_length - 1) // (pipe.model_max_length - 2) + 1,
    )
    max_embeddings_multiples = max(1, max_embeddings_multiples)
    max_length = (pipe.model_max_length - 2) * max_embeddings_multiples + 2

    # pad the length of tokens and weights
    bos = pipe.tokenizer.bos_token_id
    eos = pipe.tokenizer.eos_token_id
    prompt_tokens, prompt_weights = pad_tokens_and_weights(
        prompt_tokens,
        prompt_weights,
        max_length,
        bos,
        eos,
        no_boseos_middle=no_boseos_middle,
        chunk_length=pipe.model_max_length,
    )

    # FIXME: This is a hacky fix caused by tokenizer padding with None values
    prompt_tokens = filter_nonetype_tokens(prompt_tokens)

    # prompt_tokens = torch.tensor(prompt_tokens, dtype=torch.long, device=pipe.device)
    prompt_tokens = torch.tensor(prompt_tokens, dtype=torch.long, device="cpu")
    if uncond_prompt is not None:
        uncond_tokens, uncond_weights = pad_tokens_and_weights(
            uncond_tokens,
            uncond_weights,
            max_length,
            bos,
            eos,
            no_boseos_middle=no_boseos_middle,
            chunk_length=pipe.model_max_length,
        )

        # FIXME: This is a hacky fix caused by tokenizer padding with None values
        uncond_tokens = filter_nonetype_tokens(uncond_tokens)

        # uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=pipe.device)
        uncond_tokens = torch.tensor(
            uncond_tokens, dtype=torch.long, device="cpu"
        )

    # get the embeddings
    text_embeddings = get_unweighted_text_embeddings(
        pipe,
        prompt_tokens,
        pipe.model_max_length,
        no_boseos_middle=no_boseos_middle,
    )
    # prompt_weights = torch.tensor(prompt_weights, dtype=text_embeddings.dtype, device=pipe.device)
    prompt_weights = torch.tensor(
        prompt_weights, dtype=torch.float, device="cpu"
    )
    if uncond_prompt is not None:
        uncond_embeddings = get_unweighted_text_embeddings(
            pipe,
            uncond_tokens,
            pipe.model_max_length,
            no_boseos_middle=no_boseos_middle,
        )
        # uncond_weights = torch.tensor(uncond_weights, dtype=uncond_embeddings.dtype, device=pipe.device)
        uncond_weights = torch.tensor(
            uncond_weights, dtype=torch.float, device="cpu"
        )

    # assign weights to the prompts and normalize in the sense of mean
    # TODO: should we normalize by chunk or in a whole (current implementation)?
    if (not skip_parsing) and (not skip_weighting):
        previous_mean = (
            text_embeddings.float()
            .mean(axis=[-2, -1])
            .to(text_embeddings.dtype)
        )
        text_embeddings *= prompt_weights.unsqueeze(-1)
        current_mean = (
            text_embeddings.float()
            .mean(axis=[-2, -1])
            .to(text_embeddings.dtype)
        )
        text_embeddings *= (
            (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)
        )
        if uncond_prompt is not None:
            previous_mean = (
                uncond_embeddings.float()
                .mean(axis=[-2, -1])
                .to(uncond_embeddings.dtype)
            )
            uncond_embeddings *= uncond_weights.unsqueeze(-1)
            current_mean = (
                uncond_embeddings.float()
                .mean(axis=[-2, -1])
                .to(uncond_embeddings.dtype)
            )
            uncond_embeddings *= (
                (previous_mean / current_mean).unsqueeze(-1).unsqueeze(-1)
            )

    if uncond_prompt is not None:
        return text_embeddings, uncond_embeddings
    return text_embeddings, None

```

`apps/stable_diffusion/src/schedulers/__init__.py`:

```py
from apps.stable_diffusion.src.schedulers.sd_schedulers import get_schedulers
from apps.stable_diffusion.src.schedulers.shark_eulerdiscrete import (
    SharkEulerDiscreteScheduler,
)

```

`apps/stable_diffusion/src/schedulers/sd_schedulers.py`:

```py
from diffusers import (
    LMSDiscreteScheduler,
    PNDMScheduler,
    DDPMScheduler,
    DDIMScheduler,
    DPMSolverMultistepScheduler,
    KDPM2DiscreteScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DEISMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2AncestralDiscreteScheduler,
    HeunDiscreteScheduler,
)
from apps.stable_diffusion.src.schedulers.shark_eulerdiscrete import (
    SharkEulerDiscreteScheduler,
)


def get_schedulers(model_id):
    schedulers = dict()
    schedulers["PNDM"] = PNDMScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers["DDPM"] = DDPMScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers["KDPM2Discrete"] = KDPM2DiscreteScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers["LMSDiscrete"] = LMSDiscreteScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers["DDIM"] = DDIMScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers[
        "DPMSolverMultistep"
    ] = DPMSolverMultistepScheduler.from_pretrained(
        model_id, subfolder="scheduler", algorithm_type="dpmsolver"
    )
    schedulers[
        "DPMSolverMultistep++"
    ] = DPMSolverMultistepScheduler.from_pretrained(
        model_id, subfolder="scheduler", algorithm_type="dpmsolver++"
    )
    schedulers[
        "DPMSolverMultistepKarras"
    ] = DPMSolverMultistepScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
        use_karras_sigmas=True,
    )
    schedulers[
        "DPMSolverMultistepKarras++"
    ] = DPMSolverMultistepScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
        algorithm_type="dpmsolver++",
        use_karras_sigmas=True,
    )
    schedulers["EulerDiscrete"] = EulerDiscreteScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers[
        "EulerAncestralDiscrete"
    ] = EulerAncestralDiscreteScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers["DEISMultistep"] = DEISMultistepScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers[
        "SharkEulerDiscrete"
    ] = SharkEulerDiscreteScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers[
        "DPMSolverSinglestep"
    ] = DPMSolverSinglestepScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers[
        "KDPM2AncestralDiscrete"
    ] = KDPM2AncestralDiscreteScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers["HeunDiscrete"] = HeunDiscreteScheduler.from_pretrained(
        model_id,
        subfolder="scheduler",
    )
    schedulers["SharkEulerDiscrete"].compile()
    return schedulers

```

`apps/stable_diffusion/src/schedulers/shark_eulerdiscrete.py`:

```py
import sys
import numpy as np
from typing import List, Optional, Tuple, Union
from diffusers import (
    LMSDiscreteScheduler,
    PNDMScheduler,
    DDIMScheduler,
    DPMSolverMultistepScheduler,
    EulerDiscreteScheduler,
)
from diffusers.configuration_utils import register_to_config
from apps.stable_diffusion.src.utils import (
    compile_through_fx,
    get_shark_model,
    args,
)
import torch


class SharkEulerDiscreteScheduler(EulerDiscreteScheduler):
    @register_to_config
    def __init__(
        self,
        num_train_timesteps: int = 1000,
        beta_start: float = 0.0001,
        beta_end: float = 0.02,
        beta_schedule: str = "linear",
        trained_betas: Optional[Union[np.ndarray, List[float]]] = None,
        prediction_type: str = "epsilon",
    ):
        super().__init__(
            num_train_timesteps,
            beta_start,
            beta_end,
            beta_schedule,
            trained_betas,
            prediction_type,
        )

    def compile(self):
        SCHEDULER_BUCKET = "gs://shark_tank/stable_diffusion/schedulers"
        BATCH_SIZE = args.batch_size
        device = args.device.split(":", 1)[0].strip()

        model_input = {
            "euler": {
                "latent": torch.randn(
                    BATCH_SIZE, 4, args.height // 8, args.width // 8
                ),
                "output": torch.randn(
                    BATCH_SIZE, 4, args.height // 8, args.width // 8
                ),
                "sigma": torch.tensor(1).to(torch.float32),
                "dt": torch.tensor(1).to(torch.float32),
            },
        }

        example_latent = model_input["euler"]["latent"]
        example_output = model_input["euler"]["output"]
        if args.precision == "fp16":
            example_latent = example_latent.half()
            example_output = example_output.half()
        example_sigma = model_input["euler"]["sigma"]
        example_dt = model_input["euler"]["dt"]

        class ScalingModel(torch.nn.Module):
            def __init__(self):
                super().__init__()

            def forward(self, latent, sigma):
                return latent / ((sigma**2 + 1) ** 0.5)

        class SchedulerStepModel(torch.nn.Module):
            def __init__(self):
                super().__init__()

            def forward(self, noise_pred, sigma, latent, dt):
                pred_original_sample = latent - sigma * noise_pred
                derivative = (latent - pred_original_sample) / sigma
                return latent + derivative * dt

        iree_flags = []
        if len(args.iree_vulkan_target_triple) > 0:
            iree_flags.append(
                f"-iree-vulkan-target-triple={args.iree_vulkan_target_triple}"
            )

        def _import(self):
            scaling_model = ScalingModel()
            self.scaling_model, _ = compile_through_fx(
                model=scaling_model,
                inputs=(example_latent, example_sigma),
                extended_model_name=f"euler_scale_model_input_{BATCH_SIZE}_{args.height}_{args.width}_{device}_"
                + args.precision,
                extra_args=iree_flags,
            )

            step_model = SchedulerStepModel()
            self.step_model, _ = compile_through_fx(
                step_model,
                (example_output, example_sigma, example_latent, example_dt),
                extended_model_name=f"euler_step_{BATCH_SIZE}_{args.height}_{args.width}_{device}_"
                + args.precision,
                extra_args=iree_flags,
            )

        if args.import_mlir:
            _import(self)

        else:
            try:
                self.scaling_model = get_shark_model(
                    SCHEDULER_BUCKET,
                    "euler_scale_model_input_" + args.precision,
                    iree_flags,
                )
                self.step_model = get_shark_model(
                    SCHEDULER_BUCKET,
                    "euler_step_" + args.precision,
                    iree_flags,
                )
            except:
                print(
                    "failed to download model, falling back and using import_mlir"
                )
                args.import_mlir = True
                _import(self)

    def scale_model_input(self, sample, timestep):
        step_index = (self.timesteps == timestep).nonzero().item()
        sigma = self.sigmas[step_index]
        return self.scaling_model(
            "forward",
            (
                sample,
                sigma,
            ),
            send_to_host=False,
        )

    def step(self, noise_pred, timestep, latent):
        step_index = (self.timesteps == timestep).nonzero().item()
        sigma = self.sigmas[step_index]
        dt = self.sigmas[step_index + 1] - sigma
        return self.step_model(
            "forward",
            (
                noise_pred,
                sigma,
                latent,
                dt,
            ),
            send_to_host=False,
        )

```

`apps/stable_diffusion/src/utils/__init__.py`:

```py
from apps.stable_diffusion.src.utils.profiler import (
    start_profiling,
    end_profiling,
)
from apps.stable_diffusion.src.utils.resources import (
    prompt_examples,
    models_db,
    base_models,
    opt_flags,
    resource_path,
)
from apps.stable_diffusion.src.utils.sd_annotation import sd_model_annotation
from apps.stable_diffusion.src.utils.stable_args import args
from apps.stable_diffusion.src.utils.stencils.stencil_utils import (
    controlnet_hint_conversion,
    get_stencil_model_id,
)
from apps.stable_diffusion.src.utils.utils import (
    get_shark_model,
    compile_through_fx,
    set_iree_runtime_flags,
    map_device_to_name_path,
    set_init_device_flags,
    get_available_devices,
    get_opt_flags,
    preprocessCKPT,
    convert_original_vae,
    fetch_and_update_base_model_id,
    get_path_to_diffusers_checkpoint,
    sanitize_seed,
    parse_seed_input,
    batch_seeds,
    get_path_stem,
    get_extended_name,
    get_generated_imgs_path,
    get_generated_imgs_todays_subdir,
    clear_all,
    save_output_img,
    get_generation_text_info,
    update_lora_weight,
    resize_stencil,
    _compile_module,
)
from apps.stable_diffusion.src.utils.civitai import get_civitai_checkpoint
from apps.stable_diffusion.src.utils.resamplers import (
    resamplers,
    resampler_list,
)

```

`apps/stable_diffusion/src/utils/civitai.py`:

```py
import re
import requests
from apps.stable_diffusion.src.utils.stable_args import args

from pathlib import Path
from tqdm import tqdm


def get_civitai_checkpoint(url: str):
    with requests.get(url, allow_redirects=True, stream=True) as response:
        response.raise_for_status()

        # civitai api returns the filename in the content disposition
        base_filename = re.findall(
            '"([^"]*)"', response.headers["Content-Disposition"]
        )[0]
        destination_path = (
            Path.cwd() / (args.ckpt_dir or "models") / base_filename
        )

        # we don't have this model downloaded yet
        if not destination_path.is_file():
            print(
                f"downloading civitai model from {url} to {destination_path}"
            )

            size = int(response.headers["content-length"], 0)
            progress_bar = tqdm(total=size, unit="iB", unit_scale=True)

            with open(destination_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=65536):
                    f.write(chunk)
                    progress_bar.update(len(chunk))

            progress_bar.close()

        # we already have this model downloaded
        else:
            print(f"civitai model already downloaded to {destination_path}")

        response.close()
        return destination_path.as_posix()

```

`apps/stable_diffusion/src/utils/profiler.py`:

```py
from apps.stable_diffusion.src.utils.stable_args import args


# Helper function to profile the vulkan device.
def start_profiling(file_path="foo.rdc", profiling_mode="queue"):
    from shark.parser import shark_args

    if shark_args.vulkan_debug_utils and "vulkan" in args.device:
        import iree

        print(f"Profiling and saving to {file_path}.")
        vulkan_device = iree.runtime.get_device(args.device)
        vulkan_device.begin_profiling(mode=profiling_mode, file_path=file_path)
        return vulkan_device
    return None


def end_profiling(device):
    if device:
        return device.end_profiling()

```

`apps/stable_diffusion/src/utils/resamplers.py`:

```py
import PIL.Image as Image

resamplers = {
    "Lanczos": Image.Resampling.LANCZOS,
    "Nearest Neighbor": Image.Resampling.NEAREST,
    "Bilinear": Image.Resampling.BILINEAR,
    "Bicubic": Image.Resampling.BICUBIC,
    "Hamming": Image.Resampling.HAMMING,
    "Box": Image.Resampling.BOX,
}

resampler_list = resamplers.keys()

```

`apps/stable_diffusion/src/utils/resources.py`:

```py
import os
import json
import sys


def resource_path(relative_path):
    """Get absolute path to resource, works for dev and for PyInstaller"""
    base_path = getattr(
        sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__))
    )
    return os.path.join(base_path, relative_path)


def get_json_file(path):
    json_var = []
    loc_json = resource_path(path)
    if os.path.exists(loc_json):
        with open(loc_json, encoding="utf-8") as fopen:
            json_var = json.load(fopen)

    if not json_var:
        print(f"Unable to fetch {path}")

    return json_var


# TODO: This shouldn't be called from here, every time the file imports
# it will run all the global vars.
prompt_examples = get_json_file("resources/prompts.json")
models_db = get_json_file("resources/model_db.json")

# The base_model contains the input configuration for the different
# models and also helps in providing information for the variants.
base_models = get_json_file("resources/base_model.json")

# Contains optimization flags for different models.
opt_flags = get_json_file("resources/opt_flags.json")

```

`apps/stable_diffusion/src/utils/resources/base_model.json`:

```json
{
    "clip": {
        "token" : {
            "shape" : [
                "2*batch_size",
                "max_len"
            ],
            "dtype":"i64"
        }
    },
    "sdxl_clip": {
        "token" : {
            "shape" : [
                "1*batch_size",
                "max_len"
            ],
            "dtype":"i64"
        }
    },
    "vae_encode": {
        "image" : {
            "shape" : [
                "1*batch_size",3,"8*height","8*width"
            ],
            "dtype":"f32"
        }
    },
    "vae": {
        "vae": {
            "latents" : {
                "shape" : [
                    "1*batch_size",4,"height","width"
                ],
                "dtype":"f32"
            }
        },
        "vae_upscaler": {
            "latents" : {
                "shape" : [
                    "1*batch_size",4,"8*height","8*width"
                ],
                "dtype":"f32"
            }
        }
    },
    "unet": {
        "stabilityai/stable-diffusion-2-1": {
            "latents": {
                "shape": [
                    "1*batch_size",
                    4,
                    "height",
                    "width"
                ],
                "dtype": "f32"
            },
            "timesteps": {
                "shape": [
                    1
                ],
                "dtype": "f32"
            },
            "embedding": {
                "shape": [
                    "2*batch_size",
                    "max_len",
                    1024
                ],
                "dtype": "f32"
            },
            "guidance_scale": {
                "shape": 2,
                "dtype": "f32"
            }
        },
        "CompVis/stable-diffusion-v1-4": {
            "latents": {
                "shape": [
                    "1*batch_size",
                    4,
                    "height",
                    "width"
                ],
                "dtype": "f32"
            },
            "timesteps": {
                "shape": [
                    1
                ],
                "dtype": "f32"
            },
            "embedding": {
                "shape": [
                    "2*batch_size",
                    "max_len",
                    768
                ],
                "dtype": "f32"
            },
            "guidance_scale": {
                "shape": 2,
                "dtype": "f32"
            }
        },
        "stabilityai/stable-diffusion-2-inpainting": {
            "latents": {
                "shape": [
                    "1*batch_size",
                    9,
                    "height",
                    "width"
                ],
                "dtype": "f32"
            },
            "timesteps": {
                "shape": [
                    1
                ],
                "dtype": "f32"
            },
            "embedding": {
                "shape": [
                    "2*batch_size",
                    "max_len",
                    1024
                ],
                "dtype": "f32"
            },
            "guidance_scale": {
                "shape": 2,
                "dtype": "f32"
            }
        },
        "runwayml/stable-diffusion-inpainting": {
            "latents": {
                "shape": [
                    "1*batch_size",
                    9,
                    "height",
                    "width"
                ],
                "dtype": "f32"
            },
            "timesteps": {
                "shape": [
                    1
                ],
                "dtype": "f32"
            },
            "embedding": {
                "shape": [
                    "2*batch_size",
                    "max_len",
                    768
                ],
                "dtype": "f32"
            },
            "guidance_scale": {
                "shape": 2,
                "dtype": "f32"
            }
        },
        "stabilityai/stable-diffusion-x4-upscaler": {
            "latents": {
                "shape": [
                    "2*batch_size",
                    7,
                    "8*height",
                    "8*width"
                ],
                "dtype": "f32"
            },
            "timesteps": {
                "shape": [
                    1
                ],
                "dtype": "f32"
            },
            "embedding": {
                "shape": [
                    "2*batch_size",
                    "max_len",
                    1024
                ],
                "dtype": "f32"
            },
            "noise_level": {
                "shape": [2],
                "dtype": "i64"
            }
        },
        "stabilityai/stable-diffusion-xl-base-1.0": {
            "latents": {
                "shape": [
                    "2*batch_size",
                    4,
                    "height",
                    "width"
                ],
                "dtype": "f32"
            },
            "timesteps": {
                "shape": [
                    1
                ],
                "dtype": "f32"
            },
            "prompt_embeds": {
                "shape": [
                    "2*batch_size",
                    "max_len",
                    2048
                ],
                "dtype": "f32"
            },
            "text_embeds": {
                "shape": [
                    "2*batch_size",
                    1280
                ],
                "dtype": "f32"
            },
            "time_ids": {
                "shape": [
                    "2*batch_size",
                    6
                ],
                "dtype": "f32"
            },
            "guidance_scale": {
                "shape": 1,
                "dtype": "f32"
            }
        }
    },
    "stencil_adaptor": {
        "latents": {
            "shape": [
                "1*batch_size",
                4,
                "height",
                "width"
            ],
            "dtype": "f32"
        },
        "timesteps": {
            "shape": [
                1
            ],
            "dtype": "f32"
        },
        "embedding": {
            "shape": [
                "2*batch_size",
                "max_len",
                768
            ],
            "dtype": "f32"
        },
        "controlnet_hint": {
            "shape": [1, 3, "8*height", "8*width"],
            "dtype": "f32"
        },
        "acc1": {
            "shape": [2, 320, "height", "width"],
            "dtype": "f32"
        },
        "acc2": {
            "shape": [2, 320, "height", "width"],
            "dtype": "f32"
        },
        "acc3": {
            "shape": [2, 320, "height", "width"],
            "dtype": "f32"
        },
        "acc4": {
            "shape": [2, 320, "height/2", "width/2"],
            "dtype": "f32"
        },
        "acc5": {
            "shape": [2, 640, "height/2", "width/2"],
            "dtype": "f32"
        },
        "acc6": {
            "shape": [2, 640, "height/2", "width/2"],
            "dtype": "f32"
        },
        "acc7": {
            "shape": [2, 640, "height/4", "width/4"],
            "dtype": "f32"
        },
        "acc8": {
            "shape": [2, 1280, "height/4", "width/4"],
            "dtype": "f32"
        },
        "acc9": {
            "shape": [2, 1280, "height/4", "width/4"],
            "dtype": "f32"
        },
        "acc10": {
            "shape": [2, 1280, "height/8", "width/8"],
            "dtype": "f32"
        },
        "acc11": {
            "shape": [2, 1280, "height/8", "width/8"],
            "dtype": "f32"
        },
        "acc12": {
            "shape": [2, 1280, "height/8", "width/8"],
            "dtype": "f32"
        },
        "acc13": {
            "shape": [2, 1280, "height/8", "width/8"],
            "dtype": "f32"
        }
    },
    "stencil_unet": {
        "CompVis/stable-diffusion-v1-4": {
            "latents": {
                "shape": [
                    "1*batch_size",
                    4,
                    "height",
                    "width"
                ],
                "dtype": "f32"
            },
            "timesteps": {
                "shape": [
                    1
                ],
                "dtype": "f32"
            },
            "embedding": {
                "shape": [
                    "2*batch_size",
                    "max_len",
                    768
                ],
                "dtype": "f32"
            },
            "guidance_scale": {
                "shape": 2,
                "dtype": "f32"
            },
            "control1": {
                "shape": [2, 320, "height", "width"],
                "dtype": "f32"
            },
            "control2": {
                "shape": [2, 320, "height", "width"],
                "dtype": "f32"
            },
            "control3": {
                "shape": [2, 320, "height", "width"],
                "dtype": "f32"
            },
            "control4": {
                "shape": [2, 320, "height/2", "width/2"],
                "dtype": "f32"
            },
            "control5": {
                "shape": [2, 640, "height/2", "width/2"],
                "dtype": "f32"
            },
            "control6": {
                "shape": [2, 640, "height/2", "width/2"],
                "dtype": "f32"
            },
            "control7": {
                "shape": [2, 640, "height/4", "width/4"],
                "dtype": "f32"
            },
            "control8": {
                "shape": [2, 1280, "height/4", "width/4"],
                "dtype": "f32"
            },
            "control9": {
                "shape": [2, 1280, "height/4", "width/4"],
                "dtype": "f32"
            },
            "control10": {
                "shape": [2, 1280, "height/8", "width/8"],
                "dtype": "f32"
            },
            "control11": {
                "shape": [2, 1280, "height/8", "width/8"],
                "dtype": "f32"
            },
            "control12": {
                "shape": [2, 1280, "height/8", "width/8"],
                "dtype": "f32"
            },
            "control13": {
                "shape": [2, 1280, "height/8", "width/8"],
                "dtype": "f32"
            },
            "scale1": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale2": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale3": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale4": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale5": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale6": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale7": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale8": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale9": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale10": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale11": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale12": {
                "shape": 1,
                "dtype": "f32"
            },
            "scale13": {
                "shape": 1,
                "dtype": "f32"
            }
        }
    }
}
```

`apps/stable_diffusion/src/utils/resources/model_config.json`:

```json
[
  {
    "stablediffusion/v1_4":"CompVis/stable-diffusion-v1-4",
    "stablediffusion/v2_1base":"stabilityai/stable-diffusion-2-1-base",
    "stablediffusion/v2_1":"stabilityai/stable-diffusion-2-1",
    "stablediffusion/inpaint_v1":"runwayml/stable-diffusion-inpainting",
    "stablediffusion/inpaint_v2":"stabilityai/stable-diffusion-2-inpainting",
    "anythingv3/v1_4":"Linaqruf/anything-v3.0",
    "analogdiffusion/v1_4":"wavymulder/Analog-Diffusion",
    "openjourney/v1_4":"prompthero/openjourney",
    "dreamlike/v1_4":"dreamlike-art/dreamlike-diffusion-1.0"
  },
  {
    "stablediffusion/fp16":"fp16",
    "stablediffusion/fp32":"main",
    "anythingv3/fp16":"diffusers",
    "anythingv3/fp32":"diffusers",
    "analogdiffusion/fp16":"main",
    "analogdiffusion/fp32":"main",
    "openjourney/fp16":"main",
    "openjourney/fp32":"main"
  }
]

```

`apps/stable_diffusion/src/utils/resources/model_db.json`:

```json
[
  {
    "stablediffusion/untuned":"gs://shark_tank/nightly"
  },
  {
    "stablediffusion/v1_4/unet/fp16/length_64/untuned":"unet_1_64_512_512_fp16_stable-diffusion-v1-4_vulkan",
    "stablediffusion/v1_4/vae/fp16/length_77/untuned":"vae_1_64_512_512_fp16_stable-diffusion-v1-4_vulkan",
    "stablediffusion/v1_4/vae/fp16/length_64/untuned":"vae_1_64_512_512_fp16_stable-diffusion-v1-4_vulkan",
    "stablediffusion/v1_4/clip/fp32/length_64/untuned":"clip_1_64_512_512_fp16_stable-diffusion-v1-4_vulkan",
    "stablediffusion/v2_1base/unet/fp16/length_77/untuned":"unet_1_77_512_512_fp16_stable-diffusion-2-1-base_vulkan",
    "stablediffusion/v2_1base/unet/fp16/length_64/untuned":"unet_1_64_512_512_fp16_stable-diffusion-2-1-base_vulkan",
    "stablediffusion/v2_1base/vae/fp16/length_77/untuned":"vae_1_64_512_512_fp16_stable-diffusion-2-1-base_vulkan",
    "stablediffusion/v2_1base/clip/fp32/length_77/untuned":"clip_1_77_512_512_fp16_stable-diffusion-2-1-base_vulkan",
    "stablediffusion/v2_1base/clip/fp32/length_64/untuned":"clip_1_64_512_512_fp16_stable-diffusion-2-1-base_vulkan",
    "stablediffusion/v2_1/unet/fp16/length_77/untuned":"unet_1_77_512_512_fp16_stable-diffusion-2-1-base_vulkan",
    "stablediffusion/v2_1/vae/fp16/length_77/untuned":"vae_1_64_512_512_fp16_stable-diffusion-2-1-base_vulkan",
    "stablediffusion/v2_1/clip/fp32/length_77/untuned":"clip_1_64_512_512_fp16_stable-diffusion-2-1-base_vulkan"
  }
]

```

`apps/stable_diffusion/src/utils/resources/opt_flags.json`:

```json
{
  "unet": {
    "tuned": {
      "fp16": {
        "default_compilation_flags": []
      },
      "fp32": {
        "default_compilation_flags": []
      }
    },
    "untuned": {
      "fp16": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-global-opt-detach-elementwise-from-named-ops,iree-global-opt-convert-1x1-filter-conv2d-to-matmul,iree-preprocessing-convert-conv2d-to-img2col,iree-preprocessing-pad-linalg-ops{pad-size=32}))"
        ]
      },
      "fp32": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-global-opt-detach-elementwise-from-named-ops,iree-global-opt-convert-1x1-filter-conv2d-to-matmul,iree-preprocessing-convert-conv2d-to-img2col,iree-preprocessing-pad-linalg-ops{pad-size=16}))"
        ]
      }
    }
  },
  "vae": {
    "tuned": {
      "fp16": {
        "default_compilation_flags": [],
        "specified_compilation_flags": {
          "cuda": [],
          "default_device": [
            "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-global-opt-detach-elementwise-from-named-ops,iree-global-opt-convert-1x1-filter-conv2d-to-matmul,iree-preprocessing-convert-conv2d-to-img2col,iree-preprocessing-pad-linalg-ops{pad-size=32},iree-linalg-ext-convert-conv2d-to-winograd))"
          ]
        }
      },
      "fp32": {
        "default_compilation_flags": [],
        "specified_compilation_flags": {
          "cuda": [],
          "default_device": [
            "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-global-opt-detach-elementwise-from-named-ops,iree-global-opt-convert-1x1-filter-conv2d-to-matmul,iree-preprocessing-convert-conv2d-to-img2col,iree-preprocessing-pad-linalg-ops{pad-size=16},iree-linalg-ext-convert-conv2d-to-winograd))"
          ]
        }
      }
    },
    "untuned": {
      "fp16": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-global-opt-detach-elementwise-from-named-ops,iree-global-opt-convert-1x1-filter-conv2d-to-matmul,iree-preprocessing-convert-conv2d-to-img2col,iree-preprocessing-pad-linalg-ops{pad-size=32},iree-linalg-ext-convert-conv2d-to-winograd))"
        ]
      },
      "fp32": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-global-opt-detach-elementwise-from-named-ops,iree-global-opt-convert-1x1-filter-conv2d-to-matmul,iree-preprocessing-convert-conv2d-to-img2col,iree-preprocessing-pad-linalg-ops{pad-size=16},iree-linalg-ext-convert-conv2d-to-winograd))"
        ]
      }
    }
  },
  "clip": {
    "tuned": {
      "fp16": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-preprocessing-pad-linalg-ops{pad-size=16}))",
          "--iree-opt-data-tiling=False"
        ]
      },
      "fp32": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-preprocessing-pad-linalg-ops{pad-size=16}))",
          "--iree-opt-data-tiling=False"
        ]
      }
    },
    "untuned": {
      "fp16": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-preprocessing-pad-linalg-ops{pad-size=16}))",
          "--iree-opt-data-tiling=False"
        ]
      },
      "fp32": {
        "default_compilation_flags": [
          "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-preprocessing-pad-linalg-ops{pad-size=16}))",
          "--iree-opt-data-tiling=False"
        ]
      }
    }
  }
}

```

`apps/stable_diffusion/src/utils/resources/prompts.json`:

```json
[["A high tech solarpunk utopia in the Amazon rainforest"],
["A pikachu fine dining with a view to the Eiffel Tower"],
["A mecha robot in a favela in expressionist style"],
["an insect robot preparing a delicious meal"],
["A digital Illustration of the Babel tower, 4k, detailed, trending in artstation, fantasy vivid colors"],
["Cluttered house in the woods, anime, oil painting, high resolution, cottagecore, ghibli inspired, 4k"],
["A beautiful mansion beside a waterfall in the woods, by josef thoma, matte painting, trending on artstation HQ"],
["portrait photo of a asia old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes"],
["A photo of a beach, sunset, calm, beautiful landscape, waves, water"],
["(a large body of water with snowy mountains in the background), (fog, foggy, rolling fog), (clouds, cloudy, rolling clouds), dramatic sky and landscape, extraordinary landscape, (beautiful snow capped mountain background), (forest, dirt path)"],
["a photo taken of the front of a super-car drifting on a road near mountains at high speeds with smokes coming off the tires, front angle, front point of view, trees in the mountains of the background, ((sharp focus))"]]

```

`apps/stable_diffusion/src/utils/sd_annotation.py`:

```py
import os
import io
from shark.model_annotation import model_annotation, create_context
from shark.iree_utils._common import iree_target_map, run_cmd
from shark.shark_downloader import (
    download_model,
    download_public_file,
    WORKDIR,
)
from shark.parser import shark_args
from apps.stable_diffusion.src.utils.stable_args import args


def get_device():
    device = (
        args.device
        if "://" not in args.device
        else args.device.split("://")[0]
    )
    return device


def get_device_args():
    device = get_device()
    device_spec_args = []
    if device == "cuda":
        from shark.iree_utils.gpu_utils import get_iree_gpu_args

        gpu_flags = get_iree_gpu_args()
        for flag in gpu_flags:
            device_spec_args.append(flag)
    elif device == "vulkan":
        device_spec_args.append(
            f"--iree-vulkan-target-triple={args.iree_vulkan_target_triple} "
        )
    return device, device_spec_args


# Download the model (Unet or VAE fp16) from shark_tank
def load_model_from_tank():
    from apps.stable_diffusion.src.models import (
        get_params,
        get_variant_version,
    )

    variant, version = get_variant_version(args.hf_model_id)

    shark_args.local_tank_cache = args.local_tank_cache
    bucket_key = f"{variant}/untuned"
    if args.annotation_model == "unet":
        model_key = f"{variant}/{version}/unet/{args.precision}/length_{args.max_length}/untuned"
    elif args.annotation_model == "vae":
        is_base = "/base" if args.use_base_vae else ""
        model_key = f"{variant}/{version}/vae/{args.precision}/length_77/untuned{is_base}"

    bucket, model_name, iree_flags = get_params(
        bucket_key, model_key, args.annotation_model, "untuned", args.precision
    )
    mlir_model, func_name, inputs, golden_out = download_model(
        model_name,
        tank_url=bucket,
        frontend="torch",
    )
    return mlir_model, model_name


# Download the tuned config files from shark_tank
def load_winograd_configs():
    device = get_device()
    config_bucket = "gs://shark_tank/sd_tuned/configs/"
    config_name = f"{args.annotation_model}_winograd_{device}.json"
    full_gs_url = config_bucket + config_name
    if not os.path.exists(WORKDIR):
        os.mkdir(WORKDIR)
    winograd_config_dir = os.path.join(WORKDIR, "configs", config_name)
    print("Loading Winograd config file from ", winograd_config_dir)
    download_public_file(full_gs_url, winograd_config_dir, True)
    return winograd_config_dir


def load_lower_configs(base_model_id=None):
    from apps.stable_diffusion.src.models import get_variant_version
    from apps.stable_diffusion.src.utils.utils import (
        fetch_and_update_base_model_id,
    )

    if not base_model_id:
        if args.ckpt_loc != "":
            base_model_id = fetch_and_update_base_model_id(args.ckpt_loc)
        else:
            base_model_id = fetch_and_update_base_model_id(args.hf_model_id)
            if base_model_id == "":
                base_model_id = args.hf_model_id

    variant, version = get_variant_version(base_model_id)

    if version == "inpaint_v1":
        version = "v1_4"
    elif version == "inpaint_v2":
        version = "v2_1base"

    config_bucket = "gs://shark_tank/sd_tuned_configs/"

    device, device_spec_args = get_device_args()
    spec = ""
    if device_spec_args:
        spec = device_spec_args[-1].split("=")[-1].strip()
        if device == "vulkan":
            spec = spec.split("-")[0]

    if args.annotation_model == "vae":
        if not spec or spec in ["sm_80"]:
            config_name = (
                f"{args.annotation_model}_{args.precision}_{device}.json"
            )
        else:
            config_name = f"{args.annotation_model}_{args.precision}_{device}_{spec}.json"
    else:
        if not spec or spec in ["sm_80"]:
            if (
                version in ["v2_1", "v2_1base"]
                and args.height == 768
                and args.width == 768
            ):
                config_name = f"{args.annotation_model}_v2_1_768_{args.precision}_{device}.json"
            else:
                config_name = f"{args.annotation_model}_{version}_{args.precision}_{device}.json"
        elif spec in ["rdna3"] and version in [
            "v2_1",
            "v2_1base",
            "v1_4",
            "v1_5",
        ]:
            config_name = (
                f"{args.annotation_model}_"
                f"{version}_"
                f"{args.max_length}_"
                f"{args.precision}_"
                f"{device}_"
                f"{spec}_"
                f"{args.width}x{args.height}.json"
            )
        elif spec in ["rdna2"] and version in ["v2_1", "v2_1base", "v1_4"]:
            config_name = (
                f"{args.annotation_model}_"
                f"{version}_"
                f"{args.precision}_"
                f"{device}_"
                f"{spec}_"
                f"{args.width}x{args.height}.json"
            )
        else:
            config_name = (
                f"{args.annotation_model}_"
                f"{version}_"
                f"{args.precision}_"
                f"{device}_"
                f"{spec}.json"
            )

    lowering_config_dir = os.path.join(WORKDIR, "configs", config_name)
    print("Loading lowering config file from ", lowering_config_dir)
    full_gs_url = config_bucket + config_name
    download_public_file(full_gs_url, lowering_config_dir, True)
    return lowering_config_dir


# Annotate the model with Winograd attribute on selected conv ops
def annotate_with_winograd(input_mlir, winograd_config_dir, model_name):
    with create_context() as ctx:
        winograd_model = model_annotation(
            ctx,
            input_contents=input_mlir,
            config_path=winograd_config_dir,
            search_op="conv",
            winograd=True,
        )

    bytecode_stream = io.BytesIO()
    winograd_model.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    if args.save_annotation:
        if model_name.split("_")[-1] != "tuned":
            out_file_path = os.path.join(
                args.annotation_output, model_name + "_tuned_torch.mlir"
            )
        else:
            out_file_path = os.path.join(
                args.annotation_output, model_name + "_torch.mlir"
            )
        with open(out_file_path, "w") as f:
            f.write(str(winograd_model))
            f.close()

    return bytecode


def dump_after_mlir(input_mlir, use_winograd):
    import iree.compiler as ireec

    device, device_spec_args = get_device_args()
    if use_winograd:
        preprocess_flag = (
            "--iree-preprocessing-pass-pipeline=builtin.module"
            "(func.func(iree-global-opt-detach-elementwise-from-named-ops,"
            "iree-global-opt-convert-1x1-filter-conv2d-to-matmul,"
            "iree-preprocessing-convert-conv2d-to-img2col,"
            "iree-preprocessing-pad-linalg-ops{pad-size=32},"
            "iree-linalg-ext-convert-conv2d-to-winograd))"
        )
    else:
        preprocess_flag = (
            "--iree-preprocessing-pass-pipeline=builtin.module"
            "(func.func(iree-global-opt-detach-elementwise-from-named-ops,"
            "iree-global-opt-convert-1x1-filter-conv2d-to-matmul,"
            "iree-preprocessing-convert-conv2d-to-img2col,"
            "iree-preprocessing-pad-linalg-ops{pad-size=32}))"
        )

    dump_module = ireec.compile_str(
        input_mlir,
        target_backends=[iree_target_map(device)],
        extra_args=device_spec_args
        + [
            preprocess_flag,
            "--compile-to=preprocessing",
        ],
    )
    return dump_module


# For Unet annotate the model with tuned lowering configs
def annotate_with_lower_configs(
    input_mlir, lowering_config_dir, model_name, use_winograd
):
    # Dump IR after padding/img2col/winograd passes
    dump_module = dump_after_mlir(input_mlir, use_winograd)
    print("Applying tuned configs on", model_name)

    # Annotate the model with lowering configs in the config file
    with create_context() as ctx:
        tuned_model = model_annotation(
            ctx,
            input_contents=dump_module,
            config_path=lowering_config_dir,
            search_op="all",
        )

    bytecode_stream = io.BytesIO()
    tuned_model.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    if args.save_annotation:
        if model_name.split("_")[-1] != "tuned":
            out_file_path = (
                f"{args.annotation_output}/{model_name}_tuned_torch.mlir"
            )
        else:
            out_file_path = f"{args.annotation_output}/{model_name}_torch.mlir"
        with open(out_file_path, "w") as f:
            f.write(str(tuned_model))
            f.close()

    return bytecode


def sd_model_annotation(mlir_model, model_name, base_model_id=None):
    device = get_device()
    if args.annotation_model == "unet" and device == "vulkan":
        use_winograd = True
        winograd_config_dir = load_winograd_configs()
        winograd_model = annotate_with_winograd(
            mlir_model, winograd_config_dir, model_name
        )
        lowering_config_dir = load_lower_configs(base_model_id)
        tuned_model = annotate_with_lower_configs(
            winograd_model, lowering_config_dir, model_name, use_winograd
        )
    elif args.annotation_model == "vae" and device == "vulkan":
        if "rdna2" not in args.iree_vulkan_target_triple.split("-")[0]:
            use_winograd = True
            winograd_config_dir = load_winograd_configs()
            tuned_model = annotate_with_winograd(
                mlir_model, winograd_config_dir, model_name
            )
        else:
            tuned_model = mlir_model
    else:
        use_winograd = False
        lowering_config_dir = load_lower_configs(base_model_id)
        tuned_model = annotate_with_lower_configs(
            mlir_model, lowering_config_dir, model_name, use_winograd
        )
    return tuned_model


if __name__ == "__main__":
    mlir_model, model_name = load_model_from_tank()
    sd_model_annotation(mlir_model, model_name)

```

`apps/stable_diffusion/src/utils/stable_args.py`:

```py
import argparse
import os
from pathlib import Path

from apps.stable_diffusion.src.utils.resamplers import resampler_list


def path_expand(s):
    return Path(s).expanduser().resolve()


def is_valid_file(arg):
    if not os.path.exists(arg):
        return None
    else:
        return arg


p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)

##############################################################################
# Stable Diffusion Params
##############################################################################

p.add_argument(
    "-a",
    "--app",
    default="txt2img",
    help="Which app to use, one of: txt2img, img2img, outpaint, inpaint.",
)
p.add_argument(
    "-p",
    "--prompts",
    nargs="+",
    default=[
        "a photo taken of the front of a super-car drifting on a road near "
        "mountains at high speeds with smokes coming off the tires, front "
        "angle, front point of view, trees in the mountains of the "
        "background, ((sharp focus))"
    ],
    help="Text of which images to be generated.",
)

p.add_argument(
    "--negative_prompts",
    nargs="+",
    default=[
        "watermark, signature, logo, text, lowres, ((monochrome, grayscale)), "
        "blurry, ugly, blur, oversaturated, cropped"
    ],
    help="Text you don't want to see in the generated image.",
)

p.add_argument(
    "--img_path",
    type=str,
    help="Path to the image input for img2img/inpainting.",
)

p.add_argument(
    "--steps",
    type=int,
    default=50,
    help="The number of steps to do the sampling.",
)

p.add_argument(
    "--seed",
    type=str,
    default=-1,
    help="The seed or list of seeds to use. -1 for a random one.",
)

p.add_argument(
    "--batch_size",
    type=int,
    default=1,
    choices=range(1, 4),
    help="The number of inferences to be made in a single `batch_count`.",
)

p.add_argument(
    "--height",
    type=int,
    default=512,
    choices=range(128, 1025, 8),
    help="The height of the output image.",
)

p.add_argument(
    "--width",
    type=int,
    default=512,
    choices=range(128, 1025, 8),
    help="The width of the output image.",
)

p.add_argument(
    "--guidance_scale",
    type=float,
    default=7.5,
    help="The value to be used for guidance scaling.",
)

p.add_argument(
    "--noise_level",
    type=int,
    default=20,
    help="The value to be used for noise level of upscaler.",
)

p.add_argument(
    "--max_length",
    type=int,
    default=64,
    help="Max length of the tokenizer output, options are 64 and 77.",
)

p.add_argument(
    "--max_embeddings_multiples",
    type=int,
    default=5,
    help="The max multiple length of prompt embeddings compared to the max "
    "output length of text encoder.",
)

p.add_argument(
    "--strength",
    type=float,
    default=0.8,
    help="The strength of change applied on the given input image for "
    "img2img.",
)

p.add_argument(
    "--use_hiresfix",
    type=bool,
    default=False,
    help="Use Hires Fix to do higher resolution images, while trying to "
    "avoid the issues that come with it. This is accomplished by first "
    "generating an image using txt2img, then running it through img2img.",
)

p.add_argument(
    "--hiresfix_height",
    type=int,
    default=768,
    choices=range(128, 769, 8),
    help="The height of the Hires Fix image.",
)

p.add_argument(
    "--hiresfix_width",
    type=int,
    default=768,
    choices=range(128, 769, 8),
    help="The width of the Hires Fix image.",
)

p.add_argument(
    "--hiresfix_strength",
    type=float,
    default=0.6,
    help="The denoising strength to apply for the Hires Fix.",
)

p.add_argument(
    "--resample_type",
    type=str,
    default="Nearest Neighbor",
    choices=resampler_list,
    help="The resample type to use when resizing an image before being run "
    "through stable diffusion.",
)

##############################################################################
# Stable Diffusion Training Params
##############################################################################

p.add_argument(
    "--lora_save_dir",
    type=str,
    default="models/lora/",
    help="Directory to save the lora fine tuned model.",
)

p.add_argument(
    "--training_images_dir",
    type=str,
    default="models/lora/training_images/",
    help="Directory containing images that are an example of the prompt.",
)

p.add_argument(
    "--training_steps",
    type=int,
    default=2000,
    help="The number of steps to train.",
)

##############################################################################
# Inpainting and Outpainting Params
##############################################################################

p.add_argument(
    "--mask_path",
    type=str,
    help="Path to the mask image input for inpainting.",
)

p.add_argument(
    "--inpaint_full_res",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="If inpaint only masked area or whole picture.",
)

p.add_argument(
    "--inpaint_full_res_padding",
    type=int,
    default=32,
    choices=range(0, 257, 4),
    help="Number of pixels for only masked padding.",
)

p.add_argument(
    "--pixels",
    type=int,
    default=128,
    choices=range(8, 257, 8),
    help="Number of expended pixels for one direction for outpainting.",
)

p.add_argument(
    "--mask_blur",
    type=int,
    default=8,
    choices=range(0, 65),
    help="Number of blur pixels for outpainting.",
)

p.add_argument(
    "--left",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="If extend left for outpainting.",
)

p.add_argument(
    "--right",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="If extend right for outpainting.",
)

p.add_argument(
    "--up",
    "--top",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="If extend top for outpainting.",
)

p.add_argument(
    "--down",
    "--bottom",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="If extend bottom for outpainting.",
)

p.add_argument(
    "--noise_q",
    type=float,
    default=1.0,
    help="Fall-off exponent for outpainting (lower=higher detail) "
    "(min=0.0, max=4.0).",
)

p.add_argument(
    "--color_variation",
    type=float,
    default=0.05,
    help="Color variation for outpainting (min=0.0, max=1.0).",
)

##############################################################################
# Model Config and Usage Params
##############################################################################

p.add_argument(
    "--device", type=str, default="vulkan", help="Device to run the model."
)

p.add_argument(
    "--precision", type=str, default="fp16", help="Precision to run the model."
)

p.add_argument(
    "--import_mlir",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Imports the model from torch module to shark_module otherwise "
    "downloads the model from shark_tank.",
)

p.add_argument(
    "--load_vmfb",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Attempts to load the model from a precompiled flat-buffer "
    "and compiles + saves it if not found.",
)

p.add_argument(
    "--save_vmfb",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Saves the compiled flat-buffer to the local directory.",
)

p.add_argument(
    "--use_tuned",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Download and use the tuned version of the model if available.",
)

p.add_argument(
    "--use_base_vae",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Do conversion from the VAE output to pixel space on cpu.",
)

p.add_argument(
    "--scheduler",
    type=str,
    default="SharkEulerDiscrete",
    help="Other supported schedulers are [DDIM, PNDM, LMSDiscrete, "
    "DPMSolverMultistep, DPMSolverMultistep++, DPMSolverMultistepKarras, "
    "DPMSolverMultistepKarras++, EulerDiscrete, EulerAncestralDiscrete, "
    "DEISMultistep, KDPM2AncestralDiscrete, DPMSolverSinglestep, DDPM, "
    "HeunDiscrete].",
)

p.add_argument(
    "--output_img_format",
    type=str,
    default="png",
    help="Specify the format in which output image is save. "
    "Supported options: jpg / png.",
)

p.add_argument(
    "--output_dir",
    type=str,
    default=None,
    help="Directory path to save the output images and json.",
)

p.add_argument(
    "--batch_count",
    type=int,
    default=1,
    help="Number of batches to be generated with random seeds in "
    "single execution.",
)

p.add_argument(
    "--repeatable_seeds",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="The seed of the first batch will be used as the rng seed to "
    "generate the subsequent seeds for subsequent batches in that run.",
)

p.add_argument(
    "--ckpt_loc",
    type=str,
    default="",
    help="Path to SD's .ckpt file.",
)

p.add_argument(
    "--custom_vae",
    type=str,
    default="",
    help="HuggingFace repo-id or path to SD model's checkpoint whose VAE "
    "needs to be plugged in.",
)

p.add_argument(
    "--hf_model_id",
    type=str,
    default="stabilityai/stable-diffusion-2-1-base",
    help="The repo-id of hugging face.",
)

p.add_argument(
    "--low_cpu_mem_usage",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Use the accelerate package to reduce cpu memory consumption.",
)

p.add_argument(
    "--attention_slicing",
    type=str,
    default="none",
    help="Amount of attention slicing to use (one of 'max', 'auto', 'none', "
    "or an integer).",
)

p.add_argument(
    "--use_stencil",
    choices=["canny", "openpose", "scribble", "zoedepth"],
    help="Enable the stencil feature.",
)

p.add_argument(
    "--control_mode",
    choices=["Prompt", "Balanced", "Controlnet"],
    default="Balanced",
    help="How Controlnet injection should be prioritized.",
)

p.add_argument(
    "--use_lora",
    type=str,
    default="",
    help="Use standalone LoRA weight using a HF ID or a checkpoint "
    "file (~3 MB).",
)

p.add_argument(
    "--use_quantize",
    type=str,
    default="none",
    help="Runs the quantized version of stable diffusion model. "
    "This is currently in experimental phase. "
    "Currently, only runs the stable-diffusion-2-1-base model in "
    "int8 quantization.",
)

p.add_argument(
    "--ondemand",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Load and unload models for low VRAM.",
)

p.add_argument(
    "--hf_auth_token",
    type=str,
    default=None,
    help="Specify your own huggingface authentication tokens for models like Llama2.",
)

p.add_argument(
    "--device_allocator_heap_key",
    type=str,
    default="",
    help="Specify heap key for device caching allocator."
    "Expected form: max_allocation_size;max_allocation_capacity;max_free_allocation_count"
    "Example: --device_allocator_heap_key='*;1gib' (will limit caching on device to 1 gigabyte)",
)
##############################################################################
# IREE - Vulkan supported flags
##############################################################################

p.add_argument(
    "--iree_vulkan_target_triple",
    type=str,
    default="",
    help="Specify target triple for vulkan.",
)

p.add_argument(
    "--iree_metal_target_platform",
    type=str,
    default="",
    help="Specify target triple for metal.",
)

##############################################################################
# Misc. Debug and Optimization flags
##############################################################################

p.add_argument(
    "--use_compiled_scheduler",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Use the default scheduler precompiled into the model if available.",
)

p.add_argument(
    "--local_tank_cache",
    default="",
    help="Specify where to save downloaded shark_tank artifacts. "
    "If this is not set, the default is ~/.local/shark_tank/.",
)

p.add_argument(
    "--dump_isa",
    default=False,
    action="store_true",
    help="When enabled call amdllpc to get ISA dumps. "
    "Use with dispatch benchmarks.",
)

p.add_argument(
    "--dispatch_benchmarks",
    default=None,
    help="Dispatches to return benchmark data on. "
    'Use "All" for all, and None for none.',
)

p.add_argument(
    "--dispatch_benchmarks_dir",
    default="temp_dispatch_benchmarks",
    help="Directory where you want to store dispatch data "
    'generated with "--dispatch_benchmarks".',
)

p.add_argument(
    "--enable_rgp",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag for inserting debug frames between iterations "
    "for use with rgp.",
)

p.add_argument(
    "--hide_steps",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Flag for hiding the details of iteration/sec for each step.",
)

p.add_argument(
    "--warmup_count",
    type=int,
    default=0,
    help="Flag setting warmup count for CLIP and VAE [>= 0].",
)

p.add_argument(
    "--clear_all",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag to clear all mlir and vmfb from common locations. "
    "Recompiling will take several minutes.",
)

p.add_argument(
    "--save_metadata_to_json",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag for whether or not to save a generation information "
    "json file with the image.",
)

p.add_argument(
    "--write_metadata_to_png",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Flag for whether or not to save generation information in "
    "PNG chunk text to generated images.",
)

p.add_argument(
    "--import_debug",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="If import_mlir is True, saves mlir via the debug option "
    "in shark importer. Does nothing if import_mlir is false (the default).",
)

p.add_argument(
    "--compile_debug",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag to toggle debug assert/verify flags for imported IR in the"
    "iree-compiler. Default to false.",
)

p.add_argument(
    "--iree_constant_folding",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Controls constant folding in iree-compile for all SD models.",
)

p.add_argument(
    "--data_tiling",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Controls data tiling in iree-compile for all SD models.",
)

##############################################################################
# Web UI flags
##############################################################################

p.add_argument(
    "--progress_bar",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Flag for removing the progress bar animation during "
    "image generation.",
)

p.add_argument(
    "--ckpt_dir",
    type=str,
    default="",
    help="Path to directory where all .ckpts are stored in order to populate "
    "them in the web UI.",
)
# TODO: replace API flag when these can be run together
p.add_argument(
    "--ui",
    type=str,
    default="app" if os.name == "nt" else "web",
    help="One of: [api, app, web].",
)

p.add_argument(
    "--share",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag for generating a public URL.",
)

p.add_argument(
    "--server_port",
    type=int,
    default=8080,
    help="Flag for setting server port.",
)

p.add_argument(
    "--api",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag for enabling rest API.",
)

p.add_argument(
    "--api_accept_origin",
    action="append",
    type=str,
    help="An origin to be accepted by the REST api for Cross Origin"
    "Resource Sharing (CORS). Use multiple times for multiple origins, "
    'or use --api_accept_origin="*" to accept all origins. If no origins '
    "are set no CORS headers will be returned by the api. Use, for "
    "instance, if you need to access the REST api from Javascript running "
    "in a web browser.",
)

p.add_argument(
    "--debug",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag for enabling debugging log in WebUI.",
)

p.add_argument(
    "--output_gallery",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="Flag for removing the output gallery tab, and avoid exposing "
    "images under --output_dir in the UI.",
)

p.add_argument(
    "--output_gallery_followlinks",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag for whether the output gallery tab in the UI should "
    "follow symlinks when listing subdirectories under --output_dir.",
)


##############################################################################
# SD model auto-annotation flags
##############################################################################

p.add_argument(
    "--annotation_output",
    type=path_expand,
    default="./",
    help="Directory to save the annotated mlir file.",
)

p.add_argument(
    "--annotation_model",
    type=str,
    default="unet",
    help="Options are unet and vae.",
)

p.add_argument(
    "--save_annotation",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Save annotated mlir file.",
)
##############################################################################
# SD model auto-tuner flags
##############################################################################

p.add_argument(
    "--tuned_config_dir",
    type=path_expand,
    default="./",
    help="Directory to save the tuned config file.",
)

p.add_argument(
    "--num_iters",
    type=int,
    default=400,
    help="Number of iterations for tuning.",
)

p.add_argument(
    "--search_op",
    type=str,
    default="all",
    help="Op to be optimized, options are matmul, bmm, conv and all.",
)

##############################################################################
# DocuChat Flags
##############################################################################

p.add_argument(
    "--run_docuchat_web",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Specifies whether the docuchat's web version is running or not.",
)

##############################################################################
# rocm Flags
##############################################################################

p.add_argument(
    "--iree_rocm_target_chip",
    type=str,
    default="",
    help="Add the rocm device architecture ex gfx1100, gfx90a, etc. Use `hipinfo` "
    "or `iree-run-module --dump_devices=rocm` or `hipinfo` to get desired arch name",
)

args, unknown = p.parse_known_args()
if args.import_debug:
    os.environ["IREE_SAVE_TEMPS"] = os.path.join(
        os.getcwd(), args.hf_model_id.replace("/", "_")
    )

```

`apps/stable_diffusion/src/utils/stencils/__init__.py`:

```py
from apps.stable_diffusion.src.utils.stencils.canny import CannyDetector
from apps.stable_diffusion.src.utils.stencils.openpose import OpenposeDetector
from apps.stable_diffusion.src.utils.stencils.zoe import ZoeDetector

```

`apps/stable_diffusion/src/utils/stencils/canny/__init__.py`:

```py
import cv2


class CannyDetector:
    def __call__(self, img, low_threshold, high_threshold):
        return cv2.Canny(img, low_threshold, high_threshold)

```

`apps/stable_diffusion/src/utils/stencils/openpose/__init__.py`:

```py
import requests
from pathlib import Path

import torch
import numpy as np

# from annotator.util import annotator_ckpts_path
from apps.stable_diffusion.src.utils.stencils.openpose.body import Body
from apps.stable_diffusion.src.utils.stencils.openpose.hand import Hand
from apps.stable_diffusion.src.utils.stencils.openpose.openpose_util import (
    draw_bodypose,
    draw_handpose,
    handDetect,
)


body_model_path = "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth"
hand_model_path = "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth"


class OpenposeDetector:
    def __init__(self):
        cwd = Path.cwd()
        ckpt_path = Path(cwd, "stencil_annotator")
        ckpt_path.mkdir(parents=True, exist_ok=True)
        body_modelpath = ckpt_path / "body_pose_model.pth"
        hand_modelpath = ckpt_path / "hand_pose_model.pth"

        if not body_modelpath.is_file():
            r = requests.get(body_model_path, allow_redirects=True)
            open(body_modelpath, "wb").write(r.content)
        if not hand_modelpath.is_file():
            r = requests.get(hand_model_path, allow_redirects=True)
            open(hand_modelpath, "wb").write(r.content)

        self.body_estimation = Body(body_modelpath)
        self.hand_estimation = Hand(hand_modelpath)

    def __call__(self, oriImg, hand=False):
        oriImg = oriImg[:, :, ::-1].copy()
        with torch.no_grad():
            candidate, subset = self.body_estimation(oriImg)
            canvas = np.zeros_like(oriImg)
            canvas = draw_bodypose(canvas, candidate, subset)
            if hand:
                hands_list = handDetect(candidate, subset, oriImg)
                all_hand_peaks = []
                for x, y, w, is_left in hands_list:
                    peaks = self.hand_estimation(
                        oriImg[y : y + w, x : x + w, :]
                    )
                    peaks[:, 0] = np.where(
                        peaks[:, 0] == 0, peaks[:, 0], peaks[:, 0] + x
                    )
                    peaks[:, 1] = np.where(
                        peaks[:, 1] == 0, peaks[:, 1], peaks[:, 1] + y
                    )
                    all_hand_peaks.append(peaks)
                canvas = draw_handpose(canvas, all_hand_peaks)
            return canvas, dict(
                candidate=candidate.tolist(), subset=subset.tolist()
            )

```

`apps/stable_diffusion/src/utils/stencils/openpose/body.py`:

```py
import cv2
import numpy as np
import math
from scipy.ndimage.filters import gaussian_filter
import torch
import torch.nn as nn
from collections import OrderedDict
from apps.stable_diffusion.src.utils.stencils.openpose.openpose_util import (
    make_layers,
    transfer,
    padRightDownCorner,
)


class BodyPoseModel(nn.Module):
    def __init__(self):
        super(BodyPoseModel, self).__init__()

        # these layers have no relu layer
        no_relu_layers = [
            "conv5_5_CPM_L1",
            "conv5_5_CPM_L2",
            "Mconv7_stage2_L1",
            "Mconv7_stage2_L2",
            "Mconv7_stage3_L1",
            "Mconv7_stage3_L2",
            "Mconv7_stage4_L1",
            "Mconv7_stage4_L2",
            "Mconv7_stage5_L1",
            "Mconv7_stage5_L2",
            "Mconv7_stage6_L1",
            "Mconv7_stage6_L1",
        ]
        blocks = {}
        block0 = OrderedDict(
            [
                ("conv1_1", [3, 64, 3, 1, 1]),
                ("conv1_2", [64, 64, 3, 1, 1]),
                ("pool1_stage1", [2, 2, 0]),
                ("conv2_1", [64, 128, 3, 1, 1]),
                ("conv2_2", [128, 128, 3, 1, 1]),
                ("pool2_stage1", [2, 2, 0]),
                ("conv3_1", [128, 256, 3, 1, 1]),
                ("conv3_2", [256, 256, 3, 1, 1]),
                ("conv3_3", [256, 256, 3, 1, 1]),
                ("conv3_4", [256, 256, 3, 1, 1]),
                ("pool3_stage1", [2, 2, 0]),
                ("conv4_1", [256, 512, 3, 1, 1]),
                ("conv4_2", [512, 512, 3, 1, 1]),
                ("conv4_3_CPM", [512, 256, 3, 1, 1]),
                ("conv4_4_CPM", [256, 128, 3, 1, 1]),
            ]
        )

        # Stage 1
        block1_1 = OrderedDict(
            [
                ("conv5_1_CPM_L1", [128, 128, 3, 1, 1]),
                ("conv5_2_CPM_L1", [128, 128, 3, 1, 1]),
                ("conv5_3_CPM_L1", [128, 128, 3, 1, 1]),
                ("conv5_4_CPM_L1", [128, 512, 1, 1, 0]),
                ("conv5_5_CPM_L1", [512, 38, 1, 1, 0]),
            ]
        )

        block1_2 = OrderedDict(
            [
                ("conv5_1_CPM_L2", [128, 128, 3, 1, 1]),
                ("conv5_2_CPM_L2", [128, 128, 3, 1, 1]),
                ("conv5_3_CPM_L2", [128, 128, 3, 1, 1]),
                ("conv5_4_CPM_L2", [128, 512, 1, 1, 0]),
                ("conv5_5_CPM_L2", [512, 19, 1, 1, 0]),
            ]
        )
        blocks["block1_1"] = block1_1
        blocks["block1_2"] = block1_2

        self.model0 = make_layers(block0, no_relu_layers)

        # Stages 2 - 6
        for i in range(2, 7):
            blocks["block%d_1" % i] = OrderedDict(
                [
                    ("Mconv1_stage%d_L1" % i, [185, 128, 7, 1, 3]),
                    ("Mconv2_stage%d_L1" % i, [128, 128, 7, 1, 3]),
                    ("Mconv3_stage%d_L1" % i, [128, 128, 7, 1, 3]),
                    ("Mconv4_stage%d_L1" % i, [128, 128, 7, 1, 3]),
                    ("Mconv5_stage%d_L1" % i, [128, 128, 7, 1, 3]),
                    ("Mconv6_stage%d_L1" % i, [128, 128, 1, 1, 0]),
                    ("Mconv7_stage%d_L1" % i, [128, 38, 1, 1, 0]),
                ]
            )

            blocks["block%d_2" % i] = OrderedDict(
                [
                    ("Mconv1_stage%d_L2" % i, [185, 128, 7, 1, 3]),
                    ("Mconv2_stage%d_L2" % i, [128, 128, 7, 1, 3]),
                    ("Mconv3_stage%d_L2" % i, [128, 128, 7, 1, 3]),
                    ("Mconv4_stage%d_L2" % i, [128, 128, 7, 1, 3]),
                    ("Mconv5_stage%d_L2" % i, [128, 128, 7, 1, 3]),
                    ("Mconv6_stage%d_L2" % i, [128, 128, 1, 1, 0]),
                    ("Mconv7_stage%d_L2" % i, [128, 19, 1, 1, 0]),
                ]
            )

        for k in blocks.keys():
            blocks[k] = make_layers(blocks[k], no_relu_layers)

        self.model1_1 = blocks["block1_1"]
        self.model2_1 = blocks["block2_1"]
        self.model3_1 = blocks["block3_1"]
        self.model4_1 = blocks["block4_1"]
        self.model5_1 = blocks["block5_1"]
        self.model6_1 = blocks["block6_1"]

        self.model1_2 = blocks["block1_2"]
        self.model2_2 = blocks["block2_2"]
        self.model3_2 = blocks["block3_2"]
        self.model4_2 = blocks["block4_2"]
        self.model5_2 = blocks["block5_2"]
        self.model6_2 = blocks["block6_2"]

    def forward(self, x):
        out1 = self.model0(x)

        out1_1 = self.model1_1(out1)
        out1_2 = self.model1_2(out1)
        out2 = torch.cat([out1_1, out1_2, out1], 1)

        out2_1 = self.model2_1(out2)
        out2_2 = self.model2_2(out2)
        out3 = torch.cat([out2_1, out2_2, out1], 1)

        out3_1 = self.model3_1(out3)
        out3_2 = self.model3_2(out3)
        out4 = torch.cat([out3_1, out3_2, out1], 1)

        out4_1 = self.model4_1(out4)
        out4_2 = self.model4_2(out4)
        out5 = torch.cat([out4_1, out4_2, out1], 1)

        out5_1 = self.model5_1(out5)
        out5_2 = self.model5_2(out5)
        out6 = torch.cat([out5_1, out5_2, out1], 1)

        out6_1 = self.model6_1(out6)
        out6_2 = self.model6_2(out6)

        return out6_1, out6_2


class Body(object):
    def __init__(self, model_path):
        self.model = BodyPoseModel()
        if torch.cuda.is_available():
            self.model = self.model.cuda()
        model_dict = transfer(self.model, torch.load(model_path))
        self.model.load_state_dict(model_dict)
        self.model.eval()

    def __call__(self, oriImg):
        scale_search = [0.5]
        boxsize = 368
        stride = 8
        padValue = 128
        thre1 = 0.1
        thre2 = 0.05
        multiplier = [x * boxsize / oriImg.shape[0] for x in scale_search]
        heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))
        paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))

        for m in range(len(multiplier)):
            scale = multiplier[m]
            imageToTest = cv2.resize(
                oriImg,
                (0, 0),
                fx=scale,
                fy=scale,
                interpolation=cv2.INTER_CUBIC,
            )
            imageToTest_padded, pad = padRightDownCorner(
                imageToTest, stride, padValue
            )
            im = (
                np.transpose(
                    np.float32(imageToTest_padded[:, :, :, np.newaxis]),
                    (3, 2, 0, 1),
                )
                / 256
                - 0.5
            )
            im = np.ascontiguousarray(im)

            data = torch.from_numpy(im).float()
            if torch.cuda.is_available():
                data = data.cuda()
            with torch.no_grad():
                Mconv7_stage6_L1, Mconv7_stage6_L2 = self.model(data)
            Mconv7_stage6_L1 = Mconv7_stage6_L1.cpu().numpy()
            Mconv7_stage6_L2 = Mconv7_stage6_L2.cpu().numpy()

            # extract outputs, resize, and remove padding
            heatmap = np.transpose(
                np.squeeze(Mconv7_stage6_L2), (1, 2, 0)
            )  # output 1 is heatmaps
            heatmap = cv2.resize(
                heatmap,
                (0, 0),
                fx=stride,
                fy=stride,
                interpolation=cv2.INTER_CUBIC,
            )
            heatmap = heatmap[
                : imageToTest_padded.shape[0] - pad[2],
                : imageToTest_padded.shape[1] - pad[3],
                :,
            ]
            heatmap = cv2.resize(
                heatmap,
                (oriImg.shape[1], oriImg.shape[0]),
                interpolation=cv2.INTER_CUBIC,
            )

            # paf = np.transpose(np.squeeze(net.blobs[output_blobs.keys()[0]].data), (1, 2, 0))  # output 0 is PAFs
            paf = np.transpose(
                np.squeeze(Mconv7_stage6_L1), (1, 2, 0)
            )  # output 0 is PAFs
            paf = cv2.resize(
                paf,
                (0, 0),
                fx=stride,
                fy=stride,
                interpolation=cv2.INTER_CUBIC,
            )
            paf = paf[
                : imageToTest_padded.shape[0] - pad[2],
                : imageToTest_padded.shape[1] - pad[3],
                :,
            ]
            paf = cv2.resize(
                paf,
                (oriImg.shape[1], oriImg.shape[0]),
                interpolation=cv2.INTER_CUBIC,
            )

            heatmap_avg += heatmap_avg + heatmap / len(multiplier)
            paf_avg += +paf / len(multiplier)

        all_peaks = []
        peak_counter = 0

        for part in range(18):
            map_ori = heatmap_avg[:, :, part]
            one_heatmap = gaussian_filter(map_ori, sigma=3)

            map_left = np.zeros(one_heatmap.shape)
            map_left[1:, :] = one_heatmap[:-1, :]
            map_right = np.zeros(one_heatmap.shape)
            map_right[:-1, :] = one_heatmap[1:, :]
            map_up = np.zeros(one_heatmap.shape)
            map_up[:, 1:] = one_heatmap[:, :-1]
            map_down = np.zeros(one_heatmap.shape)
            map_down[:, :-1] = one_heatmap[:, 1:]

            peaks_binary = np.logical_and.reduce(
                (
                    one_heatmap >= map_left,
                    one_heatmap >= map_right,
                    one_heatmap >= map_up,
                    one_heatmap >= map_down,
                    one_heatmap > thre1,
                )
            )
            peaks = list(
                zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])
            )  # note reverse
            peaks_with_score = [x + (map_ori[x[1], x[0]],) for x in peaks]
            peak_id = range(peak_counter, peak_counter + len(peaks))
            peaks_with_score_and_id = [
                peaks_with_score[i] + (peak_id[i],)
                for i in range(len(peak_id))
            ]

            all_peaks.append(peaks_with_score_and_id)
            peak_counter += len(peaks)

        # find connection in the specified sequence, center 29 is in the position 15
        limbSeq = [
            [2, 3],
            [2, 6],
            [3, 4],
            [4, 5],
            [6, 7],
            [7, 8],
            [2, 9],
            [9, 10],
            [10, 11],
            [2, 12],
            [12, 13],
            [13, 14],
            [2, 1],
            [1, 15],
            [15, 17],
            [1, 16],
            [16, 18],
            [3, 17],
            [6, 18],
        ]
        # the middle joints heatmap correpondence
        mapIdx = [
            [31, 32],
            [39, 40],
            [33, 34],
            [35, 36],
            [41, 42],
            [43, 44],
            [19, 20],
            [21, 22],
            [23, 24],
            [25, 26],
            [27, 28],
            [29, 30],
            [47, 48],
            [49, 50],
            [53, 54],
            [51, 52],
            [55, 56],
            [37, 38],
            [45, 46],
        ]

        connection_all = []
        special_k = []
        mid_num = 10

        for k in range(len(mapIdx)):
            score_mid = paf_avg[:, :, [x - 19 for x in mapIdx[k]]]
            candA = all_peaks[limbSeq[k][0] - 1]
            candB = all_peaks[limbSeq[k][1] - 1]
            nA = len(candA)
            nB = len(candB)
            indexA, indexB = limbSeq[k]
            if nA != 0 and nB != 0:
                connection_candidate = []
                for i in range(nA):
                    for j in range(nB):
                        vec = np.subtract(candB[j][:2], candA[i][:2])
                        norm = math.sqrt(vec[0] * vec[0] + vec[1] * vec[1])
                        norm = max(0.001, norm)
                        vec = np.divide(vec, norm)

                        startend = list(
                            zip(
                                np.linspace(
                                    candA[i][0], candB[j][0], num=mid_num
                                ),
                                np.linspace(
                                    candA[i][1], candB[j][1], num=mid_num
                                ),
                            )
                        )

                        vec_x = np.array(
                            [
                                score_mid[
                                    int(round(startend[I][1])),
                                    int(round(startend[I][0])),
                                    0,
                                ]
                                for I in range(len(startend))
                            ]
                        )
                        vec_y = np.array(
                            [
                                score_mid[
                                    int(round(startend[I][1])),
                                    int(round(startend[I][0])),
                                    1,
                                ]
                                for I in range(len(startend))
                            ]
                        )

                        score_midpts = np.multiply(
                            vec_x, vec[0]
                        ) + np.multiply(vec_y, vec[1])
                        score_with_dist_prior = sum(score_midpts) / len(
                            score_midpts
                        ) + min(0.5 * oriImg.shape[0] / norm - 1, 0)
                        criterion1 = len(
                            np.nonzero(score_midpts > thre2)[0]
                        ) > 0.8 * len(score_midpts)
                        criterion2 = score_with_dist_prior > 0
                        if criterion1 and criterion2:
                            connection_candidate.append(
                                [
                                    i,
                                    j,
                                    score_with_dist_prior,
                                    score_with_dist_prior
                                    + candA[i][2]
                                    + candB[j][2],
                                ]
                            )

                connection_candidate = sorted(
                    connection_candidate, key=lambda x: x[2], reverse=True
                )
                connection = np.zeros((0, 5))
                for c in range(len(connection_candidate)):
                    i, j, s = connection_candidate[c][0:3]
                    if i not in connection[:, 3] and j not in connection[:, 4]:
                        connection = np.vstack(
                            [connection, [candA[i][3], candB[j][3], s, i, j]]
                        )
                        if len(connection) >= min(nA, nB):
                            break

                connection_all.append(connection)
            else:
                special_k.append(k)
                connection_all.append([])

        # last number in each row is the total parts number of that person
        # the second last number in each row is the score of the overall configuration
        subset = -1 * np.ones((0, 20))
        candidate = np.array(
            [item for sublist in all_peaks for item in sublist]
        )

        for k in range(len(mapIdx)):
            if k not in special_k:
                partAs = connection_all[k][:, 0]
                partBs = connection_all[k][:, 1]
                indexA, indexB = np.array(limbSeq[k]) - 1

                for i in range(len(connection_all[k])):  # = 1:size(temp,1)
                    found = 0
                    subset_idx = [-1, -1]
                    for j in range(len(subset)):  # 1:size(subset,1):
                        if (
                            subset[j][indexA] == partAs[i]
                            or subset[j][indexB] == partBs[i]
                        ):
                            subset_idx[found] = j
                            found += 1

                    if found == 1:
                        j = subset_idx[0]
                        if subset[j][indexB] != partBs[i]:
                            subset[j][indexB] = partBs[i]
                            subset[j][-1] += 1
                            subset[j][-2] += (
                                candidate[partBs[i].astype(int), 2]
                                + connection_all[k][i][2]
                            )
                    elif found == 2:  # if found 2 and disjoint, merge them
                        j1, j2 = subset_idx
                        membership = (
                            (subset[j1] >= 0).astype(int)
                            + (subset[j2] >= 0).astype(int)
                        )[:-2]
                        if len(np.nonzero(membership == 2)[0]) == 0:  # merge
                            subset[j1][:-2] += subset[j2][:-2] + 1
                            subset[j1][-2:] += subset[j2][-2:]
                            subset[j1][-2] += connection_all[k][i][2]
                            subset = np.delete(subset, j2, 0)
                        else:  # as like found == 1
                            subset[j1][indexB] = partBs[i]
                            subset[j1][-1] += 1
                            subset[j1][-2] += (
                                candidate[partBs[i].astype(int), 2]
                                + connection_all[k][i][2]
                            )

                    # if find no partA in the subset, create a new subset
                    elif not found and k < 17:
                        row = -1 * np.ones(20)
                        row[indexA] = partAs[i]
                        row[indexB] = partBs[i]
                        row[-1] = 2
                        row[-2] = (
                            sum(
                                candidate[
                                    connection_all[k][i, :2].astype(int), 2
                                ]
                            )
                            + connection_all[k][i][2]
                        )
                        subset = np.vstack([subset, row])
        # delete some rows of subset which has few parts occur
        deleteIdx = []
        for i in range(len(subset)):
            if subset[i][-1] < 4 or subset[i][-2] / subset[i][-1] < 0.4:
                deleteIdx.append(i)
        subset = np.delete(subset, deleteIdx, axis=0)

        # candidate: x, y, score, id
        return candidate, subset

```

`apps/stable_diffusion/src/utils/stencils/openpose/hand.py`:

```py
import cv2
import numpy as np
from scipy.ndimage.filters import gaussian_filter
import torch
import torch.nn as nn
from skimage.measure import label
from collections import OrderedDict
from apps.stable_diffusion.src.utils.stencils.openpose.openpose_util import (
    make_layers,
    transfer,
    padRightDownCorner,
    npmax,
)


class HandPoseModel(nn.Module):
    def __init__(self):
        super(HandPoseModel, self).__init__()

        # these layers have no relu layer
        no_relu_layers = [
            "conv6_2_CPM",
            "Mconv7_stage2",
            "Mconv7_stage3",
            "Mconv7_stage4",
            "Mconv7_stage5",
            "Mconv7_stage6",
        ]
        # stage 1
        block1_0 = OrderedDict(
            [
                ("conv1_1", [3, 64, 3, 1, 1]),
                ("conv1_2", [64, 64, 3, 1, 1]),
                ("pool1_stage1", [2, 2, 0]),
                ("conv2_1", [64, 128, 3, 1, 1]),
                ("conv2_2", [128, 128, 3, 1, 1]),
                ("pool2_stage1", [2, 2, 0]),
                ("conv3_1", [128, 256, 3, 1, 1]),
                ("conv3_2", [256, 256, 3, 1, 1]),
                ("conv3_3", [256, 256, 3, 1, 1]),
                ("conv3_4", [256, 256, 3, 1, 1]),
                ("pool3_stage1", [2, 2, 0]),
                ("conv4_1", [256, 512, 3, 1, 1]),
                ("conv4_2", [512, 512, 3, 1, 1]),
                ("conv4_3", [512, 512, 3, 1, 1]),
                ("conv4_4", [512, 512, 3, 1, 1]),
                ("conv5_1", [512, 512, 3, 1, 1]),
                ("conv5_2", [512, 512, 3, 1, 1]),
                ("conv5_3_CPM", [512, 128, 3, 1, 1]),
            ]
        )

        block1_1 = OrderedDict(
            [
                ("conv6_1_CPM", [128, 512, 1, 1, 0]),
                ("conv6_2_CPM", [512, 22, 1, 1, 0]),
            ]
        )

        blocks = {}
        blocks["block1_0"] = block1_0
        blocks["block1_1"] = block1_1

        # stage 2-6
        for i in range(2, 7):
            blocks["block%d" % i] = OrderedDict(
                [
                    ("Mconv1_stage%d" % i, [150, 128, 7, 1, 3]),
                    ("Mconv2_stage%d" % i, [128, 128, 7, 1, 3]),
                    ("Mconv3_stage%d" % i, [128, 128, 7, 1, 3]),
                    ("Mconv4_stage%d" % i, [128, 128, 7, 1, 3]),
                    ("Mconv5_stage%d" % i, [128, 128, 7, 1, 3]),
                    ("Mconv6_stage%d" % i, [128, 128, 1, 1, 0]),
                    ("Mconv7_stage%d" % i, [128, 22, 1, 1, 0]),
                ]
            )

        for k in blocks.keys():
            blocks[k] = make_layers(blocks[k], no_relu_layers)

        self.model1_0 = blocks["block1_0"]
        self.model1_1 = blocks["block1_1"]
        self.model2 = blocks["block2"]
        self.model3 = blocks["block3"]
        self.model4 = blocks["block4"]
        self.model5 = blocks["block5"]
        self.model6 = blocks["block6"]

    def forward(self, x):
        out1_0 = self.model1_0(x)
        out1_1 = self.model1_1(out1_0)
        concat_stage2 = torch.cat([out1_1, out1_0], 1)
        out_stage2 = self.model2(concat_stage2)
        concat_stage3 = torch.cat([out_stage2, out1_0], 1)
        out_stage3 = self.model3(concat_stage3)
        concat_stage4 = torch.cat([out_stage3, out1_0], 1)
        out_stage4 = self.model4(concat_stage4)
        concat_stage5 = torch.cat([out_stage4, out1_0], 1)
        out_stage5 = self.model5(concat_stage5)
        concat_stage6 = torch.cat([out_stage5, out1_0], 1)
        out_stage6 = self.model6(concat_stage6)
        return out_stage6


class Hand(object):
    def __init__(self, model_path):
        self.model = HandPoseModel()
        if torch.cuda.is_available():
            self.model = self.model.cuda()
        model_dict = transfer(self.model, torch.load(model_path))
        self.model.load_state_dict(model_dict)
        self.model.eval()

    def __call__(self, oriImg):
        scale_search = [0.5, 1.0, 1.5, 2.0]
        # scale_search = [0.5]
        boxsize = 368
        stride = 8
        padValue = 128
        thre = 0.05
        multiplier = [x * boxsize / oriImg.shape[0] for x in scale_search]
        heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 22))
        # paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))

        for m in range(len(multiplier)):
            scale = multiplier[m]
            imageToTest = cv2.resize(
                oriImg,
                (0, 0),
                fx=scale,
                fy=scale,
                interpolation=cv2.INTER_CUBIC,
            )
            imageToTest_padded, pad = padRightDownCorner(
                imageToTest, stride, padValue
            )
            im = (
                np.transpose(
                    np.float32(imageToTest_padded[:, :, :, np.newaxis]),
                    (3, 2, 0, 1),
                )
                / 256
                - 0.5
            )
            im = np.ascontiguousarray(im)

            data = torch.from_numpy(im).float()
            if torch.cuda.is_available():
                data = data.cuda()
            # data = data.permute([2, 0, 1]).unsqueeze(0).float()
            with torch.no_grad():
                output = self.model(data).cpu().numpy()
                # output = self.model(data).numpy()q

            # extract outputs, resize, and remove padding
            heatmap = np.transpose(
                np.squeeze(output), (1, 2, 0)
            )  # output 1 is heatmaps
            heatmap = cv2.resize(
                heatmap,
                (0, 0),
                fx=stride,
                fy=stride,
                interpolation=cv2.INTER_CUBIC,
            )
            heatmap = heatmap[
                : imageToTest_padded.shape[0] - pad[2],
                : imageToTest_padded.shape[1] - pad[3],
                :,
            ]
            heatmap = cv2.resize(
                heatmap,
                (oriImg.shape[1], oriImg.shape[0]),
                interpolation=cv2.INTER_CUBIC,
            )

            heatmap_avg += heatmap / len(multiplier)

        all_peaks = []
        for part in range(21):
            map_ori = heatmap_avg[:, :, part]
            one_heatmap = gaussian_filter(map_ori, sigma=3)
            binary = np.ascontiguousarray(one_heatmap > thre, dtype=np.uint8)
            # 全部小于阈值
            if np.sum(binary) == 0:
                all_peaks.append([0, 0])
                continue
            label_img, label_numbers = label(
                binary, return_num=True, connectivity=binary.ndim
            )
            max_index = (
                np.argmax(
                    [
                        np.sum(map_ori[label_img == i])
                        for i in range(1, label_numbers + 1)
                    ]
                )
                + 1
            )
            label_img[label_img != max_index] = 0
            map_ori[label_img == 0] = 0

            y, x = npmax(map_ori)
            all_peaks.append([x, y])
        return np.array(all_peaks)

```

`apps/stable_diffusion/src/utils/stencils/openpose/openpose_util.py`:

```py
import math
import numpy as np
import matplotlib
import cv2
from collections import OrderedDict
import torch.nn as nn


def make_layers(block, no_relu_layers):
    layers = []
    for layer_name, v in block.items():
        if "pool" in layer_name:
            layer = nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2])
            layers.append((layer_name, layer))
        else:
            conv2d = nn.Conv2d(
                in_channels=v[0],
                out_channels=v[1],
                kernel_size=v[2],
                stride=v[3],
                padding=v[4],
            )
            layers.append((layer_name, conv2d))
            if layer_name not in no_relu_layers:
                layers.append(("relu_" + layer_name, nn.ReLU(inplace=True)))

    return nn.Sequential(OrderedDict(layers))


def padRightDownCorner(img, stride, padValue):
    h = img.shape[0]
    w = img.shape[1]

    pad = 4 * [None]
    pad[0] = 0  # up
    pad[1] = 0  # left
    pad[2] = 0 if (h % stride == 0) else stride - (h % stride)  # down
    pad[3] = 0 if (w % stride == 0) else stride - (w % stride)  # right

    img_padded = img
    pad_up = np.tile(img_padded[0:1, :, :] * 0 + padValue, (pad[0], 1, 1))
    img_padded = np.concatenate((pad_up, img_padded), axis=0)
    pad_left = np.tile(img_padded[:, 0:1, :] * 0 + padValue, (1, pad[1], 1))
    img_padded = np.concatenate((pad_left, img_padded), axis=1)
    pad_down = np.tile(img_padded[-2:-1, :, :] * 0 + padValue, (pad[2], 1, 1))
    img_padded = np.concatenate((img_padded, pad_down), axis=0)
    pad_right = np.tile(img_padded[:, -2:-1, :] * 0 + padValue, (1, pad[3], 1))
    img_padded = np.concatenate((img_padded, pad_right), axis=1)

    return img_padded, pad


# transfer caffe model to pytorch which will match the layer name
def transfer(model, model_weights):
    transfered_model_weights = {}
    for weights_name in model.state_dict().keys():
        transfered_model_weights[weights_name] = model_weights[
            ".".join(weights_name.split(".")[1:])
        ]
    return transfered_model_weights


# draw the body keypoint and lims
def draw_bodypose(canvas, candidate, subset):
    stickwidth = 4
    limbSeq = [
        [2, 3],
        [2, 6],
        [3, 4],
        [4, 5],
        [6, 7],
        [7, 8],
        [2, 9],
        [9, 10],
        [10, 11],
        [2, 12],
        [12, 13],
        [13, 14],
        [2, 1],
        [1, 15],
        [15, 17],
        [1, 16],
        [16, 18],
        [3, 17],
        [6, 18],
    ]

    colors = [
        [255, 0, 0],
        [255, 85, 0],
        [255, 170, 0],
        [255, 255, 0],
        [170, 255, 0],
        [85, 255, 0],
        [0, 255, 0],
        [0, 255, 85],
        [0, 255, 170],
        [0, 255, 255],
        [0, 170, 255],
        [0, 85, 255],
        [0, 0, 255],
        [85, 0, 255],
        [170, 0, 255],
        [255, 0, 255],
        [255, 0, 170],
        [255, 0, 85],
    ]
    for i in range(18):
        for n in range(len(subset)):
            index = int(subset[n][i])
            if index == -1:
                continue
            x, y = candidate[index][0:2]
            cv2.circle(canvas, (int(x), int(y)), 4, colors[i], thickness=-1)
    for i in range(17):
        for n in range(len(subset)):
            index = subset[n][np.array(limbSeq[i]) - 1]
            if -1 in index:
                continue
            cur_canvas = canvas.copy()
            Y = candidate[index.astype(int), 0]
            X = candidate[index.astype(int), 1]
            mX = np.mean(X)
            mY = np.mean(Y)
            length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5
            angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))
            polygon = cv2.ellipse2Poly(
                (int(mY), int(mX)),
                (int(length / 2), stickwidth),
                int(angle),
                0,
                360,
                1,
            )
            cv2.fillConvexPoly(cur_canvas, polygon, colors[i])
            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)
    return canvas


# image drawed by opencv is not good.
def draw_handpose(canvas, all_hand_peaks, show_number=False):
    edges = [
        [0, 1],
        [1, 2],
        [2, 3],
        [3, 4],
        [0, 5],
        [5, 6],
        [6, 7],
        [7, 8],
        [0, 9],
        [9, 10],
        [10, 11],
        [11, 12],
        [0, 13],
        [13, 14],
        [14, 15],
        [15, 16],
        [0, 17],
        [17, 18],
        [18, 19],
        [19, 20],
    ]

    for peaks in all_hand_peaks:
        for ie, e in enumerate(edges):
            if np.sum(np.all(peaks[e], axis=1) == 0) == 0:
                x1, y1 = peaks[e[0]]
                x2, y2 = peaks[e[1]]
                cv2.line(
                    canvas,
                    (x1, y1),
                    (x2, y2),
                    matplotlib.colors.hsv_to_rgb(
                        [ie / float(len(edges)), 1.0, 1.0]
                    )
                    * 255,
                    thickness=2,
                )

        for i, keyponit in enumerate(peaks):
            x, y = keyponit
            cv2.circle(canvas, (x, y), 4, (0, 0, 255), thickness=-1)
            if show_number:
                cv2.putText(
                    canvas,
                    str(i),
                    (x, y),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.3,
                    (0, 0, 0),
                    lineType=cv2.LINE_AA,
                )
    return canvas


# detect hand according to body pose keypoints
# please refer to https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/src/openpose/hand/handDetector.cpp
def handDetect(candidate, subset, oriImg):
    # right hand: wrist 4, elbow 3, shoulder 2
    # left hand: wrist 7, elbow 6, shoulder 5
    ratioWristElbow = 0.33
    detect_result = []
    image_height, image_width = oriImg.shape[0:2]
    for person in subset.astype(int):
        # if any of three not detected
        has_left = np.sum(person[[5, 6, 7]] == -1) == 0
        has_right = np.sum(person[[2, 3, 4]] == -1) == 0
        if not (has_left or has_right):
            continue
        hands = []
        # left hand
        if has_left:
            left_shoulder_index, left_elbow_index, left_wrist_index = person[
                [5, 6, 7]
            ]
            x1, y1 = candidate[left_shoulder_index][:2]
            x2, y2 = candidate[left_elbow_index][:2]
            x3, y3 = candidate[left_wrist_index][:2]
            hands.append([x1, y1, x2, y2, x3, y3, True])
        # right hand
        if has_right:
            (
                right_shoulder_index,
                right_elbow_index,
                right_wrist_index,
            ) = person[[2, 3, 4]]
            x1, y1 = candidate[right_shoulder_index][:2]
            x2, y2 = candidate[right_elbow_index][:2]
            x3, y3 = candidate[right_wrist_index][:2]
            hands.append([x1, y1, x2, y2, x3, y3, False])

        for x1, y1, x2, y2, x3, y3, is_left in hands:
            x = x3 + ratioWristElbow * (x3 - x2)
            y = y3 + ratioWristElbow * (y3 - y2)
            distanceWristElbow = math.sqrt((x3 - x2) ** 2 + (y3 - y2) ** 2)
            distanceElbowShoulder = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)
            width = 1.5 * max(distanceWristElbow, 0.9 * distanceElbowShoulder)
            # x-y refers to the center --> offset to topLeft point
            x -= width / 2
            y -= width / 2  # width = height
            # overflow the image
            if x < 0:
                x = 0
            if y < 0:
                y = 0
            width1 = width
            width2 = width
            if x + width > image_width:
                width1 = image_width - x
            if y + width > image_height:
                width2 = image_height - y
            width = min(width1, width2)
            # the max hand box value is 20 pixels
            if width >= 20:
                detect_result.append([int(x), int(y), int(width), is_left])

    """
    return value: [[x, y, w, True if left hand else False]].
    width=height since the network require squared input.
    x, y is the coordinate of top left 
    """
    return detect_result


# get max index of 2d array
def npmax(array):
    arrayindex = array.argmax(1)
    arrayvalue = array.max(1)
    i = arrayvalue.argmax()
    j = arrayindex[i]
    return (i,)

```

`apps/stable_diffusion/src/utils/stencils/stencil_utils.py`:

```py
import numpy as np
from PIL import Image
import torch
import os
from pathlib import Path
import torchvision
import time
from apps.stable_diffusion.src.utils.stencils import (
    CannyDetector,
    OpenposeDetector,
    ZoeDetector,
)

stencil = {}


def save_img(img):
    from apps.stable_diffusion.src.utils import (
        get_generated_imgs_path,
        get_generated_imgs_todays_subdir,
    )

    subdir = Path(
        get_generated_imgs_path(), get_generated_imgs_todays_subdir()
    )
    os.makedirs(subdir, exist_ok=True)
    if isinstance(img, Image.Image):
        img.save(
            os.path.join(
                subdir, "controlnet_" + str(int(time.time())) + ".png"
            )
        )
    elif isinstance(img, np.ndarray):
        img = Image.fromarray(img)
        img.save(os.path.join(subdir, str(int(time.time())) + ".png"))
    else:
        converter = torchvision.transforms.ToPILImage()
        for i in img:
            converter(i).save(
                os.path.join(subdir, str(int(time.time())) + ".png")
            )


def HWC3(x):
    assert x.dtype == np.uint8
    if x.ndim == 2:
        x = x[:, :, None]
    assert x.ndim == 3
    H, W, C = x.shape
    assert C == 1 or C == 3 or C == 4
    if C == 3:
        return x
    if C == 1:
        return np.concatenate([x, x, x], axis=2)
    if C == 4:
        color = x[:, :, 0:3].astype(np.float32)
        alpha = x[:, :, 3:4].astype(np.float32) / 255.0
        y = color * alpha + 255.0 * (1.0 - alpha)
        y = y.clip(0, 255).astype(np.uint8)
        return y


def controlnet_hint_shaping(
    controlnet_hint, height, width, dtype, num_images_per_prompt=1
):
    channels = 3
    if isinstance(controlnet_hint, torch.Tensor):
        # torch.Tensor: acceptble shape are any of chw, bchw(b==1) or bchw(b==num_images_per_prompt)
        shape_chw = (channels, height, width)
        shape_bchw = (1, channels, height, width)
        shape_nchw = (num_images_per_prompt, channels, height, width)
        if controlnet_hint.shape in [shape_chw, shape_bchw, shape_nchw]:
            controlnet_hint = controlnet_hint.to(
                dtype=dtype, device=torch.device("cpu")
            )
            if controlnet_hint.shape != shape_nchw:
                controlnet_hint = controlnet_hint.repeat(
                    num_images_per_prompt, 1, 1, 1
                )
            return controlnet_hint
        else:
            raise ValueError(
                f"Acceptble shape of `stencil` are any of ({channels}, {height}, {width}),"
                + f" (1, {channels}, {height}, {width}) or ({num_images_per_prompt}, "
                + f"{channels}, {height}, {width}) but is {controlnet_hint.shape}"
            )
    elif isinstance(controlnet_hint, np.ndarray):
        # np.ndarray: acceptable shape is any of hw, hwc, bhwc(b==1) or bhwc(b==num_images_per_promot)
        # hwc is opencv compatible image format. Color channel must be BGR Format.
        if controlnet_hint.shape == (height, width):
            controlnet_hint = np.repeat(
                controlnet_hint[:, :, np.newaxis], channels, axis=2
            )  # hw -> hwc(c==3)
        shape_hwc = (height, width, channels)
        shape_bhwc = (1, height, width, channels)
        shape_nhwc = (num_images_per_prompt, height, width, channels)
        if controlnet_hint.shape in [shape_hwc, shape_bhwc, shape_nhwc]:
            controlnet_hint = torch.from_numpy(controlnet_hint.copy())
            controlnet_hint = controlnet_hint.to(
                dtype=dtype, device=torch.device("cpu")
            )
            controlnet_hint /= 255.0
            if controlnet_hint.shape != shape_nhwc:
                controlnet_hint = controlnet_hint.repeat(
                    num_images_per_prompt, 1, 1, 1
                )
            controlnet_hint = controlnet_hint.permute(
                0, 3, 1, 2
            )  # b h w c -> b c h w
            return controlnet_hint
        else:
            raise ValueError(
                f"Acceptble shape of `stencil` are any of ({width}, {channels}), "
                + f"({height}, {width}, {channels}), "
                + f"(1, {height}, {width}, {channels}) or "
                + f"({num_images_per_prompt}, {channels}, {height}, {width}) but is {controlnet_hint.shape}"
            )
    elif isinstance(controlnet_hint, Image.Image):
        if controlnet_hint.size == (width, height):
            controlnet_hint = controlnet_hint.convert(
                "RGB"
            )  # make sure 3 channel RGB format
            controlnet_hint = np.array(controlnet_hint)  # to numpy
            controlnet_hint = controlnet_hint[:, :, ::-1]  # RGB -> BGR
            return controlnet_hint_shaping(
                controlnet_hint, height, width, num_images_per_prompt
            )
        else:
            raise ValueError(
                f"Acceptable image size of `stencil` is ({width}, {height}) but is {controlnet_hint.size}"
            )
    else:
        raise ValueError(
            f"Acceptable type of `stencil` are any of torch.Tensor, np.ndarray, PIL.Image.Image but is {type(controlnet_hint)}"
        )


def controlnet_hint_conversion(
    image, use_stencil, height, width, dtype, num_images_per_prompt=1
):
    controlnet_hint = None
    match use_stencil:
        case "canny":
            print("Detecting edge with canny")
            controlnet_hint = hint_canny(image)
        case "openpose":
            print("Detecting human pose")
            controlnet_hint = hint_openpose(image)
        case "scribble":
            print("Working with scribble")
            controlnet_hint = hint_scribble(image)
        case "zoedepth":
            print("Working with ZoeDepth")
            controlnet_hint = hint_zoedepth(image)
        case _:
            return None
    controlnet_hint = controlnet_hint_shaping(
        controlnet_hint, height, width, dtype, num_images_per_prompt
    )
    return controlnet_hint


stencil_to_model_id_map = {
    "canny": "lllyasviel/control_v11p_sd15_canny",
    "zoedepth": "lllyasviel/control_v11f1p_sd15_depth",
    "hed": "lllyasviel/sd-controlnet-hed",
    "mlsd": "lllyasviel/control_v11p_sd15_mlsd",
    "normal": "lllyasviel/control_v11p_sd15_normalbae",
    "openpose": "lllyasviel/control_v11p_sd15_openpose",
    "scribble": "lllyasviel/control_v11p_sd15_scribble",
    "seg": "lllyasviel/control_v11p_sd15_seg",
}


def get_stencil_model_id(use_stencil):
    if use_stencil in stencil_to_model_id_map:
        return stencil_to_model_id_map[use_stencil]
    return None


# Stencil 1. Canny
def hint_canny(
    image: Image.Image,
    low_threshold=100,
    high_threshold=200,
):
    with torch.no_grad():
        input_image = np.array(image)

        if not "canny" in stencil:
            stencil["canny"] = CannyDetector()
        detected_map = stencil["canny"](
            input_image, low_threshold, high_threshold
        )
        save_img(detected_map)
        detected_map = HWC3(detected_map)
        return detected_map


# Stencil 2. OpenPose.
def hint_openpose(
    image: Image.Image,
):
    with torch.no_grad():
        input_image = np.array(image)

        if not "openpose" in stencil:
            stencil["openpose"] = OpenposeDetector()

        detected_map, _ = stencil["openpose"](input_image)
        save_img(detected_map)
        detected_map = HWC3(detected_map)
        return detected_map


# Stencil 3. Scribble.
def hint_scribble(image: Image.Image):
    with torch.no_grad():
        input_image = np.array(image)

        detected_map = np.zeros_like(input_image, dtype=np.uint8)
        detected_map[np.min(input_image, axis=2) < 127] = 255
        save_img(detected_map)
        return detected_map


# Stencil 4. Depth (Only Zoe Preprocessing)
def hint_zoedepth(image: Image.Image):
    with torch.no_grad():
        input_image = np.array(image)

        if not "depth" in stencil:
            stencil["depth"] = ZoeDetector()

        detected_map = stencil["depth"](input_image)
        save_img(detected_map)
        detected_map = HWC3(detected_map)
        return detected_map

```

`apps/stable_diffusion/src/utils/stencils/zoe/__init__.py`:

```py
import numpy as np
import torch
from pathlib import Path
import requests


from einops import rearrange

remote_model_path = (
    "https://huggingface.co/lllyasviel/Annotators/resolve/main/ZoeD_M12_N.pt"
)


class ZoeDetector:
    def __init__(self):
        cwd = Path.cwd()
        ckpt_path = Path(cwd, "stencil_annotator")
        ckpt_path.mkdir(parents=True, exist_ok=True)
        modelpath = ckpt_path / "ZoeD_M12_N.pt"

        with requests.get(remote_model_path, stream=True) as r:
            r.raise_for_status()
            with open(modelpath, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)

        model = torch.hub.load(
            "monorimet/ZoeDepth:torch_update",
            "ZoeD_N",
            pretrained=False,
            force_reload=False,
        )
        model.load_state_dict(
            torch.load(modelpath, map_location=model.device)["model"]
        )
        model.eval()
        self.model = model

    def __call__(self, input_image):
        assert input_image.ndim == 3
        image_depth = input_image
        with torch.no_grad():
            image_depth = torch.from_numpy(image_depth).float()
            image_depth = image_depth / 255.0
            image_depth = rearrange(image_depth, "h w c -> 1 c h w")
            depth = self.model.infer(image_depth)

            depth = depth[0, 0].cpu().numpy()

            vmin = np.percentile(depth, 2)
            vmax = np.percentile(depth, 85)

            depth -= vmin
            depth /= vmax - vmin
            depth = 1.0 - depth
            depth_image = (depth * 255.0).clip(0, 255).astype(np.uint8)

            return depth_image

```

`apps/stable_diffusion/src/utils/utils.py`:

```py
import os
import gc
import json
import re
from PIL import PngImagePlugin
from PIL import Image
from datetime import datetime as dt
from csv import DictWriter
from pathlib import Path
import numpy as np
from random import (
    randint,
    seed as seed_random,
    getstate as random_getstate,
    setstate as random_setstate,
)
import tempfile
import torch
from safetensors.torch import load_file
from shark.shark_inference import SharkInference
from shark.shark_importer import import_with_fx, save_mlir
from shark.iree_utils.vulkan_utils import (
    set_iree_vulkan_runtime_flags,
    get_vulkan_target_triple,
    get_iree_vulkan_runtime_flags,
)
from shark.iree_utils.metal_utils import get_metal_target_triple
from shark.iree_utils.gpu_utils import get_cuda_sm_cc, get_iree_rocm_args
from apps.stable_diffusion.src.utils.stable_args import args
from apps.stable_diffusion.src.utils.resources import opt_flags
from apps.stable_diffusion.src.utils.sd_annotation import sd_model_annotation
import sys
from diffusers.pipelines.stable_diffusion.convert_from_ckpt import (
    download_from_original_stable_diffusion_ckpt,
    create_vae_diffusers_config,
    convert_ldm_vae_checkpoint,
)
import requests
from io import BytesIO
from omegaconf import OmegaConf
from cpuinfo import get_cpu_info


def get_extended_name(model_name):
    device = args.device.split("://", 1)[0]
    extended_name = "{}_{}".format(model_name, device)
    return extended_name


def get_vmfb_path_name(model_name):
    vmfb_path = os.path.join(os.getcwd(), model_name + ".vmfb")
    return vmfb_path


def _load_vmfb(shark_module, vmfb_path, model, precision):
    model = "vae" if "base_vae" in model or "vae_encode" in model else model
    model = "unet" if "stencil" in model else model
    model = "unet" if "unet512" in model else model
    precision = "fp32" if "clip" in model else precision
    extra_args = get_opt_flags(model, precision)
    shark_module.load_module(vmfb_path, extra_args=extra_args)
    return shark_module


def _compile_module(shark_module, model_name, extra_args=[]):
    if args.load_vmfb or args.save_vmfb:
        vmfb_path = get_vmfb_path_name(model_name)
        if args.load_vmfb and os.path.isfile(vmfb_path) and not args.save_vmfb:
            print(f"loading existing vmfb from: {vmfb_path}")
            shark_module.load_module(vmfb_path, extra_args=extra_args)
        else:
            if args.save_vmfb:
                print("Saving to {}".format(vmfb_path))
            else:
                print(
                    "No vmfb found. Compiling and saving to {}".format(
                        vmfb_path
                    )
                )
            path = shark_module.save_module(
                os.getcwd(), model_name, extra_args, debug=args.compile_debug
            )
            shark_module.load_module(path, extra_args=extra_args)
    else:
        shark_module.compile(extra_args)
    return shark_module


# Downloads the model from shark_tank and returns the shark_module.
def get_shark_model(tank_url, model_name, extra_args=None):
    if extra_args is None:
        extra_args = []
    from shark.parser import shark_args

    # Set local shark_tank cache directory.
    shark_args.local_tank_cache = args.local_tank_cache
    from shark.shark_downloader import download_model

    if "cuda" in args.device:
        shark_args.enable_tf32 = True

    mlir_model, func_name, inputs, golden_out = download_model(
        model_name,
        tank_url=tank_url,
        frontend="torch",
    )
    shark_module = SharkInference(
        mlir_model, device=args.device, mlir_dialect="tm_tensor"
    )
    return _compile_module(shark_module, model_name, extra_args)


# Converts the torch-module into a shark_module.
def compile_through_fx(
    model,
    inputs,
    extended_model_name,
    is_f16=False,
    f16_input_mask=None,
    use_tuned=False,
    save_dir="",
    debug=False,
    generate_vmfb=True,
    extra_args=None,
    base_model_id=None,
    model_name=None,
    precision=None,
    return_mlir=False,
    device=None,
):
    if extra_args is None:
        extra_args = []
    if not return_mlir and model_name is not None:
        vmfb_path = get_vmfb_path_name(extended_model_name)
        if os.path.isfile(vmfb_path):
            shark_module = SharkInference(mlir_module=None, device=args.device)
            return (
                _load_vmfb(shark_module, vmfb_path, model_name, precision),
                None,
            )

    from shark.parser import shark_args

    if "cuda" in args.device:
        shark_args.enable_tf32 = True

    (
        mlir_module,
        func_name,
    ) = import_with_fx(
        model=model,
        inputs=inputs,
        is_f16=is_f16,
        f16_input_mask=f16_input_mask,
        debug=debug,
        model_name=extended_model_name,
    )

    if use_tuned:
        if "vae" in extended_model_name.split("_")[0]:
            args.annotation_model = "vae"
        if (
            "unet" in model_name.split("_")[0]
            or "unet_512" in model_name.split("_")[0]
        ):
            args.annotation_model = "unet"
        mlir_module = sd_model_annotation(
            mlir_module, extended_model_name, base_model_id
        )

    if not os.path.isdir(save_dir):
        save_dir = ""

    mlir_module = save_mlir(
        mlir_module,
        model_name=extended_model_name,
        dir=save_dir,
    )
    shark_module = SharkInference(
        mlir_module,
        device=args.device if device is None else device,
        mlir_dialect="tm_tensor",
    )
    if generate_vmfb:
        return (
            _compile_module(shark_module, extended_model_name, extra_args),
            mlir_module,
        )

    gc.collect()


def set_iree_runtime_flags():
    # TODO: This function should be device-agnostic and piped properly
    # to general runtime driver init.
    vulkan_runtime_flags = get_iree_vulkan_runtime_flags()
    if args.enable_rgp:
        vulkan_runtime_flags += [
            f"--enable_rgp=true",
            f"--vulkan_debug_utils=true",
        ]
    if args.device_allocator_heap_key:
        vulkan_runtime_flags += [
            f"--device_allocator=caching:device_local={args.device_allocator_heap_key}",
        ]
    set_iree_vulkan_runtime_flags(flags=vulkan_runtime_flags)


def get_all_devices(driver_name):
    """
    Inputs: driver_name
    Returns a list of all the available devices for a given driver sorted by
    the iree path names of the device as in --list_devices option in iree.
    """
    from iree.runtime import get_driver

    driver = get_driver(driver_name)
    device_list_src = driver.query_available_devices()
    device_list_src.sort(key=lambda d: d["path"])
    return device_list_src


def get_device_mapping(driver, key_combination=3):
    """This method ensures consistent device ordering when choosing
    specific devices for execution
    Args:
        driver (str): execution driver (vulkan, cuda, rocm, etc)
        key_combination (int, optional): choice for mapping value for
            device name.
        1 : path
        2 : name
        3 : (name, path)
        Defaults to 3.
    Returns:
        dict: map to possible device names user can input mapped to desired
            combination of name/path.
    """
    from shark.iree_utils._common import iree_device_map

    driver = iree_device_map(driver)
    device_list = get_all_devices(driver)
    device_map = dict()

    def get_output_value(dev_dict):
        if key_combination == 1:
            return f"{driver}://{dev_dict['path']}"
        if key_combination == 2:
            return dev_dict["name"]
        if key_combination == 3:
            return dev_dict["name"], f"{driver}://{dev_dict['path']}"

    # mapping driver name to default device (driver://0)
    device_map[f"{driver}"] = get_output_value(device_list[0])
    for i, device in enumerate(device_list):
        # mapping with index
        device_map[f"{driver}://{i}"] = get_output_value(device)
        # mapping with full path
        device_map[f"{driver}://{device['path']}"] = get_output_value(device)
    return device_map


def map_device_to_name_path(device, key_combination=3):
    """Gives the appropriate device data (supported name/path) for user
        selected execution device
    Args:
        device (str): user
        key_combination (int, optional): choice for mapping value for
            device name.
        1 : path
        2 : name
        3 : (name, path)
        Defaults to 3.
    Raises:
        ValueError:
    Returns:
        str / tuple: returns the mapping str or tuple of mapping str for
        the device depending on key_combination value
    """
    driver = device.split("://")[0]
    device_map = get_device_mapping(driver, key_combination)
    try:
        device_mapping = device_map[device]
    except KeyError:
        raise ValueError(f"Device '{device}' is not a valid device.")
    return device_mapping


def set_init_device_flags():
    if "vulkan" in args.device:
        # set runtime flags for vulkan.
        set_iree_runtime_flags()

        # set triple flag to avoid multiple calls to get_vulkan_triple_flag
        device_name, args.device = map_device_to_name_path(args.device)
        if not args.iree_vulkan_target_triple:
            triple = get_vulkan_target_triple(device_name)
            if triple is not None:
                args.iree_vulkan_target_triple = triple
        print(
            f"Found device {device_name}. Using target triple "
            f"{args.iree_vulkan_target_triple}."
        )
    elif "cuda" in args.device:
        args.device = "cuda"
    elif "metal" in args.device:
        device_name, args.device = map_device_to_name_path(args.device)
        if not args.iree_metal_target_platform:
            triple = get_metal_target_triple(device_name)
            if triple is not None:
                args.iree_metal_target_platform = triple.split("-")[-1]
        print(
            f"Found device {device_name}. Using target triple "
            f"{args.iree_metal_target_platform}."
        )
    elif "cpu" in args.device:
        args.device = "cpu"

    # set max_length based on availability.
    if args.hf_model_id in [
        "Linaqruf/anything-v3.0",
        "wavymulder/Analog-Diffusion",
        "dreamlike-art/dreamlike-diffusion-1.0",
    ]:
        args.max_length = 77
    elif args.hf_model_id == "prompthero/openjourney":
        args.max_length = 64

    # Use tuned models in the case of fp16, vulkan rdna3 or cuda sm devices.
    if args.ckpt_loc != "":
        base_model_id = fetch_and_update_base_model_id(args.ckpt_loc)
    else:
        base_model_id = fetch_and_update_base_model_id(args.hf_model_id)
        if base_model_id == "":
            base_model_id = args.hf_model_id

    if (
        args.precision != "fp16"
        or args.height not in [512, 768]
        or (args.height == 512 and args.width not in [512, 768])
        or (args.height == 768 and args.width not in [512, 768])
        or args.batch_size != 1
        or ("vulkan" not in args.device and "cuda" not in args.device)
    ):
        args.use_tuned = False

    elif (
        args.height != args.width
        and "rdna2" in args.iree_vulkan_target_triple
        and base_model_id
        not in [
            "CompVis/stable-diffusion-v1-4",
            "runwayml/stable-diffusion-v1-5",
        ]
    ):
        args.use_tuned = False

    elif base_model_id not in [
        "Linaqruf/anything-v3.0",
        "dreamlike-art/dreamlike-diffusion-1.0",
        "prompthero/openjourney",
        "wavymulder/Analog-Diffusion",
        "stabilityai/stable-diffusion-2-1",
        "stabilityai/stable-diffusion-2-1-base",
        "CompVis/stable-diffusion-v1-4",
        "runwayml/stable-diffusion-v1-5",
        "runwayml/stable-diffusion-inpainting",
        "stabilityai/stable-diffusion-2-inpainting",
    ]:
        args.use_tuned = False

    elif "vulkan" in args.device and not any(
        x in args.iree_vulkan_target_triple for x in ["rdna2", "rdna3"]
    ):
        args.use_tuned = False

    elif "cuda" in args.device and get_cuda_sm_cc() not in ["sm_80", "sm_89"]:
        args.use_tuned = False

    elif args.use_base_vae and args.hf_model_id not in [
        "stabilityai/stable-diffusion-2-1-base",
        "CompVis/stable-diffusion-v1-4",
    ]:
        args.use_tuned = False

    elif (
        args.height == 768
        and args.width == 768
        and (
            base_model_id
            not in [
                "stabilityai/stable-diffusion-2-1",
                "stabilityai/stable-diffusion-2-1-base",
            ]
            or "rdna" not in args.iree_vulkan_target_triple
        )
    ):
        args.use_tuned = False

    elif "rdna2" in args.iree_vulkan_target_triple and (
        base_model_id
        not in [
            "stabilityai/stable-diffusion-2-1",
            "stabilityai/stable-diffusion-2-1-base",
            "CompVis/stable-diffusion-v1-4",
        ]
    ):
        args.use_tuned = False

    if args.use_tuned:
        print(
            f"Using tuned models for {base_model_id}(fp16) on "
            f"device {args.device}."
        )
    else:
        print("Tuned models are currently not supported for this setting.")

    # set import_mlir to True for unuploaded models.
    if args.ckpt_loc != "":
        args.import_mlir = True

    elif args.hf_model_id not in [
        "Linaqruf/anything-v3.0",
        "dreamlike-art/dreamlike-diffusion-1.0",
        "prompthero/openjourney",
        "wavymulder/Analog-Diffusion",
        "stabilityai/stable-diffusion-2-1",
        "stabilityai/stable-diffusion-2-1-base",
        "CompVis/stable-diffusion-v1-4",
    ]:
        args.import_mlir = True

    elif args.height != 512 or args.width != 512 or args.batch_size != 1:
        args.import_mlir = True

    elif args.use_tuned and args.hf_model_id in [
        "dreamlike-art/dreamlike-diffusion-1.0",
        "prompthero/openjourney",
        "stabilityai/stable-diffusion-2-1",
    ]:
        args.import_mlir = True

    elif (
        args.use_tuned
        and "vulkan" in args.device
        and "rdna2" in args.iree_vulkan_target_triple
    ):
        args.import_mlir = True

    elif (
        args.use_tuned
        and "cuda" in args.device
        and get_cuda_sm_cc() == "sm_89"
    ):
        args.import_mlir = True


# Utility to get list of devices available.
def get_available_devices():
    def get_devices_by_name(driver_name):
        from shark.iree_utils._common import iree_device_map

        device_list = []
        try:
            driver_name = iree_device_map(driver_name)
            device_list_dict = get_all_devices(driver_name)
            print(f"{driver_name} devices are available.")
        except:
            print(f"{driver_name} devices are not available.")
        else:
            cpu_name = get_cpu_info()["brand_raw"]
            for i, device in enumerate(device_list_dict):
                device_name = (
                    cpu_name if device["name"] == "default" else device["name"]
                )
                if "local" in driver_name:
                    device_list.append(
                        f"{device_name} => {driver_name.replace('local', 'cpu')}"
                    )
                else:
                    # for drivers with single devices
                    # let the default device be selected without any indexing
                    if len(device_list_dict) == 1:
                        device_list.append(f"{device_name} => {driver_name}")
                    else:
                        device_list.append(
                            f"{device_name} => {driver_name}://{i}"
                        )
        return device_list

    set_iree_runtime_flags()

    available_devices = []
    from shark.iree_utils.vulkan_utils import (
        get_all_vulkan_devices,
    )

    vulkaninfo_list = get_all_vulkan_devices()
    vulkan_devices = []
    id = 0
    for device in vulkaninfo_list:
        vulkan_devices.append(f"{device.strip()} => vulkan://{id}")
        id += 1
    if id != 0:
        print(f"vulkan devices are available.")
    available_devices.extend(vulkan_devices)
    metal_devices = get_devices_by_name("metal")
    available_devices.extend(metal_devices)
    cuda_devices = get_devices_by_name("cuda")
    available_devices.extend(cuda_devices)
    rocm_devices = get_devices_by_name("rocm")
    available_devices.extend(rocm_devices)
    cpu_device = get_devices_by_name("cpu-sync")
    available_devices.extend(cpu_device)
    cpu_device = get_devices_by_name("cpu-task")
    available_devices.extend(cpu_device)
    return available_devices


def disk_space_check(path, lim=20):
    from shutil import disk_usage

    du = disk_usage(path)
    free = du.free / (1024 * 1024 * 1024)
    if free <= lim:
        print(f"[WARNING] Only {free:.2f}GB space available in {path}.")


def get_opt_flags(model, precision="fp16"):
    iree_flags = []
    is_tuned = "tuned" if args.use_tuned else "untuned"
    if len(args.iree_vulkan_target_triple) > 0:
        iree_flags.append(
            f"-iree-vulkan-target-triple={args.iree_vulkan_target_triple}"
        )
    if "rocm" in args.device:
        rocm_args = get_iree_rocm_args()
        iree_flags.extend(rocm_args)
        print(iree_flags)
    if args.iree_constant_folding == False:
        iree_flags.append("--iree-opt-const-expr-hoisting=False")
        iree_flags.append(
            "--iree-codegen-linalg-max-constant-fold-elements=9223372036854775807"
        )
    if args.data_tiling == False:
        iree_flags.append("--iree-opt-data-tiling=False")

    if "default_compilation_flags" in opt_flags[model][is_tuned][precision]:
        iree_flags += opt_flags[model][is_tuned][precision][
            "default_compilation_flags"
        ]

    if "specified_compilation_flags" in opt_flags[model][is_tuned][precision]:
        device = (
            args.device
            if "://" not in args.device
            else args.device.split("://")[0]
        )
        if (
            device
            not in opt_flags[model][is_tuned][precision][
                "specified_compilation_flags"
            ]
        ):
            device = "default_device"
        iree_flags += opt_flags[model][is_tuned][precision][
            "specified_compilation_flags"
        ][device]
    if "vae" not in model:
        # Due to lack of support for multi-reduce, we always collapse reduction
        # dims before dispatch formation right now.
        iree_flags += ["--iree-flow-collapse-reduction-dims"]
    return iree_flags


def get_path_stem(path):
    path = Path(path)
    return path.stem


def get_path_to_diffusers_checkpoint(custom_weights):
    path = Path(custom_weights)
    diffusers_path = path.parent.absolute()
    diffusers_directory_name = os.path.join("diffusers", path.stem)
    complete_path_to_diffusers = diffusers_path / diffusers_directory_name
    complete_path_to_diffusers.mkdir(parents=True, exist_ok=True)
    path_to_diffusers = complete_path_to_diffusers.as_posix()
    return path_to_diffusers


def preprocessCKPT(custom_weights, is_inpaint=False):
    path_to_diffusers = get_path_to_diffusers_checkpoint(custom_weights)
    if next(Path(path_to_diffusers).iterdir(), None):
        print("Checkpoint already loaded at : ", path_to_diffusers)
        return
    else:
        print(
            "Diffusers' checkpoint will be identified here : ",
            path_to_diffusers,
        )
    from_safetensors = (
        True if custom_weights.lower().endswith(".safetensors") else False
    )
    # EMA weights usually yield higher quality images for inference but
    # non-EMA weights have been yielding better results in our case.
    # TODO: Add an option `--ema` (`--no-ema`) for users to specify if
    #  they want to go for EMA weight extraction or not.
    extract_ema = False
    print(
        "Loading diffusers' pipeline from original stable diffusion checkpoint"
    )
    num_in_channels = 9 if is_inpaint else 4
    pipe = download_from_original_stable_diffusion_ckpt(
        checkpoint_path_or_dict=custom_weights,
        extract_ema=extract_ema,
        from_safetensors=from_safetensors,
        num_in_channels=num_in_channels,
    )
    pipe.save_pretrained(path_to_diffusers)
    print("Loading complete")


def convert_original_vae(vae_checkpoint):
    vae_state_dict = {}
    for key in list(vae_checkpoint.keys()):
        vae_state_dict["first_stage_model." + key] = vae_checkpoint.get(key)

    config_url = (
        "https://raw.githubusercontent.com/CompVis/stable-diffusion/"
        "main/configs/stable-diffusion/v1-inference.yaml"
    )
    original_config_file = BytesIO(requests.get(config_url).content)
    original_config = OmegaConf.load(original_config_file)
    vae_config = create_vae_diffusers_config(original_config, image_size=512)

    converted_vae_checkpoint = convert_ldm_vae_checkpoint(
        vae_state_dict, vae_config
    )
    return converted_vae_checkpoint


def processLoRA(model, use_lora, splitting_prefix):
    state_dict = ""
    if ".safetensors" in use_lora:
        state_dict = load_file(use_lora)
    else:
        state_dict = torch.load(use_lora)
    alpha = 0.75
    visited = []

    # directly update weight in model
    process_unet = "te" not in splitting_prefix
    for key in state_dict:
        if ".alpha" in key or key in visited:
            continue

        curr_layer = model
        if ("text" not in key and process_unet) or (
            "text" in key and not process_unet
        ):
            layer_infos = (
                key.split(".")[0].split(splitting_prefix)[-1].split("_")
            )
        else:
            continue

        # find the target layer
        temp_name = layer_infos.pop(0)
        while len(layer_infos) > -1:
            try:
                curr_layer = curr_layer.__getattr__(temp_name)
                if len(layer_infos) > 0:
                    temp_name = layer_infos.pop(0)
                elif len(layer_infos) == 0:
                    break
            except Exception:
                if len(temp_name) > 0:
                    temp_name += "_" + layer_infos.pop(0)
                else:
                    temp_name = layer_infos.pop(0)

        pair_keys = []
        if "lora_down" in key:
            pair_keys.append(key.replace("lora_down", "lora_up"))
            pair_keys.append(key)
        else:
            pair_keys.append(key)
            pair_keys.append(key.replace("lora_up", "lora_down"))

        # update weight
        if len(state_dict[pair_keys[0]].shape) == 4:
            weight_up = (
                state_dict[pair_keys[0]]
                .squeeze(3)
                .squeeze(2)
                .to(torch.float32)
            )
            weight_down = (
                state_dict[pair_keys[1]]
                .squeeze(3)
                .squeeze(2)
                .to(torch.float32)
            )
            curr_layer.weight.data += alpha * torch.mm(
                weight_up, weight_down
            ).unsqueeze(2).unsqueeze(3)
        else:
            weight_up = state_dict[pair_keys[0]].to(torch.float32)
            weight_down = state_dict[pair_keys[1]].to(torch.float32)
            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)
        # update visited list
        for item in pair_keys:
            visited.append(item)
    return model


def update_lora_weight_for_unet(unet, use_lora):
    extensions = [".bin", ".safetensors", ".pt"]
    if not any([extension in use_lora for extension in extensions]):
        # We assume if it is a HF ID with standalone LoRA weights.
        unet.load_attn_procs(use_lora)
        return unet

    main_file_name = get_path_stem(use_lora)
    if ".bin" in use_lora:
        main_file_name += ".bin"
    elif ".safetensors" in use_lora:
        main_file_name += ".safetensors"
    elif ".pt" in use_lora:
        main_file_name += ".pt"
    else:
        sys.exit("Only .bin and .safetensors format for LoRA is supported")

    try:
        dir_name = os.path.dirname(use_lora)
        unet.load_attn_procs(dir_name, weight_name=main_file_name)
        return unet
    except:
        return processLoRA(unet, use_lora, "lora_unet_")


def update_lora_weight(model, use_lora, model_name):
    if "unet" in model_name:
        return update_lora_weight_for_unet(model, use_lora)
    try:
        return processLoRA(model, use_lora, "lora_te_")
    except:
        return None


# `fetch_and_update_base_model_id` is a resource utility function which
# helps to maintain mapping of the model to run with its base model.
# If `base_model` is "", then this function tries to fetch the base model
# info for the `model_to_run`.
def fetch_and_update_base_model_id(model_to_run, base_model=""):
    variants_path = os.path.join(os.getcwd(), "variants.json")
    data = {model_to_run: base_model}
    json_data = {}
    if os.path.exists(variants_path):
        with open(variants_path, "r", encoding="utf-8") as jsonFile:
            json_data = json.load(jsonFile)
            # Return with base_model's info if base_model is "".
            if base_model == "":
                if model_to_run in json_data:
                    base_model = json_data[model_to_run]
                return base_model
    elif base_model == "":
        return base_model
    # Update JSON data to contain an entry mapping model_to_run with
    # base_model.
    json_data.update(data)
    with open(variants_path, "w", encoding="utf-8") as jsonFile:
        json.dump(json_data, jsonFile)


# Generate and return a new seed if the provided one is not in the
# supported range (including -1)
def sanitize_seed(seed: int | str):
    seed = int(seed)
    uint32_info = np.iinfo(np.uint32)
    uint32_min, uint32_max = uint32_info.min, uint32_info.max
    if seed < uint32_min or seed >= uint32_max:
        seed = randint(uint32_min, uint32_max)
    return seed


# take a seed expression in an input format and convert it to
# a list of integers, where possible
def parse_seed_input(seed_input: str | list | int):
    if isinstance(seed_input, str):
        try:
            seed_input = json.loads(seed_input)
        except (ValueError, TypeError):
            seed_input = None

    if isinstance(seed_input, int):
        return [seed_input]

    if isinstance(seed_input, list) and all(
        type(seed) is int for seed in seed_input
    ):
        return seed_input

    raise TypeError(
        "Seed input must be an integer or an array of integers in JSON format"
    )


# Generate a set of seeds from an input expression for batch_count batches,
# optionally using that input as the rng seed for any randomly generated seeds.
def batch_seeds(
    seed_input: str | list | int, batch_count: int, repeatable=False
):
    # turn the input into a list if possible
    seeds = parse_seed_input(seed_input)

    # slice or pad the list to be of batch_count length
    seeds = seeds[:batch_count] + [-1] * (batch_count - len(seeds))

    if repeatable:
        if all(seed < 0 for seed in seeds):
            seeds[0] = sanitize_seed(seeds[0])

        # set seed for the rng based on what we have so far
        saved_random_state = random_getstate()
        seed_random(str([n for n in seeds if n > -1]))

    # generate any seeds that are unspecified
    seeds = [sanitize_seed(seed) for seed in seeds]

    if repeatable:
        # reset the rng back to normal
        random_setstate(saved_random_state)

    return seeds


# clear all the cached objects to recompile cleanly.
def clear_all():
    print("CLEARING ALL, EXPECT SEVERAL MINUTES TO RECOMPILE")
    from glob import glob
    import shutil

    vmfbs = glob(os.path.join(os.getcwd(), "*.vmfb"))
    for vmfb in vmfbs:
        if os.path.exists(vmfb):
            os.remove(vmfb)
    # Temporary workaround of deleting yaml files to incorporate
    # diffusers' pipeline.
    # TODO: Remove this once we have better weight updation logic.
    inference_yaml = ["v2-inference-v.yaml", "v1-inference.yaml"]
    for yaml in inference_yaml:
        if os.path.exists(yaml):
            os.remove(yaml)
    home = os.path.expanduser("~")
    if os.name == "nt":  # Windows
        appdata = os.getenv("LOCALAPPDATA")
        shutil.rmtree(os.path.join(appdata, "AMD/VkCache"), ignore_errors=True)
        shutil.rmtree(
            os.path.join(home, ".local/shark_tank"), ignore_errors=True
        )
    elif os.name == "unix":
        shutil.rmtree(os.path.join(home, ".cache/AMD/VkCache"))
        shutil.rmtree(os.path.join(home, ".local/shark_tank"))
    if args.local_tank_cache != "":
        shutil.rmtree(args.local_tank_cache)


def get_generated_imgs_path() -> Path:
    return Path(
        args.output_dir if args.output_dir else Path.cwd(), "generated_imgs"
    )


def get_generated_imgs_todays_subdir() -> str:
    return dt.now().strftime("%Y%m%d")


# save output images and the inputs corresponding to it.
def save_output_img(output_img, img_seed, extra_info=None):
    if extra_info is None:
        extra_info = {}
    generated_imgs_path = Path(
        get_generated_imgs_path(), get_generated_imgs_todays_subdir()
    )
    generated_imgs_path.mkdir(parents=True, exist_ok=True)
    csv_path = Path(generated_imgs_path, "imgs_details.csv")

    prompt_slice = re.sub("[^a-zA-Z0-9]", "_", args.prompts[0][:15])
    out_img_name = f"{dt.now().strftime('%H%M%S')}_{prompt_slice}_{img_seed}"

    img_model = args.hf_model_id
    if args.ckpt_loc:
        img_model = Path(os.path.basename(args.ckpt_loc)).stem

    img_vae = None
    if args.custom_vae:
        img_vae = Path(os.path.basename(args.custom_vae)).stem

    img_lora = None
    if args.use_lora:
        img_lora = Path(os.path.basename(args.use_lora)).stem

    if args.output_img_format == "jpg":
        out_img_path = Path(generated_imgs_path, f"{out_img_name}.jpg")
        output_img.save(out_img_path, quality=95, subsampling=0)
    else:
        out_img_path = Path(generated_imgs_path, f"{out_img_name}.png")
        pngInfo = PngImagePlugin.PngInfo()

        if args.write_metadata_to_png:
            # Using a conditional expression caused problems, so setting a new
            # variable for now.
            if args.use_hiresfix:
                png_size_text = f"{args.hiresfix_width}x{args.hiresfix_height}"
            else:
                png_size_text = f"{args.width}x{args.height}"

            pngInfo.add_text(
                "parameters",
                f"{args.prompts[0]}"
                f"\nNegative prompt: {args.negative_prompts[0]}"
                f"\nSteps: {args.steps},"
                f"Sampler: {args.scheduler}, "
                f"CFG scale: {args.guidance_scale}, "
                f"Seed: {img_seed},"
                f"Size: {png_size_text}, "
                f"Model: {img_model}, "
                f"VAE: {img_vae}, "
                f"LoRA: {img_lora}",
            )

        output_img.save(out_img_path, "PNG", pnginfo=pngInfo)

        if args.output_img_format not in ["png", "jpg"]:
            print(
                f"[ERROR] Format {args.output_img_format} is not "
                f"supported yet. Image saved as png instead."
                f"Supported formats: png / jpg"
            )

    # To be as low-impact as possible to the existing CSV format, we append
    # "VAE" and "LORA" to the end. However, it does not fit the hierarchy of
    # importance for each data point. Something to consider.
    new_entry = {
        "VARIANT": img_model,
        "SCHEDULER": args.scheduler,
        "PROMPT": args.prompts[0],
        "NEG_PROMPT": args.negative_prompts[0],
        "SEED": img_seed,
        "CFG_SCALE": args.guidance_scale,
        "PRECISION": args.precision,
        "STEPS": args.steps,
        "HEIGHT": args.height
        if not args.use_hiresfix
        else args.hiresfix_height,
        "WIDTH": args.width if not args.use_hiresfix else args.hiresfix_width,
        "MAX_LENGTH": args.max_length,
        "OUTPUT": out_img_path,
        "VAE": img_vae,
        "LORA": img_lora,
    }

    new_entry.update(extra_info)

    csv_mode = "a" if os.path.isfile(csv_path) else "w"
    with open(csv_path, csv_mode, encoding="utf-8") as csv_obj:
        dictwriter_obj = DictWriter(csv_obj, fieldnames=list(new_entry.keys()))
        if csv_mode == "w":
            dictwriter_obj.writeheader()
        dictwriter_obj.writerow(new_entry)
        csv_obj.close()

    if args.save_metadata_to_json:
        del new_entry["OUTPUT"]
        json_path = Path(generated_imgs_path, f"{out_img_name}.json")
        with open(json_path, "w") as f:
            json.dump(new_entry, f, indent=4)


def get_generation_text_info(seeds, device):
    text_output = f"prompt={args.prompts}"
    text_output += f"\nnegative prompt={args.negative_prompts}"
    text_output += (
        f"\nmodel_id={args.hf_model_id}, " f"ckpt_loc={args.ckpt_loc}"
    )
    text_output += f"\nscheduler={args.scheduler}, " f"device={device}"
    text_output += (
        f"\nsteps={args.steps}, "
        f"guidance_scale={args.guidance_scale}, "
        f"seed={seeds}"
    )
    text_output += (
        f"\nsize={args.height}x{args.width}, "
        if not args.use_hiresfix
        else f"\nsize={args.hiresfix_height}x{args.hiresfix_width}, "
    )
    text_output += (
        f"batch_count={args.batch_count}, "
        f"batch_size={args.batch_size}, "
        f"max_length={args.max_length}"
    )

    return text_output


# For stencil, the input image can be of any size, but we need to ensure that
# it conforms with our model constraints :-
#   Both width and height should be in the range of [128, 768] and multiple of 8.
# This utility function performs the transformation on the input image while
# also maintaining the aspect ratio before sending it to the stencil pipeline.
def resize_stencil(image: Image.Image):
    width, height = image.size
    aspect_ratio = width / height
    min_size = min(width, height)
    if min_size < 128:
        n_size = 128
        if width == min_size:
            width = n_size
            height = n_size / aspect_ratio
        else:
            height = n_size
            width = n_size * aspect_ratio
    width = int(width)
    height = int(height)
    n_width = width // 8
    n_height = height // 8
    n_width *= 8
    n_height *= 8

    min_size = min(width, height)
    if min_size > 768:
        n_size = 768
        if width == min_size:
            height = n_size
            width = n_size * aspect_ratio
        else:
            width = n_size
            height = n_size / aspect_ratio
    width = int(width)
    height = int(height)
    n_width = width // 8
    n_height = height // 8
    n_width *= 8
    n_height *= 8
    new_image = image.resize((n_width, n_height))
    return new_image, n_width, n_height

```

`apps/stable_diffusion/stable_diffusion_telegram_bot.md`:

```md
You need to pre-create your bot (https://core.telegram.org/bots#how-do-i-create-a-bot)
Then create in the directory web file .env
In it the record:
TG_TOKEN="your_token"
specifying your bot's token from previous step.
Then run telegram_bot.py with the same parameters that you use when running index.py, for example:
python telegram_bot.py --max_length=77 --vulkan_large_heap_block_size=0 --use_base_vae --local_tank_cache h:\shark\TEMP

Bot commands:
/select_model
/select_scheduler
/set_steps "integer number of steps"
/set_guidance_scale "integer number"
/set_negative_prompt "negative text"
Any other text triggers the creation of an image based on it.

```

`apps/stable_diffusion/studio_bundle.spec`:

```spec
# -*- mode: python ; coding: utf-8 -*-
from apps.stable_diffusion.shark_studio_imports import pathex, datas, hiddenimports

binaries = []

block_cipher = None

a = Analysis(
    ['web\\index.py'],
    pathex=pathex,
    binaries=binaries,
    datas=datas,
    hiddenimports=hiddenimports,
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)
pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

exe = EXE(
    pyz,
    a.scripts,
    [],
    exclude_binaries=True,
    name='studio_bundle',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)
coll = COLLECT(
    exe,
    a.binaries,
    a.zipfiles,
    a.datas,
    strip=False,
    upx=True,
    upx_exclude=[],
    name='studio_bundle',
)

```

`apps/stable_diffusion/web/api/__init__.py`:

```py
from apps.stable_diffusion.web.api.sdapi_v1 import sdapi

```

`apps/stable_diffusion/web/api/sdapi_v1.py`:

```py
import os

from collections import defaultdict
from enum import Enum
from fastapi import FastAPI
from pydantic import BaseModel, Field, conlist, model_validator

from apps.stable_diffusion.web.api.utils import (
    frozen_args,
    sampler_aliases,
    encode_pil_to_base64,
    decode_base64_to_image,
    get_model_from_request,
    get_scheduler_from_request,
    get_lora_params,
    get_device,
    GenerationInputData,
    GenerationResponseData,
)

from apps.stable_diffusion.web.ui.utils import (
    get_custom_model_files,
    get_custom_model_pathfile,
    predefined_models,
    predefined_paint_models,
    predefined_upscaler_models,
    scheduler_list,
)
from apps.stable_diffusion.web.ui.txt2img_ui import txt2img_inf
from apps.stable_diffusion.web.ui.img2img_ui import img2img_inf
from apps.stable_diffusion.web.ui.inpaint_ui import inpaint_inf
from apps.stable_diffusion.web.ui.outpaint_ui import outpaint_inf
from apps.stable_diffusion.web.ui.upscaler_ui import upscaler_inf

sdapi = FastAPI()


# Rest API: /sdapi/v1/sd-models (lists available models)
class AppParam(str, Enum):
    txt2img = "txt2img"
    img2img = "img2img"
    inpaint = "inpaint"
    outpaint = "outpaint"
    upscaler = "upscaler"


@sdapi.get(
    "/v1/sd-models",
    summary="lists available models",
    description=(
        "This is all the models that this server currently knows about.\n "
        "Models listed may still have a compilation and build pending that "
        "will be triggered the first time they are used."
    ),
)
def sd_models_api(app: AppParam = frozen_args.app):
    match app:
        case "inpaint" | "outpaint":
            checkpoint_type = "inpainting"
            predefined = predefined_paint_models
        case "upscaler":
            checkpoint_type = "upscaler"
            predefined = predefined_upscaler_models
        case _:
            checkpoint_type = ""
            predefined = predefined_models

    return [
        {
            "title": model_file,
            "model_name": model_file,
            "hash": None,
            "sha256": None,
            "filename": get_custom_model_pathfile(model_file),
            "config": None,
        }
        for model_file in get_custom_model_files(
            custom_checkpoint_type=checkpoint_type
        )
    ] + [
        {
            "title": model,
            "model_name": model,
            "hash": None,
            "sha256": None,
            "filename": None,
            "config": None,
        }
        for model in predefined
    ]


# Rest API: /sdapi/v1/samplers (lists schedulers)
@sdapi.get(
    "/v1/samplers",
    summary="lists available schedulers/samplers",
    description=(
        "These are all the Schedulers defined and available. Not "
        "every scheduler is compatible with all apis. Aliases are "
        "equivalent samplers in A1111 if they are known."
    ),
)
def sd_samplers_api():
    reverse_sampler_aliases = defaultdict(list)
    for key, value in sampler_aliases.items():
        reverse_sampler_aliases[value].append(key)

    return (
        {
            "name": scheduler,
            "aliases": reverse_sampler_aliases.get(scheduler, []),
            "options": {},
        }
        for scheduler in scheduler_list
    )


# Rest API: /sdapi/v1/options (lists application level options)
@sdapi.get(
    "/v1/options",
    summary="lists current settings of application level options",
    description=(
        "A subset of the command line arguments set at startup renamed "
        "to correspond to the A1111 naming. Only a small subset of A1111 "
        "options are returned."
    ),
)
def options_api():
    # This is mostly just enough to support what Koboldcpp wants, with a
    # few other things that seemed obvious
    return {
        "samples_save": True,
        "samples_format": frozen_args.output_img_format,
        "sd_model_checkpoint": os.path.basename(frozen_args.ckpt_loc)
        if frozen_args.ckpt_loc
        else frozen_args.hf_model_id,
        "sd_lora": frozen_args.use_lora,
        "sd_vae": frozen_args.custom_vae or "Automatic",
        "enable_pnginfo": frozen_args.write_metadata_to_png,
    }


# Rest API: /sdapi/v1/cmd-flags (lists command line argument settings)
@sdapi.get(
    "/v1/cmd-flags",
    summary="lists the command line arguments value that were set on startup.",
)
def cmd_flags_api():
    return vars(frozen_args)


# Rest API: /sdapi/v1/txt2img (Text to image)
class ModelOverrideSettings(BaseModel):
    sd_model_checkpoint: str = get_model_from_request(
        fallback_model="stabilityai/stable-diffusion-2-1-base"
    )


class Txt2ImgInputData(GenerationInputData):
    enable_hr: bool = frozen_args.use_hiresfix
    hr_resize_y: int = Field(
        default=frozen_args.hiresfix_height, ge=128, le=768, multiple_of=8
    )
    hr_resize_x: int = Field(
        default=frozen_args.hiresfix_width, ge=128, le=768, multiple_of=8
    )
    override_settings: ModelOverrideSettings = None


@sdapi.post(
    "/v1/txt2img",
    summary="Does text to image generation",
    response_model=GenerationResponseData,
)
def txt2img_api(InputData: Txt2ImgInputData):
    model_id = get_model_from_request(
        InputData,
        fallback_model="stabilityai/stable-diffusion-2-1-base",
    )
    scheduler = get_scheduler_from_request(
        InputData, "txt2img_hires" if InputData.enable_hr else "txt2img"
    )
    (lora_weights, lora_hf_id) = get_lora_params(frozen_args.use_lora)

    print(
        f"Prompt: {InputData.prompt}, "
        f"Negative Prompt: {InputData.negative_prompt}, "
        f"Seed: {InputData.seed},"
        f"Model: {model_id}, "
        f"Scheduler: {scheduler}. "
    )

    res = txt2img_inf(
        InputData.prompt,
        InputData.negative_prompt,
        InputData.height,
        InputData.width,
        InputData.steps,
        InputData.cfg_scale,
        InputData.seed,
        batch_count=InputData.n_iter,
        batch_size=1,
        scheduler=scheduler,
        model_id=model_id,
        custom_vae=frozen_args.custom_vae or "None",
        precision="fp16",
        device=get_device(frozen_args.device),
        max_length=frozen_args.max_length,
        save_metadata_to_json=frozen_args.save_metadata_to_json,
        save_metadata_to_png=frozen_args.write_metadata_to_png,
        lora_weights=lora_weights,
        lora_hf_id=lora_hf_id,
        ondemand=frozen_args.ondemand,
        repeatable_seeds=False,
        use_hiresfix=InputData.enable_hr,
        hiresfix_height=InputData.hr_resize_y,
        hiresfix_width=InputData.hr_resize_x,
        hiresfix_strength=frozen_args.hiresfix_strength,
        resample_type=frozen_args.resample_type,
    )

    # Since we're not streaming we just want the last generator result
    for items_so_far in res:
        items = items_so_far

    return {
        "images": encode_pil_to_base64(items[0]),
        "parameters": {},
        "info": items[1],
    }


# Rest API: /sdapi/v1/img2img (Image to image)
class StencilParam(str, Enum):
    canny = "canny"
    openpose = "openpose"
    scribble = "scribble"
    zoedepth = "zoedepth"


class Img2ImgInputData(GenerationInputData):
    init_images: conlist(str, min_length=1, max_length=2)
    denoising_strength: float = frozen_args.strength
    use_stencil: StencilParam = frozen_args.use_stencil
    override_settings: ModelOverrideSettings = None

    @model_validator(mode="after")
    def check_image_supplied_for_scribble_stencil(self) -> "Img2ImgInputData":
        if (
            self.use_stencil == StencilParam.scribble
            and len(self.init_images) < 2
        ):
            raise ValueError(
                "a second image must be supplied for the controlnet:scribble stencil"
            )

        return self


@sdapi.post(
    "/v1/img2img",
    summary="Does image to image generation",
    response_model=GenerationResponseData,
)
def img2img_api(
    InputData: Img2ImgInputData,
):
    model_id = get_model_from_request(
        InputData,
        fallback_model="stabilityai/stable-diffusion-2-1-base",
    )
    scheduler = get_scheduler_from_request(InputData, "img2img")
    (lora_weights, lora_hf_id) = get_lora_params(frozen_args.use_lora)

    init_image = decode_base64_to_image(InputData.init_images[0])
    mask_image = (
        decode_base64_to_image(InputData.init_images[1])
        if len(InputData.init_images) > 1
        else None
    )

    print(
        f"Prompt: {InputData.prompt}, "
        f"Negative Prompt: {InputData.negative_prompt}, "
        f"Seed: {InputData.seed}, "
        f"Model: {model_id}, "
        f"Scheduler: {scheduler}."
    )

    res = img2img_inf(
        InputData.prompt,
        InputData.negative_prompt,
        {"image": init_image, "mask": mask_image},
        InputData.height,
        InputData.width,
        InputData.steps,
        InputData.denoising_strength,
        InputData.cfg_scale,
        InputData.seed,
        batch_count=InputData.n_iter,
        batch_size=1,
        scheduler=scheduler,
        model_id=model_id,
        custom_vae=frozen_args.custom_vae or "None",
        precision="fp16",
        device=get_device(frozen_args.device),
        max_length=frozen_args.max_length,
        use_stencil=InputData.use_stencil,
        save_metadata_to_json=frozen_args.save_metadata_to_json,
        save_metadata_to_png=frozen_args.write_metadata_to_png,
        lora_weights=lora_weights,
        lora_hf_id=lora_hf_id,
        ondemand=frozen_args.ondemand,
        repeatable_seeds=False,
        resample_type=frozen_args.resample_type,
    )

    # Since we're not streaming we just want the last generator result
    for items_so_far in res:
        items = items_so_far

    return {
        "images": encode_pil_to_base64(items[0]),
        "parameters": {},
        "info": items[1],
    }


# Rest API: /sdapi/v1/inpaint (Inpainting)
class PaintModelOverideSettings(BaseModel):
    sd_model_checkpoint: str = get_model_from_request(
        checkpoint_type="inpainting",
        fallback_model="stabilityai/stable-diffusion-2-inpainting",
    )


class InpaintInputData(GenerationInputData):
    image: str = Field(description="Base64 encoded input image")
    mask: str = Field(description="Base64 encoded mask image")
    is_full_res: bool = False  # Is this setting backwards in the UI?
    full_res_padding: int = Field(default=32, ge=0, le=256, multiple_of=4)
    denoising_strength: float = frozen_args.strength
    use_stencil: StencilParam = frozen_args.use_stencil
    override_settings: PaintModelOverideSettings = None


@sdapi.post(
    "/v1/inpaint",
    summary="Does inpainting generation on an image",
    response_model=GenerationResponseData,
)
def inpaint_api(
    InputData: InpaintInputData,
):
    model_id = get_model_from_request(
        InputData,
        checkpoint_type="inpainting",
        fallback_model="stabilityai/stable-diffusion-2-inpainting",
    )
    scheduler = get_scheduler_from_request(InputData, "inpaint")
    (lora_weights, lora_hf_id) = get_lora_params(frozen_args.use_lora)

    init_image = decode_base64_to_image(InputData.image)
    mask = decode_base64_to_image(InputData.mask)

    print(
        f"Prompt: {InputData.prompt}, "
        f'Negative Prompt: {InputData.negative_prompt}", '
        f'Seed: {InputData.seed}", '
        f"Model: {model_id}, "
        f"Scheduler: {scheduler}."
    )

    res = inpaint_inf(
        InputData.prompt,
        InputData.negative_prompt,
        {"image": init_image, "mask": mask},
        InputData.height,
        InputData.width,
        InputData.is_full_res,
        InputData.full_res_padding,
        InputData.steps,
        InputData.cfg_scale,
        InputData.seed,
        batch_count=InputData.n_iter,
        batch_size=1,
        scheduler=scheduler,
        model_id=model_id,
        custom_vae=frozen_args.custom_vae or "None",
        precision="fp16",
        device=get_device(frozen_args.device),
        max_length=frozen_args.max_length,
        save_metadata_to_json=frozen_args.save_metadata_to_json,
        save_metadata_to_png=frozen_args.write_metadata_to_png,
        lora_weights=lora_weights,
        lora_hf_id=lora_hf_id,
        ondemand=frozen_args.ondemand,
        repeatable_seeds=False,
    )

    # Since we're not streaming we just want the last generator result
    for items_so_far in res:
        items = items_so_far

    return {
        "images": encode_pil_to_base64(items[0]),
        "parameters": {},
        "info": items[1],
    }


# Rest API: /sdapi/v1/outpaint (Outpainting)
class DirectionParam(str, Enum):
    left = "left"
    right = "right"
    up = "up"
    down = "down"


class OutpaintInputData(GenerationInputData):
    init_images: list[str]
    pixels: int = Field(
        default=frozen_args.pixels, ge=8, le=256, multiple_of=8
    )
    mask_blur: int = Field(default=frozen_args.mask_blur, ge=0, le=64)
    directions: set[DirectionParam] = [
        direction
        for direction in ["left", "right", "up", "down"]
        if vars(frozen_args)[direction]
    ]
    noise_q: float = frozen_args.noise_q
    color_variation: float = frozen_args.color_variation
    override_settings: PaintModelOverideSettings = None


@sdapi.post(
    "/v1/outpaint",
    summary="Does outpainting generation on an image",
    response_model=GenerationResponseData,
)
def outpaint_api(
    InputData: OutpaintInputData,
):
    model_id = get_model_from_request(
        InputData,
        checkpoint_type="inpainting",
        fallback_model="stabilityai/stable-diffusion-2-inpainting",
    )
    scheduler = get_scheduler_from_request(InputData, "outpaint")
    (lora_weights, lora_hf_id) = get_lora_params(frozen_args.use_lora)

    init_image = decode_base64_to_image(InputData.init_images[0])

    print(
        f"Prompt: {InputData.prompt}, "
        f"Negative Prompt: {InputData.negative_prompt}, "
        f"Seed: {InputData.seed}, "
        f"Model: {model_id}, "
        f"Scheduler: {scheduler}."
    )

    res = outpaint_inf(
        InputData.prompt,
        InputData.negative_prompt,
        init_image,
        InputData.pixels,
        InputData.mask_blur,
        InputData.directions,
        InputData.noise_q,
        InputData.color_variation,
        InputData.height,
        InputData.width,
        InputData.steps,
        InputData.cfg_scale,
        InputData.seed,
        batch_count=InputData.n_iter,
        batch_size=1,
        scheduler=scheduler,
        model_id=model_id,
        custom_vae=frozen_args.custom_vae or "None",
        precision="fp16",
        device=get_device(frozen_args.device),
        max_length=frozen_args.max_length,
        save_metadata_to_json=frozen_args.save_metadata_to_json,
        save_metadata_to_png=frozen_args.write_metadata_to_png,
        lora_weights=lora_weights,
        lora_hf_id=lora_hf_id,
        ondemand=frozen_args.ondemand,
        repeatable_seeds=False,
    )

    # Since we're not streaming we just want the last generator result
    for items_so_far in res:
        items = items_so_far

    return {
        "images": encode_pil_to_base64(items[0]),
        "parameters": {},
        "info": items[1],
    }


# Rest API: /sdapi/v1/upscaler (Upscaling)
class UpscalerModelOverideSettings(BaseModel):
    sd_model_checkpoint: str = get_model_from_request(
        checkpoint_type="upscaler",
        fallback_model="stabilityai/stable-diffusion-x4-upscaler",
    )


class UpscalerInputData(GenerationInputData):
    init_images: list[str] = Field(
        description="Base64 encoded image to upscale"
    )
    noise_level: int = frozen_args.noise_level
    override_settings: UpscalerModelOverideSettings = None


@sdapi.post(
    "/v1/upscaler",
    summary="Does image upscaling",
    response_model=GenerationResponseData,
)
def upscaler_api(
    InputData: UpscalerInputData,
):
    model_id = get_model_from_request(
        InputData,
        checkpoint_type="upscaler",
        fallback_model="stabilityai/stable-diffusion-x4-upscaler",
    )
    scheduler = get_scheduler_from_request(InputData, "upscaler")
    (lora_weights, lora_hf_id) = get_lora_params(frozen_args.use_lora)

    init_image = decode_base64_to_image(InputData.init_images[0])

    print(
        f"Prompt: {InputData.prompt}, "
        f"Negative Prompt: {InputData.negative_prompt}, "
        f"Seed: {InputData.seed}, "
        f"Model: {model_id}, "
        f"Scheduler: {scheduler}."
    )

    res = upscaler_inf(
        InputData.prompt,
        InputData.negative_prompt,
        init_image,
        InputData.height,
        InputData.width,
        InputData.steps,
        InputData.noise_level,
        InputData.cfg_scale,
        InputData.seed,
        batch_count=InputData.n_iter,
        batch_size=1,
        scheduler=scheduler,
        model_id=model_id,
        custom_vae=frozen_args.custom_vae or "None",
        precision="fp16",
        device=get_device(frozen_args.device),
        max_length=frozen_args.max_length,
        save_metadata_to_json=frozen_args.save_metadata_to_json,
        save_metadata_to_png=frozen_args.write_metadata_to_png,
        lora_weights=lora_weights,
        lora_hf_id=lora_hf_id,
        ondemand=frozen_args.ondemand,
        repeatable_seeds=False,
    )

    # Since we're not streaming we just want the last generator result
    for items_so_far in res:
        items = items_so_far

    return {
        "images": encode_pil_to_base64(items[0]),
        "parameters": {},
        "info": items[1],
    }

```

`apps/stable_diffusion/web/api/utils.py`:

```py
import base64
import pickle

from argparse import Namespace
from fastapi.exceptions import HTTPException
from io import BytesIO
from PIL import Image
from pydantic import BaseModel, Field

from apps.stable_diffusion.src import args
from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    get_custom_model_files,
    predefined_models,
    predefined_paint_models,
    predefined_upscaler_models,
    scheduler_list,
    scheduler_list_cpu_only,
)


# Probably overly cautious, but try to ensure we only use the starting
# args in each api call, as the code does `args.<whatever> = <changed_value>`
# in lots of places and in testing, it seemed to me, these changes leaked
# into subsequent api calls.

# Roundtripping through pickle for deepcopy, there is probably a better way
frozen_args = Namespace(**(pickle.loads(pickle.dumps(vars(args)))))

# an attempt to map some of the A1111 sampler names to scheduler names
# https://github.com/huggingface/diffusers/issues/4167 is where the
# (not so obvious) ones come from
sampler_aliases = {
    # a1111/onnx (these point to diffusers classes in A1111)
    "pndm": "PNDM",
    "heun": "HeunDiscrete",
    "ddim": "DDIM",
    "ddpm": "DDPM",
    "euler": "EulerDiscrete",
    "euler-ancestral": "EulerAncestralDiscrete",
    "dpm": "DPMSolverMultistep",
    # a1111/k_diffusion (the obvious ones)
    "Euler a": "EulerAncestralDiscrete",
    "Euler": "EulerDiscrete",
    "LMS": "LMSDiscrete",
    "Heun": "HeunDiscrete",
    # a1111/k_diffusion (not so obvious)
    "DPM++ 2M": "DPMSolverMultistep",
    "DPM++ 2M Karras": "DPMSolverMultistepKarras",
    "DPM++ 2M SDE": "DPMSolverMultistep++",
    "DPM++ 2M SDE Karras": "DPMSolverMultistepKarras++",
    "DPM2": "KDPM2Discrete",
    "DPM2 a": "KDPM2AncestralDiscrete",
}

allowed_schedulers = {
    "txt2img": {
        "schedulers": scheduler_list,
        "fallback": "SharkEulerDiscrete",
    },
    "txt2img_hires": {
        "schedulers": scheduler_list_cpu_only,
        "fallback": "DEISMultistep",
    },
    "img2img": {
        "schedulers": scheduler_list_cpu_only,
        "fallback": "EulerDiscrete",
    },
    "inpaint": {
        "schedulers": scheduler_list_cpu_only,
        "fallback": "DDIM",
    },
    "outpaint": {
        "schedulers": scheduler_list_cpu_only,
        "fallback": "DDIM",
    },
    "upscaler": {
        "schedulers": scheduler_list_cpu_only,
        "fallback": "DDIM",
    },
}

# base pydantic model for sd generation apis


class GenerationInputData(BaseModel):
    prompt: str = ""
    negative_prompt: str = ""
    hf_model_id: str | None = None
    height: int = Field(
        default=frozen_args.height, ge=128, le=768, multiple_of=8
    )
    width: int = Field(
        default=frozen_args.width, ge=128, le=768, multiple_of=8
    )
    sampler_name: str = frozen_args.scheduler
    cfg_scale: float = Field(default=frozen_args.guidance_scale, ge=1)
    steps: int = Field(default=frozen_args.steps, ge=1, le=100)
    seed: int = frozen_args.seed
    n_iter: int = Field(default=frozen_args.batch_count)


class GenerationResponseData(BaseModel):
    images: list[str] = Field(description="Generated images, Base64 encoded")
    properties: dict = {}
    info: str


# image encoding/decoding


def encode_pil_to_base64(images: list[Image.Image]):
    encoded_imgs = []
    for image in images:
        with BytesIO() as output_bytes:
            if frozen_args.output_img_format.lower() == "png":
                image.save(output_bytes, format="PNG")

            elif frozen_args.output_img_format.lower() in ("jpg", "jpeg"):
                image.save(output_bytes, format="JPEG")
            else:
                raise HTTPException(
                    status_code=500, detail="Invalid image format"
                )
            bytes_data = output_bytes.getvalue()
            encoded_imgs.append(base64.b64encode(bytes_data))
    return encoded_imgs


def decode_base64_to_image(encoding: str):
    if encoding.startswith("data:image/"):
        encoding = encoding.split(";", 1)[1].split(",", 1)[1]
    try:
        image = Image.open(BytesIO(base64.b64decode(encoding)))
        return image
    except Exception as err:
        print(err)
        raise HTTPException(status_code=400, detail="Invalid encoded image")


# get valid sd models/vaes/schedulers etc.


def get_predefined_models(custom_checkpoint_type: str):
    match custom_checkpoint_type:
        case "inpainting":
            return predefined_paint_models
        case "upscaler":
            return predefined_upscaler_models
        case _:
            return predefined_models


def get_model_from_request(
    request_data=None,
    checkpoint_type: str = "",
    fallback_model: str = "",
):
    model = None
    if request_data:
        if request_data.hf_model_id:
            model = request_data.hf_model_id
        elif request_data.override_settings:
            model = request_data.override_settings.sd_model_checkpoint

    # if the request didn't specify a model try the command line args
    result = model or frozen_args.ckpt_loc or frozen_args.hf_model_id

    # make sure whatever we have is a valid model for the checkpoint type
    if result in get_custom_model_files(
        custom_checkpoint_type=checkpoint_type
    ) + get_predefined_models(checkpoint_type):
        return result
    # if not return what was specified as the fallback
    else:
        return fallback_model


def get_scheduler_from_request(
    request_data: GenerationInputData, operation: str
):
    allowed = allowed_schedulers[operation]

    requested = request_data.sampler_name
    requested = sampler_aliases.get(requested, requested)

    return (
        requested
        if requested in allowed["schedulers"]
        else allowed["fallback"]
    )


def get_lora_params(use_lora: str):
    # TODO: since the inference functions in the webui, which we are
    # still calling into for the api, jam these back together again before
    # handing them off to the pipeline, we should remove this nonsense
    # and unify their selection in the UI and command line args proper
    if use_lora in get_custom_model_files("lora"):
        return (use_lora, "")

    return ("None", use_lora)


def get_device(device_str: str):
    # first substring match in the list available devices, with first
    # device when none are matched
    return next(
        (device for device in available_devices if device_str in device),
        available_devices[0],
    )

```

`apps/stable_diffusion/web/index.py`:

```py
from multiprocessing import freeze_support
import os
import sys
import logging
import apps.stable_diffusion.web.utils.app as app

if sys.platform == "darwin":
    # import before IREE to avoid torch-MLIR library issues
    import torch_mlir

import shutil
import PIL, transformers, sentencepiece  # ensures inclusion in pysintaller exe generation
from apps.stable_diffusion.src import args, clear_all
import apps.stable_diffusion.web.utils.global_obj as global_obj

if sys.platform == "darwin":
    os.environ["DYLD_LIBRARY_PATH"] = "/usr/local/lib"
    # import before IREE to avoid MLIR library issues
    import torch_mlir

if args.clear_all:
    clear_all()


if __name__ == "__main__":
    if args.debug:
        logging.basicConfig(level=logging.DEBUG)
    # required to do multiprocessing in a pyinstaller freeze
    freeze_support()
    if args.api or "api" in args.ui.split(","):
        from apps.stable_diffusion.web.ui import (
            llm_chat_api,
        )
        from apps.stable_diffusion.web.api import sdapi

        from fastapi import FastAPI, APIRouter
        from fastapi.middleware.cors import CORSMiddleware
        import uvicorn

        # init global sd pipeline and config
        global_obj._init()

        api = FastAPI()
        api.mount("/sdapi/", sdapi)

        # chat APIs needed for compatibility with multiple extensions using OpenAI API
        api.add_api_route(
            "/v1/chat/completions", llm_chat_api, methods=["post"]
        )
        api.add_api_route("/v1/completions", llm_chat_api, methods=["post"])
        api.add_api_route("/chat/completions", llm_chat_api, methods=["post"])
        api.add_api_route("/completions", llm_chat_api, methods=["post"])
        api.add_api_route(
            "/v1/engines/codegen/completions", llm_chat_api, methods=["post"]
        )
        api.include_router(APIRouter())

        # deal with CORS requests if CORS accept origins are set
        if args.api_accept_origin:
            print(
                f"API Configured for CORS. Accepting origins: { args.api_accept_origin }"
            )
            api.add_middleware(
                CORSMiddleware,
                allow_origins=args.api_accept_origin,
                allow_methods=["GET", "POST"],
                allow_headers=["*"],
            )
        else:
            print("API not configured for CORS")

        uvicorn.run(api, host="0.0.0.0", port=args.server_port)
        sys.exit(0)

    # Setup to use shark_tmp for gradio's temporary image files and clear any
    # existing temporary images there if they exist. Then we can import gradio.
    # It has to be in this order or gradio ignores what we've set up.
    from apps.stable_diffusion.web.utils.tmp_configs import (
        config_tmp,
    )

    config_tmp()
    import gradio as gr

    # Create custom models folders if they don't exist
    from apps.stable_diffusion.web.ui.utils import (
        create_custom_models_folders,
        nodicon_loc,
    )

    create_custom_models_folders()

    def resource_path(relative_path):
        """Get absolute path to resource, works for dev and for PyInstaller"""
        base_path = getattr(
            sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__))
        )
        return os.path.join(base_path, relative_path)

    dark_theme = resource_path("ui/css/sd_dark_theme.css")

    from apps.stable_diffusion.web.ui import (
        txt2img_web,
        txt2img_custom_model,
        txt2img_gallery,
        txt2img_png_info_img,
        txt2img_status,
        txt2img_sendto_img2img,
        txt2img_sendto_inpaint,
        txt2img_sendto_outpaint,
        txt2img_sendto_upscaler,
        # SDXL
        txt2img_sdxl_inf,
        txt2img_sdxl_web,
        txt2img_sdxl_custom_model,
        txt2img_sdxl_gallery,
        txt2img_sdxl_status,
        # h2ogpt_upload,
        # h2ogpt_web,
        img2img_web,
        img2img_custom_model,
        img2img_gallery,
        img2img_init_image,
        img2img_status,
        img2img_sendto_inpaint,
        img2img_sendto_outpaint,
        img2img_sendto_upscaler,
        inpaint_web,
        inpaint_custom_model,
        inpaint_gallery,
        inpaint_init_image,
        inpaint_status,
        inpaint_sendto_img2img,
        inpaint_sendto_outpaint,
        inpaint_sendto_upscaler,
        outpaint_web,
        outpaint_custom_model,
        outpaint_gallery,
        outpaint_init_image,
        outpaint_status,
        outpaint_sendto_img2img,
        outpaint_sendto_inpaint,
        outpaint_sendto_upscaler,
        upscaler_web,
        upscaler_custom_model,
        upscaler_gallery,
        upscaler_init_image,
        upscaler_status,
        upscaler_sendto_img2img,
        upscaler_sendto_inpaint,
        upscaler_sendto_outpaint,
        #  lora_train_web,
        #  model_web,
        #  model_config_web,
        hf_models,
        modelmanager_sendto_txt2img,
        modelmanager_sendto_img2img,
        modelmanager_sendto_inpaint,
        modelmanager_sendto_outpaint,
        modelmanager_sendto_upscaler,
        stablelm_chat,
        minigpt4_web,
        outputgallery_web,
        outputgallery_tab_select,
        outputgallery_watch,
        outputgallery_filename,
        outputgallery_sendto_txt2img,
        outputgallery_sendto_img2img,
        outputgallery_sendto_inpaint,
        outputgallery_sendto_outpaint,
        outputgallery_sendto_upscaler,
    )

    # init global sd pipeline and config
    global_obj._init()

    def register_button_click(button, selectedid, inputs, outputs):
        button.click(
            lambda x: (
                x[0]["name"] if len(x) != 0 else None,
                gr.Tabs.update(selected=selectedid),
            ),
            inputs,
            outputs,
        )

    def register_modelmanager_button(button, selectedid, inputs, outputs):
        button.click(
            lambda x: (
                "None",
                x,
                gr.Tabs.update(selected=selectedid),
            ),
            inputs,
            outputs,
        )

    def register_outputgallery_button(button, selectedid, inputs, outputs):
        button.click(
            lambda x: (
                x,
                gr.Tabs.update(selected=selectedid),
            ),
            inputs,
            outputs,
        )

    with gr.Blocks(
        css=dark_theme, analytics_enabled=False, title="SHARK AI Studio"
    ) as sd_web:
        with gr.Tabs() as tabs:
            # NOTE: If adding, removing, or re-ordering tabs, make sure that they
            # have a unique id that doesn't clash with any of the other tabs,
            # and that the order in the code here is the order they should
            # appear in the ui, as the id value doesn't determine the order.

            # Where possible, avoid changing the id of any tab that is the
            # destination of one of the 'send to' buttons. If you do have to change
            # that id, make sure you update the relevant register_button_click calls
            # further down with the new id.
            with gr.TabItem(label="Text-to-Image", id=0):
                txt2img_web.render()
            with gr.TabItem(label="Image-to-Image", id=1):
                img2img_web.render()
            with gr.TabItem(label="Inpainting", id=2):
                inpaint_web.render()
            with gr.TabItem(label="Outpainting", id=3):
                outpaint_web.render()
            with gr.TabItem(label="Upscaler", id=4):
                upscaler_web.render()
            if args.output_gallery:
                with gr.TabItem(label="Output Gallery", id=5) as og_tab:
                    outputgallery_web.render()

                # extra output gallery configuration
                outputgallery_tab_select(og_tab.select)
                outputgallery_watch(
                    [
                        txt2img_status,
                        img2img_status,
                        inpaint_status,
                        outpaint_status,
                        upscaler_status,
                    ]
                )
            #  with gr.TabItem(label="Model Manager", id=6):
            #      model_web.render()
            #  with gr.TabItem(label="LoRA Training (Experimental)", id=7):
            #      lora_train_web.render()
            with gr.TabItem(label="Chat Bot", id=8):
                stablelm_chat.render()
            #  with gr.TabItem(
            #      label="Generate Sharding Config (Experimental)", id=9
            #  ):
            #      model_config_web.render()
            with gr.TabItem(label="MultiModal (Experimental)", id=10):
                minigpt4_web.render()
            # with gr.TabItem(label="DocuChat Upload", id=11):
            #     h2ogpt_upload.render()
            # with gr.TabItem(label="DocuChat(Experimental)", id=12):
            #     h2ogpt_web.render()
            with gr.TabItem(label="Text-to-Image-SDXL (Experimental)", id=13):
                txt2img_sdxl_web.render()

            actual_port = app.usable_port()
            if actual_port != args.server_port:
                sd_web.load(
                    fn=lambda: gr.Info(
                        f"Port {args.server_port} is in use by another application. "
                        f"Shark is running on port {actual_port} instead."
                    )
                )

        # send to buttons
        register_button_click(
            txt2img_sendto_img2img,
            1,
            [txt2img_gallery],
            [img2img_init_image, tabs],
        )
        register_button_click(
            txt2img_sendto_inpaint,
            2,
            [txt2img_gallery],
            [inpaint_init_image, tabs],
        )
        register_button_click(
            txt2img_sendto_outpaint,
            3,
            [txt2img_gallery],
            [outpaint_init_image, tabs],
        )
        register_button_click(
            txt2img_sendto_upscaler,
            4,
            [txt2img_gallery],
            [upscaler_init_image, tabs],
        )
        register_button_click(
            img2img_sendto_inpaint,
            2,
            [img2img_gallery],
            [inpaint_init_image, tabs],
        )
        register_button_click(
            img2img_sendto_outpaint,
            3,
            [img2img_gallery],
            [outpaint_init_image, tabs],
        )
        register_button_click(
            img2img_sendto_upscaler,
            4,
            [img2img_gallery],
            [upscaler_init_image, tabs],
        )
        register_button_click(
            inpaint_sendto_img2img,
            1,
            [inpaint_gallery],
            [img2img_init_image, tabs],
        )
        register_button_click(
            inpaint_sendto_outpaint,
            3,
            [inpaint_gallery],
            [outpaint_init_image, tabs],
        )
        register_button_click(
            inpaint_sendto_upscaler,
            4,
            [inpaint_gallery],
            [upscaler_init_image, tabs],
        )
        register_button_click(
            outpaint_sendto_img2img,
            1,
            [outpaint_gallery],
            [img2img_init_image, tabs],
        )
        register_button_click(
            outpaint_sendto_inpaint,
            2,
            [outpaint_gallery],
            [inpaint_init_image, tabs],
        )
        register_button_click(
            outpaint_sendto_upscaler,
            4,
            [outpaint_gallery],
            [upscaler_init_image, tabs],
        )
        register_button_click(
            upscaler_sendto_img2img,
            1,
            [upscaler_gallery],
            [img2img_init_image, tabs],
        )
        register_button_click(
            upscaler_sendto_inpaint,
            2,
            [upscaler_gallery],
            [inpaint_init_image, tabs],
        )
        register_button_click(
            upscaler_sendto_outpaint,
            3,
            [upscaler_gallery],
            [outpaint_init_image, tabs],
        )
        if args.output_gallery:
            register_outputgallery_button(
                outputgallery_sendto_txt2img,
                0,
                [outputgallery_filename],
                [txt2img_png_info_img, tabs],
            )
            register_outputgallery_button(
                outputgallery_sendto_img2img,
                1,
                [outputgallery_filename],
                [img2img_init_image, tabs],
            )
            register_outputgallery_button(
                outputgallery_sendto_inpaint,
                2,
                [outputgallery_filename],
                [inpaint_init_image, tabs],
            )
            register_outputgallery_button(
                outputgallery_sendto_outpaint,
                3,
                [outputgallery_filename],
                [outpaint_init_image, tabs],
            )
            register_outputgallery_button(
                outputgallery_sendto_upscaler,
                4,
                [outputgallery_filename],
                [upscaler_init_image, tabs],
            )
        register_modelmanager_button(
            modelmanager_sendto_txt2img,
            0,
            [hf_models],
            [txt2img_custom_model, tabs],
        )
        register_modelmanager_button(
            modelmanager_sendto_img2img,
            1,
            [hf_models],
            [img2img_custom_model, tabs],
        )
        register_modelmanager_button(
            modelmanager_sendto_inpaint,
            2,
            [hf_models],
            [inpaint_custom_model, tabs],
        )
        register_modelmanager_button(
            modelmanager_sendto_outpaint,
            3,
            [hf_models],
            [outpaint_custom_model, tabs],
        )
        register_modelmanager_button(
            modelmanager_sendto_upscaler,
            4,
            [hf_models],
            [upscaler_custom_model, tabs],
        )

    sd_web.queue()
    sd_web.launch(
        share=args.share,
        inbrowser=not app.launch(actual_port),
        server_name="0.0.0.0",
        server_port=actual_port,
        favicon_path=nodicon_loc,
    )

```

`apps/stable_diffusion/web/ui/__init__.py`:

```py
from apps.stable_diffusion.web.ui.txt2img_ui import (
    txt2img_inf,
    txt2img_web,
    txt2img_custom_model,
    txt2img_gallery,
    txt2img_png_info_img,
    txt2img_status,
    txt2img_sendto_img2img,
    txt2img_sendto_inpaint,
    txt2img_sendto_outpaint,
    txt2img_sendto_upscaler,
)
from apps.stable_diffusion.web.ui.txt2img_sdxl_ui import (
    txt2img_sdxl_inf,
    txt2img_sdxl_web,
    txt2img_sdxl_custom_model,
    txt2img_sdxl_gallery,
    txt2img_sdxl_status,
)
from apps.stable_diffusion.web.ui.img2img_ui import (
    img2img_inf,
    img2img_web,
    img2img_custom_model,
    img2img_gallery,
    img2img_init_image,
    img2img_status,
    img2img_sendto_inpaint,
    img2img_sendto_outpaint,
    img2img_sendto_upscaler,
)
from apps.stable_diffusion.web.ui.inpaint_ui import (
    inpaint_inf,
    inpaint_web,
    inpaint_custom_model,
    inpaint_gallery,
    inpaint_init_image,
    inpaint_status,
    inpaint_sendto_img2img,
    inpaint_sendto_outpaint,
    inpaint_sendto_upscaler,
)
from apps.stable_diffusion.web.ui.outpaint_ui import (
    outpaint_inf,
    outpaint_web,
    outpaint_custom_model,
    outpaint_gallery,
    outpaint_init_image,
    outpaint_status,
    outpaint_sendto_img2img,
    outpaint_sendto_inpaint,
    outpaint_sendto_upscaler,
)
from apps.stable_diffusion.web.ui.upscaler_ui import (
    upscaler_inf,
    upscaler_web,
    upscaler_custom_model,
    upscaler_gallery,
    upscaler_init_image,
    upscaler_status,
    upscaler_sendto_img2img,
    upscaler_sendto_inpaint,
    upscaler_sendto_outpaint,
)
from apps.stable_diffusion.web.ui.model_manager import (
    model_web,
    hf_models,
    modelmanager_sendto_txt2img,
    modelmanager_sendto_img2img,
    modelmanager_sendto_inpaint,
    modelmanager_sendto_outpaint,
    modelmanager_sendto_upscaler,
)
from apps.stable_diffusion.web.ui.lora_train_ui import lora_train_web
from apps.stable_diffusion.web.ui.stablelm_ui import (
    stablelm_chat,
    llm_chat_api,
)
from apps.stable_diffusion.web.ui.generate_config import model_config_web
from apps.stable_diffusion.web.ui.minigpt4_ui import minigpt4_web
from apps.stable_diffusion.web.ui.outputgallery_ui import (
    outputgallery_web,
    outputgallery_tab_select,
    outputgallery_watch,
    outputgallery_filename,
    outputgallery_sendto_txt2img,
    outputgallery_sendto_img2img,
    outputgallery_sendto_inpaint,
    outputgallery_sendto_outpaint,
    outputgallery_sendto_upscaler,
)

```

`apps/stable_diffusion/web/ui/common_ui_events.py`:

```py
from apps.stable_diffusion.web.ui.utils import (
    HSLHue,
    hsl_color,
    get_lora_metadata,
)


# Answers HTML to show the most frequent tags used when a LoRA was trained,
# taken from the metadata of its .safetensors file.
def lora_changed(lora_file):
    # tag frequency percentage, that gets maximum amount of the staring hue
    TAG_COLOR_THRESHOLD = 0.55
    # tag frequency percentage, above which a tag is displayed
    TAG_DISPLAY_THRESHOLD = 0.65
    # template for the html used to display a tag
    TAG_HTML_TEMPLATE = '<span class="lora-tag" style="border: 1px solid {color};">{tag}</span>'

    if lora_file == "None":
        return ["<div><i>No LoRA selected</i></div>"]
    elif not lora_file.lower().endswith(".safetensors"):
        return [
            "<div><i>Only metadata queries for .safetensors files are currently supported</i></div>"
        ]
    else:
        metadata = get_lora_metadata(lora_file)
        if metadata:
            frequencies = metadata["frequencies"]
            return [
                "".join(
                    [
                        f'<div class="lora-model">Trained against weights in: {metadata["model"]}</div>'
                    ]
                    + [
                        TAG_HTML_TEMPLATE.format(
                            color=hsl_color(
                                (tag[1] - TAG_COLOR_THRESHOLD)
                                / (1 - TAG_COLOR_THRESHOLD),
                                start=HSLHue.RED,
                                end=HSLHue.GREEN,
                            ),
                            tag=tag[0],
                        )
                        for tag in frequencies
                        if tag[1] > TAG_DISPLAY_THRESHOLD
                    ],
                )
            ]
        elif metadata is None:
            return [
                "<div><i>This LoRA does not publish tag frequency metadata</i></div>"
            ]
        else:
            return [
                "<div><i>This LoRA has empty tag frequency metadata, or we could not parse it</i></div>"
            ]

```

`apps/stable_diffusion/web/ui/css/sd_dark_theme.css`:

```css
/*
Apply Gradio dark theme to the default Gradio theme.
Procedure to upgrade the dark theme:
- Using your browser, visit http://localhost:8080/?__theme=dark
- Open your browser inspector, search for the .dark css class
- Copy .dark class declarations, apply them here into :root
*/

:root {
    --body-background-fill: var(--background-fill-primary);
    --body-text-color: var(--neutral-100);
    --color-accent-soft: var(--neutral-700);
    --background-fill-primary: var(--neutral-950);
    --background-fill-secondary: var(--neutral-900);
    --border-color-accent: var(--neutral-600);
    --border-color-primary: var(--neutral-700);
    --link-text-color-active: var(--secondary-500);
    --link-text-color: var(--secondary-500);
    --link-text-color-hover: var(--secondary-400);
    --link-text-color-visited: var(--secondary-600);
    --body-text-color-subdued: var(--neutral-400);
    --shadow-spread: 1px;
    --block-background-fill: var(--neutral-800);
    --block-border-color: var(--border-color-primary);
    --block_border_width: None;
    --block-info-text-color: var(--body-text-color-subdued);
    --block-label-background-fill: var(--background-fill-secondary);
    --block-label-border-color: var(--border-color-primary);
    --block_label_border_width: None;
    --block-label-text-color: var(--neutral-200);
    --block_shadow: None;
    --block_title_background_fill: None;
    --block_title_border_color: None;
    --block_title_border_width: None;
    --block-title-text-color: var(--neutral-200);
    --panel-background-fill: var(--background-fill-secondary);
    --panel-border-color: var(--border-color-primary);
    --panel_border_width: None;
    --checkbox-background-color: var(--neutral-800);
    --checkbox-background-color-focus: var(--checkbox-background-color);
    --checkbox-background-color-hover: var(--checkbox-background-color);
    --checkbox-background-color-selected: var(--secondary-600);
    --checkbox-border-color: var(--neutral-700);
    --checkbox-border-color-focus: var(--secondary-500);
    --checkbox-border-color-hover: var(--neutral-600);
    --checkbox-border-color-selected: var(--secondary-600);
    --checkbox-border-width: var(--input-border-width);
    --checkbox-label-background-fill: linear-gradient(to top, var(--neutral-900), var(--neutral-800));
    --checkbox-label-background-fill-hover: linear-gradient(to top, var(--neutral-900), var(--neutral-800));
    --checkbox-label-background-fill-selected: var(--checkbox-label-background-fill);
    --checkbox-label-border-color: var(--border-color-primary);
    --checkbox-label-border-color-hover: var(--checkbox-label-border-color);
    --checkbox-label-border-width: var(--input-border-width);
    --checkbox-label-text-color: var(--body-text-color);
    --checkbox-label-text-color-selected: var(--checkbox-label-text-color);
    --error-background-fill: var(--background-fill-primary);
    --error-border-color: var(--border-color-primary);
    --error_border_width: None;
    --error-text-color: #ef4444;
    --input-background-fill: var(--neutral-800);
    --input-background-fill-focus: var(--secondary-600);
    --input-background-fill-hover: var(--input-background-fill);
    --input-border-color: var(--border-color-primary);
    --input-border-color-focus: var(--neutral-700);
    --input-border-color-hover: var(--input-border-color);
    --input_border_width: None;
    --input-placeholder-color: var(--neutral-500);
    --input_shadow: None;
    --input-shadow-focus: 0 0 0 var(--shadow-spread) var(--neutral-700), var(--shadow-inset);
    --loader_color: None;
    --slider_color: None;
    --stat-background-fill: linear-gradient(to right, var(--primary-400), var(--primary-600));
    --table-border-color: var(--neutral-700);
    --table-even-background-fill: var(--neutral-950);
    --table-odd-background-fill: var(--neutral-900);
    --table-row-focus: var(--color-accent-soft);
    --button-border-width: var(--input-border-width);
    --button-cancel-background-fill: linear-gradient(to bottom right, #dc2626, #b91c1c);
    --button-cancel-background-fill-hover: linear-gradient(to bottom right, #dc2626, #dc2626);
    --button-cancel-border-color: #dc2626;
    --button-cancel-border-color-hover: var(--button-cancel-border-color);
    --button-cancel-text-color: white;
    --button-cancel-text-color-hover: var(--button-cancel-text-color);
    --button-primary-background-fill: linear-gradient(to bottom right, var(--primary-500), var(--primary-600));
    --button-primary-background-fill-hover: linear-gradient(to bottom right, var(--primary-500), var(--primary-500));
    --button-primary-border-color: var(--primary-500);
    --button-primary-border-color-hover: var(--button-primary-border-color);
    --button-primary-text-color: white;
    --button-primary-text-color-hover: var(--button-primary-text-color);
    --button-secondary-background-fill: linear-gradient(to bottom right, var(--neutral-600), var(--neutral-700));
    --button-secondary-background-fill-hover: linear-gradient(to bottom right, var(--neutral-600), var(--neutral-600));
    --button-secondary-border-color: var(--neutral-600);
    --button-secondary-border-color-hover: var(--button-secondary-border-color);
    --button-secondary-text-color: white;
    --button-secondary-text-color-hover: var(--button-secondary-text-color);
    --block-border-width: 1px;
    --block-label-border-width: 1px;
    --form-gap-width: 1px;
    --error-border-width: 1px;
    --input-border-width: 1px;
}

/* SHARK theme */
body {
    background-color: var(--background-fill-primary);
}

.generating.svelte-zlszon.svelte-zlszon {
    border: none;
}

.generating {
    border: none !important;
}

#chatbot {
    height: 100% !important;
}

/* display in full width for desktop devices */
@media (min-width: 1536px)
{
    .gradio-container {
        max-width: var(--size-full) !important;
    }
}

.gradio-container .contain {
    padding: 0 var(--size-4) !important;
}

#ui_title {
    padding: var(--size-2) 0 0 var(--size-1);
}

#top_logo {
    color: transparent;
    background-color: transparent;
    border-radius: 0 !important;
    border: 0;
}

#demo_title_outer {
    border-radius: 0;
}

#prompt_box_outer div:first-child {
    border-radius: 0 !important
}

#prompt_box textarea, #negative_prompt_box textarea {
    background-color: var(--background-fill-primary) !important;
}

#prompt_examples {
    margin: 0 !important;
}

#prompt_examples svg {
    display: none !important;
}

#ui_body {
    padding: var(--size-2) !important;
    border-radius: 0.5em !important;
}

#img_result+div {
    display: none !important;
}

footer {
    display: none !important;
}

#gallery + div {
    border-radius: 0 !important;
}

/* Gallery: Remove the default square ratio thumbnail and limit images height to the container */
#gallery .thumbnail-item.thumbnail-lg {
    aspect-ratio: unset;
    max-height: calc(55vh - (2 * var(--spacing-lg)));
}
@media (min-width: 1921px) {
    /* Force a 768px_height + 4px_margin_height + navbar_height for the gallery */
    #gallery .grid-wrap, #gallery .preview{
        min-height: calc(768px + 4px + var(--size-14));
        max-height: calc(768px + 4px + var(--size-14));
    }
    /* Limit height to 768px_height + 2px_margin_height for the thumbnails */
    #gallery .thumbnail-item.thumbnail-lg {
        max-height: 770px !important;
    }
}
/* Don't upscale when viewing in solo image mode */
#gallery .preview img {
    object-fit: scale-down;
}
/* Navbar images in cover mode*/
#gallery .preview .thumbnail-item img {
    object-fit: cover;
}

/* Limit the stable diffusion text output height */
#std_output textarea {
    max-height: 215px;
}

/* Prevent progress bar to block gallery navigation while building images (Gradio V3.19.0) */
#gallery .wrap.default {
    pointer-events: none;
}

/* Import Png info box */
#txt2img_prompt_image {
    height: var(--size-32) !important;
}

/* Hide "remove buttons" from ui dropdowns */
#custom_model .token-remove.remove-all,
#lora_weights .token-remove.remove-all,
#scheduler .token-remove.remove-all,
#device .token-remove.remove-all,
#stencil_model .token-remove.remove-all {
    display: none;
}

/* Hide selected items from ui dropdowns */
#custom_model .options .item .inner-item,
#scheduler .options .item .inner-item,
#device .options .item .inner-item,
#stencil_model .options .item .inner-item {
    display:none;
}

/* Hide the download icon from the nod logo */
#top_logo button {
    display: none;
}

/* workarounds for container=false not currently working for dropdowns */
.dropdown_no_container {
    padding: 0 !important;
}

#output_subdir_container :first-child {
    border: none;
}

/* reduced animation load when generating */
.generating {
    animation-play-state: paused !important;
}

/* better clarity when progress bars are minimal */
.meta-text {
    background-color: var(--block-label-background-fill);
}

/* lora tag pills */
.lora-tags {
    border: 1px solid var(--border-color-primary);
    color: var(--block-info-text-color) !important;
    padding: var(--block-padding);
}

.lora-tag {
    display: inline-block;
    height: 2em;
    color: rgb(212 212 212) !important;
    margin-right: 5pt;
    margin-bottom: 5pt;
    padding: 2pt 5pt;
    border-radius: 5pt;
    white-space: nowrap;
}

.lora-model {
    margin-bottom: var(--spacing-lg);
    color: var(--block-info-text-color) !important;
    line-height: var(--line-sm);
}

/* output gallery tab */
.output_parameters_dataframe table.table {
    /* works around a gradio bug that always shows scrollbars */
    overflow: clip auto;
}

.output_parameters_dataframe tbody td {
    font-size: small;
    line-height: var(--line-xs);
}

.output_icon_button {
    max-width: 30px;
    align-self: end;
    padding-bottom: 8px;
}

.outputgallery_sendto {
    min-width: 7em !important;
}

/* output gallery should take up most of the viewport height regardless of image size/number */
#outputgallery_gallery .fixed-height {
    min-height: 89vh !important;
}

/* don't stretch non-square images to be square, breaking their aspect ratio */
#outputgallery_gallery .thumbnail-item.thumbnail-lg > img {
    object-fit: contain !important;
}

/* centered logo for when there are no images */
#top_logo.logo_centered {
    height: 100%;
    width: 100%;
}

#top_logo.logo_centered img{
    object-fit: scale-down;
    position: absolute;
    width: 80%;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
}

```

`apps/stable_diffusion/web/ui/generate_config.py`:

```py
import gradio as gr
import torch
from transformers import AutoTokenizer
from apps.language_models.src.model_wrappers.vicuna_model import CombinedModel
from shark.shark_generate_model_config import GenerateConfigFile


def get_model_config():
    hf_model_path = "TheBloke/vicuna-7B-1.1-HF"
    tokenizer = AutoTokenizer.from_pretrained(hf_model_path, use_fast=False)
    compilation_prompt = "".join(["0" for _ in range(17)])
    compilation_input_ids = tokenizer(
        compilation_prompt,
        return_tensors="pt",
    ).input_ids
    compilation_input_ids = torch.tensor(compilation_input_ids).reshape(
        [1, 19]
    )
    firstVicunaCompileInput = (compilation_input_ids,)

    model = CombinedModel()
    c = GenerateConfigFile(model, 1, ["gpu_id"], firstVicunaCompileInput)
    return c.split_into_layers()


with gr.Blocks() as model_config_web:
    with gr.Row():
        hf_models = gr.Dropdown(
            label="Model List",
            choices=["Vicuna"],
            value="Vicuna",
            visible=True,
        )
        get_model_config_btn = gr.Button(value="Get Model Config")
    json_view = gr.JSON()

    get_model_config_btn.click(
        fn=get_model_config,
        inputs=[],
        outputs=[json_view],
    )

```

`apps/stable_diffusion/web/ui/h2ogpt.py`:

```py
import gradio as gr
import torch
import os
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
)
from apps.stable_diffusion.web.ui.utils import available_devices

from apps.language_models.langchain.enums import (
    DocumentChoices,
    LangChainAction,
)
import apps.language_models.langchain.gen as gen
from gpt_langchain import (
    path_to_docs,
    create_or_update_db,
)
from apps.stable_diffusion.src import args


def user(message, history):
    # Append the user's message to the conversation history
    return "", history + [[message, ""]]


sharkModel = 0
h2ogpt_model = 0


# NOTE: Each `model_name` should have its own start message
start_message = """
    SHARK DocuChat
    Chat with an AI, contextualized with provided files.
"""


def create_prompt(history):
    system_message = start_message
    for item in history:
        print("His item: ", item)

    conversation = "<|endoftext|>".join(
        [
            "<|endoftext|><|answer|>".join([item[0], item[1]])
            for item in history
        ]
    )

    msg = system_message + conversation
    msg = msg.strip()
    return msg


def chat(curr_system_message, history, device, precision):
    args.run_docuchat_web = True
    global h2ogpt_model
    global sharkModel
    global h2ogpt_tokenizer
    global model_state
    global langchain
    global userpath_selector
    from apps.language_models.langchain.h2oai_pipeline import generate_token

    if h2ogpt_model == 0:
        if "cuda" in device:
            shark_device = "cuda"
        elif "sync" in device:
            shark_device = "cpu"
        elif "task" in device:
            shark_device = "cpu"
        elif "vulkan" in device:
            shark_device = "vulkan"
        else:
            print("unrecognized device")

        device = "cpu" if shark_device == "cpu" else "cuda"

        args.device = shark_device
        args.precision = precision

        from apps.language_models.langchain.gen import Langchain

        langchain = Langchain(device, precision)
        h2ogpt_model, h2ogpt_tokenizer, _ = langchain.get_model(
            load_4bit=True
            if device == "cuda"
            else False,  # load model in 4bit if device is cuda to save memory
            load_gptq="",
            use_safetensors=False,
            infer_devices=True,
            device=device,
            base_model="h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
            inference_server="",
            tokenizer_base_model="h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
            lora_weights="",
            gpu_id=0,
            reward_type=None,
            local_files_only=False,
            resume_download=True,
            use_auth_token=False,
            trust_remote_code=True,
            offload_folder=None,
            compile_model=False,
            verbose=False,
        )
        model_state = dict(
            model=h2ogpt_model,
            tokenizer=h2ogpt_tokenizer,
            device=device,
            base_model="h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
            tokenizer_base_model="h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
            lora_weights="",
            inference_server="",
            prompt_type=None,
            prompt_dict=None,
        )
        from apps.language_models.langchain.h2oai_pipeline import (
            H2OGPTSHARKModel,
        )

        sharkModel = H2OGPTSHARKModel()

    prompt = create_prompt(history)
    output_dict = langchain.evaluate(
        model_state=model_state,
        my_db_state=None,
        instruction=prompt,
        iinput="",
        context="",
        stream_output=True,
        prompt_type="prompt_answer",
        prompt_dict={
            "promptA": "",
            "promptB": "",
            "PreInstruct": "<|prompt|>",
            "PreInput": None,
            "PreResponse": "<|answer|>",
            "terminate_response": [
                "<|prompt|>",
                "<|answer|>",
                "<|endoftext|>",
            ],
            "chat_sep": "<|endoftext|>",
            "chat_turn_sep": "<|endoftext|>",
            "humanstr": "<|prompt|>",
            "botstr": "<|answer|>",
            "generates_leading_space": False,
        },
        temperature=0.1,
        top_p=0.75,
        top_k=40,
        num_beams=1,
        max_new_tokens=256,
        min_new_tokens=0,
        early_stopping=False,
        max_time=180,
        repetition_penalty=1.07,
        num_return_sequences=1,
        do_sample=False,
        chat=True,
        instruction_nochat=prompt,
        iinput_nochat="",
        langchain_mode="UserData",
        langchain_action=LangChainAction.QUERY.value,
        top_k_docs=3,
        chunk=True,
        chunk_size=512,
        document_choice=[DocumentChoices.All_Relevant.name],
        concurrency_count=1,
        memory_restriction_level=2,
        raise_generate_gpu_exceptions=False,
        chat_context="",
        use_openai_embedding=False,
        use_openai_model=False,
        hf_embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        db_type="chroma",
        n_jobs=-1,
        first_para=False,
        max_max_time=60 * 2,
        model_state0=model_state,
        model_lock=True,
        user_path=userpath_selector.value,
    )

    output = generate_token(sharkModel, **output_dict)
    for partial_text in output:
        history[-1][1] = partial_text
        yield history
    return history


userpath_selector = gr.Textbox(
    label="Document Directory",
    value=str(os.path.abspath("apps/language_models/langchain/user_path/")),
    interactive=True,
    container=True,
)

with gr.Blocks(title="DocuChat") as h2ogpt_web:
    with gr.Row():
        supported_devices = available_devices
        enabled = len(supported_devices) > 0
        # show cpu-task device first in list for chatbot
        supported_devices = supported_devices[-1:] + supported_devices[:-1]
        supported_devices = [x for x in supported_devices if "sync" not in x]
        print(supported_devices)
        device = gr.Dropdown(
            label="Device",
            value=supported_devices[0]
            if enabled
            else "Only CUDA Supported for now",
            choices=supported_devices,
            interactive=enabled,
            allow_custom_value=True,
        )
        precision = gr.Radio(
            label="Precision",
            value="fp16",
            choices=[
                "int4",
                "int8",
                "fp16",
                "fp32",
            ],
            visible=True,
        )
    chatbot = gr.Chatbot(height=500)
    with gr.Row():
        with gr.Column():
            msg = gr.Textbox(
                label="Chat Message Box",
                placeholder="Chat Message Box",
                show_label=False,
                interactive=enabled,
                container=False,
            )
        with gr.Column():
            with gr.Row():
                submit = gr.Button("Submit", interactive=enabled)
                stop = gr.Button("Stop", interactive=enabled)
                clear = gr.Button("Clear", interactive=enabled)
    system_msg = gr.Textbox(
        start_message, label="System Message", interactive=False, visible=False
    )

    submit_event = msg.submit(
        fn=user, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False
    ).then(
        fn=chat,
        inputs=[system_msg, chatbot, device, precision],
        outputs=[chatbot],
        queue=True,
    )
    submit_click_event = submit.click(
        fn=user, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=False
    ).then(
        fn=chat,
        inputs=[system_msg, chatbot, device, precision],
        outputs=[chatbot],
        queue=True,
    )
    stop.click(
        fn=None,
        inputs=None,
        outputs=None,
        cancels=[submit_event, submit_click_event],
        queue=False,
    )
    clear.click(lambda: None, None, [chatbot], queue=False)


with gr.Blocks(title="DocuChat Upload") as h2ogpt_upload:
    import pathlib

    upload_path = None
    database = None
    database_directory = os.path.abspath(
        "apps/language_models/langchain/db_path/"
    )

    def read_path():
        global upload_path
        filenames = [
            [f]
            for f in os.listdir(upload_path)
            if os.path.isfile(os.path.join(upload_path, f))
        ]
        filenames.sort()
        return filenames

    def upload_file(f):
        names = []
        for tmpfile in f:
            name = tmpfile.name.split("/")[-1]
            basename = os.path.join(upload_path, name)
            with open(basename, "wb") as w:
                with open(tmpfile.name, "rb") as r:
                    w.write(r.read())
        update_or_create_db()
        return read_path()

    def update_userpath(newpath):
        global upload_path
        upload_path = newpath
        pathlib.Path(upload_path).mkdir(parents=True, exist_ok=True)
        return read_path()

    def update_or_create_db():
        global database
        global upload_path

        sources = path_to_docs(
            upload_path,
            verbose=True,
            fail_any_exception=False,
            n_jobs=-1,
            chunk=True,
            chunk_size=512,
            url=None,
            enable_captions=False,
            captions_model=None,
            caption_loader=None,
            enable_ocr=False,
        )

        pathlib.Path(database_directory).mkdir(parents=True, exist_ok=True)

        database = create_or_update_db(
            "chroma",
            database_directory,
            "UserData",
            sources,
            False,
            True,
            True,
            "sentence-transformers/all-MiniLM-L6-v2",
        )

    def first_run():
        global database
        if database is None:
            update_or_create_db()

    update_userpath(
        os.path.abspath("apps/language_models/langchain/user_path/")
    )
    h2ogpt_upload.load(fn=first_run)
    h2ogpt_web.load(fn=first_run)

    with gr.Column():
        text = gr.DataFrame(
            col_count=(1, "fixed"),
            type="array",
            label="Documents",
            value=read_path(),
        )
        with gr.Row():
            upload = gr.UploadButton(
                label="Upload documents",
                file_count="multiple",
            )
            upload.upload(fn=upload_file, inputs=upload, outputs=text)
            userpath_selector.render()
            userpath_selector.input(
                fn=update_userpath, inputs=userpath_selector, outputs=text
            ).then(fn=update_or_create_db)

```

`apps/stable_diffusion/web/ui/img2img_ui.py`:

```py
import os
import torch
import time
import gradio as gr
import PIL
from math import ceil
from PIL import Image

from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    nodlogo_loc,
    get_custom_model_path,
    get_custom_model_files,
    scheduler_list_cpu_only,
    predefined_models,
    cancel_sd,
)
from apps.stable_diffusion.web.ui.common_ui_events import lora_changed
from apps.stable_diffusion.src import (
    args,
    Image2ImagePipeline,
    StencilPipeline,
    resize_stencil,
    get_schedulers,
    set_init_device_flags,
    utils,
    save_output_img,
)
from apps.stable_diffusion.src.utils import (
    get_generated_imgs_path,
    get_generation_text_info,
    resampler_list,
)
from apps.stable_diffusion.src.utils.stencils import (
    CannyDetector,
    OpenposeDetector,
)
from apps.stable_diffusion.web.utils.common_label_calc import status_label
import numpy as np


# set initial values of iree_vulkan_target_triple, use_tuned and import_mlir.
init_iree_vulkan_target_triple = args.iree_vulkan_target_triple
init_use_tuned = args.use_tuned
init_import_mlir = args.import_mlir


# Exposed to UI.
def img2img_inf(
    prompt: str,
    negative_prompt: str,
    image_dict,
    height: int,
    width: int,
    steps: int,
    strength: float,
    guidance_scale: float,
    seed: str | int,
    batch_count: int,
    batch_size: int,
    scheduler: str,
    model_id: str,
    custom_vae: str,
    precision: str,
    device: str,
    max_length: int,
    save_metadata_to_json: bool,
    save_metadata_to_png: bool,
    lora_weights: str,
    lora_hf_id: str,
    ondemand: bool,
    repeatable_seeds: bool,
    resample_type: str,
    control_mode: str,
    stencils: list,
    images: list,
):
    from apps.stable_diffusion.web.ui.utils import (
        get_custom_model_pathfile,
        get_custom_vae_or_lora_weights,
        Config,
    )
    import apps.stable_diffusion.web.utils.global_obj as global_obj
    from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
        SD_STATE_CANCEL,
    )

    args.prompts = [prompt]
    args.negative_prompts = [negative_prompt]
    args.guidance_scale = guidance_scale
    args.seed = seed
    args.steps = steps
    args.strength = strength
    args.scheduler = scheduler
    args.img_path = "not none"
    args.ondemand = ondemand

    for i, stencil in enumerate(stencils):
        if images[i] is None and stencil is not None:
            return None, "A stencil must have an Image input"
        if images[i] is not None:
            images[i] = images[i].convert("RGB")

    if image_dict is None:
        return None, "An Initial Image is required"
    # if use_stencil == "scribble":
    #     image = image_dict["mask"].convert("RGB")
    if isinstance(image_dict, PIL.Image.Image):
        image = image_dict.convert("RGB")
    else:
        image = image_dict["image"].convert("RGB")

    # set ckpt_loc and hf_model_id.
    args.ckpt_loc = ""
    args.hf_model_id = ""
    args.custom_vae = ""

    # .safetensor or .chkpt on the custom model path
    if model_id in get_custom_model_files():
        args.ckpt_loc = get_custom_model_pathfile(model_id)
    # civitai download
    elif "civitai" in model_id:
        args.ckpt_loc = model_id
    # either predefined or huggingface
    else:
        args.hf_model_id = model_id

    if custom_vae != "None":
        args.custom_vae = get_custom_model_pathfile(custom_vae, model="vae")

    args.use_lora = get_custom_vae_or_lora_weights(
        lora_weights, lora_hf_id, "lora"
    )

    args.save_metadata_to_json = save_metadata_to_json
    args.write_metadata_to_png = save_metadata_to_png

    stencil_count = 0
    for stencil in stencils:
        if stencil is not None:
            stencil_count += 1
    if stencil_count > 0:
        args.scheduler = "DDIM"
        args.hf_model_id = "runwayml/stable-diffusion-v1-5"
        # image, width, height = resize_stencil(image)
    elif "Shark" in args.scheduler:
        print(
            f"Shark schedulers are not supported. Switching to EulerDiscrete "
            f"scheduler"
        )
        args.scheduler = "EulerDiscrete"
    cpu_scheduling = not args.scheduler.startswith("Shark")
    args.precision = precision
    dtype = torch.float32 if precision == "fp32" else torch.half
    new_config_obj = Config(
        "img2img",
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        precision,
        batch_size,
        max_length,
        height,
        width,
        device,
        use_lora=args.use_lora,
        stencils=stencils,
        ondemand=ondemand,
    )
    if (
        not global_obj.get_sd_obj()
        or global_obj.get_cfg_obj() != new_config_obj
    ):
        global_obj.clear_cache()
        global_obj.set_cfg_obj(new_config_obj)
        args.batch_count = batch_count
        args.batch_size = batch_size
        args.max_length = max_length
        args.height = height
        args.width = width
        args.device = device.split("=>", 1)[1].strip()
        args.iree_vulkan_target_triple = init_iree_vulkan_target_triple
        args.use_tuned = init_use_tuned
        args.import_mlir = init_import_mlir
        set_init_device_flags()
        model_id = (
            args.hf_model_id
            if args.hf_model_id
            else "stabilityai/stable-diffusion-2-1-base"
        )
        global_obj.set_schedulers(get_schedulers(model_id))
        scheduler_obj = global_obj.get_scheduler(args.scheduler)

        if stencil_count > 0:
            args.use_tuned = False
            global_obj.set_sd_obj(
                StencilPipeline.from_pretrained(
                    scheduler_obj,
                    args.import_mlir,
                    args.hf_model_id,
                    args.ckpt_loc,
                    args.custom_vae,
                    args.precision,
                    args.max_length,
                    args.batch_size,
                    args.height,
                    args.width,
                    args.use_base_vae,
                    args.use_tuned,
                    low_cpu_mem_usage=args.low_cpu_mem_usage,
                    stencils=stencils,
                    debug=args.import_debug if args.import_mlir else False,
                    use_lora=args.use_lora,
                    ondemand=args.ondemand,
                )
            )
        else:
            global_obj.set_sd_obj(
                Image2ImagePipeline.from_pretrained(
                    scheduler_obj,
                    args.import_mlir,
                    args.hf_model_id,
                    args.ckpt_loc,
                    args.custom_vae,
                    args.precision,
                    args.max_length,
                    args.batch_size,
                    args.height,
                    args.width,
                    args.use_base_vae,
                    args.use_tuned,
                    low_cpu_mem_usage=args.low_cpu_mem_usage,
                    debug=args.import_debug if args.import_mlir else False,
                    use_lora=args.use_lora,
                    ondemand=args.ondemand,
                )
            )

    global_obj.set_sd_scheduler(args.scheduler)

    start_time = time.time()
    global_obj.get_sd_obj().log = ""
    generated_imgs = []
    extra_info = {"STRENGTH": strength}
    text_output = ""
    try:
        seeds = utils.batch_seeds(seed, batch_count, repeatable_seeds)
    except TypeError as error:
        raise gr.Error(str(error)) from None

    for current_batch in range(batch_count):
        out_imgs = global_obj.get_sd_obj().generate_images(
            prompt,
            negative_prompt,
            image,
            batch_size,
            height,
            width,
            ceil(steps / strength),
            strength,
            guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
            stencils,
            images,
            resample_type=resample_type,
            control_mode=control_mode,
        )
        total_time = time.time() - start_time
        text_output = get_generation_text_info(
            seeds[: current_batch + 1], device
        )
        text_output += "\n" + global_obj.get_sd_obj().log
        text_output += f"\nTotal image(s) generation time: {total_time:.4f}sec"

        if global_obj.get_sd_status() == SD_STATE_CANCEL:
            break
        else:
            save_output_img(
                out_imgs[0],
                seeds[current_batch],
                extra_info,
            )
            generated_imgs.extend(out_imgs)
            yield generated_imgs, text_output, status_label(
                "Image-to-Image", current_batch + 1, batch_count, batch_size
            ), stencils, images

    return generated_imgs, text_output, "", stencils, images


with gr.Blocks(title="Image-to-Image") as img2img_web:
    # Stencils
    # TODO: Add more stencils here
    STENCIL_COUNT = 2
    stencils = gr.State([None] * STENCIL_COUNT)
    images = gr.State([None] * STENCIL_COUNT)
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Row():
            with gr.Column(scale=1, elem_id="demo_title_outer"):
                gr.Image(
                    value=nod_logo,
                    show_label=False,
                    interactive=False,
                    elem_id="top_logo",
                    width=150,
                    height=50,
                )
    with gr.Row(elem_id="ui_body"):
        with gr.Row():
            with gr.Column(scale=1, min_width=600):
                with gr.Row():
                    # janky fix for overflowing text
                    i2i_model_info = (
                        f"Custom Model Path: {str(get_custom_model_path())}"
                    )
                    img2img_custom_model = gr.Dropdown(
                        label=f"Models",
                        info="Select, or enter HuggingFace Model ID or Civitai model download URL",
                        elem_id="custom_model",
                        value=os.path.basename(args.ckpt_loc)
                        if args.ckpt_loc
                        else "stabilityai/stable-diffusion-2-1-base",
                        choices=get_custom_model_files() + predefined_models,
                        allow_custom_value=True,
                        scale=2,
                    )
                    # janky fix for overflowing text
                    i2i_vae_info = (str(get_custom_model_path("vae"))).replace(
                        "\\", "\n\\"
                    )
                    i2i_vae_info = f"VAE Path: {i2i_vae_info}"
                    custom_vae = gr.Dropdown(
                        label=f"Custom VAE Models",
                        info=i2i_vae_info,
                        elem_id="custom_model",
                        value=os.path.basename(args.custom_vae)
                        if args.custom_vae
                        else "None",
                        choices=["None"] + get_custom_model_files("vae"),
                        allow_custom_value=True,
                        scale=1,
                    )

                with gr.Group(elem_id="prompt_box_outer"):
                    prompt = gr.Textbox(
                        label="Prompt",
                        value=args.prompts[0],
                        lines=2,
                        elem_id="prompt_box",
                    )
                    negative_prompt = gr.Textbox(
                        label="Negative Prompt",
                        value=args.negative_prompts[0],
                        lines=2,
                        elem_id="negative_prompt_box",
                    )
                # TODO: make this import image prompt info if it exists
                img2img_init_image = gr.Image(
                    label="Input Image",
                    source="upload",
                    tool="sketch",
                    type="pil",
                    height=300,
                )

                with gr.Accordion(label="Multistencil Options", open=False):
                    choices = ["None", "canny", "openpose", "scribble"]

                    def cnet_preview(
                        checked, model, input_image, index, stencils, images
                    ):
                        if not checked:
                            stencils[index] = None
                            images[index] = None
                            return (None, stencils, images)
                        images[index] = input_image
                        stencils[index] = model
                        match model:
                            case "canny":
                                canny = CannyDetector()
                                result = canny(np.array(input_image), 100, 200)
                                return (
                                    [Image.fromarray(result), result],
                                    stencils,
                                    images,
                                )
                            case "openpose":
                                openpose = OpenposeDetector()
                                result = openpose(np.array(input_image))
                                # TODO: This is just an empty canvas, need to draw the candidates (which are in result[1])
                                return (
                                    [Image.fromarray(result[0]), result],
                                    stencils,
                                    images,
                                )
                            case _:
                                return (None, stencils, images)

                    with gr.Row():
                        cnet_1 = gr.Checkbox(show_label=False)
                        cnet_1_model = gr.Dropdown(
                            label="Controlnet 1",
                            value="None",
                            choices=choices,
                        )
                        cnet_1_image = gr.Image(
                            source="upload",
                            tool=None,
                            type="pil",
                        )
                        cnet_1_output = gr.Gallery(
                            show_label=False,
                            object_fit="scale-down",
                            rows=1,
                            columns=1,
                        )
                        cnet_1.change(
                            fn=(
                                lambda a, b, c, s, i: cnet_preview(
                                    a, b, c, 0, s, i
                                )
                            ),
                            inputs=[
                                cnet_1,
                                cnet_1_model,
                                cnet_1_image,
                                stencils,
                                images,
                            ],
                            outputs=[cnet_1_output, stencils, images],
                        )
                    with gr.Row():
                        cnet_2 = gr.Checkbox(show_label=False)
                        cnet_2_model = gr.Dropdown(
                            label="Controlnet 2",
                            value="None",
                            choices=choices,
                        )
                        cnet_2_image = gr.Image(
                            source="upload",
                            tool=None,
                            type="pil",
                        )
                        cnet_2_output = gr.Gallery(
                            show_label=False,
                            object_fit="scale-down",
                            rows=1,
                            columns=1,
                        )
                        cnet_2.change(
                            fn=(
                                lambda a, b, c, s, i: cnet_preview(
                                    a, b, c, 1, s, i
                                )
                            ),
                            inputs=[
                                cnet_2,
                                cnet_2_model,
                                cnet_2_image,
                                stencils,
                                images,
                            ],
                            outputs=[cnet_2_output, stencils, images],
                        )
                    control_mode = gr.Radio(
                        choices=["Prompt", "Balanced", "Controlnet"],
                        value="Balanced",
                        label="Control Mode",
                    )

                with gr.Accordion(label="LoRA Options", open=False):
                    with gr.Row():
                        # janky fix for overflowing text
                        i2i_lora_info = (
                            str(get_custom_model_path("lora"))
                        ).replace("\\", "\n\\")
                        i2i_lora_info = f"LoRA Path: {i2i_lora_info}"
                        lora_weights = gr.Dropdown(
                            allow_custom_value=True,
                            label=f"Standalone LoRA Weights",
                            info=i2i_lora_info,
                            elem_id="lora_weights",
                            value="None",
                            choices=["None"] + get_custom_model_files("lora"),
                        )
                        lora_hf_id = gr.Textbox(
                            elem_id="lora_hf_id",
                            placeholder="Select 'None' in the Standalone LoRA "
                            "weights dropdown on the left if you want to use "
                            "a standalone HuggingFace model ID for LoRA here "
                            "e.g: sayakpaul/sd-model-finetuned-lora-t4",
                            value="",
                            label="HuggingFace Model ID",
                            lines=3,
                        )
                    with gr.Row():
                        lora_tags = gr.HTML(
                            value="<div><i>No LoRA selected</i></div>",
                            elem_classes="lora-tags",
                        )
                with gr.Accordion(label="Advanced Options", open=False):
                    with gr.Row():
                        scheduler = gr.Dropdown(
                            elem_id="scheduler",
                            label="Scheduler",
                            value="EulerDiscrete",
                            choices=scheduler_list_cpu_only,
                            allow_custom_value=True,
                        )
                        with gr.Group():
                            save_metadata_to_png = gr.Checkbox(
                                label="Save prompt information to PNG",
                                value=args.write_metadata_to_png,
                                interactive=True,
                            )
                            save_metadata_to_json = gr.Checkbox(
                                label="Save prompt information to JSON file",
                                value=args.save_metadata_to_json,
                                interactive=True,
                            )
                    with gr.Row():
                        height = gr.Slider(
                            384, 768, value=args.height, step=8, label="Height"
                        )
                        width = gr.Slider(
                            384, 768, value=args.width, step=8, label="Width"
                        )
                        max_length = gr.Radio(
                            label="Max Length",
                            value=args.max_length,
                            choices=[
                                64,
                                77,
                            ],
                            visible=False,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            steps = gr.Slider(
                                1, 100, value=args.steps, step=1, label="Steps"
                            )
                        with gr.Column(scale=3):
                            strength = gr.Slider(
                                0,
                                1,
                                value=args.strength,
                                step=0.01,
                                label="Denoising Strength",
                            )
                            resample_type = gr.Dropdown(
                                value=args.resample_type,
                                choices=resampler_list,
                                label="Resample Type",
                                allow_custom_value=True,
                            )
                        ondemand = gr.Checkbox(
                            value=args.ondemand,
                            label="Low VRAM",
                            interactive=True,
                        )
                        precision = gr.Radio(
                            label="Precision",
                            value=args.precision,
                            choices=[
                                "fp16",
                                "fp32",
                            ],
                            visible=True,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            guidance_scale = gr.Slider(
                                0,
                                50,
                                value=args.guidance_scale,
                                step=0.1,
                                label="CFG Scale",
                            )
                        with gr.Column(scale=3):
                            batch_count = gr.Slider(
                                1,
                                100,
                                value=args.batch_count,
                                step=1,
                                label="Batch Count",
                                interactive=True,
                            )
                        repeatable_seeds = gr.Checkbox(
                            args.repeatable_seeds,
                            label="Repeatable Seeds",
                        )
                    with gr.Row():
                        batch_size = gr.Slider(
                            1,
                            4,
                            value=args.batch_size,
                            step=1,
                            label="Batch Size",
                            interactive=False,
                            visible=False,
                        )
                with gr.Row():
                    seed = gr.Textbox(
                        value=args.seed,
                        label="Seed",
                        info="An integer or a JSON list of integers, -1 for random",
                    )
                    device = gr.Dropdown(
                        elem_id="device",
                        label="Device",
                        value=available_devices[0],
                        choices=available_devices,
                        allow_custom_value=True,
                    )

            with gr.Column(scale=1, min_width=600):
                with gr.Group():
                    img2img_gallery = gr.Gallery(
                        label="Generated images",
                        show_label=False,
                        elem_id="gallery",
                        columns=2,
                        object_fit="contain",
                    )
                    std_output = gr.Textbox(
                        value=f"{i2i_model_info}\n"
                        f"Images will be saved at "
                        f"{get_generated_imgs_path()}",
                        lines=2,
                        elem_id="std_output",
                        show_label=False,
                    )
                    img2img_status = gr.Textbox(visible=False)
                with gr.Row():
                    stable_diffusion = gr.Button("Generate Image(s)")
                    random_seed = gr.Button("Randomize Seed")
                    random_seed.click(
                        lambda: -1,
                        inputs=[],
                        outputs=[seed],
                        queue=False,
                    )
                    stop_batch = gr.Button("Stop Batch")
                with gr.Row():
                    blank_thing_for_row = None
                with gr.Row():
                    img2img_sendto_inpaint = gr.Button(value="SendTo Inpaint")
                    img2img_sendto_outpaint = gr.Button(
                        value="SendTo Outpaint"
                    )
                    img2img_sendto_upscaler = gr.Button(
                        value="SendTo Upscaler"
                    )

        kwargs = dict(
            fn=img2img_inf,
            inputs=[
                prompt,
                negative_prompt,
                img2img_init_image,
                height,
                width,
                steps,
                strength,
                guidance_scale,
                seed,
                batch_count,
                batch_size,
                scheduler,
                img2img_custom_model,
                custom_vae,
                precision,
                device,
                max_length,
                save_metadata_to_json,
                save_metadata_to_png,
                lora_weights,
                lora_hf_id,
                ondemand,
                repeatable_seeds,
                resample_type,
                control_mode,
                stencils,
                images,
            ],
            outputs=[
                img2img_gallery,
                std_output,
                img2img_status,
                stencils,
                images,
            ],
            show_progress="minimal" if args.progress_bar else "none",
        )

        status_kwargs = dict(
            fn=lambda bc, bs: status_label("Image-to-Image", 0, bc, bs),
            inputs=[batch_count, batch_size],
            outputs=img2img_status,
        )

        prompt_submit = prompt.submit(**status_kwargs).then(**kwargs)
        neg_prompt_submit = negative_prompt.submit(**status_kwargs).then(
            **kwargs
        )
        generate_click = stable_diffusion.click(**status_kwargs).then(**kwargs)
        stop_batch.click(
            fn=cancel_sd,
            cancels=[prompt_submit, neg_prompt_submit, generate_click],
        )

        lora_weights.change(
            fn=lora_changed,
            inputs=[lora_weights],
            outputs=[lora_tags],
            queue=True,
        )

```

`apps/stable_diffusion/web/ui/inpaint_ui.py`:

```py
import os
import torch
import time
import sys
import gradio as gr
from PIL import Image

from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    nodlogo_loc,
    get_custom_model_path,
    get_custom_model_files,
    scheduler_list_cpu_only,
    predefined_paint_models,
    cancel_sd,
)
from apps.stable_diffusion.web.ui.common_ui_events import lora_changed
from apps.stable_diffusion.src import (
    args,
    InpaintPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    clear_all,
    save_output_img,
)
from apps.stable_diffusion.src.utils import (
    get_generated_imgs_path,
    get_generation_text_info,
)
from apps.stable_diffusion.web.utils.common_label_calc import status_label


# set initial values of iree_vulkan_target_triple, use_tuned and import_mlir.
init_iree_vulkan_target_triple = args.iree_vulkan_target_triple
init_use_tuned = args.use_tuned
init_import_mlir = args.import_mlir


# Exposed to UI.
def inpaint_inf(
    prompt: str,
    negative_prompt: str,
    image_dict,
    height: int,
    width: int,
    inpaint_full_res: bool,
    inpaint_full_res_padding: int,
    steps: int,
    guidance_scale: float,
    seed: str | int,
    batch_count: int,
    batch_size: int,
    scheduler: str,
    model_id: str,
    custom_vae: str,
    precision: str,
    device: str,
    max_length: int,
    save_metadata_to_json: bool,
    save_metadata_to_png: bool,
    lora_weights: str,
    lora_hf_id: str,
    ondemand: bool,
    repeatable_seeds: int,
):
    from apps.stable_diffusion.web.ui.utils import (
        get_custom_model_pathfile,
        get_custom_vae_or_lora_weights,
        Config,
    )
    import apps.stable_diffusion.web.utils.global_obj as global_obj
    from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
        SD_STATE_CANCEL,
    )

    args.prompts = [prompt]
    args.negative_prompts = [negative_prompt]
    args.guidance_scale = guidance_scale
    args.steps = steps
    args.scheduler = scheduler
    args.img_path = "not none"
    args.mask_path = "not none"
    args.ondemand = ondemand

    # set ckpt_loc and hf_model_id.
    args.ckpt_loc = ""
    args.hf_model_id = ""
    args.custom_vae = ""

    # .safetensor or .chkpt on the custom model path
    if model_id in get_custom_model_files(custom_checkpoint_type="inpainting"):
        args.ckpt_loc = get_custom_model_pathfile(model_id)
    # civitai download
    elif "civitai" in model_id:
        args.ckpt_loc = model_id
    # either predefined or huggingface
    else:
        args.hf_model_id = model_id

    if custom_vae != "None":
        args.custom_vae = get_custom_model_pathfile(custom_vae, model="vae")

    args.use_lora = get_custom_vae_or_lora_weights(
        lora_weights, lora_hf_id, "lora"
    )

    args.save_metadata_to_json = save_metadata_to_json
    args.write_metadata_to_png = save_metadata_to_png

    dtype = torch.float32 if precision == "fp32" else torch.half
    cpu_scheduling = not scheduler.startswith("Shark")
    new_config_obj = Config(
        "inpaint",
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        precision,
        batch_size,
        max_length,
        height,
        width,
        device,
        use_lora=args.use_lora,
        stencils=[],
        ondemand=ondemand,
    )
    if (
        not global_obj.get_sd_obj()
        or global_obj.get_cfg_obj() != new_config_obj
    ):
        global_obj.clear_cache()
        global_obj.set_cfg_obj(new_config_obj)
        args.precision = precision
        args.batch_count = batch_count
        args.batch_size = batch_size
        args.max_length = max_length
        args.height = height
        args.width = width
        args.device = device.split("=>", 1)[1].strip()
        args.iree_vulkan_target_triple = init_iree_vulkan_target_triple
        args.use_tuned = init_use_tuned
        args.import_mlir = init_import_mlir
        set_init_device_flags()
        model_id = (
            args.hf_model_id
            if args.hf_model_id
            else "stabilityai/stable-diffusion-2-inpainting"
        )
        global_obj.set_schedulers(get_schedulers(model_id))
        scheduler_obj = global_obj.get_scheduler(scheduler)
        global_obj.set_sd_obj(
            InpaintPipeline.from_pretrained(
                scheduler=scheduler_obj,
                import_mlir=args.import_mlir,
                model_id=args.hf_model_id,
                ckpt_loc=args.ckpt_loc,
                custom_vae=args.custom_vae,
                precision=args.precision,
                max_length=args.max_length,
                batch_size=args.batch_size,
                height=args.height,
                width=args.width,
                use_base_vae=args.use_base_vae,
                use_tuned=args.use_tuned,
                low_cpu_mem_usage=args.low_cpu_mem_usage,
                debug=args.import_debug if args.import_mlir else False,
                use_lora=args.use_lora,
                ondemand=args.ondemand,
            )
        )

    global_obj.set_sd_scheduler(scheduler)

    start_time = time.time()
    global_obj.get_sd_obj().log = ""
    generated_imgs = []
    image = image_dict["image"]
    mask_image = image_dict["mask"]
    text_output = ""
    try:
        seeds = utils.batch_seeds(seed, batch_count, repeatable_seeds)
    except TypeError as error:
        raise gr.Error(str(error)) from None

    for current_batch in range(batch_count):
        out_imgs = global_obj.get_sd_obj().generate_images(
            prompt,
            negative_prompt,
            image,
            mask_image,
            batch_size,
            height,
            width,
            inpaint_full_res,
            inpaint_full_res_padding,
            steps,
            guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )
        total_time = time.time() - start_time
        text_output = get_generation_text_info(
            seeds[: current_batch + 1], device
        )
        text_output += "\n" + global_obj.get_sd_obj().log
        text_output += f"\nTotal image(s) generation time: {total_time:.4f}sec"

        if global_obj.get_sd_status() == SD_STATE_CANCEL:
            break
        else:
            save_output_img(out_imgs[0], seeds[current_batch])
            generated_imgs.extend(out_imgs)
            yield generated_imgs, text_output, status_label(
                "Inpaint", current_batch + 1, batch_count, batch_size
            )

    return generated_imgs, text_output


with gr.Blocks(title="Inpainting") as inpaint_web:
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Row():
            with gr.Column(scale=1, elem_id="demo_title_outer"):
                gr.Image(
                    value=nod_logo,
                    show_label=False,
                    interactive=False,
                    elem_id="top_logo",
                    width=150,
                    height=50,
                )
    with gr.Row(elem_id="ui_body"):
        with gr.Row():
            with gr.Column(scale=1, min_width=600):
                with gr.Row():
                    # janky fix for overflowing text
                    inpaint_model_info = (
                        f"Custom Model Path: {str(get_custom_model_path())}"
                    )
                    inpaint_custom_model = gr.Dropdown(
                        label=f"Models",
                        info="Select, or enter HuggingFace Model ID or Civitai model download URL",
                        elem_id="custom_model",
                        value=os.path.basename(args.ckpt_loc)
                        if args.ckpt_loc
                        else "stabilityai/stable-diffusion-2-inpainting",
                        choices=get_custom_model_files(
                            custom_checkpoint_type="inpainting"
                        )
                        + predefined_paint_models,
                        allow_custom_value=True,
                        scale=2,
                    )
                    # janky fix for overflowing text
                    inpaint_vae_info = (
                        str(get_custom_model_path("vae"))
                    ).replace("\\", "\n\\")
                    inpaint_vae_info = f"VAE Path: {inpaint_vae_info}"
                    custom_vae = gr.Dropdown(
                        label=f"Custom VAE Models",
                        info=inpaint_vae_info,
                        elem_id="custom_model",
                        value=os.path.basename(args.custom_vae)
                        if args.custom_vae
                        else "None",
                        choices=["None"] + get_custom_model_files("vae"),
                        allow_custom_value=True,
                        scale=1,
                    )

                with gr.Group(elem_id="prompt_box_outer"):
                    prompt = gr.Textbox(
                        label="Prompt",
                        value=args.prompts[0],
                        lines=2,
                        elem_id="prompt_box",
                    )
                    negative_prompt = gr.Textbox(
                        label="Negative Prompt",
                        value=args.negative_prompts[0],
                        lines=2,
                        elem_id="negative_prompt_box",
                    )

                inpaint_init_image = gr.Image(
                    label="Masked Image",
                    source="upload",
                    tool="sketch",
                    type="pil",
                    height=350,
                )

                with gr.Accordion(label="LoRA Options", open=False):
                    with gr.Row():
                        # janky fix for overflowing text
                        inpaint_lora_info = (
                            str(get_custom_model_path("lora"))
                        ).replace("\\", "\n\\")
                        inpaint_lora_info = f"LoRA Path: {inpaint_lora_info}"
                        lora_weights = gr.Dropdown(
                            label=f"Standalone LoRA Weights",
                            info=inpaint_lora_info,
                            elem_id="lora_weights",
                            value="None",
                            choices=["None"] + get_custom_model_files("lora"),
                            allow_custom_value=True,
                        )
                        lora_hf_id = gr.Textbox(
                            elem_id="lora_hf_id",
                            placeholder="Select 'None' in the Standalone LoRA "
                            "weights dropdown on the left if you want to use "
                            "a standalone HuggingFace model ID for LoRA here "
                            "e.g: sayakpaul/sd-model-finetuned-lora-t4",
                            value="",
                            label="HuggingFace Model ID",
                            lines=3,
                        )
                    with gr.Row():
                        lora_tags = gr.HTML(
                            value="<div><i>No LoRA selected</i></div>",
                            elem_classes="lora-tags",
                        )
                with gr.Accordion(label="Advanced Options", open=False):
                    with gr.Row():
                        scheduler = gr.Dropdown(
                            elem_id="scheduler",
                            label="Scheduler",
                            value="EulerDiscrete",
                            choices=scheduler_list_cpu_only,
                            allow_custom_value=True,
                        )
                        with gr.Group():
                            save_metadata_to_png = gr.Checkbox(
                                label="Save prompt information to PNG",
                                value=args.write_metadata_to_png,
                                interactive=True,
                            )
                            save_metadata_to_json = gr.Checkbox(
                                label="Save prompt information to JSON file",
                                value=args.save_metadata_to_json,
                                interactive=True,
                            )
                    with gr.Row():
                        height = gr.Slider(
                            384, 768, value=args.height, step=8, label="Height"
                        )
                        width = gr.Slider(
                            384, 768, value=args.width, step=8, label="Width"
                        )
                        precision = gr.Radio(
                            label="Precision",
                            value=args.precision,
                            choices=[
                                "fp16",
                                "fp32",
                            ],
                            visible=False,
                        )
                        max_length = gr.Radio(
                            label="Max Length",
                            value=args.max_length,
                            choices=[
                                64,
                                77,
                            ],
                            visible=False,
                        )
                    with gr.Row():
                        inpaint_full_res = gr.Radio(
                            choices=["Whole picture", "Only masked"],
                            type="index",
                            value="Whole picture",
                            label="Inpaint area",
                        )
                        inpaint_full_res_padding = gr.Slider(
                            minimum=0,
                            maximum=256,
                            step=4,
                            value=32,
                            label="Only masked padding, pixels",
                        )
                    with gr.Row():
                        steps = gr.Slider(
                            1, 100, value=args.steps, step=1, label="Steps"
                        )
                        ondemand = gr.Checkbox(
                            value=args.ondemand,
                            label="Low VRAM",
                            interactive=True,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            guidance_scale = gr.Slider(
                                0,
                                50,
                                value=args.guidance_scale,
                                step=0.1,
                                label="CFG Scale",
                            )
                        with gr.Column(scale=3):
                            batch_count = gr.Slider(
                                1,
                                100,
                                value=args.batch_count,
                                step=1,
                                label="Batch Count",
                                interactive=True,
                            )
                        repeatable_seeds = gr.Checkbox(
                            args.repeatable_seeds,
                            label="Repeatable Seeds",
                        )
                    with gr.Row():
                        batch_size = gr.Slider(
                            1,
                            4,
                            value=args.batch_size,
                            step=1,
                            label="Batch Size",
                            interactive=False,
                            visible=False,
                        )
                with gr.Row():
                    seed = gr.Textbox(
                        value=args.seed,
                        label="Seed",
                        info="An integer or a JSON list of integers, -1 for random",
                    )
                    device = gr.Dropdown(
                        elem_id="device",
                        label="Device",
                        value=available_devices[0],
                        choices=available_devices,
                        allow_custom_value=True,
                    )

            with gr.Column(scale=1, min_width=600):
                with gr.Group():
                    inpaint_gallery = gr.Gallery(
                        label="Generated images",
                        show_label=False,
                        elem_id="gallery",
                        columns=[2],
                        object_fit="contain",
                    )
                    std_output = gr.Textbox(
                        value=f"{inpaint_model_info}\n"
                        "Images will be saved at "
                        f"{get_generated_imgs_path()}",
                        lines=2,
                        elem_id="std_output",
                        show_label=False,
                    )
                    inpaint_status = gr.Textbox(visible=False)
                with gr.Row():
                    stable_diffusion = gr.Button("Generate Image(s)")
                    random_seed = gr.Button("Randomize Seed")
                    random_seed.click(
                        lambda: -1,
                        inputs=[],
                        outputs=[seed],
                        queue=False,
                    )
                    stop_batch = gr.Button("Stop Batch")
                with gr.Row():
                    blank_thing_for_row = None
                with gr.Row():
                    inpaint_sendto_img2img = gr.Button(value="SendTo Img2Img")
                    inpaint_sendto_outpaint = gr.Button(
                        value="SendTo Outpaint"
                    )
                    inpaint_sendto_upscaler = gr.Button(
                        value="SendTo Upscaler"
                    )

        kwargs = dict(
            fn=inpaint_inf,
            inputs=[
                prompt,
                negative_prompt,
                inpaint_init_image,
                height,
                width,
                inpaint_full_res,
                inpaint_full_res_padding,
                steps,
                guidance_scale,
                seed,
                batch_count,
                batch_size,
                scheduler,
                inpaint_custom_model,
                custom_vae,
                precision,
                device,
                max_length,
                save_metadata_to_json,
                save_metadata_to_png,
                lora_weights,
                lora_hf_id,
                ondemand,
                repeatable_seeds,
            ],
            outputs=[inpaint_gallery, std_output, inpaint_status],
            show_progress="minimal" if args.progress_bar else "none",
        )
        status_kwargs = dict(
            fn=lambda bc, bs: status_label("Inpaint", 0, bc, bs),
            inputs=[batch_count, batch_size],
            outputs=inpaint_status,
        )

        prompt_submit = prompt.submit(**status_kwargs).then(**kwargs)
        neg_prompt_submit = negative_prompt.submit(**status_kwargs).then(
            **kwargs
        )
        generate_click = stable_diffusion.click(**status_kwargs).then(**kwargs)
        stop_batch.click(
            fn=cancel_sd,
            cancels=[prompt_submit, neg_prompt_submit, generate_click],
        )

        lora_weights.change(
            fn=lora_changed,
            inputs=[lora_weights],
            outputs=[lora_tags],
            queue=True,
        )

```

`apps/stable_diffusion/web/ui/lora_train_ui.py`:

```py
from pathlib import Path
import os
import gradio as gr
from PIL import Image
from apps.stable_diffusion.scripts import lora_train
from apps.stable_diffusion.src import prompt_examples, args, utils
from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    nodlogo_loc,
    get_custom_model_path,
    get_custom_model_files,
    get_custom_vae_or_lora_weights,
    scheduler_list,
    predefined_models,
)

with gr.Blocks(title="Lora Training") as lora_train_web:
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Row():
            with gr.Column(scale=1, elem_id="demo_title_outer"):
                gr.Image(
                    value=nod_logo,
                    show_label=False,
                    interactive=False,
                    elem_id="top_logo",
                    width=150,
                    height=50,
                )
    with gr.Row(elem_id="ui_body"):
        with gr.Row():
            with gr.Column(scale=1, min_width=600):
                with gr.Row():
                    with gr.Column(scale=10):
                        with gr.Row():
                            # janky fix for overflowing text
                            train_lora_model_info = (
                                str(get_custom_model_path())
                            ).replace("\\", "\n\\")
                            train_lora_model_info = (
                                f"Custom Model Path: {train_lora_model_info}"
                            )
                            custom_model = gr.Dropdown(
                                label=f"Models",
                                info=train_lora_model_info,
                                elem_id="custom_model",
                                value=os.path.basename(args.ckpt_loc)
                                if args.ckpt_loc
                                else "None",
                                choices=["None"]
                                + get_custom_model_files()
                                + predefined_models,
                                allow_custom_value=True,
                            )
                            hf_model_id = gr.Textbox(
                                elem_id="hf_model_id",
                                placeholder="Select 'None' in the Models "
                                "dropdown on the left and enter model ID here "
                                "e.g: SG161222/Realistic_Vision_V1.3",
                                value="",
                                label="HuggingFace Model ID",
                                lines=3,
                            )

                with gr.Row():
                    # janky fix for overflowing text
                    train_lora_info = (
                        str(get_custom_model_path("lora"))
                    ).replace("\\", "\n\\")
                    train_lora_info = f"LoRA Path: {train_lora_info}"
                    lora_weights = gr.Dropdown(
                        label=f"Standalone LoRA weights to initialize weights",
                        info=train_lora_info,
                        elem_id="lora_weights",
                        value="None",
                        choices=["None"] + get_custom_model_files("lora"),
                        allow_custom_value=True,
                    )
                    lora_hf_id = gr.Textbox(
                        elem_id="lora_hf_id",
                        placeholder="Select 'None' in the Standalone LoRA "
                        "weights dropdown on the left if you want to use a "
                        "standalone HuggingFace model ID for LoRA here "
                        "e.g: sayakpaul/sd-model-finetuned-lora-t4",
                        value="",
                        label="HuggingFace Model ID to initialize weights",
                        lines=3,
                    )
                with gr.Group(elem_id="image_dir_box_outer"):
                    training_images_dir = gr.Textbox(
                        label="ImageDirectory",
                        value=args.training_images_dir,
                        lines=1,
                        elem_id="prompt_box",
                    )
                with gr.Group(elem_id="prompt_box_outer"):
                    prompt = gr.Textbox(
                        label="Prompt",
                        value=args.prompts[0],
                        lines=2,
                        elem_id="prompt_box",
                    )
                with gr.Accordion(label="Advanced Options", open=False):
                    with gr.Row():
                        scheduler = gr.Dropdown(
                            elem_id="scheduler",
                            label="Scheduler",
                            value=args.scheduler,
                            choices=scheduler_list,
                            allow_custom_value=True,
                        )
                    with gr.Row():
                        height = gr.Slider(
                            384, 768, value=args.height, step=8, label="Height"
                        )
                        width = gr.Slider(
                            384, 768, value=args.width, step=8, label="Width"
                        )
                        precision = gr.Radio(
                            label="Precision",
                            value=args.precision,
                            choices=[
                                "fp16",
                                "fp32",
                            ],
                            visible=False,
                        )
                        max_length = gr.Radio(
                            label="Max Length",
                            value=args.max_length,
                            choices=[
                                64,
                                77,
                            ],
                            visible=False,
                        )
                    with gr.Row():
                        steps = gr.Slider(
                            1,
                            2000,
                            value=args.training_steps,
                            step=1,
                            label="Training Steps",
                        )
                        guidance_scale = gr.Slider(
                            0,
                            50,
                            value=args.guidance_scale,
                            step=0.1,
                            label="CFG Scale",
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            batch_count = gr.Slider(
                                1,
                                100,
                                value=args.batch_count,
                                step=1,
                                label="Batch Count",
                                interactive=True,
                            )
                        with gr.Column(scale=3):
                            batch_size = gr.Slider(
                                1,
                                4,
                                value=args.batch_size,
                                step=1,
                                label="Batch Size",
                                interactive=True,
                            )
                        stop_batch = gr.Button("Stop Batch")
                with gr.Row():
                    seed = gr.Number(
                        value=utils.parse_seed_input(args.seed)[0],
                        precision=0,
                        label="Seed",
                    )
                    device = gr.Dropdown(
                        elem_id="device",
                        label="Device",
                        value=available_devices[0],
                        choices=available_devices,
                        allow_custom_value=True,
                    )
                with gr.Row():
                    with gr.Column(scale=2):
                        random_seed = gr.Button("Randomize Seed")
                        random_seed.click(
                            lambda: -1,
                            inputs=[],
                            outputs=[seed],
                            queue=False,
                        )
                    with gr.Column(scale=6):
                        train_lora = gr.Button("Train LoRA")

                with gr.Accordion(label="Prompt Examples!", open=False):
                    ex = gr.Examples(
                        examples=prompt_examples,
                        inputs=prompt,
                        cache_examples=False,
                        elem_id="prompt_examples",
                    )

            with gr.Column(scale=1, min_width=600):
                with gr.Group():
                    std_output = gr.Textbox(
                        value="Nothing to show.",
                        lines=1,
                        show_label=False,
                    )
                lora_save_dir = (
                    args.lora_save_dir if args.lora_save_dir else Path.cwd()
                )
                lora_save_dir = Path(lora_save_dir, "lora")
                output_loc = gr.Textbox(
                    label="Saving Lora at",
                    value=lora_save_dir,
                )

        kwargs = dict(
            fn=lora_train,
            inputs=[
                prompt,
                height,
                width,
                steps,
                guidance_scale,
                seed,
                batch_count,
                batch_size,
                scheduler,
                custom_model,
                hf_model_id,
                precision,
                device,
                max_length,
                training_images_dir,
                output_loc,
                get_custom_vae_or_lora_weights(
                    lora_weights, lora_hf_id, "lora"
                ),
            ],
            outputs=[std_output],
            show_progress="minimal" if args.progress_bar else "none",
        )

        prompt_submit = prompt.submit(**kwargs)
        train_click = train_lora.click(**kwargs)
        stop_batch.click(fn=None, cancels=[prompt_submit, train_click])

```

`apps/stable_diffusion/web/ui/minigpt4_ui.py`:

```py
# ========================================
#             Gradio Setting
# ========================================
import gradio as gr

# from apps.language_models.src.pipelines.minigpt4_pipeline import (
#     # MiniGPT4,
#     CONV_VISION,
# )
from pathlib import Path

chat = None


def gradio_reset(chat_state, img_list):
    if chat_state is not None:
        chat_state.messages = []
    if img_list is not None:
        img_list = []
    return (
        None,
        gr.update(value=None, interactive=True),
        gr.update(
            placeholder="Please upload your image first", interactive=False
        ),
        gr.update(value="Upload & Start Chat", interactive=True),
        chat_state,
        img_list,
    )


def upload_img(gr_img, text_input, chat_state, device, precision, _compile):
    global chat
    if chat is None:
        from apps.language_models.src.pipelines.minigpt4_pipeline import (
            MiniGPT4,
            CONV_VISION,
        )

        vision_model_precision = precision
        if precision in ["int4", "int8"]:
            vision_model_precision = "fp16"
        vision_model_vmfb_path = Path(
            f"vision_model_{vision_model_precision}_{device}.vmfb"
        )
        qformer_vmfb_path = Path(f"qformer_fp32_{device}.vmfb")
        chat = MiniGPT4(
            model_name="MiniGPT4",
            hf_model_path=None,
            max_new_tokens=30,
            device=device,
            precision=precision,
            _compile=_compile,
            vision_model_vmfb_path=vision_model_vmfb_path,
            qformer_vmfb_path=qformer_vmfb_path,
        )
    if gr_img is None:
        return None, None, gr.update(interactive=True), chat_state, None
    chat_state = CONV_VISION.copy()
    img_list = []
    llm_message = chat.upload_img(gr_img, chat_state, img_list)
    return (
        gr.update(interactive=False),
        gr.update(interactive=True, placeholder="Type and press Enter"),
        gr.update(value="Start Chatting", interactive=False),
        chat_state,
        img_list,
    )


def gradio_ask(user_message, chatbot, chat_state):
    if len(user_message) == 0:
        return (
            gr.update(
                interactive=True, placeholder="Input should not be empty!"
            ),
            chatbot,
            chat_state,
        )
    chat.ask(user_message, chat_state)
    chatbot = chatbot + [[user_message, None]]
    return "", chatbot, chat_state


def gradio_answer(chatbot, chat_state, img_list, num_beams, temperature):
    llm_message = chat.answer(
        conv=chat_state,
        img_list=img_list,
        num_beams=num_beams,
        temperature=temperature,
        max_new_tokens=300,
        max_length=2000,
    )[0]
    print(llm_message)
    print("************")
    chatbot[-1][1] = llm_message
    return chatbot, chat_state, img_list


title = """<h1 align="center">MultiModal SHARK (experimental)</h1>"""
description = """<h3>Upload your images and start chatting!</h3>"""
article = """<p><a href='https://minigpt-4.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a></p><p><a href='https://github.com/Vision-CAIR/MiniGPT-4'><img src='https://img.shields.io/badge/Github-Code-blue'></a></p><p><a href='https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/MiniGPT_4.pdf'><img src='https://img.shields.io/badge/Paper-PDF-red'></a></p>
"""

# TODO show examples below

with gr.Blocks() as minigpt4_web:
    gr.Markdown(title)
    gr.Markdown(description)

    with gr.Row():
        with gr.Column():
            image = gr.Image(type="pil")
            upload_button = gr.Button(
                value="Upload & Start Chat",
                interactive=True,
                variant="primary",
            )
            clear = gr.Button("Restart")

            num_beams = gr.Slider(
                minimum=1,
                maximum=10,
                value=1,
                step=1,
                interactive=True,
                label="beam search numbers)",
            )

            temperature = gr.Slider(
                minimum=0.1,
                maximum=2.0,
                value=1.0,
                step=0.1,
                interactive=True,
                label="Temperature",
            )

            device = gr.Dropdown(
                label="Device",
                value="cuda",
                # if enabled
                # else "Only CUDA Supported for now",
                choices=["cuda"],
                interactive=False,
                allow_custom_value=True,
            )

        with gr.Column():
            chat_state = gr.State()
            img_list = gr.State()
            chatbot = gr.Chatbot(label="MiniGPT-4")
            text_input = gr.Textbox(
                label="User",
                placeholder="Please upload your image first",
                interactive=False,
            )
            precision = gr.Radio(
                label="Precision",
                value="int8",
                choices=[
                    "int8",
                    "fp16",
                    "fp32",
                ],
                visible=True,
            )
            _compile = gr.Checkbox(
                value=False,
                label="Compile",
                interactive=True,
            )

    upload_button.click(
        upload_img,
        [image, text_input, chat_state, device, precision, _compile],
        [image, text_input, upload_button, chat_state, img_list],
    )

    text_input.submit(
        gradio_ask,
        [text_input, chatbot, chat_state],
        [text_input, chatbot, chat_state],
    ).then(
        gradio_answer,
        [chatbot, chat_state, img_list, num_beams, temperature],
        [chatbot, chat_state, img_list],
    )
    clear.click(
        gradio_reset,
        [chat_state, img_list],
        [chatbot, image, text_input, upload_button, chat_state, img_list],
        queue=False,
    )

```

`apps/stable_diffusion/web/ui/model_manager.py`:

```py
import os
import gradio as gr
import requests
from io import BytesIO
from PIL import Image


def get_hf_list(num_of_models=20):
    path = "https://huggingface.co/api/models"
    params = {
        "search": "stable-diffusion",
        "sort": "downloads",
        "direction": "-1",
        "limit": {num_of_models},
        "full": "true",
    }
    response = requests.get(path, params=params)
    return response.json()


def get_civit_list(num_of_models=50):
    path = (
        f"https://civitai.com/api/v1/models?limit="
        f"{num_of_models}&types=Checkpoint"
    )
    headers = {"Content-Type": "application/json"}
    raw_json = requests.get(path, headers=headers).json()
    models = list(raw_json.items())[0][1]
    safe_models = [
        safe_model for safe_model in models if not safe_model["nsfw"]
    ]
    version_id = 0  # Currently just using the first version.
    safe_models = [
        safe_model
        for safe_model in safe_models
        if safe_model["modelVersions"][version_id]["files"][0]["metadata"][
            "format"
        ]
        == "SafeTensor"
    ]
    first_version_models = []
    for model_iter in safe_models:
        # The modelVersion would only keep the version name.
        if (
            model_iter["modelVersions"][version_id]["images"][0]["nsfw"]
            != "None"
        ):
            continue
        model_iter["modelVersions"][version_id]["modelName"] = model_iter[
            "name"
        ]
        model_iter["modelVersions"][version_id]["rating"] = model_iter[
            "stats"
        ]["rating"]
        model_iter["modelVersions"][version_id]["favoriteCount"] = model_iter[
            "stats"
        ]["favoriteCount"]
        model_iter["modelVersions"][version_id]["downloadCount"] = model_iter[
            "stats"
        ]["downloadCount"]
        first_version_models.append(model_iter["modelVersions"][version_id])
    return first_version_models


def get_image_from_model(model_json):
    model_id = model_json["modelId"]
    image = None
    for img_info in model_json["images"]:
        if img_info["nsfw"] == "None":
            image_url = model_json["images"][0]["url"]
            response = requests.get(image_url)
            image = BytesIO(response.content)
            break
    return image


with gr.Blocks() as model_web:
    with gr.Row():
        model_source = gr.Radio(
            value=None,
            choices=["Hugging Face", "Civitai"],
            type="value",
            label="Model Source",
        )
        model_number = gr.Slider(
            1,
            100,
            value=10,
            step=1,
            label="Number of models",
            interactive=True,
        )
        # TODO: add more filters
    get_model_btn = gr.Button(value="Get Models")

    hf_models = gr.Dropdown(
        label="Hugging Face Model List",
        choices=None,
        value=None,
        visible=False,
        allow_custom_value=True,
    )
    # TODO: select and SendTo
    civit_models = gr.Gallery(
        label="Civitai Model Gallery",
        value=None,
        interactive=True,
        visible=False,
    )

    with gr.Row(visible=False) as sendto_btns:
        modelmanager_sendto_txt2img = gr.Button(value="SendTo Txt2Img")
        modelmanager_sendto_img2img = gr.Button(value="SendTo Img2Img")
        modelmanager_sendto_inpaint = gr.Button(value="SendTo Inpaint")
        modelmanager_sendto_outpaint = gr.Button(value="SendTo Outpaint")
        modelmanager_sendto_upscaler = gr.Button(value="SendTo Upscaler")

    def get_model_list(model_source, model_number):
        if model_source == "Hugging Face":
            hf_model_list = get_hf_list(model_number)
            models = []
            for model in hf_model_list:
                # TODO: add model info
                models.append(f'{model["modelId"]}')
            return (
                gr.Dropdown.update(choices=models, visible=True),
                gr.Gallery.update(value=None, visible=False),
                gr.Row.update(visible=True),
            )
        elif model_source == "Civitai":
            civit_model_list = get_civit_list(model_number)
            models = []
            for model in civit_model_list:
                image = get_image_from_model(model)
                if image is None:
                    continue
                # TODO: add model info
                models.append(
                    (Image.open(image), f'{model["files"][0]["downloadUrl"]}')
                )
            return (
                gr.Dropdown.update(value=None, choices=None, visible=False),
                gr.Gallery.update(value=models, visible=True),
                gr.Row.update(visible=False),
            )
        else:
            return (
                gr.Dropdown.update(value=None, choices=None, visible=False),
                gr.Gallery.update(value=None, visible=False),
                gr.Row.update(visible=False),
            )

    get_model_btn.click(
        fn=get_model_list,
        inputs=[model_source, model_number],
        outputs=[
            hf_models,
            civit_models,
            sendto_btns,
        ],
    )

```

`apps/stable_diffusion/web/ui/outpaint_ui.py`:

```py
import os
import torch
import time
import gradio as gr
from PIL import Image

from apps.stable_diffusion.web.ui.common_ui_events import lora_changed
from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    nodlogo_loc,
    get_custom_model_path,
    get_custom_model_files,
    scheduler_list_cpu_only,
    predefined_paint_models,
    cancel_sd,
)
from apps.stable_diffusion.src import (
    args,
    OutpaintPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    save_output_img,
)
from apps.stable_diffusion.src.utils import (
    get_generated_imgs_path,
    get_generation_text_info,
)
from apps.stable_diffusion.web.utils.common_label_calc import status_label

# set initial values of iree_vulkan_target_triple, use_tuned and import_mlir.
init_iree_vulkan_target_triple = args.iree_vulkan_target_triple
init_use_tuned = args.use_tuned
init_import_mlir = args.import_mlir


# Exposed to UI.
def outpaint_inf(
    prompt: str,
    negative_prompt: str,
    init_image,
    pixels: int,
    mask_blur: int,
    directions: list,
    noise_q: float,
    color_variation: float,
    height: int,
    width: int,
    steps: int,
    guidance_scale: float,
    seed: str,
    batch_count: int,
    batch_size: int,
    scheduler: str,
    model_id: str,
    custom_vae: str,
    precision: str,
    device: str,
    max_length: int,
    save_metadata_to_json: bool,
    save_metadata_to_png: bool,
    lora_weights: str,
    lora_hf_id: str,
    ondemand: bool,
    repeatable_seeds: bool,
):
    from apps.stable_diffusion.web.ui.utils import (
        get_custom_model_pathfile,
        get_custom_vae_or_lora_weights,
        Config,
    )
    import apps.stable_diffusion.web.utils.global_obj as global_obj
    from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
        SD_STATE_CANCEL,
    )

    args.prompts = [prompt]
    args.negative_prompts = [negative_prompt]
    args.guidance_scale = guidance_scale
    args.steps = steps
    args.scheduler = scheduler
    args.img_path = "not none"
    args.ondemand = ondemand

    # set ckpt_loc and hf_model_id.
    args.ckpt_loc = ""
    args.hf_model_id = ""
    args.custom_vae = ""

    # .safetensor or .chkpt on the custom model path
    if model_id in get_custom_model_files(custom_checkpoint_type="inpainting"):
        args.ckpt_loc = get_custom_model_pathfile(model_id)
    # civitai download
    elif "civitai" in model_id:
        args.ckpt_loc = model_id
    # either predefined or huggingface
    else:
        args.hf_model_id = model_id

    if custom_vae != "None":
        args.custom_vae = get_custom_model_pathfile(custom_vae, model="vae")

    args.use_lora = get_custom_vae_or_lora_weights(
        lora_weights, lora_hf_id, "lora"
    )

    args.save_metadata_to_json = save_metadata_to_json
    args.write_metadata_to_png = save_metadata_to_png

    dtype = torch.float32 if precision == "fp32" else torch.half
    cpu_scheduling = not scheduler.startswith("Shark")
    new_config_obj = Config(
        "outpaint",
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        precision,
        batch_size,
        max_length,
        height,
        width,
        device,
        use_lora=args.use_lora,
        stencils=[],
        ondemand=ondemand,
    )
    if (
        not global_obj.get_sd_obj()
        or global_obj.get_cfg_obj() != new_config_obj
    ):
        global_obj.clear_cache()
        global_obj.set_cfg_obj(new_config_obj)
        args.precision = precision
        args.batch_count = batch_count
        args.batch_size = batch_size
        args.max_length = max_length
        args.height = height
        args.width = width
        args.device = device.split("=>", 1)[1].strip()
        args.iree_vulkan_target_triple = init_iree_vulkan_target_triple
        args.use_tuned = init_use_tuned
        args.import_mlir = init_import_mlir
        set_init_device_flags()
        model_id = (
            args.hf_model_id
            if args.hf_model_id
            else "stabilityai/stable-diffusion-2-inpainting"
        )
        global_obj.set_schedulers(get_schedulers(model_id))
        scheduler_obj = global_obj.get_scheduler(scheduler)
        global_obj.set_sd_obj(
            OutpaintPipeline.from_pretrained(
                scheduler_obj,
                args.import_mlir,
                args.hf_model_id,
                args.ckpt_loc,
                args.custom_vae,
                args.precision,
                args.max_length,
                args.batch_size,
                args.height,
                args.width,
                args.use_base_vae,
                args.use_tuned,
                use_lora=args.use_lora,
                ondemand=args.ondemand,
            )
        )

    global_obj.set_sd_scheduler(scheduler)

    start_time = time.time()
    global_obj.get_sd_obj().log = ""
    generated_imgs = []
    try:
        seeds = utils.batch_seeds(seed, batch_count, repeatable_seeds)
    except TypeError as error:
        raise gr.Error(str(error)) from None

    left = True if "left" in directions else False
    right = True if "right" in directions else False
    top = True if "up" in directions else False
    bottom = True if "down" in directions else False

    text_output = ""
    for current_batch in range(batch_count):
        out_imgs = global_obj.get_sd_obj().generate_images(
            prompt,
            negative_prompt,
            init_image,
            pixels,
            mask_blur,
            left,
            right,
            top,
            bottom,
            noise_q,
            color_variation,
            batch_size,
            height,
            width,
            steps,
            guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )
        total_time = time.time() - start_time
        text_output = get_generation_text_info(
            seeds[: current_batch + 1], device
        )
        text_output += "\n" + global_obj.get_sd_obj().log
        text_output += f"\nTotal image(s) generation time: {total_time:.4f}sec"

        if global_obj.get_sd_status() == SD_STATE_CANCEL:
            break
        else:
            save_output_img(out_imgs[0], seeds[current_batch])
            generated_imgs.extend(out_imgs)
            yield generated_imgs, text_output, status_label(
                "Outpaint", current_batch + 1, batch_count, batch_size
            )

    return generated_imgs, text_output, ""


with gr.Blocks(title="Outpainting") as outpaint_web:
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Row():
            with gr.Column(scale=1, elem_id="demo_title_outer"):
                gr.Image(
                    value=nod_logo,
                    show_label=False,
                    interactive=False,
                    elem_id="top_logo",
                    width=150,
                    height=50,
                )
    with gr.Row(elem_id="ui_body"):
        with gr.Row():
            with gr.Column(scale=1, min_width=600):
                with gr.Row():
                    outpaint_model_info = (
                        f"Custom Model Path: {str(get_custom_model_path())}"
                    )
                    outpaint_custom_model = gr.Dropdown(
                        label=f"Models",
                        info="Select, or enter HuggingFace Model ID or Civitai model download URL",
                        elem_id="custom_model",
                        value=os.path.basename(args.ckpt_loc)
                        if args.ckpt_loc
                        else "stabilityai/stable-diffusion-2-inpainting",
                        choices=get_custom_model_files(
                            custom_checkpoint_type="inpainting"
                        )
                        + predefined_paint_models,
                        allow_custom_value=True,
                        scale=2,
                    )
                    # janky fix for overflowing text
                    outpaint_vae_info = (
                        str(get_custom_model_path("vae"))
                    ).replace("\\", "\n\\")
                    outpaint_vae_info = f"VAE Path: {outpaint_vae_info}"
                    custom_vae = gr.Dropdown(
                        label=f"Custom VAE Models",
                        info=outpaint_vae_info,
                        elem_id="custom_model",
                        value=os.path.basename(args.custom_vae)
                        if args.custom_vae
                        else "None",
                        choices=["None"] + get_custom_model_files("vae"),
                        allow_custom_value=True,
                        scale=1,
                    )
                with gr.Group(elem_id="prompt_box_outer"):
                    prompt = gr.Textbox(
                        label="Prompt",
                        value=args.prompts[0],
                        lines=2,
                        elem_id="prompt_box",
                    )
                    negative_prompt = gr.Textbox(
                        label="Negative Prompt",
                        value=args.negative_prompts[0],
                        lines=2,
                        elem_id="negative_prompt_box",
                    )

                outpaint_init_image = gr.Image(
                    label="Input Image",
                    type="pil",
                    height=300,
                )

                with gr.Accordion(label="LoRA Options", open=False):
                    with gr.Row():
                        # janky fix for overflowing text
                        outpaint_lora_info = (
                            str(get_custom_model_path("lora"))
                        ).replace("\\", "\n\\")
                        outpaint_lora_info = f"LoRA Path: {outpaint_lora_info}"
                        lora_weights = gr.Dropdown(
                            label=f"Standalone LoRA Weights",
                            info=outpaint_lora_info,
                            elem_id="lora_weights",
                            value="None",
                            choices=["None"] + get_custom_model_files("lora"),
                            allow_custom_value=True,
                        )
                        lora_hf_id = gr.Textbox(
                            elem_id="lora_hf_id",
                            placeholder="Select 'None' in the Standalone LoRA "
                            "weights dropdown on the left if you want to use "
                            "a standalone HuggingFace model ID for LoRA here "
                            "e.g: sayakpaul/sd-model-finetuned-lora-t4",
                            value="",
                            label="HuggingFace Model ID",
                            lines=3,
                        )
                    with gr.Row():
                        lora_tags = gr.HTML(
                            value="<div><i>No LoRA selected</i></div>",
                            elem_classes="lora-tags",
                        )
                with gr.Accordion(label="Advanced Options", open=False):
                    with gr.Row():
                        scheduler = gr.Dropdown(
                            elem_id="scheduler",
                            label="Scheduler",
                            value="EulerDiscrete",
                            choices=scheduler_list_cpu_only,
                            allow_custom_value=True,
                        )
                        with gr.Group():
                            save_metadata_to_png = gr.Checkbox(
                                label="Save prompt information to PNG",
                                value=args.write_metadata_to_png,
                                interactive=True,
                            )
                            save_metadata_to_json = gr.Checkbox(
                                label="Save prompt information to JSON file",
                                value=args.save_metadata_to_json,
                                interactive=True,
                            )
                    with gr.Row():
                        pixels = gr.Slider(
                            8,
                            256,
                            value=args.pixels,
                            step=8,
                            label="Pixels to expand",
                        )
                        mask_blur = gr.Slider(
                            0,
                            64,
                            value=args.mask_blur,
                            step=1,
                            label="Mask blur",
                        )
                    with gr.Row():
                        directions = gr.CheckboxGroup(
                            label="Outpainting direction",
                            choices=["left", "right", "up", "down"],
                            value=["left", "right", "up", "down"],
                        )
                    with gr.Row():
                        noise_q = gr.Slider(
                            0.0,
                            4.0,
                            value=1.0,
                            step=0.01,
                            label="Fall-off exponent (lower=higher detail)",
                        )
                        color_variation = gr.Slider(
                            0.0,
                            1.0,
                            value=0.05,
                            step=0.01,
                            label="Color variation",
                        )
                    with gr.Row():
                        height = gr.Slider(
                            384, 768, value=args.height, step=8, label="Height"
                        )
                        width = gr.Slider(
                            384, 768, value=args.width, step=8, label="Width"
                        )
                        precision = gr.Radio(
                            label="Precision",
                            value=args.precision,
                            choices=[
                                "fp16",
                                "fp32",
                            ],
                            visible=False,
                        )
                        max_length = gr.Radio(
                            label="Max Length",
                            value=args.max_length,
                            choices=[
                                64,
                                77,
                            ],
                            visible=False,
                        )
                    with gr.Row():
                        steps = gr.Slider(
                            1, 100, value=20, step=1, label="Steps"
                        )
                        ondemand = gr.Checkbox(
                            value=args.ondemand,
                            label="Low VRAM",
                            interactive=True,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            guidance_scale = gr.Slider(
                                0,
                                50,
                                value=args.guidance_scale,
                                step=0.1,
                                label="CFG Scale",
                            )
                        with gr.Column(scale=3):
                            batch_count = gr.Slider(
                                1,
                                100,
                                value=args.batch_count,
                                step=1,
                                label="Batch Count",
                                interactive=True,
                            )
                        repeatable_seeds = gr.Checkbox(
                            args.repeatable_seeds,
                            label="Repeatable Seeds",
                        )

                    with gr.Row():
                        batch_size = gr.Slider(
                            1,
                            4,
                            value=args.batch_size,
                            step=1,
                            label="Batch Size",
                            interactive=False,
                            visible=False,
                        )
                with gr.Row():
                    seed = gr.Textbox(
                        value=args.seed,
                        label="Seed",
                        info="An integer or a JSON list of integers, -1 for random",
                    )
                    device = gr.Dropdown(
                        elem_id="device",
                        label="Device",
                        value=available_devices[0],
                        choices=available_devices,
                        allow_custom_value=True,
                    )

            with gr.Column(scale=1, min_width=600):
                with gr.Group():
                    outpaint_gallery = gr.Gallery(
                        label="Generated images",
                        show_label=False,
                        elem_id="gallery",
                        columns=[2],
                        object_fit="contain",
                    )
                    std_output = gr.Textbox(
                        value=f"{outpaint_model_info}\n"
                        f"Images will be saved at "
                        f"{get_generated_imgs_path()}",
                        lines=2,
                        elem_id="std_output",
                        show_label=False,
                    )
                    outpaint_status = gr.Textbox(visible=False)
                with gr.Row():
                    stable_diffusion = gr.Button("Generate Image(s)")
                    random_seed = gr.Button("Randomize Seed")
                    random_seed.click(
                        lambda: -1,
                        inputs=[],
                        outputs=[seed],
                        queue=False,
                    )
                    stop_batch = gr.Button("Stop Batch")
                with gr.Row():
                    blank_thing_for_row = None
                with gr.Row():
                    outpaint_sendto_img2img = gr.Button(value="SendTo Img2Img")
                    outpaint_sendto_inpaint = gr.Button(value="SendTo Inpaint")
                    outpaint_sendto_upscaler = gr.Button(
                        value="SendTo Upscaler"
                    )

        kwargs = dict(
            fn=outpaint_inf,
            inputs=[
                prompt,
                negative_prompt,
                outpaint_init_image,
                pixels,
                mask_blur,
                directions,
                noise_q,
                color_variation,
                height,
                width,
                steps,
                guidance_scale,
                seed,
                batch_count,
                batch_size,
                scheduler,
                outpaint_custom_model,
                custom_vae,
                precision,
                device,
                max_length,
                save_metadata_to_json,
                save_metadata_to_png,
                lora_weights,
                lora_hf_id,
                ondemand,
                repeatable_seeds,
            ],
            outputs=[outpaint_gallery, std_output, outpaint_status],
            show_progress="minimal" if args.progress_bar else "none",
        )
        status_kwargs = dict(
            fn=lambda bc, bs: status_label("Outpaint", 0, bc, bs),
            inputs=[batch_count, batch_size],
            outputs=outpaint_status,
        )

        prompt_submit = prompt.submit(**status_kwargs).then(**kwargs)
        neg_prompt_submit = negative_prompt.submit(**status_kwargs).then(
            **kwargs
        )
        generate_click = stable_diffusion.click(**status_kwargs).then(**kwargs)
        stop_batch.click(
            fn=cancel_sd,
            cancels=[prompt_submit, neg_prompt_submit, generate_click],
        )

        lora_weights.change(
            fn=lora_changed,
            inputs=[lora_weights],
            outputs=[lora_tags],
            queue=True,
        )

```

`apps/stable_diffusion/web/ui/outputgallery_ui.py`:

```py
import glob
import gradio as gr
import os
import subprocess
import sys
from PIL import Image

from apps.stable_diffusion.src import args
from apps.stable_diffusion.src.utils import (
    get_generated_imgs_path,
    get_generated_imgs_todays_subdir,
)
from apps.stable_diffusion.web.ui.utils import nodlogo_loc
from apps.stable_diffusion.web.utils.metadata import displayable_metadata

# -- Functions for file, directory and image info querying

output_dir = get_generated_imgs_path()


def outputgallery_filenames(subdir) -> list[str]:
    new_dir_path = os.path.join(output_dir, subdir)
    if os.path.exists(new_dir_path):
        filenames = [
            glob.glob(new_dir_path + "/" + ext)
            for ext in ("*.png", "*.jpg", "*.jpeg")
        ]

        return sorted(sum(filenames, []), key=os.path.getmtime, reverse=True)
    else:
        return []


def output_subdirs() -> list[str]:
    # Gets a list of subdirectories of output_dir and below, as relative paths.
    relative_paths = [
        os.path.relpath(entry[0], output_dir)
        for entry in os.walk(
            output_dir, followlinks=args.output_gallery_followlinks
        )
    ]

    # It is less confusing to always including the subdir that will take any
    # images generated today even if it doesn't exist yet
    if get_generated_imgs_todays_subdir() not in relative_paths:
        relative_paths.append(get_generated_imgs_todays_subdir())

    # sort subdirectories so that the date named ones we probably
    # created in this or previous sessions come first, sorted with the most
    # recent first. Other subdirs are listed after.
    generated_paths = sorted(
        [path for path in relative_paths if path.isnumeric()], reverse=True
    )
    result_paths = generated_paths + sorted(
        [
            path
            for path in relative_paths
            if (not path.isnumeric()) and path != "."
        ]
    )

    return result_paths


# --- Define UI layout for Gradio

with gr.Blocks() as outputgallery_web:
    nod_logo = Image.open(nodlogo_loc)

    with gr.Row(elem_id="outputgallery_gallery"):
        # needed to workaround gradio issue:
        # https://github.com/gradio-app/gradio/issues/2907
        dev_null = gr.Textbox("", visible=False)

        gallery_files = gr.State(value=[])
        subdirectory_paths = gr.State(value=[])

        with gr.Column(scale=6):
            logo = gr.Image(
                label="Getting subdirectories...",
                value=nod_logo,
                interactive=False,
                visible=True,
                show_label=True,
                elem_id="top_logo",
                elem_classes="logo_centered",
            )

            gallery = gr.Gallery(
                label="",
                value=gallery_files.value,
                visible=False,
                show_label=True,
                columns=4,
            )

        with gr.Column(scale=4):
            with gr.Box():
                with gr.Row():
                    with gr.Column(
                        scale=15,
                        min_width=160,
                        elem_id="output_subdir_container",
                    ):
                        subdirectories = gr.Dropdown(
                            label=f"Subdirectories of {output_dir}",
                            type="value",
                            choices=subdirectory_paths.value,
                            value="",
                            interactive=True,
                            elem_classes="dropdown_no_container",
                            allow_custom_value=True,
                        )
                    with gr.Column(
                        scale=1,
                        min_width=32,
                        elem_classes="output_icon_button",
                    ):
                        open_subdir = gr.Button(
                            variant="secondary",
                            value="\U0001F5C1",  # unicode open folder
                            interactive=False,
                            size="sm",
                        )
                    with gr.Column(
                        scale=1,
                        min_width=32,
                        elem_classes="output_icon_button",
                    ):
                        refresh = gr.Button(
                            variant="secondary",
                            value="\u21BB",  # unicode clockwise arrow circle
                            size="sm",
                        )

            image_columns = gr.Slider(
                label="Columns shown", value=4, minimum=1, maximum=16, step=1
            )
            outputgallery_filename = gr.Textbox(
                label="Filename",
                value="None",
                interactive=False,
                show_copy_button=True,
            )

            with gr.Accordion(
                label="Parameter Information", open=False
            ) as parameters_accordian:
                image_parameters = gr.DataFrame(
                    headers=["Parameter", "Value"],
                    col_count=2,
                    wrap=True,
                    elem_classes="output_parameters_dataframe",
                    value=[["Status", "No image selected"]],
                )

            with gr.Accordion(label="Send To", open=True):
                with gr.Row():
                    outputgallery_sendto_txt2img = gr.Button(
                        value="Txt2Img",
                        interactive=False,
                        elem_classes="outputgallery_sendto",
                        size="sm",
                    )

                    outputgallery_sendto_img2img = gr.Button(
                        value="Img2Img",
                        interactive=False,
                        elem_classes="outputgallery_sendto",
                        size="sm",
                    )

                    outputgallery_sendto_inpaint = gr.Button(
                        value="Inpaint",
                        interactive=False,
                        elem_classes="outputgallery_sendto",
                        size="sm",
                    )

                    outputgallery_sendto_outpaint = gr.Button(
                        value="Outpaint",
                        interactive=False,
                        elem_classes="outputgallery_sendto",
                        size="sm",
                    )

                    outputgallery_sendto_upscaler = gr.Button(
                        value="Upscaler",
                        interactive=False,
                        elem_classes="outputgallery_sendto",
                        size="sm",
                    )

    # --- Event handlers

    def on_clear_gallery():
        return [
            gr.Gallery.update(
                value=[],
                visible=False,
            ),
            gr.Image.update(
                visible=True,
            ),
        ]

    def on_image_columns_change(columns):
        return gr.Gallery.update(columns=columns)

    def on_select_subdir(subdir) -> list:
        # evt.value is the subdirectory name
        new_images = outputgallery_filenames(subdir)
        new_label = (
            f"{len(new_images)} images in {os.path.join(output_dir, subdir)}"
        )
        return [
            new_images,
            gr.Gallery.update(
                value=new_images,
                label=new_label,
                visible=len(new_images) > 0,
            ),
            gr.Image.update(
                label=new_label,
                visible=len(new_images) == 0,
            ),
        ]

    def on_open_subdir(subdir):
        subdir_path = os.path.normpath(os.path.join(output_dir, subdir))

        if os.path.isdir(subdir_path):
            if sys.platform == "linux":
                subprocess.run(["xdg-open", subdir_path])
            elif sys.platform == "darwin":
                subprocess.run(["open", subdir_path])
            elif sys.platform == "win32":
                os.startfile(subdir_path)

    def on_refresh(current_subdir: str) -> list:
        # get an up-to-date subdirectory list
        refreshed_subdirs = output_subdirs()
        # get the images using either the current subdirectory or the most
        # recent valid one
        new_subdir = (
            current_subdir
            if current_subdir in refreshed_subdirs
            else refreshed_subdirs[0]
        )
        new_images = outputgallery_filenames(new_subdir)
        new_label = (
            f"{len(new_images)} images in "
            f"{os.path.join(output_dir, new_subdir)}"
        )

        return [
            gr.Dropdown.update(
                choices=refreshed_subdirs,
                value=new_subdir,
            ),
            refreshed_subdirs,
            new_images,
            gr.Gallery.update(
                value=new_images, label=new_label, visible=len(new_images) > 0
            ),
            gr.Image.update(
                label=new_label,
                visible=len(new_images) == 0,
            ),
        ]

    def on_new_image(subdir, subdir_paths, status) -> list:
        # prevent error triggered when an image generates before the tab
        # has even been selected
        subdir_paths = (
            subdir_paths
            if len(subdir_paths) > 0
            else [get_generated_imgs_todays_subdir()]
        )

        # only update if the current subdir is the most recent one as
        # new images only go there
        if subdir_paths[0] == subdir:
            new_images = outputgallery_filenames(subdir)
            new_label = (
                f"{len(new_images)} images in "
                f"{os.path.join(output_dir, subdir)} - {status}"
            )

            return [
                new_images,
                gr.Gallery.update(
                    value=new_images,
                    label=new_label,
                    visible=len(new_images) > 0,
                ),
                gr.Image.update(
                    label=new_label,
                    visible=len(new_images) == 0,
                ),
            ]
        else:
            # otherwise change nothing,
            # (only untyped gradio gr.update() does this)
            return [gr.update(), gr.update(), gr.update()]

    def on_select_image(images: list[str], evt: gr.SelectData) -> list:
        # evt.index is an index into the full list of filenames for
        # the current subdirectory
        filename = images[evt.index]
        params = displayable_metadata(filename)

        if params:
            if params["source"] == "missing":
                return [
                    "Could not find this image file, refresh the gallery and update the images",
                    [["Status", "File missing"]],
                ]
            else:
                return [
                    filename,
                    list(map(list, params["parameters"].items())),
                ]

        return [
            filename,
            [["Status", "No parameters found"]],
        ]

    def on_outputgallery_filename_change(filename: str) -> list:
        exists = filename != "None" and os.path.exists(filename)
        return [
            # disable or enable each of the sendto button based on whether
            # an image is selected
            gr.Button.update(interactive=exists),
            gr.Button.update(interactive=exists),
            gr.Button.update(interactive=exists),
            gr.Button.update(interactive=exists),
            gr.Button.update(interactive=exists),
            gr.Button.update(interactive=exists),
        ]

    # The time first our tab is selected we need to do an initial refresh
    # to populate the subdirectory select box and the images from the most
    # recent subdirectory.
    #
    # We do it at this point rather than setting this up in the controls'
    # definitions as when you refresh the browser you always get what was
    # *initially* set, which won't include any new subdirectories or images
    # that might have created since the application was started. Doing it
    # this way means a browser refresh/reload always gets the most
    # up-to-date data.
    def on_select_tab(subdir_paths, request: gr.Request):
        local_client = request.headers["host"].startswith(
            "127.0.0.1:"
        ) or request.headers["host"].startswith("localhost:")

        if len(subdir_paths) == 0:
            return on_refresh("") + [gr.update(interactive=local_client)]
        else:
            return (
                # Change nothing, (only untyped gr.update() does this)
                gr.update(),
                gr.update(),
                gr.update(),
                gr.update(),
                gr.update(),
                gr.update(),
            )

    # clearing images when we need to completely change what's in the
    # gallery avoids current images being shown replacing piecemeal and
    # prevents weirdness and errors if the user selects an image during the
    # replacement phase.
    clear_gallery = dict(
        fn=on_clear_gallery,
        inputs=None,
        outputs=[gallery, logo],
        queue=False,
    )

    subdirectories.select(**clear_gallery).then(
        on_select_subdir,
        [subdirectories],
        [gallery_files, gallery, logo],
        queue=False,
    )

    open_subdir.click(on_open_subdir, inputs=[subdirectories], queue=False)

    refresh.click(**clear_gallery).then(
        on_refresh,
        [subdirectories],
        [subdirectories, subdirectory_paths, gallery_files, gallery, logo],
        queue=False,
    )

    image_columns.change(
        fn=on_image_columns_change,
        inputs=[image_columns],
        outputs=[gallery],
        queue=False,
    )

    gallery.select(
        on_select_image,
        [gallery_files],
        [outputgallery_filename, image_parameters],
        queue=False,
    )

    outputgallery_filename.change(
        on_outputgallery_filename_change,
        [outputgallery_filename],
        [
            outputgallery_sendto_txt2img,
            outputgallery_sendto_img2img,
            outputgallery_sendto_inpaint,
            outputgallery_sendto_outpaint,
            outputgallery_sendto_upscaler,
        ],
        queue=False,
    )

    # We should have been given the .select function for our tab, so set it up
    def outputgallery_tab_select(select):
        select(
            fn=on_select_tab,
            inputs=[subdirectory_paths],
            outputs=[
                subdirectories,
                subdirectory_paths,
                gallery_files,
                gallery,
                logo,
                open_subdir,
            ],
            queue=False,
        )

    # We should have been passed a list of components on other tabs that update
    # when a new image has generated on that tab, so set things up so the user
    # will see that new image if they are looking at today's subdirectory
    def outputgallery_watch(components: gr.Textbox):
        for component in components:
            component.change(
                on_new_image,
                inputs=[subdirectories, subdirectory_paths, component],
                outputs=[gallery_files, gallery, logo],
                queue=False,
            )

```

`apps/stable_diffusion/web/ui/stablelm_ui.py`:

```py
import gradio as gr
import torch
import os
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
)
from apps.stable_diffusion.web.ui.utils import available_devices
from shark.iree_utils.compile_utils import clean_device_info
from datetime import datetime as dt
import json
import sys


def user(message, history):
    # Append the user's message to the conversation history
    return "", history + [[message, ""]]


sharkModel = 0
sharded_model = 0
vicuna_model = 0

past_key_values = None

model_map = {
    "llama2_7b": "meta-llama/Llama-2-7b-chat-hf",
    "llama2_13b": "meta-llama/Llama-2-13b-chat-hf",
    "llama2_70b": "meta-llama/Llama-2-70b-chat-hf",
    "vicuna": "TheBloke/vicuna-7B-1.1-HF",
}

# NOTE: Each `model_name` should have its own start message
start_message = {
    "llama2_7b": (
        "You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe. Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or "
        "illegal content. Please ensure that your responses are socially "
        "unbiased and positive in nature. If a question does not make any "
        "sense, or is not factually coherent, explain why instead of "
        "answering something not correct. If you don't know the answer "
        "to a question, please don't share false information."
    ),
    "llama2_13b": (
        "You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe. Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or "
        "illegal content. Please ensure that your responses are socially "
        "unbiased and positive in nature. If a question does not make any "
        "sense, or is not factually coherent, explain why instead of "
        "answering something not correct. If you don't know the answer "
        "to a question, please don't share false information."
    ),
    "llama2_70b": (
        "You are a helpful, respectful and honest assistant. Always answer "
        "as helpfully as possible, while being safe. Your answers should not "
        "include any harmful, unethical, racist, sexist, toxic, dangerous, or "
        "illegal content. Please ensure that your responses are socially "
        "unbiased and positive in nature. If a question does not make any "
        "sense, or is not factually coherent, explain why instead of "
        "answering something not correct. If you don't know the answer "
        "to a question, please don't share false information."
    ),
    "vicuna": (
        "A chat between a curious user and an artificial intelligence "
        "assistant. The assistant gives helpful, detailed, and "
        "polite answers to the user's questions.\n"
    ),
}


def create_prompt(model_name, history, prompt_prefix):
    system_message = ""
    if prompt_prefix:
        system_message = start_message[model_name]

    if "llama2" in model_name:
        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
        conversation = "".join(
            [f"{B_INST} {item[0]} {E_INST} {item[1]} " for item in history[1:]]
        )
        if prompt_prefix:
            msg = f"{B_INST} {B_SYS}{system_message}{E_SYS}{history[0][0]} {E_INST} {history[0][1]} {conversation}"
        else:
            msg = f"{B_INST} {history[0][0]} {E_INST} {history[0][1]} {conversation}"
    elif model_name in ["vicuna"]:
        conversation = "".join(
            [
                "".join(["<|USER|>" + item[0], "<|ASSISTANT|>" + item[1]])
                for item in history
            ]
        )
        msg = system_message + conversation
        msg = msg.strip()
    else:
        conversation = "".join(
            ["".join([item[0], item[1]]) for item in history]
        )
        msg = system_message + conversation
        msg = msg.strip()
    return msg


def set_vicuna_model(model):
    global vicuna_model
    vicuna_model = model


def get_default_config():
    import torch
    from transformers import AutoTokenizer

    hf_model_path = "TheBloke/vicuna-7B-1.1-HF"
    tokenizer = AutoTokenizer.from_pretrained(hf_model_path, use_fast=False)
    compilation_prompt = "".join(["0" for _ in range(17)])
    compilation_input_ids = tokenizer(
        compilation_prompt,
        return_tensors="pt",
    ).input_ids
    compilation_input_ids = torch.tensor(compilation_input_ids).reshape(
        [1, 19]
    )
    firstVicunaCompileInput = (compilation_input_ids,)
    from apps.language_models.src.model_wrappers.vicuna_model import (
        CombinedModel,
    )
    from shark.shark_generate_model_config import GenerateConfigFile

    model = CombinedModel()
    c = GenerateConfigFile(model, 1, ["gpu_id"], firstVicunaCompileInput)
    c.split_into_layers()


model_vmfb_key = ""


# TODO: Make chat reusable for UI and API
def chat(
    prompt_prefix,
    history,
    model,
    device,
    precision,
    download_vmfb,
    config_file,
    cli=False,
    progress=gr.Progress(),
):
    global past_key_values
    global model_vmfb_key
    global vicuna_model

    model_name, model_path = list(map(str.strip, model.split("=>")))
    device, device_id = clean_device_info(device)

    from apps.language_models.scripts.vicuna import ShardedVicuna
    from apps.language_models.scripts.vicuna import UnshardedVicuna
    from apps.stable_diffusion.src import args

    new_model_vmfb_key = f"{model_name}#{model_path}#{device}#{device_id}#{precision}#{download_vmfb}"
    if vicuna_model is None or new_model_vmfb_key != model_vmfb_key:
        model_vmfb_key = new_model_vmfb_key
        max_toks = 128 if model_name == "codegen" else 512

        # get iree flags that need to be overridden, from commandline args
        _extra_args = []
        # vulkan target triple
        vulkan_target_triple = args.iree_vulkan_target_triple
        from shark.iree_utils.vulkan_utils import (
            get_all_vulkan_devices,
            get_vulkan_target_triple,
        )

        if device == "vulkan":
            vulkaninfo_list = get_all_vulkan_devices()
            if vulkan_target_triple == "":
                # We already have the device_id extracted via WebUI, so we directly use
                # that to find the target triple.
                vulkan_target_triple = get_vulkan_target_triple(
                    vulkaninfo_list[device_id]
                )
            _extra_args.append(
                f"-iree-vulkan-target-triple={vulkan_target_triple}"
            )
            if "rdna" in vulkan_target_triple:
                flags_to_add = [
                    "--iree-spirv-index-bits=64",
                ]
                _extra_args = _extra_args + flags_to_add

            if device_id is None:
                id = 0
                for device in vulkaninfo_list:
                    target_triple = get_vulkan_target_triple(
                        vulkaninfo_list[id]
                    )
                    if target_triple == vulkan_target_triple:
                        device_id = id
                        break
                    id += 1

                assert (
                    device_id
                ), f"no vulkan hardware for target-triple '{vulkan_target_triple}' exists"
            print(f"Will use vulkan target triple : {vulkan_target_triple}")

        elif "rocm" in device:
            # add iree rocm flags
            if args.iree_rocm_target_chip != "":
                _extra_args.append(
                    f"--iree-rocm-target-chip={args.iree_rocm_target_chip}"
                )
                print(f"extra args = {_extra_args}")

        if model_name == "vicuna4":
            vicuna_model = ShardedVicuna(
                model_name,
                hf_model_path=model_path,
                device=device,
                precision=precision,
                max_num_tokens=max_toks,
                compressed=True,
                extra_args_cmd=_extra_args,
            )
        else:
            #  if config_file is None:
            vicuna_model = UnshardedVicuna(
                model_name,
                hf_model_path=model_path,
                hf_auth_token=args.hf_auth_token,
                device=device,
                vulkan_target_triple=vulkan_target_triple,
                precision=precision,
                max_num_tokens=max_toks,
                download_vmfb=download_vmfb,
                load_mlir_from_shark_tank=True,
                extra_args_cmd=_extra_args,
                device_id=device_id,
            )

    if vicuna_model is None:
        sys.exit("Unable to instantiate the model object, exiting.")

    prompt = create_prompt(model_name, history, prompt_prefix)

    partial_text = ""
    token_count = 0
    total_time_ms = 0.001  # In order to avoid divide by zero error
    prefill_time = 0
    is_first = True
    for text, msg, exec_time in progress.tqdm(
        vicuna_model.generate(prompt, cli=cli),
        desc="generating response",
    ):
        if msg is None:
            if is_first:
                prefill_time = exec_time
                is_first = False
            else:
                total_time_ms += exec_time
                token_count += 1
            partial_text += text + " "
            history[-1][1] = partial_text
            yield history, f"Prefill: {prefill_time:.2f}"
        elif "formatted" in msg:
            history[-1][1] = text
            tokens_per_sec = (token_count / total_time_ms) * 1000
            yield history, f"Prefill: {prefill_time:.2f} seconds\n Decode: {tokens_per_sec:.2f} tokens/sec"
        else:
            sys.exit(
                "unexpected message from the vicuna generate call, exiting."
            )

    return history, ""


def llm_chat_api(InputData: dict):
    print(f"Input keys : {InputData.keys()}")
    # print(f"model : {InputData['model']}")
    is_chat_completion_api = (
        "messages" in InputData.keys()
    )  # else it is the legacy `completion` api
    # For Debugging input data from API
    # if is_chat_completion_api:
    #     print(f"message -> role : {InputData['messages'][0]['role']}")
    #     print(f"message -> content : {InputData['messages'][0]['content']}")
    # else:
    #     print(f"prompt : {InputData['prompt']}")
    # print(f"max_tokens : {InputData['max_tokens']}") # Default to 128 for now
    global vicuna_model
    model_name = (
        InputData["model"] if "model" in InputData.keys() else "codegen"
    )
    model_path = model_map[model_name]
    device = "cpu-task"
    precision = "fp16"
    max_toks = (
        None
        if "max_tokens" not in InputData.keys()
        else InputData["max_tokens"]
    )
    if max_toks is None:
        max_toks = 128 if model_name == "codegen" else 512

    # make it working for codegen first
    from apps.language_models.scripts.vicuna import (
        UnshardedVicuna,
    )

    device_id = None
    if vicuna_model == 0:
        device, device_id = clean_device_info(device)

        vicuna_model = UnshardedVicuna(
            model_name,
            hf_model_path=model_path,
            device=device,
            precision=precision,
            max_num_tokens=max_toks,
            download_vmfb=True,
            load_mlir_from_shark_tank=True,
            device_id=device_id,
        )

    # TODO: add role dict for different models
    if is_chat_completion_api:
        # TODO: add funtionality for multiple messages
        prompt = create_prompt(
            model_name, [(InputData["messages"][0]["content"], "")]
        )
    else:
        prompt = InputData["prompt"]
    print("prompt = ", prompt)

    res = vicuna_model.generate(prompt)
    res_op = None
    for op in res:
        res_op = op

    if is_chat_completion_api:
        choices = [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": res_op,  # since we are yeilding the result
                },
                "finish_reason": "stop",  # or length
            }
        ]
    else:
        choices = [
            {
                "text": res_op,
                "index": 0,
                "logprobs": None,
                "finish_reason": "stop",  # or length
            }
        ]
    end_time = dt.now().strftime("%Y%m%d%H%M%S%f")
    return {
        "id": end_time,
        "object": "chat.completion"
        if is_chat_completion_api
        else "text_completion",
        "created": int(end_time),
        "choices": choices,
    }


def view_json_file(file_obj):
    content = ""
    with open(file_obj.name, "r") as fopen:
        content = fopen.read()
    return content


with gr.Blocks(title="Chatbot") as stablelm_chat:
    with gr.Row():
        model_choices = list(
            map(lambda x: f"{x[0]: <10} => {x[1]}", model_map.items())
        )
        model = gr.Dropdown(
            label="Select Model",
            value=model_choices[0],
            choices=model_choices,
            allow_custom_value=True,
        )
        supported_devices = available_devices
        enabled = len(supported_devices) > 0
        # show cpu-task device first in list for chatbot
        supported_devices = supported_devices[-1:] + supported_devices[:-1]
        supported_devices = [x for x in supported_devices if "sync" not in x]
        device = gr.Dropdown(
            label="Device",
            value=supported_devices[0]
            if enabled
            else "Only CUDA Supported for now",
            choices=supported_devices,
            interactive=enabled,
            allow_custom_value=True,
            # multiselect=True,
        )
        precision = gr.Radio(
            label="Precision",
            value="int4",
            choices=[
                "int4",
                "int8",
                "fp16",
            ],
            visible=False,
        )
        tokens_time = gr.Textbox(label="Tokens generated per second")
        with gr.Column():
            download_vmfb = gr.Checkbox(
                label="Download vmfb from Shark tank if available",
                value=True,
                interactive=True,
            )
            prompt_prefix = gr.Checkbox(
                label="Add System Prompt",
                value=False,
                interactive=True,
            )

    with gr.Row(visible=False):
        with gr.Group():
            config_file = gr.File(
                label="Upload sharding configuration", visible=False
            )
            json_view_button = gr.Button(label="View as JSON", visible=False)
        json_view = gr.JSON(interactive=True, visible=False)
        json_view_button.click(
            fn=view_json_file, inputs=[config_file], outputs=[json_view]
        )
    chatbot = gr.Chatbot(elem_id="chatbot")
    with gr.Row():
        with gr.Column():
            msg = gr.Textbox(
                label="Chat Message Box",
                placeholder="Chat Message Box",
                show_label=False,
                interactive=enabled,
                container=False,
            )
        with gr.Column():
            with gr.Row():
                submit = gr.Button("Submit", interactive=enabled)
                stop = gr.Button("Stop", interactive=enabled)
                clear = gr.Button("Clear", interactive=enabled)

    submit_event = msg.submit(
        fn=user,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot],
        show_progress=False,
        queue=False,
    ).then(
        fn=chat,
        inputs=[
            prompt_prefix,
            chatbot,
            model,
            device,
            precision,
            download_vmfb,
            config_file,
        ],
        outputs=[chatbot, tokens_time],
        show_progress=False,
        queue=True,
    )
    submit_click_event = submit.click(
        fn=user,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot],
        show_progress=False,
        queue=False,
    ).then(
        fn=chat,
        inputs=[
            prompt_prefix,
            chatbot,
            model,
            device,
            precision,
            download_vmfb,
            config_file,
        ],
        outputs=[chatbot, tokens_time],
        show_progress=False,
        queue=True,
    )
    stop.click(
        fn=None,
        inputs=None,
        outputs=None,
        cancels=[submit_event, submit_click_event],
        queue=False,
    )
    clear.click(lambda: None, None, [chatbot], queue=False)

```

`apps/stable_diffusion/web/ui/txt2img_sdxl_ui.py`:

```py
import os
import torch
import time
import sys
import gradio as gr
from PIL import Image
from math import ceil
from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    nodlogo_loc,
    get_custom_model_path,
    get_custom_model_files,
    scheduler_list,
    predefined_models,
    cancel_sd,
)
from apps.stable_diffusion.web.utils.metadata import import_png_metadata
from apps.stable_diffusion.web.utils.common_label_calc import status_label
from apps.stable_diffusion.src import (
    args,
    Text2ImageSDXLPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    save_output_img,
    prompt_examples,
    Image2ImagePipeline,
)
from apps.stable_diffusion.src.utils import (
    get_generated_imgs_path,
    get_generation_text_info,
)

# set initial values of iree_vulkan_target_triple, use_tuned and import_mlir.
init_iree_vulkan_target_triple = args.iree_vulkan_target_triple
init_iree_metal_target_platform = args.iree_metal_target_platform
init_use_tuned = args.use_tuned
init_import_mlir = args.import_mlir


def txt2img_sdxl_inf(
    prompt: str,
    negative_prompt: str,
    height: int,
    width: int,
    steps: int,
    guidance_scale: float,
    seed: str | int,
    batch_count: int,
    batch_size: int,
    scheduler: str,
    model_id: str,
    precision: str,
    device: str,
    max_length: int,
    save_metadata_to_json: bool,
    save_metadata_to_png: bool,
    ondemand: bool,
    repeatable_seeds: bool,
):
    if precision != "fp16":
        print("currently we support fp16 for SDXL")
        precision = "fp16"
    from apps.stable_diffusion.web.ui.utils import (
        get_custom_model_pathfile,
        get_custom_vae_or_lora_weights,
        Config,
    )
    import apps.stable_diffusion.web.utils.global_obj as global_obj
    from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
        SD_STATE_CANCEL,
    )

    args.prompts = [prompt]
    args.negative_prompts = [negative_prompt]
    args.guidance_scale = guidance_scale
    args.steps = steps
    args.scheduler = scheduler
    args.ondemand = ondemand

    # set ckpt_loc and hf_model_id.
    args.ckpt_loc = ""
    args.hf_model_id = ""
    args.custom_vae = ""

    # .safetensor or .chkpt on the custom model path
    if model_id in get_custom_model_files():
        args.ckpt_loc = get_custom_model_pathfile(model_id)
    # civitai download
    elif "civitai" in model_id:
        args.ckpt_loc = model_id
    # either predefined or huggingface
    else:
        args.hf_model_id = model_id

    # if custom_vae != "None":
    #     args.custom_vae = get_custom_model_pathfile(custom_vae, model="vae")

    args.save_metadata_to_json = save_metadata_to_json
    args.write_metadata_to_png = save_metadata_to_png

    args.use_lora = ""

    dtype = torch.float32 if precision == "fp32" else torch.half
    cpu_scheduling = not scheduler.startswith("Shark")
    new_config_obj = Config(
        "txt2img_sdxl",
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        precision,
        batch_size,
        max_length,
        height,
        width,
        device,
        use_lora=args.use_lora,
        use_stencil=None,
        ondemand=ondemand,
    )
    if (
        not global_obj.get_sd_obj()
        or global_obj.get_cfg_obj() != new_config_obj
    ):
        global_obj.clear_cache()
        global_obj.set_cfg_obj(new_config_obj)
        args.precision = precision
        args.batch_count = batch_count
        args.batch_size = batch_size
        args.max_length = max_length
        args.height = height
        args.width = width
        args.device = device.split("=>", 1)[1].strip()
        args.iree_vulkan_target_triple = init_iree_vulkan_target_triple
        args.iree_metal_target_platform = init_iree_metal_target_platform
        args.use_tuned = init_use_tuned
        args.import_mlir = init_import_mlir
        args.img_path = None
        set_init_device_flags()
        model_id = (
            args.hf_model_id
            if args.hf_model_id
            else "stabilityai/stable-diffusion-xl-base-1.0"
        )
        global_obj.set_schedulers(get_schedulers(model_id))
        scheduler_obj = global_obj.get_scheduler(scheduler)
        # For SDXL we set max_length as 77.
        print("Setting max_length = 77")
        max_length = 77
        if global_obj.get_cfg_obj().ondemand:
            print("Running txt2img in memory efficient mode.")
        txt2img_sdxl_obj = Text2ImageSDXLPipeline.from_pretrained(
            scheduler=scheduler_obj,
            import_mlir=args.import_mlir,
            model_id=args.hf_model_id,
            ckpt_loc=args.ckpt_loc,
            precision=precision,
            max_length=max_length,
            batch_size=batch_size,
            height=height,
            width=width,
            use_base_vae=args.use_base_vae,
            use_tuned=args.use_tuned,
            custom_vae=args.custom_vae,
            low_cpu_mem_usage=args.low_cpu_mem_usage,
            debug=args.import_debug if args.import_mlir else False,
            use_lora=args.use_lora,
            use_quantize=args.use_quantize,
            ondemand=global_obj.get_cfg_obj().ondemand,
        )
        global_obj.set_sd_obj(txt2img_sdxl_obj)

    global_obj.set_sd_scheduler(scheduler)

    start_time = time.time()
    global_obj.get_sd_obj().log = ""
    generated_imgs = []
    text_output = ""
    try:
        seeds = utils.batch_seeds(seed, batch_count, repeatable_seeds)
    except TypeError as error:
        raise gr.Error(str(error)) from None

    for current_batch in range(batch_count):
        out_imgs = global_obj.get_sd_obj().generate_images(
            prompt,
            negative_prompt,
            batch_size,
            height,
            width,
            steps,
            guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )

        total_time = time.time() - start_time
        text_output = get_generation_text_info(
            seeds[: current_batch + 1], device
        )
        text_output += "\n" + global_obj.get_sd_obj().log
        text_output += f"\nTotal image(s) generation time: {total_time:.4f}sec"

        if global_obj.get_sd_status() == SD_STATE_CANCEL:
            break
        else:
            save_output_img(out_imgs[0], seeds[current_batch])
            generated_imgs.extend(out_imgs)
            yield generated_imgs, text_output, status_label(
                "Text-to-Image-SDXL",
                current_batch + 1,
                batch_count,
                batch_size,
            )

    return generated_imgs, text_output, ""


with gr.Blocks(title="Text-to-Image-SDXL") as txt2img_sdxl_web:
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Row():
            with gr.Column(scale=1, elem_id="demo_title_outer"):
                gr.Image(
                    value=nod_logo,
                    show_label=False,
                    interactive=False,
                    elem_id="top_logo",
                    width=150,
                    height=50,
                )
    with gr.Row(elem_id="ui_body"):
        with gr.Row():
            with gr.Column(scale=1, min_width=600):
                with gr.Row():
                    with gr.Column(scale=10):
                        with gr.Row():
                            t2i_model_info = f"Custom Model Path: {str(get_custom_model_path())}"
                            txt2img_sdxl_custom_model = gr.Dropdown(
                                label=f"Models",
                                info="Select, or enter HuggingFace Model ID or Civitai model download URL",
                                elem_id="custom_model",
                                value=os.path.basename(args.ckpt_loc)
                                if args.ckpt_loc
                                else "stabilityai/stable-diffusion-xl-base-1.0",
                                choices=[
                                    "stabilityai/stable-diffusion-xl-base-1.0"
                                ],
                                allow_custom_value=True,
                                scale=2,
                            )

                with gr.Group(elem_id="prompt_box_outer"):
                    prompt = gr.Textbox(
                        label="Prompt",
                        value=args.prompts[0],
                        lines=2,
                        elem_id="prompt_box",
                    )
                    negative_prompt = gr.Textbox(
                        label="Negative Prompt",
                        value=args.negative_prompts[0],
                        lines=2,
                        elem_id="negative_prompt_box",
                    )

                with gr.Accordion(label="Advanced Options", open=False):
                    with gr.Row():
                        scheduler = gr.Dropdown(
                            elem_id="scheduler",
                            label="Scheduler",
                            value="DDIM",
                            choices=["DDIM"],
                            allow_custom_value=True,
                            visible=False,
                        )
                        with gr.Column():
                            save_metadata_to_png = gr.Checkbox(
                                label="Save prompt information to PNG",
                                value=args.write_metadata_to_png,
                                interactive=True,
                            )
                            save_metadata_to_json = gr.Checkbox(
                                label="Save prompt information to JSON file",
                                value=args.save_metadata_to_json,
                                interactive=True,
                            )
                    with gr.Row():
                        height = gr.Slider(
                            1024,
                            value=1024,
                            step=8,
                            label="Height",
                            visible=False,
                        )
                        width = gr.Slider(
                            1024,
                            value=1024,
                            step=8,
                            label="Width",
                            visible=False,
                        )
                        precision = gr.Radio(
                            label="Precision",
                            value="fp16",
                            choices=[
                                "fp16",
                                "fp32",
                            ],
                            visible=False,
                        )
                        max_length = gr.Radio(
                            label="Max Length",
                            value=args.max_length,
                            choices=[
                                64,
                                77,
                            ],
                            visible=False,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            steps = gr.Slider(
                                1, 100, value=args.steps, step=1, label="Steps"
                            )
                        with gr.Column(scale=3):
                            guidance_scale = gr.Slider(
                                0,
                                50,
                                value=args.guidance_scale,
                                step=0.1,
                                label="CFG Scale",
                            )
                        ondemand = gr.Checkbox(
                            value=args.ondemand,
                            label="Low VRAM",
                            interactive=True,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            batch_count = gr.Slider(
                                1,
                                100,
                                value=args.batch_count,
                                step=1,
                                label="Batch Count",
                                interactive=True,
                            )
                        with gr.Column(scale=3):
                            batch_size = gr.Slider(
                                1,
                                4,
                                value=args.batch_size,
                                step=1,
                                label="Batch Size",
                                interactive=True,
                            )
                        repeatable_seeds = gr.Checkbox(
                            args.repeatable_seeds,
                            label="Repeatable Seeds",
                        )
                with gr.Row():
                    seed = gr.Textbox(
                        value=args.seed,
                        label="Seed",
                        info="An integer or a JSON list of integers, -1 for random",
                    )
                    device = gr.Dropdown(
                        elem_id="device",
                        label="Device",
                        value=available_devices[0],
                        choices=available_devices,
                        allow_custom_value=True,
                    )
                with gr.Accordion(label="Prompt Examples!", open=False):
                    ex = gr.Examples(
                        examples=prompt_examples,
                        inputs=prompt,
                        cache_examples=False,
                        elem_id="prompt_examples",
                    )

            with gr.Column(scale=1, min_width=600):
                with gr.Group():
                    txt2img_sdxl_gallery = gr.Gallery(
                        label="Generated images",
                        show_label=False,
                        elem_id="gallery",
                        columns=[2],
                        object_fit="contain",
                    )
                    std_output = gr.Textbox(
                        value=f"{t2i_model_info}\n"
                        f"Images will be saved at "
                        f"{get_generated_imgs_path()}",
                        lines=1,
                        elem_id="std_output",
                        show_label=False,
                    )
                    txt2img_sdxl_status = gr.Textbox(visible=False)
                with gr.Row():
                    stable_diffusion = gr.Button("Generate Image(s)")
                    random_seed = gr.Button("Randomize Seed")
                    random_seed.click(
                        lambda: -1,
                        inputs=[],
                        outputs=[seed],
                        queue=False,
                    )
                    stop_batch = gr.Button("Stop Batch")
                with gr.Row():
                    blank_thing_for_row = None

        kwargs = dict(
            fn=txt2img_sdxl_inf,
            inputs=[
                prompt,
                negative_prompt,
                height,
                width,
                steps,
                guidance_scale,
                seed,
                batch_count,
                batch_size,
                scheduler,
                txt2img_sdxl_custom_model,
                precision,
                device,
                max_length,
                save_metadata_to_json,
                save_metadata_to_png,
                ondemand,
                repeatable_seeds,
            ],
            outputs=[txt2img_sdxl_gallery, std_output, txt2img_sdxl_status],
            show_progress="minimal" if args.progress_bar else "none",
        )

        status_kwargs = dict(
            fn=lambda bc, bs: status_label("Text-to-Image-SDXL", 0, bc, bs),
            inputs=[batch_count, batch_size],
            outputs=txt2img_sdxl_status,
        )

        prompt_submit = prompt.submit(**status_kwargs).then(**kwargs)
        neg_prompt_submit = negative_prompt.submit(**status_kwargs).then(
            **kwargs
        )
        generate_click = stable_diffusion.click(**status_kwargs).then(**kwargs)
        stop_batch.click(
            fn=cancel_sd,
            cancels=[prompt_submit, neg_prompt_submit, generate_click],
        )

```

`apps/stable_diffusion/web/ui/txt2img_ui.py`:

```py
import os
import torch
import time
import sys
import gradio as gr
from PIL import Image
from math import ceil

from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    nodlogo_loc,
    get_custom_model_path,
    get_custom_model_files,
    scheduler_list,
    scheduler_list_cpu_only,
    predefined_models,
    cancel_sd,
)
from apps.stable_diffusion.web.ui.common_ui_events import lora_changed
from apps.stable_diffusion.web.utils.metadata import import_png_metadata
from apps.stable_diffusion.web.utils.common_label_calc import status_label
from apps.stable_diffusion.src import (
    args,
    Text2ImagePipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    save_output_img,
    prompt_examples,
    Image2ImagePipeline,
)
from apps.stable_diffusion.src.utils import (
    get_generated_imgs_path,
    get_generation_text_info,
    resampler_list,
)

# set initial values of iree_vulkan_target_triple, use_tuned and import_mlir.
init_iree_vulkan_target_triple = args.iree_vulkan_target_triple
init_iree_metal_target_platform = args.iree_metal_target_platform
init_use_tuned = args.use_tuned
init_import_mlir = args.import_mlir


def txt2img_inf(
    prompt: str,
    negative_prompt: str,
    height: int,
    width: int,
    steps: int,
    guidance_scale: float,
    seed: str | int,
    batch_count: int,
    batch_size: int,
    scheduler: str,
    model_id: str,
    custom_vae: str,
    precision: str,
    device: str,
    max_length: int,
    save_metadata_to_json: bool,
    save_metadata_to_png: bool,
    lora_weights: str,
    lora_hf_id: str,
    ondemand: bool,
    repeatable_seeds: bool,
    use_hiresfix: bool,
    hiresfix_height: int,
    hiresfix_width: int,
    hiresfix_strength: float,
    resample_type: str,
):
    from apps.stable_diffusion.web.ui.utils import (
        get_custom_model_pathfile,
        get_custom_vae_or_lora_weights,
        Config,
    )
    import apps.stable_diffusion.web.utils.global_obj as global_obj
    from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
        SD_STATE_CANCEL,
    )

    args.prompts = [prompt]
    args.negative_prompts = [negative_prompt]
    args.guidance_scale = guidance_scale
    args.steps = steps
    args.scheduler = scheduler
    args.ondemand = ondemand

    # set ckpt_loc and hf_model_id.
    args.ckpt_loc = ""
    args.hf_model_id = ""
    args.custom_vae = ""

    # .safetensor or .chkpt on the custom model path
    if model_id in get_custom_model_files():
        args.ckpt_loc = get_custom_model_pathfile(model_id)
    # civitai download
    elif "civitai" in model_id:
        args.ckpt_loc = model_id
    # either predefined or huggingface
    else:
        args.hf_model_id = model_id

    if custom_vae != "None":
        args.custom_vae = get_custom_model_pathfile(custom_vae, model="vae")

    args.save_metadata_to_json = save_metadata_to_json
    args.write_metadata_to_png = save_metadata_to_png

    args.use_lora = get_custom_vae_or_lora_weights(
        lora_weights, lora_hf_id, "lora"
    )

    dtype = torch.float32 if precision == "fp32" else torch.half
    cpu_scheduling = not scheduler.startswith("Shark")
    new_config_obj = Config(
        "txt2img",
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        precision,
        batch_size,
        max_length,
        height,
        width,
        device,
        use_lora=args.use_lora,
        stencils=[],
        ondemand=ondemand,
    )
    if (
        not global_obj.get_sd_obj()
        or global_obj.get_cfg_obj() != new_config_obj
    ):
        global_obj.clear_cache()
        global_obj.set_cfg_obj(new_config_obj)
        args.precision = precision
        args.batch_count = batch_count
        args.batch_size = batch_size
        args.max_length = max_length
        args.height = height
        args.width = width
        args.use_hiresfix = use_hiresfix
        args.hiresfix_height = hiresfix_height
        args.hiresfix_width = hiresfix_width
        args.hiresfix_strength = hiresfix_strength
        args.resample_type = resample_type
        args.device = device.split("=>", 1)[1].strip()
        args.iree_vulkan_target_triple = init_iree_vulkan_target_triple
        args.iree_metal_target_platform = init_iree_metal_target_platform
        args.use_tuned = init_use_tuned
        args.import_mlir = init_import_mlir
        args.img_path = None
        set_init_device_flags()
        model_id = (
            args.hf_model_id
            if args.hf_model_id
            else "stabilityai/stable-diffusion-2-1-base"
        )
        global_obj.set_schedulers(get_schedulers(model_id))
        scheduler_obj = global_obj.get_scheduler(scheduler)
        global_obj.set_sd_obj(
            Text2ImagePipeline.from_pretrained(
                scheduler=scheduler_obj,
                import_mlir=args.import_mlir,
                model_id=args.hf_model_id,
                ckpt_loc=args.ckpt_loc,
                precision=args.precision,
                max_length=args.max_length,
                batch_size=args.batch_size,
                height=args.height,
                width=args.width,
                use_base_vae=args.use_base_vae,
                use_tuned=args.use_tuned,
                custom_vae=args.custom_vae,
                low_cpu_mem_usage=args.low_cpu_mem_usage,
                debug=args.import_debug if args.import_mlir else False,
                use_lora=args.use_lora,
                ondemand=args.ondemand,
            )
        )

    global_obj.set_sd_scheduler(scheduler)

    start_time = time.time()
    global_obj.get_sd_obj().log = ""
    generated_imgs = []
    text_output = ""
    try:
        seeds = utils.batch_seeds(seed, batch_count, repeatable_seeds)
    except TypeError as error:
        raise gr.Error(str(error)) from None

    for current_batch in range(batch_count):
        out_imgs = global_obj.get_sd_obj().generate_images(
            prompt,
            negative_prompt,
            batch_size,
            height,
            width,
            steps,
            guidance_scale,
            seeds[current_batch],
            args.max_length,
            dtype,
            args.use_base_vae,
            cpu_scheduling,
            args.max_embeddings_multiples,
        )
        # TODO: allow user to save original image
        # TODO: add option to let user keep both pipelines loaded, and unload
        #  either at will
        # TODO: add custom step value slider
        # TODO: add option to use secondary model for the img2img pass
        if use_hiresfix is True:
            new_config_obj = Config(
                "img2img",
                args.hf_model_id,
                args.ckpt_loc,
                args.custom_vae,
                precision,
                1,
                max_length,
                height,
                width,
                device,
                use_lora=args.use_lora,
                stencils=[],
                ondemand=ondemand,
            )

            global_obj.clear_cache()
            global_obj.set_cfg_obj(new_config_obj)
            set_init_device_flags()
            model_id = (
                args.hf_model_id
                if args.hf_model_id
                else "stabilityai/stable-diffusion-2-1-base"
            )
            global_obj.set_schedulers(get_schedulers(model_id))
            scheduler_obj = global_obj.get_scheduler(args.scheduler)

            global_obj.set_sd_obj(
                Image2ImagePipeline.from_pretrained(
                    scheduler_obj,
                    args.import_mlir,
                    args.hf_model_id,
                    args.ckpt_loc,
                    args.custom_vae,
                    args.precision,
                    args.max_length,
                    1,
                    hiresfix_height,
                    hiresfix_width,
                    args.use_base_vae,
                    args.use_tuned,
                    low_cpu_mem_usage=args.low_cpu_mem_usage,
                    debug=args.import_debug if args.import_mlir else False,
                    use_lora=args.use_lora,
                    ondemand=args.ondemand,
                )
            )

            global_obj.set_sd_scheduler(args.scheduler)

            out_imgs = global_obj.get_sd_obj().generate_images(
                prompt,
                negative_prompt,
                out_imgs[0],
                batch_size,
                hiresfix_height,
                hiresfix_width,
                ceil(steps / hiresfix_strength),
                hiresfix_strength,
                guidance_scale,
                seeds[current_batch],
                args.max_length,
                dtype,
                args.use_base_vae,
                cpu_scheduling,
                args.max_embeddings_multiples,
                stencils=[],
                resample_type=resample_type,
            )
        total_time = time.time() - start_time
        text_output = get_generation_text_info(
            seeds[: current_batch + 1], device
        )
        text_output += "\n" + global_obj.get_sd_obj().log
        text_output += f"\nTotal image(s) generation time: {total_time:.4f}sec"

        if global_obj.get_sd_status() == SD_STATE_CANCEL:
            break
        else:
            save_output_img(out_imgs[0], seeds[current_batch])
            generated_imgs.extend(out_imgs)
            yield generated_imgs, text_output, status_label(
                "Text-to-Image", current_batch + 1, batch_count, batch_size
            )

    return generated_imgs, text_output, ""


with gr.Blocks(title="Text-to-Image") as txt2img_web:
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Row():
            with gr.Column(scale=1, elem_id="demo_title_outer"):
                gr.Image(
                    value=nod_logo,
                    show_label=False,
                    interactive=False,
                    elem_id="top_logo",
                    width=150,
                    height=50,
                )
    with gr.Row(elem_id="ui_body"):
        with gr.Row():
            with gr.Column(scale=1, min_width=600):
                with gr.Row():
                    with gr.Column(scale=10):
                        with gr.Row():
                            t2i_model_info = f"Custom Model Path: {str(get_custom_model_path())}"
                            txt2img_custom_model = gr.Dropdown(
                                label=f"Models",
                                info="Select, or enter HuggingFace Model ID or Civitai model download URL",
                                elem_id="custom_model",
                                value=os.path.basename(args.ckpt_loc)
                                if args.ckpt_loc
                                else "stabilityai/stable-diffusion-2-1-base",
                                choices=get_custom_model_files()
                                + predefined_models,
                                allow_custom_value=True,
                                scale=2,
                            )
                            # janky fix for overflowing text
                            t2i_vae_info = (
                                str(get_custom_model_path("vae"))
                            ).replace("\\", "\n\\")
                            t2i_vae_info = f"VAE Path: {t2i_vae_info}"
                            custom_vae = gr.Dropdown(
                                label=f"VAE Models",
                                info=t2i_vae_info,
                                elem_id="custom_model",
                                value=os.path.basename(args.custom_vae)
                                if args.custom_vae
                                else "None",
                                choices=["None"]
                                + get_custom_model_files("vae"),
                                allow_custom_value=True,
                                scale=1,
                            )
                    with gr.Column(scale=1, min_width=170):
                        txt2img_png_info_img = gr.Image(
                            label="Import PNG info",
                            elem_id="txt2img_prompt_image",
                            type="pil",
                            tool="None",
                            visible=True,
                        )

                with gr.Group(elem_id="prompt_box_outer"):
                    prompt = gr.Textbox(
                        label="Prompt",
                        value=args.prompts[0],
                        lines=2,
                        elem_id="prompt_box",
                    )
                    negative_prompt = gr.Textbox(
                        label="Negative Prompt",
                        value=args.negative_prompts[0],
                        lines=2,
                        elem_id="negative_prompt_box",
                    )
                with gr.Accordion(label="LoRA Options", open=False):
                    with gr.Row():
                        # janky fix for overflowing text
                        t2i_lora_info = (
                            str(get_custom_model_path("lora"))
                        ).replace("\\", "\n\\")
                        t2i_lora_info = f"LoRA Path: {t2i_lora_info}"
                        lora_weights = gr.Dropdown(
                            label=f"Standalone LoRA Weights",
                            info=t2i_lora_info,
                            elem_id="lora_weights",
                            value="None",
                            choices=["None"] + get_custom_model_files("lora"),
                            allow_custom_value=True,
                        )
                        lora_hf_id = gr.Textbox(
                            elem_id="lora_hf_id",
                            placeholder="Select 'None' in the Standalone LoRA "
                            "weights dropdown on the left if you want to use "
                            "a standalone HuggingFace model ID for LoRA here "
                            "e.g: sayakpaul/sd-model-finetuned-lora-t4",
                            value="",
                            label="HuggingFace Model ID",
                            lines=3,
                        )
                    with gr.Row():
                        lora_tags = gr.HTML(
                            value="<div><i>No LoRA selected</i></div>",
                            elem_classes="lora-tags",
                        )
                with gr.Accordion(label="Advanced Options", open=False):
                    with gr.Row():
                        scheduler = gr.Dropdown(
                            elem_id="scheduler",
                            label="Scheduler",
                            value=args.scheduler,
                            choices=scheduler_list,
                            allow_custom_value=True,
                        )
                        with gr.Column():
                            save_metadata_to_png = gr.Checkbox(
                                label="Save prompt information to PNG",
                                value=args.write_metadata_to_png,
                                interactive=True,
                            )
                            save_metadata_to_json = gr.Checkbox(
                                label="Save prompt information to JSON file",
                                value=args.save_metadata_to_json,
                                interactive=True,
                            )
                    with gr.Row():
                        height = gr.Slider(
                            384,
                            768,
                            value=args.height,
                            step=8,
                            label="Height",
                        )
                        width = gr.Slider(
                            384,
                            768,
                            value=args.width,
                            step=8,
                            label="Width",
                        )
                        precision = gr.Radio(
                            label="Precision",
                            value=args.precision,
                            choices=[
                                "fp16",
                                "fp32",
                            ],
                            visible=False,
                        )
                        max_length = gr.Radio(
                            label="Max Length",
                            value=args.max_length,
                            choices=[
                                64,
                                77,
                            ],
                            visible=False,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            steps = gr.Slider(
                                1, 100, value=args.steps, step=1, label="Steps"
                            )
                        with gr.Column(scale=3):
                            guidance_scale = gr.Slider(
                                0,
                                50,
                                value=args.guidance_scale,
                                step=0.1,
                                label="CFG Scale",
                            )
                        ondemand = gr.Checkbox(
                            value=args.ondemand,
                            label="Low VRAM",
                            interactive=True,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            batch_count = gr.Slider(
                                1,
                                100,
                                value=args.batch_count,
                                step=1,
                                label="Batch Count",
                                interactive=True,
                            )
                        with gr.Column(scale=3):
                            batch_size = gr.Slider(
                                1,
                                4,
                                value=args.batch_size,
                                step=1,
                                label="Batch Size",
                                interactive=True,
                            )
                        repeatable_seeds = gr.Checkbox(
                            args.repeatable_seeds,
                            label="Repeatable Seeds",
                        )
                with gr.Accordion(label="Hires Fix Options", open=False):
                    with gr.Group():
                        with gr.Row():
                            use_hiresfix = gr.Checkbox(
                                value=args.use_hiresfix,
                                label="Use Hires Fix",
                                interactive=True,
                            )
                            resample_type = gr.Dropdown(
                                value=args.resample_type,
                                choices=resampler_list,
                                label="Resample Type",
                                allow_custom_value=False,
                            )
                        hiresfix_height = gr.Slider(
                            384,
                            768,
                            value=args.hiresfix_height,
                            step=8,
                            label="Hires Fix Height",
                        )
                        hiresfix_width = gr.Slider(
                            384,
                            768,
                            value=args.hiresfix_width,
                            step=8,
                            label="Hires Fix Width",
                        )
                        hiresfix_strength = gr.Slider(
                            0,
                            1,
                            value=args.hiresfix_strength,
                            step=0.01,
                            label="Hires Fix Denoising Strength",
                        )
                with gr.Row():
                    seed = gr.Textbox(
                        value=args.seed,
                        label="Seed",
                        info="An integer or a JSON list of integers, -1 for random",
                    )
                    device = gr.Dropdown(
                        elem_id="device",
                        label="Device",
                        value=available_devices[0],
                        choices=available_devices,
                        allow_custom_value=True,
                    )
                with gr.Accordion(label="Prompt Examples!", open=False):
                    ex = gr.Examples(
                        examples=prompt_examples,
                        inputs=prompt,
                        cache_examples=False,
                        elem_id="prompt_examples",
                    )

            with gr.Column(scale=1, min_width=600):
                with gr.Group():
                    txt2img_gallery = gr.Gallery(
                        label="Generated images",
                        show_label=False,
                        elem_id="gallery",
                        columns=[2],
                        object_fit="contain",
                    )
                    std_output = gr.Textbox(
                        value=f"{t2i_model_info}\n"
                        f"Images will be saved at "
                        f"{get_generated_imgs_path()}",
                        lines=1,
                        elem_id="std_output",
                        show_label=False,
                    )
                    txt2img_status = gr.Textbox(visible=False)
                with gr.Row():
                    stable_diffusion = gr.Button("Generate Image(s)")
                    random_seed = gr.Button("Randomize Seed")
                    random_seed.click(
                        lambda: -1,
                        inputs=[],
                        outputs=[seed],
                        queue=False,
                    )
                    stop_batch = gr.Button("Stop Batch")
                with gr.Row():
                    blank_thing_for_row = None
                with gr.Row():
                    txt2img_sendto_img2img = gr.Button(value="SendTo Img2Img")
                    txt2img_sendto_inpaint = gr.Button(value="SendTo Inpaint")
                    txt2img_sendto_outpaint = gr.Button(
                        value="SendTo Outpaint"
                    )
                    txt2img_sendto_upscaler = gr.Button(
                        value="SendTo Upscaler"
                    )

        kwargs = dict(
            fn=txt2img_inf,
            inputs=[
                prompt,
                negative_prompt,
                height,
                width,
                steps,
                guidance_scale,
                seed,
                batch_count,
                batch_size,
                scheduler,
                txt2img_custom_model,
                custom_vae,
                precision,
                device,
                max_length,
                save_metadata_to_json,
                save_metadata_to_png,
                lora_weights,
                lora_hf_id,
                ondemand,
                repeatable_seeds,
                use_hiresfix,
                hiresfix_height,
                hiresfix_width,
                hiresfix_strength,
                resample_type,
            ],
            outputs=[txt2img_gallery, std_output, txt2img_status],
            show_progress="minimal" if args.progress_bar else "none",
        )

        status_kwargs = dict(
            fn=lambda bc, bs: status_label("Text-to-Image", 0, bc, bs),
            inputs=[batch_count, batch_size],
            outputs=txt2img_status,
        )

        prompt_submit = prompt.submit(**status_kwargs).then(**kwargs)
        neg_prompt_submit = negative_prompt.submit(**status_kwargs).then(
            **kwargs
        )
        generate_click = stable_diffusion.click(**status_kwargs).then(**kwargs)
        stop_batch.click(
            fn=cancel_sd,
            cancels=[prompt_submit, neg_prompt_submit, generate_click],
        )

        txt2img_png_info_img.change(
            fn=import_png_metadata,
            inputs=[
                txt2img_png_info_img,
                prompt,
                negative_prompt,
                steps,
                scheduler,
                guidance_scale,
                seed,
                width,
                height,
                txt2img_custom_model,
                lora_weights,
                lora_hf_id,
                custom_vae,
            ],
            outputs=[
                txt2img_png_info_img,
                prompt,
                negative_prompt,
                steps,
                scheduler,
                guidance_scale,
                seed,
                width,
                height,
                txt2img_custom_model,
                lora_weights,
                lora_hf_id,
                custom_vae,
            ],
        )

        # SharkEulerDiscrete doesn't work with img2img which hires_fix uses
        def set_compatible_schedulers(hires_fix_selected):
            if hires_fix_selected:
                return gr.Dropdown.update(
                    choices=scheduler_list_cpu_only,
                    value="DEISMultistep",
                )
            else:
                return gr.Dropdown.update(
                    choices=scheduler_list,
                    value="SharkEulerDiscrete",
                )

        use_hiresfix.change(
            fn=set_compatible_schedulers,
            inputs=[use_hiresfix],
            outputs=[scheduler],
            queue=False,
        )

        lora_weights.change(
            fn=lora_changed,
            inputs=[lora_weights],
            outputs=[lora_tags],
            queue=True,
        )

```

`apps/stable_diffusion/web/ui/upscaler_ui.py`:

```py
import os
import torch
import time
import gradio as gr
from PIL import Image

from apps.stable_diffusion.web.ui.utils import (
    available_devices,
    nodlogo_loc,
    get_custom_model_path,
    get_custom_model_files,
    scheduler_list_cpu_only,
    predefined_upscaler_models,
    cancel_sd,
)
from apps.stable_diffusion.web.ui.common_ui_events import lora_changed
from apps.stable_diffusion.web.utils.common_label_calc import status_label
from apps.stable_diffusion.src import (
    args,
    UpscalerPipeline,
    get_schedulers,
    set_init_device_flags,
    utils,
    save_output_img,
)
from apps.stable_diffusion.src.utils import get_generated_imgs_path

# set initial values of iree_vulkan_target_triple, use_tuned and import_mlir.
init_iree_vulkan_target_triple = args.iree_vulkan_target_triple
init_use_tuned = args.use_tuned
init_import_mlir = args.import_mlir


# Exposed to UI.
def upscaler_inf(
    prompt: str,
    negative_prompt: str,
    init_image,
    height: int,
    width: int,
    steps: int,
    noise_level: int,
    guidance_scale: float,
    seed: str,
    batch_count: int,
    batch_size: int,
    scheduler: str,
    model_id: str,
    custom_vae: str,
    precision: str,
    device: str,
    max_length: int,
    save_metadata_to_json: bool,
    save_metadata_to_png: bool,
    lora_weights: str,
    lora_hf_id: str,
    ondemand: bool,
    repeatable_seeds: bool,
):
    from apps.stable_diffusion.web.ui.utils import (
        get_custom_model_pathfile,
        get_custom_vae_or_lora_weights,
        Config,
    )
    import apps.stable_diffusion.web.utils.global_obj as global_obj
    from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
        SD_STATE_CANCEL,
    )

    args.prompts = [prompt]
    args.negative_prompts = [negative_prompt]
    args.guidance_scale = guidance_scale
    args.seed = seed
    args.steps = steps
    args.scheduler = scheduler
    args.ondemand = ondemand

    if init_image is None:
        return None, "An Initial Image is required"
    image = init_image.convert("RGB").resize((height, width))

    # set ckpt_loc and hf_model_id.
    args.ckpt_loc = ""
    args.hf_model_id = ""
    args.custom_vae = ""

    # .safetensor or .chkpt on the custom model path
    if model_id in get_custom_model_files(custom_checkpoint_type="upscaler"):
        args.ckpt_loc = get_custom_model_pathfile(model_id)
    # civitai download
    elif "civitai" in model_id:
        args.ckpt_loc = model_id
    # either predefined or huggingface
    else:
        args.hf_model_id = model_id

    if custom_vae != "None":
        args.custom_vae = get_custom_model_pathfile(custom_vae, model="vae")

    args.save_metadata_to_json = save_metadata_to_json
    args.write_metadata_to_png = save_metadata_to_png

    args.use_lora = get_custom_vae_or_lora_weights(
        lora_weights, lora_hf_id, "lora"
    )

    dtype = torch.float32 if precision == "fp32" else torch.half
    cpu_scheduling = not scheduler.startswith("Shark")
    args.height = 128
    args.width = 128
    new_config_obj = Config(
        "upscaler",
        args.hf_model_id,
        args.ckpt_loc,
        args.custom_vae,
        precision,
        batch_size,
        max_length,
        args.height,
        args.width,
        device,
        use_lora=args.use_lora,
        stencils=[],
        ondemand=ondemand,
    )
    if (
        not global_obj.get_sd_obj()
        or global_obj.get_cfg_obj() != new_config_obj
    ):
        global_obj.clear_cache()
        global_obj.set_cfg_obj(new_config_obj)
        args.batch_size = batch_size
        args.max_length = max_length
        args.device = device.split("=>", 1)[1].strip()
        args.iree_vulkan_target_triple = init_iree_vulkan_target_triple
        args.use_tuned = init_use_tuned
        args.import_mlir = init_import_mlir
        set_init_device_flags()
        model_id = (
            args.hf_model_id
            if args.hf_model_id
            else "stabilityai/stable-diffusion-2-1-base"
        )
        global_obj.set_schedulers(get_schedulers(model_id))
        scheduler_obj = global_obj.get_scheduler(scheduler)
        global_obj.set_sd_obj(
            UpscalerPipeline.from_pretrained(
                scheduler_obj,
                args.import_mlir,
                args.hf_model_id,
                args.ckpt_loc,
                args.custom_vae,
                args.precision,
                args.max_length,
                args.batch_size,
                args.height,
                args.width,
                args.use_base_vae,
                args.use_tuned,
                low_cpu_mem_usage=args.low_cpu_mem_usage,
                use_lora=args.use_lora,
                ondemand=args.ondemand,
            )
        )

    global_obj.set_sd_scheduler(scheduler)
    global_obj.get_sd_obj().low_res_scheduler = global_obj.get_scheduler(
        "DDPM"
    )

    start_time = time.time()
    global_obj.get_sd_obj().log = ""
    generated_imgs = []
    extra_info = {"NOISE LEVEL": noise_level}
    try:
        seeds = utils.batch_seeds(seed, batch_count, repeatable_seeds)
    except TypeError as error:
        raise gr.Error(str(error)) from None

    for current_batch in range(batch_count):
        low_res_img = image
        high_res_img = Image.new("RGB", (height * 4, width * 4))

        for i in range(0, width, 128):
            for j in range(0, height, 128):
                box = (j, i, j + 128, i + 128)
                upscaled_image = global_obj.get_sd_obj().generate_images(
                    prompt,
                    negative_prompt,
                    low_res_img.crop(box),
                    batch_size,
                    args.height,
                    args.width,
                    steps,
                    noise_level,
                    guidance_scale,
                    seeds[current_batch],
                    args.max_length,
                    dtype,
                    args.use_base_vae,
                    cpu_scheduling,
                    args.max_embeddings_multiples,
                )
                if global_obj.get_sd_status() == SD_STATE_CANCEL:
                    break
                else:
                    high_res_img.paste(upscaled_image[0], (j * 4, i * 4))

            if global_obj.get_sd_status() == SD_STATE_CANCEL:
                break

        total_time = time.time() - start_time
        text_output = f"prompt={args.prompts}"
        text_output += f"\nnegative prompt={args.negative_prompts}"
        text_output += (
            f"\nmodel_id={args.hf_model_id}, " f"ckpt_loc={args.ckpt_loc}"
        )
        text_output += f"\nscheduler={args.scheduler}, " f"device={device}"
        text_output += (
            f"\nsteps={steps}, "
            f"noise_level={noise_level}, "
            f"guidance_scale={guidance_scale}, "
            f"seed={seeds[:current_batch + 1]}"
        )
        text_output += (
            f"\ninput size={height}x{width}, "
            f"output size={height*4}x{width*4}, "
            f"batch_count={batch_count}, "
            f"batch_size={batch_size}, "
            f"max_length={args.max_length}\n"
        )

        text_output += global_obj.get_sd_obj().log
        text_output += f"\nTotal image generation time: {total_time:.4f}sec"

        if global_obj.get_sd_status() == SD_STATE_CANCEL:
            break
        else:
            save_output_img(high_res_img, seeds[current_batch], extra_info)
            generated_imgs.append(high_res_img)
            global_obj.get_sd_obj().log += "\n"
            yield generated_imgs, text_output, status_label(
                "Upscaler", current_batch + 1, batch_count, batch_size
            )

    yield generated_imgs, text_output, ""


with gr.Blocks(title="Upscaler") as upscaler_web:
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Row():
            with gr.Column(scale=1, elem_id="demo_title_outer"):
                gr.Image(
                    value=nod_logo,
                    show_label=False,
                    interactive=False,
                    elem_id="top_logo",
                    width=150,
                    height=50,
                )
    with gr.Row(elem_id="ui_body"):
        with gr.Row():
            with gr.Column(scale=1, min_width=600):
                with gr.Row():
                    upscaler_model_info = (
                        f"Custom Model Path: {str(get_custom_model_path())}"
                    )
                    upscaler_custom_model = gr.Dropdown(
                        label=f"Models",
                        info="Select, or enter HuggingFace Model ID or Civitai model download URL",
                        elem_id="custom_model",
                        value=os.path.basename(args.ckpt_loc)
                        if args.ckpt_loc
                        else "stabilityai/stable-diffusion-x4-upscaler",
                        choices=get_custom_model_files(
                            custom_checkpoint_type="upscaler"
                        )
                        + predefined_upscaler_models,
                        allow_custom_value=True,
                        scale=2,
                    )
                    # janky fix for overflowing text
                    upscaler_vae_info = (
                        str(get_custom_model_path("vae"))
                    ).replace("\\", "\n\\")
                    upscaler_vae_info = f"VAE Path: {upscaler_vae_info}"
                    custom_vae = gr.Dropdown(
                        label=f"Custom VAE Models",
                        info=upscaler_vae_info,
                        elem_id="custom_model",
                        value=os.path.basename(args.custom_vae)
                        if args.custom_vae
                        else "None",
                        choices=["None"] + get_custom_model_files("vae"),
                        allow_custom_value=True,
                        scale=1,
                    )

                with gr.Group(elem_id="prompt_box_outer"):
                    prompt = gr.Textbox(
                        label="Prompt",
                        value=args.prompts[0],
                        lines=2,
                        elem_id="prompt_box",
                    )
                    negative_prompt = gr.Textbox(
                        label="Negative Prompt",
                        value=args.negative_prompts[0],
                        lines=2,
                        elem_id="negative_prompt_box",
                    )

                upscaler_init_image = gr.Image(
                    label="Input Image",
                    type="pil",
                    height=300,
                )

                with gr.Accordion(label="LoRA Options", open=False):
                    with gr.Row():
                        # janky fix for overflowing text
                        upscaler_lora_info = (
                            str(get_custom_model_path("lora"))
                        ).replace("\\", "\n\\")
                        upscaler_lora_info = f"LoRA Path: {upscaler_lora_info}"
                        lora_weights = gr.Dropdown(
                            label=f"Standalone LoRA Weights",
                            info=upscaler_lora_info,
                            elem_id="lora_weights",
                            value="None",
                            choices=["None"] + get_custom_model_files("lora"),
                            allow_custom_value=True,
                        )
                        lora_hf_id = gr.Textbox(
                            elem_id="lora_hf_id",
                            placeholder="Select 'None' in the Standalone LoRA "
                            "weights dropdown on the left if you want to use "
                            "a standalone HuggingFace model ID for LoRA here "
                            "e.g: sayakpaul/sd-model-finetuned-lora-t4",
                            value="",
                            label="HuggingFace Model ID",
                            lines=3,
                        )
                    with gr.Row():
                        lora_tags = gr.HTML(
                            value="<div><i>No LoRA selected</i></div>",
                            elem_classes="lora-tags",
                        )
                with gr.Accordion(label="Advanced Options", open=False):
                    with gr.Row():
                        scheduler = gr.Dropdown(
                            elem_id="scheduler",
                            label="Scheduler",
                            value="DDIM",
                            choices=scheduler_list_cpu_only,
                            allow_custom_value=True,
                        )
                        with gr.Group():
                            save_metadata_to_png = gr.Checkbox(
                                label="Save prompt information to PNG",
                                value=args.write_metadata_to_png,
                                interactive=True,
                            )
                            save_metadata_to_json = gr.Checkbox(
                                label="Save prompt information to JSON file",
                                value=args.save_metadata_to_json,
                                interactive=True,
                            )
                    with gr.Row():
                        height = gr.Slider(
                            128,
                            512,
                            value=args.height,
                            step=128,
                            label="Height",
                        )
                        width = gr.Slider(
                            128,
                            512,
                            value=args.width,
                            step=128,
                            label="Width",
                        )
                        precision = gr.Radio(
                            label="Precision",
                            value=args.precision,
                            choices=[
                                "fp16",
                                "fp32",
                            ],
                            visible=True,
                        )
                        max_length = gr.Radio(
                            label="Max Length",
                            value=args.max_length,
                            choices=[
                                64,
                                77,
                            ],
                            visible=False,
                        )
                    with gr.Row():
                        steps = gr.Slider(
                            1, 100, value=args.steps, step=1, label="Steps"
                        )
                        noise_level = gr.Slider(
                            0,
                            100,
                            value=args.noise_level,
                            step=1,
                            label="Noise Level",
                        )
                        ondemand = gr.Checkbox(
                            value=args.ondemand,
                            label="Low VRAM",
                            interactive=True,
                        )
                    with gr.Row():
                        with gr.Column(scale=3):
                            guidance_scale = gr.Slider(
                                0,
                                50,
                                value=args.guidance_scale,
                                step=0.1,
                                label="CFG Scale",
                            )
                        with gr.Column(scale=3):
                            batch_count = gr.Slider(
                                1,
                                100,
                                value=args.batch_count,
                                step=1,
                                label="Batch Count",
                                interactive=True,
                            )
                        repeatable_seeds = gr.Checkbox(
                            args.repeatable_seeds,
                            label="Repeatable Seeds",
                        )
                    with gr.Row():
                        batch_size = gr.Slider(
                            1,
                            4,
                            value=args.batch_size,
                            step=1,
                            label="Batch Size",
                            interactive=False,
                            visible=False,
                        )
                with gr.Row():
                    seed = gr.Textbox(
                        value=args.seed,
                        label="Seed",
                        info="An integer or a JSON list of integers, -1 for random",
                    )
                    device = gr.Dropdown(
                        elem_id="device",
                        label="Device",
                        value=available_devices[0],
                        choices=available_devices,
                        allow_custom_value=True,
                    )

            with gr.Column(scale=1, min_width=600):
                with gr.Group():
                    upscaler_gallery = gr.Gallery(
                        label="Generated images",
                        show_label=False,
                        elem_id="gallery",
                        columns=[2],
                        object_fit="contain",
                    )
                    std_output = gr.Textbox(
                        value=f"{upscaler_model_info}\n"
                        f"Images will be saved at "
                        f"{get_generated_imgs_path()}",
                        lines=2,
                        elem_id="std_output",
                        show_label=False,
                    )
                    upscaler_status = gr.Textbox(visible=False)
                with gr.Row():
                    stable_diffusion = gr.Button("Generate Image(s)")
                    random_seed = gr.Button("Randomize Seed")
                    random_seed.click(
                        lambda: -1,
                        inputs=[],
                        outputs=[seed],
                        queue=False,
                    )
                    stop_batch = gr.Button("Stop Batch")
                with gr.Row():
                    blank_thing_for_row = None
                with gr.Row():
                    upscaler_sendto_img2img = gr.Button(value="SendTo Img2Img")
                    upscaler_sendto_inpaint = gr.Button(value="SendTo Inpaint")
                    upscaler_sendto_outpaint = gr.Button(
                        value="SendTo Outpaint"
                    )

        kwargs = dict(
            fn=upscaler_inf,
            inputs=[
                prompt,
                negative_prompt,
                upscaler_init_image,
                height,
                width,
                steps,
                noise_level,
                guidance_scale,
                seed,
                batch_count,
                batch_size,
                scheduler,
                upscaler_custom_model,
                custom_vae,
                precision,
                device,
                max_length,
                save_metadata_to_json,
                save_metadata_to_png,
                lora_weights,
                lora_hf_id,
                ondemand,
                repeatable_seeds,
            ],
            outputs=[upscaler_gallery, std_output, upscaler_status],
            show_progress="minimal" if args.progress_bar else "none",
        )
        status_kwargs = dict(
            fn=lambda bc, bs: status_label("Upscaler", 0, bc, bs),
            inputs=[batch_count, batch_size],
            outputs=upscaler_status,
        )

        prompt_submit = prompt.submit(**status_kwargs).then(**kwargs)
        neg_prompt_submit = negative_prompt.submit(**status_kwargs).then(
            **kwargs
        )
        generate_click = stable_diffusion.click(**status_kwargs).then(**kwargs)
        stop_batch.click(
            fn=cancel_sd,
            cancels=[prompt_submit, neg_prompt_submit, generate_click],
        )

        lora_weights.change(
            fn=lora_changed,
            inputs=[lora_weights],
            outputs=[lora_tags],
            queue=True,
        )

```

`apps/stable_diffusion/web/ui/utils.py`:

```py
import os
import sys
import glob
import math
import json
import safetensors

from pathlib import Path
from apps.stable_diffusion.src import args
from dataclasses import dataclass
from enum import IntEnum

from apps.stable_diffusion.src import get_available_devices
import apps.stable_diffusion.web.utils.global_obj as global_obj
from apps.stable_diffusion.src.pipelines.pipeline_shark_stable_diffusion_utils import (
    SD_STATE_CANCEL,
)


@dataclass
class Config:
    mode: str
    model_id: str
    ckpt_loc: str
    custom_vae: str
    precision: str
    batch_size: int
    max_length: int
    height: int
    width: int
    device: str
    use_lora: str
    stencils: list[str]
    ondemand: str  # should this be expecting a bool instead?


class HSLHue(IntEnum):
    RED = 0
    YELLOW = 60
    GREEN = 120
    CYAN = 180
    BLUE = 240
    MAGENTA = 300


custom_model_filetypes = (
    "*.ckpt",
    "*.safetensors",
)  # the tuple of file types

scheduler_list_cpu_only = [
    "DDIM",
    "PNDM",
    "LMSDiscrete",
    "KDPM2Discrete",
    "DPMSolverMultistep",
    "DPMSolverMultistep++",
    "DPMSolverMultistepKarras",
    "DPMSolverMultistepKarras++",
    "EulerDiscrete",
    "EulerAncestralDiscrete",
    "DEISMultistep",
    "KDPM2AncestralDiscrete",
    "DPMSolverSinglestep",
    "DDPM",
    "HeunDiscrete",
]
scheduler_list = scheduler_list_cpu_only + [
    "SharkEulerDiscrete",
]

predefined_models = [
    "Linaqruf/anything-v3.0",
    "prompthero/openjourney",
    "wavymulder/Analog-Diffusion",
    "xzuyn/PhotoMerge",
    "stabilityai/stable-diffusion-2-1",
    "stabilityai/stable-diffusion-2-1-base",
    "CompVis/stable-diffusion-v1-4",
]

predefined_paint_models = [
    "runwayml/stable-diffusion-inpainting",
    "stabilityai/stable-diffusion-2-inpainting",
    "xzuyn/PhotoMerge-inpainting",
]
predefined_upscaler_models = [
    "stabilityai/stable-diffusion-x4-upscaler",
]


def resource_path(relative_path):
    """Get absolute path to resource, works for dev and for PyInstaller"""
    base_path = getattr(
        sys, "_MEIPASS", os.path.dirname(os.path.abspath(__file__))
    )
    return os.path.join(base_path, relative_path)


def create_custom_models_folders():
    dir = ["vae", "lora"]
    if not args.ckpt_dir:
        dir.insert(0, "models")
    else:
        if not os.path.isdir(args.ckpt_dir):
            sys.exit(
                f"Invalid --ckpt_dir argument, "
                f"{args.ckpt_dir} folder does not exists."
            )
    for root in dir:
        get_custom_model_path(root).mkdir(parents=True, exist_ok=True)


def get_custom_model_path(model="models"):
    # structure in WebUI :-
    #       models or args.ckpt_dir
    #         |___lora
    #         |___vae
    sub_folder = "" if model == "models" else model
    if args.ckpt_dir:
        return Path(Path(args.ckpt_dir), sub_folder)
    else:
        return Path(Path.cwd(), "models/" + sub_folder)


def get_custom_model_pathfile(custom_model_name, model="models"):
    return os.path.join(get_custom_model_path(model), custom_model_name)


def get_custom_model_files(model="models", custom_checkpoint_type=""):
    ckpt_files = []
    file_types = custom_model_filetypes
    if model == "lora":
        file_types = custom_model_filetypes + ("*.pt", "*.bin")
    for extn in file_types:
        files = [
            os.path.basename(x)
            for x in glob.glob(
                os.path.join(get_custom_model_path(model), extn)
            )
        ]
        match custom_checkpoint_type:
            case "inpainting":
                files = [
                    val
                    for val in files
                    if val.endswith("inpainting" + extn.removeprefix("*"))
                ]
            case "upscaler":
                files = [
                    val
                    for val in files
                    if val.endswith("upscaler" + extn.removeprefix("*"))
                ]
            case _:
                files = [
                    val
                    for val in files
                    if not (
                        val.endswith("inpainting" + extn.removeprefix("*"))
                        or val.endswith("upscaler" + extn.removeprefix("*"))
                    )
                ]
        ckpt_files.extend(files)
    return sorted(ckpt_files, key=str.casefold)


def get_custom_vae_or_lora_weights(weights, hf_id, model):
    use_weight = ""
    if weights == "None" and not hf_id:
        use_weight = ""
    elif not hf_id:
        use_weight = get_custom_model_pathfile(weights, model)
    else:
        use_weight = hf_id
    return use_weight


def hsl_color(alpha: float, start, end):
    b = (end - start) * (alpha if alpha > 0 else 0)
    result = b + start

    # Return a CSS HSL string
    return f"hsl({math.floor(result)}, 80%, 35%)"


def get_lora_metadata(lora_filename):
    # get the metadata from the file
    filename = get_custom_model_pathfile(lora_filename, "lora")
    with safetensors.safe_open(filename, framework="pt", device="cpu") as f:
        metadata = f.metadata()

    # guard clause for if there isn't any metadata
    if not metadata:
        return None

    # metadata is a dictionary of strings, the values of the keys we're
    # interested in are actually json, and need to be loaded as such
    tag_frequencies = json.loads(metadata.get("ss_tag_frequency", str("{}")))
    dataset_dirs = json.loads(metadata.get("ss_dataset_dirs", str("{}")))
    tag_dirs = [dir for dir in tag_frequencies.keys()]

    # gather the tag frequency information for all the datasets trained
    all_frequencies = {}
    for dataset in tag_dirs:
        frequencies = sorted(
            [entry for entry in tag_frequencies[dataset].items()],
            reverse=True,
            key=lambda x: x[1],
        )

        # get a figure for the total number of images processed for this dataset
        # either then number actually listed or in its dataset_dir entry or
        # the highest frequency's number if that doesn't exist
        img_count = dataset_dirs.get(dir, {}).get(
            "img_count", frequencies[0][1]
        )

        # add the dataset frequencies to the overall frequencies replacing the
        # frequency counts on the tags with a percentage/ratio
        all_frequencies.update(
            [(entry[0], entry[1] / img_count) for entry in frequencies]
        )

    trained_model_id = " ".join(
        [
            metadata.get("ss_sd_model_hash", ""),
            metadata.get("ss_sd_model_name", ""),
            metadata.get("ss_base_model_version", ""),
        ]
    ).strip()

    # return the topmost <count> of all frequencies in all datasets
    return {
        "model": trained_model_id,
        "frequencies": sorted(
            all_frequencies.items(), reverse=True, key=lambda x: x[1]
        ),
    }


def cancel_sd():
    # Try catch it, as gc can delete global_obj.sd_obj while switching model
    try:
        global_obj.set_sd_status(SD_STATE_CANCEL)
    except Exception:
        pass


nodlogo_loc = resource_path("logos/nod-logo.png")
nodicon_loc = resource_path("logos/nod-icon.png")
available_devices = get_available_devices()

```

`apps/stable_diffusion/web/utils/app.py`:

```py
import os
import sys
import webview
import webview.util
import socket

from contextlib import closing
from multiprocessing import Process

from apps.stable_diffusion.src import args


def webview2_installed():
    if sys.platform != "win32":
        return False

    # On windows we want to ensure we have MS webview2 available so we don't fall back
    # to MSHTML (aka ye olde Internet Explorer) which is deprecated by pywebview, and
    # apparently causes SHARK not to load in properly.

    # Checking these registry entries is how Microsoft says to detect a webview2 installation:
    # https://learn.microsoft.com/en-us/microsoft-edge/webview2/concepts/distribution
    import winreg

    path = r"SOFTWARE\WOW6432Node\Microsoft\EdgeUpdate\Clients\{F3017226-FE2A-4295-8BDF-00C3A9A7E4C5}"

    # only way can find if a registry entry even exists is to try and open it
    try:
        # check for an all user install
        with winreg.OpenKey(
            winreg.HKEY_LOCAL_MACHINE,
            path,
            0,
            winreg.KEY_QUERY_VALUE | winreg.KEY_WOW64_64KEY,
        ) as registry_key:
            value, type = winreg.QueryValueEx(registry_key, "pv")

    # if it didn't exist, we want to continue on...
    except WindowsError:
        try:
            # ...to check for a current user install
            with winreg.OpenKey(
                winreg.HKEY_CURRENT_USER,
                path,
                0,
                winreg.KEY_QUERY_VALUE | winreg.KEY_WOW64_64KEY,
            ) as registry_key:
                value, type = winreg.QueryValueEx(registry_key, "pv")
        except WindowsError:
            value = None
    finally:
        return (value is not None) and value != "" and value != "0.0.0.0"


def window(address):
    from tkinter import Tk

    window = Tk()

    # get screen width and height of display and make it more reasonably
    # sized as we aren't making it full-screen or maximized
    width = int(window.winfo_screenwidth() * 0.81)
    height = int(window.winfo_screenheight() * 0.91)
    webview.create_window(
        "SHARK AI Studio",
        url=address,
        width=width,
        height=height,
        text_select=True,
    )
    webview.start(private_mode=False, storage_path=os.getcwd())


def usable_port():
    # Make sure we can actually use the port given in args.server_port. If
    # not ask the OS for a port and return that as our port to use.

    port = args.server_port

    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
        try:
            sock.bind(("0.0.0.0", port))
        except OSError:
            with closing(
                socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            ) as sock:
                sock.bind(("0.0.0.0", 0))
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                return sock.getsockname()[1]

    return port


def launch(port):
    # setup to launch as an app if app mode has been requested and we're able
    # to do it, answering whether we succeeded.
    if args.ui == "app" and (sys.platform != "win32" or webview2_installed()):
        try:
            t = Process(target=window, args=[f"http://localhost:{port}"])
            t.start()
            return True
        except webview.util.WebViewException:
            return False
    else:
        return False

```

`apps/stable_diffusion/web/utils/common_label_calc.py`:

```py
# functions for generating labels used in common by tabs across the UI


def status_label(tab_name, batch_index=0, batch_count=1, batch_size=1):
    if batch_index < batch_count:
        bs = f"x{batch_size}" if batch_size > 1 else ""
        return f"{tab_name} generating {batch_index+1}/{batch_count}{bs}"
    else:
        return f"{tab_name} complete"

```

`apps/stable_diffusion/web/utils/global_obj.py`:

```py
import gc


"""
The global objects include SD pipeline and config.
Maintaining the global objects would avoid creating extra pipeline objects when switching modes.
Also we could avoid memory leak when switching models by clearing the cache.
"""


def _init():
    global _sd_obj
    global _config_obj
    global _schedulers
    _sd_obj = None
    _config_obj = None
    _schedulers = None


def set_sd_obj(value):
    global _sd_obj
    _sd_obj = value


def set_sd_scheduler(key):
    global _sd_obj
    _sd_obj.scheduler = _schedulers[key]


def set_sd_status(value):
    global _sd_obj
    _sd_obj.status = value


def set_cfg_obj(value):
    global _config_obj
    _config_obj = value


def set_schedulers(value):
    global _schedulers
    _schedulers = value


def get_sd_obj():
    global _sd_obj
    return _sd_obj


def get_sd_status():
    global _sd_obj
    return _sd_obj.status


def get_cfg_obj():
    global _config_obj
    return _config_obj


def get_scheduler(key):
    global _schedulers
    return _schedulers[key]


def clear_cache():
    global _sd_obj
    global _config_obj
    global _schedulers
    del _sd_obj
    del _config_obj
    del _schedulers
    gc.collect()
    _sd_obj = None
    _config_obj = None
    _schedulers = None

```

`apps/stable_diffusion/web/utils/metadata/__init__.py`:

```py
from .png_metadata import (
    import_png_metadata,
)
from .display import (
    displayable_metadata,
)

```

`apps/stable_diffusion/web/utils/metadata/csv_metadata.py`:

```py
import csv
import os
from .format import humanize, humanizable


def csv_path(image_filename: str):
    return os.path.join(os.path.dirname(image_filename), "imgs_details.csv")


def has_csv(image_filename: str) -> bool:
    return os.path.exists(csv_path(image_filename))


def matching_filename(image_filename: str, row):
    # we assume the final column of the csv has the original filename with full path and match that
    # against the image_filename if we are given a list. Otherwise we assume a dict and and take
    # the value of the OUTPUT key
    return os.path.basename(image_filename) in (
        row[-1] if isinstance(row, list) else row["OUTPUT"]
    )


def parse_csv(image_filename: str):
    csv_filename = csv_path(image_filename)

    with open(csv_filename, "r", newline="") as csv_file:
        # We use a reader or DictReader here for images_details.csv depending on whether we think it
        # has headers or not. Having headers means less guessing of the format.
        has_header = csv.Sniffer().has_header(csv_file.read(2048))
        csv_file.seek(0)

        reader = (
            csv.DictReader(csv_file) if has_header else csv.reader(csv_file)
        )

        matches = [
            # we rely on humanize and humanizable to work out the parsing of the individual .csv rows
            humanize(row)
            for row in reader
            if row
            and (has_header or humanizable(row))
            and matching_filename(image_filename, row)
        ]

    return matches[0] if matches else {}

```

`apps/stable_diffusion/web/utils/metadata/display.py`:

```py
import json
import os
from PIL import Image
from .png_metadata import parse_generation_parameters
from .exif_metadata import has_exif, parse_exif
from .csv_metadata import has_csv, parse_csv
from .format import compact, humanize


def displayable_metadata(image_filename: str) -> dict:
    if not os.path.isfile(image_filename):
        return {"source": "missing", "parameters": {}}

    pil_image = Image.open(image_filename)

    # we have PNG generation parameters (preferred, as it's what the txt2img dropzone reads,
    # and we go via that for SendTo, and is directly tied to the image)
    if "parameters" in pil_image.info:
        return {
            "source": "png",
            "parameters": compact(
                parse_generation_parameters(pil_image.info["parameters"])
            ),
        }

    # we have a matching json file (next most likely to be accurate when it's there)
    json_path = os.path.splitext(image_filename)[0] + ".json"
    if os.path.isfile(json_path):
        with open(json_path) as params_file:
            return {
                "source": "json",
                "parameters": compact(
                    humanize(json.load(params_file), includes_filename=False)
                ),
            }

    # we have a CSV file so try that (can be different shapes, and it usually has no
    # headers/param names so of the things we we *know* have parameters, it's the
    # last resort)
    if has_csv(image_filename):
        params = parse_csv(image_filename)
        if params:  # we might not have found the filename in the csv
            return {
                "source": "csv",
                "parameters": compact(params),  # already humanized
            }

    # EXIF data, probably a .jpeg, may well not include parameters, but at least it's *something*
    if has_exif(image_filename):
        return {"source": "exif", "parameters": parse_exif(pil_image)}

    # we've got nothing
    return None

```

`apps/stable_diffusion/web/utils/metadata/exif_metadata.py`:

```py
from PIL import Image
from PIL.ExifTags import Base as EXIFKeys, TAGS, IFD, GPSTAGS


def has_exif(image_filename: str) -> bool:
    return True if Image.open(image_filename).getexif() else False


def parse_exif(pil_image: Image) -> dict:
    img_exif = pil_image.getexif()

    # See this stackoverflow answer for where most this comes from: https://stackoverflow.com/a/75357594
    # I did try to use the exif library but it broke just as much as my initial attempt at this (albeit I
    # I was probably using it wrong) so I reverted back to using PIL with more filtering and saved a
    # dependency
    exif_tags = {
        TAGS.get(key, key): str(val)
        for (key, val) in img_exif.items()
        if key in TAGS
        and key not in (EXIFKeys.ExifOffset, EXIFKeys.GPSInfo)
        and val
        and (not isinstance(val, bytes))
        and (not str(val).isspace())
    }

    def try_get_ifd(ifd_id):
        try:
            return img_exif.get_ifd(ifd_id).items()
        except KeyError:
            return {}

    ifd_tags = {
        TAGS.get(key, key): str(val)
        for ifd_id in IFD
        for (key, val) in try_get_ifd(ifd_id)
        if ifd_id != IFD.GPSInfo
        and key in TAGS
        and val
        and (not isinstance(val, bytes))
        and (not str(val).isspace())
    }

    gps_tags = {
        GPSTAGS.get(key, key): str(val)
        for (key, val) in try_get_ifd(IFD.GPSInfo)
        if key in GPSTAGS
        and val
        and (not isinstance(val, bytes))
        and (not str(val).isspace())
    }

    return {**exif_tags, **ifd_tags, **gps_tags}

```

`apps/stable_diffusion/web/utils/metadata/format.py`:

```py
# As SHARK has evolved more columns have been added to images_details.csv. However, since
# no version of the CSV has any headers (yet) we don't actually have anything within the
# file that tells us which parameter each column is for. So this is a list of known patterns
# indexed by length which is what we're going to have to use to guess which columns are the
# right ones for the file we're looking at.

# The same ordering is used for JSON, but these do have key names, however they are not very
# human friendly, nor do they match up with the what is written to the .png headers

# So these are functions to try and get something consistent out the raw input from all
# these sources

PARAMS_FORMATS = {
    9: {
        "VARIANT": "Model",
        "SCHEDULER": "Sampler",
        "PROMPT": "Prompt",
        "NEG_PROMPT": "Negative prompt",
        "SEED": "Seed",
        "CFG_SCALE": "CFG scale",
        "PRECISION": "Precision",
        "STEPS": "Steps",
        "OUTPUT": "Filename",
    },
    10: {
        "MODEL": "Model",
        "VARIANT": "Variant",
        "SCHEDULER": "Sampler",
        "PROMPT": "Prompt",
        "NEG_PROMPT": "Negative prompt",
        "SEED": "Seed",
        "CFG_SCALE": "CFG scale",
        "PRECISION": "Precision",
        "STEPS": "Steps",
        "OUTPUT": "Filename",
    },
    12: {
        "VARIANT": "Model",
        "SCHEDULER": "Sampler",
        "PROMPT": "Prompt",
        "NEG_PROMPT": "Negative prompt",
        "SEED": "Seed",
        "CFG_SCALE": "CFG scale",
        "PRECISION": "Precision",
        "STEPS": "Steps",
        "HEIGHT": "Height",
        "WIDTH": "Width",
        "MAX_LENGTH": "Max Length",
        "OUTPUT": "Filename",
    },
}

PARAMS_FORMAT_CURRENT = {
    "VARIANT": "Model",
    "VAE": "VAE",
    "LORA": "LoRA",
    "SCHEDULER": "Sampler",
    "PROMPT": "Prompt",
    "NEG_PROMPT": "Negative prompt",
    "SEED": "Seed",
    "CFG_SCALE": "CFG scale",
    "PRECISION": "Precision",
    "STEPS": "Steps",
    "HEIGHT": "Height",
    "WIDTH": "Width",
    "MAX_LENGTH": "Max Length",
    "OUTPUT": "Filename",
}


def compact(metadata: dict) -> dict:
    # we don't want to alter the original dictionary
    result = dict(metadata)

    # discard the filename because we should already have it
    if result.keys() & {"Filename"}:
        result.pop("Filename")

    # make showing the sizes more compact by using only one line each
    if result.keys() & {"Size-1", "Size-2"}:
        result["Size"] = f"{result.pop('Size-1')}x{result.pop('Size-2')}"
    elif result.keys() & {"Height", "Width"}:
        result["Size"] = f"{result.pop('Height')}x{result.pop('Width')}"

    if result.keys() & {"Hires resize-1", "Hires resize-1"}:
        hires_y = result.pop("Hires resize-1")
        hires_x = result.pop("Hires resize-2")

        if hires_x == 0 and hires_y == 0:
            result["Hires resize"] = "None"
        else:
            result["Hires resize"] = f"{hires_y}x{hires_x}"

    # remove VAE if it exists and is empty
    if (result.keys() & {"VAE"}) and (
        not result["VAE"] or result["VAE"] == "None"
    ):
        result.pop("VAE")

    # remove LoRA if it exists and is empty
    if (result.keys() & {"LoRA"}) and (
        not result["LoRA"] or result["LoRA"] == "None"
    ):
        result.pop("LoRA")

    return result


def humanizable(metadata: dict | list[str], includes_filename=True) -> dict:
    lookup_key = len(metadata) + (0 if includes_filename else 1)
    return lookup_key in PARAMS_FORMATS.keys()


def humanize(metadata: dict | list[str], includes_filename=True) -> dict:
    lookup_key = len(metadata) + (0 if includes_filename else 1)

    # For lists we can only work based on the length, we have no other information
    if isinstance(metadata, list):
        if humanizable(metadata, includes_filename):
            return dict(zip(PARAMS_FORMATS[lookup_key].values(), metadata))
        else:
            raise KeyError(
                f"Humanize could not find the format for a parameter list of length {len(metadata)}"
            )

    # For dictionaries we try to use the matching length parameter format if
    # available, otherwise we just use the current format which is assumed to
    # have everything currently known about. Then we swap keys in the metadata
    # that match keys in the format for the friendlier name that we have set
    # in the format value
    if isinstance(metadata, dict):
        if humanizable(metadata, includes_filename):
            format = PARAMS_FORMATS[lookup_key]
        else:
            format = PARAMS_FORMAT_CURRENT

        return {
            format[key]: metadata[key]
            for key in format.keys()
            if key in metadata.keys() and metadata[key]
        }

    raise TypeError("Can only humanize parameter lists or dictionaries")

```

`apps/stable_diffusion/web/utils/metadata/png_metadata.py`:

```py
import re
from pathlib import Path
from apps.stable_diffusion.web.ui.utils import (
    get_custom_model_pathfile,
    scheduler_list,
    predefined_models,
)

re_param_code = r'\s*([\w ]+):\s*("(?:\\"[^,]|\\"|\\|[^\"])+"|[^,]*)(?:,|$)'
re_param = re.compile(re_param_code)
re_imagesize = re.compile(r"^(\d+)x(\d+)$")


def parse_generation_parameters(x: str):
    res = {}
    prompt = ""
    negative_prompt = ""
    done_with_prompt = False

    *lines, lastline = x.strip().split("\n")
    if len(re_param.findall(lastline)) < 3:
        lines.append(lastline)
        lastline = ""

    for i, line in enumerate(lines):
        line = line.strip()
        if line.startswith("Negative prompt:"):
            done_with_prompt = True
            line = line[16:].strip()

        if done_with_prompt:
            negative_prompt += ("" if negative_prompt == "" else "\n") + line
        else:
            prompt += ("" if prompt == "" else "\n") + line

    res["Prompt"] = prompt
    res["Negative prompt"] = negative_prompt

    for k, v in re_param.findall(lastline):
        v = v[1:-1] if v[0] == '"' and v[-1] == '"' else v
        m = re_imagesize.match(v)
        if m is not None:
            res[k + "-1"] = m.group(1)
            res[k + "-2"] = m.group(2)
        else:
            res[k] = v

    # Missing CLIP skip means it was set to 1 (the default)
    if "Clip skip" not in res:
        res["Clip skip"] = "1"

    hypernet = res.get("Hypernet", None)
    if hypernet is not None:
        res[
            "Prompt"
        ] += f"""<hypernet:{hypernet}:{res.get("Hypernet strength", "1.0")}>"""

    if "Hires resize-1" not in res:
        res["Hires resize-1"] = 0
        res["Hires resize-2"] = 0

    return res


def try_find_model_base_from_png_metadata(
    file: str, folder: str = "models"
) -> str:
    custom = ""

    # Remove extension from file info
    if file.endswith(".safetensors") or file.endswith(".ckpt"):
        file = Path(file).stem
    # Check for the file name match with one of the local ckpt or safetensors files
    if Path(get_custom_model_pathfile(file + ".ckpt", folder)).is_file():
        custom = file + ".ckpt"
    if Path(
        get_custom_model_pathfile(file + ".safetensors", folder)
    ).is_file():
        custom = file + ".safetensors"

    return custom


def find_model_from_png_metadata(
    key: str, metadata: dict[str, str | int]
) -> tuple[str, str]:
    png_hf_id = ""
    png_custom = ""

    if key in metadata:
        model_file = metadata[key]
        png_custom = try_find_model_base_from_png_metadata(model_file)
        # Check for a model match with one of the default model list (ex: "Linaqruf/anything-v3.0")
        if model_file in predefined_models:
            png_custom = model_file
        # If nothing had matched, check vendor/hf_model_id
        if not png_custom and model_file.count("/"):
            png_hf_id = model_file
        # No matching model was found
        if not png_custom and not png_hf_id:
            print(
                "Import PNG info: Unable to find a matching model for %s"
                % model_file
            )

    return png_custom, png_hf_id


def find_vae_from_png_metadata(
    key: str, metadata: dict[str, str | int]
) -> str:
    vae_custom = ""

    if key in metadata:
        vae_file = metadata[key]
        vae_custom = try_find_model_base_from_png_metadata(vae_file, "vae")

    # VAE input is optional, should not print or throw an error if missing

    return vae_custom


def find_lora_from_png_metadata(
    key: str, metadata: dict[str, str | int]
) -> tuple[str, str]:
    lora_hf_id = ""
    lora_custom = ""

    if key in metadata:
        lora_file = metadata[key]
        lora_custom = try_find_model_base_from_png_metadata(lora_file, "lora")
        # If nothing had matched, check vendor/hf_model_id
        if not lora_custom and lora_file.count("/"):
            lora_hf_id = lora_file

    # LoRA input is optional, should not print or throw an error if missing

    return lora_custom, lora_hf_id


def import_png_metadata(
    pil_data,
    prompt,
    negative_prompt,
    steps,
    sampler,
    cfg_scale,
    seed,
    width,
    height,
    custom_model,
    custom_lora,
    hf_lora_id,
    custom_vae,
):
    try:
        png_info = pil_data.info["parameters"]
        metadata = parse_generation_parameters(png_info)

        (png_custom_model, png_hf_model_id) = find_model_from_png_metadata(
            "Model", metadata
        )
        (lora_custom_model, lora_hf_model_id) = find_lora_from_png_metadata(
            "LoRA", metadata
        )
        vae_custom_model = find_vae_from_png_metadata("VAE", metadata)

        negative_prompt = metadata["Negative prompt"]
        steps = int(metadata["Steps"])
        cfg_scale = float(metadata["CFG scale"])
        seed = int(metadata["Seed"])
        width = float(metadata["Size-1"])
        height = float(metadata["Size-2"])

        if "Model" in metadata and png_custom_model:
            custom_model = png_custom_model
        elif "Model" in metadata and png_hf_model_id:
            custom_model = png_hf_model_id

        if "LoRA" in metadata and lora_custom_model:
            custom_lora = lora_custom_model
            hf_lora_id = ""
        if "LoRA" in metadata and lora_hf_model_id:
            custom_lora = "None"
            hf_lora_id = lora_hf_model_id

        if "VAE" in metadata and vae_custom_model:
            custom_vae = vae_custom_model

        if "Prompt" in metadata:
            prompt = metadata["Prompt"]
        if "Sampler" in metadata:
            if metadata["Sampler"] in scheduler_list:
                sampler = metadata["Sampler"]
            else:
                print(
                    "Import PNG info: Unable to find a scheduler for %s"
                    % metadata["Sampler"]
                )

    except Exception as ex:
        if pil_data and pil_data.info.get("parameters"):
            print("import_png_metadata failed with %s" % ex)
        pass

    return (
        None,
        prompt,
        negative_prompt,
        steps,
        sampler,
        cfg_scale,
        seed,
        width,
        height,
        custom_model,
        custom_lora,
        hf_lora_id,
        custom_vae,
    )

```

`apps/stable_diffusion/web/utils/tmp_configs.py`:

```py
import os
import shutil
from time import time

shark_tmp = os.path.join(os.getcwd(), "shark_tmp/")


def clear_tmp_mlir():
    cleanup_start = time()
    print(
        "Clearing .mlir temporary files from a prior run. This may take some time..."
    )
    mlir_files = [
        filename
        for filename in os.listdir(shark_tmp)
        if os.path.isfile(os.path.join(shark_tmp, filename))
        and filename.endswith(".mlir")
    ]
    for filename in mlir_files:
        os.remove(shark_tmp + filename)
    print(
        f"Clearing .mlir temporary files took {time() - cleanup_start:.4f} seconds."
    )


def clear_tmp_imgs():
    # tell gradio to use a directory under shark_tmp for its temporary
    # image files unless somewhere else has been set
    if "GRADIO_TEMP_DIR" not in os.environ:
        os.environ["GRADIO_TEMP_DIR"] = os.path.join(shark_tmp, "gradio")

    print(
        f"gradio temporary image cache located at {os.environ['GRADIO_TEMP_DIR']}. "
        + "You may change this by setting the GRADIO_TEMP_DIR environment variable."
    )

    # Clear all gradio tmp images from the last session
    if os.path.exists(os.environ["GRADIO_TEMP_DIR"]):
        cleanup_start = time()
        print(
            "Clearing gradio UI temporary image files from a prior run. This may take some time..."
        )
        shutil.rmtree(os.environ["GRADIO_TEMP_DIR"], ignore_errors=True)
        print(
            f"Clearing gradio UI temporary image files took {time() - cleanup_start:.4f} seconds."
        )

    # older SHARK versions had to workaround gradio bugs and stored things differently
    else:
        image_files = [
            filename
            for filename in os.listdir(shark_tmp)
            if os.path.isfile(os.path.join(shark_tmp, filename))
            and filename.startswith("tmp")
            and filename.endswith(".png")
        ]
        if len(image_files) > 0:
            print(
                "Clearing temporary image files of a prior run of a previous SHARK version. This may take some time..."
            )
            cleanup_start = time()
            for filename in image_files:
                os.remove(shark_tmp + filename)
            print(
                f"Clearing temporary image files took {time() - cleanup_start:.4f} seconds."
            )
        else:
            print("No temporary images files to clear.")


def config_tmp():
    # create shark_tmp if it does not exist
    if not os.path.exists(shark_tmp):
        os.mkdir(shark_tmp)

    clear_tmp_mlir()
    clear_tmp_imgs()

```

`benchmarks/hf_model_benchmark.py`:

```py
import torch
from shark.parser import parser
from benchmarks.hf_transformer import SharkHFBenchmarkRunner

parser.add_argument(
    "--model_name",
    type=str,
    required=True,
    help='Specifies name of HF model to benchmark. (For exmaple "microsoft/MiniLM-L12-H384-uncased"',
)
load_args, unknown = parser.parse_known_args()

if __name__ == "__main__":
    model_name = load_args.model_name
    test_input = torch.randint(2, (1, 128))
    shark_module = SharkHFBenchmarkRunner(
        model_name, (test_input,), jit_trace=True
    )
    shark_module.benchmark_c()
    shark_module.benchmark_python((test_input,))
    shark_module.benchmark_torch(test_input)
    shark_module.benchmark_onnx(test_input)

```

`benchmarks/hf_transformer.py`:

```py
import torch
from shark.shark_benchmark_runner import SharkBenchmarkRunner
from shark.parser import shark_args
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from onnxruntime.transformers.benchmark import (
    run_pytorch,
    run_tensorflow,
    run_onnxruntime,
)
from onnxruntime.transformers.huggingface_models import MODELS
from onnxruntime.transformers.benchmark_helper import ConfigModifier, Precision
import os
import psutil


class OnnxFusionOptions(object):
    def __init__(self):
        self.disable_gelu = False
        self.disable_layer_norm = False
        self.disable_attention = False
        self.disable_skip_layer_norm = False
        self.disable_embed_layer_norm = False
        self.disable_bias_skip_layer_norm = False
        self.disable_bias_gelu = False
        self.enable_gelu_approximation = False
        self.use_mask_index = False
        self.no_attention_mask = False


class HuggingFaceLanguage(torch.nn.Module):
    def __init__(self, hf_model_name):
        super().__init__()
        self.model = AutoModelForSequenceClassification.from_pretrained(
            hf_model_name,  # The pretrained model.
            num_labels=2,  # The number of output labels--2 for binary classification.
            output_attentions=False,  # Whether the model returns attentions weights.
            output_hidden_states=False,  # Whether the model returns all hidden-states.
            torchscript=True,
        )

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


class SharkHFBenchmarkRunner(SharkBenchmarkRunner):
    # SharkRunner derived class with Benchmarking capabilities.
    def __init__(
        self,
        model_name: str,
        input: tuple,
        dynamic: bool = False,
        device: str = None,
        jit_trace: bool = False,
        from_aot: bool = False,
        frontend: str = "torch",
    ):
        self.device = device if device is not None else shark_args.device
        if self.device == "gpu":
            raise ValueError(
                "Currently GPU Benchmarking is not supported due to OOM from ORT."
            )
        self.model_name = model_name
        model = HuggingFaceLanguage(model_name)
        SharkBenchmarkRunner.__init__(
            self,
            model,
            input,
            dynamic,
            self.device,
            jit_trace,
            from_aot,
            frontend,
        )

    def benchmark_torch(self, inputs):
        use_gpu = self.device == "gpu"
        # Set set the model's layer number to automatic.
        config_modifier = ConfigModifier(None)
        num_threads = psutil.cpu_count(logical=False)
        batch_sizes = [inputs.shape[0]]
        sequence_lengths = [inputs.shape[-1]]
        cache_dir = os.path.join(".", "cache_models")
        verbose = False
        result = run_pytorch(
            use_gpu,
            [self.model_name],
            None,
            config_modifier,
            Precision.FLOAT32,
            num_threads,
            batch_sizes,
            sequence_lengths,
            shark_args.num_iterations,
            False,
            cache_dir,
            verbose,
        )
        print(
            f"ONNX Pytorch-benchmark:{result[0]['QPS']} iter/second, Total Iterations:{shark_args.num_iterations}"
        )

    # TODO: Currently non-functional due to TF runtime error. There might be some issue with, initializing TF.
    def benchmark_tf(self, inputs):
        use_gpu = self.device == "gpu"
        # Set set the model's layer number to automatic.
        config_modifier = ConfigModifier(None)
        num_threads = psutil.cpu_count(logical=False)
        batch_sizes = [inputs.shape[0]]
        sequence_lengths = [inputs.shape[-1]]
        cache_dir = os.path.join(".", "cache_models")
        verbose = False
        result = run_tensorflow(
            use_gpu,
            [self.model_name],
            None,
            config_modifier,
            Precision.FLOAT32,
            num_threads,
            batch_sizes,
            sequence_lengths,
            shark_args.num_iterations,
            cache_dir,
            verbose,
        )
        print(
            f"ONNX TF-benchmark:{result[0]['QPS']} iter/second, Total Iterations:{shark_args.num_iterations}"
        )

    def benchmark_onnx(self, inputs):
        if self.model_name not in MODELS:
            print(
                f"{self.model_name} is currently not supported in ORT's HF. Check \
https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/huggingface_models.py \
for currently supported models. Exiting benchmark ONNX."
            )
            return
        use_gpu = self.device == "gpu"
        num_threads = psutil.cpu_count(logical=False)
        batch_sizes = [inputs.shape[0]]
        sequence_lengths = [inputs.shape[-1]]
        cache_dir = os.path.join(".", "cache_models")
        onnx_dir = os.path.join(".", "onnx_models")
        verbose = False
        input_counts = [1]
        optimize_onnx = True
        validate_onnx = False
        disable_ort_io_binding = False
        use_raw_attention_mask = True
        model_fusion_statistics = {}
        overwrite = False
        model_source = "pt"  # Either "pt" or "tf"
        provider = None
        config_modifier = ConfigModifier(None)
        onnx_args = OnnxFusionOptions()
        result = run_onnxruntime(
            use_gpu,
            provider,
            [self.model_name],
            None,
            config_modifier,
            Precision.FLOAT32,
            num_threads,
            batch_sizes,
            sequence_lengths,
            shark_args.num_iterations,
            input_counts,
            optimize_onnx,
            validate_onnx,
            cache_dir,
            onnx_dir,
            verbose,
            overwrite,
            disable_ort_io_binding,
            use_raw_attention_mask,
            model_fusion_statistics,
            model_source,
            onnx_args,
        )
        print(
            f"ONNX ORT-benchmark:{result[0]['QPS']} iter/second, Total Iterations:{shark_args.num_iterations}"
        )

```

`benchmarks/tests/test_benchmark.py`:

```py
from shark.shark_inference import SharkInference
from shark.iree_utils._common import check_device_drivers

import torch
import tensorflow as tf
import numpy as np
import torchvision.models as models
from transformers import (
    AutoModelForSequenceClassification,
    BertTokenizer,
    TFBertModel,
)
import importlib
import pytest
import unittest

torch.manual_seed(0)
gpus = tf.config.experimental.list_physical_devices("GPU")
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

##################### Tensorflow Hugging Face LM Models ###################################
MAX_SEQUENCE_LENGTH = 512
BATCH_SIZE = 1

# Create a set of 2-dimensional inputs
tf_bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
]


class TFHuggingFaceLanguage(tf.Module):
    def __init__(self, hf_model_name):
        super(TFHuggingFaceLanguage, self).__init__()
        # Create a BERT trainer with the created network.
        self.m = TFBertModel.from_pretrained(hf_model_name, from_pt=True)

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m.predict = lambda x, y, z: self.m.call(
            input_ids=x, attention_mask=y, token_type_ids=z, training=False
        )

    @tf.function(input_signature=tf_bert_input, jit_compile=True)
    def forward(self, input_ids, attention_mask, token_type_ids):
        return self.m.predict(input_ids, attention_mask, token_type_ids)


def get_TFhf_model(name):
    model = TFHuggingFaceLanguage(name)
    tokenizer = BertTokenizer.from_pretrained(name)
    text = "Replace me by any text you'd like."
    encoded_input = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    for key in encoded_input:
        encoded_input[key] = tf.expand_dims(
            tf.convert_to_tensor(encoded_input[key]), 0
        )
    test_input = (
        encoded_input["input_ids"],
        encoded_input["attention_mask"],
        encoded_input["token_type_ids"],
    )
    actual_out = model.forward(*test_input)
    return model, test_input, actual_out


##################### Hugging Face LM Models ###################################


class HuggingFaceLanguage(torch.nn.Module):
    def __init__(self, hf_model_name):
        super().__init__()
        self.model = AutoModelForSequenceClassification.from_pretrained(
            hf_model_name,  # The pretrained model.
            num_labels=2,  # The number of output labels--2 for binary classification.
            output_attentions=False,  # Whether the model returns attentions weights.
            output_hidden_states=False,  # Whether the model returns all hidden-states.
            torchscript=True,
        )

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


def get_hf_model(name):
    model = HuggingFaceLanguage(name)
    # TODO: Currently the test input is set to (1,128)
    test_input = torch.randint(2, (1, 128))
    actual_out = model(test_input)
    return model, test_input, actual_out


################################################################################

##################### Torch Vision Models    ###################################


class VisionModule(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.train(False)

    def forward(self, input):
        return self.model.forward(input)


def get_vision_model(torch_model):
    model = VisionModule(torch_model)
    # TODO: Currently the test input is set to (1,128)
    test_input = torch.randn(1, 3, 224, 224)
    actual_out = model(test_input)
    return model, test_input, actual_out


#############################   Benchmark Tests ####################################

pytest_benchmark_param = pytest.mark.parametrize(
    ("dynamic", "device"),
    [
        pytest.param(False, "cpu"),
        # TODO: Language models are failing for dynamic case..
        pytest.param(True, "cpu", marks=pytest.mark.skip),
        pytest.param(
            False,
            "cuda",
            marks=pytest.mark.skipif(
                check_device_drivers("cuda"), reason="nvidia-smi not found"
            ),
        ),
        pytest.param(True, "cuda", marks=pytest.mark.skip),
        pytest.param(
            False,
            "vulkan",
            marks=pytest.mark.skipif(
                check_device_drivers("vulkan"),
                reason="vulkaninfo not found, install from https://github.com/KhronosGroup/MoltenVK/releases",
            ),
        ),
        pytest.param(
            True,
            "vulkan",
            marks=pytest.mark.skipif(
                check_device_drivers("vulkan"),
                reason="vulkaninfo not found, install from https://github.com/KhronosGroup/MoltenVK/releases",
            ),
        ),
    ],
)


@pytest.mark.skipif(
    importlib.util.find_spec("iree.tools") is None,
    reason="Cannot find tools to import TF",
)
@pytest_benchmark_param
def test_bench_minilm_torch(dynamic, device):
    model, test_input, act_out = get_hf_model(
        "microsoft/MiniLM-L12-H384-uncased"
    )
    shark_module = SharkInference(
        model,
        (test_input,),
        device=device,
        dynamic=dynamic,
        jit_trace=True,
        benchmark_mode=True,
    )
    try:
        # If becnhmarking succesful, assert success/True.
        shark_module.compile()
        shark_module.benchmark_all((test_input,))
        assert True
    except Exception as e:
        # If anything happen during benchmarking, assert False/failure.
        assert False


@pytest.mark.skipif(
    importlib.util.find_spec("iree.tools") is None,
    reason="Cannot find tools to import TF",
)
@pytest_benchmark_param
def test_bench_distilbert(dynamic, device):
    model, test_input, act_out = get_TFhf_model("distilbert-base-uncased")
    shark_module = SharkInference(
        model,
        test_input,
        device=device,
        dynamic=dynamic,
        jit_trace=True,
        benchmark_mode=True,
    )
    try:
        # If becnhmarking succesful, assert success/True.
        shark_module.set_frontend("tensorflow")
        shark_module.compile()
        shark_module.benchmark_all(test_input)
        assert True
    except Exception as e:
        # If anything happen during benchmarking, assert False/failure.
        assert False


@pytest.mark.skip(reason="XLM Roberta too large to test.")
@pytest_benchmark_param
def test_bench_xlm_roberta(dynamic, device):
    model, test_input, act_out = get_TFhf_model("xlm-roberta-base")
    shark_module = SharkInference(
        model,
        test_input,
        device=device,
        dynamic=dynamic,
        jit_trace=True,
        benchmark_mode=True,
    )
    try:
        # If becnhmarking succesful, assert success/True.
        shark_module.set_frontend("tensorflow")
        shark_module.compile()
        shark_module.benchmark_all(test_input)
        assert True
    except Exception as e:
        # If anything happen during benchmarking, assert False/failure.
        assert False

```

`benchmarks/tests/test_hf_benchmark.py`:

```py
import torch
from benchmarks.hf_transformer import SharkHFBenchmarkRunner
import importlib
import pytest

torch.manual_seed(0)

############################# HF Benchmark Tests ####################################

# Test running benchmark module without failing.
pytest_benchmark_param = pytest.mark.parametrize(
    ("dynamic", "device"),
    [
        pytest.param(False, "cpu"),
        # TODO: Language models are failing for dynamic case..
        pytest.param(True, "cpu", marks=pytest.mark.skip),
    ],
)


@pytest.mark.skipif(
    importlib.util.find_spec("onnxruntime") is None,
    reason="Cannot find ONNXRUNTIME.",
)
@pytest_benchmark_param
def test_HFbench_minilm_torch(dynamic, device):
    model_name = "bert-base-uncased"
    test_input = torch.randint(2, (1, 128))
    try:
        shark_module = SharkHFBenchmarkRunner(
            model_name,
            (test_input,),
            jit_trace=True,
            dynamic=dynamic,
            device=device,
        )
        shark_module.benchmark_c()
        shark_module.benchmark_python((test_input,))
        shark_module.benchmark_torch(test_input)
        shark_module.benchmark_onnx(test_input)
        # If becnhmarking succesful, assert success/True.
        assert True
    except Exception as e:
        # If anything happen during benchmarking, assert False/failure.
        assert False

```

`build_tools/docker/Dockerfile-ubuntu-22.04`:

```04
ARG IMAGE_NAME
FROM ${IMAGE_NAME}:12.2.0-runtime-ubuntu22.04 as base

ENV NV_CUDA_LIB_VERSION "12.2.0-1"

FROM base as base-amd64

ENV NV_CUDA_CUDART_DEV_VERSION 12.2.53-1
ENV NV_NVML_DEV_VERSION 12.2.81-1
ENV NV_LIBCUSPARSE_DEV_VERSION 12.1.1.53-1
ENV NV_LIBNPP_DEV_VERSION 12.1.1.14-1
ENV NV_LIBNPP_DEV_PACKAGE libnpp-dev-12-2=${NV_LIBNPP_DEV_VERSION}

ENV NV_LIBCUBLAS_DEV_VERSION 12.2.1.16-1
ENV NV_LIBCUBLAS_DEV_PACKAGE_NAME libcublas-dev-12-2
ENV NV_LIBCUBLAS_DEV_PACKAGE ${NV_LIBCUBLAS_DEV_PACKAGE_NAME}=${NV_LIBCUBLAS_DEV_VERSION}

ENV NV_CUDA_NSIGHT_COMPUTE_VERSION 12.2.0-1
ENV NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE cuda-nsight-compute-12-2=${NV_CUDA_NSIGHT_COMPUTE_VERSION}

ENV NV_NVPROF_VERSION 12.2.60-1
ENV NV_NVPROF_DEV_PACKAGE cuda-nvprof-12-2=${NV_NVPROF_VERSION}
FROM base as base-arm64

ENV NV_CUDA_CUDART_DEV_VERSION 12.2.53-1
ENV NV_NVML_DEV_VERSION 12.2.81-1
ENV NV_LIBCUSPARSE_DEV_VERSION 12.1.1.53-1
ENV NV_LIBNPP_DEV_VERSION 12.1.1.14-1
ENV NV_LIBNPP_DEV_PACKAGE libnpp-dev-12-2=${NV_LIBNPP_DEV_VERSION}

ENV NV_LIBCUBLAS_DEV_PACKAGE_NAME libcublas-dev-12-2
ENV NV_LIBCUBLAS_DEV_VERSION 12.2.1.16-1
ENV NV_LIBCUBLAS_DEV_PACKAGE ${NV_LIBCUBLAS_DEV_PACKAGE_NAME}=${NV_LIBCUBLAS_DEV_VERSION}

ENV NV_CUDA_NSIGHT_COMPUTE_VERSION 12.2.0-1
ENV NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE cuda-nsight-compute-12-2=${NV_CUDA_NSIGHT_COMPUTE_VERSION}

FROM base-${TARGETARCH}

ARG TARGETARCH

LABEL maintainer "SHARK<stdin@nod.com>"

# Register the ROCM package repository, and install rocm-dev package
ARG ROCM_VERSION=5.6
ARG AMDGPU_VERSION=5.6

ARG APT_PREF
RUN echo "$APT_PREF" > /etc/apt/preferences.d/rocm-pin-600
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends ca-certificates curl libnuma-dev gnupg \
  && curl -sL https://repo.radeon.com/rocm/rocm.gpg.key | apt-key add - \
  && printf "deb [arch=amd64] https://repo.radeon.com/rocm/apt/$ROCM_VERSION/ jammy main" | tee /etc/apt/sources.list.d/rocm.list \
  && printf "deb [arch=amd64] https://repo.radeon.com/amdgpu/$AMDGPU_VERSION/ubuntu jammy main" | tee /etc/apt/sources.list.d/amdgpu.list \
  && apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
  sudo \
  libelf1 \
  kmod \
  file \
  python3 \
  python3-pip \
  rocm-dev \
  rocm-libs \
  rocm-hip-libraries \
  build-essential && \
  apt-get clean && \
  rm -rf /var/lib/apt/lists/*

RUN  groupadd -g 109 render

RUN apt-get update && apt-get install -y --no-install-recommends \
    cuda-cudart-dev-12-2=${NV_CUDA_CUDART_DEV_VERSION} \
    cuda-command-line-tools-12-2=${NV_CUDA_LIB_VERSION} \
    cuda-minimal-build-12-2=${NV_CUDA_LIB_VERSION} \
    cuda-libraries-dev-12-2=${NV_CUDA_LIB_VERSION} \
    cuda-nvml-dev-12-2=${NV_NVML_DEV_VERSION} \
    ${NV_NVPROF_DEV_PACKAGE} \
    ${NV_LIBNPP_DEV_PACKAGE} \
    libcusparse-dev-12-2=${NV_LIBCUSPARSE_DEV_VERSION} \
    ${NV_LIBCUBLAS_DEV_PACKAGE} \
    ${NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE} \
    && rm -rf /var/lib/apt/lists/*

RUN apt install rocm-hip-libraries

# Keep apt from auto upgrading the cublas and nccl packages. See https://gitlab.com/nvidia/container-images/cuda/-/issues/88
RUN apt-mark hold ${NV_LIBCUBLAS_DEV_PACKAGE_NAME}
ENV LIBRARY_PATH /usr/local/cuda/lib64/stubs


```

`build_tools/docker/README.md`:

```md
On your host install your Nvidia or AMD gpu drivers. 

**HOST Setup**

*Ubuntu 23.04 Nvidia*
```
sudo ubuntu-drivers install
```

Install [docker](https://docs.docker.com/engine/install/ubuntu/) and the post-install to run as a [user](https://docs.docker.com/engine/install/linux-postinstall/)

Install Nvidia [Container and register it](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html). In Ubuntu 23.04 systems follow [this](https://github.com/NVIDIA/nvidia-container-toolkit/issues/72#issuecomment-1584574298)


Build docker with :

```
docker build . -f Dockerfile-ubuntu-22.04 -t shark/dev-22.04:5.6 --build-arg=ROCM_VERSION=5.6 --build-arg=AMDGPU_VERSION=5.6 --build-arg=APT_PREF="Package: *\nPin: release o=repo.radeon.com\nPin-Priority: 600" --build-arg=IMAGE_NAME=nvidia/cuda --build-arg=TARGETARCH=amd64
```

Run with:

*CPU*

```
docker run  -it docker.io/shark/dev-22.04:5.6
```

*Nvidia GPU*

```
docker run --rm -it --gpus all docker.io/shark/dev-22.04:5.6
```

*AMD GPUs*

```
docker run --device /dev/kfd --device /dev/dri  docker.io/shark/dev-22.04:5.6
```

More AMD instructions are [here](https://docs.amd.com/en/latest/deploy/docker.html)

```

`build_tools/image_comparison.py`:

```py
import argparse
from PIL import Image
import numpy as np

import requests
import shutil
import os
import subprocess

parser = argparse.ArgumentParser()

parser.add_argument("-n", "--newfile")
parser.add_argument(
    "-g",
    "--golden_url",
    default="https://storage.googleapis.com/shark_tank/testdata/cyberpunk_fores_42_0_230119_021148.png",
)


def get_image(url, local_filename):
    res = requests.get(url, stream=True)
    if res.status_code == 200:
        with open(local_filename, "wb") as f:
            shutil.copyfileobj(res.raw, f)


def compare_images(new_filename, golden_filename, upload=False):
    new = np.array(Image.open(new_filename)) / 255.0
    golden = np.array(Image.open(golden_filename)) / 255.0
    diff = np.abs(new - golden)
    mean = np.mean(diff)
    if mean > 0.1:
        if os.name != "nt" and upload == True:
            subprocess.run(
                [
                    "gsutil",
                    "cp",
                    new_filename,
                    "gs://shark_tank/testdata/builder/",
                ]
            )
        raise AssertionError("new and golden not close")
    else:
        print("SUCCESS")


if __name__ == "__main__":
    args = parser.parse_args()
    tempfile_name = os.path.join(os.getcwd(), "golden.png")
    get_image(args.golden_url, tempfile_name)
    compare_images(args.newfile, tempfile_name)

```

`build_tools/populate_sharktank_ci.sh`:

```sh
#!/bin/bash

IMPORTER=1 BENCHMARK=1 NO_BREVITAS=1 ./setup_venv.sh
source $GITHUB_WORKSPACE/shark.venv/bin/activate
python build_tools/stable_diffusion_testing.py --gen
python tank/generate_sharktank.py

```

`build_tools/scrape_releases.py`:

```py
"""Scrapes the github releases API to generate a static pip-install-able releases page.

See https://github.com/llvm/torch-mlir/issues/1374
"""
import argparse
import json

import requests

# Parse arguments
parser = argparse.ArgumentParser()
parser.add_argument("owner", type=str)
parser.add_argument("repo", type=str)
args = parser.parse_args()

# Get releases
response = requests.get(
    f"https://api.github.com/repos/{args.owner}/{args.repo}/releases"
)
body = json.loads(response.content)

# Parse releases
releases = []
for row in body:
    for asset in row["assets"]:
        releases.append((asset["name"], asset["browser_download_url"]))

# Output HTML
html = """<!DOCTYPE html>
<html>
  <body>
"""
for name, url in releases:
    html += f"    <a href='{url}'>{name}</a><br />\n"
html += """  </body>
</html>"""
print(html)

```

`build_tools/stable_diffusion_testing.py`:

```py
import os
from sys import executable
import subprocess
from apps.stable_diffusion.src.utils.resources import (
    get_json_file,
)
from datetime import datetime as dt
from shark.shark_downloader import download_public_file
from image_comparison import compare_images
import argparse
from glob import glob
import shutil
import requests

model_config_dicts = get_json_file(
    os.path.join(
        os.getcwd(),
        "apps/stable_diffusion/src/utils/resources/model_config.json",
    )
)


def parse_sd_out(filename, command, device, use_tune, model_name, import_mlir):
    with open(filename, "r+") as f:
        lines = f.readlines()
    metrics = {}
    vals_to_read = [
        "Clip Inference time",
        "Average step",
        "VAE Inference time",
        "Total image generation",
    ]
    for line in lines:
        for val in vals_to_read:
            if val in line:
                metrics[val] = line.split(" ")[-1].strip("\n")

    metrics["Average step"] = metrics["Average step"].strip("ms/it")
    metrics["Total image generation"] = metrics[
        "Total image generation"
    ].strip("sec")
    metrics["device"] = device
    metrics["use_tune"] = use_tune
    metrics["model_name"] = model_name
    metrics["import_mlir"] = import_mlir
    metrics["command"] = command
    return metrics


def get_inpaint_inputs():
    os.mkdir("./test_images/inputs")
    img_url = (
        "https://huggingface.co/datasets/diffusers/test-arrays/resolve"
        "/main/stable_diffusion_inpaint/input_bench_image.png"
    )
    mask_url = (
        "https://huggingface.co/datasets/diffusers/test-arrays/resolve"
        "/main/stable_diffusion_inpaint/input_bench_mask.png"
    )
    img = requests.get(img_url)
    mask = requests.get(mask_url)
    open("./test_images/inputs/image.png", "wb").write(img.content)
    open("./test_images/inputs/mask.png", "wb").write(mask.content)


def test_loop(
    device="vulkan",
    beta=False,
    extra_flags=[],
    upload_bool=True,
    exit_on_fail=True,
    do_gen=False,
):
    # Get golden values from tank
    shutil.rmtree("./test_images", ignore_errors=True)
    model_metrics = []
    os.mkdir("./test_images")
    os.mkdir("./test_images/golden")
    get_inpaint_inputs()
    hf_model_names = model_config_dicts[0].values()
    tuned_options = [
        "--no-use_tuned",
        "--use_tuned",
    ]
    import_options = ["--import_mlir", "--no-import_mlir"]
    prompt_text = "--prompt=cyberpunk forest by Salvador Dali"
    inpaint_prompt_text = "--prompt=Face of a yellow cat, high resolution, sitting on a park bench"
    if os.name == "nt":
        prompt_text = '--prompt="cyberpunk forest by Salvador Dali"'
        inpaint_prompt_text = '--prompt="Face of a yellow cat, high resolution, sitting on a park bench"'
    if beta:
        extra_flags.append("--beta_models=True")
    extra_flags.append("--no-progress_bar")
    if do_gen:
        extra_flags.append("--import_debug")
    to_skip = [
        "Linaqruf/anything-v3.0",
        "prompthero/openjourney",
        "wavymulder/Analog-Diffusion",
        "dreamlike-art/dreamlike-diffusion-1.0",
    ]
    counter = 0
    for import_opt in import_options:
        for model_name in hf_model_names:
            if model_name in to_skip:
                continue
            for use_tune in tuned_options:
                if (
                    model_name == "stabilityai/stable-diffusion-2-1"
                    and use_tune == tuned_options[0]
                ):
                    continue
                elif (
                    model_name == "stabilityai/stable-diffusion-2-1-base"
                    and use_tune == tuned_options[1]
                ):
                    continue
                elif use_tune == tuned_options[1]:
                    continue
                command = (
                    [
                        executable,  # executable is the python from the venv used to run this
                        "apps/stable_diffusion/scripts/txt2img.py",
                        "--device=" + device,
                        prompt_text,
                        "--negative_prompts=" + '""',
                        "--seed=42",
                        import_opt,
                        "--output_dir="
                        + os.path.join(os.getcwd(), "test_images", model_name),
                        "--hf_model_id=" + model_name,
                        use_tune,
                    ]
                    if "inpainting" not in model_name
                    else [
                        executable,
                        "apps/stable_diffusion/scripts/inpaint.py",
                        "--device=" + device,
                        inpaint_prompt_text,
                        "--negative_prompts=" + '""',
                        "--img_path=./test_images/inputs/image.png",
                        "--mask_path=./test_images/inputs/mask.png",
                        "--seed=42",
                        "--import_mlir",
                        "--output_dir="
                        + os.path.join(os.getcwd(), "test_images", model_name),
                        "--hf_model_id=" + model_name,
                        use_tune,
                    ]
                )
                command += extra_flags
                if os.name == "nt":
                    command = " ".join(command)
                dumpfile_name = "_".join(model_name.split("/")) + ".txt"
                dumpfile_name = os.path.join(os.getcwd(), dumpfile_name)
                with open(dumpfile_name, "w+") as f:
                    generated_image = not subprocess.call(
                        command,
                        stdout=f,
                        stderr=f,
                    )
                if os.name != "nt":
                    command = " ".join(command)
                if generated_image:
                    model_metrics.append(
                        parse_sd_out(
                            dumpfile_name,
                            command,
                            device,
                            use_tune,
                            model_name,
                            import_opt,
                        )
                    )
                    print(command)
                    print("Successfully generated image")
                    os.makedirs(
                        "./test_images/golden/" + model_name, exist_ok=True
                    )
                    download_public_file(
                        "gs://shark_tank/testdata/golden/" + model_name,
                        "./test_images/golden/" + model_name,
                    )
                    test_file_path = os.path.join(
                        os.getcwd(),
                        "test_images",
                        model_name,
                        "generated_imgs",
                        dt.now().strftime("%Y%m%d"),
                        "*.png",
                    )
                    test_file = glob(test_file_path)[0]

                    golden_path = (
                        "./test_images/golden/" + model_name + "/*.png"
                    )
                    golden_file = glob(golden_path)[0]
                    try:
                        compare_images(
                            test_file, golden_file, upload=upload_bool
                        )
                    except AssertionError as e:
                        print(e)
                        if exit_on_fail == True:
                            raise
                else:
                    print(command)
                    print("failed to generate image for this configuration")
                    with open(dumpfile_name, "r+") as f:
                        output = f.readlines()
                        print("\n".join(output))
                    exit(1)
                if os.name == "nt":
                    counter += 1
                    if counter % 2 == 0:
                        extra_flags.append(
                            "--iree_vulkan_target_triple=rdna2-unknown-windows"
                        )
                    else:
                        if counter != 1:
                            extra_flags.remove(
                                "--iree_vulkan_target_triple=rdna2-unknown-windows"
                            )
            if do_gen:
                prepare_artifacts()

    with open(os.path.join(os.getcwd(), "sd_testing_metrics.csv"), "w+") as f:
        header = "model_name;device;use_tune;import_opt;Clip Inference time(ms);Average Step (ms/it);VAE Inference time(ms);total image generation(s);command\n"
        f.write(header)
        for metric in model_metrics:
            output = [
                metric["model_name"],
                metric["device"],
                metric["use_tune"],
                metric["import_mlir"],
                metric["Clip Inference time"],
                metric["Average step"],
                metric["VAE Inference time"],
                metric["Total image generation"],
                metric["command"],
            ]
            f.write(";".join(output) + "\n")


def prepare_artifacts():
    gen_path = os.path.join(os.getcwd(), "gen_shark_tank")
    if not os.path.isdir(gen_path):
        os.mkdir(gen_path)
    for dirname in os.listdir(os.getcwd()):
        for modelname in ["clip", "unet", "vae"]:
            if modelname in dirname and "vmfb" not in dirname:
                if not os.path.isdir(os.path.join(gen_path, dirname)):
                    shutil.move(os.path.join(os.getcwd(), dirname), gen_path)
                    print(f"Moved dir: {dirname} to {gen_path}.")


parser = argparse.ArgumentParser()

parser.add_argument("-d", "--device", default="vulkan")
parser.add_argument(
    "-b", "--beta", action=argparse.BooleanOptionalAction, default=False
)
parser.add_argument("-e", "--extra_args", type=str, default=None)
parser.add_argument(
    "-u", "--upload", action=argparse.BooleanOptionalAction, default=True
)
parser.add_argument(
    "-x", "--exit_on_fail", action=argparse.BooleanOptionalAction, default=True
)
parser.add_argument(
    "-g", "--gen", action=argparse.BooleanOptionalAction, default=False
)

if __name__ == "__main__":
    args = parser.parse_args()
    print(args)
    extra_args = []
    if args.extra_args:
        for arg in args.extra_args.split(","):
            extra_args.append(arg)
    test_loop(
        args.device,
        args.beta,
        extra_args,
        args.upload,
        args.exit_on_fail,
        args.gen,
    )
    if args.gen:
        prepare_artifacts()

```

`build_tools/vicuna_testing.py`:

```py
import os
from sys import executable
import subprocess
from apps.language_models.scripts import vicuna


def test_loop():
    precisions = ["fp16", "int8", "int4"]
    devices = ["cpu"]
    for precision in precisions:
        for device in devices:
            model = vicuna.UnshardedVicuna(device=device, precision=precision)
            model.compile()
            del model

```

`conftest.py`:

```py
def pytest_addoption(parser):
    # Attaches SHARK command-line arguments to the pytest machinery.
    parser.addoption(
        "--benchmark",
        action="store",
        type=str,
        default=None,
        choices=("baseline", "native", "all"),
        help="Benchmarks specified engine(s) and writes bench_results.csv.",
    )
    parser.addoption(
        "--onnx_bench",
        action="store_true",
        default="False",
        help="Add ONNX benchmark results to pytest benchmarks.",
    )
    parser.addoption(
        "--tf32",
        action="store_true",
        default="False",
        help="Use TensorFloat-32 calculations.",
    )
    parser.addoption(
        "--save_repro",
        action="store_true",
        default="False",
        help="Pass option to save reproduction artifacts to SHARK/shark_tmp/test_case/",
    )
    parser.addoption(
        "--save_fails",
        action="store_true",
        default="False",
        help="Save reproduction artifacts for a test case only if it fails. Default is False.",
    )
    parser.addoption(
        "--ci",
        action="store_true",
        default="False",
        help="Enables uploading of reproduction artifacts upon test case failure during iree-compile or validation. Must be passed with --ci_sha option ",
    )
    parser.addoption(
        "--update_tank",
        action="store_true",
        default="False",
        help="Update local shark tank with latest artifacts if model artifact hash mismatched.",
    )
    parser.addoption(
        "--force_update_tank",
        action="store_true",
        default="False",
        help="Force-update local shark tank with artifacts from specified shark_tank URL (defaults to nightly).",
    )
    parser.addoption(
        "--ci_sha",
        action="store",
        default="None",
        help="Passes the github SHA of the CI workflow to include in google storage directory for reproduction artifacts.",
    )
    parser.addoption(
        "--local_tank_cache",
        action="store",
        default=None,
        help="Specify the directory in which all downloaded shark_tank artifacts will be cached.",
    )
    parser.addoption(
        "--tank_url",
        type=str,
        default="gs://shark_tank/nightly",
        help="URL to bucket from which to download SHARK tank artifacts. Default is gs://shark_tank/latest",
    )
    parser.addoption(
        "--tank_prefix",
        type=str,
        default=None,
        help="Prefix to gs://shark_tank/ model directories from which to download SHARK tank artifacts. Default is nightly.",
    )
    parser.addoption(
        "--benchmark_dispatches",
        default=None,
        help="Benchmark individual dispatch kernels produced by IREE compiler. Use 'All' for all, or specific dispatches e.g. '0 1 2 10'",
    )
    parser.addoption(
        "--dispatch_benchmarks_dir",
        default="./temp_dispatch_benchmarks",
        help="Directory in which dispatch benchmarks are saved.",
    )
    parser.addoption(
        "--batchsize",
        default=1,
        type=int,
        help="Batch size for the tested model.",
    )

```

`cpp/CMakeLists.txt`:

```txt
# Copyright 2022 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

cmake_minimum_required(VERSION 3.21...3.23)

#-------------------------------------------------------------------------------
# Project configuration
#-------------------------------------------------------------------------------

project(iree-samples C CXX)
set(CMAKE_C_STANDARD 11)
set(CMAKE_CXX_STANDARD 17)
set_property(GLOBAL PROPERTY USE_FOLDERS ON)

#-------------------------------------------------------------------------------
# Core project dependency
#-------------------------------------------------------------------------------

message(STATUS "Fetching core IREE repo (this may take a few minutes)...")
# Note: for log output, set -DFETCHCONTENT_QUIET=OFF,
# see https://gitlab.kitware.com/cmake/cmake/-/issues/18238#note_440475

include(FetchContent)

FetchContent_Declare(
  iree
  GIT_REPOSITORY https://github.com/nod-ai/srt.git
  GIT_TAG shark 
  GIT_SUBMODULES_RECURSE OFF
  GIT_SHALLOW OFF
  GIT_PROGRESS ON
  USES_TERMINAL_DOWNLOAD ON
)

# Extend module path to find MLIR CMake modules.
list(APPEND CMAKE_MODULE_PATH "${CMAKE_CURRENT_BINARY_DIR}/lib/cmake/mlir")

# Disable core project features not needed for these out of tree samples.
set(IREE_BUILD_TESTS OFF CACHE BOOL "" FORCE)
set(IREE_BUILD_SAMPLES OFF CACHE BOOL "" FORCE)

FetchContent_MakeAvailable(iree)
FetchContent_GetProperties(iree SOURCE_DIR IREE_SOURCE_DIR)

#-------------------------------------------------------------------------------
# Individual samples
#-------------------------------------------------------------------------------

add_subdirectory(vulkan_gui)

```

`cpp/README.md`:

```md
# SHARK C/C++ Samples

These C/C++ samples can be built using CMake. The samples depend on the main
SHARK-Runtime project's C/C++ sources, including both the runtime and the compiler. 

Individual samples may require additional dependencies. Watch CMake's output
for information about which you are missing for individual samples.

On Windows we recommend using https://github.com/microsoft/vcpkg to download packages for
your system. The general setup flow looks like

*Install and activate SHARK*

```bash
source shark.venv/bin/activate #follow main repo instructions to setup your venv
```

*Install Dependencies*

```bash
vcpkg install [library] --triplet [your platform]
vcpkg integrate install

# Then pass `-DCMAKE_TOOLCHAIN_FILE=[check logs for path]` when configuring CMake
```

In Ubuntu Linux you can install

```bash
sudo apt install libsdl2-dev
```

*Build*
```bash
cd cpp
cmake -GNinja -B build/
cmake --build build/
```

*Prepare the model*
```bash
wget https://storage.googleapis.com/shark_tank/latest/resnet50_tf/resnet50_tf.mlir
iree-compile --iree-input-type=auto --iree-vm-bytecode-module-output-format=flatbuffer-binary --iree-hal-target-backends=vulkan --iree-llvmcpu-embedded-linker-path=`python3 -c 'import sysconfig; print(sysconfig.get_paths()["purelib"])'`/iree/compiler/tools/../_mlir_libs/iree-lld --mlir-print-debuginfo --mlir-print-op-on-diagnostic=false --mlir-pass-pipeline-crash-reproducer=ist/core-reproducer.mlir --iree-llvmcpu-target-cpu-features=host -iree-vulkan-target-triple=rdna2-unknown-linux  resnet50_tf.mlir -o resnet50_tf.vmfb
```
*Prepare the input*

```bash
python save_img.py
```
Note that this requires tensorflow, e.g.
```bash
python -m pip install tensorflow
```

*Run the vulkan_gui*
```bash
./build/vulkan_gui/iree-samples-resnet-vulkan-gui
```

## Other models
A tool for benchmarking other models is built and can be invoked with a command like the following
```bash
./build/vulkan_gui/iree-vulkan-gui --module-file=path/to/.vmfb --function_input=...
```
see `./build/vulkan_gui/iree-vulkan-gui --help` for an explanation on the function input. For example, stable diffusion unet can be tested with the following commands:
```bash
wget https://storage.googleapis.com/shark_tank/quinn/stable_diff_tf/stable_diff_tf.mlir
iree-compile --iree-input-type=auto --iree-vm-bytecode-module-output-format=flatbuffer-binary --iree-hal-target-backends=vulkan --mlir-print-debuginfo --mlir-print-op-on-diagnostic=false --iree-llvmcpu-target-cpu-features=host -iree-vulkan-target-triple=rdna2-unknown-linux  stable_diff_tf.mlir -o stable_diff_tf.vmfb
./build/vulkan_gui/iree-vulkan-gui --module-file=stable_diff_tf.vmfb --function_input=2x4x64x64xf32 --function_input=1xf32 --function_input=2x77x768xf32
```
VAE and Autoencoder are also available
```bash
# VAE
wget https://storage.googleapis.com/shark_tank/quinn/stable_diff_tf/vae_tf/vae.mlir
iree-compile --iree-input-type=auto --iree-vm-bytecode-module-output-format=flatbuffer-binary --iree-hal-target-backends=vulkan --mlir-print-debuginfo --mlir-print-op-on-diagnostic=false --iree-llvmcpu-target-cpu-features=host -iree-vulkan-target-triple=rdna2-unknown-linux  vae.mlir -o vae.vmfb
./build/vulkan_gui/iree-vulkan-gui --module-file=stable_diff_tf.vmfb --function_input=1x4x64x64xf32

# CLIP Autoencoder
wget https://storage.googleapis.com/shark_tank/quinn/stable_diff_tf/clip_tf/clip_autoencoder.mlir
iree-compile --iree-input-type=auto --iree-vm-bytecode-module-output-format=flatbuffer-binary --iree-hal-target-backends=vulkan --mlir-print-debuginfo --mlir-print-op-on-diagnostic=false --iree-llvmcpu-target-cpu-features=host -iree-vulkan-target-triple=rdna2-unknown-linux  clip_autoencoder.mlir -o clip_autoencoder.vmfb
./build/vulkan_gui/iree-vulkan-gui --module-file=stable_diff_tf.vmfb --function_input=1x77xi32 --function_input=1x77xi32
```

```

`cpp/save_img.py`:

```py
import numpy as np
import tensorflow as tf
from shark.shark_inference import SharkInference


def load_and_preprocess_image(fname: str):
    image = tf.io.read_file(fname)
    image = tf.image.decode_image(image, channels=3)
    image = tf.image.resize(image, (224, 224))
    image = image[tf.newaxis, :]
    # preprocessing pipeline
    input_tensor = tf.keras.applications.resnet50.preprocess_input(image)
    return input_tensor


data = load_and_preprocess_image("dog_imagenet.jpg").numpy()

data.tofile("dog.bin")

```

`cpp/vision_inference/CMakeLists.txt`:

```txt
# Copyright 2022 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

if(NOT IREE_TARGET_BACKEND_LLVM_CPU OR
   NOT IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF)
  message(STATUS "Missing LLVM backend and/or embeddded elf loader, skipping vision_inference sample")
  return()
endif()

# vcpkg install stb
#   tested with version 2021-09-10
find_package(Stb)
if(NOT Stb_FOUND)
  message(STATUS "Could not find Stb, skipping vision inference sample")
  return()
endif()

# Compile mnist.mlir to mnist.vmfb.
set(_COMPILE_TOOL_EXECUTABLE $<TARGET_FILE:iree-compile>)
set(_COMPILE_ARGS)
list(APPEND _COMPILE_ARGS "--iree-input-type=auto")
list(APPEND _COMPILE_ARGS "--iree-hal-target-backends=llvm-cpu")
list(APPEND _COMPILE_ARGS "${IREE_SOURCE_DIR}/samples/models/mnist.mlir")
list(APPEND _COMPILE_ARGS "-o")
list(APPEND _COMPILE_ARGS "mnist.vmfb")
add_custom_command(
  OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/mnist.vmfb
  COMMAND ${_COMPILE_TOOL_EXECUTABLE} ${_COMPILE_ARGS}
  DEPENDS ${_COMPILE_TOOL_EXECUTABLE} "${IREE_SOURCE_DIR}/samples/models/mnist.mlir"
)
# Embed mnist.vmfb into a C file as mnist_bytecode_module_c.[h/c]
set(_EMBED_DATA_EXECUTABLE $<TARGET_FILE:generate_embed_data>)
set(_EMBED_ARGS)
list(APPEND _EMBED_ARGS "--output_header=mnist_bytecode_module_c.h")
list(APPEND _EMBED_ARGS "--output_impl=mnist_bytecode_module_c.c")
list(APPEND _EMBED_ARGS "--identifier=iree_samples_vision_inference_mnist_bytecode_module")
list(APPEND _EMBED_ARGS "--flatten")
list(APPEND _EMBED_ARGS "${CMAKE_CURRENT_BINARY_DIR}/mnist.vmfb")
add_custom_command(
  OUTPUT "mnist_bytecode_module_c.h" "mnist_bytecode_module_c.c"
  COMMAND ${_EMBED_DATA_EXECUTABLE} ${_EMBED_ARGS}
  DEPENDS ${_EMBED_DATA_EXECUTABLE} ${CMAKE_CURRENT_BINARY_DIR}/mnist.vmfb
)
# Define a library target for mnist_bytecode_module_c.
add_library(iree_samples_vision_inference_mnist_bytecode_module_c OBJECT)
target_sources(iree_samples_vision_inference_mnist_bytecode_module_c
  PRIVATE
    mnist_bytecode_module_c.h
    mnist_bytecode_module_c.c
)

# Define the sample executable.
set(_NAME "iree-run-mnist-module")
add_executable(${_NAME} "")
target_sources(${_NAME}
  PRIVATE
    "image_util.h"
    "image_util.c"
    "iree-run-mnist-module.c"
)
set_target_properties(${_NAME} PROPERTIES OUTPUT_NAME "iree-run-mnist-module")
target_include_directories(${_NAME} PUBLIC
    $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>
)
target_include_directories(${_NAME} PRIVATE
    ${Stb_INCLUDE_DIR}
)
target_link_libraries(${_NAME}
  iree_base_base
  iree_base_tracing
  iree_hal_hal
  iree_runtime_runtime
  iree_samples_vision_inference_mnist_bytecode_module_c
)

# Define a target that copies the test image into the build directory.
add_custom_target(iree_samples_vision_inference_test_image
  COMMAND ${CMAKE_COMMAND} -E copy "${CMAKE_CURRENT_SOURCE_DIR}/mnist_test.png" "${CMAKE_CURRENT_BINARY_DIR}/mnist_test.png")
add_dependencies(${_NAME} iree_samples_vision_inference_test_image)

message(STATUS "Configured vision_inference sample successfully")

```

`cpp/vision_inference/README.md`:

```md
# Vision Inference Sample (C code)

This sample demonstrates how to run a MNIST handwritten digit detection vision
model on an image using IREE's C API.

A similar sample is implemented using a Python script and IREE's command line
tools over in the primary iree repository at
https://github.com/iree-org/iree/tree/main/samples/vision_inference

```

`cpp/vision_inference/image_util.c`:

```c
// Copyright 2021 The IREE Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

#include "image_util.h"

#include <math.h>

#include "iree/base/internal/flags.h"
#include "iree/base/tracing.h"

#define STB_IMAGE_IMPLEMENTATION
#include "stb_image.h"

iree_status_t iree_tools_utils_pixel_rescaled_to_buffer(
    const uint8_t* pixel_data, iree_host_size_t buffer_length,
    const float* input_range, iree_host_size_t range_length,
    float* out_buffer) {
  IREE_TRACE_ZONE_BEGIN(z0);
  if (range_length != 2) {
    IREE_TRACE_ZONE_END(z0);
    return iree_make_status(IREE_STATUS_INVALID_ARGUMENT,
                            "range defined as 2-element [min, max] array.");
  }
  float input_scale = fabsf(input_range[1] - input_range[0]) / 2.0f;
  float input_offset = (input_range[0] + input_range[1]) / 2.0f;
  const float kUint8Mean = 127.5f;
  for (int i = 0; i < buffer_length; ++i) {
    out_buffer[i] =
        (((float)(pixel_data[i])) - kUint8Mean) / kUint8Mean * input_scale +
        input_offset;
  }
  IREE_TRACE_ZONE_END(z0);
  return iree_ok_status();
}

iree_status_t iree_tools_utils_load_pixel_data_impl(
    const iree_string_view_t filename, const iree_hal_dim_t* shape,
    iree_host_size_t shape_rank, iree_hal_element_type_t element_type,
    uint8_t** out_pixel_data, iree_host_size_t* out_buffer_length) {
  int img_dims[3];
  if (stbi_info(filename.data, img_dims, &(img_dims[1]), &(img_dims[2])) == 0) {
    return iree_make_status(IREE_STATUS_NOT_FOUND, "can't load image %.*s",
                            (int)filename.size, filename.data);
  }
  if (!(element_type == IREE_HAL_ELEMENT_TYPE_FLOAT_32 ||
        element_type == IREE_HAL_ELEMENT_TYPE_SINT_8 ||
        element_type == IREE_HAL_ELEMENT_TYPE_UINT_8)) {
    char element_type_str[16];
    IREE_RETURN_IF_ERROR(iree_hal_format_element_type(
        element_type, sizeof(element_type_str), element_type_str, NULL));
    return iree_make_status(IREE_STATUS_UNIMPLEMENTED,
                            "element type %s not supported", element_type_str);
  }
  switch (shape_rank) {
    case 2: {  // Assume tensor <height x width>
      if (img_dims[2] != 1 || (shape[0] != img_dims[1]) ||
          (shape[1] != img_dims[0])) {
        return iree_make_status(
            IREE_STATUS_INVALID_ARGUMENT,
            "image size: %dx%dx%d, expected: %" PRIdim "x%" PRIdim, img_dims[0],
            img_dims[1], img_dims[2], shape[1], shape[0]);
      }
      break;
    }
    case 3: {  // Assume tensor <height x width x channel>
      if (shape[0] != img_dims[1] || shape[1] != img_dims[0] ||
          shape[2] != img_dims[2]) {
        return iree_make_status(IREE_STATUS_INVALID_ARGUMENT,
                                "image size: %dx%dx%d, expected: %" PRIdim
                                "x%" PRIdim "x%" PRIdim,
                                img_dims[0], img_dims[1], img_dims[2], shape[1],
                                shape[0], shape[2]);
      }
      break;
    }
    case 4: {  // Assume tensor <batch x height x width x channel>
      if (shape[1] != img_dims[1] || shape[2] != img_dims[0] ||
          shape[3] != img_dims[2]) {
        return iree_make_status(IREE_STATUS_INVALID_ARGUMENT,
                                "image size: %dx%dx%d, expected: %" PRIdim
                                "x%" PRIdim "x%" PRIdim,
                                img_dims[0], img_dims[1], img_dims[2], shape[2],
                                shape[1], shape[3]);
      }
      break;
    }
    default:
      return iree_make_status(
          IREE_STATUS_INVALID_ARGUMENT,
          "Input buffer shape rank %" PRIhsz " not supported", shape_rank);
  }
  // Drop the alpha channel if present.
  int req_ch = (img_dims[2] >= 3) ? 3 : 0;
  *out_pixel_data = stbi_load(filename.data, img_dims, &(img_dims[1]),
                              &(img_dims[2]), req_ch);
  if (*out_pixel_data == NULL) {
    return iree_make_status(IREE_STATUS_NOT_FOUND, "can't load image %.*s",
                            (int)filename.size, filename.data);
  }
  *out_buffer_length =
      img_dims[0] * img_dims[1] * (img_dims[2] > 3 ? 3 : img_dims[2]);
  return iree_ok_status();
}

iree_status_t iree_tools_utils_load_pixel_data(
    const iree_string_view_t filename, const iree_hal_dim_t* shape,
    iree_host_size_t shape_rank, iree_hal_element_type_t element_type,
    uint8_t** out_pixel_data, iree_host_size_t* out_buffer_length) {
  IREE_TRACE_ZONE_BEGIN(z0);
  iree_status_t result = iree_tools_utils_load_pixel_data_impl(
      filename, shape, shape_rank, element_type, out_pixel_data,
      out_buffer_length);
  IREE_TRACE_ZONE_END(z0);
  return result;
}

iree_status_t iree_tools_utils_buffer_view_from_image(
    const iree_string_view_t filename, const iree_hal_dim_t* shape,
    iree_host_size_t shape_rank, iree_hal_element_type_t element_type,
    iree_hal_allocator_t* allocator, iree_hal_buffer_view_t** out_buffer_view) {
  IREE_TRACE_ZONE_BEGIN(z0);
  *out_buffer_view = NULL;
  if (element_type != IREE_HAL_ELEMENT_TYPE_SINT_8 &&
      element_type != IREE_HAL_ELEMENT_TYPE_UINT_8) {
    IREE_TRACE_ZONE_END(z0);
    return iree_make_status(IREE_STATUS_INVALID_ARGUMENT,
                            "element type should be i8 or u8");
  }

  iree_status_t result;
  uint8_t* pixel_data = NULL;
  iree_host_size_t buffer_length;
  result = iree_tools_utils_load_pixel_data(
      filename, shape, shape_rank, element_type, &pixel_data, &buffer_length);
  if (iree_status_is_ok(result)) {
    iree_host_size_t element_byte =
        iree_hal_element_dense_byte_count(element_type);
    // SINT_8 and UINT_8 perform direct buffer wrap.
    result = iree_hal_buffer_view_allocate_buffer(
        allocator, shape_rank, shape, element_type,
        IREE_HAL_ENCODING_TYPE_DENSE_ROW_MAJOR,
        (iree_hal_buffer_params_t){
            .type = IREE_HAL_MEMORY_TYPE_DEVICE_LOCAL,
            .access = IREE_HAL_MEMORY_ACCESS_READ,
            .usage = IREE_HAL_BUFFER_USAGE_DISPATCH_STORAGE |
                     IREE_HAL_BUFFER_USAGE_TRANSFER,
        },
        iree_make_const_byte_span(pixel_data, element_byte * buffer_length),
        out_buffer_view);
  }
  stbi_image_free(pixel_data);
  IREE_TRACE_ZONE_END(z0);
  return result;
}

typedef struct iree_tools_utils_buffer_view_load_params_t {
  const uint8_t* pixel_data;
  iree_host_size_t pixel_data_length;
  const float* input_range;
  iree_host_size_t input_range_length;
} iree_tools_utils_buffer_view_load_params_t;
static iree_status_t iree_tools_utils_buffer_view_load_image_rescaled(
    iree_hal_buffer_mapping_t* mapping, void* user_data) {
  iree_tools_utils_buffer_view_load_params_t* params =
      (iree_tools_utils_buffer_view_load_params_t*)user_data;
  return iree_tools_utils_pixel_rescaled_to_buffer(
      params->pixel_data, params->pixel_data_length, params->input_range,
      params->input_range_length, (float*)mapping->contents.data);
}

iree_status_t iree_tools_utils_buffer_view_from_image_rescaled(
    const iree_string_view_t filename, const iree_hal_dim_t* shape,
    iree_host_size_t shape_rank, iree_hal_element_type_t element_type,
    iree_hal_allocator_t* allocator, const float* input_range,
    iree_host_size_t input_range_length,
    iree_hal_buffer_view_t** out_buffer_view) {
  IREE_TRACE_ZONE_BEGIN(z0);
  *out_buffer_view = NULL;
  if (element_type != IREE_HAL_ELEMENT_TYPE_FLOAT_32) {
    IREE_TRACE_ZONE_END(z0);
    return iree_make_status(IREE_STATUS_INVALID_ARGUMENT,
                            "element type should be f32");
  }

  // Classic row-major image layout.
  iree_hal_encoding_type_t encoding_type =
      IREE_HAL_ENCODING_TYPE_DENSE_ROW_MAJOR;

  // Load pixel data from the file into a new host memory allocation (the only
  // interface stb_image provides). A real application would want to use the
  // generation callback to directly decode the image into the target mapped
  // device buffer.
  uint8_t* pixel_data = NULL;
  iree_host_size_t buffer_length = 0;
  IREE_RETURN_AND_END_ZONE_IF_ERROR(
      z0, iree_tools_utils_load_pixel_data(filename, shape, shape_rank,
                                           element_type, &pixel_data,
                                           &buffer_length));

  iree_tools_utils_buffer_view_load_params_t params = {
      .pixel_data = pixel_data,
      .pixel_data_length = buffer_length,
      .input_range = input_range,
      .input_range_length = input_range_length,
  };
  iree_status_t status = iree_hal_buffer_view_generate_buffer(
      allocator, shape_rank, shape, element_type, encoding_type,
      (iree_hal_buffer_params_t){
          .type = IREE_HAL_MEMORY_TYPE_DEVICE_LOCAL |
                  IREE_HAL_MEMORY_TYPE_HOST_VISIBLE,
          .usage = IREE_HAL_BUFFER_USAGE_DISPATCH_STORAGE |
                   IREE_HAL_BUFFER_USAGE_TRANSFER |
                   IREE_HAL_BUFFER_USAGE_MAPPING,
      },
      iree_tools_utils_buffer_view_load_image_rescaled, &params,
      out_buffer_view);

  stbi_image_free(pixel_data);
  IREE_TRACE_ZONE_END(z0);
  return status;
}

```

`cpp/vision_inference/image_util.h`:

```h
// Copyright 2021 The IREE Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

#ifndef IREE_SAMPLES_VISION_INFERENCE_IMAGE_UTIL_H_
#define IREE_SAMPLES_VISION_INFERENCE_IMAGE_UTIL_H_

#include "iree/base/api.h"
#include "iree/hal/api.h"
#include "iree/hal/buffer_view.h"

#if __cplusplus
extern "C" {
#endif  // __cplusplus

// Loads the image at |filename| into |out_pixel_data| and sets
// |out_buffer_length| to its length.
//
// The image dimension must match the width, height, and channel in|shape|,
// while 2 <= |shape_rank| <= 4 to match the image tensor format.
//
// The file must be in a format supported by stb_image.h.
// The returned |out_pixel_data| buffer must be released by the caller.
iree_status_t iree_tools_utils_load_pixel_data(
    const iree_string_view_t filename, const iree_hal_dim_t* shape,
    iree_host_size_t shape_rank, iree_hal_element_type_t element_type,
    uint8_t** out_pixel_data, iree_host_size_t* out_buffer_length);

// Parse the content in an image file in |filename| into a HAL buffer view
// |out_buffer_view|. |out_buffer_view| properties are defined by |shape|,
// |shape_rank|, and |element_type|, while being allocated by |allocator|.
//
// The |element_type| has to be SINT_8 or UINT_8. For FLOAT_32, use
// |iree_tools_utils_buffer_view_from_image_rescaled| instead.
//
// The returned |out_buffer_view| must be released by the caller.
iree_status_t iree_tools_utils_buffer_view_from_image(
    const iree_string_view_t filename, const iree_hal_dim_t* shape,
    iree_host_size_t shape_rank, iree_hal_element_type_t element_type,
    iree_hal_allocator_t* allocator, iree_hal_buffer_view_t** out_buffer_view);

// Parse the content in an image file in |filename| into a HAL buffer view
// |out_buffer_view|. |out_buffer_view| properties are defined by |shape|,
// |shape_rank|, and |element_type|, while being allocated by |allocator|.
// The value in |out_buffer_view| is rescaled with |input_range|.
//
// The |element_type| has to be FLOAT_32, For SINT_8 or UINT_8, use
// |iree_tools_utils_buffer_view_from_image| instead.
//
// The returned |out_buffer_view| must be released by the caller.
iree_status_t iree_tools_utils_buffer_view_from_image_rescaled(
    const iree_string_view_t filename, const iree_hal_dim_t* shape,
    iree_host_size_t shape_rank, iree_hal_element_type_t element_type,
    iree_hal_allocator_t* allocator, const float* input_range,
    iree_host_size_t input_range_length,
    iree_hal_buffer_view_t** out_buffer_view);

// Normalize uint8_t |pixel_data| of the size |buffer_length| to float buffer
// |out_buffer| with the range |input_range|.
//
// float32_x = (uint8_x - 127.5) / 127.5 * input_scale + input_offset, where
// input_scale = abs(|input_range[0]| - |input_range[1]| / 2
// input_offset = |input_range[0]| + |input_range[1]| / 2
//
// |out_buffer| needs to be allocated before the call.
iree_status_t iree_tools_utils_pixel_rescaled_to_buffer(
    const uint8_t* pixel_data, iree_host_size_t pixel_count,
    const float* input_range, iree_host_size_t input_range_length,
    float* out_buffer);

#if __cplusplus
}
#endif  // __cplusplus

#endif  // IREE_SAMPLES_VISION_INFERENCE_IMAGE_UTIL_H_

```

`cpp/vision_inference/iree-run-mnist-module.c`:

```c
// Copyright 2021 The IREE Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

// This sample uses image_util to load a hand-written image as an
// iree_hal_buffer_view_t then passes it to the bytecode module built from
// mnist.mlir on the CPU backend with the local-task driver.

#include <float.h>

#include "image_util.h"
#include "iree/runtime/api.h"
#include "mnist_bytecode_module_c.h"

iree_status_t Run(const iree_string_view_t image_path) {
  iree_runtime_instance_options_t instance_options;
  iree_runtime_instance_options_initialize(IREE_API_VERSION_LATEST,
                                           &instance_options);
  iree_runtime_instance_options_use_all_available_drivers(&instance_options);
  iree_runtime_instance_t* instance = NULL;
  IREE_RETURN_IF_ERROR(iree_runtime_instance_create(
      &instance_options, iree_allocator_system(), &instance));

  // TODO(#5724): move device selection into the compiled modules.
  iree_hal_device_t* device = NULL;
  IREE_RETURN_IF_ERROR(iree_runtime_instance_try_create_default_device(
      instance, iree_make_cstring_view("local-task"), &device));

  // Create one session per loaded module to hold the module state.
  iree_runtime_session_options_t session_options;
  iree_runtime_session_options_initialize(&session_options);
  iree_runtime_session_t* session = NULL;
  IREE_RETURN_IF_ERROR(iree_runtime_session_create_with_device(
      instance, &session_options, device,
      iree_runtime_instance_host_allocator(instance), &session));
  iree_hal_device_release(device);

  const struct iree_file_toc_t* module_file =
      iree_samples_vision_inference_mnist_bytecode_module_create();

  IREE_RETURN_IF_ERROR(iree_runtime_session_append_bytecode_module_from_memory(
      session, iree_make_const_byte_span(module_file->data, module_file->size),
      iree_allocator_null()));

  iree_runtime_call_t call;
  IREE_RETURN_IF_ERROR(iree_runtime_call_initialize_by_name(
      session, iree_make_cstring_view("module.predict"), &call));

  // Prepare the input hal buffer view with image_util library.
  // The input of the mmist model is single 28x28 pixel image as a
  // tensor<1x28x28x1xf32>, with pixels in [0.0, 1.0].
  iree_hal_buffer_view_t* buffer_view = NULL;
  iree_hal_dim_t buffer_shape[] = {1, 28, 28, 1};
  iree_hal_element_type_t hal_element_type = IREE_HAL_ELEMENT_TYPE_FLOAT_32;
  float input_range[2] = {0.0f, 1.0f};
  IREE_RETURN_IF_ERROR(
      iree_tools_utils_buffer_view_from_image_rescaled(
          image_path, buffer_shape, IREE_ARRAYSIZE(buffer_shape),
          hal_element_type, iree_hal_device_allocator(device), input_range,
          IREE_ARRAYSIZE(input_range), &buffer_view),
      "load image");
  IREE_RETURN_IF_ERROR(
      iree_runtime_call_inputs_push_back_buffer_view(&call, buffer_view));
  iree_hal_buffer_view_release(buffer_view);

  IREE_RETURN_IF_ERROR(iree_runtime_call_invoke(&call, /*flags=*/0));

  // Get the result buffers from the invocation.
  iree_hal_buffer_view_t* ret_buffer_view = NULL;
  IREE_RETURN_IF_ERROR(
      iree_runtime_call_outputs_pop_front_buffer_view(&call, &ret_buffer_view));

  // Read back the results. The output of the mnist model is a 1x10 prediction
  // confidence values for each digit in [0, 9].
  float predictions[1 * 10] = {0.0f};
  IREE_RETURN_IF_ERROR(iree_hal_device_transfer_d2h(
      iree_runtime_session_device(session),
      iree_hal_buffer_view_buffer(ret_buffer_view), 0, predictions,
      sizeof(predictions), IREE_HAL_TRANSFER_BUFFER_FLAG_DEFAULT,
      iree_infinite_timeout()));
  iree_hal_buffer_view_release(ret_buffer_view);

  // Get the highest index from the output.
  float result_val = FLT_MIN;
  int result_idx = 0;
  for (iree_host_size_t i = 0; i < IREE_ARRAYSIZE(predictions); ++i) {
    if (predictions[i] > result_val) {
      result_val = predictions[i];
      result_idx = i;
    }
  }
  fprintf(stdout, "Detected number: %d\n", result_idx);

  iree_runtime_call_deinitialize(&call);
  iree_runtime_session_release(session);
  iree_runtime_instance_release(instance);
  return iree_ok_status();
}

int main(int argc, char** argv) {
  if (argc > 2) {
    fprintf(stderr, "Usage: iree-run-mnist-module <image file>\n");
    return -1;
  }
  iree_string_view_t image_path;
  if (argc == 1) {
    image_path = iree_make_cstring_view("mnist_test.png");
  } else {
    image_path = iree_make_cstring_view(argv[1]);
  }
  iree_status_t result = Run(image_path);
  if (!iree_status_is_ok(result)) {
    iree_status_fprint(stderr, result);
    iree_status_ignore(result);
    return -1;
  }
  iree_status_ignore(result);
  return 0;
}

```

`cpp/vulkan_gui/CMakeLists.txt`:

```txt
# Copyright 2022 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

if(NOT IREE_TARGET_BACKEND_VULKAN_SPIRV OR
   NOT IREE_HAL_DRIVER_VULKAN)
  message(STATUS "Missing Vulkan backend and/or driver, skipping vulkan_gui sample")
  return()
endif()

# This target statically links against Vulkan.
# One way to achieve this is by installing the Vulkan SDK from
# https://vulkan.lunarg.com/.
include(FindVulkan)
if(NOT Vulkan_FOUND)
  message(STATUS "Could not find Vulkan, skipping vulkan_gui sample")
  return()
endif()

# vcpkg install sdl2[vulkan]
#   tested with versions 2.0.14#4 - 2.0.22#1
find_package(SDL2)
if(NOT SDL2_FOUND)
  message(STATUS "Could not find SDL2, skipping vulkan_gui sample")
  return()
endif()

FetchContent_Declare(
  imgui
  GIT_REPOSITORY https://github.com/ocornut/imgui
  GIT_TAG        master
)

FetchContent_MakeAvailable(imgui)

# Dear ImGui
set(IMGUI_DIR ${CMAKE_BINARY_DIR}/_deps/imgui-src)
message("Looking for Imgui in ${IMGUI_DIR}")
include_directories(${IMGUI_DIR} ${IMGUI_DIR}/backends ..)


function(iree_vulkan_sample)

  cmake_parse_arguments(
    _RULE
    ""
    "NAME"
    "SRCS"
    ${ARGN}
  )


  # Define the sample executable.
  set(_NAME "${_RULE_NAME}")
  set(SRCS "${_RULE_SRCS}")
  add_executable(${_NAME} "")
  target_sources(${_NAME}
    PRIVATE
      ${SRCS}
      "${IMGUI_DIR}/backends/imgui_impl_sdl.cpp"
      "${IMGUI_DIR}/backends/imgui_impl_vulkan.cpp"
      "${IMGUI_DIR}/imgui.cpp"
      "${IMGUI_DIR}/imgui_draw.cpp"
      "${IMGUI_DIR}/imgui_demo.cpp"
      "${IMGUI_DIR}/imgui_tables.cpp"
      "${IMGUI_DIR}/imgui_widgets.cpp"
  )
  set_target_properties(${_NAME} PROPERTIES OUTPUT_NAME "${_NAME}")
  target_include_directories(${_NAME} PUBLIC
      $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>
  )
  target_link_libraries(${_NAME}
    SDL2::SDL2
    Vulkan::Vulkan
    iree_runtime_runtime
    iree_base_internal_main
    iree_hal_drivers_vulkan_registration_registration
    iree_modules_hal_hal
    iree_vm_vm
    iree_vm_bytecode_module
    iree_vm_cc
    iree_tooling_vm_util_cc
    iree_tooling_context_util
  )

  if(${CMAKE_SYSTEM_NAME} STREQUAL "Windows")
    set(_GUI_LINKOPTS "-SUBSYSTEM:CONSOLE")
  else()
    set(_GUI_LINKOPTS "")
  endif()

  target_link_options(${_NAME}
    PRIVATE
      ${_GUI_LINKOPTS}
  )
endfunction()

iree_vulkan_sample(
    NAME
      iree-samples-resnet-vulkan-gui

    SRCS
      vulkan_resnet_inference_gui.cc
)

iree_vulkan_sample(
    NAME
      iree-vulkan-gui

    SRCS
      vulkan_inference_gui.cc
)

message(STATUS "Configured vulkan_gui sample successfully")

```

`cpp/vulkan_gui/stb_image.h`:

```h
/* stb_image - v2.27 - public domain image loader - http://nothings.org/stb
                                  no warranty implied; use at your own risk

   Do this:
      #define STB_IMAGE_IMPLEMENTATION
   before you include this file in *one* C or C++ file to create the implementation.

   // i.e. it should look like this:
   #include ...
   #include ...
   #include ...
   #define STB_IMAGE_IMPLEMENTATION
   #include "stb_image.h"

   You can #define STBI_ASSERT(x) before the #include to avoid using assert.h.
   And #define STBI_MALLOC, STBI_REALLOC, and STBI_FREE to avoid using malloc,realloc,free


   QUICK NOTES:
      Primarily of interest to game developers and other people who can
          avoid problematic images and only need the trivial interface

      JPEG baseline & progressive (12 bpc/arithmetic not supported, same as stock IJG lib)
      PNG 1/2/4/8/16-bit-per-channel

      TGA (not sure what subset, if a subset)
      BMP non-1bpp, non-RLE
      PSD (composited view only, no extra channels, 8/16 bit-per-channel)

      GIF (*comp always reports as 4-channel)
      HDR (radiance rgbE format)
      PIC (Softimage PIC)
      PNM (PPM and PGM binary only)

      Animated GIF still needs a proper API, but here's one way to do it:
          http://gist.github.com/urraka/685d9a6340b26b830d49

      - decode from memory or through FILE (define STBI_NO_STDIO to remove code)
      - decode from arbitrary I/O callbacks
      - SIMD acceleration on x86/x64 (SSE2) and ARM (NEON)

   Full documentation under "DOCUMENTATION" below.


LICENSE

  See end of file for license information.

RECENT REVISION HISTORY:

      2.27  (2021-07-11) document stbi_info better, 16-bit PNM support, bug fixes
      2.26  (2020-07-13) many minor fixes
      2.25  (2020-02-02) fix warnings
      2.24  (2020-02-02) fix warnings; thread-local failure_reason and flip_vertically
      2.23  (2019-08-11) fix clang static analysis warning
      2.22  (2019-03-04) gif fixes, fix warnings
      2.21  (2019-02-25) fix typo in comment
      2.20  (2019-02-07) support utf8 filenames in Windows; fix warnings and platform ifdefs
      2.19  (2018-02-11) fix warning
      2.18  (2018-01-30) fix warnings
      2.17  (2018-01-29) bugfix, 1-bit BMP, 16-bitness query, fix warnings
      2.16  (2017-07-23) all functions have 16-bit variants; optimizations; bugfixes
      2.15  (2017-03-18) fix png-1,2,4; all Imagenet JPGs; no runtime SSE detection on GCC
      2.14  (2017-03-03) remove deprecated STBI_JPEG_OLD; fixes for Imagenet JPGs
      2.13  (2016-12-04) experimental 16-bit API, only for PNG so far; fixes
      2.12  (2016-04-02) fix typo in 2.11 PSD fix that caused crashes
      2.11  (2016-04-02) 16-bit PNGS; enable SSE2 in non-gcc x64
                         RGB-format JPEG; remove white matting in PSD;
                         allocate large structures on the stack;
                         correct channel count for PNG & BMP
      2.10  (2016-01-22) avoid warning introduced in 2.09
      2.09  (2016-01-16) 16-bit TGA; comments in PNM files; STBI_REALLOC_SIZED

   See end of file for full revision history.


 ============================    Contributors    =========================

 Image formats                          Extensions, features
    Sean Barrett (jpeg, png, bmp)          Jetro Lauha (stbi_info)
    Nicolas Schulz (hdr, psd)              Martin "SpartanJ" Golini (stbi_info)
    Jonathan Dummer (tga)                  James "moose2000" Brown (iPhone PNG)
    Jean-Marc Lienher (gif)                Ben "Disch" Wenger (io callbacks)
    Tom Seddon (pic)                       Omar Cornut (1/2/4-bit PNG)
    Thatcher Ulrich (psd)                  Nicolas Guillemot (vertical flip)
    Ken Miller (pgm, ppm)                  Richard Mitton (16-bit PSD)
    github:urraka (animated gif)           Junggon Kim (PNM comments)
    Christopher Forseth (animated gif)     Daniel Gibson (16-bit TGA)
                                           socks-the-fox (16-bit PNG)
                                           Jeremy Sawicki (handle all ImageNet JPGs)
 Optimizations & bugfixes                  Mikhail Morozov (1-bit BMP)
    Fabian "ryg" Giesen                    Anael Seghezzi (is-16-bit query)
    Arseny Kapoulkine                      Simon Breuss (16-bit PNM)
    John-Mark Allen
    Carmelo J Fdez-Aguera

 Bug & warning fixes
    Marc LeBlanc            David Woo          Guillaume George     Martins Mozeiko
    Christpher Lloyd        Jerry Jansson      Joseph Thomson       Blazej Dariusz Roszkowski
    Phil Jordan                                Dave Moore           Roy Eltham
    Hayaki Saito            Nathan Reed        Won Chun
    Luke Graham             Johan Duparc       Nick Verigakis       the Horde3D community
    Thomas Ruf              Ronny Chevalier                         github:rlyeh
    Janez Zemva             John Bartholomew   Michal Cichon        github:romigrou
    Jonathan Blow           Ken Hamada         Tero Hanninen        github:svdijk
    Eugene Golushkov        Laurent Gomila     Cort Stratton        github:snagar
    Aruelien Pocheville     Sergio Gonzalez    Thibault Reuille     github:Zelex
    Cass Everitt            Ryamond Barbiero                        github:grim210
    Paul Du Bois            Engin Manap        Aldo Culquicondor    github:sammyhw
    Philipp Wiesemann       Dale Weiler        Oriol Ferrer Mesia   github:phprus
    Josh Tobin                                 Matthew Gregan       github:poppolopoppo
    Julian Raschke          Gregory Mullen     Christian Floisand   github:darealshinji
    Baldur Karlsson         Kevin Schmidt      JR Smith             github:Michaelangel007
                            Brad Weinberger    Matvey Cherevko      github:mosra
    Luca Sas                Alexander Veselov  Zack Middleton       [reserved]
    Ryan C. Gordon          [reserved]                              [reserved]
                     DO NOT ADD YOUR NAME HERE

                     Jacko Dirks

  To add your name to the credits, pick a random blank space in the middle and fill it.
  80% of merge conflicts on stb PRs are due to people adding their name at the end
  of the credits.
*/

#ifndef STBI_INCLUDE_STB_IMAGE_H
#define STBI_INCLUDE_STB_IMAGE_H

// DOCUMENTATION
//
// Limitations:
//    - no 12-bit-per-channel JPEG
//    - no JPEGs with arithmetic coding
//    - GIF always returns *comp=4
//
// Basic usage (see HDR discussion below for HDR usage):
//    int x,y,n;
//    unsigned char *data = stbi_load(filename, &x, &y, &n, 0);
//    // ... process data if not NULL ...
//    // ... x = width, y = height, n = # 8-bit components per pixel ...
//    // ... replace '0' with '1'..'4' to force that many components per pixel
//    // ... but 'n' will always be the number that it would have been if you said 0
//    stbi_image_free(data)
//
// Standard parameters:
//    int *x                 -- outputs image width in pixels
//    int *y                 -- outputs image height in pixels
//    int *channels_in_file  -- outputs # of image components in image file
//    int desired_channels   -- if non-zero, # of image components requested in result
//
// The return value from an image loader is an 'unsigned char *' which points
// to the pixel data, or NULL on an allocation failure or if the image is
// corrupt or invalid. The pixel data consists of *y scanlines of *x pixels,
// with each pixel consisting of N interleaved 8-bit components; the first
// pixel pointed to is top-left-most in the image. There is no padding between
// image scanlines or between pixels, regardless of format. The number of
// components N is 'desired_channels' if desired_channels is non-zero, or
// *channels_in_file otherwise. If desired_channels is non-zero,
// *channels_in_file has the number of components that _would_ have been
// output otherwise. E.g. if you set desired_channels to 4, you will always
// get RGBA output, but you can check *channels_in_file to see if it's trivially
// opaque because e.g. there were only 3 channels in the source image.
//
// An output image with N components has the following components interleaved
// in this order in each pixel:
//
//     N=#comp     components
//       1           grey
//       2           grey, alpha
//       3           red, green, blue
//       4           red, green, blue, alpha
//
// If image loading fails for any reason, the return value will be NULL,
// and *x, *y, *channels_in_file will be unchanged. The function
// stbi_failure_reason() can be queried for an extremely brief, end-user
// unfriendly explanation of why the load failed. Define STBI_NO_FAILURE_STRINGS
// to avoid compiling these strings at all, and STBI_FAILURE_USERMSG to get slightly
// more user-friendly ones.
//
// Paletted PNG, BMP, GIF, and PIC images are automatically depalettized.
//
// To query the width, height and component count of an image without having to
// decode the full file, you can use the stbi_info family of functions:
//
//   int x,y,n,ok;
//   ok = stbi_info(filename, &x, &y, &n);
//   // returns ok=1 and sets x, y, n if image is a supported format,
//   // 0 otherwise.
//
// Note that stb_image pervasively uses ints in its public API for sizes,
// including sizes of memory buffers. This is now part of the API and thus
// hard to change without causing breakage. As a result, the various image
// loaders all have certain limits on image size; these differ somewhat
// by format but generally boil down to either just under 2GB or just under
// 1GB. When the decoded image would be larger than this, stb_image decoding
// will fail.
//
// Additionally, stb_image will reject image files that have any of their
// dimensions set to a larger value than the configurable STBI_MAX_DIMENSIONS,
// which defaults to 2**24 = 16777216 pixels. Due to the above memory limit,
// the only way to have an image with such dimensions load correctly
// is for it to have a rather extreme aspect ratio. Either way, the
// assumption here is that such larger images are likely to be malformed
// or malicious. If you do need to load an image with individual dimensions
// larger than that, and it still fits in the overall size limit, you can
// #define STBI_MAX_DIMENSIONS on your own to be something larger.
//
// ===========================================================================
//
// UNICODE:
//
//   If compiling for Windows and you wish to use Unicode filenames, compile
//   with
//       #define STBI_WINDOWS_UTF8
//   and pass utf8-encoded filenames. Call stbi_convert_wchar_to_utf8 to convert
//   Windows wchar_t filenames to utf8.
//
// ===========================================================================
//
// Philosophy
//
// stb libraries are designed with the following priorities:
//
//    1. easy to use
//    2. easy to maintain
//    3. good performance
//
// Sometimes I let "good performance" creep up in priority over "easy to maintain",
// and for best performance I may provide less-easy-to-use APIs that give higher
// performance, in addition to the easy-to-use ones. Nevertheless, it's important
// to keep in mind that from the standpoint of you, a client of this library,
// all you care about is #1 and #3, and stb libraries DO NOT emphasize #3 above all.
//
// Some secondary priorities arise directly from the first two, some of which
// provide more explicit reasons why performance can't be emphasized.
//
//    - Portable ("ease of use")
//    - Small source code footprint ("easy to maintain")
//    - No dependencies ("ease of use")
//
// ===========================================================================
//
// I/O callbacks
//
// I/O callbacks allow you to read from arbitrary sources, like packaged
// files or some other source. Data read from callbacks are processed
// through a small internal buffer (currently 128 bytes) to try to reduce
// overhead.
//
// The three functions you must define are "read" (reads some bytes of data),
// "skip" (skips some bytes of data), "eof" (reports if the stream is at the end).
//
// ===========================================================================
//
// SIMD support
//
// The JPEG decoder will try to automatically use SIMD kernels on x86 when
// supported by the compiler. For ARM Neon support, you must explicitly
// request it.
//
// (The old do-it-yourself SIMD API is no longer supported in the current
// code.)
//
// On x86, SSE2 will automatically be used when available based on a run-time
// test; if not, the generic C versions are used as a fall-back. On ARM targets,
// the typical path is to have separate builds for NEON and non-NEON devices
// (at least this is true for iOS and Android). Therefore, the NEON support is
// toggled by a build flag: define STBI_NEON to get NEON loops.
//
// If for some reason you do not want to use any of SIMD code, or if
// you have issues compiling it, you can disable it entirely by
// defining STBI_NO_SIMD.
//
// ===========================================================================
//
// HDR image support   (disable by defining STBI_NO_HDR)
//
// stb_image supports loading HDR images in general, and currently the Radiance
// .HDR file format specifically. You can still load any file through the existing
// interface; if you attempt to load an HDR file, it will be automatically remapped
// to LDR, assuming gamma 2.2 and an arbitrary scale factor defaulting to 1;
// both of these constants can be reconfigured through this interface:
//
//     stbi_hdr_to_ldr_gamma(2.2f);
//     stbi_hdr_to_ldr_scale(1.0f);
//
// (note, do not use _inverse_ constants; stbi_image will invert them
// appropriately).
//
// Additionally, there is a new, parallel interface for loading files as
// (linear) floats to preserve the full dynamic range:
//
//    float *data = stbi_loadf(filename, &x, &y, &n, 0);
//
// If you load LDR images through this interface, those images will
// be promoted to floating point values, run through the inverse of
// constants corresponding to the above:
//
//     stbi_ldr_to_hdr_scale(1.0f);
//     stbi_ldr_to_hdr_gamma(2.2f);
//
// Finally, given a filename (or an open file or memory block--see header
// file for details) containing image data, you can query for the "most
// appropriate" interface to use (that is, whether the image is HDR or
// not), using:
//
//     stbi_is_hdr(char *filename);
//
// ===========================================================================
//
// iPhone PNG support:
//
// We optionally support converting iPhone-formatted PNGs (which store
// premultiplied BGRA) back to RGB, even though they're internally encoded
// differently. To enable this conversion, call
// stbi_convert_iphone_png_to_rgb(1).
//
// Call stbi_set_unpremultiply_on_load(1) as well to force a divide per
// pixel to remove any premultiplied alpha *only* if the image file explicitly
// says there's premultiplied data (currently only happens in iPhone images,
// and only if iPhone convert-to-rgb processing is on).
//
// ===========================================================================
//
// ADDITIONAL CONFIGURATION
//
//  - You can suppress implementation of any of the decoders to reduce
//    your code footprint by #defining one or more of the following
//    symbols before creating the implementation.
//
//        STBI_NO_JPEG
//        STBI_NO_PNG
//        STBI_NO_BMP
//        STBI_NO_PSD
//        STBI_NO_TGA
//        STBI_NO_GIF
//        STBI_NO_HDR
//        STBI_NO_PIC
//        STBI_NO_PNM   (.ppm and .pgm)
//
//  - You can request *only* certain decoders and suppress all other ones
//    (this will be more forward-compatible, as addition of new decoders
//    doesn't require you to disable them explicitly):
//
//        STBI_ONLY_JPEG
//        STBI_ONLY_PNG
//        STBI_ONLY_BMP
//        STBI_ONLY_PSD
//        STBI_ONLY_TGA
//        STBI_ONLY_GIF
//        STBI_ONLY_HDR
//        STBI_ONLY_PIC
//        STBI_ONLY_PNM   (.ppm and .pgm)
//
//   - If you use STBI_NO_PNG (or _ONLY_ without PNG), and you still
//     want the zlib decoder to be available, #define STBI_SUPPORT_ZLIB
//
//  - If you define STBI_MAX_DIMENSIONS, stb_image will reject images greater
//    than that size (in either width or height) without further processing.
//    This is to let programs in the wild set an upper bound to prevent
//    denial-of-service attacks on untrusted data, as one could generate a
//    valid image of gigantic dimensions and force stb_image to allocate a
//    huge block of memory and spend disproportionate time decoding it. By
//    default this is set to (1 << 24), which is 16777216, but that's still
//    very big.

#ifndef STBI_NO_STDIO
#include <stdio.h>
#endif // STBI_NO_STDIO

#define STBI_VERSION 1

enum
{
   STBI_default = 0, // only used for desired_channels

   STBI_grey       = 1,
   STBI_grey_alpha = 2,
   STBI_rgb        = 3,
   STBI_rgb_alpha  = 4
};

#include <stdlib.h>
typedef unsigned char stbi_uc;
typedef unsigned short stbi_us;

#ifdef __cplusplus
extern "C" {
#endif

#ifndef STBIDEF
#ifdef STB_IMAGE_STATIC
#define STBIDEF static
#else
#define STBIDEF extern
#endif
#endif

//////////////////////////////////////////////////////////////////////////////
//
// PRIMARY API - works on images of any type
//

//
// load image by filename, open file, or memory buffer
//

typedef struct
{
   int      (*read)  (void *user,char *data,int size);   // fill 'data' with 'size' bytes.  return number of bytes actually read
   void     (*skip)  (void *user,int n);                 // skip the next 'n' bytes, or 'unget' the last -n bytes if negative
   int      (*eof)   (void *user);                       // returns nonzero if we are at end of file/data
} stbi_io_callbacks;

////////////////////////////////////
//
// 8-bits-per-channel interface
//

STBIDEF stbi_uc *stbi_load_from_memory   (stbi_uc           const *buffer, int len   , int *x, int *y, int *channels_in_file, int desired_channels);
STBIDEF stbi_uc *stbi_load_from_callbacks(stbi_io_callbacks const *clbk  , void *user, int *x, int *y, int *channels_in_file, int desired_channels);

#ifndef STBI_NO_STDIO
STBIDEF stbi_uc *stbi_load            (char const *filename, int *x, int *y, int *channels_in_file, int desired_channels);
STBIDEF stbi_uc *stbi_load_from_file  (FILE *f, int *x, int *y, int *channels_in_file, int desired_channels);
// for stbi_load_from_file, file pointer is left pointing immediately after image
#endif

#ifndef STBI_NO_GIF
STBIDEF stbi_uc *stbi_load_gif_from_memory(stbi_uc const *buffer, int len, int **delays, int *x, int *y, int *z, int *comp, int req_comp);
#endif

#ifdef STBI_WINDOWS_UTF8
STBIDEF int stbi_convert_wchar_to_utf8(char *buffer, size_t bufferlen, const wchar_t* input);
#endif

////////////////////////////////////
//
// 16-bits-per-channel interface
//

STBIDEF stbi_us *stbi_load_16_from_memory   (stbi_uc const *buffer, int len, int *x, int *y, int *channels_in_file, int desired_channels);
STBIDEF stbi_us *stbi_load_16_from_callbacks(stbi_io_callbacks const *clbk, void *user, int *x, int *y, int *channels_in_file, int desired_channels);

#ifndef STBI_NO_STDIO
STBIDEF stbi_us *stbi_load_16          (char const *filename, int *x, int *y, int *channels_in_file, int desired_channels);
STBIDEF stbi_us *stbi_load_from_file_16(FILE *f, int *x, int *y, int *channels_in_file, int desired_channels);
#endif

////////////////////////////////////
//
// float-per-channel interface
//
#ifndef STBI_NO_LINEAR
   STBIDEF float *stbi_loadf_from_memory     (stbi_uc const *buffer, int len, int *x, int *y, int *channels_in_file, int desired_channels);
   STBIDEF float *stbi_loadf_from_callbacks  (stbi_io_callbacks const *clbk, void *user, int *x, int *y,  int *channels_in_file, int desired_channels);

   #ifndef STBI_NO_STDIO
   STBIDEF float *stbi_loadf            (char const *filename, int *x, int *y, int *channels_in_file, int desired_channels);
   STBIDEF float *stbi_loadf_from_file  (FILE *f, int *x, int *y, int *channels_in_file, int desired_channels);
   #endif
#endif

#ifndef STBI_NO_HDR
   STBIDEF void   stbi_hdr_to_ldr_gamma(float gamma);
   STBIDEF void   stbi_hdr_to_ldr_scale(float scale);
#endif // STBI_NO_HDR

#ifndef STBI_NO_LINEAR
   STBIDEF void   stbi_ldr_to_hdr_gamma(float gamma);
   STBIDEF void   stbi_ldr_to_hdr_scale(float scale);
#endif // STBI_NO_LINEAR

// stbi_is_hdr is always defined, but always returns false if STBI_NO_HDR
STBIDEF int    stbi_is_hdr_from_callbacks(stbi_io_callbacks const *clbk, void *user);
STBIDEF int    stbi_is_hdr_from_memory(stbi_uc const *buffer, int len);
#ifndef STBI_NO_STDIO
STBIDEF int      stbi_is_hdr          (char const *filename);
STBIDEF int      stbi_is_hdr_from_file(FILE *f);
#endif // STBI_NO_STDIO


// get a VERY brief reason for failure
// on most compilers (and ALL modern mainstream compilers) this is threadsafe
STBIDEF const char *stbi_failure_reason  (void);

// free the loaded image -- this is just free()
STBIDEF void     stbi_image_free      (void *retval_from_stbi_load);

// get image dimensions & components without fully decoding
STBIDEF int      stbi_info_from_memory(stbi_uc const *buffer, int len, int *x, int *y, int *comp);
STBIDEF int      stbi_info_from_callbacks(stbi_io_callbacks const *clbk, void *user, int *x, int *y, int *comp);
STBIDEF int      stbi_is_16_bit_from_memory(stbi_uc const *buffer, int len);
STBIDEF int      stbi_is_16_bit_from_callbacks(stbi_io_callbacks const *clbk, void *user);

#ifndef STBI_NO_STDIO
STBIDEF int      stbi_info               (char const *filename,     int *x, int *y, int *comp);
STBIDEF int      stbi_info_from_file     (FILE *f,                  int *x, int *y, int *comp);
STBIDEF int      stbi_is_16_bit          (char const *filename);
STBIDEF int      stbi_is_16_bit_from_file(FILE *f);
#endif



// for image formats that explicitly notate that they have premultiplied alpha,
// we just return the colors as stored in the file. set this flag to force
// unpremultiplication. results are undefined if the unpremultiply overflow.
STBIDEF void stbi_set_unpremultiply_on_load(int flag_true_if_should_unpremultiply);

// indicate whether we should process iphone images back to canonical format,
// or just pass them through "as-is"
STBIDEF void stbi_convert_iphone_png_to_rgb(int flag_true_if_should_convert);

// flip the image vertically, so the first pixel in the output array is the bottom left
STBIDEF void stbi_set_flip_vertically_on_load(int flag_true_if_should_flip);

// as above, but only applies to images loaded on the thread that calls the function
// this function is only available if your compiler supports thread-local variables;
// calling it will fail to link if your compiler doesn't
STBIDEF void stbi_set_unpremultiply_on_load_thread(int flag_true_if_should_unpremultiply);
STBIDEF void stbi_convert_iphone_png_to_rgb_thread(int flag_true_if_should_convert);
STBIDEF void stbi_set_flip_vertically_on_load_thread(int flag_true_if_should_flip);

// ZLIB client - used by PNG, available for other purposes

STBIDEF char *stbi_zlib_decode_malloc_guesssize(const char *buffer, int len, int initial_size, int *outlen);
STBIDEF char *stbi_zlib_decode_malloc_guesssize_headerflag(const char *buffer, int len, int initial_size, int *outlen, int parse_header);
STBIDEF char *stbi_zlib_decode_malloc(const char *buffer, int len, int *outlen);
STBIDEF int   stbi_zlib_decode_buffer(char *obuffer, int olen, const char *ibuffer, int ilen);

STBIDEF char *stbi_zlib_decode_noheader_malloc(const char *buffer, int len, int *outlen);
STBIDEF int   stbi_zlib_decode_noheader_buffer(char *obuffer, int olen, const char *ibuffer, int ilen);


#ifdef __cplusplus
}
#endif

//
//
////   end header file   /////////////////////////////////////////////////////
#endif // STBI_INCLUDE_STB_IMAGE_H

#ifdef STB_IMAGE_IMPLEMENTATION

#if defined(STBI_ONLY_JPEG) || defined(STBI_ONLY_PNG) || defined(STBI_ONLY_BMP) \
  || defined(STBI_ONLY_TGA) || defined(STBI_ONLY_GIF) || defined(STBI_ONLY_PSD) \
  || defined(STBI_ONLY_HDR) || defined(STBI_ONLY_PIC) || defined(STBI_ONLY_PNM) \
  || defined(STBI_ONLY_ZLIB)
   #ifndef STBI_ONLY_JPEG
   #define STBI_NO_JPEG
   #endif
   #ifndef STBI_ONLY_PNG
   #define STBI_NO_PNG
   #endif
   #ifndef STBI_ONLY_BMP
   #define STBI_NO_BMP
   #endif
   #ifndef STBI_ONLY_PSD
   #define STBI_NO_PSD
   #endif
   #ifndef STBI_ONLY_TGA
   #define STBI_NO_TGA
   #endif
   #ifndef STBI_ONLY_GIF
   #define STBI_NO_GIF
   #endif
   #ifndef STBI_ONLY_HDR
   #define STBI_NO_HDR
   #endif
   #ifndef STBI_ONLY_PIC
   #define STBI_NO_PIC
   #endif
   #ifndef STBI_ONLY_PNM
   #define STBI_NO_PNM
   #endif
#endif

#if defined(STBI_NO_PNG) && !defined(STBI_SUPPORT_ZLIB) && !defined(STBI_NO_ZLIB)
#define STBI_NO_ZLIB
#endif


#include <stdarg.h>
#include <stddef.h> // ptrdiff_t on osx
#include <stdlib.h>
#include <string.h>
#include <limits.h>

#if !defined(STBI_NO_LINEAR) || !defined(STBI_NO_HDR)
#include <math.h>  // ldexp, pow
#endif

#ifndef STBI_NO_STDIO
#include <stdio.h>
#endif

#ifndef STBI_ASSERT
#include <assert.h>
#define STBI_ASSERT(x) assert(x)
#endif

#ifdef __cplusplus
#define STBI_EXTERN extern "C"
#else
#define STBI_EXTERN extern
#endif


#ifndef _MSC_VER
   #ifdef __cplusplus
   #define stbi_inline inline
   #else
   #define stbi_inline
   #endif
#else
   #define stbi_inline __forceinline
#endif

#ifndef STBI_NO_THREAD_LOCALS
   #if defined(__cplusplus) &&  __cplusplus >= 201103L
      #define STBI_THREAD_LOCAL       thread_local
   #elif defined(__GNUC__) && __GNUC__ < 5
      #define STBI_THREAD_LOCAL       __thread
   #elif defined(_MSC_VER)
      #define STBI_THREAD_LOCAL       __declspec(thread)
   #elif defined (__STDC_VERSION__) && __STDC_VERSION__ >= 201112L && !defined(__STDC_NO_THREADS__)
      #define STBI_THREAD_LOCAL       _Thread_local
   #endif

   #ifndef STBI_THREAD_LOCAL
      #if defined(__GNUC__)
        #define STBI_THREAD_LOCAL       __thread
      #endif
   #endif
#endif

#ifdef _MSC_VER
typedef unsigned short stbi__uint16;
typedef   signed short stbi__int16;
typedef unsigned int   stbi__uint32;
typedef   signed int   stbi__int32;
#else
#include <stdint.h>
typedef uint16_t stbi__uint16;
typedef int16_t  stbi__int16;
typedef uint32_t stbi__uint32;
typedef int32_t  stbi__int32;
#endif

// should produce compiler error if size is wrong
typedef unsigned char validate_uint32[sizeof(stbi__uint32)==4 ? 1 : -1];

#ifdef _MSC_VER
#define STBI_NOTUSED(v)  (void)(v)
#else
#define STBI_NOTUSED(v)  (void)sizeof(v)
#endif

#ifdef _MSC_VER
#define STBI_HAS_LROTL
#endif

#ifdef STBI_HAS_LROTL
   #define stbi_lrot(x,y)  _lrotl(x,y)
#else
   #define stbi_lrot(x,y)  (((x) << (y)) | ((x) >> (-(y) & 31)))
#endif

#if defined(STBI_MALLOC) && defined(STBI_FREE) && (defined(STBI_REALLOC) || defined(STBI_REALLOC_SIZED))
// ok
#elif !defined(STBI_MALLOC) && !defined(STBI_FREE) && !defined(STBI_REALLOC) && !defined(STBI_REALLOC_SIZED)
// ok
#else
#error "Must define all or none of STBI_MALLOC, STBI_FREE, and STBI_REALLOC (or STBI_REALLOC_SIZED)."
#endif

#ifndef STBI_MALLOC
#define STBI_MALLOC(sz)           malloc(sz)
#define STBI_REALLOC(p,newsz)     realloc(p,newsz)
#define STBI_FREE(p)              free(p)
#endif

#ifndef STBI_REALLOC_SIZED
#define STBI_REALLOC_SIZED(p,oldsz,newsz) STBI_REALLOC(p,newsz)
#endif

// x86/x64 detection
#if defined(__x86_64__) || defined(_M_X64)
#define STBI__X64_TARGET
#elif defined(__i386) || defined(_M_IX86)
#define STBI__X86_TARGET
#endif

#if defined(__GNUC__) && defined(STBI__X86_TARGET) && !defined(__SSE2__) && !defined(STBI_NO_SIMD)
// gcc doesn't support sse2 intrinsics unless you compile with -msse2,
// which in turn means it gets to use SSE2 everywhere. This is unfortunate,
// but previous attempts to provide the SSE2 functions with runtime
// detection caused numerous issues. The way architecture extensions are
// exposed in GCC/Clang is, sadly, not really suited for one-file libs.
// New behavior: if compiled with -msse2, we use SSE2 without any
// detection; if not, we don't use it at all.
#define STBI_NO_SIMD
#endif

#if defined(__MINGW32__) && defined(STBI__X86_TARGET) && !defined(STBI_MINGW_ENABLE_SSE2) && !defined(STBI_NO_SIMD)
// Note that __MINGW32__ doesn't actually mean 32-bit, so we have to avoid STBI__X64_TARGET
//
// 32-bit MinGW wants ESP to be 16-byte aligned, but this is not in the
// Windows ABI and VC++ as well as Windows DLLs don't maintain that invariant.
// As a result, enabling SSE2 on 32-bit MinGW is dangerous when not
// simultaneously enabling "-mstackrealign".
//
// See https://github.com/nothings/stb/issues/81 for more information.
//
// So default to no SSE2 on 32-bit MinGW. If you've read this far and added
// -mstackrealign to your build settings, feel free to #define STBI_MINGW_ENABLE_SSE2.
#define STBI_NO_SIMD
#endif

#if !defined(STBI_NO_SIMD) && (defined(STBI__X86_TARGET) || defined(STBI__X64_TARGET))
#define STBI_SSE2
#include <emmintrin.h>

#ifdef _MSC_VER

#if _MSC_VER >= 1400  // not VC6
#include <intrin.h> // __cpuid
static int stbi__cpuid3(void)
{
   int info[4];
   __cpuid(info,1);
   return info[3];
}
#else
static int stbi__cpuid3(void)
{
   int res;
   __asm {
      mov  eax,1
      cpuid
      mov  res,edx
   }
   return res;
}
#endif

#define STBI_SIMD_ALIGN(type, name) __declspec(align(16)) type name

#if !defined(STBI_NO_JPEG) && defined(STBI_SSE2)
static int stbi__sse2_available(void)
{
   int info3 = stbi__cpuid3();
   return ((info3 >> 26) & 1) != 0;
}
#endif

#else // assume GCC-style if not VC++
#define STBI_SIMD_ALIGN(type, name) type name __attribute__((aligned(16)))

#if !defined(STBI_NO_JPEG) && defined(STBI_SSE2)
static int stbi__sse2_available(void)
{
   // If we're even attempting to compile this on GCC/Clang, that means
   // -msse2 is on, which means the compiler is allowed to use SSE2
   // instructions at will, and so are we.
   return 1;
}
#endif

#endif
#endif

// ARM NEON
#if defined(STBI_NO_SIMD) && defined(STBI_NEON)
#undef STBI_NEON
#endif

#ifdef STBI_NEON
#include <arm_neon.h>
#ifdef _MSC_VER
#define STBI_SIMD_ALIGN(type, name) __declspec(align(16)) type name
#else
#define STBI_SIMD_ALIGN(type, name) type name __attribute__((aligned(16)))
#endif
#endif

#ifndef STBI_SIMD_ALIGN
#define STBI_SIMD_ALIGN(type, name) type name
#endif

#ifndef STBI_MAX_DIMENSIONS
#define STBI_MAX_DIMENSIONS (1 << 24)
#endif

///////////////////////////////////////////////
//
//  stbi__context struct and start_xxx functions

// stbi__context structure is our basic context used by all images, so it
// contains all the IO context, plus some basic image information
typedef struct
{
   stbi__uint32 img_x, img_y;
   int img_n, img_out_n;

   stbi_io_callbacks io;
   void *io_user_data;

   int read_from_callbacks;
   int buflen;
   stbi_uc buffer_start[128];
   int callback_already_read;

   stbi_uc *img_buffer, *img_buffer_end;
   stbi_uc *img_buffer_original, *img_buffer_original_end;
} stbi__context;


static void stbi__refill_buffer(stbi__context *s);

// initialize a memory-decode context
static void stbi__start_mem(stbi__context *s, stbi_uc const *buffer, int len)
{
   s->io.read = NULL;
   s->read_from_callbacks = 0;
   s->callback_already_read = 0;
   s->img_buffer = s->img_buffer_original = (stbi_uc *) buffer;
   s->img_buffer_end = s->img_buffer_original_end = (stbi_uc *) buffer+len;
}

// initialize a callback-based context
static void stbi__start_callbacks(stbi__context *s, stbi_io_callbacks *c, void *user)
{
   s->io = *c;
   s->io_user_data = user;
   s->buflen = sizeof(s->buffer_start);
   s->read_from_callbacks = 1;
   s->callback_already_read = 0;
   s->img_buffer = s->img_buffer_original = s->buffer_start;
   stbi__refill_buffer(s);
   s->img_buffer_original_end = s->img_buffer_end;
}

#ifndef STBI_NO_STDIO

static int stbi__stdio_read(void *user, char *data, int size)
{
   return (int) fread(data,1,size,(FILE*) user);
}

static void stbi__stdio_skip(void *user, int n)
{
   int ch;
   fseek((FILE*) user, n, SEEK_CUR);
   ch = fgetc((FILE*) user);  /* have to read a byte to reset feof()'s flag */
   if (ch != EOF) {
      ungetc(ch, (FILE *) user);  /* push byte back onto stream if valid. */
   }
}

static int stbi__stdio_eof(void *user)
{
   return feof((FILE*) user) || ferror((FILE *) user);
}

static stbi_io_callbacks stbi__stdio_callbacks =
{
   stbi__stdio_read,
   stbi__stdio_skip,
   stbi__stdio_eof,
};

static void stbi__start_file(stbi__context *s, FILE *f)
{
   stbi__start_callbacks(s, &stbi__stdio_callbacks, (void *) f);
}

//static void stop_file(stbi__context *s) { }

#endif // !STBI_NO_STDIO

static void stbi__rewind(stbi__context *s)
{
   // conceptually rewind SHOULD rewind to the beginning of the stream,
   // but we just rewind to the beginning of the initial buffer, because
   // we only use it after doing 'test', which only ever looks at at most 92 bytes
   s->img_buffer = s->img_buffer_original;
   s->img_buffer_end = s->img_buffer_original_end;
}

enum
{
   STBI_ORDER_RGB,
   STBI_ORDER_BGR
};

typedef struct
{
   int bits_per_channel;
   int num_channels;
   int channel_order;
} stbi__result_info;

#ifndef STBI_NO_JPEG
static int      stbi__jpeg_test(stbi__context *s);
static void    *stbi__jpeg_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static int      stbi__jpeg_info(stbi__context *s, int *x, int *y, int *comp);
#endif

#ifndef STBI_NO_PNG
static int      stbi__png_test(stbi__context *s);
static void    *stbi__png_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static int      stbi__png_info(stbi__context *s, int *x, int *y, int *comp);
static int      stbi__png_is16(stbi__context *s);
#endif

#ifndef STBI_NO_BMP
static int      stbi__bmp_test(stbi__context *s);
static void    *stbi__bmp_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static int      stbi__bmp_info(stbi__context *s, int *x, int *y, int *comp);
#endif

#ifndef STBI_NO_TGA
static int      stbi__tga_test(stbi__context *s);
static void    *stbi__tga_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static int      stbi__tga_info(stbi__context *s, int *x, int *y, int *comp);
#endif

#ifndef STBI_NO_PSD
static int      stbi__psd_test(stbi__context *s);
static void    *stbi__psd_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri, int bpc);
static int      stbi__psd_info(stbi__context *s, int *x, int *y, int *comp);
static int      stbi__psd_is16(stbi__context *s);
#endif

#ifndef STBI_NO_HDR
static int      stbi__hdr_test(stbi__context *s);
static float   *stbi__hdr_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static int      stbi__hdr_info(stbi__context *s, int *x, int *y, int *comp);
#endif

#ifndef STBI_NO_PIC
static int      stbi__pic_test(stbi__context *s);
static void    *stbi__pic_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static int      stbi__pic_info(stbi__context *s, int *x, int *y, int *comp);
#endif

#ifndef STBI_NO_GIF
static int      stbi__gif_test(stbi__context *s);
static void    *stbi__gif_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static void    *stbi__load_gif_main(stbi__context *s, int **delays, int *x, int *y, int *z, int *comp, int req_comp);
static int      stbi__gif_info(stbi__context *s, int *x, int *y, int *comp);
#endif

#ifndef STBI_NO_PNM
static int      stbi__pnm_test(stbi__context *s);
static void    *stbi__pnm_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri);
static int      stbi__pnm_info(stbi__context *s, int *x, int *y, int *comp);
static int      stbi__pnm_is16(stbi__context *s);
#endif

static
#ifdef STBI_THREAD_LOCAL
STBI_THREAD_LOCAL
#endif
const char *stbi__g_failure_reason;

STBIDEF const char *stbi_failure_reason(void)
{
   return stbi__g_failure_reason;
}

#ifndef STBI_NO_FAILURE_STRINGS
static int stbi__err(const char *str)
{
   stbi__g_failure_reason = str;
   return 0;
}
#endif

static void *stbi__malloc(size_t size)
{
    return STBI_MALLOC(size);
}

// stb_image uses ints pervasively, including for offset calculations.
// therefore the largest decoded image size we can support with the
// current code, even on 64-bit targets, is INT_MAX. this is not a
// significant limitation for the intended use case.
//
// we do, however, need to make sure our size calculations don't
// overflow. hence a few helper functions for size calculations that
// multiply integers together, making sure that they're non-negative
// and no overflow occurs.

// return 1 if the sum is valid, 0 on overflow.
// negative terms are considered invalid.
static int stbi__addsizes_valid(int a, int b)
{
   if (b < 0) return 0;
   // now 0 <= b <= INT_MAX, hence also
   // 0 <= INT_MAX - b <= INTMAX.
   // And "a + b <= INT_MAX" (which might overflow) is the
   // same as a <= INT_MAX - b (no overflow)
   return a <= INT_MAX - b;
}

// returns 1 if the product is valid, 0 on overflow.
// negative factors are considered invalid.
static int stbi__mul2sizes_valid(int a, int b)
{
   if (a < 0 || b < 0) return 0;
   if (b == 0) return 1; // mul-by-0 is always safe
   // portable way to check for no overflows in a*b
   return a <= INT_MAX/b;
}

#if !defined(STBI_NO_JPEG) || !defined(STBI_NO_PNG) || !defined(STBI_NO_TGA) || !defined(STBI_NO_HDR)
// returns 1 if "a*b + add" has no negative terms/factors and doesn't overflow
static int stbi__mad2sizes_valid(int a, int b, int add)
{
   return stbi__mul2sizes_valid(a, b) && stbi__addsizes_valid(a*b, add);
}
#endif

// returns 1 if "a*b*c + add" has no negative terms/factors and doesn't overflow
static int stbi__mad3sizes_valid(int a, int b, int c, int add)
{
   return stbi__mul2sizes_valid(a, b) && stbi__mul2sizes_valid(a*b, c) &&
      stbi__addsizes_valid(a*b*c, add);
}

// returns 1 if "a*b*c*d + add" has no negative terms/factors and doesn't overflow
#if !defined(STBI_NO_LINEAR) || !defined(STBI_NO_HDR) || !defined(STBI_NO_PNM)
static int stbi__mad4sizes_valid(int a, int b, int c, int d, int add)
{
   return stbi__mul2sizes_valid(a, b) && stbi__mul2sizes_valid(a*b, c) &&
      stbi__mul2sizes_valid(a*b*c, d) && stbi__addsizes_valid(a*b*c*d, add);
}
#endif

#if !defined(STBI_NO_JPEG) || !defined(STBI_NO_PNG) || !defined(STBI_NO_TGA) || !defined(STBI_NO_HDR)
// mallocs with size overflow checking
static void *stbi__malloc_mad2(int a, int b, int add)
{
   if (!stbi__mad2sizes_valid(a, b, add)) return NULL;
   return stbi__malloc(a*b + add);
}
#endif

static void *stbi__malloc_mad3(int a, int b, int c, int add)
{
   if (!stbi__mad3sizes_valid(a, b, c, add)) return NULL;
   return stbi__malloc(a*b*c + add);
}

#if !defined(STBI_NO_LINEAR) || !defined(STBI_NO_HDR) || !defined(STBI_NO_PNM)
static void *stbi__malloc_mad4(int a, int b, int c, int d, int add)
{
   if (!stbi__mad4sizes_valid(a, b, c, d, add)) return NULL;
   return stbi__malloc(a*b*c*d + add);
}
#endif

// stbi__err - error
// stbi__errpf - error returning pointer to float
// stbi__errpuc - error returning pointer to unsigned char

#ifdef STBI_NO_FAILURE_STRINGS
   #define stbi__err(x,y)  0
#elif defined(STBI_FAILURE_USERMSG)
   #define stbi__err(x,y)  stbi__err(y)
#else
   #define stbi__err(x,y)  stbi__err(x)
#endif

#define stbi__errpf(x,y)   ((float *)(size_t) (stbi__err(x,y)?NULL:NULL))
#define stbi__errpuc(x,y)  ((unsigned char *)(size_t) (stbi__err(x,y)?NULL:NULL))

STBIDEF void stbi_image_free(void *retval_from_stbi_load)
{
   STBI_FREE(retval_from_stbi_load);
}

#ifndef STBI_NO_LINEAR
static float   *stbi__ldr_to_hdr(stbi_uc *data, int x, int y, int comp);
#endif

#ifndef STBI_NO_HDR
static stbi_uc *stbi__hdr_to_ldr(float   *data, int x, int y, int comp);
#endif

static int stbi__vertically_flip_on_load_global = 0;

STBIDEF void stbi_set_flip_vertically_on_load(int flag_true_if_should_flip)
{
   stbi__vertically_flip_on_load_global = flag_true_if_should_flip;
}

#ifndef STBI_THREAD_LOCAL
#define stbi__vertically_flip_on_load  stbi__vertically_flip_on_load_global
#else
static STBI_THREAD_LOCAL int stbi__vertically_flip_on_load_local, stbi__vertically_flip_on_load_set;

STBIDEF void stbi_set_flip_vertically_on_load_thread(int flag_true_if_should_flip)
{
   stbi__vertically_flip_on_load_local = flag_true_if_should_flip;
   stbi__vertically_flip_on_load_set = 1;
}

#define stbi__vertically_flip_on_load  (stbi__vertically_flip_on_load_set       \
                                         ? stbi__vertically_flip_on_load_local  \
                                         : stbi__vertically_flip_on_load_global)
#endif // STBI_THREAD_LOCAL

static void *stbi__load_main(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri, int bpc)
{
   memset(ri, 0, sizeof(*ri)); // make sure it's initialized if we add new fields
   ri->bits_per_channel = 8; // default is 8 so most paths don't have to be changed
   ri->channel_order = STBI_ORDER_RGB; // all current input & output are this, but this is here so we can add BGR order
   ri->num_channels = 0;

   // test the formats with a very explicit header first (at least a FOURCC
   // or distinctive magic number first)
   #ifndef STBI_NO_PNG
   if (stbi__png_test(s))  return stbi__png_load(s,x,y,comp,req_comp, ri);
   #endif
   #ifndef STBI_NO_BMP
   if (stbi__bmp_test(s))  return stbi__bmp_load(s,x,y,comp,req_comp, ri);
   #endif
   #ifndef STBI_NO_GIF
   if (stbi__gif_test(s))  return stbi__gif_load(s,x,y,comp,req_comp, ri);
   #endif
   #ifndef STBI_NO_PSD
   if (stbi__psd_test(s))  return stbi__psd_load(s,x,y,comp,req_comp, ri, bpc);
   #else
   STBI_NOTUSED(bpc);
   #endif
   #ifndef STBI_NO_PIC
   if (stbi__pic_test(s))  return stbi__pic_load(s,x,y,comp,req_comp, ri);
   #endif

   // then the formats that can end up attempting to load with just 1 or 2
   // bytes matching expectations; these are prone to false positives, so
   // try them later
   #ifndef STBI_NO_JPEG
   if (stbi__jpeg_test(s)) return stbi__jpeg_load(s,x,y,comp,req_comp, ri);
   #endif
   #ifndef STBI_NO_PNM
   if (stbi__pnm_test(s))  return stbi__pnm_load(s,x,y,comp,req_comp, ri);
   #endif

   #ifndef STBI_NO_HDR
   if (stbi__hdr_test(s)) {
      float *hdr = stbi__hdr_load(s, x,y,comp,req_comp, ri);
      return stbi__hdr_to_ldr(hdr, *x, *y, req_comp ? req_comp : *comp);
   }
   #endif

   #ifndef STBI_NO_TGA
   // test tga last because it's a crappy test!
   if (stbi__tga_test(s))
      return stbi__tga_load(s,x,y,comp,req_comp, ri);
   #endif

   return stbi__errpuc("unknown image type", "Image not of any known type, or corrupt");
}

static stbi_uc *stbi__convert_16_to_8(stbi__uint16 *orig, int w, int h, int channels)
{
   int i;
   int img_len = w * h * channels;
   stbi_uc *reduced;

   reduced = (stbi_uc *) stbi__malloc(img_len);
   if (reduced == NULL) return stbi__errpuc("outofmem", "Out of memory");

   for (i = 0; i < img_len; ++i)
      reduced[i] = (stbi_uc)((orig[i] >> 8) & 0xFF); // top half of each byte is sufficient approx of 16->8 bit scaling

   STBI_FREE(orig);
   return reduced;
}

static stbi__uint16 *stbi__convert_8_to_16(stbi_uc *orig, int w, int h, int channels)
{
   int i;
   int img_len = w * h * channels;
   stbi__uint16 *enlarged;

   enlarged = (stbi__uint16 *) stbi__malloc(img_len*2);
   if (enlarged == NULL) return (stbi__uint16 *) stbi__errpuc("outofmem", "Out of memory");

   for (i = 0; i < img_len; ++i)
      enlarged[i] = (stbi__uint16)((orig[i] << 8) + orig[i]); // replicate to high and low byte, maps 0->0, 255->0xffff

   STBI_FREE(orig);
   return enlarged;
}

static void stbi__vertical_flip(void *image, int w, int h, int bytes_per_pixel)
{
   int row;
   size_t bytes_per_row = (size_t)w * bytes_per_pixel;
   stbi_uc temp[2048];
   stbi_uc *bytes = (stbi_uc *)image;

   for (row = 0; row < (h>>1); row++) {
      stbi_uc *row0 = bytes + row*bytes_per_row;
      stbi_uc *row1 = bytes + (h - row - 1)*bytes_per_row;
      // swap row0 with row1
      size_t bytes_left = bytes_per_row;
      while (bytes_left) {
         size_t bytes_copy = (bytes_left < sizeof(temp)) ? bytes_left : sizeof(temp);
         memcpy(temp, row0, bytes_copy);
         memcpy(row0, row1, bytes_copy);
         memcpy(row1, temp, bytes_copy);
         row0 += bytes_copy;
         row1 += bytes_copy;
         bytes_left -= bytes_copy;
      }
   }
}

#ifndef STBI_NO_GIF
static void stbi__vertical_flip_slices(void *image, int w, int h, int z, int bytes_per_pixel)
{
   int slice;
   int slice_size = w * h * bytes_per_pixel;

   stbi_uc *bytes = (stbi_uc *)image;
   for (slice = 0; slice < z; ++slice) {
      stbi__vertical_flip(bytes, w, h, bytes_per_pixel);
      bytes += slice_size;
   }
}
#endif

static unsigned char *stbi__load_and_postprocess_8bit(stbi__context *s, int *x, int *y, int *comp, int req_comp)
{
   stbi__result_info ri;
   void *result = stbi__load_main(s, x, y, comp, req_comp, &ri, 8);

   if (result == NULL)
      return NULL;

   // it is the responsibility of the loaders to make sure we get either 8 or 16 bit.
   STBI_ASSERT(ri.bits_per_channel == 8 || ri.bits_per_channel == 16);

   if (ri.bits_per_channel != 8) {
      result = stbi__convert_16_to_8((stbi__uint16 *) result, *x, *y, req_comp == 0 ? *comp : req_comp);
      ri.bits_per_channel = 8;
   }

   // @TODO: move stbi__convert_format to here

   if (stbi__vertically_flip_on_load) {
      int channels = req_comp ? req_comp : *comp;
      stbi__vertical_flip(result, *x, *y, channels * sizeof(stbi_uc));
   }

   return (unsigned char *) result;
}

static stbi__uint16 *stbi__load_and_postprocess_16bit(stbi__context *s, int *x, int *y, int *comp, int req_comp)
{
   stbi__result_info ri;
   void *result = stbi__load_main(s, x, y, comp, req_comp, &ri, 16);

   if (result == NULL)
      return NULL;

   // it is the responsibility of the loaders to make sure we get either 8 or 16 bit.
   STBI_ASSERT(ri.bits_per_channel == 8 || ri.bits_per_channel == 16);

   if (ri.bits_per_channel != 16) {
      result = stbi__convert_8_to_16((stbi_uc *) result, *x, *y, req_comp == 0 ? *comp : req_comp);
      ri.bits_per_channel = 16;
   }

   // @TODO: move stbi__convert_format16 to here
   // @TODO: special case RGB-to-Y (and RGBA-to-YA) for 8-bit-to-16-bit case to keep more precision

   if (stbi__vertically_flip_on_load) {
      int channels = req_comp ? req_comp : *comp;
      stbi__vertical_flip(result, *x, *y, channels * sizeof(stbi__uint16));
   }

   return (stbi__uint16 *) result;
}

#if !defined(STBI_NO_HDR) && !defined(STBI_NO_LINEAR)
static void stbi__float_postprocess(float *result, int *x, int *y, int *comp, int req_comp)
{
   if (stbi__vertically_flip_on_load && result != NULL) {
      int channels = req_comp ? req_comp : *comp;
      stbi__vertical_flip(result, *x, *y, channels * sizeof(float));
   }
}
#endif

#ifndef STBI_NO_STDIO

#if defined(_WIN32) && defined(STBI_WINDOWS_UTF8)
STBI_EXTERN __declspec(dllimport) int __stdcall MultiByteToWideChar(unsigned int cp, unsigned long flags, const char *str, int cbmb, wchar_t *widestr, int cchwide);
STBI_EXTERN __declspec(dllimport) int __stdcall WideCharToMultiByte(unsigned int cp, unsigned long flags, const wchar_t *widestr, int cchwide, char *str, int cbmb, const char *defchar, int *used_default);
#endif

#if defined(_WIN32) && defined(STBI_WINDOWS_UTF8)
STBIDEF int stbi_convert_wchar_to_utf8(char *buffer, size_t bufferlen, const wchar_t* input)
{
	return WideCharToMultiByte(65001 /* UTF8 */, 0, input, -1, buffer, (int) bufferlen, NULL, NULL);
}
#endif

static FILE *stbi__fopen(char const *filename, char const *mode)
{
   FILE *f;
#if defined(_WIN32) && defined(STBI_WINDOWS_UTF8)
   wchar_t wMode[64];
   wchar_t wFilename[1024];
	if (0 == MultiByteToWideChar(65001 /* UTF8 */, 0, filename, -1, wFilename, sizeof(wFilename)/sizeof(*wFilename)))
      return 0;

	if (0 == MultiByteToWideChar(65001 /* UTF8 */, 0, mode, -1, wMode, sizeof(wMode)/sizeof(*wMode)))
      return 0;

#if defined(_MSC_VER) && _MSC_VER >= 1400
	if (0 != _wfopen_s(&f, wFilename, wMode))
		f = 0;
#else
   f = _wfopen(wFilename, wMode);
#endif

#elif defined(_MSC_VER) && _MSC_VER >= 1400
   if (0 != fopen_s(&f, filename, mode))
      f=0;
#else
   f = fopen(filename, mode);
#endif
   return f;
}


STBIDEF stbi_uc *stbi_load(char const *filename, int *x, int *y, int *comp, int req_comp)
{
   FILE *f = stbi__fopen(filename, "rb");
   unsigned char *result;
   if (!f) return stbi__errpuc("can't fopen", "Unable to open file");
   result = stbi_load_from_file(f,x,y,comp,req_comp);
   fclose(f);
   return result;
}

STBIDEF stbi_uc *stbi_load_from_file(FILE *f, int *x, int *y, int *comp, int req_comp)
{
   unsigned char *result;
   stbi__context s;
   stbi__start_file(&s,f);
   result = stbi__load_and_postprocess_8bit(&s,x,y,comp,req_comp);
   if (result) {
      // need to 'unget' all the characters in the IO buffer
      fseek(f, - (int) (s.img_buffer_end - s.img_buffer), SEEK_CUR);
   }
   return result;
}

STBIDEF stbi__uint16 *stbi_load_from_file_16(FILE *f, int *x, int *y, int *comp, int req_comp)
{
   stbi__uint16 *result;
   stbi__context s;
   stbi__start_file(&s,f);
   result = stbi__load_and_postprocess_16bit(&s,x,y,comp,req_comp);
   if (result) {
      // need to 'unget' all the characters in the IO buffer
      fseek(f, - (int) (s.img_buffer_end - s.img_buffer), SEEK_CUR);
   }
   return result;
}

STBIDEF stbi_us *stbi_load_16(char const *filename, int *x, int *y, int *comp, int req_comp)
{
   FILE *f = stbi__fopen(filename, "rb");
   stbi__uint16 *result;
   if (!f) return (stbi_us *) stbi__errpuc("can't fopen", "Unable to open file");
   result = stbi_load_from_file_16(f,x,y,comp,req_comp);
   fclose(f);
   return result;
}


#endif //!STBI_NO_STDIO

STBIDEF stbi_us *stbi_load_16_from_memory(stbi_uc const *buffer, int len, int *x, int *y, int *channels_in_file, int desired_channels)
{
   stbi__context s;
   stbi__start_mem(&s,buffer,len);
   return stbi__load_and_postprocess_16bit(&s,x,y,channels_in_file,desired_channels);
}

STBIDEF stbi_us *stbi_load_16_from_callbacks(stbi_io_callbacks const *clbk, void *user, int *x, int *y, int *channels_in_file, int desired_channels)
{
   stbi__context s;
   stbi__start_callbacks(&s, (stbi_io_callbacks *)clbk, user);
   return stbi__load_and_postprocess_16bit(&s,x,y,channels_in_file,desired_channels);
}

STBIDEF stbi_uc *stbi_load_from_memory(stbi_uc const *buffer, int len, int *x, int *y, int *comp, int req_comp)
{
   stbi__context s;
   stbi__start_mem(&s,buffer,len);
   return stbi__load_and_postprocess_8bit(&s,x,y,comp,req_comp);
}

STBIDEF stbi_uc *stbi_load_from_callbacks(stbi_io_callbacks const *clbk, void *user, int *x, int *y, int *comp, int req_comp)
{
   stbi__context s;
   stbi__start_callbacks(&s, (stbi_io_callbacks *) clbk, user);
   return stbi__load_and_postprocess_8bit(&s,x,y,comp,req_comp);
}

#ifndef STBI_NO_GIF
STBIDEF stbi_uc *stbi_load_gif_from_memory(stbi_uc const *buffer, int len, int **delays, int *x, int *y, int *z, int *comp, int req_comp)
{
   unsigned char *result;
   stbi__context s;
   stbi__start_mem(&s,buffer,len);

   result = (unsigned char*) stbi__load_gif_main(&s, delays, x, y, z, comp, req_comp);
   if (stbi__vertically_flip_on_load) {
      stbi__vertical_flip_slices( result, *x, *y, *z, *comp );
   }

   return result;
}
#endif

#ifndef STBI_NO_LINEAR
static float *stbi__loadf_main(stbi__context *s, int *x, int *y, int *comp, int req_comp)
{
   unsigned char *data;
   #ifndef STBI_NO_HDR
   if (stbi__hdr_test(s)) {
      stbi__result_info ri;
      float *hdr_data = stbi__hdr_load(s,x,y,comp,req_comp, &ri);
      if (hdr_data)
         stbi__float_postprocess(hdr_data,x,y,comp,req_comp);
      return hdr_data;
   }
   #endif
   data = stbi__load_and_postprocess_8bit(s, x, y, comp, req_comp);
   if (data)
      return stbi__ldr_to_hdr(data, *x, *y, req_comp ? req_comp : *comp);
   return stbi__errpf("unknown image type", "Image not of any known type, or corrupt");
}

STBIDEF float *stbi_loadf_from_memory(stbi_uc const *buffer, int len, int *x, int *y, int *comp, int req_comp)
{
   stbi__context s;
   stbi__start_mem(&s,buffer,len);
   return stbi__loadf_main(&s,x,y,comp,req_comp);
}

STBIDEF float *stbi_loadf_from_callbacks(stbi_io_callbacks const *clbk, void *user, int *x, int *y, int *comp, int req_comp)
{
   stbi__context s;
   stbi__start_callbacks(&s, (stbi_io_callbacks *) clbk, user);
   return stbi__loadf_main(&s,x,y,comp,req_comp);
}

#ifndef STBI_NO_STDIO
STBIDEF float *stbi_loadf(char const *filename, int *x, int *y, int *comp, int req_comp)
{
   float *result;
   FILE *f = stbi__fopen(filename, "rb");
   if (!f) return stbi__errpf("can't fopen", "Unable to open file");
   result = stbi_loadf_from_file(f,x,y,comp,req_comp);
   fclose(f);
   return result;
}

STBIDEF float *stbi_loadf_from_file(FILE *f, int *x, int *y, int *comp, int req_comp)
{
   stbi__context s;
   stbi__start_file(&s,f);
   return stbi__loadf_main(&s,x,y,comp,req_comp);
}
#endif // !STBI_NO_STDIO

#endif // !STBI_NO_LINEAR

// these is-hdr-or-not is defined independent of whether STBI_NO_LINEAR is
// defined, for API simplicity; if STBI_NO_LINEAR is defined, it always
// reports false!

STBIDEF int stbi_is_hdr_from_memory(stbi_uc const *buffer, int len)
{
   #ifndef STBI_NO_HDR
   stbi__context s;
   stbi__start_mem(&s,buffer,len);
   return stbi__hdr_test(&s);
   #else
   STBI_NOTUSED(buffer);
   STBI_NOTUSED(len);
   return 0;
   #endif
}

#ifndef STBI_NO_STDIO
STBIDEF int      stbi_is_hdr          (char const *filename)
{
   FILE *f = stbi__fopen(filename, "rb");
   int result=0;
   if (f) {
      result = stbi_is_hdr_from_file(f);
      fclose(f);
   }
   return result;
}

STBIDEF int stbi_is_hdr_from_file(FILE *f)
{
   #ifndef STBI_NO_HDR
   long pos = ftell(f);
   int res;
   stbi__context s;
   stbi__start_file(&s,f);
   res = stbi__hdr_test(&s);
   fseek(f, pos, SEEK_SET);
   return res;
   #else
   STBI_NOTUSED(f);
   return 0;
   #endif
}
#endif // !STBI_NO_STDIO

STBIDEF int      stbi_is_hdr_from_callbacks(stbi_io_callbacks const *clbk, void *user)
{
   #ifndef STBI_NO_HDR
   stbi__context s;
   stbi__start_callbacks(&s, (stbi_io_callbacks *) clbk, user);
   return stbi__hdr_test(&s);
   #else
   STBI_NOTUSED(clbk);
   STBI_NOTUSED(user);
   return 0;
   #endif
}

#ifndef STBI_NO_LINEAR
static float stbi__l2h_gamma=2.2f, stbi__l2h_scale=1.0f;

STBIDEF void   stbi_ldr_to_hdr_gamma(float gamma) { stbi__l2h_gamma = gamma; }
STBIDEF void   stbi_ldr_to_hdr_scale(float scale) { stbi__l2h_scale = scale; }
#endif

static float stbi__h2l_gamma_i=1.0f/2.2f, stbi__h2l_scale_i=1.0f;

STBIDEF void   stbi_hdr_to_ldr_gamma(float gamma) { stbi__h2l_gamma_i = 1/gamma; }
STBIDEF void   stbi_hdr_to_ldr_scale(float scale) { stbi__h2l_scale_i = 1/scale; }


//////////////////////////////////////////////////////////////////////////////
//
// Common code used by all image loaders
//

enum
{
   STBI__SCAN_load=0,
   STBI__SCAN_type,
   STBI__SCAN_header
};

static void stbi__refill_buffer(stbi__context *s)
{
   int n = (s->io.read)(s->io_user_data,(char*)s->buffer_start,s->buflen);
   s->callback_already_read += (int) (s->img_buffer - s->img_buffer_original);
   if (n == 0) {
      // at end of file, treat same as if from memory, but need to handle case
      // where s->img_buffer isn't pointing to safe memory, e.g. 0-byte file
      s->read_from_callbacks = 0;
      s->img_buffer = s->buffer_start;
      s->img_buffer_end = s->buffer_start+1;
      *s->img_buffer = 0;
   } else {
      s->img_buffer = s->buffer_start;
      s->img_buffer_end = s->buffer_start + n;
   }
}

stbi_inline static stbi_uc stbi__get8(stbi__context *s)
{
   if (s->img_buffer < s->img_buffer_end)
      return *s->img_buffer++;
   if (s->read_from_callbacks) {
      stbi__refill_buffer(s);
      return *s->img_buffer++;
   }
   return 0;
}

#if defined(STBI_NO_JPEG) && defined(STBI_NO_HDR) && defined(STBI_NO_PIC) && defined(STBI_NO_PNM)
// nothing
#else
stbi_inline static int stbi__at_eof(stbi__context *s)
{
   if (s->io.read) {
      if (!(s->io.eof)(s->io_user_data)) return 0;
      // if feof() is true, check if buffer = end
      // special case: we've only got the special 0 character at the end
      if (s->read_from_callbacks == 0) return 1;
   }

   return s->img_buffer >= s->img_buffer_end;
}
#endif

#if defined(STBI_NO_JPEG) && defined(STBI_NO_PNG) && defined(STBI_NO_BMP) && defined(STBI_NO_PSD) && defined(STBI_NO_TGA) && defined(STBI_NO_GIF) && defined(STBI_NO_PIC)
// nothing
#else
static void stbi__skip(stbi__context *s, int n)
{
   if (n == 0) return;  // already there!
   if (n < 0) {
      s->img_buffer = s->img_buffer_end;
      return;
   }
   if (s->io.read) {
      int blen = (int) (s->img_buffer_end - s->img_buffer);
      if (blen < n) {
         s->img_buffer = s->img_buffer_end;
         (s->io.skip)(s->io_user_data, n - blen);
         return;
      }
   }
   s->img_buffer += n;
}
#endif

#if defined(STBI_NO_PNG) && defined(STBI_NO_TGA) && defined(STBI_NO_HDR) && defined(STBI_NO_PNM)
// nothing
#else
static int stbi__getn(stbi__context *s, stbi_uc *buffer, int n)
{
   if (s->io.read) {
      int blen = (int) (s->img_buffer_end - s->img_buffer);
      if (blen < n) {
         int res, count;

         memcpy(buffer, s->img_buffer, blen);

         count = (s->io.read)(s->io_user_data, (char*) buffer + blen, n - blen);
         res = (count == (n-blen));
         s->img_buffer = s->img_buffer_end;
         return res;
      }
   }

   if (s->img_buffer+n <= s->img_buffer_end) {
      memcpy(buffer, s->img_buffer, n);
      s->img_buffer += n;
      return 1;
   } else
      return 0;
}
#endif

#if defined(STBI_NO_JPEG) && defined(STBI_NO_PNG) && defined(STBI_NO_PSD) && defined(STBI_NO_PIC)
// nothing
#else
static int stbi__get16be(stbi__context *s)
{
   int z = stbi__get8(s);
   return (z << 8) + stbi__get8(s);
}
#endif

#if defined(STBI_NO_PNG) && defined(STBI_NO_PSD) && defined(STBI_NO_PIC)
// nothing
#else
static stbi__uint32 stbi__get32be(stbi__context *s)
{
   stbi__uint32 z = stbi__get16be(s);
   return (z << 16) + stbi__get16be(s);
}
#endif

#if defined(STBI_NO_BMP) && defined(STBI_NO_TGA) && defined(STBI_NO_GIF)
// nothing
#else
static int stbi__get16le(stbi__context *s)
{
   int z = stbi__get8(s);
   return z + (stbi__get8(s) << 8);
}
#endif

#ifndef STBI_NO_BMP
static stbi__uint32 stbi__get32le(stbi__context *s)
{
   stbi__uint32 z = stbi__get16le(s);
   z += (stbi__uint32)stbi__get16le(s) << 16;
   return z;
}
#endif

#define STBI__BYTECAST(x)  ((stbi_uc) ((x) & 255))  // truncate int to byte without warnings

#if defined(STBI_NO_JPEG) && defined(STBI_NO_PNG) && defined(STBI_NO_BMP) && defined(STBI_NO_PSD) && defined(STBI_NO_TGA) && defined(STBI_NO_GIF) && defined(STBI_NO_PIC) && defined(STBI_NO_PNM)
// nothing
#else
//////////////////////////////////////////////////////////////////////////////
//
//  generic converter from built-in img_n to req_comp
//    individual types do this automatically as much as possible (e.g. jpeg
//    does all cases internally since it needs to colorspace convert anyway,
//    and it never has alpha, so very few cases ). png can automatically
//    interleave an alpha=255 channel, but falls back to this for other cases
//
//  assume data buffer is malloced, so malloc a new one and free that one
//  only failure mode is malloc failing

static stbi_uc stbi__compute_y(int r, int g, int b)
{
   return (stbi_uc) (((r*77) + (g*150) +  (29*b)) >> 8);
}
#endif

#if defined(STBI_NO_PNG) && defined(STBI_NO_BMP) && defined(STBI_NO_PSD) && defined(STBI_NO_TGA) && defined(STBI_NO_GIF) && defined(STBI_NO_PIC) && defined(STBI_NO_PNM)
// nothing
#else
static unsigned char *stbi__convert_format(unsigned char *data, int img_n, int req_comp, unsigned int x, unsigned int y)
{
   int i,j;
   unsigned char *good;

   if (req_comp == img_n) return data;
   STBI_ASSERT(req_comp >= 1 && req_comp <= 4);

   good = (unsigned char *) stbi__malloc_mad3(req_comp, x, y, 0);
   if (good == NULL) {
      STBI_FREE(data);
      return stbi__errpuc("outofmem", "Out of memory");
   }

   for (j=0; j < (int) y; ++j) {
      unsigned char *src  = data + j * x * img_n   ;
      unsigned char *dest = good + j * x * req_comp;

      #define STBI__COMBO(a,b)  ((a)*8+(b))
      #define STBI__CASE(a,b)   case STBI__COMBO(a,b): for(i=x-1; i >= 0; --i, src += a, dest += b)
      // convert source image with img_n components to one with req_comp components;
      // avoid switch per pixel, so use switch per scanline and massive macros
      switch (STBI__COMBO(img_n, req_comp)) {
         STBI__CASE(1,2) { dest[0]=src[0]; dest[1]=255;                                     } break;
         STBI__CASE(1,3) { dest[0]=dest[1]=dest[2]=src[0];                                  } break;
         STBI__CASE(1,4) { dest[0]=dest[1]=dest[2]=src[0]; dest[3]=255;                     } break;
         STBI__CASE(2,1) { dest[0]=src[0];                                                  } break;
         STBI__CASE(2,3) { dest[0]=dest[1]=dest[2]=src[0];                                  } break;
         STBI__CASE(2,4) { dest[0]=dest[1]=dest[2]=src[0]; dest[3]=src[1];                  } break;
         STBI__CASE(3,4) { dest[0]=src[0];dest[1]=src[1];dest[2]=src[2];dest[3]=255;        } break;
         STBI__CASE(3,1) { dest[0]=stbi__compute_y(src[0],src[1],src[2]);                   } break;
         STBI__CASE(3,2) { dest[0]=stbi__compute_y(src[0],src[1],src[2]); dest[1] = 255;    } break;
         STBI__CASE(4,1) { dest[0]=stbi__compute_y(src[0],src[1],src[2]);                   } break;
         STBI__CASE(4,2) { dest[0]=stbi__compute_y(src[0],src[1],src[2]); dest[1] = src[3]; } break;
         STBI__CASE(4,3) { dest[0]=src[0];dest[1]=src[1];dest[2]=src[2];                    } break;
         default: STBI_ASSERT(0); STBI_FREE(data); STBI_FREE(good); return stbi__errpuc("unsupported", "Unsupported format conversion");
      }
      #undef STBI__CASE
   }

   STBI_FREE(data);
   return good;
}
#endif

#if defined(STBI_NO_PNG) && defined(STBI_NO_PSD)
// nothing
#else
static stbi__uint16 stbi__compute_y_16(int r, int g, int b)
{
   return (stbi__uint16) (((r*77) + (g*150) +  (29*b)) >> 8);
}
#endif

#if defined(STBI_NO_PNG) && defined(STBI_NO_PSD)
// nothing
#else
static stbi__uint16 *stbi__convert_format16(stbi__uint16 *data, int img_n, int req_comp, unsigned int x, unsigned int y)
{
   int i,j;
   stbi__uint16 *good;

   if (req_comp == img_n) return data;
   STBI_ASSERT(req_comp >= 1 && req_comp <= 4);

   good = (stbi__uint16 *) stbi__malloc(req_comp * x * y * 2);
   if (good == NULL) {
      STBI_FREE(data);
      return (stbi__uint16 *) stbi__errpuc("outofmem", "Out of memory");
   }

   for (j=0; j < (int) y; ++j) {
      stbi__uint16 *src  = data + j * x * img_n   ;
      stbi__uint16 *dest = good + j * x * req_comp;

      #define STBI__COMBO(a,b)  ((a)*8+(b))
      #define STBI__CASE(a,b)   case STBI__COMBO(a,b): for(i=x-1; i >= 0; --i, src += a, dest += b)
      // convert source image with img_n components to one with req_comp components;
      // avoid switch per pixel, so use switch per scanline and massive macros
      switch (STBI__COMBO(img_n, req_comp)) {
         STBI__CASE(1,2) { dest[0]=src[0]; dest[1]=0xffff;                                     } break;
         STBI__CASE(1,3) { dest[0]=dest[1]=dest[2]=src[0];                                     } break;
         STBI__CASE(1,4) { dest[0]=dest[1]=dest[2]=src[0]; dest[3]=0xffff;                     } break;
         STBI__CASE(2,1) { dest[0]=src[0];                                                     } break;
         STBI__CASE(2,3) { dest[0]=dest[1]=dest[2]=src[0];                                     } break;
         STBI__CASE(2,4) { dest[0]=dest[1]=dest[2]=src[0]; dest[3]=src[1];                     } break;
         STBI__CASE(3,4) { dest[0]=src[0];dest[1]=src[1];dest[2]=src[2];dest[3]=0xffff;        } break;
         STBI__CASE(3,1) { dest[0]=stbi__compute_y_16(src[0],src[1],src[2]);                   } break;
         STBI__CASE(3,2) { dest[0]=stbi__compute_y_16(src[0],src[1],src[2]); dest[1] = 0xffff; } break;
         STBI__CASE(4,1) { dest[0]=stbi__compute_y_16(src[0],src[1],src[2]);                   } break;
         STBI__CASE(4,2) { dest[0]=stbi__compute_y_16(src[0],src[1],src[2]); dest[1] = src[3]; } break;
         STBI__CASE(4,3) { dest[0]=src[0];dest[1]=src[1];dest[2]=src[2];                       } break;
         default: STBI_ASSERT(0); STBI_FREE(data); STBI_FREE(good); return (stbi__uint16*) stbi__errpuc("unsupported", "Unsupported format conversion");
      }
      #undef STBI__CASE
   }

   STBI_FREE(data);
   return good;
}
#endif

#ifndef STBI_NO_LINEAR
static float   *stbi__ldr_to_hdr(stbi_uc *data, int x, int y, int comp)
{
   int i,k,n;
   float *output;
   if (!data) return NULL;
   output = (float *) stbi__malloc_mad4(x, y, comp, sizeof(float), 0);
   if (output == NULL) { STBI_FREE(data); return stbi__errpf("outofmem", "Out of memory"); }
   // compute number of non-alpha components
   if (comp & 1) n = comp; else n = comp-1;
   for (i=0; i < x*y; ++i) {
      for (k=0; k < n; ++k) {
         output[i*comp + k] = (float) (pow(data[i*comp+k]/255.0f, stbi__l2h_gamma) * stbi__l2h_scale);
      }
   }
   if (n < comp) {
      for (i=0; i < x*y; ++i) {
         output[i*comp + n] = data[i*comp + n]/255.0f;
      }
   }
   STBI_FREE(data);
   return output;
}
#endif

#ifndef STBI_NO_HDR
#define stbi__float2int(x)   ((int) (x))
static stbi_uc *stbi__hdr_to_ldr(float   *data, int x, int y, int comp)
{
   int i,k,n;
   stbi_uc *output;
   if (!data) return NULL;
   output = (stbi_uc *) stbi__malloc_mad3(x, y, comp, 0);
   if (output == NULL) { STBI_FREE(data); return stbi__errpuc("outofmem", "Out of memory"); }
   // compute number of non-alpha components
   if (comp & 1) n = comp; else n = comp-1;
   for (i=0; i < x*y; ++i) {
      for (k=0; k < n; ++k) {
         float z = (float) pow(data[i*comp+k]*stbi__h2l_scale_i, stbi__h2l_gamma_i) * 255 + 0.5f;
         if (z < 0) z = 0;
         if (z > 255) z = 255;
         output[i*comp + k] = (stbi_uc) stbi__float2int(z);
      }
      if (k < comp) {
         float z = data[i*comp+k] * 255 + 0.5f;
         if (z < 0) z = 0;
         if (z > 255) z = 255;
         output[i*comp + k] = (stbi_uc) stbi__float2int(z);
      }
   }
   STBI_FREE(data);
   return output;
}
#endif

//////////////////////////////////////////////////////////////////////////////
//
//  "baseline" JPEG/JFIF decoder
//
//    simple implementation
//      - doesn't support delayed output of y-dimension
//      - simple interface (only one output format: 8-bit interleaved RGB)
//      - doesn't try to recover corrupt jpegs
//      - doesn't allow partial loading, loading multiple at once
//      - still fast on x86 (copying globals into locals doesn't help x86)
//      - allocates lots of intermediate memory (full size of all components)
//        - non-interleaved case requires this anyway
//        - allows good upsampling (see next)
//    high-quality
//      - upsampled channels are bilinearly interpolated, even across blocks
//      - quality integer IDCT derived from IJG's 'slow'
//    performance
//      - fast huffman; reasonable integer IDCT
//      - some SIMD kernels for common paths on targets with SSE2/NEON
//      - uses a lot of intermediate memory, could cache poorly

#ifndef STBI_NO_JPEG

// huffman decoding acceleration
#define FAST_BITS   9  // larger handles more cases; smaller stomps less cache

typedef struct
{
   stbi_uc  fast[1 << FAST_BITS];
   // weirdly, repacking this into AoS is a 10% speed loss, instead of a win
   stbi__uint16 code[256];
   stbi_uc  values[256];
   stbi_uc  size[257];
   unsigned int maxcode[18];
   int    delta[17];   // old 'firstsymbol' - old 'firstcode'
} stbi__huffman;

typedef struct
{
   stbi__context *s;
   stbi__huffman huff_dc[4];
   stbi__huffman huff_ac[4];
   stbi__uint16 dequant[4][64];
   stbi__int16 fast_ac[4][1 << FAST_BITS];

// sizes for components, interleaved MCUs
   int img_h_max, img_v_max;
   int img_mcu_x, img_mcu_y;
   int img_mcu_w, img_mcu_h;

// definition of jpeg image component
   struct
   {
      int id;
      int h,v;
      int tq;
      int hd,ha;
      int dc_pred;

      int x,y,w2,h2;
      stbi_uc *data;
      void *raw_data, *raw_coeff;
      stbi_uc *linebuf;
      short   *coeff;   // progressive only
      int      coeff_w, coeff_h; // number of 8x8 coefficient blocks
   } img_comp[4];

   stbi__uint32   code_buffer; // jpeg entropy-coded buffer
   int            code_bits;   // number of valid bits
   unsigned char  marker;      // marker seen while filling entropy buffer
   int            nomore;      // flag if we saw a marker so must stop

   int            progressive;
   int            spec_start;
   int            spec_end;
   int            succ_high;
   int            succ_low;
   int            eob_run;
   int            jfif;
   int            app14_color_transform; // Adobe APP14 tag
   int            rgb;

   int scan_n, order[4];
   int restart_interval, todo;

// kernels
   void (*idct_block_kernel)(stbi_uc *out, int out_stride, short data[64]);
   void (*YCbCr_to_RGB_kernel)(stbi_uc *out, const stbi_uc *y, const stbi_uc *pcb, const stbi_uc *pcr, int count, int step);
   stbi_uc *(*resample_row_hv_2_kernel)(stbi_uc *out, stbi_uc *in_near, stbi_uc *in_far, int w, int hs);
} stbi__jpeg;

static int stbi__build_huffman(stbi__huffman *h, int *count)
{
   int i,j,k=0;
   unsigned int code;
   // build size list for each symbol (from JPEG spec)
   for (i=0; i < 16; ++i)
      for (j=0; j < count[i]; ++j)
         h->size[k++] = (stbi_uc) (i+1);
   h->size[k] = 0;

   // compute actual symbols (from jpeg spec)
   code = 0;
   k = 0;
   for(j=1; j <= 16; ++j) {
      // compute delta to add to code to compute symbol id
      h->delta[j] = k - code;
      if (h->size[k] == j) {
         while (h->size[k] == j)
            h->code[k++] = (stbi__uint16) (code++);
         if (code-1 >= (1u << j)) return stbi__err("bad code lengths","Corrupt JPEG");
      }
      // compute largest code + 1 for this size, preshifted as needed later
      h->maxcode[j] = code << (16-j);
      code <<= 1;
   }
   h->maxcode[j] = 0xffffffff;

   // build non-spec acceleration table; 255 is flag for not-accelerated
   memset(h->fast, 255, 1 << FAST_BITS);
   for (i=0; i < k; ++i) {
      int s = h->size[i];
      if (s <= FAST_BITS) {
         int c = h->code[i] << (FAST_BITS-s);
         int m = 1 << (FAST_BITS-s);
         for (j=0; j < m; ++j) {
            h->fast[c+j] = (stbi_uc) i;
         }
      }
   }
   return 1;
}

// build a table that decodes both magnitude and value of small ACs in
// one go.
static void stbi__build_fast_ac(stbi__int16 *fast_ac, stbi__huffman *h)
{
   int i;
   for (i=0; i < (1 << FAST_BITS); ++i) {
      stbi_uc fast = h->fast[i];
      fast_ac[i] = 0;
      if (fast < 255) {
         int rs = h->values[fast];
         int run = (rs >> 4) & 15;
         int magbits = rs & 15;
         int len = h->size[fast];

         if (magbits && len + magbits <= FAST_BITS) {
            // magnitude code followed by receive_extend code
            int k = ((i << len) & ((1 << FAST_BITS) - 1)) >> (FAST_BITS - magbits);
            int m = 1 << (magbits - 1);
            if (k < m) k += (~0U << magbits) + 1;
            // if the result is small enough, we can fit it in fast_ac table
            if (k >= -128 && k <= 127)
               fast_ac[i] = (stbi__int16) ((k * 256) + (run * 16) + (len + magbits));
         }
      }
   }
}

static void stbi__grow_buffer_unsafe(stbi__jpeg *j)
{
   do {
      unsigned int b = j->nomore ? 0 : stbi__get8(j->s);
      if (b == 0xff) {
         int c = stbi__get8(j->s);
         while (c == 0xff) c = stbi__get8(j->s); // consume fill bytes
         if (c != 0) {
            j->marker = (unsigned char) c;
            j->nomore = 1;
            return;
         }
      }
      j->code_buffer |= b << (24 - j->code_bits);
      j->code_bits += 8;
   } while (j->code_bits <= 24);
}

// (1 << n) - 1
static const stbi__uint32 stbi__bmask[17]={0,1,3,7,15,31,63,127,255,511,1023,2047,4095,8191,16383,32767,65535};

// decode a jpeg huffman value from the bitstream
stbi_inline static int stbi__jpeg_huff_decode(stbi__jpeg *j, stbi__huffman *h)
{
   unsigned int temp;
   int c,k;

   if (j->code_bits < 16) stbi__grow_buffer_unsafe(j);

   // look at the top FAST_BITS and determine what symbol ID it is,
   // if the code is <= FAST_BITS
   c = (j->code_buffer >> (32 - FAST_BITS)) & ((1 << FAST_BITS)-1);
   k = h->fast[c];
   if (k < 255) {
      int s = h->size[k];
      if (s > j->code_bits)
         return -1;
      j->code_buffer <<= s;
      j->code_bits -= s;
      return h->values[k];
   }

   // naive test is to shift the code_buffer down so k bits are
   // valid, then test against maxcode. To speed this up, we've
   // preshifted maxcode left so that it has (16-k) 0s at the
   // end; in other words, regardless of the number of bits, it
   // wants to be compared against something shifted to have 16;
   // that way we don't need to shift inside the loop.
   temp = j->code_buffer >> 16;
   for (k=FAST_BITS+1 ; ; ++k)
      if (temp < h->maxcode[k])
         break;
   if (k == 17) {
      // error! code not found
      j->code_bits -= 16;
      return -1;
   }

   if (k > j->code_bits)
      return -1;

   // convert the huffman code to the symbol id
   c = ((j->code_buffer >> (32 - k)) & stbi__bmask[k]) + h->delta[k];
   STBI_ASSERT((((j->code_buffer) >> (32 - h->size[c])) & stbi__bmask[h->size[c]]) == h->code[c]);

   // convert the id to a symbol
   j->code_bits -= k;
   j->code_buffer <<= k;
   return h->values[c];
}

// bias[n] = (-1<<n) + 1
static const int stbi__jbias[16] = {0,-1,-3,-7,-15,-31,-63,-127,-255,-511,-1023,-2047,-4095,-8191,-16383,-32767};

// combined JPEG 'receive' and JPEG 'extend', since baseline
// always extends everything it receives.
stbi_inline static int stbi__extend_receive(stbi__jpeg *j, int n)
{
   unsigned int k;
   int sgn;
   if (j->code_bits < n) stbi__grow_buffer_unsafe(j);

   sgn = j->code_buffer >> 31; // sign bit always in MSB; 0 if MSB clear (positive), 1 if MSB set (negative)
   k = stbi_lrot(j->code_buffer, n);
   j->code_buffer = k & ~stbi__bmask[n];
   k &= stbi__bmask[n];
   j->code_bits -= n;
   return k + (stbi__jbias[n] & (sgn - 1));
}

// get some unsigned bits
stbi_inline static int stbi__jpeg_get_bits(stbi__jpeg *j, int n)
{
   unsigned int k;
   if (j->code_bits < n) stbi__grow_buffer_unsafe(j);
   k = stbi_lrot(j->code_buffer, n);
   j->code_buffer = k & ~stbi__bmask[n];
   k &= stbi__bmask[n];
   j->code_bits -= n;
   return k;
}

stbi_inline static int stbi__jpeg_get_bit(stbi__jpeg *j)
{
   unsigned int k;
   if (j->code_bits < 1) stbi__grow_buffer_unsafe(j);
   k = j->code_buffer;
   j->code_buffer <<= 1;
   --j->code_bits;
   return k & 0x80000000;
}

// given a value that's at position X in the zigzag stream,
// where does it appear in the 8x8 matrix coded as row-major?
static const stbi_uc stbi__jpeg_dezigzag[64+15] =
{
    0,  1,  8, 16,  9,  2,  3, 10,
   17, 24, 32, 25, 18, 11,  4,  5,
   12, 19, 26, 33, 40, 48, 41, 34,
   27, 20, 13,  6,  7, 14, 21, 28,
   35, 42, 49, 56, 57, 50, 43, 36,
   29, 22, 15, 23, 30, 37, 44, 51,
   58, 59, 52, 45, 38, 31, 39, 46,
   53, 60, 61, 54, 47, 55, 62, 63,
   // let corrupt input sample past end
   63, 63, 63, 63, 63, 63, 63, 63,
   63, 63, 63, 63, 63, 63, 63
};

// decode one 64-entry block--
static int stbi__jpeg_decode_block(stbi__jpeg *j, short data[64], stbi__huffman *hdc, stbi__huffman *hac, stbi__int16 *fac, int b, stbi__uint16 *dequant)
{
   int diff,dc,k;
   int t;

   if (j->code_bits < 16) stbi__grow_buffer_unsafe(j);
   t = stbi__jpeg_huff_decode(j, hdc);
   if (t < 0 || t > 15) return stbi__err("bad huffman code","Corrupt JPEG");

   // 0 all the ac values now so we can do it 32-bits at a time
   memset(data,0,64*sizeof(data[0]));

   diff = t ? stbi__extend_receive(j, t) : 0;
   dc = j->img_comp[b].dc_pred + diff;
   j->img_comp[b].dc_pred = dc;
   data[0] = (short) (dc * dequant[0]);

   // decode AC components, see JPEG spec
   k = 1;
   do {
      unsigned int zig;
      int c,r,s;
      if (j->code_bits < 16) stbi__grow_buffer_unsafe(j);
      c = (j->code_buffer >> (32 - FAST_BITS)) & ((1 << FAST_BITS)-1);
      r = fac[c];
      if (r) { // fast-AC path
         k += (r >> 4) & 15; // run
         s = r & 15; // combined length
         j->code_buffer <<= s;
         j->code_bits -= s;
         // decode into unzigzag'd location
         zig = stbi__jpeg_dezigzag[k++];
         data[zig] = (short) ((r >> 8) * dequant[zig]);
      } else {
         int rs = stbi__jpeg_huff_decode(j, hac);
         if (rs < 0) return stbi__err("bad huffman code","Corrupt JPEG");
         s = rs & 15;
         r = rs >> 4;
         if (s == 0) {
            if (rs != 0xf0) break; // end block
            k += 16;
         } else {
            k += r;
            // decode into unzigzag'd location
            zig = stbi__jpeg_dezigzag[k++];
            data[zig] = (short) (stbi__extend_receive(j,s) * dequant[zig]);
         }
      }
   } while (k < 64);
   return 1;
}

static int stbi__jpeg_decode_block_prog_dc(stbi__jpeg *j, short data[64], stbi__huffman *hdc, int b)
{
   int diff,dc;
   int t;
   if (j->spec_end != 0) return stbi__err("can't merge dc and ac", "Corrupt JPEG");

   if (j->code_bits < 16) stbi__grow_buffer_unsafe(j);

   if (j->succ_high == 0) {
      // first scan for DC coefficient, must be first
      memset(data,0,64*sizeof(data[0])); // 0 all the ac values now
      t = stbi__jpeg_huff_decode(j, hdc);
      if (t < 0 || t > 15) return stbi__err("can't merge dc and ac", "Corrupt JPEG");
      diff = t ? stbi__extend_receive(j, t) : 0;

      dc = j->img_comp[b].dc_pred + diff;
      j->img_comp[b].dc_pred = dc;
      data[0] = (short) (dc * (1 << j->succ_low));
   } else {
      // refinement scan for DC coefficient
      if (stbi__jpeg_get_bit(j))
         data[0] += (short) (1 << j->succ_low);
   }
   return 1;
}

// @OPTIMIZE: store non-zigzagged during the decode passes,
// and only de-zigzag when dequantizing
static int stbi__jpeg_decode_block_prog_ac(stbi__jpeg *j, short data[64], stbi__huffman *hac, stbi__int16 *fac)
{
   int k;
   if (j->spec_start == 0) return stbi__err("can't merge dc and ac", "Corrupt JPEG");

   if (j->succ_high == 0) {
      int shift = j->succ_low;

      if (j->eob_run) {
         --j->eob_run;
         return 1;
      }

      k = j->spec_start;
      do {
         unsigned int zig;
         int c,r,s;
         if (j->code_bits < 16) stbi__grow_buffer_unsafe(j);
         c = (j->code_buffer >> (32 - FAST_BITS)) & ((1 << FAST_BITS)-1);
         r = fac[c];
         if (r) { // fast-AC path
            k += (r >> 4) & 15; // run
            s = r & 15; // combined length
            j->code_buffer <<= s;
            j->code_bits -= s;
            zig = stbi__jpeg_dezigzag[k++];
            data[zig] = (short) ((r >> 8) * (1 << shift));
         } else {
            int rs = stbi__jpeg_huff_decode(j, hac);
            if (rs < 0) return stbi__err("bad huffman code","Corrupt JPEG");
            s = rs & 15;
            r = rs >> 4;
            if (s == 0) {
               if (r < 15) {
                  j->eob_run = (1 << r);
                  if (r)
                     j->eob_run += stbi__jpeg_get_bits(j, r);
                  --j->eob_run;
                  break;
               }
               k += 16;
            } else {
               k += r;
               zig = stbi__jpeg_dezigzag[k++];
               data[zig] = (short) (stbi__extend_receive(j,s) * (1 << shift));
            }
         }
      } while (k <= j->spec_end);
   } else {
      // refinement scan for these AC coefficients

      short bit = (short) (1 << j->succ_low);

      if (j->eob_run) {
         --j->eob_run;
         for (k = j->spec_start; k <= j->spec_end; ++k) {
            short *p = &data[stbi__jpeg_dezigzag[k]];
            if (*p != 0)
               if (stbi__jpeg_get_bit(j))
                  if ((*p & bit)==0) {
                     if (*p > 0)
                        *p += bit;
                     else
                        *p -= bit;
                  }
         }
      } else {
         k = j->spec_start;
         do {
            int r,s;
            int rs = stbi__jpeg_huff_decode(j, hac); // @OPTIMIZE see if we can use the fast path here, advance-by-r is so slow, eh
            if (rs < 0) return stbi__err("bad huffman code","Corrupt JPEG");
            s = rs & 15;
            r = rs >> 4;
            if (s == 0) {
               if (r < 15) {
                  j->eob_run = (1 << r) - 1;
                  if (r)
                     j->eob_run += stbi__jpeg_get_bits(j, r);
                  r = 64; // force end of block
               } else {
                  // r=15 s=0 should write 16 0s, so we just do
                  // a run of 15 0s and then write s (which is 0),
                  // so we don't have to do anything special here
               }
            } else {
               if (s != 1) return stbi__err("bad huffman code", "Corrupt JPEG");
               // sign bit
               if (stbi__jpeg_get_bit(j))
                  s = bit;
               else
                  s = -bit;
            }

            // advance by r
            while (k <= j->spec_end) {
               short *p = &data[stbi__jpeg_dezigzag[k++]];
               if (*p != 0) {
                  if (stbi__jpeg_get_bit(j))
                     if ((*p & bit)==0) {
                        if (*p > 0)
                           *p += bit;
                        else
                           *p -= bit;
                     }
               } else {
                  if (r == 0) {
                     *p = (short) s;
                     break;
                  }
                  --r;
               }
            }
         } while (k <= j->spec_end);
      }
   }
   return 1;
}

// take a -128..127 value and stbi__clamp it and convert to 0..255
stbi_inline static stbi_uc stbi__clamp(int x)
{
   // trick to use a single test to catch both cases
   if ((unsigned int) x > 255) {
      if (x < 0) return 0;
      if (x > 255) return 255;
   }
   return (stbi_uc) x;
}

#define stbi__f2f(x)  ((int) (((x) * 4096 + 0.5)))
#define stbi__fsh(x)  ((x) * 4096)

// derived from jidctint -- DCT_ISLOW
#define STBI__IDCT_1D(s0,s1,s2,s3,s4,s5,s6,s7) \
   int t0,t1,t2,t3,p1,p2,p3,p4,p5,x0,x1,x2,x3; \
   p2 = s2;                                    \
   p3 = s6;                                    \
   p1 = (p2+p3) * stbi__f2f(0.5411961f);       \
   t2 = p1 + p3*stbi__f2f(-1.847759065f);      \
   t3 = p1 + p2*stbi__f2f( 0.765366865f);      \
   p2 = s0;                                    \
   p3 = s4;                                    \
   t0 = stbi__fsh(p2+p3);                      \
   t1 = stbi__fsh(p2-p3);                      \
   x0 = t0+t3;                                 \
   x3 = t0-t3;                                 \
   x1 = t1+t2;                                 \
   x2 = t1-t2;                                 \
   t0 = s7;                                    \
   t1 = s5;                                    \
   t2 = s3;                                    \
   t3 = s1;                                    \
   p3 = t0+t2;                                 \
   p4 = t1+t3;                                 \
   p1 = t0+t3;                                 \
   p2 = t1+t2;                                 \
   p5 = (p3+p4)*stbi__f2f( 1.175875602f);      \
   t0 = t0*stbi__f2f( 0.298631336f);           \
   t1 = t1*stbi__f2f( 2.053119869f);           \
   t2 = t2*stbi__f2f( 3.072711026f);           \
   t3 = t3*stbi__f2f( 1.501321110f);           \
   p1 = p5 + p1*stbi__f2f(-0.899976223f);      \
   p2 = p5 + p2*stbi__f2f(-2.562915447f);      \
   p3 = p3*stbi__f2f(-1.961570560f);           \
   p4 = p4*stbi__f2f(-0.390180644f);           \
   t3 += p1+p4;                                \
   t2 += p2+p3;                                \
   t1 += p2+p4;                                \
   t0 += p1+p3;

static void stbi__idct_block(stbi_uc *out, int out_stride, short data[64])
{
   int i,val[64],*v=val;
   stbi_uc *o;
   short *d = data;

   // columns
   for (i=0; i < 8; ++i,++d, ++v) {
      // if all zeroes, shortcut -- this avoids dequantizing 0s and IDCTing
      if (d[ 8]==0 && d[16]==0 && d[24]==0 && d[32]==0
           && d[40]==0 && d[48]==0 && d[56]==0) {
         //    no shortcut                 0     seconds
         //    (1|2|3|4|5|6|7)==0          0     seconds
         //    all separate               -0.047 seconds
         //    1 && 2|3 && 4|5 && 6|7:    -0.047 seconds
         int dcterm = d[0]*4;
         v[0] = v[8] = v[16] = v[24] = v[32] = v[40] = v[48] = v[56] = dcterm;
      } else {
         STBI__IDCT_1D(d[ 0],d[ 8],d[16],d[24],d[32],d[40],d[48],d[56])
         // constants scaled things up by 1<<12; let's bring them back
         // down, but keep 2 extra bits of precision
         x0 += 512; x1 += 512; x2 += 512; x3 += 512;
         v[ 0] = (x0+t3) >> 10;
         v[56] = (x0-t3) >> 10;
         v[ 8] = (x1+t2) >> 10;
         v[48] = (x1-t2) >> 10;
         v[16] = (x2+t1) >> 10;
         v[40] = (x2-t1) >> 10;
         v[24] = (x3+t0) >> 10;
         v[32] = (x3-t0) >> 10;
      }
   }

   for (i=0, v=val, o=out; i < 8; ++i,v+=8,o+=out_stride) {
      // no fast case since the first 1D IDCT spread components out
      STBI__IDCT_1D(v[0],v[1],v[2],v[3],v[4],v[5],v[6],v[7])
      // constants scaled things up by 1<<12, plus we had 1<<2 from first
      // loop, plus horizontal and vertical each scale by sqrt(8) so together
      // we've got an extra 1<<3, so 1<<17 total we need to remove.
      // so we want to round that, which means adding 0.5 * 1<<17,
      // aka 65536. Also, we'll end up with -128 to 127 that we want
      // to encode as 0..255 by adding 128, so we'll add that before the shift
      x0 += 65536 + (128<<17);
      x1 += 65536 + (128<<17);
      x2 += 65536 + (128<<17);
      x3 += 65536 + (128<<17);
      // tried computing the shifts into temps, or'ing the temps to see
      // if any were out of range, but that was slower
      o[0] = stbi__clamp((x0+t3) >> 17);
      o[7] = stbi__clamp((x0-t3) >> 17);
      o[1] = stbi__clamp((x1+t2) >> 17);
      o[6] = stbi__clamp((x1-t2) >> 17);
      o[2] = stbi__clamp((x2+t1) >> 17);
      o[5] = stbi__clamp((x2-t1) >> 17);
      o[3] = stbi__clamp((x3+t0) >> 17);
      o[4] = stbi__clamp((x3-t0) >> 17);
   }
}

#ifdef STBI_SSE2
// sse2 integer IDCT. not the fastest possible implementation but it
// produces bit-identical results to the generic C version so it's
// fully "transparent".
static void stbi__idct_simd(stbi_uc *out, int out_stride, short data[64])
{
   // This is constructed to match our regular (generic) integer IDCT exactly.
   __m128i row0, row1, row2, row3, row4, row5, row6, row7;
   __m128i tmp;

   // dot product constant: even elems=x, odd elems=y
   #define dct_const(x,y)  _mm_setr_epi16((x),(y),(x),(y),(x),(y),(x),(y))

   // out(0) = c0[even]*x + c0[odd]*y   (c0, x, y 16-bit, out 32-bit)
   // out(1) = c1[even]*x + c1[odd]*y
   #define dct_rot(out0,out1, x,y,c0,c1) \
      __m128i c0##lo = _mm_unpacklo_epi16((x),(y)); \
      __m128i c0##hi = _mm_unpackhi_epi16((x),(y)); \
      __m128i out0##_l = _mm_madd_epi16(c0##lo, c0); \
      __m128i out0##_h = _mm_madd_epi16(c0##hi, c0); \
      __m128i out1##_l = _mm_madd_epi16(c0##lo, c1); \
      __m128i out1##_h = _mm_madd_epi16(c0##hi, c1)

   // out = in << 12  (in 16-bit, out 32-bit)
   #define dct_widen(out, in) \
      __m128i out##_l = _mm_srai_epi32(_mm_unpacklo_epi16(_mm_setzero_si128(), (in)), 4); \
      __m128i out##_h = _mm_srai_epi32(_mm_unpackhi_epi16(_mm_setzero_si128(), (in)), 4)

   // wide add
   #define dct_wadd(out, a, b) \
      __m128i out##_l = _mm_add_epi32(a##_l, b##_l); \
      __m128i out##_h = _mm_add_epi32(a##_h, b##_h)

   // wide sub
   #define dct_wsub(out, a, b) \
      __m128i out##_l = _mm_sub_epi32(a##_l, b##_l); \
      __m128i out##_h = _mm_sub_epi32(a##_h, b##_h)

   // butterfly a/b, add bias, then shift by "s" and pack
   #define dct_bfly32o(out0, out1, a,b,bias,s) \
      { \
         __m128i abiased_l = _mm_add_epi32(a##_l, bias); \
         __m128i abiased_h = _mm_add_epi32(a##_h, bias); \
         dct_wadd(sum, abiased, b); \
         dct_wsub(dif, abiased, b); \
         out0 = _mm_packs_epi32(_mm_srai_epi32(sum_l, s), _mm_srai_epi32(sum_h, s)); \
         out1 = _mm_packs_epi32(_mm_srai_epi32(dif_l, s), _mm_srai_epi32(dif_h, s)); \
      }

   // 8-bit interleave step (for transposes)
   #define dct_interleave8(a, b) \
      tmp = a; \
      a = _mm_unpacklo_epi8(a, b); \
      b = _mm_unpackhi_epi8(tmp, b)

   // 16-bit interleave step (for transposes)
   #define dct_interleave16(a, b) \
      tmp = a; \
      a = _mm_unpacklo_epi16(a, b); \
      b = _mm_unpackhi_epi16(tmp, b)

   #define dct_pass(bias,shift) \
      { \
         /* even part */ \
         dct_rot(t2e,t3e, row2,row6, rot0_0,rot0_1); \
         __m128i sum04 = _mm_add_epi16(row0, row4); \
         __m128i dif04 = _mm_sub_epi16(row0, row4); \
         dct_widen(t0e, sum04); \
         dct_widen(t1e, dif04); \
         dct_wadd(x0, t0e, t3e); \
         dct_wsub(x3, t0e, t3e); \
         dct_wadd(x1, t1e, t2e); \
         dct_wsub(x2, t1e, t2e); \
         /* odd part */ \
         dct_rot(y0o,y2o, row7,row3, rot2_0,rot2_1); \
         dct_rot(y1o,y3o, row5,row1, rot3_0,rot3_1); \
         __m128i sum17 = _mm_add_epi16(row1, row7); \
         __m128i sum35 = _mm_add_epi16(row3, row5); \
         dct_rot(y4o,y5o, sum17,sum35, rot1_0,rot1_1); \
         dct_wadd(x4, y0o, y4o); \
         dct_wadd(x5, y1o, y5o); \
         dct_wadd(x6, y2o, y5o); \
         dct_wadd(x7, y3o, y4o); \
         dct_bfly32o(row0,row7, x0,x7,bias,shift); \
         dct_bfly32o(row1,row6, x1,x6,bias,shift); \
         dct_bfly32o(row2,row5, x2,x5,bias,shift); \
         dct_bfly32o(row3,row4, x3,x4,bias,shift); \
      }

   __m128i rot0_0 = dct_const(stbi__f2f(0.5411961f), stbi__f2f(0.5411961f) + stbi__f2f(-1.847759065f));
   __m128i rot0_1 = dct_const(stbi__f2f(0.5411961f) + stbi__f2f( 0.765366865f), stbi__f2f(0.5411961f));
   __m128i rot1_0 = dct_const(stbi__f2f(1.175875602f) + stbi__f2f(-0.899976223f), stbi__f2f(1.175875602f));
   __m128i rot1_1 = dct_const(stbi__f2f(1.175875602f), stbi__f2f(1.175875602f) + stbi__f2f(-2.562915447f));
   __m128i rot2_0 = dct_const(stbi__f2f(-1.961570560f) + stbi__f2f( 0.298631336f), stbi__f2f(-1.961570560f));
   __m128i rot2_1 = dct_const(stbi__f2f(-1.961570560f), stbi__f2f(-1.961570560f) + stbi__f2f( 3.072711026f));
   __m128i rot3_0 = dct_const(stbi__f2f(-0.390180644f) + stbi__f2f( 2.053119869f), stbi__f2f(-0.390180644f));
   __m128i rot3_1 = dct_const(stbi__f2f(-0.390180644f), stbi__f2f(-0.390180644f) + stbi__f2f( 1.501321110f));

   // rounding biases in column/row passes, see stbi__idct_block for explanation.
   __m128i bias_0 = _mm_set1_epi32(512);
   __m128i bias_1 = _mm_set1_epi32(65536 + (128<<17));

   // load
   row0 = _mm_load_si128((const __m128i *) (data + 0*8));
   row1 = _mm_load_si128((const __m128i *) (data + 1*8));
   row2 = _mm_load_si128((const __m128i *) (data + 2*8));
   row3 = _mm_load_si128((const __m128i *) (data + 3*8));
   row4 = _mm_load_si128((const __m128i *) (data + 4*8));
   row5 = _mm_load_si128((const __m128i *) (data + 5*8));
   row6 = _mm_load_si128((const __m128i *) (data + 6*8));
   row7 = _mm_load_si128((const __m128i *) (data + 7*8));

   // column pass
   dct_pass(bias_0, 10);

   {
      // 16bit 8x8 transpose pass 1
      dct_interleave16(row0, row4);
      dct_interleave16(row1, row5);
      dct_interleave16(row2, row6);
      dct_interleave16(row3, row7);

      // transpose pass 2
      dct_interleave16(row0, row2);
      dct_interleave16(row1, row3);
      dct_interleave16(row4, row6);
      dct_interleave16(row5, row7);

      // transpose pass 3
      dct_interleave16(row0, row1);
      dct_interleave16(row2, row3);
      dct_interleave16(row4, row5);
      dct_interleave16(row6, row7);
   }

   // row pass
   dct_pass(bias_1, 17);

   {
      // pack
      __m128i p0 = _mm_packus_epi16(row0, row1); // a0a1a2a3...a7b0b1b2b3...b7
      __m128i p1 = _mm_packus_epi16(row2, row3);
      __m128i p2 = _mm_packus_epi16(row4, row5);
      __m128i p3 = _mm_packus_epi16(row6, row7);

      // 8bit 8x8 transpose pass 1
      dct_interleave8(p0, p2); // a0e0a1e1...
      dct_interleave8(p1, p3); // c0g0c1g1...

      // transpose pass 2
      dct_interleave8(p0, p1); // a0c0e0g0...
      dct_interleave8(p2, p3); // b0d0f0h0...

      // transpose pass 3
      dct_interleave8(p0, p2); // a0b0c0d0...
      dct_interleave8(p1, p3); // a4b4c4d4...

      // store
      _mm_storel_epi64((__m128i *) out, p0); out += out_stride;
      _mm_storel_epi64((__m128i *) out, _mm_shuffle_epi32(p0, 0x4e)); out += out_stride;
      _mm_storel_epi64((__m128i *) out, p2); out += out_stride;
      _mm_storel_epi64((__m128i *) out, _mm_shuffle_epi32(p2, 0x4e)); out += out_stride;
      _mm_storel_epi64((__m128i *) out, p1); out += out_stride;
      _mm_storel_epi64((__m128i *) out, _mm_shuffle_epi32(p1, 0x4e)); out += out_stride;
      _mm_storel_epi64((__m128i *) out, p3); out += out_stride;
      _mm_storel_epi64((__m128i *) out, _mm_shuffle_epi32(p3, 0x4e));
   }

#undef dct_const
#undef dct_rot
#undef dct_widen
#undef dct_wadd
#undef dct_wsub
#undef dct_bfly32o
#undef dct_interleave8
#undef dct_interleave16
#undef dct_pass
}

#endif // STBI_SSE2

#ifdef STBI_NEON

// NEON integer IDCT. should produce bit-identical
// results to the generic C version.
static void stbi__idct_simd(stbi_uc *out, int out_stride, short data[64])
{
   int16x8_t row0, row1, row2, row3, row4, row5, row6, row7;

   int16x4_t rot0_0 = vdup_n_s16(stbi__f2f(0.5411961f));
   int16x4_t rot0_1 = vdup_n_s16(stbi__f2f(-1.847759065f));
   int16x4_t rot0_2 = vdup_n_s16(stbi__f2f( 0.765366865f));
   int16x4_t rot1_0 = vdup_n_s16(stbi__f2f( 1.175875602f));
   int16x4_t rot1_1 = vdup_n_s16(stbi__f2f(-0.899976223f));
   int16x4_t rot1_2 = vdup_n_s16(stbi__f2f(-2.562915447f));
   int16x4_t rot2_0 = vdup_n_s16(stbi__f2f(-1.961570560f));
   int16x4_t rot2_1 = vdup_n_s16(stbi__f2f(-0.390180644f));
   int16x4_t rot3_0 = vdup_n_s16(stbi__f2f( 0.298631336f));
   int16x4_t rot3_1 = vdup_n_s16(stbi__f2f( 2.053119869f));
   int16x4_t rot3_2 = vdup_n_s16(stbi__f2f( 3.072711026f));
   int16x4_t rot3_3 = vdup_n_s16(stbi__f2f( 1.501321110f));

#define dct_long_mul(out, inq, coeff) \
   int32x4_t out##_l = vmull_s16(vget_low_s16(inq), coeff); \
   int32x4_t out##_h = vmull_s16(vget_high_s16(inq), coeff)

#define dct_long_mac(out, acc, inq, coeff) \
   int32x4_t out##_l = vmlal_s16(acc##_l, vget_low_s16(inq), coeff); \
   int32x4_t out##_h = vmlal_s16(acc##_h, vget_high_s16(inq), coeff)

#define dct_widen(out, inq) \
   int32x4_t out##_l = vshll_n_s16(vget_low_s16(inq), 12); \
   int32x4_t out##_h = vshll_n_s16(vget_high_s16(inq), 12)

// wide add
#define dct_wadd(out, a, b) \
   int32x4_t out##_l = vaddq_s32(a##_l, b##_l); \
   int32x4_t out##_h = vaddq_s32(a##_h, b##_h)

// wide sub
#define dct_wsub(out, a, b) \
   int32x4_t out##_l = vsubq_s32(a##_l, b##_l); \
   int32x4_t out##_h = vsubq_s32(a##_h, b##_h)

// butterfly a/b, then shift using "shiftop" by "s" and pack
#define dct_bfly32o(out0,out1, a,b,shiftop,s) \
   { \
      dct_wadd(sum, a, b); \
      dct_wsub(dif, a, b); \
      out0 = vcombine_s16(shiftop(sum_l, s), shiftop(sum_h, s)); \
      out1 = vcombine_s16(shiftop(dif_l, s), shiftop(dif_h, s)); \
   }

#define dct_pass(shiftop, shift) \
   { \
      /* even part */ \
      int16x8_t sum26 = vaddq_s16(row2, row6); \
      dct_long_mul(p1e, sum26, rot0_0); \
      dct_long_mac(t2e, p1e, row6, rot0_1); \
      dct_long_mac(t3e, p1e, row2, rot0_2); \
      int16x8_t sum04 = vaddq_s16(row0, row4); \
      int16x8_t dif04 = vsubq_s16(row0, row4); \
      dct_widen(t0e, sum04); \
      dct_widen(t1e, dif04); \
      dct_wadd(x0, t0e, t3e); \
      dct_wsub(x3, t0e, t3e); \
      dct_wadd(x1, t1e, t2e); \
      dct_wsub(x2, t1e, t2e); \
      /* odd part */ \
      int16x8_t sum15 = vaddq_s16(row1, row5); \
      int16x8_t sum17 = vaddq_s16(row1, row7); \
      int16x8_t sum35 = vaddq_s16(row3, row5); \
      int16x8_t sum37 = vaddq_s16(row3, row7); \
      int16x8_t sumodd = vaddq_s16(sum17, sum35); \
      dct_long_mul(p5o, sumodd, rot1_0); \
      dct_long_mac(p1o, p5o, sum17, rot1_1); \
      dct_long_mac(p2o, p5o, sum35, rot1_2); \
      dct_long_mul(p3o, sum37, rot2_0); \
      dct_long_mul(p4o, sum15, rot2_1); \
      dct_wadd(sump13o, p1o, p3o); \
      dct_wadd(sump24o, p2o, p4o); \
      dct_wadd(sump23o, p2o, p3o); \
      dct_wadd(sump14o, p1o, p4o); \
      dct_long_mac(x4, sump13o, row7, rot3_0); \
      dct_long_mac(x5, sump24o, row5, rot3_1); \
      dct_long_mac(x6, sump23o, row3, rot3_2); \
      dct_long_mac(x7, sump14o, row1, rot3_3); \
      dct_bfly32o(row0,row7, x0,x7,shiftop,shift); \
      dct_bfly32o(row1,row6, x1,x6,shiftop,shift); \
      dct_bfly32o(row2,row5, x2,x5,shiftop,shift); \
      dct_bfly32o(row3,row4, x3,x4,shiftop,shift); \
   }

   // load
   row0 = vld1q_s16(data + 0*8);
   row1 = vld1q_s16(data + 1*8);
   row2 = vld1q_s16(data + 2*8);
   row3 = vld1q_s16(data + 3*8);
   row4 = vld1q_s16(data + 4*8);
   row5 = vld1q_s16(data + 5*8);
   row6 = vld1q_s16(data + 6*8);
   row7 = vld1q_s16(data + 7*8);

   // add DC bias
   row0 = vaddq_s16(row0, vsetq_lane_s16(1024, vdupq_n_s16(0), 0));

   // column pass
   dct_pass(vrshrn_n_s32, 10);

   // 16bit 8x8 transpose
   {
// these three map to a single VTRN.16, VTRN.32, and VSWP, respectively.
// whether compilers actually get this is another story, sadly.
#define dct_trn16(x, y) { int16x8x2_t t = vtrnq_s16(x, y); x = t.val[0]; y = t.val[1]; }
#define dct_trn32(x, y) { int32x4x2_t t = vtrnq_s32(vreinterpretq_s32_s16(x), vreinterpretq_s32_s16(y)); x = vreinterpretq_s16_s32(t.val[0]); y = vreinterpretq_s16_s32(t.val[1]); }
#define dct_trn64(x, y) { int16x8_t x0 = x; int16x8_t y0 = y; x = vcombine_s16(vget_low_s16(x0), vget_low_s16(y0)); y = vcombine_s16(vget_high_s16(x0), vget_high_s16(y0)); }

      // pass 1
      dct_trn16(row0, row1); // a0b0a2b2a4b4a6b6
      dct_trn16(row2, row3);
      dct_trn16(row4, row5);
      dct_trn16(row6, row7);

      // pass 2
      dct_trn32(row0, row2); // a0b0c0d0a4b4c4d4
      dct_trn32(row1, row3);
      dct_trn32(row4, row6);
      dct_trn32(row5, row7);

      // pass 3
      dct_trn64(row0, row4); // a0b0c0d0e0f0g0h0
      dct_trn64(row1, row5);
      dct_trn64(row2, row6);
      dct_trn64(row3, row7);

#undef dct_trn16
#undef dct_trn32
#undef dct_trn64
   }

   // row pass
   // vrshrn_n_s32 only supports shifts up to 16, we need
   // 17. so do a non-rounding shift of 16 first then follow
   // up with a rounding shift by 1.
   dct_pass(vshrn_n_s32, 16);

   {
      // pack and round
      uint8x8_t p0 = vqrshrun_n_s16(row0, 1);
      uint8x8_t p1 = vqrshrun_n_s16(row1, 1);
      uint8x8_t p2 = vqrshrun_n_s16(row2, 1);
      uint8x8_t p3 = vqrshrun_n_s16(row3, 1);
      uint8x8_t p4 = vqrshrun_n_s16(row4, 1);
      uint8x8_t p5 = vqrshrun_n_s16(row5, 1);
      uint8x8_t p6 = vqrshrun_n_s16(row6, 1);
      uint8x8_t p7 = vqrshrun_n_s16(row7, 1);

      // again, these can translate into one instruction, but often don't.
#define dct_trn8_8(x, y) { uint8x8x2_t t = vtrn_u8(x, y); x = t.val[0]; y = t.val[1]; }
#define dct_trn8_16(x, y) { uint16x4x2_t t = vtrn_u16(vreinterpret_u16_u8(x), vreinterpret_u16_u8(y)); x = vreinterpret_u8_u16(t.val[0]); y = vreinterpret_u8_u16(t.val[1]); }
#define dct_trn8_32(x, y) { uint32x2x2_t t = vtrn_u32(vreinterpret_u32_u8(x), vreinterpret_u32_u8(y)); x = vreinterpret_u8_u32(t.val[0]); y = vreinterpret_u8_u32(t.val[1]); }

      // sadly can't use interleaved stores here since we only write
      // 8 bytes to each scan line!

      // 8x8 8-bit transpose pass 1
      dct_trn8_8(p0, p1);
      dct_trn8_8(p2, p3);
      dct_trn8_8(p4, p5);
      dct_trn8_8(p6, p7);

      // pass 2
      dct_trn8_16(p0, p2);
      dct_trn8_16(p1, p3);
      dct_trn8_16(p4, p6);
      dct_trn8_16(p5, p7);

      // pass 3
      dct_trn8_32(p0, p4);
      dct_trn8_32(p1, p5);
      dct_trn8_32(p2, p6);
      dct_trn8_32(p3, p7);

      // store
      vst1_u8(out, p0); out += out_stride;
      vst1_u8(out, p1); out += out_stride;
      vst1_u8(out, p2); out += out_stride;
      vst1_u8(out, p3); out += out_stride;
      vst1_u8(out, p4); out += out_stride;
      vst1_u8(out, p5); out += out_stride;
      vst1_u8(out, p6); out += out_stride;
      vst1_u8(out, p7);

#undef dct_trn8_8
#undef dct_trn8_16
#undef dct_trn8_32
   }

#undef dct_long_mul
#undef dct_long_mac
#undef dct_widen
#undef dct_wadd
#undef dct_wsub
#undef dct_bfly32o
#undef dct_pass
}

#endif // STBI_NEON

#define STBI__MARKER_none  0xff
// if there's a pending marker from the entropy stream, return that
// otherwise, fetch from the stream and get a marker. if there's no
// marker, return 0xff, which is never a valid marker value
static stbi_uc stbi__get_marker(stbi__jpeg *j)
{
   stbi_uc x;
   if (j->marker != STBI__MARKER_none) { x = j->marker; j->marker = STBI__MARKER_none; return x; }
   x = stbi__get8(j->s);
   if (x != 0xff) return STBI__MARKER_none;
   while (x == 0xff)
      x = stbi__get8(j->s); // consume repeated 0xff fill bytes
   return x;
}

// in each scan, we'll have scan_n components, and the order
// of the components is specified by order[]
#define STBI__RESTART(x)     ((x) >= 0xd0 && (x) <= 0xd7)

// after a restart interval, stbi__jpeg_reset the entropy decoder and
// the dc prediction
static void stbi__jpeg_reset(stbi__jpeg *j)
{
   j->code_bits = 0;
   j->code_buffer = 0;
   j->nomore = 0;
   j->img_comp[0].dc_pred = j->img_comp[1].dc_pred = j->img_comp[2].dc_pred = j->img_comp[3].dc_pred = 0;
   j->marker = STBI__MARKER_none;
   j->todo = j->restart_interval ? j->restart_interval : 0x7fffffff;
   j->eob_run = 0;
   // no more than 1<<31 MCUs if no restart_interal? that's plenty safe,
   // since we don't even allow 1<<30 pixels
}

static int stbi__parse_entropy_coded_data(stbi__jpeg *z)
{
   stbi__jpeg_reset(z);
   if (!z->progressive) {
      if (z->scan_n == 1) {
         int i,j;
         STBI_SIMD_ALIGN(short, data[64]);
         int n = z->order[0];
         // non-interleaved data, we just need to process one block at a time,
         // in trivial scanline order
         // number of blocks to do just depends on how many actual "pixels" this
         // component has, independent of interleaved MCU blocking and such
         int w = (z->img_comp[n].x+7) >> 3;
         int h = (z->img_comp[n].y+7) >> 3;
         for (j=0; j < h; ++j) {
            for (i=0; i < w; ++i) {
               int ha = z->img_comp[n].ha;
               if (!stbi__jpeg_decode_block(z, data, z->huff_dc+z->img_comp[n].hd, z->huff_ac+ha, z->fast_ac[ha], n, z->dequant[z->img_comp[n].tq])) return 0;
               z->idct_block_kernel(z->img_comp[n].data+z->img_comp[n].w2*j*8+i*8, z->img_comp[n].w2, data);
               // every data block is an MCU, so countdown the restart interval
               if (--z->todo <= 0) {
                  if (z->code_bits < 24) stbi__grow_buffer_unsafe(z);
                  // if it's NOT a restart, then just bail, so we get corrupt data
                  // rather than no data
                  if (!STBI__RESTART(z->marker)) return 1;
                  stbi__jpeg_reset(z);
               }
            }
         }
         return 1;
      } else { // interleaved
         int i,j,k,x,y;
         STBI_SIMD_ALIGN(short, data[64]);
         for (j=0; j < z->img_mcu_y; ++j) {
            for (i=0; i < z->img_mcu_x; ++i) {
               // scan an interleaved mcu... process scan_n components in order
               for (k=0; k < z->scan_n; ++k) {
                  int n = z->order[k];
                  // scan out an mcu's worth of this component; that's just determined
                  // by the basic H and V specified for the component
                  for (y=0; y < z->img_comp[n].v; ++y) {
                     for (x=0; x < z->img_comp[n].h; ++x) {
                        int x2 = (i*z->img_comp[n].h + x)*8;
                        int y2 = (j*z->img_comp[n].v + y)*8;
                        int ha = z->img_comp[n].ha;
                        if (!stbi__jpeg_decode_block(z, data, z->huff_dc+z->img_comp[n].hd, z->huff_ac+ha, z->fast_ac[ha], n, z->dequant[z->img_comp[n].tq])) return 0;
                        z->idct_block_kernel(z->img_comp[n].data+z->img_comp[n].w2*y2+x2, z->img_comp[n].w2, data);
                     }
                  }
               }
               // after all interleaved components, that's an interleaved MCU,
               // so now count down the restart interval
               if (--z->todo <= 0) {
                  if (z->code_bits < 24) stbi__grow_buffer_unsafe(z);
                  if (!STBI__RESTART(z->marker)) return 1;
                  stbi__jpeg_reset(z);
               }
            }
         }
         return 1;
      }
   } else {
      if (z->scan_n == 1) {
         int i,j;
         int n = z->order[0];
         // non-interleaved data, we just need to process one block at a time,
         // in trivial scanline order
         // number of blocks to do just depends on how many actual "pixels" this
         // component has, independent of interleaved MCU blocking and such
         int w = (z->img_comp[n].x+7) >> 3;
         int h = (z->img_comp[n].y+7) >> 3;
         for (j=0; j < h; ++j) {
            for (i=0; i < w; ++i) {
               short *data = z->img_comp[n].coeff + 64 * (i + j * z->img_comp[n].coeff_w);
               if (z->spec_start == 0) {
                  if (!stbi__jpeg_decode_block_prog_dc(z, data, &z->huff_dc[z->img_comp[n].hd], n))
                     return 0;
               } else {
                  int ha = z->img_comp[n].ha;
                  if (!stbi__jpeg_decode_block_prog_ac(z, data, &z->huff_ac[ha], z->fast_ac[ha]))
                     return 0;
               }
               // every data block is an MCU, so countdown the restart interval
               if (--z->todo <= 0) {
                  if (z->code_bits < 24) stbi__grow_buffer_unsafe(z);
                  if (!STBI__RESTART(z->marker)) return 1;
                  stbi__jpeg_reset(z);
               }
            }
         }
         return 1;
      } else { // interleaved
         int i,j,k,x,y;
         for (j=0; j < z->img_mcu_y; ++j) {
            for (i=0; i < z->img_mcu_x; ++i) {
               // scan an interleaved mcu... process scan_n components in order
               for (k=0; k < z->scan_n; ++k) {
                  int n = z->order[k];
                  // scan out an mcu's worth of this component; that's just determined
                  // by the basic H and V specified for the component
                  for (y=0; y < z->img_comp[n].v; ++y) {
                     for (x=0; x < z->img_comp[n].h; ++x) {
                        int x2 = (i*z->img_comp[n].h + x);
                        int y2 = (j*z->img_comp[n].v + y);
                        short *data = z->img_comp[n].coeff + 64 * (x2 + y2 * z->img_comp[n].coeff_w);
                        if (!stbi__jpeg_decode_block_prog_dc(z, data, &z->huff_dc[z->img_comp[n].hd], n))
                           return 0;
                     }
                  }
               }
               // after all interleaved components, that's an interleaved MCU,
               // so now count down the restart interval
               if (--z->todo <= 0) {
                  if (z->code_bits < 24) stbi__grow_buffer_unsafe(z);
                  if (!STBI__RESTART(z->marker)) return 1;
                  stbi__jpeg_reset(z);
               }
            }
         }
         return 1;
      }
   }
}

static void stbi__jpeg_dequantize(short *data, stbi__uint16 *dequant)
{
   int i;
   for (i=0; i < 64; ++i)
      data[i] *= dequant[i];
}

static void stbi__jpeg_finish(stbi__jpeg *z)
{
   if (z->progressive) {
      // dequantize and idct the data
      int i,j,n;
      for (n=0; n < z->s->img_n; ++n) {
         int w = (z->img_comp[n].x+7) >> 3;
         int h = (z->img_comp[n].y+7) >> 3;
         for (j=0; j < h; ++j) {
            for (i=0; i < w; ++i) {
               short *data = z->img_comp[n].coeff + 64 * (i + j * z->img_comp[n].coeff_w);
               stbi__jpeg_dequantize(data, z->dequant[z->img_comp[n].tq]);
               z->idct_block_kernel(z->img_comp[n].data+z->img_comp[n].w2*j*8+i*8, z->img_comp[n].w2, data);
            }
         }
      }
   }
}

static int stbi__process_marker(stbi__jpeg *z, int m)
{
   int L;
   switch (m) {
      case STBI__MARKER_none: // no marker found
         return stbi__err("expected marker","Corrupt JPEG");

      case 0xDD: // DRI - specify restart interval
         if (stbi__get16be(z->s) != 4) return stbi__err("bad DRI len","Corrupt JPEG");
         z->restart_interval = stbi__get16be(z->s);
         return 1;

      case 0xDB: // DQT - define quantization table
         L = stbi__get16be(z->s)-2;
         while (L > 0) {
            int q = stbi__get8(z->s);
            int p = q >> 4, sixteen = (p != 0);
            int t = q & 15,i;
            if (p != 0 && p != 1) return stbi__err("bad DQT type","Corrupt JPEG");
            if (t > 3) return stbi__err("bad DQT table","Corrupt JPEG");

            for (i=0; i < 64; ++i)
               z->dequant[t][stbi__jpeg_dezigzag[i]] = (stbi__uint16)(sixteen ? stbi__get16be(z->s) : stbi__get8(z->s));
            L -= (sixteen ? 129 : 65);
         }
         return L==0;

      case 0xC4: // DHT - define huffman table
         L = stbi__get16be(z->s)-2;
         while (L > 0) {
            stbi_uc *v;
            int sizes[16],i,n=0;
            int q = stbi__get8(z->s);
            int tc = q >> 4;
            int th = q & 15;
            if (tc > 1 || th > 3) return stbi__err("bad DHT header","Corrupt JPEG");
            for (i=0; i < 16; ++i) {
               sizes[i] = stbi__get8(z->s);
               n += sizes[i];
            }
            L -= 17;
            if (tc == 0) {
               if (!stbi__build_huffman(z->huff_dc+th, sizes)) return 0;
               v = z->huff_dc[th].values;
            } else {
               if (!stbi__build_huffman(z->huff_ac+th, sizes)) return 0;
               v = z->huff_ac[th].values;
            }
            for (i=0; i < n; ++i)
               v[i] = stbi__get8(z->s);
            if (tc != 0)
               stbi__build_fast_ac(z->fast_ac[th], z->huff_ac + th);
            L -= n;
         }
         return L==0;
   }

   // check for comment block or APP blocks
   if ((m >= 0xE0 && m <= 0xEF) || m == 0xFE) {
      L = stbi__get16be(z->s);
      if (L < 2) {
         if (m == 0xFE)
            return stbi__err("bad COM len","Corrupt JPEG");
         else
            return stbi__err("bad APP len","Corrupt JPEG");
      }
      L -= 2;

      if (m == 0xE0 && L >= 5) { // JFIF APP0 segment
         static const unsigned char tag[5] = {'J','F','I','F','\0'};
         int ok = 1;
         int i;
         for (i=0; i < 5; ++i)
            if (stbi__get8(z->s) != tag[i])
               ok = 0;
         L -= 5;
         if (ok)
            z->jfif = 1;
      } else if (m == 0xEE && L >= 12) { // Adobe APP14 segment
         static const unsigned char tag[6] = {'A','d','o','b','e','\0'};
         int ok = 1;
         int i;
         for (i=0; i < 6; ++i)
            if (stbi__get8(z->s) != tag[i])
               ok = 0;
         L -= 6;
         if (ok) {
            stbi__get8(z->s); // version
            stbi__get16be(z->s); // flags0
            stbi__get16be(z->s); // flags1
            z->app14_color_transform = stbi__get8(z->s); // color transform
            L -= 6;
         }
      }

      stbi__skip(z->s, L);
      return 1;
   }

   return stbi__err("unknown marker","Corrupt JPEG");
}

// after we see SOS
static int stbi__process_scan_header(stbi__jpeg *z)
{
   int i;
   int Ls = stbi__get16be(z->s);
   z->scan_n = stbi__get8(z->s);
   if (z->scan_n < 1 || z->scan_n > 4 || z->scan_n > (int) z->s->img_n) return stbi__err("bad SOS component count","Corrupt JPEG");
   if (Ls != 6+2*z->scan_n) return stbi__err("bad SOS len","Corrupt JPEG");
   for (i=0; i < z->scan_n; ++i) {
      int id = stbi__get8(z->s), which;
      int q = stbi__get8(z->s);
      for (which = 0; which < z->s->img_n; ++which)
         if (z->img_comp[which].id == id)
            break;
      if (which == z->s->img_n) return 0; // no match
      z->img_comp[which].hd = q >> 4;   if (z->img_comp[which].hd > 3) return stbi__err("bad DC huff","Corrupt JPEG");
      z->img_comp[which].ha = q & 15;   if (z->img_comp[which].ha > 3) return stbi__err("bad AC huff","Corrupt JPEG");
      z->order[i] = which;
   }

   {
      int aa;
      z->spec_start = stbi__get8(z->s);
      z->spec_end   = stbi__get8(z->s); // should be 63, but might be 0
      aa = stbi__get8(z->s);
      z->succ_high = (aa >> 4);
      z->succ_low  = (aa & 15);
      if (z->progressive) {
         if (z->spec_start > 63 || z->spec_end > 63  || z->spec_start > z->spec_end || z->succ_high > 13 || z->succ_low > 13)
            return stbi__err("bad SOS", "Corrupt JPEG");
      } else {
         if (z->spec_start != 0) return stbi__err("bad SOS","Corrupt JPEG");
         if (z->succ_high != 0 || z->succ_low != 0) return stbi__err("bad SOS","Corrupt JPEG");
         z->spec_end = 63;
      }
   }

   return 1;
}

static int stbi__free_jpeg_components(stbi__jpeg *z, int ncomp, int why)
{
   int i;
   for (i=0; i < ncomp; ++i) {
      if (z->img_comp[i].raw_data) {
         STBI_FREE(z->img_comp[i].raw_data);
         z->img_comp[i].raw_data = NULL;
         z->img_comp[i].data = NULL;
      }
      if (z->img_comp[i].raw_coeff) {
         STBI_FREE(z->img_comp[i].raw_coeff);
         z->img_comp[i].raw_coeff = 0;
         z->img_comp[i].coeff = 0;
      }
      if (z->img_comp[i].linebuf) {
         STBI_FREE(z->img_comp[i].linebuf);
         z->img_comp[i].linebuf = NULL;
      }
   }
   return why;
}

static int stbi__process_frame_header(stbi__jpeg *z, int scan)
{
   stbi__context *s = z->s;
   int Lf,p,i,q, h_max=1,v_max=1,c;
   Lf = stbi__get16be(s);         if (Lf < 11) return stbi__err("bad SOF len","Corrupt JPEG"); // JPEG
   p  = stbi__get8(s);            if (p != 8) return stbi__err("only 8-bit","JPEG format not supported: 8-bit only"); // JPEG baseline
   s->img_y = stbi__get16be(s);   if (s->img_y == 0) return stbi__err("no header height", "JPEG format not supported: delayed height"); // Legal, but we don't handle it--but neither does IJG
   s->img_x = stbi__get16be(s);   if (s->img_x == 0) return stbi__err("0 width","Corrupt JPEG"); // JPEG requires
   if (s->img_y > STBI_MAX_DIMENSIONS) return stbi__err("too large","Very large image (corrupt?)");
   if (s->img_x > STBI_MAX_DIMENSIONS) return stbi__err("too large","Very large image (corrupt?)");
   c = stbi__get8(s);
   if (c != 3 && c != 1 && c != 4) return stbi__err("bad component count","Corrupt JPEG");
   s->img_n = c;
   for (i=0; i < c; ++i) {
      z->img_comp[i].data = NULL;
      z->img_comp[i].linebuf = NULL;
   }

   if (Lf != 8+3*s->img_n) return stbi__err("bad SOF len","Corrupt JPEG");

   z->rgb = 0;
   for (i=0; i < s->img_n; ++i) {
      static const unsigned char rgb[3] = { 'R', 'G', 'B' };
      z->img_comp[i].id = stbi__get8(s);
      if (s->img_n == 3 && z->img_comp[i].id == rgb[i])
         ++z->rgb;
      q = stbi__get8(s);
      z->img_comp[i].h = (q >> 4);  if (!z->img_comp[i].h || z->img_comp[i].h > 4) return stbi__err("bad H","Corrupt JPEG");
      z->img_comp[i].v = q & 15;    if (!z->img_comp[i].v || z->img_comp[i].v > 4) return stbi__err("bad V","Corrupt JPEG");
      z->img_comp[i].tq = stbi__get8(s);  if (z->img_comp[i].tq > 3) return stbi__err("bad TQ","Corrupt JPEG");
   }

   if (scan != STBI__SCAN_load) return 1;

   if (!stbi__mad3sizes_valid(s->img_x, s->img_y, s->img_n, 0)) return stbi__err("too large", "Image too large to decode");

   for (i=0; i < s->img_n; ++i) {
      if (z->img_comp[i].h > h_max) h_max = z->img_comp[i].h;
      if (z->img_comp[i].v > v_max) v_max = z->img_comp[i].v;
   }

   // check that plane subsampling factors are integer ratios; our resamplers can't deal with fractional ratios
   // and I've never seen a non-corrupted JPEG file actually use them
   for (i=0; i < s->img_n; ++i) {
      if (h_max % z->img_comp[i].h != 0) return stbi__err("bad H","Corrupt JPEG");
      if (v_max % z->img_comp[i].v != 0) return stbi__err("bad V","Corrupt JPEG");
   }

   // compute interleaved mcu info
   z->img_h_max = h_max;
   z->img_v_max = v_max;
   z->img_mcu_w = h_max * 8;
   z->img_mcu_h = v_max * 8;
   // these sizes can't be more than 17 bits
   z->img_mcu_x = (s->img_x + z->img_mcu_w-1) / z->img_mcu_w;
   z->img_mcu_y = (s->img_y + z->img_mcu_h-1) / z->img_mcu_h;

   for (i=0; i < s->img_n; ++i) {
      // number of effective pixels (e.g. for non-interleaved MCU)
      z->img_comp[i].x = (s->img_x * z->img_comp[i].h + h_max-1) / h_max;
      z->img_comp[i].y = (s->img_y * z->img_comp[i].v + v_max-1) / v_max;
      // to simplify generation, we'll allocate enough memory to decode
      // the bogus oversized data from using interleaved MCUs and their
      // big blocks (e.g. a 16x16 iMCU on an image of width 33); we won't
      // discard the extra data until colorspace conversion
      //
      // img_mcu_x, img_mcu_y: <=17 bits; comp[i].h and .v are <=4 (checked earlier)
      // so these muls can't overflow with 32-bit ints (which we require)
      z->img_comp[i].w2 = z->img_mcu_x * z->img_comp[i].h * 8;
      z->img_comp[i].h2 = z->img_mcu_y * z->img_comp[i].v * 8;
      z->img_comp[i].coeff = 0;
      z->img_comp[i].raw_coeff = 0;
      z->img_comp[i].linebuf = NULL;
      z->img_comp[i].raw_data = stbi__malloc_mad2(z->img_comp[i].w2, z->img_comp[i].h2, 15);
      if (z->img_comp[i].raw_data == NULL)
         return stbi__free_jpeg_components(z, i+1, stbi__err("outofmem", "Out of memory"));
      // align blocks for idct using mmx/sse
      z->img_comp[i].data = (stbi_uc*) (((size_t) z->img_comp[i].raw_data + 15) & ~15);
      if (z->progressive) {
         // w2, h2 are multiples of 8 (see above)
         z->img_comp[i].coeff_w = z->img_comp[i].w2 / 8;
         z->img_comp[i].coeff_h = z->img_comp[i].h2 / 8;
         z->img_comp[i].raw_coeff = stbi__malloc_mad3(z->img_comp[i].w2, z->img_comp[i].h2, sizeof(short), 15);
         if (z->img_comp[i].raw_coeff == NULL)
            return stbi__free_jpeg_components(z, i+1, stbi__err("outofmem", "Out of memory"));
         z->img_comp[i].coeff = (short*) (((size_t) z->img_comp[i].raw_coeff + 15) & ~15);
      }
   }

   return 1;
}

// use comparisons since in some cases we handle more than one case (e.g. SOF)
#define stbi__DNL(x)         ((x) == 0xdc)
#define stbi__SOI(x)         ((x) == 0xd8)
#define stbi__EOI(x)         ((x) == 0xd9)
#define stbi__SOF(x)         ((x) == 0xc0 || (x) == 0xc1 || (x) == 0xc2)
#define stbi__SOS(x)         ((x) == 0xda)

#define stbi__SOF_progressive(x)   ((x) == 0xc2)

static int stbi__decode_jpeg_header(stbi__jpeg *z, int scan)
{
   int m;
   z->jfif = 0;
   z->app14_color_transform = -1; // valid values are 0,1,2
   z->marker = STBI__MARKER_none; // initialize cached marker to empty
   m = stbi__get_marker(z);
   if (!stbi__SOI(m)) return stbi__err("no SOI","Corrupt JPEG");
   if (scan == STBI__SCAN_type) return 1;
   m = stbi__get_marker(z);
   while (!stbi__SOF(m)) {
      if (!stbi__process_marker(z,m)) return 0;
      m = stbi__get_marker(z);
      while (m == STBI__MARKER_none) {
         // some files have extra padding after their blocks, so ok, we'll scan
         if (stbi__at_eof(z->s)) return stbi__err("no SOF", "Corrupt JPEG");
         m = stbi__get_marker(z);
      }
   }
   z->progressive = stbi__SOF_progressive(m);
   if (!stbi__process_frame_header(z, scan)) return 0;
   return 1;
}

// decode image to YCbCr format
static int stbi__decode_jpeg_image(stbi__jpeg *j)
{
   int m;
   for (m = 0; m < 4; m++) {
      j->img_comp[m].raw_data = NULL;
      j->img_comp[m].raw_coeff = NULL;
   }
   j->restart_interval = 0;
   if (!stbi__decode_jpeg_header(j, STBI__SCAN_load)) return 0;
   m = stbi__get_marker(j);
   while (!stbi__EOI(m)) {
      if (stbi__SOS(m)) {
         if (!stbi__process_scan_header(j)) return 0;
         if (!stbi__parse_entropy_coded_data(j)) return 0;
         if (j->marker == STBI__MARKER_none ) {
            // handle 0s at the end of image data from IP Kamera 9060
            while (!stbi__at_eof(j->s)) {
               int x = stbi__get8(j->s);
               if (x == 255) {
                  j->marker = stbi__get8(j->s);
                  break;
               }
            }
            // if we reach eof without hitting a marker, stbi__get_marker() below will fail and we'll eventually return 0
         }
      } else if (stbi__DNL(m)) {
         int Ld = stbi__get16be(j->s);
         stbi__uint32 NL = stbi__get16be(j->s);
         if (Ld != 4) return stbi__err("bad DNL len", "Corrupt JPEG");
         if (NL != j->s->img_y) return stbi__err("bad DNL height", "Corrupt JPEG");
      } else {
         if (!stbi__process_marker(j, m)) return 0;
      }
      m = stbi__get_marker(j);
   }
   if (j->progressive)
      stbi__jpeg_finish(j);
   return 1;
}

// static jfif-centered resampling (across block boundaries)

typedef stbi_uc *(*resample_row_func)(stbi_uc *out, stbi_uc *in0, stbi_uc *in1,
                                    int w, int hs);

#define stbi__div4(x) ((stbi_uc) ((x) >> 2))

static stbi_uc *resample_row_1(stbi_uc *out, stbi_uc *in_near, stbi_uc *in_far, int w, int hs)
{
   STBI_NOTUSED(out);
   STBI_NOTUSED(in_far);
   STBI_NOTUSED(w);
   STBI_NOTUSED(hs);
   return in_near;
}

static stbi_uc* stbi__resample_row_v_2(stbi_uc *out, stbi_uc *in_near, stbi_uc *in_far, int w, int hs)
{
   // need to generate two samples vertically for every one in input
   int i;
   STBI_NOTUSED(hs);
   for (i=0; i < w; ++i)
      out[i] = stbi__div4(3*in_near[i] + in_far[i] + 2);
   return out;
}

static stbi_uc*  stbi__resample_row_h_2(stbi_uc *out, stbi_uc *in_near, stbi_uc *in_far, int w, int hs)
{
   // need to generate two samples horizontally for every one in input
   int i;
   stbi_uc *input = in_near;

   if (w == 1) {
      // if only one sample, can't do any interpolation
      out[0] = out[1] = input[0];
      return out;
   }

   out[0] = input[0];
   out[1] = stbi__div4(input[0]*3 + input[1] + 2);
   for (i=1; i < w-1; ++i) {
      int n = 3*input[i]+2;
      out[i*2+0] = stbi__div4(n+input[i-1]);
      out[i*2+1] = stbi__div4(n+input[i+1]);
   }
   out[i*2+0] = stbi__div4(input[w-2]*3 + input[w-1] + 2);
   out[i*2+1] = input[w-1];

   STBI_NOTUSED(in_far);
   STBI_NOTUSED(hs);

   return out;
}

#define stbi__div16(x) ((stbi_uc) ((x) >> 4))

static stbi_uc *stbi__resample_row_hv_2(stbi_uc *out, stbi_uc *in_near, stbi_uc *in_far, int w, int hs)
{
   // need to generate 2x2 samples for every one in input
   int i,t0,t1;
   if (w == 1) {
      out[0] = out[1] = stbi__div4(3*in_near[0] + in_far[0] + 2);
      return out;
   }

   t1 = 3*in_near[0] + in_far[0];
   out[0] = stbi__div4(t1+2);
   for (i=1; i < w; ++i) {
      t0 = t1;
      t1 = 3*in_near[i]+in_far[i];
      out[i*2-1] = stbi__div16(3*t0 + t1 + 8);
      out[i*2  ] = stbi__div16(3*t1 + t0 + 8);
   }
   out[w*2-1] = stbi__div4(t1+2);

   STBI_NOTUSED(hs);

   return out;
}

#if defined(STBI_SSE2) || defined(STBI_NEON)
static stbi_uc *stbi__resample_row_hv_2_simd(stbi_uc *out, stbi_uc *in_near, stbi_uc *in_far, int w, int hs)
{
   // need to generate 2x2 samples for every one in input
   int i=0,t0,t1;

   if (w == 1) {
      out[0] = out[1] = stbi__div4(3*in_near[0] + in_far[0] + 2);
      return out;
   }

   t1 = 3*in_near[0] + in_far[0];
   // process groups of 8 pixels for as long as we can.
   // note we can't handle the last pixel in a row in this loop
   // because we need to handle the filter boundary conditions.
   for (; i < ((w-1) & ~7); i += 8) {
#if defined(STBI_SSE2)
      // load and perform the vertical filtering pass
      // this uses 3*x + y = 4*x + (y - x)
      __m128i zero  = _mm_setzero_si128();
      __m128i farb  = _mm_loadl_epi64((__m128i *) (in_far + i));
      __m128i nearb = _mm_loadl_epi64((__m128i *) (in_near + i));
      __m128i farw  = _mm_unpacklo_epi8(farb, zero);
      __m128i nearw = _mm_unpacklo_epi8(nearb, zero);
      __m128i diff  = _mm_sub_epi16(farw, nearw);
      __m128i nears = _mm_slli_epi16(nearw, 2);
      __m128i curr  = _mm_add_epi16(nears, diff); // current row

      // horizontal filter works the same based on shifted vers of current
      // row. "prev" is current row shifted right by 1 pixel; we need to
      // insert the previous pixel value (from t1).
      // "next" is current row shifted left by 1 pixel, with first pixel
      // of next block of 8 pixels added in.
      __m128i prv0 = _mm_slli_si128(curr, 2);
      __m128i nxt0 = _mm_srli_si128(curr, 2);
      __m128i prev = _mm_insert_epi16(prv0, t1, 0);
      __m128i next = _mm_insert_epi16(nxt0, 3*in_near[i+8] + in_far[i+8], 7);

      // horizontal filter, polyphase implementation since it's convenient:
      // even pixels = 3*cur + prev = cur*4 + (prev - cur)
      // odd  pixels = 3*cur + next = cur*4 + (next - cur)
      // note the shared term.
      __m128i bias  = _mm_set1_epi16(8);
      __m128i curs = _mm_slli_epi16(curr, 2);
      __m128i prvd = _mm_sub_epi16(prev, curr);
      __m128i nxtd = _mm_sub_epi16(next, curr);
      __m128i curb = _mm_add_epi16(curs, bias);
      __m128i even = _mm_add_epi16(prvd, curb);
      __m128i odd  = _mm_add_epi16(nxtd, curb);

      // interleave even and odd pixels, then undo scaling.
      __m128i int0 = _mm_unpacklo_epi16(even, odd);
      __m128i int1 = _mm_unpackhi_epi16(even, odd);
      __m128i de0  = _mm_srli_epi16(int0, 4);
      __m128i de1  = _mm_srli_epi16(int1, 4);

      // pack and write output
      __m128i outv = _mm_packus_epi16(de0, de1);
      _mm_storeu_si128((__m128i *) (out + i*2), outv);
#elif defined(STBI_NEON)
      // load and perform the vertical filtering pass
      // this uses 3*x + y = 4*x + (y - x)
      uint8x8_t farb  = vld1_u8(in_far + i);
      uint8x8_t nearb = vld1_u8(in_near + i);
      int16x8_t diff  = vreinterpretq_s16_u16(vsubl_u8(farb, nearb));
      int16x8_t nears = vreinterpretq_s16_u16(vshll_n_u8(nearb, 2));
      int16x8_t curr  = vaddq_s16(nears, diff); // current row

      // horizontal filter works the same based on shifted vers of current
      // row. "prev" is current row shifted right by 1 pixel; we need to
      // insert the previous pixel value (from t1).
      // "next" is current row shifted left by 1 pixel, with first pixel
      // of next block of 8 pixels added in.
      int16x8_t prv0 = vextq_s16(curr, curr, 7);
      int16x8_t nxt0 = vextq_s16(curr, curr, 1);
      int16x8_t prev = vsetq_lane_s16(t1, prv0, 0);
      int16x8_t next = vsetq_lane_s16(3*in_near[i+8] + in_far[i+8], nxt0, 7);

      // horizontal filter, polyphase implementation since it's convenient:
      // even pixels = 3*cur + prev = cur*4 + (prev - cur)
      // odd  pixels = 3*cur + next = cur*4 + (next - cur)
      // note the shared term.
      int16x8_t curs = vshlq_n_s16(curr, 2);
      int16x8_t prvd = vsubq_s16(prev, curr);
      int16x8_t nxtd = vsubq_s16(next, curr);
      int16x8_t even = vaddq_s16(curs, prvd);
      int16x8_t odd  = vaddq_s16(curs, nxtd);

      // undo scaling and round, then store with even/odd phases interleaved
      uint8x8x2_t o;
      o.val[0] = vqrshrun_n_s16(even, 4);
      o.val[1] = vqrshrun_n_s16(odd,  4);
      vst2_u8(out + i*2, o);
#endif

      // "previous" value for next iter
      t1 = 3*in_near[i+7] + in_far[i+7];
   }

   t0 = t1;
   t1 = 3*in_near[i] + in_far[i];
   out[i*2] = stbi__div16(3*t1 + t0 + 8);

   for (++i; i < w; ++i) {
      t0 = t1;
      t1 = 3*in_near[i]+in_far[i];
      out[i*2-1] = stbi__div16(3*t0 + t1 + 8);
      out[i*2  ] = stbi__div16(3*t1 + t0 + 8);
   }
   out[w*2-1] = stbi__div4(t1+2);

   STBI_NOTUSED(hs);

   return out;
}
#endif

static stbi_uc *stbi__resample_row_generic(stbi_uc *out, stbi_uc *in_near, stbi_uc *in_far, int w, int hs)
{
   // resample with nearest-neighbor
   int i,j;
   STBI_NOTUSED(in_far);
   for (i=0; i < w; ++i)
      for (j=0; j < hs; ++j)
         out[i*hs+j] = in_near[i];
   return out;
}

// this is a reduced-precision calculation of YCbCr-to-RGB introduced
// to make sure the code produces the same results in both SIMD and scalar
#define stbi__float2fixed(x)  (((int) ((x) * 4096.0f + 0.5f)) << 8)
static void stbi__YCbCr_to_RGB_row(stbi_uc *out, const stbi_uc *y, const stbi_uc *pcb, const stbi_uc *pcr, int count, int step)
{
   int i;
   for (i=0; i < count; ++i) {
      int y_fixed = (y[i] << 20) + (1<<19); // rounding
      int r,g,b;
      int cr = pcr[i] - 128;
      int cb = pcb[i] - 128;
      r = y_fixed +  cr* stbi__float2fixed(1.40200f);
      g = y_fixed + (cr*-stbi__float2fixed(0.71414f)) + ((cb*-stbi__float2fixed(0.34414f)) & 0xffff0000);
      b = y_fixed                                     +   cb* stbi__float2fixed(1.77200f);
      r >>= 20;
      g >>= 20;
      b >>= 20;
      if ((unsigned) r > 255) { if (r < 0) r = 0; else r = 255; }
      if ((unsigned) g > 255) { if (g < 0) g = 0; else g = 255; }
      if ((unsigned) b > 255) { if (b < 0) b = 0; else b = 255; }
      out[0] = (stbi_uc)r;
      out[1] = (stbi_uc)g;
      out[2] = (stbi_uc)b;
      out[3] = 255;
      out += step;
   }
}

#if defined(STBI_SSE2) || defined(STBI_NEON)
static void stbi__YCbCr_to_RGB_simd(stbi_uc *out, stbi_uc const *y, stbi_uc const *pcb, stbi_uc const *pcr, int count, int step)
{
   int i = 0;

#ifdef STBI_SSE2
   // step == 3 is pretty ugly on the final interleave, and i'm not convinced
   // it's useful in practice (you wouldn't use it for textures, for example).
   // so just accelerate step == 4 case.
   if (step == 4) {
      // this is a fairly straightforward implementation and not super-optimized.
      __m128i signflip  = _mm_set1_epi8(-0x80);
      __m128i cr_const0 = _mm_set1_epi16(   (short) ( 1.40200f*4096.0f+0.5f));
      __m128i cr_const1 = _mm_set1_epi16( - (short) ( 0.71414f*4096.0f+0.5f));
      __m128i cb_const0 = _mm_set1_epi16( - (short) ( 0.34414f*4096.0f+0.5f));
      __m128i cb_const1 = _mm_set1_epi16(   (short) ( 1.77200f*4096.0f+0.5f));
      __m128i y_bias = _mm_set1_epi8((char) (unsigned char) 128);
      __m128i xw = _mm_set1_epi16(255); // alpha channel

      for (; i+7 < count; i += 8) {
         // load
         __m128i y_bytes = _mm_loadl_epi64((__m128i *) (y+i));
         __m128i cr_bytes = _mm_loadl_epi64((__m128i *) (pcr+i));
         __m128i cb_bytes = _mm_loadl_epi64((__m128i *) (pcb+i));
         __m128i cr_biased = _mm_xor_si128(cr_bytes, signflip); // -128
         __m128i cb_biased = _mm_xor_si128(cb_bytes, signflip); // -128

         // unpack to short (and left-shift cr, cb by 8)
         __m128i yw  = _mm_unpacklo_epi8(y_bias, y_bytes);
         __m128i crw = _mm_unpacklo_epi8(_mm_setzero_si128(), cr_biased);
         __m128i cbw = _mm_unpacklo_epi8(_mm_setzero_si128(), cb_biased);

         // color transform
         __m128i yws = _mm_srli_epi16(yw, 4);
         __m128i cr0 = _mm_mulhi_epi16(cr_const0, crw);
         __m128i cb0 = _mm_mulhi_epi16(cb_const0, cbw);
         __m128i cb1 = _mm_mulhi_epi16(cbw, cb_const1);
         __m128i cr1 = _mm_mulhi_epi16(crw, cr_const1);
         __m128i rws = _mm_add_epi16(cr0, yws);
         __m128i gwt = _mm_add_epi16(cb0, yws);
         __m128i bws = _mm_add_epi16(yws, cb1);
         __m128i gws = _mm_add_epi16(gwt, cr1);

         // descale
         __m128i rw = _mm_srai_epi16(rws, 4);
         __m128i bw = _mm_srai_epi16(bws, 4);
         __m128i gw = _mm_srai_epi16(gws, 4);

         // back to byte, set up for transpose
         __m128i brb = _mm_packus_epi16(rw, bw);
         __m128i gxb = _mm_packus_epi16(gw, xw);

         // transpose to interleave channels
         __m128i t0 = _mm_unpacklo_epi8(brb, gxb);
         __m128i t1 = _mm_unpackhi_epi8(brb, gxb);
         __m128i o0 = _mm_unpacklo_epi16(t0, t1);
         __m128i o1 = _mm_unpackhi_epi16(t0, t1);

         // store
         _mm_storeu_si128((__m128i *) (out + 0), o0);
         _mm_storeu_si128((__m128i *) (out + 16), o1);
         out += 32;
      }
   }
#endif

#ifdef STBI_NEON
   // in this version, step=3 support would be easy to add. but is there demand?
   if (step == 4) {
      // this is a fairly straightforward implementation and not super-optimized.
      uint8x8_t signflip = vdup_n_u8(0x80);
      int16x8_t cr_const0 = vdupq_n_s16(   (short) ( 1.40200f*4096.0f+0.5f));
      int16x8_t cr_const1 = vdupq_n_s16( - (short) ( 0.71414f*4096.0f+0.5f));
      int16x8_t cb_const0 = vdupq_n_s16( - (short) ( 0.34414f*4096.0f+0.5f));
      int16x8_t cb_const1 = vdupq_n_s16(   (short) ( 1.77200f*4096.0f+0.5f));

      for (; i+7 < count; i += 8) {
         // load
         uint8x8_t y_bytes  = vld1_u8(y + i);
         uint8x8_t cr_bytes = vld1_u8(pcr + i);
         uint8x8_t cb_bytes = vld1_u8(pcb + i);
         int8x8_t cr_biased = vreinterpret_s8_u8(vsub_u8(cr_bytes, signflip));
         int8x8_t cb_biased = vreinterpret_s8_u8(vsub_u8(cb_bytes, signflip));

         // expand to s16
         int16x8_t yws = vreinterpretq_s16_u16(vshll_n_u8(y_bytes, 4));
         int16x8_t crw = vshll_n_s8(cr_biased, 7);
         int16x8_t cbw = vshll_n_s8(cb_biased, 7);

         // color transform
         int16x8_t cr0 = vqdmulhq_s16(crw, cr_const0);
         int16x8_t cb0 = vqdmulhq_s16(cbw, cb_const0);
         int16x8_t cr1 = vqdmulhq_s16(crw, cr_const1);
         int16x8_t cb1 = vqdmulhq_s16(cbw, cb_const1);
         int16x8_t rws = vaddq_s16(yws, cr0);
         int16x8_t gws = vaddq_s16(vaddq_s16(yws, cb0), cr1);
         int16x8_t bws = vaddq_s16(yws, cb1);

         // undo scaling, round, convert to byte
         uint8x8x4_t o;
         o.val[0] = vqrshrun_n_s16(rws, 4);
         o.val[1] = vqrshrun_n_s16(gws, 4);
         o.val[2] = vqrshrun_n_s16(bws, 4);
         o.val[3] = vdup_n_u8(255);

         // store, interleaving r/g/b/a
         vst4_u8(out, o);
         out += 8*4;
      }
   }
#endif

   for (; i < count; ++i) {
      int y_fixed = (y[i] << 20) + (1<<19); // rounding
      int r,g,b;
      int cr = pcr[i] - 128;
      int cb = pcb[i] - 128;
      r = y_fixed + cr* stbi__float2fixed(1.40200f);
      g = y_fixed + cr*-stbi__float2fixed(0.71414f) + ((cb*-stbi__float2fixed(0.34414f)) & 0xffff0000);
      b = y_fixed                                   +   cb* stbi__float2fixed(1.77200f);
      r >>= 20;
      g >>= 20;
      b >>= 20;
      if ((unsigned) r > 255) { if (r < 0) r = 0; else r = 255; }
      if ((unsigned) g > 255) { if (g < 0) g = 0; else g = 255; }
      if ((unsigned) b > 255) { if (b < 0) b = 0; else b = 255; }
      out[0] = (stbi_uc)r;
      out[1] = (stbi_uc)g;
      out[2] = (stbi_uc)b;
      out[3] = 255;
      out += step;
   }
}
#endif

// set up the kernels
static void stbi__setup_jpeg(stbi__jpeg *j)
{
   j->idct_block_kernel = stbi__idct_block;
   j->YCbCr_to_RGB_kernel = stbi__YCbCr_to_RGB_row;
   j->resample_row_hv_2_kernel = stbi__resample_row_hv_2;

#ifdef STBI_SSE2
   if (stbi__sse2_available()) {
      j->idct_block_kernel = stbi__idct_simd;
      j->YCbCr_to_RGB_kernel = stbi__YCbCr_to_RGB_simd;
      j->resample_row_hv_2_kernel = stbi__resample_row_hv_2_simd;
   }
#endif

#ifdef STBI_NEON
   j->idct_block_kernel = stbi__idct_simd;
   j->YCbCr_to_RGB_kernel = stbi__YCbCr_to_RGB_simd;
   j->resample_row_hv_2_kernel = stbi__resample_row_hv_2_simd;
#endif
}

// clean up the temporary component buffers
static void stbi__cleanup_jpeg(stbi__jpeg *j)
{
   stbi__free_jpeg_components(j, j->s->img_n, 0);
}

typedef struct
{
   resample_row_func resample;
   stbi_uc *line0,*line1;
   int hs,vs;   // expansion factor in each axis
   int w_lores; // horizontal pixels pre-expansion
   int ystep;   // how far through vertical expansion we are
   int ypos;    // which pre-expansion row we're on
} stbi__resample;

// fast 0..255 * 0..255 => 0..255 rounded multiplication
static stbi_uc stbi__blinn_8x8(stbi_uc x, stbi_uc y)
{
   unsigned int t = x*y + 128;
   return (stbi_uc) ((t + (t >>8)) >> 8);
}

static stbi_uc *load_jpeg_image(stbi__jpeg *z, int *out_x, int *out_y, int *comp, int req_comp)
{
   int n, decode_n, is_rgb;
   z->s->img_n = 0; // make stbi__cleanup_jpeg safe

   // validate req_comp
   if (req_comp < 0 || req_comp > 4) return stbi__errpuc("bad req_comp", "Internal error");

   // load a jpeg image from whichever source, but leave in YCbCr format
   if (!stbi__decode_jpeg_image(z)) { stbi__cleanup_jpeg(z); return NULL; }

   // determine actual number of components to generate
   n = req_comp ? req_comp : z->s->img_n >= 3 ? 3 : 1;

   is_rgb = z->s->img_n == 3 && (z->rgb == 3 || (z->app14_color_transform == 0 && !z->jfif));

   if (z->s->img_n == 3 && n < 3 && !is_rgb)
      decode_n = 1;
   else
      decode_n = z->s->img_n;

   // nothing to do if no components requested; check this now to avoid
   // accessing uninitialized coutput[0] later
   if (decode_n <= 0) { stbi__cleanup_jpeg(z); return NULL; }

   // resample and color-convert
   {
      int k;
      unsigned int i,j;
      stbi_uc *output;
      stbi_uc *coutput[4] = { NULL, NULL, NULL, NULL };

      stbi__resample res_comp[4];

      for (k=0; k < decode_n; ++k) {
         stbi__resample *r = &res_comp[k];

         // allocate line buffer big enough for upsampling off the edges
         // with upsample factor of 4
         z->img_comp[k].linebuf = (stbi_uc *) stbi__malloc(z->s->img_x + 3);
         if (!z->img_comp[k].linebuf) { stbi__cleanup_jpeg(z); return stbi__errpuc("outofmem", "Out of memory"); }

         r->hs      = z->img_h_max / z->img_comp[k].h;
         r->vs      = z->img_v_max / z->img_comp[k].v;
         r->ystep   = r->vs >> 1;
         r->w_lores = (z->s->img_x + r->hs-1) / r->hs;
         r->ypos    = 0;
         r->line0   = r->line1 = z->img_comp[k].data;

         if      (r->hs == 1 && r->vs == 1) r->resample = resample_row_1;
         else if (r->hs == 1 && r->vs == 2) r->resample = stbi__resample_row_v_2;
         else if (r->hs == 2 && r->vs == 1) r->resample = stbi__resample_row_h_2;
         else if (r->hs == 2 && r->vs == 2) r->resample = z->resample_row_hv_2_kernel;
         else                               r->resample = stbi__resample_row_generic;
      }

      // can't error after this so, this is safe
      output = (stbi_uc *) stbi__malloc_mad3(n, z->s->img_x, z->s->img_y, 1);
      if (!output) { stbi__cleanup_jpeg(z); return stbi__errpuc("outofmem", "Out of memory"); }

      // now go ahead and resample
      for (j=0; j < z->s->img_y; ++j) {
         stbi_uc *out = output + n * z->s->img_x * j;
         for (k=0; k < decode_n; ++k) {
            stbi__resample *r = &res_comp[k];
            int y_bot = r->ystep >= (r->vs >> 1);
            coutput[k] = r->resample(z->img_comp[k].linebuf,
                                     y_bot ? r->line1 : r->line0,
                                     y_bot ? r->line0 : r->line1,
                                     r->w_lores, r->hs);
            if (++r->ystep >= r->vs) {
               r->ystep = 0;
               r->line0 = r->line1;
               if (++r->ypos < z->img_comp[k].y)
                  r->line1 += z->img_comp[k].w2;
            }
         }
         if (n >= 3) {
            stbi_uc *y = coutput[0];
            if (z->s->img_n == 3) {
               if (is_rgb) {
                  for (i=0; i < z->s->img_x; ++i) {
                     out[0] = y[i];
                     out[1] = coutput[1][i];
                     out[2] = coutput[2][i];
                     out[3] = 255;
                     out += n;
                  }
               } else {
                  z->YCbCr_to_RGB_kernel(out, y, coutput[1], coutput[2], z->s->img_x, n);
               }
            } else if (z->s->img_n == 4) {
               if (z->app14_color_transform == 0) { // CMYK
                  for (i=0; i < z->s->img_x; ++i) {
                     stbi_uc m = coutput[3][i];
                     out[0] = stbi__blinn_8x8(coutput[0][i], m);
                     out[1] = stbi__blinn_8x8(coutput[1][i], m);
                     out[2] = stbi__blinn_8x8(coutput[2][i], m);
                     out[3] = 255;
                     out += n;
                  }
               } else if (z->app14_color_transform == 2) { // YCCK
                  z->YCbCr_to_RGB_kernel(out, y, coutput[1], coutput[2], z->s->img_x, n);
                  for (i=0; i < z->s->img_x; ++i) {
                     stbi_uc m = coutput[3][i];
                     out[0] = stbi__blinn_8x8(255 - out[0], m);
                     out[1] = stbi__blinn_8x8(255 - out[1], m);
                     out[2] = stbi__blinn_8x8(255 - out[2], m);
                     out += n;
                  }
               } else { // YCbCr + alpha?  Ignore the fourth channel for now
                  z->YCbCr_to_RGB_kernel(out, y, coutput[1], coutput[2], z->s->img_x, n);
               }
            } else
               for (i=0; i < z->s->img_x; ++i) {
                  out[0] = out[1] = out[2] = y[i];
                  out[3] = 255; // not used if n==3
                  out += n;
               }
         } else {
            if (is_rgb) {
               if (n == 1)
                  for (i=0; i < z->s->img_x; ++i)
                     *out++ = stbi__compute_y(coutput[0][i], coutput[1][i], coutput[2][i]);
               else {
                  for (i=0; i < z->s->img_x; ++i, out += 2) {
                     out[0] = stbi__compute_y(coutput[0][i], coutput[1][i], coutput[2][i]);
                     out[1] = 255;
                  }
               }
            } else if (z->s->img_n == 4 && z->app14_color_transform == 0) {
               for (i=0; i < z->s->img_x; ++i) {
                  stbi_uc m = coutput[3][i];
                  stbi_uc r = stbi__blinn_8x8(coutput[0][i], m);
                  stbi_uc g = stbi__blinn_8x8(coutput[1][i], m);
                  stbi_uc b = stbi__blinn_8x8(coutput[2][i], m);
                  out[0] = stbi__compute_y(r, g, b);
                  out[1] = 255;
                  out += n;
               }
            } else if (z->s->img_n == 4 && z->app14_color_transform == 2) {
               for (i=0; i < z->s->img_x; ++i) {
                  out[0] = stbi__blinn_8x8(255 - coutput[0][i], coutput[3][i]);
                  out[1] = 255;
                  out += n;
               }
            } else {
               stbi_uc *y = coutput[0];
               if (n == 1)
                  for (i=0; i < z->s->img_x; ++i) out[i] = y[i];
               else
                  for (i=0; i < z->s->img_x; ++i) { *out++ = y[i]; *out++ = 255; }
            }
         }
      }
      stbi__cleanup_jpeg(z);
      *out_x = z->s->img_x;
      *out_y = z->s->img_y;
      if (comp) *comp = z->s->img_n >= 3 ? 3 : 1; // report original components, not output
      return output;
   }
}

static void *stbi__jpeg_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri)
{
   unsigned char* result;
   stbi__jpeg* j = (stbi__jpeg*) stbi__malloc(sizeof(stbi__jpeg));
   if (!j) return stbi__errpuc("outofmem", "Out of memory");
   STBI_NOTUSED(ri);
   j->s = s;
   stbi__setup_jpeg(j);
   result = load_jpeg_image(j, x,y,comp,req_comp);
   STBI_FREE(j);
   return result;
}

static int stbi__jpeg_test(stbi__context *s)
{
   int r;
   stbi__jpeg* j = (stbi__jpeg*)stbi__malloc(sizeof(stbi__jpeg));
   if (!j) return stbi__err("outofmem", "Out of memory");
   j->s = s;
   stbi__setup_jpeg(j);
   r = stbi__decode_jpeg_header(j, STBI__SCAN_type);
   stbi__rewind(s);
   STBI_FREE(j);
   return r;
}

static int stbi__jpeg_info_raw(stbi__jpeg *j, int *x, int *y, int *comp)
{
   if (!stbi__decode_jpeg_header(j, STBI__SCAN_header)) {
      stbi__rewind( j->s );
      return 0;
   }
   if (x) *x = j->s->img_x;
   if (y) *y = j->s->img_y;
   if (comp) *comp = j->s->img_n >= 3 ? 3 : 1;
   return 1;
}

static int stbi__jpeg_info(stbi__context *s, int *x, int *y, int *comp)
{
   int result;
   stbi__jpeg* j = (stbi__jpeg*) (stbi__malloc(sizeof(stbi__jpeg)));
   if (!j) return stbi__err("outofmem", "Out of memory");
   j->s = s;
   result = stbi__jpeg_info_raw(j, x, y, comp);
   STBI_FREE(j);
   return result;
}
#endif

// public domain zlib decode    v0.2  Sean Barrett 2006-11-18
//    simple implementation
//      - all input must be provided in an upfront buffer
//      - all output is written to a single output buffer (can malloc/realloc)
//    performance
//      - fast huffman

#ifndef STBI_NO_ZLIB

// fast-way is faster to check than jpeg huffman, but slow way is slower
#define STBI__ZFAST_BITS  9 // accelerate all cases in default tables
#define STBI__ZFAST_MASK  ((1 << STBI__ZFAST_BITS) - 1)
#define STBI__ZNSYMS 288 // number of symbols in literal/length alphabet

// zlib-style huffman encoding
// (jpegs packs from left, zlib from right, so can't share code)
typedef struct
{
   stbi__uint16 fast[1 << STBI__ZFAST_BITS];
   stbi__uint16 firstcode[16];
   int maxcode[17];
   stbi__uint16 firstsymbol[16];
   stbi_uc  size[STBI__ZNSYMS];
   stbi__uint16 value[STBI__ZNSYMS];
} stbi__zhuffman;

stbi_inline static int stbi__bitreverse16(int n)
{
  n = ((n & 0xAAAA) >>  1) | ((n & 0x5555) << 1);
  n = ((n & 0xCCCC) >>  2) | ((n & 0x3333) << 2);
  n = ((n & 0xF0F0) >>  4) | ((n & 0x0F0F) << 4);
  n = ((n & 0xFF00) >>  8) | ((n & 0x00FF) << 8);
  return n;
}

stbi_inline static int stbi__bit_reverse(int v, int bits)
{
   STBI_ASSERT(bits <= 16);
   // to bit reverse n bits, reverse 16 and shift
   // e.g. 11 bits, bit reverse and shift away 5
   return stbi__bitreverse16(v) >> (16-bits);
}

static int stbi__zbuild_huffman(stbi__zhuffman *z, const stbi_uc *sizelist, int num)
{
   int i,k=0;
   int code, next_code[16], sizes[17];

   // DEFLATE spec for generating codes
   memset(sizes, 0, sizeof(sizes));
   memset(z->fast, 0, sizeof(z->fast));
   for (i=0; i < num; ++i)
      ++sizes[sizelist[i]];
   sizes[0] = 0;
   for (i=1; i < 16; ++i)
      if (sizes[i] > (1 << i))
         return stbi__err("bad sizes", "Corrupt PNG");
   code = 0;
   for (i=1; i < 16; ++i) {
      next_code[i] = code;
      z->firstcode[i] = (stbi__uint16) code;
      z->firstsymbol[i] = (stbi__uint16) k;
      code = (code + sizes[i]);
      if (sizes[i])
         if (code-1 >= (1 << i)) return stbi__err("bad codelengths","Corrupt PNG");
      z->maxcode[i] = code << (16-i); // preshift for inner loop
      code <<= 1;
      k += sizes[i];
   }
   z->maxcode[16] = 0x10000; // sentinel
   for (i=0; i < num; ++i) {
      int s = sizelist[i];
      if (s) {
         int c = next_code[s] - z->firstcode[s] + z->firstsymbol[s];
         stbi__uint16 fastv = (stbi__uint16) ((s << 9) | i);
         z->size [c] = (stbi_uc     ) s;
         z->value[c] = (stbi__uint16) i;
         if (s <= STBI__ZFAST_BITS) {
            int j = stbi__bit_reverse(next_code[s],s);
            while (j < (1 << STBI__ZFAST_BITS)) {
               z->fast[j] = fastv;
               j += (1 << s);
            }
         }
         ++next_code[s];
      }
   }
   return 1;
}

// zlib-from-memory implementation for PNG reading
//    because PNG allows splitting the zlib stream arbitrarily,
//    and it's annoying structurally to have PNG call ZLIB call PNG,
//    we require PNG read all the IDATs and combine them into a single
//    memory buffer

typedef struct
{
   stbi_uc *zbuffer, *zbuffer_end;
   int num_bits;
   stbi__uint32 code_buffer;

   char *zout;
   char *zout_start;
   char *zout_end;
   int   z_expandable;

   stbi__zhuffman z_length, z_distance;
} stbi__zbuf;

stbi_inline static int stbi__zeof(stbi__zbuf *z)
{
   return (z->zbuffer >= z->zbuffer_end);
}

stbi_inline static stbi_uc stbi__zget8(stbi__zbuf *z)
{
   return stbi__zeof(z) ? 0 : *z->zbuffer++;
}

static void stbi__fill_bits(stbi__zbuf *z)
{
   do {
      if (z->code_buffer >= (1U << z->num_bits)) {
        z->zbuffer = z->zbuffer_end;  /* treat this as EOF so we fail. */
        return;
      }
      z->code_buffer |= (unsigned int) stbi__zget8(z) << z->num_bits;
      z->num_bits += 8;
   } while (z->num_bits <= 24);
}

stbi_inline static unsigned int stbi__zreceive(stbi__zbuf *z, int n)
{
   unsigned int k;
   if (z->num_bits < n) stbi__fill_bits(z);
   k = z->code_buffer & ((1 << n) - 1);
   z->code_buffer >>= n;
   z->num_bits -= n;
   return k;
}

static int stbi__zhuffman_decode_slowpath(stbi__zbuf *a, stbi__zhuffman *z)
{
   int b,s,k;
   // not resolved by fast table, so compute it the slow way
   // use jpeg approach, which requires MSbits at top
   k = stbi__bit_reverse(a->code_buffer, 16);
   for (s=STBI__ZFAST_BITS+1; ; ++s)
      if (k < z->maxcode[s])
         break;
   if (s >= 16) return -1; // invalid code!
   // code size is s, so:
   b = (k >> (16-s)) - z->firstcode[s] + z->firstsymbol[s];
   if (b >= STBI__ZNSYMS) return -1; // some data was corrupt somewhere!
   if (z->size[b] != s) return -1;  // was originally an assert, but report failure instead.
   a->code_buffer >>= s;
   a->num_bits -= s;
   return z->value[b];
}

stbi_inline static int stbi__zhuffman_decode(stbi__zbuf *a, stbi__zhuffman *z)
{
   int b,s;
   if (a->num_bits < 16) {
      if (stbi__zeof(a)) {
         return -1;   /* report error for unexpected end of data. */
      }
      stbi__fill_bits(a);
   }
   b = z->fast[a->code_buffer & STBI__ZFAST_MASK];
   if (b) {
      s = b >> 9;
      a->code_buffer >>= s;
      a->num_bits -= s;
      return b & 511;
   }
   return stbi__zhuffman_decode_slowpath(a, z);
}

static int stbi__zexpand(stbi__zbuf *z, char *zout, int n)  // need to make room for n bytes
{
   char *q;
   unsigned int cur, limit, old_limit;
   z->zout = zout;
   if (!z->z_expandable) return stbi__err("output buffer limit","Corrupt PNG");
   cur   = (unsigned int) (z->zout - z->zout_start);
   limit = old_limit = (unsigned) (z->zout_end - z->zout_start);
   if (UINT_MAX - cur < (unsigned) n) return stbi__err("outofmem", "Out of memory");
   while (cur + n > limit) {
      if(limit > UINT_MAX / 2) return stbi__err("outofmem", "Out of memory");
      limit *= 2;
   }
   q = (char *) STBI_REALLOC_SIZED(z->zout_start, old_limit, limit);
   STBI_NOTUSED(old_limit);
   if (q == NULL) return stbi__err("outofmem", "Out of memory");
   z->zout_start = q;
   z->zout       = q + cur;
   z->zout_end   = q + limit;
   return 1;
}

static const int stbi__zlength_base[31] = {
   3,4,5,6,7,8,9,10,11,13,
   15,17,19,23,27,31,35,43,51,59,
   67,83,99,115,131,163,195,227,258,0,0 };

static const int stbi__zlength_extra[31]=
{ 0,0,0,0,0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,0,0,0 };

static const int stbi__zdist_base[32] = { 1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,
257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0};

static const int stbi__zdist_extra[32] =
{ 0,0,0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13};

static int stbi__parse_huffman_block(stbi__zbuf *a)
{
   char *zout = a->zout;
   for(;;) {
      int z = stbi__zhuffman_decode(a, &a->z_length);
      if (z < 256) {
         if (z < 0) return stbi__err("bad huffman code","Corrupt PNG"); // error in huffman codes
         if (zout >= a->zout_end) {
            if (!stbi__zexpand(a, zout, 1)) return 0;
            zout = a->zout;
         }
         *zout++ = (char) z;
      } else {
         stbi_uc *p;
         int len,dist;
         if (z == 256) {
            a->zout = zout;
            return 1;
         }
         z -= 257;
         len = stbi__zlength_base[z];
         if (stbi__zlength_extra[z]) len += stbi__zreceive(a, stbi__zlength_extra[z]);
         z = stbi__zhuffman_decode(a, &a->z_distance);
         if (z < 0) return stbi__err("bad huffman code","Corrupt PNG");
         dist = stbi__zdist_base[z];
         if (stbi__zdist_extra[z]) dist += stbi__zreceive(a, stbi__zdist_extra[z]);
         if (zout - a->zout_start < dist) return stbi__err("bad dist","Corrupt PNG");
         if (zout + len > a->zout_end) {
            if (!stbi__zexpand(a, zout, len)) return 0;
            zout = a->zout;
         }
         p = (stbi_uc *) (zout - dist);
         if (dist == 1) { // run of one byte; common in images.
            stbi_uc v = *p;
            if (len) { do *zout++ = v; while (--len); }
         } else {
            if (len) { do *zout++ = *p++; while (--len); }
         }
      }
   }
}

static int stbi__compute_huffman_codes(stbi__zbuf *a)
{
   static const stbi_uc length_dezigzag[19] = { 16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15 };
   stbi__zhuffman z_codelength;
   stbi_uc lencodes[286+32+137];//padding for maximum single op
   stbi_uc codelength_sizes[19];
   int i,n;

   int hlit  = stbi__zreceive(a,5) + 257;
   int hdist = stbi__zreceive(a,5) + 1;
   int hclen = stbi__zreceive(a,4) + 4;
   int ntot  = hlit + hdist;

   memset(codelength_sizes, 0, sizeof(codelength_sizes));
   for (i=0; i < hclen; ++i) {
      int s = stbi__zreceive(a,3);
      codelength_sizes[length_dezigzag[i]] = (stbi_uc) s;
   }
   if (!stbi__zbuild_huffman(&z_codelength, codelength_sizes, 19)) return 0;

   n = 0;
   while (n < ntot) {
      int c = stbi__zhuffman_decode(a, &z_codelength);
      if (c < 0 || c >= 19) return stbi__err("bad codelengths", "Corrupt PNG");
      if (c < 16)
         lencodes[n++] = (stbi_uc) c;
      else {
         stbi_uc fill = 0;
         if (c == 16) {
            c = stbi__zreceive(a,2)+3;
            if (n == 0) return stbi__err("bad codelengths", "Corrupt PNG");
            fill = lencodes[n-1];
         } else if (c == 17) {
            c = stbi__zreceive(a,3)+3;
         } else if (c == 18) {
            c = stbi__zreceive(a,7)+11;
         } else {
            return stbi__err("bad codelengths", "Corrupt PNG");
         }
         if (ntot - n < c) return stbi__err("bad codelengths", "Corrupt PNG");
         memset(lencodes+n, fill, c);
         n += c;
      }
   }
   if (n != ntot) return stbi__err("bad codelengths","Corrupt PNG");
   if (!stbi__zbuild_huffman(&a->z_length, lencodes, hlit)) return 0;
   if (!stbi__zbuild_huffman(&a->z_distance, lencodes+hlit, hdist)) return 0;
   return 1;
}

static int stbi__parse_uncompressed_block(stbi__zbuf *a)
{
   stbi_uc header[4];
   int len,nlen,k;
   if (a->num_bits & 7)
      stbi__zreceive(a, a->num_bits & 7); // discard
   // drain the bit-packed data into header
   k = 0;
   while (a->num_bits > 0) {
      header[k++] = (stbi_uc) (a->code_buffer & 255); // suppress MSVC run-time check
      a->code_buffer >>= 8;
      a->num_bits -= 8;
   }
   if (a->num_bits < 0) return stbi__err("zlib corrupt","Corrupt PNG");
   // now fill header the normal way
   while (k < 4)
      header[k++] = stbi__zget8(a);
   len  = header[1] * 256 + header[0];
   nlen = header[3] * 256 + header[2];
   if (nlen != (len ^ 0xffff)) return stbi__err("zlib corrupt","Corrupt PNG");
   if (a->zbuffer + len > a->zbuffer_end) return stbi__err("read past buffer","Corrupt PNG");
   if (a->zout + len > a->zout_end)
      if (!stbi__zexpand(a, a->zout, len)) return 0;
   memcpy(a->zout, a->zbuffer, len);
   a->zbuffer += len;
   a->zout += len;
   return 1;
}

static int stbi__parse_zlib_header(stbi__zbuf *a)
{
   int cmf   = stbi__zget8(a);
   int cm    = cmf & 15;
   /* int cinfo = cmf >> 4; */
   int flg   = stbi__zget8(a);
   if (stbi__zeof(a)) return stbi__err("bad zlib header","Corrupt PNG"); // zlib spec
   if ((cmf*256+flg) % 31 != 0) return stbi__err("bad zlib header","Corrupt PNG"); // zlib spec
   if (flg & 32) return stbi__err("no preset dict","Corrupt PNG"); // preset dictionary not allowed in png
   if (cm != 8) return stbi__err("bad compression","Corrupt PNG"); // DEFLATE required for png
   // window = 1 << (8 + cinfo)... but who cares, we fully buffer output
   return 1;
}

static const stbi_uc stbi__zdefault_length[STBI__ZNSYMS] =
{
   8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8, 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
   8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8, 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
   8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8, 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
   8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8, 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
   8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8, 9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,
   9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9, 9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,
   9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9, 9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,
   9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9, 9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,
   7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7, 7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8
};
static const stbi_uc stbi__zdefault_distance[32] =
{
   5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
};
/*
Init algorithm:
{
   int i;   // use <= to match clearly with spec
   for (i=0; i <= 143; ++i)     stbi__zdefault_length[i]   = 8;
   for (   ; i <= 255; ++i)     stbi__zdefault_length[i]   = 9;
   for (   ; i <= 279; ++i)     stbi__zdefault_length[i]   = 7;
   for (   ; i <= 287; ++i)     stbi__zdefault_length[i]   = 8;

   for (i=0; i <=  31; ++i)     stbi__zdefault_distance[i] = 5;
}
*/

static int stbi__parse_zlib(stbi__zbuf *a, int parse_header)
{
   int final, type;
   if (parse_header)
      if (!stbi__parse_zlib_header(a)) return 0;
   a->num_bits = 0;
   a->code_buffer = 0;
   do {
      final = stbi__zreceive(a,1);
      type = stbi__zreceive(a,2);
      if (type == 0) {
         if (!stbi__parse_uncompressed_block(a)) return 0;
      } else if (type == 3) {
         return 0;
      } else {
         if (type == 1) {
            // use fixed code lengths
            if (!stbi__zbuild_huffman(&a->z_length  , stbi__zdefault_length  , STBI__ZNSYMS)) return 0;
            if (!stbi__zbuild_huffman(&a->z_distance, stbi__zdefault_distance,  32)) return 0;
         } else {
            if (!stbi__compute_huffman_codes(a)) return 0;
         }
         if (!stbi__parse_huffman_block(a)) return 0;
      }
   } while (!final);
   return 1;
}

static int stbi__do_zlib(stbi__zbuf *a, char *obuf, int olen, int exp, int parse_header)
{
   a->zout_start = obuf;
   a->zout       = obuf;
   a->zout_end   = obuf + olen;
   a->z_expandable = exp;

   return stbi__parse_zlib(a, parse_header);
}

STBIDEF char *stbi_zlib_decode_malloc_guesssize(const char *buffer, int len, int initial_size, int *outlen)
{
   stbi__zbuf a;
   char *p = (char *) stbi__malloc(initial_size);
   if (p == NULL) return NULL;
   a.zbuffer = (stbi_uc *) buffer;
   a.zbuffer_end = (stbi_uc *) buffer + len;
   if (stbi__do_zlib(&a, p, initial_size, 1, 1)) {
      if (outlen) *outlen = (int) (a.zout - a.zout_start);
      return a.zout_start;
   } else {
      STBI_FREE(a.zout_start);
      return NULL;
   }
}

STBIDEF char *stbi_zlib_decode_malloc(char const *buffer, int len, int *outlen)
{
   return stbi_zlib_decode_malloc_guesssize(buffer, len, 16384, outlen);
}

STBIDEF char *stbi_zlib_decode_malloc_guesssize_headerflag(const char *buffer, int len, int initial_size, int *outlen, int parse_header)
{
   stbi__zbuf a;
   char *p = (char *) stbi__malloc(initial_size);
   if (p == NULL) return NULL;
   a.zbuffer = (stbi_uc *) buffer;
   a.zbuffer_end = (stbi_uc *) buffer + len;
   if (stbi__do_zlib(&a, p, initial_size, 1, parse_header)) {
      if (outlen) *outlen = (int) (a.zout - a.zout_start);
      return a.zout_start;
   } else {
      STBI_FREE(a.zout_start);
      return NULL;
   }
}

STBIDEF int stbi_zlib_decode_buffer(char *obuffer, int olen, char const *ibuffer, int ilen)
{
   stbi__zbuf a;
   a.zbuffer = (stbi_uc *) ibuffer;
   a.zbuffer_end = (stbi_uc *) ibuffer + ilen;
   if (stbi__do_zlib(&a, obuffer, olen, 0, 1))
      return (int) (a.zout - a.zout_start);
   else
      return -1;
}

STBIDEF char *stbi_zlib_decode_noheader_malloc(char const *buffer, int len, int *outlen)
{
   stbi__zbuf a;
   char *p = (char *) stbi__malloc(16384);
   if (p == NULL) return NULL;
   a.zbuffer = (stbi_uc *) buffer;
   a.zbuffer_end = (stbi_uc *) buffer+len;
   if (stbi__do_zlib(&a, p, 16384, 1, 0)) {
      if (outlen) *outlen = (int) (a.zout - a.zout_start);
      return a.zout_start;
   } else {
      STBI_FREE(a.zout_start);
      return NULL;
   }
}

STBIDEF int stbi_zlib_decode_noheader_buffer(char *obuffer, int olen, const char *ibuffer, int ilen)
{
   stbi__zbuf a;
   a.zbuffer = (stbi_uc *) ibuffer;
   a.zbuffer_end = (stbi_uc *) ibuffer + ilen;
   if (stbi__do_zlib(&a, obuffer, olen, 0, 0))
      return (int) (a.zout - a.zout_start);
   else
      return -1;
}
#endif

// public domain "baseline" PNG decoder   v0.10  Sean Barrett 2006-11-18
//    simple implementation
//      - only 8-bit samples
//      - no CRC checking
//      - allocates lots of intermediate memory
//        - avoids problem of streaming data between subsystems
//        - avoids explicit window management
//    performance
//      - uses stb_zlib, a PD zlib implementation with fast huffman decoding

#ifndef STBI_NO_PNG
typedef struct
{
   stbi__uint32 length;
   stbi__uint32 type;
} stbi__pngchunk;

static stbi__pngchunk stbi__get_chunk_header(stbi__context *s)
{
   stbi__pngchunk c;
   c.length = stbi__get32be(s);
   c.type   = stbi__get32be(s);
   return c;
}

static int stbi__check_png_header(stbi__context *s)
{
   static const stbi_uc png_sig[8] = { 137,80,78,71,13,10,26,10 };
   int i;
   for (i=0; i < 8; ++i)
      if (stbi__get8(s) != png_sig[i]) return stbi__err("bad png sig","Not a PNG");
   return 1;
}

typedef struct
{
   stbi__context *s;
   stbi_uc *idata, *expanded, *out;
   int depth;
} stbi__png;


enum {
   STBI__F_none=0,
   STBI__F_sub=1,
   STBI__F_up=2,
   STBI__F_avg=3,
   STBI__F_paeth=4,
   // synthetic filters used for first scanline to avoid needing a dummy row of 0s
   STBI__F_avg_first,
   STBI__F_paeth_first
};

static stbi_uc first_row_filter[5] =
{
   STBI__F_none,
   STBI__F_sub,
   STBI__F_none,
   STBI__F_avg_first,
   STBI__F_paeth_first
};

static int stbi__paeth(int a, int b, int c)
{
   int p = a + b - c;
   int pa = abs(p-a);
   int pb = abs(p-b);
   int pc = abs(p-c);
   if (pa <= pb && pa <= pc) return a;
   if (pb <= pc) return b;
   return c;
}

static const stbi_uc stbi__depth_scale_table[9] = { 0, 0xff, 0x55, 0, 0x11, 0,0,0, 0x01 };

// create the png data from post-deflated data
static int stbi__create_png_image_raw(stbi__png *a, stbi_uc *raw, stbi__uint32 raw_len, int out_n, stbi__uint32 x, stbi__uint32 y, int depth, int color)
{
   int bytes = (depth == 16? 2 : 1);
   stbi__context *s = a->s;
   stbi__uint32 i,j,stride = x*out_n*bytes;
   stbi__uint32 img_len, img_width_bytes;
   int k;
   int img_n = s->img_n; // copy it into a local for later

   int output_bytes = out_n*bytes;
   int filter_bytes = img_n*bytes;
   int width = x;

   STBI_ASSERT(out_n == s->img_n || out_n == s->img_n+1);
   a->out = (stbi_uc *) stbi__malloc_mad3(x, y, output_bytes, 0); // extra bytes to write off the end into
   if (!a->out) return stbi__err("outofmem", "Out of memory");

   if (!stbi__mad3sizes_valid(img_n, x, depth, 7)) return stbi__err("too large", "Corrupt PNG");
   img_width_bytes = (((img_n * x * depth) + 7) >> 3);
   img_len = (img_width_bytes + 1) * y;

   // we used to check for exact match between raw_len and img_len on non-interlaced PNGs,
   // but issue #276 reported a PNG in the wild that had extra data at the end (all zeros),
   // so just check for raw_len < img_len always.
   if (raw_len < img_len) return stbi__err("not enough pixels","Corrupt PNG");

   for (j=0; j < y; ++j) {
      stbi_uc *cur = a->out + stride*j;
      stbi_uc *prior;
      int filter = *raw++;

      if (filter > 4)
         return stbi__err("invalid filter","Corrupt PNG");

      if (depth < 8) {
         if (img_width_bytes > x) return stbi__err("invalid width","Corrupt PNG");
         cur += x*out_n - img_width_bytes; // store output to the rightmost img_len bytes, so we can decode in place
         filter_bytes = 1;
         width = img_width_bytes;
      }
      prior = cur - stride; // bugfix: need to compute this after 'cur +=' computation above

      // if first row, use special filter that doesn't sample previous row
      if (j == 0) filter = first_row_filter[filter];

      // handle first byte explicitly
      for (k=0; k < filter_bytes; ++k) {
         switch (filter) {
            case STBI__F_none       : cur[k] = raw[k]; break;
            case STBI__F_sub        : cur[k] = raw[k]; break;
            case STBI__F_up         : cur[k] = STBI__BYTECAST(raw[k] + prior[k]); break;
            case STBI__F_avg        : cur[k] = STBI__BYTECAST(raw[k] + (prior[k]>>1)); break;
            case STBI__F_paeth      : cur[k] = STBI__BYTECAST(raw[k] + stbi__paeth(0,prior[k],0)); break;
            case STBI__F_avg_first  : cur[k] = raw[k]; break;
            case STBI__F_paeth_first: cur[k] = raw[k]; break;
         }
      }

      if (depth == 8) {
         if (img_n != out_n)
            cur[img_n] = 255; // first pixel
         raw += img_n;
         cur += out_n;
         prior += out_n;
      } else if (depth == 16) {
         if (img_n != out_n) {
            cur[filter_bytes]   = 255; // first pixel top byte
            cur[filter_bytes+1] = 255; // first pixel bottom byte
         }
         raw += filter_bytes;
         cur += output_bytes;
         prior += output_bytes;
      } else {
         raw += 1;
         cur += 1;
         prior += 1;
      }

      // this is a little gross, so that we don't switch per-pixel or per-component
      if (depth < 8 || img_n == out_n) {
         int nk = (width - 1)*filter_bytes;
         #define STBI__CASE(f) \
             case f:     \
                for (k=0; k < nk; ++k)
         switch (filter) {
            // "none" filter turns into a memcpy here; make that explicit.
            case STBI__F_none:         memcpy(cur, raw, nk); break;
            STBI__CASE(STBI__F_sub)          { cur[k] = STBI__BYTECAST(raw[k] + cur[k-filter_bytes]); } break;
            STBI__CASE(STBI__F_up)           { cur[k] = STBI__BYTECAST(raw[k] + prior[k]); } break;
            STBI__CASE(STBI__F_avg)          { cur[k] = STBI__BYTECAST(raw[k] + ((prior[k] + cur[k-filter_bytes])>>1)); } break;
            STBI__CASE(STBI__F_paeth)        { cur[k] = STBI__BYTECAST(raw[k] + stbi__paeth(cur[k-filter_bytes],prior[k],prior[k-filter_bytes])); } break;
            STBI__CASE(STBI__F_avg_first)    { cur[k] = STBI__BYTECAST(raw[k] + (cur[k-filter_bytes] >> 1)); } break;
            STBI__CASE(STBI__F_paeth_first)  { cur[k] = STBI__BYTECAST(raw[k] + stbi__paeth(cur[k-filter_bytes],0,0)); } break;
         }
         #undef STBI__CASE
         raw += nk;
      } else {
         STBI_ASSERT(img_n+1 == out_n);
         #define STBI__CASE(f) \
             case f:     \
                for (i=x-1; i >= 1; --i, cur[filter_bytes]=255,raw+=filter_bytes,cur+=output_bytes,prior+=output_bytes) \
                   for (k=0; k < filter_bytes; ++k)
         switch (filter) {
            STBI__CASE(STBI__F_none)         { cur[k] = raw[k]; } break;
            STBI__CASE(STBI__F_sub)          { cur[k] = STBI__BYTECAST(raw[k] + cur[k- output_bytes]); } break;
            STBI__CASE(STBI__F_up)           { cur[k] = STBI__BYTECAST(raw[k] + prior[k]); } break;
            STBI__CASE(STBI__F_avg)          { cur[k] = STBI__BYTECAST(raw[k] + ((prior[k] + cur[k- output_bytes])>>1)); } break;
            STBI__CASE(STBI__F_paeth)        { cur[k] = STBI__BYTECAST(raw[k] + stbi__paeth(cur[k- output_bytes],prior[k],prior[k- output_bytes])); } break;
            STBI__CASE(STBI__F_avg_first)    { cur[k] = STBI__BYTECAST(raw[k] + (cur[k- output_bytes] >> 1)); } break;
            STBI__CASE(STBI__F_paeth_first)  { cur[k] = STBI__BYTECAST(raw[k] + stbi__paeth(cur[k- output_bytes],0,0)); } break;
         }
         #undef STBI__CASE

         // the loop above sets the high byte of the pixels' alpha, but for
         // 16 bit png files we also need the low byte set. we'll do that here.
         if (depth == 16) {
            cur = a->out + stride*j; // start at the beginning of the row again
            for (i=0; i < x; ++i,cur+=output_bytes) {
               cur[filter_bytes+1] = 255;
            }
         }
      }
   }

   // we make a separate pass to expand bits to pixels; for performance,
   // this could run two scanlines behind the above code, so it won't
   // intefere with filtering but will still be in the cache.
   if (depth < 8) {
      for (j=0; j < y; ++j) {
         stbi_uc *cur = a->out + stride*j;
         stbi_uc *in  = a->out + stride*j + x*out_n - img_width_bytes;
         // unpack 1/2/4-bit into a 8-bit buffer. allows us to keep the common 8-bit path optimal at minimal cost for 1/2/4-bit
         // png guarante byte alignment, if width is not multiple of 8/4/2 we'll decode dummy trailing data that will be skipped in the later loop
         stbi_uc scale = (color == 0) ? stbi__depth_scale_table[depth] : 1; // scale grayscale values to 0..255 range

         // note that the final byte might overshoot and write more data than desired.
         // we can allocate enough data that this never writes out of memory, but it
         // could also overwrite the next scanline. can it overwrite non-empty data
         // on the next scanline? yes, consider 1-pixel-wide scanlines with 1-bit-per-pixel.
         // so we need to explicitly clamp the final ones

         if (depth == 4) {
            for (k=x*img_n; k >= 2; k-=2, ++in) {
               *cur++ = scale * ((*in >> 4)       );
               *cur++ = scale * ((*in     ) & 0x0f);
            }
            if (k > 0) *cur++ = scale * ((*in >> 4)       );
         } else if (depth == 2) {
            for (k=x*img_n; k >= 4; k-=4, ++in) {
               *cur++ = scale * ((*in >> 6)       );
               *cur++ = scale * ((*in >> 4) & 0x03);
               *cur++ = scale * ((*in >> 2) & 0x03);
               *cur++ = scale * ((*in     ) & 0x03);
            }
            if (k > 0) *cur++ = scale * ((*in >> 6)       );
            if (k > 1) *cur++ = scale * ((*in >> 4) & 0x03);
            if (k > 2) *cur++ = scale * ((*in >> 2) & 0x03);
         } else if (depth == 1) {
            for (k=x*img_n; k >= 8; k-=8, ++in) {
               *cur++ = scale * ((*in >> 7)       );
               *cur++ = scale * ((*in >> 6) & 0x01);
               *cur++ = scale * ((*in >> 5) & 0x01);
               *cur++ = scale * ((*in >> 4) & 0x01);
               *cur++ = scale * ((*in >> 3) & 0x01);
               *cur++ = scale * ((*in >> 2) & 0x01);
               *cur++ = scale * ((*in >> 1) & 0x01);
               *cur++ = scale * ((*in     ) & 0x01);
            }
            if (k > 0) *cur++ = scale * ((*in >> 7)       );
            if (k > 1) *cur++ = scale * ((*in >> 6) & 0x01);
            if (k > 2) *cur++ = scale * ((*in >> 5) & 0x01);
            if (k > 3) *cur++ = scale * ((*in >> 4) & 0x01);
            if (k > 4) *cur++ = scale * ((*in >> 3) & 0x01);
            if (k > 5) *cur++ = scale * ((*in >> 2) & 0x01);
            if (k > 6) *cur++ = scale * ((*in >> 1) & 0x01);
         }
         if (img_n != out_n) {
            int q;
            // insert alpha = 255
            cur = a->out + stride*j;
            if (img_n == 1) {
               for (q=x-1; q >= 0; --q) {
                  cur[q*2+1] = 255;
                  cur[q*2+0] = cur[q];
               }
            } else {
               STBI_ASSERT(img_n == 3);
               for (q=x-1; q >= 0; --q) {
                  cur[q*4+3] = 255;
                  cur[q*4+2] = cur[q*3+2];
                  cur[q*4+1] = cur[q*3+1];
                  cur[q*4+0] = cur[q*3+0];
               }
            }
         }
      }
   } else if (depth == 16) {
      // force the image data from big-endian to platform-native.
      // this is done in a separate pass due to the decoding relying
      // on the data being untouched, but could probably be done
      // per-line during decode if care is taken.
      stbi_uc *cur = a->out;
      stbi__uint16 *cur16 = (stbi__uint16*)cur;

      for(i=0; i < x*y*out_n; ++i,cur16++,cur+=2) {
         *cur16 = (cur[0] << 8) | cur[1];
      }
   }

   return 1;
}

static int stbi__create_png_image(stbi__png *a, stbi_uc *image_data, stbi__uint32 image_data_len, int out_n, int depth, int color, int interlaced)
{
   int bytes = (depth == 16 ? 2 : 1);
   int out_bytes = out_n * bytes;
   stbi_uc *final;
   int p;
   if (!interlaced)
      return stbi__create_png_image_raw(a, image_data, image_data_len, out_n, a->s->img_x, a->s->img_y, depth, color);

   // de-interlacing
   final = (stbi_uc *) stbi__malloc_mad3(a->s->img_x, a->s->img_y, out_bytes, 0);
   if (!final) return stbi__err("outofmem", "Out of memory");
   for (p=0; p < 7; ++p) {
      int xorig[] = { 0,4,0,2,0,1,0 };
      int yorig[] = { 0,0,4,0,2,0,1 };
      int xspc[]  = { 8,8,4,4,2,2,1 };
      int yspc[]  = { 8,8,8,4,4,2,2 };
      int i,j,x,y;
      // pass1_x[4] = 0, pass1_x[5] = 1, pass1_x[12] = 1
      x = (a->s->img_x - xorig[p] + xspc[p]-1) / xspc[p];
      y = (a->s->img_y - yorig[p] + yspc[p]-1) / yspc[p];
      if (x && y) {
         stbi__uint32 img_len = ((((a->s->img_n * x * depth) + 7) >> 3) + 1) * y;
         if (!stbi__create_png_image_raw(a, image_data, image_data_len, out_n, x, y, depth, color)) {
            STBI_FREE(final);
            return 0;
         }
         for (j=0; j < y; ++j) {
            for (i=0; i < x; ++i) {
               int out_y = j*yspc[p]+yorig[p];
               int out_x = i*xspc[p]+xorig[p];
               memcpy(final + out_y*a->s->img_x*out_bytes + out_x*out_bytes,
                      a->out + (j*x+i)*out_bytes, out_bytes);
            }
         }
         STBI_FREE(a->out);
         image_data += img_len;
         image_data_len -= img_len;
      }
   }
   a->out = final;

   return 1;
}

static int stbi__compute_transparency(stbi__png *z, stbi_uc tc[3], int out_n)
{
   stbi__context *s = z->s;
   stbi__uint32 i, pixel_count = s->img_x * s->img_y;
   stbi_uc *p = z->out;

   // compute color-based transparency, assuming we've
   // already got 255 as the alpha value in the output
   STBI_ASSERT(out_n == 2 || out_n == 4);

   if (out_n == 2) {
      for (i=0; i < pixel_count; ++i) {
         p[1] = (p[0] == tc[0] ? 0 : 255);
         p += 2;
      }
   } else {
      for (i=0; i < pixel_count; ++i) {
         if (p[0] == tc[0] && p[1] == tc[1] && p[2] == tc[2])
            p[3] = 0;
         p += 4;
      }
   }
   return 1;
}

static int stbi__compute_transparency16(stbi__png *z, stbi__uint16 tc[3], int out_n)
{
   stbi__context *s = z->s;
   stbi__uint32 i, pixel_count = s->img_x * s->img_y;
   stbi__uint16 *p = (stbi__uint16*) z->out;

   // compute color-based transparency, assuming we've
   // already got 65535 as the alpha value in the output
   STBI_ASSERT(out_n == 2 || out_n == 4);

   if (out_n == 2) {
      for (i = 0; i < pixel_count; ++i) {
         p[1] = (p[0] == tc[0] ? 0 : 65535);
         p += 2;
      }
   } else {
      for (i = 0; i < pixel_count; ++i) {
         if (p[0] == tc[0] && p[1] == tc[1] && p[2] == tc[2])
            p[3] = 0;
         p += 4;
      }
   }
   return 1;
}

static int stbi__expand_png_palette(stbi__png *a, stbi_uc *palette, int len, int pal_img_n)
{
   stbi__uint32 i, pixel_count = a->s->img_x * a->s->img_y;
   stbi_uc *p, *temp_out, *orig = a->out;

   p = (stbi_uc *) stbi__malloc_mad2(pixel_count, pal_img_n, 0);
   if (p == NULL) return stbi__err("outofmem", "Out of memory");

   // between here and free(out) below, exitting would leak
   temp_out = p;

   if (pal_img_n == 3) {
      for (i=0; i < pixel_count; ++i) {
         int n = orig[i]*4;
         p[0] = palette[n  ];
         p[1] = palette[n+1];
         p[2] = palette[n+2];
         p += 3;
      }
   } else {
      for (i=0; i < pixel_count; ++i) {
         int n = orig[i]*4;
         p[0] = palette[n  ];
         p[1] = palette[n+1];
         p[2] = palette[n+2];
         p[3] = palette[n+3];
         p += 4;
      }
   }
   STBI_FREE(a->out);
   a->out = temp_out;

   STBI_NOTUSED(len);

   return 1;
}

static int stbi__unpremultiply_on_load_global = 0;
static int stbi__de_iphone_flag_global = 0;

STBIDEF void stbi_set_unpremultiply_on_load(int flag_true_if_should_unpremultiply)
{
   stbi__unpremultiply_on_load_global = flag_true_if_should_unpremultiply;
}

STBIDEF void stbi_convert_iphone_png_to_rgb(int flag_true_if_should_convert)
{
   stbi__de_iphone_flag_global = flag_true_if_should_convert;
}

#ifndef STBI_THREAD_LOCAL
#define stbi__unpremultiply_on_load  stbi__unpremultiply_on_load_global
#define stbi__de_iphone_flag  stbi__de_iphone_flag_global
#else
static STBI_THREAD_LOCAL int stbi__unpremultiply_on_load_local, stbi__unpremultiply_on_load_set;
static STBI_THREAD_LOCAL int stbi__de_iphone_flag_local, stbi__de_iphone_flag_set;

STBIDEF void stbi__unpremultiply_on_load_thread(int flag_true_if_should_unpremultiply)
{
   stbi__unpremultiply_on_load_local = flag_true_if_should_unpremultiply;
   stbi__unpremultiply_on_load_set = 1;
}

STBIDEF void stbi_convert_iphone_png_to_rgb_thread(int flag_true_if_should_convert)
{
   stbi__de_iphone_flag_local = flag_true_if_should_convert;
   stbi__de_iphone_flag_set = 1;
}

#define stbi__unpremultiply_on_load  (stbi__unpremultiply_on_load_set           \
                                       ? stbi__unpremultiply_on_load_local      \
                                       : stbi__unpremultiply_on_load_global)
#define stbi__de_iphone_flag  (stbi__de_iphone_flag_set                         \
                                ? stbi__de_iphone_flag_local                    \
                                : stbi__de_iphone_flag_global)
#endif // STBI_THREAD_LOCAL

static void stbi__de_iphone(stbi__png *z)
{
   stbi__context *s = z->s;
   stbi__uint32 i, pixel_count = s->img_x * s->img_y;
   stbi_uc *p = z->out;

   if (s->img_out_n == 3) {  // convert bgr to rgb
      for (i=0; i < pixel_count; ++i) {
         stbi_uc t = p[0];
         p[0] = p[2];
         p[2] = t;
         p += 3;
      }
   } else {
      STBI_ASSERT(s->img_out_n == 4);
      if (stbi__unpremultiply_on_load) {
         // convert bgr to rgb and unpremultiply
         for (i=0; i < pixel_count; ++i) {
            stbi_uc a = p[3];
            stbi_uc t = p[0];
            if (a) {
               stbi_uc half = a / 2;
               p[0] = (p[2] * 255 + half) / a;
               p[1] = (p[1] * 255 + half) / a;
               p[2] = ( t   * 255 + half) / a;
            } else {
               p[0] = p[2];
               p[2] = t;
            }
            p += 4;
         }
      } else {
         // convert bgr to rgb
         for (i=0; i < pixel_count; ++i) {
            stbi_uc t = p[0];
            p[0] = p[2];
            p[2] = t;
            p += 4;
         }
      }
   }
}

#define STBI__PNG_TYPE(a,b,c,d)  (((unsigned) (a) << 24) + ((unsigned) (b) << 16) + ((unsigned) (c) << 8) + (unsigned) (d))

static int stbi__parse_png_file(stbi__png *z, int scan, int req_comp)
{
   stbi_uc palette[1024], pal_img_n=0;
   stbi_uc has_trans=0, tc[3]={0};
   stbi__uint16 tc16[3];
   stbi__uint32 ioff=0, idata_limit=0, i, pal_len=0;
   int first=1,k,interlace=0, color=0, is_iphone=0;
   stbi__context *s = z->s;

   z->expanded = NULL;
   z->idata = NULL;
   z->out = NULL;

   if (!stbi__check_png_header(s)) return 0;

   if (scan == STBI__SCAN_type) return 1;

   for (;;) {
      stbi__pngchunk c = stbi__get_chunk_header(s);
      switch (c.type) {
         case STBI__PNG_TYPE('C','g','B','I'):
            is_iphone = 1;
            stbi__skip(s, c.length);
            break;
         case STBI__PNG_TYPE('I','H','D','R'): {
            int comp,filter;
            if (!first) return stbi__err("multiple IHDR","Corrupt PNG");
            first = 0;
            if (c.length != 13) return stbi__err("bad IHDR len","Corrupt PNG");
            s->img_x = stbi__get32be(s);
            s->img_y = stbi__get32be(s);
            if (s->img_y > STBI_MAX_DIMENSIONS) return stbi__err("too large","Very large image (corrupt?)");
            if (s->img_x > STBI_MAX_DIMENSIONS) return stbi__err("too large","Very large image (corrupt?)");
            z->depth = stbi__get8(s);  if (z->depth != 1 && z->depth != 2 && z->depth != 4 && z->depth != 8 && z->depth != 16)  return stbi__err("1/2/4/8/16-bit only","PNG not supported: 1/2/4/8/16-bit only");
            color = stbi__get8(s);  if (color > 6)         return stbi__err("bad ctype","Corrupt PNG");
            if (color == 3 && z->depth == 16)                  return stbi__err("bad ctype","Corrupt PNG");
            if (color == 3) pal_img_n = 3; else if (color & 1) return stbi__err("bad ctype","Corrupt PNG");
            comp  = stbi__get8(s);  if (comp) return stbi__err("bad comp method","Corrupt PNG");
            filter= stbi__get8(s);  if (filter) return stbi__err("bad filter method","Corrupt PNG");
            interlace = stbi__get8(s); if (interlace>1) return stbi__err("bad interlace method","Corrupt PNG");
            if (!s->img_x || !s->img_y) return stbi__err("0-pixel image","Corrupt PNG");
            if (!pal_img_n) {
               s->img_n = (color & 2 ? 3 : 1) + (color & 4 ? 1 : 0);
               if ((1 << 30) / s->img_x / s->img_n < s->img_y) return stbi__err("too large", "Image too large to decode");
               if (scan == STBI__SCAN_header) return 1;
            } else {
               // if paletted, then pal_n is our final components, and
               // img_n is # components to decompress/filter.
               s->img_n = 1;
               if ((1 << 30) / s->img_x / 4 < s->img_y) return stbi__err("too large","Corrupt PNG");
               // if SCAN_header, have to scan to see if we have a tRNS
            }
            break;
         }

         case STBI__PNG_TYPE('P','L','T','E'):  {
            if (first) return stbi__err("first not IHDR", "Corrupt PNG");
            if (c.length > 256*3) return stbi__err("invalid PLTE","Corrupt PNG");
            pal_len = c.length / 3;
            if (pal_len * 3 != c.length) return stbi__err("invalid PLTE","Corrupt PNG");
            for (i=0; i < pal_len; ++i) {
               palette[i*4+0] = stbi__get8(s);
               palette[i*4+1] = stbi__get8(s);
               palette[i*4+2] = stbi__get8(s);
               palette[i*4+3] = 255;
            }
            break;
         }

         case STBI__PNG_TYPE('t','R','N','S'): {
            if (first) return stbi__err("first not IHDR", "Corrupt PNG");
            if (z->idata) return stbi__err("tRNS after IDAT","Corrupt PNG");
            if (pal_img_n) {
               if (scan == STBI__SCAN_header) { s->img_n = 4; return 1; }
               if (pal_len == 0) return stbi__err("tRNS before PLTE","Corrupt PNG");
               if (c.length > pal_len) return stbi__err("bad tRNS len","Corrupt PNG");
               pal_img_n = 4;
               for (i=0; i < c.length; ++i)
                  palette[i*4+3] = stbi__get8(s);
            } else {
               if (!(s->img_n & 1)) return stbi__err("tRNS with alpha","Corrupt PNG");
               if (c.length != (stbi__uint32) s->img_n*2) return stbi__err("bad tRNS len","Corrupt PNG");
               has_trans = 1;
               if (z->depth == 16) {
                  for (k = 0; k < s->img_n; ++k) tc16[k] = (stbi__uint16)stbi__get16be(s); // copy the values as-is
               } else {
                  for (k = 0; k < s->img_n; ++k) tc[k] = (stbi_uc)(stbi__get16be(s) & 255) * stbi__depth_scale_table[z->depth]; // non 8-bit images will be larger
               }
            }
            break;
         }

         case STBI__PNG_TYPE('I','D','A','T'): {
            if (first) return stbi__err("first not IHDR", "Corrupt PNG");
            if (pal_img_n && !pal_len) return stbi__err("no PLTE","Corrupt PNG");
            if (scan == STBI__SCAN_header) { s->img_n = pal_img_n; return 1; }
            if ((int)(ioff + c.length) < (int)ioff) return 0;
            if (ioff + c.length > idata_limit) {
               stbi__uint32 idata_limit_old = idata_limit;
               stbi_uc *p;
               if (idata_limit == 0) idata_limit = c.length > 4096 ? c.length : 4096;
               while (ioff + c.length > idata_limit)
                  idata_limit *= 2;
               STBI_NOTUSED(idata_limit_old);
               p = (stbi_uc *) STBI_REALLOC_SIZED(z->idata, idata_limit_old, idata_limit); if (p == NULL) return stbi__err("outofmem", "Out of memory");
               z->idata = p;
            }
            if (!stbi__getn(s, z->idata+ioff,c.length)) return stbi__err("outofdata","Corrupt PNG");
            ioff += c.length;
            break;
         }

         case STBI__PNG_TYPE('I','E','N','D'): {
            stbi__uint32 raw_len, bpl;
            if (first) return stbi__err("first not IHDR", "Corrupt PNG");
            if (scan != STBI__SCAN_load) return 1;
            if (z->idata == NULL) return stbi__err("no IDAT","Corrupt PNG");
            // initial guess for decoded data size to avoid unnecessary reallocs
            bpl = (s->img_x * z->depth + 7) / 8; // bytes per line, per component
            raw_len = bpl * s->img_y * s->img_n /* pixels */ + s->img_y /* filter mode per row */;
            z->expanded = (stbi_uc *) stbi_zlib_decode_malloc_guesssize_headerflag((char *) z->idata, ioff, raw_len, (int *) &raw_len, !is_iphone);
            if (z->expanded == NULL) return 0; // zlib should set error
            STBI_FREE(z->idata); z->idata = NULL;
            if ((req_comp == s->img_n+1 && req_comp != 3 && !pal_img_n) || has_trans)
               s->img_out_n = s->img_n+1;
            else
               s->img_out_n = s->img_n;
            if (!stbi__create_png_image(z, z->expanded, raw_len, s->img_out_n, z->depth, color, interlace)) return 0;
            if (has_trans) {
               if (z->depth == 16) {
                  if (!stbi__compute_transparency16(z, tc16, s->img_out_n)) return 0;
               } else {
                  if (!stbi__compute_transparency(z, tc, s->img_out_n)) return 0;
               }
            }
            if (is_iphone && stbi__de_iphone_flag && s->img_out_n > 2)
               stbi__de_iphone(z);
            if (pal_img_n) {
               // pal_img_n == 3 or 4
               s->img_n = pal_img_n; // record the actual colors we had
               s->img_out_n = pal_img_n;
               if (req_comp >= 3) s->img_out_n = req_comp;
               if (!stbi__expand_png_palette(z, palette, pal_len, s->img_out_n))
                  return 0;
            } else if (has_trans) {
               // non-paletted image with tRNS -> source image has (constant) alpha
               ++s->img_n;
            }
            STBI_FREE(z->expanded); z->expanded = NULL;
            // end of PNG chunk, read and skip CRC
            stbi__get32be(s);
            return 1;
         }

         default:
            // if critical, fail
            if (first) return stbi__err("first not IHDR", "Corrupt PNG");
            if ((c.type & (1 << 29)) == 0) {
               #ifndef STBI_NO_FAILURE_STRINGS
               // not threadsafe
               static char invalid_chunk[] = "XXXX PNG chunk not known";
               invalid_chunk[0] = STBI__BYTECAST(c.type >> 24);
               invalid_chunk[1] = STBI__BYTECAST(c.type >> 16);
               invalid_chunk[2] = STBI__BYTECAST(c.type >>  8);
               invalid_chunk[3] = STBI__BYTECAST(c.type >>  0);
               #endif
               return stbi__err(invalid_chunk, "PNG not supported: unknown PNG chunk type");
            }
            stbi__skip(s, c.length);
            break;
      }
      // end of PNG chunk, read and skip CRC
      stbi__get32be(s);
   }
}

static void *stbi__do_png(stbi__png *p, int *x, int *y, int *n, int req_comp, stbi__result_info *ri)
{
   void *result=NULL;
   if (req_comp < 0 || req_comp > 4) return stbi__errpuc("bad req_comp", "Internal error");
   if (stbi__parse_png_file(p, STBI__SCAN_load, req_comp)) {
      if (p->depth <= 8)
         ri->bits_per_channel = 8;
      else if (p->depth == 16)
         ri->bits_per_channel = 16;
      else
         return stbi__errpuc("bad bits_per_channel", "PNG not supported: unsupported color depth");
      result = p->out;
      p->out = NULL;
      if (req_comp && req_comp != p->s->img_out_n) {
         if (ri->bits_per_channel == 8)
            result = stbi__convert_format((unsigned char *) result, p->s->img_out_n, req_comp, p->s->img_x, p->s->img_y);
         else
            result = stbi__convert_format16((stbi__uint16 *) result, p->s->img_out_n, req_comp, p->s->img_x, p->s->img_y);
         p->s->img_out_n = req_comp;
         if (result == NULL) return result;
      }
      *x = p->s->img_x;
      *y = p->s->img_y;
      if (n) *n = p->s->img_n;
   }
   STBI_FREE(p->out);      p->out      = NULL;
   STBI_FREE(p->expanded); p->expanded = NULL;
   STBI_FREE(p->idata);    p->idata    = NULL;

   return result;
}

static void *stbi__png_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri)
{
   stbi__png p;
   p.s = s;
   return stbi__do_png(&p, x,y,comp,req_comp, ri);
}

static int stbi__png_test(stbi__context *s)
{
   int r;
   r = stbi__check_png_header(s);
   stbi__rewind(s);
   return r;
}

static int stbi__png_info_raw(stbi__png *p, int *x, int *y, int *comp)
{
   if (!stbi__parse_png_file(p, STBI__SCAN_header, 0)) {
      stbi__rewind( p->s );
      return 0;
   }
   if (x) *x = p->s->img_x;
   if (y) *y = p->s->img_y;
   if (comp) *comp = p->s->img_n;
   return 1;
}

static int stbi__png_info(stbi__context *s, int *x, int *y, int *comp)
{
   stbi__png p;
   p.s = s;
   return stbi__png_info_raw(&p, x, y, comp);
}

static int stbi__png_is16(stbi__context *s)
{
   stbi__png p;
   p.s = s;
   if (!stbi__png_info_raw(&p, NULL, NULL, NULL))
	   return 0;
   if (p.depth != 16) {
      stbi__rewind(p.s);
      return 0;
   }
   return 1;
}
#endif

// Microsoft/Windows BMP image

#ifndef STBI_NO_BMP
static int stbi__bmp_test_raw(stbi__context *s)
{
   int r;
   int sz;
   if (stbi__get8(s) != 'B') return 0;
   if (stbi__get8(s) != 'M') return 0;
   stbi__get32le(s); // discard filesize
   stbi__get16le(s); // discard reserved
   stbi__get16le(s); // discard reserved
   stbi__get32le(s); // discard data offset
   sz = stbi__get32le(s);
   r = (sz == 12 || sz == 40 || sz == 56 || sz == 108 || sz == 124);
   return r;
}

static int stbi__bmp_test(stbi__context *s)
{
   int r = stbi__bmp_test_raw(s);
   stbi__rewind(s);
   return r;
}


// returns 0..31 for the highest set bit
static int stbi__high_bit(unsigned int z)
{
   int n=0;
   if (z == 0) return -1;
   if (z >= 0x10000) { n += 16; z >>= 16; }
   if (z >= 0x00100) { n +=  8; z >>=  8; }
   if (z >= 0x00010) { n +=  4; z >>=  4; }
   if (z >= 0x00004) { n +=  2; z >>=  2; }
   if (z >= 0x00002) { n +=  1;/* >>=  1;*/ }
   return n;
}

static int stbi__bitcount(unsigned int a)
{
   a = (a & 0x55555555) + ((a >>  1) & 0x55555555); // max 2
   a = (a & 0x33333333) + ((a >>  2) & 0x33333333); // max 4
   a = (a + (a >> 4)) & 0x0f0f0f0f; // max 8 per 4, now 8 bits
   a = (a + (a >> 8)); // max 16 per 8 bits
   a = (a + (a >> 16)); // max 32 per 8 bits
   return a & 0xff;
}

// extract an arbitrarily-aligned N-bit value (N=bits)
// from v, and then make it 8-bits long and fractionally
// extend it to full full range.
static int stbi__shiftsigned(unsigned int v, int shift, int bits)
{
   static unsigned int mul_table[9] = {
      0,
      0xff/*0b11111111*/, 0x55/*0b01010101*/, 0x49/*0b01001001*/, 0x11/*0b00010001*/,
      0x21/*0b00100001*/, 0x41/*0b01000001*/, 0x81/*0b10000001*/, 0x01/*0b00000001*/,
   };
   static unsigned int shift_table[9] = {
      0, 0,0,1,0,2,4,6,0,
   };
   if (shift < 0)
      v <<= -shift;
   else
      v >>= shift;
   STBI_ASSERT(v < 256);
   v >>= (8-bits);
   STBI_ASSERT(bits >= 0 && bits <= 8);
   return (int) ((unsigned) v * mul_table[bits]) >> shift_table[bits];
}

typedef struct
{
   int bpp, offset, hsz;
   unsigned int mr,mg,mb,ma, all_a;
   int extra_read;
} stbi__bmp_data;

static int stbi__bmp_set_mask_defaults(stbi__bmp_data *info, int compress)
{
   // BI_BITFIELDS specifies masks explicitly, don't override
   if (compress == 3)
      return 1;

   if (compress == 0) {
      if (info->bpp == 16) {
         info->mr = 31u << 10;
         info->mg = 31u <<  5;
         info->mb = 31u <<  0;
      } else if (info->bpp == 32) {
         info->mr = 0xffu << 16;
         info->mg = 0xffu <<  8;
         info->mb = 0xffu <<  0;
         info->ma = 0xffu << 24;
         info->all_a = 0; // if all_a is 0 at end, then we loaded alpha channel but it was all 0
      } else {
         // otherwise, use defaults, which is all-0
         info->mr = info->mg = info->mb = info->ma = 0;
      }
      return 1;
   }
   return 0; // error
}

static void *stbi__bmp_parse_header(stbi__context *s, stbi__bmp_data *info)
{
   int hsz;
   if (stbi__get8(s) != 'B' || stbi__get8(s) != 'M') return stbi__errpuc("not BMP", "Corrupt BMP");
   stbi__get32le(s); // discard filesize
   stbi__get16le(s); // discard reserved
   stbi__get16le(s); // discard reserved
   info->offset = stbi__get32le(s);
   info->hsz = hsz = stbi__get32le(s);
   info->mr = info->mg = info->mb = info->ma = 0;
   info->extra_read = 14;

   if (info->offset < 0) return stbi__errpuc("bad BMP", "bad BMP");

   if (hsz != 12 && hsz != 40 && hsz != 56 && hsz != 108 && hsz != 124) return stbi__errpuc("unknown BMP", "BMP type not supported: unknown");
   if (hsz == 12) {
      s->img_x = stbi__get16le(s);
      s->img_y = stbi__get16le(s);
   } else {
      s->img_x = stbi__get32le(s);
      s->img_y = stbi__get32le(s);
   }
   if (stbi__get16le(s) != 1) return stbi__errpuc("bad BMP", "bad BMP");
   info->bpp = stbi__get16le(s);
   if (hsz != 12) {
      int compress = stbi__get32le(s);
      if (compress == 1 || compress == 2) return stbi__errpuc("BMP RLE", "BMP type not supported: RLE");
      if (compress >= 4) return stbi__errpuc("BMP JPEG/PNG", "BMP type not supported: unsupported compression"); // this includes PNG/JPEG modes
      if (compress == 3 && info->bpp != 16 && info->bpp != 32) return stbi__errpuc("bad BMP", "bad BMP"); // bitfields requires 16 or 32 bits/pixel
      stbi__get32le(s); // discard sizeof
      stbi__get32le(s); // discard hres
      stbi__get32le(s); // discard vres
      stbi__get32le(s); // discard colorsused
      stbi__get32le(s); // discard max important
      if (hsz == 40 || hsz == 56) {
         if (hsz == 56) {
            stbi__get32le(s);
            stbi__get32le(s);
            stbi__get32le(s);
            stbi__get32le(s);
         }
         if (info->bpp == 16 || info->bpp == 32) {
            if (compress == 0) {
               stbi__bmp_set_mask_defaults(info, compress);
            } else if (compress == 3) {
               info->mr = stbi__get32le(s);
               info->mg = stbi__get32le(s);
               info->mb = stbi__get32le(s);
               info->extra_read += 12;
               // not documented, but generated by photoshop and handled by mspaint
               if (info->mr == info->mg && info->mg == info->mb) {
                  // ?!?!?
                  return stbi__errpuc("bad BMP", "bad BMP");
               }
            } else
               return stbi__errpuc("bad BMP", "bad BMP");
         }
      } else {
         // V4/V5 header
         int i;
         if (hsz != 108 && hsz != 124)
            return stbi__errpuc("bad BMP", "bad BMP");
         info->mr = stbi__get32le(s);
         info->mg = stbi__get32le(s);
         info->mb = stbi__get32le(s);
         info->ma = stbi__get32le(s);
         if (compress != 3) // override mr/mg/mb unless in BI_BITFIELDS mode, as per docs
            stbi__bmp_set_mask_defaults(info, compress);
         stbi__get32le(s); // discard color space
         for (i=0; i < 12; ++i)
            stbi__get32le(s); // discard color space parameters
         if (hsz == 124) {
            stbi__get32le(s); // discard rendering intent
            stbi__get32le(s); // discard offset of profile data
            stbi__get32le(s); // discard size of profile data
            stbi__get32le(s); // discard reserved
         }
      }
   }
   return (void *) 1;
}


static void *stbi__bmp_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri)
{
   stbi_uc *out;
   unsigned int mr=0,mg=0,mb=0,ma=0, all_a;
   stbi_uc pal[256][4];
   int psize=0,i,j,width;
   int flip_vertically, pad, target;
   stbi__bmp_data info;
   STBI_NOTUSED(ri);

   info.all_a = 255;
   if (stbi__bmp_parse_header(s, &info) == NULL)
      return NULL; // error code already set

   flip_vertically = ((int) s->img_y) > 0;
   s->img_y = abs((int) s->img_y);

   if (s->img_y > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");
   if (s->img_x > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");

   mr = info.mr;
   mg = info.mg;
   mb = info.mb;
   ma = info.ma;
   all_a = info.all_a;

   if (info.hsz == 12) {
      if (info.bpp < 24)
         psize = (info.offset - info.extra_read - 24) / 3;
   } else {
      if (info.bpp < 16)
         psize = (info.offset - info.extra_read - info.hsz) >> 2;
   }
   if (psize == 0) {
      if (info.offset != s->callback_already_read + (s->img_buffer - s->img_buffer_original)) {
        return stbi__errpuc("bad offset", "Corrupt BMP");
      }
   }

   if (info.bpp == 24 && ma == 0xff000000)
      s->img_n = 3;
   else
      s->img_n = ma ? 4 : 3;
   if (req_comp && req_comp >= 3) // we can directly decode 3 or 4
      target = req_comp;
   else
      target = s->img_n; // if they want monochrome, we'll post-convert

   // sanity-check size
   if (!stbi__mad3sizes_valid(target, s->img_x, s->img_y, 0))
      return stbi__errpuc("too large", "Corrupt BMP");

   out = (stbi_uc *) stbi__malloc_mad3(target, s->img_x, s->img_y, 0);
   if (!out) return stbi__errpuc("outofmem", "Out of memory");
   if (info.bpp < 16) {
      int z=0;
      if (psize == 0 || psize > 256) { STBI_FREE(out); return stbi__errpuc("invalid", "Corrupt BMP"); }
      for (i=0; i < psize; ++i) {
         pal[i][2] = stbi__get8(s);
         pal[i][1] = stbi__get8(s);
         pal[i][0] = stbi__get8(s);
         if (info.hsz != 12) stbi__get8(s);
         pal[i][3] = 255;
      }
      stbi__skip(s, info.offset - info.extra_read - info.hsz - psize * (info.hsz == 12 ? 3 : 4));
      if (info.bpp == 1) width = (s->img_x + 7) >> 3;
      else if (info.bpp == 4) width = (s->img_x + 1) >> 1;
      else if (info.bpp == 8) width = s->img_x;
      else { STBI_FREE(out); return stbi__errpuc("bad bpp", "Corrupt BMP"); }
      pad = (-width)&3;
      if (info.bpp == 1) {
         for (j=0; j < (int) s->img_y; ++j) {
            int bit_offset = 7, v = stbi__get8(s);
            for (i=0; i < (int) s->img_x; ++i) {
               int color = (v>>bit_offset)&0x1;
               out[z++] = pal[color][0];
               out[z++] = pal[color][1];
               out[z++] = pal[color][2];
               if (target == 4) out[z++] = 255;
               if (i+1 == (int) s->img_x) break;
               if((--bit_offset) < 0) {
                  bit_offset = 7;
                  v = stbi__get8(s);
               }
            }
            stbi__skip(s, pad);
         }
      } else {
         for (j=0; j < (int) s->img_y; ++j) {
            for (i=0; i < (int) s->img_x; i += 2) {
               int v=stbi__get8(s),v2=0;
               if (info.bpp == 4) {
                  v2 = v & 15;
                  v >>= 4;
               }
               out[z++] = pal[v][0];
               out[z++] = pal[v][1];
               out[z++] = pal[v][2];
               if (target == 4) out[z++] = 255;
               if (i+1 == (int) s->img_x) break;
               v = (info.bpp == 8) ? stbi__get8(s) : v2;
               out[z++] = pal[v][0];
               out[z++] = pal[v][1];
               out[z++] = pal[v][2];
               if (target == 4) out[z++] = 255;
            }
            stbi__skip(s, pad);
         }
      }
   } else {
      int rshift=0,gshift=0,bshift=0,ashift=0,rcount=0,gcount=0,bcount=0,acount=0;
      int z = 0;
      int easy=0;
      stbi__skip(s, info.offset - info.extra_read - info.hsz);
      if (info.bpp == 24) width = 3 * s->img_x;
      else if (info.bpp == 16) width = 2*s->img_x;
      else /* bpp = 32 and pad = 0 */ width=0;
      pad = (-width) & 3;
      if (info.bpp == 24) {
         easy = 1;
      } else if (info.bpp == 32) {
         if (mb == 0xff && mg == 0xff00 && mr == 0x00ff0000 && ma == 0xff000000)
            easy = 2;
      }
      if (!easy) {
         if (!mr || !mg || !mb) { STBI_FREE(out); return stbi__errpuc("bad masks", "Corrupt BMP"); }
         // right shift amt to put high bit in position #7
         rshift = stbi__high_bit(mr)-7; rcount = stbi__bitcount(mr);
         gshift = stbi__high_bit(mg)-7; gcount = stbi__bitcount(mg);
         bshift = stbi__high_bit(mb)-7; bcount = stbi__bitcount(mb);
         ashift = stbi__high_bit(ma)-7; acount = stbi__bitcount(ma);
         if (rcount > 8 || gcount > 8 || bcount > 8 || acount > 8) { STBI_FREE(out); return stbi__errpuc("bad masks", "Corrupt BMP"); }
      }
      for (j=0; j < (int) s->img_y; ++j) {
         if (easy) {
            for (i=0; i < (int) s->img_x; ++i) {
               unsigned char a;
               out[z+2] = stbi__get8(s);
               out[z+1] = stbi__get8(s);
               out[z+0] = stbi__get8(s);
               z += 3;
               a = (easy == 2 ? stbi__get8(s) : 255);
               all_a |= a;
               if (target == 4) out[z++] = a;
            }
         } else {
            int bpp = info.bpp;
            for (i=0; i < (int) s->img_x; ++i) {
               stbi__uint32 v = (bpp == 16 ? (stbi__uint32) stbi__get16le(s) : stbi__get32le(s));
               unsigned int a;
               out[z++] = STBI__BYTECAST(stbi__shiftsigned(v & mr, rshift, rcount));
               out[z++] = STBI__BYTECAST(stbi__shiftsigned(v & mg, gshift, gcount));
               out[z++] = STBI__BYTECAST(stbi__shiftsigned(v & mb, bshift, bcount));
               a = (ma ? stbi__shiftsigned(v & ma, ashift, acount) : 255);
               all_a |= a;
               if (target == 4) out[z++] = STBI__BYTECAST(a);
            }
         }
         stbi__skip(s, pad);
      }
   }

   // if alpha channel is all 0s, replace with all 255s
   if (target == 4 && all_a == 0)
      for (i=4*s->img_x*s->img_y-1; i >= 0; i -= 4)
         out[i] = 255;

   if (flip_vertically) {
      stbi_uc t;
      for (j=0; j < (int) s->img_y>>1; ++j) {
         stbi_uc *p1 = out +      j     *s->img_x*target;
         stbi_uc *p2 = out + (s->img_y-1-j)*s->img_x*target;
         for (i=0; i < (int) s->img_x*target; ++i) {
            t = p1[i]; p1[i] = p2[i]; p2[i] = t;
         }
      }
   }

   if (req_comp && req_comp != target) {
      out = stbi__convert_format(out, target, req_comp, s->img_x, s->img_y);
      if (out == NULL) return out; // stbi__convert_format frees input on failure
   }

   *x = s->img_x;
   *y = s->img_y;
   if (comp) *comp = s->img_n;
   return out;
}
#endif

// Targa Truevision - TGA
// by Jonathan Dummer
#ifndef STBI_NO_TGA
// returns STBI_rgb or whatever, 0 on error
static int stbi__tga_get_comp(int bits_per_pixel, int is_grey, int* is_rgb16)
{
   // only RGB or RGBA (incl. 16bit) or grey allowed
   if (is_rgb16) *is_rgb16 = 0;
   switch(bits_per_pixel) {
      case 8:  return STBI_grey;
      case 16: if(is_grey) return STBI_grey_alpha;
               // fallthrough
      case 15: if(is_rgb16) *is_rgb16 = 1;
               return STBI_rgb;
      case 24: // fallthrough
      case 32: return bits_per_pixel/8;
      default: return 0;
   }
}

static int stbi__tga_info(stbi__context *s, int *x, int *y, int *comp)
{
    int tga_w, tga_h, tga_comp, tga_image_type, tga_bits_per_pixel, tga_colormap_bpp;
    int sz, tga_colormap_type;
    stbi__get8(s);                   // discard Offset
    tga_colormap_type = stbi__get8(s); // colormap type
    if( tga_colormap_type > 1 ) {
        stbi__rewind(s);
        return 0;      // only RGB or indexed allowed
    }
    tga_image_type = stbi__get8(s); // image type
    if ( tga_colormap_type == 1 ) { // colormapped (paletted) image
        if (tga_image_type != 1 && tga_image_type != 9) {
            stbi__rewind(s);
            return 0;
        }
        stbi__skip(s,4);       // skip index of first colormap entry and number of entries
        sz = stbi__get8(s);    //   check bits per palette color entry
        if ( (sz != 8) && (sz != 15) && (sz != 16) && (sz != 24) && (sz != 32) ) {
            stbi__rewind(s);
            return 0;
        }
        stbi__skip(s,4);       // skip image x and y origin
        tga_colormap_bpp = sz;
    } else { // "normal" image w/o colormap - only RGB or grey allowed, +/- RLE
        if ( (tga_image_type != 2) && (tga_image_type != 3) && (tga_image_type != 10) && (tga_image_type != 11) ) {
            stbi__rewind(s);
            return 0; // only RGB or grey allowed, +/- RLE
        }
        stbi__skip(s,9); // skip colormap specification and image x/y origin
        tga_colormap_bpp = 0;
    }
    tga_w = stbi__get16le(s);
    if( tga_w < 1 ) {
        stbi__rewind(s);
        return 0;   // test width
    }
    tga_h = stbi__get16le(s);
    if( tga_h < 1 ) {
        stbi__rewind(s);
        return 0;   // test height
    }
    tga_bits_per_pixel = stbi__get8(s); // bits per pixel
    stbi__get8(s); // ignore alpha bits
    if (tga_colormap_bpp != 0) {
        if((tga_bits_per_pixel != 8) && (tga_bits_per_pixel != 16)) {
            // when using a colormap, tga_bits_per_pixel is the size of the indexes
            // I don't think anything but 8 or 16bit indexes makes sense
            stbi__rewind(s);
            return 0;
        }
        tga_comp = stbi__tga_get_comp(tga_colormap_bpp, 0, NULL);
    } else {
        tga_comp = stbi__tga_get_comp(tga_bits_per_pixel, (tga_image_type == 3) || (tga_image_type == 11), NULL);
    }
    if(!tga_comp) {
      stbi__rewind(s);
      return 0;
    }
    if (x) *x = tga_w;
    if (y) *y = tga_h;
    if (comp) *comp = tga_comp;
    return 1;                   // seems to have passed everything
}

static int stbi__tga_test(stbi__context *s)
{
   int res = 0;
   int sz, tga_color_type;
   stbi__get8(s);      //   discard Offset
   tga_color_type = stbi__get8(s);   //   color type
   if ( tga_color_type > 1 ) goto errorEnd;   //   only RGB or indexed allowed
   sz = stbi__get8(s);   //   image type
   if ( tga_color_type == 1 ) { // colormapped (paletted) image
      if (sz != 1 && sz != 9) goto errorEnd; // colortype 1 demands image type 1 or 9
      stbi__skip(s,4);       // skip index of first colormap entry and number of entries
      sz = stbi__get8(s);    //   check bits per palette color entry
      if ( (sz != 8) && (sz != 15) && (sz != 16) && (sz != 24) && (sz != 32) ) goto errorEnd;
      stbi__skip(s,4);       // skip image x and y origin
   } else { // "normal" image w/o colormap
      if ( (sz != 2) && (sz != 3) && (sz != 10) && (sz != 11) ) goto errorEnd; // only RGB or grey allowed, +/- RLE
      stbi__skip(s,9); // skip colormap specification and image x/y origin
   }
   if ( stbi__get16le(s) < 1 ) goto errorEnd;      //   test width
   if ( stbi__get16le(s) < 1 ) goto errorEnd;      //   test height
   sz = stbi__get8(s);   //   bits per pixel
   if ( (tga_color_type == 1) && (sz != 8) && (sz != 16) ) goto errorEnd; // for colormapped images, bpp is size of an index
   if ( (sz != 8) && (sz != 15) && (sz != 16) && (sz != 24) && (sz != 32) ) goto errorEnd;

   res = 1; // if we got this far, everything's good and we can return 1 instead of 0

errorEnd:
   stbi__rewind(s);
   return res;
}

// read 16bit value and convert to 24bit RGB
static void stbi__tga_read_rgb16(stbi__context *s, stbi_uc* out)
{
   stbi__uint16 px = (stbi__uint16)stbi__get16le(s);
   stbi__uint16 fiveBitMask = 31;
   // we have 3 channels with 5bits each
   int r = (px >> 10) & fiveBitMask;
   int g = (px >> 5) & fiveBitMask;
   int b = px & fiveBitMask;
   // Note that this saves the data in RGB(A) order, so it doesn't need to be swapped later
   out[0] = (stbi_uc)((r * 255)/31);
   out[1] = (stbi_uc)((g * 255)/31);
   out[2] = (stbi_uc)((b * 255)/31);

   // some people claim that the most significant bit might be used for alpha
   // (possibly if an alpha-bit is set in the "image descriptor byte")
   // but that only made 16bit test images completely translucent..
   // so let's treat all 15 and 16bit TGAs as RGB with no alpha.
}

static void *stbi__tga_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri)
{
   //   read in the TGA header stuff
   int tga_offset = stbi__get8(s);
   int tga_indexed = stbi__get8(s);
   int tga_image_type = stbi__get8(s);
   int tga_is_RLE = 0;
   int tga_palette_start = stbi__get16le(s);
   int tga_palette_len = stbi__get16le(s);
   int tga_palette_bits = stbi__get8(s);
   int tga_x_origin = stbi__get16le(s);
   int tga_y_origin = stbi__get16le(s);
   int tga_width = stbi__get16le(s);
   int tga_height = stbi__get16le(s);
   int tga_bits_per_pixel = stbi__get8(s);
   int tga_comp, tga_rgb16=0;
   int tga_inverted = stbi__get8(s);
   // int tga_alpha_bits = tga_inverted & 15; // the 4 lowest bits - unused (useless?)
   //   image data
   unsigned char *tga_data;
   unsigned char *tga_palette = NULL;
   int i, j;
   unsigned char raw_data[4] = {0};
   int RLE_count = 0;
   int RLE_repeating = 0;
   int read_next_pixel = 1;
   STBI_NOTUSED(ri);
   STBI_NOTUSED(tga_x_origin); // @TODO
   STBI_NOTUSED(tga_y_origin); // @TODO

   if (tga_height > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");
   if (tga_width > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");

   //   do a tiny bit of precessing
   if ( tga_image_type >= 8 )
   {
      tga_image_type -= 8;
      tga_is_RLE = 1;
   }
   tga_inverted = 1 - ((tga_inverted >> 5) & 1);

   //   If I'm paletted, then I'll use the number of bits from the palette
   if ( tga_indexed ) tga_comp = stbi__tga_get_comp(tga_palette_bits, 0, &tga_rgb16);
   else tga_comp = stbi__tga_get_comp(tga_bits_per_pixel, (tga_image_type == 3), &tga_rgb16);

   if(!tga_comp) // shouldn't really happen, stbi__tga_test() should have ensured basic consistency
      return stbi__errpuc("bad format", "Can't find out TGA pixelformat");

   //   tga info
   *x = tga_width;
   *y = tga_height;
   if (comp) *comp = tga_comp;

   if (!stbi__mad3sizes_valid(tga_width, tga_height, tga_comp, 0))
      return stbi__errpuc("too large", "Corrupt TGA");

   tga_data = (unsigned char*)stbi__malloc_mad3(tga_width, tga_height, tga_comp, 0);
   if (!tga_data) return stbi__errpuc("outofmem", "Out of memory");

   // skip to the data's starting position (offset usually = 0)
   stbi__skip(s, tga_offset );

   if ( !tga_indexed && !tga_is_RLE && !tga_rgb16 ) {
      for (i=0; i < tga_height; ++i) {
         int row = tga_inverted ? tga_height -i - 1 : i;
         stbi_uc *tga_row = tga_data + row*tga_width*tga_comp;
         stbi__getn(s, tga_row, tga_width * tga_comp);
      }
   } else  {
      //   do I need to load a palette?
      if ( tga_indexed)
      {
         if (tga_palette_len == 0) {  /* you have to have at least one entry! */
            STBI_FREE(tga_data);
            return stbi__errpuc("bad palette", "Corrupt TGA");
         }

         //   any data to skip? (offset usually = 0)
         stbi__skip(s, tga_palette_start );
         //   load the palette
         tga_palette = (unsigned char*)stbi__malloc_mad2(tga_palette_len, tga_comp, 0);
         if (!tga_palette) {
            STBI_FREE(tga_data);
            return stbi__errpuc("outofmem", "Out of memory");
         }
         if (tga_rgb16) {
            stbi_uc *pal_entry = tga_palette;
            STBI_ASSERT(tga_comp == STBI_rgb);
            for (i=0; i < tga_palette_len; ++i) {
               stbi__tga_read_rgb16(s, pal_entry);
               pal_entry += tga_comp;
            }
         } else if (!stbi__getn(s, tga_palette, tga_palette_len * tga_comp)) {
               STBI_FREE(tga_data);
               STBI_FREE(tga_palette);
               return stbi__errpuc("bad palette", "Corrupt TGA");
         }
      }
      //   load the data
      for (i=0; i < tga_width * tga_height; ++i)
      {
         //   if I'm in RLE mode, do I need to get a RLE stbi__pngchunk?
         if ( tga_is_RLE )
         {
            if ( RLE_count == 0 )
            {
               //   yep, get the next byte as a RLE command
               int RLE_cmd = stbi__get8(s);
               RLE_count = 1 + (RLE_cmd & 127);
               RLE_repeating = RLE_cmd >> 7;
               read_next_pixel = 1;
            } else if ( !RLE_repeating )
            {
               read_next_pixel = 1;
            }
         } else
         {
            read_next_pixel = 1;
         }
         //   OK, if I need to read a pixel, do it now
         if ( read_next_pixel )
         {
            //   load however much data we did have
            if ( tga_indexed )
            {
               // read in index, then perform the lookup
               int pal_idx = (tga_bits_per_pixel == 8) ? stbi__get8(s) : stbi__get16le(s);
               if ( pal_idx >= tga_palette_len ) {
                  // invalid index
                  pal_idx = 0;
               }
               pal_idx *= tga_comp;
               for (j = 0; j < tga_comp; ++j) {
                  raw_data[j] = tga_palette[pal_idx+j];
               }
            } else if(tga_rgb16) {
               STBI_ASSERT(tga_comp == STBI_rgb);
               stbi__tga_read_rgb16(s, raw_data);
            } else {
               //   read in the data raw
               for (j = 0; j < tga_comp; ++j) {
                  raw_data[j] = stbi__get8(s);
               }
            }
            //   clear the reading flag for the next pixel
            read_next_pixel = 0;
         } // end of reading a pixel

         // copy data
         for (j = 0; j < tga_comp; ++j)
           tga_data[i*tga_comp+j] = raw_data[j];

         //   in case we're in RLE mode, keep counting down
         --RLE_count;
      }
      //   do I need to invert the image?
      if ( tga_inverted )
      {
         for (j = 0; j*2 < tga_height; ++j)
         {
            int index1 = j * tga_width * tga_comp;
            int index2 = (tga_height - 1 - j) * tga_width * tga_comp;
            for (i = tga_width * tga_comp; i > 0; --i)
            {
               unsigned char temp = tga_data[index1];
               tga_data[index1] = tga_data[index2];
               tga_data[index2] = temp;
               ++index1;
               ++index2;
            }
         }
      }
      //   clear my palette, if I had one
      if ( tga_palette != NULL )
      {
         STBI_FREE( tga_palette );
      }
   }

   // swap RGB - if the source data was RGB16, it already is in the right order
   if (tga_comp >= 3 && !tga_rgb16)
   {
      unsigned char* tga_pixel = tga_data;
      for (i=0; i < tga_width * tga_height; ++i)
      {
         unsigned char temp = tga_pixel[0];
         tga_pixel[0] = tga_pixel[2];
         tga_pixel[2] = temp;
         tga_pixel += tga_comp;
      }
   }

   // convert to target component count
   if (req_comp && req_comp != tga_comp)
      tga_data = stbi__convert_format(tga_data, tga_comp, req_comp, tga_width, tga_height);

   //   the things I do to get rid of an error message, and yet keep
   //   Microsoft's C compilers happy... [8^(
   tga_palette_start = tga_palette_len = tga_palette_bits =
         tga_x_origin = tga_y_origin = 0;
   STBI_NOTUSED(tga_palette_start);
   //   OK, done
   return tga_data;
}
#endif

// *************************************************************************************************
// Photoshop PSD loader -- PD by Thatcher Ulrich, integration by Nicolas Schulz, tweaked by STB

#ifndef STBI_NO_PSD
static int stbi__psd_test(stbi__context *s)
{
   int r = (stbi__get32be(s) == 0x38425053);
   stbi__rewind(s);
   return r;
}

static int stbi__psd_decode_rle(stbi__context *s, stbi_uc *p, int pixelCount)
{
   int count, nleft, len;

   count = 0;
   while ((nleft = pixelCount - count) > 0) {
      len = stbi__get8(s);
      if (len == 128) {
         // No-op.
      } else if (len < 128) {
         // Copy next len+1 bytes literally.
         len++;
         if (len > nleft) return 0; // corrupt data
         count += len;
         while (len) {
            *p = stbi__get8(s);
            p += 4;
            len--;
         }
      } else if (len > 128) {
         stbi_uc   val;
         // Next -len+1 bytes in the dest are replicated from next source byte.
         // (Interpret len as a negative 8-bit int.)
         len = 257 - len;
         if (len > nleft) return 0; // corrupt data
         val = stbi__get8(s);
         count += len;
         while (len) {
            *p = val;
            p += 4;
            len--;
         }
      }
   }

   return 1;
}

static void *stbi__psd_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri, int bpc)
{
   int pixelCount;
   int channelCount, compression;
   int channel, i;
   int bitdepth;
   int w,h;
   stbi_uc *out;
   STBI_NOTUSED(ri);

   // Check identifier
   if (stbi__get32be(s) != 0x38425053)   // "8BPS"
      return stbi__errpuc("not PSD", "Corrupt PSD image");

   // Check file type version.
   if (stbi__get16be(s) != 1)
      return stbi__errpuc("wrong version", "Unsupported version of PSD image");

   // Skip 6 reserved bytes.
   stbi__skip(s, 6 );

   // Read the number of channels (R, G, B, A, etc).
   channelCount = stbi__get16be(s);
   if (channelCount < 0 || channelCount > 16)
      return stbi__errpuc("wrong channel count", "Unsupported number of channels in PSD image");

   // Read the rows and columns of the image.
   h = stbi__get32be(s);
   w = stbi__get32be(s);

   if (h > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");
   if (w > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");

   // Make sure the depth is 8 bits.
   bitdepth = stbi__get16be(s);
   if (bitdepth != 8 && bitdepth != 16)
      return stbi__errpuc("unsupported bit depth", "PSD bit depth is not 8 or 16 bit");

   // Make sure the color mode is RGB.
   // Valid options are:
   //   0: Bitmap
   //   1: Grayscale
   //   2: Indexed color
   //   3: RGB color
   //   4: CMYK color
   //   7: Multichannel
   //   8: Duotone
   //   9: Lab color
   if (stbi__get16be(s) != 3)
      return stbi__errpuc("wrong color format", "PSD is not in RGB color format");

   // Skip the Mode Data.  (It's the palette for indexed color; other info for other modes.)
   stbi__skip(s,stbi__get32be(s) );

   // Skip the image resources.  (resolution, pen tool paths, etc)
   stbi__skip(s, stbi__get32be(s) );

   // Skip the reserved data.
   stbi__skip(s, stbi__get32be(s) );

   // Find out if the data is compressed.
   // Known values:
   //   0: no compression
   //   1: RLE compressed
   compression = stbi__get16be(s);
   if (compression > 1)
      return stbi__errpuc("bad compression", "PSD has an unknown compression format");

   // Check size
   if (!stbi__mad3sizes_valid(4, w, h, 0))
      return stbi__errpuc("too large", "Corrupt PSD");

   // Create the destination image.

   if (!compression && bitdepth == 16 && bpc == 16) {
      out = (stbi_uc *) stbi__malloc_mad3(8, w, h, 0);
      ri->bits_per_channel = 16;
   } else
      out = (stbi_uc *) stbi__malloc(4 * w*h);

   if (!out) return stbi__errpuc("outofmem", "Out of memory");
   pixelCount = w*h;

   // Initialize the data to zero.
   //memset( out, 0, pixelCount * 4 );

   // Finally, the image data.
   if (compression) {
      // RLE as used by .PSD and .TIFF
      // Loop until you get the number of unpacked bytes you are expecting:
      //     Read the next source byte into n.
      //     If n is between 0 and 127 inclusive, copy the next n+1 bytes literally.
      //     Else if n is between -127 and -1 inclusive, copy the next byte -n+1 times.
      //     Else if n is 128, noop.
      // Endloop

      // The RLE-compressed data is preceded by a 2-byte data count for each row in the data,
      // which we're going to just skip.
      stbi__skip(s, h * channelCount * 2 );

      // Read the RLE data by channel.
      for (channel = 0; channel < 4; channel++) {
         stbi_uc *p;

         p = out+channel;
         if (channel >= channelCount) {
            // Fill this channel with default data.
            for (i = 0; i < pixelCount; i++, p += 4)
               *p = (channel == 3 ? 255 : 0);
         } else {
            // Read the RLE data.
            if (!stbi__psd_decode_rle(s, p, pixelCount)) {
               STBI_FREE(out);
               return stbi__errpuc("corrupt", "bad RLE data");
            }
         }
      }

   } else {
      // We're at the raw image data.  It's each channel in order (Red, Green, Blue, Alpha, ...)
      // where each channel consists of an 8-bit (or 16-bit) value for each pixel in the image.

      // Read the data by channel.
      for (channel = 0; channel < 4; channel++) {
         if (channel >= channelCount) {
            // Fill this channel with default data.
            if (bitdepth == 16 && bpc == 16) {
               stbi__uint16 *q = ((stbi__uint16 *) out) + channel;
               stbi__uint16 val = channel == 3 ? 65535 : 0;
               for (i = 0; i < pixelCount; i++, q += 4)
                  *q = val;
            } else {
               stbi_uc *p = out+channel;
               stbi_uc val = channel == 3 ? 255 : 0;
               for (i = 0; i < pixelCount; i++, p += 4)
                  *p = val;
            }
         } else {
            if (ri->bits_per_channel == 16) {    // output bpc
               stbi__uint16 *q = ((stbi__uint16 *) out) + channel;
               for (i = 0; i < pixelCount; i++, q += 4)
                  *q = (stbi__uint16) stbi__get16be(s);
            } else {
               stbi_uc *p = out+channel;
               if (bitdepth == 16) {  // input bpc
                  for (i = 0; i < pixelCount; i++, p += 4)
                     *p = (stbi_uc) (stbi__get16be(s) >> 8);
               } else {
                  for (i = 0; i < pixelCount; i++, p += 4)
                     *p = stbi__get8(s);
               }
            }
         }
      }
   }

   // remove weird white matte from PSD
   if (channelCount >= 4) {
      if (ri->bits_per_channel == 16) {
         for (i=0; i < w*h; ++i) {
            stbi__uint16 *pixel = (stbi__uint16 *) out + 4*i;
            if (pixel[3] != 0 && pixel[3] != 65535) {
               float a = pixel[3] / 65535.0f;
               float ra = 1.0f / a;
               float inv_a = 65535.0f * (1 - ra);
               pixel[0] = (stbi__uint16) (pixel[0]*ra + inv_a);
               pixel[1] = (stbi__uint16) (pixel[1]*ra + inv_a);
               pixel[2] = (stbi__uint16) (pixel[2]*ra + inv_a);
            }
         }
      } else {
         for (i=0; i < w*h; ++i) {
            unsigned char *pixel = out + 4*i;
            if (pixel[3] != 0 && pixel[3] != 255) {
               float a = pixel[3] / 255.0f;
               float ra = 1.0f / a;
               float inv_a = 255.0f * (1 - ra);
               pixel[0] = (unsigned char) (pixel[0]*ra + inv_a);
               pixel[1] = (unsigned char) (pixel[1]*ra + inv_a);
               pixel[2] = (unsigned char) (pixel[2]*ra + inv_a);
            }
         }
      }
   }

   // convert to desired output format
   if (req_comp && req_comp != 4) {
      if (ri->bits_per_channel == 16)
         out = (stbi_uc *) stbi__convert_format16((stbi__uint16 *) out, 4, req_comp, w, h);
      else
         out = stbi__convert_format(out, 4, req_comp, w, h);
      if (out == NULL) return out; // stbi__convert_format frees input on failure
   }

   if (comp) *comp = 4;
   *y = h;
   *x = w;

   return out;
}
#endif

// *************************************************************************************************
// Softimage PIC loader
// by Tom Seddon
//
// See http://softimage.wiki.softimage.com/index.php/INFO:_PIC_file_format
// See http://ozviz.wasp.uwa.edu.au/~pbourke/dataformats/softimagepic/

#ifndef STBI_NO_PIC
static int stbi__pic_is4(stbi__context *s,const char *str)
{
   int i;
   for (i=0; i<4; ++i)
      if (stbi__get8(s) != (stbi_uc)str[i])
         return 0;

   return 1;
}

static int stbi__pic_test_core(stbi__context *s)
{
   int i;

   if (!stbi__pic_is4(s,"\x53\x80\xF6\x34"))
      return 0;

   for(i=0;i<84;++i)
      stbi__get8(s);

   if (!stbi__pic_is4(s,"PICT"))
      return 0;

   return 1;
}

typedef struct
{
   stbi_uc size,type,channel;
} stbi__pic_packet;

static stbi_uc *stbi__readval(stbi__context *s, int channel, stbi_uc *dest)
{
   int mask=0x80, i;

   for (i=0; i<4; ++i, mask>>=1) {
      if (channel & mask) {
         if (stbi__at_eof(s)) return stbi__errpuc("bad file","PIC file too short");
         dest[i]=stbi__get8(s);
      }
   }

   return dest;
}

static void stbi__copyval(int channel,stbi_uc *dest,const stbi_uc *src)
{
   int mask=0x80,i;

   for (i=0;i<4; ++i, mask>>=1)
      if (channel&mask)
         dest[i]=src[i];
}

static stbi_uc *stbi__pic_load_core(stbi__context *s,int width,int height,int *comp, stbi_uc *result)
{
   int act_comp=0,num_packets=0,y,chained;
   stbi__pic_packet packets[10];

   // this will (should...) cater for even some bizarre stuff like having data
    // for the same channel in multiple packets.
   do {
      stbi__pic_packet *packet;

      if (num_packets==sizeof(packets)/sizeof(packets[0]))
         return stbi__errpuc("bad format","too many packets");

      packet = &packets[num_packets++];

      chained = stbi__get8(s);
      packet->size    = stbi__get8(s);
      packet->type    = stbi__get8(s);
      packet->channel = stbi__get8(s);

      act_comp |= packet->channel;

      if (stbi__at_eof(s))          return stbi__errpuc("bad file","file too short (reading packets)");
      if (packet->size != 8)  return stbi__errpuc("bad format","packet isn't 8bpp");
   } while (chained);

   *comp = (act_comp & 0x10 ? 4 : 3); // has alpha channel?

   for(y=0; y<height; ++y) {
      int packet_idx;

      for(packet_idx=0; packet_idx < num_packets; ++packet_idx) {
         stbi__pic_packet *packet = &packets[packet_idx];
         stbi_uc *dest = result+y*width*4;

         switch (packet->type) {
            default:
               return stbi__errpuc("bad format","packet has bad compression type");

            case 0: {//uncompressed
               int x;

               for(x=0;x<width;++x, dest+=4)
                  if (!stbi__readval(s,packet->channel,dest))
                     return 0;
               break;
            }

            case 1://Pure RLE
               {
                  int left=width, i;

                  while (left>0) {
                     stbi_uc count,value[4];

                     count=stbi__get8(s);
                     if (stbi__at_eof(s))   return stbi__errpuc("bad file","file too short (pure read count)");

                     if (count > left)
                        count = (stbi_uc) left;

                     if (!stbi__readval(s,packet->channel,value))  return 0;

                     for(i=0; i<count; ++i,dest+=4)
                        stbi__copyval(packet->channel,dest,value);
                     left -= count;
                  }
               }
               break;

            case 2: {//Mixed RLE
               int left=width;
               while (left>0) {
                  int count = stbi__get8(s), i;
                  if (stbi__at_eof(s))  return stbi__errpuc("bad file","file too short (mixed read count)");

                  if (count >= 128) { // Repeated
                     stbi_uc value[4];

                     if (count==128)
                        count = stbi__get16be(s);
                     else
                        count -= 127;
                     if (count > left)
                        return stbi__errpuc("bad file","scanline overrun");

                     if (!stbi__readval(s,packet->channel,value))
                        return 0;

                     for(i=0;i<count;++i, dest += 4)
                        stbi__copyval(packet->channel,dest,value);
                  } else { // Raw
                     ++count;
                     if (count>left) return stbi__errpuc("bad file","scanline overrun");

                     for(i=0;i<count;++i, dest+=4)
                        if (!stbi__readval(s,packet->channel,dest))
                           return 0;
                  }
                  left-=count;
               }
               break;
            }
         }
      }
   }

   return result;
}

static void *stbi__pic_load(stbi__context *s,int *px,int *py,int *comp,int req_comp, stbi__result_info *ri)
{
   stbi_uc *result;
   int i, x,y, internal_comp;
   STBI_NOTUSED(ri);

   if (!comp) comp = &internal_comp;

   for (i=0; i<92; ++i)
      stbi__get8(s);

   x = stbi__get16be(s);
   y = stbi__get16be(s);

   if (y > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");
   if (x > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");

   if (stbi__at_eof(s))  return stbi__errpuc("bad file","file too short (pic header)");
   if (!stbi__mad3sizes_valid(x, y, 4, 0)) return stbi__errpuc("too large", "PIC image too large to decode");

   stbi__get32be(s); //skip `ratio'
   stbi__get16be(s); //skip `fields'
   stbi__get16be(s); //skip `pad'

   // intermediate buffer is RGBA
   result = (stbi_uc *) stbi__malloc_mad3(x, y, 4, 0);
   if (!result) return stbi__errpuc("outofmem", "Out of memory");
   memset(result, 0xff, x*y*4);

   if (!stbi__pic_load_core(s,x,y,comp, result)) {
      STBI_FREE(result);
      result=0;
   }
   *px = x;
   *py = y;
   if (req_comp == 0) req_comp = *comp;
   result=stbi__convert_format(result,4,req_comp,x,y);

   return result;
}

static int stbi__pic_test(stbi__context *s)
{
   int r = stbi__pic_test_core(s);
   stbi__rewind(s);
   return r;
}
#endif

// *************************************************************************************************
// GIF loader -- public domain by Jean-Marc Lienher -- simplified/shrunk by stb

#ifndef STBI_NO_GIF
typedef struct
{
   stbi__int16 prefix;
   stbi_uc first;
   stbi_uc suffix;
} stbi__gif_lzw;

typedef struct
{
   int w,h;
   stbi_uc *out;                 // output buffer (always 4 components)
   stbi_uc *background;          // The current "background" as far as a gif is concerned
   stbi_uc *history;
   int flags, bgindex, ratio, transparent, eflags;
   stbi_uc  pal[256][4];
   stbi_uc lpal[256][4];
   stbi__gif_lzw codes[8192];
   stbi_uc *color_table;
   int parse, step;
   int lflags;
   int start_x, start_y;
   int max_x, max_y;
   int cur_x, cur_y;
   int line_size;
   int delay;
} stbi__gif;

static int stbi__gif_test_raw(stbi__context *s)
{
   int sz;
   if (stbi__get8(s) != 'G' || stbi__get8(s) != 'I' || stbi__get8(s) != 'F' || stbi__get8(s) != '8') return 0;
   sz = stbi__get8(s);
   if (sz != '9' && sz != '7') return 0;
   if (stbi__get8(s) != 'a') return 0;
   return 1;
}

static int stbi__gif_test(stbi__context *s)
{
   int r = stbi__gif_test_raw(s);
   stbi__rewind(s);
   return r;
}

static void stbi__gif_parse_colortable(stbi__context *s, stbi_uc pal[256][4], int num_entries, int transp)
{
   int i;
   for (i=0; i < num_entries; ++i) {
      pal[i][2] = stbi__get8(s);
      pal[i][1] = stbi__get8(s);
      pal[i][0] = stbi__get8(s);
      pal[i][3] = transp == i ? 0 : 255;
   }
}

static int stbi__gif_header(stbi__context *s, stbi__gif *g, int *comp, int is_info)
{
   stbi_uc version;
   if (stbi__get8(s) != 'G' || stbi__get8(s) != 'I' || stbi__get8(s) != 'F' || stbi__get8(s) != '8')
      return stbi__err("not GIF", "Corrupt GIF");

   version = stbi__get8(s);
   if (version != '7' && version != '9')    return stbi__err("not GIF", "Corrupt GIF");
   if (stbi__get8(s) != 'a')                return stbi__err("not GIF", "Corrupt GIF");

   stbi__g_failure_reason = "";
   g->w = stbi__get16le(s);
   g->h = stbi__get16le(s);
   g->flags = stbi__get8(s);
   g->bgindex = stbi__get8(s);
   g->ratio = stbi__get8(s);
   g->transparent = -1;

   if (g->w > STBI_MAX_DIMENSIONS) return stbi__err("too large","Very large image (corrupt?)");
   if (g->h > STBI_MAX_DIMENSIONS) return stbi__err("too large","Very large image (corrupt?)");

   if (comp != 0) *comp = 4;  // can't actually tell whether it's 3 or 4 until we parse the comments

   if (is_info) return 1;

   if (g->flags & 0x80)
      stbi__gif_parse_colortable(s,g->pal, 2 << (g->flags & 7), -1);

   return 1;
}

static int stbi__gif_info_raw(stbi__context *s, int *x, int *y, int *comp)
{
   stbi__gif* g = (stbi__gif*) stbi__malloc(sizeof(stbi__gif));
   if (!g) return stbi__err("outofmem", "Out of memory");
   if (!stbi__gif_header(s, g, comp, 1)) {
      STBI_FREE(g);
      stbi__rewind( s );
      return 0;
   }
   if (x) *x = g->w;
   if (y) *y = g->h;
   STBI_FREE(g);
   return 1;
}

static void stbi__out_gif_code(stbi__gif *g, stbi__uint16 code)
{
   stbi_uc *p, *c;
   int idx;

   // recurse to decode the prefixes, since the linked-list is backwards,
   // and working backwards through an interleaved image would be nasty
   if (g->codes[code].prefix >= 0)
      stbi__out_gif_code(g, g->codes[code].prefix);

   if (g->cur_y >= g->max_y) return;

   idx = g->cur_x + g->cur_y;
   p = &g->out[idx];
   g->history[idx / 4] = 1;

   c = &g->color_table[g->codes[code].suffix * 4];
   if (c[3] > 128) { // don't render transparent pixels;
      p[0] = c[2];
      p[1] = c[1];
      p[2] = c[0];
      p[3] = c[3];
   }
   g->cur_x += 4;

   if (g->cur_x >= g->max_x) {
      g->cur_x = g->start_x;
      g->cur_y += g->step;

      while (g->cur_y >= g->max_y && g->parse > 0) {
         g->step = (1 << g->parse) * g->line_size;
         g->cur_y = g->start_y + (g->step >> 1);
         --g->parse;
      }
   }
}

static stbi_uc *stbi__process_gif_raster(stbi__context *s, stbi__gif *g)
{
   stbi_uc lzw_cs;
   stbi__int32 len, init_code;
   stbi__uint32 first;
   stbi__int32 codesize, codemask, avail, oldcode, bits, valid_bits, clear;
   stbi__gif_lzw *p;

   lzw_cs = stbi__get8(s);
   if (lzw_cs > 12) return NULL;
   clear = 1 << lzw_cs;
   first = 1;
   codesize = lzw_cs + 1;
   codemask = (1 << codesize) - 1;
   bits = 0;
   valid_bits = 0;
   for (init_code = 0; init_code < clear; init_code++) {
      g->codes[init_code].prefix = -1;
      g->codes[init_code].first = (stbi_uc) init_code;
      g->codes[init_code].suffix = (stbi_uc) init_code;
   }

   // support no starting clear code
   avail = clear+2;
   oldcode = -1;

   len = 0;
   for(;;) {
      if (valid_bits < codesize) {
         if (len == 0) {
            len = stbi__get8(s); // start new block
            if (len == 0)
               return g->out;
         }
         --len;
         bits |= (stbi__int32) stbi__get8(s) << valid_bits;
         valid_bits += 8;
      } else {
         stbi__int32 code = bits & codemask;
         bits >>= codesize;
         valid_bits -= codesize;
         // @OPTIMIZE: is there some way we can accelerate the non-clear path?
         if (code == clear) {  // clear code
            codesize = lzw_cs + 1;
            codemask = (1 << codesize) - 1;
            avail = clear + 2;
            oldcode = -1;
            first = 0;
         } else if (code == clear + 1) { // end of stream code
            stbi__skip(s, len);
            while ((len = stbi__get8(s)) > 0)
               stbi__skip(s,len);
            return g->out;
         } else if (code <= avail) {
            if (first) {
               return stbi__errpuc("no clear code", "Corrupt GIF");
            }

            if (oldcode >= 0) {
               p = &g->codes[avail++];
               if (avail > 8192) {
                  return stbi__errpuc("too many codes", "Corrupt GIF");
               }

               p->prefix = (stbi__int16) oldcode;
               p->first = g->codes[oldcode].first;
               p->suffix = (code == avail) ? p->first : g->codes[code].first;
            } else if (code == avail)
               return stbi__errpuc("illegal code in raster", "Corrupt GIF");

            stbi__out_gif_code(g, (stbi__uint16) code);

            if ((avail & codemask) == 0 && avail <= 0x0FFF) {
               codesize++;
               codemask = (1 << codesize) - 1;
            }

            oldcode = code;
         } else {
            return stbi__errpuc("illegal code in raster", "Corrupt GIF");
         }
      }
   }
}

// this function is designed to support animated gifs, although stb_image doesn't support it
// two back is the image from two frames ago, used for a very specific disposal format
static stbi_uc *stbi__gif_load_next(stbi__context *s, stbi__gif *g, int *comp, int req_comp, stbi_uc *two_back)
{
   int dispose;
   int first_frame;
   int pi;
   int pcount;
   STBI_NOTUSED(req_comp);

   // on first frame, any non-written pixels get the background colour (non-transparent)
   first_frame = 0;
   if (g->out == 0) {
      if (!stbi__gif_header(s, g, comp,0)) return 0; // stbi__g_failure_reason set by stbi__gif_header
      if (!stbi__mad3sizes_valid(4, g->w, g->h, 0))
         return stbi__errpuc("too large", "GIF image is too large");
      pcount = g->w * g->h;
      g->out = (stbi_uc *) stbi__malloc(4 * pcount);
      g->background = (stbi_uc *) stbi__malloc(4 * pcount);
      g->history = (stbi_uc *) stbi__malloc(pcount);
      if (!g->out || !g->background || !g->history)
         return stbi__errpuc("outofmem", "Out of memory");

      // image is treated as "transparent" at the start - ie, nothing overwrites the current background;
      // background colour is only used for pixels that are not rendered first frame, after that "background"
      // color refers to the color that was there the previous frame.
      memset(g->out, 0x00, 4 * pcount);
      memset(g->background, 0x00, 4 * pcount); // state of the background (starts transparent)
      memset(g->history, 0x00, pcount);        // pixels that were affected previous frame
      first_frame = 1;
   } else {
      // second frame - how do we dispose of the previous one?
      dispose = (g->eflags & 0x1C) >> 2;
      pcount = g->w * g->h;

      if ((dispose == 3) && (two_back == 0)) {
         dispose = 2; // if I don't have an image to revert back to, default to the old background
      }

      if (dispose == 3) { // use previous graphic
         for (pi = 0; pi < pcount; ++pi) {
            if (g->history[pi]) {
               memcpy( &g->out[pi * 4], &two_back[pi * 4], 4 );
            }
         }
      } else if (dispose == 2) {
         // restore what was changed last frame to background before that frame;
         for (pi = 0; pi < pcount; ++pi) {
            if (g->history[pi]) {
               memcpy( &g->out[pi * 4], &g->background[pi * 4], 4 );
            }
         }
      } else {
         // This is a non-disposal case eithe way, so just
         // leave the pixels as is, and they will become the new background
         // 1: do not dispose
         // 0:  not specified.
      }

      // background is what out is after the undoing of the previou frame;
      memcpy( g->background, g->out, 4 * g->w * g->h );
   }

   // clear my history;
   memset( g->history, 0x00, g->w * g->h );        // pixels that were affected previous frame

   for (;;) {
      int tag = stbi__get8(s);
      switch (tag) {
         case 0x2C: /* Image Descriptor */
         {
            stbi__int32 x, y, w, h;
            stbi_uc *o;

            x = stbi__get16le(s);
            y = stbi__get16le(s);
            w = stbi__get16le(s);
            h = stbi__get16le(s);
            if (((x + w) > (g->w)) || ((y + h) > (g->h)))
               return stbi__errpuc("bad Image Descriptor", "Corrupt GIF");

            g->line_size = g->w * 4;
            g->start_x = x * 4;
            g->start_y = y * g->line_size;
            g->max_x   = g->start_x + w * 4;
            g->max_y   = g->start_y + h * g->line_size;
            g->cur_x   = g->start_x;
            g->cur_y   = g->start_y;

            // if the width of the specified rectangle is 0, that means
            // we may not see *any* pixels or the image is malformed;
            // to make sure this is caught, move the current y down to
            // max_y (which is what out_gif_code checks).
            if (w == 0)
               g->cur_y = g->max_y;

            g->lflags = stbi__get8(s);

            if (g->lflags & 0x40) {
               g->step = 8 * g->line_size; // first interlaced spacing
               g->parse = 3;
            } else {
               g->step = g->line_size;
               g->parse = 0;
            }

            if (g->lflags & 0x80) {
               stbi__gif_parse_colortable(s,g->lpal, 2 << (g->lflags & 7), g->eflags & 0x01 ? g->transparent : -1);
               g->color_table = (stbi_uc *) g->lpal;
            } else if (g->flags & 0x80) {
               g->color_table = (stbi_uc *) g->pal;
            } else
               return stbi__errpuc("missing color table", "Corrupt GIF");

            o = stbi__process_gif_raster(s, g);
            if (!o) return NULL;

            // if this was the first frame,
            pcount = g->w * g->h;
            if (first_frame && (g->bgindex > 0)) {
               // if first frame, any pixel not drawn to gets the background color
               for (pi = 0; pi < pcount; ++pi) {
                  if (g->history[pi] == 0) {
                     g->pal[g->bgindex][3] = 255; // just in case it was made transparent, undo that; It will be reset next frame if need be;
                     memcpy( &g->out[pi * 4], &g->pal[g->bgindex], 4 );
                  }
               }
            }

            return o;
         }

         case 0x21: // Comment Extension.
         {
            int len;
            int ext = stbi__get8(s);
            if (ext == 0xF9) { // Graphic Control Extension.
               len = stbi__get8(s);
               if (len == 4) {
                  g->eflags = stbi__get8(s);
                  g->delay = 10 * stbi__get16le(s); // delay - 1/100th of a second, saving as 1/1000ths.

                  // unset old transparent
                  if (g->transparent >= 0) {
                     g->pal[g->transparent][3] = 255;
                  }
                  if (g->eflags & 0x01) {
                     g->transparent = stbi__get8(s);
                     if (g->transparent >= 0) {
                        g->pal[g->transparent][3] = 0;
                     }
                  } else {
                     // don't need transparent
                     stbi__skip(s, 1);
                     g->transparent = -1;
                  }
               } else {
                  stbi__skip(s, len);
                  break;
               }
            }
            while ((len = stbi__get8(s)) != 0) {
               stbi__skip(s, len);
            }
            break;
         }

         case 0x3B: // gif stream termination code
            return (stbi_uc *) s; // using '1' causes warning on some compilers

         default:
            return stbi__errpuc("unknown code", "Corrupt GIF");
      }
   }
}

static void *stbi__load_gif_main_outofmem(stbi__gif *g, stbi_uc *out, int **delays)
{
   STBI_FREE(g->out);
   STBI_FREE(g->history);
   STBI_FREE(g->background);

   if (out) STBI_FREE(out);
   if (delays && *delays) STBI_FREE(*delays);
   return stbi__errpuc("outofmem", "Out of memory");
}

static void *stbi__load_gif_main(stbi__context *s, int **delays, int *x, int *y, int *z, int *comp, int req_comp)
{
   if (stbi__gif_test(s)) {
      int layers = 0;
      stbi_uc *u = 0;
      stbi_uc *out = 0;
      stbi_uc *two_back = 0;
      stbi__gif g;
      int stride;
      int out_size = 0;
      int delays_size = 0;

      STBI_NOTUSED(out_size);
      STBI_NOTUSED(delays_size);

      memset(&g, 0, sizeof(g));
      if (delays) {
         *delays = 0;
      }

      do {
         u = stbi__gif_load_next(s, &g, comp, req_comp, two_back);
         if (u == (stbi_uc *) s) u = 0;  // end of animated gif marker

         if (u) {
            *x = g.w;
            *y = g.h;
            ++layers;
            stride = g.w * g.h * 4;

            if (out) {
               void *tmp = (stbi_uc*) STBI_REALLOC_SIZED( out, out_size, layers * stride );
               if (!tmp)
                  return stbi__load_gif_main_outofmem(&g, out, delays);
               else {
                   out = (stbi_uc*) tmp;
                   out_size = layers * stride;
               }

               if (delays) {
                  int *new_delays = (int*) STBI_REALLOC_SIZED( *delays, delays_size, sizeof(int) * layers );
                  if (!new_delays)
                     return stbi__load_gif_main_outofmem(&g, out, delays);
                  *delays = new_delays;
                  delays_size = layers * sizeof(int);
               }
            } else {
               out = (stbi_uc*)stbi__malloc( layers * stride );
               if (!out)
                  return stbi__load_gif_main_outofmem(&g, out, delays);
               out_size = layers * stride;
               if (delays) {
                  *delays = (int*) stbi__malloc( layers * sizeof(int) );
                  if (!*delays)
                     return stbi__load_gif_main_outofmem(&g, out, delays);
                  delays_size = layers * sizeof(int);
               }
            }
            memcpy( out + ((layers - 1) * stride), u, stride );
            if (layers >= 2) {
               two_back = out - 2 * stride;
            }

            if (delays) {
               (*delays)[layers - 1U] = g.delay;
            }
         }
      } while (u != 0);

      // free temp buffer;
      STBI_FREE(g.out);
      STBI_FREE(g.history);
      STBI_FREE(g.background);

      // do the final conversion after loading everything;
      if (req_comp && req_comp != 4)
         out = stbi__convert_format(out, 4, req_comp, layers * g.w, g.h);

      *z = layers;
      return out;
   } else {
      return stbi__errpuc("not GIF", "Image was not as a gif type.");
   }
}

static void *stbi__gif_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri)
{
   stbi_uc *u = 0;
   stbi__gif g;
   memset(&g, 0, sizeof(g));
   STBI_NOTUSED(ri);

   u = stbi__gif_load_next(s, &g, comp, req_comp, 0);
   if (u == (stbi_uc *) s) u = 0;  // end of animated gif marker
   if (u) {
      *x = g.w;
      *y = g.h;

      // moved conversion to after successful load so that the same
      // can be done for multiple frames.
      if (req_comp && req_comp != 4)
         u = stbi__convert_format(u, 4, req_comp, g.w, g.h);
   } else if (g.out) {
      // if there was an error and we allocated an image buffer, free it!
      STBI_FREE(g.out);
   }

   // free buffers needed for multiple frame loading;
   STBI_FREE(g.history);
   STBI_FREE(g.background);

   return u;
}

static int stbi__gif_info(stbi__context *s, int *x, int *y, int *comp)
{
   return stbi__gif_info_raw(s,x,y,comp);
}
#endif

// *************************************************************************************************
// Radiance RGBE HDR loader
// originally by Nicolas Schulz
#ifndef STBI_NO_HDR
static int stbi__hdr_test_core(stbi__context *s, const char *signature)
{
   int i;
   for (i=0; signature[i]; ++i)
      if (stbi__get8(s) != signature[i])
          return 0;
   stbi__rewind(s);
   return 1;
}

static int stbi__hdr_test(stbi__context* s)
{
   int r = stbi__hdr_test_core(s, "#?RADIANCE\n");
   stbi__rewind(s);
   if(!r) {
       r = stbi__hdr_test_core(s, "#?RGBE\n");
       stbi__rewind(s);
   }
   return r;
}

#define STBI__HDR_BUFLEN  1024
static char *stbi__hdr_gettoken(stbi__context *z, char *buffer)
{
   int len=0;
   char c = '\0';

   c = (char) stbi__get8(z);

   while (!stbi__at_eof(z) && c != '\n') {
      buffer[len++] = c;
      if (len == STBI__HDR_BUFLEN-1) {
         // flush to end of line
         while (!stbi__at_eof(z) && stbi__get8(z) != '\n')
            ;
         break;
      }
      c = (char) stbi__get8(z);
   }

   buffer[len] = 0;
   return buffer;
}

static void stbi__hdr_convert(float *output, stbi_uc *input, int req_comp)
{
   if ( input[3] != 0 ) {
      float f1;
      // Exponent
      f1 = (float) ldexp(1.0f, input[3] - (int)(128 + 8));
      if (req_comp <= 2)
         output[0] = (input[0] + input[1] + input[2]) * f1 / 3;
      else {
         output[0] = input[0] * f1;
         output[1] = input[1] * f1;
         output[2] = input[2] * f1;
      }
      if (req_comp == 2) output[1] = 1;
      if (req_comp == 4) output[3] = 1;
   } else {
      switch (req_comp) {
         case 4: output[3] = 1; /* fallthrough */
         case 3: output[0] = output[1] = output[2] = 0;
                 break;
         case 2: output[1] = 1; /* fallthrough */
         case 1: output[0] = 0;
                 break;
      }
   }
}

static float *stbi__hdr_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri)
{
   char buffer[STBI__HDR_BUFLEN];
   char *token;
   int valid = 0;
   int width, height;
   stbi_uc *scanline;
   float *hdr_data;
   int len;
   unsigned char count, value;
   int i, j, k, c1,c2, z;
   const char *headerToken;
   STBI_NOTUSED(ri);

   // Check identifier
   headerToken = stbi__hdr_gettoken(s,buffer);
   if (strcmp(headerToken, "#?RADIANCE") != 0 && strcmp(headerToken, "#?RGBE") != 0)
      return stbi__errpf("not HDR", "Corrupt HDR image");

   // Parse header
   for(;;) {
      token = stbi__hdr_gettoken(s,buffer);
      if (token[0] == 0) break;
      if (strcmp(token, "FORMAT=32-bit_rle_rgbe") == 0) valid = 1;
   }

   if (!valid)    return stbi__errpf("unsupported format", "Unsupported HDR format");

   // Parse width and height
   // can't use sscanf() if we're not using stdio!
   token = stbi__hdr_gettoken(s,buffer);
   if (strncmp(token, "-Y ", 3))  return stbi__errpf("unsupported data layout", "Unsupported HDR format");
   token += 3;
   height = (int) strtol(token, &token, 10);
   while (*token == ' ') ++token;
   if (strncmp(token, "+X ", 3))  return stbi__errpf("unsupported data layout", "Unsupported HDR format");
   token += 3;
   width = (int) strtol(token, NULL, 10);

   if (height > STBI_MAX_DIMENSIONS) return stbi__errpf("too large","Very large image (corrupt?)");
   if (width > STBI_MAX_DIMENSIONS) return stbi__errpf("too large","Very large image (corrupt?)");

   *x = width;
   *y = height;

   if (comp) *comp = 3;
   if (req_comp == 0) req_comp = 3;

   if (!stbi__mad4sizes_valid(width, height, req_comp, sizeof(float), 0))
      return stbi__errpf("too large", "HDR image is too large");

   // Read data
   hdr_data = (float *) stbi__malloc_mad4(width, height, req_comp, sizeof(float), 0);
   if (!hdr_data)
      return stbi__errpf("outofmem", "Out of memory");

   // Load image data
   // image data is stored as some number of sca
   if ( width < 8 || width >= 32768) {
      // Read flat data
      for (j=0; j < height; ++j) {
         for (i=0; i < width; ++i) {
            stbi_uc rgbe[4];
           main_decode_loop:
            stbi__getn(s, rgbe, 4);
            stbi__hdr_convert(hdr_data + j * width * req_comp + i * req_comp, rgbe, req_comp);
         }
      }
   } else {
      // Read RLE-encoded data
      scanline = NULL;

      for (j = 0; j < height; ++j) {
         c1 = stbi__get8(s);
         c2 = stbi__get8(s);
         len = stbi__get8(s);
         if (c1 != 2 || c2 != 2 || (len & 0x80)) {
            // not run-length encoded, so we have to actually use THIS data as a decoded
            // pixel (note this can't be a valid pixel--one of RGB must be >= 128)
            stbi_uc rgbe[4];
            rgbe[0] = (stbi_uc) c1;
            rgbe[1] = (stbi_uc) c2;
            rgbe[2] = (stbi_uc) len;
            rgbe[3] = (stbi_uc) stbi__get8(s);
            stbi__hdr_convert(hdr_data, rgbe, req_comp);
            i = 1;
            j = 0;
            STBI_FREE(scanline);
            goto main_decode_loop; // yes, this makes no sense
         }
         len <<= 8;
         len |= stbi__get8(s);
         if (len != width) { STBI_FREE(hdr_data); STBI_FREE(scanline); return stbi__errpf("invalid decoded scanline length", "corrupt HDR"); }
         if (scanline == NULL) {
            scanline = (stbi_uc *) stbi__malloc_mad2(width, 4, 0);
            if (!scanline) {
               STBI_FREE(hdr_data);
               return stbi__errpf("outofmem", "Out of memory");
            }
         }

         for (k = 0; k < 4; ++k) {
            int nleft;
            i = 0;
            while ((nleft = width - i) > 0) {
               count = stbi__get8(s);
               if (count > 128) {
                  // Run
                  value = stbi__get8(s);
                  count -= 128;
                  if (count > nleft) { STBI_FREE(hdr_data); STBI_FREE(scanline); return stbi__errpf("corrupt", "bad RLE data in HDR"); }
                  for (z = 0; z < count; ++z)
                     scanline[i++ * 4 + k] = value;
               } else {
                  // Dump
                  if (count > nleft) { STBI_FREE(hdr_data); STBI_FREE(scanline); return stbi__errpf("corrupt", "bad RLE data in HDR"); }
                  for (z = 0; z < count; ++z)
                     scanline[i++ * 4 + k] = stbi__get8(s);
               }
            }
         }
         for (i=0; i < width; ++i)
            stbi__hdr_convert(hdr_data+(j*width + i)*req_comp, scanline + i*4, req_comp);
      }
      if (scanline)
         STBI_FREE(scanline);
   }

   return hdr_data;
}

static int stbi__hdr_info(stbi__context *s, int *x, int *y, int *comp)
{
   char buffer[STBI__HDR_BUFLEN];
   char *token;
   int valid = 0;
   int dummy;

   if (!x) x = &dummy;
   if (!y) y = &dummy;
   if (!comp) comp = &dummy;

   if (stbi__hdr_test(s) == 0) {
       stbi__rewind( s );
       return 0;
   }

   for(;;) {
      token = stbi__hdr_gettoken(s,buffer);
      if (token[0] == 0) break;
      if (strcmp(token, "FORMAT=32-bit_rle_rgbe") == 0) valid = 1;
   }

   if (!valid) {
       stbi__rewind( s );
       return 0;
   }
   token = stbi__hdr_gettoken(s,buffer);
   if (strncmp(token, "-Y ", 3)) {
       stbi__rewind( s );
       return 0;
   }
   token += 3;
   *y = (int) strtol(token, &token, 10);
   while (*token == ' ') ++token;
   if (strncmp(token, "+X ", 3)) {
       stbi__rewind( s );
       return 0;
   }
   token += 3;
   *x = (int) strtol(token, NULL, 10);
   *comp = 3;
   return 1;
}
#endif // STBI_NO_HDR

#ifndef STBI_NO_BMP
static int stbi__bmp_info(stbi__context *s, int *x, int *y, int *comp)
{
   void *p;
   stbi__bmp_data info;

   info.all_a = 255;
   p = stbi__bmp_parse_header(s, &info);
   if (p == NULL) {
      stbi__rewind( s );
      return 0;
   }
   if (x) *x = s->img_x;
   if (y) *y = s->img_y;
   if (comp) {
      if (info.bpp == 24 && info.ma == 0xff000000)
         *comp = 3;
      else
         *comp = info.ma ? 4 : 3;
   }
   return 1;
}
#endif

#ifndef STBI_NO_PSD
static int stbi__psd_info(stbi__context *s, int *x, int *y, int *comp)
{
   int channelCount, dummy, depth;
   if (!x) x = &dummy;
   if (!y) y = &dummy;
   if (!comp) comp = &dummy;
   if (stbi__get32be(s) != 0x38425053) {
       stbi__rewind( s );
       return 0;
   }
   if (stbi__get16be(s) != 1) {
       stbi__rewind( s );
       return 0;
   }
   stbi__skip(s, 6);
   channelCount = stbi__get16be(s);
   if (channelCount < 0 || channelCount > 16) {
       stbi__rewind( s );
       return 0;
   }
   *y = stbi__get32be(s);
   *x = stbi__get32be(s);
   depth = stbi__get16be(s);
   if (depth != 8 && depth != 16) {
       stbi__rewind( s );
       return 0;
   }
   if (stbi__get16be(s) != 3) {
       stbi__rewind( s );
       return 0;
   }
   *comp = 4;
   return 1;
}

static int stbi__psd_is16(stbi__context *s)
{
   int channelCount, depth;
   if (stbi__get32be(s) != 0x38425053) {
       stbi__rewind( s );
       return 0;
   }
   if (stbi__get16be(s) != 1) {
       stbi__rewind( s );
       return 0;
   }
   stbi__skip(s, 6);
   channelCount = stbi__get16be(s);
   if (channelCount < 0 || channelCount > 16) {
       stbi__rewind( s );
       return 0;
   }
   STBI_NOTUSED(stbi__get32be(s));
   STBI_NOTUSED(stbi__get32be(s));
   depth = stbi__get16be(s);
   if (depth != 16) {
       stbi__rewind( s );
       return 0;
   }
   return 1;
}
#endif

#ifndef STBI_NO_PIC
static int stbi__pic_info(stbi__context *s, int *x, int *y, int *comp)
{
   int act_comp=0,num_packets=0,chained,dummy;
   stbi__pic_packet packets[10];

   if (!x) x = &dummy;
   if (!y) y = &dummy;
   if (!comp) comp = &dummy;

   if (!stbi__pic_is4(s,"\x53\x80\xF6\x34")) {
      stbi__rewind(s);
      return 0;
   }

   stbi__skip(s, 88);

   *x = stbi__get16be(s);
   *y = stbi__get16be(s);
   if (stbi__at_eof(s)) {
      stbi__rewind( s);
      return 0;
   }
   if ( (*x) != 0 && (1 << 28) / (*x) < (*y)) {
      stbi__rewind( s );
      return 0;
   }

   stbi__skip(s, 8);

   do {
      stbi__pic_packet *packet;

      if (num_packets==sizeof(packets)/sizeof(packets[0]))
         return 0;

      packet = &packets[num_packets++];
      chained = stbi__get8(s);
      packet->size    = stbi__get8(s);
      packet->type    = stbi__get8(s);
      packet->channel = stbi__get8(s);
      act_comp |= packet->channel;

      if (stbi__at_eof(s)) {
          stbi__rewind( s );
          return 0;
      }
      if (packet->size != 8) {
          stbi__rewind( s );
          return 0;
      }
   } while (chained);

   *comp = (act_comp & 0x10 ? 4 : 3);

   return 1;
}
#endif

// *************************************************************************************************
// Portable Gray Map and Portable Pixel Map loader
// by Ken Miller
//
// PGM: http://netpbm.sourceforge.net/doc/pgm.html
// PPM: http://netpbm.sourceforge.net/doc/ppm.html
//
// Known limitations:
//    Does not support comments in the header section
//    Does not support ASCII image data (formats P2 and P3)

#ifndef STBI_NO_PNM

static int      stbi__pnm_test(stbi__context *s)
{
   char p, t;
   p = (char) stbi__get8(s);
   t = (char) stbi__get8(s);
   if (p != 'P' || (t != '5' && t != '6')) {
       stbi__rewind( s );
       return 0;
   }
   return 1;
}

static void *stbi__pnm_load(stbi__context *s, int *x, int *y, int *comp, int req_comp, stbi__result_info *ri)
{
   stbi_uc *out;
   STBI_NOTUSED(ri);

   ri->bits_per_channel = stbi__pnm_info(s, (int *)&s->img_x, (int *)&s->img_y, (int *)&s->img_n);
   if (ri->bits_per_channel == 0)
      return 0;

   if (s->img_y > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");
   if (s->img_x > STBI_MAX_DIMENSIONS) return stbi__errpuc("too large","Very large image (corrupt?)");

   *x = s->img_x;
   *y = s->img_y;
   if (comp) *comp = s->img_n;

   if (!stbi__mad4sizes_valid(s->img_n, s->img_x, s->img_y, ri->bits_per_channel / 8, 0))
      return stbi__errpuc("too large", "PNM too large");

   out = (stbi_uc *) stbi__malloc_mad4(s->img_n, s->img_x, s->img_y, ri->bits_per_channel / 8, 0);
   if (!out) return stbi__errpuc("outofmem", "Out of memory");
   stbi__getn(s, out, s->img_n * s->img_x * s->img_y * (ri->bits_per_channel / 8));

   if (req_comp && req_comp != s->img_n) {
      out = stbi__convert_format(out, s->img_n, req_comp, s->img_x, s->img_y);
      if (out == NULL) return out; // stbi__convert_format frees input on failure
   }
   return out;
}

static int      stbi__pnm_isspace(char c)
{
   return c == ' ' || c == '\t' || c == '\n' || c == '\v' || c == '\f' || c == '\r';
}

static void     stbi__pnm_skip_whitespace(stbi__context *s, char *c)
{
   for (;;) {
      while (!stbi__at_eof(s) && stbi__pnm_isspace(*c))
         *c = (char) stbi__get8(s);

      if (stbi__at_eof(s) || *c != '#')
         break;

      while (!stbi__at_eof(s) && *c != '\n' && *c != '\r' )
         *c = (char) stbi__get8(s);
   }
}

static int      stbi__pnm_isdigit(char c)
{
   return c >= '0' && c <= '9';
}

static int      stbi__pnm_getinteger(stbi__context *s, char *c)
{
   int value = 0;

   while (!stbi__at_eof(s) && stbi__pnm_isdigit(*c)) {
      value = value*10 + (*c - '0');
      *c = (char) stbi__get8(s);
   }

   return value;
}

static int      stbi__pnm_info(stbi__context *s, int *x, int *y, int *comp)
{
   int maxv, dummy;
   char c, p, t;

   if (!x) x = &dummy;
   if (!y) y = &dummy;
   if (!comp) comp = &dummy;

   stbi__rewind(s);

   // Get identifier
   p = (char) stbi__get8(s);
   t = (char) stbi__get8(s);
   if (p != 'P' || (t != '5' && t != '6')) {
       stbi__rewind(s);
       return 0;
   }

   *comp = (t == '6') ? 3 : 1;  // '5' is 1-component .pgm; '6' is 3-component .ppm

   c = (char) stbi__get8(s);
   stbi__pnm_skip_whitespace(s, &c);

   *x = stbi__pnm_getinteger(s, &c); // read width
   stbi__pnm_skip_whitespace(s, &c);

   *y = stbi__pnm_getinteger(s, &c); // read height
   stbi__pnm_skip_whitespace(s, &c);

   maxv = stbi__pnm_getinteger(s, &c);  // read max value
   if (maxv > 65535)
      return stbi__err("max value > 65535", "PPM image supports only 8-bit and 16-bit images");
   else if (maxv > 255)
      return 16;
   else
      return 8;
}

static int stbi__pnm_is16(stbi__context *s)
{
   if (stbi__pnm_info(s, NULL, NULL, NULL) == 16)
	   return 1;
   return 0;
}
#endif

static int stbi__info_main(stbi__context *s, int *x, int *y, int *comp)
{
   #ifndef STBI_NO_JPEG
   if (stbi__jpeg_info(s, x, y, comp)) return 1;
   #endif

   #ifndef STBI_NO_PNG
   if (stbi__png_info(s, x, y, comp))  return 1;
   #endif

   #ifndef STBI_NO_GIF
   if (stbi__gif_info(s, x, y, comp))  return 1;
   #endif

   #ifndef STBI_NO_BMP
   if (stbi__bmp_info(s, x, y, comp))  return 1;
   #endif

   #ifndef STBI_NO_PSD
   if (stbi__psd_info(s, x, y, comp))  return 1;
   #endif

   #ifndef STBI_NO_PIC
   if (stbi__pic_info(s, x, y, comp))  return 1;
   #endif

   #ifndef STBI_NO_PNM
   if (stbi__pnm_info(s, x, y, comp))  return 1;
   #endif

   #ifndef STBI_NO_HDR
   if (stbi__hdr_info(s, x, y, comp))  return 1;
   #endif

   // test tga last because it's a crappy test!
   #ifndef STBI_NO_TGA
   if (stbi__tga_info(s, x, y, comp))
       return 1;
   #endif
   return stbi__err("unknown image type", "Image not of any known type, or corrupt");
}

static int stbi__is_16_main(stbi__context *s)
{
   #ifndef STBI_NO_PNG
   if (stbi__png_is16(s))  return 1;
   #endif

   #ifndef STBI_NO_PSD
   if (stbi__psd_is16(s))  return 1;
   #endif

   #ifndef STBI_NO_PNM
   if (stbi__pnm_is16(s))  return 1;
   #endif
   return 0;
}

#ifndef STBI_NO_STDIO
STBIDEF int stbi_info(char const *filename, int *x, int *y, int *comp)
{
    FILE *f = stbi__fopen(filename, "rb");
    int result;
    if (!f) return stbi__err("can't fopen", "Unable to open file");
    result = stbi_info_from_file(f, x, y, comp);
    fclose(f);
    return result;
}

STBIDEF int stbi_info_from_file(FILE *f, int *x, int *y, int *comp)
{
   int r;
   stbi__context s;
   long pos = ftell(f);
   stbi__start_file(&s, f);
   r = stbi__info_main(&s,x,y,comp);
   fseek(f,pos,SEEK_SET);
   return r;
}

STBIDEF int stbi_is_16_bit(char const *filename)
{
    FILE *f = stbi__fopen(filename, "rb");
    int result;
    if (!f) return stbi__err("can't fopen", "Unable to open file");
    result = stbi_is_16_bit_from_file(f);
    fclose(f);
    return result;
}

STBIDEF int stbi_is_16_bit_from_file(FILE *f)
{
   int r;
   stbi__context s;
   long pos = ftell(f);
   stbi__start_file(&s, f);
   r = stbi__is_16_main(&s);
   fseek(f,pos,SEEK_SET);
   return r;
}
#endif // !STBI_NO_STDIO

STBIDEF int stbi_info_from_memory(stbi_uc const *buffer, int len, int *x, int *y, int *comp)
{
   stbi__context s;
   stbi__start_mem(&s,buffer,len);
   return stbi__info_main(&s,x,y,comp);
}

STBIDEF int stbi_info_from_callbacks(stbi_io_callbacks const *c, void *user, int *x, int *y, int *comp)
{
   stbi__context s;
   stbi__start_callbacks(&s, (stbi_io_callbacks *) c, user);
   return stbi__info_main(&s,x,y,comp);
}

STBIDEF int stbi_is_16_bit_from_memory(stbi_uc const *buffer, int len)
{
   stbi__context s;
   stbi__start_mem(&s,buffer,len);
   return stbi__is_16_main(&s);
}

STBIDEF int stbi_is_16_bit_from_callbacks(stbi_io_callbacks const *c, void *user)
{
   stbi__context s;
   stbi__start_callbacks(&s, (stbi_io_callbacks *) c, user);
   return stbi__is_16_main(&s);
}

#endif // STB_IMAGE_IMPLEMENTATION

/*
   revision history:
      2.20  (2019-02-07) support utf8 filenames in Windows; fix warnings and platform ifdefs
      2.19  (2018-02-11) fix warning
      2.18  (2018-01-30) fix warnings
      2.17  (2018-01-29) change sbti__shiftsigned to avoid clang -O2 bug
                         1-bit BMP
                         *_is_16_bit api
                         avoid warnings
      2.16  (2017-07-23) all functions have 16-bit variants;
                         STBI_NO_STDIO works again;
                         compilation fixes;
                         fix rounding in unpremultiply;
                         optimize vertical flip;
                         disable raw_len validation;
                         documentation fixes
      2.15  (2017-03-18) fix png-1,2,4 bug; now all Imagenet JPGs decode;
                         warning fixes; disable run-time SSE detection on gcc;
                         uniform handling of optional "return" values;
                         thread-safe initialization of zlib tables
      2.14  (2017-03-03) remove deprecated STBI_JPEG_OLD; fixes for Imagenet JPGs
      2.13  (2016-11-29) add 16-bit API, only supported for PNG right now
      2.12  (2016-04-02) fix typo in 2.11 PSD fix that caused crashes
      2.11  (2016-04-02) allocate large structures on the stack
                         remove white matting for transparent PSD
                         fix reported channel count for PNG & BMP
                         re-enable SSE2 in non-gcc 64-bit
                         support RGB-formatted JPEG
                         read 16-bit PNGs (only as 8-bit)
      2.10  (2016-01-22) avoid warning introduced in 2.09 by STBI_REALLOC_SIZED
      2.09  (2016-01-16) allow comments in PNM files
                         16-bit-per-pixel TGA (not bit-per-component)
                         info() for TGA could break due to .hdr handling
                         info() for BMP to shares code instead of sloppy parse
                         can use STBI_REALLOC_SIZED if allocator doesn't support realloc
                         code cleanup
      2.08  (2015-09-13) fix to 2.07 cleanup, reading RGB PSD as RGBA
      2.07  (2015-09-13) fix compiler warnings
                         partial animated GIF support
                         limited 16-bpc PSD support
                         #ifdef unused functions
                         bug with < 92 byte PIC,PNM,HDR,TGA
      2.06  (2015-04-19) fix bug where PSD returns wrong '*comp' value
      2.05  (2015-04-19) fix bug in progressive JPEG handling, fix warning
      2.04  (2015-04-15) try to re-enable SIMD on MinGW 64-bit
      2.03  (2015-04-12) extra corruption checking (mmozeiko)
                         stbi_set_flip_vertically_on_load (nguillemot)
                         fix NEON support; fix mingw support
      2.02  (2015-01-19) fix incorrect assert, fix warning
      2.01  (2015-01-17) fix various warnings; suppress SIMD on gcc 32-bit without -msse2
      2.00b (2014-12-25) fix STBI_MALLOC in progressive JPEG
      2.00  (2014-12-25) optimize JPG, including x86 SSE2 & NEON SIMD (ryg)
                         progressive JPEG (stb)
                         PGM/PPM support (Ken Miller)
                         STBI_MALLOC,STBI_REALLOC,STBI_FREE
                         GIF bugfix -- seemingly never worked
                         STBI_NO_*, STBI_ONLY_*
      1.48  (2014-12-14) fix incorrectly-named assert()
      1.47  (2014-12-14) 1/2/4-bit PNG support, both direct and paletted (Omar Cornut & stb)
                         optimize PNG (ryg)
                         fix bug in interlaced PNG with user-specified channel count (stb)
      1.46  (2014-08-26)
              fix broken tRNS chunk (colorkey-style transparency) in non-paletted PNG
      1.45  (2014-08-16)
              fix MSVC-ARM internal compiler error by wrapping malloc
      1.44  (2014-08-07)
              various warning fixes from Ronny Chevalier
      1.43  (2014-07-15)
              fix MSVC-only compiler problem in code changed in 1.42
      1.42  (2014-07-09)
              don't define _CRT_SECURE_NO_WARNINGS (affects user code)
              fixes to stbi__cleanup_jpeg path
              added STBI_ASSERT to avoid requiring assert.h
      1.41  (2014-06-25)
              fix search&replace from 1.36 that messed up comments/error messages
      1.40  (2014-06-22)
              fix gcc struct-initialization warning
      1.39  (2014-06-15)
              fix to TGA optimization when req_comp != number of components in TGA;
              fix to GIF loading because BMP wasn't rewinding (whoops, no GIFs in my test suite)
              add support for BMP version 5 (more ignored fields)
      1.38  (2014-06-06)
              suppress MSVC warnings on integer casts truncating values
              fix accidental rename of 'skip' field of I/O
      1.37  (2014-06-04)
              remove duplicate typedef
      1.36  (2014-06-03)
              convert to header file single-file library
              if de-iphone isn't set, load iphone images color-swapped instead of returning NULL
      1.35  (2014-05-27)
              various warnings
              fix broken STBI_SIMD path
              fix bug where stbi_load_from_file no longer left file pointer in correct place
              fix broken non-easy path for 32-bit BMP (possibly never used)
              TGA optimization by Arseny Kapoulkine
      1.34  (unknown)
              use STBI_NOTUSED in stbi__resample_row_generic(), fix one more leak in tga failure case
      1.33  (2011-07-14)
              make stbi_is_hdr work in STBI_NO_HDR (as specified), minor compiler-friendly improvements
      1.32  (2011-07-13)
              support for "info" function for all supported filetypes (SpartanJ)
      1.31  (2011-06-20)
              a few more leak fixes, bug in PNG handling (SpartanJ)
      1.30  (2011-06-11)
              added ability to load files via callbacks to accomidate custom input streams (Ben Wenger)
              removed deprecated format-specific test/load functions
              removed support for installable file formats (stbi_loader) -- would have been broken for IO callbacks anyway
              error cases in bmp and tga give messages and don't leak (Raymond Barbiero, grisha)
              fix inefficiency in decoding 32-bit BMP (David Woo)
      1.29  (2010-08-16)
              various warning fixes from Aurelien Pocheville
      1.28  (2010-08-01)
              fix bug in GIF palette transparency (SpartanJ)
      1.27  (2010-08-01)
              cast-to-stbi_uc to fix warnings
      1.26  (2010-07-24)
              fix bug in file buffering for PNG reported by SpartanJ
      1.25  (2010-07-17)
              refix trans_data warning (Won Chun)
      1.24  (2010-07-12)
              perf improvements reading from files on platforms with lock-heavy fgetc()
              minor perf improvements for jpeg
              deprecated type-specific functions so we'll get feedback if they're needed
              attempt to fix trans_data warning (Won Chun)
      1.23    fixed bug in iPhone support
      1.22  (2010-07-10)
              removed image *writing* support
              stbi_info support from Jetro Lauha
              GIF support from Jean-Marc Lienher
              iPhone PNG-extensions from James Brown
              warning-fixes from Nicolas Schulz and Janez Zemva (i.stbi__err. Janez (U+017D)emva)
      1.21    fix use of 'stbi_uc' in header (reported by jon blow)
      1.20    added support for Softimage PIC, by Tom Seddon
      1.19    bug in interlaced PNG corruption check (found by ryg)
      1.18  (2008-08-02)
              fix a threading bug (local mutable static)
      1.17    support interlaced PNG
      1.16    major bugfix - stbi__convert_format converted one too many pixels
      1.15    initialize some fields for thread safety
      1.14    fix threadsafe conversion bug
              header-file-only version (#define STBI_HEADER_FILE_ONLY before including)
      1.13    threadsafe
      1.12    const qualifiers in the API
      1.11    Support installable IDCT, colorspace conversion routines
      1.10    Fixes for 64-bit (don't use "unsigned long")
              optimized upsampling by Fabian "ryg" Giesen
      1.09    Fix format-conversion for PSD code (bad global variables!)
      1.08    Thatcher Ulrich's PSD code integrated by Nicolas Schulz
      1.07    attempt to fix C++ warning/errors again
      1.06    attempt to fix C++ warning/errors again
      1.05    fix TGA loading to return correct *comp and use good luminance calc
      1.04    default float alpha is 1, not 255; use 'void *' for stbi_image_free
      1.03    bugfixes to STBI_NO_STDIO, STBI_NO_HDR
      1.02    support for (subset of) HDR files, float interface for preferred access to them
      1.01    fix bug: possible bug in handling right-side up bmps... not sure
              fix bug: the stbi__bmp_load() and stbi__tga_load() functions didn't work at all
      1.00    interface to zlib that skips zlib header
      0.99    correct handling of alpha in palette
      0.98    TGA loader by lonesock; dynamically add loaders (untested)
      0.97    jpeg errors on too large a file; also catch another malloc failure
      0.96    fix detection of invalid v value - particleman@mollyrocket forum
      0.95    during header scan, seek to markers in case of padding
      0.94    STBI_NO_STDIO to disable stdio usage; rename all #defines the same
      0.93    handle jpegtran output; verbose errors
      0.92    read 4,8,16,24,32-bit BMP files of several formats
      0.91    output 24-bit Windows 3.0 BMP files
      0.90    fix a few more warnings; bump version number to approach 1.0
      0.61    bugfixes due to Marc LeBlanc, Christopher Lloyd
      0.60    fix compiling as c++
      0.59    fix warnings: merge Dave Moore's -Wall fixes
      0.58    fix bug: zlib uncompressed mode len/nlen was wrong endian
      0.57    fix bug: jpg last huffman symbol before marker was >9 bits but less than 16 available
      0.56    fix bug: zlib uncompressed mode len vs. nlen
      0.55    fix bug: restart_interval not initialized to 0
      0.54    allow NULL for 'int *comp'
      0.53    fix bug in png 3->4; speedup png decoding
      0.52    png handles req_comp=3,4 directly; minor cleanup; jpeg comments
      0.51    obey req_comp requests, 1-component jpegs return as 1-component,
              on 'test' only check type, not whether we support this variant
      0.50  (2006-11-19)
              first released version
*/


/*
------------------------------------------------------------------------------
This software is available under 2 licenses -- choose whichever you prefer.
------------------------------------------------------------------------------
ALTERNATIVE A - MIT License
Copyright (c) 2017 Sean Barrett
Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
------------------------------------------------------------------------------
ALTERNATIVE B - Public Domain (www.unlicense.org)
This is free and unencumbered software released into the public domain.
Anyone is free to copy, modify, publish, use, compile, sell, or distribute this
software, either in source code form or as a compiled binary, for any purpose,
commercial or non-commercial, and by any means.
In jurisdictions that recognize copyright laws, the author or authors of this
software dedicate any and all copyright interest in the software to the public
domain. We make this dedication for the benefit of the public at large and to
the detriment of our heirs and successors. We intend this dedication to be an
overt act of relinquishment in perpetuity of all present and future rights to
this software under copyright law.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
------------------------------------------------------------------------------
*/

```

`cpp/vulkan_gui/vulkan_inference_gui.cc`:

```cc
// Copyright 2019 The IREE Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

// Vulkan Graphics + IREE API Integration Sample.

#include <SDL.h>
#include <SDL_vulkan.h>
#include <imgui.h>
#include <imgui_impl_sdl.h>
#include <imgui_impl_vulkan.h>
#include <vulkan/vulkan.h>


#include <cstring>
#include <set>
#include <vector>
#include <fstream>
#include <array>
#include <cstdio>
#include <cstdlib>
#include <iterator>
#include <string>
#include <utility>

#include "iree/hal/drivers/vulkan/api.h"

// IREE's C API:
#include "iree/base/api.h"
#include "iree/hal/api.h"
#include "iree/hal/drivers/vulkan/registration/driver_module.h"
#include "iree/modules/hal/module.h"
#include "iree/vm/api.h"
#include "iree/vm/bytecode_module.h"
#include "iree/vm/ref_cc.h"

// iree-run-module
#include "iree/base/internal/flags.h"
#include "iree/base/status_cc.h"
#include "iree/base/tracing.h"
#include "iree/modules/hal/types.h"
#include "iree/tooling/comparison.h"
#include "iree/tooling/context_util.h"
#include "iree/tooling/vm_util_cc.h"

// Other dependencies (helpers, etc.)
#include "iree/base/internal/main.h"

#define IMGUI_UNLIMITED_FRAME_RATE

#define STB_IMAGE_IMPLEMENTATION
#include "stb_image.h"

IREE_FLAG(string, entry_function, "",
          "Name of a function contained in the module specified by module_file "
          "to run.");

// TODO(benvanik): move --function_input= flag into a util.
static iree_status_t parse_function_io(iree_string_view_t flag_name,
                                       void* storage,
                                       iree_string_view_t value) {
  auto* list = (std::vector<std::string>*)storage;
  list->push_back(std::string(value.data, value.size));
  return iree_ok_status();
}
static void print_function_io(iree_string_view_t flag_name, void* storage,
                              FILE* file) {
  auto* list = (std::vector<std::string>*)storage;
  if (list->empty()) {
    fprintf(file, "# --%.*s=\n", (int)flag_name.size, flag_name.data);
  } else {
    for (size_t i = 0; i < list->size(); ++i) {
      fprintf(file, "--%.*s=\"%s\"\n", (int)flag_name.size, flag_name.data,
              list->at(i).c_str());
    }
  }
}
static std::vector<std::string> FLAG_function_inputs;
IREE_FLAG_CALLBACK(
    parse_function_io, print_function_io, &FLAG_function_inputs, function_input,
    "An input (a) value or (b) buffer of the format:\n"
    "  (a) scalar value\n"
    "     value\n"
    "     e.g.: --function_input=\"3.14\"\n"
    "  (b) buffer:\n"
    "     [shape]xtype=[value]\n"
    "     e.g.: --function_input=\"2x2xi32=1 2 3 4\"\n"
    "Optionally, brackets may be used to separate the element values:\n"
    "  2x2xi32=[[1 2][3 4]]\n"
    "Raw binary files can be read to provide buffer contents:\n"
    "  2x2xi32=@some/file.bin\n"
    "numpy npy files (from numpy.save) can be read to provide 1+ values:\n"
    "  @some.npy\n"
    "Each occurrence of the flag indicates an input in the order they were\n"
    "specified on the command line.");

typedef struct iree_file_toc_t {
  const char* name;             // the file's original name
  char* data;             // beginning of the file
  size_t size;                  // length of the file
} iree_file_toc_t;

bool load_file(const char* filename, char** pOut, size_t* pSize)
{
    FILE* f = fopen(filename, "rb");
    if (f == NULL)
    {
        fprintf(stderr, "Can't open %s\n", filename);
        return false;
    }

    fseek(f, 0L, SEEK_END);
    *pSize = ftell(f);
    fseek(f, 0L, SEEK_SET);

    *pOut = (char*)malloc(*pSize);

    size_t size = fread(*pOut, *pSize, 1, f);

    fclose(f);

    return size != 0;
}

static VkAllocationCallbacks* g_Allocator = NULL;
static VkInstance g_Instance = VK_NULL_HANDLE;
static VkPhysicalDevice g_PhysicalDevice = VK_NULL_HANDLE;
static VkDevice g_Device = VK_NULL_HANDLE;
static uint32_t g_QueueFamily = (uint32_t)-1;
static VkQueue g_Queue = VK_NULL_HANDLE;
static VkPipelineCache g_PipelineCache = VK_NULL_HANDLE;
static VkDescriptorPool g_DescriptorPool = VK_NULL_HANDLE;

static ImGui_ImplVulkanH_Window g_MainWindowData;
static uint32_t g_MinImageCount = 2;
static bool g_SwapChainRebuild = false;
static int g_SwapChainResizeWidth = 0;
static int g_SwapChainResizeHeight = 0;

static void check_vk_result(VkResult err) {
  if (err == 0) return;
  fprintf(stderr, "VkResult: %d\n", err);
  abort();
}

// Returns the names of the Vulkan layers used for the given IREE
// |extensibility_set| and |features|.
std::vector<const char*> GetIreeLayers(
    iree_hal_vulkan_extensibility_set_t extensibility_set,
    iree_hal_vulkan_features_t features) {
  iree_host_size_t required_count;
  iree_hal_vulkan_query_extensibility_set(
      features, extensibility_set, /*string_capacity=*/0, &required_count,
      /*out_string_values=*/NULL);
  std::vector<const char*> layers(required_count);
  iree_hal_vulkan_query_extensibility_set(features, extensibility_set,
                                          layers.size(), &required_count,
                                          layers.data());
  return layers;
}

// Returns the names of the Vulkan extensions used for the given IREE
// |extensibility_set| and |features|.
std::vector<const char*> GetIreeExtensions(
    iree_hal_vulkan_extensibility_set_t extensibility_set,
    iree_hal_vulkan_features_t features) {
  iree_host_size_t required_count;
  iree_hal_vulkan_query_extensibility_set(
      features, extensibility_set, /*string_capacity=*/0, &required_count,
      /*out_string_values=*/NULL);
  std::vector<const char*> extensions(required_count);
  iree_hal_vulkan_query_extensibility_set(features, extensibility_set,
                                          extensions.size(), &required_count,
                                          extensions.data());
  return extensions;
}

// Returns the names of the Vulkan extensions used for the given IREE
// |vulkan_features|.
std::vector<const char*> GetDeviceExtensions(
    VkPhysicalDevice physical_device,
    iree_hal_vulkan_features_t vulkan_features) {
  std::vector<const char*> iree_required_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_DEVICE_EXTENSIONS_REQUIRED,
      vulkan_features);
  std::vector<const char*> iree_optional_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_DEVICE_EXTENSIONS_OPTIONAL,
      vulkan_features);

  uint32_t extension_count = 0;
  check_vk_result(vkEnumerateDeviceExtensionProperties(
      physical_device, nullptr, &extension_count, nullptr));
  std::vector<VkExtensionProperties> extension_properties(extension_count);
  check_vk_result(vkEnumerateDeviceExtensionProperties(
      physical_device, nullptr, &extension_count, extension_properties.data()));

  // Merge extensions lists, including optional and required for simplicity.
  std::set<const char*> ext_set;
  ext_set.insert("VK_KHR_swapchain");
  ext_set.insert(iree_required_extensions.begin(),
                 iree_required_extensions.end());
  for (int i = 0; i < iree_optional_extensions.size(); ++i) {
    const char* optional_extension = iree_optional_extensions[i];
    for (int j = 0; j < extension_count; ++j) {
      if (strcmp(optional_extension, extension_properties[j].extensionName) ==
          0) {
        ext_set.insert(optional_extension);
        break;
      }
    }
  }
  std::vector<const char*> extensions(ext_set.begin(), ext_set.end());
  return extensions;
}

std::vector<const char*> GetInstanceLayers(
    iree_hal_vulkan_features_t vulkan_features) {
  // Query the layers that IREE wants / needs.
  std::vector<const char*> required_layers = GetIreeLayers(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_LAYERS_REQUIRED, vulkan_features);
  std::vector<const char*> optional_layers = GetIreeLayers(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_LAYERS_OPTIONAL, vulkan_features);

  // Query the layers that are available on the Vulkan ICD.
  uint32_t layer_property_count = 0;
  check_vk_result(
      vkEnumerateInstanceLayerProperties(&layer_property_count, NULL));
  std::vector<VkLayerProperties> layer_properties(layer_property_count);
  check_vk_result(vkEnumerateInstanceLayerProperties(&layer_property_count,
                                                     layer_properties.data()));

  // Match between optional/required and available layers.
  std::vector<const char*> layers;
  for (const char* layer_name : required_layers) {
    bool found = false;
    for (const auto& layer_property : layer_properties) {
      if (std::strcmp(layer_name, layer_property.layerName) == 0) {
        found = true;
        layers.push_back(layer_name);
        break;
      }
    }
    if (!found) {
      fprintf(stderr, "Required layer %s not available\n", layer_name);
      abort();
    }
  }
  for (const char* layer_name : optional_layers) {
    for (const auto& layer_property : layer_properties) {
      if (std::strcmp(layer_name, layer_property.layerName) == 0) {
        layers.push_back(layer_name);
        break;
      }
    }
  }

  return layers;
}

std::vector<const char*> GetInstanceExtensions(
    SDL_Window* window, iree_hal_vulkan_features_t vulkan_features) {
  // Ask SDL for its list of required instance extensions.
  uint32_t sdl_extensions_count = 0;
  SDL_Vulkan_GetInstanceExtensions(window, &sdl_extensions_count, NULL);
  std::vector<const char*> sdl_extensions(sdl_extensions_count);
  SDL_Vulkan_GetInstanceExtensions(window, &sdl_extensions_count,
                                   sdl_extensions.data());

  std::vector<const char*> iree_required_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_EXTENSIONS_REQUIRED,
      vulkan_features);
  std::vector<const char*> iree_optional_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_EXTENSIONS_OPTIONAL,
      vulkan_features);

  // Merge extensions lists, including optional and required for simplicity.
  std::set<const char*> ext_set;
  ext_set.insert(sdl_extensions.begin(), sdl_extensions.end());
  ext_set.insert(iree_required_extensions.begin(),
                 iree_required_extensions.end());
  ext_set.insert(iree_optional_extensions.begin(),
                 iree_optional_extensions.end());
  std::vector<const char*> extensions(ext_set.begin(), ext_set.end());
  return extensions;
}

void SetupVulkan(iree_hal_vulkan_features_t vulkan_features,
                 const char** instance_layers, uint32_t instance_layers_count,
                 const char** instance_extensions,
                 uint32_t instance_extensions_count,
                 const VkAllocationCallbacks* allocator, VkInstance* instance,
                 uint32_t* queue_family_index,
                 VkPhysicalDevice* physical_device, VkQueue* queue,
                 VkDevice* device, VkDescriptorPool* descriptor_pool) {
  VkResult err;

  // Create Vulkan Instance
  {
    VkInstanceCreateInfo create_info = {};
    create_info.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO;
    create_info.enabledLayerCount = instance_layers_count;
    create_info.ppEnabledLayerNames = instance_layers;
    create_info.enabledExtensionCount = instance_extensions_count;
    create_info.ppEnabledExtensionNames = instance_extensions;
    err = vkCreateInstance(&create_info, allocator, instance);
    check_vk_result(err);
  }

  // Select GPU
  {
    uint32_t gpu_count;
    err = vkEnumeratePhysicalDevices(*instance, &gpu_count, NULL);
    check_vk_result(err);
    IM_ASSERT(gpu_count > 0);

    VkPhysicalDevice* gpus =
        (VkPhysicalDevice*)malloc(sizeof(VkPhysicalDevice) * gpu_count);
    err = vkEnumeratePhysicalDevices(*instance, &gpu_count, gpus);
    check_vk_result(err);

    // Use the first reported GPU for simplicity.
    *physical_device = gpus[0];

    VkPhysicalDeviceProperties properties;
    vkGetPhysicalDeviceProperties(*physical_device, &properties);
    fprintf(stdout, "Selected Vulkan device: '%s'\n", properties.deviceName);
    free(gpus);
  }

  // Select queue family. We want a single queue with graphics and compute for
  // simplicity, but we could also discover and use separate queues for each.
  {
    uint32_t count;
    vkGetPhysicalDeviceQueueFamilyProperties(*physical_device, &count, NULL);
    VkQueueFamilyProperties* queues = (VkQueueFamilyProperties*)malloc(
        sizeof(VkQueueFamilyProperties) * count);
    vkGetPhysicalDeviceQueueFamilyProperties(*physical_device, &count, queues);
    for (uint32_t i = 0; i < count; i++) {
      if (queues[i].queueFlags &
          (VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT)) {
        *queue_family_index = i;
        break;
      }
    }
    free(queues);
    IM_ASSERT(*queue_family_index != (uint32_t)-1);
  }

  // Create Logical Device (with 1 queue)
  {
    std::vector<const char*> device_extensions =
        GetDeviceExtensions(*physical_device, vulkan_features);
    const float queue_priority[] = {1.0f};
    VkDeviceQueueCreateInfo queue_info = {};
    queue_info.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
    queue_info.queueFamilyIndex = *queue_family_index;
    queue_info.queueCount = 1;
    queue_info.pQueuePriorities = queue_priority;
    VkDeviceCreateInfo create_info = {};
    create_info.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;
    create_info.queueCreateInfoCount = 1;
    create_info.pQueueCreateInfos = &queue_info;
    create_info.enabledExtensionCount =
        static_cast<uint32_t>(device_extensions.size());
    create_info.ppEnabledExtensionNames = device_extensions.data();

    // Enable timeline semaphores.
    VkPhysicalDeviceFeatures2 features2;
    memset(&features2, 0, sizeof(features2));
    features2.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2;
    create_info.pNext = &features2;
    VkPhysicalDeviceTimelineSemaphoreFeatures semaphore_features;
    memset(&semaphore_features, 0, sizeof(semaphore_features));
    semaphore_features.sType =
        VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TIMELINE_SEMAPHORE_FEATURES;
    semaphore_features.pNext = features2.pNext;
    features2.pNext = &semaphore_features;
    semaphore_features.timelineSemaphore = VK_TRUE;

    err = vkCreateDevice(*physical_device, &create_info, allocator, device);
    check_vk_result(err);
    vkGetDeviceQueue(*device, *queue_family_index, 0, queue);
  }

  // Create Descriptor Pool
  {
    VkDescriptorPoolSize pool_sizes[] = {
        {VK_DESCRIPTOR_TYPE_SAMPLER, 1000},
        {VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 1000},
        {VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_IMAGE, 1000},
        {VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC, 1000},
        {VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT, 1000}};
    VkDescriptorPoolCreateInfo pool_info = {};
    pool_info.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
    pool_info.flags = VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT;
    pool_info.maxSets = 1000 * IREE_ARRAYSIZE(pool_sizes);
    pool_info.poolSizeCount = (uint32_t)IREE_ARRAYSIZE(pool_sizes);
    pool_info.pPoolSizes = pool_sizes;
    err =
        vkCreateDescriptorPool(*device, &pool_info, allocator, descriptor_pool);
    check_vk_result(err);
  }
}

void SetupVulkanWindow(ImGui_ImplVulkanH_Window* wd,
                       const VkAllocationCallbacks* allocator,
                       VkInstance instance, uint32_t queue_family_index,
                       VkPhysicalDevice physical_device, VkDevice device,
                       VkSurfaceKHR surface, int width, int height,
                       uint32_t min_image_count) {
  wd->Surface = surface;

  // Check for WSI support
  VkBool32 res;
  vkGetPhysicalDeviceSurfaceSupportKHR(physical_device, queue_family_index,
                                       wd->Surface, &res);
  if (res != VK_TRUE) {
    fprintf(stderr, "Error no WSI support on physical device 0\n");
    exit(-1);
  }

  // Select Surface Format
  const VkFormat requestSurfaceImageFormat[] = {
      VK_FORMAT_B8G8R8A8_UNORM, VK_FORMAT_R8G8B8A8_UNORM,
      VK_FORMAT_B8G8R8_UNORM, VK_FORMAT_R8G8B8_UNORM};
  const VkColorSpaceKHR requestSurfaceColorSpace =
      VK_COLORSPACE_SRGB_NONLINEAR_KHR;
  wd->SurfaceFormat = ImGui_ImplVulkanH_SelectSurfaceFormat(
      physical_device, wd->Surface, requestSurfaceImageFormat,
      (size_t)IREE_ARRAYSIZE(requestSurfaceImageFormat),
      requestSurfaceColorSpace);

  // Select Present Mode
#ifdef IMGUI_UNLIMITED_FRAME_RATE
  VkPresentModeKHR present_modes[] = {VK_PRESENT_MODE_MAILBOX_KHR,
                                      VK_PRESENT_MODE_IMMEDIATE_KHR,
                                      VK_PRESENT_MODE_FIFO_KHR};
#else
  VkPresentModeKHR present_modes[] = {VK_PRESENT_MODE_FIFO_KHR};
#endif
  wd->PresentMode = ImGui_ImplVulkanH_SelectPresentMode(
      physical_device, wd->Surface, &present_modes[0],
      IREE_ARRAYSIZE(present_modes));

  // Create SwapChain, RenderPass, Framebuffer, etc.
  IM_ASSERT(min_image_count >= 2);
  ImGui_ImplVulkanH_CreateOrResizeWindow(instance, physical_device, device, wd,
                                         queue_family_index, allocator, width,
                                         height, min_image_count);

  // Set clear color.
  ImVec4 clear_color = ImVec4(0.45f, 0.55f, 0.60f, 1.00f);
  memcpy(&wd->ClearValue.color.float32[0], &clear_color, 4 * sizeof(float));
}

void RenderFrame(ImGui_ImplVulkanH_Window* wd, VkDevice device, VkQueue queue) {
  VkResult err;

  VkSemaphore image_acquired_semaphore =
      wd->FrameSemaphores[wd->SemaphoreIndex].ImageAcquiredSemaphore;
  VkSemaphore render_complete_semaphore =
      wd->FrameSemaphores[wd->SemaphoreIndex].RenderCompleteSemaphore;
  err = vkAcquireNextImageKHR(device, wd->Swapchain, UINT64_MAX,
                              image_acquired_semaphore, VK_NULL_HANDLE,
                              &wd->FrameIndex);
  check_vk_result(err);

  ImGui_ImplVulkanH_Frame* fd = &wd->Frames[wd->FrameIndex];
  {
    err = vkWaitForFences(
        device, 1, &fd->Fence, VK_TRUE,
        UINT64_MAX);  // wait indefinitely instead of periodically checking
    check_vk_result(err);

    err = vkResetFences(device, 1, &fd->Fence);
    check_vk_result(err);
  }
  {
    err = vkResetCommandPool(device, fd->CommandPool, 0);
    check_vk_result(err);
    VkCommandBufferBeginInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    info.flags |= VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
    err = vkBeginCommandBuffer(fd->CommandBuffer, &info);
    check_vk_result(err);
  }
  {
    VkRenderPassBeginInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_RENDER_PASS_BEGIN_INFO;
    info.renderPass = wd->RenderPass;
    info.framebuffer = fd->Framebuffer;
    info.renderArea.extent.width = wd->Width;
    info.renderArea.extent.height = wd->Height;
    info.clearValueCount = 1;
    info.pClearValues = &wd->ClearValue;
    vkCmdBeginRenderPass(fd->CommandBuffer, &info, VK_SUBPASS_CONTENTS_INLINE);
  }

  // Record Imgui Draw Data and draw funcs into command buffer
  ImGui_ImplVulkan_RenderDrawData(ImGui::GetDrawData(), fd->CommandBuffer);

  // Submit command buffer
  vkCmdEndRenderPass(fd->CommandBuffer);
  {
    VkPipelineStageFlags wait_stage =
        VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT;
    VkSubmitInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    info.waitSemaphoreCount = 1;
    info.pWaitSemaphores = &image_acquired_semaphore;
    info.pWaitDstStageMask = &wait_stage;
    info.commandBufferCount = 1;
    info.pCommandBuffers = &fd->CommandBuffer;
    info.signalSemaphoreCount = 1;
    info.pSignalSemaphores = &render_complete_semaphore;

    err = vkEndCommandBuffer(fd->CommandBuffer);
    check_vk_result(err);
    err = vkQueueSubmit(queue, 1, &info, fd->Fence);
    check_vk_result(err);
  }
}

void PresentFrame(ImGui_ImplVulkanH_Window* wd, VkQueue queue) {
  VkSemaphore render_complete_semaphore =
      wd->FrameSemaphores[wd->SemaphoreIndex].RenderCompleteSemaphore;
  VkPresentInfoKHR info = {};
  info.sType = VK_STRUCTURE_TYPE_PRESENT_INFO_KHR;
  info.waitSemaphoreCount = 1;
  info.pWaitSemaphores = &render_complete_semaphore;
  info.swapchainCount = 1;
  info.pSwapchains = &wd->Swapchain;
  info.pImageIndices = &wd->FrameIndex;
  VkResult err = vkQueuePresentKHR(queue, &info);
  check_vk_result(err);
  wd->SemaphoreIndex =
      (wd->SemaphoreIndex + 1) %
      wd->ImageCount;  // Now we can use the next set of semaphores
}

static void CleanupVulkan() {
  vkDestroyDescriptorPool(g_Device, g_DescriptorPool, g_Allocator);

  vkDestroyDevice(g_Device, g_Allocator);
  vkDestroyInstance(g_Instance, g_Allocator);
}

static void CleanupVulkanWindow() {
  ImGui_ImplVulkanH_DestroyWindow(g_Instance, g_Device, &g_MainWindowData,
                                  g_Allocator);
}

namespace iree {

extern "C" int iree_main(int argc, char** argv) {

  iree_flags_parse_checked(IREE_FLAGS_PARSE_MODE_DEFAULT, &argc, &argv);
  if (argc > 1) {
    // Avoid iree-run-module spinning endlessly on stdin if the user uses single
    // dashes for flags.
    printf(
        "[ERROR] unexpected positional argument (expected none)."
        " Did you use pass a flag with a single dash ('-')?"
        " Use '--' instead.\n");
    return 1;
  }

  // --------------------------------------------------------------------------
  // Create a window.
  if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_TIMER) != 0) {
    fprintf(stderr, "Failed to initialize SDL\n");
    abort();
    return 1;
  }

  // Setup window
  // clang-format off
  SDL_WindowFlags window_flags = (SDL_WindowFlags)(
      SDL_WINDOW_VULKAN | SDL_WINDOW_RESIZABLE | SDL_WINDOW_ALLOW_HIGHDPI);
  // clang-format on
  SDL_Window* window = SDL_CreateWindow(
      "IREE Samples - Vulkan Inference GUI", SDL_WINDOWPOS_CENTERED,
      SDL_WINDOWPOS_CENTERED, 1280, 720, window_flags);
  if (window == nullptr)
  {
    const char* sdl_err = SDL_GetError();
    fprintf(stderr, "Error, SDL_CreateWindow returned: %s\n", sdl_err);
    abort();
    return 1;
  }

  // Setup Vulkan
  iree_hal_vulkan_features_t iree_vulkan_features =
      static_cast<iree_hal_vulkan_features_t>(
          IREE_HAL_VULKAN_FEATURE_ENABLE_VALIDATION_LAYERS |
          IREE_HAL_VULKAN_FEATURE_ENABLE_DEBUG_UTILS);
  std::vector<const char*> layers = GetInstanceLayers(iree_vulkan_features);
  std::vector<const char*> extensions =
      GetInstanceExtensions(window, iree_vulkan_features);
  SetupVulkan(iree_vulkan_features, layers.data(),
              static_cast<uint32_t>(layers.size()), extensions.data(),
              static_cast<uint32_t>(extensions.size()), g_Allocator,
              &g_Instance, &g_QueueFamily, &g_PhysicalDevice, &g_Queue,
              &g_Device, &g_DescriptorPool);

  // Create Window Surface
  VkSurfaceKHR surface;
  VkResult err;
  if (SDL_Vulkan_CreateSurface(window, g_Instance, &surface) == 0) {
    fprintf(stderr, "Failed to create Vulkan surface.\n");
    abort();
    return 1;
  }

  // Create Framebuffers
  int w, h;
  SDL_GetWindowSize(window, &w, &h);
  ImGui_ImplVulkanH_Window* wd = &g_MainWindowData;
  SetupVulkanWindow(wd, g_Allocator, g_Instance, g_QueueFamily,
                    g_PhysicalDevice, g_Device, surface, w, h, g_MinImageCount);

  // Setup Dear ImGui context
  IMGUI_CHECKVERSION();
  ImGui::CreateContext();
  ImGuiIO& io = ImGui::GetIO();
  (void)io;

  ImGui::StyleColorsDark();

  // Setup Platform/Renderer bindings
  ImGui_ImplSDL2_InitForVulkan(window);
  ImGui_ImplVulkan_InitInfo init_info = {};
  init_info.Instance = g_Instance;
  init_info.PhysicalDevice = g_PhysicalDevice;
  init_info.Device = g_Device;
  init_info.QueueFamily = g_QueueFamily;
  init_info.Queue = g_Queue;
  init_info.PipelineCache = g_PipelineCache;
  init_info.DescriptorPool = g_DescriptorPool;
  init_info.Allocator = g_Allocator;
  init_info.MinImageCount = g_MinImageCount;
  init_info.ImageCount = wd->ImageCount;
  init_info.CheckVkResultFn = check_vk_result;
  ImGui_ImplVulkan_Init(&init_info, wd->RenderPass);

  // Upload Fonts
  {
    // Use any command queue
    VkCommandPool command_pool = wd->Frames[wd->FrameIndex].CommandPool;
    VkCommandBuffer command_buffer = wd->Frames[wd->FrameIndex].CommandBuffer;

    err = vkResetCommandPool(g_Device, command_pool, 0);
    check_vk_result(err);
    VkCommandBufferBeginInfo begin_info = {};
    begin_info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    begin_info.flags |= VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
    err = vkBeginCommandBuffer(command_buffer, &begin_info);
    check_vk_result(err);

    ImGui_ImplVulkan_CreateFontsTexture(command_buffer);

    VkSubmitInfo end_info = {};
    end_info.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    end_info.commandBufferCount = 1;
    end_info.pCommandBuffers = &command_buffer;
    err = vkEndCommandBuffer(command_buffer);
    check_vk_result(err);
    err = vkQueueSubmit(g_Queue, 1, &end_info, VK_NULL_HANDLE);
    check_vk_result(err);

    err = vkDeviceWaitIdle(g_Device);
    check_vk_result(err);
    ImGui_ImplVulkan_DestroyFontUploadObjects();
  }

  // Demo state.
  bool show_iree_window = true;
  // --------------------------------------------------------------------------
  // Setup IREE.

  // Check API version.
  iree_api_version_t actual_version;
  iree_status_t status =
      iree_api_version_check(IREE_API_VERSION_LATEST, &actual_version);
  if (iree_status_is_ok(status)) {
    fprintf(stdout, "IREE runtime API version: %d\n", actual_version);
  } else {
    fprintf(stderr, "Unsupported runtime API version: %d\n", actual_version);
    abort();
  }

  // Create a runtime Instance.
  iree_vm_instance_t* iree_instance = nullptr;
  IREE_CHECK_OK(
      iree_vm_instance_create(iree_allocator_system(), &iree_instance));

  // Register HAL drivers and VM module types.
  IREE_CHECK_OK(iree_hal_vulkan_driver_module_register(
      iree_hal_driver_registry_default()));
  IREE_CHECK_OK(iree_hal_module_register_all_types(iree_instance));

  // Create IREE Vulkan Driver and Device, sharing our VkInstance/VkDevice.
  fprintf(stdout, "Creating Vulkan driver/device\n");
  // Load symbols from our static `vkGetInstanceProcAddr` for IREE to use.
  iree_hal_vulkan_syms_t* iree_vk_syms = nullptr;
  IREE_CHECK_OK(iree_hal_vulkan_syms_create(
      reinterpret_cast<void*>(&vkGetInstanceProcAddr), iree_allocator_system(),
      &iree_vk_syms));
  // Create the driver sharing our VkInstance.
  iree_hal_driver_t* iree_vk_driver = nullptr;
  iree_string_view_t driver_identifier = iree_make_cstring_view("vulkan");
  iree_hal_vulkan_driver_options_t driver_options;
  driver_options.api_version = VK_API_VERSION_1_0;
  driver_options.requested_features = static_cast<iree_hal_vulkan_features_t>(
      IREE_HAL_VULKAN_FEATURE_ENABLE_DEBUG_UTILS);
  IREE_CHECK_OK(iree_hal_vulkan_driver_create_using_instance(
      driver_identifier, &driver_options, iree_vk_syms, g_Instance,
      iree_allocator_system(), &iree_vk_driver));
  // Create a device sharing our VkDevice and queue.
  // We could also create a separate (possibly low priority) compute queue for
  // IREE, and/or provide a dedicated transfer queue.
  iree_string_view_t device_identifier = iree_make_cstring_view("vulkan");
  iree_hal_vulkan_queue_set_t compute_queue_set;
  compute_queue_set.queue_family_index = g_QueueFamily;
  compute_queue_set.queue_indices = 1 << 0;
  iree_hal_vulkan_queue_set_t transfer_queue_set;
  transfer_queue_set.queue_indices = 0;
  iree_hal_device_t* iree_vk_device = nullptr;
  IREE_CHECK_OK(iree_hal_vulkan_wrap_device(
      device_identifier, &driver_options.device_options, iree_vk_syms,
      g_Instance, g_PhysicalDevice, g_Device, &compute_queue_set,
      &transfer_queue_set, iree_allocator_system(), &iree_vk_device));
  // Create a HAL module using the HAL device.
  iree_vm_module_t* hal_module = nullptr;
  IREE_CHECK_OK(iree_hal_module_create(iree_instance, iree_vk_device,
                                       IREE_HAL_MODULE_FLAG_NONE,
                                       iree_allocator_system(), &hal_module));


  // Load bytecode module
  //iree_file_toc_t module_file_toc;
  //const char network_model[] = "resnet50_tf.vmfb";
  //fprintf(stdout, "Loading: %s\n", network_model);
  //if (load_file(network_model, &module_file_toc.data, &module_file_toc.size) == false)
  //{
  //    abort();
  //    return 1;
  //}
  //fprintf(stdout, "module size: %zu\n", module_file_toc.size);

  iree_vm_module_t* bytecode_module = nullptr;
  iree_status_t module_status = iree_tooling_load_module_from_flags(
      iree_instance, iree_allocator_system(), &bytecode_module);
  if (!iree_status_is_ok(module_status))
    return -1;
  //IREE_CHECK_OK(iree_vm_bytecode_module_create(
  //    iree_instance,
  //    iree_const_byte_span_t{
  //        reinterpret_cast<const uint8_t*>(module_file_toc.data),
  //        module_file_toc.size},
  //    iree_allocator_null(), iree_allocator_system(), &bytecode_module));
  //// Query for details about what is in the loaded module.
  //iree_vm_module_signature_t bytecode_module_signature =
  //    iree_vm_module_signature(bytecode_module);
  //fprintf(stdout, "Module loaded, have <%" PRIhsz "> exported functions:\n",
  //        bytecode_module_signature.export_function_count);
  //for (int i = 0; i < bytecode_module_signature.export_function_count; ++i) {
  //  iree_vm_function_t function;
  //  IREE_CHECK_OK(iree_vm_module_lookup_function_by_ordinal(
  //      bytecode_module, IREE_VM_FUNCTION_LINKAGE_EXPORT, i, &function));
  //  auto function_name = iree_vm_function_name(&function);
  //  auto function_signature = iree_vm_function_signature(&function);

  //  fprintf(stdout, "  %d: '%.*s' with calling convention '%.*s'\n", i,
  //          (int)function_name.size, function_name.data,
  //          (int)function_signature.calling_convention.size,
  //          function_signature.calling_convention.data);
  //}

  // Allocate a context that will hold the module state across invocations.
  iree_vm_context_t* iree_context = nullptr;
  std::vector<iree_vm_module_t*> modules = {hal_module, bytecode_module};
  IREE_CHECK_OK(iree_vm_context_create_with_modules(
      iree_instance, IREE_VM_CONTEXT_FLAG_NONE, modules.size(), modules.data(),
      iree_allocator_system(), &iree_context));
  fprintf(stdout, "Context with modules is ready for use\n");

  // Lookup the entry point function.
  iree_vm_function_t main_function;
  const char kMainFunctionName[] = "module.forward";
  IREE_CHECK_OK(iree_vm_context_resolve_function(
      iree_context,
      iree_string_view_t{kMainFunctionName, sizeof(kMainFunctionName) - 1},
      &main_function));
  iree_string_view_t main_function_name = iree_vm_function_name(&main_function);
  fprintf(stdout, "Resolved main function named '%.*s'\n",
          (int)main_function_name.size, main_function_name.data);

  // --------------------------------------------------------------------------

        // Write inputs into mappable buffers.
        iree_hal_allocator_t* allocator =
            iree_hal_device_allocator(iree_vk_device);
        //iree_hal_memory_type_t input_memory_type =
        //    static_cast<iree_hal_memory_type_t>(
        //        IREE_HAL_MEMORY_TYPE_HOST_LOCAL |
        //        IREE_HAL_MEMORY_TYPE_DEVICE_VISIBLE);
        //iree_hal_buffer_usage_t input_buffer_usage =
        //    static_cast<iree_hal_buffer_usage_t>(IREE_HAL_BUFFER_USAGE_DEFAULT);
        //iree_hal_buffer_params_t buffer_params;
        //buffer_params.type = input_memory_type;
        //buffer_params.usage = input_buffer_usage;
        //buffer_params.access = IREE_HAL_MEMORY_ACCESS_READ | IREE_HAL_MEMORY_ACCESS_WRITE;

       // Wrap input buffers in buffer views.

        vm::ref<iree_vm_list_t> inputs;
        iree_status_t input_status = ParseToVariantList(
            allocator,
            iree::span<const std::string>{FLAG_function_inputs.data(),
                                          FLAG_function_inputs.size()},
            iree_allocator_system(), &inputs);
        if (!iree_status_is_ok(input_status))
            return -1;
        //vm::ref<iree_vm_list_t> inputs;
        //IREE_CHECK_OK(iree_vm_list_create(/*element_type=*/nullptr, 6, iree_allocator_system(), &inputs));

        //iree_hal_buffer_view_t* input0_buffer_view = nullptr;
        //constexpr iree_hal_dim_t input_buffer_shape[] = {1, 224, 224, 3};
        //IREE_CHECK_OK(iree_hal_buffer_view_allocate_buffer(
        //    allocator,
        //    /*shape_rank=*/4, /*shape=*/input_buffer_shape,
        //    IREE_HAL_ELEMENT_TYPE_FLOAT_32,
        //    IREE_HAL_ENCODING_TYPE_DENSE_ROW_MAJOR, buffer_params,
        //    iree_make_const_byte_span(&input_res50, sizeof(input_res50)),
        //    &input0_buffer_view));

        //auto input0_buffer_view_ref = iree_hal_buffer_view_move_ref(input0_buffer_view);
        //IREE_CHECK_OK(iree_vm_list_push_ref_move(inputs.get(), &input0_buffer_view_ref));

        // Prepare outputs list to accept results from the invocation.

        vm::ref<iree_vm_list_t> outputs;
        constexpr iree_hal_dim_t kOutputCount = 1000;
        IREE_CHECK_OK(iree_vm_list_create(/*element_type=*/nullptr, kOutputCount * sizeof(float), iree_allocator_system(), &outputs));

  // --------------------------------------------------------------------------

  // Main loop.
  bool done = false;
  while (!done) {
    SDL_Event event;

    while (SDL_PollEvent(&event)) {
      if (event.type == SDL_QUIT) {
        done = true;
      }

      ImGui_ImplSDL2_ProcessEvent(&event);
      if (event.type == SDL_QUIT) done = true;
      if (event.type == SDL_WINDOWEVENT &&
          event.window.event == SDL_WINDOWEVENT_RESIZED &&
          event.window.windowID == SDL_GetWindowID(window)) {
        g_SwapChainResizeWidth = (int)event.window.data1;
        g_SwapChainResizeHeight = (int)event.window.data2;
        g_SwapChainRebuild = true;
      }
    }

    if (g_SwapChainRebuild) {
      g_SwapChainRebuild = false;
      ImGui_ImplVulkan_SetMinImageCount(g_MinImageCount);
      ImGui_ImplVulkanH_CreateOrResizeWindow(
          g_Instance, g_PhysicalDevice, g_Device, &g_MainWindowData,
          g_QueueFamily, g_Allocator, g_SwapChainResizeWidth,
          g_SwapChainResizeHeight, g_MinImageCount);
      g_MainWindowData.FrameIndex = 0;
    }

    // Start the Dear ImGui frame
    ImGui_ImplVulkan_NewFrame();
    ImGui_ImplSDL2_NewFrame(window);
    ImGui::NewFrame();

    // Custom window.
    {
      ImGui::Begin("IREE Vulkan Integration Demo", &show_iree_window);

      ImGui::Separator();

      // ImGui Inputs for two input tensors.
      // Run computation whenever any of the values changes.
      static bool dirty = true;
      if (dirty) {

        // Synchronously invoke the function.
        IREE_CHECK_OK(iree_vm_invoke(iree_context, main_function,
                                     IREE_VM_INVOCATION_FLAG_NONE,
                                     /*policy=*/nullptr, inputs.get(),
                                     outputs.get(), iree_allocator_system()));


        // we want to run continuously so we can use tools like RenderDoc, RGP, etc...
        dirty = true;
      }

      // Framerate counter.
      ImGui::Text("Application average %.3f ms/frame (%.1f FPS)",
                  1000.0f / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate);

      ImGui::End();
    }

    // Rendering
    ImGui::Render();
    RenderFrame(wd, g_Device, g_Queue);

    PresentFrame(wd, g_Queue);
  }
  // --------------------------------------------------------------------------

  // --------------------------------------------------------------------------
  // Cleanup
  iree_vm_module_release(hal_module);
  iree_vm_module_release(bytecode_module);
  iree_vm_context_release(iree_context);
  iree_hal_device_release(iree_vk_device);
  iree_hal_allocator_release(allocator);
  iree_hal_driver_release(iree_vk_driver);
  iree_hal_vulkan_syms_release(iree_vk_syms);
  iree_vm_instance_release(iree_instance);

  err = vkDeviceWaitIdle(g_Device);
  check_vk_result(err);
  ImGui_ImplVulkan_Shutdown();
  ImGui_ImplSDL2_Shutdown();
  ImGui::DestroyContext();

  CleanupVulkanWindow();
  CleanupVulkan();

  SDL_DestroyWindow(window);
  SDL_Quit();
  // --------------------------------------------------------------------------

  return 0;
}

}  // namespace iree

```

`cpp/vulkan_gui/vulkan_resnet_inference_gui.cc`:

```cc
// Copyright 2019 The IREE Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

// Vulkan Graphics + IREE API Integration Sample.

#include <SDL.h>
#include <SDL_vulkan.h>
#include <imgui.h>
#include <imgui_impl_sdl.h>
#include <imgui_impl_vulkan.h>
#include <vulkan/vulkan.h>


#include <cstring>
#include <set>
#include <vector>
#include <fstream>

#include "iree/hal/drivers/vulkan/api.h"

// IREE's C API:
#include "iree/base/api.h"
#include "iree/hal/api.h"
#include "iree/hal/drivers/vulkan/registration/driver_module.h"
#include "iree/modules/hal/module.h"
#include "iree/vm/api.h"
#include "iree/vm/bytecode_module.h"
#include "iree/vm/ref_cc.h"

// Other dependencies (helpers, etc.)
#include "iree/base/internal/main.h"

#define IMGUI_UNLIMITED_FRAME_RATE

#define STB_IMAGE_IMPLEMENTATION
#include "stb_image.h"

typedef struct iree_file_toc_t {
  const char* name;             // the file's original name
  char* data;             // beginning of the file
  size_t size;                  // length of the file
} iree_file_toc_t;

bool load_file(const char* filename, char** pOut, size_t* pSize)
{
    FILE* f = fopen(filename, "rb");
    if (f == NULL)
    {
        fprintf(stderr, "Can't open %s\n", filename);
        return false;
    }

    fseek(f, 0L, SEEK_END);
    *pSize = ftell(f);
    fseek(f, 0L, SEEK_SET);

    *pOut = (char*)malloc(*pSize);

    size_t size = fread(*pOut, *pSize, 1, f);

    fclose(f);

    return size != 0;
}

static VkAllocationCallbacks* g_Allocator = NULL;
static VkInstance g_Instance = VK_NULL_HANDLE;
static VkPhysicalDevice g_PhysicalDevice = VK_NULL_HANDLE;
static VkDevice g_Device = VK_NULL_HANDLE;
static uint32_t g_QueueFamily = (uint32_t)-1;
static VkQueue g_Queue = VK_NULL_HANDLE;
static VkPipelineCache g_PipelineCache = VK_NULL_HANDLE;
static VkDescriptorPool g_DescriptorPool = VK_NULL_HANDLE;

static ImGui_ImplVulkanH_Window g_MainWindowData;
static uint32_t g_MinImageCount = 2;
static bool g_SwapChainRebuild = false;
static int g_SwapChainResizeWidth = 0;
static int g_SwapChainResizeHeight = 0;

static void check_vk_result(VkResult err) {
  if (err == 0) return;
  fprintf(stderr, "VkResult: %d\n", err);
  abort();
}

// Helper function to find Vulkan memory type bits. See ImGui_ImplVulkan_MemoryType() in imgui_impl_vulkan.cpp
uint32_t findMemoryType(uint32_t type_filter, VkMemoryPropertyFlags properties)
{
  VkPhysicalDeviceMemoryProperties mem_properties;
  vkGetPhysicalDeviceMemoryProperties(g_PhysicalDevice, &mem_properties);

  for (uint32_t i = 0; i < mem_properties.memoryTypeCount; i++)
  {
    if ((type_filter & (1 << i)) && (mem_properties.memoryTypes[i].propertyFlags & properties) == properties)
    {
      return i;
    }
  }

  return 0xFFFFFFFF; // Unable to find memoryType
}

// Helper function to load an image with common settings and return a VkDescriptorSet as a sort of Vulkan pointer
bool LoadTextureFromFile(const char* filename, VkDescriptorSet* img_ds, int* image_width, int* image_height)
{
  // Specifying 4 channels forces stb to load the image in RGBA which is an easy format for Vulkan
  int image_channels = 4;
  unsigned char* image_data = stbi_load(filename, image_width, image_height, 0, image_channels);

  if (image_data == NULL)
  {
    return false;
  }

  // Calculate allocation size (in number of bytes)
  size_t image_size = (*image_width)*(*image_height)*image_channels;

  VkResult err;

  // Create the Vulkan image.
  VkImage texture_image;
  VkDeviceMemory texture_image_memory;
  {
    VkImageCreateInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO;
    info.imageType = VK_IMAGE_TYPE_2D;
    info.format = VK_FORMAT_R8G8B8A8_UNORM;
    info.extent.width = *image_width;
    info.extent.height = *image_height;
    info.extent.depth = 1;
    info.mipLevels = 1;
    info.arrayLayers = 1;
    info.samples = VK_SAMPLE_COUNT_1_BIT;
    info.tiling = VK_IMAGE_TILING_OPTIMAL;
    info.usage = VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT;
    info.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
    info.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED;
    err = vkCreateImage(g_Device, &info, g_Allocator, &texture_image);
    check_vk_result(err);
    VkMemoryRequirements req;
    vkGetImageMemoryRequirements(g_Device, texture_image, &req);
    VkMemoryAllocateInfo alloc_info = {};
    alloc_info.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
    alloc_info.allocationSize = req.size;
    alloc_info.memoryTypeIndex = findMemoryType(req.memoryTypeBits, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT);
    err = vkAllocateMemory(g_Device, &alloc_info, g_Allocator, &texture_image_memory);
    check_vk_result(err);
    err = vkBindImageMemory(g_Device, texture_image, texture_image_memory, 0);
    check_vk_result(err);
  }

  // Create the Image View
  VkImageView image_view;
  {
    VkImageViewCreateInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
    info.image = texture_image;
    info.viewType = VK_IMAGE_VIEW_TYPE_2D;
    info.format = VK_FORMAT_R8G8B8A8_UNORM;
    info.subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
    info.subresourceRange.levelCount = 1;
    info.subresourceRange.layerCount = 1;
    err = vkCreateImageView(g_Device, &info, g_Allocator, &image_view);
    check_vk_result(err);
  }

  // Create Sampler
  VkSampler sampler;
  {
    VkSamplerCreateInfo sampler_info{};
    sampler_info.sType = VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO;
    sampler_info.magFilter = VK_FILTER_LINEAR;
    sampler_info.minFilter = VK_FILTER_LINEAR;
    sampler_info.mipmapMode  = VK_SAMPLER_MIPMAP_MODE_LINEAR;
    sampler_info.addressModeU = VK_SAMPLER_ADDRESS_MODE_REPEAT; // outside image bounds just use border color
    sampler_info.addressModeV = VK_SAMPLER_ADDRESS_MODE_REPEAT;
    sampler_info.addressModeW = VK_SAMPLER_ADDRESS_MODE_REPEAT;
    sampler_info.minLod = -1000;
    sampler_info.maxLod = 1000;
    sampler_info.maxAnisotropy = 1.0f;
    err = vkCreateSampler(g_Device, &sampler_info, g_Allocator, &sampler);
    check_vk_result(err);
  }

  // Create Descriptor Set using ImGUI's implementation
  *img_ds = ImGui_ImplVulkan_AddTexture(sampler, image_view, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL);

  // Create Upload Buffer
  VkBuffer upload_buffer;
  VkDeviceMemory upload_buffer_memory;
  {
    VkBufferCreateInfo buffer_info = {};
    buffer_info.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
    buffer_info.size = image_size;
    buffer_info.usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT;
    buffer_info.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
    err = vkCreateBuffer(g_Device, &buffer_info, g_Allocator, &upload_buffer);
    check_vk_result(err);
    VkMemoryRequirements req;
    vkGetBufferMemoryRequirements(g_Device, upload_buffer, &req);
    VkMemoryAllocateInfo alloc_info = {};
    alloc_info.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
    alloc_info.allocationSize = req.size;
    alloc_info.memoryTypeIndex = findMemoryType(req.memoryTypeBits, VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT);
    err = vkAllocateMemory(g_Device, &alloc_info, g_Allocator, &upload_buffer_memory);
    check_vk_result(err);
    err = vkBindBufferMemory(g_Device, upload_buffer, upload_buffer_memory, 0);
    check_vk_result(err);
  }

  // Upload to Buffer:
  {
    void* map = NULL;
    err = vkMapMemory(g_Device, upload_buffer_memory, 0, image_size, 0, &map);
    check_vk_result(err);
    memcpy(map, image_data, image_size);
    VkMappedMemoryRange range[1] = {};
    range[0].sType = VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE;
    range[0].memory = upload_buffer_memory;
    range[0].size = image_size;
    err = vkFlushMappedMemoryRanges(g_Device, 1, range);
    check_vk_result(err);
    vkUnmapMemory(g_Device, upload_buffer_memory);
  }

  // Release image memory using stb
  stbi_image_free(image_data);

  // Create a command buffer that will perform following steps when hit in the command queue.
  // TODO: this works in the example, but may need input if this is an acceptable way to access the pool/create the command buffer.
  VkCommandPool command_pool = g_MainWindowData.Frames[g_MainWindowData.FrameIndex].CommandPool;
  VkCommandBuffer command_buffer;
  {
    VkCommandBufferAllocateInfo alloc_info{};
    alloc_info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
    alloc_info.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
    alloc_info.commandPool = command_pool;
    alloc_info.commandBufferCount = 1;

    err = vkAllocateCommandBuffers(g_Device, &alloc_info, &command_buffer);
    check_vk_result(err);

    VkCommandBufferBeginInfo begin_info = {};
    begin_info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    begin_info.flags |= VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
    err = vkBeginCommandBuffer(command_buffer, &begin_info);
    check_vk_result(err);
  }

  // Copy to Image
  {
    VkImageMemoryBarrier copy_barrier[1] = {};
    copy_barrier[0].sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
    copy_barrier[0].dstAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
    copy_barrier[0].oldLayout = VK_IMAGE_LAYOUT_UNDEFINED;
    copy_barrier[0].newLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
    copy_barrier[0].srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
    copy_barrier[0].dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
    copy_barrier[0].image = texture_image;
    copy_barrier[0].subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
    copy_barrier[0].subresourceRange.levelCount = 1;
    copy_barrier[0].subresourceRange.layerCount = 1;
    vkCmdPipelineBarrier(command_buffer, VK_PIPELINE_STAGE_HOST_BIT, VK_PIPELINE_STAGE_TRANSFER_BIT, 0, 0, NULL, 0, NULL, 1, copy_barrier);

    VkBufferImageCopy region = {};
    region.imageSubresource.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
    region.imageSubresource.layerCount = 1;
    region.imageExtent.width = *image_width;
    region.imageExtent.height = *image_height;
    region.imageExtent.depth = 1;
    vkCmdCopyBufferToImage(command_buffer, upload_buffer, texture_image, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 1, &region);

    VkImageMemoryBarrier use_barrier[1] = {};
    use_barrier[0].sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
    use_barrier[0].srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
    use_barrier[0].dstAccessMask = VK_ACCESS_SHADER_READ_BIT;
    use_barrier[0].oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
    use_barrier[0].newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
    use_barrier[0].srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
    use_barrier[0].dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
    use_barrier[0].image = texture_image;
    use_barrier[0].subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
    use_barrier[0].subresourceRange.levelCount = 1;
    use_barrier[0].subresourceRange.layerCount = 1;
    vkCmdPipelineBarrier(command_buffer, VK_PIPELINE_STAGE_TRANSFER_BIT, VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 0, 0, NULL, 0, NULL, 1, use_barrier);
  }

  // End command buffer
  {
    VkSubmitInfo end_info = {};
    end_info.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    end_info.commandBufferCount = 1;
    end_info.pCommandBuffers = &command_buffer;
    err = vkEndCommandBuffer(command_buffer);
    check_vk_result(err);
    err = vkQueueSubmit(g_Queue, 1, &end_info, VK_NULL_HANDLE);
    check_vk_result(err);
    err = vkDeviceWaitIdle(g_Device);
    check_vk_result(err);
  }

  return true;
}

// Returns the names of the Vulkan layers used for the given IREE
// |extensibility_set| and |features|.
std::vector<const char*> GetIreeLayers(
    iree_hal_vulkan_extensibility_set_t extensibility_set,
    iree_hal_vulkan_features_t features) {
  iree_host_size_t required_count;
  iree_hal_vulkan_query_extensibility_set(
      features, extensibility_set, /*string_capacity=*/0, &required_count,
      /*out_string_values=*/NULL);
  std::vector<const char*> layers(required_count);
  iree_hal_vulkan_query_extensibility_set(features, extensibility_set,
                                          layers.size(), &required_count,
                                          layers.data());
  return layers;
}

// Returns the names of the Vulkan extensions used for the given IREE
// |extensibility_set| and |features|.
std::vector<const char*> GetIreeExtensions(
    iree_hal_vulkan_extensibility_set_t extensibility_set,
    iree_hal_vulkan_features_t features) {
  iree_host_size_t required_count;
  iree_hal_vulkan_query_extensibility_set(
      features, extensibility_set, /*string_capacity=*/0, &required_count,
      /*out_string_values=*/NULL);
  std::vector<const char*> extensions(required_count);
  iree_hal_vulkan_query_extensibility_set(features, extensibility_set,
                                          extensions.size(), &required_count,
                                          extensions.data());
  return extensions;
}

// Returns the names of the Vulkan extensions used for the given IREE
// |vulkan_features|.
std::vector<const char*> GetDeviceExtensions(
    VkPhysicalDevice physical_device,
    iree_hal_vulkan_features_t vulkan_features) {
  std::vector<const char*> iree_required_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_DEVICE_EXTENSIONS_REQUIRED,
      vulkan_features);
  std::vector<const char*> iree_optional_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_DEVICE_EXTENSIONS_OPTIONAL,
      vulkan_features);

  uint32_t extension_count = 0;
  check_vk_result(vkEnumerateDeviceExtensionProperties(
      physical_device, nullptr, &extension_count, nullptr));
  std::vector<VkExtensionProperties> extension_properties(extension_count);
  check_vk_result(vkEnumerateDeviceExtensionProperties(
      physical_device, nullptr, &extension_count, extension_properties.data()));

  // Merge extensions lists, including optional and required for simplicity.
  std::set<const char*> ext_set;
  ext_set.insert("VK_KHR_swapchain");
  ext_set.insert(iree_required_extensions.begin(),
                 iree_required_extensions.end());
  for (int i = 0; i < iree_optional_extensions.size(); ++i) {
    const char* optional_extension = iree_optional_extensions[i];
    for (int j = 0; j < extension_count; ++j) {
      if (strcmp(optional_extension, extension_properties[j].extensionName) ==
          0) {
        ext_set.insert(optional_extension);
        break;
      }
    }
  }
  std::vector<const char*> extensions(ext_set.begin(), ext_set.end());
  return extensions;
}

std::vector<const char*> GetInstanceLayers(
    iree_hal_vulkan_features_t vulkan_features) {
  // Query the layers that IREE wants / needs.
  std::vector<const char*> required_layers = GetIreeLayers(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_LAYERS_REQUIRED, vulkan_features);
  std::vector<const char*> optional_layers = GetIreeLayers(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_LAYERS_OPTIONAL, vulkan_features);

  // Query the layers that are available on the Vulkan ICD.
  uint32_t layer_property_count = 0;
  check_vk_result(
      vkEnumerateInstanceLayerProperties(&layer_property_count, NULL));
  std::vector<VkLayerProperties> layer_properties(layer_property_count);
  check_vk_result(vkEnumerateInstanceLayerProperties(&layer_property_count,
                                                     layer_properties.data()));

  // Match between optional/required and available layers.
  std::vector<const char*> layers;
  for (const char* layer_name : required_layers) {
    bool found = false;
    for (const auto& layer_property : layer_properties) {
      if (std::strcmp(layer_name, layer_property.layerName) == 0) {
        found = true;
        layers.push_back(layer_name);
        break;
      }
    }
    if (!found) {
      fprintf(stderr, "Required layer %s not available\n", layer_name);
      abort();
    }
  }
  for (const char* layer_name : optional_layers) {
    for (const auto& layer_property : layer_properties) {
      if (std::strcmp(layer_name, layer_property.layerName) == 0) {
        layers.push_back(layer_name);
        break;
      }
    }
  }

  return layers;
}

std::vector<const char*> GetInstanceExtensions(
    SDL_Window* window, iree_hal_vulkan_features_t vulkan_features) {
  // Ask SDL for its list of required instance extensions.
  uint32_t sdl_extensions_count = 0;
  SDL_Vulkan_GetInstanceExtensions(window, &sdl_extensions_count, NULL);
  std::vector<const char*> sdl_extensions(sdl_extensions_count);
  SDL_Vulkan_GetInstanceExtensions(window, &sdl_extensions_count,
                                   sdl_extensions.data());

  std::vector<const char*> iree_required_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_EXTENSIONS_REQUIRED,
      vulkan_features);
  std::vector<const char*> iree_optional_extensions = GetIreeExtensions(
      IREE_HAL_VULKAN_EXTENSIBILITY_INSTANCE_EXTENSIONS_OPTIONAL,
      vulkan_features);

  // Merge extensions lists, including optional and required for simplicity.
  std::set<const char*> ext_set;
  ext_set.insert(sdl_extensions.begin(), sdl_extensions.end());
  ext_set.insert(iree_required_extensions.begin(),
                 iree_required_extensions.end());
  ext_set.insert(iree_optional_extensions.begin(),
                 iree_optional_extensions.end());
  std::vector<const char*> extensions(ext_set.begin(), ext_set.end());
  return extensions;
}

void SetupVulkan(iree_hal_vulkan_features_t vulkan_features,
                 const char** instance_layers, uint32_t instance_layers_count,
                 const char** instance_extensions,
                 uint32_t instance_extensions_count,
                 const VkAllocationCallbacks* allocator, VkInstance* instance,
                 uint32_t* queue_family_index,
                 VkPhysicalDevice* physical_device, VkQueue* queue,
                 VkDevice* device, VkDescriptorPool* descriptor_pool) {
  VkResult err;

  // Create Vulkan Instance
  {
    VkInstanceCreateInfo create_info = {};
    create_info.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO;
    create_info.enabledLayerCount = instance_layers_count;
    create_info.ppEnabledLayerNames = instance_layers;
    create_info.enabledExtensionCount = instance_extensions_count;
    create_info.ppEnabledExtensionNames = instance_extensions;
    err = vkCreateInstance(&create_info, allocator, instance);
    check_vk_result(err);
  }

  // Select GPU
  {
    uint32_t gpu_count;
    err = vkEnumeratePhysicalDevices(*instance, &gpu_count, NULL);
    check_vk_result(err);
    IM_ASSERT(gpu_count > 0);

    VkPhysicalDevice* gpus =
        (VkPhysicalDevice*)malloc(sizeof(VkPhysicalDevice) * gpu_count);
    err = vkEnumeratePhysicalDevices(*instance, &gpu_count, gpus);
    check_vk_result(err);

    // Use the first reported GPU for simplicity.
    *physical_device = gpus[0];

    VkPhysicalDeviceProperties properties;
    vkGetPhysicalDeviceProperties(*physical_device, &properties);
    fprintf(stdout, "Selected Vulkan device: '%s'\n", properties.deviceName);
    free(gpus);
  }

  // Select queue family. We want a single queue with graphics and compute for
  // simplicity, but we could also discover and use separate queues for each.
  {
    uint32_t count;
    vkGetPhysicalDeviceQueueFamilyProperties(*physical_device, &count, NULL);
    VkQueueFamilyProperties* queues = (VkQueueFamilyProperties*)malloc(
        sizeof(VkQueueFamilyProperties) * count);
    vkGetPhysicalDeviceQueueFamilyProperties(*physical_device, &count, queues);
    for (uint32_t i = 0; i < count; i++) {
      if (queues[i].queueFlags &
          (VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT)) {
        *queue_family_index = i;
        break;
      }
    }
    free(queues);
    IM_ASSERT(*queue_family_index != (uint32_t)-1);
  }

  // Create Logical Device (with 1 queue)
  {
    std::vector<const char*> device_extensions =
        GetDeviceExtensions(*physical_device, vulkan_features);
    const float queue_priority[] = {1.0f};
    VkDeviceQueueCreateInfo queue_info = {};
    queue_info.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
    queue_info.queueFamilyIndex = *queue_family_index;
    queue_info.queueCount = 1;
    queue_info.pQueuePriorities = queue_priority;
    VkDeviceCreateInfo create_info = {};
    create_info.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;
    create_info.queueCreateInfoCount = 1;
    create_info.pQueueCreateInfos = &queue_info;
    create_info.enabledExtensionCount =
        static_cast<uint32_t>(device_extensions.size());
    create_info.ppEnabledExtensionNames = device_extensions.data();

    // Enable timeline semaphores.
    VkPhysicalDeviceFeatures2 features2;
    memset(&features2, 0, sizeof(features2));
    features2.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2;
    create_info.pNext = &features2;
    VkPhysicalDeviceTimelineSemaphoreFeatures semaphore_features;
    memset(&semaphore_features, 0, sizeof(semaphore_features));
    semaphore_features.sType =
        VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TIMELINE_SEMAPHORE_FEATURES;
    semaphore_features.pNext = features2.pNext;
    features2.pNext = &semaphore_features;
    semaphore_features.timelineSemaphore = VK_TRUE;

    err = vkCreateDevice(*physical_device, &create_info, allocator, device);
    check_vk_result(err);
    vkGetDeviceQueue(*device, *queue_family_index, 0, queue);
  }

  // Create Descriptor Pool
  {
    VkDescriptorPoolSize pool_sizes[] = {
        {VK_DESCRIPTOR_TYPE_SAMPLER, 1000},
        {VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 1000},
        {VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_IMAGE, 1000},
        {VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1000},
        {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC, 1000},
        {VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC, 1000},
        {VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT, 1000}};
    VkDescriptorPoolCreateInfo pool_info = {};
    pool_info.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
    pool_info.flags = VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT;
    pool_info.maxSets = 1000 * IREE_ARRAYSIZE(pool_sizes);
    pool_info.poolSizeCount = (uint32_t)IREE_ARRAYSIZE(pool_sizes);
    pool_info.pPoolSizes = pool_sizes;
    err =
        vkCreateDescriptorPool(*device, &pool_info, allocator, descriptor_pool);
    check_vk_result(err);
  }
}

void SetupVulkanWindow(ImGui_ImplVulkanH_Window* wd,
                       const VkAllocationCallbacks* allocator,
                       VkInstance instance, uint32_t queue_family_index,
                       VkPhysicalDevice physical_device, VkDevice device,
                       VkSurfaceKHR surface, int width, int height,
                       uint32_t min_image_count) {
  wd->Surface = surface;

  // Check for WSI support
  VkBool32 res;
  vkGetPhysicalDeviceSurfaceSupportKHR(physical_device, queue_family_index,
                                       wd->Surface, &res);
  if (res != VK_TRUE) {
    fprintf(stderr, "Error no WSI support on physical device 0\n");
    exit(-1);
  }

  // Select Surface Format
  const VkFormat requestSurfaceImageFormat[] = {
      VK_FORMAT_B8G8R8A8_UNORM, VK_FORMAT_R8G8B8A8_UNORM,
      VK_FORMAT_B8G8R8_UNORM, VK_FORMAT_R8G8B8_UNORM};
  const VkColorSpaceKHR requestSurfaceColorSpace =
      VK_COLORSPACE_SRGB_NONLINEAR_KHR;
  wd->SurfaceFormat = ImGui_ImplVulkanH_SelectSurfaceFormat(
      physical_device, wd->Surface, requestSurfaceImageFormat,
      (size_t)IREE_ARRAYSIZE(requestSurfaceImageFormat),
      requestSurfaceColorSpace);

  // Select Present Mode
#ifdef IMGUI_UNLIMITED_FRAME_RATE
  VkPresentModeKHR present_modes[] = {VK_PRESENT_MODE_MAILBOX_KHR,
                                      VK_PRESENT_MODE_IMMEDIATE_KHR,
                                      VK_PRESENT_MODE_FIFO_KHR};
#else
  VkPresentModeKHR present_modes[] = {VK_PRESENT_MODE_FIFO_KHR};
#endif
  wd->PresentMode = ImGui_ImplVulkanH_SelectPresentMode(
      physical_device, wd->Surface, &present_modes[0],
      IREE_ARRAYSIZE(present_modes));

  // Create SwapChain, RenderPass, Framebuffer, etc.
  IM_ASSERT(min_image_count >= 2);
  ImGui_ImplVulkanH_CreateOrResizeWindow(instance, physical_device, device, wd,
                                         queue_family_index, allocator, width,
                                         height, min_image_count);

  // Set clear color.
  ImVec4 clear_color = ImVec4(0.45f, 0.55f, 0.60f, 1.00f);
  memcpy(&wd->ClearValue.color.float32[0], &clear_color, 4 * sizeof(float));
}

void RenderFrame(ImGui_ImplVulkanH_Window* wd, VkDevice device, VkQueue queue) {
  VkResult err;

  VkSemaphore image_acquired_semaphore =
      wd->FrameSemaphores[wd->SemaphoreIndex].ImageAcquiredSemaphore;
  VkSemaphore render_complete_semaphore =
      wd->FrameSemaphores[wd->SemaphoreIndex].RenderCompleteSemaphore;
  err = vkAcquireNextImageKHR(device, wd->Swapchain, UINT64_MAX,
                              image_acquired_semaphore, VK_NULL_HANDLE,
                              &wd->FrameIndex);
  check_vk_result(err);

  ImGui_ImplVulkanH_Frame* fd = &wd->Frames[wd->FrameIndex];
  {
    err = vkWaitForFences(
        device, 1, &fd->Fence, VK_TRUE,
        UINT64_MAX);  // wait indefinitely instead of periodically checking
    check_vk_result(err);

    err = vkResetFences(device, 1, &fd->Fence);
    check_vk_result(err);
  }
  {
    err = vkResetCommandPool(device, fd->CommandPool, 0);
    check_vk_result(err);
    VkCommandBufferBeginInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    info.flags |= VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
    err = vkBeginCommandBuffer(fd->CommandBuffer, &info);
    check_vk_result(err);
  }
  {
    VkRenderPassBeginInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_RENDER_PASS_BEGIN_INFO;
    info.renderPass = wd->RenderPass;
    info.framebuffer = fd->Framebuffer;
    info.renderArea.extent.width = wd->Width;
    info.renderArea.extent.height = wd->Height;
    info.clearValueCount = 1;
    info.pClearValues = &wd->ClearValue;
    vkCmdBeginRenderPass(fd->CommandBuffer, &info, VK_SUBPASS_CONTENTS_INLINE);
  }

  // Record Imgui Draw Data and draw funcs into command buffer
  ImGui_ImplVulkan_RenderDrawData(ImGui::GetDrawData(), fd->CommandBuffer);

  // Submit command buffer
  vkCmdEndRenderPass(fd->CommandBuffer);
  {
    VkPipelineStageFlags wait_stage =
        VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT;
    VkSubmitInfo info = {};
    info.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    info.waitSemaphoreCount = 1;
    info.pWaitSemaphores = &image_acquired_semaphore;
    info.pWaitDstStageMask = &wait_stage;
    info.commandBufferCount = 1;
    info.pCommandBuffers = &fd->CommandBuffer;
    info.signalSemaphoreCount = 1;
    info.pSignalSemaphores = &render_complete_semaphore;

    err = vkEndCommandBuffer(fd->CommandBuffer);
    check_vk_result(err);
    err = vkQueueSubmit(queue, 1, &info, fd->Fence);
    check_vk_result(err);
  }
}

void PresentFrame(ImGui_ImplVulkanH_Window* wd, VkQueue queue) {
  VkSemaphore render_complete_semaphore =
      wd->FrameSemaphores[wd->SemaphoreIndex].RenderCompleteSemaphore;
  VkPresentInfoKHR info = {};
  info.sType = VK_STRUCTURE_TYPE_PRESENT_INFO_KHR;
  info.waitSemaphoreCount = 1;
  info.pWaitSemaphores = &render_complete_semaphore;
  info.swapchainCount = 1;
  info.pSwapchains = &wd->Swapchain;
  info.pImageIndices = &wd->FrameIndex;
  VkResult err = vkQueuePresentKHR(queue, &info);
  check_vk_result(err);
  wd->SemaphoreIndex =
      (wd->SemaphoreIndex + 1) %
      wd->ImageCount;  // Now we can use the next set of semaphores
}

static void CleanupVulkan() {
  vkDestroyDescriptorPool(g_Device, g_DescriptorPool, g_Allocator);

  vkDestroyDevice(g_Device, g_Allocator);
  vkDestroyInstance(g_Instance, g_Allocator);
}

static void CleanupVulkanWindow() {
  ImGui_ImplVulkanH_DestroyWindow(g_Instance, g_Device, &g_MainWindowData,
                                  g_Allocator);
}

namespace iree {

extern "C" int iree_main(int argc, char** argv) {

  fprintf(stdout, "starting yo\n");

  // --------------------------------------------------------------------------
  // Create a window.
  if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_TIMER) != 0) {
    fprintf(stderr, "Failed to initialize SDL\n");
    abort();
    return 1;
  }

  // Setup window
  // clang-format off
  SDL_WindowFlags window_flags = (SDL_WindowFlags)(
      SDL_WINDOW_VULKAN | SDL_WINDOW_RESIZABLE | SDL_WINDOW_ALLOW_HIGHDPI);
  // clang-format on
  SDL_Window* window = SDL_CreateWindow(
      "IREE Samples - Vulkan Inference GUI", SDL_WINDOWPOS_CENTERED,
      SDL_WINDOWPOS_CENTERED, 1280, 720, window_flags);
  if (window == nullptr)
  {
    const char* sdl_err = SDL_GetError();
    fprintf(stderr, "Error, SDL_CreateWindow returned: %s\n", sdl_err);
    abort();
    return 1;
  }

  // Setup Vulkan
  iree_hal_vulkan_features_t iree_vulkan_features =
      static_cast<iree_hal_vulkan_features_t>(
          IREE_HAL_VULKAN_FEATURE_ENABLE_VALIDATION_LAYERS |
          IREE_HAL_VULKAN_FEATURE_ENABLE_DEBUG_UTILS);
  std::vector<const char*> layers = GetInstanceLayers(iree_vulkan_features);
  std::vector<const char*> extensions =
      GetInstanceExtensions(window, iree_vulkan_features);
  SetupVulkan(iree_vulkan_features, layers.data(),
              static_cast<uint32_t>(layers.size()), extensions.data(),
              static_cast<uint32_t>(extensions.size()), g_Allocator,
              &g_Instance, &g_QueueFamily, &g_PhysicalDevice, &g_Queue,
              &g_Device, &g_DescriptorPool);

  // Create Window Surface
  VkSurfaceKHR surface;
  VkResult err;
  if (SDL_Vulkan_CreateSurface(window, g_Instance, &surface) == 0) {
    fprintf(stderr, "Failed to create Vulkan surface.\n");
    abort();
    return 1;
  }

  // Create Framebuffers
  int w, h;
  SDL_GetWindowSize(window, &w, &h);
  ImGui_ImplVulkanH_Window* wd = &g_MainWindowData;
  SetupVulkanWindow(wd, g_Allocator, g_Instance, g_QueueFamily,
                    g_PhysicalDevice, g_Device, surface, w, h, g_MinImageCount);

  // Setup Dear ImGui context
  IMGUI_CHECKVERSION();
  ImGui::CreateContext();
  ImGuiIO& io = ImGui::GetIO();
  (void)io;

  ImGui::StyleColorsDark();

  // Setup Platform/Renderer bindings
  ImGui_ImplSDL2_InitForVulkan(window);
  ImGui_ImplVulkan_InitInfo init_info = {};
  init_info.Instance = g_Instance;
  init_info.PhysicalDevice = g_PhysicalDevice;
  init_info.Device = g_Device;
  init_info.QueueFamily = g_QueueFamily;
  init_info.Queue = g_Queue;
  init_info.PipelineCache = g_PipelineCache;
  init_info.DescriptorPool = g_DescriptorPool;
  init_info.Allocator = g_Allocator;
  init_info.MinImageCount = g_MinImageCount;
  init_info.ImageCount = wd->ImageCount;
  init_info.CheckVkResultFn = check_vk_result;
  ImGui_ImplVulkan_Init(&init_info, wd->RenderPass);

  // Upload Fonts
  {
    // Use any command queue
    VkCommandPool command_pool = wd->Frames[wd->FrameIndex].CommandPool;
    VkCommandBuffer command_buffer = wd->Frames[wd->FrameIndex].CommandBuffer;

    err = vkResetCommandPool(g_Device, command_pool, 0);
    check_vk_result(err);
    VkCommandBufferBeginInfo begin_info = {};
    begin_info.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    begin_info.flags |= VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
    err = vkBeginCommandBuffer(command_buffer, &begin_info);
    check_vk_result(err);

    ImGui_ImplVulkan_CreateFontsTexture(command_buffer);

    VkSubmitInfo end_info = {};
    end_info.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    end_info.commandBufferCount = 1;
    end_info.pCommandBuffers = &command_buffer;
    err = vkEndCommandBuffer(command_buffer);
    check_vk_result(err);
    err = vkQueueSubmit(g_Queue, 1, &end_info, VK_NULL_HANDLE);
    check_vk_result(err);

    err = vkDeviceWaitIdle(g_Device);
    check_vk_result(err);
    ImGui_ImplVulkan_DestroyFontUploadObjects();
  }

  // Demo state.
  bool show_iree_window = true;
  // --------------------------------------------------------------------------

  // --------------------------------------------------------------------------
  // Setup IREE.

  // Check API version.
  iree_api_version_t actual_version;
  iree_status_t status =
      iree_api_version_check(IREE_API_VERSION_LATEST, &actual_version);
  if (iree_status_is_ok(status)) {
    fprintf(stdout, "IREE runtime API version: %d\n", actual_version);
  } else {
    fprintf(stderr, "Unsupported runtime API version: %d\n", actual_version);
    abort();
  }

  // Create a runtime Instance.
  iree_vm_instance_t* iree_instance = nullptr;
  IREE_CHECK_OK(
      iree_vm_instance_create(iree_allocator_system(), &iree_instance));

  // Register HAL drivers and VM module types.
  IREE_CHECK_OK(iree_hal_vulkan_driver_module_register(
      iree_hal_driver_registry_default()));
  IREE_CHECK_OK(iree_hal_module_register_all_types(iree_instance));

  // Create IREE Vulkan Driver and Device, sharing our VkInstance/VkDevice.
  fprintf(stdout, "Creating Vulkan driver/device\n");
  // Load symbols from our static `vkGetInstanceProcAddr` for IREE to use.
  iree_hal_vulkan_syms_t* iree_vk_syms = nullptr;
  IREE_CHECK_OK(iree_hal_vulkan_syms_create(
      reinterpret_cast<void*>(&vkGetInstanceProcAddr), iree_allocator_system(),
      &iree_vk_syms));
  // Create the driver sharing our VkInstance.
  iree_hal_driver_t* iree_vk_driver = nullptr;
  iree_string_view_t driver_identifier = iree_make_cstring_view("vulkan");
  iree_hal_vulkan_driver_options_t driver_options;
  driver_options.api_version = VK_API_VERSION_1_0;
  driver_options.requested_features = static_cast<iree_hal_vulkan_features_t>(
      IREE_HAL_VULKAN_FEATURE_ENABLE_DEBUG_UTILS);
  IREE_CHECK_OK(iree_hal_vulkan_driver_create_using_instance(
      driver_identifier, &driver_options, iree_vk_syms, g_Instance,
      iree_allocator_system(), &iree_vk_driver));
  // Create a device sharing our VkDevice and queue.
  // We could also create a separate (possibly low priority) compute queue for
  // IREE, and/or provide a dedicated transfer queue.
  iree_string_view_t device_identifier = iree_make_cstring_view("vulkan");
  iree_hal_vulkan_queue_set_t compute_queue_set;
  compute_queue_set.queue_family_index = g_QueueFamily;
  compute_queue_set.queue_indices = 1 << 0;
  iree_hal_vulkan_queue_set_t transfer_queue_set;
  transfer_queue_set.queue_indices = 0;
  iree_hal_device_t* iree_vk_device = nullptr;
  IREE_CHECK_OK(iree_hal_vulkan_wrap_device(
      device_identifier, &driver_options.device_options, iree_vk_syms,
      g_Instance, g_PhysicalDevice, g_Device, &compute_queue_set,
      &transfer_queue_set, iree_allocator_system(), &iree_vk_device));
  // Create a HAL module using the HAL device.
  iree_vm_module_t* hal_module = nullptr;
  IREE_CHECK_OK(iree_hal_module_create(iree_instance, iree_vk_device,
                                       IREE_HAL_MODULE_FLAG_NONE,
                                       iree_allocator_system(), &hal_module));


  // Load bytecode module
  iree_file_toc_t module_file_toc;
  const char network_model[] = "resnet50_tf.vmfb";
  fprintf(stdout, "Loading: %s\n", network_model);
  if (load_file(network_model, &module_file_toc.data, &module_file_toc.size) == false)
  {
      abort();
      return 1;
  }
  fprintf(stdout, "module size: %zu\n", module_file_toc.size);

  static float input_res50[224*224*3];
  static float output_res50[1000];

  char filename[] = "dog_imagenet.jpg";
  fprintf(stdout, "loading: %s\n", filename);
  int x,y,n;
  //unsigned char *image_raw = stbi_load(filename, &x, &y, &n, 3);
  stbi_load(filename, &x, &y, &n, 3);
  fprintf(stdout, "res: %i x %i x %i\n", x, y, n);

  /* Preprocessing needs to go here. For now use a buffer preprocessed in python.

  //convert image into floating point format
  for(int i=0;i<224*224*3;i++)
  {
    input_res50[i]= ((float)image_raw[i])/255.0f;
  }*/

  std::ifstream fin("dog.bin", std::ifstream::in | std::ifstream::binary);
  fin.read((char*)input_res50, 224*224*3*sizeof(float));

  // load image again so imgui can display it
  int my_image_width = 0;
  int my_image_height = 0;
  VkDescriptorSet my_image_texture = 0;
  bool ret = LoadTextureFromFile(filename, &my_image_texture, &my_image_width, &my_image_height);
  fprintf(stdout, "creating vulkan image: %s\n", ret ?"OK":"FAIL");
  IM_ASSERT(ret);

  iree_vm_module_t* bytecode_module = nullptr;
  IREE_CHECK_OK(iree_vm_bytecode_module_create(
      iree_instance,
      iree_const_byte_span_t{
          reinterpret_cast<const uint8_t*>(module_file_toc.data),
          module_file_toc.size},
      iree_allocator_null(), iree_allocator_system(), &bytecode_module));
  // Query for details about what is in the loaded module.
  iree_vm_module_signature_t bytecode_module_signature =
      iree_vm_module_signature(bytecode_module);
  fprintf(stdout, "Module loaded, have <%" PRIhsz "> exported functions:\n",
          bytecode_module_signature.export_function_count);
  for (int i = 0; i < bytecode_module_signature.export_function_count; ++i) {
    iree_vm_function_t function;
    IREE_CHECK_OK(iree_vm_module_lookup_function_by_ordinal(
        bytecode_module, IREE_VM_FUNCTION_LINKAGE_EXPORT, i, &function));
    auto function_name = iree_vm_function_name(&function);
    auto function_signature = iree_vm_function_signature(&function);

    fprintf(stdout, "  %d: '%.*s' with calling convention '%.*s'\n", i,
            (int)function_name.size, function_name.data,
            (int)function_signature.calling_convention.size,
            function_signature.calling_convention.data);
  }

  // Allocate a context that will hold the module state across invocations.
  iree_vm_context_t* iree_context = nullptr;
  std::vector<iree_vm_module_t*> modules = {hal_module, bytecode_module};
  IREE_CHECK_OK(iree_vm_context_create_with_modules(
      iree_instance, IREE_VM_CONTEXT_FLAG_NONE, modules.size(), modules.data(),
      iree_allocator_system(), &iree_context));
  fprintf(stdout, "Context with modules is ready for use\n");

  // Lookup the entry point function.
  iree_vm_function_t main_function;
  const char kMainFunctionName[] = "module.forward";
  IREE_CHECK_OK(iree_vm_context_resolve_function(
      iree_context,
      iree_string_view_t{kMainFunctionName, sizeof(kMainFunctionName) - 1},
      &main_function));
  iree_string_view_t main_function_name = iree_vm_function_name(&main_function);
  fprintf(stdout, "Resolved main function named '%.*s'\n",
          (int)main_function_name.size, main_function_name.data);

  // --------------------------------------------------------------------------

        // Write inputs into mappable buffers.
        iree_hal_allocator_t* allocator =
            iree_hal_device_allocator(iree_vk_device);
        iree_hal_memory_type_t input_memory_type =
            static_cast<iree_hal_memory_type_t>(
                IREE_HAL_MEMORY_TYPE_HOST_LOCAL |
                IREE_HAL_MEMORY_TYPE_DEVICE_VISIBLE);
        iree_hal_buffer_usage_t input_buffer_usage =
            static_cast<iree_hal_buffer_usage_t>(IREE_HAL_BUFFER_USAGE_DEFAULT);
        iree_hal_buffer_params_t buffer_params;
        buffer_params.type = input_memory_type;
        buffer_params.usage = input_buffer_usage;
        buffer_params.access = IREE_HAL_MEMORY_ACCESS_READ | IREE_HAL_MEMORY_ACCESS_WRITE;

       // Wrap input buffers in buffer views.

        iree_hal_buffer_view_t* input0_buffer_view = nullptr;
        constexpr iree_hal_dim_t input_buffer_shape[] = {1, 224, 224, 3};
        IREE_CHECK_OK(iree_hal_buffer_view_allocate_buffer(
            allocator,
            /*shape_rank=*/4, /*shape=*/input_buffer_shape,
            IREE_HAL_ELEMENT_TYPE_FLOAT_32,
            IREE_HAL_ENCODING_TYPE_DENSE_ROW_MAJOR, buffer_params,
            iree_make_const_byte_span(&input_res50, sizeof(input_res50)),
            &input0_buffer_view));

        vm::ref<iree_vm_list_t> inputs;
        IREE_CHECK_OK(iree_vm_list_create(/*element_type=*/nullptr, 6, iree_allocator_system(), &inputs));
        auto input0_buffer_view_ref = iree_hal_buffer_view_move_ref(input0_buffer_view);
        IREE_CHECK_OK(iree_vm_list_push_ref_move(inputs.get(), &input0_buffer_view_ref));

        // Prepare outputs list to accept results from the invocation.

        vm::ref<iree_vm_list_t> outputs;
        constexpr iree_hal_dim_t kOutputCount = 1000;
        IREE_CHECK_OK(iree_vm_list_create(/*element_type=*/nullptr, kOutputCount * sizeof(float), iree_allocator_system(), &outputs));

  // --------------------------------------------------------------------------
  // Main loop.
  bool done = false;
  while (!done) {
    SDL_Event event;

    while (SDL_PollEvent(&event)) {
      if (event.type == SDL_QUIT) {
        done = true;
      }

      ImGui_ImplSDL2_ProcessEvent(&event);
      if (event.type == SDL_QUIT) done = true;
      if (event.type == SDL_WINDOWEVENT &&
          event.window.event == SDL_WINDOWEVENT_RESIZED &&
          event.window.windowID == SDL_GetWindowID(window)) {
        g_SwapChainResizeWidth = (int)event.window.data1;
        g_SwapChainResizeHeight = (int)event.window.data2;
        g_SwapChainRebuild = true;
      }
    }

    if (g_SwapChainRebuild) {
      g_SwapChainRebuild = false;
      ImGui_ImplVulkan_SetMinImageCount(g_MinImageCount);
      ImGui_ImplVulkanH_CreateOrResizeWindow(
          g_Instance, g_PhysicalDevice, g_Device, &g_MainWindowData,
          g_QueueFamily, g_Allocator, g_SwapChainResizeWidth,
          g_SwapChainResizeHeight, g_MinImageCount);
      g_MainWindowData.FrameIndex = 0;
    }

    // Start the Dear ImGui frame
    ImGui_ImplVulkan_NewFrame();
    ImGui_ImplSDL2_NewFrame(window);
    ImGui::NewFrame();

    // Custom window.
    {
      ImGui::Begin("IREE Vulkan Integration Demo", &show_iree_window);

      ImGui::Separator();

      // ImGui Inputs for two input tensors.
      // Run computation whenever any of the values changes.
      static bool dirty = true;
      if (dirty) {

        // Synchronously invoke the function.
        IREE_CHECK_OK(iree_vm_invoke(iree_context, main_function,
                                     IREE_VM_INVOCATION_FLAG_NONE,
                                     /*policy=*/nullptr, inputs.get(),
                                     outputs.get(), iree_allocator_system()));

        // Read back the results.
        auto* output_buffer_view = reinterpret_cast<iree_hal_buffer_view_t*>(
            iree_vm_list_get_ref_deref(outputs.get(),
            0,
            iree_hal_buffer_view_get_descriptor()));
        IREE_CHECK_OK(iree_hal_device_transfer_d2h(
            iree_vk_device,
            iree_hal_buffer_view_buffer(output_buffer_view),
            0,
            output_res50, sizeof(output_res50),
            IREE_HAL_TRANSFER_BUFFER_FLAG_DEFAULT, iree_infinite_timeout()));

        // we want to run continuously so we can use tools like RenderDoc, RGP, etc...
        dirty = true;
      }

      // find maxarg from results
      float max = 0.0f;
      int max_idx = -1;
      for(int i=0;i<1000;i++)
      {
        if (output_res50[i] > max)
        {
          max = output_res50[i];
          max_idx = i;
        }
      }

      ImGui::Text("pointer = %p", my_image_texture);
      ImGui::Text("size = %d x %d", my_image_width, my_image_height);
      ImGui::Image((ImTextureID)my_image_texture, ImVec2(my_image_width, my_image_height));

      // Display the latest computation output.
      ImGui::Text("Max   idx = [%i]", max_idx);
      ImGui::Text("Max value = [%f]", max);

      ImGui::Text("Resnet50 categories:");
      ImGui::PlotHistogram("Histogram", output_res50, IM_ARRAYSIZE(output_res50), 0, NULL, 0.0f, 1.0f, ImVec2(0,80));
      ImGui::Separator();

      // Framerate counter.
      ImGui::Text("Application average %.3f ms/frame (%.1f FPS)",
                  1000.0f / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate);

      ImGui::End();
    }

    // Rendering
    ImGui::Render();
    RenderFrame(wd, g_Device, g_Queue);

    PresentFrame(wd, g_Queue);
  }
  // --------------------------------------------------------------------------

  // --------------------------------------------------------------------------
  // Cleanup
  iree_vm_module_release(hal_module);
  iree_vm_module_release(bytecode_module);
  iree_vm_context_release(iree_context);
  iree_hal_device_release(iree_vk_device);
  iree_hal_driver_release(iree_vk_driver);
  iree_hal_vulkan_syms_release(iree_vk_syms);
  iree_vm_instance_release(iree_instance);

  err = vkDeviceWaitIdle(g_Device);
  check_vk_result(err);
  ImGui_ImplVulkan_Shutdown();
  ImGui_ImplSDL2_Shutdown();
  ImGui::DestroyContext();

  CleanupVulkanWindow();
  CleanupVulkan();

  SDL_DestroyWindow(window);
  SDL_Quit();
  // --------------------------------------------------------------------------

  return 0;
}

}  // namespace iree

```

`dataset/README.md`:

```md
# Dataset annotation tool

SHARK annotator for adding or modifying prompts of dataset images

## Set up

Activate SHARK Python virtual environment and install additional packages
```shell
source ../shark.venv/bin/activate
pip install -r requirements.txt
```

## Run annotator

```shell
python annotation_tool.py
```

<img width="1280" alt="annotator" src="https://user-images.githubusercontent.com/49575973/214521137-7ef6ae10-7cd8-46e6-b270-b6c0445157f1.png">

* Select a dataset from `Dataset` dropdown list
* Select an image from `Image` dropdown list
* Image and the existing prompt will be loaded
* Select a prompt from `Prompt` dropdown list to modify or "Add new" to add a prompt
* Click `Save` to save changes, click `Delete` to delete prompt
* Click `Back` or `Next` to switch image, you could also select other images from `Image`
* Click `Finish` when finishing annotation or before switching dataset

```

`dataset/annotation_tool.py`:

```py
import gradio as gr
import json
import jsonlines
import os
from args import args
from pathlib import Path
from PIL import Image
from utils import get_datasets


shark_root = Path(__file__).parent.parent
demo_css = shark_root.joinpath("web/demo.css").resolve()
nodlogo_loc = shark_root.joinpath(
    "web/models/stable_diffusion/logos/nod-logo.png"
)


with gr.Blocks(title="Dataset Annotation Tool", css=demo_css) as shark_web:
    with gr.Row(elem_id="ui_title"):
        nod_logo = Image.open(nodlogo_loc)
        with gr.Column(scale=1, elem_id="demo_title_outer"):
            gr.Image(
                value=nod_logo,
                show_label=False,
                interactive=False,
                elem_id="top_logo",
                width=150,
                height=100,
            )

    datasets, images, ds_w_prompts = get_datasets(args.gs_url)
    prompt_data = dict()

    with gr.Row(elem_id="ui_body"):
        # TODO: add multiselect dataset, there is a gradio version conflict
        dataset = gr.Dropdown(label="Dataset", choices=datasets)
        image_name = gr.Dropdown(label="Image", choices=[])

    with gr.Row(elem_id="ui_body"):
        # TODO: add ability to search image by typing
        with gr.Column(scale=1, min_width=600):
            image = gr.Image(type="filepath", height=512)

        with gr.Column(scale=1, min_width=600):
            prompts = gr.Dropdown(
                label="Prompts",
                choices=[],
            )
            prompt = gr.Textbox(
                label="Editor",
                lines=3,
            )
            with gr.Row():
                save = gr.Button("Save")
                delete = gr.Button("Delete")
            with gr.Row():
                back_image = gr.Button("Back")
                next_image = gr.Button("Next")
            finish = gr.Button("Finish")

    def filter_datasets(dataset):
        if dataset is None:
            return gr.Dropdown.update(value=None, choices=[])

        # create the dataset dir if doesn't exist and download prompt file
        dataset_path = str(shark_root) + "/dataset/" + dataset
        if not os.path.exists(dataset_path):
            os.mkdir(dataset_path)

        # read prompt jsonlines file
        prompt_data.clear()
        if dataset in ds_w_prompts:
            prompt_gs_path = args.gs_url + "/" + dataset + "/metadata.jsonl"
            os.system(f'gsutil cp "{prompt_gs_path}" "{dataset_path}"/')
            with jsonlines.open(dataset_path + "/metadata.jsonl") as reader:
                for line in reader.iter(type=dict, skip_invalid=True):
                    prompt_data[line["file_name"]] = (
                        [line["text"]]
                        if type(line["text"]) is str
                        else line["text"]
                    )

        return gr.Dropdown.update(choices=images[dataset])

    dataset.change(fn=filter_datasets, inputs=dataset, outputs=image_name)

    def display_image(dataset, image_name):
        if dataset is None or image_name is None:
            return gr.Image.update(value=None), gr.Dropdown.update(value=None)

        # download and load the image
        img_gs_path = args.gs_url + "/" + dataset + "/" + image_name
        img_sub_path = "/".join(image_name.split("/")[:-1])
        img_dst_path = (
            str(shark_root) + "/dataset/" + dataset + "/" + img_sub_path + "/"
        )
        if not os.path.exists(img_dst_path):
            os.mkdir(img_dst_path)
        os.system(f'gsutil cp "{img_gs_path}" "{img_dst_path}"')
        img = Image.open(img_dst_path + image_name.split("/")[-1])

        if image_name not in prompt_data.keys():
            prompt_data[image_name] = []
        prompt_choices = ["Add new"]
        prompt_choices += prompt_data[image_name]
        return gr.Image.update(value=img), gr.Dropdown.update(
            choices=prompt_choices
        )

    image_name.change(
        fn=display_image,
        inputs=[dataset, image_name],
        outputs=[image, prompts],
    )

    def edit_prompt(prompts):
        if prompts == "Add new":
            return gr.Textbox.update(value=None)

        return gr.Textbox.update(value=prompts)

    prompts.change(fn=edit_prompt, inputs=prompts, outputs=prompt)

    def save_prompt(dataset, image_name, prompts, prompt):
        if (
            dataset is None
            or image_name is None
            or prompts is None
            or prompt is None
        ):
            return

        if prompts == "Add new":
            prompt_data[image_name].append(prompt)
        else:
            idx = prompt_data[image_name].index(prompts)
            prompt_data[image_name][idx] = prompt

        prompt_path = (
            str(shark_root) + "/dataset/" + dataset + "/metadata.jsonl"
        )
        # write prompt jsonlines file
        with open(prompt_path, "w") as f:
            for key, value in prompt_data.items():
                if not value:
                    continue
                v = value if len(value) > 1 else value[0]
                f.write(json.dumps({"file_name": key, "text": v}))
                f.write("\n")

        prompt_choices = ["Add new"]
        prompt_choices += prompt_data[image_name]
        return gr.Dropdown.update(choices=prompt_choices, value=None)

    save.click(
        fn=save_prompt,
        inputs=[dataset, image_name, prompts, prompt],
        outputs=prompts,
    )

    def delete_prompt(dataset, image_name, prompts):
        if dataset is None or image_name is None or prompts is None:
            return
        if prompts == "Add new":
            return

        prompt_data[image_name].remove(prompts)
        prompt_path = (
            str(shark_root) + "/dataset/" + dataset + "/metadata.jsonl"
        )
        # write prompt jsonlines file
        with open(prompt_path, "w") as f:
            for key, value in prompt_data.items():
                if not value:
                    continue
                v = value if len(value) > 1 else value[0]
                f.write(json.dumps({"file_name": key, "text": v}))
                f.write("\n")

        prompt_choices = ["Add new"]
        prompt_choices += prompt_data[image_name]
        return gr.Dropdown.update(choices=prompt_choices, value=None)

    delete.click(
        fn=delete_prompt,
        inputs=[dataset, image_name, prompts],
        outputs=prompts,
    )

    def get_back_image(dataset, image_name):
        if dataset is None or image_name is None:
            return

        # remove local image
        img_path = str(shark_root) + "/dataset/" + dataset + "/" + image_name
        os.system(f'rm "{img_path}"')
        # get the index for the back image
        idx = images[dataset].index(image_name)
        if idx == 0:
            return gr.Dropdown.update(value=None)

        return gr.Dropdown.update(value=images[dataset][idx - 1])

    back_image.click(
        fn=get_back_image, inputs=[dataset, image_name], outputs=image_name
    )

    def get_next_image(dataset, image_name):
        if dataset is None or image_name is None:
            return

        # remove local image
        img_path = str(shark_root) + "/dataset/" + dataset + "/" + image_name
        os.system(f'rm "{img_path}"')
        # get the index for the next image
        idx = images[dataset].index(image_name)
        if idx == len(images[dataset]) - 1:
            return gr.Dropdown.update(value=None)

        return gr.Dropdown.update(value=images[dataset][idx + 1])

    next_image.click(
        fn=get_next_image, inputs=[dataset, image_name], outputs=image_name
    )

    def finish_annotation(dataset):
        if dataset is None:
            return

        # upload prompt and remove local data
        dataset_path = str(shark_root) + "/dataset/" + dataset
        dataset_gs_path = args.gs_url + "/" + dataset + "/"
        os.system(
            f'gsutil cp "{dataset_path}/metadata.jsonl" "{dataset_gs_path}"'
        )
        os.system(f'rm -rf "{dataset_path}"')

        return gr.Dropdown.update(value=None)

    finish.click(fn=finish_annotation, inputs=dataset, outputs=dataset)


if __name__ == "__main__":
    shark_web.launch(
        share=args.share,
        inbrowser=True,
        server_name="0.0.0.0",
        server_port=args.server_port,
    )

```

`dataset/args.py`:

```py
import argparse

p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)

##############################################################################
### Dataset Annotator flags
##############################################################################

p.add_argument(
    "--gs_url",
    type=str,
    required=True,
    help="URL to datasets in GS bucket",
)

p.add_argument(
    "--share",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="flag for generating a public URL",
)

p.add_argument(
    "--server_port",
    type=int,
    default=8080,
    help="flag for setting server port",
)

##############################################################################

args = p.parse_args()

```

`dataset/requirements.txt`:

```txt
# SHARK Annotator
gradio==3.34.0
jsonlines

```

`dataset/utils.py`:

```py
from google.cloud import storage


def get_datasets(gs_url):
    datasets = set()
    images = dict()
    ds_w_prompts = []

    storage_client = storage.Client()
    bucket_name = gs_url.split("/")[2]
    source_blob_name = "/".join(gs_url.split("/")[3:])
    blobs = storage_client.list_blobs(bucket_name, prefix=source_blob_name)

    for blob in blobs:
        dataset_name = blob.name.split("/")[1]
        if dataset_name == "":
            continue
        datasets.add(dataset_name)
        if dataset_name not in images.keys():
            images[dataset_name] = []

        # check if image or jsonl
        file_sub_path = "/".join(blob.name.split("/")[2:])
        if "/" in file_sub_path:
            images[dataset_name] += [file_sub_path]
        elif "metadata.jsonl" in file_sub_path:
            ds_w_prompts.append(dataset_name)

    return list(datasets), images, ds_w_prompts

```

`docs/shark_iree_profiling.md`:

```md
# Overview

This document is intended to provide a starting point for profiling with SHARK/IREE. At it's core
[SHARK](https://github.com/nod-ai/SHARK/tree/main/tank) is a python API that links the MLIR lowerings from various
frameworks + frontends (e.g. PyTorch -> Torch-MLIR) with the compiler + runtime offered by IREE. More information
on model coverage and framework support can be found [here](https://github.com/nod-ai/SHARK/tree/main/tank). The intended
use case for SHARK is for compilation and deployment of performant state of the art AI models.

![image](https://user-images.githubusercontent.com/22101546/217151219-9bb184a3-cfb9-4788-bb7e-5b502953525c.png)

## Benchmarking with SHARK

TODO: Expand this section.

SHARK offers native benchmarking support, although because it is model focused, fine grain profiling is
hidden when compared against the common "model benchmarking suite" use case SHARK is good at.

### SharkBenchmarkRunner

SharkBenchmarkRunner is a class designed for benchmarking models against other runtimes.
TODO: List supported runtimes for comparison + example on how to benchmark with it.

## Directly profiling IREE

A number of excellent developer resources on profiling with IREE can be
found [here](https://github.com/iree-org/iree/tree/main/docs/developers/developing_iree). As a result this section will
focus on the bridging the gap between the two.
 - https://github.com/iree-org/iree/blob/main/docs/developers/developing_iree/profiling.md
 - https://github.com/iree-org/iree/blob/main/docs/developers/developing_iree/profiling_with_tracy.md
 - https://github.com/iree-org/iree/blob/main/docs/developers/developing_iree/profiling_vulkan_gpu.md
 - https://github.com/iree-org/iree/blob/main/docs/developers/developing_iree/profiling_cpu_events.md

Internally, SHARK builds a pair of IREE commands to compile + run a model. At a high level the flow starts with the
model represented with a high level dialect (commonly Linalg) and is compiled to a flatbuffer (.vmfb) that
the runtime is capable of ingesting. At this point (with potentially a few runtime flags) the compiled model is then run
through the IREE runtime. This is all facilitated with the IREE python bindings, which offers a convenient method
to capture the compile command SHARK comes up with. This is done by setting the environment variable
`IREE_SAVE_TEMPS` to point to a directory of choice, e.g. for stable diffusion
```
# Linux
$ export IREE_SAVE_TEMPS=/path/to/some/directory
# Windows
$ $env:IREE_SAVE_TEMPS="C:\path\to\some\directory"
$ python apps/stable_diffusion/scripts/txt2img.py -p "a photograph of an astronaut riding a horse" --save_vmfb
```
NOTE: Currently this will only save the compile command + input MLIR for a single model if run in a pipeline.
In the case of stable diffusion this (should) be UNet so to get examples for other models in the pipeline they
need to be extracted and tested individually.

The save temps directory should contain three files: `core-command-line.txt`, `core-input.mlir`, and `core-output.bin`.
The command line for compilation will start something like this, where the `-` needs to be replaced with the path to `core-input.mlir`.
```
/home/quinn/nod/iree-build/compiler/bindings/python/iree/compiler/tools/../_mlir_libs/iree-compile - --iree-input-type=none ...
```
The `-o output_filename.vmfb` flag can be used to specify the location to save the compiled vmfb. Note that a dump of the
dispatches that can be compiled + run in isolation can be generated by adding `--iree-hal-dump-executable-benchmarks-to=/some/directory`. Say, if they are in the `benchmarks` directory, the following compile/run commands would work for Vulkan on RDNA3.
```
iree-compile --iree-input-type=none --iree-hal-target-backends=vulkan --iree-vulkan-target-triple=rdna3-unknown-linux  benchmarks/module_forward_dispatch_${NUM}_vulkan_spirv_fb.mlir -o benchmarks/module_forward_dispatch_${NUM}_vulkan_spirv_fb.vmfb

iree-benchmark-module --module=benchmarks/module_forward_dispatch_${NUM}_vulkan_spirv_fb.vmfb --function=forward --device=vulkan
```
Where `${NUM}` is the dispatch number that you want to benchmark/profile in isolation.

### Enabling Tracy for Vulkan profiling

To begin profiling with Tracy, a build of IREE runtime with tracing enabled is needed. SHARK-Runtime (SRT) builds an
instrumented version alongside the normal version nightly (.whls typically found [here](https://github.com/nod-ai/SRT/releases)), however this is only available for Linux. For Windows, tracing can be enabled by enabling a CMake flag.
```
$env:IREE_ENABLE_RUNTIME_TRACING="ON"
```
Getting a trace can then be done by setting environment variable `TRACY_NO_EXIT=1` and running the program that is to be
traced. Then, to actually capture the trace, use the `iree-tracy-capture` tool in a different terminal. Note that to get
the capture and profiler tools the `IREE_BUILD_TRACY=ON` CMake flag needs to be set.
```
TRACY_NO_EXIT=1 python apps/stable_diffusion/scripts/txt2img.py -p "a photograph of an astronaut riding a horse"

# (in another terminal, either on the same machine or through ssh with a tunnel through port 8086)
iree-tracy-capture -o trace_filename.tracy
```
To do it over ssh, the flow looks like this
```
# From terminal 1 on local machine
ssh -L 8086:localhost:8086 <remote_server_name>
TRACY_NO_EXIT=1 python apps/stable_diffusion/scripts/txt2img.py -p "a photograph of an astronaut riding a horse"

# From terminal 2 on local machine. Requires having built IREE with the CMake flag `IREE_BUILD_TRACY=ON` to build the required tooling.
iree-tracy-capture -o /path/to/trace.tracy
```

The trace can then be viewed with
```
iree-tracy-profiler /path/to/trace.tracy
```
Capturing a runtime trace will work with any IREE tooling that uses the runtime. For example, `iree-benchmark-module`
can be used for benchmarking an individual module. Importantly this means that any SHARK script can be profiled with tracy.

NOTE: Not all backends have the same tracy support. This writeup is focused on CPU/Vulkan backends but there is recently added support for tracing on CUDA (requires the `--cuda_tracing` flag).

## Experimental RGP support

TODO: This section is temporary until proper RGP support is added.

Currently, for stable diffusion there is a flag for enabling UNet to be visible to RGP with `--enable_rgp`. To get a proper capture though, the `DevModeSqttPrepareFrameCount=1` flag needs to be set for the driver (done with `VkPanel` on Windows).
With these two settings, a single iteration of UNet can be captured.

(AMD only) To get a dump of the pipelines (result of compiled SPIR-V) the `EnablePipelineDump=1` driver flag can be set. The
files will typically be dumped to a directory called `spvPipeline` (on Linux `/var/tmp/spvPipeline`. The dumped files will
include header information that can be used to map back to the source dispatch/SPIR-V, e.g.
```
[Version]
version = 57 

[CsSpvFile]
fileName = Shader_0x946C08DFD0C10D9A.spv

[CsInfo]
entryPoint = forward_dispatch_193_matmul_256x65536x2304
```

```

`docs/shark_sd_blender.md`:

```md
# Overview

This document is intended to provide a starting point for using SHARK stable diffusion with Blender. 

We currently make use of the [AI-Render Plugin](https://github.com/benrugg/AI-Render) to integrate with Blender.

## Setup SHARK and prerequisites:

 * Download the latest SHARK SD webui .exe from [here](https://github.com/nod-ai/SHARK/releases) or follow instructions on the [README](https://github.com/nod-ai/SHARK#readme)
 * Once you have the .exe where you would like SHARK to install, run the .exe from terminal/PowerShell with the `--api` flag:
```
## Run the .exe in API mode:
.\shark_sd_<date>_<ver>.exe --api

## For example:
.\shark_sd_20230411_671.exe --api --server_port=8082

## From a the base directory of a source clone of SHARK:
./setup_venv.ps1
python apps\stable_diffusion\web\index.py --api

```

Your local SD server should start and look something like this:
![image](https://user-images.githubusercontent.com/87458719/231369758-e2c3c45a-eccc-4fe5-a788-4a3bf1ace1d1.png)

 * Note: When running in api mode with `--api`, the .exe will not function as a webUI. Thus, the address in the terminal output will only be useful for API requests.

### Install AI Render

- Get AI Render on [Blender Market](https://blendermarket.com/products/ai-render) or [Gumroad](https://airender.gumroad.com/l/ai-render)
- Open Blender, then go to Edit > Preferences > Add-ons > Install and then find the zip file
- We will be using the Automatic1111 SD backend for the AI-Render plugin. Follow instructions [here](https://github.com/benrugg/AI-Render/wiki/Local-Installation) to setup local SD backend.

Your AI-Render preferences should be configured as shown; the highlighted part should match your terminal output:
![image](https://user-images.githubusercontent.com/87458719/231390322-59a54a09-520a-4a08-b658-6e37bd63e932.png)


The [AI-Render README](https://github.com/benrugg/AI-Render/blob/main/README.md) has more details on installation and usage, as well as video tutorials.

## Using AI-Render + SHARK in your Blender project

- In the Render Properties tab, in the AI-Render dropdown, enable AI-Render.

![image](https://user-images.githubusercontent.com/87458719/231392843-9bd51744-3ce2-464e-843a-0c4d4c96df0c.png)

- Select an image size (it's usually better to upscale later than go high on the img2img resolution here.)

![image](https://user-images.githubusercontent.com/87458719/231394288-0c4ab8c5-dc30-4dbe-8bc1-7520ded5efe8.png)

- From here, you can enter a prompt and configure img2img Stable Diffusion parameters, and AI-Render will run SHARK SD img2img on the rendered scene.
- AI-Render has useful presets for aesthetic styles, so you should be able to keep your subject prompt simple and focus on creating a decent Blender scene to start from.

![image](https://user-images.githubusercontent.com/87458719/231440729-2fe69586-41cb-4274-9ce7-f6c08def600b.png)

## Examples:
Scene (Input image):

![blender-sample-2](https://user-images.githubusercontent.com/87458719/231450408-0e680086-3e52-4962-a5c1-c703a94d1583.png)

Prompt:
"A bowl of tangerines in front of rocks, masterpiece, oil on canvas, by Georgia O'Keefe, trending on artstation, landscape painting by Caspar David Friedrich"

Negative Prompt (default):
"ugly, bad art, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, tiling, signature, cut off, draft"

Example output:

![blender-sample-2_out](https://user-images.githubusercontent.com/87458719/231451145-a0b56897-a7d0-4add-bbed-7e8af21a65df.png)







```

`docs/shark_sd_koboldcpp.md`:

```md
# Overview

In [1.47.2](https://github.com/LostRuins/koboldcpp/releases/tag/v1.47.2) [Koboldcpp](https://github.com/LostRuins/koboldcpp) added AUTOMATIC1111 integration for image generation. Since SHARK implements a small subset of the A1111 REST api, you can also use SHARK for this. This document gives a starting point for how to get this working.

## In Action

![preview](https://user-images.githubusercontent.com/121311569/280557602-bb97bad0-fdf5-4922-a2cc-4f327f2760db.jpg)

## Memory considerations

Since both Koboldcpp and SHARK will use VRAM on your graphic card(s) running both at the same time using the same card will impose extra limitations on the model size you can fully offload to the video card in Koboldcpp. For me, on a RX 7900 XTX on Windows with 24 GiB of VRAM, the limit was about a 13 Billion parameter model with Q5_K_M quantisation.

## Performance Considerations

When using SHARK for image generation, especially with Koboldcpp, you need to be aware that it is currently designed to pay a large upfront cost in time compiling and tuning the model you select, to get an optimal individual image generation time. You need to be the judge as to whether this trade-off is going to be worth it for your OS and hardware combination.

It means that the first time you run a particular Stable Diffusion model for a particular combination of image size, LoRA, and VAE, SHARK will spend *many minutes* - even on a beefy machaine with very fast graphics card with lots of memory - building that model combination just so it can save it to disk. It may even have to go away and download the model if it doesn't already have it locally. Once it has done its build of a model combination for your hardware once, it shouldn't need to do it again until you upgrade to a newer SHARK version, install different drivers or change your graphics hardware. It will just upload the files it generated the first time to your graphics card and proceed from there.

This does mean however, that on a brand new fresh install of SHARK that has not generated any images on a model you haven't selected before, the first image Koboldcpp requests may look like it is *never* going finish and that the whole process has broken. Be forewarned, make yourself a cup of coffee, and expect a lot of messages about compilation and tuning from SHARK in the terminal you ran it from.

## Setup SHARK and prerequisites:

 * Make sure you have suitable drivers for your graphics card installed. See the prerequisties section of the [README](https://github.com/nod-ai/SHARK#readme).
 * Download the latest SHARK studio .exe from [here](https://github.com/nod-ai/SHARK/releases) or follow the instructions in the [README](https://github.com/nod-ai/SHARK#readme) for an advanced, Linux or Mac install.
 * Run SHARK from terminal/PowerShell with the `--api` flag. Since koboldcpp also expects both CORS support and the image generator to be running on port `7860` rather than SHARK default of `8080`, also include both the `--api_accept_origin` flag with a suitable origin (use `="*"` to enable all origins) and `--server_port=7860` on the command line. (See the if you want to run SHARK on a different port)

```powershell
## Run the .exe in API mode, with CORS support, on the A1111 endpoint port:
.\node_ai_shark_studio_<date>_<ver>.exe --api --api_accept_origin="*"  --server_port=7860

## Run trom the base directory of a source clone of SHARK on Windows:
.\setup_venv.ps1
python .\apps\stable_diffusion\web\index.py --api --api_accept_origin="*"  --server_port=7860

## Run a the base directory of a source clone of SHARK on Linux:
./setup_venv.sh
source shark.venv/bin/activate
python ./apps/stable_diffusion/web/index.py --api --api_accept_origin="*"  --server_port=7860

## An example giving improved performance on AMD cards using vulkan, that runs on the same port as A1111
.\node_ai_shark_studio_20320901_2525.exe --api --api_accept_origin="*" --device_allocator="caching" --server_port=7860

## Since the api respects most applicable SHARK command line arguments for options not specified,
## or currently unimplemented by API, there might be some you want to set, as listed in `--help`
.\node_ai_shark_studio_20320901_2525.exe --help

## For instance, the example above, but with a a custom VAE specified
.\node_ai_shark_studio_20320901_2525.exe --api --api_accept_origin="*" --device_allocator="caching" --server_port=7860 --custom_vae="clearvae_v23.safetensors"

## An example with multiple specific CORS origins
python apps/stable_diffusion/web/index.py --api --api_accept_origin="koboldcpp.example.com:7001" --api_accept_origin="koboldcpp.example.com:7002" --server_port=7860
```

SHARK should start in server mode, and you should see something like this:

![SHARK API startup](https://user-images.githubusercontent.com/121311569/280556294-c3f7fc1a-c8e2-467d-afe6-365638d6823a.png)

* Note: When running in api mode with `--api`, the .exe will not function as a webUI. Thus, the address or port shown in the terminal output will only be useful for API requests.


## Configure Koboldcpp for local image generation:

* Get the latest [Koboldcpp](https://github.com/LostRuins/koboldcpp/releases) if you don't already have it. If you have a recent AMD card that has ROCm HIP [support for Windows](https://rocmdocs.amd.com/en/latest/release/windows_support.html#windows-supported-gpus) or [support for Linux](https://rocmdocs.amd.com/en/latest/release/gpu_os_support.html#linux-supported-gpus), you'll likely prefer [YellowRosecx's ROCm fork](https://github.com/YellowRoseCx/koboldcpp-rocm).
* Start Koboldcpp in another terminal/Powershell and setup your model configuration. Refer to the [Koboldcpp README](https://github.com/YellowRoseCx/koboldcpp-rocm) for more details on how to do this if this is your first time using Koboldcpp.
* Once the main UI has loaded into your browser click the settings button, go to the advanced tab, and then choose *Local A1111* from the generate images dropdown:

  ![Settings button location](https://user-images.githubusercontent.com/121311569/280556246-10692d79-e89f-4fdf-87ba-82f3d78ed49d.png)

  ![Advanced Settings with 'Local A1111' location](https://user-images.githubusercontent.com/121311569/280556234-6ebc8ba7-1469-442a-93a7-5626a094ddf1.png)

  *if you get an error here, see the next section [below](#connecting-to-shark-on-a-different-address-or-port)*

* A list of Stable Diffusion models available to your SHARK instance should now be listed in the box below *generate images*. The default value will usually be set to `stabilityai/stable-diffusion-2-1-base`. Choose the model you want to use for image generation from the list (but see [performance considerations](#performance-considerations)).
* You should now be ready to generate images, either by clicking the 'Add Img' button above the text entry box:

  ![Add Image Button](https://user-images.githubusercontent.com/121311569/280556161-846c7883-4a83-4458-a56a-bd9f93ca354c.png)

  ...or by selecting the 'Autogenerate' option in the settings:

  ![Setting the autogenerate images option](https://user-images.githubusercontent.com/121311569/280556230-ae221a46-ba68-499b-a519-c8f290bbbeae.png)

  *I often find that even if I have selected autogenerate I have to do an 'add img' to get things started off*

* There is one final piece of image generation configuration within Koboldcpp you might want to do. This is also in the generate images section of advanced settings. Here there is, not very obviously, a 'style' button:

  ![Selecting the 'styles' button](https://user-images.githubusercontent.com/121311569/280556694-55cd1c55-a059-4b54-9293-63d66a32368e.png)

  This will bring up a dialog box where you can enter a short text that will sent as a prefix to the Prompt sent to SHARK:

  ![Entering extra image styles](https://user-images.githubusercontent.com/121311569/280556172-4aab9794-7a77-46d7-bdda-43df570ad19a.png)


## Connecting to SHARK on a different address or port

If you didn't set the port to `--server_port=7860` when starting SHARK, or you are running it on different machine on your network than you are running Koboldcpp, or to where you are running the koboldcpp's kdlite client frontend, then you very likely got the following error:

  ![Can't find the A1111 endpoint error](https://user-images.githubusercontent.com/121311569/280555857-601f53dc-35e9-4027-9180-baa61d2393ba.png)

As long as SHARK is running correctly, this means you need to set the url and port to the correct values in Koboldcpp. For instance. to set the port that Koboldcpp looks for an image generator to SHARK's default port of 8080:

* Select the cog icon the Generate Images section of Advanced settings:

     ![Selecting the endpoint cog](https://user-images.githubusercontent.com/121311569/280555866-4287ecc5-f29f-4c03-8f5a-abeaf31b0442.png)

* Then edit the port number at the end of the url in the 'A1111 Endpoint Selection' dialog box to read 8080:

     ![Changing the endpoint port](https://user-images.githubusercontent.com/121311569/280556170-f8848b7b-6fc9-4cf7-80eb-5c312f332fd9.png)

* Similarly, when running SHARK on a different machine you will need to change host part of the endpoint url to the hostname or ip address where SHARK is running, similarly:

    ![Changing the endpoint hostname](https://user-images.githubusercontent.com/121311569/280556167-c6541dea-0f85-417a-b661-fdf4dc40d05f.png)

## Examples

Here's how Koboldcpp shows an image being requested:

  ![An image being generated]((https://user-images.githubusercontent.com/121311569/280556210-bb1c9efd-79ac-478e-b726-b25b82ef2186.png)

The generated image in context in story mode:

 ![A generated image](https://user-images.githubusercontent.com/121311569/280556179-4e9f3752-f349-4cba-bc6a-f85f8dc79b10.jpg)

And the same image when clicked on:

 ![A selected image](https://user-images.githubusercontent.com/121311569/280556216-2ca4c0a4-3889-4ef5-8a09-30084fb34081.jpg)


## Where to find the images in SHARK

Even though Koboldcpp requests images at a size of 512x512, it resizes then to 256x256, converts them to `.jpeg`, and only shows them at 200x200 in the main text window. It does this so it can save them compactly embedded in your story as a `data://` uri.

However the images at the original size are saved by SHARK in its `output_dir` which is usually a folder named for the current date. inside `generated_imgs` folder in the SHARK installation directory.

You can browse these, either using the Output Gallery tab from within the SHARK web ui:

  ![SHARK web ui output gallery tab](https://user-images.githubusercontent.com/121311569/280556582-9303ca85-2594-4a8c-97a2-fbd72337980b.jpg)

...or by browsing to the `output_dir` in your operating system's file manager:

  ![SHARK output directory subfolder in Windows File Explorer](https://user-images.githubusercontent.com/121311569/280556297-66173030-2324-415c-a236-ef3fcd73e6ed.jpg)

```

`process_skipfiles.py`:

```py
# This script will toggle the comment/uncommenting aspect for dealing
# with __file__ AttributeError arising in case of a few modules in
# `torch/_dynamo/skipfiles.py` (within shark.venv)

from distutils.sysconfig import get_python_lib
import fileinput
from pathlib import Path

# Temporary workaround for transformers/__init__.py.
path_to_transformers_hook = Path(
    get_python_lib()
    + "/_pyinstaller_hooks_contrib/hooks/stdhooks/hook-transformers.py"
)
if path_to_transformers_hook.is_file():
    pass
else:
    with open(path_to_transformers_hook, "w") as f:
        f.write("module_collection_mode = 'pyz+py'")

path_to_skipfiles = Path(get_python_lib() + "/torch/_dynamo/skipfiles.py")

modules_to_comment = ["abc,", "os,", "posixpath,", "_collections_abc,"]
startMonitoring = 0
for line in fileinput.input(path_to_skipfiles, inplace=True):
    if "SKIP_DIRS = " in line:
        startMonitoring = 1
        print(line, end="")
    elif startMonitoring in [1, 2]:
        if "]" in line:
            startMonitoring += 1
            print(line, end="")
        else:
            flag = True
            for module in modules_to_comment:
                if module in line:
                    if not line.startswith("#"):
                        print(f"#{line}", end="")
                    else:
                        print(f"{line[1:]}", end="")
                    flag = False
                    break
            if flag:
                print(line, end="")
    else:
        print(line, end="")

# For getting around scikit-image's packaging, laze_loader has had a patch merged but yet to be released.
# Refer: https://github.com/scientific-python/lazy_loader
path_to_lazy_loader = Path(get_python_lib() + "/lazy_loader/__init__.py")

for line in fileinput.input(path_to_lazy_loader, inplace=True):
    if 'stubfile = filename if filename.endswith("i")' in line:
        print(
            '    stubfile = (filename if filename.endswith("i") else f"{os.path.splitext(filename)[0]}.pyi")',
            end="",
        )
    else:
        print(line, end="")

# For getting around timm's packaging.
# Refer: https://github.com/pyinstaller/pyinstaller/issues/5673#issuecomment-808731505
path_to_timm_activations = Path(
    get_python_lib() + "/timm/layers/activations_jit.py"
)
for line in fileinput.input(path_to_timm_activations, inplace=True):
    if "@torch.jit.script" in line:
        print("@torch.jit._script_if_tracing", end="\n")
    else:
        print(line, end="")

```

`pyproject.toml`:

```toml
[build-system]
requires = [
    "setuptools>=42",
    "wheel",
    "packaging",

    "numpy>=1.22.4",
    "torch-mlir>=20230620.875",
    "iree-compiler>=20221022.190",
    "iree-runtime>=20221022.190",
]
build-backend = "setuptools.build_meta"

[tool.black]
line-length = 79
include = '\.pyi?$'
exclude = "apps/language_models/scripts/vicuna.py"
extend-exclude = "apps/language_models/src/pipelines/minigpt4_pipeline.py"

```

`pytest.ini`:

```ini
[pytest]
addopts = --verbose -s -p no:warnings
norecursedirs = inference tank/tflite examples benchmarks shark 

```

`requirements-importer-macos.txt`:

```txt
-f https://download.pytorch.org/whl/nightly/cpu/
--pre

numpy
torch
torchvision

tqdm

#iree-compiler  | iree-runtime should already be installed

transformers
#jax[cpu]

# tflitehub dependencies.
Pillow

# web dependecies.
gradio
altair

# Testing and support.
#lit
#pyyaml

#ONNX and ORT for benchmarking
#--extra-index-url https://test.pypi.org/simple/
#protobuf
#coloredlogs
#flatbuffers
#sympy
#psutil
#onnx-weekly
#ort-nightly

```

`requirements-importer.txt`:

```txt
-f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
--pre

numpy>1.22.4
pytorch-triton
torchvision 
tabulate

tqdm

#iree-compiler  | iree-runtime should already be installed
iree-tools-xla

# Modelling and JAX.
gin-config
transformers
diffusers
#jax[cpu]
Pillow

# Testing and support.
lit
pyyaml
python-dateutil
sacremoses
sentencepiece

# web dependecies.
gradio==3.44.3
altair
scipy

#ONNX and ORT for benchmarking
#--extra-index-url https://test.pypi.org/simple/
#protobuf
#coloredlogs
#flatbuffers
#sympy
#psutil
#onnx-weekly
#ort-nightly

```

`requirements.txt`:

```txt
-f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
--pre

setuptools
wheel

# SHARK Runner
tqdm

# SHARK Downloader
google-cloud-storage

# Testing
pytest
pytest-xdist
pytest-forked
Pillow
parameterized

#shark-turbine @ git+https://github.com/nod-ai/SHARK-Turbine.git@main
# Add transformers, diffusers and scipy since it most commonly used
tokenizers==0.13.3
transformers
diffusers
#accelerate is now required for diffusers import from ckpt.
accelerate
scipy
ftfy
gradio==3.44.3
altair
omegaconf
# 0.3.2 doesn't have binaries for arm64
safetensors==0.3.1
opencv-python
scikit-image
pytorch_lightning # for runwayml models
tk
pywebview
sentencepiece
py-cpuinfo
tiktoken # for codegen
joblib # for langchain
timm # for MiniGPT4
langchain
einops # for zoedepth
pydantic==2.4.1 # pin until pyinstaller-hooks-contrib works with beta versions

# Keep PyInstaller at the end. Sometimes Windows Defender flags it but most folks can continue even if it errors
pefile
pyinstaller

# vicuna quantization
brevitas @ git+https://github.com/Xilinx/brevitas.git@56edf56a3115d5ac04f19837b388fd7d3b1ff7ea

# For quantized GPTQ models
optimum
auto_gptq

```

`rest_api_tests/api_test.py`:

```py
import requests
from PIL import Image
import base64
from io import BytesIO


def upscaler_test(verbose=False):
    # Define values here
    prompt = ""
    negative_prompt = ""
    seed = 2121991605
    height = 512
    width = 512
    steps = 50
    noise_level = 10
    cfg_scale = 7
    image_path = r"./rest_api_tests/dog.png"

    # Converting Image to base64
    img_file = open(image_path, "rb")
    init_images = [
        "data:image/png;base64," + base64.b64encode(img_file.read()).decode()
    ]

    url = "http://127.0.0.1:8080/sdapi/v1/upscaler"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "seed": seed,
        "height": height,
        "width": width,
        "steps": steps,
        "noise_level": noise_level,
        "cfg_scale": cfg_scale,
        "init_images": init_images,
    }

    res = requests.post(url=url, json=data, headers=headers, timeout=1000)

    print(
        f"[upscaler] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(
            f"\n{res.json()['info'] if res.status_code == 200 else res.content}\n"
        )


def img2img_test(verbose=False):
    # Define values here
    prompt = "Paint a rabbit riding on the dog"
    negative_prompt = "ugly, bad art, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, tiling, signature, cut off, draft"
    seed = 2121991605
    height = 512
    width = 512
    steps = 50
    denoising_strength = 0.75
    cfg_scale = 7
    image_path = r"./rest_api_tests/dog.png"

    # Converting Image to Base64
    img_file = open(image_path, "rb")
    init_images = [
        "data:image/png;base64," + base64.b64encode(img_file.read()).decode()
    ]

    url = "http://127.0.0.1:8080/sdapi/v1/img2img"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "init_images": init_images,
        "height": height,
        "width": width,
        "steps": steps,
        "denoising_strength": denoising_strength,
        "cfg_scale": cfg_scale,
        "seed": seed,
    }

    res = requests.post(url=url, json=data, headers=headers, timeout=1000)

    res = requests.post(url=url, json=data, headers=headers, timeout=1000)

    print(
        f"[img2img] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(
            f"\n{res.json()['info'] if res.status_code == 200 else res.content}\n"
        )

    # NOTE Uncomment below to save the picture

    # print("Extracting response object")
    # response_obj = res.json()
    # img_b64 = response_obj.get("images", [False])[0] or response_obj.get(
    #     "image"
    # )
    # img_b2 = base64.b64decode(img_b64.replace("data:image/png;base64,", ""))
    # im_file = BytesIO(img_b2)
    # response_img = Image.open(im_file)
    # print("Saving Response Image to: response_img")
    # response_img.save(r"rest_api_tests/response_img.png")


def inpainting_test(verbose=False):
    prompt = "Paint a rabbit riding on the dog"
    negative_prompt = "ugly, bad art, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, tiling, signature, cut off, draft"
    seed = 2121991605
    height = 512
    width = 512
    steps = 50
    noise_level = 10
    cfg_scale = 7
    is_full_res = False
    full_res_padding = 32
    image_path = r"./rest_api_tests/dog.png"

    img_file = open(image_path, "rb")
    image = (
        "data:image/png;base64," + base64.b64encode(img_file.read()).decode()
    )
    img_file = open(image_path, "rb")
    mask = (
        "data:image/png;base64," + base64.b64encode(img_file.read()).decode()
    )

    url = "http://127.0.0.1:8080/sdapi/v1/inpaint"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "image": image,
        "mask": mask,
        "height": height,
        "width": width,
        "steps": steps,
        "noise_level": noise_level,
        "cfg_scale": cfg_scale,
        "seed": seed,
        "is_full_res": is_full_res,
        "full_res_padding": full_res_padding,
    }

    res = requests.post(url=url, json=data, headers=headers, timeout=1000)

    print(
        f"[inpaint] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(
            f"\n{res.json()['info'] if res.status_code == 200 else res.content}\n"
        )


def outpainting_test(verbose=False):
    prompt = "Paint a rabbit riding on the dog"
    negative_prompt = "ugly, bad art, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, tiling, signature, cut off, draft"
    seed = 2121991605
    height = 512
    width = 512
    steps = 50
    cfg_scale = 7
    color_variation = 0.2
    noise_q = 0.2
    directions = ["up", "down", "right", "left"]
    pixels = 32
    mask_blur = 64
    image_path = r"./rest_api_tests/dog.png"

    # Converting Image to Base64
    img_file = open(image_path, "rb")
    init_images = [
        "data:image/png;base64," + base64.b64encode(img_file.read()).decode()
    ]

    url = "http://127.0.0.1:8080/sdapi/v1/outpaint"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "seed": seed,
        "height": height,
        "width": width,
        "steps": steps,
        "cfg_scale": cfg_scale,
        "color_variation": color_variation,
        "noise_q": noise_q,
        "directions": directions,
        "pixels": pixels,
        "mask_blur": mask_blur,
        "init_images": init_images,
    }

    res = requests.post(url=url, json=data, headers=headers, timeout=1000)

    print(
        f"[outpaint] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(
            f"\n{res.json()['info'] if res.status_code == 200 else res.content}\n"
        )


def txt2img_test(verbose=False):
    prompt = "Paint a rabbit in a top hate"
    negative_prompt = "ugly, bad art, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, tiling, signature, cut off, draft"
    seed = 2121991605
    height = 512
    width = 512
    steps = 50
    cfg_scale = 7

    url = "http://127.0.0.1:8080/sdapi/v1/txt2img"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "seed": seed,
        "height": height,
        "width": width,
        "steps": steps,
        "cfg_scale": cfg_scale,
    }

    res = requests.post(url=url, json=data, headers=headers, timeout=1000)

    print(
        f"[txt2img] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(
            f"\n{res.json()['info'] if res.status_code == 200 else res.content}\n"
        )


def sd_models_test(verbose=False):
    url = "http://127.0.0.1:8080/sdapi/v1/sd-models"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    res = requests.get(url=url, headers=headers, timeout=1000)

    print(
        f"[sd_models] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(f"\n{res.json() if res.status_code == 200 else res.content}\n")


def sd_samplers_test(verbose=False):
    url = "http://127.0.0.1:8080/sdapi/v1/samplers"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    res = requests.get(url=url, headers=headers, timeout=1000)

    print(
        f"[sd_samplers] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(f"\n{res.json() if res.status_code == 200 else res.content}\n")


def options_test(verbose=False):
    url = "http://127.0.0.1:8080/sdapi/v1/options"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    res = requests.get(url=url, headers=headers, timeout=1000)

    print(
        f"[options] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(f"\n{res.json() if res.status_code == 200 else res.content}\n")


def cmd_flags_test(verbose=False):
    url = "http://127.0.0.1:8080/sdapi/v1/cmd-flags"

    headers = {
        "User-Agent": "PythonTest",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
    }

    res = requests.get(url=url, headers=headers, timeout=1000)

    print(
        f"[cmd-flags] response from server was : {res.status_code} {res.reason}"
    )

    if verbose or res.status_code != 200:
        print(f"\n{res.json() if res.status_code == 200 else res.content}\n")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description=(
            "Exercises the Stable Diffusion REST API of Shark. Make sure "
            "Shark is running in API mode on 127.0.0.1:8080 before running"
            "this script."
        ),
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help=(
            "also display selected info from the JSON response for "
            "successful requests"
        ),
    )
    args = parser.parse_args()

    sd_models_test(args.verbose)
    sd_samplers_test(args.verbose)
    options_test(args.verbose)
    cmd_flags_test(args.verbose)
    txt2img_test(args.verbose)
    img2img_test(args.verbose)
    upscaler_test(args.verbose)
    inpainting_test(args.verbose)
    outpainting_test(args.verbose)

```

`setup.py`:

```py
from setuptools import find_packages
from setuptools import setup

import os
import glob

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

PACKAGE_VERSION = os.environ.get("SHARK_PACKAGE_VERSION") or "0.0.5"
backend_deps = []
if "NO_BACKEND" in os.environ.keys():
    backend_deps = [
        "iree-compiler>=20221022.190",
        "iree-runtime>=20221022.190",
    ]

setup(
    name="nodai-SHARK",
    version=f"{PACKAGE_VERSION}",
    description="SHARK provides a High Performance Machine Learning Framework",
    author="nod.ai",
    author_email="stdin@nod.ai",
    url="https://nod.ai",
    long_description=long_description,
    long_description_content_type="text/markdown",
    project_urls={
        "Code": "https://github.com/nod-ai/SHARK",
        "Bug Tracker": "https://github.com/nod-ai/SHARK/issues",
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    packages=find_packages(exclude=("examples")),
    python_requires=">=3.9",
    data_files=glob.glob("apps/stable_diffusion/resources/**"),
    install_requires=[
        "numpy",
        "PyYAML",
        "torch-mlir",
    ]
    + backend_deps,
)

```

`setup_venv.ps1`:

```ps1
<#
.SYNOPSIS
  A script to update and install the SHARK runtime and its dependencies.

.DESCRIPTION
  This script updates and installs the SHARK runtime and its dependencies.
  It checks the Python version installed and installs any required build
  dependencies into a Python virtual environment.
  If that environment does not exist, it creates it.
  
.PARAMETER update-src
  git pulls latest version

.PARAMETER force
  removes and recreates venv to force update of all dependencies
  
.EXAMPLE
  .\setup_venv.ps1 --force

.EXAMPLE
  .\setup_venv.ps1 --update-src

.INPUTS
  None

.OUTPUTS
  None

#>

param([string]$arguments)

if ($arguments -eq "--update-src"){
	git pull
}

if ($arguments -eq "--force"){
	if (Test-Path env:VIRTUAL_ENV) {
        Write-Host "deactivating..."
        Deactivate
    }
    
    if (Test-Path .\shark.venv\) {
        Write-Host "removing and recreating venv..."
        Remove-Item .\shark.venv -Force -Recurse
        if (Test-Path .\shark.venv\) {
            Write-Host 'could not remove .\shark-venv - please try running ".\setup_venv.ps1 --force" again!'
            exit 1
        }
    }
}

# redirect stderr into stdout
$p = &{python -V} 2>&1
# check if an ErrorRecord was returned
$version = if($p -is [System.Management.Automation.ErrorRecord])
{
    # grab the version string from the error message
    $p.Exception.Message
}
else
{
    # otherwise return complete Python list
    $ErrorActionPreference = 'SilentlyContinue'
    $PyVer = py --list
}

# deactivate any activated venvs
if ($PyVer -like "*venv*")
{
  deactivate # make sure we don't update the wrong venv
  $PyVer = py --list # update list
}

Write-Host "Python versions found are"
Write-Host ($PyVer | Out-String) # formatted output with line breaks
if (!($PyVer.length -ne 0)) {$p} # return Python --version String if py.exe is unavailable
if (!($PyVer -like "*3.11*") -and !($p -like "*3.11*")) # if 3.11 is not in any list
{
    Write-Host "Please install Python 3.11 and try again"
    exit 34
}

Write-Host "Installing Build Dependencies"
# make sure we really use 3.11 from list, even if it's not the default.
if ($NULL -ne $PyVer) {py -3.11 -m venv .\shark.venv\}
else {python -m venv .\shark.venv\}
.\shark.venv\Scripts\activate
python -m pip install --upgrade pip
pip install wheel
pip install -r requirements.txt
pip install --pre torch-mlir torchvision torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu -f https://llvm.github.io/torch-mlir/package-index/
pip install --upgrade -f https://nod-ai.github.io/SRT/pip-release-links.html iree-compiler iree-runtime
Write-Host "Building SHARK..."
pip install -e . -f https://llvm.github.io/torch-mlir/package-index/ -f https://nod-ai.github.io/SRT/pip-release-links.html
Write-Host "Build and installation completed successfully"
Write-Host "Source your venv with ./shark.venv/Scripts/activate"

```

`setup_venv.sh`:

```sh
#!/bin/bash
# Sets up a venv suitable for running samples.
# e.g:
# ./setup_venv.sh  #setup a default $PYTHON3 shark.venv
# Environment variables used by the script.
# PYTHON=$PYTHON3.10 ./setup_venv.sh  #pass a version of $PYTHON to use
# VENV_DIR=myshark.venv #create a venv called myshark.venv
# SKIP_VENV=1 #Don't create and activate a Python venv. Use the current environment. 
# USE_IREE=1 #use stock IREE instead of Nod.ai's SHARK build
# IMPORTER=1 #Install importer deps
# BENCHMARK=1 #Install benchmark deps
# NO_BACKEND=1 #Don't install iree or shark backend
# if you run the script from a conda env it will install in your conda env

TD="$(cd $(dirname $0) && pwd)"
if [ -z "$PYTHON" ]; then
  PYTHON="$(which python3)"
fi

function die() {
  echo "Error executing command: $*"
  exit 1
}

PYTHON_VERSION_X_Y=`${PYTHON} -c 'import sys; version=sys.version_info[:2]; print("{0}.{1}".format(*version))'`

echo "Python: $PYTHON"
echo "Python version: $PYTHON_VERSION_X_Y"

if [ "$PYTHON_VERSION_X_Y" != "3.11" ]; then
    echo "Error: Python version 3.11 is required."
    exit 1
fi

if [[ "$SKIP_VENV" != "1" ]]; then
  if [[ -z "${CONDA_PREFIX}" ]]; then
    # Not a conda env. So create a new VENV dir
    VENV_DIR=${VENV_DIR:-shark.venv}
    echo "Using pip venv.. Setting up venv dir: $VENV_DIR"
    $PYTHON -m venv "$VENV_DIR" || die "Could not create venv."
    source "$VENV_DIR/bin/activate" || die "Could not activate venv"
    PYTHON="$(which python3)"
  else
    echo "Found conda env $CONDA_DEFAULT_ENV. Running pip install inside the conda env"
  fi
fi

Red=`tput setaf 1`
Green=`tput setaf 2`
Yellow=`tput setaf 3`

# Assume no binary torch-mlir.
# Currently available for macOS m1&intel (3.11) and Linux(3.8,3.10,3.11)
torch_mlir_bin=false
if [[ $(uname -s) = 'Darwin' ]]; then
  echo "${Yellow}Apple macOS detected"
  if [[ $(uname -m) == 'arm64' ]]; then
    echo "${Yellow}Apple M1 Detected"
    hash rustc 2>/dev/null
    if [ $? -eq 0 ];then
      echo "${Green}rustc found to compile HF tokenizers"
    else
      echo "${Red}Could not find rustc" >&2
      echo "${Red}Please run:"
      echo "${Red}curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"
      exit 1
    fi
  fi
  echo "${Yellow}Run the following commands to setup your SSL certs for your Python version if you see SSL errors with tests"
  echo "${Yellow}/Applications/Python\ 3.XX/Install\ Certificates.command"
  if [ "$PYTHON_VERSION_X_Y" == "3.11" ]; then
    torch_mlir_bin=true
  fi
elif [[ $(uname -s) = 'Linux' ]]; then
  echo "${Yellow}Linux detected"
  if [ "$PYTHON_VERSION_X_Y" == "3.8" ]  || [ "$PYTHON_VERSION_X_Y" == "3.10" ] || [ "$PYTHON_VERSION_X_Y" == "3.11" ] ; then
    torch_mlir_bin=true
  fi
else
  echo "${Red}OS not detected. Pray and Play"
fi

# Upgrade pip and install requirements.
$PYTHON -m pip install --upgrade pip || die "Could not upgrade pip"
$PYTHON -m pip install --upgrade -r "$TD/requirements.txt"
if [ "$torch_mlir_bin" = true ]; then
  if [[ $(uname -s) = 'Darwin' ]]; then
    echo "MacOS detected. Installing torch-mlir from .whl, to avoid dependency problems with torch."
    $PYTHON -m pip uninstall -y timm #TEMP FIX FOR MAC
    $PYTHON -m pip install --pre --no-cache-dir torch-mlir -f https://llvm.github.io/torch-mlir/package-index/ -f https://download.pytorch.org/whl/nightly/torch/
  else
    $PYTHON -m pip install --pre torch-mlir -f https://llvm.github.io/torch-mlir/package-index/
    if [ $? -eq 0 ];then
      echo "Successfully Installed torch-mlir"
    else
      echo "Could not install torch-mlir" >&2
    fi
  fi
else
  echo "${Red}No binaries found for Python $PYTHON_VERSION_X_Y on $(uname -s)"
  echo "${Yello}Python 3.11 supported on macOS and 3.8,3.10 and 3.11 on Linux"
  echo "${Red}Please build torch-mlir from source in your environment"
  exit 1
fi
if [[ -z "${USE_IREE}" ]]; then
  rm .use-iree
  RUNTIME="https://nod-ai.github.io/SRT/pip-release-links.html"
else
  touch ./.use-iree
  RUNTIME="https://openxla.github.io/iree/pip-release-links.html"
fi
if [[ -z "${NO_BACKEND}" ]]; then
  echo "Installing ${RUNTIME}..."
  $PYTHON -m pip install --pre --upgrade --no-index --find-links ${RUNTIME} iree-compiler iree-runtime
else
  echo "Not installing a backend, please make sure to add your backend to PYTHONPATH"
fi

if [[ ! -z "${IMPORTER}" ]]; then
  echo "${Yellow}Installing importer tools.."
  if [[ $(uname -s) = 'Linux' ]]; then
    echo "${Yellow}Linux detected.. installing Linux importer tools"
    #Always get the importer tools from upstream IREE
    $PYTHON -m pip install --no-warn-conflicts --upgrade -r "$TD/requirements-importer.txt" -f https://openxla.github.io/iree/pip-release-links.html --extra-index-url https://download.pytorch.org/whl/nightly/cpu
  elif [[ $(uname -s) = 'Darwin' ]]; then
    echo "${Yellow}macOS detected.. installing macOS importer tools"
    #Conda seems to have some problems installing these packages and hope they get resolved upstream.
    $PYTHON -m pip install --no-warn-conflicts --upgrade -r "$TD/requirements-importer-macos.txt" -f ${RUNTIME} --extra-index-url https://download.pytorch.org/whl/nightly/cpu
  fi
fi

if [[ $(uname -s) = 'Darwin' ]]; then
  PYTORCH_URL=https://download.pytorch.org/whl/nightly/torch/
else
  PYTORCH_URL=https://download.pytorch.org/whl/nightly/cpu/
fi

$PYTHON -m pip install --no-warn-conflicts -e . -f https://llvm.github.io/torch-mlir/package-index/ -f ${RUNTIME} -f ${PYTORCH_URL}

if [[ $(uname -s) = 'Linux' && ! -z "${IMPORTER}" ]]; then
  T_VER=$($PYTHON -m pip show torch | grep Version)
  T_VER_MIN=${T_VER:14:12}
  TV_VER=$($PYTHON -m pip show torchvision | grep Version)
  TV_VER_MAJ=${TV_VER:9:6}
  $PYTHON -m pip uninstall -y torchvision
  $PYTHON -m pip install torchvision==${TV_VER_MAJ}${T_VER_MIN} --no-deps -f https://download.pytorch.org/whl/nightly/cpu/torchvision/
  if [ $? -eq 0 ];then
    echo "Successfully Installed torch + cu118."
  else
    echo "Could not install torch + cu118." >&2
  fi
fi

if [[ -z "${NO_BREVITAS}" ]]; then
  $PYTHON -m pip install git+https://github.com/Xilinx/brevitas.git@dev
fi

if [[ -z "${CONDA_PREFIX}" && "$SKIP_VENV" != "1" ]]; then
  echo "${Green}Before running examples activate venv with:"
  echo "  ${Green}source $VENV_DIR/bin/activate"
fi

```

`shark/__init__.py`:

```py
import importlib
import logging

from torch._dynamo import register_backend

log = logging.getLogger(__name__)


@register_backend
def shark(model, inputs, *, options):
    try:
        from shark.dynamo_backend.utils import SharkBackend
    except ImportError:
        log.exception(
            "Unable to import SHARK - High Performance Machine Learning Distribution"
            "Please install the right version of SHARK that matches the PyTorch version being used. "
            "Refer to https://github.com/nod-ai/SHARK/ for details."
        )
        raise
    return SharkBackend(model, inputs, options)


def has_shark():
    try:
        importlib.import_module("shark")
        return True
    except ImportError:
        return False

```

`shark/backward_makefx.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
from torch._decomp import get_decompositions
from torch.fx.experimental.proxy_tensor import make_fx
from torch.nn.utils import stateless

from torch import fx
import tempfile


class MakeFxModule:
    def __init__(self, model, inputs, labels=None, custom_inference_fn=None):
        self.model = model
        self.inputs = inputs
        self.custom_inference_fn = custom_inference_fn
        self.training_graph = None

    # Doesn't replace the None type.
    def change_fx_graph_return_to_tuple(self, fx_g: fx.GraphModule):
        for node in fx_g.graph.nodes:
            if node.op == "output":
                # output nodes always have one argument
                node_arg = node.args[0]
                out_nodes = []
                if isinstance(node_arg, list):
                    # Don't return NoneType elements.
                    for out_node in node_arg:
                        if not isinstance(out_node, type(None)):
                            out_nodes.append(out_node)
                    # If there is a single tensor/element to be returned don't
                    # a tuple for it.
                    if len(out_nodes) == 1:
                        node.args = out_nodes
                    else:
                        node.args = (tuple(out_nodes),)
        fx_g.graph.lint()
        fx_g.recompile()
        return fx_g

    def generate_graph(self):
        fx_g = make_fx(
            self.custom_inference_fn,
            decomposition_table=get_decompositions(
                [
                    torch.ops.aten.embedding_dense_backward,
                    torch.ops.aten.native_layer_norm_backward,
                    torch.ops.aten.slice_backward,
                    torch.ops.aten.select_backward,
                ]
            ),
        )(
            dict(self.model.named_parameters()),
            dict(self.model.named_buffers()),
            self.inputs,
        )
        fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
        fx_g.recompile()
        fx_g = self.change_fx_graph_return_to_tuple(fx_g)
        ts_g = torch.jit.script(fx_g)
        temp = tempfile.NamedTemporaryFile(
            suffix="_shark_ts", prefix="temp_ts_"
        )
        ts_g.save(temp.name)
        new_ts = torch.jit.load(temp.name)
        self.training_graph = new_ts

```

`shark/dynamo_backend/utils.py`:

```py
import functools
from typing import List, Optional
import torch
from torch.fx.experimental.proxy_tensor import make_fx
from torch._functorch.compile_utils import strip_overloads
from shark.shark_inference import SharkInference
from torch._decomp import get_decompositions
from torch.func import functionalize
import io
import torch_mlir


# TODO: Control decompositions.
def default_decompositions():
    return get_decompositions(
        [
            torch.ops.aten.embedding_dense_backward,
            torch.ops.aten.native_layer_norm_backward,
            torch.ops.aten.slice_backward,
            torch.ops.aten.select_backward,
            torch.ops.aten.norm.ScalarOpt_dim,
            torch.ops.aten.native_group_norm,
            torch.ops.aten.upsample_bilinear2d.vec,
            torch.ops.aten.split.Tensor,
            torch.ops.aten.split_with_sizes,
            torch.ops.aten.native_layer_norm,
            torch.ops.aten.masked_fill.Tensor,
            torch.ops.aten.masked_fill.Scalar,
        ]
    )


def _remove_nones(fx_g: torch.fx.GraphModule) -> List[int]:
    removed_indexes = []
    for node in fx_g.graph.nodes:
        if node.op == "output":
            assert (
                len(node.args) == 1
            ), "Output node must have a single argument"
            node_arg = node.args[0]
            if isinstance(node_arg, (list, tuple)):
                node_arg = list(node_arg)
                node_args_len = len(node_arg)
                for i in range(node_args_len):
                    curr_index = node_args_len - (i + 1)
                    if node_arg[curr_index] is None:
                        removed_indexes.append(curr_index)
                        node_arg.pop(curr_index)
                node.args = (tuple(node_arg),)
                break

    if len(removed_indexes) > 0:
        fx_g.graph.lint()
        fx_g.graph.eliminate_dead_code()
        fx_g.recompile()
    removed_indexes.sort()
    return removed_indexes


def _returns_nothing(fx_g: torch.fx.GraphModule) -> bool:
    for node in fx_g.graph.nodes:
        if node.op == "output":
            assert (
                len(node.args) == 1
            ), "Output node must have a single argument"
            node_arg = node.args[0]
            if isinstance(node_arg, tuple):
                return len(node_arg) == 0
    return False


def _unwrap_single_tuple_return(fx_g: torch.fx.GraphModule) -> bool:
    """
    Replace tuple with tuple element in functions that return one-element tuples.
    Returns true if an unwrapping took place, and false otherwise.
    """
    unwrapped_tuple = False
    for node in fx_g.graph.nodes:
        if node.op == "output":
            assert (
                len(node.args) == 1
            ), "Output node must have a single argument"
            node_arg = node.args[0]
            if isinstance(node_arg, tuple):
                if len(node_arg) == 1:
                    node.args = (node_arg[0],)
                    unwrapped_tuple = True
                    break

    if unwrapped_tuple:
        fx_g.graph.lint()
        fx_g.recompile()
    return unwrapped_tuple


class SharkBackend:
    def __init__(
        self, fx_g: torch.fx.GraphModule, inputs: tuple, options: dict
    ):
        self.fx_g = fx_g
        self.inputs = inputs
        self.shark_module = None
        self.device: str = options.get("device", "cpu")
        self.was_unwrapped: bool = False
        self.none_indices: list = []
        self._modify_fx_g()
        self.compile()

    def _modify_fx_g(self):
        self.none_indices = _remove_nones(self.fx_g)
        self.was_unwrapped = _unwrap_single_tuple_return(self.fx_g)

    def compile(self):
        gm = make_fx(
            functionalize(self.fx_g),
            decomposition_table=default_decompositions(),
        )(*self.inputs)
        gm.graph.set_codegen(torch.fx.graph.CodeGen())
        gm.recompile()
        strip_overloads(gm)
        ts_g = torch.jit.script(gm)
        mlir_module = torch_mlir.compile(
            ts_g, self.inputs, output_type="linalg-on-tensors"
        )
        bytecode_stream = io.BytesIO()
        mlir_module.operation.write_bytecode(bytecode_stream)
        bytecode = bytecode_stream.getvalue()
        from shark.shark_inference import SharkInference

        shark_module = SharkInference(
            mlir_module=bytecode,
            device=self.device,
            mlir_dialect="tm_tensor",
        )
        shark_module.compile(extra_args=[])
        self.shark_module = shark_module

    def __call__(self, *inputs):
        np_inputs = [x.contiguous().detach().cpu().numpy() for x in inputs]
        np_outs = self.shark_module("forward", np_inputs)
        if self.was_unwrapped:
            np_outs = [
                np_outs,
            ]

        if not isinstance(np_outs, list):
            res = torch.from_numpy(np_outs)
            return res

        result = [torch.from_numpy(x) for x in np_outs]
        for r_in in self.none_indices:
            result.insert(r_in, None)
        result = tuple(result)
        return result

```

`shark/examples/shark_dynamo/basic_examples.py`:

```py
import torch
import shark


def foo(x, a):
    if x.shape[0] > 3:
        return x + a
    else:
        return x + 3


shark_options = {"device": "cpu"}
compiled = torch.compile(foo, backend="shark", options=shark_options)

input = torch.ones(4)

x = compiled(input, input)

print(x)

input = torch.ones(3)

x = compiled(input, input)

print(x)

```

`shark/examples/shark_eager/dynamo_demo.ipynb`:

```ipynb
Jupyter Notebook Summary:
Total cells: 10 (7 code, 3 markdown, 0 raw)

Code Cell #1:
```python
# standard imports
import torch
from shark.iree_utils import get_iree_compiled_module
```

Code Cell #2:
```python
# torch dynamo related imports
try:
    import torchdynamo
    from torchdynamo.optimizations.backends import create_backend
    from torchdynamo.optimizations.subgraph import SubGraph
except ModuleNotFoundError:
    print(
        "Please install TorchDynamo using pip install git+https://github.com/pytorch/torchdynamo"
    )
    exit()

# torch-mlir imports for compiling
from torch_mlir import compile, OutputType
```

Code Cell #3:
```python
def toy_example(*args):
    a, b = args

    x = a / (torch.abs(a) + 1)
    if b.sum() < 0:
        b = b * -1
    return x * b
```

... [4 more code cells omitted]

```

`shark/examples/shark_eager/dynamo_demo.py`:

```py
import torch
from torch_mlir import compile, OutputType

from shark.iree_utils import get_iree_compiled_module

try:
    import torchdynamo
    from torchdynamo.optimizations.backends import create_backend
    from torchdynamo.optimizations.subgraph import SubGraph
except ModuleNotFoundError:
    print(
        "Please install TorchDynamo using pip install git+https://github.com/pytorch/torchdynamo"
    )
    exit()

NUM_ITERS = 10


def __torch_mlir(fx_graph, *args, **kwargs):
    assert isinstance(
        fx_graph, torch.fx.GraphModule
    ), "Model must be an FX GraphModule."

    def _unwrap_single_tuple_return(fx_g: torch.fx.GraphModule):
        """Replace tuple with tuple element in functions that return one-element tuples."""

        for node in fx_g.graph.nodes:
            if node.op == "output":
                assert (
                    len(node.args) == 1
                ), "Output node must have a single argument"
                node_arg = node.args[0]
                if isinstance(node_arg, tuple) and len(node_arg) == 1:
                    node.args = (node_arg[0],)
        fx_g.graph.lint()
        fx_g.recompile()
        return fx_g

    fx_graph = _unwrap_single_tuple_return(fx_graph)
    ts_graph = torch.jit.script(fx_graph)

    if isinstance(args, tuple):
        args = list(args)
    assert isinstance(args, list)
    if len(args) == 1 and isinstance(args[0], list):
        args = args[0]

    linalg_module = compile(
        ts_graph, args, output_type=OutputType.LINALG_ON_TENSORS
    )
    callable, _ = get_iree_compiled_module(
        linalg_module, "cuda", func_name="forward"
    )

    def forward(*inputs):
        return callable(*inputs)

    return forward


def toy_example(*args):
    a, b = args

    x = a / (torch.abs(a) + 1)
    if b.sum() < 0:
        b = b * -1
    return x * b


with torchdynamo.optimize(__torch_mlir):
    for _ in range(10):
        print(toy_example(torch.randn(10), torch.randn(10)))


@create_backend
def torch_mlir(subgraph, *args, **kwargs):
    assert isinstance(subgraph, SubGraph), "Model must be a dynamo SubGraph."
    return __torch_mlir(subgraph.model, *list(subgraph.example_inputs))


@torchdynamo.optimize("torch_mlir")
def toy_example2(*args):
    a, b = args

    x = a / (torch.abs(a) + 1)
    if b.sum() < 0:
        b = b * -1
    return x * b


for _ in range(10):
    print(toy_example2(torch.randn(10), torch.randn(10)))

```

`shark/examples/shark_eager/eager_mode.ipynb`:

```ipynb
Jupyter Notebook Summary:
Total cells: 14 (8 code, 6 markdown, 0 raw)

Code Cell #1:
```python
# standard imports
import torch
from torch_mlir.eager_mode import torch_mlir_tensor
```

Code Cell #2:
```python
# eager mode imports
from torch_mlir.eager_mode.torch_mlir_tensor import TorchMLIRTensor
from shark.iree_eager_backend import EagerModeIREELinalgOnTensorsBackend
```

Code Cell #3:
```python
torch_mlir_tensor.backend = EagerModeIREELinalgOnTensorsBackend("cpu")
```

... [5 more code cells omitted]

```

`shark/examples/shark_eager/eager_mode.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
from torch.utils.cpp_extension import load_inline, include_paths
from torch_mlir.eager_mode import torch_mlir_tensor
from torch_mlir.eager_mode.torch_mlir_tensor import TorchMLIRTensor

from shark.iree_eager_backend import EagerModeIREELinalgOnTensorsBackend
from shark.shark_runner import SharkEagerMode


def test_cpu():
    torch_mlir_tensor.backend = EagerModeIREELinalgOnTensorsBackend("cpu")

    t = torch.ones((10, 10), device="cpu")
    u = 2 * torch.ones((10, 10), device="cpu")

    tt = TorchMLIRTensor(t)
    print(tt)
    uu = TorchMLIRTensor(u)
    print(uu)

    for i in range(NUM_ITERS):
        yy = tt + uu
        print(type(yy))
        print(yy.elem.to_host())
        yy = tt * uu
        print(type(yy))
        print(yy.elem.to_host())


def test_gpu():
    source = """
    #include <iostream>
    #include "cuda.h"
    #include "cuda_runtime_api.h"

    using namespace std;

    void print_free_mem() {
        int num_gpus;
        size_t free, total;
        cudaSetDevice(0);
        int id;
        cudaGetDevice(&id);
        cudaMemGetInfo(&free, &total);
        cout << "GPU " << id << " memory: used=" << (total-free)/(1<<20) << endl;
    }
    """
    gpu_stats = load_inline(
        name="inline_extension",
        cpp_sources=[source],
        extra_include_paths=include_paths(cuda=True),
        functions=["print_free_mem"],
    )
    torch_mlir_tensor.backend = EagerModeIREELinalgOnTensorsBackend("gpu")

    t = torch.ones((10, 10), device="cpu")
    u = 2 * torch.ones((10, 10), device="cpu")

    tt = TorchMLIRTensor(t)
    print(tt)
    uu = TorchMLIRTensor(u)
    print(uu)

    for i in range(NUM_ITERS):
        yy = tt + uu
        print(yy.elem.to_host())
        yy = tt * uu
        print(yy.elem.to_host())
        gpu_stats.print_free_mem()


def test_python_mode_ref_backend():
    # hide this wherever you want?
    _ = SharkEagerMode("refbackend")

    t = torch.ones((10, 10), device="cpu")
    u = torch.ones((10, 10), device="cpu")

    print(t)
    print(u)

    for i in range(NUM_ITERS):
        print(i)
        yy = t + u
        print(yy.elem)
        yy = t * u
        print(yy.elem)


def test_python_mode_iree_cpu():
    # hide this wherever you want?
    _ = SharkEagerMode("cpu")

    t = torch.ones((10, 10), device="cpu")
    u = torch.ones((10, 10), device="cpu")

    print(t)
    print(u)

    for i in range(NUM_ITERS):
        yy = t + u
        print(type(yy))
        print(yy.elem.to_host())
        yy = t * u
        print(type(yy))
        print(yy.elem.to_host())


def test_python_mode_iree_gpu():
    _ = SharkEagerMode("gpu")

    t = torch.ones((10, 10), device="cpu")
    u = torch.ones((10, 10), device="cpu")

    print(t)
    print(u)

    for i in range(NUM_ITERS):
        yy = t + u
        print(type(yy))
        print(yy.elem.to_host())
        yy = t * u
        print(type(yy))
        print(yy.elem.to_host())


if __name__ == "__main__":
    NUM_ITERS = 10
    test_cpu()
    if torch.cuda.is_available():
        test_gpu()
    test_python_mode_ref_backend()
    test_python_mode_iree_cpu()
    test_python_mode_iree_gpu()

```

`shark/examples/shark_eager/squeezenet_lockstep.py`:

```py
import torch
import numpy as np

model = torch.hub.load(
    "pytorch/vision:v0.10.0", "squeezenet1_0", pretrained=True
)
model.eval()

# from PIL import Image
# from torchvision import transforms
# import urllib
#
# url, filename = ("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")
# try: urllib.URLopener().retrieve(url, filename)
# except: urllib.request.urlretrieve(url, filename)
#
#
# input_image = Image.open(filename)
# preprocess = transforms.Compose([
#     transforms.Resize(256),
#     transforms.CenterCrop(224),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
# ])
# input_tensor = preprocess(input_image)
# input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model
# print(input_batch.shape) # size = [1, 3, 224, 224]

# The above is code for generating sample inputs from an image. We can just use
# random values for accuracy testing though
input_batch = torch.randn(1, 3, 224, 224)


# Focus on CPU for now
if False and torch.cuda.is_available():
    input_batch = input_batch.to("cuda")
    model.to("cuda")

with torch.no_grad():
    output = model(input_batch)
# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes
golden_confidences = output[0]
# The output has unnormalized scores. To get probabilities, you can run a softmax on it.
golden_probabilities = torch.nn.functional.softmax(
    golden_confidences, dim=0
).numpy()

golden_confidences = golden_confidences.numpy()

from shark.torch_mlir_lockstep_tensor import TorchMLIRLockstepTensor

input_detached_clone = input_batch.clone()
eager_input_batch = TorchMLIRLockstepTensor(input_detached_clone)

print("getting torch-mlir result")

output = model(eager_input_batch)

static_output = output.elem
confidences = static_output[0]
probabilities = torch.nn.functional.softmax(
    torch.from_numpy(confidences), dim=0
).numpy()

print("The obtained result via shark is: ", confidences)
print("The golden result is:", golden_confidences)

np.testing.assert_allclose(
    golden_confidences, confidences, rtol=1e-02, atol=1e-03
)
np.testing.assert_allclose(
    golden_probabilities, probabilities, rtol=1e-02, atol=1e-03
)

```

`shark/examples/shark_inference/CLIPModel_tf.py`:

```py
from PIL import Image
import requests

from transformers import CLIPProcessor, TFCLIPModel
import tensorflow as tf
from shark.shark_inference import SharkInference

# Create a set of inputs
clip_vit_inputs = [
    tf.TensorSpec(shape=[2, 7], dtype=tf.int32),
    tf.TensorSpec(shape=[2, 7], dtype=tf.int32),
    tf.TensorSpec(shape=[1, 3, 224, 224], dtype=tf.float32),
]


class CLIPModule(tf.Module):
    def __init__(self):
        super(CLIPModule, self).__init__()
        self.m = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")

        self.m.predict = lambda x, y, z: self.m(
            input_ids=x, attention_mask=y, pixel_values=z
        )

    @tf.function(input_signature=clip_vit_inputs, jit_compile=True)
    def forward(self, input_ids, attention_mask, pixel_values):
        return self.m.predict(
            input_ids, attention_mask, pixel_values
        ).logits_per_image


if __name__ == "__main__":
    # Prepping Data
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    image = Image.open(requests.get(url, stream=True).raw)

    inputs = processor(
        text=["a photo of a cat", "a photo of a dog"],
        images=image,
        return_tensors="tf",
        padding=True,
    )

    shark_module = SharkInference(
        CLIPModule(),
        (
            inputs["input_ids"],
            inputs["attention_mask"],
            inputs["pixel_values"],
        ),
    )
    shark_module.set_frontend("tensorflow")
    shark_module.compile()

    print(
        shark_module.forward(
            (
                inputs["input_ids"],
                inputs["attention_mask"],
                inputs["pixel_values"],
            )
        )
    )

```

`shark/examples/shark_inference/ESRGAN/README.md`:

```md
## Running ESRGAN

```
1. pip install numpy opencv-python
2. mkdir InputImages
   (this is where all the input images will reside in)
3. mkdir OutputImages
   (this is where the model will generate all the images)
4. mkdir models
   (save the .pth checkpoint file here)
5. python esrgan.py
```

- Download [RRDB_ESRGAN_x4.pth](https://drive.google.com/drive/u/0/folders/17VYV_SoZZesU6mbxz2dMAIccSSlqLecY) and place it in the `models` directory as mentioned above in step 4.
- Credits : [ESRGAN](https://github.com/xinntao/ESRGAN)

```

`shark/examples/shark_inference/ESRGAN/esrgan.py`:

```py
from ast import arg
import os.path as osp
import glob
import cv2
import numpy as np
import torch

from torch.fx.experimental.proxy_tensor import make_fx
from torch._decomp import get_decompositions
from shark.shark_inference import SharkInference
import torch_mlir
import tempfile
import functools
import torch
import torch.nn as nn
import torch.nn.functional as F


def make_layer(block, n_layers):
    layers = []
    for _ in range(n_layers):
        layers.append(block())
    return nn.Sequential(*layers)


class ResidualDenseBlock_5C(nn.Module):
    def __init__(self, nf=64, gc=32, bias=True):
        super(ResidualDenseBlock_5C, self).__init__()
        # gc: growth channel, i.e. intermediate channels
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)
        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)
        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)
        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)

        # initialization
        # mutil.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)

    def forward(self, x):
        x1 = self.lrelu(self.conv1(x))
        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))
        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))
        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
        return x5 * 0.2 + x


class RRDB(nn.Module):
    """Residual in Residual Dense Block"""

    def __init__(self, nf, gc=32):
        super(RRDB, self).__init__()
        self.RDB1 = ResidualDenseBlock_5C(nf, gc)
        self.RDB2 = ResidualDenseBlock_5C(nf, gc)
        self.RDB3 = ResidualDenseBlock_5C(nf, gc)

    def forward(self, x):
        out = self.RDB1(x)
        out = self.RDB2(out)
        out = self.RDB3(out)
        return out * 0.2 + x


class RRDBNet(nn.Module):
    def __init__(self, in_nc, out_nc, nf, nb, gc=32):
        super(RRDBNet, self).__init__()
        RRDB_block_f = functools.partial(RRDB, nf=nf, gc=gc)

        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)
        self.RRDB_trunk = make_layer(RRDB_block_f, nb)
        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        #### upsampling
        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)

        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)

    def forward(self, x):
        fea = self.conv_first(x)
        trunk = self.trunk_conv(self.RRDB_trunk(fea))
        fea = fea + trunk

        fea = self.lrelu(
            self.upconv1(F.interpolate(fea, scale_factor=2, mode="nearest"))
        )
        fea = self.lrelu(
            self.upconv2(F.interpolate(fea, scale_factor=2, mode="nearest"))
        )
        out = self.conv_last(self.lrelu(self.HRconv(fea)))

        return out


############### Parsing args #####################
import argparse

p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)

p.add_argument("--device", type=str, default="cpu", help="the device to use")
p.add_argument(
    "--mlir_loc",
    type=str,
    default=None,
    help="location of the model's mlir file",
)
args = p.parse_args()
###################################################


def inference(input_m):
    return model(input_m)


def load_mlir(mlir_loc):
    import os

    if mlir_loc == None:
        return None
    print(f"Trying to load the model from {mlir_loc}.")
    with open(os.path.join(mlir_loc)) as f:
        mlir_module = f.read()
    return mlir_module


def compile_through_fx(model, inputs, mlir_loc=None):
    module = load_mlir(mlir_loc)
    if module == None:
        fx_g = make_fx(
            model,
            decomposition_table=get_decompositions(
                [
                    torch.ops.aten.embedding_dense_backward,
                    torch.ops.aten.native_layer_norm_backward,
                    torch.ops.aten.slice_backward,
                    torch.ops.aten.select_backward,
                    torch.ops.aten.norm.ScalarOpt_dim,
                    torch.ops.aten.native_group_norm,
                    torch.ops.aten.upsample_bilinear2d.vec,
                    torch.ops.aten.split.Tensor,
                    torch.ops.aten.split_with_sizes,
                ]
            ),
        )(inputs)

        fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
        fx_g.recompile()

        def strip_overloads(gm):
            """
            Modifies the target of graph nodes in :attr:`gm` to strip overloads.
            Args:
                gm(fx.GraphModule): The input Fx graph module to be modified
            """
            for node in gm.graph.nodes:
                if isinstance(node.target, torch._ops.OpOverload):
                    node.target = node.target.overloadpacket
            gm.recompile()

        strip_overloads(fx_g)

        ts_g = torch.jit.script(fx_g)

        print("Torchscript graph generated successfully")
        module = torch_mlir.compile(
            ts_g,
            inputs,
            torch_mlir.OutputType.LINALG_ON_TENSORS,
            use_tracing=False,
            verbose=False,
        )

    mlir_model = str(module)
    func_name = "forward"
    shark_module = SharkInference(
        mlir_model, device=args.device, mlir_dialect="linalg"
    )
    shark_module.compile()

    return shark_module


model_path = "models/RRDB_ESRGAN_x4.pth"  # models/RRDB_ESRGAN_x4.pth OR models/RRDB_PSNR_x4.pth
# device = torch.device('cuda')  # if you want to run on CPU, change 'cuda' -> cpu
device = torch.device("cpu")

test_img_folder = "InputImages/*"

model = RRDBNet(3, 3, 64, 23, gc=32)
model.load_state_dict(torch.load(model_path), strict=True)
model.eval()
model = model.to(device)

print("Model path {:s}. \nTesting...".format(model_path))

if __name__ == "__main__":
    idx = 0
    for path in glob.glob(test_img_folder):
        idx += 1
        base = osp.splitext(osp.basename(path))[0]
        print(idx, base)
        # read images
        img = cv2.imread(path, cv2.IMREAD_COLOR)
        img = img * 1.0 / 255
        img = torch.from_numpy(
            np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))
        ).float()
        img_LR = img.unsqueeze(0)
        img_LR = img_LR.to(device)

        with torch.no_grad():
            shark_module = compile_through_fx(inference, img_LR)
            shark_output = shark_module.forward((img_LR,))
            shark_output = torch.from_numpy(shark_output)
            shark_output = (
                shark_output.data.squeeze().float().cpu().clamp_(0, 1).numpy()
            )
            esrgan_output = (
                model(img_LR).data.squeeze().float().cpu().clamp_(0, 1).numpy()
            )
        # SHARK OUTPUT
        shark_output = np.transpose(shark_output[[2, 1, 0], :, :], (1, 2, 0))
        shark_output = (shark_output * 255.0).round()
        cv2.imwrite(
            "OutputImages/{:s}_rlt_shark_output.png".format(base), shark_output
        )
        print("Generated SHARK's output")
        # ESRGAN OUTPUT
        esrgan_output = np.transpose(esrgan_output[[2, 1, 0], :, :], (1, 2, 0))
        esrgan_output = (esrgan_output * 255.0).round()
        cv2.imwrite(
            "OutputImages/{:s}_rlt_esrgan_output.png".format(base),
            esrgan_output,
        )
        print("Generated ESRGAN's output")

```

`shark/examples/shark_inference/albert_maskfill_pt.py`:

```py
from transformers import AutoModelForMaskedLM, AutoTokenizer
import torch
from shark.shark_inference import SharkInference
from shark.shark_importer import SharkImporter
from iree.compiler import compile_str
from iree import runtime as ireert
import os
import numpy as np

MAX_SEQUENCE_LENGTH = 512
BATCH_SIZE = 1


class AlbertModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = AutoModelForMaskedLM.from_pretrained("albert-base-v2")
        self.model.eval()

    def forward(self, input_ids, attention_mask):
        return self.model(
            input_ids=input_ids, attention_mask=attention_mask
        ).logits


if __name__ == "__main__":
    # Prepping Data
    tokenizer = AutoTokenizer.from_pretrained("albert-base-v2")
    text = "This [MASK] is very tasty."
    encoded_inputs = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
        return_tensors="pt",
    )
    inputs = (encoded_inputs["input_ids"], encoded_inputs["attention_mask"])
    mlir_importer = SharkImporter(
        AlbertModule(),
        inputs,
        frontend="torch",
    )
    minilm_mlir, func_name = mlir_importer.import_mlir(
        is_dynamic=False, tracing_required=True
    )
    shark_module = SharkInference(minilm_mlir)
    shark_module.compile()
    token_logits = torch.tensor(shark_module.forward(inputs))
    mask_id = torch.where(
        encoded_inputs["input_ids"] == tokenizer.mask_token_id
    )[1]
    mask_token_logits = token_logits[0, mask_id, :]
    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()
    for token in top_5_tokens:
        print(
            f"'>>> Sample/Warmup output: {text.replace(tokenizer.mask_token, tokenizer.decode(token))}'"
        )
    while True:
        try:
            new_text = input("Give me a sentence with [MASK] to fill: ")
            encoded_inputs = tokenizer(
                new_text,
                padding="max_length",
                truncation=True,
                max_length=MAX_SEQUENCE_LENGTH,
                return_tensors="pt",
            )
            inputs = (
                encoded_inputs["input_ids"],
                encoded_inputs["attention_mask"],
            )
            token_logits = torch.tensor(shark_module.forward(inputs))
            mask_id = torch.where(
                encoded_inputs["input_ids"] == tokenizer.mask_token_id
            )[1]
            mask_token_logits = token_logits[0, mask_id, :]
            top_5_tokens = (
                torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()
            )
            for token in top_5_tokens:
                print(
                    f"'>>> {new_text.replace(tokenizer.mask_token, tokenizer.decode(token))}'"
                )
        except KeyboardInterrupt:
            print("Exiting program.")
            break

```

`shark/examples/shark_inference/albert_maskfill_tf.py`:

```py
from PIL import Image
import requests

from transformers import TFAutoModelForMaskedLM, AutoTokenizer
import tensorflow as tf
from shark.shark_inference import SharkInference
from shark.shark_importer import SharkImporter
from iree.compiler import tf as tfc
from iree.compiler import compile_str
from iree import runtime as ireert
import os
import numpy as np
import sys

MAX_SEQUENCE_LENGTH = 512
BATCH_SIZE = 1

# Create a set of inputs
t5_inputs = [
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
]


class AlbertModule(tf.Module):
    def __init__(self):
        super(AlbertModule, self).__init__()
        self.m = TFAutoModelForMaskedLM.from_pretrained("albert-base-v2")
        self.m.predict = lambda x, y: self.m(input_ids=x, attention_mask=y)

    @tf.function(input_signature=t5_inputs, jit_compile=True)
    def forward(self, input_ids, attention_mask):
        return self.m.predict(input_ids, attention_mask)


if __name__ == "__main__":
    # Prepping Data
    tokenizer = AutoTokenizer.from_pretrained("albert-base-v2")
    # text = "This is a great [MASK]."
    text = "This [MASK] is very tasty."
    encoded_inputs = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
        return_tensors="tf",
    )
    inputs = (encoded_inputs["input_ids"], encoded_inputs["attention_mask"])
    mlir_importer = SharkImporter(
        AlbertModule(),
        inputs,
        frontend="tf",
    )
    minilm_mlir, func_name = mlir_importer.import_mlir(
        is_dynamic=False, tracing_required=False
    )
    shark_module = SharkInference(minilm_mlir, mlir_dialect="mhlo")
    shark_module.compile()
    output_idx = 0
    data_idx = 1
    token_logits = shark_module.forward(inputs)[output_idx][data_idx]
    mask_id = np.where(
        tf.squeeze(encoded_inputs["input_ids"]) == tokenizer.mask_token_id
    )
    mask_token_logits = token_logits[0, mask_id, :]
    top_5_tokens = np.flip(np.argsort(mask_token_logits)).squeeze()[0:5]
    for token in top_5_tokens:
        print(
            f"'>>> Sample/Warmup output: {text.replace(tokenizer.mask_token, tokenizer.decode(token))}'"
        )
    while True:
        try:
            new_text = input("Give me a sentence with [MASK] to fill: ")
            encoded_inputs = tokenizer(
                new_text,
                padding="max_length",
                truncation=True,
                max_length=MAX_SEQUENCE_LENGTH,
                return_tensors="tf",
            )
            inputs = (
                encoded_inputs["input_ids"],
                encoded_inputs["attention_mask"],
            )
            token_logits = shark_module.forward(inputs)[output_idx][data_idx]
            mask_id = np.where(
                tf.squeeze(encoded_inputs["input_ids"])
                == tokenizer.mask_token_id
            )
            mask_token_logits = token_logits[0, mask_id, :]
            top_5_tokens = np.flip(np.argsort(mask_token_logits)).squeeze()[
                0:5
            ]
            for token in top_5_tokens:
                print(
                    f"'>>> {new_text.replace(tokenizer.mask_token, tokenizer.decode(token))}'"
                )
        except KeyboardInterrupt:
            print("Exiting program.")
            sys.exit()

```

`shark/examples/shark_inference/bloom_tank.py`:

```py
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model

mlir_model, func_name, inputs, golden_out = download_model(
    "bloom", frontend="torch"
)

shark_module = SharkInference(
    mlir_model, device="cpu", mlir_dialect="tm_tensor"
)
shark_module.compile()
result = shark_module.forward(inputs)
print("The obtained result via shark is: ", result)
print("The golden result is:", golden_out)

```

`shark/examples/shark_inference/gpt2_tf.py`:

```py
from PIL import Image
import requests

from transformers import GPT2Tokenizer, TFGPT2Model
import tensorflow as tf
from shark.shark_inference import SharkInference

# Create a set of inputs
gpt2_inputs = [
    tf.TensorSpec(shape=[1, 8], dtype=tf.int32),
    tf.TensorSpec(shape=[1, 8], dtype=tf.int32),
]


class GPT2Module(tf.Module):
    def __init__(self):
        super(GPT2Module, self).__init__()
        self.m = TFGPT2Model.from_pretrained("distilgpt2")

        self.m.predict = lambda x, y: self.m(input_ids=x, attention_mask=y)

    @tf.function(input_signature=gpt2_inputs, jit_compile=True)
    def forward(self, input_ids, attention_mask):
        return self.m.predict(input_ids, attention_mask)


if __name__ == "__main__":
    # Prepping Data
    tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
    text = "I love the distilled version of models."

    inputs = tokenizer(text, return_tensors="tf")
    shark_module = SharkInference(
        GPT2Module(), (inputs["input_ids"], inputs["attention_mask"])
    )
    shark_module.set_frontend("tensorflow")
    shark_module.compile()
    print(
        shark_module.forward((inputs["input_ids"], inputs["attention_mask"]))
    )

```

`shark/examples/shark_inference/llama/README.md`:

```md
# SHARK LLaMA

## TORCH-MLIR Version

```
https://github.com/nod-ai/torch-mlir.git
```
Then check out the `complex` branch and `git submodule update --init` and then build with `.\build_tools\python_deploy\build_windows.ps1`

### Setup & Run
```
git clone https://github.com/nod-ai/llama.git
```
Then in this repository
```
pip install -e .
python llama/shark_model.py
```

```

`shark/examples/shark_inference/mega_test.py`:

```py
import torch
import torch_mlir
from shark.shark_inference import SharkInference
from shark.shark_compile import shark_compile_through_fx
from MEGABYTE_pytorch import MEGABYTE

import os


class MegaModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = MEGABYTE(
            num_tokens=16000,  # number of tokens
            dim=(
                512,
                256,
            ),  # transformer model dimension (512 for coarsest, 256 for fine in this example)
            max_seq_len=(
                1024,
                4,
            ),  # sequence length for global and then local. this can be more than 2
            depth=(
                6,
                4,
            ),  # number of layers for global and then local. this can be more than 2, but length must match the max_seq_len's
            dim_head=64,  # dimension per head
            heads=8,  # number of attention heads
            flash_attn=True,  # use flash attention
        )

    def forward(self, input):
        return self.model(input)


megaModel = MegaModel()
inputs = [torch.randint(0, 16000, (1, 1024, 4))]

# CURRENTLY IT BAILS OUT HERE BECAUSE OF MISSING OP LOWERINGS :-
# 1. aten.alias
shark_module, _ = shark_compile_through_fx(
    model=megaModel,
    inputs=inputs,
    extended_model_name="mega_shark",
    is_f16=False,
    f16_input_mask=None,
    save_dir=os.getcwd(),
    debug=False,
    generate_or_load_vmfb=True,
    extra_args=[],
    device="cuda",
    mlir_dialect="tm_tensor",
)
# logits = model(x)


def print_output_info(output, msg):
    print("\n", msg)
    print("\n\t", output.shape)


ans = shark_module("forward", inputs)
print_output_info(torch.from_numpy(ans), "SHARK's output")

ans = megaModel.forward(*inputs)
print_output_info(ans, "ORIGINAL Model's output")

# and sample from the logits accordingly
# or you can use the generate function

# NEED TO LOOK AT THIS LATER IF REQUIRED IN SHARK.
# sampled = model.generate(temperature = 0.9, filter_thres = 0.9) # (1, 1024, 4)

```

`shark/examples/shark_inference/mhlo_example.py`:

```py
from shark.shark_inference import SharkInference
import numpy as np

mhlo_ir = r"""builtin.module  {
      func.func @forward(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {
        %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<1x4xf32>, tensor<4x1xf32>) -> tensor<4x4xf32>
        %1 = "mhlo.abs"(%0) : (tensor<4x4xf32>) -> tensor<4x4xf32>
        return %1 : tensor<4x4xf32>
      }
}"""

arg0 = np.ones((1, 4)).astype(np.float32)
arg1 = np.ones((4, 1)).astype(np.float32)

print("Running shark on cpu backend")
shark_module = SharkInference(mhlo_ir, device="cpu", mlir_dialect="mhlo")

# Generate the random inputs and feed into the graph.
x = shark_module.generate_random_inputs()
shark_module.compile()
print(shark_module.forward(x))

print("Running shark on cuda backend")
shark_module = SharkInference(mhlo_ir, device="cuda", mlir_dialect="mhlo")
shark_module.compile()
print(shark_module.forward(x))

print("Running shark on vulkan backend")
shark_module = SharkInference(mhlo_ir, device="vulkan", mlir_dialect="mhlo")
shark_module.compile()
print(shark_module.forward(x))

```

`shark/examples/shark_inference/minilm_benchmark.py`:

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from shark.shark_inference import SharkInference

torch.manual_seed(0)
tokenizer = AutoTokenizer.from_pretrained("microsoft/MiniLM-L12-H384-uncased")


class MiniLMSequenceClassification(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "microsoft/MiniLM-L12-H384-uncased",  # The pretrained model.
            num_labels=2,  # The number of output labels--2 for binary classification.
            output_attentions=False,  # Whether the model returns attentions weights.
            output_hidden_states=False,  # Whether the model returns all hidden-states.
            torchscript=True,
        )

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


test_input = torch.randint(2, (1, 128))

shark_module = SharkInference(
    MiniLMSequenceClassification(),
    (test_input,),
    jit_trace=True,
    benchmark_mode=True,
)

shark_module.compile()
shark_module.forward((test_input,))
shark_module.benchmark_all((test_input,))

```

`shark/examples/shark_inference/minilm_benchmark_tf.py`:

```py
import tensorflow as tf
from transformers import BertModel, BertTokenizer, TFBertModel
from shark.shark_inference import SharkInference

MAX_SEQUENCE_LENGTH = 512
BATCH_SIZE = 1

# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        # Create a BERT trainer with the created network.
        self.m = TFBertModel.from_pretrained(
            "microsoft/MiniLM-L12-H384-uncased", from_pt=True
        )

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m.predict = lambda x, y, z: self.m.call(
            input_ids=x, attention_mask=y, token_type_ids=z, training=False
        )

    @tf.function(input_signature=bert_input, jit_compile=True)
    def forward(self, input_ids, attention_mask, token_type_ids):
        return self.m.predict(input_ids, attention_mask, token_type_ids)


if __name__ == "__main__":
    # Prepping Data
    tokenizer = BertTokenizer.from_pretrained(
        "microsoft/MiniLM-L12-H384-uncased"
    )
    text = "Replace me by any text you'd like."
    encoded_input = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    for key in encoded_input:
        encoded_input[key] = tf.expand_dims(
            tf.convert_to_tensor(encoded_input[key]), 0
        )

    test_input = (
        encoded_input["input_ids"],
        encoded_input["attention_mask"],
        encoded_input["token_type_ids"],
    )
    shark_module = SharkInference(
        BertModule(), test_input, benchmark_mode=True
    )
    shark_module.set_frontend("tensorflow")
    shark_module.compile()
    shark_module.benchmark_all(test_input)

```

`shark/examples/shark_inference/minilm_jax.py`:

```py
from transformers import AutoTokenizer, FlaxAutoModel
import torch
import jax
from typing import Union, Dict, List, Any
import numpy as np
from shark.shark_inference import SharkInference
import io

NumpyTree = Union[np.ndarray, Dict[str, np.ndarray], List[np.ndarray]]


def convert_torch_tensor_tree_to_numpy(
    tree: Union[torch.tensor, Dict[str, torch.tensor], List[torch.tensor]]
) -> NumpyTree:
    return jax.tree_util.tree_map(
        lambda torch_tensor: torch_tensor.cpu().detach().numpy(), tree
    )


def convert_int64_to_int32(tree: NumpyTree) -> NumpyTree:
    return jax.tree_util.tree_map(
        lambda tensor: np.array(tensor, dtype=np.int32)
        if tensor.dtype == np.int64
        else tensor,
        tree,
    )


def get_sample_input():
    tokenizer = AutoTokenizer.from_pretrained(
        "microsoft/MiniLM-L12-H384-uncased"
    )
    inputs_torch = tokenizer("Hello, World!", return_tensors="pt")
    return convert_int64_to_int32(
        convert_torch_tensor_tree_to_numpy(inputs_torch.data)
    )


def get_jax_model():
    return FlaxAutoModel.from_pretrained("microsoft/MiniLM-L12-H384-uncased")


def export_jax_to_mlir(jax_model: Any, sample_input: NumpyTree):
    model_mlir = jax.jit(jax_model).lower(**sample_input).compiler_ir()
    byte_stream = io.BytesIO()
    model_mlir.operation.write_bytecode(file=byte_stream)
    return byte_stream.getvalue()


def assert_array_list_allclose(x, y, *args, **kwargs):
    assert len(x) == len(y)
    for a, b in zip(x, y):
        np.testing.assert_allclose(
            np.asarray(a), np.asarray(b), *args, **kwargs
        )


sample_input = get_sample_input()
jax_model = get_jax_model()
mlir = export_jax_to_mlir(jax_model, sample_input)

# Compile and load module.
shark_inference = SharkInference(mlir_module=mlir, mlir_dialect="mhlo")
shark_inference.compile()

# Run main function.
result = shark_inference("main", jax.tree_util.tree_flatten(sample_input)[0])

# Run JAX model.
reference_result = jax.tree_util.tree_flatten(jax_model(**sample_input))[0]

# Verify result.
assert_array_list_allclose(result, reference_result, atol=1e-5)

```

`shark/examples/shark_inference/minilm_jax_requirements.txt`:

```txt
flax
jax[cpu]
nodai-SHARK
orbax
transformers
torch

```

`shark/examples/shark_inference/minilm_jit.py`:

```py
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model


mlir_model, func_name, inputs, golden_out = download_model(
    "microsoft/MiniLM-L12-H384-uncased",
    frontend="torch",
)


shark_module = SharkInference(mlir_model, device="cpu", mlir_dialect="linalg")
shark_module.compile()
result = shark_module.forward(inputs)
print("The obtained result via shark is: ", result)
print("The golden result is:", golden_out)


# Let's generate random inputs, currently supported
# for static models.
rand_inputs = shark_module.generate_random_inputs()
rand_results = shark_module.forward(rand_inputs)

print("Running shark_module with random_inputs is: ", rand_results)

```

`shark/examples/shark_inference/minilm_tf.py`:

```py
import tensorflow as tf
from transformers import BertModel, BertTokenizer, TFBertModel
from shark.shark_inference import SharkInference

MAX_SEQUENCE_LENGTH = 512
BATCH_SIZE = 1

# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        # Create a BERT trainer with the created network.
        self.m = TFBertModel.from_pretrained(
            "microsoft/MiniLM-L12-H384-uncased", from_pt=True
        )

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m.predict = lambda x, y, z: self.m.call(
            input_ids=x, attention_mask=y, token_type_ids=z, training=False
        )

    @tf.function(input_signature=bert_input, jit_compile=True)
    def forward(self, input_ids, attention_mask, token_type_ids):
        return self.m.predict(input_ids, attention_mask, token_type_ids)


if __name__ == "__main__":
    # Prepping Data
    tokenizer = BertTokenizer.from_pretrained(
        "microsoft/MiniLM-L12-H384-uncased"
    )
    text = "Replace me by any text you'd like."
    encoded_input = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    for key in encoded_input:
        encoded_input[key] = tf.expand_dims(
            tf.convert_to_tensor(encoded_input[key]), 0
        )

    shark_module = SharkInference(
        BertModule(),
        (
            encoded_input["input_ids"],
            encoded_input["attention_mask"],
            encoded_input["token_type_ids"],
        ),
    )
    shark_module.set_frontend("tensorflow")
    shark_module.compile()

    print(
        shark_module.forward(
            (
                encoded_input["input_ids"],
                encoded_input["attention_mask"],
                encoded_input["token_type_ids"],
            )
        )
    )

```

`shark/examples/shark_inference/minilm_tf_gpu_config.json`:

```json
{"options": [{"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 64, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 32, 32, 16], "work_group_sizes": [64, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 32, 16], "work_group_sizes": [64, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [32, 64, 32], "work_group_sizes": [128, 1, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4}, {"work_group_tile_sizes": [1, 64, 64, 32], "work_group_sizes": [128, 2, 1], "pipeline": "GPU_TENSORCORE", "pipeline_depth": 4, "split_k": 8}, {"work_group_tile_sizes": [1, 32, 128], "work_group_sizes": [32, 1, 1], "pipeline": "GPU"}]}
```

`shark/examples/shark_inference/resnest.py`:

```py
import torch
import torchvision.models as models
from shark.shark_inference import SharkInference
from shark.shark_importer import SharkImporter

torch.hub.list("zhanghang1989/ResNeSt", force_reload=True)


class ResnestModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = torch.hub.load(
            "zhanghang1989/ResNeSt", "resnest50", pretrained=True
        )
        self.model.eval()

    def forward(self, input):
        return self.model.forward(input)


input = torch.randn(1, 3, 224, 224)


mlir_importer = SharkImporter(
    ResnestModule(),
    (input,),
    frontend="torch",
)

(vision_mlir, func_name), inputs, golden_out = mlir_importer.import_debug(
    tracing_required=True
)

print(golden_out)

shark_module = SharkInference(vision_mlir, mlir_dialect="linalg")
shark_module.compile()
result = shark_module.forward((input,))
print("Obtained result", result)

```

`shark/examples/shark_inference/resnet50_fp16.py`:

```py
from shark.shark_inference import SharkInference
from shark.parser import shark_args

import torch
import numpy as np
import sys
import torchvision.models as models
import torch_mlir

torch.manual_seed(0)


class VisionModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = models.resnet50(pretrained=True)
        self.train(False)

    def forward(self, input):
        return self.model.forward(input)


model = VisionModule()
test_input = torch.randn(1, 3, 224, 224)
actual_out = model(test_input)

test_input_fp16 = test_input.to(device=torch.device("cuda"), dtype=torch.half)
model_fp16 = model.half()
model_fp16.eval()
model_fp16.to("cuda")
actual_out_fp16 = model_fp16(test_input_fp16)

ts_g = torch.jit.trace(model_fp16, [test_input_fp16])

module = torch_mlir.compile(
    ts_g,
    (test_input_fp16),
    torch_mlir.OutputType.LINALG_ON_TENSORS,
    use_tracing=True,
    verbose=False,
)

# from contextlib import redirect_stdout

# with open('resnet50_fp16_linalg_ir.mlir', 'w') as f:
#     with redirect_stdout(f):
#         print(module.operation.get_asm())

mlir_model = module
func_name = "forward"

shark_module = SharkInference(mlir_model, device="cuda", mlir_dialect="linalg")
shark_module.compile()


def shark_result(x):
    x_ny = x.cpu().detach().numpy()
    inputs = (x_ny,)
    result = shark_module.forward(inputs)
    return torch.from_numpy(result)


observed_out = shark_result(test_input_fp16)

print("Golden result:", actual_out_fp16)
print("SHARK result:", observed_out)

actual_out_fp16 = actual_out_fp16.to(device=torch.device("cpu"))

print(
    torch.testing.assert_allclose(
        actual_out_fp16, observed_out, rtol=1e-2, atol=1e-2
    )
)

```

`shark/examples/shark_inference/resnet50_script.py`:

```py
from PIL import Image
import requests
import torch
import torchvision.models as models
from torchvision import transforms
import sys
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model


################################## Preprocessing inputs and model ############
def load_and_preprocess_image(url: str):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"
    }
    img = Image.open(
        requests.get(url, headers=headers, stream=True).raw
    ).convert("RGB")
    # preprocessing pipeline
    preprocess = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
            ),
        ]
    )
    img_preprocessed = preprocess(img)
    return torch.unsqueeze(img_preprocessed, 0)


def load_labels():
    classes_text = requests.get(
        "https://raw.githubusercontent.com/cathyzhyi/ml-data/main/imagenet-classes.txt",
        stream=True,
    ).text
    labels = [line.strip() for line in classes_text.splitlines()]
    return labels


def top3_possibilities(res):
    _, indexes = torch.sort(res, descending=True)
    percentage = torch.nn.functional.softmax(res, dim=1)[0] * 100
    top3 = [(labels[idx], percentage[idx].item()) for idx in indexes[0][:3]]
    return top3


class Resnet50Module(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.resnet = models.resnet50(pretrained=True)
        self.train(False)

    def forward(self, img):
        return self.resnet.forward(img)


image_url = "https://upload.wikimedia.org/wikipedia/commons/2/26/YellowLabradorLooking_new.jpg"
print("load image from " + image_url, file=sys.stderr)
img = load_and_preprocess_image(image_url)
labels = load_labels()

##############################################################################


## Can pass any img or input to the forward module.
mlir_model, func_name, inputs, golden_out = download_model(
    "resnet50", frontend="torch"
)

shark_module = SharkInference(mlir_model, mlir_dialect="linalg")
shark_module.compile()
path = shark_module.save_module()
shark_module.load_module(path)
result = shark_module("forward", (img.detach().numpy(),))

print("The top 3 results obtained via shark_runner is:")
print(top3_possibilities(torch.from_numpy(result)))

print()

print("The top 3 results obtained via torch is:")
print(top3_possibilities(Resnet50Module()(img)))

```

`shark/examples/shark_inference/sharded_bloom.py`:

```py
####################################################################################
# Please make sure you have transformers 4.21.2 installed before running this demo
#
# -p --model_path: the directory in which you want to store the bloom files.
# -dl --device_list: the list of device indices you want to use.  if you want to only use the first device, or you are running on cpu leave this blank.
#                     Otherwise, please give this argument in this format: "[0, 1, 2]"
# -de --device: the device you want to run bloom on.  E.G. cpu, cuda
# -c, --recompile: set to true if you want to recompile to vmfb.
# -d, --download: set to true if you want to redownload the mlir files
# -cm, --create_mlirs: set to true if you want to create the mlir files from scratch.  please make sure you have transformers 4.21.2 before using this option
# -t --token_count: the number of tokens you want to generate
# -pr --prompt: the prompt you want to feed to the model
# -m --model_name: the name of the model, e.g. bloom-560m
#
# If you don't specify a prompt when you run this example, you will be able to give prompts through the terminal.  Run the
# example in this way if you want to run multiple examples without reinitializing the model
#####################################################################################

import os
import io
import torch
import torch.nn as nn
from collections import OrderedDict
import torch_mlir
from torch_mlir import TensorPlaceholder
import re
from transformers.models.bloom.configuration_bloom import BloomConfig
import json
import sys
import argparse
import json
import urllib.request
import subprocess

from torch.fx.experimental.proxy_tensor import make_fx
from torch._decomp import get_decompositions
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_public_file
from transformers import (
    BloomTokenizerFast,
    BloomForSequenceClassification,
    BloomForCausalLM,
)
from transformers.models.bloom.modeling_bloom import (
    BloomBlock,
    build_alibi_tensor,
)

IS_CUDA = False


class ShardedBloom:
    def __init__(self, src_folder):
        f = open(f"{src_folder}/config.json")
        config = json.load(f)
        f.close()

        self.layers_initialized = False

        self.src_folder = src_folder
        try:
            self.n_embed = config["n_embed"]
        except KeyError:
            self.n_embed = config["hidden_size"]
        self.vocab_size = config["vocab_size"]
        self.n_layer = config["n_layer"]
        try:
            self.n_head = config["num_attention_heads"]
        except KeyError:
            self.n_head = config["n_head"]

    def _init_layer(self, layer_name, device, replace, device_idx):
        if replace or not os.path.exists(
            f"{self.src_folder}/{layer_name}.vmfb"
        ):
            f_ = open(f"{self.src_folder}/{layer_name}.mlir", encoding="utf-8")
            module = f_.read()
            f_.close()
            module = bytes(module, "utf-8")
            shark_module = SharkInference(
                module,
                device=device,
                mlir_dialect="tm_tensor",
                device_idx=device_idx,
            )
            shark_module.save_module(
                module_name=f"{self.src_folder}/{layer_name}",
                extra_args=[
                    "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                    "--iree-stream-resource-max-allocation-size=1000000000",
                    "--iree-codegen-check-ir-before-llvm-conversion=false",
                ],
            )
        else:
            shark_module = SharkInference(
                "",
                device=device,
                mlir_dialect="tm_tensor",
                device_idx=device_idx,
            )

        return shark_module

    def init_layers(self, device, replace=False, device_idx=[0]):
        if device_idx is not None:
            n_devices = len(device_idx)

        self.word_embeddings_module = self._init_layer(
            "word_embeddings",
            device,
            replace,
            device_idx if device_idx is None else device_idx[0 % n_devices],
        )
        self.word_embeddings_layernorm_module = self._init_layer(
            "word_embeddings_layernorm",
            device,
            replace,
            device_idx if device_idx is None else device_idx[1 % n_devices],
        )
        self.ln_f_module = self._init_layer(
            "ln_f",
            device,
            replace,
            device_idx if device_idx is None else device_idx[2 % n_devices],
        )
        self.lm_head_module = self._init_layer(
            "lm_head",
            device,
            replace,
            device_idx if device_idx is None else device_idx[3 % n_devices],
        )
        self.block_modules = [
            self._init_layer(
                f"bloom_block_{i}",
                device,
                replace,
                device_idx
                if device_idx is None
                else device_idx[(i + 4) % n_devices],
            )
            for i in range(self.n_layer)
        ]

        self.layers_initialized = True

    def load_layers(self):
        assert self.layers_initialized

        self.word_embeddings_module.load_module(
            f"{self.src_folder}/word_embeddings.vmfb"
        )
        self.word_embeddings_layernorm_module.load_module(
            f"{self.src_folder}/word_embeddings_layernorm.vmfb"
        )
        for block_module, i in zip(self.block_modules, range(self.n_layer)):
            block_module.load_module(f"{self.src_folder}/bloom_block_{i}.vmfb")
        self.ln_f_module.load_module(f"{self.src_folder}/ln_f.vmfb")
        self.lm_head_module.load_module(f"{self.src_folder}/lm_head.vmfb")

    def forward_pass(self, input_ids, device):
        if IS_CUDA:
            cudaSetDevice(self.word_embeddings_module.device_idx)

        input_embeds = self.word_embeddings_module(
            inputs=(input_ids,), function_name="forward"
        )

        input_embeds = torch.tensor(input_embeds).float()
        if IS_CUDA:
            cudaSetDevice(self.word_embeddings_layernorm_module.device_idx)
        hidden_states = self.word_embeddings_layernorm_module(
            inputs=(input_embeds,), function_name="forward"
        )

        hidden_states = torch.tensor(hidden_states).float()

        attention_mask = torch.ones(
            [hidden_states.shape[0], len(input_ids[0])]
        )
        alibi = build_alibi_tensor(
            attention_mask,
            self.n_head,
            hidden_states.dtype,
            hidden_states.device,
        )

        causal_mask = _prepare_attn_mask(
            attention_mask, input_ids.size(), input_embeds, 0
        )
        causal_mask = torch.tensor(causal_mask).float()

        presents = ()
        all_hidden_states = tuple(hidden_states)

        for block_module, i in zip(self.block_modules, range(self.n_layer)):
            if IS_CUDA:
                cudaSetDevice(block_module.device_idx)

            output = block_module(
                inputs=(
                    hidden_states.detach().numpy(),
                    alibi.detach().numpy(),
                    causal_mask.detach().numpy(),
                ),
                function_name="forward",
            )
            hidden_states = torch.tensor(output[0]).float()
            all_hidden_states = all_hidden_states + (hidden_states,)
            presents = presents + (
                tuple(
                    (
                        output[1],
                        output[2],
                    )
                ),
            )
        if IS_CUDA:
            cudaSetDevice(self.ln_f_module.device_idx)

        hidden_states = self.ln_f_module(
            inputs=(hidden_states,), function_name="forward"
        )
        if IS_CUDA:
            cudaSetDevice(self.lm_head_module.device_idx)

        logits = self.lm_head_module(
            inputs=(hidden_states,), function_name="forward"
        )
        logits = torch.tensor(logits).float()

        return torch.argmax(logits[:, -1, :], dim=-1)


def _make_causal_mask(
    input_ids_shape: torch.Size,
    dtype: torch.dtype,
    past_key_values_length: int = 0,
):
    """
    Make causal mask used for bi-directional self-attention.
    """
    batch_size, target_length = input_ids_shape
    mask = torch.full((target_length, target_length), torch.finfo(dtype).min)
    mask_cond = torch.arange(mask.size(-1))
    intermediate_mask = mask_cond < (mask_cond + 1).view(mask.size(-1), 1)
    mask.masked_fill_(intermediate_mask, 0)
    mask = mask.to(dtype)

    if past_key_values_length > 0:
        mask = torch.cat(
            [
                torch.zeros(
                    target_length, past_key_values_length, dtype=dtype
                ),
                mask,
            ],
            dim=-1,
        )
    expanded_mask = mask[None, None, :, :].expand(
        batch_size, 1, target_length, target_length + past_key_values_length
    )
    return expanded_mask


def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: int = None):
    """
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    """
    batch_size, source_length = mask.size()
    tgt_len = tgt_len if tgt_len is not None else source_length

    expanded_mask = (
        mask[:, None, None, :]
        .expand(batch_size, 1, tgt_len, source_length)
        .to(dtype)
    )

    inverted_mask = 1.0 - expanded_mask

    return inverted_mask.masked_fill(
        inverted_mask.to(torch.bool), torch.finfo(dtype).min
    )


def _prepare_attn_mask(
    attention_mask, input_shape, inputs_embeds, past_key_values_length
):
    # create causal mask
    # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
    combined_attention_mask = None
    if input_shape[-1] > 1:
        combined_attention_mask = _make_causal_mask(
            input_shape,
            inputs_embeds.dtype,
            past_key_values_length=past_key_values_length,
        ).to(attention_mask.device)

    if attention_mask is not None:
        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
        expanded_attn_mask = _expand_mask(
            attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
        )
        combined_attention_mask = (
            expanded_attn_mask
            if combined_attention_mask is None
            else expanded_attn_mask + combined_attention_mask
        )

    return combined_attention_mask


def download_model(destination_folder, model_name):
    download_public_file(
        f"gs://shark_tank/sharded_bloom/{model_name}/", destination_folder
    )


def compile_embeddings(embeddings_layer, input_ids, path):
    input_ids_placeholder = torch_mlir.TensorPlaceholder.like(
        input_ids, dynamic_axes=[1]
    )
    module = torch_mlir.compile(
        embeddings_layer,
        (input_ids_placeholder),
        torch_mlir.OutputType.LINALG_ON_TENSORS,
        use_tracing=False,
        verbose=False,
    )

    bytecode_stream = io.BytesIO()
    module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    f_ = open(path, "w+")
    f_.write(str(module))
    f_.close()
    return


def compile_word_embeddings_layernorm(
    embeddings_layer_layernorm, embeds, path
):
    embeds_placeholder = torch_mlir.TensorPlaceholder.like(
        embeds, dynamic_axes=[1]
    )
    module = torch_mlir.compile(
        embeddings_layer_layernorm,
        (embeds_placeholder),
        torch_mlir.OutputType.LINALG_ON_TENSORS,
        use_tracing=False,
        verbose=False,
    )

    bytecode_stream = io.BytesIO()
    module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    f_ = open(path, "w+")
    f_.write(str(module))
    f_.close()
    return


def strip_overloads(gm):
    """
    Modifies the target of graph nodes in :attr:`gm` to strip overloads.
    Args:
        gm(fx.GraphModule): The input Fx graph module to be modified
    """
    for node in gm.graph.nodes:
        if isinstance(node.target, torch._ops.OpOverload):
            node.target = node.target.overloadpacket
    gm.recompile()


def compile_to_mlir(
    bblock,
    hidden_states,
    layer_past=None,
    attention_mask=None,
    head_mask=None,
    use_cache=None,
    output_attentions=False,
    alibi=None,
    block_index=0,
    path=".",
):
    fx_g = make_fx(
        bblock,
        decomposition_table=get_decompositions(
            [
                torch.ops.aten.split.Tensor,
                torch.ops.aten.split_with_sizes,
            ]
        ),
        tracing_mode="real",
        _allow_non_fake_inputs=False,
    )(hidden_states, alibi, attention_mask)

    fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
    fx_g.recompile()

    strip_overloads(fx_g)

    hidden_states_placeholder = TensorPlaceholder.like(
        hidden_states, dynamic_axes=[1]
    )
    attention_mask_placeholder = TensorPlaceholder.like(
        attention_mask, dynamic_axes=[2, 3]
    )
    alibi_placeholder = TensorPlaceholder.like(alibi, dynamic_axes=[2])

    ts_g = torch.jit.script(fx_g)

    module = torch_mlir.compile(
        ts_g,
        (
            hidden_states_placeholder,
            alibi_placeholder,
            attention_mask_placeholder,
        ),
        torch_mlir.OutputType.LINALG_ON_TENSORS,
        use_tracing=False,
        verbose=False,
    )

    module_placeholder = module
    module_context = module_placeholder.context

    def check_valid_line(line, line_n, mlir_file_len):
        if "private" in line:
            return False
        if "attributes" in line:
            return False
        if mlir_file_len - line_n == 2:
            return False

        return True

    mlir_file_len = len(str(module).split("\n"))

    def remove_constant_dim(line):
        if "17x" in line:
            line = re.sub("17x", "?x", line)
            line = re.sub("tensor.empty\(\)", "tensor.empty(%dim)", line)
        if "tensor.empty" in line and "?x?" in line:
            line = re.sub(
                "tensor.empty\(%dim\)", "tensor.empty(%dim, %dim)", line
            )
        if "arith.cmpi eq" in line:
            line = re.sub("c17", "dim", line)
        if " 17," in line:
            line = re.sub(" 17,", " %dim,", line)
        return line

    module = "\n".join(
        [
            remove_constant_dim(line)
            for line, line_n in zip(
                str(module).split("\n"), range(mlir_file_len)
            )
            if check_valid_line(line, line_n, mlir_file_len)
        ]
    )

    module = module_placeholder.parse(module, context=module_context)
    bytecode_stream = io.BytesIO()
    module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    f_ = open(path, "w+")
    f_.write(str(module))
    f_.close()
    return


def compile_ln_f(ln_f, hidden_layers, path):
    hidden_layers_placeholder = torch_mlir.TensorPlaceholder.like(
        hidden_layers, dynamic_axes=[1]
    )
    module = torch_mlir.compile(
        ln_f,
        (hidden_layers_placeholder),
        torch_mlir.OutputType.LINALG_ON_TENSORS,
        use_tracing=False,
        verbose=False,
    )

    bytecode_stream = io.BytesIO()
    module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    f_ = open(path, "w+")
    f_.write(str(module))
    f_.close()
    return


def compile_lm_head(lm_head, hidden_layers, path):
    hidden_layers_placeholder = torch_mlir.TensorPlaceholder.like(
        hidden_layers, dynamic_axes=[1]
    )
    module = torch_mlir.compile(
        lm_head,
        (hidden_layers_placeholder),
        torch_mlir.OutputType.LINALG_ON_TENSORS,
        use_tracing=False,
        verbose=False,
    )

    bytecode_stream = io.BytesIO()
    module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    f_ = open(path, "w+")
    f_.write(str(module))
    f_.close()
    return


def create_mlirs(destination_folder, model_name):
    model_config = "bigscience/" + model_name
    sample_input_ids = torch.ones([1, 17], dtype=torch.int64)

    urllib.request.urlretrieve(
        f"https://huggingface.co/bigscience/{model_name}/resolve/main/config.json",
        filename=f"{destination_folder}/config.json",
    )
    urllib.request.urlretrieve(
        f"https://huggingface.co/bigscience/bloom/resolve/main/tokenizer.json",
        filename=f"{destination_folder}/tokenizer.json",
    )

    class HuggingFaceLanguage(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.model = BloomForCausalLM.from_pretrained(model_config)

        def forward(self, tokens):
            return self.model.forward(tokens)[0]

    class HuggingFaceBlock(torch.nn.Module):
        def __init__(self, block):
            super().__init__()
            self.model = block

        def forward(self, tokens, alibi, attention_mask):
            output = self.model(
                hidden_states=tokens,
                alibi=alibi,
                attention_mask=attention_mask,
                use_cache=True,
                output_attentions=False,
            )
            return (output[0], output[1][0], output[1][1])

    model = HuggingFaceLanguage()

    compile_embeddings(
        model.model.transformer.word_embeddings,
        sample_input_ids,
        f"{destination_folder}/word_embeddings.mlir",
    )

    inputs_embeds = model.model.transformer.word_embeddings(sample_input_ids)

    compile_word_embeddings_layernorm(
        model.model.transformer.word_embeddings_layernorm,
        inputs_embeds,
        f"{destination_folder}/word_embeddings_layernorm.mlir",
    )

    hidden_states = model.model.transformer.word_embeddings_layernorm(
        inputs_embeds
    )

    input_shape = sample_input_ids.size()

    current_sequence_length = hidden_states.shape[1]
    past_key_values_length = 0
    past_key_values = tuple([None] * len(model.model.transformer.h))

    attention_mask = torch.ones(
        (hidden_states.shape[0], current_sequence_length), device="cpu"
    )

    alibi = build_alibi_tensor(
        attention_mask,
        model.model.transformer.n_head,
        hidden_states.dtype,
        "cpu",
    )

    causal_mask = _prepare_attn_mask(
        attention_mask, input_shape, inputs_embeds, past_key_values_length
    )

    head_mask = model.model.transformer.get_head_mask(
        None, model.model.transformer.config.n_layer
    )
    output_attentions = model.model.transformer.config.output_attentions

    all_hidden_states = ()

    for i, (block, layer_past) in enumerate(
        zip(model.model.transformer.h, past_key_values)
    ):
        all_hidden_states = all_hidden_states + (hidden_states,)

        proxy_model = HuggingFaceBlock(block)

        compile_to_mlir(
            proxy_model,
            hidden_states,
            layer_past=layer_past,
            attention_mask=causal_mask,
            head_mask=head_mask[i],
            use_cache=True,
            output_attentions=output_attentions,
            alibi=alibi,
            block_index=i,
            path=f"{destination_folder}/bloom_block_{i}.mlir",
        )

    compile_ln_f(
        model.model.transformer.ln_f,
        hidden_states,
        f"{destination_folder}/ln_f.mlir",
    )
    hidden_states = model.model.transformer.ln_f(hidden_states)
    compile_lm_head(
        model.model.lm_head,
        hidden_states,
        f"{destination_folder}/lm_head.mlir",
    )


def run_large_model(
    token_count,
    recompile,
    model_path,
    prompt,
    device_list,
    script_path,
    device,
):
    f = open(f"{model_path}/prompt.txt", "w+")
    f.write(prompt)
    f.close()
    for i in range(token_count):
        if i == 0:
            will_compile = recompile
        else:
            will_compile = False
            f = open(f"{model_path}/prompt.txt", "r")
            prompt = f.read()
            f.close()

        subprocess.run(
            [
                "python",
                script_path,
                model_path,
                "start",
                str(will_compile),
                "cpu",
                "None",
                prompt,
            ]
        )
        for i in range(config["n_layer"]):
            if device_list is not None:
                device_idx = str(device_list[i % len(device_list)])
            else:
                device_idx = "None"
            subprocess.run(
                [
                    "python",
                    script_path,
                    model_path,
                    str(i),
                    str(will_compile),
                    device,
                    device_idx,
                    prompt,
                ]
            )
        subprocess.run(
            [
                "python",
                script_path,
                model_path,
                "end",
                str(will_compile),
                "cpu",
                "None",
                prompt,
            ]
        )

    f = open(f"{model_path}/prompt.txt", "r")
    output = f.read()
    f.close()
    print(output)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="Bloom-560m")
    parser.add_argument("-p", "--model_path")
    parser.add_argument("-dl", "--device_list", default=None)
    parser.add_argument("-de", "--device", default="cpu")
    parser.add_argument("-c", "--recompile", default=False, type=bool)
    parser.add_argument("-d", "--download", default=False, type=bool)
    parser.add_argument("-t", "--token_count", default=10, type=int)
    parser.add_argument("-m", "--model_name", default="bloom-560m")
    parser.add_argument("-cm", "--create_mlirs", default=False, type=bool)

    parser.add_argument(
        "-lm", "--large_model_memory_efficient", default=False, type=bool
    )

    parser.add_argument(
        "-pr",
        "--prompt",
        default=None,
    )
    args = parser.parse_args()

    if args.create_mlirs and args.large_model_memory_efficient:
        print(
            "Warning: If you need to use memory efficient mode, you probably want to use 'download' instead"
        )

    if not os.path.isdir(args.model_path):
        os.mkdir(args.model_path)

    if args.device_list is not None:
        args.device_list = json.loads(args.device_list)

    if args.device == "cuda" and args.device_list is not None:
        IS_CUDA = True
        from cuda.cudart import cudaSetDevice
    if args.download and args.create_mlirs:
        print(
            "WARNING: It is not advised to turn on both download and create_mlirs"
        )
    if args.download:
        download_model(args.model_path, args.model_name)
    if args.create_mlirs:
        create_mlirs(args.model_path, args.model_name)
    from transformers import AutoTokenizer, AutoModelForCausalLM, BloomConfig

    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    if args.prompt is not None:
        input_ids = tokenizer.encode(args.prompt, return_tensors="pt")

    if args.large_model_memory_efficient:
        f = open(f"{args.model_path}/config.json")
        config = json.load(f)
        f.close()

        self_path = os.path.dirname(os.path.abspath(__file__))
        script_path = os.path.join(self_path, "sharded_bloom_large_models.py")

        if args.prompt is not None:
            run_large_model(
                args.token_count,
                args.recompile,
                args.model_path,
                args.prompt,
                args.device_list,
                script_path,
                args.device,
            )

        else:
            while True:
                prompt = input("Enter Prompt: ")
                try:
                    token_count = int(
                        input("Enter number of tokens you want to generate: ")
                    )
                except:
                    print(
                        "Invalid integer entered.  Using default value of 10"
                    )
                    token_count = 10

                run_large_model(
                    token_count,
                    args.recompile,
                    args.model_path,
                    prompt,
                    args.device_list,
                    script_path,
                    args.device,
                )

    else:
        shardedbloom = ShardedBloom(args.model_path)
        shardedbloom.init_layers(
            device=args.device,
            replace=args.recompile,
            device_idx=args.device_list,
        )
        shardedbloom.load_layers()

        if args.prompt is not None:
            for _ in range(args.token_count):
                next_token = shardedbloom.forward_pass(
                    torch.tensor(input_ids), device=args.device
                )
                input_ids = torch.cat(
                    [input_ids, next_token.unsqueeze(-1)], dim=-1
                )

            print(tokenizer.decode(input_ids.squeeze()))

        else:
            while True:
                prompt = input("Enter Prompt: ")
                try:
                    token_count = int(
                        input("Enter number of tokens you want to generate: ")
                    )
                except:
                    print(
                        "Invalid integer entered.  Using default value of 10"
                    )
                    token_count = 10

                input_ids = tokenizer.encode(prompt, return_tensors="pt")

                for _ in range(token_count):
                    next_token = shardedbloom.forward_pass(
                        torch.tensor(input_ids), device=args.device
                    )
                    input_ids = torch.cat(
                        [input_ids, next_token.unsqueeze(-1)], dim=-1
                    )

                print(tokenizer.decode(input_ids.squeeze()))

```

`shark/examples/shark_inference/sharded_bloom_large_models.py`:

```py
import sys
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, BloomConfig
import re
from shark.shark_inference import SharkInference
import torch
import torch.nn as nn
from collections import OrderedDict
from transformers.models.bloom.modeling_bloom import (
    BloomBlock,
    build_alibi_tensor,
)
import time
import json


def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: int = None):
    """
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    """
    batch_size, source_length = mask.size()
    tgt_len = tgt_len if tgt_len is not None else source_length

    expanded_mask = (
        mask[:, None, None, :]
        .expand(batch_size, 1, tgt_len, source_length)
        .to(dtype)
    )

    inverted_mask = 1.0 - expanded_mask

    return inverted_mask.masked_fill(
        inverted_mask.to(torch.bool), torch.finfo(dtype).min
    )


def _prepare_attn_mask(
    attention_mask, input_shape, inputs_embeds, past_key_values_length
):
    # create causal mask
    # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
    combined_attention_mask = None
    if input_shape[-1] > 1:
        combined_attention_mask = _make_causal_mask(
            input_shape,
            inputs_embeds.dtype,
            past_key_values_length=past_key_values_length,
        ).to(attention_mask.device)

    if attention_mask is not None:
        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
        expanded_attn_mask = _expand_mask(
            attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
        )
        combined_attention_mask = (
            expanded_attn_mask
            if combined_attention_mask is None
            else expanded_attn_mask + combined_attention_mask
        )

    return combined_attention_mask


def _make_causal_mask(
    input_ids_shape: torch.Size,
    dtype: torch.dtype,
    past_key_values_length: int = 0,
):
    """
    Make causal mask used for bi-directional self-attention.
    """
    batch_size, target_length = input_ids_shape
    mask = torch.full((target_length, target_length), torch.finfo(dtype).min)
    mask_cond = torch.arange(mask.size(-1))
    intermediate_mask = mask_cond < (mask_cond + 1).view(mask.size(-1), 1)
    mask.masked_fill_(intermediate_mask, 0)
    mask = mask.to(dtype)

    if past_key_values_length > 0:
        mask = torch.cat(
            [
                torch.zeros(
                    target_length, past_key_values_length, dtype=dtype
                ),
                mask,
            ],
            dim=-1,
        )
    expanded_mask = mask[None, None, :, :].expand(
        batch_size, 1, target_length, target_length + past_key_values_length
    )
    return expanded_mask


if __name__ == "__main__":
    working_dir = sys.argv[1]
    layer_name = sys.argv[2]
    will_compile = sys.argv[3]
    device = sys.argv[4]
    device_idx = sys.argv[5]
    prompt = sys.argv[6]

    if device_idx.lower().strip() == "none":
        device_idx = None
    else:
        device_idx = int(device_idx)

    if will_compile.lower().strip() == "true":
        will_compile = True
    else:
        will_compile = False

    f = open(f"{working_dir}/config.json")
    config = json.load(f)
    f.close()

    layers_initialized = False
    try:
        n_embed = config["n_embed"]
    except KeyError:
        n_embed = config["hidden_size"]
    vocab_size = config["vocab_size"]
    n_layer = config["n_layer"]
    try:
        n_head = config["num_attention_heads"]
    except KeyError:
        n_head = config["n_head"]

    if not os.path.isdir(working_dir):
        os.mkdir(working_dir)

    if layer_name == "start":
        tokenizer = AutoTokenizer.from_pretrained(working_dir)
        input_ids = tokenizer.encode(prompt, return_tensors="pt")

        mlir_str = ""

        if will_compile:
            f = open(f"{working_dir}/word_embeddings.mlir", encoding="utf-8")
            mlir_str = f.read()
            f.close()

            mlir_str = bytes(mlir_str, "utf-8")

        shark_module = SharkInference(
            mlir_str,
            device="cpu",
            mlir_dialect="tm_tensor",
            device_idx=None,
        )

        if will_compile:
            shark_module.save_module(
                module_name=f"{working_dir}/word_embeddings",
                extra_args=[
                    "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                    "--iree-stream-resource-max-allocation-size=1000000000",
                    "--iree-codegen-check-ir-before-llvm-conversion=false",
                ],
            )

        shark_module.load_module(f"{working_dir}/word_embeddings.vmfb")
        input_embeds = shark_module(
            inputs=(input_ids,), function_name="forward"
        )
        input_embeds = torch.tensor(input_embeds).float()

        mlir_str = ""

        if will_compile:
            f = open(
                f"{working_dir}/word_embeddings_layernorm.mlir",
                encoding="utf-8",
            )
            mlir_str = f.read()
            f.close()

        shark_module = SharkInference(
            mlir_str,
            device="cpu",
            mlir_dialect="tm_tensor",
            device_idx=None,
        )

        if will_compile:
            shark_module.save_module(
                module_name=f"{working_dir}/word_embeddings_layernorm",
                extra_args=[
                    "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                    "--iree-stream-resource-max-allocation-size=1000000000",
                    "--iree-codegen-check-ir-before-llvm-conversion=false",
                ],
            )

        shark_module.load_module(
            f"{working_dir}/word_embeddings_layernorm.vmfb"
        )
        hidden_states = shark_module(
            inputs=(input_embeds,), function_name="forward"
        )
        hidden_states = torch.tensor(hidden_states).float()

        torch.save(hidden_states, f"{working_dir}/hidden_states_0.pt")

        attention_mask = torch.ones(
            [hidden_states.shape[0], len(input_ids[0])]
        )

        attention_mask = torch.tensor(attention_mask).float()

        alibi = build_alibi_tensor(
            attention_mask,
            n_head,
            hidden_states.dtype,
            device="cpu",
        )

        torch.save(alibi, f"{working_dir}/alibi.pt")

        causal_mask = _prepare_attn_mask(
            attention_mask, input_ids.size(), input_embeds, 0
        )
        causal_mask = torch.tensor(causal_mask).float()

        torch.save(causal_mask, f"{working_dir}/causal_mask.pt")

    elif layer_name in [str(x) for x in range(n_layer)]:
        hidden_states = torch.load(
            f"{working_dir}/hidden_states_{layer_name}.pt"
        )
        alibi = torch.load(f"{working_dir}/alibi.pt")
        causal_mask = torch.load(f"{working_dir}/causal_mask.pt")

        mlir_str = ""

        if will_compile:
            f = open(
                f"{working_dir}/bloom_block_{layer_name}.mlir",
                encoding="utf-8",
            )
            mlir_str = f.read()
            f.close()

            mlir_str = bytes(mlir_str, "utf-8")

        shark_module = SharkInference(
            mlir_str,
            device=device,
            mlir_dialect="tm_tensor",
            device_idx=device_idx,
        )

        if will_compile:
            shark_module.save_module(
                module_name=f"{working_dir}/bloom_block_{layer_name}",
                extra_args=[
                    "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                    "--iree-stream-resource-max-allocation-size=1000000000",
                    "--iree-codegen-check-ir-before-llvm-conversion=false",
                ],
            )

        shark_module.load_module(
            f"{working_dir}/bloom_block_{layer_name}.vmfb"
        )

        output = shark_module(
            inputs=(
                hidden_states.detach().numpy(),
                alibi.detach().numpy(),
                causal_mask.detach().numpy(),
            ),
            function_name="forward",
        )

        hidden_states = torch.tensor(output[0]).float()

        torch.save(
            hidden_states,
            f"{working_dir}/hidden_states_{int(layer_name) + 1}.pt",
        )

    elif layer_name == "end":
        mlir_str = ""

        if will_compile:
            f = open(f"{working_dir}/ln_f.mlir", encoding="utf-8")
            mlir_str = f.read()
            f.close()

            mlir_str = bytes(mlir_str, "utf-8")

        shark_module = SharkInference(
            mlir_str,
            device="cpu",
            mlir_dialect="tm_tensor",
            device_idx=None,
        )

        if will_compile:
            shark_module.save_module(
                module_name=f"{working_dir}/ln_f",
                extra_args=[
                    "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                    "--iree-stream-resource-max-allocation-size=1000000000",
                    "--iree-codegen-check-ir-before-llvm-conversion=false",
                ],
            )

        shark_module.load_module(f"{working_dir}/ln_f.vmfb")

        hidden_states = torch.load(f"{working_dir}/hidden_states_{n_layer}.pt")

        hidden_states = shark_module(
            inputs=(hidden_states,), function_name="forward"
        )

        mlir_str = ""

        if will_compile:
            f = open(f"{working_dir}/lm_head.mlir", encoding="utf-8")
            mlir_str = f.read()
            f.close()

            mlir_str = bytes(mlir_str, "utf-8")

        if config["n_embed"] == 14336:

            def get_state_dict():
                d = torch.load(
                    f"{working_dir}/pytorch_model_00001-of-00072.bin"
                )
                return OrderedDict(
                    (k.replace("word_embeddings.", ""), v)
                    for k, v in d.items()
                )

            def load_causal_lm_head():
                linear = nn.utils.skip_init(
                    nn.Linear, 14336, 250880, bias=False, dtype=torch.float
                )
                linear.load_state_dict(get_state_dict(), strict=False)
                return linear.float()

            lm_head = load_causal_lm_head()

            logits = lm_head(torch.tensor(hidden_states).float())

        else:
            shark_module = SharkInference(
                mlir_str,
                device="cpu",
                mlir_dialect="tm_tensor",
                device_idx=None,
            )

            if will_compile:
                shark_module.save_module(
                    module_name=f"{working_dir}/lm_head",
                    extra_args=[
                        "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
                        "--iree-stream-resource-max-allocation-size=1000000000",
                        "--iree-codegen-check-ir-before-llvm-conversion=false",
                    ],
                )

            shark_module.load_module(f"{working_dir}/lm_head.vmfb")

            logits = shark_module(
                inputs=(hidden_states,), function_name="forward"
            )

        logits = torch.tensor(logits).float()

        tokenizer = AutoTokenizer.from_pretrained(working_dir)

        next_token = tokenizer.decode(torch.argmax(logits[:, -1, :], dim=-1))

        f = open(f"{working_dir}/prompt.txt", "w+")
        f.write(prompt + next_token)
        f.close()

```

`shark/examples/shark_inference/simple_dlrm.py`:

```py
# Description: an implementation of a deep learning recommendation model (DLRM)
# The model input consists of dense and sparse features. The former is a vector
# of floating point values. The latter is a list of sparse indices into
# embedding tables, which consist of vectors of floating point values.
# The selected vectors are passed to mlp networks denoted by triangles,
# in some cases the vectors are interacted through operators (Ops).
#
# output:
#                         vector of values
# model:                        |
#                              /\
#                             /__\
#                               |
#       _____________________> Op  <___________________
#     /                         |                      \
#    /\                        /\                      /\
#   /__\                      /__\           ...      /__\
#    |                          |                       |
#    |                         Op                      Op
#    |                    ____/__\_____           ____/__\____
#    |                   |_Emb_|____|__|    ...  |_Emb_|__|___|
# input:
# [ dense features ]     [sparse indices] , ..., [sparse indices]
#
# More precise definition of model layers:
# 1) fully connected layers of an mlp
# z = f(y)
# y = Wx + b
#
# 2) embedding lookup (for a list of sparse indices p=[p1,...,pk])
# z = Op(e1,...,ek)
# obtain vectors e1=E[:,p1], ..., ek=E[:,pk]
#
# 3) Operator Op can be one of the following
# Sum(e1,...,ek) = e1 + ... + ek
# Dot(e1,...,ek) = [e1'e1, ..., e1'ek, ..., ek'e1, ..., ek'ek]
# Cat(e1,...,ek) = [e1', ..., ek']'
# where ' denotes transpose operation
#
# References:
# [1] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
# Narayanan Sundaram, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,
# Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii,
# Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko,
# Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong,
# Misha Smelyanskiy, "Deep Learning Recommendation Model for Personalization and
# Recommendation Systems", CoRR, arXiv:1906.00091, 2019


import argparse
import sys
import numpy as np
import torch
import torch.nn as nn
from shark.shark_inference import SharkInference
from shark.shark_importer import SharkImporter


torch.manual_seed(0)
np.random.seed(0)


### define dlrm in PyTorch ###
class DLRM_Net(nn.Module):
    def create_mlp(self, ln, sigmoid_layer):
        # build MLP layer by layer
        layers = nn.ModuleList()
        for i in range(0, ln.size - 1):
            n = ln[i]
            m = ln[i + 1]

            # construct fully connected operator
            LL = nn.Linear(int(n), int(m), bias=True)

            # initialize the weights
            # with torch.no_grad():
            # custom Xavier input, output or two-sided fill

            mean = 0.0  # std_dev = np.sqrt(variance)
            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)
            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)
            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))
            bt = np.random.normal(mean, std_dev, size=m).astype(np.float32)
            LL.weight.data = torch.tensor(W, requires_grad=True)
            LL.bias.data = torch.tensor(bt, requires_grad=True)

            # approach 2
            # LL.weight.data.copy_(torch.tensor(W))
            # LL.bias.data.copy_(torch.tensor(bt))
            # approach 3
            # LL.weight = Parameter(torch.tensor(W),requires_grad=True)
            # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)
            layers.append(LL)

            # construct sigmoid or relu operator
            if i == sigmoid_layer:
                layers.append(nn.Sigmoid())
            else:
                layers.append(nn.ReLU())

        # approach 1: use ModuleList
        # return layers
        # approach 2: use Sequential container to wrap all layers
        return torch.nn.Sequential(*layers)

    def create_emb(self, m, ln, weighted_pooling=None):
        emb_l = nn.ModuleList()
        v_W_l = []
        for i in range(0, ln.size):
            n = ln[i]

            # construct embedding operator
            EE = nn.EmbeddingBag(n, m, mode="sum")
            # initialize embeddings
            # nn.init.uniform_(EE.weight, a=-np.sqrt(1 / n), b=np.sqrt(1 / n))
            W = np.random.uniform(
                low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)
            ).astype(np.float32)
            # approach 1
            print(W)
            EE.weight.data = torch.tensor(W, requires_grad=True)
            # approach 2
            # EE.weight.data.copy_(torch.tensor(W))
            # approach 3
            # EE.weight = Parameter(torch.tensor(W),requires_grad=True)
            if weighted_pooling is None:
                v_W_l.append(None)
            else:
                v_W_l.append(torch.ones(n, dtype=torch.float32))
            emb_l.append(EE)
        return emb_l, v_W_l

    def __init__(
        self,
        m_spa=None,
        ln_emb=None,
        ln_bot=None,
        ln_top=None,
        arch_interaction_op=None,
        arch_interaction_itself=False,
        sigmoid_bot=-1,
        sigmoid_top=-1,
        weighted_pooling=None,
    ):
        super(DLRM_Net, self).__init__()

        if (
            (m_spa is not None)
            and (ln_emb is not None)
            and (ln_bot is not None)
            and (ln_top is not None)
            and (arch_interaction_op is not None)
        ):
            # save arguments
            self.output_d = 0
            self.arch_interaction_op = arch_interaction_op
            self.arch_interaction_itself = arch_interaction_itself
            if weighted_pooling is not None and weighted_pooling != "fixed":
                self.weighted_pooling = "learned"
            else:
                self.weighted_pooling = weighted_pooling

            # create operators
            self.emb_l, w_list = self.create_emb(
                m_spa, ln_emb, weighted_pooling
            )
            if self.weighted_pooling == "learned":
                self.v_W_l = nn.ParameterList()
                for w in w_list:
                    self.v_W_l.append(nn.Parameter(w))
            else:
                self.v_W_l = w_list
            self.bot_l = self.create_mlp(ln_bot, sigmoid_bot)
            self.top_l = self.create_mlp(ln_top, sigmoid_top)

    def apply_mlp(self, x, layers):
        return layers(x)

    def apply_emb(self, lS_o, lS_i, emb_l, v_W_l):
        # WARNING: notice that we are processing the batch at once. We implicitly
        # assume that the data is laid out such that:
        # 1. each embedding is indexed with a group of sparse indices,
        #   corresponding to a single lookup
        # 2. for each embedding the lookups are further organized into a batch
        # 3. for a list of embedding tables there is a list of batched lookups
        # TORCH-MLIR
        # We are passing all the embeddings as arguments for easy parsing.

        ly = []
        for k, sparse_index_group_batch in enumerate(lS_i):
            sparse_offset_group_batch = lS_o[k]

            # embedding lookup
            # We are using EmbeddingBag, which implicitly uses sum operator.
            # The embeddings are represented as tall matrices, with sum
            # happening vertically across 0 axis, resulting in a row vector
            # E = emb_l[k]

            if v_W_l[k] is not None:
                per_sample_weights = v_W_l[k].gather(
                    0, sparse_index_group_batch
                )
            else:
                per_sample_weights = None

            E = emb_l[k]
            V = E(
                sparse_index_group_batch,
                sparse_offset_group_batch,
                per_sample_weights=per_sample_weights,
            )

            ly.append(V)

        return ly

    def interact_features(self, x, ly):
        if self.arch_interaction_op == "dot":
            # concatenate dense and sparse features
            (batch_size, d) = x.shape
            T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))
            # perform a dot product
            Z = torch.bmm(T, torch.transpose(T, 1, 2))
            # append dense feature with the interactions (into a row vector)
            # approach 1: all
            # Zflat = Z.view((batch_size, -1))
            # approach 2: unique
            _, ni, nj = Z.shape
            # approach 1: tril_indices
            # offset = 0 if self.arch_interaction_itself else -1
            # li, lj = torch.tril_indices(ni, nj, offset=offset)
            # approach 2: custom
            offset = 1 if self.arch_interaction_itself else 0
            li = torch.tensor(
                [i for i in range(ni) for j in range(i + offset)]
            )
            lj = torch.tensor(
                [j for i in range(nj) for j in range(i + offset)]
            )
            Zflat = Z[:, li, lj]
            # concatenate dense features and interactions
            R = torch.cat([x] + [Zflat], dim=1)
        elif self.arch_interaction_op == "cat":
            # concatenation features (into a row vector)
            R = torch.cat([x] + ly, dim=1)
        else:
            sys.exit(
                "ERROR: --arch-interaction-op="
                + self.arch_interaction_op
                + " is not supported"
            )

        return R

    def forward(self, dense_x, lS_o, *lS_i):
        return self.sequential_forward(dense_x, lS_o, lS_i)

    def sequential_forward(self, dense_x, lS_o, lS_i):
        # process dense features (using bottom mlp), resulting in a row vector
        x = self.apply_mlp(dense_x, self.bot_l)
        # debug prints
        # print("intermediate")
        # print(x.detach().cpu().numpy())

        # process sparse features(using embeddings), resulting in a list of row vectors
        ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)
        # for y in ly:
        #     print(y.detach().cpu().numpy())

        # interact features (dense and sparse)
        z = self.interact_features(x, ly)
        # print(z.detach().cpu().numpy())

        # obtain probability of a click (using top mlp)
        p = self.apply_mlp(z, self.top_l)

        # # clamp output if needed
        # if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:
        # z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))
        # else:
        # z = p

        return p


def dash_separated_ints(value):
    vals = value.split("-")
    for val in vals:
        try:
            int(val)
        except ValueError:
            raise argparse.ArgumentTypeError(
                "%s is not a valid dash separated list of ints" % value
            )

    return value


# model related parameters
parser = argparse.ArgumentParser(
    description="Train Deep Learning Recommendation Model (DLRM)"
)
parser.add_argument("--arch-sparse-feature-size", type=int, default=2)
parser.add_argument(
    "--arch-embedding-size", type=dash_separated_ints, default="4-3-2"
)
# j will be replaced with the table number
parser.add_argument(
    "--arch-mlp-bot", type=dash_separated_ints, default="4-3-2"
)
parser.add_argument(
    "--arch-mlp-top", type=dash_separated_ints, default="8-2-1"
)
parser.add_argument(
    "--arch-interaction-op", type=str, choices=["dot", "cat"], default="dot"
)
parser.add_argument(
    "--arch-interaction-itself", action="store_true", default=False
)
parser.add_argument("--weighted-pooling", type=str, default=None)

args = parser.parse_args()

ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep="-")
ln_top = np.fromstring(args.arch_mlp_top, dtype=int, sep="-")
m_den = ln_bot[0]
ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep="-")
m_spa = args.arch_sparse_feature_size
ln_emb = np.asarray(ln_emb)
num_fea = ln_emb.size + 1  # num sparse + num dense features


# Initialize the model.
dlrm_model = DLRM_Net(
    m_spa=m_spa,
    ln_emb=ln_emb,
    ln_bot=ln_bot,
    ln_top=ln_top,
    arch_interaction_op=args.arch_interaction_op,
)


# Inputs to the model.
dense_inp = torch.tensor([[0.6965, 0.2861, 0.2269, 0.5513]])
vs0 = torch.tensor([[0], [0], [0]], dtype=torch.int64)
vsi = torch.tensor([1, 2, 3]), torch.tensor([1]), torch.tensor([1])

input_dlrm = (dense_inp, vs0, *vsi)

golden_output = dlrm_model(dense_inp, vs0, *vsi)

mlir_importer = SharkImporter(
    dlrm_model,
    input_dlrm,
    frontend="torch",
)

(dlrm_mlir, func_name), inputs, golden_out = mlir_importer.import_debug(
    tracing_required=True
)

shark_module = SharkInference(
    dlrm_mlir, device="vulkan", mlir_dialect="linalg"
)
shark_module.compile()
result = shark_module.forward(input_dlrm)
np.testing.assert_allclose(
    golden_output.detach().numpy(), result, rtol=1e-02, atol=1e-03
)


# Verified via torch-mlir.
# import torch_mlir
# from torch_mlir_e2e_test.linalg_on_tensors_backends import refbackend


# module = torch_mlir.compile(
# dlrm_model, inputs, use_tracing=True, output_type="linalg-on-tensors"
# )
# backend = refbackend.RefBackendLinalgOnTensorsBackend()
# compiled = backend.compile(module)
# jit_module = backend.load(compiled)

# dense_numpy = dense_inp.numpy()
# vs0_numpy = vs0.numpy()
# vsi_numpy = [inp.numpy() for inp in vsi]

# numpy_inp = (dense_numpy, vs0_numpy, *vsi_numpy)

# print(jit_module.forward(*numpy_inp))

```

`shark/examples/shark_inference/sparse_arch.py`:

```py
import torch
from torch import nn
from torchrec.datasets.utils import Batch
from torchrec.modules.crossnet import LowRankCrossNet
from torchrec.sparse.jagged_tensor import KeyedJaggedTensor, KeyedTensor
from torchrec.modules.embedding_configs import EmbeddingBagConfig
from torchrec.modules.embedding_modules import EmbeddingBagCollection
from torchrec.sparse.jagged_tensor import KeyedJaggedTensor
from typing import Dict, List, Optional, Tuple
from torchrec.models.dlrm import (
    choose,
    DenseArch,
    DLRM,
    InteractionArch,
    SparseArch,
    OverArch,
)
from shark.shark_inference import SharkInference
from shark.shark_importer import SharkImporter
import numpy as np

torch.manual_seed(0)

np.random.seed(0)


def calculate_offsets(tensor_list, prev_values, prev_offsets):
    offset_init = 0
    offset_list = []
    values_list = []

    if prev_offsets != None:
        offset_init = prev_values.shape[-1]
    for tensor in tensor_list:
        offset_list.append(offset_init)
        offset_init += tensor.shape[0]

    concatendated_tensor_list = torch.cat(tensor_list)

    if prev_values != None:
        concatendated_tensor_list = torch.cat(
            [prev_values, concatendated_tensor_list]
        )

    concatenated_offsets = torch.tensor(offset_list)

    if prev_offsets != None:
        concatenated_offsets = torch.cat([prev_offsets, concatenated_offsets])

    return concatendated_tensor_list, concatenated_offsets


# Have to make combined_keys as dict as to which embedding bags they
# point to. {f1: 0, f3: 0, f2: 1}
# The result will be a triple containing values, indices and pointer tensor.
def to_list(key_jagged, combined_keys):
    key_jagged_dict = key_jagged.to_dict()
    combined_list = []

    for key in combined_keys:
        prev_values, prev_offsets = calculate_offsets(
            key_jagged_dict[key].to_dense(), None, None
        )
        print(prev_values)
        print(prev_offsets)
        combined_list.append(prev_values)
        combined_list.append(prev_offsets)
        combined_list.append(torch.tensor(combined_keys[key]))

    return combined_list


class SparseArchShark(nn.Module):
    def create_emb(self, embedding_dim, num_embeddings_list):
        embedding_list = nn.ModuleList()
        for i in range(0, num_embeddings_list.size):
            num_embeddings = num_embeddings_list[i]
            EE = nn.EmbeddingBag(num_embeddings, embedding_dim, mode="sum")
            W = np.random.uniform(
                low=-np.sqrt(1 / num_embeddings),
                high=np.sqrt(1 / num_embeddings),
                size=(num_embeddings, embedding_dim),
            ).astype(np.float32)
            EE.weight.data = torch.tensor(W, requires_grad=True)
            embedding_list.append(EE)
        return embedding_list

    def __init__(
        self,
        embedding_dim,
        total_features,
        num_embeddings_list,
    ):
        super(SparseArchShark, self).__init__()
        self.embedding_dim = embedding_dim
        self.num_features = total_features
        self.embedding_list = self.create_emb(
            embedding_dim, num_embeddings_list
        )

    def forward(self, *batched_inputs):
        concatenated_list = []
        input_enum, embedding_enum = 0, 0

        for k in range(len(batched_inputs) // 3):
            values = batched_inputs[input_enum]
            input_enum += 1
            offsets = batched_inputs[input_enum]
            input_enum += 1
            embedding_pointer = int(batched_inputs[input_enum])
            input_enum += 1

            E = self.embedding_list[embedding_pointer]
            V = E(values, offsets)
            concatenated_list.append(V)

        return torch.cat(concatenated_list, dim=1).reshape(
            -1, self.num_features, self.embedding_dim
        )


def test_sparse_arch() -> None:
    D = 3
    eb1_config = EmbeddingBagConfig(
        name="t1",
        embedding_dim=D,
        num_embeddings=10,
        feature_names=["f1", "f3"],
    )
    eb2_config = EmbeddingBagConfig(
        name="t2",
        embedding_dim=D,
        num_embeddings=10,
        feature_names=["f2"],
    )

    ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])

    w1 = ebc.embedding_bags["t1"].weight
    w2 = ebc.embedding_bags["t2"].weight

    sparse_arch = SparseArch(ebc)

    keys = ["f1", "f2", "f3", "f4", "f5"]
    offsets = torch.tensor([0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 19])
    features = KeyedJaggedTensor.from_offsets_sync(
        keys=keys,
        values=torch.tensor(
            [1, 2, 4, 5, 4, 3, 2, 9, 1, 2, 4, 5, 4, 3, 2, 9, 1, 2, 3]
        ),
        offsets=offsets,
    )
    sparse_archi = SparseArchShark(D, 3, np.array([10, 10]))
    sparse_archi.embedding_list[0].weight = w1
    sparse_archi.embedding_list[1].weight = w2
    inputs = to_list(features, {"f1": 0, "f3": 0, "f2": 1})

    test_results = sparse_archi(*inputs)
    sparse_features = sparse_arch(features)

    torch.allclose(
        sparse_features,
        test_results,
        rtol=1e-4,
        atol=1e-4,
    )


test_sparse_arch()


class DLRMShark(nn.Module):
    def __init__(
        self,
        embedding_dim,
        total_features,
        num_embeddings_list,
        dense_in_features: int,
        dense_arch_layer_sizes: List[int],
        over_arch_layer_sizes: List[int],
    ) -> None:
        super().__init__()

        self.sparse_arch: SparseArchShark = SparseArchShark(
            embedding_dim, total_features, num_embeddings_list
        )
        num_sparse_features: int = total_features

        self.dense_arch = DenseArch(
            in_features=dense_in_features,
            layer_sizes=dense_arch_layer_sizes,
        )

        self.inter_arch = InteractionArch(
            num_sparse_features=num_sparse_features,
        )

        over_in_features: int = (
            embedding_dim
            + choose(num_sparse_features, 2)
            + num_sparse_features
        )

        self.over_arch = OverArch(
            in_features=over_in_features,
            layer_sizes=over_arch_layer_sizes,
        )

    def forward(
        self, dense_features: torch.Tensor, *sparse_features
    ) -> torch.Tensor:
        embedded_dense = self.dense_arch(dense_features)
        embedded_sparse = self.sparse_arch(*sparse_features)
        concatenated_dense = self.inter_arch(
            dense_features=embedded_dense, sparse_features=embedded_sparse
        )
        logits = self.over_arch(concatenated_dense)
        return logits


def test_dlrm() -> None:
    B = 2
    D = 8
    dense_in_features = 100

    eb1_config = EmbeddingBagConfig(
        name="t1",
        embedding_dim=D,
        num_embeddings=100,
        feature_names=["f1", "f3"],
    )
    eb2_config = EmbeddingBagConfig(
        name="t2",
        embedding_dim=D,
        num_embeddings=100,
        feature_names=["f2"],
    )

    ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])

    sparse_features = KeyedJaggedTensor.from_offsets_sync(
        keys=["f1", "f3", "f2"],
        values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9, 1, 2, 3]),
        offsets=torch.tensor([0, 2, 4, 6, 8, 10, 11]),
    )
    ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])
    sparse_nn = DLRM(
        embedding_bag_collection=ebc,
        dense_in_features=dense_in_features,
        dense_arch_layer_sizes=[20, D],
        over_arch_layer_sizes=[5, 1],
    )
    sparse_nn_nod = DLRMShark(
        embedding_dim=8,
        total_features=3,
        num_embeddings_list=np.array([100, 100]),
        dense_in_features=dense_in_features,
        dense_arch_layer_sizes=[20, D],
        over_arch_layer_sizes=[5, 1],
    )

    dense_features = torch.rand((B, dense_in_features))

    x = to_list(sparse_features, {"f1": 0, "f3": 0, "f2": 1})

    w1 = ebc.embedding_bags["t1"].weight
    w2 = ebc.embedding_bags["t2"].weight

    sparse_nn_nod.sparse_arch.embedding_list[0].weight = w1
    sparse_nn_nod.sparse_arch.embedding_list[1].weight = w2

    sparse_nn_nod.dense_arch.load_state_dict(sparse_nn.dense_arch.state_dict())
    sparse_nn_nod.inter_arch.load_state_dict(sparse_nn.inter_arch.state_dict())
    sparse_nn_nod.over_arch.load_state_dict(sparse_nn.over_arch.state_dict())

    logits = sparse_nn(
        dense_features=dense_features,
        sparse_features=sparse_features,
    )
    logits_nod = sparse_nn_nod(dense_features, *x)

    # print(logits)
    # print(logits_nod)

    # Import the module and print.
    mlir_importer = SharkImporter(
        sparse_nn_nod,
        (dense_features, *x),
        frontend="torch",
    )

    (dlrm_mlir, func_name), inputs, golden_out = mlir_importer.import_debug(
        tracing_required=True
    )

    shark_module = SharkInference(
        dlrm_mlir, device="cpu", mlir_dialect="linalg"
    )
    shark_module.compile()
    result = shark_module.forward(inputs)
    np.testing.assert_allclose(golden_out, result, rtol=1e-02, atol=1e-03)

    torch.allclose(
        logits,
        logits_nod,
        rtol=1e-4,
        atol=1e-4,
    )


test_dlrm()

```

`shark/examples/shark_inference/t5_tf.py`:

```py
from PIL import Image
import requests

from transformers import T5Tokenizer, TFT5Model
import tensorflow as tf
from shark.shark_inference import SharkInference

# Create a set of inputs
t5_inputs = [
    tf.TensorSpec(shape=[1, 10], dtype=tf.int32),
    tf.TensorSpec(shape=[1, 10], dtype=tf.int32),
]


class T5Module(tf.Module):
    def __init__(self):
        super(T5Module, self).__init__()
        self.m = TFT5Model.from_pretrained("t5-small")
        self.m.predict = lambda x, y: self.m(input_ids=x, decoder_input_ids=y)

    @tf.function(input_signature=t5_inputs, jit_compile=True)
    def forward(self, input_ids, decoder_input_ids):
        return self.m.predict(input_ids, decoder_input_ids)


if __name__ == "__main__":
    # Prepping Data
    tokenizer = T5Tokenizer.from_pretrained("t5-small")
    text = "I love the distilled version of models."
    inputs = tokenizer(text, return_tensors="tf").input_ids

    shark_module = SharkInference(T5Module(), (inputs, inputs))
    shark_module.set_frontend("tensorflow")
    shark_module.compile()
    print(shark_module.forward((inputs, inputs)))

```

`shark/examples/shark_inference/torch_vision_models_script.py`:

```py
import torch
import torchvision.models as models
from shark.shark_inference import SharkInference


class VisionModule(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.train(False)

    def forward(self, input):
        return self.model.forward(input)


input = torch.randn(1, 3, 224, 224)

## The vision models present here: https://pytorch.org/vision/stable/models.html
vision_models_list = [
    models.resnet18(pretrained=True),
    models.alexnet(pretrained=True),
    models.vgg16(pretrained=True),
    models.squeezenet1_0(pretrained=True),
    models.densenet161(pretrained=True),
    models.inception_v3(pretrained=True),
    models.shufflenet_v2_x1_0(pretrained=True),
    models.mobilenet_v2(pretrained=True),
    models.mobilenet_v3_small(pretrained=True),
    models.resnext50_32x4d(pretrained=True),
    models.wide_resnet50_2(pretrained=True),
    models.mnasnet1_0(pretrained=True),
    models.efficientnet_b0(pretrained=True),
    models.regnet_y_400mf(pretrained=True),
    models.regnet_x_400mf(pretrained=True),
]

for i, vision_model in enumerate(vision_models_list):
    shark_module = SharkInference(
        VisionModule(vision_model),
        (input,),
    )
    shark_module.compile()
    shark_module.forward((input,))

```

`shark/examples/shark_inference/unet_script.py`:

```py
import torch
import numpy as np
from shark.shark_inference import SharkInference
from shark.shark_importer import SharkImporter


class UnetModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = torch.hub.load(
            "mateuszbuda/brain-segmentation-pytorch",
            "unet",
            in_channels=3,
            out_channels=1,
            init_features=32,
            pretrained=True,
        )
        self.model.eval()

    def forward(self, input):
        return self.model(input)


input = torch.randn(1, 3, 224, 224)

mlir_importer = SharkImporter(
    UnetModule(),
    (input,),
    frontend="torch",
)

(vision_mlir, func_name), inputs, golden_out = mlir_importer.import_debug(
    tracing_required=False
)

shark_module = SharkInference(vision_mlir, mlir_dialect="linalg")
shark_module.compile()
result = shark_module.forward((input,))
np.testing.assert_allclose(golden_out, result, rtol=1e-02, atol=1e-03)

```

`shark/examples/shark_inference/upscaler/main.py`:

```py
import requests
from PIL import Image
from io import BytesIO
from pipeline_shark_stable_diffusion_upscale import (
    SharkStableDiffusionUpscalePipeline,
)
import torch

model_id = "stabilityai/stable-diffusion-x4-upscaler"
pipeline = SharkStableDiffusionUpscalePipeline(model_id)

# let's download an  image
url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png"
response = requests.get(url)
low_res_img = Image.open(BytesIO(response.content)).convert("RGB")
low_res_img = low_res_img.resize((128, 128))

prompt = "a white cat"

upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]
upscaled_image.save("upsampled_cat.png")

```

`shark/examples/shark_inference/upscaler/model_wrappers.py`:

```py
from diffusers import AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel
from utils import compile_through_fx
import torch

model_id = "stabilityai/stable-diffusion-x4-upscaler"

model_input = {
    "clip": (torch.randint(1, 2, (1, 77)),),
    "vae": (torch.randn(1, 4, 128, 128),),
    "unet": (
        torch.randn(2, 7, 128, 128),  # latents
        torch.tensor([1]).to(torch.float32),  # timestep
        torch.randn(2, 77, 1024),  # embedding
        torch.randn(2).to(torch.int64),  # noise_level
    ),
}


def get_clip_mlir(model_name="clip_text", extra_args=[]):
    text_encoder = CLIPTextModel.from_pretrained(
        model_id,
        subfolder="text_encoder",
    )

    class CLIPText(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.text_encoder = text_encoder

        def forward(self, input):
            return self.text_encoder(input)[0]

    clip_model = CLIPText()
    shark_clip = compile_through_fx(
        clip_model,
        model_input["clip"],
        model_name=model_name,
        extra_args=extra_args,
    )
    return shark_clip


def get_vae_mlir(model_name="vae", extra_args=[]):
    class VaeModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.vae = AutoencoderKL.from_pretrained(
                model_id,
                subfolder="vae",
            )

        def forward(self, input):
            x = self.vae.decode(input, return_dict=False)[0]
            return x

    vae = VaeModel()
    shark_vae = compile_through_fx(
        vae,
        model_input["vae"],
        model_name=model_name,
        extra_args=extra_args,
    )
    return shark_vae


def get_unet_mlir(model_name="unet", extra_args=[]):
    class UnetModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.unet = UNet2DConditionModel.from_pretrained(
                model_id,
                subfolder="unet",
            )
            self.in_channels = self.unet.in_channels
            self.train(False)

        def forward(self, latent, timestep, text_embedding, noise_level):
            unet_out = self.unet.forward(
                latent,
                timestep,
                text_embedding,
                noise_level,
                return_dict=False,
            )[0]
            return unet_out

    unet = UnetModel()
    f16_input_mask = (True, True, True, False)
    shark_unet = compile_through_fx(
        unet,
        model_input["unet"],
        model_name=model_name,
        is_f16=True,
        f16_input_mask=f16_input_mask,
        extra_args=extra_args,
    )
    return shark_unet

```

`shark/examples/shark_inference/upscaler/opt_params.py`:

```py
import sys
from model_wrappers import (
    get_vae_mlir,
    get_unet_mlir,
    get_clip_mlir,
)
from upscaler_args import args
from utils import get_shark_model

BATCH_SIZE = len(args.prompts)
if BATCH_SIZE != 1:
    sys.exit("Only batch size 1 is supported.")


unet_flag = [
    "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-preprocessing-convert-conv2d-to-img2col,iree-preprocessing-pad-linalg-ops{pad-size=32}))"
]

vae_flag = [
    "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-flow-convert-conv-nchw-to-nhwc,iree-preprocessing-pad-linalg-ops{pad-size=16}))"
]

clip_flag = [
    "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-preprocessing-pad-linalg-ops{pad-size=16}))"
]

bucket = "gs://shark_tank/stable_diffusion/"


def get_unet():
    model_name = "upscaler_unet"
    if args.import_mlir:
        return get_unet_mlir(model_name, unet_flag)
    return get_shark_model(bucket, model_name, unet_flag)


def get_vae():
    model_name = "upscaler_vae"
    if args.import_mlir:
        return get_vae_mlir(model_name, vae_flag)
    return get_shark_model(bucket, model_name, vae_flag)


def get_clip():
    model_name = "upscaler_clip"
    if args.import_mlir:
        return get_clip_mlir(model_name, clip_flag)
    return get_shark_model(bucket, model_name, clip_flag)

```

`shark/examples/shark_inference/upscaler/pipeline_shark_stable_diffusion_upscale.py`:

```py
import inspect
from typing import Callable, List, Optional, Union

import numpy as np
import torch

import PIL
from PIL import Image
from diffusers.utils import is_accelerate_available
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel
from diffusers import (
    DDIMScheduler,
    DDPMScheduler,
    LMSDiscreteScheduler,
    PNDMScheduler,
)
from diffusers import logging
from diffusers.pipeline_utils import ImagePipelineOutput
from opt_params import get_unet, get_vae, get_clip
from tqdm.auto import tqdm

logger = logging.get_logger(__name__)  # pylint: disable=invalid-name


def preprocess(image):
    if isinstance(image, torch.Tensor):
        return image
    elif isinstance(image, PIL.Image.Image):
        image = [image]

    if isinstance(image[0], PIL.Image.Image):
        w, h = image[0].size
        w, h = map(
            lambda x: x - x % 64, (w, h)
        )  # resize to integer multiple of 64

        image = [np.array(i.resize((w, h)))[None, :] for i in image]
        image = np.concatenate(image, axis=0)
        image = np.array(image).astype(np.float32) / 255.0
        image = image.transpose(0, 3, 1, 2)
        image = 2.0 * image - 1.0
        image = torch.from_numpy(image)
    elif isinstance(image[0], torch.Tensor):
        image = torch.cat(image, dim=0)
    return image


def shark_run_wrapper(model, *args):
    np_inputs = tuple([x.detach().numpy() for x in args])
    outputs = model("forward", np_inputs)
    return torch.from_numpy(outputs)


class SharkStableDiffusionUpscalePipeline:
    def __init__(
        self,
        model_id,
    ):
        self.tokenizer = CLIPTokenizer.from_pretrained(
            model_id, subfolder="tokenizer"
        )
        self.low_res_scheduler = DDPMScheduler.from_pretrained(
            model_id,
            subfolder="scheduler",
        )
        self.scheduler = DDIMScheduler.from_pretrained(
            model_id,
            subfolder="scheduler",
        )
        self.vae = get_vae()
        self.unet = get_unet()
        self.text_encoder = get_clip()
        self.max_noise_level = (350,)
        self._execution_device = "cpu"

    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._encode_prompt
    def _encode_prompt(
        self,
        prompt,
        device,
        num_images_per_prompt,
        do_classifier_free_guidance,
        negative_prompt,
    ):
        r"""
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `list(int)`):
                prompt to be encoded
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`):
                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored
                if `guidance_scale` is less than `1`).
        """
        batch_size = len(prompt) if isinstance(prompt, list) else 1

        text_inputs = self.tokenizer(
            prompt,
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        text_input_ids = text_inputs.input_ids
        untruncated_ids = self.tokenizer(
            prompt, padding="longest", return_tensors="pt"
        ).input_ids

        if untruncated_ids.shape[-1] >= text_input_ids.shape[
            -1
        ] and not torch.equal(text_input_ids, untruncated_ids):
            removed_text = self.tokenizer.batch_decode(
                untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]
            )
            logger.warning(
                "The following part of your input was truncated because CLIP can only handle sequences up to"
                f" {self.tokenizer.model_max_length} tokens: {removed_text}"
            )

        # if (
        # hasattr(self.text_encoder.config, "use_attention_mask")
        # and self.text_encoder.config.use_attention_mask
        # ):
        # attention_mask = text_inputs.attention_mask.to(device)
        # else:
        # attention_mask = None

        text_embeddings = shark_run_wrapper(
            self.text_encoder, text_input_ids.to(device)
        )

        # duplicate text embeddings for each generation per prompt, using mps friendly method
        bs_embed, seq_len, _ = text_embeddings.shape
        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)
        text_embeddings = text_embeddings.view(
            bs_embed * num_images_per_prompt, seq_len, -1
        )

        # get unconditional embeddings for classifier free guidance
        if do_classifier_free_guidance:
            uncond_tokens: List[str]
            if negative_prompt is None:
                uncond_tokens = [""] * batch_size
            elif type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !="
                    f" {type(prompt)}."
                )
            elif isinstance(negative_prompt, str):
                uncond_tokens = [negative_prompt]
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:"
                    f" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches"
                    " the batch size of `prompt`."
                )
            else:
                uncond_tokens = negative_prompt

            max_length = text_input_ids.shape[-1]
            uncond_input = self.tokenizer(
                uncond_tokens,
                padding="max_length",
                max_length=max_length,
                truncation=True,
                return_tensors="pt",
            )

            # if (
            # hasattr(self.text_encoder.config, "use_attention_mask")
            # and self.text_encoder.config.use_attention_mask
            # ):
            # attention_mask = uncond_input.attention_mask.to(device)
            # else:
            # attention_mask = None

            uncond_embeddings = shark_run_wrapper(
                self.text_encoder,
                uncond_input.input_ids.to(device),
            )
            uncond_embeddings = uncond_embeddings

            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
            seq_len = uncond_embeddings.shape[1]
            uncond_embeddings = uncond_embeddings.repeat(
                1, num_images_per_prompt, 1
            )
            uncond_embeddings = uncond_embeddings.view(
                batch_size * num_images_per_prompt, seq_len, -1
            )

            # For classifier free guidance, we need to do two forward passes.
            # Here we concatenate the unconditional and text embeddings into a single batch
            # to avoid doing two forward passes
            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

        return text_embeddings

    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]

        accepts_eta = "eta" in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs["eta"] = eta

        # check if the scheduler accepts generator
        accepts_generator = "generator" in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs["generator"] = generator
        return extra_step_kwargs

    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.decode_latents with 0.18215->0.08333
    def decode_latents(self, latents):
        latents = 1 / 0.08333 * latents
        image = shark_run_wrapper(self.vae, latents)
        image = (image / 2 + 0.5).clamp(0, 1)
        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16
        image = image.cpu().permute(0, 2, 3, 1).float().numpy()
        return image

    def check_inputs(self, prompt, image, noise_level, callback_steps):
        if not isinstance(prompt, str) and not isinstance(prompt, list):
            raise ValueError(
                f"`prompt` has to be of type `str` or `list` but is {type(prompt)}"
            )

        if (
            not isinstance(image, torch.Tensor)
            and not isinstance(image, PIL.Image.Image)
            and not isinstance(image, list)
        ):
            raise ValueError(
                f"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or `list` but is {type(image)}"
            )

        # verify batch size of prompt and image are same if image is a list or tensor
        if isinstance(image, list) or isinstance(image, torch.Tensor):
            if isinstance(prompt, str):
                batch_size = 1
            else:
                batch_size = len(prompt)
            if isinstance(image, list):
                image_batch_size = len(image)
            else:
                image_batch_size = image.shape[0]
            if batch_size != image_batch_size:
                raise ValueError(
                    f"`prompt` has batch size {batch_size} and `image` has batch size {image_batch_size}."
                    " Please make sure that passed `prompt` matches the batch size of `image`."
                )

    @staticmethod
    def numpy_to_pil(images):
        """
        Convert a numpy image or a batch of images to a PIL image.
        """
        if images.ndim == 3:
            images = images[None, ...]
        images = (images * 255).round().astype("uint8")
        if images.shape[-1] == 1:
            # special case for grayscale (single channel) images
            pil_images = [
                Image.fromarray(image.squeeze(), mode="L") for image in images
            ]
        else:
            pil_images = [Image.fromarray(image) for image in images]

        return pil_images

    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
    ):
        shape = (batch_size, num_channels_latents, height, width)
        if latents is None:
            if device == "mps":
                # randn does not work reproducibly on mps
                latents = torch.randn(
                    shape, generator=generator, device="cpu", dtype=dtype
                ).to(device)
            else:
                latents = torch.randn(
                    shape, generator=generator, device=device, dtype=dtype
                )
        else:
            if latents.shape != shape:
                raise ValueError(
                    f"Unexpected latents shape, got {latents.shape}, expected {shape}"
                )
            latents = latents.to(device)

        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    @torch.no_grad()
    def __call__(
        self,
        prompt: Union[str, List[str]],
        image: Union[
            torch.FloatTensor, PIL.Image.Image, List[PIL.Image.Image]
        ],
        num_inference_steps: int = 75,
        guidance_scale: float = 9.0,
        noise_level: int = 20,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[
            Union[torch.Generator, List[torch.Generator]]
        ] = None,
        latents: Optional[torch.FloatTensor] = None,
        output_type: Optional[str] = "pil",
        return_dict: bool = True,
        callback: Optional[
            Callable[[int, int, torch.FloatTensor], None]
        ] = None,
        callback_steps: Optional[int] = 1,
    ):
        # 1. Check inputs
        self.check_inputs(prompt, image, noise_level, callback_steps)

        # 2. Define call parameters
        batch_size = 1 if isinstance(prompt, str) else len(prompt)
        device = self._execution_device
        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
        # corresponds to doing no classifier free guidance.
        do_classifier_free_guidance = guidance_scale > 1.0

        # 3. Encode input prompt
        text_embeddings = self._encode_prompt(
            prompt,
            device,
            num_images_per_prompt,
            do_classifier_free_guidance,
            negative_prompt,
        )

        # 4. Preprocess image
        image = preprocess(image)
        image = image.to(dtype=text_embeddings.dtype, device=device)

        # 5. set timesteps
        self.scheduler.set_timesteps(num_inference_steps, device=device)
        timesteps = self.scheduler.timesteps

        # 5. Add noise to image
        noise_level = torch.tensor(
            [noise_level], dtype=torch.long, device=device
        )
        if device == "mps":
            # randn does not work reproducibly on mps
            noise = torch.randn(
                image.shape,
                generator=generator,
                device="cpu",
                dtype=text_embeddings.dtype,
            ).to(device)
        else:
            noise = torch.randn(
                image.shape,
                generator=generator,
                device=device,
                dtype=text_embeddings.dtype,
            )
        image = self.low_res_scheduler.add_noise(image, noise, noise_level)

        batch_multiplier = 2 if do_classifier_free_guidance else 1
        image = torch.cat([image] * batch_multiplier * num_images_per_prompt)
        noise_level = torch.cat([noise_level] * image.shape[0])

        # 6. Prepare latent variables
        height, width = image.shape[2:]
        # num_channels_latents = self.vae.config.latent_channels
        num_channels_latents = 4
        latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            text_embeddings.dtype,
            device,
            generator,
            latents,
        )

        # 7. Check that sizes of image and latents match
        num_channels_image = image.shape[1]
        # if (
        # num_channels_latents + num_channels_image
        # != self.unet.config.in_channels
        # ):
        # raise ValueError(
        # f"Incorrect configuration settings! The config of `pipeline.unet`: {self.unet.config} expects"
        # f" {self.unet.config.in_channels} but received `num_channels_latents`: {num_channels_latents} +"
        # f" `num_channels_image`: {num_channels_image} "
        # f" = {num_channels_latents+num_channels_image}. Please verify the config of"
        # " `pipeline.unet` or your `image` input."
        # )

        # 8. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

        # 9. Denoising loop
        num_warmup_steps = (
            len(timesteps) - num_inference_steps * self.scheduler.order
        )
        for i, t in tqdm(enumerate(timesteps)):
            # expand the latents if we are doing classifier free guidance
            latent_model_input = (
                torch.cat([latents] * 2)
                if do_classifier_free_guidance
                else latents
            )

            # concat latents, mask, masked_image_latents in the channel dimension
            latent_model_input = self.scheduler.scale_model_input(
                latent_model_input, t
            )
            latent_model_input = torch.cat([latent_model_input, image], dim=1)

            timestep = torch.tensor([t]).to(torch.float32)

            # predict the noise residual
            noise_pred = shark_run_wrapper(
                self.unet,
                latent_model_input.half(),
                timestep,
                text_embeddings.half(),
                noise_level,
            )

            # perform guidance
            if do_classifier_free_guidance:
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (
                noise_pred_text - noise_pred_uncond
            )

            # compute the previous noisy sample x_t -> x_t-1
            latents = self.scheduler.step(
                noise_pred, t, latents, **extra_step_kwargs
            ).prev_sample

            # # call the callback, if provided
            # if i == len(timesteps) - 1 or (
            # (i + 1) > num_warmup_steps
            # and (i + 1) % self.scheduler.order == 0
            # ):
            # progress_bar.update()
            # if callback is not None and i % callback_steps == 0:
            # callback(i, t, latents)

        # 10. Post-processing
        # make sure the VAE is in float32 mode, as it overflows in float16
        # self.vae.to(dtype=torch.float32)
        image = self.decode_latents(latents.float())

        # 11. Convert to PIL
        if output_type == "pil":
            image = self.numpy_to_pil(image)

        if not return_dict:
            return (image,)

        return ImagePipelineOutput(images=image)

```

`shark/examples/shark_inference/upscaler/upscaler_args.py`:

```py
import argparse

p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)

##############################################################################
### Stable Diffusion Params
##############################################################################

p.add_argument(
    "--prompts",
    nargs="+",
    default=["cyberpunk forest by Salvador Dali"],
    help="text of which images to be generated.",
)

p.add_argument(
    "--negative-prompts",
    nargs="+",
    default=[""],
    help="text you don't want to see in the generated image.",
)

p.add_argument(
    "--steps",
    type=int,
    default=50,
    help="the no. of steps to do the sampling.",
)

p.add_argument(
    "--seed",
    type=int,
    default=42,
    help="the seed to use.",
)

p.add_argument(
    "--guidance_scale",
    type=float,
    default=7.5,
    help="the value to be used for guidance scaling.",
)

##############################################################################
### Model Config and Usage Params
##############################################################################

p.add_argument(
    "--device", type=str, default="vulkan", help="device to run the model."
)

p.add_argument(
    "--precision", type=str, default="fp16", help="precision to run the model."
)

p.add_argument(
    "--import_mlir",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="imports the model from torch module to shark_module otherwise downloads the model from shark_tank.",
)

p.add_argument(
    "--load_vmfb",
    default=True,
    action=argparse.BooleanOptionalAction,
    help="attempts to load the model from a precompiled flatbuffer and compiles + saves it if not found.",
)

p.add_argument(
    "--save_vmfb",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="saves the compiled flatbuffer to the local directory",
)

##############################################################################
### IREE - Vulkan supported flags
##############################################################################

p.add_argument(
    "--iree-vulkan-target-triple",
    type=str,
    default="",
    help="Specify target triple for vulkan",
)

p.add_argument(
    "--vulkan_debug_utils",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Profiles vulkan device and collects the .rdc info",
)


args = p.parse_args()

```

`shark/examples/shark_inference/upscaler/utils.py`:

```py
import os
import torch
from shark.shark_inference import SharkInference
from upscaler_args import args
from shark.shark_importer import import_with_fx
from shark.iree_utils.vulkan_utils import (
    set_iree_vulkan_runtime_flags,
    get_vulkan_target_triple,
    get_iree_vulkan_runtime_flags,
)


def _compile_module(shark_module, model_name, extra_args=[]):
    if args.load_vmfb or args.save_vmfb:
        device = (
            args.device
            if "://" not in args.device
            else "-".join(args.device.split("://"))
        )
        extended_name = "{}_{}".format(model_name, device)
        vmfb_path = os.path.join(os.getcwd(), extended_name + ".vmfb")
        if args.load_vmfb and os.path.isfile(vmfb_path) and not args.save_vmfb:
            print(f"loading existing vmfb from: {vmfb_path}")
            shark_module.load_module(vmfb_path, extra_args=extra_args)
        else:
            if args.save_vmfb:
                print("Saving to {}".format(vmfb_path))
            else:
                print(
                    "No vmfb found. Compiling and saving to {}".format(
                        vmfb_path
                    )
                )
            path = shark_module.save_module(
                os.getcwd(), extended_name, extra_args
            )
            shark_module.load_module(path, extra_args=extra_args)
    else:
        shark_module.compile(extra_args)
    return shark_module


# Downloads the model from shark_tank and returns the shark_module.
def get_shark_model(tank_url, model_name, extra_args=[]):
    from shark.shark_downloader import download_model
    from shark.parser import shark_args

    # Set local shark_tank cache directory.
    # shark_args.local_tank_cache = args.local_tank_cache

    mlir_model, func_name, inputs, golden_out = download_model(
        model_name,
        tank_url=tank_url,
        frontend="torch",
    )
    shark_module = SharkInference(
        mlir_model, device=args.device, mlir_dialect="linalg"
    )
    return _compile_module(shark_module, model_name, extra_args)


# Converts the torch-module into a shark_module.
def compile_through_fx(
    model, inputs, model_name, is_f16=False, f16_input_mask=None, extra_args=[]
):
    mlir_module, func_name = import_with_fx(
        model, inputs, is_f16, f16_input_mask
    )
    shark_module = SharkInference(
        mlir_module,
        device=args.device,
        mlir_dialect="linalg",
    )

    return _compile_module(shark_module, model_name, extra_args)


def set_iree_runtime_flags():
    vulkan_runtime_flags = get_iree_vulkan_runtime_flags()
    if args.enable_rgp:
        vulkan_runtime_flags += [
            f"--enable_rgp=true",
            f"--vulkan_debug_utils=true",
        ]
    set_iree_vulkan_runtime_flags(flags=vulkan_runtime_flags)


def get_all_devices(driver_name):
    """
    Inputs: driver_name
    Returns a list of all the available devices for a given driver sorted by
    the iree path names of the device as in --list_devices option in iree.
    """
    from iree.runtime import get_driver

    driver = get_driver(driver_name)
    device_list_src = driver.query_available_devices()
    device_list_src.sort(key=lambda d: d["path"])
    return device_list_src


def get_device_mapping(driver, key_combination=3):
    """This method ensures consistent device ordering when choosing
    specific devices for execution
    Args:
        driver (str): execution driver (vulkan, cuda, rocm, etc)
        key_combination (int, optional): choice for mapping value for device name.
        1 : path
        2 : name
        3 : (name, path)
        Defaults to 3.
    Returns:
        dict: map to possible device names user can input mapped to desired combination of name/path.
    """
    from shark.iree_utils._common import iree_device_map

    driver = iree_device_map(driver)
    device_list = get_all_devices(driver)
    device_map = dict()

    def get_output_value(dev_dict):
        if key_combination == 1:
            return f"{driver}://{dev_dict['path']}"
        if key_combination == 2:
            return dev_dict["name"]
        if key_combination == 3:
            return (dev_dict["name"], f"{driver}://{dev_dict['path']}")

    # mapping driver name to default device (driver://0)
    device_map[f"{driver}"] = get_output_value(device_list[0])
    for i, device in enumerate(device_list):
        # mapping with index
        device_map[f"{driver}://{i}"] = get_output_value(device)
        # mapping with full path
        device_map[f"{driver}://{device['path']}"] = get_output_value(device)
    return device_map


def map_device_to_name_path(device, key_combination=3):
    """Gives the appropriate device data (supported name/path) for user selected execution device
    Args:
        device (str): user
        key_combination (int, optional): choice for mapping value for device name.
        1 : path
        2 : name
        3 : (name, path)
        Defaults to 3.
    Raises:
        ValueError:
    Returns:
        str / tuple: returns the mapping str or tuple of mapping str for the device depending on key_combination value
    """
    driver = device.split("://")[0]
    device_map = get_device_mapping(driver, key_combination)
    try:
        device_mapping = device_map[device]
    except KeyError:
        raise ValueError(f"Device '{device}' is not a valid device.")
    return device_mapping


def set_init_device_flags():
    if "vulkan" in args.device:
        # set runtime flags for vulkan.
        set_iree_runtime_flags()

        # set triple flag to avoid multiple calls to get_vulkan_triple_flag
        device_name, args.device = map_device_to_name_path(args.device)
        if not args.iree_vulkan_target_triple:
            triple = get_vulkan_target_triple(device_name)
            if triple is not None:
                args.iree_vulkan_target_triple = triple
        print(
            f"Found device {device_name}. Using target triple {args.iree_vulkan_target_triple}."
        )
    elif "cuda" in args.device:
        args.device = "cuda"
    elif "cpu" in args.device:
        args.device = "cpu"

    # set max_length based on availability.
    if args.variant in ["anythingv3", "analogdiffusion", "dreamlike"]:
        args.max_length = 77
    elif args.variant == "openjourney":
        args.max_length = 64

    # use tuned models only in the case of stablediffusion/fp16 and rdna3 cards.
    if (
        args.variant in ["openjourney", "dreamlike"]
        or args.precision != "fp16"
        or "vulkan" not in args.device
        or "rdna3" not in args.iree_vulkan_target_triple
    ):
        args.use_tuned = False
        print("Tuned models are currently not supported for this setting.")

    elif args.use_base_vae and args.variant != "stablediffusion":
        args.use_tuned = False
        print("Tuned models are currently not supported for this setting.")

    if args.use_tuned:
        print("Using tuned models for stablediffusion/fp16 and rdna3 card.")


# Utility to get list of devices available.
def get_available_devices():
    def get_devices_by_name(driver_name):
        from shark.iree_utils._common import iree_device_map

        device_list = []
        try:
            driver_name = iree_device_map(driver_name)
            device_list_dict = get_all_devices(driver_name)
            print(f"{driver_name} devices are available.")
        except:
            print(f"{driver_name} devices are not available.")
        else:
            for i, device in enumerate(device_list_dict):
                device_list.append(f"{driver_name}://{i} => {device['name']}")
        return device_list

    set_iree_runtime_flags()

    available_devices = []
    vulkan_devices = get_devices_by_name("vulkan")
    available_devices.extend(vulkan_devices)
    cuda_devices = get_devices_by_name("cuda")
    available_devices.extend(cuda_devices)
    available_devices.append("cpu")
    return available_devices

```

`shark/examples/shark_inference/v_diffusion.py`:

```py
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model


mlir_model, func_name, inputs, golden_out = download_model(
    "v_diffusion", frontend="torch"
)

shark_module = SharkInference(
    mlir_model, device="vulkan", mlir_dialect="linalg"
)
shark_module.compile()
result = shark_module.forward(inputs)
print("The obtained result via shark is: ", result)
print("The golden result is:", golden_out)

```

`shark/examples/shark_training/bert_training.py`:

```py
import torch
from torch.nn.utils import stateless
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from shark.shark_trainer import SharkTrainer


class MiniLMSequenceClassification(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "microsoft/MiniLM-L12-H384-uncased",  # The pretrained model.
            num_labels=2,  # The number of output labels--2 for binary classification.
            output_attentions=False,  # Whether the model returns attentions weights.
            output_hidden_states=False,  # Whether the model returns all hidden-states.
            torchscript=True,
        )

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


mod = MiniLMSequenceClassification()


def get_sorted_params(named_params):
    return [i[1] for i in sorted(named_params.items())]


print(dict(mod.named_buffers()))

inp = (torch.randint(2, (1, 128)),)


def forward(params, buffers, args):
    params_and_buffers = {**params, **buffers}
    stateless.functional_call(
        mod, params_and_buffers, args, {}
    ).sum().backward()
    optim = torch.optim.SGD(get_sorted_params(params), lr=0.01)
    # optim.load_state_dict(optim_state)
    optim.step()
    return params, buffers


shark_module = SharkTrainer(mod, inp)
shark_module.compile(forward)
shark_module.train(num_iters=2)
print("training done")

```

`shark/examples/shark_training/bert_training_load_tf.py`:

```py
import numpy as np
import os
import time
import tensorflow as tf

from shark.shark_trainer import SharkTrainer
from shark.parser import parser
from urllib import request

parser.add_argument(
    "--download_mlir_path",
    type=str,
    default="bert_tf_training.mlir",
    help="Specifies path to target mlir file that will be loaded.",
)
load_args, unknown = parser.parse_known_args()

tf.random.set_seed(0)
vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1

# Download BERT model from tank and train.
if __name__ == "__main__":
    predict_sample_input = [
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
    ]
    file_link = "https://storage.googleapis.com/shark_tank/users/stanley/bert_tf_training.mlir"
    response = request.urlretrieve(file_link, load_args.download_mlir_path)
    sample_input_tensors = [
        tf.convert_to_tensor(val, dtype=tf.int32)
        for val in predict_sample_input
    ]
    num_iter = 10
    if not os.path.isfile(load_args.download_mlir_path):
        raise ValueError(
            f"Tried looking for target mlir in {load_args.download_mlir_path}, but cannot be found."
        )
    with open(load_args.download_mlir_path, "rb") as input_file:
        bert_mlir = input_file.read()
    shark_module = SharkTrainer(
        bert_mlir,
        (
            sample_input_tensors,
            tf.convert_to_tensor(
                np.random.randint(5, size=(BATCH_SIZE)), dtype=tf.int32
            ),
        ),
    )
    shark_module.set_frontend("mhlo")
    shark_module.compile()
    start = time.time()
    print(shark_module.train(num_iter))
    end = time.time()
    total_time = end - start
    print("time: " + str(total_time))
    print("time/iter: " + str(total_time / num_iter))

```

`shark/examples/shark_training/bert_training_tf.py`:

```py
from absl import app
import time

import numpy as np
import tensorflow as tf

from official.nlp.modeling import layers
from official.nlp.modeling import networks
from official.nlp.modeling.models import bert_classifier

from shark.shark_trainer import SharkTrainer


tf.random.set_seed(0)
vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False
        test_network = networks.BertEncoder(
            vocab_size=vocab_size, num_layers=2, dict_outputs=dict_outputs
        )

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            test_network, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.predict = tf.function(input_signature=[bert_input])(
            self.m.predict
        )
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            bert_input,  # inputs
            tf.TensorSpec(shape=[BATCH_SIZE], dtype=tf.int32),  # labels
        ],
        jit_compile=True,
    )
    def forward(self, inputs, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            probs = self.m(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss


if __name__ == "__main__":
    predict_sample_input = [
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
    ]
    sample_input_tensors = [
        tf.convert_to_tensor(val, dtype=tf.int32)
        for val in predict_sample_input
    ]
    num_iter = 10
    shark_module = SharkTrainer(
        BertModule(),
        (
            sample_input_tensors,
            tf.convert_to_tensor(
                np.random.randint(5, size=(BATCH_SIZE)), dtype=tf.int32
            ),
        ),
    )
    shark_module.set_frontend("tensorflow")
    shark_module.compile()
    start = time.time()
    print(shark_module.train(num_iter))
    end = time.time()
    total_time = end - start
    print("time: " + str(total_time))
    print("time/iter: " + str(total_time / num_iter))

```

`shark/examples/shark_training/neural_net_training.py`:

```py
import torch
from torch.nn.utils import _stateless
from shark.shark_trainer import SharkTrainer


class Foo(torch.nn.Module):
    def __init__(self):
        super(Foo, self).__init__()
        self.l1 = torch.nn.Linear(10, 16)
        self.relu = torch.nn.ReLU()
        self.l2 = torch.nn.Linear(16, 2)

    def forward(self, x):
        out = self.l1(x)
        out = self.relu(out)
        out = self.l2(out)
        return out


mod = Foo()
inp = (torch.randn(10, 10),)


def get_sorted_params(named_params):
    return [i[1] for i in sorted(named_params.items())]


def forward(params, buffers, args):
    params_and_buffers = {**params, **buffers}
    _stateless.functional_call(
        mod, params_and_buffers, args, {}
    ).sum().backward()
    optim = torch.optim.SGD(get_sorted_params(params), lr=0.01)
    optim.step()
    return params, buffers


# fx_graph = forward(dict(mod.named_parameters()), dict(mod.named_buffers()), inp)

shark_module = SharkTrainer(mod, inp)
# Pass the training function in case of torch
shark_module.compile(training_fn=forward)

shark_module.train(num_iters=10)

```

`shark/examples/shark_training/stable-diffusion-img2img/README.md`:

```md
# Stable Diffusion Img2Img model

## Installation

<details>
  <summary>Installation (Linux)</summary>

### Activate shark.venv Virtual Environment

```shell
source shark.venv/bin/activate

# Some older pip installs may not be able to handle the recent PyTorch deps
python -m pip install --upgrade pip
```

### Install dependencies

# Run the setup.sh script

```shell
./setup.sh
```

### Run the Stable diffusion Img2Img model

To run the model with the default set of images and params, run:
```shell
python stable_diffusion_img2img.py
```
To run the model with your set of images, and parameters you need to specify the following params:
1.) Input images directory with the arg `--input_dir` containing 3-5 images.
2.) What to teach the model? Using the arg `--what_to_teach`, allowed values are `object` or `style`.
3.) Placeholder token using the arg `--placeholder_token`, that represents your new concept. It should be passed with the opening and closing angle brackets. For ex: token is `cat-toy`, it should be passed as `<cat-toy>`.
4.) Initializer token using the arg `--initializer_token`, which summarise what is your new concept.

For the result, you need to pass the text prompt with the arg: `--prompt`. The prompt string should contain a "*s" in it, which will be replaced by the placeholder token during the inference.

By default the result images will go into the `sd_result` dir. To specify your output dir use the arg: `--output_dir`.

The default value of max_training_steps is `3000`, which takes some hours to complete. You can pass the smaller value with the arg `--training_steps`. Specify the number of images to be sampled for the result with the `--num_inference_samples` arg.

```

`shark/examples/shark_training/stable-diffusion-img2img/setup.sh`:

```sh
#!/bin/bash

TD="$(cd $(dirname $0) && pwd)"
if [ -z "$PYTHON" ]; then
  PYTHON="$(which python3)"
fi

function die() {
  echo "Error executing command: $*"
  exit 1
}

PYTHON_VERSION_X_Y=`${PYTHON} -c 'import sys; version=sys.version_info[:2]; print("{0}.{1}".format(*version))'`

echo "Python: $PYTHON"
echo "Python version: $PYTHON_VERSION_X_Y"

mkdir input_images

wget https://huggingface.co/datasets/valhalla/images/resolve/main/2.jpeg -P input_images/
wget https://huggingface.co/datasets/valhalla/images/resolve/main/3.jpeg -P input_images/
wget https://huggingface.co/datasets/valhalla/images/resolve/main/5.jpeg -P input_images/
wget https://huggingface.co/datasets/valhalla/images/resolve/main/6.jpeg -P input_images/

pip install diffusers["training"]==0.4.1 transformers ftfy opencv-python

```

`shark/examples/shark_training/stable-diffusion-img2img/stable_diffusion_img2img.py`:

```py
# Textual-inversion fine-tuning for Stable Diffusion using diffusers
# This script shows how to "teach" Stable Diffusion a new concept via
# textual-inversion using 🤗 Hugging Face [🧨 Diffusers library](https://github.com/huggingface/diffusers).
# By using just 3-5 images you can teach new concepts to Stable Diffusion
# and personalize the model on your own images.

import argparse
import itertools
import math
import os
import random
import cv2

import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import Dataset

import PIL
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    PNDMScheduler,
    StableDiffusionPipeline,
    UNet2DConditionModel,
)
from diffusers.hub_utils import init_git_repo, push_to_hub
from diffusers.optimization import get_scheduler
from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm
from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer

YOUR_TOKEN = "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"

p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
p.add_argument(
    "--input_dir",
    type=str,
    default="input_images/",
    help="the directory contains the images used for fine tuning",
)
p.add_argument(
    "--output_dir",
    type=str,
    default="sd_result",
    help="the directory contains the images used for fine tuning",
)
p.add_argument(
    "--training_steps",
    type=int,
    default=3000,
    help="the maximum number of training steps",
)
p.add_argument("--seed", type=int, default=42, help="the random seed")
p.add_argument(
    "--what_to_teach",
    type=str,
    choices=["object", "style"],
    default="object",
    help="what is it that you are teaching?",
)
p.add_argument(
    "--placeholder_token",
    type=str,
    default="<cat-toy>",
    help="It is the token you are going to use to represent your new concept",
)
p.add_argument(
    "--initializer_token",
    type=str,
    default="toy",
    help="It is a word that can summarise what is your new concept",
)
p.add_argument(
    "--inference_steps",
    type=int,
    default=50,
    help="the number of steps for inference",
)
p.add_argument(
    "--num_inference_samples",
    type=int,
    default=4,
    help="the number of samples for inference",
)
p.add_argument(
    "--prompt",
    type=str,
    default="a grafitti in a wall with a *s on it",
    help="the text prompt to use",
)
args = p.parse_args()

if "*s" not in args.prompt:
    raise ValueError(
        f'The prompt should have a "*s" which will be replaced by a placeholder token.'
    )

prompt1, prompt2 = args.prompt.split("*s")
args.prompt = prompt1 + args.placeholder_token + prompt2

pretrained_model_name_or_path = "CompVis/stable-diffusion-v1-4"

# Load input images.
images = []
for filename in os.listdir(args.input_dir):
    img = cv2.imread(os.path.join(args.input_dir, filename))
    if img is not None:
        images.append(img)

# Setup the prompt templates for training
imagenet_templates_small = [
    "a photo of a {}",
    "a rendering of a {}",
    "a cropped photo of the {}",
    "the photo of a {}",
    "a photo of a clean {}",
    "a photo of a dirty {}",
    "a dark photo of the {}",
    "a photo of my {}",
    "a photo of the cool {}",
    "a close-up photo of a {}",
    "a bright photo of the {}",
    "a cropped photo of a {}",
    "a photo of the {}",
    "a good photo of the {}",
    "a photo of one {}",
    "a close-up photo of the {}",
    "a rendition of the {}",
    "a photo of the clean {}",
    "a rendition of a {}",
    "a photo of a nice {}",
    "a good photo of a {}",
    "a photo of the nice {}",
    "a photo of the small {}",
    "a photo of the weird {}",
    "a photo of the large {}",
    "a photo of a cool {}",
    "a photo of a small {}",
]

imagenet_style_templates_small = [
    "a painting in the style of {}",
    "a rendering in the style of {}",
    "a cropped painting in the style of {}",
    "the painting in the style of {}",
    "a clean painting in the style of {}",
    "a dirty painting in the style of {}",
    "a dark painting in the style of {}",
    "a picture in the style of {}",
    "a cool painting in the style of {}",
    "a close-up painting in the style of {}",
    "a bright painting in the style of {}",
    "a cropped painting in the style of {}",
    "a good painting in the style of {}",
    "a close-up painting in the style of {}",
    "a rendition in the style of {}",
    "a nice painting in the style of {}",
    "a small painting in the style of {}",
    "a weird painting in the style of {}",
    "a large painting in the style of {}",
]


# Setup the dataset
class TextualInversionDataset(Dataset):
    def __init__(
        self,
        data_root,
        tokenizer,
        learnable_property="object",  # [object, style]
        size=512,
        repeats=100,
        interpolation="bicubic",
        flip_p=0.5,
        set="train",
        placeholder_token="*",
        center_crop=False,
    ):
        self.data_root = data_root
        self.tokenizer = tokenizer
        self.learnable_property = learnable_property
        self.size = size
        self.placeholder_token = placeholder_token
        self.center_crop = center_crop
        self.flip_p = flip_p

        self.image_paths = [
            os.path.join(self.data_root, file_path)
            for file_path in os.listdir(self.data_root)
        ]

        self.num_images = len(self.image_paths)
        self._length = self.num_images

        if set == "train":
            self._length = self.num_images * repeats

        self.interpolation = {
            "linear": PIL.Image.LINEAR,
            "bilinear": PIL.Image.BILINEAR,
            "bicubic": PIL.Image.BICUBIC,
            "lanczos": PIL.Image.LANCZOS,
        }[interpolation]

        self.templates = (
            imagenet_style_templates_small
            if learnable_property == "style"
            else imagenet_templates_small
        )
        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)

    def __len__(self):
        return self._length

    def __getitem__(self, i):
        example = {}
        image = Image.open(self.image_paths[i % self.num_images])

        if not image.mode == "RGB":
            image = image.convert("RGB")

        placeholder_string = self.placeholder_token
        text = random.choice(self.templates).format(placeholder_string)

        example["input_ids"] = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt",
        ).input_ids[0]

        # default to score-sde preprocessing
        img = np.array(image).astype(np.uint8)

        if self.center_crop:
            crop = min(img.shape[0], img.shape[1])
            (
                h,
                w,
            ) = (
                img.shape[0],
                img.shape[1],
            )
            img = img[
                (h - crop) // 2 : (h + crop) // 2,
                (w - crop) // 2 : (w + crop) // 2,
            ]

        image = Image.fromarray(img)
        image = image.resize(
            (self.size, self.size), resample=self.interpolation
        )

        image = self.flip_transform(image)
        image = np.array(image).astype(np.uint8)
        image = (image / 127.5 - 1.0).astype(np.float32)

        example["pixel_values"] = torch.from_numpy(image).permute(2, 0, 1)
        return example


# Setting up the model
# Load the tokenizer and add the placeholder token as a additional special token.
# Please read and if you agree accept the LICENSE
# [here](https://huggingface.co/CompVis/stable-diffusion-v1-4) if you see an error
tokenizer = CLIPTokenizer.from_pretrained(
    pretrained_model_name_or_path,
    subfolder="tokenizer",
    use_auth_token=YOUR_TOKEN,
)

# Add the placeholder token in tokenizer
num_added_tokens = tokenizer.add_tokens(args.placeholder_token)
if num_added_tokens == 0:
    raise ValueError(
        f"The tokenizer already contains the token {args.placeholder_token}. Please pass a different"
        " `placeholder_token` that is not already in the tokenizer."
    )

# Get token ids for our placeholder and initializer token.
# This code block will complain if initializer string is not a single token
# Convert the initializer_token, placeholder_token to ids
token_ids = tokenizer.encode(args.initializer_token, add_special_tokens=False)
# Check if initializer_token is a single token or a sequence of tokens
if len(token_ids) > 1:
    raise ValueError("The initializer token must be a single token.")

initializer_token_id = token_ids[0]
placeholder_token_id = tokenizer.convert_tokens_to_ids(args.placeholder_token)

# Load the Stable Diffusion model
# Load models and create wrapper for stable diffusion
text_encoder = CLIPTextModel.from_pretrained(
    pretrained_model_name_or_path,
    subfolder="text_encoder",
    use_auth_token=YOUR_TOKEN,
)
vae = AutoencoderKL.from_pretrained(
    pretrained_model_name_or_path,
    subfolder="vae",
    use_auth_token=YOUR_TOKEN,
)
unet = UNet2DConditionModel.from_pretrained(
    pretrained_model_name_or_path,
    subfolder="unet",
    use_auth_token=YOUR_TOKEN,
)

# We have added the `placeholder_token` in the `tokenizer` so we resize the token embeddings here,
#  this will a new embedding vector in the token embeddings for our `placeholder_token`
text_encoder.resize_token_embeddings(len(tokenizer))

# Initialise the newly added placeholder token with the embeddings of the initializer token
token_embeds = text_encoder.get_input_embeddings().weight.data
token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]

# In Textual-Inversion we only train the newly added embedding vector,
# so lets freeze rest of the model parameters here.


def freeze_params(params):
    for param in params:
        param.requires_grad = False


# Freeze vae and unet
freeze_params(vae.parameters())
freeze_params(unet.parameters())
# Freeze all parameters except for the token embeddings in text encoder
params_to_freeze = itertools.chain(
    text_encoder.text_model.encoder.parameters(),
    text_encoder.text_model.final_layer_norm.parameters(),
    text_encoder.text_model.embeddings.position_embedding.parameters(),
)
freeze_params(params_to_freeze)

# Creating our training data

train_dataset = TextualInversionDataset(
    data_root=args.input_dir,
    tokenizer=tokenizer,
    size=512,
    placeholder_token=args.placeholder_token,
    repeats=100,
    learnable_property=args.what_to_teach,  # Option selected above between object and style
    center_crop=False,
    set="train",
)


def create_dataloader(train_batch_size=1):
    return torch.utils.data.DataLoader(
        train_dataset, batch_size=train_batch_size, shuffle=True
    )


# Create noise_scheduler for training.
noise_scheduler = DDPMScheduler(
    beta_start=0.00085,
    beta_end=0.012,
    beta_schedule="scaled_linear",
    num_train_timesteps=1000,
    tensor_format="pt",
)

# Define hyperparameters for our training
hyperparameters = {
    "learning_rate": 5e-04,
    "scale_lr": True,
    "max_train_steps": args.training_steps,
    "train_batch_size": 1,
    "gradient_accumulation_steps": 4,
    "seed": args.seed,
    "output_dir": "sd-concept-output",
}


def training_function(text_encoder, vae, unet):
    logger = get_logger(__name__)

    train_batch_size = hyperparameters["train_batch_size"]
    gradient_accumulation_steps = hyperparameters[
        "gradient_accumulation_steps"
    ]
    learning_rate = hyperparameters["learning_rate"]
    max_train_steps = hyperparameters["max_train_steps"]
    output_dir = hyperparameters["output_dir"]

    accelerator = Accelerator(
        gradient_accumulation_steps=gradient_accumulation_steps,
    )

    train_dataloader = create_dataloader(train_batch_size)

    if hyperparameters["scale_lr"]:
        learning_rate = (
            learning_rate
            * gradient_accumulation_steps
            * train_batch_size
            * accelerator.num_processes
        )

    # Initialize the optimizer
    optimizer = torch.optim.AdamW(
        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings
        lr=learning_rate,
    )

    text_encoder, optimizer, train_dataloader = accelerator.prepare(
        text_encoder, optimizer, train_dataloader
    )

    # Move vae and unet to device
    vae.to(accelerator.device)
    unet.to(accelerator.device)

    # Keep vae and unet in eval model as we don't train these
    vae.eval()
    unet.eval()

    # We need to recalculate our total training steps as the size of the training dataloader may have changed.
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / gradient_accumulation_steps
    )
    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)

    # Train!
    total_batch_size = (
        train_batch_size
        * accelerator.num_processes
        * gradient_accumulation_steps
    )

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
    logger.info(
        f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}"
    )
    logger.info(
        f"  Gradient Accumulation steps = {gradient_accumulation_steps}"
    )
    logger.info(f"  Total optimization steps = {max_train_steps}")
    # Only show the progress bar once on each machine.
    progress_bar = tqdm(
        range(max_train_steps), disable=not accelerator.is_local_main_process
    )
    progress_bar.set_description("Steps")
    global_step = 0

    for epoch in range(num_train_epochs):
        text_encoder.train()
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(text_encoder):
                # Convert images to latent space
                latents = (
                    vae.encode(batch["pixel_values"])
                    .latent_dist.sample()
                    .detach()
                )
                latents = latents * 0.18215

                # Sample noise that we'll add to the latents
                noise = torch.randn(latents.shape).to(latents.device)
                bsz = latents.shape[0]
                # Sample a random timestep for each image
                timesteps = torch.randint(
                    0,
                    noise_scheduler.num_train_timesteps,
                    (bsz,),
                    device=latents.device,
                ).long()

                # Add noise to the latents according to the noise magnitude at each timestep
                # (this is the forward diffusion process)
                noisy_latents = noise_scheduler.add_noise(
                    latents, noise, timesteps
                )

                # Get the text embedding for conditioning
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                # Predict the noise residual
                noise_pred = unet(
                    noisy_latents, timesteps, encoder_hidden_states
                ).sample

                loss = (
                    F.mse_loss(noise_pred, noise, reduction="none")
                    .mean([1, 2, 3])
                    .mean()
                )
                accelerator.backward(loss)

                # Zero out the gradients for all token embeddings except the newly added
                # embeddings for the concept, as we only want to optimize the concept embeddings
                if accelerator.num_processes > 1:
                    grads = (
                        text_encoder.module.get_input_embeddings().weight.grad
                    )
                else:
                    grads = text_encoder.get_input_embeddings().weight.grad
                # Get the index for tokens that we want to zero the grads for
                index_grads_to_zero = (
                    torch.arange(len(tokenizer)) != placeholder_token_id
                )
                grads.data[index_grads_to_zero, :] = grads.data[
                    index_grads_to_zero, :
                ].fill_(0)

                optimizer.step()
                optimizer.zero_grad()

            # Checks if the accelerator has performed an optimization step behind the scenes
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

            logs = {"loss": loss.detach().item()}
            progress_bar.set_postfix(**logs)

            if global_step >= max_train_steps:
                break

        accelerator.wait_for_everyone()

    # Create the pipeline using using the trained modules and save it.
    if accelerator.is_main_process:
        pipeline = StableDiffusionPipeline(
            text_encoder=accelerator.unwrap_model(text_encoder),
            vae=vae,
            unet=unet,
            tokenizer=tokenizer,
            scheduler=PNDMScheduler(
                beta_start=0.00085,
                beta_end=0.012,
                beta_schedule="scaled_linear",
                skip_prk_steps=True,
            ),
            safety_checker=StableDiffusionSafetyChecker.from_pretrained(
                "CompVis/stable-diffusion-safety-checker"
            ),
            feature_extractor=CLIPFeatureExtractor.from_pretrained(
                "openai/clip-vit-base-patch32"
            ),
        )
        pipeline.save_pretrained(output_dir)
        # Also save the newly trained embeddings
        learned_embeds = (
            accelerator.unwrap_model(text_encoder)
            .get_input_embeddings()
            .weight[placeholder_token_id]
        )
        learned_embeds_dict = {
            args.placeholder_token: learned_embeds.detach().cpu()
        }
        torch.save(
            learned_embeds_dict, os.path.join(output_dir, "learned_embeds.bin")
        )


import accelerate

accelerate.notebook_launcher(
    training_function, args=(text_encoder, vae, unet), num_processes=1
)

# Set up the pipeline
pipe = StableDiffusionPipeline.from_pretrained(
    hyperparameters["output_dir"],
    # torch_dtype=torch.float16,
)

all_images = []
for _ in range(args.num_inference_samples):
    images = pipe(
        [args.prompt],
        num_inference_steps=args.inference_steps,
        guidance_scale=7.5,
    ).images
    all_images.extend(images)

# output_path = os.path.abspath(os.path.join(os.getcwd(), args.output_dir))
if not os.path.isdir(args.output_dir):
    os.mkdir(args.output_dir)

[
    image.save(f"{args.output_dir}/{i}.jpeg")
    for i, image in enumerate(all_images)
]

```

`shark/examples/shark_training/stable_diffusion/README.md`:

```md
# Stable Diffusion Fine Tuning

## Installation (Linux)

### Activate shark.venv Virtual Environment

```shell
source shark.venv/bin/activate

# Some older pip installs may not be able to handle the recent PyTorch deps
python -m pip install --upgrade pip
```

## Install dependencies

### Run the following installation commands:
```
pip install -U git+https://github.com/huggingface/diffusers.git
pip install accelerate transformers ftfy
```

### Build torch-mlir with the following branch:

Please cherry-pick this branch of torch-mlir: https://github.com/vivekkhandelwal1/torch-mlir/tree/sd-ops
and build it locally. You can find the instructions for using locally build Torch-MLIR,
here: https://github.com/nod-ai/SHARK#how-to-use-your-locally-built-iree--torch-mlir-with-shark

## Run the Stable diffusion fine tuning

To run the model with the default set of images and params, run:
```shell
python stable_diffusion_fine_tuning.py
```
By default the training is run through the PyTorch path. If you want to train the model using the Torchdynamo path of Torch-MLIR, you need to specify `--use_torchdynamo=True`.

The default number of training steps are `2000`, which would take many hours to complete based on your system config. You can pass the smaller value with the arg `--training_steps`. You can specify the number of images to be sampled for the result with the `--num_inference_samples` arg. For the number of inference steps you can use `--inference_steps` flag.

For example, you can run the training for a limited set of steps via the dynamo path by using the following command:
```
python stable_diffusion_fine_tuning.py --training_steps=1 --inference_steps=1 --num_inference_samples=1 --train_batch_size=1 --use_torchdynamo=True
```

You can also specify the device to be used via the flag `--device`. The default value is `cpu`, for GPU execution you can specify `--device="cuda"`.

```

`shark/examples/shark_training/stable_diffusion/stable_diffusion_fine_tuning.py`:

```py
# Install the required libs
# pip install -U git+https://github.com/huggingface/diffusers.git
# pip install accelerate transformers ftfy

# Import required libraries
import argparse
import itertools
import math
import os
from typing import List
import random

import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import Dataset

import PIL
import logging

import torch_mlir
from torch_mlir.dynamo import make_simple_dynamo_backend
import torch._dynamo as dynamo
from torch.fx.experimental.proxy_tensor import make_fx
from torch_mlir_e2e_test.linalg_on_tensors_backends import refbackend
from shark.shark_inference import SharkInference

torch._dynamo.config.verbose = True

from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    PNDMScheduler,
    StableDiffusionPipeline,
    UNet2DConditionModel,
)
from diffusers.optimization import get_scheduler
from diffusers.pipelines.stable_diffusion import (
    StableDiffusionSafetyChecker,
)
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm
from transformers import (
    CLIPFeatureExtractor,
    CLIPTextModel,
    CLIPTokenizer,
)


# Enter your HuggingFace Token
# Note: You can comment this prompt and just set your token instead of passing it through cli for every execution.
hf_token = input("Please enter your huggingface token here: ")
YOUR_TOKEN = hf_token


def image_grid(imgs, rows, cols):
    assert len(imgs) == rows * cols

    w, h = imgs[0].size
    grid = Image.new("RGB", size=(cols * w, rows * h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i % cols * w, i // cols * h))
    return grid


# `pretrained_model_name_or_path` which Stable Diffusion checkpoint you want to use
# Options: 1.) "stabilityai/stable-diffusion-2"
#          2.) "stabilityai/stable-diffusion-2-base"
#          3.) "CompVis/stable-diffusion-v1-4"
#          4.) "runwayml/stable-diffusion-v1-5"
pretrained_model_name_or_path = "stabilityai/stable-diffusion-2"

# Add here the URLs to the images of the concept you are adding. 3-5 should be fine
urls = [
    "https://huggingface.co/datasets/valhalla/images/resolve/main/2.jpeg",
    "https://huggingface.co/datasets/valhalla/images/resolve/main/3.jpeg",
    "https://huggingface.co/datasets/valhalla/images/resolve/main/5.jpeg",
    "https://huggingface.co/datasets/valhalla/images/resolve/main/6.jpeg",
    ## You can add additional images here
]

# Downloading Images
import requests
import glob
from io import BytesIO


def download_image(url):
    try:
        response = requests.get(url)
    except:
        return None
    return Image.open(BytesIO(response.content)).convert("RGB")


images = list(filter(None, [download_image(url) for url in urls]))
save_path = "./my_concept"
if not os.path.exists(save_path):
    os.mkdir(save_path)
[image.save(f"{save_path}/{i}.jpeg") for i, image in enumerate(images)]

p = argparse.ArgumentParser(
    description=__doc__,
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
p.add_argument(
    "--input_dir",
    type=str,
    default="my_concept/",
    help="the directory contains the images used for fine tuning",
)
p.add_argument(
    "--output_dir",
    type=str,
    default="sd_result",
    help="the directory contains the images used for fine tuning",
)
p.add_argument(
    "--training_steps",
    type=int,
    default=2000,
    help="the maximum number of training steps",
)
p.add_argument(
    "--train_batch_size",
    type=int,
    default=4,
    help="The batch size for training",
)
p.add_argument(
    "--save_steps",
    type=int,
    default=250,
    help="the number of steps after which to save the learned concept",
)
p.add_argument("--seed", type=int, default=42, help="the random seed")
p.add_argument(
    "--what_to_teach",
    type=str,
    choices=["object", "style"],
    default="object",
    help="what is it that you are teaching?",
)
p.add_argument(
    "--placeholder_token",
    type=str,
    default="<cat-toy>",
    help="It is the token you are going to use to represent your new concept",
)
p.add_argument(
    "--initializer_token",
    type=str,
    default="toy",
    help="It is a word that can summarise what is your new concept",
)
p.add_argument(
    "--inference_steps",
    type=int,
    default=50,
    help="the number of steps for inference",
)
p.add_argument(
    "--num_inference_samples",
    type=int,
    default=4,
    help="the number of samples for inference",
)
p.add_argument(
    "--prompt",
    type=str,
    default="a grafitti in a wall with a *s on it",
    help="the text prompt to use",
)
p.add_argument(
    "--device",
    type=str,
    default="cpu",
    help="The device to use",
)
p.add_argument(
    "--use_torchdynamo",
    type=bool,
    default=False,
    help="This flag is used to determine whether the training has to be done through the torchdynamo path or not.",
)
args = p.parse_args()
torch.manual_seed(args.seed)

if "*s" not in args.prompt:
    raise ValueError(
        f'The prompt should have a "*s" which will be replaced by a placeholder token.'
    )

prompt1, prompt2 = args.prompt.split("*s")
args.prompt = prompt1 + args.placeholder_token + prompt2

# `images_path` is a path to directory containing the training images.
images_path = args.input_dir
while not os.path.exists(str(images_path)):
    print(
        "The images_path specified does not exist, use the colab file explorer to copy the path :"
    )
    images_path = input("")
save_path = images_path

# Setup and check the images you have just added
images = []
for file_path in os.listdir(save_path):
    try:
        image_path = os.path.join(save_path, file_path)
        images.append(Image.open(image_path).resize((512, 512)))
    except:
        print(
            f"{image_path} is not a valid image, please make sure to remove this file from the directory otherwise the training could fail."
        )
image_grid(images, 1, len(images))

########### Create Dataset ##########

# Setup the prompt templates for training
imagenet_templates_small = [
    "a photo of a {}",
    "a rendering of a {}",
    "a cropped photo of the {}",
    "the photo of a {}",
    "a photo of a clean {}",
    "a photo of a dirty {}",
    "a dark photo of the {}",
    "a photo of my {}",
    "a photo of the cool {}",
    "a close-up photo of a {}",
    "a bright photo of the {}",
    "a cropped photo of a {}",
    "a photo of the {}",
    "a good photo of the {}",
    "a photo of one {}",
    "a close-up photo of the {}",
    "a rendition of the {}",
    "a photo of the clean {}",
    "a rendition of a {}",
    "a photo of a nice {}",
    "a good photo of a {}",
    "a photo of the nice {}",
    "a photo of the small {}",
    "a photo of the weird {}",
    "a photo of the large {}",
    "a photo of a cool {}",
    "a photo of a small {}",
]

imagenet_style_templates_small = [
    "a painting in the style of {}",
    "a rendering in the style of {}",
    "a cropped painting in the style of {}",
    "the painting in the style of {}",
    "a clean painting in the style of {}",
    "a dirty painting in the style of {}",
    "a dark painting in the style of {}",
    "a picture in the style of {}",
    "a cool painting in the style of {}",
    "a close-up painting in the style of {}",
    "a bright painting in the style of {}",
    "a cropped painting in the style of {}",
    "a good painting in the style of {}",
    "a close-up painting in the style of {}",
    "a rendition in the style of {}",
    "a nice painting in the style of {}",
    "a small painting in the style of {}",
    "a weird painting in the style of {}",
    "a large painting in the style of {}",
]


# Setup the dataset
class TextualInversionDataset(Dataset):
    def __init__(
        self,
        data_root,
        tokenizer,
        learnable_property="object",  # [object, style]
        size=512,
        repeats=100,
        interpolation="bicubic",
        flip_p=0.5,
        set="train",
        placeholder_token="*",
        center_crop=False,
    ):
        self.data_root = data_root
        self.tokenizer = tokenizer
        self.learnable_property = learnable_property
        self.size = size
        self.placeholder_token = placeholder_token
        self.center_crop = center_crop
        self.flip_p = flip_p

        self.image_paths = [
            os.path.join(self.data_root, file_path)
            for file_path in os.listdir(self.data_root)
        ]

        self.num_images = len(self.image_paths)
        self._length = self.num_images

        if set == "train":
            self._length = self.num_images * repeats

        self.interpolation = {
            "linear": PIL.Image.LINEAR,
            "bilinear": PIL.Image.BILINEAR,
            "bicubic": PIL.Image.BICUBIC,
            "lanczos": PIL.Image.LANCZOS,
        }[interpolation]

        self.templates = (
            imagenet_style_templates_small
            if learnable_property == "style"
            else imagenet_templates_small
        )
        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)

    def __len__(self):
        return self._length

    def __getitem__(self, i):
        example = {}
        image = Image.open(self.image_paths[i % self.num_images])

        if not image.mode == "RGB":
            image = image.convert("RGB")

        placeholder_string = self.placeholder_token
        text = random.choice(self.templates).format(placeholder_string)

        example["input_ids"] = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt",
        ).input_ids[0]

        # default to score-sde preprocessing
        img = np.array(image).astype(np.uint8)

        if self.center_crop:
            crop = min(img.shape[0], img.shape[1])
            (
                h,
                w,
            ) = (
                img.shape[0],
                img.shape[1],
            )
            img = img[
                (h - crop) // 2 : (h + crop) // 2,
                (w - crop) // 2 : (w + crop) // 2,
            ]

        image = Image.fromarray(img)
        image = image.resize(
            (self.size, self.size), resample=self.interpolation
        )

        image = self.flip_transform(image)
        image = np.array(image).astype(np.uint8)
        image = (image / 127.5 - 1.0).astype(np.float32)

        example["pixel_values"] = torch.from_numpy(image).permute(2, 0, 1)
        return example


########## Setting up the model ##########

# Load the tokenizer and add the placeholder token as a additional special token.
tokenizer = CLIPTokenizer.from_pretrained(
    pretrained_model_name_or_path,
    subfolder="tokenizer",
)

# Add the placeholder token in tokenizer
num_added_tokens = tokenizer.add_tokens(args.placeholder_token)
if num_added_tokens == 0:
    raise ValueError(
        f"The tokenizer already contains the token {args.placeholder_token}. Please pass a different"
        " `placeholder_token` that is not already in the tokenizer."
    )

# Get token ids for our placeholder and initializer token.
# This code block will complain if initializer string is not a single token
# Convert the initializer_token, placeholder_token to ids
token_ids = tokenizer.encode(args.initializer_token, add_special_tokens=False)
# Check if initializer_token is a single token or a sequence of tokens
if len(token_ids) > 1:
    raise ValueError("The initializer token must be a single token.")

initializer_token_id = token_ids[0]
placeholder_token_id = tokenizer.convert_tokens_to_ids(args.placeholder_token)

# Load the Stable Diffusion model
# Load models and create wrapper for stable diffusion
# pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path)
# del pipeline
text_encoder = CLIPTextModel.from_pretrained(
    pretrained_model_name_or_path, subfolder="text_encoder"
)
vae = AutoencoderKL.from_pretrained(
    pretrained_model_name_or_path, subfolder="vae"
)
unet = UNet2DConditionModel.from_pretrained(
    pretrained_model_name_or_path, subfolder="unet"
)

# We have added the placeholder_token in the tokenizer so we resize the token embeddings here
# this will a new embedding vector in the token embeddings for our placeholder_token
text_encoder.resize_token_embeddings(len(tokenizer))

# Initialise the newly added placeholder token with the embeddings of the initializer token
token_embeds = text_encoder.get_input_embeddings().weight.data
token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]

# In Textual-Inversion we only train the newly added embedding vector
#  so lets freeze rest of the model parameters here


def freeze_params(params):
    for param in params:
        param.requires_grad = False


# Freeze vae and unet
freeze_params(vae.parameters())
freeze_params(unet.parameters())
# Freeze all parameters except for the token embeddings in text encoder
params_to_freeze = itertools.chain(
    text_encoder.text_model.encoder.parameters(),
    text_encoder.text_model.final_layer_norm.parameters(),
    text_encoder.text_model.embeddings.position_embedding.parameters(),
)
freeze_params(params_to_freeze)


# Move vae and unet to device
# For the dynamo path default compilation device is `cpu`, since torch-mlir
# supports only that. Therefore, convert to device only for PyTorch path.
if not args.use_torchdynamo:
    vae.to(args.device)
    unet.to(args.device)

# Keep vae in eval mode as we don't train it
vae.eval()
# Keep unet in train mode to enable gradient checkpointing
unet.train()


class VaeModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.vae = vae

    def forward(self, input):
        x = self.vae.encode(input, return_dict=False)[0]
        return x


class UnetModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.unet = unet

    def forward(self, x, y, z):
        return self.unet.forward(x, y, z, return_dict=False)[0]


shark_vae = VaeModel()
shark_unet = UnetModel()

####### Creating our training data ########

# Let's create the Dataset and Dataloader
train_dataset = TextualInversionDataset(
    data_root=save_path,
    tokenizer=tokenizer,
    size=vae.sample_size,
    placeholder_token=args.placeholder_token,
    repeats=100,
    learnable_property=args.what_to_teach,  # Option selected above between object and style
    center_crop=False,
    set="train",
)


def create_dataloader(train_batch_size=1):
    return torch.utils.data.DataLoader(
        train_dataset, batch_size=train_batch_size, shuffle=True
    )


# Create noise_scheduler for training
noise_scheduler = DDPMScheduler.from_config(
    pretrained_model_name_or_path, subfolder="scheduler"
)

######## Training ###########

# Define hyperparameters for our training. If you are not happy with your results,
# you can tune the `learning_rate` and the `max_train_steps`

# Setting up all training args
hyperparameters = {
    "learning_rate": 5e-04,
    "scale_lr": True,
    "max_train_steps": args.training_steps,
    "save_steps": args.save_steps,
    "train_batch_size": args.train_batch_size,
    "gradient_accumulation_steps": 1,
    "gradient_checkpointing": True,
    "mixed_precision": "fp16",
    "seed": 42,
    "output_dir": "sd-concept-output",
}
# creating output directory
cwd = os.getcwd()
out_dir = os.path.join(cwd, hyperparameters["output_dir"])
while not os.path.exists(str(out_dir)):
    try:
        os.mkdir(out_dir)
    except OSError as error:
        print("Output directory not created")

###### Torch-MLIR Compilation ######


def _remove_nones(fx_g: torch.fx.GraphModule) -> List[int]:
    removed_indexes = []
    for node in fx_g.graph.nodes:
        if node.op == "output":
            assert (
                len(node.args) == 1
            ), "Output node must have a single argument"
            node_arg = node.args[0]
            if isinstance(node_arg, (list, tuple)):
                node_arg = list(node_arg)
                node_args_len = len(node_arg)
                for i in range(node_args_len):
                    curr_index = node_args_len - (i + 1)
                    if node_arg[curr_index] is None:
                        removed_indexes.append(curr_index)
                        node_arg.pop(curr_index)
                node.args = (tuple(node_arg),)
                break

    if len(removed_indexes) > 0:
        fx_g.graph.lint()
        fx_g.graph.eliminate_dead_code()
        fx_g.recompile()
    removed_indexes.sort()
    return removed_indexes


def _unwrap_single_tuple_return(fx_g: torch.fx.GraphModule) -> bool:
    """
    Replace tuple with tuple element in functions that return one-element tuples.
    Returns true if an unwrapping took place, and false otherwise.
    """
    unwrapped_tuple = False
    for node in fx_g.graph.nodes:
        if node.op == "output":
            assert (
                len(node.args) == 1
            ), "Output node must have a single argument"
            node_arg = node.args[0]
            if isinstance(node_arg, tuple):
                if len(node_arg) == 1:
                    node.args = (node_arg[0],)
                    unwrapped_tuple = True
                    break

    if unwrapped_tuple:
        fx_g.graph.lint()
        fx_g.recompile()
    return unwrapped_tuple


def _returns_nothing(fx_g: torch.fx.GraphModule) -> bool:
    for node in fx_g.graph.nodes:
        if node.op == "output":
            assert (
                len(node.args) == 1
            ), "Output node must have a single argument"
            node_arg = node.args[0]
            if isinstance(node_arg, tuple):
                return len(node_arg) == 0
    return False


def transform_fx(fx_g):
    for node in fx_g.graph.nodes:
        if node.op == "call_function":
            if node.target in [
                torch.ops.aten.empty,
            ]:
                # aten.empty should be filled with zeros.
                if node.target in [torch.ops.aten.empty]:
                    with fx_g.graph.inserting_after(node):
                        new_node = fx_g.graph.call_function(
                            torch.ops.aten.zero_,
                            args=(node,),
                        )
                        node.append(new_node)
                        node.replace_all_uses_with(new_node)
                        new_node.args = (node,)

    fx_g.graph.lint()


@make_simple_dynamo_backend
def refbackend_torchdynamo_backend(
    fx_graph: torch.fx.GraphModule, example_inputs: List[torch.Tensor]
):
    # handling usage of empty tensor without initializing
    transform_fx(fx_graph)
    fx_graph.recompile()
    if _returns_nothing(fx_graph):
        return fx_graph
    removed_none_indexes = _remove_nones(fx_graph)
    was_unwrapped = _unwrap_single_tuple_return(fx_graph)

    mlir_module = torch_mlir.compile(
        fx_graph, example_inputs, output_type="linalg-on-tensors"
    )

    bytecode_stream = BytesIO()
    mlir_module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()

    shark_module = SharkInference(
        mlir_module=bytecode, device=args.device, mlir_dialect="tm_tensor"
    )
    shark_module.compile()

    def compiled_callable(*inputs):
        inputs = [x.numpy() for x in inputs]
        result = shark_module("forward", inputs)
        if was_unwrapped:
            result = [
                result,
            ]
        if not isinstance(result, list):
            result = torch.from_numpy(result)
        else:
            result = tuple(torch.from_numpy(x) for x in result)
            result = list(result)
            for removed_index in removed_none_indexes:
                result.insert(removed_index, None)
            result = tuple(result)
        return result

    return compiled_callable


def predictions(torch_func, jit_func, batchA, batchB):
    res = jit_func(batchA.numpy(), batchB.numpy())
    if res is not None:
        prediction = res
    else:
        prediction = None
    return prediction


logger = logging.getLogger(__name__)


# def save_progress(text_encoder, placeholder_token_id, accelerator, save_path):
def save_progress(text_encoder, placeholder_token_id, save_path):
    logger.info("Saving embeddings")
    learned_embeds = (
        # accelerator.unwrap_model(text_encoder)
        text_encoder.get_input_embeddings().weight[placeholder_token_id]
    )
    learned_embeds_dict = {
        args.placeholder_token: learned_embeds.detach().cpu()
    }
    torch.save(learned_embeds_dict, save_path)


train_batch_size = hyperparameters["train_batch_size"]
gradient_accumulation_steps = hyperparameters["gradient_accumulation_steps"]
learning_rate = hyperparameters["learning_rate"]
if hyperparameters["scale_lr"]:
    learning_rate = (
        learning_rate
        * gradient_accumulation_steps
        * train_batch_size
        # * accelerator.num_processes
    )

# Initialize the optimizer
optimizer = torch.optim.AdamW(
    text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings
    lr=learning_rate,
)


# Training function
def train_func(batch_pixel_values, batch_input_ids):
    # Convert images to latent space
    latents = shark_vae(batch_pixel_values).sample().detach()
    latents = latents * 0.18215

    # Sample noise that we'll add to the latents
    noise = torch.randn_like(latents)
    bsz = latents.shape[0]
    # Sample a random timestep for each image
    timesteps = torch.randint(
        0,
        noise_scheduler.num_train_timesteps,
        (bsz,),
        device=latents.device,
    ).long()

    # Add noise to the latents according to the noise magnitude at each timestep
    # (this is the forward diffusion process)
    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

    # Get the text embedding for conditioning
    encoder_hidden_states = text_encoder(batch_input_ids)[0]

    # Predict the noise residual
    noise_pred = shark_unet(
        noisy_latents,
        timesteps,
        encoder_hidden_states,
    )

    # Get the target for loss depending on the prediction type
    if noise_scheduler.config.prediction_type == "epsilon":
        target = noise
    elif noise_scheduler.config.prediction_type == "v_prediction":
        target = noise_scheduler.get_velocity(latents, noise, timesteps)
    else:
        raise ValueError(
            f"Unknown prediction type {noise_scheduler.config.prediction_type}"
        )

    loss = (
        F.mse_loss(noise_pred, target, reduction="none").mean([1, 2, 3]).mean()
    )
    loss.backward()

    # Zero out the gradients for all token embeddings except the newly added
    # embeddings for the concept, as we only want to optimize the concept embeddings
    grads = text_encoder.get_input_embeddings().weight.grad
    # Get the index for tokens that we want to zero the grads for
    index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id
    grads.data[index_grads_to_zero, :] = grads.data[
        index_grads_to_zero, :
    ].fill_(0)

    optimizer.step()
    optimizer.zero_grad()

    return loss


def training_function():
    max_train_steps = hyperparameters["max_train_steps"]
    output_dir = hyperparameters["output_dir"]
    gradient_checkpointing = hyperparameters["gradient_checkpointing"]

    train_dataloader = create_dataloader(train_batch_size)

    # We need to recalculate our total training steps as the size of the training dataloader may have changed.
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / gradient_accumulation_steps
    )
    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)

    # Train!
    total_batch_size = (
        train_batch_size
        * gradient_accumulation_steps
        # train_batch_size * accelerator.num_processes * gradient_accumulation_steps
    )

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
    logger.info(
        f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}"
    )
    logger.info(
        f"  Gradient Accumulation steps = {gradient_accumulation_steps}"
    )
    logger.info(f"  Total optimization steps = {max_train_steps}")
    # Only show the progress bar once on each machine.
    progress_bar = tqdm(
        # range(max_train_steps), disable=not accelerator.is_local_main_process
        range(max_train_steps)
    )
    progress_bar.set_description("Steps")
    global_step = 0

    params_ = [i for i in text_encoder.get_input_embeddings().parameters()]
    if args.use_torchdynamo:
        print("******** TRAINING STARTED - TORCHYDNAMO PATH ********")
    else:
        print("******** TRAINING STARTED - PYTORCH PATH ********")
    print("Initial weights:")
    print(params_, params_[0].shape)

    for epoch in range(num_train_epochs):
        text_encoder.train()
        for step, batch in enumerate(train_dataloader):
            if args.use_torchdynamo:
                dynamo_callable = dynamo.optimize(
                    refbackend_torchdynamo_backend
                )(train_func)
                lam_func = lambda x, y: dynamo_callable(
                    torch.from_numpy(x), torch.from_numpy(y)
                )
                loss = predictions(
                    train_func,
                    lam_func,
                    batch["pixel_values"],
                    batch["input_ids"],
                    # params[0].detach(),
                )
            else:
                loss = train_func(batch["pixel_values"], batch["input_ids"])
            print(loss)

            # Checks if the accelerator has performed an optimization step behind the scenes
            progress_bar.update(1)
            global_step += 1
            if global_step % hyperparameters["save_steps"] == 0:
                save_path = os.path.join(
                    output_dir,
                    f"learned_embeds-step-{global_step}.bin",
                )
                save_progress(
                    text_encoder,
                    placeholder_token_id,
                    save_path,
                )

            logs = {"loss": loss.detach().item()}
            progress_bar.set_postfix(**logs)

            if global_step >= max_train_steps:
                break

    # Create the pipeline using using the trained modules and save it.
    params__ = [i for i in text_encoder.get_input_embeddings().parameters()]
    print("******** TRAINING PROCESS FINISHED ********")
    print("Updated weights:")
    print(params__, params__[0].shape)
    pipeline = StableDiffusionPipeline.from_pretrained(
        pretrained_model_name_or_path,
        # text_encoder=accelerator.unwrap_model(text_encoder),
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        vae=vae,
        unet=unet,
    )
    pipeline.save_pretrained(output_dir)
    # Also save the newly trained embeddings
    save_path = os.path.join(output_dir, f"learned_embeds.bin")
    save_progress(text_encoder, placeholder_token_id, save_path)


training_function()

for param in itertools.chain(unet.parameters(), text_encoder.parameters()):
    if param.grad is not None:
        del param.grad  # free some memory
    torch.cuda.empty_cache()

# Set up the pipeline
from diffusers import DPMSolverMultistepScheduler

pipe = StableDiffusionPipeline.from_pretrained(
    hyperparameters["output_dir"],
    scheduler=DPMSolverMultistepScheduler.from_pretrained(
        hyperparameters["output_dir"], subfolder="scheduler"
    ),
)
if not args.use_torchdynamo:
    pipe.to(args.device)

# Run the Stable Diffusion pipeline
# Don't forget to use the placeholder token in your prompt

all_images = []
for _ in range(args.num_inference_samples):
    images = pipe(
        [args.prompt],
        num_inference_steps=args.inference_steps,
        guidance_scale=7.5,
    ).images
    all_images.extend(images)

output_path = os.path.abspath(os.path.join(os.getcwd(), args.output_dir))
if not os.path.isdir(args.output_dir):
    os.mkdir(args.output_dir)

[
    image.save(f"{args.output_dir}/{i}.jpeg")
    for i, image in enumerate(all_images)
]

```

`shark/iree_eager_backend.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict, Any

import iree
import iree.runtime as ireert
import numpy as np
import torch
from iree.runtime import DeviceArray
from torch_mlir._mlir_libs._mlir.ir import Module
from torch_mlir.compiler_utils import (
    run_pipeline_with_repro_report,
)
from torch_mlir.eager_mode.torch_mlir_eager_backend import (
    TorchMLIREagerBackend,
    TensorMetaData,
)
from torch_mlir_e2e_test.eager_backends.refbackend import (
    NUMPY_TO_TORCH_DTYPE_DICT,
)

from shark.iree_utils.compile_utils import (
    get_iree_compiled_module,
    IREE_DEVICE_MAP,
)


class EagerModeIREELinalgOnTensorsBackend(TorchMLIREagerBackend):
    """Main entry-point for the iree backend for torch-mlir eager mode.

    EagerModeIREELinalgOnTensorsBackend uses iree.DeviceArray representations of tensors and
    thus all of the wrapping and unwrapping and munging here is done to between torch.Tensor and iree.DeviceArray,
    with np.ndarray as an intermediary.
    """

    def __init__(self, device: str):
        self.torch_device_str = device
        self.config = ireert.Config(IREE_DEVICE_MAP[device])
        self.raw_device_str = device

    def get_torch_metadata(
        self, tensor: DeviceArray, kwargs: Dict[str, Any]
    ) -> TensorMetaData:
        return TensorMetaData(
            size=tensor.shape,
            dtype=NUMPY_TO_TORCH_DTYPE_DICT[tensor.dtype.type],
            device=torch.device(self.torch_device_str),
            requires_grad=tensor.dtype.type
            in {np.float, np.float32, np.float64}
            and kwargs.get("requires_grad", False),
        )

    def compile(self, imported_module: Module):
        run_pipeline_with_repro_report(
            imported_module,
            "torch-function-to-torch-backend-pipeline,torch-backend-to-linalg-on-tensors-backend-pipeline",
            "EagerMode",
        )
        callable, _ = get_iree_compiled_module(
            imported_module, self.raw_device_str
        )
        return callable

    def copy_into(self, dst, src):
        """Copy output back to appropriate arg that it should alias."""
        np.copyto(dst, src)

    def transfer_from_device_to_torch(self, e):
        return torch.from_numpy(e.to_host())

    def transfer_from_torch_to_device(
        self, tensor: torch.Tensor
    ) -> DeviceArray:
        return iree.runtime.asdevicearray(self.config.device, tensor.numpy())

```

`shark/iree_utils/_common.py`:

```py
# Copyright 2023 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## Common utilities to be shared by iree utilities.
import functools
import os
import sys
import subprocess


def run_cmd(cmd, debug=False, raise_err=False):
    """
    Inputs:
      cmd : cli command string.
      debug : if True, prints debug info
      raise_err : if True, raise exception to caller
    """
    if debug:
        print("IREE run command: \n\n")
        print(cmd)
        print("\n\n")
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True,
        )
        stdout = result.stdout.decode()
        stderr = result.stderr.decode()
        return stdout, stderr
    except subprocess.CalledProcessError as e:
        if raise_err:
            raise Exception from e
        else:
            print(e.output)
            sys.exit(f"Exiting program due to error running {cmd}")


def iree_device_map(device):
    uri_parts = device.split("://", 2)
    iree_driver = (
        _IREE_DEVICE_MAP[uri_parts[0]]
        if uri_parts[0] in _IREE_DEVICE_MAP
        else uri_parts[0]
    )
    if len(uri_parts) == 1:
        return iree_driver
    elif "rocm" in uri_parts:
        return "rocm"
    else:
        return f"{iree_driver}://{uri_parts[1]}"


def get_supported_device_list():
    return list(_IREE_DEVICE_MAP.keys())


_IREE_DEVICE_MAP = {
    "cpu": "local-task",
    "cpu-task": "local-task",
    "cpu-sync": "local-sync",
    "cuda": "cuda",
    "vulkan": "vulkan",
    "metal": "metal",
    "rocm": "rocm",
    "intel-gpu": "level_zero",
}


def iree_target_map(device):
    if "://" in device:
        device = device.split("://")[0]
    return _IREE_TARGET_MAP[device] if device in _IREE_TARGET_MAP else device


_IREE_TARGET_MAP = {
    "cpu": "llvm-cpu",
    "cpu-task": "llvm-cpu",
    "cpu-sync": "llvm-cpu",
    "cuda": "cuda",
    "vulkan": "vulkan",
    "metal": "metal",
    "rocm": "rocm",
    "intel-gpu": "opencl-spirv",
}


# Finds whether the required drivers are installed for the given device.
@functools.cache
def check_device_drivers(device):
    """
    Checks necessary drivers present for gpu and vulkan devices
    False => drivers present!
    """
    if "://" in device:
        device = device.split("://")[0]

    from iree.runtime import get_driver

    device_mapped = iree_device_map(device)

    try:
        _ = get_driver(device_mapped)
    except ValueError as ve:
        print(
            f"[ERR] device `{device}` not registered with IREE. "
            "Ensure IREE is configured for use with this device.\n"
            f"Full Error: \n {repr(ve)}"
        )
        return True
    except RuntimeError as re:
        print(
            f"[ERR] Failed to get driver for {device} with error:\n{repr(re)}"
        )
        return True

    # Unknown device. We assume drivers are installed.
    return False


# Installation info for the missing device drivers.
def device_driver_info(device):
    device_driver_err_map = {
        "cuda": {
            "debug": "Try `nvidia-smi` on system to check.",
            "solution": " from https://www.nvidia.in/Download/index.aspx?lang=en-in for your system.",
        },
        "vulkan": {
            "debug": "Try `vulkaninfo` on system to check.",
            "solution": " from https://vulkan.lunarg.com/sdk/home for your distribution.",
        },
        "metal": {
            "debug": "Check if Bare metal is supported and enabled on your system.",
            "solution": ".",
        },
        "rocm": {
            "debug": f"Try `{'hip' if sys.platform == 'win32' else 'rocm'}info` on system to check.",
            "solution": " from https://rocm.docs.amd.com/en/latest/rocm.html for your system.",
        },
    }

    if device in device_driver_err_map:
        err_msg = (
            f"Required drivers for {device} not found. {device_driver_err_map[device]['debug']} "
            f"Please install the required drivers{device_driver_err_map[device]['solution']} "
            f"For further assistance please reach out to the community on discord [https://discord.com/invite/RUqY2h2s9u]"
            f" and/or file a bug at https://github.com/nod-ai/SHARK/issues"
        )
        return err_msg
    else:
        return f"{device} is not supported."

```

`shark/iree_utils/benchmark_utils.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from shark.iree_utils._common import run_cmd, iree_device_map
from shark.iree_utils.cpu_utils import get_cpu_count
import numpy as np
import os
import re
import platform

UNIT_TO_SECOND_MAP = {"us": 1e-6, "ms": 0.001, "s": 1}


def tensor_to_type_str(input_tensors: tuple, mlir_dialect: str):
    """
    Input: A tuple of input tensors i.e tuple(torch.tensor)
    Output: list of string that represent mlir types (i.e 1x24xf64)
    # TODO: Support more than floats, and ints
    """
    list_of_type = []
    for input_tensor in input_tensors:
        type_string = "x".join([str(dim) for dim in input_tensor.shape])
        if mlir_dialect in ["linalg", "tosa"]:
            dtype_string = str(input_tensor.dtype).replace("torch.", "")
        elif mlir_dialect in ["mhlo", "tflite"]:
            dtype = input_tensor.dtype
            try:
                dtype_string = re.findall("'[^\"]*'", str(dtype))[0].replace(
                    "'", ""
                )
            except IndexError:
                dtype_string = str(dtype)
        regex_split = re.compile("([a-zA-Z]+)([0-9]+)")
        match = regex_split.match(dtype_string)
        mlir_type_string = str(match.group(1)[0]) + str(match.group(2))
        type_string += f"x{mlir_type_string}"
        list_of_type.append(type_string)
    return list_of_type


def build_benchmark_args(
    input_file: str,
    device: str,
    input_tensors: tuple,
    mlir_dialect: str,
    training=False,
):
    """
    Inputs: input_file leading to vmfb, input_tensor to function, target device,
    and whether it is training or not.
    Outputs: string that execute benchmark-module on target model.
    """
    path = os.path.join(os.environ["VIRTUAL_ENV"], "bin")
    if platform.system() == "Windows":
        benchmarker_path = os.path.join(path, "iree-benchmark-module.exe")
        time_extractor = None
    else:
        benchmarker_path = os.path.join(path, "iree-benchmark-module")
        time_extractor = "| awk 'END{{print $2 $3}}'"
    benchmark_cl = [benchmarker_path, f"--module={input_file}"]
    # TODO: The function named can be passed as one of the args.
    fn_name = "forward"
    if training == True:
        # TODO: Replace name of train with actual train fn name.
        fn_name = "train"
    benchmark_cl.append(f"--function={fn_name}")
    benchmark_cl.append(f"--device={iree_device_map(device)}")
    mlir_input_types = tensor_to_type_str(input_tensors, mlir_dialect)
    for mlir_input in mlir_input_types:
        benchmark_cl.append(f"--input={mlir_input}")
    if device == "cpu":
        num_cpus = get_cpu_count()
        if num_cpus is not None:
            benchmark_cl.append(f"--task_topology_max_group_count={num_cpus}")
    # if time_extractor:
    #    benchmark_cl.append(time_extractor)
    benchmark_cl.append(f"--print_statistics=true")
    return benchmark_cl


def build_benchmark_args_non_tensor_input(
    input_file: str,
    device: str,
    inputs: tuple,
    mlir_dialect: str,
    function_name: str,
):
    """
    Inputs: input_file leading to vmfb, input_tensor to function, target device,
    and whether it is training or not.
    Outputs: string that execute benchmark-module on target model.
    """
    path = os.path.join(os.environ["VIRTUAL_ENV"], "bin")
    if platform.system() == "Windows":
        benchmarker_path = os.path.join(path, "iree-benchmark-module.exe")
        time_extractor = None
    else:
        benchmarker_path = os.path.join(path, "iree-benchmark-module")
        time_extractor = "| awk 'END{{print $2 $3}}'"
    benchmark_cl = [benchmarker_path, f"--module={input_file}"]
    # TODO: The function named can be passed as one of the args.
    if function_name:
        benchmark_cl.append(f"--function={function_name}")
    benchmark_cl.append(f"--device={iree_device_map(device)}")
    for input in inputs:
        benchmark_cl.append(f"--input={input}")
    if platform.system() != "Windows":
        time_extractor = "| awk 'END{{print $2 $3}}'"
        benchmark_cl.append(time_extractor)
    return benchmark_cl


def run_benchmark_module(benchmark_cl):
    """
    Run benchmark command, extract result and return iteration/seconds, host
    peak memory, and device peak memory.

    # TODO: Add an example of the benchmark command.
    Input: benchmark command.
    """
    benchmark_path = benchmark_cl[0]
    assert os.path.exists(
        benchmark_path
    ), "Cannot find iree_benchmark_module, Please contact SHARK maintainer on discord."
    bench_stdout, bench_stderr = run_cmd(" ".join(benchmark_cl))
    try:
        regex_split = re.compile("(\d+[.]*\d*)(  *)([a-zA-Z]+)")
        match = regex_split.search(bench_stdout)
        time_ms = float(match.group(1))
        unit = match.group(3)
    except AttributeError:
        regex_split = re.compile("(\d+[.]*\d*)([a-zA-Z]+)")
        match = regex_split.search(bench_stdout)
        time_ms = float(match.group(1))
        unit = match.group(2)
    iter_per_second = 1.0 / (time_ms * 0.001)

    # Extract peak memory.
    host_regex = re.compile(r".*HOST_LOCAL:\s*([0-9]+)B peak")
    host_peak_b = int(host_regex.search(bench_stderr).group(1))
    device_regex = re.compile(r".*DEVICE_LOCAL:\s*([0-9]+)B peak")
    device_peak_b = int(device_regex.search(bench_stderr).group(1))
    return iter_per_second, host_peak_b, device_peak_b

```

`shark/iree_utils/compile_utils.py`:

```py
# Copyright 2023 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import functools
import numpy as np
import os
import re
import tempfile
from pathlib import Path

import iree.runtime as ireert
import iree.compiler as ireec
from shark.parser import shark_args

from .trace import DetailLogger
from ._common import iree_device_map, iree_target_map
from .cpu_utils import get_iree_cpu_rt_args
from .benchmark_utils import *


# Get the iree-compile arguments given device.
def get_iree_device_args(device, extra_args=[]):
    print("Configuring for device:" + device)
    device, device_num = clean_device_info(device)

    if "cpu" in device:
        from shark.iree_utils.cpu_utils import get_iree_cpu_args

        u_kernel_flag = ["--iree-llvmcpu-enable-ukernels"]
        stack_size_flag = ["--iree-llvmcpu-stack-allocation-limit=256000"]

        return (
            get_iree_cpu_args()
            + u_kernel_flag
            + stack_size_flag
            + ["--iree-global-opt-enable-quantized-matmul-reassociation"]
        )
    if device == "cuda":
        from shark.iree_utils.gpu_utils import get_iree_gpu_args

        return get_iree_gpu_args()
    if device == "vulkan":
        from shark.iree_utils.vulkan_utils import get_iree_vulkan_args

        return get_iree_vulkan_args(
            device_num=device_num, extra_args=extra_args
        )
    if device == "metal":
        from shark.iree_utils.metal_utils import get_iree_metal_args

        return get_iree_metal_args(extra_args=extra_args)
    if device == "rocm":
        from shark.iree_utils.gpu_utils import get_iree_rocm_args

        return get_iree_rocm_args(device_num=device_num, extra_args=extra_args)
    return []


def clean_device_info(raw_device):
    # return appropriate device and device_id for consumption by Studio pipeline
    # Multiple devices only supported for vulkan and rocm (as of now).
    # default device must be selected for all others

    device_id = None
    device = (
        raw_device
        if "=>" not in raw_device
        else raw_device.split("=>")[1].strip()
    )
    if "://" in device:
        device, device_id = device.split("://")
        if len(device_id) <= 2:
            device_id = int(device_id)

    if device not in ["rocm", "vulkan"]:
        device_id = ""
    if device in ["rocm", "vulkan"] and device_id == None:
        device_id = 0
    return device, device_id


# Get the iree-compiler arguments given frontend.
def get_iree_frontend_args(frontend):
    if frontend in ["torch", "pytorch", "linalg", "tm_tensor"]:
        return ["--iree-llvmcpu-target-cpu-features=host"]
    elif frontend in ["tensorflow", "tf", "mhlo", "stablehlo"]:
        return [
            "--iree-llvmcpu-target-cpu-features=host",
            "--iree-input-demote-i64-to-i32",
        ]
    else:
        # Frontend not found.
        return []


# Common args to be used given any frontend or device.
def get_iree_common_args(debug=False):
    common_args = [
        "--iree-stream-resource-max-allocation-size=4294967295",
        "--iree-vm-bytecode-module-strip-source-map=true",
        "--iree-util-zero-fill-elided-attrs",
    ]
    if debug == True:
        common_args.extend(
            [
                "--iree-opt-strip-assertions=false",
                "--verify=true",
            ]
        )
    else:
        common_args.extend(
            [
                "--iree-opt-strip-assertions=true",
                "--verify=false",
            ]
        )
    return common_args


# Args that are suitable only for certain models or groups of models.
# shark_args are passed down from pytests to control which models compile with these flags,
# but they can also be set in shark/parser.py
def get_model_specific_args():
    ms_args = []
    if shark_args.enable_conv_transform == True:
        ms_args += [
            "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-flow-convert-conv-nchw-to-nhwc))"
        ]
    if shark_args.enable_img2col_transform == True:
        ms_args += [
            "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-preprocessing-convert-conv2d-to-img2col))"
        ]
    if shark_args.use_winograd == True:
        ms_args += [
            "--iree-preprocessing-pass-pipeline=builtin.module(func.func(iree-linalg-ext-convert-conv2d-to-winograd))"
        ]
    return ms_args


def create_dispatch_dirs(bench_dir, device):
    protected_files = ["ordered-dispatches.txt"]
    bench_dir_path = bench_dir.split("/")
    bench_dir_path[-1] = "temp_" + bench_dir_path[-1]
    tmp_bench_dir = "/".join(bench_dir_path)
    for f_ in os.listdir(bench_dir):
        if os.path.isfile(f"{bench_dir}/{f_}") and f_ not in protected_files:
            dir_name = re.sub("\.\S*$", "", f_)
            if os.path.exists(f"{bench_dir}/{dir_name}"):
                os.system(f"rm -rf {bench_dir}/{dir_name}")
            os.system(f"mkdir {bench_dir}/{dir_name}")
            os.system(f"mv {bench_dir}/{f_} {bench_dir}/{dir_name}/{f_}")
    for f_ in os.listdir(tmp_bench_dir):
        if os.path.isfile(f"{tmp_bench_dir}/{f_}"):
            dir_name = ""
            for d_ in os.listdir(bench_dir):
                if re.search(f"{d_}(?=\D)", f_):
                    dir_name = d_
            if dir_name != "":
                os.system(
                    f"mv {tmp_bench_dir}/{f_} {bench_dir}/{dir_name}/{dir_name}_benchmark.mlir"
                )


def dump_isas(bench_dir):
    for d_ in os.listdir(bench_dir):
        if os.path.isdir(f"{bench_dir}/{d_}"):
            for f_ in os.listdir(f"{bench_dir}/{d_}"):
                if f_.endswith(".spv"):
                    os.system(
                        f"amdllpc -gfxip 11.0 {bench_dir}/{d_}/{f_} -v > \
                         {bench_dir}/{d_}/isa.txt"
                    )


def compile_benchmark_dirs(bench_dir, device, dispatch_benchmarks):
    benchmark_runtimes = {}
    dispatch_list = []
    all_dispatches = False

    if dispatch_benchmarks.lower().strip() == "all":
        all_dispatches = True
    else:
        try:
            dispatch_list = [
                int(dispatch_index)
                for dispatch_index in dispatch_benchmarks.split(" ")
            ]
        except:
            print("ERROR: Invalid dispatch benchmarks")
            return None
    for d_ in os.listdir(bench_dir):
        if os.path.isdir(f"{bench_dir}/{d_}"):
            in_dispatches = False
            for dispatch in dispatch_list:
                if str(dispatch) in d_:
                    in_dispatches = True
            if all_dispatches or in_dispatches:
                for f_ in os.listdir(f"{bench_dir}/{d_}"):
                    if "benchmark.mlir" in f_:
                        dispatch_file = open(f"{bench_dir}/{d_}/{f_}", "r")
                        module = dispatch_file.read()
                        dispatch_file.close()

                        flatbuffer_blob = ireec.compile_str(
                            module, target_backends=[iree_target_map(device)]
                        )

                        vmfb_file = open(
                            f"{bench_dir}/{d_}/{d_}_benchmark.vmfb", "wb"
                        )
                        vmfb_file.write(flatbuffer_blob)
                        vmfb_file.close()

                        config = get_iree_runtime_config(device)
                        vm_module = ireert.VmModule.from_buffer(
                            config.vm_instance,
                            flatbuffer_blob,
                            warn_if_copy=False,
                        )

                        benchmark_cl = build_benchmark_args_non_tensor_input(
                            input_file=f"{bench_dir}/{d_}/{d_}_benchmark.vmfb",
                            device=device,
                            inputs=(0,),
                            mlir_dialect="linalg",
                            function_name="",
                        )

                        benchmark_bash = open(
                            f"{bench_dir}/{d_}/{d_}_benchmark.sh", "w+"
                        )
                        benchmark_bash.write("#!/bin/bash\n")
                        benchmark_bash.write(" ".join(benchmark_cl))
                        benchmark_bash.close()

                        iter_per_second, _, _ = run_benchmark_module(
                            benchmark_cl
                        )

                        benchmark_file = open(
                            f"{bench_dir}/{d_}/{d_}_data.txt", "w+"
                        )
                        benchmark_file.write(f"DISPATCH: {d_}\n")
                        benchmark_file.write(str(iter_per_second) + "\n")
                        benchmark_file.write(
                            "SHARK BENCHMARK RESULT: "
                            + str(1 / (iter_per_second * 0.001))
                            + "\n"
                        )
                        benchmark_file.close()

                        benchmark_runtimes[d_] = 1 / (iter_per_second * 0.001)

                    elif ".mlir" in f_ and "benchmark" not in f_:
                        dispatch_file = open(f"{bench_dir}/{d_}/{f_}", "r")
                        module = dispatch_file.read()
                        dispatch_file.close()

                        module = re.sub(
                            "hal.executable private",
                            "hal.executable public",
                            module,
                        )

                        flatbuffer_blob = ireec.compile_str(
                            module,
                            target_backends=[iree_target_map(device)],
                            extra_args=["--compile-mode=hal-executable"],
                        )

                        spirv_file = open(
                            f"{bench_dir}/{d_}/{d_}_spirv.vmfb", "wb"
                        )
                        spirv_file.write(flatbuffer_blob)
                        spirv_file.close()

    ordered_dispatches = [
        (k, v)
        for k, v in sorted(
            benchmark_runtimes.items(), key=lambda item: item[1]
        )
    ][::-1]
    f_ = open(f"{bench_dir}/ordered-dispatches.txt", "w+")
    for dispatch in ordered_dispatches:
        f_.write(f"{dispatch[0]}: {dispatch[1]}ms\n")
    f_.close()


def compile_module_to_flatbuffer(
    module,
    device,
    frontend,
    model_config_path,
    extra_args,
    model_name="None",
    debug=False,
    compile_str=False,
):
    # Setup Compile arguments wrt to frontends.
    input_type = "auto"
    args = get_iree_frontend_args(frontend)
    args += get_iree_device_args(device, extra_args)
    args += get_iree_common_args(debug=debug)
    args += get_model_specific_args()
    args += extra_args
    args += shark_args.additional_compile_args

    if frontend in ["tensorflow", "tf"]:
        input_type = "auto"
    elif frontend in ["stablehlo", "tosa"]:
        input_type = frontend
    elif frontend in ["tflite", "tflite-tosa"]:
        input_type = "tosa"
    elif frontend in ["tm_tensor"]:
        input_type = ireec.InputType.TM_TENSOR
    elif frontend in ["torch", "pytorch"]:
        input_type = "torch"

    if compile_str:
        flatbuffer_blob = ireec.compile_str(
            module,
            target_backends=[iree_target_map(device)],
            extra_args=args,
            input_type=input_type,
        )
    else:
        assert os.path.isfile(module)
        flatbuffer_blob = ireec.compile_file(
            str(module),
            input_type=input_type,
            target_backends=[iree_target_map(device)],
            extra_args=args,
        )

    return flatbuffer_blob


def get_iree_module(
    flatbuffer_blob, device, device_idx=None, rt_flags: list = []
):
    # Returns the compiled module and the configs.
    for flag in rt_flags:
        ireert.flags.parse_flag(flag)
    if device_idx is not None:
        device = iree_device_map(device)
        print("registering device id: ", device_idx)
        haldriver = ireert.get_driver(device)
        haldevice = haldriver.create_device(
            haldriver.query_available_devices()[device_idx]["device_id"],
            allocators=shark_args.device_allocator,
        )
        config = ireert.Config(device=haldevice)
    else:
        config = get_iree_runtime_config(device)
    vm_module = ireert.VmModule.from_buffer(
        config.vm_instance, flatbuffer_blob, warn_if_copy=False
    )
    ctx = ireert.SystemContext(config=config)
    ctx.add_vm_module(vm_module)
    ModuleCompiled = getattr(ctx.modules, vm_module.name)
    return ModuleCompiled, config


def load_vmfb_using_mmap(
    flatbuffer_blob_or_path,
    device: str,
    device_idx: int = None,
    rt_flags: list = [],
):
    print(f"Loading module {flatbuffer_blob_or_path}...")
    if "task" in device:
        print(
            f"[DEBUG] setting iree runtime flags for cpu:\n{' '.join(get_iree_cpu_rt_args())}"
        )
        for flag in get_iree_cpu_rt_args():
            rt_flags.append(flag)
    for flag in rt_flags:
        print(flag)
        ireert.flags.parse_flags(flag)

    if "rocm" in device:
        device = "rocm"
    with DetailLogger(timeout=2.5) as dl:
        # First get configs.
        if device_idx is not None:
            dl.log(f"Mapping device id: {device_idx}")
            device = iree_device_map(device)
            haldriver = ireert.get_driver(device)
            dl.log(f"ireert.get_driver()")

            haldevice = haldriver.create_device(
                haldriver.query_available_devices()[device_idx]["device_id"],
                allocators=shark_args.device_allocator,
            )
            dl.log(f"ireert.create_device()")
            config = ireert.Config(device=haldevice)
            config.id = haldriver.query_available_devices()[device_idx][
                "device_id"
            ]
            dl.log(f"ireert.Config()")
        else:
            config = get_iree_runtime_config(device)
            dl.log("get_iree_runtime_config")
        if "task" in device:
            print(
                f"[DEBUG] setting iree runtime flags for cpu:\n{' '.join(get_iree_cpu_rt_args())}"
            )
            for flag in get_iree_cpu_rt_args():
                ireert.flags.parse_flags(flag)

        # Now load vmfb.
        # Two scenarios we have here :-
        #      1. We either have the vmfb already saved and therefore pass the path of it.
        #         (This would arise if we're invoking `load_module` from a SharkInference obj)
        #   OR 2. We are compiling on the fly, therefore we have the flatbuffer blob to play with.
        #         (This would arise if we're invoking `compile` from a SharkInference obj)
        temp_file_to_unlink = None
        if isinstance(flatbuffer_blob_or_path, Path):
            flatbuffer_blob_or_path = flatbuffer_blob_or_path.__str__()
        if (
            isinstance(flatbuffer_blob_or_path, str)
            and ".vmfb" in flatbuffer_blob_or_path
        ):
            vmfb_file_path = flatbuffer_blob_or_path
            mmaped_vmfb = ireert.VmModule.mmap(
                config.vm_instance, flatbuffer_blob_or_path
            )
            dl.log(f"mmap {flatbuffer_blob_or_path}")
            ctx = ireert.SystemContext(config=config)
            for flag in shark_args.additional_runtime_args:
                ireert.flags.parse_flags(flag)
            dl.log(f"ireert.SystemContext created")
            if "vulkan" in device:
                # Vulkan pipeline creation consumes significant amount of time.
                print(
                    "\tCompiling Vulkan shaders. This may take a few minutes."
                )
            ctx.add_vm_module(mmaped_vmfb)
            dl.log(f"module initialized")
            mmaped_vmfb = getattr(ctx.modules, mmaped_vmfb.name)
        else:
            with tempfile.NamedTemporaryFile(delete=False) as tf:
                tf.write(flatbuffer_blob_or_path)
                tf.flush()
                vmfb_file_path = tf.name
            temp_file_to_unlink = vmfb_file_path
            mmaped_vmfb = ireert.VmModule.mmap(instance, vmfb_file_path)
            dl.log(f"mmap temp {vmfb_file_path}")
        return mmaped_vmfb, config, temp_file_to_unlink


def get_iree_compiled_module(
    module,
    device: str,
    frontend: str = "torch",
    model_config_path: str = None,
    extra_args: list = [],
    rt_flags: list = [],
    device_idx: int = None,
    mmap: bool = False,
    debug: bool = False,
    compile_str: bool = False,
):
    """Given a module returns the compiled .vmfb and configs"""
    flatbuffer_blob = compile_module_to_flatbuffer(
        module=module,
        device=device,
        frontend=frontend,
        model_config_path=model_config_path,
        extra_args=extra_args,
        debug=debug,
        compile_str=compile_str,
    )
    temp_file_to_unlink = None
    # TODO: Currently mmap=True control flow path has been switched off for mmap.
    #       Got to find a cleaner way to unlink/delete the temporary file since
    #       we're setting delete=False when creating NamedTemporaryFile. That's why
    #       I'm getting hold of the name of the temporary file in `temp_file_to_unlink`.
    if mmap:
        vmfb, config, temp_file_to_unlink = load_vmfb_using_mmap(
            flatbuffer_blob, device, device_idx, rt_flags
        )
    else:
        vmfb, config = get_iree_module(
            flatbuffer_blob,
            device,
            device_idx=device_idx,
            rt_flags=rt_flags,
        )
    ret_params = {
        "vmfb": vmfb,
        "config": config,
        "temp_file_to_unlink": temp_file_to_unlink,
    }
    return ret_params


def load_flatbuffer(
    flatbuffer_path: str,
    device: str,
    device_idx: int = None,
    mmap: bool = False,
    rt_flags: list = [],
):
    temp_file_to_unlink = None
    if mmap:
        vmfb, config, temp_file_to_unlink = load_vmfb_using_mmap(
            flatbuffer_path, device, device_idx, rt_flags
        )
    else:
        with open(os.path.join(flatbuffer_path), "rb") as f:
            flatbuffer_blob = f.read()
        vmfb, config = get_iree_module(
            flatbuffer_blob,
            device,
            device_idx=device_idx,
            rt_flags=rt_flags,
        )
    ret_params = {
        "vmfb": vmfb,
        "config": config,
        "temp_file_to_unlink": temp_file_to_unlink,
    }
    return ret_params


def export_iree_module_to_vmfb(
    module,
    device: str,
    directory: str,
    mlir_dialect: str = "linalg",
    model_config_path: str = None,
    module_name: str = None,
    extra_args: list = [],
    debug: bool = False,
    compile_str: bool = False,
):
    # Compiles the module given specs and saves it as .vmfb file.
    flatbuffer_blob = compile_module_to_flatbuffer(
        module=module,
        device=device,
        frontend=mlir_dialect,
        model_config_path=model_config_path,
        extra_args=extra_args,
        debug=debug,
        compile_str=compile_str,
    )
    if module_name is None:
        device_name = (
            device if "://" not in device else "-".join(device.split("://"))
        )
        module_name = f"{mlir_dialect}_{device_name}"
    filename = os.path.join(directory, module_name + ".vmfb")
    with open(filename, "wb") as f:
        f.write(flatbuffer_blob)
    print(f"Saved vmfb in {filename}.")
    return filename


def export_module_to_mlir_file(module, frontend, directory: str):
    # TODO: write proper documentation.
    mlir_str = module
    if frontend in ["tensorflow", "tf", "mhlo", "stablehlo", "tflite"]:
        mlir_str = module.decode("utf-8")
    elif frontend in ["pytorch", "torch"]:
        mlir_str = module.operation.get_asm()
    filename = os.path.join(directory, "model.mlir")
    with open(filename, "w") as f:
        f.write(mlir_str)
    print(f"Saved mlir in {filename}.")
    return filename


def get_results(
    compiled_vm,
    function_name,
    input,
    config,
    frontend="torch",
    send_to_host=True,
    debug_timeout: float = 5.0,
    device: str = None,
):
    """Runs a .vmfb file given inputs and config and returns output."""
    with DetailLogger(debug_timeout) as dl:
        device_inputs = []
        if device == "rocm" and hasattr(config, "id"):
            haldriver = ireert.get_driver("rocm")
            haldevice = haldriver.create_device(
                config.id,
                allocators=shark_args.device_allocator,
            )
        for input_array in input:
            dl.log(f"Load to device: {input_array.shape}")
            device_inputs.append(
                ireert.asdevicearray(config.device, input_array)
            )
        dl.log(f"Invoke function: {function_name}")
        result = compiled_vm[function_name](*device_inputs)
        dl.log(f"Invoke complete")
        result_tensors = []
        if isinstance(result, tuple):
            if send_to_host:
                for val in result:
                    dl.log(f"Result to host: {val.shape}")
                    result_tensors.append(np.asarray(val, val.dtype))
            else:
                for val in result:
                    result_tensors.append(val)
            return result_tensors
        elif isinstance(result, dict):
            data = list(result.items())
            if send_to_host:
                res = np.array(data, dtype=object)
                return np.copy(res)
            return data
        else:
            if send_to_host and result is not None:
                dl.log("Result to host")
                return result.to_host()
            return result
        dl.log("Execution complete")


@functools.cache
def get_iree_runtime_config(device):
    device = iree_device_map(device)
    haldriver = ireert.get_driver(device)
    if "metal" in device and shark_args.device_allocator == "caching":
        print(
            "[WARNING] metal devices can not have a `caching` allocator."
            "\nUsing default allocator `None`"
        )
    haldevice = haldriver.create_device_by_uri(
        device,
        # metal devices have a failure with caching allocators atm. blcking this util it gets fixed upstream.
        allocators=shark_args.device_allocator
        if "metal" not in device
        else None,
    )
    config = ireert.Config(device=haldevice)
    return config

```

`shark/iree_utils/cpu_utils.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# All the iree_cpu related functionalities go here.

import functools
import subprocess
import platform
from shark.parser import shark_args


def get_cpu_count():
    import multiprocessing

    try:
        cpu_count = multiprocessing.cpu_count()
        return cpu_count
    except NotImplementedError:
        return None


# Get the default cpu args.
@functools.cache
def get_iree_cpu_args():
    uname = platform.uname()
    os_name, proc_name = uname.system, uname.machine

    if os_name == "Darwin":
        kernel_version = uname.release
        target_triple = f"{proc_name}-apple-darwin{kernel_version}"
    elif os_name == "Linux":
        target_triple = f"{proc_name}-linux-gnu"
    elif os_name == "Windows":
        target_triple = "x86_64-pc-windows-msvc"
    else:
        error_message = f"OS Type f{os_name} not supported and triple can't be determined, open issue to dSHARK team please :)"
        raise Exception(error_message)
    print(f"Target triple found:{target_triple}")
    return [
        f"--iree-llvmcpu-target-triple={target_triple}",
    ]


# Get iree runtime flags for cpu
@functools.cache
def get_iree_cpu_rt_args():
    default = get_cpu_count()
    default = default if default <= 8 else default - 2
    cpu_count = (
        default
        if shark_args.task_topology_max_group_count is None
        else shark_args.task_topology_max_group_count
    )
    return [f"--task_topology_max_group_count={cpu_count}"]

```

`shark/iree_utils/gpu_utils.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# All the iree_gpu related functionalities go here.

import functools
import iree.runtime as ireert
import ctypes
import sys
from subprocess import CalledProcessError
from shark.parser import shark_args
from shark.iree_utils._common import run_cmd

# TODO: refactor to rocm and cuda utils


# Get the default gpu args given the architecture.
@functools.cache
def get_iree_gpu_args():
    ireert.flags.FUNCTION_INPUT_VALIDATION = False
    ireert.flags.parse_flags("--cuda_allow_inline_execution")
    # TODO: Give the user_interface to pass the sm_arch.
    sm_arch = get_cuda_sm_cc()
    if (
        sm_arch
        in ["sm_70", "sm_72", "sm_75", "sm_80", "sm_84", "sm_86", "sm_89"]
    ) and (shark_args.enable_tf32 == True):
        return [
            f"--iree-hal-cuda-llvm-target-arch={sm_arch}",
        ]
    else:
        return []


def check_rocm_device_arch_in_args(extra_args):
    # Check if the target arch flag for rocm device present in extra_args
    for flag in extra_args:
        if "iree-rocm-target-chip" in flag:
            flag_arch = flag.split("=")[1]
            return flag_arch
    return None


def get_rocm_device_arch(device_num=0, extra_args=[]):
    # ROCM Device Arch selection:
    # 1 : User given device arch using `--iree-rocm-target-chip` flag
    # 2 : Device arch from `iree-run-module --dump_devices=rocm` for device on index <device_num>
    # 3 : default arch : gfx1100

    arch_in_flag = check_rocm_device_arch_in_args(extra_args)
    if arch_in_flag is not None:
        print(
            f"User Specified rocm target device arch from flag : {arch_in_flag} will be used"
        )
        return arch_in_flag

    arch_in_device_dump = None

    # get rocm arch from iree dump devices
    def get_devices_info_from_dump(dump):
        from os import linesep

        dump_clean = list(
            filter(
                lambda s: "--device=rocm" in s or "gpu-arch-name:" in s,
                dump.split(linesep),
            )
        )
        arch_pairs = [
            (
                dump_clean[i].split("=")[1].strip(),
                dump_clean[i + 1].split(":")[1].strip(),
            )
            for i in range(0, len(dump_clean), 2)
        ]
        return arch_pairs

    dump_device_info = None
    try:
        dump_device_info = run_cmd(
            "iree-run-module --dump_devices=rocm", raise_err=True
        )
    except Exception as e:
        print("could not execute `iree-run-module --dump_devices=rocm`")

    if dump_device_info is not None:
        device_num = 0 if device_num is None else device_num
        device_arch_pairs = get_devices_info_from_dump(dump_device_info[0])
        if len(device_arch_pairs) > device_num:  # can find arch in the list
            arch_in_device_dump = device_arch_pairs[device_num][1]

    if arch_in_device_dump is not None:
        print(f"Found ROCm device arch : {arch_in_device_dump}")
        return arch_in_device_dump

    default_rocm_arch = "gfx1100"
    print(
        "Did not find ROCm architecture from `--iree-rocm-target-chip` flag"
        "\n or from `iree-run-module --dump_devices=rocm` command."
        f"\nUsing {default_rocm_arch} as ROCm arch for compilation."
    )
    return default_rocm_arch


# Get the default gpu args given the architecture.
def get_iree_rocm_args(device_num=0, extra_args=[]):
    ireert.flags.FUNCTION_INPUT_VALIDATION = False
    rocm_flags = ["--iree-rocm-link-bc=true"]

    if check_rocm_device_arch_in_args(extra_args) is None:
        rocm_arch = get_rocm_device_arch(device_num, extra_args)
        rocm_flags.append(f"--iree-rocm-target-chip={rocm_arch}")

    return rocm_flags


# Some constants taken from cuda.h
CUDA_SUCCESS = 0
CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT = 16
CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR = 39
CU_DEVICE_ATTRIBUTE_CLOCK_RATE = 13
CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE = 36


@functools.cache
def get_cuda_sm_cc():
    libnames = ("libcuda.so", "libcuda.dylib", "nvcuda.dll")
    for libname in libnames:
        try:
            cuda = ctypes.CDLL(libname)
        except OSError:
            continue
        else:
            break
    else:
        raise OSError("could not load any of: " + " ".join(libnames))

    nGpus = ctypes.c_int()
    name = b" " * 100
    cc_major = ctypes.c_int()
    cc_minor = ctypes.c_int()

    result = ctypes.c_int()
    device = ctypes.c_int()
    context = ctypes.c_void_p()
    error_str = ctypes.c_char_p()

    result = cuda.cuInit(0)
    if result != CUDA_SUCCESS:
        cuda.cuGetErrorString(result, ctypes.byref(error_str))
        print(
            "cuInit failed with error code %d: %s"
            % (result, error_str.value.decode())
        )
        return 1
    result = cuda.cuDeviceGetCount(ctypes.byref(nGpus))
    if result != CUDA_SUCCESS:
        cuda.cuGetErrorString(result, ctypes.byref(error_str))
        print(
            "cuDeviceGetCount failed with error code %d: %s"
            % (result, error_str.value.decode())
        )
        return 1
    print("Found %d device(s)." % nGpus.value)
    for i in range(nGpus.value):
        result = cuda.cuDeviceGet(ctypes.byref(device), i)
        if result != CUDA_SUCCESS:
            cuda.cuGetErrorString(result, ctypes.byref(error_str))
            print(
                "cuDeviceGet failed with error code %d: %s"
                % (result, error_str.value.decode())
            )
            return 1
        print("Device: %d" % i)
        if (
            cuda.cuDeviceGetName(ctypes.c_char_p(name), len(name), device)
            == CUDA_SUCCESS
        ):
            print("  Name: %s" % (name.split(b"\0", 1)[0].decode()))
        if (
            cuda.cuDeviceComputeCapability(
                ctypes.byref(cc_major), ctypes.byref(cc_minor), device
            )
            == CUDA_SUCCESS
        ):
            print(
                "  Compute Capability: %d.%d"
                % (cc_major.value, cc_minor.value)
            )
    sm = f"sm_{cc_major.value}{cc_minor.value}"
    return sm

```

`shark/iree_utils/metal_utils.py`:

```py
# Copyright 2023 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# All the iree_vulkan related functionalities go here.

import functools

from shark.iree_utils._common import run_cmd
import iree.runtime as ireert
from sys import platform
from shark.iree_utils.vulkan_target_env_utils import get_vulkan_target_env_flag


@functools.cache
def get_metal_device_name(device_num=0):
    iree_device_dump = run_cmd("iree-run-module --dump_devices")
    iree_device_dump = iree_device_dump[0].split("\n\n")
    metal_device_list = [
        s.split("\n#")[2] for s in iree_device_dump if "--device=metal" in s
    ]
    if len(metal_device_list) == 0:
        raise ValueError("No device name found in device dump!")
    if len(metal_device_list) > 1:
        print("Following devices found:")
        for i, dname in enumerate(metal_device_list):
            print(f"{i}. {dname}")
        print(f"Choosing device: {metal_device_list[device_num]}")
    return metal_device_list[device_num]


def get_os_name():
    if platform.startswith("linux"):
        return "linux"
    elif platform == "darwin":
        return "macos"
    elif platform == "win32":
        return "windows"
    else:
        print("Cannot detect OS type, defaulting to linux.")
        return "linux"


def get_metal_target_triple(device_name):
    """This method provides a target triple str for specified vulkan device.

    Args:
        device_name (str): name of the hardware device to be used with vulkan

    Returns:
        str or None: target triple or None if no match found for given name
    """
    return "macos"


def get_metal_triple_flag(device_name="", device_num=0, extra_args=[]):
    for flag in extra_args:
        if "-iree-metal-target-platform=" in flag:
            print(f"Using target triple {flag.split('=')[1]}")
            return None

    if device_name == "" or device_name == [] or device_name is None:
        metal_device = get_metal_device_name(device_num=device_num)
    else:
        metal_device = device_name
    triple = get_metal_target_triple(metal_device)
    if triple is not None:
        print(
            f"Found metal device {metal_device}. Using metal target platform {triple}"
        )
        return f"-iree-metal-target-platform={triple}"
    print(
        """Optimized kernel for your target device is not added yet.
        Contact SHARK Admin on discord[https://discord.com/invite/RUqY2h2s9u]
        or pull up an issue."""
    )
    print(f"Target : {metal_device}")
    return None


def get_iree_metal_args(device_num=0, extra_args=[]):
    # Add any metal spefic compilation flags here
    res_metal_flag = []
    if len(extra_args) > 0:
        res_metal_flag.extend(extra_args)
    return res_metal_flag


def set_iree_metal_runtime_flags(flags):
    for flag in flags:
        ireert.flags.parse_flags(flag)
    return

```

`shark/iree_utils/trace.py`:

```py
# Copyright 2023 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import List, Tuple

import os
import threading
import time


def _enable_detail_trace() -> bool:
    return os.getenv("SHARK_DETAIL_TRACE", "0") == "1"


class DetailLogger:
    """Context manager which can accumulate detailed log messages.

    Detailed log is only emitted if the operation takes a long time
    or errors.
    """

    def __init__(self, timeout: float):
        self._timeout = timeout
        self._messages: List[Tuple[float, str]] = []
        self._start_time = time.time()
        self._active = not _enable_detail_trace()
        self._lock = threading.RLock()
        self._cond = threading.Condition(self._lock)
        self._thread = None

    def __enter__(self):
        self._thread = threading.Thread(target=self._run)
        self._thread.start()
        return self

    def __exit__(self, type, value, traceback):
        with self._lock:
            self._active = False
            self._cond.notify()
        if traceback:
            self.dump_on_error(f"exception")

    def _run(self):
        with self._lock:
            timed_out = not self._cond.wait(self._timeout)
        if timed_out:
            self.dump_on_error(f"took longer than {self._timeout}s")

    def log(self, msg):
        with self._lock:
            timestamp = time.time()
            if self._active:
                self._messages.append((timestamp, msg))
            else:
                print(f"  +{(timestamp - self._start_time) * 1000}ms: {msg}")

    def dump_on_error(self, summary: str):
        with self._lock:
            if self._active:
                print(f"::: Detailed report ({summary}):")
                for timestamp, msg in self._messages:
                    print(
                        f"  +{(timestamp - self._start_time) * 1000}ms: {msg}"
                    )
            self._active = False

```

`shark/iree_utils/vulkan_target_env_utils.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections import OrderedDict
import functools


@functools.cache
def get_vulkan_target_env(vulkan_target_triple):
    arch, product, os = vulkan_target_triple.split("=")[1].split("-")
    triple = (arch, product, os)
    # get version
    version = get_version(triple=triple)
    # TODO get revision
    revision = 120

    # extensions
    extensions = get_extensions(triple)
    # get vendor
    vendor = get_vendor(triple)
    # get device type
    device_type = get_device_type(triple)
    # get capabilities
    capabilities = get_vulkan_target_capabilities(triple)
    target_env = f"#vk.target_env<{version}, r({revision}), {extensions}, {vendor}:{device_type}, #vk.caps< {capabilities} >>"
    return target_env


def get_vulkan_target_env_flag(vulkan_target_triple):
    target_env = get_vulkan_target_env(vulkan_target_triple)
    target_env_flag = f"--iree-vulkan-target-env={target_env}"
    return target_env_flag


def get_version(triple):
    arch, product, os = triple
    if os in ["android30", "android31"]:
        return "v1.1"
    if product in ["android30", "android31"]:
        return "v1.1"
    if arch in ["unknown"]:
        return "v1.1"
    return "v1.3"


@functools.cache
def get_extensions(triple):
    def make_ext_list(ext_list):
        res = ", ".join(ext_list)
        return f"[{res}]"

    arch, product, os = triple
    if arch == "m1":
        ext = [
            "VK_KHR_16bit_storage",
            "VK_KHR_8bit_storage",
            "VK_KHR_shader_float16_int8",
            "VK_KHR_storage_buffer_storage_class",
            "VK_KHR_variable_pointers",
        ]
        return make_ext_list(ext_list=ext)

    if arch == "valhall":
        ext = [
            "VK_KHR_16bit_storage",
            "VK_KHR_8bit_storage",
            "VK_KHR_shader_float16_int8",
            "VK_KHR_spirv_1_4",
            "VK_KHR_storage_buffer_storage_class",
            "VK_KHR_variable_pointers",
        ]
        return make_ext_list(ext_list=ext)

    if arch == "adreno":
        ext = [
            "VK_KHR_16bit_storage",
            "VK_KHR_shader_float16_int8",
            "VK_KHR_spirv_1_4",
            "VK_KHR_storage_buffer_storage_class",
            "VK_KHR_variable_pointers",
        ]
        if os == "android31":
            ext.append("VK_KHR_8bit_storage")
        return make_ext_list(ext_list=ext)

    if get_vendor(triple) == "SwiftShader":
        ext = ["VK_KHR_storage_buffer_storage_class"]
        return make_ext_list(ext_list=ext)

    if arch == "unknown":
        ext = [
            "VK_KHR_storage_buffer_storage_class",
            "VK_KHR_variable_pointers",
        ]
        return make_ext_list(ext_list=ext)

    ext = [
        "VK_KHR_16bit_storage",
        "VK_KHR_8bit_storage",
        "VK_KHR_shader_float16_int8",
        "VK_KHR_spirv_1_4",
        "VK_KHR_storage_buffer_storage_class",
        "VK_KHR_variable_pointers",
        "VK_EXT_subgroup_size_control",
    ]

    if get_vendor(triple) == "NVIDIA" or arch == "rdna3":
        ext.append("VK_KHR_cooperative_matrix")
    if get_vendor(triple) == ["NVIDIA", "AMD", "Intel"]:
        ext.append("VK_KHR_shader_integer_dot_product")
    return make_ext_list(ext_list=ext)


@functools.cache
def get_vendor(triple):
    arch, product, os = triple
    if arch == "unknown":
        return "Unknown"
    if arch in ["rdna1", "rdna2", "rdna3", "rgcn3", "rgcn4", "rgcn5"]:
        return "AMD"
    if arch == "valhall":
        return "ARM"
    if arch == "m1":
        return "Apple"
    if arch in ["arc", "UHD"]:
        return "Intel"
    if arch in ["turing", "ampere", "pascal"]:
        return "NVIDIA"
    if arch == "adreno":
        return "Qualcomm"
    if arch == "cpu":
        if product == "swiftshader":
            return "SwiftShader"
        return "Unknown"
    print(f"Vendor for target triple - {triple} not found. Using unknown")
    return "Unknown"


@functools.cache
def get_device_type(triple):
    arch, product, _ = triple
    if arch == "unknown":
        return "Unknown"
    if arch == "cpu":
        return "CPU"
    if arch in ["turing", "ampere", "arc", "pascal"]:
        return "DiscreteGPU"
    if arch in ["rdna1", "rdna2", "rdna3", "rgcn3", "rgcn5"]:
        if product == "ivega10":
            return "IntegratedGPU"
        return "DiscreteGPU"
    if arch in ["m1", "valhall", "adreno"]:
        return "IntegratedGPU"
    print(f"Device type for target triple - {triple} not found. Using unknown")
    return "Unknown"


# get all the capabilities for the device
# TODO: make a dataclass for capabilites and init using vulkaninfo
@functools.cache
def get_vulkan_target_capabilities(triple):
    def get_subgroup_val(l):
        return int(sum([subgroup_feature[sgf] for sgf in l]))

    cap = OrderedDict()
    arch, product, os = triple
    subgroup_feature = {
        "Basic": 1,
        "Vote": 2,
        "Arithmetic": 4,
        "Ballot": 8,
        "Shuffle": 16,
        "ShuffleRelative": 32,
        "Clustered": 64,
        "Quad": 128,
        "PartitionedNV": 256,
    }
    cap["maxComputeSharedMemorySize"] = 16384
    cap["maxComputeWorkGroupInvocations"] = 128
    cap["maxComputeWorkGroupSize"] = [128, 128, 64]
    cap["subgroupSize"] = 32
    cap["subgroupFeatures"] = ["Basic"]
    cap["minSubgroupSize"] = None
    cap["maxSubgroupSize"] = None
    cap["shaderFloat16"] = False
    cap["shaderFloat64"] = False
    cap["shaderInt8"] = False
    cap["shaderInt16"] = False
    cap["shaderInt64"] = False
    cap["storageBuffer16BitAccess"] = False
    cap["storagePushConstant16"] = False
    cap["uniformAndStorageBuffer16BitAccess"] = False
    cap["storageBuffer8BitAccess"] = False
    cap["storagePushConstant8"] = False
    cap["uniformAndStorageBuffer8BitAccess"] = False
    cap["variablePointers"] = False
    cap["variablePointersStorageBuffer"] = False
    cap["coopmatCases"] = None

    if arch in ["rdna1", "rdna2", "rdna3"]:
        cap["maxComputeSharedMemorySize"] = 65536
        cap["maxComputeWorkGroupInvocations"] = 1024
        cap["maxComputeWorkGroupSize"] = [1024, 1024, 1024]

        cap["subgroupSize"] = 64
        cap["minSubgroupSize"] = 32
        cap["maxSubgroupSize"] = 64
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Shuffle",
            "ShuffleRelative",
            "Clustered",
            "Quad",
        ]

        cap["shaderFloat16"] = True
        cap["shaderFloat64"] = True
        cap["shaderInt8"] = True
        cap["shaderInt16"] = True
        cap["shaderInt64"] = True
        cap["shaderIntegerDotProduct"] = True
        cap["storageBuffer16BitAccess"] = True
        cap["storagePushConstant16"] = True
        cap["uniformAndStorageBuffer16BitAccess"] = True
        cap["storageBuffer8BitAccess"] = True
        cap["storagePushConstant8"] = True
        cap["uniformAndStorageBuffer8BitAccess"] = True
        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True
        if arch == "rdna3":
            # TODO: Get scope value
            cap["coopmatCases"] = [
                "mSize = 16, nSize = 16, kSize = 16, aType = f16, bType = f16, cType = f16, resultType = f16, accSat = false, scope = #vk.scope<Subgroup>"
            ]

        if product == "rx5700xt":
            cap["storagePushConstant16"] = False
            cap["storagePushConstant8"] = False

    elif arch in ["rgcn5", "rgcn4", "rgcn3"]:
        cap["maxComputeSharedMemorySize"] = 65536
        cap["maxComputeWorkGroupInvocations"] = 1024
        cap["maxComputeWorkGroupSize"] = [1024, 1024, 1024]

        cap["subgroupSize"] = 64
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Shuffle",
            "ShuffleRelative",
            "Clustered",
            "Quad",
        ]
        cap["minSubgroupSize"] = 64
        cap["maxSubgroupSize"] = 64

        if arch == "rgcn5":
            cap["shaderFloat16"] = True
            cap["shaderFloat64"] = True

            cap["storageBuffer16BitAccess"] = True

        cap["shaderInt8"] = True
        cap["shaderInt16"] = True
        cap["shaderInt64"] = True
        cap["shaderIntegerDotProduct"] = True
        cap["storagePushConstant16"] = False
        cap["uniformAndStorageBuffer16BitAccess"] = True
        cap["storageBuffer8BitAccess"] = True
        cap["storagePushConstant8"] = False
        cap["uniformAndStorageBuffer8BitAccess"] = True

        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True

    elif arch == "m1":
        cap["maxComputeSharedMemorySize"] = 32768
        cap["maxComputeWorkGroupInvocations"] = 1024
        cap["maxComputeWorkGroupSize"] = [1024, 1024, 1024]

        cap["subgroupSize"] = 32
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Shuffle",
            "ShuffleRelative",
            "Quad",
        ]

        cap["shaderFloat16"] = True
        cap["shaderFloat64"] = True
        cap["shaderInt8"] = True
        cap["shaderInt16"] = True
        cap["shaderInt64"] = True
        cap["shaderIntegerDotProduct"] = False
        cap["storageBuffer16BitAccess"] = True
        cap["storagePushConstant16"] = True
        cap["uniformAndStorageBuffer16BitAccess"] = True
        cap["storageBuffer8BitAccess"] = True
        cap["storagePushConstant8"] = True
        cap["uniformAndStorageBuffer8BitAccess"] = True
        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True

    elif arch == "valhall":
        cap["maxComputeSharedMemorySize"] = 32768
        cap["maxComputeWorkGroupInvocations"] = 512
        cap["maxComputeWorkGroupSize"] = [512, 512, 512]

        cap["subgroupSize"] = 16
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Clustered",
            "Quad",
        ]

        if os == "android31":
            cap["subgroupFeatures"].append("Shuffle")
            cap["subgroupFeatures"].append("ShuffleRelative")

        cap["shaderFloat16"] = True
        cap["shaderInt8"] = True
        cap["shaderInt16"] = True
        cap["storageBuffer16BitAccess"] = True
        cap["storagePushConstant16"] = True
        cap["uniformAndStorageBuffer16BitAccess"] = True
        cap["storageBuffer8BitAccess"] = True
        cap["storagePushConstant8"] = True
        cap["uniformAndStorageBuffer8BitAccess"] = True
        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True

    elif arch == "arc":
        cap["maxComputeSharedMemorySize"] = 32768
        cap["maxComputeWorkGroupInvocations"] = 1024
        cap["maxComputeWorkGroupSize"] = [1024, 1024, 64]

        cap["subgroupSize"] = 32
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Shuffle",
            "ShuffleRelative",
            "Clustered",
            "Quad",
        ]

        cap["shaderFloat16"] = True
        cap["shaderFloat64"] = False
        cap["shaderInt8"] = True
        cap["shaderInt16"] = True
        cap["shaderInt64"] = False
        cap["shaderIntegerDotProduct"] = True
        cap["storageBuffer16BitAccess"] = True
        cap["storagePushConstant16"] = True
        cap["uniformAndStorageBuffer16BitAccess"] = True
        cap["storageBuffer8BitAccess"] = True
        cap["storagePushConstant8"] = True
        cap["uniformAndStorageBuffer8BitAccess"] = True
        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True

    elif arch == "cpu":
        if product == "swiftshader":
            cap["maxComputeSharedMemorySize"] = 16384
            cap["subgroupSize"] = 4
            cap["subgroupFeatures"] = [
                "Basic",
                "Vote",
                "Arithmetic",
                "Ballot",
                "Shuffle",
                "ShuffleRelative",
            ]

    elif arch in ["pascal"]:
        cap["maxComputeSharedMemorySize"] = 49152
        cap["maxComputeWorkGroupInvocations"] = 1536
        cap["maxComputeWorkGroupSize"] = [1536, 1024, 64]

        cap["subgroupSize"] = 32
        cap["minSubgroupSize"] = 32
        cap["maxSubgroupSize"] = 32
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Shuffle",
            "ShuffleRelative",
            "Clustered",
            "Quad",
        ]

        cap["shaderFloat16"] = False
        cap["shaderFloat64"] = True
        cap["shaderInt8"] = True
        cap["shaderInt16"] = True
        cap["shaderInt64"] = True
        cap["shaderIntegerDotProduct"] = True
        cap["storageBuffer16BitAccess"] = True
        cap["storagePushConstant16"] = True
        cap["uniformAndStorageBuffer16BitAccess"] = True
        cap["storageBuffer8BitAccess"] = True
        cap["storagePushConstant8"] = True
        cap["uniformAndStorageBuffer8BitAccess"] = True
        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True

    elif arch in ["ampere", "turing"]:
        cap["maxComputeSharedMemorySize"] = 49152
        cap["maxComputeWorkGroupInvocations"] = 1024
        cap["maxComputeWorkGroupSize"] = [1024, 1024, 1024]

        cap["subgroupSize"] = 32
        cap["minSubgroupSize"] = 32
        cap["maxSubgroupSize"] = 32
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Shuffle",
            "ShuffleRelative",
            "Clustered",
            "Quad",
        ]

        cap["shaderFloat16"] = True
        cap["shaderFloat64"] = True
        cap["shaderInt8"] = True
        cap["shaderInt16"] = True
        cap["shaderInt64"] = True
        cap["shaderIntegerDotProduct"] = True
        cap["storageBuffer16BitAccess"] = True
        cap["storagePushConstant16"] = True
        cap["uniformAndStorageBuffer16BitAccess"] = True
        cap["storageBuffer8BitAccess"] = True
        cap["storagePushConstant8"] = True
        cap["uniformAndStorageBuffer8BitAccess"] = True
        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True

        cap["coopmatCases"] = [
            "mSize = 8, nSize = 8, kSize = 32, aType = i8, bType = i8, cType = i32, resultType = i32, accSat = false, scope = #vk.scope<Subgroup>",
            "mSize = 16, nSize = 16, kSize = 16, aType = f16, bType = f16, cType = f16, resultType = f16, accSat = false, scope = #vk.scope<Subgroup>",
            "mSize = 16, nSize = 16, kSize = 16, aType = f16, bType = f16, cType = f32, resultType = f32, accSat = false, scope = #vk.scope<Subgroup>",
        ]

    elif arch == "adreno":
        cap["maxComputeSharedMemorySize"] = 32768
        cap["maxComputeWorkGroupInvocations"] = 1024
        cap["maxComputeWorkGroupSize"] = [1024, 1024, 64]

        cap["subgroupSize"] = 64
        cap["subgroupFeatures"] = [
            "Basic",
            "Vote",
            "Arithmetic",
            "Ballot",
            "Shuffle",
            "ShuffleRelative",
            "Quad",
        ]

        cap["shaderFloat16"] = True
        cap["shaderInt8"] = True
        cap["shaderInt16"] = True

        cap["storageBuffer16BitAccess"] = True
        if os == "andorid31":
            cap["uniformAndStorageBuffer8BitAccess"] = True

        cap["variablePointers"] = True
        cap["variablePointersStorageBuffer"] = True

    elif arch == "unknown":
        cap["subgroupSize"] = 64
        cap["variablePointers"] = False
        cap["variablePointersStorageBuffer"] = False
    else:
        print(
            f"Architecture {arch} not matched. Using default vulkan target device capability"
        )

    def get_comma_sep_str(ele_list):
        l = ""
        for ele in ele_list:
            l += f"{ele}, "
        l = f"[{l[:-2]}]"
        return l

    res = ""
    for k, v in cap.items():
        if v is None or v == False:
            continue
        if isinstance(v, bool):
            res += f"{k} = {'unit' if v == True else None}, "
        elif isinstance(v, list):
            if k == "subgroupFeatures":
                res += f"subgroupFeatures = {get_subgroup_val(v)}: i32, "
            elif k == "maxComputeWorkGroupSize":
                res += f"maxComputeWorkGroupSize = dense<{get_comma_sep_str(v)}>: vector<{len(v)}xi32>, "
            elif k == "coopmatCases":
                cmc = ""
                for case in v:
                    cmc += f"#vk.coop_matrix_props<{case}>, "
                res += f"cooperativeMatrixPropertiesKHR = [{cmc[:-2]}], "
            else:
                res += f"{k} = {get_comma_sep_str(v)}, "
        else:
            res += f"{k} = {v}, "
    res = res[:-2]
    return res

```

`shark/iree_utils/vulkan_utils.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# All the iree_vulkan related functionalities go here.

import functools
from os import linesep
from shark.iree_utils._common import run_cmd
import iree.runtime as ireert
from sys import platform
from shark.iree_utils.vulkan_target_env_utils import get_vulkan_target_env_flag
from shark.parser import shark_args


@functools.cache
def get_all_vulkan_devices():
    from iree.runtime import get_driver

    try:
        driver = get_driver("vulkan")
        device_list_src = driver.query_available_devices()
    except:
        device_list_src = {}

    return [d["name"] for d in device_list_src]


@functools.cache
def get_vulkan_device_name(device_num=0):
    if isinstance(device_num, int):
        vulkaninfo_list = get_all_vulkan_devices()

        if len(vulkaninfo_list) == 0:
            raise ValueError("No device name found in VulkanInfo!")
        if len(vulkaninfo_list) > 1:
            print("Following devices found:")
            for i, dname in enumerate(vulkaninfo_list):
                print(f"{i}. {dname}")
            print(f"Choosing device: vulkan://{device_num}")
        vulkan_device_name = vulkaninfo_list[device_num]
    else:
        from iree.runtime import get_driver

        vulkan_device_driver = get_driver(device_num)
        vulkan_device_name = vulkan_device_driver.query_available_devices()[0]
        print(vulkan_device_name)
    return vulkan_device_name


def get_os_name():
    if platform.startswith("linux"):
        return "linux"
    elif platform == "darwin":
        return "macos"
    elif platform == "win32":
        return "windows"
    else:
        print("Cannot detect OS type, defaulting to linux.")
        return "linux"


@functools.cache
def get_vulkan_target_triple(device_name):
    """This method provides a target triple str for specified vulkan device.

    Args:
        device_name (str): name of the hardware device to be used with vulkan

    Returns:
        str or None: target triple or None if no match found for given name
    """

    # TODO: Replace this with a dict or something smarter.
    system_os = get_os_name()
    # Apple Targets
    if all(x in device_name for x in ("Apple", "M1")):
        triple = "m1-moltenvk-macos"
    elif all(x in device_name for x in ("Apple", "M2")):
        triple = "m1-moltenvk-macos"

    # Nvidia Targets
    elif all(x in device_name for x in ("RTX", "2080")):
        triple = f"turing-rtx2080-{system_os}"
    elif all(x in device_name for x in ("A100", "SXM4")):
        triple = f"ampere-a100-{system_os}"
    elif all(x in device_name for x in ("RTX", "3090")):
        triple = f"ampere-rtx3090-{system_os}"
    elif all(x in device_name for x in ("RTX", "3080")):
        triple = f"ampere-rtx3080-{system_os}"
    elif all(x in device_name for x in ("RTX", "3070")):
        triple = f"ampere-rtx3070-{system_os}"
    elif all(x in device_name for x in ("RTX", "3060")):
        triple = f"ampere-rtx3060-{system_os}"
    elif all(x in device_name for x in ("RTX", "3050")):
        triple = f"ampere-rtx3050-{system_os}"
    # We use ampere until lovelace target triples are plumbed in.
    elif all(x in device_name for x in ("RTX", "4090")):
        triple = f"ampere-rtx4090-{system_os}"
    elif all(x in device_name for x in ("RTX", "4080")):
        triple = f"ampere-rtx4080-{system_os}"
    elif all(x in device_name for x in ("RTX", "4070")):
        triple = f"ampere-rtx4070-{system_os}"
    elif all(x in device_name for x in ("RTX", "4000")):
        triple = f"turing-rtx4000-{system_os}"
    elif all(x in device_name for x in ("RTX", "5000")):
        triple = f"turing-rtx5000-{system_os}"
    elif all(x in device_name for x in ("RTX", "6000")):
        triple = f"turing-rtx6000-{system_os}"
    elif all(x in device_name for x in ("RTX", "8000")):
        triple = f"turing-rtx8000-{system_os}"
    elif all(x in device_name for x in ("TITAN", "RTX")):
        triple = f"turing-titanrtx-{system_os}"
    elif all(x in device_name for x in ("GTX", "1060")):
        triple = f"pascal-gtx1060-{system_os}"
    elif all(x in device_name for x in ("GTX", "1070")):
        triple = f"pascal-gtx1070-{system_os}"
    elif all(x in device_name for x in ("GTX", "1080")):
        triple = f"pascal-gtx1080-{system_os}"

    # Amd Targets
    # Linux: Radeon RX 7900 XTX
    # Windows: AMD Radeon RX 7900 XTX
    elif all(x in device_name for x in ("RX", "7800")):
        triple = f"rdna3-7800-{system_os}"
    elif all(x in device_name for x in ("RX", "7900")):
        triple = f"rdna3-7900-{system_os}"
    elif all(x in device_name for x in ("Radeon", "780M")):
        triple = f"rdna3-780m-{system_os}"
    elif all(x in device_name for x in ("AMD", "PRO", "W7900")):
        triple = f"rdna3-w7900-{system_os}"
    elif any(x in device_name for x in ("AMD", "Radeon")):
        triple = f"rdna2-unknown-{system_os}"
    # Intel Targets
    elif any(x in device_name for x in ("A770", "A750")):
        triple = f"arc-770-{system_os}"

    # Adreno Targets
    elif all(x in device_name for x in ("Adreno", "740")):
        triple = f"adreno-a740-{system_os}"

    else:
        triple = None
    return triple


def get_vulkan_triple_flag(device_name="", device_num=0, extra_args=[]):
    for flag in extra_args:
        if "-iree-vulkan-target-triple=" in flag:
            print(f"Using target triple {flag.split('=')[1]}")
            return None

    if device_name == "" or device_name == [] or device_name is None:
        vulkan_device = get_vulkan_device_name(device_num=device_num)
    else:
        vulkan_device = device_name
    triple = get_vulkan_target_triple(vulkan_device)
    if triple is not None:
        print(
            f"Found vulkan device {vulkan_device}. Using target triple {triple}"
        )
        return f"-iree-vulkan-target-triple={triple}"
    print(
        """Optimized kernel for your target device is not added yet.
        Contact SHARK Admin on discord[https://discord.com/invite/RUqY2h2s9u]
        or pull up an issue."""
    )
    print(f"Target : {vulkan_device}")
    return None


def get_iree_vulkan_args(device_num=0, extra_args=[]):
    # res_vulkan_flag = ["--iree-flow-demote-i64-to-i32"]

    res_vulkan_flag = []
    vulkan_triple_flag = None
    for arg in extra_args:
        if "-iree-vulkan-target-triple=" in arg:
            print(f"Using target triple {arg} from command line args")
            vulkan_triple_flag = arg
            break

    if vulkan_triple_flag is None:
        vulkan_triple_flag = get_vulkan_triple_flag(
            device_num=device_num, extra_args=extra_args
        )

    if vulkan_triple_flag is not None:
        vulkan_target_env = get_vulkan_target_env_flag(vulkan_triple_flag)
        res_vulkan_flag.append(vulkan_target_env)
    return res_vulkan_flag


@functools.cache
def get_iree_vulkan_runtime_flags():
    vulkan_runtime_flags = [
        f"--vulkan_validation_layers={'true' if shark_args.vulkan_validation_layers else 'false'}",
    ]
    return vulkan_runtime_flags


def set_iree_vulkan_runtime_flags(flags):
    for flag in flags:
        ireert.flags.parse_flags(flag)
    return

```

`shark/model_annotation.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Usage:
This function takes the model mlir file and the tuned config file as input,
and output a new mlir file with lowering configs annotated on certain ops.
There are two ways to utilize the function:
1. Call model_annotation function within another python script
from shark.model_annotation import model_annotation
with create_context() as ctx:
   module = model_annotation(ctx, input_contents=..., config_path=..., search_op=...)
2. Run model_annotation.py directly
python model_annotation.py -model path_to_original_mlir -config_path path_to_config_file
"""

import json
import os
import sys
from typing import Dict, List

import iree.compiler._mlir_libs
from iree.compiler import ir


def model_annotation(
    ctx: ir.Context,
    *,
    input_contents: str,
    config_path: str,
    search_op: str,
    winograd: bool = False,
):
    if os.path.isfile(input_contents):
        with open(input_contents, "rb") as f:
            input_contents = f.read()
    module = ir.Module.parse(input_contents)

    if config_path == "":
        return module

    if winograd:
        with open(config_path, "r") as f:
            data = json.load(f)
            configs = data["c,f"]
    else:
        configs = load_model_configs(config_path)

    # The Python API does not expose a general walk() function, so we just
    # do it ourselves.
    walk_children(module.operation, configs, search_op, winograd)

    if not module.operation.verify():
        raise RuntimeError("Modified program does not verify!")

    return module


def load_model_configs(config_path: str):
    config = {}
    with open(config_path, "r") as f:
        for line in f:
            data = json.loads(line)

            if "identifier" not in data.keys():
                continue
            if data["identifier"] == "matmul":
                matrix_size = [data["m"], data["n"], data["k"]]
            elif data["identifier"] == "bmm":
                matrix_size = [data["b"], data["m"], data["n"], data["k"]]
            elif data["identifier"] == "generic":
                matrix_size = [1, data["b"], data["m"], data["n"], data["k"]]
            elif data["identifier"] == "conv":
                matrix_size = [
                    data["n"],
                    data["ih"],
                    data["iw"],
                    data["c"],
                    data["kh"],
                    data["kw"],
                    data["f"],
                    data["oh"],
                    data["ow"],
                    data["d"],
                    data["s"],
                    data["p"],
                ]
            config[shape_list_to_string(matrix_size)] = data
        f.close()
        return config


def walk_children(
    op: ir.Operation, configs: List[Dict], search_op: str, winograd: bool
):
    if search_op == "matmul":
        op_names = ["linalg.matmul", "mhlo.dot"]
    elif search_op == "bmm":
        op_names = ["linalg.batch_matmul", "mhlo.dot_general"]
    elif search_op == "conv":
        op_names = ["mhlo.convolution", "linalg.conv_2d_nhwc_hwcf"]
    elif search_op == "generic":
        op_names = ["linalg.generic"]
    elif search_op == "all":
        op_names = [
            "mhlo.dot",
            "mhlo.dot_general",
            "mhlo.convolution",
            "linalg.matmul",
            "linalg.batch_matmul",
            "linalg.conv_2d_nhwc_hwcf",
            "linalg.generic",
        ]
    else:
        raise ValueError(f"{search_op} op is not tunable.")

    for region in op.regions:
        for block in region.blocks:
            for child_op in block.operations:
                # TODO: This is dumb. Both Operation and OpView should expose
                # 'operation' and 'name' attributes.
                if isinstance(child_op, ir.OpView):
                    child_op = child_op.operation
                if winograd and child_op.name in [
                    "linalg.conv_2d_nchw_fchw",
                    "linalg.conv_2d_nhwc_hwcf",
                ]:
                    add_winograd_attribute(child_op, configs)
                if child_op.name in op_names:
                    if child_op.name == "linalg.generic":
                        # This is for generic op that has contractionOpInterface
                        # which is basically einsum("mk,bkn->bmn")
                        op_result = str(child_op.results[0])
                        op_iterator = str(
                            child_op.attributes["iterator_types"]
                        )
                        if len(child_op.operands) != 3:
                            continue
                        if "reduction" not in op_iterator:
                            continue
                        if (
                            "arith.addf" not in op_result
                            or "arith.mulf" not in op_result
                        ):
                            continue
                        if "arith.subf" in op_result:
                            continue

                    child_op_shape = get_op_shape(child_op, search_op)
                    if (
                        child_op_shape in configs.keys()
                        and configs[child_op_shape]["options"][0] != None
                    ):
                        add_attributes(
                            child_op, configs[child_op_shape]["options"][0]
                        )

                walk_children(child_op, configs, search_op, winograd)


def get_op_shape(op: ir.Operation, search_op: str):
    shape_list = []
    if search_op in ["generic", "all"]:
        if op.name in ["linalg.generic"]:
            input1 = str(op.operands[0].type)
            input2 = str(op.operands[1].type)
            m = input1.split("tensor<")[1].split("x")[0]
            b = input2.split("tensor<")[1].split("x")[0]
            k = input2.split("tensor<")[1].split("x")[1]
            n = input2.split("tensor<")[1].split("x")[2]
            shape_list = [1, int(b), int(m), int(n), int(k)]

    if search_op in ["matmul", "all"]:
        if op.name in ["mhlo.dot"]:
            op_result = str(op.results[0])
            m = op_result.split("tensor<")[1].split("x")[0]
            k = op_result.split("tensor<")[1].split("x")[1]
            n = op_result.split("tensor<")[2].split("x")[1]
            shape_list = [int(m), int(n), int(k)]
        elif op.name in ["linalg.matmul"]:
            op_result = str(op.results[0]).split("ins(")[1]
            m = op_result.split("tensor<")[1].split("x")[0]
            k = op_result.split("tensor<")[1].split("x")[1]
            n = op_result.split("tensor<")[2].split("x")[1]
            shape_list = [int(m), int(n), int(k)]

    if search_op in ["bmm", "all"]:
        if op.name in ["mhlo.dot_general"]:
            op_result = str(op.results[0])
            b = op_result.split("tensor<")[1].split("x")[1]
            m = op_result.split("tensor<")[1].split("x")[2]
            k = op_result.split("tensor<")[1].split("x")[3]
            n = op_result.split("tensor<")[3].split("x")[3]
            shape_list = [int(b), int(m), int(n), int(k)]
        elif op.name in ["linalg.batch_matmul"]:
            op_result = str(op.results[0]).split("ins(")[1]
            b = op_result.split("tensor<")[1].split("x")[0]
            m = op_result.split("tensor<")[1].split("x")[1]
            k = op_result.split("tensor<")[1].split("x")[2]
            n = op_result.split("tensor<")[3].split("x")[2]
            shape_list = [int(b), int(m), int(n), int(k)]

    if search_op in ["conv", "all"]:
        if op.name in ["mhlo.convolution"]:
            op_result = str(op.results[0])
            dilation = (
                str(op.attributes["rhs_dilation"])
                .split("dense<")[1]
                .split(">")[0]
            )
            stride = (
                str(op.attributes["window_strides"])
                .split("dense<")[1]
                .split(">")[0]
            )
            pad = (
                str(op.attributes["padding"]).split("dense<")[1].split(">")[0]
            )
            n = op_result.split("tensor<")[1].split("x")[0]
            ih = op_result.split("tensor<")[1].split("x")[1]
            iw = op_result.split("tensor<")[1].split("x")[2]
            c = op_result.split("tensor<")[1].split("x")[3]
            kh = op_result.split("tensor<")[2].split("x")[0]
            kw = op_result.split("tensor<")[2].split("x")[1]
            f = op_result.split("tensor<")[2].split("x")[3]
            oh = op_result.split("tensor<")[3].split("x")[1]
            ow = op_result.split("tensor<")[3].split("x")[2]
            shape_list = [
                int(n),
                int(ih),
                int(iw),
                int(c),
                int(kh),
                int(kw),
                int(f),
                int(oh),
                int(ow),
                int(dilation),
                int(stride),
                int(pad),
            ]

        elif op.name in ["linalg.conv_2d_nhwc_hwcf"]:
            op_result = str(op.results[0]).split("ins(")[1]
            dilation = (
                str(op.attributes["dilations"])
                .split("dense<")[1]
                .split(">")[0]
            )
            stride = (
                str(op.attributes["strides"]).split("dense<")[1].split(">")[0]
            )
            pad = 0
            n = op_result.split("tensor<")[1].split("x")[0]
            ih = op_result.split("tensor<")[1].split("x")[1]
            iw = op_result.split("tensor<")[1].split("x")[2]
            c = op_result.split("tensor<")[1].split("x")[3]
            kh = op_result.split("tensor<")[2].split("x")[0]
            kw = op_result.split("tensor<")[2].split("x")[1]
            f = op_result.split("tensor<")[2].split("x")[3]
            oh = op_result.split("tensor<")[3].split("x")[1]
            ow = op_result.split("tensor<")[3].split("x")[2]
            shape_list = [
                int(n),
                int(ih),
                int(iw),
                int(c),
                int(kh),
                int(kw),
                int(f),
                int(oh),
                int(ow),
                int(dilation),
                int(stride),
                int(pad),
            ]

    shape_str = shape_list_to_string(shape_list)
    return shape_str


def add_attributes(op: ir.Operation, config: List[Dict]):
    # Parse the config file
    split_k = None
    pipeline_depth = None
    store_stage = None
    subgroup_size = None

    if "GPU" in config["pipeline"]:
        pipeline = (
            "LLVMGPUMatmulSimt"
            if config["pipeline"] == "GPU"
            else "LLVMGPUMatmulTensorCore"
        )
        tile_sizes = [config["work_group_tile_sizes"]]
        workgroup_size = config["work_group_sizes"]
        if "pipeline_depth" in config.keys():
            pipeline_depth = config["pipeline_depth"]
        if "split_k" in config.keys():
            split_k = config["split_k"]
    elif "SPIRV" in config["pipeline"]:
        pipeline = config["pipeline"]
        if pipeline == "SPIRVMatmulPromoteVectorize":
            tile_sizes = [
                config["work_group_tile_sizes"]
                + [config["reduction_tile_sizes"][-1]],
            ]
        else:
            tile_sizes = [
                config["work_group_tile_sizes"],
                config["parallel_tile_sizes"],
                config["reduction_tile_sizes"],
            ]

        workgroup_size = config["work_group_sizes"]
        if "vector_tile_sizes" in config.keys():
            tile_sizes += [config["vector_tile_sizes"]]
        if "window_tile_sizes" in config.keys():
            tile_sizes += [config["window_tile_sizes"]]
        if "subgroup_size" in config.keys():
            subgroup_size = config["subgroup_size"]
        if "pipeline_depth" in config.keys():
            pipeline_depth = config["pipeline_depth"]
        if "store_stage" in config.keys():
            store_stage = config["store_stage"]
    else:
        # For IREE CPU pipelines
        pipeline = config["pipeline"]
        tile_sizes = [
            config["work_group_tile_sizes"],
            config["parallel_tile_sizes"],
            config["reduction_tile_sizes"],
        ]
        workgroup_size = []

    # Add compilation info as an attribute. We don't have a Python binding for CompilationInfo,
    # so we just parse its string form.
    if pipeline_depth != None:
        translation_info = f"{pipeline} pipeline_depth = {pipeline_depth}"
        if store_stage != None:
            translation_info += f" store_stage = {store_stage}"
    else:
        translation_info = f"{pipeline}"

    compilation_info = (
        f"#iree_codegen.compilation_info<"
        f"lowering_config = <tile_sizes = {repr(tile_sizes)}>, "
        f"translation_info = <{translation_info}>, "
        f"workgroup_size = {repr(workgroup_size)} "
    )

    if subgroup_size != None:
        compilation_info += f", subgroup_size = {subgroup_size}>"
    else:
        compilation_info += ">"

    attr = ir.Attribute.parse(compilation_info)
    op.attributes["compilation_info"] = attr

    # Add other attributes if required.
    if split_k:
        add_attribute_by_name(op, "iree_flow_split_k", split_k)


def add_winograd_attribute(op: ir.Operation, config: List):
    op_result = str(op.results[0]).split("ins(")[1]
    dilation = int(
        str(op.attributes["dilations"]).split("dense<")[1].split(">")[0]
    )
    stride = int(
        str(op.attributes["strides"]).split("dense<")[1].split(">")[0]
    )

    if op.name == "linalg.conv_2d_nchw_fchw":
        f = int(op_result.split("tensor<")[2].split("x")[0])
        c = int(op_result.split("tensor<")[2].split("x")[1])
        kh = int(op_result.split("tensor<")[2].split("x")[2])
        kw = int(op_result.split("tensor<")[2].split("x")[3])
    else:
        kh = int(op_result.split("tensor<")[2].split("x")[0])
        kw = int(op_result.split("tensor<")[2].split("x")[1])
        c = int(op_result.split("tensor<")[2].split("x")[2])
        f = int(op_result.split("tensor<")[2].split("x")[3])

    if (
        dilation == 1
        and stride == 1
        and kh == 3
        and kw == 3
        and [c, f] in config
    ):
        op.attributes["iree_winograd_conv"] = ir.IntegerAttr.get(
            ir.IntegerType.get_signless(64), 1
        )


def add_attribute_by_name(op: ir.Operation, name: str, val: int):
    attr = ir.IntegerAttr.get(ir.IntegerType.get_signless(64), val)
    op.attributes[name] = attr


def shape_list_to_string(input):
    return "x".join([str(d) for d in input])


def create_context() -> ir.Context:
    context = ir.Context()
    context.allow_unregistered_dialects = True
    return context


if __name__ == "__main__":
    import argparse
    from pathlib import Path

    def path_expand(s):
        return Path(s).expanduser().resolve()

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-model",
        type=path_expand,
        default="model.mlir",
        help="Path to the input mlir file",
    )
    parser.add_argument(
        "-config_path",
        type=path_expand,
        default="best_configs.json",
        help="Path where stores the op config file",
    )
    parser.add_argument(
        "-output_path",
        type=path_expand,
        default="tuned_model.mlir",
        help="Path to save the annotated mlir file",
    )
    parser.add_argument(
        "-search_op",
        type=str,
        default="all",
        help="Op to be optimized. options are matmul, bmm, conv.",
    )

    args = parser.parse_args()

    with create_context() as ctx:
        module = model_annotation(
            ctx,
            input_contents=args.model,
            config_path=args.config_path,
            search_op=args.search_op,
        )
        mlir_str = str(module)
        with open(args.output_path, "w") as f:
            f.write(mlir_str)
        print(f"Saved mlir in {args.output_path}.")

```

`shark/parser.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import os
import shlex
import subprocess


class SplitStrToListAction(argparse.Action):
    def __init__(self, option_strings, dest, *args, **kwargs):
        super(SplitStrToListAction, self).__init__(
            option_strings=option_strings, dest=dest, *args, **kwargs
        )

    def __call__(self, parser, namespace, values, option_string=None):
        del parser, option_string
        setattr(namespace, self.dest, shlex.split(" "))


parser = argparse.ArgumentParser(description="SHARK runner.")

parser.add_argument(
    "--device",
    type=str,
    default="cpu",
    help="Device on which shark_runner runs. options are cpu, cuda, and vulkan",
)
parser.add_argument(
    "--additional_compile_args",
    default=list(),
    nargs=1,
    action=SplitStrToListAction,
    help="Additional arguments to pass to the compiler. These are appended as the last arguments.",
)
parser.add_argument(
    "--additional_runtime_args",
    default=list(),
    nargs=1,
    action=SplitStrToListAction,
    help="Additional arguments to pass to the IREE runtime. These are appended as the last arguments.",
)
parser.add_argument(
    "--enable_tf32",
    type=bool,
    default=False,
    help="Enables TF32 precision calculations on supported GPUs.",
)
parser.add_argument(
    "--model_config_path",
    help="Directory to where the tuned model config file is located.",
    default=None,
)

parser.add_argument(
    "--num_warmup_iterations",
    type=int,
    default=5,
    help="Run the model for the specified number of warmup iterations.",
)
parser.add_argument(
    "--num_iterations",
    type=int,
    default=100,
    help="Run the model for the specified number of iterations.",
)
parser.add_argument(
    "--onnx_bench",
    default=False,
    action="store_true",
    help="When enabled, pytest bench results will include ONNX benchmark results.",
)
parser.add_argument(
    "--shark_prefix",
    default=None,
    help="gs://shark_tank/<this_flag>/model_directories",
)
parser.add_argument(
    "--update_tank",
    default=True,
    action="store_true",
    help="When enabled, SHARK downloader will update local shark_tank if local hash is different from latest upstream hash.",
)
parser.add_argument(
    "--force_update_tank",
    default=False,
    action="store_true",
    help="When enabled, SHARK downloader will force an update of local shark_tank artifacts for each request.",
)
parser.add_argument(
    "--local_tank_cache",
    default=None,
    help="Specify where to save downloaded shark_tank artifacts. If this is not set, the default is ~/.local/shark_tank/.",
)

parser.add_argument(
    "--dispatch_benchmarks",
    default=None,
    help='dispatches to return benchamrk data on.  use "All" for all, and None for none.',
)

parser.add_argument(
    "--dispatch_benchmarks_dir",
    default="temp_dispatch_benchmarks",
    help='directory where you want to store dispatch data generated with "--dispatch_benchmarks"',
)

parser.add_argument(
    "--enable_conv_transform",
    default=False,
    action="store_true",
    help="Enables the --iree-flow-enable-conv-nchw-to-nhwc-transform flag.",
)

parser.add_argument(
    "--enable_img2col_transform",
    default=False,
    action="store_true",
    help="Enables the --iree-flow-enable-conv-img2col-transform flag.",
)

parser.add_argument(
    "--use_winograd",
    default=False,
    action="store_true",
    help="Enables the --iree-flow-enable-conv-winograd-transform flag.",
)

parser.add_argument(
    "--device_allocator",
    type=str,
    nargs="*",
    default=["caching"],
    help="Specifies one or more HAL device allocator specs "
    "to augment the base device allocator",
    choices=["debug", "caching"],
)
parser.add_argument(
    "--task_topology_max_group_count",
    type=str,
    default=None,
    help="passthrough flag for the iree flag of the same name. If None, defaults to cpu-count",
)

parser.add_argument(
    "--vulkan_debug_utils",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Profiles vulkan device and collects the .rdc info.",
)

parser.add_argument(
    "--vulkan_validation_layers",
    default=False,
    action=argparse.BooleanOptionalAction,
    help="Flag for disabling vulkan validation layers when benchmarking.",
)

shark_args, unknown = parser.parse_known_args()

```

`shark/shark_benchmark_runner.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from shark.shark_runner import SharkRunner
from shark.iree_utils.compile_utils import (
    export_iree_module_to_vmfb,
    load_flatbuffer,
    get_iree_runtime_config,
)
from shark.iree_utils.benchmark_utils import (
    build_benchmark_args,
    run_benchmark_module,
)
from shark.parser import shark_args
from datetime import datetime
import time
from typing import Optional
import csv
import os

TF_CPU_DEVICE = "/CPU:0"
TF_GPU_DEVICE = "/GPU:0"


def _bytes_to_mb_str(bytes_: Optional[int]) -> str:
    return "" if bytes_ is None else f"{bytes_ / 1e6:.6f}"


class OnnxFusionOptions(object):
    def __init__(self):
        self.disable_gelu = False
        self.disable_layer_norm = False
        self.disable_attention = False
        self.disable_skip_layer_norm = False
        self.disable_embed_layer_norm = False
        self.disable_bias_skip_layer_norm = False
        self.disable_bias_gelu = False
        self.enable_gelu_approximation = False
        self.use_mask_index = False
        self.no_attention_mask = False


def check_requirements(frontend):
    import importlib

    has_pkgs = False
    if frontend == "torch":
        tv_spec = importlib.util.find_spec("torchvision")
        has_pkgs = tv_spec is not None

    elif frontend in ["tensorflow", "tf"]:
        keras_spec = importlib.util.find_spec("keras")
        tf_spec = importlib.util.find_spec("tensorflow")
        has_pkgs = keras_spec is not None and tf_spec is not None

    return has_pkgs


class SharkBenchmarkRunner(SharkRunner):
    # SharkRunner derived class with Benchmarking capabilities.
    def __init__(
        self,
        mlir_module: bytes,
        device: str = "none",
        mlir_dialect: str = "linalg",
        extra_args: list = [],
    ):
        self.device = shark_args.device if device == "none" else device
        self.enable_tf32 = shark_args.enable_tf32
        self.frontend_model = None
        self.vmfb_file = None
        self.mlir_dialect = mlir_dialect
        self.extra_args = extra_args
        self.import_args = {}
        self.temp_file_to_unlink = None
        if not os.path.isfile(mlir_module):
            print(
                "Warning: Initializing SharkRunner with a mlir string/bytecode object will duplicate the model in RAM at compile time. To avoid this, initialize SharkInference with a path to a MLIR module on your hard disk instead."
            )
            self.compile_str = True
        else:
            self.compile_str = False
        SharkRunner.__init__(
            self,
            mlir_module,
            device,
            self.mlir_dialect,
            self.extra_args,
            compile_vmfb=False,
        )
        self.vmfb_file = export_iree_module_to_vmfb(
            mlir_module,
            device,
            ".",
            self.mlir_dialect,
            extra_args=self.extra_args,
            compile_str=self.compile_str,
        )
        params = load_flatbuffer(
            self.vmfb_file,
            device,
            mmap=True,
        )
        self.iree_compilation_module = params["vmfb"]
        self.iree_config = params["config"]
        self.temp_file_to_unlink = params["temp_file_to_unlink"]
        del params

    def setup_cl(self, input_tensors):
        self.benchmark_cl = build_benchmark_args(
            self.vmfb_file,
            self.device,
            input_tensors,
            mlir_dialect=self.mlir_dialect,
        )

    def benchmark_frontend(self, modelname):
        if self.mlir_dialect in ["linalg", "torch"]:
            return self.benchmark_torch(modelname)

        elif self.mlir_dialect in ["mhlo", "tf"]:
            return self.benchmark_tf(modelname)

    def benchmark_torch(self, modelname, device="cpu"):
        import torch
        from tank.model_utils import get_torch_model

        # TODO: Pass this as an arg. currently the best way is to setup with BENCHMARK=1 if we want to use torch+cuda, else use cpu.
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if device == "cuda":
            torch.set_default_device("cuda:0")
            # if self.enable_tf32:
            #    torch.backends.cuda.matmul.allow_tf32 = True
        else:
            torch.set_default_dtype(torch.float32)
            torch.set_default_device("cpu")
        torch_device = torch.device("cuda:0" if device == "cuda" else "cpu")
        HFmodel, input = get_torch_model(modelname, self.import_args)[:2]
        frontend_model = HFmodel.model
        frontend_model.to(torch_device)
        if device == "cuda":
            frontend_model.cuda()
            input.to(torch.device("cuda:0"))
            print(input)
        else:
            frontend_model.cpu()
            input.cpu()

        for i in range(shark_args.num_warmup_iterations):
            frontend_model.forward(input)

        if device == "cuda":
            torch.cuda.reset_peak_memory_stats()
        begin = time.time()
        for i in range(shark_args.num_iterations):
            out = frontend_model.forward(input)
        end = time.time()
        if device == "cuda":
            stats = torch.cuda.memory_stats()
            device_peak_b = stats["allocated_bytes.all.peak"]
            frontend_model.to(torch.device("cpu"))
            input.to(torch.device("cpu"))
            torch.cuda.empty_cache()
        else:
            device_peak_b = None

        print(
            f"Torch benchmark:{shark_args.num_iterations/(end-begin)} iter/second, Total Iterations:{shark_args.num_iterations}"
        )
        if device == "cuda":
            # Set device to CPU so we don't run into segfaults exiting pytest subprocesses.
            torch_device = torch.device("cpu")
        return [
            f"{shark_args.num_iterations/(end-begin)}",
            f"{((end-begin)/shark_args.num_iterations)*1000}",
            "",  # host_peak_b (CPU usage) is not reported by PyTorch.
            _bytes_to_mb_str(device_peak_b),
        ]

    def benchmark_tf(self, modelname):
        import os

        os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
        import tensorflow as tf

        visible_default = tf.config.list_physical_devices("GPU")
        try:
            tf.config.set_visible_devices([], "GPU")
            visible_devices = tf.config.get_visible_devices()
            for device in visible_devices:
                assert device.device_type != "GPU"
        except:
            # Invalid device or cannot modify virtual devices once initialized.
            pass

        from tank.model_utils_tf import get_tf_model

        # tf_device = TF_GPU_DEVICE if self.device == "cuda" else TF_CPU_DEVICE
        tf_device = TF_CPU_DEVICE
        with tf.device(tf_device):
            (
                model,
                input,
            ) = get_tf_model(
                modelname, self.import_args
            )[:2]
            frontend_model = model

            for i in range(shark_args.num_warmup_iterations):
                frontend_model.forward(*input)

            if tf_device == TF_GPU_DEVICE:
                tf.config.experimental.reset_memory_stats(tf_device)
            begin = time.time()
            for i in range(shark_args.num_iterations):
                out = frontend_model.forward(*input)
            end = time.time()
            if tf_device == TF_GPU_DEVICE:
                memory_info = tf.config.experimental.get_memory_info(tf_device)
                device_peak_b = memory_info["peak"]
            else:
                # tf.config.experimental does not currently support measuring
                # CPU memory usage.
                device_peak_b = None

            print(
                f"TF benchmark:{shark_args.num_iterations/(end-begin)} iter/second, Total Iterations:{shark_args.num_iterations}"
            )
            return [
                f"{shark_args.num_iterations/(end-begin)}",
                f"{((end-begin)/shark_args.num_iterations)*1000}",
                "",  # host_peak_b (CPU usage) is not reported by TensorFlow.
                _bytes_to_mb_str(device_peak_b),
            ]

    def benchmark_c(self):
        iter_per_second, host_peak_b, device_peak_b = run_benchmark_module(
            self.benchmark_cl
        )
        print(f"Shark-IREE-C benchmark:{iter_per_second} iter/second")
        return [
            f"{iter_per_second}",
            f"{1000/iter_per_second}",
            _bytes_to_mb_str(host_peak_b),
            _bytes_to_mb_str(device_peak_b),
        ]

    def benchmark_python(self, inputs):
        input_list = [x for x in inputs]
        for i in range(shark_args.num_warmup_iterations):
            self.run("forward", input_list)

        begin = time.time()
        for i in range(shark_args.num_iterations):
            out = self.run("forward", input_list)
        end = time.time()
        print(
            f"Shark-IREE Python benchmark:{shark_args.num_iterations/(end-begin)} iter/second, Total Iterations:{shark_args.num_iterations}"
        )
        return [
            f"{shark_args.num_iterations/(end-begin)}",
            f"{((end-begin)/shark_args.num_iterations)*1000}",
        ]

    def benchmark_onnx(self, modelname, inputs):
        if self.device == "cuda":
            print(
                "Currently GPU benchmarking on ONNX is not supported in SHARK."
            )
            return ["N/A", "N/A"]
        else:
            from onnxruntime.transformers.benchmark import run_onnxruntime
            from onnxruntime.transformers.huggingface_models import MODELS
            from onnxruntime.transformers.benchmark_helper import (
                ConfigModifier,
                Precision,
            )
            import psutil

            if modelname == "microsoft/MiniLM-L12-H384-uncased":
                modelname = "bert-base-uncased"
            if modelname not in MODELS:
                print(
                    f"{modelname} is currently not supported in ORT's HF. Check \
https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/huggingface_models.py \
for currently supported models. Exiting benchmark ONNX."
                )
                return ["N/A", "N/A"]
            use_gpu = self.device == "cuda"
            num_threads = psutil.cpu_count(logical=False)
            batch_sizes = [1]
            sequence_lengths = [128]
            cache_dir = os.path.join(".", "cache_models")
            onnx_dir = os.path.join(".", "onnx_models")
            verbose = False
            input_counts = [1]
            optimize_onnx = True
            validate_onnx = False
            disable_ort_io_binding = False
            use_raw_attention_mask = True
            model_fusion_statistics = {}
            overwrite = False
            model_source = "pt"  # Either "pt" or "tf"
            provider = None
            config_modifier = ConfigModifier(None)
            onnx_args = OnnxFusionOptions()
            result = run_onnxruntime(
                use_gpu,
                provider,
                (modelname,),
                None,
                config_modifier,
                Precision.FLOAT32,
                num_threads,
                batch_sizes,
                sequence_lengths,
                shark_args.num_iterations,
                input_counts,
                optimize_onnx,
                validate_onnx,
                cache_dir,
                onnx_dir,
                verbose,
                overwrite,
                disable_ort_io_binding,
                use_raw_attention_mask,
                model_fusion_statistics,
                model_source,
                onnx_args,
            )
            print(
                f"ONNX ORT-benchmark:{result[0]['QPS']} iter/second, Total Iterations:{shark_args.num_iterations}"
            )
            return [
                result[0]["QPS"],
                result[0]["average_latency_ms"],
            ]

    def get_metadata(self, modelname):
        metadata_path = os.path.join(".", "tank", "model_metadata.csv")
        with open(metadata_path, mode="r") as csvfile:
            torch_reader = csv.reader(csvfile, delimiter=",")
            fields = next(torch_reader)
            for row in torch_reader:
                torch_model_name = row[0]
                if torch_model_name == modelname:
                    param_count = row[3]
                    model_tags = row[4]
                    model_notes = row[5]
                    return [param_count, model_tags, model_notes]

    def compare_bench_results(self, baseline: str, result: str):
        if baseline is not None:
            # Takes a baseline and a result string and calculates a comparison, e.g. "1.04x baseline".
            a = float(baseline)
            b = float(result)
            comparison = a / b
            comp_str = f"{round(comparison, 2)}x baseline"
        else:
            comp_str = "N/A"

        return comp_str

    def benchmark_all_csv(
        self,
        inputs: tuple,
        modelname,
        dynamic,
        device_str,
        frontend,
        import_args,
        mode="native",
    ):
        self.setup_cl(inputs)
        self.import_args = import_args
        self.mode = mode
        field_names = [
            "model",
            "batch_size",
            "engine",
            "dialect",
            "device",
            "shape_type",
            "data_type",
            "iter/sec",
            "ms/iter",
            "vs. PyTorch/TF",
            "iterations",
            "param_count",
            "tags",
            "notes",
            "datetime",
            "host_memory_mb",
            "device_memory_mb",
            "measured_host_memory_mb",
            "measured_device_memory_mb",
        ]
        # "frontend" must be the first element.
        if self.mode == "native":
            engines = ["shark_python", "shark_iree_c"]
        if self.mode == "baseline":
            engines = ["frontend"]
        if self.mode == "all":
            engines = ["frontend", "shark_python", "shark_iree_c"]

        if shark_args.onnx_bench == True:
            engines.append("onnxruntime")

        if not os.path.exists("bench_results.csv"):
            with open("bench_results.csv", mode="w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(field_names)

        with open("bench_results.csv", mode="a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=field_names)
            bench_info = {}
            bench_info["model"] = modelname
            bench_info["batch_size"] = str(import_args["batch_size"])
            bench_info["dialect"] = self.mlir_dialect
            bench_info["iterations"] = shark_args.num_iterations
            if dynamic == True:
                bench_info["shape_type"] = "dynamic"
            else:
                bench_info["shape_type"] = "static"
            bench_info["device"] = device_str
            if "fp16" in modelname:
                bench_info["data_type"] = "float16"
            else:
                bench_info["data_type"] = inputs[0].dtype

            for e in engines:
                engine_result = {}
                self.frontend_result = None
                if e == "frontend":
                    engine_result["engine"] = frontend
                    if check_requirements(frontend):
                        (
                            engine_result["iter/sec"],
                            engine_result["ms/iter"],
                            engine_result["host_memory_mb"],
                            engine_result["device_memory_mb"],
                        ) = self.benchmark_frontend(modelname)
                        self.frontend_result = engine_result["ms/iter"]
                        engine_result["vs. PyTorch/TF"] = "baseline"
                        (
                            engine_result["param_count"],
                            engine_result["tags"],
                            engine_result["notes"],
                        ) = self.get_metadata(modelname)
                    else:
                        self.frontend_result = None
                        continue

                elif e == "shark_python":
                    engine_result["engine"] = "shark_python"
                    (
                        engine_result["iter/sec"],
                        engine_result["ms/iter"],
                    ) = self.benchmark_python(inputs)

                    engine_result[
                        "vs. PyTorch/TF"
                    ] = self.compare_bench_results(
                        self.frontend_result, engine_result["ms/iter"]
                    )

                elif e == "shark_iree_c":
                    engine_result["engine"] = "shark_iree_c"
                    (
                        engine_result["iter/sec"],
                        engine_result["ms/iter"],
                        engine_result["host_memory_mb"],
                        engine_result["device_memory_mb"],
                    ) = self.benchmark_c()

                    engine_result[
                        "vs. PyTorch/TF"
                    ] = self.compare_bench_results(
                        self.frontend_result, engine_result["ms/iter"]
                    )

                elif e == "onnxruntime":
                    engine_result["engine"] = "onnxruntime"
                    (
                        engine_result["iter/sec"],
                        engine_result["ms/iter"],
                    ) = self.benchmark_onnx(modelname, inputs)

                engine_result["datetime"] = str(datetime.now())
                writer.writerow(bench_info | engine_result)

```

`shark/shark_compile.py`:

```py
import os
import tempfile
from shark.shark_inference import SharkInference
from shark.shark_importer import import_with_fx, save_mlir
import torch
import torch_mlir
from torch_mlir.compiler_utils import run_pipeline_with_repro_report
from typing import List, Tuple
from io import BytesIO
from brevitas_examples.common.generative.quantize import quantize_model
from brevitas_examples.llm.llm_quant.run_utils import get_model_impl


# fmt: off
def quant〇matmul_rhs_group_quant〡shape(lhs: List[int], rhs: List[int], rhs_scale: List[int], rhs_zero_point: List[int], rhs_bit_width: int, rhs_group_size: int) -> List[int]:
    if len(lhs) == 3 and len(rhs) == 2:
        return [lhs[0], lhs[1], rhs[0]]
    elif len(lhs) == 2 and len(rhs) == 2:
        return [lhs[0], rhs[0]]
    else:
        raise ValueError("Input shapes not supported.")


def quant〇matmul_rhs_group_quant〡dtype(lhs_rank_dtype: Tuple[int, int], rhs_rank_dtype: Tuple[int, int], rhs_scale_rank_dtype: Tuple[int, int], rhs_zero_point_rank_dtype: Tuple[int, int], rhs_bit_width: int, rhs_group_size: int) -> int:
    # output dtype is the dtype of the lhs float input
    lhs_rank, lhs_dtype = lhs_rank_dtype
    return lhs_dtype


def quant〇matmul_rhs_group_quant〡has_value_semantics(lhs, rhs, rhs_scale, rhs_zero_point, rhs_bit_width, rhs_group_size) -> None:
    return


brevitas_matmul_rhs_group_quant_library = [
    quant〇matmul_rhs_group_quant〡shape,
    quant〇matmul_rhs_group_quant〡dtype,
    quant〇matmul_rhs_group_quant〡has_value_semantics]
# fmt: on


def load_vmfb(extended_model_name, device, mlir_dialect, extra_args=[]):
    vmfb_path = os.path.join(os.getcwd(), extended_model_name + ".vmfb")
    shark_module = None
    if os.path.isfile(vmfb_path):
        shark_module = SharkInference(
            None,
            device=device,
            mlir_dialect=mlir_dialect,
        )
        print(f"loading existing vmfb from: {vmfb_path}")
        shark_module.load_module(vmfb_path, extra_args=extra_args)
    return shark_module


def compile_module(
    shark_module, extended_model_name, generate_vmfb, extra_args=[]
):
    if generate_vmfb:
        vmfb_path = os.path.join(os.getcwd(), extended_model_name + ".vmfb")
        if os.path.isfile(vmfb_path):
            print(f"loading existing vmfb from: {vmfb_path}")
            shark_module.load_module(vmfb_path, extra_args=extra_args)
        else:
            print(
                "No vmfb found. Compiling and saving to {}".format(vmfb_path)
            )
            path = shark_module.save_module(
                os.getcwd(), extended_model_name, extra_args
            )
            shark_module.load_module(path, extra_args=extra_args)
    else:
        shark_module.compile(extra_args)
    return shark_module


def compile_int_precision(
    model, inputs, precision, device, generate_vmfb, extended_model_name
):
    weight_bit_width = 4 if precision == "int4" else 8
    weight_group_size = 128
    quantize_model(
        get_model_impl(model),
        dtype=torch.float32,
        weight_quant_type="asym",
        weight_bit_width=weight_bit_width,
        weight_param_method="stats",
        weight_scale_precision="float_scale",
        weight_quant_granularity="per_group",
        weight_group_size=weight_group_size,
        quantize_weight_zero_point=False,
        input_bit_width=None,
        input_scale_type="float",
        input_param_method="stats",
        input_quant_type="asym",
        input_quant_granularity="per_tensor",
        quantize_input_zero_point=False,
        seqlen=2048,
    )
    print("Weight quantization applied.")
    torchscript_module = import_with_fx(
        model,
        inputs,
        precision=precision,
        mlir_type="torchscript",
    )
    mlir_module = torch_mlir.compile(
        torchscript_module,
        inputs,
        output_type="torch",
        backend_legal_ops=["quant.matmul_rhs_group_quant"],
        extra_library=brevitas_matmul_rhs_group_quant_library,
        use_tracing=False,
        verbose=False,
    )
    print(f"[DEBUG] converting torch to linalg")
    run_pipeline_with_repro_report(
        mlir_module,
        "builtin.module(func.func(torch-unpack-quant-tensor),func.func(torch-convert-custom-quant-op),torch-backend-to-linalg-on-tensors-backend-pipeline)",
        description="Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR",
    )
    from contextlib import redirect_stdout

    mlir_file_path = os.path.join(
        os.getcwd(), f"{extended_model_name}_linalg.mlir"
    )
    with open(mlir_file_path, "w") as f:
        with redirect_stdout(f):
            print(mlir_module.operation.get_asm())
    mlir_module = str(mlir_module)
    mlir_module = mlir_module.encode("UTF-8")
    mlir_module = BytesIO(mlir_module)
    bytecode = mlir_module.read()
    bytecode_path = os.path.join(
        os.getcwd(), f"{extended_model_name}_linalg.mlirbc"
    )
    with open(bytecode_path, "wb") as f:
        f.write(bytecode)
    del bytecode
    del mlir_module
    print(f"Elided IR written for {extended_model_name}")
    return bytecode_path
    shark_module = SharkInference(
        mlir_module=bytecode_path, device=device, mlir_dialect="tm_tensor"
    )
    extra_args = [
        "--iree-hal-dump-executable-sources-to=ies",
        "--iree-vm-target-truncate-unsupported-floats",
        "--iree-codegen-check-ir-before-llvm-conversion=false",
        "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
    ]
    return (
        compile_module(
            shark_module,
            extended_model_name=extended_model_name,
            generate_vmfb=generate_vmfb,
            extra_args=extra_args,
        ),
        bytecode_path,
    )


def shark_compile_through_fx(
    model,
    inputs,
    extended_model_name,
    precision,
    f16_input_mask=None,
    save_dir=tempfile.gettempdir(),
    debug=False,
    generate_or_load_vmfb=True,
    extra_args=[],
    device=None,
    mlir_dialect="tm_tensor",
):
    is_f16 = precision == "fp16"
    if generate_or_load_vmfb:
        shark_module = load_vmfb(
            extended_model_name=extended_model_name,
            device=device,
            mlir_dialect=mlir_dialect,
            extra_args=extra_args,
        )
        if shark_module:
            return (
                shark_module,
                None,
            )

    from shark.parser import shark_args

    if "cuda" in device:
        shark_args.enable_tf32 = True

    if precision in ["int4", "int8"]:
        mlir_module = compile_int_precision(
            model,
            inputs,
            precision,
            device,
            generate_or_load_vmfb,
            extended_model_name,
        )
        extra_args = [
            "--iree-hal-dump-executable-sources-to=ies",
            "--iree-vm-target-truncate-unsupported-floats",
            "--iree-codegen-check-ir-before-llvm-conversion=false",
            "--iree-vm-bytecode-module-output-format=flatbuffer-binary",
        ]
    else:
        (
            bytecode,
            _,
        ) = import_with_fx(
            model=model,
            inputs=inputs,
            is_f16=is_f16,
            f16_input_mask=f16_input_mask,
            debug=debug,
            model_name=extended_model_name,
            save_dir=save_dir,
        )
        mlir_module = save_mlir(
            mlir_module=bytecode,
            model_name=extended_model_name,
            mlir_dialect=mlir_dialect,
        )

    shark_module = SharkInference(
        mlir_module,
        device=device,
        mlir_dialect=mlir_dialect,
    )
    return (
        compile_module(
            shark_module,
            extended_model_name,
            generate_vmfb=generate_or_load_vmfb,
            extra_args=extra_args,
        ),
        mlir_module,
    )

```

`shark/shark_downloader.py`:

```py
# Lint as: python3
"""SHARK Downloader"""
# Requirements : Put shark_tank in SHARK directory
#   /SHARK
#     /gen_shark_tank
#       /tflite
#         /albert_lite_base
#         /...model_name...
#       /tf
#       /pytorch
#
#
#

import numpy as np
import os
from tqdm.std import tqdm
import sys
from pathlib import Path
from shark.parser import shark_args
from google.cloud import storage


def download_public_file(
    full_gs_url, destination_folder_name, single_file=False
):
    """Downloads a public blob from the bucket."""
    # bucket_name = "gs://your-bucket-name/path/to/file"
    # destination_file_name = "local/path/to/file"

    storage_client = storage.Client.create_anonymous_client()
    bucket_name = full_gs_url.split("/")[2]
    source_blob_name = None
    dest_filename = None
    desired_file = None
    if single_file:
        desired_file = full_gs_url.split("/")[-1]
        source_blob_name = "/".join(full_gs_url.split("/")[3:-1])
        destination_folder_name, dest_filename = os.path.split(
            destination_folder_name
        )
    else:
        source_blob_name = "/".join(full_gs_url.split("/")[3:])
    bucket = storage_client.bucket(bucket_name)
    blobs = bucket.list_blobs(prefix=source_blob_name)
    if not os.path.exists(destination_folder_name):
        os.mkdir(destination_folder_name)
    for blob in blobs:
        blob_name = blob.name.split("/")[-1]
        if single_file:
            if blob_name == desired_file:
                destination_filename = os.path.join(
                    destination_folder_name, dest_filename
                )
                with open(destination_filename, "wb") as f:
                    with tqdm.wrapattr(
                        f, "write", total=blob.size
                    ) as file_obj:
                        storage_client.download_blob_to_file(blob, file_obj)
            else:
                continue

        else:
            destination_filename = os.path.join(
                destination_folder_name, blob_name
            )
            if os.path.isdir(destination_filename):
                continue
            with open(destination_filename, "wb") as f:
                with tqdm.wrapattr(f, "write", total=blob.size) as file_obj:
                    storage_client.download_blob_to_file(blob, file_obj)


input_type_to_np_dtype = {
    "float32": np.float32,
    "float64": np.float64,
    "bool": np.bool_,
    "int32": np.int32,
    "int64": np.int64,
    "uint8": np.uint8,
    "int8": np.int8,
}

# Save the model in the home local so it needn't be fetched everytime in the CI.
home = str(Path.home())
alt_path = os.path.join(os.path.dirname(__file__), "../gen_shark_tank/")
custom_path = shark_args.local_tank_cache

if custom_path is not None:
    if not os.path.exists(custom_path):
        os.mkdir(custom_path)

    WORKDIR = custom_path

    print(f"Using {WORKDIR} as local shark_tank cache directory.")

elif os.path.exists(alt_path):
    WORKDIR = alt_path
    print(
        f"Using {WORKDIR} as shark_tank directory. Delete this directory if you aren't working from locally generated shark_tank."
    )
else:
    WORKDIR = os.path.join(home, ".local/shark_tank/")
    print(
        f"shark_tank local cache is located at {WORKDIR} . You may change this by setting the --local_tank_cache= flag"
    )
os.makedirs(WORKDIR, exist_ok=True)


# Checks whether the directory and files exists.
def check_dir_exists(model_name, frontend="torch", dynamic=""):
    model_dir = os.path.join(WORKDIR, model_name)

    # Remove the _tf keyword from end only for non-SD models.
    if not any(model in model_name for model in ["clip", "unet", "vae"]):
        if frontend in ["tf", "tensorflow"]:
            model_name = model_name[:-3]
        elif frontend in ["tflite"]:
            model_name = model_name[:-7]
        elif frontend in ["torch", "pytorch"]:
            model_name = model_name[:-6]

    model_mlir_file_name = f"{model_name}{dynamic}_{frontend}.mlir"

    if os.path.isdir(model_dir):
        if (
            os.path.isfile(os.path.join(model_dir, model_mlir_file_name))
            and os.path.isfile(os.path.join(model_dir, "function_name.npy"))
            and os.path.isfile(os.path.join(model_dir, "inputs.npz"))
            and os.path.isfile(os.path.join(model_dir, "golden_out.npz"))
            and os.path.isfile(os.path.join(model_dir, "hash.npy"))
        ):
            print(
                f"""Model artifacts for {model_name} found at {WORKDIR}..."""
            )
            return True
    return False


def _internet_connected():
    import requests as req

    try:
        req.get("http://1.1.1.1")
        return True
    except:
        return False


def get_git_revision_short_hash() -> str:
    import subprocess

    if shark_args.shark_prefix is not None:
        prefix_kw = shark_args.shark_prefix
    else:
        import json

        dir_path = os.path.dirname(os.path.realpath(__file__))
        src = os.path.join(dir_path, "..", "tank_version.json")
        with open(src, "r") as f:
            data = json.loads(f.read())
            prefix_kw = data["version"]
    print(f"Checking for updates from gs://shark_tank/{prefix_kw}")
    return prefix_kw


def get_sharktank_prefix():
    tank_prefix = ""
    if not _internet_connected():
        print(
            "No internet connection. Using the model already present in the tank."
        )
        tank_prefix = "none"
    else:
        desired_prefix = get_git_revision_short_hash()
        storage_client_a = storage.Client.create_anonymous_client()
        base_bucket_name = "shark_tank"
        base_bucket = storage_client_a.bucket(base_bucket_name)
        dir_blobs = base_bucket.list_blobs(prefix=f"{desired_prefix}")
        for blob in dir_blobs:
            dir_blob_name = blob.name.split("/")
            if desired_prefix in dir_blob_name[0]:
                tank_prefix = dir_blob_name[0]
                break
            else:
                continue
        if tank_prefix == "":
            print(
                f"shark_tank bucket not found matching ({desired_prefix}). Defaulting to nightly."
            )
            tank_prefix = "nightly"
    return tank_prefix


# Downloads the torch model from gs://shark_tank dir.
def download_model(
    model_name,
    dynamic=False,
    tank_url=None,
    frontend=None,
    tuned=None,
    import_args={"batch_size": 1},
):
    model_name = model_name.replace("/", "_")
    dyn_str = "_dynamic" if dynamic else ""
    os.makedirs(WORKDIR, exist_ok=True)
    shark_args.shark_prefix = get_sharktank_prefix()
    if import_args["batch_size"] and import_args["batch_size"] != 1:
        model_dir_name = (
            model_name
            + "_"
            + frontend
            + "_BS"
            + str(import_args["batch_size"])
        )
    elif any(model in model_name for model in ["clip", "unet", "vae"]):
        # TODO(Ean Garvey): rework extended naming such that device is only included in model_name after .vmfb compilation.
        model_dir_name = model_name
    else:
        model_dir_name = model_name + "_" + frontend
    model_dir = os.path.join(WORKDIR, model_dir_name)

    if not tank_url:
        tank_url = "gs://shark_tank/" + shark_args.shark_prefix

    full_gs_url = tank_url.rstrip("/") + "/" + model_dir_name
    if not check_dir_exists(
        model_dir_name, frontend=frontend, dynamic=dyn_str
    ):
        print(
            f"Downloading artifacts for model {model_name} from: {full_gs_url}"
        )
        download_public_file(full_gs_url, model_dir)

    elif shark_args.force_update_tank == True:
        print(
            f"Force-updating artifacts for model {model_name} from: {full_gs_url}"
        )
        download_public_file(full_gs_url, model_dir)
    else:
        if not _internet_connected():
            print(
                "No internet connection. Using the model already present in the tank."
            )
        else:
            local_hash = str(np.load(os.path.join(model_dir, "hash.npy")))
            gs_hash_url = (
                tank_url.rstrip("/") + "/" + model_dir_name + "/hash.npy"
            )
            download_public_file(
                gs_hash_url,
                os.path.join(model_dir, "upstream_hash.npy"),
                single_file=True,
            )
            try:
                upstream_hash = str(
                    np.load(os.path.join(model_dir, "upstream_hash.npy"))
                )
            except FileNotFoundError:
                print(f"Model artifact hash not found at {model_dir}.")
                upstream_hash = None
            if local_hash != upstream_hash and shark_args.update_tank == True:
                print(f"Updating artifacts for model {model_name}...")
                download_public_file(full_gs_url, model_dir)

            elif local_hash != upstream_hash:
                print(
                    "Hash does not match upstream in gs://shark_tank/. If you want to use locally generated artifacts, this is working as intended. Otherwise, run with --update_tank."
                )
            else:
                print(
                    "Local and upstream hashes match. Using cached model artifacts."
                )

    model_dir = os.path.join(WORKDIR, model_dir_name)
    tuned_str = "" if tuned is None else "_" + tuned
    suffix = f"{dyn_str}_{frontend}{tuned_str}.mlir"
    mlir_filename = os.path.join(model_dir, model_name + suffix)
    print(
        f"Verifying that model artifacts were downloaded successfully to {mlir_filename}..."
    )
    if not os.path.exists(mlir_filename):
        from tank.generate_sharktank import gen_shark_files

        print(
            "The model data was not found. Trying to generate artifacts locally."
        )
        gen_shark_files(model_name, frontend, WORKDIR, import_args)

    assert os.path.exists(mlir_filename), f"MLIR not found at {mlir_filename}"
    function_name = str(np.load(os.path.join(model_dir, "function_name.npy")))
    inputs = np.load(os.path.join(model_dir, "inputs.npz"))
    golden_out = np.load(os.path.join(model_dir, "golden_out.npz"))

    inputs_tuple = tuple([inputs[key] for key in inputs])
    golden_out_tuple = tuple([golden_out[key] for key in golden_out])
    return mlir_filename, function_name, inputs_tuple, golden_out_tuple

```

`shark/shark_eager/shark_eager.py`:

```py
from typing import Any, Dict, List, Tuple
from collections import defaultdict
from shark.shark_importer import import_with_fx, save_mlir
import torchvision.models as models
import copy
import io
import numpy as np
import sys
import torch
import torch.fx
from torch.fx.node import Node
from typing import Dict
import torch_mlir


def shark_backend(fx_g: torch.fx.GraphModule, inputs, device: str = "cpu"):
    mlir_module = torch_mlir.compile(
        fx_g, inputs, output_type="linalg-on-tensors"
    )
    bytecode_stream = io.BytesIO()
    mlir_module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()
    bytecode_path = save_mlir(
        bytecode,
        model_name="shark_eager_module",
        frontend="torch",
        mlir_dialect="tm_tensor",
    )
    from shark.shark_inference import SharkInference

    shark_module = SharkInference(
        mlir_module=bytecode_path,
        device=device,
        mlir_dialect="tm_tensor",
    )
    shark_module.compile(extra_args=[])
    return shark_module


def _make_single_op_gm(node, captured_val, compiled_graph):
    """Make a GraphModule that just executes the given node."""
    g = torch.fx.Graph()
    env = {}
    inputs = []
    for arg in node.args:
        if arg and hasattr(arg, "name"):
            env[arg.name] = g.placeholder(arg.name)
            if isinstance(captured_val[arg.name], (list, tuple)):
                for val in captured_val[arg.name]:
                    inputs.append(val)
            else:
                inputs.append(captured_val[arg.name])

    call = g.node_copy(node, lambda n: env[n.name])
    g.output(call)
    g.lint()
    single_node = torch.fx.GraphModule(torch.nn.Module(), g)
    compiled_module = shark_backend(single_node, inputs)
    compiled_graph[node.name] = {
        "module": compiled_module,
        "inputs": [i for i in env],
        "result": None,
    }
    return


def compiled_graph(gm: torch.fx.GraphModule, attr_info):
    compiled_graph = {}
    g = gm.graph
    for node in g.nodes:
        if node.op == "call_function":
            if not (
                node.target in [torch.ops.aten.empty]
                or node.name.startswith("getitem")
            ):
                _make_single_op_gm(node, attr_info, compiled_graph)

            # Currently torch.aten.empty has an compilation issue, so running natively.
            elif node.target in [torch.ops.aten.empty]:
                compiled_graph[node.name] = {
                    "target": node.target,
                    "args": node.args,
                    "kwargs": node.kwargs,
                    "result": None,
                }
            # Get item is a simple case takes a tuple and return the tensor at a particular index.
            elif node.name.startswith("getitem"):
                compiled_graph[node.name] = {
                    "input": node.args[0].name,
                    "pos": node.args[1],
                    "result": None,
                }

    return compiled_graph


class ShapeProp:
    """
    Shape propagation. This class takes a `GraphModule`.
    Then, its `propagate` method executes the `GraphModule`
    node-by-node with the given arguments. As each operation
    executes, the ShapeProp class stores away the shape and
    element type for the output values of each operation on
    the `shape` and `dtype` attributes of the operation's
    `Node`.
    """

    def __init__(self, mod):
        self.mod = mod
        self.graph = mod.graph
        self.modules = dict(self.mod.named_modules())

    def propagate(self, *args):
        args_iter = iter(args)
        env: Dict[str, Node] = {}

        def load_arg(a):
            return torch.fx.graph.map_arg(a, lambda n: env[n.name])

        def fetch_attr(target: str):
            target_atoms = target.split(".")
            attr_itr = self.mod
            for i, atom in enumerate(target_atoms):
                if not hasattr(attr_itr, atom):
                    raise RuntimeError(
                        f"Node referenced nonexistant target {'.'.join(target_atoms[:i])}"
                    )
                attr_itr = getattr(attr_itr, atom)
            return attr_itr

        for node in self.graph.nodes:
            if node.op == "placeholder":
                result = next(args_iter)
            elif node.op == "get_attr":
                result = fetch_attr(node.target)
            elif node.op == "call_function":
                result = node.target(
                    *load_arg(node.args), **load_arg(node.kwargs)
                )
            elif node.op == "call_method":
                self_obj, *args = load_arg(node.args)
                kwargs = load_arg(node.kwargs)
                result = getattr(self_obj, node.target)(*args, **kwargs)
            elif node.op == "call_module":
                result = self.modules[node.target](
                    *load_arg(node.args), **load_arg(node.kwargs)
                )

            # This is the only code specific to shape propagation.
            # you can delete this `if` branch and this becomes
            # a generic GraphModule interpreter.
            if isinstance(result, torch.Tensor):
                node.shape = result.shape
                node.dtype = result.dtype

            env[node.name] = result

        return env

        # return load_arg(self.graph.result)


resnet18 = models.resnet18(pretrained=True)
resnet18.train(False)
input = (torch.randn(1, 3, 224, 224),)

print(resnet18(input[0]))

fx_graph = import_with_fx(resnet18, input, mlir_type="fx")

shape_prop = ShapeProp(fx_graph)

x = shape_prop.propagate(input[0])

shark_graph = compiled_graph(fx_graph, x)


for key in shark_graph:
    if key.startswith("getitem"):
        input_val = shark_graph[key]["input"]
        pos = shark_graph[key]["pos"]
        if input_val not in shark_graph:
            shark_graph[key]["result"] = x[input_val][pos].detach()
        else:
            shark_graph[key]["result"] = shark_graph[input_val]["result"][
                pos
            ].detach()
    elif key.startswith("empty"):
        operator = shark_graph[key]["target"]
        args = shark_graph[key]["args"]
        kwargs = shark_graph[key]["kwargs"]
        shark_graph[key]["result"] = operator(*args, **kwargs).detach()
    else:
        input_val = shark_graph[key]["inputs"]
        input_tensors = []
        for input in input_val:
            if input not in shark_graph:
                input_tensors.append(x[input].detach())
            else:
                input_tensors.append(shark_graph[input]["result"])

        val = shark_graph[key]["module"]("forward", input_tensors)
        if isinstance(val, (tuple, list)):
            list_val = []
            for v in val:
                list_val.append(torch.from_numpy(v))
            shark_graph[key]["result"] = list_val
        else:
            shark_graph[key]["result"] = torch.from_numpy(val)


print(shark_graph)

```

`shark/shark_generate_model_config.py`:

```py
import re
import json
import numpy as np

import torch_mlir
from iree.compiler import compile_file
from shark.shark_importer import import_with_fx, get_f16_inputs, save_mlir


class GenerateConfigFile:
    def __init__(
        self,
        model,
        num_sharding_stages: int,
        sharding_stages_id: list[str],
        units_in_each_stage: list[int],
        model_input=None,
        config_file_path="model_config.json",
    ):
        self.model = model
        self.num_sharding_stages = num_sharding_stages
        self.sharding_stages_id = sharding_stages_id
        assert self.num_sharding_stages == len(
            self.sharding_stages_id
        ), "Number of sharding stages should be equal to the list of their ID"
        self.model_input = model_input
        self.config_file_path = config_file_path
        # (Nithin) this is a quick fix - revisit and rewrite
        self.units_in_each_stage = np.array(units_in_each_stage)
        self.track_loop = np.zeros(len(self.sharding_stages_id)).astype(int)

    def split_into_dispatches(
        self,
        backend,
        fx_tracing_required=False,
        f16_model=False,
        torch_mlir_tracing=True,
    ):
        graph_for_compilation = self.model
        if fx_tracing_required:
            graph_for_compilation = import_with_fx(
                self.model,
                self.model_input,
                is_f16=f16_model,
                f16_input_mask=[False, False],
                mlir_type="torchscript",
            )

        module = torch_mlir.compile(
            graph_for_compilation,
            (self.model_input),
            torch_mlir.OutputType.LINALG_ON_TENSORS,
            use_tracing=torch_mlir_tracing,
            verbose=False,
        )
        module = module.operation.get_asm(large_elements_limit=4)
        module_file = save_mlir(
            module,
            model_name="module_pre_split",
            frontend="torch",
            mlir_dialect="linalg",
        )
        compiled_module_str = str(
            compile_file(
                module_file,
                target_backends=[backend],
                extra_args=[
                    "--compile-to=flow",
                    "--mlir-elide-elementsattrs-if-larger=4",
                ],
            )
        )

        substring_start_idx = [
            m.start()
            for m in re.finditer("flow.dispatch @", compiled_module_str)
        ]
        dispatch_list = dict()

        # dispatch_no is the 'i'th index of a dispatch out of n total dispatches of a model
        # dispatch_id is the unique id of a dispatch, multiple instances of the same dispatch
        # can occur in a model
        for dispatch_no, substring_idx in enumerate(substring_start_idx):
            dispatch_idx = (
                compiled_module_str[substring_idx:]
                .split(":")[0]
                .split("@")[-1]
            )
            key = "dispatch_no_" + str(dispatch_no)
            dispatch_list[key] = {n: "None" for n in self.sharding_stages_id}
            dispatch_list[key]["dispatch_id"] = dispatch_idx

        self.generate_json(dispatch_list)

    def split_into_layers(self):
        model_dictionary = dict()

        for name, m in self.model.named_modules():
            if name == "":
                continue

            # Remove non-leaf nodes from the config as they aren't an operation
            substring_before_final_period = name.split(".")[:-1]
            substring_before_final_period = ".".join(
                substring_before_final_period
            )
            if substring_before_final_period in model_dictionary:
                del model_dictionary[substring_before_final_period]

            # layer_dict = {n: "None" for n in self.sharding_stages_id}

            # By default embed increasing device id's for each layer
            increasing_wraparound_idx_list = (
                self.track_loop % self.units_in_each_stage
            )
            layer_dict = {
                n: int(increasing_wraparound_idx_list[idx][0][0])
                for idx, n in enumerate(self.sharding_stages_id)
            }
            self.track_loop += 1
            model_dictionary[name] = layer_dict

        self.generate_json(model_dictionary)

    def generate_json(self, artifacts):
        with open(self.config_file_path, "w") as outfile:
            json.dump(artifacts, outfile)


if __name__ == "__main__":
    import torch
    from transformers import AutoTokenizer

    hf_model_path = "TheBloke/vicuna-7B-1.1-HF"
    tokenizer = AutoTokenizer.from_pretrained(hf_model_path, use_fast=False)
    compilation_prompt = "".join(["0" for _ in range(17)])
    compilation_input_ids = tokenizer(
        compilation_prompt,
        return_tensors="pt",
    ).input_ids
    compilation_input_ids = torch.tensor(compilation_input_ids).reshape(
        [1, 19]
    )
    firstVicunaCompileInput = (compilation_input_ids,)
    from apps.language_models.src.model_wrappers.vicuna_model import (
        FirstVicuna,
        SecondVicuna7B,
        CombinedModel,
    )

    model = CombinedModel()
    c = GenerateConfigFile(model, 1, ["gpu_id"], firstVicunaCompileInput)
    c.split_into_layers()

```

`shark/shark_importer.py`:

```py
# Lint as: python3
"""SHARK Importer"""

import sys
import tempfile
import os
import hashlib


def create_hash(file_name):
    with open(file_name, "rb") as f:
        file_hash = hashlib.blake2b(digest_size=64)
        while chunk := f.read(2**10):
            file_hash.update(chunk)

    return file_hash.hexdigest()


# List of the supported frontends.
supported_frontends = {
    "tensorflow",
    "tf",
    "pytorch",
    "torch",
    "tf-lite",
    "tflite",
}


class SharkImporter:
    """
    SharkImporter converts frontend modules into a
    mlir_module. The supported frameworks are tensorflow,
    pytorch, and tf-lite.

    ...

    Attributes
    ----------
    module :
        torch, tensorflow or tf-lite module.
    inputs :
        inputs to the module, may be required for the shape
        information.
    frontend: str
        frontend to which the module belongs.
    raw_model_file: str
        temp tflite model path

    Methods
    -------
    import_mlir(is_dynamic, tracing_required, func_name):
        is_dynamic: input shapes to be totally dynamic (pytorch specific).
        tracing_required: whether tracing is required (pytorch specific.
        func_name: The function to be traced out or imported to mlir.

    import_debug(is_dynamic, tracing_required, func_name):
        returns the converted (mlir_module,func_name) with inputs and golden
        outputs.
        The inputs and outputs are converted into np array.
    """

    def __init__(
        self,
        module,
        inputs: tuple = (),
        frontend: str = "torch",
        raw_model_file: str = "",
        return_str: bool = False,
    ):
        self.module = module
        self.inputs = None if len(inputs) == 0 else inputs
        self.frontend = frontend
        if not self.frontend in supported_frontends:
            print(
                f"The frontend is not in the supported_frontends: {supported_frontends}"
            )
            sys.exit(1)
        self.raw_model_file = raw_model_file
        self.return_str = return_str

    # NOTE: The default function for torch is "forward" and tf-lite is "main".

    def _torch_mlir(self, is_dynamic, tracing_required, mlir_type):
        from shark.torch_mlir_utils import get_torch_mlir_module

        return get_torch_mlir_module(
            self.module,
            self.inputs,
            is_dynamic,
            tracing_required,
            self.return_str,
            mlir_type,
        )

    def _tf_mlir(self, func_name, save_dir="."):
        from iree.compiler import tf as tfc

        return tfc.compile_module(
            self.module,
            exported_names=[func_name],
            import_only=True,
            output_file=save_dir,
        )

    def _tflite_mlir(self, func_name, save_dir="."):
        from iree.compiler import tflite as tflitec

        self.mlir_model = tflitec.compile_file(
            self.raw_model_file,  # in tflite, it is a path to .tflite file, not a tflite interpreter
            input_type="tosa",
            import_only=True,
            output_file=save_dir,
        )
        return self.mlir_model

    # Adds the conversion of the frontend with the private function.
    def import_mlir(
        self,
        is_dynamic=False,
        tracing_required=False,
        func_name="forward",
        save_dir="./shark_tmp/",
        mlir_type="linalg",
    ):
        if self.frontend in ["torch", "pytorch"]:
            if self.inputs == None:
                print(
                    "Please pass in the inputs, the inputs are required to determine the shape of the mlir_module"
                )
                sys.exit(1)
            return (
                self._torch_mlir(is_dynamic, tracing_required, mlir_type),
                func_name,
            )
        if self.frontend in ["tf", "tensorflow"]:
            return self._tf_mlir(func_name, save_dir), func_name
        if self.frontend in ["tflite", "tf-lite"]:
            func_name = "main"
            return self._tflite_mlir(func_name, save_dir), func_name

    # Converts the frontend specific tensors into np array.
    def convert_to_numpy(self, array_tuple: tuple):
        if self.frontend in ["torch", "pytorch"]:
            return [x.detach().cpu().numpy() for x in array_tuple]
        if self.frontend in ["tf", "tensorflow"]:
            return [x.numpy() for x in array_tuple]

    # Saves `function_name.npy`, `inputs.npz`, `golden_out.npz` and `model_name.mlir` in the directory `dir`.
    def save_data(
        self,
        dir,
        model_name,
        mlir_data,
        func_name,
        inputs,
        outputs,
        mlir_type="linalg",
    ):
        import numpy as np

        inputs_name = "inputs.npz"
        outputs_name = "golden_out.npz"
        func_file_name = "function_name"
        model_name_mlir = (
            model_name + "_" + self.frontend + "_" + mlir_type + ".mlir"
        )
        print(f"saving {model_name_mlir} to {dir}")
        try:
            inputs = [x.cpu().detach() for x in inputs]
        except AttributeError:
            try:
                inputs = [x.numpy() for x in inputs]
            except AttributeError:
                inputs = [x for x in inputs]
        np.savez(os.path.join(dir, inputs_name), *inputs)
        np.savez(os.path.join(dir, outputs_name), *outputs)
        np.save(os.path.join(dir, func_file_name), np.array(func_name))
        if self.frontend == "torch":
            with open(os.path.join(dir, model_name_mlir), "wb") as mlir_file:
                mlir_file.write(mlir_data)
        hash_gen_attempts = 2
        for i in range(hash_gen_attempts):
            try:
                mlir_hash = create_hash(os.path.join(dir, model_name_mlir))
            except FileNotFoundError as err:
                if i < hash_gen_attempts:
                    continue
                else:
                    raise err

        np.save(os.path.join(dir, "hash"), np.array(mlir_hash))
        return

    def import_debug(
        self,
        is_dynamic=False,
        tracing_required=False,
        func_name="forward",
        dir=tempfile.gettempdir(),
        model_name="model",
        golden_values=None,
        mlir_type="linalg",
    ):
        if self.inputs == None:
            print(
                f"There is no input provided: {self.inputs}, please provide inputs or simply run import_mlir."
            )
            sys.exit(1)
        model_name_mlir = (
            model_name + "_" + self.frontend + "_" + mlir_type + ".mlir"
        )
        artifact_path = os.path.join(dir, model_name_mlir)
        imported_mlir = self.import_mlir(
            is_dynamic,
            tracing_required,
            func_name,
            save_dir=artifact_path,
            mlir_type=mlir_type,
        )
        # TODO: Make sure that any generic function name is accepted. Currently takes in the default function names.
        # TODO: Check for multiple outputs.
        if self.frontend in ["torch", "pytorch"]:
            import torch

            golden_out = None
            if golden_values is not None:
                golden_out = golden_values
            else:
                golden_out = self.module(*self.inputs)
            if torch.is_tensor(golden_out):
                golden_out = tuple(
                    golden_out.detach().cpu().numpy(),
                )
            else:
                golden_out = self.convert_to_numpy(golden_out)
            # Save the artifacts in the directory dir.
            self.save_data(
                dir,
                model_name,
                imported_mlir[0],
                imported_mlir[1],
                self.inputs,
                golden_out,
                mlir_type,
            )
            return (
                imported_mlir,
                self.convert_to_numpy(self.inputs),
                golden_out,
            )
        if self.frontend in ["tf", "tensorflow"]:
            import tensorflow as tf

            golden_out = self.module.forward(*self.inputs)
            if tf.is_tensor(golden_out):
                golden_out = tuple(
                    golden_out.numpy(),
                )
            elif golden_out is tuple:
                golden_out = self.convert_to_numpy(golden_out)
            elif hasattr(golden_out, "logits"):
                # from transformers import TFSequenceClassifierOutput
                golden_out = golden_out.logits
            else:
                golden_out = golden_out.last_hidden_state
            # Save the artifacts in the directory dir.
            self.save_data(
                dir,
                model_name,
                imported_mlir[0],
                imported_mlir[1],
                self.inputs,
                golden_out,
            )
            return (
                imported_mlir,
                self.convert_to_numpy(self.inputs),
                golden_out,
            )
        if self.frontend in ["tflite", "tf-lite"]:
            # TODO(Chi): Validate it for tflite models.
            golden_out = self.module.invoke_tflite(self.inputs)
            self.save_data(
                dir,
                model_name,
                imported_mlir[0],
                imported_mlir[1],
                self.inputs,
                golden_out,
            )
            return (
                imported_mlir,
                self.inputs,
                golden_out,
            )


def get_f16_inputs(inputs, is_f16, f16_input_mask):
    if is_f16 == False:
        return inputs
    if f16_input_mask == None:
        return tuple([x.half() for x in inputs])

    f16_masked_inputs = []
    for i in range(len(inputs)):
        if f16_input_mask[i]:
            f16_masked_inputs.append(inputs[i].half())
        else:
            f16_masked_inputs.append(inputs[i])

    return tuple(f16_masked_inputs)


# Upcasts the block/list of ops.
def add_upcast(fx_g):
    import torch

    for node in fx_g.graph.nodes:
        if node.target in [torch.ops.aten.mul]:
            # This is a very strict check.
            if hasattr(node.args[1], "target"):
                if (
                    node.args[1].target in [torch.ops.aten.rsqrt]
                    and node.args[1].args[0].target in [torch.ops.aten.add]
                    and node.args[1].args[0].args[0].target
                    in [torch.ops.aten.mean]
                    and node.args[1].args[0].args[0].args[0].target
                    in [torch.ops.aten.pow]
                ):
                    print("found an upcasting block let's upcast it.")
                    pow_node = node.args[1].args[0].args[0].args[0]
                    mul_node = node
                    with fx_g.graph.inserting_before(pow_node):
                        lhs = pow_node.args[0]
                        upcast_lhs = fx_g.graph.call_function(
                            torch.ops.aten._to_copy,
                            args=(lhs,),
                            kwargs={"dtype": torch.float32},
                        )
                        pow_node.args = (upcast_lhs, pow_node.args[1])
                    with fx_g.graph.inserting_before(mul_node):
                        new_node = fx_g.graph.call_function(
                            torch.ops.aten._to_copy,
                            args=(mul_node,),
                            kwargs={"dtype": torch.float16},
                        )
                        mul_node.append(new_node)
                        mul_node.replace_all_uses_with(new_node)
                        new_node.args = (mul_node,)
                        new_node.kwargs = {"dtype": torch.float16}

    fx_g.graph.lint()


def transform_fx(fx_g, quantized=False):
    import torch

    kwargs_dict = {
        "dtype": torch.float16,
        "device": torch.device(type="cpu"),
        "pin_memory": False,
    }
    kwargs_dict1 = {
        "dtype": torch.float16,
    }
    for node in fx_g.graph.nodes:
        if node.op == "call_function":
            # aten.empty should be filled with zeros.
            if node.target in [torch.ops.aten.empty]:
                with fx_g.graph.inserting_after(node):
                    new_node = fx_g.graph.call_function(
                        torch.ops.aten.zero_,
                        args=(node,),
                    )
                    node.append(new_node)
                    node.replace_all_uses_with(new_node)
                    new_node.args = (node,)
            if quantized:
                continue

            if node.target in [
                torch.ops.aten.arange,
                torch.ops.aten.empty,
                torch.ops.aten.zeros,
                torch.ops.aten.zeros_like,
            ]:
                if node.kwargs.get("dtype") == torch.float32:
                    node.kwargs = kwargs_dict

            # Vicuna
            if node.target in [
                torch.ops.aten._to_copy,
            ]:
                if node.kwargs.get("dtype") == torch.float32:
                    node.kwargs = kwargs_dict1

            if node.target in [
                torch.ops.aten.masked_fill,
            ]:
                if node.args[2] > torch.finfo(torch.half).max:
                    max_val = torch.finfo(torch.half).max
                    node.args = (node.args[0], node.args[1], max_val)
                elif node.args[2] < torch.finfo(torch.half).min:
                    min_val = torch.finfo(torch.half).min
                    node.args = (node.args[0], node.args[1], min_val)

            if node.target in [
                torch.ops.aten.full,
            ]:
                if node.args[1] > torch.finfo(torch.half).max:
                    max_val = torch.finfo(torch.half).max
                    node.args = (node.args[0], max_val)
                    node.kwargs = kwargs_dict
                elif node.args[1] < torch.finfo(torch.half).min:
                    min_val = torch.finfo(torch.half).min
                    node.args = (node.args[0], min_val)
                    node.kwargs = kwargs_dict

            # Inputs and outputs of aten.var.mean should be upcasted to fp32.
            if node.target in [torch.ops.aten.var_mean]:
                with fx_g.graph.inserting_before(node):
                    new_node = fx_g.graph.call_function(
                        torch.ops.prims.convert_element_type,
                        args=(node.args[0], torch.float32),
                        kwargs={},
                    )
                    node.args = (new_node, node.args[1])

            if node.name.startswith("getitem"):
                with fx_g.graph.inserting_before(node):
                    if node.args[0].target in [torch.ops.aten.var_mean]:
                        new_node = fx_g.graph.call_function(
                            torch.ops.aten._to_copy,
                            args=(node,),
                            kwargs={"dtype": torch.float16},
                        )
                        node.append(new_node)
                        node.replace_all_uses_with(new_node)
                        new_node.args = (node,)
                        new_node.kwargs = {"dtype": torch.float16}

    # Required for cuda debugging.
    # for node in fx_g.graph.nodes:
    # if node.op == "call_function":
    # if node.kwargs.get("device") == torch.device(type="cpu"):
    # new_kwargs = node.kwargs.copy()
    # new_kwargs["device"] = torch.device(type="cuda")
    # node.kwargs = new_kwargs

    fx_g.graph.lint()


def gptq_transforms(fx_g):
    import torch

    for node in fx_g.graph.nodes:
        if node.op == "call_function":
            if node.target in [
                torch.ops.aten.arange,
                torch.ops.aten.empty,
                torch.ops.aten.ones,
                torch.ops.aten._to_copy,
            ]:
                if node.kwargs.get("device") == torch.device(device="cuda:0"):
                    updated_kwargs = node.kwargs.copy()
                    updated_kwargs["device"] = torch.device(device="cpu")
                    node.kwargs = updated_kwargs

            if node.target in [
                torch.ops.aten._to_copy,
            ]:
                if node.kwargs.get("dtype") == torch.bfloat16:
                    updated_kwargs = node.kwargs.copy()
                    updated_kwargs["dtype"] = torch.float16
                    node.kwargs = updated_kwargs

            # Inputs of aten.native_layer_norm should be upcasted to fp32.
            if node.target in [torch.ops.aten.native_layer_norm]:
                with fx_g.graph.inserting_before(node):
                    new_node_arg0 = fx_g.graph.call_function(
                        torch.ops.prims.convert_element_type,
                        args=(node.args[0], torch.float32),
                        kwargs={},
                    )
                    node.args = (
                        new_node_arg0,
                        node.args[1],
                        node.args[2],
                        node.args[3],
                        node.args[4],
                    )

            # Inputs of aten.mm should be upcasted to fp32.
            if node.target in [torch.ops.aten.mm]:
                with fx_g.graph.inserting_before(node):
                    new_node_arg0 = fx_g.graph.call_function(
                        torch.ops.prims.convert_element_type,
                        args=(node.args[0], torch.float32),
                        kwargs={},
                    )
                    new_node_arg1 = fx_g.graph.call_function(
                        torch.ops.prims.convert_element_type,
                        args=(node.args[1], torch.float32),
                        kwargs={},
                    )
                    node.args = (new_node_arg0, new_node_arg1)

            # Outputs of aten.mm should be downcasted to fp16.
            if type(node.args[0]) == torch.fx.node.Node and node.args[
                0
            ].target in [torch.ops.aten.mm]:
                with fx_g.graph.inserting_before(node):
                    tmp = node.args[0]
                    new_node = fx_g.graph.call_function(
                        torch.ops.aten._to_copy,
                        args=(node.args[0],),
                        kwargs={"dtype": torch.float16},
                    )
                    node.args[0].append(new_node)
                    node.args[0].replace_all_uses_with(new_node)
                    new_node.args = (tmp,)
                    new_node.kwargs = {"dtype": torch.float16}

            # Inputs of aten._softmax should be upcasted to fp32.
            if node.target in [torch.ops.aten._softmax]:
                with fx_g.graph.inserting_before(node):
                    new_node_arg0 = fx_g.graph.call_function(
                        torch.ops.prims.convert_element_type,
                        args=(node.args[0], torch.float32),
                        kwargs={},
                    )
                    node.args = (new_node_arg0, node.args[1], node.args[2])

            # Outputs of aten._softmax should be downcasted to fp16.
            if (
                type(node.args[0]) == torch.fx.node.Node
                and node.args[0].target in [torch.ops.aten._softmax]
                and node.target in [torch.ops.aten.expand]
            ):
                with fx_g.graph.inserting_before(node):
                    tmp = node.args[0]
                    new_node = fx_g.graph.call_function(
                        torch.ops.aten._to_copy,
                        args=(node.args[0],),
                        kwargs={"dtype": torch.float16},
                    )
                    node.args[0].append(new_node)
                    node.args[0].replace_all_uses_with(new_node)
                    new_node.args = (tmp,)
                    new_node.kwargs = {"dtype": torch.float16}

    fx_g.graph.lint()


# Doesn't replace the None type.
def change_fx_graph_return_to_tuple(fx_g):
    for node in fx_g.graph.nodes:
        if node.op == "output":
            # output nodes always have one argument
            node_arg = node.args[0]
            out_nodes = []
            if isinstance(node_arg, list):
                # Don't return NoneType elements.
                for out_node in node_arg:
                    if not isinstance(out_node, type(None)):
                        out_nodes.append(out_node)
                # If there is a single tensor/element to be returned don't
                # a tuple for it.
                if len(out_nodes) == 1:
                    node.args = out_nodes
                else:
                    node.args = (tuple(out_nodes),)
    fx_g.graph.lint()
    fx_g.recompile()
    return fx_g


def flatten_training_input(inputs):
    flattened_input = []
    for i in inputs:
        if isinstance(i, dict):
            for value in i.values():
                flattened_input.append(value.detach())
        elif isinstance(i, tuple):
            for value in i:
                flattened_input.append(value)
        else:
            flattened_input.append(i)
    return tuple(flattened_input)


# TODO: Remove is_f16 and fix all calls with using precision instead
# Applies fx conversion to the model and imports the mlir.
def import_with_fx(
    model,
    inputs,
    is_f16=False,
    f16_input_mask=None,
    debug=False,
    training=False,
    return_str=False,
    save_dir=tempfile.gettempdir(),
    model_name="model",
    mlir_type="linalg",
    is_dynamic=False,
    tracing_required=False,
    precision="fp32",
    is_gptq=False,
):
    import torch
    from torch.fx.experimental.proxy_tensor import make_fx
    from torch._decomp import get_decompositions
    from typing import List

    golden_values = None
    if debug:
        try:
            golden_values = model(*inputs)
        except:
            golden_values = None

    def _remove_nones(fx_g: torch.fx.GraphModule) -> List[int]:
        removed_indexes = []
        for node in fx_g.graph.nodes:
            if node.op == "output":
                assert (
                    len(node.args) == 1
                ), "Output node must have a single argument"
                node_arg = node.args[0]
                if isinstance(node_arg, (list, tuple)):
                    node_arg = list(node_arg)
                    node_args_len = len(node_arg)
                    for i in range(node_args_len):
                        curr_index = node_args_len - (i + 1)
                        if node_arg[curr_index] is None:
                            removed_indexes.append(curr_index)
                            node_arg.pop(curr_index)
                    node.args = (tuple(node_arg),)
                    break

        if len(removed_indexes) > 0:
            fx_g.graph.lint()
            fx_g.graph.eliminate_dead_code()
            fx_g.recompile()
        removed_indexes.sort()
        return removed_indexes

    def _unwrap_single_tuple_return(fx_g: torch.fx.GraphModule) -> bool:
        """
        Replace tuple with tuple element in functions that return one-element tuples.
        Returns true if an unwrapping took place, and false otherwise.
        """
        unwrapped_tuple = False
        for node in fx_g.graph.nodes:
            if node.op == "output":
                assert (
                    len(node.args) == 1
                ), "Output node must have a single argument"
                node_arg = node.args[0]
                if isinstance(node_arg, tuple):
                    if len(node_arg) == 1:
                        node.args = (node_arg[0],)
                        unwrapped_tuple = True
                        break

        if unwrapped_tuple:
            fx_g.graph.lint()
            fx_g.recompile()
        return unwrapped_tuple

    # TODO: Control the decompositions.
    decomps_list = [
        torch.ops.aten.embedding_dense_backward,
        torch.ops.aten.native_layer_norm_backward,
        torch.ops.aten.slice_backward,
        torch.ops.aten.select_backward,
        torch.ops.aten.norm.ScalarOpt_dim,
        torch.ops.aten.native_group_norm,
        torch.ops.aten.upsample_bilinear2d.vec,
        torch.ops.aten.split.Tensor,
        torch.ops.aten.split_with_sizes,
        torch.ops.aten.native_layer_norm,
        torch.ops.aten.masked_fill.Tensor,
        torch.ops.aten.masked_fill.Scalar,
        torch.ops.aten._scaled_dot_product_flash_attention.default,
        torch.ops.aten.index_add,
        torch.ops.aten.index_add_,
    ]
    if precision in ["int4", "int8"] and not is_gptq:
        from brevitas_examples.llm.llm_quant.export import (
            block_quant_layer_level_manager,
        )
        from brevitas_examples.llm.llm_quant.export import (
            brevitas_layer_export_mode,
        )
        from brevitas_examples.llm.llm_quant.sharded_mlir_group_export import (
            LinearWeightBlockQuantHandlerFwd,
        )
        from brevitas_examples.llm.llm_quant.export import (
            replace_call_fn_target,
        )
        from brevitas_examples.llm.llm_quant.sharded_mlir_group_export import (
            matmul_rhs_group_quant_placeholder,
        )
        from brevitas.backport.fx.experimental.proxy_tensor import (
            make_fx as brevitas_make_fx,
        )

        export_context_manager = brevitas_layer_export_mode
        export_class = block_quant_layer_level_manager(
            export_handlers=[LinearWeightBlockQuantHandlerFwd]
        )
        with export_context_manager(model, export_class):
            fx_g = brevitas_make_fx(
                model,
                decomposition_table=get_decompositions(decomps_list),
            )(*inputs)

        transform_fx(fx_g, quantized=True)
        replace_call_fn_target(
            fx_g,
            src=matmul_rhs_group_quant_placeholder,
            target=torch.ops.quant.matmul_rhs_group_quant,
        )

        fx_g.recompile()
        removed_none_indexes = _remove_nones(fx_g)
        was_unwrapped = _unwrap_single_tuple_return(fx_g)
    else:
        fx_g = make_fx(
            model,
            decomposition_table=get_decompositions(decomps_list),
        )(*inputs)

    fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
    fx_g.recompile()

    def strip_overloads(gm):
        """
        Modifies the target of graph nodes in :attr:`gm` to strip overloads.
        Args:
            gm(fx.GraphModule): The input Fx graph module to be modified
        """
        for node in gm.graph.nodes:
            if isinstance(node.target, torch._ops.OpOverload):
                node.target = node.target.overloadpacket
        gm.recompile()

    strip_overloads(fx_g)

    if is_f16:
        fx_g = fx_g.half()
        transform_fx(fx_g)
        # TODO: Have to make it more generic.
        add_upcast(fx_g)
        fx_g.recompile()

    if is_gptq:
        gptq_transforms(fx_g)
        fx_g.recompile()

    if mlir_type == "fx":
        return fx_g

    if training:
        change_fx_graph_return_to_tuple(fx_g)
        inputs = flatten_training_input(inputs)

    ts_graph = torch.jit.script(fx_g)
    if mlir_type == "torchscript":
        return ts_graph

    inputs = get_f16_inputs(inputs, is_f16, f16_input_mask)
    mlir_importer = SharkImporter(
        ts_graph,
        inputs,
        frontend="torch",
        return_str=return_str,
    )

    if debug:  # and not is_f16:
        (mlir_module, func_name), _, _ = mlir_importer.import_debug(
            dir=save_dir,
            model_name=model_name,
            golden_values=golden_values,
            mlir_type=mlir_type,
            is_dynamic=is_dynamic,
            tracing_required=tracing_required,
        )
        return mlir_module, func_name

    mlir_module, func_name = mlir_importer.import_mlir(mlir_type=mlir_type)
    return mlir_module, func_name


# Saves a .mlir module python object to the directory 'dir' with 'model_name' and returns a path to the saved file.
def save_mlir(
    mlir_module,
    model_name,
    mlir_dialect="linalg",
    frontend="torch",
    dir="",
):
    model_name_mlir = (
        model_name + "_" + frontend + "_" + mlir_dialect + ".mlir"
    )
    if dir == "":
        dir = os.path.join(".", "shark_tmp")
    mlir_path = os.path.join(dir, model_name_mlir)
    print(f"saving {model_name_mlir} to {dir}")
    if not os.path.exists(dir):
        os.makedirs(dir)
    if frontend == "torch":
        with open(mlir_path, "wb") as mlir_file:
            mlir_file.write(mlir_module)

    return mlir_path

```

`shark/shark_inference.py`:

```py
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from shark.iree_utils.compile_utils import (
    export_iree_module_to_vmfb,
    load_flatbuffer,
    create_dispatch_dirs,
    compile_benchmark_dirs,
)
import os
from shark.shark_runner import SharkRunner
from shark.parser import shark_args
import numpy as np


dtype_to_np_dtype = {
    "f32": np.float32,
    "f64": np.float64,
    "i32": np.int32,
    "i64": np.int64,
    "i1": np.bool_,
}


class SharkInference:
    """
    Runs prediction or inference on mlir_module.

    ...

    Attributes
    ----------
    mlir_module : str
        mlir_module or path represented in string; modules from torch-mlir are serialized in bytecode format.
    device : str
        device to execute the mlir_module on.
        currently supports cpu, cuda, vulkan, and metal backends.
    mlir_dialect: str
        The dialect in which the given mlir_module is in.
        Refer to {https://mlir.llvm.org/docs/Dialects/}
    is_benchmark: bool
        Whether this SharkInference module should be benchmark-enabled.
    mmap: bool
        Whether to load/run vmfb using mmap. It's `True` by default.

    Methods
    -------
    __call__(function_name, inputs=None):
        Runs the function with `function_name` within the mlir_module along
        with the given inputs, if the inputs are not given it autogenerates the
        inputs. Also, the inputs should be a numpy array.
    input_info():
        Gives the information about the inputs required by the `function_name`.
        This can be expensive as it does string matching to do so.

    """

    def __init__(
        self,
        mlir_module,
        device: str = "none",
        mlir_dialect: str = "linalg",
        is_benchmark: bool = False,
        dispatch_benchmark: str = None,
        dispatch_benchmark_dir: str = "temp_dispatch_benchmarks",
        device_idx: int = None,
        mmap: bool = True,
        rt_flags: list = [],
    ):
        self.mlir_module = mlir_module
        if mlir_module is not None:
            if mlir_module and not os.path.isfile(mlir_module):
                print(
                    "Warning: Initializing SharkInference with a mlir string/bytecode object will duplicate the model in RAM at compile time. To avoid this, initialize SharkInference with a path to a MLIR module on your hard disk instead."
                )
                self.compile_str = True
            else:
                self.compile_str = False
        self.device = shark_args.device if device == "none" else device
        self.mlir_dialect = mlir_dialect
        self.is_benchmark = is_benchmark
        self.device_idx = device_idx
        self.dispatch_benchmarks = (
            shark_args.dispatch_benchmarks
            if dispatch_benchmark is None
            else dispatch_benchmark
        )
        self.dispatch_benchmarks_dir = (
            shark_args.dispatch_benchmarks_dir
            if dispatch_benchmark_dir == "temp_dispatch_benchmarks"
            else dispatch_benchmark_dir
        )

        self.shark_runner = None
        self.mmap = mmap
        self.rt_flags = rt_flags

    def compile(self, extra_args=[]):
        if self.dispatch_benchmarks is not None:
            extra_args.append(
                f"--iree-hal-dump-executable-sources-to={self.dispatch_benchmarks_dir}"
            )
            extra_args.append(
                f"--iree-hal-dump-executable-binaries-to={self.dispatch_benchmarks_dir}"
            )
            temp_dir = self.dispatch_benchmarks_dir.split("/")
            temp_dir[-1] = "temp_" + temp_dir[-1]
            temp_dir = "/".join(temp_dir)
            self.temp_dispatch_benchmarks_dir = temp_dir
            extra_args.append(
                f"--iree-hal-dump-executable-benchmarks-to={self.temp_dispatch_benchmarks_dir}"
            )

        if self.is_benchmark == True:
            from shark.shark_benchmark_runner import SharkBenchmarkRunner

            self.shark_runner = SharkBenchmarkRunner(
                self.mlir_module,
                self.device,
                self.mlir_dialect,
                extra_args=extra_args,
            )

        else:
            self.shark_runner = SharkRunner(
                self.mlir_module,
                self.device,
                self.mlir_dialect,
                extra_args=extra_args,
                device_idx=self.device_idx,
                rt_flags=self.rt_flags,
            )

        if self.dispatch_benchmarks is not None:
            create_dispatch_dirs(self.dispatch_benchmarks_dir, self.device)
            compile_benchmark_dirs(
                self.dispatch_benchmarks_dir,
                self.device,
                self.dispatch_benchmarks,
            )
            os.system(f"rm -rf {self.temp_dispatch_benchmarks_dir}")

    # inputs are considered to be tuple of np.array.
    def __call__(self, function_name: str, inputs: tuple, send_to_host=True):
        return self.shark_runner.run(
            function_name, inputs, send_to_host, device=self.device
        )

    # forward function.
    def forward(self, inputs: tuple, send_to_host=True):
        return self.shark_runner.run(
            "forward", inputs, send_to_host, device=self.device
        )

    # Get all function names defined within the compiled module.
    def get_functions_in_module(self):
        return self.shark_runner.get_functions_in_module()

    # Captures the static input information from the mlir_module.
    # TODO(pashu123): Generate the input information for dynamic shapes.
    def _input_info(self, function_name):
        # func_key to get the line which contains the function.
        func_key = "func.func @" + function_name
        func_header = None
        for line in str(self.mlir_module).splitlines():
            if func_key in line:
                func_header = line
                break
        if func_header is None:
            print(f"Function: {function_name} not found")

        import re

        inputs = re.findall("\(.*?\)", func_header)[0].split(",")
        shapes = []
        dtype = []
        for inp in inputs:
            shape_dtype = re.findall(r"<[^>]*>", inp)[0].split("x")
            shape_dtype[0], shape_dtype[-1] = (
                shape_dtype[0][1:],
                shape_dtype[-1][:-1],
            )
            shapes.append(tuple([int(x) for x in shape_dtype[:-1]]))
            dtype.append(shape_dtype[-1])

        return shapes, dtype

    # Generates random input to be feed into the graph.
    def generate_random_inputs(self, low=0, high=1):
        shapes, dtype = self._input_info()
        inputs = []
        for i, j in zip(shapes, dtype):
            inputs.append(
                np.random.uniform(low, high, size=i).astype(
                    dtype_to_np_dtype[j]
                )
            )
        return tuple(inputs)

    # TODO: Instead of passing directory and having names decided by the module
    # , user may want to save the module with manual names.
    def save_module(
        self, dir=os.getcwd(), module_name=None, extra_args=[], debug=False
    ):
        return export_iree_module_to_vmfb(
            self.mlir_module,
            self.device,
            dir,
            self.mlir_dialect,
            module_name=module_name,
            extra_args=extra_args,
            debug=debug,
            compile_str=self.compile_str,
        )

    # load and return the module.
    def load_module(self, path, extra_args=[]):
        self.shark_runner = SharkRunner(
            device=self.device,
            compile_vmfb=False,
            extra_args=extra_args,
            rt_flags=self.rt_flags,
        )
        params = load_flatbuffer(
            path,
            self.device,
            self.device_idx,
            mmap=self.mmap,
            rt_flags=self.rt_flags,
        )
        self.shark_runner.iree_compilation_module = params["vmfb"]
        self.shark_runner.iree_config = params["config"]
        self.shark_runner.temp_file_to_unlink = params["temp_file_to_unlink"]
        del params
        return

```

`shark/shark_runner.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from shark.iree_utils.compile_utils import (
    get_iree_compiled_module,
    get_results,
    export_iree_module_to_vmfb,
    load_flatbuffer,
)
from shark.iree_utils._common import check_device_drivers, device_driver_info
from shark.parser import shark_args
import os
import sys


# supported dialects by the shark-runtime.
supported_dialects = {
    "linalg",
    "auto",
    "stablehlo",
    "tosa",
    "tf-lite",
    "tm_tensor",
}


class SharkRunner:
    """
    Base class for SharkInference and SharkTrainer
    used to execute an mlir_module.

    ...

    Attributes
    ----------
    mlir_module : str
        mlir_module path, string, or bytecode.
    device : str
        device to execute the mlir_module on.
        currently supports cpu, cuda, vulkan, and metal backends.
    mlir_dialect: str
        The dialect in which the given mlir_module is in.
        Refer to {https://mlir.llvm.org/docs/Dialects/}

    Methods
    -------
    run(function_name, inputs=None):
        Runs the function with `function_name` within the mlir_module along
        with the given inputs, if the inputs are not given it autogenerates the
        inputs. Also, the inputs should be a numpy array.
    input_info():
        Gives the information about the inputs required by the `function_name`.
        This can be expensive as it does string matching to do so.
    """

    def __init__(
        self,
        mlir_module: bytes = None,
        device: str = "none",
        mlir_dialect: str = "linalg",
        extra_args: list = [],
        compile_vmfb: bool = True,
        device_idx: int = None,
        rt_flags: list = [],
    ):
        self.mlir_module = mlir_module
        if self.mlir_module is not None:
            if not os.path.isfile(mlir_module):
                print(
                    "Warning: Initializing SharkRunner with a mlir string/bytecode object will duplicate the model in RAM at compile time. To avoid this, initialize SharkInference with a path to a MLIR module on your hard disk instead."
                )
                self.compile_str = True
            else:
                self.compile_str = False
        self.device = shark_args.device if device == "none" else device
        self.mlir_dialect = mlir_dialect
        self.extra_args = extra_args
        self.device_idx = device_idx
        self.rt_flags = rt_flags

        if check_device_drivers(self.device):
            print(device_driver_info(self.device))
            sys.exit(1)

        if compile_vmfb == True:
            # Compile the module to get the .vmfb.
            params = get_iree_compiled_module(
                self.mlir_module,
                self.device,
                self.mlir_dialect,
                extra_args=self.extra_args,
                device_idx=self.device_idx,
                rt_flags=self.rt_flags,
                compile_str=self.compile_str,
            )
            self.iree_compilation_module = params["vmfb"]
            self.iree_config = params["config"]
            self.temp_file_to_unlink = params["temp_file_to_unlink"]
            del params

    def run(
        self, function_name, inputs: tuple, send_to_host=False, device=None
    ):
        return get_results(
            self.iree_compilation_module,
            function_name,
            inputs,
            self.iree_config,
            self.mlir_dialect,
            send_to_host,
            device=device,
        )

    # Get all function names defined within the compiled module.
    def get_functions_in_module(self):
        return self.iree_compilation_module._vm_module.function_names

```

`shark/shark_trainer.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from shark.parser import shark_args
from shark.shark_runner import SharkRunner
from shark.backward_makefx import MakeFxModule
from shark.shark_importer import import_with_fx, save_mlir
import numpy as np
from tqdm import tqdm
import sys


# Prints to stderr.
def print_err(*a):
    print(*a, file=sys.stderr)


class SharkTrainer:
    """Training pytorch, tensorflow module on shark runtime."""

    def __init__(
        self,
        model,
        input: tuple,
        dynamic: bool = False,
        device: str = None,
        jit_trace: bool = False,
        from_aot: bool = True,
    ):
        self.model = model
        # Change tuple to list.
        self.input = [x for x in input]
        self.dynamic = dynamic
        self.from_aot = from_aot
        self.jit_trace = jit_trace
        self.from_aot = from_aot

        # By default it's the torch frontend.
        self.frontend = "pytorch"
        self.device = device if device is not None else shark_args.device

        self.shark_runner = None

    # Sets the frontend i.e `pytorch` or `tensorflow`.
    def set_frontend(self, frontend: str):
        if frontend not in [
            "pytorch",
            "torch",
            "tensorflow",
            "tf",
            "stablehlo",
            "mhlo",
            "linalg",
            "tosa",
        ]:
            print_err("frontend not supported.")
        else:
            self.frontend = frontend

    # Training function is needed in the case of torch_fn.
    def compile(self, training_fn=None, mlir_type="linalg", extra_args=[]):
        if self.frontend in ["torch", "pytorch"]:
            packed_inputs = (
                dict(self.model.named_parameters()),
                dict(self.model.named_buffers()),
                tuple(self.input),
            )
            mlir_module, func_name = import_with_fx(
                training_fn,
                packed_inputs,
                False,
                [],
                training=True,
                mlir_type=mlir_type,
            )
            mlir_module = save_mlir(
                mlir_module,
                model_name="shark_model",
                frontend="torch",
                mlir_dialect=mlir_type,
            )
            self.shark_runner = SharkRunner(
                mlir_module,
                self.device,
                "tm_tensor",
                extra_args=extra_args,
            )
        elif self.frontend in ["tensorflow", "tf", "mhlo", "stablehlo"]:
            self.shark_runner = SharkRunner(
                self.model,
                self.input,
                self.dynamic,
                self.device,
                self.jit_trace,
                self.from_aot,
                self.frontend,
            )
        else:
            print_err("Unknown frontend")
            return

    # The inputs to the mlir-graph are weights, buffers and inputs respectively.
    def get_torch_params(self):
        params = [i.detach() for i in self.model.parameters()]
        buffers = [i.detach() for i in self.model.buffers()]
        return params + buffers

    # Function to train pytorch module.
    def _train_torch(self, num_iters):
        """Returns the updated weights after num_iters"""
        params = self.get_torch_params()
        params = [x.numpy() for x in params]
        print(f"Training started for {num_iters} iterations:")
        for i in tqdm(range(num_iters)):
            params = self.shark_runner.run(
                "forward", params + self.input, self.frontend
            )

        return params

    # Function to train tensorflow module.
    # Output final loss.
    # TODO(raikonenfnu): Save updated weight/states in SHARK.
    def _train_tf(self, num_iters):
        input_list = []
        for x in self.input:
            if isinstance(x, list):
                nested_list = []
                for val in x:
                    if isinstance(val, np.ndarray):
                        nested_list.append(val)
                    else:
                        nested_list.append(val.numpy())
                input_list.append(nested_list)
            elif isinstance(x, np.ndarray):
                input_list.append(x)
            else:
                input_list.append(x.numpy())

        print(f"Training started for {num_iters} iterations:")
        for i in tqdm(range(num_iters)):
            outputs = self.shark_runner.forward(input_list, self.frontend)
        return outputs

    def train(self, num_iters=1):
        if self.frontend in ["torch", "pytorch"]:
            return self._train_torch(num_iters)
        elif self.frontend in ["tf", "tensorflow", "mhlo"]:
            return self._train_tf(num_iters)
        else:
            print_err("Unknown frontend")
            return

```

`shark/stress_test.py`:

```py
# Copyright 2022 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from iree.runtime import query_available_drivers, get_driver
from shark.shark_downloader import download_model
from shark.shark_inference import SharkInference
from typing import List, Optional, Tuple
import numpy as np
import argparse
from shark.iree_utils._common import _IREE_DEVICE_MAP
import multiprocessing
from shark.shark_runner import supported_dialects
import logging
from concurrent.futures import ProcessPoolExecutor
from concurrent.futures.thread import ThreadPoolExecutor
import time
import numpy as np

IREE_TO_SHARK_DRIVER_MAP = {v: k for k, v in _IREE_DEVICE_MAP.items()}


def stress_test_compiled_model(
    shark_module_path: str,
    function_name: str,
    device: str,
    inputs: List[np.ndarray],
    golden_out: List[np.ndarray],
    batch_size: int,
    max_iterations: int,
    max_duration_seconds: float,
    inference_timeout_seconds: float,
    tolerance_nulp: int,
    stress_test_index: int,
):
    logging.info(
        f"Running stress test {stress_test_index} on device {device}."
    )
    # All interactions with the module must run in a single thread.
    # We are using execution in a sperate thread in order to be able
    # to wait with a timeout on the inference operation.
    module_executor = ThreadPoolExecutor(1)
    shark_module = module_executor.submit(
        SharkInference,
        mlir_module=bytes(),
        function_name=function_name,
        device=device,
    ).result()
    module_executor.submit(
        shark_module.load_module, shark_module_path
    ).result()
    input_batches = [np.repeat(arr, batch_size, axis=0) for arr in inputs]
    golden_output_batches = np.repeat(golden_out, batch_size, axis=0)
    report_interval_seconds = 10
    start_time = time.time()
    previous_report_time = start_time
    first_iteration_output = None
    for i in range(max_iterations):
        output = module_executor.submit(
            shark_module.forward, input_batches
        ).result(inference_timeout_seconds)
        if first_iteration_output is None:
            np.testing.assert_array_almost_equal_nulp(
                golden_output_batches, output, nulp=tolerance_nulp
            )
            first_iteration_output = output
        else:
            np.testing.assert_array_equal(output, first_iteration_output)
        current_time = time.time()
        if report_interval_seconds < current_time - previous_report_time:
            logging.info(
                f"Stress test {stress_test_index} on device "
                f"{device} at iteration {i+1}"
            )
            previous_report_time = current_time
        if max_duration_seconds < current_time - start_time:
            return
    logging.info(f"Stress test {stress_test_index} on device {device} done.")


def get_device_type(device_name: str):
    return device_name.split("://", 1)[0]


def get_device_types(device_names: str):
    return [get_device_type(device_name) for device_name in device_names]


def query_devices(device_types: Optional[List[str]] = None) -> List[str]:
    devices = []
    if device_types is None:
        device_types = [
            IREE_TO_SHARK_DRIVER_MAP[name]
            for name in query_available_drivers()
            if name in IREE_TO_SHARK_DRIVER_MAP
        ]
    for device_type in device_types:
        driver = get_driver(_IREE_DEVICE_MAP[device_type])
        device_infos = driver.query_available_devices()
        for device_info in device_infos:
            uri_path = (
                device_info["path"]
                if device_info["path"] != ""
                else str(device_info["device_id"])
            )
            device_uri = f"{device_type}://{uri_path}"
            devices.append(device_uri)
    return devices


def compile_stress_test_module(
    device_types: List[str], mlir_model: str, func_name: str, mlir_dialect: str
) -> List[str]:
    shark_module_paths = []
    for device_type in device_types:
        logging.info(
            f"Compiling stress test model for device type {device_type}."
        )
        shark_module = SharkInference(
            mlir_model,
            func_name,
            mlir_dialect=mlir_dialect,
            device=device_type,
        )
        shark_module_paths.append(shark_module.save_module())
    return shark_module_paths


def stress_test(
    model_name: str,
    dynamic_model: bool = False,
    device_types: Optional[List[str]] = None,
    device_names: Optional[List[str]] = None,
    batch_size: int = 1,
    max_iterations: int = 10**7,
    max_duration_seconds: float = 3600,
    inference_timeout_seconds: float = 60,
    mlir_dialect: str = "linalg",
    frontend: str = "torch",
    oversubscription_factor: int = 1,
    tolerance_nulp: int = 50000,
):
    logging.info(f"Downloading stress test model {model_name}.")
    mlir_model, func_name, inputs, golden_out = download_model(
        model_name=model_name, dynamic=dynamic_model, frontend=frontend
    )

    if device_names is None or device_types is not None:
        device_names = [] if device_names is None else device_names
        with ProcessPoolExecutor() as executor:
            # query_devices needs to run in a separate process,
            # because it will interfere with other processes that are forked later.
            device_names.extend(
                executor.submit(query_devices, device_types).result()
            )

    device_types_set = list(set(get_device_types(device_names)))
    with ProcessPoolExecutor() as executor:
        # This needs to run in a subprocess because when compiling for CUDA,
        # some stuff get intialized and cuInit will fail in a forked process
        # later. It should be just compiling, but alas.
        shark_module_paths_set = executor.submit(
            compile_stress_test_module,
            device_types_set,
            mlir_model,
            func_name,
            mlir_dialect,
        ).result()
    device_type_shark_module_path_map = {
        device_type: module_path
        for device_type, module_path in zip(
            device_types_set, shark_module_paths_set
        )
    }
    device_name_shark_module_path_map = {
        device_name: device_type_shark_module_path_map[
            get_device_type(device_name)
        ]
        for device_name in device_names
    }

    # This needs to run in a spearate process, because it uses the drvier chache
    # in IREE and a subsequent call to `iree.runtime.SystemContext.add_vm_module`
    # in a forked process will hang.
    with multiprocessing.Pool(
        len(device_name_shark_module_path_map) * oversubscription_factor
    ) as process_pool:
        process_pool.starmap(
            stress_test_compiled_model,
            [
                (
                    module_path,
                    func_name,
                    device_name,
                    inputs,
                    golden_out,
                    batch_size,
                    max_iterations,
                    max_duration_seconds,
                    inference_timeout_seconds,
                    tolerance_nulp,
                    stress_test_index,
                )
                for stress_test_index, (device_name, module_path) in enumerate(
                    list(device_name_shark_module_path_map.items())
                    * oversubscription_factor
                )
            ],
        )


if __name__ == "__main__":
    logging.basicConfig(encoding="utf-8", level=logging.INFO)
    parser = argparse.ArgumentParser(
        description="Downloads, compiles and runs a model from the tank to stress test the system."
    )
    parser.add_argument(
        "--model", type=str, help="Model name in the tank.", default="alexnet"
    )
    parser.add_argument(
        "--dynamic",
        help="Use dynamic version of the model.",
        action="store_true",
        default=False,
    )
    parser.add_argument(
        "--frontend", type=str, help="Frontend of the model.", default="torch"
    )
    parser.add_argument(
        "--mlir-dialect",
        type=str,
        help="MLIR dialect of the model.",
        default="linalg",
        choices=supported_dialects,
    )
    parser.add_argument(
        "--device-types",
        type=str,
        nargs="*",
        choices=_IREE_DEVICE_MAP.keys(),
        help="Runs the stress test on all devices with that type. "
        "If absent and no deveices are specified "
        "will run against all available devices.",
    )
    parser.add_argument(
        "--devices",
        type=str,
        nargs="*",
        help="List of devices to run the stress test on. "
        "If device-types is specified will run against the union of the two.",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        help="Number of inputs to feed into the model",
        default=1,
    )
    parser.add_argument(
        "--oversubscription",
        type=int,
        help="Oversubscrption factor. Each device will execute the model simultaneously "
        "this many number of times.",
        default=1,
    )
    parser.add_argument(
        "--max-iterations",
        type=int,
        help="Maximum number of iterations to run the stress test per device.",
        default=10**7,
    )
    parser.add_argument(
        "--max-duration",
        type=float,
        help="Maximum number of seconds to run the stress test.",
        default=3600,
    )
    parser.add_argument(
        "--inference-timeout",
        type=float,
        help="Timeout in seconds for a single model inference operation.",
        default=60,
    )
    parser.add_argument(
        "--tolerance-nulp",
        type=int,
        help="The maximum number of unit in the last place for tolerance "
        "when verifing results with the golden reference output.",
        default=50000,
    )

    args = parser.parse_known_args()[0]
    stress_test(
        model_name=args.model,
        dynamic_model=args.dynamic,
        frontend=args.frontend,
        mlir_dialect=args.mlir_dialect,
        device_types=args.device_types,
        device_names=args.devices,
        batch_size=args.batch_size,
        oversubscription_factor=args.oversubscription,
        max_iterations=args.max_iterations,
        max_duration_seconds=args.max_duration,
        inference_timeout_seconds=args.inference_timeout,
        tolerance_nulp=args.tolerance_nulp,
    )

```

`shark/tests/test_shark_importer.py`:

```py
# RUN: %PYTHON %s
import numpy as np
from shark.shark_importer import SharkImporter
import pytest
from shark.parser import shark_args
from shark.shark_inference import SharkInference
from shark.tflite_utils import TFLitePreprocessor
import sys

# model_path = "https://tfhub.dev/tensorflow/lite-model/albert_lite_base/squadv1/1?lite-format=tflite"


# Inputs modified to be useful albert inputs.
def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)

    args = []
    args.append(
        np.random.randint(
            low=0,
            high=256,
            size=input_details[0]["shape"],
            dtype=input_details[0]["dtype"],
        )
    )
    args.append(
        np.ones(
            shape=input_details[1]["shape"], dtype=input_details[1]["dtype"]
        )
    )
    args.append(
        np.zeros(
            shape=input_details[2]["shape"], dtype=input_details[2]["dtype"]
        )
    )
    return args


def compare_results(mlir_results, tflite_results, details):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(details)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class AlbertTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb
        tflite_preprocessor = TFLitePreprocessor(model_name="albert_lite_base")

        raw_model_file_path = tflite_preprocessor.get_raw_model_file()
        inputs = tflite_preprocessor.get_inputs()
        tflite_interpreter = tflite_preprocessor.get_interpreter()

        my_shark_importer = SharkImporter(
            module=tflite_interpreter,
            inputs=inputs,
            frontend="tflite",
            raw_model_file=raw_model_file_path,
        )
        mlir_model, func_name = my_shark_importer.import_mlir()

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        ## post process results for compare
        input_details, output_details = tflite_preprocessor.get_model_details()
        mlir_results = list(mlir_results)
        for i in range(len(output_details)):
            dtype = output_details[i]["dtype"]
            mlir_results[i] = mlir_results[i].astype(dtype)
        tflite_results = tflite_preprocessor.get_golden_output()
        compare_results(mlir_results, tflite_results, output_details)

        # Case2: Use manually set inputs
        input_details, output_details = tflite_preprocessor.get_model_details()
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        ## post process results for compare
        tflite_results = tflite_preprocessor.get_golden_output()
        compare_results(mlir_results, tflite_results, output_details)
        # print(mlir_results)


# A specific case can be run by commenting different cases. Runs all the test
# across cpu, gpu and vulkan according to available drivers.
pytest_param = pytest.mark.parametrize(
    ("dynamic", "device"),
    [
        pytest.param(False, "cpu"),
        # TODO: Language models are failing for dynamic case..
        pytest.param(True, "cpu", marks=pytest.mark.skip),
    ],
)


@pytest_param
@pytest.mark.xfail(
    sys.platform == "darwin", reason="known macos tflite install issue"
)
def test_albert(dynamic, device):
    module_tester = AlbertTfliteModuleTester(dynamic=dynamic, device=device)
    module_tester.create_and_check_module()


if __name__ == "__main__":
    test_albert(False, "cpu")

```

`shark/tests/test_stress_test.py`:

```py
# Copyright 2022 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import pytest
import subprocess
import sys
import importlib.util


def test_stress_test():
    subprocess.check_call(
        [
            sys.executable,
            importlib.util.find_spec("shark.stress_test").origin,
            "--model=squeezenet1_0",
            "--devices",
            "cpu",
            "--max-iterations=1",
        ]
    )

```

`shark/tflite_utils.py`:

```py
import tensorflow as tf
import numpy as np
import os
import csv
import urllib.request


class TFLiteModelUtil:
    def __init__(self, raw_model_file):
        self.raw_model_file = str(raw_model_file)
        self.tflite_interpreter = None
        self.input_details = None
        self.output_details = None
        self.inputs = []

    def setup_tflite_interpreter(self):
        self.tflite_interpreter = tf.lite.Interpreter(
            model_path=self.raw_model_file
        )
        self.tflite_interpreter.allocate_tensors()
        # default input initialization
        return self.get_model_details()

    def get_model_details(self):
        print("Get tflite input output details")
        self.input_details = self.tflite_interpreter.get_input_details()
        self.output_details = self.tflite_interpreter.get_output_details()
        return self.input_details, self.output_details

    def invoke_tflite(self, inputs):
        self.inputs = inputs
        print("invoke_tflite")
        for i, input in enumerate(self.inputs):
            self.tflite_interpreter.set_tensor(
                self.input_details[i]["index"], input
            )
        self.tflite_interpreter.invoke()

        # post process tflite_result for compare with mlir_result,
        # for tflite the output is a list of numpy.tensor
        tflite_results = []
        for output_detail in self.output_details:
            tflite_results.append(
                np.array(
                    self.tflite_interpreter.get_tensor(output_detail["index"])
                )
            )

        for i in range(len(self.output_details)):
            # print("output_details ", i, "shape", self.output_details[i]["shape"].__name__,
            #       ", dtype: ", self.output_details[i]["dtype"].__name__)
            out_dtype = self.output_details[i]["dtype"]
            tflite_results[i] = tflite_results[i].astype(out_dtype)
        return tflite_results


class TFLitePreprocessor:
    def __init__(
        self,
        model_name,
        input_details=None,
        output_details=None,
        model_path=None,
    ):
        self.model_name = model_name
        self.input_details = (
            input_details  # used for tflite, optional for tf/pytorch
        )
        self.output_details = (
            output_details  # used for tflite, optional for tf/pytorch
        )
        self.inputs = []
        self.model_path = model_path  # url to download the model
        self.raw_model_file = (
            None  # local address for raw tf/tflite/pytorch model
        )
        self.mlir_file = (
            None  # local address for .mlir file of tf/tflite/pytorch model
        )
        self.mlir_model = None  # read of .mlir file
        self.output_tensor = (
            None  # the raw tf/pytorch/tflite_output_tensor, not mlir_tensor
        )
        self.interpreter = (
            None  # could be tflite/tf/torch_interpreter in utils
        )
        self.input_file = None
        self.output_file = None

        # create tmp model file directory
        if self.model_path is None and self.model_name is None:
            print(
                "Error. No model_path, No model name,Please input either one."
            )
            return

        print("Setting up for TMP_WORK_DIR")
        self.workdir = os.path.join(
            os.path.dirname(__file__), "./../gen_shark_tank"
        )
        os.makedirs(self.workdir, exist_ok=True)
        print(f"TMP_WORK_DIR = {self.workdir}")

        # compile and run tfhub tflite
        load_model_success = self.load_tflite_model()
        if not load_model_success:
            print("Error, load tflite model fail")
            return

        if (self.input_details is None) or (self.output_details is None):
            # print("Setting up tflite interpreter to get model input details")
            self.setup_interpreter()

            inputs = self.generate_inputs(self.input_details)  # device_inputs
        self.setup_inputs(inputs)

    def load_tflite_model(self):
        # use model name get dir.
        tflite_model_name_dir = os.path.join(
            self.workdir, str(self.model_name)
        )

        os.makedirs(tflite_model_name_dir, exist_ok=True)
        print(f"TMP_TFLITE_MODELNAME_DIR = {tflite_model_name_dir}")

        self.raw_model_file = "/".join(
            [tflite_model_name_dir, str(self.model_name) + "_tflite.tflite"]
        )
        self.mlir_file = "/".join(
            [tflite_model_name_dir, str(self.model_name) + "_tflite.mlir"]
        )
        self.input_file = "/".join([tflite_model_name_dir, "inputs"])
        self.output_file = "/".join([tflite_model_name_dir, "golden_out"])
        # np.save("/".join([tflite_model_name_dir, "function_name"]), np.array("main"))

        if os.path.exists(self.raw_model_file):
            print(
                "Local address for .tflite model file Exists: ",
                self.raw_model_file,
            )
        else:
            print("No local tflite file, Download tflite model")
            if self.model_path is None:
                # get model file from tflite_model_list.csv or download from gs://bucket
                print("No model_path, get from tflite_model_list.csv")
                tflite_model_list_path = os.path.join(
                    os.path.dirname(__file__),
                    "../tank/tflite/tflite_model_list.csv",
                )
                tflite_model_list = csv.reader(open(tflite_model_list_path))
                for row in tflite_model_list:
                    if str(row[0]) == str(self.model_name):
                        self.model_path = row[1]
                        print("tflite_model_name", str(row[0]))
                        print("tflite_model_link", self.model_path)
            if self.model_path is None:
                print("Error, No model path find in tflite_model_list.csv")
                return False
            urllib.request.urlretrieve(self.model_path, self.raw_model_file)
        return True

    def setup_interpreter(self):
        self.interpreter = TFLiteModelUtil(self.raw_model_file)
        (
            self.input_details,
            self.output_details,
        ) = self.interpreter.setup_tflite_interpreter()

    def generate_inputs(self, input_details):
        self.inputs = []
        for tmp_input in input_details:
            print(
                "input_details shape:",
                str(tmp_input["shape"]),
                " type:",
                tmp_input["dtype"].__name__,
            )
            self.inputs.append(
                np.ones(shape=tmp_input["shape"], dtype=tmp_input["dtype"])
            )
        return self.inputs

    def setup_inputs(self, inputs):
        # print("Setting up inputs")
        self.inputs = inputs

    def get_mlir_model(self):
        return self.mlir_model

    def get_mlir_file(self):
        return self.mlir_file

    def get_inputs(self):
        return self.inputs

    def get_golden_output(self):
        self.output_tensor = self.interpreter.invoke_tflite(self.inputs)
        np.savez(self.output_file, *self.output_tensor)
        return self.output_tensor

    def get_model_details(self):
        return self.input_details, self.output_details

    def get_raw_model_file(self):
        return self.raw_model_file

    def get_interpreter(self):
        return self.interpreter

```

`shark/torch_mlir_lockstep_tensor.py`:

```py
# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
# Also available under a BSD-style license. See LICENSE.
import contextlib
import re
import traceback
import warnings
from typing import Any
import numpy as np

import torch
from torch.utils._pytree import tree_map

from torch_mlir.eager_mode.ir_building import build_mlir_module
from torch_mlir.eager_mode.torch_mlir_dispatch import (
    UnsupportedByTorchMlirEagerMode,
    normalize_args_kwargs,
    check_get_aliased_arg,
)
from torch_mlir.eager_mode import EAGER_MODE_DEBUG
from torch_mlir.eager_mode.torch_mlir_tensor import (
    TorchMLIRTensor,
    check_requires_grad,
    make_wrapper_subclass_from_torch_tensor,
    make_bare_wrapper_subclass,
    UNSUPPORTED_OPS,
    no_dispatch,
)
from torch_mlir.eager_mode import torch_mlir_tensor
from shark.iree_eager_backend import EagerModeIREELinalgOnTensorsBackend


backend = EagerModeIREELinalgOnTensorsBackend("cpu")
torch_mlir_tensor.backend = backend
rtol = 1e-04
atol = 1e-05


class TorchMLIRLockstepTensor(TorchMLIRTensor):
    """This class overrides the dispatching for TorchMLIRTensor to allow for an op-by-op numerical comparison between PyTorch and the Torch-MLIR -> IREE backend compilation pipeline. This only supports the IREE backend and focuses on op-by-op level verification.

    TODO: Extend this to do a cumulative trace with summary statistics at the end. Possibly requires a wrapper environment to store full trace info.
    """

    def __new__(cls, elem, **kwargs):
        if kwargs.get("constructing_from_device_tensor", False):
            tensor_meta_data = backend.get_torch_metadata(elem, kwargs)
            r = make_bare_wrapper_subclass(
                cls=cls,
                size=tensor_meta_data.size,
                strides=tensor_meta_data.strides,
                storage_offset=tensor_meta_data.storage_offset,
                dtype=tensor_meta_data.dtype,
                layout=tensor_meta_data.layout,
                device=tensor_meta_data.device,
                requires_grad=tensor_meta_data.requires_grad,
            )
            r.elem = elem
        elif isinstance(elem, torch.nn.Parameter):
            r = make_wrapper_subclass_from_torch_tensor(
                cls, elem.data, **kwargs
            )
            # This is a hack to handle non-contiguous data through IREE-backend
            nt = elem.detach().data.numpy()
            if not nt.flags["C_CONTIGUOUS"]:
                nt = np.ascontiguousarray(nt, dtype=nt.dtype)
            r.elem = backend.transfer_from_torch_to_device(
                torch.from_numpy(nt)
            )
        elif isinstance(elem, torch.Tensor):
            r = make_wrapper_subclass_from_torch_tensor(cls, elem, **kwargs)
            # Ditto TODO: Find a better way to handle this
            nt = elem.numpy()
            if not nt.flags["C_CONTIGUOUS"]:
                nt = np.ascontiguousarray(nt, dtype=nt.dtype)
            r.elem = backend.transfer_from_torch_to_device(
                torch.from_numpy(nt)
            )
        # This branch handles the case when a python scalar is passed to some op
        # or is returned from some aten op, such as _local_scalar_dense.
        elif isinstance(elem, (int, float, bool)):
            return elem
        else:
            raise ValueError(f"Unknown element type: {type(elem)}")
        return r

    def __repr__(self):
        if self.grad_fn:
            return f"TorchMLIRLockstepTensor({self.elem}, backend={backend.__class__.__name__}, grad_fn={self.grad_fn})"
        else:
            return f"TorchMLIRLockstepTensor({self.elem}, backend={backend.__class__.__name__})"

    """This does essentially the same dispatch as TorchMLIRTensor but operates as if debug mode is enabled. The numeric verification happens after the Torch-MLIR result is obtained by comparing against the 
    """

    @classmethod
    def __torch_dispatch__(cls, func, _types, args=(), kwargs=None):
        requires_grad = check_requires_grad(*args, **kwargs)
        try:
            with no_dispatch():
                if hasattr(func, "op_name"):
                    op_name = func.op_name
                elif hasattr(func, "__name__"):
                    # Handle builtin_function_or_method.
                    op_name = func.__name__
                else:
                    raise RuntimeError(f"op {func} has no name")

                if UNSUPPORTED_OPS.match(op_name):
                    raise UnsupportedByTorchMlirEagerMode(op_name)

                if not hasattr(func, "_schema"):
                    raise RuntimeError(f"op {func} has no schema.")

                normalized_kwargs = normalize_args_kwargs(func, args, kwargs)

                if "layout" in normalized_kwargs and normalized_kwargs[
                    "layout"
                ] not in {0, None}:
                    raise UnsupportedByTorchMlirEagerMode(
                        f"{normalized_kwargs['layout']} layout not supported."
                    )
                if "memory_format" in normalized_kwargs and normalized_kwargs[
                    "memory_format"
                ] not in {0, None}:
                    raise UnsupportedByTorchMlirEagerMode(
                        f"{normalized_kwargs['memory_format']} memory format not supported."
                    )
                eager_module = build_mlir_module(func, normalized_kwargs)
            device_tensor_args = [
                kwarg.elem
                for _, kwarg in normalized_kwargs.items()
                if isinstance(kwarg, cls)
            ]
            assert len(eager_module.body.operations[0].arguments) == len(
                device_tensor_args
            ), "Number of parameters and number of arguments differs."
            op_mlir_backend_callable = backend.compile(eager_module)
            out = op_mlir_backend_callable(*device_tensor_args)
            out = tree_map(
                lambda x: cls(
                    x,
                    requires_grad=requires_grad,
                    constructing_from_device_tensor=True,
                ),
                out,
            )

            # Numeric verification; Value for comparison comes from PyTorch eager
            with no_dispatch():
                unwrapped_args = tree_map(cls.unwrap, args)
                unwrapped_kwargs = tree_map(cls.unwrap, kwargs)
                if "_reshape_alias" in op_name:
                    native_out = torch.ops.aten.view(
                        unwrapped_args[0], unwrapped_args[1]
                    )
                else:
                    native_out = func(*unwrapped_args, **unwrapped_kwargs)

            native_out = tree_map(
                lambda x: cls(x, requires_grad=requires_grad), native_out
            ).elem
            tmp_out = out.elem

            try:
                np.testing.assert_allclose(
                    native_out.to_host(),
                    tmp_out.to_host(),
                    rtol=rtol,
                    atol=atol,
                )
            except Exception as e:
                shaped_args = [
                    arg.shape if torch.is_tensor(arg) else arg
                    for arg in unwrapped_args
                ]
                shaped_kwargs = [
                    kwarg.shape if torch.is_tensor(kwarg) else kwarg
                    for kwarg in unwrapped_kwargs
                ]
                warnings.warn(
                    f"Lockstep accuracy verification failed with error: *{str(e)}*; "
                    f"Dispatched function name: *{str(func)}*; "
                    f"Dispatched function args: *{str(shaped_args)}*; "
                    f"Dispatched function kwargs: *{str(shaped_kwargs)}*; "
                )
        except Exception as e:
            warnings.warn(traceback.format_exc())
            if isinstance(e, UnsupportedByTorchMlirEagerMode):
                warnings.warn(
                    f"Couldn't use TorchMLIR eager because current incompatibility: *{str(e)}*; running through PyTorch eager."
                )
            else:
                warnings.warn(
                    f"Couldn't use TorchMLIR eager because of error: *{str(e)}*; "
                    f"Running through PyTorch eager"
                )

            with no_dispatch():
                unwrapped_args = tree_map(cls.unwrap, args)
                unwrapped_kwargs = tree_map(cls.unwrap, kwargs)
                if "_reshape_alias" in op_name:
                    out = torch.ops.aten.view(
                        unwrapped_args[0], unwrapped_args[1]
                    )
                else:
                    out = func(*unwrapped_args, **unwrapped_kwargs)

            out = tree_map(lambda x: cls(x, requires_grad=requires_grad), out)

        maybe_aliased_arg_name = check_get_aliased_arg(func)
        if maybe_aliased_arg_name is not None:
            warnings.warn(
                f"Found aliased arg, but didn't copy tensor contents. This could lead to incorrect results for E2E model execution but doesn't affect the validity of the lockstep op verification."
            )
            # TODO: Find a way to handle argument aliasing for IREE backend
            # backend.copy_into(normalized_kwargs[maybe_aliased_arg_name].elem, out.elem)

        return out

```

`shark/torch_mlir_utils.py`:

```py
# Copyright 2020 The Nod Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from torch_mlir.ir import StringAttr
import torch_mlir
from torch_mlir_e2e_test.linalg_on_tensors_backends import refbackend
import tempfile
from shark.parser import shark_args
import io

mlir_type_mapping_dict = {
    "linalg": torch_mlir.OutputType.LINALG_ON_TENSORS,
    "stablehlo": torch_mlir.OutputType.STABLEHLO,
    "tosa": torch_mlir.OutputType.TOSA,
}


def get_module_name_for_asm_dump(module):
    """Gets a name suitable for an assembly dump.
    The name is not guaranteed to be unique.
    """
    if not "torch.debug_module_name" in module.operation.attributes:
        return "UnnammedModule"
    return StringAttr(
        module.operation.attributes["torch.debug_module_name"]
    ).value


def run_on_refbackend(torch_module, inputs):
    backend = refbackend.RefBackendLinalgOnTensorsBackend()
    compiled = backend.compile(torch_module)
    jit_module = backend.load(compiled)
    np_inputs = [x.numpy() for x in inputs]
    return jit_module.forward(np_inputs[0])


# Creates dynamic dims for all dims.
# TODO: Pass user specified dynamic dims.
def create_dynamic_placeholders(inputs):
    placeholders = []
    for inp in inputs:
        placeholder = torch_mlir.TensorPlaceholder.like(
            inp, dynamic_axes=[i for i in range(len(inp.shape))]
        )
        placeholders.append(placeholder)
    return tuple(placeholders)


def get_torch_mlir_module(
    module,
    input: tuple,
    dynamic: bool,
    jit_trace: bool,
    return_str: bool = False,
    mlir_type: str = "linalg",
):
    """Get the MLIR's linalg-on-tensors module from the torchscipt module."""
    ignore_traced_shapes = False
    if dynamic:
        input = create_dynamic_placeholders(input)
    if jit_trace:
        ignore_traced_shapes = True

    tempfile.tempdir = "."

    mlir_module = torch_mlir.compile(
        module,
        input,
        output_type=mlir_type_mapping_dict[mlir_type],
        use_tracing=jit_trace,
        ignore_traced_shapes=ignore_traced_shapes,
    )

    if return_str:
        return mlir_module.operation.get_asm()
    bytecode_stream = io.BytesIO()
    mlir_module.operation.write_bytecode(bytecode_stream)
    bytecode = bytecode_stream.getvalue()
    return bytecode

```

`tank/README.md`:

```md
## Supported and Validated Models

### PyTorch HuggingFace Models

| PyTorch Language Models | Torch-MLIR lowerable | SHARK-CPU | SHARK-CUDA | SHARK-METAL |
|---------------------|----------------------|----------|----------|-------------|
| BERT                | :green_heart: (JIT)          | :green_heart:         | :green_heart:         | :green_heart:            |
| Albert              | :green_heart: (JIT)            | :green_heart:         | :green_heart:         | :green_heart:            |
| BigBird             | :green_heart: (AOT)            |          |          |             |
| dbmdz/ConvBERT      | :green_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |
| DistilBERT          | :broken_heart: (JIT)            |          |          |             |
| GPT2                | :green_heart:            | :green_heart:         |  :green_heart:        | :green_heart:            |
| MobileBert          | :green_heart: (JIT)            | :green_heart:         | :green_heart:         | :green_heart:            |
| microsoft/beit      | :green_heart:                  | :green_heart:         | :broken_heart:         | :broken_heart:            |
| facebook/deit       | :green_heart:          | :green_heart:         | :broken_heart:         | :broken_heart:            |
| facebook/convnext   | :green_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |

### Torchvision  Models

| TORCHVISION Models | Torch-MLIR lowerable | SHARK-CPU | SHARK-CUDA | SHARK-METAL |
|--------------------|----------------------|----------|----------|-------------|
| AlexNet            | :green_heart: (Script)         | :green_heart:         | :green_heart:         | :green_heart:            |
| MobileNetV2        | :green_heart: (Script)         | :green_heart:         | :green_heart:         | :green_heart:            |
| MobileNetV3        | :green_heart: (Script)         | :green_heart:         | :green_heart:         | :green_heart:            |
| Unet               | :green_heart: (Script)         | :green_heart:         | :green_heart:         | :green_heart:            |
| Resnet18           | :green_heart: (Script)         | :green_heart:         |  :green_heart:        | :green_heart:            |
| Resnet50           | :green_heart: (Script)         | :green_heart:         |   :green_heart:       | :green_heart:            |
| Resnet101           | :green_heart: (Script)         | :green_heart:         |   :green_heart:       | :green_heart:            |
| Resnext50_32x4d    | :green_heart: (Script)         |          |          |             |
| SqueezeNet         | :green_heart: (Script)         | :green_heart:         |   :broken_heart:       | :broken_heart:            |
| EfficientNet       | :green_heart: (Script)         |          |          |             |
| Regnet             | :green_heart: (Script)         |          |          |             |
| Resnest            | :broken_heart: (Script)         |          |          |             |
| Vision Transformer | :green_heart: (Script)         | :green_heart:         | :green_heart:         | :green_heart:            |
| VGG 16             | :green_heart: (Script)         | :green_heart:         |   :green_heart:       |             |
| Wide Resnet        | :green_heart: (Script)         | :green_heart:         | :green_heart:         | :green_heart:            |
| RAFT               | :broken_heart: (JIT)            |          |          |             |

For more information refer to [MODEL TRACKING SHEET](https://docs.google.com/spreadsheets/d/15PcjKeHZIrB5LfDyuw7DGEEE8XnQEX2aX8lm8qbxV8A/edit#gid=0)

### Tensorflow Models (Inference)

| Hugging Face Models | tf-mhlo lowerable | SHARK-CPU | SHARK-CUDA | SHARK-METAL |
|---------------------|----------------------|----------|----------|-------------|
| BERT                | :green_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |
| MiniLM                | :green_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |
| albert-base-v2              | :green_heart:            | :green_heart:         | :green_heart:         | :green_heart:            |
| DistilBERT          | :green_heart:            | :green_heart:         | :green_heart:         | :green_heart:            |
| CamemBert                | :green_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |
| ConvBert              | :green_heart:            | :green_heart:         | :green_heart:         | :green_heart:            |
| Deberta              |            |         |          |             |
| electra          | :green_heart:            | :green_heart:         | :green_heart:         | :green_heart:            |
| funnel              |            |         |          |             |
| layoutlm              | :green_heart:            | :green_heart:         | :green_heart:         | :green_heart:            |
| longformer              |            |         |          |             |
| mobile-bert                | :green_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |
| rembert              |            |         |          |             |
| tapas              |            |         |          |             |
| flaubert                | :broken_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |
| roberta                | :green_heart:          | :green_heart:         | :green_heart:         | :green_heart:            |
| xlm-roberta              | :green_heart:            | :green_heart:         | :green_heart:         | :green_heart:            |
| mpnet              | :green_heart:            | :green_heart:         | :green_heart:         | :green_heart:            |

### PyTorch Training Models

| Models | Torch-MLIR lowerable | SHARK-CPU | SHARK-CUDA | SHARK-METAL |
|---------------------|----------------------|----------|----------|-------------|
| BERT                | :green_heart:           | :green_heart:         |          |             |
| FullyConnected                | :green_heart:           | :green_heart:         |          |             |

### JAX  Models

| Models | JAX-MHLO lowerable | SHARK-CPU | SHARK-CUDA | SHARK-METAL |
|---------------------|----------------------|----------|----------|-------------|
| DALL-E                | :broken_heart:           | :broken_heart:         |          |             |
| FullyConnected                | :green_heart:           | :green_heart:         |          |             |

<details>
  <summary>TFLite Models</summary>

### TFLite Models

| Models | TOSA/LinAlg  | SHARK-CPU | SHARK-CUDA | SHARK-METAL |
|---------------------|----------------------|----------|----------|-------------|
| BERT                | :broken_heart:           | :broken_heart:         |          |             |
| FullyConnected      | :green_heart:           | :green_heart:         |          |             |
| albert | :green_heart:           | :green_heart:         |          |             |
| asr_conformer | :green_heart:           | :green_heart:         |          |             |
| bird_classifier | :green_heart:           | :green_heart:         |          |             |
| cartoon_gan | :green_heart:           | :green_heart:         |          |             |
| craft_text | :green_heart:           | :green_heart:         |          |             |
| deeplab_v3 | :green_heart:           | :green_heart:         |          |             |
| densenet | :green_heart:           | :green_heart:         |          |             |
| east_text_detector | :green_heart:           | :green_heart:         |          |             |
| efficientnet_lite0_int8 | :green_heart:           | :green_heart:         |          |             |
| efficientnet | :green_heart:           | :green_heart:         |          |             |
| gpt2 | :green_heart:           | :green_heart:         |          |             |
| image_stylization | :green_heart:           | :green_heart:         |          |             |
| inception_v4 | :green_heart:           | :green_heart:         |          |             |
| inception_v4_uint8 | :green_heart:           | :green_heart:         |          |             |
| lightning_fp16 | :green_heart:           | :green_heart:         |          |             |
| lightning_i8 | :green_heart:           | :green_heart:         |          |             |
| lightning | :green_heart:           | :green_heart:         |          |             |
| magenta | :green_heart:           | :green_heart:         |          |             |
| midas | :green_heart:           | :green_heart:         |          |             |
| mirnet | :green_heart:           | :green_heart:         |          |             |
| mnasnet | :green_heart:           | :green_heart:         |          |             |
| mobilebert_edgetpu_s_float | :green_heart:           | :green_heart:         |          |             |
| mobilebert_edgetpu_s_quant | :green_heart:           | :green_heart:         |          |             |
| mobilebert | :green_heart:           | :green_heart:         |          |             |
| mobilebert_tf2_float | :green_heart:           | :green_heart:         |          |             |
| mobilebert_tf2_quant | :green_heart:           | :green_heart:         |          |             |
| mobilenet_ssd_quant | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v1 | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v1_uint8 | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v2_int8 | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v2 | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v2_uint8 | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v3-large | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v3-large_uint8 | :green_heart:           | :green_heart:         |          |             |
| mobilenet_v35-int8 | :green_heart:           | :green_heart:         |          |             |
| nasnet | :green_heart:           | :green_heart:         |          |             |
| person_detect | :green_heart:           | :green_heart:         |          |             |
| posenet | :green_heart:           | :green_heart:         |          |             |
| resnet_50_int8 | :green_heart:           | :green_heart:         |          |             |
| rosetta | :green_heart:           | :green_heart:         |          |             |
| spice | :green_heart:           | :green_heart:         |          |             |
| squeezenet | :green_heart:           | :green_heart:         |          |             |
| ssd_mobilenet_v1 | :green_heart:           | :green_heart:         |          |             |
| ssd_mobilenet_v1_uint8 | :green_heart:           | :green_heart:         |          |             |
| ssd_mobilenet_v2_fpnlite | :green_heart:           | :green_heart:         |          |             |
| ssd_mobilenet_v2_fpnlite_uint8 | :green_heart:           | :green_heart:         |          |             |
| ssd_mobilenet_v2_int8 | :green_heart:           | :green_heart:         |          |             |
| ssd_mobilenet_v2 | :green_heart:           | :green_heart:         |          |             |
| ssd_spaghettinet_large | :green_heart:           | :green_heart:         |          |             |
| ssd_spaghettinet_large_uint8 | :green_heart:           | :green_heart:         |          |             |
| visual_wake_words_i8 | :green_heart:           | :green_heart:         |          |             |

</details>

## Testing and Benchmarks

### Run all model tests on CPU/GPU/VULKAN/Metal

For a list of models included in our pytest model suite, see https://github.com/nod-ai/SHARK/blob/main/tank/all_models.csv

```shell
pytest tank/test_models.py

# Models included in the pytest suite can be found listed in all_models.csv.

# If on Linux for multithreading on CPU (faster results):
pytest tank/test_models.py -n auto
```

### Running specific tests
```shell

# Search for test cases by including a keyword that matches all or part of the test case's name;
pytest tank/test_models.py -k "keyword" 

# Test cases are named uniformly by format test_module_<model_name_underscores_only>_<torch/tf>_<static/dynamic>_<device>.

# Example: Test all models on nvidia gpu:
pytest tank/test_models.py -k "cuda"

# Example: Test all tensorflow resnet models on Vulkan backend:
pytest tank/test_models.py -k "resnet and tf and vulkan"

# Exclude a test case:
pytest tank/test_models.py -k "not ..."

### Run benchmarks on SHARK tank pytests and generate bench_results.csv with results.

(the following requires source installation with `IMPORTER=1 ./setup_venv.sh`)

```shell
pytest --benchmark tank/test_models.py
  
# Just do static GPU benchmarks for PyTorch tests:
pytest --benchmark tank/test_models.py -k "pytorch and static and cuda"

```
  
### Benchmark Resnet50, MiniLM on CPU

(requires source installation with `IMPORTER=1 ./setup_venv.sh`)  
  
```shell
# We suggest running the following commands as root before running benchmarks on CPU:
  
cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | awk -F, '{print $2}' | sort -n | uniq | ( while read X ; do echo $X ; echo 0 > /sys/devices/system/cpu/cpu$X/online ; done )
echo 1 > /sys/devices/system/cpu/intel_pstate/no_turbo

# Benchmark canonical Resnet50 on CPU via pytest
pytest --benchmark tank/test_models.py -k "resnet50 and tf_static_cpu"

# Benchmark canonical MiniLM on CPU via pytest
pytest --benchmark tank/test_models.py -k "MiniLM and cpu"

# Benchmark MiniLM on CPU via transformer-benchmarks:
git clone --recursive https://github.com/nod-ai/transformer-benchmarks.git
cd transformer-benchmarks
./perf-ci.sh -n
# Check detail.csv for MLIR/IREE results.

```

To run the fine tuning example, from the root SHARK directory, run:

```shell
IMPORTER=1 ./setup_venv.sh
source shark.venv/bin/activate
pip install jupyter tf-models-nightly tf-datasets
jupyter-notebook
```
if running from a google vm, you can view jupyter notebooks on your local system with:
```shell
gcloud compute ssh <YOUR_INSTANCE_DETAILS> --ssh-flag="-N -L localhost:8888:localhost:8888"
```




```

`tank/examples/MiniLM_tf/huggingface_MiniLM_gen.py`:

```py
from iree import runtime as ireert
from iree.compiler import tf as tfc
from absl import app

import numpy as np
import os
import tensorflow as tf

from transformers import BertModel, BertTokenizer, TFBertModel

SEQUENCE_LENGTH = 512
BATCH_SIZE = 1

# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        # Create a BERT trainer with the created network.
        self.m = TFBertModel.from_pretrained(
            "microsoft/MiniLM-L12-H384-uncased", from_pt=True
        )

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m.predict = lambda x, y, z: self.m.call(
            input_ids=x, attention_mask=y, token_type_ids=z, training=False
        )

    @tf.function(input_signature=bert_input, jit_compile=True)
    def predict(self, input_word_ids, input_mask, segment_ids):
        return self.m.predict(input_word_ids, input_mask, segment_ids)


if __name__ == "__main__":
    # BertModule()
    # Compile the model using IREE
    compiler_module = tfc.compile_module(
        BertModule(), exported_names=["predict"], import_only=True
    )
    # Save module as MLIR file in a directory
    ARITFACTS_DIR = os.getcwd()
    mlir_path = os.path.join(ARITFACTS_DIR, "model.mlir")
    with open(mlir_path, "wt") as output_file:
        output_file.write(compiler_module.decode("utf-8"))
    print(f"Wrote MLIR to path '{mlir_path}'")

```

`tank/examples/MiniLM_tf/huggingface_MiniLM_run.py`:

```py
from iree import runtime as ireert
from iree.compiler import tf as tfc
from iree.compiler import compile_str
from absl import app

import numpy as np
import os
import tensorflow as tf

from transformers import BertModel, BertTokenizer, TFBertModel

MAX_SEQUENCE_LENGTH = 512
BATCH_SIZE = 1

# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        # Create a BERT trainer with the created network.
        self.m = TFBertModel.from_pretrained(
            "microsoft/MiniLM-L12-H384-uncased", from_pt=True
        )

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m.predict = lambda x, y, z: self.m.call(
            input_ids=x, attention_mask=y, token_type_ids=z, training=False
        )

    @tf.function(input_signature=bert_input, jit_compile=True)
    def predict(self, input_ids, attention_mask, token_type_ids):
        return self.m.predict(input_ids, attention_mask, token_type_ids)


if __name__ == "__main__":
    # Prepping Data
    tokenizer = BertTokenizer.from_pretrained(
        "microsoft/MiniLM-L12-H384-uncased"
    )
    text = "Replace me by any text you'd like."
    encoded_input = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    for key in encoded_input:
        encoded_input[key] = tf.expand_dims(
            tf.convert_to_tensor(encoded_input[key]), 0
        )

    # Compile the model using IREE
    compiler_module = tfc.compile_module(
        BertModule(), exported_names=["predict"], import_only=True
    )

    # Compile the model using IREE
    backend = "dylib-llvm-aot"
    args = [
        "--iree-llvmcpu-target-cpu-features=host",
        "--iree-mhlo-demote-i64-to-i32=false",
        "--iree-flow-demote-i64-to-i32",
    ]
    backend_config = "dylib"
    # backend = "cuda"
    # backend_config = "cuda"
    # args = ["--iree-cuda-llvm-target-arch=sm_80", "--iree-enable-fusion-with-reduction-ops"]
    flatbuffer_blob = compile_str(
        compiler_module,
        target_backends=[backend],
        extra_args=args,
        input_type="auto",
    )
    # flatbuffer_blob = compile_str(compiler_module, target_backends=["dylib-llvm-aot"])

    # Save module as MLIR file in a directory
    vm_module = ireert.VmModule.from_flatbuffer(flatbuffer_blob)
    tracer = ireert.Tracer(os.getcwd())
    config = ireert.Config("dylib", tracer)
    ctx = ireert.SystemContext(config=config)
    ctx.add_vm_module(vm_module)
    BertCompiled = ctx.modules.module
    result = BertCompiled.predict(
        encoded_input["input_ids"],
        encoded_input["attention_mask"],
        encoded_input["token_type_ids"],
    )
    print(result)

```

`tank/examples/MiniLM_tf/huggingface_MiniLM_tf.py`:

```py
import tensorflow as tf
from transformers import BertModel, BertTokenizer, TFBertModel

tf_model = TFBertModel.from_pretrained(
    "microsoft/MiniLM-L12-H384-uncased", from_pt=True
)
tokenizer = BertTokenizer.from_pretrained("microsoft/MiniLM-L12-H384-uncased")

text = "Replace me by any text you'd like."
encoded_input = tokenizer(
    text, padding="max_length", truncation=True, max_length=512
)
for key in encoded_input:
    encoded_input[key] = tf.expand_dims(
        tf.convert_to_tensor(encoded_input[key]), 0
    )
output = tf_model(encoded_input)

print(output)

```

`tank/examples/MiniLM_tf/seq_classification.py`:

```py
#!/usr/bin/env python
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf
from shark.shark_inference import SharkInference
from shark.parser import shark_args
import argparse


seq_parser = argparse.ArgumentParser(
    description="Shark Sequence Classification."
)
seq_parser.add_argument(
    "--hf_model_name",
    type=str,
    default="bert-base-uncased",
    help="Hugging face model to run sequence classification.",
)

seq_args, unknown = seq_parser.parse_known_args()


BATCH_SIZE = 1
MAX_SEQUENCE_LENGTH = 16

# Create a set of input signature.
inputs_signature = [
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
]

# For supported models please see here:
# https://huggingface.co/docs/transformers/model_doc/auto#transformers.TFAutoModelForSequenceClassification


def preprocess_input(text="This is just used to compile the model"):
    tokenizer = AutoTokenizer.from_pretrained(seq_args.hf_model_name)
    inputs = tokenizer(
        text,
        padding="max_length",
        return_tensors="tf",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    return inputs


class SeqClassification(tf.Module):
    def __init__(self, model_name):
        super(SeqClassification, self).__init__()
        self.m = TFAutoModelForSequenceClassification.from_pretrained(
            model_name, output_attentions=False, num_labels=2
        )
        self.m.predict = lambda x, y: self.m(input_ids=x, attention_mask=y)[0]

    @tf.function(input_signature=inputs_signature, jit_compile=True)
    def forward(self, input_ids, attention_mask):
        return tf.math.softmax(
            self.m.predict(input_ids, attention_mask), axis=-1
        )


if __name__ == "__main__":
    inputs = preprocess_input()
    shark_module = SharkInference(
        SeqClassification(seq_args.hf_model_name),
        (inputs["input_ids"], inputs["attention_mask"]),
    )
    shark_module.set_frontend("tensorflow")
    shark_module.compile()
    print(f"Model has been successfully compiled on {shark_args.device}")

    while True:
        input_text = input(
            "Enter the text to classify (press q or nothing to exit): "
        )
        if not input_text or input_text == "q":
            break
        inputs = preprocess_input(input_text)
        print(
            shark_module.forward(
                (inputs["input_ids"], inputs["attention_mask"])
            )
        )

```

`tank/examples/bert-base-uncased_tosa_torch/bert_base_uncased_tosa.py`:

```py
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model

mlir_model, func_name, inputs, golden_out = download_model(
    "bert-base-uncased_tosa",
    frontend="torch",
)

shark_module = SharkInference(
    mlir_model, func_name, device="cpu", mlir_dialect="tosa"
)
shark_module.compile()
result = shark_module.forward(inputs)
print("The obtained result via shark is: ", result)
print("The golden result is:", golden_out)

import numpy as np

result_unsqueeze = np.expand_dims(result, axis=0)

print(
    np.testing.assert_allclose(
        result_unsqueeze, golden_out, rtol=1e-3, atol=1e-3
    )
)

```

`tank/examples/bert_fine_tuning/bert_fine_tune_tf.py`:

```py
import numpy as np

from iree import runtime as ireert
from iree.tf.support import module_utils
from iree.compiler import tf as tfc
from iree.compiler import compile_str

import tensorflow as tf

try:
    import tensorflow_datasets as tfds
    import tensorflow_models as tfm
    from official.nlp.modeling import layers
    from official.nlp.modeling import networks
    from official.nlp.modeling.models import bert_classifier
except ModuleNotFoundError:
    print(
        "tensorflow models or datasets not found please run the following command with your virtual env active:\npip install tf-models-nightly tf-datasets"
    )
import json
import time
import os

gs_folder_bert = "gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12"
tf.io.gfile.listdir(gs_folder_bert)
vocab_size = 100
NUM_CLASSES = 2
SEQUENCE_LENGTH = 128
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False

        bert_config_file = os.path.join(gs_folder_bert, "bert_config.json")

        config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())
        encoder_config = tfm.nlp.encoders.EncoderConfig(
            {"type": "bert", "bert": config_dict}
        )
        bert_encoder = tfm.nlp.encoders.build_encoder(encoder_config)

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            bert_encoder, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()
        checkpoint = tf.train.Checkpoint(encoder=bert_encoder)
        checkpoint.read(
            os.path.join(gs_folder_bert, "bert_model.ckpt")
        ).assert_consumed()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.predict = tf.function(input_signature=[bert_input])(
            self.m.predict
        )
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            bert_input,  # inputs
            tf.TensorSpec(shape=[BATCH_SIZE], dtype=tf.int32),  # labels
        ],
        jit_compile=True,
    )
    def learn(self, inputs, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            probs = self.m.call(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss


if __name__ == "__main__":
    glue, info = tfds.load("glue/mrpc", with_info=True, batch_size=BATCH_SIZE)

    tokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(
        vocab_file=os.path.join(gs_folder_bert, "vocab.txt"), lower_case=True
    )

    max_seq_length = SEQUENCE_LENGTH

    packer = tfm.nlp.layers.BertPackInputs(
        seq_length=max_seq_length,
        special_tokens_dict=tokenizer.get_special_tokens_dict(),
    )

    class BertInputProcessor(tf.keras.layers.Layer):
        def __init__(self, tokenizer, packer):
            super().__init__()
            self.tokenizer = tokenizer
            self.packer = packer

        def call(self, inputs):
            tok1 = self.tokenizer(inputs["sentence1"])
            tok2 = self.tokenizer(inputs["sentence2"])

            packed = self.packer([tok1, tok2])

            if "label" in inputs:
                return packed, inputs["label"]
            else:
                return packed

    bert_inputs_processor = BertInputProcessor(tokenizer, packer)
    glue_train = glue["train"].map(bert_inputs_processor).prefetch(1)
    glue_validation = glue["validation"].map(bert_inputs_processor).prefetch(1)
    glue_test = glue["test"].map(bert_inputs_processor).prefetch(1)

    # base tensorflow model
    bert_model = BertModule()

    # Compile the model using IREE
    compiler_module = tfc.compile_module(
        bert_model, exported_names=["learn"], import_only=True
    )

    # choose from dylib-llvm-aot or cuda
    backend = "dylib-llvm-aot"
    if backend == "dylib-llvm-aot":
        args = [
            "--iree-llvmcpu-target-cpu-features=host",
            "--iree-mhlo-demote-i64-to-i32=false",
            "--iree-flow-demote-i64-to-i32",
        ]
        backend_config = "dylib"

    else:
        backend_config = "cuda"
        args = [
            "--iree-cuda-llvm-target-arch=sm_80",
            "--iree-enable-fusion-with-reduction-ops",
        ]

    flatbuffer_blob = compile_str(
        compiler_module,
        target_backends=[backend],
        extra_args=args,
        input_type="auto",
    )

    # Save module as MLIR file in a directory
    vm_module = ireert.VmModule.from_flatbuffer(flatbuffer_blob)
    tracer = ireert.Tracer(os.getcwd())
    config = ireert.Config("local-sync", tracer)
    ctx = ireert.SystemContext(config=config)
    ctx.add_vm_module(vm_module)
    BertCompiled = ctx.modules.module

    # compare output losses:

    iterations = 10
    for i in range(iterations):
        example_inputs, example_labels = next(iter(glue_train))
        example_labels = tf.cast(example_labels, tf.int32)
        example_inputs = [value for key, value in example_inputs.items()]

        # iree version
        iree_loss = BertCompiled.learn(
            example_inputs, example_labels
        ).to_host()

        # base tensorflow
        tf_loss = np.array(bert_model.learn(example_inputs, example_labels))
        print(np.allclose(iree_loss, tf_loss))

```

`tank/examples/bert_tf/bert_large_gen.py`:

```py
from iree import runtime as ireert
from iree.tf.support import module_utils
from iree.compiler import tf as tfc
from absl import app

import numpy as np
import os
import tensorflow as tf

from official.nlp.modeling import layers
from official.nlp.modeling import networks
from official.nlp.modeling.models import bert_classifier

vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False
        test_network = networks.BertEncoder(
            vocab_size=vocab_size,
            num_layers=24,
            hidden_size=1024,
            num_attention_heads=16,
            dict_outputs=dict_outputs,
        )

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            test_network, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            tf.TensorSpec(
                shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32
            ),  # input0: input_word_ids
            tf.TensorSpec(
                shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32
            ),  # input1: input_mask
            tf.TensorSpec(
                shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32
            ),  # input2: segment_ids
            tf.TensorSpec([BATCH_SIZE], tf.int32),  # input3: labels
        ],
        jit_compile=True,
    )
    def learn(self, input_word_ids, input_mask, segment_ids, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            inputs = [input_word_ids, input_mask, segment_ids]
            probs = self.m(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss

    @tf.function(input_signature=bert_input, jit_compile=True)
    def predict(self, input_word_ids, input_mask, segment_ids):
        inputs = [input_word_ids, input_mask, segment_ids]
        return self.m.predict(inputs)


if __name__ == "__main__":
    # BertModule()
    # Compile the model using IREE
    compiler_module = tfc.compile_module(
        BertModule(), exported_names=["learn"], import_only=True
    )
    # Save module as MLIR file in a directory
    ARITFACTS_DIR = os.getcwd()
    mlir_path = os.path.join(ARITFACTS_DIR, "model.mlir")
    with open(mlir_path, "wt") as output_file:
        output_file.write(compiler_module.decode("utf-8"))
    print(f"Wrote MLIR to path '{mlir_path}'")

```

`tank/examples/bert_tf/bert_large_run.py`:

```py
from iree import runtime as ireert
from iree.tf.support import module_utils
from iree.compiler import tf as tfc
from iree.compiler import compile_str
from absl import app
import time

import numpy as np
import os
import tensorflow as tf

from official.nlp.modeling import layers
from official.nlp.modeling import networks
from official.nlp.modeling.models import bert_classifier

vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False
        test_network = networks.BertEncoder(
            vocab_size=vocab_size,
            num_layers=24,
            hidden_size=1024,
            num_attention_heads=16,
            dict_outputs=dict_outputs,
        )

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            test_network, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.predict = tf.function(input_signature=[bert_input])(
            self.m.predict
        )
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            bert_input,  # inputs
            tf.TensorSpec(shape=[BATCH_SIZE], dtype=tf.int32),  # labels
        ],
        jit_compile=True,
    )
    def learn(self, inputs, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            probs = self.m(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss


if __name__ == "__main__":
    # BertModule()
    # Compile the model using IREE
    compiler_module = tfc.compile_module(
        BertModule(), exported_names=["learn"], import_only=True
    )

    # Compile the model using IREE
    backend = "dylib-llvm-aot"
    args = [
        "--iree-llvmcpu-target-cpu-features=host",
        "--iree-mhlo-demote-i64-to-i32=false",
    ]
    backend_config = "dylib"
    # backend = "cuda"
    # backend_config = "cuda"
    # args = ["--iree-cuda-llvm-target-arch=sm_80", "--iree-enable-fusion-with-reduction-ops"]
    flatbuffer_blob = compile_str(
        compiler_module,
        target_backends=[backend],
        extra_args=args,
        input_type="auto",
    )
    # flatbuffer_blob = compile_str(compiler_module, target_backends=["dylib-llvm-aot"])

    # Save module as MLIR file in a directory
    vm_module = ireert.VmModule.from_flatbuffer(flatbuffer_blob)
    tracer = ireert.Tracer(os.getcwd())
    config = ireert.Config("dylib", tracer)
    ctx = ireert.SystemContext(config=config)
    ctx.add_vm_module(vm_module)
    BertCompiled = ctx.modules.module
    predict_sample_input = [
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
    ]
    learn_sample_input = [
        predict_sample_input,
        np.random.randint(5, size=(BATCH_SIZE)),
    ]
    warmup = 5
    total_iter = 10
    num_iter = total_iter - warmup
    for i in range(10):
        if i == warmup - 1:
            start = time.time()
        print(
            BertCompiled.learn(
                predict_sample_input, np.random.randint(5, size=(BATCH_SIZE))
            )
        )
    end = time.time()
    total_time = end - start
    print("time: " + str(total_time))
    print("time/iter: " + str(total_time / num_iter))

```

`tank/examples/bert_tf/bert_large_tf.py`:

```py
import numpy as np
import tensorflow as tf
import time

from official.nlp.modeling import layers
from official.nlp.modeling import networks
from official.nlp.modeling.models import bert_classifier

vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False
        test_network = networks.BertEncoder(
            vocab_size=vocab_size,
            num_layers=24,
            hidden_size=1024,
            num_attention_heads=16,
            dict_outputs=dict_outputs,
        )

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            test_network, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.predict = tf.function(input_signature=[bert_input])(
            self.m.predict
        )
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            bert_input,  # inputs
            tf.TensorSpec(shape=[BATCH_SIZE], dtype=tf.int32),  # labels
        ],
        jit_compile=True,
    )
    def learn(self, inputs, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            probs = self.m(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss


if __name__ == "__main__":
    # BertModule()
    predict_sample_input = [
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
    ]
    bert_model = BertModule()
    warmup = 1
    total_iter = 10
    num_iter = total_iter - warmup
    for i in range(total_iter):
        print(
            bert_model.learn(
                predict_sample_input, np.random.randint(5, size=(BATCH_SIZE))
            )
        )
        if i == warmup - 1:
            start = time.time()

    end = time.time()
    total_time = end - start
    print("time: " + str(total_time))
    print("time/iter: " + str(total_time / num_iter))

```

`tank/examples/bert_tf/bert_small_gen.py`:

```py
from iree import runtime as ireert

# from iree.tf.support import module_utils
from iree.compiler import tf as tfc
from absl import app

import numpy as np
import os
import tensorflow as tf

from official.nlp.modeling import layers
from official.nlp.modeling import networks
from official.nlp.modeling.models import bert_classifier

vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False
        test_network = networks.BertEncoder(
            vocab_size=vocab_size, num_layers=2, dict_outputs=dict_outputs
        )

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            test_network, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            tf.TensorSpec(
                shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32
            ),  # input0: input_word_ids
            tf.TensorSpec(
                shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32
            ),  # input1: input_mask
            tf.TensorSpec(
                shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32
            ),  # input2: segment_ids
            tf.TensorSpec([BATCH_SIZE], tf.int32),  # input3: labels
        ],
        jit_compile=True,
    )
    def learn(self, input_word_ids, input_mask, segment_ids, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            inputs = [input_word_ids, input_mask, segment_ids]
            probs = self.m(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss

    @tf.function(input_signature=bert_input, jit_compile=True)
    def predict(self, input_word_ids, input_mask, segment_ids):
        inputs = [input_word_ids, input_mask, segment_ids]
        return self.m.predict(inputs)


if __name__ == "__main__":
    # BertModule()
    # Compile the model using IREE
    compiler_module = tfc.compile_module(
        BertModule(), exported_names=["learn"], import_only=True
    )
    print(type(compiler_module))
    # Save module as MLIR file in a directory
    ARITFACTS_DIR = os.getcwd()
    mlir_path = os.path.join(ARITFACTS_DIR, "model.mlir")
    with open(mlir_path, "wt") as output_file:
        output_file.write(compiler_module.decode("utf-8"))
    print(f"Wrote MLIR to path '{mlir_path}'")

```

`tank/examples/bert_tf/bert_small_run.py`:

```py
from iree import runtime as ireert
from iree.tf.support import module_utils
from iree.compiler import tf as tfc
from iree.compiler import compile_str
from absl import app
import time

import numpy as np
import os
import tensorflow as tf

from official.nlp.modeling import layers
from official.nlp.modeling import networks
from official.nlp.modeling.models import bert_classifier

vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False
        test_network = networks.BertEncoder(
            vocab_size=vocab_size, num_layers=2, dict_outputs=dict_outputs
        )

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            test_network, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.predict = tf.function(input_signature=[bert_input])(
            self.m.predict
        )
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            bert_input,  # inputs
            tf.TensorSpec(shape=[BATCH_SIZE], dtype=tf.int32),  # labels
        ],
        jit_compile=True,
    )
    def learn(self, inputs, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            probs = self.m(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss


if __name__ == "__main__":
    # BertModule()
    # Compile the model using IREE
    compiler_module = tfc.compile_module(
        BertModule(), exported_names=["learn"], import_only=True
    )

    # Compile the model using IREE
    backend = "dylib-llvm-aot"
    args = [
        "--iree-llvmcpu-target-cpu-features=host",
        "--iree-mhlo-demote-i64-to-i32=false",
        "--iree-flow-demote-i64-to-i32",
    ]
    backend_config = "dylib"
    # backend = "cuda"
    # backend_config = "cuda"
    # args = ["--iree-cuda-llvm-target-arch=sm_80", "--iree-enable-fusion-with-reduction-ops"]
    flatbuffer_blob = compile_str(
        compiler_module,
        target_backends=[backend],
        extra_args=args,
        input_type="auto",
    )
    # flatbuffer_blob = compile_str(compiler_module, target_backends=["dylib-llvm-aot"])

    # Save module as MLIR file in a directory
    vm_module = ireert.VmModule.from_flatbuffer(flatbuffer_blob)
    tracer = ireert.Tracer(os.getcwd())
    config = ireert.Config("dylib", tracer)
    ctx = ireert.SystemContext(config=config)
    ctx.add_vm_module(vm_module)
    BertCompiled = ctx.modules.module
    predict_sample_input = [
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
    ]
    learn_sample_input = [
        predict_sample_input,
        np.random.randint(5, size=(BATCH_SIZE)),
    ]
    warmup = 5
    total_iter = 10
    num_iter = total_iter - warmup
    for i in range(10):
        if i == warmup - 1:
            start = time.time()
        print(
            BertCompiled.learn(
                predict_sample_input, np.random.randint(5, size=(BATCH_SIZE))
            )
        )
    end = time.time()
    total_time = end - start
    print("time: " + str(total_time))
    print("time/iter: " + str(total_time / num_iter))

```

`tank/examples/bert_tf/bert_small_tf_run.py`:

```py
import numpy as np
import tensorflow as tf
import time

from official.nlp.modeling import layers
from official.nlp.modeling import networks
from official.nlp.modeling.models import bert_classifier

vocab_size = 100
NUM_CLASSES = 5
SEQUENCE_LENGTH = 512
BATCH_SIZE = 1
# Create a set of 2-dimensional inputs
bert_input = [
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, SEQUENCE_LENGTH], dtype=tf.int32),
]


class BertModule(tf.Module):
    def __init__(self):
        super(BertModule, self).__init__()
        dict_outputs = False
        test_network = networks.BertEncoder(
            vocab_size=vocab_size, num_layers=2, dict_outputs=dict_outputs
        )

        # Create a BERT trainer with the created network.
        bert_trainer_model = bert_classifier.BertClassifier(
            test_network, num_classes=NUM_CLASSES
        )
        bert_trainer_model.summary()

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m = bert_trainer_model
        self.m.predict = lambda x: self.m.call(x, training=False)
        self.predict = tf.function(input_signature=[bert_input])(
            self.m.predict
        )
        self.m.learn = lambda x, y: self.m.call(x, training=False)
        self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)

    @tf.function(
        input_signature=[
            bert_input,  # inputs
            tf.TensorSpec(shape=[BATCH_SIZE], dtype=tf.int32),  # labels
        ],
        jit_compile=True,
    )
    def learn(self, inputs, labels):
        with tf.GradientTape() as tape:
            # Capture the gradients from forward prop...
            probs = self.m(inputs, training=True)
            loss = self.loss(labels, probs)

        # ...and use them to update the model's weights.
        variables = self.m.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss


if __name__ == "__main__":
    # BertModule()
    predict_sample_input = [
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
        np.random.randint(5, size=(BATCH_SIZE, SEQUENCE_LENGTH)),
    ]
    bert_model = BertModule()
    warmup = 1
    total_iter = 10
    num_iter = total_iter - warmup
    for i in range(total_iter):
        print(
            bert_model.learn(
                predict_sample_input, np.random.randint(5, size=(BATCH_SIZE))
            )
        )
        if i == warmup - 1:
            start = time.time()

    end = time.time()
    total_time = end - start
    print("time: " + str(total_time))
    print("time/iter: " + str(total_time / num_iter))

```

`tank/examples/bert_tf/seq_classification.py`:

```py
#!/usr/bin/env python
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf
from shark.shark_inference import SharkInference
from shark.parser import shark_args
import argparse


seq_parser = argparse.ArgumentParser(
    description="Shark Sequence Classification."
)
seq_parser.add_argument(
    "--hf_model_name",
    type=str,
    default="bert-base-uncased",
    help="Hugging face model to run sequence classification.",
)

seq_args, unknown = seq_parser.parse_known_args()


BATCH_SIZE = 1
MAX_SEQUENCE_LENGTH = 16

# Create a set of input signature.
inputs_signature = [
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
    tf.TensorSpec(shape=[BATCH_SIZE, MAX_SEQUENCE_LENGTH], dtype=tf.int32),
]

# For supported models please see here:
# https://huggingface.co/docs/transformers/model_doc/auto#transformers.TFAutoModelForSequenceClassification


def preprocess_input(text="This is just used to compile the model"):
    tokenizer = AutoTokenizer.from_pretrained(seq_args.hf_model_name)
    inputs = tokenizer(
        text,
        padding="max_length",
        return_tensors="tf",
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
    )
    return inputs


class SeqClassification(tf.Module):
    def __init__(self, model_name):
        super(SeqClassification, self).__init__()
        self.m = TFAutoModelForSequenceClassification.from_pretrained(
            model_name, output_attentions=False, num_labels=2
        )
        self.m.predict = lambda x, y: self.m(input_ids=x, attention_mask=y)[0]

    @tf.function(input_signature=inputs_signature, jit_compile=True)
    def forward(self, input_ids, attention_mask):
        return tf.math.softmax(
            self.m.predict(input_ids, attention_mask), axis=-1
        )


if __name__ == "__main__":
    inputs = preprocess_input()
    shark_module = SharkInference(
        SeqClassification(seq_args.hf_model_name),
        (inputs["input_ids"], inputs["attention_mask"]),
    )
    shark_module.set_frontend("tensorflow")
    shark_module.compile()
    print(f"Model has been successfully compiled on {shark_args.device}")

    while True:
        input_text = input(
            "Enter the text to classify (press q or nothing to exit): "
        )
        if not input_text or input_text == "q":
            break
        inputs = preprocess_input(input_text)
        print(
            shark_module.forward(
                (inputs["input_ids"], inputs["attention_mask"])
            )
        )

```

`tank/examples/bloom/README.md`:

```md
# Bloom model

## Installation

<details>
  <summary>Installation (Linux)</summary>

### Activate shark.venv Virtual Environment

```shell
source shark.venv/bin/activate

# Some older pip installs may not be able to handle the recent PyTorch deps
python -m pip install --upgrade pip
```

### Install dependencies

```shell
pip install transformers==4.21.2
```
Use this branch of Torch-MLIR for running the model: https://github.com/vivekkhandelwal1/torch-mlir/tree/bloom-ops


### Run bloom model

```shell
python bloom_model.py
```

The runtime device, model config, and text prompt can be specified with `--device <device string>`, `--config <config string>`, `--prompt <prompt string>` respectively.

To run the complete 176B params bloom model, run the following command:
```shell
python bloom_model.py --config "bloom"
```

```

`tank/examples/bloom/bloom_model.py`:

```py
### Please do `pip install transformers==4.21.2` before running this script.

### To run the complete bloom model: pass as argument "--config bloom".

import argparse
import torch
import torch_mlir
from transformers import BloomTokenizerFast, BloomForSequenceClassification

from torch.fx.experimental.proxy_tensor import make_fx
from torch._decomp import get_decompositions
from shark.shark_inference import SharkInference

p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
p.add_argument(
    "--prompt",
    type=str,
    default="Hello, my dog is cute",
    help="the text prompt to use",
)
p.add_argument("--device", type=str, default="cpu", help="the device to use")
p.add_argument("--seed", type=int, default=0, help="the random seed")
p.add_argument(
    "--config",
    type=str,
    default="bloom-560m",
    help="the configuration of model to use",
)
args = p.parse_args()

torch.manual_seed(args.seed)

model_config = "bigscience/" + args.config
tokenizer = BloomTokenizerFast.from_pretrained(model_config)
test_input = tokenizer(args.prompt, return_tensors="pt")["input_ids"]


class HuggingFaceLanguage(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = BloomForSequenceClassification.from_pretrained(
            model_config
        )

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


model = HuggingFaceLanguage()
actual_out = model(test_input)

# import numpy as np
# test_input_ny = test_input.detach().numpy()
# input_tuple = (test_input_ny,)
# np.savez('inputs.npz', *input_tuple)
# output_ny = actual_out.detach().numpy()
# output_tuple = (output_ny,)
# np.savez('golden_out.npz', *output_tuple)

fx_g = make_fx(
    model,
    decomposition_table=get_decompositions(
        [
            torch.ops.aten.split.Tensor,
            torch.ops.aten.split_with_sizes,
        ]
    ),
)(test_input)

# # print(fx_g.graph)

fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
fx_g.recompile()


def strip_overloads(gm):
    """
    Modifies the target of graph nodes in :attr:`gm` to strip overloads.
    Args:
        gm(fx.GraphModule): The input Fx graph module to be modified
    """
    for node in gm.graph.nodes:
        if isinstance(node.target, torch._ops.OpOverload):
            node.target = node.target.overloadpacket
    gm.recompile()


strip_overloads(fx_g)

ts_g = torch.jit.script(fx_g)

module = torch_mlir.compile(
    ts_g,
    [test_input],
    torch_mlir.OutputType.LINALG_ON_TENSORS,
    use_tracing=True,
    verbose=False,
)
# # module.dump()

mlir_model = module
func_name = "forward"

shark_module = SharkInference(
    mlir_model, func_name, device=args.device, mlir_dialect="tm_tensor"
)
shark_module.compile()


def shark_result(x):
    x_ny = x.detach().numpy()
    inputs = (x_ny,)
    result = shark_module.forward(inputs)
    return torch.from_numpy(result)


observed_out = shark_result(test_input)

print("Golden result:", actual_out)
print("SHARK result:", observed_out)

```

`tank/examples/deberta-base_tf/deberta-base_tf_test.py`:

```py
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model
from shark.parser import shark_args
from tank.test_utils import get_valid_test_params, shark_test_name_func
from parameterized import parameterized

import iree.compiler as ireec
import unittest
import pytest
import numpy as np
import tempfile
import os


class DebertaBaseModuleTester:
    def __init__(
        self,
        benchmark=False,
    ):
        self.benchmark = benchmark

    def create_and_check_module(self, dynamic, device):
        model, func_name, inputs, golden_out = download_model(
            "microsoft/deberta-base", frontend="tf"
        )

        shark_module = SharkInference(
            model, func_name, device=device, mlir_dialect="mhlo"
        )
        shark_module.compile()
        result = shark_module.forward(inputs)
        np.testing.assert_allclose(golden_out, result, rtol=1e-02, atol=1e-03)


class DebertaBaseModuleTest(unittest.TestCase):
    @pytest.skip(reason="Model can't be imported.", allow_module_level=True)
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.module_tester = DebertaBaseModuleTester(self)
        self.module_tester.benchmark = pytestconfig.getoption("benchmark")

    param_list = get_valid_test_params()

    @parameterized.expand(param_list, name_func=shark_test_name_func)
    def test_module(self, dynamic, device):
        self.module_tester.create_and_check_module(dynamic, device)


if __name__ == "__main__":
    unittest.main()

```

`tank/examples/gpt2-64/gpt2-64_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-64.tflite"


def generate_inputs(input_details):
    args = []
    args.append(
        np.random.randint(
            low=0,
            high=256,
            size=input_details[0]["shape"],
            dtype=input_details[0]["dtype"],
        )
    )
    return args


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        # mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class GptTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_model(
            model_name="gpt2-64", backend="tflite"
        )
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 64],
                "dtype": np.int32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class GptTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = GptTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = GptTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/examples/opt/README.md`:

```md
# Run OPT for sentence completion through SHARK

From base SHARK directory, follow instructions to set up a virtual environment with SHARK. (`./setup_venv.sh` or `./setup_venv.ps1`)
Then, you may run opt_causallm.py to get a very simple sentence completion application running through SHARK
```
python opt_causallm.py
```

# Run OPT performance comparison on SHARK vs. PyTorch

```
python opt_perf_comparison.py --max-seq-len=512 --model-name=facebook/opt-1.3b \
        --platform=shark
```
Any OPT model from huggingface should work with this script, and you can choose between `--platform=shark` or `--platform=huggingface` to generate benchmarks of OPT inference on SHARK / PyTorch. 

# Run a small suite of OPT models through the benchmark script

```
python opt_perf_comparison_batch.py
```
This script will run benchmarks from a suite of OPT configurations:
- Sequence Lengths: 32, 128, 256, 512
- Parameter Counts: 125m, 350m, 1.3b

note: Most of these scripts are written for use on CPU, as perf comparisons against pytorch can be problematic across platforms otherwise.

```

`tank/examples/opt/opt_causallm.py`:

```py
import argparse
import os
import torch
import numpy as np
from shark_opt_wrapper import OPTForCausalLMModel
from shark.shark_inference import SharkInference
from shark.shark_importer import import_with_fx
from transformers import AutoTokenizer, OPTForCausalLM
from typing import Iterable


def create_module(model_name, tokenizer, device, args):
    opt_base_model = OPTForCausalLM.from_pretrained(
        model_name, allow_mismatched_sizes=True
    )
    opt_base_model.eval()
    opt_model = OPTForCausalLMModel(opt_base_model)
    encoded_inputs = tokenizer(
        "What is the meaning of life?",
        padding="max_length",
        truncation=True,
        max_length=args.max_seq_len,
        return_tensors="pt",
    )
    inputs = (
        encoded_inputs["input_ids"],
        encoded_inputs["attention_mask"],
    )
    # np.save("model_inputs_0.npy", inputs[0])
    # np.save("model_inputs_1.npy", inputs[1])
    opt_fs_name = "-".join(
        "_".join(args.model_name.split("/")[1].split("-")).split(".")
    )

    mlir_path = f"./{opt_fs_name}_causallm_{args.max_seq_len}_torch.mlir"
    if os.path.isfile(mlir_path):
        print(f"Found .mlir from {mlir_path}")
    else:
        (model_mlir, func_name) = import_with_fx(
            model=opt_model,
            inputs=inputs,
            is_f16=False,
            model_name=opt_fs_name,
            return_str=True,
        )
        with open(mlir_path, "w") as f:
            f.write(model_mlir)
        print(f"Saved mlir at {mlir_path}")
        del model_mlir

    shark_module = SharkInference(
        mlir_path,
        device=device,
        mlir_dialect="tm_tensor",
        is_benchmark=False,
    )

    vmfb_name = f"{opt_fs_name}_causallm_{args.max_seq_len}_torch_cpu"
    shark_module.save_module(module_name=vmfb_name, debug=False)
    vmfb_path = vmfb_name + ".vmfb"
    return vmfb_path


def shouldStop(tokens):
    stop_ids = [50278, 50279, 50277, 0]
    for stop_id in stop_ids:
        if tokens[0][-1] == stop_id:
            return True
    return False


def generate_new_token(shark_module, tokenizer, new_text, max_seq_len: int):
    model_inputs = tokenizer(
        new_text,
        padding="max_length",
        max_length=max_seq_len,
        truncation=True,
        return_tensors="pt",
    )
    inputs = (
        model_inputs["input_ids"],
        model_inputs["attention_mask"],
    )
    sum_attentionmask = torch.sum(model_inputs.attention_mask)
    output = shark_module("forward", inputs)
    output = torch.FloatTensor(output[0])
    next_toks = torch.topk(output, 1)
    stop_generation = False
    if shouldStop(next_toks.indices):
        stop_generation = True
    new_token = next_toks.indices[int(sum_attentionmask) - 1]
    detok = tokenizer.decode(
        new_token,
        skip_special_tokens=False,
        clean_up_tokenization_spaces=False,
    )
    ret_dict = {
        "new_token": new_token,
        "detok": detok,
        "stop_generation": stop_generation,
    }
    return ret_dict


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--max-seq-len", type=int, default=32)
    parser.add_argument(
        "--model-name",
        help="Model name",
        type=str,
        choices=[
            "facebook/opt-125m",
            "facebook/opt-350m",
            "facebook/opt-1.3b",
            "facebook/opt-6.7b",
            "mit-han-lab/opt-125m-smoothquant",
            "mit-han-lab/opt-1.3b-smoothquant",
            "mit-han-lab/opt-2.7b-smoothquant",
            "mit-han-lab/opt-6.7b-smoothquant",
            "mit-han-lab/opt-13b-smoothquant",
        ],
        default="facebook/opt-1.3b",
    )
    parser.add_argument(
        "--recompile",
        help="If set, recompiles MLIR -> .vmfb",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--plugin-path",
        help="path to executable plugin",
        type=str,
        default=None,
    )
    args = parser.parse_args()
    print("args={}".format(args))
    return args


def generate_tokens(
    opt_shark_module: "SharkInference",
    tokenizer,
    input_text: str,
    max_output_len: int,
    print_intermediate_results: True,
) -> Iterable[str]:
    words_list = []
    new_text = input_text
    try:
        for _ in range(max_output_len):
            generated_token_op = generate_new_token(
                opt_shark_module, tokenizer, new_text, max_output_len
            )
            detok = generated_token_op["detok"]
            if generated_token_op["stop_generation"]:
                break
            if print_intermediate_results:
                print(detok, end="", flush=True)
            words_list.append(detok)
            if detok == "":
                break
            new_text += detok
    except KeyboardInterrupt as e:
        print("Exiting token generation.")
    return words_list


if __name__ == "__main__":
    args = parse_args()
    if "smoothquant" in args.model_name:
        token_model_name = f"facebook/opt-{args.model_name.split('-')[3]}"
    else:
        token_model_name = args.model_name
    tokenizer = AutoTokenizer.from_pretrained(token_model_name, use_fast=False)
    opt_fs_name = "-".join(
        "_".join(args.model_name.split("/")[1].split("-")).split(".")
    )
    vmfb_path = f"./{opt_fs_name}_causallm_{args.max_seq_len}_torch_cpu.vmfb"
    if args.plugin_path is not None:
        rt_flags = [f"--executable_plugin={args.plugin_path}"]
    else:
        rt_flags = []
    opt_shark_module = SharkInference(
        mlir_module=None, device="cpu-task", rt_flags=rt_flags
    )
    if os.path.isfile(vmfb_path):
        opt_shark_module.load_module(vmfb_path)
    else:
        vmfb_path = create_module(args.model_name, tokenizer, "cpu-task", args)
        opt_shark_module.load_module(vmfb_path)
    while True:
        input_text = input("Give me a sentence to complete:")
        generate_tokens(
            opt_shark_module, tokenizer, input_text, args.max_seq_len
        )

```

`tank/examples/opt/opt_causallm_samples.py`:

```py
import argparse
import os

import opt_causallm
import opt_util

from shark.shark_inference import SharkInference
from transformers import AutoTokenizer, OPTForCausalLM


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--max-seq-len", type=int, default=32)
    parser.add_argument(
        "--model-name",
        help="Model name",
        type=str,
        choices=[
            "facebook/opt-125m",
            "facebook/opt-350m",
            "facebook/opt-1.3b",
            "facebook/opt-6.7b",
        ],
        default="facebook/opt-1.3b",
    )
    parser.add_argument(
        "--recompile",
        help="If set, recompiles MLIR -> .vmfb",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--plugin-path",
        help="path to executable plugin",
        type=str,
        default=None,
    )
    args = parser.parse_args()
    print("args={}".format(args))
    return args


if __name__ == "__main__":
    args = parse_args()
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    opt_fs_name = "-".join(
        "_".join(args.model_name.split("/")[1].split("-")).split(".")
    )
    vmfb_path = f"./{opt_fs_name}_causallm_{args.max_seq_len}_torch_cpu.vmfb"
    if args.plugin_path is not None:
        rt_flags = [f"--executable_plugin={args.plugin_path}"]
    else:
        rt_flags = []
    opt_shark_module = SharkInference(
        mlir_module=None, device="cpu-task", rt_flags=rt_flags
    )
    if os.path.isfile(vmfb_path):
        opt_shark_module.load_module(vmfb_path)
    else:
        vmfb_path = opt_causallm.create_module(
            args.model_name, tokenizer, "cpu-task", args
        )
        opt_shark_module.load_module(vmfb_path)

    for prompt in opt_util.PROMPTS:
        print("\n\nprompt: {}".format(prompt))
        response = opt_causallm.generate_tokens(
            opt_shark_module,
            tokenizer,
            prompt,
            args.max_seq_len,
            print_intermediate_results=False,
        )
        print("response: {}".format("".join(response)))

```

`tank/examples/opt/opt_causallm_torch_test.py`:

```py
import unittest
import os
import pytest
import torch
import numpy as np
from shark_opt_wrapper import OPTForCausalLMModel
from shark.iree_utils._common import check_device_drivers, device_driver_info
from shark.shark_inference import SharkInference
from shark.shark_importer import import_with_fx, save_mlir
from transformers import AutoTokenizer, OPTForCausalLM

OPT_MODEL = "facebook/opt-1.3b"
OPT_FS_NAME = "opt-1_3b"
OPT_MODEL_66B = "facebook/opt-66b"


class OPTModuleTester:
    def __init__(
        self,
        benchmark=False,
    ):
        self.benchmark = benchmark

    def create_and_check_module(self, dynamic, device, model_name):
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        opt_model = OPTForCausalLM.from_pretrained(
            model_name, return_dict=False
        )
        opt_model.eval()

        model_inputs = tokenizer(
            "The meaning of life is",
            padding="max_length",
            max_length=30,
            truncation=True,
            return_tensors="pt",
        )
        inputs = (
            model_inputs.data["input_ids"],
            model_inputs.data["attention_mask"],
        )
        act_out = opt_model(
            inputs[0], attention_mask=inputs[1], return_dict=False
        )[0]
        (
            mlir_module,
            func_name,
        ) = import_with_fx(
            model=opt_model,
            inputs=inputs,
            is_f16=False,
            model_name=OPT_FS_NAME,
        )
        del opt_model
        opt_filename = f"./{OPT_FS_NAME}_causallm_30_torch_{device}"
        mlir_path = os.path.join(opt_filename, ".mlir")
        with open(mlir_path, "w") as f:
            f.write(mlir_module)
        print(f"Saved mlir at {mlir_path}")
        del mlir_module

        shark_module = SharkInference(
            mlir_path,
            device=device,
            mlir_dialect="tm_tensor",
            is_benchmark=self.benchmark,
        )

        shark_module.compile()
        results = shark_module("forward", inputs)
        print(
            "SHARK logits have shape: ",
            str(results[0].shape) + " : " + str(results[0]),
        )
        print(
            "PyTorch logits have shape: "
            + str(act_out[0].shape)
            + " : "
            + str(act_out[0])
        )
        # exp_out = tokenizer.decode(act_out[0][0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
        # shark_out = tokenizer.decode(results[0][0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
        np.testing.assert_allclose(act_out[0].detach(), results[0])

        if self.benchmark:
            shark_module.shark_runner.benchmark_all_csv(
                inputs,
                "opt",
                dynamic,
                device,
                "torch",
            )


class OPTModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.module_tester = OPTModuleTester(self)
        self.module_tester.save_mlir = False
        self.module_tester.save_vmfb = False
        self.module_tester.benchmark = pytestconfig.getoption("benchmark")

    def test_1_3b_static_cpu(self):
        dynamic = False
        device = "cpu"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    def test_1_3b_dynamic_cpu(self):
        dynamic = True
        device = "cpu"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("cuda"), reason=device_driver_info("cuda")
    )
    def test_1_3b_static_cuda(self):
        dynamic = False
        device = "cuda"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("cuda"), reason=device_driver_info("cuda")
    )
    def test_1_3b_dynamic_cuda(self):
        dynamic = True
        device = "cuda"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    )
    def test_1_3b_static_vulkan(self):
        dynamic = False
        device = "vulkan"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    )
    def test_1_3b_dynamic_vulkan(self):
        dynamic = True
        device = "vulkan"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    # def test_66b_static_cpu(self):
    #    dynamic = False
    #    device = "cpu"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # def test_66b_dynamic_cpu(self):
    #    dynamic = True
    #    device = "cpu"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("cuda"), reason=device_driver_info("cuda")
    # )
    # def test_66b_static_cuda(self):
    #    dynamic = False
    #    device = "cuda"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("cuda"), reason=device_driver_info("cuda")
    # )
    # def test_66b_dynamic_cuda(self):
    #    dynamic = True
    #    device = "cuda"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    # )
    # def test_66b_static_vulkan(self):
    #    dynamic = False
    #    device = "vulkan"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    # )
    # def test_66b_dynamic_vulkan(self):
    #    dynamic = True
    #    device = "vulkan"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )


if __name__ == "__main__":
    unittest.main()

```

`tank/examples/opt/opt_perf_comparison.py`:

```py
"""
Script for comparing OPT model performance between SHARK and Huggingface
PyTorch.

Usage Example:

python opt_perf_comparison.py --max-seq-len=32 --model-name=facebook/opt-125m \
        --platform=shark

python opt_perf_comparison.py --max-seq-len=512 --model-name=facebook/opt-1.3b \
        --platform=shark

See parse_args() below for command line argument usage.
"""

import argparse
import collections
import json
import os
import psutil
import time
import numpy as np
from typing import Tuple

from opt_util import PROMPTS
from shark.shark_inference import SharkInference
from shark.shark_importer import import_with_fx
from transformers import AutoTokenizer, OPTForCausalLM
from shark_opt_wrapper import OPTForCausalLMModel
from shark.parser import shark_args
import iree.compiler as ireec

DEVICE = "cpu"
PLATFORM_SHARK = "shark"
PLATFORM_HUGGINGFACE = "huggingface"

# Dict keys for reports.
REPORT_PLATFORM = "platform"
REPORT_MODEL_NAME = "model"
REPORT_MAX_SEQ_LEN = "max_seq_len"
REPORT_LOAD_TIME = "load_time_sec"
REPORT_RUN_TIME = "run_time_sec"
REPORT_LOAD_PHYSICAL_MEMORY_MB = "load_physical_MB"
REPORT_LOAD_VIRTUAL_MEMORY_MB = "load_virtual_MB"
REPORT_RUN_PHYSICAL_MEMORY_MB = "run_physical_MB"
REPORT_RUN_VIRTUAL_MEMORY_MB = "run_virtual_MB"

ModelWrapper = collections.namedtuple("ModelWrapper", ["model", "tokenizer"])


def get_memory_info():
    pid = os.getpid()
    process = psutil.Process(pid)
    return process.memory_info()


def import_mlir_module(
    model_name: str,
    tokenizer,
    device: str,
    max_seq_len: int,
):
    opt_base_model = OPTForCausalLM.from_pretrained(
        model_name, ignore_mismatched_sizes=True
    )
    opt_base_model.eval()
    opt_model = OPTForCausalLMModel(opt_base_model)
    encoded_inputs = tokenizer(
        PROMPTS[0],
        padding="max_length",
        truncation=True,
        max_length=max_seq_len,
        return_tensors="pt",
    )
    inputs = (
        encoded_inputs["input_ids"],
        encoded_inputs["attention_mask"],
    )
    # np.save("model_inputs_0.npy", inputs[0])
    # np.save("model_inputs_1.npy", inputs[1])

    opt_fs_name = get_opt_fs_name(model_name)
    mlir_path = f"./{opt_fs_name}_causallm_{max_seq_len}_torch.mlir"
    (model_mlir, func_name) = import_with_fx(
        model=opt_model,
        inputs=inputs,
        is_f16=False,
        model_name=opt_fs_name,
        return_str=True,
    )
    with open(mlir_path, "w") as f:
        f.write(model_mlir)
    print(f"Saved mlir at {mlir_path}")


def create_vmfb_module(
    model_name: str,
    tokenizer,
    device: str,
    max_seq_len: int,
    recompile_shark: bool,
):
    opt_fs_name = get_opt_fs_name(model_name)
    mlir_path = f"./{opt_fs_name}_causallm_{max_seq_len}_torch.mlir"
    # If MLIR has already been loaded and recompilation is not requested, use
    # the loaded MLIR file.
    has_mlir = os.path.isfile(mlir_path)
    # The purpose of recompile_shark is to measure compilation time; the
    # compilation time can be correctly measured only when MLIR has already been
    # loaded.
    assert not recompile_shark or has_mlir
    if not has_mlir:
        import_mlir_module(
            model_name,
            tokenizer,
            device,
            max_seq_len,
        )
    shark_module = SharkInference(
        mlir_path,
        device=device,
        mlir_dialect="tm_tensor",
        is_benchmark=False,
        rt_flags=[],
    )

    vmfb_name = f"{opt_fs_name}_causallm_{max_seq_len}_torch_{DEVICE}"
    shark_module.save_module(module_name=vmfb_name)
    vmfb_path = vmfb_name + ".vmfb"
    return vmfb_path


def load_shark_model(
    model_name: str,
    token_model_name: str,
    max_seq_len: int,
    recompile_shark: bool,
    plugin_path: str = [],
) -> ModelWrapper:
    opt_fs_name = get_opt_fs_name(model_name)
    vmfb_name = f"{opt_fs_name}_causallm_{max_seq_len}_torch_{DEVICE}.vmfb"
    tokenizer = AutoTokenizer.from_pretrained(token_model_name, use_fast=False)
    if recompile_shark or not os.path.isfile(vmfb_name):
        print(f"vmfb not found. compiling and saving to {vmfb_name}")
        create_vmfb_module(
            model_name, tokenizer, DEVICE, max_seq_len, recompile_shark
        )
    if plugin_path is not None:
        rt_flags = [f"--executable_plugin={plugin_path}"]
    else:
        rt_flags = []
    shark_module = SharkInference(
        mlir_module=None, device="cpu-task", rt_flags=rt_flags
    )
    shark_module.load_module(vmfb_name)
    return ModelWrapper(model=shark_module, tokenizer=tokenizer)


def run_shark_model(model_wrapper: ModelWrapper, tokens):
    # Generate logits output of OPT model.
    return model_wrapper.model("forward", tokens)


def load_huggingface_model(
    model_name: str, token_model_name: str
) -> ModelWrapper:
    return ModelWrapper(
        model=OPTForCausalLM.from_pretrained(model_name),
        tokenizer=AutoTokenizer.from_pretrained(token_model_name),
    )


def run_huggingface_model(model_wrapper: ModelWrapper, tokens):
    return model_wrapper.model.forward(
        tokens.input_ids, tokens.attention_mask, return_dict=False
    )


def save_json(data, filename):
    with open(filename, "w") as file:
        json.dump(data, file)


def collect_huggingface_logits(
    model_name: str,
    token_model_name: str,
    max_seq_len: int,
    to_save_json: bool,
) -> Tuple[float, float]:
    # Load
    t0 = time.time()
    model_wrapper = load_huggingface_model(model_name, token_model_name)
    load_time = time.time() - t0
    print("--- Took {} seconds to load Huggingface.".format(load_time))
    load_memory_info = get_memory_info()

    results = []
    tokenized_prompts = []
    for prompt in PROMPTS:
        tokens = model_wrapper.tokenizer(
            prompt,
            padding="max_length",
            max_length=max_seq_len,
            truncation=True,
            return_tensors="pt",
        )
        tokenized_prompts.append(tokens)

    # Run
    t0 = time.time()
    for idx, tokens in enumerate(tokenized_prompts):
        print("prompt: {}".format(PROMPTS[idx]))
        logits = run_huggingface_model(model_wrapper, tokens)
        if to_save_json:
            results.append([PROMPTS[idx], logits[0].tolist()])
    run_time = time.time() - t0
    print("--- Took {} seconds to run Huggingface.".format(run_time))
    if to_save_json:
        save_json(results, "/tmp/huggingface.json")
    run_memory_info = get_memory_info()
    return {
        REPORT_PLATFORM: PLATFORM_HUGGINGFACE,
        REPORT_MODEL_NAME: model_name,
        REPORT_MAX_SEQ_LEN: max_seq_len,
        REPORT_LOAD_TIME: load_time,
        REPORT_RUN_TIME: run_time / len(PROMPTS),
        REPORT_LOAD_PHYSICAL_MEMORY_MB: load_memory_info.rss >> 20,
        REPORT_LOAD_VIRTUAL_MEMORY_MB: load_memory_info.vms >> 20,
        REPORT_RUN_PHYSICAL_MEMORY_MB: run_memory_info.rss >> 20,
        REPORT_RUN_VIRTUAL_MEMORY_MB: run_memory_info.vms >> 20,
    }


def collect_shark_logits(
    model_name: str,
    token_model_name: str,
    max_seq_len: int,
    recompile_shark: bool,
    to_save_json: bool,
    plugin_path: str,
) -> Tuple[float, float]:
    # Load
    t0 = time.time()
    model_wrapper = load_shark_model(
        model_name, token_model_name, max_seq_len, recompile_shark, plugin_path
    )
    load_time = time.time() - t0
    print("--- Took {} seconds to load Shark.".format(load_time))
    load_memory_info = get_memory_info()

    results = []
    tokenized_prompts = []
    for prompt in PROMPTS:
        tokens = model_wrapper.tokenizer(
            prompt,
            padding="max_length",
            truncation=True,
            max_length=max_seq_len,
            return_tensors="pt",
        )
        inputs = (
            tokens["input_ids"],
            tokens["attention_mask"],
        )
        tokenized_prompts.append(inputs)

    # Run
    t0 = time.time()
    for idx, tokens in enumerate(tokenized_prompts):
        print("prompt: {}".format(PROMPTS[idx]))
        logits = run_shark_model(model_wrapper, tokens)
        lst = [e.tolist() for e in logits]
        if to_save_json:
            results.append([PROMPTS[idx], lst])
    run_time = time.time() - t0
    print("--- Took {} seconds to run Shark.".format(run_time))
    if to_save_json:
        save_json(results, "/tmp/shark.json")
    platform_postfix = "-compile" if recompile_shark else "-precompiled"
    run_memory_info = get_memory_info()
    return {
        REPORT_PLATFORM: PLATFORM_SHARK + platform_postfix,
        REPORT_MODEL_NAME: model_name,
        REPORT_MAX_SEQ_LEN: max_seq_len,
        REPORT_LOAD_TIME: load_time,
        REPORT_RUN_TIME: run_time / len(PROMPTS),
        REPORT_LOAD_PHYSICAL_MEMORY_MB: load_memory_info.rss >> 20,
        REPORT_LOAD_VIRTUAL_MEMORY_MB: load_memory_info.vms >> 20,
        REPORT_RUN_PHYSICAL_MEMORY_MB: run_memory_info.rss >> 20,
        REPORT_RUN_VIRTUAL_MEMORY_MB: run_memory_info.vms >> 20,
    }


def get_opt_fs_name(model_name: str) -> str:
    """Cleanses the model name ino a file system-friendly name.

    Example: get_opt_fs_name('facebook/opt-1.3b') == 'opt_1-3b'
    """
    slash_split = model_name.split("/")
    assert 1 <= len(slash_split) <= 2, "There should be at most one slash."
    model_name = slash_split[-1]
    for src_pattern, dest_pattern in (("-", "_"), (".", "-")):
        model_name = model_name.replace(src_pattern, dest_pattern)
    return model_name


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--save-json",
        help="If set, saves output JSON.",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--max-seq-len", help="Max sequence length", type=int, default=32
    )
    parser.add_argument(
        "--model-name",
        help="Model name",
        type=str,
        choices=[
            "facebook/opt-125m",
            "facebook/opt-350m",
            "facebook/opt-1.3b",
            "facebook/opt-6.7b",
            "mit-han-lab/opt-125m-smoothquant",
            "mit-han-lab/opt-1.3b-smoothquant",
            "mit-han-lab/opt-2.7b-smoothquant",
            "mit-han-lab/opt-6.7b-smoothquant",
            "mit-han-lab/opt-13b-smoothquant",
        ],
        default="facebook/opt-1.3b",
    )
    parser.add_argument(
        "--recompile-shark",
        help="If set, recompiles MLIR",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--platform",
        help="Either shark or huggingface",
        type=str,
        choices=[PLATFORM_SHARK, PLATFORM_HUGGINGFACE],
        default=PLATFORM_SHARK,
    )
    parser.add_argument(
        "--plugin-path",
        help="path to executable plugin",
        type=str,
        default=None,
    )
    parser.add_argument(
        "--token-model-name",
        help="HF ID to create tokenizer.",
        type=str,
        default=None,
    )
    args = parser.parse_args()
    print("args={}".format(args))
    return args


if __name__ == "__main__":
    args = parse_args()
    if args.token_model_name == None:
        if "smoothquant" in args.model_name:
            args.token_model_name = (
                f"facebook/opt-{args.model_name.split('-')[3]}"
            )
        else:
            args.token_model_name = args.model_name
    if args.platform == PLATFORM_SHARK:
        shark_report = collect_shark_logits(
            args.model_name,
            args.token_model_name,
            args.max_seq_len,
            args.recompile_shark,
            args.save_json,
            args.plugin_path,
        )
        print("# Summary: {}".format(json.dumps(shark_report)))
    else:
        huggingface_report = collect_huggingface_logits(
            args.model_name,
            args.token_model_name,
            args.max_seq_len,
            args.save_json,
        )
        print("# Summary: {}".format(json.dumps(huggingface_report)))

```

`tank/examples/opt/opt_perf_comparison_batch.py`:

```py
"""
Script for running opt_perf_comparison.py in batch with a series of arguments.

Usage: python opt_perf_comparison_batch.py
"""

from typing import Iterable, List
import shlex
import subprocess


def make_commands() -> Iterable[List[str]]:
    command = shlex.split("python opt_perf_comparison.py --no-save-json")
    max_seq_lens = [32, 128, 256, 512]
    model_names = ["facebook/opt-" + e for e in ["125m", "350m", "1.3b"]]
    for max_seq_len in max_seq_lens:
        for model_name in model_names:
            yield command + [
                f"--max-seq-len={max_seq_len}",
                f"--model-name={model_name}",
            ]


def main():
    for command in make_commands():
        result = subprocess.run(command, check=True)


if __name__ == "__main__":
    main()

```

`tank/examples/opt/opt_torch_test.py`:

```py
import unittest

import pytest
import torch_mlir
from shark_hf_opt import OPTModel
from shark.iree_utils._common import check_device_drivers, device_driver_info
from shark.shark_inference import SharkInference
from tank.model_utils import compare_tensors
from transformers import AutoTokenizer

OPT_MODEL = "facebook/opt-350m"
OPT_MODEL_66B = "facebook/opt-66b"


class OPTModuleTester:
    def __init__(
        self,
        benchmark=False,
    ):
        self.benchmark = benchmark

    def create_and_check_module(self, dynamic, device, model_name):
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        # config = OPTConfig()
        # opt_model = OPTModel(config)
        opt_model = OPTModel.from_pretrained(model_name)
        opt_model.eval()

        inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
        input_ids, attention_mask = (
            inputs.data["input_ids"],
            inputs.data["attention_mask"],
        )

        module = torch_mlir.compile(
            opt_model,
            (input_ids, attention_mask),
            output_type=torch_mlir.OutputType.LINALG_ON_TENSORS,
            use_tracing=True,
        )

        model_mlir = module.operation.get_asm(
            large_elements_limit=None, enable_debug_info=True
        )
        func_name = "forward"
        act_out = opt_model(input_ids, attention_mask).detach()

        # mlir_importer = SharkImporter(
        #    model,
        #    (input,),
        #    frontend="torch",
        # )
        # minilm_mlir, func_name = mlir_importer.import_mlir(
        #    is_dynamic=dynamic, tracing_required=True
        # )

        shark_module = SharkInference(
            model_mlir,
            device=device,
            mlir_dialect="tm_tensor",
            is_benchmark=self.benchmark,
        )
        shark_module.compile()
        results = shark_module("forward", (input_ids, attention_mask))
        assert compare_tensors(act_out, results)

        if self.benchmark:
            shark_module.shark_runner.benchmark_all_csv(
                (input_ids, attention_mask),
                "opt",
                dynamic,
                device,
                "torch",
            )


class OPTModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.module_tester = OPTModuleTester(self)
        self.module_tester.save_mlir = False
        self.module_tester.save_vmfb = False
        self.module_tester.benchmark = pytestconfig.getoption("benchmark")

    def test_350m_static_cpu(self):
        dynamic = False
        device = "cpu"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    def test_350m_dynamic_cpu(self):
        dynamic = True
        device = "cpu"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("cuda"), reason=device_driver_info("cuda")
    )
    def test_350m_static_cuda(self):
        dynamic = False
        device = "cuda"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("cuda"), reason=device_driver_info("cuda")
    )
    def test_350m_dynamic_cuda(self):
        dynamic = True
        device = "cuda"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    )
    def test_350m_static_vulkan(self):
        dynamic = False
        device = "vulkan"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    @pytest.mark.skipif(
        check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    )
    def test_350m_dynamic_vulkan(self):
        dynamic = True
        device = "vulkan"
        self.module_tester.create_and_check_module(dynamic, device, OPT_MODEL)

    # def test_66b_static_cpu(self):
    #    dynamic = False
    #    device = "cpu"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # def test_66b_dynamic_cpu(self):
    #    dynamic = True
    #    device = "cpu"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("cuda"), reason=device_driver_info("cuda")
    # )
    # def test_66b_static_cuda(self):
    #    dynamic = False
    #    device = "cuda"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("cuda"), reason=device_driver_info("cuda")
    # )
    # def test_66b_dynamic_cuda(self):
    #    dynamic = True
    #    device = "cuda"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    # )
    # def test_66b_static_vulkan(self):
    #    dynamic = False
    #    device = "vulkan"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )

    # @pytest.mark.skipif(
    #    check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    # )
    # def test_66b_dynamic_vulkan(self):
    #    dynamic = True
    #    device = "vulkan"
    #    self.module_tester.create_and_check_module(
    #        dynamic, device, OPT_MODEL_66B
    #    )


if __name__ == "__main__":
    unittest.main()

```

`tank/examples/opt/opt_util.py`:

```py
PROMPTS = [
    "What is the meaning of life?",
    "Tell me something you don't know.",
    "What does Xilinx do?",
    "What is the mass of earth?",
    "What is a poem?",
    "What is recursion?",
    "Tell me a one line joke.",
    "Who is Gilgamesh?",
    "Tell me something about cryptocurrency.",
    "How did it all begin?",
]

```

`tank/examples/opt/shark_hf_base_opt.py`:

```py
import os
import torch
from transformers import AutoTokenizer, OPTForCausalLM
from shark.shark_inference import SharkInference
from shark.shark_importer import import_with_fx, save_mlir
from shark_opt_wrapper import OPTForCausalLMModel

model_name = "facebook/opt-1.3b"
base_model = OPTForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

model = OPTForCausalLMModel(base_model)

prompt = "What is the meaning of life?"
model_inputs = tokenizer(prompt, return_tensors="pt")
inputs = (
    model_inputs["input_ids"],
    model_inputs["attention_mask"],
)

(
    mlir_module,
    func_name,
) = import_with_fx(
    model=model,
    inputs=inputs,
    is_f16=False,
)
mlir_module = save_mlir(
    mlir_module,
    model_name=model_name.split("/")[1],
    frontend="torch",
    mlir_dialect="linalg",
)
shark_module = SharkInference(
    mlir_module,
    device="cpu-sync",
    mlir_dialect="tm_tensor",
)
shark_module.compile()
# Generated logits.
logits = shark_module("forward", inputs=inputs)
print("SHARK module returns logits:")
print(logits[0])

hf_logits = base_model.forward(inputs[0], inputs[1], return_dict=False)[0]

print("PyTorch baseline returns logits:")
print(hf_logits)

```

`tank/examples/opt/shark_opt_wrapper.py`:

```py
import torch


class OPTForCausalLMModel(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask):
        combine_input_dict = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        }
        output = self.model(**combine_input_dict)
        return output.logits

```

`tank/examples/opt/shark_opt_wrapper_train.py`:

```py
# coding=utf-8
# Copyright 2022 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" PyTorch OPT model."""
import random
from typing import List, Optional, Tuple, Union

import torch
import torch.utils.checkpoint
from torch import nn
from torch.nn import CrossEntropyLoss
from transformers import OPTConfig, PreTrainedModel
from transformers.activations import ACT2FN
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
)

_CHECKPOINT_FOR_DOC = "facebook/opt-350m"
_CONFIG_FOR_DOC = "OPTConfig"
_TOKENIZER_FOR_DOC = "GPT2Tokenizer"

# Base model docstring
_EXPECTED_OUTPUT_SHAPE = [1, 8, 1024]


OPT_PRETRAINED_MODEL_ARCHIVE_LIST = [
    "facebook/opt-125m",
    "facebook/opt-350m",
    "facebook/opt-1.3b",
    "facebook/opt-2.7b",
    "facebook/opt-6.7b",
    "facebook/opt-13b",
    "facebook/opt-30b",
    # See all OPT models at https://huggingface.co/models?filter=opt
]


def _make_causal_mask(
    input_ids_shape: torch.Size,
    dtype: torch.dtype,
    past_key_values_length: int = 0,
):
    """
    Make causal mask used for bi-directional self-attention.
    """
    bsz, tgt_len = input_ids_shape
    mask = torch.full((tgt_len, tgt_len), float("-inf"))
    mask_cond = torch.arange(int(mask.size(-1)))
    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
    # mask = mask.to(dtype)

    if past_key_values_length > 0:
        mask = torch.cat(
            [torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask],
            dim=-1,
        )
    return mask[None, None, :, :].expand(
        bsz, 1, tgt_len, tgt_len + past_key_values_length
    )


def _expand_mask(
    mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None
):
    """
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    """
    bsz, src_len = map(int, mask.size())
    tgt_len = tgt_len if tgt_len is not None else src_len

    expanded_mask = (
        mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
    )

    inverted_mask = 1.0 - expanded_mask

    return inverted_mask.masked_fill(
        inverted_mask.to(torch.bool), torch.finfo(dtype).min
    )
    # return inverted_mask.masked_fill(inverted_mask, torch.finfo(dtype).min)


class OPTLearnedPositionalEmbedding(nn.Embedding):
    """
    This module learns positional embeddings up to a fixed maximum size.
    """

    def __init__(self, num_embeddings: int, embedding_dim: int):
        # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2
        # and adjust num_embeddings appropriately. Other models don't have this hack
        self.offset = 2
        super().__init__(num_embeddings + self.offset, embedding_dim)

    def forward(
        self,
        attention_mask: torch.LongTensor,
        past_key_values_length: int = 0,
    ):
        """`input_ids_shape` is expected to be [bsz x seqlen]."""
        attention_mask = attention_mask.long()

        # create positions depending on attention_mask
        positions = (
            torch.cumsum(attention_mask, dim=1).type_as(attention_mask)
            * attention_mask
        ).long() - 1

        # cut positions if `past_key_values_length` is > 0
        positions = positions[:, past_key_values_length:]

        return super().forward(positions + self.offset)


# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->OPT
class OPTAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        is_decoder: bool = False,
        bias: bool = True,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads

        if (self.head_dim * num_heads) != self.embed_dim:
            raise ValueError(
                "embed_dim must be divisible by num_heads (got `embed_dim`:"
                f" {self.embed_dim} and `num_heads`: {num_heads})."
            )
        self.scaling = self.head_dim**-0.5
        self.is_decoder = is_decoder

        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)

    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return (
            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)
            .transpose(1, 2)
            .contiguous()
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        layer_head_mask: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
    ) -> Tuple[
        torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]
    ]:
        """Input shape: Batch x Time x Channel"""

        # if key_value_states are provided this layer is used as a cross-attention layer
        # for the decoder
        is_cross_attention = key_value_states is not None

        # bsz, tgt_len, _ = map(int, hidden_states.size())
        bsz, tgt_len, _ = hidden_states.size()

        # get query proj
        query_states = self.q_proj(hidden_states) * self.scaling
        # get key, value proj
        if is_cross_attention and past_key_value is not None:
            # reuse k,v, cross_attentions
            key_states = past_key_value[0]
            value_states = past_key_value[1]
        elif is_cross_attention:
            # cross_attentions
            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
        elif past_key_value is not None:
            # reuse k, v, self_attention
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        else:
            # self_attention
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)

        if self.is_decoder:
            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
            # Further calls to cross_attention layer can then reuse all cross-attention
            # key/value_states (first "if" case)
            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
            # all previous decoder key/value_states. Further calls to uni-directional self-attention
            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
            # if encoder bi-directional self-attention `past_key_value` is always `None`
            past_key_value = (key_states, value_states)

        proj_shape = (bsz * self.num_heads, -1, self.head_dim)
        query_states = self._shape(query_states, tgt_len, bsz).view(
            *proj_shape
        )
        key_states = key_states.view(*proj_shape)
        value_states = value_states.view(*proj_shape)

        src_len = key_states.size(1)
        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))

        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
            raise ValueError(
                "Attention weights should be of size"
                f" {(bsz * self.num_heads, tgt_len, src_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, tgt_len, src_len):
                raise ValueError(
                    "Attention mask should be of size"
                    f" {(bsz, 1, tgt_len, src_len)}, but is"
                    f" {attention_mask.size()}"
                )
            attn_weights = (
                attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
                + attention_mask
            )
            attn_weights = attn_weights.view(
                bsz * self.num_heads, tgt_len, src_len
            )

        attn_weights = nn.functional.softmax(attn_weights, dim=-1)

        if layer_head_mask is not None:
            if layer_head_mask.size() != (self.num_heads,):
                raise ValueError(
                    "Head mask for a single layer should be of size"
                    f" {(self.num_heads,)}, but is {layer_head_mask.size()}"
                )
            attn_weights = layer_head_mask.view(
                1, -1, 1, 1
            ) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.view(
                bsz * self.num_heads, tgt_len, src_len
            )

        if output_attentions:
            # this operation is a bit awkward, but it's required to
            # make sure that attn_weights keeps its gradient.
            # In order to do so, attn_weights have to be reshaped
            # twice and have to be reused in the following
            attn_weights_reshaped = attn_weights.view(
                bsz, self.num_heads, tgt_len, src_len
            )
            attn_weights = attn_weights_reshaped.view(
                bsz * self.num_heads, tgt_len, src_len
            )
        else:
            attn_weights_reshaped = None

        attn_probs = nn.functional.dropout(
            attn_weights, p=self.dropout, training=self.training
        )

        attn_output = torch.bmm(attn_probs, value_states)
        if attn_output.size() != (
            bsz * self.num_heads,
            tgt_len,
            self.head_dim,
        ):
            raise ValueError(
                "`attn_output` should be of size"
                f" {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.view(
            bsz, self.num_heads, tgt_len, self.head_dim
        )
        attn_output = attn_output.transpose(1, 2)

        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
        # partitioned aross GPUs when using tensor-parallelism.
        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)

        attn_output = self.out_proj(attn_output)

        return attn_output, attn_weights_reshaped, past_key_value


class OPTDecoderLayer(nn.Module):
    def __init__(self, config: OPTConfig):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.self_attn = OPTAttention(
            embed_dim=self.embed_dim,
            num_heads=config.num_attention_heads,
            dropout=config.attention_dropout,
            is_decoder=True,
            bias=config.enable_bias,
        )
        self.do_layer_norm_before = config.do_layer_norm_before
        self.dropout = config.dropout
        self.activation_fn = ACT2FN[config.activation_function]

        self.activation_dropout = config.activation_dropout

        self.self_attn_layer_norm = nn.LayerNorm(
            self.embed_dim,
            elementwise_affine=config.layer_norm_elementwise_affine,
        )
        self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim)
        self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(
            self.embed_dim,
            elementwise_affine=config.layer_norm_elementwise_affine,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        layer_head_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
    ) -> Tuple[
        torch.FloatTensor,
        Optional[Tuple[torch.FloatTensor, torch.FloatTensor]],
    ]:
        # TODO: Refactor this function

        residual = hidden_states

        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
        if self.do_layer_norm_before:
            hidden_states = self.self_attn_layer_norm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            past_key_value=past_key_value,
            attention_mask=attention_mask,
            layer_head_mask=layer_head_mask,
            output_attentions=output_attentions,
        )
        hidden_states = nn.functional.dropout(
            hidden_states, p=self.dropout, training=self.training
        )
        hidden_states = residual + hidden_states

        # 350m applies layer norm AFTER attention
        if not self.do_layer_norm_before:
            hidden_states = self.self_attn_layer_norm(hidden_states)

        # Fully Connected
        hidden_states_shape = hidden_states.shape
        hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))
        residual = hidden_states

        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
        if self.do_layer_norm_before:
            hidden_states = self.final_layer_norm(hidden_states)

        hidden_states = self.fc1(hidden_states)
        hidden_states = self.activation_fn(hidden_states)

        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(
            hidden_states, p=self.dropout, training=self.training
        )

        hidden_states = (residual + hidden_states).view(hidden_states_shape)

        # 350m applies layer norm AFTER attention
        if not self.do_layer_norm_before:
            hidden_states = self.final_layer_norm(hidden_states)

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs


class OPTPreTrainedModel(PreTrainedModel):
    config_class = OPTConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["OPTDecoderLayer"]
    _keys_to_ignore_on_load_unexpected = [r"decoder.version"]

    def _init_weights(self, module):
        std = self.config.init_std
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, (OPTDecoder)):
            module.gradient_checkpointing = value


class OPTDecoder(OPTPreTrainedModel):
    def __init__(self, config: OPTConfig):
        super().__init__(config)
        self.dropout = config.dropout
        self.layerdrop = config.layerdrop
        self.padding_idx = config.pad_token_id
        self.max_target_positions = config.max_position_embeddings
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(
            config.vocab_size, config.word_embed_proj_dim, self.padding_idx
        )
        self.embed_positions = OPTLearnedPositionalEmbedding(
            config.max_position_embeddings, config.hidden_size
        )

        if config.word_embed_proj_dim != config.hidden_size:
            self.project_out = nn.Linear(
                config.hidden_size, config.word_embed_proj_dim, bias=False
            )
        else:
            self.project_out = None

        if config.word_embed_proj_dim != config.hidden_size:
            self.project_in = nn.Linear(
                config.word_embed_proj_dim, config.hidden_size, bias=False
            )
        else:
            self.project_in = None

        if config.do_layer_norm_before and not config._remove_final_layer_norm:
            self.final_layer_norm = nn.LayerNorm(
                config.hidden_size,
                elementwise_affine=config.layer_norm_elementwise_affine,
            )
        else:
            self.final_layer_norm = None

        self.layers = nn.ModuleList(
            [OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)]
        )

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask
    def _prepare_decoder_attention_mask(
        self,
        attention_mask,
        input_shape,
        inputs_embeds,
        past_key_values_length,
    ):
        # create causal mask
        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
        combined_attention_mask = None
        if input_shape[-1] > 1:
            combined_attention_mask = _make_causal_mask(
                input_shape,
                inputs_embeds.dtype,
                past_key_values_length=past_key_values_length,
            )  # .to(inputs_embeds.device)

        if attention_mask is not None:
            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
            expanded_attn_mask = _expand_mask(
                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
            )
            combined_attention_mask = (
                expanded_attn_mask
                if combined_attention_mask is None
                else expanded_attn_mask + combined_attention_mask
            )

        return combined_attention_mask

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        # TODO: Refactor this function

        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        use_cache = (
            use_cache if use_cache is not None else self.config.use_cache
        )

        return_dict = (
            return_dict
            if return_dict is not None
            else self.config.use_return_dict
        )

        # retrieve input_ids and inputs_embeds
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError(
                "You cannot specify both decoder_input_ids and"
                " decoder_inputs_embeds at the same time"
            )
        elif input_ids is not None:
            input_shape = input_ids.size()
            input_ids = input_ids.view(-1, input_shape[-1])
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
        else:
            raise ValueError(
                "You have to specify either decoder_input_ids or"
                " decoder_inputs_embeds"
            )

        past_key_values_length = (
            past_key_values[0][0].shape[2]
            if past_key_values is not None
            else 0
        )

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        # embed positions
        if attention_mask is None:
            attention_mask = torch.ones(
                inputs_embeds.shape[:2],
                dtype=torch.bool,
                device=inputs_embeds.device,
            )
        pos_embeds = self.embed_positions(
            attention_mask, past_key_values_length
        )

        attention_mask = self._prepare_decoder_attention_mask(
            attention_mask, input_shape, inputs_embeds, past_key_values_length
        )

        if self.project_in is not None:
            inputs_embeds = self.project_in(inputs_embeds)

        hidden_states = inputs_embeds + pos_embeds
        hidden_states = nn.functional.dropout(
            hidden_states, p=self.dropout, training=self.training
        )

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = () if use_cache else None

        # check if head_mask has a correct number of layers specified if desired
        for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
            if attn_mask is not None:
                if attn_mask.size()[0] != (len(self.layers)):
                    raise ValueError(
                        f"The `{mask_name}` should be specified for"
                        f" {len(self.layers)} layers, but it is for"
                        f" {head_mask.size()[0]}."
                    )

        for idx, decoder_layer in enumerate(self.layers):
            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            dropout_probability = random.uniform(0, 1)
            if self.training and (dropout_probability < self.layerdrop):
                continue

            past_key_value = (
                past_key_values[idx] if past_key_values is not None else None
            )

            if self.gradient_checkpointing and self.training:
                if use_cache:
                    use_cache = False

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # None for past_key_value
                        return module(*inputs, output_attentions, None)

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(decoder_layer),
                    hidden_states,
                    attention_mask,
                    head_mask[idx] if head_mask is not None else None,
                    None,
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    layer_head_mask=(
                        head_mask[idx] if head_mask is not None else None
                    ),
                    past_key_value=past_key_value,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                )

            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache += (
                    layer_outputs[2 if output_attentions else 1],
                )

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        if self.final_layer_norm is not None:
            hidden_states = self.final_layer_norm(hidden_states)

        if self.project_out is not None:
            hidden_states = self.project_out(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        next_cache = next_decoder_cache if use_cache else None
        if not return_dict:
            # TODO: This tuple needs to be a static list (of tensors)
            # return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
            return hidden_states
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )


class OPTModel(OPTPreTrainedModel):
    def __init__(self, config: OPTConfig):
        super().__init__(config)
        self.decoder = OPTDecoder(config)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.decoder.embed_tokens

    def set_input_embeddings(self, value):
        self.decoder.embed_tokens = value

    def get_decoder(self):
        return self.decoder

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        use_cache = (
            use_cache if use_cache is not None else self.config.use_cache
        )
        return_dict = (
            return_dict
            if return_dict is not None
            else self.config.use_return_dict
        )

        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)
        decoder_outputs = self.decoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            head_mask=head_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # if not return_dict:
        #     return decoder_outputs

        # return BaseModelOutputWithPast(
        #     last_hidden_state=decoder_outputs.last_hidden_state,
        #     past_key_values=decoder_outputs.past_key_values,
        #     hidden_states=decoder_outputs.hidden_states,
        #     attentions=decoder_outputs.attentions,
        # )
        return decoder_outputs.last_hidden_state


class OPTForCausalLM(OPTPreTrainedModel):
    _keys_to_ignore_on_load_missing = [r"lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.model = OPTModel(config)

        # the lm_head weight is automatically tied to the embed tokens weight
        self.lm_head = nn.Linear(
            config.word_embed_proj_dim, config.vocab_size, bias=False
        )

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.decoder.embed_tokens

    def set_input_embeddings(self, value):
        self.model.decoder.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model.decoder = decoder

    def get_decoder(self):
        return self.model.decoder

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        # TODO: Refactor this function

        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict
            if return_dict is not None
            else self.config.use_return_dict
        )

        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs = self.model.decoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            head_mask=head_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        logits = self.lm_head(outputs[0]).contiguous()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(
                shift_logits.view(-1, self.config.vocab_size),
                shift_labels.view(-1),
            )

        if not return_dict:
            if isinstance(outputs[1:], tuple):
                output = (logits,) + outputs[1:]
            else:
                output = (logits, outputs[1:])
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past=None,
        attention_mask=None,
        use_cache=None,
        **kwargs,
    ):
        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly
        if attention_mask is None:
            attention_mask = input_ids.new_ones(input_ids.shape)

        if past:
            input_ids = input_ids[:, -1:]
        # first step, decoder_cached_states are empty
        return {
            "input_ids": input_ids,  # encoder_outputs is defined. input_ids not needed
            "attention_mask": attention_mask,
            "past_key_values": past,
            "use_cache": use_cache,
        }

    @staticmethod
    def _reorder_cache(past, beam_idx):
        reordered_past = ()
        for layer_past in past:
            reordered_past += (
                tuple(
                    past_state.index_select(0, beam_idx)
                    for past_state in layer_past
                ),
            )
        return reordered_past

```

`tank/examples/rembert_tf/rembert_tf_test.py`:

```py
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model
from tank.test_utils import get_valid_test_params, shark_test_name_func
from parameterized import parameterized

import iree.compiler as ireec
import unittest
import pytest
import numpy as np


class RemBertModuleTester:
    def __init__(
        self,
        benchmark=False,
    ):
        self.benchmark = benchmark

    def create_and_check_module(self, dynamic, device):
        model, func_name, inputs, golden_out = download_model(
            "google/rembert", frontend="tf"
        )

        shark_module = SharkInference(
            model, func_name, device=device, mlir_dialect="mhlo"
        )
        shark_module.compile()
        result = shark_module.forward(inputs)
        np.testing.assert_allclose(golden_out, result, rtol=1e-02, atol=1e-03)


class RemBertModuleTest(unittest.TestCase):
    @pytest.skip(reason="Model too large to convert.", allow_module_level=True)
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.module_tester = RemBertModuleTester(self)
        self.module_tester.benchmark = pytestconfig.getoption("benchmark")

    param_list = get_valid_test_params()

    @parameterized.expand(param_list, name_func=shark_test_name_func)
    def test_module(self, dynamic, device):
        self.module_tester.create_and_check_module(dynamic, device)


if __name__ == "__main__":
    unittest.main()

```

`tank/examples/tapas-base_tf/tapas-base_tf_test.py`:

```py
from shark.iree_utils._common import check_device_drivers, device_driver_info
from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model

import iree.compiler as ireec
import unittest
import pytest
import numpy as np


class TapasBaseModuleTester:
    def __init__(
        self,
        benchmark=False,
    ):
        self.benchmark = benchmark

    def create_and_check_module(self, dynamic, device):
        model, func_name, inputs, golden_out = download_model(
            "google/tapas-base",
            frontend="tf",
        )

        shark_module = SharkInference(
            model, func_name, device=device, mlir_dialect="mhlo"
        )
        shark_module.compile()
        result = shark_module.forward(inputs)
        np.testing.assert_allclose(golden_out, result, rtol=1e-02, atol=1e-03)


class TapasBaseModuleTest(unittest.TestCase):
    @pytest.skip(
        reason="Input must be a pandas dataframe.", allow_module_level=True
    )
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.module_tester = TapasBaseModuleTester(self)
        self.module_tester.benchmark = pytestconfig.getoption("benchmark")

    def test_module_static_cpu(self):
        dynamic = False
        device = "cpu"
        self.module_tester.create_and_check_module(dynamic, device)

    @pytest.mark.skipif(
        check_device_drivers("cuda"), reason=device_driver_info("cuda")
    )
    def test_module_static_cuda(self):
        dynamic = False
        device = "cuda"
        self.module_tester.create_and_check_module(dynamic, device)

    @pytest.mark.skipif(
        check_device_drivers("vulkan"), reason=device_driver_info("vulkan")
    )
    def test_module_static_vulkan(self):
        dynamic = False
        device = "vulkan"
        self.module_tester.create_and_check_module(dynamic, device)


if __name__ == "__main__":
    unittest.main()

```

`tank/examples/v_diffusion_pytorch/README.md`:

```md
# v-diffusion model

## Installation

<details>
  <summary>Installation (Linux)</summary>

### Activate shark.venv Virtual Environment

```shell
source shark.venv/bin/activate

# Some older pip installs may not be able to handle the recent PyTorch deps
python -m pip install --upgrade pip
```

### Install v-diffusion model and its dependencies

```shell
cd tank/pytorch/v_diffusion/
Run the script setup_v_diffusion_pytorch.sh
```

### Run v-diffusion-pytorch model

```shell
./v-diffusion-pytorch/cfg_sample.py "New York City, oil on canvas":5 -n 5 -bs 5
```

The runtime device can be specified with `--runtime_device=<device string>`

### Run the v-diffusion model via torch-mlir
```shell
./cfg_sample.py "New York City, oil on canvas":5 -n 1 -bs 1 --steps 2
```

### Run the model stored in the tank
```shell
./cfg_sample_from_mlir.py "New York City, oil on canvas":5 -n 1 -bs 1 --steps 2
```
Note that the current model in the tank requires batch size 1 statically.

### Run the model with preprocessing elements taken out
To run the model without preprocessing copy `cc12m_1.py` to replace the version in `v-diffusion-pytorch`
```shell
cp cc12m_1.py v-diffusion-pytorch/diffusion/models
```
Then run
```shell
./cfg_sample_preprocess.py "New York City, oil on canvas":5 -n 1 -bs 1 --steps 2
```

```

`tank/examples/v_diffusion_pytorch/cc12m_1.py`:

```py
from functools import partial
import math

import torch
from torch import nn
from torch.nn import functional as F


class ResidualBlock(nn.Module):
    def __init__(self, main, skip=None):
        super().__init__()
        self.main = nn.Sequential(*main)
        self.skip = skip if skip else nn.Identity()

    def forward(self, input):
        return self.main(input) + self.skip(input)


class ResLinearBlock(ResidualBlock):
    def __init__(self, f_in, f_mid, f_out, is_last=False):
        skip = None if f_in == f_out else nn.Linear(f_in, f_out, bias=False)
        super().__init__(
            [
                nn.Linear(f_in, f_mid),
                nn.ReLU(inplace=True),
                nn.Linear(f_mid, f_out),
                nn.ReLU(inplace=True) if not is_last else nn.Identity(),
            ],
            skip,
        )


class Modulation2d(nn.Module):
    def __init__(self, state, feats_in, c_out):
        super().__init__()
        self.state = state
        self.layer = nn.Linear(feats_in, c_out * 2, bias=False)

    def forward(self, input):
        scales, shifts = self.layer(self.state["cond"]).chunk(2, dim=-1)
        return torch.addcmul(
            shifts[..., None, None], input, scales[..., None, None] + 1
        )


class ResModConvBlock(ResidualBlock):
    def __init__(self, state, feats_in, c_in, c_mid, c_out, is_last=False):
        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)
        super().__init__(
            [
                nn.Conv2d(c_in, c_mid, 3, padding=1),
                nn.GroupNorm(1, c_mid, affine=False),
                Modulation2d(state, feats_in, c_mid),
                nn.ReLU(inplace=True),
                nn.Conv2d(c_mid, c_out, 3, padding=1),
                nn.GroupNorm(1, c_out, affine=False)
                if not is_last
                else nn.Identity(),
                Modulation2d(state, feats_in, c_out)
                if not is_last
                else nn.Identity(),
                nn.ReLU(inplace=True) if not is_last else nn.Identity(),
            ],
            skip,
        )


class SkipBlock(nn.Module):
    def __init__(self, main, skip=None):
        super().__init__()
        self.main = nn.Sequential(*main)
        self.skip = skip if skip else nn.Identity()

    def forward(self, input):
        return torch.cat([self.main(input), self.skip(input)], dim=1)


class FourierFeatures(nn.Module):
    def __init__(self, in_features, out_features, std=1.0):
        super().__init__()
        assert out_features % 2 == 0
        self.weight = nn.Parameter(
            torch.randn([out_features // 2, in_features]) * std
        )
        self.weight.requires_grad_(False)
        # self.register_buffer('weight', torch.randn([out_features // 2, in_features]) * std)

    def forward(self, input):
        f = 2 * math.pi * input @ self.weight.T
        return torch.cat([f.cos(), f.sin()], dim=-1)


class SelfAttention2d(nn.Module):
    def __init__(self, c_in, n_head=1, dropout_rate=0.1):
        super().__init__()
        assert c_in % n_head == 0
        self.norm = nn.GroupNorm(1, c_in)
        self.n_head = n_head
        self.qkv_proj = nn.Conv2d(c_in, c_in * 3, 1)
        self.out_proj = nn.Conv2d(c_in, c_in, 1)
        self.dropout = (
            nn.Identity()
        )  # nn.Dropout2d(dropout_rate, inplace=True)

    def forward(self, input):
        n, c, h, w = input.shape
        qkv = self.qkv_proj(self.norm(input))
        qkv = qkv.view(
            [n, self.n_head * 3, c // self.n_head, h * w]
        ).transpose(2, 3)
        q, k, v = qkv.chunk(3, dim=1)
        scale = k.shape[3] ** -0.25
        att = ((q * scale) @ (k.transpose(2, 3) * scale)).softmax(3)
        y = (att @ v).transpose(2, 3).contiguous().view([n, c, h, w])
        return input + self.dropout(self.out_proj(y))


def expand_to_planes(input, shape):
    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])


class CC12M1Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.shape = (3, 256, 256)
        self.clip_model = "ViT-B/16"
        self.min_t = 0.0
        self.max_t = 1.0

        c = 128  # The base channel count
        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8, c * 8]

        self.mapping_timestep_embed = FourierFeatures(1, 128)
        self.mapping = nn.Sequential(
            ResLinearBlock(512 + 128, 1024, 1024),
            ResLinearBlock(1024, 1024, 1024, is_last=True),
        )

        with torch.no_grad():
            for param in self.mapping.parameters():
                param *= 0.5**0.5

        self.state = {}
        conv_block = partial(ResModConvBlock, self.state, 1024)

        self.timestep_embed = FourierFeatures(1, 16)
        self.down = nn.AvgPool2d(2)
        self.up = nn.Upsample(
            scale_factor=2, mode="bilinear", align_corners=False
        )

        self.net = nn.Sequential(  # 256x256
            conv_block(3 + 16, cs[0], cs[0]),
            conv_block(cs[0], cs[0], cs[0]),
            conv_block(cs[0], cs[0], cs[0]),
            conv_block(cs[0], cs[0], cs[0]),
            SkipBlock(
                [
                    self.down,  # 128x128
                    conv_block(cs[0], cs[1], cs[1]),
                    conv_block(cs[1], cs[1], cs[1]),
                    conv_block(cs[1], cs[1], cs[1]),
                    conv_block(cs[1], cs[1], cs[1]),
                    SkipBlock(
                        [
                            self.down,  # 64x64
                            conv_block(cs[1], cs[2], cs[2]),
                            conv_block(cs[2], cs[2], cs[2]),
                            conv_block(cs[2], cs[2], cs[2]),
                            conv_block(cs[2], cs[2], cs[2]),
                            SkipBlock(
                                [
                                    self.down,  # 32x32
                                    conv_block(cs[2], cs[3], cs[3]),
                                    conv_block(cs[3], cs[3], cs[3]),
                                    conv_block(cs[3], cs[3], cs[3]),
                                    conv_block(cs[3], cs[3], cs[3]),
                                    SkipBlock(
                                        [
                                            self.down,  # 16x16
                                            conv_block(cs[3], cs[4], cs[4]),
                                            SelfAttention2d(
                                                cs[4], cs[4] // 64
                                            ),
                                            conv_block(cs[4], cs[4], cs[4]),
                                            SelfAttention2d(
                                                cs[4], cs[4] // 64
                                            ),
                                            conv_block(cs[4], cs[4], cs[4]),
                                            SelfAttention2d(
                                                cs[4], cs[4] // 64
                                            ),
                                            conv_block(cs[4], cs[4], cs[4]),
                                            SelfAttention2d(
                                                cs[4], cs[4] // 64
                                            ),
                                            SkipBlock(
                                                [
                                                    self.down,  # 8x8
                                                    conv_block(
                                                        cs[4], cs[5], cs[5]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[5], cs[5] // 64
                                                    ),
                                                    conv_block(
                                                        cs[5], cs[5], cs[5]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[5], cs[5] // 64
                                                    ),
                                                    conv_block(
                                                        cs[5], cs[5], cs[5]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[5], cs[5] // 64
                                                    ),
                                                    conv_block(
                                                        cs[5], cs[5], cs[5]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[5], cs[5] // 64
                                                    ),
                                                    SkipBlock(
                                                        [
                                                            self.down,  # 4x4
                                                            conv_block(
                                                                cs[5],
                                                                cs[6],
                                                                cs[6],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[6],
                                                                cs[6] // 64,
                                                            ),
                                                            conv_block(
                                                                cs[6],
                                                                cs[6],
                                                                cs[6],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[6],
                                                                cs[6] // 64,
                                                            ),
                                                            conv_block(
                                                                cs[6],
                                                                cs[6],
                                                                cs[6],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[6],
                                                                cs[6] // 64,
                                                            ),
                                                            conv_block(
                                                                cs[6],
                                                                cs[6],
                                                                cs[6],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[6],
                                                                cs[6] // 64,
                                                            ),
                                                            conv_block(
                                                                cs[6],
                                                                cs[6],
                                                                cs[6],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[6],
                                                                cs[6] // 64,
                                                            ),
                                                            conv_block(
                                                                cs[6],
                                                                cs[6],
                                                                cs[6],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[6],
                                                                cs[6] // 64,
                                                            ),
                                                            conv_block(
                                                                cs[6],
                                                                cs[6],
                                                                cs[6],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[6],
                                                                cs[6] // 64,
                                                            ),
                                                            conv_block(
                                                                cs[6],
                                                                cs[6],
                                                                cs[5],
                                                            ),
                                                            SelfAttention2d(
                                                                cs[5],
                                                                cs[5] // 64,
                                                            ),
                                                            self.up,
                                                        ]
                                                    ),
                                                    conv_block(
                                                        cs[5] * 2, cs[5], cs[5]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[5], cs[5] // 64
                                                    ),
                                                    conv_block(
                                                        cs[5], cs[5], cs[5]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[5], cs[5] // 64
                                                    ),
                                                    conv_block(
                                                        cs[5], cs[5], cs[5]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[5], cs[5] // 64
                                                    ),
                                                    conv_block(
                                                        cs[5], cs[5], cs[4]
                                                    ),
                                                    SelfAttention2d(
                                                        cs[4], cs[4] // 64
                                                    ),
                                                    self.up,
                                                ]
                                            ),
                                            conv_block(
                                                cs[4] * 2, cs[4], cs[4]
                                            ),
                                            SelfAttention2d(
                                                cs[4], cs[4] // 64
                                            ),
                                            conv_block(cs[4], cs[4], cs[4]),
                                            SelfAttention2d(
                                                cs[4], cs[4] // 64
                                            ),
                                            conv_block(cs[4], cs[4], cs[4]),
                                            SelfAttention2d(
                                                cs[4], cs[4] // 64
                                            ),
                                            conv_block(cs[4], cs[4], cs[3]),
                                            SelfAttention2d(
                                                cs[3], cs[3] // 64
                                            ),
                                            self.up,
                                        ]
                                    ),
                                    conv_block(cs[3] * 2, cs[3], cs[3]),
                                    conv_block(cs[3], cs[3], cs[3]),
                                    conv_block(cs[3], cs[3], cs[3]),
                                    conv_block(cs[3], cs[3], cs[2]),
                                    self.up,
                                ]
                            ),
                            conv_block(cs[2] * 2, cs[2], cs[2]),
                            conv_block(cs[2], cs[2], cs[2]),
                            conv_block(cs[2], cs[2], cs[2]),
                            conv_block(cs[2], cs[2], cs[1]),
                            self.up,
                        ]
                    ),
                    conv_block(cs[1] * 2, cs[1], cs[1]),
                    conv_block(cs[1], cs[1], cs[1]),
                    conv_block(cs[1], cs[1], cs[1]),
                    conv_block(cs[1], cs[1], cs[0]),
                    self.up,
                ]
            ),
            conv_block(cs[0] * 2, cs[0], cs[0]),
            conv_block(cs[0], cs[0], cs[0]),
            conv_block(cs[0], cs[0], cs[0]),
            conv_block(cs[0], cs[0], 3, is_last=True),
        )

        with torch.no_grad():
            for param in self.net.parameters():
                param *= 0.5**0.5

    def forward(self, input, timestep_embed, selfcond):
        self.state["cond"] = selfcond
        out = self.net(torch.cat([input, timestep_embed], dim=1))
        self.state.clear()
        return out

```

`tank/examples/v_diffusion_pytorch/cfg_sample.py`:

```py
#!/usr/bin/env python3

"""Classifier-free guidance sampling from a diffusion model."""

import argparse
from functools import partial
from pathlib import Path

from PIL import Image
import torch
from torch import nn
from torch.nn import functional as F
from torchvision import transforms
from torchvision.transforms import functional as TF
from tqdm import trange

from shark.shark_inference import SharkInference

import sys

sys.path.append("v-diffusion-pytorch")
from CLIP import clip
from diffusion import get_model, get_models, sampling, utils

MODULE_DIR = Path(__file__).resolve().parent


def parse_prompt(prompt, default_weight=3.0):
    if prompt.startswith("http://") or prompt.startswith("https://"):
        vals = prompt.rsplit(":", 2)
        vals = [vals[0] + ":" + vals[1], *vals[2:]]
    else:
        vals = prompt.rsplit(":", 1)
    vals = vals + ["", default_weight][len(vals) :]
    return vals[0], float(vals[1])


def resize_and_center_crop(image, size):
    fac = max(size[0] / image.size[0], size[1] / image.size[1])
    image = image.resize(
        (int(fac * image.size[0]), int(fac * image.size[1])), Image.LANCZOS
    )
    return TF.center_crop(image, size[::-1])


# def main():
p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
p.add_argument(
    "prompts", type=str, default=[], nargs="*", help="the text prompts to use"
)
p.add_argument(
    "--images",
    type=str,
    default=[],
    nargs="*",
    metavar="IMAGE",
    help="the image prompts",
)
p.add_argument(
    "--batch-size",
    "-bs",
    type=int,
    default=1,
    help="the number of images per batch",
)
p.add_argument("--checkpoint", type=str, help="the checkpoint to use")
p.add_argument("--device", type=str, help="the device to use")
p.add_argument(
    "--runtime_device",
    type=str,
    help="the device to use with SHARK",
    default="cpu",
)
p.add_argument(
    "--eta",
    type=float,
    default=0.0,
    help="the amount of noise to add during sampling (0-1)",
)
p.add_argument("--init", type=str, help="the init image")
p.add_argument(
    "--method",
    type=str,
    default="plms",
    choices=["ddpm", "ddim", "prk", "plms", "pie", "plms2", "iplms"],
    help="the sampling method to use",
)
p.add_argument(
    "--model",
    type=str,
    default="cc12m_1_cfg",
    choices=["cc12m_1_cfg"],
    help="the model to use",
)
p.add_argument(
    "-n", type=int, default=1, help="the number of images to sample"
)
p.add_argument("--seed", type=int, default=0, help="the random seed")
p.add_argument("--size", type=int, nargs=2, help="the output image size")
p.add_argument(
    "--starting-timestep",
    "-st",
    type=float,
    default=0.9,
    help="the timestep to start at (used with init images)",
)
p.add_argument("--steps", type=int, default=50, help="the number of timesteps")
args = p.parse_args()

if args.device:
    device = torch.device(args.device)
else:
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

model = get_model(args.model)()
_, side_y, side_x = model.shape
if args.size:
    side_x, side_y = args.size
checkpoint = args.checkpoint
if not checkpoint:
    checkpoint = MODULE_DIR / f"checkpoints/{args.model}.pth"
model.load_state_dict(torch.load(checkpoint, map_location="cpu"))
if device.type == "cuda":
    model = model.half()
model = model.to(device).eval().requires_grad_(False)
clip_model_name = (
    model.clip_model if hasattr(model, "clip_model") else "ViT-B/16"
)
clip_model = clip.load(clip_model_name, jit=False, device=device)[0]
clip_model.eval().requires_grad_(False)
normalize = transforms.Normalize(
    mean=[0.48145466, 0.4578275, 0.40821073],
    std=[0.26862954, 0.26130258, 0.27577711],
)

if args.init:
    init = Image.open(utils.fetch(args.init)).convert("RGB")
    init = resize_and_center_crop(init, (side_x, side_y))
    init = (
        utils.from_pil_image(init).to(device)[None].repeat([args.n, 1, 1, 1])
    )

zero_embed = torch.zeros([1, clip_model.visual.output_dim], device=device)
target_embeds, weights = [zero_embed], []

for prompt in args.prompts:
    txt, weight = parse_prompt(prompt)
    target_embeds.append(
        clip_model.encode_text(clip.tokenize(txt).to(device)).float()
    )
    weights.append(weight)

for prompt in args.images:
    path, weight = parse_prompt(prompt)
    img = Image.open(utils.fetch(path)).convert("RGB")
    clip_size = clip_model.visual.input_resolution
    img = resize_and_center_crop(img, (clip_size, clip_size))
    batch = TF.to_tensor(img)[None].to(device)
    embed = F.normalize(
        clip_model.encode_image(normalize(batch)).float(), dim=-1
    )
    target_embeds.append(embed)
    weights.append(weight)

weights = torch.tensor([1 - sum(weights), *weights], device=device)

torch.manual_seed(args.seed)


def cfg_model_fn(x, t):
    n = x.shape[0]
    n_conds = len(target_embeds)
    x_in = x.repeat([n_conds, 1, 1, 1])
    t_in = t.repeat([n_conds])
    clip_embed_in = torch.cat([*target_embeds]).repeat([n, 1])
    vs = model(x_in, t_in, clip_embed_in).view([n_conds, n, *x.shape[1:]])
    v = vs.mul(weights[:, None, None, None, None]).sum(0)
    return v


x = torch.randn([args.n, 3, side_y, side_x], device=device)
t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
steps = utils.get_spliced_ddpm_cosine_schedule(t)
min_batch_size = min(args.n, args.batch_size)
x_in = x[0:min_batch_size, :, :, :]
ts = x_in.new_ones([x_in.shape[0]])
t_in = t[0] * ts

from torch.fx.experimental.proxy_tensor import make_fx
from torch._decomp import get_decompositions
import torch_mlir

fx_g = make_fx(
    cfg_model_fn,
    decomposition_table=get_decompositions(
        [
            torch.ops.aten.embedding_dense_backward,
            torch.ops.aten.native_layer_norm_backward,
            torch.ops.aten.slice_backward,
            torch.ops.aten.select_backward,
            torch.ops.aten.norm.ScalarOpt_dim,
            torch.ops.aten.native_group_norm,
            torch.ops.aten.upsample_bilinear2d.vec,
            torch.ops.aten.split.Tensor,
            torch.ops.aten.split_with_sizes,
        ]
    ),
)(x_in, t_in)

fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
fx_g.recompile()


def strip_overloads(gm):
    """
    Modifies the target of graph nodes in :attr:`gm` to strip overloads.
    Args:
        gm(fx.GraphModule): The input Fx graph module to be modified
    """
    for node in gm.graph.nodes:
        if isinstance(node.target, torch._ops.OpOverload):
            node.target = node.target.overloadpacket
    gm.recompile()


strip_overloads(fx_g)

ts_g = torch.jit.script(fx_g)

module = torch_mlir.compile(
    ts_g,
    [x_in, t_in],
    torch_mlir.OutputType.LINALG_ON_TENSORS,
    use_tracing=False,
)

mlir_model = module
func_name = "forward"

shark_module = SharkInference(
    mlir_model, func_name, device=args.runtime_device, mlir_dialect="linalg"
)
shark_module.compile()


def compiled_cfg_model_fn(x, t):
    x_ny = x.detach().numpy()
    t_ny = t.detach().numpy()
    inputs = (x_ny, t_ny)
    result = shark_module.forward(inputs)
    return torch.from_numpy(result)


from typing import Dict


def save_intermediate_images(args: Dict):
    x = args["x"]
    num_iter = args["i"]
    for j, out in enumerate(x):
        utils.to_pil_image(out).save(f"out_iter_" + str(num_iter) + ".png")
    return


def run(x, steps):
    if args.method == "ddpm":
        return sampling.sample(compiled_cfg_model_fn, x, steps, 1.0, {})
    if args.method == "ddim":
        return sampling.sample(compiled_cfg_model_fn, x, steps, args.eta, {})
    if args.method == "prk":
        return sampling.prk_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms":
        return sampling.plms_sample(
            compiled_cfg_model_fn,
            x,
            steps,
            {},
            callback=save_intermediate_images,
        )
    if args.method == "pie":
        return sampling.pie_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms2":
        return sampling.plms2_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "iplms":
        return sampling.iplms_sample(compiled_cfg_model_fn, x, steps, {})
    assert False


def run_all(x, t, steps, n, batch_size):
    x = torch.randn([n, 3, side_y, side_x], device=device)
    t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
    steps = utils.get_spliced_ddpm_cosine_schedule(t)
    if args.init:
        steps = steps[steps < args.starting_timestep]
        alpha, sigma = utils.t_to_alpha_sigma(steps[0])
        x = init * alpha + x * sigma
    for i in trange(0, n, batch_size):
        cur_batch_size = min(n - i, batch_size)
        outs = run(x[i : i + cur_batch_size], steps)
        for j, out in enumerate(outs):
            utils.to_pil_image(out).save(f"out_{i + j:05}.png")


run_all(x, t, steps, args.n, args.batch_size)

```

`tank/examples/v_diffusion_pytorch/cfg_sample_eager.py`:

```py
#!/usr/bin/env python3

"""Classifier-free guidance sampling from a diffusion model."""

import argparse
from functools import partial
from pathlib import Path

from PIL import Image
import torch
from torch import nn
from torch.nn import functional as F
from torchvision import transforms
from torchvision.transforms import functional as TF
from tqdm import trange

from shark.shark_inference import SharkInference
from shark.torch_mlir_lockstep_tensor import TorchMLIRLockstepTensor

import sys

sys.path.append("v-diffusion-pytorch")
from CLIP import clip
from diffusion import get_model, get_models, sampling, utils

MODULE_DIR = Path(__file__).resolve().parent


def parse_prompt(prompt, default_weight=3.0):
    if prompt.startswith("http://") or prompt.startswith("https://"):
        vals = prompt.rsplit(":", 2)
        vals = [vals[0] + ":" + vals[1], *vals[2:]]
    else:
        vals = prompt.rsplit(":", 1)
    vals = vals + ["", default_weight][len(vals) :]
    return vals[0], float(vals[1])


def resize_and_center_crop(image, size):
    fac = max(size[0] / image.size[0], size[1] / image.size[1])
    image = image.resize(
        (int(fac * image.size[0]), int(fac * image.size[1])), Image.LANCZOS
    )
    return TF.center_crop(image, size[::-1])


# def main():
p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
p.add_argument(
    "prompts", type=str, default=[], nargs="*", help="the text prompts to use"
)
p.add_argument(
    "--images",
    type=str,
    default=[],
    nargs="*",
    metavar="IMAGE",
    help="the image prompts",
)
p.add_argument(
    "--batch-size",
    "-bs",
    type=int,
    default=1,
    help="the number of images per batch",
)
p.add_argument("--checkpoint", type=str, help="the checkpoint to use")
p.add_argument("--device", type=str, help="the device to use")
p.add_argument(
    "--eta",
    type=float,
    default=0.0,
    help="the amount of noise to add during sampling (0-1)",
)
p.add_argument("--init", type=str, help="the init image")
p.add_argument(
    "--method",
    type=str,
    default="plms",
    choices=["ddpm", "ddim", "prk", "plms", "pie", "plms2", "iplms"],
    help="the sampling method to use",
)
p.add_argument(
    "--model",
    type=str,
    default="cc12m_1_cfg",
    choices=["cc12m_1_cfg"],
    help="the model to use",
)
p.add_argument(
    "-n", type=int, default=1, help="the number of images to sample"
)
p.add_argument("--seed", type=int, default=0, help="the random seed")
p.add_argument("--size", type=int, nargs=2, help="the output image size")
p.add_argument(
    "--starting-timestep",
    "-st",
    type=float,
    default=0.9,
    help="the timestep to start at (used with init images)",
)
p.add_argument("--steps", type=int, default=50, help="the number of timesteps")
args = p.parse_args()

if args.device:
    device = torch.device(args.device)
else:
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

model = get_model(args.model)()
_, side_y, side_x = model.shape
if args.size:
    side_x, side_y = args.size
checkpoint = args.checkpoint
if not checkpoint:
    checkpoint = MODULE_DIR / f"checkpoints/{args.model}.pth"
model.load_state_dict(torch.load(checkpoint, map_location="cpu"))
if device.type == "cuda":
    model = model.half()
model = model.to(device).eval().requires_grad_(False)
clip_model_name = (
    model.clip_model if hasattr(model, "clip_model") else "ViT-B/16"
)
clip_model = clip.load(clip_model_name, jit=False, device=device)[0]
clip_model.eval().requires_grad_(False)
normalize = transforms.Normalize(
    mean=[0.48145466, 0.4578275, 0.40821073],
    std=[0.26862954, 0.26130258, 0.27577711],
)

if args.init:
    init = Image.open(utils.fetch(args.init)).convert("RGB")
    init = resize_and_center_crop(init, (side_x, side_y))
    init = (
        utils.from_pil_image(init).to(device)[None].repeat([args.n, 1, 1, 1])
    )

zero_embed = torch.zeros([1, clip_model.visual.output_dim], device=device)
target_embeds, weights = [zero_embed], []

for prompt in args.prompts:
    txt, weight = parse_prompt(prompt)
    target_embeds.append(
        clip_model.encode_text(clip.tokenize(txt).to(device)).float()
    )
    weights.append(weight)

for prompt in args.images:
    path, weight = parse_prompt(prompt)
    img = Image.open(utils.fetch(path)).convert("RGB")
    clip_size = clip_model.visual.input_resolution
    img = resize_and_center_crop(img, (clip_size, clip_size))
    batch = TF.to_tensor(img)[None].to(device)
    embed = F.normalize(
        clip_model.encode_image(normalize(batch)).float(), dim=-1
    )
    target_embeds.append(embed)
    weights.append(weight)

weights = torch.tensor([1 - sum(weights), *weights], device=device)

torch.manual_seed(args.seed)


def cfg_model_fn(x, t):
    n = x.shape[0]
    n_conds = len(target_embeds)
    x_in = x.repeat([n_conds, 1, 1, 1])
    t_in = t.repeat([n_conds])
    clip_embed_in = torch.cat([*target_embeds]).repeat([n, 1])
    vs = model(x_in, t_in, clip_embed_in).view([n_conds, n, *x.shape[1:]])
    v = vs.mul(weights[:, None, None, None, None]).sum(0)
    return v


x = torch.randn([args.n, 3, side_y, side_x], device=device)
t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
steps = utils.get_spliced_ddpm_cosine_schedule(t)
min_batch_size = min(args.n, args.batch_size)
x_in = x[0:min_batch_size, :, :, :]
ts = x_in.new_ones([x_in.shape[0]])
t_in = t[0] * ts

from torch.fx.experimental.proxy_tensor import make_fx
from torch._decomp import get_decompositions
import torch_mlir

fx_g = make_fx(
    cfg_model_fn,
    decomposition_table=get_decompositions(
        [
            torch.ops.aten.embedding_dense_backward,
            torch.ops.aten.native_layer_norm_backward,
            torch.ops.aten.slice_backward,
            torch.ops.aten.select_backward,
            torch.ops.aten.norm.ScalarOpt_dim,
            torch.ops.aten.native_group_norm,
            torch.ops.aten.upsample_bilinear2d.vec,
            torch.ops.aten.split.Tensor,
            torch.ops.aten.split_with_sizes,
        ]
    ),
)(x_in, t_in)

fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
fx_g.recompile()


def strip_overloads(gm):
    """
    Modifies the target of graph nodes in :attr:`gm` to strip overloads.
    Args:
        gm(fx.GraphModule): The input Fx graph module to be modified
    """
    for node in gm.graph.nodes:
        if isinstance(node.target, torch._ops.OpOverload):
            node.target = node.target.overloadpacket
    gm.recompile()


strip_overloads(fx_g)

ts_g = torch.jit.script(fx_g)

# module = torch_mlir.compile(
#    ts_g,
#    [x_in, t_in],
#    torch_mlir.OutputType.LINALG_ON_TENSORS,
#    use_tracing=False,
# )
#
# mlir_model = module
# func_name = "forward"
#
# shark_module = SharkInference(
#    mlir_model, func_name, device="cuda", mlir_dialect="linalg"
# )
# shark_module.compile()


def compiled_cfg_model_fn(x, t):
    x_in_eager = TorchMLIRLockstepTensor(x.clone())
    t_in_eager = TorchMLIRLockstepTensor(t.clone())
    return ts_g(x_in_eager, t_in_eager)


from typing import Dict


def save_intermediate_images(args: Dict):
    x = args["x"]
    num_iter = args["i"]
    for j, out in enumerate(x):
        utils.to_pil_image(out).save(f"out_iter_" + str(num_iter) + ".png")
    return


def run(x, steps):
    if args.method == "ddpm":
        return sampling.sample(compiled_cfg_model_fn, x, steps, 1.0, {})
    if args.method == "ddim":
        return sampling.sample(compiled_cfg_model_fn, x, steps, args.eta, {})
    if args.method == "prk":
        return sampling.prk_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms":
        return sampling.plms_sample(
            compiled_cfg_model_fn,
            x,
            steps,
            {},
            callback=save_intermediate_images,
        )
    if args.method == "pie":
        return sampling.pie_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms2":
        return sampling.plms2_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "iplms":
        return sampling.iplms_sample(compiled_cfg_model_fn, x, steps, {})
    assert False


def run_all(x, t, steps, n, batch_size):
    x = torch.randn([n, 3, side_y, side_x], device=device)
    t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
    steps = utils.get_spliced_ddpm_cosine_schedule(t)
    if args.init:
        steps = steps[steps < args.starting_timestep]
        alpha, sigma = utils.t_to_alpha_sigma(steps[0])
        x = init * alpha + x * sigma
    for i in trange(0, n, batch_size):
        cur_batch_size = min(n - i, batch_size)
        outs = run(x[i : i + cur_batch_size], steps)
        for j, out in enumerate(outs):
            utils.to_pil_image(out).save(f"out_{i + j:05}.png")


steps = 1

run_all(x, t, steps, args.n, args.batch_size)

```

`tank/examples/v_diffusion_pytorch/cfg_sample_from_mlir.py`:

```py
#!/usr/bin/env python3

"""Classifier-free guidance sampling from a diffusion model."""

import argparse
from functools import partial
from pathlib import Path

from PIL import Image
import torch
from torch import nn
from torch.nn import functional as F
from torchvision import transforms
from torchvision.transforms import functional as TF
from tqdm import trange

from shark.shark_inference import SharkInference
from shark.shark_downloader import download_model
import numpy as np

import sys

sys.path.append("v-diffusion-pytorch")
from CLIP import clip
from diffusion import get_model, get_models, sampling, utils

MODULE_DIR = Path(__file__).resolve().parent


def parse_prompt(prompt, default_weight=3.0):
    if prompt.startswith("http://") or prompt.startswith("https://"):
        vals = prompt.rsplit(":", 2)
        vals = [vals[0] + ":" + vals[1], *vals[2:]]
    else:
        vals = prompt.rsplit(":", 1)
    vals = vals + ["", default_weight][len(vals) :]
    return vals[0], float(vals[1])


def resize_and_center_crop(image, size):
    fac = max(size[0] / image.size[0], size[1] / image.size[1])
    image = image.resize(
        (int(fac * image.size[0]), int(fac * image.size[1])), Image.LANCZOS
    )
    return TF.center_crop(image, size[::-1])


# def main():
p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
p.add_argument(
    "prompts", type=str, default=[], nargs="*", help="the text prompts to use"
)
p.add_argument(
    "--images",
    type=str,
    default=[],
    nargs="*",
    metavar="IMAGE",
    help="the image prompts",
)
p.add_argument(
    "--batch-size",
    "-bs",
    type=int,
    default=1,
    help="the number of images per batch",
)
p.add_argument("--checkpoint", type=str, help="the checkpoint to use")
p.add_argument("--device", type=str, help="the device to use")
p.add_argument(
    "--runtime_device",
    type=str,
    help="the device to use with SHARK",
    default="cpu",
)
p.add_argument(
    "--eta",
    type=float,
    default=0.0,
    help="the amount of noise to add during sampling (0-1)",
)
p.add_argument("--init", type=str, help="the init image")
p.add_argument(
    "--method",
    type=str,
    default="plms",
    choices=["ddpm", "ddim", "prk", "plms", "pie", "plms2", "iplms"],
    help="the sampling method to use",
)
p.add_argument(
    "--model",
    type=str,
    default="cc12m_1_cfg",
    choices=["cc12m_1_cfg"],
    help="the model to use",
)
p.add_argument(
    "-n", type=int, default=1, help="the number of images to sample"
)
p.add_argument("--seed", type=int, default=0, help="the random seed")
p.add_argument("--size", type=int, nargs=2, help="the output image size")
p.add_argument(
    "--starting-timestep",
    "-st",
    type=float,
    default=0.9,
    help="the timestep to start at (used with init images)",
)
p.add_argument("--steps", type=int, default=50, help="the number of timesteps")
args = p.parse_args()

if args.device:
    device = torch.device(args.device)
else:
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

model = get_model(args.model)()
_, side_y, side_x = model.shape
if args.size:
    side_x, side_y = args.size
checkpoint = args.checkpoint
if not checkpoint:
    checkpoint = MODULE_DIR / f"checkpoints/{args.model}.pth"
model.load_state_dict(torch.load(checkpoint, map_location="cpu"))
if device.type == "cuda":
    model = model.half()
model = model.to(device).eval().requires_grad_(False)
clip_model_name = (
    model.clip_model if hasattr(model, "clip_model") else "ViT-B/16"
)
clip_model = clip.load(clip_model_name, jit=False, device=device)[0]
clip_model.eval().requires_grad_(False)
normalize = transforms.Normalize(
    mean=[0.48145466, 0.4578275, 0.40821073],
    std=[0.26862954, 0.26130258, 0.27577711],
)

if args.init:
    init = Image.open(utils.fetch(args.init)).convert("RGB")
    init = resize_and_center_crop(init, (side_x, side_y))
    init = (
        utils.from_pil_image(init).to(device)[None].repeat([args.n, 1, 1, 1])
    )

zero_embed = torch.zeros([1, clip_model.visual.output_dim], device=device)
target_embeds, weights = [zero_embed], []

for prompt in args.prompts:
    txt, weight = parse_prompt(prompt)
    target_embeds.append(
        clip_model.encode_text(clip.tokenize(txt).to(device)).float()
    )
    weights.append(weight)

for prompt in args.images:
    path, weight = parse_prompt(prompt)
    img = Image.open(utils.fetch(path)).convert("RGB")
    clip_size = clip_model.visual.input_resolution
    img = resize_and_center_crop(img, (clip_size, clip_size))
    batch = TF.to_tensor(img)[None].to(device)
    embed = F.normalize(
        clip_model.encode_image(normalize(batch)).float(), dim=-1
    )
    target_embeds.append(embed)
    weights.append(weight)

weights = torch.tensor([1 - sum(weights), *weights], device=device)

torch.manual_seed(args.seed)


def cfg_model_fn(x, t):
    n = x.shape[0]
    n_conds = len(target_embeds)
    x_in = x.repeat([n_conds, 1, 1, 1])
    t_in = t.repeat([n_conds])
    clip_embed_in = torch.cat([*target_embeds]).repeat([n, 1])
    vs = model(x_in, t_in, clip_embed_in).view([n_conds, n, *x.shape[1:]])
    v = vs.mul(weights[:, None, None, None, None]).sum(0)
    return v


x = torch.randn([args.n, 3, side_y, side_x], device=device)
t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
steps = utils.get_spliced_ddpm_cosine_schedule(t)
min_batch_size = min(args.n, args.batch_size)
x_in = x[0:min_batch_size, :, :, :]
ts = x_in.new_ones([x_in.shape[0]])
t_in = t[0] * ts

mlir_model, func_name, inputs, golden_out = download_model(
    "v_diffusion", frontend="torch"
)

shark_module = SharkInference(
    mlir_model, func_name, device=args.runtime_device, mlir_dialect="linalg"
)
shark_module.compile()


def compiled_cfg_model_fn(x, t):
    x_ny = x.detach().numpy()
    t_ny = t.detach().numpy()
    inputs = (x_ny, t_ny)
    result = shark_module.forward(inputs)
    return torch.from_numpy(result)


from typing import Dict


def save_intermediate_images(args: Dict):
    x = args["x"]
    num_iter = args["i"]
    for j, out in enumerate(x):
        utils.to_pil_image(out).save(f"out_iter_" + str(num_iter) + ".png")
    return


def run(x, steps):
    if args.method == "ddpm":
        return sampling.sample(compiled_cfg_model_fn, x, steps, 1.0, {})
    if args.method == "ddim":
        return sampling.sample(compiled_cfg_model_fn, x, steps, args.eta, {})
    if args.method == "prk":
        return sampling.prk_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms":
        return sampling.plms_sample(
            compiled_cfg_model_fn,
            x,
            steps,
            {},
            callback=save_intermediate_images,
        )
    if args.method == "pie":
        return sampling.pie_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms2":
        return sampling.plms2_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "iplms":
        return sampling.iplms_sample(compiled_cfg_model_fn, x, steps, {})
    assert False


def run_all(x, t, steps, n, batch_size):
    x = torch.randn([n, 3, side_y, side_x], device=device)
    t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
    steps = utils.get_spliced_ddpm_cosine_schedule(t)
    if args.init:
        steps = steps[steps < args.starting_timestep]
        alpha, sigma = utils.t_to_alpha_sigma(steps[0])
        x = init * alpha + x * sigma
    for i in trange(0, n, batch_size):
        cur_batch_size = min(n - i, batch_size)
        outs = run(x[i : i + cur_batch_size], steps)
        for j, out in enumerate(outs):
            utils.to_pil_image(out).save(f"out_{i + j:05}.png")


run_all(x, t, steps, args.n, args.batch_size)

```

`tank/examples/v_diffusion_pytorch/cfg_sample_preprocess.py`:

```py
#!/usr/bin/env python3

"""Classifier-free guidance sampling from a diffusion model."""

import argparse
from functools import partial
from pathlib import Path

from PIL import Image
import torch
from torch import nn
from torch.nn import functional as F
from torchvision import transforms
from torchvision.transforms import functional as TF
from tqdm import trange
import numpy as np

from shark.shark_inference import SharkInference

import sys

sys.path.append("v-diffusion-pytorch")
from CLIP import clip
from diffusion import get_model, get_models, sampling, utils
from torch.nn import functional as F

MODULE_DIR = Path(__file__).resolve().parent


def parse_prompt(prompt, default_weight=3.0):
    if prompt.startswith("http://") or prompt.startswith("https://"):
        vals = prompt.rsplit(":", 2)
        vals = [vals[0] + ":" + vals[1], *vals[2:]]
    else:
        vals = prompt.rsplit(":", 1)
    vals = vals + ["", default_weight][len(vals) :]
    return vals[0], float(vals[1])


def resize_and_center_crop(image, size):
    fac = max(size[0] / image.size[0], size[1] / image.size[1])
    image = image.resize(
        (int(fac * image.size[0]), int(fac * image.size[1])), Image.LANCZOS
    )
    return TF.center_crop(image, size[::-1])


# def main():
p = argparse.ArgumentParser(
    description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter
)
p.add_argument(
    "prompts", type=str, default=[], nargs="*", help="the text prompts to use"
)
p.add_argument(
    "--images",
    type=str,
    default=[],
    nargs="*",
    metavar="IMAGE",
    help="the image prompts",
)
p.add_argument(
    "--batch-size",
    "-bs",
    type=int,
    default=1,
    help="the number of images per batch",
)
p.add_argument("--checkpoint", type=str, help="the checkpoint to use")
p.add_argument("--device", type=str, help="the device to use")
p.add_argument(
    "--runtime_device",
    type=str,
    help="the device to use with SHARK",
    default="intel-gpu",
)
p.add_argument(
    "--eta",
    type=float,
    default=0.0,
    help="the amount of noise to add during sampling (0-1)",
)
p.add_argument("--init", type=str, help="the init image")
p.add_argument(
    "--method",
    type=str,
    default="plms",
    choices=["ddpm", "ddim", "prk", "plms", "pie", "plms2", "iplms"],
    help="the sampling method to use",
)
p.add_argument(
    "--model",
    type=str,
    default="cc12m_1_cfg",
    choices=["cc12m_1_cfg"],
    help="the model to use",
)
p.add_argument(
    "-n", type=int, default=1, help="the number of images to sample"
)
p.add_argument("--seed", type=int, default=0, help="the random seed")
p.add_argument("--size", type=int, nargs=2, help="the output image size")
p.add_argument(
    "--starting-timestep",
    "-st",
    type=float,
    default=0.9,
    help="the timestep to start at (used with init images)",
)
p.add_argument("--steps", type=int, default=50, help="the number of timesteps")
args = p.parse_args()

if args.device:
    device = torch.device(args.device)
else:
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

model = get_model(args.model)()
_, side_y, side_x = model.shape
if args.size:
    side_x, side_y = args.size
checkpoint = args.checkpoint
if not checkpoint:
    checkpoint = MODULE_DIR / f"checkpoints/{args.model}.pth"
model.load_state_dict(torch.load(checkpoint, map_location="cpu"))
if device.type == "cuda":
    model = model.half()
model = model.to(device).eval().requires_grad_(False)
clip_model_name = (
    model.clip_model if hasattr(model, "clip_model") else "ViT-B/16"
)
clip_model = clip.load(clip_model_name, jit=False, device=device)[0]
clip_model.eval().requires_grad_(False)
normalize = transforms.Normalize(
    mean=[0.48145466, 0.4578275, 0.40821073],
    std=[0.26862954, 0.26130258, 0.27577711],
)

if args.init:
    init = Image.open(utils.fetch(args.init)).convert("RGB")
    init = resize_and_center_crop(init, (side_x, side_y))
    init = (
        utils.from_pil_image(init).to(device)[None].repeat([args.n, 1, 1, 1])
    )

zero_embed = torch.zeros([1, clip_model.visual.output_dim], device=device)
target_embeds, weights = [zero_embed], []

for prompt in args.prompts:
    txt, weight = parse_prompt(prompt)
    target_embeds.append(
        clip_model.encode_text(clip.tokenize(txt).to(device)).float()
    )
    weights.append(weight)

for prompt in args.images:
    path, weight = parse_prompt(prompt)
    img = Image.open(utils.fetch(path)).convert("RGB")
    clip_size = clip_model.visual.input_resolution
    img = resize_and_center_crop(img, (clip_size, clip_size))
    batch = TF.to_tensor(img)[None].to(device)
    embed = F.normalize(
        clip_model.encode_image(normalize(batch)).float(), dim=-1
    )
    target_embeds.append(embed)
    weights.append(weight)

weights = torch.tensor([1 - sum(weights), *weights], device=device)

torch.manual_seed(args.seed)


def cfg_model_fn(x, timestep_embed, selfcond):
    vs = model(x, timestep_embed, selfcond)
    return vs


def expand_to_planes(input, shape):
    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])


x = torch.randn([args.n, 3, side_y, side_x], device=device)
t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
steps = utils.get_spliced_ddpm_cosine_schedule(t)
min_batch_size = min(args.n, args.batch_size)
x_in = x[0:min_batch_size, :, :, :]
ts = x_in.new_ones([x_in.shape[0]])
t_in = t[0] * ts

n_conds = len(target_embeds)
x_in = x.repeat([n_conds, 1, 1, 1])
t_in = t.repeat([n_conds])
clip_embed_in = torch.cat([*target_embeds]).repeat([args.n, 1])

x_in = torch.randn(2, 3, 256, 256)
t_in = torch.randn(2)
clip_embed_in = torch.randn(2, 512)

clip_embed = (
    F.normalize(clip_embed_in, dim=-1) * clip_embed_in.shape[-1] ** 0.5
)
mapping_timestep_embed = model.mapping_timestep_embed(t_in[:, None])
selfcond = model.mapping(
    torch.cat([clip_embed, mapping_timestep_embed], dim=1)
)
timestep_embed = expand_to_planes(
    model.timestep_embed(t_in[:, None]), x_in.shape
)

# x_in = torch.randn(2, 3, 256, 256)
# selfcond = torch.randn(2, 1024)
# timestep_embed = torch.randn(2, 512)


from torch.fx.experimental.proxy_tensor import make_fx
from torch._decomp import get_decompositions
import torch_mlir

fx_g = make_fx(
    cfg_model_fn,
    decomposition_table=get_decompositions(
        [
            torch.ops.aten.embedding_dense_backward,
            torch.ops.aten.native_layer_norm_backward,
            torch.ops.aten.slice_backward,
            torch.ops.aten.select_backward,
            torch.ops.aten.norm.ScalarOpt_dim,
            torch.ops.aten.native_group_norm,
            torch.ops.aten.upsample_bilinear2d.vec,
            torch.ops.aten.split.Tensor,
            torch.ops.aten.split_with_sizes,
        ]
    ),
)(x_in, timestep_embed, selfcond)

fx_g.graph.set_codegen(torch.fx.graph.CodeGen())
fx_g.recompile()


def strip_overloads(gm):
    """
    Modifies the target of graph nodes in :attr:`gm` to strip overloads.
    Args:
        gm(fx.GraphModule): The input Fx graph module to be modified
    """
    for node in gm.graph.nodes:
        if isinstance(node.target, torch._ops.OpOverload):
            node.target = node.target.overloadpacket
    gm.recompile()


strip_overloads(fx_g)

ts_g = torch.jit.script(fx_g)

module = torch_mlir.compile(
    ts_g,
    [x_in, timestep_embed, selfcond],
    torch_mlir.OutputType.LINALG_ON_TENSORS,
    use_tracing=False,
)

mlir_model = module
func_name = "forward"

shark_module = SharkInference(
    mlir_model, func_name, device=args.runtime_device, mlir_dialect="linalg"
)
shark_module.compile()


def compiled_cfg_model_fn(x, t):
    # Preprocessing previously found in cfg_model_fn
    n = x.shape[0]
    n_conds = len(target_embeds)
    x_in = x.repeat([n_conds, 1, 1, 1])
    t_in = t.repeat([n_conds])
    clip_embed_in = torch.cat([*target_embeds]).repeat([n, 1])

    # Initial setup found in base v-diffusion
    clip_embed = (
        F.normalize(clip_embed_in, dim=-1) * clip_embed_in.shape[-1] ** 0.5
    )
    mapping_timestep_embed = model.mapping_timestep_embed(t_in[:, None])
    selfcond = model.mapping(
        torch.cat([clip_embed, mapping_timestep_embed], dim=1)
    )
    timestep_embed = expand_to_planes(
        model.timestep_embed(t_in[:, None]), x_in.shape
    )

    x_ny = x_in.detach().numpy()
    timestep_embed_ny = timestep_embed.detach().numpy()
    selfcond_ny = selfcond.detach().numpy()
    inputs = (x_ny, timestep_embed_ny, selfcond_ny)
    result = shark_module.forward(inputs)

    vs = torch.from_numpy(result).view([n_conds, n, *x.shape[1:]])
    v = vs.mul(weights[:, None, None, None, None]).sum(0)
    return v


from typing import Dict


def save_intermediate_images(args: Dict):
    x = args["x"]
    num_iter = args["i"]
    for j, out in enumerate(x):
        utils.to_pil_image(out).save(f"out_iter_" + str(num_iter) + ".png")
    return


def run(x, steps):
    if args.method == "ddpm":
        return sampling.sample(compiled_cfg_model_fn, x, steps, 1.0, {})
    if args.method == "ddim":
        return sampling.sample(compiled_cfg_model_fn, x, steps, args.eta, {})
    if args.method == "prk":
        return sampling.prk_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms":
        return sampling.plms_sample(
            compiled_cfg_model_fn,
            x,
            steps,
            {},
            callback=save_intermediate_images,
        )
    if args.method == "pie":
        return sampling.pie_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "plms2":
        return sampling.plms2_sample(compiled_cfg_model_fn, x, steps, {})
    if args.method == "iplms":
        return sampling.iplms_sample(compiled_cfg_model_fn, x, steps, {})
    assert False


def run_all(x, t, steps, n, batch_size):
    x = torch.randn([n, 3, side_y, side_x], device=device)
    t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]
    steps = utils.get_spliced_ddpm_cosine_schedule(t)
    if args.init:
        steps = steps[steps < args.starting_timestep]
        alpha, sigma = utils.t_to_alpha_sigma(steps[0])
        x = init * alpha + x * sigma
    for i in trange(0, n, batch_size):
        cur_batch_size = min(n - i, batch_size)
        outs = run(x[i : i + cur_batch_size], steps)
        for j, out in enumerate(outs):
            utils.to_pil_image(out).save(f"out_{i + j:05}.png")


run_all(x, t, steps, args.n, args.batch_size)

```

`tank/examples/v_diffusion_pytorch/setup_v_diffusion_pytorch.sh`:

```sh
#!/bin/bash

TD="$(cd $(dirname $0) && pwd)"
if [ -z "$PYTHON" ]; then
  PYTHON="$(which python3)"
fi

function die() {
  echo "Error executing command: $*"
  exit 1
}

PYTHON_VERSION_X_Y=`${PYTHON} -c 'import sys; version=sys.version_info[:2]; print("{0}.{1}".format(*version))'`

echo "Python: $PYTHON"
echo "Python version: $PYTHON_VERSION_X_Y"

git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch.git
pip install ftfy regex tqdm
pip uninstall -y torch torchvision
pip install -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre torch torchvision

mkdir checkpoints
wget https://the-eye.eu/public/AI/models/v-diffusion/cc12m_1_cfg.pth -P checkpoints/

cp -r checkpoints/ v-diffusion-pytorch/

```

`tank/generate_sharktank.py`:

```py
# Lint as: python3
"""SHARK Tank"""
# python generate_sharktank.py, you have to give a csv tile with [model_name, model_download_url]
# will generate local shark tank folder like this:
#   /SHARK
#     /gen_shark_tank
#       /albert_lite_base
#       /...model_name...
#

import os
import csv
import argparse
from shark.shark_importer import SharkImporter
import subprocess as sp
import hashlib
import numpy as np
from pathlib import Path


def create_hash(file_name):
    with open(file_name, "rb") as f:
        file_hash = hashlib.blake2b(digest_size=64)
        while chunk := f.read(2**10):
            file_hash.update(chunk)

    return file_hash.hexdigest()


def save_torch_model(torch_model_list, local_tank_cache, import_args):
    from tank.model_utils import (
        get_hf_model,
        get_hf_seq2seq_model,
        get_hf_causallm_model,
        get_vision_model,
        get_hf_img_cls_model,
        get_fp16_model,
    )
    from shark.shark_importer import import_with_fx, save_mlir

    with open(torch_model_list) as csvfile:
        torch_reader = csv.reader(csvfile, delimiter=",")
        fields = next(torch_reader)
        for row in torch_reader:
            torch_model_name = row[0]
            tracing_required = row[1]
            model_type = row[2]
            is_dynamic = row[3]
            mlir_type = row[4]
            is_decompose = row[5]

            tracing_required = False if tracing_required == "False" else True
            is_dynamic = False
            print("generating artifacts for: " + torch_model_name)
            model = None
            input = None
            if model_type == "vision":
                model, input, _ = get_vision_model(
                    torch_model_name, import_args
                )
            elif model_type == "hf":
                model, input, _ = get_hf_model(torch_model_name, import_args)
            elif model_type == "hf_seq2seq":
                model, input, _ = get_hf_seq2seq_model(
                    torch_model_name, import_args
                )
            elif model_type == "hf_causallm":
                model, input, _ = get_hf_causallm_model(
                    torch_model_name, import_args
                )
            elif model_type == "hf_img_cls":
                model, input, _ = get_hf_img_cls_model(
                    torch_model_name, import_args
                )
            torch_model_name = torch_model_name.replace("/", "_")
            if import_args["batch_size"] > 1:
                print(
                    f"Batch size for this model set to {import_args['batch_size']}"
                )
                torch_model_dir = os.path.join(
                    local_tank_cache,
                    str(torch_model_name)
                    + "_torch"
                    + f"_BS{str(import_args['batch_size'])}",
                )
            else:
                torch_model_dir = os.path.join(
                    local_tank_cache, str(torch_model_name) + "_torch"
                )
            os.makedirs(torch_model_dir, exist_ok=True)

            if is_decompose:
                # Add decomposition to some torch ops
                # TODO add op whitelist/blacklist
                import_with_fx(
                    model,
                    (input,),
                    is_f16=False,
                    f16_input_mask=None,
                    debug=True,
                    training=False,
                    return_str=False,
                    save_dir=torch_model_dir,
                    model_name=torch_model_name,
                    mlir_type=mlir_type,
                    is_dynamic=False,
                    tracing_required=True,
                )
            else:
                mlir_importer = SharkImporter(
                    model,
                    (input,),
                    frontend="torch",
                )
                mlir_importer.import_debug(
                    is_dynamic=False,
                    tracing_required=True,
                    dir=torch_model_dir,
                    model_name=torch_model_name,
                    mlir_type=mlir_type,
                )
                # Generate torch dynamic models.
                if is_dynamic:
                    mlir_importer.import_debug(
                        is_dynamic=True,
                        tracing_required=True,
                        dir=torch_model_dir,
                        model_name=torch_model_name + "_dynamic",
                        mlir_type=mlir_type,
                    )


def check_requirements(frontend):
    import importlib

    has_pkgs = False
    if frontend == "torch":
        tv_spec = importlib.util.find_spec("torchvision")
        has_pkgs = tv_spec is not None

    return has_pkgs


class NoImportException(Exception):
    "Raised when requirements are not met for OTF model artifact generation."
    pass


def gen_shark_files(modelname, frontend, tank_dir, importer_args):
    # If a model's artifacts are requested by shark_downloader but they don't exist in the cloud, we call this function to generate the artifacts on-the-fly.
    # TODO: Add TFlite support.
    import tempfile

    import_args = importer_args
    if check_requirements(frontend):
        torch_model_csv = os.path.join(
            os.path.dirname(__file__), "torch_model_list.csv"
        )
        custom_model_csv = tempfile.NamedTemporaryFile(
            dir=os.path.dirname(__file__),
            delete=True,
        )
        if frontend == "torch":
            with open(torch_model_csv, mode="r") as src:
                reader = csv.reader(src)
                for row in reader:
                    if row[0] == modelname:
                        target = row
            with open(custom_model_csv.name, mode="w") as trg:
                writer = csv.writer(trg)
                writer.writerow(["modelname", "src"])
                writer.writerow(target)
            save_torch_model(custom_model_csv.name, tank_dir, import_args)
    else:
        raise NoImportException


# Validates whether the file is present or not.
def is_valid_file(arg):
    if not os.path.exists(arg):
        return None
    else:
        return arg


if __name__ == "__main__":
    # Note, all of these flags are overridden by the import of import_args from stable_args.py, flags are duplicated temporarily to preserve functionality
    # parser = argparse.ArgumentParser()
    # parser.add_argument(
    #    "--torch_model_csv",
    #    type=lambda x: is_valid_file(x),
    #    default="./tank/torch_model_list.csv",
    #    help="""Contains the file with torch_model name and args.
    #         Please see: https://github.com/nod-ai/SHARK/blob/main/tank/torch_model_list.csv""",
    # )
    # parser.add_argument(
    #    "--ci_tank_dir",
    #    type=bool,
    #    default=False,
    # )
    # parser.add_argument("--upload", type=bool, default=False)

    # old_import_args = parser.parse_import_args()
    import_args = {
        "batch_size": 1,
    }
    print(import_args)
    home = str(Path.home())
    WORKDIR = os.path.join(os.path.dirname(__file__), "..", "gen_shark_tank")
    torch_model_csv = os.path.join(
        os.path.dirname(__file__), "torch_model_list.csv"
    )

    save_torch_model(torch_model_csv, WORKDIR, import_args)

```

`tank/model_utils.py`:

```py
from shark.shark_inference import SharkInference

import torch
import numpy as np
import sys

torch.manual_seed(0)

BATCH_SIZE = 1

vision_models = [
    "alexnet",
    "resnet101",
    "resnet18",
    "resnet50",
    "resnet50_fp16",
    "squeezenet1_0",
    "wide_resnet50_2",
    "mobilenet_v3_small",
    "mnasnet1_0",
    "efficientnet_b0",
    "efficientnet_b7",
]
hf_img_cls_models = [
    "google/vit-base-patch16-224",
    "microsoft/resnet-50",
    "facebook/deit-small-distilled-patch16-224",
    "microsoft/beit-base-patch16-224-pt22k-ft22k",
    "nvidia/mit-b0",
]
hf_seq2seq_models = [
    "t5-base",
    "t5-large",
]


def get_torch_model(modelname, import_args):
    if modelname in vision_models:
        return get_vision_model(modelname, import_args)
    elif modelname in hf_img_cls_models:
        return get_hf_img_cls_model(modelname, import_args)
    elif modelname in hf_seq2seq_models:
        return get_hf_seq2seq_model(modelname, import_args)
    elif "fp16" in modelname:
        return get_fp16_model(modelname, import_args)
    else:
        return get_hf_model(modelname, import_args)


##################### Hugging Face Image Classification Models ###################################
from transformers import AutoModelForImageClassification
from transformers import AutoFeatureExtractor
from PIL import Image
import requests


def preprocess_input_image(model_name):
    # from datasets import load_dataset
    # dataset = load_dataset("huggingface/cats-image")
    # image1 = dataset["test"]["image"][0]
    # # print("image1: ", image1) # <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7FA0B86BB6D0>
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7FA0B86BB6D0>
    image = Image.open(requests.get(url, stream=True).raw)
    # feature_extractor = img_models_fe_dict[model_name].from_pretrained(
    #     model_name
    # )
    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)
    inputs = feature_extractor(images=image, return_tensors="pt")
    # inputs = {'pixel_values': tensor([[[[ 0.1137..., -0.2000, -0.4275, -0.5294]]]])}
    #           torch.Size([1, 3, 224, 224]), torch.FloatTensor

    return inputs[str(*inputs)]


class HuggingFaceImageClassification(torch.nn.Module):
    def __init__(self, hf_model_name):
        super().__init__()
        self.model = AutoModelForImageClassification.from_pretrained(
            hf_model_name,  # The pretrained model.
            output_attentions=False,  # Whether the model returns attentions weights.
            return_dict=False,  # https://github.com/huggingface/transformers/issues/9095
            torchscript=True,
        )

    def forward(self, inputs):
        return self.model.forward(inputs)[0]


def get_hf_img_cls_model(name, import_args):
    model = HuggingFaceImageClassification(name)
    # you can use preprocess_input_image to get the test_input or just random value.
    test_input = preprocess_input_image(name)
    # test_input = torch.FloatTensor(1, 3, 224, 224).uniform_(-1, 1)
    # print("test_input.shape: ", test_input.shape)
    # test_input.shape:  torch.Size([1, 3, 224, 224])
    test_input = test_input.repeat(int(import_args["batch_size"]), 1, 1, 1)
    actual_out = model(test_input)
    # print("actual_out.shape： ", actual_out.shape)
    # actual_out.shape：  torch.Size([1, 1000])
    return model, test_input, actual_out


##################### Hugging Face LM Models ###################################


class HuggingFaceLanguage(torch.nn.Module):
    def __init__(self, hf_model_name):
        super().__init__()
        from transformers import AutoModelForSequenceClassification
        import transformers as trf

        transformers_path = trf.__path__[0]
        hf_model_path = f"{transformers_path}/models/{hf_model_name}"
        self.model = AutoModelForSequenceClassification.from_pretrained(
            hf_model_name,  # The pretrained model.
            num_labels=2,  # The number of output labels--2 for binary classification.
            output_attentions=False,  # Whether the model returns attentions weights.
            output_hidden_states=False,  # Whether the model returns all hidden-states.
            torchscript=True,
        )

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


def get_hf_model(name, import_args):
    from transformers import (
        BertTokenizer,
    )

    model = HuggingFaceLanguage(name)
    test_input = torch.randint(2, (int(import_args["batch_size"]), 128))
    actual_out = model(test_input)
    return model, test_input, actual_out


##################### Hugging Face Seq2SeqLM Models ###################################

# We use a maximum sequence length of 512 since this is the default used in the T5 config.
T5_MAX_SEQUENCE_LENGTH = 512


class HFSeq2SeqLanguageModel(torch.nn.Module):
    def __init__(self, model_name):
        super().__init__()
        from transformers import AutoTokenizer, T5Model

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenization_kwargs = {
            "pad_to_multiple_of": T5_MAX_SEQUENCE_LENGTH,
            "padding": True,
            "return_tensors": "pt",
        }
        self.model = T5Model.from_pretrained(model_name, return_dict=True)

    def preprocess_input(self, text):
        return self.tokenizer(text, **self.tokenization_kwargs)

    def forward(self, input_ids, decoder_input_ids):
        return self.model.forward(
            input_ids, decoder_input_ids=decoder_input_ids
        )[0]


def get_hf_seq2seq_model(name, import_args):
    m = HFSeq2SeqLanguageModel(name)
    encoded_input_ids = m.preprocess_input(
        "Studies have been shown that owning a dog is good for you"
    ).input_ids
    decoder_input_ids = m.preprocess_input("Studies show that").input_ids
    decoder_input_ids = m.model._shift_right(decoder_input_ids)

    test_input = (encoded_input_ids, decoder_input_ids)
    actual_out = m.forward(*test_input)
    return m, test_input, actual_out


##################### Hugging Face CausalLM Models ###################################
from transformers import AutoTokenizer, AutoModelForCausalLM


def prepare_sentence_tokens(hf_model: str, sentence: str):
    tokenizer = AutoTokenizer.from_pretrained(hf_model)
    return torch.tensor([tokenizer.encode(sentence)])


class HFCausalLM(torch.nn.Module):
    def __init__(self, model_name: str):
        super().__init__()
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,  # The pretrained model name.
            # The number of output labels--2 for binary classification.
            num_labels=2,
            # Whether the model returns attentions weights.
            output_attentions=False,
            # Whether the model returns all hidden-states.
            output_hidden_states=False,
            torchscript=True,
        )
        self.model.eval()

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


def get_hf_causallm_model(name, import_args):
    m = HFCausalLM(name)
    test_input = prepare_sentence_tokens(
        name, "this project is very interesting"
    )
    actual_out = m.forward(*test_input)
    return m, test_input, actual_out


################################################################################

##################### Torch Vision Models    ###################################


class VisionModule(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.train(False)

    def forward(self, input):
        return self.model.forward(input)


def get_vision_model(torch_model, import_args):
    import torchvision.models as models

    default_image_size = (224, 224)
    modelname = torch_model
    if modelname == "alexnet":
        torch_model = models.alexnet(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "resnet18":
        torch_model = models.resnet18(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "resnet50":
        torch_model = models.resnet50(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "resnet50_fp16":
        torch_model = models.resnet50(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "resnet50_fp16":
        torch_model = models.resnet50(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "resnet101":
        torch_model = models.resnet101(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "squeezenet1_0":
        torch_model = models.squeezenet1_0(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "wide_resnet50_2":
        torch_model = models.wide_resnet50_2(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "mobilenet_v3_small":
        torch_model = models.mobilenet_v3_small(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "mnasnet1_0":
        torch_model = models.mnasnet1_0(weights="DEFAULT")
        input_image_size = default_image_size
    if modelname == "efficientnet_b0":
        torch_model = models.efficientnet_b0(weights="DEFAULT")
        input_image_size = (224, 224)
    if modelname == "efficientnet_b7":
        torch_model = models.efficientnet_b7(weights="DEFAULT")
        input_image_size = (600, 600)

    fp16_model = False
    if "fp16" in modelname:
        fp16_model = True
    model = VisionModule(torch_model)
    test_input = torch.randn(
        int(import_args["batch_size"]), 3, *input_image_size
    )
    actual_out = model(test_input)
    if fp16_model == True:
        test_input_fp16 = test_input.to(
            device=torch.device("cuda"), dtype=torch.half
        )
        model_fp16 = model.half()
        model_fp16.eval()
        model_fp16.to("cuda")
        actual_out_fp16 = model_fp16(test_input_fp16)
        model, test_input, actual_out = (
            model_fp16,
            test_input_fp16,
            actual_out_fp16,
        )
    return model, test_input, actual_out


################################################################################

####################### Other PyTorch HF Models ###############################


class BertHalfPrecisionModel(torch.nn.Module):
    def __init__(self, hf_model_name):
        super().__init__()
        from transformers import AutoModelForMaskedLM

        self.model = AutoModelForMaskedLM.from_pretrained(
            hf_model_name,  # The pretrained model.
            num_labels=2,  # The number of output labels--2 for binary classification.
            output_attentions=False,  # Whether the model returns attentions weights.
            output_hidden_states=False,  # Whether the model returns all hidden-states.
            torchscript=True,
            torch_dtype=torch.float16,
        ).to("cuda")

    def forward(self, tokens):
        return self.model.forward(tokens)[0]


def get_fp16_model(torch_model, import_args):
    from transformers import AutoTokenizer

    modelname = torch_model.replace("_fp16", "")
    model = BertHalfPrecisionModel(modelname)
    tokenizer = AutoTokenizer.from_pretrained(modelname)
    text = "Replace me by any text you like."
    text = [text] * int(import_args["batch_size"])
    test_input_fp16 = tokenizer(
        text,
        truncation=True,
        max_length=128,
        return_tensors="pt",
    ).input_ids.to("cuda")
    # test_input = torch.randint(2, (1, 128))
    # test_input_fp16 = test_input.to(
    #    device=torch.device("cuda")
    # )
    model_fp16 = model.half()
    model_fp16.eval()
    with torch.no_grad():
        actual_out_fp16 = model_fp16(test_input_fp16)
    return model_fp16, test_input_fp16, actual_out_fp16


# Utility function for comparing two tensors (torch).
def compare_tensors(torch_tensor, numpy_tensor, rtol=1e-02, atol=1e-03):
    # torch_to_numpy = torch_tensor.detach().numpy()
    return np.allclose(torch_tensor, numpy_tensor, rtol, atol)

```

`tank/model_utils_tf.py`:

```py
import os

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import tensorflow as tf
import numpy as np

BATCH_SIZE = 1

################################## MHLO/TF models #########################################
# TODO : Generate these lists or fetch model source from tank/tf/tf_model_list.csv
keras_models = [
    "resnet50",
    "efficientnet_b0",
    "efficientnet_b7",
    "efficientnet-v2-s",
]
maskedlm_models = [
    "albert-base-v2",
    "bert-base-uncased",
    "bert-large-uncased",
    "camembert-base",
    "dbmdz/convbert-base-turkish-cased",
    "deberta-base",
    "distilbert-base-uncased",
    "google/electra-small-discriminator",
    "funnel-transformer/small",
    "microsoft/layoutlm-base-uncased",
    "longformer-base-4096",
    "google/mobilebert-uncased",
    "microsoft/mpnet-base",
    "google/rembert",
    "roberta-base",
    "tapas-base",
    "hf-internal-testing/tiny-random-flaubert",
    "xlm-roberta",
]
causallm_models = [
    "gpt2",
]
tfhf_models = [
    "microsoft/MiniLM-L12-H384-uncased",
]
tfhf_seq2seq_models = [
    "t5-base",
    "t5-large",
]
img_models = [
    "google/vit-base-patch16-224",
    "facebook/convnext-tiny-224",
]


def get_tf_model(name, import_args):
    if name in keras_models:
        return get_keras_model(name, import_args)
    elif name in maskedlm_models:
        return get_masked_lm_model(name, import_args)
    elif name in causallm_models:
        return get_causal_lm_model(name, import_args)
    elif name in tfhf_models:
        return get_TFhf_model(name, import_args)
    elif name in img_models:
        return get_causal_image_model(name, import_args)
    elif name in tfhf_seq2seq_models:
        return get_tfhf_seq2seq_model(name, import_args)
    else:
        raise Exception(
            "TF model not found! Please check that the modelname has been input correctly."
        )


##################### Tensorflow Hugging Face Bert Models ###################################
from transformers import (
    AutoModelForSequenceClassification,
    BertTokenizer,
    TFBertModel,
)

BERT_MAX_SEQUENCE_LENGTH = 128

# Create a set of 2-dimensional inputs
tf_bert_input = [
    tf.TensorSpec(
        shape=[BATCH_SIZE, BERT_MAX_SEQUENCE_LENGTH], dtype=tf.int32
    ),
    tf.TensorSpec(
        shape=[BATCH_SIZE, BERT_MAX_SEQUENCE_LENGTH], dtype=tf.int32
    ),
    tf.TensorSpec(
        shape=[BATCH_SIZE, BERT_MAX_SEQUENCE_LENGTH], dtype=tf.int32
    ),
]


class TFHuggingFaceLanguage(tf.Module):
    def __init__(self, hf_model_name):
        super(TFHuggingFaceLanguage, self).__init__()
        # Create a BERT trainer with the created network.
        self.m = TFBertModel.from_pretrained(hf_model_name, from_pt=True)

        # Invoke the trainer model on the inputs. This causes the layer to be built.
        self.m.predict = lambda x, y, z: self.m.call(
            input_ids=x, attention_mask=y, token_type_ids=z, training=False
        )

    @tf.function(input_signature=tf_bert_input, jit_compile=True)
    def forward(self, input_ids, attention_mask, token_type_ids):
        return self.m.predict(input_ids, attention_mask, token_type_ids)


def get_TFhf_model(name, import_args):
    model = TFHuggingFaceLanguage(name)
    tokenizer = BertTokenizer.from_pretrained(
        "microsoft/MiniLM-L12-H384-uncased"
    )
    text = "Replace me by any text you'd like."
    text = [text] * BATCH_SIZE
    encoded_input = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=BERT_MAX_SEQUENCE_LENGTH,
    )
    test_input = [
        tf.reshape(
            tf.convert_to_tensor(encoded_input["input_ids"], dtype=tf.int32),
            [BATCH_SIZE, BERT_MAX_SEQUENCE_LENGTH],
        ),
        tf.reshape(
            tf.convert_to_tensor(
                encoded_input["attention_mask"], dtype=tf.int32
            ),
            [BATCH_SIZE, BERT_MAX_SEQUENCE_LENGTH],
        ),
        tf.reshape(
            tf.convert_to_tensor(
                encoded_input["token_type_ids"], dtype=tf.int32
            ),
            [BATCH_SIZE, BERT_MAX_SEQUENCE_LENGTH],
        ),
    ]
    actual_out = model.forward(*test_input)
    return model, test_input, actual_out


# Utility function for comparing two tensors (tensorflow).
def compare_tensors_tf(tf_tensor, numpy_tensor):
    # setting the absolute and relative tolerance
    rtol = 1e-02
    atol = 1e-03
    tf_to_numpy = tf_tensor.numpy()
    return np.allclose(tf_to_numpy, numpy_tensor, rtol, atol)


# Tokenizer for language models
def preprocess_input(
    model_name, max_length, text="This is just used to compile the model"
):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    text = [text] * BATCH_SIZE
    inputs = tokenizer(
        text,
        return_tensors="tf",
        padding="max_length",
        truncation=True,
        max_length=max_length,
    )
    return inputs


##################### Tensorflow Hugging Face Masked LM Models ###################################
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

MASKED_LM_MAX_SEQUENCE_LENGTH = 128

# Create a set of input signature.
input_signature_maskedlm = [
    tf.TensorSpec(
        shape=[BATCH_SIZE, MASKED_LM_MAX_SEQUENCE_LENGTH], dtype=tf.int32
    ),
    tf.TensorSpec(
        shape=[BATCH_SIZE, MASKED_LM_MAX_SEQUENCE_LENGTH], dtype=tf.int32
    ),
]


# For supported models please see here:
# https://huggingface.co/docs/transformers/model_doc/auto#transformers.TFAutoModelForMaskedLM
class MaskedLM(tf.Module):
    def __init__(self, model_name):
        super(MaskedLM, self).__init__()
        self.m = TFAutoModelForMaskedLM.from_pretrained(
            model_name, output_attentions=False, num_labels=2
        )
        self.m.predict = lambda x, y: self.m(input_ids=x, attention_mask=y)[0]

    @tf.function(input_signature=input_signature_maskedlm, jit_compile=True)
    def forward(self, input_ids, attention_mask):
        return self.m.predict(input_ids, attention_mask)


def get_masked_lm_model(
    hf_name, import_args, text="Hello, this is the default text."
):
    model = MaskedLM(hf_name)
    encoded_input = preprocess_input(
        hf_name, MASKED_LM_MAX_SEQUENCE_LENGTH, text
    )
    test_input = (encoded_input["input_ids"], encoded_input["attention_mask"])
    actual_out = model.forward(*test_input)
    return model, test_input, actual_out


##################### Tensorflow Hugging Face Causal LM Models ###################################

from transformers import AutoConfig, TFAutoModelForCausalLM, TFGPT2Model

CAUSAL_LM_MAX_SEQUENCE_LENGTH = 1024

input_signature_causallm = [
    tf.TensorSpec(
        shape=[BATCH_SIZE, CAUSAL_LM_MAX_SEQUENCE_LENGTH], dtype=tf.int32
    ),
    tf.TensorSpec(
        shape=[BATCH_SIZE, CAUSAL_LM_MAX_SEQUENCE_LENGTH], dtype=tf.int32
    ),
]


# For supported models please see here:
# https://huggingface.co/docs/transformers/model_doc/auto#transformers.TFAutoModelForCausalLM
# For more background, see:
# https://huggingface.co/blog/tf-xla-generate
class CausalLM(tf.Module):
    def __init__(self, model_name):
        super(CausalLM, self).__init__()
        # Decoder-only models need left padding.
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, padding_side="left", pad_token="</s>"
        )
        self.tokenization_kwargs = {
            "pad_to_multiple_of": CAUSAL_LM_MAX_SEQUENCE_LENGTH,
            "padding": True,
            "return_tensors": "tf",
        }
        self.model = TFGPT2Model.from_pretrained(model_name, return_dict=True)
        self.model.predict = lambda x, y: self.model(
            input_ids=x, attention_mask=y
        )[0]

    def preprocess_input(self, text):
        return self.tokenizer(text, **self.tokenization_kwargs)

    @tf.function(input_signature=input_signature_causallm, jit_compile=True)
    def forward(self, input_ids, attention_mask):
        return self.model.predict(input_ids, attention_mask)


def get_causal_lm_model(
    hf_name, import_args, text="Hello, this is the default text."
):
    model = CausalLM(hf_name)
    batched_text = [text] * BATCH_SIZE
    encoded_input = model.preprocess_input(batched_text)
    test_input = (encoded_input["input_ids"], encoded_input["attention_mask"])
    actual_out = model.forward(*test_input)
    return model, test_input, actual_out


##################### TensorflowHugging Face Seq2SeqLM Models ###################################

# We use a maximum sequence length of 512 since this is the default used in the T5 config.
T5_MAX_SEQUENCE_LENGTH = 512

input_signature_t5 = [
    tf.TensorSpec(
        shape=[BATCH_SIZE, T5_MAX_SEQUENCE_LENGTH],
        dtype=tf.int32,
        name="input_ids",
    ),
    tf.TensorSpec(
        shape=[BATCH_SIZE, T5_MAX_SEQUENCE_LENGTH],
        dtype=tf.int32,
        name="attention_mask",
    ),
]


class TFHFSeq2SeqLanguageModel(tf.Module):
    def __init__(self, model_name):
        super(TFHFSeq2SeqLanguageModel, self).__init__()
        from transformers import (
            AutoTokenizer,
            AutoConfig,
            TFAutoModelForSeq2SeqLM,
            TFT5Model,
        )

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenization_kwargs = {
            "pad_to_multiple_of": T5_MAX_SEQUENCE_LENGTH,
            "padding": True,
            "return_tensors": "tf",
        }
        self.model = TFT5Model.from_pretrained(model_name, return_dict=True)
        self.model.predict = lambda x, y: self.model(x, decoder_input_ids=y)[0]

    def preprocess_input(self, text):
        return self.tokenizer(text, **self.tokenization_kwargs)

    @tf.function(input_signature=input_signature_t5, jit_compile=True)
    def forward(self, input_ids, decoder_input_ids):
        return self.model.predict(input_ids, decoder_input_ids)


def get_tfhf_seq2seq_model(name, import_args):
    m = TFHFSeq2SeqLanguageModel(name)
    text = "Studies have been shown that owning a dog is good for you"
    batched_text = [text] * BATCH_SIZE
    encoded_input_ids = m.preprocess_input(batched_text).input_ids

    text = "Studies show that"
    batched_text = [text] * BATCH_SIZE
    decoder_input_ids = m.preprocess_input(batched_text).input_ids
    decoder_input_ids = m.model._shift_right(decoder_input_ids)

    test_input = (encoded_input_ids, decoder_input_ids)
    actual_out = m.forward(*test_input)
    return m, test_input, actual_out


##################### TensorFlow Keras Resnet Models #########################################################
# Static shape, including batch size (1).
# Can be dynamic once dynamic shape support is ready.
RESNET_INPUT_SHAPE = [BATCH_SIZE, 224, 224, 3]
EFFICIENTNET_V2_S_INPUT_SHAPE = [BATCH_SIZE, 384, 384, 3]
EFFICIENTNET_B0_INPUT_SHAPE = [BATCH_SIZE, 224, 224, 3]
EFFICIENTNET_B7_INPUT_SHAPE = [BATCH_SIZE, 600, 600, 3]


class ResNetModule(tf.Module):
    def __init__(self):
        super(ResNetModule, self).__init__()
        self.m = tf.keras.applications.resnet50.ResNet50(
            weights="imagenet",
            include_top=True,
            input_shape=tuple(RESNET_INPUT_SHAPE[1:]),
        )
        self.m.predict = lambda x: self.m.call(x, training=False)

    @tf.function(
        input_signature=[tf.TensorSpec(RESNET_INPUT_SHAPE, tf.float32)],
        jit_compile=True,
    )
    def forward(self, inputs):
        return self.m.predict(inputs)

    def input_shape(self):
        return RESNET_INPUT_SHAPE

    def preprocess_input(self, image):
        return tf.keras.applications.resnet50.preprocess_input(image)


class EfficientNetB0Module(tf.Module):
    def __init__(self):
        super(EfficientNetB0Module, self).__init__()
        self.m = tf.keras.applications.efficientnet.EfficientNetB0(
            weights="imagenet",
            include_top=True,
            input_shape=tuple(EFFICIENTNET_B0_INPUT_SHAPE[1:]),
        )
        self.m.predict = lambda x: self.m.call(x, training=False)

    @tf.function(
        input_signature=[
            tf.TensorSpec(EFFICIENTNET_B0_INPUT_SHAPE, tf.float32)
        ],
        jit_compile=True,
    )
    def forward(self, inputs):
        return self.m.predict(inputs)

    def input_shape(self):
        return EFFICIENTNET_B0_INPUT_SHAPE

    def preprocess_input(self, image):
        return tf.keras.applications.efficientnet.preprocess_input(image)


class EfficientNetB7Module(tf.Module):
    def __init__(self):
        super(EfficientNetB7Module, self).__init__()
        self.m = tf.keras.applications.efficientnet.EfficientNetB7(
            weights="imagenet",
            include_top=True,
            input_shape=tuple(EFFICIENTNET_B7_INPUT_SHAPE[1:]),
        )
        self.m.predict = lambda x: self.m.call(x, training=False)

    @tf.function(
        input_signature=[
            tf.TensorSpec(EFFICIENTNET_B7_INPUT_SHAPE, tf.float32)
        ],
        jit_compile=True,
    )
    def forward(self, inputs):
        return self.m.predict(inputs)

    def input_shape(self):
        return EFFICIENTNET_B7_INPUT_SHAPE

    def preprocess_input(self, image):
        return tf.keras.applications.efficientnet.preprocess_input(image)


class EfficientNetV2SModule(tf.Module):
    def __init__(self):
        super(EfficientNetV2SModule, self).__init__()
        self.m = tf.keras.applications.efficientnet_v2.EfficientNetV2S(
            weights="imagenet",
            include_top=True,
            input_shape=tuple(EFFICIENTNET_V2_S_INPUT_SHAPE[1:]),
        )
        self.m.predict = lambda x: self.m.call(x, training=False)

    @tf.function(
        input_signature=[
            tf.TensorSpec(EFFICIENTNET_V2_S_INPUT_SHAPE, tf.float32)
        ],
        jit_compile=True,
    )
    def forward(self, inputs):
        return self.m.predict(inputs)

    def input_shape(self):
        return EFFICIENTNET_V2_S_INPUT_SHAPE

    def preprocess_input(self, image):
        return tf.keras.applications.efficientnet_v2.preprocess_input(image)


def load_image(path_to_image, width, height, channels):
    image = tf.io.read_file(path_to_image)
    image = tf.image.decode_image(image, channels=channels)
    image = tf.image.resize(image, (width, height))
    image = image[tf.newaxis, :]
    image = tf.tile(image, [BATCH_SIZE, 1, 1, 1])
    return image


def get_keras_model(modelname, import_args):
    if modelname == "efficientnet-v2-s":
        model = EfficientNetV2SModule()
    elif modelname == "efficientnet_b0":
        model = EfficientNetB0Module()
    elif modelname == "efficientnet_b7":
        model = EfficientNetB7Module()
    else:
        model = ResNetModule()

    content_path = tf.keras.utils.get_file(
        "YellowLabradorLooking_new.jpg",
        "https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg",
    )
    input_shape = model.input_shape()
    content_image = load_image(
        content_path, input_shape[1], input_shape[2], input_shape[3]
    )
    input_tensor = model.preprocess_input(content_image)
    input_data = tf.expand_dims(input_tensor, 0)
    actual_out = model.forward(*input_data)
    return model, input_data, actual_out


##################### Tensorflow Hugging Face  Image Classification Models ###################################
from transformers import TFAutoModelForImageClassification
from transformers import ConvNextFeatureExtractor, ViTFeatureExtractor
from transformers import BeitFeatureExtractor, AutoFeatureExtractor
from PIL import Image
import requests

# Create a set of input signature.
input_signature_img_cls = [
    tf.TensorSpec(shape=[BATCH_SIZE, 3, 224, 224], dtype=tf.float32),
]


class AutoModelImageClassfication(tf.Module):
    def __init__(self, model_name):
        super(AutoModelImageClassfication, self).__init__()
        self.m = TFAutoModelForImageClassification.from_pretrained(
            model_name, output_attentions=False
        )
        self.m.predict = lambda x: self.m(x)

    @tf.function(input_signature=input_signature_img_cls, jit_compile=True)
    def forward(self, inputs):
        return self.m.predict(inputs)


fail_models = [
    "facebook/data2vec-vision-base-ft1k",
    "microsoft/swin-tiny-patch4-window7-224",
]

supported_models = [
    "facebook/convnext-tiny-224",
    "google/vit-base-patch16-224",
]

img_models_fe_dict = {
    "facebook/convnext-tiny-224": ConvNextFeatureExtractor,
    "facebook/data2vec-vision-base-ft1k": BeitFeatureExtractor,
    "microsoft/swin-tiny-patch4-window7-224": AutoFeatureExtractor,
    "google/vit-base-patch16-224": ViTFeatureExtractor,
}


def preprocess_input_image(model_name):
    # from datasets import load_dataset
    # dataset = load_dataset("huggingface/cats-image")
    # image1 = dataset["test"]["image"][0]
    # # print("image1: ", image1) # <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7FA0B86BB6D0>
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7FA0B86BB6D0>
    image = Image.open(requests.get(url, stream=True).raw)
    feature_extractor = img_models_fe_dict[model_name].from_pretrained(
        model_name
    )
    # inputs: {'pixel_values': <tf.Tensor: shape=(1, 3, 224, 224), dtype=float32, numpy=array([[[[]]]], dtype=float32)>}
    inputs = feature_extractor(images=image, return_tensors="tf")
    inputs["pixel_values"] = tf.tile(
        inputs["pixel_values"], [BATCH_SIZE, 1, 1, 1]
    )

    return [inputs[str(*inputs)]]


def get_causal_image_model(hf_name, import_args):
    model = AutoModelImageClassfication(hf_name)
    test_input = preprocess_input_image(hf_name)
    # TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 1000), dtype=float32, numpy=
    # array([[]], dtype=float32)>, hidden_states=None, attentions=None)
    actual_out = model.forward(*test_input)
    return model, test_input, actual_out

```

`tank/test_models.py`:

```py
from shark.iree_utils._common import (
    check_device_drivers,
    device_driver_info,
    get_supported_device_list,
)
from shark.iree_utils.vulkan_utils import get_vulkan_triple_flag
from shark.parser import shark_args
from parameterized import parameterized
import iree.compiler as ireec
import pytest
import unittest
import numpy as np
import csv
import tempfile
import os
import sys
import shutil


def load_csv_and_convert(filename, gen=False):
    """
    takes in a csv filename and generates a dict for consumption by get_valid_test_params
    """
    model_configs = []
    with open(filename, "r+") as f:
        reader = csv.reader(f, delimiter=",")
        for row in reader:
            if len(row) < 5:
                print("invalid model: " + row)
                continue
            model_configs.append(
                {
                    "model_name": row[0],
                    "dialect": row[1],
                    "framework": row[2],
                    "rtol": float(row[3]),
                    "atol": float(row[4]),
                    "out_type": row[5],
                    "flags": row[6],
                    "xfail_cpu": row[7],
                    "xfail_cuda": row[8],
                    "xfail_vkm": row[9],
                    "xfail_reason": row[10],
                    "xfail_other": row[11],
                }
            )
    # This is a pytest workaround
    if gen:
        with open(
            os.path.join(os.path.dirname(__file__), "dict_configs.py"), "w+"
        ) as out:
            out.write("ALL = [\n")
            for c in model_configs:
                out.write(str(c) + ",\n")
            out.write("]")
    return model_configs


def get_valid_test_params():
    """
    Generate a list of all combinations of available devices and static/dynamic flag.
    """
    device_list = [
        device
        for device in get_supported_device_list()
        if not check_device_drivers(device)
        and device not in ["cpu-sync", "cpu-task"]
    ]
    dynamic_list = (True, False)
    # TODO: This is soooo ugly, but for some reason creating the dict at runtime
    # results in strange pytest failures.
    load_csv_and_convert(
        os.path.join(os.path.dirname(__file__), "all_models.csv"), True
    )
    from tank.dict_configs import ALL

    config_list = ALL

    param_list = [
        (dynamic, device, config)
        for dynamic in dynamic_list
        for device in device_list
        for config in config_list
    ]

    filtered_param_list = [
        params for params in param_list if is_valid_case(params)
    ]

    return filtered_param_list


def is_valid_case(test_params):
    if test_params[0] == True and test_params[2]["framework"] == "tf":
        return False
    if test_params[2]["framework"] == "tf":
        return False
    elif "fp16" in test_params[2]["model_name"] and test_params[1] != "cuda":
        return False
    else:
        return True


def shark_test_name_func(testcase_func, param_num, param):
    """
    Generate function name string which shows dynamic/static and device name.
    this will be ingested by 'parameterized' package to rename the pytest.
    """
    param_names = []
    for x in param.args:
        if x == True:
            param_names.append("dynamic")
        elif x == False:
            param_names.append("static")
        elif "model" in str(x):
            as_list = str(x).split(" ")
            as_list = [
                parameterized.to_safe_name(x).strip("_") for x in as_list
            ]
            param_names.insert(0, as_list[as_list.index("model_name") + 1])
            param_names.insert(1, as_list[as_list.index("framework") + 1])
            # param_names.append(as_list[3])

        else:
            param_names.append(x)
    return "%s_%s" % (
        testcase_func.__name__,
        parameterized.to_safe_name("_".join(str(x) for x in param_names)),
    )


class SharkModuleTester:
    def __init__(self, config):
        """config should be a dict containing minimally:
        dialect: (str) name of input dialect
        framework: (str) one of tf, tflite, pytorch
        model_name: (str) name of the model in the tank ("resnet50")
        rtol/atol: (float) tolerances for golden values
        """
        self.config = config

    def create_and_check_module(self, dynamic, device):
        shark_args.update_tank = self.update_tank
        shark_args.force_update_tank = self.force_update_tank
        shark_args.shark_prefix = self.shark_tank_prefix
        shark_args.local_tank_cache = self.local_tank_cache
        shark_args.dispatch_benchmarks = self.benchmark_dispatches
        shark_args.enable_tf32 = self.tf32

        if self.benchmark_dispatches is not None:
            _m = self.config["model_name"].split("/")
            _m.extend([self.config["framework"], str(dynamic), device])
            _m = "_".join(_m)
            shark_args.dispatch_benchmarks_dir = os.path.join(
                self.dispatch_benchmarks_dir,
                _m,
            )
            if not os.path.exists(self.dispatch_benchmarks_dir):
                os.mkdir(self.dispatch_benchmarks_dir)
            if not os.path.exists(shark_args.dispatch_benchmarks_dir):
                os.mkdir(shark_args.dispatch_benchmarks_dir)
        if "nhcw-nhwc" in self.config["flags"] and not os.path.isfile(
            ".use-iree"
        ):
            shark_args.enable_conv_transform = True
        else:
            shark_args.enable_conv_transform = False
        if "img2col" in self.config["flags"]:
            shark_args.enable_img2col_transform = True
        if "winograd" in self.config["flags"]:
            shark_args.use_winograd = True

        import_config = {
            "batch_size": self.batch_size,
        }

        from shark.shark_downloader import download_model
        from shark.shark_inference import SharkInference
        from tank.generate_sharktank import NoImportException

        dl_gen_attempts = 2
        for i in range(dl_gen_attempts):
            try:
                model, func_name, inputs, golden_out = download_model(
                    self.config["model_name"],
                    frontend=self.config["framework"],
                    import_args=import_config,
                )
            except NoImportException as err:
                pytest.xfail(
                    reason=f"Artifacts for this model/config must be generated locally. Please make sure {self.config['framework']} is installed."
                )
            except AssertionError as err:
                if i < dl_gen_attempts - 1:
                    continue
                else:
                    pytest.xfail(
                        "Generating OTF may require exiting the subprocess for files to be available."
                    )
            break
        is_bench = True if self.benchmark is not None else False
        shark_module = SharkInference(
            model,
            device=device,
            mlir_dialect=self.config["dialect"],
            is_benchmark=is_bench,
        )

        try:
            shark_module.compile()
        except:
            if any([self.ci, self.save_repro, self.save_fails]) == True:
                self.save_reproducers()
            if self.ci == True:
                self.upload_repro()
            raise

        result = shark_module(func_name, inputs)
        golden_out, result = self.postprocess_outputs(golden_out, result)
        if self.tf32 == True:
            print(
                "Validating with relaxed tolerances for TensorFloat32 calculations."
            )
            self.config["atol"] = 1e-01
            self.config["rtol"] = 1e-02
        try:
            np.testing.assert_allclose(
                golden_out,
                result,
                rtol=self.config["rtol"],
                atol=self.config["atol"],
            )
        except AssertionError as msg:
            if any([self.ci, self.save_repro, self.save_fails]) == True:
                self.save_reproducers()
            if self.ci == True:
                self.upload_repro()
            if self.benchmark is not None:
                self.benchmark_module(
                    shark_module, inputs, dynamic, device, mode=self.benchmark
                )
                print(msg)
                pytest.xfail(
                    reason=f"Numerics Mismatch: Use -s flag to print stderr during pytests."
                )
        if self.benchmark is not None:
            self.benchmark_module(
                shark_module, inputs, dynamic, device, mode=self.benchmark
            )

        if self.save_repro == True:
            self.save_reproducers()

    def benchmark_module(
        self, shark_module, inputs, dynamic, device, mode="native"
    ):
        model_config = {
            "batch_size": self.batch_size,
        }

        shark_args.onnx_bench = self.onnx_bench
        shark_module.shark_runner.benchmark_all_csv(
            (inputs),
            self.config["model_name"],
            dynamic,
            device,
            self.config["framework"],
            import_args=model_config,
            mode=mode,
        )

    def save_reproducers(self):
        # Saves contents of IREE TempFileSaver temporary directory to ./{temp_dir}/saved/<test_case>.
        src = self.temp_dir
        trg = os.path.join("reproducers", self.tmp_prefix)
        if not os.path.isdir("reproducers"):
            os.mkdir("reproducers")
        if not os.path.isdir(trg):
            os.mkdir(trg)
        files = os.listdir(src)
        for fname in files:
            shutil.copy2(os.path.join(src, fname), trg)

    def upload_repro(self):
        import subprocess

        repro_path = os.path.join("reproducers", self.tmp_prefix, "*")

        bashCommand = f"gsutil cp -r {repro_path} gs://shark-public/builder/repro_artifacts/{self.ci_sha}/{self.tmp_prefix}/"
        print(
            f"Uploading reproducer {repro_path} to gs://shark-public/builder/repro_artifacts/{self.ci_sha}/{self.tmp_prefix}/"
        )
        process = subprocess.run(bashCommand.split())

    def postprocess_outputs(self, golden_out, result):
        # Prepares result tensors of forward pass and golden values for comparison, when needed.
        if self.config["out_type"] == "tf_vit":
            ir_device_array = result[0][1]
            logits = ir_device_array.astype(ir_device_array.dtype)
            logits = np.squeeze(logits, axis=0)
            expected = golden_out[0]
        elif self.config["out_type"] == "tf_hf":
            logits = result[0][1].to_host()
            expected = golden_out
        elif self.config["out_type"] == "default":
            logits = result
            expected = golden_out

        return expected, logits


class SharkModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.pytestconfig = pytestconfig

    param_list = get_valid_test_params()

    @parameterized.expand(param_list, name_func=shark_test_name_func)
    def test_module(self, dynamic, device, config):
        self.module_tester = SharkModuleTester(config)
        self.module_tester.batch_size = self.pytestconfig.getoption(
            "batchsize"
        )
        self.module_tester.benchmark = self.pytestconfig.getoption("benchmark")
        self.module_tester.save_repro = self.pytestconfig.getoption(
            "save_repro"
        )
        self.module_tester.save_fails = self.pytestconfig.getoption(
            "save_fails"
        )
        self.module_tester.onnx_bench = self.pytestconfig.getoption(
            "onnx_bench"
        )
        self.module_tester.tf32 = self.pytestconfig.getoption("tf32")
        self.module_tester.ci = self.pytestconfig.getoption("ci")
        self.module_tester.ci_sha = self.pytestconfig.getoption("ci_sha")
        self.module_tester.local_tank_cache = self.pytestconfig.getoption(
            "local_tank_cache"
        )
        self.module_tester.update_tank = self.pytestconfig.getoption(
            "update_tank"
        )
        self.module_tester.force_update_tank = self.pytestconfig.getoption(
            "force_update_tank"
        )
        self.module_tester.shark_tank_prefix = self.pytestconfig.getoption(
            "tank_prefix"
        )
        self.module_tester.benchmark_dispatches = self.pytestconfig.getoption(
            "benchmark_dispatches"
        )
        self.module_tester.dispatch_benchmarks_dir = (
            self.pytestconfig.getoption("dispatch_benchmarks_dir")
        )

        if config["xfail_cpu"] == "True" and device in [
            "cpu",
            "cpu-sync",
            "cpu-task",
        ]:
            pytest.xfail(reason=config["xfail_reason"])

        if config["xfail_cuda"] == "True" and device == "cuda":
            pytest.xfail(reason=config["xfail_reason"])

        if config["xfail_vkm"] == "True" and device in ["metal", "vulkan"]:
            pytest.xfail(reason=config["xfail_reason"])

        if (
            self.pytestconfig.getoption("ci") == True
            and os.name == "nt"
            and "enabled_windows" not in config["xfail_other"]
        ):
            pytest.xfail(reason="this model skipped on windows")

        # Special cases that need to be marked.
        if (
            "macos" in config["xfail_other"]
            and device
            in [
                "metal",
                "vulkan",
            ]
            and sys.platform == "darwin"
        ):
            pytest.skip(
                reason="conv-related issue on MacStudio, returns VK_ERROR_DEVICE_LOST."
            )
        if (
            config["model_name"]
            in [
                "facebook/convnext-tiny-224",
                "squeezenet1_0",
            ]
            and device == "rocm"
        ):
            pytest.xfail(
                reason="iree-compile buffer limit issue: https://github.com/nod-ai/SHARK/issues/475"
            )
        if (
            config["model_name"]
            in [
                "funnel-transformer/small",
                "mobilenet_v3_small",
            ]
            and device == "rocm"
        ):
            pytest.xfail(
                reason="Numerics issues: https://github.com/nod-ai/SHARK/issues/476"
            )
        if config["framework"] == "tf" and self.module_tester.batch_size != 1:
            pytest.xfail(
                reason="Configurable batch sizes temp. unavailable for tensorflow models."
            )
        safe_name = (
            f"{config['model_name']}_{config['framework']}_{dynamic}_{device}"
        )
        self.module_tester.tmp_prefix = safe_name.replace("/", "_")

        tempdir = tempfile.TemporaryDirectory(
            prefix=self.module_tester.tmp_prefix, dir="."
        )
        self.module_tester.temp_dir = tempdir.name

        with ireec.tools.TempFileSaver(tempdir.name):
            self.module_tester.create_and_check_module(dynamic, device)

```

`tank/tflite/README.md`:

```md
# Sample compile and execution of TFLite models

This directory contains test scripts to compile/run/compare various TFLite
models from TFHub. It aims for simplicity and hackability.

Follow the instructions at the repository root to install a functioning
python venv. Then you can just run individual python files.

Or, use something like the following to collect all artifacts and traces,
which can be fed to other tools:

```
export IREE_SAVE_TEMPS="/tmp/iree/models/{main}/{id}"
for i in *.py; do export IREE_SAVE_CALLS=/tmp/iree/traces/$i; python $i; done
```

```

`tank/tflite/albert.py`:

```py
# RUN: %PYTHON %s
import numpy as np
from shark.shark_importer import SharkImporter
import pytest

model_path = "https://tfhub.dev/tensorflow/lite-model/albert_lite_base/squadv1/1?lite-format=tflite"


# Inputs modified to be useful albert inputs.
def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)

    args = []
    args.append(
        np.random.randint(
            low=0,
            high=256,
            size=input_details[0]["shape"],
            dtype=input_details[0]["dtype"],
        )
    )
    args.append(
        np.ones(
            shape=input_details[1]["shape"], dtype=input_details[1]["dtype"]
        )
    )
    args.append(
        np.zeros(
            shape=input_details[2]["shape"], dtype=input_details[2]["dtype"]
        )
    )
    return args


if __name__ == "__main__":
    my_shark_importer = SharkImporter(
        model_path=model_path,
        model_type="tflite",
        model_source_hub="tfhub",
        device="cpu",
        dynamic=False,
        jit_trace=True,
    )
    # Case1: Use default inputs
    my_shark_importer.compile()
    shark_results = my_shark_importer.forward()
    # Case2: Use manually set inputs
    input_details, output_details = my_shark_importer.get_model_details()
    inputs = generate_inputs(input_details)  # device_inputs
    my_shark_importer.compile(inputs)
    shark_results = my_shark_importer.forward(inputs)
    # print(shark_results)

```

`tank/tflite/albert_lite_base/albert_lite_base_tflite_sharkimporter.txt`:

```txt
# import numpy as np
# from shark.shark_importer import SharkImporter
# from shark.shark_inference import SharkInference
# import pytest
# import unittest
# from shark.parser import shark_args
# from shark.tflite_utils import TFLitePreprocessor
#
#
# # model_path = "https://tfhub.dev/tensorflow/lite-model/albert_lite_base/squadv1/1?lite-format=tflite"
# # model_path = model_path
#
# # Inputs modified to be useful albert inputs.
# def generate_inputs(input_details):
#     for input in input_details:
#         print(str(input["shape"]), input["dtype"].__name__)
#         # [  1 384] int32
#         # [  1 384] int32
#         # [  1 384] int32
#
#     args = []
#     args.append(
#         np.random.randint(
#             low=0,
#             high=256,
#             size=input_details[0]["shape"],
#             dtype=input_details[0]["dtype"],
#         )
#     )
#     args.append(
#         np.ones(
#             shape=input_details[1]["shape"], dtype=input_details[1]["dtype"]
#         )
#     )
#     args.append(
#         np.zeros(
#             shape=input_details[2]["shape"], dtype=input_details[2]["dtype"]
#         )
#     )
#     return args
#
#
# def compare_results(mlir_results, tflite_results):
#     print("Compare mlir_results VS tflite_results: ")
#     assert len(mlir_results) == len(
#         tflite_results
#     ), "Number of results do not match"
#     rtol = 1e-02
#     atol = 1e-03
#     print(
#         "numpy.allclose: ",
#         np.allclose(mlir_results, tflite_results, rtol, atol),
#     )
#     for i in range(len(mlir_results)):
#         mlir_result = mlir_results[i]
#         tflite_result = tflite_results[i]
#         mlir_result = mlir_result.astype(np.single)
#         tflite_result = tflite_result.astype(np.single)
#         assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
#         max_error = np.max(np.abs(mlir_result - tflite_result))
#         print("Max error (%d): %f", i, max_error)
#
#
# class AlbertTfliteModuleTester:
#     def __init__(
#         self,
#         dynamic=False,
#         device="cpu",
#         save_mlir=False,
#         save_vmfb=False,
#     ):
#         self.dynamic = dynamic
#         self.device = device
#         self.save_mlir = save_mlir
#         self.save_vmfb = save_vmfb
#
#     def create_and_check_module(self):
#         shark_args.save_mlir = self.save_mlir
#         shark_args.save_vmfb = self.save_vmfb
#
#         # Preprocess to get SharkImporter input args
#         tflite_preprocessor = TFLitePreprocessor(model_name="albert_lite_base")
#         raw_model_file_path = tflite_preprocessor.get_raw_model_file()
#         inputs = tflite_preprocessor.get_inputs()
#         tflite_interpreter = tflite_preprocessor.get_interpreter()
#
#         # Use SharkImporter to get SharkInference input args
#         my_shark_importer = SharkImporter(
#             module=tflite_interpreter,
#             inputs=inputs,
#             frontend="tflite",
#             raw_model_file=raw_model_file_path,
#         )
#         mlir_model, func_name = my_shark_importer.import_mlir()
#
#         # Use SharkInference to get inference result
#         shark_module = SharkInference(
#             mlir_module=mlir_model,
#             function_name=func_name,
#             device=self.device,
#             mlir_dialect="tflite",
#         )
#
#         # Case1: Use shark_importer default generate inputs
#         shark_module.compile()
#         mlir_results = shark_module.forward(inputs)
#         ## post process results for compare
#         # input_details, output_details = tflite_preprocessor.get_model_details()
#         # mlir_results = list(mlir_results)
#         # for i in range(len(output_details)):
#         #     dtype = output_details[i]["dtype"]
#         #     mlir_results[i] = mlir_results[i].astype(dtype)
#         tflite_results = tflite_preprocessor.get_golden_output()
#         compare_results(mlir_results, tflite_results)
#         # import pdb
#         # pdb.set_trace()
#
#         # Case2: Use manually set inputs
#         # input_details, output_details = tflite_preprocessor.get_model_details()
#         input_details = [
#             {
#                 "shape": [1, 384],
#                 "dtype": np.int32,
#             },
#             {
#                 "shape": [1, 384],
#                 "dtype": np.int32,
#             },
#             {
#                 "shape": [1, 384],
#                 "dtype": np.int32,
#             },
#         ]
#         inputs = generate_inputs(input_details)  # new inputs
#
#         shark_module = SharkInference(
#             mlir_module=mlir_model,
#             function_name=func_name,
#             device=self.device,
#             mlir_dialect="tflite",
#         )
#         shark_module.compile()
#         mlir_results = shark_module.forward(inputs)
#         ## post process results for compare
#         tflite_results = tflite_preprocessor.get_golden_output()
#         compare_results(mlir_results, tflite_results)
#         # print(mlir_results)
#
#
# class AlbertTfliteModuleTest(unittest.TestCase):
#     @pytest.fixture(autouse=True)
#     def configure(self, pytestconfig):
#         self.save_mlir = pytestconfig.getoption("save_mlir")
#         self.save_vmfb = pytestconfig.getoption("save_vmfb")
#
#     def setUp(self):
#         self.module_tester = AlbertTfliteModuleTester(self)
#         self.module_tester.save_mlir = self.save_mlir
#
#     import sys
#
#     @pytest.mark.xfail(
#         sys.platform == "darwin", reason="known macos tflite install issue"
#     )
#     def test_module_static_cpu(self):
#         self.module_tester.dynamic = False
#         self.module_tester.device = "cpu"
#         self.module_tester.create_and_check_module()


# if __name__ == "__main__":
# module_tester = AlbertTfliteModuleTester()
# module_tester.save_mlir = True
# module_tester.save_vmfb = True
# module_tester.create_and_check_module()

# unittest.main()

```

`tank/tflite/albert_lite_base/albert_lite_base_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/tensorflow/lite-model/albert_lite_base/squadv1/1?lite-format=tflite"
# model_path = model_path


# Inputs modified to be useful albert inputs.
def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)
    # [  1 384] int32
    # [  1 384] int32
    # [  1 384] int32

    args = []
    args.append(
        np.random.randint(
            low=0,
            high=256,
            size=input_details[0]["shape"],
            dtype=input_details[0]["dtype"],
        )
    )
    args.append(
        np.ones(
            shape=input_details[1]["shape"], dtype=input_details[1]["dtype"]
        )
    )
    args.append(
        np.zeros(
            shape=input_details[2]["shape"], dtype=input_details[2]["dtype"]
        )
    )
    return args


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    rtol = 1e-02
    atol = 1e-03
    print(
        "numpy.allclose: ",
        np.allclose(mlir_results, tflite_results, rtol, atol),
    )
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class AlbertTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        (
            mlir_model,
            function_name,
            inputs,
            tflite_results,
        ) = download_tflite_model(model_name="albert_lite_base")

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        # print(shark_results)
        compare_results(mlir_results, tflite_results)


class AlbertTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = AlbertTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    unittest.main()
    # module_tester = AlbertTfliteModuleTester()
    # module_tester.create_and_check_module()

```

`tank/tflite/arbitrary-image-stylization-v1-256/arbitrary-image-stylization-v1-256_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class ArbitraryImageStylizationV1TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        (
            mlir_model,
            function_name,
            inputs,
            tflite_results,
        ) = download_tflite_model(
            model_name="arbitrary-image-stylization-v1-256"
        )
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )
        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        # print(shark_results)
        compare_results(mlir_results, tflite_results)


class ArbitraryImageStylizationV1TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = ArbitraryImageStylizationV1TfliteModuleTester(
            self
        )
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="'tosa.conv2d' op attribute 'quantization_info' failed ",
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = ArbitraryImageStylizationV1TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/asr_conformer_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import test_util

model_path = "https://tfhub.dev/neso613/lite-model/ASR_TFLite/pre_trained_models/English/1?lite-format=tflite"


# Failure is due to dynamic shapes:
# - Some improvements to tfl.strided_slice lowering are next steps
class AsrConformerTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(AsrConformerTest, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/bird_classifier_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util
import urllib.request

from PIL import Image

model_path = "https://tfhub.dev/google/lite-model/aiy/vision/classifier/birds_V1/3?lite-format=tflite"


class BirdClassifierTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(BirdClassifierTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(BirdClassifierTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-3).all()
        )

    def generate_inputs(self, input_details):
        img_path = (
            "https://github.com/google-coral/test_data/raw/master/bird.bmp"
        )
        local_path = "/".join([self.workdir, "bird.bmp"])
        urllib.request.urlretrieve(img_path, local_path)

        shape = input_details[0]["shape"]
        im = numpy.array(Image.open(local_path).resize((shape[1], shape[2])))
        args = [im.reshape(shape)]
        return args

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/birds_V1/birds_V1_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
import urllib.request
from PIL import Image

# model_path = "https://tfhub.dev/google/lite-model/aiy/vision/classifier/birds_V1/3?lite-format=tflite"


def generate_inputs(input_details):
    # input_details shape: [  1 224 224   3]  type: uint8
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    img_path = "https://github.com/google-coral/test_data/raw/master/bird.bmp"
    local_path = "/".join([workdir, "bird.bmp"])
    urllib.request.urlretrieve(img_path, local_path)

    shape = input_details[0]["shape"]
    im = np.array(Image.open(local_path).resize((shape[1], shape[2])))
    args = [im.reshape(shape)]
    return args


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class BirdsV1TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        (
            mlir_model,
            function_name,
            inputs,
            tflite_results,
        ) = download_tflite_model(model_name="birds_V1")
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # device_inputs
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class BirdsV1TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = BirdsV1TfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="'tosa.conv2d' op attribute 'quantization_info' failed ",
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = BirdsV1TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/cartoon_gan_test.py`:

```py
# RUN: %PYTHON %s
# REQUIRES: hugetest

import absl.testing
import test_util

model_path = (
    "https://tfhub.dev/sayakpaul/lite-model/cartoongan/dr/1?lite-format=tflite"
)


class CartoonGanTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(CartoonGanTest, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/cartoongan/cartoongan_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/sayakpaul/lite-model/cartoongan/dr/1?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class CartoonganTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        (
            mlir_model,
            function_name,
            inputs,
            tflite_results,
        ) = download_tflite_model(model_name="cartoongan")
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class CartoonganTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = CartoonganTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = CartoonganTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/coco_data.py`:

```py
import numpy as np
import urllib.request
from PIL import Image


# Returns a sample image in the COCO 2017 dataset in uint8.
def generate_input(workdir, input_details):
    # We use an image of a bear since this is an easy example.
    img_path = "https://storage.googleapis.com/iree-model-artifacts/coco_2017_000000000285.jpg"
    local_path = "/".join([workdir, "coco_2017_000000000285.jpg"])
    urllib.request.urlretrieve(img_path, local_path)

    shape = input_details[0]["shape"]
    im = np.array(Image.open(local_path).resize((shape[1], shape[2])))
    return im.reshape(shape)

```

`tank/tflite/coco_test_data.py`:

```py
import numpy as np
import urllib.request

from PIL import Image


# Returns a sample image in the COCO 2017 dataset in uint8.
def generate_input(workdir, input_details):
    # We use an image of a bear since this is an easy example.
    img_path = "https://storage.googleapis.com/iree-model-artifacts/coco_2017_000000000285.jpg"
    local_path = "/".join([workdir, "coco_2017_000000000285.jpg"])
    urllib.request.urlretrieve(img_path, local_path)

    shape = input_details[0]["shape"]
    im = np.array(Image.open(local_path).resize((shape[1], shape[2])))
    return im.reshape(shape)

```

`tank/tflite/craft_text_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import test_util

model_path = "https://tfhub.dev/tulasiram58827/lite-model/craft-text-detector/dr/1?lite-format=tflite"


# Failure: Resize lowering does not handle inferred dynamic shapes. Furthermore, the entire model
# requires dynamic shape support.
class CraftTextTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(CraftTextTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(CraftTextTest, self).compare_results(
            iree_results, tflite_results, details
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/deeplab_v3_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/metadata/2?lite-format=tflite"


class DeepLabV3Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(DeepLabV3Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(DeepLabV3Test, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-3).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/deeplabv3/deeplabv3_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/google/lite-model/aiy/vision/classifier/birds_V1/3?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class DeepLabV3TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # preprocess to get SharkImporter input args
        (
            mlir_model,
            function_name,
            inputs,
            tflite_results,
        ) = download_tflite_model(model_name="deeplabv3")

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class DeepLabV3TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = DeepLabV3TfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = DeepLabV3TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/densenet/densenet_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/tensorflow/lite-model/densenet/1/metadata/1?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class DensenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        (
            mlir_model,
            function_name,
            inputs,
            tflite_results,
        ) = download_tflite_model(model_name="densenet")

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class DensenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = DensenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = DensenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/densenet_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/tensorflow/lite-model/densenet/1/metadata/1?lite-format=tflite"


class DenseNetTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(DenseNetTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(DenseNetTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-5).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/east_text_detector_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/sayakpaul/lite-model/east-text-detector/dr/1?lite-format=tflite"


class EastTextDetectorTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(EastTextDetectorTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(EastTextDetectorTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-3).all()
        )

        # The second return is extremely noisy as it is not a binary classification. To handle we
        # check normalized correlation with an expectation of "close enough".
        iree_norm = numpy.sqrt(iree_results[1] * iree_results[1])
        tflite_norm = numpy.sqrt(tflite_results[1] * tflite_results[1])

        correlation = numpy.average(
            iree_results[1] * tflite_results[1] / iree_norm / tflite_norm
        )
        self.assertTrue(numpy.isclose(correlation, 1.0, atol=1e-2).all())

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/efficientnet_224_fp32/efficientnet_224_fp32_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# # Source https://tfhub.dev/sayannath/lite-model/image-scene/1
# model_path = "https://storage.googleapis.com/iree-model-artifacts/efficientnet_224_fp32.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)
    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class Efficientnet_224_fp32TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="efficientnet_224_fp32"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class Efficientnet_224_fp32TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = Efficientnet_224_fp32TfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = Efficientnet_224_fp32TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/efficientnet_lite0_fp32_2/efficientnet_lite0_fp32_2_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# # Source https://tfhub.dev/tensorflow/lite-model/efficientnet/lite0/fp32/2
# model_path = "https://storage.googleapis.com/iree-model-artifacts/efficientnet_lite0_fp32_2.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)
    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class Efficientnet_lite0_fp32_2TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="efficientnet_lite0_fp32_2"
        )

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class Efficientnet_lite0_fp32_2TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = Efficientnet_lite0_fp32_2TfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = Efficientnet_lite0_fp32_2TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/efficientnet_lite0_int8_2/efficientnet_lite0_int8_2_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# # Source https://tfhub.dev/tensorflow/lite-model/efficientnet/lite0/int8/2
# model_path = "https://storage.googleapis.com/iree-model-artifacts/efficientnet_lite0_int8_2.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    return [imagenet_data.generate_input(workdir, input_details)]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class Efficientnet_lite0_int8_2TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="efficientnet_lite0_int8_2"
        )

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name="main",
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class Efficientnet_lite0_int8_2TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = Efficientnet_lite0_int8_2TfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = Efficientnet_lite0_int8_2TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/efficientnet_lite0_int8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

# Source https://tfhub.dev/tensorflow/lite-model/efficientnet/lite0/int8/2
model_path = "https://storage.googleapis.com/iree-model-artifacts/efficientnet_lite0_int8_2.tflite"


class EfficientnetLite0Int8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(EfficientnetLite0Int8Test, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(EfficientnetLite0Int8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # Dequantize outputs.
        zero_point = details[0]["quantization_parameters"]["zero_points"][0]
        scale = details[0]["quantization_parameters"]["scales"][0]
        dequantized_iree_results = (iree_results - zero_point) * scale
        dequantized_tflite_results = (tflite_results - zero_point) * scale
        self.assertTrue(
            numpy.isclose(
                dequantized_iree_results, dequantized_tflite_results, atol=5e-3
            ).all()
        )

    def generate_inputs(self, input_details):
        return [imagenet_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/efficientnet_lite0_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

# Source https://tfhub.dev/tensorflow/lite-model/efficientnet/lite0/fp32/2
model_path = "https://storage.googleapis.com/iree-model-artifacts/efficientnet_lite0_fp32_2.tflite"


class EfficientnetLite0Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(EfficientnetLite0Test, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(EfficientnetLite0Test, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=1e-4).all()
        )

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/efficientnet_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import imagenet_test_data
import numpy
import test_util

# Source https://tfhub.dev/sayannath/lite-model/image-scene/1
model_path = "https://storage.googleapis.com/iree-model-artifacts/efficientnet_224_fp32.tflite"


class EfficientnetTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(EfficientnetTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(EfficientnetTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=1e-4).all()
        )

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/gpt2_test.py`:

```py
# RUN: %PYTHON %s
# REQUIRES: horrendoustest

import absl.testing
import numpy
import test_util

model_path = (
    "https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-64.tflite"
)


# This test is a massive download and excluded due to causing timeouts.
class GPT2Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(GPT2Test, self).__init__(model_path, *args, **kwargs)

    # Inputs modified to be useful mobilebert inputs.
    def generate_inputs(self, input_details):
        args = []
        args.append(
            numpy.random.randint(
                low=0,
                high=256,
                size=input_details[0]["shape"],
                dtype=input_details[0]["dtype"],
            )
        )
        return args

    def compare_results(self, iree_results, tflite_results, details):
        super(GPT2Test, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            self.assertTrue(
                numpy.isclose(
                    iree_results[i], tflite_results[i], atol=5e-3
                ).all()
            )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/image_stylization_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3-dynamic-shapes/int8/predict/1?lite-format=tflite"


# Failure is due to avg_pool2d.
class ImageStylizationTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(ImageStylizationTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(ImageStylizationTest, self).compare_results(
            iree_results, tflite_results, details
        )
        iree = iree_results[0].flatten().astype(numpy.single)
        tflite = tflite_results[0].flatten().astype(numpy.single)
        # Error is not tiny but appears close.
        self.assertTrue(
            numpy.isclose(
                numpy.max(numpy.abs(iree - tflite)), 0.0, atol=5e-2
            ).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/imagenet_data.py`:

```py
import numpy as np
import urllib.request

from PIL import Image


# Returns a sample image in the Imagenet dataset in uint8.
def generate_input(workdir, input_details):
    # We use an image of apples since this is an easy example.
    img_path = "https://storage.googleapis.com/iree-model-artifacts/ILSVRC2012_val_00000023.JPEG"
    local_path = "/".join([workdir, "ILSVRC2012_val_00000023.JPEG"])
    urllib.request.urlretrieve(img_path, local_path)

    shape = input_details[0]["shape"]
    im = np.array(Image.open(local_path).resize((shape[1], shape[2])))
    return im.reshape(shape)

```

`tank/tflite/imagenet_test_data.py`:

```py
import numpy as np
import urllib.request

from PIL import Image


# Returns a sample image in the Imagenet dataset in uint8.
def generate_input(workdir, input_details):
    # We use an image of apples since this is an easy example.
    img_path = "https://storage.googleapis.com/iree-model-artifacts/ILSVRC2012_val_00000023.JPEG"
    local_path = "/".join([workdir, "ILSVRC2012_val_00000023.JPEG"])
    urllib.request.urlretrieve(img_path, local_path)

    shape = input_details[0]["shape"]
    im = np.array(Image.open(local_path).resize((shape[1], shape[2])))
    return im.reshape(shape)

```

`tank/tflite/inception_v4_299_fp32/inception_v4_299_fp32_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# # Source https://tfhub.dev/tensorflow/lite-model/inception_v4/1/default/1
# model_path = "https://storage.googleapis.com/iree-model-artifacts/inception_v4_299_fp32.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)
    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class Inception_v4_299_fp32TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="inception_v4_299_fp32"
        )

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 299, 299, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class Inception_v4_299_fp32TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = Inception_v4_299_fp32TfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = Inception_v4_299_fp32TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/inception_v4_299_uint8/inception_v4_299_uint8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# Source https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1
# model_path = "https://storage.googleapis.com/iree-model-artifacts/inception_v4_299_uint8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)
    return [imagenet_data.generate_input(workdir, input_details)]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class Inception_v4_299_uint8TfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="inception_v4_299_uint8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 299, 299, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class Inception_v4_299_uint8TfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = Inception_v4_299_uint8TfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = Inception_v4_299_uint8TfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/inception_v4_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

# Source https://tfhub.dev/tensorflow/lite-model/inception_v4/1/default/1
model_path = "https://storage.googleapis.com/iree-model-artifacts/inception_v4_299_fp32.tflite"


class InceptionV4Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(InceptionV4Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(InceptionV4Test, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=1e-4).all()
        )

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/inception_v4_uint8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

# Source https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1
model_path = "https://storage.googleapis.com/iree-model-artifacts/inception_v4_299_uint8.tflite"


class InceptionV4Uint8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(InceptionV4Uint8Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(InceptionV4Uint8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # Dequantize outputs.
        zero_point = details[0]["quantization_parameters"]["zero_points"][0]
        scale = details[0]["quantization_parameters"]["scales"][0]
        dequantized_iree_results = (iree_results - zero_point) * scale
        dequantized_tflite_results = (tflite_results - zero_point) * scale
        self.assertTrue(
            numpy.isclose(
                dequantized_iree_results, dequantized_tflite_results, atol=5e-3
            ).all()
        )

    def generate_inputs(self, input_details):
        return [imagenet_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/keras_ocr_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import test_util

model_path = "https://tfhub.dev/tulasiram58827/lite-model/keras-ocr/dr/2?lite-format=tflite"


class KerasOCRTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(KerasOCRTest, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/lightning_fp16_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite"


# Currently failing further in the linalg stack:
#   Bug related to linalg fusion. Collapsing dimension despite linalg index.
class LightningFp16Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(LightningFp16Test, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/lightning_i8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util
import urllib.request

from PIL import Image

model_path = "https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite"


# Currently failing further in the linalg stack:
#   Invalid cast from ui8 to f32 TODO: make tfl.cast insert a rescale for ui8
class LightningI8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(LightningI8Test, self).__init__(model_path, *args, **kwargs)

    # Optional utility for visualizing results on the input image. This verifies
    # the model works-ish as the dots are in roughly the same area. Still need to
    # debug the source of numerical differences.
    def plot_results(self, iree_results, tflite_results, details):
        from matplotlib import pyplot as plt

        local_path = "/".join([self.workdir, "person.jpg"])
        im = numpy.array(Image.open(local_path))

        width = im.shape[0]
        height = im.shape[1]

        tflite_result = tflite_results[0]
        iree_result = iree_results[0]

        tflite_x = tflite_result[0, 0, :, 0] * width
        tflite_y = tflite_result[0, 0, :, 1] * height

        iree_x = iree_result[0, 0, :, 0] * width
        iree_y = iree_result[0, 0, :, 1] * height

        plt.imshow(im)
        plt.scatter(tflite_y, tflite_x, label="tflite")
        plt.scatter(iree_y, iree_x, label="iree")
        plt.legend()
        plt.show()

    def compare_results(self, iree_results, tflite_results, details):
        super(LightningI8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # This value is a discretized location of the persons joints. If we are
        # *close* to the expected position we can consider this good enough.
        self.assertTrue(
            numpy.isclose(
                iree_results[0][:, :, :, 0],
                tflite_results[0][:, :, :, 0],
                atol=25e-3,
            ).all()
        )
        self.assertTrue(
            numpy.isclose(
                iree_results[0][:, :, :, 1],
                tflite_results[0][:, :, :, 1],
                atol=25e-3,
            ).all()
        )
        # self.plot_results(iree_results, tflite_results, details)

    def generate_inputs(self, input_details):
        img_path = "https://github.com/tensorflow/examples/raw/master/lite/examples/pose_estimation/raspberry_pi/test_data/image3.jpeg"
        local_path = "/".join([self.workdir, "person.jpg"])
        urllib.request.urlretrieve(img_path, local_path)

        shape = input_details[0]["shape"]
        im = numpy.array(Image.open(local_path).resize((shape[1], shape[2])))
        args = [im.reshape(shape)]
        return args

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/lightning_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/3?lite-format=tflite"


# Currently failing further in the linalg stack:
#   Fusion appears to produce an invalid IR.
class LightningTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(LightningTest, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/lit.cfg.py`:

```py
import os
import sys

import lit.formats
import lit.util

import lit.llvm

# Configuration file for the 'lit' test runner.
lit.llvm.initialize(lit_config, config)

# name: The name of this test suite.
config.name = "TFLITEHUB"

config.test_format = lit.formats.ShTest()

# suffixes: A list of file extensions to treat as test files.
config.suffixes = [".py"]

# test_source_root: The root path where tests are located.
config.test_source_root = os.path.dirname(__file__)

# config.use_default_substitutions()
config.excludes = [
    "coco_test_data.py",
    "imagenet_test_data.py",
    "lit.cfg.py",
    "lit.site.cfg.py",
    "manual_test.py",
    "squad_test_data.py",
    "test_util.py",
]

config.substitutions.extend(
    [
        ("%PYTHON", sys.executable),
    ]
)

config.environment["PYTHONPATH"] = ":".join(sys.path)

project_root = os.path.dirname(os.path.dirname(__file__))

# Enable features based on -D FEATURES=hugetest,vulkan
# syntax.
features_param = lit_config.params.get("FEATURES")
if features_param:
    config.available_features.update(features_param.split(","))

```

`tank/tflite/magenta_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite"


class MagentaTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MagentaTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MagentaTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=2e-1).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/manual_test.py`:

```py
import absl.flags
import absl.testing
import test_util

absl.flags.DEFINE_string("model", None, "model path to execute")


class ManualTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(ManualTest, self).__init__(
            absl.flags.FLAGS.model, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(ManualTest, self).compare_results(
            iree_results, tflite_results, details
        )

    def test_compile_tflite(self):
        if self.model_path is not None:
            self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/midas/midas_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/intel/lite-model/midas/v2_1_small/1/lite/1?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MidasTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="midas"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class MidasTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MidasTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MidasTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/midas_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/intel/lite-model/midas/v2_1_small/1/lite/1?lite-format=tflite"


class MidasTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MidasTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MidasTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-3).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mirnet_test.py`:

```py
# RUN: %PYTHON %s
# REQUIRES: hugetest

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/sayakpaul/lite-model/mirnet-fixed/dr/1?lite-format=tflite"


# Note this one takes forever right now. Great for performance work!
class MirnetTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MirnetTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MirnetTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=5e-3).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mnasnet_1.0_224/mnasnet_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/tensorflow/lite-model/mnasnet_1.0_224/1/metadata/1?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MnasnetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mnasnet_1.0_224"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class MnasnetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MnasnetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MnasnetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mnasnet_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/tensorflow/lite-model/mnasnet_1.0_224/1/metadata/1?lite-format=tflite"


class MnasnetTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MnasnetTest, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilebert-baseline-tf2-float/mobilebert-baseline-tf2-float_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
from tank.tflite import squad_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-baseline-tf2-float.tflite"


def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)

    input_0 = np.asarray(
        squad_data._INPUT_WORD_ID, dtype=input_details[0]["dtype"]
    )
    input_1 = np.asarray(
        squad_data._INPUT_TYPE_ID, dtype=input_details[1]["dtype"]
    )
    input_2 = np.asarray(
        squad_data._INPUT_MASK, dtype=input_details[2]["dtype"]
    )
    return [
        input_0.reshape(input_details[0]["shape"]),
        input_1.reshape(input_details[1]["shape"]),
        input_2.reshape(input_details[2]["shape"]),
    ]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilebertTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilebert-baseline-tf2-float"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilebertTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilebertTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilebertTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilebert-baseline-tf2-quant/mobilebert-baseline-tf2-quant_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
from tank.tflite import squad_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-baseline-tf2-quant.tflite"


def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)

    input_0 = np.asarray(
        squad_data._INPUT_WORD_ID, dtype=input_details[0]["dtype"]
    )
    input_1 = np.asarray(
        squad_data._INPUT_TYPE_ID, dtype=input_details[1]["dtype"]
    )
    input_2 = np.asarray(
        squad_data._INPUT_MASK, dtype=input_details[2]["dtype"]
    )
    return [
        input_0.reshape(input_details[0]["shape"]),
        input_1.reshape(input_details[1]["shape"]),
        input_2.reshape(input_details[2]["shape"]),
    ]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilebertTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilebert-baseline-tf2-quant"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilebertTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilebertTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilebertTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilebert-edgetpu-s-float/mobilebert-edgetpu-s-float_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-edgetpu-s-float.tflite"


def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)

    args = []
    args.append(
        np.random.randint(
            low=0,
            high=256,
            size=input_details[0]["shape"],
            dtype=input_details[0]["dtype"],
        )
    )
    args.append(
        np.ones(
            shape=input_details[1]["shape"], dtype=input_details[1]["dtype"]
        )
    )
    args.append(
        np.zeros(
            shape=input_details[2]["shape"], dtype=input_details[2]["dtype"]
        )
    )
    return args


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilebertTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilebert-edgetpu-s-float"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilebertTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilebertTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilebertTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilebert-edgetpu-s-quant/mobilebert-edgetpu-s-quant_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-edgetpu-s-quant.tflite"


def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)

    args = []
    args.append(
        np.random.randint(
            low=0,
            high=256,
            size=input_details[0]["shape"],
            dtype=input_details[0]["dtype"],
        )
    )
    args.append(
        np.ones(
            shape=input_details[1]["shape"], dtype=input_details[1]["dtype"]
        )
    )
    args.append(
        np.zeros(
            shape=input_details[2]["shape"], dtype=input_details[2]["dtype"]
        )
    )
    return


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilebertTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilebert-edgetpu-s-quant"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class MobilebertTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilebertTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilebertTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilebert/mobilebert_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
from tank.tflite import squad_data

# model_path = "https://tfhub.dev/tensorflow/lite-model/mobilebert/1/metadata/1?lite-format=tflite"


def generate_inputs(input_details):
    for input in input_details:
        print(str(input["shape"]), input["dtype"].__name__)

    input_0 = np.asarray(
        squad_data._INPUT_WORD_ID, dtype=input_details[0]["dtype"]
    )
    input_1 = np.asarray(
        squad_data._INPUT_TYPE_ID, dtype=input_details[1]["dtype"]
    )
    input_2 = np.asarray(
        squad_data._INPUT_MASK, dtype=input_details[2]["dtype"]
    )
    return [
        input_0.reshape(input_details[0]["shape"]),
        input_1.reshape(input_details[1]["shape"]),
        input_2.reshape(input_details[2]["shape"]),
    ]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilebertTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilebert"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
            {
                "shape": [1, 384],
                "dtype": np.int32,
            },
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilebertTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilebertTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilebertTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilebert_edgetpu_s_float_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-edgetpu-s-float.tflite"


class MobileBertTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobileBertTest, self).__init__(model_path, *args, **kwargs)

    # Inputs modified to be useful mobilebert inputs.
    def generate_inputs(self, input_details):
        for input in input_details:
            absl.logging.info(
                "\t%s, %s", str(input["shape"]), input["dtype"].__name__
            )

        args = []
        args.append(
            numpy.random.randint(
                low=0,
                high=256,
                size=input_details[0]["shape"],
                dtype=input_details[0]["dtype"],
            )
        )
        args.append(
            numpy.ones(
                shape=input_details[1]["shape"],
                dtype=input_details[1]["dtype"],
            )
        )
        args.append(
            numpy.zeros(
                shape=input_details[2]["shape"],
                dtype=input_details[2]["dtype"],
            )
        )
        return args

    def compare_results(self, iree_results, tflite_results, details):
        super(MobileBertTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-4).all()
        )
        self.assertTrue(
            numpy.isclose(iree_results[1], tflite_results[1], atol=1e-4).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilebert_edgetpu_s_quant_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-edgetpu-s-quant.tflite"


class MobileBertTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobileBertTest, self).__init__(model_path, *args, **kwargs)

    # Inputs modified to be useful mobilebert inputs.
    def generate_inputs(self, input_details):
        for input in input_details:
            absl.logging.info(
                "\t%s, %s", str(input["shape"]), input["dtype"].__name__
            )

        args = []
        args.append(
            numpy.random.randint(
                low=0,
                high=256,
                size=input_details[0]["shape"],
                dtype=input_details[0]["dtype"],
            )
        )
        args.append(
            numpy.ones(
                shape=input_details[1]["shape"],
                dtype=input_details[1]["dtype"],
            )
        )
        args.append(
            numpy.zeros(
                shape=input_details[2]["shape"],
                dtype=input_details[2]["dtype"],
            )
        )
        return args

    def compare_results(self, iree_results, tflite_results, details):
        super(MobileBertTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1.0).all()
        )
        self.assertTrue(
            numpy.isclose(iree_results[1], tflite_results[1], atol=1.0).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilebert_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy as np
import squad_test_data
import test_util

model_path = "https://tfhub.dev/tensorflow/lite-model/mobilebert/1/metadata/1?lite-format=tflite"


class MobileBertTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobileBertTest, self).__init__(model_path, *args, **kwargs)

    # Inputs modified to be useful mobilebert inputs.
    def generate_inputs(self, input_details):
        for input in input_details:
            absl.logging.info(
                "\t%s, %s", str(input["shape"]), input["dtype"].__name__
            )

        input_0 = np.asarray(
            squad_test_data._INPUT_WORD_ID, dtype=input_details[0]["dtype"]
        )
        input_1 = np.asarray(
            squad_test_data._INPUT_TYPE_ID, dtype=input_details[1]["dtype"]
        )
        input_2 = np.asarray(
            squad_test_data._INPUT_MASK, dtype=input_details[2]["dtype"]
        )
        return [
            input_0.reshape(input_details[0]["shape"]),
            input_1.reshape(input_details[1]["shape"]),
            input_2.reshape(input_details[2]["shape"]),
        ]

    def compare_results(self, iree_results, tflite_results, details):
        super(MobileBertTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            np.isclose(iree_results[0], tflite_results[0], atol=1e-4).all()
        )
        self.assertTrue(
            np.isclose(iree_results[1], tflite_results[1], atol=1e-4).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilebert_tf2_float_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy as np
import squad_test_data
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-baseline-tf2-float.tflite"


class MobileBertTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobileBertTest, self).__init__(model_path, *args, **kwargs)

    # Inputs modified to be useful mobilebert inputs.
    def generate_inputs(self, input_details):
        for input in input_details:
            absl.logging.info(
                "\t%s, %s", str(input["shape"]), input["dtype"].__name__
            )

        input_0 = np.asarray(
            squad_test_data._INPUT_WORD_ID, dtype=input_details[0]["dtype"]
        )
        input_1 = np.asarray(
            squad_test_data._INPUT_TYPE_ID, dtype=input_details[1]["dtype"]
        )
        input_2 = np.asarray(
            squad_test_data._INPUT_MASK, dtype=input_details[2]["dtype"]
        )
        return [
            input_0.reshape(input_details[0]["shape"]),
            input_1.reshape(input_details[1]["shape"]),
            input_2.reshape(input_details[2]["shape"]),
        ]

    def compare_results(self, iree_results, tflite_results, details):
        super(MobileBertTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            np.isclose(iree_results[0], tflite_results[0], atol=1e-4).all()
        )
        self.assertTrue(
            np.isclose(iree_results[1], tflite_results[1], atol=1e-4).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilebert_tf2_quant_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy as np
import squad_test_data
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilebert-baseline-tf2-quant.tflite"


class MobileBertTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobileBertTest, self).__init__(model_path, *args, **kwargs)

    # Inputs modified to be useful mobilebert inputs.
    def generate_inputs(self, input_details):
        for input in input_details:
            absl.logging.info(
                "\t%s, %s", str(input["shape"]), input["dtype"].__name__
            )

        input_0 = np.asarray(
            squad_test_data._INPUT_WORD_ID, dtype=input_details[0]["dtype"]
        )
        input_1 = np.asarray(
            squad_test_data._INPUT_TYPE_ID, dtype=input_details[1]["dtype"]
        )
        input_2 = np.asarray(
            squad_test_data._INPUT_MASK, dtype=input_details[2]["dtype"]
        )
        return [
            input_0.reshape(input_details[0]["shape"]),
            input_1.reshape(input_details[1]["shape"]),
            input_2.reshape(input_details[2]["shape"]),
        ]

    def compare_results(self, iree_results, tflite_results, details):
        super(MobileBertTest, self).compare_results(
            iree_results, tflite_results, details
        )
        # We have confirmed in large scale accuracy tests that differences this large is acceptable.
        self.assertTrue(
            np.isclose(iree_results[0], tflite_results[0], atol=5.0).all()
        )
        self.assertTrue(
            np.isclose(iree_results[1], tflite_results[1], atol=5.0).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_ssd_quant_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util
import urllib.request

from PIL import Image

# Model from https://github.com/google-coral/test_data/raw/master/ssd_mobilenet_v2_face_quant_postprocess.tflite
# but trimmed the final TFLite_PostProcess op.
model_path = "https://storage.googleapis.com/iree-shared-files/models/ssd_mobilenet_v2_face_quant.tflite"


class MobilenetSsdQuantTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetSsdQuantTest, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetSsdQuantTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1.0).all()
        )

    def generate_inputs(self, input_details):
        img_path = "https://github.com/google-coral/test_data/raw/master/grace_hopper.bmp"
        local_path = "/".join([self.workdir, "grace_hopper.bmp"])
        urllib.request.urlretrieve(img_path, local_path)

        shape = input_details[0]["shape"]
        im = numpy.array(Image.open(local_path).resize((shape[1], shape[2])))
        args = [im.reshape(shape)]
        return args

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v1_224_1.0_float/mobilenet_v1_float_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data

# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v1_224_1.0_float.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v1_224_1.0_float"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v1_224_1.0_uint8/mobilenet_v1_uint8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v1_224_1.0_uint8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    return [imagenet_data.generate_input(workdir, input_details)]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v1_224_1.0_uint8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v1_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v1_224_1.0_float.tflite"


class MobilenetV1Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV1Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV1Test, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=1e-4).all()
        )

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v1_uint8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v1_224_1.0_uint8.tflite"


class MobilenetV1Uint8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV1Uint8Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV1Uint8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # Dequantize outputs.
        zero_point = details[0]["quantization_parameters"]["zero_points"][0]
        scale = details[0]["quantization_parameters"]["scales"][0]
        dequantized_iree_results = (iree_results - zero_point) * scale
        dequantized_tflite_results = (tflite_results - zero_point) * scale
        self.assertTrue(
            numpy.isclose(
                dequantized_iree_results, dequantized_tflite_results, atol=5e-3
            ).all()
        )

    def generate_inputs(self, input_details):
        return [imagenet_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v2_1.00_224_int8/mobilenet_v2_int8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data

# model_path = "https://storage.googleapis.com/tf_model_garden/vision/mobilenet/v2_1.0_int8/mobilenet_v2_1.00_224_int8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v2_1.00_224_int8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v2_1.0_224/mobilenet_v2_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v2_1.0_224.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v2_1.0_224"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v2_224_1.0_uint8/mobilenet_v2_uint8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v2_224_1.0_uint8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    return [imagenet_data.generate_input(workdir, input_details)]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v2_224_1.0_uint8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v2_int8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/tf_model_garden/vision/mobilenet/v2_1.0_int8/mobilenet_v2_1.00_224_int8.tflite"


class MobilenetV2Int8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV2Int8Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV2Int8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # Although this a quantized model, inputs and outputs are in float.
        # The difference here is quite high for a dequantized output.
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=0.5).all()
        )

        # Make sure the predicted class is the same.
        iree_predicted_class = numpy.argmax(iree_results[0][0])
        tflite_predicted_class = numpy.argmax(tflite_results[0][0])
        self.assertEqual(iree_predicted_class, tflite_predicted_class)

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v2_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v2_1.0_224.tflite"


class MobilenetV2Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV2Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV2Test, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=1e-4).all()
        )

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v2_uint8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v2_224_1.0_uint8.tflite"


class MobilenetV2Uint8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV2Uint8Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV2Uint8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # Dequantize outputs.
        zero_point = details[0]["quantization_parameters"]["zero_points"][0]
        scale = details[0]["quantization_parameters"]["scales"][0]
        dequantized_iree_results = (iree_results - zero_point) * scale
        dequantized_tflite_results = (tflite_results - zero_point) * scale
        self.assertTrue(
            numpy.isclose(
                dequantized_iree_results, dequantized_tflite_results, atol=5e-3
            ).all()
        )

    def generate_inputs(self, input_details):
        return [imagenet_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v3-large_224_1.0_float/mobilenet_v3_float_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v3-large_224_1.0_float.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v3-large_224_1.0_float"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v3-large_224_1.0_uint8/mobilenet_v3_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v3-large_224_1.0_uint8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    return [imagenet_data.generate_input(workdir, input_details)]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v3-large_224_1.0_uint8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v3-large_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v3-large_224_1.0_float.tflite"


class MobilenetV3LargeTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV3LargeTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV3LargeTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=1e-4).all()
        )

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v3-large_uint8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/mobilenet_v3-large_224_1.0_uint8.tflite"


class MobilenetV3LargeUint8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV3LargeUint8Test, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV3LargeUint8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # Dequantize outputs.
        zero_point = details[0]["quantization_parameters"]["zero_points"][0]
        scale = details[0]["quantization_parameters"]["scales"][0]
        dequantized_iree_results = (iree_results - zero_point) * scale
        dequantized_tflite_results = (tflite_results - zero_point) * scale
        self.assertTrue(
            numpy.isclose(
                dequantized_iree_results, dequantized_tflite_results, atol=5e-3
            ).all()
        )

    def generate_inputs(self, input_details):
        return [imagenet_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/mobilenet_v3.5multiavg_1.00_224_int8/mobilenet_v35_int8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import imagenet_data


# model_path = "https://storage.googleapis.com/tf_model_garden/vision/mobilenet/v3.5multiavg_1.0_int8/mobilenet_v3.5multiavg_1.00_224_int8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    inputs = imagenet_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v3.5multiavg_1.00_224_int8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        ## post process results for compare
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 224, 224, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/mobilenet_v35_int8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import imagenet_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/tf_model_garden/vision/mobilenet/v3.5multiavg_1.0_int8/mobilenet_v3.5multiavg_1.00_224_int8.tflite"


class MobilenetV35Int8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MobilenetV35Int8Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(MobilenetV35Int8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        # The difference here is quite high for a dequantized output.
        self.assertTrue(
            numpy.isclose(iree_results, tflite_results, atol=0.5).all()
        )

        # Make sure the predicted class is the same.
        iree_predicted_class = numpy.argmax(iree_results[0][0])
        tflite_predicted_class = numpy.argmax(tflite_results[0][0])
        self.assertEqual(iree_predicted_class, tflite_predicted_class)

    def generate_inputs(self, input_details):
        inputs = imagenet_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/multi_person_mobilenet_v1_075_float/multi_person_mobilenet_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/multi_person_mobilenet_v1_075_float.tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        mlir_result = np.expand_dims(mlir_result, axis=0)
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        print("mlir_result.shape: ", mlir_result.shape)
        print("tflite_result.shape: ", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="mobilenet_v2_1.0_224"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/nasnet/nasnet_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/tensorflow/lite-model/nasnet/large/1/default/1?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class NasnetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="nasnet"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class NasnetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = NasnetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = NasnetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/nasnet_test.py`:

```py
# RUN: %PYTHON %s
# REQUIRES: hugetest

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/tensorflow/lite-model/nasnet/large/1/default/1?lite-format=tflite"


class MnasnetTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(MnasnetTest, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/person_detect_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util
import urllib.request

from PIL import Image

model_path = "https://github.com/tensorflow/tflite-micro/raw/aeac6f39e5c7475cea20c54e86d41e3a38312546/tensorflow/lite/micro/models/person_detect.tflite"


class PersonDetectTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(PersonDetectTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(PersonDetectTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-3).all()
        )

    # TFLite is broken with this model so we hardcode the input/output details.
    def setup_tflite(self):
        self.input_details = [
            {
                "shape": [1, 96, 96, 1],
                "dtype": numpy.int8,
                "index": 0,
            }
        ]
        self.output_details = [
            {
                "shape": [1, 2],
                "dtype": numpy.int8,
            }
        ]

    # The input has known expected values. We hardcode this value.
    def invoke_tflite(self, args):
        return [numpy.array([[-113, 113]], dtype=numpy.int8)]

    def generate_inputs(self, input_details):
        img_path = "https://github.com/tensorflow/tflite-micro/raw/aeac6f39e5c7475cea20c54e86d41e3a38312546/tensorflow/lite/micro/examples/person_detection/testdata/person.bmp"
        local_path = "/".join([self.workdir, "person.bmp"])
        urllib.request.urlretrieve(img_path, local_path)

        shape = input_details[0]["shape"]
        im = numpy.array(
            Image.open(local_path).resize((shape[1], shape[2]))
        ).astype(input_details[0]["dtype"])
        args = [im.reshape(shape)]
        return args

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/posenet_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/multi_person_mobilenet_v1_075_float.tflite"


class PoseTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(PoseTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(PoseTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1e-3).all()
        )
        self.assertTrue(
            numpy.isclose(iree_results[1], tflite_results[1], atol=1e-2).all()
        )
        self.assertTrue(
            numpy.isclose(iree_results[2], tflite_results[2], atol=1e-2).all()
        )
        self.assertTrue(
            numpy.isclose(iree_results[3], tflite_results[3], atol=1e-3).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/resnet_50_224_int8/resnet_50_224_int8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://storage.googleapis.com/tf_model_garden/vision/resnet50_imagenet/resnet_50_224_int8.tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class ResnetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="resnet_50_224_int8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class ResnetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = ResnetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = ResnetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/resnet_50_int8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util

model_path = "https://storage.googleapis.com/tf_model_garden/vision/resnet50_imagenet/resnet_50_224_int8.tflite"


class ResNet50Int8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(ResNet50Int8Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(ResNet50Int8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1.0).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/rosetta_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/tulasiram58827/lite-model/rosetta/dr/1?lite-format=tflite"


# tfl.padv2 cannot be lowered to tosa.pad. May be possible to switch tosa.concat
class RosettaTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(RosettaTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(RosettaTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=5e-3).all()
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/spice_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import numpy
import test_util

model_path = "https://tfhub.dev/google/lite-model/spice/1?lite-format=tflite"


# Currently unsupported:
# 1. Multiple unsupported dynamic operations (tfl.stride, range, gather).
# 2. Static version blocked by tfl.range not having a lowering for static fixed shapes.
class SpiceTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SpiceTest, self).__init__(model_path, *args, **kwargs)

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/squad_data.py`:

```py
# An example input combination from the Squad 1.1 dataset.
_INPUT_WORD_ID = [
    101,
    2129,
    2116,
    19576,
    2015,
    2106,
    3854,
    4679,
    2486,
    1029,
    102,
    1996,
    14169,
    2165,
    2019,
    2220,
    2599,
    1999,
    3565,
    4605,
    2753,
    1998,
    2196,
    11145,
    1012,
    8446,
    2001,
    3132,
    2011,
    7573,
    1005,
    1055,
    3639,
    1010,
    2029,
    14159,
    2032,
    2698,
    2335,
    1998,
    3140,
    2032,
    2046,
    2093,
    20991,
    2015,
    1010,
    2164,
    1037,
    19576,
    2029,
    2027,
    6757,
    2005,
    1037,
    7921,
    1012,
    7573,
    15674,
    3854,
    4679,
    2001,
    2315,
    3565,
    4605,
    12041,
    1010,
    3405,
    2274,
    3948,
    10455,
    1010,
    1016,
    13714,
    14918,
    1010,
    1998,
    2048,
    3140,
    19576,
    2015,
    1012,
    102,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
]
_INPUT_TYPE_ID = [
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
]
_INPUT_MASK = [
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
]

```

`tank/tflite/squad_test_data.py`:

```py
# An example input combination from the Squad 1.1 dataset.
_INPUT_WORD_ID = [
    101,
    2129,
    2116,
    19576,
    2015,
    2106,
    3854,
    4679,
    2486,
    1029,
    102,
    1996,
    14169,
    2165,
    2019,
    2220,
    2599,
    1999,
    3565,
    4605,
    2753,
    1998,
    2196,
    11145,
    1012,
    8446,
    2001,
    3132,
    2011,
    7573,
    1005,
    1055,
    3639,
    1010,
    2029,
    14159,
    2032,
    2698,
    2335,
    1998,
    3140,
    2032,
    2046,
    2093,
    20991,
    2015,
    1010,
    2164,
    1037,
    19576,
    2029,
    2027,
    6757,
    2005,
    1037,
    7921,
    1012,
    7573,
    15674,
    3854,
    4679,
    2001,
    2315,
    3565,
    4605,
    12041,
    1010,
    3405,
    2274,
    3948,
    10455,
    1010,
    1016,
    13714,
    14918,
    1010,
    1998,
    2048,
    3140,
    19576,
    2015,
    1012,
    102,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
]
_INPUT_TYPE_ID = [
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
]
_INPUT_MASK = [
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
]

```

`tank/tflite/squeezenet/squeezenet_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args


# model_path = "https://tfhub.dev/tensorflow/lite-model/squeezenet/1/default/1?lite-format=tflite"


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        mlir_result = np.expand_dims(mlir_result, axis=0)
        print("mlir_result.shape", mlir_result.shape)
        print("tflite_result.shape", tflite_result.shape)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class SequeezeNetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="squeezenet"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)


class SequeezeNetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = SequeezeNetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = SequeezeNetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/squeezenet_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import test_util

model_path = "https://tfhub.dev/tensorflow/lite-model/squeezenet/1/default/1?lite-format=tflite"


class SqueezeNetTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SqueezeNetTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(SqueezeNetTest, self).compare_results(
            iree_results, tflite_results, details
        )

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_mobilenet_v1_320_1.0_float/ssd_mobilenet_v1_float_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import coco_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v1_320_1.0_float.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    inputs = coco_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="ssd_mobilenet_v1_320_1.0_float"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 320, 320, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/ssd_mobilenet_v1_320_1.0_uint8/ssd_mobilenet_v1_uint8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import coco_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v1_320_1.0_uint8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    return [coco_data.generate_input(workdir, input_details)]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="ssd_mobilenet_v1_320_1.0_uint8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 320, 320, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/ssd_mobilenet_v1_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v1_320_1.0_float.tflite"


class SsdMobilenetV1Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdMobilenetV1Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdMobilenetV1Test, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            self.assertTrue(
                numpy.isclose(
                    iree_results[i], tflite_results[i], atol=1e-4
                ).all()
            )

    def generate_inputs(self, input_details):
        inputs = coco_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_mobilenet_v1_uint8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v1_320_1.0_uint8.tflite"


class SsdMobilenetV1Uint8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdMobilenetV1Uint8Test, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdMobilenetV1Uint8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            # Dequantize outputs.
            zero_point = details[i]["quantization_parameters"]["zero_points"][
                0
            ]
            scale = details[i]["quantization_parameters"]["scales"][0]
            dequantized_iree_results = (iree_results[i] - zero_point) * scale
            dequantized_tflite_results = (
                tflite_results[i] - zero_point
            ) * scale
            self.assertTrue(
                numpy.isclose(
                    dequantized_iree_results,
                    dequantized_tflite_results,
                    atol=0.1,
                ).all()
            )

    def generate_inputs(self, input_details):
        return [coco_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_mobilenet_v2_face_quant/ssd_mobilenet_v2_face_quant_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
import urllib.request
from PIL import Image


# Model from https://github.com/google-coral/test_data/raw/master/ssd_mobilenet_v2_face_quant_postprocess.tflite
# but trimmed the final TFLite_PostProcess op.
# model_path = "https://storage.googleapis.com/iree-shared-files/models/ssd_mobilenet_v2_face_quant.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    img_path = (
        "https://github.com/google-coral/test_data/raw/master/grace_hopper.bmp"
    )
    local_path = "/".join([workdir, "grace_hopper.bmp"])
    urllib.request.urlretrieve(img_path, local_path)

    shape = input_details[0]["shape"]
    im = np.array(Image.open(local_path).resize((shape[1], shape[2])))
    args = [im.reshape(shape)]
    return args


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class MobilenetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="ssd_mobilenet_v2_face_quant"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 320, 320, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class MobilenetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = MobilenetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.pad' op attribute 'quantization_info' failed  "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = MobilenetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/ssd_mobilenet_v2_fpnlite_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v2_fpnlite_dynamic_1.0_float.tflite"


class SsdMobilenetV2FpnliteTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdMobilenetV2FpnliteTest, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdMobilenetV2FpnliteTest, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            self.assertTrue(
                numpy.isclose(
                    iree_results[i], tflite_results[i], atol=1e-4
                ).all()
            )

    def generate_inputs(self, input_details):
        inputs = coco_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_mobilenet_v2_fpnlite_uint8_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v2_fpnlite_dynamic_1.0_uint8.tflite"


class SsdMobilenetV2FpnliteUint8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdMobilenetV2FpnliteUint8Test, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdMobilenetV2FpnliteUint8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            # Dequantize outputs.
            zero_point = details[i]["quantization_parameters"]["zero_points"][
                0
            ]
            scale = details[i]["quantization_parameters"]["scales"][0]
            dequantized_iree_results = (iree_results[i] - zero_point) * scale
            dequantized_tflite_results = (
                tflite_results[i] - zero_point
            ) * scale
            self.assertTrue(
                numpy.isclose(
                    dequantized_iree_results,
                    dequantized_tflite_results,
                    atol=0.1,
                ).all()
            )

    def generate_inputs(self, input_details):
        return [coco_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_mobilenet_v2_int8_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v2_dynamic_1.0_int8.tflite"


class SsdMobilenetV2Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdMobilenetV2Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdMobilenetV2Test, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            # Dequantize outputs.
            zero_point = details[i]["quantization_parameters"]["zero_points"][
                0
            ]
            scale = details[i]["quantization_parameters"]["scales"][0]
            dequantized_iree_results = (iree_results[i] - zero_point) * scale
            dequantized_tflite_results = (
                tflite_results[i] - zero_point
            ) * scale
            self.assertTrue(
                numpy.isclose(
                    dequantized_iree_results,
                    dequantized_tflite_results,
                    atol=0.1,
                ).all()
            )

    def generate_inputs(self, input_details):
        inputs = coco_test_data.generate_input(self.workdir, input_details)
        # Move input values from [0, 255] to [-128, 127].
        inputs = inputs - 128
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_mobilenet_v2_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_mobilenet_v2_dynamic_1.0_float.tflite"


class SsdMobilenetV2Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdMobilenetV2Test, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdMobilenetV2Test, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            self.assertTrue(
                numpy.isclose(
                    iree_results[i], tflite_results[i], atol=1e-4
                ).all()
            )

    def generate_inputs(self, input_details):
        inputs = coco_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_spaghettinet_edgetpu_large/ssd_spaghettinet_edgetpu_large_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import coco_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_spaghettinet_edgetpu_large.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    inputs = coco_data.generate_input(workdir, input_details)
    # Normalize inputs to [-1, 1].
    inputs = (inputs.astype("float32") / 127.5) - 1
    return [inputs]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class SpaghettinetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb
        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="ssd_spaghettinet_edgetpu_large"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        ## post process results for compare
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 320, 320, 3],
                "dtype": np.float32,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class SpaghettinetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = SpaghettinetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = SpaghettinetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/ssd_spaghettinet_edgetpu_large_uint8/ssd_spaghettinet_edgetpu_large_uint8_tflite_test.py`:

```py
import numpy as np
from shark.shark_downloader import download_tflite_model
from shark.shark_inference import SharkInference
import pytest
import unittest
from shark.parser import shark_args
import os
import sys
from tank.tflite import coco_data


# model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_spaghettinet_edgetpu_large_uint8.tflite"


def generate_inputs(input_details):
    exe_basename = os.path.basename(sys.argv[0])
    workdir = os.path.join(os.path.dirname(__file__), "../tmp", exe_basename)
    os.makedirs(workdir, exist_ok=True)

    return [coco_data.generate_input(workdir, input_details)]


def compare_results(mlir_results, tflite_results):
    print("Compare mlir_results VS tflite_results: ")
    assert len(mlir_results) == len(
        tflite_results
    ), "Number of results do not match"
    for i in range(len(mlir_results)):
        mlir_result = mlir_results[i]
        tflite_result = tflite_results[i]
        mlir_result = mlir_result.astype(np.single)
        tflite_result = tflite_result.astype(np.single)
        assert mlir_result.shape == tflite_result.shape, "shape doesnot match"
        max_error = np.max(np.abs(mlir_result - tflite_result))
        print("Max error (%d): %f", i, max_error)


class SpaghettinetTfliteModuleTester:
    def __init__(
        self,
        dynamic=False,
        device="cpu",
        save_mlir=False,
        save_vmfb=False,
    ):
        self.dynamic = dynamic
        self.device = device
        self.save_mlir = save_mlir
        self.save_vmfb = save_vmfb

    def create_and_check_module(self):
        shark_args.save_mlir = self.save_mlir
        shark_args.save_vmfb = self.save_vmfb

        # Preprocess to get SharkImporter input args
        mlir_model, func_name, inputs, tflite_results = download_tflite_model(
            model_name="ssd_spaghettinet_edgetpu_large_uint8"
        )

        # Use SharkInference to get inference result
        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )

        # Case1: Use shark_importer default generate inputs
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)

        # Case2: Use manually set inputs
        input_details = [
            {
                "shape": [1, 320, 320, 3],
                "dtype": np.uint8,
            }
        ]
        inputs = generate_inputs(input_details)  # new inputs

        shark_module = SharkInference(
            mlir_module=mlir_model,
            function_name=func_name,
            device=self.device,
            mlir_dialect="tflite",
        )
        shark_module.compile()
        mlir_results = shark_module.forward(inputs)
        compare_results(mlir_results, tflite_results)
        # print(mlir_results)


class SpaghettinetTfliteModuleTest(unittest.TestCase):
    @pytest.fixture(autouse=True)
    def configure(self, pytestconfig):
        self.save_mlir = pytestconfig.getoption("save_mlir")
        self.save_vmfb = pytestconfig.getoption("save_vmfb")

    def setUp(self):
        self.module_tester = SpaghettinetTfliteModuleTester(self)
        self.module_tester.save_mlir = self.save_mlir

    import sys

    @pytest.mark.xfail(
        reason="known macos tflite install issue & "
        "'tosa.conv2d' op attribute 'quantization_info' failed "
    )
    def test_module_static_cpu(self):
        self.module_tester.dynamic = False
        self.module_tester.device = "cpu"
        self.module_tester.create_and_check_module()


if __name__ == "__main__":
    # module_tester = SpaghettinetTfliteModuleTester()
    # module_tester.save_mlir = True
    # module_tester.save_vmfb = True
    # module_tester.create_and_check_module()

    unittest.main()

```

`tank/tflite/ssd_spaghettinet_large_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_spaghettinet_edgetpu_large.tflite"


class SsdSpaghettinetLargeTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdSpaghettinetLargeTest, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdSpaghettinetLargeTest, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            print("iree_results: " + str(iree_results[i]))
            print("tflite_results: " + str(tflite_results[i]))
            self.assertTrue(
                numpy.isclose(
                    iree_results[i], tflite_results[i], atol=1e-4
                ).all()
            )

    def generate_inputs(self, input_details):
        inputs = coco_test_data.generate_input(self.workdir, input_details)
        # Normalize inputs to [-1, 1].
        inputs = (inputs.astype("float32") / 127.5) - 1
        return [inputs]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/ssd_spaghettinet_large_uint8_test.py`:

```py
# RUN: %PYTHON %s
# XFAIL: *

import absl.testing
import coco_test_data
import numpy
import test_util

model_path = "https://storage.googleapis.com/iree-model-artifacts/ssd_spaghettinet_edgetpu_large_uint8.tflite"


class SsdSpaghettinetLargeUint8Test(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(SsdSpaghettinetLargeUint8Test, self).__init__(
            model_path, *args, **kwargs
        )

    def compare_results(self, iree_results, tflite_results, details):
        super(SsdSpaghettinetLargeUint8Test, self).compare_results(
            iree_results, tflite_results, details
        )
        for i in range(len(iree_results)):
            # Dequantize outputs.
            zero_point = details[i]["quantization_parameters"]["zero_points"][
                0
            ]
            scale = details[i]["quantization_parameters"]["scales"][0]
            dequantized_iree_results = (iree_results[i] - zero_point) * scale
            dequantized_tflite_results = (
                tflite_results[i] - zero_point
            ) * scale
            self.assertTrue(
                numpy.isclose(
                    dequantized_iree_results,
                    dequantized_tflite_results,
                    atol=0.1,
                ).all()
            )

    def generate_inputs(self, input_details):
        return [coco_test_data.generate_input(self.workdir, input_details)]

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank/tflite/test_util.py`:

```py
# Lint as: python3
# Copyright 2021 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
"""Test architecture for a set of tflite tests."""

import absl
from absl.flags import FLAGS
import absl.testing as testing
import iree.compiler.tflite as iree_tflite_compile
import iree.runtime as iree_rt
import numpy as np
import os
import sys
import tensorflow.compat.v2 as tf
import time
import urllib.request

targets = {
    "dylib": "dylib-llvm-aot",
    "vulkan": "vulkan-spirv",
}

configs = {
    "dylib": "dylib",
    "vulkan": "vulkan",
}

absl.flags.DEFINE_string("config", "dylib", "model path to execute")


class TFLiteModelTest(testing.absltest.TestCase):
    def __init__(self, model_path, *args, **kwargs):
        super(TFLiteModelTest, self).__init__(*args, **kwargs)
        self.model_path = model_path

    def setUp(self):
        if self.model_path is None:
            return
        exe_basename = os.path.basename(sys.argv[0])
        self.workdir = os.path.join(
            os.path.dirname(__file__), "tmp", exe_basename
        )
        print(f"TMP_DIR = {self.workdir}")
        os.makedirs(self.workdir, exist_ok=True)
        self.tflite_file = "/".join([self.workdir, "model.tflite"])
        self.tflite_ir = "/".join([self.workdir, "tflite.mlir"])
        self.iree_ir = "/".join([self.workdir, "tosa.mlir"])
        if os.path.exists(self.model_path):
            self.tflite_file = self.model_path
        else:
            urllib.request.urlretrieve(self.model_path, self.tflite_file)
        self.binary = "/".join([self.workdir, "module.bytecode"])

    def generate_inputs(self, input_details):
        args = []
        for input in input_details:
            absl.logging.info(
                "\t%s, %s", str(input["shape"]), input["dtype"].__name__
            )
            args.append(np.zeros(shape=input["shape"], dtype=input["dtype"]))
        return args

    def compare_results(self, iree_results, tflite_results, details):
        self.assertEqual(
            len(iree_results),
            len(tflite_results),
            "Number of results do not match",
        )

        for i in range(len(details)):
            iree_result = iree_results[i]
            tflite_result = tflite_results[i]
            iree_result = iree_result.astype(np.single)
            tflite_result = tflite_result.astype(np.single)
            self.assertEqual(iree_result.shape, tflite_result.shape)
            maxError = np.max(np.abs(iree_result - tflite_result))
            absl.logging.info("Max error (%d): %f", i, maxError)

    def setup_tflite(self):
        absl.logging.info("Setting up tflite interpreter")
        self.tflite_interpreter = tf.lite.Interpreter(
            model_path=self.tflite_file
        )
        self.tflite_interpreter.allocate_tensors()
        self.input_details = self.tflite_interpreter.get_input_details()
        self.output_details = self.tflite_interpreter.get_output_details()

    def setup_iree(self):
        absl.logging.info("Setting up iree runtime")
        with open(self.binary, "rb") as f:
            config = iree_rt.Config(configs[absl.flags.FLAGS.config])
            self.iree_context = iree_rt.SystemContext(config=config)
            vm_module = iree_rt.VmModule.from_flatbuffer(f.read())
            self.iree_context.add_vm_module(vm_module)

    def invoke_tflite(self, args):
        for i, input in enumerate(args):
            self.tflite_interpreter.set_tensor(
                self.input_details[i]["index"], input
            )
        start = time.perf_counter()
        self.tflite_interpreter.invoke()
        end = time.perf_counter()
        tflite_results = []
        absl.logging.info(f"Invocation time: {end - start:0.4f} seconds")
        for output_detail in self.output_details:
            tflite_results.append(
                np.array(
                    self.tflite_interpreter.get_tensor(output_detail["index"])
                )
            )

        for i in range(len(self.output_details)):
            dtype = self.output_details[i]["dtype"]
            tflite_results[i] = tflite_results[i].astype(dtype)
        return tflite_results

    def invoke_iree(self, args):
        invoke = self.iree_context.modules.module["main"]
        start = time.perf_counter()
        iree_results = invoke(*args)
        end = time.perf_counter()
        absl.logging.info(f"Invocation time: {end - start:0.4f} seconds")
        if not isinstance(iree_results, tuple):
            iree_results = (iree_results,)
        return iree_results

    def compile_and_execute(self):
        self.assertIsNotNone(self.model_path)

        absl.logging.info("Setting up for IREE")
        iree_tflite_compile.compile_file(
            self.tflite_file,
            input_type="tosa",
            output_file=self.binary,
            save_temp_tfl_input=self.tflite_ir,
            save_temp_iree_input=self.iree_ir,
            target_backends=[targets[absl.flags.FLAGS.config]],
            import_only=False,
        )

        self.setup_tflite()
        self.setup_iree()

        absl.logging.info("Setting up test inputs")
        args = self.generate_inputs(self.input_details)

        absl.logging.info("Invoking TFLite")
        tflite_results = self.invoke_tflite(args)

        absl.logging.info("Invoke IREE")
        iree_results = self.invoke_iree(args)

        # Fix type information for unsigned cases.
        iree_results = list(iree_results)
        for i in range(len(self.output_details)):
            dtype = self.output_details[i]["dtype"]
            iree_results[i] = iree_results[i].astype(dtype)

        self.compare_results(iree_results, tflite_results, self.output_details)

```

`tank/tflite/visual_wake_words_i8_test.py`:

```py
# RUN: %PYTHON %s

import absl.testing
import numpy
import test_util
import urllib.request

from PIL import Image

model_path = "https://github.com/mlcommons/tiny/raw/0b04bcd402ee28f84e79fa86d8bb8e731d9497b8/v0.5/training/visual_wake_words/trained_models/vww_96_int8.tflite"


# Failure is due to dynamic shapes. This model has a dynamic batch dimension
# and there is not currently supported. Flatbuffer was modified to use static
#  shapes and was otherwise numerically correct.
class VisualWakeWordsTest(test_util.TFLiteModelTest):
    def __init__(self, *args, **kwargs):
        super(VisualWakeWordsTest, self).__init__(model_path, *args, **kwargs)

    def compare_results(self, iree_results, tflite_results, details):
        super(VisualWakeWordsTest, self).compare_results(
            iree_results, tflite_results, details
        )
        self.assertTrue(
            numpy.isclose(iree_results[0], tflite_results[0], atol=1).all()
        )

    def generate_inputs(self, input_details):
        img_path = "https://github.com/tensorflow/tflite-micro/raw/main/tensorflow/lite/micro/examples/person_detection/testdata/person.bmp"
        local_path = "/".join([self.workdir, "person.bmp"])
        urllib.request.urlretrieve(img_path, local_path)

        shape = input_details[0]["shape"]
        input_type = input_details[0]["dtype"]
        im = numpy.array(
            Image.open(local_path)
            .resize((shape[1], shape[2]))
            .convert(mode="RGB")
        )
        args = [im.reshape(shape).astype(input_type)]
        return args

    def test_compile_tflite(self):
        self.compile_and_execute()


if __name__ == "__main__":
    absl.testing.absltest.main()

```

`tank_version.json`:

```json
{
	"version": "nightly"
}

```