Project Path: arc_gmh5225_mambo_0zzx8of1

Source Tree:

```txt
arc_gmh5225_mambo_0zzx8of1
├── LICENSE
├── README.md
├── api
│   ├── branch_decoder_support.c
│   ├── generate_emit_wrapper.rb
│   ├── hash_table.c
│   ├── hash_table.h
│   ├── helpers.c
│   ├── helpers.h
│   ├── internal.c
│   ├── load_store.c
│   ├── plugin_support.c
│   └── plugin_support.h
├── arch
│   ├── aarch32
│   │   ├── dispatcher_aarch32.S
│   │   ├── dispatcher_aarch32.c
│   │   ├── scanner_a32.c
│   │   └── scanner_t32.c
│   └── aarch64
│       ├── dispatcher_aarch64.S
│       ├── dispatcher_aarch64.c
│       └── scanner_a64.c
├── common.c
├── common.h
├── dbm.c
├── dbm.h
├── dispatcher.c
├── elf
│   ├── elf_loader.c
│   ├── elf_loader.h
│   └── symbol_parser.c
├── kernel_sigaction.h
├── makefile
├── pie
├── plugins
│   ├── branch_count.c
│   ├── cachesim
│   │   ├── README.md
│   │   ├── cachesim.S
│   │   ├── cachesim.c
│   │   ├── cachesim_buffer.h
│   │   ├── cachesim_model.c
│   │   └── cachesim_model.h
│   ├── follow_exec.c
│   ├── instruction_mix.c
│   ├── memcheck
│   │   ├── README.md
│   │   ├── memcheck.S
│   │   ├── memcheck.c
│   │   ├── memcheck.h
│   │   └── naive_stdlib.c
│   ├── mtrace.S
│   ├── mtrace.c
│   ├── mtrace.h
│   ├── poc_log_returns.c
│   ├── soft_div.c
│   ├── strace.c
│   ├── symbol_example.c
│   └── tb_count.c
├── plugins.h
├── scanner_common.h
├── scanner_public.h
├── signals.c
├── syscalls.c
├── syscalls.h
├── test
│   ├── hw_div.S
│   ├── load_store.S
│   ├── load_store.c
│   ├── makefile
│   ├── mmap_munmap.c
│   ├── mprotect_exec.c
│   ├── self_modifying.c
│   ├── signals.S
│   └── signals.c
├── traces.c
├── util.S
└── util.h

```

`LICENSE`:

```

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

```

`README.md`:

```md
MAMBO: A Low-Overhead Dynamic Binary Modification Tool for ARM
==============================================================

News
----

* 2021-09-21: We've released a partial port of MAMBO to RISC-V in the [riscv branch](https://github.com/beehive-lab/mambo/tree/riscv), where development is continuing. The initial porting was done by Guillermo Callaghan and Cosmin Gorgovan.
* 2021-02-24: PhD opportunities related to MAMBO and security available. If interested, contact mikel.lujan at manchester.ac.uk with the subject *APT MAMBO PhD \[your surname\]*
* 2020-03-16: We've presented (virtually) our VEE paper about AArch64 optimizations in MAMBO. The talk is [available on youtube](https://www.youtube.com/watch?v=3jxLu1zGpV0).
* 2020-02-22: We've presented our CC paper about MAMBO's API (see *Publications* and *Plugin API* below) and the [`cachesim` (an online cache simulator)](/plugins/cachesim) and [`memcheck` (a memory error checker)](/plugins/memcheck) plugins. The slides are available [here](https://github.com/beehive-lab/mambo/releases/download/2/slides_mambo_cc20.pdf).
* 2018-04-11: We've presented our ICPE paper. The slides are available [here](https://github.com/beehive-lab/mambo/releases/download/1/slides_icpe_2018.pdf).
* 2018-01-23: We've ran a tutorial on using MAMBO's API at HiPEAC 2018. The slides are available [here](https://github.com/beehive-lab/mambo/releases/download/1/mambo_tutorial_hipeac_2018.pdf).
* 2017-04-24: An address decoder for load and store instructions was added to the API: `mambo_calc_ld_st_addr()`. It allows plugin developers to automatically obtain the base address of all data memory accesses. This API function is available for all supported instruction sets: A32, T32, A64. Its usage is demonstrated in the `plugins/mtrace.c` plugin.
* 2017-04-04: Significantly improved support for Linux signals was implemented.
* 2017-04-03: The AArch64 port of MAMBO is officially released. The initial AArch64 porting was done by Guillermo Callaghan <guillermocallaghan at hotmail dot com>.
* We have presented the TACO paper at [HiPEAC](https://www.hipeac.net/events/activities/7477/session-9-binary-translation/) 2017, on 25th of January. The slides are available [here](https://github.com/beehive-lab/mambo/releases/download/1/slides_hipeac_2017.pdf).

Publications
------------
* [Cosmin Gorgovan, Amanieu d’Antras, and Mikel Luján. 2016. MAMBO: A low-overhead dynamic binary modification tool for ARM. ACM Trans. Archit. Code Optim. 13, 1, Article 14 (April 2016)](http://dl.acm.org/citation.cfm?id=2896451). **Open access**. If you use MAMBO for your research, please cite this paper.

* [Cosmin Gorgovan, Amanieu d’Antras, and Mikel Luján. 2018. Optimising Dynamic Binary Modification Across ARM Microarchitectures. In Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering (ICPE '18)](https://dl.acm.org/citation.cfm?id=3184425). **Free download** [via research.manchester.ac.uk](https://www.research.manchester.ac.uk/portal/en/publications/optimising-dynamic-binary-modification-across-arm-microarchitectures(6eedcdc7-d5af-488a-815e-6e4968f96fc5).html).

* [Cosmin Gorgovan, Guillermo Callaghan, and Mikel Luján. Balancing Performance and Productivity for the Development of Dynamic Binary Instrumentation Tools - A Case Study on Arm Systems. In Proceedings of the 29th International Conference on Compiler Construction (CC '20)](https://dl.acm.org/doi/abs/10.1145/3377555.3377895) **Free download** [via research.manchester.ac.uk](https://www.research.manchester.ac.uk/portal/en/publications/balancing-performance-and-productivity-for-the-development-of-dynamic-binary-instrumentation-tools--a-case-study-on-arm-systems(80e57c1b-9e38-4a15-942d-eb240888b12b).html).

* [Guillermo Callaghan, Cosmin  Gorgovan and Mikel Luján. Optimising dynamic binary modification across 64-bit Arm microarchitectures. In Proceedings of the 16th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE '20)](https://dl.acm.org/doi/abs/10.1145/3381052.3381322) **Free download** [via research.manchester.ac.uk](https://www.research.manchester.ac.uk/portal/en/publications/optimising-dynamic-binary-modification-across-64bit-arm-microarchitectures(f8e4680f-9410-46c0-960a-8d8a932b974d).html).


MAMBO was created as part of Cosmin's [EPSRC](https://www.epsrc.ac.uk)-funded PhD in the [School of Computer Science](http://www.cs.manchester.ac.uk/) at the [University of Manchester](http://www.manchester.ac.uk/). MAMBO is currently being developed as part of the [PAMELA EP/K008730/1](http://apt.cs.manchester.ac.uk/projects/PAMELA/) and DOME EP/J016330/1 EPSRC projects.

Status
------

MAMBO's compatibility with applications is continuously being improved as needed. We are using it on ARMv7 and ARMv8 systems. Our systems run the *armhf* / *arm64* builds of Debian, Ubuntu and Arch Linux ARM. Most GNU/Linux applications work correctly. The following more complex applications and benchmark suites are working correctly under MAMBO on our systems (this is not an exhaustive list):

* [SPEC CPU2006](https://www.spec.org/cpu2006/)
* [PARSEC 3.0](http://parsec.cs.princeton.edu/)
* [SLAMBench](http://apt.cs.manchester.ac.uk/projects/PAMELA/tools/SLAMBench/)
* [GCC](https://gcc.gnu.org/) (GCC running under MAMBO can build MAMBO)
* [LibreOffice](https://www.libreoffice.org/)
* [GIMP](https://www.gimp.org/)
* [SuperTuxKart](http://supertuxkart.sourceforge.net/)
* [XMoto](https://xmoto.tuxfamily.org/)

Also read the *Known issues* section below.


Build
-----

Prerequisites: an ARM system (physical or virtual) to build and run MAMBO on; dependencies: gcc toolchain, libelf(-dev), ruby (>=1.9.1). Debian or Ubuntu users can install the required packages with `sudo apt-get install build-essential libelf-dev ruby`.

    git clone --recurse-submodules https://github.com/beehive-lab/mambo.git
    cd mambo
    make


Usage
-----

To launch an application under MAMBO, run:

    ./dbm <path_to_executable> [application's command line arguments]

For example to run `ls -a` under MAMBO, execute:

    ./dbm /bin/ls -a

Tip: When an application running under MAMBO exits, the string `We're done; exiting with status: <APPLICATION'S EXIT CODE>` will be printed to stderr.


Plugin API
----------

The plugin API is event-driven. Plugins should use a init function with `__attribute__((constructor))` to register themselves using `mambo_register_plugin()`. Once a plugin is registered, it can install callbacks for various events using the `mambo_register_*_cb()` functions. Callback-related functions are listed in `api/plugin_support.h`. Code generation functions are listed in `api/emit_<INST SET>.h` and code generation helpers are listed in `api/helpers.h`. You can also inspect the sample plugin in the `plugins/` directory.

To build MAMBO with plugin support, the source code or object file(s) of the plugin you're trying to build must be added to the `PLUGINS=` line in the `makefile`, or provided as an argument/envvar. Note that multiple plugins can be enabled at the same time (and will work correctly if properly designed). For performance reasons, it is recommended to remove unused plugins from the `PLUGINS=` list.


Known issues
------------

* There are two limitations related to signal handling: the data in the `siginfo_t` structure passed to `SA_SIGINFO` signal handlers is incorrect: most signals will appear to have been sent via `kill()` from the application itself; and synchronous signal (SIGSEGV, SIGBUS, SIGFPE, SIGTRAP, SIGILL, SIGSYS) handlers cannot `sigreturn()`, but can `(sig)longjmp()`.
* At the moment, code cache invalidation in response to the `munmap` and `__cache_flush` system calls are only done in the thread in which the system call is executed. This can potentially lead to execution of stale cached code in other threads.


Reporting bugs
--------------

If you think you have found a bug which is not in the list of *Known issues*, please report it [here, on Github](https://github.com/beehive-lab/mambo/issues). However, note that we have limited time available to investigate and fix bugs which are not affecting the workloads we are using. Therefore, if you can't pinpoint the cause of the bug yourself, we ask that you provide as many details on how to reproduce it, and preferably provide a statically linked executable which triggers it.


Contributions
-------------

We welcome contributions. Use pull requests on Github. However, note that we are doing most development in a private git tree and we are working on a number of features which are not quite ready for public release. Therefore, we would strongly encourage you to get in touch before starting to work on anything large, to avoid duplication of effort. We can probably expedite our release of any WIP features you might be interested in, if you do that.


Sandboxing
----------

Note that similarly to [most other DBM / DBI frameworks](https://github.com/lgeek/dynamorio_pin_escape) and to optimise performance / development effort, MAMBO is not designed to secure itself against malicious activity from the application it is translating. This means that without hardening MAMBO itself, it would not be possible to use it to implement a secure sandbox.

```

`api/branch_decoder_support.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>

#include "../dbm.h"
#include "../common.h"
#include "plugin_support.h"

#ifdef __arm__
  #include "../pie/pie-thumb-decoder.h"
  #include "../pie/pie-thumb-field-decoder.h"
  #include "../pie/pie-arm-decoder.h"
  #include "../pie/pie-arm-field-decoder.h"
#endif
#ifdef __aarch64__
  #include "../pie/pie-a64-decoder.h"
  #include "../pie/pie-a64-field-decoder.h"
#endif

#ifdef PLUGINS_NEW

#ifdef __arm__
mambo_branch_type __get_thumb_branch_type(mambo_context *ctx) {
  mambo_branch_type type = BRANCH_NONE;

  switch (ctx->code.inst) {
    case THUMB_MOVH16: {
      uint32_t dn, rm, rdn;
      thumb_movh16_decode_fields(ctx->code.read_address, &dn, &rm, &rdn);
      rdn |= dn << 3;
      if (rdn == pc) {
        type =  BRANCH_INDIRECT;
        if (rm == lr) {
          type |= BRANCH_RETURN;
        }
      }
      break;
    }
    case THUMB_POP16: {
      uint32_t reglist;
      thumb_pop16_decode_fields(ctx->code.read_address, &reglist);
      if (reglist & (1 << 8)) {
        type =  BRANCH_INDIRECT | BRANCH_RETURN | BRANCH_INTERWORKING;
      }
      break;
    }
    case THUMB_LDRI32: {
      uint32_t rn, rt, imm8, p, u, w;
      thumb_ldri32_decode_fields(ctx->code.read_address, &rt, &rn, &imm8, &p, &u, &w);
      if (rt == pc) {
        type =  BRANCH_INDIRECT | BRANCH_INTERWORKING;
        if (rn == sp) {
          type |= BRANCH_RETURN;
        }
      }
      break;
    }
    case THUMB_LDR32: {
      uint32_t rn, rt, shift, rm;
      thumb_ldr32_decode_fields(ctx->code.read_address, &rn, &rt, &shift, &rm);
      if (rt == pc) {
        type =  BRANCH_INDIRECT | BRANCH_INTERWORKING;
        if (rn == sp) {
          type |= BRANCH_RETURN;
        }
      }
      break;
    }
    case THUMB_LDMFD32:
    case THUMB_LDMEA32: {
      uint32_t w, rn, reglist;
      thumb_ldmfd32_decode_fields(ctx->code.read_address, &w, &rn, &reglist);
	    if (reglist & (1 << pc)) {
	      type =  BRANCH_INDIRECT | BRANCH_INTERWORKING;
        if (rn == sp) {
          type |= BRANCH_RETURN;
        }
	    }
	    break;
    }
    case THUMB_BX16: {
      uint32_t rm;
      thumb_bx16_decode_fields(ctx->code.read_address, &rm);
      type =  BRANCH_INDIRECT | BRANCH_INTERWORKING;
      if (rm == lr) {
        type |= BRANCH_RETURN;
      }
      break;
    }
    case THUMB_BLX16:
      type =  BRANCH_INDIRECT | BRANCH_CALL | BRANCH_INTERWORKING;
      break;
    case THUMB_BL32:
      type =  BRANCH_DIRECT | BRANCH_CALL;
      break;
    case THUMB_BL_ARM32:
      type =  BRANCH_DIRECT | BRANCH_CALL | BRANCH_INTERWORKING;
      break;
    case THUMB_B16:
    case THUMB_B32:
      type =  BRANCH_DIRECT;
      break;
    case THUMB_CBZ16:
    case THUMB_CBNZ16:
      type =  BRANCH_DIRECT | BRANCH_COND | BRANCH_COND_CBZ;
      break;
    case THUMB_B_COND16:
    case THUMB_B_COND32:
      type =  BRANCH_DIRECT | BRANCH_COND | BRANCH_COND_PSR;
      break;
    case THUMB_TBB32:
    case THUMB_TBH32:
      type =  BRANCH_INDIRECT | BRANCH_TABLE;
      break;
  } // switch

  if (type != BRANCH_NONE && (type & BRANCH_COND) == 0 && mambo_get_cond(ctx) != AL) {
    type |= BRANCH_COND | BRANCH_COND_PSR | BRANCH_COND_IT;
  }

  return type;
}

mambo_branch_type __get_arm_branch_type(mambo_context *ctx) {
  mambo_branch_type type = BRANCH_NONE;

  switch (ctx->code.inst) {
    case ARM_ADC:
    case ARM_ADD:
    case ARM_EOR:
    case ARM_MOV:
    case ARM_ORR:
    case ARM_SBC:
    case ARM_SUB:
    case ARM_RSC: {
      uint32_t immediate, opcode, set_flags, rd, rn, operand2;
      arm_data_proc_decode_fields(ctx->code.read_address, &immediate, &opcode, &set_flags, &rd, &rn, &operand2);
      if (rd == pc) {
        type = BRANCH_INDIRECT | BRANCH_INTERWORKING;
      }
      break;
    }
    case ARM_BX: {
      uint32_t rn;
      arm_bx_decode_fields(ctx->code.read_address, &rn);
      type = BRANCH_INDIRECT | BRANCH_INTERWORKING;
      if (rn == lr) {
        type |= BRANCH_RETURN;
      }
      break;
    }
    case ARM_LDM: {
      uint32_t rn, regs, p, u, w, s;
      arm_ldm_decode_fields(ctx->code.read_address, &rn, &regs, &p, &u, &w, &s);
	    if (regs & (1 << pc)) {
	      type = BRANCH_INDIRECT | BRANCH_INTERWORKING;
	      if (rn == sp) {
	        type |= BRANCH_RETURN;
	      }
	    }
      break;
    }
    case ARM_LDR: {
      uint32_t i, rd, rn, op2, p, u, w;
      arm_ldr_decode_fields(ctx->code.read_address, &i, &rd, &rn, &op2, &p, &u, &w);
      if (rd == pc) {
        type = BRANCH_INDIRECT | BRANCH_INTERWORKING;
	      if (rn == sp) {
	        type |= BRANCH_RETURN;
	      }
      }
      break;
    }
    case ARM_BLX:
      type = BRANCH_INDIRECT | BRANCH_INTERWORKING | BRANCH_CALL;
      break;
    case ARM_B:
      type = BRANCH_DIRECT;
      break;
    case ARM_BL:
      type = BRANCH_DIRECT | BRANCH_CALL;
      break;
    case ARM_BLXI:
      type = BRANCH_DIRECT | BRANCH_CALL | BRANCH_INTERWORKING;
      break;
  }

  if (type != BRANCH_NONE && mambo_get_cond(ctx) != AL) {
    type |= BRANCH_COND | BRANCH_COND_PSR;
  }

  return type;
}
#endif // __arm__

mambo_branch_type mambo_get_branch_type(mambo_context *ctx) {
  mambo_branch_type type;

#ifdef __arm__
  if (mambo_get_inst_type(ctx) == THUMB_INST) {
   type = __get_thumb_branch_type(ctx);
  } else { // ARM
    type = __get_arm_branch_type(ctx);
  }
#endif
#ifdef __aarch64__
  type = BRANCH_NONE;

  switch (ctx->code.inst) {
    case A64_CBZ_CBNZ:
      type = BRANCH_DIRECT | BRANCH_COND | BRANCH_COND_CBZ;
      break;
    case A64_B_COND:
      type = BRANCH_DIRECT | BRANCH_COND | BRANCH_COND_PSR;
      break;
    case A64_TBZ_TBNZ:
      type = BRANCH_DIRECT | BRANCH_COND | BRANCH_COND_TBZ;
      break;
    case A64_BR:
      type = BRANCH_INDIRECT;
      break;
    case A64_BLR:
      type = BRANCH_INDIRECT | BRANCH_CALL;
      break;
    case A64_RET:
      type = BRANCH_INDIRECT | BRANCH_RETURN;
      break;
    case A64_B_BL: {
      uint32_t op, imm26;
      a64_B_BL_decode_fields(ctx->code.read_address, &op, &imm26);

      type = BRANCH_DIRECT;
      if (op == 1) { // BL
        type |= BRANCH_CALL;
      }
      break;
    }
  }
#endif // __aarch64__

  return type;
}

#endif // PLUGINS_NEW

```

`api/generate_emit_wrapper.rb`:

```rb
=begin

  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

=end

# Quick and dirty generator for emit-style function encoding wrappers for plugins
# Takes as argument the path to the C file of an instruction encoder generated by PIE

def get_filecode()
  "__EMIT_#{ARGV[0].gsub(/[^\w]/, "_").upcase}__"
end

def generate_header()
  puts "#ifdef PLUGINS_NEW"
  if (@header_only)
    puts "#ifndef #{get_filecode()}"
    puts "#define #{get_filecode()}"
  end
  puts "#include \"../dbm.h\"\n"
  puts "#include \"../#{ARGV[0].gsub(".c", ".h")}\""
  puts "#include \"plugin_support.h\"\n"
end

def generate_footer()
  if (@header_only)
    puts "#endif"
  end
  puts "#endif"
end

def rewrite_name(name)
  name.gsub('void ', 'void emit_')
end

def generate_body(name, fields, size)
  puts ")\n{"
  print "\t#{name}((uint#{@min_size * 8}_t **)(&ctx->code.write_p)"

  fields.each do |field|
    print ", #{field}"
  end

  puts ");"
  puts "\tctx->code.write_p += #{size};\n}"
end

def process_file(filename)
  fields = []
  name = ""
  size = 0
  File.readlines(filename).each do |line|
    if (line.match(/^void/))
      puts rewrite_name(line)
      line = line.split(' ')
      name = line[1]
      fields = []
    elsif (line.match(/address,?$/))
      print "\tmambo_context *ctx"
      #get the base instruction size, should be either uint16_t or uint32_t
      size = line.match(/[0-9]+/)[0].to_i / 8
      @min_size = size if (size < @min_size)
    elsif (line.match(/^\tunsigned int/))
      # function arguments, stripped of commas and excluding the address
      line = line.match(/[\w\s]+/)[0]
      print ",\n#{line}" 
      line = line.split(' ')
      fields.push(line[line.size-1])
    elsif (line.include?("**address = ") or line.include?("*(*address) ="))
      puts if (fields.size == 0)
      if (@header_only)
        puts ");"
      else
        size *= 2 if (line.include?("*(*address"))
        generate_body(name, fields, size)
      end
    end
  end
end

abort "Syntax: generate_emit_wrapper.rb <PIE_ENCODER.c> [header]" unless (ARGV[0])
@header_only = true if (ARGV[1] and ARGV[1] == "header")
@min_size = 4;

generate_header()
process_file(ARGV[0])
generate_footer()


```

`api/hash_table.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <pthread.h>
#include <assert.h>
#include <stdint.h>
#include <stdlib.h>

#include "hash_table.h"

int mambo_ht_init(mambo_ht_t *ht, size_t initial_size, int index_shift, int fill_factor, bool allow_resize) {
  if (fill_factor < 10 || fill_factor > 90) return -1;
  if (index_shift < 0 || index_shift > 20) return -1;

  // Round up the size to a power of 2
  size_t size = 1;
  while (size < initial_size) size <<= 1;

  int ret = pthread_mutex_init(&ht->lock, NULL);
  if (ret != 0) return -1;

  ht->entries = calloc(size, sizeof(mambo_ht_entry_t));
  if (ht->entries == NULL) return -1;

  ht->entry_count = 0;
  ht->size = size;
  ht->allow_resize = allow_resize;
  ht->fill_factor = fill_factor;
  ht->index_shift = index_shift;
  ht->resize_threshold = (ht->size * ht->fill_factor) / 100;

  return 0;
}

void __mambo_ht_lock(mambo_ht_t *ht) {
  int ret = pthread_mutex_lock(&ht->lock);
  assert(ret == 0);
}

void __mambo_ht_unlock(mambo_ht_t *ht) {
  int ret = pthread_mutex_unlock(&ht->lock);
  assert(ret == 0);
}

int __mambo_ht_resize(mambo_ht_t *ht) {
  mambo_ht_entry_t *prev_entries = ht->entries;
  size_t prev_size = ht->size;

  size_t new_size = ht->size << 1;
  mambo_ht_entry_t *new_entries = calloc(new_size, sizeof(mambo_ht_entry_t));
  if (new_entries == NULL) return -1;
 
  ht->entries = new_entries;
  ht->entry_count = 0;
  ht->size = new_size;
  ht->resize_threshold = ht->size * ht->fill_factor / 100;

  for (size_t i = 0; i < prev_size; i++) {
    if (prev_entries[i].key != 0) {
      int ret = mambo_ht_add_nolock(ht, prev_entries[i].key, prev_entries[i].value);
      assert(ret == 0);
    }
  }
  return 0;
}

int mambo_ht_add_nolock(mambo_ht_t *ht, uintptr_t key, uintptr_t value) {
  if (key == 0) return -1;

  if (ht->entry_count >= ht->resize_threshold) {
    if (ht->allow_resize) {
      const int ret = __mambo_ht_resize(ht);
      assert(ret == 0);
    } else {
      return -1;
    }
  }

  size_t index_max = (ht->size - 1);
  size_t index = (key >> ht->index_shift) & index_max;

  while (ht->entries[index].key != 0 && ht->entries[index].key != key) {
    index = (index + 1) & index_max;
  }
  if (ht->entries[index].key == 0) {
    ht->entry_count++;
  }
  ht->entries[index].key = key;
  ht->entries[index].value = value;

  return 0;
}

int mambo_ht_add(mambo_ht_t *ht, uintptr_t key, uintptr_t value) {
  __mambo_ht_lock(ht);
  int ret = mambo_ht_add_nolock(ht, key, value);
  __mambo_ht_unlock(ht);
  return ret;
}

int mambo_ht_get_nolock(mambo_ht_t *ht, uintptr_t key, uintptr_t *value) {
  if (key == 0) return -1;

  size_t index_max = (ht->size - 1);
  size_t index = (key >> ht->index_shift) & index_max;
  while (ht->entries[index].key != 0 && ht->entries[index].key != key) {
    index = (index + 1) & index_max;
  }
  if (ht->entries[index].key == key) {
    *value = ht->entries[index].value;
    return 0;
  }
  return -1;
}

int mambo_ht_get(mambo_ht_t *ht, uintptr_t key, uintptr_t *value) {
  __mambo_ht_lock(ht);
  int ret = mambo_ht_get_nolock(ht, key, value);
  __mambo_ht_unlock(ht);
  return ret;
}

int mambo_ht_delete_nolock(mambo_ht_t *ht, uintptr_t key) {
  
}

```

`api/hash_table.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdbool.h>

typedef struct {
  uintptr_t key;
  uintptr_t value;
} mambo_ht_entry_t;

typedef struct {
  size_t size;
  size_t entry_count;

  int index_shift;

  bool allow_resize;
  int fill_factor;
  size_t resize_threshold;

  pthread_mutex_t lock;

  mambo_ht_entry_t *entries; 
} mambo_ht_t;

int mambo_ht_init(mambo_ht_t *ht, size_t initial_size, int index_shift, int fill_factor, bool allow_resize);
int mambo_ht_add_nolock(mambo_ht_t *ht, uintptr_t key, uintptr_t value);
int mambo_ht_add(mambo_ht_t *ht, uintptr_t key, uintptr_t value);
int mambo_ht_get_nolock(mambo_ht_t *ht, uintptr_t key, uintptr_t *value);
int mambo_ht_get(mambo_ht_t *ht, uintptr_t key, uintptr_t *value);

```

`api/helpers.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW

#include <stdio.h>
#include <assert.h>
#include <stdarg.h>
#include "../plugins.h"
#ifdef __arm__
#include "../pie/pie-thumb-encoder.h"
#elif __aarch64__
#include "../pie/pie-a64-encoder.h"
#include "../api/emit_a64.h"
#endif

#define not_implemented() \
  fprintf(stderr, "%s: Implement me\n", __PRETTY_FUNCTION__); \
  while(1);

#ifdef __arm__
void emit_thumb_push_cpsr(mambo_context *ctx, enum reg tmp_reg) {
  uint16_t *write_p = ctx->code.write_p;

  // MRS tmp_reg, CPSR
  thumb_mrs32(&write_p, tmp_reg);
  write_p += 2;

  // PUSH {tmp_reg}
  thumb_push_regs(&write_p, 1 << tmp_reg);

  ctx->code.write_p = write_p;
}

void emit_arm_push_cpsr(mambo_context *ctx, enum reg tmp_reg) {
  emit_arm_mrs(ctx, tmp_reg);
  emit_arm_push(ctx, 1 << tmp_reg);
}

void emit_thumb_pop_cpsr(mambo_context *ctx, enum reg tmp_reg) {
  uint16_t *write_p = ctx->code.write_p;

  // POP {tmp_reg}
  thumb_pop_regs(&write_p, 1 << tmp_reg);

  // MSR tmp_reg, CPSR_fs
  thumb_msr32(&write_p, tmp_reg, 3);
  write_p += 2;

  ctx->code.write_p = write_p;
}

void emit_arm_pop_cpsr(mambo_context *ctx, enum reg tmp_reg) {
  emit_arm_pop(ctx, 1 << tmp_reg);
  emit_arm_msr(ctx, tmp_reg, 3);
}

void emit_thumb_copy_to_reg_32bit(mambo_context *ctx, enum reg reg, uint32_t value) {
  if (value <= 0xFFFF) {
    copy_to_reg_16bit((uint16_t **)&ctx->code.write_p, reg, value);
  } else {
    copy_to_reg_32bit((uint16_t **)&ctx->code.write_p, reg, value);
  }
}

void emit_arm_copy_to_reg_32bit(mambo_context *ctx, enum reg reg, uint32_t value) {
  if (value <= 0xFFFF) {
    arm_copy_to_reg_16bit((uint32_t **)&ctx->code.write_p, reg, value);
  } else {
    arm_copy_to_reg_32bit((uint32_t **)&ctx->code.write_p, reg, value);
  }
}

void emit_thumb_b16_cond(void *write_p, void *target, mambo_cond cond) {
  thumb_b16_cond_helper((uint16_t *)write_p, (uint32_t)target, cond);
}

void emit_thumb_push(mambo_context *ctx, uint32_t regs) {
  ctx->code.plugin_pushed_reg_count += count_bits(regs);

  uint16_t *write_p = ctx->code.write_p;
  thumb_push_regs(&write_p, regs);
  ctx->code.write_p = write_p;
}

void emit_arm_push(mambo_context *ctx, uint32_t regs) {
  ctx->code.plugin_pushed_reg_count += count_bits(regs);

  uint32_t *write_p = ctx->code.write_p;
  arm_push_regs(regs);
  ctx->code.write_p = write_p;
}

void emit_thumb_pop(mambo_context *ctx, uint32_t regs) {
  ctx->code.plugin_pushed_reg_count -= count_bits(regs);
  assert(ctx->code.plugin_pushed_reg_count >= 0);

  uint16_t *write_p = ctx->code.write_p;
  thumb_pop_regs(&write_p, regs);
  ctx->code.write_p = write_p;
}

void emit_arm_pop(mambo_context *ctx, uint32_t regs) {
  ctx->code.plugin_pushed_reg_count -= count_bits(regs);
  assert(ctx->code.plugin_pushed_reg_count >= 0);

  uint32_t *write_p = ctx->code.write_p;
  arm_pop_regs(regs);
  ctx->code.write_p = write_p;
}

void emit_arm_fcall(mambo_context *ctx, void *function_ptr) {
  emit_arm_copy_to_reg_32bit(ctx, lr, (uint32_t)function_ptr);
  emit_arm_blx(ctx, lr);
}

void emit_thumb_fcall(mambo_context *ctx, void *function_ptr) {
  emit_thumb_copy_to_reg_32bit(ctx, lr, (uint32_t)function_ptr);
  emit_thumb_blx16(ctx, lr);
}

static inline int emit_arm_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                                         unsigned int shift_type, unsigned int shift) {
  if (shift < 0 || shift > 31 || shift_type > ROR) {
    return -1;
  }

  if (rm < 0) {
    rm = -rm;
    emit_arm_sub(ctx, REG_PROC, 0, rd, rn, rm | (shift_type << 5) | (shift << 7));
  } else {
    emit_arm_add(ctx, REG_PROC, 0, rd, rn, rm | (shift_type << 5) | (shift << 7));
  }
  return 0;
}

static inline int emit_arm_add_sub(mambo_context *ctx, int rd, int rn, int rm) {
  return emit_arm_add_sub_shift(ctx, rd, rn, rm, LSL, 0);
}

static inline int emit_thumb_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                                           unsigned int shift_type, unsigned int shift) {
  if (shift < 0 || shift > 31 || shift_type > ROR) {
    return -1;
  }
  if (rm < 0) {
    rm = -rm;
    emit_thumb_sub32(ctx, 0, rn, shift >> 2, rd, shift, shift_type, rm);
  } else {
    emit_thumb_add32(ctx, 0, rn, shift >> 2, rd, shift, shift_type, rm);
  }
  return 0;
}

static inline int emit_thumb_add_sub(mambo_context *ctx, int rd, int rn, int rm) {
  return emit_thumb_add_sub_shift(ctx, rd, rn, rm, LSL, 0);
}
#endif // __arm__

#ifdef __aarch64__
void emit_a64_push(mambo_context *ctx, uint32_t regs) {
  int reg_no = count_bits(regs);
  ctx->code.plugin_pushed_reg_count += reg_no;

  uint32_t *write_p = ctx->code.write_p;
  uint32_t to_push[2];

  if (reg_no & 1) {
    reg_no = get_highest_n_regs(regs, to_push, 1);
    assert(reg_no == 1);
    a64_push_reg(to_push[0]);
    regs &= ~(1 << to_push[0]);
  }

  while (regs != 0) {
    reg_no = get_highest_n_regs(regs, to_push, 2);
    assert(reg_no == 2);
    a64_push_pair_reg(to_push[1], to_push[0]);
    regs &= ~((1 << to_push[0]) | (1 << to_push[1]));
  }

  ctx->code.write_p = write_p;
}

void emit_a64_pop(mambo_context *ctx, uint32_t regs) {
  ctx->code.plugin_pushed_reg_count -= count_bits(regs);
  assert(ctx->code.plugin_pushed_reg_count >= 0);

  uint32_t *write_p = ctx->code.write_p;
  uint32_t to_pop[2];
  int reg_no;

  while (regs != 0) {
    reg_no = get_lowest_n_regs(regs, to_pop, 2);
    assert(reg_no == 1 || reg_no == 2);
    if (reg_no == 2) {
      a64_pop_pair_reg(to_pop[0], to_pop[1]);
      regs &= ~((1 << to_pop[0]) | (1 << to_pop[1]));
    } else if (reg_no == 1) {
      a64_pop_reg(to_pop[0]);
      regs &= ~(1 << to_pop[0]);
    }
  }

  ctx->code.write_p = write_p;
}

static inline int emit_a64_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                                         unsigned int shift_type, unsigned int shift) {
  if (shift < 0 || shift > 63 || shift_type > ASR) return -1;
  int op = (rm < 0);
  rm = abs(rm);
  emit_a64_ADD_SUB_shift_reg(ctx, 1, op, 0, shift_type, rm, shift, rn, rd);
  return 0;
}

static inline int emit_a64_add_sub(mambo_context *ctx, int rd, int rn, int rm) {
  return emit_a64_add_sub_shift(ctx, rd, rn, rm, LSL, 0);
}

int emit_a64_add_sub_ext(mambo_context *ctx, int rd, int rn, int rm, int ext_option, int shift) {
  int op = (rm < 0);
  rm = abs(rm);
  if (shift > 4 || shift < 0) return -1;
  emit_a64_ADD_SUB_ext_reg(ctx, 1, op, 0, rm, ext_option, shift, rn, rd);
  return 0;
}
#endif

void emit_push(mambo_context *ctx, uint32_t regs) {
#ifdef __arm__
  inst_set isa = mambo_get_inst_type(ctx);
  if (isa == ARM_INST) {
    emit_arm_push(ctx, regs);
  } else {
    emit_thumb_push(ctx, regs);
  }
#elif __aarch64__
  emit_a64_push(ctx, regs);
#endif
}

void emit_pop(mambo_context *ctx, uint32_t regs) {
  assert(ctx->code.plugin_pushed_reg_count >= 0);
#ifdef __arm__
  inst_set isa = mambo_get_inst_type(ctx);
  if (isa == ARM_INST) {
    emit_arm_pop(ctx, regs);
  } else {
    emit_thumb_pop(ctx, regs);
  }
#elif __aarch64__
  emit_a64_pop(ctx, regs);
#endif
}

void emit_set_reg(mambo_context *ctx, enum reg reg, uintptr_t value) {
#ifdef __arm__
  inst_set isa = mambo_get_inst_type(ctx);
  if (isa == ARM_INST) {
    emit_arm_copy_to_reg_32bit(ctx, reg, value);
  } else {
    emit_thumb_copy_to_reg_32bit(ctx, reg, value);
  }
#elif __aarch64__
  a64_copy_to_reg_64bits((uint32_t **)&ctx->code.write_p, reg, value);
#endif
}

int __emit_branch_cond(inst_set inst_type, void *write, uintptr_t target, mambo_cond cond, bool link) {
  intptr_t diff = (target & (~THUMB)) - (uintptr_t)write;
  if (cond != AL && link) return -1;
#ifdef __arm__
  switch (inst_type) {
    case THUMB_INST:
      diff -= 4;
      if (cond == AL) {
        bool to_arm = link && !(target & THUMB);
        target &= ~THUMB;
        if (diff < -16777216 || diff > 16777214) return -1;
        thumb_b_bl_helper(write, target, link, to_arm);
      } else {
        if (diff < -1048576 || diff > 1048574) return -1;
        void *write_c = write;
        thumb_b32_cond_helper((uint16_t **)&write, target, cond);
        assert((write_c + 4) == write);
      }
      break;
    case ARM_INST:
      if (target & THUMB) return -1;
      diff -= 8;
      if (diff < -33554432 || diff > 33554428) return -1;
      arm_branch_helper(write, target, link, cond);
      break;
    default:
      return -1;
  }
#endif
#ifdef __aarch64__
  if (cond == AL) {
    if (diff < -134217728 || diff > 134217724) return -1;
    a64_branch_helper(write, target, link);
  } else {
    if (diff < -1048576 || diff > 1048572) return -1;
    a64_b_cond_helper(write, target, cond);
  }
#endif
  return 0;
}

void emit_fcall(mambo_context *ctx, void *function_ptr) {
  // First try an immediate call, and if that is out of range then generate an indirect call
  int ret = __emit_branch_cond(ctx->code.inst_type, ctx->code.write_p, (uintptr_t)function_ptr, AL, true);
  if (ret == 0) return;

  emit_set_reg(ctx, lr, (uintptr_t)function_ptr);
#ifdef __arm__
  inst_set type = mambo_get_inst_type(ctx);
  if (type == ARM_INST) {
    emit_arm_blx(ctx, lr);
  } else {
    emit_thumb_blx16(ctx, lr);
  }
#elif __aarch64__
  emit_a64_BLR(ctx, lr);
#endif
}

int emit_safe_fcall(mambo_context *ctx, void *function_ptr, int argno) {
  uintptr_t to_push = (1 << lr);
#ifdef __arm__
  to_push |= (1 << r0) | (1 << r1) | (1 << r2) | (1 << r3) | (1 << r4);
#elif __aarch64__
  to_push |= 0x1FF;
#endif

  if (argno > MAX_FCALL_ARGS) return -1;
  to_push &= ~(((1 << MAX_FCALL_ARGS)-1) >> (MAX_FCALL_ARGS - argno));

  emit_push(ctx, to_push);
  emit_set_reg_ptr(ctx, MAX_FCALL_ARGS, function_ptr);
  emit_fcall(ctx, safe_fcall_trampoline);
  emit_pop(ctx, to_push);

  return 0;
}

int emit_safe_fcall_static_args(mambo_context *ctx, void *fptr, int argno, ...) {
  va_list args;
  uint32_t reglist = 0;

  if (argno > MAX_FCALL_ARGS || argno < 0) return -1;
  if (argno > 0) {
    reglist = 0xFF >> (8-argno);
    emit_push(ctx, reglist);

    va_start(args, argno);
    for (int a = 0; a < argno; a++) {
      emit_set_reg(ctx, a, va_arg(args, uintptr_t));
    }
    va_end(args);
  }

  emit_safe_fcall(ctx, fptr, argno);

  if (argno > 0) {
    emit_pop(ctx, reglist);
  }

  return 0;
}

void emit_mov(mambo_context *ctx, enum reg rd, enum reg rn) {
#ifdef __arm__
  assert(rd >= 0 && rd < pc && rn >= 0 && rn < pc);
  if (mambo_get_inst_type(ctx) == THUMB_INST) {
    emit_thumb_movh16(ctx, rd >> 3, rn, rd);
  } else {
    emit_arm_mov(ctx, REG_PROC, 0, rd, rn);
  }
#elif __aarch64__
  if (rn == sp) {
    emit_a64_ADD_SUB_immed(ctx, 1, 0, 0, 0, 0, rn, rd);
  } else {
    emit_a64_logical_reg(ctx, 1, 1, 0, 0, rn, 0, 0x1F, rd);
  }
#endif
}

#ifdef __arm__
  #define SHIFTED_ADD_SUB_I_BITS 8
  #define _emit_add_shift_imm(rd, rn, offset, shift) \
           assert((shift & 1) == 0); \
           emit_arm_add(ctx, IMM_PROC, 0, rd, rn, ((16 - (shift / 2)) << 8) | offset);
  #define _emit_sub_shift_imm(rd, rn, offset, shift) \
           assert((shift & 1) == 0); \
           emit_arm_sub(ctx, IMM_PROC, 0, rd, rn, ((16 - (shift / 2)) << 8) | offset);
#elif __aarch64__
  #define SHIFTED_ADD_SUB_I_BITS 12
  #define _emit_add_shift_imm(rd, rn, offset, shift) \
           assert((shift) == 0 || (shift) == 12); \
           emit_a64_ADD_SUB_immed(ctx, 1, 0, 0, (shift == 12), (offset), (rn), (rd));
  #define _emit_sub_shift_imm(rd, rn, offset, shift) \
           assert((shift) == 0 || (shift) == 12); \
           emit_a64_ADD_SUB_immed(ctx, 1, 1, 0, (shift == 12), (offset), (rn), (rd));
#endif
#define SHIFTED_ADD_SUB_I_MASK ((1 << SHIFTED_ADD_SUB_I_BITS) - 1)
#define SHIFTED_ADD_SUB_MAX (SHIFTED_ADD_SUB_I_MASK | (SHIFTED_ADD_SUB_I_MASK << SHIFTED_ADD_SUB_I_BITS))

int emit_add_sub_i(mambo_context *ctx, int rd, int rn, int offset) {
  if (offset == 0) {
    if (rd != rn) {
      emit_mov(ctx, rd, rn);
      return 0;
    }
  } else {
#ifdef __arm__
    inst_set isa = mambo_get_inst_type(ctx);
    if (isa == THUMB_INST) {
      if (offset > 0xFFF || offset < -0xFFF) return -1;

      if (offset < 0) {
        offset = -offset;
        emit_thumb_subwi32(ctx, offset >> 11, rn, offset >> 8, rd, offset);
      } else {
        emit_thumb_addwi32(ctx, offset >> 11, rn, offset >> 8, rd, offset);
      }
      return 0;
    }
#endif
    if (offset < -SHIFTED_ADD_SUB_MAX || offset > SHIFTED_ADD_SUB_MAX) return -1;

    if (offset < 0) {
      offset = -offset;
      if (offset & SHIFTED_ADD_SUB_I_MASK) {
        _emit_sub_shift_imm(rd, rn, offset & SHIFTED_ADD_SUB_I_MASK, 0);
        rn = rd;
      }
      if (offset & (SHIFTED_ADD_SUB_I_MASK << SHIFTED_ADD_SUB_I_BITS)) {
        _emit_sub_shift_imm(rd, rn, offset >> SHIFTED_ADD_SUB_I_BITS, SHIFTED_ADD_SUB_I_BITS);
      }
    } else {
      if (offset & SHIFTED_ADD_SUB_I_MASK) {
        _emit_add_shift_imm(rd, rn, offset & SHIFTED_ADD_SUB_I_MASK, 0);
        rn = rd;
      }
      if (offset & (SHIFTED_ADD_SUB_I_MASK << SHIFTED_ADD_SUB_I_BITS)) {
        _emit_add_shift_imm(rd, rn, offset >> SHIFTED_ADD_SUB_I_BITS, SHIFTED_ADD_SUB_I_BITS);
      }
    }
  } // offset != 0
  return 0;
}

inline int emit_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                       unsigned int shift_type, unsigned int shift) {
#ifdef __arm__
  if (mambo_get_inst_type(ctx) == THUMB_INST) {
    return emit_thumb_add_sub_shift(ctx, rd, rn, rm, shift_type, shift);
  } else {
    return emit_arm_add_sub_shift(ctx, rd, rn, rm, shift_type, shift);
  }
#elif __aarch64__
  return emit_a64_add_sub_shift(ctx, rd, rn, rm, shift_type, shift);
#endif
}

inline int emit_add_sub(mambo_context *ctx, int rd, int rn, int rm) {
  return emit_add_sub_shift(ctx, rd, rn, rm, LSL, 0);
}

int emit_branch_cond(mambo_context *ctx, void *target, mambo_cond cond) {
  void *write_p = mambo_get_cc_addr(ctx);
  int ret = __emit_branch_cond(mambo_get_inst_type(ctx), write_p, (uintptr_t)target, cond, false);
  if (ret == 0) {
    mambo_set_cc_addr(ctx, write_p + 4);
  }
  return ret;
}

int emit_branch(mambo_context *ctx, void *target) {
  return emit_branch_cond(ctx, target, AL);
}

int __emit_branch_cbz_cbnz(mambo_context *ctx, void *write_p, void *target, enum reg reg, bool is_cbz) {
  int ret = -1;
#ifdef __aarch64__
  ret = a64_cbz_cbnz_helper((uint32_t *)write_p, !is_cbz, (uint64_t)target, 1, reg);
#elif __arm__
  if (mambo_get_inst_type(ctx) == THUMB_INST) {
    ret = thumb_cbz_cbnz_helper((uint16_t *)write_p, (uint32_t)target, reg, is_cbz);
  }
#endif
  return ret;
}

int emit_branch_cbz_cbnz(mambo_context *ctx, void *target, enum reg reg, bool is_cbz) {
  void *write_p = mambo_get_cc_addr(ctx);

  int ret = __emit_branch_cbz_cbnz(ctx, write_p, target, reg, is_cbz);
  if (ret == 0) {
#ifdef __aarch64__
    mambo_set_cc_addr(ctx, write_p + 4);
#elif __arm__
    mambo_set_cc_addr(ctx, write_p + 2);
#endif
  }
  return ret;
}

int emit_branch_cbz(mambo_context *ctx, void *target, enum reg reg) {
  return emit_branch_cbz_cbnz(ctx, target, reg, true);
}

int emit_branch_cbnz(mambo_context *ctx, void *target, enum reg reg) {
  return emit_branch_cbz_cbnz(ctx, target, reg, false);
}

int __mambo_reserve(mambo_context *ctx, mambo_branch *br, size_t incr) {
  if (ctx->code.write_p) {
    br->loc = ctx->code.write_p;
    ctx->code.write_p += incr;
    return 0;
  }
  return -1;
}

int mambo_reserve_branch(mambo_context *ctx, mambo_branch *br) {
  return __mambo_reserve(ctx, br, 4);
}

int mambo_reserve_branch_cbz(mambo_context *ctx, mambo_branch *br) {
#ifdef __arm__
  if (mambo_get_inst_type(ctx) == THUMB_INST) {
    return __mambo_reserve(ctx, br, 2);
  }
  return -1;
#endif
  return __mambo_reserve(ctx, br, 4);
}

int __emit_local_branch(mambo_context *ctx, mambo_branch *br, mambo_cond cond, bool link) {
  uintptr_t target = (uintptr_t)mambo_get_cc_addr(ctx);
#ifdef __arm__
  if (ctx->code.inst_type == THUMB_INST) {
    target |= THUMB;
  }
#endif
  return __emit_branch_cond(mambo_get_inst_type(ctx), br->loc, target, cond, link);
}

int emit_local_branch_cond(mambo_context *ctx, mambo_branch *br, mambo_cond cond) {
  return __emit_local_branch(ctx, br, cond, false);
}

int emit_local_branch(mambo_context *ctx, mambo_branch *br) {
  return __emit_local_branch(ctx, br, AL, false);
}

int emit_local_fcall(mambo_context *ctx, mambo_branch *br) {
  return __emit_local_branch(ctx, br, AL, true);
}

int emit_local_branch_cbz_cbnz(mambo_context *ctx, mambo_branch *br, enum reg reg, bool is_cbz) {
  return __emit_branch_cbz_cbnz(ctx, br->loc, mambo_get_cc_addr(ctx), reg, is_cbz);
}

int emit_local_branch_cbz(mambo_context *ctx, mambo_branch *br, enum reg reg) {
  return emit_local_branch_cbz_cbnz(ctx, br, reg, true);
}

int emit_local_branch_cbnz(mambo_context *ctx, mambo_branch *br, enum reg reg) {
  return emit_local_branch_cbz_cbnz(ctx, br, reg, false);
}

void emit_counter64_incr(mambo_context *ctx, void *counter, unsigned incr) {
#ifdef __arm__
  /* On AArch32 we use NEON rather than ADD and ADC to avoid having to save
     and restore the PSR register, which is slow.

     VPUSH {D0, D1}
     PUSH {R0}

     MOV{W,T} R0, counter
     VLDR D1, [R0]
     VMOV.I32 D0, #incr
     VSHR.U64 D0, D0, #32
     VADD.I64 D0, D1, D0
     VSTR D0, [R0]

     POP {R0}
     VPOP {D0, D1}
  */
  assert(incr <= 255);

  switch(mambo_get_inst_type(ctx)) {
    case THUMB_INST: {
      emit_thumb_vfp_vpush(ctx, 1, 0, 0, 4);
      emit_thumb_push(ctx, 1 << r0);

      emit_thumb_copy_to_reg_32bit(ctx, r0, (uintptr_t)counter);
      emit_thumb_vfp_vldr_dp(ctx, 1, r0, 0, 1, 0);
      emit_thumb_neon_vmovi(ctx, 0, 0, 0, 0, 0, incr >> 7, incr >> 4, incr);
      emit_thumb_neon_vshr(ctx, 1, 0, 0, 0, 0, 0, 1, 32);
      emit_thumb_neon_vadd_i(ctx, 3, 0, 0, 0, 0, 1, 0, 0);
      emit_thumb_vfp_vstr_dp(ctx, 1, 0, r0, 0, 0);

      emit_thumb_pop(ctx, 1 << r0);
      emit_thumb_vfp_vpop(ctx, 1, 0, 0, 4);
      break;
    }

    case ARM_INST:
      emit_arm_vfp_vpush_dp(ctx, 0, 0, 4);
      emit_arm_push(ctx, (1 << r0));

      emit_arm_copy_to_reg_32bit(ctx, r0, (uintptr_t)counter);
      emit_arm_vfp_vldr_dp(ctx, 1, 0, r0, 1, 0);
      emit_arm_neon_vmovi(ctx, 0, 0, 0, 0, 0, incr >> 7, incr >> 4, incr);
      emit_arm_neon_vshr(ctx, 1, 0, 0, 0, 0, 0, 1, 32);
      emit_arm_neon_vadd_i(ctx, 3, 0, 0, 0, 0, 1, 0, 0);
      emit_arm_vfp_vstr_dp(ctx, 1, 0, r0, 0, 0);

      emit_arm_pop(ctx, (1 << r0));
      emit_arm_vfp_vpop_dp(ctx, 0, 0, 4);
      break;
  }
#endif
#ifdef __aarch64__
  assert(incr <= 0xFFF);
  emit_a64_push(ctx, (1 << x0) | (1 << x1));
  a64_copy_to_reg_64bits((uint32_t **)&ctx->code.write_p, x0, (uintptr_t)counter);
  emit_a64_LDR_STR_unsigned_immed(ctx, 3, 0, 1, 0, x0, x1);
  emit_a64_ADD_SUB_immed(ctx, 1, 0, 0, 0, incr, x1, x1);
  emit_a64_LDR_STR_unsigned_immed(ctx, 3, 0, 0, 0, x0, x1);
  emit_a64_pop(ctx, (1 << x0) | (1 << x1));
#endif
}

int emit_indirect_branch_by_spc(mambo_context *ctx, enum reg reg) {
#ifdef __aarch64__
  // Uses fragment id 0 to prevent the dispatcher from attempting linking on an IHL miss
  a64_inline_hash_lookup(current_thread, 0, (uint32_t **)&ctx->code.write_p, ctx->code.read_address, reg, false, false);
#else
  switch(ctx->code.inst_type) {
    case ARM_INST:
      emit_push(ctx, (1 << r4) | (1 << 5) | (1 << 6));
      arm_inline_hash_lookup(current_thread, (uint32_t **)&ctx->code.write_p, 0, reg);
      break;
    case THUMB_INST: {
      uint16_t *write_p = (uint16_t *)ctx->code.write_p;
      if (reg != r5 && reg != r6) {
        thumb_push16(&write_p, (1 << r5) | (1 << r6));
      } else {
        thumb_push16(&write_p, (1 << r4) | (1 << r5) | (1 << r6));
        write_p++;
        thumb_movh16(&write_p, 0, reg, r5);
        reg = -1;
      }
      write_p++;

      thumb_inline_hash_lookup(current_thread, &write_p, 0, reg);
      ctx->code.write_p = write_p;
      break;
    }
    default:
      assert(0);
  }
#endif
}
#endif

```

`api/helpers.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifndef __API_HELPERS_H__
#define __API_HELPERS_H__

typedef struct {
  void *loc;
} mambo_branch;

#define LSL 0
#define LSR 1
#define ASR 2

#ifdef __arm__
  #define MAX_FCALL_ARGS 4
#elif __aarch64__
  #define MAX_FCALL_ARGS 8
#endif

void emit_counter64_incr(mambo_context *ctx, void *counter, unsigned incr);
void emit_push(mambo_context *ctx, uint32_t regs);
void emit_pop(mambo_context *ctx, uint32_t regs);
void emit_set_reg(mambo_context *ctx, enum reg reg, uintptr_t value);
void emit_fcall(mambo_context *ctx, void *function_ptr);
int emit_safe_fcall(mambo_context *ctx, void *function_ptr, int argno);
int emit_safe_fcall_static_args(mambo_context *ctx, void *fptr, int argno, ...);
int emit_indirect_branch_by_spc(mambo_context *ctx, enum reg reg);

void emit_mov(mambo_context *ctx, enum reg rd, enum reg rn);
int emit_add_sub_i(mambo_context *ctx, int rd, int rn, int offset);
int emit_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                       unsigned int shift_type, unsigned int shift);
int emit_add_sub(mambo_context *ctx, int rd, int rn, int rm);
int mambo_calc_ld_st_addr(mambo_context *ctx, enum reg reg);

int emit_branch(mambo_context *ctx, void *target);
int emit_branch_cond(mambo_context *ctx, void *target, mambo_cond cond);
int emit_branch_cbz_cbnz(mambo_context *ctx, void *target, enum reg reg, bool is_cbz);
int emit_branch_cbz(mambo_context *ctx, void *target, enum reg reg);
int emit_branch_cbnz(mambo_context *ctx, void *target, enum reg reg);

int mambo_reserve_branch(mambo_context *ctx, mambo_branch *br);
int mambo_reserve_branch_cbz(mambo_context *ctx, mambo_branch *br);

int emit_local_branch_cond(mambo_context *ctx, mambo_branch *br, mambo_cond cond);
int emit_local_branch(mambo_context *ctx, mambo_branch *br);
int emit_local_fcall(mambo_context *ctx, mambo_branch *br);
int emit_local_branch_cbz_cbnz(mambo_context *ctx, mambo_branch *br, enum reg reg, bool is_cbz);
int emit_local_branch_cbz(mambo_context *ctx, mambo_branch *br, enum reg reg);
int emit_local_branch_cbnz(mambo_context *ctx, mambo_branch *br, enum reg reg);

static inline void emit_set_reg_ptr(mambo_context *ctx, enum reg reg, void *ptr) {
  emit_set_reg(ctx, reg, (uintptr_t)ptr);
}

#ifdef __arm__
#define ROR 3
void emit_thumb_push_cpsr(mambo_context *ctx, enum reg reg);
void emit_arm_push_cpsr(mambo_context *ctx, enum reg reg);
void emit_thumb_pop_cpsr(mambo_context *ctx, enum reg reg);
void emit_arm_pop_cpsr(mambo_context *ctx, enum reg reg);
void emit_thumb_copy_to_reg_32bit(mambo_context *ctx, enum reg reg, uint32_t value);
void emit_arm_copy_to_reg_32bit(mambo_context *ctx, enum reg reg, uint32_t value);
void emit_thumb_b16_cond(void *write_p, void *target, mambo_cond cond);
void emit_thumb_push(mambo_context *ctx, uint32_t regs);
void emit_arm_push(mambo_context *ctx, uint32_t regs);
void emit_thumb_pop(mambo_context *ctx, uint32_t regs);
void emit_arm_pop(mambo_context *ctx, uint32_t regs);
void emit_thumb_fcall(mambo_context *ctx, void *function_ptr);
void emit_arm_fcall(mambo_context *ctx, void *function_ptr);
static inline int emit_arm_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                                         unsigned int shift_type, unsigned int shift);
static inline int emit_thumb_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                                           unsigned int shift_type, unsigned int shift);
static inline int emit_arm_add_sub(mambo_context *ctx, int rd, int rn, int rm);
static inline int emit_thumb_add_sub(mambo_context *ctx, int rd, int rn, int rm);
#endif

#ifdef __aarch64__
void emit_a64_push(mambo_context *ctx, uint32_t regs);
void emit_a64_pop(mambo_context *ctx, uint32_t regs);
static inline int emit_a64_add_sub_shift(mambo_context *ctx, int rd, int rn, int rm,
                                   unsigned int shift_type, unsigned int shift);
static inline int emit_a64_add_sub(mambo_context *ctx, int rd, int rn, int rm);
int emit_a64_add_sub_ext(mambo_context *ctx, int rd, int rn, int rm, int ext_option, int shift);
#endif

#endif

```

`api/internal.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <assert.h>

#include "../dbm.h"
#include "../plugins.h"

/* Helpers used internally by MAMBO to implement the API */

#ifdef PLUGINS_NEW
void set_mambo_context(mambo_context *ctx, dbm_thread *thread_data, mambo_cb_idx event_type) {
  ctx->thread_data = thread_data;
  ctx->event_type = event_type;
}

void set_mambo_context_code(mambo_context *ctx, dbm_thread *thread_data, mambo_cb_idx event_type,
                            cc_type fragment_type, int fragment_id, inst_set inst_type, int inst,
                            mambo_cond cond, void *read_address, void *write_p, void *data_p, bool *stop) {
  set_mambo_context(ctx, thread_data, event_type);
  ctx->code.inst_type = inst_type;
  ctx->code.fragment_type = fragment_type;
  ctx->code.fragment_id = fragment_id;
  ctx->code.inst = inst;
  ctx->code.cond = cond;
  ctx->code.read_address = read_address;
  ctx->code.write_p = write_p;
  ctx->code.data_p = data_p;
  ctx->code.replace = false;
  ctx->code.pushed_regs = 0;
  ctx->code.available_regs = 0;
  ctx->code.plugin_pushed_reg_count = 0;
  ctx->code.stop = stop;
}

void set_mambo_context_syscall(mambo_context *ctx, dbm_thread *thread_data, mambo_cb_idx event_type,
                               uintptr_t number, uintptr_t *regs) {
  set_mambo_context(ctx, thread_data, event_type);
  ctx->syscall.number = number;
  ctx->syscall.regs = regs;
  ctx->syscall.replace = false;
}
#endif

void mambo_deliver_callbacks_for_ctx(mambo_context *ctx) {
#ifdef PLUGINS_NEW
  unsigned cb_id = ctx->event_type;
  assert(cb_id < CALLBACK_MAX_IDX);

  for (int i = 0; i < global_data.free_plugin; i++) {
    if (global_data.plugins[i].cbs[cb_id] != NULL) {
      ctx->plugin_id = i;
      global_data.plugins[i].cbs[cb_id](ctx);
    } // if
  } // for
#endif
}

void mambo_deliver_callbacks(unsigned cb_id, dbm_thread *thread_data) {
#ifdef PLUGINS_NEW
  mambo_context ctx;

  if (global_data.free_plugin > 0) {
    set_mambo_context(&ctx, thread_data, cb_id);
    mambo_deliver_callbacks_for_ctx(&ctx);
  }
#endif
}

void mambo_deliver_callbacks_code(unsigned cb_id, dbm_thread *thread_data, cc_type fragment_type,
                                  int fragment_id, inst_set inst_type, int inst, mambo_cond cond,
                                  void *read_address, void *write_p, void *data_p, bool *stop) {
#ifdef PLUGINS_NEW
  mambo_context ctx;

  if (global_data.free_plugin > 0) {
    set_mambo_context_code(&ctx, thread_data, cb_id, fragment_type, fragment_id,
                           inst_type, inst, cond, read_address, write_p, data_p, stop);
    mambo_deliver_callbacks_for_ctx(&ctx);
  }
#endif
}

void _function_callback_wrapper(mambo_context *ctx, watched_func_t *func) {
#ifdef PLUGINS_NEW
  ctx->plugin_id = func->plugin_id;
  ctx->code.available_regs = ctx->code.pushed_regs;
  ctx->code.func_name = func->name;

  if (func->post_callback != NULL) {
    emit_push(ctx, (1 << es) | (1 << lr));
  }
  if (func->pre_callback != NULL) {
    ctx->event_type = PRE_FN_C;
    func->pre_callback(ctx);
  }
  if (func->post_callback != NULL) {
    mambo_branch fcall;
    int ret = mambo_reserve_branch(ctx, &fcall);
    assert(ret == 0);

    ret = mambo_add_identity_mapping(ctx);
    assert(ret == 0);

    // Hack; pop the registers pushed by IHL routines
    ctx->code.plugin_pushed_reg_count += 2;
#ifdef __aarch64__
    emit_pop(ctx, (1 << x0) | (1 << x1));
#elif __arm__
    emit_pop(ctx, (1 << r5) | (1 << r6));
#endif

    ctx->event_type = POST_FN_C;
    func->post_callback(ctx);

    emit_pop(ctx, (1 << es) | (1 << lr));
    // IHL(LR) - emulated return to the caller of malloc()
    emit_indirect_branch_by_spc(ctx, lr);
    emit_local_fcall(ctx, &fcall);
  }
#endif
}

```

`api/load_store.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2014-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW

#include <stdio.h>
#include <assert.h>

#include "../plugins.h"
#ifdef __arm__
  #include "../pie/pie-thumb-field-decoder.h"
  #include "../pie/pie-arm-field-decoder.h"
#elif __aarch64__
  #include "../pie/pie-a64-field-decoder.h"
  #include "../pie/pie-a64-decoder.h"
#endif

#ifdef __arm__
  #define read_addr_to_pc(addr) (((uint32_t)addr + 4) & 0xFFFFFFFC)
#endif

#ifdef __aarch64__
#define IS_LOAD (1 << 22)

void _a64_is_load_or_store(mambo_context *ctx, bool *is_load, bool *is_store) {
  *is_load = false;
  *is_store = false;

  uint32_t *inst = (uint32_t *)ctx->code.read_address;

  switch (ctx->code.inst) {
    case A64_LDR_LIT: {
      uint32_t opc, v, imm19, rt;
      a64_LDR_lit_decode_fields(ctx->code.read_address, &opc, &v, &imm19, &rt);
      // !PRFM
      if (opc == 3 && v == 0) break;

      *is_load = true;
      break;
    }
    case A64_LDX_STX:
    case A64_LDP_STP:
    case A64_LDX_STX_MULTIPLE:
    case A64_LDX_STX_MULTIPLE_POST:
    case A64_LDX_STX_SINGLE:
    case A64_LDX_STX_SINGLE_POST:
      if (*inst & IS_LOAD) {
        *is_load = true;
      } else {
        *is_store = true;
      }
      break;
    case A64_LDR_STR_IMMED:
    case A64_LDR_STR_REG:
    case A64_LDR_STR_UNSIGNED_IMMED: {
      uint32_t sz, v, opc, imm12, rn, rt;
      a64_LDR_STR_unsigned_immed_decode_fields(ctx->code.read_address, &sz, &v, &opc, &imm12, &rn, &rt);
      // !PRFM - the sz, v, and opc fields are identical between the three encodings
      if (sz == 3 && v == 0 && opc == 2) break;

      if ((*inst >> 22) & 3) {
        *is_load = true;
      } else {
        *is_store = true;
      }
      break;
    }
    case A64_LDADD:
    case A64_LDCLR:
    case A64_LDEOR:
    case A64_LDSET:
    case A64_SWP:
      *is_load = true;
      *is_store = true;
      break;
  }
}
#endif

bool mambo_is_load(mambo_context *ctx) {
  if (ctx->code.inst == -1) return false;
#ifdef __arm__
  if (ctx->code.inst_type == THUMB_INST) {
    switch(ctx->code.inst) {
      case THUMB_LDMFD16:
      case THUMB_LDR16:
      case THUMB_LDRB16:
      case THUMB_LDRBI16:
      case THUMB_LDRH16:
      case THUMB_LDRHI16:
      case THUMB_LDRI16:
      case THUMB_LDR_PC_16:
      case THUMB_LDRSB16:
      case THUMB_LDRSH16:
      case THUMB_LDR_SP16:
      case THUMB_POP16:
      case THUMB_LDC232:
      case THUMB_LDC32:
      case THUMB_LDMEA32:
      case THUMB_LDMFD32:
      case THUMB_LDR32:
      case THUMB_LDRB32:
      case THUMB_LDRBI32:
      case THUMB_LDRBL32:
      case THUMB_LDRBT32:
      case THUMB_LDRBWI32:
      case THUMB_LDRD32:
      case THUMB_LDREX32:
      case THUMB_LDREXB32:
      case THUMB_LDREXD32:
      case THUMB_LDREXH32:
      case THUMB_LDRH32:
      case THUMB_LDRHI32:
      case THUMB_LDRHL32:
      case THUMB_LDRHT32:
      case THUMB_LDRHWI32:
      case THUMB_LDRI32:
      case THUMB_LDRL32:
      case THUMB_LDRSB32:
      case THUMB_LDRSBI32:
      case THUMB_LDRSBL32:
      case THUMB_LDRSBT32:
      case THUMB_LDRSBWI32:
      case THUMB_LDRSH32:
      case THUMB_LDRSHI32:
      case THUMB_LDRSHL32:
      case THUMB_LDRSHT32:
      case THUMB_LDRSHWI32:
      case THUMB_LDRT32:
      case THUMB_LDRWI32:
      case THUMB_NEON_VLDX_M:
      case THUMB_NEON_VLDX_S_O:
      case THUMB_NEON_VLDX_S_A:
      case THUMB_VFP_VLDM_DP:
      case THUMB_VFP_VLDM_SP:
      case THUMB_VFP_VLDR_DP:
      case THUMB_VFP_VLDR_SP:
      case THUMB_VFP_VPOP:
        return true;
    }
  } else if (ctx->code.inst_type == ARM_INST) {
    switch(ctx->code.inst) {
      case ARM_LDC:
      case ARM_LDM:
      case ARM_LDR:
      case ARM_LDRB:
      case ARM_LDRBT:
      case ARM_LDRD:
      case ARM_LDREX:
      case ARM_LDREXB:
      case ARM_LDREXD:
      case ARM_LDREXH:
      case ARM_LDRH:
      case ARM_LDRHT:
      case ARM_LDRSB:
      case ARM_LDRSBT:
      case ARM_LDRSH:
      case ARM_LDRSHT:
      case ARM_LDRT:
      case ARM_NEON_VLDX_M:
      case ARM_NEON_VLDX_S_O:
      case ARM_NEON_VLDX_S_A:
      case ARM_VFP_VLDM_DP:
      case ARM_VFP_VLDM_SP:
      case ARM_VFP_VLDR_DP:
      case ARM_VFP_VLDR_SP:
      case ARM_VFP_VPOP_DP:
      case ARM_VFP_VPOP_SP:
        return true;
    }
  }
#elif __aarch64__
  bool is_load, is_store;
  _a64_is_load_or_store(ctx, &is_load, &is_store);
  return is_load;
#endif
  return false;
}

bool mambo_is_store(mambo_context *ctx) {
  if (ctx->code.inst == -1) return false;
#ifdef __arm__
  if (ctx->code.inst_type == THUMB_INST) {
    switch(ctx->code.inst) {
      case THUMB_STMEA16:
      case THUMB_STR16:
      case THUMB_STRB16:
      case THUMB_STRBI16:
      case THUMB_STRH16:
      case THUMB_STRHI16:
      case THUMB_STRI16:
      case THUMB_STR_SP16:
      case THUMB_PUSH16:
      case THUMB_STC32:
      case THUMB_STC232:
      case THUMB_STMEA32:
      case THUMB_STMFD32:
      case THUMB_STR32:
      case THUMB_STRB32:
      case THUMB_STRBI32:
      case THUMB_STRBT32:
      case THUMB_STRBWI32:
      case THUMB_STRD32:
      case THUMB_STREX32:
      case THUMB_STREXB32:
      case THUMB_STREXD32:
      case THUMB_STREXH32:
      case THUMB_STRH32:
      case THUMB_STRHI32:
      case THUMB_STRHT32:
      case THUMB_STRHWI32:
      case THUMB_STRI32:
      case THUMB_STRT32:
      case THUMB_STRWI32:
      case THUMB_NEON_VSTX_M:
      case THUMB_NEON_VSTX_S_O:
      case THUMB_VFP_VSTM_DP:
      case THUMB_VFP_VSTM_SP:
      case THUMB_VFP_VSTR_DP:
      case THUMB_VFP_VSTR_SP:
      case THUMB_VFP_VPUSH:
        return true;
    }
  } else if (ctx->code.inst_type == ARM_INST) {
    switch(ctx->code.inst) {
      case ARM_STC:
      case ARM_STM:
      case ARM_STR:
      case ARM_STRB:
      case ARM_STRBT:
      case ARM_STRD:
      case ARM_STREX:
      case ARM_STREXB:
      case ARM_STREXD:
      case ARM_STREXH:
      case ARM_STRH:
      case ARM_STRHT:
      case ARM_STRT:
      case ARM_NEON_VSTX_M:
      case ARM_NEON_VSTX_S_O:
      case ARM_VFP_VSTM_DP:
      case ARM_VFP_VSTM_SP:
      case ARM_VFP_VSTR_DP:
      case ARM_VFP_VSTR_SP:
      case ARM_VFP_VPUSH_DP:
      case ARM_VFP_VPUSH_SP:
        return true;
    }
  }
#elif __aarch64__
  bool is_load, is_store;
  _a64_is_load_or_store(ctx, &is_load, &is_store);
  return is_store;
#endif
  return false;
}

bool mambo_is_load_or_store(mambo_context *ctx) {
#ifdef __arm__
  return mambo_is_load(ctx) || mambo_is_store(ctx);
#elif __aarch64__
  bool is_load, is_store;
  _a64_is_load_or_store(ctx, &is_load, &is_store);
  return is_load || is_store;
#endif
}

void _generate_addr(mambo_context *ctx, int reg, int rn, int rm, int offset) {
#ifdef __arm__
  enum reg rtmp = reg_invalid;

  assert(rm != pc && rm != sp);
#elif __aarch64__
  assert(rm != sp);
#endif
  int apply_offset = 0;
  if (rn == sp) {
    apply_offset = ctx->code.plugin_pushed_reg_count;
    apply_offset *= sizeof(uintptr_t);
  }

#ifdef __arm__
  if (rn == pc) {
    uint32_t addr = read_addr_to_pc(ctx->code.read_address);
    if (rm <= -reg_invalid || rm >= reg_invalid) {
      addr += offset;
      offset = 0;
    }
    int rtpc = reg;
    if (reg == rm) {
      rtmp = (reg == 0) ? 1 : 0;
      emit_push(ctx, 1 << rtmp);
      rtpc = rtmp;
    }
    emit_set_reg(ctx, rtpc, addr);
    rn = rtpc;
  }
#endif

  if (rm <= -reg_invalid || rm >= reg_invalid) {
    offset += apply_offset;
    emit_add_sub_i(ctx, reg, rn, offset);
  } else {
#ifdef __arm__
    emit_add_sub_shift(ctx, reg, rn, rm, offset & 3, offset >> 2);
#elif __aarch64__
    emit_a64_add_sub_ext(ctx, reg, rn, rm, offset & 7, offset >> 3);
#endif
    if (apply_offset != 0) {
      assert(apply_offset <= 0xFFF && apply_offset > 0);
      emit_add_sub_i(ctx, reg, reg, apply_offset);
    }
  }

#ifdef __arm__
  if (rtmp != reg_invalid) {
    emit_pop(ctx, 1 << rtmp);
  }
#endif
}

#ifdef __arm__
int _thumb_calc_ld_st_addr(mambo_context *ctx, enum reg reg) {
  switch(ctx->code.inst) {
    case THUMB_LDMFD16:
    case THUMB_STMEA16: {
      uint32_t rn, reglist;
      thumb_stmea16_decode_fields(ctx->code.read_address, &rn, &reglist);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }

    case THUMB_LDR16:
    case THUMB_LDRB16:
    case THUMB_LDRH16:
    case THUMB_LDRSB16:
    case THUMB_LDRSH16:
    case THUMB_STR16:
    case THUMB_STRB16:
    case THUMB_STRH16: {
      uint32_t rm, rn, rt;
      thumb_ldr16_decode_fields(ctx->code.read_address, &rm, &rn, &rt);
      _generate_addr(ctx, reg, rn, rm, 0);
      return 0;
    }

    case THUMB_LDRI16:
    case THUMB_LDRBI16:
    case THUMB_LDRHI16:
    case THUMB_STRI16:
    case THUMB_STRBI16:
    case THUMB_STRHI16: {
      uint32_t imm5, rn, rd;
      thumb_ldri16_decode_fields(ctx->code.read_address, &imm5, &rn, &rd);
      if (ctx->code.inst == THUMB_LDRI16 || ctx->code.inst == THUMB_STRI16) {
        imm5 <<= 2;
      } else if (ctx->code.inst == THUMB_LDRHI16 || ctx->code.inst == THUMB_STRHI16) {
        imm5 <<= 1;
      }
      _generate_addr(ctx, reg, rn, reg_invalid, imm5);
      return 0;
    }

    case THUMB_LDR_PC_16: {
      uint32_t rd, imm8, addr;
      thumb_ldr_pc_16_decode_fields(ctx->code.read_address, &rd, &imm8);
      addr = (uint32_t)ctx->code.read_address + 4;
      addr &= ~3;
      addr += imm8 << 2;
      emit_thumb_copy_to_reg_32bit(ctx, reg, addr);
      return 0;
    }

    case THUMB_LDR_SP16:
    case THUMB_STR_SP16: {
      uint32_t rd, imm8;
      thumb_ldr_sp16_decode_fields(ctx->code.read_address, &rd, &imm8);
      imm8 <<= 2;
      _generate_addr(ctx, reg, sp, reg_invalid, imm8);
      return 0;
    }

    case THUMB_POP16:
    case THUMB_VFP_VPOP: {
      _generate_addr(ctx, reg, sp, reg_invalid, 0);
      return 0;
    }

    case THUMB_PUSH16: {
      uint32_t regs, offset;
      thumb_push16_decode_fields(ctx->code.read_address, &regs);
      offset = count_bits(regs) << 2;
      _generate_addr(ctx, reg, sp, reg_invalid, -offset);
      return 0;
    }

    case THUMB_LDMEA32:
    case THUMB_LDMFD32:
    case THUMB_STMEA32:
    case THUMB_STMFD32: {
      uint32_t w, rn, regs, offset = 0;
      thumb_ldmea32_decode_fields(ctx->code.read_address, &w, &rn, &regs);
      if (ctx->code.inst == THUMB_LDMEA32 || ctx->code.inst == THUMB_STMFD32) {
        offset = count_bits(regs) << 2;
      }
      _generate_addr(ctx, reg, rn, reg_invalid, -offset);
      return 0;
    }

    case THUMB_LDR32:
    case THUMB_LDRB32:
    case THUMB_LDRH32:
    case THUMB_LDRSB32:
    case THUMB_LDRSH32:
    case THUMB_STR32:
    case THUMB_STRB32:
    case THUMB_STRH32: {
      uint32_t rn, rd, shift, rm;
      thumb_ldr32_decode_fields(ctx->code.read_address, &rn, &rd, &shift, &rm);
      _generate_addr(ctx, reg, rn, rm, LSL | (shift << 2));
      return 0;
    }

    case THUMB_LDRI32:
    case THUMB_LDRBI32:
    case THUMB_LDRHI32:
    case THUMB_LDRSBI32:
    case THUMB_LDRSHI32:
    case THUMB_LDRT32:
    case THUMB_LDRBT32:
    case THUMB_LDRHT32:
    case THUMB_LDRSBT32:
    case THUMB_LDRSHT32:
    case THUMB_STRI32:
    case THUMB_STRBI32:
    case THUMB_STRHI32:
    case THUMB_STRT32:
    case THUMB_STRBT32:
    case THUMB_STRHT32: {
      uint32_t rn, rd, imm8, p, u, w;
      thumb_ldri32_decode_fields(ctx->code.read_address, &rd, &rn, &imm8, &p, &u, &w);
      if (u == 0) {
        imm8 = -imm8;
      }
      _generate_addr(ctx, reg, rn, reg_invalid, p ? imm8 : 0);
      return 0;
    }

    case THUMB_LDRWI32:
    case THUMB_LDRBWI32:
    case THUMB_LDRHWI32:
    case THUMB_LDRSBWI32:
    case THUMB_LDRSHWI32:
    case THUMB_STRWI32:
    case THUMB_STRBWI32:
    case THUMB_STRHWI32: {
      uint32_t rd, rn, imm12;
      thumb_ldrwi32_decode_fields(ctx->code.read_address, &rd, &rn, &imm12);
      _generate_addr(ctx, reg, rn, reg_invalid, imm12);
      return 0;
    }

    case THUMB_LDRD32:
    case THUMB_STRD32: {
      uint32_t p, u, w, rn, rt, rt2, imm8;
      thumb_ldrd32_decode_fields(ctx->code.read_address, &p, &u, &w, &rn, &rt, &rt2, &imm8);
      imm8 <<= 2;
      if (u == 0) {
        imm8 = -imm8;
      }
      _generate_addr(ctx, reg, rn, reg_invalid, p ? imm8 : 0);
      return 0;
    }

    case THUMB_LDRBL32:
    case THUMB_LDRHL32:
    case THUMB_LDRL32:
    case THUMB_LDRSBL32:
    case THUMB_LDRSHL32: {
      uint32_t rt, imm12, u, addr;
      thumb_ldrl32_decode_fields(ctx->code.read_address, &rt, &imm12, &u);
      addr = read_addr_to_pc(ctx->code.read_address) + (u ? imm12 : -imm12);
      emit_thumb_copy_to_reg_32bit(ctx, reg, addr);
      return 0;
    }

    case THUMB_VFP_VPUSH: {
      uint32_t size, d, vd, regs;
      thumb_vfp_vpush_decode_fields(ctx->code.read_address, &size, &d, &vd, &regs);
      _generate_addr(ctx, reg, sp, reg_invalid, -4 * regs);
      return 0;
    }

    case THUMB_STREX32:
    case THUMB_LDREX32: {
      uint32_t rn, rt, rd, imm8;
      thumb_strex32_decode_fields(ctx->code.read_address, &rn, &rt, &rd, &imm8);
      _generate_addr(ctx, reg, rn, reg_invalid, imm8 << 2);
      return 0;
    }

    case THUMB_VFP_VLDM_SP:
    case THUMB_VFP_VLDM_DP:
    case THUMB_VFP_VSTM_SP:
    case THUMB_VFP_VSTM_DP: {
      uint32_t p, u, w, rn, d, vd, imm8;
      thumb_vfp_vstm_dp_decode_fields(ctx->code.read_address, &p, &u, &w, &rn, &d, &vd, &imm8);
      assert(p != u);
      int offset = 0;
	    if (u == 0) {
	      offset = -count_bits(imm8) * 4;
	    }
	    _generate_addr(ctx, reg, rn, reg_invalid, offset);
      return 0;
    }

    case THUMB_VFP_VLDR_SP:
    case THUMB_VFP_VLDR_DP:
    case THUMB_VFP_VSTR_SP:
    case THUMB_VFP_VSTR_DP: {
      uint32_t u, rn, d, vd, imm8;
      thumb_vfp_vstr_sp_decode_fields(ctx->code.read_address, &u, &rn, &d, &vd, &imm8);
      if (u == 0) {
        imm8 = -imm8;
      }
      _generate_addr(ctx, reg, rn, reg_invalid, imm8 << 2);
      return 0;
    }

    case THUMB_LDREXB32:
    case THUMB_LDREXD32:
    case THUMB_LDREXH32:
    case THUMB_STREXB32:
    case THUMB_STREXD32:
    case THUMB_STREXH32: {
      uint32_t rn, rt;
      thumb_ldrexb32_decode_fields(ctx->code.read_address, &rn, &rt);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }

    case THUMB_NEON_VLDX_M:
    case THUMB_NEON_VLDX_S_O:
    case THUMB_NEON_VLDX_S_A:
    case THUMB_NEON_VSTX_M:
    case THUMB_NEON_VSTX_S_O: {
      uint32_t opcode, size, d, vd, rn, align, rm;
      // rm only used for post-incrementing
      thumb_neon_vldx_m_decode_fields(ctx->code.read_address, &opcode, &size, &d, &vd, &rn, &align, &rm);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }

    case THUMB_LDC32:
    case THUMB_LDC232:
    case THUMB_STC32:
    case THUMB_STC232:
      fprintf(stderr, "Address decoding for T32 instruction %d not implemented yet\n", ctx->code.inst);
      assert(0);
      break;
  }
  return -1;
}

void _decode_arm(bool is_imm, uint32_t p, uint32_t u, uint32_t op2, uint32_t *rm, int *imm) {
  *rm = reg_invalid;

  if (p) {
    if (is_imm) {
      if (u) {
        *imm = op2;
      } else {
        *imm = -op2;
      }
    } else {
      *rm = op2 & 0xF;
      *imm = op2 >> 5;
      if (u == 0) {
        *rm = -(*rm);
      }
    }
  } else {
    *imm = 0;
  }
}

int _arm_calc_ld_st_addr(mambo_context *ctx, enum reg reg) {
  switch(ctx->code.inst) {
    case ARM_LDR:
    case ARM_STR:
    case ARM_LDRB:
    case ARM_STRB:
    case ARM_LDRT:
    case ARM_STRT:
    case ARM_LDRBT:
    case ARM_STRBT: {
      uint32_t i, rd, rn, operand2, p, u, w;
      int rm, imm;
      arm_str_decode_fields(ctx->code.read_address, &i, &rd, &rn, &operand2, &p, &u, &w);
      _decode_arm(i == IMM_LDR, p, u, operand2, &rm, &imm);
      _generate_addr(ctx, reg, rn, rm, imm);
      return 0;
    }

    case ARM_LDRD:
    case ARM_STRD:
    case ARM_LDRH:
    case ARM_STRH:
    case ARM_LDRSB:
    case ARM_LDRSH:
    case ARM_LDRHT:
    case ARM_STRHT:
    case ARM_LDRSBT:
    case ARM_LDRSHT: {
      uint32_t i, rd, rn, rm_imm4l, imm4h, p, u, w;
      int rm, imm;
      arm_ldrd_decode_fields(ctx->code.read_address, &i, &rd, &rn, &rm_imm4l, &imm4h, &p, &u, &w);
      _decode_arm(i, p, u, (imm4h << 4) | rm_imm4l, &rm, &imm);
      if (i == 0) imm = 0;
      _generate_addr(ctx, reg, rn, rm, imm);
      return 0;
    }

    case ARM_LDM:
    case ARM_STM: {
      uint32_t rn, regs, p, u, w, s;
      arm_stm_decode_fields(ctx->code.read_address, &rn, &regs, &p, &u, &w, &s);
      int offset = u ? 0 : -4 *(count_bits(regs) -1);
      if (p) {
        offset += u ? 4 : -4;
      }
      _generate_addr(ctx, reg, rn, reg_invalid, offset);
      return 0;
    }

    case ARM_NEON_VLDX_M:
    case ARM_NEON_VLDX_S_O:
    case ARM_NEON_VLDX_S_A:
    case ARM_NEON_VSTX_M:
    case ARM_NEON_VSTX_S_O: {
      uint32_t op, sz, d, vd, rn, align, rm;
      arm_neon_vldx_m_decode_fields(ctx->code.read_address, &op, &sz, &d, &vd, &rn, &align, &rm);
      // rm only used for post-incrementing
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
	    return 0;
    }

    case ARM_LDREX:
    case ARM_LDREXB:
    case ARM_LDREXD:
    case ARM_LDREXH:
    case ARM_STREX:
    case ARM_STREXB:
    case ARM_STREXD:
    case ARM_STREXH: {
      uint32_t rd, rn;
      arm_ldrex_decode_fields(ctx->code.read_address, &rd, &rn);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }

    case ARM_VFP_VLDM_DP:
    case ARM_VFP_VLDM_SP:
    case ARM_VFP_VSTM_DP:
    case ARM_VFP_VSTM_SP:
    case ARM_VFP_VPOP_DP:
    case ARM_VFP_VPOP_SP:
    case ARM_VFP_VPUSH_DP:
    case ARM_VFP_VPUSH_SP: {
      uint32_t p, u, d, w, rn, vd, imm8;
      arm_vfp_vldm_dp_decode_fields(ctx->code.read_address, &p, &u, &d, &w, &rn, &vd, &imm8);
      assert(p != u);
      _generate_addr(ctx, reg, rn, reg_invalid, u ? 0 : -(imm8 << 2));
      return 0;
    }

    case ARM_VFP_VLDR_DP:
    case ARM_VFP_VLDR_SP:
    case ARM_VFP_VSTR_DP:
    case ARM_VFP_VSTR_SP: {
      uint32_t u, d, rn, vd, imm8;
      arm_vfp_vldr_dp_decode_fields(ctx->code.read_address, &u, &d, &rn, &vd, &imm8);
      if (u == 0) {
        imm8 = -imm8;
      }
	    _generate_addr(ctx, reg, rn, reg_invalid, imm8 << 2);
      return 0;
    }

    case ARM_LDC:
    case ARM_STC:
      fprintf(stderr, "Address decoding for A32 instruction %d not implemented yet\n", ctx->code.inst);
      assert(0);
      break;
  }
  return -1;
}
#endif

#ifdef __aarch64__
int _a64_calc_ld_st_addr(mambo_context *ctx, enum reg reg) {
  switch (ctx->code.inst) {
    case A64_LDP_STP: {
      uint32_t opc, v, type, l, imm7, rt2, rn, rt;
      a64_LDP_STP_decode_fields(ctx->code.read_address, &opc, &v, &type, &l, &imm7, &rt2, &rn, &rt);
      int offset = sign_extend32(7, imm7) << (2 + (opc >> (1 - v)));
      _generate_addr(ctx, reg, rn, reg_invalid, (type != 1) ? offset : 0);
      return 0;
    }
    case A64_LDR_STR_UNSIGNED_IMMED: {
      uint32_t size, v, opc, imm12, rn, rt;
      a64_LDR_STR_unsigned_immed_decode_fields(ctx->code.read_address, &size, &v, &opc, &imm12, &rn, &rt);
      int offset = imm12 << (((v & (opc >> 1)) << 2) + size);
      _generate_addr(ctx, reg, rn, reg_invalid, offset);
      return 0;
    }
    case A64_LDR_STR_IMMED: {
      uint32_t size, v, opc, imm9, type, rn, rt;
      a64_LDR_STR_immed_decode_fields(ctx->code.read_address, &size, &v, &opc, &imm9, &type, &rn, &rt);
      int offset = sign_extend32(9, imm9);
      _generate_addr(ctx, reg, rn, reg_invalid, (type != 1) ? offset : 0);
      return 0;
    }
    case A64_LDR_LIT: {
      uint32_t opc, v, imm19, rt;
      a64_LDR_lit_decode_fields(ctx->code.read_address, &opc, &v, &imm19, &rt);
      uintptr_t offset = sign_extend64(19, imm19) << 2;
      uintptr_t addr = (uintptr_t)ctx->code.read_address + offset;
      emit_set_reg(ctx, reg, addr);
      return 0;
    }
    case A64_LDR_STR_REG: {
      uint32_t size, v, opc, rm, opt, s, rn, rt;
      a64_LDR_STR_reg_decode_fields(ctx->code.read_address, &size, &v, &opc, &rm, &opt, &s, &rn, &rt);
      if (rm == x31) {
        _generate_addr(ctx, reg, rn, reg_invalid, 0);
      } else {
        int shift = s ? (((v & (opc >> 1)) << 2) + size) : 0;
        _generate_addr(ctx, reg, rn, rm, (shift << 3) | opt);
      }
      return 0;
    }
    case A64_LDX_STX: {
      uint32_t size, o2, l, o1, rs, o0, rt2, rn, rt;
      a64_LDX_STX_decode_fields(ctx->code.read_address, &size, &o2, &l, &o1, &rs, &o0, &rt2, &rn, &rt);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }
    case A64_LDX_STX_MULTIPLE: {
      uint32_t q, l, op, size, rn, rt;
      a64_LDx_STx_multiple_decode_fields(ctx->code.read_address, &q, &l, &op, &size, &rn, &rt);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }
    case A64_LDX_STX_MULTIPLE_POST: {
      uint32_t q, l, rm, op, sz, rn, rt;
      a64_LDx_STx_multiple_post_decode_fields(ctx->code.read_address, &q, &l, &rm, &op, &sz, &rn, &rt);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }
    case A64_LDX_STX_SINGLE: {
      uint32_t q, l, r, op, s, size, rn, rt;
      a64_LDx_STx_single_decode_fields(ctx->code.read_address, &q, &l, &r, &op, &s, &size, &rn, &rt);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }
    case A64_LDX_STX_SINGLE_POST: {
      uint32_t q, l, r, rm, op, s, size, rn, rt;
      a64_LDx_STx_single_post_decode_fields(ctx->code.read_address, &q, &l, &r, &rm, &op, &s, &size, &rn, &rt);
      _generate_addr(ctx, reg, rn, reg_invalid, 0);
      return 0;
    }
  }

  return -1;
}
#endif


int mambo_calc_ld_st_addr(mambo_context *ctx, enum reg reg) {
#ifdef __arm__
  if (ctx->code.inst_type == THUMB_INST) {
    return _thumb_calc_ld_st_addr(ctx, reg);
  } else if (ctx->code.inst_type == ARM_INST) {
    return _arm_calc_ld_st_addr(ctx, reg);
  }
  return -1;
#elif __aarch64__
  return _a64_calc_ld_st_addr(ctx, reg);
#endif
}

#ifdef __aarch64__
int _a64_get_ld_st_size(mambo_context *ctx) {
  int size = -1;

  switch (ctx->code.inst) {
    case A64_LDR_LIT: {
      uint32_t opc, v, imm19, rt;
      a64_LDR_lit_decode_fields(ctx->code.read_address, &opc, &v, &imm19, &rt);
      if (v) {
        size = 4 << opc;
      } else {
        size = 4 << (opc & 1);
      }
      break;
    }
    case A64_LDP_STP: {
      uint32_t opc, v, type, l, imm7, rt2, rn, rt;
      a64_LDP_STP_decode_fields(ctx->code.read_address, &opc, &v, &type, &l, &imm7, &rt2, &rn, &rt);
      if (v) {
        size = (8 << opc);
      } else {
        size = 8 << (opc >> 1);
      }
      break;
    }
    case A64_LDR_STR_REG:
    case A64_LDR_STR_IMMED:
    case A64_LDR_STR_UNSIGNED_IMMED: {
      uint32_t sz, v, opc, imm12, rn, rt;
      a64_LDR_STR_unsigned_immed_decode_fields(ctx->code.read_address, &sz, &v, &opc, &imm12, &rn, &rt);
      if (v) {
        size = (1 << (sz + ((opc >> 1) << 2)));
      } else {
        size = 1 << sz;
      }
      break;
    }
    case A64_LDX_STX: {
      uint32_t sz, o2, l, o1, rs, o0, rt2, rn, rt;
      a64_LDX_STX_decode_fields(ctx->code.read_address, &sz, &o2, &l, &o1, &rs, &o0, &rt2, &rn, &rt);
      size = 1 << (sz + o1);
      break;
    }
    case A64_LDX_STX_MULTIPLE:
    case A64_LDX_STX_MULTIPLE_POST: {
      uint32_t q, l, op, sz, rn, rt;
      a64_LDx_STx_multiple_decode_fields(ctx->code.read_address, &q, &l, &op, &sz, &rn, &rt);
      int regs = 0;
      switch (op) {
        case 0x0: // LD/ST4
        case 0x2: // LD/ST1
          regs = 4;
          break;
        case 0x4: // LD/ST3
        case 0x6: // LD/ST1
          regs = 3;
          break;
        case 0x8: // LD/ST2
        case 0xA: // LD/ST1
          regs = 2;
          break;
        case 0x7: // LD/ST1
          regs = 1;
          break;
        default:
          fprintf(stderr, "Unsupported LDx/STx opcode %x at %p\n", op, ctx->code.read_address);
          exit(EXIT_FAILURE);
      }
      size = regs * (8 << q);
      break;
    }
    case A64_LDX_STX_SINGLE:
    case A64_LDX_STX_SINGLE_POST: {
      uint32_t q, l, r, op, s, sz, rn, rt;
      a64_LDx_STx_single_decode_fields(ctx->code.read_address, &q, &l, &r, &op, &s, &sz, &rn, &rt);
      int regs = (((op & 1) << 1) | r) + 1;
      int scale = (op >> 1);
      switch (scale) {
        case 3:
          scale = sz;
          break;
        case 2:
          if (sz & 1) {
            scale = 3;
          }
          break;
      }
      size = (1 << scale) * regs;
      break;
    }
  } // switch

  return size;
}
#endif

#ifdef __arm__
// Same decoding logic on A32 and T32
int _get_size_vldx_vstx_m(void *read_addr, bool is_thumb) {
  uint32_t op, sz, d, vd, rn, align, rm;
  if (is_thumb) {
    thumb_neon_vldx_m_decode_fields(read_addr, &op, &sz, &d, &vd, &rn, &align, &rm);
  } else {
    arm_neon_vldx_m_decode_fields(read_addr, &op, &sz, &d, &vd, &rn, &align, &rm);
  }
  int regs = 0;

  switch (op) {
    case 0x7:
      regs = 1;
      break;
    case 0x8:
    case 0x9:
    case 0xA:
      regs = 2;
      break;
    case 0x4:
    case 0x5:
    case 0x6:
      regs = 3;
      break;
    case 0x0:
    case 0x1:
    case 0x2:
    case 0x3:
      regs = 4;
      break;
    default:
      fprintf(stderr, "Unsupported VLDx (multiple) opcode %x at %p\n", op, read_addr);
      exit(EXIT_FAILURE);
  }
  return regs * 8;
}

int _thumb_get_ld_st_size(mambo_context *ctx) {
  int size = -1;

  switch(ctx->code.inst) {
    // Fixed-size loads / stores
    case THUMB_LDRB16:
    case THUMB_LDRBI16:
    case THUMB_LDRSB16:
    case THUMB_STRB16:
    case THUMB_STRBI16:
    case THUMB_LDRB32:
    case THUMB_LDRBI32:
    case THUMB_LDRBL32:
    case THUMB_LDRBT32:
    case THUMB_LDRBWI32:
    case THUMB_LDRSB32:
    case THUMB_LDRSBI32:
    case THUMB_LDRSBL32:
    case THUMB_LDRSBT32:
    case THUMB_LDRSBWI32:
    case THUMB_STRB32:
    case THUMB_STRBI32:
    case THUMB_STRBT32:
    case THUMB_STRBWI32:
    case THUMB_LDREXB32:
    case THUMB_STREXB32:
      size = 1;
      break;

    case THUMB_LDRH16:
    case THUMB_LDRHI16:
    case THUMB_LDRSH16:
    case THUMB_STRH16:
    case THUMB_STRHI16:
    case THUMB_LDRH32:
    case THUMB_LDRHI32:
    case THUMB_LDRHL32:
    case THUMB_LDRHT32:
    case THUMB_LDRHWI32:
    case THUMB_LDRSH32:
    case THUMB_LDRSHI32:
    case THUMB_LDRSHL32:
    case THUMB_LDRSHT32:
    case THUMB_LDRSHWI32:
    case THUMB_STRH32:
    case THUMB_STRHI32:
    case THUMB_STRHT32:
    case THUMB_STRHWI32:
    case THUMB_LDREXH32:
    case THUMB_STREXH32:
      size = 2;
      break;

    case THUMB_LDR16:
    case THUMB_LDRI16:
    case THUMB_LDR_PC_16:
    case THUMB_LDR_SP16:
    case THUMB_STR16:
    case THUMB_STRI16:
    case THUMB_STR_SP16:
    case THUMB_LDR32:
    case THUMB_LDRI32:
    case THUMB_LDRL32:
    case THUMB_LDRT32:
    case THUMB_LDRWI32:
    case THUMB_STR32:
    case THUMB_STRI32:
    case THUMB_STRT32:
    case THUMB_STRWI32:
    case THUMB_LDREX32:
    case THUMB_STREX32:
    case THUMB_VFP_VLDR_SP:
    case THUMB_VFP_VSTR_SP:
      size = 4;
      break;

    case THUMB_LDRD32:
    case THUMB_STRD32:
    case THUMB_LDREXD32:
    case THUMB_STREXD32:
    case THUMB_VFP_VLDR_DP:
    case THUMB_VFP_VSTR_DP:
      size = 8;
      break;

    // Variable-sized loads / stores
    case THUMB_LDMFD32:
    case THUMB_STMFD32:
    case THUMB_LDMEA32:
    case THUMB_STMEA32: {
      uint32_t w, rn, reglist;
      thumb_stmfd32_decode_fields(ctx->code.read_address, &w, &rn, &reglist);
      size = count_bits(reglist) * 4;
      break;
    }

    case THUMB_LDMFD16:
    case THUMB_STMEA16: {
      uint32_t rn, reglist;
      thumb_ldmfd16_decode_fields(ctx->code.read_address, &rn, &reglist);
      size = count_bits(reglist) * 4;
      break;
    }

    case THUMB_PUSH16:
    case THUMB_POP16: {
      uint32_t reglist;
      thumb_push16_decode_fields(ctx->code.read_address, &reglist);
      size = count_bits(reglist) * 4;
      break;
    }

    case THUMB_VFP_VPUSH:
    case THUMB_VFP_VPOP:
    case THUMB_VFP_VLDM_DP:
    case THUMB_VFP_VLDM_SP:
    case THUMB_VFP_VSTM_DP:
    case THUMB_VFP_VSTM_SP: {
      uint32_t sz, d, vd, regs;
      thumb_vfp_vpush_decode_fields(ctx->code.read_address, &sz, &d, &vd, &regs);
      size = regs * 4;
      break;
    }

    case THUMB_NEON_VLDX_S_O:
    case THUMB_NEON_VSTX_S_O: {
      uint32_t op, sz, d, vd, rn, align, rm;
      thumb_neon_vldx_s_o_decode_fields(ctx->code.read_address, &op, &sz, &d, &vd, &rn, &align, &rm);
      size = (1 << sz) * (op + 1);
      break;
    }

    case THUMB_NEON_VLDX_S_A: {
      uint32_t op, sz, d, vd, inc, rn, align, rm;
      thumb_neon_vldx_s_a_decode_fields(ctx->code.read_address, &op, &sz, &d, &vd, &inc, &rn, &align, &rm);
      size = (1 << sz) * (op + 1);
	    break;
    }

    case THUMB_NEON_VLDX_M:
    case THUMB_NEON_VSTX_M:
      size = _get_size_vldx_vstx_m(ctx->code.read_address, true);
      break;

    case THUMB_LDC232:
    case THUMB_LDC32:
    case THUMB_STC32:
    case THUMB_STC232:
      fprintf(stderr, "Size decoding for T32 instruction %d not implemented yet\n", ctx->code.inst);
      assert(0);
      break;
  }

  return size;
}

int _arm_get_ld_st_size(mambo_context *ctx) {
  int size = -1;

  switch(ctx->code.inst) {
    // Fixed-size loads / stores
    case ARM_LDRB:
    case ARM_LDRBT:
    case ARM_LDRSB:
    case ARM_LDRSBT:
    case ARM_STRB:
    case ARM_STRBT:
    case ARM_LDREXB:
    case ARM_STREXB:
      size = 1;
      break;

    case ARM_LDRH:
    case ARM_LDRHT:
    case ARM_LDRSH:
    case ARM_LDRSHT:
    case ARM_STRH:
    case ARM_STRHT:
    case ARM_LDREXH:
    case ARM_STREXH:
      size = 2;
      break;

    case ARM_LDR:
    case ARM_LDRT:
    case ARM_STR:
    case ARM_STRT:
    case ARM_LDREX:
    case ARM_STREX:
    case ARM_VFP_VLDR_SP:
    case ARM_VFP_VSTR_SP:
      size = 4;
      break;

    case ARM_LDRD:
    case ARM_STRD:
    case ARM_LDREXD:
    case ARM_STREXD:
    case ARM_VFP_VLDR_DP:
    case ARM_VFP_VSTR_DP:
      size = 8;
      break;

    // Variable-sized loads / stores
    case ARM_LDM:
    case ARM_STM: {
      uint32_t rn, reglist, p, u, w, s;
      arm_ldm_decode_fields(ctx->code.read_address, &rn, &reglist, &p, &u, &w, &s);
      size = count_bits(reglist) * 4;
      break;
    }

    case ARM_VFP_VLDM_DP:
    case ARM_VFP_VLDM_SP:
    case ARM_VFP_VSTM_DP:
    case ARM_VFP_VSTM_SP:
    case ARM_VFP_VPUSH_DP:
    case ARM_VFP_VPUSH_SP:
    case ARM_VFP_VPOP_DP:
    case ARM_VFP_VPOP_SP: {
      uint32_t p, u, d, w, rn, vd, imm8;
      arm_vfp_vldm_sp_decode_fields(ctx->code.read_address, &p, &u, &d, &w, &rn, &vd, &imm8);
      size = imm8 * 4;
      break;
    }

    case ARM_NEON_VLDX_S_O:
    case ARM_NEON_VSTX_S_O: {
      uint32_t op, sz, d, vd, rn, align, rm;
      arm_neon_vldx_s_o_decode_fields(ctx->code.read_address, &op, &sz, &d, &vd, &rn, &align, &rm);
      size = (1 << sz) * (op + 1);
      break;
    }

    case ARM_NEON_VLDX_S_A: {
      uint32_t op, sz, d, vd, inc, rn, align, rm;
      arm_neon_vldx_s_a_decode_fields(ctx->code.read_address, &op, &sz, &d, &vd, &inc, &rn, &align, &rm);
      size = (1 << sz) * (op + 1);
      break;
    }

    case ARM_NEON_VLDX_M:
    case ARM_NEON_VSTX_M:
      size = _get_size_vldx_vstx_m(ctx->code.read_address, false);
      break;

    case ARM_LDC:
    case ARM_STC:
      fprintf(stderr, "Size decoding for A32 instruction %d not implemented yet\n", ctx->code.inst);
      assert(0);
  }

  return size;
}
#endif

int mambo_get_ld_st_size(mambo_context *ctx) {
#ifdef __arm__
  if (ctx->code.inst_type == THUMB_INST) {
    return _thumb_get_ld_st_size(ctx);
  } else if (ctx->code.inst_type == ARM_INST) {
    return _arm_get_ld_st_size(ctx);
  }
#elif __aarch64__
  return _a64_get_ld_st_size(ctx);
#endif
  return -1;
}


#endif

```

`api/plugin_support.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>
#include <assert.h>
#include <sys/mman.h>
#include <stdarg.h>

#include "../dbm.h"
#include "../common.h"
#include "helpers.h"

#ifdef PLUGINS_NEW

/* Plugin management */
mambo_context *mambo_register_plugin(void) {
  int index = global_data.free_plugin++;
  static mambo_context tmp_ctx;

  if (index >= MAX_PLUGIN_NO) {
    return NULL;
  }

  set_mambo_context(&tmp_ctx, NULL, PLUGIN_REG);
  tmp_ctx.plugin_id = index;

  return &tmp_ctx;
}

/* Callback management */
int __mambo_register_cb(mambo_context *ctx, mambo_cb_idx cb_idx, mambo_callback cb) {
  unsigned int p_id = ctx->plugin_id;

  if (cb_idx >= CALLBACK_MAX_IDX || cb_idx < 0) {
    return MAMBO_INVALID_CB;
  }

  if (p_id >= MAX_PLUGIN_NO) {
    return MAMBO_INVALID_PLUGIN_ID;
  }

  if (global_data.plugins[p_id].cbs[cb_idx] != NULL) {
    return MAMBO_CB_ALREADY_SET;
  }

  global_data.plugins[p_id].cbs[cb_idx] = cb;

  return MAMBO_SUCCESS;
}

int mambo_register_pre_inst_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, PRE_INST_C, cb);
}

int mambo_register_post_inst_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, POST_INST_C, cb);
}

int mambo_register_pre_basic_block_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, PRE_BB_C, cb);
}

int mambo_register_post_basic_block_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, POST_BB_C, cb);
}

int mambo_register_pre_fragment_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, PRE_FRAGMENT_C, cb);
}

int mambo_register_post_fragment_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, POST_FRAGMENT_C, cb);
}

int mambo_register_pre_syscall_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, PRE_SYSCALL_C, cb);
}

int mambo_register_post_syscall_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, POST_SYSCALL_C, cb);
}

int mambo_register_pre_thread_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, PRE_THREAD_C, cb);
}

int mambo_register_post_thread_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, POST_THREAD_C, cb);
}

int mambo_register_exit_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, EXIT_C, cb);
}

int mambo_register_vm_op_cb(mambo_context *ctx, mambo_callback cb) {
  return __mambo_register_cb(ctx, VM_OP_C, cb);
}

int mambo_register_function_cb(mambo_context *ctx, char *fn_name,
                               mambo_callback cb_pre, mambo_callback cb_post, int max_args) {
#ifdef __arm__
  #define ARG_LIMIT 4
#elif __aarch64__
  #define ARG_LIMIT 8
#endif
  if (cb_pre == NULL && cb_post == NULL) return -1;
  if (cb_post && (max_args > ARG_LIMIT || max_args < 0)) return -2;
  return function_watch_add(&global_data.watched_functions, fn_name, ctx->plugin_id, cb_pre, cb_post);
}

/* Access plugin data */
int mambo_set_plugin_data(mambo_context *ctx, void *data) {
  unsigned int p_id = ctx->plugin_id;
  if (p_id >= global_data.free_plugin) {
    return MAMBO_INVALID_PLUGIN_ID;
  }
  global_data.plugins[p_id].data = data;
  return MAMBO_SUCCESS;
}

void *mambo_get_plugin_data(mambo_context *ctx) {
  unsigned int p_id = ctx->plugin_id;
  if (p_id >= global_data.free_plugin) {
    return NULL;
  }
  return global_data.plugins[p_id].data;
}

int mambo_set_thread_plugin_data(mambo_context *ctx, void *data) {
  unsigned int p_id = ctx->plugin_id;
  if (p_id >= global_data.free_plugin) {
    return MAMBO_INVALID_PLUGIN_ID;
  }
  if (ctx->thread_data == NULL) {
    return MAMBO_INVALID_THREAD;
  }
  ctx->thread_data->plugin_priv[p_id] = data;
  return MAMBO_SUCCESS;
}

void *mambo_get_thread_plugin_data(mambo_context *ctx) {
  unsigned int p_id = ctx->plugin_id;
  if (p_id >= global_data.free_plugin) {
    return NULL;
  }
  if (ctx->thread_data == NULL) {
    return NULL;
  }
  return ctx->thread_data->plugin_priv[p_id];
}

/* Memory management */
void *mambo_alloc(mambo_context *ctx, size_t size) {
  return mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
}

void mambo_free(mambo_context *ctx, void *ptr) {
}

/* Other */
int mambo_get_inst(mambo_context *ctx) {
  return ctx->code.inst;
}

inst_set mambo_get_inst_type(mambo_context *ctx) {
  return ctx->code.inst_type;
}

int mambo_get_fragment_id(mambo_context *ctx) {
  return ctx->code.fragment_id;
}

cc_type mambo_get_fragment_type(mambo_context *ctx) {
  return ctx->code.fragment_type;
}

int mambo_get_inst_len(mambo_context *ctx) {
  int inst = mambo_get_inst(ctx);
  // Not an instruction event
  if (inst == -1) {
    return -1;
  }
#ifdef __arm__
  if (mambo_get_inst_type(ctx) == ARM_INST) {
    return 4;
  } else {
    return (inst < THUMB_ADC32) ? 2 : 4;
  }
#elif __aarch64__
  return 4;
#endif
}

void *mambo_get_source_addr(mambo_context *ctx) {
  return ctx->code.read_address;
}

void *mambo_get_cc_addr(mambo_context *ctx) {
  return ctx->code.write_p;
}

void mambo_set_cc_addr(mambo_context *ctx, void *addr) {
  assert(ctx->code.write_p != NULL);
  ctx->code.write_p = addr;
}

int mambo_get_thread_id(mambo_context *ctx) {
  return ctx->thread_data->tid;
}

mambo_cond mambo_get_cond(mambo_context *ctx) {
  return ctx->code.cond;
}

bool mambo_is_cond(mambo_context *ctx) {
  return ctx->code.cond != AL;
}

mambo_cond mambo_get_inverted_cond(mambo_context *ctx, mambo_cond cond) {
  return invert_cond(cond & 0xF);
}

void mambo_replace_inst(mambo_context *ctx) {
  ctx->code.replace = true;
}

int mambo_set_source_addr(mambo_context *ctx, void *source_addr) {
  if (ctx->event_type != PRE_FN_C) return -1;

#ifdef __arm__
  bool change_type = false;
  if (((uintptr_t)source_addr & 1) && mambo_get_inst_type(ctx) == ARM_INST) {
    change_type = true;
  } else if ((((uintptr_t)source_addr & 1) == 0) && mambo_get_inst_type(ctx) == THUMB_INST) {
    change_type = true;
  }

  if (change_type) {
    uint16_t *write_p = (uint16_t *)mambo_get_cc_addr(ctx);
    int fragment_id = mambo_get_fragment_id(ctx);
    ctx->thread_data->code_cache_meta[fragment_id].exit_branch_addr = write_p;

    if (mambo_get_inst_type(ctx) == THUMB_INST) {
      ctx->thread_data->code_cache_meta[fragment_id].exit_branch_type = uncond_blxi_thumb;
      thumb_simple_exit(ctx->thread_data, &write_p, fragment_id, (uintptr_t)source_addr);
    } else {
      ctx->thread_data->code_cache_meta[fragment_id].exit_branch_type = uncond_blxi_arm;
      arm_simple_exit(ctx->thread_data, (uint32_t **)&write_p, fragment_id,
                      0, (uint32_t *)(source_addr-8), AL);
    }

    mambo_set_cc_addr(ctx, write_p);
    mambo_stop_scan(ctx);

    return 0;
  }
#endif

  ctx->code.read_address = (void *)((uintptr_t)source_addr & ~1);
  ctx->code.replace = true;

  return 0;
}

/* Allows scratch registers to be shared by multiple plugins
  This will likely be modified in the future to allocate dead
  application registers if available.
*/
int mambo_get_scratch_regs(mambo_context *ctx, int count, ...) {
  int *regp;
  int min_pushed_reg = 8; // subject to change; selected for thumb-16 push/pops
  int allocated_regs = 0;
  uint32_t to_push = 0;

  va_list args;
  va_start(args, count);

  if (ctx->code.pushed_regs) {
    min_pushed_reg = next_reg_in_list(ctx->code.pushed_regs, 0);
  }

  for (int i = 0; i < count; i++) {
    regp = va_arg(args, int *);
    int reg = next_reg_in_list(ctx->code.available_regs, 0);
    if (reg != reg_invalid) {
      ctx->code.available_regs &= ~(1 << reg);
    } else {
      min_pushed_reg--;
      if (min_pushed_reg >= 0) {
        to_push |= 1 << min_pushed_reg;
        reg = min_pushed_reg;
      }
    }
    if (reg >= 0 && reg < reg_invalid) {
      *regp = reg;
      allocated_regs++;
    } else {
      break;
    }
  }

  ctx->code.pushed_regs |= to_push;
  if (to_push) {
    emit_push(ctx, to_push);
  }

  return allocated_regs;
}

int mambo_get_scratch_reg(mambo_context *ctx, int *regp) {
  return mambo_get_scratch_regs(ctx, 1, regp);
}

int mambo_free_scratch_regs(mambo_context *ctx, uint32_t regs) {
  if ((regs & ctx->code.pushed_regs) != regs) {
    return -1;
  }
  ctx->code.available_regs |= regs;
  return 0;
}

int mambo_free_scratch_reg(mambo_context *ctx, int reg) {
  return mambo_free_scratch_reg(ctx, 1 << reg);
}

/* Syscall helpers */
int mambo_syscall_get_no(mambo_context *ctx, uintptr_t *no) {
  if (ctx->event_type == PRE_SYSCALL_C ||
      ctx->event_type == POST_SYSCALL_C) {
    *no = ctx->syscall.number;
    return 0;
  }
  return -1;
}

void mambo_syscall_get_args(mambo_context *ctx, uintptr_t **args) {
  *args = NULL;
  if (ctx->event_type == PRE_SYSCALL_C) {
    *args = ctx->syscall.regs;
  }
}

int mambo_syscall_bypass(mambo_context *ctx) {
  if (ctx->event_type == PRE_SYSCALL_C) {
    assert(ctx->syscall.replace == false);
    ctx->syscall.replace = true;
    // Initialise the return value
    // This is similar to leaving all registers untouched, as opposed to a NOP syscall
    ctx->syscall.ret = ctx->syscall.regs[reg0];
    return 0;
  }
  return -1;
}

int mambo_syscall_get_return(mambo_context *ctx, uintptr_t *ret) {
  if (ctx->event_type == POST_SYSCALL_C) {
    *ret = ctx->syscall.regs[reg0];
    return 0;
  }
  if (ctx->event_type == PRE_SYSCALL_C && ctx->syscall.replace) {
    *ret = ctx->syscall.ret;
    return 0;
  }
  return -1;
}

int mambo_syscall_set_return(mambo_context *ctx, uintptr_t ret) {
  if (ctx->event_type == POST_SYSCALL_C) {
    ctx->syscall.regs[reg0] = ret;
    return 0;
  }
  // This way we preserve the arguments even after setting a return
  if (ctx->event_type == PRE_SYSCALL_C && ctx->syscall.replace) {
    ctx->syscall.ret = ret;
    return 0;
  }
  return -1;
}

// enables indirect control transfers directly to the current code cache location
int mambo_add_identity_mapping(mambo_context *ctx) {
  if (ctx->code.write_p == NULL) {
    return -1;
  }

  uintptr_t addr = (uintptr_t)mambo_get_cc_addr(ctx);
  if (ctx->code.inst_type == THUMB_INST) {
    addr |= THUMB;
  }

  int ret = hash_add(&current_thread->entry_address, addr, addr);
  return (ret) ? 0 : -1;
}

vm_op_t mambo_get_vm_op(mambo_context *ctx) {
  assert(ctx->event_type == VM_OP_C);
  return ctx->vm.op;
}

void *mambo_get_vm_addr(mambo_context *ctx) {
  assert(ctx->event_type == VM_OP_C);
  return ctx->vm.addr;
}

size_t mambo_get_vm_size(mambo_context *ctx) {
  assert(ctx->event_type == VM_OP_C);
  return ctx->vm.size;
}

int mambo_get_vm_prot(mambo_context *ctx) {
  assert(ctx->event_type == VM_OP_C);
  return ctx->vm.prot;
}

int mambo_get_vm_flags(mambo_context *ctx) {
  assert(ctx->event_type == VM_OP_C);
  return ctx->vm.flags;
}

int mambo_get_vm_filedes(mambo_context *ctx) {
  assert(ctx->event_type == VM_OP_C);
  return ctx->vm.filedes;
}

int mambo_get_vm_off(mambo_context *ctx) {
  assert(ctx->event_type == VM_OP_C);
  return ctx->vm.off;
}

char *mambo_get_cb_function_name(mambo_context *ctx) {
  return ctx->code.func_name;
}

int mambo_stop_scan(mambo_context *ctx) {
  if (ctx->event_type != PRE_INST_C
      && ctx->event_type != POST_INST_C
      && ctx->event_type != PRE_BB_C
      && ctx->event_type != POST_BB_C
      && ctx->event_type != PRE_FRAGMENT_C
      && ctx->event_type != POST_FRAGMENT_C
      && ctx->event_type != PRE_FN_C
      && ctx->event_type != POST_FN_C) {
    return -1;
  }

  if (ctx->code.stop == NULL) return -1;
  *ctx->code.stop = true;

  return 0;
}

void thumb_check_free_space(dbm_thread *thread_data, uint16_t **o_write_p, uint32_t **o_data_p,
                            void *it_state, bool handle_it, size_t size, int cur_block);
void arm_check_free_space(dbm_thread *thread_data, uint32_t **write_p,
                          uint32_t **data_p, uint32_t size, int cur_block);
void a64_check_free_space(dbm_thread *thread_data, uint32_t **write_p,
                          uint32_t **data_p, uint32_t size, int cur_block);

int mambo_reserve_cc_space(mambo_context *ctx, size_t size) {
  if (ctx->code.write_p == NULL || ctx->code.data_p == NULL) return -1;
#ifdef __arm__
  if (mambo_get_inst_type(ctx) == THUMB_INST) {
    thumb_check_free_space(ctx->thread_data, (uint16_t **)&ctx->code.write_p, (uint32_t **)&ctx->code.data_p,
                           NULL, false, size, mambo_get_fragment_id(ctx));
  } else { // ARM_INST
    arm_check_free_space(ctx->thread_data, (uint32_t **)&ctx->code.write_p, (uint32_t **)&ctx->code.data_p,
                         size, mambo_get_fragment_id(ctx));
  }
#elif __aarch64__
  a64_check_free_space(ctx->thread_data, (uint32_t **)&ctx->code.write_p, (uint32_t **)&ctx->code.data_p,
                       size, mambo_get_fragment_id(ctx));
#endif
  return 0;
}
#endif

```

`api/plugin_support.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifndef __PLUGIN_SUPPORT_H__
#define __PLUGIN_SUPPORT_H__

#include "../dbm.h"
#include "../scanner_public.h"

struct code_ctx {
  cc_type fragment_type;
  int fragment_id;

  inst_set inst_type;
  void *read_address;
  int inst;
  mambo_cond cond;

  void *write_p;
  void *data_p;

  uint32_t pushed_regs;
  uint32_t available_regs;
  int plugin_pushed_reg_count;

  char *func_name;

  bool replace;
  bool *stop;
};

struct syscall_ctx {
  uintptr_t number;
  uintptr_t *regs;
  bool replace;
  uintptr_t ret;
};

struct vm_ctx {
  vm_op_t op;
  void *addr;
  size_t size;
  int prot;
  int flags;
  int filedes;
  off_t off;
};

typedef enum {
  PLUGIN_REG,
  PRE_INST_C,
  POST_INST_C,
  PRE_BB_C,
  POST_BB_C,
  PRE_FRAGMENT_C,
  POST_FRAGMENT_C,
  PRE_SYSCALL_C,
  POST_SYSCALL_C,
  PRE_THREAD_C,
  POST_THREAD_C,
  PRE_FN_C,
  POST_FN_C,
  EXIT_C,
  VM_OP_C,
  CALLBACK_MAX_IDX,
} mambo_cb_idx;

typedef struct {
  dbm_thread *thread_data;
  mambo_cb_idx event_type;
  int plugin_id;
  union {
    struct code_ctx code;
    struct syscall_ctx syscall;
    struct vm_ctx vm;
  };
} mambo_context;

typedef int (*mambo_callback)(mambo_context *ctx);

typedef enum {
  BRANCH_NONE = (1 << 0),
  BRANCH_DIRECT = (1 << 1),
  BRANCH_INDIRECT = (1 << 2),
  BRANCH_RETURN = (1 << 3),
  BRANCH_COND = (1 << 4),
  BRANCH_COND_PSR = (1 << 5),
  BRANCH_COND_CBZ = (1 << 6),
  BRANCH_COND_TBZ = (1 << 7), // A64-only
  BRANCH_COND_IT = (1 << 8),  // T32-only
  BRANCH_CALL = (1 << 9),
  BRANCH_INTERWORKING = (1 << 10), // A32 and T32
  BRANCH_TABLE = (1 << 11),        // T32-only
} mambo_branch_type;

typedef struct {
  mambo_callback cbs[CALLBACK_MAX_IDX];
  void *data;
} mambo_plugin;

enum mambo_plugin_error {
  MAMBO_SUCCESS = 0,
  MAMBO_INVALID_PLUGIN_ID = -1,
  MAMBO_CB_ALREADY_SET = -2,
  MAMBO_INVALID_CB = -3,
  MAMBO_INVALID_THREAD = -4,
};

/* Stack frame */
typedef struct stack_frame stack_frame_t;
struct stack_frame {
  stack_frame_t *prev;
  uintptr_t lr;
};

/* Public functions */
mambo_context *mambo_register_plugin(void);

int mambo_register_pre_inst_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_post_inst_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_pre_basic_block_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_post_basic_block_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_pre_fragment_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_post_fragment_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_pre_syscall_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_post_syscall_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_pre_thread_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_post_thread_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_exit_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_vm_op_cb(mambo_context *ctx, mambo_callback cb);
int mambo_register_function_cb(mambo_context *ctx, char *fn_name,
                               mambo_callback cb_pre, mambo_callback cb_post, int max_args);

/* Memory management */
void *mambo_alloc(mambo_context *ctx, size_t size);
void mambo_free(mambo_context *ctx, void *ptr);

/* Access plugin data */
int mambo_set_plugin_data(mambo_context *ctx, void *data);
void *mambo_get_plugin_data(mambo_context *ctx);
int mambo_set_thread_plugin_data(mambo_context *ctx, void *data);
void *mambo_get_thread_plugin_data(mambo_context *ctx);

/* Scratch register management */
int mambo_get_scratch_regs(mambo_context *ctx, int count, ...);
int mambo_get_scratch_reg(mambo_context *ctx, int *regp);
int mambo_free_scratch_regs(mambo_context *ctx, uint32_t regs);
int mambo_free_scratch_reg(mambo_context *ctx, int reg);

/* Syscalls */
int mambo_syscall_get_no(mambo_context *ctx, uintptr_t *no);
void mambo_syscall_get_args(mambo_context *ctx, uintptr_t **args);
int mambo_syscall_bypass(mambo_context *ctx);
int mambo_syscall_get_return(mambo_context *ctx, uintptr_t *ret);
int mambo_syscall_set_return(mambo_context *ctx, uintptr_t ret);

/* VM-callback specific */
vm_op_t mambo_get_vm_op(mambo_context *ctx);
void *mambo_get_vm_addr(mambo_context *ctx);
size_t mambo_get_vm_size(mambo_context *ctx);
int mambo_get_vm_prot(mambo_context *ctx);
int mambo_get_vm_flags(mambo_context *ctx);
int mambo_get_vm_filedes(mambo_context *ctx);
int mambo_get_vm_off(mambo_context *ctx);

/* Other */
int mambo_get_inst(mambo_context *ctx);
inst_set mambo_get_inst_type(mambo_context *ctx);
int mambo_get_fragment_id(mambo_context *ctx);
cc_type mambo_get_fragment_type(mambo_context *ctx);
int mambo_get_inst_len(mambo_context *ctx);
void *mambo_get_source_addr(mambo_context *ctx);
int mambo_set_source_addr(mambo_context *ctx, void *source_addr);
void *mambo_get_cc_addr(mambo_context *ctx);
void mambo_set_cc_addr(mambo_context *ctx, void *addr);
int mambo_get_thread_id(mambo_context *ctx);
bool mambo_is_cond(mambo_context *ctx);
mambo_cond mambo_get_cond(mambo_context *ctx);
mambo_cond mambo_get_inverted_cond(mambo_context *ctx, mambo_cond cond);
void mambo_replace_inst(mambo_context *ctx);
bool mambo_is_load(mambo_context *ctx);
bool mambo_is_store(mambo_context *ctx);
bool mambo_is_load_or_store(mambo_context *ctx);
int mambo_get_ld_st_size(mambo_context *ctx);
int mambo_add_identity_mapping(mambo_context *ctx);
char *mambo_get_cb_function_name(mambo_context *ctx);
int mambo_stop_scan(mambo_context *ctx);
int mambo_reserve_cc_space(mambo_context *ctx, size_t size);

mambo_branch_type mambo_get_branch_type(mambo_context *ctx);

/* Symbol-related functions */
int get_symbol_info_by_addr(uintptr_t addr, char **sym_name, void **start_addr, char **filename);
typedef int (*stack_frame_handler)(void *data, void *addr, char *sym_name, void *symbol_start_addr, char *filename);
int get_backtrace(stack_frame_t *fp, stack_frame_handler handler, void *ptr);

#endif

```

`arch/aarch32/dispatcher_aarch32.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2015-2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/
.syntax unified

.global start_of_dispatcher_s
start_of_dispatcher_s:

.global th_to_arm
.thumb_func
th_to_arm:
  bx pc

.global dispatcher_trampoline
.code 32
dispatcher_trampoline:
  #R2 is available at this point
  #TODO: INSTALL our own stack

#A subroutine must preserve the contents of the registers r4-r8, r10, r11 and SP (and r9 in PCS variants that designate r9 as v6).
  PUSH {r3 - r6, r9, r12, lr}
  STR R0, [R3, #-4] // save the SPC
  SUB R2, R3, #8    // set up the TPC pointer
  VPUSH {d16-d31}
  VPUSH {d0-d7}

  MRS r5, CPSR
  VMRS r6, FPSCR

  LDR R3, disp_thread_data
  LDR R9, dispatcher_addr

  # provide 8-byte alignment of the SP
  MOV R4, SP
  BIC SP, #0x7
  BLX R9
  MOV SP, R4

  MSR CPSR, r5
  VMSR FPSCR, r6

  VPOP {d0-d7}
  VPOP {d16-d31}
  POP {r3 - r6, r9, r12, lr}

/*  SP ->  R0
           R1
           R2
           R3
       <0 or more words>
           EMPTY
           TPC
           SPC
    R3 ->
  */

  LDR R1, [SP, #12]
  STR R1, [R3, #-12]
  POP {R0, R1, R2}
  SUB SP, R3, #12
  POP {R3}
  B checked_cc_return

dispatcher_addr: .word dispatcher

.global trace_head_incr
.code 32
  SUB PC, PC, #3
.thumb_func
trace_head_incr:
  STR LR, [SP, #16]
  NOP // MOVW R1, #counter_base & 0xFFFF    
  NOP
  NOP // MOVT R1, #counter_base >> 16
  NOP
  LDRB R2, [R1, R0]
  SUBW  R2, R2, #1
  STRB R2, [R1, R0]
  CBZ  R2, create_trace_trampoline
  POP {R0-R2, LR}
  LDR PC, [SP], #8

.align 2
create_trace_trampoline:
  BX PC
  NOP
.code 32
  ADD R2, SP, #16
  PUSH {R3 - R6, R9, R12}
  VPUSH {D16-D31}
  VPUSH {D0-D7}

  MRS R5, CPSR
  VMRS R6, FPSCR

  MOV R1, R0
  LDR R0, disp_thread_data
  LDR R3, =create_trace

  MOV R4, SP
  BIC SP, #0x7
  BLX R3
  MOV SP, R4

  MSR CPSR, R5
  VMSR FPSCR, R6

  VPOP {D0-D7}
  VPOP {D16-D31}
  POP {R3 - R6, R9, R12}

  POP {R0-R2, LR}
  B checked_cc_return

.global syscall_wrapper
.global syscall_wrapper_svc
.code 32
syscall_wrapper:
  # R8 is the SPC of the following instruction
  # R14 is the address where to return in the code cache
  STR LR, [SP, #56] // save the TPC
  STR R8, [SP, #60] // save the SPC

  MOV R0, R7 // syscall id
  MOV R1, SP // pointer to saved regs
  MOV R2, R8 // SPC of the next instr.
  LDR R3, disp_thread_data

  LDR R4, syscall_handler_pre_addr
  // provide 8-byte alignment of the SP
  MOV R5, SP
  BIC SP, #0x7
  BLX R4
  MOV SP, R5

  // don't execute the syscall if pre handler returns 0
  CMP R0, #0
  BEQ s_w_r

  // only allow overriding R0-R7
  // the value of R8 must be preserved
  LDM SP, {R0-R7}

  // Balance the stack on sigreturn and rt_sigreturn, which don't return here
  CMP R7, #119
  ADDEQ SP, SP, #64
  CMP R7, #173
  ADDEQ SP, SP, #64

  SVC 0
syscall_wrapper_svc:
  STM SP, {R0-R7}

  MOV R0, R7
  MOV R1, SP
  MOV R2, R8
  LDR R3, disp_thread_data

  LDR R4, syscall_handler_post_addr
  // provide 8-byte alignment of the SP
  MOV R5, SP
  BIC SP, #0x7
  BLX R4
  MOV SP, R5
s_w_r: POP {R0-R12, R14}
  B checked_cc_return

syscall_handler_pre_addr: .word syscall_handler_pre
syscall_handler_post_addr: .word syscall_handler_post


.global disp_thread_data
disp_thread_data: .word 0

.global send_self_signal

.global checked_cc_return
checked_cc_return:
.code 32
  SUB PC, PC, #3
.thumb_func
  PUSH {R0}
  LDR R0, th_is_pending_ptr
  LDR R0, [R0]
  CBZ R0, gotocc
  B deliver_signals_trampoline
gotocc:
  POP {R0}
  LDR PC, [SP], #8
deliver_signals_trampoline:
  LDR R0, [SP, #8] // load SPC
  SUB SP, SP, #12
  PUSH {r1 - r2}
  PUSH {r3 - r6, r9, r12, lr}
  ADD R1, SP, #36
  VPUSH {d16-d31}
  VPUSH {d0-d7}

  MRS r5, CPSR
  VMRS r6, FPSCR

  LDR R9, =deliver_signals

  # provide 8-byte alignment of the SP
  MOV R4, SP
  BIC R2, r4, #0x7
  MOV SP, R2
  BLX R9
  MOV SP, R4

  MSR CPSR, r5
  VMSR FPSCR, r6

  VPOP {d0-d7}
  VPOP {d16-d31}
  POP {r3 - r6, r9, r12, lr}

  /*  SP-> r1
           r2
           PID
           TID
           SIGNO
           R0
           TPC
           SPC
  */

  CBZ R0, abort_self_signal

  PUSH {R7}

  ADD R2, SP, #12
  LDM R2, {R0-R2}
  MOV R7, #268
  SVC 0
send_self_signal:
  POP {r7}
abort_self_signal:
  POP {r1-r2}
  ADD SP, SP, #12
  POP {r0}
  LDR PC, [SP], #8

.global th_is_pending_ptr
th_is_pending_ptr: .word 0

# place the literal pool before the end_of_dispatcher_s symbol
.ltorg

.global end_of_dispatcher_s
end_of_dispatcher_s:


```

`arch/aarch32/dispatcher_aarch32.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2015-2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>

#include "../../dbm.h"
#include "../../scanner_common.h"

#include "../../pie/pie-thumb-encoder.h"
#include "../../pie/pie-arm-encoder.h"

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

void insert_cond_exit_branch(dbm_code_cache_meta *bb_meta, void **o_write_p, int cond) {
  void *write_p = *o_write_p;
  switch(bb_meta->exit_branch_type) {
    case cond_imm_thumb:
      thumb_it16((uint16_t **)&write_p, cond, 0x8);
      write_p += 2;
      break;
    case cbz_thumb:
      if (bb_meta->branch_cache_status & FALLTHROUGH_LINKED) {
        thumb_cbz16((uint16_t **)&write_p, 0, 0x01, bb_meta->rn);
      } else {
        thumb_cbnz16((uint16_t **)&write_p, 0, 0x01, bb_meta->rn);
      }
      write_p += 2;
      break;
    case cond_imm_arm:
      break;
    default:
      fprintf(stderr, "insert_cond_exit_branch(): unknown branch type\n");
      while(1);
  }

  *o_write_p = write_p;
}

void dispatcher_aarch32(dbm_thread *thread_data, uint32_t source_index, branch_type exit_type,
                        uintptr_t target, uintptr_t block_address) {
  uint16_t  *branch_addr;
  int       cache_index;
  bool      is_taken;
  uintptr_t other_target;
  bool      other_target_in_cache;
  uint32_t  cond;

  switch (exit_type) {
#ifdef DBM_TB_DIRECT
    case tbb:
    case tbh: {
      /* the index is invalid only when the inline hash lookup is called for a new BB,
         no linking is required */
  #ifdef FAST_BT
      if (thread_data->code_cache_meta[source_index].rn >= TB_CACHE_SIZE) {
        break;
      }
    #else
      if (thread_data->code_cache_meta[source_index].rn >= MAX_TB_INDEX) {
        break;
      }
    #endif
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
    #ifdef FAST_BT
      uint32_t *branch_table = (uint32_t *)(((uint32_t)branch_addr + 20 + 2) & 0xFFFFFFFC);
      branch_table[thread_data->code_cache_meta[source_index].rn] = block_address;
    #else
      branch_addr += 7;
      uint8_t *table = (uint8_t *)branch_addr;
      if (thread_data->code_cache_meta[source_index].free_b == TB_CACHE_SIZE) {
        // if the list of linked blocks is full, link this index to the inline hash lookup
      #ifdef DBM_D_INLINE_HASH
        table[thread_data->code_cache_meta[source_index].rn] = MAX_TB_INDEX / 2 + TB_CACHE_SIZE * 2 + 1;
      #else
        table[thread_data->code_cache_meta[source_index].rn] = MAX_TB_INDEX / 2 + TB_CACHE_SIZE * 2;
      #endif
      } else {
        // allocate a branch slot and link it
        cache_index = thread_data->code_cache_meta[source_index].free_b++;
        table[thread_data->code_cache_meta[source_index].rn] = MAX_TB_INDEX / 2 + cache_index * 2;
        
        // insert the branch to the target BB
        branch_addr += MAX_TB_INDEX / 2 + cache_index * 2;
        thumb_cc_branch(thread_data, branch_addr, (uint32_t)block_address);
        __clear_cache(branch_addr, branch_addr + 5);
      }
    #endif
      
      // invalidate the saved rm value - required to detect calls from the inline hash lookup
      thread_data->code_cache_meta[source_index].rn = INT_MAX;

      break;
    }
  #endif // DBM_TB_DIRECT
  #ifdef DBM_LINK_UNCOND_IMM
    case uncond_imm_thumb:
    case uncond_b_to_bl_thumb:
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
      if (block_address & 0x1) {
        if (exit_type == uncond_b_to_bl_thumb) {
          thumb_b32_helper(branch_addr, (uint32_t)block_address);
        } else {
          thumb_cc_branch(thread_data, branch_addr, (uint32_t)block_address);
        }
        __clear_cache((char *)branch_addr-1, (char *)(branch_addr) + 8);
      } else {
        // The data word used for the address is word-aligned
        if (((uint32_t)branch_addr) & 2) {
          thumb_ldrl32(&branch_addr, pc, 4, 1);
          branch_addr += 3;
        } else {
          thumb_ldrl32(&branch_addr, pc, 0, 1);
          branch_addr += 2;
        }
        *(uint32_t *)branch_addr = block_address;
        __clear_cache((char *)branch_addr-7, (char *)branch_addr);
      }
      break;

    case uncond_imm_arm:
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
      arm_cc_branch(thread_data, (uint32_t *)branch_addr, (uint32_t)block_address, AL);
      __clear_cache(branch_addr, (char *)branch_addr+5);
      break;
  #endif
  #ifdef DBM_LINK_COND_IMM
    case cond_imm_arm:
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
      is_taken = target == thread_data->code_cache_meta[source_index].branch_taken_addr;

      if (thread_data->code_cache_meta[source_index].branch_cache_status == 0) {
        if (is_taken) {
          other_target = thread_data->code_cache_meta[source_index].branch_skipped_addr;
        } else {
          other_target = thread_data->code_cache_meta[source_index].branch_taken_addr;
        }
        other_target = cc_lookup(thread_data, other_target);
        other_target_in_cache = (other_target != UINT_MAX);

        cond = thread_data->code_cache_meta[source_index].branch_condition;
        if (!is_taken) {
          cond = invert_cond(cond);
        }

        thread_data->code_cache_meta[source_index].branch_cache_status =
                     (is_taken ? BRANCH_LINKED : FALLTHROUGH_LINKED);
      } else {
        branch_addr += 2;
        other_target_in_cache = false;
        cond = AL;
        thread_data->code_cache_meta[source_index].branch_cache_status |= BOTH_LINKED;
      }
      arm_cc_branch(thread_data, (uint32_t *)branch_addr, (uint32_t)block_address, cond);
      branch_addr += 2;

      if (other_target_in_cache) {
        arm_cc_branch(thread_data, (uint32_t *)branch_addr, (uint32_t)other_target, AL);
        branch_addr += 2;
        thread_data->code_cache_meta[source_index].branch_cache_status |= BOTH_LINKED;
      }

      __clear_cache(thread_data->code_cache_meta[source_index].exit_branch_addr, branch_addr);
      break;

    case cond_imm_thumb:
      if (block_address & 0x1) {
        branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
        debug("Target is: 0x%x, b taken addr: 0x%x, b skipped addr: 0x%x\n",
               target, thread_data->code_cache_meta[source_index].branch_taken_addr,
               thread_data->code_cache_meta[source_index].branch_skipped_addr);
        debug("Overwriting branches at %p\n", branch_addr);
        if (target == thread_data->code_cache_meta[source_index].branch_taken_addr) {
          other_target = cc_lookup(thread_data, thread_data->code_cache_meta[source_index].branch_skipped_addr);
          other_target_in_cache = (other_target != UINT_MAX);
          thumb_encode_cond_imm_branch(thread_data, &branch_addr, 
                                      source_index,
                                      block_address,
                                      (other_target_in_cache ? other_target : thread_data->code_cache_meta[source_index].branch_skipped_addr),
                                      thread_data->code_cache_meta[source_index].branch_condition,
                                      true,
                                      other_target_in_cache, true);
        } else {
          other_target = cc_lookup(thread_data, thread_data->code_cache_meta[source_index].branch_taken_addr);
          other_target_in_cache = (other_target != UINT_MAX);
          thumb_encode_cond_imm_branch(thread_data, &branch_addr, 
                                      source_index,
                                      (other_target_in_cache ? other_target : thread_data->code_cache_meta[source_index].branch_taken_addr),
                                      block_address,
                                      thread_data->code_cache_meta[source_index].branch_condition,
                                      other_target_in_cache,
                                      true, true);
        }
        debug("Target at 0x%x, other target at 0x%x\n", block_address, other_target);
        // thumb_encode_cond_imm_branch updates branch_addr to point to the next free word
        __clear_cache((char *)(branch_addr)-100, (char *)branch_addr);
      } else {
        fprintf(stderr, "WARN: cond_imm_thumb to arm\n");
        while(1);
      }
      break;
  #endif // DBM_LINK_COND_IMM
  #ifdef DBM_LINK_CBZ
    case cbz_thumb:
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
      debug("Target is: 0x%x, b taken addr: 0x%x, b skipped addr: 0x%x\n",
             target, thread_data->code_cache_meta[source_index].branch_taken_addr,
             thread_data->code_cache_meta[source_index].branch_skipped_addr);
      debug("Overwriting branches at %p\n", branch_addr);
      if (target == thread_data->code_cache_meta[source_index].branch_taken_addr) {
        other_target = cc_lookup(thread_data, thread_data->code_cache_meta[source_index].branch_skipped_addr);
        other_target_in_cache = (other_target != UINT_MAX);
        thumb_encode_cbz_branch(thread_data,
                                thread_data->code_cache_meta[source_index].rn,
                                &branch_addr,
                                source_index,
                                block_address,
                                (other_target_in_cache ? other_target : thread_data->code_cache_meta[source_index].branch_skipped_addr),
                                true,
                                other_target_in_cache, true);
      } else {
        other_target = cc_lookup(thread_data, thread_data->code_cache_meta[source_index].branch_taken_addr);
        other_target_in_cache = (other_target != UINT_MAX);
        thumb_encode_cbz_branch(thread_data,
                                thread_data->code_cache_meta[source_index].rn,
                                &branch_addr,
                                source_index,
                                (other_target_in_cache ? other_target : thread_data->code_cache_meta[source_index].branch_taken_addr),
                                block_address,
                                other_target_in_cache,
                                true, true);
      }
      debug("Target at 0x%x, other target at 0x%x\n", block_address, other_target);
      // tthumb_encode_cbz_branch updates branch_addr to point to the next free word
      __clear_cache((char *)(branch_addr)-100, (char *)branch_addr);
      break;
  #endif // DBM_LINK_CBZ

    case uncond_blxi_thumb:
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;

      thumb_ldrl32(&branch_addr, pc, ((uint32_t)branch_addr & 2) ? 4 : 0, 1);
	    branch_addr += 2;
	    // The target is word-aligned
	    if ((uint32_t)branch_addr & 2) { branch_addr++; }
	    *(uint32_t *)branch_addr = block_address;
	    __clear_cache((char *)(branch_addr)-6, (char *)branch_addr);

	    record_cc_link(thread_data, (uint32_t)branch_addr|FULLADDR, block_address);
      break;

    case uncond_blxi_arm:
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;

      arm_ldr((uint32_t **)&branch_addr, IMM_LDR, pc, pc, 4, 1, 0, 0);
      branch_addr += 2;
      *(uint32_t *)branch_addr = block_address;
      __clear_cache((char *)(branch_addr-2), (char *)branch_addr);

      record_cc_link(thread_data, (uint32_t)branch_addr|FULLADDR, block_address);
      break;
  }
}

```

`arch/aarch32/scanner_a32.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef __arm__
#include <stdlib.h>
#include <stdio.h>
#include <stdint.h>
#include <stdbool.h>
#include <assert.h>
#include <string.h>
#include <limits.h>

#include "../../dbm.h"
#include "../../common.h"
#include "../../scanner_common.h"

#include "../../pie/pie-arm-decoder.h"
#include "../../pie/pie-arm-encoder.h"
#include "../../pie/pie-arm-field-decoder.h"

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

#define copy_arm() *(write_p++) = *read_address;

#define ALLOWED_IHL_REGS (0x5FF8) // {R3 - R12, R14}
#define IHL_SPACE (88)

void arm_copy_to_reg_16bit(uint32_t **write_p, enum reg reg, uint32_t value) {
  arm_movw(write_p, reg, (value >> 12) & 0xF, value & 0xFFF);
  (*write_p)++;
}

void arm_cond_copy_to_reg_16bit(uint32_t **write_p, enum arm_cond_codes cond, enum reg reg, uint32_t value) {
  arm_movw_cond(write_p, cond, reg, (value >> 12) & 0xF, value & 0xFFF);
  (*write_p)++;
}

void arm_copy_to_reg_32bit(uint32_t **write_p, enum reg reg, uint32_t value) {
  arm_movw(write_p, reg, (value >> 12) & 0xF, value & 0xFFF);
  (*write_p)++;
  arm_movt(write_p, reg, (value >> 28), (value >> 16) & 0xFFF);
  (*write_p)++;
}

void arm_cond_copy_to_reg_32bit(uint32_t **write_p, enum arm_cond_codes cond, enum reg reg, uint32_t value) {
  arm_movw_cond(write_p, cond, reg, (value >> 12) & 0xF, value & 0xFFF);
  (*write_p)++;
  arm_movt_cond(write_p, cond, reg, (value >> 28), (value >> 16) & 0xFFF);
  (*write_p)++;
}

void arm_proc_i32bit(uint32_t **write_p, arm_instruction inst, enum reg rd, enum reg rn, uint32_t value) {
  int shift = 0;
  uint32_t op2 = INT_MAX;

  /* Ensure that at least one instruction is generated even for value == 0,
     by executing the loop at least once. */
  while (value || op2 == INT_MAX) {
    while (value && ((0x3 << shift) & value) == 0) {
      shift += 2;
    }

    op2 = ((((32 - shift) >> 1) & 0xF) << 8) | ((value >> shift) & 0xFF);
    switch (inst) {
      case ARM_ADD:
        arm_add(write_p, IMM_PROC, 0, rd, rn, op2);
        *write_p += 1;
        break;
      case ARM_SUB:
        arm_sub(write_p, IMM_PROC, 0, rd, rn, op2);
        *write_p += 1;
        break;
      default:
        fprintf(stderr, "arm_proc_i32bit unknown insts: %d\n", inst);
        while(1);
    }

    rn = rd;
    value &= ~(0xFF << shift);
  }
}

void arm_add_sub_32_bit(uint32_t **write_p, enum reg rd, enum reg rn, int value) {
  arm_instruction inst = ARM_ADD;
  if (value < 0) {
    value = -value;
    inst = ARM_SUB;
  }

  arm_proc_i32bit(write_p, inst, rd, rn, value);
}

void arm_branch_save_context(dbm_thread *thread_data, uint32_t **o_write_p, bool late_app_sp) {
  uint32_t *write_p = *o_write_p;

  arm_sub(&write_p, IMM_PROC, 0, sp, sp, DISP_RES_WORDS*4);
  write_p++;

  arm_push_regs((1 << r0) | (1 << r1) | (1 << r2) | (1 << r3));

  if (!late_app_sp) {
    arm_add(&write_p, IMM_PROC, 0, r3, sp, DISP_SP_OFFSET);
    write_p++;
  }

  *o_write_p = write_p;
}

void arm_branch_jump(dbm_thread *thread_data, uint32_t **o_write_p, int basic_block,
                     uint32_t offset, uint32_t *read_address, uint32_t cond, uint32_t flags) {
  uint32_t *write_p = *o_write_p;
  int32_t  branch_offset;

  debug("ARM branch: read_addr: %p, offset: 0x%x\n", read_address, offset);

  if (flags & SETUP) {
    if (cond < 14) {
      arm_cond_copy_to_reg_32bit(&write_p, arm_inverse_cond_code[cond], r0, (uint32_t)read_address + 4);
    }
  }

  if (flags & REPLACE_TARGET) {  
    branch_offset = (offset & 0x800000) ? 0xFC000000 : 0;
    branch_offset |= (offset<<2);

    arm_cond_copy_to_reg_32bit(&write_p, cond, r0, (uint32_t)read_address + 8 + branch_offset);
  }
   
  if (flags & INSERT_BRANCH) {
    if (flags & LATE_APP_SP) {
      arm_add(&write_p, IMM_PROC, 0, r3, sp, 24);
      write_p++;
    }
    arm_copy_to_reg_32bit(&write_p, r1, basic_block);

    arm_b(&write_p, (thread_data->dispatcher_addr - (uint32_t)write_p - 8) >> 2);
    write_p++;
  }
  
  *o_write_p = write_p;
}

void arm_simple_exit(dbm_thread *thread_data, uint32_t **o_write_p, int bb_index,
                     uint32_t offset, uint32_t *read_address, uint32_t cond) {
  uint32_t *write_p = *o_write_p;
  arm_branch_save_context(thread_data, &write_p, false);
  arm_branch_jump(thread_data, &write_p, bb_index, offset, read_address,
                  cond, SETUP|REPLACE_TARGET|INSERT_BRANCH);
  *o_write_p = write_p;
}

void arm_check_free_space(dbm_thread *thread_data, uint32_t **write_p,
                          uint32_t **data_p, uint32_t size, int cur_block) {
  int basic_block;

  assert(*write_p < (*data_p)+BASIC_BLOCK_SIZE);

  if ((((uint32_t)*write_p)+size) >= (uint32_t)*data_p) {
    basic_block = allocate_bb(thread_data);
    thread_data->code_cache_meta[basic_block].actual_id = cur_block;
    if (*write_p >= *data_p) {
      assert(&thread_data->code_cache->blocks[basic_block].words[0] == *data_p);
    } else {
      arm_b(write_p, ((uint32_t)&thread_data->code_cache->blocks[basic_block] - (uint32_t)*write_p - 8) >> 2);
      *write_p = (uint32_t *)&thread_data->code_cache->blocks[basic_block];
    }
    *data_p = (uint32_t *)&thread_data->code_cache->blocks[basic_block] + BASIC_BLOCK_SIZE;
  }
}

void arm_branch_helper(uint32_t *write_p, uint32_t target, bool link, uint32_t cond) {
  if ((target & 3) == 0) {
    if (link) {
      arm_bl_cond(&write_p, cond, (target - (uint32_t)write_p - 8)>>2);
    } else {
      arm_b_cond(&write_p, cond, (target - (uint32_t)write_p - 8)>>2);
    }
  } else {
    fprintf(stderr, "ERROR: Cannot insert branch from ARM to Thumb\n");
    while(1);
  }
}

void arm_adjust_b_bl_target(uint32_t *write_p, uint32_t dest_addr) {
  arm_instruction inst = arm_decode(write_p);

  if (inst != ARM_B) {
    fprintf(stderr, "ARM: Trying to adjust target of invalid branch instruction.\n");
    while(1);
  }

  arm_branch_helper(write_p, dest_addr, inst == ARM_BL, *write_p >> 28);
}

void arm_b32_helper(uint32_t *write_p, uint32_t target, uint32_t cond) {
  arm_branch_helper(write_p, target, false, cond);
}

void arm_cc_branch(dbm_thread *thread_data, uint32_t *write_p, uint32_t target, uint32_t cond) {
  arm_b32_helper(write_p, target, cond);

  record_cc_link(thread_data, (uint32_t)write_p, target);
}

void arm_bl32_helper(uint32_t *write_p, uint32_t target, uint32_t cond) {
  arm_branch_helper(write_p, target, true, cond);
}

#define MIN_FSPACE 72

bool arm_scanner_deliver_callbacks(dbm_thread *thread_data, mambo_cb_idx cb_id, uint32_t **o_read_address,
                                   arm_instruction inst, uint32_t **o_write_p, uint32_t **o_data_p,
                                   int basic_block, cc_type type, bool allow_write, bool *stop) {
  bool replaced = false;
#ifdef PLUGINS_NEW
  if (global_data.free_plugin > 0) {
    uint32_t *write_p = *o_write_p;
    uint32_t *data_p = *o_data_p;
    uint32_t *read_address = *o_read_address;

    mambo_cond cond = (*read_address >> 28);
    if (cond == ALT) {
      cond = AL;
    }

    mambo_context ctx;
    set_mambo_context_code(&ctx, thread_data, PRE_INST_C, type, basic_block, ARM_INST, inst, cond, read_address, write_p, data_p, stop);

    for (int i = 0; i < global_data.free_plugin; i++) {
      if (global_data.plugins[i].cbs[cb_id] != NULL) {
        ctx.plugin_id = i;
        ctx.code.replace = false;
        ctx.code.write_p = write_p;
        ctx.code.data_p = data_p;
        ctx.code.available_regs = ctx.code.pushed_regs;
        global_data.plugins[i].cbs[cb_id](&ctx);
        if (allow_write) {
          if (replaced && (write_p != ctx.code.write_p || ctx.code.replace)) {
            fprintf(stderr, "MAMBO API WARNING: plugin %d added code for overridden"
                            "instruction (%p).\n", i, read_address);
          }
          if (ctx.code.replace) {
            if (cb_id == PRE_INST_C) {
              replaced = true;
            } else {
              fprintf(stderr, "MAMBO API WARNING: plugin %d set replace_inst for "
                              "a disallowed event (at %p).\n", i, read_address);
            }
          }
          assert(count_bits(ctx.code.pushed_regs) == ctx.code.plugin_pushed_reg_count);
          if (allow_write && ctx.code.pushed_regs) {
            arm_pop_regs(ctx.code.pushed_regs);
          }
          write_p = ctx.code.write_p;
          data_p = ctx.code.data_p;
          arm_check_free_space(thread_data, &write_p, &data_p, MIN_FSPACE, basic_block);
        } else {
          assert(ctx.code.write_p == write_p);
          assert(ctx.code.data_p == data_p);
        }
      }
    }

    if (cb_id == PRE_BB_C) {
      watched_functions_t *wf = &global_data.watched_functions;
      for (int i = 0; i < wf->funcp_count; i++) {
        if (read_address == wf->funcps[i].addr) {
          _function_callback_wrapper(&ctx, wf->funcps[i].func);
          if (ctx.code.replace) {
            read_address = ctx.code.read_address;
          }
          write_p = ctx.code.write_p;
          data_p = ctx.code.data_p;
          arm_check_free_space(thread_data, &write_p, &data_p, MIN_FSPACE, basic_block);
        }
      }
    }

    *o_write_p = write_p;
    *o_data_p = data_p;
    *o_read_address = read_address;
  }
#endif
  return replaced;
}

bool inline_uncond_imm(dbm_thread *thread_data, bool insert_branch, uint32_t **write_p,
                       uint32_t **data_p, uint32_t **read_addr, uint32_t **bb_entry, uint32_t target,
                       int *inlined_back_count, int basic_block, cc_type type, bool *stop) {
  if (target <= (uint32_t)*read_addr) {
    if (*inlined_back_count >= MAX_BACK_INLINE) {
      if (insert_branch) {
        uint32_t cc_addr = lookup_or_stub(thread_data, target);
        arm_cc_branch(thread_data, *write_p, cc_addr, AL);
        *write_p += 1;
      }

      return false;
    } else {
      *inlined_back_count += 1;
    }
  }

  if (insert_branch) {
    arm_scanner_deliver_callbacks(thread_data, POST_BB_C, bb_entry, -1,
                                  write_p, data_p, basic_block, type, false, stop);
  }
  *read_addr = (uint32_t *)target;
  if (insert_branch) {
    *bb_entry = *read_addr;
    arm_scanner_deliver_callbacks(thread_data, PRE_BB_C, read_addr, -1,
                                  write_p, data_p, basic_block, type, true, stop);
  }

  // Assummes the read pointer is incremented at the end of the current scanner iteration
  *read_addr -= 1;

  return true;
}

void pass1_arm(dbm_thread *thread_data, uint32_t *read_address, branch_type *bb_type) {
  uint32_t null, reglist, rd, offset;
  int32_t branch_offset;
  *bb_type = unknown;
  int inlined_back_count = 0;

  while(*bb_type == unknown) {
    arm_instruction inst = arm_decode(read_address);

    switch(inst) {
      case ARM_B:
      case ARM_BL:
        arm_b_decode_fields(read_address, &offset);

        if ((*read_address >> 28) == AL) {
#ifdef DBM_INLINE_UNCOND_IMM
          branch_offset = (offset & 0x800000) ? 0xFC000000 : 0;
          branch_offset |= (offset<<2);
          uint32_t target = (int32_t)read_address + 8 + branch_offset;

          if(!inline_uncond_imm(thread_data, false, NULL, NULL, &read_address, NULL,
                                target, &inlined_back_count, -1, -1, NULL)) {
            *bb_type = uncond_imm_arm;
          }
#else
          *bb_type = uncond_imm_arm;
#endif
        } else {
          *bb_type = cond_imm_arm;
        }

        break;
      case ARM_BX:
      case ARM_BLX:
        *bb_type = ((*read_address >> 28) == AL) ? uncond_reg_arm : cond_reg_arm;
        break;

      case ARM_BLXI:
        *bb_type = ((*read_address >> 28) == AL) ? uncond_blxi_arm : cond_blxi_arm;
        break;

      case ARM_LDM:
        arm_ldm_decode_fields(read_address, &null, &reglist, &null, &null, &null, &null);

        if (reglist & (1 << pc)) {
          *bb_type = ((*read_address >> 28) == AL) ? uncond_reg_arm : cond_reg_arm;
        }
        break;

      case ARM_LDR:
        arm_ldr_decode_fields(read_address, &null, &rd, &null, &null, &null, &null, &null);

        if (rd == pc) {
          *bb_type = ((*read_address >> 28) == AL) ? uncond_reg_arm : cond_reg_arm;
        }
        break;

      case ARM_ADC:
      case ARM_ADD:
      case ARM_EOR:
      case ARM_MOV:
      case ARM_ORR:
      case ARM_SBC:
      case ARM_SUB:
      case ARM_RSC:
        arm_data_proc_decode_fields(read_address, &null, &null, &null, &rd, &null, &null);
        if (rd == pc) {
          *bb_type = ((*read_address >> 28) == AL) ? uncond_reg_arm : cond_reg_arm;
        }
        break;
    }

    read_address++;
  }
}

void arm_inline_hash_lookup(dbm_thread *thread_data, uint32_t **o_write_p, int basic_block, int r_target) {
  uint32_t *write_p = *o_write_p;
  uint32_t *loop_start;
  uint32_t *branch_miss;

  bool target_reg_clean = (r_target >= r0);
  int target = target_reg_clean ? r_target : r5;
  int r_tmp = target_reg_clean ? r5 : r4;

  if (basic_block != 0) {
    thread_data->code_cache_meta[basic_block].rn = target;
  }

  // MOVW+MOVT r_tmp, hash_mask
  arm_copy_to_reg_32bit(&write_p, r_tmp, CODE_CACHE_HASH_SIZE);

  // MOVW+MOVT r6, hash_table
  arm_copy_to_reg_32bit(&write_p, r6, (uint32_t)thread_data->entry_address.entries);

  // AND r_tmp, target, r_tmp
  arm_and(&write_p, REG_PROC, 0, r_tmp, target, r_tmp);
  write_p++;

  // ADD r_tmp, r6, r_tmp, LSL #3
  arm_add(&write_p, REG_PROC, 0, r_tmp, r6, r_tmp | (LSL << 5) | (3 << 7));
  write_p++;

  // loop:
  loop_start = write_p;

  // LDR r6, [r_tmp], #8
  arm_ldr(&write_p, IMM_LDR, r6, r_tmp, 8, 0, 1, 0);
  write_p++;

  // CMP r6, target
  arm_cmp(&write_p, REG_PROC, r6, target);
  write_p++;

  // BNE miss
  branch_miss = write_p++;

  // jump:
  // LDR r6, [r_tmp, #-4]
  arm_ldr(&write_p, IMM_LDR, r6, r_tmp, 4, 1, 0, 0);
  write_p++;

  // POP {r4}
  arm_pop_reg(r4);

  // BX R6
  arm_bx(&write_p, r6);
  write_p++;

  // miss:
  arm_b32_helper(branch_miss, (uint32_t)write_p, NE);

  // CMP R6, #0
  arm_cmp(&write_p, IMM_PROC, r6, 0);
  write_p++;

  // BNE loop
  arm_b32_helper(write_p, (uint32_t)loop_start, NE);
  write_p++;

  // SUB sp, sp, #8
  arm_sub(&write_p, IMM_PROC, 0, sp, sp, DISP_RES_WORDS*4);
  write_p++;

  // PUSH {r0 - r3}
  arm_push_regs((1 << r0) | (1 << r1) | (1 << r2) | (1 << r3));

  //ADD r3, sp, #24
  arm_add(&write_p, IMM_PROC, 0, r3, sp, DISP_SP_OFFSET);
  write_p++;

  // MOV r0, target
  arm_mov(&write_p, REG_PROC, 0, r0, target);
  write_p++;

  // MOV r1, #bb_id
  arm_copy_to_reg_32bit(&write_p, r1, basic_block);

  // LDMFD R3!, {R4-R6}
  arm_ldm(&write_p, r3, (1 << r4) | (1 << r5) | (1 << r6), 0, 1, 1, 0);
  write_p++;

  // B dispatcher
  arm_b32_helper(write_p, thread_data->dispatcher_addr, AL);
  write_p++;

  *o_write_p = write_p;
}

void arm_ihl_tr_rn_rm(uint32_t **o_write_p, uint32_t *read_address, uint32_t available_regs,
                      enum reg *rn, enum reg *rm, uint32_t *operand2) {
  uint32_t scratch_reg;
  uint32_t *write_p = *o_write_p;

  assert(count_bits(available_regs) >= 2);
  uint32_t sr[2];
  sr[0] = next_reg_in_list(available_regs, 0);
  sr[1] = next_reg_in_list(available_regs, sr[0] + 1);

  assert(*rn != pc || *rm != pc);
  if (*rn == pc || *rm == pc) {
    if (*rn == pc) {
      scratch_reg = (*rm == sr[0]) ? sr[1] : sr[0];
      *rn = scratch_reg;
    } else if (*rm == pc) {
      scratch_reg = (*rn == sr[0]) ? sr[1] : sr[0];
      *rm = scratch_reg;
      *operand2 = (*operand2 & (~0xF)) | *rm;
    }
    arm_copy_to_reg_32bit(&write_p, scratch_reg, (uint32_t)read_address + 8);
  }

  *o_write_p = write_p;
}

size_t scan_a32(dbm_thread *thread_data, uint32_t *read_address, int basic_block, cc_type type, uint32_t *write_p) {
  bool stop = false;

  uint32_t condition_code;
  uint32_t scratch_reg;

  int32_t  branch_offset;
  uint32_t target;
  uint32_t *tr_start;
  uint32_t *start_scan = read_address, *bb_entry = read_address;

  int inlined_back_count = 0;
  
  if (write_p == NULL) {
    write_p = (uint32_t *)&thread_data->code_cache->blocks[basic_block];
  }
  uint32_t start_address = (uint32_t)write_p;

  uint32_t *data_p;
  if (type == mambo_bb) {
    data_p = write_p + BASIC_BLOCK_SIZE;
  } else {
    data_p = (uint32_t *)&thread_data->code_cache->traces + (TRACE_CACHE_SIZE/4);
  }
  
  debug("write_p: %p\n", write_p);
  
  if (type != mambo_trace) {
    arm_pop_regs((1 << r5) | (1 << r6));
  }

#ifdef DBM_TRACES
  branch_type bb_type;
  pass1_arm(thread_data, read_address, &bb_type);
  
  if (type == mambo_bb && bb_type == cond_imm_arm) {
    arm_sub(&write_p, IMM_PROC, 0, sp, sp, 8);
    write_p++;

    arm_push_regs((1 << r0) | (1 << r1) | (1 << r2) | (1 << lr));

    arm_copy_to_reg_32bit(&write_p, r0, basic_block);

    arm_bl32_helper(write_p, thread_data->trace_head_incr_addr-5, AL);
    write_p++;
  }
#endif

  arm_scanner_deliver_callbacks(thread_data, PRE_FRAGMENT_C, &read_address, -1,
                                &write_p, &data_p, basic_block, type, true, &stop);
  arm_scanner_deliver_callbacks(thread_data, PRE_BB_C, &read_address, -1,
                                &write_p, &data_p, basic_block, type, true, &stop);
  
  while(!stop) {
    debug("arm scan read_address: %p\n", read_address);
    arm_instruction inst = arm_decode(read_address);
    debug("Instruction enum: %d\n", (inst == ARM_INVALID) ? -1 : inst);
    
    debug("instruction word: 0x%x\n", *read_address); 
#ifdef PLUGINS_NEW
    bool skip_inst = arm_scanner_deliver_callbacks(thread_data, PRE_INST_C, &read_address, inst,
                                                   &write_p, &data_p, basic_block, type, true, &stop);
    if (!skip_inst) {
#endif

    switch(inst) {
      /* Instructions which are allowed to use the PC */
      case ARM_ADC:
      case ARM_ADD:
      case ARM_EOR:
      case ARM_MOV:
      case ARM_ORR:
      case ARM_SBC:
      case ARM_SUB:
      case ARM_RSC: {
        uint32_t immediate, opcode, set_flags, rd, rn, operand2, rm = reg_invalid;
        arm_data_proc_decode_fields(read_address, &immediate, &opcode, &set_flags, &rd, &rn, &operand2);

        if(rd != pc && rn != pc && (immediate == IMM_PROC || (operand2 & 0xF) != pc)) {
          copy_arm();
        } else {
          if (immediate == REG_PROC) {
            rm = operand2 & 0xF;
          }
          if (rd == pc) {
            assert(rn != sp && rm != sp);
            assert(set_flags == 0);
#ifdef LINK_BX_ALT
            if ((*read_address >> 28) != AL) {
              target = lookup_or_stub(thread_data, (uint32_t)read_address + 4);
              arm_cc_branch(thread_data, write_p, target,
                            arm_inverse_cond_code[*read_address >> 28]);
              write_p++;
            }
#endif

            thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_arm;
            thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;

#ifdef DBM_INLINE_HASH
  #ifndef LINK_BX_ALT
            assert(0);
  #endif
            uint32_t saved_regs = (1 << r4) | (1 << r5) | (1 << r6);

            arm_push_regs(saved_regs);
            arm_ihl_tr_rn_rm(&write_p, read_address, saved_regs, &rn, &rm, &operand2);
            arm_data_proc(&write_p, immediate, opcode, set_flags, r5, rn, operand2);
            write_p++;

            arm_check_free_space(thread_data, &write_p, &data_p, IHL_SPACE, basic_block);
            arm_inline_hash_lookup(thread_data, &write_p, basic_block, -1);

            stop = true;
            break;
#endif
            /* This is an indirect branch */
            arm_branch_save_context(thread_data, &write_p, true);
            arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address, (*read_address >> 28), SETUP);
          }
          if (rn == pc && rm == pc) {
            fprintf(stderr, "Unhandled ARM ADD, etc\n");
            while(1);
          }

          if (rn == pc || rm == pc) {
            /* If rd != PC and rd != rn && rd != rm, we can use rd as a scratch register */
            if (rd == pc || (rn == pc && rm == rd) || (rm == pc && rn == rd)) {
              scratch_reg = r0;
              while ((rm == scratch_reg) || (rn == scratch_reg) || (rd == scratch_reg)) {
                scratch_reg++;
              }
              // In this case the context hasn't been saved, we need to preserve the value of the scratch register
              if (rd != pc) {
                arm_cond_push_reg(*read_address >> 28, scratch_reg);
              } else {
                // r0, r1 (and optionally r2) are saved, but the value of r1 is set by the prev. call to arm_branch_jump
                assert(scratch_reg == r0);
              }
            } else {
              scratch_reg = rd;
            }
            arm_cond_copy_to_reg_32bit(&write_p, *read_address >> 28, scratch_reg, (uint32_t)read_address + 8);
          }

          arm_data_proc_cond(&write_p, (*read_address >> 28), immediate, opcode, set_flags,
                             (rd == pc) ? r0 : rd, (rn == pc) ? scratch_reg : rn,
                             (rm == pc) ? (scratch_reg | (operand2 & 0xFF0)) : operand2);
          write_p++;

          // Restore the value of the scratch register
          if ((rn == pc || rm == pc) && rd != pc && scratch_reg != rd) {
            arm_cond_pop_reg(*read_address >> 28, scratch_reg);
          }

          if (rd == pc) {
            arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address,
                            (*read_address >> 28), INSERT_BRANCH|LATE_APP_SP);
            stop = true;
          }
        }
        break;
      }

      case ARM_B:
      case ARM_BL: {
        uint32_t offset;
        arm_b_decode_fields(read_address, &offset);

        branch_offset = (offset & 0x800000) ? 0xFC000000 : 0;
        branch_offset |= (offset<<2);
        target = (uint32_t)read_address + 8 + branch_offset;
        condition_code = (*read_address >> 28);

        if (inst == ARM_BL) {
          arm_copy_to_reg_32bit(&write_p, lr, (uint32_t)read_address + 4);
        }

#ifdef DBM_INLINE_UNCOND_IMM
        if (condition_code == AL) {
          thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;
          if (!inline_uncond_imm(thread_data, true, &write_p, &data_p, &read_address, &bb_entry,
                                 target, &inlined_back_count, basic_block, type, &stop)) {
            thread_data->code_cache_meta[basic_block].exit_branch_type = trace_inline_max;
            thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
            stop = true;
          }
          break;
        }
#endif

        thread_data->code_cache_meta[basic_block].exit_branch_type = (condition_code == AL) ? uncond_imm_arm : cond_imm_arm;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;
        thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
        thread_data->code_cache_meta[basic_block].branch_skipped_addr = (uint32_t)read_address + 4;
        thread_data->code_cache_meta[basic_block].branch_condition = condition_code;

        if (condition_code != AL) {
          // Reserve space for the conditional branch instruction
          arm_nop(&write_p);
          write_p++;
        }

        arm_simple_exit(thread_data, &write_p, basic_block, offset, read_address, condition_code);
        stop = true;

        break;
      }

      case ARM_BX:
      case ARM_BLX: {
        uint32_t link, rn;
        arm_bx_t_decode_fields(read_address, &link, &rn);
        assert(rn != pc && rn != sp);

#ifdef LINK_BX_ALT
        if ((*read_address >> 28) != AL) {
          debug("w: %p, r: %p, bb: %d\n", write_p, read_address, basic_block);
          target = lookup_or_stub(thread_data, (uint32_t)read_address + 4);
          debug("stub: 0x%x\n", target);
          arm_cc_branch(thread_data,write_p, target,
                        arm_inverse_cond_code[(*read_address >> 28)]);
          write_p++;
        }
#endif // LINK_BX_ALT

        if (inst == ARM_BLX) {
          arm_copy_to_reg_32bit(&write_p, lr, (uint32_t)read_address + 4);
        }
        thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_arm;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;
        
#ifdef DBM_INLINE_HASH
  #ifndef LINK_BX_ALT
    #error LINK_BX_ALT is required
  #endif
        arm_push_regs((1 << r4) | (1 << r5) | (1 << r6));

        if (rn != r5) {
          arm_mov(&write_p, REG_PROC, 0, r5, rn);
          write_p++;
        }

        arm_check_free_space(thread_data, &write_p, &data_p, IHL_SPACE, basic_block);
        arm_inline_hash_lookup(thread_data, &write_p, basic_block, -1);
#else
        arm_branch_save_context(thread_data, &write_p, true);
        arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address, (*read_address >> 28), SETUP);

        // Branch taken (not taken MOV is inserted by arm_branch_jump(SETUP))
        arm_mov_cond(&write_p, (*read_address >> 28), REG_PROC, false, r0, rn);
        write_p++;

        arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address,
                        (*read_address >> 28), INSERT_BRANCH|LATE_APP_SP);
#endif
        stop = true;
        break;
      }
      
      case ARM_BLXI: {
        uint32_t h, offset;
        arm_blxi_decode_fields(read_address, &h, &offset);
        
        branch_offset = ((h << 1) | (offset << 2)) + 1;
        if (branch_offset & 0x2000000) { branch_offset |= 0xFC000000; }

        arm_copy_to_reg_32bit(&write_p, lr, (uint32_t)read_address + 4);
        
        thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_blxi_arm;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;

        arm_simple_exit(thread_data, &write_p, basic_block, 0,
                        (uint32_t *)((uint32_t)read_address + branch_offset), AL);
        stop = true;

        break;
      }
      
      case ARM_LDM: {
        uint32_t rn, registers, prepostindex, updown, writeback, psr;
        arm_ldm_decode_fields(read_address, &rn, &registers, &prepostindex, &updown, &writeback, &psr);
        assert(rn != pc);

        if ((registers & (1 << 15)) == 0) {
          copy_arm();
        } else {
          condition_code = *read_address & 0xF0000000;
#ifdef LINK_BX_ALT
          if ((condition_code >> 28) != AL) {
            target = lookup_or_stub(thread_data, (uint32_t)read_address + 4);
            arm_cc_branch(thread_data, write_p, target,
                          arm_inverse_cond_code[condition_code >> 28]);
            write_p++;
          }
#else
          assert((condition_code >> 28) == AL);
#endif
          if ((1 << rn) & registers) {
            // Handles LDM sp, {*, sp, pc}
            assert(rn == sp && !writeback && !prepostindex);
            if (registers & 0x1FFF) {
              arm_ldm(&write_p, rn, registers & 0x1FFF, prepostindex, updown, writeback, psr);
              write_p++;
            }
          } else {
            assert(writeback);
            if (registers & 0x7FFF) {
              arm_ldm(&write_p, rn, registers & 0x7FFF, prepostindex, updown, writeback, psr);
              write_p++;
            }
          }

          thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_arm;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;

#ifdef DBM_INLINE_HASH
          if ((1 << rn) & registers) {
            // We should adjust the offset if LR is also popped
            assert(((1 << lr) & registers) == 0);

            // PUSH {R0, R1}
            arm_push_regs((1 << r0)|(1 << r1));
            // LDR R0 [SP, offset_to_sp]
            arm_ldr(&write_p, IMM_LDR, r0, sp, (count_bits(registers)-2+2)<<2, 1, updown, writeback);
            write_p++;
            // LDR R1 [SP, offset_to_pc]
            arm_ldr(&write_p, IMM_LDR, r1, sp, (count_bits(registers)-1+2)<<2, 1, updown, writeback);
            write_p++;
            // STMFD R0!, {R4-R6}
            arm_stm(&write_p, r0, (1 << r4) | (1 << r5) | (1 << r6), 1, 0, 1, 0);
            write_p++;
            // MOV R5, R1
            arm_mov(&write_p, 0, 0, r5, r1);
            write_p++;
            // MOV R6, R0
            arm_mov(&write_p, 0, 0, r6, r0);
            write_p++;
            // POP {R0, R1}
            arm_pop_regs((1 << r0)|(1 << r1));
            // MOV SP, R6
            arm_mov(&write_p, 0, 0, sp, r6);
            write_p++;
          } else if (rn == sp) {
            assert(!prepostindex && updown && writeback && !psr);
            arm_push_regs((1 << r4) | (1 << r5));
            arm_ldr(&write_p, IMM_LDR, r5, sp, 8, 1, 1, 0);
            write_p++;
            arm_str(&write_p, IMM_LDR, r6, sp, 8, 1, 1, 0);
            write_p++;
          } else {
            arm_push_regs((1 << r4) | (1 << r5) | (1 << r6));
            arm_ldm(&write_p, rn, 1 << r5, prepostindex, updown, writeback, psr);
            write_p++;
            while(1);
          }

          arm_check_free_space(thread_data, &write_p, &data_p, IHL_SPACE, basic_block);
          arm_inline_hash_lookup(thread_data, &write_p, basic_block, -1);
#else
          // instructions of this type are only supported with inline hash table lookups
          assert((1 << rn) & registers == 0);

          arm_branch_save_context(thread_data, &write_p, false);
          arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address, (*read_address >> 28), SETUP);

          assert(rn != r3);
          if (rn == sp) {
            rn = APP_SP;
          }

          arm_ldm_cond(&write_p, (*read_address >> 28), rn, (1 << r0), prepostindex, updown, writeback, psr);
          write_p++;

          arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address, (*read_address >> 28), INSERT_BRANCH);
#endif
          stop = true;
        }
        break;
      }

      case ARM_LDRB:
      case ARM_LDR: {
        uint32_t immediate, rd, rn, offset, prepostindex, updown, writeback, rm = reg_invalid;

        switch (inst) {
          case ARM_LDRB:
            arm_ldrb_decode_fields(read_address, &immediate, &rd, &rn, &offset, &prepostindex, &updown, &writeback);
            break;
          case ARM_LDR:
            arm_ldr_decode_fields(read_address, &immediate, &rd, &rn, &offset, &prepostindex, &updown, &writeback);
            break;
        }
        if (immediate == LDR_REG) {
          rm = offset & 0xF;
          assert(rm != pc);
        }

        condition_code = *read_address & 0xF0000000;

#ifdef LINK_BX_ALT
        if (rd == pc && (*read_address >> 28) != AL) {
          target = lookup_or_stub(thread_data, (uint32_t)read_address + 4);
          arm_cc_branch(thread_data, write_p, target,
                        arm_inverse_cond_code[*read_address >> 28]);
          write_p++;
        }
#endif

        if (rd == pc || rn == pc) {
          if (rd == pc) {
            assert(inst == ARM_LDR);
            thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_arm;
            thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;

#ifdef DBM_INLINE_HASH
  #ifndef LINK_BX_ALT
            assert(0);
  #endif
            uint32_t saved_regs;
            assert(rm != pc);
            if (rn == sp && (writeback || !prepostindex)) {
              // POP {PC}
              assert(immediate == IMM_LDR && !prepostindex && updown
                       && !writeback && (offset & 3) == 0 && offset >= 4);
              if (offset == 4) {
                arm_push_regs((1 << r4) | (1 << r5));
                arm_ldr(&write_p, IMM_LDR, r5, sp, 8, 1, 1, 0);
                write_p++;
                arm_str(&write_p, IMM_LDR, r6, sp, 8, 1, 1, 0);
                write_p++;
              } else { // offset > 4
                // STR R6, [SP, #offset-4]
                arm_str(&write_p, IMM_LDR, r6, sp, offset-4, 1, 1, 0);
                write_p++;

                // LDR R6, [SP], #offset-4
                arm_ldr(&write_p, IMM_LDR, r6, sp, offset-4, 0, 1, 0);
                write_p++;

                // PUSH {R4, R5}
                arm_push_regs((1 << r4) | (1 << r5));

                // MOV R5, R6
                arm_mov(&write_p, REG_PROC, 0, r5, r6);
                write_p++;
              }
            } else {
              assert((!writeback && prepostindex) || (rn != r4 && rn != r5 && rn != r6 && rn != sp));
              saved_regs =  (1 << r4) | (1 << r5) | (1 << r6);
              arm_push_regs(saved_regs);
              arm_ihl_tr_rn_rm(&write_p, read_address, saved_regs, &rn, &rm, &offset);
              if (rn == sp) {
                assert(rm == reg_invalid && updown);
                offset += 12;
              }
              arm_ldr(&write_p, immediate, r5, rn, offset, prepostindex, updown, writeback);
              write_p++;
            }
            arm_check_free_space(thread_data, &write_p, &data_p, IHL_SPACE, basic_block);
            arm_inline_hash_lookup(thread_data, &write_p, basic_block, -1);

            stop = true;
            break;
#endif
            assert(rm != sp && rn != r3 && rm != r3);

            arm_branch_save_context(thread_data, &write_p, false);
            arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address, (*read_address >> 28), SETUP);
          }
          scratch_reg = r0;
          if (rn == pc) {
            while ((rd != pc && scratch_reg == rd) || (immediate == LDR_REG && ((offset & 0xF) == scratch_reg))) {
              scratch_reg++;
            }
            if (rd != pc) {
              arm_cond_push_reg(condition_code >> 28, scratch_reg);
            }
            arm_cond_copy_to_reg_32bit(&write_p, condition_code >> 28, scratch_reg, (uint32_t)read_address + 8);
          }

          if (rd == pc && rn == sp) {
            rn = APP_SP;
          }

          switch (inst) {
            case ARM_LDRB:
              arm_ldrb_cond(&write_p, (condition_code >> 28), immediate, (rd == pc) ? r0 : rd, (rn == pc) ? scratch_reg : rn, offset, prepostindex, updown, writeback);
              break;
            case ARM_LDR:
              arm_ldr_cond(&write_p, (condition_code >> 28), immediate, (rd == pc) ? r0 : rd, (rn == pc) ? scratch_reg : rn, offset, prepostindex, updown, writeback);
              break;
          }
          *write_p |= condition_code;
          write_p++;
          
          // TODO: fixme
          /* LDR is used to load a value into PC, with writeback. 
             If the base register is R0, R1, or R2, it would have 
             already been saved before it is written back. 
             A proper fix should refactor the branching code to save */
          if (rd == pc && writeback) {
            switch (rn) {
              case r0:
              case r1:
              case r2:
                fprintf(stderr, "LDR with writeback");
                while(1);
                break;
            }
          }
            
          if (rd == pc) {
            arm_branch_jump(thread_data, &write_p, basic_block, 0, read_address, (*read_address >> 28), INSERT_BRANCH);
            stop = true;
          } else {
            arm_cond_pop_reg(condition_code >> 28, scratch_reg);
          }
        } else {
          copy_arm();
        }
        break;
      }

      case ARM_STM: {
        uint32_t rn, registers, prepostindex, updown, writeback, psr, offset;
        arm_stm_decode_fields(read_address, &rn, &registers, &prepostindex, &updown, &writeback, &psr);
        assert(rn != pc);

        if (registers & (1 << pc)) {
          /* Example :            STMFD SP!, {SP, LR, PC}

             is translated to:    STMFD SP!, {SP, LR, PC}
                                  MOW LR, #(spc & 0xFFFF)
                                  MOVT LR, #(spc >> 16)
                                  STR LR, [SP, #8]
                                  LDR LR, [SP, #4]

             Note that if rn is in the reglist, then its value must be stored first.
             This example was encountered in libgcc.
          */
          condition_code = (*read_address >> 28);
          if (condition_code != AL) {
            tr_start = write_p;
            write_p++;
          }

          scratch_reg = ~((1 << pc) | (1 << rn)) & registers;
          assert(scratch_reg);
          copy_arm();

          // Overwrite the saved TPC
          scratch_reg = next_reg_in_list(scratch_reg, 0);
          assert(scratch_reg < pc && scratch_reg != rn);
          arm_copy_to_reg_32bit(&write_p, scratch_reg, (uint32_t)read_address + 8);
          if (writeback) {
            offset = (count_bits(registers) - 1) << 2;
          }
          if (!prepostindex) {
            offset -= 4;
          }
          arm_str(&write_p, IMM_LDR, scratch_reg, rn, offset, 1, 1, 0);
          write_p++;

          // Calculate the offset of the location where the SR was saved
          offset = (scratch_reg > rn && (registers & (1 << rn))) ? 4 : 0;
          offset += prepostindex ? 0 : 4;

          arm_ldr(&write_p, IMM_LDR, scratch_reg, rn, offset, 1, updown ? 0 : 1, 0);
          write_p++;

          while (!writeback || !prepostindex || updown); // implement this

          if (condition_code != AL) {
            arm_b32_helper(tr_start, (uint32_t)write_p, arm_inverse_cond_code[condition_code]);
          }

          while(rn != sp); // Check me
        } else {
          copy_arm();
        }
        break;
      }
        
      case ARM_STRB:
      case ARM_STR: {
        uint32_t immediate, rd, rn, offset, prepostindex, updown, writeback;
        switch (inst) {
          case ARM_STRB:
            arm_strb_decode_fields(read_address, &immediate, &rd, &rn, &offset, &prepostindex, &updown, &writeback);
            break;
          case ARM_STR:
            arm_str_decode_fields(read_address, &immediate, &rd, &rn, &offset, &prepostindex, &updown, &writeback);
            break;
        }
        if (immediate == 1) assert((offset & 0xF) != pc);
        
        if (rd == pc) {
          condition_code = (*read_address & 0xF0000000) >> 28;
          assert(condition_code == AL && rn == sp & prepostindex && !updown && writeback); // PUSH {PC}

          // SUB SP, SP, #8
          arm_add_sub_32_bit(&write_p, sp, sp, -8);

          // STR R0, [SP, #0]
          arm_str(&write_p, IMM_LDR, r0, sp, 0, 1, 1, 0);
          write_p++;

          // MOV{W,T} R0, addr
          arm_copy_to_reg_32bit(&write_p, r0, (uint32_t)read_address + 8);

          // STR R0, [SP, #4]
          arm_str(&write_p, IMM_LDR, r0, sp, 4, 1, 1, 0);
          write_p++;

          // POP {R0}
          arm_pop_reg(r0);
        } else if(rn == pc) {
          condition_code = *read_address & 0xF0000000;

          scratch_reg = r0;
          while (rd == scratch_reg || (offset & 0xF) == scratch_reg) {
            scratch_reg++;
          }

          arm_cond_push_reg(condition_code >> 28, scratch_reg);
          arm_cond_copy_to_reg_32bit(&write_p, condition_code >> 28, scratch_reg, (uint32_t)read_address + 8);
          
          switch (inst) {
            case ARM_STRB:
              arm_strb_cond(&write_p, (condition_code >> 28), immediate, rd, scratch_reg, offset, prepostindex, updown, writeback);
              break;
            case ARM_STR:
              arm_str_cond(&write_p, (condition_code >> 28), immediate, rd, scratch_reg, offset, prepostindex, updown, writeback);
              break;
          }
          *write_p |= condition_code;
          write_p++;
          
          arm_cond_pop_reg(condition_code >> 28, scratch_reg);
        } else {
          copy_arm();
        }
        break;
      }

      /* Other translated sensitive instructions */
      case ARM_MCR:
      case ARM_MRC: {
        uint32_t opc1, load_store, crn, rd, coproc, opc2, crm;
        arm_coproc_trans_decode_fields(read_address, &opc1, &load_store, &crn, &rd, &coproc, &opc2, &crm);
        
        // thread id
        if (coproc == 15 && opc1 == 0 && crn == 13 && crm == 0 && opc2 == 3 && load_store == 1 && rd != pc) {
          condition_code = (*read_address >> 28);
          if (condition_code != AL) {
            tr_start = write_p++;
          }
          arm_copy_to_reg_32bit(&write_p, rd, (uint32_t)(&thread_data->tls));
          arm_ldr(&write_p, IMM_LDR, rd, rd, 0, 1, 1, 0);
          write_p++;
          if (condition_code != AL) {
            arm_b32_helper(tr_start, (uint32_t)write_p, condition_code ^ 1);
          }

        // NEON / FP VMRS/VMSR
        } else if (coproc == 10 && opc1 == 7 && crn == 1 && rd != pc) {
          copy_arm();
        // Performance counter
        // This is used in OpenSSL in OPENSSL_cpuid_setup, a constructor. WTF
        } else if (coproc == 15 && opc1 == 0 && crn == 9 && crm == 13 && opc2 == 0 && load_store == 1 && rd != pc) {
          copy_arm();

        // Data memory barrier operation, deprecated in ARMv7-a in favor of
        // dmb(). This is used in Raspbian Jessie's libc6, which is apparently
        // compiled for ARMv6 due to compatibility reasons. Section B3.12.33 in
        // page B3-136 of ARM DDI 0406B has more information on this mcr op.
        } else if (coproc == 15 && opc1 == 0 && crn == 7 && crm == 10 && opc2 == 5 && load_store == 0 && rd != pc) {
          copy_arm();

        } else {
          fprintf(stderr, "unknown coproc: %d %d %d %d %d %d\n", opc1, crn, rd, coproc, opc2, crm);
          while(1);
        }
        break;
      }

      case ARM_SVC: {
        condition_code = (*read_address >> 28) & 0xF;

        if (condition_code != AL) {
          tr_start = write_p;
          write_p++;
        }

        arm_sub(&write_p, IMM_PROC, 0, sp, sp, 8);
        write_p++;

        // PUSH {R0-R12, R14}
        arm_push_regs(0x5FFF);

        arm_copy_to_reg_32bit(&write_p, r8, (uint32_t)read_address + 4);

        arm_bl(&write_p, (thread_data->syscall_wrapper_addr - (uint32_t)write_p - 8) >> 2);
        write_p++;

        if (condition_code != AL) {
          arm_b32_helper(tr_start, (uint32_t)write_p, condition_code);
        }

        arm_scanner_deliver_callbacks(thread_data, POST_BB_C, &bb_entry, -1,
                                &write_p, &data_p, basic_block, type, false, &stop);
        // set the correct address for the PRE_BB_C event
        read_address++;
        bb_entry = read_address;
        arm_scanner_deliver_callbacks(thread_data, PRE_BB_C, &read_address, -1,
                                &write_p, &data_p, basic_block, type, true, &stop);
        read_address--;
        break;
      }

      /* Instructions which could access the PC, but shouldn't. */
      case ARM_AND:
      case ARM_BIC:
      case ARM_RSB: {
        uint32_t immediate, set_flags, rd, rn, operand2;
        arm_and_decode_fields(read_address, &immediate, &set_flags, &rd, &rn, &operand2);
        assert(rd != pc && rn != pc);
        if (immediate == 0) assert((operand2 & 0xF) != pc);
        copy_arm();
        break;
      }

      case ARM_CMN:
      case ARM_CMP:
      case ARM_TEQ:
      case ARM_TST: {
        uint32_t immediate, rn, operand2;
        arm_cmn_decode_fields(read_address, &immediate, &rn, &operand2);
        assert(rn != pc);
        if (immediate == 0) assert((operand2 & 0xF) != pc);
        copy_arm();
        break;
      }

      case ARM_MOVW:
      case ARM_MOVT: {
        uint32_t immediate, opcode, set_flags, rd, rn, operand2;
        arm_data_proc_decode_fields(read_address, &immediate, &opcode, &set_flags, &rd, &rn, &operand2);
        // Rn is actually the top 4 bits of the immediate value
        assert(rd != pc);
        copy_arm();
        break;
      }

      case ARM_MVN: {
        uint32_t immediate, set_flags, rd, operand2;
        arm_mvn_decode_fields(read_address, &immediate, &set_flags, &rd, &operand2);
        assert(rd != pc);
        if (immediate == REG_PROC) assert((operand2 & 0xF) != pc);
        copy_arm();
        break;
      }

      case ARM_MUL:
      case ARM_MLA:
      case ARM_MLS: {
        uint32_t accumulate, set_flags, rd, rm, rs, rn;
        arm_multiply_decode_fields(read_address, &accumulate, &set_flags, &rd, &rm, &rs, &rn);
        assert(rd != pc && rm != pc && rs != pc && rn != pc);
        copy_arm();
        break;
      }

      case ARM_LDRD:
      case ARM_LDRH:
      case ARM_LDRHT:
      case ARM_LDRSB:
      case ARM_LDRSBT:
      case ARM_LDRSH:
      case ARM_LDRSHT:
      case ARM_STRD:
      case ARM_STRH:
      case ARM_STRHT: {
        uint32_t opcode, size, opcode2, immediate, rd, rn, rm, imm4h, prepostindex, updown, writeback;
        arm_h_data_transfer_decode_fields(read_address, &opcode, &size, &opcode2, &immediate,
                                          &rd, &rn, &rm, &imm4h, &prepostindex, &updown, &writeback);
        assert(rd != pc);
        if (immediate == REG_PROC) assert(rm != pc);
        if (rn == pc) {
          assert(inst != ARM_STRD && inst != ARM_STRH && inst != ARM_STRHT);
          assert(prepostindex && !writeback);
          condition_code = *read_address >> 28;
          arm_cond_copy_to_reg_32bit(&write_p, condition_code, rd, (uint32_t)read_address + 8);
          arm_h_data_transfer_cond(&write_p, condition_code, opcode, size, opcode2, immediate, rd,
                                   rd, rm, imm4h, prepostindex, updown, writeback);
          write_p++;
        } else {
          copy_arm();
        }
        break;
      }

      case ARM_LDREX:
      case ARM_LDREXB:
      case ARM_LDREXD:
      case ARM_LDREXH: {
        uint32_t rd, rn;
        arm_ldrex_decode_fields(read_address, &rd, &rn);
        assert(rd != pc && rn != pc);
        copy_arm();
        break;
      }

      case ARM_STREX:
      case ARM_STREXB:
      case ARM_STREXD:
      case ARM_STREXH: {
        uint32_t rd, rn, rm;
        arm_strex_decode_fields(read_address, &rd, &rn, &rm);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_REV:
      case ARM_REV16:
      case ARM_CLZ: {
        uint32_t opcode, size, opcode2, immediate, rd, rn, rm, imm4h, prepostindex, updown, writeback;
        arm_h_data_transfer_decode_fields(read_address, &opcode, &size, &opcode2, &immediate, &rd, &rn, &rm, &imm4h, &prepostindex, &updown, &writeback);
        assert(rd != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_PLD: {
        uint32_t imm, updown, readonly, rn, operand2, rm = reg_invalid;
        arm_pld_decode_fields(read_address, &imm, &updown, &readonly, &rn, &operand2);
        if (imm == LDR_REG) {
          rm = operand2 & 0xF;
        }
        /* The cost of obtaining a scratch register and copying the source PC
             is *probably* too high to be worth translating this. */
        if (rn != pc && rm != pc ) {
          copy_arm();
        }
        break;
      }

      case ARM_STC: {
        uint32_t p, updown, d, writeback, load_store, rn, vd, opcode, immediate;
        arm_vfp_ldm_stm_decode_fields(read_address, &p, &updown, &d, &writeback, &load_store, &rn, &vd, &opcode, &immediate);
        assert(rn != pc);
        copy_arm();
        break;
      }

      case ARM_CDP: {
        uint32_t opc1, crn, crd, coproc, opc2, crm;
        arm_coproc_dp_decode_fields(read_address, &opc1, &crn, &crd, &coproc, &opc2, &crm);
        copy_arm();
        fprintf(stderr, "Untested CDP\n");
        while(1);
        break;
      }
        
      case ARM_UMLAL:
      case ARM_UMULL:
      case ARM_SMULL:
      case ARM_SMLAL: {
        uint32_t opcode, set_flags, rdhi, rdlo, rm, opcode2, setting, rn;
        arm_dsp_long_res_decode_fields(read_address, &opcode, &set_flags, &rdhi, &rdlo, &rm, &opcode2, &setting, &rn);
        assert(rdhi != pc && rdlo != pc && rm != pc && rn != pc);
        copy_arm();
        break;
      }
        
      case ARM_MRS: {
        uint32_t rd;
        arm_mrs_decode_fields(read_address, &rd);
        assert(rd != pc);
        copy_arm();
        break;
      }

      case ARM_MSR: {
        uint32_t rn, mask;
        arm_msr_decode_fields(read_address, &rn, &mask);
        assert(rn != pc);
        copy_arm();
        break;
      }

      case ARM_UDIV:
      case ARM_SDIV: {
        uint32_t opcode, rd, rn, rm;
        arm_divide_decode_fields(read_address, &opcode, &rd, &rn, &rm);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_SXTB:
      case ARM_SXTH:
      case ARM_SXTAH:
      case ARM_UXTB:
      case ARM_UXTB16:
      case ARM_UXTH:
      case ARM_UXTAH:
      case ARM_UXTAB:
      case ARM_UXTAB16: {
        uint32_t opcode, rd, rn, rm, rotate;
        arm_extend_decode_fields(read_address, &opcode, &rd, &rn, &rm, &rotate);
        // if rn == pc, it's the version without add which doesn't use pc
        assert(rd != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_LDRBT:
      case ARM_LDRT:
      case ARM_STRBT:
      case ARM_STRT: {
        uint32_t immediate, rd, rn, updown, operand2;
        arm_ldrt_decode_fields(read_address, &immediate, &rd, &rn, &updown, &operand2);
        assert(rd != pc && rn != pc);
        if (immediate == LDR_REG) assert((operand2 & 0xF) != pc);
        copy_arm();
        break;
      }

      case ARM_BFI: {
        uint32_t rd, rn, lsb, msb;
        arm_bfi_decode_fields(read_address, &rd, &rn, &lsb, &msb);
        assert(rd != pc && rn != pc);
        copy_arm();
        break;
      }

      case ARM_UBFX:
      case ARM_SBFX: {
        uint32_t rd, rn, lsb, width;
        arm_ubfx_decode_fields(read_address, &rd, &rn, &lsb, &width);
        assert(rd != pc && rn != pc);
        copy_arm();
        break;
      }

      case ARM_UQSUB8: {
        uint32_t rd, rn, rm;
        arm_uqsub8_decode_fields(read_address, &rd, &rn, &rm);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_BFC: {
        uint32_t rd, lsb, msb;
        arm_bfc_decode_fields(read_address, &rd, &lsb, &msb);
        assert(rd != pc);
        copy_arm();
        break;
      }

      case ARM_MRRC: {
        uint32_t coproc, opc1, rd, rd2, crm;
        arm_mrrc_decode_fields(read_address, &coproc, &opc1, &rd, &rd2, &crm);
        assert(rd != pc && rd2 != pc);
        copy_arm();
        break;
      }

      case ARM_UMAAL: {
        uint32_t rd, rd2, rn, rm;
        arm_umaal_decode_fields(read_address, &rd, &rd2, &rn, &rm);
        assert(rd != pc && rd2 != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_SMULBB:
      case ARM_SMULTT: {
        uint32_t rd, rn, rm;
        arm_smulbb_decode_fields(read_address, &rd, &rn, &rm);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_SMULWB:
      case ARM_SMULWT: {
        uint32_t rd, rn, rm;
        arm_smulwb_decode_fields(read_address, &rd, &rn, &rm);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_RBIT: {
        uint32_t rd, rm;
        arm_rbit_decode_fields(read_address, &rd, &rm);
        assert(rd != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_SMLABB: {
        uint32_t rd, rn, rm, ra;
        arm_smlabb_decode_fields(read_address, &rd, &rn, &rm, &ra);
        assert(rd != pc && rn != pc && rm != pc && ra != pc);
        copy_arm();
        break;
      }

      case ARM_SMLAWB:
      case ARM_SMLAWT: {
        uint32_t rd, rn, rm, ra;
        arm_smlawb_decode_fields(read_address, &rd, &rn, &rm, &ra);
        assert(rd != pc && rn != pc && rm != pc && ra != pc);
        copy_arm();
        break;
      }

      case ARM_PKH: {
        uint32_t rd, rn, rm, tb, imm5;
        arm_pkh_decode_fields(read_address, &rd, &rn, &rm, &tb, &imm5);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_SSAT:
      case ARM_USAT: {
        uint32_t rd, sat_imm, rn, sh, imm5;
        arm_usat_decode_fields(read_address, &rd, &sat_imm, &rn, &sh, &imm5);
        assert(rd != pc && rn != pc);
        copy_arm();
        break;
      }

      case ARM_USAT16: {
        uint32_t rd, sat_imm, rn;
        arm_usat16_decode_fields(read_address, &rd, &sat_imm, &rn);
        assert(rd != pc && rn != pc);
        copy_arm();
        break;
      }

      case ARM_UADD8:
      case ARM_UQADD8: {
        uint32_t rd, rn, rm;
        arm_uadd8_decode_fields(read_address, &rd, &rn, &rm);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_SEL: {
        uint32_t rd, rn, rm;
        arm_sel_decode_fields(read_address, &rd, &rn, &rm);
        assert(rd != pc && rn != pc && rm != pc);
        copy_arm();
	      break;
      }

      case ARM_RRX: {
        uint32_t set_flags, rd, rm;
        arm_rrx_decode_fields(read_address, &set_flags, &rd, &rm);
        assert(rd != pc && rm != pc);
        copy_arm();
        break;
      }

      case ARM_UDF:
        if (start_scan == read_address) {
          copy_arm();
        } else {
          thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_imm_arm;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = (uint16_t *)write_p;
          thread_data->code_cache_meta[basic_block].branch_taken_addr = (uint32_t)read_address;

          arm_simple_exit(thread_data, &write_p, basic_block, -2, read_address, AL);
          stop = true;
        }
        break;

      /* ARM instructions which can be copied directly */
      case ARM_BKPT:
      case ARM_CLREX:
      case ARM_DMB:
      case ARM_DSB:
      case ARM_ISB:
      case ARM_MSRI:
      case ARM_NOP:
        copy_arm();
        break;

      /* Discarded ARM instructions */
      case ARM_PLII:
        /* Discard instruction preload hints, since they would otherwise only pollute our icache */
        break;

      /* NEON and VFP instructions which might access the PC */
      case ARM_VFP_VSTM_DP:
      case ARM_VFP_VSTM_SP:
      case ARM_VFP_VLDM_SP:
      case ARM_VFP_VLDM_DP:
      case ARM_VFP_VSTR_DP:
      case ARM_VFP_VSTR_SP:
      case ARM_VFP_VLDR_DP:
      case ARM_VFP_VLDR_SP: {
        uint32_t p, updown, d, writeback, load_store, rn, vd, opcode, immediate;
        arm_vfp_ldm_stm_decode_fields(read_address, &p, &updown, &d, &writeback, &load_store, &rn, &vd, &opcode, &immediate);

        if (rn == pc) {
          assert(writeback == 0);

          condition_code = *read_address & 0xF0000000;
          arm_cond_push_reg(condition_code >> 28, r0);

          arm_cond_copy_to_reg_32bit(&write_p, condition_code >> 28, r0, (uint32_t)read_address + 8);
          arm_vfp_ldm_stm_cond(&write_p, (*read_address) >> 28, p, updown, d, writeback, load_store, r0, vd, opcode, immediate);
          write_p++;

          arm_cond_pop_reg(condition_code >> 28, r0);
        } else {
          copy_arm();
        }
        break;
      }

      case ARM_NEON_VLDX_M:
      case ARM_NEON_VLDX_S_O:
      case ARM_NEON_VLDX_S_A:
      case ARM_NEON_VSTX_M:
      case ARM_NEON_VSTX_S_O: {
        uint32_t opcode, opcode2, opcode3, opcode4, params, d, vd, rn, rm;
        arm_v_trans_mult_decode_fields(read_address, &opcode, &opcode2, &opcode3, &opcode4, &params, &d, &vd, &rn, &rm);
        assert(rn != pc); // rm is guaranteed not to be pc
        copy_arm();
        break;
      }

      case ARM_VFP_VMOV_2CORE_DP: {
        uint32_t opcode, rd, rd2, m, vm;
        arm_vfp_vmov_2core_dp_decode_fields(read_address, &opcode, &rd, &rd2, &m, &vm);
        assert(rd != pc && rd2 != pc); // rm is guaranteed not to be pc
        copy_arm();
        break;
      }

      case ARM_VFP_VMOV_CORE_SCAL: {
        uint32_t d, vd, opcode, opcode2, rd;
        arm_vfp_vmov_core_scal_decode_fields(read_address, &d, &vd, &opcode, &opcode2, &rd);
	      assert(rd != pc);
	      copy_arm();
        break;
      }

      case ARM_VFP_VMOV_CORE_SP: {
        uint32_t opcode, rd, n, vn;
        arm_vfp_vmov_core_sp_decode_fields(read_address, &opcode, &rd, &n, &vn);
        assert(rd != pc);
        copy_arm();
        break;
      }

      case ARM_NEON_VDUP_CORE: {
        uint32_t b, e, q, d, vd, rd;
        arm_neon_vdup_core_decode_fields(read_address, &b, &e, &q, &d, &vd, &rd);
        assert(rd != pc);
        copy_arm();
        break;
      }
        
      case ARM_VFP_VMOV_SCAL_CORE: {
        uint32_t opcode, rd, n, vn, opcode2, opcode3;
        arm_vfp_vmov_scal_core_decode_fields(read_address, &opcode, &rd, &n, &vn, &opcode2, &opcode3);
        assert(rd != pc);
        copy_arm();
        break;
      }

      case ARM_VFP_VMSR: {
        uint32_t rd;
        arm_vfp_vmsr_decode_fields(read_address, &rd);
        assert(rd != pc);
        copy_arm();
        break;
      }

      /* NEON and VFP instructions which can't access the PC */
      case ARM_NEON_VABD_I:
      case ARM_NEON_VABDL:
      case ARM_NEON_VABS:
      case ARM_NEON_VADD_F:
      case ARM_NEON_VADD_I:
      case ARM_NEON_VADDL:
      case ARM_NEON_VADDW:
      case ARM_NEON_VAND:
      case ARM_NEON_VBIC:
      case ARM_NEON_VBICI:
      case ARM_NEON_VBSL:
      case ARM_NEON_VCEQ_I:
      case ARM_NEON_VCEQZ:
      case ARM_NEON_VCGE_F:
      case ARM_NEON_VCGE_I:
      case ARM_NEON_VCGEZ:
      case ARM_NEON_VCGT_F:
      case ARM_NEON_VCGT_I:
      case ARM_NEON_VCGTZ:
      case ARM_NEON_VCLEZ:
      case ARM_NEON_VCLTZ:
      case ARM_NEON_VCNT:
      case ARM_NEON_VCVT_F_FP:
      case ARM_NEON_VCVT_F_I:
      case ARM_NEON_VDUP_SCAL:
      case ARM_NEON_VEOR:
      case ARM_NEON_VEXT:
      case ARM_NEON_VHADD:
      case ARM_NEON_VMAX_I:
      case ARM_NEON_VMIN_I:
      case ARM_NEON_VMLA_F:
      case ARM_NEON_VMLA_I:
      case ARM_NEON_VMLAL_I:
      case ARM_NEON_VMLAL_SCAL:
      case ARM_NEON_VMLA_SCAL:
      case ARM_NEON_VMLS_F:
      case ARM_NEON_VMLSL_I:
      case ARM_NEON_VMLSL_SCAL:
      case ARM_NEON_VMLS_SCAL:
      case ARM_NEON_VMOVI:
      case ARM_NEON_VMOVL:
      case ARM_NEON_VMOVN:
      case ARM_NEON_VMUL_F:
      case ARM_NEON_VMUL_I:
      case ARM_NEON_VMULL_I:
      case ARM_NEON_VMULL_SCAL:
      case ARM_NEON_VMUL_SCAL:
      case ARM_NEON_VMVN:
      case ARM_NEON_VMVNI:
      case ARM_NEON_VNEG:
      case ARM_NEON_VORN:
      case ARM_NEON_VORR:
      case ARM_NEON_VORRI:
      case ARM_NEON_VPADD_F:
      case ARM_NEON_VPADD_I:
      case ARM_NEON_VPADDL:
      case ARM_NEON_VQADD:
      case ARM_NEON_VQDMULH_I:
      case ARM_NEON_VQDMULH_SCAL:
      case ARM_NEON_VQMOVUN:
      case ARM_NEON_VQRSHRN:
      case ARM_NEON_VQRSHRUN:
      case ARM_NEON_VQSHRN:
      case ARM_NEON_VQSHRUN:
      case ARM_NEON_VQSUB:
      case ARM_NEON_VREV32:
      case ARM_NEON_VREV64:
      case ARM_NEON_VRHADD:
      case ARM_NEON_VRSHL:
      case ARM_NEON_VRSHR:
      case ARM_NEON_VRSHRN:
      case ARM_NEON_VRSRA:
      case ARM_NEON_VSHL:
      case ARM_NEON_VSHLI:
      case ARM_NEON_VSHLL:
      case ARM_NEON_VSHLL2:
      case ARM_NEON_VSHR:
      case ARM_NEON_VSHRN:
      case ARM_NEON_VSLI:
      case ARM_NEON_VSRA:
      case ARM_NEON_VSUB_F:
      case ARM_NEON_VSUB_I:
      case ARM_NEON_VSUBL:
      case ARM_NEON_VSUBW:
      case ARM_NEON_VSWP:
      case ARM_NEON_VTRN:
      case ARM_NEON_VTST:
      case ARM_NEON_VUZP:
      case ARM_NEON_VZIP:
      case ARM_VFP_VABS:
      case ARM_VFP_VADD:
      case ARM_VFP_VCMP:
      case ARM_VFP_VCMPE:
      case ARM_VFP_VCMPEZ:
      case ARM_VFP_VCMPZ:
      case ARM_VFP_VCVT_DP_SP:
      case ARM_VFP_VCVT_F_FP:
      case ARM_VFP_VCVT_F_I:
      case ARM_VFP_VDIV:
      case ARM_VFP_VFMA:
      case ARM_VFP_VFMS:
      case ARM_VFP_VFNMS:
      case ARM_VFP_VMLA_F:
      case ARM_VFP_VMLS_F:
      case ARM_VFP_VMOV:
      case ARM_VFP_VMOVI:
      case ARM_VFP_VMRS:
      case ARM_VFP_VMUL_F:
      case ARM_VFP_VNEG:
      case ARM_VFP_VNMLA:
      case ARM_VFP_VNMLS:
      case ARM_VFP_VNMUL:
      case ARM_VFP_VPOP_DP:
      case ARM_VFP_VPOP_SP:
      case ARM_VFP_VPUSH_DP:
      case ARM_VFP_VPUSH_SP:
      case ARM_NEON_VRADDHN:
      case ARM_VFP_VSQRT:
      case ARM_VFP_VSUB_F:
        copy_arm();
        break;

      default:
        fprintf(stderr, "Unknown arm instruction: %d at %p\n", inst, read_address);
        while(1);
        exit(EXIT_FAILURE);
    }
#ifdef PLUGINS_NEW
    } // if (!skip_inst)
#endif

    if (write_p >= data_p) {
      printf("w: %p r: %p\n", write_p, data_p);
    }
    assert (write_p < data_p);

    if (!stop) arm_check_free_space(thread_data, &write_p, &data_p, MIN_FSPACE, basic_block);

#ifdef PLUGINS_NEW
    arm_scanner_deliver_callbacks(thread_data, POST_INST_C, &read_address, inst, &write_p, &data_p, basic_block, type, !stop, &stop);
#endif

    debug("write_p: %p\n", write_p);

    read_address++;
    debug("\n");
  }

  arm_scanner_deliver_callbacks(thread_data, POST_BB_C, &bb_entry, -1,
                                &write_p, &data_p, basic_block, type, false, &stop);
  arm_scanner_deliver_callbacks(thread_data, POST_FRAGMENT_C, &start_scan, -1,
                                &write_p, &data_p, basic_block, type, false, &stop);

  // We haven't strictly enforced updating write_p after the last instruction
  return ((uint32_t)write_p - start_address + 4);
}

void arm_encode_stub_bb(dbm_thread *thread_data, int basic_block, uint32_t target) {
  uint32_t *write_p = (uint32_t *)&thread_data->code_cache->blocks[basic_block];
  uint32_t *data_p = (uint32_t *)write_p;
  data_p += BASIC_BLOCK_SIZE;

  debug("Stub BB: %p\n", write_p);
  debug("ARM stub target: 0x%x\n", target);

  arm_pop_regs((1 << r5) | (1 << r6));

  arm_simple_exit(thread_data, &write_p, basic_block, 0, (uint32_t *)(target - 8), AL);
}

#endif // __arm__

```

`arch/aarch32/scanner_t32.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef __arm__
#include <stdlib.h>
#include <stdio.h>
#include <stdint.h>
#include <stdbool.h>
#include <assert.h>
#include <limits.h>
#include <string.h>

#include "../../dbm.h"
#include "../../common.h"
#include "../../scanner_common.h"

#include "../../pie/pie-thumb-decoder.h"
#include "../../pie/pie-thumb-encoder.h"
#include "../../pie/pie-thumb-field-decoder.h"

#include "../../pie/pie-arm-decoder.h"
#include "../../pie/pie-arm-encoder.h"
#include "../../pie/pie-arm-field-decoder.h"

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

#define MIN_FSPACE (60)
#define IHL_FSPACE (76)

#define copy_thumb_16() *(write_p++) = *read_address;
#define copy_thumb_32() *(write_p++) = *read_address;\
        *(write_p++) = *(read_address + 1)

#define get_original_pc() (((uint32_t)read_address + 4) & 0xFFFFFFFC)
  
#define modify_in_it_pre(skip_size) \
  if (it_state.cond_inst_after_it > 0) { \
          debug("LDR_PC_16 in IT block\n"); \
          if (write_p == it_state.it_inst_addr + 1) { \
            write_p--; \
          } else { \
            debug("LDR_PC_16 in middle of IT block\n"); \
            int inst_to_keep = it_get_no_of_inst(it_state.it_initial_mask) - it_state.cond_inst_after_it; \
            debug("inst to keep in pre: %d\n", inst_to_keep);\
            switch (inst_to_keep) { \
              case 1: \
                it_state.it_initial_mask = 0x8; \
                break; \
              case 2: \
                it_state.it_initial_mask = it_state.it_initial_mask & 0x8 | 0x4; \
                break; \
              case 3: \
                it_state.it_initial_mask = it_state.it_initial_mask & 0xc | 0x2; \
                break; \
              case 4: \
                it_state.it_initial_mask = it_state.it_initial_mask & 0xe | 0x1; \
                break; \
              default: \
                fprintf(stderr, "check me\n"); \
                while(1); \
            } \
            debug("inst to keep: %d\n", inst_to_keep); \
            thumb_it16 (&it_state.it_inst_addr, it_state.it_cond, it_state.it_initial_mask); \
          } \
          /* Encode as: \
             B OP_COND +5 \
             STUFF \
             IT samecond, len -1*/ \
          bool same_cond = (((it_state.it_mask >> 5) & 0x1) == (it_state.it_cond & 1)); \
          debug("it_mask: 0x%x, it_cond: %d, same_cond: %d\n", it_state.it_mask, it_state.it_cond, same_cond); \
          thumb_cond_branch_16 (&write_p, same_cond ? arm_inverse_cond_code[it_state.it_cond] : it_state.it_cond, skip_size); \
          write_p++; \
        }

#define modify_in_it_post() \
  /* Insert IT after the translated instruction */\
        if (it_state.cond_inst_after_it > 1) { \
          debug("cond inst following: %d\n", it_state.cond_inst_after_it-1); \
          it_state.it_initial_mask = it_state.it_mask & 0xF; \
          bool same_cond = (((it_state.it_mask >> 4) & 0x1) == (it_state.it_cond & 1)); \
          if (!same_cond) { \
            assert(it_state.it_cond < 14); \
            switch (it_state.cond_inst_after_it-1) { \
              case 3: \
                it_state.it_initial_mask = it_state.it_initial_mask & 0xC | 0x2; \
                break; \
              case 2: \
                it_state.it_initial_mask = it_state.it_initial_mask & 0x8 | 0x4; \
                break; \
              case 1:  \
                it_state.it_initial_mask = it_state.it_initial_mask & 0x0 | 0x8; \
                break; \
            } \
            it_state.it_cond = arm_inverse_cond_code[it_state.it_cond]; \
          } \
          thumb_it16 (&write_p,	it_state.it_cond, it_state.it_initial_mask); \
          it_state.it_inst_addr = write_p; \
          write_p++; \
        }

typedef struct {
  int cond_inst_after_it;
  uint16_t *it_inst_addr;
  uint32_t it_cond;
  uint32_t it_mask;
  uint32_t it_initial_mask;
  bool is_overwritten;
} thumb_it_state;

int it_get_no_of_inst(uint32_t mask) {
  int cond_inst_after_it = 1;

  if (mask & 1) {
    cond_inst_after_it = 4;
  } else if (mask & 2) {
    cond_inst_after_it = 3;
  } else if (mask & 4) {
    cond_inst_after_it = 2;
  }
  return cond_inst_after_it;
}

void it_clip_len(uint16_t *write_p, uint32_t cond, uint32_t mask, int it_len) {
  switch (it_len) {
    case 1:
      mask = 0x8;
      break;
    case 2:
      mask = mask & 0x8 | 0x4;
      break;
    case 3:
      mask = mask & 0xc | 0x2;
      break;
    case 4:
      mask = mask & 0xe | 0x1;
      break;
    default:
      fprintf(stderr, "check me\n");
      while(1);
  }

  thumb_it16 (&write_p, cond, mask);
}

void it_clip_from_offset(uint16_t *write_p, uint32_t *cond, uint32_t *mask, int offset) {
  int initial_len = it_get_no_of_inst(*mask);
  while (offset < 0 || offset >= initial_len);
  assert(offset >= 0 && offset < initial_len);

  if (offset > 0) {
    /* The first condition in the IT block always executes when the condition is true. If the
       instruction at the new offset has the opposite condition, switch the block to the opposite
       condition, which also flips the 'then'/'else' flags for each subsequent instruction. */
    bool same_cond = ((*mask >> (4 - offset)) & 1) == ((*cond) & 1);
    if (!same_cond) {
     *cond = arm_inverse_cond_code[*cond];
    }
    *mask = (*mask << offset) & 0xF;
  }

  thumb_it16 (&write_p, *cond, *mask);
}

bool create_it_gap(uint16_t **write_p, thumb_it_state *it_state) {
  if (it_state->cond_inst_after_it > 0 && it_state->is_overwritten == false) {
    if ((it_get_no_of_inst(it_state->it_initial_mask) - it_state->cond_inst_after_it) > 0) {
      it_clip_len(it_state->it_inst_addr, it_state->it_cond, it_state->it_initial_mask,
                  it_get_no_of_inst(it_state->it_initial_mask) - it_state->cond_inst_after_it);
    } else {
      assert(it_state->it_inst_addr == *write_p - 1);
      *write_p = it_state->it_inst_addr;
      it_state->is_overwritten = true;
    }
    return true;
  }
  return false;
}

bool close_it_gap(uint16_t **write_p, thumb_it_state *it_state) {
  if (it_state->cond_inst_after_it > 0) {
    it_clip_from_offset(*write_p, &it_state->it_cond, &it_state->it_initial_mask,
                        it_get_no_of_inst(it_state->it_initial_mask) - it_state->cond_inst_after_it);
    it_state->it_inst_addr = *write_p;
    it_state->is_overwritten = false;
    *write_p += 1;
    return true;
  }
  return false;
}

void thumb_check_free_space(dbm_thread *thread_data, uint16_t **o_write_p, uint32_t **o_data_p,
                            thumb_it_state *it_state, bool handle_it, size_t size, int cur_block) {
  uint16_t *write_p = *o_write_p;
  uint32_t *data_p = *o_data_p;

  if ((uint16_t *)data_p <= (write_p + 2)) {
    fprintf(stderr, "Thumb fragment overflowed: limit %p, write_p: %p\n", data_p, write_p);
    while(1);
  }
  if ((((uint32_t)write_p + size) >= (uint32_t)data_p)) {
    int new_block = allocate_bb(thread_data);
    thread_data->code_cache_meta[new_block].actual_id = cur_block;

    if ((uint32_t *)&thread_data->code_cache->blocks[new_block] != data_p) {
      if (handle_it && it_state->cond_inst_after_it > 0) {
        create_it_gap(&write_p, it_state);
      }

      thumb_b32_helper(write_p, (uint32_t)&thread_data->code_cache->blocks[new_block]);
      write_p = (uint16_t *)&thread_data->code_cache->blocks[new_block];

      if (handle_it && it_state->cond_inst_after_it > 0) {
        close_it_gap(&write_p, it_state);
      }
    }
    data_p = (uint32_t *)&thread_data->code_cache->blocks[new_block + 1];
  }

  *o_write_p = write_p;
  *o_data_p = data_p;
}

void copy_to_reg_16bit(uint16_t **write_p, enum reg reg, uint32_t value) {
  thumb_movwi32 (write_p, (value >> 11) & 0x1, (value >> 12) & 0xF, (value >> 8) & 0x7, reg, (value >> 0) & 0xFF);
  *write_p += 2;
}

void copy_to_reg_32bit(uint16_t **write_p, enum reg reg, uint32_t value) {
  thumb_movwi32 (write_p, (value >> 11) & 0x1, (value >> 12) & 0xF, (value >> 8) & 0x7, reg, (value >> 0) & 0xFF);
  *write_p += 2;
  thumb_movti32 (write_p, (value >> 27) & 0x1, (value >> 28) & 0xF, (value >> 24) & 0x7, reg, (value >> 16) & 0xFF);
  *write_p += 2;
}

void thumb_push_regs(uint16_t **write_p, uint32_t regs) {
  if (regs & 0xFFFFA000 || regs == 0) {
    fprintf(stderr, "Trying to push invalid reglist\n");
    while(1);
  } else if (regs & 0xBF00) {
    thumb_stmfd32(write_p, 1, sp, regs);
    *write_p += 2;
  } else {
    if (regs & (1 << lr)) {
      regs &= 0xFF;
      regs |= (1 << 8);
    }
    thumb_push16(write_p, regs);
    *write_p += 1;
  }
}

void thumb_pop_regs(uint16_t **write_p, uint32_t regs) {
  if (regs & 0xFFFFA000 || regs == 0) {
    fprintf(stderr, "Trying to pop invalid reglist\n");
    while(1);
  } else if (regs & 0x7F00) {
    thumb_ldmfd32(write_p, 1, sp, regs);
    *write_p += 2;
  } else {
    if (regs & (1 << pc)) {
      regs &= 0xFF;
      regs |= (1 << 8);
    }
    thumb_pop16(write_p, regs);
    *write_p += 1;
  }
}
  
enum arm_cond_codes arm_inverse_cond_code[] = {NE, EQ, CC, CS, PL, MI, VC, VS, LS, HI, LT, GE, LE, GT, AL, AL};

void thumb_b_bl_helper(uint16_t *write_p, uint32_t dest_addr, bool link, bool to_arm) {
  int difference = dest_addr - ((uint32_t)write_p & (to_arm ? ~2 : ~0)) - 4;

  if (difference < -(16*1024*1024) || difference >= (16*1024*1024)) {
    fprintf(stderr, "Branch out of range\n");
    while(1);
  }
  uint32_t sign_bit = (difference & 0x80000000) ? 1 : 0;
  uint32_t i1 = ~((difference >> 23) ^ sign_bit) & 0x1;
  uint32_t i2 = ~((difference >> 22) ^ sign_bit) & 0x1;
  uint32_t offset_high = (difference >> 12) & 0x3FF;
  uint32_t offset_low = (difference >> 1) & 0x7FF;
  if (link) {
    if (to_arm) {
      thumb_bl_arm32 (&write_p, sign_bit, offset_high, i1, i2, offset_low);
    } else {
      thumb_bl32 (&write_p, sign_bit,	offset_high, i1, i2, offset_low);
    }
  } else {
    thumb_b32 (&write_p, sign_bit, offset_high, i1, i2, offset_low);
  }
}

void thumb_adjust_b_bl_target(dbm_thread *thread_data, uint16_t *write_p, uint32_t dest_addr) {
  thumb_instruction inst = thumb_decode(write_p);

  if (inst != THUMB_BL32 && inst != THUMB_BL_ARM32 && inst != THUMB_B32) {
    fprintf(stderr, "Thumb: Trying to adjust target of invalid branch instruction.\n");
    while(1);
  }

  if (inst == THUMB_BL32) {
    dest_addr -= 4;
  } else if (inst == THUMB_BL_ARM32) {
    dest_addr -= 8;
  }

  thumb_b_bl_helper(write_p, dest_addr, inst != THUMB_B32, inst == THUMB_BL_ARM32);
}

void thumb_b16_cond_helper(uint16_t *write_p, uint32_t dest_addr, mambo_cond cond) {
  int difference = dest_addr -(uint32_t)write_p - 4;
  assert(difference >= -256 && difference < 256);

  thumb_b_cond16(&write_p, cond, (difference >> 1) & 0xFF);
}

void thumb_b32_helper(uint16_t *write_p, uint32_t dest_addr) {
  thumb_b_bl_helper(write_p, dest_addr, false, false);
}

void thumb_cc_branch(dbm_thread *thread_data, uint16_t *write_p, uint32_t dest_addr) {
  thumb_b32_helper(write_p, dest_addr);

  record_cc_link(thread_data, (uint32_t)write_p|THUMB, dest_addr);
}

void thumb_bl32_helper(uint16_t *write_p, uint32_t dest_addr) {
  thumb_b_bl_helper(write_p, dest_addr, true, false);
}

void thumb_blx32_helper(uint16_t *write_p, uint32_t dest_addr) {
  thumb_b_bl_helper(write_p, dest_addr, true, true);
}

void thumb_b32_cond_helper(uint16_t **write_p, uint32_t dest_addr, enum arm_cond_codes condition) {
  int difference = dest_addr -(uint32_t)(*write_p) - 4;
  if (difference < -(1*1024*1024) || difference >= (1*1024*1024)) {
    assert(condition < 14);
    thumb_b_cond16(write_p, arm_inverse_cond_code[condition], 1);
    (*write_p)++;
    thumb_b32_helper(*write_p, dest_addr);
    (*write_p) += 2;
  } else {
    uint32_t sign_bit = (difference & 0x80000000) ? 1 : 0;
    uint32_t j2 = (difference >> 19) & 0x1;
    uint32_t j1 = (difference >> 18) & 0x1;
    uint32_t offset_high = (difference >> 12) & 0x3F;
    uint32_t offset_low = (difference >> 1) & 0x7FF;
    thumb_b_cond32 (write_p, sign_bit, condition, offset_high, j1, j2, offset_low);
    (*write_p) += 2;
  }
}

void thumb_b16_helper(uint16_t *write_p, uint32_t dest_addr, enum arm_cond_codes cond) {
  int difference = dest_addr -(uint32_t)write_p - 4;

  if (cond >= EQ && cond < AL) {
    // Use encoding T1 (conditional with 8b imm)
    assert(difference >= -256 && difference <= 254);
    thumb_b_cond16(&write_p, cond, (difference >> 1) & 0xFF);
    write_p++;
  } else if (cond == AL) {
    // Use encoding T2 (unconditional with 11b imm)
    assert(difference >= -2048 && difference <= 2046);
    thumb_b16(&write_p, (difference >> 1) & 0x7FF);
    write_p++;
    while(1); // Check me
  } else {
    fprintf(stderr, "Requested invalid B16 condition\n");
    while(1);
  }
}

int thumb_cbz_cbnz_helper(uint16_t *write_p, uint32_t target, enum reg reg, bool cbz) {
  int difference = target - (uintptr_t)write_p - 4;

  if (difference < 0 || difference >= 127) return -1;

  thumb_misc_cbz_16(&write_p, cbz ? 0 : 1, difference >> 6, difference >> 1, reg);

  return 0;
}

void thumb_cbz_helper(uint16_t *write_p, uint32_t target, enum reg reg) {
  int ret = thumb_cbz_cbnz_helper(write_p, target, reg, true);
  assert(ret == 0);
}

void thumb_cbnz_helper(uint16_t *write_p, uint32_t target, enum reg reg) {
  int ret = thumb_cbz_cbnz_helper(write_p, target, reg, false);
  assert(ret == 0);
}

#define DISP_CALL_SIZE 76
void branch_save_context(dbm_thread *thread_data, uint16_t **o_write_p, bool late_app_sp) {
  uint16_t *write_p = *o_write_p;

  thumb_sub_sp_i16(&write_p, DISP_RES_WORDS);
  write_p++;

  thumb_push16(&write_p, (1 << r0) | (1 << r1) | (1 << r2) | (1 << r3));
  write_p++;

  if (!late_app_sp) {
    thumb_addi32(&write_p, 0, 0, sp, 0, r3, DISP_SP_OFFSET);
    write_p += 2;
  }

  *o_write_p = write_p;
}

#define SETUP (1 << 0)
#define REPLACE_TARGET (1 << 1)
#define INSERT_BRANCH (1 << 2)

void branch_jump(dbm_thread *thread_data, uint16_t **o_write_p, int bb_index, uint32_t target, uint32_t flags) {
  uint16_t *write_p = *o_write_p;

  if (flags & SETUP) {
    copy_to_reg_32bit(&write_p, r1, bb_index);
  }
  if (flags & REPLACE_TARGET) {
    copy_to_reg_32bit(&write_p, r0, target);
  }
  if (flags & INSERT_BRANCH) {
    if (flags & LATE_APP_SP) {
      thumb_addi32(&write_p, 0, 0, sp, 0, r3, DISP_SP_OFFSET);
      write_p += 2;
    }
    thumb_b32_helper(write_p, (uint32_t)thread_data->dispatcher_addr-4);
    write_p += 2;
  }
  
  *o_write_p = write_p;
}

void thumb_simple_exit(dbm_thread *thread_data, uint16_t **o_write_p, int bb_index, uint32_t target) {
  uint16_t *write_p = *o_write_p;
  branch_save_context(thread_data, &write_p, false);
  branch_jump(thread_data, &write_p, bb_index, target, SETUP|REPLACE_TARGET|INSERT_BRANCH);
  *o_write_p = write_p;
}

void set_cc_imm_links(dbm_thread *thread_data,
                         int16_t *write_p,
                         int basic_block,
                         uint32_t address_taken,
                         uint32_t address_skipped,
                         bool taken_in_cache,
                         bool skipped_in_cache
                         ) {
  uint32_t offset;

  if ((taken_in_cache || skipped_in_cache) &&
      thread_data->code_cache_meta[basic_block].branch_cache_status == 0) {
    thread_data->code_cache_meta[basic_block].branch_cache_status = taken_in_cache ? BRANCH_LINKED : FALLTHROUGH_LINKED;
    offset = ((uint32_t)write_p + 2) | THUMB;
    if (taken_in_cache) {
      record_cc_link(thread_data, offset, address_taken);
    } else {
      record_cc_link(thread_data, offset, address_skipped);
    }
  }

  if (taken_in_cache && skipped_in_cache &&
      (thread_data->code_cache_meta[basic_block].branch_cache_status & BOTH_LINKED) == 0) {
    thread_data->code_cache_meta[basic_block].branch_cache_status |= BOTH_LINKED;
    offset = ((uint32_t)write_p + 4 + 2) | THUMB;
    if (thread_data->code_cache_meta[basic_block].branch_cache_status & BRANCH_LINKED) {
      record_cc_link(thread_data, offset, address_skipped);
    } else {
      record_cc_link(thread_data, offset, address_taken);
    }
  }
}

#define IMM_SIZE 102
void thumb_encode_cond_imm_branch(dbm_thread *thread_data,
                                  uint16_t **o_write_p,
                                  int basic_block,
                                  uint32_t address_taken,
                                  uint32_t address_skipped,
                                  enum arm_cond_codes condition,
                                  bool taken_in_cache,
                                  bool skipped_in_cache,
                                  bool update) {
  uint16_t *write_p = *o_write_p;

  if (taken_in_cache && skipped_in_cache) {
    if (update && (thread_data->code_cache_meta[basic_block].branch_cache_status & FALLTHROUGH_LINKED)) {
      thumb_it16(&write_p, arm_inverse_cond_code[condition], 0x8);
      write_p++;
      thumb_b32_helper(write_p, address_skipped);
      write_p += 2;
      thumb_b32_helper(write_p, address_taken);
    } else {
      thumb_it16(&write_p, condition, 0x8);
      write_p++;
      thumb_b32_helper(write_p, address_taken);
      write_p += 2;
      thumb_b32_helper(write_p, address_skipped);
    }
    write_p += 2;
  } else {
    if (taken_in_cache) {
      thumb_it16(&write_p, condition, 0x8);
      write_p++;
      thumb_b32_helper(write_p, address_taken);
      write_p += 2;
    }
    if (skipped_in_cache) {
      assert(condition < 14);
      thumb_it16(&write_p, arm_inverse_cond_code[condition], 0x8);
      write_p++;
      thumb_b32_helper(write_p, address_skipped);
      write_p += 2;
    }
    if (!update) {
      if (!taken_in_cache && !skipped_in_cache) {
        // Here we reserve space for one conditional branch, either 2 or 3 halfwords depending on offset
        thumb_nop16(&write_p);
        write_p++;
        thumb_nop16(&write_p);
        write_p++;
        thumb_nop16(&write_p);
        write_p++;
      }
        
      branch_save_context(thread_data, &write_p, false);

      branch_jump(thread_data, &write_p, basic_block, 0, SETUP);
      if (!taken_in_cache && !skipped_in_cache) {
        debug("Writing cond branch at: %p\n", write_p);
        // Branch to branch taken trampoline
        thumb_b_cond16(&write_p, condition, 0x05);
        write_p++;
      }

      if (!skipped_in_cache) {
        // Branch not taken trampoline
        branch_jump(thread_data, &write_p, basic_block, address_skipped, REPLACE_TARGET|INSERT_BRANCH);
      }
      if (!taken_in_cache) {
        // Branch taken trampoline
        branch_jump(thread_data, &write_p, basic_block, address_taken, REPLACE_TARGET|INSERT_BRANCH);
      }
    }
  }

  set_cc_imm_links(thread_data, *o_write_p, basic_block, address_taken, address_skipped, taken_in_cache, skipped_in_cache);

  *o_write_p = write_p;
}

#define CBZ_SIZE 124
void thumb_encode_cbz_branch(dbm_thread *thread_data,
                                  uint32_t rn,
                                  uint16_t **o_write_p,
                                  int basic_block,
                                  uint32_t address_taken,
                                  uint32_t address_skipped,
                                  bool taken_in_cache,
                                  bool skipped_in_cache,
                                  bool update) {
  uint16_t *write_p = *o_write_p;
              
  if (taken_in_cache && skipped_in_cache) {
    if (update && (thread_data->code_cache_meta[basic_block].branch_cache_status & FALLTHROUGH_LINKED)) {
      thumb_cbz16(&write_p, 0, 0x01, rn);
      write_p++;
      thumb_b32_helper(write_p, address_skipped);
      write_p += 2;
      thumb_b32_helper(write_p, address_taken);
      write_p += 2;
    } else {
      thumb_cbnz16(&write_p, 0, 0x01, rn);
      write_p++;
      thumb_b32_helper(write_p, address_taken);
      write_p += 2;
      thumb_b32_helper(write_p, address_skipped);
      write_p += 2;
    }
  } else {
    if (taken_in_cache) {
      thumb_cbnz16(&write_p, 0, 0x01, rn);
      write_p++;
      thumb_b32_helper(write_p, address_taken);
      write_p += 2;
    }
    if (skipped_in_cache) {
      thumb_cbz16(&write_p, 0, 0x01, rn);
      write_p++;
      thumb_b32_helper(write_p, address_skipped);
      write_p += 2;
    }
    if (!update) {
      if (!taken_in_cache && !skipped_in_cache) {
        thumb_nop16(&write_p);
        write_p++;
        thumb_nop16(&write_p);
        write_p++;
        thumb_nop16(&write_p);
        write_p++;
      }
      assert(rn != sp);
      branch_save_context(thread_data, &write_p, true);

      if (!taken_in_cache && !skipped_in_cache) {
        debug("Writing C(N)BZ at: %p\n", write_p);
        // Branch to branch taken trampoline
        thumb_cbz16(&write_p, 0, 0xb, rn);
        write_p++;
      }

      if (!skipped_in_cache) {
        // Branch not taken trampoline
        branch_jump(thread_data, &write_p, basic_block, address_skipped,
                    SETUP|REPLACE_TARGET|INSERT_BRANCH|LATE_APP_SP);
      }

      if (!taken_in_cache) {
        // Branch taken trampoline
        branch_jump(thread_data, &write_p, basic_block, address_taken,
                    SETUP|REPLACE_TARGET|INSERT_BRANCH|LATE_APP_SP);
      }
    }
  } // not both in cache

  set_cc_imm_links(thread_data, *o_write_p, basic_block, address_taken, address_skipped, taken_in_cache, skipped_in_cache);

  *o_write_p = write_p;
}

void thumb_inline_hash_lookup(dbm_thread *thread_data, uint16_t **o_write_p, int basic_block, int r_target) {
  uint16_t *loop_start;
  uint16_t *branch_miss;
  uint16_t *write_p = *o_write_p;

  bool target_reg_clean = (r_target >= r0);
  int target = target_reg_clean ? r_target : r5;
  int r_tmp = target_reg_clean ? r5 : r4;

  thread_data->code_cache_meta[basic_block].rn = target;

  // MOVW+MOVT r_tmp, hash_mask
  copy_to_reg_32bit(&write_p, r_tmp, CODE_CACHE_HASH_SIZE);

  // MOVW+MOVT r6, hash_table
  copy_to_reg_32bit(&write_p, r6, (uint32_t)thread_data->entry_address.entries);

  // AND r_tmp, target, r_tmp
  thumb_and32(&write_p, 0, target, 0, r_tmp, 0, 0, r_tmp);
  write_p += 2;

  // ADD r_tmp, r6, r_tmp, LSL #3
  thumb_add32(&write_p, 0, r6, 0, r_tmp, 3, 0, r_tmp);
  write_p += 2;

  // loop:
  loop_start = write_p;

  // LDR r6, [r_tmp], #8
  thumb_ldri32(&write_p, r6, r_tmp, 8, 0, 1, 1);
  write_p += 2;

  // CMP r6, target
  thumb_cmp32(&write_p, r6, 0, 0, 0, target);
  write_p += 2;

  // BNE miss
  branch_miss = write_p++;

  // jump:
  // LDR r6, [r_tmp, #-4]
  thumb_ldri32(&write_p, r6, r_tmp, 4, 1, 0, 0);
  write_p += 2;

  if (!target_reg_clean) {
    // POP {R4}
    thumb_pop16(&write_p, (1 << r4));
    write_p++;
  }

  // BX r6
  thumb_bx16(&write_p, r6);
  write_p++;

  // miss:
  thumb_b16_helper(branch_miss, (uint32_t)write_p, NE);

  // CMP r6, #0
  thumb_cmpri16(&write_p, r6, 0);
  write_p++;

  // BNE loop
  thumb_b16_helper(write_p, (uint32_t)loop_start, NE);
  write_p++;

  // SUB sp, sp, #8
  // PUSH {R0 - R3}
  branch_save_context(thread_data, &write_p, true);

  // MOV R0, target
  thumb_movh16(&write_p, r0 >> 3, target, r0);
  write_p++;

  // ADD r3, sp, #24
  thumb_addi32(&write_p, 0, 0, sp, 0, r3, DISP_SP_OFFSET);
  write_p += 2;

  // LDMFD r3!, {r4-r6}
  if (target_reg_clean) {
    thumb_ldmfd32(&write_p, 1, r3, (1 << r5) | (1 << r6));
  } else {
    thumb_ldmfd32(&write_p, 1, r3, (1 << r4) | (1 << r5) | (1 << r6));
  }
  write_p += 2;

  // MOV r1, #bb_id
  // B dispatcher
  branch_jump(thread_data, &write_p, basic_block, 0, SETUP | INSERT_BRANCH);

  *o_write_p = write_p;
}

bool link_bx_alt(dbm_thread *thread_data, uint16_t **write_p, int cond_inst_after_it, uint32_t alt_addr) {
#ifdef LINK_BX_ALT
  if (cond_inst_after_it > 0) {
    assert(cond_inst_after_it == 1);
    thumb_b16(write_p, 1);
    (*write_p)++;
    uint32_t block_address = lookup_or_stub(thread_data, (uint32_t)alt_addr);
    thumb_cc_branch(thread_data, *write_p, block_address);
    *write_p += 2;
    return true;
  }
#endif
  return false;
}

void pass1_thumb(dbm_thread *thread_data, uint16_t *read_address, branch_type *bb_type) {
  uint32_t null, reglist, rd, dn, imm;
  int32_t branch_offset;
  *bb_type = unknown;

  while(*bb_type == unknown) {
    thumb_instruction inst = thumb_decode(read_address);

    switch(inst) {
      case THUMB_ADDH16:
      case THUMB_CMPH16:
      case THUMB_MOVH16:
        thumb_special_data_proc_16_decode_fields(read_address, &null, &dn, &null, &rd);
        rd |= dn << 3;
        if (rd == pc) {
          *bb_type = uncond_reg_thumb;
        }
        break;

      case THUMB_BX16:
      case THUMB_BLX16:
        *bb_type = uncond_reg_thumb;
        break;

      case THUMB_CBZ16:
      case THUMB_CBNZ16:
        *bb_type = cbz_thumb;
        break;

      case THUMB_POP16:
        thumb_pop16_decode_fields(read_address, &reglist);
        if(reglist & (1<<8)) {
          *bb_type = uncond_reg_thumb;
        }
        break;

      case THUMB_B_COND16:
        *bb_type = cond_imm_thumb;
        break;

      case THUMB_B16:
#ifdef DBM_INLINE_UNCOND_IMM
        thumb_b16_decode_fields(read_address, &imm);

        branch_offset = (imm & 0x400) ? 0xFFFFF000 : 0;
        branch_offset |= imm << 1;

        read_address = (uint16_t *)((uint32_t)read_address + 4 -2 + branch_offset);
#else
        *bb_type = uncond_imm_thumb;
#endif
        break;

      case THUMB_LDRI32:
      case THUMB_LDRHI32:
      case THUMB_LDRBI32:
        thumb_load_store_single_reg_imm12_32_decode_fields(read_address, &null, &null, &null,
                                                           &null, &null, &rd, &null);
        if (rd == pc) {
          *bb_type = uncond_reg_thumb;
        }
        break;

      case THUMB_LDR32:
      case THUMB_LDRH32:
      case THUMB_LDRB32:
        thumb_load_store_single_reg_off_32_decode_fields(read_address, &null, &null, &null, &null, &rd, &null, &null);
        if (rd == pc) {
          *bb_type = uncond_reg_thumb;
        }
        break;

      case THUMB_B32:
      case THUMB_BL32:
        *bb_type = uncond_imm_thumb;
        break;

      case THUMB_BL_ARM32:
        *bb_type = uncond_blxi_thumb;
        break;

      case THUMB_B_COND32:
        *bb_type = cond_imm_thumb;
        break;

      case THUMB_TBB32:
      case THUMB_TBH32:
        *bb_type = tb_indirect;
        break;

      case THUMB_LDMFD32:
      case THUMB_LDMEA32:
        thumb_load_store_multiple32_decode_fields(read_address, &null, &null, &null, &null, &reglist);
        if(reglist & (1<<pc)) {
          *bb_type = uncond_reg_thumb;
        }
        break;
    }

    if (inst < THUMB_ADC32) {
      read_address++;
    } else {
      read_address+= 2;
    }
  }
}

void do_it_iter(thumb_it_state *state) {
  if (state->cond_inst_after_it > 0) {
    state->cond_inst_after_it--;
    state->it_mask = (state->it_mask << 1) & 0x3F;
  }
}

bool thumb_scanner_deliver_callbacks(dbm_thread *thread_data, mambo_cb_idx cb_id, thumb_it_state *state,
                                     uint16_t **o_read_address, thumb_instruction inst, uint16_t **o_write_p,
                                     uint32_t **o_data_p, int basic_block, cc_type type,
                                     bool allow_write, bool *stop) {
  bool replaced = false;
#ifdef PLUGINS_NEW
  void *prev_write_p;
  if (global_data.free_plugin > 0) {
    uint16_t *write_p = *o_write_p;
    uint32_t *data_p = *o_data_p;
    uint16_t *read_address = *o_read_address;

    mambo_cond cond;
    uint32_t tmp;
    switch(inst) {
      case THUMB_B_COND16:
        thumb_b_cond16_decode_fields(read_address, &cond, &tmp);
        break;
      case THUMB_B_COND32:
        thumb_b_cond32_decode_fields(read_address, &tmp, &cond, &tmp, &tmp, &tmp, &tmp);
        break;
      default:
        if (state->cond_inst_after_it > 0) {
          cond = (((state->it_mask >> 5) & 1) == (state->it_cond & 1))
                 ? state->it_cond : arm_inverse_cond_code[state->it_cond];
        } else {
          cond = AL;
        }
    }

    /* If the previous instruction was IT, allow the plugins to overwrite it */
    if (allow_write && state->cond_inst_after_it > 0) {
      if (state->it_inst_addr == (write_p -1)) {
        write_p--;
        state->is_overwritten = true;
      }
    }

    mambo_context ctx;
    set_mambo_context_code(&ctx, thread_data, PRE_INST_C, type, basic_block, THUMB_INST, inst, cond, read_address, write_p, data_p, stop);

    for (int i = 0; i < global_data.free_plugin; i++) {
      if (global_data.plugins[i].cbs[cb_id] != NULL) {
        ctx.plugin_id = i;
        ctx.code.replace = false;
        ctx.code.available_regs = ctx.code.pushed_regs;
        prev_write_p = ctx.code.write_p;
        global_data.plugins[i].cbs[cb_id](&ctx);

        if (allow_write) {
          if (replaced && (prev_write_p != ctx.code.write_p || ctx.code.replace)) {
            fprintf(stderr, "MAMBO API WARNING: plugin %d added code for overridden "
                            "instruction (at %p).\n", i, read_address);
          }
          if (ctx.code.replace) {
            if (cb_id == PRE_INST_C) {
              replaced = true;
            } else {
              fprintf(stderr, "MAMBO API WARNING: plugin %d set replace_inst for "
                              "a disallowed event (at %p).\n", i, read_address);
            }
          }
          assert(count_bits(ctx.code.pushed_regs) == ctx.code.plugin_pushed_reg_count);
          if (allow_write && ctx.code.pushed_regs) {
            thumb_pop_regs((uint16_t **)&ctx.code.write_p, ctx.code.pushed_regs);
          }

          thumb_check_free_space(thread_data, (uint16_t **)&ctx.code.write_p, (uint32_t **)&ctx.code.data_p,
                                 state, false, MIN_FSPACE, basic_block);
        } else {
          assert(ctx.code.write_p == write_p);
          assert(ctx.code.data_p == data_p);
        }
      } // global_data.plugins[i].cbs[cb_id] != NULL
    } // plugin iterator

    if (cb_id == PRE_BB_C) {
      watched_functions_t *wf = &global_data.watched_functions;
      for (int i = 0; i < wf->funcp_count; i++) {
        if (read_address == (wf->funcps[i].addr -1)) {
          _function_callback_wrapper(&ctx, wf->funcps[i].func);
          if (ctx.code.replace) {
            read_address = ctx.code.read_address;
          }
          thumb_check_free_space(thread_data, (uint16_t **)&ctx.code.write_p, (uint32_t **)&ctx.code.data_p,
                                 state, false, MIN_FSPACE, basic_block);
        }
      }
    }

    if (allow_write && state->cond_inst_after_it > 0) {
      if (ctx.code.write_p != write_p) {
        // Code was inserted.
        // Reduce the length of the IT block
        create_it_gap((uint16_t **)&ctx.code.write_p, state);
        if (replaced) {
          // If the instruction was replaced by a plugin, remove its
          // condition from the head of the IT block
          do_it_iter(state);
        }
        // Insert an IT instruction for the remaining instructions
        close_it_gap((uint16_t **)&ctx.code.write_p, state);
      } else {
        // If no code was inserted, keep the IT instruction
        if (state->is_overwritten) {
          ctx.code.write_p += 2;
          state->is_overwritten = false;
        }
      }
    }

    write_p = ctx.code.write_p;
    data_p = ctx.code.data_p;

    *o_write_p = write_p;
    *o_data_p = data_p;
    *o_read_address = read_address;
  }
#endif
  return replaced;
}

size_t scan_t32(dbm_thread *thread_data, uint16_t *read_address, int basic_block, cc_type type, uint16_t *write_p) {
  bool stop = false;

  uint16_t *start_scan = read_address, *bb_entry = read_address;
  if (write_p == NULL) {
    write_p = (uint16_t *)&thread_data->code_cache->blocks[basic_block];
  }
  uint32_t start_address = (uint32_t)write_p;
  uint32_t *data_p;
  if (type == mambo_bb) {
    data_p = (uint32_t *)write_p + BASIC_BLOCK_SIZE;
  } else {
    data_p = (uint32_t *)&thread_data->code_cache->traces + (TRACE_CACHE_SIZE/4);
  }
  
  debug("write_p: %p\n", write_p);


  // Todo: check that the compiler can optimize the fact that only
  // a small number of these variables is alive per iteration
  uint32_t rm;
  uint32_t rn;
  uint32_t dn;
  uint32_t rdn;
  uint32_t rt;
  uint32_t rdlo;
  uint32_t rdhi;
  uint32_t racc;
  uint32_t imm1;
  uint32_t imm2;
  uint32_t imm3;
  uint32_t imm4;
  uint32_t imm5;
  uint32_t imm8;
  uint32_t set_flags;
  uint32_t sign_ext;
  uint32_t sign_bit;
  uint32_t upwards;
  uint32_t original_pc;
  uint32_t offset_high;
  uint32_t offset_low;
  uint32_t j1;
  uint32_t j2;
  uint32_t reglist;
  uint32_t shift_type;
  uint32_t condition;
  uint32_t writeback;
  uint32_t mask;
  uint32_t pre_index;
  uint32_t shift;
  uint32_t opcode;
  uint32_t opcode2;
  uint32_t loadstore;
  uint32_t datasize;
  uint32_t rotate;
  uint32_t rt2;
  uint32_t m;
  uint32_t vm;
  uint32_t to_arm;
  uint32_t thumb_arm;
  uint32_t d;
  uint32_t vd;
  uint32_t load_store;
  uint32_t p;
  uint32_t n;
  uint32_t vn;
  uint32_t n_high;
  uint32_t m_swap;
  uint32_t link;
  uint32_t size;
  uint32_t align;
  
  uint32_t opc1;
  uint32_t crn;
  uint32_t coproc;
  uint32_t opc2;
  uint32_t crm;
  
  uint32_t target;
  uint32_t offset;
  uint32_t scratch_reg;
  uint32_t scratch_reg2;

  int32_t  branch_offset;
  uint32_t block_address;
  uint32_t branch_taken_address;
  uint32_t branch_skipped_address;
  uint32_t branch_taken_cached;
  uint32_t branch_skipped_cached;

  bool it_cond_handled = false;
  thumb_it_state it_state;
  it_state.cond_inst_after_it = 0;
  it_state.is_overwritten = false;

  bool ldrex = false;

  uint16_t *inst_pop_regs;
  uint16_t *set_inst_pop_regs = NULL;
  uint32_t *inst_pop_regs_data;
  uint32_t poped_regs = 0;

#ifdef DBM_INLINE_UNCOND_IMM
  int inline_back_count = 0;
#endif

  if (type != mambo_trace) {
    thumb_pop16(&write_p, (1 << r5) | (1 << r6));
    write_p++;
  }

#ifdef DBM_TRACES
  branch_type bb_type;
  pass1_thumb(thread_data, read_address, &bb_type);

  if (type == mambo_bb && (bb_type == uncond_imm_thumb || bb_type == cond_imm_thumb || bb_type == cbz_thumb
  #ifdef BLXI_AS_TRACE_HEAD
    || bb_type == uncond_blxi_thumb
  #endif
  #ifdef TB_AS_TRACE_HEAD
      || bb_type == tb_indirect
  #endif
      )) {
    thumb_sub_sp_i16(&write_p, 2);
    write_p++;

    thumb_push16(&write_p, (1 << r0) | (1 << r1) | (1 << r2) | (1 << 8));
    write_p++;

    copy_to_reg_32bit(&write_p, r0, basic_block);

    thumb_bl32_helper(write_p, thread_data->trace_head_incr_addr);
    write_p += 2;
  }
#endif

  thumb_scanner_deliver_callbacks(thread_data, PRE_FRAGMENT_C, &it_state, &read_address, -1,
                                  &write_p, &data_p, basic_block, type, true, &stop);
  thumb_scanner_deliver_callbacks(thread_data, PRE_BB_C, &it_state, &read_address, -1,
                                  &write_p, &data_p, basic_block, type, true, &stop);

  while(!stop) {
    debug("thumb scan read_address: %p\n", read_address);
    thumb_instruction inst = thumb_decode(read_address);
    debug("Instruction enum: %d\n", (inst == THUMB_INVALID) ? -1 : inst);
    
    debug("instruction word: 0x%x\n", (inst < THUMB_ADC32) ? *read_address : ((*read_address) << 16) |*(read_address+1));
    it_cond_handled = false;

#ifdef PLUGINS_NEW
    bool skip_inst = thumb_scanner_deliver_callbacks(thread_data, PRE_INST_C, &it_state, &read_address,
                                                     inst, &write_p, &data_p, basic_block, type, true, &stop);
#endif

    // Check if the previous instruction is a POP
    if (set_inst_pop_regs) {
      inst_pop_regs = set_inst_pop_regs;
      set_inst_pop_regs = NULL;
    } else {
      inst_pop_regs = NULL;
    }
#ifdef PLUGINS_NEW
    if (skip_inst) {
      it_cond_handled = true;
    } else {
#endif
    switch(inst) {
      case THUMB_MOVI16:
      case THUMB_LSLI16:
      case THUMB_LSRI16:
      case THUMB_ASRI16:
        thumb_shift_i_mov_16_decode_fields(read_address, &opcode, &imm5, &rm, &rdn);

        assert(rm != pc && rdn != pc);
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
      case THUMB_ADD16:
      case THUMB_SUB16:
        thumb_add_sub_16_decode_fields(read_address, &opcode, &rm, &rn, &rdn);

        assert(rm != pc && rn != pc && rdn != pc);
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
      case THUMB_ADDI16:
      case THUMB_SUBI16:
        thumb_add_sub_i_16_decode_fields(read_address, &opcode, &imm3, &rn, &rdn);

        assert(rn != pc && rdn != pc);
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
      case THUMB_MOVRI16:
      case THUMB_CMPRI16:
      case THUMB_ADDRI16:
      case THUMB_SUBRI16:
        thumb_add_sub_comp_mov_i_16_decode_fields(read_address, &opcode, &rdn, &imm8);
        
        assert(rdn != pc);
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
      case THUMB_AND16:
      case THUMB_EOR16:
      case THUMB_LSL16:
      case THUMB_LSR16:
      case THUMB_ASR16:
      case THUMB_ADC16:
      case THUMB_SBC16:
      case THUMB_TST16:
      case THUMB_ROR16:
      case THUMB_RSBI16:
      case THUMB_CMP16:
      case THUMB_CMN16:
      case THUMB_ORR16:
      case THUMB_BIC16:
      case THUMB_MUL16:
      case THUMB_MVN16:
        thumb_data_proc_16_decode_fields(read_address, &opcode, &rm, &rdn);
        
        assert(rm != pc && rdn != pc);
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
      case THUMB_ADDH16:
      case THUMB_CMPH16:
      case THUMB_MOVH16:
        thumb_special_data_proc_16_decode_fields(read_address, &opcode, &dn, &rm, &rdn);

        rdn |= dn << 3;
        debug("ADD/CMP/MOVH16 rm: %d, rdn: %d\n", rm, rdn);

        if (rdn != pc && rm != pc) {
          copy_thumb_16();
          it_cond_handled = true;
          break;
        }

        assert(!(rdn == pc && rm == pc));

        if (rdn == pc) {
          assert(rm != sp);
          thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_thumb;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;

          uint32_t r_target = r0;

#ifdef DBM_INLINE_HASH
          thumb_push16(&write_p, (1 << r4) | (1 << r5) | (1 << r6));
          write_p++;
          r_target = r5;
#else
          branch_save_context(thread_data, &write_p, true);
#endif
          switch(inst) {
            case THUMB_MOVH16:
              if (rm != r_target) {
                thumb_movh16(&write_p, r_target >> 3, rm, r_target);
                write_p++;
              }
              break;
            default:
              fprintf(stderr, "Unsupported encoding\n");
              while(1);
          }

          // ORR Rtarget, Rtarget, #1 - to mark as thumb insts
          thumb_orri32(&write_p, 0, 0, r_target, 0, r_target, 1);
          write_p += 2;

#ifdef DBM_INLINE_HASH
          thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                 true, IHL_FSPACE, basic_block);
          thumb_inline_hash_lookup(thread_data, &write_p, basic_block, -1);
#else
          branch_jump(thread_data, &write_p, basic_block, 0, SETUP|INSERT_BRANCH|LATE_APP_SP);
#endif
          stop = true;
        } else { // rm == pc
          assert(rdn != pc);
          scratch_reg = (rdn == r0) ? r1 : r0;

          thumb_push16(&write_p, 1 << scratch_reg);
          write_p++;
          
          copy_to_reg_32bit(&write_p, scratch_reg, (uint32_t)read_address + 4);
          switch(inst) {
            case THUMB_ADDH16:
              thumb_addh16(&write_p, dn, scratch_reg, rdn & 0x7);
              break;
            case THUMB_CMPH16:
              thumb_cmph16(&write_p, dn, rm, rdn);
              fprintf(stderr, "Untested CMPH16\n");
              while(1);
              break;
            case THUMB_MOVH16:
              thumb_movh16(&write_p, dn, scratch_reg, rdn & 0x7);
              break;
          }
          write_p++;
          
          thumb_pop16(&write_p, 1 << scratch_reg);
          write_p++;
        }
        
        break;
      case THUMB_BX16:
      case THUMB_BLX16:
        thumb_bx_16_decode_fields(read_address, &link, &rm);
        assert(rm != sp && (rm != pc || inst == THUMB_BX16));
        /* Handle conditional execution: either a direct branch to the basic block for
           read_address + 2 or a call to the dispatcher */
        thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_thumb;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        if (it_state.cond_inst_after_it == 1) {
#ifdef LINK_BX_ALT
          /* If the previous instruction was POP, we'll overwrite it and place a copy:
             - on the code path where the branch wasn't taken
             - on the code path where the branch was taken
             This allows following code (e.g. the inline hash lookup) to use the dead
             registers as scratch registers.
          */
          if (inst_pop_regs) {
            write_p = inst_pop_regs;
            data_p = inst_pop_regs_data;
            
            thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                   true, 8, basic_block);

            thumb_b16(&write_p, 3);
            write_p++;

            thumb_ldmfd32(&write_p, 1, sp, poped_regs);
            write_p += 2;
          } else {
            thumb_b16(&write_p, 1);
            write_p++;
          }

          target = lookup_or_stub(thread_data, (uint32_t)read_address + 2 + 1);
          thumb_cc_branch(thread_data, write_p, target);
          write_p += 2;
          
          if (inst_pop_regs) {
            /* If the inline hash lookup is going to use the dead registers, it needs
               to replace the following instruction from the branch-taken path.
            */
            inst_pop_regs = write_p;
            inst_pop_regs_data = data_p;
          
            thumb_ldmfd32(&write_p, 1, sp, poped_regs);
            write_p += 2;
            
            while(1);
          }
#else
          assert(0); // incorrect range
          thumb_b16(&write_p, (((uint32_t)write_p) & 2) ? 29 : 28);
          write_p++;
          
          // This is branch not taken
          thumb_simple_exit(thread_data, &write_p, basic_block, (uint32_t)read_address+2+1);
#endif
      
          it_cond_handled = true;
          it_state.cond_inst_after_it = 0; // allows check_free_space to insert branches
        } else if (it_state.cond_inst_after_it > 1) {
          fprintf(stderr, "BL in middle of IT block\n");
          while(1);
        }

        /* BX PC can be handled as an immediate branch to ARM mode*/
        if (inst == THUMB_BX16 && rm == pc) {
          target = lookup_or_stub(thread_data, get_original_pc());

          if (((uint32_t)write_p) & 2) {
            thumb_ldrl32(&write_p, pc, 4, 1);
            write_p += 3;
          } else {
            thumb_ldrl32(&write_p, pc, 0, 1);
            write_p += 2;
          }

          *(uint32_t *)write_p = target;
          record_cc_link(thread_data, (uint32_t)write_p|FULLADDR, target);
          write_p += 2;

          stop = true;

          break;
        }

#ifdef DBM_INLINE_HASH
        assert(rm != sp && rm != pc);
        int r_target = -1;
        if (rm != r5 && rm != r6 && (inst != THUMB_BLX16 || rm != lr)) {
          r_target = rm;
          thumb_push16(&write_p, (1 << r5) | (1 << r6));
        } else {
          thumb_push16(&write_p, (1 << r4) | (1 << r5) | (1 << r6));
        }
        write_p++;
        if (r_target < r0) {
          thumb_movh16(&write_p, 0, rm, r5);
          write_p++;
        }

        if (inst == THUMB_BLX16) {
          copy_to_reg_32bit(&write_p, lr, ((uint32_t)read_address) + 2 + 1);
        }

        thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                               true, IHL_FSPACE, basic_block);
        thumb_inline_hash_lookup(thread_data, &write_p, basic_block, r_target);
#else
        branch_save_context(thread_data, &write_p, true);

        if (rm == pc) {
          copy_to_reg_32bit(&write_p, r0, get_original_pc());
        } else {
          thumb_movh16(&write_p, 0, rm, 0);
          write_p++;
          if (inst == THUMB_BLX16) {
            copy_to_reg_32bit(&write_p, lr, ((uint32_t)read_address) + 2 + 1);
          }
        }

        branch_jump(thread_data, &write_p, basic_block, 0, SETUP|INSERT_BRANCH|LATE_APP_SP);
#endif
        stop = true;
        
        break;
      case THUMB_LDR_PC_16:
        // needs rewriting
        thumb_ldr_pc_16_decode_fields(read_address, &rdn, &imm8);
        original_pc = get_original_pc();
        offset = imm8 << 2;
        
        modify_in_it_pre(5);
        copy_to_reg_32bit(&write_p, rdn, original_pc);
        thumb_ldrwi32(&write_p, rdn, rdn, offset);
        write_p += 2;
        modify_in_it_post();
        
        it_cond_handled = true;
        
        break;
        
      case THUMB_STR16:
      case THUMB_STRH16:
      case THUMB_STRB16:
      case THUMB_LDRSB16:
      case THUMB_LDR16:
      case THUMB_LDRH16:
      case THUMB_LDRB16:
      case THUMB_LDRSH16:
        // only low 8 regs
        copy_thumb_16();
        it_cond_handled = true;
        break;
        
      case THUMB_STRI16:
      case THUMB_LDRI16:
      case THUMB_STRBI16:
      case THUMB_LDRBI16:
        // only low 8 regs
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
        
      case THUMB_LDRHI16:
      case THUMB_STRHI16:
        // only low 8 regs
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
        
      case THUMB_STR_SP16:
        // check that source isn't pc
        thumb_str_sp16_decode_fields(read_address, &rdn, &imm8);

        assert(rdn != pc);
        copy_thumb_16();
        it_cond_handled = true;

        break;
      case THUMB_LDR_SP16:
        // check that dest isn't pc
        thumb_ldr_sp16_decode_fields(read_address, &rdn, &imm8);
        
        assert(rdn != pc);
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
      
      case THUMB_ADD_FROM_SP16:
        copy_thumb_16();
        it_cond_handled = true;
        break;
        
      case THUMB_ADD_FROM_PC16:
        thumb_add_from_pc16_decode_fields(read_address, &rdn, &imm8);

        modify_in_it_pre(3);
        copy_to_reg_32bit(&write_p, rdn, get_original_pc() + (imm8 << 2));
        modify_in_it_post();

        it_cond_handled = true;

        break;

      // Only affects the SP, safe to copy
      case THUMB_ADD_SP_I16:
      case THUMB_SUB_SP_I16:
        copy_thumb_16();
        it_cond_handled = true;
        break;
        
      case THUMB_SXTH16:
      case THUMB_SXTB16:
      case THUMB_UXTH16:
      case THUMB_UXTB16:
        // Operand fields are 3 bits in width
        copy_thumb_16();
        it_cond_handled = true;
        
        break;
      case THUMB_CBZ16:
      case THUMB_CBNZ16:
        thumb_misc_cbz_16_decode_fields(read_address, &n, &imm1, &imm5, &rn);
        assert(rn != pc);
        
        branch_offset = (imm1 << 6) | (imm5 << 1);
        debug("Branch offset: %d\n", branch_offset);
        
        // Seems ok, but keep an eye on this
        target = (uint32_t)read_address + branch_offset + 4 + 1;
        debug("Branch taken: 0x%x\n", target);

        thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                               true, CBZ_SIZE, basic_block);

        // Mark this as the beggining of code emulating B
        thread_data->code_cache_meta[basic_block].exit_branch_type = cbz_thumb;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        thread_data->code_cache_meta[basic_block].branch_taken_addr = (inst == THUMB_CBZ16) ? target : ((uint32_t)read_address + 2 + 1);
        thread_data->code_cache_meta[basic_block].branch_skipped_addr = (inst == THUMB_CBZ16) ? ((uint32_t)read_address + 2 + 1) : target;
        thread_data->code_cache_meta[basic_block].rn = rn;

#ifdef DBM_LINK_CBZ
        if (type == mambo_bb) {
          branch_taken_address = cc_lookup(thread_data, thread_data->code_cache_meta[basic_block].branch_taken_addr);
          branch_taken_cached = (branch_taken_address != UINT_MAX);
          branch_skipped_address = cc_lookup(thread_data, thread_data->code_cache_meta[basic_block].branch_skipped_addr);
          branch_skipped_cached = (branch_skipped_address != UINT_MAX);

          thumb_encode_cbz_branch(thread_data, rn, &write_p, basic_block,
                                  (branch_taken_cached) ? branch_taken_address : thread_data->code_cache_meta[basic_block].branch_taken_addr,
                                  (branch_skipped_cached) ? branch_skipped_address : thread_data->code_cache_meta[basic_block].branch_skipped_addr,
                                  branch_taken_cached,
                                  branch_skipped_cached,
                                  false);
        } else {
#endif
          thumb_encode_cbz_branch(thread_data, rn, &write_p, basic_block,
                                  (inst == THUMB_CBZ16) ? target : ((uint32_t)read_address + 2 + 1),
                                  (inst == THUMB_CBZ16) ? ((uint32_t)read_address + 2 + 1) : target,
                                  false,
                                  false,
                                  false);
#ifdef DBM_LINK_CBZ
        }
#endif

        stop = true;

        break;
      case THUMB_PUSH16:
        copy_thumb_16();
        break;
      case THUMB_POP16:
        thumb_pop16_decode_fields(read_address, &reglist);

        if ((reglist & (1<<8)) == 0) {
          set_inst_pop_regs = write_p;
          inst_pop_regs_data = data_p;
          poped_regs = reglist;

          copy_thumb_16();
        } else { // PC is POPed
          thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_thumb;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;

          if (link_bx_alt(thread_data, &write_p, it_state.cond_inst_after_it, (uint32_t)read_address + 3)) {
            it_cond_handled = true;
          }
#ifdef DBM_INLINE_HASH
          if (reglist != ((1 << r4) | (1 << r5) | (1 << 8))) {
            if (reglist & 0xFF) {
              thumb_pop16(&write_p, reglist & 0xFF);
              write_p++;
            }
            thumb_push16(&write_p, (1 << r4) | (1 << r5));
            write_p++;
          }
          thumb_ldr_sp16(&write_p, r5, 2);
          write_p++;
          thumb_str_sp16(&write_p, r6, 2);
          write_p++;

          thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                 true, IHL_FSPACE, basic_block);
          thumb_inline_hash_lookup(thread_data, &write_p, basic_block, -1);
#else
          thumb_pop16(&write_p, reglist & 0xFF);
          write_p++;

          branch_save_context(thread_data, &write_p, false);
  #ifndef LINK_BX_ALT
          if (it_state.cond_inst_after_it == 1 && type == mambo_bb) {
            fprintf(stderr, "Cond POP16, check if BX PC is marked conditional\n");
            thumb_it16 (&write_p, arm_inverse_cond_code[it_state.it_cond], (arm_inverse_cond_code[it_state.it_cond] & 1) ? 0xa : 0x6 );
            write_p++;
            copy_to_reg_32bit(&write_p, r0, get_original_pc() + 1);
            it_cond_handled = true;
            while(1);
          }
  #endif
          thumb_ldri32(&write_p, r0, APP_SP, 4, 0, 1, 1);
          write_p += 2;
          branch_jump(thread_data, &write_p, basic_block, 0, SETUP|INSERT_BRANCH);
#endif
          stop = true;    
        }
        
        break;
        
      case THUMB_REV16:
      case THUMB_REV1616:
      case THUMB_REVSH16:
        copy_thumb_16();
        it_cond_handled = true;
        break;
        
      case THUMB_IT16:
        thumb_it16_decode_fields(read_address, &condition, &mask);
        it_state.cond_inst_after_it = it_get_no_of_inst(mask) + 1;
        debug("No of cond instructions following from %p: %d\n", read_address, it_state.cond_inst_after_it);
        it_cond_handled = true;
        
        it_state.it_inst_addr = write_p;
        it_state.it_cond = condition;
        it_state.it_mask = mask | (it_state.it_cond & 0x1) << 4;
        it_state.it_initial_mask = mask;
        debug("initial it mask: 0x%x\n", mask);
        
        copy_thumb_16();
        break;
        
      case THUMB_NOP16:
      case THUMB_BKPT16:
      case THUMB_UDF16:
        copy_thumb_16();
        it_cond_handled = true;
        break;
        
      case THUMB_LDMFD16:
      case THUMB_STMEA16:
        copy_thumb_16();
        it_cond_handled = true;
        break;
        
      case THUMB_B_COND16:
        thumb_b_cond16_decode_fields(read_address, &condition, &imm8);
        branch_offset = ((int8_t)imm8) << 1;
        debug("Branch offset: %d\n", branch_offset);
        
        // Seems ok, but keep an eye on this
        target = (uint32_t)read_address + 4 + 1 + branch_offset;
        debug("Branch taken: 0x%x\n", target);

        thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                               true, IMM_SIZE, basic_block);

        // Mark this as the beggining of code emulating B
        thread_data->code_cache_meta[basic_block].exit_branch_type = cond_imm_thumb;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
        thread_data->code_cache_meta[basic_block].branch_skipped_addr = (uint32_t)read_address + 2 + 1;
        thread_data->code_cache_meta[basic_block].branch_condition = condition;

#ifdef DBM_LINK_COND_IMM
        if (type == mambo_bb) {
          branch_taken_address = cc_lookup(thread_data, target);
          branch_taken_cached = (branch_taken_address != UINT_MAX);
          branch_skipped_address = cc_lookup(thread_data, (uint32_t)read_address + 2 + 1);
          branch_skipped_cached = (branch_skipped_address != UINT_MAX);

          thumb_encode_cond_imm_branch(thread_data, &write_p, basic_block,
                                       branch_taken_cached ? branch_taken_address : target,
                                       branch_skipped_cached ? branch_skipped_address : ((uint32_t)read_address + 2 + 1),
                                       condition, branch_taken_cached, branch_skipped_cached, false);
        } else {
#endif
          thumb_encode_cond_imm_branch(thread_data, &write_p, basic_block,
                                       target,
                                       ((uint32_t)read_address + 2 + 1),
                                       condition, false, false, false);
#ifdef DBM_LINK_COND_IMM
        }
#endif
        stop = true;
        
        break;
        
      case THUMB_SVC16:
        thumb_sub_sp_i16(&write_p, 2);
        write_p++;

        // PUSH {R0-R12, R14}
        thumb_push_regs(&write_p, 0x5FFF);
        
        copy_to_reg_32bit(&write_p, r8, (uint32_t)read_address + 2 + 1);
        
        thumb_blx32_helper(write_p, thread_data->syscall_wrapper_addr);
        write_p += 2;

        thumb_scanner_deliver_callbacks(thread_data, POST_BB_C, &it_state, &bb_entry, -1,
                                        &write_p, &data_p, basic_block, type, false, &stop);
        // set the correct address for the PRE_BB_C event
        read_address++;
        bb_entry = read_address;
        thumb_scanner_deliver_callbacks(thread_data, PRE_BB_C, &it_state, &read_address, -1,
                                        &write_p, &data_p, basic_block, type, true, &stop);
        read_address--;
        break;
      
      case THUMB_B16:
        thumb_b16_decode_fields(read_address, &imm1);
        
        branch_offset = (imm1 & 0x400) ? 0xFFFFF000 : 0;
        branch_offset |= imm1 << 1;
        debug("offset: %d\n", branch_offset);
        target = (uint32_t)read_address + 4 + 1 + branch_offset;
        debug("target : 0x%x\n", target);
#ifdef DBM_INLINE_UNCOND_IMM
        if ((target - 1) <= (uint32_t)read_address) {
          if (inline_back_count >= MAX_BACK_INLINE) {
            block_address = lookup_or_stub(thread_data, target);
            thumb_cc_branch(thread_data, write_p, block_address);
            write_p += 2;

            thread_data->code_cache_meta[basic_block].exit_branch_type = trace_inline_max;

            stop = true;
            break;
          } else {
            inline_back_count++;
          }
        }
        /* TODO: handle branches to unmapped addresses cleanly
           This is a hack to avoid trying to elide the b.n 0x7e8c instruction in
           in some versions of ld.so */
        if ((uint32_t)target >= 0x8000) {
          thumb_scanner_deliver_callbacks(thread_data, POST_BB_C, &it_state, &bb_entry, -1,
                                          &write_p, &data_p, basic_block, type, false, &stop);
          // set the correct address for the PRE_BB_C event
          read_address = (uint16_t *)(target -1);
          bb_entry = read_address;
          thumb_scanner_deliver_callbacks(thread_data, PRE_BB_C, &it_state, &read_address, -1,
                                          &write_p, &data_p, basic_block, type, true, &stop);
          read_address--;
          break;
        }
#endif
        // Mark this as the beggining of code emulating B
        thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_imm_thumb;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
#ifdef DBM_LINK_UNCOND_IMM
        block_address = cc_lookup(thread_data, target);

        if (type == mambo_bb && block_address != UINT_MAX && (target & 0x1)) {
          thumb_cc_branch(thread_data, write_p, block_address);
        } else {
#endif
          thumb_simple_exit(thread_data, &write_p, basic_block, target);
#ifdef DBM_LINK_UNCOND_IMM
        }
#endif
        stop = true;
        break;
        
      // thumb_data_proc_12bit_i_32 instructions that use rn and rd
      case THUMB_ADCI32:
      case THUMB_ADDI32:
      case THUMB_ANDI32:
      case THUMB_BICI32:
      case THUMB_EORI32:
      case THUMB_ORNI32:
      case THUMB_ORRI32:
      case THUMB_RSBI32:
      case THUMB_SBCI32:
      case THUMB_SUBI32:
        thumb_data_proc_12bit_i_32_decode_fields(read_address, &imm1, &opcode, &set_flags, &rn, &imm3, &rdn, &imm8);
        
        assert(rn != pc && rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;

        break;
      // thumb_data_proc_12bit_i_32 instructions that only use rn
      case THUMB_CMNI32:
      case THUMB_CMPI32:
      case THUMB_TEQI32:
      case THUMB_TSTI32:
        thumb_data_proc_12bit_i_32_decode_fields(read_address, &imm1, &opcode, &set_flags, &rn, &imm3, &rdn, &imm8);
        
        assert(rn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_MOVI32:
        // check if dest is pc
        thumb_movi32_decode_fields(read_address, &imm1, &set_flags, &imm3, &rdn, &imm8);
        debug("MOVI32 rdn: %d, imm: %d\n", rdn, (imm1 << 11) | (imm3) | (imm8));
 
        assert(rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;
        
      case THUMB_MVNI32:
        thumb_mvni32_decode_fields(read_address, &imm1, &set_flags, &imm3, &rdn, &imm8);
        debug("MVNI32 rdn: %d, imm: %d\n", rdn, (imm1 << 11) | (imm3) | (imm8));
 
        assert(rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_MOVTI32:
        // check if dest is pc
        thumb_movti32_decode_fields(read_address, &imm1, &imm4, &imm3, &rdn, &imm8);
 
        assert(rdn != pc); 
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_MOVWI32:
        // check that dest isn't pc
        thumb_movwi32_decode_fields(read_address, &imm1, &imm4, &imm3, &rdn, &imm8);
        
        assert(rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;

        break;
        
      case THUMB_BFC32:
        thumb_bfc32_decode_fields(read_address, &imm3, &rdn, &imm2, &imm5);
        
        assert(rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_BFI32:
      case THUMB_SBFX32:
      case THUMB_SSAT_LSL32:
      case THUMB_SSAT_ASR32:
      case THUMB_SSAT1632:
      case THUMB_UBFX32:
      case THUMB_USAT_LSL32:
      case THUMB_USAT_ASR32:
      case THUMB_USAT1632:
        thumb_data_proc_bit_field_decode_fields(read_address, &opcode, &rn, &imm3, &rdn, &imm2, &imm5);
        
        assert(rn != pc && rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;

      case THUMB_NOP32:
        copy_thumb_32();
        it_cond_handled = true;
        break;
        
      case THUMB_ADDWI32:
        thumb_addwi32_decode_fields(read_address, &imm1, &rn, &imm3, &rdn, &imm8);
        
        assert(rdn != pc);
        if (rn == pc) {
          modify_in_it_pre(3);
          copy_to_reg_32bit(&write_p, rdn, get_original_pc() + ((imm1 << 11) | (imm3 << 8) | imm8 ));
          modify_in_it_post();
        } else {
          copy_thumb_32();
        }
        it_cond_handled = true;
        
        break;
        
      case THUMB_SUBWI32:
        thumb_subwi32_decode_fields(read_address, &imm1, &rn, &imm3, &rdn, &imm8);
        
        assert(rdn != pc);
        if (rn == pc) {
          modify_in_it_pre(3);
          copy_to_reg_32bit(&write_p, rdn, get_original_pc() - ((imm1 << 11) | (imm3 << 8) | imm8 ));
          modify_in_it_post();
        } else {
          copy_thumb_32();
        }
        it_cond_handled = true;
        
        break;
        
      case THUMB_LDRI32:
      case THUMB_LDRHI32:
      case THUMB_LDRSHI32:
      case THUMB_LDRBI32:
      case THUMB_LDRSBI32:
      case THUMB_LDRT32:
      case THUMB_LDRBT32:
      case THUMB_LDRHT32:
      case THUMB_LDRSBT32:
      case THUMB_LDRSHT32:
        thumb_ldri32_decode_fields(read_address, &rdn, &rn, &imm8, &pre_index, &upwards, &writeback);

        assert(rn != pc);

        if (rdn == pc) {
          thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_thumb;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        }

        if (rdn != pc) {
          copy_thumb_32();
          it_cond_handled = true;
        } else {
          if (rdn == pc) {
            assert(inst == THUMB_LDRI32);
#ifdef DBM_INLINE_HASH
            if (rn == sp) {
              if (writeback) {
                assert(upwards && pre_index == 0 && (imm8 & 3) == 0 && imm8 >= 4);
                if (imm8 == 4) {
                  thumb_push16(&write_p, (1 << r4) | (1 << r5));
                  write_p++;
                  thumb_ldr_sp16(&write_p, r5, 2);
                  write_p++;
                  thumb_str_sp16(&write_p, r6, 2);
                  write_p++;
                } else { // imm8 > 4
                  thumb_str_sp16(&write_p, r6, (imm8 >> 2) - 1);
                  write_p++;

                  thumb_ldri32(&write_p, r6, sp, imm8 - 4, 0, 1, 1);
                  write_p += 2;

                  thumb_push16(&write_p, (1 << r4) | (1 << r5));
                  write_p++;

                  thumb_movh16(&write_p, r5 >> 4, r6, r5);
                  write_p++;
                }
              } else { // !writeback
                assert(pre_index);

                int offset = (int)imm8;
                if (upwards == 0) {
                  offset = -offset;
                }
                offset += 12;
                upwards = (offset >= 0);
                imm8 = (uint32_t)abs(offset);
                assert(imm8 <= 0xFF);

                thumb_push16(&write_p, (1 << r4) | (1 << r5) | (1 << r6));
                write_p++;

                thumb_ldri32(&write_p, rdn, rn, imm8, pre_index, upwards, writeback);
                write_p += 2;

                while(1);
              }
            } else { // rn != sp
              while(1);
            }

            thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                   true, IHL_FSPACE, basic_block);
            thumb_inline_hash_lookup(thread_data, &write_p, basic_block, -1);
#else
            scratch_reg = (rn == r0) ? 1 : 0;
            branch_save_context(thread_data, &write_p, false);
            assert(rn != r3);

            if (rn == sp) {
              rn = APP_SP;
            }
            thumb_ldri32(&write_p, r0, rn, imm8, pre_index, upwards, writeback);
            write_p+=2;

            branch_jump(thread_data, &write_p, basic_block, target, SETUP|INSERT_BRANCH);
#endif
            stop = true;
          }
        }
        break;

      case THUMB_LDRWI32:
      case THUMB_LDRHWI32:
      case THUMB_LDRSHWI32:
      case THUMB_LDRBWI32:
      case THUMB_LDRSBWI32:
      case THUMB_STRWI32:
      case THUMB_STRBWI32:
      case THUMB_STRHWI32:
        thumb_strwi32_decode_fields(read_address, &rdn, &rn, &imm1);
        assert(rdn != pc && rn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;

      case THUMB_LDRBL32:
      case THUMB_LDRHL32:
      case THUMB_LDRL32:
      case THUMB_LDRSBL32:
      case THUMB_LDRSHL32:
        thumb_load_store_single_reg_imm12_32_decode_fields(read_address, &sign_ext, &upwards,
                                                           &size, &loadstore, &rn, &rdn, &imm1);
        assert(rdn != pc);

        modify_in_it_pre(5);
        copy_to_reg_32bit(&write_p, rdn, get_original_pc() + (upwards ? imm1 : -imm1));
        thumb_load_store_single_reg_imm12_32(&write_p, sign_ext, upwards, size,
                                             loadstore, rdn, rdn, 0);
        write_p += 2;
        modify_in_it_post();

        it_cond_handled = true;
        break;

      case THUMB_PLDI32:
        thumb_load_store_single_reg_imm12_32_decode_fields(read_address, &sign_ext, &upwards, &datasize, &loadstore, &rn, &rdn, &imm1);
        
        assert(rn != pc);
        
        copy_thumb_32();
        
        break;
        
      case THUMB_STRI32:
      case THUMB_STRHI32:
      case THUMB_STRBI32:
      case THUMB_STRT32:
      case THUMB_STRBT32:
      case THUMB_STRHT32:
        // check if src or address is pc
        thumb_load_store_single_reg_imm12_32_decode_fields(read_address, &sign_ext, &upwards, &datasize, &loadstore, &rn, &rdn, &imm1);
        
        debug("STR(B/H)32 sign_ext: %d, upwards: %d, rn: %d, rt: %d, imm: %d\n", sign_ext, upwards, rn, rdn, imm1);
        if (rn != pc && rdn != pc) {      
          copy_thumb_32();
          it_cond_handled = true;
        } else {
          fprintf(stderr, "PC involved\n");
          while(1);
        }

        break;

      case THUMB_LDR32:
      case THUMB_LDRH32:
      case THUMB_LDRSH32:
      case THUMB_LDRB32:
      case THUMB_LDRSB32:
        thumb_load_store_single_reg_off_32_decode_fields(read_address, &sign_ext, &datasize, &loadstore, &rn, &rt, &shift, &rm);
        
        assert(rn != pc && rm != pc);
        
        if (rt == pc) {
          if (inst != THUMB_LDR32) {
            fprintf(stderr, "LDR(S)H/B into PC at %p\n", read_address);
            while(1);
          }

          thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_thumb;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;

          assert(rn != sp && rm != sp);
          uint32_t r_target = r0;
#ifdef DBM_INLINE_HASH
          thumb_push16(&write_p, (1 << r4) | (1 << r5) | (1 << r6));
          write_p++;
          r_target = r5;
#else
          branch_save_context(thread_data, &write_p, true);
#endif
          thumb_ldr32 (&write_p, rn, r_target, shift, rm);
          write_p += 2;

#ifdef DBM_INLINE_HASH
        thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                               true, IHL_FSPACE, basic_block);
          thumb_inline_hash_lookup(thread_data, &write_p, basic_block, -1);
#else
          branch_jump(thread_data, &write_p, basic_block, target, SETUP|INSERT_BRANCH|LATE_APP_SP);
#endif
          stop = true;
        } else {        
          copy_thumb_32();
          it_cond_handled = true;
        }
        
        break;
        
      case THUMB_STR32:
      case THUMB_STRH32:
      case THUMB_STRB32:
        thumb_load_store_single_reg_off_32_decode_fields(read_address, &sign_ext, &datasize, &loadstore, &rn, &rt, &shift, &rm);
        
        assert(rn != pc && rt != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;

        break;
        
      case THUMB_PLD32:
        thumb_load_store_single_reg_off_32_decode_fields(read_address, &sign_ext, &datasize, &loadstore, &rn, &rt, &shift, &rm);
      
        assert(rm != pc && rn != pc);
        copy_thumb_32();
      
        break;
        
      case THUMB_PLDIM32:
        thumb_pld_t2_32_decode_fields(read_address, &sign_ext, &datasize, &load_store, &rn, &rt, &opcode, &imm8);
        
        assert(rn != pc);
        copy_thumb_32();
        
        break;
        
      // data_proc_const_shift instructions that use rn, rd and rm
      case THUMB_ADC32:
      case THUMB_ADD32:
      case THUMB_AND32:
      case THUMB_BIC32:
      case THUMB_EOR32:
      case THUMB_ORN32:
      case THUMB_ORR32:
      case THUMB_PKH32:
      case THUMB_RSB32:
      case THUMB_SBC32:
      case THUMB_SUB32:
        thumb_data_proc_const_shift_decode_fields(read_address, &opcode, &set_flags, &rn, &imm3, &rdn, &imm2, &shift_type, &rm);

        assert(rn != pc && rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;

        break;

      // data_proc_const_shift instructions that use rm and rd
      case THUMB_MOV32:
      case THUMB_LSLI32:
      case THUMB_LSRI32:
      case THUMB_ASRI32:
      case THUMB_RORI32:
        thumb_data_proc_const_shift_decode_fields(read_address, &opcode, &set_flags, &rn, &imm3, &rdn, &imm2, &shift_type, &rm);
        
        assert(rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_RRX32:
        thumb_rrx32_decode_fields(read_address, &set_flags, &rdn, &rm);
        
        assert(rdn != pc && rm != pc);
        copy_thumb_32();
        
        break;
       
      case THUMB_MVN32:
        thumb_mvn32_decode_fields(read_address, &set_flags, &imm3, &rdn, &imm2, &shift_type, &rm);
        
        assert(rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;

      // data_proc_const_shift instructions that use rm and rn
      case THUMB_CMN32:
      case THUMB_CMP32:
      case THUMB_TEQ32:
      case THUMB_TST32:
        thumb_data_proc_const_shift_decode_fields(read_address, &opcode, &set_flags, &rn, &imm3, &rdn, &imm2, &shift_type, &rm);
        
        assert(rn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_LSL32:
      case THUMB_LSR32:
      case THUMB_ASR32:
      case THUMB_ROR32:
        thumb_data_proc_reg_shift_decode_fields(read_address, &opcode, &set_flags, &rn, &rdn, &opcode2, &rm);
        
        assert(rn != pc && rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_SXTAB32:
      case THUMB_SXTAB1632:
      case THUMB_SXTAH32:
      case THUMB_UXTAB32:
      case THUMB_UXTAB1632:
      case THUMB_UXTAH32:
        thumb_data_proc_sign_zero_ext_decode_fields(read_address, &opcode, &rn, &rdn, &rotate, &rm);
        
        assert(rdn != pc && rm != pc && rn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_SXTB32:
      case THUMB_SXTB1632:
      case THUMB_SXTH32:
      case THUMB_UXTB32:
      case THUMB_UXTB1632:
      case THUMB_UXTH32:
        thumb_data_proc_sign_zero_ext_decode_fields(read_address, &opcode, &rn, &rdn, &rotate, &rm);
        
        assert(rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;

      case THUMB_SIMD_ADD_SUB32:
        thumb_simd_add_sub32_decode_fields(read_address, &opcode, &rn, &rdn, &opcode2, &rm);
        
        assert(rn != pc && rdn != pc && rm != pc);
        copy_thumb_32();

        break;
        
      case THUMB_CLZ32:
      case THUMB_QADD32:
      case THUMB_QDADD32:
      case THUMB_QDSUB32:
      case THUMB_QSUB32:
      case THUMB_RBIT32:
      case THUMB_REV32:
      case THUMB_REV1632:
      case THUMB_REVSH32:
      case THUMB_SEL32:
        thumb_data_proc_other_3reg_decode_fields(read_address, &opcode, &rn, &rdn, &opcode2, &rm);
        
        assert(rn != pc && rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
      
      case THUMB_MLA32:
      case THUMB_MLS32:
      case THUMB_SMMLA32:
      case THUMB_SMMLS32:
        thumb_data_proc_32_mult_decode_fields(read_address, &opcode, &rn, &racc, &rdn, &opcode2, &rm);
        
        assert(rn != pc && racc != pc && rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_MUL32:
        thumb_mul32_decode_fields(read_address, &rn, &rdn, &rm);
        
        assert(rn != pc && rdn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;

      case THUMB_SMLAD32:
      case THUMB_SMLSD32:
      case THUMB_USADA832:
      case THUMB_SMLABB32:
        thumb_data_proc_rd_rn_rm_ra_decode_fields(read_address, &rdn, &rn, &rm, &racc);

        assert(rn != pc && rdn != pc && rm != pc && racc != pc);
        copy_thumb_32();
        it_cond_handled = true;

        break;

      // data proc with rd, rn, rm
      case THUMB_SMUSD32:
      case THUMB_USAD832:
      case THUMB_SMMUL32:
      case THUMB_SMUAD32:
      case THUMB_UADD832:
      case THUMB_UQSUB832:
      case THUMB_SMULBB32:
        thumb_data_proc_rd_rn_rm_decode_fields(read_address, &rdn, &rn, &rm);
        
        assert(rdn != pc && rn != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_SMULL32:
        thumb_smull32_decode_fields(read_address, &rn, &rdlo, &rdhi, &rm);
        
        assert(rn != pc && rdlo != pc && rdhi != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_UMULL32:
        thumb_umull32_decode_fields(read_address, &rn, &rdlo, &rdhi, &rm);
        
        assert(rn != pc && rdlo != pc && rdhi != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;

      case THUMB_SDIV32:
      case THUMB_UDIV32:
        thumb_data_proc_64_mult_decode_fields(read_address, &opcode, &rn, &rdlo, &rdhi, &opcode2, &n_high, &m_swap, &rm);
        
        assert(rn != pc && rdhi != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;

        break;

      case THUMB_SMLAL32:
      case THUMB_SMLALD32:
      case THUMB_SMLSLD32:
      case THUMB_UMLAL32:
      case THUMB_UMAAL32:
        thumb_data_proc_64_mult_decode_fields(read_address, &opcode, &rn, &rdlo, &rdhi, &opcode2, &n_high, &m_swap, &rm);
        
        assert(rn != pc && rdlo != pc && rdhi != pc && rm != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;

      case THUMB_B32:
      case THUMB_BL32:
      case THUMB_BL_ARM32:
        thumb_branch32_decode_fields(read_address, &sign_bit, &offset_high, &link, &j1, &thumb_arm, &j2, &offset_low);
        debug("B32/BL32/BL_ARM32 sign_bit: %d, j1: %d, j2: %d, offset_high 0x%x, offset_low 0x%x\n",
              sign_bit, j1, j2, offset_high, offset_low);

        branch_offset = sign_bit ? 0xFF000000 : 0;
        branch_offset |= (j1 ^ sign_bit) ? 0 : 1 << 23;
        branch_offset |= (j2 ^ sign_bit) ? 0: 1 << 22;
        branch_offset |= offset_high << 12;
        branch_offset |= offset_low << 1;

        debug("branch_offset = 0x%x\n", branch_offset);

        if (link_bx_alt(thread_data, &write_p, it_state.cond_inst_after_it, (uint32_t)read_address + 5)) {
          it_cond_handled = true;
        }

        // Seems ok, but keep an eye on this
        target = (uint32_t)read_address + branch_offset + 4 + 1;
        if(inst == THUMB_BL_ARM32) target &= 0xFFFFFFFC;
        debug("branch_target = 0x%x\n", target);

        // Set the link register
        if (inst != THUMB_B32) {
          copy_to_reg_32bit(&write_p, lr, ((uint32_t)read_address) + 4 + 1);
        }

#ifdef DBM_INLINE_UNCOND_IMM
        if (inst != THUMB_BL_ARM32 && (type == mambo_trace || type == mambo_trace_entry)) {
          if ((target - 1) <= (uint32_t)read_address) {
            if (inline_back_count >= MAX_BACK_INLINE) {
              block_address = lookup_or_stub(thread_data, target);
              thumb_cc_branch(thread_data, write_p, block_address);
              write_p += 2;

              thread_data->code_cache_meta[basic_block].exit_branch_type = trace_inline_max;

              stop = true;
              break;
            } else {
              inline_back_count++;
            }
          }

          thumb_scanner_deliver_callbacks(thread_data, POST_BB_C, &it_state, &bb_entry, -1,
                                          &write_p, &data_p, basic_block, type, false, &stop);
          // set the correct address for the PRE_BB_C event
          read_address = (uint16_t *)(target - 1);
          bb_entry = read_address;
          thumb_scanner_deliver_callbacks(thread_data, PRE_BB_C, &it_state, &read_address, -1,
                                          &write_p, &data_p, basic_block, type, true, &stop);
          read_address -= 2;
        } else {
#endif
          thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                 true, DISP_CALL_SIZE, basic_block);

          if (inst == THUMB_BL_ARM32) {
            thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_blxi_thumb;
          } else {
            thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_imm_thumb;
          }
          thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
#ifdef DBM_LINK_UNCOND_IMM
          block_address = cc_lookup(thread_data, target);
          if (type == mambo_bb && block_address != UINT_MAX && (target & 0x1)) {
            debug("Found block for 0x%x at 0x%x\n", target, block_address);
            thumb_cc_branch(thread_data, write_p, block_address);
          } else {
#endif
            thumb_simple_exit(thread_data, &write_p, basic_block, target);
#ifdef DBM_LINK_UNCOND_IMM
          }
#endif
          stop = true;
#ifdef DBM_INLINE_UNCOND_IMM
        }
#endif

        break;
      case THUMB_B_COND32:
        // Warning: at some point we might want to restore the values of any scratch registers here
        thumb_b_cond32_decode_fields(read_address, &sign_bit, &condition, &offset_high, &j1, &j2, &offset_low);
        debug("B_COND32: sign_bit %d, j2: %d, j1: %d, offset_high: %x, offset_low %x\n", sign_bit, j2, j1, offset_high, offset_low);
        branch_offset = sign_bit ? 0xFFF00000 : 0;
        branch_offset |= j2 << 19;
        branch_offset |= j1 << 18;
        branch_offset |= offset_high << 12;
        branch_offset |= offset_low << 1;

        debug("branch_offset = %d\n", branch_offset);

        // Seems ok, but keep an eye on this
        target = (uint32_t)read_address + branch_offset + 4 + 1;
        debug("Computed target: 0x%x\n", target);

        thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                               true, IMM_SIZE, basic_block);

        // Mark this as the beggining of code emulating B
        thread_data->code_cache_meta[basic_block].exit_branch_type = cond_imm_thumb;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
        thread_data->code_cache_meta[basic_block].branch_skipped_addr = (uint32_t)read_address + 4 + 1;
        thread_data->code_cache_meta[basic_block].branch_condition = condition;

#ifdef DBM_LINK_COND_IMM
        if (type == mambo_bb) {
          if (target & 0x1) {
            branch_taken_address = cc_lookup(thread_data, target);
            branch_taken_cached = (branch_taken_address != UINT_MAX);
          } else {
            branch_taken_cached = false;
          }
          branch_skipped_address = cc_lookup(thread_data, (uint32_t)read_address + 4 + 1);
          branch_skipped_cached = (branch_skipped_address != UINT_MAX);

          thumb_encode_cond_imm_branch(thread_data, &write_p, basic_block,
                                       branch_taken_cached ? branch_taken_address : target,
                                       branch_skipped_cached ? branch_skipped_address : ((uint32_t)read_address + 4 + 1),
                                       condition, (branch_taken_address != UINT_MAX), (branch_skipped_address != UINT_MAX), false);
        } else {
#endif
          thumb_encode_cond_imm_branch(thread_data, &write_p, basic_block,
                                       target,
                                       ((uint32_t)read_address + 4 + 1),
                                       condition, false, false, false);
#ifdef DBM_LINK_COND_IMM
        }
#endif
        stop = true;
        break;

      case THUMB_DSB32:
      case THUMB_DMB32:
      case THUMB_ISB32:
      case THUMB_CLREX32:
        copy_thumb_32();
        break;

      case THUMB_MSR32:
        thumb_msr32_decode_fields(read_address, &rn, &mask);
        assert(rn != pc);
        copy_thumb_32();
        break;

      case THUMB_MRS32:
        thumb_mrs32_decode_fields(read_address, &rdn);
        assert(rdn != pc);
        copy_thumb_32();
        break;
        
      case THUMB_LDRD32:
        thumb_ldrd32_decode_fields(read_address, &pre_index, &upwards, &writeback, &rn, &rt, &rdn, &imm8);
        assert(rt != pc && rdn != pc);

        if (rn == pc) {
          assert(pre_index == 1 && writeback == 0);
          imm8 <<= 2;
          uint32_t addr = get_original_pc() + (upwards ? imm8 : -imm8);
          modify_in_it_pre(5);
          copy_to_reg_32bit(&write_p, rdn, addr);
          thumb_ldrd32(&write_p, 1, 1, 0, rdn, rt, rdn, 0);
          write_p += 2;
          modify_in_it_post();
        } else {
          copy_thumb_32();
        }
        it_cond_handled = true;
        break;

      case THUMB_STRD32:
        thumb_strd32_decode_fields(read_address, &pre_index, &upwards, &writeback, &rn, &rt, &rdn, &imm8);
        
        assert(rn != pc && rt != pc && rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_LDREX32:
      case THUMB_STREX32:
        switch(inst) {
          case THUMB_LDREX32:
            thumb_ldrex32_decode_fields(read_address, &rn, &rt, &imm8);
            ldrex = true;
            rdn = 0;
            break;
          case THUMB_STREX32:
            thumb_strex32_decode_fields(read_address, &rn, &rt, &rdn, &imm8);
            ldrex = false;
            break;
        }
        
        assert(rn != pc && rt != pc && rdn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        
        break;
        
      case THUMB_LDREXB32:
      case THUMB_LDREXH32:
      case THUMB_STREXB32:
      case THUMB_STREXH32:
        thumb_strexb32_decode_fields(read_address, &rn, &rt, &rdn);
        assert(rn != pc && rt != pc);
        if (inst == THUMB_STREXB32 || inst == THUMB_STREXH32) {
          assert(rdn != pc);
          ldrex = false;
        } else {
          ldrex = true;
        }
        copy_thumb_32();
        it_cond_handled = true;

        break;

      case THUMB_LDREXD32:
      case THUMB_STREXD32:
        thumb_strexd32_decode_fields(read_address, &rn, &rt, &rt2, &rdn);
        assert(rn != pc && rt != pc && rt2 != pc);
        if (inst == THUMB_STREX32) assert(rdn != pc);
        ldrex = (inst == THUMB_STREXD32);
        copy_thumb_32();
        it_cond_handled = true;

        break;

      case THUMB_TBB32:
      case THUMB_TBH32:
        // Branch to PC + [value from rn + rm << 1]
        thumb_tbh32_decode_fields(read_address, &rn, &rm);
        assert(rm != pc);
        assert(rn != sp && rm != sp);
        
        scratch_reg = r0;
        while (rn == scratch_reg || rm == scratch_reg) {
          scratch_reg++;
        }
        scratch_reg2 = scratch_reg+1;
        while (rn == scratch_reg2 || rm == scratch_reg2) {
          scratch_reg2++;
        }
        assert(scratch_reg2 <= 2);

#ifdef DBM_TRACES
        if (type == mambo_trace || type == mambo_trace_entry) {
#endif
          thread_data->code_cache_meta[basic_block].exit_branch_type = (inst == THUMB_TBB32) ? tbb : tbh;
#ifdef DBM_TRACES
        } else {
          thread_data->code_cache_meta[basic_block].exit_branch_type = tb_indirect;
        }
#endif
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;

#ifdef DBM_TB_DIRECT
        if (rn == pc) {
          debug("TB: w: %p r: %p, BB: %d\n", write_p, read_address, basic_block);

  #ifndef DBM_TRACES
          // At least two consecutive BBs are needed
          assert(thread_data->free_block == basic_block+1);
          /*basic_block = */thread_data->free_block++;
          data_p += BASIC_BLOCK_SIZE;
          thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                 true, 472, basic_block);
  #else
          if (type == mambo_trace || type == mambo_trace_entry) {
  #endif
            thread_data->code_cache_meta[basic_block].rn = INT_MAX;
            thread_data->code_cache_meta[basic_block].free_b = 0;

  #ifdef FAST_BT
            thumb_cmpi32 (&write_p, 0, rm, 0, TB_CACHE_SIZE-1);
  #else
            thumb_cmpi32 (&write_p, 0, rm, 0, MAX_TB_INDEX-1);
  #endif
            write_p += 2;
            thumb_it16(&write_p, HI, 8);
            write_p++;
  #if defined(DBM_D_INLINE_HASH) && !defined(TB_STATS)
    #ifdef FAST_BT
            thumb_b32_helper(write_p, (uint32_t)write_p + TB_CACHE_SIZE*4 + 16 + (((uint32_t)write_p & 2) ? 0 : 2));
    #else
            thumb_b32_helper(write_p, (uint32_t)write_p + MAX_TB_INDEX + TB_CACHE_SIZE*4 + 10);
    #endif
  #else
    #ifdef FAST_BT
            thumb_b32_helper(write_p, (uint32_t)write_p + TB_CACHE_SIZE*4 + 14 + (((uint32_t)write_p & 2) ? 0 : 2));
    #else
            thumb_b32_helper(write_p, (uint32_t)write_p + MAX_TB_INDEX + TB_CACHE_SIZE*4 + 8);
    #endif
  #endif
            write_p += 2;

  #ifdef FAST_BT
            thumb_bx16(&write_p, pc);
            write_p++;

            if (((uint32_t)write_p) & 2) {
              write_p++;
            }
            arm_ldr((uint32_t **)&write_p, LDR_REG, pc, pc, (LSL << 5) | (2 << 7) | rm, 1, 1, 0);
            write_p += 2;
  #else
            thumb_tbb32(&write_p, pc, rm);
            write_p += 2;
  #endif

  #ifdef FAST_BT
            *write_p = 0;
            write_p++;
            *write_p = 0;
            write_p++;
            for (int i = 0; i < TB_CACHE_SIZE; i++) {
              *(uint32_t *)write_p = (uint32_t)write_p + ((TB_CACHE_SIZE -i) * 4) + 1;
              write_p += 2;
            }
  #else
            // Initially all indexes go to the slow dispatcher
            for (int i = 0; i < MAX_TB_INDEX/2; i++) {
              *write_p = (MAX_TB_INDEX/2 + TB_CACHE_SIZE*2);
              *write_p |= *write_p << 8;
              write_p++;
            }
            
            for (int i = 0; i < TB_CACHE_SIZE; i++) {
              thumb_b32_helper(write_p, (uint32_t)write_p + (TB_CACHE_SIZE -i) * 4);
              write_p += 2;
            }
  #endif
#endif // DBM_TB_DIRECT
#if defined(DBM_D_INLINE_HASH) && defined(DBM_TB_DIRECT)
            thumb_b16(&write_p, ((uint32_t)write_p) & 2 ? 61 : 60);
            write_p++;
#endif
#if defined(DBM_TB_DIRECT) && defined (DBM_TRACES)
          }
#endif
#ifdef DBM_D_INLINE_HASH
          uint32_t sr[3];

          sr[0] = 3;
          while (sr[0] == rn || sr[0] == rm) {
            sr[0]++;
          }
          
          sr[1] = sr[0] + 1;
          while (sr[1] == rn || sr[1] == rm) {
            sr[1]++;
          }
          
          sr[2] = sr[1] + 1;
          while (sr[2] == rn || sr[2] == rm) {
            sr[2]++;
          }
          
          reglist = (1 << sr[0]) | (1 << sr[1]) | (1 << sr[2]);
          thumb_push16(&write_p, reglist);
          write_p++;

          thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                 true, 118, basic_block);
   
          if (rn == pc) {
            copy_to_reg_32bit(&write_p, sr[1], (uint32_t)read_address + 4);
            rn = sr[1];
          }
          
          if (inst == THUMB_TBB32) {
            thumb_ldrb32(&write_p, rn, sr[0], 0, rm);
          } else {
            thumb_ldrh32(&write_p, rn, sr[0], 1, rm);
          }
          write_p += 2;
          
          thumb_add32(&write_p, 0, sr[1], 0, sr[1], 1,  LSL, sr[0]);
          write_p+=2;
          thumb_addi32 (&write_p, 0, 0,	sr[1], 0, sr[0], 1);
          write_p+=2;

          thumb_inline_hash_lookup(thread_data, &write_p, basic_block, sr[0], sr[1], sr[2], reglist, false, 4);

          ihl_result_branch(thread_data, IHL_BRANCH_LDR_PC_PC, &write_p, reglist, sr, false, 4);
          
          rn = pc;

  #ifdef DBM_TRACES
          if (type == mambo_bb) {
            stop = true;
            break;
          }
  #endif
#endif // DBM_D_INLINE_HASH
#ifdef DBM_TB_DIRECT
        }
#endif
        assert(rn == pc);

        branch_save_context(thread_data, &write_p, true);

        // Save the index for use by the TB linker
        copy_to_reg_32bit(&write_p, scratch_reg, (uint32_t)&thread_data->code_cache_meta[basic_block].rn);
        thumb_strwi32(&write_p, rm, scratch_reg, 0);
        write_p += 2;
 
        copy_to_reg_32bit(&write_p, scratch_reg, (uint32_t)read_address + 4);
        if (rn == pc) {
          rn = scratch_reg;
        }
        
        // load into R1, from rn + rm << 1
        if (inst == THUMB_TBB32) {
          thumb_ldrb32(&write_p, rn, scratch_reg2, 0, rm);
        } else {
          thumb_ldrh32(&write_p, rn, scratch_reg2, 1, rm);
        }
        write_p += 2;
        
        thumb_add32(&write_p, 0, scratch_reg, 0, scratch_reg, 1,  LSL, scratch_reg2);
        write_p+=2;
        thumb_addi32 (&write_p, 0, 0,	scratch_reg, 0, r0, 1);
        write_p+=2;

        branch_jump(thread_data, &write_p, basic_block, 0, SETUP|INSERT_BRANCH|LATE_APP_SP);

        stop = true;
        
        break;

      case THUMB_STMEA32:        
      case THUMB_STMFD32:
        thumb_load_store_multiple32_decode_fields(read_address, &opcode, &writeback, &load_store, &rn, &reglist);
        assert(rn != pc);
        assert((reglist & (1 << pc)) == 0);
        
        copy_thumb_32();
        it_cond_handled = true;
        break;
        
      case THUMB_LDMFD32:
      case THUMB_LDMEA32:
        thumb_load_store_multiple32_decode_fields(read_address, &opcode, &writeback, &load_store, &rn, &reglist);
        assert(rn != pc && (!writeback || (reglist & (1 << rn)) == 0));

        if (reglist & (1<<pc)) {
          if (link_bx_alt(thread_data, &write_p, it_state.cond_inst_after_it, (uint32_t)read_address + 5)) {
            it_cond_handled = true;
          }

          assert(writeback);
          if (reglist & 0x7FFF) {
            thumb_load_store_multiple32(&write_p, opcode, writeback, load_store, rn, reglist & 0x7FFF);
            write_p += 2;
          }

          thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_reg_thumb;
          thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;

#ifdef DBM_INLINE_HASH
          if (rn == sp) {
            assert(inst == THUMB_LDMFD32);

            thumb_push16(&write_p, (1 << r4) | (1 << r5));
            write_p++;
            thumb_ldr_sp16(&write_p, r5, 2);
            write_p++;
            thumb_str_sp16(&write_p, r6, 2);
            write_p++;
          } else {
            thumb_push16(&write_p, (1 << r4) | (1 << r5) | (1 << r6));
            write_p++;

            thumb_load_store_multiple32(&write_p, opcode, writeback, load_store, r0, reglist);
            write_p += 2;
          }
          thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                                 true, IHL_FSPACE, basic_block);
          thumb_inline_hash_lookup(thread_data, &write_p, basic_block, -1);
#else
          branch_save_context(thread_data, &write_p, false);
          assert(rn != r3);
          if (rn == sp) {
            rn = APP_SP;
          }
          thumb_load_store_multiple32(&write_p, opcode, writeback, load_store, rn, 1 << 0);
          write_p+=2;
          branch_jump(thread_data, &write_p, basic_block, 0, SETUP|INSERT_BRANCH);
#endif
          stop = true;
        } else {
          if (inst == THUMB_LDMFD32 && writeback && rn == sp) {
            set_inst_pop_regs = write_p;
            inst_pop_regs_data = data_p;
            poped_regs = reglist;
          }
          
          copy_thumb_32();
          it_cond_handled = true;
        }

        break;
      
      case THUMB_MCR32:
        thumb_mcr32_decode_fields(read_address, &opc1, &crn, &rt, &coproc, &opc2, &crm);

        assert(rt != pc);
        copy_thumb_32();

        break;
      case THUMB_MRC32:
        thumb_mrc32_decode_fields(read_address, &opc1, &crn, &rt, &coproc, &opc2, &crm);

        if (coproc == 15 && opc1 == 0 && crn == 13 && crm == 0 && opc2 == 3) {
          assert(rt != pc);

          modify_in_it_pre(5);
          copy_to_reg_32bit(&write_p, rt, (uint32_t)(&thread_data->tls));
          thumb_ldrwi32(&write_p, rt, rt, 0);
          write_p+=2;
          modify_in_it_post();
        } else if (opc1 == 0b111 && crn == 0b0001 && coproc == 0b1010) {
          // This instruction transfers the FPSCR.{N, Z, C, V} condition flags to the APSR.{N, Z, C, V} condition flags.
          copy_thumb_32();
        } else {
          assert(rt != pc);
          copy_thumb_32();
        }
        it_cond_handled = true;
        
        break;
        
      /* NEON and VFP instructions which might access the PC */
      case THUMB_VFP_VLDM_DP:
      case THUMB_VFP_VLDM_SP:
      case THUMB_VFP_VSTM_DP:
      case THUMB_VFP_VSTM_SP:
        thumb_vfp_ld_st_m_decode_fields(read_address, &p, &upwards, &writeback, &rn, &d, &vd, &imm8);
        assert(rn != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;
        
      case THUMB_VFP_VLDR_DP:
      case THUMB_VFP_VLDR_SP:
      case THUMB_VFP_VSTR_DP:
      case THUMB_VFP_VSTR_SP:
        thumb_vfp_vldr_vstr_decode_fields(read_address, &upwards, &rn, &d, &vd, &imm8);
        
        if(rn == pc) {
          modify_in_it_pre(7);

          thumb_push16(&write_p, 1 << r0);
          write_p++;

          copy_to_reg_32bit(&write_p, r0, get_original_pc());

          switch(inst) {
            case THUMB_VFP_VLDR_DP:
              thumb_vfp_vldr_dp(&write_p, upwards, r0, d, vd, imm8);
              break;
            case THUMB_VFP_VLDR_SP:
              thumb_vfp_vldr_sp(&write_p, upwards, r0, d, vd, imm8);
              break;
            default:
              fprintf(stderr, "inst: %d unimplemented\n", inst);
              while(1);
          }
          write_p += 2;

          thumb_pop16(&write_p, 1 << r0);
          write_p++;

          modify_in_it_post();
        } else {
          copy_thumb_32();
        }

        it_cond_handled = true;
        break;

      case THUMB_VFP_VMOV_CORE_SP:
        thumb_vfp_vmov_core_sp_decode_fields(read_address, &opcode, &rt, &n, &vn);
        assert(rt != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;

      case THUMB_VFP_VMOV_2CORE_DP:
        thumb_vfp_vmov_2core_dp_decode_fields(read_address, &to_arm, &rt, &rt2, &m, &vm);
        assert(rt != pc && rt2 != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;

      case THUMB_VFP_VMSR:
        thumb_vfp_vmsr_decode_fields(read_address, &rt);
        assert(rt != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;

      case THUMB_NEON_VDUP_CORE: {
        uint32_t b, e, q;
        thumb_neon_vdup_core_decode_fields(read_address, &b, &e, &q, &d, &vd, &rt);
        assert(rt != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;
      }

      case THUMB_NEON_VLDX_S_O:
      case THUMB_NEON_VSTX_S_O:
      case THUMB_NEON_VLDX_S_A:
      case THUMB_NEON_VLDX_M:
      case THUMB_NEON_VSTX_M: {
        thumb_neon_vstx_m_decode_fields(read_address, &opcode, &size, &d, &vd, &rn, &align, &rm);
        assert(rn != pc); // rm == pc has a special meaning, doesn't actually use the PC
        copy_thumb_32();
        it_cond_handled = true;
        break;
      }

      case THUMB_VFP_VMOV_CORE_SCAL:
        thumb_vfp_vmov_core_scal_decode_fields(read_address, &d, &vd, &opc1, &opc2, &rt);
        assert(rt != pc);
        copy_thumb_32();
        it_cond_handled = true;
        break;

      /* NEON and VFP instructions which can't access the PC */
      case THUMB_NEON_VABD_I:
      case THUMB_NEON_VADD_I:
      case THUMB_NEON_VADDL:
      case THUMB_NEON_VADDW:
      case THUMB_NEON_VAND:
      case THUMB_NEON_VBIC:
      case THUMB_NEON_VBSL:
      case THUMB_NEON_VCEQ_I:
      case THUMB_NEON_VCGT_I:
      case THUMB_NEON_VCLTZ:
      case THUMB_NEON_VDUP_SCAL:
      case THUMB_NEON_VEOR:
      case THUMB_NEON_VEXT:
      case THUMB_NEON_VHADD:
      case THUMB_NEON_VMAX_I:
      case THUMB_NEON_VMIN_I:
      case THUMB_NEON_VMLAL_I:
      case THUMB_NEON_VMLA_SCAL:
      case THUMB_NEON_VMLS_SCAL:
      case THUMB_NEON_VMOVI:
      case THUMB_NEON_VMOVL:
      case THUMB_NEON_VMOVN:
      case THUMB_NEON_VMUL_I:
      case THUMB_NEON_VMULL_I:
      case THUMB_NEON_VMUL_SCAL:
      case THUMB_NEON_VMVN:
      case THUMB_NEON_VNEG:
      case THUMB_NEON_VORN:
      case THUMB_NEON_VORR:
      case THUMB_NEON_VPADD_I:
      case THUMB_NEON_VPADDL:
      case THUMB_NEON_VQADD:
      case THUMB_NEON_VQMOVN:
      case THUMB_NEON_VQMOVUN:
      case THUMB_NEON_VQRSHRUN:
      case THUMB_NEON_VQSHRUN:
      case THUMB_NEON_VQSUB:
      case THUMB_NEON_VREV32:
      case THUMB_NEON_VREV64:
      case THUMB_NEON_VRHADD:
      case THUMB_NEON_VRSHR:
      case THUMB_NEON_VRSHRN:
      case THUMB_NEON_VSHL:
      case THUMB_NEON_VSHLI:
      case THUMB_NEON_VSHLL:
      case THUMB_NEON_VSHR:
      case THUMB_NEON_VSHRN:
      case THUMB_NEON_VSLI:
      case THUMB_NEON_VSUB_I:
      case THUMB_NEON_VSUBL:
      case THUMB_NEON_VSUBW:
      case THUMB_NEON_VSWP:
      case THUMB_NEON_VTRN:
      case THUMB_NEON_VTST:
      case THUMB_VFP_VABS:
      case THUMB_VFP_VADD:
      case THUMB_VFP_VCMP:
      case THUMB_VFP_VCMPE:
      case THUMB_VFP_VCMPEZ:
      case THUMB_VFP_VCMPZ:
      case THUMB_VFP_VCVT_DP_SP:
      case THUMB_VFP_VCVT_F_FP:
      case THUMB_VFP_VCVT_F_I:
      case THUMB_VFP_VDIV:
      case THUMB_VFP_VMLA_F:
      case THUMB_VFP_VMLS_F:
      case THUMB_VFP_VMOV:
      case THUMB_VFP_VMOVI:
      case THUMB_VFP_VMRS: // rt=0xF is CPSR
      case THUMB_VFP_VMUL:
      case THUMB_VFP_VNEG:
      case THUMB_VFP_VNMLA:
      case THUMB_VFP_VNMLS:
      case THUMB_VFP_VNMUL:
      case THUMB_VFP_VPOP:
      case THUMB_VFP_VPUSH:
      case THUMB_VFP_VSQRT:
      case THUMB_VFP_VSUB:
        copy_thumb_32();
        it_cond_handled = true;
        break;

      case THUMB_INVALID:
      default:
        if (read_address != start_scan) {
          thumb_b32_helper(write_p, lookup_or_stub(thread_data, (uint32_t)read_address + 1));
          stop = true;
          it_cond_handled = true; // If execution actually reached this inst, something is broken anyway
          fprintf(stderr, "WARN: deferred scanning because of unknown instruction at: %p\n", read_address);
          break;
        } else {
          fprintf(stderr, "Unknown thumb instruction: %d at %p\n", inst, read_address);
          while(1);
          exit(EXIT_FAILURE);
       }
    }
    
    if (it_state.cond_inst_after_it > 0) {
      if(!it_cond_handled) {
        fprintf(stderr, "Didn't handle instruction-after IT at %p, inst: %d\n", read_address, inst);
        while(1);
      }
      do_it_iter(&it_state);
    }
#ifdef PLUGINS_NEW
    } // if(!skip_inst)
#endif
    
    if ((uint16_t *)data_p <= write_p) {
      fprintf(stderr, "%d, inst: %p, :write: %p\n", inst, data_p, write_p);
      while(1);
    }
    
    if (!stop) {
      thumb_check_free_space(thread_data, &write_p, &data_p, &it_state,
                             true, MIN_FSPACE, basic_block);
    }
    debug("\n");
#ifdef PLUGINS_NEW
    thumb_scanner_deliver_callbacks(thread_data, POST_INST_C, &it_state, &read_address, inst, &write_p,
                                    &data_p, basic_block, type, !stop, &stop);
#endif

    if (inst < THUMB_ADC32) {
      read_address++;
    } else {
      read_address+= 2;
    }
  }

  thumb_scanner_deliver_callbacks(thread_data, POST_BB_C, &it_state, &bb_entry, -1,
                                  &write_p, &data_p, basic_block, type, false, &stop);
  thumb_scanner_deliver_callbacks(thread_data, POST_FRAGMENT_C, &it_state, &start_scan, -1,
                                  &write_p, &data_p, basic_block, type, false, &stop);

  if (ldrex) {
    if (thread_data->code_cache_meta[basic_block].exit_branch_type != uncond_imm_thumb
        && thread_data->code_cache_meta[basic_block].exit_branch_type != cond_imm_thumb
        && thread_data->code_cache_meta[basic_block].exit_branch_type != cbz_thumb) {
      fprintf(stderr, "WARN: Basic block containing LDREX and no matching STREX "
                      "ends with branch type that can not be directly linked\n");
    }
  }
  
  // We haven't strictly enforced updating write_p after the last instruction
  return ((uint32_t)write_p - start_address + 4);
}

void thumb_encode_stub_bb(dbm_thread *thread_data, int basic_block, uint32_t target) {
  uint16_t *write_p = (uint16_t *)&thread_data->code_cache->blocks[basic_block];
  uint32_t *data_p = (uint32_t *)write_p;
  data_p += BASIC_BLOCK_SIZE;

  thumb_pop16(&write_p, (1 << r5) | (1 << r6));
  write_p++;

  thumb_simple_exit(thread_data, &write_p, basic_block, target);
}

#endif // __arm__

```

`arch/aarch64/dispatcher_aarch64.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2015-2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/
.global start_of_dispatcher_s
start_of_dispatcher_s:

.global push_neon
push_neon:
  STP  Q0,  Q1, [SP, #-512]!
  STP  Q2,  Q3, [SP,   #32]
  STP  Q4,  Q5, [SP,   #64]
  STP  Q6,  Q7, [SP,   #96]
  STP  Q8,  Q9, [SP,  #128]
  STP Q10, Q11, [SP,  #160]
  STP Q12, Q13, [SP,  #192]
  STP Q14, Q15, [SP,  #224]
  STP Q16, Q17, [SP,  #256]
  STP Q18, Q19, [SP,  #288]
  STP Q20, Q21, [SP,  #320]
  STP Q22, Q23, [SP,  #352]
  STP Q24, Q25, [SP,  #384]
  STP Q26, Q27, [SP,  #416]
  STP Q28, Q29, [SP,  #448]
  STP Q30, Q31, [SP,  #480]
  RET

.global pop_neon
pop_neon:
  LDP  Q2,  Q3, [SP, #32]
  LDP  Q4,  Q5, [SP, #64]
  LDP  Q6,  Q7, [SP, #96]
  LDP  Q8,  Q9, [SP, #128]
  LDP Q10, Q11, [SP, #160]
  LDP Q12, Q13, [SP, #192]
  LDP Q14, Q15, [SP, #224]
  LDP Q16, Q17, [SP, #256]
  LDP Q18, Q19, [SP, #288]
  LDP Q20, Q21, [SP, #320]
  LDP Q22, Q23, [SP, #352]
  LDP Q24, Q25, [SP, #384]
  LDP Q26, Q27, [SP, #416]
  LDP Q28, Q29, [SP, #448]
  LDP Q30, Q31, [SP, #480]
  LDP  Q0,  Q1, [SP], #512
  RET

.global push_x4_x21
push_x4_x21:
  STP  X4,  X5, [SP, #-144]!
  STP  X6,  X7, [SP, #16]
  STP  X8,  X9, [SP, #32]
  STP X10, X11, [SP, #48]
  STP X12, X13, [SP, #64]
  STP X14, X15, [SP, #80]
  STP X16, X17, [SP, #96]
  STP X18, X19, [SP, #112]
  STP X20, X21, [SP, #128]
  RET

.global pop_x4_x21
pop_x4_x21:
  LDP  X6,  X7, [SP, #16]
  LDP  X8,  X9, [SP, #32]
  LDP X10, X11, [SP, #48]
  LDP X12, X13, [SP, #64]
  LDP X14, X15, [SP, #80]
  LDP X16, X17, [SP, #96]
  LDP X18, X19, [SP, #112]
  LDP X20, X21, [SP, #128]
  LDP  X4,  X5, [SP], #144
  RET

.global dispatcher_trampoline
dispatcher_trampoline:
  // PUSH all general purpose registers but X0, X1
  // X0 and X1 are pushed by the exit stub
  STP  X2,  X3, [SP, #-48]!
  STP X29, X30, [SP, #16]
  STR  X0,      [SP, #40]
  BL push_x4_x21

  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR

  ADD X2, SP, #176
  LDR X3, disp_thread_data
  LDR X9, dispatcher_addr
  BL push_neon

  BLR X9

  BL pop_neon
  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21

  BL pop_x4_x21
  LDP X29, X30, [SP, #16]
  LDP  X0,  X1, [SP, #32]
  LDP  X2,  X3, [SP], #48

  B checked_cc_return

dispatcher_addr: .quad dispatcher


.global trace_head_incr
trace_head_incr:
  /*
   * X1 = Basic Block number
   * X30 = Address to return on the code cache.
   * X2 = address of the counter
   */
  STP      X2,  X3, [SP, #-16]!
  // Leave space for storing the address of the counter to X2
  NOP  // MOVZ X2, #address_first_half_word
  NOP  // MOVK X2, #address_second_half_word, lsl #16
  NOP  // MOVK X2, #address_third_half_word,  lsl #32
  NOP  // MOVK X2, #address_fourth_half_word, lsl #48
  LDRB W3, [X2, X1]
  SUB  W3,  W3, #1
  STRB W3, [X2, X1]
  CBZ  W3,  create_trace_trampoline
  LDP  X2,  X3, [SP], #16
  RET

create_trace_trampoline:
  LDP X2, X30, [SP, #16]
  STP X0,  X2, [SP, #16]

  STP X29, X30, [SP, #-32]!
  BL push_x4_x21

  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR

  /*
   * create_trace(dbm_thread   *thread_data,   X0
   *              uint32_t      bb_source,     X1
   *              cc_addr_pair *trace_addr)    X2
   */
  ADD X2, SP, #160
  LDR X0, disp_thread_data
  LDR X3, =create_trace
  BL push_neon

  BLR X3

  BL pop_neon
  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21

  BL pop_x4_x21
  /* Stack layout:
   * SP ->| X29 | X30 | SP + 0
   *      | TPC | SPC | SP + 16
   *      | X2  | X3  | SP + 32
   *      ---------------------
   *      | X0  | X1  | SP + 48 (Pushed by the basic block
   *                             and popped in the newly created trace)
   */
  LDP  X0,  X1, [SP, #16]
  LDP  X2,  X3, [SP, #32]
  LDP X29, X30, [SP], #48

  B checked_cc_return


.global syscall_wrapper
.global syscall_wrapper_svc
syscall_wrapper:
  STP X30, X29, [SP, #-16]!
  BL push_x4_x21
  STP X0, X1, [SP, #-32]!
  STP X2, X3, [SP, #16]
  BL push_neon

  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR

  MOV X0, X8
  ADD X1, SP, #512
  MOV X2, X29
  LDR X3, disp_thread_data
  LDR X4, syscall_handler_pre_addr

  BLR X4

  CBZ X0, s_w_r

  ADD X9, SP, #512
  LDP X0, X1, [X9, #0]
  LDP X2, X3, [X9, #16]
  LDP X4, X5, [X9, #32]
  LDP X6, X7, [X9, #48]
  LDR X8,     [X9, #64]

  // Balance the stack on rt_sigreturn, which doesn't return here
  CMP X8, #0x8b
  BNE svc
  ADD SP, SP, #(64 + 144 + 512)

svc: SVC 0
syscall_wrapper_svc:
  ADD X1, SP, #512
  STR X0, [X1, #0]
  MOV X0, X8
  MOV X2, X29
  LDR X3, disp_thread_data
  LDR X4, syscall_handler_post_addr
  BLR X4

s_w_r:
  BL pop_neon
  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21

  LDP X2, X3, [SP, #16]
  LDP X0, X1, [SP], #32
  BL pop_x4_x21
  LDP X29, X30, [SP, #16]
  STP X0, X1, [SP, #16]
  LDP X0, X1, [SP], #16

  B checked_cc_return

syscall_handler_pre_addr: .quad syscall_handler_pre
syscall_handler_post_addr: .quad syscall_handler_post

.global disp_thread_data
disp_thread_data: .quad 0

.global send_self_signal

.global checked_cc_return
checked_cc_return:
  STR X2, [SP, #-16]!
  LDR X2, th_is_pending_ptr
  LDR W2, [X2]
  CBNZ W2, deliver_signals_trampoline
  LDR X2, [SP], #16
  BR X0
deliver_signals_trampoline:
  STP X0, X1, [SP, #-16]!
  MOV X0, X1 // set the SPC argument

  STR X3, [SP, #-64]!
  STP X29, X30, [SP, #16]
  ADD X1, SP, #32
  BL push_x4_x21
  BL push_neon

  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR

  MOV X2, #0xd6db
  CMP X0, X2
  BEQ .

  LDR X3, =deliver_signals
  BLR X3

  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21

  BL pop_neon
  BL pop_x4_x21
  LDP X29, X30, [SP, #16]
  LDR X3, [SP], #32

  CBZ X0, abort_self_signal

  LDR X2, [SP, #16]
  LDP X0, X1, [SP], #32

  STR X8, [SP, #24]

  /*
    TPC, SPC
    X2, X8
    X0, X1
  */
r:
  MOV X8, #131
  SVC 0
send_self_signal:
  LDP X2, X8, [SP, #16]
  LDR X0, [SP], #32
  BR X0
abort_self_signal:
  ADD SP, SP, #32
  LDR X2, [SP, #16]
  LDR X0, [SP], #32
  BR X0

.global th_is_pending_ptr
th_is_pending_ptr: .quad 0

# place the literal pool before the end_of_dispatcher_s symbol
.ltorg

.global end_of_dispatcher_s
end_of_dispatcher_s:


```

`arch/aarch64/dispatcher_aarch64.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2015-2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>

#include "../../dbm.h"
#include "../../scanner_common.h"

#include "../../pie/pie-a64-encoder.h"

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

void insert_cond_exit_branch(dbm_code_cache_meta *bb_meta, void **o_write_p, int cond) {
  void *write_p = *o_write_p;
  switch(bb_meta->exit_branch_type) {
    case uncond_imm_a64:
      return;
    case cond_imm_a64:
      a64_b_cond_helper(write_p, (uint64_t)write_p + 8, cond);
      break;
    case cbz_a64:
      a64_cbz_cbnz_helper(write_p, cond, (uint64_t)write_p + 8,
                          bb_meta->rn >> 5, bb_meta->rn & 0x1F);
      break;
    case tbz_a64:
      a64_tbz_tbnz_helper(write_p, cond, (uint64_t)write_p + 8,
                          bb_meta->rn & 0x1F, bb_meta->rn >> 5);
      break;
    default:
      fprintf(stderr, "insert_cond_exit_branch(): unknown branch type\n");
      while(1);
  }

  write_p += 4;
  *o_write_p = write_p;
}

void dispatcher_aarch64(dbm_thread *thread_data, uint32_t source_index, branch_type exit_type,
                        uintptr_t target, uintptr_t block_address) {
  uint32_t *branch_addr;
  bool is_taken;
  uintptr_t other_target;
  bool other_target_in_cache;
  mambo_cond cond;

  switch (exit_type) {
  #ifdef DBM_LINK_UNCOND_IMM
    case uncond_imm_a64:
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
      a64_cc_branch(thread_data, branch_addr, block_address + 4);
      __clear_cache((void *)branch_addr, (void *)branch_addr + 4 + 1);
      thread_data->code_cache_meta[source_index].branch_cache_status = BRANCH_LINKED;
      break;
  #endif
  #ifdef DBM_LINK_COND_IMM
    case cond_imm_a64:
  #endif
  #ifdef DBM_LINK_CBZ
    case cbz_a64:
  #endif
  #ifdef DBM_LINK_TBZ
    case tbz_a64:
  #endif
  #if defined(DBM_LINK_COND_IMM) || defined(DBM_LINK_CBZ) || defined(DBM_LINK_TBZ)
      branch_addr = thread_data->code_cache_meta[source_index].exit_branch_addr;
      is_taken = target == thread_data->code_cache_meta[source_index].branch_taken_addr;

      if (thread_data->code_cache_meta[source_index].branch_cache_status == 0) {
        if (is_taken) {
          other_target = thread_data->code_cache_meta[source_index].branch_skipped_addr;
        } else {
          other_target = thread_data->code_cache_meta[source_index].branch_taken_addr;
        }
        other_target = cc_lookup(thread_data, other_target);
        other_target_in_cache = (other_target != UINT_MAX);

        cond = thread_data->code_cache_meta[source_index].branch_condition;
        if (is_taken) {
          cond = invert_cond(cond);
        }
        insert_cond_exit_branch(&thread_data->code_cache_meta[source_index], (void **)&branch_addr, cond);

        thread_data->code_cache_meta[source_index].branch_cache_status =
                      (is_taken ? BRANCH_LINKED : FALLTHROUGH_LINKED);
      } else {
        branch_addr += 2;
        other_target_in_cache = false;
        thread_data->code_cache_meta[source_index].branch_cache_status |= BOTH_LINKED;
      }

      a64_cc_branch(thread_data, branch_addr, block_address + 4);
      branch_addr++;

      if (other_target_in_cache) {
        a64_cc_branch(thread_data, branch_addr, other_target + 4);
        branch_addr++;
        thread_data->code_cache_meta[source_index].branch_cache_status |= BOTH_LINKED;
      }

      __clear_cache((void *)thread_data->code_cache_meta[source_index].exit_branch_addr,
                    (void *)branch_addr);
      break;
  #endif
  }
}

```

`arch/aarch64/scanner_a64.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2015-2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2016-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef __aarch64__

#include <assert.h>
#include <stdio.h>

#include "../../dbm.h"
#include "../../scanner_common.h"

#include "../../pie/pie-a64-decoder.h"
#include "../../pie/pie-a64-encoder.h"
#include "../../pie/pie-a64-field-decoder.h"

#include "../../api/helpers.h"

#define NOP_INSTRUCTION 0xD503201F
#define MIN_FSPACE      60

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

#define a64_copy() *(write_p++) = *read_address;

void a64_branch_helper(uint32_t *write_p, uint64_t target, bool link) {
  int64_t difference = target - (uint64_t)write_p;
  assert(((difference & 3) == 0)
         && (difference < 128*1024*1024 && difference >= -128*1024*1024));

  a64_B_BL(&write_p, link ? 1 : 0, difference >> 2);
}

void a64_b_helper(uint32_t *write_p, uint64_t target) {
  a64_branch_helper(write_p, target, false);
}

void a64_cc_branch(dbm_thread *thread_data, uint32_t *write_p, uint64_t target) {
  a64_b_helper(write_p, target);

  record_cc_link(thread_data, (uintptr_t)write_p, target);
}

void a64_bl_helper(uint32_t *write_p, uint64_t target) {
  a64_branch_helper(write_p, target, true);
}

void a64_b_cond_helper(uint32_t *write_p, uint64_t target, mambo_cond cond) {
  int64_t difference = target - (uint64_t)write_p;
  assert(((difference & 3) == 0)
         && (difference < 1024*1024 && difference >= - 1024*1024));

  a64_B_cond(&write_p, difference >> 2, cond);
}

int a64_cbz_cbnz_helper(uint32_t *write_p, bool cbnz, uint64_t target, uint32_t sf, uint32_t rt) {
  int64_t difference = target - (uint64_t)write_p;
  if (((difference & 3) != 0) ||
      (difference >= 1024*1024 && difference < - 1024*1024)) {
    return -1;
  }

  a64_CBZ_CBNZ(&write_p, sf, cbnz ? 1 : 0, difference >> 2, rt);
  return 0;
}

void a64_cbz_helper(uint32_t *write_p, uint64_t target, uint32_t sf, uint32_t rt) {
  int ret = a64_cbz_cbnz_helper(write_p, false, target, sf, rt);
  assert(ret == 0);
}

void a64_cbnz_helper(uint32_t *write_p, uint64_t target, uint32_t sf, uint32_t rt) {
  int ret = a64_cbz_cbnz_helper(write_p, true, target, sf, rt);
  assert(ret == 0);
}

void a64_tbz_tbnz_helper(uint32_t *write_p, bool is_tbnz,
                         uint64_t target, enum reg reg, uint32_t bit) {
  int64_t difference = target - (uint64_t)write_p;
  assert(((difference & 3) == 0)
         && (difference < 32*1024 && difference >= - 32*1024));

  a64_TBZ_TBNZ(&write_p, bit >> 5, is_tbnz ? 1 : 0, bit & 0x1F, difference >> 2, reg);
}

void a64_tbz_helper(uint32_t *write_p, uint64_t target, enum reg reg, uint32_t bit) {
  a64_tbz_tbnz_helper(write_p, false, target, reg, bit);
}

void a64_tbnz_helper(uint32_t *write_p, uint64_t target, enum reg reg, uint32_t bit) {
  a64_tbz_tbnz_helper(write_p, true, target, reg, bit);
}

/*
 * Copy a value up to 64 bits to a register.
 */
void a64_copy_to_reg_64bits(uint32_t **write_p, enum reg reg, uint64_t value)
{
  uint32_t first_half_word = value & 0xFFFF;
  uint32_t second_half_word = (value >> 16) & 0xFFFF;
  uint32_t third_half_word = (value >> 32) & 0xFFFF;
  uint32_t fourth_half_word = (value >> 48) & 0xFFFF;

  // MOVZ
  a64_MOV_wide(write_p, 1, 2, 0, first_half_word, reg);
  (*write_p)++;

  if (second_half_word > 0) { // MOVK
    a64_MOV_wide(write_p, 1, 3, 1, second_half_word, reg);
    (*write_p)++;
  }

  if (third_half_word > 0) { // MOVK
    a64_MOV_wide(write_p, 1, 3, 2, third_half_word, reg);
    (*write_p)++;
  }

  if (fourth_half_word > 0) { // MOVK
    a64_MOV_wide(write_p, 1, 3, 3, fourth_half_word, reg);
    (*write_p)++;
  }
}

void a64_branch_save_context (uint32_t **o_write_p)
{
  uint32_t *write_p = *o_write_p;
  a64_push_pair_reg(x0, x1);
  *o_write_p = write_p;
}

void a64_branch_jump(dbm_thread *thread_data, uint32_t **o_write_p,
                     int basic_block, uint64_t target, uint32_t flags) {
  /*
   *                   +------------------------------+
   *                   |          STP                 |
   *                   |          MOV                 |
   *                   |          MOV                 |
   *                   |          B       DISPATCHER  |
   *                   +------------------------------+
   */
  uint32_t *write_p = *o_write_p;

  debug("A64 branch target: 0x%lx\n", target);

  if (flags & REPLACE_TARGET) {
    a64_copy_to_reg_64bits(&write_p, x0, target);
  }

  if (flags & INSERT_BRANCH) {
    a64_copy_to_reg_64bits(&write_p, x1, basic_block);
    a64_b_helper(write_p, thread_data->dispatcher_addr);
    write_p++;
  }
  *o_write_p = write_p;
}

void a64_branch_jump_cond(dbm_thread *thread_data, uint32_t **o_write_p, int basic_block,
                          uint64_t target, uint32_t *read_address, uint32_t cond) {
   /*
   *                   +-------------------------------+
   * branch_cond    -> |          NOP                  |
   * branch_1       -> |          NOP                  |
   *                   |                               |
   * branch_2       -> |          STP                  |
   *                   |          MOV       X1, BB_ID  |
   *                   |                               |
   *                   |          B.op_cond SKIPPED    |
   *                   |                               |
   *                   |          MOV       X0, TARGET |
   *                   |          B         DISPATCHER |
   *                   |                               |
   *                   | SKIPPED: MOV       X0, READ+4 |
   *                   |          B         DISPATCHER |
   *                   +-------------------------------+
   */
  uint32_t *write_p = *o_write_p;
  uint32_t *cond_branch;

  debug("A64 branch: read_addr: %p, target: 0x%lx\n", read_address, target);

  *write_p = NOP_INSTRUCTION;
  write_p++;
  *write_p = NOP_INSTRUCTION;
  write_p++;

  a64_branch_save_context(&write_p);
  a64_copy_to_reg_64bits(&write_p, x1, basic_block);

  cond_branch = write_p++;

  a64_copy_to_reg_64bits(&write_p, x0, target);
  a64_b_helper(write_p, thread_data->dispatcher_addr);
  write_p++;

  a64_b_cond_helper(cond_branch, (uint64_t)write_p, invert_cond(cond));

  a64_copy_to_reg_64bits(&write_p, x0, (uint64_t)read_address + 4);
  a64_b_helper(write_p, thread_data->dispatcher_addr);
  write_p++;

  *o_write_p = write_p;
}

void a64_branch_imm_reg(dbm_thread *thread_data, uint32_t **o_write_p,
                        int basic_block, a64_instruction inst, uint32_t *read_address) {
  /*
   *                   +------------------------------+
   * cb(n)z_branch     |          NOP                 |
   * b taken/not taken |          NOP                 |
   *                   |                              |
   * b not taken/taken |          STP                 |
   *                   |                              |
   *                   | TAKEN:   [C/T](N)BZ SKIPPED  |
   *                   |                              |
   *                   |          MOV                 |
   *                   |          MOV                 |
   *                   |          B        DISPATCHER |
   *                   |                              |
   *                   | SKIPPED: MOV                 |
   *                   |          MOV                 |
   *                   |          B        DISPATCHER |
   *                   +------------------------------+
   */
  uint32_t *write_p = *o_write_p;
  uint32_t *cbz_branch;
  uint32_t sf, op, b5, b40, imm, rt, bit;
  uint64_t branch_offset, target;

  debug("A64 [c/t](n)bz: read_addr: %p, target: 0x%lx\n", read_address, target);

  switch(inst) {
    case A64_CBZ_CBNZ:
      a64_CBZ_CBNZ_decode_fields(read_address, &sf, &op, &imm, &rt);
      branch_offset = sign_extend64(19, imm) << 2;
#ifdef DBM_LINK_CBZ
      thread_data->code_cache_meta[basic_block].exit_branch_type = cbz_a64;
      thread_data->code_cache_meta[basic_block].branch_condition = op;
      thread_data->code_cache_meta[basic_block].rn = (sf << 5) | rt;
#endif
      break;
    case A64_TBZ_TBNZ:
      a64_TBZ_TBNZ_decode_fields(read_address, &b5, &op, &b40, &imm, &rt);
      branch_offset = sign_extend64(14, imm) << 2;
      bit = (b5 << 5) | b40;
#ifdef DBM_LINK_TBZ
      thread_data->code_cache_meta[basic_block].exit_branch_type = tbz_a64;
      thread_data->code_cache_meta[basic_block].branch_condition = op;
      thread_data->code_cache_meta[basic_block].rn = (bit << 5) | rt ;
#endif
      break;
  }
  target = (uint64_t)read_address + branch_offset;

  thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
  thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
  thread_data->code_cache_meta[basic_block].branch_skipped_addr = (uint64_t)read_address + 4;

  *write_p = NOP_INSTRUCTION;
  write_p++;
  *write_p = NOP_INSTRUCTION;
  write_p++;

  a64_branch_save_context(&write_p);

  cbz_branch = write_p++;

  // TAKEN
  a64_branch_jump(thread_data, &write_p, basic_block, target,
                  REPLACE_TARGET | INSERT_BRANCH);

  switch(inst) {
    case A64_CBZ_CBNZ:
      // Compare and Branch on [Not] Zero to SKIPPED
      a64_cbz_cbnz_helper(cbz_branch, op^1, (uint64_t)write_p, sf, rt);
      break;
    case A64_TBZ_TBNZ:
      // Test bit and Branch on [Not] Zero to SKIPPED
      a64_tbz_tbnz_helper(cbz_branch, op^1, (uint64_t)write_p, rt, bit);
      break;
  }

  // SKIPPED
  a64_branch_jump(thread_data, &write_p, basic_block, (uint64_t)read_address + 4,
                  REPLACE_TARGET | INSERT_BRANCH);

  *o_write_p = write_p;
}

void a64_check_free_space(dbm_thread *thread_data, uint32_t **write_p,
                          uint32_t **data_p, uint32_t size, int cur_block) {
  int basic_block;

  if ((((uint64_t)*write_p) + size) >= (uint64_t)*data_p) {
    basic_block = allocate_bb(thread_data);
    thread_data->code_cache_meta[basic_block].actual_id = cur_block;
    if ((uint32_t *)&thread_data->code_cache->blocks[basic_block] != *data_p) {
      a64_b_helper(*write_p, (uint64_t)&thread_data->code_cache->blocks[basic_block]);
      *write_p = (uint32_t *)&thread_data->code_cache->blocks[basic_block];
    }
    *data_p = (uint32_t *)&thread_data->code_cache->blocks[basic_block];
    *data_p += BASIC_BLOCK_SIZE;
  }
}

void pass1_a64(uint32_t *read_address, branch_type *bb_type) {

  *bb_type = unknown;

  while(*bb_type == unknown) {
    a64_instruction instruction = a64_decode(read_address);

    switch(instruction) {
      case A64_B_BL:
        *bb_type = uncond_imm_a64;
        break;
      case A64_CBZ_CBNZ:
        *bb_type = cbz_a64;
        break;
      case A64_B_COND:
        *bb_type = cond_imm_a64;
        break;
      case A64_TBZ_TBNZ:
        *bb_type = tbz_a64;
        break;
      case A64_BR:
      case A64_BLR:
      case A64_RET:
        *bb_type = uncond_branch_reg;
        break;
      case A64_INVALID:
        return;
    }
    read_address++;
  }
}

bool a64_scanner_deliver_callbacks(dbm_thread *thread_data, mambo_cb_idx cb_id, uint32_t **o_read_address,
                                   a64_instruction inst, uint32_t **o_write_p, uint32_t **o_data_p,
                                   int basic_block, cc_type type, bool allow_write, bool *stop) {
  bool replaced = false;
#ifdef PLUGINS_NEW
  if (global_data.free_plugin > 0) {
    uint32_t *write_p = *o_write_p;
    uint32_t *data_p = *o_data_p;
    uint32_t *read_address = *o_read_address;

    mambo_cond cond = AL;
    if (inst == A64_B_COND) {
      uint32_t tmp;
      a64_B_cond_decode_fields(read_address, &tmp, &cond);
    }

    mambo_context ctx;
    set_mambo_context_code(&ctx, thread_data, cb_id, type, basic_block, A64_INST, inst, cond, read_address, write_p, data_p, stop);

    for (int i = 0; i < global_data.free_plugin; i++) {
      if (global_data.plugins[i].cbs[cb_id] != NULL) {
        ctx.code.write_p = write_p;
        ctx.code.data_p = data_p;
        ctx.plugin_id = i;
        ctx.code.replace = false;
        ctx.code.available_regs = ctx.code.pushed_regs;
        global_data.plugins[i].cbs[cb_id](&ctx);
        if (allow_write) {
          if (replaced && (write_p != ctx.code.write_p || ctx.code.replace)) {
            fprintf(stderr, "MAMBO API WARNING: plugin %d added code for overridden"
                            "instruction (%p).\n", i, read_address);
          }
          if (ctx.code.replace) {
            if (cb_id == PRE_INST_C) {
              replaced = true;
            } else {
              fprintf(stderr, "MAMBO API WARNING: plugin %d set replace_inst for "
                              "a disallowed event (at %p).\n", i, read_address);
            }
          }
          assert(count_bits(ctx.code.pushed_regs) == ctx.code.plugin_pushed_reg_count);
          if (allow_write && ctx.code.pushed_regs) {
            emit_pop(&ctx, ctx.code.pushed_regs);
          }
          write_p = ctx.code.write_p;
          data_p = ctx.code.data_p;
          a64_check_free_space(thread_data, &write_p, &data_p, MIN_FSPACE, basic_block);
        } else {
          assert(ctx.code.write_p == write_p);
          assert(ctx.code.data_p == data_p);
        }
      }
    }

    if (cb_id == PRE_BB_C) {
      watched_functions_t *wf = &global_data.watched_functions;
      for (int i = 0; i < wf->funcp_count; i++) {
        if (read_address == wf->funcps[i].addr) {
          _function_callback_wrapper(&ctx, wf->funcps[i].func);
          if (ctx.code.replace) {
            read_address = ctx.code.read_address;
          }
          write_p = ctx.code.write_p;
          data_p = ctx.code.data_p;
          a64_check_free_space(thread_data, &write_p, &data_p, MIN_FSPACE, basic_block);
        }
      }
    }

    *o_write_p = write_p;
    *o_data_p = data_p;
    *o_read_address = read_address;
  }
#endif
  return replaced;
}

void a64_inline_hash_lookup(dbm_thread *thread_data, int basic_block, uint32_t **o_write_p,
                            uint32_t *read_address, enum reg rn, bool link, bool set_meta) {
  /*
   * Indirect Branch LookUp
   * ======== ====== ======
   *
   *                 STP  X0, X1, [SP, #-16]!
   *                 STP  X2, [SP, #-16]!        **
   *                 MOV  X1, rn                 ** rn = X1
   *                 MOV  LR, read_address + 4   ##
   *                 MOV  X0, #hash_table
   *                 AND  Xtmp, rn, #(hash_mask << 2)
   *                 ADD  X0, X0, Xtmp, LSL #2
   *          loop:
   *                 LDR  Xtmp, [X0], #16
   *                 CBZ  Xtmp, not_found
   *                 SUB  Xtmp, Xtmp, rn
   *                 CBNZ Xtmp, loop
   *                 LDR  X0, [X0,  #-8]
   *                 LDR  X2, [SP], #16           **
   *                 BR   X0
   *     not_found:
   *                 MOV  X0, rn
   *                 MOV  X1, #bb
   *                 LDR  X2, [SP], #16           **
   *                 B    dispatcher
   *
   * ** if rn is X0, X1 or (BLR LR)
   * ## for BLR
   */

  uint32_t *write_p = *o_write_p;
  uint32_t *loop;
  uint32_t *branch_to_not_found;
  uint32_t reg_spc, reg_tmp;
  bool use_x2 = false;

  if ((rn == x0) || (rn == x1) || (link && rn == lr)) {
    reg_spc = x1;
    reg_tmp = x2;
    use_x2 = true;
  } else {
    reg_spc = rn;
    reg_tmp = x1;
  }

  if (set_meta) {
    thread_data->code_cache_meta[basic_block].rn = reg_spc;
  }

  a64_push_pair_reg(x0, x1);

  if (use_x2) {
    a64_push_reg(x2);
    if (rn != reg_spc) {
      a64_logical_reg(&write_p, 1, 1, 0, 0, rn, 0, xzr, reg_spc);
      write_p++;
    }
  }

  if (link) {
    // MOV LR, read_address+4
    a64_copy_to_reg_64bits(&write_p, lr, (uint64_t)read_address + 4);
  }

  a64_copy_to_reg_64bits(&write_p, x0,
                         (uint64_t)&thread_data->entry_address.entries);

  a64_logical_immed(&write_p, 1, 0, 1, 62, 18, reg_spc, reg_tmp);
  write_p++;

  a64_ADD_SUB_shift_reg(&write_p, 1, 0, 0, 0, reg_tmp, 0x2, x0, x0);
  write_p++;

  loop = write_p;
  a64_LDR_STR_immed(&write_p, 3, 0, 1, 16, 1, x0, reg_tmp);
  write_p++;

  branch_to_not_found = write_p++;

  a64_ADD_SUB_shift_reg(&write_p, 1, 1, 0, 0, reg_spc, 0, reg_tmp, reg_tmp);
  write_p++;

  a64_cbnz_helper(write_p, (uint64_t)loop, 1, reg_tmp);
  write_p++;

  a64_LDR_STR_immed(&write_p, 3, 0, 1, -8, 0, x0, x0);
  write_p++;

  if (use_x2) {
    a64_pop_reg(x2);
  }

  a64_BR(&write_p, x0);
  write_p++;

  a64_cbz_helper(branch_to_not_found, (uint64_t)write_p, 1, reg_tmp);

  a64_logical_reg(&write_p, 1, 1, 0, 0, reg_spc, 0, xzr, x0);
  write_p++;

  a64_copy_to_reg_64bits(&write_p, x1, basic_block);

  if (use_x2) {
    a64_pop_reg(x2);
  }

  a64_b_helper(write_p, (uint64_t)thread_data->dispatcher_addr);
  write_p++;

  *o_write_p = write_p;
}

size_t scan_a64(dbm_thread *thread_data, uint32_t *read_address,
                int basic_block, cc_type type, uint32_t *write_p) {
  bool stop = false;

  uint32_t *start_scan = read_address, *bb_entry = read_address;
  uint32_t *data_p;
  uint32_t *start_address;
  enum reg spilled_reg;

  uint64_t imm;
  uint32_t immlo, immhi, imm19, imm26;
  uint32_t CRn, CRm, Rd, Rn, Rt;
  uint32_t cond, o0, op, op1, op2, opc, R, V;

  uint64_t branch_offset;
  uint64_t PC_relative_address;
  uint64_t size;
  uint64_t target;

  bool TPIDR_EL0;

  if (write_p == NULL) {
    write_p = (uint32_t *) &thread_data->code_cache->blocks[basic_block];
  }

  start_address = write_p;

  if (type == mambo_bb) {
    data_p = write_p + BASIC_BLOCK_SIZE;
  } else { // mambo_trace
    data_p = (uint32_t *)&thread_data->code_cache->traces + (TRACE_CACHE_SIZE / 4);
    thread_data->code_cache_meta[basic_block].free_b = 0;
  }

  /*
   * On context switches registers X0 and X1 are used to store the target
   * address and the Basic Block number respectively. Before
   * overwriting the values of these two registers they are pushed to the
   * Stack. This means that at the start of every Basic Block X0 and X1 have
   * to be popped from the Stack. The same is true for trace entries, however
   * trace fragments do not need a pop instruction.
   */
  if (type != mambo_trace) {
    a64_pop_pair_reg(x0, x1);
  }

#ifdef DBM_TRACES
  branch_type bb_type;
  pass1_a64(read_address, &bb_type);

  if (type == mambo_bb && bb_type != uncond_branch_reg && bb_type != unknown) {
    a64_push_pair_reg(x1, x30);

    a64_copy_to_reg_64bits(&write_p, x1, (int)basic_block);

    a64_bl_helper(write_p, thread_data->trace_head_incr_addr);
    write_p++;

    a64_pop_pair_reg(x1, x30);
  }
#endif

  a64_scanner_deliver_callbacks(thread_data, PRE_FRAGMENT_C, &read_address, -1,
                                &write_p, &data_p, basic_block, type, true, &stop);

  a64_scanner_deliver_callbacks(thread_data, PRE_BB_C, &read_address, -1,
                                &write_p, &data_p, basic_block, type, true, &stop);

  while(!stop) {
    debug("A64 scan read_address: %p, w: : %p, bb: %d\n", read_address, write_p, basic_block);
    a64_instruction inst = a64_decode(read_address);
    debug("  instruction enum: %d\n", (inst == A64_INVALID) ? -1 : inst);
    debug("  instruction word: 0x%x\n", *read_address);

#ifdef PLUGINS_NEW
    bool skip_inst = a64_scanner_deliver_callbacks(thread_data, PRE_INST_C, &read_address, inst,
                                                   &write_p, &data_p, basic_block, type, true, &stop);
    if (!skip_inst) {
#endif

    switch (inst){
      case A64_CBZ_CBNZ:
        a64_branch_imm_reg(thread_data, &write_p, basic_block, inst, read_address);
        stop = true;
        break;

      case A64_B_COND:
        a64_B_cond_decode_fields(read_address, &imm19, &cond);

        branch_offset = sign_extend64(19, imm19) << 2;
        target = (uint64_t)read_address + branch_offset;

#ifdef DBM_LINK_COND_IMM
        // Mark this as the beggining of code emulating B.cond
        thread_data->code_cache_meta[basic_block].exit_branch_type = cond_imm_a64;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
        thread_data->code_cache_meta[basic_block].branch_skipped_addr = (uint64_t)read_address + 4;
        thread_data->code_cache_meta[basic_block].branch_condition = cond;
        thread_data->code_cache_meta[basic_block].branch_cache_status = 0;
#endif
        a64_branch_jump_cond(thread_data, &write_p, basic_block, target, read_address, cond);
        stop = true;
        break;

      case A64_SVC:
        a64_push_pair_reg(x29, x30);
        a64_copy_to_reg_64bits(&write_p, x29, (uint64_t)read_address + 4);
        a64_bl_helper(write_p, thread_data->syscall_wrapper_addr);
        write_p++;
        a64_pop_pair_reg(x0, x1);

        a64_scanner_deliver_callbacks(thread_data, POST_BB_C, &bb_entry, -1,
                                &write_p, &data_p, basic_block, type, false, &stop);
        // set the correct address for the PRE_BB_C event
        read_address++;
        bb_entry = read_address;
        a64_scanner_deliver_callbacks(thread_data, PRE_BB_C, &read_address, -1,
                                &write_p, &data_p, basic_block, type, true, &stop);
        read_address--;
        break;

      case A64_MRS_MSR_REG:
        /*
         * The R variable defines if the instruction is MSR (R = 0) or
         * MRS (R = 1)
         * Page 617 MRS and 620 MSR
         *

         * MRS
         * Move System Register allows the PE to read an AArch64 System
         * register into a general-purpose register.
         *
         * MSR (immediate)
         * Move immediate value to Special Register moves an immediate
         * value to selected bits of the PSTATE, namely D, A, I, F, and
         * SP. For more information, see PSTATE.
         *
         * MSR (register)
         * Move general-purpose register to System Register allows the PE
         * to write an AArch64 System register from a general-purpose
         * register.
         *
         * TPIDR_EL0 (page 2032)
         *      op0   op1    CRn    CRm    op2
         *       11   011   1101   0000    010
         */
        a64_MRS_MSR_reg_decode_fields(read_address, &R, &o0, &op1, &CRn, &CRm, &op2, &Rt);

        TPIDR_EL0 = (o0 == 1) && (op1 == 3) && (CRn == 13) && (CRm == 0) && (op2 == 2);
        if (TPIDR_EL0) {
          if (Rt == x0) {
            spilled_reg = x1;
          } else {
            spilled_reg = x0;
          }

          a64_push_reg(spilled_reg);
          a64_copy_to_reg_64bits(&write_p, spilled_reg, (uint64_t)&thread_data->tls);

          if (R == 0) { // MSR
            a64_LDR_STR_immed(&write_p, 3, 0, 0, 0, 0, spilled_reg, Rt);
            write_p++;
          } else { // MRS
            a64_LDR_STR_immed(&write_p, 3, 0, 1, 0, 0, spilled_reg, Rt);
            write_p++;
          }

          a64_pop_reg(spilled_reg);
          break;
        } else {
          a64_copy();
        }
        break;

      case A64_TBZ_TBNZ:
        a64_branch_imm_reg(thread_data, &write_p, basic_block, inst, read_address);
        stop = true;
        break;

      case A64_B_BL:
        a64_B_BL_decode_fields(read_address, &op, &imm26);

        if (op == 1) { // Branch Link
          a64_copy_to_reg_64bits(&write_p, lr, (uint64_t)read_address + 4);
        }

        branch_offset = sign_extend64(26, imm26) << 2;
        target = (uint64_t)read_address + branch_offset;

#ifdef DBM_LINK_UNCOND_IMM
        thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_imm_a64;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        thread_data->code_cache_meta[basic_block].branch_taken_addr = target;
        *write_p = NOP_INSTRUCTION; // Reserves space for linking branch.
        write_p++;
#endif
        a64_branch_save_context(&write_p);
        a64_branch_jump(thread_data, &write_p, basic_block, target,
                        REPLACE_TARGET | INSERT_BRANCH);
        stop = true;
        break;

      case A64_BR:
      case A64_BLR:
      case A64_RET:
        a64_BR_decode_fields(read_address, &Rn);

#ifdef DBM_INLINE_HASH
        a64_check_free_space(thread_data, &write_p, &data_p, 88, basic_block);
#endif

        thread_data->code_cache_meta[basic_block].exit_branch_type = uncond_branch_reg;
        thread_data->code_cache_meta[basic_block].exit_branch_addr = write_p;
        thread_data->code_cache_meta[basic_block].rn = Rn;

#ifndef DBM_INLINE_HASH
        a64_branch_save_context(&write_p);

        // MOV X0, Rn (Alias of ORR X0, Rn, XZR)
        a64_logical_reg(&write_p, 1, 1, 0, 0, Rn, 0, xzr, x0);
        write_p++;

        if (inst == A64_BLR) {
          // MOV LR, read_address+4
          a64_copy_to_reg_64bits(&write_p, lr, (uint64_t)read_address + 4);
        }

        a64_branch_jump(thread_data, &write_p, basic_block, 0, INSERT_BRANCH);
#else
        a64_inline_hash_lookup(thread_data, basic_block, &write_p, read_address, Rn, (inst == A64_BLR), true);
#endif
        stop = true;
        break;

      case A64_LDR_LIT:
        /*
         * LDR (literal) calculates an address from the PC value and an
         * immediate offset, loads a word from memory, and writes it to a
         * register.
         *
         *                         LOAD LITERAL
         * ----------------------------------------------------------------
         * opc  V  Instruction                Variant
         * ----------------------------------------------------------------
         *  00  0  LDR   (literal)            32-bit variant on page C6-527
         *  01  0  LDR   (literal)            64-bit variant on page C6-527
         *  10  0  LDRSW (literal)            -
         *  11  0  PRFM  (literal)            -
         *
         *  00  1  LDR   (literal, SIMD&FP)   32-bit variant on page C7-1027
         *  01  1  LDR   (literal, SIMD&FP)   64-bit variant on page C7-1027
         *  10  1  LDR   (literal, SIMD&FP)  128-bit variant on page C7-1027
         */
        a64_LDR_lit_decode_fields(read_address, &opc, &V, &imm19, &Rt);

        uint64_t offset = sign_extend64(19, imm19) << 2;
        PC_relative_address = (uint64_t)read_address + offset;

        if (V== 0) {
          switch(opc) {
            case 0: // LDR literal 32-bit variant
              a64_copy_to_reg_64bits(&write_p, Rt, PC_relative_address);
              a64_LDR_STR_unsigned_immed(&write_p, 2, V, 1, 0, Rt, Rt);
              write_p++;
              break;
            case 1: // LDR literal 64-bit variant
              a64_copy_to_reg_64bits(&write_p, Rt, PC_relative_address);
              a64_LDR_STR_unsigned_immed(&write_p, 3, V, 1, 0, Rt, Rt);
              write_p++;
              break;
            case 2: // LDR Signed Word (literal)
              a64_copy_to_reg_64bits(&write_p, Rt, PC_relative_address);
              a64_LDR_STR_unsigned_immed(&write_p, 2, V, 2, 0, Rt, Rt);
              write_p++;
              break;
            case 3: // PRFM Prefetch
              a64_push_reg(x0);
              a64_copy_to_reg_64bits(&write_p, x0, PC_relative_address);
              a64_LDR_STR_unsigned_immed(&write_p, 3, V, 2, 0, x0, Rt);
              write_p++;
              a64_pop_reg(x0);
              break;
          }
        } else if (V == 1) {
          switch(opc) {
            case 0: // LDR (literal, SIMD&FP) 32-bit variant
              size = 2;
              opc  = 1;
              break;
            case 1: // LDR (literal, SIMD&FP) 64-bit variant
              size = 3;
              opc  = 1;
              break;
            case 2: // LDR (literal, SIMD&FP) 128-bit variant
              size = 0;
              opc  = 3;
              break;
            default:
              printf("unallocated encoding\n");
              while(1);
          }
          a64_push_reg(x0);
          a64_copy_to_reg_64bits(&write_p, x0, PC_relative_address);
          a64_LDR_STR_unsigned_immed(&write_p, size, V, opc, 0, x0, Rt);
          write_p++;
          a64_pop_reg(x0);
        }
        break;

      case A64_ADR:
        /*
         * The ADR instruction needs to be translated as a MOV instruction.
         * Otherwise it will point to the wrong address (somewhere
         * in the code cache).
         */
        a64_ADR_decode_fields(read_address, &op, &immlo, &immhi, &Rd);
        imm = (immhi << 2) | immlo;

        if (op == 0){ // ADR
          imm = sign_extend64(21, imm);
          PC_relative_address = (uint64_t)read_address;
        } else { // ADRP
          imm = sign_extend64(21, imm) << 12;
          PC_relative_address = (uint64_t)read_address & ~(0xFFF);
        }

        PC_relative_address += imm;
        a64_copy_to_reg_64bits(&write_p, Rd, PC_relative_address);
        break;

      case A64_LDADD:
      case A64_LDCLR:
      case A64_LDEOR:
      case A64_LDSET:
      case A64_SWP:
      case A64_HVC:
      case A64_BRK:
      case A64_HINT:
      case A64_CLREX:
      case A64_DSB:
      case A64_DMB:
      case A64_ISB:
      case A64_SYS:
      case A64_LDX_STX:
      case A64_LDP_STP:
      case A64_LDR_STR_IMMED:
      case A64_LDR_STR_REG:
      case A64_LDR_STR_UNSIGNED_IMMED:
      case A64_LDX_STX_MULTIPLE:
      case A64_LDX_STX_MULTIPLE_POST:
      case A64_LDX_STX_SINGLE:
      case A64_LDX_STX_SINGLE_POST:
      case A64_ADD_SUB_IMMED:
      case A64_BFM:
      case A64_EXTR:
      case A64_LOGICAL_IMMED:
      case A64_MOV_WIDE:
      case A64_ADD_SUB_EXT_REG:
      case A64_ADD_SUB_SHIFT_REG:
      case A64_ADC_SBC:
      case A64_CCMP_CCMN_IMMED:
      case A64_CCMP_CCMN_REG:
      case A64_COND_SELECT:
      case A64_DATA_PROC_REG1:
      case A64_DATA_PROC_REG2:
      case A64_DATA_PROC_REG3:
      case A64_LOGICAL_REG:
      case A64_SIMD_ACROSS_LANE:
      case A64_SIMD_COPY:
      case A64_SIMD_EXTRACT:
      case A64_SIMD_MODIFIED_IMMED:
      case A64_SIMD_PERMUTE:
      case A64_SIMD_SCALAR_COPY:
      case A64_SIMD_SCALAR_PAIRWISE:
      case A64_SIMD_SCALAR_SHIFT_IMMED:
      case A64_SIMD_SCALAR_THREE_DIFF:
      case A64_SIMD_SCALAR_THREE_SAME:
      case A64_SIMD_SHIFT_IMMED:
      case A64_SIMD_TABLE_LOOKUP:
      case A64_SIMD_THREE_DIFF:
      case A64_SIMD_THREE_SAME:
      case A64_SIMD_SCALAR_TWO_REG:
      case A64_SIMD_SCALAR_X_INDEXED:
      case A64_SIMD_TWO_REG:
      case A64_SIMD_X_INDEXED:
      case A64_CRYPTO_AES:
      case A64_CRYPTO_SHA_REG3:
      case A64_CRYPTO_SHA_REG2:
      case A64_FCMP:
      case A64_FCCMP:
      case A64_FCSEL:
      case A64_FLOAT_REG1:
      case A64_FLOAT_REG2:
      case A64_FLOAT_REG3:
      case A64_FMOV_IMMED:
      case A64_FLOAT_CVT_FIXED:
      case A64_FLOAT_CVT_INT:
        a64_copy();
        break;

      case A64_INVALID:
        if (read_address != start_scan) {
          // Branch to lookup_or_stub(thread_data, (uintptr_t)read_address);
          a64_branch_save_context(&write_p);
          a64_branch_jump(thread_data, &write_p, basic_block, (uint64_t)read_address,
                          REPLACE_TARGET | INSERT_BRANCH);
          stop = true;
          debug("WARN: deferred scanning because of unknown instruction at: %p\n", read_address);
        } else {
          fprintf(stderr, "\nMAMBO: Unknown A64 instruction: %d (0x%x) at %p\n"
                          "Copying it unmodified, but future problems are possible\n"
                          "Report crashes at https://github.com/beehive-lab/mambo\n\n",
                          inst, *read_address, read_address);
          a64_copy();
        }
        break;

      default:
        fprintf(stderr, "Unhandled A64 instruction: %d at %p\n", inst, read_address);
        while(1);
        exit(EXIT_FAILURE);
    }
#ifdef PLUGINS_NEW
    } // if (!skip_inst)
#endif

    if (data_p <= write_p) {
      fprintf(stderr, "%d, inst: %p, :write: %p\n", inst, data_p, write_p);
      while(1);
    }

    if (!stop) {
      a64_check_free_space(thread_data, &write_p, &data_p, MIN_FSPACE, basic_block);
    }
#ifdef PLUGINS_NEW
    a64_scanner_deliver_callbacks(thread_data, POST_INST_C, &read_address, inst, &write_p, &data_p, basic_block, type, !stop, &stop);
#endif

    read_address++;
  } // while(!stop)

  a64_scanner_deliver_callbacks(thread_data, POST_BB_C, &bb_entry, -1,
                                &write_p, &data_p, basic_block, type, false, &stop);
  a64_scanner_deliver_callbacks(thread_data, POST_FRAGMENT_C, &start_scan, -1,
                                &write_p, &data_p, basic_block, type, false, &stop);

  return ((write_p - start_address + 1) * sizeof(*write_p));
}
#endif // __aarch64__

```

`common.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdlib.h>
#include <stdio.h>
#include <inttypes.h>
#include <limits.h>
#include <stdbool.h>
#include <assert.h>
#include <pthread.h>
#include <errno.h>
#include <string.h>
#include <unistd.h>
#include <ucontext.h>

#include "dbm.h"
#include "common.h"
#include "scanner_public.h"

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

/* Hash table */

// Breaks linear probing, don't use
void hash_delete(hash_table *table, uintptr_t key) {
  assert(false);
  int index = GET_INDEX(key);
  int end = index - 1;
  bool found = false;
  uintptr_t c_key;
  
  do {
    c_key = table->entries[index].key;
    if (c_key == key) {
      table->entries[index].key = 0;
      found = true;
    } else {
      index = (index + 1) & table->size;
    }
  } while(!found && index != end && c_key != 0);
}

/* To simplify the inline hash lookup code, we avoid looping around for linear probing.
   A few slots are overprovisioned at the end of the table and the last one is reserved
   empty to mark the end of the structure. */
uintptr_t hash_lookup(hash_table *table, uintptr_t key) {
  int index = GET_INDEX(key);
  bool found = false;
  uintptr_t entry = UINT_MAX;
  uintptr_t c_key;
  
  do {
    c_key = table->entries[index].key;
    if (c_key == key) {
      entry = table->entries[index].value;
      found = true;
    } else {
      index++;
    }
  } while(!found && index < (table->size - 1) && c_key != 0);
  
  return entry;
}

bool hash_add(hash_table *table, uintptr_t key, uintptr_t value) {
  int index = GET_INDEX(key);
  bool done = false;
  
  do {
    if (table->entries[index].key == 0 || table->entries[index].key == key) {
      if (table->entries[index].key == 0) {
        table->count++;
      }
      table->entries[index].key = key;
      table->entries[index].value = value;
      done = true;
    } else {
      index++;
      if (index >= table->size -1) {
        fprintf(stderr, "Hash table index overflow\n");
        while(1);
      }
      table->collisions++;
    }
  } while(!done && index < (table->size - 1));
  
  return done;
}

void hash_init(hash_table *table, int size) {
  table->size = size;
  table->collisions = 0;
  table->count = 0;
  for (int i = size-1; i >= 0; i--) {
    table->entries[i].key = 0;
  }
}


/* Linked list */
void linked_list_init(ll *list, int size) {
  assert(size >= 1);
  list->size = size;
  list->free_list = &list->pool[0];
  
  for (int i = 0; i < size-1; i++) {
    list->pool[i].next = &list->pool[i+1];
  }
  
  list->pool[size-1].next = NULL;
}

ll_entry *linked_list_alloc(ll *list) {
  if (list->free_list == NULL) return NULL;
  
  ll_entry *entry = list->free_list;
  list->free_list = entry->next;
  entry->next = NULL;
  
  return entry;
}

/* Interval map */
/* Private interval_map functions; obtain lock before calling */
void interval_map_print(interval_map *imap) {
  fprintf(stderr, "imap %p:\n", imap);
  for (ssize_t i = 0; i < imap->entry_count; i++) {
    fprintf(stderr, "  %"PRIxPTR" - %"PRIxPTR"\n",
            imap->entries[i].start, imap->entries[i].end);
  }
}

int interval_map_delete_entry(interval_map *imap, ssize_t index) {
  if (index < 0 || index >= imap->entry_count) {
    return -1;
  }

  if (imap->entries[index].fd >= 0) {
    close(imap->entries[index].fd);
  }
  if (imap->entry_count >= 2) {
    imap->entries[index] = imap->entries[imap->entry_count - 1];
  }
  imap->entry_count--;
  return 0;
}

int interval_map_add_entry(interval_map *imap, uintptr_t start, uintptr_t end, int fd) {
  if (imap->entry_count >= imap->mem_size || start >= end) {
    return -1;
  }
  ssize_t index = imap->entry_count++;

  imap->entries[index].start = start;
  imap->entries[index].end = end;
  imap->entries[index].fd = fd;

  return 0;
}

/* Public interval_map functions */
int interval_map_init(interval_map *imap, ssize_t size) {
  assert(size > 0);
  interval_map_entry *entries = malloc(sizeof(interval_map_entry) * size);
  if (entries == NULL) return -1;

  imap->mem_size = size;
  imap->entry_count = 0;
  imap->entries = entries;
  int ret = pthread_mutex_init(&imap->mutex, NULL);
  if (ret != 0 && ret != EBUSY) {
    return -1;
  }

  return 0;
}

int interval_map_add(interval_map *imap, uintptr_t start, uintptr_t end, int fd) {
  int ret;
  ssize_t overlap_ind = -1;

  if (start >= end) return -1;

  if (fd >= 0) {
    fd = dup(fd);
    assert(fd >= 0);
  }

  ret = pthread_mutex_lock(&imap->mutex);
  if (ret != 0) return -1;

  // Check for overlapping regions
  for (ssize_t i = imap->entry_count -1; i >= 0; i--) {
    if ((start < imap->entries[i].end) && (end > imap->entries[i].start)) {
      assert(fd < 0 && imap->entries[i].fd < 0);
      if (overlap_ind == -1) {
        overlap_ind = i;
      } else {
        start = min(imap->entries[i].start, start);
        end = max(imap->entries[i].end, end);
        ret = interval_map_delete_entry(imap, i);
        assert(ret == 0);
      }
      imap->entries[overlap_ind].start = min(imap->entries[overlap_ind].start, start);
      imap->entries[overlap_ind].end = max(imap->entries[overlap_ind].end, end);
    }
  }

  // No overlapping region found
  if (overlap_ind == -1) {
    ret = interval_map_add_entry(imap, start, end, fd);
    assert(ret == 0);
  }

#ifdef DEBUG
  fprintf(stderr, "imap added: %"PRIxPTR" %"PRIxPTR"\n", start, end);
  interval_map_print(imap);
#endif

  ret = pthread_mutex_unlock(&imap->mutex);
  if (ret != 0) return -1;

  return 0;
}

ssize_t interval_map_search(interval_map *imap, uintptr_t start, uintptr_t end) {
  int ret;
  ssize_t status = 0;

  if (start >= end) return -1;

  ret = pthread_mutex_lock(&imap->mutex);
  if (ret != 0) return -1;

  for (ssize_t i = imap->entry_count - 1; i >= 0; i--) {
    if ((start < imap->entries[i].end) && (end > imap->entries[i].start)) {
      status++;
    }
  }

  ret = pthread_mutex_unlock(&imap->mutex);
  if (ret != 0) return -1;

  return status;
}

int interval_map_search_by_addr(interval_map *imap, uintptr_t addr, interval_map_entry *entry) {
  bool found = false;

  if (entry == NULL) return -1;

  int ret = pthread_mutex_lock(&imap->mutex);
  if (ret != 0) return -1;

  for (ssize_t i = imap->entry_count - 1; i >= 0 && !found; i--) {
    if ((addr >= imap->entries[i].start) && (addr < imap->entries[i].end)) {
      memcpy(entry, &imap->entries[i], sizeof(*entry));
      found = true;
    }
  }

  ret = pthread_mutex_unlock(&imap->mutex);
  assert(ret == 0);

  return found ? 1 : 0;
}

ssize_t interval_map_delete(interval_map *imap, uintptr_t start, uintptr_t end) {
  ssize_t status = 0;

  if (start >= end) return -1;

  int ret = pthread_mutex_lock(&imap->mutex);
  if (ret != 0) return -1;

  for (ssize_t i = imap->entry_count - 1; i >= 0; i--) {
    if ((start < imap->entries[i].end) && (end > imap->entries[i].start)) {
      status++;

      if (start <= imap->entries[i].start && end >= imap->entries[i].end) {
        ret = interval_map_delete_entry(imap, i);
        assert(ret == 0);
      } else if (start == imap->entries[i].start && end < imap->entries[i].end) {
        imap->entries[i].start = end;
      } else if (end == imap->entries[i].end && start > imap->entries[i].start) {
        imap->entries[i].end = start;
      } else {
        uintptr_t tmp = imap->entries[i].end;
        imap->entries[i].end = start;
        int fd = imap->entries[i].fd;
        if (fd >= 0) {
          fd = dup(fd);
          assert(fd >= 0);
        }
        ret = interval_map_add_entry(imap, end, tmp, fd);
        assert(ret == 0);
      }
    } // if hit
  } // for

#ifdef DEBUG
  if (status > 0) {
    fprintf(stderr, "imap deleted: %"PRIxPTR" %"PRIxPTR"\n", start, end);
    interval_map_print(imap);
  }
#endif

  ret = pthread_mutex_unlock(&imap->mutex);
  if (ret != 0) return -1;

  return status;
}

/* Other useful functions*/
#ifdef __arm__
  #define first_reg r0
  #define last_reg pc
#endif
#ifdef __aarch64__
  #define first_reg x0
  #define last_reg sp
#endif

uint32_t next_reg_in_list(uint32_t reglist, uint32_t start) {
  for (; start <= last_reg; start++) {
    if (reglist & (1 << start)) {
      return start;
     }
   }
   
   return reg_invalid;
}

uint32_t last_reg_in_list(uint32_t reglist, uint32_t start) {
  for (; start >= first_reg; start--) {
    if (reglist & (1 << start)) {
      return start;
     }
   }

   return reg_invalid;
}

int get_lowest_n_regs(uint32_t reglist, uint32_t *regs, int n) {
  int count = 0, prev = -1;
  if (n < 1) return count;

  for (int i = 0; i < n; i++) {
    regs[i] = next_reg_in_list(reglist, prev + 1);
    if (regs[i] < reg_invalid) {
      count++;
    }
    prev = regs[i];
  }

  return count;
}

int get_highest_n_regs(uint32_t reglist, uint32_t *regs, int n) {
  int count = 0, prev = reg_invalid;
  if (n < 1) return count;

  for (int i = 0; i < n; i++) {
    regs[i] = last_reg_in_list(reglist, prev - 1);
    if (regs[i] < reg_invalid) {
      count++;
    }
    prev = regs[i];
  }

  return count;
}

int count_bits(uint32_t n) {
  int c;
  for (c = 0; n; c++) 
    n &= n - 1;
  return c;
}

// Used to avoid calling stdlib's memcpy implementation which overwrites NEON regs
void mambo_memcpy(void *dst, void *src, size_t l) {
  char *d = (char *)dst;
  char *s = (char *)src;
  for (int i = 0; i < l; i++) {
    d[i] = s[i];
  }
}

extern int __try_memcpy(void *dst, const void *src, size_t n);
extern void __try_memcpy_error();

#ifdef __arm__
  #define pc_reg uc_mcontext.arm_pc
#elif __aarch64__
  #define pc_reg uc_mcontext.pc
#endif
void memcpy_fault(int i, siginfo_t *info, void *ctx_ptr) {
  ucontext_t *ctx = (ucontext_t *)ctx_ptr;
  ctx->pc_reg = (uintptr_t)__try_memcpy_error;
}
#undef pc_reg

int try_memcpy(void *dst, void *src, size_t n) {
  struct sigaction act, oldact;
  act.sa_sigaction = memcpy_fault;
  sigemptyset(&act.sa_mask);
  act.sa_flags = SA_SIGINFO;
  int ret = sigaction(SIGSEGV, &act, &oldact);
  assert(ret == 0);

  int status = __try_memcpy(dst, src, n);

  ret = sigaction(SIGSEGV, &oldact, NULL);
  assert(ret == 0);

  return status;
}

```

`common.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifndef __COMMON_H__
#define __COMMON_H__

#include <stdlib.h>

#define CODE_CACHE_HASH_SIZE 0x7FFFF//14071
#define CODE_CACHE_HASH_OVERP 10

/* Warning, size MUST be (a power of 2) */
#ifdef __arm__
#define GET_INDEX(key) ((key) & (table->size - CODE_CACHE_HASH_OVERP))
#endif
#ifdef __aarch64__
#define GET_INDEX(key) ((key >> 2) & (table->size - CODE_CACHE_HASH_OVERP))
#endif
typedef struct {
  uintptr_t key;
  uintptr_t value;
} hash_entry;

typedef struct {
  int size;
  int collisions;
  int count;
  hash_entry entries[CODE_CACHE_HASH_SIZE + CODE_CACHE_HASH_OVERP];
} hash_table;

struct ll_entry_s {
  struct ll_entry_s *next;
  uintptr_t data;
};
typedef struct ll_entry_s ll_entry;

typedef struct {
  ll_entry *free_list;
  int size;
  ll_entry pool[];
} ll;

typedef struct {
  uintptr_t start;
  uintptr_t end;
  int fd;
} interval_map_entry;

typedef struct {
  ssize_t mem_size;
  ssize_t entry_count;
  pthread_mutex_t mutex;
  interval_map_entry *entries;
} interval_map;

bool hash_add(hash_table *table, uintptr_t key, uintptr_t value);
void hash_delete(hash_table *table, uintptr_t key);
uintptr_t hash_lookup(hash_table *table, uintptr_t key);
void hash_init(hash_table *table, int size);

void linked_list_init(ll *list, int size);
ll_entry *linked_list_alloc(ll *list);

int interval_map_init(interval_map *imap, ssize_t size);
int interval_map_add(interval_map *imap, uintptr_t start, size_t len, int fd);
ssize_t interval_map_search(interval_map *imap, uintptr_t start, size_t len);
int interval_map_search_by_addr(interval_map *imap, uintptr_t addr, interval_map_entry *entry);
ssize_t interval_map_delete(interval_map *imap, uintptr_t start, size_t len);

uint32_t next_reg_in_list(uint32_t reglist, uint32_t start);
uint32_t last_reg_in_list(uint32_t reglist, uint32_t start);
int get_lowest_n_regs(uint32_t reglist, uint32_t *regs, int n);
int get_highest_n_regs(uint32_t reglist, uint32_t *regs, int n);
int count_bits(uint32_t n);
int try_memcpy(void *dst, void *src, size_t n);

static inline uintptr_t align_lower(uintptr_t address, uintptr_t alignment) {
  uintptr_t aligned_address = address / alignment * alignment;

  return aligned_address;
}

static inline uintptr_t align_higher(uintptr_t address, uintptr_t alignment) {
  uintptr_t aligned_address = align_lower(address, alignment);
  if (aligned_address != address) {
    aligned_address += alignment;
  }

  return aligned_address;
}

static inline bool is_offset_within_range(intptr_t const offset, intptr_t const range) {
  return ((offset <= (range - 4)) && (offset >= (- range)));
}
#endif

```

`dbm.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdlib.h>
#include <stdio.h>
#include <stdbool.h>
#include <assert.h>
#include <string.h>
#include <limits.h>
#include <asm/unistd.h>
#include <pthread.h>
#include <sys/auxv.h>
#include <sys/mman.h>
#include <unistd.h>

#include <libelf.h>

#include "dbm.h"
#include "common.h"
#include "scanner_common.h"

#include "elf/elf_loader.h"

#ifdef __arm__
#include "pie/pie-thumb-decoder.h"
#include "pie/pie-thumb-encoder.h"
#include "pie/pie-arm-encoder.h"
#endif

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
  #ifndef VERBOSE
    #define VERBOSE
  #endif
#else
  #define debug(...)
#endif

#ifdef VERBOSE
  #define info(...) fprintf(stderr, __VA_ARGS__)
#else
  #define info(...)
#endif

#define dispatcher_thread_data_offset ((uintptr_t)&disp_thread_data - (uintptr_t)&start_of_dispatcher_s)
#define th_is_pending_ptr_offset      ((uintptr_t)&th_is_pending_ptr - (uintptr_t)&start_of_dispatcher_s)
#define dispatcher_wrapper_offset     ((uintptr_t)dispatcher_trampoline - (uintptr_t)&start_of_dispatcher_s)
#define syscall_wrapper_offset        ((uintptr_t)syscall_wrapper - (uintptr_t)&start_of_dispatcher_s)
#define trace_head_incr_offset        ((uintptr_t)trace_head_incr - (uintptr_t)&start_of_dispatcher_s)

uintptr_t page_size;
dbm_global global_data;
__thread dbm_thread *current_thread;

void flush_code_cache(dbm_thread *thread_data) {
  thread_data->was_flushed = true;
  thread_data->free_block = trampolines_size_bbs;
  hash_init(&thread_data->entry_address, CODE_CACHE_HASH_SIZE + CODE_CACHE_HASH_OVERP);
#ifdef DBM_TRACES
  thread_data->trace_cache_next = thread_data->code_cache->traces;
  thread_data->trace_id = CODE_CACHE_SIZE;
  thread_data->active_trace.id = CODE_CACHE_SIZE;
#endif

  for (int i = 0; i < CODE_CACHE_SIZE; i++) {
    thread_data->code_cache_meta[i].exit_branch_type = unknown;
    thread_data->code_cache_meta[i].linked_from = NULL;
    thread_data->code_cache_meta[i].branch_cache_status = 0;
    thread_data->code_cache_meta[i].actual_id = 0;
#ifdef DBM_TRACES
    thread_data->exec_count[i] = 0;
#endif
  }

  linked_list_init(thread_data->cc_links, MAX_CC_LINKS);
}

uintptr_t cc_lookup(dbm_thread *thread_data, uintptr_t target) {
  uintptr_t addr = hash_lookup(&thread_data->entry_address, target);
  return adjust_cc_entry(addr);
}

static inline uintptr_t _lookup_or_scan(dbm_thread * const thread_data,
                                 const uintptr_t target,
                                 bool * const cached) {
  debug("Thread_data: %p\n", thread_data);

  uintptr_t block_address = cc_lookup(thread_data, target);
  int basic_block = ALLOCATE_BB;

  if (block_address != UINT_MAX) {
    /* Only basic blocks can be stubs
       use addr_to_bb_id over addr_to_fragment_id as it's a O(1) lookup vs O(log n) */
    if (!is_trace(thread_data, block_address)) {
      basic_block = addr_to_bb_id(thread_data, block_address);
      if (thread_data->code_cache_meta[basic_block].exit_branch_type == stub) {
        block_address = UINT_MAX;
      }
    }
  }

  if (cached != NULL) {
    *cached = (block_address != UINT_MAX);
  }

  if (block_address != UINT_MAX) {
    return block_address;
  }

  return scan(thread_data, (uint16_t *)target, basic_block);
}

uintptr_t lookup_or_scan(dbm_thread * const thread_data, const uintptr_t target) {
    return lookup_or_scan_with_cached(thread_data, target, NULL);
}

inline uintptr_t lookup_or_scan_with_cached(dbm_thread * const thread_data,
                                            const uintptr_t target,
                                            bool *const cached) {
    return _lookup_or_scan(thread_data, target, cached);
}

int allocate_bb(dbm_thread *thread_data) {
  // Reserve CODE_CACHE_OVERP basic blocks to be able to scan large blocks
  if(thread_data->free_block >= (CODE_CACHE_SIZE - CODE_CACHE_OVERP)) {
    fprintf(stderr, "code cache full, flushing it\n");
    flush_code_cache(thread_data);
  }

  return thread_data->free_block++;
}

/* Stub BBs only contain a call to the dispatcher
   Stub BBs are used when a basic block can be optimised by directly linking
   to a target, but it's not clear if the target will ever be reached, e.g.:
   branch-not-taken path for conditional branches, RAS prediction */
uintptr_t stub_bb(dbm_thread *thread_data, uintptr_t target) {
  unsigned int basic_block;
  uintptr_t block_address;
  uintptr_t thumb = target & THUMB;
  
  basic_block = allocate_bb(thread_data);
  block_address = (uintptr_t)&thread_data->code_cache->blocks[basic_block];
  
  debug("Stub BB: 0x%" PRIxPTR "\n", block_address + thumb);
  
  thread_data->code_cache_meta[basic_block].exit_branch_type = stub;
  if (!hash_add(&thread_data->entry_address, target, block_address + thumb)) {
    fprintf(stderr, "Failed to add hash table entry for newly created stub basic block\n");
    while(1);
  }

#ifdef __arm__
  if (thumb) {
    thumb_encode_stub_bb(thread_data, basic_block, target);
  } else {
    arm_encode_stub_bb(thread_data, basic_block, target);
  }
#endif
#ifdef __aarch64__
  assert(0); // TODO
#endif
  
  return adjust_cc_entry(block_address + thumb);
}

uintptr_t lookup_or_stub(dbm_thread *thread_data, uintptr_t target) {
  uintptr_t block_address;
  
  debug("Stub (0x%" PRIxPTR ")\n", target);
  debug("Thread_data: %p\n", thread_data);
  
  block_address = cc_lookup(thread_data, target);
  if (block_address == UINT_MAX) {
    block_address = stub_bb(thread_data, target);
    __clear_cache((char *)block_address, (char *)(block_address + BASIC_BLOCK_SIZE * 4 + 1));
  }

  return block_address;
}

uintptr_t scan(dbm_thread *thread_data, uint16_t *address, int basic_block) {
  uintptr_t thumb = (uintptr_t)address & THUMB;
  uintptr_t block_address;
  size_t block_size;
  bool stub = false;

  debug("scan(%p)\n", address);

  // Alocate a basic block
  if (basic_block == ALLOCATE_BB) {
    basic_block = allocate_bb(thread_data);
  } else {
    stub = true;
  }

  block_address = (uintptr_t)&thread_data->code_cache->blocks[basic_block];
  thread_data->code_cache_meta[basic_block].source_addr = address;
  thread_data->code_cache_meta[basic_block].tpc = block_address;

  // Add entry into the code cache hash table
  // It must be added before scan_ is called, otherwise a call for scan
  // from scan_x could result in duplicate BBS or an infinite recursive call
  block_address |= thumb;
  if (!stub) {
    if (!hash_add(&thread_data->entry_address, (uintptr_t)address, block_address)) {
      fprintf(stderr, "Failed to add hash table entry for newly created basic block\n");
      while(1);
    }
  }

  // Build a basic block
  // Scan functions return size of the generated basic block, in bytes
#ifdef __arm__
  if (thumb) {
    debug("scan address: %p\n", address);
    block_size = scan_t32(thread_data, (uint16_t *)((uint32_t)address & 0xFFFFFFFE), basic_block, mambo_bb, NULL);
  } else {
    block_size = scan_a32(thread_data, (uint32_t *)address, basic_block, mambo_bb, NULL);
  }
#endif
#ifdef __aarch64__
  block_size = scan_a64(thread_data, (uint32_t *)address, basic_block, mambo_bb, NULL);
#endif

  // Flush modified instructions from caches
  // End address is exclusive
  if (thread_data->free_block < basic_block) {
    /* The code cache has been flushed. Play it safe, because we don't know how
       much space has been used in each of the two areas. */
    __clear_cache((char *)block_address, &thread_data->code_cache->traces);
    __clear_cache(&thread_data->code_cache->blocks[trampolines_size_bbs],
                  &thread_data->code_cache->blocks[thread_data->free_block]);
  } else {
    __clear_cache((char *)block_address, (char *)(block_address + block_size + 1));
  }

  return adjust_cc_entry(block_address);
}

int lock_thread_list() {
  return pthread_mutex_lock(&global_data.thread_list_mutex);
}

int unlock_thread_list() {
  return pthread_mutex_unlock(&global_data.thread_list_mutex);
}

int register_thread(dbm_thread *thread_data, bool caller_has_lock) {
  int ret;

  if (!caller_has_lock) {
    ret = lock_thread_list();
    assert(ret == 0);
  }

  thread_data->next_thread = global_data.threads;
  global_data.threads = thread_data;

  mambo_deliver_callbacks(PRE_THREAD_C, thread_data);

  if (!caller_has_lock) {
    ret = unlock_thread_list();
    assert(ret == 0);
  }

  return 0;
}

int unregister_thread(dbm_thread *thread_data, bool caller_has_lock) {
  int ret, status = 0;

  if (!caller_has_lock) {
    ret = lock_thread_list();
    assert(ret == 0);
  }

  if (global_data.threads == thread_data) {
    global_data.threads = thread_data->next_thread;
  } else {
    dbm_thread *prev_thread = global_data.threads;
    while (prev_thread->next_thread != thread_data && prev_thread->next_thread != NULL) {
      prev_thread = prev_thread->next_thread;
    }
    if (prev_thread->next_thread == thread_data) {
      prev_thread->next_thread = thread_data->next_thread;
    } else {
      status = -1;
    }
  }

  if (status == 0) {
    mambo_deliver_callbacks(POST_THREAD_C, thread_data);
  }

  if (!caller_has_lock) {
    ret = unlock_thread_list();
    assert(ret == 0);
  }

  return status;
}

void dbm_exit(dbm_thread *thread_data, uint32_t code) {
  fprintf(stderr, "We're done; exiting with status: %d\n", code);

#ifdef PLUGINS_NEW
  lock_thread_list();
  pid_t pid = getpid();
  global_data.exit_group = 1;

  bool done;
  do {
    for (dbm_thread *thread = global_data.threads; thread != NULL; thread = thread->next_thread) {
      if (thread != thread_data && thread->status == THREAD_RUNNING) {
        syscall(__NR_tgkill, pid, thread->tid, UNLINK_SIGNAL);
      }
    }
    usleep(100);

    done = true;
    for (dbm_thread *thread = global_data.threads; thread != NULL; thread = thread->next_thread) {
      if (thread != thread_data) {
        if (thread->status == THREAD_RUNNING) {
          done = false;
        }
      }
    }
  } while (!done);

  for (dbm_thread *thread = global_data.threads; thread != NULL; thread = thread->next_thread) {
    mambo_deliver_callbacks(POST_THREAD_C, thread);
  }

  mambo_deliver_callbacks(EXIT_C, thread_data);
#endif

  exit(code);
}

void thread_abort(dbm_thread *thread_data) {
  thread_data->status = THREAD_EXIT;
  pthread_exit(NULL);
}

bool allocate_thread_data(dbm_thread **thread_data) {
  dbm_thread *data = mmap(NULL, sizeof(dbm_thread), PROT_READ | PROT_WRITE, METADATA_MMAP_OPTS, -1, 0);
  if (data != MAP_FAILED) {
    *thread_data = data;
    return true;
  }
  return false;
}

int free_thread_data(dbm_thread *thread_data) {
  if (munmap(thread_data->code_cache, CC_SZ_ROUND(sizeof(dbm_code_cache))) != 0) {
    fprintf(stderr, "Error freeing code cache on exit()\n");
    while(1);
  }
  if (munmap(thread_data->cc_links, METADATA_SZ_ROUND(sizeof(ll) + sizeof(ll_entry) * MAX_CC_LINKS)) != 0) {
    fprintf(stderr, "Error freeing CC link struct on exit()\n");
    while(1);
  }
  if (munmap(thread_data, METADATA_SZ_ROUND(sizeof(dbm_thread))) != 0) {
    fprintf(stderr, "Error freeing thread private structure on exit()\n");
    while(1);
  }
  return 0;
}

void init_thread(dbm_thread *thread_data) {
  dbm_thread **dispatcher_thread_data;

  // Initialize code cache
  thread_data->code_cache = mmap(NULL, sizeof(dbm_code_cache), PROT_EXEC | PROT_READ | PROT_WRITE, CC_MMAP_OPTS, -1, 0);
  if (thread_data->code_cache == MAP_FAILED) {
    fprintf(stderr, "Allocating code cache space failed\n");
    while(1);
  }
  info("Code cache: %p\n", thread_data->code_cache);

  thread_data->cc_links = mmap(NULL, sizeof(ll) + sizeof(ll_entry) * MAX_CC_LINKS, PROT_READ | PROT_WRITE, METADATA_MMAP_OPTS, -1, 0);
  assert(thread_data->cc_links != MAP_FAILED);

  // Initialize the hash table and basic block allocator, mark all BBs as unknown type
  flush_code_cache(thread_data);

  // Copy the trampolines to the code cache
  memcpy(&thread_data->code_cache->blocks[0], &start_of_dispatcher_s, trampolines_size_bytes);

  dispatcher_thread_data = (dbm_thread **)((uintptr_t)&thread_data->code_cache->blocks[0]
                                           + dispatcher_thread_data_offset);
  *dispatcher_thread_data = thread_data;

  uint32_t **dispatcher_is_pending = (uint32_t **)((uintptr_t)&thread_data->code_cache->blocks[0]
                                           + th_is_pending_ptr_offset);
  *dispatcher_is_pending = &thread_data->is_signal_pending;

  debug("*thread_data in dispatcher at: %p\n", dispatcher_thread_data);

#ifdef DBM_TRACES
  thread_data->trace_head_incr_addr = (uintptr_t)&thread_data->code_cache[0] + trace_head_incr_offset;

  #ifdef __arm__
  uint16_t *write_p = (uint16_t *)(thread_data->trace_head_incr_addr + 4 - 1);
  copy_to_reg_32bit(&write_p, r1, (uint32_t)thread_data->exec_count);
  #endif
  #ifdef __aarch64__
  uint32_t *write_p = (uint32_t *)(thread_data->trace_head_incr_addr + 4);
  a64_copy_to_reg_64bits(&write_p, x2, (uintptr_t)thread_data->exec_count);
  #endif

  info("Traces start at: %p\n", &thread_data->code_cache->traces);
#endif // DBM_TRACES

  __clear_cache((char *)&thread_data->code_cache->blocks[0], (char *)&thread_data->code_cache->blocks[thread_data->free_block]);
 
  thread_data->dispatcher_addr = (uintptr_t)&thread_data->code_cache[0] + dispatcher_wrapper_offset;
  thread_data->syscall_wrapper_addr = (uintptr_t)&thread_data->code_cache[0] + syscall_wrapper_offset;

  thread_data->status = THREAD_RUNNING;
                        
  debug("Syscall wrapper addr: 0x%" PRIxPTR "\n", thread_data->syscall_wrapper_addr);
}

void free_all_other_threads(dbm_thread *thread_data) {
  dbm_thread *it = global_data.threads;
  while(it != NULL) {
    dbm_thread *next = thread_data->next_thread;
    if (it != thread_data) {
      assert(free_thread_data(it) == 0);
    }
    it = next;
  }
  global_data.threads = thread_data;
}


void reset_process(dbm_thread *thread_data) {
  thread_data->tid = syscall(__NR_gettid);

  int ret = pthread_mutex_init(&global_data.thread_list_mutex, NULL);
  assert(ret == 0);

  current_thread = thread_data;
  free_all_other_threads(thread_data);

  /*
      MASSIVE HACK

      After fork in a multithreaded application, only async-signal-safe functions
      are safe to call. However, instrumentation plugins are likely to need
      printf, which might have been locked by a different thread in the parent
      process. Here we open new, unlocked, stdout and stderr streams.
  */
  stdout = fdopen(1, "a");
  stderr = fdopen(2, "a");

  mambo_deliver_callbacks(PRE_THREAD_C, thread_data);
}

bool is_bb(dbm_thread * const thread_data, const uintptr_t addr) {
  const uintptr_t cc_start = (uintptr_t)thread_data->code_cache->blocks;
  const uintptr_t bbc_end = (uintptr_t)thread_data->code_cache->traces;

  return (addr >= cc_start) && (addr < bbc_end);
}

bool is_trace(dbm_thread * const thread_data, const uintptr_t addr) {
  const uintptr_t bbc_end = (uintptr_t)thread_data->code_cache->traces;
  const uintptr_t cc_end = bbc_end + TRACE_CACHE_SIZE;

  return (addr >= bbc_end) && (addr < cc_end);
}

int addr_to_bb_id(dbm_thread * const thread_data, const uintptr_t addr) {
  const uintptr_t cc_start = (uintptr_t)thread_data->code_cache->blocks;
  const uintptr_t bbc_end = (uintptr_t)thread_data->code_cache->traces;

  if (addr < cc_start || addr > bbc_end) {
    return -1;
  }

  return (addr - cc_start) / sizeof(dbm_block);
}

int addr_to_fragment_id(dbm_thread * const thread_data, const uintptr_t addr) {
  const uintptr_t cc_start = (uintptr_t)thread_data->code_cache->blocks;
  assert(addr >= cc_start && addr < (cc_start + MAX_BRANCH_RANGE));

  int id = addr_to_bb_id(thread_data, addr);
  if (id >= 0) {
    if (thread_data->code_cache_meta[id].actual_id != 0) {
      id = thread_data->code_cache_meta[id].actual_id;
    }
    return id;
  }

#ifdef DBM_TRACES
  int first = CODE_CACHE_SIZE;
  int last = thread_data->active_trace.id - 1;
  int pivot;

  if (addr >= thread_data->code_cache_meta[last].tpc) {
    assert((void *)addr < (((void *)&thread_data->code_cache) + sizeof(dbm_code_cache)));
    return last;
  }

  while (first <= last) {
    pivot = (first + last) / 2;
    if (addr < thread_data->code_cache_meta[pivot].tpc) {
      last = pivot - 1;
    } else if (addr >= thread_data->code_cache_meta[pivot+1].tpc) {
      first = pivot + 1;
    } else {
      return pivot;
    }
  }
#endif

  return -1;
}

// TODO: handle links to traces
void record_cc_link(dbm_thread *thread_data, uintptr_t linked_from, uintptr_t linked_to_addr) {
  int linked_to = addr_to_bb_id(thread_data, linked_to_addr);

  debug("Linked 0x%" PRIxPTR " (%d) from 0x%" PRIxPTR "\n", linked_to_addr, linked_to, linked_from);

  if (linked_to < 0) return;

  ll_entry *entry = linked_list_alloc(thread_data->cc_links);
  assert(entry != NULL);

  entry->data = linked_from;
  entry->next = thread_data->code_cache_meta[linked_to].linked_from;
  thread_data->code_cache_meta[linked_to].linked_from = entry;
}

void notify_vm_op(vm_op_t op, uintptr_t addr, size_t size, int prot, int flags, int fd, off_t off) {
  switch(op) {
    case VM_MAP: {
      if (prot & PROT_EXEC) {
        int ret = interval_map_add(&global_data.exec_allocs, addr, addr + size, fd);
        assert(ret == 0);
      }
#ifdef PLUGINS_NEW
      if (fd >= 0 && (prot & PROT_EXEC)) {
        Elf *elf = elf_begin(fd, ELF_C_READ, NULL);
        if (elf != NULL) {
          function_watch_parse_elf(&global_data.watched_functions, elf, (void *)addr);
        }
        int ret = elf_end(elf);
        assert(ret == 0);
      }
#endif // PLUGINS_NEW
      break;
    }
    case VM_UNMAP: {
      ssize_t ret = interval_map_delete(&global_data.exec_allocs, addr, addr + size);
      assert(ret >= 0);
      // TODO: flush the code cache in all threads
      if (ret >= 1) {
        flush_code_cache(current_thread);
      }
      break;
    }
    case VM_PROT: {
      /* BUG: adding PROT_EXEC to an existing mapping results in the fd always being -1
         Fortunately, the fd is only used for symbol resolution and the GNU linker marks
         file mappings with PROT_EXEC from the beginning, so it shouldn't really be an issue */
      if (prot & PROT_EXEC) {
        int ret = interval_map_add(&global_data.exec_allocs, addr, addr + size, fd);
        assert(ret == 0);
      }
      break;
    }
  } // switch

#ifdef PLUGINS_NEW
  mambo_context ctx;
  set_mambo_context(&ctx, current_thread, VM_OP_C);
  ctx.vm.op = op;
  ctx.vm.addr = (void *)addr;
  ctx.vm.size = size;
  ctx.vm.prot = prot;
  ctx.vm.flags = flags;
  ctx.vm.filedes = fd;
  ctx.vm.off = off;

  mambo_deliver_callbacks_for_ctx(&ctx);
#endif
}

void main(int argc, char **argv, char **envp) {
  Elf *elf = NULL;
  
  if (argc < 2) {
    printf("Syntax: dbm elf_file arguments\n");
    exit(EXIT_FAILURE);
  }

  global_data.argc = argc;
  global_data.argv = argv;

  // Obtain the page size if it's not already known
  PAGE_SIZE;
  assert(page_size > 0);

  int ret = pthread_mutex_init(&global_data.thread_list_mutex, NULL);
  assert(ret == 0);

  ret = interval_map_init(&global_data.exec_allocs, 512);
  assert(ret == 0);

  ret = pthread_mutex_init(&global_data.signal_handlers_mutex, NULL);
  assert(ret == 0);

  install_system_sig_handlers();

  global_data.brk = 0;
  struct elf_loader_auxv auxv;
  uintptr_t entry_address;
  load_elf(argv[1], &elf, &auxv, &entry_address, false);
  debug("entry address: 0x%" PRIxPTR "\n", entry_address);

  // Set up brk emulation
  ret = pthread_mutex_init(&global_data.brk_mutex, NULL);
  assert(ret == 0);
  void *map = mmap((void *)global_data.brk, PAGE_SIZE, PROT_READ | PROT_WRITE,
                     MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
  assert(map != MAP_FAILED);
  global_data.initial_brk = global_data.brk = (uintptr_t)map;
  global_data.brk += PAGE_SIZE;
  
  dbm_thread *thread_data;
  if (!allocate_thread_data(&thread_data)) {
    fprintf(stderr, "Failed to allocate initial thread data\n");
    while(1);
  }
  current_thread = thread_data;
  init_thread(thread_data);
  thread_data->tid = syscall(__NR_gettid);
  register_thread(thread_data, false);

  uintptr_t block_address = scan(thread_data, (uint16_t *)entry_address, ALLOCATE_BB);
  debug("Address of first basic block is: 0x%" PRIxPTR "\n", block_address);

  #define ARGDIFF 2
  elf_run(block_address, argv[1], argc-ARGDIFF, &argv[ARGDIFF], envp, &auxv);
}


```

`dbm.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifndef __DBM_H__
#define __DBM_H__

#include <stdbool.h>
#include <signal.h>
#include <limits.h>
#include <stdint.h>
#include <sys/auxv.h>
#include <libelf.h>
#include <inttypes.h>
#ifdef __arm__
#include "pie/pie-arm-decoder.h"
#include "pie/pie-thumb-decoder.h"
#endif

#include "common.h"
#include "util.h"

/* Various parameters which can be tuned */

// BASIC_BLOCK_SIZE should be a power of 2
#define BASIC_BLOCK_SIZE 64
#ifdef DBM_TRACES
  #define CODE_CACHE_SIZE 55000
#else
  #define CODE_CACHE_SIZE 65000
#endif
#define TRACE_FRAGMENT_NO 60000
#define CODE_CACHE_OVERP 30
#define TRACE_FRAGMENT_OVERP 50
#define MAX_BRANCH_RANGE (16*1024*1024)
#define TRACE_CACHE_SIZE (MAX_BRANCH_RANGE - (CODE_CACHE_SIZE*BASIC_BLOCK_SIZE * 4))
#define TRACE_LIMIT_OFFSET (2*1024)

#define TRACE_ALIGN 4 // must be a power of 2
#define TRACE_ALIGN_MASK (TRACE_ALIGN-1)

#define INST_CNT 400

#define MAX_TB_INDEX  152
#define TB_CACHE_SIZE 32

#define MAX_BACK_INLINE 5
#define MAX_TRACE_FRAGMENTS 20

#define RAS_SIZE (4096*5)
#define TBB_TARGET_REACHED_SIZE 30

#define MAX_CC_LINKS 100000

#define THUMB 0x1
#define FULLADDR 0x2

#define MAX_PLUGIN_NO (10)

typedef enum {
  mambo_bb = 0,
  mambo_trace,
  mambo_trace_entry
} cc_type;

typedef enum {
  unknown,
  stub,
  trace_inline_max,
#ifdef __arm__
  uncond_b_to_bl_thumb,
  uncond_imm_thumb,
  uncond_reg_thumb,
  cond_imm_thumb,
  cond_reg_thumb,
  cbz_thumb,
  uncond_blxi_thumb,
  cond_blxi_thumb,
  cond_imm_arm,
  uncond_imm_arm,
  cond_reg_arm,
  uncond_reg_arm,
  uncond_blxi_arm,
  cond_blxi_arm,
  tbb,
  tbh,
  tb_indirect,
  pred_bxlr,
  pred_pop16pc,
  pred_ldmfd32pc,
  pred_armbxlr,
  pred_ldrpcsp,
  pred_armldmpc,
  pred_bad,
#endif //__arm__
#ifdef __aarch64__
  uncond_imm_a64,
  uncond_branch_reg,
  cond_imm_a64,
  cbz_a64,
  tbz_a64,
  trace_exit
#endif // __aarch64__
} branch_type;

typedef struct {
  uint32_t words[BASIC_BLOCK_SIZE];
} dbm_block;

typedef struct {
  dbm_block blocks[CODE_CACHE_SIZE];
  uint8_t  traces[TRACE_CACHE_SIZE];
} dbm_code_cache;

#define FALLTHROUGH_LINKED (1 << 0)
#define BRANCH_LINKED (1 << 1)
#define BOTH_LINKED (1 << 2)

#define MAX_SAVED_EXIT_SZ 12
typedef struct {
  uint16_t *source_addr;
  uintptr_t tpc;
  branch_type exit_branch_type;
  int actual_id;
#ifdef __arm__
  uint16_t *exit_branch_addr;
#endif // __arm__
#ifdef __aarch64__
  uint32_t *exit_branch_addr;
#endif // __arch64__
  uintptr_t branch_taken_addr;
  uintptr_t branch_skipped_addr;
  uintptr_t branch_condition;
  uintptr_t branch_cache_status;
  uint32_t rn;
  uint32_t free_b;
  ll_entry *linked_from;
  uint8_t saved_exit[MAX_SAVED_EXIT_SZ];
} dbm_code_cache_meta;

typedef struct {
  unsigned long flags;
  void *child_stack;
  pid_t *ptid;
  uintptr_t tls;
  pid_t *ctid;
} sys_clone_args;

struct trace_exits {
  uintptr_t from;
  uintptr_t to;
#ifdef __aarch64__
  int fragment_id;
#endif
};

#define MAX_TRACE_REC_EXITS (MAX_TRACE_FRAGMENTS+1)
typedef struct {
  int id;
  int source_bb;
  void *write_p;
  uintptr_t entry_addr;
  bool active;
  int free_exit_rec;
  struct trace_exits exits[MAX_TRACE_REC_EXITS];
} trace_in_prog;

enum dbm_thread_status {
  THREAD_RUNNING = 0,
  THREAD_SYSCALL,
  THREAD_EXIT
};

typedef struct dbm_thread_s dbm_thread;
struct dbm_thread_s {
  dbm_thread *next_thread;
  enum dbm_thread_status status;

  int free_block;
  bool was_flushed;
  uintptr_t dispatcher_addr;
  uintptr_t syscall_wrapper_addr;

  dbm_code_cache *code_cache;
  dbm_code_cache_meta code_cache_meta[CODE_CACHE_SIZE + TRACE_FRAGMENT_NO];
  hash_table entry_address;
#ifdef DBM_TRACES
  uint8_t   exec_count[CODE_CACHE_SIZE];
  uintptr_t trace_head_incr_addr;
  uint8_t  *trace_cache_next;
  int       trace_id;
  int       trace_fragment_count;
  trace_in_prog active_trace;
#endif

  ll *cc_links;

  uintptr_t tls;
  uintptr_t child_tls;

#ifdef PLUGINS_NEW
  void *plugin_priv[MAX_PLUGIN_NO];
#endif
  void *clone_ret_addr;
  pid_t tid;
  volatile pid_t *set_tid;
  sys_clone_args *clone_args;
  bool clone_vm;
  int pending_signals[_NSIG];
  uint32_t is_signal_pending;
  void *mambo_sp;
};

typedef enum {
  ARM_INST,
  THUMB_INST,
  A64_INST,
} inst_set;

typedef enum {
  VM_MAP,
  VM_UNMAP,
  VM_PROT
} vm_op_t;

#include "api/plugin_support.h"

typedef struct {
  char *name;
  int plugin_id;
  mambo_callback pre_callback;
  mambo_callback post_callback;
} watched_func_t;

typedef struct {
  void *addr;
  watched_func_t *func;
} watched_funcp_t;

#define MAX_WATCHED_FUNCS 40
#define MAX_WATCHED_FUNC_PTRS 80
typedef struct {
  int func_count;
  pthread_mutex_t funcs_lock;
  watched_func_t  funcs[MAX_WATCHED_FUNCS];

  int funcp_count;
  pthread_mutex_t funcps_lock;
  watched_funcp_t funcps[MAX_WATCHED_FUNC_PTRS];
} watched_functions_t;

typedef struct {
  int argc;
  char **argv;
  interval_map exec_allocs;

  uintptr_t signal_handlers[_NSIG];
  pthread_mutex_t signal_handlers_mutex;

  uintptr_t brk;
  uintptr_t initial_brk;
  pthread_mutex_t brk_mutex;

  dbm_thread *threads;
  pthread_mutex_t thread_list_mutex;

  volatile int exit_group;

#ifdef PLUGINS_NEW
  int free_plugin;
  mambo_plugin plugins[MAX_PLUGIN_NO];
  watched_functions_t watched_functions;
#endif
} dbm_global;

typedef struct {
  uintptr_t tpc;
  uintptr_t spc;
} cc_addr_pair;

void dbm_exit(dbm_thread *thread_data, uint32_t code);
void thread_abort(dbm_thread *thread_data);

extern void dispatcher_trampoline();
extern void syscall_wrapper();
extern void trace_head_incr();
extern void* start_of_dispatcher_s;
extern void* end_of_dispatcher_s;
extern void th_to_arm();
extern void th_enter(void *stack, uintptr_t cc_addr);
extern void send_self_signal();
extern void syscall_wrapper_svc();

int lock_thread_list(void);
int unlock_thread_list(void);
int register_thread(dbm_thread *thread_data, bool caller_has_lock);
int unregister_thread(dbm_thread *thread_data, bool caller_has_lock);
bool allocate_thread_data(dbm_thread **thread_data);
int free_thread_data(dbm_thread *thread_data);
void init_thread(dbm_thread *thread_data);
void reset_process(dbm_thread *thread_data);

uintptr_t cc_lookup(dbm_thread *thread_data, uintptr_t target);
uintptr_t lookup_or_scan(dbm_thread * const thread_data, uintptr_t target);
uintptr_t lookup_or_scan_with_cached(dbm_thread * const thread_data,
                                     const uintptr_t target,
                                     bool * const cached);
uintptr_t lookup_or_stub(dbm_thread *thread_data, uintptr_t target);
uintptr_t scan(dbm_thread *thread_data, uint16_t *address, int basic_block);
uint32_t scan_a32(dbm_thread *thread_data, uint32_t *read_address, int basic_block, cc_type type, uint32_t *write_p);
uint32_t scan_t32(dbm_thread *thread_data, uint16_t *read_address, int basic_block, cc_type type, uint16_t *write_p);
size_t   scan_a64(dbm_thread *thread_data, uint32_t *read_address, int basic_block, cc_type type, uint32_t *write_p);
int allocate_bb(dbm_thread *thread_data);
void trace_dispatcher(uintptr_t target, uintptr_t *next_addr, uint32_t source_index, dbm_thread *thread_data);
void flush_code_cache(dbm_thread *thread_data);
void insert_cond_exit_branch(dbm_code_cache_meta *bb_meta, void **o_write_p, int cond);
void sigret_dispatcher_call(dbm_thread *thread_data, ucontext_t *cont, uintptr_t target);

void thumb_encode_stub_bb(dbm_thread *thread_data, int basic_block, uint32_t target);
void arm_encode_stub_bb(dbm_thread *thread_data, int basic_block, uint32_t target);

int addr_to_bb_id(dbm_thread * const thread_data, const uintptr_t addr);
int addr_to_fragment_id(dbm_thread * const thread_data, const uintptr_t addr);
void record_cc_link(dbm_thread *thread_data, uintptr_t linked_from, uintptr_t linked_to_addr);
bool is_bb(dbm_thread * const thread_data, const uintptr_t addr);
bool is_trace(dbm_thread * const thread_data, const uintptr_t addr);

void install_system_sig_handlers();

#define MAP_INTERP (0x40000000)
#define MAP_APP (0x20000000)
void notify_vm_op(vm_op_t op, uintptr_t addr, size_t size, int prot, int flags, int fd, off_t off);

#ifdef __arm__
void thumb_simple_exit(dbm_thread *thread_data, uint16_t **o_write_p, int bb_index, uint32_t target);
void arm_simple_exit(dbm_thread *thread_data, uint32_t **o_write_p, int bb_index,
                     uint32_t offset, uint32_t *read_address, uint32_t cond);
#endif

inline static uintptr_t adjust_cc_entry(uintptr_t addr) {
#ifdef __arm__
  if (addr != UINT_MAX) {
    addr += 4 - ((addr & 1) << 1); // +4 for ARM, +2 for Thumb
  }
#endif
  return addr;
}

extern dbm_global global_data;
extern uintptr_t page_size;
extern dbm_thread *disp_thread_data;
extern uint32_t *th_is_pending_ptr;
extern __thread dbm_thread *current_thread;

/* API-related functions */
#ifdef PLUGINS_NEW
void set_mambo_context(mambo_context *ctx, dbm_thread *thread_data, mambo_cb_idx event_type);
void set_mambo_context_code(mambo_context *ctx, dbm_thread *thread_data, mambo_cb_idx event_type,
                            cc_type fragment_type, int fragment_id, inst_set inst_type, int inst,
                            mambo_cond cond, void *read_address, void *write_p, void *data_p, bool *stop);
void set_mambo_context_syscall(mambo_context *ctx, dbm_thread *thread_data, mambo_cb_idx event_type,
                               uintptr_t number, uintptr_t *regs);
#endif
void mambo_deliver_callbacks_for_ctx(mambo_context *ctx);
void mambo_deliver_callbacks(unsigned cb_id, dbm_thread *thread_data);
void mambo_deliver_callbacks_code(unsigned cb_id, dbm_thread *thread_data, cc_type fragment_type,
                                  int fragment_id, inst_set inst_type, int inst, mambo_cond cond,
                                  void *read_address, void *write_p, void *data_p, bool *stop);
void _function_callback_wrapper(mambo_context *ctx, watched_func_t *func);
int function_watch_parse_elf(watched_functions_t *self, Elf *elf, void *base_addr);
int function_watch_add(watched_functions_t *self, char *name, int plugin_id,
                       mambo_callback pre_callback, mambo_callback post_callback);

#define min(a, b) (((a) < (b)) ? (a) : (b))
#define max(a, b) (((a) > (b)) ? (a) : (b))

/* Constants */

#define ALLOCATE_BB 0

#ifdef CC_HUGETLB
  #define CC_PAGE_SIZE (2*1024*1024)
  #define CC_MMAP_OPTS (MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB)
#else
  #define CC_PAGE_SIZE (page_size)
  #define CC_MMAP_OPTS (MAP_PRIVATE|MAP_ANONYMOUS)
#endif

#ifdef METADATA_HUGETLB
  #define METADATA_PAGE_SIZE (2*1024*1024)
  #define METADATA_MMAP_OPTS (MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB)
#else
  #define METADATA_PAGE_SIZE (page_size)
  #define METADATA_MMAP_OPTS (MAP_PRIVATE|MAP_ANONYMOUS)
#endif

#define ROUND_UP(input, multiple_of) \
  ((((input) / (multiple_of)) * (multiple_of)) + (((input) % (multiple_of)) ? (multiple_of) : 0))

#define CC_SZ_ROUND(input) ROUND_UP(input, CC_PAGE_SIZE)
#define METADATA_SZ_ROUND(input) ROUND_UP(input, CC_PAGE_SIZE)

#define PAGE_SIZE (page_size != 0 ? page_size : (page_size = getauxval(AT_PAGESZ)))

#define trampolines_size_bytes         ((uintptr_t)&end_of_dispatcher_s - (uintptr_t)&start_of_dispatcher_s)
#define trampolines_size_bbs           ((trampolines_size_bytes / sizeof(dbm_block)) \
                                      + ((trampolines_size_bytes % sizeof(dbm_block)) ? 1 : 0))

#define UNLINK_SIGNAL (SIGILL)
#define CPSR_T (0x20)

#ifdef __arm__
  #define context_pc uc_mcontext.arm_pc
  #define context_sp uc_mcontext.arm_sp
  #define context_reg(reg) uc_mcontext.arm_r##reg
#elif __aarch64__
  #define context_pc uc_mcontext.pc
  #define context_sp uc_mcontext.sp
  #define context_reg(reg) uc_mcontext.regs[reg]
#endif

#endif


```

`dispatcher.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2015-2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>

#include "dbm.h"
#include "scanner_common.h"
#ifdef __arm__
void dispatcher_aarch32(dbm_thread *thread_data, uint32_t source_index,
                        branch_type exit_type, uintptr_t target,
                        uintptr_t block_address);
#elif __aarch64__
void dispatcher_aarch64(dbm_thread *thread_data, uint32_t source_index,
                        branch_type exit_type, uintptr_t target,
                        uintptr_t block_address);
#endif

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

void dispatcher(const uintptr_t target, const uint32_t source_index,
                uintptr_t * const next_addr, dbm_thread * const thread_data) {
/* It's essential to copy exit_branch_type before calling lookup_or_scan
     because when scanning a stub basic block the source block and its
     meta-information get overwritten */
  debug("Source block index: %d\n", source_index);
  branch_type source_branch_type =
                    thread_data->code_cache_meta[source_index].exit_branch_type;

#ifdef DBM_TRACES
  // Handle trace exits separately
  if (source_index >= CODE_CACHE_SIZE) {
#ifdef __arm__
    if (source_branch_type != tbb && source_branch_type != tbh)
#endif
      return trace_dispatcher(target, next_addr, source_index, thread_data);
  }
#endif

  debug("Reached the dispatcher, target: 0x%" PRIxPTR ", ret: %p, src: %d thr: %p\n",
        target, next_addr, source_index, thread_data);
  thread_data->was_flushed = false;

#ifdef DEBUG
  bool cached;
  *next_addr = lookup_or_scan_with_cached(thread_data, target, &cached);
  if (cached) {
    debug("Found block from %d for 0x%" PRIxPTR " in cache at 0x%" PRIxPTR "\n",
          source_index, target, *next_addr);
  } else {
    debug("Scanned at 0x%" PRIxPTR " for 0x%" PRIxPTR "\n", *next_addr, target);
  }
#else
   *next_addr = lookup_or_scan(thread_data, target);
#endif

  // Bypass any linking
  if (source_index == 0 || thread_data->was_flushed) {
    return;
  }

#ifdef __arm__
  dispatcher_aarch32(thread_data, source_index, source_branch_type, target,
                     *next_addr);
#endif
#ifdef __aarch64__
  dispatcher_aarch64(thread_data, source_index, source_branch_type, target,
                     *next_addr);
#endif
}

```

`elf/elf_loader.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdlib.h>
#include <stdio.h>
#include <libelf.h>
#include <fcntl.h>
#include <inttypes.h>
#include <errno.h>
#include <string.h>
#include <assert.h>
#include <sys/mman.h>

#include "elf_loader.h"
#include "../dbm.h"
#include "../common.h"

#ifndef AT_MINSIGSTKSZ
  #define AT_MINSIGSTKSZ 51
#endif

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

extern void *__ehdr_start;

void load_segment(uintptr_t base_addr, ELF_PHDR *phdr, int fd, Elf32_Half type, bool is_interp) {
  uint32_t *mem;
  int prot = 0;
  uintptr_t aligned_vaddr, aligned_fsize, aligned_msize, page_offset, map_file_end;

#ifdef ENABLE_EXECUTE
  if (phdr->p_flags & PF_X) {
    prot |= PROT_EXEC;
  }
#endif

  if (phdr->p_flags & PF_W) {
    prot |= PROT_WRITE;
  }

  if (phdr->p_flags & PF_R) {
    prot |= PROT_READ;
  }

  if (type == ET_DYN) {
    assert(base_addr != 0);
    phdr->p_vaddr += base_addr;
  }

  aligned_vaddr = align_lower(phdr->p_vaddr, PAGE_SIZE);
  page_offset = phdr->p_vaddr - aligned_vaddr;
  aligned_fsize = align_higher(phdr->p_filesz + page_offset, PAGE_SIZE);
  map_file_end = phdr->p_vaddr + phdr->p_filesz;
  aligned_msize = align_higher(phdr->p_memsz + page_offset, PAGE_SIZE);

  // Map a page-aligned file-backed segment including the (vaddr, vaddr + filesize) area
  #define MAP_EL_FILE (MAP_PRIVATE|MAP_FIXED)
  off_t offset = phdr->p_offset - page_offset;
  mem = mmap((void *)aligned_vaddr, aligned_fsize, prot,
             MAP_EL_FILE, fd, offset);
  assert(mem != MAP_FAILED);
  int flags = MAP_EL_FILE|(is_interp ? MAP_INTERP : MAP_APP);
  notify_vm_op(VM_MAP, aligned_vaddr, aligned_fsize,
               prot | ((phdr->p_flags & PF_X) ? PROT_EXEC : 0), flags, fd, offset);

  // Zero the area from (vaddr + filesize) to the end of the page
  if (phdr->p_flags & PF_W) {
    memset((void *)map_file_end, 0, (aligned_vaddr + aligned_fsize) - map_file_end);
  }

  // Allocate anonymous pages if aligned memsize > filesize
  #define MAP_EL_ANON (MAP_ANONYMOUS|MAP_EL_FILE)
  if (aligned_msize > aligned_fsize) {
    mem = mmap((void *)(aligned_vaddr + aligned_fsize), aligned_msize-aligned_fsize, prot,
               MAP_EL_ANON, -1, 0);
    assert(mem != MAP_FAILED);
    notify_vm_op(VM_MAP, aligned_vaddr + aligned_fsize, aligned_msize-aligned_fsize,
                 prot | ((phdr->p_flags & PF_X) ? PROT_EXEC : 0), MAP_EL_ANON, -1, 0);
  }

#ifdef ENABLE_EXECUTE
  if (phdr->p_flags & PF_X) {
    __clear_cache((char *)phdr->p_vaddr, (char *)phdr->p_vaddr + (char *)phdr->p_memsz);
  }
#endif

  if (!is_interp && (aligned_vaddr + aligned_msize) > global_data.brk) {
    global_data.brk = aligned_vaddr + aligned_msize;
  }
}

void load_elf(char *filename, Elf **ret_elf, struct elf_loader_auxv *auxv, uintptr_t *entry_addr, bool is_interp) {
  int fd;
  FILE *file;
  Elf *elf;
  Elf_Kind kind;
  ELF_EHDR *ehdr;
  ELF_PHDR *phdr;
  char interp[256];
  errno = 0;
  size_t phnum;

  fd = open(filename, O_RDONLY);
  if (fd < 0) {
    printf("Couldn't open file %s\n", filename);
    exit(EXIT_FAILURE);
  }
  
  if (elf_version(EV_CURRENT) == EV_NONE) {
    printf("Error setting ELF version\n");
    exit(EXIT_FAILURE);
  }
  
  elf = elf_begin(fd, ELF_C_READ, NULL);
  *ret_elf = elf;
  if (elf == NULL) {
    printf("Error opening ELF file: %s: %s\n", filename, elf_errmsg(-1));
    exit(EXIT_FAILURE);
  }
  
  kind = elf_kind(elf);
  if (kind != ELF_K_ELF) {
    printf("File %s isn't an ELF file\n", filename);
    exit(EXIT_FAILURE);
  }
  
  ehdr = ELF_GETEHDR(elf);
  if (ehdr == NULL) {
    printf("Error reading the ELF executable header: %s\n", elf_errmsg(-1));
    exit(EXIT_FAILURE);
  }
  
  if (ehdr->e_ident[EI_CLASS] != ELF_CLASS) {
    printf("Not a 32-bit ELF file\n");
    exit(EXIT_FAILURE);
  }

  if (ehdr->e_machine != EM_MACHINE) {
    printf("Not compiled for ARM\n");
    exit(EXIT_FAILURE);
  }

  file = fdopen(fd, "r");

  elf_getphdrnum(elf, &phnum);
  phdr = ELF_GETPHDR(elf);

  /* Allocate the whole memory region, using ASLR if enabled in the kernel */
  void *base_addr = NULL;
  uintptr_t min_addr = UINTPTR_MAX;
  uintptr_t max_addr = 0;

  for (int i = 0; i < phnum; i++) {
    if (phdr[i].p_type == PT_LOAD) {
      uintptr_t end = phdr[i].p_vaddr + phdr[i].p_memsz;
      if (end > max_addr) {
        max_addr = end;
      }
      if (phdr[i].p_vaddr < min_addr) {
        min_addr = phdr[i].p_vaddr;
      }
    }
  }

  if (ehdr->e_type == ET_DYN) {
    assert(min_addr == 0);
  } else {
    assert(min_addr != 0);
  }

  base_addr = mmap((void *)min_addr, max_addr - min_addr, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
  if (ehdr->e_type == ET_DYN) {
    assert(base_addr != MAP_FAILED);
    ehdr->e_entry += (uintptr_t)base_addr;
  } else {
    assert(base_addr == (void*)min_addr);
  }

  /* entry address is the actual execution entry point, either in the interpreter
     (if one is used), or in the executable */
  *entry_addr = ehdr->e_entry;

  // AT_ENTRY in the AUXV points to the original executable
  if (!is_interp) {
    auxv->at_entry = ehdr->e_entry;
  }

  // Look for an INTERP header
  for (int i = 0; i < phnum; i++) {
    if (phdr[i].p_type == PT_INTERP) {
      debug("INTERP field found\n");
      assert(!is_interp);
      
      if (phdr[i].p_filesz > 255) {
        printf("INTERP filename longer than the buffer\n");
        while(1);
      }
      
      if (fseek(file, phdr[i].p_offset, SEEK_SET) != 0) {
        printf("Seek to INTERP field failed");
        while(1);
      }
      
      if(fread(interp, sizeof(char), phdr[i].p_filesz, file) != phdr[i].p_filesz) {
        printf("Failed reading INTERP string\n");
        while(1);
      }
      interp[phdr[i].p_filesz] = '\0';
      
      load_elf(interp, ret_elf, auxv, entry_addr, true);
    }
  }

  if (is_interp) {
    auxv->at_base = 0;
  } else {
    auxv->at_phdr = 0;
    auxv->at_phnum = phnum;
  }

  for (int i = 0; i < phnum; i++) {
    debug("\np_type: 0x%x\n", phdr[i].p_type);
    debug("p_offset: 0x%x\n", phdr[i].p_offset);
    debug("p_vaddr: 0x%x\n", phdr[i].p_vaddr);
    debug("p_paddr: 0x%x\n", phdr[i].p_paddr);
    debug("p_filesz: 0x%x\n", phdr[i].p_filesz);
    debug("p_memsz: 0x%x\n", phdr[i].p_memsz);
    debug("p_flags: 0x%x\n", phdr[i].p_flags);
    debug("p_align: 0x%x\n", phdr[i].p_align);
    
    switch(phdr[i].p_type) {
      case PT_LOAD:
        load_segment((uintptr_t)base_addr, &phdr[i], fd, ehdr->e_type, is_interp);
        if (is_interp) {
          if (phdr[i].p_offset == 0) {
            auxv->at_base = phdr[i].p_vaddr;
          }
        } else { // !is_interp
          if (phdr[i].p_offset == 0) {
            auxv->at_phdr = phdr[i].p_vaddr + ehdr->e_phoff;
          }
        }
        break;
      default:
        debug("Unhandled program header table entry type\n");
        break;
    }
  }

  if (is_interp) {
    assert(auxv->at_base);
  } else { // !is_interp
    assert(auxv->at_phdr);
  }
}

size_t find_stack_data_size(char *filename, int argc, char **argv, char **envp) {
  size_t size = (4 + argc) * sizeof(uintptr_t); // ARGC, ARGV[0], NULL after ARGs and ENVP
  size += 16; // AT_RANDOM
  size += strlen(filename) + 1;

  for (int i = 0; i < argc; i++) {
    size += strlen(argv[i]) + 1;
  }

  for (; *envp != NULL; envp++) {
    size += sizeof(uintptr_t) + strlen(*envp) + 1;
  }

  ELF_AUXV_T *s_aux = (ELF_AUXV_T *)(envp + 1);
  while(s_aux->a_type != AT_NULL) {
    switch(s_aux->a_type) {
      case AT_PLATFORM:
      case AT_EXECFN: {
        char *s = (char *)s_aux->a_un.a_val;
        size += strlen(s) + 1;
      }
    } // switch
    size += sizeof(*s_aux);
    s_aux++;
  } // while
  size += sizeof(*s_aux);

  return size;
}

char *copy_string_to_stack(char *string, char **stack_strings) {
  size_t len = strlen(string) + 1;
  *stack_strings -= len;
  strncpy(*stack_strings, string, len);
  return *stack_strings;
}

#define INITIAL_STACK_SIZE (4*1024*1024)
#define STACK_PROT  (PROT_READ | PROT_WRITE)
#define STACK_FLAGS (MAP_PRIVATE|MAP_ANONYMOUS|MAP_GROWSDOWN|MAP_STACK)
#define stack_push(val) stack[stack_i++] = (val);

void elf_run(uintptr_t entry_address, char *filename, int argc, char **argv, char **envp, struct elf_loader_auxv *auxv) {
  // Allocate a new stack for the execution of the application
  void *stack_space = mmap(NULL, INITIAL_STACK_SIZE, STACK_PROT, STACK_FLAGS, -1, 0);
  assert(stack_space != MAP_FAILED);
  notify_vm_op(VM_MAP, (uintptr_t)stack_space, INITIAL_STACK_SIZE, STACK_PROT, STACK_FLAGS, -1, 0);

  // Grows up (towards lower addresses)
  char *stack_strings = stack_space + INITIAL_STACK_SIZE;

  // Grows down (towards higher addresses)
  size_t data_size = find_stack_data_size(filename, argc, argv, envp);
  uintptr_t *stack = (uintptr_t *)align_lower((uintptr_t)(stack_space + INITIAL_STACK_SIZE - data_size), 16);
  int stack_i = 0;

  // Copy args
  stack_push(argc + 1);
  stack_push((uintptr_t)copy_string_to_stack(filename, &stack_strings));
  for (int i = 0; i < argc; i++) {
    stack_push((uintptr_t)copy_string_to_stack(argv[i], &stack_strings));
  }
  stack_push((uintptr_t)NULL);
  
  // Copy env
  while (*envp != NULL) {
    stack_push((uintptr_t)copy_string_to_stack(*envp, &stack_strings));
    envp++;
  }
  stack_push((uintptr_t)NULL);
  
  // Copy the Auxiliary Vector
  ELF_AUXV_T *s_aux = (ELF_AUXV_T *)(envp + 1);
  ELF_AUXV_T *d_aux = (ELF_AUXV_T *)&stack[stack_i];
  while(s_aux->a_type != AT_NULL) {
    d_aux->a_type = s_aux->a_type;

    switch(s_aux->a_type) {
      case AT_PAGESZ:
      case AT_HWCAP:
      case AT_HWCAP2:
      case AT_CLKTCK:
      case AT_FLAGS:
      case AT_UID:
      case AT_EUID:
      case AT_GID:
      case AT_EGID:
      case AT_SECURE:
      case AT_SYSINFO_EHDR:
      case AT_MINSIGSTKSZ:
      case AT_PHENT:
        d_aux->a_un.a_val = s_aux->a_un.a_val;
        break;

      case AT_RANDOM: {
        stack_strings -= 15;
        memcpy(stack_strings, (void *)s_aux->a_un.a_val, 16);
        d_aux->a_un.a_val = (uintptr_t)stack_strings;
        stack_strings--;
        break;
      }

      case AT_PLATFORM:
      case AT_EXECFN:
        d_aux->a_un.a_val = (uintptr_t)copy_string_to_stack((char *)s_aux->a_un.a_val, &stack_strings);
        break;

      case AT_BASE:
        d_aux->a_un.a_val = auxv->at_base;
        break;

      case AT_PHDR:
        d_aux->a_un.a_val = auxv->at_phdr;
        break;

      case AT_PHNUM:
        d_aux->a_un.a_val = auxv->at_phnum;
        break;

      case AT_ENTRY:
        d_aux->a_un.a_val = auxv->at_entry;
        break;  

      default:
        #ifdef __arm__
          #define auxv_type "%d"
        #elif __aarch64__
          #define auxv_type "%ld"
        #endif
        printf("Unhandled auxv entry type: " auxv_type "\n", s_aux->a_type);
        exit(EXIT_FAILURE);
        break;
    }
    
    s_aux++;
    d_aux++;

    stack_i += 2;
  }
  // End of list
  d_aux->a_type = AT_NULL;
  d_aux->a_un.a_val = 0;
  stack_i += 2;

  /* Stack:
  sp->argc     [LOW]
      argv[0]
      argv[1]
      ...
      argv[x]
      NULL
      envp[0]
      envp[1]
      ...
      envp[y]
      NULL
      auxv[0]
      auxv[1]
      ...
      auxv[z]  [HIGH]
  */
  assert((char *)&stack[stack_i] <= stack_strings);

  dbm_client_entry(entry_address, &stack[0]);
  
  // If we return here, something is horribly wrong
  while(1);
}

```

`elf/elf_loader.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdbool.h>

#ifdef __arm__
  #define ELF_CLASS  ELFCLASS32
  #define EM_MACHINE EM_ARM
  #define ELF_EHDR   Elf32_Ehdr
  #define ELF_PHDR   Elf32_Phdr
  #define ELF_GETEHDR(...) elf32_getehdr(__VA_ARGS__)
  #define ELF_GETPHDR(...) elf32_getphdr(__VA_ARGS__)
  #define ELF_AUXV_T Elf32_auxv_t
#endif
#ifdef __aarch64__
  #define ELF_CLASS  ELFCLASS64
  #define EM_MACHINE EM_AARCH64
  #define ELF_EHDR   Elf64_Ehdr
  #define ELF_PHDR   Elf64_Phdr
  #define ELF_GETEHDR(...) elf64_getehdr(__VA_ARGS__)
  #define ELF_GETPHDR(...) elf64_getphdr(__VA_ARGS__)
  #define ELF_AUXV_T Elf64_auxv_t
#endif

struct elf_loader_auxv {
  uintptr_t at_base;
  uintptr_t at_entry;
  uintptr_t at_phdr;
  uintptr_t at_phnum;
};

void load_elf(char *filename, Elf **ret_elf, struct elf_loader_auxv *auxv, uintptr_t *entry_addr, bool is_interp);
void elf_run(uintptr_t entry_address, char *filename, int argc, char **argv, char **envp, struct elf_loader_auxv *auxv);


```

`elf/symbol_parser.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <libelf.h>
#include <stdlib.h>
#include <stdio.h>
#include <gelf.h>
#include <assert.h>
#include <stdbool.h>
#include <string.h>
#include <pthread.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>

#include "../dbm.h"
#include "elf_loader.h"

int get_symbol_info_by_addr(uintptr_t addr, char **sym_name, void **start_addr, char **filename) {
  interval_map_entry fm;
  int ret = interval_map_search_by_addr(&global_data.exec_allocs, addr, &fm);
  *sym_name = NULL;
  if (start_addr) {
    *start_addr = NULL;
  }
  if (filename) {
    *filename = NULL;
  }
  if (ret != 1 || fm.fd < 0) return -1;

  uintptr_t sym_addr = 0;

  Elf *elf = elf_begin(fm.fd, ELF_C_READ, NULL);
  ELF_EHDR *ehdr;
  if (elf != NULL) {
    Elf_Scn *scn = NULL;
    GElf_Shdr shdr;
    GElf_Sym sym;
    ehdr = ELF_GETEHDR(elf);
    if (ehdr->e_type == ET_DYN) {
      addr -= fm.start;
    }

    while((scn = elf_nextscn(elf, scn)) != NULL) {
      gelf_getshdr(scn, &shdr);
      if(shdr.sh_type == SHT_SYMTAB || shdr.sh_type == SHT_DYNSYM) {
        Elf_Data *edata = elf_getdata(scn, NULL);
        assert(edata != NULL);
        int sym_count = shdr.sh_size / shdr.sh_entsize;

        for (int i = 0; i < sym_count; i++) {
          gelf_getsym(edata, i, &sym);
          if (sym.st_value != 0 && ELF32_ST_TYPE(sym.st_info) == STT_FUNC) {
            if (addr >= sym.st_value && addr < (sym.st_value + sym.st_size)) {
              sym_addr = sym.st_value;
              *sym_name = elf_strptr(elf, shdr.sh_link, sym.st_name);
            }
          }
        }
      } // shdr.sh_type == SHT_SYMTAB
    } // while scn iterator

    if (*sym_name != NULL) {
      *sym_name = strdup(*sym_name);
      assert(*sym_name != NULL);
    }
  }

  if (start_addr) {
    if (ehdr->e_type == ET_DYN || sym_addr == 0) {
      sym_addr += fm.start;
    }
    *start_addr = (void *)sym_addr;
  }

  if (filename != NULL) {
    const size_t buf_proc_size = 30;
    char buf_proc[buf_proc_size];
    const size_t buf_path_size = PATH_MAX + 1;
    char buf_path[buf_path_size];
    ret = snprintf(buf_proc, buf_proc_size, "/proc/self/fd/%d", fm.fd);
    assert(ret > 0);
    ret = readlink(buf_proc, buf_path, buf_path_size-1);
    assert(ret > 0);
    buf_path[ret] = '\0';
    *filename = strdup(buf_path);
    assert(*filename != NULL);
  }

  ret = elf_end(elf);
  assert(ret == 0);

  return 0;
}


stack_frame_t *get_frame(stack_frame_t *frame) {
  void *fp = frame;
#ifdef __arm__
  fp -= 4;
#endif
  return (stack_frame_t *)fp;
}

int get_backtrace(stack_frame_t *fp, stack_frame_handler handler, void *data) {
  char *filename;
  char *symbol;
  void *symbol_base;

  if (fp != NULL && fp != (void *)1) {
    stack_frame_t frame;
    int frame_rerror;
    do {
      fp = get_frame(fp);
      frame_rerror = try_memcpy(&frame, fp, sizeof(frame));
      if (frame_rerror == 0) {
        int ret = get_symbol_info_by_addr(frame.lr, &symbol, &symbol_base, &filename);
        if (ret == 0) {
          ret = handler(data, (void *)fp->lr, symbol, symbol_base, filename);
          free(symbol);
          free(filename);
          if (ret != 0) return ret;
        }
      }
    } while(frame_rerror == 0 && (frame.prev > fp) && (fp = frame.prev));

    return 0;
  }

  return -1;
}

void function_watch_lock_funcs(watched_functions_t *self) {
  int ret = pthread_mutex_lock(&self->funcs_lock);
  assert(ret == 0);
}

void function_watch_lock_funcps(watched_functions_t *self) {
  int ret = pthread_mutex_lock(&self->funcps_lock);
  assert(ret == 0);
}

void function_watch_unlock_funcs(watched_functions_t *self) {
  int ret = pthread_mutex_unlock(&self->funcs_lock);
  assert(ret == 0);
}

void function_watch_unlock_funcps(watched_functions_t *self) {
  int ret = pthread_mutex_unlock(&self->funcps_lock);
  assert(ret == 0);
}

void function_watch_init(watched_functions_t *self) {
  int ret = pthread_mutex_init(&self->funcs_lock, NULL);
  assert(ret == 0);
  ret = pthread_mutex_init(&self->funcps_lock, NULL);
  assert(ret == 0);
}

int function_watch_search(watched_functions_t *self, char *name) {
  for (int i = 0; i < self->func_count; i++) {
    if (strcmp(name, self->funcs[i].name) == 0) {
      return 1;
    }
  }
  return 0;
}

int function_watch_add(watched_functions_t *self, char *name, int plugin_id,
                       mambo_callback pre_callback, mambo_callback post_callback) {
  function_watch_lock_funcs(self);

  if (function_watch_search(self, name) > 0) return -101;

  int idx = self->func_count++;
  if (idx >= MAX_WATCHED_FUNCS) return -102;

  self->funcs[idx].name = name;
  self->funcs[idx].plugin_id = plugin_id;
  self->funcs[idx].pre_callback = pre_callback;
  self->funcs[idx].post_callback = post_callback;

  function_watch_unlock_funcs(self);

  return 0;
}

/* Memory barriers used in function modifying funcps because the
   mutex doesn't protect from reading */
int function_watch_addp(watched_functions_t *self, watched_func_t *func, void *addr) {
  int err = 0;

  function_watch_lock_funcps(self);

  for (int i = 0; i < self->funcp_count; i++) {
    if (self->funcps[i].func == func && self->funcps[i].addr == addr) {
      goto ret;
    }
  }

  int idx = self->funcp_count;
  if (idx >= MAX_WATCHED_FUNC_PTRS) {
    err = -2;
    goto ret;
  }

  self->funcps[idx].func = func;
  self->funcps[idx].addr = addr;
  asm volatile("DMB SY" ::: "memory");
  self->funcp_count++;

ret:
  function_watch_unlock_funcps(self);

  return err;
}

void function_watch_try_addp(watched_functions_t *self, char *name, void *addr) {
  function_watch_lock_funcs(self);

  for (int i = 0; i < self->func_count; i++) {
    if (strcmp(name, self->funcs[i].name) == 0) {
      int ret = function_watch_addp(self, &self->funcs[i], addr);
      assert(ret == 0);
    }
  }

  function_watch_unlock_funcs(self);
}

int function_watch_delete_addp(watched_functions_t *self, int i) {
  int last = self->funcp_count-1;
  if (i > last) {
    return -1;
  }

  if (i < last) {
    self->funcps[i].addr = NULL;
    asm volatile("DMB SY" ::: "memory");

    self->funcps[i].func = self->funcps[last].func;
    asm volatile("DMB SY" ::: "memory");

    self->funcps[i].addr = self->funcps[last].addr;
    asm volatile("DMB SY" ::: "memory");
  }

  self->funcp_count = last;
  asm volatile("DMB SY" ::: "memory");

  return 0;
}

int function_watch_addp_invalidate(watched_functions_t *self, void *addr, size_t size) {
  int ret = -1;

  function_watch_lock_funcps(self);

  for (int i = 0; i < self->funcp_count; i++) {
    if (self->funcps[i].addr >= addr && self->funcps[i].addr < (addr + size)) {
      ret = function_watch_delete_addp(self, i);
      assert(ret == 0);
    }
  }

  function_watch_unlock_funcps(self);

  return ret;
}

int function_watch_parse_elf(watched_functions_t *self, Elf *elf, void *base_addr) {
  Elf_Scn *scn = NULL;
  GElf_Shdr shdr;
  GElf_Sym sym;
  ELF_EHDR *ehdr = ELF_GETEHDR(elf);
  if (ehdr == NULL) {
    printf("Error reading the ELF executable header: %s\n", elf_errmsg(-1));
    return -1;
  }
  if (ehdr->e_type == ET_EXEC) {
    base_addr = NULL;
  }

  while((scn = elf_nextscn(elf, scn)) != NULL) {
    gelf_getshdr(scn, &shdr);
    if(shdr.sh_type == SHT_SYMTAB || shdr.sh_type == SHT_DYNSYM) {
      Elf_Data *edata = elf_getdata(scn, NULL);
      assert(edata != NULL);
      int sym_count = shdr.sh_size / shdr.sh_entsize;

      for (int i = 0; i < sym_count; i++) {
        gelf_getsym(edata, i, &sym);
        if (sym.st_value != 0 && ELF32_ST_TYPE(sym.st_info) == STT_FUNC) {
          char *sym_name = elf_strptr(elf, shdr.sh_link, sym.st_name);
          assert(sym_name != NULL);
          function_watch_try_addp(self, sym_name, base_addr + sym.st_value);
        }
      }
    } // shdr.sh_type == SHT_SYMTAB
  } // while scn iterator
  return 0;
}

```

`kernel_sigaction.h`:

```h
/* This is the sigaction structure from the Linux 2.1.20 kernel.  */

#define HAVE_SA_RESTORER

struct old_kernel_sigaction {
	__sighandler_t k_sa_handler;
	unsigned long sa_mask;
	unsigned long sa_flags;
	void (*sa_restorer) (void);
};

/* This is the sigaction structure from the Linux 2.1.68 kernel.  */

struct kernel_sigaction {
	__sighandler_t k_sa_handler;
	unsigned long sa_flags;
	void (*sa_restorer) (void);
	sigset_t sa_mask;
};

```

`makefile`:

```
#PLUGINS+=plugins/branch_count.c
#PLUGINS+=plugins/soft_div.c
#PLUGINS+=plugins/tb_count.c
#PLUGINS+=plugins/mtrace.c plugins/mtrace.S
#PLUGINS+=plugins/cachesim/cachesim.c plugins/cachesim/cachesim.S plugins/cachesim/cachesim_model.c
#PLUGINS+=plugins/poc_log_returns.c
#PLUGINS+=plugins/instruction_mix.c
#PLUGINS+=plugins/strace.c
#PLUGINS+=plugins/symbol_example.c
#PLUGINS+=plugins/memcheck/memcheck.S plugins/memcheck/memcheck.c plugins/memcheck/naive_stdlib.c
#PLUGINS+=plugins/follow_exec.c

OPTS= -DDBM_LINK_UNCOND_IMM
OPTS+=-DDBM_INLINE_UNCOND_IMM
OPTS+=-DDBM_LINK_COND_IMM
OPTS+=-DDBM_LINK_CBZ
OPTS+=-DDBM_LINK_TBZ
OPTS+=-DDBM_TB_DIRECT #-DFAST_BT
OPTS+=-DLINK_BX_ALT
OPTS+=-DDBM_INLINE_HASH
OPTS+=-DDBM_TRACES #-DTB_AS_TRACE_HEAD #-DBLXI_AS_TRACE_HEAD
#OPTS+=-DCC_HUGETLB -DMETADATA_HUGETLB

VERSION?=$(shell git describe --abbrev=8 --dirty --always || echo '\<nogit\>')
CFLAGS+=-D_GNU_SOURCE -g -std=gnu99 -O2 -Wunused-variable
CFLAGS+=-DVERSION=\"$(VERSION)\"

LDFLAGS+=-static -ldl
LIBS=-lelf -lpthread -lz
HEADERS=*.h makefile
INCLUDES=-I/usr/include/libelf -I.
SOURCES= common.c dbm.c traces.c syscalls.c dispatcher.c signals.c util.S
SOURCES+=api/helpers.c api/plugin_support.c api/branch_decoder_support.c api/load_store.c api/internal.c api/hash_table.c
SOURCES+=elf/elf_loader.o elf/symbol_parser.o

ARCH=$(shell $(CC) -dumpmachine | awk -F '-' '{print $$1}')
ifeq ($(findstring arm, $(ARCH)), arm)
	CFLAGS += -march=armv7-a -mfpu=neon
	LDFLAGS += -Wl,-Ttext-segment=$(or $(TEXT_SEGMENT),0xa8000000)
	HEADERS += api/emit_arm.h api/emit_thumb.h
	PIE = pie/pie-arm-encoder.o pie/pie-arm-decoder.o pie/pie-arm-field-decoder.o
	PIE += pie/pie-thumb-encoder.o pie/pie-thumb-decoder.o pie/pie-thumb-field-decoder.o
	SOURCES += arch/aarch32/dispatcher_aarch32.S arch/aarch32/dispatcher_aarch32.c
	SOURCES += arch/aarch32/scanner_t32.c arch/aarch32/scanner_a32.c
	SOURCES += api/emit_arm.c api/emit_thumb.c
endif
ifeq ($(ARCH),aarch64)
	HEADERS += api/emit_a64.h
	LDFLAGS += -Wl,-Ttext-segment=$(or $(TEXT_SEGMENT),0x7000000000)
	PIE += pie/pie-a64-field-decoder.o pie/pie-a64-encoder.o pie/pie-a64-decoder.o
	SOURCES += arch/aarch64/dispatcher_aarch64.S arch/aarch64/dispatcher_aarch64.c
	SOURCES += arch/aarch64/scanner_a64.c
	SOURCES += api/emit_a64.c
endif

ifdef PLUGINS
	CFLAGS += -DPLUGINS_NEW
endif

.PHONY: pie clean cleanall

all:
	$(info MAMBO: detected architecture "$(ARCH)")
	@$(MAKE) --no-print-directory pie && $(MAKE) --no-print-directory $(or $(OUTPUT_FILE),dbm)

pie:
	@$(MAKE) --no-print-directory -C pie/ native

%.o: %.c %.h
	$(CC) $(CFLAGS) -c -o $@ $<

$(or $(OUTPUT_FILE),dbm): $(HEADERS) $(SOURCES) $(PLUGINS)
	$(CC) $(CFLAGS) $(LDFLAGS) $(OPTS) $(INCLUDES) -o $@ $(SOURCES) $(PLUGINS) $(PIE) $(LIBS) $(PLUGIN_ARGS)

cachesim:
	PLUGINS="plugins/cachesim/cachesim.c plugins/cachesim/cachesim.S plugins/cachesim/cachesim_model.c" OUTPUT_FILE=mambo_cachesim make

memcheck:
	PLUGINS="plugins/memcheck/memcheck.S plugins/memcheck/memcheck.c plugins/memcheck/naive_stdlib.c" OUTPUT_FILE=mambo_memcheck make

clean:
	rm -f dbm elf/elf_loader.o elf/symbol_parser.o

cleanall: clean
	$(MAKE) -C pie/ clean

api/emit_%.c: pie/pie-%-encoder.c api/generate_emit_wrapper.rb
	ruby api/generate_emit_wrapper.rb $< > $@

api/emit_%.h: pie/pie-%-encoder.c api/generate_emit_wrapper.rb
	ruby api/generate_emit_wrapper.rb $< header > $@

```

`plugins.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include "dbm.h"
#include "scanner_public.h"
#ifdef __arm__
  #include "api/emit_thumb.h"
  #include "api/emit_arm.h"
  #include "pie/pie-arm-field-decoder.h"
  #include "pie/pie-arm-decoder.h"
  #include "pie/pie-thumb-field-decoder.h"
  #include "pie/pie-thumb-decoder.h"
#elif __aarch64__
  #include "api/emit_a64.h"
  #include "pie/pie-a64-field-decoder.h"
  #include "pie/pie-a64-decoder.h"
#endif
#include "api/helpers.h"
#include "scanner_common.h"
#include "api/hash_table.h"

```

`plugins/branch_count.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW

#include <stdio.h>
#include <assert.h>
#include <locale.h>
#include <inttypes.h>
#include "../plugins.h"

struct br_count {
  uint64_t direct_branch_count;
  uint64_t indirect_branch_count;
  uint64_t return_branch_count;
};

struct br_count global_counters;

int branch_count_pre_thread_handler(mambo_context *ctx) {
  struct br_count *counters = mambo_alloc(ctx, sizeof(struct br_count));
  assert(counters != NULL);
  mambo_set_thread_plugin_data(ctx, counters);

  counters->direct_branch_count = 0;
  counters->indirect_branch_count = 0;
  counters->return_branch_count = 0;
}

void print_counters(struct br_count *counters) {
  fprintf(stderr, "  direct branches: %'" PRIu64 "\n", counters->direct_branch_count);
  fprintf(stderr, "  indirect branches: %'" PRIu64 "\n", counters->indirect_branch_count);
  fprintf(stderr, "  returns: %'" PRIu64 "\n", counters->return_branch_count);
}

int branch_count_post_thread_handler(mambo_context *ctx) {
  struct br_count *counters = mambo_get_thread_plugin_data(ctx);

  fprintf(stderr, "Thread: %d\n", mambo_get_thread_id(ctx));
  print_counters(counters);
  atomic_increment_u64(&global_counters.direct_branch_count,
                       counters->direct_branch_count);
  atomic_increment_u64(&global_counters.indirect_branch_count,
                       counters->indirect_branch_count);
  atomic_increment_u64(&global_counters.return_branch_count,
                       counters->return_branch_count);
  mambo_free(ctx, counters);
}

int branch_count_exit_handler(mambo_context *ctx) {
  fprintf(stderr, "Total:\n");
  print_counters(&global_counters);
}

int branch_count_pre_inst_handler(mambo_context *ctx) {
  struct br_count *counters = mambo_get_thread_plugin_data(ctx);
  uint64_t *counter = NULL;

  mambo_branch_type type = mambo_get_branch_type(ctx);
  if (type & BRANCH_RETURN) {
    counter = &counters->return_branch_count;
  } else if (type & BRANCH_DIRECT) {
    counter = &counters->direct_branch_count;
  } else if (type & BRANCH_INDIRECT) {
    counter = &counters->indirect_branch_count;
  }
 
  if (counter != NULL) {
    emit_counter64_incr(ctx, counter, 1);
  }
}

__attribute__((constructor)) void branch_count_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  mambo_register_pre_inst_cb(ctx, &branch_count_pre_inst_handler);
  mambo_register_pre_thread_cb(ctx, &branch_count_pre_thread_handler);
  mambo_register_post_thread_cb(ctx, &branch_count_post_thread_handler);
  mambo_register_exit_cb(ctx, &branch_count_exit_handler);
  
  setlocale(LC_NUMERIC, "");
}
#endif

```

`plugins/cachesim/README.md`:

```md
MAMBO cachesim
==============

This instrumentation plugin for [MAMBO](https://github.com/beehive-lab/mambo) is a configurable cache simulator with relatively low performance overhead. It is an online simulator, meaning that the simulation is done simultaneously with the execution of the application. You can use it, for example, to analyse the impact of various applications and workloads on the processor cache hierachy or for cache subsystem design space exploration. This is still experimental software, please report any problems using [github's issue tracker](https://github.com/beehive-lab/mambo/issues).


Publication:
------------

* [Cosmin Gorgovan, Guillermo Callaghan, and Mikel Luján. Balancing Performance and Productivity for the Development of Dynamic Binary Instrumentation Tools - A Case Study on Arm Systems. In Proceedings of the 29th International Conference on Compiler Construction (CC ’20)](https://dl.acm.org/doi/abs/10.1145/3377555.3377895) **Free download** [via research.manchester.ac.uk](https://www.research.manchester.ac.uk/portal/en/publications/balancing-performance-and-productivity-for-the-development-of-dynamic-binary-instrumentation-tools--a-case-study-on-arm-systems(80e57c1b-9e38-4a15-942d-eb240888b12b).html).


Building:
---------

    git clone --recurse-submodules https://github.com/beehive-lab/mambo.git
    cd mambo
    make cachesim


Usage:
------

To run an application under MAMBO cachesim, simply prefix the command with a call to `mambo_cachesim`. For example to execute `lscpu`, from the mambo source directory run:

    ./mambo_cachesim /usr/bin/lscpu
    
or
    
    ./mambo_cachesim `which lscpu`
    
When an application runs under MAMBO cachesim, the version information and the results of the simulation are printed just before the application exits, e.g.:

    $ ./mambo_cachesim `which lscpu`
    [...]
    L1i cache:           128 KiB
    L2 cache:            512 KiB
    Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32

    -- MAMBO cachesim 0f202444 --

    Cache L1i: 49,152 bytes, 64 byte lines, 3-way set-associative, LRU replacement policy

           3,375,828 references
           3,375,828 reads
                   0 writes
               3,413 misses total       (0.10% of references)
               3,413 misses reads       (0.10% of references)
                   0 misses writes      (0.00% of references)
                   0 writebacks total   (0.00% of references)
                   0 writebacks reads   (0.00% of references)
                   0 writebacks writes  (0.00% of references)

    Cache L1d: 32,768 bytes, 64 byte lines, 2-way set-associative, LRU replacement policy

           1,668,551 references
           1,366,619 reads
             301,932 writes
              93,455 misses total       (5.60% of references)
              89,540 misses reads       (5.37% of references)
               3,915 misses writes      (0.23% of references)
               8,492 writebacks total   (0.51% of references)
               7,530 writebacks reads   (0.45% of references)
                 962 writebacks writes  (0.06% of references)

    Cache L2: 1,048,576 bytes, 64 byte lines, 16-way set-associative, random replacement policy

              96,868 references
              92,953 reads
               3,915 writes
               7,267 misses total       (7.50% of references)
               4,898 misses reads       (5.06% of references)
               2,369 misses writes      (2.45% of references)
                 313 writebacks total   (0.32% of references)
                 200 writebacks reads   (0.21% of references)
                 113 writebacks writes  (0.12% of references)

    
Please include the git version in any bug reports.

You can also copy `mambo_cachesim` somewhere in your `PATH`, for example `/usr/local/bin`.


Configuration
-------------

At the moment, the configuration of the simulated cache hierachy is done through editing the [cachesim.c](cachesim.c) file and recompiling the code. The default configuration is for a modified Harvard architecture, with separate instruction and data L1 caches and a unified L2 cache. The parameters of each cache are set through the `L1I_*`, `L1D_*` and `L2_*` macros. The configurable parameters are: size, cache line length, associativity, and replacement policy (with LRU and random replacement policies implemented). We provide templates to simulate the cache hierarchy of various Arm Cortex-A cores. Furthermore, the cache hierarchy can be modified, for example to add more levels of caching, simply by instantiating and connecting additional cache models - see `cachesim_pre_thread_handler()`.

```

`plugins/cachesim/cachesim.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include "cachesim_buffer.h"

#ifdef __arm__
.syntax unified
#endif

.global cachesim_buf_write // (value, info, &buf_ptr.entries[0])
.func
.type mtrace_buf_write, %function

#ifdef __arm__
.thumb_func
cachesim_buf_write:
  PUSH {R3, R4}
  LDR R3, [R2, #-4]
  ADD R4, R2, R3, LSL #3
  STRD R0, R1, [R4]
  ADD R3, R3, #1
  STR R3, [R2, #-4]
  SUB R3, R3, #BUFLEN
  CBZ R3, call
  POP {R3, R4}
  BX LR

call:
  PUSH {R0-R2, R5-R6, R9, R12, LR}
  VPUSH {D16-D31}
  VPUSH {D0-D7}
  MRS R4, CPSR
  VMRS R5, FPSCR

  SUB R0, R2, #4

  MOV R6, SP
  BIC R2, R6, #7
  MOV SP, R2
  LDR R1, =cachesim_proc_buf
  BLX R1
  MOV SP, R6

  MSR CPSR, R4
  VMSR FPSCR, R5
  VPOP {D0-D7}
  VPOP {D16-D31}
  POP {R0-R2, R5-R6, R9, R12, LR}
  POP {R3 - R4}
  BX LR
#endif

#ifdef __aarch64__
cachesim_buf_write:
  STP X3, X4, [SP, #-16]!
  LDR W3, [X2, #-8]
  ADD X4, X2, W3, UXTW #4
  STP X0, X1, [X4]

  ADD W3, W3, #1
  STR W3, [X2, #-8]
  SUB W3, W3, #BUFLEN
  CBZ W3, call
  LDP X3, X4, [SP], #16
  RET

call:
  STP X29, X30, [SP, #-16]!

  BL push_x4_x21
  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR
  BL push_neon

  SUB X0, X2, #8
  LDR X1, =cachesim_proc_buf
  BLR X1

  BL pop_neon
  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21
  BL pop_x4_x21

  LDP X3, X4, [SP, #16]
  LDP X29, X30, [SP], #32
  RET
#endif

.endfunc

```

`plugins/cachesim/cachesim.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW

#include <stdio.h>
#include <assert.h>
#include <inttypes.h>
#include <locale.h>
#include "../../plugins.h"

#include "cachesim_buffer.h"
#include "cachesim_model.h"

// Instruction cache configurations

// Cortex-A5, Cortex-A7
/*
#define L1I_SIZE       (32 * 1024) // 4, 8, 16, 32 or 64 KiB for A5
                                   // 8, 16, 32 or 64 KiB for A7
#define L1I_LINE_SIZE  32
#define L1I_ASSOC      2
#define L1I_REPL       REPLACE_RANDOM
*/

// Cortex-A32, Cortex-A35
/*
#define L1I_SIZE       (32 * 1024) // 8, 16, 32 or 64 KiB for A32, A35
#define L1I_LINE_SIZE  64
#define L1I_ASSOC      2
#define L1I_REPL       REPLACE_RANDOM
*/

// Cortex-A9
/*
#define L1I_SIZE       (32 * 1024) // 16, 32 or 64 KiB for A9
#define L1I_LINE_SIZE  32
#define L1I_ASSOC      4
#define L1I_REPL       REPLACE_RANDOM
*/

// Cortex-A8, Cortex-A17
/*
#define L1I_SIZE       (32 * 1024) // 16 or 32 KiB for A8
                                   // 32 or 64 KiB for A17
#define L1I_LINE_SIZE  64
#define L1I_ASSOC      4
#define L1I_REPL       REPLACE_RANDOM
*/

// Cortex-A15
/*
#define L1I_SIZE       (32 * 1024)
#define L1I_LINE_SIZE  64
#define L1I_ASSOC      2
#define L1I_REPL       REPLACE_LRU
*/

// Cortex-A57, Cortex-A72
#define L1I_SIZE       (48 * 1024)
#define L1I_LINE_SIZE  64
#define L1I_ASSOC      3
#define L1I_REPL       REPLACE_LRU
#define L1I_MAX_FETCH  16

// Data cache configurations

// Cortex-A5, Cortex-A9
/*
#define L1D_SIZE       (32 * 1024) // 4, 8, 16, 32 or 64 KiB for A5
                                   // 16, 32 or 64 KiB for A9
#define L1D_LINE_SIZE  32
#define L1D_ASSOC      4
#define L1D_REPL       REPLACE_RANDOM
*/

// Cortex-A7, Cortex-A8, Cortex-A17, Cortex-A32, Cortex-A35, Cortex-A53
/*
#define L1D_SIZE       (32 * 1024) // 16, 32 or 64 KiB for A7
                                   // 16 or 32 KiB for A8
                                   // always 32 KiB for A17
                                   // 8, 16, 32 or 64 KiB for A32, A35, A53
#define L1D_LINE_SIZE  64
#define L1D_ASSOC      4
#define L1D_REPL       REPLACE_RANDOM
*/

// Cortex-A15, Cortex-A57, Cortex-A72
#define L1D_SIZE       (32 * 1024) // always 32 KiB
#define L1D_LINE_SIZE  64
#define L1D_ASSOC      2
#define L1D_REPL       REPLACE_LRU

// Denver 2
/*
#define L1D_SIZE       (64*1024)
#define L1D_LINE_SIZE  64
#define L1D_ASSOC      4
#define L1D_REPL       REPLACE_RANDOM //?
*/


//#define L2_SIZE       (256*1024)
//#define L2_SIZE       (512*1024)
#define L2_SIZE       (1*1024*1024)
//#define L2_SIZE       (2*1024*1024)
#define L2_LINE_SIZE  64
#define L2_ASSOC      16
#define L2_REPL       REPLACE_RANDOM

typedef struct {
  uintptr_t addr;
  uintptr_t info;
} cachesim_trace_entry_t;

typedef struct {
  uint32_t len;
  cachesim_trace_entry_t entries[BUFLEN];
  cachesim_model_t *model;
} cachesim_trace_t;

typedef struct {
  cachesim_trace_t inst_trace_buf;
  cachesim_model_t l1i_model;
  cachesim_trace_t data_trace_buf;
  cachesim_model_t l1d_model;
  void *set_inst_size;
  int fragment_size;
} cachesim_thread_t;

cachesim_model_t global_l1i;
cachesim_model_t global_l1d;
cachesim_model_t l2_model;

extern void cachesim_buf_write(uintptr_t value, cachesim_trace_t *trace);

void cachesim_proc_buf(cachesim_trace_t *trace_buf) {
  cachesim_model_t *model = trace_buf->model;
  unsigned len = trace_buf->len;
  for (int i = 0; i < len; i++) {
    cachesim_ref(model, trace_buf->entries[i].addr,
                 trace_buf->entries[i].info >> 1, trace_buf->entries[i].info & 1);
  }
  trace_buf->len = 0;
}

void inst_code(mambo_context *ctx, cachesim_thread_t *cachesim_thread) {
  emit_push(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << lr));

  void *addr = mambo_get_source_addr(ctx);
  assert(addr != NULL);
  emit_set_reg_ptr(ctx, 0, addr);

  cachesim_thread->set_inst_size = mambo_get_cc_addr(ctx);
  emit_set_reg(ctx, 1, 0);

  emit_set_reg_ptr(ctx, 2, &cachesim_thread->inst_trace_buf.entries);
  emit_fcall(ctx, cachesim_buf_write);

  emit_pop(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << lr));
}

void set_inst_size(mambo_context *ctx, cachesim_thread_t *cachesim_thread ) {
  void *tmp = mambo_get_cc_addr(ctx);
  mambo_set_cc_addr(ctx, cachesim_thread->set_inst_size);
  emit_set_reg(ctx, 1, cachesim_thread->fragment_size << 1);
  mambo_set_cc_addr(ctx, tmp);
}

int cachesim_pre_inst_handler(mambo_context *ctx) {
  cachesim_thread_t *cachesim_thread = mambo_get_thread_plugin_data(ctx);

  bool is_load = mambo_is_load(ctx);
  bool is_store = mambo_is_store(ctx);
  if (is_load || is_store) {
    mambo_cond cond = mambo_get_cond(ctx);
    mambo_branch skip_br;
    int ret;
    if (cond != AL) {
      ret = mambo_reserve_branch(ctx, &skip_br);
      assert(ret == 0);
    }

    emit_push(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << lr));    

    ret = mambo_calc_ld_st_addr(ctx, 0);
    assert(ret == 0);
    int size = mambo_get_ld_st_size(ctx);
    assert(size > 0);

    uintptr_t info = (size << 1) | (is_store ? 1 : 0);
    emit_set_reg(ctx, 1, info);
    emit_set_reg_ptr(ctx, 2, &cachesim_thread->data_trace_buf.entries);
    emit_fcall(ctx, cachesim_buf_write);

    emit_pop(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << lr));

    if (cond != AL) {
      ret = emit_local_branch_cond(ctx, &skip_br, invert_cond(cond));
      assert(ret == 0);
    }
  }

  // The maximum size we can set in one instruction
  if (cachesim_thread->fragment_size > (0x7FFF - 4)) {
    set_inst_size(ctx, cachesim_thread);
    inst_code(ctx, cachesim_thread);
    cachesim_thread->fragment_size = 0;
  }

  cachesim_thread->fragment_size += mambo_get_inst_len(ctx);
}

int cachesim_pre_bb_handler(mambo_context *ctx) {
  cachesim_thread_t *cachesim_thread = mambo_get_thread_plugin_data(ctx);

  cachesim_thread->fragment_size = 0;
  inst_code(ctx, cachesim_thread);
}

int cachesim_post_bb_handler(mambo_context *ctx) {
  cachesim_thread_t *cachesim_thread = mambo_get_thread_plugin_data(ctx);
  set_inst_size(ctx, cachesim_thread);
}

int cachesim_pre_thread_handler(mambo_context *ctx) {
  cachesim_thread_t *cachesim_thread = mambo_alloc(ctx, sizeof(*cachesim_thread));
  assert(cachesim_thread != NULL);

  int ret = cachesim_model_init(&cachesim_thread->l1i_model, "L1i", L1I_SIZE,
                                L1I_LINE_SIZE, L1I_MAX_FETCH, L1I_ASSOC, L1I_REPL);
  assert(ret == 0);
  cachesim_thread->l1i_model.parent = &l2_model;
  cachesim_thread->inst_trace_buf.model = &cachesim_thread->l1i_model;

  ret = cachesim_model_init(&cachesim_thread->l1d_model, "L1d", L1D_SIZE,
                                L1D_LINE_SIZE, 0, L1D_ASSOC, L1D_REPL);
  assert(ret == 0);
  cachesim_thread->l1d_model.parent = &l2_model;
  cachesim_thread->data_trace_buf.model = &cachesim_thread->l1d_model;

  cachesim_thread->inst_trace_buf.len = 0;
  cachesim_thread->data_trace_buf.len = 0;

  ret = mambo_set_thread_plugin_data(ctx, cachesim_thread);
  assert(ret == MAMBO_SUCCESS);
}

int cachesim_post_thread_handler(mambo_context *ctx) {
  cachesim_thread_t *cachesim_thread = mambo_get_thread_plugin_data(ctx);
  cachesim_proc_buf(&cachesim_thread->data_trace_buf);
  cachesim_proc_buf(&cachesim_thread->inst_trace_buf);

  for (int i = 0; i < 2; i++) {
    atomic_increment_u64(&global_l1i.stats.references[i],
                         cachesim_thread->l1i_model.stats.references[i]);
    atomic_increment_u64(&global_l1i.stats.misses[i],
                         cachesim_thread->l1i_model.stats.misses[i]);
    atomic_increment_u64(&global_l1i.stats.writebacks[i],
                         cachesim_thread->l1i_model.stats.writebacks[i]);

    atomic_increment_u64(&global_l1d.stats.references[i],
                         cachesim_thread->l1d_model.stats.references[i]);
    atomic_increment_u64(&global_l1d.stats.misses[i],
                         cachesim_thread->l1d_model.stats.misses[i]);
    atomic_increment_u64(&global_l1d.stats.writebacks[i],
                         cachesim_thread->l1d_model.stats.writebacks[i]);
  }

  cachesim_model_free(&cachesim_thread->l1i_model);
  cachesim_model_free(&cachesim_thread->l1d_model);
  mambo_free(ctx, cachesim_thread);
}

int cachesim_exit_handler(mambo_context *ctx) {
  printf("\n-- MAMBO cachesim " VERSION " --\n\n");
  cachesim_print_stats(&global_l1i);
  cachesim_print_stats(&global_l1d);
  cachesim_print_stats(&l2_model);
}

__attribute__((constructor)) void cachesim_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  // These L1 models aren't used, they just store the global L1 stats and configuration
  int ret = cachesim_model_init(&global_l1i, "L1i", L1I_SIZE,
                                L1I_LINE_SIZE, L1I_MAX_FETCH, L1I_ASSOC, L1I_REPL);
  assert(ret == 0);
  ret = cachesim_model_init(&global_l1d, "L1d", L1D_SIZE,
                            L1D_LINE_SIZE, 0, L1D_ASSOC, L1D_REPL);
  assert(ret == 0);

  ret = cachesim_model_init(&l2_model, "L2", L2_SIZE,
                            L2_LINE_SIZE, 0, L2_ASSOC, L2_REPL);
  assert(ret == 0);

  mambo_register_pre_thread_cb(ctx, &cachesim_pre_thread_handler);
  mambo_register_post_thread_cb(ctx, &cachesim_post_thread_handler);
  mambo_register_pre_inst_cb(ctx, &cachesim_pre_inst_handler);
  mambo_register_exit_cb(ctx, &cachesim_exit_handler);
  mambo_register_pre_basic_block_cb(ctx, &cachesim_pre_bb_handler);
  mambo_register_post_basic_block_cb(ctx, &cachesim_post_bb_handler);
}
#endif

```

`plugins/cachesim/cachesim_buffer.h`:

```h
#define BUFLEN 256

```

`plugins/cachesim/cachesim_model.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdlib.h>
#include <stdio.h>
#include <inttypes.h>
#include <string.h>
#include <locale.h>
#include <assert.h>
#include <pthread.h>

#include "cachesim_model.h"

#define READ_INDEX 0
#define WRITE_INDEX 1

#define IS_DIRTY 1

static inline bool is_pow2(unsigned int val) {
  return (val & (val -1)) == 0;
}

__attribute__((always_inline))
static inline int cachesim_get_set(cachesim_model_t *cache, addr_t addr) {
  addr_t set = addr >> cache->set_shift;
  return (int)(set & cache->set_mask);
}

__attribute__((always_inline))
static inline addr_t cachesim_get_tag(cachesim_model_t *cache, addr_t addr) {
  return addr >> cache->tag_shift;
}

int cachesim_model_init(cachesim_model_t *cache, char *name, unsigned size,
                        unsigned line_size, unsigned max_fetch, unsigned assoc,
                        cachesim_policy repl_policy) {
  if (size == 0 || line_size == 0 || assoc == 0 ||
      !is_pow2(line_size) || !is_pow2(max_fetch) ||
      (assoc * line_size) > size ||
      (size % (line_size * assoc)) != 0) {
    return -1;
  }

  unsigned sets = cache->sets = size / (line_size * assoc);
  if (!is_pow2(sets)) {
    return -1;
  }

  int ret = pthread_mutex_init(&cache->mutex, NULL);
  if (ret != 0) {
    return -1;
  }

  cache->lines = calloc(sets * assoc, sizeof(cachesim_model_line_t));
  if (cache->lines == NULL) {
    return -1;
  }

  cache->max_fetch = (max_fetch != 0) ? max_fetch : line_size;

  strncpy(cache->name, name, CACHESIM_NAME_LEN);
  cache->name[CACHESIM_NAME_LEN - 1] = '\0';

  cache->size = size;
  cache->line_size = line_size;
  cache->assoc = assoc;
  cache->replacement_policy = repl_policy;

  cache->sets = sets;
  cache->set_shift = __builtin_ctz(line_size);
  cache->set_mask = cache->sets - 1;
  cache->tag_shift = __builtin_ctz(cache->sets) + cache->set_shift;
  cache->max_fetch_shift = __builtin_ctz(cache->max_fetch);

  memset(&cache->stats, 0, sizeof(cache->stats));

  return 0;
}

void cachesim_model_free(cachesim_model_t *cache) {
  if (cache->lines) {
    free(cache->lines);
  }

  pthread_mutex_destroy(&cache->mutex);
}

int cachesim_lock(cachesim_model_t *cache) {
  return pthread_mutex_lock(&cache->mutex);
}

int cachesim_unlock(cachesim_model_t *cache) {
  return pthread_mutex_unlock(&cache->mutex);
}

__attribute__((always_inline))
static inline void cachesim_load_line(cachesim_model_t *cache, int line_index, addr_t addr, bool is_write) {
  if (cache->lines[line_index].tag & 1) {
    int counter_index = is_write ? 1 : 0;
    cache->stats.writebacks[counter_index]++;
  }
  cache->lines[line_index].tag = cachesim_get_tag(cache, addr) << 1;
}

__attribute__((always_inline))
static inline
int cachesim_evict_line(cachesim_model_t *cache, int line_index) {
  int line = -1;

  switch (cache->replacement_policy) {
    case REPLACE_RANDOM:
      line = random() % cache->assoc;
      break;
    case REPLACE_LRU: {
      uint64_t min_timestamp = -1;
      for (int i = 0; i < cache->assoc; i++) {
        if (cache->lines[line_index + i].timestamp < min_timestamp) {
          min_timestamp = cache->lines[line_index + i].timestamp;
          line = i;
        }
      }
      break;
    }
    default:
      printf("Unimplemented cache replacement policy %d\n", cache->replacement_policy);
      exit(EXIT_FAILURE);
  }

  return line;
}

__attribute__((always_inline))
static inline void update_dirty_bit(cachesim_model_t *cache, int line, bool is_write) {
  if (is_write) {
    cache->lines[line].tag |= IS_DIRTY;
  }
}

__attribute__((always_inline))
static inline void update_line(cachesim_model_t *cache, int line, bool is_write) {
  update_dirty_bit(cache, line, is_write);
  cache->lines[line].timestamp = cache->stats.references[0] + cache->stats.references[1];
}

int cachesim_ref(cachesim_model_t *cache, addr_t addr, unsigned size, bool is_write) {
  int counter_index = is_write ? 1 : 0;
  addr_t end = addr + size;

  unsigned mask = cache->max_fetch - 1;
  unsigned offset = (unsigned)addr & mask;
  unsigned t_size = size + offset;
  cache->stats.references[counter_index] += t_size >> cache->max_fetch_shift;
  cache->stats.references[counter_index] += (t_size & mask) ? 1 : 0;

  addr = (addr >> cache->set_shift) << cache->set_shift;
  int line;

  for (; addr < end; addr += cache->line_size) {
    /* If it's the same as the previously used address, we have its cache line cached.
       Also, we don't have to update the timestamp since it's already the highest one
       in the cache. */
    if (addr == cache->last_addr) {
      line = cache->last_line;
      update_dirty_bit(cache, line, is_write);
      continue;
    }

    line = cachesim_get_set(cache, addr) * cache->assoc;
    addr_t tag = cachesim_get_tag(cache, addr);
    bool hit = false;

    /* We keep the MRU tag in the first position of the line.
       We can also avoid updating its timestamp because it's the highest one in this line */
    if ((cache->lines[line].tag >> 1) == tag) {
      update_dirty_bit(cache, line, is_write);
      continue;
    }

    for (int i = 1; i < cache->assoc && !hit; i++) {
      if ((cache->lines[line + i].tag >> 1) == tag) {
        // Move the tag to the first position on the line
        cachesim_model_line_t tmp = cache->lines[line];
        cache->lines[line] = cache->lines[line+i];
        cache->lines[line+i] = tmp;

        hit = true;
      }
    }

    // Miss
    if (!hit) {
      cache->stats.misses[counter_index]++;

      if (cache->parent) {
        int ret = cachesim_lock(cache->parent);
        assert(ret == 0);
        cachesim_ref(cache->parent, addr, cache->line_size, is_write);
        ret = cachesim_unlock(cache->parent);
        assert(ret == 0);
      }

      line += cachesim_evict_line(cache, line);
      cachesim_load_line(cache, line, addr, is_write);
    }

    update_line(cache, line, is_write);
  }

  cache->last_addr = addr;
  cache->last_line = line;

  return 0;
}

void cachesim_print_stats(cachesim_model_t *cache) {
  uint64_t references = cache->stats.references[READ_INDEX] + cache->stats.references[WRITE_INDEX];
  uint64_t misses = cache->stats.misses[READ_INDEX] + cache->stats.misses[WRITE_INDEX];
  uint64_t writebacks = cache->stats.writebacks[READ_INDEX] + cache->stats.writebacks[WRITE_INDEX];
  float rate;
  char *repl;

  switch (cache->replacement_policy) {
    case REPLACE_RANDOM:
      repl = "random";
      break;
    case REPLACE_LRU:
      repl = "LRU";
      break;
  }

  setlocale(LC_NUMERIC, "");

  printf("Cache %s: %'d bytes, %d byte lines, %d-way set-associative, %s replacement policy\n\n",
         cache->name, cache->size, cache->line_size, cache->assoc, repl);
  printf("%'16" PRIu64 " references\n", references);
  printf("%'16" PRIu64 " reads\n", cache->stats.references[READ_INDEX]);
  printf("%'16" PRIu64 " writes\n", cache->stats.references[WRITE_INDEX]);

  rate = (float)misses / (float)references;
  printf("%'16" PRIu64 " misses total       (%.2f%% of references)\n",
         misses, rate * 100.0);
  rate = (float)cache->stats.misses[READ_INDEX] / (float)references;
  printf("%'16" PRIu64 " misses reads       (%.2f%% of references)\n",
         cache->stats.misses[READ_INDEX], rate * 100.0);
  rate = (float)cache->stats.misses[WRITE_INDEX] / (float)references;
  printf("%'16" PRIu64 " misses writes      (%.2f%% of references)\n",
         cache->stats.misses[WRITE_INDEX], rate * 100.0);

  rate = (float)writebacks / (float)references;
  printf("%'16" PRIu64 " writebacks total   (%.2f%% of references)\n",
         writebacks, rate * 100.0);
  rate = (float)cache->stats.writebacks[READ_INDEX] / (float)references;
  printf("%'16" PRIu64 " writebacks reads   (%.2f%% of references)\n",
         cache->stats.writebacks[READ_INDEX], rate * 100.0);
  rate = (float)cache->stats.writebacks[WRITE_INDEX] / (float)references;
  printf("%'16" PRIu64 " writebacks writes  (%.2f%% of references)\n\n",
         cache->stats.writebacks[WRITE_INDEX], rate * 100.0);
}

```

`plugins/cachesim/cachesim_model.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdint.h>
#include <stdbool.h>


typedef uintptr_t addr_t;

typedef enum {
  REPLACE_RANDOM,
  REPLACE_LRU,
} cachesim_policy;

typedef struct {
  addr_t tag;
  uint64_t timestamp;
} cachesim_model_line_t;

typedef struct {
  uint64_t references[2];
  uint64_t misses[2];
  uint64_t writebacks[2];
} cachesim_stats_t;

typedef struct cachesim_model cachesim_model_t;
#define CACHESIM_NAME_LEN 20
struct cachesim_model {
  char name[CACHESIM_NAME_LEN];
  unsigned size;
  unsigned line_size;
  unsigned max_fetch;
  unsigned assoc;
  cachesim_policy replacement_policy;

  unsigned sets;
  unsigned set_shift;
  unsigned set_mask;
  unsigned tag_shift;
  unsigned max_fetch_shift;

  addr_t last_addr;
  int last_line;

  pthread_mutex_t mutex;

  cachesim_model_t *parent;
  cachesim_stats_t stats;
  cachesim_model_line_t *lines;
};

int cachesim_model_init(cachesim_model_t *cache, char *name, unsigned size,
                        unsigned line_size, unsigned max_fetch, unsigned assoc,
                        cachesim_policy repl_policy);
void cachesim_model_free(cachesim_model_t *cache);
int cachesim_ref(cachesim_model_t *cache, addr_t addr, unsigned size, bool is_write);
void cachesim_print_stats(cachesim_model_t *cache);

```

`plugins/follow_exec.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2021 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

/*
  This plugin will make MAMBO 'follow' into any new process started by the
  application under its control by prepending a call to itself on every execve
*/

#ifdef PLUGINS_NEW

//#include <stdio.h>
#include <assert.h>
#include <sys/syscall.h>
#include <string.h>
#include <unistd.h>
#include <errno.h>

#include "../plugins.h"

char self_exe[NAME_MAX];

int count_args(uintptr_t *args) {
  // count includes the NULL terminator
  int count = 1;
  while ((*args) != 0) {
    count++;
    args++;
  }
  return count;
}

int follow_exec_syscall(mambo_context *ctx) {
  uintptr_t call_no;
  uintptr_t *args;
  int ret = mambo_syscall_get_no(ctx, &call_no);
  assert(ret == 0);

  if (call_no == __NR_execve) {
    mambo_syscall_get_args(ctx, &args);
    assert(args != NULL);

    mambo_syscall_bypass(ctx);

    /*
      We have to do some error checking here to emulate execve errors.
      If we just try to launch a missing or invalid executable with MAMBO,
      we'll only encounter an error after the parent has been replaced by
      the newly launched MAMBO process, and at that point there's no parent
      to return the error to
    */
    if (access((char *)args[0], F_OK)) {
      mambo_syscall_set_return(ctx, -ENOENT);
      return 0;
    }

    // copy argv
    int arg_count = count_args((uintptr_t *)args[1]);
    uintptr_t *tmp_argv = alloca((arg_count+1) * sizeof(uintptr_t));
    memcpy(&tmp_argv[1], (uintptr_t *)args[1], arg_count * sizeof(uintptr_t));

    // set the original path as the first argument for MAMBO
    tmp_argv[1] = args[0];
    // prepend the path of our own executable to MAMBO's argv
    tmp_argv[0] = (uintptr_t)self_exe;

    uintptr_t ret = raw_syscall(call_no, self_exe, tmp_argv, args[2]);
    /* Normally we shouldn't return here. If we did, it means that
       MAMBO failed to start, rather than the application. */
    mambo_syscall_set_return(ctx, ret);
  }

  return 0;
}

__attribute__((constructor)) void follow_exec_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  ssize_t ret = readlink("/proc/self/exe", self_exe, NAME_MAX);
  assert(ret > 0 && ret < NAME_MAX);
  self_exe[ret] = '\0';

  mambo_register_pre_syscall_cb(ctx, &follow_exec_syscall);
}
#endif

```

`plugins/instruction_mix.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2018 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2018 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW

// Uncomment to count prefetch instructions
// #define COUNT_PRFM

#include <stdio.h>
#include <assert.h>
#include <locale.h>
#include <inttypes.h>
#include "../plugins.h"

struct instructions {
  uint64_t integer;
  uint64_t floating;
  uint64_t load;
  uint64_t store;
  uint64_t branch;
#ifdef COUNT_PRFM
  uint64_t prefetch;
#endif
};

struct instructions global_counters = {0};

// Callback function prototypes
int instruction_count_pre_thread_handler(mambo_context *ctx);
int instruction_count_pre_inst_handler(mambo_context *ctx);
int instruction_count_post_thread_handler(mambo_context *ctx);
int instruction_count_exit_handler(mambo_context *ctx);

// Auxiliary function to print the counters
void print_counters(struct instructions *counters);

// Plugin registration and event callbacks function
__attribute__((constructor)) void branch_count_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  mambo_register_pre_thread_cb(ctx, &instruction_count_pre_thread_handler);
  mambo_register_pre_inst_cb(ctx, &instruction_count_pre_inst_handler);
  mambo_register_post_thread_cb(ctx, &instruction_count_post_thread_handler);
  mambo_register_exit_cb(ctx, &instruction_count_exit_handler);
}

int instruction_count_pre_thread_handler(mambo_context *ctx) {
  // Thread private counters initialisation
  struct instructions *counters = mambo_alloc(ctx, sizeof(struct instructions));
  assert(counters != NULL);
  mambo_set_thread_plugin_data(ctx, counters);

  counters->integer = 0;
  counters->floating = 0;
  counters->load = 0;
  counters->store = 0;
  counters->branch = 0;
#ifdef COUNT_PRFM
  counters->prefetch = 0;
#endif
}

int instruction_count_post_thread_handler(mambo_context *ctx) {
  // On thread exit, the counters are added to the global counters
  struct instructions *counters = mambo_get_thread_plugin_data(ctx);

  fprintf(stderr, "Thread: %d\n", mambo_get_thread_id(ctx));

  // Prints thread private counters
  print_counters(counters); // comment this out if not needed

  atomic_increment_u64(&global_counters.integer, counters->integer);
  atomic_increment_u64(&global_counters.floating, counters->floating);
  atomic_increment_u64(&global_counters.load, counters->load);
  atomic_increment_u64(&global_counters.store, counters->store);
  atomic_increment_u64(&global_counters.branch, counters->branch);
#ifdef COUNT_PRFM
  atomic_increment_u64(&global_counters.prefetch, counters->prefetch);
#endif
  mambo_free(ctx, counters);
}

int instruction_count_pre_inst_handler(mambo_context *ctx) {
  struct instructions *counters = mambo_get_thread_plugin_data(ctx);
  uint64_t *inst_counter = NULL;

#ifdef __aarch64__
  // Variables are used for decoding the fields of intructions
	uint32_t sf, rm, opcode, rn, rd;

  // Selects the appropriate counter according to the instruction type
  switch (ctx->code.inst) {
  // Branches, Exception Generating and System instructions Category
      // * Branches instructions
  case A64_B_BL:
  case A64_B_COND:
  case A64_CBZ_CBNZ:
  case A64_TBZ_TBNZ:
  case A64_BR:
  case A64_BLR:
  case A64_RET:
    inst_counter = &counters->branch;
    break;

      // * Exception Generating
  case A64_SVC:
  case A64_HVC:
  case A64_BRK:
    break;

      // * System instructions
  case A64_SYS:
  case A64_MRS_MSR_REG:
  case A64_HINT:
  case A64_DSB:
  case A64_DMB:
  case A64_ISB:
  case A64_CLREX:
    break;

  // Data Processing -- Immediate
  case A64_ADD_SUB_IMMED:
  case A64_LOGICAL_IMMED:
  case A64_BFM:
  case A64_ADR:
  case A64_EXTR:
  case A64_MOV_WIDE:
    inst_counter = &counters->integer;
    break;

  // Loads and Stores
  case A64_LDR_LIT:
  case A64_LDX_STX:
  case A64_LDP_STP:
  case A64_LDR_STR_IMMED:
  case A64_LDR_STR_REG:
  case A64_LDR_STR_UNSIGNED_IMMED:
  case A64_LDX_STX_MULTIPLE:
  case A64_LDX_STX_MULTIPLE_POST:
  case A64_LDX_STX_SINGLE:
  case A64_LDX_STX_SINGLE_POST:
    if (mambo_is_load(ctx)) {
      inst_counter = &counters->load;
    } else if (mambo_is_store(ctx)) {
      inst_counter = &counters->store;
    } else {
#ifdef COUNT_PRFM
      inst_counter = &counters->prefetch;
#endif
    }
    break;

  // Data Processing -- Register
  case A64_ADD_SUB_EXT_REG:
  case A64_ADD_SUB_SHIFT_REG:
  case A64_ADC_SBC:
    inst_counter = &counters->integer;
    break;

  case A64_DATA_PROC_REG1:
  case A64_CCMP_CCMN_IMMED:
  case A64_CCMP_CCMN_REG:
  case A64_COND_SELECT:
  case A64_LOGICAL_REG:
  case A64_DATA_PROC_REG3:
    inst_counter = &counters->integer;
    break;

  case A64_DATA_PROC_REG2:
    a64_data_proc_reg2_decode_fields(ctx->code.read_address, &sf, &rm, &opcode, &rn, &rd);
    if ((opcode == 2) || (opcode == 3)) { // UDIV or SDIV
      inst_counter = &counters->integer;
    }
    break;


  // Data Processing -- Scalar Floating-Point and Advanced SIMD
    // * Floating point instructions
  case A64_FCMP:
  case A64_FCCMP:
  case A64_FCSEL:
  case A64_FLOAT_REG1:
  case A64_FLOAT_REG2:
  case A64_FLOAT_REG3:
  case A64_FMOV_IMMED:
  case A64_FLOAT_CVT_FIXED:
  case A64_FLOAT_CVT_INT:
    inst_counter = &counters->floating;
    break;

    // *SIMD
  case A64_SIMD_ACROSS_LANE:
  case A64_SIMD_COPY:
  case A64_SIMD_EXTRACT:
  case A64_SIMD_MODIFIED_IMMED:
  case A64_SIMD_PERMUTE:
  case A64_SIMD_SCALAR_COPY:
  case A64_SIMD_SCALAR_PAIRWISE:
  case A64_SIMD_SCALAR_SHIFT_IMMED:
  case A64_SIMD_SCALAR_THREE_DIFF:
  case A64_SIMD_SCALAR_THREE_SAME:
  case A64_SIMD_SHIFT_IMMED:
  case A64_SIMD_TABLE_LOOKUP:
  case A64_SIMD_THREE_DIFF:
  case A64_SIMD_THREE_SAME:
  case A64_SIMD_SCALAR_TWO_REG:
  case A64_SIMD_SCALAR_X_INDEXED:
  case A64_SIMD_TWO_REG:
  case A64_SIMD_X_INDEXED:
  case A64_CRYPTO_AES:
  case A64_CRYPTO_SHA_REG3:
  case A64_CRYPTO_SHA_REG2:
    break;

  default:
    break;
  }

#else
  #error Unsupported architecture
#endif
  if (inst_counter != NULL) {
    emit_counter64_incr(ctx, inst_counter, 1);
  }
}

int instruction_count_exit_handler(mambo_context *ctx) {
  // On application exit prints the global counters
  fprintf(stderr, "Total:\n");
  print_counters(&global_counters);
}

void print_counters(struct instructions *counters) {
  // Auxiliary function to print the counters
  fprintf(stderr, "  integer : %'" PRIu64 "\n", counters->integer);
  fprintf(stderr, "  floating: %'" PRIu64 "\n", counters->floating);
  fprintf(stderr, "  load    : %'" PRIu64 "\n", counters->load);
  fprintf(stderr, "  store   : %'" PRIu64 "\n", counters->store);
  fprintf(stderr, "  branch  : %'" PRIu64 "\n", counters->branch);
#ifdef COUNT_PRFM
  fprintf(stderr, "  prefetch: %'" PRIu64 "\n", counters->prefetch);
#endif
}
#endif

```

`plugins/memcheck/README.md`:

```md
MAMBO memcheck
==============

This instrumentation plugin for [MAMBO](https://github.com/beehive-lab/mambo) detects memory usage errors such as out-of-bounds accesses and invalid `free()` calls with relatively low performance overhead. This is still experimental software, please report any problems using [github's issue tracker](https://github.com/beehive-lab/mambo/issues).


Publications
------------

* [Cosmin Gorgovan, Guillermo Callaghan, and Mikel Luján. Balancing Performance and Productivity for the Development of Dynamic Binary Instrumentation Tools - A Case Study on Arm Systems. In Proceedings of the 29th International Conference on Compiler Construction (CC ’20)](https://dl.acm.org/doi/abs/10.1145/3377555.3377895) **Free download** [via research.manchester.ac.uk](https://www.research.manchester.ac.uk/portal/en/publications/balancing-performance-and-productivity-for-the-development-of-dynamic-binary-instrumentation-tools--a-case-study-on-arm-systems(80e57c1b-9e38-4a15-942d-eb240888b12b).html).


Building
--------

    git clone --recurse-submodules https://github.com/beehive-lab/mambo.git
    cd mambo
    make memcheck


Usage
-----

To run an application under MAMBO memcheck, simply prefix the command with a call to `mambo_memcheck`. For example to execute `lscpu`, from the mambo source directory run:

    ./mambo_memcheck /usr/bin/lscpu

or

    ./mambo_memcheck `which lscpu`

When an application runs under MAMBO memcheck, the first output should be its git version, e.g.:

    $ ./mambo_memcheck `which lscpu`

    -- MAMBO memcheck 29f87421 --

    Architecture:        aarch64
    CPU op-mode(s):      32-bit, 64-bit
    [...]

Please include the git version in any bug reports.

You can also copy `mambo_memcheck` somewhere in your `PATH`, for example `/usr/local/bin`.


Example output from a buggy application
---------------------------------------

    $ mambo_memcheck ~/test

    -- MAMBO memcheck 29f87421 --

    ==memcheck== Invalid store (size 4) to 0x3ffce462c8
    ==memcheck==  at [main]+0x60 (0x3ffffac978) in /home/cosmin/test
    ==memcheck==  Backtrace:
    ==memcheck==  at [__libc_start_main]+0xe4 (0x3ffd06c12c) in /usr/lib/libc-2.30.so
    ==memcheck==  at [(null)]+0x7e4 (0x3ffffac7e4) in /home/cosmin/test

    ==memcheck== Invalid load (size 4) from 0x3ffce462cc
    ==memcheck==  at [main]+0x80 (0x3ffffac998) in /home/cosmin/test
    ==memcheck==  Backtrace:
    ==memcheck==  at [__libc_start_main]+0xe4 (0x3ffd06c12c) in /usr/lib/libc-2.30.so
    ==memcheck==  at [(null)]+0x7e4 (0x3ffffac7e4) in /home/cosmin/test

    ==memcheck== double free for 0x3ffce466e0


Advanced configuration
----------------------

One of the more challenging aspects of this software is avoiding noisy false positive errors, e.g. harmless out-of-bounds reads in the hand written assembly code from glibc. We have implemented a number of techniques to avoid reporting such errors, which are documented and can be enabled or disabled in [memcheck.h](memcheck.h).

```

`plugins/memcheck/memcheck.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include "memcheck.h"

#ifdef __arm__
.syntax unified
#endif

// 1 to 3 arguments need to be preserved
// (es-1) (X18/R3) is a pointer to <in_malloc>
.global memcheck_malloc_pre
.func
memcheck_malloc_pre:
#ifdef __aarch64__
  LDR X4, [X18]
  ADD X4, X4, #1
  STR X4, [X18]
  RET
#elif __arm__
  LDR R12, [R3]
  ADD R12, R12, #1
  STR R12, [R3]
  BX LR
#endif
.endfunc


// X0/R0 - address
// (es-1) (X18/R3) is a pointer to <in_malloc>
// (es) (X19/R4) - size
.global memcheck_malloc_post
.func
memcheck_malloc_post:
#ifdef __aarch64__
  LDR X2, [X18]
  SUB X2, X2, #1
  STR X2, [X18]

  CBZ X0, mmp_ret

  #ifdef COMPACT_SHADOW
  STP X0, X30, [SP, #-16]!
  MOV X1, X19
  BL memcheck_alloc_hook
  LDP X0, X30, [SP], #16
  #else
  MOV X1, #0x200000000
  STR X19, [X1, X0]
  STR X19, [X0]

  MOV W4, #1
  MOV X2, #0x100000000
  MOV X3, X19
  ADD X2, X0, X2

mmp_loop:
  CBZ X3, mmp_ret
  STRB W4, [X2], #1
  SUB X3, X3, #1
  B mmp_loop
#endif

mmp_ret:
  RET

#elif __arm__
  LDR R2, [R3]
  SUB R2, R2, #1
  STR R2, [R3]

  CMP R0, #0
  BEQ mmp_ret

  PUSH {R0, LR}
  MOV R1, R4
  BL memcheck_alloc_hook
  POP {R0, LR}

mmp_ret:
  BX LR
#endif
.endfunc


// X0/R0 - address
// (es-1) (X18/R3) is a pointer to <in_malloc>
.global memcheck_free_pre
.func
memcheck_free_pre:
#ifdef __aarch64__
  LDR X3, [X18]
  ADD X3, X3, #1
  STR X3, [X18]

  #ifdef COMPACT_SHADOW
  STR X0, [SP, #-32]!
  STP X1, X30, [SP, #16]
  BL memcheck_free_hook
  LDP X1, X30, [SP, #16]
  LDR X0, [SP], #32
  #else
  MOV X2, #0x200000000
  LDR X2, [X2, X0]

  MOV X3, #0x100000000
  ADD X3, X3, X0

mfp_l:
  CBZ X2, mfp_l_exit
  STRB WZR, [X3], #1
  SUB X2, X2, #1
  B mfp_l

mfp_l_exit:
  #endif
  RET

#elif __arm__
  LDR R2, [R3]
  ADD R2, R2, #1
  STR R2, [R3]

  /* We don't really need to preserve the value of R2 here,
     but by pushing it we maintain stack alignment */
  PUSH {R0-R2, LR}
  BL memcheck_free_hook
  POP {R0-R2, PC}
#endif
.endfunc


// (es-1) (X18/R3) is a pointer to <in_malloc>
.global memcheck_free_post
.func
memcheck_free_post:
#ifdef __aarch64__
  LDR X2, [X18]
  SUB X2, X2, #1
  STR X2, [X18]
  RET
#elif __arm__
  LDR R2, [R3]
  SUB R2, R2, #1
  STR R2, [R3]
  BX LR
#endif
.endfunc


// X0/R0 - access address
// X1/R1 - access size | IS_STORE
// X2/R2 - SPC
// X3/R3 is a pointer to <in_malloc>
// X4/R4 and LR are also pushed
.global memcheck_unalloc
.func
#ifdef __arm__
.thumb_func
#endif
memcheck_unalloc:
#ifdef __aarch64__
  LDR X3, [X3]
  CBNZ X3, skip_err

  STR X30, [SP, #-16]!

  BL push_x4_x21
  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR
  BL push_neon

  MOV X3, X29 // frame pointer
  BL memcheck_print_error

  BL pop_neon
  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21
  BL pop_x4_x21

  LDR X30, [SP], #16

skip_err:
  RET

#elif __arm__
  LDR R3, [R3]
  CBNZ R3, skip_err

  PUSH {R5-R6, R9, R12, LR}
  VPUSH {d16-d31}
  VPUSH {d0-d7}

  MRS r4, CPSR
  VMRS r5, FPSCR

  // align the SP
  MOV R6, SP
  BIC R3, R6, #7
  MOV SP, R3

  MOV R3, R11 // frame pointer
  BL memcheck_print_error

  MOV SP, R6

  VMSR FPSCR, r5
  MSR CPSR, r4

  VPOP {d0-d7}
  VPOP {d16-d31}
  POP {R5-R6, R9, R12, LR}

skip_err:
  BX LR
#endif
.endfunc


.global memcheck_ret
.func
memcheck_ret:
#ifdef __aarch64__
  RET
#elif __arm__
  BX LR
#endif

```

`plugins/memcheck/memcheck.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

// On AArch64, compile with TEXT_SEGMENT = 0x7000000000

#include <sys/mman.h>
#include <assert.h>
#include <dlfcn.h>
#include <stdio.h>
#include <string.h>
#include <sys/auxv.h>
#include <unistd.h>

#include "../../plugins.h"

#include "memcheck.h"

extern void memcheck_install_naive_stdlib(mambo_context *ctx);


#define IS_STORE (1 << 15)

#define MAGIC_FREED (UINTPTR_MAX-1)

void *shadow_mem = NULL;
void *loader_base = NULL;

extern void memcheck_malloc_pre();
extern void memcheck_malloc_post();
extern void memcheck_free_pre();
extern void memcheck_free_post();
extern void memcheck_unalloc();

mambo_ht_t allocs;

typedef struct {
  uintptr_t in_malloc;
} memcheck_thread_t;

typedef struct {
  size_t size;
} malloc_header_t;

int print_backtrace(void *data, void *addr, char *sym_name, void *symbol_base, char *filename) {
  fprintf(stderr, "==memcheck==  at [%s]+%p (%p) in %s\n", sym_name, (void *)(addr - symbol_base), addr, filename);
  return 0;
}

void memcheck_print_error(void *addr, uintptr_t meta, void *pc, stack_frame_t *frame) {
  bool is_store = meta & IS_STORE;
  size_t size = meta & (~IS_STORE);

  char *filename;
  char *symbol;
  void *symbol_base;
  int ret = get_symbol_info_by_addr((uintptr_t)pc, &symbol, &symbol_base, &filename);
#ifdef MC_IGNORE_INTERP
  if(symbol_base == loader_base) return;
#endif

  fprintf(stderr, "\n==memcheck== Invalid %s (size %zu) %s %p\n", is_store ? "store" : "load", size, is_store ? "to" : "from", addr);

  if (ret == 0) {
    while(filename == NULL || symbol_base == NULL);
    fprintf(stderr, "==memcheck==  at [%s]+%p (%p) in %s\n", symbol, (void *)(pc - symbol_base), pc, filename);
    free(filename);
    free(symbol);
  } else {
    fprintf(stderr, "==memcheck==  at %p\n", pc);
  }

  fprintf(stderr, "==memcheck==  Backtrace:\n");
  ret = get_backtrace(frame, &print_backtrace, NULL);
  assert(ret == 0);

  fprintf(stderr, "\n");
}

void memcheck_mark(void *start, size_t size, bool valid) {
  assert(start < (void *)RESERVED_BASE
        && (start + size) < (void *)RESERVED_BASE
        && (start + size) >= start);

  if (size == 0) return;

#ifdef COMPACT_SHADOW
  void *shadow_addr = (void *)RESERVED_BASE + (((uintptr_t)start) / 8);
  uintptr_t start_b_offset = (uintptr_t)start & 0x7;

  if (start_b_offset) {
    uint8_t mask = (1 << max(size, 8)) - 1;
    uint8_t *sb = shadow_addr;
    if (valid) {
      *sb |= (mask << start_b_offset);
    } else {
      *sb &= ~(mask << start_b_offset);
    }
    shadow_addr++;
    size -= start_b_offset;
    while(1);
  }

  if (size > 0) {
    uintptr_t end_b_offset = ((uintptr_t)start + size);
    uintptr_t end_b_size = end_b_offset & 0x7;
    if (end_b_size) {
      uint8_t *eb = (void *)RESERVED_BASE + end_b_offset / 8;
      if (valid) {
        *eb |= (0xFF >> (8-end_b_size));
      } else {
        *eb &= ~(0xFF >> (8-end_b_size));
      }
      size -= end_b_size;
    }
  }
  
  if (size) {
    memset(shadow_addr, valid ? 0xFF : 0, size/8);
  }

#else
  memset(start + RESERVED_BASE, valid ? 1 : 0, size);
#endif
}

void memcheck_mark_valid(void *start, size_t size) {
  return memcheck_mark(start, size, true);
}

void memcheck_mark_invalid(void *start, size_t size) {
  return memcheck_mark(start, size, false);
}

void memcheck_alloc_hook(void *start, size_t size) {
  int ret = mambo_ht_add(&allocs, (uintptr_t)start, (uintptr_t)size);
  assert(ret == 0);
  memcheck_mark_valid(start, size);
}

void memcheck_free_hook(void *start, size_t size) {
  if (start == NULL) return;

  uintptr_t alloc_size;
  int ret = mambo_ht_get(&allocs, (uintptr_t)start, &alloc_size);
  if (ret != 0) {
    fprintf(stderr, "\n==memcheck== invalid free for %p\n\n", start);
    return;
  }
  size = alloc_size;

  if (size == MAGIC_FREED) {
    fprintf(stderr, "\n==memcheck== double free for %p\n\n", start);
  } else {
    ret = mambo_ht_add(&allocs, (uintptr_t)start, MAGIC_FREED);
    assert(ret == 0);

    memcheck_mark_invalid(start, size);
  }
}

void set_in_malloc_ptr(mambo_context *ctx, enum reg reg) {
  memcheck_thread_t *td = mambo_get_thread_plugin_data(ctx);
  assert(td != NULL);
  emit_set_reg(ctx, reg, (uintptr_t)&td->in_malloc);
}

void __memcheck_inst_aarch32(mambo_context *ctx, int size) {
#ifdef __arm__
  while (size > 24);

  inst_set type = mambo_get_inst_type(ctx);
  if (type == ARM_INST) {
    emit_add_sub_i(ctx, pc, pc, -3);
  }

  /*
    AND R1, R0, #7
    BIC R2, R0, #0x80000000 // mask off high bit
    LSR R2, R2, #3
    ORR R2, R2, RESERVED_BASE
    MOV R3, MASK (based on access size)
    LSL R3, R3, R1
    LDR R1, [R2]
    AND R1, R1, R3
    EOR R1, R1, R3
  */
  emit_thumb_andi32(ctx, 0, 0, r0, 0, r1, 7);
  uint32_t imm12 = 0x400;
  emit_thumb_bici32(ctx, imm12 >> 11, 0, r0, imm12 >> 8, r2, imm12);
  emit_thumb_lsri32(ctx, 0, 0, r2, 3, r2);
  emit_thumb_orri32(ctx, imm12 >> 11, 0, r2, imm12 >> 8, r2, imm12);

  ctx->code.inst_type = THUMB_INST;
  emit_set_reg(ctx, r3, (1 << size) -1);
  ctx->code.inst_type = type;

  emit_thumb_lsl32(ctx, 0, r3, r3, r1);
  emit_thumb_ldri16(ctx, 0, r2, r1);
  emit_thumb_and32(ctx, 0, r1, 0, r1, 0, 0, r3);
  emit_thumb_eor32(ctx, 0, r1, 0, r1, 0, 0, r3);
#endif
}

void __memcheck_inst_aarch64(mambo_context *ctx, int size) {
#ifdef __aarch64__
  // AND X1, X0, #7
  emit_a64_logical_immed(ctx, 1, 0, 1, 0, 2, x0, x1);
  // AND X2, X0, #(RESERVED_BASE -1) 0x3F_FFFF_FFFF
  emit_a64_logical_immed(ctx, 1, 0, 1, 0, 37, x0, x2);
  // LSR X2, X2, #3
  emit_a64_BFM(ctx, 1, 2, 1, 3, 0x3F, x2, x2);
  // ORR X2, X2, RESERVED_BASE (0x40_0000_0000)
  emit_a64_logical_immed(ctx, 1, 1, 1, 26, 0, x2, x2);
  // MOV X3, #mask
  emit_a64_logical_immed(ctx, 1, 1, 1, 0, size-1, 0x1F, x3);
  // LSL X3, X3, X1
  emit_a64_data_proc_reg2(ctx, 1, x1, 0x8, x3, x3);
  // LDR X1, [X2]
  emit_a64_LDR_STR_unsigned_immed(ctx, 3, 0, 1, 0, x2, x1);
  // AND X1, X1, X3
  emit_a64_logical_reg(ctx, 1, 0, 0, 0, x3, 0, x1, x1);
  // EOR X1, X1, X3
  emit_a64_logical_reg(ctx, 1, 2, 0, 0, x3, 0, x1, x1);
#endif
}

// Pattern matching accesses which cause false positives
bool memcheck_should_ignore(mambo_context *ctx) {
#ifdef __aarch64__
  #ifdef MC_IGNORE_LIST
  uint32_t iw = *(uint32_t *)mambo_get_source_addr(ctx);
  if (iw == 0x4cdfa041 ||  // strchr
      iw == 0x4cdfa061 ||  // memchr
      iw == 0xa9410c22 ||  // strlen + 0x70
      iw == 0xa9400c02 ||  // strlen + 0x10
      iw == 0xf8408402 ||  // strcmp + 0x18
      iw == 0xf8408423 ||  // strcmp + 0x1c
      iw == 0xa9401424 ||  // __stpcpy + 0x18
      iw == 0xa9c20c22 ||  // strlen + 0x58
      iw == 0xf8408403 ||  // strncmp + 0x24
      iw == 0xa8c11043 ||  // strnlen + 0x94
      iw == 0xa9c10c22 ||  // strlen + 0xc4
      iw == 0xa9400c22 ||  // strlen + 0x114
      iw == 0xa8c11444 ||  // strcpy + 0xd8
      iw == 0xf8408424 ||  // bcmp + 0x24
      iw == 0x39400820 ||  // strcspn + 0xdc
      iw == 0x39400c22 ||  // strcspn + 0xe0
      iw == 0x39400424 ||  // strcspn + +0xe4
      iw == 0x39400c43     // strspn + 0xe4
  ) return true;
  #endif

  if (mambo_get_inst(ctx) == A64_LDX_STX) {
    uint32_t size, o2, l, o1, rs, o0, rt2, rn, rt;
    a64_LDX_STX_decode_fields(mambo_get_source_addr(ctx), &size, &o2, &l, &o1, &rs, &o0, &rt2, &rn, &rt);
    // don't instrument store exclusive
    if (o2 == 0 && l == 0 && o1 == 0) {
      return true;
    }
  }
#elif __arm__ && MC_IGNORE_LIST
  inst_set inst_type = mambo_get_inst_type(ctx);
  if (inst_type == THUMB_INST) {
    if (iw == 0x2300e9d1 || // strlen + 0x23
        iw == 0x2302e9d1 || // strlen + 0x41
        iw == 0x2304e9d1 || // strlen + 0x5b
        iw == 0x2306e9d1 || // strlen + 0x75
        iw == 0x2304e8f0 || // strlen + 0x6f
        iw == 0x6704e8f1 || // strcmp + 0x73
        iw == 0x2302e950 || // strcmp + 0x93
        iw == 0x6702e951 || // strcmp + 0x97
        iw == 0x2b08f850 || // strcmp + 0x115
        iw == 0x3b08f851 || // strcmp + 0x119
        iw == 0x2c04f850 || // strcmp + 0x12b
        iw == 0x3c04f851 || // strcmp + 0x12f
        iw == 0x3b04f851 || // strcmp + 0x1eb
        iw == 0x2b04f850 || // strcmp + 0x291
        iw == 0xea4f680b || // strcmp + 0x2a3
        iw == 0x4b04f853 || // strnlen + 0x2d
        iw == 0x2302e8f0    // rawmemchr + 0x57
    ) return true;
  } else if (inst_type == ARM_INST) {
    if (iw == 0x00c020d8 || // index + 0x84
        iw == 0xe491e004 || // memcpy + 0x1b4
        iw == 0xe0c340d8 || // rindex + 0x48
        iw == 0xe493c004    // strnlen + 0x68
    ) return true;
  }
#endif
  return false;
}

int memcheck_pre_inst_handler(mambo_context *ctx) {
  int ret;
  if (mambo_is_load_or_store(ctx)) {
    if (memcheck_should_ignore(ctx)) return 0;

    mambo_cond cond = mambo_get_cond(ctx);
    mambo_branch cond_br;
    mambo_branch zbr;

    if (cond != AL) {
      ret = mambo_reserve_branch(ctx, &cond_br);
      assert(ret == 0);
    }

#ifdef __arm__
    ret = mambo_reserve_cc_space(ctx, 88);
    assert(ret == 0);
#endif

#ifdef COMPACT_SHADOW
    int access_size = mambo_get_ld_st_size(ctx);
    bool is_store = mambo_is_store(ctx);
    
    emit_push(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << 3));

    mambo_calc_ld_st_addr(ctx, 0);
    __memcheck_inst_aarch64(ctx, min(access_size, 56));
    __memcheck_inst_aarch32(ctx, min(access_size, 24));
  #ifdef __arm__
    inst_set type = mambo_get_inst_type(ctx);
    ctx->code.inst_type = THUMB_INST;
  #endif
    mambo_reserve_branch_cbz(ctx, &zbr);

    emit_push(ctx, (1 << 4) | (1 << lr));

    emit_set_reg(ctx, 1, access_size | (is_store ? IS_STORE : 0));
    emit_set_reg(ctx, 2, (uintptr_t)mambo_get_source_addr(ctx));
    set_in_malloc_ptr(ctx, 3);
    emit_fcall(ctx, memcheck_unalloc);

    emit_pop(ctx, (1 << 4) | (1 << lr));

    emit_local_branch_cbz(ctx, &zbr, 1);
  #ifdef __arm__
    if (type == ARM_INST) {
      if ((uintptr_t)ctx->code.write_p & 2) {
        emit_thumb_nop16(ctx);
      }
      emit_thumb_bx16(ctx, pc);
      emit_thumb_nop16(ctx);
    }
    ctx->code.inst_type = type;
  #endif

    emit_pop(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << 3));
    if (cond != AL) {
      ret = emit_local_branch_cond(ctx, &cond_br, invert_cond(cond));
      assert(ret == 0);
    }

#else
    int regs[2];
    ret = mambo_get_scratch_regs(ctx, 2, &regs[0], &regs[1]);
    assert(ret == 2);

    mambo_calc_ld_st_addr(ctx, regs[0]);

    emit_set_reg(ctx, regs[1], RESERVED_BASE);
    emit_a64_LDR_STR_reg(ctx, 0, 0, 1, regs[0], 2, 0, regs[1], regs[1]);

    mambo_reserve_branch_cbz(ctx, &cbz);

    emit_push(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << 3) | (1 << 4) | (1 << lr));
    emit_mov(ctx, 0, regs[0]);
    emit_set_reg(ctx, 1, mambo_get_ld_st_size(ctx) | (mambo_is_store(ctx) ? IS_STORE : 0));
    emit_set_reg(ctx, 2, (uintptr_t)mambo_get_source_addr(ctx));
    set_in_malloc_ptr(ctx, 3);
    emit_fcall(ctx, memcheck_unalloc);
    emit_pop(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << 3) | (1 << 4) | (1 << lr));

    emit_local_branch_cbz(ctx, &zbr, regs[1]);

    ret = mambo_free_scratch_regs(ctx, (1 << regs[0]) | (1 << regs[1]));
    assert(ret == 0);
#endif
  }

  return 0;
}

int _memcheck_inst_alloc_reg(mambo_context *ctx, enum reg reg) {
  emit_mov(ctx, es, reg);
  set_in_malloc_ptr(ctx, es-1);
  emit_fcall(ctx, memcheck_malloc_pre);
  return 0;
}

int memcheck_inst_alloc0(mambo_context *ctx) {
  return _memcheck_inst_alloc_reg(ctx, 0);
}

int memcheck_inst_alloc1(mambo_context *ctx) {
  return _memcheck_inst_alloc_reg(ctx, 1);
}

int memcheck_inst_alloc_post(mambo_context *ctx) {
  set_in_malloc_ptr(ctx, es-1);
  emit_fcall(ctx, memcheck_malloc_post);
  return 0;
}

const enum reg sr1 = es + 1, sr2 = es + 2;

int memcheck_inst_posix_memalign(mambo_context *ctx) {
  emit_push(ctx, (1 << sr1) | (1 << sr2));
  emit_mov(ctx, es, 2);
  emit_mov(ctx, sr1, 0);
  set_in_malloc_ptr(ctx, es-1);

  // BL memcheck_malloc_pre
  emit_fcall(ctx, memcheck_malloc_pre);
}

int memcheck_inst_posix_memalign_post(mambo_context *ctx) {
  emit_mov(ctx, sr2, 0); // keep the return value in SR2

#ifdef __aarch64__
  // LDR X0, [sr1]
  emit_a64_LDR_STR_unsigned_immed(ctx, 3, 0, 1, 0, sr1, 0);
#elif __arm__
  // LDR R0, [sr1]
  switch(mambo_get_inst_type(ctx)) {
    case ARM_INST:
      emit_arm_ldr(ctx, IMM_LDR, r0, sr1, 0, 1, 1, 0);
      break;
    case THUMB_INST:
      emit_thumb_ldri16(ctx, 0, sr1, r0);
      break;
    default:
      assert(0);
  }
#endif

  set_in_malloc_ptr(ctx, es-1);

  // BL memcheck_malloc_post
  emit_fcall(ctx, memcheck_malloc_post);

  emit_mov(ctx, 0, sr2); // restore the return value

  emit_pop(ctx, (1 << sr1) | (1 << sr2));
}

int memcheck_inst_calloc(mambo_context *ctx) {
#ifdef __aarch64__
  // MUL X19, X0, X1
  emit_a64_data_proc_reg3(ctx, 1, 0, x0, 0, 0x1F, x1, x19);
#elif __arm__
  switch (mambo_get_inst_type(ctx)) {
    case ARM_INST:
      emit_arm_mul(ctx, es, r0, r1);
      break;
    case THUMB_INST:
      emit_thumb_mul32(ctx, r0, es, r1);
      break;
    default:
      assert(0);
  }
#endif
  set_in_malloc_ptr(ctx, es-1);
  emit_fcall(ctx, memcheck_malloc_pre);
}

int memcheck_inst_realloc(mambo_context *ctx) {
  emit_mov(ctx, es, 1);
  set_in_malloc_ptr(ctx, es-1);
  emit_fcall(ctx, memcheck_free_pre);
}

int memcheck_inst_free(mambo_context *ctx) {
  memcheck_thread_t *td = mambo_get_thread_plugin_data(ctx);
  assert(td != NULL);

  emit_set_reg(ctx, es-1, (uintptr_t)&td->in_malloc);

  // BL memcheck_free_pre
  emit_fcall(ctx, memcheck_free_pre);
}

int memcheck_inst_free_post(mambo_context *ctx) {
  memcheck_thread_t *td = mambo_get_thread_plugin_data(ctx);
  assert(td != NULL);

  emit_set_reg(ctx, es-1, (uintptr_t)&td->in_malloc);
  emit_fcall(ctx, memcheck_free_post);
}

int memcheck_inst_ignored_fn(mambo_context *ctx) {
  set_in_malloc_ptr(ctx, es-1);
  emit_fcall(ctx, memcheck_malloc_pre);
}

int memcheck_inst_ignored_fn_post(mambo_context *ctx) {
  set_in_malloc_ptr(ctx, es-1);
  emit_fcall(ctx, memcheck_free_post);
}

int memcheck_vm_op_handler(mambo_context *ctx) {
  memcheck_thread_t *td = mambo_get_thread_plugin_data(ctx);

  if (td == NULL || !td->in_malloc) {
    vm_op_t op = mambo_get_vm_op(ctx);
    switch(op) {
      case VM_MAP:
        memcheck_mark_valid(mambo_get_vm_addr(ctx), mambo_get_vm_size(ctx));
        if (loader_base == NULL && (mambo_get_vm_flags(ctx) & MAP_INTERP)) {
          loader_base = mambo_get_vm_addr(ctx);
        }
        break;
      case VM_UNMAP:
        memcheck_mark_invalid(mambo_get_vm_addr(ctx), mambo_get_vm_size(ctx));
        break;
    }
  }
}

void extend_stack() {
  // First, increase our stack allocation
  uint8_t *stack = alloca(1024*1024);
  stack[0] = 1;
}

int memcheck_pre_thread_handler(mambo_context *ctx) {
  memcheck_thread_t *td = mambo_alloc(ctx, sizeof(*td));
  assert(td != NULL);
  int ret = mambo_set_thread_plugin_data(ctx, td);
  assert(ret == MAMBO_SUCCESS);

  td->in_malloc = 0;
}

void __memcheck_mark_valid(uintptr_t addr, size_t size) {
  void *alias = (void*)(addr & (RESERVED_BASE-1));
  if ((uintptr_t)alias != addr) {
    void *shadow = mmap(alias, size, PROT_NONE,
                        MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);
    assert(shadow == alias);
  }
  memcheck_mark_valid(alias, size);
}

size_t memcheck_malloc_usable_size(uintptr_t ptr) {
  uintptr_t alloc_size = 0;
  mambo_ht_get(&allocs, ptr, &alloc_size);
  return alloc_size;
}

extern void memcheck_ret();
int memcheck_replace_malloc_usable_size(mambo_context *ctx) {
  int ret = emit_safe_fcall(ctx, memcheck_malloc_usable_size, MAX_FCALL_ARGS);
  assert(ret == 0);
  ret = mambo_set_source_addr(ctx, memcheck_ret);
  assert(ret == 0);
}

__attribute__((constructor)) void memcheck_init_plugin() {
  int ret;

  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  printf("\n-- MAMBO memcheck " VERSION " --\n\n");

  /* Reserve the highest page of the application's memory range */
  void *guard_page = mmap((void *)RESERVED_BASE - PAGE_SIZE, PAGE_SIZE, PROT_NONE,
                          MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);
  assert(guard_page == (void *)RESERVED_BASE - PAGE_SIZE);

  /* Reserve shadow memory */
  shadow_mem = mmap((void *)RESERVED_BASE, SHADOW_SIZE, PROT_READ|PROT_WRITE,
                    MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);
  assert(shadow_mem == (void *)RESERVED_BASE);

  /* The MAMBO image and its stack and heap can be within the reserved region.
     Rather than attempting to identify their boundaries, we'll reserve the
     range below RESERVED_BASE for the application and then fill the address space. */
  // First, increase our stack allocation
  extend_stack();

  uintptr_t app_base = max(0x8000, PAGE_SIZE);
  size_t app_size = (size_t)guard_page - app_base;
  void *app_range = mmap((void *)app_base, app_size, PROT_NONE,
                         MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);
  assert(app_range == (void *)app_base);

  for (size_t size = INITIAL_RES_SIZE; size >= PAGE_SIZE; size /= 2) {
    while(mmap(NULL, size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) != MAP_FAILED);
  }

  ret = munmap(app_range, app_size);
  assert(ret == 0);

  // VDSO and VVAR
  /* We can't move them below RESERVED_BASE, so instead we reserve their alias and rely on the load / store
     instrumentation to mask the addresses of accesses to the VDSO region.
  */
  // AT_SYSINFO_EHDR points to the base of the VDSO. The adjacent lower page is VVAR
  uintptr_t vdso_base = getauxval(AT_SYSINFO_EHDR);
  if (vdso_base != 0) {
    __memcheck_mark_valid(vdso_base - PAGE_SIZE, VDSO_SIZE);
  }

  // Kernel helpers page
#ifdef __arm__
  __memcheck_mark_valid(0xffff0000, PAGE_SIZE);
#endif

  ret = mambo_ht_init(&allocs, 10000, 2, 70, true);
  assert(ret == 0);

  ret = mambo_register_pre_thread_cb(ctx, &memcheck_pre_thread_handler);
  assert(ret == MAMBO_SUCCESS);
  ret = mambo_register_pre_inst_cb(ctx, &memcheck_pre_inst_handler);
  assert(ret == MAMBO_SUCCESS);
  ret = mambo_register_vm_op_cb(ctx, &memcheck_vm_op_handler);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "malloc", &memcheck_inst_alloc0,
                                                  &memcheck_inst_alloc_post, 1);
  assert(ret == MAMBO_SUCCESS);
  ret = mambo_register_function_cb(ctx, "valloc", &memcheck_inst_alloc0, &memcheck_inst_alloc_post, 1);
  assert(ret == MAMBO_SUCCESS);
  ret = mambo_register_function_cb(ctx, "pvalloc", &memcheck_inst_alloc0, &memcheck_inst_alloc_post, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "alligned_alloc", &memcheck_inst_alloc1, &memcheck_inst_alloc_post, 2);
  assert(ret == MAMBO_SUCCESS);
  ret = mambo_register_function_cb(ctx, "memalign", &memcheck_inst_alloc1, &memcheck_inst_alloc_post, 2);
  assert(ret == MAMBO_SUCCESS);
  ret = mambo_register_function_cb(ctx, "__libc_memalign", &memcheck_inst_alloc1, &memcheck_inst_alloc_post, 2);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "posix_memalign", &memcheck_inst_posix_memalign, &memcheck_inst_posix_memalign_post, 3);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "calloc", &memcheck_inst_calloc, &memcheck_inst_alloc_post, 2);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "realloc", &memcheck_inst_realloc, &memcheck_inst_alloc_post, 2);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "free", &memcheck_inst_free, &memcheck_inst_free_post, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "__malloc_arena_thread_freeres", &memcheck_inst_ignored_fn, &memcheck_inst_ignored_fn_post, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "malloc_usable_size", &memcheck_replace_malloc_usable_size, NULL, 1);
  assert(ret == MAMBO_SUCCESS);
#ifdef MC_REPLACE_FNS
  memcheck_install_naive_stdlib(ctx);
#endif
}

```

`plugins/memcheck/memcheck.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#define COMPACT_SHADOW 1

/* ignore invalid accesses from a whitelist of instructions found in glibc functions
   known to perform safe (aligned) out-of-bounds reads for performance optimisation
   may lead to false negatives */
//#define MC_IGNORE_LIST 1

/* replace the standard library functions known to have glibc implementations that
   perform safe (aligned) out-of-bounds reads for performance reasons
   the replacement implementations are naive - may affect application performance */
#define MC_REPLACE_FNS 1

/* ignore errors in the dynamic loader
   some distributions seem to ship stripped linker executables, which means that we
   can't hook their embedded stdlib functions, so we'll get some false positive errors
   as the dynamic loader sets up
   this options will cause memcheck to ignore all invalid accesses in the loader */
#define MC_IGNORE_INTERP 1

#ifdef COMPACT_SHADOW
  #define SHADOW_SIZE (RESERVED_BASE/8)
#else
  #define SHADOW_SIZE (RESERVED_BASE*2)
  #ifdef __arm__
    #error Uncompacted shadow memory not supported on AArch32
  #endif
#endif

#define VDSO_SIZE     (PAGE_SIZE*2)
#ifdef __aarch64__
  #define RESERVED_BASE 0x4000000000 // 256 GiB
  #define RESERVED_TOP  (vdso_base - PAGE_SIZE)
  #define INITIAL_RES_SIZE (0x1000000000) // 64 GiB
#elif __arm__
  #define RESERVED_BASE 0x80000000
  #define INITIAL_RES_SIZE (128*1024*1024)
#endif


```

`plugins/memcheck/naive_stdlib.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stddef.h>
#include <stdint.h>
#include <assert.h>
#include "../../plugins.h"

int memcheck_bcmp(const void *s1, const void *s2, size_t n) {
  char *a = (char *)s1, *b = (char *)s2;
  for (size_t i = 0; i < n; i++) {
    if (a[i] != b[i]) return -1;
  }
  return 0;
}

char *memcheck_index(const char *s, int c) {
  char *ptr = (char *)s;
  for (; *ptr != '\0'; ptr++) {
    if (*ptr == c) return ptr;
  }
  if (c == '\0' && *ptr == '\0') return ptr;
  return NULL;
}

void *memcheck_memchr(const void *s, int c, size_t n) {
  unsigned char *p = (unsigned char *)s;
  for (size_t i = 0; i < n; i++) {
    if (p[i] == (unsigned char) c) return &p[i];
  }
  return NULL;
}

void *memcheck_rawmemchr(const void *s, int c) {
  return memcheck_memchr(s, c, SIZE_MAX);
}

char *memcheck_rindex(const char *s, int c) {
  char *ret = NULL;
  char *ptr = (char *)s;
  for (; *ptr != '\0'; ptr++) {
    if (*ptr == c) ret = ptr;
  }
  if (c == '\0' && *ptr == '\0') return ptr;
  return ret;
}

char *memcheck_stpcpy(char *dest, const char *src) {
  do {
    *dest = *src;
  } while(*src != '\0' && src++ && dest++);

  return dest;
}

int memcheck_strcmp(const char *s1, const char *s2) {
  uintptr_t i = 0;

  while(s1[i] == s2[i] && s1[i] != '\0') {
    i++;
  }
  if (s1[i] < s2[i]) return -1;
  if (s1[i] > s2[i]) return 1;
  return 0;
}

char *memcheck_strcpy(char *dest, const char *src) {
  size_t i;
  for (i = 0; src[i] != '\0'; i++) {
    dest[i] = src[i];
  }
  dest[i] = '\0';
  return dest;
}

size_t memcheck_strlen(const char *s) {
  size_t len = 0;
  for (len = 0; s[len] != '\0'; len++);
  return len;
}

int memcheck_strncmp(const char *s1, const char *s2, size_t n) {
  size_t i = 0;

  if (n == 0) return 0;

  while(s1[i] == s2[i] && s1[i] != '\0' && (i+1) < n) {
    i++;
  }
  if (s1[i] < s2[i]) return -1;
  if (s1[i] > s2[i]) return 1;
  return 0;
}

size_t memcheck_strnlen(const char *s, size_t maxlen) {
  size_t len = 0;
  for (len = 0; (len < maxlen) && (s[len] != '\0'); len++);
  return len;
}

char *memcheck_strchrnul(const char *s, int c) {
  char *p = (char *)s;
  for (; *p != (char)c && (*p != '\0') ; p++);
  return p;
}

size_t memcheck_strspn(const char *s, const char *accept) {
  size_t len = 0;
  char *p = (char *)s;
  for (; *p != '\0'; p++) {
    bool match = false;
    for (int i = 0; accept[i] != '\0' && !match; i++) {
      if (*p == accept[i]) {
        match = true;
        break;
      }
    }
    if (!match) return len;
    len++;
  } // end of the string
  return len;
}

size_t memcheck_strcspn(const char *s, const char *reject) {
  size_t len = 0;
  char *p = (char *)s;
  for (; *p != '\0'; p++) {
    for (int i = 0; reject[i] != '\0'; i++) {
      if (*p == reject[i]) {
        return len;
      }
    }
    len++;
  } // end of the string
  return len;
}

int memcheck_replace_strlen(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strlen);
  assert(ret == 0);
}

int memcheck_replace_bcmp(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_bcmp);
  assert(ret == 0);
}

int memcheck_replace_index(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_index);
  assert(ret == 0);
}

int memcheck_replace_memchr(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_memchr);
  assert(ret == 0);
}

int memcheck_replace_rawmemchr(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_rawmemchr);
  assert(ret == 0);
}

int memcheck_replace_rindex(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_rindex);
  assert(ret == 0);
}

int memcheck_replace_stpcpy(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_stpcpy);
  assert(ret == 0);
}

int memcheck_replace_strcmp(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strcmp);
  assert(ret == 0);
}

int memcheck_replace_strcpy(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strcpy);
  assert(ret == 0);
}

int memcheck_replace_strncmp(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strncmp);
  assert(ret == 0);
}

int memcheck_replace_strnlen(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strnlen);
  assert(ret == 0);
}

int memcheck_replace_strchrnul(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strchrnul);
  assert(ret == 0);
}

int memcheck_replace_strspn(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strspn);
  assert(ret == 0);
}

int memcheck_replace_strcspn(mambo_context *ctx) {
  int ret = mambo_set_source_addr(ctx, memcheck_strcspn);
  assert(ret == 0);
}

void memcheck_install_naive_stdlib(mambo_context *ctx) {
  int ret;
  /* Replace the stdlib functions which use hand-optimised assembly with
     deliberate out-of-bounds accesses with naive versions*/ 
  ret = mambo_register_function_cb(ctx, "bcmp", &memcheck_replace_bcmp, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "index", &memcheck_replace_index, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "memchr", &memcheck_replace_memchr, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "rawmemchr", &memcheck_replace_rawmemchr, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "rindex", &memcheck_replace_rindex, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "stpcpy", &memcheck_replace_stpcpy, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strchrnul", &memcheck_replace_strchrnul, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strcmp", &memcheck_replace_strcmp, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strcpy", &memcheck_replace_strcpy, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strlen", &memcheck_replace_strlen, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strncmp", &memcheck_replace_strncmp, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strnlen", &memcheck_replace_strnlen, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strspn", &memcheck_replace_strspn, NULL, 1);
  assert(ret == MAMBO_SUCCESS);

  ret = mambo_register_function_cb(ctx, "strcspn", &memcheck_replace_strcspn, NULL, 1);
  assert(ret == MAMBO_SUCCESS);
}

```

`plugins/mtrace.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include "mtrace.h"

#ifdef __arm__
.syntax unified
#endif

.global mtrace_buf_write // (value, info, &buf_ptr.entries[0])
.func
.type mtrace_buf_write, %function

#ifdef __arm__
.thumb_func
mtrace_buf_write:
  PUSH {R3, R4}
  LDR R3, [R2, #-4]
  ADD R4, R2, R3, LSL #3
  STRD R0, R1, [R4]
  ADD R3, R3, #1
  STR R3, [R2, #-4]
  SUB R3, R3, #BUFLEN
  CBZ R3, call
  POP {R3, R4}
  BX LR

call:
  PUSH {R0-R2, R5-R6, R9, R12, LR}
  VPUSH {D16-D31}
  VPUSH {D0-D7}
  MRS R4, CPSR
  VMRS R5, FPSCR

  SUB R0, R2, #4

  MOV R6, SP
  BIC R2, R6, #7
  MOV SP, R2
  LDR R1, =mtrace_print_buf
  BLX R1
  MOV SP, R6

  MSR CPSR, R4
  VMSR FPSCR, R5
  VPOP {D0-D7}
  VPOP {D16-D31}
  POP {R0-R2, R5-R6, R9, R12, LR}
  POP {R3 - R4}
  BX LR
#endif

#ifdef __aarch64__
mtrace_buf_write:
  STP X3, X4, [SP, #-16]!
  LDR W3, [X2, #-8]
  ADD X4, X2, W3, UXTW #4
  STP X0, X1, [X4]

  ADD W3, W3, #1
  STR W3, [X2, #-8]
  SUB W3, W3, #BUFLEN
  CBZ W3, call
  LDP X3, X4, [SP], #16
  RET

call:
  STP X29, X30, [SP, #-16]!

  BL push_x4_x21
  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR
  BL push_neon

  SUB X0, X2, #8
  LDR X1, =mtrace_print_buf
  BLR X1

  BL pop_neon
  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21
  BL pop_x4_x21

  LDP X3, X4, [SP, #16]
  LDP X29, X30, [SP], #32
  RET
#endif

.endfunc

```

`plugins/mtrace.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW

#include <stdio.h>
#include <assert.h>
#include <inttypes.h>
#include "../plugins.h"

#include "mtrace.h"

struct mtrace_entry {
  uintptr_t addr;
  uintptr_t info;
};

struct mtrace {
  uint32_t len;
  struct mtrace_entry entries[BUFLEN];
};

extern void mtrace_print_buf_trampoline(struct mtrace *trace);
extern void mtrace_buf_write(uintptr_t value, struct mtrace *trace);

void mtrace_print_buf(struct mtrace *mtrace_buf) {
  for (int i = 0; i < mtrace_buf->len; i++) {
    /* Warning: printing formatted strings is very slow
       For practical use, you are encouraged to process the data in memory
       or write the trace in the raw binary format */
    int size = (int)(mtrace_buf->entries[i].info >> 1);
    char *type = (mtrace_buf->entries[i].info & 1) ? "w" : "r";
    fprintf(stderr, "%s: %p\t%d\n", type, (void *)mtrace_buf->entries[i].addr, size);
  }
  mtrace_buf->len = 0;
}

int mtrace_pre_inst_handler(mambo_context *ctx) {
  struct mtrace *mtrace_buf = mambo_get_thread_plugin_data(ctx);
  bool is_load = mambo_is_load(ctx);
  bool is_store = mambo_is_store(ctx);
  if (is_load || is_store) {
    mambo_cond cond = mambo_get_cond(ctx);
    mambo_branch skip_br;
    int ret;
    if (cond != AL) {
      ret = mambo_reserve_branch(ctx, &skip_br);
      assert(ret == 0);
    }

    emit_push(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << lr));    

    ret = mambo_calc_ld_st_addr(ctx, 0);
    assert(ret == 0);
    int size = mambo_get_ld_st_size(ctx);
    assert(size > 0);

    uintptr_t info = (size << 1) | (is_store ? 1 : 0);
    emit_set_reg(ctx, 1, info);
    emit_set_reg_ptr(ctx, 2, &mtrace_buf->entries);
    emit_fcall(ctx, mtrace_buf_write);

    emit_pop(ctx, (1 << 0) | (1 << 1) | (1 << 2) | (1 << lr));

    if (cond != AL) {
      ret = emit_local_branch_cond(ctx, &skip_br, invert_cond(cond));
      assert(ret == 0);
    }
  }
}

int mtrace_pre_thread_handler(mambo_context *ctx) {
  struct mtrace *mtrace_buf = mambo_alloc(ctx, sizeof(*mtrace_buf));
  assert(mtrace_buf != NULL);
  mtrace_buf->len = 0;

  int ret = mambo_set_thread_plugin_data(ctx, mtrace_buf);
  assert(ret == MAMBO_SUCCESS);
}

int mtrace_post_thread_handler(mambo_context *ctx) {
  struct mtrace *mtrace_buf = mambo_get_thread_plugin_data(ctx);
  mtrace_print_buf(mtrace_buf);
  mambo_free(ctx, mtrace_buf);
}

__attribute__((constructor)) void mtrace_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  mambo_register_pre_thread_cb(ctx, &mtrace_pre_thread_handler);
  mambo_register_post_thread_cb(ctx, &mtrace_post_thread_handler);
  mambo_register_pre_inst_cb(ctx, &mtrace_pre_inst_handler);
}
#endif

```

`plugins/mtrace.h`:

```h
#define BUFLEN 2047

```

`plugins/poc_log_returns.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2018 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW

#include <stdio.h>
#include <assert.h>
#include <locale.h>
#include <inttypes.h>
#include "../plugins.h"

void log_returns_print(void *return_from, void *return_to) {
  fprintf(stderr, "Return from %p to %p\n", return_from, return_to);
}

/* Proof of concept. Note that only a subset of returns are currently instrumented by this code */
int log_returns_pre_inst(mambo_context *ctx) {
  bool instrument = false;
#ifdef __arm__
  inst_set isa = mambo_get_inst_type(ctx);
  int inst = mambo_get_inst(ctx);
  if (isa == ARM_INST) {
    if (inst == ARM_BX) {
      uint32_t rn;
      arm_bx_decode_fields(mambo_get_source_addr(ctx), &rn);
      if (rn == lr) {
        instrument = true;
      }
    }
  } else if (isa == THUMB_INST) {
    fprintf(stderr, "poc_log_returns: Thumb support not implemented yet\n");
  }
#else
  #error "Current ISA not supported yet"
#endif
  if (instrument) {
    emit_push(ctx, (1 << reg0) | (1 << reg1));
    emit_set_reg_ptr(ctx, reg0, mambo_get_source_addr(ctx));
    emit_mov(ctx, reg1, lr);
    emit_safe_fcall(ctx, log_returns_print, 2);
    emit_pop(ctx, (1 << reg0) | (1 << reg1));
  }
}

__attribute__((constructor)) void branch_count_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  mambo_register_pre_inst_cb(ctx, &log_returns_pre_inst);
}
#endif

```

`plugins/soft_div.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2016 Cosmin Gorgovan <cosmin at linux-geek dot org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

/*
   This plugin replaces the SDIV and UDIV instructions with calls to the 
   __aeabi_idiv / __aeabi_uidiv ABI functions. This allows code compiled
   for cores with support for these optional instructions to be executed
   on machines which don't implement them.
*/

#ifdef PLUGINS_NEW
#ifdef __arm__

#include <stdio.h>
#include <assert.h>
#include "../plugins.h"

#define DEBUG

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

extern int __aeabi_idiv(int numerator, int denominator); 
extern unsigned __aeabi_uidiv(unsigned numerator, unsigned denominator);

int get_scratch_reg(uint32_t reglist, uint32_t exclude_list) {
  reglist &= ~exclude_list;
  return (int)next_reg_in_list(reglist, 0);
}

int soft_div_pre_inst(mambo_context *ctx) {
  uint32_t opcode, rd, rn, rm, reglist, sr_reglist;
  int inst_set = mambo_get_inst_type(ctx);
  int inst = mambo_get_inst(ctx);
  int sr;
  void *tr_start;
  mambo_cond cond;

  sr_reglist = (1 << r4) | (1 << r5) | (1 <<r6) | (1 << r7);
  reglist = (1 << r0) | (1 << r1) | (1 << r2) | (1 << r3) | (1 << lr);

  if (inst_set == ARM_INST) {
    if (inst == ARM_SDIV || inst == ARM_UDIV) {
      if (mambo_is_cond(ctx)) {
        tr_start = mambo_get_cc_addr(ctx);
        mambo_set_cc_addr(ctx, tr_start + 4);
      }

      arm_divide_decode_fields(mambo_get_source_addr(ctx), &opcode, &rd, &rn, &rm);
      assert(rd != sp && rn != sp && rm != sp);

      debug("Replacing A32 %sdiv from %p at: %p\n", (inst == ARM_SDIV) ? "s" : "u",
            mambo_get_source_addr(ctx), mambo_get_cc_addr(ctx));

      sr = get_scratch_reg(sr_reglist, (1 << rn) | (1 << rm) | (1 << rd));
      assert(sr != reg_invalid);

      reglist |= 1 << sr;
      reglist &= ~(1 << rd);
      emit_arm_push(ctx, reglist);
      emit_arm_mrs(ctx, sr);

      if (rn != r0) {
        emit_arm_mov(ctx, REG_PROC, 0, r0, rn);
      }
      if (rm != r1) {
        emit_arm_mov(ctx, REG_PROC, 0, r1, rm);
      }
      emit_arm_fcall(ctx, (inst == ARM_SDIV) ? (void *)__aeabi_idiv : (void *)__aeabi_uidiv);
      if (rd != r0) {
        emit_arm_mov(ctx, REG_PROC, 0, rd, r0);
      }
      sr = get_scratch_reg(reglist, (1 << rd));
      emit_arm_msr(ctx, sr, 3);
      emit_arm_pop(ctx, reglist);

      mambo_replace_inst(ctx);

      if (mambo_is_cond(ctx)) {
        cond = mambo_get_inverted_cond(ctx, mambo_get_cond(ctx));
        arm_b32_helper(tr_start, (uint32_t)mambo_get_cc_addr(ctx), cond);
      }
    }
  } else if(inst_set == THUMB_INST) {
    if (inst == THUMB_SDIV32 || inst == THUMB_UDIV32) {
      if (mambo_is_cond(ctx)) {
        tr_start = mambo_get_cc_addr(ctx);
        mambo_set_cc_addr(ctx, tr_start + 2);
      }

      thumb_sdiv32_decode_fields(mambo_get_source_addr(ctx), &rn, &rd, &rm);
      assert(rd != sp && rn != sp && rm != sp);

      debug("Replacing T32 %sdiv from %p at: %p\n", (inst == THUMB_SDIV32) ? "s" : "u",
            mambo_get_source_addr(ctx), mambo_get_cc_addr(ctx));

      sr = get_scratch_reg(sr_reglist, (1 << rn) | (1 << rm) | (1 << rd));
      assert(sr != reg_invalid);

      reglist |= 1 << sr;
      reglist &= ~(1 << rd);
      emit_thumb_push(ctx, reglist);
      emit_thumb_mrs32(ctx, sr);

      if (rn != r0) {
        emit_thumb_movh16(ctx, r0 >> 3, rn, r0 & 7);
      }
      if (rm != r1) {
        emit_thumb_movh16(ctx, r1 >> 3, rm, r1 & 7);
      }
      emit_thumb_fcall(ctx, (inst == THUMB_SDIV32) ? (void *)__aeabi_idiv : (void *)__aeabi_uidiv);
      if (rd != r0) {
        emit_thumb_movh16(ctx, rd >> 3, r0, rd & 7);
      }
      emit_thumb_msr32(ctx, sr, 3);
      emit_thumb_pop(ctx, reglist);

      mambo_replace_inst(ctx);

      if (mambo_is_cond(ctx)) {
        cond = mambo_get_inverted_cond(ctx, mambo_get_cond(ctx));
        emit_thumb_b16_cond(tr_start, mambo_get_cc_addr(ctx), cond);
      }
    }
  }
}

__attribute__((constructor)) void init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);
  mambo_register_pre_inst_cb(ctx, &soft_div_pre_inst);
}

#else // __arm__
  #error The soft_div plugin is only implemented for AArch32
#endif
#endif // PLUGINS_NEW

```

`plugins/strace.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2019 University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW
#include <stdio.h>
#include <assert.h>
#include <inttypes.h>
#include "../plugins.h"

#define DEBUG

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

int strace_pre(mambo_context *ctx) {
  uintptr_t call_no;
  uintptr_t *args;
  int ret = mambo_syscall_get_no(ctx, &call_no);
  assert(ret == 0);
  mambo_syscall_get_args(ctx, &args);
  assert(args != NULL);
  fprintf(stderr, "syscall(%"PRIuPTR", 0x%"PRIxPTR", 0x%"PRIxPTR", 0x%"PRIxPTR", 0x%"PRIxPTR", [...])", call_no, args[0], args[1], args[2], args[3]);
}

int strace_post(mambo_context *ctx) {
  uintptr_t syscall_ret;
  int ret = mambo_syscall_get_return(ctx, &syscall_ret);
  assert(ret == 0);
  fprintf(stderr, " = 0x%"PRIxPTR"\n", syscall_ret);
}

__attribute__((constructor)) void init_strace() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);
  mambo_register_pre_syscall_cb(ctx, &strace_pre);
  mambo_register_post_syscall_cb(ctx, &strace_post);
}

#endif

```

`plugins/symbol_example.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2019-2020 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <sys/mman.h>
#include <assert.h>
#include <dlfcn.h>
#include <inttypes.h>
#include <stdio.h>
#include <string.h>
#include <sys/auxv.h>
#include <unistd.h>

#include "../plugins.h"

void se_print(size_t size, uintptr_t at, uintptr_t caller) {
  char *at_name = NULL;
  char *caller_name = NULL;

  get_symbol_info_by_addr(at, &at_name, NULL, NULL);
  get_symbol_info_by_addr(caller, &caller_name, NULL, NULL);

  printf("malloc(%zu) at %" PRIxPTR "(%s)\n", size, at, at_name);
  printf("  called from %" PRIxPTR "(%s)\n", caller, caller_name);
}

int se_hook(mambo_context *ctx) {
  emit_push(ctx, (1 << reg0) | (1 << reg1) | (1 << reg2));
  emit_set_reg_ptr(ctx, reg1, mambo_get_source_addr(ctx));
  emit_mov(ctx, reg2, lr);
  emit_safe_fcall(ctx, se_print, 3);
  emit_pop(ctx, (1 << reg0) | (1 << reg1) | (1 << reg2));
  return 0;
}

__attribute__((constructor)) void memcheck_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  int ret = mambo_register_function_cb(ctx, "malloc", &se_hook, NULL, 1);
  assert(ret == MAMBO_SUCCESS);
}

```

`plugins/tb_count.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef PLUGINS_NEW
#ifdef __arm__

#include <stdio.h>
#include <assert.h>
#include <locale.h>
#include "../plugins.h"

// Called for each instruction scanned by MAMBO, before the translation is generated
int tb_cnt_pre_inst_handler(mambo_context *ctx) {
  void *skip_branch = NULL;

  if (mambo_get_inst_type(ctx) == THUMB_INST
      && (mambo_get_inst(ctx) == THUMB_TBB32) || (mambo_get_inst(ctx) == THUMB_TBH32)) {

    if (mambo_is_cond(ctx)) {
      skip_branch = mambo_get_cc_addr(ctx);
      mambo_set_cc_addr(ctx, skip_branch + 2);
    }

    emit_thumb_push16(ctx, (1 << r0) | (1 << r1) | (1 << r2)); // PUSH {R0-R2}
    emit_thumb_push_cpsr(ctx, r0);                  // MRS  R0, CPSR; PUSH {R0}
    // MOVW R0, #(ptr_to_ctr & 0xFFFF)
    // MOVT R0, #(ptr_to_ctr >> 16)
    emit_thumb_copy_to_reg_32bit(ctx, r0, (uint32_t)mambo_get_thread_plugin_data(ctx));
    emit_thumb_ldrd32(ctx, 1, 1, 0, r0, r1, r2, 0); // LDRD R1, R2, [R0, #0]
    emit_thumb_addi16(ctx, 1, r1, r1);              // ADDS R1, R1, #1
    emit_thumb_adci32(ctx, 0, 0, r2, 0, r2, 0);     // ADC  R2, R2, #0
    emit_thumb_strd32(ctx, 1, 1, 0, r0, r1, r2, 0); // STRD R1, R2, [R0, #0]
    emit_thumb_pop_cpsr(ctx, r0);                   // POP {R0}; MSR CPSR, R0
    emit_thumb_pop16(ctx, (1 << r0) | (1 << r1) | (1 << r2));  // POP {R0-R2}

    if (skip_branch != NULL) {
      emit_thumb_b16_cond(skip_branch, mambo_get_cc_addr(ctx), mambo_get_cond(ctx));
      fprintf(stderr, "TB count: cond TB instrumentation is untested.\n");
      while(1);
    }
  }
  return 0;
}

// Called when a new thread is created, including the initial thread
int tb_cnt_pre_thread_handler(mambo_context *ctx) {
  uint64_t *inst_counter = mambo_alloc(ctx, sizeof(uint64_t));
  *inst_counter = 0;
  assert(inst_counter != NULL);
  mambo_set_thread_plugin_data(ctx, inst_counter);
  return 0;
}

// Called when a thread exits, or in all threads when the process exits
int tb_cnt_post_thread_handler(mambo_context *ctx) {
  uint64_t *inst_counter = mambo_get_thread_plugin_data(ctx);
  fprintf(stderr, "%'llu TB instructions executed in thread %d\n", *inst_counter, mambo_get_thread_id(ctx));
  mambo_free(ctx, inst_counter);
  return 0;
}

__attribute__((constructor)) void tb_init_plugin() {
  mambo_context *ctx = mambo_register_plugin();
  assert(ctx != NULL);

  mambo_register_pre_inst_cb(ctx, &tb_cnt_pre_inst_handler);
  mambo_register_pre_thread_cb(ctx, &tb_cnt_pre_thread_handler);
  mambo_register_post_thread_cb(ctx, &tb_cnt_post_thread_handler);

  setlocale(LC_NUMERIC, "");
}

#else // __arm__
  #error The tb_count plugin is only available for AArch32 because the table branch instructions are only available on this architecture
#endif
#endif

```

`scanner_common.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifndef __SCANNER_COMMON_H__
#define __SCANNER_COMMON_H__

#include "scanner_public.h"

#define SETUP (1 << 0)
#define REPLACE_TARGET (1 << 1)
#define INSERT_BRANCH (1 << 2)
#define LATE_APP_SP (1 << 3)

#ifdef __arm__
  #define APP_SP (r3)
  #define DISP_SP_OFFSET (28)
  #define DISP_RES_WORDS (3)

void thumb_cc_branch(dbm_thread *thread_data, uint16_t *write_p, uint32_t dest_addr);
void thumb_b16_cond_helper(uint16_t *write_p, uint32_t dest_addr, mambo_cond cond);
void thumb_b32_helper(uint16_t *write_p, uint32_t dest_addr);
void thumb_b32_cond_helper(uint16_t **write_p, uint32_t dest_addr, enum arm_cond_codes condition);
void thumb_bl32_helper(uint16_t *write_p, uint32_t dest_addr);
void thumb_blx32_helper(uint16_t *write_p, uint32_t dest_addr);
void thumb_b_bl_helper(uint16_t *write_p, uint32_t dest_addr, bool link, bool to_arm);
void thumb_adjust_b_bl_target(dbm_thread *thread_data, uint16_t *write_p, uint32_t dest_addr);
void thumb_encode_cond_imm_branch(dbm_thread *thread_data,
                                       uint16_t **o_write_p,
                                       int basic_block,
                                       uint32_t address_taken,
                                       uint32_t address_skipped,
                                       enum arm_cond_codes condition,
                                       bool taken_in_cache,
                                       bool skipped_in_cache,
                                       bool update);
void thumb_encode_cbz_branch(dbm_thread *thread_data,
                                  uint32_t rn,
                                  uint16_t **o_write_p,
                                  int basic_block,
                                  uint32_t address_taken,
                                  uint32_t address_skipped,
                                  bool taken_in_cache,
                                  bool skipped_in_cache,
                                  bool update);

void arm_cc_branch(dbm_thread *thread_data, uint32_t *write_p, uint32_t target, uint32_t cond);
void arm_b32_helper(uint32_t *write_p, uint32_t target, uint32_t cond);
void arm_branch_helper(uint32_t *write_p, uint32_t target, bool link, uint32_t cond);
int thumb_cbz_cbnz_helper(uint16_t *write_p, uint32_t target, enum reg reg, bool cbz);
void arm_adjust_b_bl_target(uint32_t *write_p, uint32_t dest_addr);
void branch_save_context(dbm_thread *thread_data, uint16_t **o_write_p, bool late_app_sp);
void arm_inline_hash_lookup(dbm_thread *thread_data, uint32_t **o_write_p, int basic_block, int r_target);
void thumb_inline_hash_lookup(dbm_thread *thread_data, uint16_t **o_write_p, int basic_block, int r_target);
#endif

#ifdef __aarch64__
void a64_branch_helper(uint32_t *write_p, uint64_t target, bool link);
void a64_b_helper(uint32_t *write_p, uint64_t target);
void a64_bl_helper(uint32_t *write_p, uint64_t target);
void a64_b_cond_helper(uint32_t *write_p, uint64_t target, mambo_cond cond);
int a64_cbz_cbnz_helper(uint32_t *write_p, bool cbnz, uint64_t target, uint32_t sf, uint32_t rt);
void a64_cbz_helper(uint32_t *write_p, uint64_t target, uint32_t sf, uint32_t rt);
void a64_cbnz_helper(uint32_t *write_p, uint64_t target, uint32_t sf, uint32_t rt);
void a64_tbz_tbnz_helper(uint32_t *write_p, bool is_tbnz,
                         uint64_t target, enum reg reg, uint32_t bit);
void a64_tbz_helper(uint32_t *write_p, uint64_t target, enum reg reg, uint32_t bit);
void a64_tbnz_helper(uint32_t *write_p, uint64_t target, enum reg reg, uint32_t bit);
void a64_cc_branch(dbm_thread *thread_data, uint32_t *write_p, uint64_t target);
void a64_inline_hash_lookup(dbm_thread *thread_data, int basic_block, uint32_t **o_write_p,
                            uint32_t *read_address, enum reg rn, bool link, bool set_meta);
#endif

#endif

```

`scanner_public.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2015-2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifndef __SCANNER_PUBLIC_H__
#define __SCANNER_PUBLIC_H__

#include <stdint.h>

#define IMM_LDR 0
#define LDR_REG 1
#define IMM_PROC 1
#define REG_PROC 0

#ifdef __arm__
enum reg {
  r0 = 0,
  r1 = 1,
  r2 = 2,
  r3 = 3,
  r4 = 4,
  r5 = 5,
  r6 = 6,
  r7 = 7,
  r8 = 8, 
  r9 = 9,
  r10 = 10,
  r11 = 11,
  r12 = 12,
  r13 = 13,
  r14 = 14,
  r15 = 15,
  reg_invalid = 16
};

enum reg_alt {
  es = r4, // the first calleE-Saved register - not a standard alias
  sp = r13,
  lr = r14,
  pc = r15
};

#define m_r0 (1 << r0)
#define m_r1 (1 << r1)
#define m_r2 (1 << r2)
#define m_r3 (1 << r3)
#define m_r4 (1 << r4)
#define m_r5 (1 << r5)
#define m_r6 (1 << r6)
#define m_r7 (1 << r7)
#define m_r8 (1 << r8)
#define m_r9 (1 << r9)
#define m_r10 (1 << r10)
#define m_r11 (1 << r11)
#define m_r12 (1 << r12)
#define m_r13 (1 << r13)
#define m_r14 (1 << r14)
#define m_r15 (1 << r15)
#endif // __arm__

#ifdef __aarch64__
enum reg {      // +--------------+
  x0   =   0,   // | X0           |
  x1   =   1,   // | X1           |
  x2   =   2,   // | X2           |
  x3   =   3,   // | X3           |
  x4   =   4,   // | X4           |
  x5   =   5,   // | X5           |
  x6   =   6,   // | X6           |
  x7   =   7,   // | X7           |
  x8   =   8,   // | X8 (XR)      |
  x9   =   9,   // | X9           |
  x10  =  10,   // | X10          |
  x11  =  11,   // | X11          |
  x12  =  12,   // | X12          |
  x13  =  13,   // | X13          |
  x14  =  14,   // | X14          |
  x15  =  15,   // | X15          |
  x16  =  16,   // | X16 (IP0)    |
  x17  =  17,   // | X17 (IP1)    |
  x18  =  18,   // | X18 (PR)     |
  x19  =  19,   // | X19          |
  x20  =  20,   // | X20          |
  x21  =  21,   // | X21          |
  x22  =  22,   // | X22          |
  x23  =  23,   // | X23          |
  x24  =  24,   // | X24          |
  x25  =  25,   // | X25          |
  x26  =  26,   // | X26          |
  x27  =  27,   // | X27          |
  x28  =  28,   // | X28          |
  x29  =  29,   // | X29 (FP)     |
  x30  =  30,   // | X30 (LR)     |
  x31  =  31,   // | X31 (SP/XZR) |
  reg_invalid = 32
};              // +--------------+

enum reg_alt {
  xr   =  x8,   // Designated Indirect Result Location Parameter
  ip0  =  x16,  // Intra-Procedure Call temporary registers
  ip1  =  x17,  // Intra-Procedure Call temporary registers
  pr   =  x18,  // Platform Register
  es   =  x19,  // the first calleE-Saved register - not a standard alias
  fp   =  x29,  // Frame Pointer
  lr   =  x30,  // Link register
  sp   =  x31,  // Stack Pointer
  xzr  =  x31,  // Zero Register
};

#define m_x0 (1 << x0)
#define m_x1 (1 << x1)
#define m_x2 (1 << x2)
#define m_x3 (1 << x3)
#define m_x4 (1 << x4)
#define m_x5 (1 << x5)
#define m_x6 (1 << x6)
#define m_x7 (1 << x7)
#define m_x8 (1 << x8)
#define m_x9 (1 << x9)
#define m_x10 (1 << x10)
#define m_x11 (1 << x11)
#define m_x12 (1 << x12)
#define m_x13 (1 << x13)
#define m_x14 (1 << x14)
#define m_x15 (1 << x15)
#define m_x16 (1 << x16)
#define m_x17 (1 << x17)
#define m_x18 (1 << x18)
#define m_x19 (1 << x19)
#define m_x20 (1 << x20)
#define m_x21 (1 << x21)
#define m_x22 (1 << x22)
#define m_x23 (1 << x23)
#define m_x24 (1 << x24)
#define m_x25 (1 << x25)
#define m_x26 (1 << x26)
#define m_x27 (1 << x27)
#define m_x28 (1 << x28)
#define m_x29 (1 << x29)
#define m_x30 (1 << x30)
#define m_x31 (1 << x31)

#define m_xr  (1 << xr)
#define m_ip0 (1 << ip0)
#define m_ip1 (1 << ip1)
#define m_pr  (1 << pr)
#define m_xzr (1 << xzr)
#endif

enum reg_portable {
  reg0 = 0,
  reg1 = 1,
  reg2 = 2,
  reg3 = 3,
  reg4 = 4,
  reg5 = 5,
  reg6 = 6,
  reg7 = 7,
  reg8 = 8,
  reg9 = 9,
  reg10 = 10,
  reg11 = 11,
  reg12 = 12
};

typedef enum arm_cond_codes {
  EQ = 0,
  NE = 1,
  CS = 2,
  CC = 3,
  MI = 4,
  PL = 5,
  VS = 6,
  VC = 7,
  HI = 8,
  LS = 9,
  GE = 10,
  LT = 11,
  GT = 12,
  LE = 13,
  AL = 14,
  ALT = 15
} mambo_cond;

enum shift_type {
  LSL = 0,
  LSR = 1,
  ASR = 2,
  ROR = 3
};

extern enum arm_cond_codes arm_inverse_cond_code[];

#define invert_cond(cond) ((cond) ^ 1)

#define arm_cond_push_reg(cond, reg) \
  arm_str_cond(&write_p, cond, IMM_LDR, reg, sp, 4, 1, 0, 1); \
  write_p++;

#define arm_cond_pop_reg(cond, reg) \
  arm_ldr_cond(&write_p, cond, IMM_LDR, reg, sp, 4, 0, 1, 0); \
  write_p++;

#define arm_push_reg(reg) \
  arm_str(&write_p, IMM_LDR, reg, sp, 4, 1, 0, 1); \
  write_p++;

#define arm_pop_reg(reg) \
  arm_ldr(&write_p, IMM_LDR, reg, sp, 4, 0, 1, 0); \
  write_p++;

#define arm_push_regs(regs) \
  arm_stm(&write_p, sp, regs, 1, 0, 1, 0); \
  write_p++;

#define arm_pop_regs(regs) \
  if ((regs) & (1 << sp)) { \
    arm_ldm(&write_p, sp, regs, 0, 1, 0, 0); \
  } else { \
    arm_ldm(&write_p, sp, regs, 0, 1, 1, 0); \
  } \
  write_p++;

/*
 * PUSH PAIR
 * STP Xt1, Xt2, [SP]!
 */
#define a64_push_pair_reg(Xt1, Xt2) \
  a64_LDP_STP(&write_p, 2, 0, 3, 0, -2, Xt2, sp, Xt1); \
  write_p++;

/*
 * POP PAIR
 * LDP Xt1, Xt2, [SP], #16
 */
#define a64_pop_pair_reg(Xt1, Xt2) \
  a64_LDP_STP(&write_p, 2, 0, 1, 1, 2, Xt2, sp, Xt1); \
  write_p++;

/*
 * PUSH REGISTER
 * STR reg, [SP, #-16]!
 */
#define a64_push_reg(reg) \
  a64_LDR_STR_immed(&write_p, 3, 0, 0, -16, 3, sp, reg); \
  write_p++;

/*
 * POP REGISTER
 * LDR reg, [SP], #16
 */
#define a64_pop_reg(reg) \
  a64_LDR_STR_immed(&write_p, 3, 0, 1, 16, 1, sp, reg); \
  write_p++;

void copy_to_reg_16bit(uint16_t **write_p, enum reg reg, uint32_t value);
void copy_to_reg_32bit(uint16_t **write_p, enum reg reg, uint32_t value);
void a64_copy_to_reg_64bits(uint32_t **write_p, enum reg reg, uint64_t value);

void thumb_push_regs(uint16_t **write_p, uint32_t regs);
void thumb_pop_regs(uint16_t **write_p, uint32_t regs);
void arm_copy_to_reg_16bit(uint32_t **write_p, enum reg reg, uint32_t value);
void arm_cond_copy_to_reg_16bit(uint32_t **write_p, enum arm_cond_codes cond, enum reg reg, uint32_t value);
void arm_copy_to_reg_32bit(uint32_t **write_p, enum reg reg, uint32_t value);
void arm_cond_copy_to_reg_32bit(uint32_t **write_p, enum arm_cond_codes cond, enum reg reg, uint32_t value);
void arm_add_sub_32_bit(uint32_t **write_p, enum reg rd, enum reg rn, int value);

void init_plugin();

void mambo_memcpy(void *dst, void *src, size_t l);

static inline uint64_t sign_extend64(unsigned int bits, uint64_t value)
{
    uint64_t C = (-1) << (bits - (uint64_t) 1);
    return (value + C) ^ C;
}

static inline int32_t sign_extend32(unsigned int bits, uint32_t value)
{
  uint32_t C = (-1) << (bits - 1);
  return (int32_t)((value + C) ^ C);
}
#endif


```

`signals.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>
#include <signal.h>
#include <assert.h>
#include <unistd.h>
#include <asm/unistd.h>
#include <string.h>

#include "dbm.h"
#include "scanner_common.h"
#ifdef __arm__
#include "pie/pie-thumb-encoder.h"
#include "pie/pie-thumb-decoder.h"
#include "pie/pie-thumb-field-decoder.h"
#include "pie/pie-arm-encoder.h"
#include "pie/pie-arm-decoder.h"
#include "pie/pie-arm-field-decoder.h"
#endif
#ifdef __aarch64__
#include "pie/pie-a64-encoder.h"
#include "pie/pie-a64-decoder.h"
#include "pie/pie-a64-field-decoder.h"
#endif

#define self_send_signal_offset        ((uintptr_t)send_self_signal - (uintptr_t)&start_of_dispatcher_s)
#define syscall_wrapper_svc_offset     ((uintptr_t)syscall_wrapper_svc - (uintptr_t)&start_of_dispatcher_s)

#define SIGNAL_TRAP_IB (0x94)
#define SIGNAL_TRAP_DB (0x95)

#ifdef __arm__
  #define pc_field uc_mcontext.arm_pc
  #define sp_field uc_mcontext.arm_sp
#elif __aarch64__
  #define pc_field uc_mcontext.pc
  #define sp_field uc_mcontext.sp
#endif

typedef struct {
  uintptr_t pid;
  uintptr_t tid;
  uintptr_t signo;
} self_signal;

void install_system_sig_handlers() {
  struct sigaction act;
  act.sa_sigaction = signal_trampoline;
  sigemptyset(&act.sa_mask);
  act.sa_flags = SA_SIGINFO;
  int ret = sigaction(UNLINK_SIGNAL, &act, NULL);
  assert(ret == 0);
}

int deliver_signals(uintptr_t spc, self_signal *s) {
  uint64_t sigmask;

  if (global_data.exit_group) {
    thread_abort(current_thread);
  }

  int ret = syscall(__NR_rt_sigprocmask, 0, NULL, &sigmask, sizeof(sigmask));
  assert (ret == 0);

  for (int i = 0; i < _NSIG; i++) {
    if ((sigmask & (1 << i)) == 0
        && atomic_decrement_if_positive_i32(&current_thread->pending_signals[i], 1) >= 0) {
      s->pid = syscall(__NR_getpid);
      s->tid = syscall(__NR_gettid);
      s->signo = i;
      atomic_increment_u32(&current_thread->is_signal_pending, -1);
      return 1;
    }
  }

  return 0;
}

typedef int (*inst_decoder)(void *);
#ifdef __arm__
  #define inst_size(inst, is_thumb) (((is_thumb) && ((inst) < THUMB_ADC32)) ? 2 : 4)
  #define write_trap(code)  if (is_thumb) { \
                              thumb_udf16((uint16_t **)&write_p, (code)); \
                              write_p += 2; \
                            } else { \
                              arm_udf((uint32_t **)&write_p, (code) >> 4, (code) & 0xF); \
                              write_p += 4; \
                            }
  #define TRAP_INST_TYPE ((is_thumb) ? THUMB_UDF16 : ARM_UDF)
#elif __aarch64__
  #define inst_size(inst, is_thumb) (4)
  #define write_trap(code) a64_HVC((uint32_t **)&write_p, (code)); write_p += 4;
  #define TRAP_INST_TYPE (A64_HVC)
#endif

bool unlink_indirect_branch(dbm_code_cache_meta *bb_meta, void **o_write_p) {
  int br_inst_type, trap_inst_type;
  inst_decoder decoder;
  void *write_p = *o_write_p;
  bool __attribute__((unused)) is_thumb = false;
#ifdef __arm__
  if (bb_meta->exit_branch_type == uncond_reg_thumb) {
    is_thumb = true;
    br_inst_type = THUMB_BX16;
    decoder = (inst_decoder)thumb_decode;
  } else if (bb_meta->exit_branch_type == uncond_reg_arm) {
    br_inst_type = ARM_BX;
    decoder = (inst_decoder)arm_decode;
  }
#elif __aarch64__
  br_inst_type = A64_BR;
  decoder = (inst_decoder)a64_decode;
#endif
  trap_inst_type = TRAP_INST_TYPE;

  int inst = decoder(write_p);
  while(inst != br_inst_type && inst != trap_inst_type) {
    write_p += inst_size(inst, is_thumb);
    inst = decoder(write_p);
  }

  if (inst == trap_inst_type) {
    return false;
  }

  write_trap(SIGNAL_TRAP_IB);
  *o_write_p = write_p;
  return true;
}

int get_direct_branch_exit_trap_sz(dbm_code_cache_meta *bb_meta, int fragment_id) {
  int sz;
  switch(bb_meta->exit_branch_type) {
#ifdef __arm__
    case cond_imm_thumb:
    case cbz_thumb:
      sz = (bb_meta->branch_cache_status & BOTH_LINKED) ? 10 : 6;
      break;
    case cond_imm_arm:
      sz = (bb_meta->branch_cache_status & BOTH_LINKED) ? 8 : 4;
      break;
#elif __aarch64__
    case uncond_imm_a64:
      sz = 4;
      break;
    case cond_imm_a64:
    case cbz_a64:
    case tbz_a64:
      if (fragment_id >= CODE_CACHE_SIZE) {
        // a single branch is inserted for a conditional exit in a trace
        // however a second branch may follow for an early exit to an existing trace
        sz = 8;
      } else {
        sz = (bb_meta->branch_cache_status & BOTH_LINKED) ? 12 : 8;
      }
      break;
#endif
    default:
      while(1);
  }
  return sz;
}

bool unlink_direct_branch(dbm_code_cache_meta *bb_meta, void **o_write_p, int fragment_id, uintptr_t pc) {
  int offset = 0;
  bool __attribute__((unused)) is_thumb = false;
  void *write_p = *o_write_p;

  offset = get_direct_branch_exit_trap_sz(bb_meta, fragment_id);

  if (pc < ((uintptr_t)bb_meta->exit_branch_addr + offset)) {
    if (bb_meta->branch_cache_status != 0) {
      inst_decoder decoder;

#ifdef __arm__
      is_thumb = (bb_meta->exit_branch_type == cond_imm_thumb) || (bb_meta->exit_branch_type == cbz_thumb);
      if (is_thumb) {
        decoder = (inst_decoder)thumb_decode;
      } else {
        decoder = (inst_decoder)arm_decode;
      }
#elif __aarch64__
      decoder = (inst_decoder)a64_decode;
#endif
      int inst = decoder(write_p);
      if (inst == TRAP_INST_TYPE) {
        return false;
      }
      memcpy(&bb_meta->saved_exit, write_p, offset);
      for (int i = 0; i < offset; i += inst_size(TRAP_INST_TYPE, is_thumb)) {
        write_trap(SIGNAL_TRAP_DB);
      }
    } // if (bb_meta->branch_cache_status != 0)
  } else {
    /* It's already setting up a call to the dispatcher. Ensure that the
       fragment is not supposed to be linked */
    assert((bb_meta->branch_cache_status & BOTH_LINKED) == 0);
    return false;
  }

  *o_write_p = write_p;
  return true;
}

void unlink_fragment(int fragment_id, uintptr_t pc) {
  dbm_code_cache_meta *bb_meta;

#ifdef DBM_TRACES
  // Skip over trace fragments with elided unconditional branches
  branch_type type;

  do {
    bb_meta = &current_thread->code_cache_meta[fragment_id];
    type = bb_meta->exit_branch_type;
    fragment_id++;
  }
  #ifdef __arm__
  while ((type == uncond_imm_arm || type == uncond_imm_thumb ||
          type == uncond_blxi_thumb || type == uncond_blxi_arm) &&
  #elif __aarch64__
  while (type == uncond_imm_a64 &&
  #endif
         (bb_meta->branch_cache_status & BOTH_LINKED) == 0 &&
         fragment_id >= CODE_CACHE_SIZE &&
         fragment_id < current_thread->active_trace.id);

  fragment_id--;
  // If the fragment isn't installed, make sure it's active
  if (fragment_id >= current_thread->trace_id) {
    assert(current_thread->active_trace.active);
  }
#else
  bb_meta = &current_thread->code_cache_meta[fragment_id];
#endif

#ifdef __aarch64__
  // we don't try to unlink trace exits, we unlink the fragment they jump to
  if (bb_meta->exit_branch_type == trace_exit) {
    fragment_id = addr_to_fragment_id(current_thread, bb_meta->branch_taken_addr);
    bb_meta = &current_thread->code_cache_meta[fragment_id];
    pc = bb_meta->tpc;
  }
#endif

  void *write_p = bb_meta->exit_branch_addr;
  void *start_addr = write_p;

#ifdef __arm__
  if (bb_meta->exit_branch_type == uncond_reg_thumb ||
      bb_meta->exit_branch_type == uncond_reg_arm) {
#elif __aarch64__
  if (bb_meta->exit_branch_type == uncond_branch_reg) {
#endif
    if (!unlink_indirect_branch(bb_meta, &write_p)) {
      return;
    }
  } else if (bb_meta->branch_cache_status != 0) {
    if (!unlink_direct_branch(bb_meta, &write_p, fragment_id, pc)) {
      return;
    }
  }

  __clear_cache(start_addr, write_p);
}

void translate_delayed_signal_frame(ucontext_t *cont) {
  uintptr_t *sp = (uintptr_t *)cont->sp_field;
#ifdef __arm__
  /*
         r7
         r1
         r2
         PID
         TID
         SIGNO
         R0
         TPC
         SPC
  */
  cont->uc_mcontext.arm_r7 = sp[0];
  cont->uc_mcontext.arm_r1 = sp[1];
  cont->uc_mcontext.arm_r2 = sp[2];
  cont->uc_mcontext.arm_r0 = sp[6];
  cont->uc_mcontext.arm_pc = sp[8];

  sp += 9;
#elif __aarch64__
  /*
    TPC, SPC
    X2, X8
    X0, X1
  */
  cont->uc_mcontext.regs[x8] = sp[3];
  cont->uc_mcontext.regs[x2] = sp[2];
  cont->uc_mcontext.pc = sp[1];
  cont->uc_mcontext.regs[x0] = sp[4];
  cont->uc_mcontext.regs[x1] = sp[5];
  sp += 6;
#endif

  cont->sp_field = (uintptr_t)sp;
}

void translate_svc_frame(ucontext_t *cont) {
  uintptr_t *sp = (uintptr_t *)cont->sp_field;
#ifdef __arm__
  cont->uc_mcontext.arm_r8  = sp[8];
  cont->uc_mcontext.arm_r9  = sp[9];
  cont->uc_mcontext.arm_r10 = sp[10];
  cont->uc_mcontext.arm_fp  = sp[11];
  cont->uc_mcontext.arm_ip  = sp[12];
  cont->uc_mcontext.arm_lr  = sp[13];
  cont->uc_mcontext.arm_pc  = sp[15];
  sp += 16;
#elif __aarch64__
  #define FPSIMD_SIZE  (0x210)

  assert(cont->uc_mcontext.regs[x8] != __NR_rt_sigreturn);
  struct fpsimd_context *fpstate = (struct fpsimd_context *)&cont->uc_mcontext.__reserved;

  // Set up the FP state first
  assert(fpstate->head.magic == FPSIMD_MAGIC && fpstate->head.size == FPSIMD_SIZE);
  memcpy(fpstate->vregs, sp, sizeof(fpstate->vregs));
  fpstate->fpsr = cont->uc_mcontext.regs[x21];
  fpstate->fpcr = cont->uc_mcontext.regs[x20];
  sp += 512 / sizeof(sp[0]);

  // Now set the general purpose registers  & PSTATE
  cont->uc_mcontext.pstate = cont->uc_mcontext.regs[x19];
  for (int r = 9; r <= 21; r++) {
    cont->uc_mcontext.regs[r] = sp[r];
  }
  cont->uc_mcontext.pc = sp[23];
  cont->uc_mcontext.regs[x29] = sp[24];
  cont->uc_mcontext.regs[x30] = sp[25];
  sp += 26;
#endif
  cont->sp_field = (uintptr_t)sp;
}

#define PSTATE_N (1 << 31)
#define PSTATE_Z (1 << 30)
#define PSTATE_C (1 << 29)
#define PSTATE_V (1 << 28)
bool interpret_condition(uint32_t pstate, mambo_cond cond) {
  assert(cond >= 0 && cond <= 0xF);
  bool state = true;
  switch (cond >> 1) {
    case 0:
      state = pstate & PSTATE_Z;
      break;
    case 1:
      state = pstate & PSTATE_C;
      break;
    case 2:
      state = pstate & PSTATE_N;
      break;
    case 3:
      state = pstate & PSTATE_V;
      break;
    case 4:
      state = (pstate & PSTATE_C) && ((pstate & PSTATE_Z) == 0);
      break;
    case 5:
      state = ((pstate & PSTATE_N) ? true : false) == ((pstate & PSTATE_V) ? true : false);
      break;
    case 6:
      state = ((pstate & PSTATE_N) ? true : false) == ((pstate & PSTATE_V) ? true : false);
      state = state && ((pstate & PSTATE_Z) == 0);
      break;
    case 7:
      state = true;
      break;
  }

  state = state ? true : false;

  if (cond < 14 && (cond & 1)) {
    state = !state;
  }

  return state;
}

#ifdef __aarch64__
bool interpret_cbz(ucontext_t *cont, dbm_code_cache_meta *bb_meta) {
  int reg = (bb_meta->rn) & 0x1F;
  uint64_t val = cont->uc_mcontext.regs[reg];
  if (bb_meta->rn & (1 << 5)) {
    val &= 0xFFFFFFFF;
  }

  return (val == 0) ^ (bb_meta->branch_condition);
}

bool interpret_tbz(ucontext_t *cont, dbm_code_cache_meta *bb_meta) {
  int reg = (bb_meta->rn) & 0x1F;
  int bit = (bb_meta->rn) >> 5;
  bool is_taken = (cont->uc_mcontext.regs[reg] & (1 << bit)) == 0;

  return is_taken ^ bb_meta->branch_condition;
}
#endif

#ifdef __arm__
  #define direct_branch(write_p, target, cond)  if (is_thumb) { \
                                                  thumb_b32_helper((write_p), (target)); \
                                                } else { \
                                                  arm_b32_helper((write_p), (target), cond); \
                                                }
#elif __aarch64__
  #define direct_branch(write_p, target, cond)  a64_b_helper((write_p), (target) + 4);
#endif

#ifdef __arm__
void restore_exit(dbm_thread *thread_data, int fragment_id, void **o_write_p, bool is_thumb) {
#elif __aarch64__
void restore_exit(dbm_thread *thread_data, int fragment_id, void **o_write_p) {
#endif
  void *write_p = *o_write_p;
  dbm_code_cache_meta *bb_meta = &thread_data->code_cache_meta[fragment_id];

  int restore_sz = get_direct_branch_exit_trap_sz(bb_meta, fragment_id);
  memcpy(write_p, &bb_meta->saved_exit, restore_sz);
  write_p += restore_sz;

  *o_write_p = write_p;
}

void restore_ihl_regs(ucontext_t *cont) {
  uintptr_t *sp = (uintptr_t *)cont->sp_field;

#ifdef __arm__
  cont->context_reg(5) = sp[0];
  cont->context_reg(6) = sp[1];
#elif __aarch64__
  cont->context_reg(0) = sp[0];
  cont->context_reg(1) = sp[1];
#endif
  sp += 2;

  cont->sp_field = (uintptr_t)sp;
}

void sigret_dispatcher_call(dbm_thread *thread_data, ucontext_t *cont, uintptr_t target) {
  uintptr_t *sp = (uintptr_t *)cont->context_sp;

#ifdef __arm__
  sp -= DISP_SP_OFFSET / 4;
#elif __aarch64__
  sp -= 2;
#endif
  sp[0] = cont->context_reg(0);
  sp[1] = cont->context_reg(1);
#ifdef __arm__
  sp[2] = cont->context_reg(2);
  sp[3] = cont->context_reg(3);
#endif
  cont->context_reg(0) = target;
  cont->context_reg(1) = 0;
  cont->context_pc = thread_data->dispatcher_addr;
#ifdef __arm__
  cont->context_reg(3) = cont->context_sp;
  cont->uc_mcontext.arm_cpsr &= ~CPSR_T;
#endif

  cont->context_sp = (uintptr_t)sp;
}

#ifdef __arm__
  #define restore_ihl_inst(addr)  if (is_thumb) { \
                                    thumb_bx16((uint16_t **)&addr, r6); \
                                    __clear_cache((void *)addr, (void *)addr + 2); \
                                  } else { \
                                    arm_bx((uint32_t **)&addr, r6); \
                                    __clear_cache((void *)addr, (void *)addr + 4); \
                                  }

#elif __aarch64__
  #define restore_ihl_inst(addr) a64_BR((uint32_t **)&addr, x0); \
                                __clear_cache((void *)addr, (void *)addr + 4);
#endif

/* If type == indirect && pc >= exit, read the pc and deliver the signal */
/* If pc < <type specific>, unlink the fragment and resume execution */
uintptr_t signal_dispatcher(int i, siginfo_t *info, void *context) {
  uintptr_t handler = 0;
  bool deliver_now = false;

  assert(i >= 0 && i < _NSIG);
  ucontext_t *cont = (ucontext_t *)context;

  uintptr_t pc = (uintptr_t)cont->pc_field;
  uintptr_t cc_start = (uintptr_t)&current_thread->code_cache->blocks[trampolines_size_bbs];
  uintptr_t cc_end = cc_start + MAX_BRANCH_RANGE;

  if (global_data.exit_group > 0) {
    if (pc >= cc_start && pc < cc_end) {
      int fragment_id = addr_to_fragment_id(current_thread, (uintptr_t)pc);
      dbm_code_cache_meta *bb_meta = &current_thread->code_cache_meta[fragment_id];
      if (pc >= (uintptr_t)bb_meta->exit_branch_addr) {
        thread_abort(current_thread);
      }
      unlink_fragment(fragment_id, pc);
    }
    atomic_increment_u32(&current_thread->is_signal_pending, 1);
    return 0;
  }

  if (pc == ((uintptr_t)current_thread->code_cache + self_send_signal_offset)) {
    translate_delayed_signal_frame(cont);
    deliver_now = true;
  } else if (pc == ((uintptr_t)current_thread->code_cache + syscall_wrapper_svc_offset)) {
    translate_svc_frame(cont);
    deliver_now = true;
  }

  if (deliver_now) {
    handler = lookup_or_scan(current_thread, global_data.signal_handlers[i]);
    return handler;
  }

  if (pc >= cc_start && pc < cc_end) {
    int fragment_id = addr_to_fragment_id(current_thread, (uintptr_t)pc);
    dbm_code_cache_meta *bb_meta = &current_thread->code_cache_meta[fragment_id];

    if (pc >= (uintptr_t)bb_meta->exit_branch_addr) {
      void *write_p;

      if (i == UNLINK_SIGNAL) {
        uint32_t imm;
#ifdef __arm__
        bool is_thumb = cont->uc_mcontext.arm_cpsr & CPSR_T;
        if (is_thumb) {
          thumb_udf16_decode_fields((uint16_t *)pc, &imm);
        } else {
          uint32_t imm12, imm4;
          arm_udf_decode_fields((uint32_t *)pc, &imm12, &imm4);
          imm = (imm12 << 4) | imm4;
        }
#elif __aarch64__
        a64_HVC_decode_fields((uint32_t *)pc, &imm);
#endif
        if (imm == SIGNAL_TRAP_IB) {
          restore_ihl_inst(pc);

          int rn = current_thread->code_cache_meta[fragment_id].rn;
          uintptr_t target;
#ifdef __arm__
          unsigned long *regs = &cont->uc_mcontext.arm_r0;
          target = regs[rn];
#elif __aarch64__
          target = cont->uc_mcontext.regs[rn];
#endif
          restore_ihl_regs(cont);
          sigret_dispatcher_call(current_thread, cont, target);
          return 0;
        } else if (imm == SIGNAL_TRAP_DB) {
          write_p = bb_meta->exit_branch_addr;
          void *start_addr = write_p;
#ifdef __arm__
          restore_exit(current_thread, fragment_id, &write_p, is_thumb);
#elif __aarch64__
          restore_exit(current_thread, fragment_id, &write_p);
#endif
          __clear_cache(start_addr, write_p);

          bool is_taken;
          switch(bb_meta->exit_branch_type) {
#ifdef __arm__
            case cond_imm_thumb:
            case cond_imm_arm:
              is_taken = interpret_condition(cont->uc_mcontext.arm_cpsr, bb_meta->branch_condition);
              break;
            case cbz_thumb: {
              unsigned long *regs = &cont->uc_mcontext.arm_r0;
              is_taken = regs[bb_meta->rn] == 0;
              break;
            }
#elif __aarch64__
            case uncond_imm_a64:
              is_taken = true;
              break;
            case cond_imm_a64:
              is_taken = interpret_condition(cont->uc_mcontext.pstate, bb_meta->branch_condition);
              break;
            case cbz_a64:
              is_taken = interpret_cbz(cont, bb_meta);
              break;
            case tbz_a64:
              is_taken = interpret_tbz(cont, bb_meta);
              break;
#endif
            default:
              fprintf(stderr, "Signal: interpreting of %d fragments not implemented\n", bb_meta->exit_branch_type);
              while(1);
          }

          // Set up *sigreturn* to the dispatcher
          sigret_dispatcher_call(current_thread, cont,
                                 is_taken ? bb_meta->branch_taken_addr : bb_meta->branch_skipped_addr);
          return 0;
        } else {
          fprintf(stderr, "Error: unknown MAMBO trap code\n");
          while(1);
        }
      } // i == UNLINK_SIGNAL
    } // if (pc >= (uintptr_t)bb_meta->exit_branch_addr)
    unlink_fragment(fragment_id, pc);
  }

  /* Call the handlers of synchronous signals immediately
     The SPC of the instruction is unknown, so sigreturning to addresses derived
     from the PC value in the signal frame is not supported.
     We mangle the PC in the context to hopefully trap such attempts.
  */
  if (i == SIGSEGV || i == SIGBUS || i == SIGFPE || i == SIGTRAP || i == SIGILL || i == SIGSYS) {
    handler = global_data.signal_handlers[i];

    if (pc < cc_start || pc >= cc_end) {
      fprintf(stderr, "Synchronous signal outside the code cache\n");
      while(1);
    }

    // Check if the application actually has a handler installed for the signal used by MAMBO
    if (handler == (uintptr_t)SIG_IGN || handler == (uintptr_t)SIG_DFL) {
      assert(i == UNLINK_SIGNAL);

      // Remove this handler
      struct sigaction act;
      act.sa_sigaction = (void *)handler;
      sigemptyset(&act.sa_mask);
      int ret = sigaction(i, &act, NULL);
      assert(ret == 0);

      // sigreturn so the same signal is raised again without an installed signal handler
      return 0;
    }

    cont->pc_field = 0;
    handler = lookup_or_scan(current_thread, handler);
    return handler;
  }

  atomic_increment_int(&current_thread->pending_signals[i], 1);
  atomic_increment_u32(&current_thread->is_signal_pending, 1);

  return handler;
}

```

`syscalls.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>
#include <asm/unistd.h>
#include <pthread.h>
#include <sys/mman.h>
#include <unistd.h>
#include <linux/sched.h>
#include <assert.h>
#include <limits.h>
#include <string.h>
#include <errno.h>
#include <sys/types.h>
#include <sys/shm.h>
#include <inttypes.h>

#include "dbm.h"
#include "kernel_sigaction.h"
#include "scanner_common.h"
#include "syscalls.h"

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
#else
  #define debug(...)
#endif

#ifdef __aarch64__
  #define SIG_FRAG_OFFSET 4
#else
  #define SIG_FRAG_OFFSET 0
#endif

void *dbm_start_thread_pth(void *ptr, void *mambo_sp) {
  dbm_thread *thread_data = (dbm_thread *)ptr;
  assert(thread_data->clone_args->child_stack);
  current_thread = thread_data;
  current_thread->mambo_sp = mambo_sp;

  pid_t tid = syscall(__NR_gettid);
  thread_data->tid = tid;
  if (thread_data->clone_args->flags & CLONE_PARENT_SETTID) {
    *thread_data->clone_args->ptid = tid;
  }
  if (thread_data->clone_args->flags & CLONE_CHILD_SETTID) {
    *thread_data->clone_args->ctid = tid;
  }
  if (thread_data->clone_args->flags & CLONE_CHILD_CLEARTID) {
		syscall(__NR_set_tid_address, thread_data->clone_args->ctid);
  }
  thread_data->tls = thread_data->clone_args->tls;

  // Copy the parent's saved register values to the child's stack
#ifdef __arm__
  uint32_t *child_stack = thread_data->clone_args->child_stack;
  child_stack -= 15; // reserve 15 words on the child's stack
  mambo_memcpy(child_stack, thread_data->clone_args, sizeof(uintptr_t) * 14);
  child_stack[r0] = 0; // return 0
#endif
#ifdef __aarch64__
  uint64_t *child_stack = thread_data->clone_args->child_stack;
  child_stack -= 34;
  mambo_memcpy(child_stack, (void *)thread_data->clone_args, sizeof(uintptr_t) * 32);
  // move the values for X0 and X1 to the bottom of the stack
  child_stack[32] = 0; // X0
  child_stack[33] = child_stack[1]; // X1
  child_stack += 2;
#endif

  // Release the lock
  asm volatile("DMB SY" ::: "memory");
  *(thread_data->set_tid) = tid;

  assert(register_thread(thread_data, false) == 0);

  uintptr_t addr = scan(thread_data, thread_data->clone_ret_addr, ALLOCATE_BB);
  th_enter(child_stack, addr);

  return NULL;
}

dbm_thread *dbm_create_thread(dbm_thread *thread_data, void *next_inst, sys_clone_args *args, volatile pid_t *set_tid) {
  pthread_t thread;
  dbm_thread *new_thread_data;

  if (!allocate_thread_data(&new_thread_data)) {
    fprintf(stderr, "Failed to allocate thread data\n");
    while(1);
  }
  init_thread(new_thread_data);
  new_thread_data->clone_ret_addr = next_inst;
  new_thread_data->set_tid = set_tid;
  new_thread_data->clone_args = args;

  pthread_attr_t attr;
  pthread_attr_init(&attr);
  pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
  pthread_attr_setinheritsched(&attr, PTHREAD_INHERIT_SCHED);
  /* We're switching to the stack allocated by the application immediately, so make this
     as small as possible. Our glibc stores data here, so we can't unmap it.
     Also see man pthread_attr_setguardsize BUGS. */
  pthread_attr_setstacksize(&attr, PTHREAD_STACK_MIN + 4096);
  pthread_attr_setguardsize(&attr, 4096);
  pthread_create(&thread, &attr, new_thread_trampoline, new_thread_data);

  return new_thread_data;
}

uintptr_t emulate_brk(uintptr_t addr) {
  int ret;

  // Fast path
  if (addr == 0 || addr == global_data.brk) {
    return global_data.brk;
  }

  ret = pthread_mutex_lock(&global_data.brk_mutex);
  assert(ret == 0);

  /* We use mremap for non-overlapping re-allocation, therefore
     we must always always keep at least one allocated page. */
  if (addr >= (global_data.initial_brk + PAGE_SIZE)) {
    void *map = mremap((void *)global_data.initial_brk,
                       global_data.brk - global_data.initial_brk,
                       addr - global_data.initial_brk, 0);
    if (map != MAP_FAILED) {
      vm_op_t op = VM_MAP;
      size_t size = addr - global_data.brk;
      if (addr < global_data.brk) {
        op = VM_UNMAP;
        size = global_data.brk - addr;
      }
      notify_vm_op(op, min(addr, global_data.brk), size, PROT_READ | PROT_WRITE,
                   MAP_PRIVATE|MAP_ANONYMOUS|MAP_FIXED, -1, 0);
      global_data.brk = addr;
    }
  }

  ret = pthread_mutex_unlock(&global_data.brk_mutex);
  assert(ret == 0);

  return global_data.brk;
}

ssize_t readlink_handler(char *sys_path, char *sys_buf, ssize_t bufsize) {
  const int proc_buflen = 100;
  char buf[proc_buflen];
  snprintf(buf, proc_buflen, "/proc/%d/exe", getpid());
  if (strcmp(sys_path, buf) == 0 ||
      strcmp(sys_path, "/proc/self/exe") == 0 ||
      strcmp(sys_path, "/proc/thread-self/exe") == 0) {
    char path[PATH_MAX];
    char *rp = realpath(global_data.argv[1], path);
    assert(rp != NULL);
    size_t path_len = strlen(rp);

    /* realpath() null-terminates strings, while readlinkat shouldn't.
       Therefore, if PATH_MAX has been filled and bufsize == PATH_MAX, then it's possible
       that we've lost a valid last character which realpath set to null. */
    assert((bufsize < PATH_MAX) || (path_len < (PATH_MAX - 1)));

    strncpy(sys_buf, path, bufsize);
    return min(path_len, bufsize);
  }

  return -1;
}

int syscall_handler_pre(uintptr_t syscall_no, uintptr_t *args, uint16_t *next_inst, dbm_thread *thread_data) {
  int do_syscall = 1;
  sys_clone_args *clone_args;
  debug("syscall pre %" PRIdPTR "\n", syscall_no);

#ifdef PLUGINS_NEW
  mambo_context ctx;

  if (global_data.free_plugin > 0) {
    set_mambo_context_syscall(&ctx, thread_data, PRE_SYSCALL_C, syscall_no, args);
    mambo_deliver_callbacks_for_ctx(&ctx);
  }

  if (ctx.syscall.replace) {
    do_syscall = 0;
    args[0] = ctx.syscall.ret;
  } else {
#endif

  switch(syscall_no) {
    case __NR_brk:
      args[0] = emulate_brk(args[0]);
      do_syscall = 0;
      break;
    case __NR_clone:
      clone_args = (sys_clone_args *)args;

      if (clone_args->flags & CLONE_THREAD) {
        assert(clone_args->flags & CLONE_VM);
        if (!(clone_args->flags & CLONE_SETTLS)) {
          clone_args->tls = thread_data->tls;
        }
        thread_data->clone_vm = true;

        volatile pid_t child_tid = 0;
        dbm_create_thread(thread_data, next_inst, clone_args, &child_tid);
        while(child_tid == 0);
        asm volatile("DMB SY" ::: "memory");
        args[0] = child_tid;

        do_syscall = 0;
        break;
      }

      if (clone_args->flags & CLONE_VFORK) {
        clone_args->flags &= ~CLONE_VM;
      }
      assert((clone_args->flags & CLONE_VM) == 0);
      thread_data->clone_vm = false;

      thread_data->child_tls = (clone_args->flags & CLONE_SETTLS) ? clone_args->tls : thread_data->tls;
      clone_args->flags &= ~CLONE_SETTLS;

      if (clone_args->child_stack != NULL) {
        if (clone_args->child_stack == &args[SYSCALL_WRAPPER_STACK_OFFSET]) {
          clone_args->child_stack = NULL;
        } else {
          const size_t copy_size = SYSCALL_WRAPPER_FRAME_SIZE * sizeof(uintptr_t);
          clone_args->child_stack -= copy_size;
          void *source = args + SYSCALL_WRAPPER_STACK_OFFSET - SYSCALL_WRAPPER_FRAME_SIZE;
          mambo_memcpy(clone_args->child_stack, source, copy_size);
        }
      } // if child_stack != NULL
      break;
    case __NR_exit:
      debug("thread exit\n");
      void *sp = thread_data->mambo_sp;
      assert(unregister_thread(thread_data, false) == 0);
      assert(free_thread_data(thread_data) == 0);

      return_with_sp(sp); // this should never return
      while(1); 
      break;
#ifdef __arm__
    case __NR_sigaction:
      fprintf(stderr, "check sigaction()\n");
      while(1);
#endif
    case __NR_rt_sigaction: {
      uintptr_t handler = 0xdead;
      assert(args[3] == 8 && args[0] >= 0 && args[0] < _NSIG);

      struct kernel_sigaction *act = (struct kernel_sigaction *)args[1];
      if (act != NULL) {
        handler = (uintptr_t)act->k_sa_handler;
        // Never remove the UNLINK_SIGNAL handler, which is used internally by MAMBO
        if (args[0] == UNLINK_SIGNAL || (act->k_sa_handler != SIG_IGN && act->k_sa_handler != SIG_DFL)) {
          act->k_sa_handler = (__sighandler_t)signal_trampoline;
          act->sa_flags |= SA_SIGINFO;
        }
      }

      // A mutex is used to ensure that changes to the handler and all other options appear atomic
      int ret = pthread_mutex_lock(&global_data.signal_handlers_mutex);
      assert(ret == 0);

      uintptr_t syscall_ret = raw_syscall(syscall_no, args[0], args[1], args[2], args[3]);
      if (syscall_ret == 0) {
        struct kernel_sigaction *oldact = (struct kernel_sigaction *)args[2];
        if (oldact != NULL && oldact->k_sa_handler != SIG_IGN && oldact->k_sa_handler != SIG_DFL) {
          oldact->k_sa_handler = (void *)global_data.signal_handlers[args[0]];
        }

        if (act != NULL) {
          global_data.signal_handlers[args[0]] = handler;
        }
      }

      ret = pthread_mutex_unlock(&global_data.signal_handlers_mutex);
      assert(ret == 0);

      args[0] = syscall_ret;

      do_syscall = 0;
      break;
    }
    case __NR_exit_group:
      dbm_exit(thread_data, args[0]);
      break;
    case __NR_close:
      if (args[0] <= 2) { // stdin, stdout, stderr
        args[0] = 0;
        do_syscall = 0;
      }
      break;
    case __NR_readlinkat: {
      ssize_t len = readlink_handler((char *)args[1], (char *)args[2], args[3]);
      if (len >= 0) {
        args[0] = (uintptr_t)len;
        do_syscall = 0;
      }
      break;
    }
    /* Remove the execute permission from application mappings. At this point, this mostly acts
       as a safeguard in case a translation bug causes a branch to unmodified application code.
       Page permissions happen to be passed in the third argument both for mmap and mprotect. */
#ifdef __arm__
    case __NR_mmap2:
#endif
#ifdef __aarch64__
    case __NR_mmap:
#endif
    case __NR_mprotect: {
      uintptr_t syscall_ret, prot = args[2];

      /* Ensure that code pages are readable by the code scanner. */
      if (args[2] & PROT_EXEC) {
        if (!(args[2] & PROT_READ)) {
          debug("MAMBO: adding read permission to executable mapping at 0x%" PRIxPTR "\n", args[0]);
          args[2] |= PROT_READ;
        }
        args[2] &= ~PROT_EXEC;
      }

#ifdef __arm__
      if (syscall_no == __NR_mmap2) {
#elif __aarch64__
      if (syscall_no == __NR_mmap) {
#endif
        syscall_ret = raw_syscall(syscall_no, args[0], args[1], args[2], args[3], args[4], args[5]);
        if (syscall_ret <= -ERANGE) {
          uintptr_t start = align_lower(syscall_ret, PAGE_SIZE);
          uintptr_t end = align_higher(syscall_ret + args[1], PAGE_SIZE);
          notify_vm_op(VM_MAP, start, end-start, prot, args[3], args[4], args[5]);
        }
      } else {
        assert(syscall_no == __NR_mprotect);

        syscall_ret = raw_syscall(syscall_no, args[0], args[1], args[2]);
        if (syscall_ret == 0) {
          uintptr_t start = align_lower(args[0], PAGE_SIZE);
          uintptr_t end = align_higher(args[0] + args[1], PAGE_SIZE);
          notify_vm_op(VM_PROT, start, end-start, args[2], 0, -1, 0);
        }
      }

      args[0] = syscall_ret;
      do_syscall = 0;
      break;
    }
    case __NR_munmap: {
      uintptr_t syscall_ret = raw_syscall(syscall_no, args[0], args[1]);

      if (syscall_ret == 0) {
        uintptr_t start = align_lower(args[0], PAGE_SIZE);
        uintptr_t end = align_higher(args[0] + args[1], PAGE_SIZE);
        notify_vm_op(VM_UNMAP, start, end-start, 0, 0, -1, 0);
      }

      args[0] = syscall_ret;
      do_syscall = 0;
      break;
    }

    case __NR_shmat: {
      uintptr_t syscall_ret = raw_syscall(syscall_no, args[0], args[1], args[2]);
      if (syscall_ret != -1) {
        struct shmid_ds shm;
        int prot = PROT_READ;
        prot |= (args[2] & SHM_EXEC) ? PROT_EXEC : 0;
        prot |= (args[2] & SHM_RDONLY) ? 0 : PROT_WRITE;
        int ret = shmctl(args[0], IPC_STAT, &shm);
        assert(ret == 0);

        notify_vm_op(VM_MAP, syscall_ret, shm.shm_segsz, prot, 0, -1, 0);
      }

      args[0] = syscall_ret;
      do_syscall = 0;
      break;
    }

    case __NR_shmdt: {
      struct shmid_ds shm;
      int ret = shmctl(args[0], IPC_STAT, &shm);
      if (ret == 0) {
        uintptr_t syscall_ret = raw_syscall(syscall_no, args[0]);
        if (syscall_ret == 0) {
          notify_vm_op(VM_UNMAP, args[0], shm.shm_segsz, 0, 0, -1, 0);
        }
        args[0] = syscall_ret;
      } else {
        args[0] = -errno;
      }

      do_syscall = 0;
      break;
    }

#ifdef __arm__
    case __NR_sigreturn:
#endif
    case __NR_rt_sigreturn: {
      void *app_sp = args;
#ifdef __arm__
      /* We force all signal handler to the SA_SIGINFO type, which must return
         with rt_sigreturn() and not sigreturn(). Some applications don't return
         to the rt_sigreturn wrapper set by the kernel in the LR, so we need to
         override it here. See linux/arm/kernel/signal.c for the difference
         between the two types of signal handlers.
      */
      args[7] = __NR_rt_sigreturn;
      app_sp += 64;
#elif __aarch64__
      app_sp += 64 + 144;
#endif
      ucontext_t *cont = (ucontext_t *)(app_sp + sizeof(siginfo_t));
      sigret_dispatcher_call(thread_data, cont, cont->context_pc);

      // Don't mark the thread as executing a syscall
      return 1;
    }

#ifdef __arm__
    case __NR_vfork:
      // vfork without sharing the address space
      args[0] = raw_syscall(__NR_clone, CLONE_VFORK, NULL, NULL, NULL, NULL);
      if (args[0] == 0) {
        reset_process(thread_data);
      }
      do_syscall = 0;
      break;
    case __ARM_NR_cacheflush:
      debug("cache flush\n");
      /* Returning to the calling BB is potentially unsafe because the remaining
         contents of the BB or other basic blocks it is linked against could be stale */
      flush_code_cache(thread_data);
      break;
    case __ARM_NR_set_tls:
      debug("set tls to %x\n", args[0]);
      thread_data->tls = args[0];
      args[0] = 0;
      do_syscall = 0;
      break;
    case __NR_readlink: {
      ssize_t len = readlink_handler((char *)args[0], (char *)args[1], args[2]);
      if (len >= 0) {
        args[0] = (uintptr_t)len;
        do_syscall = 0;
      }
      break;
    }
#endif
  }

#ifdef PLUGINS_NEW
  } // if (!ctx.syscall.replace)
  if (do_syscall == 0 && global_data.free_plugin > 0) {
    set_mambo_context_syscall(&ctx, thread_data, POST_SYSCALL_C, syscall_no, (uintptr_t *)args);
    mambo_deliver_callbacks_for_ctx(&ctx);
  }
#endif

  if (do_syscall) {
    thread_data->status = THREAD_SYSCALL;
  }

  return do_syscall;
}

void syscall_handler_post(uintptr_t syscall_no, uintptr_t *args, uint16_t *next_inst, dbm_thread *thread_data) {
  debug("syscall post %" PRIdPTR "\n", syscall_no);

  if (global_data.exit_group) {
    thread_abort(thread_data);
  }
  thread_data->status = THREAD_RUNNING;

  switch(syscall_no) {
    case __NR_clone:
      debug("r0 (tid): %" PRIdPTR "\n", args[0]);
      if (args[0] == 0) { // the child
        assert(!thread_data->clone_vm);
        /* Without CLONE_VM, the child runs in a separate memory space,
           no synchronisation is needed.*/
        thread_data->tls = thread_data->child_tls;
        reset_process(thread_data);
      }
      break;
  }

#ifdef PLUGINS_NEW
  mambo_context ctx;

  set_mambo_context_syscall(&ctx, thread_data, POST_SYSCALL_C, syscall_no, (uintptr_t *)args);
  mambo_deliver_callbacks_for_ctx(&ctx);
#endif
}

```

`syscalls.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

/* Defined as a multiple of <native register width> (i.e. 4 bytes on AArch32
   and 8 bytes on AArch64), not bytes
*/
#ifdef __arm__
  /* SP decremented twice +
     14 regs pushed by the SVC translation
  */
  #define SYSCALL_WRAPPER_STACK_OFFSET (2 + 14)
  #define SYSCALL_WRAPPER_FRAME_SIZE   (SYSCALL_WRAPPER_STACK_OFFSET)
#elif __aarch64__
  /* 2  regs(x29, x30) pushed by the SVC translation
     2  (TPC, SVC) +
     22 (X0-X21) +
     (32*2) NEON/FP registers saved in the wrapper
  */
  #define SYSCALL_WRAPPER_STACK_OFFSET (2 + 2 + 22)
  #define SYSCALL_WRAPPER_FRAME_SIZE   (SYSCALL_WRAPPER_STACK_OFFSET + 2*32)
#endif

```

`test/hw_div.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2016 Cosmin Gorgovan <cosmin at linux-geek dot org>

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

/*
  Test file for plugin instr replacement and plugins/soft_div. It should print:

  827 413 1655
  9478 7108 7108
  4 434 1304

*/

.syntax unified

.global main
.func main
.code 32
main:
  PUSH {R4, LR}

  MOVW R3, #1655
  MOV R4, #2
  SDIV R1, R3, R4

  MOV R4, #4
  UDIV R2, R3, R4

  CMP R4, #4
  UDIVNE R3, R3, R4

  ADR R0, _tf
  BL printf

  BL div_thumb

  POP {R4, PC}
.endfunc

.global div_thumb
.func div_thumb
.thumb_func
div_thumb:
  PUSH {R4, LR}
  
  ITTTT EQ
  NOP
  NOP
  NOP
  NOP

  MOVW R3, #28435
  MOV R4, #3
  SDIV R1, R3, R4
  CMP R4, #3
  ITTET EQ
  MOVEQ R4, #4
  UDIVEQ R2, R3, R4
  UDIVNE R3, R3, R4
  UDIVEQ R3, R3, R4
  LDR R0, =_tf
  BL printf

  MOVW R1, #11
  MOVW R2, #12
  MOVW R3, #1304
  MOVW R4, #3
  CMP R4, #3
  ITTEE EQ
  MOVEQ R1, #4
  UDIVEQ R2, R3, R4
  MOVNE R4, #5
  UDIVNE R3, R3, R4
  LDR R0, =_tf
  BL printf

  POP {R4, PC}
.endfunc

_tf: .string "%d %d %d\n"

```

`test/load_store.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

/* Reg0 - heap pointer
   Reg1 - stack pointer */
#ifdef __arm__
.syntax unified

.global test_thumb16
.func
.thumb_func
.type test_thumb16, %function

test_thumb16:
  PUSH {R4 - R12, LR}
  MOV R12, SP
  MOV SP, R1

  // LDR{ , B, H}, STR{ ,B, H}
  MOV  R2, #0
  LDR  R3, [R0, R2]
  LDRB R3, [R0, R2]
  LDRH R3, [R0, R2]
  STR  R3, [R0, R2]
  STRB R3, [R0, R2]
  STRH R3, [R0, R2]

  MOV  R7, #4
  LDR  R3, [R0, R7]
  LDRB R3, [R0, R7]
  LDRH R3, [R0, R7]
  STR  R3, [R0, R7]
  STRB R3, [R0, R7]
  STRH R3, [R0, R7]

  MOV  R4, #100
  LDR  R3, [R0, R4]
  LDRB R3, [R0, R4]
  LDRH R3, [R0, R4]
  STR  R3, [R0, R4]
  STRB R3, [R0, R4]
  STRH R3, [R0, R4]

  // LDRI, LDRBI, LDRHI
  LDR  R2, [R0, #0]
  LDRB R2, [R0, #0]
  LDRH R2, [R0, #0]
  STR  R2, [R0, #0]
  STRB R2, [R0, #0]
  STRH R2, [R0, #0]

  LDR  R4, [R0, #12]
  LDRB R4, [R0, #12]
  LDRH R4, [R0, #12]
  STR  R4, [R0, #12]
  STRB R4, [R0, #12]
  STRH R4, [R0, #12]

  LDR  R5, [R0, #0x7c]
  LDRB R5, [R0, #0x1f]
  LDRH R5, [R0, #0x3e]
  STR  R5, [R0, #0x7c]
  STRB R5, [R0, #0x1f]
  STRH R5, [R0, #0x3e]

  // LDRSB, LDRSH
  MOV R2, #0
  LDRSB R3, [R0, R2]
  LDRSH R4, [R0, R2]
  ADD R5, R0, #100
  MOV R6, #80
  LDRSB R2, [R5, R6]
  LDRSH R4, [R5, R6]

  // LDMFD, STMEA
  ADD R5, R0, #1000
  LDMFD R5, {r1}
  LDMFD R5, {r1-r7}
  ADD R5, R0, #1000
  LDMFD R5!, {r1}
  LDMFD R5!, {r1-r4, r6-r7}

  MOV R4, R0
  STMEA R4, {r3}
  STMEA R4!, {r3}
  STMEA R4, {r0-r7}
  STMEA R4!, {r0-r3, r5-r7}

  // PUSH, POP
  PUSH {r0}
  PUSH {r0, r4}
  PUSH {r0-r7, lr}
  POP  {r0-r7}
  ADD  SP, SP, #4
  POP  {r0, r4}
  POP  {r0}

  // SP-relative load and store
  SUB SP, SP, #10240
  LDR R2, [SP, #0]
  LDR R3, [SP, #40]
  LDR R4, [SP, #0x3FC]
  STR R2, [SP, #0]
  STR R3, [SP, #40]
  STR R4, [SP, #0x3FC]

  MOV SP, R12
  POP {R4 - R12, PC}
.endfunc
.ltorg

.global end_test_thumb16
end_test_thumb16:

.global test_thumb32
.func
.thumb_func
.type test_thumb32, %function

test_thumb32:
  PUSH {R4 - R12, LR}
  VPUSH {D16-D31}
  VPUSH {D0-D15}
  MOV R12, SP
  MOV SP, R1

  // LDR{ , B, H}, STR{ ,B, H}
  MOV R2, #0
  LDR.W  R3, [R0, R2]
  LDRB.W R3, [R0, R2]
  LDRH.W R3, [R0, R2]
  STR.W  R3, [R0, R2]
  STRB.W R3, [R0, R2]
  STRH.W R3, [R0, R2]

  MOV R9, R0
  MOV R2, #8
  LDR.W  R3, [R9, R2]
  LDRB.W R3, [R9, R2]
  LDRH.W R3, [R9, R2]
  STR.W  R3, [R9, R2]
  STRB.W R3, [R9, R2]
  STRH.W R3, [R9, R2]

  MOV R11, #16
  LDR.W  R3, [R9, R11]
  LDRB.W R3, [R9, R11]
  LDRH.W R3, [R9, R11]
  STR.W  R3, [R9, R11]
  STRB.W R3, [R9, R11]
  STRH.W R3, [R9, R11]

  // LDR{ , B, H}, STR{ ,B, H} with LSL
  MOV R2, #0
  LDR.W  R3, [R0, R2, LSL #2]
  LDRB.W R3, [R0, R2, LSL #2]
  LDRH.W R3, [R0, R2, LSL #2]
  STR.W  R3, [R0, R2, LSL #2]
  STRB.W R3, [R0, R2, LSL #2]
  STRH.W R3, [R0, R2, LSL #2]

  MOV R9, R0
  MOV R2, #8
  LDR.W  R3, [R9, R2, LSL #1]
  LDRB.W R3, [R9, R2, LSL #1]
  LDRH.W R3, [R9, R2, LSL #1]
  STR.W  R3, [R9, R2, LSL #1]
  STRB.W R3, [R9, R2, LSL #1]
  STRH.W R3, [R9, R2, LSL #1]

  MOV R11, #16
  LDR.W  R3, [R9, R11, LSL #3]
  LDRB.W R3, [R9, R11, LSL #3]
  LDRH.W R3, [R9, R11, LSL #3]
  STR.W  R3, [R9, R11, LSL #3]
  STRB.W R3, [R9, R11, LSL #3]
  STRH.W R3, [R9, R11, LSL #3]

  // LDRI{ , B, H}, STR{ , B, H}
  LDR.W  R3, [R0, #0]
  LDRB.W R3, [R0, #0]
  LDRH.W R3, [R0, #0]
  STR.W  R3, [R0, #0]
  STRB.W R3, [R0, #0]
  STRH.W R3, [R0, #0]

  MOV R14, R0
  LDR.W  R3, [R14, #12]
  LDRB.W R3, [R14, #12]
  LDRH.W R3, [R14, #12]
  STR.W  R3, [R14, #12]
  STRB.W R3, [R14, #12]
  STRH.W R3, [R14, #12]

  MOV R8, R0
  LDR.W  R3, [R8, #0xFFF]
  LDRB.W R3, [R8, #0xFFF]
  LDRH.W R3, [R8, #0xFFF]
  STR.W  R3, [R8, #0xFFF]
  STRB.W R3, [R8, #0xFFF]
  STRH.W R3, [R8, #0xFFF]

  // LDRI{ , B, H}, STR{ , B, H} with negative offset
  ADD R2, R0, #2000
  LDR  R8, [R2, #-1]
  LDRB R8, [R2, #-1]
  LDRH R8, [R2, #-1]
  STR  R8, [R2, #-1]
  STRB R8, [R2, #-1]
  STRH R8, [R2, #-1]

  MOV R9, R2
  LDR  R8, [R9, #-255]
  LDRB R8, [R9, #-255]
  LDRH R8, [R9, #-255]
  STR  R8, [R9, #-255]
  STRB R8, [R9, #-255]
  STRH R8, [R9, #-255]

  // LDRI{ , B, H}, STR{ , B, H} with pre-index
  MOV R2, R0
  LDR  R8, [R2, #1]!
  LDRB R8, [R2, #1]!
  LDRH R8, [R2, #1]!
  STR  R8, [R2, #1]!
  STRB R8, [R2, #1]!
  STRH R8, [R2, #1]!

  LDR  R8, [R2, #255]!
  LDRB R8, [R2, #255]!
  LDRH R8, [R2, #255]!
  STR  R8, [R2, #255]!
  STRB R8, [R2, #255]!
  STRH R8, [R2, #255]!

  LDR  R8, [R2, #-255]!
  LDRB R8, [R2, #-255]!
  LDRH R8, [R2, #-255]!
  STR  R8, [R2, #-255]!
  STRB R8, [R2, #-255]!
  STRH R8, [R2, #-255]!

  // LDRI{ , B, H}, STR{ , B, H} with post-index
  MOV R9, R0
  LDR  R4, [R9], #1
  LDRB R4, [R9], #1
  LDRH R4, [R9], #1
  STR  R4, [R9], #1
  STRB R4, [R9], #1
  STRH R4, [R9], #1

  LDR  R5, [R9], #255
  LDRB R5, [R9], #255
  LDRH R5, [R9], #255
  STR  R5, [R9], #255
  STRB R5, [R9], #255
  STRH R5, [R9], #255

  LDR  R6, [R9], #-255
  LDRB R6, [R9], #-255
  LDRH R6, [R9], #-255
  STR  R6, [R9], #-255
  STRB R6, [R9], #-255
  STRH R6, [R9], #-255

  // LDRD, STRD, offset
  ADD R4, R0, #0x800
  LDRD R2, R3, [R4]
  LDRD R2, R3, [R4, #0x3FC]
  LDRD R2, R3, [R4, #-0x3FC]
  STRD R2, R3, [R4]
  STRD R2, R3, [R4, #0x3FC]
  STRD R2, R3, [R4, #-0x3FC]

  // LDRD, STRD, pre-index
  LDRD R6, R7, [R4, #4]!
  LDRD R6, R7, [R4, #0x3FC]!
  LDRD R6, R7, [R4, #-0x3FC]!
  STRD R6, R7, [R4, #4]!
  STRD R6, R7, [R4, #0x3FC]!
  STRD R6, R7, [R4, #-0x3FC]!

  // LDRD, STRD, post-index
  MOV R5, R0
  LDRD R2, R3, [R5], #4
  LDRD R2, R3, [R5], #0x3FC
  LDRD R2, R3, [R5], #-0x3FC
  STRD R2, R3, [R5], #4
  STRD R2, R3, [R5], #0x3FC
  STRD R2, R3, [R5], #-0x3FC

  // {LDM,STM}{FD,EA}
  ADD R3, R0, #0x800
  STMFD.W R3, {R2}
  LDMFD.W R3, {R2}
  STMFD.W R3, {R2-R11, R14}
  LDMFD.W R3, {R2-R11, R14}
  STMFD.W SP!, {R0}
  LDMFD.W SP!, {R0}
  STMFD.W SP!, {R0-R11, R14}
  LDMFD.W SP!, {R0-R11, R14}

  ADD R5, R0, #0x800
  STMEA.W R5, {R1}
  LDMEA.W R5, {R1}
  STMEA.W R5, {R2-R11, R14}
  LDMEA.W R5, {R2-R11, R14}
  STMEA.W R5!, {R4}
  LDMEA.W R5!, {R4}
  STMEA.W R5!, {R2-R4, R6-R11, R14}
  LDMEA.W R5!, {R2-R4, R6-R11, R14}

  // {LDR,STR}{,B,H}T
  MOV R8, R0
  LDRBT R7, [R0]
  LDRBT R2, [R8, #255]
  STRBT R7, [R8]
  STRBT R7, [R0, #255]
  LDRHT R4, [R0]
  LDRHT R3, [R0, #255]
  STRHT R5, [R0]
  STRHT R9, [R8, #255]
  LDRT  R3, [R0]
  STRT  R3, [R0]
  LDRT  R4, [R8, #255]
  STRT  R4, [R8, #255]

  // LDRS{B,H}{,T}
  ADD R9, R0, #0x800
  LDRSB.W R3, [R0]
  LDRSB.W R3, [R9, #0x3FF]
  LDRSB.W R3, [R9, #-0xFF]
  LDRSB.W R3, [R9], #0xFF
  LDRSB.W R3, [R9, #0xFF]!
  LDRSBT R4,  [R9, #0xFF]
  LDRSH.W R3, [R0]
  LDRSH.W R3, [R9, #0x3FF]
  LDRSH.W R3, [R9, #-0xFF]
  LDRSH.W R3, [R9], #0xFF
  LDRSH.W R3, [R9, #0xFF]!
  LDRSHT  R5, [R9, #0xff]

  // VPUSH,VPOP
  VPUSH {D0}
  VPUSH {S0}
  VPOP  {S0}
  VPOP  {D0}
  VPUSH {S4-S8}
  VPOP  {S4-S8}
  VPUSH {D4-D15}
  VPOP  {D4-D15}
  vstmdb sp!, {s0-s31} // VPUSH {s0-s31}
  vldm   sp!, {s0-s31} // VPOP  {S0-S31}
  VPUSH {D16-D31}
  VPOP  {D16-D31}

  // VSTM, VLDM
  MOV R2, R0
  VSTM R2, {S0}
  VLDM R2, {S0}
  VSTM R2, {D0-D15}
  VLDM R2, {D0-D15}
  VSTM R2!, {S0}
  VLDM R2!, {S0}
  VSTM R2!, {D16-D31}
  VLDM R2!, {D16-D31}

  // VLDR, VSTR
  ADD R3, R0, 0x800
  VSTR S0, [R3]
  VLDR S0, [R3]
  VSTR D0, [R3]
  VLDR D0, [R3]
  VSTR S16, [R3, #0x3FC]
  VLDR S16, [R3, #0x3FC]
  VSTR D17, [R3, #-0x3FC]
  VLDR D17, [R3, #-0x3FC]

  // load exclusive
  ADD R8, R0, #0x100
  LDREX R2, [R8]
  LDREX R3, [R0, 0x3FC]
  LDREXB R4, [R0]
  LDREXH R5, [R8]
  LDREXD R6, R7, [R0]
  CLREX

  // VLD1 / VST1
  ADD R2, R0, #0x8000
  MOV R3, #0x20
  MOV R4, #0x40
  VLD1.8 D0[2], [R2]
  VLD1.8 D0[1], [R2]!
  VLD1.8 D0[0], [R2], R3
  VLD1.16 D0[2], [R2]
  VLD1.16 D0[1], [R2]!
  VLD1.16 D0[0], [R2], R4
  VLD1.32 D0[0], [R2]
  VLD1.32 D0[1], [R2]!
  VLD1.32 D0[0], [R2], R3

  VLD1.8  {D2[]}, [R2]
  VLD1.8  {D2[]}, [R2]!
  VLD1.8  {D2[]}, [R2], R3
  VLD1.16 {D2[]}, [R2]
  VLD1.16 {D2[]}, [R2]!
  VLD1.16 {D2[]}, [R2], R4
  VLD1.32 {D2[]}, [R2]
  VLD1.32 {D2[]}, [R2]!
  VLD1.32 {D2[]}, [R2], R3

  VLD1.8  {D1}, [R2]
  VLD1.8  {D1-D2}, [R2]
  VLD1.8  {D1-D3}, [R2]
  VLD1.8  {D1-D4}, [R2]
  VLD1.8  {D1}, [R2]!
  VLD1.8  {D1-D2}, [R2]!
  VLD1.8  {D1-D3}, [R2]!
  VLD1.8  {D1-D4}, [R2]!
  VLD1.8  {D1}, [R2], R3
  VLD1.8  {D1-D2}, [R2], R3
  VLD1.8  {D1-D3}, [R2], R3
  VLD1.8  {D1-D4}, [R2], R3
  VLD1.16 {D1}, [R2]
  VLD1.16 {D1-D2}, [R2]
  VLD1.16 {D1-D3}, [R2]
  VLD1.16 {D1-D4}, [R2]
  VLD1.16 {D1}, [R2]!
  VLD1.16 {D1-D2}, [R2]!
  VLD1.16 {D1-D3}, [R2]!
  VLD1.16 {D1-D4}, [R2]!
  VLD1.16 {D1}, [R2], R4
  VLD1.16 {D1-D2}, [R2], R4
  VLD1.16 {D1-D3}, [R2], R4
  VLD1.16 {D1-D4}, [R2], R4
  VLD1.32 {D1}, [R2]
  VLD1.32 {D1-D2}, [R2]
  VLD1.32 {D1-D3}, [R2]
  VLD1.32 {D1-D4}, [R2]
  VLD1.32 {D1}, [R2]!
  VLD1.32 {D1-D2}, [R2]!
  VLD1.32 {D1-D3}, [R2]!
  VLD1.32 {D1-D4}, [R2]!
  VLD1.32 {D1}, [R2], R3
  VLD1.32 {D1-D2}, [R2], R3
  VLD1.32 {D1-D3}, [R2], R3
  VLD1.32 {D1-D4}, [R2], R3

  VST1.8 D0[2], [R2]
  VST1.8 D0[1], [R2]!
  VST1.8 D0[0], [R2], R3
  VST1.16 D0[2], [R2]
  VST1.16 D0[1], [R2]!
  VST1.16 D0[0], [R2], R4
  VST1.32 D0[0], [R2]
  VST1.32 D0[1], [R2]!
  VST1.32 D0[0], [R2], R3

  VST1.8  {D1}, [R2]
  VST1.8  {D1-D2}, [R2]
  VST1.8  {D1-D3}, [R2]
  VST1.8  {D1-D4}, [R2]
  VST1.8  {D1}, [R2]!
  VST1.8  {D1-D2}, [R2]!
  VST1.8  {D1-D3}, [R2]!
  VST1.8  {D1-D4}, [R2]!
  VST1.8  {D1}, [R2], R3
  VST1.8  {D1-D2}, [R2], R3
  VST1.8  {D1-D3}, [R2], R3
  VST1.8  {D1-D4}, [R2], R3
  VST1.16 {D1}, [R2]
  VST1.16 {D1-D2}, [R2]
  VST1.16 {D1-D3}, [R2]
  VST1.16 {D1-D4}, [R2]
  VST1.16 {D1}, [R2]!
  VST1.16 {D1-D2}, [R2]!
  VST1.16 {D1-D3}, [R2]!
  VST1.16 {D1-D4}, [R2]!
  VST1.16 {D1}, [R2], R4
  VST1.16 {D1-D2}, [R2], R4
  VST1.16 {D1-D3}, [R2], R4
  VST1.16 {D1-D4}, [R2], R4
  VST1.32 {D1}, [R2]
  VST1.32 {D1-D2}, [R2]
  VST1.32 {D1-D3}, [R2]
  VST1.32 {D1-D4}, [R2]
  VST1.32 {D1}, [R2]!
  VST1.32 {D1-D2}, [R2]!
  VST1.32 {D1-D3}, [R2]!
  VST1.32 {D1-D4}, [R2]!
  VST1.32 {D1}, [R2], R3
  VST1.32 {D1-D2}, [R2], R3
  VST1.32 {D1-D3}, [R2], R3
  VST1.32 {D1-D4}, [R2], R3

  // VLD2 / VST2
  VLD2.8  {D0[2], D1[2]}, [R2]
  VLD2.8  {D0[1], D1[1]}, [R2]!
  VLD2.8  {D0[0], D1[0]}, [R2], R3
  VLD2.16 {D0[2], D1[2]}, [R2]
  VLD2.16 {D0[1], D1[1]}, [R2]!
  VLD2.16 {D0[0], D1[0]}, [R2], R4
  VLD2.32 {D0[0], D1[0]}, [R2]
  VLD2.32 {D0[1], D1[1]}, [R2]!
  VLD2.32 {D0[0], D1[0]}, [R2], R3

  VLD2.8  {D2[], D3[]}, [R2]
  VLD2.8  {D2[], D3[]}, [R2]!
  VLD2.8  {D2[], D3[]}, [R2], R3
  VLD2.16 {D2[], D3[]}, [R2]
  VLD2.16 {D2[], D3[]}, [R2]!
  VLD2.16 {D2[], D3[]}, [R2], R4
  VLD2.32 {D2[], D3[]}, [R2]
  VLD2.32 {D2[], D3[]}, [R2]!
  VLD2.32 {D2[], D3[]}, [R2], R3

  VLD2.8  {D1-D2}, [R2]
  VLD2.8  {D1-D4}, [R2]
  VLD2.8  {D1-D2}, [R2]!
  VLD2.8  {D1-D4}, [R2]!
  VLD2.8  {D1-D2}, [R2], R3
  VLD2.8  {D1-D4}, [R2], R3
  VLD2.16 {D1-D2}, [R2]
  VLD2.16 {D1-D4}, [R2]
  VLD2.16 {D1-D2}, [R2]!
  VLD2.16 {D1-D4}, [R2]!
  VLD2.16 {D1-D2}, [R2], R4
  VLD2.16 {D1-D4}, [R2], R4
  VLD2.32 {D1-D2}, [R2]
  VLD2.32 {D1-D4}, [R2]
  VLD2.32 {D1-D2}, [R2]!
  VLD2.32 {D1-D4}, [R2]!
  VLD2.32 {D1-D2}, [R2], R3
  VLD2.32 {D1-D4}, [R2], R3

  VST2.8  {D0[2], D1[2]}, [R2]
  VST2.8  {D0[1], D1[1]}, [R2]!
  VST2.8  {D0[0], D1[0]}, [R2], R3
  VST2.16 {D0[2], D1[2]}, [R2]
  VST2.16 {D0[1], D1[1]}, [R2]!
  VST2.16 {D0[0], D1[0]}, [R2], R4
  VST2.32 {D0[0], D1[0]}, [R2]
  VST2.32 {D0[1], D1[1]}, [R2]!
  VST2.32 {D0[0], D1[0]}, [R2], R3

  VST2.8  {D1-D2}, [R2]
  VST2.8  {D1-D4}, [R2]
  VST2.8  {D1-D2}, [R2]!
  VST2.8  {D1-D4}, [R2]!
  VST2.8  {D1-D2}, [R2], R3
  VST2.8  {D1-D4}, [R2], R3
  VST2.16 {D1-D2}, [R2]
  VST2.16 {D1-D4}, [R2]
  VST2.16 {D1-D2}, [R2]!
  VST2.16 {D1-D4}, [R2]!
  VST2.16 {D1-D2}, [R2], R4
  VST2.16 {D1-D4}, [R2], R4
  VST2.32 {D1-D2}, [R2]
  VST2.32 {D1-D4}, [R2]
  VST2.32 {D1-D2}, [R2]!
  VST2.32 {D1-D4}, [R2]!
  VST2.32 {D1-D2}, [R2], R3
  VST2.32 {D1-D4}, [R2], R3

  // VLD3 / VST3
  VLD3.8  {D0[2], D1[2], D2[2]}, [R2]
  VLD3.8  {D0[1], D1[1], D2[1]}, [R2]!
  VLD3.8  {D0[0], D1[0], D2[0]}, [R2], R3
  VLD3.16 {D0[2], D1[2], D2[2]}, [R2]
  VLD3.16 {D0[1], D1[1], D2[1]}, [R2]!
  VLD3.16 {D0[0], D1[0], D2[0]}, [R2], R4
  VLD3.32 {D0[0], D1[0], D2[0]}, [R2]
  VLD3.32 {D0[1], D1[1], D2[1]}, [R2]!
  VLD3.32 {D0[0], D1[0], D2[0]}, [R2], R3

  VLD3.8  {D2[], D3[], D4[]}, [R2]
  VLD3.8  {D2[], D3[], D4[]}, [R2]!
  VLD3.8  {D2[], D3[], D4[]}, [R2], R3
  VLD3.16 {D2[], D3[], D4[]}, [R2]
  VLD3.16 {D2[], D3[], D4[]}, [R2]!
  VLD3.16 {D2[], D3[], D4[]}, [R2], R4
  VLD3.32 {D2[], D3[], D4[]}, [R2]
  VLD3.32 {D2[], D3[], D4[]}, [R2]!
  VLD3.32 {D2[], D3[], D4[]}, [R2], R3

  VLD3.8  {D1-D3}, [R2]
  VLD3.8  {D1-D3}, [R2]!
  VLD3.8  {D1-D3}, [R2], R3
  VLD3.16 {D1-D3}, [R2]
  VLD3.16 {D1-D3}, [R2]!
  VLD3.16 {D1-D3}, [R2], R4
  VLD3.32 {D1-D3}, [R2]
  VLD3.32 {D1-D3}, [R2]!
  VLD3.32 {D1-D3}, [R2], R3

  VST3.8  {D0[2], D1[2], D2[2]}, [R2]
  VST3.8  {D0[1], D1[1], D2[1]}, [R2]!
  VST3.8  {D0[0], D1[0], D2[0]}, [R2], R3
  VST3.16 {D0[2], D1[2], D2[2]}, [R2]
  VST3.16 {D0[1], D1[1], D2[1]}, [R2]!
  VST3.16 {D0[0], D1[0], D2[0]}, [R2], R4
  VST3.32 {D0[0], D1[0], D2[0]}, [R2]
  VST3.32 {D0[1], D1[1], D2[1]}, [R2]!
  VST3.32 {D0[0], D1[0], D2[0]}, [R2], R3

  VST3.8  {D1-D3}, [R2]
  VST3.8  {D1-D3}, [R2]!
  VST3.8  {D1-D3}, [R2], R3
  VST3.16 {D1-D3}, [R2]
  VST3.16 {D1-D3}, [R2]!
  VST3.16 {D1-D3}, [R2], R4
  VST3.32 {D1-D3}, [R2]
  VST3.32 {D1-D3}, [R2]!
  VST3.32 {D1-D3}, [R2], R3

  // VLD4 / VST4
  VLD4.8  {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VLD4.8  {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VLD4.8  {D0[0], D1[0], D2[0], D3[0]}, [R2], R3
  VLD4.16 {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VLD4.16 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VLD4.16 {D0[0], D1[0], D2[0], D3[0]}, [R2], R4
  VLD4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2]
  VLD4.32 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VLD4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2], R3

  VLD4.8  {D2[], D3[], D4[], D5[]}, [R2]
  VLD4.8  {D2[], D3[], D4[], D5[]}, [R2]!
  VLD4.8  {D2[], D3[], D4[], D5[]}, [R2], R3
  VLD4.16 {D2[], D3[], D4[], D5[]}, [R2]
  VLD4.16 {D2[], D3[], D4[], D5[]}, [R2]!
  VLD4.16 {D2[], D3[], D4[], D5[]}, [R2], R4
  VLD4.32 {D2[], D3[], D4[], D5[]}, [R2]
  VLD4.32 {D2[], D3[], D4[], D5[]}, [R2]!
  VLD4.32 {D2[], D3[], D4[], D5[]}, [R2], R3

  VLD4.8  {D1-D4}, [R2]
  VLD4.8  {D1-D4}, [R2]!
  VLD4.8  {D1-D4}, [R2], R3
  VLD4.16 {D1-D4}, [R2]
  VLD4.16 {D1-D4}, [R2]!
  VLD4.16 {D1-D4}, [R2], R4
  VLD4.32 {D1-D4}, [R2]
  VLD4.32 {D1-D4}, [R2]!
  VLD4.32 {D1-D4}, [R2], R3

  VST4.8  {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VST4.8  {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VST4.8  {D0[0], D1[0], D2[0], D3[0]}, [R2], R3
  VST4.16 {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VST4.16 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VST4.16 {D0[0], D1[0], D2[0], D3[0]}, [R2], R4
  VST4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2]
  VST4.32 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VST4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2], R3

  VST4.8  {D1-D4}, [R2]
  VST4.8  {D1-D4}, [R2]!
  VST4.8  {D1-D4}, [R2], R3
  VST4.16 {D1-D4}, [R2]
  VST4.16 {D1-D4}, [R2]!
  VST4.16 {D1-D4}, [R2], R4
  VST4.32 {D1-D4}, [R2]
  VST4.32 {D1-D4}, [R2]!
  VST4.32 {D1-D4}, [R2], R3

  // literal loads - these will not trap
  LDR.W  R2, litl
  LDRB.W R3, litl
  LDRH.W R4, litl
  LDRD.W R5, litl
  LDRSB.W R6, litl
  LDRSH.W R7, litl

  MOV SP, R12
  VPOP {D0-D15}
  VPOP {D16-D31}
  POP {R4 - R12, PC}
.endfunc
.ltorg

.align 3
litl: .quad 0

.global end_test_thumb32
end_test_thumb32:

.global test_a32
.func
.code 32
.type test_a32, %function
test_a32:
  PUSH {R4-R12, LR}
  MOV R12, SP
  MOV SP, R1

  // LDR{,B, H, SB, SH, T, BT, HT, SBT, SHT} (imm)
  ADD R5, R0, #0x1000
  LDR R2, [R5, #0]
  LDR R2, [R5, #12]
  LDR R8, [R5, #0xFFF]
  LDR R9, [R5, #-4]
  LDR R9, [R5, #-0xFFF]

  LDR R4, [R5], #4
  LDR R6, [R5], #0xFFF
  LDR R7, [R5], #-0xFFF
  LDR R8, [R5], #0

  LDR R2, [R5, #0]!
  LDR R3, [R5, #0xFFF]!
  LDR R4, [R5, #-0xFFF]!

  LDRB R2, [R5, #0]
  LDRB R2, [R5, #16]
  LDRB R8, [R5, #0xFFF]
  LDRB R9, [R5, #-7]
  LDRB R9, [R5, #-0xFFF]

  LDRB R4, [R5], #8
  LDRB R6, [R5], #0xFFF
  LDRB R7, [R5], #-0xFFF
  LDRB R8, [R5], #0

  LDRB R2, [R5, #0]!
  LDRB R3, [R5, #0xFFF]!
  LDRB R4, [R5, #-0xFFF]!

  LDRH R2, [R5, #0]
  LDRH R2, [R5, #20]
  LDRH R8, [R5, #0xFF]
  LDRH R9, [R5, #-40]
  LDRH R9, [R5, #-0xFF]

  LDRH R4, [R5], #20
  LDRH R6, [R5], #0xFF
  LDRH R7, [R5], #-0xFF
  LDRH R8, [R5], #0

  LDRH R2, [R5, #0]!
  LDRH R3, [R5, #0xFF]!
  LDRH R4, [R5, #-0xFF]!

  LDRSB R2, [R5, #0]
  LDRSB R2, [R5, #20]
  LDRSB R8, [R5, #0xFF]
  LDRSB R9, [R5, #-40]
  LDRSB R9, [R5, #-0xFF]

  LDRSB R4, [R5], #20
  LDRSB R6, [R5], #0xFF
  LDRSB R7, [R5], #-0xFF
  LDRSB R8, [R5], #0

  LDRSB R2, [R5, #0]!
  LDRSB R3, [R5, #0xFF]!
  LDRSB R4, [R5, #-0xFF]!

  LDRSH R2, [R5, #0]
  LDRSH R2, [R5, #20]
  LDRSH R8, [R5, #0xFF]
  LDRSH R9, [R5, #-40]
  LDRSH R9, [R5, #-0xFF]

  LDRSH R4, [R5], #20
  LDRSH R6, [R5], #0xFF
  LDRSH R7, [R5], #-0xFF
  LDRSH R8, [R5], #0

  LDRSH R2, [R5, #0]!
  LDRSH R6, [R5, #36]!
  LDRSH R3, [R5, #0xFF]!
  LDRSH R4, [R5, #-0xFF]!

  LDRT R4, [R5], #4
  LDRT R6, [R5], #0xFFF
  LDRT R7, [R5], #-0xFFF
  LDRT R8, [R5], #0

  LDRBT R4, [R5], #4
  LDRBT R6, [R5], #0xFFF
  LDRBT R7, [R5], #-0xFFF
  LDRBT R8, [R5], #0

  LDRHT R4, [R5], #4
  LDRHT R6, [R5], #0xFF
  LDRHT R7, [R5], #-0xFF
  LDRHT R8, [R5], #0

  LDRSBT R4, [R5], #4
  LDRSBT R6, [R5], #0xFF
  LDRSBT R7, [R5], #-0xFF
  LDRSBT R8, [R5], #0

  LDRSHT R4, [R5], #4
  LDRSHT R6, [R5], #0xFF
  LDRSHT R7, [R5], #-0xFF
  LDRSHT R8, [R5], #0

  // STR{,B ,H, T, BT, HT} (imm)
  ADD R10, R0, 0x2000
  STR R3, [R10, #0]
  STR R4, [R10, #12]
  STR R5, [R10, #-12]
  STR R5, [R10, #0xFFF]
  STR R4, [R10, #-0xFFF]

  STR R3, [R10], #0
  STR R3, [R10], #12
  STR R3, [R10], #-12
  STR R4, [R10], #0xFFF
  STR R4, [R10], #-0xFFF

  STR R3, [R10, #0]!
  STR R3, [R10, #12]!
  STR R3, [R10, #-12]!
  STR R4, [R10, #0xFFF]!
  STR R4, [R10, #-0xFFF]!

  STRB R3, [R10, #0]
  STRB R4, [R10, #24]
  STRB R5, [R10, #24]
  STRB R5, [R10, #0xFFF]
  STRB R4, [R10, #-0xFFF]

  STRB R3, [R10], #0
  STRB R3, [R10], #24
  STRB R3, [R10], #-24
  STRB R4, [R10], #0xFFF
  STRB R4, [R10], #-0xFFF

  STRB R3, [R10, #0]!
  STRB R3, [R10, #24]!
  STRB R3, [R10, #-24]!
  STRB R4, [R10, #0xFFF]!
  STRB R4, [R10, #-0xFFF]!

  STRH R3, [R10, #0]
  STRH R4, [R10, #24]
  STRH R5, [R10, #24]
  STRH R5, [R10, #0xFF]
  STRH R4, [R10, #-0xFF]

  STRH R3, [R10], #0
  STRH R3, [R10], #24
  STRH R3, [R10], #-24
  STRH R4, [R10], #0xFF
  STRH R4, [R10], #-0xFF

  STRH R3, [R10, #0]!
  STRH R3, [R10, #24]!
  STRH R3, [R10, #-24]!
  STRH R4, [R10, #0xFF]!
  STRH R4, [R10, #-0xFF]!

  STRT R3, [R10], #0
  STRT R3, [R10], #60
  STRT R3, [R10], #-60
  STRT R4, [R10], #0xFFF
  STRT R4, [R10], #-0xFFF

  STRBT R3, [R10], #0
  STRBT R3, [R10], #64
  STRBT R3, [R10], #-64
  STRBT R4, [R10], #0xFF
  STRBT R4, [R10], #-0xFF

  STRHT R3, [R10], #0
  STRHT R3, [R10], #68
  STRHT R3, [R10], #-68
  STRHT R4, [R10], #0xFF
  STRHT R4, [R10], #-0xFF

  // LDR{, B, H, SB, SH} (reg)
  MOV R3, #0
  MOV R4, #0x300
  ADD R5, R0, #0x3000
  LDR R2, [R5, R3]
  LDR R2, [R5, R4]
  LDR R2, [R5, R4, LSL #1]
  LDR R2, [R5, R4, LSR #1]
  LDR R2, [R5, R4, ASR #1]
  LDR R2, [R5, R4, ROR #1]
  LDR R2, [R5, R4]!
  LDR R2, [R5, R4, LSL #1]!
  LDR R2, [R5, R4, LSR #1]!
  LDR R2, [R5, R4, ASR #1]!
  LDR R2, [R5, R4, ROR #1]!
  LDR R2, [R5], R4
  LDR R2, [R5], R4, LSL #2
  LDR R2, [R5], R4, LSR #2
  LDR R2, [R5], R4, ASR #2
  LDR R2, [R5], R4, ROR #2
  LDR R2, [R5, -R4]
  LDR R2, [R5, -R4, LSL #3]
  LDR R2, [R5, -R4, LSR #3]
  LDR R2, [R5, -R4, ASR #3]
  LDR R2, [R5, -R4, ROR #3]
  LDR R2, [R5, -R4]!
  LDR R2, [R5, -R4, LSL #3]!
  LDR R2, [R5, -R4, LSR #3]!
  LDR R2, [R5, -R4, ASR #3]!
  LDR R2, [R5, -R4, ROR #3]!
  LDR R2, [R5], -R4
  LDR R2, [R5], -R4, LSL #2
  LDR R2, [R5], -R4, LSR #2
  LDR R2, [R5], -R4, ASR #2
  LDR R2, [R5], -R4, ROR #2

  ADD R5, R0, #0x4000
  LDRB R2, [R5, R3]
  LDRB R2, [R5, R4]
  LDRB R2, [R5, R4, LSL #1]
  LDRB R2, [R5, R4, LSR #1]
  LDRB R2, [R5, R4, ASR #1]
  LDRB R2, [R5, R4, ROR #1]
  LDRB R2, [R5, R4]!
  LDRB R2, [R5, R4, LSL #1]!
  LDRB R2, [R5, R4, LSR #1]!
  LDRB R2, [R5, R4, ASR #1]!
  LDRB R2, [R5, R4, ROR #1]!
  LDRB R2, [R5], R4
  LDRB R2, [R5], R4, LSL #2
  LDRB R2, [R5], R4, LSR #2
  LDRB R2, [R5], R4, ASR #2
  LDRB R2, [R5], R4, ROR #2
  LDRB R2, [R5, -R4]
  LDRB R2, [R5, -R4, LSL #3]
  LDRB R2, [R5, -R4, LSR #3]
  LDRB R2, [R5, -R4, ASR #3]
  LDRB R2, [R5, -R4, ROR #3]
  LDRB R2, [R5, -R4]!
  LDRB R2, [R5, -R4, LSL #3]!
  LDRB R2, [R5, -R4, LSR #3]!
  LDRB R2, [R5, -R4, ASR #3]!
  LDRB R2, [R5, -R4, ROR #3]!
  LDRB R2, [R5], -R4
  LDRB R2, [R5], -R4, LSL #2
  LDRB R2, [R5], -R4, LSR #2
  LDRB R2, [R5], -R4, ASR #2
  LDRB R2, [R5], -R4, ROR #2

  ADD R5, R0, #0x5000
  LDRH R2, [R5, R3]
  LDRH R2, [R5, R4]
  LDRH R2, [R5, -R4]
  LDRH R2, [R5], R4
  LDRH R2, [R5], -R4
  LDRH R2, [R5, R4]!
  LDRH R2, [R5, -R4]!

  ADD R5, R0, #0x1000
  LDRSB R2, [R5, R3]
  LDRSB R2, [R5, R4]
  LDRSB R2, [R5, -R4]
  LDRSB R2, [R5], R4
  LDRSB R2, [R5], -R4
  LDRSB R2, [R5, R4]!
  LDRSB R2, [R5, -R4]!

  ADD R5, R0, #0x2000
  LDRSH R2, [R5, R3]
  LDRSH R2, [R5, R4]
  LDRSH R2, [R5, -R4]
  LDRSH R2, [R5], R4
  LDRSH R2, [R5], -R4
  LDRSH R2, [R5, R4]!
  LDRSH R2, [R5, -R4]!

  // STR{, B, H} (reg)
  MOV R3, #0
  MOV R4, #0x300
  STR R2, [R5, R3]
  STR R2, [R5, R4]
  STR R2, [R5, R4, LSL #1]
  STR R2, [R5, R4, LSR #1]
  STR R2, [R5, R4, ASR #1]
  STR R2, [R5, R4, ROR #1]
  STR R2, [R5, R4]!
  STR R2, [R5, R4, LSL #1]!
  STR R2, [R5, R4, LSR #1]!
  STR R2, [R5, R4, ASR #1]!
  STR R2, [R5, R4, ROR #1]!
  STR R2, [R5], R4
  STR R2, [R5], R4, LSL #2
  STR R2, [R5], R4, LSR #2
  STR R2, [R5], R4, ASR #2
  STR R2, [R5], R4, ROR #2
  STR R2, [R5, -R4]
  STR R2, [R5, -R4, LSL #3]
  STR R2, [R5, -R4, LSR #3]
  STR R2, [R5, -R4, ASR #3]
  STR R2, [R5, -R4, ROR #3]
  STR R2, [R5, -R4]!
  STR R2, [R5, -R4, LSL #3]!
  STR R2, [R5, -R4, LSR #3]!
  STR R2, [R5, -R4, ASR #3]!
  STR R2, [R5, -R4, ROR #3]!
  STR R2, [R5], -R4
  STR R2, [R5], -R4, LSL #2
  STR R2, [R5], -R4, LSR #2
  STR R2, [R5], -R4, ASR #2
  STR R2, [R5], -R4, ROR #2

  STRB R2, [R5, R3]
  STRB R2, [R5, R4]
  STRB R2, [R5, R4, LSL #1]
  STRB R2, [R5, R4, LSR #1]
  STRB R2, [R5, R4, ASR #1]
  STRB R2, [R5, R4, ROR #1]
  STRB R2, [R5, R4]!
  STRB R2, [R5, R4, LSL #1]!
  STRB R2, [R5, R4, LSR #1]!
  STRB R2, [R5, R4, ASR #1]!
  STRB R2, [R5, R4, ROR #1]!
  STRB R2, [R5], R4
  STRB R2, [R5], R4, LSL #2
  STRB R2, [R5], R4, LSR #2
  STRB R2, [R5], R4, ASR #2
  STRB R2, [R5], R4, ROR #2
  STRB R2, [R5, -R4]
  STRB R2, [R5, -R4, LSL #3]
  STRB R2, [R5, -R4, LSR #3]
  STRB R2, [R5, -R4, ASR #3]
  STRB R2, [R5, -R4, ROR #3]
  STRB R2, [R5, -R4]!
  STRB R2, [R5, -R4, LSL #3]!
  STRB R2, [R5, -R4, LSR #3]!
  STRB R2, [R5, -R4, ASR #3]!
  STRB R2, [R5, -R4, ROR #3]!
  STRB R2, [R5], -R4
  STRB R2, [R5], -R4, LSL #2
  STRB R2, [R5], -R4, LSR #2
  STRB R2, [R5], -R4, ASR #2
  STRB R2, [R5], -R4, ROR #2

  ADD R7, R0, #0x6000
  STRH R2, [R7, R3]
  STRH R2, [R7, R4]
  STRH R2, [R7, -R4]
  STRH R2, [R7], R4
  STRH R2, [R7], -R4
  STRH R2, [R7, R4]!
  STRH R2, [R7, -R4]!

  // LDRD, STRD (imm)
  LDRD R2, R3, [R7]
  LDRD R2, R3, [R7, #4]
  LDRD R2, R3, [R7, #-4]
  LDRD R2, R3, [R7, #0xFC]
  LDRD R2, R3, [R7, #-0xFC]
  LDRD R2, R3, [R7, #4]!
  LDRD R2, R3, [R7, #-4]!
  LDRD R2, R3, [R7, #0xFC]!
  LDRD R2, R3, [R7, #-0xFC]!
  LDRD R2, R3, [R7], #4
  LDRD R2, R3, [R7], #-4
  LDRD R2, R3, [R7], #0xFC
  LDRD R2, R3, [R7], #-0xFC

  STRD R2, R3, [R7]
  STRD R2, R3, [R7, #4]
  STRD R2, R3, [R7, #-4]
  STRD R2, R3, [R7, #0xFC]
  STRD R2, R3, [R7, #-0xFC]
  STRD R2, R3, [R7, #4]!
  STRD R2, R3, [R7, #-4]!
  STRD R2, R3, [R7, #0xFC]!
  STRD R2, R3, [R7, #-0xFC]!
  STRD R2, R3, [R7], #4
  STRD R2, R3, [R7], #-4
  STRD R2, R3, [R7], #0xFC
  STRD R2, R3, [R7], #-0xFC

  // LDRD, STRD (reg)
  MOV R3, #0
  MOV R4, #0x300
  LDRD R8, R9, [R7, R3]
  LDRD R8, R9, [R7, R4]
  LDRD R8, R9, [R7, -R4]
  LDRD R8, R9, [R7, R4]!
  LDRD R8, R9, [R7, -R4]!
  LDRD R8, R9, [R7], R4
  LDRD R8, R9, [R7], -R4

  STRD R8, R9, [R7, R3]
  STRD R8, R9, [R7, R4]
  STRD R8, R9, [R7, -R4]
  STRD R8, R9, [R7, R4]!
  STRD R8, R9, [R7, -R4]!
  STRD R8, R9, [R7], R4
  STRD R8, R9, [R7], -R4

  // LDM, STM
  ADD R5, R0, #0x400
  STMEA R5, {R2}
  LDMEA R5, {R2}
  STMEA R5!, {R0-R4, R6-R12, LR}
  LDMEA R5!, {R0-R4, R6-R12, LR}
  STMED R5, {R3}
  LDMED R5, {R3}
  STMED R5!, {R0-R4, R6-R12, LR}
  LDMED R5!, {R0-R4, R6-R12, LR}
  STMFA R5, {R4}
  LDMFA R5, {R4}
  STMFA R5!, {R0-R4, R6-R12, LR}
  LDMFA R5!, {R0-R4, R6-R12, LR}
  STMFD R5, {R6}
  LDMFD R5, {R6}
  STMFD R5!, {R0-R4, R6-R12, LR}
  LDMFD R5!, {R0-R4, R6-R12, LR}

  // PUSH, POP
  PUSH {R0}
  PUSH {R3-R5}
  POP  {R3-R5}
  POP  {R0}
  PUSH {R0-R12, LR}
  POP  {R0-R12, LR}

  // LDREX{, B, H, D}
  LDREX R2, [R0]
  LDREXB R2, [R0]
  LDREXH R2, [R0]
  LDREXD R2, [R0]
  CLREX

  // VLDR, VSTR
  ADD R4, R0, #0x800
  VLDR S0, [R4]
  VLDR S0, [R4, #32]
  VLDR S0, [R4, #-32]
  VLDR S0, [R4, #0x3FC]
  VLDR S0, [R4, #-0x3FC]
  VSTR S0, [R4]
  VSTR S0, [R4, #32]
  VSTR S0, [R4, #-32]
  VSTR S0, [R4, #0x3FC]
  VSTR S0, [R4, #-0x3FC]

  VLDR D0, [R4]
  VLDR D0, [R4, #32]
  VLDR D0, [R4, #-32]
  VLDR D0, [R4, #0x3FC]
  VLDR D0, [R4, #-0x3FC]
  VSTR D0, [R4]
  VSTR D0, [R4, #32]
  VSTR D0, [R4, #-32]
  VSTR D0, [R4, #0x3FC]
  VSTR D0, [R4, #-0x3FC]

  // VLDM, VSTM
  VSTM R0, {S1}
  VLDM R0, {S1}
  VSTM R0, {S0-S31}
  VLDM R0, {S0-S31}
  VSTM R0, {D1}
  VLDM R0, {D1}
  VSTM R0, {D0-D15}
  VLDM R0, {D0-D15}

  // VPUSH, VPOP
  VPUSH {S0}
  VPUSH {D0}
  vstmdb sp!, {s0-s31} // VPUSH {S0-S31}
  VPUSH {D0-D15}
  VPOP {D0-D15}
  vldm   sp!, {s0-s31} // VPOP  {S0-S31}
  VPOP {D0}
  VPOP {S0}

  // VLD1 / VST1
  ADD R2, R0, #0x8000
  MOV R3, #0x20
  MOV R4, #0x40
  VLD1.8 D0[2], [R2]
  VLD1.8 D0[1], [R2]!
  VLD1.8 D0[0], [R2], R3
  VLD1.16 D0[2], [R2]
  VLD1.16 D0[1], [R2]!
  VLD1.16 D0[0], [R2], R4
  VLD1.32 D0[0], [R2]
  VLD1.32 D0[1], [R2]!
  VLD1.32 D0[0], [R2], R3

  VLD1.8  {D2[]}, [R2]
  VLD1.8  {D2[]}, [R2]!
  VLD1.8  {D2[]}, [R2], R3
  VLD1.16 {D2[]}, [R2]
  VLD1.16 {D2[]}, [R2]!
  VLD1.16 {D2[]}, [R2], R4
  VLD1.32 {D2[]}, [R2]
  VLD1.32 {D2[]}, [R2]!
  VLD1.32 {D2[]}, [R2], R3

  VLD1.8  {D1}, [R2]
  VLD1.8  {D1-D2}, [R2]
  VLD1.8  {D1-D3}, [R2]
  VLD1.8  {D1-D4}, [R2]
  VLD1.8  {D1}, [R2]!
  VLD1.8  {D1-D2}, [R2]!
  VLD1.8  {D1-D3}, [R2]!
  VLD1.8  {D1-D4}, [R2]!
  VLD1.8  {D1}, [R2], R3
  VLD1.8  {D1-D2}, [R2], R3
  VLD1.8  {D1-D3}, [R2], R3
  VLD1.8  {D1-D4}, [R2], R3
  VLD1.16 {D1}, [R2]
  VLD1.16 {D1-D2}, [R2]
  VLD1.16 {D1-D3}, [R2]
  VLD1.16 {D1-D4}, [R2]
  VLD1.16 {D1}, [R2]!
  VLD1.16 {D1-D2}, [R2]!
  VLD1.16 {D1-D3}, [R2]!
  VLD1.16 {D1-D4}, [R2]!
  VLD1.16 {D1}, [R2], R4
  VLD1.16 {D1-D2}, [R2], R4
  VLD1.16 {D1-D3}, [R2], R4
  VLD1.16 {D1-D4}, [R2], R4
  VLD1.32 {D1}, [R2]
  VLD1.32 {D1-D2}, [R2]
  VLD1.32 {D1-D3}, [R2]
  VLD1.32 {D1-D4}, [R2]
  VLD1.32 {D1}, [R2]!
  VLD1.32 {D1-D2}, [R2]!
  VLD1.32 {D1-D3}, [R2]!
  VLD1.32 {D1-D4}, [R2]!
  VLD1.32 {D1}, [R2], R3
  VLD1.32 {D1-D2}, [R2], R3
  VLD1.32 {D1-D3}, [R2], R3
  VLD1.32 {D1-D4}, [R2], R3

  VST1.8 D0[2], [R2]
  VST1.8 D0[1], [R2]!
  VST1.8 D0[0], [R2], R3
  VST1.16 D0[2], [R2]
  VST1.16 D0[1], [R2]!
  VST1.16 D0[0], [R2], R4
  VST1.32 D0[0], [R2]
  VST1.32 D0[1], [R2]!
  VST1.32 D0[0], [R2], R3

  VST1.8  {D1}, [R2]
  VST1.8  {D1-D2}, [R2]
  VST1.8  {D1-D3}, [R2]
  VST1.8  {D1-D4}, [R2]
  VST1.8  {D1}, [R2]!
  VST1.8  {D1-D2}, [R2]!
  VST1.8  {D1-D3}, [R2]!
  VST1.8  {D1-D4}, [R2]!
  VST1.8  {D1}, [R2], R3
  VST1.8  {D1-D2}, [R2], R3
  VST1.8  {D1-D3}, [R2], R3
  VST1.8  {D1-D4}, [R2], R3
  VST1.16 {D1}, [R2]
  VST1.16 {D1-D2}, [R2]
  VST1.16 {D1-D3}, [R2]
  VST1.16 {D1-D4}, [R2]
  VST1.16 {D1}, [R2]!
  VST1.16 {D1-D2}, [R2]!
  VST1.16 {D1-D3}, [R2]!
  VST1.16 {D1-D4}, [R2]!
  VST1.16 {D1}, [R2], R4
  VST1.16 {D1-D2}, [R2], R4
  VST1.16 {D1-D3}, [R2], R4
  VST1.16 {D1-D4}, [R2], R4
  VST1.32 {D1}, [R2]
  VST1.32 {D1-D2}, [R2]
  VST1.32 {D1-D3}, [R2]
  VST1.32 {D1-D4}, [R2]
  VST1.32 {D1}, [R2]!
  VST1.32 {D1-D2}, [R2]!
  VST1.32 {D1-D3}, [R2]!
  VST1.32 {D1-D4}, [R2]!
  VST1.32 {D1}, [R2], R3
  VST1.32 {D1-D2}, [R2], R3
  VST1.32 {D1-D3}, [R2], R3
  VST1.32 {D1-D4}, [R2], R3

  // VLD2 / VST2
  VLD2.8  {D0[2], D1[2]}, [R2]
  VLD2.8  {D0[1], D1[1]}, [R2]!
  VLD2.8  {D0[0], D1[0]}, [R2], R3
  VLD2.16 {D0[2], D1[2]}, [R2]
  VLD2.16 {D0[1], D1[1]}, [R2]!
  VLD2.16 {D0[0], D1[0]}, [R2], R4
  VLD2.32 {D0[0], D1[0]}, [R2]
  VLD2.32 {D0[1], D1[1]}, [R2]!
  VLD2.32 {D0[0], D1[0]}, [R2], R3

  VLD2.8  {D2[], D3[]}, [R2]
  VLD2.8  {D2[], D3[]}, [R2]!
  VLD2.8  {D2[], D3[]}, [R2], R3
  VLD2.16 {D2[], D3[]}, [R2]
  VLD2.16 {D2[], D3[]}, [R2]!
  VLD2.16 {D2[], D3[]}, [R2], R4
  VLD2.32 {D2[], D3[]}, [R2]
  VLD2.32 {D2[], D3[]}, [R2]!
  VLD2.32 {D2[], D3[]}, [R2], R3

  VLD2.8  {D1-D2}, [R2]
  VLD2.8  {D1-D4}, [R2]
  VLD2.8  {D1-D2}, [R2]!
  VLD2.8  {D1-D4}, [R2]!
  VLD2.8  {D1-D2}, [R2], R3
  VLD2.8  {D1-D4}, [R2], R3
  VLD2.16 {D1-D2}, [R2]
  VLD2.16 {D1-D4}, [R2]
  VLD2.16 {D1-D2}, [R2]!
  VLD2.16 {D1-D4}, [R2]!
  VLD2.16 {D1-D2}, [R2], R4
  VLD2.16 {D1-D4}, [R2], R4
  VLD2.32 {D1-D2}, [R2]
  VLD2.32 {D1-D4}, [R2]
  VLD2.32 {D1-D2}, [R2]!
  VLD2.32 {D1-D4}, [R2]!
  VLD2.32 {D1-D2}, [R2], R3
  VLD2.32 {D1-D4}, [R2], R3

  VST2.8  {D0[2], D1[2]}, [R2]
  VST2.8  {D0[1], D1[1]}, [R2]!
  VST2.8  {D0[0], D1[0]}, [R2], R3
  VST2.16 {D0[2], D1[2]}, [R2]
  VST2.16 {D0[1], D1[1]}, [R2]!
  VST2.16 {D0[0], D1[0]}, [R2], R4
  VST2.32 {D0[0], D1[0]}, [R2]
  VST2.32 {D0[1], D1[1]}, [R2]!
  VST2.32 {D0[0], D1[0]}, [R2], R3

  VST2.8  {D1-D2}, [R2]
  VST2.8  {D1-D4}, [R2]
  VST2.8  {D1-D2}, [R2]!
  VST2.8  {D1-D4}, [R2]!
  VST2.8  {D1-D2}, [R2], R3
  VST2.8  {D1-D4}, [R2], R3
  VST2.16 {D1-D2}, [R2]
  VST2.16 {D1-D4}, [R2]
  VST2.16 {D1-D2}, [R2]!
  VST2.16 {D1-D4}, [R2]!
  VST2.16 {D1-D2}, [R2], R4
  VST2.16 {D1-D4}, [R2], R4
  VST2.32 {D1-D2}, [R2]
  VST2.32 {D1-D4}, [R2]
  VST2.32 {D1-D2}, [R2]!
  VST2.32 {D1-D4}, [R2]!
  VST2.32 {D1-D2}, [R2], R3
  VST2.32 {D1-D4}, [R2], R3

  // VLD3 / VST3
  VLD3.8  {D0[2], D1[2], D2[2]}, [R2]
  VLD3.8  {D0[1], D1[1], D2[1]}, [R2]!
  VLD3.8  {D0[0], D1[0], D2[0]}, [R2], R3
  VLD3.16 {D0[2], D1[2], D2[2]}, [R2]
  VLD3.16 {D0[1], D1[1], D2[1]}, [R2]!
  VLD3.16 {D0[0], D1[0], D2[0]}, [R2], R4
  VLD3.32 {D0[0], D1[0], D2[0]}, [R2]
  VLD3.32 {D0[1], D1[1], D2[1]}, [R2]!
  VLD3.32 {D0[0], D1[0], D2[0]}, [R2], R3

  VLD3.8  {D2[], D3[], D4[]}, [R2]
  VLD3.8  {D2[], D3[], D4[]}, [R2]!
  VLD3.8  {D2[], D3[], D4[]}, [R2], R3
  VLD3.16 {D2[], D3[], D4[]}, [R2]
  VLD3.16 {D2[], D3[], D4[]}, [R2]!
  VLD3.16 {D2[], D3[], D4[]}, [R2], R4
  VLD3.32 {D2[], D3[], D4[]}, [R2]
  VLD3.32 {D2[], D3[], D4[]}, [R2]!
  VLD3.32 {D2[], D3[], D4[]}, [R2], R3

  VLD3.8  {D1-D3}, [R2]
  VLD3.8  {D1-D3}, [R2]!
  VLD3.8  {D1-D3}, [R2], R3
  VLD3.16 {D1-D3}, [R2]
  VLD3.16 {D1-D3}, [R2]!
  VLD3.16 {D1-D3}, [R2], R4
  VLD3.32 {D1-D3}, [R2]
  VLD3.32 {D1-D3}, [R2]!
  VLD3.32 {D1-D3}, [R2], R3

  VST3.8  {D0[2], D1[2], D2[2]}, [R2]
  VST3.8  {D0[1], D1[1], D2[1]}, [R2]!
  VST3.8  {D0[0], D1[0], D2[0]}, [R2], R3
  VST3.16 {D0[2], D1[2], D2[2]}, [R2]
  VST3.16 {D0[1], D1[1], D2[1]}, [R2]!
  VST3.16 {D0[0], D1[0], D2[0]}, [R2], R4
  VST3.32 {D0[0], D1[0], D2[0]}, [R2]
  VST3.32 {D0[1], D1[1], D2[1]}, [R2]!
  VST3.32 {D0[0], D1[0], D2[0]}, [R2], R3

  VST3.8  {D1-D3}, [R2]
  VST3.8  {D1-D3}, [R2]!
  VST3.8  {D1-D3}, [R2], R3
  VST3.16 {D1-D3}, [R2]
  VST3.16 {D1-D3}, [R2]!
  VST3.16 {D1-D3}, [R2], R4
  VST3.32 {D1-D3}, [R2]
  VST3.32 {D1-D3}, [R2]!
  VST3.32 {D1-D3}, [R2], R3

  // VLD4 / VST4
  VLD4.8  {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VLD4.8  {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VLD4.8  {D0[0], D1[0], D2[0], D3[0]}, [R2], R3
  VLD4.16 {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VLD4.16 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VLD4.16 {D0[0], D1[0], D2[0], D3[0]}, [R2], R4
  VLD4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2]
  VLD4.32 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VLD4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2], R3

  VLD4.8  {D2[], D3[], D4[], D5[]}, [R2]
  VLD4.8  {D2[], D3[], D4[], D5[]}, [R2]!
  VLD4.8  {D2[], D3[], D4[], D5[]}, [R2], R3
  VLD4.16 {D2[], D3[], D4[], D5[]}, [R2]
  VLD4.16 {D2[], D3[], D4[], D5[]}, [R2]!
  VLD4.16 {D2[], D3[], D4[], D5[]}, [R2], R4
  VLD4.32 {D2[], D3[], D4[], D5[]}, [R2]
  VLD4.32 {D2[], D3[], D4[], D5[]}, [R2]!
  VLD4.32 {D2[], D3[], D4[], D5[]}, [R2], R3

  VLD4.8  {D1-D4}, [R2]
  VLD4.8  {D1-D4}, [R2]!
  VLD4.8  {D1-D4}, [R2], R3
  VLD4.16 {D1-D4}, [R2]
  VLD4.16 {D1-D4}, [R2]!
  VLD4.16 {D1-D4}, [R2], R4
  VLD4.32 {D1-D4}, [R2]
  VLD4.32 {D1-D4}, [R2]!
  VLD4.32 {D1-D4}, [R2], R3

  VST4.8  {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VST4.8  {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VST4.8  {D0[0], D1[0], D2[0], D3[0]}, [R2], R3
  VST4.16 {D0[2], D1[2], D2[2], D3[2]}, [R2]
  VST4.16 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VST4.16 {D0[0], D1[0], D2[0], D3[0]}, [R2], R4
  VST4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2]
  VST4.32 {D0[1], D1[1], D2[1], D3[1]}, [R2]!
  VST4.32 {D0[0], D1[0], D2[0], D3[0]}, [R2], R3

  VST4.8  {D1-D4}, [R2]
  VST4.8  {D1-D4}, [R2]!
  VST4.8  {D1-D4}, [R2], R3
  VST4.16 {D1-D4}, [R2]
  VST4.16 {D1-D4}, [R2]!
  VST4.16 {D1-D4}, [R2], R4
  VST4.32 {D1-D4}, [R2]
  VST4.32 {D1-D4}, [R2]!
  VST4.32 {D1-D4}, [R2], R3

  // Literal loads - will not trap
  MOV R2, #16
  LDR R3, lita32
  LDR R3, [PC, R2]
  LDRB R3, lita32
  LDRB R3, [PC, R2]
  LDRHEQ R4, lita32
  LDRH R4, [PC, R2]
  LDRSB R5, lita32
  LDRSB R5, [PC, R2]
  LDRSH R6, lita32
  LDRSH R6, [PC, R2]
  LDRD R6, lita32
  LDRD R6, [PC, R2]

  MOV SP, R12
  POP {R4-R12, PC}
.endfunc

lita32: .quad 0

.global end_test_a32
end_test_a32:

#elif __aarch64__
.global test_a64
.func
.type test_a64, %function

test_a64:
  MOV X29, SP
  MOV SP, X1

  // LDRI {, B, H, SB, SH, SW}
  ADD X4, X0, #0x400
  LDR   X2, [X4]
  LDR   X2, [X4, #8]
  LDR   X2, [X4, #7]
  LDR   X2, [X4, #-5]
  LDR   X2, [X4, #32760]
  LDR   X2, [X4, #255]
  LDR   X2, [X4, #-256]
  LDR   X2, [X4, #0]!
  LDR   X2, [X4, #255]!
  LDR   X2, [X4, #-256]!
  LDR   X2, [X4], #0
  LDR   X2, [X4], #1
  LDR   X2, [X4], #255
  LDR   X2, [X4], #-256

  LDR   B0, [X4]
  LDR   B0, [X4, #8]
  LDR   B0, [X4, #7]
  LDR   B0, [X4, #-5]
  LDR   B0, [X4, #4095]
  LDR   B0, [X4, #255]
  LDR   B0, [X4, #-256]
  LDR   B0, [X4, #0]!
  LDR   B0, [X4, #255]!
  LDR   B0, [X4, #-256]!
  LDR   B0, [X4], #0
  LDR   B0, [X4], #1
  LDR   B0, [X4], #255
  LDR   B0, [X4], #-256

  LDR   H0, [X4]
  LDR   H0, [X4, #8]
  LDR   H0, [X4, #7]
  LDR   H0, [X4, #-5]
  LDR   H0, [X4, #8190]
  LDR   H0, [X4, #255]
  LDR   H0, [X4, #-256]
  LDR   H0, [X4, #0]!
  LDR   H0, [X4, #255]!
  LDR   H0, [X4, #-256]!
  LDR   H0, [X4], #0
  LDR   H0, [X4], #1
  LDR   H0, [X4], #255
  LDR   H0, [X4], #-256

  LDR   S0, [X4]
  LDR   S0, [X4, #8]
  LDR   S0, [X4, #7]
  LDR   S0, [X4, #-5]
  LDR   S0, [X4, #16380]
  LDR   S0, [X4, #255]
  LDR   S0, [X4, #-256]
  LDR   S0, [X4, #0]!
  LDR   S0, [X4, #255]!
  LDR   S0, [X4, #-256]!
  LDR   S0, [X4], #0
  LDR   S0, [X4], #1
  LDR   S0, [X4], #255
  LDR   S0, [X4], #-256

  LDR   D0, [X4]
  LDR   D0, [X4, #8]
  LDR   D0, [X4, #7]
  LDR   D0, [X4, #-5]
  LDR   D0, [X4, #32760]
  LDR   D0, [X4, #255]
  LDR   D0, [X4, #-256]
  LDR   D0, [X4, #0]!
  LDR   D0, [X4, #255]!
  LDR   D0, [X4, #-256]!
  LDR   D0, [X4], #0
  LDR   D0, [X4], #1
  LDR   D0, [X4], #255
  LDR   D0, [X4], #-256

  LDR   Q0, [X4]
  LDR   Q0, [X4, #8]
  LDR   Q0, [X4, #7]
  LDR   Q0, [X4, #-5]
  LDR   Q0, [X4, #65520]
  LDR   Q0, [X4, #255]
  LDR   Q0, [X4, #-256]
  LDR   Q0, [X4, #0]!
  LDR   Q0, [X4, #255]!
  LDR   Q0, [X4, #-256]!
  LDR   Q0, [X4], #0
  LDR   Q0, [X4], #1
  LDR   Q0, [X4], #255
  LDR   Q0, [X4], #-256

  ADD X4, X4, #0x30
  LDR   W3, [X4]
  LDR   W3, [X4, #8]
  LDR   W3, [X4, #7]
  LDR   W3, [X4, #-5]
  LDR   W3, [X4, #16380]
  LDR   W3, [X4, #255]
  LDR   W3, [X4, #-256]
  LDR   W3, [X4, #0]!
  LDR   W3, [X4, #255]!
  LDR   W3, [X4, #-256]!
  LDR   W3, [X4], #0
  LDR   W3, [X4], #1
  LDR   W3, [X4], #255
  LDR   W3, [X4], #-256

  ADD X4, X4, #0x30
  LDRB  W3, [X4]
  LDRB  W3, [X4, #8]
  LDRB  W3, [X4, #7]
  LDRB  W3, [X4, #-5]
  LDRB  W3, [X4, #4095]
  LDRB  W3, [X4, #255]
  LDRB  W3, [X4, #-256]
  LDRB  W3, [X4, #0]!
  LDRB  W3, [X4, #255]!
  LDRB  W3, [X4, #-256]!
  LDRB  W3, [X4], #0
  LDRB  W3, [X4], #1
  LDRB  W3, [X4], #255
  LDRB  W3, [X4], #-256

  ADD X4, X4, #0x30
  LDRH  W3, [X4]
  LDRH  W3, [X4, #8]
  LDRH  W3, [X4, #7]
  LDRH  W3, [X4, #-5]
  LDRH  W3, [X4, #8190]
  LDRH  W3, [X4, #255]
  LDRH  W3, [X4, #-256]
  LDRH  W3, [X4, #0]!
  LDRH  W3, [X4, #255]!
  LDRH  W3, [X4, #-256]!
  LDRH  W3, [X4], #0
  LDRH  W3, [X4], #1
  LDRH  W3, [X4], #255
  LDRH  W3, [X4], #-256

  ADD X4, X4, #0x30
  LDRSB X2, [X4]
  LDRSB X2, [X4, #8]
  LDRSB X2, [X4, #7]
  LDRSB X2, [X4, #-5]
  LDRSB X2, [X4, #4095]
  LDRSB X2, [X4, #255]
  LDRSB X2, [X4, #-256]
  LDRSB X2, [X4, #0]!
  LDRSB X2, [X4, #255]!
  LDRSB X2, [X4, #-256]!
  LDRSB X2, [X4], #0
  LDRSB X2, [X4], #1
  LDRSB X2, [X4], #255
  LDRSB X2, [X4], #-256

  ADD X4, X4, #0x30
  LDRSB W3, [X4]
  LDRSB W3, [X4, #8]
  LDRSB W3, [X4, #7]
  LDRSB W3, [X4, #-5]
  LDRSB W3, [X4, #4095]
  LDRSB W3, [X4, #255]
  LDRSB W3, [X4, #-256]
  LDRSB W3, [X4, #0]!
  LDRSB W3, [X4, #255]!
  LDRSB W3, [X4, #-256]!
  LDRSB W3, [X4], #0
  LDRSB W3, [X4], #1
  LDRSB W3, [X4], #255
  LDRSB W3, [X4], #-256

  ADD X4, X4, #0x30
  LDRSH X2, [X4]
  LDRSH X2, [X4, #8]
  LDRSH X2, [X4, #7]
  LDRSH X2, [X4, #-5]
  LDRSH X2, [X4, #8190]
  LDRSH X2, [X4, #255]
  LDRSH X2, [X4, #-256]
  LDRSH X2, [X4, #0]!
  LDRSH X2, [X4, #255]!
  LDRSH X2, [X4, #-256]!
  LDRSH X2, [X4], #0
  LDRSH X2, [X4], #1
  LDRSH X2, [X4], #255
  LDRSH X2, [X4], #-256

  ADD X4, X4, #0x30
  LDRSH W3, [X4]
  LDRSH W3, [X4, #8]
  LDRSH W3, [X4, #7]
  LDRSH W3, [X4, #-5]
  LDRSH W3, [X4, #8190]
  LDRSH W3, [X4, #255]
  LDRSH W3, [X4, #-256]
  LDRSH W3, [X4, #0]!
  LDRSH W3, [X4, #255]!
  LDRSH W3, [X4, #-256]!
  LDRSH W3, [X4], #0
  LDRSH W3, [X4], #1
  LDRSH W3, [X4], #255
  LDRSH W3, [X4], #-256

  ADD X4, X4, #0x30
  LDRSW X2, [X4]
  LDRSW X2, [X4, #8]
  LDRSW X2, [X4, #7]
  LDRSW X2, [X4, #-5]
  LDRSW X2, [X4, #16380]
  LDRSW X2, [X4, #255]
  LDRSW X2, [X4, #-256]
  LDRSW X2, [X4, #0]!
  LDRSW X2, [X4, #255]!
  LDRSW X2, [X4, #-256]!
  LDRSW X2, [X4], #0
  LDRSW X2, [X4], #1
  LDRSW X2, [X4], #255
  LDRSW X2, [X4], #-256


  // LDR {, B, H, SB, SH, SW}
  ADD X5, X0, #0x800
  MOV X6, #0
  MOV X7, #0x200
  LDR X2, [X5, X6]
  LDR X2, [X5, X7]
  LDR X2, [X5, X7, LSL #3]
  LDR X2, [X5, W7, UXTW #0]
  LDR X2, [X5, W7, UXTW #3]
  LDR X2, [X5, W7, SXTW #0]
  LDR X2, [X5, W7, SXTW #3]
  LDR X2, [X5, X7, SXTX #0]
  LDR X2, [X5, X7, SXTX #3]

  LDR B1, [X5, X6]
  LDR B1, [X5, X7]
  LDR B1, [X5, X7, LSL #0]
  LDR B1, [X5, W7, UXTW #0]
  LDR B1, [X5, W7, SXTW #0]
  LDR B1, [X5, X7, SXTX #0]

  LDR H2, [X5, X6]
  LDR H2, [X5, X7]
  LDR H2, [X5, X7, LSL #1]
  LDR H2, [X5, W7, UXTW #0]
  LDR H2, [X5, W7, UXTW #1]
  LDR H2, [X5, W7, SXTW #0]
  LDR H2, [X5, W7, SXTW #1]
  LDR H2, [X5, X7, SXTX #0]
  LDR H2, [X5, X7, SXTX #1]

  LDR S3, [X5, X6]
  LDR S3, [X5, X7]
  LDR S3, [X5, X7, LSL #2]
  LDR S3, [X5, W7, UXTW #0]
  LDR S3, [X5, W7, UXTW #2]
  LDR S3, [X5, W7, SXTW #0]
  LDR S3, [X5, W7, SXTW #2]
  LDR S3, [X5, X7, SXTX #0]
  LDR S3, [X5, X7, SXTX #2]

  LDR D4, [X5, X6]
  LDR D4, [X5, X7]
  LDR D4, [X5, X7, LSL #3]
  LDR D4, [X5, W7, UXTW #0]
  LDR D4, [X5, W7, UXTW #3]
  LDR D4, [X5, W7, SXTW #0]
  LDR D4, [X5, W7, SXTW #3]
  LDR D4, [X5, X7, SXTX #0]
  LDR D4, [X5, X7, SXTX #3]

  LDR Q0, [X5, X6]
  LDR Q0, [X5, X7]
  LDR Q0, [X5, X7, LSL #4]
  LDR Q0, [X5, W7, UXTW #0]
  LDR Q0, [X5, W7, UXTW #4]
  LDR Q0, [X5, W7, SXTW #0]
  LDR Q0, [X5, W7, SXTW #4]
  LDR Q0, [X5, X7, SXTX #0]
  LDR Q0, [X5, X7, SXTX #4]

  ADD X5, X5, #0x40
  LDRB W3, [X5, X6]
  LDRB W3, [X5, X7]
  LDRB W3, [X5, W7, UXTW #0]
  LDRB W3, [X5, W7, SXTW #0]
  LDRB W3, [X5, X7, SXTX #0]

  ADD X5, X5, #0x40
  LDRH w3, [X5, X6]
  LDRH w3, [X5, X7]
  LDRH w3, [X5, X7, LSL #1]
  LDRH w3, [X5, W7, UXTW #0]
  LDRH w3, [X5, W7, UXTW #1]
  LDRH w3, [X5, W7, SXTW #0]
  LDRH w3, [X5, W7, SXTW #1]
  LDRH w3, [X5, X7, SXTX #0]
  LDRH w3, [X5, X7, SXTX #1]

  ADD X5, X5, #0x40
  LDRSB X2, [X5, X6]
  LDRSB X2, [X5, X7]
  LDRSB X2, [X5, X7, LSL #0]
  LDRSB X2, [X5, W7, UXTW #0]
  LDRSB X2, [X5, W7, SXTW #0]
  LDRSB X2, [X5, X7, SXTX #0]

  ADD X5, X5, #0x40
  LDRSB W3, [X5, X6]
  LDRSB W3, [X5, X7]
  LDRSB W3, [X5, X7, LSL #0]
  LDRSB W3, [X5, W7, UXTW #0]
  LDRSB W3, [X5, W7, SXTW #0]
  LDRSB W3, [X5, X7, SXTX #0]

  ADD X5, X5, #0x40
  LDRSH X2, [X5, X6]
  LDRSH X2, [X5, X7]
  LDRSH X2, [X5, X7, LSL #0]
  LDRSH X2, [X5, X7, LSL #1]
  LDRSH X2, [X5, W7, UXTW #0]
  LDRSH X2, [X5, W7, UXTW #1]
  LDRSH X2, [X5, W7, SXTW #0]
  LDRSH X2, [X5, W7, SXTW #1]
  LDRSH X2, [X5, X7, SXTX #0]
  LDRSH X2, [X5, X7, SXTX #1]

  ADD X5, X5, #0x40
  LDRSH W3, [X5, X6]
  LDRSH W3, [X5, X7]
  LDRSH W3, [X5, X7, LSL #0]
  LDRSH W3, [X5, X7, LSL #1]
  LDRSH W3, [X5, W7, UXTW #0]
  LDRSH W3, [X5, W7, UXTW #1]
  LDRSH W3, [X5, W7, SXTW #0]
  LDRSH W3, [X5, W7, SXTW #1]
  LDRSH W3, [X5, X7, SXTX #0]
  LDRSH W3, [X5, X7, SXTX #1]

  ADD X5, X5, #0x40
  LDRSW X2, [X5, X6]
  LDRSW X2, [X5, X7]
  LDRSW X2, [X5, X7, LSL #0]
  LDRSW X2, [X5, X7, LSL #2]
  LDRSW X2, [X5, W7, UXTW #0]
  LDRSW X2, [X5, W7, UXTW #2]
  LDRSW X2, [X5, W7, SXTW #0]
  LDRSW X2, [X5, W7, SXTW #2]
  LDRSW X2, [X5, X7, SXTX #0]
  LDRSW X2, [X5, X7, SXTX #2]


  // LDRT {, B, H, SB, SH, SW}
  ADD X6, X0, #0x700
  LDTR X2, [X6]
  LDTR X2, [X6, #8]
  LDTR X2, [X6, #-6]
  LDTR X2, [X6, #255]
  LDTR X2, [X6, #-256]

  ADD X6, X6, #0x80
  LDTRB W3, [X6]
  LDTRB W3, [X6, #8]
  LDTRB W3, [X6, #-6]
  LDTRB W3, [X6, #255]
  LDTRB W3, [X6, #-256]

  ADD X6, X6, #0x80
  LDTRH W3, [X6]
  LDTRH W3, [X6, #8]
  LDTRH W3, [X6, #-6]
  LDTRH W3, [X6, #255]
  LDTRH W3, [X6, #-256]

  ADD X6, X6, #0x80
  LDTRSB X2, [X6]
  LDTRSB X2, [X6, #8]
  LDTRSB X2, [X6, #-6]
  LDTRSB X2, [X6, #255]
  LDTRSB X2, [X6, #-256]

  ADD X6, X6, #0x80
  LDTRSB W3, [X6]
  LDTRSB W3, [X6, #8]
  LDTRSB W3, [X6, #-6]
  LDTRSB W3, [X6, #255]
  LDTRSB W3, [X6, #-256]

  ADD X6, X6, #0x80
  LDTRSH X2, [X6]
  LDTRSH X2, [X6, #8]
  LDTRSH X2, [X6, #-6]
  LDTRSH X2, [X6, #255]
  LDTRSH X2, [X6, #-256]

  ADD X6, X6, #0x80
  LDTRSH W3, [X6]
  LDTRSH W3, [X6, #8]
  LDTRSH W3, [X6, #-6]
  LDTRSH W3, [X6, #255]
  LDTRSH W3, [X6, #-256]


  // STRI {, B, H}
  ADD X4, X0, #0x400
  STR   X2, [X4]
  STR   X2, [X4, #8]
  STR   X2, [X4, #7]
  STR   X2, [X4, #-5]
  STR   X2, [X4, #32760]
  STR   X2, [X4, #255]
  STR   X2, [X4, #-256]
  STR   X2, [X4, #0]!
  STR   X2, [X4, #255]!
  STR   X2, [X4, #-256]!
  STR   X2, [X4], #0
  STR   X2, [X4], #1
  STR   X2, [X4], #255
  STR   X2, [X4], #-256

  STR   B0, [X4]
  STR   B0, [X4, #8]
  STR   B0, [X4, #7]
  STR   B0, [X4, #-5]
  STR   B0, [X4, #4095]
  STR   B0, [X4, #255]
  STR   B0, [X4, #-256]
  STR   B0, [X4, #0]!
  STR   B0, [X4, #255]!
  STR   B0, [X4, #-256]!
  STR   B0, [X4], #0
  STR   B0, [X4], #1
  STR   B0, [X4], #255
  STR   B0, [X4], #-256

  STR   H0, [X4]
  STR   H0, [X4, #8]
  STR   H0, [X4, #7]
  STR   H0, [X4, #-5]
  STR   H0, [X4, #8190]
  STR   H0, [X4, #255]
  STR   H0, [X4, #-256]
  STR   H0, [X4, #0]!
  STR   H0, [X4, #255]!
  STR   H0, [X4, #-256]!
  STR   H0, [X4], #0
  STR   H0, [X4], #1
  STR   H0, [X4], #255
  STR   H0, [X4], #-256

  STR   S0, [X4]
  STR   S0, [X4, #8]
  STR   S0, [X4, #7]
  STR   S0, [X4, #-5]
  STR   S0, [X4, #16380]
  STR   S0, [X4, #255]
  STR   S0, [X4, #-256]
  STR   S0, [X4, #0]!
  STR   S0, [X4, #255]!
  STR   S0, [X4, #-256]!
  STR   S0, [X4], #0
  STR   S0, [X4], #1
  STR   S0, [X4], #255
  STR   S0, [X4], #-256

  STR   D0, [X4]
  STR   D0, [X4, #8]
  STR   D0, [X4, #7]
  STR   D0, [X4, #-5]
  STR   D0, [X4, #32760]
  STR   D0, [X4, #255]
  STR   D0, [X4, #-256]
  STR   D0, [X4, #0]!
  STR   D0, [X4, #255]!
  STR   D0, [X4, #-256]!
  STR   D0, [X4], #0
  STR   D0, [X4], #1
  STR   D0, [X4], #255
  STR   D0, [X4], #-256

  STR   Q0, [X4]
  STR   Q0, [X4, #8]
  STR   Q0, [X4, #7]
  STR   Q0, [X4, #-5]
  STR   Q0, [X4, #65520]
  STR   Q0, [X4, #255]
  STR   Q0, [X4, #-256]
  STR   Q0, [X4, #0]!
  STR   Q0, [X4, #255]!
  STR   Q0, [X4, #-256]!
  STR   Q0, [X4], #0
  STR   Q0, [X4], #1
  STR   Q0, [X4], #255
  STR   Q0, [X4], #-256

  ADD X4, X4, #0x30
  STR   W3, [X4]
  STR   W3, [X4, #8]
  STR   W3, [X4, #7]
  STR   W3, [X4, #-5]
  STR   W3, [X4, #16380]
  STR   W3, [X4, #255]
  STR   W3, [X4, #-256]
  STR   W3, [X4, #0]!
  STR   W3, [X4, #255]!
  STR   W3, [X4, #-256]!
  STR   W3, [X4], #0
  STR   W3, [X4], #1
  STR   W3, [X4], #255
  STR   W3, [X4], #-256

  ADD X4, X4, #0x30
  STRB  W3, [X4]
  STRB  W3, [X4, #8]
  STRB  W3, [X4, #7]
  STRB  W3, [X4, #-5]
  STRB  W3, [X4, #4095]
  STRB  W3, [X4, #255]
  STRB  W3, [X4, #-256]
  STRB  W3, [X4, #0]!
  STRB  W3, [X4, #255]!
  STRB  W3, [X4, #-256]!
  STRB  W3, [X4], #0
  STRB  W3, [X4], #1
  STRB  W3, [X4], #255
  STRB  W3, [X4], #-256

  ADD X4, X4, #0x30
  STRH  W3, [X4]
  STRH  W3, [X4, #8]
  STRH  W3, [X4, #7]
  STRH  W3, [X4, #-5]
  STRH  W3, [X4, #8190]
  STRH  W3, [X4, #255]
  STRH  W3, [X4, #-256]
  STRH  W3, [X4, #0]!
  STRH  W3, [X4, #255]!
  STRH  W3, [X4, #-256]!
  STRH  W3, [X4], #0
  STRH  W3, [X4], #1
  STRH  W3, [X4], #255
  STRH  W3, [X4], #-256


  // STR {, B, H}
  ADD X5, X0, #0x800
  MOV X6, #0
  MOV X7, #0x450
  STR X2, [X5, X6]
  STR X2, [X5, X7]
  STR X2, [X5, X7, LSL #3]
  STR X2, [X5, W7, UXTW #0]
  STR X2, [X5, W7, UXTW #3]
  STR X2, [X5, W7, SXTW #0]
  STR X2, [X5, W7, SXTW #3]
  STR X2, [X5, X7, SXTX #0]
  STR X2, [X5, X7, SXTX #3]

  STR B1, [X5, X6]
  STR B1, [X5, X7]
  STR B1, [X5, X7, LSL #0]
  STR B1, [X5, W7, UXTW #0]
  STR B1, [X5, W7, SXTW #0]
  STR B1, [X5, X7, SXTX #0]

  STR H2, [X5, X6]
  STR H2, [X5, X7]
  STR H2, [X5, X7, LSL #1]
  STR H2, [X5, W7, UXTW #0]
  STR H2, [X5, W7, UXTW #1]
  STR H2, [X5, W7, SXTW #0]
  STR H2, [X5, W7, SXTW #1]
  STR H2, [X5, X7, SXTX #0]
  STR H2, [X5, X7, SXTX #1]

  STR S3, [X5, X6]
  STR S3, [X5, X7]
  STR S3, [X5, X7, LSL #2]
  STR S3, [X5, W7, UXTW #0]
  STR S3, [X5, W7, UXTW #2]
  STR S3, [X5, W7, SXTW #0]
  STR S3, [X5, W7, SXTW #2]
  STR S3, [X5, X7, SXTX #0]
  STR S3, [X5, X7, SXTX #2]

  STR D4, [X5, X6]
  STR D4, [X5, X7]
  STR D4, [X5, X7, LSL #3]
  STR D4, [X5, W7, UXTW #0]
  STR D4, [X5, W7, UXTW #3]
  STR D4, [X5, W7, SXTW #0]
  STR D4, [X5, W7, SXTW #3]
  STR D4, [X5, X7, SXTX #0]
  STR D4, [X5, X7, SXTX #3]

  STR Q0, [X5, X6]
  STR Q0, [X5, X7]
  STR Q0, [X5, X7, LSL #4]
  STR Q0, [X5, W7, UXTW #0]
  STR Q0, [X5, W7, UXTW #4]
  STR Q0, [X5, W7, SXTW #0]
  STR Q0, [X5, W7, SXTW #4]
  STR Q0, [X5, X7, SXTX #0]
  STR Q0, [X5, X7, SXTX #4]

  ADD X5, X5, #0x40
  STRB W3, [X5, X6]
  STRB W3, [X5, X7]
  STRB W3, [X5, W7, UXTW #0]
  STRB W3, [X5, W7, SXTW #0]
  STRB W3, [X5, X7, SXTX #0]

  ADD X5, X5, #0x40
  STRH W3, [X5, X6]
  STRH W3, [X5, X7]
  STRH W3, [X5, X7, LSL #1]
  STRH W3, [X5, W7, UXTW #0]
  STRH W3, [X5, W7, UXTW #1]
  STRH W3, [X5, W7, SXTW #0]
  STRH W3, [X5, W7, SXTW #1]
  STRH W3, [X5, X7, SXTX #0]
  STRH W3, [X5, X7, SXTX #1]


  // STTR {, B, H}
  ADD X6, X0, #0x790
  STTR X2, [X6]
  STTR X2, [X6, #8]
  STTR X2, [X6, #-6]
  STTR X2, [X6, #255]
  STTR X2, [X6, #-256]

  ADD X6, X6, #0x80
  STTRB W3, [X6]
  STTRB W3, [X6, #8]
  STTRB W3, [X6, #-6]
  STTRB W3, [X6, #255]
  STTRB W3, [X6, #-256]

  ADD X6, X6, #0x80
  STTRH W3, [X6]
  STTRH W3, [X6, #8]
  STTRH W3, [X6, #-6]
  STTRH W3, [X6, #255]
  STTRH W3, [X6, #-256]


  // LDP
  ADD X7, X0, #0x900
  LDP X3, X4, [X7]
  LDP X3, X4, [X7, #-16]
  LDP X3, X4, [X7, #16]
  LDP X3, X4, [X7, #-512]
  LDP X3, X4, [X7, #504]
  LDP X5, X6, [X7], #0
  LDP X5, X6, [X7], #8
  LDP X5, X6, [X7], #504
  LDP X5, X6, [X7], #-512
  LDP X2, X3, [X7, #0]!
  LDP X2, X3, [X7, #8]!
  LDP X2, X3, [X7, #504]!
  LDP X2, X3, [X7, #-512]!

  LDP W3, W4, [X7]
  LDP W3, W4, [X7, #-16]
  LDP W3, W4, [X7, #16]
  LDP W3, W4, [X7, #-256]
  LDP W3, W4, [X7, #252]
  LDP W5, W6, [X7], #0
  LDP W5, W6, [X7], #8
  LDP W5, W6, [X7], #252
  LDP W5, W6, [X7], #-256
  LDP W2, W3, [X7, #0]!
  LDP W2, W3, [X7, #8]!
  LDP W2, W3, [X7, #252]!
  LDP W2, W3, [X7, #-256]!

  LDP S0, S1, [X7]
  LDP S0, S1, [X7, #-16]
  LDP S0, S1, [X7, #16]
  LDP S0, S1, [X7, #-256]
  LDP S0, S1, [X7, #252]
  LDP S0, S1, [X7], #0
  LDP S0, S1, [X7], #8
  LDP S0, S1, [X7], #252
  LDP S0, S1, [X7], #-256
  LDP S0, S1, [X7, #0]!
  LDP S0, S1, [X7, #8]!
  LDP S0, S1, [X7, #252]!
  LDP S0, S1, [X7, #-256]!

  LDP D0, D1, [X7]
  LDP D0, D1, [X7, #-16]
  LDP D0, D1, [X7, #16]
  LDP D0, D1, [X7, #-512]
  LDP D0, D1, [X7, #504]
  LDP D0, D1, [X7], #0
  LDP D0, D1, [X7], #8
  LDP D0, D1, [X7], #504
  LDP D0, D1, [X7], #-512
  LDP D0, D1, [X7, #0]!
  LDP D0, D1, [X7, #8]!
  LDP D0, D1, [X7, #504]!
  LDP D0, D1, [X7, #-512]!

  LDP Q0, Q1, [X7]
  LDP Q0, Q1, [X7, #-16]
  LDP Q0, Q1, [X7, #16]
  LDP Q0, Q1, [X7, #-1024]
  LDP Q0, Q1, [X7, #1008]
  LDP Q0, Q1, [X7], #0
  LDP Q0, Q1, [X7], #32
  LDP Q0, Q1, [X7], #1008
  LDP Q0, Q1, [X7], #-1024
  LDP Q0, Q1, [X7, #0]!
  LDP Q0, Q1, [X7, #48]!
  LDP Q0, Q1, [X7, #1008]!
  LDP Q0, Q1, [X7, #-1024]!


  // STP
  STP X3, X4, [X7]
  STP X3, X4, [X7, #-16]
  STP X3, X4, [X7, #16]
  STP X3, X4, [X7, #-512]
  STP X3, X4, [X7, #504]
  STP X5, X6, [X7], #0
  STP X5, X6, [X7], #8
  STP X5, X6, [X7], #504
  STP X5, X6, [X7], #-512
  STP X2, X3, [X7, #0]!
  STP X2, X3, [X7, #8]!
  STP X2, X3, [X7, #504]!
  STP X2, X3, [X7, #-512]!

  STP W3, W4, [X7]
  STP W3, W4, [X7, #-16]
  STP W3, W4, [X7, #16]
  STP W3, W4, [X7, #-256]
  STP W3, W4, [X7, #252]
  STP W5, W6, [X7], #0
  STP W5, W6, [X7], #8
  STP W5, W6, [X7], #252
  STP W5, W6, [X7], #-256
  STP W2, W3, [X7, #0]!
  STP W2, W3, [X7, #8]!
  STP W2, W3, [X7, #252]!
  STP W2, W3, [X7, #-256]!

  STP S2, S3, [X7]
  STP S2, S3, [X7, #-16]
  STP S2, S3, [X7, #16]
  STP S2, S3, [X7, #-256]
  STP S2, S3, [X7, #252]
  STP S2, S3, [X7], #0
  STP S2, S3, [X7], #8
  STP S2, S3, [X7], #252
  STP S2, S3, [X7], #-256
  STP S2, S3, [X7, #0]!
  STP S2, S3, [X7, #8]!
  STP S2, S3, [X7, #252]!
  STP S2, S3, [X7, #-256]!

  STP D2, D3, [X7]
  STP D2, D3, [X7, #-16]
  STP D2, D3, [X7, #16]
  STP D2, D3, [X7, #-512]
  STP D2, D3, [X7, #504]
  STP D2, D3, [X7], #0
  STP D2, D3, [X7], #8
  STP D2, D3, [X7], #504
  STP D2, D3, [X7], #-512
  STP D2, D3, [X7, #0]!
  STP D2, D3, [X7, #8]!
  STP D2, D3, [X7, #504]!
  STP D2, D3, [X7, #-512]!

  STP Q2, Q3, [X7]
  STP Q2, Q3, [X7, #-32]
  STP Q2, Q3, [X7, #32]
  STP Q2, Q3, [X7, #-1024]
  STP Q2, Q3, [X7, #1008]
  STP Q2, Q3, [X7], #0
  STP Q2, Q3, [X7], #32
  STP Q2, Q3, [X7], #1008
  STP Q2, Q3, [X7], #-1024
  STP Q2, Q3, [X7, #0]!
  STP Q2, Q3, [X7, #32]!
  STP Q2, Q3, [X7, #1008]!
  STP Q2, Q3, [X7, #-1024]!


  // LDNP
  LDNP X3, X4, [X7]
  LDNP X3, X4, [X7, #-16]
  LDNP X3, X4, [X7, #16]
  LDNP X3, X4, [X7, #-512]
  LDNP X3, X4, [X7, #504]
  LDNP W3, W4, [X7]
  LDNP W3, W4, [X7, #-16]
  LDNP W3, W4, [X7, #16]
  LDNP W3, W4, [X7, #-256]
  LDNP W3, W4, [X7, #252]

  LDNP S0, S1, [X7]
  LDNP S0, S1, [X7, #-16]
  LDNP S0, S1, [X7, #16]
  LDNP S0, S1, [X7, #-256]
  LDNP S0, S1, [X7, #252]
  LDNP D2, D3, [X7]
  LDNP D2, D3, [X7, #-16]
  LDNP D2, D3, [X7, #16]
  LDNP D2, D3, [X7, #-512]
  LDNP D2, D3, [X7, #504]
  LDNP Q2, Q3, [X7]
  LDNP Q2, Q3, [X7, #-16]
  LDNP Q2, Q3, [X7, #16]
  LDNP Q2, Q3, [X7, #-1024]
  LDNP Q2, Q3, [X7, #1008]


  // STNP
  STNP X3, X4, [X7]
  STNP X3, X4, [X7, #-16]
  STNP X3, X4, [X7, #16]
  STNP X3, X4, [X7, #-512]
  STNP X3, X4, [X7, #504]
  STNP W3, W4, [X7]
  STNP W3, W4, [X7, #-16]
  STNP W3, W4, [X7, #16]
  STNP W3, W4, [X7, #-256]
  STNP W3, W4, [X7, #252]

  STNP S0, S1, [X7]
  STNP S0, S1, [X7, #-16]
  STNP S0, S1, [X7, #16]
  STNP S0, S1, [X7, #-256]
  STNP S0, S1, [X7, #252]
  STNP D0, D1, [X7]
  STNP D0, D1, [X7, #-16]
  STNP D0, D1, [X7, #16]
  STNP D0, D1, [X7, #-512]
  STNP D0, D1, [X7, #504]
  STNP Q0, Q1, [X7]
  STNP Q0, Q1, [X7, #-16]
  STNP Q0, Q1, [X7, #16]
  STNP Q0, Q1, [X7, #-1024]
  STNP Q0, Q1, [X7, #1008]


  // LDPSW
  LDPSW X3, X4, [X7]
  LDPSW X3, X4, [X7, #-16]
  LDPSW X3, X4, [X7, #16]
  LDPSW X3, X4, [X7, #-256]
  LDPSW X3, X4, [X7, #252]
  LDPSW X5, X6, [X7], #0
  LDPSW X5, X6, [X7], #8
  LDPSW X5, X6, [X7], #252
  LDPSW X5, X6, [X7], #-256
  LDPSW X2, X3, [X7, #0]!
  LDPSW X2, X3, [X7, #8]!
  LDPSW X2, X3, [X7, #252]!
  LDPSW X2, X3, [X7, #-256]!


  // load-acquire
  MOV X2, X0
  MOV X3, X0
  LDAR X6, [X0]
  LDARB W7, [X2]
  LDARH W2, [X3]


  // store-release
  MOV X2, X0
  MOV X3, X0
  STLR X4, [X0]
  STLRB W5, [X2]
  STLRH W6, [X3]


  // Stack accesses
  STR X3, [SP, #-32]!
  STP X4, X5, [SP, #16]
  LDP X4, X5, [SP, #16]
  LDR X3, [SP], #32

  STP X2, X3, [SP, #-32]!
  STR X4, [SP, #24]
  LDR X4, [SP, #24]
  LDP X2, X3, [SP], #32

  SUB SP, SP, #16
  STR W2, [SP, #0]
  STR W3, [SP, #4]
  STP W4, W5, [SP, #8]
  LDP W4, W5, [SP, #8]
  LDR W3, [SP, #4]
  LDR W2, [SP], #16


  // Load / store exclusive
  ADD X5, X0, #0x30
  ADD X6, X0, #0x230

  LDXP X2, X3, [X0]
  STXP W4, X2, X3, [X0]

  LDXR X2, [X5]
  STXR W4, X2, [X5]

  LDXRB W3, [X6]
  STXRB W4, W3, [X6]

  LDXRH W2, [X0]
  STXRH W4, W2, [X0]


  // Prefetch hints - should not generate exceptions or be traced
  PRFM PLDL1KEEP, [X0]
  PRFM PLDL1STRM, [X0]
  PRFM PLDL2KEEP, [X0]
  PRFM PLDL2STRM, [X0]
  PRFM PLDL3KEEP, [X0]
  PRFM PLDL3STRM, [X0]
  PRFM PLDL1KEEP, [X0, #32760]
  PRFM PLDL1STRM, [X0, #32760]
  PRFM PLDL2KEEP, [X0, #32760]
  PRFM PLDL2STRM, [X0, #32760]
  PRFM PLDL3KEEP, [X0, #32760]
  PRFM PLDL3STRM, [X0, #32760]
  PRFM PLDL1KEEP, [X0, #32760]
  PRFM PLDL1STRM, [X0, #32760]
  PRFM PLDL2KEEP, [X0, #32760]
  PRFM PLDL2STRM, [X0, #32760]
  PRFM PLDL3KEEP, [X0, #32760]
  PRFM PLDL3STRM, [X0, #32760]

  PRFM PLIL1KEEP, [X0]
  PRFM PLIL1STRM, [X0]
  PRFM PLIL2KEEP, [X0]
  PRFM PLIL2STRM, [X0]
  PRFM PLIL3KEEP, [X0]
  PRFM PLIL3STRM, [X0]
  PRFM PLIL1KEEP, [X0, #32760]
  PRFM PLIL1STRM, [X0, #32760]
  PRFM PLIL2KEEP, [X0, #32760]
  PRFM PLIL2STRM, [X0, #32760]
  PRFM PLIL3KEEP, [X0, #32760]
  PRFM PLIL3STRM, [X0, #32760]

  PRFM PSTL1KEEP, [X0]
  PRFM PSTL1STRM, [X0]
  PRFM PSTL2KEEP, [X0]
  PRFM PSTL2STRM, [X0]
  PRFM PSTL3KEEP, [X0]
  PRFM PSTL3STRM, [X0]
  PRFM PSTL1KEEP, [X0, #32760]
  PRFM PSTL1STRM, [X0, #32760]
  PRFM PSTL2KEEP, [X0, #32760]
  PRFM PSTL2STRM, [X0, #32760]
  PRFM PSTL3KEEP, [X0, #32760]
  PRFM PSTL3STRM, [X0, #32760]

  PRFM PLDL1KEEP, [X0, #1]
  PRFM PLDL1STRM, [X0, #1]
  PRFM PLDL2KEEP, [X0, #1]
  PRFM PLDL2STRM, [X0, #1]
  PRFM PLDL3KEEP, [X0, #1]
  PRFM PLDL3STRM, [X0, #1]
  PRFM PLDL1KEEP, [X0, #-256]
  PRFM PLDL1STRM, [X0, #-256]
  PRFM PLDL2KEEP, [X0, #-256]
  PRFM PLDL2STRM, [X0, #-256]
  PRFM PLDL3KEEP, [X0, #-256]
  PRFM PLDL3STRM, [X0, #-256]

  ADD X6, X0, #0x330
  PRFM PLDL1KEEP, [X0, X6]
  PRFM PLDL1KEEP, [X0, X6, LSL #3]
  PRFM PLDL1KEEP, [X0, X6, SXTX #0]
  PRFM PLDL1KEEP, [X0, X6, SXTX #3]
  PRFM PLDL1KEEP, [X0, W6, UXTW #0]
  PRFM PLDL1KEEP, [X0, W6, UXTW #3]
  PRFM PLDL1KEEP, [X0, W6, SXTW #0]
  PRFM PLDL1KEEP, [X0, W6, SXTW #3]

  PRFM PSTL1KEEP, [X0, X6]
  PRFM PSTL1KEEP, [X0, X6, LSL #3]
  PRFM PSTL1KEEP, [X0, X6, SXTX #0]
  PRFM PSTL1KEEP, [X0, X6, SXTX #3]
  PRFM PSTL1KEEP, [X0, W6, UXTW #0]
  PRFM PSTL1KEEP, [X0, W6, UXTW #3]
  PRFM PSTL1KEEP, [X0, W6, SXTW #0]
  PRFM PSTL1KEEP, [X0, W6, SXTW #3]

  PRFM PLIL1KEEP, [X0, X6]
  PRFM PLIL1KEEP, [X0, X6, LSL #3]
  PRFM PLIL1KEEP, [X0, X6, SXTX #0]
  PRFM PLIL1KEEP, [X0, X6, SXTX #3]
  PRFM PLIL1KEEP, [X0, W6, UXTW #0]
  PRFM PLIL1KEEP, [X0, W6, UXTW #3]
  PRFM PLIL1KEEP, [X0, W6, SXTW #0]
  PRFM PLIL1KEEP, [X0, W6, SXTW #3]

  PRFM PLDL1KEEP, lit_a64
  PRFM PLDL1STRM, lit_a64
  PRFM PLDL2KEEP, lit_a64
  PRFM PLDL2STRM, lit_a64
  PRFM PLDL3KEEP, lit_a64
  PRFM PLDL3STRM, lit_a64

  PRFM PLIL1KEEP, lit_a64
  PRFM PLIL1STRM, lit_a64
  PRFM PLIL2KEEP, lit_a64
  PRFM PLIL2STRM, lit_a64
  PRFM PLIL3KEEP, lit_a64
  PRFM PLIL3STRM, lit_a64

  PRFM PSTL1KEEP, lit_a64
  PRFM PSTL1STRM, lit_a64
  PRFM PSTL2KEEP, lit_a64
  PRFM PSTL2STRM, lit_a64
  PRFM PSTL3KEEP, lit_a64
  PRFM PSTL3STRM, lit_a64


  // ADVSIMD LD1
  ADD X5, X0, #0x500
  LD1 {v0.8b}, [X5]
  LD1 {v0.8b}, [X5], #8
  LD1 {v0.16b}, [X5]
  LD1 {v0.16b}, [X5], #16
  LD1 {v0.4h}, [X5]
  LD1 {v0.4h}, [X5], #8
  LD1 {v0.8h}, [X5]
  LD1 {v0.8h}, [X5], #16
  LD1 {v0.2s}, [X5]
  LD1 {v0.2s}, [X5], #8
  LD1 {v0.4s}, [X5]
  LD1 {v0.4s}, [X5], #16
  LD1 {v0.1d}, [X5]
  LD1 {v0.1d}, [X5], #8
  LD1 {v0.2d}, [X5]
  LD1 {v0.2d}, [X5], #16

  MOV X1, #16
  LD1 {v0.b}[1], [X0]
  LD1 {v0.b}[1], [X0], #1
  LD1 {v0.b}[1], [X0], X1
  LD1 {v2.h}[2], [X0]
  LD1 {v2.h}[2], [X0], #2
  LD1 {v2.h}[2], [X0], X1
  LD1 {v2.s}[2], [X0]
  LD1 {v2.s}[2], [X0], #4
  LD1 {v2.s}[2], [X0], X1
  LD1 {v2.d}[1], [X0]
  LD1 {v2.d}[1], [X0], #8
  LD1 {v2.d}[1], [X0], X1

  MOV X1, #32
  LD1R {v1.8b}, [X0]
  LD1R {v1.8b}, [X0], #1
  LD1R {v1.8b}, [X0], X1
  LD1R {v1.16b}, [X0]
  LD1R {v1.16b}, [X0], #1
  LD1R {v1.16b}, [X0], X1
  LD1R {v1.4h}, [X0]
  LD1R {v1.4h}, [X0], #2
  LD1R {v1.4h}, [X0], X1
  LD1R {v1.8h}, [X0]
  LD1R {v1.8h}, [X0], #2
  LD1R {v1.8h}, [X0], X1
  LD1R {v1.2s}, [X0]
  LD1R {v1.2s}, [X0], #4
  LD1R {v1.2s}, [X0], X1
  LD1R {v1.4s}, [X0]
  LD1R {v1.4s}, [X0], #4
  LD1R {v1.4s}, [X0], X1
  LD1R {v1.1d}, [X0]
  LD1R {v1.1d}, [X0], #8
  LD1R {v1.1d}, [X0], X1
  LD1R {v1.2d}, [X0]
  LD1R {v1.2d}, [X0], #8
  LD1R {v1.2d}, [X0], X1


  // ADVSIMD LD2
  ADD X6, X0, #0x200
  LD2 {v0.8b-v1.8b}, [X6]
  LD2 {v0.8b-v1.8b}, [X6], #16
  LD2 {v0.16b-v1.16b}, [X6]
  LD2 {v0.16b-v1.16b}, [X6], #32
  LD2 {v0.4h-v1.4h}, [X6]
  LD2 {v0.4h-v1.4h}, [X6], #16
  LD2 {v0.8h-v1.8h}, [X6]
  LD2 {v0.8h-v1.8h}, [X6], #32
  LD2 {v0.2s-v1.2s}, [X6]
  LD2 {v0.2s-v1.2s}, [X6], #16
  LD2 {v0.4s-v1.4s}, [X6]
  LD2 {v0.4s-v1.4s}, [X6], #32
  LD2 {v0.2d-v1.2d}, [X6]
  LD2 {v0.2d-v1.2d}, [X6], #32

  MOV X1, #64
  LD2 {v0.b-v1.b}[1], [X0]
  LD2 {v0.b-v1.b}[1], [X0], #2
  LD2 {v0.b-v1.b}[1], [X0], X1
  LD2 {v2.h-v3.h}[2], [X0]
  LD2 {v2.h-v3.h}[2], [X0], #4
  LD2 {v2.h-v3.h}[2], [X0], X1
  LD2 {v2.s-v3.s}[2], [X0]
  LD2 {v2.s-v3.s}[2], [X0], #8
  LD2 {v2.s-v3.s}[2], [X0], X1
  LD2 {v2.d-v3.d}[1], [X0]
  LD2 {v2.d-v3.d}[1], [X0], #16
  LD2 {v2.d-v3.d}[1], [X0], X1

  MOV X1, #96
  LD2R {v1.8b-v2.8b}, [X0]
  LD2R {v1.8b-v2.8b}, [X0], #2
  LD2R {v1.8b-v2.8b}, [X0], X1
  LD2R {v1.16b-v2.16b}, [X0]
  LD2R {v1.16b-v2.16b}, [X0], #2
  LD2R {v1.16b-v2.16b}, [X0], X1
  LD2R {v1.4h-v2.4h}, [X0]
  LD2R {v1.4h-v2.4h}, [X0], #4
  LD2R {v1.4h-v2.4h}, [X0], X1
  LD2R {v1.8h-v2.8h}, [X0]
  LD2R {v1.8h-v2.8h}, [X0], #4
  LD2R {v1.8h-v2.8h}, [X0], X1
  LD2R {v1.2s-v2.2s}, [X0]
  LD2R {v1.2s-v2.2s}, [X0], #8
  LD2R {v1.2s-v2.2s}, [X0], X1
  LD2R {v1.4s-v2.4s}, [X0]
  LD2R {v1.4s-v2.4s}, [X0], #8
  LD2R {v1.4s-v2.4s}, [X0], X1
  LD2R {v1.1d-v2.1d}, [X0]
  LD2R {v1.1d-v2.1d}, [X0], #16
  LD2R {v1.1d-v2.1d}, [X0], X1
  LD2R {v1.2d-v2.2d}, [X0]
  LD2R {v1.2d-v2.2d}, [X0], #16
  LD2R {v1.2d-v2.2d}, [X0], X1


  // ADVSIMD LD3
  ADD X6, X0, #0x200
  LD3 {v0.8b-v2.8b}, [X6]
  LD3 {v0.8b-v2.8b}, [X6], #24
  LD3 {v0.16b-v2.16b}, [X6]
  LD3 {v0.16b-v2.16b}, [X6], #48
  LD3 {v0.4h-v2.4h}, [X6]
  LD3 {v0.4h-v2.4h}, [X6], #24
  LD3 {v0.8h-v2.8h}, [X6]
  LD3 {v0.8h-v2.8h}, [X6], #48
  LD3 {v0.2s-v2.2s}, [X6]
  LD3 {v0.2s-v2.2s}, [X6], #24
  LD3 {v0.4s-v2.4s}, [X6]
  LD3 {v0.4s-v2.4s}, [X6], #48
  LD3 {v0.2d-v2.2d}, [X6]
  LD3 {v0.2d-v2.2d}, [X6], #48

  MOV X1, #64
  LD3 {v0.b-v2.b}[1], [X0]
  LD3 {v0.b-v2.b}[1], [X0], #3
  LD3 {v0.b-v2.b}[1], [X0], X1
  LD3 {v2.h-v4.h}[2], [X0]
  LD3 {v2.h-v4.h}[2], [X0], #6
  LD3 {v2.h-v4.h}[2], [X0], X1
  LD3 {v2.s-v4.s}[2], [X0]
  LD3 {v2.s-v4.s}[2], [X0], #12
  LD3 {v2.s-v4.s}[2], [X0], X1
  LD3 {v2.d-v4.d}[1], [X0]
  LD3 {v2.d-v4.d}[1], [X0], #24
  LD3 {v2.d-v4.d}[1], [X0], X1

  MOV X1, #96
  LD3R {v1.8b-v3.8b}, [X0]
  LD3R {v1.8b-v3.8b}, [X0], #3
  LD3R {v1.8b-v3.8b}, [X0], X1
  LD3R {v1.16b-v3.16b}, [X0]
  LD3R {v1.16b-v3.16b}, [X0], #3
  LD3R {v1.16b-v3.16b}, [X0], X1
  LD3R {v1.4h-v3.4h}, [X0]
  LD3R {v1.4h-v3.4h}, [X0], #6
  LD3R {v1.4h-v3.4h}, [X0], X1
  LD3R {v1.8h-v3.8h}, [X0]
  LD3R {v1.8h-v3.8h}, [X0], #6
  LD3R {v1.8h-v3.8h}, [X0], X1
  LD3R {v1.2s-v3.2s}, [X0]
  LD3R {v1.2s-v3.2s}, [X0], #12
  LD3R {v1.2s-v3.2s}, [X0], X1
  LD3R {v1.4s-v3.4s}, [X0]
  LD3R {v1.4s-v3.4s}, [X0], #12
  LD3R {v1.4s-v3.4s}, [X0], X1
  LD3R {v1.1d-v3.1d}, [X0]
  LD3R {v1.1d-v3.1d}, [X0], #24
  LD3R {v1.1d-v3.1d}, [X0], X1
  LD3R {v1.2d-v3.2d}, [X0]
  LD3R {v1.2d-v3.2d}, [X0], #24
  LD3R {v1.2d-v3.2d}, [X0], X1


  // ADVSIMD LD4
  ADD X6, X0, #0x200
  LD4 {v0.8b-v3.8b}, [X6]
  LD4 {v0.8b-v3.8b}, [X6], #32
  LD4 {v0.16b-v3.16b}, [X6]
  LD4 {v0.16b-v3.16b}, [X6], #64
  LD4 {v0.4h-v3.4h}, [X6]
  LD4 {v0.4h-v3.4h}, [X6], #32
  LD4 {v0.8h-v3.8h}, [X6]
  LD4 {v0.8h-v3.8h}, [X6], #64
  LD4 {v0.2s-v3.2s}, [X6]
  LD4 {v0.2s-v3.2s}, [X6], #32
  LD4 {v0.4s-v3.4s}, [X6]
  LD4 {v0.4s-v3.4s}, [X6], #64
  LD4 {v0.2d-v3.2d}, [X6]
  LD4 {v0.2d-v3.2d}, [X6], #64

  MOV X1, #64
  LD4 {v0.b-v3.b}[1], [X0]
  LD4 {v0.b-v3.b}[1], [X0], #4
  LD4 {v0.b-v3.b}[1], [X0], X1
  LD4 {v2.h-v5.h}[2], [X0]
  LD4 {v2.h-v5.h}[2], [X0], #8
  LD4 {v2.h-v5.h}[2], [X0], X1
  LD4 {v2.s-v5.s}[2], [X0]
  LD4 {v2.s-v5.s}[2], [X0], #16
  LD4 {v2.s-v5.s}[2], [X0], X1
  LD4 {v2.d-v5.d}[1], [X0]
  LD4 {v2.d-v5.d}[1], [X0], #32
  LD4 {v2.d-v5.d}[1], [X0], X1

  MOV X1, #96
  LD4R {v1.8b-v4.8b}, [X0]
  LD4R {v1.8b-v4.8b}, [X0], #4
  LD4R {v1.8b-v4.8b}, [X0], X1
  LD4R {v1.16b-v4.16b}, [X0]
  LD4R {v1.16b-v4.16b}, [X0], #4
  LD4R {v1.16b-v4.16b}, [X0], X1
  LD4R {v1.4h-v4.4h}, [X0]
  LD4R {v1.4h-v4.4h}, [X0], #8
  LD4R {v1.4h-v4.4h}, [X0], X1
  LD4R {v1.8h-v4.8h}, [X0]
  LD4R {v1.8h-v4.8h}, [X0], #8
  LD4R {v1.8h-v4.8h}, [X0], X1
  LD4R {v1.2s-v4.2s}, [X0]
  LD4R {v1.2s-v4.2s}, [X0], #16
  LD4R {v1.2s-v4.2s}, [X0], X1
  LD4R {v1.4s-v4.4s}, [X0]
  LD4R {v1.4s-v4.4s}, [X0], #16
  LD4R {v1.4s-v4.4s}, [X0], X1
  LD4R {v1.1d-v4.1d}, [X0]
  LD4R {v1.1d-v4.1d}, [X0], #32
  LD4R {v1.1d-v4.1d}, [X0], X1
  LD4R {v1.2d-v4.2d}, [X0]
  LD4R {v1.2d-v4.2d}, [X0], #32
  LD4R {v1.2d-v4.2d}, [X0], X1


  // ADVSIMD ST1
  ADD X5, X0, #0xd00
  ST1 {v0.8b}, [X5]
  ST1 {v0.8b}, [X5], #8
  ST1 {v0.16b}, [X5]
  ST1 {v0.16b}, [X5], #16
  ST1 {v0.4h}, [X5]
  ST1 {v0.4h}, [X5], #8
  ST1 {v0.8h}, [X5]
  ST1 {v0.8h}, [X5], #16
  ST1 {v0.2s}, [X5]
  ST1 {v0.2s}, [X5], #8
  ST1 {v0.4s}, [X5]
  ST1 {v0.4s}, [X5], #16
  ST1 {v0.1d}, [X5]
  ST1 {v0.1d}, [X5], #8
  ST1 {v0.2d}, [X5]
  ST1 {v0.2d}, [X5], #16

  MOV X1, #16
  ST1 {v0.b}[1], [X0]
  ST1 {v0.b}[1], [X0], #1
  ST1 {v0.b}[1], [X0], X1
  ST1 {v2.h}[2], [X0]
  ST1 {v2.h}[2], [X0], #2
  ST1 {v2.h}[2], [X0], X1
  ST1 {v2.s}[2], [X0]
  ST1 {v2.s}[2], [X0], #4
  ST1 {v2.s}[2], [X0], X1
  ST1 {v2.d}[1], [X0]
  ST1 {v2.d}[1], [X0], #8
  ST1 {v2.d}[1], [X0], X1


  // ADVSIMD ST2
  ADD X6, X0, #0x200
  ST2 {v0.8b-v1.8b}, [X6]
  ST2 {v0.8b-v1.8b}, [X6], #16
  ST2 {v0.16b-v1.16b}, [X6]
  ST2 {v0.16b-v1.16b}, [X6], #32
  ST2 {v0.4h-v1.4h}, [X6]
  ST2 {v0.4h-v1.4h}, [X6], #16
  ST2 {v0.8h-v1.8h}, [X6]
  ST2 {v0.8h-v1.8h}, [X6], #32
  ST2 {v0.2s-v1.2s}, [X6]
  ST2 {v0.2s-v1.2s}, [X6], #16
  ST2 {v0.4s-v1.4s}, [X6]
  ST2 {v0.4s-v1.4s}, [X6], #32
  ST2 {v0.2d-v1.2d}, [X6]
  ST2 {v0.2d-v1.2d}, [X6], #32

  MOV X1, #64
  ST2 {v0.b-v1.b}[1], [X0]
  ST2 {v0.b-v1.b}[1], [X0], #2
  ST2 {v0.b-v1.b}[1], [X0], X1
  ST2 {v2.h-v3.h}[2], [X0]
  ST2 {v2.h-v3.h}[2], [X0], #4
  ST2 {v2.h-v3.h}[2], [X0], X1
  ST2 {v2.s-v3.s}[2], [X0]
  ST2 {v2.s-v3.s}[2], [X0], #8
  ST2 {v2.s-v3.s}[2], [X0], X1
  ST2 {v2.d-v3.d}[1], [X0]
  ST2 {v2.d-v3.d}[1], [X0], #16
  ST2 {v2.d-v3.d}[1], [X0], X1


  // ADVSIMD ST3
  ADD X6, X0, #0x200
  ST3 {v0.8b-v2.8b}, [X6]
  ST3 {v0.8b-v2.8b}, [X6], #24
  ST3 {v0.16b-v2.16b}, [X6]
  ST3 {v0.16b-v2.16b}, [X6], #48
  ST3 {v0.4h-v2.4h}, [X6]
  ST3 {v0.4h-v2.4h}, [X6], #24
  ST3 {v0.8h-v2.8h}, [X6]
  ST3 {v0.8h-v2.8h}, [X6], #48
  ST3 {v0.2s-v2.2s}, [X6]
  ST3 {v0.2s-v2.2s}, [X6], #24
  ST3 {v0.4s-v2.4s}, [X6]
  ST3 {v0.4s-v2.4s}, [X6], #48
  ST3 {v0.2d-v2.2d}, [X6]
  ST3 {v0.2d-v2.2d}, [X6], #48

  MOV X1, #64
  ST3 {v0.b-v2.b}[1], [X0]
  ST3 {v0.b-v2.b}[1], [X0], #3
  ST3 {v0.b-v2.b}[1], [X0], X1
  ST3 {v2.h-v4.h}[2], [X0]
  ST3 {v2.h-v4.h}[2], [X0], #6
  ST3 {v2.h-v4.h}[2], [X0], X1
  ST3 {v2.s-v4.s}[2], [X0]
  ST3 {v2.s-v4.s}[2], [X0], #12
  ST3 {v2.s-v4.s}[2], [X0], X1
  ST3 {v2.d-v4.d}[1], [X0]
  ST3 {v2.d-v4.d}[1], [X0], #24
  ST3 {v2.d-v4.d}[1], [X0], X1


  // ADVSIMD ST4
  ADD X6, X0, #0x200
  ST4 {v0.8b-v3.8b}, [X6]
  ST4 {v0.8b-v3.8b}, [X6], #32
  ST4 {v0.16b-v3.16b}, [X6]
  ST4 {v0.16b-v3.16b}, [X6], #64
  ST4 {v0.4h-v3.4h}, [X6]
  ST4 {v0.4h-v3.4h}, [X6], #32
  ST4 {v0.8h-v3.8h}, [X6]
  ST4 {v0.8h-v3.8h}, [X6], #64
  ST4 {v0.2s-v3.2s}, [X6]
  ST4 {v0.2s-v3.2s}, [X6], #32
  ST4 {v0.4s-v3.4s}, [X6]
  ST4 {v0.4s-v3.4s}, [X6], #64
  ST4 {v0.2d-v3.2d}, [X6]
  ST4 {v0.2d-v3.2d}, [X6], #64

  MOV X1, #64
  ST4 {v0.b-v3.b}[1], [X0]
  ST4 {v0.b-v3.b}[1], [X0], #4
  ST4 {v0.b-v3.b}[1], [X0], X1
  ST4 {v2.h-v5.h}[2], [X0]
  ST4 {v2.h-v5.h}[2], [X0], #8
  ST4 {v2.h-v5.h}[2], [X0], X1
  ST4 {v2.s-v5.s}[2], [X0]
  ST4 {v2.s-v5.s}[2], [X0], #16
  ST4 {v2.s-v5.s}[2], [X0], X1
  ST4 {v2.d-v5.d}[1], [X0]
  ST4 {v2.d-v5.d}[1], [X0], #32
  ST4 {v2.d-v5.d}[1], [X0], X1


  // Literal loads - not trapped
  LDR   X2, lit_a64
  LDR   W3, lit_a64
  LDRSW X2, lit_a64
  LDR   S0, lit_a64
  LDR   D1, lit_a64
  LDR   Q2, lit_a64


  MOV SP, X29
  RET

lit_a64: .quad 0

.endfunc

.global end_test_a64
end_test_a64:
#endif

```

`test/load_store.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>
#include <string.h>
#include <assert.h>
#include <signal.h>
#include <stdbool.h>
#include <stdint.h>
#include <sys/mman.h>

#ifdef __arm__
#include "../pie/pie-thumb-encoder.h"
#include "../pie/pie-thumb-decoder.h"
#include "../pie/pie-arm-encoder.h"
#include "../pie/pie-arm-decoder.h"
#elif __aarch64__
#include "../pie/pie-a64-encoder.h"
#endif

#define CODE_SIZE  (1024*1024)
#define HEAP_SIZE  (1024*1024)
#define STACK_SIZE (1024*1024)
#define CODE_BASE  ((void *)0x50000000)
#define HEAP_BASE  ((void *)0x60000000)
#define STACK_BASE ((void *)0x70000000)

#define CPSR_T (0x20)

typedef void (*ld_st_test)(void *heap, void *stack);
void *heap, *stack, *code;
uint32_t orig_inst = 0;

#ifdef __arm__
extern void test_thumb16(void *, void *);
extern void *end_test_thumb16;
extern void test_thumb32(void *, void *);
extern void *end_test_thumb32;
extern void test_a32(void *, void *);
extern void *end_test_a32;
#define ucontext_pc uc_mcontext.arm_pc

#elif __aarch64__
extern void test_a64(void *, void *);
extern void *end_test_a64;

#define ucontext_pc uc_mcontext.pc
#endif

void test_wrapper(char *name, ld_st_test test, void* test_end, void *heap, void *stack) {
  uintptr_t test_addr = (uintptr_t)test;
  ld_st_test test_rw = code;
  size_t test_size = test_end - (void *)test + 1;
  assert(test_size <= CODE_SIZE);

#ifdef __arm__
  uintptr_t is_thumb = test_addr & 1;
  test_addr &= ~1;
  test_rw = code + is_thumb;
#endif
  memcpy(code, (void *)test_addr, test_size);
  __clear_cache(code, code + test_size);

  printf("start: %s\n", name);
  test_rw(heap, stack);
  printf("end: %s\n", name);
}

void print_addr_and_retry(int sig, siginfo_t *info, void *c) {
  assert(sig == SIGSEGV);
  ucontext_t *cont = (ucontext_t *)c;
  int ret = mprotect(heap,  HEAP_SIZE, PROT_READ | PROT_WRITE);
  assert(ret == 0);
  ret = mprotect(stack, STACK_SIZE, PROT_READ | PROT_WRITE);
  assert(ret == 0);
#ifdef __arm__
  if (cont->uc_mcontext.arm_cpsr & CPSR_T) {
    uint16_t *bkpt = (uint16_t *)cont->uc_mcontext.arm_pc;
    thumb_instruction inst = thumb_decode(bkpt);
    bkpt += (inst < THUMB_ADC32) ? 1 : 2;
    orig_inst = *(uint32_t *)bkpt;
    thumb_bkpt16(&bkpt, 0);
    __clear_cache(bkpt, bkpt+1);
  } else {
    uint32_t *bkpt = (uint32_t *)cont->uc_mcontext.arm_pc;
    bkpt++;
    orig_inst = *bkpt;
    arm_bkpt(&bkpt, 0, 0);
    __clear_cache(bkpt, bkpt+1);
  }
#elif __aarch64__
  uint32_t *bkpt = (uint32_t *)cont->ucontext_pc;
  bkpt++;
  orig_inst = *bkpt;
  a64_BRK(&bkpt, 0);
  __clear_cache(bkpt, bkpt+1);
#endif
  printf("%p\n", info->si_addr);
}

void remove_breakpoint(int sig, siginfo_t *info, void *c) {
  int ret = mprotect(heap, HEAP_SIZE, PROT_NONE);
  assert(ret == 0);
  ret = mprotect(stack, STACK_SIZE, PROT_NONE);
  assert(ret == 0);

  ucontext_t *cont = (ucontext_t *)c;
  uint32_t *bkpt = (uint32_t *)cont->ucontext_pc;
  *bkpt = orig_inst;
  __clear_cache(bkpt, bkpt+1);

}

int main(int argc, char **argv) {
  bool print_trace = false;
  if (argc >= 2 && strcmp(argv[1], "-t") == 0) {
    print_trace = true;

    // Set up a signal stack
    void *sigstack = mmap(NULL, SIGSTKSZ, PROT_READ | PROT_WRITE,
                          MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    assert(sigstack != MAP_FAILED);
    
    stack_t ss;
    ss.ss_sp = sigstack;
    ss.ss_flags = 0;
    ss.ss_size = SIGSTKSZ;
    int ret = sigaltstack(&ss, NULL);
    
    // Register the signal handler
    struct sigaction act;
    act.sa_sigaction = print_addr_and_retry;
    act.sa_flags = SA_SIGINFO | SA_ONSTACK;
    sigemptyset(&act.sa_mask);
    ret = sigaction(SIGSEGV, &act, NULL);
    assert(ret == 0);
    
    act.sa_sigaction = remove_breakpoint;
    ret = sigaction(SIGTRAP, &act, NULL);
    assert(ret == 0);
  }

  code = mmap(CODE_BASE, CODE_SIZE, PROT_READ | PROT_WRITE | PROT_EXEC,
              MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED, -1, 0);
  assert(code != MAP_FAILED);

  heap = mmap(HEAP_BASE, HEAP_SIZE, print_trace ? 0 : (PROT_READ | PROT_WRITE),
              MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED, -1, 0);
  assert(heap != MAP_FAILED);

  stack = mmap(STACK_BASE, STACK_SIZE, print_trace ? 0 : (PROT_READ | PROT_WRITE),
               MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED, -1, 0);
  assert(stack != MAP_FAILED);

#ifdef __arm__
  test_wrapper("thumb16", test_thumb16, (void *)&end_test_thumb16,
               heap, stack+STACK_SIZE);
  test_wrapper("thumb32", test_thumb32, (void *)&end_test_thumb32,
               heap, stack+STACK_SIZE);
  test_wrapper("a32", test_a32, (void *)&end_test_a32, heap, stack+STACK_SIZE);
#elif __aarch64__
  test_wrapper("a64", test_a64, (void *)&end_test_a64, heap, stack+STACK_SIZE);
#endif
}

```

`test/makefile`:

```
ARCH=$(shell $(CC) -dumpmachine | awk -F '-' '{print $$1}')
ifeq ($(findstring arm, $(ARCH)), arm)
	PIE_ENCODER = ../pie/pie-arm-encoder.o ../pie/pie-thumb-encoder.o
	PIE_DECODER = ../pie/pie-arm-decoder.o ../pie/pie-thumb-decoder.o
	CFLAGS+=-march=armv7-a -mfpu=neon
endif
ifeq ($(ARCH),aarch64)
	PIE_ENCODER = ../pie/pie-a64-encoder.o
	PIE_DECODER = ../pie/pie-a64-decoder.o
endif

CFLAGS+=-std=gnu99
LDFLAGS+=-lpthread

.PHONY: clean

portable: mmap_munmap mprotect_exec self_modifying signals load_store

aarch32: portable hw_div

aarch64: portable

hw_div: hw_div.S
	$(CC) -mcpu=cortex-a15 $< $(LDFLAGS) -o $@

self_modifying: $(PIE_ENCODER) self_modifying.c
	$(CC) $(CFLAGS) $^ $(LDFLAGS) -o $@

signals: $(PIE_ENCODER) signals.c signals.S
	$(CC) $(CFLAGS) $^ $(LDFLAGS) -o $@

load_store: $(PIE_ENCODER) $(PIE_DECODER) load_store.c load_store.S
	$(CC) -g $(CFLAGS) $^ $(LDFLAGS) -o $@

clean:
	rm -f mmap_munmap mprotect_exec self_modifying signals hw_div load_store

```

`test/mmap_munmap.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdlib.h>
#include <assert.h>
#include <sys/mman.h>
#include <errno.h>

#define PAGESZ   4096
#define BASEADDR 0x80000

int main() {
  int ret;

  // Failing allocation
  void *alloc = mmap(NULL, 0, PROT_EXEC | PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
  assert(alloc == MAP_FAILED);

  // One page allocation
  alloc = mmap(NULL, PAGESZ, PROT_EXEC | PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
  assert(alloc != MAP_FAILED);

  ret = munmap(alloc, PAGESZ);
  assert (ret == 0);

  // Multiple page allocation, for testing partial unmapping
  alloc = mmap(NULL, PAGESZ*10, PROT_EXEC | PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
  assert(alloc != MAP_FAILED);

  // Unmap the first page
  ret = munmap(alloc, PAGESZ);
  assert (ret == 0);
  alloc += PAGESZ;

  // Unmap the last page
  ret = munmap(alloc + PAGESZ*8, PAGESZ);
  assert (ret == 0);

  // Unmap the second remaining page
  ret = munmap(alloc + PAGESZ, PAGESZ);
  assert(ret == 0);

  // Map back the second page
  void *alloc2 = mmap(alloc + PAGESZ, PAGESZ, PROT_EXEC | PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
  assert(alloc2 != MAP_FAILED);
  
  // Unmap the whole region
  ret = munmap(alloc, PAGESZ*8);
  assert(ret == 0);

  return 0;
}

```

`test/mprotect_exec.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdlib.h>
#include <stdio.h>
#include <assert.h>
#include <sys/mman.h>
#include <errno.h>

#define PAGESZ   4096
#define BASEADDR 0x80000

int main() {
  int ret;

  printf("main()\n");

  // One page allocation
  void *alloc = mmap((void *)BASEADDR, PAGESZ, PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED, -1, 0);
  assert(alloc != MAP_FAILED);

  // No actual change
  ret = mprotect(alloc, PAGESZ, PROT_READ);
  assert(ret == 0);

  // Make executable
  ret = mprotect(alloc, PAGESZ, PROT_READ | PROT_EXEC);
  assert(ret == 0);

  // Not executable
  ret = mprotect(alloc, PAGESZ, PROT_NONE);
  assert(ret == 0);

  void *large_alloc = mmap((void *)BASEADDR + PAGESZ, PAGESZ*10, PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED, -1, 0);
  assert(large_alloc != MAP_FAILED);

  // Make the first page executable
  ret = mprotect(alloc, PAGESZ, PROT_READ | PROT_EXEC);
  assert(ret == 0);

  // Make the third page executable
  ret = mprotect(alloc + PAGESZ*2, PAGESZ, PROT_READ | PROT_EXEC);
  assert(ret == 0);

  // Make the fifth page executable
  ret = mprotect(alloc + PAGESZ*4, PAGESZ, PROT_READ | PROT_EXEC);
  assert(ret == 0);

  // Make the first six pages executable
  ret = mprotect(alloc, PAGESZ*6, PROT_READ | PROT_EXEC);
  assert(ret == 0);

  // Execute-only - should fail
  ret = mprotect(alloc, PAGESZ, PROT_EXEC);
  assert(ret == 0);
}

```

`test/self_modifying.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>
#include <string.h>
#include <assert.h>
#include <pthread.h>
#include <sys/mman.h>
#include <asm/unistd.h>

#ifdef __arm__
#include "../pie/pie-arm-encoder.h"
#elif __aarch64__
#include "../pie/pie-a64-encoder.h"
#endif
#include "../scanner_public.h"

#define PAGESZ   4096

#ifdef __arm__
  #define push(reg) arm_push_reg(reg);
  #define pop(reg) arm_pop_reg(reg);
  #define reg_svc_no r7
  #define mov(wptr, rd, rn) arm_mov(&wptr, 0, 0, rd, rn);
  #define movi(wptr, rd, imm) arm_mov(&wptr, 1, 0, rd, imm);
  #define svc(wptr) arm_svc(&wptr, 0);
  #define return_inst(wptr) arm_bx(&wptr, lr);
#elif __aarch64__
  #define push(reg)
  #define pop(reg)
  #define r0 x0
  #define r1 x1
  #define r2 x2
  #define reg_svc_no x8
  #define mov(wptr, rd, rn) a64_logical_reg(&write_p, 1, 1, 0, 0, rn, 0, xzr, rd);
  #define movi(wptr, rd, imm) a64_MOV_wide(&wptr, 1, 2, 0, imm, rd);
  #define svc(wptr) a64_SVC (&wptr, 0);
  #define return_inst(wptr) a64_RET(&wptr, lr);
#else
  #error Unknown architecture
#endif

typedef void (*jit_f)(char *, size_t);
jit_f our_f;
int ready = 0;

void generate_print(uint32_t *write_p) {
  push(reg_svc_no);
  mov(write_p, r2, r1);
  write_p++;
  mov(write_p, r1, r0);
  write_p++;
  movi(write_p, r0, 1);
  write_p++;
  movi(write_p, reg_svc_no, __NR_write);
  write_p++;
  svc(write_p);
  write_p++;
  pop(reg_svc_no);
  return_inst(write_p);
}

void generate_empty(uint32_t *write_p) {
  return_inst(write_p);
}

void dispatcher(char *string) {
  our_f(string, strlen(string));
}

void *compiler_thread(void *alloc) {
  generate_print(alloc);
  __clear_cache(alloc, alloc + PAGESZ);
  ready = 1;
}

int main() {
  void *alloc = mmap(NULL, PAGESZ, PROT_READ|PROT_WRITE|PROT_EXEC,
                     MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
  assert(alloc != MAP_FAILED);
  our_f = (jit_f)alloc;

  generate_print(alloc);
  __clear_cache(alloc, alloc + PAGESZ);
  dispatcher("This should be printed\n");

  generate_empty(alloc);
  __clear_cache(alloc, alloc + PAGESZ);
  dispatcher("This shouldn't be printed\n");

  pthread_t thread;
  pthread_create(&thread, NULL, compiler_thread, alloc);

  while(!ready);
  asm volatile("isb");
  dispatcher("This should also be printed\n");
}

```

`test/signals.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifdef __arm__
.syntax unified
#endif

.global test_cbz
.func

#ifdef __aarch64__
test_cbz:
  MOV W1, #0
cbz_loop:
  SUB W0, W0, #1
  AND W2, W0, #1
  CBZ W2, cbz_skip
  AND W2, W0, #2
  CBNZ W2, cbz_skip
  ADD W1, W1, #1
cbz_skip:
  CBNZ W0, cbz_loop

  MOV W0, W1
  RET
#endif

#ifdef __arm__
.type test_cbz, %function
.thumb_func
test_cbz:
  MOV R1, #0
cbz_loop:
  SUB R0, R0, #1
  AND R2, R0, #1
  CBZ R2, cbz_skip
  AND R2, R0, #2
  CBNZ R2, cbz_skip
  ADD R1, R1, #1
cbz_skip:
  CMP R0, #0
  BNE cbz_loop

  MOV R0, R1
  BX LR
#endif

.endfunc

#ifdef __arm__
.global test_a32_direct
.func
.type test_a32_direct, %function
.code 32
test_a32_direct:
  MOV R1, #0
a32d_loop:
  SUB R0, R0, #1
  ANDS R2, R0, #1
  BEQ a32d_skip
  ANDS R2, R0, #2
  BNE a32d_skip
  ADD R1, R1, #1
a32d_skip:
  CMP R0, #0
  BNE a32d_loop
  MOV R0, R1
  BX LR
.endfunc

.global test_a32_indirect
.func
.type test_a32_indirect, %function
.code 32
test_a32_indirect:
  MOV R1, #0
a32i_loop:
  SUB R0, R0, #1
  ANDS R2, R0, #1
  ADREQ R2, a32i_skip
  ADRNE R2, a32i_cont
  BX R2
a32i_cont:
  ADD R1, R1, #1
a32i_skip:
  CMP R0, #0
  ADREQ R2, a32i_ret
  ADRNE R2, a32i_loop
  BX R2
a32i_ret:
  MOV R0, R1
  BX LR
.endfunc
#endif


#ifdef __aarch64__

.global test_tbz
.func
test_tbz:
  MOV W1, #0
tbz_loop:
  SUB W0, W0, #1
  TBZ W0, #0, tbz_skip
  TBNZ W0, #1, tbz_skip
  ADD W1, W1, #1
tbz_skip: 
  CBNZ W0, tbz_loop

  MOV W0, W1
  RET
.endfunc

#endif

```

`test/signals.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdio.h>
#include <signal.h>
#include <assert.h>
#include <unistd.h>
#include <asm/unistd.h>
#include <stdint.h>
#include <sys/mman.h>
#include <pthread.h>
#include <sys/time.h>
#include <setjmp.h>
#include <errno.h>
#ifdef __arm__
#include "../pie/pie-arm-encoder.h"
#elif __aarch64__
#include "../pie/pie-a64-encoder.h"
#endif
#include "../scanner_public.h"

FILE *nulldev;
int count = 0;
sigjmp_buf main_env;
volatile int sig_received = 0;

#define SIGNAL_CNT (100*1000)
#define AS_TEST_ITER (100*1000*1000)

int test_cbz(int count);
int test_tbz(int count);
int test_a32_direct(int count);
int test_a32_indirect(int count);

void sigusr_handler(int i, siginfo_t *info, void *ptr) {
  sig_received++;
  printf("success\n");
}

void alarm_handler(int i, siginfo_t *info, void *ptr) {
  // Here we need a relatively slow function which uses a high number of registers.
  // We call fprintf for convenience.
  fprintf(nulldev, "alarm\n");
  count++;
}

void handle_sync(int i, siginfo_t *info, void *ptr) {
  sig_received++;
  siglongjmp(main_env, 1);
}

#ifdef __arm__
  #define add_inst(wptr) arm_add(&wptr, 1, 0, r0, r0, 1);
  #define return_inst(wptr) arm_bx(&wptr, lr);
#elif __aarch64__
  #define add_inst(wptr) a64_ADD_SUB_immed(&wptr, 1, 0, 0, 0, 1, x0, x0);
  #define return_inst(wptr) a64_RET(&wptr, lr);
#else
  #error Unknown architecture
#endif

// Fill the CC
#define JUNK_CODE_SIZE (8*1024*1024)
void fill_cc() {
  int i;
  uint32_t *code = mmap(NULL, JUNK_CODE_SIZE, PROT_EXEC | PROT_READ | PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
  assert(code != MAP_FAILED);
  uint32_t *iptr = code;
  for (i = 0; i < ((JUNK_CODE_SIZE / 4) - 1); i++) {
    add_inst(iptr);
    iptr++;
  }
  return_inst(iptr);
  iptr++;
  __clear_cache(code, iptr);

  ((void (*)())code)();

  munmap(code, JUNK_CODE_SIZE);
}

void *signal_parent(void *data) {
  int tid = *(int *)data;
  int pid = syscall(__NR_getpid);
  int ret;

  for (int i = 0; i < SIGNAL_CNT; i++) {
    do {
      ret = syscall(__NR_tgkill, pid, tid, SIGRTMIN);
      usleep((ret == 0) ? 10 : 1000);
    } while (ret != 0 && errno == EAGAIN);

    assert(ret == 0);
  }
}

int main (int argc, char **argv) {
  int ret;

  struct sigaction act;
  act.sa_sigaction = sigusr_handler;
  sigemptyset(&act.sa_mask);
  act.sa_flags = SA_SIGINFO;
  ret = sigaction(SIGUSR1, &act, NULL);
  assert(ret == 0);

  printf("Simple signal handler: ");
  fflush(stdout);
  ret = kill(getpid(), SIGUSR1);
  assert(ret == 0);

  printf("Signal after flushing the code cache: ");
  fflush(stdout);
  fill_cc();
  ret = kill(getpid(), SIGUSR1);
  assert(ret == 0);

  printf("Test sigsuspend: ");
  fflush(stdout);
  sig_received = 0;
  sigset_t blocked_sigs;
  ret = sigfillset(&blocked_sigs);
  assert(ret == 0);
  ret = sigprocmask(SIG_SETMASK, &blocked_sigs, NULL);
  assert(ret == 0);
  ret = sigemptyset(&blocked_sigs);
  assert(ret == 0);
  ret = kill(getpid(), SIGUSR1);
  assert(ret == 0);
  while(sig_received == 0) {
    sigsuspend(&blocked_sigs);
    assert(errno == EINTR);
  }
  ret = sigprocmask(SIG_SETMASK, &blocked_sigs, NULL);
  assert(ret == 0);
  sig_received = 0;

  printf("Test against race conditions between code generation and signals: ");
  fflush(stdout);
  nulldev = fopen("/dev/null", "r+");
  assert(nulldev != NULL);

  act.sa_sigaction = alarm_handler;
  ret = sigaction(SIGALRM, &act, NULL);
  assert(ret == 0);

  struct itimerval it;
  it.it_interval.tv_sec = 0;
  it.it_interval.tv_usec = 100;
  it.it_value.tv_sec = 0;
  it.it_value.tv_usec = 100;

  ret = setitimer(ITIMER_REAL, &it, NULL);
  assert(ret == 0);

  for (int i = 0; i < 40; i++) {
    fill_cc();
  }

  it.it_value.tv_usec = 0;
  ret = setitimer(ITIMER_REAL, &it, NULL);
  assert(ret == 0);
  printf("success\n");

  printf("Test for missed signals: ");
  fflush(stdout);
  count = 0;
  int tid = syscall(__NR_gettid);

  ret = sigaction(SIGRTMIN, &act, NULL);
  assert(ret == 0);

  pthread_t thread;
  pthread_create(&thread, NULL, signal_parent, &tid);
  pthread_join(thread, NULL);
  assert(count == SIGNAL_CNT);
  printf("success\n");

  printf("Test signal handling in fragments containing CB(N)Z: ");
  fflush(stdout);
  pthread_create(&thread, NULL, signal_parent, &tid);
  int64_t count = test_cbz(AS_TEST_ITER);
  pthread_join(thread, NULL);
  assert(count == AS_TEST_ITER/4);
  printf("success\n");

#ifdef __aarch64__
  printf("Test signal handling in fragments containing TB(N)Z: ");
  fflush(stdout);
  pthread_create(&thread, NULL, signal_parent, &tid);
  count = test_tbz(AS_TEST_ITER);
  pthread_join(thread, NULL);
  assert(count == AS_TEST_ITER/4);
  printf("success\n");
#endif

#ifdef __arm__
  printf("Test signal handling in fragments containing A32 conditional branches: ");
  fflush(stdout);
  pthread_create(&thread, NULL, signal_parent, &tid);
  count = test_a32_direct(AS_TEST_ITER);
  pthread_join(thread, NULL);
  assert(count == AS_TEST_ITER/4);
  printf("success\n");

  printf("Test signal handling in fragments containing A32 indirect branches: ");
  fflush(stdout);
  pthread_create(&thread, NULL, signal_parent, &tid);
  count = test_a32_indirect(AS_TEST_ITER);
  pthread_join(thread, NULL);
  assert(count == AS_TEST_ITER/2);
  printf("success\n");
#endif

  printf("Test handling of a synchronous SIGTRAP signal: ");
  fflush(stdout);
  act.sa_sigaction = handle_sync;
  ret = sigaction(SIGTRAP, &act, NULL);
  assert(ret == 0);

  ret = sigsetjmp(main_env, 1);
  if (ret == 0) {
#ifdef __arm__
    asm volatile ("bkpt 0");
#elif __aarch64__
    asm volatile ("brk 0");
#else
    #error Unsupported architecture
#endif
  }
  assert(sig_received == 1);
  printf("success\n");

  printf("Test handling of a synchronous SIGILL signal: ");
  fflush(stdout);
  ret = sigaction(SIGILL, &act, NULL);
  assert(ret == 0);

  ret = sigsetjmp(main_env, 1);
  if (ret == 0) {
#ifdef __arm__
    asm volatile ("udf");
#elif __aarch64__
    asm volatile ("hvc 0");
#else
    #error Unsupported architecture
#endif
  }
  assert(sig_received == 2);
  printf("success\n");

  printf("Test receiving SIGILL when no handler is installed\n");
  act.sa_handler = SIG_DFL;
  ret = sigaction(SIGILL, &act, NULL);
#ifdef __arm__
    asm volatile ("udf");
#elif __aarch64__
    asm volatile ("hvc 0");
#else
    #error Unsupported architecture
#endif
}

```

`traces.c`:

```c
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 Guillermo Callaghan <guillermocallaghan at hotmail dot com>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <stdlib.h>
#include <stdio.h>
#include <stdint.h>
#include <limits.h>
#include <assert.h>

#include "dbm.h"
#include "common.h"
#include "scanner_common.h"

#ifdef __arm__
#include "pie/pie-thumb-decoder.h"
#include "pie/pie-thumb-encoder.h"
#include "pie/pie-arm-encoder.h"
#elif __aarch64__
#include "pie/pie-a64-encoder.h"
#include "pie/pie-a64-decoder.h"
#include "pie/pie-a64-field-decoder.h"
#define NINETEEN_BITS        0x7FFFF
#define FOURTEEN_BITS        0x3FFF
#define NOP_INSTRUCTION      0xD503201F
#define THIRTY_TWO_KB        32 * 1024
#define ONE_MEGABYTE         1024 * 1024
#endif

#ifdef DEBUG
  #define debug(...) fprintf(stderr, __VA_ARGS__)
  #ifndef VERBOSE
    #define VERBOSE
  #endif
#else
  #define debug(...)
#endif

#ifdef DBM_TRACES
uintptr_t get_active_trace_spc(dbm_thread *thread_data) {
  int bb_id = thread_data->active_trace.source_bb;
  return (uintptr_t)thread_data->code_cache_meta[bb_id].source_addr;
}

uintptr_t active_trace_lookup(dbm_thread *thread_data, uintptr_t target) {
  uintptr_t spc = get_active_trace_spc(thread_data);
  if (target == spc) {
    return adjust_cc_entry(thread_data->active_trace.entry_addr);
  }

  uintptr_t tpc = hash_lookup(&thread_data->entry_address, target);

  return is_trace(thread_data, tpc) ? adjust_cc_entry(tpc) : UINT_MAX;
}

uintptr_t active_trace_lookup_or_scan(dbm_thread *thread_data, uintptr_t target) {
  uintptr_t spc = get_active_trace_spc(thread_data);
  if (target == spc) {
    return adjust_cc_entry(thread_data->active_trace.entry_addr);
  }
  return lookup_or_scan(thread_data, target);
}

uintptr_t active_trace_lookup_or_stub(dbm_thread *thread_data, uintptr_t target) {
  uintptr_t spc = get_active_trace_spc(thread_data);
  if (target == spc) {
    return adjust_cc_entry(thread_data->active_trace.entry_addr);
  }
  return lookup_or_stub(thread_data, target);
}

int allocate_trace_fragment(dbm_thread *thread_data) {
  int id = thread_data->active_trace.id++;
  assert(id < (CODE_CACHE_SIZE + TRACE_FRAGMENT_NO));
  return id;
}

uint32_t scan_trace(dbm_thread *thread_data, void *address, cc_type type, int *set_trace_id) {
  size_t fragment_len;
  uint8_t *write_p = thread_data->active_trace.write_p;
  int trace_id = allocate_trace_fragment(thread_data);
  if (set_trace_id != NULL) {
    *set_trace_id = trace_id;
  }

  debug("Trace scan: %p to %p, id %d\n", address, write_p, trace_id);

  thread_data->code_cache_meta[trace_id].source_addr = address;
  thread_data->code_cache_meta[trace_id].tpc = (uintptr_t)write_p;
  thread_data->code_cache_meta[trace_id].branch_cache_status = 0;

#ifdef __arm__
  unsigned long thumb = (unsigned long)address & THUMB;
  if (thumb) {
    fragment_len = scan_t32(thread_data, (uint16_t *)(((uint32_t)address)-1), trace_id, type, (uint16_t*)write_p);
  } else {
    fragment_len = scan_a32(thread_data, (uint32_t *)address, trace_id, type, (uint32_t*)write_p);
  }
#endif // __arm__
#ifdef __aarch64__
  fragment_len = scan_a64(thread_data, (uint32_t *)address, trace_id, type, (uint32_t*)write_p);
#endif

  __clear_cache(write_p, write_p + fragment_len);

  thread_data->trace_fragment_count++;

  return fragment_len;
}

#ifdef __aarch64__
bool is_instruction_position_independent(uint32_t * address) {
  int instruction = a64_decode(address);
  switch(instruction) {
    case A64_B_BL:
    case A64_CBZ_CBNZ:
    case A64_B_COND:
    case A64_TBZ_TBNZ:
    case A64_BR:
    case A64_BLR:
    case A64_RET:
    case A64_LDR_LIT:
    case A64_ADR: // Includes ADRP
      return false;
    default:
      return true;
  }
}

void get_cond_branch_attributes(uintptr_t inst_addr, uint32_t *mask, int64_t *max) {
  int instruction = a64_decode((uint32_t *)inst_addr);
  switch(instruction) {
    case A64_CBZ_CBNZ:
    case A64_B_COND:
      *mask = NINETEEN_BITS;
      *max = ONE_MEGABYTE;
      break;
    case A64_TBZ_TBNZ:
      *mask = FOURTEEN_BITS;
      *max = THIRTY_TWO_KB;
      break;
    case A64_B_BL:
      printf("Direct branch (B or BL). (Not allowed)\n");
      while(1);
    default:
      fprintf(stderr, "Linking instruction unknown at %p instruction %d\n",
      (void *)inst_addr, instruction);
      while(1);
  }
}

void patch_trace_branches(dbm_thread *thread_data, uint32_t *orig_branch, uintptr_t tpc) {
  uint32_t *exit_address;
  uint32_t sf, op, b5, b40, imm, rt, bit, cond;

  int64_t new_offset = (int64_t)tpc - (int64_t)orig_branch;

  int instruction = a64_decode(orig_branch);
  switch(instruction) {
    case A64_CBZ_CBNZ:
      a64_CBZ_CBNZ_decode_fields(orig_branch, &sf, &op, &imm, &rt);
      // Check if new_offset fits
      if (is_offset_within_range(new_offset, ONE_MEGABYTE)) {
        a64_cbz_cbnz_helper(orig_branch, op, (uint64_t)tpc, sf, rt);
        return;
      } else {
        exit_address = (uint32_t *)(sign_extend64(21, (imm << 2)) + (uint64_t) orig_branch);
      }
      break;
    case A64_B_COND:
      a64_B_cond_decode_fields(orig_branch, &imm, &cond);
      // Check if new_offset fits
      if (is_offset_within_range(new_offset, ONE_MEGABYTE)) {
        a64_b_cond_helper(orig_branch, (uint64_t)tpc, cond);
        return;
      } else {
        exit_address = (uint32_t *)(sign_extend64(21, (imm << 2)) + (uint64_t) orig_branch);
      }
      break;
    case A64_TBZ_TBNZ:
      a64_TBZ_TBNZ_decode_fields(orig_branch, &b5, &op, &b40, &imm, &rt);
      // Check if new_offset fits
      if (is_offset_within_range(new_offset, THIRTY_TWO_KB)) {
        bit = (b5 << 5) | b40;
        a64_tbz_tbnz_helper(orig_branch, op, (uint64_t)tpc, rt, bit);
        return;
      } else {
        exit_address = (uint32_t *)(sign_extend64(16, imm << 2) + (uint64_t) orig_branch);
      }
      break;
    case A64_B_BL:
      // Patch the branch
      a64_b_helper((uint32_t *)orig_branch, tpc);
      return;
      break;
    default:
        fprintf(stderr, "[install_trace] Linking instruction unknown at %p instruction %d\n",
                        orig_branch, instruction);
        while(1);
  }
  /*
   * +----------------+ Exit
   * | NOP            | These NOPs are here to avoid the core fetching more
   * | NOP            | than one branch in the same cycle
   * | branch to trace| Also, to maintain the branch address location in all
   * | NOP            | the exits stubs
   * +----------------+
   */
  *exit_address = NOP_INSTRUCTION;
  exit_address++;
  *exit_address = NOP_INSTRUCTION;
  exit_address++;

  // Update metadata of the exit
  int const fragment_id = addr_to_fragment_id(thread_data, (uintptr_t)exit_address);
  thread_data->code_cache_meta[fragment_id].branch_taken_addr = tpc;

  a64_b_helper((uint32_t *)exit_address, tpc);
  __clear_cache((void *)(exit_address - 3), (void *)(exit_address + 1));
}
#endif

void install_trace(dbm_thread *thread_data) {
  ll_entry *cc_link;
  uintptr_t orig_branch;
  int bb_source = thread_data->active_trace.source_bb;
  uintptr_t spc = (uintptr_t)thread_data->code_cache_meta[bb_source].source_addr;
  uintptr_t tpc = thread_data->active_trace.entry_addr;
#ifdef __arm__
  uintptr_t tpc_direct = adjust_cc_entry(tpc);
#endif

  assert(thread_data->active_trace.active);
  thread_data->active_trace.active = false;

  cc_link = thread_data->code_cache_meta[bb_source].linked_from;
  while(cc_link != NULL) {
    debug("Link from: 0x%" PRIxPTR ", update to: 0x%" PRIxPTR "\n", cc_link->data, tpc);
    orig_branch = cc_link->data;
#ifdef __arm__
    orig_branch &= 0xFFFFFFFE;
    if (cc_link->data & THUMB) {
      thumb_adjust_b_bl_target(thread_data, (uint16_t *)orig_branch, tpc_direct);
    } else if ((cc_link->data & 3) == FULLADDR) {
      *(uint32_t *)(orig_branch & (~FULLADDR)) = tpc_direct;
    } else {
      arm_adjust_b_bl_target((uintptr_t *)orig_branch, tpc_direct);
    }
#elif __aarch64__
    if (is_trace(thread_data, orig_branch)) {
      patch_trace_branches(thread_data, (uint32_t *)orig_branch, tpc + 4);
    } else {
      a64_b_helper((uint32_t *)orig_branch, tpc + 4);
    }
#endif
    cc_link = cc_link->next;
    __clear_cache((void *)orig_branch, (void *)orig_branch + 4);
  }

  hash_add(&thread_data->entry_address, spc, tpc);

#ifdef __arm__
  thread_data->trace_id = thread_data->active_trace.id;
  thread_data->trace_cache_next = thread_data->active_trace.write_p;

  // Record the trace exits
  for (int i = 0; i < thread_data->active_trace.free_exit_rec; i++) {
    record_cc_link(thread_data, thread_data->active_trace.exits[i].from,
                   thread_data->active_trace.exits[i].to);
  }

  /* Add traps to the source basic block to detect if it remains reachable */
  void *write_p = (void *)adjust_cc_entry(thread_data->code_cache_meta[bb_source].tpc);
  if (spc & THUMB) {
    thumb_bkpt16((uint16_t **)&write_p, 0);
  } else {
    arm_bkpt((uint32_t **)&write_p, 0, 0);
  }
  __clear_cache(write_p, write_p + 4);
#elif __aarch64__
  /*
   *          Trace
   *    +----------------+
   *    | inst           |
   *    | inst           |
   *    | cond branch 1  |
   *    | inst           |
   *    | inst           |
   *    | cond branch 2  |
   *    | inst           |
   *    | cond branch 3  |
   *    | branch         |
   *    |                | / Aligned to 16 bytes
   *    +----------------+ Exit 1
   *  1:| BB first inst  | <- exit_stub_addr
   *    | BB second inst | <- exit_stub_addr + 4
   *    | branch         | <- exit_stub_addr + 8
   *    | NOP            |
   *    +----------------+ Exit 2 (Second instruction not position-independent)
   *  2:| BB first inst  | <- exit_stub_addr
   *    | NOP            | <- exit_stub_addr + 4
   *    | branch         | <- exit_stub_addr + 8
   *    | NOP            |
   *    +----------------+ Exit 3 (Both instructions are not position-independent)
   *  3:| NOP            | <- exit_stub_addr
   *    | NOP            | <- exit_stub_addr + 4
   *    | branch         | <- exit_stub_addr + 8
   *    | NOP            |
   *    +----------------+
   */

  uint32_t *exit_stub_addr = thread_data->active_trace.write_p;
  for (int i = 0; i < thread_data->active_trace.free_exit_rec; i++) {
    uint32_t *from  = (uint32_t *)thread_data->active_trace.exits[i].from;
    uintptr_t const to = thread_data->active_trace.exits[i].to;
    // Align to (next) 16 bytes
    exit_stub_addr = (uint32_t *)((((uintptr_t)exit_stub_addr) + 0xF) & ~0xF);
    uint32_t *exit_start = exit_stub_addr;

    int64_t max;
    uint32_t mask;
    get_cond_branch_attributes(thread_data->active_trace.exits[i].from, &mask, &max);

    bool const is_basic_block = (to < (uintptr_t)thread_data->code_cache->traces);
    if (is_basic_block) {
      record_cc_link(thread_data, (uintptr_t)from, to);
    }

    int64_t offset = (to - (uintptr_t)from);
    if (is_basic_block || !is_offset_within_range(offset, max)) {
      // Give the exit a number and set metadata
      int const exit_id = allocate_trace_fragment(thread_data);
      thread_data->code_cache_meta[exit_id].tpc = (uintptr_t)exit_start;
      thread_data->code_cache_meta[exit_id].exit_branch_type = trace_exit;
      thread_data->code_cache_meta[exit_id].branch_cache_status = BRANCH_LINKED;

      // Record the exit id used in the trace fragment
      int const fragment_id = thread_data->active_trace.exits[i].fragment_id;
      thread_data->code_cache_meta[fragment_id].free_b = exit_id;

      uintptr_t target_offset = 0;
      for (size_t j = 0; j < 2; j++) {
        if (is_instruction_position_independent((uint32_t *)(to + j * 4))) {
          *exit_stub_addr = *(uint32_t *) (to + j * 4);
          exit_stub_addr++;
          target_offset += 4;
        } else {
          for (size_t k = 2; k == j; k--) {
            *exit_stub_addr = NOP_INSTRUCTION;
            exit_stub_addr++;
          }
          break;
        }
      }

      uint64_t const target = (to + target_offset);
      thread_data->code_cache_meta[exit_id].exit_branch_addr = exit_stub_addr;
      thread_data->code_cache_meta[exit_id].branch_taken_addr = target; // Code Cache target

      a64_b_helper(exit_stub_addr, target);
      exit_stub_addr++;
      *exit_stub_addr = NOP_INSTRUCTION;
      exit_stub_addr++;

      __clear_cache((void *)(exit_start), (void *)(exit_stub_addr + 1));
      offset = ((uint64_t)exit_start - (uint64_t)from);
    }

    assert(is_offset_within_range(offset, max));
    *from |= (((offset >> 2) & mask) << 5);
    __clear_cache((void *)from, (void *)(from + 1));
  }
  thread_data->trace_id = thread_data->active_trace.id;
  thread_data->active_trace.write_p = exit_stub_addr;
  thread_data->trace_cache_next = (uint8_t  *)exit_stub_addr;
  uint32_t *write_p = (uint32_t*)(thread_data->code_cache_meta[bb_source].tpc + 4);
  a64_BRK(&write_p, 0); // BRK trap
  __clear_cache(write_p, write_p + 1);
#endif
}

#ifdef __arm__
int trace_record_exit(dbm_thread *thread_data, uintptr_t from, uintptr_t to) {
#endif // __arm__
#ifdef __aarch64__
int trace_record_exit(dbm_thread *thread_data, uintptr_t from, uintptr_t to, int fragment_id) {
#endif // __arch64__
  int record = thread_data->active_trace.free_exit_rec++;
  if (record >= MAX_TRACE_REC_EXITS) {
    return -1;
  }

  thread_data->active_trace.exits[record].from = from;
  thread_data->active_trace.exits[record].to = to;
#ifdef __aarch64__
  thread_data->active_trace.exits[record].fragment_id = fragment_id;
#endif

  return 0;
}

#ifdef __arm__
void thumb_trace_exit_branch(dbm_thread *thread_data, uint16_t *write_p, uint32_t target) {
  thumb_b32_helper(write_p, target);
  int ret = trace_record_exit(thread_data, (uintptr_t)write_p|THUMB, target);
  assert(ret == 0);
}

void arm_trace_exit_branch(dbm_thread *thread_data, uint32_t *write_p, uint32_t target, uint32_t cond) {
  arm_b32_helper(write_p, target, cond);
  int ret = trace_record_exit(thread_data, (uintptr_t)write_p, target);
  assert(ret == 0);
}
#endif

#ifdef __aarch64__
void set_up_trace_exit(dbm_thread *thread_data, uint32_t **o_write_p, int fragment_id, bool is_taken) {
  dbm_code_cache_meta *bb_meta = &thread_data->code_cache_meta[fragment_id];
  uint32_t *write_p = *o_write_p;
  uint32_t condition = bb_meta->branch_condition;
  uint32_t new_condition = is_taken ? (condition ^ 1) : condition;

  switch (bb_meta->exit_branch_type) {
    case cbz_a64:
      a64_CBZ_CBNZ(&write_p, bb_meta->rn >> 5, new_condition, 0, bb_meta->rn);
      break;
    case cond_imm_a64:
      a64_B_cond(&write_p, 0, new_condition);
      break;
    case tbz_a64:
      a64_TBZ_TBNZ(&write_p, bb_meta->rn >> 10, new_condition, bb_meta->rn >> 5, 0, bb_meta->rn);
      break;
    default:
      fprintf(stderr, "Unknown branch type\n");
      while(1);
  }

  uintptr_t addr = is_taken ? bb_meta->branch_skipped_addr : bb_meta->branch_taken_addr;
  uintptr_t tpc = active_trace_lookup_or_scan(thread_data, addr) + 4;
  int ret = trace_record_exit(thread_data, (uintptr_t)write_p, tpc, fragment_id);
  assert(ret == 0);
  __clear_cache(write_p, (write_p + 4));
  write_p++;

  *o_write_p = write_p;
}
#endif
#endif

/* This is called from trace_head_incr, which is called by trace heads */
int hot_bb_cnt = 0;
void create_trace(dbm_thread *thread_data, uint32_t bb_source, cc_addr_pair *ret_addr) {
#ifdef DBM_TRACES
  uint16_t *source_addr;
  uint32_t fragment_len;
  int trace_id;
  uintptr_t trace_entry;

  thread_data->trace_fragment_count = 0;
#ifdef __arm__
  if (thread_data->code_cache_meta[bb_source].exit_branch_type == cbz_thumb ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == cond_imm_thumb ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == uncond_imm_thumb ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == uncond_b_to_bl_thumb ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == tb_indirect ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == uncond_reg_thumb ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == cond_imm_arm ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == uncond_imm_arm ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == uncond_reg_arm ||
      thread_data->code_cache_meta[bb_source].exit_branch_type == uncond_blxi_thumb) {
#endif
#ifdef __aarch64__
  if (thread_data->code_cache_meta[bb_source].exit_branch_type == cbz_a64
      || thread_data->code_cache_meta[bb_source].exit_branch_type == tbz_a64
      || thread_data->code_cache_meta[bb_source].exit_branch_type == cond_imm_a64
      || thread_data->code_cache_meta[bb_source].exit_branch_type == uncond_imm_a64) {
#endif
    source_addr = thread_data->code_cache_meta[bb_source].source_addr;
    ret_addr->spc = (uintptr_t)source_addr;

    /* Alignment doesn't seem to make much of a difference */
    thread_data->trace_cache_next += (TRACE_ALIGN -
                                     ((uintptr_t)thread_data->trace_cache_next & TRACE_ALIGN_MASK))
                                     & TRACE_ALIGN_MASK;
    if ((uintptr_t)thread_data->trace_cache_next >= (uintptr_t)thread_data->code_cache + MAX_BRANCH_RANGE - TRACE_LIMIT_OFFSET
        || thread_data->trace_id >= (CODE_CACHE_SIZE + TRACE_FRAGMENT_NO - TRACE_FRAGMENT_OVERP)) {
      fprintf(stderr, "trace cache full, flushing the CC\n");
      flush_code_cache(thread_data);
      ret_addr->tpc = lookup_or_scan(thread_data, (uintptr_t)source_addr);
      return;
    }

    debug("bb: %d, source: %p, ret to: 0x%" PRIxPTR "\n", bb_source, source_addr, ret_addr->tpc);
    hot_bb_cnt++;

    trace_entry = (uintptr_t)thread_data->trace_cache_next;
    trace_entry |= ((uintptr_t)source_addr) & THUMB;

    thread_data->active_trace.active = true;
    thread_data->active_trace.id = thread_data->trace_id;
    thread_data->active_trace.source_bb = bb_source;
    thread_data->active_trace.write_p = thread_data->trace_cache_next;
    thread_data->active_trace.entry_addr = trace_entry;
    thread_data->active_trace.free_exit_rec = 0;

    debug("Create trace: %d (%p), source_bb: %d, entry: %" PRIxPTR "\n",
          thread_data->active_trace.id, thread_data->active_trace.write_p,
          thread_data->active_trace.source_bb, thread_data->active_trace.entry_addr);
    debug("\n    Trace head: %p at 0x%" PRIxPTR "\n\n", source_addr, ret_addr->tpc);

    ret_addr->tpc = adjust_cc_entry(trace_entry);
    fragment_len = scan_trace(thread_data, source_addr, mambo_trace_entry, &trace_id);
    debug("len: %d\n\n", fragment_len);

    // this could be used to detect bugs if first fragment is unlinkable
    switch(thread_data->code_cache_meta[trace_id].exit_branch_type) {
#ifdef __arm__
      case uncond_reg_thumb:
      case cond_reg_thumb:
      case trace_inline_max:
      case tbb:
      case tbh:
      case uncond_reg_arm:
        thread_data->active_trace.write_p += fragment_len;
        install_trace(thread_data);
        break;
#endif
#ifdef __aarch64__
      // allowed exit types
      case cbz_a64:
      case cond_imm_a64:
      case tbz_a64:
      case uncond_imm_a64:
        break;
      default:
        fprintf(stderr, "Disallowed type of exit in the first trace fragment: %d\n",
                thread_data->code_cache_meta[trace_id].exit_branch_type);
        while(1);
#endif
    }
  } else {
    fprintf(stderr, "\nUnknown exit branch type in trace head: %d\n", thread_data->code_cache_meta[bb_source].exit_branch_type);
    while(1);
  }
}

void early_trace_exit(dbm_thread *thread_data, dbm_code_cache_meta* bb_meta,
                      void *write_p, uintptr_t spc, uintptr_t tpc) {
#ifdef __arm__
  if (spc & THUMB) {
    thumb_cc_branch(thread_data, (uint16_t *)write_p, tpc);
  } else {
    arm_cc_branch(thread_data, (uint32_t *)write_p, tpc, AL);
  }
#endif
#ifdef __aarch64__
  a64_cc_branch(thread_data, (uint32_t *)write_p, tpc + 4);
#endif
  __clear_cache(write_p, write_p+4);
  write_p += 4;
  thread_data->active_trace.write_p = (uint8_t *)write_p;
  install_trace(thread_data);

  bb_meta->branch_cache_status |= BOTH_LINKED;
}

/* Handles dispatcher calls from traces */
void trace_dispatcher(uintptr_t target, uintptr_t *next_addr, uint32_t source_index, dbm_thread *thread_data) {
  uintptr_t addr;
  dbm_code_cache_meta *bb_meta = &thread_data->code_cache_meta[source_index];
  bool is_taken = (bb_meta->branch_taken_addr == target);
#ifdef __arm__
  uint16_t *write_p = (uint16_t *)bb_meta->exit_branch_addr;
#endif
#ifdef __aarch64__
  uint32_t *write_p = (uint32_t *) bb_meta->exit_branch_addr;
#endif
  size_t fragment_len;
  thread_data->was_flushed = false;

  debug("Trace dispatcher (target: 0x%" PRIxPTR ")\n", target);

  switch(bb_meta->exit_branch_type) {
#ifdef __arm__
    case cbz_thumb:
      thumb_misc_cbz_16(&write_p, (bb_meta->branch_skipped_addr == target) ? 1: 0, 0, 1, bb_meta->rn);
      write_p++;

      addr = (bb_meta->branch_skipped_addr == target) ? bb_meta->branch_taken_addr : bb_meta->branch_skipped_addr;
      debug("other addr: %x %d\n", addr, bb_meta->branch_skipped_addr == target);
      thumb_trace_exit_branch(thread_data, write_p, active_trace_lookup_or_stub(thread_data, addr));
      write_p += 2;
      __clear_cache(write_p - 4, write_p);

      bb_meta->branch_cache_status = is_taken ? FALLTHROUGH_LINKED : BRANCH_LINKED;

      break;
    case cond_imm_thumb:
      thumb_it16(&write_p, (bb_meta->branch_taken_addr == target) ? arm_inverse_cond_code[bb_meta->branch_condition] : bb_meta->branch_condition, 0x8);
      write_p++;

      addr = (bb_meta->branch_taken_addr == target) ? bb_meta->branch_skipped_addr : bb_meta->branch_taken_addr;
      debug("other addr: %x %d\n", addr, bb_meta->branch_skipped_addr == target);
      thumb_trace_exit_branch(thread_data, write_p, active_trace_lookup_or_stub(thread_data, addr));
      write_p += 2;
      __clear_cache(write_p - 4, write_p);

      bb_meta->branch_cache_status = is_taken ? FALLTHROUGH_LINKED : BRANCH_LINKED;

      break;
    case uncond_imm_thumb:
    case uncond_b_to_bl_thumb:
    case uncond_imm_arm:
      bb_meta->branch_cache_status = BRANCH_LINKED;
      break;

    case uncond_blxi_thumb:
    #if 1
      if ((uint32_t)write_p & 2) {
        thumb_nop16(&write_p);
        write_p++;
      }
      thumb_bx16(&write_p, pc);
      __clear_cache(write_p-2, write_p+2);
      write_p += 2;
    #endif

    /* Alternative implementations might be faster on other microarchitectures */
    #if 0
      thumb_push16(&write_p, (1 << r7));
      write_p++;
      thumb_add_from_pc16(&write_p, r7, ((uint32_t)write_p & 2) ? 0 : 0);
      write_p++;
      thumb_bx16(&write_p, r7);
      write_p += ((uint32_t)write_p & 2) ? 1 : 2;
      arm_pop_reg(r7);
      write_p++;
      __clear_cache(write_p-8, write_p + 1);
    #endif

    #if 0
      thumb_ldrl32(&write_p, pc, ((uint32_t)write_p & 2) << 1, 1);
      __clear_cache(write_p, write_p + 3);
      write_p += ((uint32_t)write_p & 2) ? 3 : 2;
      *((uint32_t *)write_p) = (uint32_t)write_p + 4;
      write_p += 2;
    #endif
      break;

    case uncond_blxi_arm:
      arm_sub((uint32_t **)&write_p, IMM_PROC, 0, pc, pc, 3);
      write_p += 2;
      __clear_cache(write_p-2, write_p);
      break;

    /* This is a new target for an indirect branch from the trace cache, generate a trace head */
    case uncond_reg_thumb:
    case tbh:
    case tbb:
    case uncond_reg_arm:
      *next_addr = lookup_or_scan(thread_data, target);
      return;

      break;

    case cond_imm_arm:
      addr = (bb_meta->branch_taken_addr == target) ? bb_meta->branch_skipped_addr : bb_meta->branch_taken_addr;

      arm_trace_exit_branch(thread_data, (uint32_t *)write_p, active_trace_lookup_or_stub(thread_data, addr),
                            is_taken ? invert_cond(bb_meta->branch_condition) : bb_meta->branch_condition);
      write_p += 2;
      __clear_cache(write_p-4, write_p);

      bb_meta->branch_cache_status = is_taken ? FALLTHROUGH_LINKED : BRANCH_LINKED;

      break;
#endif
#ifdef __aarch64__
    // TODO change lookup_or_scan to lookup_or_stub
    case cbz_a64:
    case cond_imm_a64:
    case tbz_a64:
      set_up_trace_exit(thread_data, &write_p, source_index, is_taken);
      bb_meta->branch_cache_status = is_taken ? BRANCH_LINKED : FALLTHROUGH_LINKED;
      break;
    case uncond_imm_a64:
      bb_meta->branch_cache_status = BRANCH_LINKED;
      break;
    case uncond_branch_reg:
      *next_addr = lookup_or_scan(thread_data, target);
      return;
      break;
#endif
    default:
      fprintf(stderr, "Trace dispatcher unknown %p\n", write_p);
      while(1);
  }

  *next_addr = (uintptr_t)write_p + (target & THUMB);
  thread_data->active_trace.write_p = (uint8_t *)write_p;

  // If the CC was flushed to generate exits, then abort the active trace
  if (thread_data->was_flushed) {
    *next_addr = lookup_or_scan(thread_data, target);
    return;
  }

  // Check if the fragment count has reached the max limit
  if (thread_data->trace_fragment_count > MAX_TRACE_FRAGMENTS) {
    debug("Trace fragment count limit, branch to: 0x%" PRIxPTR ", written at: %p\n", target, write_p);
    addr = active_trace_lookup_or_scan(thread_data, target);
    early_trace_exit(thread_data, bb_meta, write_p, target, addr);
    *next_addr = addr;
    return;
  }

  // Check if the target is already a trace
  addr = active_trace_lookup(thread_data, target);
  debug("Hash lookup for 0x%" PRIxPTR ": 0x%" PRIxPTR "\n", target, addr);
  if (addr != UINT_MAX) {
    early_trace_exit(thread_data, bb_meta, write_p, target, addr);
    *next_addr = addr;
    return;
  }

  debug("\n   Trace fragment: 0x%" PRIxPTR "\n", target);
  int fragment_id;
#ifdef __arm__
  fragment_len = scan_trace(thread_data, (uint16_t *)target, mambo_trace, &fragment_id);
#endif
#ifdef __aarch64__
  fragment_len = scan_trace(thread_data, (uint32_t *)target, mambo_trace, &fragment_id);
#endif
  debug("len: %" PRIdPTR "\n\n", fragment_len);

  thread_data->active_trace.write_p += fragment_len;
  switch(thread_data->code_cache_meta[fragment_id].exit_branch_type) {
#ifdef __arm__
    case uncond_reg_thumb:
    case cond_reg_thumb:
    case uncond_reg_arm:
    case cond_reg_arm:
    case tbb:
    case tbh:
    case tb_indirect:
    case trace_inline_max:
#elif __aarch64__
    case uncond_branch_reg:
#endif
      install_trace(thread_data);
      break;
  }

#ifdef __aarch64__
  // Insert a trampoline which pops {X0, X1} and branches to the new fragment
  write_p = (uint32_t *)(thread_data->active_trace.write_p - 4);
  a64_pop_pair_reg(x0, x1);
  a64_b_helper(write_p, *next_addr);
  write_p++;
  __clear_cache(write_p - 2, write_p);

  *next_addr = (uintptr_t)(write_p - 2);
#endif
#endif // DBM_TRACES
}

```

`util.S`:

```S
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#include <asm/unistd.h>

# These helpers are executed from .text and are not copied to the code cache

#ifdef __arm__
.syntax unified
#endif

.global dbm_client_entry
.func dbm_client_entry
.type dbm_client_entry, %function

#ifdef __arm__
.code 32
dbm_client_entry:
  MOV SP, R1
  MOV LR, R0
  MOV R0, #0
  MOV R1, #0
  MOV R2, #0
  MOV R3, #0
  BLX LR
  BX LR
#endif // __arm__

#ifdef __aarch64__
dbm_client_entry:
  MOV SP, X1
  STP XZR, XZR, [SP, #-16]!
  BR X0
#endif
.endfunc

# R0 - new SP
.global th_enter
.func   th_enter
.type   th_enter, %function

#ifdef __arm__
.thumb_func
th_enter:
  MOV SP, R0
  STR R1, [SP, #56]
  POP {R0-R12, R14}
  POP {PC}
#endif

#ifdef __aarch64__
th_enter:
  MOV SP, X0
  LDP  X4,  X5, [SP,  #16]
  LDP  X6,  X7, [SP,  #32]
  LDP  X8,  X9, [SP,  #48]
  LDP X10, X11, [SP,  #64]
  LDP X12, X13, [SP,  #80]
  LDP X14, X15, [SP,  #96]
  LDP X16, X17, [SP, #112]
  LDP X18, X19, [SP, #128]
  LDP X20, X21, [SP, #144]
  LDP X22, X23, [SP, #160]
  LDP X24, X25, [SP, #176]
  LDP X26, X27, [SP, #192]
  LDR X28,      [SP, #208]
  LDP X29, X30, [SP, #224]
  LDP  X2,  X3, [SP], #240

  BR X1
#endif
.endfunc

.global new_thread_trampoline
.func
.type   new_thread_trampoline, %function
new_thread_trampoline:
#ifdef __arm__
  PUSH {R4-R12, LR}
  MOV R1, SP
#elif __aarch64__
  STP X19, X20, [SP, #-96]!
  STP X21, X22, [SP, #16]
  STP X23, X24, [SP, #32]
  STP X25, X26, [SP, #48]
  STP X27, X28, [SP, #64]
  STP X29, X30, [SP, #80]
  MOV X1, SP
#endif
  B dbm_start_thread_pth
.endfunc

.global return_with_sp
.func
.type   return_with_sp, %function
return_with_sp:
#ifdef __arm__
  MOV SP, R0
  POP {R4-R12, PC}
#elif __aarch64__
  MOV SP, X0
  LDP X21, X22, [SP, #16]
  LDP X23, X24, [SP, #32]
  LDP X25, X26, [SP, #48]
  LDP X27, X28, [SP, #64]
  LDP X29, X30, [SP, #80]
  LDP X19, X20, [SP], #96
  RET
#endif
.endfunc

.global raw_syscall
.func   raw_syscall
.type   raw_syscall, %function

raw_syscall:
#ifdef __arm__
  MOV R12, SP
  PUSH {R4 - R7}
  MOV R7, R0
  MOV R0, R1
  MOV R1, R2
  MOV R2, R3
  LDM R12, {R3 - R6}
  SVC 0
  POP {R4 - R7}
  BX LR
#endif
#ifdef __aarch64__
  MOV W8, W0
  MOV X0, X1
  MOV X1, X2
  MOV X2, X3
  MOV X3, X4
  MOV X4, X5
  MOV X5, X6
  MOV X6, X7
  SVC 0
  RET
#endif
.endfunc

.global signal_trampoline
.func signal_trampoline
.type signal_trampoline, %function

signal_trampoline:
#ifdef __arm__
  SUB SP, SP, #4
  PUSH {r0-r3, r9, r12, lr}
  BL signal_dispatcher
  CBZ R0, sigret
  STR R0, [SP, #28]
  POP {r0-r3, r9, r12, lr}
  POP {PC}
sigret:
  ADD SP, SP, #32
  MOV R7, #__NR_rt_sigreturn
  SVC 0
#endif
#ifdef __aarch64__
  STP  X2,  X3, [SP, #-176]!
  STP  X4,  X5, [SP, #16]
  STP  X6,  X7, [SP, #32]
  STP  X8,  X9, [SP, #48]
  STP X10, X11, [SP, #64]
  STP X12, X13, [SP, #80]
  STP X14, X15, [SP, #96]
  STP X16, X17, [SP, #112]
  STP X18, X29, [SP, #128]
  STR X30,      [SP, #144]
  STP  X0,  X1, [SP, #160]

  BL signal_dispatcher

  LDP  X4,  X5, [SP, #16]
  LDP  X6,  X7, [SP, #32]
  LDP  X8,  X9, [SP, #48]
  LDP X10, X11, [SP, #64]
  LDP X12, X13, [SP, #80]
  LDP X14, X15, [SP, #96]
  LDP X16, X17, [SP, #112]
  LDP X18, X29, [SP, #128]
  LDR X30,      [SP, #144]
  LDP  X2,  X3, [SP], #160

  CBZ X0, sigret

  BR X0
sigret:
  ADD SP, SP, #16
  MOV X8, #__NR_rt_sigreturn
  SVC 0
#endif
.endfunc

.global atomic_increment_u64
.func atomic_increment_u64
.type atomic_increment_u64, %function

atomic_increment_u64:
#ifdef __arm__
  // R0 - ptr, R2 inc (low), R3, inc (high)
  PUSH {R4, R5}

retry:
  LDREXD R4, R5, [R0]
  ADDS R4, R2
  ADC R5, R3
  STREXD R1, R4, R5, [R0]
  CMP R1, #0
  BNE retry

  MOV R0, R4
  MOV R1, R5
  POP {R4, R5}
  BX LR

#elif __aarch64__
  LDXR X2, [X0]
  ADD X2, X2, X1
  STXR W3, X2, [X0]
  CBNZ W3, atomic_increment_u64
  MOV X0, X2
  RET

#endif
.endfunc

.global atomic_increment_u32
.func atomic_increment_u32
.type atomic_increment_u32, %function

atomic_increment_u32:
#ifdef __arm__
  LDREX R2, [R0]
  ADD R2, R1
  STREX R3, R2, [R0]
  CMP R3, #0
  BNE atomic_increment_u32
  MOV R0, R2
  BX LR

#elif __aarch64__
  LDXR W2, [X0]
  ADD W2, W2, W1
  STXR W3, W2, [X0]
  CBNZ W3, atomic_increment_u32
  MOV W0, W2
  RET

#endif
.endfunc

.global atomic_decrement_if_positive_i32
.func atomic_decrement_if_positive_i32
.type atomic_decrement_if_positive_i32, %function

atomic_decrement_if_positive_i32:
#ifdef __arm__
  LDREX R2, [R0]
  CMP R2, R1
  BLT abort
  SUB R2, R2, R1
  STREX R3, R2, [R0]
  CMP R3, #0
  BNE atomic_decrement_if_positive_i32
  MOV R0, R2
  BX LR
abort:
  CLREX
  MOV R0, #-1
  BX LR

#elif __aarch64__
  LDXR W2, [X0]
  CMP W2, W1
  BLT abort
  SUB W2, W2, W1
  STXR W3, W2, [X0]
  CBNZ W3, atomic_decrement_if_positive_i32
  MOV W0, W2
  RET
abort:
  CLREX
  MOV W0, #-1
  RET

#endif
.endfunc


.global safe_fcall_trampoline
.func safe_fcall_trampoline
.type safe_fcall_trampoline, %function

safe_fcall_trampoline:
#ifdef __arm__
  PUSH {R5-R7, R9, R12, LR}
  VPUSH {d16-d31}
  VPUSH {d0-d7}

  MOV R7, SP
  BIC R6, R7, #7
  MOV SP, R6

  MRS R5, CPSR
  VMRS R6, FPSCR

  BLX R4

  MOV SP, R7

  MSR CPSR, R5
  VMSR FPSCR, R6

  VPOP {d0-d7}
  VPOP {d16-d31}
  POP {R5-R7, R9, R12, PC}

#elif __aarch64__
  STP X8,  X9,  [SP, #-128]!
  STP X10, X11, [SP, #16]
  STP X12, X13, [SP, #32]
  STP X14, X15, [SP, #48]
  STP X16, X17, [SP, #64]
  STP X18, X19, [SP, #80]
  STP X20, X21, [SP, #96]
  STP X29, X30, [SP, #112]

  MRS X19, NZCV
  MRS X20, FPCR
  MRS X21, FPSR

  BL push_neon

  BLR X8

  BL pop_neon

  MSR NZCV, X19
  MSR FPCR, X20
  MSR FPSR, X21

  LDP X10, X11, [SP, #16]
  LDP X12, X13, [SP, #32]
  LDP X14, X15, [SP, #48]
  LDP X16, X17, [SP, #64]
  LDP X18, X19, [SP, #80]
  LDP X20, X21, [SP, #96]
  LDP X29, X30, [SP, #112]
  LDP X8,  X9,  [SP], #128

  RET
#endif

.endfunc

.global __try_memcpy_error
.type __try_memcpy_error, %function
.global __try_memcpy
.type __try_memcpy, %function

__try_memcpy:
#ifdef __arm__
  LDRB R3, [R1], #1
  STRB R3, [R0], #1
  SUB R2, #1
  CBZ R2, __try_memcpy_ret
  B __try_memcpy
__try_memcpy_ret:
  MOV R0, #0
  BX LR

__try_memcpy_error:
  MOV R0, #-1
  BX LR
#elif __aarch64__
  LDRB W3, [X1], #1
  STRB W3, [X0], #1
  SUB X2, X2, #1
  CBNZ X2, __try_memcpy
  MOV X0, #0
  RET

__try_memcpy_error:
  MOV X0, #-1
  RET
#endif

```

`util.h`:

```h
/*
  This file is part of MAMBO, a low-overhead dynamic binary modification tool:
      https://github.com/beehive-lab/mambo

  Copyright 2013-2016 Cosmin Gorgovan <cosmin at linux-geek dot org>
  Copyright 2017 The University of Manchester

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
*/

#ifndef __DBM_UTIL_H__
#define __DBM_UTIL_H__

extern void dbm_client_entry(uintptr_t addr, uintptr_t *stack_top);
extern uint32_t atomic_increment_u32(uint32_t *loc, uint32_t inc);
extern uint64_t atomic_increment_u64(uint64_t *loc, uint64_t inc);
extern int32_t atomic_decrement_if_positive_i32(int32_t *loc, int32_t inc);

static inline int32_t atomic_increment_i32(int32_t *loc, int32_t inc) {
  return (int32_t)atomic_increment_u32((uint32_t *)loc, (uint32_t)inc);
}
static inline int64_t atomic_increment_i64(int64_t *loc, int64_t inc) {
  return (int64_t)atomic_increment_u64((uint64_t *)loc, (uint64_t)inc);
}
#ifdef __arm__
  #define atomic_increment_uptr(loc, inc) atomic_increment_u32(loc, inc);
#elif __aarch64__
  #define atomic_increment_uptr(loc, inc) atomic_increment_u64(loc, inc);
#endif
#define atomic_increment_int(loc, inc) atomic_increment_i32(loc, inc);
#define atomic_increment_uint(loc, inc) atomic_increment_u32(loc, inc);

// syscall() without errno handling
extern uintptr_t raw_syscall(long number, ...);
void signal_trampoline(int i, siginfo_t *, void *);

void safe_fcall_trampoline();
void *new_thread_trampoline();
void return_with_sp(void *sp);
#endif


```