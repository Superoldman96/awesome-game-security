Project Path: arc_gmh5225_CUTracer_tj61qca5

Source Tree:

```txt
arc_gmh5225_CUTracer_tj61qca5
‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md
‚îú‚îÄ‚îÄ CONTRIBUTING.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ LICENSE-BSD
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ format.sh
‚îú‚îÄ‚îÄ include
‚îÇ   ‚îú‚îÄ‚îÄ analysis.h
‚îÇ   ‚îú‚îÄ‚îÄ common.h
‚îÇ   ‚îú‚îÄ‚îÄ env_config.h
‚îÇ   ‚îú‚îÄ‚îÄ instrument.h
‚îÇ   ‚îî‚îÄ‚îÄ log.h
‚îú‚îÄ‚îÄ install_third_party.sh
‚îú‚îÄ‚îÄ logo.svg
‚îú‚îÄ‚îÄ readme.md
‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îî‚îÄ‚îÄ parse_instr_hist_trace.py
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ analysis.cu
‚îÇ   ‚îú‚îÄ‚îÄ cutracer.cu
‚îÇ   ‚îú‚îÄ‚îÄ env_config.cu
‚îÇ   ‚îú‚îÄ‚îÄ inject_funcs.cu
‚îÇ   ‚îú‚îÄ‚îÄ instrument.cu
‚îÇ   ‚îî‚îÄ‚îÄ log.cu
‚îî‚îÄ‚îÄ tests
    ‚îú‚îÄ‚îÄ hang_test
    ‚îÇ   ‚îî‚îÄ‚îÄ test_hang.py
    ‚îú‚îÄ‚îÄ proton_tests
    ‚îÇ   ‚îî‚îÄ‚îÄ vector-add-instrumented.py
    ‚îú‚îÄ‚îÄ py_add
    ‚îÇ   ‚îî‚îÄ‚îÄ test_add.py
    ‚îî‚îÄ‚îÄ vectoradd
        ‚îú‚îÄ‚îÄ Makefile
        ‚îî‚îÄ‚îÄ vectoradd.cu

```

`CODE_OF_CONDUCT.md`:

```md
# Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to make participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies within all project spaces, and it also applies when
an individual is representing the project or its community in public spaces.
Examples of representing a project or community include using an official
project e-mail address, posting via an official social media account, or acting
as an appointed representative at an online or offline event. Representation of
a project may be further defined and clarified by project maintainers.

This Code of Conduct also applies outside the project spaces when there is a
reasonable belief that an individual's behavior may have a negative impact on
the project or its community.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at <opensource-conduct@meta.com>. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq

```

`CONTRIBUTING.md`:

```md
# Contributing to Meta Open Source Projects

We want to make contributing to this project as easy and transparent as
possible.

## Pull Requests
We actively welcome your pull requests.

Note: pull requests are not imported into the GitHub directory in the usual way. There is an internal Meta repository that is the "source of truth" for the project. The GitHub repository is generated *from* the internal Meta repository. So we don't merge GitHub PRs directly to the GitHub repository -- they must first be imported into internal Meta repository. When Meta employees look at the GitHub PR, there is a special button visible only to them that executes that import. The changes are then automatically reflected from the internal Meta repository back to GitHub. This is why you won't see your PR having being directly merged, but you still see your changes in the repository once it reflects the imported changes.

1. Fork the repo and create your branch from `main`.
2. If you've added code that should be tested, add tests.
3. If you've changed APIs, update the documentation.
4. Ensure the test suite passes.
5. Make sure your code lints.
6. If you haven't already, complete the Contributor License Agreement ("CLA").

## Contributor License Agreement ("CLA")
In order to accept your pull request, we need you to submit a CLA. You only need
to do this once to work on any of Meta's open source projects.

Complete your CLA here: <https://code.facebook.com/cla>

## Issues
We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

Meta has a [bounty program](https://www.facebook.com/whitehat/) for the safe
disclosure of security bugs. In those cases, please go through the process
outlined on that page and do not file a public issue.

## License
By contributing to this project, you agree that your contributions will be licensed
under the LICENSE file in the root directory of this source tree.

```

`LICENSE`:

```
MIT License

Copyright (c) Meta Platforms, Inc. and affiliates.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`LICENSE-BSD`:

```
/*
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice, this
 * list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
```

`Makefile`:

```
# MIT License
# Copyright (c) Meta Platforms, Inc. and affiliates.
# See LICENSE file for details.
# Project name
PROJECT := cutracer

# Compiler settings
CXX ?=
NVCC=nvcc -ccbin=$(CXX) -D_FORCE_INLINES
PTXAS=ptxas

# Version checks
NVCC_VER_REQ=10.1
NVCC_VER=$(shell $(NVCC) --version | grep release | cut -f2 -d, | cut -f3 -d' ')
NVCC_VER_CHECK=$(shell echo "${NVCC_VER} >= $(NVCC_VER_REQ)" | bc)

ifeq ($(NVCC_VER_CHECK),0)
$(error ERROR: nvcc version >= $(NVCC_VER_REQ) required to compile an nvbit tool! Instrumented applications can still use lower versions of nvcc.)
endif

PTXAS_VER_ADD_FLAG=12.3
PTXAS_VER=$(shell $(PTXAS) --version | grep release | cut -f2 -d, | cut -f3 -d' ')
PTXAS_VER_CHECK=$(shell echo "${PTXAS_VER} >= $(PTXAS_VER_ADD_FLAG)" | bc)

ifeq ($(PTXAS_VER_CHECK), 0)
MAXRREGCOUNT_FLAG=-maxrregcount=24
else
MAXRREGCOUNT_FLAG=
endif

# Debug settings
ifeq ($(DEBUG),1)
DEBUG_FLAGS := -g -O0
else
DEBUG_FLAGS := -O3 -g
endif

# Directory structure
SRC_DIR := src/
OBJ_DIR := obj/
LIB_DIR := lib/
INCLUDE_DIR := include/

# NVBIT settings
NVBIT_PATH=./third_party/nvbit/core
INCLUDES=-I$(NVBIT_PATH) -I./$(INCLUDE_DIR)

# Libraries
LIBS=-L$(NVBIT_PATH) -lnvbit
NVCC_PATH=-L $(subst bin/nvcc,lib64,$(shell which nvcc | tr -s /))

# Identify inject_funcs.cu specifically
INJECT_FUNCS_SRC := $(SRC_DIR)inject_funcs.cu
INJECT_FUNCS_OBJ := $(OBJ_DIR)inject_funcs.o

# Source files (excluding inject_funcs.cu)
CU_SRCS := $(filter-out $(INJECT_FUNCS_SRC),$(wildcard $(SRC_DIR)*.cu))
REGULAR_OBJS := $(patsubst $(SRC_DIR)%.cu,$(OBJ_DIR)%.o,$(CU_SRCS))

# All objects (regular + inject_funcs)
OBJS := $(REGULAR_OBJS) $(INJECT_FUNCS_OBJ)

# Architecture
ARCH?=all

# Output file
NVBIT_TOOL=$(LIB_DIR)/$(PROJECT).so

# Main targets
all: dirs $(NVBIT_TOOL)

dirs: $(OBJ_DIR) $(LIB_DIR)

$(OBJ_DIR):
	mkdir -p $@

$(LIB_DIR):
	mkdir -p $@

# Linking rule
$(NVBIT_TOOL): $(REGULAR_OBJS) $(INJECT_FUNCS_OBJ) $(NVBIT_PATH)/libnvbit.a
	$(NVCC) -arch=$(ARCH) $(DEBUG_FLAGS) $(REGULAR_OBJS) $(INJECT_FUNCS_OBJ) $(LIBS) $(NVCC_PATH) -lcuda -lcudart_static -shared -o $@

# Compilation rule for regular CUDA files (excluding inject_funcs.cu)
$(REGULAR_OBJS): $(OBJ_DIR)%.o: $(SRC_DIR)%.cu
	$(NVCC) -dc -c -std=c++17 $(INCLUDES) -Xptxas -cloning=no -Xcompiler -Wall -arch=$(ARCH) $(DEBUG_FLAGS) -Xcompiler -fPIC $< -o $@

# Special rule for inject_funcs.cu
$(INJECT_FUNCS_OBJ): $(INJECT_FUNCS_SRC)
	$(NVCC) $(INCLUDES) $(MAXRREGCOUNT_FLAG) -Xptxas -astoolspatch --keep-device-functions -arch=$(ARCH) -Xcompiler -Wall -Xcompiler -fPIC -c $< -o $@

clean:
	rm -rf $(OBJ_DIR) $(LIB_DIR)

.PHONY: all clean dirs

```

`format.sh`:

```sh
#!/bin/bash
# MIT License
# Copyright (c) Meta Platforms, Inc. and affiliates.
# See LICENSE file for details.
# A script to check or apply code formatting using clang-format.
# It automatically finds all relevant source files in the project.

# --- Go to script's directory ---
# This ensures that the script can be called from any location and that all
# subsequent paths are relative to the project root.
SCRIPT_DIR=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
cd -- "$SCRIPT_DIR" || exit

# --- Check for dependencies ---
if ! command -v clang-format &>/dev/null; then
  echo "‚ùå Error: clang-format is not installed or not in your PATH." >&2
  echo "Please install clang-format to use this script." >&2
  echo "  - On Debian/Ubuntu: sudo apt install clang-format" >&2
  echo "  - On Fedora/CentOS: sudo dnf install clang-tools-extra" >&2
  echo "  - On macOS (Homebrew): brew install clang-format" >&2
  exit 1
fi

# --- Configuration ---
# Define the directories to be processed.
# Add or remove directories as needed for your project.
DIRECTORIES_TO_SCAN=(src include tests tutorial .ci)

# Define the file extensions to be processed.
# Add or remove extensions as needed for your project.
FILE_PATTERNS=(-name "*.h" -o -name "*.hpp" -o -name "*.cpp" -o -name "*.cu" -o -name "*.cuh")

# (Optional) Python formatting configuration can be supplied via pyproject.toml

# --- Helper Functions ---

# Function to process a single file.
# It checks if a file needs formatting, formats it if necessary,
# and prints the filename to stdout if it was changed.
format_and_report_changes() {
  file="$1"
  # Use diff to compare the original file with clang-format's output.
  # If they differ, format the file in-place and print its name.
  if ! diff -q "${file}" <(clang-format "${file}") >/dev/null; then
    clang-format -i "${file}"
    echo "${file}"
  fi
}
# Export the function so it's available to the subshells created by xargs.
export -f format_and_report_changes

# Python checks/format aligned with format_fix.py: usort -> ruff -> black
python_check() {
  local failed=0

  if command -v usort &>/dev/null; then
    usort check .
    [ $? -eq 0 ] || failed=1
  else
    echo "‚ùå usort not found (required for Python import sorting)." >&2
    failed=1
  fi

  if command -v ruff &>/dev/null; then
    ruff check . --diff
    [ $? -eq 0 ] || failed=1
  else
    echo "‚ùå ruff not found (required for Python linting)." >&2
    failed=1
  fi

  if command -v black &>/dev/null; then
    black --check --diff .
    [ $? -eq 0 ] || failed=1
  else
    echo "‚ùå black not found (required for Python formatting)." >&2
    failed=1
  fi

  return $failed
}

python_format() {
  if command -v usort &>/dev/null; then
    echo "üé®  Sorting Python imports with usort..."
    usort format .
  else
    echo "‚ö†Ô∏è  usort not found; skipping import sorting." >&2
  fi

  if command -v ruff &>/dev/null; then
    echo "üîß  Fixing Python linting issues with ruff..."
    ruff check . --fix
  else
    echo "‚ö†Ô∏è  ruff not found; skipping lint fixes." >&2
  fi

  if command -v black &>/dev/null; then
    echo "üé®  Formatting Python code with black..."
    black .
  else
    echo "‚ö†Ô∏è  black not found; skipping code formatting." >&2
  fi
}

# Function to print usage instructions
usage() {
  echo "Usage: $0 {check|format}"
  echo "  check      Check for formatting issues without modifying files."
  echo "             Exits with an error code if issues are found."
  echo "  format     Apply formatting to files in-place."
  exit 1
}

# --- Main Script Logic ---

# Check if an argument was provided
if [ "$#" -ne 1 ]; then
  echo "Error: No mode specified."
  usage
fi

MODE=$1

# Filter for directories that actually exist to prevent 'find' errors.
EXISTING_DIRS=()
for dir in "${DIRECTORIES_TO_SCAN[@]}"; do
  if [ -d "$dir" ]; then
    EXISTING_DIRS+=("$dir")
  fi
done

if [ ${#EXISTING_DIRS[@]} -eq 0 ]; then
  echo "‚è©  No source directories found to process. Searched for: ${DIRECTORIES_TO_SCAN[*]}. Exiting."
  exit 0
fi

# We pipe the output of find directly to xargs.
# This avoids issues with storing null-delimited strings in shell variables.
# The -r flag for xargs prevents it from running the command if find returns no files.

case "$MODE" in
check)
  echo "üîé  Checking code formatting in: ${EXISTING_DIRS[*]}..."

  # Step 1: C/C++ check with --Werror to get a reliable exit code
  find "${EXISTING_DIRS[@]}" -type f \( "${FILE_PATTERNS[@]}" \) -print0 | xargs -0 -r clang-format --dry-run --Werror
  CXX_STATUS=$?

  # Step 2: Python check using ufmt or black/usort
  python_check
  PY_STATUS=$?

  if [ $CXX_STATUS -ne 0 ] || [ $PY_STATUS -ne 0 ]; then
    echo "----------------------------------------------------"
    echo "Please run './format.sh format' to fix them."
    exit 1
  fi

  echo "‚úÖ  All files are correctly formatted (or no files were found to check)."
  exit 0
  ;;

format)
  echo "üé®  Applying code formatting to: ${EXISTING_DIRS[*]}..."

  # Use xargs to run the formatting function in parallel.
  # -P 0 tells xargs to use as many processes as available CPU cores.
  # The output will be a list of files that were actually changed.
  CHANGED_FILES=$(find "${EXISTING_DIRS[@]}" -type f \( "${FILE_PATTERNS[@]}" \) -print0 | \
    xargs -0 -P 0 -I {} bash -c 'format_and_report_changes "{}"')

  if [ -n "$CHANGED_FILES" ]; then
    echo "‚ú® Changed files:"
    # Use printf to format the list of changed files neatly.
    printf "  - %s\n" $CHANGED_FILES
  else
    echo "No files needed formatting."
  fi

  # Python formatting
  python_format

  echo "‚úÖ  Formatting complete."
  ;;

*)
  echo "Error: Invalid mode '$MODE'."
  usage
  ;;
esac

exit 0

```

`include/analysis.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef ANALYSIS_H
#define ANALYSIS_H
#include <ctime>
#include <deque>
#include <map>
#include <set>
#include <string>
#include <unordered_map>
#include <unordered_set>
#include <vector>

#include "common.h"
#include "nvbit.h"
/* for channel */
#include "utils/channel.hpp"

/* Channel buffer size */
#define CHANNEL_SIZE (1l << 20)

/* Thread state enum */
enum class RecvThreadState {
  WORKING,
  STOP,
  FINISHED,
};

/* ===== Data Structures ===== */

/* Structure to uniquely identify a warp */
struct WarpKey {
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  // global warp id
  int warp_id;

  // Operator for map comparison
  bool operator<(const WarpKey &other) const {
    if (cta_id_x != other.cta_id_x) return cta_id_x < other.cta_id_x;
    if (cta_id_y != other.cta_id_y) return cta_id_y < other.cta_id_y;
    if (cta_id_z != other.cta_id_z) return cta_id_z < other.cta_id_z;
    return warp_id < other.warp_id;
  }

  // Hash function for unordered_map
  struct Hash {
    size_t operator()(const WarpKey &k) const {
      return (size_t)k.cta_id_x ^ ((size_t)k.cta_id_y << 10) ^ ((size_t)k.cta_id_z << 20) ^ ((size_t)k.warp_id << 30);
    }
  };

  // Equality operator for unordered_map
  bool operator==(const WarpKey &other) const {
    return cta_id_x == other.cta_id_x && cta_id_y == other.cta_id_y && cta_id_z == other.cta_id_z &&
           warp_id == other.warp_id;
  }
};

// Merged trace record containing mandatory reg trace and optional mem trace
struct TraceRecordMerged {
  reg_info_t reg;
  bool has_mem = false;
  uint64_t mem_addrs[32] = {0};
};

// Structure to track the loop state of a warp
struct WarpLoopState {
  // Circular buffer of recent merged trace records
  std::vector<TraceRecordMerged> history;
  uint8_t head;    // Next write position in circular buffer
  uint8_t filled;  // Number of valid entries written, capped at buffer size
  uint64_t last_sig;
  uint8_t last_period;
  uint32_t repeat_cnt;
  bool loop_flag;
  time_t first_loop_time;

  // Structure to hold complete loop information
  struct LoopInfo {
    std::vector<TraceRecordMerged> instructions;  // Copy of one canonical period
    uint8_t period;
  };

  LoopInfo current_loop;

  WarpLoopState()
      : head(0), filled(0), last_sig(0), last_period(0), repeat_cnt(0), loop_flag(false), first_loop_time(0) {
  }
};

/**
 * @brief Represents the state of a single warp during instruction histogram
 * analysis.
 *
 * This structure tracks whether a warp is currently in a region of interest
 * for collection and stores the histogram data for that region.
 */
struct WarpState {
  /**
   * @brief A flag indicating whether instruction collection is active for this
   * warp.
   *
   * This acts as a switch, turned on by a "start" clock instruction and off
   * by an "end" clock instruction.
   */
  bool is_collecting = false;
  /**
   * @brief A counter for the number of regions analyzed for this warp.
   *
   * This helps in uniquely identifying each region within a warp's execution.
   */
  int region_counter = 0;
  /**
   * @brief The histogram of instructions collected for the current region.
   *
   * Maps an instruction name (string) to its execution count (int).
   */
  std::map<std::string, int> histogram;
};

/**
 * @brief Stores the completed instruction histogram for a specific region of a
 * warp.
 */
struct RegionHistogram {
  /**
   * @brief The ID of the warp.
   */
  int warp_id;
  /**
   * @brief The ID of the region within the warp.
   */
  int region_id;
  /**
   * @brief The completed histogram for this region.
   */
  std::map<std::string, int> histogram;
};

/**
 * @brief Stores the completed instruction histogram for a specific region of a
 * warp.
 */
struct CTXstate {
  /* context id */
  int id;

  /* Channel used to communicate from GPU to CPU receiving thread */
  ChannelDev *channel_dev;
  ChannelHost channel_host;

  // After initialization, set it to WORKING to make recv thread get data,
  // parent thread sets it to STOP to make recv thread stop working.
  // recv thread sets it to FINISHED when it cleans up.
  // parent thread should wait until the state becomes FINISHED to clean up.
  volatile RecvThreadState recv_thread_done = RecvThreadState::STOP;

  // Per-function SASS mappings for instruction histogram feature
  std::unordered_map<CUfunction, std::map<int, std::string>> id_to_sass_map;
  std::unordered_map<CUfunction, std::unordered_set<int>> clock_opcode_ids;
  // Per-function EXIT opcode ids (statically identified at instrumentation time)
  std::unordered_map<CUfunction, std::unordered_set<int>> exit_opcode_ids;

  /* State for Deadlock/Hang Detection */
  std::map<WarpKey, WarpLoopState> loop_states;
  std::set<WarpKey> active_warps;
  time_t last_hang_check_time;

  // Pending mem traces per warp for out-of-order arrival (mem before reg)
  std::unordered_map<WarpKey, std::deque<mem_access_t>, WarpKey::Hash> pending_mem_by_warp;

  // Per-warp activity timestamps for inactive cleanup
  std::unordered_map<WarpKey, time_t, WarpKey::Hash> last_seen_time_by_warp;
  std::unordered_map<WarpKey, time_t, WarpKey::Hash> exit_candidate_since_by_warp;

  // Deadlock handling
  int deadlock_consecutive_hits = 0;
  bool deadlock_termination_initiated = false;
};

// =================================================================================
// Function Declarations
// =================================================================================

/**
 * @brief The main thread function for receiving and processing data from the
 * GPU.
 *
 * This function runs in a separate CPU thread, continuously receiving data
 * packets (like `reg_info_t`, `mem_access_t`, `opcode_only_t`) from the GPU
 * channel and dispatching them for analysis.
 *
 * @param args A pointer to the `CUcontext` for which this thread is launched.
 * @return void*
 */
void *recv_thread_fun(void *args);

/**
 * @brief Writes a set of histograms to a formatted CSV file.
 *
 * @param ctx The CUDA context.
 * @param func The kernel function.
 * @param iteration The iteration number of the kernel launch.
 * @param histograms The histogram data to be written.
 */
void dump_histograms_to_csv(CUcontext ctx, CUfunction func, uint32_t iteration,
                            const std::vector<RegionHistogram> &histograms);

#endif /* ANALYSIS_H */

```

`include/common.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed under MIT,
 * based on original NVIDIA nvbit sample code licensed under BSD-3-Clause.
 * See LICENSE file in the root directory for Meta's license terms.
 * See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#ifndef COMMON_H
#define COMMON_H

#include <stdint.h>

/* Message type enum to identify different message types */
typedef enum { MSG_TYPE_REG_INFO = 0, MSG_TYPE_MEM_ACCESS = 1, MSG_TYPE_OPCODE_ONLY = 2 } message_type_t;

/* Common header for all message types */
typedef struct {
  message_type_t type;  // Type of the message
} message_header_t;

/* Based on NVIDIA record_reg_vals example with Meta modifications for message type support and adds CUTracer specific
 * extensions */
typedef struct {
  message_header_t header;  // Common header with type=MSG_TYPE_REG_INFO
  int32_t cta_id_x;
  int32_t cta_id_y;
  int32_t cta_id_z;
  int32_t warp_id;
  int32_t opcode_id;
  int32_t num_regs;
  /* 32 lanes, each thread can store up to 8 register values */
  uint32_t reg_vals[32][8];

  // CUTracer extensions
  uint64_t kernel_launch_id;  // Global kernel launch id
  uint64_t pc;                // Program counter for the instruction
  int32_t num_uregs;          // Number of unified registers
  uint32_t ureg_vals[8];      // Unified registers shared by all threads in the same warp
} reg_info_t;

/* Based on NVIDIA mem_trace example with Meta modifications for message type support */
typedef struct {
  message_header_t header;  // Common header with type=MSG_TYPE_MEM_ACCESS
  uint64_t kernel_launch_id;
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  uint64_t pc;
  int warp_id;
  int opcode_id;
  uint64_t addrs[32];
} mem_access_t;

/**
 * @brief A lightweight data packet for instruction histogram analysis.
 *
 * This structure is sent from the GPU to the CPU when `OPCODE_ONLY`
 * instrumentation is enabled. It contains the minimal information required
 * to identify an instruction and its execution context without the overhead
 * of register or memory data.
 */
typedef struct {
  message_header_t header;  // Common header with type=MSG_TYPE_OPCODE_ONLY
  uint64_t kernel_launch_id;
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  uint64_t pc;
  int warp_id;
  int opcode_id;
} opcode_only_t;

#endif /* COMMON_H */

```

`include/env_config.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#pragma once

#include <stdint.h>

#include <string>
#include <unordered_set>
#include <vector>
// Forward declaration to avoid circular dependency
enum class InstrumentType;

/**
 * @brief Defines the type of analysis to be performed on the collected trace
 * data.
 */
enum class AnalysisType {
  /**
   * @brief No analysis is performed.
   */
  ANALYSIS_NONE = 0,
  /**
   * @brief Enables instruction histogram analysis.
   *
   * This corresponds to the `proton_instr_histogram` setting and requires
   * `OPCODE_ONLY` instrumentation.
   */
  PROTON_INSTR_HISTOGRAM = 1,

  /**
   * @brief Enables deadlock detection analysis.
   */
  DEADLOCK_DETECTION = 2,
};

// Configuration variables
extern uint32_t instr_begin_interval;
extern uint32_t instr_end_interval;
extern int verbose;

// Kernel name filters
extern std::vector<std::string> kernel_filters;
// Instrumentation configuration
extern std::unordered_set<InstrumentType> enabled_instrument_types;

// Analysis configuration
extern std::unordered_set<AnalysisType> enabled_analysis_types;

// Initialize configuration from environment variables
void init_config_from_env();

// Check if a specific instrumentation type is enabled
bool is_instrument_type_enabled(InstrumentType type);

// Check if a specific analysis type is enabled
bool is_analysis_type_enabled(AnalysisType type);

// Initialize instrumentation configuration
void init_instrumentation(const std::string &instrument_str);

// Initialize analysis configuration
void init_analysis(const std::string &analysis_str);

```

`include/instrument.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef INSTRUMENT_H
#define INSTRUMENT_H

#include <vector>

#include "analysis.h"

/**
 * @brief Instrumentation types for different data collection modes
 */
enum class InstrumentType {
  OPCODE_ONLY,  // Lightweight: only collect opcode information
  REG_TRACE,    // Medium: collect register values
  MEM_TRACE     // Heavy: collect memory access information
};

/**
 * @brief Insert lightweight opcode-only instrumentation for instruction histogram analysis
 *
 * This is optimized for Proton instruction statistic analysis where only opcode
 * information is needed for histogram generation.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 */
void instrument_opcode_only(Instr* instr, int opcode_id, CTXstate* ctx_state);

/**
 * @brief Insert register tracing instrumentation
 *
 * Collects register values for detailed register flow analysis.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 * @param reg_num_list List of register numbers to trace
 * @param ureg_num_list List of uniform register numbers to trace
 */
void instrument_register_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, const std::vector<int>& reg_num_list,
                               const std::vector<int>& ureg_num_list);

/**
 * @brief Insert memory access tracing instrumentation
 *
 * Collects memory access information for memory pattern analysis.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 * @param mref_idx Memory reference index
 */
void instrument_memory_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, int mref_idx);

#endif /* INSTRUMENT_H */

```

`include/log.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef LOG_HANDLE_H
#define LOG_HANDLE_H

#include <stdint.h>
#include <stdio.h>
#include <time.h>

#include <string>

#include "cuda.h"

// Forward declarations - use void* to avoid conflicts with CUDA types
typedef void *CUcontext_ptr;
typedef void *CUfunction_ptr;

/* ===== Logging Functions ===== */

/**
 * lprintf - print to the currently active log file only (log print)
 */
void lprintf(const char *format, ...);

/**
 * oprintf - print to stdout only (output print)
 */
void oprintf(const char *format, ...);

/**
 * loprintf - print to the currently active log file and stdout (log and output print)
 */
void loprintf(const char *format, ...);

/**
 * trace_lprintf - print to the kernel trace log file only
 */
void trace_lprintf(const char *format, ...);

/* ===== File Management Functions ===== */

/**
 * Opens a new log file for a specific kernel invocation.
 * This should be called on kernel entry.
 * @param ctx CUDA context
 * @param func CUfunction representing the kernel
 * @param iteration Current iteration of the kernel execution
 */
void log_open_kernel_file(CUcontext_ptr ctx, CUfunction_ptr func, uint32_t iteration);

/**
 * Closes the kernel-specific log file.
 * This should be called on kernel exit.
 */
void log_close_kernel_file();

/**
 * Initializes the log handle system. Creates the main process log file.
 */
void init_log_handle();

/**
 * Cleans up the log handle system. Closes the main process log file.
 */
void cleanup_log_handle();

/**
 * Computes a per-kernel hash as a lowercase hexadecimal string (without the "0x" prefix).
 *
 * Implementation details:
 * - Retrieves the mangled kernel name via nvbit_get_func_name(ctx, func, true).
 * - Falls back to the literal string "unknown_kernel" when the name is not available.
 * - Applies std::hash<std::string> to the full mangled name and formats the result in hex.
 *
 * Notes:
 * - The exact numeric value of std::hash is implementation-dependent, but this function
 *   is the single source of truth used by both cutracer.cu and log.cu, ensuring consistency
 *   across modules in the same build.
 * - When printing, prepend "0x" yourself if a prefixed form is desired (e.g., "0x%" + str).
 */
std::string compute_kernel_name_hash_hex(CUcontext ctx, CUfunction func);
/**
 * Builds a deterministic base filename for a kernel's trace log.
 *
 * Format:
 *   "kernel_<hash_hex>_iter<iteration>_<truncated_mangled_name>"
 *
 * Details:
 * - Uses the mangled kernel name and compute_kernel_name_hash_hex(ctx, func) to
 *   derive a hex hash for uniqueness and name stability across modules.
 * - Appends the kernel iteration number to distinguish repeated launches.
 * - Includes a truncated (up to 150 chars) copy of the mangled name to aid
 *   human readability while keeping the filename manageable.
 *
 * Args:
 *   ctx: CUDA context associated with the kernel function.
 *   func: The CUfunction handle of the kernel.
 *   iteration: Per-kernel iteration counter maintained by the caller.
 *
 * Returns:
 *   The base filename (without extension) for the kernel-specific log file.
 */
std::string generate_kernel_log_basename(CUcontext ctx, CUfunction func, uint32_t iteration);

#endif /* LOG_HANDLE_H */

```

`install_third_party.sh`:

```sh
#!/bin/bash
# Copyright (c) Meta Platforms, Inc. and affiliates.

# Script to automatically download and install the latest version of NVBit to the third_party directory

# Create third_party directory (if it doesn't exist)
mkdir -p third_party

# Use GitHub API to get the latest release information
echo "Getting latest NVBit version information..."
RELEASE_INFO=$(curl -s https://api.github.com/repos/NVlabs/NVBit/releases/latest)

# Check if API call was successful
if [ $? -ne 0 ]; then
  echo "Error: Unable to get NVBit release information. Please check your network connection or GitHub API access."
  exit 1
fi

# Get the latest version number
VERSION=$(echo $RELEASE_INFO | grep -o '"tag_name": "[^"]*' | cut -d'"' -f4)
echo "Latest version: $VERSION"

# Find the download link for the x86_64 version
DOWNLOAD_URL=$(echo $RELEASE_INFO | grep -o '"browser_download_url": "[^"]*x86_64[^"]*\.tar\.bz2"' | cut -d'"' -f4)

# Check if download link was found
if [ -z "$DOWNLOAD_URL" ]; then
  echo "Error: Unable to find download link for x86_64 version."
  exit 1
fi

echo "Download link: $DOWNLOAD_URL"

# Download NVBit package
echo "Downloading NVBit..."
TEMP_FILE=$(mktemp)
curl -L -o "$TEMP_FILE" "$DOWNLOAD_URL"

# Check if download was successful
if [ $? -ne 0 ]; then
  echo "Error: Download failed."
  rm -f "$TEMP_FILE"
  exit 1
fi

# Clean up old version (if exists)
echo "Cleaning up old version..."
rm -rf third_party/nvbit

# Extract to temporary directory
echo "Extracting NVBit..."
TEMP_DIR=$(mktemp -d)
tar -xjf "$TEMP_FILE" -C "$TEMP_DIR"

# Check if extraction was successful
if [ $? -ne 0 ]; then
  echo "Error: Extraction failed."
  rm -f "$TEMP_FILE"
  rm -rf "$TEMP_DIR"
  exit 1
fi

# Find the extracted directory
EXTRACTED_DIR=$(find "$TEMP_DIR" -maxdepth 1 -name "nvbit*" -type d | head -1)
if [ -z "$EXTRACTED_DIR" ]; then
  echo "Error: Unable to find extracted NVBit directory."
  rm -f "$TEMP_FILE"
  rm -rf "$TEMP_DIR"
  exit 1
fi

# Move the extracted directory to third_party/nvbit
echo "Installing NVBit to third_party/nvbit..."
mv "$EXTRACTED_DIR" third_party/nvbit

# Clean up temporary files and directories
rm -f "$TEMP_FILE"
rm -rf "$TEMP_DIR"

echo "NVBit $VERSION has been successfully installed to third_party/nvbit directory."

```

`logo.svg`:

```svg
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 280 280" role="img" aria-label="CUTracer icon ‚Äî 9 centered outer pins (fixed)">
  <style>
    :root{
      --ink:#0b1221;
      --accent:#76B900;   /* NVIDIA green */
      --chip:#f7f9fb;     /* chip background */
      --pin:#76B900;      /* pin green */
      --mem:#9aa3ad;      /* memory grid gray */
      --reg:#9aa3ad;      /* register gray */
    }
    @media (prefers-color-scheme: dark){
      :root{
        --ink:#e5e7eb; --chip:#1e293b; --mem:#7a828c; --reg:#7a828c; --pin:#76B900;
      }
    }
    .ns{vector-effect:non-scaling-stroke}
  </style>

  <!-- chip body (internal content limited to 40‚Äì240) -->
  <rect x="40" y="40" width="200" height="200" rx="20"
        fill="var(--chip)" stroke="var(--ink)" stroke-width="2" class="ns"/>

  <!-- outer pins: 9 per side, 10√ó10, 13 spacing, start at 43, strictly centered -->
  <g fill="var(--pin)">
    <!-- top -->
    <g transform="translate(43,28)">
      <rect x="0"   y="0" width="10" height="10" rx="2"/>
      <rect x="23"  y="0" width="10" height="10" rx="2"/>
      <rect x="46"  y="0" width="10" height="10" rx="2"/>
      <rect x="69"  y="0" width="10" height="10" rx="2"/>
      <rect x="92"  y="0" width="10" height="10" rx="2"/>
      <rect x="115" y="0" width="10" height="10" rx="2"/>
      <rect x="138" y="0" width="10" height="10" rx="2"/>
      <rect x="161" y="0" width="10" height="10" rx="2"/>
      <rect x="184" y="0" width="10" height="10" rx="2"/>
    </g>
    <!-- bottom -->
    <g transform="translate(43,242)">
      <rect x="0"   y="0" width="10" height="10" rx="2"/>
      <rect x="23"  y="0" width="10" height="10" rx="2"/>
      <rect x="46"  y="0" width="10" height="10" rx="2"/>
      <rect x="69"  y="0" width="10" height="10" rx="2"/>
      <rect x="92"  y="0" width="10" height="10" rx="2"/>
      <rect x="115" y="0" width="10" height="10" rx="2"/>
      <rect x="138" y="0" width="10" height="10" rx="2"/>
      <rect x="161" y="0" width="10" height="10" rx="2"/>
      <rect x="184" y="0" width="10" height="10" rx="2"/>
    </g>
    <!-- left -->
    <g transform="translate(28,43)">
      <rect x="0" y="0"   width="10" height="10" rx="2"/>
      <rect x="0" y="23"  width="10" height="10" rx="2"/>
      <rect x="0" y="46"  width="10" height="10" rx="2"/>
      <rect x="0" y="69"  width="10" height="10" rx="2"/>
      <rect x="0" y="92"  width="10" height="10" rx="2"/>
      <rect x="0" y="115" width="10" height="10" rx="2"/>
      <rect x="0" y="138" width="10" height="10" rx="2"/>
      <rect x="0" y="161" width="10" height="10" rx="2"/>
      <rect x="0" y="184" width="10" height="10" rx="2"/>
    </g>
    <!-- right -->
    <g transform="translate(242,43)">
      <rect x="0" y="0"   width="10" height="10" rx="2"/>
      <rect x="0" y="23"  width="10" height="10" rx="2"/>
      <rect x="0" y="46"  width="10" height="10" rx="2"/>
      <rect x="0" y="69"  width="10" height="10" rx="2"/>
      <rect x="0" y="92"  width="10" height="10" rx="2"/>
      <rect x="0" y="115" width="10" height="10" rx="2"/>
      <rect x="0" y="138" width="10" height="10" rx="2"/>
      <rect x="0" y="161" width="10" height="10" rx="2"/>
      <rect x="0" y="184" width="10" height="10" rx="2"/>
    </g>
  </g>

  <!-- internal elements -->
  <!-- memory grid -->
  <g transform="translate(56,56)" fill="var(--mem)">
    <rect x="0"  y="0"  width="24" height="24" rx="4"/>
    <rect x="30" y="0"  width="24" height="24" rx="4"/>
    <rect x="60" y="0"  width="24" height="24" rx="4"/>
    <rect x="0"  y="30" width="24" height="24" rx="4"/>
    <rect x="30" y="30" width="24" height="24" rx="4"/>
    <rect x="60" y="30" width="24" height="24" rx="4"/>
    <rect x="0"  y="60" width="24" height="24" rx="4"/>
    <rect x="30" y="60" width="24" height="24" rx="4"/>
    <rect x="60" y="60" width="24" height="24" rx="4"/>
  </g>

  <!-- register lanes -->
  <g transform="translate(164,56)" fill="var(--reg)">
    <rect x="0" y="0"  width="60" height="6" rx="3"/>
    <rect x="0" y="10" width="60" height="6" rx="3"/>
    <rect x="0" y="20" width="60" height="6" rx="3"/>
    <rect x="0" y="30" width="60" height="6" rx="3"/>
    <rect x="0" y="40" width="60" height="6" rx="3"/>
    <rect x="0" y="50" width="60" height="6" rx="3"/>
    <rect x="0" y="60" width="60" height="6" rx="3"/>
    <rect x="0" y="70" width="60" height="6" rx="3"/>
  </g>

  <!-- SASS trace (avoid overlapping with magnifier) -->
  <polyline points="60,230 100,204 130,214 160,190 190,202 220,192"
            fill="none" stroke="var(--accent)" stroke-width="8" stroke-linecap="round" stroke-linejoin="round" class="ns"/>
  <g fill="var(--accent)">
    <circle cx="60" cy="230" r="6"/>
    <circle cx="130" cy="214" r="6"/>
    <circle cx="160" cy="190" r="6"/>
    <circle cx="220" cy="192" r="6"/>
  </g>

  <!-- magnifier -->
  <g transform="translate(180,140)">
    <circle r="22" fill="none" stroke="var(--accent)" stroke-width="4" class="ns"/>
    <line x1="14" y1="14" x2="30" y2="30" stroke="var(--accent)" stroke-width="6" stroke-linecap="round" class="ns"/>
    <circle cx="8" cy="4" r="5" fill="var(--accent)"/>
  </g>

  <!-- deadlock cue -->
  <g transform="translate(220,120)">
    <path d="M0,-12 A12,12 0 1 1 -0.01,-12" fill="none" stroke="var(--accent)" stroke-width="3.5"
          stroke-dasharray="20 12" stroke-linecap="round" class="ns"/>
  </g>

  <!-- IPC semicircle -->
  <g transform="translate(110,130)">
    <path d="M0,0 A26,26 0 1 1 0.01,0" fill="none" stroke="rgba(118,185,0,.25)" stroke-width="7" class="ns"/>
    <path d="M0,0 A26,26 0 0 1 20,-12" fill="none" stroke="var(--accent)" stroke-width="7" stroke-linecap="round" class="ns"/>
  </g>
</svg>

```

`readme.md`:

```md
# CUTracer

CUTracer is an NVBit-based CUDA binary instrumentation tool. It cleanly separates lightweight data collection (instrumentation) from host-side processing (analysis). Typical workflows include per-warp instruction histograms (delimited by GPU clock reads) and kernel hang detection.

## Features

-   NVBit-powered, runtime attach via `CUDA_INJECTION64_PATH` (no app rebuild needed)
-   Multiple instrumentation modes: opcode-only, register trace, memory trace
-   Built-in analyses:
    -   Instruction Histogram (for Proton/Triton workflows)
    -   Deadlock/Hang Detection
-   CUDA Graph and stream-capture aware flows
-   Deterministic kernel log file naming and CSV outputs

## Quickstart

1. Install third-party dependency (NVBit):

```bash
git clone git@github.com:facebookresearch/CUTracer.git
cd CUTracer
./install_third_party.sh
```

2. Build the tool:

```bash
make -j$(nproc)
```

3. Run your CUDA app with CUTracer (example: No instrumentation):

```bash
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so \
./your_app
```

## Configuration (env vars)

-   `CUTRACER_INSTRUMENT`: comma-separated modes: `opcode_only`, `reg_trace`, `mem_trace`
-   `CUTRACER_ANALYSIS`: comma-separated analyses: `proton_instr_histogram`, `deadlock_detection`
    -   Enabling `proton_instr_histogram` auto-enables `opcode_only`
    -   Enabling `deadlock_detection` auto-enables `reg_trace`
-   `KERNEL_FILTERS`: comma-separated substrings matching unmangled or mangled kernel names
-   `INSTR_BEGIN`, `INSTR_END`: static instruction index gate during instrumentation
-   `TOOL_VERBOSE`: 0/1/2

Note: The tool sets `CUDA_MANAGED_FORCE_DEVICE_ALLOC=1` to simplify channel memory handling.

## Analyses

### Instruction Histogram (proton_instr_histogram)

-   Counts SASS instruction mnemonics per warp within regions delimited by clock reads (start/stop model; nested regions not supported)
-   Output: one CSV per kernel launch with columns `warp_id,region_id,instruction,count`

### Deadlock / Hang Detection (deadlock_detection)

-   Detects sustained hangs by identifying warps stuck in stable PC loops; logs and issues SIGTERM‚ÜíSIGKILL if sustained
-   Requires `reg_trace` (auto-enabled)

## Examples

### Triton/Proton (histogram + IPC):

```bash
cd ~/CUTracer/tests/proton_tests

# 1) Collect histogram with CUTracer
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so \
CUTRACER_ANALYSIS=proton_instr_histogram \
KERNEL_FILTERS=add_kernel \
python ./vector-add-instrumented.py

# 2) Run without CUTracer to generate a clean Chrome trace
python ./vector-add-instrumented.py

# 3) Merge and compute IPC
python ~/CUTracer/scripts/parse_instr_hist_trace.py \
  --chrome-trace ./vector.chrome_trace \
  --cutracer-trace ./kernel_*_add_kernel_hist.csv \
  --cutracer-log ./cutracer_main_*.log \
  --output vectoradd_ipc.csv
```

### Deadlock/Hang detection (intentional loop example):

```bash
cd ~/CUTracer/tests/hang_test
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so \
CUTRACER_ANALYSIS=deadlock_detection \
python ./test_hang.py
```

## Troubleshooting

-   No CSV/log: check `CUDA_INJECTION64_PATH`, `KERNEL_FILTERS`, and write permissions
-   Empty histogram: ensure kernels emit clock instructions (e.g., Triton `pl.scope`)
-   High overhead: prefer opcode-only; narrow filters; use `INSTR_BEGIN/INSTR_END`
-   CUDA Graph/stream capture: data is flushed at `cuGraphLaunch` exit; ensure stream sync
-   IPC merge issues: resolve warp mismatches and kernel hash ambiguity with parser flags

## License

This repository contains code under the MIT license (Meta) and the BSD-3-Clause license (NVIDIA). See [LICENSE](LICENSE) and [LICENSE-BSD](LICENSE-BSD) for details.

## More Documentation

The full documentation lives in the [Wiki](https://github.com/facebookresearch/CUTracer/wiki). Key topics include Quickstart, Analyses, Post-processing, Configuration, Outputs, API & Data Structures, Developer Guide, and Troubleshooting.

```

`scripts/parse_instr_hist_trace.py`:

```py
#!/usr/bin/env python3
#  Copyright (c) Meta Platforms, Inc. and affiliates.
import argparse
import json
import os
import re
import sys

import pandas as pd


def get_chrome_trace_df(input_file_path):
    """
    Parses a Chrome trace file and returns a pandas DataFrame.

    Args:
        input_file_path (str): The path to the input Chrome trace JSON file.

    Returns:
        pandas.DataFrame: A DataFrame containing the parsed event data.
    """
    try:
        with open(input_file_path, "r") as f:
            trace_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Input file not found at {input_file_path}")
        return None
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {input_file_path}")
        return None

    trace_events = trace_data.get("traceEvents", [])
    if not trace_events:
        print("No trace events found in the file.")
        return None

    pid_pattern = re.compile(r"Core(\d+)\s+CTA(\d+)")
    tid_pattern = re.compile(r"warp (\d+)")

    parsed_data = []
    for event in trace_events:
        pid = event.get("pid", "")
        tid = event.get("tid", "")

        pid_match = pid_pattern.search(pid)
        tid_match = tid_pattern.search(tid)

        if pid_match:
            core_id = int(pid_match.group(1))
            cta_id = int(pid_match.group(2))
            # This is the local warp ID within a CTA
            local_warp_id = int(tid_match.group(1)) if tid_match else 0

            dur = event.get("dur")
            parsed_data.append(
                {
                    "name": event.get("name"),
                    "category": event.get("cat"),
                    "cycles": dur * 1000 if dur is not None else None,
                    "timestamp_ns": (
                        event.get("ts") * 1000 if event.get("ts") is not None else None
                    ),
                    "core": core_id,
                    "cta": cta_id,
                    "local_warp_id": local_warp_id,
                }
            )

    return pd.DataFrame(parsed_data)


def get_cutracer_hist_df(input_file_path):
    """
    Parses a CUTRICER trace histogram file and returns an aggregated DataFrame.

    Args:
        input_file_path (str): The path to the input CUTRICER trace CSV file.

    Returns:
        pandas.DataFrame: A DataFrame with aggregated instruction counts per warp and region.
    """
    try:
        df = pd.read_csv(input_file_path)
    except FileNotFoundError:
        print(f"Error: Input file not found at {input_file_path}")
        return None
    except Exception as e:
        print(f"An error occurred while reading the CSV file: {e}")
        return None

    required_columns = {"warp_id", "region_id", "count"}
    if not required_columns.issubset(df.columns):
        print(f"Error: Input CSV must contain the columns: {list(required_columns)}")
        return None

    summary = df.groupby(["warp_id", "region_id"])["count"].sum().reset_index()
    summary = summary.rename(
        columns={
            "count": "total_instruction_count",
            "warp_id": "global_warp_id",  # Clarify that this is the global warp ID
        }
    )
    return summary


def parse_cutracer_log(log_file_path, kernel_hash_hex):
    """
    Parses a CUTRICER log file to find the launch parameters for a *specific* kernel hash.
    If multiple launches with the same hash are found, it uses the first one and prints a warning.

    Args:
        log_file_path (str): Path to the CUTRICER log file.
        kernel_hash_hex (str): The mandatory target kernel hash (e.g., "0x7fa21c3").

    Returns:
        dict: { 'grid_size': (gx, gy, gz), 'block_size': (bx, by, bz) } or None if not found.
    """
    grid_pattern = re.compile(r"grid size\s+([0-9]+),([0-9]+),([0-9]+)")
    block_pattern = re.compile(r"block size\s+([0-9]+),([0-9]+),([0-9]+)")
    hash_pattern = re.compile(r"kernel hash\s+0x([0-9a-fA-F]+)")

    expected_hash = kernel_hash_hex.lower().lstrip("0x")
    matching_launches = []

    try:
        with open(log_file_path, "r") as f:
            for line in f:
                if "LAUNCH" not in line:
                    continue

                hash_match = hash_pattern.search(line)
                if not hash_match or hash_match.group(1).lower() != expected_hash:
                    continue

                # Found a matching launch line
                grid_match = grid_pattern.search(line)
                block_match = block_pattern.search(line)

                if grid_match and block_match:
                    launch_info = {
                        "grid_size": tuple(map(int, grid_match.groups())),
                        "block_size": tuple(map(int, block_match.groups())),
                    }
                    matching_launches.append(launch_info)

    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file_path}")
        return None

    if not matching_launches:
        return None

    if len(matching_launches) > 1:
        print(
            f"Warning: Found {len(matching_launches)} launches for kernel hash '{kernel_hash_hex}'. "
            "Defaulting to the first one.",
            file=sys.stderr,
        )

    return matching_launches[0]


def calculate_ipc(cycles, instruction_count):
    """
    Calculates Instructions Per Cycle (IPC).

    Args:
        cycles (float): Number of cycles.
        instruction_count (float): Number of instructions.

    Returns:
        float: IPC value, or None if calculation not possible.
    """
    if (
        pd.isna(cycles)
        or pd.isna(instruction_count)
        or cycles <= 0
        or instruction_count <= 0
    ):
        return None

    # Calculate IPC
    ipc = instruction_count / cycles
    return ipc


def validate_warp_coverage(chrome_df, hist_df):
    """
    Validates that Chrome trace and CUTRICER histogram have matching warp IDs.

    Args:
        chrome_df (pandas.DataFrame): Chrome trace data with global_warp_id
        hist_df (pandas.DataFrame): CUTRICER histogram data with global_warp_id

    Returns:
        bool: True if all warp IDs match, False otherwise
    """
    chrome_warp_ids = set(chrome_df["global_warp_id"].unique())
    hist_warp_ids = set(hist_df["global_warp_id"].unique())

    chrome_only = chrome_warp_ids - hist_warp_ids
    hist_only = hist_warp_ids - chrome_warp_ids

    if chrome_only:
        print(
            f"ERROR: Chrome trace has {len(chrome_only)} warps not found in histogram:"
        )
        sorted_chrome_only = sorted(list(chrome_only))
        if len(sorted_chrome_only) <= 10:
            print(f"  Missing warp IDs: {sorted_chrome_only}")
        else:
            print(f"  First 10 missing warp IDs: {sorted_chrome_only[:10]}")
            print(f"  ... and {len(sorted_chrome_only) - 10} more")

    if hist_only:
        print(f"ERROR: Histogram has {len(hist_only)} warps not found in Chrome trace:")
        sorted_hist_only = sorted(list(hist_only))
        if len(sorted_hist_only) <= 10:
            print(f"  Extra warp IDs: {sorted_hist_only}")
        else:
            print(f"  First 10 extra warp IDs: {sorted_hist_only[:10]}")
            print(f"  ... and {len(sorted_hist_only) - 10} more")

    if len(chrome_only) == 0 and len(hist_only) == 0:
        print("‚úì Warp ID validation passed: All warps match between data sources")
        return True
    else:
        print(
            f"‚úó Warp ID validation failed: {len(chrome_only)} Chrome-only + {len(hist_only)} histogram-only warps"
        )
        return False


def merge_traces(
    chrome_trace_path,
    cutracer_hist_path,
    cutracer_log_path,
    output_path,
    kernel_hash_hex=None,
):
    """
    Merges data from Chrome trace, CUTRICER histogram, and CUTRICER log.
    """
    # If kernel_hash_hex is not provided, try to extract it from the histogram file name
    if not kernel_hash_hex:
        hist_filename = os.path.basename(cutracer_hist_path)
        hash_match = re.search(r"kernel_([0-9a-fA-F]+)_", hist_filename)
        if hash_match:
            kernel_hash_hex = hash_match.group(1)
            print(f"Successfully extracted kernel hash: {kernel_hash_hex}")
        else:
            print(
                f"ERROR: Could not extract kernel hash from filename: {hist_filename}"
            )
            print(
                "       Please provide a kernel hash using the --kernel-hash argument or"
            )
            print(
                "       ensure the histogram file name is in the format 'kernel_<hash>_...'."
            )
            return

    print("Parsing log file to get metadata...")
    launch_info = parse_cutracer_log(cutracer_log_path, kernel_hash_hex)
    if not launch_info:
        print(
            f"Could not find launch info for kernel hash {kernel_hash_hex} in log file. Aborting merge."
        )
        return

    grid_size = launch_info["grid_size"]  # (x, y, z)
    block_size = launch_info["block_size"]  # (x, y, z)

    # Calculate total threads per block and warps per block
    threads_per_block = block_size[0] * block_size[1] * block_size[2]
    warp_size = 32  # Standard for NVIDIA GPUs
    warps_per_block = (threads_per_block + warp_size - 1) // warp_size

    if kernel_hash_hex:
        print(
            f"Launch info parsed for kernel hash {kernel_hash_hex}: Grid size = {grid_size}, Block size = {block_size}, Warps per block = {warps_per_block}"
        )
    else:
        print(
            f"Launch info parsed: Grid size = {grid_size}, Block size = {block_size}, Warps per block = {warps_per_block}"
        )

    print("Parsing Chrome trace file...")
    chrome_df = get_chrome_trace_df(chrome_trace_path)
    if chrome_df is None:
        print("Failed to parse Chrome trace. Aborting merge.")
        return

    print("Parsing CUTRICER histogram file...")
    hist_df = get_cutracer_hist_df(cutracer_hist_path)
    if hist_df is None:
        print("Failed to parse CUTRICER histogram. Aborting merge.")
        return

    # Calculate global_warp_id in the chrome trace data
    # For 3D grids, we need to check if cta is already linearized or if we need to linearize it
    # Based on your grid (1, 528, 1), it seems cta is already a linear block ID
    # global_warp_id = block_linear_id * warps_per_block + local_warp_id

    print("Analyzing CTA ID distribution...")
    print(f"CTA ID range: {chrome_df['cta'].min()} to {chrome_df['cta'].max()}")
    total_blocks = grid_size[0] * grid_size[1] * grid_size[2]
    print(f"Expected total blocks from grid size: {total_blocks}")

    # If cta IDs are 1-based and linearized, we need to convert to 0-based
    # If cta IDs start from a different base, we need to adjust accordingly
    chrome_df["global_warp_id"] = (
        chrome_df["cta"] * warps_per_block + chrome_df["local_warp_id"]
    )

    print(
        f"Global warp ID range: {chrome_df['global_warp_id'].min()} to {chrome_df['global_warp_id'].max()}"
    )

    print("Validating warp coverage...")
    is_valid = validate_warp_coverage(chrome_df, hist_df)
    if not is_valid:
        print("WARNING: Proceeding with merge despite warp ID mismatches.")
        print("         This may result in missing data in the output.")

    print("Merging the dataframes...")
    # Merge all regions - create cross join between chrome trace events and histogram regions
    merged_df = pd.merge(chrome_df, hist_df, on="global_warp_id", how="left")

    # Reorder and select columns for clarity
    output_columns = [
        "core",
        "cta",
        "local_warp_id",
        "global_warp_id",
        "region_id",
        "name",
        "category",
        "cycles",
        "timestamp_ns",
        "total_instruction_count",
    ]

    # Add IPC calculation
    print("Calculating IPC (Instructions Per Cycle)...")
    merged_df["ipc"] = merged_df.apply(
        lambda row: calculate_ipc(row["cycles"], row["total_instruction_count"]), axis=1
    )
    output_columns.append("ipc")

    # Filter for columns that actually exist after the merge
    final_columns = [col for col in output_columns if col in merged_df.columns]
    final_df = merged_df[final_columns]

    # Sort by global_warp_id first, then by region_id for better organization
    print("Sorting output by global_warp_id, then region_id...")
    final_df = final_df.sort_values(
        ["global_warp_id", "region_id"], ascending=True
    ).reset_index(drop=True)

    final_df.to_csv(output_path, index=False)
    print(f"Successfully merged data and saved to {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Parse and merge trace files from Chrome's tracer and CUTRICER.",
        formatter_class=argparse.RawTextHelpFormatter,
    )

    parser.add_argument(
        "--chrome-trace",
        dest="chrome_trace_input",
        help="Path to the Chrome trace JSON file.",
    )
    parser.add_argument(
        "--cutracer-trace",
        dest="cutracer_trace_input",
        help="Path to the CUTRICER histogram CSV file.",
    )
    parser.add_argument(
        "--cutracer-log",
        dest="cutracer_log_input",
        help="Path to the CUTRICER log file to enable merge mode.",
    )
    parser.add_argument(
        "--kernel-hash",
        dest="kernel_hash_hex",
        help="Optional kernel hash (e.g., 0x7fa21c3) to select a specific launch from the log.",
    )
    parser.add_argument("--output", required=True, help="Path for the output CSV file.")

    args = parser.parse_args()

    # --- Main Logic ---
    if args.cutracer_log_input:
        # Merge mode
        if not all([args.chrome_trace_input, args.cutracer_trace_input]):
            parser.error("--cutracer-log requires --chrome-trace and --cutracer-trace.")

        merge_traces(
            args.chrome_trace_input,
            args.cutracer_trace_input,
            args.cutracer_log_input,
            args.output,
            args.kernel_hash_hex,
        )
    elif args.chrome_trace_input:
        # Standalone Chrome trace parsing
        df = get_chrome_trace_df(args.chrome_trace_input)
        if df is not None:
            df.to_csv(args.output, index=False)
            print(f"Successfully parsed Chrome trace and saved to {args.output}")
    elif args.cutracer_trace_input:
        # Standalone CUTRICER hist parsing
        df = get_cutracer_hist_df(args.cutracer_trace_input)
        if df is not None:
            df.to_csv(args.output, index=False)
            print(f"Successfully parsed CUTRICER histogram and saved to {args.output}")
    else:
        parser.print_help()

```

`src/analysis.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed under MIT,
 * based on original NVIDIA nvbit sample code licensed under BSD-3-Clause.
 * See LICENSE file in the root directory for Meta's license terms.
 * See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <assert.h>
#include <pthread.h>
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>

#include <map>
#include <string>
#include <unordered_map>
#include <unordered_set>

#include "analysis.h"
#include "common.h"
#include "cuda.h"
#include "env_config.h"
#include "log.h"
#include "utils/channel.hpp"

#define PC_HISTORY_LEN 32
#define LOOP_REPEAT_THRESH 3
// Throttle interval for hang checks (seconds)
#define HANG_CHECK_THROTTLE_SECS 1

extern pthread_mutex_t mutex;
extern std::unordered_map<CUcontext, CTXstate *> ctx_state_map;
extern std::map<uint64_t, std::pair<CUcontext, CUfunction>> kernel_launch_to_func_map;
extern std::map<uint64_t, uint32_t> kernel_launch_to_iter_map;

/**
 * @brief Extracts the full instruction mnemonic from a SASS line.
 *
 * This function parses a SASS instruction string to extract the mnemonic,
 * which includes the base instruction and any dot-separated modifiers
 * (e.g., "IMAD.MOV.U32"). It correctly handles and skips optional
 * predicates (e.g., "@!P0").
 *
 * @param sass_line The full SASS instruction line.
 * @return The extracted instruction mnemonic as a string.
 */
std::string extract_instruction_name(const std::string &sass_line) {
  // SASS format examples:
  // CS2R.32 R7, SR_CLOCKLO ;
  // @!P0 IMAD.MOV.U32 R6, RZ, RZ, 0x800000 ;

  size_t start_pos = 0;

  // Skip whitespace
  while (start_pos < sass_line.length() && isspace(sass_line[start_pos])) {
    start_pos++;
  }

  // Skip predicate if present (starts with @)
  if (start_pos < sass_line.length() && sass_line[start_pos] == '@') {
    // Find the end of predicate part (next space)
    while (start_pos < sass_line.length() && !isspace(sass_line[start_pos])) {
      start_pos++;
    }
    // Skip whitespace after predicate
    while (start_pos < sass_line.length() && isspace(sass_line[start_pos])) {
      start_pos++;
    }
  }

  // Extract instruction name (until first space)
  size_t end_pos = start_pos;
  while (end_pos < sass_line.length() && !isspace(sass_line[end_pos])) {
    end_pos++;
  }

  if (start_pos >= sass_line.length() || end_pos <= start_pos) {
    return "UNKNOWN";
  }

  return sass_line.substr(start_pos, end_pos - start_pos);
}

/**
 * @brief Processes a single instruction packet for histogram analysis.
 *
 * This function is the core of the instruction histogram feature. It uses
 * special "clock" instructions (generated by `pl.scope`) as markers to define
 * regions of interest.
 *
 * The logic operates in a start/stop fashion:
 * - The first clock instruction encountered by a warp starts the collection.
 * - The second clock instruction stops the collection and saves the histogram for
 *   the completed region.
 * - The third starts a new region, the fourth stops it, and so on.
 *
 * @warning This start/stop model does not support nested `pl.scope` blocks.
 * A nested scope will be flattened into a single sequence of start/stop
 * markers, which may lead to unintended region definitions.
 *
 * @param ri Pointer to the received opcode data packet (`opcode_only_t`).
 * @param ctx_state Pointer to the state for the current CUDA context.
 * @param warp_states A map tracking the collection state of each warp.
 * @param completed_histograms A vector where histograms of completed regions are
 * stored.
 */
void process_instruction_histogram(const opcode_only_t *ri, CTXstate *ctx_state,
                                   std::unordered_map<int, WarpState> &warp_states,
                                   std::vector<RegionHistogram> &completed_histograms) {
  // Get current function from kernel launch ID to find the correct SASS maps.
  std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
      kernel_launch_to_func_map.find(ri->kernel_launch_id);
  if (func_iter == kernel_launch_to_func_map.end()) {
    return;  // Unknown kernel, skip histogram processing
  }

  CUfunction current_func = func_iter->second.second;

  // Get clock opcode IDs for this function, which mark region boundaries.
  const std::unordered_set<int> *clock_opcode_ids = nullptr;
  if (ctx_state->clock_opcode_ids.count(current_func)) {
    clock_opcode_ids = &ctx_state->clock_opcode_ids.at(current_func);
  }

  // Get SASS mapping for this function
  const std::map<int, std::string> *sass_map_for_func = nullptr;
  if (ctx_state->id_to_sass_map.count(current_func)) {
    sass_map_for_func = &ctx_state->id_to_sass_map.at(current_func);
  }

  if (!clock_opcode_ids || !sass_map_for_func) {
    return;  // No SASS or clock instruction mapping available for this function.
  }

  int warp_id = ri->warp_id;
  WarpState &current_state = warp_states[warp_id];
  bool is_clock_instruction = clock_opcode_ids->count(ri->opcode_id) > 0;

  // This block implements the start/stop logic for regions.
  if (is_clock_instruction) {
    if (current_state.is_collecting) {
      // This is an "end" clock: the region is complete.
      if (!current_state.histogram.empty()) {
        // Save the completed histogram.
        completed_histograms.push_back({warp_id, current_state.region_counter, current_state.histogram});
        current_state.histogram.clear();
        current_state.region_counter++;
      }
      // Stop collecting until the next "start" clock is found.
      current_state.is_collecting = false;
    } else {
      // This is a "start" clock: begin collecting instructions.
      current_state.is_collecting = true;
    }
  }

  // If collection is active, record the current instruction.
  if (current_state.is_collecting && sass_map_for_func->count(ri->opcode_id)) {
    // Extract the base instruction name from the full SASS string.
    const std::string &sass_line = sass_map_for_func->at(ri->opcode_id);
    std::string instruction_name = extract_instruction_name(sass_line);
    current_state.histogram[instruction_name]++;
  }
}

/**
 * @brief Dumps the collected histograms for a completed kernel launch to a file.
 *
 * This function is called when a kernel boundary is detected (i.e., when a new
 * kernel_launch_id is seen). It collates all histograms from the *previous*
 * kernel launch and triggers the process to write them to a CSV file.
 *
 * @param kernel_launch_id The ID of the kernel launch that has just finished.
 * @param histograms A vector containing all the completed region histograms for
 * that kernel.
 */
void dump_previous_kernel_data(uint64_t kernel_launch_id, const std::vector<RegionHistogram> &histograms) {
  if (histograms.empty()) {
    return;  // Nothing to dump.
  }

  // Find kernel info from global mapping
  if (kernel_launch_to_func_map.find(kernel_launch_id) != kernel_launch_to_func_map.end()) {
    auto [ctx, func] = kernel_launch_to_func_map[kernel_launch_id];
    uint32_t iteration = kernel_launch_to_iter_map[kernel_launch_id];

    // Use existing CSV generation logic.
    dump_histograms_to_csv(ctx, func, iteration, histograms);

    // Clean up mapping tables to free memory for subsequent kernels.
    kernel_launch_to_func_map.erase(kernel_launch_id);
    kernel_launch_to_iter_map.erase(kernel_launch_id);
  }
}

/**
 * @brief Writes a set of histograms to a formatted CSV file.
 *
 * This function handles the file I/O for persisting the analysis results. It
 * creates a uniquely named CSV file for a given kernel launch and writes the
 * histogram data in a structured format.
 *
 * @param ctx The CUDA context of the kernel.
 * @param func The kernel function.
 * @param iteration The iteration number of the kernel launch.
 * @param histograms The histogram data to be written to the file.
 */
void dump_histograms_to_csv(CUcontext ctx, CUfunction func, uint32_t iteration,
                            const std::vector<RegionHistogram> &histograms) {
  if (histograms.empty()) {
    return;  // Nothing to dump.
  }

  std::string basename = generate_kernel_log_basename(ctx, func, iteration);
  std::string csv_filename = basename + "_hist.csv";

  FILE *fp = fopen(csv_filename.c_str(), "w");
  if (!fp) {
    oprintf("ERROR: Could not open histogram file %s\n", csv_filename.c_str());
    return;
  }

  // Header for the CSV file.
  fprintf(fp, "warp_id,region_id,instruction,count\n");
  // Iterate through each completed region and write its histogram data.
  for (const RegionHistogram &region_result : histograms) {
    for (const std::pair<const std::string, int> &pair : region_result.histogram) {
      const std::string &instruction_name = pair.first;
      int count = pair.second;
      fprintf(fp, "%d,%d,\"%s\",%d\n", region_result.warp_id, region_result.region_id, instruction_name.c_str(), count);
    }
  }
  fclose(fp);
  loprintf("Histogram data dumped to %s\n", csv_filename.c_str());
}

/**
 * @brief Extract kernel launch ID from different message types
 *
 * This helper function provides a unified interface to retrieve the kernel_launch_id
 * field from various message structures. It's used for kernel boundary detection
 * to determine when processing transitions from one CUDA kernel to another.
 *
 * @param header Pointer to the message header containing the message type
 * @return The kernel launch ID for the message, or 0 if the message type is unknown
 */
static uint64_t get_kernel_launch_id(const message_header_t *header) {
  switch (header->type) {
    case MSG_TYPE_REG_INFO:
      return ((const reg_info_t *)header)->kernel_launch_id;
    case MSG_TYPE_OPCODE_ONLY:
      return ((const opcode_only_t *)header)->kernel_launch_id;
    case MSG_TYPE_MEM_ACCESS:
      return ((const mem_access_t *)header)->kernel_launch_id;
    default:
      return 0;  // Invalid/unknown message type - no kernel ID available
  }
}

/**
 * @brief Computes a canonical signature for a sequence of PCs to detect loops.
 *
 * This function analyzes the recent history of Program Counters (PCs) for a warp
 * to identify repeating patterns, which indicate a loop. The process involves:
 * 1.  **Period Detection**: It finds the shortest repeating sequence of PCs in
 *     the history buffer. If no repeating pattern is found, it returns 0.
 * 2.  **Canonicalization**: To ensure that the same loop produces the same
 *     signature regardless of the entry point, it finds the lexicographically
 *     smallest rotation of the detected period. For example, `[3,1,2]` becomes
 *     `[1,2,3]`.
 * 3.  **Hashing**: It computes an FNV-1a hash of the canonical sequence, seeded
 *     with the period length, to produce the final signature.
 *
 * @param history A constant reference to the ring buffer of merged trace records.
 * @param ring_size The total size of the ring buffer (must be `PC_HISTORY_LEN`).
 * @param head The index of the oldest element in the ring buffer.
 * @param out_period A reference to a `uint8_t` that will be set to the detected
 *                   period length. If no period is found, it's set to 0.
 * @return A 64-bit canonical signature of the loop, or 0 if no loop is detected.
 */
static uint64_t compute_canonical_signature(const std::vector<TraceRecordMerged> &history, int ring_size, uint8_t head,
                                            uint8_t &out_period) {
  // Reconstruct linear PC sequence in chronological order (oldest -> newest).
  uint64_t pcs[PC_HISTORY_LEN];
  for (int i = 0; i < ring_size; ++i) {
    int idx = (head + i) % ring_size;  // head points to oldest element position
    pcs[i] = history[idx].reg.pc;
  }

  // Detect the shortest repeating period p (1..N-1) such that pcs[i] == pcs[i-p]
  uint8_t period = 0;
  for (uint8_t p = 1; p < ring_size; ++p) {
    bool match = true;
    for (uint8_t i = p; i < ring_size; ++i) {
      if (pcs[i] != pcs[i - p]) {
        match = false;
        break;
      }
    }
    if (match) {
      period = p;
      break;
    }
  }
  if (period == 0) {
    out_period = 0;
    return 0;
  }

  // Find minimal rotation of the period segment to get canonical representation
  // Build candidate of length period from the first period entries
  uint64_t seg[PC_HISTORY_LEN];
  for (uint8_t i = 0; i < period; ++i) seg[i] = pcs[i];
  uint8_t min_rot = 0;
  for (uint8_t r = 1; r < period; ++r) {
    // Compare rotation r with current min_rot lexicographically
    bool smaller = false;
    for (uint8_t i = 0; i < period; ++i) {
      uint64_t a = seg[(i + r) % period];
      uint64_t b = seg[(i + min_rot) % period];
      if (a == b) continue;
      if (a < b) smaller = true;
      break;
    }
    if (smaller) min_rot = r;
  }

  // FNV-1a style hash seeded by period for better distribution
  const uint64_t FNV_OFFSET = 14695981039346656037ULL;
  const uint64_t FNV_PRIME = 1099511628211ULL;
  uint64_t h = FNV_OFFSET ^ period;
  for (uint8_t i = 0; i < period; ++i) {
    uint64_t pcv = seg[(i + min_rot) % period];
    h = (h ^ pcv) * FNV_PRIME;
  }
  out_period = period;
  return h;
}

/**
 * @brief Updates the loop detection state for a given warp.
 *
 * This function is the core of the host-side loop detection logic. It maintains
 * a history of instructions for each warp and uses it to detect when a warp
 * enters a stable loop.
 *
 * The process for each new instruction is as follows:
 * 1.  **History Update**: The new instruction record (`reg_info_t`) is added to
 *     the warp's ring buffer. The function also tries to match it with any
 *     pending memory access records for the same instruction.
 * 2.  **Signature Calculation**: Once the history buffer is full, it calls
 *     `compute_canonical_signature` to get a signature of the current PC sequence.
 * 3.  **Loop State Tracking**:
 *     - If the new signature and period match the previous one, a `repeat_cnt`
 *       is incremented.
 *     - If they don't match, the counter is reset.
 *     - When `repeat_cnt` exceeds `LOOP_REPEAT_THRESH`, the warp is officially
 *       considered to be in a loop (`loop_flag` is set to true), and the loop
 *       body (one full period) is captured and stored.
 *
 * @param ctx_state Pointer to the state for the current CUDA context.
 * @param key The `WarpKey` identifying the warp to be updated.
 * @param ri Pointer to the `reg_info_t` packet for the current instruction.
 */
static void update_loop_state(CTXstate *ctx_state, const WarpKey &key, const reg_info_t *ri) {
  WarpLoopState &state = ctx_state->loop_states[key];
  // One-time buffer allocation per warp
  if (state.history.size() != PC_HISTORY_LEN) {
    state.history.assign(PC_HISTORY_LEN, TraceRecordMerged());
    state.head = 0;
    state.filled = 0;
  }

  // Write the incoming reg record into ring buffer
  TraceRecordMerged &slot = state.history[state.head];
  slot.reg = *ri;
  slot.has_mem = false;  // will be flipped if we find a matching pending mem
  memset(slot.mem_addrs, 0, sizeof(slot.mem_addrs));

  // Try to match any pending mem for this warp (mem may arrive before reg)
  auto &pending = ctx_state->pending_mem_by_warp[key];
  if (!pending.empty()) {
    // Find first matching mem by pc/opcode
    for (auto it = pending.begin(); it != pending.end(); ++it) {
      if (it->pc == ri->pc && it->opcode_id == ri->opcode_id) {
        slot.has_mem = true;
        memcpy(slot.mem_addrs, it->addrs, sizeof(slot.mem_addrs));
        pending.erase(it);
        break;
      }
    }
  }

  // Advance ring pointers
  state.head = (uint8_t)((state.head + 1) % PC_HISTORY_LEN);
  if (state.filled < PC_HISTORY_LEN) state.filled++;

  // Only check for loops once the history buffer is full
  if (state.filled < PC_HISTORY_LEN) {
    return;
  }

  // Compute canonical signature and period from the ring buffer
  uint8_t period = 0;
  uint64_t current_sig = compute_canonical_signature(state.history, PC_HISTORY_LEN, state.head, period);

  if (current_sig != 0 && current_sig == state.last_sig && period == state.last_period) {
    state.repeat_cnt++;
  } else {
    state.repeat_cnt = 1;  // current observed once
    state.loop_flag = false;
  }

  if (state.repeat_cnt > LOOP_REPEAT_THRESH) {
    if (!state.loop_flag) {
      state.loop_flag = true;
      state.first_loop_time = time(nullptr);
      // Capture the loop body records (one period) from the ring buffer in chronological order
      state.current_loop.period = period;
      state.current_loop.instructions.clear();
      state.current_loop.instructions.reserve(period);
      // head points to oldest, so sequence starts from head index
      for (uint8_t i = 0; i < period; ++i) {
        int idx = (state.head + i) % PC_HISTORY_LEN;
        state.current_loop.instructions.push_back(state.history[idx]);
      }
    }
  }
  state.last_sig = current_sig;
  state.last_period = period;
}

/**
 * @brief Clears all state related to deadlock and hang detection.
 *
 * This function is called at the boundary of a new kernel launch to ensure that
 * the state from the previous kernel does not interfere with the analysis of the
 * new one. It clears all maps and sets that track warp activity, loop states,
 * and pending memory operations.
 *
 * @param ctx_state Pointer to the state for the current CUDA context.
 */
static void clear_deadlock_state(CTXstate *ctx_state) {
  ctx_state->loop_states.clear();
  ctx_state->active_warps.clear();
  ctx_state->pending_mem_by_warp.clear();
  ctx_state->last_seen_time_by_warp.clear();
  ctx_state->exit_candidate_since_by_warp.clear();
}

/**
 * @brief Prints detailed status information for all active warps including loop states.
 *
 * This function provides a comprehensive view of each warp's current state including:
 * - Basic warp identification (CTA coordinates, warp ID)
 * - Loop detection status (whether in loop, loop period, repeat count)
 * - Activity timestamps and exit candidate status
 * - Loop body instruction details if available
 *
 * @param ctx_state Pointer to the state for the current CUDA context
 * @param current_kernel_launch_id The current kernel launch ID for context
 */
static void print_warp_status_summary(CTXstate *ctx_state, uint64_t current_kernel_launch_id) {
  if (ctx_state->active_warps.empty()) {
    loprintf("==> WARP STATUS: No active warps for kernel_launch_id=%lu\n", current_kernel_launch_id);
    return;
  }

  time_t now = time(nullptr);
  loprintf("==> WARP STATUS SUMMARY for kernel_launch_id=%lu (%zu active warps):\n", current_kernel_launch_id,
           ctx_state->active_warps.size());
  loprintf("    Format: WarpID[CTA_x,y,z] - LoopStatus - Activity\n");
  loprintf("    -----------------------------------------------------------------------\n");

  // Resolve SASS map for the current function, if available
  const std::map<int, std::string> *sass_map_for_func = nullptr;
  {
    std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
        kernel_launch_to_func_map.find(current_kernel_launch_id);
    if (func_iter != kernel_launch_to_func_map.end()) {
      CUfunction f_func = func_iter->second.second;
      if (ctx_state->id_to_sass_map.count(f_func)) {
        sass_map_for_func = &ctx_state->id_to_sass_map[f_func];
      }
    }
  }

  for (const auto &warp_key : ctx_state->active_warps) {
    // Basic warp info
    loprintf("    Warp%d[%d,%d,%d]: ", warp_key.warp_id, warp_key.cta_id_x, warp_key.cta_id_y, warp_key.cta_id_z);

    // Loop state info
    auto loop_iter = ctx_state->loop_states.find(warp_key);
    if (loop_iter != ctx_state->loop_states.end()) {
      const WarpLoopState &loop_state = loop_iter->second;
      if (loop_state.loop_flag) {
        time_t loop_duration = now - loop_state.first_loop_time;
        loprintf("LOOPING(period=%d, repeat=%d, %lds) ", loop_state.last_period, loop_state.repeat_cnt, loop_duration);
      } else {
        loprintf("NO_LOOP(period=%d, repeat=%d) ", loop_state.last_period, loop_state.repeat_cnt);
      }
    } else {
      loprintf("NO_STATE ");
    }

    // Activity info
    auto seen_iter = ctx_state->last_seen_time_by_warp.find(warp_key);
    if (seen_iter != ctx_state->last_seen_time_by_warp.end()) {
      time_t inactive_duration = now - seen_iter->second;
      loprintf("- Last_seen=%lds_ago ", inactive_duration);
    } else {
      loprintf("- Last_seen=UNKNOWN ");
    }

    // Exit candidate status
    auto exit_iter = ctx_state->exit_candidate_since_by_warp.find(warp_key);
    if (exit_iter != ctx_state->exit_candidate_since_by_warp.end()) {
      time_t exit_duration = now - exit_iter->second;
      loprintf("- EXIT_CANDIDATE(%lds)", exit_duration);
    }

    loprintf("\n");

    // Print loop body details if warp is in a confirmed loop
    if (loop_iter != ctx_state->loop_states.end() && loop_iter->second.loop_flag) {
      const WarpLoopState &loop_state = loop_iter->second;
      if (!loop_state.current_loop.instructions.empty()) {
        loprintf("      Loop Body (%d instructions):\n", loop_state.current_loop.period);
        // Determine index field width based on period digit count for aligned indices
        int index_width = 1;
        {
          int tmp_period = loop_state.current_loop.period;
          while (tmp_period >= 10) {
            index_width++;
            tmp_period /= 10;
          }
          if (index_width < 1) index_width = 1;
        }
        // Pre-compute alignment for PC/Offset columns across the loop body
        int pc_dec_width = 1;
        int hex_nibbles_max = 4;
        {
          size_t upper = std::min(static_cast<size_t>(loop_state.current_loop.period),
                                  loop_state.current_loop.instructions.size());
          for (size_t j = 0; j < upper; ++j) {
            uint64_t pc = loop_state.current_loop.instructions[j].reg.pc;
            // decimal width
            int dec_w = 1;
            uint64_t td = pc;
            while (td >= 10) {
              td /= 10;
              dec_w++;
            }
            if (dec_w > pc_dec_width) pc_dec_width = dec_w;
            // hex width in nibbles
            int nibbles = 1;
            if (pc != 0) {
              nibbles = 0;
              uint64_t th = pc;
              while (th) {
                th >>= 4;
                nibbles++;
              }
            }
            if (nibbles > hex_nibbles_max) hex_nibbles_max = nibbles;
          }
          // Round hex width up to multiple of 4, clamp to [4,16]
          hex_nibbles_max = ((hex_nibbles_max + 3) / 4) * 4;
          if (hex_nibbles_max < 4) hex_nibbles_max = 4;
          if (hex_nibbles_max > 16) hex_nibbles_max = 16;
        }
        for (size_t i = 0;
             i < static_cast<size_t>(loop_state.current_loop.period) && i < loop_state.current_loop.instructions.size();
             ++i) {
          const auto &instr = loop_state.current_loop.instructions[i];
          uint64_t pc_val = instr.reg.pc;

          const char *sass_cstr = "UNKNOWN";
          if (sass_map_for_func && sass_map_for_func->count(instr.reg.opcode_id)) {
            sass_cstr = sass_map_for_func->at(instr.reg.opcode_id).c_str();
          }

          // Desired format with aligned columns: "[ 4] PC  128; Offset  128 /*0x0080*/;  <SASS>"
          loprintf("        [%*zu] PC %*lu; Offset %*lu /*0x%0*lx*/;  %s", index_width, i, pc_dec_width,
                   (unsigned long)pc_val, pc_dec_width, (unsigned long)pc_val, hex_nibbles_max, (unsigned long)pc_val,
                   sass_cstr);
          if (instr.has_mem) {
            loprintf(" (has_mem)");
          }
          loprintf("\n");
        }
      }
    }
  }
  loprintf("    -----------------------------------------------------------------------\n");
}

// Checks for potential kernel hangs by determining if all active warps are stuck in loops.
// The check is throttled to run at most once every HANG_CHECK_THROTTLE_SECS.
// If all warps are looping and the condition persists for several checks, it terminates the process.
// Before checking, it prunes warps that are candidates for exiting and have been inactive.
static void check_kernel_hang(CTXstate *ctx_state, uint64_t current_kernel_launch_id) {
  time_t now = time(nullptr);
  if (now - ctx_state->last_hang_check_time < HANG_CHECK_THROTTLE_SECS) {  // Throttle to configured interval
    return;
  }
  ctx_state->last_hang_check_time = now;

  if (ctx_state->active_warps.empty()) {
    return;
  }

  // Cleanup exit-candidate warps before evaluating loop state
  std::vector<WarpKey> to_remove;
  to_remove.reserve(ctx_state->active_warps.size());
  for (const WarpKey &key : ctx_state->active_warps) {
    auto itExit = ctx_state->exit_candidate_since_by_warp.find(key);
    if (itExit == ctx_state->exit_candidate_since_by_warp.end()) continue;
    time_t exit_since = itExit->second;
    time_t last_seen = 0;
    auto itSeen = ctx_state->last_seen_time_by_warp.find(key);
    if (itSeen != ctx_state->last_seen_time_by_warp.end()) last_seen = itSeen->second;
    // Remove only if no activity since EXIT and at least one throttle interval has passed
    if (last_seen <= exit_since && (now - exit_since) >= HANG_CHECK_THROTTLE_SECS) {
      to_remove.push_back(key);
    }
  }
  if (!to_remove.empty()) {
    for (const WarpKey &key : to_remove) {
      ctx_state->active_warps.erase(key);
      ctx_state->loop_states.erase(key);
      ctx_state->pending_mem_by_warp.erase(key);
      ctx_state->last_seen_time_by_warp.erase(key);
      ctx_state->exit_candidate_since_by_warp.erase(key);
    }
  }

  bool all_warps_in_loop = true;
  for (const auto &warp_key : ctx_state->active_warps) {
    if (ctx_state->loop_states.find(warp_key) == ctx_state->loop_states.end() ||
        !ctx_state->loop_states.at(warp_key).loop_flag) {
      all_warps_in_loop = false;
      break;
    }
  }

  if (all_warps_in_loop) {
    time_t hang_time = now - ctx_state->loop_states.begin()->second.first_loop_time;
    loprintf("Possible kernel hang: launch_id=%lu ‚Äî all %zu active warps have been looping for %ld seconds.\n",
             current_kernel_launch_id, ctx_state->active_warps.size(), hang_time);
    print_warp_status_summary(ctx_state, current_kernel_launch_id);
    // Deadlock sustained handling: count consecutive hits and terminate after threshold
    if (!ctx_state->deadlock_termination_initiated) {
      ctx_state->deadlock_consecutive_hits++;
      if (ctx_state->deadlock_consecutive_hits >= 3) {
        ctx_state->deadlock_termination_initiated = true;
        loprintf("Deadlock sustained for %d checks; sending SIGTERM.\n", ctx_state->deadlock_consecutive_hits);
        fflush(stdout);
        fflush(stderr);
        raise(SIGTERM);
        // Grace period; if not terminated externally, force kill
        sleep(2);
        loprintf("Process still alive after SIGTERM; sending SIGKILL.\n");
        raise(SIGKILL);
      }
    }
    // Optional: Add detailed printout of loop info here
  } else {
    // Reset hit counter if condition is not sustained
    ctx_state->deadlock_consecutive_hits = 0;
  }
}

/**
 * @brief The main thread function for receiving and processing data from the
 * GPU.
 *
 * This function is based on the `recv_thread_fun` from NVIDIA's `mem_trace`
 * example. It runs in a separate CPU thread for each CUDA context, continuously
 * receiving data packets from the GPU channel and processing them.
 *
 * Meta's enhancements transform this from a simple single-purpose function to a
 * versatile multi-analysis pipeline:
 *  - **Generic Message-Passing System**: The original function only handled one
 *    data type (`mem_access_t`). This version uses a `message_header_t` to
 *    identify different packet types (`reg_info_t`, `opcode_only_t`, etc.) and
 *    dispatch them to the appropriate analysis logic.
 *  - **Instruction Histogram Analysis**: It contains the complete host-side logic
 *    for the `PROTON_INSTR_HISTOGRAM` feature, including state management for
 *    each warp (`warp_states`) and tracking completed regions.
 *  - **Kernel Boundary Detection**: It introduces robust state management across
 *    kernel launches by tracking `kernel_launch_id`. This allows it to detect
 *    when a kernel has finished, ensuring that all pending data for that kernel
 *    is finalized and dumped before processing the next one.
 *  - **SASS String Enrichment**: For richer logging, it looks up the SASS string
 *    for a given `opcode_id` to provide more context in the trace output.
 *
 * @param args A void pointer to the `CUcontext` for which this thread is
 * launched.
 * @return void* Always returns NULL.
 */
void *recv_thread_fun(void *args) {
  CUcontext ctx = (CUcontext)args;

  pthread_mutex_lock(&mutex);
  /* get context state from map */
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate *ctx_state = ctx_state_map[ctx];

  ChannelHost *ch_host = &ctx_state->channel_host;
  pthread_mutex_unlock(&mutex);
  char *recv_buffer = (char *)malloc(CHANNEL_SIZE);

  // Per-thread, per-context state for histogram analysis.
  std::unordered_map<int, WarpState> warp_states;
  std::vector<RegionHistogram> local_completed_histograms;

  // Used to detect when a new kernel begins.
  uint64_t last_seen_kernel_launch_id = UINT64_MAX;  // Initial invalid value

  while (ctx_state->recv_thread_done == RecvThreadState::WORKING) {
    uint32_t num_recv_bytes = ch_host->recv(recv_buffer, CHANNEL_SIZE);

    if (num_recv_bytes > 0) {
      // Process data packets in this chunk

      uint32_t num_processed_bytes = 0;
      while (num_processed_bytes < num_recv_bytes) {
        // First read the message header to determine the message type
        message_header_t *header = (message_header_t *)&recv_buffer[num_processed_bytes];
        const char *sass_str = "N/A";

        uint64_t current_launch_id = get_kernel_launch_id(header);
        bool is_new_kernel = false;
        if (current_launch_id != 0 && current_launch_id != last_seen_kernel_launch_id) {
          is_new_kernel = true;
          if (last_seen_kernel_launch_id != UINT64_MAX) {
            // Cleanup for the previous kernel
            if (is_analysis_type_enabled(AnalysisType::PROTON_INSTR_HISTOGRAM)) {
              // Dump any remaining histograms for warps that were collecting
              for (auto &pair : warp_states) {
                if (pair.second.is_collecting && !pair.second.histogram.empty()) {
                  local_completed_histograms.push_back({pair.first, pair.second.region_counter, pair.second.histogram});
                }
              }
              dump_previous_kernel_data(last_seen_kernel_launch_id, local_completed_histograms);
              local_completed_histograms.clear();
              warp_states.clear();
            }
            if (is_analysis_type_enabled(AnalysisType::DEADLOCK_DETECTION)) {
              clear_deadlock_state(ctx_state);
            }
          }
          last_seen_kernel_launch_id = current_launch_id;
        }

        if (header->type == MSG_TYPE_REG_INFO) {
          reg_info_t *ri = (reg_info_t *)&recv_buffer[num_processed_bytes];

          if (is_analysis_type_enabled(AnalysisType::DEADLOCK_DETECTION)) {
            WarpKey key = {ri->cta_id_x, ri->cta_id_y, ri->cta_id_z, ri->warp_id};
            if (is_new_kernel) {
              ctx_state->last_hang_check_time = time(nullptr);
            }
            ctx_state->active_warps.insert(key);
            // Update last seen time for this warp
            ctx_state->last_seen_time_by_warp[key] = time(nullptr);
            update_loop_state(ctx_state, key, ri);

            // Mark EXIT candidate if this opcode_id is an EXIT for the current function
            auto func_iter2 = kernel_launch_to_func_map.find(ri->kernel_launch_id);
            if (func_iter2 != kernel_launch_to_func_map.end()) {
              CUfunction f_func2 = func_iter2->second.second;
              if (ctx_state->exit_opcode_ids.count(f_func2) &&
                  ctx_state->exit_opcode_ids[f_func2].count(ri->opcode_id)) {
                if (!ctx_state->exit_candidate_since_by_warp.count(key)) {
                  ctx_state->exit_candidate_since_by_warp[key] = time(nullptr);
                }
              }
            }
          }
          // Get SASS string for trace output
          auto func_iter = kernel_launch_to_func_map.find(ri->kernel_launch_id);
          if (func_iter != kernel_launch_to_func_map.end()) {
            auto [f_ctx, f_func] = func_iter->second;
            if (ctx_state->id_to_sass_map.count(f_func) && ctx_state->id_to_sass_map[f_func].count(ri->opcode_id)) {
              sass_str = ctx_state->id_to_sass_map[f_func][ri->opcode_id].c_str();
            }
          }

          trace_lprintf("CTX %p - CTA %d,%d,%d - warp %d - %s:\n", ctx, ri->cta_id_x, ri->cta_id_y, ri->cta_id_z,
                        ri->warp_id, sass_str);

          // Print register values
          for (int reg_idx = 0; reg_idx < ri->num_regs; reg_idx++) {
            trace_lprintf("  * ");
            for (int i = 0; i < 32; i++) {
              trace_lprintf("Reg%d_T%02d: 0x%08x ", reg_idx, i, ri->reg_vals[i][reg_idx]);
            }
            trace_lprintf("\n");
          }
          trace_lprintf("\n");
          num_processed_bytes += sizeof(reg_info_t);

        } else if (header->type == MSG_TYPE_OPCODE_ONLY) {
          if (is_analysis_type_enabled(AnalysisType::PROTON_INSTR_HISTOGRAM)) {
            opcode_only_t *oi = (opcode_only_t *)&recv_buffer[num_processed_bytes];

            process_instruction_histogram(oi, ctx_state, warp_states, local_completed_histograms);
          }
          num_processed_bytes += sizeof(opcode_only_t);

        } else if (header->type == MSG_TYPE_MEM_ACCESS) {
          mem_access_t *mem = (mem_access_t *)&recv_buffer[num_processed_bytes];

          // Get SASS string for trace output.
          std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
              kernel_launch_to_func_map.find(mem->kernel_launch_id);
          if (func_iter != kernel_launch_to_func_map.end()) {
            std::pair<CUcontext, CUfunction> kernel_info = func_iter->second;
            CUfunction f_func = kernel_info.second;
            if (ctx_state->id_to_sass_map.count(f_func) && ctx_state->id_to_sass_map[f_func].count(mem->opcode_id)) {
              sass_str = ctx_state->id_to_sass_map[f_func][mem->opcode_id].c_str();
            }
          }

          trace_lprintf(
              "CTX %p - kernel_launch_id %ld - CTA %d,%d,%d - warp %d - PC %ld - "
              "%s:\n",
              ctx, mem->kernel_launch_id, mem->cta_id_x, mem->cta_id_y, mem->cta_id_z, mem->warp_id, mem->pc, sass_str);
          trace_lprintf("  Memory Addresses:\n  * ");
          int printed = 0;
          for (int i = 0; i < 32; i++) {
            if (mem->addrs[i] != 0) {  // Only print non-zero addresses
              trace_lprintf("T%02d: 0x%016lx ", i, mem->addrs[i]);
              printed++;
              if (printed % 4 == 0 && i < 31) {
                trace_lprintf("\n    ");
              }
            }
          }
          trace_lprintf("\n\n");
          num_processed_bytes += sizeof(mem_access_t);
        } else {
          // Unknown message type, print error and break loop
          // TODO: handle error message in our current log mechanism
          fprintf(stderr,
                  "ERROR: Unknown message type %d received in recv_thread_fun. "
                  "Stopping processing of this chunk.\n",
                  header->type);
          continue;
        }
      }
    }

    if (is_analysis_type_enabled(AnalysisType::DEADLOCK_DETECTION) && last_seen_kernel_launch_id != UINT64_MAX) {
      check_kernel_hang(ctx_state, last_seen_kernel_launch_id);
    }
  }

  // Dump data for the very last kernel if it exists.
  if (last_seen_kernel_launch_id != UINT64_MAX) {
    // Dump any remaining histograms for warps that were still collecting.
    for (const std::pair<const int, WarpState> &pair : warp_states) {
      if (pair.second.is_collecting && !pair.second.histogram.empty()) {
        local_completed_histograms.push_back({pair.first, pair.second.region_counter, pair.second.histogram});
      }
    }
    if (!local_completed_histograms.empty()) {
      dump_previous_kernel_data(last_seen_kernel_launch_id, local_completed_histograms);
    }
  }

  free(recv_buffer);
  ctx_state->recv_thread_done = RecvThreadState::FINISHED;
  return NULL;
}

```

`src/cutracer.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed
 * under MIT, based on original NVIDIA NVBit sample code licensed under
 * BSD-3-Clause. See LICENSE file in the root directory for Meta's license
 * terms. See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <assert.h>
#include <pthread.h>
#include <stdio.h>
#include <unistd.h>

#include <map>
#include <string>
#include <unordered_map>
#include <unordered_set>

/* every tool needs to include this once */
#include "nvbit_tool.h"

/* nvbit interface file */
#include "nvbit.h"

/* contains definition of the reg_info_t and mem_access_t structure */
#include "common.h"

/* analysis functionality */
#include "analysis.h"

/* instrumentation functionality */
#include "instrument.h"

/* env config */
#include "env_config.h"

/* logging functionality */
#include "log.h"

/* Channel used to communicate from GPU to CPU receiving thread */
#define CHANNEL_SIZE (1l << 20)

#define CUDA_CHECK_LAST_ERROR()                                                                       \
  do {                                                                                                \
    cudaError_t err = cudaGetLastError();                                                             \
    if (err != cudaSuccess) {                                                                         \
      loprintf("FATAL: CUDA Last Error: %s at %s:%d\n", cudaGetErrorString(err), __FILE__, __LINE__); \
      fflush(stderr);                                                                                 \
      assert(err == cudaSuccess);                                                                     \
    }                                                                                                 \
  } while (0)

/* lock */
pthread_mutex_t mutex;
pthread_mutex_t cuda_event_mutex;

/* map to store context state */
std::unordered_map<CUcontext, CTXstate *> ctx_state_map;

/* skip flag used to avoid re-entry on the nvbit_callback when issuing
 * flush_channel kernel call */
bool skip_callback_flag = false;

/* The kernel launch is identified by a global id */
uint64_t global_kernel_launch_id = 0;

// Global mapping tables for kernel launch tracking
std::map<uint64_t, std::pair<CUcontext, CUfunction>> kernel_launch_to_func_map;
std::map<uint64_t, uint32_t> kernel_launch_to_iter_map;

// map to store the iteration count for each kernel
static std::map<CUfunction, uint32_t> kernel_iter_map;

/* ===== Main Functionality ===== */
/**
 * @brief Conditionally instruments a CUDA function by delegating to specialized
 * instrumentation functions.
 *
 * This function is based on the `instrument_kernel` function from NVIDIA's
 * `mem_trace` example. It inspects a function and, if it matches the filters,
 * iterates through its SASS instructions, delegating the actual instrumentation
 * to functions in `instrument.cu`.
 *
 * Meta's enhancements include:
 *  - Combining memory (MREF) and register (REG) tracing.
 *  - Adding support for Unified Registers (UREG).
 *  - Kernel Filtering: A system (`kernel_filters`) to selectively
 *    instrument kernels based on environment variables, avoiding unnecessary
 *    overhead.
 *  - Modular Instrumentation API: The core instrumentation logic is refactored
 *    into `instrument.cu`, providing a clear API for different tracing types.
 *
 * @param ctx The CUDA context of the function.
 * @param func The `CUfunction` to inspect and potentially instrument.
 * @return `true` if any related function was matched by the filters, `false` otherwise.
 */
bool instrument_function_if_needed(CUcontext ctx, CUfunction func) {
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate *ctx_state = ctx_state_map[ctx];

  /* Get related functions of the kernel (device function that can be
   * called by the kernel) */
  std::vector<CUfunction> related_functions = nvbit_get_related_functions(ctx, func);

  /* add kernel itself to the related function vector */
  related_functions.push_back(func);

  bool any_related_function_matched = kernel_filters.empty();
  /* iterate on function */
  for (auto f : related_functions) {
    // Get function name (both mangled and unmangled versions)
    const char *unmangled_name = nvbit_get_func_name(ctx, f, false);
    const char *mangled_name = nvbit_get_func_name(ctx, f, true);

    // Check if function name contains any of the patterns
    bool should_instrument = true;  // Default to true if no filters specified

    if (!kernel_filters.empty()) {
      should_instrument = false;  // Start with false when we have filters
      for (const auto &filter : kernel_filters) {
        if ((unmangled_name && strstr(unmangled_name, filter.c_str()) != NULL) ||
            (mangled_name && strstr(mangled_name, filter.c_str()) != NULL)) {
          should_instrument = true;
          any_related_function_matched = true;  // Mark that at least one kernel matched
          if (verbose) {
            loprintf("Found matching kernel for filter '%s': %s (mangled: %s)\n", filter.c_str(),
                     unmangled_name ? unmangled_name : "unknown", mangled_name ? mangled_name : "unknown");
          }
          break;
        }
      }
    } else if (verbose) {
      loprintf("Instrumenting kernel: %s (mangled: %s)\n", unmangled_name ? unmangled_name : "unknown",
               mangled_name ? mangled_name : "unknown");
    }

    const std::vector<Instr *> &instrs = nvbit_get_instrs(ctx, f);
    if (verbose) {
      loprintf("Inspecting kernel %s at address 0x%lx\n", nvbit_get_func_name(ctx, f), nvbit_get_func_addr(ctx, f));
    }

    if (!should_instrument) {
      continue;
    }

    uint32_t cnt = 0;
    /* iterate on all the static instructions in the function */
    for (auto instr : instrs) {
      if (cnt < instr_begin_interval || cnt >= instr_end_interval) {
        cnt++;
        continue;
      }
      if (verbose) {
        instr->printDecoded();
      }

      std::vector<int> reg_num_list;
      std::vector<int> ureg_num_list;
      int mref_idx = 0;
      int opcode_id = instr->getIdx();
      ctx_state->id_to_sass_map[f][opcode_id] = std::string(instr->getSass());
      /* iterate on the operands */
      for (int i = 0; i < instr->getNumOperands(); i++) {
        /* get the operand "i" */
        const InstrType::operand_t *op = instr->getOperand(i);
        if (op->type == InstrType::OperandType::REG) {
          for (int reg_idx = 0; reg_idx < instr->getSize() / 4; reg_idx++) {
            reg_num_list.push_back(op->u.reg.num + reg_idx);
          }
        } else if (op->type == InstrType::OperandType::UREG) {
          for (int reg_idx = 0; reg_idx < instr->getSize() / 4; reg_idx++) {
            ureg_num_list.push_back(op->u.reg.num + reg_idx);
          }
        } else if (op->type == InstrType::OperandType::MREF) {
          // TODO: double check this with NVIDIA people
          if (op->u.mref.has_desc) {
            ureg_num_list.push_back(op->u.mref.desc_ureg_num);
            ureg_num_list.push_back(op->u.mref.desc_ureg_num + 1);
          }

          // Use new instrumentation interface for memory tracing
          if (is_instrument_type_enabled(InstrumentType::MEM_TRACE)) {
            instrument_memory_trace(instr, opcode_id, ctx_state, mref_idx);
          }
          mref_idx++;
        }
      }

      // Choose instrumentation based on enabled types.
      if (is_instrument_type_enabled(InstrumentType::OPCODE_ONLY)) {
        // Lightweight instrumentation for instruction histogram analysis.
        // This sends minimal data (opcode_id, warp_id) to the CPU, reducing
        // overhead.
        instrument_opcode_only(instr, opcode_id, ctx_state);
      } else if (is_instrument_type_enabled(InstrumentType::REG_TRACE)) {
        // Full register tracing.
        instrument_register_trace(instr, opcode_id, ctx_state, reg_num_list, ureg_num_list);
      }
    }

    // Statically identify special instructions for this function:
    // - "clock" (CS2R SR_CLOCKLO) used for histogram region boundaries
    // - "EXIT" used as a candidate signal for warp completion on host side
    if (should_instrument) {
      for (std::map<int, std::string>::const_iterator it_sass = ctx_state->id_to_sass_map[f].begin();
           it_sass != ctx_state->id_to_sass_map[f].end(); ++it_sass) {
        const char *sass_cstr = it_sass->second.c_str();
        if (strstr(sass_cstr, "CS2R") && strstr(sass_cstr, "SR_CLOCKLO")) {
          ctx_state->clock_opcode_ids[f].insert(it_sass->first);
        }
        // Simple substring detection for EXIT mnemonic (predication handled by extract on host side)
        if (strstr(sass_cstr, "EXIT")) {
          ctx_state->exit_opcode_ids[f].insert(it_sass->first);
        }
      }
    }
    /* ============================================ */
  }
  return any_related_function_matched;
}

// Reference code from NVIDIA nvbit mem_trace tool
/* flush channel */
__global__ void flush_channel(ChannelDev *ch_dev) {
  ch_dev->flush();
}

// Reference code from NVIDIA nvbit mem_trace tool
void init_context_state(CUcontext ctx) {
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate *ctx_state = ctx_state_map[ctx];
  ctx_state->recv_thread_done = RecvThreadState::WORKING;
  cudaMallocManaged(&ctx_state->channel_dev, sizeof(ChannelDev));
  ctx_state->channel_host.init((int)ctx_state_map.size() - 1, CHANNEL_SIZE, ctx_state->channel_dev, recv_thread_fun,
                               ctx);
  nvbit_set_tool_pthread(ctx_state->channel_host.get_thread());
}

// Reference code from NVIDIA nvbit mem_trace tool
/**
 * @brief Prepares for a kernel launch, conditionally instrumenting and logging.
 *
 * Based on `enter_kernel_launch` from NVIDIA's `mem_trace` example, this
 * function is called just before a kernel launch. It determines if a kernel
 * should be instrumented based on active filters.
 *
 * Key modifications by Meta include making instrumentation and log file creation
 * conditional on the filtering result. This prevents empty log files for
 * skipped kernels.
 *
 * @param ctx The current CUDA context.
 * @param func The kernel function being launched.
 * @param kernel_launch_id A reference to the global kernel launch counter.
 * @param cbid The NVBit callback ID for the CUDA API call.
 * @param params A pointer to the parameters of the CUDA launch call.
 * @param stream_capture True if the launch is part of a CUDA stream capture.
 * @param build_graph True if the launch is part of a manual graph build.
 * @return `true` if the kernel was instrumented, `false` otherwise.
 */
static bool enter_kernel_launch(CUcontext ctx, CUfunction func, uint64_t &kernel_launch_id, nvbit_api_cuda_t cbid,
                                void *params, bool stream_capture = false, bool build_graph = false) {
  CTXstate *ctx_state = ctx_state_map[ctx];
  // no need to sync during stream capture or manual graph build, since no
  // kernel is actually launched.
  if (!stream_capture && !build_graph) {
    /* Make sure GPU is idle */
    cudaDeviceSynchronize();
    CUDA_CHECK_LAST_ERROR();
  }

  bool should_instrument = instrument_function_if_needed(ctx, func);

  int nregs = 0;
  CUDA_SAFECALL(cuFuncGetAttribute(&nregs, CU_FUNC_ATTRIBUTE_NUM_REGS, func));

  int shmem_static_nbytes = 0;
  CUDA_SAFECALL(cuFuncGetAttribute(&shmem_static_nbytes, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, func));

  /* get function name and pc */
  const char *func_name = nvbit_get_func_name(ctx, func);
  uint64_t pc = nvbit_get_func_addr(ctx, func);
  std::string kernel_hash_hex = compute_kernel_name_hash_hex(ctx, func);

  // during stream capture or manual graph build, no kernel is launched, so
  // do not set launch argument, do not print kernel info, do not increase
  // grid_launch_id. All these should be done at graph node launch time.
  if (!stream_capture && !build_graph) {
    /* set kernel launch id at launch time */
    nvbit_set_at_launch(ctx, func, (uint64_t)kernel_launch_id);

    if (cbid == API_CUDA_cuLaunchKernelEx_ptsz || cbid == API_CUDA_cuLaunchKernelEx) {
      cuLaunchKernelEx_params *p = (cuLaunchKernelEx_params *)params;
      loprintf(
          "CUTracer: CTX 0x%016lx - LAUNCH - Kernel pc 0x%016lx - "
          "Kernel name %s - kernel hash 0x%s - kernel launch id %ld - grid size %d,%d,%d "
          "- block size %d,%d,%d - nregs %d - shmem %d - cuda stream "
          "id %ld\n",
          (uint64_t)ctx, pc, func_name, kernel_hash_hex.c_str(), kernel_launch_id, p->config->gridDimX,
          p->config->gridDimY, p->config->gridDimZ, p->config->blockDimX, p->config->blockDimY, p->config->blockDimZ,
          nregs, shmem_static_nbytes + p->config->sharedMemBytes, (uint64_t)p->config->hStream);
    } else {
      cuLaunchKernel_params *p = (cuLaunchKernel_params *)params;
      loprintf(
          "CUTracer: CTX 0x%016lx - LAUNCH - Kernel pc 0x%016lx - "
          "Kernel name %s - kernel hash 0x%s - kernel launch id %ld - grid size %d,%d,%d "
          "- block size %d,%d,%d - nregs %d - shmem %d - cuda stream "
          "id %ld\n",
          (uint64_t)ctx, pc, func_name, kernel_hash_hex.c_str(), kernel_launch_id, p->gridDimX, p->gridDimY,
          p->gridDimZ, p->blockDimX, p->blockDimY, p->blockDimZ, nregs, shmem_static_nbytes + p->sharedMemBytes,
          (uint64_t)p->hStream);
    }

    // For histogram analysis, we need to map the kernel launch ID back to its
    // function and context to access the correct SASS and clock opcode maps.
    // We also track the iteration count for unique file naming.
    uint32_t current_iter = kernel_iter_map[func];
    kernel_launch_to_func_map[kernel_launch_id] = {ctx, func};
    kernel_launch_to_iter_map[kernel_launch_id] = current_iter;

    // increment kernel launch id for next launch
    // kernel id can be changed here, since nvbit_set_at_launch() has copied
    // its value above.
    kernel_launch_id++;
  }

  // This open should be done before enabling instrumented code to run,
  // otherwise the log file will not be created.
  if (should_instrument) {
    log_open_kernel_file(ctx, func, kernel_iter_map[func]++);
  }
  /* enable instrumented code to run */
  nvbit_enable_instrumented(ctx, func, should_instrument);
  return should_instrument;
}

// the function is only called for non cuda graph launch cases.
static void leave_kernel_launch(CTXstate *ctx_state, uint64_t &grid_launch_id) {
  // make sure user kernel finishes to avoid deadlock
  cudaDeviceSynchronize();
  /* push a flush channel kernel */
  flush_channel<<<1, 1>>>(ctx_state->channel_dev);

  /* Make sure GPU is idle */
  cudaDeviceSynchronize();
  CUDA_CHECK_LAST_ERROR();
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_cuda_event(CUcontext ctx, int is_exit, nvbit_api_cuda_t cbid, const char *name, void *params,
                         CUresult *pStatus) {
  pthread_mutex_lock(&cuda_event_mutex);

  /* we prevent re-entry on this callback when issuing CUDA functions inside
   * this function */
  if (skip_callback_flag) {
    pthread_mutex_unlock(&cuda_event_mutex);
    return;
  }
  skip_callback_flag = true;

  CTXstate *ctx_state = ctx_state_map[ctx];

  switch (cbid) {
    // Identify all the possible CUDA launch events without stream
    // parameters, they will not get involved with cuda graph
    case API_CUDA_cuLaunch:
    case API_CUDA_cuLaunchGrid: {
      cuLaunch_params *p = (cuLaunch_params *)params;
      CUfunction func = p->f;
      if (!is_exit) {
        enter_kernel_launch(ctx, func, global_kernel_launch_id, cbid, params);
      } else {
        leave_kernel_launch(ctx_state, global_kernel_launch_id);
      }
    } break;
    // To support kernel launched by cuda graph (in addition to existing kernel
    // launche method), we need to do:
    //
    // 1. instrument kernels at cudaGraphAddKernelNode event. This is for cases
    // that kernels are manually added to a cuda graph.
    // 2. distinguish captured kernels when kernels are recorded to a graph
    // using stream capture. cudaStreamIsCapturing() tells us whether a stream
    // is capturiong.
    // 3. per-kernel instruction counters, since cuda graph can launch multiple
    // kernels at the same time.
    //
    // Three cases:
    //
    // 1. original kernel launch:
    //     1a. for any kernel launch without using a stream, we instrument it
    //     before it is launched, call cudaDeviceSynchronize after it is
    //     launched and read the instruction counter of the kernel.
    //     1b. for any kernel launch using a stream, but the stream is not
    //     capturing, we do the same thing as 1a.
    //
    //  2. cuda graph using stream capturing: if a kernel is launched in a
    //  stream and the stream is capturing. We instrument the kernel before it
    //  is launched and do nothing after it is launched, because the kernel is
    //  not running until cudaGraphLaunch. Instead, we issue a
    //  cudaStreamSynchronize after cudaGraphLaunch is done and reset the
    //  instruction counters, since a cloned graph might be launched afterwards.
    //
    //  3. cuda graph manual: we instrument the kernel added by
    //  cudaGraphAddKernelNode and do the same thing for cudaGraphLaunch as 2.
    //
    // The above method should handle most of cuda graph launch cases.
    // kernel launches with stream parameter, they can be used for cuda graph
    case API_CUDA_cuLaunchKernel_ptsz:
    case API_CUDA_cuLaunchKernel:
    case API_CUDA_cuLaunchCooperativeKernel:
    case API_CUDA_cuLaunchCooperativeKernel_ptsz:
    case API_CUDA_cuLaunchKernelEx:
    case API_CUDA_cuLaunchKernelEx_ptsz:
    case API_CUDA_cuLaunchGridAsync: {
      CUfunction func;
      CUstream hStream;

      if (cbid == API_CUDA_cuLaunchKernelEx_ptsz || cbid == API_CUDA_cuLaunchKernelEx) {
        cuLaunchKernelEx_params *p = (cuLaunchKernelEx_params *)params;
        func = p->f;
        hStream = p->config->hStream;
      } else if (cbid == API_CUDA_cuLaunchKernel_ptsz || cbid == API_CUDA_cuLaunchKernel ||
                 cbid == API_CUDA_cuLaunchCooperativeKernel_ptsz || cbid == API_CUDA_cuLaunchCooperativeKernel) {
        cuLaunchKernel_params *p = (cuLaunchKernel_params *)params;
        func = p->f;
        hStream = p->hStream;
      } else {
        cuLaunchGridAsync_params *p = (cuLaunchGridAsync_params *)params;
        func = p->f;
        hStream = p->hStream;
      }

      cudaStreamCaptureStatus streamStatus;
      /* check if the stream is capturing, if yes, do not sync */
      CUDA_SAFECALL(cudaStreamIsCapturing(hStream, &streamStatus));
      if (!is_exit) {
        bool stream_capture = (streamStatus == cudaStreamCaptureStatusActive);
        enter_kernel_launch(ctx, func, global_kernel_launch_id, cbid, params, stream_capture);
      } else {
        if (streamStatus != cudaStreamCaptureStatusActive) {
          if (verbose >= 1) {
            loprintf("kernel %s not captured by cuda graph\n", nvbit_get_func_name(ctx, func));
          }
          leave_kernel_launch(ctx_state, global_kernel_launch_id);
        } else {
          if (verbose >= 1) {
            loprintf("kernel %s captured by cuda graph\n", nvbit_get_func_name(ctx, func));
          }
        }
      }
    } break;
    case API_CUDA_cuGraphAddKernelNode: {
      cuGraphAddKernelNode_params *p = (cuGraphAddKernelNode_params *)params;
      CUfunction func = p->nodeParams->func;

      if (!is_exit) {
        // cuGraphAddKernelNode_params->nodeParams is the same as
        // cuLaunchKernel_params up to sharedMemBytes
        enter_kernel_launch(ctx, func, global_kernel_launch_id, cbid, (void *)p->nodeParams, false, true);
      }
    } break;
    case API_CUDA_cuGraphLaunch: {
      // if we are exiting a cuda graph launch:
      // Wait until the graph is completed using
      // cudaStreamSynchronize()
      if (is_exit) {
        cuGraphLaunch_params *p = (cuGraphLaunch_params *)params;

        CUDA_SAFECALL(cudaStreamSynchronize(p->hStream));
        CUDA_CHECK_LAST_ERROR();
        /* push a flush channel kernel */
        flush_channel<<<1, 1, 0, p->hStream>>>(ctx_state->channel_dev);
        CUDA_SAFECALL(cudaStreamSynchronize(p->hStream));
        CUDA_CHECK_LAST_ERROR();
      }

    } break;
    default:
      break;
  };

  skip_callback_flag = false;
  pthread_mutex_unlock(&cuda_event_mutex);
}

// Reference NVIDIA record_reg_vals example
void nvbit_tool_init(CUcontext ctx) {
  pthread_mutex_lock(&mutex);
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  init_context_state(ctx);
  pthread_mutex_unlock(&mutex);
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_ctx_init(CUcontext ctx) {
  pthread_mutex_lock(&mutex);
  if (verbose) {
    printf("MEMTRACE: STARTING CONTEXT %p\n", ctx);
  }
  assert(ctx_state_map.find(ctx) == ctx_state_map.end());
  CTXstate *ctx_state = new CTXstate;
  ctx_state_map[ctx] = ctx_state;
  pthread_mutex_unlock(&mutex);
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_ctx_term(CUcontext ctx) {
  pthread_mutex_lock(&mutex);
  skip_callback_flag = true;
  if (verbose) {
    loprintf("CUTracer: TERMINATING CONTEXT %p\n", ctx);
  }
  /* get context state from map */
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate *ctx_state = ctx_state_map[ctx];

  /* Notify receiver thread and wait for receiver thread to
   * notify back */
  ctx_state->recv_thread_done = RecvThreadState::STOP;
  while (ctx_state->recv_thread_done != RecvThreadState::FINISHED);

  // Clean up any remaining kernel mapping entries
  // (in case there were kernels launched but no data received)
  kernel_launch_to_func_map.clear();
  kernel_launch_to_iter_map.clear();

  // Clean up any remaining kernel mapping entries
  // (in case there were kernels launched but no data received)
  kernel_launch_to_func_map.clear();

  ctx_state->channel_host.destroy(false);
  cudaFree(ctx_state->channel_dev);
  skip_callback_flag = false;
  delete ctx_state;
  pthread_mutex_unlock(&mutex);
  // Cleanup log handle system

  cleanup_log_handle();
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_graph_node_launch(CUcontext ctx, CUfunction func, CUstream stream, uint64_t launch_handle) {
  func_config_t config = {0};
  const char *func_name = nvbit_get_func_name(ctx, func);
  uint64_t pc = nvbit_get_func_addr(ctx, func);

  pthread_mutex_lock(&mutex);
  nvbit_set_at_launch(ctx, func, (uint64_t)global_kernel_launch_id, stream, launch_handle);
  nvbit_get_func_config(ctx, func, &config);

  loprintf(
      "MEMTRACE: CTX 0x%016lx - LAUNCH - Kernel pc 0x%016lx - "
      "Kernel name %s - grid launch id %ld - grid size %d,%d,%d "
      "- block size %d,%d,%d - nregs %d - shmem %d - cuda stream "
      "id %ld\n",
      (uint64_t)ctx, pc, func_name, global_kernel_launch_id, config.gridDimX, config.gridDimY, config.gridDimZ,
      config.blockDimX, config.blockDimY, config.blockDimZ, config.num_registers,
      config.shmem_static_nbytes + config.shmem_dynamic_nbytes, (uint64_t)stream);
  // grid id can be changed here, since nvbit_set_at_launch() has copied its
  // value above.
  global_kernel_launch_id++;
  pthread_mutex_unlock(&mutex);
}

// Reference code from NVIDIA nvbit mem_trace tool with Meta modifications for
// env config
void nvbit_at_init() {
  // Initialize configuration from environment variables
  init_config_from_env();
  /* set mutex as recursive */
  pthread_mutexattr_t attr;
  pthread_mutexattr_init(&attr);
  pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);
  pthread_mutex_init(&mutex, &attr);

  pthread_mutex_init(&cuda_event_mutex, &attr);
}

```

`src/env_config.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#include <stdio.h>
#include <stdlib.h>

#include "env_config.h"
#include "instrument.h"
#include "log.h"

// Define configuration variables
// EVERY VARIABLE MUST BE INITIALIZED IN init_config_from_env()
uint32_t instr_begin_interval;
uint32_t instr_end_interval;
int verbose;
// kernel name filters
std::vector<std::string> kernel_filters;
// enabled instrumentation types
std::unordered_set<InstrumentType> enabled_instrument_types;
// enabled analysis types
std::unordered_set<AnalysisType> enabled_analysis_types;

/**
 * @brief Parses a comma-separated string of kernel name filters for substring matching.
 *
 * This function takes a string from an environment variable, splits it by commas,
 * and populates the global `kernel_filters` vector with the individual filters.
 * These filters are then used to determine which CUDA kernels to instrument by
 * checking if a filter string appears as a **substring** of the kernel's name
 * (either mangled or unmangled).
 * Empty tokens resulting from ",," or trailing/leading commas are ignored.
 *
 * @param filters_env A C-style string containing comma-separated kernel name filters.
 *                     If NULL, the function does nothing.
 *
 * @example
 * If the environment variable (e.g., `KERNEL_FILTERS`) is set as:
 * `export KERNEL_FILTERS="add,_Z2_gemm,reduce"`
 *
 * A kernel named "add_kernel" would be matched by the "add" filter. A kernel
 * named "my_reduce_kernel" would be matched by "reduce".
 *
 * After calling this function with the example string, the `kernel_filters`
 * vector will contain: `{"add", "_Z2_gemm", "reduce"}`
 */
static void parse_kernel_filters(const std::string &filters_env) {
  if (filters_env.empty()) return;

  std::string filters_str = filters_env;
  size_t pos = 0;
  std::string token;

  // Split by commas
  while ((pos = filters_str.find(',')) != std::string::npos) {
    token = filters_str.substr(0, pos);
    if (!token.empty()) {
      kernel_filters.push_back(token);
    }
    filters_str.erase(0, pos + 1);
  }

  // Add the last token (if it exists)
  if (!filters_str.empty()) {
    kernel_filters.push_back(filters_str);
  }

  printf("Kernel name filters to instrument:\n");
  for (const auto &filter : kernel_filters) {
    printf("  - %s\n", filter.c_str());
  }
}

// Helper function for reading environment variables
static void get_var_int(int &var, const char *env_name, int default_val, const char *description) {
  const char *env_val = getenv(env_name);
  if (env_val) {
    var = atoi(env_val);
  } else {
    var = default_val;
  }
  loprintf("%s = %d (%s)\n", env_name, var, description);
}

static void get_var_uint32(uint32_t &var, const char *env_name, uint32_t default_val, const char *description) {
  const char *env_val = getenv(env_name);
  if (env_val) {
    var = (uint32_t)atoll(env_val);
  } else {
    var = default_val;
  }
  loprintf("%s = %u (%s)\n", env_name, var, description);
}

static void get_var_str(std::string &var, const char *env_name, const std::string &default_val,
                        const char *description) {
  const char *env_val = getenv(env_name);
  if (env_val) {
    var = std::string(env_val);
  } else {
    var = default_val;
  }
  loprintf("%s = %s (%s)\n", env_name, var.c_str(), description);
}

/**
 * @brief Initialize instrumentation system based on environment variables
 *
 * Parses CUTRACER_INSTRUMENT environment variable and sets up enabled types.
 * This function is called within init_config_from_env().
 */
void init_instrumentation(const std::string &instrument_str) {
  if (instrument_str.empty()) {
    return;
  }
  loprintf("Using instrumentation types: %s\n", instrument_str.c_str());

  if (instrument_str.find("reg_trace") != std::string::npos) {
    enabled_instrument_types.insert(InstrumentType::REG_TRACE);
    loprintf("  - Enabled: reg_trace (register value tracing)\n");
  }
  if (instrument_str.find("mem_trace") != std::string::npos) {
    enabled_instrument_types.insert(InstrumentType::MEM_TRACE);
    loprintf("  - Enabled: mem_trace (memory access tracing)\n");
  }
}

void init_analysis(const std::string &analysis_str) {
  enabled_analysis_types.clear();

  if (analysis_str.empty()) {
    loprintf("No analysis types specified.\n");
    return;
  }
  loprintf("Using analysis types: %s\n", analysis_str.c_str());

  // Parse comma-separated values
  if (analysis_str.find("proton_instr_histogram") != std::string::npos) {
    enabled_analysis_types.insert(AnalysisType::PROTON_INSTR_HISTOGRAM);
    loprintf("  - Enabled: proton_instr_histogram\n");

    // If proton_instr_histogram is enabled, force opcode_only instrumentation
    if (!is_instrument_type_enabled(InstrumentType::OPCODE_ONLY)) {
      enabled_instrument_types.insert(InstrumentType::OPCODE_ONLY);
      loprintf(
          "`proton_instr_histogram` analysis is enabled, forcing `opcode_only` "
          "instrumentation.\n");
    }
  }

  // deadlock_detection: enable analysis type and ensure REG_TRACE is on
  if (analysis_str.find("deadlock_detection") != std::string::npos) {
    enabled_analysis_types.insert(AnalysisType::DEADLOCK_DETECTION);
    loprintf("  - Enabled: deadlock_detection\n");
    if (!is_instrument_type_enabled(InstrumentType::REG_TRACE)) {
      enabled_instrument_types.insert(InstrumentType::REG_TRACE);
      loprintf("  - deadlock_detection: forcing reg_trace instrumentation\n");
    }
  }
}

/**
 * @brief Check if a specific instrumentation type is enabled
 *
 * @param type The instrumentation type to check
 * @return true if the instrumentation type is enabled
 */
bool is_instrument_type_enabled(InstrumentType type) {
  return enabled_instrument_types.count(type);
}

bool is_analysis_type_enabled(AnalysisType type) {
  return enabled_analysis_types.count(type);
}

// Initialize all configuration variables
void init_config_from_env() {
  // Enable device memory allocation
  setenv("CUDA_MANAGED_FORCE_DEVICE_ALLOC", "1", 1);
  // Initialize log handle
  init_log_handle();
  // Get other configuration variables
  get_var_int(verbose, "TOOL_VERBOSE", 0, "Enable verbosity inside the tool");
  // If INSTRS is not set, fall back to the old INSTR_BEGIN/INSTR_END behavior
  get_var_uint32(instr_begin_interval, "INSTR_BEGIN", 0,
                 "Beginning of the instruction interval where to apply instrumentation");
  get_var_uint32(instr_end_interval, "INSTR_END", UINT32_MAX,
                 "End of the instruction interval where to apply instrumentation");
  std::string instrument_str;
  get_var_str(instrument_str, "CUTRACER_INSTRUMENT", "",
              "Instrumentation types to enable (opcode_only,reg_trace,mem_trace)");
  std::string kernel_filters_env;
  get_var_str(kernel_filters_env, "KERNEL_FILTERS", "", "Kernel name filters");
  std::string analysis_str;
  get_var_str(analysis_str, "CUTRACER_ANALYSIS", "",
              "Analysis types to enable (proton_instr_histogram, deadlock_detection)");

  //===== Initializations ==========
  // Get kernel name filters
  parse_kernel_filters(kernel_filters_env);

  // Clear enabled types at the beginning
  enabled_instrument_types.clear();

  // Initialize analysis first, as it may enable instrumentation types
  init_analysis(analysis_str);
  // Initialize instrumentation from user settings
  init_instrumentation(instrument_str);

  std::string pad(100, '-');
  loprintf("%s\n", pad.c_str());
}

```

`src/inject_funcs.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed under MIT,
 * based on original NVIDIA nvbit sample code licensed under BSD-3-Clause.
 * See LICENSE file in the root directory for Meta's license terms.
 * See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <stdarg.h>
#include <stdint.h>

#include "utils/utils.h"

/* for channel */
#include "common.h"
#include "utils/channel.hpp"

/* Based on NVIDIA NVBit reg_trace example with Meta modifications for
message type, unified register, and kernel launch id support*/
extern "C" __device__ __noinline__ void instrument_reg_val(int pred, int opcode_id, uint64_t pchannel_dev,
                                                           uint64_t kernel_launch_id, uint64_t pc, int32_t num_regs,
                                                           int32_t num_uregs, ...) {
  if (!pred) {
    return;
  }

  int active_mask = __ballot_sync(__activemask(), 1);
  const int laneid = get_laneid();
  const int first_laneid = __ffs(active_mask) - 1;

  reg_info_t ri;

  ri.header.type = MSG_TYPE_REG_INFO;

  int4 cta = get_ctaid();
  ri.cta_id_x = cta.x;
  ri.cta_id_y = cta.y;
  ri.cta_id_z = cta.z;
  ri.warp_id = get_global_warp_id();
  ri.opcode_id = opcode_id;
  ri.num_regs = num_regs;
  ri.num_uregs = num_uregs;
  ri.kernel_launch_id = kernel_launch_id;
  ri.pc = pc;

  if (num_regs || num_uregs) {
    // Initialize variable argument list
    va_list vl;
    va_start(vl, num_uregs);
    for (int i = 0; i < num_regs; i++) {
      uint32_t val = va_arg(vl, uint32_t);

      /* collect register values from other threads */
      for (int tid = 0; tid < 32; tid++) {
        ri.reg_vals[tid][i] = __shfl_sync(active_mask, val, tid);
      }
    }
    // Only the first thread in the warp needs to process unified registers
    if (first_laneid == laneid) {
      for (int i = 0; i < num_uregs; i++) {
        ri.ureg_vals[i] = va_arg(vl, uint32_t);
      }
    }

    va_end(vl);
  }

  if (first_laneid == laneid) {
    ChannelDev *channel_dev = (ChannelDev *)pchannel_dev;
    channel_dev->push(&ri, sizeof(reg_info_t));
  }
}

/* Based on NVIDIA NVBit mem_trace example with Meta modifications for message type */
extern "C" __device__ __noinline__ void instrument_mem(int pred, int opcode_id, uint64_t addr,
                                                       uint64_t kernel_launch_id, uint64_t pc, uint64_t pchannel_dev) {
  /* if thread is predicated off, return */
  if (!pred) {
    return;
  }

  int active_mask = __ballot_sync(__activemask(), 1);
  const int laneid = get_laneid();
  const int first_laneid = __ffs(active_mask) - 1;

  mem_access_t ma;

  ma.header.type = MSG_TYPE_MEM_ACCESS;

  /* collect memory address information from other threads */
  for (int i = 0; i < 32; i++) {
    ma.addrs[i] = __shfl_sync(active_mask, addr, i);
  }
  ma.kernel_launch_id = kernel_launch_id;
  int4 cta = get_ctaid();
  ma.cta_id_x = cta.x;
  ma.cta_id_y = cta.y;
  ma.cta_id_z = cta.z;
  ma.pc = pc;
  ma.warp_id = get_global_warp_id();
  ma.opcode_id = opcode_id;

  /* first active lane pushes information on the channel */
  if (first_laneid == laneid) {
    ChannelDev *channel_dev = (ChannelDev *)pchannel_dev;
    channel_dev->push(&ma, sizeof(mem_access_t));
  }
}

extern "C" __device__ __noinline__ void instrument_opcode(int pred, int opcode_id, uint64_t pchannel_dev,
                                                          uint64_t kernel_launch_id, uint64_t pc) {
  if (!pred) {
    return;
  }

  int active_mask = __ballot_sync(__activemask(), 1);
  const int laneid = get_laneid();
  const int first_laneid = __ffs(active_mask) - 1;

  opcode_only_t oi;
  oi.header.type = MSG_TYPE_OPCODE_ONLY;

  int4 cta = get_ctaid();
  oi.cta_id_x = cta.x;
  oi.cta_id_y = cta.y;
  oi.cta_id_z = cta.z;
  oi.warp_id = get_global_warp_id();
  oi.opcode_id = opcode_id;
  oi.kernel_launch_id = kernel_launch_id;
  oi.pc = pc;

  if (first_laneid == laneid) {
    ChannelDev *channel_dev = (ChannelDev *)pchannel_dev;
    channel_dev->push(&oi, sizeof(opcode_only_t));
  }
}

```

`src/instrument.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed
 * under MIT, based on original NVIDIA NVBit sample code licensed under
 * BSD-3-Clause. See LICENSE file in the root directory for Meta's license
 * terms. See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <cstdlib>

#include "analysis.h"
#include "instrument.h"
#include "nvbit.h"

/**
 * @brief Instruments an instruction to record its opcode for lightweight analysis.
 *
 * This function was developed by Meta to support analyses like instruction
 * histograms, where only the instruction's identity is needed, not its operand
 * values.
 *
 * It injects a call to the `instrument_opcode` device function, passing the
 * opcode ID, program counter (PC), and global kernel launch ID.
 */
void instrument_opcode_only(Instr* instr, int opcode_id, CTXstate* ctx_state) {
  /* insert call to the instrumentation function with its arguments */
  nvbit_insert_call(instr, "instrument_opcode", IPOINT_BEFORE);
  /* guard predicate value */
  nvbit_add_call_arg_guard_pred_val(instr);
  /* opcode id */
  nvbit_add_call_arg_const_val32(instr, opcode_id);
  /* pass the pointer to the channel on the device */
  nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
  /* add "space" for kernel function pointer that will be set
   * at launch time (64 bit value at offset 0 of the dynamic
   * arguments). it is used to pass global kernel launch id*/
  nvbit_add_call_arg_launch_val64(instr, 0);
  /* add instruction offset */
  nvbit_add_call_arg_const_val64(instr, instr->getOffset());
}

/**
 * @brief Instruments an instruction to trace the values of its register operands.
 *
 * This function is based on the instrumentation logic from NVIDIA's
 * `record_reg_vals` example (`third_party/nvbit/tools/record_reg_vals/record_reg_vals.cu`).
 * It injects a call to the `instrument_reg_val` device function to capture operand values.
 *
 * Key enhancements by Meta include:
 *  - **Unified Register (UREG) Support**: Added tracing for UREGs, which was
 *    not present in the original example.
 *  - **Enhanced Context**: Passes the global kernel launch ID and the
 *    instruction's program counter (PC) to correlate data more effectively
 *    during analysis.
 *  - **Refactoring**: Encapsulated the logic into this dedicated function,
 *    separating it from the main instruction iteration loop in `cutracer.cu`.
 */
void instrument_register_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, const std::vector<int>& reg_num_list,
                               const std::vector<int>& ureg_num_list) {
  /* insert call to the instrumentation function with its arguments */
  nvbit_insert_call(instr, "instrument_reg_val", IPOINT_BEFORE);
  /* guard predicate value */
  nvbit_add_call_arg_guard_pred_val(instr);
  /* opcode id */
  nvbit_add_call_arg_const_val32(instr, opcode_id);
  /* add pointer to channel_dev*/
  nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
  /* add "space" for kernel function pointer that will be set
   * at launch time (64 bit value at offset 0 of the dynamic
   * arguments). it is used to pass global kernel launch id*/
  nvbit_add_call_arg_launch_val64(instr, 0);
  /* add instruction offset */
  nvbit_add_call_arg_const_val64(instr, instr->getOffset());
  /* how many register values are passed next */
  nvbit_add_call_arg_const_val32(instr, reg_num_list.size());
  nvbit_add_call_arg_const_val32(instr, ureg_num_list.size());

  for (int num : reg_num_list) {
    /* last parameter tells it is a variadic parameter passed to
     * the instrument function record_reg_val() */
    nvbit_add_call_arg_reg_val(instr, num, true);
  }
  for (int num : ureg_num_list) {
    nvbit_add_call_arg_ureg_val(instr, num, true);
  }
}

/**
 * @brief Instruments a memory instruction to trace memory access details.
 *
 * This function is based on the instrumentation logic from NVIDIA's `mem_trace`
 * example. It injects a call to the `instrument_mem` device function.
 *
 * Meta's enhancements include:
 *  - **Refactoring**: Moving the instrumentation logic from the main loop in
 *    `cutracer.cu` into this modular function.
 *  - **Contextual Information**: Passing the global kernel launch ID and the
 *    instruction's program counter (PC) alongside the memory address. This
 *    allows for more detailed analysis by linking each memory access to a
 *    specific kernel launch and instruction.
 */
void instrument_memory_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, int mref_idx) {
  /* insert call to the instrumentation function with its
   * arguments */
  nvbit_insert_call(instr, "instrument_mem", IPOINT_BEFORE);
  /* predicate value */
  nvbit_add_call_arg_guard_pred_val(instr);
  /* opcode id */
  nvbit_add_call_arg_const_val32(instr, opcode_id);
  /* memory reference 64 bit address */
  nvbit_add_call_arg_mref_addr64(instr, mref_idx);
  /* add "space" for kernel function pointer that will be set
   * at launch time (64 bit value at offset 0 of the dynamic
   * arguments). it is used to pass global kernel launch id*/
  nvbit_add_call_arg_launch_val64(instr, 0);
  /* add instruction offset */
  nvbit_add_call_arg_const_val64(instr, instr->getOffset());
  /* add pointer to channel_dev*/
  nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
}

```

`src/log.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#include <assert.h>
#include <stdarg.h>
#include <stdint.h>
#include <stdio.h>
#include <string.h>
#include <sys/stat.h>
#include <time.h>
#include <unistd.h>

#include <functional>
#include <sstream>
#include <string>

/* nvbit interface file */
#include "nvbit.h"

/* include environment configuration */
#include "env_config.h"

/* include log handle header */
#include "log.h"

/**
 * Computes a stable per-kernel hash (lowercase hex string without "0x") from the mangled name.
 *
 * The function uses the full mangled name (nvbit_get_func_name(ctx, func, true)) and applies
 * std::hash<std::string>, then formats the numeric value in hexadecimal for display/storage.
 */
std::string compute_kernel_name_hash_hex(CUcontext ctx, CUfunction func) {
  const char *mangled_name_raw = nvbit_get_func_name(ctx, func, true);
  if (!mangled_name_raw) {
    mangled_name_raw = "unknown_kernel";
  }
  std::string mangled_name(mangled_name_raw);
  std::hash<std::string> hasher;
  size_t value = hasher(mangled_name);
  std::ostringstream oss;
  oss << std::hex << std::nouppercase << value;
  return oss.str();
}

/**
 * Builds a deterministic base filename for a kernel's trace log.
 *
 * The resulting string embeds:
 *   - A hex hash of the full mangled name (via compute_kernel_name_hash)
 *   - The iteration number (decimal)
 *   - A truncated copy (first 150 chars) of the mangled name for readability
 *
 * Example: "kernel_7fa21c3_iter42__Z23my_kernelPiS_..."
 */
std::string generate_kernel_log_basename(CUcontext ctx, CUfunction func, uint32_t iteration) {
  const char *mangled_name_raw = nvbit_get_func_name(ctx, func, true);
  if (!mangled_name_raw) {
    mangled_name_raw = "unknown_kernel";
  }

  std::string mangled_name(mangled_name_raw);

  // Hash the full name to ensure uniqueness (shared with cutracer)
  std::string name_hash_hex = compute_kernel_name_hash_hex(ctx, func);
  // Truncate the name for the filename string part
  std::string truncated_name = mangled_name.substr(0, 150);

  std::stringstream ss;
  // Format to hex for the hash
  ss << "kernel_" << name_hash_hex << "_iter" << std::dec << iteration << "_" << truncated_name;

  return ss.str();
}

/* ===== Global Variables ===== */

// The main log file for the entire process run
static FILE *g_main_log_file = NULL;
// The currently active log file for kernel traces
static FILE *g_kernel_log_file = NULL;

/* ===== Utility Functions for Logging ===== */

/**
 * @brief Base function for formatted output. Uses va_list to avoid re-formatting.
 *
 * @param file_output if true, output to the active log file
 * @param stdout_output if true, output to stdout
 * @param format format string
 * @param args variable argument list
 */
static void vfprintf_base(bool file_output, bool stdout_output, const char *format, va_list args) {
  if (!file_output && !stdout_output) {
    return;
  }

  char output_buffer[2048];
  vsnprintf(output_buffer, sizeof(output_buffer), format, args);

  if (stdout_output) {
    fprintf(stdout, "%s", output_buffer);
  }

  if (file_output && g_main_log_file) {
    fprintf(g_main_log_file, "%s", output_buffer);
  }
}

void lprintf(const char *format, ...) {
  va_list args;
  va_start(args, format);
  vfprintf_base(true, false, format, args);
  va_end(args);
}

void oprintf(const char *format, ...) {
  va_list args;
  va_start(args, format);
  vfprintf_base(false, true, format, args);
  va_end(args);
}

void loprintf(const char *format, ...) {
  va_list args;
  va_start(args, format);
  vfprintf_base(true, true, format, args);
  // Flush the main log file if it exists
  if (!g_main_log_file) {
    oprintf("ERROR: Main log file not initialized before loprintf\n");
  }

  va_end(args);
}

void trace_lprintf(const char *format, ...) {
  if (!g_kernel_log_file) {
    oprintf("ERROR: Kernel trace log file not initialized before trace_lprintf\n");
    return;
  }

  va_list args;
  va_start(args, format);
  vfprintf(g_kernel_log_file, format, args);
  va_end(args);
}

/* ===== File Management Functions ===== */

void log_open_kernel_file(CUcontext_ptr ctx, CUfunction_ptr func, uint32_t iteration) {
  // close previous log file if it's open
  log_close_kernel_file();

  std::string basename = generate_kernel_log_basename((CUcontext)ctx, (CUfunction)func, iteration);
  std::string log_filename = basename + ".log";

  g_kernel_log_file = fopen(log_filename.c_str(), "w");
  if (g_kernel_log_file) {
    loprintf("Opened kernel trace log: %s\n", log_filename.c_str());
  } else {
    oprintf("ERROR: Failed to open kernel trace log file: %s\n", log_filename.c_str());
  }
}

void log_close_kernel_file() {
  if (g_kernel_log_file) {
    fclose(g_kernel_log_file);
    g_kernel_log_file = NULL;
  }
}

void init_log_handle() {
  // Get current timestamp for filename
  time_t now = time(0);
  struct tm *timeinfo = localtime(&now);
  char timestamp[32];
  strftime(timestamp, sizeof(timestamp), "%Y%m%d_%H%M%S", timeinfo);

  char main_log_filename[256];
  snprintf(main_log_filename, sizeof(main_log_filename), "cutracer_main_%s.log", timestamp);

  g_main_log_file = fopen(main_log_filename, "w");
  if (!g_main_log_file) {
    // Fallback to stdout if file creation fails
    g_main_log_file = stdout;
    oprintf("WARNING: Failed to create main log file. Falling back to stdout.\n");
  }

  loprintf("Log handle system initialized. Main log is %s.\n",
           (g_main_log_file == stdout) ? "stdout" : main_log_filename);
}

void cleanup_log_handle() {
  log_close_kernel_file();

  if (g_main_log_file && g_main_log_file != stdout) {
    fclose(g_main_log_file);
  }

  g_main_log_file = NULL;

  if (verbose) {
    oprintf("Log handle system cleaned up.\n");
  }
}

```

`tests/hang_test/test_hang.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.
import torch
import triton
import triton.language as tl


@triton.jit
def add_kernel(
    a_ptr,
    b_ptr,
    c_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    if pid == 0:
        while pid == pid:
            tl.atomic_add(a_ptr, 1)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    a = tl.load(a_ptr + offsets, mask=mask)
    b = tl.load(b_ptr + offsets, mask=mask)
    c = a + b
    tl.store(c_ptr + offsets, c, mask=mask)


def tensor_add(a, b):
    n_elements = a.numel()
    c = torch.empty_like(a)
    BLOCK_SIZE = 1024
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    add_kernel[grid](a, b, c, n_elements, BLOCK_SIZE)
    return c


def test_tensor_add():
    torch.manual_seed(0)
    size = (1024, 1024)
    a = torch.randn(size, device="cuda", dtype=torch.float32)
    b = torch.randn(size, device="cuda", dtype=torch.float32)

    # Test Triton kernel
    c_triton = tensor_add(a, b)
    c_triton.sum()
    tensor_add(a, b)
    print("Triton kernel executed successfully")


if __name__ == "__main__":
    test_tensor_add()

```

`tests/proton_tests/vector-add-instrumented.py`:

```py
from typing import NamedTuple

import torch
import triton
import triton.language as tl
import triton.profiler as proton
import triton.profiler.language as pl

DEVICE = triton.runtime.driver.active.get_active_torch_device()

pl.enable_semantic("triton")


def metadata_fn(grid: tuple, metadata: NamedTuple, args: dict):
    BLOCK_SIZE = args["BLOCK_SIZE"]
    return {"name": f"add_{BLOCK_SIZE}"}


@triton.jit(launch_metadata=metadata_fn)
def add_kernel(
    x_ptr,  # *Pointer* to first input vector.
    y_ptr,  # *Pointer* to second input vector.
    output_ptr,  # *Pointer* to output vector.
    n_elements,  # Size of the vector.
    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
    # NOTE: `constexpr` so it can be used as a shape value.
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    with pl.scope("load_ops"):
        x = tl.load(x_ptr + offsets, mask=mask)
        y = tl.load(y_ptr + offsets, mask=mask)
        output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)


def add(x: torch.Tensor, y: torch.Tensor):
    output = torch.empty_like(x)
    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE
    n_elements = output.numel()

    def grid(meta):
        return (triton.cdiv(n_elements, meta["BLOCK_SIZE"]),)

    mode = proton.mode.Default(metric_type="cycle", optimizations="clock32")
    proton.start("vector", data="trace", backend="instrumentation", mode=mode)

    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024, num_warps=1)

    proton.finalize()
    return output


torch.manual_seed(0)
size = 98432
x = torch.rand(size, device=DEVICE)
y = torch.rand(size, device=DEVICE)
output_torch = x + y
output_triton = add(x, y)
output_triton = add(x, y)

```

`tests/py_add/test_add.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.
import torch


def test_tensor_addition_on_gpu():
    device = torch.device("cuda")

    a = torch.tensor([1, 2, 3], dtype=torch.float32, device=device)
    b = torch.tensor([2, 3, 4], dtype=torch.float32, device=device)

    print("Tensor A:", a)
    print("Tensor B:", b)
    for _ in range(3):
        a = a + b

    print("Result (A + B):", a)

    return a


if __name__ == "__main__":
    test_tensor_addition_on_gpu()

```

`tests/vectoradd/Makefile`:

```
NVCCFLAGS := -arch=all -O3 -g -lineinfo

all: vectoradd.cu
	nvcc $(NVCCFLAGS) vectoradd.cu -o vectoradd

clean:
	rm -f vectoradd

```

`tests/vectoradd/vectoradd.cu`:

```cu
// Copyright (c) Meta Platforms, Inc. and affiliates.
#include <math.h>
#include <stdio.h>
#include <stdlib.h>

#define CUDA_SAFECALL(call)                                                                                  \
  {                                                                                                          \
    call;                                                                                                    \
    cudaError err = cudaGetLastError();                                                                      \
    if (cudaSuccess != err) {                                                                                \
      fprintf(stderr, "Cuda error in function '%s' file '%s' in line %i : %s.\n", #call, __FILE__, __LINE__, \
              cudaGetErrorString(err));                                                                      \
      fflush(stderr);                                                                                        \
      exit(EXIT_FAILURE);                                                                                    \
    }                                                                                                        \
  }

// CUDA kernel. Each thread takes care of one element of c
__global__ void vecAdd(double *a, double *b, double *c, int n) {
  // Get our global thread ID
  auto id = blockIdx.x * blockDim.x + threadIdx.x;

  // Make sure we do not go out of bounds
  if (id < n) c[id] = a[id] + b[id];
}

int main(int argc, char *argv[]) {
  // Size of vectors
  int n = 1024;
  if (argc > 1) n = atoi(argv[1]);

  // Host input vectors
  double *h_a;
  double *h_b;
  // Host output vector
  double *h_c;

  // Device input vectors
  double *d_a;
  double *d_b;
  // Device output vector
  double *d_c;

  // Size, in bytes, of each vector
  size_t bytes = n * sizeof(double);

  // Allocate memory for each vector on host
  h_a = (double *)malloc(bytes);
  h_b = (double *)malloc(bytes);
  h_c = (double *)malloc(bytes);

  // Allocate memory for each vector on GPU
  cudaMalloc(&d_a, bytes);
  cudaMalloc(&d_b, bytes);
  cudaMalloc(&d_c, bytes);

  int i;
  // Initialize vectors on host
  for (i = 0; i < n; i++) {
    h_a[i] = sin(i) * sin(i);
    h_b[i] = cos(i) * cos(i);
    h_c[i] = 0;
  }

  // Copy host vectors to device
  cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);
  cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);
  cudaMemcpy(d_c, h_c, bytes, cudaMemcpyHostToDevice);

  int blockSize, gridSize;

  // Number of threads in each thread block
  blockSize = 128;

  // Number of thread blocks in grid
  gridSize = (n + blockSize - 1) / blockSize;

  // Execute the kernel
  CUDA_SAFECALL((vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n)));

  // Copy array back to host
  cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);

  // Sum up vector c and print result divided by n, this should equal 1 within
  // error
  double sum = 0;
  for (i = 0; i < n; i++) sum += h_c[i];
  printf("Final sum = %f; sum/n = %f (should be ~1)\n", sum, sum / n);

  // Release device memory
  cudaFree(d_a);
  cudaFree(d_b);
  cudaFree(d_c);

  // Release host memory
  free(h_a);
  free(h_b);
  free(h_c);

  return 0;
}

```